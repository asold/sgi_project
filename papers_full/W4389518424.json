{
  "title": "Beyond English: Evaluating LLMs for Arabic Grammatical Error Correction",
  "url": "https://openalex.org/W4389518424",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2544175496",
      "name": "Sang Kwon",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2464517642",
      "name": "Gagan Bhatia",
      "affiliations": [
        "Language Science (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2620602306",
      "name": "El Moatez Billah Nagoudi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3119026467",
      "name": "Muhammad Abdul-Mageed",
      "affiliations": [
        "Language Science (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2803237843",
    "https://openalex.org/W2806962830",
    "https://openalex.org/W4385573810",
    "https://openalex.org/W3089057597",
    "https://openalex.org/W2970744242",
    "https://openalex.org/W2251944986",
    "https://openalex.org/W2970814728",
    "https://openalex.org/W3175441946",
    "https://openalex.org/W4362655849",
    "https://openalex.org/W3102516861",
    "https://openalex.org/W2948335087",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2250504723",
    "https://openalex.org/W2890328620",
    "https://openalex.org/W3104196571",
    "https://openalex.org/W2251816508",
    "https://openalex.org/W2970868759",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W2785047343",
    "https://openalex.org/W2964082031",
    "https://openalex.org/W3174828871",
    "https://openalex.org/W2160233880",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4205163778",
    "https://openalex.org/W3156993586",
    "https://openalex.org/W2251851155",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4285127897",
    "https://openalex.org/W4285077564",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2098297786",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4361019538",
    "https://openalex.org/W3156962704",
    "https://openalex.org/W3116890009",
    "https://openalex.org/W4367599042",
    "https://openalex.org/W2251862950",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4381461529",
    "https://openalex.org/W4385572487",
    "https://openalex.org/W3037162118",
    "https://openalex.org/W4385574095",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4378473736",
    "https://openalex.org/W2153013403",
    "https://openalex.org/W4361229539",
    "https://openalex.org/W3169483174"
  ],
  "abstract": "Large language models (LLMs) finetuned to follow human instruction have recently exhibited significant capabilities in various English NLP tasks. However, their performance in grammatical error correction (GEC), especially on languages other than English, remains significantly unexplored. In this work, we evaluate the abilities of instruction finetuned LLMs in Arabic GEC, a complex task due to Arabic’s rich morphology. Our findings suggest that various prompting methods, coupled with (in-context) few-shot learning, demonstrate considerable effectiveness, with GPT-4 achieving up to 65.49 F1 score under expert prompting (approximately 5 points higher than our established baseline). Despite these positive results, we find that instruction finetuned models, regardless of their size, are still outperformed by fully finetuned ones, even if they are significantly smaller in size. This disparity highlights substantial room for improvements for LLMs. Inspired by methods used in low-resource machine translation, we also develop a method exploiting synthetic data that significantly outperforms previous models on two standard Arabic benchmarks. Our best model achieves a new SOTA on Arabic GEC, with 73.29 and 73.26 F1 on the 2014 and 2015 QALB datasets, respectively, compared to peer-reviewed published baselines.",
  "full_text": "Proceedings of the The First Arabic Natural Language Processing Conference (ArabicNLP 2023), pages 101–119\nDecember 7, 2023 ©2023 Association for Computational Linguistics\nBeyond English:\nEvaluating LLMs for Arabic Grammatical Error Correction\nSang Yun Kwonξ Gagan Bhatia ξ El Moatez Billah Nagoudiξ\nMuhammad Abdul-Mageedξ,ω\nξDeep Learning & Natural Language Processing Group, The University of British Columbia\nωDepartment of Natural Language Processing & Department of Machine Learning, MBZUAI\n{skwon01@student.,gagan30@student.,muhammad.mageed@}ubc.ca\nAbstract\nLarge language models (LLMs) finetuned to\nfollow human instruction have recently exhib-\nited significant capabilities in various English\nNLP tasks. However, their performance in\ngrammatical error correction (GEC), especially\non languages other than English, remains sig-\nnificantly unexplored. In this work, we evaluate\nthe abilities of instruction finetuned LLMs in\nArabic GEC, a complex task due to Arabic’s\nrich morphology. Our findings suggest that\nvarious prompting methods, coupled with (in-\ncontext) few-shot learning, demonstrate con-\nsiderable effectiveness, with GPT-4 achieving\nup to 65.49 F1 score under expert prompting\n(approximately 5 points higher than our estab-\nlished baseline). Despite these positive results,\nwe find that instruction finetuned models, re-\ngardless of their size, are still outperformed by\nfully finetuned ones, even if they are signifi-\ncantly smaller in size. This disparity highlights\nsubstantial room for improvements for LLMs.\nInspired by methods used in low-resource ma-\nchine translation, we also develop a method\nexploiting synthetic data that significantly out-\nperforms previous models on two standard Ara-\nbic benchmarks. Our best model achieves a\nnew SOTA on Arabic GEC, with 73.29 and\n73.26 F1 on the 2014 and 2015 QALB datasets,\nrespectively, compared to peer-reviewed pub-\nlished baselines.\n1 Introduction\nAs interest in second language learning continues\nto grow, ensuring the accuracy and effectiveness\nof written language becomes increasingly signif-\nicant for pedagogical tools and language evalua-\ntion (Rothe et al., 2021; Tarnavskyi et al., 2022). A\nkey component in this respect is grammatical error\ncorrection (GEC), a sub-area of natural language\ngeneration (NLG), which analyzes written text to\nautomatically detect and correct diverse grammat-\nical errors. Figure 1 shows an instance of GEC\nfrom Mohit et al. (2014). Despite the growing at-\ntention to GEC, it is predominantly studied within\nFigure 1: An example of an Arabic GEC system show-\ncasing six types of errors: character replacement ,\nmissing word , hamza error , missing punctuation ,\nadditional character , and punctuation confusion .\nthe English language. Extending GEC systems to\nother languages presents significant challenge, due\nto lack of high-quality parallel data and/or inher-\nent challenges in these languages. Recognizing\nthis, our work focuses on Arabic. In addition to\nbeing less-explored for GEC (Mohit et al., 2014;\nRozovskaya et al., 2015a; Mohit et al., 2014; Ro-\nzovskaya et al., 2015a; Solyman et al., 2022; Al-\nhafni et al., 2023), Arabic has complex grammar\nand rich morphology that present significant chal-\nlenges and further motivate our work.\nFocusing primarily on English, the field of\nGEC has witnessed significant advancements,\nspecifically with the emergence of sequence-to-\nsequence (seq2seq) (Chollampatt and Ng, 2018;\nGong et al., 2022) and sequence-to-edit approaches\n(seq2edit) (Awasthi et al., 2019; Omelianchuk\net al., 2020) achieving SoTA results in the CONLL-\n2014 (Ng et al., 2014) and the BEA-2019 shared\ntask (Bryant et al., 2019), respectively. In spite of\nthe efficacy of these approaches, they rely heavily\non large amounts of labeled data. This poses is-\nsues in low-resource scenarios (Feng et al., 2021).\nYet, scaled up language models, aka large lan-\nguage models (LLMs) have recently demonstrated\nremarkable potential in various NLP tasks. The\ncore strength of LLMs lies in their capacity to gen-\n101\neralize across a wide range of languages and tasks,\nand in-context learning (ICL), enabling them to\nhandle various NLP tasks with just a few examples\n(i.e., few-shot learning). A key strategy for LLMs\nis instruction fine-tuning, where they are refined on\na collection of tasks formulated as instructions (Wei\net al., 2022a). This process amplifies the models’\nability to respond accurately to directives, reduc-\ning the need for few-shot examples (Ouyang et al.,\n2022; Wei et al., 2022b; Sanh et al., 2021).\nGiven the ability of LLMs to adeptly address the\nlow-resource challenge, we investigate them in the\ncontext of GEC. Focusing primarily on ChatGPT,\nwe examine the effectiveness of various prompt-\ning strategies such as few-shot chain of thought\n(CoT) prompting (Kojima et al., 2022) and expert\nprompting (Xu et al., 2023). Our research extends\nthe realm of GEC research by concentrating on\nthe unique challenges posed by Arabic. Drawing\nupon the work of Junczys-Dowmunt et al. (2018a),\nwe frame these challenges within the context of a\nlow-resource MT task. We then carefully conduct\na thorough comparison of the different methodolo-\ngies employed in addressing GEC in Arabic. Our\nkey contributions in this paper are as follows:\n1. We conduct a comprehensive investigation of\nthe potential of LLMs for tasks involving GEC\nin Arabic.\n2. We methodically investigate the utility of dif-\nferent prompting methods for generating syn-\nthetic data with ChatGPT for GEC.\n3. We further carry out in-depth comparisons be-\ntween several approaches (seq2seq, seq2edit,\nand instruction fine-tuning) for Arabic GEC\n(AGEC), allowing us to offer novel insights\nas to the utility of these approaches.\nThe rest of this paper is organized as follows: In\nSection 2, we review related work with a particu-\nlar emphasis on Arabic. In Section 3, we outline\nour experimental setups. We present our experi-\nments on LLMs and prompting strategies in Sec-\ntion 4. In Section 5, we introduce our seq2seq\napproach along with data augmentation techniques;\nSection 6 discusses our seq2edit approach. In Sec-\ntion 7, we conduct a comprehensive analysis of our\nbest model. We discuss our results in Section 8,\nand conclude in Section 9.\n2 Related Work\nProgress in GEC. Pretrained Transformer models\nhave reframed GEC as an MT task, achieving SoTA\nresults (Ng et al., 2014; Felice et al., 2014; Junczys-\nDowmunt et al., 2018b; Grundkiewicz et al., 2019).\nIn contrast, sequence2edit approaches view the task\nas text-to-edit, converting input sentences into edit\noperations to produce corrected sentences (Malmi\net al., 2019; Awasthi et al., 2019; Omelianchuk\net al., 2020). These approaches both streamline\nthe training process and enhance model accuracy.\nFurther progress has also been made through meth-\nods such as instruction fine-tuning (Chung et al.,\n2022) and innovative prompting techniques, such\nas CoT (Kojima et al., 2022) and Expert (Xu et al.,\n2023) prompting. Recent applications of LLMs,\nlike ChatGPT in GEC, highlight their potential. We\nprovide further details on each of these methods in\nAppendix A.\nArabic GEC. Challenges in AGEC stem from the\ncomplexity and morphological richness of Arabic.\nArabic, being a collection of a diverse array of lan-\nguages and dialectal varieties with Modern Stan-\ndard Arabic (MSA) as a contemporary variety, is\nfurther complicated by the optional use of diacrit-\nics. This introduces orthographic ambiguity, fur-\nther complicating GEC in Arabic (Abdul-Mageed\net al., 2020; Belkebir and Habash, 2021). De-\nspite these challenges, progress in AGEC has been\nmade. This includes development of benchmark\ndatasets through the QALB-2014 and 2015 shared\ntasks (Mohit et al., 2014; Rozovskaya et al., 2015b;\nHabash and Palfreyman, 2022), and introduction\nof synthetic datasets (Solyman et al., 2021, 2023).\nAs for model development, character-level seq2seq\nmodels (Watson et al., 2018) and other novel ap-\nproaches are shown to be effective on AGEC L1\ndata. Further details about progress in AGEC are\nprovided in Appendix A. Despite this progress, no\nexploration has been undertaken into the utility of\nusing ChatGPT (or other LLMs) for AGEC. More-\nover, substantial work remains in exploring syn-\nthetic data generation, including the use of LLMs\nand the adoption of diverse machine learning ap-\nproaches. Our research aims to address these gap.\n3 Experimental Setup\n3.1 Datasets\nIn this study, we make use of the QALB-2014 (Mo-\nhit et al., 2014) and 2015 (Rozovskaya et al.,\n2015b) datasets to evaluate the performance of our\n102\nDataset Statistics Train Dev Test Level\nQALB-2014Number of sents.19,411 1 ,017 968 L1Number of words.1,021,165 54,000 51,000 L1Number of error.306,000 16,000 16,000 L1\nQALB-2015Number of sents.310 154 920 L2Number of words.43,353 24,742 48,547 L2Number of error.13,200 7 ,300 13,000 L2\nTable 1: Statistics for QALB-2014 and 2015 Train, de-\nvelopment (Dev), and Test datasets.\nmodels. Both datasets make use of the QALB cor-\npus (Zaghouani et al., 2014), a manually corrected\ncollection of Arabic texts. These texts include on-\nline commentaries from Aljazeera articles in MSA\nby L1 native speakers, as well as texts produced\nby L2 learners of Arabic. Both the QALB 2014\nand 2015 datasets are split into training (Train),\ndevelopment (Dev), and test (Test) sets based on\ntheir annotated dates. QALB 2015 includes L1\ncommentaries and L2 texts that cover different gen-\nres and error types. For the purposes of our study,\nwe exclusively use the L1 test set (2015), as we\nfocus on sentence-level AGEC, where L2 test sets\nare document-level. We used Train, Dev, and Test\nsplits described in Table 1.\n3.2 Evaluation\nMetrics. For evaluation, we utilize the overlap-\nbased metric MaxMatch (M2) (Dahlmeier and Ng,\n2012), which aligns source and hypothesis sen-\ntences based on Levenshtein distance , selecting\nmaximal matching edits, scoring the precision (P),\nrecall (R), and F1 measure. Moreover, we report\nthe F 0.5 score , a variation of the F 1 score that\nplaces twice as much weight on precision than on\nrecall. This reflects a consensus, in alignment with\nrecent works on GEC, that precision holds greater\nimportance than error correction in GEC systems.\nImportantly, we use the exact scripts provided from\nthe shared task for evaluation, ensuring consistency\nwith other studies.\n3.3 Models & Fine-tuning\nLLMs. To evaluate the capabilities of LLMs for\nAGEC, we prompt and fine-tune LLMs of varying\nsizes, including LLaMA-7B (Touvron et al., 2023),\nVicuna-13B (Chiang et al., 2023), Bactrian-Xbloom-\n7B (Li et al., 2023), and Bactrian-X llama-7B (Li\net al., 2023). For experiments with ChatGPT, we\nuse the official API to prompt ChatGPT-3.5 Turbo\nand GPT-4. We instruction fine-tune each smaller\nmodel for 4 epochs using a learning rate of 2e-5 and\na batch size of 4. We then pick the best-performing\nmodel on our Dev, then report on our blind Test.\nSeq2seq models. Our baseline settings for seq2seq\nmodels include AraBart (Eddine et al., 2022) and\nAraT5v2 (Nagoudi et al., 2022), both of which are\ntext-to-text transformers specifically tailored for\nArabic. We also evaluate the performance of the\nmT0 (Muennighoff et al., 2022) and mT5 (Xue\net al., 2020) variants of the T5 model (Raffel et al.,\n2020), both configured for multilingual tasks. Each\nmodel is fine-tuned for 50 epochs, with an early\nstopping patience of 5 using a learning rate of 5e-\n5 and a batch size of 32. These models serve as\nthe baseline for comparison throughout our experi-\nments.\nSeq2edit models. ARBERTv2 and\nMARBERTv2 (Abdul-Mageed et al., 2021)\nserve as the baselines for our seq2edit experiments.\nWe fine-tune each model for 100 epochs for each\ntraining stage, employing a learning rate of 1e-5\nand a batch size of 4, with an early stopping\npatience of 5.\nAll models are trained for three runs, with\nseeds of 22, 32, and 42. We then select the best-\nperforming model based on our Dev data for blind-\ntesting on the Test sets. We report the mean score\nof the three runs, along with its standard deviation.\nResults on the Dev set, and more details regarding\nhyperparameters are provided in Appendix 15, and\nAppendix 14.\n4 LLMs and Prompting Techniques\nThis section outlines our experiments designed to\ninstruction fine-tune LLMs and explore different\nprompting methods for ChatGPT in the context\nof AGEC. We begin by experimenting with vari-\nous prompting strategies using ChatGPT, compar-\ning its performance against smaller LLMs and our\nlisted baselines. We evaluate the performance of\nChatGPT-3.5 Turbo (ChatGPT) and GPT-4, under\ntwo prompting strategies: Few-shot CoT (Fang\net al., 2023) and Expert Prompting (Xu et al.,\n2023). We now describe our prompting strategies.\n4.1 ChatGPT Prompting\nPreliminary experiment. Initially, we experiment\nwith a diverse set of prompt templates to assess\nChatGPT’s capabilities in zero-shot learning as\nwell as two aspects of few-shot learning: vanilla\nfew-shot and few-shot CoT (Fang et al., 2023). We\nalso experiment with prompts in both English and\nArabic. However, we discover that the responses\nfrom these prompt templates contain extraneous\n103\nexplanations and are disorganized, necessitating\nsubstantial preprocessing for compatibility with\nthe M2 scorer. This problem is particularly notable\nin the zero-shot and Arabic prompt setups, which\nfails to yield output we can automatically evaluate.\nFew-shot CoT.Adopting the few-shot CoT prompt\ndesign strategy from Kojima et al. (2022) and Fang\net al. (2023), we implement a two-stage approach.\nInitially, we engage in ‘reasoning extraction’ ,\nprompting the model to formulate an elaborate\nreasoning pathway. This is followed by an ‘an-\nswer extraction’ phase, where the reasoning text\nis combined with an answer-specific trigger sen-\ntence to form a comprehensive prompt. In our few-\nshot CoT settings, we include labeled instances\nfrom the Dev set in our prompts to implement\nICL, facilitating learning from examples (Brown\net al., 2020). This involves providing erro-\nneous sentences, labeled <input> SRC </input> ,\nalong with their corrected versions, labeled\n<output> TGT </output> , from the original Dev\nset.\nExpert prompting. Xu et al. (2023) introduces\na novel strategy, which leverages the expert-like\ncapabilities of LLMs. This method involves assign-\ning expert personas to LLMs, providing specific\ninstructions to enhance the relevance and quality of\nthe generated responses. Following the framework\nof Xu et al. (2023), we ensure that our AGEC cor-\nrection tool exhibits three key characteristics: being\ndistinguished, informative, and automatic during\nthe ‘reasoning extraction’ stage of our prompt. To\nachieve this, we employ a distinct and informa-\ntive collection of various error types as proposed\nin the Arabic Learner Corpus taxonomy (Alfaifi\nand Atwell, 2012). We then prompt to automate\nthe system by instructing it to operate on sentences\nlabeled with <input> and <output> tags. Both\nprompts are illustrated in Figure 2.\n4.2 ChatGPT Results.\nTable 2 presents the performance of ChatGPT un-\nder different prompting strategies, compared to the\nbaseline settings. We observe improvements, par-\nticularly as we progress from the one-shot to five-\nshot configurations for both the few-shot CoT and\nexpert prompting (EP) strategies. Under the CoT\nprompt, ChatGPT’s F1.0 score increases from 53.59\nin the one-shot setting to 62.04 in the five-shot set-\nting. A similar upward trend is evident with the EP\nstrategy, where the F1.0 score rises from 55.56 (one-\nshot) to 63.98 (five-shot). Among all experiments\nSettings Models Exact Match\nP R F 1.0 F0.5\nBaselines\nmT0 70.76±0.03 50.78±0.07 59.12±0.05 65.59±0.03\nmT5 70.64±0.12 50.16±0.05 58.66±0.05 65.30±0.09\nAraBART 70.71±0.06 60.46±0.04 65.18±0.07 68.39±0.08\nAraT5v2 73.04±0.1063.09±0.1567.70±0.1270.81±0.11\n+ CoTChatGPT (1-shot)58.71 49 .29 53 .59 56 .55ChatGPT (3-shot)64.60 60 .37 62 .41 63 .71ChatGPT (5-shot)64.70 59 .59 62 .04 63 .61\n+ EP ChatGPT (1-shot)60.49 51 .37 55 .56 58 .42ChatGPT (3-shot)65.83 61 .41 63 .54 64 .90ChatGPT (5-shot)66.53 61 .62 63 .98 65 .49\n+ CoTGPT4 (1-shot)∗ − − − −GPT4 (3-shot)69.31 59 .24 63 .88 67 .03GPT4 (5-shot)69.46 61 .96 65 .49 67 .82\nTable 2: Performance of ChatGPT under different\nprompting strategies on QALB-2014 Test set.\n∗\nResults\nfor QALB-2015 Test and GPT4 1-shot are not included\ndue to the high cost in producing these results, and a\npattern has already been established showing that per-\nformance increases as we increase the number of N-shot\nexamples. More details are in Appendix B.2.\ninvolving ChatGPT, the three-shot and five-shot\nsettings of GPT-4, CoT, achieve the highest scores,\nwith F1.0 of 63.98 and 65.49, respectively.\n4.3 Instruction-Finetuning LLMs\nFine-tuning LLMs. To instruct fine-tune rela-\ntively large models, henceforth just LLMs, we\nfirst train these models on the translated Alpaca\ndataset (Taori et al., 2023) 1 to allow the models\nto gain deeper understanding of the Arabic lan-\nguage and its complexities. Following this, we\nfurther fine-tune the models on the QALB dataset,\nto specifically target the task of GEC. Then, we\nemploy well-structured task instructions and input\nprompts, enabling the models to take on GEC tasks.\nEach model is assigned a task, given an instruction\nand an input for output generation. We provide\nan illustration of the instructions we use for model\ntraining in Appendix B.\nLLM results. As shown in Figure 3, larger mod-\nels such as Vicuna-13B and models trained on\nmultilingual datasets like Bactrian-Xllama-7B, and\nBactrian-Xbloom-7B exhibit an overall trend of bet-\nter performance, achieving F1 of 58.30, 50.1, and\n52.5, respectively. Despite these improvements,\nit is noteworthy that all models fall short of Chat-\nGPT’s. This reaffirms ChatGPT’s superior ability\non AGEC.\n5 Data Augmentation\nMotivated by the significant improvements ob-\nserved in low-resource GEC tasks in languages\n1We translate the Alpaca datasets using NLLB MT\nmodel (Costa-jussà et al., 2022)\n104\nFigure 2: Illustration of Few-Shot CoT and Expert Prompts for Arabic Grammatical Error Correction.\nFigure 3: Comparison of F1 scores between LLMs and\nChatGPT on the QALB-2014 Test set.\nsuch as German, Russian, and Czech through syn-\nthetic data (Flachs et al., 2021), and recognizing\nthe recent efforts to develop synthetic data for\nAGEC (Solyman et al., 2021), we experiment with\nthree distinctive data augmentation methods.\nChatGPT as corruptor. With slight adaptation\nto our original prompt, we engage ChatGPT as an\nAI model with the role of introducing grammatical\nerrors into Arabic text to generate artificial data.\nWe randomly sample 10,000 correct sentences from\nthe QALB-2014 Train set and, using the taxonomy\nput forth by the Arabic Learner Corpus (Alfaifi and\nAtwell, 2012), prompt ChatGPT to corrupt these,\ncreating a parallel dataset. We refer to the resulting\ndataset as syntheticGPT.\nReverse noising. We adopt a reverse noising ap-\nproach (Xie et al., 2018), training a reverse model\nthat converts clean sentences Y into noisy coun-\nterparts X. This involves implementing a standard\nbeam search to create noisy targets ˆY from clean\ninput sentences Y. Our approach incorporates two\ntypes of reverse models: the first trains on both\nQALB-2014 and 2015 gold datasets, and the sec-\nond on the syntheticGPT dataset. Subsequently\nwe generate a parallel dataset using commentaries\nfrom the same newspaper domain as our primary\nclean inputs, matching the original Train data. We\nname the respective parallel datasets reverseGold,\nand reverseGPT.\nData augmentation evaluation. To evaluate the\nefficacy of ChatGPT in generating artificial data,\nwe select 10,000 parallel sentences from synthet-\nicGPT, 10,000 examples from reverseGPT, and\n10,000 parallel sentences from the original train-\ning set. We then further fine-tune each model on the\noriginal training dataset and the two synthetically\ngenerated reverse noised datasets, aiming to assess\nif these artificially crafted datasets can replace the\ngold standard training set. Figure 4 shows our re-\nsults. In our initial tests (Figure 4.a), fine-tuning\nthe AraT5v2 model exclusively on the 10,000 sen-\ntences from syntheticGPT, registers an F1 of 65.87,\nand reverseGPT an F 1 score of 46.85 falling be-\nhind the original QALB 2014 training data (which\nrecords an F1 of 68.34). Following this, when fur-\n105\nFigure 4: Scores of models fine-tuned on 10,000 parallel sentences from different sources: Original training data,\nsyntheticGPT, and reverseGPT evaluated on the QALB-2014 Test set.\nther fine-tuned on the original training set (Fig-\nure 4.b). We find that both syntheticGPT and the re-\nverseGPT surpass model fine-tuned on equivalent-\nsized gold dataset, with F 1 of 69.01 and 68.54,\nrespectively. This confirms the utility of ChatGPT\nfor generating synthetic data. Conversely, when\nwe further fine-tune the model with the two reverse\nnoised datasets (see Figures 4.c and d), we observe\na sharp decline in performance. This emphasizes\nthe critical importance of relevant, high-quality syn-\nthetic data over randomly generated samples.\n5.1 Decoding Methods.\nDecoding strategies for text generation are essen-\ntial and can vary based on the task (Zhang et al.,\n2023). We compare three decoding strategies to\nidentify the best method for AGEC task. As shown\nin Table 3, we compare greedy decoding (Ger-\nmann, 2003) (temperature=0), Beam search (Fre-\nitag and Al-Onaizan, 2017) (num_beams=5, tem-\nperature=1), and Top-P sampling (Holtzman et al.,\n2019) (top-p=0.8, top-k=75, and temperature=0.8).\nWith the highest scoring strategy identified, we\nscale up our data augmentation experiments, by\ngenerating sets of 5million and 10million re-\nverseGold datasets. In addition to these datasets,\nwe utilize the complete AGEC dataset from Soly-\nman et al. (2021) (referred to as AraT5v2 (11M) in\nour experiments) for further evaluation.\nOutlined in Table 4, AraT5v2 shows consistent\nimprovement as the number of training samples\nincreases from 5M to 11M. Results indicate Top-\nP sampling is the best decoding method for GEC,\nexhibiting a balance between number of correct\nStrategy QALB-2014 QALB-2015\nP R F 1 F0.5 P R F 1 F0.5\nGreedy74.09±0.5765.63±0.5969.60±0.5472.23±0.5567.41±0.8266.85±0.9767.13±0.8267.30±0.80\nBeam75.47±1.1168.61±1.2671.87±1.1973.99±1.1470.54±0.4468.04±0.1469.27±0.2470.03±0.35\nTop-p76.94±0.6769.26±0.7372.90±0.6875.27±0.6772.64±0.3274.21±0.7573.41±0.5172.94±0.39\nTable 3: Performance of AraT5v2 (11M) on QALB-2014\nand 2015 Test set under different decoding methods.\nDatasets QALB-2014 QALB-2015\nP R F 1 F0.5 P R F 1 F0.5\nM1 71.35±0.1464.45±0.4167.73±0.1769.85±0.0469.65±0.5764.74±0.5767.11±0.1468.61±0.33\nM2 73.14±0.2667.48±1.0770.23±0.1572.37±1.0570.26±1.1665.74±1.3767.93±1.2769.31±1.20\nM3 76.94±0.6769.26±0.7372.90±0.6875.27±0.6772.64±0.3274.21±0.7573.41±0.5172.94±0.39\nTable 4: Performance of AraT5v2 models using the ’Top-\nP’ decoding method on QALB-2014 and 2015 Test sets,\non different amounts of training data. M1 : AraT5 v2\n(5M), M2 : AraT5v2 (10M), M3 : AraT5v2 (11M)\nedits and total number of edits made.\n6 Sequence Tagging Approach\nIn this section, we detail our methods to adapt the\nGECToR model (Omelianchuk et al., 2020) to ex-\nperiment with the seq2edit approach.\nToken-level transformations. We first perform\ntoken-level transformations on the source to re-\ncover the target text. ‘Basic-transformations’ are\napplied to perform the most common token-level\nedit operations, such as keeping the current to-\nken unchanged ( $KEEP), deleting current token\n($DELETE), appending new token t_ 1 next to\nthe current token x i ($APPEND_t1) or replac-\ning the current token x i with another token t_ 2\n($REPLACE_t2). To apply tokens with more task-\nspecific operations, we employ‘g-transformations’\n106\nMethods ModelsQALB-2014 QALB-2015P R F 1.0 F0.5 P R F 1.0 F0.5\nSeq2Seq\nmT0 70.76±0.0350.78±0.0759.12±0.0565.59±0.0368.11±0.2059.68±0.1263.61±0.1566.23±0.18\nmT5 70.64±0.1250.16±0.0558.66±0.0565.30±0.0968.20±0.1059.02±0.1563.28±0.0466.14±0.11\nAraBART70.71±0.0660.46±0.0465.18±0.0768.39±0.0868.39±0.0967.95±0.0265.62±0.0566.76±0.07\nAraT5v2 73.04±0.1063.09±0.1567.70±0.1270.81±0.1171.40±0.9072.83±1.1172.11±0.9971.68±0.93\nSeq2edit\nARBERTv273.89±0.3548.33±0.3358.43±0.3566.82±0.3573.10±0.2955.40±1.1563.03±0.8668.70±0.56\nARBERTv2† 74.39±0.2247.62±0.3058.07±0.2966.87±0.2674.20±0.2853.80±0.5962.37±0.4968.96±0.39\nMARBERTv2 73.53±0.2448.21±0.3958.24±0.3666.54±0.3072.90±0.2154.90±0.5262.63±0.4268.41±0.31\nMARBERTv2†74.21±0.1646.45±0.2557.14±0.2466.29±0.2074.00±0.1752.70±0.3461.56±0.2968.46±0.23\nTable 5: Performance of the seq2edit approach com-\npared to baselines on the QALB-2014 and QALB-2015\nTest sets. †: Models trained on 3-stage training.\nsuch as the ($MERGE) tag to merge the current to-\nken and the next token into a single one. Edit space\nafter applying token-level transformations results\nin KEEP (725K op), $REPLACE_t2 (201K op),\n$APPEND_t1 (75K op), $DELETE (13K op),\nand $MERGE (5.7K op) tags.\nPreprocessing and fine-tuning. We start the pre-\nprocessing stage by aligning source tokens with tar-\nget subsequences, preparing them for token-level\ntransformations. We then fine-tune ARBERT v2\n(Elmadany et al., 2022) and MARBERTv2 (Abdul-\nMageed et al., 2021) on the preprocessed data. We\nadhere to the training approach detailed in the orig-\ninal paper (Omelianchuk et al., 2020), adopting its\nthree-stage training and setting the iterative correc-\ntion to three. More details about the fine-tuning\nprocedure can be found in Appendix C.\nSequence tagging evaluation. As shown in Ta-\nble 5, ARBERTv2 and MARBERTv2, exhibit high\nprecision (e.g., ARBERT v2’s three-step training\nis at 74.39 precision). However, relatively lower\nrecall scores indicate challenges in ability of the\ntwo models to detect errors. Unlike the findings in\nthe original paper, our implementation of a three-\nstage training approach yields mixed results: while\naccuracy improves, recall scores decrease, lead-\ning to a drop in the overall F 1 score (by 0.36 for\nARBERTv2 and 1.10 for MARBERTv2, respec-\ntively). Consequently, all models fall behind the\n’seq2seq’ models. We note that both ARBERT v2\nand MARBERTv2 surpass mT0 and mT5 in terms\nof F0.5 scores, highlighting their abilities in correct-\ning errors with precision.\n7 Error Analysis\n7.1 Error type evaluation.\nWe use the Automatic Error Type Annotation\n(ARETA) tool (Belkebir and Habash, 2021) to\nassess our models’ performance on different er-\nror types. We focus on seven errors types: Or-\nthographic, Morphological, Syntactic, Semantic,\nError Type Incorrect Sentence Correct Sentence\nOrthographic /char2e /char80/char51/char09/charae/charcb/char40 /char48/char2e/char51/char4b/char0a/charc9/char67/char2e/char51/charcb/char40 /char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb /char51/char4b/char0a/charc9/char67/char2e/char51/charcb/char40\nThe man rears the horse. The man rides the horse.\nPunctuations /char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb /char51/char4b/char0a/char2c /charc9/char67/char2e/char51/charcb/char40 /char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb /char51/char4b/char0a/charc9/char67/char2e/char51/charcb/char40\nThe man, rides the horse. The man rides the horse.\nSyntax /char2e /char80/char51/char09/charaf /char49/char2e/charbb /char51/char4b/char0a/char43/char67/char2e/char50 /char59/char67/char2e/charf0 /char2e /char41/char83/char51/char09/charaf /char49/char2e/charbb /char51/char4b/char0a/char43/char67/char2e/char50 /char59/char67/char2e/charf0\nHe found a man riding a hors. He found a man riding a horse.\nMerge /char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb/char51/char1e/char0a/char83 /charc9/char67/char2e/char51/charcb/char40/char40/char59/char09/charab /char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb/char51/char1e/char0a/char83 /charc9/char67/char2e/char51/charcb/char40 /char40/char59/char09/charab\nTomorrowtheman will ride the horse. Tomorrow the man will ride the horse.\nSplits /char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb /char51/char4b/char0a/charc9/char67/char2e/char51/charcb/char40 /char40/char59/char09/charab /char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb /char51/char4b/char0a/charc9/char67/char2e/char51/charcb/char40 /char40/char59/char09/charab\nThe man ri des the horse. The man rides the horse.\nSemantic /char2e /char80/char51/char09/charae/charcb/char40 /char51/charea/char09/chara3 /charfa/char0a/char09/charaf /char81/charca/char6d/char2e/char1a/char27/char0a/charc9/char67/char2e/char51/charcb/char40 /char2e /char80/char51/char09/charae/charcb/char40 /char51/charea/char09/chara3/charfa/charce/charab /char81/charca/char6d/char2e/char1a/char27/char0a/charc9/char67/char2e/char51/charcb/char40\nThe man is sitting in the horse’s back. The man is sitting on the horse’s back.\nMorphological/char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb /char50 /charc9/char67/char2e/char51/charcb/char40 /char40/char59/char09/charab /char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb/char51/char1e/char0a/char83 /charc9/char67/char2e/char51/charcb/char40 /char40/char59/char09/charab\nTomorrow the man rode the horse. Tomorrow the man will ride the horse.\nTable 6: Examples of Merge, Morphological, Ortho-\ngraphic, Punctuation, Semantic, Split, and Syntactic\nerrors, along with their corresponding corrections and\nEnglish translations.\nPunctuation, Merge, and Split. Examples of each\nerror types alongside their translations can be found\nin Table 6. We examine top models from each\napproach, including ARBERTv2 (3-step), GPT-4\n(5-shot) + CoT, and AraT5v2(11M). Figure 5 illus-\ntrates the performance of selected models under\neach error type. AraT5v2(11M), surpasses all other\nmodels across all error categories. In particular, it\nexcels in handling Orthographic (ORTH) errors,\nMorphological (MORPH) errors, and Punctuation\n(PUNCT) errors, consistently achieving over 65 F1\nscore. However, it is worth observing that all mod-\nels encounter challenges with Semantic (SEM) and\nSyntactic (SYN) errors. These disparate outcomes\nunderscore the significance of selecting the appro-\npriate model based on the error types prevalent in\na specific dataset.\n7.2 Normalization methods.\nIn addition to the ‘Exact Match’ score, we also\nanalyze system performance under different nor-\nmalization methods. Namely, we assess the system\non normalized text (1) without Alif/Ya errors, (2)\nwithout punctuation, and (3) free from both Alif/Ya\nand punctuation errors. Examples of text under\neach setting can be found in Appendix D.1.\n7.3 Normalisation results\nLooking at Table 7, in the ‘No punctuation’ set-\nting, all models perform better, reflecting models’\nlimitations in handling punctuation which is due to\nabsence of clearly agreed upon punctuation rules\nin Arabic. Moreover, the datasets used are based\non commentaries where punctuation is inherently\ninconsistent and varied. Another noteworthy obser-\n107\nFigure 5: Best model F1 scores for each approach on specific error types in the QALB-2014 Test set.\nTest Set Models Exact Match No Alif / Ya Errors No Punctuation No Punctuation and Alif / Ya Errors\nP R F 1.0 F0.5 P R F 1.0 F0.5 P R F 1.0 F0.5 P R F 1.0 F0.5\nQALB-2014\nSolyman et al. (2021)79.06 65.79 71 .82 75.99 - - - - - - - - - - - -\nMohit et al. (2014)73.34 63 .23 67 .91 71 .07 64.05 50.86 56 .7 60.89 76.99 49 .91 60 .56 69 .45 76.99 49 .91 60 .56 69 .45\nGPT4 (5-shot) 69.46 61 .96 65 .49 67 .82 58.44 51 .47 54 .73 56 .90 74.59 78 .15 76 .33 75 .28 60.06 65 .75 62 .78 61 .12\nARBERTv2(3-step) 74.17±0.22 47.34±0.30 57.79±0.29 66.62±0.26 64.90±0.57 41.86±0.24 50.89±0.17 58.46±0.33 76.90±0.85s 46.33±0.58 57.83±0.66 67.94±0.75 56.66±0.57 29.30±0.61 38.62±0.39 47.74±0.03\nAraT5v2(11m) 76.94±0.67 69.26±0.73 72.90±0.68 75.27±0.67 62.42±0.68 52.56±0.51 57.06±0.08 60.16±0.38 86.52±0.50 82.90±0.17 84.67±0.25 85.77±0.39 79.44±0.51 67.40±0.53 72.92±0.52 76.70±0.52\nQALB-2015\nSolyman et al. (2021)80.23 63.59 70 .91 76.24 - - - - - - - - - - - -\nRozovskaya et al. (2015a)88.85 61 .76 72 .87 81 .68 84.25 43.29 57.19 70.84 85.8 77 .98 81 .7 84 .11 80.12 58.24 67.45 74.52\nChatGPT (3-shot) + EP52.33 47 .57 49 .83 54 .10 37.93 39 .97 38 .92 32 .95 53.38 56 .63 54 .96 54 .00 33.33 46 .77 38 .92 35 .36\nARBERTv2(3-step) 73.92±0.28 53.15±0.59 61.84±0.49 68.56±0.39 57.14±0.21 39.17±0.76 46.47±0.47 52.34±0.13 66.90±0.17 61.50±0.50 64.09±0.28 65.74±0.18 71.18±0.16 39.00±0.87 50.39±0.75 61.09±0.49\nAraT5v2(11m) 72.10±0.31 73.59±0.70 72.84±0.40 72.40±0.30 55.80±0.30 43.51±0.50 48.89±0.22 52.81±0.11 85.82±0.31 72.85±0.25 78.81±0.28 82.87±0.30 75.08±0.13 53.30±0.93 62.34±0.60 69.40±0.26\nTable 7: Results on QALB-2014, QALB-2015 Test sets under Normalization Methods.\nvation is the drop in F1 scores when Alif/Ya errors\nare removed. This can be attributed to the fact\nthat Alif/Ya errors are relatively simpler compared\nto other error categories. Moreover, AraT5 v2 is\ntrained on formal texts such as AraNews (Nagoudi\net al., 2020) and Hindawi Books 2, which contain\nproper Alif/Ya indicating the model’s proficiency\nwith the correct usage of these letters.\n8 Discussion\nLLMs and ChatGPT. ChatGPT demonstrates re-\nmarkable ability to outperform other fully trained\nmodels by learning from only a few examples, par-\nticularly five-shot under both few-shot CoT and EP\nprompting strategies. Nevertheless, ChatGPT’s per-\nformance lags behind AraT5v2 and AraBART, sug-\ngesting potential areas for improvements in prompt-\ning strategies to fully exploit ChatGPT models.\nModels such as Vicuna-13B as well as those trained\non multilingual datasets like Bactrian-X llama-7B\nand Bactrian-X bloom-7B, tend to perform better.\nHowever, these models fail to match ChatGPT’s\nperformance which reinforces ChatGPT’s superior-\nity in this domain.\nSeq2seq approach. Despite being smaller in size,\npretrained Language Models (PLMs) often outper-\nform LLMs, especially models specifically trained\nfor Arabic tasks, such as AraT5v2 and AraBART.\n2www.hindawi.org/books\nIn contrast, mT0 and mT5, both of which are mul-\ntilingual models, are surpassed by ChatGPT when\nusing both prompting strategies from 3-shot, but\nstill outperform smaller LLMs such as LLaMA,\nAlpaca and Vicuna. Moreover, the results under-\nscore the advantages of synthetic data for PLMs, as\nevidenced by the consistent improvement in scores\nwith additional data.\nSeq2edit approach. These models exhibit high\nprecision scores and relatively low recall scores,\nsuggesting their strengths in making corrections\nrather than detecting errors. This trend can be ex-\nplained by the absence of g-transformations. For\ninstance, in the case of English GECToR mod-\nels, g-transformations enable a variety of changes,\nsuch as case alterations and grammatical transfor-\nmations. However, in this work we only rely on\nthe ’merge’ g-transformations from the GECToR\nmodel as crafting effective g-transformations for\nArabic, a language with rich morphological fea-\ntures, poses significant challenges, limiting the\nmodel’s ability to effectively detect errors. Devel-\noping specific g-transformations for Arabic could\nsignificantly improve performance in these models.\nData augmentation. Data augmentation results\nunderscore the potential of synthetic data, gener-\nated by ChatGPT, in enhancing model performance.\nOur findings reveal that not just the quantity, but\nthe quality of synthetic data, is crucial for achiev-\ning optimal performance. The relative underperfor-\n108\nTest Set Models Exact Match\nP R F 1.0 F0.5\nQALB-2014\nSolyman et al. (2021) 79.06 65.79 71 .82 75.99\nMohit et al. (2014) 73.34 63 .23 67 .91 71 .07\nGPT4 (5-shot) 69.46 61 .96 65 .49 67 .82\nARBERTv2(3-step) 74.17±0.22 47.34±0.30 57.79±0.29 66.62±0.26\nAraT5v2(11m) 76.94±0.67 69.26±0.73 72.90±0.68 75.27±0.67\nQALB-2015\nSolyman et al. (2021) 80.23 63.59 70 .91 76.24\nRozovskaya et al. (2015a)88.85 61 .76 72 .87 81 .68\nChatGPT (3-shot) + EP 52.33 47 .57 49 .83 54 .10\nARBERTv2(3-step) 73.92±0.28 53.15±0.59 61.84±0.49 68.56±0.39\nAraT5v2(11m) 72.10±0.31 73.59±0.70 72.84±0.40 72.40±0.30\nTable 8: Results on QALB-2014, QALB-2015 Test sets\ncompared to recent works.\nmance of models further trained with synthetically\ngenerated data examples emphasizes this conclu-\nsion. Improvements we observe when expanding\nthe dataset from 5M to 10M and from 10M to 11M\nare similar, even though the quantity of additional\ndata vary. This can be attributed to the quality\nof the sources as the data for 5M and 10M were\nderived from noisier online commentaries, while\nthe 11M data was derived from the OSIAN cor-\npus (Zeroual et al., 2019). Furthermore, our results\non decoding methods on scaled datasets indicate\nthat the chosen method can significantly influence\nprecision and recall, emphasizing the need to select\nthe right method depending on the specific task at\nhand.\nBest model in comparison. Although our main ob-\njective is not to develop the best model for AGEC,\nour AraT5v2 (11M) model as detailed in Table 8\nexcels in comparison to previous SOTA (71.82 vs.\n72.90). It is worth noting that contemporaneous\nwork by Alhafni et al. (2023) introduces a new\nalignment algorithm that is much better than that\nemployed by the shared task evaluation code we\nuse. They also present an AGEC model. In per-\nsonal communication with the authors, they con-\nfirmed their alignment algorithm through which we\ncan perform direct and fair comparisons, and the\ndata split on ZAEBUC dataset (Habash and Pal-\nfreyman, 2022) will be released once their work\nis published through peer-review. Different from\ntheir work, our models are also dependency-free.\nFor example, we do not exploit any morphological\nanalyzers.\n9 Conclusion\nThis paper provided a detailed exploration of the\npotential of LLMs, with a particular emphasis on\nChatGPT for AGEC. Our study highlights Chat-\nGPT’s promising capabilities, in low-resource sce-\nnarios, as evidenced by its competitive perfor-\nmance on few-shot setttings. However, AraT5 v2\nand AraBART still exhibit superior results across\nvarious settings and error types. Our findings also\nemphasize the role of high-quality synthetic data,\nreinforcing that both quantity and quality matter\nin achieving optimal performance. Moreover, our\nwork unveils trade-offs between precision and re-\ncall in relation to dataset size and throughout all the\nother experimental settings. These insight, again,\ncould inform future strategies for improving GEC\nsystems. Although our exploration of ChatGPT’s\nperformance on AGEC tasks showcases encourag-\ning results, it also uncovers areas ripe for further\nstudy. Notably, there remains significant room for\nimprovement in GEC systems, particularly within\nthe context of low-resource languages. Future re-\nsearch may include refining prompting strategies,\nenhancing synthetic data generation techniques,\nand addressing the complexities and rich morpho-\nlogical features inherent in the Arabic language.\n10 Limitations\nWe identify the following limitations in this work:\n1. This work is primarily focused on MSA and\ndoes not delve into dialectal Arabic (DA) or\nthe classical variety of Arabic (CA). While\nthere exist DA resources such as the MADAR\ncorpus (Bouamor et al., 2018), their primary\napplication is for dialect identification (DID)\nand machine translation (MT), making them\nunsuitable for our specific AGEC objectives.\nA more comprehensive coverage could be\nachieved with the development and introduc-\ntion of datasets specifically tailored for the\ndialects in question.\n2. This work aimed to examine the potential of\nLLMs, with an emphasis on ChatGPT, by\ncomparing them to fully pretrained models.\nHowever, uncertainty surrounding the extent\nof Arabic data on which ChatGPT has been\ntrained, poses challenges for direct compar-\nisons with other pretrained models. Addi-\ntionally, LLMs are primarily fine-tuned for\nEnglish-language data. While prior studies\nhave demonstrated their effectiveness in other\nlanguages, the limited amount of pretraining\ndata for non-English languages complicates a\nstraightforward comparison.\n3. The scope of this work is primarily centered\non sentence-level GEC. This limitation arose\ndue to the official ChatGPT API, at the time\n109\nof our study, allowed a maximum of 4,097 to-\nkens, making it unsuitable for longer texts and\nprecluding document-level GEC tasks. How-\never, it’s worth noting that document-level cor-\nrection, offers a broader context that’s vital for\naddressing certain grammatical inconsisten-\ncies and errors (Yuan and Bryant, 2021). With\nthe recent introduction of a newer API that ac-\ncommodates extended texts, future endeavors\ncan potentially address document-level GEC,\nutilizing datasets such as QALB-2015 L2 and\nthe newly introduced ZAEBUC corpus.\n11 Ethics Statement and Broad Impact\nEncouraging research development and con-\ntributing to a collaborative research culture.\nProgress in AGEC has been stagnant for a long\ntime due to the lack of benchmark datasets. This\ncan be attributed to the extensive time and cost\ninvolved in creating these datasets. As a result, ad-\nvancing AGEC has proven challenging. With the\nrecent development of LLMs and their capabilities,\nthere is potential for these models to expedite the\ncreation of datasets. By doing so, they can sig-\nnificantly reduce both time and cost, as has been\nobserved in other languages. We hope our work\nwill inspire further exploration into the capabilities\nof LLMs for AGEC, thus aiding in the progress of\nthis field.\nAdvancing Second Language Learning through\nLLMs. With increasing interest in second language\nlearning, ensuring accuracy and effectiveness of\nwritten language has become significant for peda-\ngogical tools. Nowadays, individuals treat LLMs\nas their own writing assistants. Therefore, LLMs\nin the context of educational applications and more\nspecifically GEC is becoming increasingly impor-\ntant. As such, introducing works in the develop-\nment of tools that aid assistance in writing can help\nbridge the gap between non-native speakers and\nfluent written communication, enhancing the effi-\ncacy of educational tools. Especially with Arabic,\nbeing a collection of a diverse array of languages\nand dialectal varieties, we hope this will inspire\nmore work to ensure comprehensive coverage and\nimproved support for all learners. However, it is\ncrucial to emphasize the ethical implications of us-\ning AI-driven educational tools. It’s essential that\nthese tools remain unbiased, transparent, and con-\nsiderate of individual learning differences, ensur-\ning the trustworthiness and integrity of educational\nplatforms for every learner.\nData privacy. In relation to the data used in this\nwork, all datasets are publicly available. Therefore,\nwe do not have privacy concerns.\nAcknowledgments\nWe acknowledge support from Canada Research\nChairs (CRC), the Natural Sciences and Engineer-\ning Research Council of Canada (NSERC; RGPIN-\n2018-04267), the Social Sciences and Humanities\nResearch Council of Canada (SSHRC; 435-2018-\n0576; 895-2020-1004; 895-2021-1008), Canadian\nFoundation for Innovation (CFI; 37771), Digital\nResearch Alliance of Canada,3 and UBC Advanced\nResearch Computing-Sockeye.4\nReferences\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021. ARBERT &\nMARBERT: Deep bidirectional transformers for Ara-\nbic. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n7088–7105, Online. Association for Computational\nLinguistics.\nMuhammad Abdul-Mageed, Chiyu Zhang, Houda\nBouamor, and Nizar Habash. 2020. NADI 2020:\nThe first nuanced Arabic dialect identification shared\ntask. In Proceedings of the Fifth Arabic Natu-\nral Language Processing Workshop, pages 97–110,\nBarcelona, Spain (Online). Association for Computa-\ntional Linguistics.\nAbdullah Alfaifi and Eric Atwell. 2012. Arabic learner\ncorpora (alc): A taxonomy of coding errors. In The\n8th International Computing Conference in Arabic.\nBashar Alhafni, Go Inoue, Christian Khairallah, and\nNizar Habash. 2023. Advancements in arabic gram-\nmatical error detection and correction: An empirical\ninvestigation. arXiv preprint arXiv:2305.14734.\nAbhijeet Awasthi, Sunita Sarawagi, Rasna Goyal,\nSabyasachi Ghosh, and Vihari Piratla. 2019. Par-\nallel iterative edit models for local sequence trans-\nduction. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n4260–4270, Hong Kong, China. Association for Com-\nputational Linguistics.\nRiadh Belkebir and Nizar Habash. 2021. Automatic\nerror type annotation for arabic. arXiv preprint\narXiv:2109.08068.\n3https://alliancecan.ca\n4https://arc.ubc.ca/ubc-arc-sockeye\n110\nHouda Bouamor, Nizar Habash, Mohammad Salameh,\nWajdi Zaghouani, Owen Rambow, Dana Abdul-\nrahim, Ossama Obeid, Salam Khalifa, Fadhl Eryani,\nAlexander Erdmann, and Kemal Oflazer. 2018. The\nMADAR Arabic dialect corpus and lexicon. In Pro-\nceedings of the Eleventh International Conference on\nLanguage Resources and Evaluation (LREC 2018),\nMiyazaki, Japan. European Language Resources As-\nsociation (ELRA).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nChristopher Bryant, Mariano Felice, Øistein E. Ander-\nsen, and Ted Briscoe. 2019. The BEA-2019 shared\ntask on grammatical error correction. In Proceedings\nof the Fourteenth Workshop on Innovative Use of NLP\nfor Building Educational Applications, pages 52–75,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nChristopher Bryant, Zheng Yuan, Muhammad Reza\nQorib, Hannan Cao, Hwee Tou Ng, and Ted Briscoe.\n2022. Grammatical error correction: A survey of the\nstate of the art. arXiv preprint arXiv:2211.05166.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nShamil Chollampatt and Hwee Tou Ng. 2018. A multi-\nlayer convolutional encoder-decoder neural network\nfor grammatical error correction. Proceedings of the\nAAAI Conference on Artificial Intelligence, 32(1).\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nMarta R Costa-jussà, James Cross, Onur Çelebi, Maha\nElbayad, Kenneth Heafield, Kevin Heffernan, Elahe\nKalbassi, Janice Lam, Daniel Licht, Jean Maillard,\net al. 2022. No language left behind: Scaling\nhuman-centered machine translation. arXiv preprint\narXiv:2207.04672.\nDaniel Dahlmeier and Hwee Tou Ng. 2012. Better\nevaluation for grammatical error correction. In Pro-\nceedings of the 2012 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n568–572, Montréal, Canada. Association for Compu-\ntational Linguistics.\nMoussa Kamal Eddine, Nadi Tomeh, Nizar Habash,\nJoseph Le Roux, and Michalis Vazirgiannis. 2022.\nArabart: a pretrained arabic sequence-to-sequence\nmodel for abstractive summarization. arXiv preprint\narXiv:2203.10945.\nAbdelRahim Elmadany, El Moatez Billah Nagoudi, and\nMuhammad Abdul-Mageed. 2022. Orca: A challeng-\ning benchmark for arabic language understanding.\narXiv preprint arXiv:2212.10758.\nTao Fang, Shu Yang, Kaixin Lan, Derek F Wong, Jin-\npeng Hu, Lidia S Chao, and Yue Zhang. 2023. Is\nchatgpt a highly fluent grammatical error correction\nsystem? a comprehensive evaluation. arXiv preprint\narXiv:2304.01746.\nMariano Felice, Zheng Yuan, Øistein E. Andersen, He-\nlen Yannakoudakis, and Ekaterina Kochmar. 2014.\nGrammatical error correction using hybrid systems\nand type filtering. In Proceedings of the Eighteenth\nConference on Computational Natural Language\nLearning: Shared Task , pages 15–24, Baltimore,\nMaryland. Association for Computational Linguis-\ntics.\nSteven Y . Feng, Varun Gangal, Jason Wei, Sarath Chan-\ndar, Soroush V osoughi, Teruko Mitamura, and Ed-\nuard Hovy. 2021. A survey of data augmentation\napproaches for NLP. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 968–988, Online. Association for Computa-\ntional Linguistics.\nSimon Flachs, Felix Stahlberg, and Shankar Kumar.\n2021. Data strategies for low-resource grammat-\nical error correction. In Proceedings of the 16th\nWorkshop on Innovative Use of NLP for Building\nEducational Applications, pages 117–122, Online.\nAssociation for Computational Linguistics.\nMarkus Freitag and Yaser Al-Onaizan. 2017. Beam\nsearch strategies for neural machine translation.\narXiv preprint arXiv:1702.01806.\nUlrich Germann. 2003. Greedy decoding for statistical\nmachine translation in almost linear time. In Pro-\nceedings of the 2003 Human Language Technology\nConference of the North American Chapter of the\nAssociation for Computational Linguistics, pages 72–\n79.\nPeiyuan Gong, Xuebo Liu, Heyan Huang, and Min\nZhang. 2022. Revisiting grammatical error correc-\ntion evaluation and beyond. In Proceedings of the\n2022 Conference on Empirical Methods in Natu-\nral Language Processing , pages 6891–6902, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nRoman Grundkiewicz, Marcin Junczys-Dowmunt, and\nKenneth Heafield. 2019. Neural grammatical error\ncorrection systems with unsupervised pre-training\non synthetic data. In Proceedings of the Fourteenth\nWorkshop on Innovative Use of NLP for Building\nEducational Applications, pages 252–263, Florence,\nItaly. Association for Computational Linguistics.\n111\nNizar Habash and David Palfreyman. 2022. ZAEBUC:\nAn annotated Arabic-English bilingual writer corpus.\nIn Proceedings of the Thirteenth Language Resources\nand Evaluation Conference, pages 79–88, Marseille,\nFrance. European Language Resources Association.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2019. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751.\nSerena Jeblee, Houda Bouamor, Wajdi Zaghouani, and\nKemal Oflazer. 2014. CMUQ@QALB-2014: An\nSMT-based system for automatic Arabic error correc-\ntion. In Proceedings of the EMNLP 2014 Workshop\non Arabic Natural Language Processing (ANLP) ,\npages 137–142, Doha, Qatar. Association for Com-\nputational Linguistics.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nShubha Guha, and Kenneth Heafield. 2018a. Ap-\nproaching neural grammatical error correction as a\nlow-resource machine translation task. In Proceed-\nings of the 2018 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Vol-\nume 1 (Long Papers), pages 595–606, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nMarcin Junczys-Dowmunt, Roman Grundkiewicz,\nShubha Guha, and Kenneth Heafield. 2018b. Ap-\nproaching neural grammatical error correction as\na low-resource machine translation task. arXiv\npreprint arXiv:1804.05940.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. arXiv preprint\narXiv:2205.11916.\nHaonan Li, Fajri Koto, Minghao Wu, Alham Fikri Aji,\nand Timothy Baldwin. 2023. Bactrian-x: A multilin-\ngual replicable instruction-following model. https:\n//github.com/MBZUAI-nlp/Bactrian-X.\nEric Malmi, Sebastian Krause, Sascha Rothe, Daniil\nMirylenka, and Aliaksei Severyn. 2019. Encode, tag,\nrealize: High-precision text editing. arXiv preprint\narXiv:1909.01187.\nBehrang Mohit, Alla Rozovskaya, Nizar Habash, Wajdi\nZaghouani, and Ossama Obeid. 2014. The first qalb\nshared task on automatic text correction for arabic.\nIn Proceedings of the EMNLP 2014 Workshop on\nArabic Natural Language Processing (ANLP), pages\n39–47.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev, Al-\nham Fikri Aji, Khalid Almubarak, Samuel Albanie,\nZaid Alyafeai, Albert Webson, Edward Raff, and\nColin Raffel. 2022. Crosslingual generalization\nthrough multitask finetuning.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany, and\nMuhammad Abdul-Mageed. 2022. AraT5: Text-to-\ntext transformers for Arabic language generation. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 628–647, Dublin, Ireland. Asso-\nciation for Computational Linguistics.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany,\nMuhammad Abdul-Mageed, Tariq Alhindi, and\nHasan Cavusoglu. 2020. Machine generation and\ndetection of arabic manipulated and fake news. arXiv\npreprint arXiv:2011.03092.\nHwee Tou Ng, Siew Mei Wu, Ted Briscoe, Christian\nHadiwinoto, Raymond Hendy Susanto, and Christo-\npher Bryant. 2014. The CoNLL-2014 shared task\non grammatical error correction. In Proceedings of\nthe Eighteenth Conference on Computational Natu-\nral Language Learning: Shared Task , pages 1–14,\nBaltimore, Maryland. Association for Computational\nLinguistics.\nHwee Tou Ng, Siew Mei Wu, Yuanbin Wu, Christian\nHadiwinoto, and Joel Tetreault. 2013. The CoNLL-\n2013 shared task on grammatical error correction.\nIn Proceedings of the Seventeenth Conference on\nComputational Natural Language Learning: Shared\nTask, pages 1–12, Sofia, Bulgaria. Association for\nComputational Linguistics.\nKostiantyn Omelianchuk, Vitaliy Atrasevych, Artem\nChernodub, and Oleksandr Skurzhanskyi. 2020.\nGector–grammatical error correction: tag, not rewrite.\narXiv preprint arXiv:2005.12592.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. The Journal of Machine Learning Research,\n21(1):5485–5551.\nSascha Rothe, Jonathan Mallinson, Eric Malmi, Sebas-\ntian Krause, and Aliaksei Severyn. 2021. A simple\nrecipe for multilingual grammatical error correction.\nIn Proceedings of the 59th Annual Meeting of the As-\nsociation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 2: Short Papers), pages 702–707,\nOnline. Association for Computational Linguistics.\nAlla Rozovskaya, Houda Bouamor, Nizar Habash, Wa-\njdi Zaghouani, Ossama Obeid, and Behrang Mohit.\n2015a. The second qalb shared task on automatic text\ncorrection for arabic. In Proceedings of the Second\nworkshop on Arabic natural language processing ,\npages 26–35.\n112\nAlla Rozovskaya, Houda Bouamor, Nizar Habash, Wa-\njdi Zaghouani, Ossama Obeid, and Behrang Mohit.\n2015b. The second QALB shared task on automatic\ntext correction for Arabic. In Proceedings of the\nSecond Workshop on Arabic Natural Language Pro-\ncessing, pages 26–35, Beijing, China. Association\nfor Computational Linguistics.\nAlla Rozovskaya, Nizar Habash, Ramy Eskander, Noura\nFarra, and Wael Salloum. 2014. The Columbia sys-\ntem in the QALB-2014 shared task on Arabic er-\nror correction. In Proceedings of the EMNLP 2014\nWorkshop on Arabic Natural Language Processing\n(ANLP), pages 160–164, Doha, Qatar. Association\nfor Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun\nRaja, et al. 2021. Multitask prompted training en-\nables zero-shot task generalization. arXiv preprint\narXiv:2110.08207.\nAiman Solyman, Zhenyu Wang, Qian Tao, Arafat\nAbdulgader Mohammed Elhag, Rui Zhang, and\nZeinab Mahmoud. 2022. Automatic arabic gram-\nmatical error correction based on expectation-\nmaximization routing and target-bidirectional agree-\nment. Knowledge-Based Systems, 241:108180.\nAiman Solyman, Marco Zappatore, Wang Zhenyu,\nZeinab Mahmoud, Ali Alfatemi, Ashraf Osman\nIbrahim, and Lubna Abdelkareim Gabralla. 2023.\nOptimizing the impact of data augmentation for low-\nresource grammatical error correction. Journal of\nKing Saud University - Computer and Information\nSciences, 35(6):101572.\nAiman Solyman, Wang Zhenyu, Tao Qian, Arafat Ab-\ndulgader Mohammed Elhag, Muhammad Toseef, and\nZeinab Aleibeid. 2021. Synthetic data with neural\nmachine translation for automatic correction in arabic\ngrammar. Egyptian Informatics Journal, 22(3):303–\n315.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy\nLiang, and Tatsunori B. Hashimoto. 2023. Stan-\nford alpaca: An instruction-following llama\nmodel. https://github.com/tatsu-lab/\nstanford_alpaca.\nMaksym Tarnavskyi, Artem Chernodub, and Kostiantyn\nOmelianchuk. 2022. Ensembling and knowledge dis-\ntilling of large sequence taggers for grammatical error\ncorrection. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 3842–3852, Dublin,\nIreland. Association for Computational Linguistics.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nDaniel Watson, Nasser Zalmout, and Nizar Habash.\n2018. Utilizing character and word embeddings for\ntext normalization with sequence-to-sequence mod-\nels. arXiv preprint arXiv:1809.01534.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M.\nDai, and Quoc V Le. 2022a. Finetuned language\nmodels are zero-shot learners. In International Con-\nference on Learning Representations.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nHaoran Wu, Wenxuan Wang, Yuxuan Wan, Wenxiang\nJiao, and Michael Lyu. 2023. Chatgpt or grammarly?\nevaluating chatgpt on grammatical error correction\nbenchmark. arXiv preprint arXiv:2303.13648.\nZiang Xie, Guillaume Genthial, Stanley Xie, Andrew\nNg, and Dan Jurafsky. 2018. Noising and denois-\ning natural language: Diverse backtranslation for\ngrammar correction. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, Volume 1 (Long Papers) ,\npages 619–628, New Orleans, Louisiana. Associa-\ntion for Computational Linguistics.\nBenfeng Xu, An Yang, Junyang Lin, Quan Wang,\nChang Zhou, Yongdong Zhang, and Zhendong Mao.\n2023. Expertprompting: Instructing large language\nmodels to be distinguished experts. arXiv preprint\narXiv:2305.14688.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2020. mt5: A massively multilingual\npre-trained text-to-text transformer. arXiv preprint\narXiv:2010.11934.\nZheng Yuan and Christopher Bryant. 2021. Document-\nlevel grammatical error correction. In Proceedings\nof the 16th Workshop on Innovative Use of NLP for\nBuilding Educational Applications, pages 75–84, On-\nline. Association for Computational Linguistics.\nWajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-\nsama Obeid, Nadi Tomeh, Alla Rozovskaya, Noura\nFarra, Sarah Alkuhlani, and Kemal Oflazer. 2014.\nLarge scale Arabic error annotation: Guidelines and\nframework. In Proceedings of the Ninth International\nConference on Language Resources and Evaluation\n(LREC’14), Reykjavik, Iceland. European Language\nResources Association (ELRA).\nImad Zeroual, Dirk Goldhahn, Thomas Eckart, and\nAbdelhak Lakhouaja. 2019. OSIAN: Open source\ninternational Arabic news corpus - preparation and\nintegration into the CLARIN-infrastructure. In Pro-\nceedings of the Fourth Arabic Natural Language Pro-\ncessing Workshop, pages 175–182, Florence, Italy.\nAssociation for Computational Linguistics.\n113\nRenrui Zhang, Jiaming Han, Chris Liu, Peng Gao, Ao-\njun Zhou, Xiangfei Hu, Shilin Yan, Pan Lu, Hong-\nsheng Li, and Yu Qiao. 2023. Llama-adapter: Effi-\ncient fine-tuning of language models with zero-init\nattention.\n114\nA Related Works\nSequence to sequence approach. Transformer-\nbased Language Models (LMs) have been integral\nto advancements in GEC. These models have sub-\nstantially transformed the perception of GEC, re-\nframing it as a MT task. In this framework, erro-\nneous sentences are considered as the source lan-\nguage, and the corrected versions as the target lan-\nguage. This perspective, which has led to SOTA\nresults in the CONLL 2013 and 2014 shared tasks\n(Bryant et al., 2022; Ng et al., 2013, 2014), reinter-\nprets GEC as a low-resource or mid-resource MT\ntask. Building on this paradigm, Junczys-Dowmunt\net al. (2018a) successfully adopted techniques from\nlow-resource NMT and Statistical Machine Trans-\nlation (SMT)-based GEC methods, leading to con-\nsiderable improvements on both the CONLL and\nJFLEG datasets.\nSequence tagging approach. Sequence tagging\nmethods, another successful route to GEC, are\nshowcased by models like GECToR (Omelianchuk\net al., 2020), LaserTagger (Malmi et al., 2019), and\nthe Parallel Iterative Edit (PIE) model (Awasthi\net al., 2019). By viewing GEC as a text editing\ntask, these models make edits predictions instead\nof tokens, label sequences rather than generating\nthem, and iteratively refine predictions to tackle de-\npendencies. Employing a limited set of output tags,\nthese models apply edit operations on the input se-\nquence, reconstructing the output. This technique\nnot only capably mirrors a significant chunk of the\ntarget training data, but it also diminishes the vo-\ncabulary size and establishes the output length as\nthe source text’s word count. Consequently, it cur-\ntails the number of training examples necessary for\nmodel accuracy, which is particularly beneficial in\nsettings with sparse human-labeled data (Awasthi\net al., 2019).\nInstruction fine-tuning. LLMs have revolution-\nized NLP, their vast data-learning capability en-\nabling diverse task generalizations. Key to their en-\nhancement has been instructional finetuning, which\nfortifies the model’s directive response and miti-\ngates the need for few-shot examples (Ouyang\net al., 2022; Wei et al., 2022b; Sanh et al., 2021). A\nnovel approach, Chain of Thought (CoT), directs\nLLMs through a series of natural language reason-\ning, generating superior outputs. Proven beneficial\nin ’Let’s think step by step’ prompts (Wei et al.,\n2022b), CoT has harnessed LLMs for multi-task\ncognitive tasks (Kojima et al., 2022) and achieved\nSOTA results in complex system-2 tasks like arith-\nmetic and symbolic reasoning.\nChatGPT. In the specific realm of GEC, LLMs\nhave demonstrated its potential. Fang et al. (2023)\napplied zero-shot and few-shot CoT settings us-\ning in-context learning for ChatGPT (Brown\net al., 2020) and evaluated its performance on three\ndocument-level English GEC test sets. Similarly,\nWu et al. (2023) carried out an empirical study to\nassess the effectiveness of ChatGPT in GEC, in the\nCoNLL2014 benchmark dataset.\nDevelopment in AGEC Arabic consists of a col-\nlection of diverse languages and dialectal varieties\nwith Modern Standard Arabic (MSA) being the cur-\nrent standard variety used in government and pan-\narab media as well as education (Abdul-Mageed\net al., 2020). The inherent ambiguity of Arabic\nat the orthographic, morphological, syntactic, and\nsemantic levels makes AGEC particularly challeng-\ning. Optional use of diacritics further introduces or-\nthographic ambiguity (Belkebir and Habash, 2021),\nmaking AGEC even harder.\nDespite these hurdles, progress has been made\nin AGEC. For dataset development, the QALB cor-\npus (Zaghouani et al., 2014) was utilized. Dur-\ning the QALB-2014 and 2015 shared tasks (Mohit\net al., 2014; Rozovskaya et al., 2015b), the first\nAGEC datasets containing comments and docu-\nments from both native (L1) and Arabic learner\n(L2) speakers were released. Furthermore, the\nmore recent ZAEBUC corpus (Habash and Palfrey-\nman, 2022), which features essays from first-year\nuniversity students at Zayed University in the UAE,\nhas also been released. There has also been work\non generating synthetic data. Solyman et al. (2021,\n2023) apply Convolutional neural network (CNN)\nto generate synthetic parallel data using unsuper-\nvised noise injection techniques showing improve-\nments in the QALB-2014 and 2015 benchmark\ndatasets. In terms of model development, Watson\net al. (2018) developed a character-level seq2seq\nmodel that achieved notable results on AGEC L1\ndata, marking prgoress from basic classifier mod-\nels (Rozovskaya et al., 2014) and statistical ma-\nchine translation models (Jeblee et al., 2014). More\nrecently, Solyman et al. (2022, 2021) introduced\nnovel design that incorporates dynamic linear com-\nbinations and the EM routing algorithm within a\nseq2seq Transformer framework.\n115\nB Instruction Fine-tuning LLMs\nB.1 Instructions for LLMs\nInstruction format used for training is provided in\nTable 9 and instructions used for training are shown\nin Table 10.\nB.2 Baseline and experimental setup for\nLLMs and ChatGPT\nFor LLMs, evaluation was only done on the QALB-\n2014 Test set, for two main reasons. First was due\nto the high cost in producing results using ChatGPT\nand we were able to observation of a similar trend\nin our preliminary experiment with ChatGPT-3.5\nTurbo on the QALB-2015. Second, as instruction\nfine-tuned were predominantly compared against\nChatGPT’s performance, we also evaluate them\nonly on the QALB-2014 Test set. These Results\ncan be found in Table 11.\nC Sequence Tagging Approach\nThe training procedure detailed in the original\nGECToR paper (Omelianchuk et al., 2020) encom-\npasses three stages:\n1. Pre-training on synthetically generated sen-\ntences with errors.\n2. Fine-tuning solely on sentences that contain\nerrors.\n3. Further fine-tuning on a mix of sentences,\nboth with and without errors.\nFor our training process, we pre-train the model on\nthe complete AGEC dataset (Solyman et al., 2021),\nuse the reverseGold dataset for stage 2, and employ\nthe gold training data in the third stage. More-\nover, as some corrections in a sentence depend on\nothers, applying edit sequences once may not be\nenough to correct the sentence fully. To address\nthis issue, GECToR employs an iterative correction\napproach from Awasthi et al. (2019). However,\nin our experiments, we find that the iterative cor-\nrection approach does not result in any tangible\nimprovement. Therefore, we set our iterations to 3.\nD Normalization Methods\nD.1 Normalization examples\nExamples of text under each normalization meth-\nods can be found in Table 12\nD.2 Arabic Learner Corpus error type\ntaxonomy\nThe ALC error type taxonomy can be found in\nTable 13.\nD.3 Hyperparameters\nThe Hyperparameters used for training are shown\nin Table 14.\nD.4 Dev results\nResults on the Dev set are presented in Table 15.\nD.5 ARETA results\nFull results evaluated using ARETA are presented\nin Table 16.\n116\nFine-tune Instruction Example\n/char59/char4b/char0a/charf0/char09/char51/char10/char1e/charcb /charc9/char09/char67/char59/chard6/chardf/char2e/char10/chare9/chara2/char4a/char2e/char10/char4b/char51/chard3/char10/chare9/chard2/charea/chard3/char09/charad/char92/char1d/char0a/chare9/char4a/char0a/char6b/char2e/charf1/char10/char4b /char51/chard3/char0d/char40/charfa/char0a/charce/char4b/char0a/char41/chard2/char4a/char0a/char09/charaf\n/char10/char87/char10/charae/char6a/char10/char4a/charcb/char10/chare9/char4a/char2e/char83/char41/char09/char4a/chard3 /char58/charf0/char58/char50/char10/chare9/char09/charab/char41/char4a/char0a/char93 /charfa/char6b/char2e/char51/char4b/char0a/char2e /charfa/char0a\n/char09/charaf/char41/char09/char93/char40/char10/char86/char41/char4a/char0a/char82/char1d/char2e/char91/char09/char4a/charcb/char40\n/char2e/char10/chare9/char10/charae/char4a/char0a/char10/charaf/char58 /charf0/char10/chare9/char4a/char2e/char83/char41/char09/char4a/chard3/char10/chare9/char10/charae/char4b/char0a/char51/chara2/char1d/char2e/char49/char2e/charca/chara2/charcb/char40\n### /chare9/char4a/char0a/char6b/char2e/charf1/char10/char4a/charcb/char40 /char51/chard3/char0d/char42/char40 /char3a\n/char3a /charfa/char0a/charcd/char41/char10/char4a/charcb/char40 /char91/char09/char4a/charcb/char40 /charfa/char0a\n/char09/charaf/char10/chare9/char4a/char0a/char4b/char2e/char41/char10/char4a/charba/charcb/char40 /char5a/char41/chara2/char09/char6b/char0d/char42/char40 /charc9/charbf /char69/char4a/char0a/char6a/char92/char10/char1c/char4b/char2e/chard5/char10/charaf\n### /charc9/char09/char67/char59/chard6/charcf/char40 /char3a\n/char2e /char80/char51/char09/charae/charcb/char40 /char48/char2e/char51/char4b/char0a/charc9/char67/char2e/char51/charcb/char40\n### /char58/char51/charcb/char40 /char3a\n/char2e /char80/char51/char09/charae/charcb/char40 /char49/char2e/charbb /char51/char4b/char0a/charc9/char67/char2e/char51/charcb/char40\nTable 9: Modified data format for the LLaMA instruction fine-tuning step.\nTranslated in English Instructions Samples\nCorrect all written errors in the following text except for a thousand, ya and punctuation:/char3a /chard5/chare6/char0a/char10/charaf/char51/char10/char1e/charcb/char40/char10/char48/char41/chard3/char43/charab/charf0 /char5a/char41/char4a/char0a/charcb/char40/charf0/char09/charad/charcb/char0d/char42/char41/char4b/char2e/char10/chare9/char10/charae/charca/charaa/char10/char4a/chard6/charcf/char40 /char40/char59/charab/char41/chard3 /charfa/char0a/charcd/char41/char10/char4a/charcb/char40 /char91/char09/char4a/charcb/char40 /charfa/char0a/char09/charaf/char10/chare9/char4a/char0a/char4b/char2e/char41/char10/char4a/charba/charcb/char40 /char5a/char41/chara2/char09/char6b/char0d/char42/char40 /charc9/charbf /char69/char4a/char0a/char6a/char92/char10/char1c/char4b/char2e/chard5/char10/charaf\nPlease verify spelling, grammatical scrutiny, and correct all errors in the following sentence, except for punctuation:/char3a /chard5/chare6/char0a/char10/charaf/char51/char10/char1e/charcb/char40/char10/char48/char41/chard3/char43/charaa/char4b/char2e/char10/chare9/char93/char41/char09/char6d/charcc/char27/char40 /char42/char40/char0d/char10/chare9/char4a/char0a/charcb/char41/char10/char4a/charcb/char40/char10/chare9/charca/chard2/char6d/char2e/charcc/char27/char40 /charfa/char0a/char09/charaf /char5a/char41/chara2/char09/char6b/char0d/char42/char40 /charc9/charbf /char69/char4a/char0a/char6a/char92/char10/char1d /charf0 /charf8/char0a/charf1/char6a/char09/char4a/charcb/char40/char10/char87/char4a/char0a/char10/charaf/char59/char10/char4a/charcb/char40/charf0 /charf9/char0a/char0d/char4b/char43/chard3/char42/char0d/char40/char10/char87/char4a/char0a/char10/charaf/char59/char10/char4a/charcb/char40 /char5a/char41/char67/char2e/char51/charcb/char40\nExplore the grammatical errors and repair them except for punctuation marks such as a comma, or a question marks, etc:/char3a /char74/charcc/char27/char40/char0d/char2c /chard0/char41/charea/char09/charae/char10/char4a/char83/char40/char0d/char10/chare9/chard3/char43/charab /charf0/char0d/char40/char10/chare9/charca/char93/char41/char09/charae/charcb/char41/charbf /chard5/chare6/char0a/char10/charaf/char51/char10/char1e/charcb/char40/char10/char48/char41/chard3/char43/charaa/char4b/char2e/char10/chare9/char10/charae/charca/charaa/char10/char4a/chard6/charcf/char40 /char40/char59/charab/char41/chard3 /char41/charea/char6b/char43/char93/char40/char0d/charf0 /charf9/char0a/char0d/char4b/char43/chard3/char42/char0d/char40/char10/char87/char4a/char0a/char10/charaf/char59/char10/char4a/charcb/char40 /char5a/char41/chara2/char09/char6b/char0d/char40/char09/charac/char41/char11/char82/charba/char10/char4a/char83/char41/char0d/char4b/char2e/chard5/char10/charaf\nCan you correct all errors in the following text except those related to punctuation such as commas, periods, etc:/char3a /char74/charcc/char27/char40/char0d/char2c/char10/chare9/chara2/char10/charae/char09/char4a/charcb/char40 /char2c/char10/chare9/charca/char93/char41/char09/charae/charcb/char41/charbf /chard5/chare6/char0a/char10/charaf/char51/char10/char1e/charcb/char40/char10/char48/char41/chard3/char43/charaa/char4b/char2e/char10/chare9/char10/charae/charca/charaa/char10/char4a/chard6/charcf/char40 /char40/char59/charab/char41/chard3 /charfa/char0a/charcd/char41/char10/char4a/charcb/char40 /char91/char09/char4a/charcb/char40 /charfa/char0a/char09/charaf/char10/chare8/char58/charf1/char6b/char2e/charf1/chard6/charcf/char40 /char5a/char41/chara2/char09/char6b/char0d/char42/char40 /charc9/charbf /charbd/char09/char4a/charba/chard6/chardf /char0a/charc9/chareb\nCan you fix all spelling and grammatical errors, except for the mistakes of the \"Alif\" and \"Ya\":/char3a /char5a/char41/char4a/char0a/charcb/char40/charf0/char09/charad/charcb/char0d/char42/char41/char4b/char2e/char10/chare9/char93/char41/char09/char6d/charcc/char27/char40 /char5a/char41/chara2/char09/char6b/char0d/char42/char40 /char40/char59/charab/char41/chard3/char10/chare9/char4b/char0a/charf1/char6a/char09/char4a/charcb/char40/charf0/char10/chare9/char4a/char0a/char0d/char4b/char43/chard3/char42/char0d/char40 /char5a/char41/chara2/char09/char6b/char0d/char42/char40 /charc9/charbf /char68/char43/char93/char40/char0d/charbd/char09/char4a/charba/chard6/chardf /char0a/charc9/chareb\nPlease explore the grammatical spelling errors and repair them all, except for the mistakes related to the \"Alif\" and \"Ya\"/char3a /char5a/char41/char4a/char0a/charcb/char40/charf0/char09/charad/charcb/char0d/char42/char41/char4b/char2e/char10/chare9/char10/charae/charca/charaa/char10/char4a/chard6/charcf/char40 /char5a/char41/chara2/char09/char6b/char0d/char42/char40 /char40/char59/charab/char41/chard3 /char41/charea/charca/charbf /char41/charea/char6b/char43/char93/char40/char0d/charf0 /charf8/char0a/charf1/char6a/char09/char4a/charcb/char40 /charf9/char0a/char0d/char4b/char43/chard3/char42/char0d/char40/char10/char87/char4a/char0a/char10/charaf/char59/char10/char4a/charcb/char40 /char5a/char41/chara2/char09/char6b/char0d/char40/char09/charac/char41/char11/char82/charba/char10/char4a/char83/char40/char0d/char5a/char41/char67/char2e/char51/charcb/char40\nCorrect all the written errors in the following text except for the \"Alif\" and \"Ya\":/char3a /char5a/char41/char4a/char0a/charcb/char40/charf0/char09/charad/charcb/char0d/char42/char41/char4b/char2e/char10/chare9/char10/charae/charca/charaa/char10/char4a/chard6/charcf/char40 /char40/char59/charab/char41/chard3 /charfa/char0a/charcd/char41/char10/char4a/charcb/char40 /char91/char09/char4a/charcb/char40 /charfa/char0a/char09/charaf/char10/chare9/char4a/char0a/char4b/char2e/char41/char10/char4a/charba/charcb/char40 /char5a/char41/chara2/char09/char6b/char0d/char42/char40 /charc9/charbf /char69/char4a/char0a/char6a/char92/char10/char1c/char4b/char2e/chard5/char10/charaf\nPlease correct all errors in the following sentence: /char3a/char10/chare9/char4a/char0a/charcb/char41/char10/char4a/charcb/char40/char10/chare9/charca/chard2/char6d/char2e/charcc/char27/char40 /charfa/char0a/char09/charaf/char10/chare8/char58/charf1/char6b/char2e/charf1/chard6/charcf/char40 /char5a/char41/chara2/char09/char6b/char0d/char42/char40 /charc9/charbf /char69/char4a/char0a/char6a/char92/char10/char1d /char5a/char41/char67/char2e/char51/charcb/char40\nTable 10: Different instructions used for instruction fine-tuning.\nSettings Models Exact Match\nP R F 1.0 F0.5\n+ CoT ChatGPT (3-shot)49.89 46.72 48.22 49.49\nChatGPT (5-shot)52.33 47.57 49.83 51.15\nTable 11: Performance of ChatGPT-3.5 on QALB-2015 Test set.\nNormalisation Method Example\nNormal /char2e /charfa/char0a/chard7/char43/char83/char42/char0d/char40 /char58/char41/char92/char10/char4a/char10/charaf/char42/char40/char09/chare1/chard3 /char41/charee/char09/char44/chard3 /char68/char2e/char51/char09/char6a/chard6/charcf/char40/char09/chare1/char6d/char1a/char09/char27/charf0/char10/chare9/charcb/char41/char6d/charcc/char27/char40 /chare8/char09/char59/chareb /char80/char50/char59/char09/char4b/char09/chare0/char0d/char40 /char49/char2e/char6d/char2e/char1a/char27/char0a/char09/chare1/charba/charcb/charf0 /char2c/char10/chare9/char10/char4b/char41/chard2/char11/char82/charcb/char40 /char42/char40/char0d/char09/charac/char51/charaa/char09/char4b /char48/char2e/char51/charaa/charcb/char40 /char51/chare5/char11/char84/charaa/chard3/char09/chare1/char6d/char1a/char09/char27\nNo Alif/Ya /char2e /charfa/char0a/chard7/char43/char83/char42/char40 /char58/char41/char92/char10/char4a/char10/charaf/char42/char40/char09/chare1/chard3 /char41/charee/char09/char44/chard3 /char68/char2e/char51/char09/char6a/chard6/charcf/char40/char09/chare1/char6d/char1a/char09/char27/charf0/char10/chare9/charcb/char41/char6d/charcc/char27/char40 /chare8/char09/char59/chareb /char80/char50/char59/char09/char4b/char09/chare0/char40 /char49/char2e/char6d/char2e/char1a/char27/char0a/char09/chare1/charba/charcb/charf0 /char2c/char10/chare9/char10/char4b/char41/chard2/char11/char82/charcb/char40 /char42/char40/char09/charac/char51/charaa/char09/char4b /char48/char2e/char51/charaa/charcb/char40 /char51/chare5/char11/char84/charaa/chard3/char09/chare1/char6d/char1a/char09/char27\nNo Punct /charfa/char0a/chard7/char43/char83/char42/char0d/char40 /char58/char41/char92/char10/char4a/char10/charaf/char42/char40/char09/chare1/chard3 /char41/charee/char09/char44/chard3 /char68/char2e/char51/char09/char6a/chard6/charcf/char40/char09/chare1/char6d/char1a/char09/char27/charf0/char10/chare9/charcb/char41/char6d/charcc/char27/char40 /chare8/char09/char59/chareb /char80/char50/char59/char09/char4b/char09/chare0/char0d/char40 /char49/char2e/char6d/char2e/char1a/char27/char0a/char09/chare1/charba/charcb/charf0/char10/chare9/char10/char4b/char41/chard2/char11/char82/charcb/char40 /char42/char40/char0d/char09/charac/char51/charaa/char09/char4b /char48/char2e/char51/charaa/charcb/char40 /char51/chare5/char11/char84/charaa/chard3/char09/chare1/char6d/char1a/char09/char27\nNo Alif/Ya & Punct /charfa/char0a/chard7/char43/char83/char42/char40 /char58/char41/char92/char10/char4a/char10/charaf/char42/char40/char09/chare1/chard3 /char41/charee/char09/char44/chard3 /char68/char2e/char51/char09/char6a/chard6/charcf/char40/char09/chare1/char6d/char1a/char09/char27/charf0/char10/chare9/charcb/char41/char6d/charcc/char27/char40 /chare8/char09/char59/chareb /char80/char50/char59/char09/char4b/char09/chare0/char40 /char49/char2e/char6d/char2e/char1a/char27/char0a/char09/chare1/charba/charcb/charf0/char10/chare9/char10/char4b/char41/chard2/char11/char82/charcb/char40 /char42/char40/char09/charac/char51/charaa/char09/char4b /char48/char2e/char51/charaa/charcb/char40 /char51/chare5/char11/char84/charaa/chard3/char09/chare1/char6d/char1a/char09/char27\nTable 12: Examples of normalized text: with Alif/Ya errors removed, punctuation removed, and both Alif/Ya errors\nand punctuation removed.\n117\nClass Sub-class Description\nOrthographic\nOH Hamza error\nOT Confusion in Ha and Ta Mutadarrifatin\nOA Confusuion in Alif and Ya Mutadarrifatin\nOW Confusion in Alif Fariqa\nON Confusion Between Nun and Tanwin\nOS Shortening the long vowels\nOG Lengthening the short vowels\nOC Wrong order of word characters\nOR Replacement in word character(s)\nOD Additional character(s)\nOM Missing character(s)\nOO Other orthographic errors\nMorphological\nMI Word inflection\nMT Verb tense\nMO Other morphological errors\nXF Definiteness\nXG Gender\nXN Number\nXT Unnecessary word\nXM Missing word\nXO Other syntactic errors\nSemantic\nSW Word selection error\nSF Fasl wa wasl (confusion in conjunction use/non-use)\nSO Other semantic errors\nPunctuation\nPC Punctuation confusion\nPT Unnecessary punctuation\nPM Missing punctuation\nPO Other errors in punctuation\nMerge MG Words are merged\nSplit SP Words are split\nTable 13: The ALC error type taxonomy extended with merge and split classes\nHyperparameter Seq2seq Decoder Only (LLMs) Seq2Edit Encoder Only\nLearning Rate 5×10−5 2×10−5 1×10−5\nTrain Batch Size 4 8 8\nEval Batch Size 4 8 8\nSeed 42 42 42\nGradient Accumulation Steps 8 8 8\nTotal Train Batch Size 32 64 64\nOptimizer Adam (betas=(0.9,0.999), epsilon= 1×10−8) AdamW (betas=(0.9,0.999), epsilon=1×10−7) AdamW (betas=(0.9,0.999), epsilon=1×10−8)\nLR Scheduler Type Cosine Linear Cosine\nNum Epochs 50 4 100\nTable 14: Summary of hyperparameters used for model training.\n118\nSettings Models Exact Match No Alif / Ya Errors No Punctuation No Puncation and Alif / Ya Errors\nP R F 1.0 F0.5 P R F 1.0 F0.5 P R F 1.0 F0.5 P R F 1.0 F0.5\nSeq2Edit\nARBERTv2 73.30 47.85 57.90 66.25 65.60 44.20 52.81 59.81 72.38 48.75 58.26 65.98 57.40 33.90 42.63 50.41\nARBERTv2 3-stage 74.65 46.70 57.46 66.67 65.00 41.20 50.43 58.27 75.50 44.50 56.00 66.27 55.70 27.50 36.82 46.22\nMARBERTv2 72.95 47.65 57.65 65.95 64.60 43.20 51.78 58.78 73.72 44.16 55.23 65.02 56.80 34.20 42.69 50.17\nMARBERTv2 3-stage 74.55 45.75 56.70 66.21 65.10 41.30 50.54 58.37 75.41 45.52 56.77 66.66 56.00 29.20 38.38 47.31\nLLMs\nLLama-7B 58.20 32.50 41.71 50.25 35.50 16.70 22.71 28.98 19.60 54.30 28.80 22.47 65.10 32.00 42.91 53.94\nAlpaca-7B 42.20 31.20 35.88 39.42 42.20 33.40 37.29 40.09 82.20 62.20 70.81 77.23 62.20 39.50 48.32 55.79\nVicuna-13B 63.90 51.00 56.73 60.82 51.40 39.30 44.54 48.42 83.90 73.90 78.58 81.69 68.50 49.00 57.13 63.45\nBactrian-Xbloom-7B 60.80 43.80 50.92 56.42 53.70 41.00 46.50 50.57 79.40 63.00 70.26 75.47 62.00 51.00 55.96 59.44\nBactrian-Xllama-7B 58.60 41.40 48.52 54.10 51.00 38.10 43.62 47.77 77.00 59.20 66.94 72.63 58.60 48.10 52.83 56.15\nSeq2Seq\nmT0 69.35 54.29 60.90 65.70 57.45 42.50 48.86 53.67 82.35 75.34 78.69 80.85 70.20 50.30 58.61 65.05\nmT5 69.00 53.20 60.08 65.13 56.70 39.50 46.56 52.16 81.00 70.00 75.10 78.53 68.00 48.00 56.28 62.77\nAraBART 72.00 61.50 66.34 69.62 60.00 49.70 54.37 57.61 85.00 78.50 81.62 83.62 74.00 60.50 66.57 70.84\nAraT5v2 74.50 64.50 69.14 72.26 63.50 52.70 57.60 61.00 88.00 84.50 86.21 87.28 81.50 69.50 75.02 78.78\nAraT5v2 (5M) 75.33 67.44 71.17 73.61 64.55 51.55 57.32 61.45 89.22 83.40 86.21 87.99 81.30 70.24 75.37 78.82\nAraT5v2 (10M) 75.90 68.33 71.92 74.25 65.34 52.44 58.18 62.28 89.88 84.22 86.96 88.69 82.34 71.44 76.50 79.90\nAraT5v2 (11M) 77.85 68.90 73.10 75.88 66.33 55.20 60.26 63.76 90.10 85.21 87.59 89.08 84.55 71.50 77.48 81.57\nTable 15: Dev Set results on the QALB-2014 benchmark dataset.\nCLASS GECToR_ARBERT five-shot_2014_expertprompt five-shot_2014-chatgpt4 AraT5 (11M) COUNT\nOH 73.73 89.80 92.91 87.34 4902\nOT 76.59 94.12 95.58 90.84 708\nOA 78.63 84.66 88.93 87.35 275\nOW 38.57 80.79 86.96 83.70 107\nON 0.00 0.00 0.00 0.00 0\nOG 48.00 55.74 63.64 90.32 34\nOC 21.43 28.57 53.66 87.18 22\nOR 38.24 53.02 65.96 77.10 528\nOD 33.76 51.89 59.60 73.07 321\nOM 41.80 44.53 57.35 86.44 393\nOO 0.00 0.00 0.00 0.00 0\nMI 11.02 13.25 20.53 75.00 83\nMT 0.00 7.84 11.43 62.50 7\nXC 32.95 46.10 50.78 88.35 526\nXF 6.06 17.98 23.81 76.92 29\nXG 37.10 19.57 31.35 89.47 79\nXN 25.19 25.79 31.25 88.12 108\nXT 3.95 3.78 5.48 2.48 66\nXM 2.04 4.14 6.38 1.07 26\nXO 0.00 0.00 0.00 0.00 0\nSW 50.51 21.25 33.38 8.29 219\nSF 0.00 6.67 3.45 57.14 3\nPC 60.89 56.25 47.59 74.98 713\nPT 29.62 29.58 21.40 57.42 480\nPM 55.24 54.21 52.09 67.08 5599\nMG 25.05 75.96 79.70 64.80 434\nSP 42.27 90.93 91.61 86.70 805\nmicro avg 55.67 60.05 64.51 57.28 16467\nmacro avg 30.84 39.13 43.51 61.62 16467\nweighted avg 56.98 66.96 68.24 76.35 16467\nTable 16: Analysis of Error Type performances on the QALB-2014 Test set.\n119",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8666280508041382
    },
    {
      "name": "Arabic",
      "score": 0.5752257108688354
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5646921396255493
    },
    {
      "name": "Natural language processing",
      "score": 0.5615637302398682
    },
    {
      "name": "Artificial intelligence",
      "score": 0.560642659664154
    },
    {
      "name": "Task (project management)",
      "score": 0.5539547204971313
    },
    {
      "name": "Linguistics",
      "score": 0.17583641409873962
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210113480",
      "name": "Mohamed bin Zayed University of Artificial Intelligence",
      "country": "AE"
    },
    {
      "id": "https://openalex.org/I141945490",
      "name": "University of British Columbia",
      "country": "CA"
    }
  ]
}