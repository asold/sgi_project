{
    "title": "Performance of the Large Language Models in African rheumatology: a diagnostic test accuracy study of ChatGPT-4, Gemini, Copilot, and Claude artificial intelligence",
    "url": "https://openalex.org/W4410444203",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A5093160020",
            "name": "Yannick Laurent Tchenadoyo Bayala",
            "affiliations": [
                "Université Joseph Ki-Zerbo"
            ]
        },
        {
            "id": "https://openalex.org/A4207539992",
            "name": "Wendlassida Joelle Stéphanie Zabsonre Tiendrebeogo",
            "affiliations": [
                "Université Joseph Ki-Zerbo"
            ]
        },
        {
            "id": "https://openalex.org/A4207539997",
            "name": "Dieu-Donné Ouedraogo",
            "affiliations": [
                "Université Joseph Ki-Zerbo"
            ]
        },
        {
            "id": "https://openalex.org/A2795979483",
            "name": "Fulgence Kaboré",
            "affiliations": [
                "Université Joseph Ki-Zerbo"
            ]
        },
        {
            "id": "https://openalex.org/A3102832721",
            "name": "Charles Sougué",
            "affiliations": [
                "Nazi Boni University",
                "West African Health Organisation"
            ]
        },
        {
            "id": "https://openalex.org/A2054228104",
            "name": "Aristide Relwendé Yaméogo",
            "affiliations": [
                "Université Joseph Ki-Zerbo"
            ]
        },
        {
            "id": "https://openalex.org/A5093378624",
            "name": "Wendlassida Martin Nacanabo",
            "affiliations": [
                "Université Joseph Ki-Zerbo"
            ]
        },
        {
            "id": "https://openalex.org/A4311021870",
            "name": "Ismaël Ayouba-Tinni",
            "affiliations": [
                "Université Joseph Ki-Zerbo"
            ]
        },
        {
            "id": "https://openalex.org/A4311021871",
            "name": "Aboubakar OUEDRAOGO",
            "affiliations": [
                "Université Joseph Ki-Zerbo"
            ]
        },
        {
            "id": "https://openalex.org/A5109683799",
            "name": "Yamyellé Enselme Zongo",
            "affiliations": [
                "Université Joseph Ki-Zerbo"
            ]
        },
        {
            "id": "https://openalex.org/A5093160020",
            "name": "Yannick Laurent Tchenadoyo Bayala",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4207539992",
            "name": "Wendlassida Joelle Stéphanie Zabsonre Tiendrebeogo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4207539997",
            "name": "Dieu-Donné Ouedraogo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2795979483",
            "name": "Fulgence Kaboré",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3102832721",
            "name": "Charles Sougué",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2054228104",
            "name": "Aristide Relwendé Yaméogo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5093378624",
            "name": "Wendlassida Martin Nacanabo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4311021870",
            "name": "Ismaël Ayouba-Tinni",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4311021871",
            "name": "Aboubakar OUEDRAOGO",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5109683799",
            "name": "Yamyellé Enselme Zongo",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4313454335",
        "https://openalex.org/W4386894187",
        "https://openalex.org/W4387030216",
        "https://openalex.org/W4402949381",
        "https://openalex.org/W1631304658",
        "https://openalex.org/W2554140915",
        "https://openalex.org/W4406119010",
        "https://openalex.org/W4396610906",
        "https://openalex.org/W2944626463",
        "https://openalex.org/W1990748933",
        "https://openalex.org/W4292528167",
        "https://openalex.org/W4380290798",
        "https://openalex.org/W4386998832",
        "https://openalex.org/W4380730209",
        "https://openalex.org/W4389685420",
        "https://openalex.org/W4394785902",
        "https://openalex.org/W4401916372",
        "https://openalex.org/W4386033569",
        "https://openalex.org/W4387184874"
    ],
    "abstract": "Not applicable.",
    "full_text": "RESEARCH Open Access\n© The Author(s) 2025. Open Access  This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long as you \ngive appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if you modified the \nlicensed material. You do not have permission under this licence to share adapted material derived from this article or parts of it. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory regulation or \nexceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this licence, visit  h t t p  : / /  c r e a  t i  \nv e c  o m m  o n s .  o r  g / l  i c e  n s e s  / b  y - n c - n d / 4 . 0 /.\nBayala et al. BMC Rheumatology            (2025) 9:54 \nhttps://doi.org/10.1186/s41927-025-00512-z\nBMC Rheumatology\n*Correspondence:\nYannick Laurent Tchenadoyo Bayala\nbayalayannick7991@gmail.com\nFull list of author information is available at the end of the article\nAbstract\nBackground Artificial intelligence (AI) tools, particularly Large Language Models (LLMs), are revolutionizing medical \npractice, including rheumatology. However, their diagnostic capabilities remain underexplored in the African context. \nTo assess the diagnostic accuracy of ChatGPT-4, Gemini, Copilot, and Claude AI in rheumatology within an African \npopulation.\nMethods This was a cross-sectional analytical study with retrospective data collection, conducted at the \nRheumatology Department of Bogodogo University Hospital Center (Burkina Faso) from January 1 to June 30, 2024. \nStandardized clinical and paraclinical data from 103 patients were submitted to the four AI models. The diagnoses \nproposed by the AIs were compared to expert-confirmed diagnoses established by a panel of senior rheumatologists. \nDiagnostic accuracy, sensitivity, specificity, and predictive values were calculated for each AI model.\nResults Among the patients enrolled in the study period, infectious diseases constituted the most common \ndiagnostic category, representing 47.57% (n = 49). ChatGPT-4 achieved the highest diagnostic accuracy (86.41%), \nfollowed by Claude AI (85.44%), Copilot (75.73%), and Gemini (71.84%). The inter-model agreement was moderate, \nwith Cohen’s kappa coefficients ranging from 0.43 to 0.59. ChatGPT-4 and Claude AI demonstrated high sensitivity \n(> 90%) for most conditions but had lower performance for neoplastic diseases (sensitivity < 67%). Patients under 50 \nyears old had a significantly higher probability of receiving a correct diagnosis with Copilot (OR = 3.36; 95% CI [1.16–\n9.71]; p = 0.025).\nConclusion LLMs, particularly ChatGPT-4 and Claude AI, show high diagnostic capabilities in rheumatology, despite \nsome limitations in specific disease categories.\nClinical trial number Not applicable.\nKeywords Artificial intelligence, Large Language Models, Rheumatology, Diagnostic accuracy, Africa\nPerformance of the Large Language Models \nin African rheumatology: a diagnostic test \naccuracy study of ChatGPT-4, Gemini, Copilot, \nand Claude artificial intelligence\nYannick Laurent Tchenadoyo Bayala1* , Wendlassida Joelle Stéphanie Zabsonré/Tiendrebeogo1 , Dieu-\nDonné Ouedraogo1 , Fulgence Kaboré1 , Charles Sougué2 , Aristide Relwendé Yameogo3 , Wendlassida \nMartin Nacanabo4 , Ismael Ayouba Tinni1 , Aboubakar Ouedraogo1  and Yamyellé Enselme Zongo1\nPage 2 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \nIntroduction\nRheumatology is a specialized field of internal medicine \ndedicated to studying, diagnosing, and managing mus -\nculoskeletal and autoimmune disorders [ 1]. Before the \nadvent of artificial intelligence (AI), disease diagnosis in \nrheumatology relied primarily on clinical expertise, imag-\ning techniques, and laboratory tests, whose limitations \nin accuracy, reproducibility, and time efficiency have \nfostered the development and integration of AI-based \ndiagnostic tools [ 1]. Since November 2022, significant \nadvances in AI technologies have led to the emergence of \ninnovative platforms in rheumatology [2].\nAI tools, particularly Large Language Models (LLMs), \nhave played an increasingly significant role in rheumatol -\nogy diagnosis [ 3]. LLMs are advanced natural language \nprocessing (NLP) systems designed to interpret and \nproduce human-like language [ 3]. In contrast to con -\nventional supervised deep learning approaches, LLMs \nleverage self-supervised learning methods to extract pat -\nterns and knowledge from extensive unlabelled datasets \n[4]. Subsequently, their performance on specific tasks is \noptimized through fine-tuning using smaller, annotated \ndatasets tailored to the intended application [ 4]. Recent \nLLMs integrate advanced algorithms and machine learn -\ning methods that have the potential to offer a wide range \nof applications in rheumatology [ 4]. These technologies \ncan contribute to various aspects of practice, including \ndisease diagnosis, therapeutic decision support, pre -\ndiction of adverse drug events, and the development of \npersonalized treatment strategies. By improving data \nanalysis and supporting clinical reasoning, these LLMs \nhold promise for optimizing the accuracy and efficiency \nof musculoskeletal and autoimmune diseases decision-\nmaking. The strength of these LLMs lies in their ability to \nprocess large volumes of medical data, enabling them to \ndetect patterns that may not be immediately apparent to \nclinicians [5].\nChatGPT-4 remains the most widely used LLM to \nassess the diagnostic performance of AI in rheumatol -\nogy, as in other medical specialties, due to its advanced \nreasoning capabilities, superior accuracy, and broad vali -\ndation in healthcare applications [ 4]. A study conducted \nin 2023 using multiple-choice trivia items, LLMs were \nevaluated on their ability to assist clinicians in establish -\ning differential diagnoses of rheumatic diseases based \non clinical vignettes derived from scenarios [ 6]. GPT-4 \ncorrectly answered 47 (81%) of questions, whereas \nClaude 1.3 answered 42 (72%) [ 6]. In a separate study by \nEnes et al., conducted using board-level rheumatology \nquestions, ChatGPT-4 demonstrated a high diagnos -\ntic performance with an accuracy of 86.9%, significantly \noutperforming Gemini (60.2%) [ 7]. Beyond ChatGPT-4, \nother LLMs are increasingly used in clinical settings and \ndeserve to be compared for their potential role in medical \ndecision-making in rheumatology [4]. Copilot, integrated \ninto various Microsoft products and directly embedded \nin new hardware, is gaining importance in patient inter -\nactions; however, its medical capabilities remain poorly \nstudied [ 4]. Google Gemini, launched in 2023, aims \nto enhance human-computer interaction, particularly \nin clinical reasoning [ 4]. Finally, Claude, developed by \nAnthropic, has shown promising performance in some \ndiagnostic tasks but still lacks extensive validation in \nhealthcare compared to GPT-4 [4].\nAlthough AI has increasingly been integrated into \nrheumatology for disease diagnosis, significant geo -\ngraphic disparities persist in its implementation [ 8]. In \nparticular, the lower penetration of internet access and \ndigital technologies across the African continent may \nlimit the availability and utilization of AI-based diagnos -\ntic tools [ 8]. This digital divide may also contribute to \nreduced awareness and adoption of AI in clinical practice \ncompared to other world regions, although no knowl -\nedge-attitudes-practice survey has yet confirmed this \nassumption [ 8]. Furthermore, to the best of our knowl -\nedge, no diagnostic test accuracy study has evaluated \nthe performance of LLMs such as ChatGPT-4, Gemini, \nCopilot, and Claude AI in rheumatology within an Afri -\ncan context. Addressing these gaps is essential to under -\nstand the applicability and limitations of these models in \nresource-limited settings. Therefore, this study aimed to \nassess the diagnostic accuracy, sensitivity, and specificity \nof ChatGPT-4, Gemini, Copilot, and Claude AI in rheu -\nmatology using clinical vignettes derived from African \npatients. Additionally, we sought to evaluate their abil -\nity to generate differential diagnoses and the associated \nconfidence levels, with the ultimate goal of informing \ntheir potential integration into rheumatology practice in \nAfrica.\nMethods\nThis study was conducted and reported in accordance \nwith the STARD (Standards for Reporting of Diagnostic \nAccuracy Studies) guidelines [9].\nStudy design\nThis was a cross-sectional, analytical, and comparative \nstudy with retrospective data collection.\nParticipants\nPatients’ data were collected retrospectively from hospi -\ntalization records over a six-month period (January 1 to \nJune 30, 2024) in the Rheumatology Department of the \nBogodogo University Hospital Center, Ouagadougou, \na national referral center for rheumatology in Burkina \nFaso. The department is a specialized unit dedicated to \nthe management of musculoskeletal and autoimmune \ndiseases. The department is staffed by a team of seven \nPage 3 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \nsenior rheumatologists, including one full professor, one \nassociate professor, and one rheumatology assistants \nboth considered senior experts in the field. The medical \nstaff also includes four rheumatologists and several rheu -\nmatology residents undergoing specialty training. The \ndepartment does not currently have an electronic health \nrecord system; patient data and clinical records are man -\naged using paper-based files. All patients hospitalized \nfor a rheumatologic condition during the study period \nand who provided informed consent were included. An \nexhaustive sampling method was used. This approach \naimed to include all eligible patients hospitalized during \nthe study period in order to minimize selection bias and \nensure the representativeness of the study population. \nData were extracted from paper-based files in our cen -\nter. Patients were excluded if their medical records were \ndeemed unusable, defined as having more than 75% miss-\ning data in the data collection form. In other words, a \nmedical record was considered exploitable when at least \n75% of the required variables were available. Patients \nwere also excluded in cases of persistent disagreement \namong senior rheumatologists regarding the final diagno-\nsis. The reference diagnosis (gold standard) corresponded \nto the diagnosis retained by senior rheumatologists after \na collegial discussion.\nTests methods\nIndex test\nThese AI models were selected based on their popular -\nity, free access, and widespread use as AI-driven clinical \ndiagnostic tools [10]:\n  • ChatGPT-4, developed by OpenAI (San Francisco, \nUSA). The study used the version available from \nDecember 1 to December 31, 2024, accessible at \nwww.openai.com.\n  • Copilot, formerly known as Bing AI, developed by \nMicrosoft Corporation (Washington, USA). The \nversion evaluated was available from December 1 to \nDecember 31, 2024, at www.microsoft.com.\n  • Gemini 2.0 Flash, formerly known as Bard, \ndeveloped by Google LLC (Alphabet Inc.) \n(California, USA). The version tested was available \nfrom January 1 to January 31, 2024, at www.google.\ncom/gemini.\n  • Claude 3.5 Sonnet, developed by Anthropic (San \nFrancisco, USA). The version evaluated was available \nfrom December 1 to December 31, 2024, at www.\nanthropic.com.\nReference standard\nThe gold standard diagnosis was determined by a panel \nof three senior rheumatologists (D-D.O, W.J.S.Z/T, F.K) \nwith at least five years of experience, following consensus \nduring clinical staff meetings. Patients were excluded in \ncases of persistent disagreement among senior rheuma -\ntologists regarding the final diagnosis. Diagnoses were \nestablished based on epidemiological, clinical, biological, \nand radiological criteria.\nData collection and harmonization\nData collection was performed by a team of trained \nrheumatology residents under the supervision of senior \nrheumatologists. Before data extraction, the residents \nreceived specific training on the use of the standardized \ndata collection form, the definitions of the clinical vari -\nables, and the modalities for querying the AI tools used \nin the study. The extracted data included demographic \ncharacteristics (sex, patient origin), medical history, life -\nstyle factors, physical examination findings, laboratory \nand imaging results, and the final diagnosis established \nby the rheumatologist.\nData were standardized prior to submission to AI mod-\nels, text correction was performed. This involved cor -\nrecting spelling errors, unifying medical terminology, \nremoving unnecessary or redundant information, and \nensuring consistency and readability. The expert rheu -\nmatologist diagnoses, as well as radiology and laboratory \nconclusions, were removed from the records before sub -\nmission. Radiological images were not directly uploaded \nto the chatbot systems; instead, detailed written descrip -\ntions of the imaging findings were provided as part of the \nclinical vignettes.\nClinical vignettes were presented in a harmonized, \nstructured format in English, maintaining comprehen -\nsive clinical details including history, physical examina -\ntion findings, and diagnostic test results, with identical \nprompts utilized across all AI sessions to ensure meth -\nodological consistency. The study’s rigor was enhanced \nthrough a blinded evaluation process where all AI-gen -\nerated responses were analyzed by a single independent \nevaluator who possesses both clinical expertise as a rheu -\nmatologist and technical knowledge in artificial intel -\nligence, ensuring comprehensive assessment without \nknowledge of the gold standard diagnoses, employing a \nvalidated assessment rubric to minimize cognitive bias \nduring the comparative analysis between AI diagnos -\ntic output and reference standards. We conducted two \ntest sessions spaced 10 days apart to minimize response \nvariability.\nEvaluation of index test\nEach AI model was queried in two phases using identical \nprompts, with the patient’s origin specified: first, based \nsolely on clinical data, followed by a second query after \nincorporating paraclinical results (Supplementary file \n1). To minimize bias, a separate session was initiated for \neach patient on each AI platform.\nPage 4 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \n  • First set of questions (clinical data only):\n  • What is the most probable diagnosis?\n  • What are the differential diagnoses?\n  • Second set of questions (after adding paraclinical \nresults):\n  • What is the most probable diagnosis?\n  • What is your confidence level in percentage?\nDefinition and rationale for test positivity\nThe AI-generated diagnoses were standardized using the \nICD-10 classification (Supplementary file 2) [11]. AI-gen-\nerated diagnoses were classified as follows (Supplemen -\ntary file 2):\n  • Correct: When they matched exactly with the expert \nrheumatologist’s diagnosis, following ICD-10 criteria.\n  • Partial: When the AI identified the correct disease \ncategory or pathophysiological mechanism \nbut lacked diagnostic specificity (for example, \nidentifying “infectious spondylitis” but not specifying \n“tuberculous Pott’s disease”).\n  • Incorrect: When the diagnosis had no clinical \nrelevance to the rheumatologist’s diagnosis.\nThe quality of differential diagnoses was assessed using \nBond et al. ordinal score [ 12]: (5) The actual diagnosis is \nincluded in the differential. (4) A very close suggestion is \nincluded. (3) A roughly approximate but useful sugges -\ntion is included. (2) A related but unlikely useful sugges -\ntion is included. (0) No relevant suggestion.\nAnalysis\nQuantitative variables were summarized as mean ± stan-\ndard deviation (SD) if normally distributed or median \nwith interquartile range (IQR) if non-normally distrib -\nuted, after assessing normality using the Shapiro-Wilk \ntest. Qualitative variables were presented as frequencies, \npercentages, and their respective 95% confidence inter -\nvals (CI) calculated using the Wilson method.\nThe degree of agreement between AI-generated diag -\nnoses was assessed using Cohen’s kappa coefficient (κ) \nwith 95% CI [13]. Kappa values were interpreted as: <0.20 \n(poor), 0.21–0.40 (fair), 0.41–0.60 (moderate), 0.61–0.80 \n(good), and 0.81-1.00 (very good agreement). The con -\nfidence levels of the AI models were compared using \nthe Friedman test for repeated measures with post-hoc \nDunn’s multiple comparison tests when significant dif -\nferences were found. Statistical significance was set at \np < 0.05.\nDiagnostic performance was assessed across etiologi -\ncal groups using sensitivity, specificity, positive predictive \nvalue, negative predictive value, and accuracy, all \nreported with 95% CI. The accuracy of each AI was eval -\nuated using the area under the receiver operating charac-\nteristic curve (AUC) with 95% CI, interpreted according \nto Swets et al. criteria [ 14]: AUC ≥ 0.80 considered excel-\nlent, 0.70–0.79 as good, 0.60–0.69 as fair, and < 0.60 as \npoor.\nTo identify factors potentially affecting AI diagnostic \nperformance, we selected variables based on both previ -\nous literature for rheumatological conditions and clinical \nexpertise. These factors included patient demographics \n(age, sex) and etiological groups. Pearson’s chi-square \ntest or Fisher’s exact test was used in univariate analysis \nto determine associations between categorical variables \nand AI diagnostic accuracy. Variables with p < 0.2 in uni-\nvariate analysis were included in a multivariate logistic \nregression model to assess independent associations \nwith AI diagnostic performance. Odds ratios with 95% \nCI were calculated, and statistical significance was set at \np < 0.05.\nResults are presented as text supplemented by tables \nand figures. Data were entered and analyzed using Excel \n(Microsoft Office Professional Plus 2019, Washington, \nWA, USA), Epi Info 7.2 (Centers for Disease Control \nand Prevention, CDC, Atlanta, USA), and GraphPad \nPrism (version 10.0.2; GraphPad Software Inc., Boston, \nMassachusetts).\nEthical considerations\nThis study was conducted in accordance with the prin -\nciples of the Declaration of Helsinki [ 15]. Anonymity and \ndata confidentiality were strictly maintained throughout \nthe research process. Informed consent was obtained \nfrom all participants prior to enrollment. For participants \nunable to provide consent due to cognitive impairment \nor severe illness, consent was obtained from legal guard -\nians or next of kin before any data collection commenced. \nSpecial attention was paid to participant inclusivity \nacross demographic categories including age, gender, \nethnicity, and socioeconomic status, following recent \nguidelines on equitable research participation. The study \nprotocol received approval from the Ethics Committee of \nthe Bogodogo University Hospital Center (approval num-\nber: N202202-32) before participant recruitment began.\nResults\nGeneral characteristics of the population\nA total of 112 patients were included during the study \nperiod, with 9 patients excluded due to incomplete medi-\ncal records. In total, 103 hospitalization reports were \nanalyzed (Fig.  1). The mean age was 51.9 ± 20.9 years, \nranging from 10 to 88 years. The sex ratio was 1.64, with \na predominance of male patients. Infectious diseases \nwere the most common diagnoses, accounting for 47.57% \nPage 5 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \n(n = 49), followed by chronic inflammatory rheumatic \ndiseases (16.50%, n = 17). The general characteristics of \nthe study population and the distribution of etiological \ngroups are summarized in Table 1.\nOverall performance of AI models\nChatGPT-4 correctly identified the rheumatologist’s \ndiagnosis based on clinical data alone in 80 patients \n(77.66%), compared to 56 patients (54.36%) for Gemini, \n58 patients (56.31%) for Copilot, and 79 patients (76.69%) \nfor Claude AI. When combining clinical and paraclini -\ncal data, the overall diagnostic accuracy was 86.41% for \nChatGPT-4 (n = 89), 71.84% for Gemini ( n = 74), 75.73% \nfor Copilot ( n = 78), and 85.44% for Claude AI ( n = 88). \nThe concordance between AI models and rheumatolo -\ngists is illustrated in Fig. 2.\nThe agreement among AI models was moderate, with \nCohen’s kappa coefficients ranging from 0.43 to 0.59. The \nhighest concordance was observed between ChatGPT-4 \nand Copilot (κ = 0.59; 95% CI [0.405–0.786]), followed by \nCopilot and Claude AI (κ = 0.57; 95% CI [0.377–0.766]) \n(Table 2).\nUsing Bond et al. ordinal score, the rheumatologist’s \ndiagnosis was included as a differential diagnosis with \na score of 5 in 75.72% of cases ( n = 78) for ChatGPT-4, \n57.28% ( n = 59) for Gemini, 55.33% ( n = 57) for Copilot, \nand 75.72% ( n = 78) for Claude AI. The differential diag -\nnosis scoring for each AI model is illustrated in Fig. 3.\nConfidence rating\nThe median global confidence level was 90% (IQR: \n85–92.5) for ChatGPT-4, 85% (IQR: 80–90) for Gemini, \n90% (IQR: 85–90) for Copilot, and 92.5% (IQR: 90–95) \nfor Claude AI (Fig. 4).\nPerformance metrics by etiological group\nThe diagnostic performance of AI models varied signifi -\ncantly across different etiological groups, with notable \ndifferences in sensitivity, specificity, and accuracy.\nChatGPT-4 showed the highest sensitivity for infec -\ntious diseases (91.83%) and chronic inflammatory rheu -\nmatic diseases (94.11%), but with relatively low specificity \n(18.51% and 15.11%, respectively). Claude AI also dem -\nonstrated high sensitivity for infectious diseases (91.83%) \nand chronic inflammatory rheumatic diseases (94.11%). \nDegenerative diseases were best diagnosed by Chat -\nGPT-4, with a sensitivity of 86.66%. Gemini exhibited the \nbest balance between sensitivity (80.00%) and specificity \n(29.54%) for degenerative diseases. Neoplastic diseases \nremained a challenge for all AI models, with sensitivities \nnot exceeding 67%. The detailed performance metrics \nfor each AI across etiological groups are summarized in \nTable 3.\nROC curve analysis\nIn our study, Gemini achieved the highest AUC of \n0.633 (95% CI: 0.533–0.726) for chronic inflammatory \nrheumatic diseases and 0.548 (95% CI: 0.447–0.646) \nfor degenerative diseases. Claude AI and ChatGPT-4 \nFig. 1 Flowchart of the study design\n \nPage 6 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \ndisplayed similar performance, with AUC values close \nto 0.55 for infectious diseases and chronic inflammatory \nrheumatic diseases. However, AI models demonstrated \npoor classification ability for neoplastic diseases, with \nAUC values below 0.40, indicating low discrimination \ncapacity in this category. The ROC curves for AI perfor -\nmance by etiological group are presented in Fig. 5.\nFactors affecting the diagnostic performance of AI tools\nMultivariate analysis revealed that neoplastic diseases \nwere significantly associated with a lower probability \nof correct diagnosis by ChatGPT-4 (OR = 0.08; 95% CI \n[0.01–0.45]; p = 0.004) and Copilot (OR = 0.09; 95% CI \n[0.01–0.54]; p = 0.007). Conversely, patients under 50 \nyears old had a significantly higher probability of receiv -\ning a correct diagnosis with Copilot (OR = 3.36; 95% CI \n[1.16–9.71]; p = 0.025). The complete logistic regression \nanalysis is summarized in Table 4.\nDiscussion\nPrincipal finding\nIn our study, ChatGPT-4 demonstrated the highest \noverall diagnostic accuracy rate (86.41%) defined as the \nproportion of correctly identified diagnoses out of total \ncases, for all-cause rheumatological diseases. It fol -\nlowed by Claude AI (85.44%), Copilot (75.73%), and \nGemini (71.84%). Gemini achieved the highest AUC of \n0.633 (95% CI: 0.533–0.726) for chronic inflammatory \nTable 1 General and etiological characteristics of the study \npopulation\nVariables n Percentage (%)\nMean age (years) 51.9 ± 20.9\nAge groups\n< 50 years 53 51.46\n≥ 50 years 50 48.54\nSex\nMale 64 62.14\nFemale 39 37.86\nRheumatological disease categories\nInfectious diseases 49 47.57\nPott’s disease 23 22.33\nPyogenic spondylodiscitis 7 6.80\nSeptic arthritis of peripheral joints 10 9.71\nPyogenic zygapophyseal arthritis 2 1.94\nInfectious myositis 3 2.91\nAcute rheumatic fever 1 0.97\nSeptic osteonecrosis of the femoral head 2 1.94\nChronic inflammatory rheumatic diseases 17 16.50\nRheumatoid arthritis 7 6.80\nSystemic lupus erythematosus 4 3.88\nSystemic sclerosis 1 0.97\nDermatomyositis 1 0.97\nPost-streptococcal rheumatism 1 0.97\nAnkylosing spondylitis 2 1.94\nSjögren’s syndrome 1 0.97\nDegenerative diseases 15 14.56\nCommon low back pain in adults 9 8.74\nAvascular necrosis of the femoral head 2 1.94\nAcute flare of knee osteoarthritis 2 1.94\nCommon cervical pain 1 0.97\nMicrocrystalline diseases 13 12.62\nGout 13 12.62\nNeoplastic diseases 9 8.74\nSpinal bone metastasis 4 3.88\nBenign bone tumor of the spine 1 0.97\nMultiple myeloma 3 2.91\nTable 2 Cohen’s kappa coefficient for agreement between AI \nmodel pairs\nCoefficient Kappa de Cohen CI à 95%\nChatGPT-4 vs. Gemini 0.45 [0.265–0.652]\nChatGPT-4 vs. Copilot 0.59 [0.405–0.786]\nChatGPT-4 vs. Claude AI 0.47 [0.235–0.721]\nGemini vs. Copilot 0.44 [0.254–0.643]\nGemini vs. Claude AI 0.43 [0.241–0.633]\nCopilot vs. Claude AI 0.57 [0.377–0.766]\nFig. 2 Stacked bar chart representing the concordance between AI models and the rheumatologist’s diagnosis\n \nPage 7 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \nrheumatic diseases and 0.548 (95% CI: 0.447–0.646) for \ndegenerative diseases. Gemini stood out for its best bal -\nance between sensitivity and specificity for degenerative \ndiseases, suggesting it may be the most suitable tool for \nthis condition. Conversely, neoplastic musculoskeletal \ndiseases were significantly associated with a lower cor -\nrect diagnosis rate for ChatGPT-4 (OR = 0.08; 95% CI \n[0.01–0.45]; p = 0.004) and Copilot (OR = 0.09; 95% CI \n[0.01–0.54]; p = 0.007), with AUC values below 0.40 and \nsensitivities not exceeding 67%.\nInterpretation of the principal findings\nThis difficulty in correctly identifying neoplastic diseases \nmay be due to the rarity and clinical polymorphism of \nosteoarticular tumors, which can mimic more common \nconditions such as infections and chronic inflammatory \nrheumatic diseases [ 16–20]. Additionally, their diagnosis \nis often confirmed through histopathological examina -\ntion, which was unavailable in our study population.\nIn contrast, patients under 50 years old had a signifi -\ncantly higher probability of receiving a correct diagnosis \nwith Copilot (OR = 3.36; 95% CI [1.16–9.71]; p = 0.025). \nFig. 4 Boxplot displaying the distribution of AI confidence levels. Black \nsolid line represents the median, boxplot represents the 25th (Q1) and \n75th (Q3) percentile. Whiskers range from the minimum to the maximum \nvalue. * = p < 0.0001\n \nFig. 3 Histogram of differential diagnosis scores based on Bond et al. ordinal score [ 6]: (a) ChatGPT-4 (b) Gemini (c) Copilot (d) Claude AI. (5) The actual \ndiagnosis is included in the differential. (4) A very close suggestion is included. (3) A roughly approximate but useful suggestion is included. (2) A related \nbut unlikely useful suggestion is included. (0) No relevant suggestion\n \nPage 8 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \nThis trend may be attributed to biases in AI training \nmodels, which are often based on clinical cases from \nyounger populations, as frequently reported in the litera -\nture [21].\nComparison with previous studies\nOur findings are generally consistent with existing lit -\nerature on the diagnostic performance of AI models in \nrheumatology. Venerito et al. (2023) evaluated three AI \nmodels on theoretical rheumatology questions, reporting \ndiagnostic accuracy rates of 81% for GPT-4 and Claude 2, \nwhile Claude 1.3 achieved 72% [ 16]. While their results \nalign with ours, some key differences emerge, particularly \nin infectious rheumatologic diseases [ 16]. In our study, \ndiagnostic performance for these conditions ranged from \n40 to 55%, whereas in Venerito et al. study, ChatGPT-4’s \ncorrect response rate dropped to 57%, while Claude 1.3 \nand Bard did not exceed 14% [16].\nOur results also differ from Krusche et al., who com -\npared ChatGPT-4’s diagnostic performance to that of \nrheumatologists. Their study found that ChatGPT-4 \ncorrectly identified the final diagnosis in 35% of cases, \ncompared to 39% for rheumatologists [ 17]. Furthermore, \nChatGPT-4 achieved relevant differential diagnoses in \n60% of cases, and its accuracy for chronic inflammatory \nrheumatic diseases was 71%, whereas in our study, it was \n94.12%. This discrepancy may be explained by the critical \nrole of immunological tests and imaging in diagnosing \nchronic inflammatory rheumatic diseases, which were \nnot systematically incorporated into AI prompts in Krus -\nche et al. study [17].\nOther studies have confirmed the high diagnostic per -\nformance of ChatGPT-4 in rheumatology, particularly in \nstudent examination settings. ChatGPT-4o achieved an \naccuracy of 86.9%, significantly outperforming Gemini \n(60.2%), with particularly high accuracy in subfields such \nas osteoarthritis ( p = 0.023) and rheumatoid arthritis \n(p < 0.001) [18]. Similarly, Madrid-Garcia et al. reported \na 93.71% accuracy rate for ChatGPT-4 in rheumatology \nexaminations, suggesting its potential as an educational \ntool for rheumatology learning [19].\nStrengths\nOur study offers several methodological strengths. First, \nit represents the first comparison of four widely acces -\nsible AI tools in a clinical rheumatology setting, adhering \nto established diagnostic test accuracy study guidelines. \nAll data were extracted systematically from paper-based \nfiles records using a standardized protocol, ensuring con-\nsistency in data collection and minimizing selection bias.\nThe comprehensive performance assessment using \nmultiple metrics provides a robust evaluation framework. \nNotably, the AUC analysis revealed Gemini’s superior \ndiscriminative ability for chronic inflammatory rheu -\nmatic diseases (AUC = 0.633; 95% CI: 0.533–0.726) and \ndegenerative conditions (AUC = 0.548; 95% CI: 0.447–\n0.646), offering insights beyond raw accuracy metrics. \nFurthermore, our stratified analysis across different etio -\nlogical groups provides granular performance assessment \nessential for clinical implementation.\nUnlike simulation-based evaluations, our study uti -\nlized real clinical cases from routine practice, enhancing \nTable 3 Performance metrics of each AI model based on etiological groups\nAI Models Etiological Groups Sensitivity (%) Specificity (%) PPV (%) NPV (%) Accuracy (%) AUC (95% CI)\nChatGPT-4 Infectious diseases 91.83 18.51 50.56 71.42 53.39 0.552 (0.451–0.650)\nDegenerative diseases 86.66 13.63 14.60 85.71 24.72 0.502 (0.401–0.602)\nChronic inflammatory rheumatic diseases 94.11 15.11 17.97 92.85 28.15 0.546 (0.445–0.645)\nMicrocrystalline diseases 84.61 13.33 12.36 85.71 22.33 0.490 (0.390–0.590)\nNeoplastic diseases 44.44 9.57 4.49 64.28 12.62 0.270 (0.187–0.367)\nGemini Infectious diseases 69.38 25.92 45.94 48.27 46.60 0.477 (0.377–0.577)\nDegenerative diseases 80.00 29.54 16.21 89.65 36.89 0.548 (0.447–0.646)\nChronic inflammatory rheumatic diseases 94.11 32.55 21.62 96.55 42.71 0.633 (0.533–0.726)\nMicrocrystalline diseases 61.53 26.66 10.81 82.75 31.06 0.441 (0.343-0542)\nNeoplastic diseases 44.44 25.53 5.40 82.75 27.18 0.350 (0.259–0.450)\nCopilot Infectious diseases 81.63 29.63 51.28 64.00 54.36 0.556 (0.455–0.654)\nDegenerative diseases 80.00 25.00 15.38 88.00 33.01 0.525 (0.424–0.624)\nChronic inflammatory rheumatic diseases 82.35 25.58 17.94 88.00 34.95 0.540 (0.439-0649)\nMicrocrystalline diseases 69.23 23.33 11.53 84.00 29.12 0.463 (0.364–0.564)\nNeoplastic diseases 33.33 20.21 3.84 76.00 21.35 0.268 (0.185–0.364)\nClaude AI Infectious diseases 91.83 20.37 51.13 73.33 54.36 0.561 (0.460–0.659)\nDegenerative diseases 73.33 12.50 12.50 73.33 21.35 0.429 (0.332–0.530)\nChronic inflammatory rheumatic diseases 94.11 16.27 18.18 93.33 29.12 0.552 (0.451–2.568)\nMicrocrystalline diseases 76.92 13.33 11.36 80.00 21.35 0.451 (0.353–0.552)\nNeoplastic diseases 66.66 12.76 6.81 80.00 17.47 0.397 (0.302–0.492)\nPPV: Positive Predictive Value NPV: Negative Predictive Value\nPage 9 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \necological validity and clinical relevance. Additionally, \nthis research represents one of the first diagnostic accu -\nracy studies of AI tools conducted in an African health -\ncare setting, providing crucial contextual validation \nnecessary for appropriate AI implementation in diverse \nmedical environments, a critical consideration for equita-\nble AI development in medicine across the continent [4].\nLimitations\nOur study presents several methodological limitations \nthat warrant consideration. This cross-sectional diag -\nnostic test accuracy design, while practical for initial \nassessment, inherently restricts external validity and \napplication to broader clinical contexts.\nFig. 5 Receiver Operating Characteristic (ROC) curves for each AI model by etiological group: (a) Infectious diseases (b) Degenerative diseases (c) Chronic \ninflammatory rheumatic diseases (d) Microcrystalline diseases (e) Neoplastic diseases\n \nPage 10 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \nA significant limitation concerns our gold standard, \nthe rheumatologist’s final diagnosis which, despite being \nstandard clinical practice, lacks the objectivity and repro-\nducibility of universally accepted diagnostic biomarkers. \nThis reference standard may introduce inter-observer \nvariability and potentially limit replication in future \nstudies.\nThe diagnostic accuracy of AI models demonstrated \nin this study may vary across different languages, as \ntheir performance was only assessed using English \nclinical vignettes, which limits generalizability to non-\nEnglish clinical settings including French or Spanish \nconsultations.\nAdditionally, our study evaluated AI performance using \nretrospective data from hospitalized patients only, poten-\ntially overlooking the heterogeneity of presentations in \noutpatient settings. The AI models’ inability to access \nlongitudinal patient data and disease evolution often \ncrucial diagnostic elements in rheumatology further con -\nstrains the clinical applicability of our findings. Moreover, \nAI diagnostic capabilities are fundamentally bounded by \ntheir training datasets, which may harbor inherent biases \nregarding disease prevalence, demographic representa -\ntion, and clinical presentations, potentially affecting diag-\nnostic reliability across diverse patient populations.\nImplications of our findings for future research, policies, \nand clinical practice\nOur findings have several implications for rheumato -\nlogical practice in Burkina Faso and in Africa. Given the \nsevere shortage of rheumatologists, AI-assisted diagnos -\ntic tools could serve as valuable clinical decision sup -\nport systems for non-specialist healthcare providers who \nmanage the majority of musculoskeletal conditions. For \nAfrica specifically, the development of context-adapted \nAI algorithms that account for the local disease profile, \nTable 4 Univariate analysis and logistic regression of the association between sociodemographic variables, etiological groups, and AI \ndiagnostic performance\nVariables Univariate analysis Multivariate analysis\nCorrect Diagnosis n (%) OR p value OR [CI 95%] p value\nChatGPT-4\nAge < 50 years 43 (48.31) 0.93 0.454\nMale sex 53 (59.55) 0.40 0.092 0.28 [0.06 ; 1.32] 0.108\nInfectious diseases 45 (50.56) 2.55 0.106 1.49 [0.36 ; 6.05] 0.571\nDegenerative diseases 13 (14.61) 1.02 0.644\nChronic inflammatory rheumatic diseases 16 (17.98) 2.84 0.280\nMicrocrystalline diseases 11 (12.36) 0.84 0.558\nNeoplastic diseases 4 (4.49) 0.08 0.002 0.08 [0.01; 0.45] 0.004\nGemini\nAge < 50 years 38 (51.35) 1.49 0.187 1.89 [0.74; 4.81] 0.178\nMale sex 42 (56.76) 0.41 0.056 0.40 [0.14; 1.14] 0.089\nInfectious diseases 34 (45.95) 0.79 0.378\nDegenerative diseases 12 (16.22) 1.67 0.337\nChronic inflammatory rheumatic diseases 16 (21.62) 7.72 0.018 6.13 [0.74; 50.43] 0.091\nMicrocrystalline diseases 8 (10.81) 0.58 0.281\nNeoplastic diseases 4 (5.41) 0.27 0.041 0.27 [0.06; 1.22] 0.089\nCopilot\nAge < 50 years 42 (53.85) 2.47 0.046 3.36 [1.16; 9.71] 0.025\nMale sex 46 (58.97) 0.55 0.176 0.40 [0.13; 1.20] 0.105\nInfectious diseases 40 (51.28) 1.87 0.135 1.33 [0.46; 3.80] 0.587\nDegenerative diseases 12 (15.38) 1.33 0.480\nChronic inflammatory rheumatic diseases 14 (17.95) 1.60 0.361\nMicrocrystalline diseases 9 (11.54) 0.68 0.389\nNeoplastic diseases 3 (3.85) 0.12 0.005 0.09 [0.01; 0.54] 0.007\nClaude AI\nAge < 50 years 43 (48.86) 1.09 0.549\nMale sex 55 (62.50) 1.11 0.534\nInfectious diseases 45 (51.14) 2.87 0.068 3.37 [0.65; 17.50] 0.147\nDegenerative diseases 11 (12.50) 0.39 0.148 0.82 [0.04; 0.82] 0.826\nChronic inflammatory rheumatic diseases 16 (18.18) 3.11 0.147 4.80 [0.43; 52.76] 0.199\nMicrocrystalline diseases 10 (11.36) 0.51 0.286\nNeoplastic diseases 6 (6.82) 0.29 0.122 0.60 [0.09; 3.98] 0.597\nPage 11 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \nparticularly the high prevalence of infectious diseases \nobserved in our study. These tools should be optimized \nto function with limited paraclinical resources, as many \nhealthcare facilities in Africa operate without advanced \nimaging or laboratory capabilities.\nThe implementation strategy should prioritize training \nprograms for general practitioners and nurses at periph -\neral health centers, where rheumatological expertise \nis most scarce. Given our finding that ChatGPT-4 and \nClaude AI demonstrated high sensitivity for infectious \ndiseases and chronic inflammatory rheumatic conditions, \nthese models could be particularly valuable for initial \nscreening and triage in primary care settings throughout \nAfrica [4].\nFor broader African applications, our results must \nbe contextualized within the digital health landscape \ndescribed by recent perspectives on rheumatologi -\ncal disease diagnosis in Africa. The diagnostic per -\nformance variations we observed across different AI \nmodels reinforce the importance of rigorous validation \nstudies in diverse African populations before widespread \nimplementation.\nFurthermore, the poor performance of all AI models in \ndetecting neoplastic diseases (AUC values < 0.40) high -\nlights a critical limitation that must be addressed through \nspecialized algorithms and clear clinical guidelines to \nprevent missed diagnoses of malignancies [21].\nSuccessful integration of AI into rheumatological care \nin Africa will require multi-stakeholder collaboration \nincluding ministries of health, medical associations, \npatient advocacy groups, and technology developers. \nRegulatory frameworks must be established to ensure \ndata protection, ethical use, and equitable access across \ndifferent socioeconomic groups. Educational initiatives \nshould target both healthcare providers and patients to \nbuild trust and facilitate appropriate utilization of these \ntechnologies.\nIn the context of patient data confidentiality and \nhealthcare security concerns, future research should \nexplore smaller locally deployable language models like \nLlama, which could offer viable alternatives for clinical \nsettings where on-premises deployment would mitigate \nthe privacy risks associated with transmitting sensitive \npatient information to cloud-based AI systems [21].\nFinally, future research should focus on developing AI \nmodels specifically trained on African patient data to \nimprove diagnostic accuracy for conditions with unique \nlocal presentations and to account for regional genetic, \nenvironmental, and socioeconomic factors that influence \ndisease manifestation and progression [4].\nConclusion\nThis study demonstrated that AI models exhibit promis -\ning diagnostic capabilities in rheumatology, with remark-\nable accuracy for ChatGPT-4 (86.41%) and Claude AI \n(85.44%), followed by Copilot (75.73%) and Gemini \n(71.84%). These tools were particularly effective in diag -\nnosing infectious diseases and chronic inflammatory \nrheumatic conditions, with sensitivities exceeding 90% \nin some models. However, neoplastic diseases were \nmore challenging to identify for ChatGPT-4 and Copilot, \nreducing their performance in this domain. In contrast, \npatients under 50 years old had a higher probability of \nreceiving a correct diagnosis with Copilot. These find -\nings highlight both the potential of AI in rheumatology \nand its diagnostic limitations, particularly for certain \ndisease groups. Further research is needed to evaluate \nAI integration into clinical practice, including its impact \non patient management timelines, cost-effectiveness, and \nacceptance by both clinicians and patients.\nSupplementary Information\nThe online version contains supplementary material available at  h t t p  s : /  / d o i  . o  r \ng /  1 0 .  1 1 8 6  / s  4 1 9 2 7 - 0 2 5 - 0 0 5 1 2 - z.\nSupplementary Material 1\nSupplementary Material 2\nAcknowledgements\nNot applicable.\nAuthor contributions\nY.L.T.B: Contributing to the conceptualization, the data curation, the \nmethodology, the formal analysis, the supervision, the validation and the \nwriting of the original draft. W.J.S.Z/T: Contributing to the conceptualization, \nthe data curation and the methodology. D-D.O: Contributing to the \nconceptualization, the data curation the methodology, the supervision, \nthe validation and the writing of the original draft. F.K: Contributing to the \nconceptualization, the methodology, the validation and the writing of the \noriginal draft. C.S: Contributing to the validation and the writing of the original \ndraft. A.R.Y: Contributing to the supervision, the validation and writing – \nreview and editing. I.A.T: Contributing to the supervision, the validation and \nthe writing of the original draft. W.M.N: Contributing to the visualization, \nthe validation and the writing of the original draft. A.O: Contributing to the \nvalidation and the writing of the original draft. Y.E.Z: Contributing to the \nvalidation and the writing of the original draft.\nFunding\nNo funding.\nData Availability\nData supporting the results of this study are available from the corresponding \nauthor on reasonable request.\nDeclarations\nEthics approval and consent to participate\nThe study was approved by the local Ethics Committee of the Bogodogo \nUniversity Hospital center. (N202202-32). Informed consent was obtained from \neach included patient or their relatives in the study.\nConsent for publication\nNot applicable\nPage 12 of 12\nBayala et al. BMC Rheumatology            (2025) 9:54 \nCompeting interests\nThe authors declare no competing interests.\nAuthor details\n1Department of Rheumatology, Bogodogo University Hospital Center, \nSector 51, 14 BP 371, Ouagadougou, Burkina Faso\n2Department of Internal Medicine, Sourou Sanou University Hospital \nCenter, Bobo-Dioulasso, Burkina Faso\n3Department of Cardiology, Tengandogo University Hospital Center, \nOuagadougou, Burkina Faso\n4Department of Cardiology, Bogodogo University Hospital Center, \nOuagadougou, Burkina Faso\nReceived: 13 February 2025 / Accepted: 12 May 2025\nReferences\n1. Silverman ED. Celebrating the journal of rheumatology’s 50th year of publica-\ntion. J Rheumatol. 2023;50(1):1–2.\n2. Al-Ashwal FY, Zawiah M, Gharaibeh L, Abu-Farha R, Bitar AN. Evaluating the \nsensitivity, specificity, and accuracy of ChatGPT-3.5, ChatGPT-4, Bing AI, and \nbard against conventional Drug-Drug interactions clinical tools. Drug Healthc \nPatient Saf. 2023;15:137–47.\n3. Chinnadurai S, Mahadevan S, Navaneethakrishnan B, Mamadapur M. \nDecoding applications of artificial intelligence in rheumatology. Cureus. \n2023;15(9):e46164.\n4. Takita H, Kabata D, Walston SL, Tatekawa H, Saito K, Tsujimoto Y et al. Diag-\nnostic performance comparison between generative AI and physicians: a \nsystematic review and meta-analysis. medRxiv; 2024.  h t t p  s : /  / w w w  . m  e d r  x i v  \n. o r g  / c  o n t  e n t  /  h t  t p s  : / /  d o i .  o r  g / 1  0 . 1  1 0 1 /  2 0  2 4 . 0 1 . 2 0 . 2 4 3 0 1 5 6 3 v 2. Accessed 10 \nfebruary 2025.\n5. Kumari A, Kumari A, Singh A, Singh SK, Juhi A, Dhanvijay AKD, et al. Large \nLanguage models in hematology case solving: A comparative study of Chat-\nGPT-3.5, Google bard, and Microsoft Bing. Cureus. 2023;15(8):e43861.\n6. Venerito V, Puttaswamy D, Iannone F, Gupta L. Large Language mod-\nels and rheumatology: a comparative evaluation. Lancet Rheumatol. \n2023;5(10):e574–8.\n7. Is EE, Menekseoglu AK. Comparative performance of artificial intelligence \nmodels in rheumatology board-level questions: evaluating Google gemini \nand ChatGPT-4o. Clin Rheumatol. 2024;43(11):3507–13.\n8. Boateng G, John S, Boateng S, Badu P , Agyeman-Budu P , Kumbol V. Real-\nWorld deployment and evaluation of Kwame for science, an AI teaching \nassistant for science education in West Africa. Artificial intelligence in educa-\ntion. Cham: Springer Nature Switzerland; 2024. pp. 119–33.\n9. Cohen JF, Korevaar DA, Altman DG, Bruns DE, Gatsonis CA, Hooft L, et al. \nSTARD 2015 guidelines for reporting diagnostic accuracy studies: explanation \nand elaboration. BMJ Open. 2016;6(11):e012799.\n10. Kaya Kaçar H, Kaçar ÖF, Avery A. Diet quality and caloric accuracy in \nAI-Generated diet plans: A comparative study across chatbots. Nutrients. \n2025;17(2):206.\n11. CIM-10 FR à usage PMSI. 2025| Publication ATIH.  h t t p s :   /  / w w  w .  a t i   h . s  a n  t  e .   f r /   c i \nm   - 1   0 -  f r - u  s a g e  - p m s i - 2 0 2 5. Accessed 29 Jan 2025.\n12. Carini C, Seyhan AA. Tribulations and future opportunities for artificial intel-\nligence in precision medicine. J Transl Med. 2024;22(1):411.\n13. Martín Andrés A, Álvarez Hernández M. Hubert’s multi-rater kappa revisited. \nBr J Math Stat Psychol. 2020;73(1):1–22.\n14. Swets JA. Measuring the accuracy of diagnostic systems. Science. \n1988;240(4857):1285–93.\n15. World Medical Association. World medical association declaration of Helsinki: \nethical principles for medical research involving human subjects. JAMA. \n2013;310(20):2191–4.\n16. Venerito V, Bilgin E, Iannone F, Kiraz S. AI am a rheumatologist: a practical \nprimer to large Language models for rheumatologists. Rheumatol Oxf Engl. \n2023;62(10):3256–60.\n17. Krusche M, Callhoff J, Knitza J, Ruffer N. Diagnostic accuracy of a large \nLanguage model in rheumatology: comparison of physician and ChatGPT-4. \nRheumatol Int. 2024;44(2):303–6.\n18. Kanjee Z, Crowe B, Rodman A. Accuracy of a generative artificial intelligence \nmodel in a complex diagnostic challenge. JAMA. 2023;330(1):78–80.\n19. Madrid-García A, Rosales-Rosado Z, Freites-Nuñez D, Pérez-Sancristóbal I, \nPato-Cour E, Plasencia-Rodríguez C, et al. Harnessing ChatGPT and GPT-4 \nfor evaluating the rheumatology questions of the Spanish access exam to \nspecialized medical training. Sci Rep. 2023;13(1):22129.\n20. Makrygiannakis MA, Giannakopoulos K, Kaklamanos EG. Evidence-based \npotential of generative artificial intelligence large Language models in ortho-\ndontics: a comparative study of ChatGPT, Google bard, and Microsoft Bing. \nEur J Orthod. 2024;46:cjae017.\n21. Nacanabo M, Seghda TAA, Bayala YLT, Millogo G, Thiam A, Yameogo N, et al. \nEvaluation of the diagnostic capabilities of artificial intelligence in the cardiol-\nogy department of the Bogodogo University Hospital Center using Chat GPT; \n2024.\nPublisher’s note\nSpringer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations."
}