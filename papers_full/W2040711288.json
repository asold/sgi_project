{
    "title": "Factored neural language models",
    "url": "https://openalex.org/W2040711288",
    "year": 2006,
    "authors": [
        {
            "id": "https://openalex.org/A2186115718",
            "name": "Andrei Alexandrescu",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2068133273",
            "name": "Katrin Kirchhoff",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2186115718",
            "name": "Andrei Alexandrescu",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2068133273",
            "name": "Katrin Kirchhoff",
            "affiliations": [
                "University of Washington"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2399351785",
        "https://openalex.org/W2056250865",
        "https://openalex.org/W2140679639",
        "https://openalex.org/W2015093644",
        "https://openalex.org/W1707124376",
        "https://openalex.org/W2062289558",
        "https://openalex.org/W2437096199",
        "https://openalex.org/W36903255",
        "https://openalex.org/W274041255",
        "https://openalex.org/W2171645483",
        "https://openalex.org/W2121227244",
        "https://openalex.org/W2110372834"
    ],
    "abstract": "We present a new type of neural probabilistic language model that learns a mapping from both words and explicit word features into a continuous space that is then used for word prediction. Additionally, we investigate several ways of deriving continuous word representations for unknown words from those of known words. The resulting model significantly reduces perplexity on sparse-data tasks when compared to standard backoff models, standard neural language models, and factored language models.",
    "full_text": null
}