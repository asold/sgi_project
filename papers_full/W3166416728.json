{
  "title": "What Context Features Can Transformer Language Models Use?",
  "url": "https://openalex.org/W3166416728",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4287287040",
      "name": "O'Connor, Joe",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965627067",
      "name": "Andreas Jacob",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2950220847",
    "https://openalex.org/W2132339004",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035305735",
    "https://openalex.org/W3134998451",
    "https://openalex.org/W2563734883",
    "https://openalex.org/W2149741699",
    "https://openalex.org/W1974564104",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W3117667887",
    "https://openalex.org/W1995875735",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2140676672",
    "https://openalex.org/W2948710720",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2110485445",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W2996336810",
    "https://openalex.org/W2562979205",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2996068536",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2963951265",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W3117312003",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2986922898",
    "https://openalex.org/W2963466651"
  ],
  "abstract": "Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations -- including shuffling word order within sentences and deleting all words other than nouns -- remove less than 15% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.",
  "full_text": "What Context Features Can Transformer Language Models Use?\nJoe O’Connor Jacob Andreas\nMassachusetts Institute of Technology\n{joeoc,jda}@mit.edu\nAbstract\nTransformer-based language models beneﬁt\nfrom conditioning on contexts of hundreds to\nthousands of previous tokens. What aspects\nof these contexts contribute to accurate model\nprediction? We describe a series of experi-\nments that measure usable information by se-\nlectively ablating lexical and structural infor-\nmation in transformer language models trained\non English Wikipedia. In both mid- and long-\nrange contexts, we ﬁnd that several extremely\ndestructive context manipulations—including\nshufﬂing word order within sentences and\ndeleting all words other than nouns—remove\nless than 15% of the usable information. Our\nresults suggest that long contexts, but not their\ndetailed syntactic and propositional content,\nare important for the low perplexity of current\ntransformer language models.1\n1 Introduction\nRecent years have seen a signiﬁcant improvement\nin the predictive accuracy of neural language mod-\nels (LMs), owing to a combination of improve-\nments in model architecture (especially transform-\ners; Vaswani et al. 2017) and training infrastructure\n(Wolf et al., 2020). The most striking change, rela-\ntive to both recurrent neural LMs (Mikolov et al.,\n2010) and count-based models (Kneser and Ney,\n1995), is the length of the context that these models\ncan effectively condition on. While count-based\nLMs in production speech recognition and machine\ntranslation systems typically used 10–20 tokens at a\nmaximum (e.g., Brown, 2011), and recurrent LMs\nhave an effective context size of 200 (Khandelwal\net al., 2018), the predictive accuracy of transformer\nLMs appears to improve when conditioning on as\nmany as a thousand previous tokens (Beltagy et al.,\n2020). A signiﬁcant amount of recent work has\n1Code for all experiments in this paper is available at\nhttps://github.com/lingo-mit/context-ablations.\nfocused on making use of even longer contexts\ncomputationally feasible (Rae et al., 2019; Wang\net al., 2020; Child et al., 2019; Dai et al., 2019;\nKitaev et al., 2020).\nBut despite empirical evidence that long contexts\nare helpful, little is understood about why. If the\nfuture of language modeling will include a focus\non contexts of increasing size, it is important to\nﬁrst understand what contextual information con-\ntributes to accurate prediction in current models.\nThis paper offers an answer to that question via the\nV-information framework of Xu et al. (2020). V-\ninformation, discussed more in Section 2, provides\na formal framework for reasoning about how much\nusable information a computationally constrained\npredictor (like a neural LM) can extract from an\ninput. Our experiments measure the amount of us-\nable information that is added when increasing LM\ncontext size, then attempt to pinpoint the source of\nthis information by ablating features of the added\ncontext (via controlled shufﬂing and word deletion)\nand measuring the resulting loss of model predic-\ntive power. While this framework is general, we\nfocus on transformer LMs.\nOur work is closely related to an earlier study by\nKhandelwal et al. (2018), which measured changes\nin a pre-trained LSTM LM when context words\nwere permuted and deleted at evaluation time. But\nneural language models are known to be highly\nsensitive to distributional shifts—and in particular\nmight be unable to use information from long-range\ncontext but still be adversely affected when the\nstructure of that context changes at evaluation time.\nDirectly measuring usable information makes it\npossible to clearly distinguish accuracy decreases\nthat result from loss of information and decreases\nthat result from out-of-distribution inputs.\nOur experiments reveal a number of surprising\nfacts about the use of long- and mid-range context\nin transformers. While increasing context length\narXiv:2106.08367v1  [cs.CL]  15 Jun 2021\nfrom 256 to 768 tokens is beneﬁcial (decreasing\nperplexity by roughly 4%), many destructive trans-\nformations of this context (including transforma-\ntions that cause large changes in the paradigm of\nKhandelwal et al. 2018) remove essentially no us-\nable information. Our results suggest that for cur-\nrent models, the primary carriers of information in\nlong-range context are content words and local co-\noccurrence statistics: deleting function words and\nshufﬂing within local windows both have very little\neffect on models’ predictive power. Context mat-\nters, but not all features of context matter equally;\nas discussed in Section 5, these results motivate\nfuture language modeling research focused on al-\nternative context representations rather than simply\nmore tokens.\n2 Approach\nA language model (LM) places a probability distri-\nbution p(x) over discrete token sequences x. Most\nlearned LMs do so by decomposingp(x) according\nto the chain rule and modeling the conditional dis-\ntribution over a single target token given a (ﬁxed-\nor variable-length) context of previous tokens:\np(x) =\n∏\ni\np(xi |x0,x1,...,x i−1) . (1)\nIn transformer language models, this conditional\ndistribution is modeled via a sequence of alternat-\ning neural feed-forward layers and self-attention\nlayers; see Vaswani et al. (2017) for more details.\nWhile input sequences x can in principle be\nmade arbitrarily long, there are both theoretical\nand practical limits to transformers’ ability to make\neffective use of it (Hahn, 2020; Wang et al., 2019).\nHere, we wish to understand when (and why) in-\ncreasing the size of the context improves model\npredictions.\nUsable information Consider a hypothetical\nLM context consisting of the tokens The user’s\npassword is. . .. This context suggests that subse-\nquent tokens will be a password: (hopefully!) a\nhigh-entropy sequence. Now suppose this context\nis extended to include earlier tokens, becoming\nThe user’s hashed password is ave$@To9!. The\nuser’s password is. . .. Information-theoretically,\nthis context is extremely informative: only a small\nnumber of passwords will hash to the given string,\nand a predictor capable of testing all passwords\nwould be able to identify the candidates and signif-\nicantly reduce its uncertainty about future tokens.\nBut in practice, this extra context is useless: no\nknown efﬁcient predictor can learn anything about\nthe password from its hash code, and the extra con-\ntext has not made the language modeling problem\nany easier. This is an extreme case, but a simi-\nlar intuition applies to more conventional questions\nabout language models. A newspaper article whose\nﬁrst sentence begins A dog bit a man is likely to\nend very differently from one that begins A man\nbit a dog. Can LMs reason effectively about this\ndistinction, or is it (like a hashed password) com-\nputationally inaccessible to current models?\nA framework for answering questions of this\nkind was introduced by Xu et al. (2020):\nDeﬁnition 1. The usable predictive information\n(formally, predictive V-information) from a ran-\ndom variable X to a random variable Y as:\nIV(X →Y) =\n[\ninf\np1∈V\n−E log p1(Y)\n]\n−\n[\ninf\np2∈V\n−E log p2(Y |X)\n]\n(2)\nfor a class Vof distributions p.\nIntuitively, this deﬁnition measures how much\nextra information about Y can be extracted from\nX by any predictor in V. In language modeling,\nwe will take Y to be the target word, X its con-\ntext, and Va class of parametric models. While\nthis deﬁnition generalizes Shannon mutual infor-\nmation (Shannon, 1948) and has deep connections\nto other information-theoretic quantities (see Xu\net al. 2020 for details) it ultimately corresponds to\na simple and common-sense evaluation: if we want\nto know how much the extra context Xhelps a lan-\nguage model, we should train a model p1 without\naccess to X, train a model p2 with access to X, and\ncompare the accuracy of their predictions.\nMeasuring what is used But the original ques-\ntion raised by the introduction was not just how\nmuch information is contributed by context. It is\nalready well-established that conditioning on long\ncontexts is helpful, with existing experiments on\nlong-range transformers effectively implementing\nthe measurement in Eq. (2). Instead, we want to\nknow what information in this context is actually\nused by models.\nAs a prototypical example, let us hypothesize\nthat more than ﬁve tokens away from the target,\nmodels are only able to extract usable information\nfrom nouns. (In our experiments in Section 3, this\n“long-range context” will be considerably longer\nthan 5 words.) For example, given the sentence:\nPierre Vinken, 61 years old, will join the board as\na nonexecutive director Nov. 29.\nwe hypothesize that the LM distributions:\np1(director |Pierre Vinken, 61 years old, will\njoin the board as a nonexecutive) (3)\n≈p2(director |Pierre Vinken years  \nnoun-only context\n,\nthe board as a nonexecutive  \nordinary context\n) , (4)\nand more generally that\nIV(X0:n →Xn)\n≈IV([nouns(X0:n−5),Xn−5:n] →Xn) (5)\nwhere Xi:j is the sequence of tokens\n[Xi,Xi+1,...,X j−1], V is a class of LMs,\nand nouns is a context ablation that extracts\nonly the nouns from a given string. That is, we\nhypothesize that the amount of usable information\ncontributed by the full context X0:n is the same\nas the amount contributed by the ablated context\n[nouns(X0:n−5),Xn−5:n], so ablation removes no\ninformation.\nThe experiments in this paper generalize this\nexperimental framework to other context ablations\nand hypotheses. Let f be an ablation and k an\ninteger offset, and denote an ablated context:\nfk(X) = [f(X0:n−k),Xn−k:n] (6)\nand an ablated negative log-likelihood:\nL(θ,f,k ) =−E log pθ(Xn |fk(X0:n)) (7)\nThen, we can measure the effect of each ablation f\non usable information via the following quantity:\nDeﬁnition 2. The ablated information due to an\nablation f at an offset kis:\nA(f,k) =\nIV(X0:n→Xn)−IV(fk(X0:n)→Xn)\nIV(X0:n→Xn)−IV(Xn−k:n→Xn)\n(8)\n=\ninfθL(θ,f,k)−infθ′L(θ′,n)\ninfθ′′L(θ′′,n−k)−infθ′L(θ′,n)\n, (9)\nwhere L(θ,i) is the (unablated) negative log-\nlikelihood −E log pθ(Xn |Xn−i:n).\nIntuitively, A(f,k) measures how much of the\nusable information added by an extra ktokens (the\ndenominator) is removed by applying the ablation\nf to those ktokens (the numerator). If it is close to\n0, almost no information is removed; if it is close\nto 1, almost all information is removed.\nTransformer LM\nPierre Vinken years   | will join the board   | as\na director\na\nNov\ndirector   |\nℓ ℓ+m ℓ+n\nablated context ordered context\nℒ( , ℓ: m∼n)nouns\nFigure 1: Calculation of the ablated likelihood\nL(nouns,ℓ : m ∼n) (Eq. (10)). A context ablation\nnouns (which deletes all non-noun words) is applied to\nthe ﬁrst ℓtokens of the context, and likelihood is com-\nputed on the last n−m(unablated) context tokens.\nEvaluation in practice Eq. (9) provides a gen-\neral framework for answering our core question in\nthis paper: for a diverse set of context ablations and\noffsets, we will measure how much information is\nlost when a given ablation is applied at a given off-\nset. A few modiﬁcations are required to turn this\nequation into a practical evaluation scheme:\nHeld-out evaluation: Eq. (7) involves an expec-\ntation over the sequence distributionp(X). In prac-\ntice, LMs must be trained on ﬁnite corpora, creat-\ning a risk of overﬁtting (Zhang et al., 2016). To\naddress this issue, we approximate the inﬁmum in\nEq. (7) by ﬁtting θ1 on a training set, and comput-\ning ablated information on a held-out validation\nset. All reported results are an average of held-out\nlikelihoods from two random initializations.\nBatching: Given a ﬁxed (training or test)\ndataset of strings Xand a maximum context size\nof m, Eq. (7) should be estimated empirically\nas −1\n|X|\n∑\nx\n1\n|x|\n∑|x|\ni=0 log p(Xi | fk(Xi−m:i)).\nThis requires re-computing model predictions\nonce for every token in the dataset. However,\nthe transformer models we use here support ef-\nﬁcient batch inference: training data is pre-\nsegmented into sequences of at most length n, and\n− 1\n|X|n\n∑\nx\n∑n\ni=0 log p(Xi |fk(X0:i)) can be com-\nputed in a single forward pass. This is considerably\nmore efﬁcient but means that most tokens are eval-\nuated with a context of length <n. As a compro-\nmise to ensure that evaluations contain long-range\ncontext, we accumulate losses on a subset:\nL(θ,f,ℓ : m∼n) =− 1\n|X|(n−m)\n∑\nx\nℓ+n∑\ni=ℓ+m\nlog pθ(Xi |[f(X0:ℓ),Xℓ:i])\n(10)\n(visualized in Fig. 1). This can be read as “ℓtokens\nof f-ablated context, followed by mto ntokens\nof unablated context”. We will write L(θ,m ∼n)\nwhen only unablated context is used. Because of\nthe large number of experiments in this paper, we\nuse Eq. (10) for all training and evaluation.\nModel, data and training details For all exper-\niments, our LM uses the GPT-2 model architec-\nture (Radford et al., 2019) in the implementation\nof Wolf et al. (2020) with default hyperparame-\nters. All models are trained from scratch on the\nWikiText-103 dataset (Merity et al., 2016), an En-\nglish language modeling benchmark. Aside from\nablations, no preprocessing is applied. A spe-\ncial separator token is inserted between ablated\nand unablated context. The training set contains\n103,221,021 words, while the evaluation set con-\ntains 217,646 words.\nA note on evaluation As in past work on eval-\nuating language models (Brown et al., 1992), our\nevaluation of relative predictive information ulti-\nmately bottoms out in a conditional entropy (log-\nperplexity). Recent work has shown that other met-\nrics, such as diversity of outputs, are important for\nevaluating the quality of LMs as models for lan-\nguage generation (Hashimoto et al., 2019; Caccia\net al., 2020). Generation also depends on a number\nof other factors, such as choice of decoding proce-\ndure (Caglayan et al., 2020). Here, we focus on\nLMs as predictive models, measuring their ability\nto place an accurate distribution over future words\nand sentences, rather than their ability to gener-\nate useful or coherent text (see Appendix C). We\nwant to emphasize that these results below apply to\nlanguage models speciﬁcally, and not transformers\napplied to NLP tasks in general—the same analysis\nmight give very different conclusions if applied to,\ne.g., question answering or summarization.\n3 Experiments\nIn this section, we attempt to determine what in-\nformation in transformer LM contexts is usable\nby measuring ablated information (Eq. (9)). Sec-\ntions 3.1 and 3.2 describe our main results, with\nSection 3.1 focused on ordering and Section 3.2\nfocused on lexical information. Section 3.3 com-\npares these results to ablations applied at evaluation\ntime. Section 3.4 explores whether contexts can be\nfurther manipulated to improve model predictions.\n3.1 Does order matter?\nIn this section we will examine the effects of differ-\nent augmentations to the order within long-range\ncontext. We ﬁrst train a no information model to\nminimize L(θ,0 ∼512) and a full information\nmodel to minimize L(θ,512 ∼1024). For each\ncontext ablation f, we train a model to minimize\nL(θ,f, 512 : 0∼512). Each ablation has access\nto more information than the no information model\n(because it conditions on extra tokens) and less\ninformation than the full information model (be-\ncause an ablation has been applied to those tokens).\nNote that the LM operates on BPE-derived sub-\nword tokens for consistency with the way GPT-2 is\ntypically used, but all ablations are deﬁned at the\nword level, meaning, e.g., that we shufﬂe words\nrather than tokens.\nWe use these trained models to calculate ab-\nlated information (Eq. (9)). To explore the ef-\nfect of different context lengths, we stratify eval-\nuation of the ablated information into two con-\nditions: a mid-range condition in which likeli-\nhoods in Eq. (9) are of the form L(·,f, 512 : 0∼\n256), and a long-range condition with likelihoods\nL(·,f, 512 : 256 ∼ 512). (We call the former\n“mid-range” rather than “short-range” because most\ntokens are still predicted with signiﬁcant unab-\nlated context; our experiments do not character-\nize sentence-internal modeling of syntactic well-\nformedness.) Results are shown in Figure 2 and\ndiscussed below.\nOverall word order\nshufﬂe all\n61 N.V ., director the of Mr. Vinken Dutch group. as\nnonexecutive the 29. is Vinken, years Elsevier join old,\npublishing a Nov. will Pierre board chairman\nshuf. trigrams globally\npublishing group. N.V ., the Dutch Mr. Vinken is join the\nboard as a nonexecutive years old, will chairman of\nElsevier Pierre Vinken, 61 director Nov. 29.\nIn the shufﬂe all ablation, f shufﬂes words uni-\nformly at random, forcing the model to treat ablated\ncontext as a bag of words. In the shuf. trigrams\nglobally ablation, the context is divided up into non-\noverlapping trigrams, the order of which is then\npermuted uniformly at random. Shufﬂing all words\nremoves 41% of usable information in the mid-\nrange condition and 84% in the long-range condi-\ntion: ordering information is important even very\nfar from the target. On the other hand, shufﬂing\nall trigrams removes 31% of usable information in\nthe mid-range condition and 50% in the long-range\ncondition: local co-occurrence statistics carry a\nsigniﬁcant amount of usable information.\n4.20 4.25 4.30 4.35 4.40 4.45\nbits\nfull information\nshuf. within trigrams\nshuf. trigrams within sent.\nsent. shuf.\nshuf. within sent.\nshuf. trigrams globally\nshuffle all\nreplace w/ old\nno information\n4.19 (0%)\n+0.04 (14%)\n+0.04 (16%)\n+0.04 (17%)\n+0.07 (26%)\n+0.08 (31%)\n+0.10 (41%)\n+0.14 (55%)\n4.45 (100%)\n(a) Mid-range condition (ﬁrst 256 tokens after ablation)\n4.17 4.18 4.19 4.20 4.21 4.22 4.23\nbits\nfull information\nsent. shuf.\nshuf. trigrams within sent.\nshuf. within trigrams\nshuf. trigrams globally\nshuf. within sent.\nreplace w/ old\nshuffle all\nno information\n4.17 (0%)\n+0.01 (14%)\n+0.02 (35%)\n+0.02 (41%)\n+0.02 (50%)\n+0.03 (55%)\n+0.03 (69%)\n+0.04 (84%)\n4.22 (100%)\n(b) Long-range condition (tokens 256-512 after ablation)\nFigure 2: Effect of word order on usable information.\nBar labels show “change in ablated likelihood (ablated\ninformation)”. The x axis shows ablated likelihood.\nError bars represent 95% conﬁdence intervals. Word-\norder changes that preserve local ordering remove only\na small amount of information, while shufﬂing or re-\nplacement with thematically similar text remove more.\nWord order within sentences\nshuf. within sent.\n61 director as the old, join will a Nov. board nonexecutive\nyears Vinken, 29. Pierre is publishing the Vinken N.V ., Mr.\ngroup. chairman Elsevier of Dutch\nshuf. within trigrams\nVinken, Pierre 61 will old, years the board join a\nnonexecutive as Nov. director 29. Mr. Vinken is of\nElsevier chairman the Dutch N.V ., group. publishing\nshuf. trigrams within sent.\nyears old, will as a nonexecutive join the board Pierre\nVinken, 61 director Nov. 29. N.V ., the Dutch chairman of\nElsevier Mr. Vinken is publishing group.\nWords are shufﬂed only within sentences according\nto one of three procedures: (1) a uniform random\npermutation of all the words in the sentence (shuf.\nwithin sent.), (2) a uniform random permutation\nof the words within each non-overlapping trigram\nin the sentence ( shuf. within trigrams), and (3) a\nuniform random permutation of the order of the\ntrigrams within the sentence (shuf. trigrams within\nsent.). (1) and (2) were also recently explored\nby Pham et al. (2020) in models for entailment,\nand more complex shufﬂing procedures have been\nexplored in neuroscience contexts (Mollica et al.,\n2020). Here, (2) and (3) are chosen because they\npreserve local co-occurrence statistics ((3) more\nthan (2)), while (2) also preserves the general lin-\near information ﬂow of the sentence.\nNotably, the shuf. within trigrams (14% and\n41%) and the shuf. trigrams within sent. (16% and\n35%) ablations both remove relatively little usable\ninformation in both the mid- and long-range condi-\ntions. Usable information is decreased only slightly\nby ablations that preserve local co-occurrence\nstatistics and/or linear information ﬂow . (This\nincludes transformations like man bites dog →dog\nbites man with signiﬁcant effects on semantics!) In\nthe long-range condition, uniform shufﬂing within\nsentences produces a larger effect, removing 55%\nof usable information.\nSentence order\nshuf. sent.\nMr. Vinken is chairman of Elsevier N.V ., the Dutch\npublishing group. Pierre Vinken, 61 years old, will join\nthe board as a nonexecutive director Nov. 29.\nNext, sentences are shufﬂed within the context\nwhile their internal word order is unchanged. In\nthe mid-range condition, this produces results com-\nparable to the trigram shufﬂing experiments above\n(removing 17% of usable information); in the long-\nrange condition, it has an even smaller effect (14%).\nTogether with the previous experiment these results\nsuggest that prediction accuracy depends on infor-\nmation about local word co-occurrence, but not\nﬁne-grained word order or global position.\nOrder of entire sections\nreplace w/ old\nRudolph Agnew, 55 years old and former chairman of\nConsolidated Gold Fields PLC, was named a\nnonexecutive director of this British industrial\nconglomerate.\nA possible hypothesis about LM behavior is that the\nmain function of long-range context is to provide\nmore information about the general topic of the\ndocument, including clues about vocabulary and\nstyle. To test this, the ablation replaces its entire\ninput with the 512 tokens that immediately precede\nit in the source document (which in general will\nbe topically similar). This transformation removes\nsigniﬁcant information in both mid- and long-range\nconditions (55% and 69%). Long-range context is\n4.20 4.25 4.30 4.35 4.40 4.45\nbits\nfull information\ncont. words\nN&VB&ADJ\nN&VB\nN\ncommon\nnamed entities\nrare\nfunc. words\nno information\n4.19 (0%)\n+0.02 (9%)\n+0.03 (11%)\n+0.03 (13%)\n+0.05 (20%)\n+0.10 (38%)\n+0.10 (39%)\n+0.15 (58%)\n+0.18 (69%)\n4.45 (100%)\n(a) Mid-range condition (ﬁrst 256 tokens after context)\n4.15 4.16 4.17 4.18 4.19 4.20 4.21 4.22 4.23\nbits\ncont. words\nN&VB&ADJ\nN&VB\nN\nfull information\nnamed entities\ncommon\nrare\nfunc. words\nno information\n+-0.01 (-31%)\n+-0.01 (-29%)\n+-0.01 (-22%)\n+-0.00 (-9%)\n4.17 (0%)\n+0.01 (31%)\n+0.02 (33%)\n+0.03 (73%)\n+0.04 (89%)\n4.22 (100%)\n(b) Long-range condition (tokens 256-512 after context)\nFigure 3: Effect of word identity on usable informa-\ntion. Labels are as in Fig. 2. Several ablations, includ-\ning deletion of all words except nouns, preserve most\nusable information in the mid-range condition, and im-\nprove model accuracy in the in the long range.\nnot simply a source of topic information: earlier\ntext on the same theme is in some cases nearly as\nuninformative as no text at all.\n3.2 Do all words matter?\nOur next experiments focus on lexical rather than\nstructural information, using ablations that delete\nselected words from the context. Training and eval-\nuation setups are exactly as in Section 3.1. Here,\nunlike the previous section, ablations will generally\ncause the number of tokens in a given context to\ndecrease; in this case ablations also insert padding\ntokens to the beginning of the context window to\npreserve the original number of tokens. Results are\nshown in Fig. 3.\nParts of speech\nN\nPierre Vinken years board director Nov. Mr. Vinken\nchairman Elsevier N.V . publishing group\nN & VB\nPierre Vinken years will join board director Nov. Mr.\nVinken chairman Elsevier N.V . publishing group\nN & VB & ADJ\nPierre Vinken years old will join board nonexecutive\ndirector Nov. Mr. Vinken chairman Elsevier N.V . Dutch\npublishing group\ncont. words (N & VB & ADJ & ADV)\nPierre Vinken years old will join board nonexecutive\ndirector Nov. Mr. Vinken chairman Elsevier N.V . Dutch\npublishing group\nfunc. words\n, 61 , the as a 29 . is of , the .\nAs in the initial example from Section 2, we\nretain only words whose part of speech tag is in a\ngiven set. We use the spaCy model (Honnibal et al.,\n2020) for part-of-speech tagging, and examine ﬁve\nsets: (1) nouns only, (2) nouns and verbs, (3) nouns,\nverbs, and adjectives , (4) content words (nouns,\nverbs, adjectives, and adverbs), and (5) function\nwords (all words except nouns, verbs, adjectives,\nand adverbs).\nIn the mid-range condition, deleting all words\nbut nouns removes only 20% of usable informa-\ntion; deleting all but nouns and verbs removes only\n13%. Most usable information, even in mid-range\ncontext, appears to be captured by nouns and verbs.\nRetaining only function words causes a consider-\nably greater loss of information.\nIn the long-range condition, results are even\nmore striking: retaining only content words\nimproves predictions over the “full informa-\ntion” experiment. Like Shannon information, V-\ninformation is deﬁned to be non-negative (Xu et al.,\n2020), and the result in Fig. 3 is a consequence\nof our ﬁnite-sample approximation based on held-\nout likelihood. The effect is robust across multiple\ntraining runs from random initializations. As there\nis a signiﬁcant gap between the training and vali-\ndation perplexity of our model (roughly 11%), we\nhypothesize that this change occurs because the\nablation preserves semantic content while reducing\nthe original model’s ability to overﬁt. We believe\nthis is an important subject for future investigation.\nNamed entities\nnamed entities\nPierre Vinken 61 years old Nov. 29 Vinken Elsevier N.V .\nDutch\nAs an alternative to the topic hypothesis evaluated\nunder “Order of entire sections” above, we might\nhypothesize that long-range contexts are useful be-\ncause they provide a reservoir of named entities\nlikely to be referred to again. Here, the ablation\nretains only spans tagged as named entities or quan-\ntities by spaCy. While signiﬁcantly worse than the\nnoun ablation discussed above, retaining only en-\ntities results removes only about a third of usable\ninformation in both conditions (39% and 31%).\nWord frequency\ncommon\nPierre years old join board director . Mr. chairman\nDutch publishing group .\nrare\nVinken nonexecutive Nov. Vinken Elsevier N.V .\nAnother natural question is whether rare words or\nfrequent words are more important: information\nabout frequent context words might help models\nestimate ﬁne-grained document-level frequencies\nof those words account for most of the terms in\nEq. (7); rare words are likely to be more informa-\ntive about the content of the document itself.\nWe partition the vocabulary into a set of rare\nwords, corresponding to the least frequent ∼98%\nof word types and 20% of word tokens, and fre-\nquent words, the most frequent ∼2% of types and\n80% of tokens. Both ablations remove a signiﬁcant\namount of information relative to the POS-based\nablations above, but retaining only frequent words\nimproves perplexity relative to rare words in both\nthe mid- and long-range conditions.\nAppendix B presents versions of these experi-\nments trained and evaluated on even longer con-\ntexts. Conclusions are largely the same as above.\n3.3 Evaluating on augmented data\nWe motivated the use ofV-information in Section 2\nby arguing that it more clearly distinguished be-\ntween prediction errors attributable to loss of in-\nformation and prediction errors attributable to mal-\nformed and out-of-distribution model inputs. To\nput our results in context, we repeat several of the\nprevious experiments in the evaluation paradigm\nof Khandelwal et al. (2018), which is designed\nto measure test-time sensitivity rather than usable\ninformation.\nWe train a new model to minimize L(θ,512 ∼\n1024) while randomly truncating the ﬁrst 512 con-\ntext tokens and replacing them with padding tokens\n(to ensure that the model has seen padding tokens\nat training time). We then evaluate this model on\n4.20 4.25 4.30 4.35 4.40 4.45 4.50 4.55\nbits\nfull information\nshuf. within trigrams\nshuf. trigrams within sent.\nsent. shuf.\nshuf. within sent.\ncont. words\nN&VB&ADJ\nshuf. trigrams globally\nN&VB\nN\ncommon\nnamed entities\nshuffle all\nrare\nreplace w/ old\nno information\nfunc. words\n4.18\n+0.06\n+0.08\n+0.10\n+0.14\n+0.16\n+0.16\n+0.16\n+0.16\n+0.18\n+0.20\n+0.20\n+0.22\n+0.24\n+0.25\n4.48\n+0.36\n(a) Mid-range condition (ﬁrst 256 tokens after ablation)\n4.14 4.16 4.18 4.20 4.22\nbits\nfull information\nsent. shuf.\nshuf. trigrams within sent.\nshuf. within trigrams\nshuf. trigrams globally\nreplace w/ old\nN&VB&ADJ\ncont. words\nN&VB\nN\nnamed entities\nshuf. within sent.\nrare\nshuffle all\ncommon\nno information\nfunc. words\n4.15\n+0.00\n+0.02\n+0.03\n+0.03\n+0.03\n+0.03\n+0.03\n+0.03\n+0.03\n+0.03\n+0.04\n+0.04\n+0.04\n+0.05\n4.20\n+0.07\n(b) Long-range condition (tokens 256-512 after ablation)\nFigure 4: Loss of information resulting from ablations\nat evaluation time only. x-axis and labels show ablated\nnegative log-likelihoods. Some locality-preserving ab-\nlations (high PMI, shuf. sent.) have a small effect, but\nmost affect likelihood signiﬁcantly (including lexical\nablations that do not remove usable information).\nthe set of ablations shown in Section 3.1 and Sec-\ntion 3.2. For the full information model in Fig. 4,\nwe evaluate on ordered context windows with no\npadding tokens; for the no information model, we\nevaluate on context windows in which the ﬁrst 512\ntokens are all padding tokens.\nIn the mid-range condition, the least destructive\nablations are shufﬂing within trigrams and shufﬂing\nthe order of trigrams within sentences: models ap-\npear to be reasonably robust to this kind of data\ntransformation without speciﬁc training on it. Im-\nportantly, lexical ablation experiments have a large\nimpact in this evaluation, underlining the extent to\nwhich the two experimental paradigms characterize\ndifferent aspects of model behavior. Figure 5 in\nAppendix A shows a side-by-side comparison of\nthese experiments and the ones in Sections 3.1–3.2.\n3.4 Making better language models?\nThe lexical ablation experiments in Section 3.2 in-\ndicated that model accuracy could be improved by\nselective deletion of context words. Can this ef-\nfect be exploited to further improve models? As\na simple experiment, we attempted to replace all\npadding tokens in the nouns+verbs ablation of Sec-\ntion 3.2 with nouns and verbs from further back\nin the context—effectively providing the model\nwith an even longer-range view of an informative\ncontext representation.\nThis experiment slightly increased usable infor-\nmation in the mid-range condition (0.2%), but de-\ncreased it in the long range-range condition (0.6%).\nLonger contexts, even of a kind previously found\nto be informative, did not provide additional us-\nable information. These results are consistent with\nour earlier hypothesis that the previously observed\neffect resulted from a reduction in overﬁtting—if\nremoving information increased performance by re-\nducing overﬁtting, then it is reasonable that adding\ninformation back results in more overﬁtting.\n4 Related Work\nContext in count-based and discriminative\nLMs The earliest learned LMs were count-based\n(e.g., Kneser and Ney, 1995): they estimated\np(xn |x0:n) based on a (smoothed) empirical n-\ngram frequency #(x0:n)/#(x0:n−1) (where #(x)\nis the number of times the sequence xappears in\ntraining data). As the number of distinct n-gram\ncounts grows exponentially in n, it was typically\nset to a small value. Count-based models have a\nclear dependence on context: any token within the\nlast nwords that also appears in a training n-gram\nis relevant, anything further back is not.\nSubsequent models improved on these by allow-\ning the use of skip-grams, caches, and feature-\nbased models (Goodman, 2001; Bengio et al.,\n2003). Some of these in principle allowed the use\nof unlimited-length contexts, but only by imposing\nstrong restrictions on the ways in which context\nfeatures could interact.\nContext in RNN LMs Recurrent neural network\nlanguage models (Mikolov et al., 2010; Elman,\n1990) provide a more expressive mechanism for\nthe use of long-range context: models write to a\nrecurrent “state vector” which can be carried ar-\nbitrarily far into the future. Computational issues\nlimit the effective context size such models can\nbe practically trained on, but this size is still sig-\nniﬁcantly greater the models mentioned above: as\npreviously noted, Khandelwal et al. (2018) revealed\ninﬂuence from up to 200 tokens of context. Similar\neffects are reported by Sankar et al. (2019) for neu-\nral dialogue models, and Li et al. (2016) describe\nan alternative procedure for ablating contexts.\nContext in Transformer LMs Transformers in-\ntroduce yet another mechanism for extracting infor-\nmation from long-range context: attention. Atten-\ntion is also used with RNNs, but typically with just\na single head—the hidden state still carries most\nof the information. In transformers, context enters\ninto predictions primarily via unbounded random\naccess. These models appear to beneﬁt from signif-\nicantly longer contexts than previous models.\nSome recent work that investigates the behavior\nof individual transformer attention heads (Clark\net al., 2019; V oita et al., 2019). This work ﬁnds that\ncertain attention heads are sensitive to things like\nword frequency, positional information, and certain\nsyntactic phenomena. While extremely informative\nabout the computational structures implemented by\nﬁxed models, these approaches do not necessarily\nreveal anything about usable information: indeed,\npatterns of attention do not necessarily correlate\nwith model predictions (Jain and Wallace, 2019).\nOther related work Our ﬁnding that ﬁne-\ngrained ordering information contributes little us-\nable information is consistent with Rae et al.\n(2019)’s ﬁnding that long-range contexts could\nbe informatively summarized in ﬁxed-sized vec-\ntors; our ﬁnding that most usable information is\ncarried by nouns is consistent with earlier ﬁnd-\nings about both specialized neural architectures\n(Henaff et al., 2016) and discourse representa-\ntions in feature-based models (Barzilay and La-\npata, 2008). Our approach also shares similar mo-\ntivations to information-theoretic work on prob-\ning (V oita and Titov, 2020; Pimentel et al., 2020),\nwhich uses related tools to interpret linguistic struc-\nture in LM representations rather than characteriz-\ning their effect on LM predictions. Several recent\npapers have explored the effect of training-time and\ntest-time ablations in models for other data analysis\ntasks: Pham et al. (2020) ﬁnd that shufﬂing exper-\niments have a limited effect on the accuracy of\nmodels for natural language inference, while Perez\net al. (2021) describe several experiments aimed at\nintroducing usable information for several question\nanswering and sentence understanding tasks.\n5 Discussion\nWe have investigated the extent to which trans-\nformer models can use structural and lexical infor-\nmation in long-range contexts for English language\nmodeling. Experiments demonstrated that this in-\nformation is primarily contained in content words\nand local ordering statistics: ablations that remove\nother kinds of information from context have little\neffect on models’ predictive accuracies. In contrast,\nretaining only information about document identity\nor named entities causes signiﬁcant drops in pre-\ndictive accuracy: the effectiveness of long contexts\nis not explained by the presence of topic or named\nentity information alone.\nCrucial to obtaining these results was a mea-\nsure of ablated usable information grounded in the\naccuracy of models trained and tested on ablated\ncontexts. Past work on context in LMs has pri-\nmarily measured the inﬂuence of evaluation-time\nablations. Sometimes these two notions of context-\nsensitivity coincide (e.g., trigram shufﬂing) and\nsometimes they do not (e.g., removal of lexical in-\nformation). Our results also offer a jumping-off\npoint for future modeling work. They motivate\nmore efﬁcient, compressed context representations\nthat better preserve the information that is usable by\ncurrent models. They motivate more accurate mod-\nels by developing new context representations that\nmake currently unusable information more promi-\nnent.\nSeveral questions remain unanswered by our\nexperiments. Do ablations affect the quality of\ntext generated by models? (In particular, does\nthe usable information added by long contexts im-\nprove predictability of syntax, semantics, or simply\ndocument-level word frequency statistics?) More\nfundamentally, do observations about usable infor-\nmation reﬂect limitations of transformers or fun-\ndamental, (Shannon-)information-theoretic proper-\nties of English? Our results suggest that at least\nsome of these effects are model-speciﬁc: delet-\ning function words cannot add information, but\nimproves held-out model accuracy. A complete\nanswer to this question will require more detailed\nexploration, including a better understanding of\nhuman predictions in comparable settings.\nAcknowledgments\nThanks to Carina Kauf and Greta Tuckute, Evelina\nFedorenko and Roger Levy for valuable discus-\nsions. We acknowledge the MIT SuperCloud and\nLincoln Laboratory Supercomputing Center for\nproviding HPC resources that contributed to the\nresults reported within this paper.\nImpact Statement\nAcross initial exploration, evaluation conditions\nand training runs, experiments in this paper re-\nquired roughly 100 training runs on the WikiText-\n103 dataset. As discussed in Section 2, model size\nand batched evaluation were both used to mini-\nmize the energy demands of these experiments; ex-\nperiments themselves were performed at the Mas-\nsachusetts Green HPC center, a carbon-neutral su-\npercomputing facility. Ultimately, results in Sec-\ntion 3 provide guidance toward the design of mod-\nels that use context more efﬁciently and motivate\nthe large-scale empirical study conducted here.\nReferences\nRegina Barzilay and Mirella Lapata. 2008. Modeling\nlocal coherence: An entity-based approach. Compu-\ntational Linguistics, 34(1):1–34.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Janvin. 2003. A neural probabilistic lan-\nguage model. The Journal of Machine Learning Re-\nsearch, 3:1137–1155.\nPeter F Brown, Stephen A Della Pietra, Vincent J\nDella Pietra, Jennifer C Lai, and Robert L Mercer.\n1992. An estimate of an upper bound for the entropy\nof english. Computational Linguistics, 18(1):31–40.\nRalf D Brown. 2011. The CMU-EBMT machine trans-\nlation system. Machine translation, 25(2):179.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin.\n2020. Language GANs falling short. In Interna-\ntional Conference on Learning Representations.\nOzan Caglayan, Pranava Madhyastha, and Lucia Spe-\ncia. 2020. Curious case of language generation\nevaluation metrics: A cautionary tale. In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 2322–2328, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. URL\nhttps://openai.com/blog/sparse-transformers.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJeffrey L Elman. 1990. Finding structure in time. Cog-\nnitive science, 14(2):179–211.\nJoshua T Goodman. 2001. A bit of progress in lan-\nguage modeling. Computer Speech & Language ,\n15(4):403–434.\nMichael Hahn. 2020. Theoretical limitations of self-\nattention in neural sequence models. Transactions\nof the Association for Computational Linguistics ,\n8:156–171.\nTatsunori Hashimoto, Hugh Zhang, and Percy Liang.\n2019. Unifying human and statistical evaluation for\nnatural language generation. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 1689–1701, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine\nBordes, and Yann LeCun. 2016. Tracking the world\nstate with recurrent entity networks. In ICLR.\nMatthew Honnibal, Ines Montani, Soﬁe Van Lan-\ndeghem, and Adriane Boyd. 2020. spaCy:\nIndustrial-strength Natural Language Processing in\nPython.\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot explanation. In NAACL-HLT.\nUrvashi Khandelwal, He He, Peng Qi, and Dan Juraf-\nsky. 2018. Sharp nearby, fuzzy far away: How neu-\nral language models use context. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 284–294, Melbourne, Australia. Association\nfor Computational Linguistics.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In1995\nInternational Conference on Acoustics, Speech, and\nSignal Processing, volume 1, pages 181–184. IEEE.\nJiwei Li, Will Monroe, and Dan Jurafsky. 2016. Un-\nderstanding neural networks through representation\nerasure. arXiv preprint arXiv:1612.08220.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2016. Pointer sentinel mixture mod-\nels. CoRR, abs/1609.07843.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Eleventh\nannual conference of the International Speech Com-\nmunication Association.\nF. Mollica, Matthew Siegelman, Evgeniia Diachek,\nS. Piantadosi, Zachary Mineroff, Richard Futrell,\nHope H. Kean, Peng Qian, and E. Fedorenko. 2020.\nComposition is the core driver of the language-\nselective network. Neurobiology of Language ,\n1:104–134.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nRissanen data analysis: Examining dataset char-\nacteristics via description length. arXiv preprint\narXiv:2103.03872.\nThang M Pham, Trung Bui, Long Mai, and Anh\nNguyen. 2020. Out of order: How important is\nthe sequential order of words in a sentence in nat-\nural language understanding tasks? arXiv preprint\narXiv:2012.15180.\nTiago Pimentel, Josef Valvoda, Rowan Hall Maudslay,\nRan Zmigrod, Adina Williams, and Ryan Cotterell.\n2020. Information-theoretic probing for linguistic\nstructure. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4609–4622, Online. Association for Computa-\ntional Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJack W. Rae, Anna Potapenko, Siddhant M. Jayakumar,\nand Timothy P. Lillicrap. 2019. Compressive trans-\nformers for long-range sequence modelling.\nChinnadhurai Sankar, Sandeep Subramanian, Christo-\npher Pal, Sarath Chandar, and Yoshua Bengio. 2019.\nDo neural dialog systems use the conversation his-\ntory effectively? an empirical study. arXiv preprint\narXiv:1906.01603.\nClaude E Shannon. 1948. A mathematical theory of\ncommunication. The Bell system technical journal ,\n27(3):379–423.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description length.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 183–196, Online. Association for Computa-\ntional Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia,\nAmanpreet Singh, Julian Michael, Felix Hill, Omer\nLevy, and Samuel R Bowman. 2019. Super-\nGLUE: A stickier benchmark for general-purpose\nlanguage understanding systems. arXiv preprint\narXiv:1905.00537.\nSinong Wang, Belinda Li, Madian Khabsa, Han\nFang, and Hao Ma. 2020. Linformer: Self-\nattention with linear complexity. arXiv preprint\narXiv:2006.04768.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander M. Rush. 2020.\nTransformers: State-of-the-art natural language pro-\ncessing. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nYilun Xu, Shengjia Zhao, Jiaming Song, Russell Stew-\nart, and Stefano Ermon. 2020. A theory of usable in-\nformation under computational constraints. In Inter-\nnational Conference on Learning Representations.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Ben-\njamin Recht, and Oriol Vinyals. 2016. Understand-\ning deep learning requires rethinking generalization.\narXiv preprint arXiv:1611.03530.\nA Comparison of Experimental\nParadigms\nIn Figure 5 we show the contrast between the ex-\nperimental paradigm of Sections 3.1–3.2 and that\nof Section 3.3. Especially for the experiments in-\nvolving parts of speech, we see a signiﬁcant differ-\nence in both the quantitative and qualitative results\nacross the two paradigms.\nB Longer Context Window\nHere we report the results of repeating the experi-\nments of Sections 3.1 and 3.2 with ablated contexts\n40\n 20\n 0 20 40 60 80 100 120 140\nnormalized accuracy (train+eval ablation)\n40\n20\n0\n20\n40\n60\n80\n100\n120\n140normalized accuracy (eval ablation)\nfull information\ncont. words\nN&VB&ADJ\nN&VB\nshuf. within trigrams\nshuf. trigrams within sent.\nsent. shuf.\nN\nshuf. within sent.\nshuf. trigrams globally\ncommon named entities\nshuffle all\nreplace w/ old\nrare\nfunc. words\nno information\n(a) Mid-range condition (ﬁrst 256 tokens after ablation)\n40\n 20\n 0 20 40 60 80 100 120 140\nnormalized accuracy (train+eval ablation)\n40\n20\n0\n20\n40\n60\n80\n100\n120\n140normalized accuracy (eval ablation)\ncont. wordsN&VB&ADJ\nN&VB N\nfull information\nsent. shuf.\nnamed entities\ncommon\nshuf. trigrams within sent.\nshuf. within trigrams\nshuf. trigrams globally\nshuf. within sent.\nreplace w/ old\nrare shuffle all\nfunc. words\nno information\n(b) Long-range condition (tokens 256-512 after ablation)\nFigure 5: Comparison of model performance in the\ntrain+eval and eval-only settings. The units represent\nthe percentage of the gap between the full information\nand no information models/contexts. That way, if a\npoint falls on the dotted y = xline, then that ablation\nhas the same relative effect in each paradigm. If a point\nfalls above the dotted line, then that ablation leads to\nbetter relative performance in the train+eval paradigm,\nand if a point falls below the dotted line, then that abla-\ntion leads to better relative performance in the eval-only\nparadigm.\nof size 1024 tokens instead of 512 tokens in or-\nder to verify that the behavior we observed is not\nspeciﬁc to the size of context window we chose.\n4.20 4.25 4.30 4.35 4.40 4.45\nbits\nfull information\nsent. shuf.\nshuf. within sent.\nshuffle all\nreplace w/ old\nno information\n4.16 (0%)\n+0.06 (21%)\n+0.08 (29%)\n+0.14 (49%)\n+0.21 (73%)\n4.45 (100%)\n(a) Mid-range condition (ﬁrst 256 tokens after ablation)\n4.15 4.16 4.17 4.18 4.19 4.20 4.21 4.22 4.23\nbits\nfull information\nsent. shuf.\nshuf. within sent.\nshuffle all\nno information\nreplace w/ old\n4.16 (0%)\n+0.01 (24%)\n+0.04 (65%)\n+0.06 (88%)\n4.22 (100%)\n+0.06 (100%)\n(b) Long-range condition (tokens 256-512 after ablation)\nFigure 6: Effect of word order on usable information.\nBar labels show “change in ablated likelihood (ablated\ninformation)”. The xaxis shows ablated likelihood. Er-\nror bars represent 95% conﬁdence intervals. Ablated\ncontexts contain 1024 tokens, but results are consistent\nwith results on 512-token contexts.\nC Sample Generations\nThe purpose of this section is to verify that models\ntrained on ablated contexts can still generate text\nthat is comparable to text generated by a model\ntrained with full contextual information. We select\na prompt from a randomly chosen Wikipedia article\nin the WikiText-103 validation set; each model\ngenerates a sentence (after ﬁnishing the sentence in\nprogress) given the appropriately ablated version\nof the prompt. The prompt consists of 768 tokens,\nthe last 256 of which remain unchanged for all\nversions of the prompt, so that the ablations are in\nthe long range relative to the point of generation.\nThe prompt and generations are as follows:\nprompt:\nat two independent schools for boys:\nSussex House School, a day school in\nChelsea’s Cadogan Square, and the City\nof London School, a day school on the\nNorth Bank of the River Thames in Lon-\ndon’s ﬁnancial district (known as the City\n4.20 4.25 4.30 4.35 4.40 4.45\nbits\nfull information\nN&VB&ADJ\nN&VB\nN\nnamed entities\nfunc. words\nno information\n4.16 (0%)\n+0.04 (13%)\n+0.05 (17%)\n+0.07 (23%)\n+0.12 (41%)\n+0.22 (76%)\n4.45 (100%)\n(a) Mid-range condition (ﬁrst 256 tokens after ablation)\n4.16 4.18 4.20 4.22\nbits\nN&VB&ADJ\nfull information\nN&VB\nN\nnamed entities\nno information\nfunc. words\n+-0.00 (-6%)\n4.16 (0%)\n+0.00 (2%)\n+0.01 (15%)\n+0.03 (48%)\n4.22 (100%)\n+0.07 (114%)\n(b) Long-range condition (tokens 256-512 after ablation)\nFigure 7: Effect of word identity on usable informa-\ntion. Labels are as in Fig. 6. Ablated contexts contain\n1024 tokens, but results are consistent with results on\n512-token contexts.\nof London). Attending school became\ndifﬁcult for Radcliffe after the release of\nthe ﬁrst Harry Potter ﬁlm, with some fel-\nlow pupils becoming hostile, though he\nsays it was people just trying to ”have a\ncrack at the kid that plays Harry Potter”\nrather than jealousy.\nAs his acting career began to consume\nhis schedule, Radcliffe continued his ed-\nucation through on-set tutors. He admit-\nted he was not very good at school, con-\nsidering it useless and ﬁnding the work\n”really difﬁcult.” He achieved A grades\nin the three AS-level exams that he took\nin 2006, but decided to take a break from\neducation and did not go to college or\nuniversity. Part of his reasoning was that\nhe already knew he wanted to act and\nwrite, and that it would be difﬁcult to\nhave a normal college experience. ”The\npaparazzi, they’d love it,” he told Details\nmagazine in 2007. ”If there were any\nparties going on, they’d be tipped off as\nto where they were.”\n= = Career = =\n= = = Harry Potter = = =\nIn 2000, producer David Heyman asked\nRadcliffe to audition for the role of Harry\nPotter for the ﬁlm adaptation of Harry\nPotter and the Philosopher’s Stone, the\nbest-selling book by British author J.K.\nRowling. Rowling had been searching\nfor an unknown British actor to person-\nify the character, and the movie’s director\nChris Columbus recalled thinking, ”This\nis what I want. This is Harry Potter”, af-\nter he saw a video of the young actor in\nDavid Copperﬁeld. Eight months later,\nand after several auditions, Radcliffe was\nselected to play the part. Rowling also\nendorsed the selection saying, ”I don’t\nthink Chris Columbus could have found\na better Harry.” Radcliffe’s parents origi-\nnally turned down the offer, as they had\nbeen told that it would involve six ﬁlms\nshot in Los Angeles. Warner Bros. in-\nstead offered Radcliffe a two-movie con-\ntract with shooting in the UK; Radcliffe\nwas unsure at the time if he would do any\nmore than that.\nThe release of Harry Potter and the\nPhilosopher’s Stone (released as Harry\nPotter and the Sorcerer’s Stone in the\nUnited States) took place in 2001. Rad-\ncliffe received a seven ﬁgure salary for\nthe lead role, but asserted that the fee\nwas ”not that important” to him; his par-\nents chose to invest the money for him.\nThe ﬁlm was highly popular and was met\nwith positive reviews, and critics took no-\ntice of Radcliffe: ”Radcliffe is the em-\nbodiment of every reader’s imagination.\nIt is wonderful to see a young hero who\nis so scholarly looking and ﬁlled with cu-\nriosity and who connects with very real\nemotions, from solemn intelligence and\nthe delight of discovery to deep family\nlonging, ”wrote Bob Graham of the San\nFrancisco Chronicle.\nA year later Radcliffe starred in Harry\nPotter and the Chamber of Secrets, the\nsecond installment of the series. Review-\ners were positive about the lead actors’\nperformances but had polarised opinions\non the movie as a whole. The 2004 re-\nlease Harry Potter and the Prisoner of\nAzkaban was the third ﬁlm in the series.\nRadcliffe ’s performance was panned by\nNew York Times journalist A.O. Scott,\nwho wrote that Watson had to carry him\nwith her performance. Next was Harry\nPotter and the Goblet of Fire in 2005.\nThe ﬁlm was the second-highest gross\nfull information:\nsing Potter ﬁlm in the series at the time\nof its release. After appearing in Harry\nPotter and the Philosopher’s Stone, Rad-\ncliffe tried to avoid trying to improve his\nrole.\nshufﬂe all:\nsing Potter ﬁlm in the series (behind\nHarry Potter and the Chamber of Se-\ncrets), the highest grossing Harry Potter\nﬁlm in the series. The following year,\nHarry Potter and the Goblet of Fire was\nthe top-grossing Potter ﬁlm, making it\nthe highest grossing Harry Potter ﬁlm in\nthe series.\nshuf. within sent.:\nsing Potter ﬁlm in the series (until 2009’s\nGold Glove), selling 1.5 million tickets\nin its ﬁrst week. The title of the ﬁlm’s\nsequel was also a major factor in the suc-\ncess of Philosopher, while its underper-\nformance dropped because of series 3’s\nSci-Fi Channel.\nshuf. within trigrams:\nsing Potter ﬁlm at the time of its re-\nlease. The ﬁlm had a ﬁnancial success\nworldwide and received ﬁve Academy\nAward nominations at the 56th Academy\nAwards.\nshuf. trigrams within sent.:\nsing Potter ﬁlm in the series at the United\nStates box ofﬁce. His appearance in\nHarry Potter and the Philosopher’s Stone\ndid not make it to the United States until\n2006.\nshuf. trigrams globally:\nsing Potter ﬁlm at the time and was the\nthird highest grossing ﬁlm of 2006, the\nhighest-grossing ﬁlm in the series. In\n2006, Radcliffe acted in his ﬁrst major\nrole in Harry Potter and the Deathly Hal-\nlows – Part I, where he played Harry’s\nChief Engineer, Harry Potter’s primary\nantagonist.\nsent. shuf.:\nsing Potter ﬁlm in the series at the time of\nits release. At the end of 2009, new back-\ners were looking for a leading role in the\nﬁlm, with Radcliffe starting the role of\nHarry Potter and the Philosopher’s Stone.\ncont. words:\nsing Harry Potter ﬁlm of all time after\nHarry Potter and the Goblet of Fire, the\nhighest-grossing ﬁlm of the 2007–2008\nﬁnancial year. The ﬁlm’s success meant\nthat he and his wife bought a four-\nbedroom apartment in the 1960s, “ready\nto keep them warm”, adding “They could\nnever make another effort to make some-\nthing exciting for people who were usu-\nally very aware of the housing shortage\nbut hadn’t been very conscious of that”.\nN & VB & ADJ:\nsing animated ﬁlm in the series after The\nLion King. A similar amount of praise\ncame in 2008 when The Magic King was\nreleased.\nN & VB:\nsing animated ﬁlm in the series. A sig-\nniﬁcant amount of critical reviews were\ngiven by publications in The Washington\nPost, San Francisco Chronicle, and The\nWashington Post. The ﬁlm was released\nin theaters in 2008.\nN:\nsing animated ﬁlm in the series.\n= Appearances = =\nnamed entities:\nsing animated ﬁlm in the series.\n= = Appearances extended Persons (1990\n– 2014) = =\nrare:\nsing animated ﬁlm in the series.\n= = Part two = =\ncommon:\nsing animated ﬁlm in the series. A review\nin The New York Times found that Hans\nwas not as strong as Watson but as well\nas Mr. Trough and Mr. Trough.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.7960571050643921
    },
    {
      "name": "Computer science",
      "score": 0.7760868072509766
    },
    {
      "name": "Language model",
      "score": 0.6721988916397095
    },
    {
      "name": "Natural language processing",
      "score": 0.6707643270492554
    },
    {
      "name": "USable",
      "score": 0.6623486876487732
    },
    {
      "name": "Transformer",
      "score": 0.6411290168762207
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5869277715682983
    },
    {
      "name": "Engineering",
      "score": 0.07245317101478577
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "World Wide Web",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ]
}