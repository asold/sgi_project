{
  "title": "InforMask: Unsupervised Informative Masking for Language Model Pretraining",
  "url": "https://openalex.org/W4385567255",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2945243773",
      "name": "Nafis Sadeq",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2970865618",
      "name": "Canwen Xu",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2041520510",
      "name": "Julian McAuley",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W3100353583",
    "https://openalex.org/W2057463094",
    "https://openalex.org/W3035153870",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4287646293",
    "https://openalex.org/W3090716330",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W4289259401",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W3105111366",
    "https://openalex.org/W3154449322",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W3151929433",
    "https://openalex.org/W3104415840",
    "https://openalex.org/W4386566638",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W2963323070"
  ],
  "abstract": "Masked language modeling is widely used for pretraining large language models for natural language understanding (NLU). However, random masking is suboptimal, allocating an equal masking rate for all tokens. In this paper, we propose InforMask, a new unsupervised masking strategy for training masked language models. InforMask exploits Pointwise Mutual Information (PMI) to select the most informative tokens to mask. We further propose two optimizations for InforMask to improve its efficiency. With a one-off preprocessing step, InforMask outperforms random masking and previously proposed masking strategies on the factual recall benchmark LAMA and the question answering benchmark SQuAD v1 and v2.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 5866–5878\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nInforMask: Unsupervised Informative Masking for\nLanguage Model Pretraining\nNafis Sadeq∗, Canwen Xu∗, Julian McAuley\nUniversity of California, San Diego\n{nsadeq,cxu,jmcauley}@ucsd.edu\nAbstract\nMasked language modeling is widely used for\npretraining large language models for natural\nlanguage understanding (NLU). However, ran-\ndom masking is suboptimal, allocating an equal\nmasking rate for all tokens. In this paper, we\npropose InforMask, a new unsupervised mask-\ning strategy for training masked language mod-\nels. InforMask exploits Pointwise Mutual In-\nformation (PMI) to select the most informa-\ntive tokens to mask. We further propose two\noptimizations for InforMask to improve its ef-\nficiency. With a one-off preprocessing step,\nInforMask outperforms random masking and\npreviously proposed masking strategies on the\nfactual recall benchmark LAMA and the ques-\ntion answering benchmark SQuAD v1 and v2.1\n1 Introduction\nMasked Language Modeling (MLM) is widely\nused for training language models (Devlin et al.,\n2019; Liu et al., 2019; Lewis et al., 2020; Raffel\net al., 2020). MLM randomly selects a portion\nof tokens from a text sample and replaces them\nwith a special mask token (e.g., [MASK]). However,\nrandom masking has a few drawbacks — it some-\ntimes produces masks that are too easy to guess,\nproviding a small loss that is inefficient for train-\ning; some randomly masked tokens can be guessed\nwith only local cues (Joshi et al., 2020); all tokens\nhave an identical probability to be masked, while\n(e.g.) named entities are more important and need\nspecial attention (Sun et al., 2019; Levine et al.,\n2021).\nIn this paper, we propose a new strategy for\nchoosing tokens to mask in text samples. We\naim to select words with the most information\nthat can benefit the language model, especially for\nknowledge-intense tasks. To tackle this challenge,\n∗Equal contribution.\n1The code and model checkpoints are available at https:\n//github.com/NafisSadeq/InforMask.\nwe propose InforMask, an unsupervised informa-\ntive masking strategy for language model pretrain-\ning. First, we introduce Informative Relevance,\na metric based on Pointwise Mutual Information\n(PMI, Fano, 1961) to measure the quality of a mask-\ning choice. Optimizing this measure ensures the\ninformativeness of the masked token while main-\ntaining a moderate difficulty for the model to pre-\ndict the masked tokens. This metric is based on the\nstatistical analysis of the corpus, which does not\nrequire any supervision or external resource.\nHowever, maximizing the total Informative Rel-\nevance of a text sample with multiple masks can\nbe computationally challenging. Thus, we propose\na sample-and-score algorithm to reduce the time\ncomplexity of masking and diversify the patterns\nin the output. An example is shown in Figure 1.\nFor training a language model with more epochs,\nwe can further accelerate the masking process by\nonly running the algorithm once as a preprocess-\ning step and assigning a token-specific masking\nrate for each token according to their masking fre-\nquency in the corpus, to approximate the masking\ndecisions of the sample-and-score algorithm. After\nthis one-off preprocessing step, masking can be as\nfast as the original random masking without any\nfurther overhead, which can be desirable for large-\nscale distributed language model training of many\nepochs.\nTo verify the effectiveness of our proposed\nmethod, we conduct extensive experiments on two\nknowledge-intense tasks — factual recall and ques-\ntion answering. On the factual recall benchmark\nLAMA (Petroni et al., 2019), InforMask outper-\nforms other masking strategies by a large margin.\nAlso, our base-size model, InformBERT, trained\nwith the same corpus and epochs as BERT (De-\nvlin et al., 2019) outperforms BERT-base on ques-\ntion answering benchmark SQuAD (Rajpurkar\net al., 2016, 2018). Notably, on the LAMA\nbenchmark, InformBERT outperforms BERT and\n5866\nThomas Edison was an inventor and businessman.\nThomas Edison was an inventor and businessman.\nThomas Edison was an inventor and businessman.\nThomas Edison was an inventor and businessman.\nThomas Edison was an inventor and businessman.\n[M] [M] 17.2\n[M] 21.9\n[M]\n[M]\n13.4\n[M] [M] 2.5\n✅\nInteresting and challenging!\nSteve Jobs? Ben Franklin?\nThe first mask is too easy!\nBoring! Too easy to guess!\n[M]\nFigure 1: The informative scores of randomly sampled masking candidates (s = 4). [M] denotes the masked tokens.\nThe pretraining objective of the masked language model (MLM) is to predict the masked tokens based on the\ncontext.\nRoBERTa (Liu et al., 2019) models that have 3×\nparameters and 10×corpus size.\nTo summarize, our contributions are as follows:\n• We propose InforMask, an informative mask-\ning strategy for language model pretraining\nthat does not require extra supervision or ex-\nternal resource.\n• We pretrain and release InformBERT, a base-\nsize English BERT model that substantially\noutperforms BERT and RoBERTa on the fac-\ntual recall benchmark LAMA despite having\nmuch fewer parameters and less training data.\nInformBERT also achieves competitive results\non the question answering datasets SQuAD\nv1 and v2.\n2 Related Work\nRandom Masking For pretraining Trans-\nformer (Vaswani et al., 2017) based language\nmodels such as BERT (Devlin et al., 2019), a\nportion of the tokens is randomly chosen to be\nmasked to set up the masked language model\n(MLM) objective. Prior studies have commonly\nused a masking rate of 15% (Devlin et al., 2019;\nJoshi et al., 2020; Levine et al., 2021; Sun et al.,\n2019; Lan et al., 2020; He et al., 2021), while\nsome recent studies argue that masking rate of 15%\nmay be a limitation (Clark et al., 2020) and the\npretraining process may benefit from increasing\nthe masking rate to 40% (Wettig et al., 2022).\nHowever, random masking is not an ideal choice\nfor learning factual and commonsense knowledge.\nWords that have high informative value may be\nmasked less frequently compared to (e.g.) stop\nwords, given their frequencies in the corpus.\nSpan Masking Although random masking is ef-\nfective for pretraining a language model, some\nprior works have attempted to optimize the mask-\ning procedure. Joshi et al. (2020) propose Span-\nBERT where they show improved performance on\ndownstream NLP tasks by masking a span of words\ninstead of individual tokens. They randomly select\nthe starting point of a span, then sample a span size\nfrom a geometric distribution and mask the selected\nspan. They continue to mask spans until the target\nmasking rate is met. This paper suggests mask-\ning spans instead of single words can prevent the\nmodel from predicting masked words by only look-\ning at local cues. However, this masking strategy\ninevitably reduces the modeling between the words\nin a span, etc., Mount-Fuji, Mona-Lisa, which may\nhinder its performance in knowledge-intense tasks.\nEntity-based Masking Baidu-ERNIE (Sun et al.,\n2019) introduces an informed masking strategy\nwhere a span containing named entities will be\nmasked. This approach shows improvement com-\npared to random masking but requires prior knowl-\nedge regarding named entities. Similarly, Guu\net al. (2020) propose Salient Span Masking where\na span corresponding to a unique entity will be\nmasked. They rely on an off-the-shelf named entity\nrecognition (NER) system to identify entity names.\nLUKE (Yamada et al., 2020) exploits an annotated\nentity corpus to explicitly mark out the named enti-\nties in the pretraining corpus, and masks non-entity\nwords and named entities separately.\nPMI Masking Levine et al. (2021) propose a\nmasking strategy based on Pointwise Mutual In-\nformation (PMI, Fano, 1961), where a span of up\nto five words can be masked based on the joint\nPMI of the span of words. PMI-Masking is an\nadaption of SpanBERT (Joshi et al., 2020) where\nmeaningful spans are masked instead of random\nones. However, PMI-Masking only considers cor-\nrelated spans and fails to focus on unigram named\n5867\nentities. This may lead to suboptimal performance\non knowledge intense tasks (details in Section 4.2).\nIn our proposed method, we exploit PMI to deter-\nmine the informative value of tokens to encourage\nmore efficient training and improve performance\non knowledge-intense tasks.\nKnowledge-Enhanced LMs KnowBERT (Pe-\nters et al., 2019) shows that factual recall perfor-\nmance in BERT can be improved significantly by\nembedding knowledge bases into additional layers\nof the model. Tsinghua-ERNIE (Zhang et al., 2019)\nexploits a similar approach that injects knowledge\ngraphs into the language model during pretraining.\nKEPLER (Wang et al., 2021) uses a knowledge\nbase to jointly optimizes the knowledge embedding\nloss and MLM loss on a general corpus, to improve\nthe knowledge capacity of the language model.\nSimilar ideas are also explored in K-BERT (Liu\net al., 2020) and CoLAKE (Sun et al., 2020). Coke-\nBERT (Su et al., 2021) demonstrates that incor-\nporating embeddings for dynamic knowledge con-\ntext can be more effective than incorporating static\nknowledge graphs. Other works have attempted to\nincorporate knowledge in the form of lexical rela-\ntion (Lauscher et al., 2020), word sense (Levine\net al., 2020), syntax (Bai et al., 2021), and parts-of-\nspeech (POS) tags (Ke et al., 2020). However, a\nhigh-quality knowledge base is expensive to con-\nstruct and not available for many languages. Differ-\nent from these methods, our method is fully unsu-\npervised and does not rely on any external resource.\n3 Methodology\nInforMask aims to make masking decisions more\n‘informative’. Since not all words are equally rich\nin information (Levine et al., 2021), we aim to\nautomatically identify more important tokens (e.g.,\nnamed entities) and increase their probability to\nbe masked while preserving the factual hints to\nrecover them. On the other hand, we would like\nto reduce the frequency of masking stop words.\nStop words are naturally common in the corpus\nand they can be important for learning the syntax\nand structure of a sentence. However, masked stop\nwords can be too easy for a language model to\npredict, especially in later stages of LM pretraining.\nThus, properly reducing the masking frequency of\nstop words can improve both the efficiency and\nperformance of the model.\nthe dual\nis\nbetween\nharry potter\nand lord\nvoldemort\nthe\ndual\nis\nbetween\nharry\npotter\nand\nlord\nvoldemort\n0 -0.1 0.2 0.3 -0.3 -0 0 0.1 -0\n-0.1 0 0.4 0.5 -1.8 -0.5 0.2 -0.9 0\n0.2 0.4 0 0.2 -0.4 -0.2 0 -0.1 0.6\n0.3 0.5 0.2 0 -0.9 -0.5 0.9 -0.2 0.3\n-0.3 -1.8 -0.4 -0.9 0 5.8 0.1 0.8 6.1\n-0 -0.5 -0.2 -0.5 5.8 0 0.3 1.2 6.2\n0 0.2 0 0.9 0.1 0.3 0 0 0.3\n0.1 -0.9 -0.1 -0.2 0.8 1.2 0 0 4.6\n-0 0 0.6 0.3 6.1 6.2 0.3 4.6 0\n2\n 1\n 0 1 2 3 4 5\nFigure 2: The PMI matrix of the words in the sentence\n‘The dual is between Harry Potter and Lord V oldemort.’\n3.1 Informative Relevance\nTo generate highly informative masking decisions\nfor a sentence, we introduce a new concept, namely\nInformative Relevance. Informative Relevance is\nused to measure how relevant a masked word is\nto the unmasked words so that it can be meaning-\nful and predictable. The Informative Relevance\nof a word is calculated by summing up the Point-\nwise Mutual Information (PMI, Fano, 1961) be-\ntween the masked word and all unmasked words in\nthe sentence. PMI between two words w1 and w2\nrepresents how ‘surprising’ is the co-occurrence\nbetween two words, accounting for their own prob-\nabilities. Formally, the PMI of the combination\nw1w2 is defined as:\npmi(w1, w2) = log p(w1, w2)\np(w1)p(w2) (1)\nThe PMI matrix is calculated corpus-wise. Note\nthat instead of using bigrams (i.e., two words have\nto be next to each other), we consider the skip-\ngram co-occurrence within a window. The window\nsize is selected in a way that enables sentence-level\nco-occurrence to be considered as well as local\nco-occurrence.\nMaximizing the Informative Relevance enables\nthe model to better memorize knowledge and fo-\ncus on more informative words. Since Informative\nRelevance is calculated between a masked word\nand the unmasked words, it also encourages hints\nto be preserved so that the model can reasonably\n5868\nAlgorithm 1 InforMask Algorithm\n1: D ←Set of text\n2: s ←Size of randomly sampled candidates\n3: Fd\ni ←Informative score for i-th masking can-\ndidate for text d\n4: for d ∈D do\n5: for i = 1, 2, . . . , sdo\n6: Generate i-th masking candidate for d\n7: Md\ni ←Masked Tokens\n8: Ud\ni ←Unmasked Tokens\n9: Fd\ni ←0\n10: for w1 ∈Md\ni do\n11: for w2 ∈Ud\ni do\n12: Fd\ni = Fd\ni + pmi(w1, w2)\n13: end for\n14: end for\n15: end for\n16: Choose candidate with maximum Fd\ni\n17: end for\nguess the masked words. As shown in Figure 2, the\nwords inside a named entity have a high PMI (e.g.,\n‘Harry-Potter’ and ‘Lord-V oldemort’) while the two\nclosely related entities also show a high PMI (e.g.,\nHarry-V oldemort). Thus, if we are asked to mask\none word, we would mask ‘V oldemort’ since it has\nthe highest Informative Relevance with the remain-\ning words (by summing up the last row or column).\n3.2 Scoring Masking Candidates\nOne text sample can have multiple masks. Thus, we\ndefine the informative score of a masking decision\nas the sum of the Informative Relevance of each\nmasked token. However, given the PMI matrix,\nfinding the best k words to mask (i.e., the masking\ndecision with the highest informative score) in a\nsentence of n words is time-consuming. Iterating\nall possibilities has time complexity O(Ck\nn). By\nconverting it to a minimum cut problem, the time\ncomplexity can be reduced to O(n2 log n) (Stoer\nand Wagner, 1997), which is still prohibitive in\npractice.\nTherefore, we propose to samples random mask-\ning candidates and then rank them by calculating\ntheir informative scores. As shown in Figure 1, we\nrandomly generate four masking candidates and\nrank them by their informative scores. We select\nthe candidate with the highest score. This allows us\nto make a masking decision with time complexity\nO(kn). Random sampling also introduces more\ndiverse patterns for masking, which could help\nData Subset #Relations #Samples\nConceptNet 1 29774\nSquad 1 305\nGoogleRE 3 4994\nTREx 41 34032\nTotal 46 69105\nTable 1: Statistics of LAMA (Petroni et al., 2019).\nDataset SQuAD v1 SQuAD v2\n#Examples 108k 151k\n#Negative Examples 0 54k\n#Articles 536 505\nTable 2: Statistics of SQuAD v1 and v2 (Rajpurkar et al.,\n2016, 2018).\ntraining of language models and prevent overfit-\nting. This process is illustrated in Algorithm 1.\n3.3 Token-Specific Masking Rates\nAlgorithm 1 is already usable by processing the\ninput text on the fly. However, to avoid overfit-\nting, masking should change across epochs. This\nmeans we have to run Algorithm 1 every epoch,\ncreating a bottleneck for pretraining. To address\nthis efficiency issue, we use token-specific masking\nrates to approximate the masking decisions of Infor-\nMask. Specifically, we generate masks for a corpus\nusing Algorithm 1, and then count the frequency\nof each token in the vocabulary to be masked as\ntheir token-specific masking rates. Note that in this\nway, Algorithm 1 is only executed once, as a pre-\npossessing step. Furthermore, we can use a small\nportion of the corpus to calculate the token-specific\nmasking rates, making it even faster.2 After this,\nwe can perform random masking, except that every\ntoken has its own masking rate.\n4 Experiments\n4.1 Experimental Settings\nPretraining Corpus Following BERT (Devlin\net al., 2019), we use the Wikipedia and Book Cor-\npus datasets available from Hugging Face (Lhoest\net al., 2021). The corpus contains ∼3.3B tokens.\nTo be consistent with BERT, we use an overall\nmasking rate of 15%. The PMI matrix is calculated\n2For the Wikipedia corpus, the average rate of change for\ntoken-specific masking rates falls below 0.8% after processing\nonly 1% of the corpus.\n5869\nModel #Param. Corpus Size Epochs LAMA (Petroni et al., 2019)\nConceptNet Squad GoogleRE TREx Overall\n(a)\nRandom (2019) 125M 16 GB 3 0.091 0.124 0.396 0.582 0.549\nSpan (2020) 125M 16 GB 3 0.056 0.102 0.377 0.524 0.495\nPMI (2021) 125M 16 GB 3 0.075 0.115 0.396 0.552 0.522\nInforMask 125M 16 GB 3 0.109 0.133 0.410 0.627 0.591\n(b)\nBERT-base 110M 16 GB 40 0.191 0.229 0.340 0.587 0.553\nBERT-large 340M 16 GB 40 0.218 0.284 0.354 0.621 0.585\nRoBERTa-base 125M 160 GB 40 0.223 0.307 0.423 0.630 0.592\nRoBERTa-large 355M 160 GB 40 0.260 0.329 0.435 0.672 0.632\nInformBERT 125M 16 GB 40 0.201 0.384 0.509 0.739 0.698\nTable 3: Performance of different masking strategies and models on LAMA (Petroni et al., 2019). (a) We compare\nthe models trained with different masking strategies for 3 epochs. (b) We compare InformBERT, a BERT model\ntrained with InforMask for 40 epochs with BERT and RoBERTa models.\nModel #Param. Corpus Size Epochs SQuAD v1 SQuAD v2\nF1 EM F1 EM\n(a)\nRandom (2019) 125M 16 GB 3 79.08 69.44 66.48 63.15\nSpan (2020) 125M 16 GB 3 78.88 69.04 64.95 61.38\nPMI (2021) 125M 16 GB 3 80.31 70.98 66.25 62.82\nInforMask 125M 16 GB 3 80.47 71.41 67.29 63.90\n(b) BERT-base 110M 16 GB 40 81.07 88.52 72.35 75.75\nInformBERT 125M 16 GB 40 81.22 88.61 72.71 75.86\nTable 4: Performance on SQuAD v1 and v2 (Rajpurkar et al., 2016, 2018) development set.\non the Wikipedia corpus, with a size of 100k ×\n100k. Word co-occurrence statistics are computed\nwith a window size of 11. We set the candidate sam-\npling size per document s to 30. It takes ∼4 hours\nto preprocess and generate token-specific masking\nrates on a 16-core CPU server with 256 GB RAM.\nEvaluation Benchmarks To evaluate different\nmasking strategies, we use the LAMA bench-\nmark (Petroni et al., 2019) to test the knowledge\nof the models. LAMA is a probe for analyzing the\nfactual and commonsense knowledge contained in\npretrained language models. Thus, it is suitable\nfor evaluating the knowledge learned during pre-\ntraining. LAMA has around 70,000 factual probing\nsamples across 46 factual relations. A summary of\nthe benchmark is shown in Table 1. We use Mean\nReciprocal Rank (MRR) as the metric for factual\nrecall performance.\nIn addition to the knowledge probing task, we\nalso conduct experiments on real-world question\nanswering datasets, which requires commonsense\nknowledge as well. We conduct experiments on\nSQuAD v1 and v2 (Rajpurkar et al., 2016, 2018)\nand report the F1 and Exact Match (EM) scores on\nthe development set. The statistics of the bench-\nmark are shown in Table 2. We provide additional\nresults on GLUE (Wang et al., 2018) benchmark in\nAppendix C.\nBaselines We compare InforMask in two settings:\n(a) We use the same tokenizer and hyperparame-\nters to pretrain BERT random masking (Devlin\net al., 2019), SpanBERT (Joshi et al., 2020) and\nPMI-Masking (Levine et al., 2021) for 3 epochs.\nThe choice of 3 epochs is according to our limited\ncomputational budget. (b) We continue training\nInforMask until 40 epochs. The 40-epoch model is\ndenoted as InformBERT. We compare InformBERT\nto BERT-base (Devlin et al., 2019), which is trained\nwith the same corpus for 40 epochs as well. We\nalso include results of BERT-large and RoBERTa\nfor reference, though they are either larger in size\nor trained with more data and thus are not directly\ncomparable.\nTraining Details Our implementation is based on\nHugging Face Transformers (Wolf et al., 2020). We\ntrain the baselines and our model with 16 Nvidia\n5870\n10k 20k 30k 40k 50k 60k 70k 80k 90k\nTraining Steps\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6MRR\nrandom\nspan\npmi\nInforMask\nFigure 3: Macro average MRR of different masking\nstrategies on LAMA, evaluated every 10k steps.\nV100 32GB GPU. For our model and all baselines\ntrained, we use a fixed vocabulary size of 50,265.\nThe model architecture is a base-size BERT model,\nwith 12 Transformer layers with 12 attention heads.\nThe hidden size is set to 768. The overall batch size\nis 256. We use an AdamW optimizer (Loshchilov\nand Hutter, 2019) with a learning rate of 5e-5. Note\nthat we do not perform any hyperparameter search-\ning or tuning for any model (including Inform-\nBERT) given our limited computational budget.\n4.2 Experimental Results\nImpact of Masking Strategies We conduct a fair\ncomparison among different masking strategies, us-\ning the same tokenization and hyperparameters. As\nshown in Table 3(a), InforMask outperforms other\nmasking strategies by a large margin on all sub-\nsets of LAMA (Petroni et al., 2019). As shown\nin Table 4(a), on both SQuAD v1 and v2 (Ra-\njpurkar et al., 2016, 2018), InforMask outperforms\nother masking strategies. Notably, PMI-Masking\nachieves higher performance on SQuAD while un-\nderperforming random masking on LAMA (to be\ndetailed shortly) but our InforMask achieves better\nresults on both of them.\nAlso, we compare our 40-epoch InformBERT\nmodel with BERT and RoBERTa models. As\nshown in Table 3(b), InformBERT outperforms\nthe BERT model trained with the same epochs and\ncorpus by 0.145 overall. It also achieves higher\nperformance than RoBERTa-base, despite being\ntrained with 10% of RoBERTa’s corpus size. To\nour surprise, it also outperforms both BERT-large\nand RoBERTa-large, with only 1/3 parameters. The\n0 200k 400k 600k 800k 1000k 1200k\nTraining Steps\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70MRR\nBERT-base\nBERT-large\nRoBERT a-base\nRoBERT a-large\nInformBERT\nFigure 4: Performance of InformBERT for the full\npretraining process. It achieves comparable perfor-\nmance with BERT-base after 40k training steps and\neven RoBERTa-large after 120k training steps.\nbreakdown of performance for each relation can\nbe found in Appendix A. Moreover, InformBERT\noutperforms BERT-base for fine-tuning on SQuAD\nv1 and v2, demonstrating its capability for down-\nstream question answering, as shown in Table 4(b).\nTraining Dynamics As shown in Figure 3, In-\nforMask demonstrates an outstanding training ef-\nficiency. InforMask outperforms other masking\nstrategies from the beginning of the training pro-\ncess and keeps the lead through the training. No-\ntably, span masking and PMI-Masking underper-\nform random masking, indicating their inability on\nthe knowledge-intense task. Span masking also\nsignificantly underperforms other masking strate-\ngies in the early stage of pretraining, suggesting\nit may take longer to train the model. For the en-\ntire pretraining process, as shown in Figure 4, the\nmodel trained with InforMask outperforms BERT\nand RoBERTa with fewer than∼15% of the train-\ning steps, verifying the efficiency of our masking\nstrategy.\nImpact on Stop Words and Entities As shown\nin Figure 5, without explicitly specifying the stop\nwords, InforMask can identify the stop words and\nreduce their probability to be masked. InforMask\ncan also automatically increase the masking prob-\nability of named entities. The average masking\nprobability of named entities is 0.25 with a stan-\ndard deviation of 0.07, while the overall masking\n5871\ntheof in\nand\na to\nwas\nis as for on by he s\nwith\nat\nfrom\nit his an\nT okens\n0\n1\n2\n3\n4Frequency (%)\nT oken occurrence\nRandom masking rate\nInforMask masking rate\nFigure 5: Frequency of common stop words and their\ncorresponding masking rates by InforMask.\nprobability of all tokens is around 0.15.3 This al-\nlows the model to focus on more important tokens\nand maintain an appropriate difficulty of prediction,\nfacilitating the pretraining process.\nImpact of Token-Specific Masking Rates As\nmentioned before, the use of token-specific mask-\ning rate can enormously save time and RAM for\ndata processing, as spending hours of processing\nfor each epoch can be infeasible and becomes a bot-\ntleneck for distributed training. Another possible\nsolution is to loop the same masked data for every\nepoch. Thus, we conduct an experiment to compare\nthe two solutions: approximation and repetition.\nNote that for simplicity, the token-specific mask-\ning rate is applied from the first epoch. As shown\nin Figure 6, our approximation strategy keeps out-\nperforming the repetition strategy even in the first\nepoch. As we analyze, this can be attributed to\nthe more diverse patterns introduced during the ap-\nproximation. Also, the performance of the model\ntrained with the repetition strategy converges or\neven slightly declines after 60k training steps while\nthe performance of the model trained with approxi-\nmation keeps increasing.\nInforMask vs. PMI Masking PMI Mask-\ning (Levine et al., 2021) uses PMI to mask a span of\ncorrelated tokens. A named entity often constitutes\na correlated span and therefore, is more likely to\nbe masked in PMI-Masking. As mentioned before,\n3We use an off-the-shelf named entity recognition system\nto verify the effectiveness of our approach only. It is not a\nnecessary component of the proposed system.\n10k 20k 30k 40k 50k 60k 70k 80k 90k\nTraining Steps\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6MRR\nRepeat epoch\nRandom approx.\nFigure 6: Comparison between looping the same data\nand using token-specific masking rate to approximate\nthe masking decisions. The models are trained for 3\nepochs.\nthe for then heart\ncoloradoairwaysedison nairobi\n0.00\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40Mask Probability (%)\nrandom\nspan\npmi\nInforMask\nFigure 7: Masking rate of tokens according to different\nmasking policies.\nwe observe that PMI-Masking performs worse than\nrandom masking on LAMA (see Figure 3).\nTo investigate the reason, we compute the in-\ndividual masking rates of some tokens according\nto each masking policy. As shown in Figure 7,\nwe can see that PMI-Masking increases the mask-\ning rate of tokens that are part of correlated spans.\nHowever, it decreases the masking rate of tokens\nthat are not within any correlated span, even if that\ntoken is a named entity. Consider the token ‘Air-\nways’ for example. This token may be part of a\ncorrelated span such as ‘British Airways’ or ‘Qatar\nAirways’. PMI-Masking, therefore, increases the\nmasking rate of this token compared to random\n5872\nQuery Ground Truth InformBERT RoBERTa-base\nPrediction Score Prediction Score\nfrance 0.09 montreal 0.12\nAntoine Coypel was born in[MASK]. paris paris 0.08 toronto 0.03\nhaiti 0.04 paris 0.03\nespn 0.20 cbs 0.18\nSpeedWeek is an American television program on[MASK]. espn nbc 0.10 cnbc 0.13\nmtv 0.09 spike 0.10\nmicrosoft 0.20 intel 0.06\nPhil Harrison is a corporate vice president of[MASK]. microsoft ibm 0.15 ibm 0.05\nmotorola 0.05 microsoft 0.03\nfrench 0.43 young 0.13\nLaurent Casanova was a[MASK]politician. french canadian 0.32 french 0.09\nhaitian 0.05 successful 0.04\nbishops 0.13 men 0.17\nThe chief administrators of the church are[MASK]. bishops priests 0.07 christians 0.09\nappointed 0.06 women 0.08\nTable 5: Some examples of InformBERT and RoBERTa-base predictions on LAMA (Petroni et al., 2019). We show\nthe queries and the ground-truth answers with the model predictions. We only show the top-3 predictions made by\neach model.\nmasking. On the other hand, the tokens ‘Colorado’\nand ‘Nairobi’, which are unigram named entities,\nare less likely to be masked, compared to random\nmasking. Given that the overall masking rate is\nfixed and PMI-Masking favors correlated spans,\nthe masking rates of ‘Colorado’ and ‘Nairobi’ in-\nevitably get lower. This can be the reason behind\nPMI-Masking’s failure.\nIn contrast, InforMask uses PMI to compute the\nindividual Informative Relevance of tokens. It can\nincrease the masking rate of tokens with high infor-\nmative saliency, regardless of whether they are part\nof a correlated span or not. This helps InforMask\nachieve superior factual recall performance.\n4.3 Case Study\nTable 5 shows the example knowledge probes and\nanswers produced by InformBERT and RoBERTa.\nFor the query ‘SpeedWeek is an American tele-\nvision program on [MASK].’, RoBERTa is unable\nto produce the correct answer in the top-3 predic-\ntions. But InformBERT correctly predicts ‘ESPN’\nto be the top candidate. Similarly, InformBERT cor-\nrectly predicts the answer ‘bishops’ for the query\n‘The chief administrators of the church are[MASK].’\nRoBERTa is unable to predict the answer and pro-\nduces more generic words such as ‘men’, ‘women’,\nand ‘Christians’.\nWe summarize the errors into two notable cat-\negories. They are relevant for all the models in-\nvolved, not just InformBERT. First, we observe\nthat many errors involve rare named entities. Some\nnamed entities are less frequent so the model is\nunable to learn anything useful about them, or they\noccur so rarely that they do not even appear in\nthe language model vocabulary. We found that\naround 19% of the errors made by our model on\nthe LAMA benchmark is associated with out-of-\nvocabulary tokens. Second, it is challenging for\na language model to predict the granularity of the\nfact being asked or distinguish it from an alternate\nfact that may hold for a query. For the example\nquery ‘Antoine Coypel was born in [MASK].’, the\nLAMA dataset has only one true label ‘Paris’. In\nthis example, InformBERT prefers the name of the\ncountry (‘France’) over the name of a city (‘Paris’).\nThis confusion is related to the granularity of lo-\ncation and both answers can be considered correct.\nHowever, it is being classified as an error because\nthe labels in the test set are not comprehensive.\nAnother type of confusion can be found for\nRoBERTa with the query ‘Laurent Casanova was a\n[MASK] politician.’. The model is trying to decide\nwhether to use the adjective ‘young’, ‘French’, or\n‘successful’. In theory, these three adjectives may\nbe valid simultaneously for the same entity. It can\nbe challenging for the language model to pick the\nexpected one in the context. We include more ex-\namples of knowledge probes with InformBERT in\nAppendix B.\n5873\n5 Conclusion\nIn this work, we propose InforMask, an unsuper-\nvised masking policy that masks tokens based on\ntheir informativeness. InforMask achieves supe-\nrior performance in knowledge-intense tasks in-\ncluding factual recall and question answering. We\nexplore the impact of different masking strategies\non learning factual and commonsense knowledge\nfrom pretraining and analyze why previously pro-\nposed masking techniques are suboptimal. For fu-\nture work, we would like to scale up the pretraining\nand explore more factors for knowledge acquisition\nduring unsupervised text pretraining.\nLimitations\nWe conduct experiments to compare InforMask to\nseveral prior works on better masking strategies\nby training them for 3 epochs. We also compare a\nfully trained InformBERT-base model to BERT and\nRoBERTa. However, one limitation of our paper\nis due to our limited computational budget, we are\nnot able to scale the experiments for larger model\nsize, larger corpus, or compare all baselines under\nthe full pretraining setting. Also, our InformBERT\nmodel is arguably suboptimal, with a relatively\nsmall batch size and no hyperparameter tuning or\nsearch at all.\nEthics Statement\nSimilar to BERT or RoBERTa, our model may con-\ntain social biases that preexist in the training corpus.\nThus, we do not anticipate any major ethical con-\ncerns in addition to those identified in language\nmodels (Bender et al., 2021). However, to the best\nof our knowledge, there is no research on the im-\npact of masking strategies on social biases, which\ncould be an interesting and important direction for\nfuture research.\nAcknowledgements\nWe would like to thank the anonymous reviewers\nfor their insightful comments. This project is partly\nsupported by NSF Award #1750063.\nReferences\nJiangang Bai, Yujing Wang, Yiren Chen, Yaming Yang,\nJing Bai, Jing Yu, and Yunhai Tong. 2021. Syntax-\nbert: Improving pre-trained transformers with syntax\ntrees. In Proceedings of the 16th Conference of the\nEuropean Chapter of the Association for Computa-\ntional Linguistics: Main Volume, EACL 2021, Online,\nApril 19 - 23, 2021, pages 3011–3020. Association\nfor Computational Linguistics.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big? In FAccT, pages 610–623. ACM.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, NAACL-HLT 2019, Minneapolis, MN, USA,\nJune 2-7, 2019, Volume 1 (Long and Short Papers),\npages 4171–4186. Association for Computational\nLinguistics.\nRobert M Fano. 1961. Transmission of information:\nA statistical theory of communications. American\nJournal of Physics, 29(11):793–794.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Retrieval augmented\nlanguage model pre-training. In International Con-\nference on Machine Learning , pages 3929–3938.\nPMLR.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nPei Ke, Haozhe Ji, Siyang Liu, Xiaoyan Zhu, and Minlie\nHuang. 2020. Sentilare: Sentiment-aware language\nrepresentation learning with linguistic knowledge. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020, pages 6975–\n6988. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\n5874\nAnne Lauscher, Olga Majewska, Leonardo F. R. Ribeiro,\nIryna Gurevych, Nikolai Rozanov, and Goran Glavas.\n2020. Common sense or world knowledge? inves-\ntigating adapter-based knowledge injection into pre-\ntrained transformers. CoRR, abs/2005.11787.\nYoav Levine, Barak Lenz, Or Dagan, Ori Ram, Dan\nPadnos, Or Sharir, Shai Shalev-Shwartz, Amnon\nShashua, and Yoav Shoham. 2020. Sensebert: Driv-\ning some sense into BERT. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 4656–4667. Association for Computa-\ntional Linguistics.\nYoav Levine, Barak Lenz, Opher Lieber, Omri Abend,\nKevin Leyton-Brown, Moshe Tennenholtz, and Yoav\nShoham. 2021. Pmi-masking: Principled masking of\ncorrelated spans. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, et al. 2021. Datasets: A community\nlibrary for natural language processing. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing: System Demon-\nstrations, pages 175–184, Online and Punta Cana,\nDominican Republic. Association for Computational\nLinguistics.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-BERT: en-\nabling language representation with knowledge graph.\nIn The Thirty-Fourth AAAI Conference on Artificial\nIntelligence, AAAI 2020, The Thirty-Second Innova-\ntive Applications of Artificial Intelligence Conference,\nIAAI 2020, The Tenth AAAI Symposium on Educa-\ntional Advances in Artificial Intelligence, EAAI 2020,\nNew York, NY, USA, February 7-12, 2020 , pages\n2901–2908. AAAI Press.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nIlya Loshchilov and Frank Hutter. 2019. Decoupled\nweight decay regularization. In 7th International\nConference on Learning Representations, ICLR 2019,\nNew Orleans, LA, USA, May 6-9, 2019 . OpenRe-\nview.net.\nMatthew E. Peters, Mark Neumann, Robert L. Logan\nIV , Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced contex-\ntual word representations. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019, pages 43–54. Association for Compu-\ntational Linguistics.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language mod-\nels as knowledge bases? In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing,\nEMNLP-IJCNLP 2019, Hong Kong, China, Novem-\nber 3-7, 2019 , pages 2463–2473. Association for\nComputational Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nthe 2016 Conference on Empirical Methods in Natu-\nral Language Processing, pages 2383–2392, Austin,\nTexas. Association for Computational Linguistics.\nMechthild Stoer and Frank Wagner. 1997. A simple\nmin-cut algorithm. Journal of the ACM (JACM) ,\n44(4):585–591.\nYusheng Su, Xu Han, Zhengyan Zhang, Yankai Lin,\nPeng Li, Zhiyuan Liu, Jie Zhou, and Maosong Sun.\n2021. Cokebert: Contextual knowledge selection and\nembedding towards enhanced pre-trained language\nmodels. AI Open, 2:127–134.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo,\nYaru Hu, Xuanjing Huang, and Zheng Zhang. 2020.\nColake: Contextualized language and knowledge em-\nbedding. In Proceedings of the 28th International\nConference on Computational Linguistics, COLING\n2020, Barcelona, Spain (Online), December 8-13,\n2020, pages 3660–3670. International Committee on\nComputational Linguistics.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng,\nXuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,\n5875\nHao Tian, and Hua Wu. 2019. ERNIE: enhanced\nrepresentation through knowledge integration. CoRR,\nabs/1904.09223.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. GLUE:\nA multi-task benchmark and analysis platform for nat-\nural language understanding. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP , pages\n353–355, Brussels, Belgium. Association for Com-\nputational Linguistics.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021.\nKepler: A unified model for knowledge embedding\nand pre-trained language representation. Transac-\ntions of the Association for Computational Linguis-\ntics, 9:176–194.\nAlexander Wettig, Tianyu Gao, Zexuan Zhong, and\nDanqi Chen. 2022. Should you mask 15% in masked\nlanguage modeling? CoRR, abs/2202.08005.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing: System Demon-\nstrations, EMNLP 2020 - Demos, Online, November\n16-20, 2020, pages 38–45. Association for Computa-\ntional Linguistics.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6442–6454, On-\nline. Association for Computational Linguistics.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: enhanced\nlanguage representation with informative entities. In\nProceedings of the 57th Conference of the Associa-\ntion for Computational Linguistics, ACL 2019, Flo-\nrence, Italy, July 28- August 2, 2019, Volume 1: Long\nPapers, pages 1441–1451. Association for Computa-\ntional Linguistics.\n5876\nA Performance Breakdown on LAMA\nSubset Relation BERT-base BERT-large RoBERTa-base RoBERTa-large InformBERT\nConceptNet test 0.191 0.218 0.223 0.260 0.201\nGoogleRE dateOfBirth 0.108 0.115 0.092 0.108 0.122\nGoogleRE placeOfBirth 0.475 0.493 0.610 0.612 0.732\nGoogleRE placeOfDeath 0.388 0.403 0.528 0.582 0.607\nSquad test 0.229 0.284 0.307 0.329 0.384\nTREx P1001 0.786 0.817 0.810 0.846 0.881\nTREx P101 0.453 0.499 0.307 0.380 0.507\nTREx P103 0.842 0.876 0.841 0.857 0.907\nTREx P106 0.656 0.675 0.540 0.599 0.674\nTREx P108 0.584 0.596 0.658 0.725 0.704\nTREx P127 0.546 0.570 0.661 0.688 0.743\nTREx P1303 0.387 0.442 0.233 0.277 0.445\nTREx P131 0.650 0.685 0.742 0.778 0.867\nTREx P136 0.621 0.666 0.557 0.596 0.675\nTREx P1376 0.730 0.768 0.631 0.630 0.840\nTREx P138 0.509 0.533 0.515 0.548 0.742\nTREx P140 0.606 0.674 0.668 0.728 0.751\nTREx P1412 0.777 0.801 0.799 0.824 0.860\nTREx P159 0.468 0.486 0.660 0.701 0.789\nTREx P170 0.860 0.886 0.878 0.908 0.928\nTREx P176 0.687 0.731 0.717 0.770 0.777\nTREx P178 0.631 0.683 0.711 0.744 0.721\nTREx P19 0.424 0.441 0.620 0.652 0.760\nTREx P190 0.267 0.312 0.486 0.542 0.662\nTREx P20 0.516 0.553 0.675 0.703 0.791\nTREx P264 0.273 0.300 0.003 0.005 0.380\nTREx P27 0.767 0.796 0.853 0.884 0.895\nTREx P276 0.549 0.577 0.646 0.682 0.824\nTREx P279 0.554 0.589 0.512 0.560 0.594\nTREx P30 0.832 0.868 0.845 0.896 0.918\nTREx P31 0.650 0.665 0.597 0.631 0.652\nTREx P36 0.425 0.447 0.484 0.511 0.758\nTREx P361 0.554 0.596 0.442 0.480 0.607\nTREx P364 0.738 0.767 0.661 0.704 0.811\nTREx P37 0.734 0.766 0.711 0.743 0.788\nTREx P39 0.615 0.647 0.501 0.550 0.636\nTREx P407 0.648 0.705 0.665 0.710 0.695\nTREx P413 0.480 0.501 0.508 0.564 0.508\nTREx P449 0.470 0.473 0.652 0.685 0.735\nTREx P463 0.676 0.692 0.641 0.683 0.736\nTREx P47 0.532 0.582 0.606 0.628 0.860\nTREx P495 0.707 0.737 0.805 0.855 0.823\nTREx P527 0.499 0.571 0.492 0.585 0.575\nTREx P530 0.448 0.493 0.740 0.812 0.802\nTREx P740 0.343 0.369 0.672 0.715 0.731\nTREx P937 0.554 0.587 0.720 0.741 0.797\nTable 6: Relation by relation performance comparison on LAMA (Petroni et al., 2019).\n5877\nB More LAMA Examples\nQuery Ground truth Top predictions (with confidence)\nCommunicating is for gaining[M]. knowledge knowledge(0.22), information(0.09), insight(0.04)\nCompeting against someone requires a desire to[M]. win compete(0.35), win(0.25), fight(0.08)\nGoing on the stage is for performing an[M]. act act(0.65), opera(0.2), improvisation(0.02)\nPlaying is a way to[M]social skills. learn learn(0.22), develop(0.15), improve(0.14)\nGallagher was born on 14 December 1978 in[M]. scotland ireland(0.25), scotland(0.07), dublin(0.05)\nCrisp died at her home in[M], Arizona . phoenix tucson(0.34), phoenix(0.12), prescott(0.12)\nFrank Marion died in 1963 in[M], Connecticut . stamford hartford(0.12), stamford(0.1), middletown(0.1)\nMattingly died in 1951 in[M], Kentucky . louisville louisville(0.4), lexington(0.14), ashland(0.03)\nSmith died on 26 February 1832 in[M]. london england(0.08), london(0.07), ireland(0.03)\nNewton played as[M]during Super Bowl 50. quarterback quarterback(0.09), referee(0.05), mvp(0.05)\nWarsaw is the most diverse[M]in Poland. city city(0.63), town(0.13), settlement(0.03)\nQuran is a[M]text. religious religious(0.21), muslim(0.1), biblical(0.08)\npresident, and Thomas Watson, founder of[M]. ibm ibm(0.21), microsoft(0.02), motorola(0.02)\nLetham is a village in[M], Scotland. angus fife(0.52), angus(0.24), highland(0.06)\nHugh Ragin is an American[M]trumpeter. jazz jazz(0.97), classical(0.01), rock(0.01)\nAvishkaar is a 1974[M]movie. hindi bollywood(0.31), hindi(0.29), malayalam(0.11)\nWest of Bern, the population generally speaks[M]. french german(0.72), french(0.13), italian(0.05)\nHe was succeeded as[M]by Christoph Ahlhaus. mayor chancellor(0.1), bishop(0.09), mayor(0.05)\nHis son Hugh became[M]of Saint-Gilles. abbot bishop(0.48), abbot(0.28), archbishop(0.13)\nDuring his terms Romania joined[M]. nato nato(0.25), yugoslavia(0.15), czechoslovakia(0.07)\nIt seized[M]and Czechoslovakia in 1938 and 1939. austria hungary(0.3), poland(0.23), austria(0.11)\nHostage Life was a Canadian punk band from[M]. toronto toronto(0.25), vancouver(0.12), montreal(0.11)\nTable 7: More factual probe examples of InformBERT on LAMA (Petroni et al., 2019). [M] denotes the masked\ntoken.\nC GLUE Performance\nWe have conducted additional experiments on GLUE (Wang et al., 2018). InformBERT outperforms\nBERT-base on six out of nine tasks. Notably, InformBERT seems to underperform BERT by a large margin\non CoLA, which is focused on the grammatical correctness. We suspect this is because InformBERT pays\nless attention to stop words that can be important for this task.\nModel GLUE (Wang et al., 2018)\nCoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE WNLI\nBERT-base 56.53 92.32 84.07 88.64 90.71 83.91 90.66 65.57 56.34\nInformBERT 52.16 92.66 87.50 88.75 90.90 83.13 89.82 65.70 56.93\nTable 8: Comparison of InformBERT and BERT-base on the dev. set of GLUE (Wang et al., 2018). both models are\ntrained for 40 epochs using the same corpus. We report Matthews correlation for CoLA, Pearson correlation for\nSTS-B and accuracy for other tasks.\n5878",
  "topic": "Masking (illustration)",
  "concepts": [
    {
      "name": "Masking (illustration)",
      "score": 0.8452425599098206
    },
    {
      "name": "Computer science",
      "score": 0.7995520830154419
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6946122646331787
    },
    {
      "name": "Language model",
      "score": 0.6529864072799683
    },
    {
      "name": "Preprocessor",
      "score": 0.5687601566314697
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5524935126304626
    },
    {
      "name": "Exploit",
      "score": 0.5084606409072876
    },
    {
      "name": "Pointwise",
      "score": 0.49175623059272766
    },
    {
      "name": "Word (group theory)",
      "score": 0.4766060709953308
    },
    {
      "name": "Natural language processing",
      "score": 0.41027742624282837
    },
    {
      "name": "Speech recognition",
      "score": 0.3685615062713623
    },
    {
      "name": "Machine learning",
      "score": 0.36688077449798584
    },
    {
      "name": "Mathematics",
      "score": 0.06668296456336975
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ]
}