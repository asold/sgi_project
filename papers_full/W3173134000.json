{
  "title": "Reconstructing Implicit Knowledge with Language Models",
  "url": "https://openalex.org/W3173134000",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2106086056",
      "name": "Maria Becker",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2316255186",
      "name": "Siting Liang",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University"
      ]
    },
    {
      "id": "https://openalex.org/A2113562383",
      "name": "Anette Frank",
      "affiliations": [
        "Heidelberg University",
        "Heidelberg University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891012317",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2994863194",
    "https://openalex.org/W3101850416",
    "https://openalex.org/W2950339735",
    "https://openalex.org/W3101638716",
    "https://openalex.org/W3100103516",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2807873315",
    "https://openalex.org/W1968494857",
    "https://openalex.org/W2996403597",
    "https://openalex.org/W2561529111",
    "https://openalex.org/W3104007871",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3113425182",
    "https://openalex.org/W3014521650",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W3174660442",
    "https://openalex.org/W3118135949",
    "https://openalex.org/W3115582371",
    "https://openalex.org/W1489525520",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W3084179749",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W1991145433",
    "https://openalex.org/W2994644100",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3035586188",
    "https://openalex.org/W3028869933",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W3022006665",
    "https://openalex.org/W2936695845",
    "https://openalex.org/W3039207389",
    "https://openalex.org/W2621173615",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3116316726",
    "https://openalex.org/W3102187933"
  ],
  "abstract": "In this work we propose an approach for generating statements that explicate implicit knowledge connecting sentences in text. We make use of pre-trained language models which we refine by fine-tuning them on specifically prepared corpora that we enriched with implicit information, and by constraining them with relevant concepts and connecting commonsense knowledge paths. Manual and automatic evaluation of the generations shows that by refining language models as proposed, we can generate coherent and grammatically sound sentences that explicate implicit knowledge which connects sentence pairs in texts – on both in-domain and out-of-domain test data.",
  "full_text": "Proceedings of Deep Learning Inside Out (DeeLIO):\nThe 2nd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures,pages 11–24\nOnline, June 10, 2021. ©2021 Association for Computational Linguistics\n11\nReconstructing Implicit Knowledge with Language Models\nMaria Becker, Siting Liang, Anette Frank\nDepartment of Computational Linguistics, Heidelberg University\nmbecker|liang|frank@cl.uni-heidelberg.de\nAbstract\nIn this work we propose an approach for gener-\nating statements that explicate implicit knowl-\nedge connecting sentences in text. We make\nuse of pre-trained language models which we\nreﬁne by ﬁne-tuning them on speciﬁcally pre-\npared corpora that we enriched with implicit\ninformation, and by constraining them with\nrelevant concepts and connecting common-\nsense knowledge paths. Manual and automatic\nevaluation of the generations shows that by re-\nﬁning language models as proposed, we can\ngenerate coherent and grammatically sound\nsentences that explicate implicit knowledge\nwhich connects sentence pairs in texts – on\nboth in-domain and out-of-domain test data.\n1 Introduction\nIn everyday communication and in texts people\nusually omit information that seems clear and\nevident, such that only part of the message needs to\nbe expressed in words. In the following sentence:\n(1-i) Students should be allowed to use com-\nputers during the lectures, (1-ii) even though that\nbears the risk that they are writing emails instead\nof listening to the teacher.\nin order to understand the connection between (i)\nand (ii) we must know that Computers are used\nfor sending emails, or that Lectures are given by\nteachers. Such implicit knowledge can easily be\ninferred by humans, since it is part of their back-\nground knowledge. By contrast, for computational\nsystems implicitness in texts represents a challenge.\nIn this work we propose an approach for gen-\nerating implicit knowledge sentences in-between\ncontiguous sentences, which explicate their logi-\ncal connection, utilizing pre-trained language mod-\nels (LMs) that we reﬁne as follows: i) we inject\n’explanatory’ knowledge by ﬁne-tuning LMs on\nspeciﬁcally prepared corpora, and (ii) condition\ntext generation through constraints in form of rel-\nevant concepts and knowledge paths. Our work is\ninspired by the recent success of pre-trained LMs\n(Devlin et al., 2018; Radford et al., 2019; Yang\net al., 2019a) in various downstream NLP tasks,\nincluding text generation and NL inference (Wang\net al., 2018). However, for the task of reconstruct-\ning implicit knowledge, such LMs need to be care-\nfully guided, not only to yield coherent statements,\nbut to also ensure that they convey the missing, im-\nplicit information that connects given sentences in\na text. To this end we create corpora with sentence\npairs enriched with implicit information based on\non Generics-KB (Bhakthavatsalam et al., 2020) and\ne-SNLI (Camburu et al., 2018), which we use for\nLM ﬁne-tuning. For improved performance we ex-\nplore methods of constrained language generation,\nguiding the model by way of relevant concepts and\nconnecting commonsense knowledge paths.\nWe aim to build a system that is not limited to\nspeciﬁc text genres or knowledge domains, and\nthus evaluate our models in-domain – on testsets\nfrom our ﬁne-tuning corpora; and out-of-domain –\nusing IKAT (Becker et al., 2020), an argumentative\ncorpus which offers sentence pairs annotated with\nimplicit knowledge that connects them.\nA central contribution of this work is an in-\ndepth evaluation of the quality of generations de-\nlivered by different model variants, and their ability\nof expressing implicitly conveyed knowledge. We\npropose a manual evaluation setup covering four\ndimensions – grammaticality, coherence, content,\nand comparison to gold references – , and compare\nthese to various automatic evaluation metrics. Our\nexperiments show that with our proposed approach\nwe can generate coherent sentences that explicate\nimplicit knowledge that connects given sentence\npairs; and that current text generation metrics are\nnot sufﬁcient to evaluate this challenging task.\nOur contributions are: (i) We empirically com-\npare different types of LMs, exploring which model\nis best suited for the task of generating sentences\nthat express implicit information between sen-\n12\ntences. (ii) We create datasets that include implicit\ninformation holding between sentence pairs, which\nwe use for ﬁne-tuning our LMs, and which can\nbe used for general commonsense reasoning tasks.\n(iii) We propose a method for constrained genera-\ntion by injecting concepts or commonsense knowl-\nedge paths as language modeling constraints, and\nshow that key concepts, and even more, knowl-\nedge paths improve the quality of generations. (iv)\nWe carefully evaluate the quality of the generated\nimplicit knowledge sentences, both manually and\nautomatically, and discuss strengths and limitations\nof automatic similarity metrics.1\n2 Related Work\nRecent progress in pretraining LMs on large text\ncorpora led to improvements for various down-\nstream NLP tasks. It has also been shown that\nknowledge acquired during pre-training can be\nleveraged by ﬁne-tuning these models to advanced\nsemantic inference or NL generation tasks (Wang\net al. 2018). Recently, pre-trained LMs have been\naugmented with external knowledge from com-\nmonsense knowledge bases such as ConceptNet,\nwhich provides more explicit knowledge ground-\ning and improves their performance on downstream\ntasks that require reasoning abilities. Wang et al.\n(2020b), for example, retrieve multi-hop knowl-\nedge paths from ConceptNet for ﬁne-tuning LMs\nfor multiple choice question answering. Chang\net al. (2020) and Bosselut et al. (2021) incorporate\nknowledge paths from ConceptNet into pre-trained\nLMs for solving the SocialIQA task (Sap et al.,\n2019). However, all these approaches evaluate the\neffectiveness of integrating commonsense knowl-\nedge indirectly on downstream tasks, and do not ex-\nplicitly evaluate the impact and relevance of knowl-\nedge for a speciﬁc system prediction. We address\nthis shortcoming by generating and carefully eval-\nuating statements that connect pairs of sentences\nas explanations of their underlying, implicit knowl-\nedge link. Closest to this aim is the task ofexplana-\ntion generation, which has received attention very\nrecently. Wang et al. (2020a) propose the SemEval-\n2020 Task 4 (Subtask C), which is to generate an\nexplanation for why a statement does not make\nsense, by way of a natural language statement. A\ncomparison of the participating systems (cf. Peru-\n1The code for our proposed approach can be found here:\nhttps://github.com/Heidelberg-NLP/LMs4Im\nplicit-Knowledge-Generation.\nmal et al./Jon et al. 2020) shows that pre-trained\nLMs play a central role in the success of the top-\nperforming systems, demonstrating that they con-\ntain commonsense information to a good extent.\nThe success of models enriched with knowledge\nfrom external sourcessuch as ConceptNet further-\nmore shows that additional knowledge supports the\ngeneration of commonsense explanations. How-\never, there is still a large gap between systems and\nhuman performance.\nPre-trained LMs enhanced with commonsense\nknowledge have also been the models of choice for\nother text generation tasks, e.g. dialogue genera-\ntion (Zhou et al., 2018), story ending generation\n(Guan et al., 2020), or abductive NLI (Ji et al.,\n2020b). While these models aim at generating ex-\nplanations for a single statement, or completing a\ngiven sequence of sentences, we investigate how to\nmake use of LMs to generate a sentence that ﬁlls\nin implicit knowledge between two sentences.\nConstraining LMs. Recent work addresses\nhow to control content in LM text generation, while\nmaintaining ﬂuency, coherence and plausibility of\nthe generated text. Lin et al. (2020) explore how\nto generate a coherent and plausible situation de-\nscription given an unordered set of concepts as in-\nput, and ﬁnd that even pre-trained LMs (BART, T5)\nﬁne-tuned to this task cannot solve it: the generated\nsentences are grammatical, but highly implausible,\nlacking commonsense. This suggests that either\nthe underlying LMs, or input constraints for gener-\nation need to incorporate commonsense knowledge.\nOrbach and Goldberg (2020) attempt to control the\ncontent when generating longer stories by specify-\ning facts the story needs to include. They propose\na plan-and-cloze model that ﬁrst creates a cloze\ntemplate, placing input facts at ﬁxed positions in\nthe output. In the cloze step, the system expands\nthe fact tokens into complex sentences that com-\nplete the story. While uni-directional LMs such\nas GPT-2 or BART generate ﬂuent text but do not\nwell adhere to the desired content, the ﬁne-tuned\nmulti-directional XLNet outputs coherent text and\nadheres to the facts.\nWhile none of the above works incorporate exter-\nnal knowledge to guide generation, Ji et al. (2020a)\nperform explanation generation for single state-\nments, using ConceptNet background knowledge.\nThe model selects concepts from the statement,\nretrieves connecting paths from ConceptNet, and\nselects bridge concepts from a subgraph. A pre-\n13\ntrained decoder generates the explanation, using as\ninput the statement and top-ranked concepts from\nthe subgraph. In our work we also select concepts\nfrom texts, but dynamically generate commonsense\nknowledge paths as constraints. Importantly, we\naim to generate coherent explanations in-between\nsentences – a challenge for uni-directional LMs.\n3 Knowledge-constrained text generation\n3.1 Task Deﬁnition and Approach\nThe task we tackle in this work is: given two con-\ntiguous sentences (source sentences S1, S2), gen-\nerate an explanatory sentence (target sentence T)\nthat explains the underlying, implicit information\nthat connects them. We explore different types of\nLMs and their aptness for solving this task. We\nﬁne-tune them on existing or adapted datasets to\ninject relevant knowledge, and add key concepts\nor connecting knowledge-paths as constraints to\nachieve coherent and informative explanations.\n3.2 Types of Language Models\nWe compare three types of LMs: GPT-2 (Radford\net al., 2019), an autoregressive model which gener-\nates the output sequence from left to right; XLNet\n(Yang et al., 2019b), a bidirectional generalized au-\ntoregressive LM; and BART (Lewis et al., 2019),\na seq2seq model with a bidirectional masked en-\ncoder and a left-to-right decoder. While GPT-2 and\nBART generate the next tokens seeing only the left\n(previous) context, XLNet predicts the next tokens\nbased on the left and right context, in a random or-\nder. GPT-2 is pre-trained on web pages from Com-\nmonCrawl, XLNet on CommonCrawl+ClueWeb\n(Callan et al., 2009), and BART on the CNN/DM\nsummarization dataset (Hermann et al., 2015).\n3.3 Fine-tuning LMs\nTask-adapted Datasets for LM Fine-tuning.All\nchosen LMs are pre-trained on information that is\nexplicit in text. To condition them to generate im-\nplicit information that connects sentences, we ﬁne-\ntune them on datasets that include knowledge state-\nments connecting contiguous sentence pairs. We\ncreate two such corpora, one based on Generics-KB\n(Bhakthavatsalam et al., 2020), which offers state-\nments expressing generic knowledge; the other on\ne-SNLI (Camburu et al., 2018), which comprises\nexplanations of inferential commonsense knowl-\nedge. Each data instance contains two source sen-\ntences S1, S2, a target sentence T, and two key\nconcepts c1, c2 which we extract from the original\ndata as described below. For examples see Table 1.\nGenerics-KB contains naturally occurring\ngeneric sentences crawled from the web using lin-\nguistic rules and BERT-based scoring. It is rich in\nhigh-quality statements that express generic knowl-\nedge. Each generic sentence occurs in its surround-\ning context (1-5 sents before/after), hence each\ninstance forms a triple consisting of the context\nbefore ( Cb), the generic sentence ( GS) and the\ncontext after (Ca). We collect all instances where\na phrase p1 (NP, VP, ADJP or ADVP) from GS\nalso occurs in Cb, and another phrase p2 from GS\noccurs in Ca. For each instance we extract the\nsentence containing p1 and the one containing p2\nas our source sentences S1, S2; GS as our target\nsentence T; and p1 and p2 as key concepts c1, c2.\ne-SNLI is an extension of the SNLI dataset\n(Bowman et al., 2015), additionally annotated with\nexplanations: Given a premise-hypothesis pair and\nthe relation between them (entailment, contradic-\ntion, or neutral), annotators added natural language\nsentences that explain why the pair is in the rela-\ntion. Annotators had to mark essential key phrases\nfor the relation in premise and hypothesis, and had\nto formulate explanations that employ these key\nphrases. For ﬁne-tuning and testing our models,\nwe consider all instances labelled with entailment\nand contradiction relations (but do not include the\nlabels in ﬁne-tuning). We interpret premise and hy-\npothesis as our source sentences S1 and S2, the ex-\nplanation as our target sentence T, and the marked\nkey phrases as our key concepts c1 and c2.\nIn- and Out-Of-Domain Test Sets. We test the\nresulting models in-domain – on testsets from our\nﬁne-tuning corpora; and out-of-domain – on the\nIKAT dataset (Becker et al., 2020), which is based\non the argumentative Microtexts Corpus (Peldszus\nand Stede, 2015). For all sentence pairs S1 and S2\nthat are adjacent or argumentatively related, anno-\ntators added the implicit knowledge that connects\nthem, using simple sentences, which we use as tar-\ngets T. They also marked two key phrases in each\nimplicit knowledge sentence, where in most cases\none key phrase appears in the ﬁrst source sentence,\nand the other in the second – which we interpret as\nkey concepts c1 and c2 in our approach.\n3.4 Constraining Explanation Generation\nOur hypothesis is that unconditioned generation\nmay not be sufﬁcient to produce statements carry-\n14\nDataset Source Sentence 1 Source Sentence 2 Target Sentence Key Concepts Paths\nG-KB The patient is assessed\nfor pain every 4 hours.\nCombination of treatments\nare needed for effective\npain management.\nPain management\nis provided for pa-\ntients in pain.\npain manage-\nment, patients\npain manage-\nment USED FOR\npatients\neSNLI The city has a lot of peo-\nple in it.\nIt is busy city that looks\nlike New York City\nA city that is busy\nhas a lot of people\nin it.\na lot of people,\nbusy city\nbusy city HAS-\nPROPERTY a lot\nof people\nIKAT Education and training\nare fundamental rights\nwhich the state must pro-\nvide.\nIf a university lacks the\nfundings, sponsors must\nbe found.\nEducation and train-\ning must be funded.\neducation and\ntraining, fund-\nings\neducation and\ntraining HAS\nPREREQUISITE\nfundings\nTable 1: Source sentence pairs and target sentences (reference) from our three datasets, with marked key concepts and\nautomatically predicted knowledge paths between them.\nGen-KB\nBL Patients often report back to the clinic with a worsening pain condition within one to two hours of ﬁrst assessment.\n+c Patients often have few if any symptoms at ﬁrst, but pain becomes less intense and less frequent in coming hours.\n+p Patients are admitted to the hospital with moderate to high intensity pain.\ne-SNLI\nBL A busy city that looks like new york city has a lot of people in it, so the city has to have a lot to people in the city.\n+c The city has a lot of people in it because it is a busy city.\n+p A busy city implies that there are a lot of people in the city.\nIKAT\nBL The state and society must be found if a university lacks the funds to provide education and training.\n+c The state and the society must pay for education and training if the university lacks the funds.\n+p If a university lacks the funds, it can not be providing education and training to its students.\nTable 2: Example generations for pairs from Tab. 1, from BART: w/o constraints or constrained w/ concepts (c) or paths (p).\ning relevant knowledge which explains the connec-\ntion between two sentences. Hence we experiment\nwith direct injection of constraints or triggers to\nguide the generation to emit meaningful and coher-\nent implicit knowledge statements: We include (i)\nkey concepts as offered by each dataset, since we\nexpect them to direct the model towards concepts\nthat are relevant for explaining how the two sen-\ntences are related. We also include (ii) relational\nknowledge between the key concepts as constraints,\nby establishing multi-hop knowledge paths be-\ntween them. To this end we combine relation clas-\nsiﬁcation and target prediction models speciﬁcally\nadapted to ConceptNet. The two respective models\nare based on LMs ﬁne-tuned on ConceptNet (Speer\net al., 2017), a large network that represents com-\nmonsense facts.2 We generate single- and multihop\npaths between key concepts from a sentence pair,\nand use these paths as constraints when generating\ntarget sentences. We expect the generated paths to\nprovide useful relational information for the model.\nExample paths appear in Table 1.\n4 Data and Experimental Setup\nDatasets. We use the data from GenericsKB and\ne-SLNI for ﬁne-tuning and testing models (in-\n2Details about the models appear in the Appendix.\ndomain), and IKAT for testing out-of-domain.3 For\nstatistics see Table 3. All instances contain two\nsource sentences S1,2, a target sentence T, and two\nkey concepts c1,2, where c1∈S1, c2∈S2, and c1,2\n∈ T. We experiment with c1,2, and with paths p\ngenerated between c1 and c2 as constraints, which\nwe establish as explained above.\nInput Sequences. We build the input sequences\nby concatenating the source sentences S1 and S2,\nseparated by a SEP token. When including key con-\ncepts c1,2 or knowledge paths p as constraints, we\nappend them to the input sequence right after S1\nand S2, separated by a SEP token. Thus, the con-\ncepts and paths we use as constraints are encoded\nby the tokenizer of each language model together\nwith the rest of the input sequence. Accordingly,\nour input sequences are structured as follows:\nS1 <SEP> S2 <SEP> (c1, c2|p) <EOT> T.\nFine-tuning LMs. For LM ﬁne-tuning, we ap-\npend the target sentence to the input sequence, sep-\narated from the rest of the input by an EOT tag.\nGPT-2 and XLNet are trained to reconstruct the\ntarget sentence T. During inference, the models\nonly see the source sentences, and constraints if\n3In preliminary experiments we also tried to ﬁne-tune our\nLMs on GenericsKB and e-SNLI together, which did not im-\nprove results compared to when using these datasets separately\nfor ﬁne-tuning – most likely because the datasets are very dif-\nferent from each other in terms of linguistic characteristics\n(e.g. sentence lengths and structure) and the covered topics.\n15\ntrain dev test eval-1 eval-2\nG-KB 21,644 6,184 3,091 10 30\ne-SNLI 18,160 2,028 1,002 10 30\nIKAT - - 719 10 40\nTable 3: Datasets: Nb. of source sentence pairs with associ-\nated implicit knowledge sentences, used for ﬁne-tuning and\ntesting; and subsets from test used in evaluations.\ngiven, and they complete the input sequence by\ngenerating T. In contrast, BART encodes S1 and\nS2, and its decoder is trained to predict T based on\nthe encoded source sentences.\nWe use the pre-trained models from Hugging-\nFace Transformers (Wolf et al., 2019) and adapt\nthem for ﬁne-tuning on our customized training\ndata. In order to generate compact sentences cap-\nturing the relevant implicit knowledge (instead of\nlong explanations), we set a length limitation of 20\ntokens for each generation. More details about our\nmodels are listed in the Appendix.\n5 Evaluation and Results\nThis section presents an in-depth evaluation of\nthe quality of generations from different model\nvariants, and their ability of expressing implicitly\nconveyed knowledge. We design a manual evalua-\ntion setup covering various dimensions, and com-\npare the results to several automatic evaluation\nmetrics. We conduct evaluation in-domain on our\ncustomized test data; and out-of-domain on IKAT.\n5.1 Manual Evaluation\nQuestions to Annotators. 4 To ﬁlter out source\nsentence pairs between which no implicit infor-\nmation is missing, we ﬁrst ask the annotators for\neach source sentence pair if they areimplicitly con-\nnected by some (unexpressed) piece of knowledge\n(yes/no). The annotators are then guided through\nfollow-up questions covering four dimensions:\n(1) Grammaticality – we ask if the generated sen-\ntence is grammatically correct, given the choices\ncorrect, almost correct (minor grammatical errors),\nand incorrect (major grammatical errors);\n(2) Coherence – we ask if the generated sentence is\nlogically and semantically consistent with respect\nto the two source sentences, given the choices fully\ncoherent, partly coherent, or incoherent;\n(3) Content – we ask if the generated sentence\n4The annotation manual together with example annotations\ncan be found here: https://github.com/Heidelber\ng-NLP/LMs4Implicit-Knowledge-Generation/\nblob/main/manual.pdf\ngives an explanation of the connection between\nthe two source sentences, given the choices yes,\nneutral (if the generated sentence is related to the\nsource sentences, but not in a clear logical relation),\nand no (if the sentence is misleading or contradic-\ntory in the context of the source sentences); 5 (4)\nComparison to the annotated reference sentence\n6 – we ask if the generated sentence is similar in\nmeaning to the reference, given the choices similar,\npartly similar, or not similar. In addition, we ask\nif the reference sentence or the generated sentence\nis a more meaningful explanation of the implicit\nknowledge that connects the source sentences, or\nif both are equally meaningful explanations.\nAnnotation Setup. Our goal is to investigate\nwhich model variant is best suited for generating\ngrammatically sound, coherent and meaningful ex-\nplanations. We approach this question with two\nannotation rounds: In a ﬁrst round we aim to de-\ntermine which model is best suited for generating\nimplicitly conveyed knowledge, and whichdataset\nis best suited for ﬁne-tuning the model for gen-\nerating statements on out-of-domain test sets. In\na second annotation round we aim to determine\nwhich types of constraints yield best results, now\nrestricted to the best performing model and training\nsetup, as determined in round one.\nAnnotator Agreement. Annotation was per-\nformed by two annotators with a background in\ncomputational linguistics. We measure IAA using\nCohen’s Kappa, combined over round one and two,\nand achieve an agreement of 95% on dimension 1,\n80% on 2, 77% on 3, and on dimension 4 82% for\nthe ﬁrst and 78% for the second question. Remain-\ning conﬂicts were resolved by an expert annotator.\n5.1.1 Best Model Type and Fine-Tuning Data\nFor the ﬁrst annotation round we sample 10 source\nsentence pairs from each testset, hence 30 pairs\noverall, and the sentences generated by GPT-2, XL-\nNet and BART for each instance, using concepts as\n5The difference between dimension 2 and 3 is that with\ndimension 2 (coherence), we want to explore if the generated\nsentence semantically ﬁts to the two given source sentences.\nWe understand coherence together with Hobbs (1979) as the\nexistence of speciﬁc knowledge relations that hold between\nconcepts in a text (or discourse), such as Cause-Effect, Con-\ndition, or Temporal Sequence, cf. Wolf and Gibson (2004).\nThese relations make the texts interpretable and informative\nand are motivated ultimately by the speaker’s or writer’s need\nto be understood (Hobbs, 1979). In contrast, when evaluat-\ning the content of the generated sentence in dimension 3, we\nwant to discover if the sentence really explains the connection\nbetween the two source sentences.\n6The reference sentence is only provided for Question 4.\n16\nFigure 1: Example generations for the IKAT test set, for all three models, ﬁne-tuned on e-SNLI vs. GenericsKB, with concepts\nvs. paths as constraints.\nconstraints. For IKAT, we consider the sentences\ngenerated by each model ﬁne-tuned on e-SNLI\nvs. GenericsKB. This sums up to 120 annotation\nsamples (generated sentences).7 In Fig. 1 we give\nexample generations for IKAT, for all three model\ntypes, comparing ﬁne-tuning on e-SNLI vs. Gener-\nicsKB; and constraining with concepts vs. with\npaths. More examples appear in the Appendix.\nResults. For all 30 sentence pairs the annota-\ntors agreed that there is some implicit information\nconnecting them. Table 4 displays the results of\nthe ﬁrst annotation round for the four dimensions\ndescribed above. All three models are able to gener-\nate grammatically correct sentences (col. 1), with\nBART’s generations scored as correct most often.\nBART also generates the most coherent sentences\n(col. 2), in-domain (e-SNLI and GenericsKB) and\nout-of-domain (IKAT), followed by XLNet. For\ndimension 3, which evaluates whether the gener-\nations are meaningful explanations of implicit\nknowledge connecting the source sentences (col.\n3), only BART ﬁne-tuned on e-SNLI gives satisfac-\ntory results (in-domain, when ﬁne-tuned and tested\non e-SNLI; and out-of domain, when ﬁne-tuned on\n730 generated sents for e-SNLI and GenericsKB, resp. (10\nsource sents x 3 models), and 60 generated sents for IKAT (10\nsource sents x 3 models x 2 different ﬁne-tuning datasets).\ne-SNLI and tested on IKAT). Many of the genera-\ntions from GPT-2 are judged as neutral (orange in\nTable 4) or misleading (red). The last two columns\nreﬂect the comparison of the generated vs. anno-\ntated reference sentence (dimension 4). BART’s\ngenerations are overall rated as most similar to the\nreference sentence, especially when ﬁne-tuned on\ne-SNLI (in- and out-of-domain), and are judged as\nbetter or equally good explanations compared to the\nreference sentences in 70% (e-SNLI, in-domain)\nand 50% (IKAT–e-SNLI, out-of-domain).\nTo summarize, according to our ﬁrst round of\nevaluation, the BART model generates the most\ngrammatical and coherent statements that are found\nto explain the connection between the source sen-\ntences best. They are also judged to be most sim-\nilar to the reference sentence. When applied on\nout-of-domain testsets, BART performs best when\nﬁne-tuned on e-SNLI.\n5.1.2 Best Constraints\nWhile the ﬁrst round of annotations used a rela-\ntively small set of 120 generated target sentences\nthat helped us to determine BART as the best-suited\nmodel type, we now aim to deeper investigate the\ngenerations of BART to study the effect of differ-\nent types of constraints on the quality of expla-\n17\nDIMENSION Grammaticality Coherence Explanation Sim. to Reference Gen. vs. Ref.\nCHOICES Yes/Almost/No Yes/Partly/No Yes/Neutral/No Yes/Partly/No GS/Both/RS\nGPT-2 e-SNLI 60/30/10 30/20/50 60/20/20 40/20/40 20/20/60\nG-KB 100/0/0 40/50/10 30/70/0 20/40/40 0/20/ 80\nIKAT - e-SNLI 70/10/20 20/30/50 20/80/0 20/60/20 0/40/60\nIKAT - G-KB 100/0/0 40/50/10 20/60/20 20/20/60 10/10/80\nXLNet e-SNLI 90/10/0 60/20/20 60/20/20 60/20/20 30/30/40\nG-KB 90/10/0 40/50/10 50/50/0 0/60/40 20/10/70\nIKAT - e-SNLI 80/20/0 60/20/20 50/40/10 30/60/10 0/40/60\nIKAT - G-KB 90/0/10 20/80/0 50/30/20 10/20/70 0/10/90\nBART e-SNLI 100/0/0 100/0/0 100/0/0 80/20/0 40/30/30\nG-KB 100/0/0 40/60/0 40/60/0 20/80/0 20/10/70\nIKAT - e-SNLI 90/0/0 60/40/0 70/20/10 60/30/10 20/30/50\nIKAT - G-KB 100/0/0 50/50/0 50/40/10 50/40/10 40/0/60\nTable 4: Results of the 1st manual evaluation (in %). For all 10 source sentence pairs, each model generates a target sentence\nwhen ﬁne-tuned and tested in-domain on (i) e-SNLI and (ii) GenericsKB; or out-of-domain testing on IKAT, when ﬁne-tuned on\n(iii) e-SNLI or (iv) GenericsKB; with marked best/worst scores for in- and out-of domain testing.\nnations. We provide our annotators with 70 new\nsource sentence pairs (20 from e-SNLI, 20 from\nGenericsKB, 30 from IKAT), and three different tar-\ngets per pair, generated by three model variants of\nBART: (i) a baseline ﬁne-tuned without any knowl-\nedge constraints; (ii) BART ﬁne-tuned using the\nkey concepts as constraints; and (iii) BART ﬁne-\ntuned using an automatically generated common-\nsense knowledge path between the key concepts\nas constraint. Since ﬁne-tuning on e-SNLI has\nbeen determined as best suited for out-of-domain\ntesting, we consider only generations from BART\nﬁne-tuned on e-SNLI for testing on IKAT. In our\nevaluation we consider the 70 sentence pairs and\nthe respective sentence generations from Round\n2, and the generations for the 30 source sentence\npairs from the best performing model BART from\nRound 1, resulting in 100 sentence pairs, with three\ngenerations per pair.\nResults. Similar to Round 1, for 98% of the\nsource sentence pairs the annotators agreed that\nthere is some implicit information connecting them.\nFig. 2 shows the results of the second round\nof evaluations, example generations appear in Ta-\nble 2. We ﬁnd that using knowledge constraints\nimproves the quality of generations compared to\nthe baseline without constraints, on all four dimen-\nsions: on each of our three test sets, generations\nare rated as more grammatical when constrained\nwith concepts and paths (with GenericsKB as only\nexception); they are annotated as more coherent,\nand rated as better explanations of implicit knowl-\nedge. Knowledge constraints also lead to a higher\nsimilarity to the reference sentence on all three\ndatasets, and sentences generated with knowledge\nconstraints are more often rated as better explana-\ntions than the reference sentences. Overall we ﬁnd\nthat knowledge paths improve scores over the base-\nline more than concepts (a plus of 2–15 pp). The\nimprovements are most signiﬁcant for IKAT, where\nadding concepts boosts evaluation scores between\n18 (Grammaticality) and 53 pp (Coherence), and\nadding paths by 20 (Grammaticality) and 55 pp\n(Coherence). The generations of BART, ﬁne-tuned\non e-SNLI, as shown in the ﬁrst example in Fig. 1,\ndemonstrate how the integration of paths as con-\nstraints can improve text generation even more than\nwhen only injecting key concepts. The path used\nas constraint is Germany’s aging societyCAUSES\nincreasing costs. When constraining BART with\nkey concepts, it generates The social security and\npension costs are being paid for by the people of\nGermany, while the generation with the knowledge\npath as constraint is Social security and pension\ncosts are rising because more pension is needed for\nelderly people in Germany). This shows that the\nrelation CAUSES gives our model an important hint\nabout the causal relation that is needed to explain\nthe connection between the two given sentences.\nTo summarize, the results from our second eval-\nuation round clearly show that constraints in form\nof relevant concepts and knowledge paths can help\nLMs for generating grammatically sound, coherent\nand meaningful explanations of the missing knowl-\nedge between sentences, especially when applied\non out-of-domain test sets.\n5.2 Automatic Evaluation\nIn our automatic evaluation setup, we apply a range\nof different evaluation metrics commonly applied\nin text generation tasks, which either measure the\nsimilarity to a reference sentence (in our case, the\n18\nFigure 2: Results of 2nd manual evaluation: comparing models constrained with concepts (+c) or paths (+p) against a baseline\nwithout constraints. We display improvements in percentage points (pp) for the best option (blue bar) per dimension.\ngeneric sentences in GenericsKB, inference expla-\nnations in e-SNLI, or implicit knowledge state-\nments in IKAT); or the linguistic quality and di-\nversity of the generated sentence.\n(i) BLEU (Papineni et al., 2002) and ROUGE\n(Lin, 2004) measure token overlap using ngrams.\nWe apply BLEU-1 to measure precision and\nROUGE-1 to measure recall based on unigrams;\n(ii) BERT-Score (Zhang* et al., 2020) and\nSentence-BERT (Reimers and Gurevych, 2019)\ncompute semantic similarity scores for text se-\nquences based on word or sentence representations.\nBERT-Score uses BERT’s contextualized word em-\nbeddings to calculate a cross similarity score for\neach token in the generation with each token in the\nreference, while Sentence-BERT is ﬁne-tuned on\nNLI and STS to predict the similarity of two se-\nquences. For BERT-Score we report F1 scores; for\nSentence-BERT we average the similarity scores\nobtained for the generated vs. reference sentences.\n(iii) S2Match (Opitz et al., 2020) is an AMR\ngraph matching metric, which measures the overlap\nof the AMR semantic graphs that we construct from\nthe reference and generated sentence using Cai and\nLam (2020)’s parser, and reports accuracy;\n(iv) Distinct-N (Li et al., 2015) and GRUEN\n(Zhu and Bhat, 2020) arereference-freemetrics that\nonly consider properties of the generated sentence.\nDistinct-N measures the diversity of a sentence\nby focusing on the number of distinct unigrams\n(Distinct-1) and bigrams (Distinct-2); GRUEN eval-\nuates the linguistic quality of a sentence in terms\nof grammaticality, non-redundancy, and structure.\nIn a preliminary experiment based on the com-\nplete test sets of Generics-KB, e-SNLI and IKAT\n(cf. Table 3) we ﬁrst investigate which model gen-\nerates sentences that are most similar to the refer-\nence sentence (using reference-based metrics), or\nwhich show highest linguistic quality and diversity\n(using reference-free metrics); and which dataset\nis best suited for ﬁne-tuning the models for gener-\nBLEU-1\nROU-1\nS2M\nBERT\nS-BERT\ndist1\ndist2\nGRUEN\ne-SNLI 7.27 0.4 0.34 0.89 0.56 0.72 0.58 0.63\ne-SNLI+c 12.71 0.47 0.38 0.90 0.63 0.75 0.66 0.63\ne-SNLI+p 9.51 0.48 0.39 0.89 0.65 0.76 0.67 0.66\nG-KB 1.22 0.15 0.31 0.88 0.53 0.71 0.62 0.82\nG-KB+c 1.58 0.18 0.32 0.88 0.54 0.72 0.66 0.83\nG-KB+p 1.14 0.17 0.31 0.89 0.56 0.73 0.67 0.80\nIKAT 4.6 0.22 0.33 0.88 0.49 0.70 0.64 0.66\nIKAT+c 6.06 0.31 0.42 0.90 0.63 0.72 0.67 0.71\nIKAT+p 7.23 0.33 0.46 0.91 0.64 0.74 0.70 0.76\nTable 5: Automatic similarity scores for generations of\nbest performing model BART, w/o constraints or with con-\ncepts/paths as constraints. Adding concepts and paths im-\nproves scores in-domain (e-SNLI and Generics-KB), and out-\nof-domain (IKAT ﬁnetuned on e-SLNI).\nating statements on out-of-domain test sets (here,\nIKAT). Results and detailed analysis of this experi-\nment appear in our Appendix. We ﬁnd that decid-\ning which model performs best depends a lot on the\nchosen similarity metric, but overall we don’t see\nthe clear superiority of the BART model (nor the\ninferiority of GPT-2) that we determined through\nmanual evaluation. While in Dimension 4 of the\nmanual evaluation setup (where annotators judged\nwhether generated and reference sentence express\nthe same or similar meaning), BART was clearly\nrated as the best performing model, this is not re-\nﬂected in the automatic evaluation scores. Among\nall metrics only SentenceBERT, giving highest\nscores to BART, followed by XLNet, aligns with\nour observations from manual evaluation. How-\never, our other observation from manual evaluation\n– that e-SNLI is the most appropriate dataset for\nﬁne-tuing LMs for out-of-domain testing — aligns\nwith the scores obtained by automatic evaluation\nmetrics (for details, cf. Appendix).\nWe next analyse which types of constraints im-\nprove generation, focusing on the BART model,\nwhich has shown to be best for generating im-\nplicit knowledge statements in our manual eval-\nuation setup. Our automatic evaluation is based\n19\non the same subset of source sentence pairs used\nfor the second round of manual annotations (cf.\nTable 3), and we again compare generations with-\nout constraints to conditioning on key concepts or\nknowledge paths.8 Results are displayed in Table\n5. We observe that for all metrics, scores increase\nwhen constraining LMs with concepts or knowl-\nedge paths, with BLEU and S2Match scores for\nGenericsKB as only exceptions. As in manual eval-\nuation (Fig. 1), we ﬁnd that improvements are most\nsigniﬁcant for IKAT. The observed improvements\nmay in part be traced back to increased word over-\nlap due to key concepts being used as constraints.\nYet we also observe that automatically generated\nknowledge paths between these concepts improve\nscores additionally – according to reference-based\nmetrics (showing that generations become more\nsimilar to references), and reference-free metrics\n(showing improvement of the linguistic quality and\ndiversity of generations). This points to the fact\nthat constraining LMs with automatically generated\nrelational knowledge is a promising step towards\ngenerating grammatically correct and meaningful\nimplicit knowledge statements.\n6 Discussion\nLimitations of Automatic Evaluation Metrics\nfor Text Generations. Concluding, we pinpoint\ntwo important limitations of automatic text genera-\ntions metrics – especially reference-based ones: Be-\nsides well-known issues regarding the reliability, in-\nterpretability and biases of such metrics (Callison-\nBurch et al., 2006), scores are mostly obtained by\ncomparing generations against a single reference,\nwhich is – here, as in other generation tasks – often\nonly one among several valid options. For the task\nof reconstructing implicit information, Becker et al.\n(2017) show that annotators often propose differ-\nent valid sentences for ﬁlling knowledge gaps in\nargumentative texts. For our setting this means that\na generated sentence may be a relevant explicita-\ntion of implicit information, even if not similar to\nthe reference. Such cases are poorly or not at all\ncaptured by automatic similarity metrics. An ex-\nception we found is SentenceBERT, which is based\non sentence representations, and which aligned rea-\nsonably well with insights from our manual evalua-\ntion. Still, automatic evaluation of text generations\n8The automatic evaluation scores for the complete test\nsets, which conﬁrm our ﬁndings from the subset of the second\nannotation round, appear in the Appendix.\nneeds to be considered with caution, and should\nalways be accompanied by manual evaluation.\nOur Implicitness Assumption. Our experi-\nments are based on the underlying assumption that\nusually some information between pairs of sen-\ntences stays implicit, which has been conﬁrmed\nempirically for our datasets: Our annotators stated\nfor 100% (ﬁrst round) and 98% (second round)\nof all sentence pairs that they are implicitly con-\nnected by some unexpressed piece of knowledge.\nHowever, we did not speciﬁcally address the cases\nof sentence pairs between which no implicit in-\nformation is missing (even though these cases are\nrare), nor did we investigate how our models would\nperform when provided with sentence pairs that\nare not related (arbitrary pairs). For a real-world\napplication, both aspects would be considerable.\n7 Conclusion\nIn this work we propose an approach for generat-\ning statements that explicate implicit knowledge\nconnecting sentences in text, using pre-trained\nLMs. We show that despite their great success\nin many NLP downstream tasks, LMs need to be\nwell equipped and carefully guided for the chal-\nlenging task of reconstructing implicit knowledge,\nto ensure that they convey the missing, implicit in-\nformation that connects sentences in text. We reﬁne\ndifferent pre-trained LMs by ﬁne-tuning on speciﬁ-\ncally prepared corpora that we enrich with implicit\ninformation, ﬁlled in between sentences, and ex-\nplore methods of constrained language generation,\nguiding the models by way of relevant concepts\nand connecting commonsense knowledge paths.\nWhile most current automatic NLG metrics are\nnot sufﬁcient to evaluate this challenging task, our\nin-depth evaluation of the quality of generations\nfrom different model variants shows that the BART\nmodel, which attends over its full input when gen-\nerating text, yields most informative and relevant\nexplanations. We also establish that e-SNLI, being\nfocused on the NLI task, is best suited for condi-\ntioning LMs for our task, especially for out-of do-\nmain settings. Finally, by providing the LMs with\nrelevant connecting key concepts as constraints,\nand further by connecting commonsense knowl-\nedge paths, we achieve generation of coherent and\ngrammatically sound sentences that – according\nto manual evaluation – can explicate the implicit\nknowledge that connects sentence pairs in texts –\nfor in-domain and out-of-domain test data.\n20\nReferences\nMaria Becker, Katharina Korfhage, and Anette Frank.\n2020. Implicit Knowledge in Argumentative Texts:\nAn Annotated Corpus. In Proceedings of the 12th\nConference on Language Resources and Evaluation\n(LREC), pages 2316–2324, Marseille, France.\nMaria Becker, Michael Staniek, Vivi Nastase, and\nAnette Frank. 2017. Enriching Argumentative Texts\nwith Implicit Knowledge. In Applications of Natu-\nral Language to Data Bases (NLDB) - Natural Lan-\nguage Processing and Information Systems, Lecture\nNotes in Computer Science. Springer.\nMaria Becker, Michael Staniek, Vivi Nastase, and\nAnette Frank. 2019. Assessing the difﬁculty of\nclassifying ConceptNet relations in a multi-label\nclassiﬁcation setting. In RELATIONS - Workshop\non meaning relations between phrases and sen-\ntences, Gothenburg, Sweden. Association for Com-\nputational Linguistics.\nSumithra Bhakthavatsalam, Chloe Anastasiades, and\nPeter Clark. 2020. GenericsKB: A Knowledge Base\nof Generic Statements. In Arxiv Preprint.\nAntoine Bosselut, Ronan Le Bras, , and Yejin Choi.\n2021. Dynamic neuro-symbolic knowledge graph\nconstruction for zero-shot commonsense question\nanswering. In Proceedings of the 35th AAAI Con-\nference on Artiﬁcial Intelligence (AAAI).\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chai-\ntanya Malaviya, Celikyilmaz Asli, and Choi Yejin.\n2019. Comet: Commonsense transformers for auto-\nmatic knowledge graph construction. In ACL, pages\n4762–4779.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632–642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nDeng Cai and Wai Lam. 2020. Amr parsing via graph-\nsequence iterative inference.\nJamie Callan, Mark Hoy, Changkuk Yoo, and Le Zhao.\n2009. Clueweb09 data set.\nChris Callison-Burch, Miles Osborne, and Philipp\nKoehn. 2006. Re-evaluating the role of Bleu in ma-\nchine translation research. In 11th Conference of\nthe European Chapter of the Association for Com-\nputational Linguistics, Trento, Italy. Association for\nComputational Linguistics.\nOana-Maria Camburu, Tim Rockt ¨aschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Nat-\nural language inference with natural language expla-\nnations. In Advances in Neural Information Process-\ning Systems, volume 31, pages 9539–9549. Curran\nAssociates, Inc.\nTing-Yun Chang, Yang Liu, Karthik Gopalakrishnan,\nBehnam Hedayatnia, Pei Zhou, and Dilek Hakkani-\nTur. 2020. Incorporating commonsense knowledge\ngraph in pretrained models for social commonsense\ntasks. In Proceedings of Deep Learning Inside Out\n(DeeLIO): The First Workshop on Knowledge Ex-\ntraction and Integration for Deep Learning Architec-\ntures, pages 74–79, Online. Association for Compu-\ntational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning.\nJian Guan, Fei Huang, Zhihao Zhao, Xiaoyan Zhu, and\nMinlie Huang. 2020. A knowledge-enhanced pre-\ntraining model for commonsense story generation.\nTransactions of the Association for Computational\nLinguistics, 8:93–108.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefen-\nstette, Lasse Espeholt, Will Kay, Mustafa Suleyman,\nand Phil Blunsom. 2015. Teaching machines to read\nand comprehend. In NIPS.\nJerry R. Hobbs. 1979. Coherence and coreference*.\nCognitive Science, 3(1):67–90.\nHaozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, and Min-\nlie Huang. 2020a. Generating commonsense expla-\nnation by extracting bridge concepts from reasoning\npaths. In Proceedings of the 1st Conference of the\nAsia-Paciﬁc Chapter of the Association for Compu-\ntational Linguistics and the 10th International Joint\nConference on Natural Language Processing, pages\n248–257, Suzhou, China. Association for Computa-\ntional Linguistics.\nHaozhe Ji, Pei Ke, Shaohan Huang, Furu Wei, Xiaoyan\nZhu, and Minlie Huang. 2020b. Language gen-\neration with multi-hop reasoning on commonsense\nknowledge graph. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 725–736, Online. Asso-\nciation for Computational Linguistics.\nJosef Jon, Martin Fajcik, Martin Docekal, and Pavel\nSmrz. 2020. BUT-FIT at SemEval-2020 task 4:\nMultilingual commonsense. In Proceedings of the\nFourteenth Workshop on Semantic Evaluation, pages\n374–390, Barcelona (online). International Commit-\ntee for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2019. Bart: Denoising sequence-to-sequence pre-\ntraining for natural language generation, trans-\nlation, and comprehension. arXiv preprint\narXiv:1910.13461.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2015. A diversity-promoting objec-\ntive function for neural conversation models. CoRR,\nabs/1510.03055.\n21\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nChin-Yew Lin. 2004. ROUGE: A package for auto-\nmatic evaluation of summaries. In Text Summariza-\ntion Branches Out , pages 74–81, Barcelona, Spain.\nAssociation for Computational Linguistics.\nJuri Opitz, Anette Frank, and Letitia Parcalabescu.\n2020. Amr similarity metrics from principles.\nTransactions of the Association for Computational\nLinguistics, 8(0):522–538.\nEyal Orbach and Yoav Goldberg. 2020. Facts2Story:\nControlling text generation by key facts. InProceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 2329–2345, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei\njing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. pages 311–318.\nAndreas Peldszus and Manfred Stede. 2015. An an-\nnotated corpus of argumentative microtexts. In Pro-\nceedings of the First European Conference on Argu-\nmentation.\nAnandh Perumal, Chenyang Huang, Amine Trabelsi,\nand Osmar Za¨ıane. 2020. Ana at semeval-2020 task\n4: multi-task learning for commonsense reasoning\n(union).\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nbert: Sentence embeddings using siamese bert-\nnetworks. CoRR, abs/1908.10084.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan Le-\nBras, and Yejin Choi. 2019. Socialiqa: Common-\nsense reasoning about social interactions. ArXiv,\nabs/1904.09728.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptNet 5.5: An Open Multilingual Graph of\nGeneral Knowledge. In AAAI.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nCunxiang Wang, Shuailong Liang, Yili Jin, Yi-\nlong Wang, Xiaodan Zhu, and Yue Zhang. 2020a.\nSemEval-2020 task 4: Commonsense validation\nand explanation. In Proceedings of the Four-\nteenth Workshop on Semantic Evaluation , pages\n307–321, Barcelona (online). International Commit-\ntee for Computational Linguistics.\nPeifeng Wang, Nanyun Peng, Filip Ilievski, Pedro\nSzekely, and Xiang Ren. 2020b. Connecting the\ndots: A knowledgeable path generator for common-\nsense question answering. Findings of the Associa-\ntion for Computational Linguistics: EMNLP 2020 ,\npages 4129–4140.\nFlorian Wolf and Edward Gibson. 2004. Represent-\ning discourse coherence: A corpus-based analysis.\nIn COLING 2004: Proceedings of the 20th Inter-\nnational Conference on Computational Linguistics ,\npages 134–140, Geneva, Switzerland. COLING.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R ´emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. CoRR, abs/1910.03771.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le.\n2019a. Xlnet: Generalized autoregressive pretrain-\ning for language understanding. In Advances in Neu-\nral Information Processing Systems, volume 32. Cur-\nran Associates, Inc.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G.\nCarbonell, Ruslan Salakhutdinov, and Quoc V .\nLe. 2019b. Xlnet: Generalized autoregressive\npretraining for language understanding. CoRR,\nabs/1906.08237.\nTianyi Zhang*, Varsha Kishore*, Felix Wu*, Kilian Q.\nWeinberger, and Yoav Artzi. 2020. Bertscore: Eval-\nuating text generation with bert. In International\nConference on Learning Representations.\nHao Zhou, Tom Young, Minlie Huang, Haizhou\nZhao, J. Xu, and Xiaoyan Zhu. 2018. Com-\nmonsense knowledge aware conversation generation\nwith graph attention. In IJCAI.\nWanzheng Zhu and Suma Bhat. 2020. GRUEN for\nevaluating linguistic quality of generated text. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 94–108, Online. As-\nsociation for Computational Linguistics.\nA Training Details\nFinetuning Language Models. Details about the\nmodels and ﬁne-tuning procedure as well as the\nrunning time for one batch are listed in Table 6.\nWe ﬁne-tuned all models with 2 GPUs on 3 epochs.\nOur training batch size is 8 as suggested by the Hug-\ngingFace’s Transformers framework (Wolf et al.,\n2019). GPT-2 is the lightest one of our three models\nand takes 4 hours for ﬁne-tuning on our e-SNLI and\n22\nGenericsKB datasets, respectively, while BART re-\nquires 8 hours, and XLNet around 20 hours (due to\nits permutation procedure) for the same data.\nLimiting Length of Generations. In order to\ngenerate compact sentences capturing the relevant\nimplicit knowledge (instead of long explanations),\nwe set a length limitation of 20 tokens for each\ngeneration. In the left-to-right decoding procedure\nof GPT-2 and BART, the generation can be stopped\nearlier than 20 tokens, when the model predicts an\nEOT token. Thus, both GPT-2 and BART models\ncan predict complete sentences of up to 20 tokens\ndue to the autoregressive decoder. In contrast, XL-\nNet has a permutation language modeling mech-\nanism and predicts the next tokens based on the\nprevious and next tokens. Its generations usually\ndon’t contain a signiﬁcant EOT token. predicted\ntarget sequence of tokens in a post-processing step\nby cutting it after a generated comma (,).\nMaximum Sequence Lengths. Our customized\ntrain sets have different maximum sequence\nlengths: e-SNLI has a maximum sequence length\nof 80 tokens including the target sentence, while\nGenericsKB has up to 140 tokens per sequence.\nB Establishing Knowledge Paths for\nConstraining Text Generation\nFor dynamically establishing connections between\nthe key concepts from two source sentences, we\ncombine two model types: COREC-LM (Becker\net al., 2019), an open-world multi-label relation\nclassiﬁer enhanced with a pretrained language\nmodel, that predicts relation types between two\ngiven concepts – for establishing direct connections\nbetween concepts; and COMET (Bosselut et al.,\n2019), a pretrained transformer model that learns\nto generate target concepts given a source concept\nand a relation, for generating multihop paths. By\ncombining the generations of these models, we\ngenerate single- and multihop paths between key\nconcepts c1, c2 from a sentence pair, and use these\npaths as constraints when generating target sen-\ntences. We are able to retrieve paths for 86.2%\nof all key concept pairs from GenericsKB, respec-\ntively, for 30.2% from e-SNLI and for 44.2% from\nIKAT. The differences can be explained by the fact\nthat while the key concepts in GenericsKB are ex-\ntracted phrases (NPs, VPs, ADJPs and ADVPs),\nthe key concepts in e-SNLI and IKAT are manu-\nally labelled, and thus are often very speciﬁc and\ncontain nested phrases (e.g. leans over a pickup\ntruck (e-SNLI)). Therefore, it is more difﬁcult to\npredict a relation or path between them. When\nwe experiment with paths as constraints; for all in-\nstances where no path could be established between\nthe key concepts, we only use the key concepts as\nconstraints.\nC Automatic Evaluation of the Complete\nTest Sets\nAs mentioned in Section 5.2 of our main paper,\nin a preliminary study based on the complete test\nsets of Generics-KB, e-SNLI and IKAT, we inves-\ntigate which model generated sentences that are\nmost similar to the reference sentence, or which\nshow highest linguistic quality and diversity; and\nwhich dataset is best suited for ﬁnetuning the mod-\nels for generating statements on out-of-domain test\nsets (here, IKAT). Results for this ﬁrst analysis ap-\npear in Table 7. For metrics that measure token\noverlap (BLEU and ROUGE), highest scores are\nobtained when ﬁnetuning and testing on e-SNLI,\nwhich can be traced back to frequently used linguis-\ntic patterns (e.g., x implies y, or x is the same as\ny) that occur in train and test sets of e-SNLI. The\nreference-free metrics Distinct and GRUEN that\nmeasure diversity and non-redundancy, therefore\nyield higher scores when models are ﬁnetuned on\nthe more diverse GenericsKB data, for both in- and\nout-of-domain testing. The AMR metric S2Match\ngives higher scores on e-SNLI than GenericsKB\nin in-domain testing, and ﬁnetuning on e-SNLI\nyields higher S2Match scores for out-of-domain\ntesting on IKAT. This also aligns with the sen-\ntence representation based metric SentenceBERT.\nBertScore, ﬁnally, is not at all discriminative – it\nyields uniformly high scores for each model and\nconﬁguration, ranging only between .88 and .9.\nWe also ﬁnd that the scores differ considerably\nfor in-domain vs. out-of-domain testing: results\non IKAT are lower compared to testing on e-SNLI\nor GenericsKB according to all reference-based\nmetrics, while we observe the opposite for the\nreference-free metrics.\nWe next analyse on the complete test set which\ntypes of constraints improve generation, focusing\non the BART model, which has shown to be best\nfor generating implicit knowledge statements in our\nmanual evaluation setup. The automatic evaluation\nscores for the complete test sets are displayed in\nTable 8 and conﬁrm our ﬁndings from the subset\nof the second annotation round, as presented in\n23\nPretrained model ID Model details Parameters Time in s(seq\nlength = 80)\nTime in s(seq\nlength = 140)\ngpt2 12-layer, 768-hidden, 12-heads 117M 0.039 0.056\nxlnet-large-case 24-layer, 1024-hidden, 16-heads 340M 0.166 0.297\nfacebook/bart-large-cnn 24-layer, 1024-hidden, 16-heads 406M 0.075 0.116\nTable 6: Benchmarks of the used pre-trained models.\nSection 5.2 of our main paper.\nD Example Generations\nIn addition to the examples shown in our main\npaper, in Fig. 1 we give some more example gen-\nerations for the IKAT test set, for all three model\ntypes, comparing ﬁnetuning on e-SNLI vs. Gener-\nicsKB; and constraining with concepts vs. with\npaths.\nTEST TRAIN\nBLEU-1\nROU-1\nS2M\nBERT\nS-BERT\ndist1\ndist2\nGRUEN\nGPT-2\nG-KB G-KB 5.3 .2 .33 .88 .5 .95 .89 .79\ne-SNLI e-SNLI 14.9 .46 .44 .89 .58 .91 .86 .52\nIKAT G-KB 2.9 .19 .3 .88 .45 .96 .85 .78\nIKAT e-SNLI 4.7 .26 .37 .89 .51 .88 .86 .64\nXLNet\nG-KB G-KB 6.6 .27 .36 .89 .53 .92 .87 .74\ne-SNLI e-SNLI 10.7 .43 .38 .89 .59 .88 .85 .58\nIKAT G-KB 4.2 .22 .34 .9 .48 .97 .88 .79\nIKAT e-SNLI 10.5 .33 .42 .9 .56 .9 .85 .69\nBART\nG-KB G-KB 5.2 .27 .35 .89 .57 .86 .93 .75\ne-SNLI e-SNLI 10.7 .44 .42 .89 .61 .81 .91 .59\nIKAT G-KB 2.37 .22 .3 .88 .53 .88 .93 .80\nIKAT e-SNLI 3.92 .29 .38 .9 .58 .87 .93 .71\nTable 7: Automatic Similarity scores computed for the gen-\nerations of all models, on the complete test sets. We compare\nthe impact of (i) model types and (ii) data used for ﬁnetun-\ning (train), in-domain (GenericsKB and e-SNLI) and out-of-\ndomain (IKAT).\nBLEU-1\nROU-1\nS2M\nBERT\nS-BERT\ndist1\ndist2\nGRUEN\ne-SNLI 7.36 0.37 0.36 0.88 0.54 0.77 0.89 0.59\ne-SNLI+c 10.73 0.44 0.42 0.89 0.61 0.81 0.91 0.59\ne-SNLI+p 11.71 0.44 0.43 0.89 0.62 0.84 0.92 0.59\nG-KB 5.21 0.23 0.32 0.88 0.55 0.86 0.93 0.75\nG-KB+c 5.2 0.27 0.35 0.89 0.57 0.86 0.93 0.75\nG-KB+p 5.4 0.28 0.35 0.89 0.58 0.87 0.93 0.75\nIKAT 2,74 0.19 0.29 0.87 0.43 0.86 0.92 0.67\nIKAT+c 3.92 0.28 0.38 0.89 0.56 0.87 0.92 0.7\nIKAT+p 4.84 0.3 0.4 0.9 0.57 0.9 0.93 0.72\nTable 8: Automatic similarity scores for generations of best\nperforming model BART on the complete test sets, w/o con-\nstraints or with concepts/paths as constraints. Adding concepts\nand paths improves scores in-domain (e-SNLI and Generics-\nKB), and out-of-domain (IKAT ﬁnetuned on e-SLNI).\n24\nFigure 3: Example generations for IKAT, for all three models, ﬁnetuned on e-SNLI vs. GenericsKB, with concepts vs. paths as\nconstraints.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8461695909500122
    },
    {
      "name": "Natural language processing",
      "score": 0.7202825546264648
    },
    {
      "name": "Sentence",
      "score": 0.6600643396377563
    },
    {
      "name": "Language model",
      "score": 0.5981159806251526
    },
    {
      "name": "Commonsense knowledge",
      "score": 0.5827248096466064
    },
    {
      "name": "Artificial intelligence",
      "score": 0.570634663105011
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.5385412573814392
    },
    {
      "name": "Implicit knowledge",
      "score": 0.5307246446609497
    },
    {
      "name": "Language understanding",
      "score": 0.46868330240249634
    },
    {
      "name": "Domain knowledge",
      "score": 0.4675297737121582
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Knowledge management",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I223822909",
      "name": "Heidelberg University",
      "country": "DE"
    },
    {
      "id": "https://openalex.org/I85384741",
      "name": "Heidelberg University",
      "country": "US"
    }
  ]
}