{
  "title": "Exploring Large-Scale Language Models to Evaluate EEG-Based Multimodal Data for Mental Health",
  "url": "https://openalex.org/W4402427685",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1824246431",
      "name": "Hu, Yongquan",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A51008091",
      "name": "Zhang Shuning",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2352349051",
      "name": "Dang Ting",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2105455488",
      "name": "Jia Hong",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A4225478810",
      "name": "Salim, Flora D.",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A2089848430",
      "name": "Hu Wen",
      "affiliations": [
        "UNSW Sydney"
      ]
    },
    {
      "id": "https://openalex.org/A4284591055",
      "name": "Quigley, Aaron J",
      "affiliations": [
        "Commonwealth Scientific and Industrial Research Organisation",
        "Data61"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3091936762",
    "https://openalex.org/W4283172211",
    "https://openalex.org/W4224247713",
    "https://openalex.org/W4210662381",
    "https://openalex.org/W2340346722",
    "https://openalex.org/W3084624816",
    "https://openalex.org/W4388705935",
    "https://openalex.org/W3034144394",
    "https://openalex.org/W3152856303",
    "https://openalex.org/W3163763055",
    "https://openalex.org/W2243147875",
    "https://openalex.org/W4360991088",
    "https://openalex.org/W6910555758",
    "https://openalex.org/W4396832321",
    "https://openalex.org/W3201057025",
    "https://openalex.org/W4206493125",
    "https://openalex.org/W4321521234",
    "https://openalex.org/W2946726116",
    "https://openalex.org/W3034674374",
    "https://openalex.org/W3182607842",
    "https://openalex.org/W4387087094",
    "https://openalex.org/W4396832059",
    "https://openalex.org/W4319837253",
    "https://openalex.org/W4385373745",
    "https://openalex.org/W4389917881",
    "https://openalex.org/W4389664922",
    "https://openalex.org/W2276056294",
    "https://openalex.org/W4255222684"
  ],
  "abstract": "Integrating physiological signals such as electroencephalogram (EEG), with other data such as interview audio, may offer valuable multimodal insights into psychological states or neurological disorders. Recent advancements with Large Language Models (LLMs) position them as prospective ``health agents'' for mental health assessment. However, current research predominantly focus on single data modalities, presenting an opportunity to advance understanding through multimodal data. Our study aims to advance this approach by investigating multimodal data using LLMs for mental health assessment, specifically through zero-shot and few-shot prompting. Three datasets are adopted for depression and emotion classifications incorporating EEG, facial expressions, and audio (text). The results indicate that multimodal information confers substantial advantages over single modality approaches in mental health assessment. Notably, integrating EEG alongside commonly used LLM modalities such as audio and images demonstrates promising potential. Moreover, our findings reveal that 1-shot learning offers greater benefits compared to zero-shot learning methods.",
  "full_text": "Exploring Large-Scale Language Models to Evaluate EEG-Based\nMultimodal Data for Mental Health\nYongquan Hu\nyongquan.hu@unsw.edu.au\nUniversity of New South Wales\nSydney, NSW, Australia\nShuning Zhang\nzsn23@mails.tsinghua.edu.cn\nTsinghua University\nBeijing, China\nTing Dang\nting.dang@unimelb.edu.au\nUniversity of Melbourne\nMelbourne, VIC, Australia\nHong Jia\nh.jia.cam@gmail.com\nUniversity of Melbourne\nMelbourne, VIC, Australia\nFlora D. Salim\nflora.salim@unsw.edu.au\nUniversity of New South Wales\nSydney, NSW, Australia\nWen Hu\nwen.hu@unsw.edu.au\nUniversity of New South Wales\nSydney, NSW, Australia\nAaron J. Quigley\naquigley@acm.org\nCSIRO’s Data61\nSydney, NSW, Australia\nAbstract\nIntegrating physiological signals such as electroencephalogram\n(EEG), with other data such as interview audio, may offer valu-\nable multimodal insights into psychological states or neurologi-\ncal disorders. Recent advancements with Large Language Models\n(LLMs) position them as prospective “health agents” for mental\nhealth assessment. However, current research predominantly focus\non single data modalities, presenting an opportunity to advance\nunderstanding through multimodal data. Our study aims to ad-\nvance this approach by investigating multimodal data using LLMs\nfor mental health assessment, specifically through zero-shot and\nfew-shot prompting. Three datasets are adopted for depression\nand emotion classifications incorporating EEG, facial expressions,\nand audio (text). The results indicate that multimodal information\nconfers substantial advantages over single modality approaches in\nmental health assessment. Notably, integrating EEG alongside com-\nmonly used LLM modalities such as audio and images demonstrates\npromising potential. Moreover, our findings reveal that 1-shot learn-\ning offers greater benefits compared to zero-shot learning methods.\nCCS Concepts\n• Human-centered computing →Ubiquitous and mobile com-\nputing; • Applied computing →Life and medical sciences .\nKeywords\nMental Health, EEG, Large Language Model, Prompt Engineering.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nUbiComp Companion ’24, October 5–9, 2024, Melbourne, VIC, Australia\n© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-1058-2/24/10\nhttps://doi.org/10.1145/3675094.3678494\nACM Reference Format:\nYongquan Hu, Shuning Zhang, Ting Dang, Hong Jia, Flora D. Salim, Wen Hu,\nand Aaron J. Quigley. 2024. Exploring Large-Scale Language Models to Eval-\nuate EEG-Based Multimodal Data for Mental Health. In Companion of the\n2024 ACM International Joint Conference on Pervasive and Ubiquitous Comput-\ning (UbiComp Companion ’24), October 5–9, 2024, Melbourne, VIC, Australia.\nACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3675094.3678494\n1 Introduction\nMental health, as defined by the World Health Organization (WHO),\nis a state of well-being where individuals can realise their poten-\ntial, handle normal life stresses, work productively, and contribute\nto their communities [31]. Mental health issues are increasingly\nimpacting the global economy [ 11], with conditions such as de-\npression and anxiety estimated to cost trillions of dollars in lost\nproductivity annually [8].\nThe accurate measurement and classification of such health con-\nditions requires psychological evaluation which can include the\nrecording of various indicators. Commonly, many physiological sig-\nnals, such as Electroencephalogram (EEG) [12], Heart Rate Variabil-\nity (HRV) [15], and Electrodermal Activity (EDA) [14], are integral\nfor mental health assessments due to their reliability and difficulty\nto mask, ensuring more accurate identification [18, 40]. These sig-\nnals are readily captured by widely available sensors [2, 10, 35].\nIn addition to capturing data, advancements in Artificial Intel-\nligence (AI) technology have led researchers to develop various\nalgorithms (e.g., machine learning) for the timely and accurately\ndetection [1], modeling [43] and inference [29] of health conditions\nbased on physiological signals. Recently, the capabilities of Large-\nscale Language Models (LLMs) have introduced a new paradigm for\nprediction and assessment in mental health [24, 42, 44]. LLMs offer\nseveral advantages, including enhanced multimodal data processing\nfor improved assessment accuracy [25], interactive communication\nmethods like human-in-the-loop to create more configurable health\nagents [4], and the potential for fine-tuning domain-specific pur-\npose based on general models to reduce costs [ 39, 44]. However,\nmost work using LLMs to detect mental health focuses on tasks of\narXiv:2408.07313v1  [cs.HC]  14 Aug 2024\nUbiComp Companion ’24, October 5–9, 2024, Melbourne, VIC, Australia Yongquan Hu et al.\nsingle modality data such as Mental-LLM [44] and EEG-GPT [21],\nand the exploration of LLMs in evaluating multimodal sensing data\nfor mental health remains limited. Moreover, existing multimodal\nLLMs have been developed primarily using audio and video modal-\nities. They may lack the capabilities in handling other types of data,\nsuch as EEG and other physiological signals which play a crucial\nrole [12] in mental health assessment. Among various physiologi-\ncal signals, EEG is particularly valuable, providing high-frequency\ndata that accurately assesses conditions such as depression, mood,\nand stress levels [ 5]. Therefore, understanding how these LLMs\nprocess EEG data and how to effectively combine EEG with existing\nmodalities remains an open question.\nThis paper introduces MultiEEG-GPT, a method for assessing\nmental health using multimodalities, especially with EEG, i.e., EEG\nand facial expression or audio. The latest GPT-4o API1 is adopted\nfor processing multimodalities to recognize the health conditions.\nUnlike its predecessors such as GPT-4 and GPT-4v, which require\nseparate interface calls, GPT-4o integrates multimodal data pro-\ncessing into a single interface, enhancing the development of this\nmethod [30]. This work aims to understand the capabilities of mul-\ntimodal LLMs in categorising various mental health conditions.\nThis work seeks to compare their ability to model different modali-\nties and EEG and design optimal prompt engineering to facilitate\nreliable prediction.\nThe contributions of this paper include: i) the prompt engineer-\ning design using both zero-shot and few-shot approaches to exam-\nine the predictive capability of MultiEEG-GPT using multimodal-\nities in recognizing different health conditions; ii) experiments\nacross three different databases to validate the effectiveness of\nMultiEEG-GPT. iii) an in-depth analysis to understand how multi-\nmodalities enhance health condition predictions compared to single\nmodalities. We aim to open up further developments, such as health-\nsupportive social robots [4, 19, 23], within the context of ubiquitous\ncomputing, human-computer interaction, and affective computing.\n2 Related Work\nEEG-based physiological signal analysis has long been essential\nfor monitoring mental health, evolving alongside AI advancements.\nInitially focused on traditional machine algorithms like k-nearest\nNeighbor (k-NN) and Support Vector Machine (SVM) for EEG data,\nHou et al. demonstrated the potential of EEG for stress level recog-\nnition, with the accuracy of 67.07% [17]. Later, the field has shifted\ntowards integrating deep learning and multimodal data. Zhongjie\net al. developed a fusion algorithm levering deep neural networks\nthat combines Convolutional Neural Networks (CNNs) and Bidirec-\ntional Long Short-Term Memory (BiLSTM) networks for emotion\nclassification, markedly demonstrating the impressive accuracy in\nvalence and arousal classifications to 93.20±2.55% and 93.18±2.71%,\nrespectively [26].\nRecently, the advent of general LLMs capable of processing mul-\ntimodal data has further pivoted the focus towards using LLMs\nfor evaluating mental health data, anticipating their role as future\nevaluation agents. For example, Xuhai et al. tested various LLMs,\nincluding GPT-3.5 and GPT-4, across multiple datasets using meth-\nods like zero-shot and few-shot prompting [ 44]. Jonathan et al.\n1https://openai.com/index/hello-GPT-4o/, accessed on June 11, 2024\nintroduced EEG-GPT, using GPT models to classify and interpret\nEEG data [21]. However, these studies still focus on single modality,\nsuch as text or EEG. Given various modalities can provide rich\nand complementary information to infer health conditions, it is\nproposed to consider different modalities in the automatic systems\nas well, especially with EEG in many mental health applications.\nHowever, research on LLMs for multimodal data with EEG is still\nlimited for mental health prediction. Our proposed MultiEEG-GPT\npioneers the work in examining multimodal data including EEG to\ninfer health conditions, aiming to bridge this gap by enhancing the\nprocessing of multimodal signals, with a particular focus on EEG\ndata.\n3 Methodology\n3.1 Dataset Selection\nVarious mental health dataset existed, of which numerous con-\ntained EEG modality. Applying the criteria that the dataset need to\ncontain at least EEG modality, we selected 3 most commonly used\ndatasets: (1) MODMA [5] was developed by Hanshu et al., and this\nmultimodal dataset is designed for analyzing depression disorders\nand includes oral records (audio) of both patients and controls, and\nEEG data (convertible to images) from these groups. This dataset\nhas binary labels of whether the participant was diagonsed with\nMajor Depressive Disorder (MDD). (2) PME4 [7] is a multimodal\nemotion dataset featuring four modalities: audio, video (not publicly\navailable), EEG, and electromyography (EMG) [7]. It was collected\nfrom 11 acting students (five female and six male) who provided in-\nformed consent. This dataset focuses on identifying seven emotions:\nanger, fear, disgust, sadness, happiness, surprise, and a neutral state;\n(3) LUMED-2 [9] was collected by Loughborough University and\nHacettepe University, and it was designed to analyze facial expres-\nsion, EEG, and galvanic skin response (GSR) data to recognize and\nclassify three categories of human emotions (neutral, happy, sad)\nunder various stimuli , advancing the understanding in affective\ncomputing.\nFor MODMA and PME4, we used audio and EEG modalities,\nwhile for LUMED-2, we used facial expression and EEG modalities.\nWe chose audio and facial expression features because they were the\namong the most prevalent modalities in mental health analysis [28,\n37]. Besides, the focus of this paper was to explore the possibility\nof GPT to analyze multimodal data, particularly with the important\nEEG modality [ 16]. Thus, we did not include the physiological\nmodalities (e.g., GSR, Resp).\n3.2 Prompt Design\nFor our MultiEEG-GPT method, we use prompt engineering strate-\ngies (including zero-shot prompting and few-shot prompting) for\nprediction tasks on multiple datasets. These prompts are model-\nagnostic, and we present the details of language models and settings\nemployed for our experiment in the next section.\nFor the prompting strategies, we built upon the design in [44]\nand [45]. We have designed the prompt to account for different\nmodalities and incorporated flexibility in altering the number of\nmodalities for evaluation. Additionally, we have verified and com-\npared few-shot and zero-shot prompts for evaluation.\nExploring LLMs to Evaluate EEG-Based Multimodal Data UbiComp Companion ’24, October 5–9, 2024, Melbourne, VIC, Australia\nZero-shot prompting. As shown in Table 1, the zero-shot\nprompting strategy consists of a role-play prompt, a specific task\ndescription, and an additional rule to avoid unnecessary output and\nrestricted models to focus on the current task. The role-play prompt\naims to inform the LLMs of the general task, while the specific\ntask description provides the information for different modalities.\nSuch description also provides the flexibility in adding or deleting\nmodalities. Therefore, the final prompt for the model consisted of:\n{role-play prompt} + {task specification} + {rules}.\nFew-shot prompting. The few-shot prompt added the few-shot\nsamples after the same zero-shot prompt template. Specifically, we\ninclude the task-specific prompt followed the zero-shot prompt,\nbut providing the correct class labels instead of offering different\ncandidate class labels for prediction, which is similar to Xuhai et\nal’s setting [44].\nTable 1: The zero-shot and few-shot prompting strategies.\n<MOD1>, <MOD2> and <MOD3> as placeholders denote three\ndifferent modalities. XXX is the description of collection and\nvisualization process. <SYM> as a placeholder denotes the\nsymptom to be diagnosed. For example, for depression anal-\nysis <SYM> should be replaced with depression. The example\nis for mental health diagonsis with three classes. The label\ndescription “0 denotes XXX” of the classes could be added or\nremoved to accommodate for more or less classes.\nRole-play prompt Imagine you are a mental health expert expert\nat analyzing the emotion and mental health\nstatus.\nTask specification The below is <MOD1>, <MOD2> and <MOD3>\ndata. <MOD1> data is collected through XXX\nand visualized in XXX form. <MOD2> data\nis collected through XXX and visualized in\nXXX form. <MOD3> data is collected through\nXXX and visualized in XXX form. Analyze the\n<SYM> status of the person. 0 denotes XXX, 1\ndenotes XXX, 2 denotes XXX.\nRules [Rule]: Do not output other text.\n4 Experiment\n4.1 Settings\n4.1.1 Dataset Settings. As all the datasets used standard 10-20\nelectrode layout, we set the electrodes following this layout. MNE\nlibrary is used for processing EEG signal. We processed the datasets\nusing the raw data instead of their pre-processed data (e.g., PME4)\nbecause the pre-processed data only contained features instead\nof the original signals, which were infeasible for plotting topol-\nogy map. We used a bandpass filter (low-frequency cutoff 0.1Hz,\nhigh-frequency cutoff 45Hz, Hamming Window) [34] with firwin\nwindow design. Afterward, the filtered data were re-referenced to\nan average reference [34]. Since the elicitation presented with dif-\nferent length for different datasets, we chose 530s, 5s and 1.65-4.15s\nfor LUMED-2, PME4 and MODMA datasets respectively, to account\nfor randomly set elicitation time , with 10 equidistant sampled\ntimestamps to create topology maps. For the facial expression, we\nchose the middle frame of the video (e.g., if the video’s length is 10s,\nwe chose the frame at exactly the 5s timestamp) or the image. For\nthe audio, because GPT-4o 2 have not yet released the audio input\nsupport, we used both the audio features and the text as inputs. For\nthe audio features, we used librosa library to extract the features\nfrom the audio and represent these features in text format (which is\nsimilar to EEG-GPT’s representation [21]), which includes MFCCs,\nMel Spectrogram, Chroma STFT, etc. For the text, we transcribed\nthe audio using automatic speech recognition (ASR) systems. We\nchose the open-sourced vosk library 3 with vosk-model-cn-0.15\n(Chinese version) or vosk-model-en-0.22 (English version) accord-\ning to the need. These models were the largest and most advanced\nASR systems in the vosk library, which ensured the accuracy of\nrecognition and was used in health care tasks [13, 32].\n4.1.2 Model Settings. For all datasets and all tasks, we transformed\nthe tasks into multi-class classification problems as in previous\nwork [21, 44]. For MODMA, the binary class labels are ’MDD’ or\n’healthy’. For PME4, we followed the labels in the original datasets to\nclassify the emotion into seven classes: anger, fear, disgust, sadness,\nhappiness, surprise and neutral. For LUMED-2, we set the 3-class\nlabels as in the original paper, which included neutral, happy ans\nsadness.\nPrevious work showed that GPT-4 generally performed better\nthan GPT-3.5 [44]. Given that GPT-4o is the most recent series of\nGPT-4 that naturally supports multimodal capabilities, we adopted\nGPT-4o as the tested LLMs. Specifically, we used “GPT-4o-2024-05-\n13”4 as the targeted model through OpenAI Azure’s API5. For the\nfew-shot experiment, we tested the 1-shot learning scenario to ex-\namine the capability of multi-model LLMs with limited information\nprovided. In each repeated trial, we randomly selected one sample\nfrom the corresponding dataset to act as the 1-shot sample. This\nstrategy mitigates the bias of selecting samples. For all zero-shot\nand few-shot experiments, we tested across each dataset (for the\nfew-shot experiment, we excluded that selected sample) for 5 times\nand reported the average accuracy and the standard deviation.\nWe use the image updating module of GPT-4o. However, we use\nno other any additional techniques (e.g., Chain-of-Thoughts [41])\nto serve as a preliminary study in understanding how multimodal\nLLMs process multimodal information. This approach ensures the\nresults reflect teh basic capability of the models, which was also\nconsistent with previous work [21, 44].\n4.2 Results and Discussions\n4.2.1 Multimodal analysis. We showed two examples of zero-shot\ncases using LUMED-2 and PME4 dataset in Figure 1. The first per-\nson in the LUMED-2 video is in neutral mood. The MultiEEG-GPT\naims to recognize the participant’s mental state through the fa-\ncial expression and the EEG topology map. As seen in Figure 1(a),\nMultiEEG-GPT first processed the image, and then analyzed the\nEEG topology map. It subsequently aggregated the results from\n2https://community.openai.com/t/when-the-new-voice-model-for-chatgpt-4o-will-\nbe-released/789928, accessed by Jun 16th, 2024\n3https://alphacephei.com/vosk/, accessed by Jun 16th, 2024\n4https://openai.com/index/hello-GPT-4o/, accessed by 11st June, 2024\n5https://azure.microsoft.com/en-us/products/ai-services/openai-service, accessed by\n11st June, 2024\nUbiComp Companion ’24, October 5–9, 2024, Melbourne, VIC, Australia Yongquan Hu et al.\nTable 2: Ablation experiment on 3 different multimodal data (EEG image, facial expression, audio). The line with no EEG image,\nfacial expression, audio was determined through majority voting. For few-shot prompting, we chose M=1, which meant we\nadded one few-shot sample in the prompt.\nPrediction Accuracy (%)\nStrategy EEG Facial Expression Audio MODMA PME4 LUMED-2\nZero-shot Prompting\n× × × 50.0±0.00 14.28±0.00 33.33±0.00\n✓ × × 53.79±2.46 21.05±1.71 34.61±1.28\n× ✓ × – – 38.46±1.54\n× × ✓ 69.35±2.53 15.38±1.42 –\n✓ ✓ × – – 46.13±2.4246.13±2.4246.13±2.42\n✓ × ✓ 73.54±2.0373.54±2.0373.54±2.03 28.57±2.4128.57±2.4128.57±2.41 –\nFew-shot Prompting (M=1)\n× × × 50.0±0.00 14.28±0.00 33.33±0.00\n✓ × × 62.71±3.23 26.00±1.78 36.37±1.62\n× ✓ × – – 43.64±1.85\n× × ✓ 69.92±1.53 19.13±1.29 –\n✓ ✓ × – – 52.73±2.1652.73±2.1652.73±2.16\n✓ × ✓ 79.00±1.5979.00±1.5979.00±1.59 37.00±2.3037.00±2.3037.00±2.30 –\nimage and EEG jointly, and predicted the participant’s emotion\nstate as neural.\nFor Figure 1 (b), the participant is in a sad mood. The MultiEEG-\nGPT first analyzed the person’s audio features, and then analyzed\nthe EEG features in the topology map through the color of the\nmap. It finally combined different features and predicted that the\nparticipant is in a sad mood. These cases showed the capability of\nMultiEEG-GPT to (1) analyze each modality separately, (2) aggre-\ngated the outputs based on different modalities jointly. It is also\nevident that a single modality is not adequate to identify the mood\ncorrectly. For example, MultiEEG-GPT identified the status of Fig-\nure 1 (b) as “an emotional reaction”. However, it did not accurately\nstate that the participant is sad from the EEG features. By com-\nbining the audio features and the EEG features, MUltiEEG-GPT\nachieved the accurate prediction.\n4.2.2 Performance of MultiEEG-GPT. Table 2 presents the zero-\nshot and few-shot prompting performance for all three databases.\nThe modalities used for MultiEEG-GPT depend on their availabil-\nity in the datasets. For zero-shot prompting, our proposed model,\nutilizing both modalities—either EEG + facial expression or EEG +\naudio—achieved the best performance compared to other models\nusing a single modality. The proposed model demonstrated relative\nimprovements of 4.19%, 7.52%, 7.67% over the best single-modality\nperformance for the three databases, respectively. This also high-\nlighted the importance of including EEG data in addition to the\ncommonly used modalities in LLMs, such as audio and video. It\nshould be noted that the cases with all modalities removed (the\nfirst line) used majority voting similar to Xuhai et al. ’s setting [44],\nserving as the baseline for model performance.\nFor the few-shot prompting, we observed a similar trend, with\nmultimodal models outperforming single-modality models. Addi-\ntionally, the 1-shot prompting achieved higher performance than\nzero-shot prompting, with relative improvements of 5.45%, 8.43%,\n6.60% over zero-shot prompting for the MODMA, PME4 and LUMED-\n2 databases, respectively. This suggests that additional examples\nenhance recognition, consistent with previous findings [ 27, 38].\nThe extra example likely serves as a benchmark for feature com-\nparison, allowing LLMs to assess the users’ mental health status\nmore effectively by comparing features of the few-shot and test\nsamples. The results indicate the general benefit of an additional ex-\nample, as no specific sample was intentionally selected in the 1-shot\nprompting setting. In summary, LLMs leveraging multimodalities\nincluding EEG could significantly benefit depression and emotion\nrecognition.\n5 Conclusion and Future Work\nThis paper proposes MultiEEG-GPT to explore multimodal data,\nspecifically with EEG, for mental health recognition. We have de-\nsigned zero-shot and few-shot prompting strategies to enhance\nprediction accuracy, leveraging the most recent GPT-4o as the LLM\nbase model. Three datasets, including MODMA, PME4, and LUMED-\n2, were adopted for evaluation. Our study showed that predictions\nusing multimodal data significantly outperform those using single-\nmodal data. While the current prediction accuracy approaches that\nof traditional machine learning methods even without tuning the\nLLMs, there is significant potential for improvement with strategies\nsuch as instruction fine-tuning or multi-strategy hierarchical pre-\ndiction in future research for mental health leveraging multimodal\nLLMs.\nMoreover, the use of LLMs as health agents raises important\nethical considerations. LLMs may exhibit value alignment problems,\nleading to racial and gender disparities [46] or producing outcomes\nmisaligned with health assessment standards [20]. LLMs also pose\nprivacy risks [3, 33] due to data memorization and extraction [6].\nFine-tuning with mental health data can lead to data leakage. These\nissues necessitate careful attention to ensure ethical compliance and\naccuracy. For example, input data should be anonymized beforehand\nExploring LLMs to Evaluate EEG-Based Multimodal Data UbiComp Companion ’24, October 5–9, 2024, Melbourne, VIC, Australia\nFigure 1: Case analysis for LUMED-2 and PME4 datasets (the person’s face has been blurred for ethical reasons). Figure\n(a) illustrates one subject’s input EEG topology map and his facial expression, as well as the prediction result and the text\nexplanation from LUMED-2 dataset. Figure (b) illustrates one subject’s input EEG topology map, audio features, input audio\ntranscription “The sky is green.”, as well as the prediction result and the explanation, from PME4 dataset. In both cases, the\nmodel makes the accurate predictions when processing both modalities.\n[36], and un-learning and alignment should be integrated to the\ntraining process to protect privacy and avoid harm [22].\nReferences\n[1] Rohizah Abd Rahman, Khairuddin Omar, Shahrul Azman Mohd Noah, Mohd\nShahrul Nizam Mohd Danuri, and Mohammed Ali Al-Garadi. 2020. Application\nof machine learning methods in mental health detection: a systematic review.\nIeee Access 8 (2020), 183952–183964.\n[2] Usman Arshad, Cecilia Mascolo, and Marcus Mellor. 2003. Exploiting mobile\ncomputing in health-care. In Proceedings of demo session of the 3rd international\nworkshop on smart appliances, ICDCS03 . Citeseer.\n[3] Hannah Brown, Katherine Lee, Fatemehsadat Mireshghallah, Reza Shokri, and\nFlorian Tramèr. 2022. What does it mean for a language model to preserve\nprivacy?. In Proceedings of the 2022 ACM conference on fairness, accountability,\nand transparency . 2280–2292.\n[4] Johana Cabrera, M Soledad Loyola, Irene Magaña, and Rodrigo Rojas. 2023.\nEthical dilemmas, mental health, artificial intelligence, and llm-based chatbots.\nIn International Work-Conference on Bioinformatics and Biomedical Engineering .\nUbiComp Companion ’24, October 5–9, 2024, Melbourne, VIC, Australia Yongquan Hu et al.\nSpringer, 313–326.\n[5] Hanshu Cai, Zhenqin Yuan, Yiwen Gao, Shuting Sun, Na Li, Fuze Tian, Han Xiao,\nJianxiu Li, Zhengwu Yang, Xiaowei Li, et al. 2022. A multi-modal open dataset\nfor mental-disorder analysis. Scientific Data 9, 1 (2022), 178.\n[6] Nicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-\nVoss, Katherine Lee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson,\net al. 2021. Extracting training data from large language models. In 30th USENIX\nSecurity Symposium (USENIX Security 21) . 2633–2650.\n[7] Jin Chen, Tony Ro, and Zhigang Zhu. 2022. Emotion recognition with audio,\nvideo, EEG, and EMG: a dataset and baseline approaches. IEEE Access 10 (2022),\n13229–13242.\n[8] Dan Chisholm, Kim Sweeny, Peter Sheehan, Bruce Rasmussen, Filip Smit, Pim\nCuijpers, and Shekhar Saxena. 2016. Scaling-up treatment of depression and\nanxiety: a global return on investment analysis. The Lancet Psychiatry 3, 5 (2016),\n415–424.\n[9] Yucel Cimtay, Erhan Ekmekcioglu, and Seyma Caglar-Ozhan. 2020. Cross-subject\nmultimodal emotion recognition based on hybrid fusion. IEEE Access 8 (2020),\n168865–168878.\n[10] Ting Dang, Dimitris Spathis, Abhirup Ghosh, and Cecilia Mascolo. 2023. Human-\ncentred artificial intelligence for mobile health sensing: challenges and opportu-\nnities. Royal Society Open Science 10, 11 (2023), 230806.\n[11] Nan Gao, Soundariya Ananthan, Chun Yu, Yuntao Wang, and Flora D Salim. 2023.\nCritiquing Self-report Practices for Human Mental and Wellbeing Computing at\nUbicomp. arXiv preprint arXiv:2311.15496 (2023).\n[12] Ela Gore and Sheetal Rathi. 2019. Surveying machine learning algorithms on eeg\nsignals data for mental health assessment. In 2019 IEEE Pune Section International\nConference (PuneCon) . IEEE, 1–6.\n[13] Lukas Grasse, Sylvain J Boutros, and Matthew S Tata. 2021. Speech interaction to\ncontrol a hands-free delivery robot for high-risk health care scenarios. Frontiers\nin Robotics and AI 8 (2021), 612750.\n[14] Alberto Greco, Gaetano Valenza, and Enzo Pasquale Scilingo. 2016. Advances in\nElectrodermal activity processing with applications for mental health . Springer.\n[15] Unsoo Ha, Yongsu Lee, Hyunki Kim, Taehwan Roh, Joonsung Bae, Changhyeon\nKim, and Hoi-Jun Yoo. 2015. A wearable EEG-HEG-HRV multimodal system with\nsimultaneous monitoring of tES for mental health management.IEEE transactions\non biomedical circuits and systems 9, 6 (2015), 758–766.\n[16] Blake Anthony Hickey, Taryn Chalmers, Phillip Newton, Chin-Teng Lin, David\nSibbritt, Craig S McLachlan, Roderick Clifton-Bligh, John Morley, and Sara Lal.\n2021. Smart devices and wearable technologies to detect and monitor mental\nhealth conditions and stress: A systematic review. Sensors 21, 10 (2021), 3461.\n[17] Xiyuan Hou, Yisi Liu, Olga Sourina, Yun Rui Eileen Tan, Lipo Wang, and Wolfgang\nMueller-Wittig. 2015. EEG based stress monitoring. In 2015 IEEE international\nconference on systems, man, and cybernetics . IEEE, 3110–3115.\n[18] Xiaozhu Hu, Yanwen Huang, Bo Liu, Ruolan Wu, Yongquan Hu, Aaron J Quigley,\nMingming Fan, Chun Yu, and Yuanchun Shi. 2023. SmartRecorder: An IMU-based\nVideo Tutorial Creation by Demonstration System for Smartphone Interaction\nTasks. In Proceedings of the 28th International Conference on Intelligent User Inter-\nfaces. 278–293.\n[19] Yongquan Hu, Hui-Shyong Yeo, Mingyue Yuan, Haoran Fan, Don Samitha Elvit-\nigala, Wen Hu, and Aaron Quigley. 2023. Microcam: Leveraging smartphone\nmicroscope camera for context-aware contact surface sensing. Proceedings of\nthe ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 7, 3 (2023),\n1–28.\n[20] Inthrani Raja Indran, Priya Paranthaman, Neelima Gupta, and Nurulhuda Mustafa.\n2024. Twelve tips to leverage AI for efficient and effective medical question\ngeneration: a guide for educators using Chat GPT. Medical Teacher (2024), 1–6.\n[21] Jonathan W Kim, Ahmed Alaa, and Danilo Bernardo. 2024. EEG-GPT: Exploring\nCapabilities of Large Language Models for EEG Classification and Interpretation.\narXiv preprint arXiv:2401.18006 (2024).\n[22] Hannah Rose Kirk, Bertie Vidgen, Paul Röttger, and Scott A Hale. 2024. The\nbenefits, risks and bounds of personalizing the alignment of large language\nmodels to individuals. Nature Machine Intelligence (2024), 1–10.\n[23] Tin Lai, Yukun Shi, Zicong Du, Jiajie Wu, Ken Fu, Yichao Dou, and Ziqi Wang.\n2023. Psy-llm: Scaling up global mental health psychological services with ai-\nbased large language models. arXiv preprint arXiv:2307.11991 (2023).\n[24] Bishal Lamichhane. 2023. Evaluation of chatgpt for nlp-based mental health\napplications. arXiv preprint arXiv:2303.15727 (2023).\n[25] Jiahao Nick Li, Yan Xu, Tovi Grossman, Stephanie Santosa, and Michelle Li. 2024.\nOmniActions: Predicting Digital Actions in Response to Real-World Multimodal\nSensory Inputs with LLMs. InProceedings of the CHI Conference on Human Factors\nin Computing Systems . 1–22.\n[26] Zhongjie Li, Gaoyan Zhang, Jianwu Dang, Longbiao Wang, and Jianguo Wei.\n2021. Multi-modal emotion recognition based on deep learning of EEG and audio\nsignals. In 2021 International Joint Conference on Neural Networks (IJCNN) . IEEE,\n1–6.\n[27] Liangliang Liu, Zhihong Liu, Jing Chang, and Xue Xu. 2024. A multi-modal\nextraction integrated model for neuropsychiatric disorders classification. Pattern\nRecognition (2024), 110646.\n[28] Daniel M Low, Kate H Bentley, and Satrajit S Ghosh. 2020. Automated assess-\nment of psychiatric disorders using speech: A systematic review. Laryngoscope\ninvestigative otolaryngology 5, 1 (2020), 96–116.\n[29] Lakmal Meegahapola, William Droz, Peter Kun, Amalia De Götzen, Chaitanya\nNutakki, Shyam Diwakar, Salvador Ruiz Correa, Donglei Song, Hao Xu, Miriam\nBidoglia, et al . 2023. Generalization and personalization of mobile sensing-\nbased mood inference models: an analysis of college students in eight countries.\nProceedings of the ACM on interactive, mobile, wearable and ubiquitous technologies\n6, 4 (2023), 1–32.\n[30] OpenAI. 2024. Hello GPT-4o. https://openai.com/index/hello-gpt-4o/ [Ac-\ncessed:June 2024].\n[31] World Health Organization et al. 2022. World mental health report: Transforming\nmental health for all. (2022).\n[32] Tiago F Pereira, Arthur Matta, Carlos M Mayea, Frederico Pereira, Nelson Monroy,\nJoão Jorge, Tiago Rosa, Carlos E Salgado, Ana Lima, Ricardo J Machado, et al. 2022.\nA web-based Voice Interaction framework proposal for enhancing Information\nSystems user experience. Procedia Computer Science 196 (2022), 235–244.\n[33] Charith Peris, Christophe Dupuy, Jimit Majmudar, Rahil Parikh, Sami Smaili,\nRichard Zemel, and Rahul Gupta. 2023. Privacy in the time of language models.\nIn Proceedings of the sixteenth ACM international conference on web search and\ndata mining . 1291–1292.\n[34] Kerstin Pieper, Robert P Spang, Pablo Prietz, Sebastian Möller, Erkki Paajanen,\nMarkus Vaalgamaa, and Jan-Niklas Voigt-Antons. 2021. Working with envi-\nronmental noise and noise-cancelation: a workload assessment with EEG and\nsubjective measures. Frontiers in neuroscience 15 (2021), 771533.\n[35] Dimitris Spathis, Sandra Servia-Rodriguez, Katayoun Farrahi, Cecilia Mascolo,\nand Jason Rentfrow. 2019. Passive mobile sensing and psychological traits for\nlarge scale mood prediction. InProceedings of the 13th EAI international conference\non pervasive computing technologies for healthcare . 272–281.\n[36] Robin Staab, Mark Vero, Mislav Balunovic, and Martin Vechev. 2024. Large\nLanguage Models are Anonymizers. In ICLR 2024 Workshop on Reliable and\nResponsible Foundation Models .\n[37] Chang Su, Zhenxing Xu, Jyotishman Pathak, and Fei Wang. 2020. Deep learning\nin mental health outcome research: a scoping review. Translational Psychiatry\n10, 1 (2020), 116.\n[38] Hao Sun, Jiaqing Liu, Shurong Chai, Zhaolin Qiu, Lanfen Lin, Xinyin Huang, and\nYenwei Chen. 2021. Multi-Modal Adaptive Fusion Transformer Network for the\nEstimation of Depression Level. Sensors 21, 14 (2021). https://doi.org/10.3390/\ns21144764\n[39] Teo Susnjak, Peter Hwang, Napoleon H Reyes, Andre LC Barczak, Timothy R\nMcIntosh, and Surangika Ranathunga. 2024. Automating research synthesis with\ndomain-specific large language model fine-tuning.arXiv preprint arXiv:2404.08680\n(2024).\n[40] Zhiyuan Wang, Maria A Larrazabal, Mark Rucker, Emma R Toner, Katharine E\nDaniel, Shashwat Kumar, Mehdi Boukhechba, Bethany A Teachman, and Laura E\nBarnes. 2023. Detecting social contexts from mobile sensing indicators in vir-\ntual interactions with socially anxious individuals. Proceedings of the ACM on\nInteractive, Mobile, Wearable and Ubiquitous Technologies 7, 3 (2023), 1–26.\n[41] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural information processing systems 35\n(2022), 24824–24837.\n[42] Ruolan Wu, Chun Yu, Xiaole Pan, Yujia Liu, Ningning Zhang, Yue Fu, Yuhan\nWang, Zhi Zheng, Li Chen, Qiaolei Jiang, et al . 2024. MindShift: Leveraging\nLarge Language Models for Mental-States-Based Problematic Smartphone Use\nIntervention. In Proceedings of the CHI Conference on Human Factors in Computing\nSystems. 1–24.\n[43] Xuhai Xu, Xin Liu, Han Zhang, Weichen Wang, Subigya Nepal, Yasaman Se-\nfidgar, Woosuk Seo, Kevin S Kuehn, Jeremy F Huckins, Margaret E Morris, et al.\n2023. GLOBEM: cross-dataset generalization of longitudinal human behavior\nmodeling. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous\nTechnologies 6, 4 (2023), 1–34.\n[44] Xuhai Xu, Bingsheng Yao, Yuanzhe Dong, Saadia Gabriel, Hong Yu, James Hendler,\nMarzyeh Ghassemi, Anind K Dey, and Dakuo Wang. 2024. Mental-llm: Leveraging\nlarge language models for mental health prediction via online text data. Proceed-\nings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies 8, 1\n(2024), 1–32.\n[45] Hao Xue and Flora D Salim. 2023. Promptcast: A new prompt-based learning\nparadigm for time series forecasting. IEEE Transactions on Knowledge and Data\nEngineering (2023).\n[46] Travis Zack, Eric Lehman, Mirac Suzgun, Jorge A Rodriguez, Leo Anthony Celi,\nJudy Gichoya, Dan Jurafsky, Peter Szolovits, David W Bates, Raja-Elie E Abdul-\nnour, et al. 2024. Assessing the potential of GPT-4 to perpetuate racial and gender\nbiases in health care: a model evaluation study. The Lancet Digital Health 6, 1\n(2024), e12–e22.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6912549734115601
    },
    {
      "name": "Electroencephalography",
      "score": 0.6185923218727112
    },
    {
      "name": "Scale (ratio)",
      "score": 0.5993479490280151
    },
    {
      "name": "Natural language processing",
      "score": 0.4181958734989166
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4116736650466919
    },
    {
      "name": "Psychology",
      "score": 0.1958596408367157
    },
    {
      "name": "Cartography",
      "score": 0.08765506744384766
    },
    {
      "name": "Neuroscience",
      "score": 0.07397675514221191
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I31746571",
      "name": "UNSW Sydney",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ]
}