{
  "title": "Large Language Models are biased to overestimate profoundness",
  "url": "https://openalex.org/W4389519077",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5092973341",
      "name": "Eugenio Herrera-Berg",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117285323",
      "name": "Tomas Browne",
      "affiliations": [
        "Pontificia Universidad Católica de Chile"
      ]
    },
    {
      "id": "https://openalex.org/A2604224557",
      "name": "Pablo Leon-Villagra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2779325638",
      "name": "Marc-Lluís Vives",
      "affiliations": [
        "Leiden University"
      ]
    },
    {
      "id": "https://openalex.org/A2112788476",
      "name": "Cristian Calderón",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4360836968",
    "https://openalex.org/W2116433828",
    "https://openalex.org/W2282631200",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4378472959",
    "https://openalex.org/W4317463334",
    "https://openalex.org/W4235652519",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4283263983",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W4392637287",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W2549835527",
    "https://openalex.org/W4285594979",
    "https://openalex.org/W4385571689",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385571325"
  ],
  "abstract": "Recent advancements in natural language processing by large language models (LLMs), such as GPT-4, have been suggested to approach Artificial General Intelligence. And yet, it is still under dispute whether LLMs possess similar reasoning abilities to humans. This study evaluates GPT-4 and various other LLMs in judging the profoundness of mundane, motivational, and pseudo-profound statements. We found a significant statement-to-statement correlation between the LLMs and humans, irrespective of the type of statements and the prompting technique used. However, LLMs systematically overestimate the profoundness of nonsensical statements, with the exception of Tk-instruct, which uniquely underestimates the profoundness of statements. Only few-shot learning prompts, as opposed to chain-of-thought prompting, draw LLMs ratings closer to humans. Furthermore, this work provides insights into the potential biases induced by Reinforcement Learning from Human Feedback (RLHF), inducing an increase in the bias to overestimate the profoundness of statements.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 9653–9661\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models are biased to overestimate profoundness\nEugenio Herrera-Berg*1, Tomás Vergara Browne*1,2, Pablo León-Villagrá†3,\nMarc-Lluís Vives†4, Cristian Buc Calderon†1\n1Centro Nacional de Inteligencia Artificial (CENIA), Chile\n2Pontificia Universidad Católica de Chile, Chile\n3Brown University, United States\n4Leiden University, the Netherlands\nAbstract\nRecent advancements in natural language pro-\ncessing by large language models (LLMs), such\nas GPT-4, have been suggested to approach\nArtificial General Intelligence. And yet, it is\nstill under dispute whether LLMs possess sim-\nilar reasoning abilities to humans. This study\nevaluates GPT-4 and various other LLMs in\njudging the profoundness of mundane, motiva-\ntional, and pseudo-profound statements. We\nfound a significant statement-to-statement cor-\nrelation between the LLMs and humans, ir-\nrespective of the type of statements and the\nprompting technique used. However, LLMs\nsystematically overestimate the profoundness\nof nonsensical statements, with the exception\nof Tk-instruct, which uniquely underestimates\nthe profoundness of statements. Only few-\nshot learning prompts, as opposed to chain-of-\nthought prompting, draw LLMs ratings closer\nto humans. Furthermore, this work provides\ninsights into the potential biases induced by Re-\ninforcement Learning from Human Feedback\n(RLHF), inducing an increase in the bias to\noverestimate the profoundness of statements.\n1 Introduction\nGPT-4 has now achieved the ability to perform a\nwide range of tasks on par with humans (OpenAI,\n2023). Given that current LLMs excel at text inter-\npretation and generation, recent work has assessed\nLLMs’ ability to perform tasks beyond simple lan-\nguage understanding. And the evidence for LLMs’\nability to perform these tasks is changing rapidly:\nwhile previous versions of LLMs (e.g., GPT-3.5)\nfell short in tasks seen as unsolvable with textual\ninput alone (Ullman, 2023), more recent models,\nsuch as GPT-4, have been suggested to achieve\nthem (Bubeck et al., 2023). However, most of\nthese studies have focused on presenting the LLM\nwith statements following conversational maxims.\n*Equal contribution.\n†Co-senior authors.\nThus, when presenting prompts to an LLM, one\ngenerally strives to make the statement informative\nand truthful to convey one’s ideas or goals suc-\ncessfully. However, not all language use is aimed\nat efficiently communicating information. Some-\ntimes, it can be beneficial for a speaker to obscure\nthe meaning of an utterance, for example, to de-\nceive or persuade or to hide one’s true intentions.\nSuccessful, human-level communication requires\nthe listener to detect such language use; otherwise,\nthey are susceptible to deception. Here, we as-\nsess whether GPT-4 and other LLMs can identify\nlanguage created with the aim of impressing the\nlistener rather than communicating meaning.\nThe term “pseudo-profound bullshit” (BS) refers\nto sentences that seem to have a deep meaning\nat first glance, but are meaningless (Pennycook\net al., 2015). These sentences are syntactically\ncorrect and presented as true and significant but,\nupon further consideration, lack substance. For\nexample:\n“Consciousness is the growth of coher-\nence, and of us.”\nPseudo-profound bullshit is thus an example of\nlanguage use that is not aimed at conveying in-\nformation but evoking an interpretation in the lis-\ntener in order to seem meaningful and insightful;\nand could potentially fool LLMs to produce non-\ndesired responses. Cognitive strategies deployed\nduring language exchanges to uncover (and create)\nthese strategies require sophisticated skills (Musker\nand Pavlick, 2023), such as making recursive in-\nferences or assessing the actual meaning of hard-\nto-parse sentences (Bubeck et al., 2023). In fact,\nin humans, the ability to detect the shallowness\nof pseudo-profound bullshit correlates with classi-\ncal measures of cognitive sophistication like verbal\nintelligence or individual tendencies to exert delib-\neration (Pennycook et al., 2015).\nBecause LLMs perform better in tasks that re-\n9653\nquire knowledge of syntax, morphology, or phonol-\nogy while struggling in tasks that require formal\nreasoning (e.g., logic), causal world knowledge,\nsituation modeling, or communicative intent (Ma-\nhowald et al., 2023), it is an open question whether\nan LLM will be able to detect the presence of\npseudo-profound bullshit.\nWe find that GPT-4, and most other LLMs tested\nhere, display a strong bias towards profoundness,\ni.e., the pseudo-profound statements are systemati-\ncally ranked above a mid-point level of profound-\nness. In contrast, humans rank these statements\nbelow this mid-point level. The one exception we\nfound is with Tk-Instruct, which consistently un-\nderestimates the profoundness of every statement,\nincluding that of motivational statements.\nMoreover, we show that chain-of-thought\nprompting methods (Wei et al., 2022), which typi-\ncally increase the reasoning abilities in LLMs, have\nno statistical effect on this ranking; and that only\nfew-shot learning allows GPT-4 to rate statements\nmore similar to humans and below the mid-point\nranking. Finally, we show that, despite the biases\nfound, the statement-to-statement rankings display\na strong correlation between the LLMs and hu-\nmans.\n2 Related Work\nProbing Language Understanding. LLMs can\nbe treated as participants in psycholinguistic\n(Linzen et al., 2016; Dillion et al., 2023) or cogni-\ntive science experimental studies (Binz and Schulz,\n2023). Thus, recent work suggests a series of diag-\nnostics to analyze LLMs inspired by human exper-\niments (Ettinger, 2020). For instance, LLMs can\nrepresent hierarchical syntactic structure (Lin et al.,\n2019) but tend to struggle in semantic tasks (Ten-\nney et al., 2019) or display similar content effects\nas humans (Dasgupta et al., 2022). Furthermore,\nHu et al. (2023) compared a variety of LLMs to\nhuman evaluation on seven pragmatic phenomena\nto test possible correlations between human and\nmodels’ judgments.\nBoosting Reasoning in LLMs. Recent work\nevaluates the reasoning abilities in LLMs by an-\nalyzing the techniques that elicit reasoning (Huang\nand Chang, 2022). For instance, several prompting\nmethods increase the reasoning abilities of LLMs.\nChain-of-thought (CoT) prompting is a method that\nimplements a sequence of interposed natural lan-\nguage processing steps leading to a final answer\n(Wei et al., 2022; Zhao et al., 2023; Lyu et al.,\n2023). Furthermore, increasing reasoning abilities\ncan also be elicited via Zero-shot CoT by simply\nadding the sentence “let’s think step by step\" at\nthe end of the prompt (Kojima et al., 2022). In\nthe same vein, showing a few examples to LLMs\nallows them to quickly learn to reason on com-\nplex tasks (Brown et al., 2020; Tsimpoukelli et al.,\n2021), an ability known as few-shot learning.\n3 Method\nOur study is based on previous research assessing\nwhether humans are receptive to pseudo-profound\nstatements (Pennycook et al., 2015). In the origi-\nnal study by Pennycook et al., 198 humans ranked\nthe profoundness of statements on a 5-point Likert\nscale: 1 = Not at all profound, 2 = somewhat pro-\nfound, 3 = fairly profound, 4 = definitely profound,\n5 = very profound.\nWe replicate this study using a variety of LLMs\nto judge the profoundness of statements. The\nLLMs used are GPT-4 and various other models\n(Flan-T5 XL (Chung et al., 2022), Llama-2 13B\nwith and without RLHF (Touvron et al., 2023), Vi-\ncuna 13B (Chiang et al., 2023) and Tk-Instruct\n11B (Wang et al., 2022)). We performed 20 repe-\ntitions for each experiment to account for the non-\ndeterministic nature of token generation (given that\nwe use non-zero temperature). We investigate how\nsensitive this model is to pseudo-profound bullshit\nby systematically comparing it with human ratings\non these same and similar statements. Furthermore,\nwe use distinct prompting methods that have been\nshown to increase the reasoning abilities of LLMs\n(see below). All of our code and results are publicly\navailable1.\n3.1 Dataset\nWe generated five distinct datasets of sentences.\nFor dataset 1, we used the same 30 pseudo-\nprofound BS statements as those used in experi-\nment 2 of Pennycook et al. (2015). For dataset\n2, we built a dataset comprising 30 novel pseudo-\nprofound BS statements generated following the\nsame procedure as Pennycook et al. (2015). Dataset\n3 was generated on the basis of the first dataset, but\nwe generated 30 novel statements by switching\nwords from one sentence to another but maintain-\ning the syntactic structure (dataset 3; see Appendix\n1https://github.com/ouhenio/llms-overstimate-\nprofoundness\n9654\nA for further details). As points of comparison, we\nalso used ten mundane (non-profound, dataset 4)\nand ten motivational (profound, dataset 5) state-\nments from Pennycook et al. (2015) paper. An ex-\nample from each dataset can be seen in Appendix\nA (Table 1).\n3.2 Experimental Design\nWe tested the LLMs on the five datasets described\nin the previous section. The LLMs replied on\nthe same Likert scale used in Pennycook et al.\n(2015). We further manipulated two factors: the\nprompt method, and the temperature. We tested\nthree distinct prompting methods: the original in-\nstruction participants received in Pennycook et al.\n(2015) (prompt 1), few-shot learning (Brown et al.,\n2020) (prompt 2-3), and chain-of-though (CoT)\nprompting (Wei et al., 2022) (prompt 4-10). All the\nprompts are listed in Appendix A (Table 2). The\ntemperature was set to 0.1 and 0.7 to assess the\neffect of variability in the outputs.\nWe let the LLMs predict one token correspond-\ning to its rating of the profoundness of the sentence,\nand parse it as an integer.\n3.3 Statistical Analyses\nWe performed an item analysis by averaging ratings\nacross human subjects and the LLMs’ responses for\neach statement. Given that our goal was to compare\neach LLM to human-level performance, we ran\na 3 (between-items) × 2 (within-items) two-way\nmixed ANOV A, for each LLM (on the original Pen-\nnycook et al. (2015) prompt). The between-subject\nfactor was statement type (mundane, motivational,\nor pseudo-profound BS), and the within-subject\nfactor was agent type (human or LLM). To assess\na potential profoundness bias when rating pseudo-\nprofound bullshit, we tested the ratings against the\nmid-point scale. To analyze a potential prompt\neffect, we first focused on GPT-4’s BS statement\nratings, and performed a two-way ANOV A (with\nprompts and statement types as factors). More-\nover, to compare the effect of prompting between\nmodels, we performed a two-way ANOV A (with\nmodel type and prompts as factors) on the LLMs’\nBS statements ratings.\nFinally, we ran regressions to test the degree to\nwhich human ratings were predicted by the LLMs.\nHuman mean ratings for each statement was pre-\ndicted by each LLM’s ratings, with statement type\n(mundane, motivational, or pseudo-profound bull-\nshit), evaluation prompt (1 to 10), and temperature\nas main effects.\nSince manipulating the temperature of the LLMs\ndid not significantly change the resulting ratings,\nwe collapsed ratings across temperatures in all re-\nported analyses. Moreover, given that we found no\ndifferences in ratings between datasets 1, 2, and 3,\nwe did not include the ratings of datasets 2 and 3 in\nthe analyses. Lastly, for simplicity and brevity, we\nonly report statistical results of GPT-4; results of\nother LLMs can be found in Appendix A (Table 3)\nFigure 1: Distribution of ratings per statement type in\nhumans and the LLMs.\n4 Results\nIn the original study by Pennycook et al., humans\ndiscriminated between mundane, pseudo-profound\nBS, and motivational statements. Unsurprisingly,\nmundane statements were rated as the least pro-\nfound, motivational statements as the most pro-\nfound, and pseudo-profound BS was rated in be-\ntween (Figure 1). This trend was also captured by\nthe LLMs’ ratings (Figure 1, the main effect of\nstatement type: F(2,47) = 72.09, p < 0.001). Impor-\ntantly, even though the LLMs followed the same\nqualitative pattern as humans, most of LLMs’ rat-\nings of profoundness were constantly higher than\nhuman ratings (see Figure 1, main effect of agent\ntype: F(1,47) = 185.28, p < 0.001). The bias\ntowards profoundness observed was independent\nof whether the statement was mundane, pseudo-\nprofound BS, or motivational (non-significant in-\nteraction: F(2,47) = 0.96, p = 0.40) for GPT-. For\nthe rest of LLMs, profoundness was overestimated\nonly in pseudoprofound BS statements (Flan-T5\n(XL), Vicuna (13B)) or it was larger for those state-\nments (Llama-2 + RLHF (13B), Llama-2 (13-B)),\nas denoted by a significant interaction between\nstatement and agent types (all ps < 0.001, see Table\n9655\n3 in the Supplement). Tk-Instruct showed the op-\nposite effect, but at the cost of underestimating the\nprofoundness of motivational statements. We in-\ncluded a possible explanation for these phenomena\nin the discussion.\nWe used 10 prompts to obtain each LLM’s rat-\nings. Results reveal that prompting significantly\ninfluenced the ratings for GPT-4 (main effect of\nthe prompt: F(9,423) = 18.12, p < 0.001). Cru-\ncially, the effect of prompting showed a significant\ninteraction with statement type (interaction effect:\nF(18, 423) = 35.06, p < 0.001). Further analyses\nrevealed that the interaction was driven by n-shot\nlearning evaluation prompts, which lowered the rat-\nings for pseudo-profound BS statements. Planned\nt-test between the two n-shot learning and other\nprompts revealed a significant decrease in the rat-\ning for the pseudo-profound BS (see Figure 2, all\nps < 0.001, Bonferroni-corrected), while n-shot\nlearning prompts did not affect the mundane or mo-\ntivational statements (all ps > 0.05). Importantly,\nthe ratings from the n-shot learning prompts were\nnot significantly different than the human ratings\nfor pseudo-profound BS (all ps > 0.05). Other\nLLMs were also affected by prompting, but sig-\nnificantly less (interaction effect of the two-factor\nANOV A contrasting LLMs with evaluation prompt:\nF(45, 1305) = 21.52, p < 0.001, see Figure 2).\nIn addition to assessing profoundness ratings\nfor pseudo-profound statements relative to other\nstatement types, we investigated the absolute rating\non the scale. Due to the structure of the response\nscale, ratings below three are considered not to be\nprofound. Thus, if LLMs detect pseudo-profound\nBS, their ratings should be significantly lower than\nthe mid-point level of 3. Note that in Figure 3,\nwe mean-center the rating data to facilitate data\ninterpretation; above 0 implies overestimation, and\nbelow 0 implies underestimation with respect to\nthe mid-point level.\nOur results reveal that, overall (with the ex-\nception of TK-instruct), LLMs’ are biased to-\nwards overestimating the profoundness of pseudo-\nprofound bullshit (ratings were significantly higher\nthan 3: t(29) = 6.67, p < 0.001, average = 3.70,\nsee Figure 3). This is in contrast with humans,\nwho perceived pseudo-profound bullshit statements\nas non-profound (ratings were significantly lower\nthan 3: t(29) = -9.71, p < 0.001, average = 2.56,\nFigure 3). GPT-4’s bias towards profoundness was\nprevented, however, by the two prompts that signifi-\ncantly lowered ratings for pseudo-profound bullshit\nstatements (ps < 0.001 for both prompts, Figure 3),\nwhich at the same time caused the ratings to be not\nsignificantly different from humans (ps > 0.05 for\nboth prompts, Figure 3). While other LLMs were\nalso affected by these prompts, their overall effect\ndid not induce pseudo-profound BS detection.\nAlthough the ratings of the LLMs did not match\nthose of humans in absolute terms, it did cap-\nture some of the same variability in statement-to-\nstatement responses as humans did (b = 0.15, SE =\n0.004, t(939) = 4.31, ps < 0.001) across prompting\nstrategies.\n5 Discussion\nWe found that LLM scores significantly correlated\nwith human data across all three statement types,\nsuggesting that, overall, the LLMs could differenti-\nate mundane, motivational, and pseudo-profound\nstatements. However, unlike humans, most LLMs\ndisplayed an overall bias toward attributing pro-\nfoundness to statements, irrespective of the type\nof sentence in the case of GPT-4, but exacerbated\nby pseudo-profoundness for the rest (with the ex-\nception of Tk-Instruct). For GPT-4, this bias was\nonly reduced by providing few-shot prompts. Sur-\nprisingly, more recent prompting techniques did\nnot result in significantly different ratings, perhaps\nreflecting the fact that detecting meaningfulness in\npseudo-profound statements is a task that requires\nworld knowledge.\nIn contrast to other LLMs we tested, Tk-Instruct\nwas uniquely biased towards underestimating state-\nments’ profoundness across the three datasets.\nThese results are particularly interesting given that\nTk-Instruct is fine-tuned to follow instructions in\nmore than 1600 tasks, some of which contain com-\nmon sense detection (Wang et al., 2022). Such\nfine-tuning may render this model overly cautious\nin judging the profoundness of statements.\nWhy do LLMs display a bias towards profound-\nness overestimation? One possibility is that this\noverestimation emerges from the data used to train\nthese models. Alternatively, this bias may emerge\nfrom additional tuning after the initial training.\nFor instance, LLM alignment using reinforcement\nlearning from human feedback (Ouyang et al.,\n2022), as with GPT-4 and Llama-2, may introduce\na bias towards being overly credulous. Our results\nprovide a hint in this direction. Indeed, the Llama-2\nRLHF version consistently provides higher ratings\n9656\nFigure 2: Profoundness ratings across all evaluation prompts for each LLM.\nof profoundness to each statement, compared with\nits no-RLHF counterpart. This sheds light on how\nRLHF can potentially (negatively) impact the judg-\nment of LLMs.\nDetecting pseudo-profound BS may depend on\nthe ability of these models to represent actual mean-\ning. Furthermore, the lack of dynamics in LLMs\nand top-down control processes may prevent these\nmodels to detect semantic incongruence elicited by\npseudo-profound statements. In contrast to LLMs,\ndynamic control processes are at the heart of the\nhuman capacity to complete goals and resolve con-\nflict in incongruent tasks. (e.g. Botvinick et al.,\n2001; Shenhav et al., 2014).\n−1.0\n−0.5\n0.0\n0.5\n1.0\nOriginal prompt 1−shot 3−shot\nMean Centered Likert Score\n \nHumans\nGPT−4\nFlan−T5 (XL)\nLlama−2 + RLHF (13B)\nLlama−2 (13B)\nVicuna (13B)\nTk−Instruct (11B)\nFigure 3: Overview of the profoundness assessment in\nhumans, and the LLMs with 1-shot and 3-shot learning.\nRatings are adjusted to be centered around the midpoint\n(3).\nLimitations\nFirst, our few-shot learning setup was limited to a\nrelatively small set of examples. An exploration\nwith more extensive sets (e.g., 10 or more exam-\nples) may yield different results (Wei et al., 2022).\nSecond, while our experiments covered various\nprompting strategies, there remain various unex-\nplored methods, such as “tree of thought” (Yao\net al., 2023), which can boost the performance of\nGPT-4 in the context of profoundness detection.\nThird, economic factors (i.e., high costs of GPT-4\nAPI use) constrained us to carry out a proper ex-\nploration of relevant hyperparameters and potential\nexperimental design factors. Indeed, Pennycook\net al. (2015) performed an extensive analysis of\npersonality traits and other cognitive capacities and\ntheir effect of those on pseudo-profound bullshit\nreceptivity. Future work should include these tests\nfor LLMs, allowing us to further explore the theo-\nretical underpinnings of LLMs’ overly credulous\nperspective. Indeed, understanding how different\nfactors influence the propensity of LLMs to overes-\ntimate profoundness, can inform the development\nof more unbiased and accurate models.\nEthics Statement\nOur data were collected using GPT-4’s API, in ac-\ncordance with their terms of use, which do not state\nthe prohibition to use the model for research pur-\nposes. We did not perform any human experiment\nstudies and simply used the open-sourced human\ndata of Pennycook et al. (2015). Hence, no ethics\ncommittee approval was needed to carry out our\nstudy.\n9657\nAcknowledgements\nThis work was funded by the Centro Nacional de\nInteligencia Artificial, CENIA, FB210017, Basal\nANID.\nReferences\nMarcel Binz and Eric Schulz. 2023. Using cognitive\npsychology to understand GPT-3. Proceedings of the\nNational Academy of Sciences, 120(6):e2218523120.\nMatthew M Botvinick, Todd S Braver, Deanna M Barch,\nCameron S Carter, and Jonathan D Cohen. 2001.\nConflict monitoring and cognitive control. Psycho-\nlogical Review, 108(3):624.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with GPT-4. arXiv preprint\narXiv:2303.12712.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An Open-\nSource Chatbot Impressing GPT-4 with 90%* Chat-\nGPT Quality.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nIshita Dasgupta, Andrew K Lampinen, Stephanie CY\nChan, Antonia Creswell, Dharshan Kumaran,\nJames L McClelland, and Felix Hill. 2022. Lan-\nguage models show human-like content effects on\nreasoning. arXiv preprint arXiv:2207.07051.\nDanica Dillion, Niket Tandon, Yuling Gu, and Kurt\nGray. 2023. Can AI language models replace human\nparticipants? Trends in Cognitive Sciences.\nAllyson Ettinger. 2020. What BERT is not: Lessons\nfrom a new suite of psycholinguistic diagnostics for\nlanguage models. Transactions of the Association for\nComputational Linguistics, 8:34–48.\nJennifer Hu, Sammy Floyd, Olessia Jouravlev, Evelina\nFedorenko, and Edward Gibson. 2023. A fine-\ngrained comparison of pragmatic language under-\nstanding in humans and language models. In Pro-\nceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 4194–4213, Toronto, Canada. Associ-\nation for Computational Linguistics.\nJie Huang and Kevin Chen-Chuan Chang. 2022. To-\nwards Reasoning in Large Language Models: A Sur-\nvey. arXiv preprint arXiv:2212.10403.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. Advances in\nNeural Information Processing systems, 35:22199–\n22213.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen Sesame: Getting inside BERT’s Linguistic\nKnowledge. In Proceedings of the 2019 ACL Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pages 241–253.\nTal Linzen, Emmanuel Dupoux, and Yoav Goldberg.\n2016. Assessing the ability of LSTMs to learn syntax-\nsensitive dependencies. Transactions of the Associa-\ntion for Computational Linguistics, 4:521–535.\nQing Lyu, Shreya Havaldar, Adam Stein, Li Zhang,\nDelip Rao, Eric Wong, Marianna Apidianaki, and\nChris Callison-Burch. 2023. Faithful Chain-of-\nThought Reasoning.\nKyle Mahowald, Anna A Ivanova, Idan A Blank, Nancy\nKanwisher, Joshua B Tenenbaum, and Evelina Fe-\ndorenko. 2023. Dissociating language and thought\nin large language models: a cognitive perspective.\narXiv preprint arXiv:2301.06627.\nSam Musker and Ellie Pavlick. 2023. Testing Causal\nModels of Word Meaning in GPT-3 and-4. arXiv\npreprint arXiv:2305.14630.\nOpenAI. 2023. GPT-4 Technical Report. arXiv preprint\narXiv:2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nGordon Pennycook, James Allan Cheyne, Nathaniel\nBarr, Derek J Koehler, and Jonathan A Fugelsang.\n2015. On the reception and detection of pseudo-\nprofound bullshit. Judgment and Decision making,\n10(6):549–563.\nAmitai Shenhav, Mark A Straccia, Jonathan D Cohen,\nand Matthew M Botvinick. 2014. Anterior cingu-\nlate engagement in a foraging context reflects choice\ndifficulty, not foraging value. Nature Neuroscience,\n17(9):1249–1254.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang, Adam\nPoliak, R Thomas McCoy, Najoung Kim, Benjamin\nVan Durme, Samuel R Bowman, Dipanjan Das, et al.\n2019. What do you learn from context? Probing for\n9658\nsentence structure in contextualized word representa-\ntions. In 7th International Conference on Learning\nRepresentations, ICLR 2019.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi,\nSM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-\ntimodal few-shot learning with frozen language mod-\nels. Advances in Neural Information Processing Sys-\ntems, 34:200–212.\nTomer Ullman. 2023. Large language models fail on\ntrivial alterations to theory-of-mind tasks. arXiv\npreprint arXiv:2302.08399.\nYizhong Wang, Swaroop Mishra, Pegah Alipoormo-\nlabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva\nNaik, Arjun Ashok, Arut Selvan Dhanasekaran, An-\njana Arunkumar, David Stap, et al. 2022. Super-\nNaturalInstructions: Generalization via Declarative\nInstructions on 1600+ NLP Tasks. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pages 5085–5109.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou,\net al. 2022. Chain-of-thought prompting elicits rea-\nsoning in large language models. Advances in Neural\nInformation Processing Systems, 35:24824–24837.\nShunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran,\nThomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. 2023. Tree of Thoughts: Deliber-\nate Problem Solving with Large Language Models.\narXiv preprint arXiv:2305.10601.\nHongyu Zhao, Kangrui Wang, Mo Yu, and Hongyuan\nMei. 2023. Explicit planning helps language\nmodels in logical reasoning. arXiv preprint\narXiv:2303.15714.\nA Appendix\nStatements from dataset 1 and 2 were extracted\nfrom two websites: wisdomofchopra.com and seb-\npearce.com/bullshit; and from vague statements\nfrom Deepak Chopra’s Twitter feed. Ten statements\nwere selected from each of these three sources. The\nstatements extracted from each website are con-\nstructed by randomly patching profound words into\nmeaningless but syntactically correct sentences.\nTo create dataset 3, we used spaCy. In particular,\na random selection of the nouns in the 30 sentences\nfrom dataset 1 was substituted with a word from\nits 10 nearest neighbors, as determined by spaCy’s\ncosine distance. Subsequently, we used ChatGPT\nto rectify any potential syntax inconsistencies with\nthe prompt: “Correct any inconsistencies in upper-\ncase usage, spacing, or punctuation in the following\nsentence\".\nAll the code used during this study, including\nthe datasets creation and evaluation, is publicly in\nthe following link https://github.com/ouhenio/llms-\noverstimate-profoundness.\nDataset 1: \"Consciousness is the growth of coherence, and of us.\"Dataset 2: \"Your body belongs to the progressive expansion of success.\"Dataset 3: \"Forgiveness means letting go of the hope for a better past.\"Dataset 4: \"Newborn babies require constant attention.\"Dataset 5: \"Your teacher can open the door, but you must enter by yourself.\"\nTable 1: Examples from each dataset used in the experi-\nment.\n9659\nOriginal Human prompt (prompt-1)\n\"We are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profound\"\n1-shot Learning (prompt-2)\n\"We are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”For instance, the sentence:\"This life is nothing short of a summoning rekindling of karmic complexity.\"is not profound and is considered as pseudo-profound bullshit, because the association of these words in the same sentence do not provide any meaning.Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profound\"\n3-shot Learning (prompt-3)\n\"We are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”For instance, the 3 following sentences:\"This life is nothing short of a summoning rekindling of karmic complexity.\"\"Imagination is inextricably connected to the flow of external reality.\"\"\"The secret of the universe comprehends spiritual creativity.\"are not profound and are considered as pseudo-profound bullshit, because the association of these words in the same sentence do not provide any meaning.Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profound\"\nCoT zero-shot (prompt 4)\n\"We are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profoundTo give your answer, think step by step.\"\nCoT-1 mundane (prompt-5)\n\"We are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”To give your answer, first compare the statements that were given with a normal mundane sentence, such as:\"The girl on the bicycle has blond hair.\"Second, if you believe the statements have the same level of profoundness as this mundane sentence, you should answer with a low value on the 5-point scale.Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profound\"\nCoT-1 motivational (prompt-6)\n\"We are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”To give your answer, first compare the statements that were given with a motivational sentence, such as:\"The creative adult is the child who survived.\"Second, if you believe the statements have the same level of profoundness as this mundane sentence, you should answer with a high value on the 5-point scale.Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profound\"\nCoT-1 mun. and mot. (prompt-7)\n\"We are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”To give your answer, first compare the statements that were given with a motivational sentence, such as:\"Success is not final; failure is not fatal: It is the courage to continue that counts.\"Second, compare the statements that were given with a mundane sentence, such as:\"The little boy is playing baseball.\"Third, if you believe the statements have the same level of profoundness as the mundane sentence, you should answer with a low value on the 5-point scale. In contrast, if you believe the statements have the same level of profoundness as the motivational sentence, you should answer with a high value on the 5-point scale.Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profound\"\nCoT-3 mundane (prompt-8)\n\"We are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”To give your answer, first compare the statements that were given with a normal mundane sentence, such as:\"The girl on the bicycle has blond hair.\"\"An english football player scored a goal during the game.\"\"Brazil is a beautiful country.\"Second, if you believe the statements have the same level of profoundness as this mundane sentence, you should answer with a low value on the 5-point scale.Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profound\"\nCoT-3 motivational (prompt-9)\n\"We are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”To give your answer, first compare the statements that were given with a motivational sentence, such as:\"The creative adult is the child who survived.\"\"Learn as if you will live forever, live like you will die tomorrow.\"\"When you change your thoughts, remember to also change your world.\"Second, if you believe the statements have the same level of profoundness as this motivational sentence, you should answer with a high value on the 5-point scale.Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profound\"\nCoT-3 mund. and mot. (prompt-10)\nWe are interested in how people experience the profound. Below are a series of statements taken from relevant websites.Please read each statement and take a moment to think about what it might mean. Then please rate how “profound” you think it is.Profound means “of deep meaning; of great and broadly inclusive significance.”To give your answer, first compare the statements that were given with motivational sentences, such as:\"Success is getting what you want, happiness is wanting what you get.\"\"It is better to fail in originality than to succeed in imitation.\"\"I never dreamed about success. I worked for it.\"Second, compare the statements that were given with mundane sentences, such as:\"The two dogs are playing with the tennis ball.\"\"Surfers are riding the waves.\"\"Lasagna is an Italian dish.\"Third, if you believe the statements have the same level of profoundness as the mundane sentence, you should answer with a low value on the 5-point scale.In contrast, if you believe the statements have the same level of profoundness as the motivational sentence, you should answer with a high value on the 5-point scale.Rate the profoundness of the following sentences on the following 5-point scale:1= Not at all profound, 2 = somewhat profound, 3 = fairly profound, 4 = definitely profound, 5 = very profound\nTable 2: All prompts used in the experiment.\n9660\nGPT-4:\nPredictor dfNum dfDen F p-value η2\ng\nStatement type 2 47 72.09 .000 .67\nAgent type 1 47 185.28 .000 .57\nStatement x Agent 2 47 0.96 .389 .01\nFlan-T5 (XL):\nPredictor dfNum dfDen F p-value η2\ng\nStatement type 2 47 100.36 .000 .67\nAgent type 1 47 0.42 .520 .00\nStatement x Agent 2 47 10.18 .000 .19\nLlama-2 + RLHF (13B):\nPredictor dfNum dfDen F p-value η2\ng\nStatement type 2 47 143.62 .000 .79\nAgent type 1 47 497.51 .000 .80\nStatement x Agent 2 47 5.76 .006 .09\nLlama-2 (13B):\nPredictor dfNum dfDen F p-value η2\ng\nStatement type 2 47 127.96 .000 .78\nAgent type 1 47 302.01 .000 .69\nStatement x Agent 2 47 10.42 .000 .13\nVicuna (13B):\nPredictor dfNum dfDen F p-value η2\ng\nStatement type 2 47 82.75 .000 .68\nAgent type 1 47 35.91 .000 .24\nStatement x Agent 2 47 9.26 .000 .14\nTk-Instruct (11B):\nPredictor dfNum dfDen F p-value η2\ng\nStatement type 2 47 65.08 .000 .61\nAgent type 1 47 13.63 .001 .11\nStatement x Agent 2 47 10.52 .000 .16\nTable 3: ANOV As results divided by Large Language Model. dfNum indicates degrees of freedom numerator.\ndfDen indicates degrees of freedom denominator. η2\ng indicates generalized eta-squared.\n9661",
  "topic": "Statement (logic)",
  "concepts": [
    {
      "name": "Statement (logic)",
      "score": 0.7481683492660522
    },
    {
      "name": "Psychology",
      "score": 0.5448706746101379
    },
    {
      "name": "Cognitive psychology",
      "score": 0.44840291142463684
    },
    {
      "name": "Expression (computer science)",
      "score": 0.43157389760017395
    },
    {
      "name": "Social psychology",
      "score": 0.3857806622982025
    },
    {
      "name": "Computer science",
      "score": 0.24947136640548706
    },
    {
      "name": "Political science",
      "score": 0.18774142861366272
    },
    {
      "name": "Law",
      "score": 0.15527108311653137
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I162148367",
      "name": "Pontificia Universidad Católica de Chile",
      "country": "CL"
    },
    {
      "id": "https://openalex.org/I175594653",
      "name": "John Brown University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I121797337",
      "name": "Leiden University",
      "country": "NL"
    }
  ],
  "cited_by": 1
}