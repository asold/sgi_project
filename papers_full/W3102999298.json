{
  "title": "Birds have four legs?! NumerSense: Probing Numerical Commonsense Knowledge of Pre-Trained Language Models",
  "url": "https://openalex.org/W3102999298",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2563984569",
      "name": "Bill Yuchen Lin",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2142831431",
      "name": "Seyeon Lee",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2058937921",
      "name": "Rahul Khanna",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A2108009659",
      "name": "Xiang Ren",
      "affiliations": [
        "University of Southern California"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998696444",
    "https://openalex.org/W2908460844",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2970161131",
    "https://openalex.org/W2991223644",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2107901333",
    "https://openalex.org/W2963921564",
    "https://openalex.org/W2964207259",
    "https://openalex.org/W2990928880",
    "https://openalex.org/W2970308008",
    "https://openalex.org/W2919420119",
    "https://openalex.org/W3111372685",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2805206884",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2983331060",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W2998557616",
    "https://openalex.org/W2948445958",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3022006665",
    "https://openalex.org/W2986266667",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2983995706",
    "https://openalex.org/W2995638926",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W2999121455",
    "https://openalex.org/W2987805863",
    "https://openalex.org/W2989322838",
    "https://openalex.org/W2972324944"
  ],
  "abstract": "Recent works show that pre-trained language models (PTLMs), such as BERT, possess certain commonsense and factual knowledge. They suggest that it is promising to use PTLMs as “neural knowledge bases” via predicting masked words. Surprisingly, we find that this may not work for numerical commonsense knowledge (e.g., a bird usually has two legs). In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. In this paper, we investigate whether and to what extent we can induce numerical commonsense knowledge from PTLMs as well as the robustness of this process. To study this, we introduce a novel probing task with a diagnostic dataset, NumerSense, containing 13.6k masked-word-prediction probes (10.5k for fine-tuning and 3.1k for testing). Our analysis reveals that: (1) BERT and its stronger variant RoBERTa perform poorly on the diagnostic dataset prior to any fine-tuning; (2) fine-tuning with distant supervision brings some improvement; (3) the best supervised model still performs poorly as compared to human performance (54.06% vs. 96.3% in accuracy).",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6862–6868,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n6862\nBirds have four legs?!\nNumerSense: Probing Numerical Commonsense Knowledge\nof Pre-Trained Language Models\nBill Yuchen Lin Seyeon Lee Rahul Khanna Xiang Ren\n{yuchen.lin,seyeonle,rahulkha,xiangren}@usc.edu\nDepartment of Computer Science,\nUniversity of Southern California\nAbstract\nRecent works show that pre-trained language\nmodels (PTLMs), such as BERT, possess\ncertain commonsense and factual knowledge.\nThey suggest that it is promising to use PTLMs\nas “neural knowledge bases” via predicting\nmasked words. Surprisingly, we ﬁnd that this\nmay not work for numerical commonsense\nknowledge (e.g., a bird usually has two legs).\nIn this paper, we investigate whether and to\nwhat extent we can induce numerical com-\nmonsense knowledge from PTLMs as well as\nthe robustness of this process. To study this,\nwe introduce a novel probing task with a di-\nagnostic dataset, N UMER SENSE 1, containing\n13.6k masked-word-prediction probes (10.5k\nfor ﬁne-tuning and 3.1k for testing). Our anal-\nysis reveals that: (1) BERT and its stronger\nvariant RoBERTa perform poorly on the diag-\nnostic dataset prior to any ﬁne-tuning; (2) ﬁne-\ntuning with distant supervision brings some\nimprovement; (3) the best supervised model\nstill performs poorly as compared to human\nperformance (54.06% vs. 96.3% in accuracy).\n1 Introduction\nPre-trained language models (PTLMs), such as\nBERT (Devlin et al., 2019), have yielded state-\nof-the-art performance on many natural language\nprocessing tasks. Given PTLMs’ cited ability\nto create general, yet useful text representations,\nan investigation of their ability to encode com-\nmonsense knowledge into representations is war-\nranted––commonsense knowledge is often required\nto have a full understanding of language.\nRecently there have been a few recent works\nthat do investigate the inquiry of whether PTLMs\npossess commonsense knowledge (Petroni et al.,\n2019; Davison et al., 2019; Bouraoui et al., 2020).\nOverall, these prior studies suggest that PTLMs are\n1https://inklab.usc.edu/NumerSense/\n1st:fly(79.5%)2nd:sing(9.1%)\n1st:four(44.8%)2nd:two(18.7%)\nBirdscan[MASK].\nAbirdusuallyhas[MASK]legs.Acarusuallyhas[MASK]wheels.Acarusuallyhas[MASK]roundwheels.\nHowever,forNumericalCommonsenseKnowledge:\n1st:four(53.7%)2nd:two(20.5%)1st:two(37.1%)2nd:four(20.2%)\nBERT-LargeMaskedWordPrediction\nFigure 1: Top: PTLMs often cannot solve masked language\nmodeling tasks needing numerical commonsense knowledge,\nhence our title. Bottom: Even when PTLMs seemingly suc-\nceed, they fail to stay consistent under small perturbations.\ncreating text representations that often have com-\nmonsense knowledge encoded in them. We, how-\never, ﬁnd it surprising that when posed with a simi-\nlar reasoning-based masked-word-prediction task,\nPTLMs perform poorly in recalling the required\nnumerical commonsense knowledge (see Figure 1).\nTherefore, in this paper, our goal is to study\nwhether PTLMs capture numerical commonsense\nknowledge, i.e., commonsense knowledge that pro-\nvides an understanding of the numeric relation be-\ntween entities. We propose measuring this capa-\nbility via a masked-word-prediction based probing\ntask, where, the ranking of numeric words by what\nthe model believes most probably ﬁlls the mask\nwould expose the capabilities of PTLMs to capture\nnumeric commonsense knowledge. For example,\nthe masked position in the sentence “A bird usually\nhas [MASK] legs.” is best ﬁlled by the number\n“two” when considering only numerical words.\nAround this concept, we built a carefully crafted\ndataset, NUMER SENSE , of 3,145 probes that covers\nquestions from 8 different categories such as every-\nday objects, biology, geometry, etc. In our initial\nexperiments, we ﬁnd PTLMs to be brittle against\nadversarial attacks. As shown in the bottom sec-\ntion of Figure 1, BERT initially correctly predicts\nthe masked word to be “four”, but it changes its\ntop result to “two” in the slightly perturbed second\n6863\nsentence (a simple insertion of the word ‘round’).\nThus, we intentionally included adversarial exam-\nples in the probes to test the robustness.\nWe evaluate PTLMs in two settings (Section 3):\n(1) a zero-shot setting, meaning no probes from our\ndataset were used to ﬁne-tune the models before\nevaluation; (2) a distant supervision setting, where\nmodels were ﬁne-tuned on examples from related\ncommonsense reasoning datasets before being eval-\nuated on ours. Our ﬁndings reveal that PTLMs are\nstill much worse than humans on the task, although\nﬁne-tuning with distant supervision can help. We\nalso provide some cursory analysis on why PTLMs\nperhaps perform so poorly, pointing to interesting\nfuture research. We also hope our work can beneﬁt\nfuture works in: 1) improving PTLMs’ abilities\nto faithfully capture (numerical) commonsense, 2)\npopulating numerical facts in current commonsense\nknowledge bases, and 3) open-domain QA ––“Q:\nHow many legs do ants have?” “A: Six! ”\n2 The N UMER SENSE Probing Task\nWe introduce our numerical commonsense reason-\ning probing task, as well as the creation process of\nthe namesake dataset, NUMER SENSE . Then, we\nprovide a breakdown of what types of knowledge\nare covered by the probes and ﬁnally include ad-\nditional high-quality distant supervision to test if\nﬁne-tuning can improve performance.\n2.1 Task Formulation\nWe essentially probe PTLMs with the distribution\nof words a PTLM thinks could ﬁll the masked po-\nsition, by ranking their softmax scores (greatest to\nleast). If the ranking demonstrates numerical com-\nmonsense knowledge––the highest ranked number\nword (e.g., “one”, “two”, and so on) is the correct\nanswer––then that probe is successfully completed\nby the PTLM. The masked position in each probe\nis chosen such that a number word is an extremely\nprobable way of ﬁlling in the blank.\n2.2 Probing Data Collection\nTo build a suitable dataset for the proposed probing\ntask, we make use of an existing corpus consisting\nof commonsense assertions, named Open Mind\nCommon Sense (OMCS) (Singh et al., 2002). We\nﬁrst extracted the sentences from OMCS that had\nat least one of the following 12 number words:\nCategoryExampleObjects(35.2%)Abicycle has twotires. Biology(13.5%)Ants havesixlegs.Geometry(11.7%)Acube has sixfaces.Unit(6.3%)Therearesevendaysinaweek.Math(7.3%)Iwillbetennextyear,asIamninenow.Physics(5.7%)Water will freeze at zerodegrees centigrade.Geography(2.9%)The world contains sevencontinents.Misc.(17.5%)There arenoprinces in the United States.\nTable 1: NUMER SENSE examples of each category.\n{“no”2, “zero”, “one”, “two”, ..., “ten”}.\nHowever, as to be expected, there were many\nnoisy statements which were either 1) incorrect, 2)\ncontaining typos, or 3) having no numerical com-\nmonsense logic. We thus manually and pragmati-\ncally reﬁned these sentences and did two rounds of\nvetting by different graduate students, from which\nwe only kept the statements that were accepted by\nall annotators. After this strict ﬁltration process,\nwe ended up 1,131 cleaned statements for probing.\nWe did an initial test and observed that PTLMs\ncan be brittle under a simple perturbation of insert-\ning an adjective near the masked number word.\nThus, in order to study the robustness of mod-\nels in our proposed task, we also added adver-\nsarial examples to our dataset by adding adjec-\ntives before the noun involved in the numerical\nreasoning in each probe. The candidate adjec-\ntives are generated by querying relevant triples (e.g.\n<wheel, HasProperty, round> for the ex-\nample in Fig. 1) in the commonsense knowledge\ngraph, ConceptNet (Speer et al., 2017), and fur-\nther selected or modiﬁed by human annotators to\nassure adversarial examples are still valid and nat-\nural. We ﬁnally have 3,145 testing probes for NU-\nMER SENSE as the diagnostic dataset.\nWe also manually annotated the category label\nfor each instance so that we can better understand\nthe covered topics and their percentage. We found 8\ntypes of numerical commonsense knowledge rang-\ning from tangible everyday objects (e.g., car, guitar,\nand table) to geometry (e.g., cube). Table 1 lists\nsome concrete examples of each category.\n2.3 Supervision for Fine-Tuning PTLMs\nOne may wonder if ﬁne-tuning towards this task\ncould improve the performance. In order to an-\n2We include “no”, as there exists statements involving\nnumerical commonsense knowledge, where “no” is used in\nplace of zero, “There are no princes in the United States.”\n6864\nCore Probes + Adversarial Examples\nModels hit@1hit@2hit@3hit@1hit@2 hit@3\nGPT-2 29.86 50.88 67.49 24.73 44.21 62.30\nBERT-Base31.98 55.92 70.58 25.24 48.66 64.81\nRoBERTa-Base36.04 60.42 72.08 28.39 51.91 67.29\nBERT-Large37.63 62.01 76.77 27.18 52.89 70.22\nRoBERTa-Large45.85 66.70 80.04 35.66 58.52 74.44\nFt. BERT-L.50.00 66.34 74.91 43.58 62.27 72.92\nFt. RoBERTa-L.54.06 69.61 79.15 47.52 66.43 76.76\nHuman Bound89.7(α) / 96.3(β) 88.3(α) / 93.7(β)\nTable 2: Results (%) of PTLMs on NUMER SENSE . ‘Ft.’\nstands for ‘Fine-tuned.’ The human performance is\nshown by closed testing (α=‘no external information’)\n/ open testing (β=‘Wikipedia is allowed’).\nswer this question, we further collected training\nsentences from the GenericsKB corpus (Bhaktha-\nvatsalam et al., 2020). The sentences in Generic-\nsKB are generic commonsense statements that are\nextracted from Simple Wikipedia, Common Crawl\nwithin educational domains, ARC corpus, etc.\nWe collected these sentences by ﬁrst obtaining a\nlist of frequent nouns from various caption cor-\npora such as MSCOCO (Lin et al., 2014) and\nV ATEX (Wang et al., 2019). Then, we selected\ncollected sentences contained at least one number\nword of interest and ﬁnally go through the same hu-\nman annotator veriﬁcation process as the test data.\nWe ended up collecting 10,492 sentences for ﬁne-\ntuning and believe these sentences, if used properly,\ncan improve PTLMs’ ability to recall the numerical\ncommonsense knowledge.\n2.4 Statistics of NUMER SENSE\nWe show the distribution of the truth number words\nin the test data in Fig. 2. The average length of the\nsentence in training data is 11.1 and it is 8.9 in test\ndata.\nFigure 2: Truth number distribution of the test set.\n3 Empirical Analysis\nWe introduce the set-up of the experiments and\nthen present results from different PTLMs in both\na zero-shot setting and a distantly supervised ﬁne-\ntuned one. We will also provide some analysis on\nthe robustness and biases in the various models,\nand ﬁnally a study of the performance of a state-of-\nthe-art open-domain question-answering model.\n3.1 Experiment Set-up\nWe run our experiments in two settings, zero-\nshot inference and additional supervision via ﬁne-\ntuning. In the ﬁrst setting, we probe PTLMs\nwithout any modiﬁcations, speciﬁcally we use\nBERT and RoBERTa with pre-trained masked-\nword-prediction heads.\nIn our second setting, we use our collected addi-\ntional supervision dataset (Sec. 2.3) and mask the\nnumber words in each sentence. We then proceed\nto ﬁne tune the models above on these masked sen-\ntences, before evaluating them on NUMER SENSE .\n3.2 Evaluation Metric and Human Bound\nA masked-word-prediction head (either ﬁne-tuned\nor not) produces a probability distribution over its\nwhole vocabulary via a softax layer. As mentioned\nin Sec. 2.1, NUMER SENSE is the task of using this\nprobability distribution to rank all number words,\nand evaluating this ranking. To evaluate, we use\nhit@1/2/3 accuracy, which calculates the percent-\nage of predictions where the correct number word\nis ranked in the top knumber words.3\nTo estimate human performance on the task, we\nsampled 300 examples and asked two groups of\nthree people to ﬁll in the masked word, where one\ngroup had access to external information ( open-\nbook test) from the Web such as Wikipedia and\nthe other did not (closed-book test). We take the\nmajority label as the ﬁnal human label.\n3.3 Experimental results\nWe show our experimental results in Table 2. The\nﬁrst four lines are results from PTLMs in the zero-\nshot inference setting. We see that size matters, as\nthere is a clear performance gain when the model\nsizes increases. Also, RoBERTa’s results are con-\nsistently better than BERT’s, which is probably\nbecause RoBERTa uses a larger training corpora\n3We also report the performance of GPT-2 by iteratively\nﬁlling the masked word and rank with their perplexity.\n6865\nTable 1\nCategory RoBERTa-L Human (closed-\nbook)\nObjects 46.78 93.88\nBiology 41.06 85.71\nGeometry 33.08 97.14\nUnit 26.39 88.89\nMath 43.37 94.44\nPhysics 27.69 73.68\nGeography 36.36 60.00\nMisc. 44.72 81.82\nCommmon Obj.\nBiology\nGeometry\nUnit\nMath\nPhy. & Ast.\nGeography\nMisc.\nCategory\nObjects\nBiology\nGeometryUnit\nMath\nPhysics\nGeographyMisc.0.00 25.00 50.00 75.00\n81.82\n60.00\n73.68\n88.89\n85.71\n44.72\n36.36\n27.69\n43.37\n26.39\n33.08\n41.06\n46.78\nRoBERTa-L Human (closed-book)\nObjects\nBiology\nGeometry\nUnit\nMath\nPhysics\nGeography\nMisc.\n0.00 25.00 50.00 75.00 100.00\n81.82\n60.00\n73.68\n94.44\n88.89\n97.14\n85.71\n93.88\n44.72\n36.36\n27.69\n43.37\n26.39\n33.08\n41.06\n46.78\nRoBERTa-L Human (closed-book)\n1\nFigure 3: Performance of RoBERTa-Large V .S. human\nperformance (closed-book tests) on different categories\nof numerical commonsense knowledge.\nand focuses more on masked language modeling in\nits pre-training stage.\nWe see that our ﬁne-tuning efforts do help im-\nprove model performance: “ 37.63 → 50.00” for\nBERT-large and “45.85 → 54.06” for RoBERTa-\nlarge. However, both are still far from the human’s\nclosed-book evaluation. Figure 3 shows PTLMs\nperformance is poor across all categories within the\ncore set of NUMER SENSE .\nComparing the performance of a PTLM on the\n“Core Probes” set (#=1,131) versus the “+ Adver-\nsarial Examples” set (#=3,145), we can measure\ntheir robustness. We found all models incur a sig-\nniﬁcant performance drop when being evaluated\non the adversarial set. This suggests that PTLMs\n(even when ﬁne-tuned) can be brittle towards adver-\nsarial attacks, and future direction in pre-training\nlanguage models should consider more structured\ninductive biases such as dependencies and semantic\nroles when learning contextual representations.\n4 Case Studies\nObject bias. Recall the example “a bird usually\nhas [MASK] legs,” which BERT-Large predicts to\nbe “four”. Does BERT-Large always predict “four”\nas long as the adjacent word after the [MASK] is\n‘legs’? To investigate if the bias exists, we show\nsome case studies in Table 3. As 1,000 different\nrandomly generated words ﬁll the ‘[x]’s we see\nthat both BERT and RoBERTa have a bias towards\na certain answer, evidenced by the existence of a\ndominant answer in the softmax distribution. How-\never, it seems that RoBERTa’s (Liu et al., 2019)\nmodiﬁed pre-training strategy helps it have less\nbias. We argue that future studies should further\ncontrol the bias in masked language modeling.\nAttention distribution. Following the prior prob-\ning work (Clark et al., 2019) on the relationship\nbetween attention weights and syntactic structures,\nwe plot the attention distribution of the sentence\n“A bird usually has two legs.” with respect to the\nword ‘two’ in Figure 4. We ﬁnd that the root word\n‘has’ enjoys the maximum attention at in the ﬁrst\nfew and middle layers, while the word ‘two’ gets\nthe maximum attention to itself in the end. The\nimportant words for querying the numerical com-\nmonsense, namely ‘birds’ and ‘legs’, always have\nlow attention weights. This suggests that the BERT\n(and RoBERTa) may inherently lose the relation-\nship between subject/object and number words.\n5 Open-Domain ‘How-Many’ Questions\nThe examples in the NUMER SENSE can be also\nseen as open-domain questions targeting ‘how-\nmany’ commonsense––“how many legs does a\nﬂy usually have?” Answering these open-domain\nnumerical commonsense questions is a practical\ndownstream application of models that are success-\nful in the NUMER SENSE . Thus, as a side note, we\nalso report the performance of the state-of-the-art\nopen-domain QA model (Asai et al., 2020).\nWe use the model that is trained on the Natural\nQuestion (NQ) dataset (Kwiatkowski et al., 2019),\nwhere we replace the ‘[MASK]’s in our examples\nwith ‘how many’, so that our probes are in a sim-\nilar format to NQ examples. For example “a ﬂy\nusually has [MASK] legs” is converted to “ how\nmany legs a ﬂy usually has?”4 The accuracy of the\nstate-of-the-art model is only15.4%, which is even\nlower than using BERT-base without ﬁne-tuning.\nThis indicates that improving performance on NU-\nMER SENSE can help improve the performance on\nanswering open-domain “how-many” questions.\n6 Related Work\nProbing Tasks for PTLMs.Prior work in probing\nlanguage models have primarily focused on anal-\nysis of linguistic phenomena. Clark et al. (2019)\ninvestigated the relationship between BERT’s atten-\ntion weights and syntactic structures, while such as\ndependency (e.g. direct objects, noun modiﬁers),\ncoreference, and sentence segmentation. Tenney\net al. (2019) was able to display where certain\ntypes of linguistic information is captured within\nBERT––they in fact ﬁnd the layers in a PTLM rep-\nresent the steps of a classical NLP pipeline: POS\n4We also manually test some queries such as “how many\nlegs does a ﬂy usually have?”, which have similar results.\n6866\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1\n1 13 25 37 49 61 73 85 97 109 121 133\nA\nbird\nusually\nhas\ntwo\nlegs\n.\nFigure 4: The attention distribution of the sentence “A bird usually has two legs.” on RoBERTa-base. We plot the attention\nweights (y) between each word and the number word ‘two’ at different position (x), e.g., x = 13means (Layer 2, Head 1).\nTemplate:a[x]usuallyhas[MASK]legs.BERT-L four: 39.3%,two: 18.3%,three: 10.1% RoBERTa-L four: 20.8%, two: 9.0%, three: 8.1%Template:most[x]have[MASK]wheels.BERT-L four: 25.3%, two: 14.1%, three: 5.1%RoBERTa-L four: 9.2%, two: 7.8%, three: 4.6%Template:all[x]have[MASK]sides.BERT-L two: 28.3%, three: 12.9%, four: 12.9%RoBERTa-L two: 16.6%, no: 2.9%, three: 2.3%\nTable 3: The average Softmax of top 3 predictions in\ntemplates where ‘[x]’ is ﬁlled with 1k random words.\ntagging, parsing, NER, semantic roles, and coref-\nerence. This line of work has indeed helped us\nunderstand the ability of PTLMs to capture linguis-\ntic knowledge via self-supervised learning from\nunlabeled data. We are interested in the numerical\ncommonsense knowledge of PTLMs.\nProbing Commonsense Knowledge.Besides the\nworks that we have discussed in Section 1, Zhou\net al. (2020) and Talmor et al. (2019a) also pro-\nposed to probe the commonsense knowledge of pre-\ntrained language models, following the prior work\nby Trinh and Le (2018a and 2018b). They both\nutilized various existing language understanding\ndatasets targeting commonsense knowledge to test\nif PTLMs can capture certain commonsense knowl-\nedge. Lin et al. (2019a) also show that PTLMs\ncan retrieve paths from ConceptNet that aid in in-\nterpreting the decision made by the PTLMs on the\nCommonsenseQA dataset (Talmor et al., 2019b).\nLin et al. (2019b) probe the commonsense knowl-\nedge in pre-trained language generation models via\na constrained text generation task. However, they\ndo not consider numerical commonsense knowl-\nedge, which is relatively under-explored area.\nNumerical Commonsense Knowledge. Forbes\nand Choi (2017) and Goel et al. (2019) studied\ncommonsense comparisons between two physi-\ncal objects (e.g., a house is usually bigger than\na person) in pre-trained word embeddings. Elazar\net al. (2019) and Yamane et al. (2020) propose to\ninduce the commonsense distribution of quantita-\ntive attributes (e.g., mass, length, and currency) of\nobjects. Their goal is to extract or crowd-source\nsuch numerical attributes, and then obtain distribu-\ntions that reﬂect commonsense knowledge. NU-\nMER SENSE , however, mainly focuses on exact nu-\nmerical commonsense facts (e.g., a bird has two\nlegs) instead of a range of values (e.g., a tiger\nweighs around 120kg), and have a larger number\nof arguments besides physical attributes.\nEncoding Numerics for Computation.Wallace\net al. (2019) probe PTLMs in terms of the abil-\nity to represent numeracy tokens by a regression\ntask (e.g., “71”→ 71.0), and also ﬁnd that BERT\nis not good at encoding numerical tokens. Some\nworks focus on incorporate algebra computation\nability in PTLMs (Zou and Lu, 2019; Geva et al.,\n2020), thus making them able to answer math rea-\nsoning tasks such as MAWPS (Koncel-Kedziorski\net al., 2016) and DROP (Dua et al., 2019). Note\nthat these models and tasks are not targeting nu-\nmerical commonsense knowledge but mainly the\nnumerical-related computation within text.\n7 Conclusion\nWe present a probing task, NUMER SENSE , to in-\nduce numerical commonsense knowledge from pre-\ntrained language models. We collect a new diag-\nnostic dataset carefully veriﬁed by human anno-\ntators, which covers 8 different topics. Powerful\npre-trained models such as BERT and RoBERTa\nperform surprisingly poorly, even after ﬁne-tuning\nwith high-quality distant supervision. We hope our\nﬁndings and probing dataset will provide a basis\nfor improving pre-trained masked language mod-\nels’ numerical and other concrete types of com-\nmonsense knowledge.\n6867\nAcknowledgements\nThis research is based upon work supported in part\nby the Ofﬁce of the Director of National Intelli-\ngence (ODNI), Intelligence Advanced Research\nProjects Activity (IARPA), via Contract No. 2019-\n19051600007, the DARPA MCS program under\nContract No. N660011924033 with the United\nStates Ofﬁce Of Naval Research, the Defense\nAdvanced Research Projects Agency with award\nW911NF-19-20271, and NSF SMA 18-29268. The\nviews and conclusions contained herein are those\nof the authors and should not be interpreted as\nnecessarily representing the ofﬁcial policies, either\nexpressed or implied, of ODNI, IARPA, or the\nU.S. Government. We would like to thank all the\ncollaborators in USC INK research lab for their\nconstructive feedback on the work.\nReferences\nAkari Asai, Kazuma Hashimoto, Hannaneh Hajishirzi,\nRichard Socher, and Caiming Xiong. 2020. Learn-\ning to retrieve reasoning paths over wikipedia graph\nfor question answering. In International Conference\non Learning Representations.\nSumithra Bhakthavatsalam, Chloe Anastasiades, and\nPeter Clark. 2020. Genericskb: A knowledge base\nof generic statements. ArXiv, abs/2005.00660.\nZied Bouraoui, Jose Camacho-Collados, and Steven\nSchockaert. 2020. Inducing relational knowledge\nfrom bert. In Proceedings of the Thirty-Fourth AAAI\nConference on Artiﬁcial Intelligence.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP.\nJoe Davison, Joshua Feldman, and Alexander Rush.\n2019. Commonsense knowledge mining from pre-\ntrained models. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 1173–1178, Hong Kong, China. As-\nsociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nDheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel\nStanovsky, Sameer Singh, and Matt Gardner. 2019.\nDROP: A reading comprehension benchmark requir-\ning discrete reasoning over paragraphs. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers) , pages 2368–2378, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nYanai Elazar, Abhijit Mahabal, Deepak Ramachandran,\nTania Bedrax-Weiss, and Dan Roth. 2019. How\nlarge are lions? inducing distributions over quanti-\ntative attributes. In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3973–3983, Florence, Italy. Associa-\ntion for Computational Linguistics.\nMaxwell Forbes and Yejin Choi. 2017. Verb physics:\nRelative physical knowledge of actions and objects.\nIn Proceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 266–276, Vancouver, Canada.\nAssociation for Computational Linguistics.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting numerical reasoning skills into language\nmodels. ArXiv, abs/2004.04487.\nPranav Goel, Shi Feng, and Jordan Boyd-Graber. 2019.\nHow pre-trained word representations capture com-\nmonsense physical comparisons. In Proceedings\nof the First Workshop on Commonsense Inference\nin Natural Language Processing , pages 130–135,\nHong Kong, China. Association for Computational\nLinguistics.\nRik Koncel-Kedziorski, Subhro Roy, Aida Amini,\nNate Kushman, and Hannaneh Hajishirzi. 2016.\nMAWPS: A math word problem repository. In Pro-\nceedings of the 2016 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n1152–1157, San Diego, California. Association for\nComputational Linguistics.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Al-\nberti, Danielle Epstein, Illia Polosukhin, Jacob De-\nvlin, Kenton Lee, Kristina Toutanova, Llion Jones,\nMatthew Kelcey, Ming-Wei Chang, Andrew M. Dai,\nJakob Uszkoreit, Quoc Le, and Slav Petrov. 2019.\nNatural questions: A benchmark for question an-\nswering research. Transactions of the Association\nfor Computational Linguistics, 7:453–466.\nBill Yuchen Lin, Xinyue Chen, Jamin Chen, and Xi-\nang Ren. 2019a. KagNet: Knowledge-aware graph\nnetworks for commonsense reasoning. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 2829–2839, Hong\nKong, China. Association for Computational Lin-\nguistics.\n6868\nBill Yuchen Lin, Ming Shen, Yu Xing, Pei Zhou, and\nXiang Ren. 2019b. CommonGen: A constrained\ntext generation dataset towards generative common-\nsense reasoning. ArXiv, abs/1911.03705.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar S. Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke S. Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized bert pretraining ap-\nproach. ArXiv, abs/1907.11692.\nFabio Petroni, Tim Rockt ¨aschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nPush Singh, Thomas Lin, Erik T Mueller, Grace Lim,\nTravell Perkins, and Wan Li Zhu. 2002. Open mind\ncommon sense: Knowledge acquisition from the\ngeneral public. In OTM Confederated International\nConferences” On the Move to Meaningful Internet\nSystems”, pages 1223–1237. Springer.\nRobyn Speer, Joshua Chin, and Catherine Havasi. 2017.\nConceptnet 5.5: An open multilingual graph of\ngeneral knowledge. In Proceedings of the Thirty-\nFirst AAAI Conference on Artiﬁcial Intelligence ,\nAAAI’17, page 4444–4451. AAAI Press.\nAlon Talmor, Yanai Elazar, Yoav Goldberg, and\nJonathan Berant. 2019a. olmpics - on what\nlanguage model pre-training captures. ArXiv,\nabs/1912.13283.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019b. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4149–4158, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 4593–\n4601, Florence, Italy. Association for Computational\nLinguistics.\nTrieu H. Trinh and Quoc V . Le. 2018a. Do language\nmodels have common sense.\nTrieu H. Trinh and Quoc V . Le. 2018b. A sim-\nple method for commonsense reasoning. ArXiv,\nabs/1806.02847.\nEric Wallace, Yizhong Wang, Sujian Li, Sameer Singh,\nand Matt Gardner. 2019. Do NLP models know\nnumbers? probing numeracy in embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 5307–\n5315, Hong Kong, China. Association for Computa-\ntional Linguistics.\nXin Wang, Jiawei Wu, Junkun Chen, Lei Li, Yuan-\nFang Wang, and William Yang Wang. 2019. Vatex:\nA large-scale, high-quality multilingual dataset for\nvideo-and-language research. In The IEEE Interna-\ntional Conference on Computer Vision (ICCV).\nHiroaki Yamane, Chin-Yew Lin, and Tatsuya Harada.\n2020. Measuring numerical common sense: Is a\nword embedding approach effective?\nXuhui Zhou, Y . Zhang, Leyang Cui, and Dandan\nHuang. 2020. Evaluating commonsense in pre-\ntrained language models. In Proceedings of the\nThirty-Fourth AAAI Conference on Artiﬁcial Intelli-\ngence.\nYanyan Zou and Wei Lu. 2019. Text2Math: End-to-\nend parsing text into math expressions. In Proceed-\nings of the 2019 Conference on Empirical Methods\nin Natural Language Processing and the 9th Inter-\nnational Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 5327–5337, Hong\nKong, China. Association for Computational Lin-\nguistics.",
  "topic": "Commonsense knowledge",
  "concepts": [
    {
      "name": "Commonsense knowledge",
      "score": 0.8211405277252197
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.7968450784683228
    },
    {
      "name": "Computer science",
      "score": 0.7881314754486084
    },
    {
      "name": "Artificial intelligence",
      "score": 0.605451226234436
    },
    {
      "name": "Natural language processing",
      "score": 0.524569034576416
    },
    {
      "name": "Task (project management)",
      "score": 0.4928727149963379
    },
    {
      "name": "Word-sense disambiguation",
      "score": 0.4817366898059845
    },
    {
      "name": "Language model",
      "score": 0.4776017367839813
    },
    {
      "name": "Deep neural networks",
      "score": 0.468860924243927
    },
    {
      "name": "Commonsense reasoning",
      "score": 0.4491285979747772
    },
    {
      "name": "Machine learning",
      "score": 0.44803568720817566
    },
    {
      "name": "Artificial neural network",
      "score": 0.4338090717792511
    },
    {
      "name": "Process (computing)",
      "score": 0.42314282059669495
    },
    {
      "name": "Knowledge extraction",
      "score": 0.21852228045463562
    },
    {
      "name": "WordNet",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1174212",
      "name": "University of Southern California",
      "country": "US"
    }
  ],
  "cited_by": 114
}