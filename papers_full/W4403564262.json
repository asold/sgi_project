{
    "title": "Automating Bibliometric Analysis with Sentence Transformers and Retrieval-Augmented Generation (RAG): A Pilot Study in Semantic and Contextual Search for Customized Literature Characterization for High-Impact Urban Research",
    "url": "https://openalex.org/W4403564262",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2099777408",
            "name": "Haowen Xu",
            "affiliations": [
                "Oak Ridge National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A2098364502",
            "name": "Xueping Li",
            "affiliations": [
                "University of Tennessee at Knoxville"
            ]
        },
        {
            "id": "https://openalex.org/A4315061758",
            "name": "Jose Tupayachi",
            "affiliations": [
                "University of Tennessee at Knoxville"
            ]
        },
        {
            "id": "https://openalex.org/A4307330207",
            "name": "Jianming (Jamie) Lian",
            "affiliations": [
                "Oak Ridge National Laboratory"
            ]
        },
        {
            "id": "https://openalex.org/A204505041",
            "name": "Olufemi A Omitaomu",
            "affiliations": [
                "Oak Ridge National Laboratory"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3046572078",
        "https://openalex.org/W3160856016",
        "https://openalex.org/W4392067260",
        "https://openalex.org/W4307154772",
        "https://openalex.org/W2954472560",
        "https://openalex.org/W4402372906",
        "https://openalex.org/W2948852192",
        "https://openalex.org/W3115886087",
        "https://openalex.org/W2915118509",
        "https://openalex.org/W4402243741",
        "https://openalex.org/W4393065402",
        "https://openalex.org/W4308441536",
        "https://openalex.org/W4399251643",
        "https://openalex.org/W2989886696"
    ],
    "abstract": "Bibliometric analysis is essential for understanding research trends, scope,\\nand impact in urban science, especially in high-impact journals, such Nature\\nPortfolios. However, traditional methods, relying on keyword searches and basic\\nNLP techniques, often fail to uncover valuable insights not explicitly stated\\nin article titles or keywords. These approaches are unable to perform semantic\\nsearches and contextual understanding, limiting their effectiveness in\\nclassifying topics and characterizing studies. In this paper, we address these\\nlimitations by leveraging Generative AI models, specifically transformers and\\nRetrieval-Augmented Generation (RAG), to automate and enhance bibliometric\\nanalysis. We developed a technical workflow that integrates a vector database,\\nSentence Transformers, a Gaussian Mixture Model (GMM), Retrieval Agent, and\\nLarge Language Models (LLMs) to enable contextual search, topic ranking, and\\ncharacterization of research using customized prompt templates. A pilot study\\nanalyzing 223 urban science-related articles published in Nature Communications\\nover the past decade highlights the effectiveness of our approach in generating\\ninsightful summary statistics on the quality, scope, and characteristics of\\npapers in high-impact journals. This study introduces a new paradigm for\\nenhancing bibliometric analysis and knowledge retrieval in urban research,\\npositioning an AI agent as a powerful tool for advancing research evaluation\\nand understanding.\\n",
    "full_text": "Automating Bibliometric Analysis with Sentence Transformers and\nRetrieval-Augmented Generation (RAG): A Pilot Study in Semantic and\nContextual Search for Customized Literature Characterization for High-Impact\nUrban Research\nHaowen Xua, Xueping Lib,∗, Jose Tupayachib, Jianming (Jamie) Lianc, Femi Omitaomua\naComputational Sciences and Engineering Division, Oak Ridge National Laboratory, Oak Ridge, TN 37830, USA\nbThe Department of Industrial and Systems Engineering, The University of Tennessee, Knoxville, TN 37996, USA\ncBuilding Technologies Research and Integration Center, Oak Ridge National Laboratory, Oak Ridge, TN 37830, USA\nAbstract\nBibliometric analysis is essential for understanding research trends, scope, and impact in urban science, especially\nin high-impact journals, such Nature Portfolios. However, traditional methods, relying on keyword searches and\nbasic NLP techniques, often fail to uncover valuable insights not explicitly stated in article titles or keywords. These\napproaches are unable to perform semantic searches and contextual understanding, limiting their e ffectiveness in\nclassifying topics and characterizing studies. In this paper, we address these limitations by leveraging Generative AI\nmodels, specifically transformers and Retrieval-Augmented Generation (RAG), to automate and enhance bibliometric\nanalysis. We developed a technical workflow that integrates a vector database, Sentence Transformers, a Gaussian\nMixture Model (GMM), Retrieval Agent, and Large Language Models (LLMs) to enable contextual search, topic\nranking, and characterization of research using customized prompt templates. A pilot study analyzing 223 urban\nscience-related articles published in Nature Communications over the past decade highlights the e ffectiveness of our\napproach in generating insightful summary statistics on the quality, scope, and characteristics of papers in high-impact\njournals. This study introduces a new paradigm for enhancing bibliometric analysis and knowledge retrieval in urban\nresearch, positioning an AI agent as a powerful tool for advancing research evaluation and understanding.\nKeywords: Bibliometrics Analysis, Large Language Models, Retrieval-Augmented Generation, Transformers\n⋆This manuscript has been authored by UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the US Department of Energy (DOE).\nThe US government retains and the publisher, by accepting the article for publication, acknowledges that the US government retains a nonexclusive,\npaid-up, irrevocable, worldwide license to publish or reproduce the published form of this manuscript, or allow others to do so, for US government\npurposes. DOE will provide public access to these results of federally sponsored research in accordance with the DOE Public Access Plan\n(http://energy.gov/downloads/doe-public-access-plan).\n∗Corresponding author.\nEmail addresses: xuh4@ornl.gov (Haowen Xu ), xueping.li@utk.edu (Xueping Li), jtupayac@vols.utk.edu (Jose Tupayachi),\nlianj@ornl.gov (Jianming (Jamie) Lian), omitaomuoa@ornl.gov (Femi Omitaomu)\nPreprint submitted to Elsevier October 15, 2024\narXiv:2410.09090v1  [cs.DL]  8 Oct 2024\n1. Introduction\nBibliometric analysis is a widely used method for evaluating and mapping research trends, impact, and scope\nacross various scientific domains (Donthu et al., 2021). It provides quantitative insights by analyzing publication\nrecords, citations, and other scholarly outputs, helping researchers and policymakers understand the evolution of\nspecific fields (Gan et al., 2022). Over the past few decades, bibliometric analysis has evolved from basic citation\ncounts and keyword frequency metrics to more sophisticated approaches, incorporating co-authorship networks, ci-\ntation flows, and research topic clusters (Ram ´ırez et al., 2019). These methods are particularly important in fields\nlike urban science, where emerging topics such as smart cities require continuous monitoring to shape the direction\nof future research and innovation (Guo et al., 2019). Bibliometric analysis plays a key role in identifying influential\nworks, emerging themes, and research gaps, thus guiding strategic decision-making in urban science and smart city\ndevelopment (Zhao et al., 2019).\nHowever, traditional bibliometric methods face several limitations. Most rely heavily on keyword searches and\nbasic text mining techniques, which depend on exact matches of terminologies and predefined keywords. These tech-\nniques often miss critical insights that are not explicitly captured in the titles or abstracts of research articles, thereby\nlimiting the ability to fully understand and classify research topics (Romero-Silva and De Leeuw, 2021). Furthermore,\ntraditional natural language processing (NLP) approaches, such as term frequency-inverse document frequency (TF-\nIDF) or simple word co-occurrence metrics, fail to capture the semantic meaning and contextual relationships between\nconcepts(Safder and Hassan, 2019). Although topic modeling methods like Latent Dirichlet Allocation (LDA) can\noffer significant benefits for bibliometric analysis by providing deeper insights into the relationships and structures\nwithin research literature (Chen and Xie, 2020), they are primarily used for uncovering thematic structures and clas-\nsifying article topics and are not designed for enabling semantic search or providing a contextual understanding of an\narticle that involves deeper reasoning and interpretation. As a result, traditional bibliometric analysis often falls short\nin generating deeper insights that require a thorough review and interpretation of the article’s full content. Relying\nprimarily on the analysis of titles, keywords, and standard metadata, limits the ability to provide a more customized\nand nuanced characterization of research based on the full textual data.\nRecent advancements in generative AI models, such as large language models (LLMs), have opened new oppor-\ntunities for enhancing research (Wang et al., 2024). These models, including transformers and Retrieval-Augmented\nGeneration (RAG) systems, excel at semantic understanding and contextual interpretation of complex texts, making\nthem highly suitable for extracting valuable insights from research articles and technical manuals (Xu et al., 2024b;\nTupayachi et al., 2024). In the field of urban informatics, LLMs have been increasingly applied to analyze large\nvolumes of text, uncovering patterns and trends that traditional methods would overlook (Liang et al., 2024; Xu et al.,\n2024a). In this paper, we propose a novel technical workflow for automating and enhancing bibliometric analysis\nby integrating Vector Databases, Sentence Transformers, Gaussian Mixture Models (GMM), Retrieval Agents, and\nan LLM. Our approach enables contextual search, topic ranking, and customized characterization of research arti-\n2\ncles, which we demonstrate through a pilot study analyzing 201 urban science-related articles published in Nature\nCommunications over the past decade. This work addresses the limitations of traditional bibliometric methods, in-\ntroducing a new paradigm for urban research analysis and knowledge retrieval through the development of AI agents\nwith contextual understanding and reasoning capabilities.\n2. Literature Review\nTo overcome the limitations and knowledge gaps in traditional bibliometric analysis, recent studies have em-\nployed generative AI models, particularly transformer-based language models, to automate and enhance bibliometric\nmethodologies.\nFijaˇcko et al. (2024) explores the application of generative AI in bibliometric analysis, focusing on 10 years\nof research abstracts from the European Resuscitation Congresses (ERC). Using ChatGPT-4, the study classified\n2,491 abstracts into ERC guideline topics, with Basic Life Support and Adult Advanced Life Support being the\nmost frequent. The research highlights the potential of large language models like ChatGPT-4 in categorizing and\nanalyzing scientific literature and identifying trends. However, challenges included potential misclassification, the\nlimited use of abstract titles rather than full-text, and heavy reliance on the model’s capabilities. These constraints\nhighlight the challenges of automating bibliometric analysis in the absence of comprehensive datasets. However,\nthe study effectively showcases the potential of AI to significantly improve bibliometric methodologies despite these\nlimitations.\nWeng et al. (2022) introduces a methodology for detecting and visualizing key research topics using GPT-3 em-\nbeddings and the HDBSCAN clustering algorithm on 593 abstracts related to urban studies and machine learning.\nBy clustering abstracts based on semantic similarity and extracting keywords using the Maximal Marginal Relevance\n(MMR) algorithm, the study provides an interactive tool for exploring abstract clusters and their associated topics.\nChallenges included optimizing clustering parameters and relying solely on abstracts, which may not fully repre-\nsent the research. Some clusters contained outliers or minimal data, a ffecting accuracy. Despite these limitations,\nthe study demonstrates the potential of transformer-based models in facilitating unsupervised bibliometric analysis,\nthough refinement is needed.\nBoth articles emphasize the benefits of transformer-based and large language models for bibliometric analysis,\nwhile also addressing critical limitations such as data quality, optimization challenges, and input constraints when\nworking with abstract-based datasets. To overcome these challenges, there is a need to harness recent advancements\nin sentence transformer models and RAG technologies. These innovations can enable the development of an AI agent\ncapable of advanced contextual understanding of research articles, facilitating semantic search and providing tailored\ninsights based on user-specific queries. This, in turn, can generate new bibliometric metrics, offering deeper and more\ncomprehensive analysis.\n3\n3. Methodology\nThis section starts by outlining the design requirements for our proposed methods, then presents the conceptual\nworkflow and its implementation, which combines Generative AI techniques with statistical models.\nFigure 1: The overall design of the transformer and RAG-powered workflow.\n3.1. Design Requirements\nOverall, we aim to develop an AI-agent styled tool that can interact with users, who are primarily researchers and\ncollege students, through human nature conversations, to get their inquire on the current-state of cutting edge research\nin a specific domain, such as smart city and urban science. Based on the inquiry, our workflow will automate a\nsequence of procedures that leverage the unique capabilities of sentence transformers and RAG techniques on a batch\nof selected literature filtered and downloaded from academics databases, such as Scopous, IEEE Xplore, and Web of\nScience. Aiming to shed lights on more advanced, intelligent, and automonous biblimetric analysis, our workflow\naims to enable the following features:\nConversational Interaction: A chatbot-style interface will be implemented, allowing users to ask questions through\nnatural human conversations, without the need for pre-defined keywords or technical jargon. This feature will\nenable users to define search and filter criteria for subsets of bibliographic data (e.g., research articles, confer-\nence proceedings, technical reports, and manuals) that have been pre-selected and downloaded from popular\n4\nacademic databases. The search process will be guided by broad categories, such as domains, disciplines, and\njournals, to streamline access to relevant literature.\nSemantic and Contextual Search:Based on the user-defined inquiry, this process matches and retrieves relevant\nresearch documents or specific sections by analyzing the underlying meaning and contextual relationships be-\ntween words, rather than relying solely on keyword matching. The use of sentence transformers and text\nembeddings, enables users to access information and knowledge based on conceptual relevance, rather than\nsimple term frequency. This enhances the precision of literature filtering and facilitates deeper, more insightful\nknowledge discovery, which will plays important role as the retrieval agent within the RAG paradigm to benefit\nfurther analytics using Generative AI models.\nCustomized Literature Characterization:Using the output literature from the semantic and contextual search as\ninput, Generative Pre-trained Transformer (GPT) models will be employed for contextual understanding, rea-\nsoning, and interpretation. These GPT models will process user inquiries to generate customized characteri-\nzations and interpretations of the selected literature, providing deeper insights and creating more sophisticated\nmetrics for advanced bibliometric analyses. This approach aims to enhance the overall understanding of re-\nsearch trends and offer tailored, in-depth evaluations of the literature.\nAs an example of our end-user capability, a user could ask the chatbot, powered by our method, a question like,\n“What percentage of research published in Computers, Environment and Urban Systems over the past 5 years in the\nurban mobility sector uses traffic simulation-based methods versus crowd-sourced data-driven methods, and what are\ntheir spatial scales?” The semantic and contextual search then filters and retrieves relevant articles based on the query,\nranks them by relevance, and feeds them to the generative AI model. This enables advanced contextual understanding\nand reasoning to provide customized characterizations on individual research’s simulation types and spatial scales,\nwhich involve information often not found in keywords or titles. These characterizations can be later used to generate\nsummary statics and insights to facilitate more detailed trend analysis and thematic mapping.\n3.2. Workflow Design\nOur workflow consists of four key procedures, as depicted in Figure 1. The workflow is later implemented in a\nJupyter lab environment using Python-based libraries. Each procedure is detailed through the following following list.\n3.2.1. Bibliography Selection and Data Preparation\nIn the first step, users select literature based on generic search criteria such as discipline, publication year, and\njournal name. Data is extracted from academic databases like Scopus, IEEE Xplore, and SerpApi using their respective\nweb services and platforms, or through custom-built web scrapers, such as those powered by SerpAPI. The retrieved\ndata includes bibliographic summaries in CSV format and individual articles in formats like PDF and HTML, which\nare then stored in a file-based system for further processing.\n5\n3.2.2. Text Embedding and Data Warehousing\nAfter retrieving the essential documents, a Python script powered by PyPDF2 is used to parse the bibliographic\nsummaries, which include the list of downloaded articles along with supportive metadata (e.g., authors, year, source,\ncitations, and h-index), as well as the PDF and HTML versions of the individual articles. This parsing process\nis designed to upload key textual information into a datastore, building the knowledge base for the proposed AI\nagent. Unlike traditional information and content management systems, our workflow utilizes a sentence transformer,\nspecifically the all-MiniLM-L6-v2 model from Hugging Face, to generate text embeddings—vector representations\nthat encode the semantic and contextual meaning of the text. Compared with traditional NLP methods, sentence\ntransformers, with its unique self-attention mechanism, have superior advantages in capturing semantic meaning,\nenabling contextual understanding, handling synonyms, and long-range dependencies between words in a sentence.\nThese embeddings facilitate more efficient semantic and contextual searches in later stages of the workflow. The text\nembeddings, along with essential metadata and article content, are uploaded into the datastore. We selected Neo4j, a\ngraph database, as the datastore for this workflow due to its graph data model, which better represents the relationships\nbetween data entities stored as nodes in the database. In our project, individual articles are represented as nodes within\nNeo4j, with associated metadata, content, and text embeddings stored as properties of each node.\n3.2.3. Semantic and Contextual Search\nIn the third step, the workflow enables semantic and contextual searches within the literature stored in the knowl-\nedge base, leveraging the Neo4j database and sentence transformers. User queries, collected through a chatbot in-\nterface, serve as input for this advanced search. The core functionality compares the text embeddings of the user\nqueries with those of the article contents. We employ an enhanced cosine similarity analysis, as described in Eq. 1,\nto calculate a similarity score ranging from 0 to 1, where 0 represents complete irrelevance and 1 represents high\nrelevance. Our implementation extends the standard cosine similarity formula by using Python to chunk the original\narticle content into sections and paragraphs, enabling more granular comparisons between the query and specific parts\nof the article. This process is applied to each article in the database, generating a similarity score based on semantic\nsimilarity with the user’s query. At the contextual level, the framework evaluates the query’s context and intelligently\nselects embeddings from different sections of the articles to perform a targeted and accurate search.\nSimilarity Score = a ·b\n∥a∥∥b∥ (1)\nTo draw the decision boundary based on a list of individual article’s similarity score, we employed GMM to rank\nand cluster articles by their similarity score, which reflects their relevance. A GMM is a probabilistic model that\nrepresents a distribution of data as a mixture of multiple Gaussian (normal) distributions, each characterized by its\nown mean and variance, making it e ffective for modeling complex, multimodal datasets. We employed the Akaike\nInformation Criterion (AIC) and Bayesian Information Criterion (BIC), alongside the elbow method, to determine the\noptimal number of clusters for the Gaussian Mixture Model (GMM) analysis. After the clustering analysis, the cluster\n6\nwith the highest average similarity scores implies it contains the most relevant articles, which are also further ranked\nbased on its similarity score.\nArticles in the top-ranked clusters are subsequently fed into generative AI models, specifically GPT, to enable\nmore in-depth analysis and interpretation. The semantic and contextual search within this workflow is a critical\ncomponent of the RAG paradigm, allowing for further subsetting and refining of input information to ensure more\naccurate and relevant results. This process also helps prevent exceeding the token limits of the GPT model context\nwindow by optimizing the selection of input texts.\n3.2.4. Customized Article Characterization\nThe top-ranked clusters, containing the most relevant articles, are then imported into a GPT model as an external\nknowledge source to generate customized characterizations for each article. These tailored bibliographic characteris-\ntics serve as metrics, providing more detailed descriptions and classifications of the articles. This approach uncovers\nvaluable insights into research trends, focusing on individual articles’ topics, technologies, methods, and contribu-\ntions.\nWe leverage the contextual reasoning capabilities of large language models to classify and justify findings based on\nthe semantic meaning of sections and paragraphs within the articles, extracting useful information without relying on\nprecise names or keywords. This process is guided by instructional prompting strategies, where we design engineered\nprompt templates and feed them into the GPT model along with the relevant article text segments (specific sections).\nThese segments are further refined and filtered based on their content relevance to ensure accurate classification and\nextraction.\nAt the technical level, we explored and tested the capabilities of two GPT models, including a local instance of\nEleutherAI/gpt-neo-1.3B models and the ChatGPT-3.5 Turbo API. Our experiments reveals that small models with\non 1.3B parameters suffer from severe hallucination, and are unable to analyze large size of tokens. The ChatGPT-3.5\nAPI demonstrates stable performance, particularly in its ability to process large text segments efficiently and produce\nreasoned characterizations.\n4. Pilot Study\nThis pilot study aims to demonstrate the feasibility and performance of our proposed methods. For this study, we\ncompiled a dataset of 223 high-impact urban research articles published in Nature Communications, obtained through\nthe following Scopus query: TITLE-ABS-KEY ( “smart city” OR “urban” OR “urban management” OR “urban\nplanning” ) AND SRCTITLE ( “Nature Communications” ) AND PUBYEAR ¿ 2013. We preprocessed the dataset\nby removing all intermediate versions labeled as “Author Correction” or “Publisher Correction.” The final dataset\nconsists of a CSV file containing bibliometric summaries with all Scopus fields selected, along with 223 individual\nPDF documents of the actual articles.\n7\n(a) Semantic and contextual search based on user’s query.\n(b) Customized Literature Characterization using GPT’s reasoning capability.\nFigure 2: Demonstration of the workflow through two use case.\n4.1. Use Case Demonstration\nOur first use case on semantic and contextual search is demonstrated in Figure 2a, where the user submits an\ninquiry to identify articles related to urban green space. The chatbot responds by visualizing a histogram of similarity\nscores for all articles and displaying the GMM clusters of the articles. Additionally, a link is provided to download a\n8\nCSV file that ranks and clusters the articles based on their relevance to the user’s query.\nOur second use case builds on the output articles from the first use case and demonstrates the capability to generate\ncustomized literature characterizations, creating new metrics for bibliometric analysis. Through the chatbot interface,\nusers specify requests and instructions via prompts to guide the GPT model in generating tailored metrics. Examples\nof these prompts are illustrated in Figure 2b. Based on the prompts, the GPT processes the 67 retrieved articles and\ntheir critical content, leveraging its contextual reasoning ability to derive new literature characteristics, which can then\nbe developed into metrics and summary statistics. Figure 2b also visualizes the responses to the user’s queries using\npie charts and box plots. Users can submit additional questions and custom requests through the chatbot to extract\ninformation and develop unique metrics tailored to the needs of bibliometric analysis.\nOur major contribution lies in the development of an autonomous AI agent designed to assist researchers in au-\ntomating the characterization and information extraction of large volumes of literature, including datasets exceeding\n1,000 articles. This system enables the generation of in-depth insights for bibliometric analysis, significantly enhanc-\ning the scalability and depth of literature review and research trend identification processes. By automating these\ntasks, the AI agent o ffers a powerful tool for e fficiently managing and analyzing extensive collections of scholarly\narticles, ultimately facilitating more comprehensive and insightful bibliometric analyses.\n4.2. Limitation and Future Work\nDeveloped as a prototype for a more advanced knowledge base and management system, our workflow still faces\na few limitations, as the following:\nToken Size Limitation:The current implementation using the ChatGPT API has a maximum token size limitation\nand incurs service fees based on the number of tokens processed. This makes it less suitable for analyzing large\nvolumes of literature.\nDatabase Query Performance:The current datastore implementation using the Neo4j database may encounter chal-\nlenges in querying and managing large volumes of embedding data, as Neo4j is not optimized as a dedicated\nvector database.\nLack of Evaluation and Validation:The GPT-generated literature characteristics are not currently evaluated by hu-\nman experts, which introduces uncertainty regarding their accuracy and reliability.\nAs future work to address these limitations, we propose several experimental solutions. These include (a) deploy-\ning a local version of large language models, such as GPT-Neo, to minimize service fees for large-scale data analysis,\n(b) fine-tuning the GPT model to reduce unnecessary content and instructions sent to the model, thereby mitigating\ntoken size limitations, (c) transitioning our datastore implementation to dedicated vector databases, such as FAISS\nor Pinecone, to enhance latency and accuracy, and (d) developing a comprehensive strategy to evaluate the GPT’s\nperformance in analyzing and characterizing literature. Additionally, more advanced bibliometric analysis methods\ncould be integrated into the current workflow to extend its analytical capabilities.\n9\n5. Conclusion\nIn this paper, we have presented a novel workflow that integrates generative AI models and advanced analytical\ntechniques through the RAG paradigm to address the limitations of traditional bibliometric analysis methods. By\nleveraging the contextual reasoning capabilities of large language models and enhanced semantic search techniques,\nour system offers a more nuanced and insightful analysis of research literature. This approach, demonstrated through\nthe analysis of urban science-related articles, enables customized characterizations and generates new metrics for\nbibliometric analysis, providing deeper insights into research trends, methodologies, and contributions.\nOur pilot study demonstrates the feasibility of this workflow, showcasing its ability to facilitate advanced semantic\nand contextual searches, cluster relevant articles, and produce tailored bibliographic insights through generative AI.\nHowever, the current implementation faces challenges, including token size limitations, database query performance\nissues, and the lack of expert evaluation for the AI-generated results.\nTo address these limitations, future work will explore the deployment of local language models, fine-tuning of\nGPT models to optimize token usage, and transitioning to vector databases like FAISS or Pinecone to improve perfor-\nmance. Additionally, we aim to establish a comprehensive validation framework involving human experts to ensure\nthe accuracy and reliability of the generated bibliometric insights. As advancements in AI and bibliometric method-\nologies continue, our workflow has the potential to serve as a powerful and autonomous tool for researchers and\npolicymakers seeking to analyze and interpret vast bodies of scientific literature more effectively.\n6. Acknowledgments\nThis work was supported by the U.S. Department of Energy (U.S DOE), Advanced Research Projects Agency–Energy\n(ARPA-E) under the project #DE-AR0001780. We thank our collaborators from the University of Tennessee Knoxville.\n10\nReferences\nChen, X., Xie, H., 2020. A structural topic modeling-based bibliometric study of sentiment analysis literature. Cognitive Computation 12, 1097–\n1129.\nDonthu, N., Kumar, S., Mukherjee, D., Pandey, N., Lim, W.M., 2021. How to conduct a bibliometric analysis: An overview and guidelines. Journal\nof business research 133, 285–296.\nFijaˇcko, N., Creber, R.M., Abella, B.S., Kocbek, P., Metliˇcar, ˇS., Greif, R., ˇStiglic, G., 2024. Using generative artificial intelligence in bibliometric\nanalysis: 10 years of research trends from the european resuscitation congresses. Resuscitation Plus 18, 100584.\nGan, Y .n., Li, D.d., Robinson, N., Liu, J.p., 2022. Practical guidance on bibliometric analysis and mapping knowledge domains methodology–a\nsummary. European Journal of Integrative Medicine 56, 102203.\nGuo, Y .M., Huang, Z.L., Guo, J., Li, H., Guo, X.R., Nkeli, M.J., 2019. Bibliometric analysis on smart cities research. Sustainability 11, 3606.\nLiang, J., Zhao, A., Hou, S., Jin, F., Wu, H., 2024. A gpt-enhanced framework on knowledge extraction and reuse for geographic analysis models\nin google earth engine. International Journal of Digital Earth 17, 2398063.\nRam´ırez, L.J.C., S ´anchez-Ca˜nizares, S.M., Fuentes-Garc´ıa, F.J., 2019. Past themes and tracking research trends in entrepreneurship: A co-word,\ncites and usage count analysis. Sustainability 11, 3121.\nRomero-Silva, R., De Leeuw, S., 2021. Learning from the past to shape the future: A comprehensive text mining analysis of or/ms reviews. Omega\n100, 102388.\nSafder, I., Hassan, S.U., 2019. Bibliometric-enhanced information retrieval: a novel deep feature engineering approach for algorithm searching\nfrom full-text publications. Scientometrics 119, 257–277.\nTupayachi, J., Xu, H., Omitaomu, O.A., Camur, M.C., Sharmin, A., Li, X., 2024. Towards next-generation urban decision support systems through\nai-powered construction of scientific ontology using large language models—a case in optimizing intermodal freight transportation. Smart Cities\n7, 2392–2421.\nWang, L., Ma, C., Feng, X., Zhang, Z., Yang, H., Zhang, J., Chen, Z., Tang, J., Chen, X., Lin, Y ., et al., 2024. A survey on large language model\nbased autonomous agents. Frontiers of Computer Science 18, 186345.\nWeng, M.H., Wu, S., Dyer, M., 2022. Identification and visualization of key topics in scientific publications with transformer-based language\nmodels and document clustering methods. Applied Sciences 12, 11220.\nXu, H., Omitaomu, F., Sabri, S., Zlatanova, S., Li, X., Song, Y ., 2024a. Leveraging generative ai for urban digital twins: A scoping review on\nthe autonomous generation of urban data, scenarios, designs, and 3d city models for smart city advancement. arXiv preprint arXiv:2405.19464\nURL: https://arxiv.org/abs/2405.19464, doi:10.48550/arXiv.2405.19464, arXiv:2405.19464. computer Science ¿ Artificial In-\ntelligence.\nXu, H., Yuan, J., Zhou, A., Xu, G., Li, W., Ye, X., et al., 2024b. Genai-powered multi-agent paradigm for smart urban mobility: Opportunities and\nchallenges for integrating large language models (llms) and retrieval-augmented generation (rag) with intelligent transportation systems. arXiv\npreprint arXiv:2409.00494 .\nZhao, L., Tang, Z.y., Zou, X., 2019. Mapping the knowledge domain of smart-city research: A bibliometric and scientometric analysis. Sustain-\nability 11, 6648.\n11"
}