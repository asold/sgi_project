{
  "title": "A Comprehensive Analysis of the Effectiveness of Large Language Models as Automatic Dialogue Evaluators",
  "url": "https://openalex.org/W4393147013",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2096971521",
      "name": "Chen Zhang",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2628761525",
      "name": "Luis Fernando D Haro",
      "affiliations": [
        "Universidad Politécnica de Madrid"
      ]
    },
    {
      "id": "https://openalex.org/A2099591825",
      "name": "Yi-Ming Chen",
      "affiliations": [
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2641055566",
      "name": "Malu Zhang",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2096728398",
      "name": "Haizhou Li",
      "affiliations": [
        "Chinese University of Hong Kong, Shenzhen",
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2628761525",
      "name": "Luis Fernando D Haro",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096728398",
      "name": "Haizhou Li",
      "affiliations": [
        "National University of Singapore",
        "Chinese University of Hong Kong, Shenzhen"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4221167741",
    "https://openalex.org/W4307479606",
    "https://openalex.org/W4318719686",
    "https://openalex.org/W4221138568",
    "https://openalex.org/W3199246459",
    "https://openalex.org/W4287891007",
    "https://openalex.org/W4378464977",
    "https://openalex.org/W2328886022",
    "https://openalex.org/W4221160645",
    "https://openalex.org/W3034424015",
    "https://openalex.org/W3023366413",
    "https://openalex.org/W6840678358",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W3201300710",
    "https://openalex.org/W2916772188",
    "https://openalex.org/W4221151028",
    "https://openalex.org/W4389009519",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W6800875267",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3172728711",
    "https://openalex.org/W4200634294",
    "https://openalex.org/W3015487035",
    "https://openalex.org/W4285273040",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4366735744",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W3034808773",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4205686084",
    "https://openalex.org/W4389009545",
    "https://openalex.org/W4385573116",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W3036394672",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W4380353763",
    "https://openalex.org/W4285292592",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3034950505",
    "https://openalex.org/W4380136143",
    "https://openalex.org/W4288567412",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W4385734161",
    "https://openalex.org/W4319793767",
    "https://openalex.org/W4362598574",
    "https://openalex.org/W4362655426",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4289447057",
    "https://openalex.org/W3211384372",
    "https://openalex.org/W4362655261",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W4285188834",
    "https://openalex.org/W4226120574",
    "https://openalex.org/W4226278401"
  ],
  "abstract": "Automatic evaluation is an integral aspect of dialogue system research. The traditional reference-based NLG metrics are generally found to be unsuitable for dialogue assessment. Consequently, recent studies have suggested various unique, reference-free neural metrics that better align with human evaluations. Notably among them, large language models (LLMs), particularly the instruction-tuned variants like ChatGPT, are shown to be promising substitutes for human judges. Yet, existing works on utilizing LLMs for automatic dialogue evaluation are limited in their scope in terms of the number of meta-evaluation datasets, mode of evaluation, coverage of LLMs, etc. Hence, it remains inconclusive how effective these LLMs are. To this end, we conduct a comprehensive study on the application of LLMs for automatic dialogue evaluation. Specifically, we analyze the multi-dimensional evaluation capability of 30 recently emerged LLMs at both turn and dialogue levels, using a comprehensive set of 12 meta-evaluation datasets. Additionally, we probe the robustness of the LLMs in handling various adversarial perturbations at both turn and dialogue levels. Finally, we explore how model-level and dimension-level ensembles impact the evaluation performance. All resources are available at https://github.com/e0397123/comp-analysis.",
  "full_text": "A Comprehensive Analysis of the Effectiveness of Large Language Models as\nAutomatic Dialogue Evaluators\nChen Zhang1, Luis Fernando D’Haro2, Yiming Chen1, Malu Zhang3*, Haizhou Li1,4\n1 National University of Singapore\n2 Speech Technology Group - Universidad Polit´ecnica de Madrid, Spain\n3 University of Electronic Science and Technology of China\n4 The Chinese University of Hong Kong (Shenzhen), China\nchen zhang@u.nus.edu, maluzhang@uestc.edu.cn\nAbstract\nAutomatic evaluation is an integral aspect of dialogue sys-\ntem research. The traditional reference-based NLG metrics\nare generally found to be unsuitable for dialogue assessment.\nConsequently, recent studies have suggested various unique,\nreference-free neural metrics that better align with human\nevaluations. Notably among them, large language models\n(LLMs), particularly the instruction-tuned variants like Chat-\nGPT, are shown to be promising substitutes for human judges.\nYet, existing works on utilizing LLMs for automatic dialogue\nevaluation are limited in their scope in terms of the num-\nber of meta-evaluation datasets, mode of evaluation, cover-\nage of LLMs, etc. Hence, it remains inconclusive how effec-\ntive these LLMs are. To this end, we conduct a comprehen-\nsive study on the application of LLMs for automatic dialogue\nevaluation. Specifically, we analyze the multi-dimensional\nevaluation capability of 30 recently emerged LLMs at both\nturn and dialogue levels, using a comprehensive set of 12\nmeta-evaluation datasets. Additionally, we probe the robust-\nness of the LLMs in handling various adversarial perturba-\ntions at both turn and dialogue levels. Finally, we explore\nhow model-level and dimension-level ensembles impact the\nevaluation performance. All resources are available at https:\n//github.com/e0397123/comp-analysis.\nIntroduction\nEvaluation remains a persistent challenge in dialogue sys-\ntem research (Mehri et al. 2022a). At present, human eval-\nuation is regarded as the most reliable method for com-\nprehensively assessing the quality of dialogue. However,\nbecause of the considerable expense and lack of repro-\nducibility associated with human evaluations, automatic\nmeasures have been proposed to complement human eval-\nuation. The automatic measures can be categorized into\ntwo main groups: reference-based and reference-free. Due\nto the poor alignment of reference-based metrics, such as\nBLEU (Papineni et al. 2002), with human evaluation (Liu\net al. 2016), existing studies mainly focus on developing\nneural-based reference-free evaluators (Yeh, Eskenazi, and\nMehri 2021). Although such reference-free evaluators have\n*corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\ndemonstrated improved correlations with human evaluation\nover reference-based metrics, they are still far from being\nthe perfect proxy of human judges. Additionally, they ex-\nhibit poor generalization to dialogue data that are different\nfrom what they are trained on (Zhang et al. 2022a).\nThe recent advancement in large language models (Brown\net al. 2020; Chowdhery et al. 2022; Touvron et al. 2023a)\ncoupled with refined alignment techniques (Wei et al. 2022a;\nOuyang and et al. 2022) lead to an extensive array of\ngeneral-purpose AI assistants that are capable of tackling a\nbroad spectrum of NLP tasks. Harnessing their strong lan-\nguage understanding capability, LLMs, especially the fam-\nily of instruction-tuned models, offer great promise as ef-\nfective and generalized automatic dialogue evaluators. Sev-\neral recent works (Huynh et al. 2023; Chen et al. 2023a;\nLiu et al. 2023; Lin and Chen 2023; Fu et al. 2023) report\nstrong correlations of LLMs with human evaluation. Yet, the\nscope of their assessment is limited: (1) The coverage of\nLLMs is restricted, with a primary emphasis on proprietary\nmodels, such as OpenAI ChatGPT/GPT-4. Lately, there has\nbeen an exponential growth of open-source foundation mod-\nels (Almazrouei et al. 2023; Touvron et al. 2023a; Scao et al.\n2023) and ChatGPT-like LLMs (Taori et al. 2023; Chiang\net al. 2023; Chen et al. 2023b), a timely survey is necces-\nsary to examine their effectiveness as automatic dialogue\nevaluators. (2) The mode of evaluation primarily concen-\ntrates on correlation analysis with a limited number of meta-\nevaluation datasets. We not only conduct a comprehensive\ncorrelation analysis on 12 meta-evaluation datasets 1 along\ndifferent evaluation dimensions2, but also probe the robust-\nness of LLMs against adversarial perturbations at both turn\nand dialogue levels.\nOur work serves as a useful guide for future research on\napplying LLMs to automatic dialogue evaluation and the\ncontributions are listed as follows:\n• We conduct a comprehensive analysis of the multi-\ndimensional evaluation ability of 30 recent LLMs at both\n16 turn-level and 6 dialogue-level respectively.\n2Dimension refers to the quality aspect that is usually assessed\nin the human evaluation process, such as relevance at the turn level\nand dialogue coherence at the dialogue level.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19515\nturn and dialogue levels. Specifically, we evaluate coher-\nence, engagingness, diversity, informativeness, and over-\nall quality at the dialogue level. For turn-level evaluation,\nwe assess context relevance, understandability, interest-\ningness, specificity, and overall quality.\n• Such a comprehensive assessment is impossible without\nthe availability of large-scale meta-evaluation datasets\nwith annotations. Hence, we complement 12 existing\nmeta-evaluation datasets by providing annotations that\nwere previously unavailable. The datasets together with\nthe full annotations will be made publicly available for\nresearchers and practitioners to benchmark their new\nevaluation metrics.\n• Besides correlation analysis, we introduce a series of ad-\nversarial perturbation strategies to reduce the response or\ndialogue quality along various dimensions. In this way,\nwe can probe the robustness of the LLMs, which has not\nbeen explored in existing works.\n• Lastly, we study the impact of different ensemble strate-\ngies on the dialogue evaluation performance, including\ndimension-level and model-level ensembles.\nPreliminaries\nMeta-Evaluation\nDatasets We adopt 12 meta-evaluation datasets in our\nanalysis comprising 6 at the turn level and another 6 at the\ndialogue level. Table 1 summarizes the dataset details. For\nboth turn-level and dialogue-level analysis, we evaluate five\ndifferent quality aspects/dimensions respectively. At the turn\nlevel, we assess context relevance (rel), understandability\n(und), specificity (spe), interestingness (int), and overall re-\nsponse quality (ovr) while at the dialogue level, we evaluate\ncoherence (coh), engagingness (eng), informativeness (inf),\ndiversity (div), and overall dialogue quality (ovr).\nFill Up Missing Annotations With GPT-4To save costs\nand speed up the annotation process, we perform the nec-\nessary annotations with GPT-4 instead of using crowd-\nworkers. The motivation is that existing works show that\nGPT-4 can achieve human-level judgment while being more\nscalable and less expensive (Zheng et al. 2023; Gilardi, Al-\nizadeh, and Kubli 2023; Liu et al. 2023). It serves as a sup-\nplementary tool to human evaluation, especially when con-\nsidering the high costs and the reproducibility issues of hu-\nman annotations in open-ended generation tasks (Karpinska,\nAkoury, and Iyyer 2021). Note that GPT-4 annotations may\ncontain biases and future works can explore automatic and\nmanual ways to mitigate the biases.\nFigure 1 illustrates how we prompt GPT-4 to perform the\nannotation task. When calling the GPT-4 API, we set the\ntemperature and top\np to 0.7 and 0.95 respectively. The an-\nnotation process is repeated five times to mimic the sce-\nnario of having multiple crowd workers annotate each data\ninstance. The inter-annotator or inter-round agreement for\nGPT-4 is derived by averaging the pairwise Pearson corre-\nlations between the annotation scores from any two annota-\ntion rounds. In general, a good inter-annotator or inter-round\n### Dialogues:\n[Here is the input dialogue for annotation]\n## Instruction:\nRate the coherence, engagingness, diversity, infor-\nmativeness, and overall quality of the input dialogue\non a scale of 1 to 5 and just output the corresponding\nratings.\n### Output Format:\ncoherence - x\nengagingness - x\ndiversity - x\ninformativeness - x\noverall - x\n### Your Response:\n[Here is GPT-4’s output]\n### Context:\n[Here is the dialogue context]\n### Response:\n[Here is the input response for annotation]\n### Instruction:\nRate the context relevance, specificity, interesting-\nness, understandability, and overall quality of the re-\nsponse on a scale of 1 to 5 and just output the corre-\nsponding ratings.\n### Output Format:\nrelevance - x\nspecificity - x\ninterestingness - x\nunderstandability - x\noverall - x\n### Your Response:\n[Here is GPT-4’s output]\nFigure 1: The instruction template for prompting GPT-4 to\nannotate both dialogue-level (top) and turn-level (bottom)\ndata. For our meta-evaluation of the proprietary models in-\ncluding ChatGPT and Palm-2 Bison, we also use this in-\nstruction template.\nagreement exceeding 0.65 is observed when annotating the\nmissing dimensions of different datasets.\nMeta-Evaluation Metrics The reliability of LLMs as au-\ntomatic evaluators is assessed by computing how well their\nevaluation scores (s dim\nllm ) correlate with the corresponding\nhuman (sdim\nhuman) or GPT-4 judgment (s dim\ngpt4) with a corre-\nlation function g for a specific dimension. sdim\nhuman is derived\nby averaging the annotations of multiple annotators while\nsdim\ngpt4 is obtained by averaging the annotations from the mul-\ntiple annotation rounds. We adopt the commonly-used Pear-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19516\nTur\nn-Level Datasets #Data #Utt Doc Len IAA Range Reused Annotations Missing Annotations\nPersona-USR (2020b) 300 9.3 98.4 /\n12.0 0.3 ∼ 0.7 rel, int,\nund, ovr spe\nPersona-Zhao (2020) 900 5.1 48.8 /\n11.5 > 0.7 ovr rel, int,\nund, spe\nDailyDialog-Zhao (2020) 900 4.7 47.5 /\n11.0 > 0.7 rel, o\nvr int, und,\nspe\nTopical-USR (2020b) 360 11.2 236.3 /\n22.4 0.5 ∼ 0.7 rel, int,\nund, ovr spe\nFED-Turn\n(2020a) 375 10.4 87.3 /\n13.3 0.5 ∼ 0.8 rel, int,\nspe, und, ovr -\nConTurE-T\nurn (2022) 1066 3.8 21.7 /\n11.0 ∼ 0.3 ovr rel, int,\nund, spe\nDialogue-Lev\nel Datasets #Data #Utt Doc Len IAA Range Reused Annotations Missing Annotations\nIEval-Dial\n(2022) 500 6.0 74.4 - - coh, eng,\ninf, div, ovr\nPersona-See (2019) 500 12.0 91.2 - - coh, eng,\ninf, div, ovr\nReliable-Dial (2022) 500 21.2 178.1 - - coh, eng,\ninf, div, ovr\nConTurE-Dial (2022b) 119 17.9 153.9 - - coh, eng,\ninf, div, ovr\nFED-Dial (2020a) 125 12.7 116.8 0.7 ∼ 0.8 coh, eng,\ninf, div, ovr -\nHuman-Eval\n(2022) 286 12.0 139.2 - - coh, eng,\ninf, div, ovr\nTable 1: Details of the meta-evaluation datasets. The “Reused” columns indicate the dimensions with available human-annotated\nscores. The “Missing” column denotes the dimensions that need to be annotated by us. The doc length is the average #words\nper context/response for turn-level datasets and dialogue for dialogue-level datasets. The IAA range shows the range of inter-\nannotator agreements of available human annotations. #Utt refers to the number of context utterances and dialogue utterances\nfor the turn-level and dialogue-level datasets respectively.\nson (ρ) measure as g.\nLarge Language Models\n30 LLMs comprising 28 open-source and 2 proprietary\nLLMs are examined. The proprietary LLMs are OpenAI\nChatGPT3 (gpt-3.5-turbo) and Google Palm-2 Bison (text-\nbison-001). The performance of OpenAI GPT-4 on human-\nannotated data is also reported. The 28 open-source LLMs\ncan be grouped into two categories, the vanilla foundation\nmodels and the instruction-tuned models. The foundation\nmodels include different variants of Meta LLaMA (Touvron\net al. 2023a,b), SalesForce XGen 4, TII-UAE Falcon (Al-\nmazrouei et al. 2023), MosaicML MPT 5, OpenLLaMA 6,\nPythia (Biderman et al. 2023), and BLOOM (Scao et al.\n2023). The instruction-tuned models are mainly derivatives\nof the aforementioned vanilla foundation models, such as\nAlpaca (Taori et al. 2023), Vicuna (Chiang et al. 2023),\nTulu (Wang et al. 2023), and Chimera (Chen et al. 2023b).\nThey are finetuned to mimic the abilities of proprietary\nLLMs, such as ChatGPT and GPT-4. Alignment techniques,\nsuch as instruction-based supervised finetuning (SFT) and\nreinforcement learning from human feedback (RLHF), are\napplied to align these models with humans’ general task-\nsolving abilities. We refer interested readers to their respec-\ntive technical reports for the details of these LLMs.\nDialogue Evaluation with LLMs\nAs we cannot obtain the output probabilities of the propri-\netary models at the time of paper preparation, we adopt the\n3We use the 2023-03-15-preview version on Azure.\n4https://blog.salesforceairesearch.com/xgen/\n5https://huggingface.co/mosaicml/mpt-7b\n6https://github.com/openlm-research/open llama\nexplicit scoring procedure to directly prompt them to pro-\nduce multi-dimensional ratings of dialogues or responses.\nChatGPT, Palm-2 Bison, and GPT-4 share the same in-\nstruction template outlined in Figure 1. Due to their strong\ninstruction-following abilities, we can easily extract the\nscores with matching heuristics. The rare erroneous cases\nare manually fixed. We repeat the scoring process of each\nproprietary model five times and apply the average score of\nthe five runs as the corresponding sdim\nllm .\nWe do not apply the same approach for obtaining ratings\nfrom other open-source LLMs because their instruction-\nfollowing abilities are weaker than the proprietary mod-\nels. Sometimes, their generation becomes intractable. In-\nstead, we follow Gupta et al. (2022) by applying an im-\nplicit scoring procedure. More specifically, given an instruc-\ntion prompt input, we concentrate on the output probabili-\n### Context:\n[Here is the dialogue context]\n### Response:\n[Here is the input response for evaluation]\n### Instruction:\nAbove is a dialogue context and the \ncorresponding response.\nQuestion: Is the response relevant to the context?\n### Your Answer:\n[Here is LLM’s output in terms of “Yes” or “No”]\nFigure 2: An example for prompting open-source LLMs to\nevaluate the contextual relevance of the input response.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19517\nties related to the label words ”Yes” and ”No” as generated\nby the LLM. Then, we normalize the probability of “Yes”\nas P(Y es) = P(Y es)/(P(Y es) + P(No)) and P(Y es)\nserves as the corresponding sdim\nllm . Greedy decoding is ap-\nplied such that the generation process of the LLMs is deter-\nministic. Figure 2 showcases an example to prompt open-\nsource LLMs to evaluate response relevance7. For different\nLLMs, we adapt the instruction template to match that used\nin their instruction-tuning process.\nMulti-Dimensional Correlation Analysis\nWe report dimension-wise Pearson correlation scores av-\neraged across either the turn-level or the dialogue-level\ndatasets8 in Table 2.\nProprietary vs Open-Source Models It can be observed\nthat ChatGPT and Palm-2 Bison are among the top 5 mod-\nels across all the dimensions at both turn and dialogue\nlevels. On average, they ranked first and second respec-\ntively. Out of the 28 open-source models available, Tulu-\n13B (Wang et al. 2023), Chimera-inst-chat-13B (Chen et al.\n2023b), and Baize-v2-13B (Xu et al. 2023a) are distin-\nguished as the top three performers at the turn-level. When\nit comes to dialogue-level performance, the top three mod-\nels are Chimera-inst-chat-13B, Tulu-13B, and WizardLM-\n13B-V1.2 (Xu et al. 2023b). A significant gap can be ob-\nserved between the proprietary models and the best open-\nsource model. For example, at the turn level, ChatGPT out-\nperforms Tulu-13B by 11% on average while at the dia-\nlogue level, ChatGPT outperforms Chimera-inst-chat-13B\nby 7.7%. The observations suggest the importance of the\nmodel scale and the quality of instruction data. The propri-\netary models are much larger than other open-source LLMs.\nThey are trained on more sophisticated human-annotated\ninstruction data while the open-source models are mainly\ninstruction-tuned on data distilled from proprietary models.\nInstruction-Tuned vs Vanilla Models We can also ob-\nserve that instruction-tuned variants generally outperform\ntheir corresponding vanilla backbone models. For instance,\nAlpaca-7B surpasses LLaMA-7B by a significant 25.1% on\naverage at the turn level and 36.4% on average at the di-\nalogue level. Similarly, Tulu-13B beats LLaMA-13B by a\nnotable 9.1% and 18.2% on average. The observations show-\ncase that alignment techniques, such as instruction-based su-\npervised finetuning, can greatly enhance the dialogue under-\nstanding capabilities of LLMs, thereby making them more\nuseful for automatic dialogue evaluation.\nLLaMA vs Other Open-Source Families Among 7B\nvanilla models, LLaMA-2-7B tops the list at the turn level\nwith an average of 0.200, while XGen-8K-7B leads at the\ndialogue level with an average of 0.404. For the 13B vanilla\nLLMs, LLaMA-13B stands out at the turn level with a 0.353\n7All dimension-specific questions for prompting the open-\nsource LLMs can be found in the Appendix.\n8For example, a model’s performance on a dimension at the\nturn level is derived by averaging the correlations obtained across\nthe six turn-level meta-evaluation datasets w.r.t. that dimension.\nscore on average, and OpenLLaMA-13 takes the lead at the\ndialogue level with an average score of 0.397. We can also\nobserve a large performance variation among the vanilla\nmodels. For example, the gap between LLaMA-2-13B and\nBLOOM-7B is more than 25% at the turn level. This can be\nattributed to the differences in their model sizes, pretraining\ncorpora, and training strategies.\nIn the 7B instruction-tuned category, Alpaca-7B and\nChimera-inst-chat-7B stand out as the top two performers\nacross both turn and dialogue levels, both of which are\nderivatives of LLaMA-7B. Given that LLaMA-13B serves\nas a strong foundational model for automated dialogue eval-\nuation, it’s understandable why its instruction-tuned vari-\nants, including Tulu-13B, Chimera-inst-chat-13B, Baize-v2-\n13B, and WizardLM-13B-V1.2, are among the best open-\nsource models. With the observations, we can conclude that\nby far, models in the LLaMA family are stronger automatic\ndialogue evaluators than other open-source LLMs.\nImpact of Instruction DataEven among the models with\nthe same number of parameters and the same backbone,\nthe performance varies greatly. For instance, Tulu-13B out-\nperforms Vicuna-13B by 0.148 and 0.149 at turn and di-\nalogue levels respectively in terms of the average Pearson\ncorrelations. The common attribute shared by Tulu-13B and\nChimera-inst-chat-13B is that they use a more diverse set\nof instruction data than other models. Their instruction data\ncome from different sources. For instance, the training data\nof Tulu consist of collections of NLP datasets with human-\nwritten instructions like FLAN V2 (Chung et al. 2022) and\nCoT (Wei et al. 2022b), collections of human-written in-\nstruction data from scratch, such as Dolly9, and data mixture\nthat is distilled from proprietary models like text-Davinci-\n003, ChatGPT, and GPT-4. Thus, it’s evident that the diver-\nsity and quality of instruction data can significantly influ-\nence the model performance, emphasizing the importance\nof sourcing diverse datasets that better align with the task\nwhen finetuning our LLM-based evaluators.\nPerformance Across Dimensions Generally, the majority\nof models excel in areas like relevance, coherence, engage-\nment, and overall quality. However, their performance di-\nminishes slightly when assessing interestingness and under-\nstandability at the turn level, and diversity at the dialogue\nlevel. Future explorations into leveraging LLMs for auto-\nmatic dialogue evaluation might benefit from designing ob-\njectives that target these specific dimensions.\nPerformance of GPT-4 The turn-level and dialogue-level\nevaluation abilities of GPT-4 are compared to the respective\ntop-5 LLMs. The results are presented in Table 3. Note that\nTable 3 cannot be directly matched to Table 2 because we\nonly use the human-annotated data for correlation compu-\ntation here while in the previous section, we conduct corre-\nlation analysis on the full data, which contain both human\nannotations and GPT-4 annotations. From Table 3, we can\nsee that GPT-4 achieves the best correlations with human\n9https://www.databricks.com/blog/2023/04/12/dolly-first-\nopen-commercially-viable-instruction-tuned-llm\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19518\nTur\nn-Level Dialogue-Level\nModels Rel Spe\nInt Und Ovr Avg Coh Eng\nDiv Inf Ovr Avg\nLLaMA-7B 0.114 0.056\n0.073 0.113 0.018 0.075 0.341 0.142\n0.049 0.166 0.109 0.161\nLLaMA-13B 0.442 0.240\n0.333 0.348 0.402 0.353 0.522 0.425\n0.237 0.324 0.404 0.382\nLLaMA-2-7B 0.241 0.187\n0.178 0.134 0.259 0.200 0.225 0.227\n0.044 0.261 0.292 0.210\nLLaMA-2-13B 0.303 0.122\n0.161 0.269 0.225 0.216 0.318 0.362\n-0.068 0.343 0.417 0.274\nXGen-8K-7B 0.214 0.152\n0.173 0.119 0.218 0.175 0.419 0.431\n0.325 0.387 0.456 0.404\nFalcon-7B 0.133 0.061\n0.174 0.101 0.104 0.114 0.366 0.476\n0.329 0.341 0.413 0.385\nMPT-8K-7B 0.209 0.048\n0.123 0.197 0.172 0.150 0.126 0.221\n0.112 0.187 0.088 0.147\nOpenLLaMA-7B 0.158 0.008\n0.105 0.154 0.184 0.122 0.402 0.452\n0.270 0.336 0.429 0.378\nOpenLLaMA-13B 0.231 0.112\n0.175 0.184 0.226 0.186 0.425 0.506\n0.239 0.342 0.471 0.397\nPythia-7B 0.145 0.097\n0.083 0.106 0.146 0.115 0.079 0.204\n0.026 0.094 0.220 0.124\nBLOOM-7B 0.082 0.021\n0.124 0.099 0.122 0.089 0.224 0.271\n0.137 0.217 0.277 0.225\nLLaMA-2-Chat-7B 0.434 0.191\n0.243 0.251 0.240 0.272 0.534 0.485\n0.375 0.447 0.498 0.468\nLLaMA-2-Chat-13B 0.559 0.353 0.207\n0.320 0.380 0.364 0.644 0.610 0.228\n0.514 0.613 0.522\nAlpaca-7B 0.430 0.293\n0.253 0.267 0.386 0.326 0.586 0.624 0.372 0.47\n7 0.566 0.525\nVicuna-7B 0.368 0.096\n0.221 0.100 0.219 0.201 0.490 0.468\n0.279 0.488 0.482 0.441\nVicuna-13B 0.400 0.318\n0.229 0.224 0.309 0.296 0.515 0.420\n0.306 0.419 0.417 0.415\nFalcon-Ins-7B 0.272 0.152\n0.293 0.179 0.246 0.228 0.504 0.513\n0.375 0.448 0.500 0.468\nTulu-13B 0.585 0.427 0.369 0.350 0.488 0.444 0.659 0.661 0.326 0.51\n8 0.657 0.564\nChimera-7B 0.489 0.276\n0.373 0.309 0.368\n0.363 0.563 0.599\n0.439 0.525 0.607 0.547\nChimera-13B 0.547 0.449 0.377 0.366 0.404 0.428 0.582 0.671 0.432 0.585 0.563 0.567\nPhoenix-7B 0.314 0.101\n0.291 0.258 0.234 0.240 0.480 0.493\n0.146 0.334 0.416 0.374\nOasst-sft-Pythia-12B 0.144 0.028\n0.203 0.132 0.110 0.123 0.386 0.358\n0.236 0.346 0.423 0.350\nBaize-v2-13B 0.477 0.350\n0.333 0.353 0.337 0.370 0.568 0.544\n0.397 0.482 0.469 0.492\nDolly-v2-12B 0.030 -0.009\n0.061 -0.004 0.020 0.020 0.182 0.238\n0.071 0.139 0.105 0.147\nMPT-8K-7B-Instruct 0.139 0.092\n0.176 0.095 0.099 0.120 0.321 0.352\n0.316 0.308 0.315 0.322\nXGen-8K-7B-Inst 0.272 0.293\n0.267 0.145 0.265 0.248 0.502 0.515\n0.308 0.417 0.506 0.450\nChatGLM-v2-6B 0.368 0.267\n0.191 0.181 0.184 0.238 0.214 0.236\n0.262 0.248 0.359 0.264\nWizardLM-13B-V1.2 0.463 0.422 0.390 0.245 0.314\n0.367 0.572 0.580\n0.455 0.513 0.632 0.550\nPalm-2\n(text-bison-001) 0.666 0.563 0.454 0.422 0.601 0.541 0.649 0.674 0.473 0.557 0.674 0.605\nChatGPT (gpt-3.5-turbo) 0.595 0.578 0.518 0.536 0.542 0.554 0.724 0.705 0.516 0.568 0.707 0.644\nTable 2: Dimension-specific Pearson correlation scores of different LLMs on fully-annotated data. The top-five scores in each\ndimension are underlined. 7B refers to 7 billion number of parameters.\nevaluation in almost all the dimensions, except for speci-\nficity at the turn level and diversity at the dialogue level.\nIt performs exceptionally well for relevance, coherence, and\noverall quality. On average, GPT-4 outperforms the second-\nbest LLM (ChatGPT) by a large absolute margin of 0.085\nand 0.042 at turn and dialogue levels respectively. The ob-\nservations justify our using GPT-4 to complete the missing\nannotations in the datasets. However, we should note that\neven the powerful GPT-4 model cannot reach perfect cor-\nrelations (> 0.8) on average suggesting that automatic dia-\nlogue evaluation remains an open problem. Future research\nshould continue enhancing the conversation understanding\ncapabilities of the language models.\nDeviation Between GPT-4 and Human PreferencesWe\nanalyze how much the assessment of the LLMs based on\nGPT-4 annotations deviates from that based on human an-\nnotations. Specifically, we compare the ranking results of\nthe 32 LLMs evaluated by GPT-4 vs those assessed by hu-\nman annotations. The deviation is quantified by the Spear-\nman correlation between the two ranking lists. A greater\nSpearman correlation indicates smaller deviations in the\nmodel rankings. We perform the analysis on the FED\ndataset (Mehri and Eskenazi 2020a), which contains human\nannotations for all the dimensions. As shown in Table 4, the\nresults reveal minimal deviation of GPT-4’s evaluation from\nhuman evaluation (>0.85 agreements) in all dimensions ex-\ncept response specificity.\nEnsemble Analysis\nIn this section, we delve into two straightforward ensem-\nble strategies. The first approach involves averaging the\nscores of each LLM assigned to different dimensions 10 to\nsee if this average provides a stronger correlation with over-\nall human evaluations than directly prompting the LLM to\nassess the overall quality. We refer to this method as the\n”dimension-wise ensemble”. The second strategy entails av-\neraging scores from multiple LLMs for a given dimension,\nallowing us to determine if this method can match the per-\n10At turn level, the relevance, specificity, understandability, and\ninterestingness scores are averaged to derive an overall score while\nat the dialogue level, the coherence, engagingness, diversity, and\ninformativeness scores are averaged.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19519\nTur\nn-Level\nRel Spe\nInt Und Ovr Avg\nBaize 0.449 0.147\n0.302 0.290 0.337 0.305\nTulu 0.544 0.193\n0.254 0.324 0.488 0.361\nChimera 0.507 0.234\n0.312 0.316 0.404 0.354\nPalm-2 0.616 0.317\n0.343 0.384 0.601 0.452\nChatGPT 0.576 0.408 0.446 0.424\n0.542 0.479\nGPT-4 0.704 0.342 0.538 0.558\n0.677 0.564\nDialogue-Lev\nel\nCoh Eng\nDiv Inf Ovr Avg\nTulu 0.668 0.629\n0.414 0.584 0.681 0.595\nChimera 0.595 0.628\n0.507 0.609 0.525 0.573\nWizardLM 0.536 0.522\n0.477 0.540 0.605 0.536\nPalm-2 0.584 0.633\n0.550 0.604 0.614 0.597\nChatGPT 0.650 0.647 0.551 0.570\n0.715 0.627\nGPT-4 0.760 0.689 0.534 0.620\n0.744 0.669\nTable 3: Dimension-specific Pearson correlation scores of\nGPT-4 vs other strong LLMs on human-annotated data. The\nbest score in each dimension is highlighted in bold. All the\nopen-source LLMs are the 13B variants.\nRel Spe\nInt Und Ovr\nFED-Turn 0.950 0.551\n0.887 0.869 0.971\nCoh Eng\nDiv Inf Ovr\nFED-Dial 0.861 0.900\n0.883 0.869 0.945\nTable 4: Spearman of model rankings evaluated by GPT-4\nratings vs evaluated by human ratings.\nformance of the stronger proprietary LLMs. We call this ap-\nproach the ”model-wise ensemble”.\nTur\nn Level Dialogue Level\nModel Ensemble Dir\nect Ensemble Direct\nTulu 0.492 0.488\n0.656 0.657\nChimera 0.476 0.404\n0.680 0.563\nBaize 0.417 0.337\n0.578 0.469\nWizardLM 0.408 0.314\n0.598 0.632\nPalm-2 0.602 0.601\n0.693 0.674\nChatGPT 0.564 0.542\n0.709 0.707\nTable 5: Dimension-wise ensemble results of difference\nmodels for the overall quality evaluation. All the open-\nsource LLMs are the 13B variants.\nDimension-Wise Ensemble We limit the analysis to Tulu-\n13B, Chimera-inst-chat-13B, Baize-v2-13B, WizardLM-\n13B-V1.2, Palm-2 Bison, and ChatGPT, which have strong\nmulti-dimensional evaluation performance. Table 5 presents\nperformance comparisons between the dimension-wise en-\nsemble and the direct prompting approaches for evaluating\nthe overall response or dialogue quality of each model. We\nTur\nn Level\nDimensions Palm-2\nBison ChatGPT Ensemble\nRel 0.666 0.595\n0.632\nInt 0.454 0.518\n0.465\nUnd 0.422 0.536\n0.407\nSpe 0.563 0.578\n0.487\nOvr 0.601 0.542\n0.491\nAv\nerage 0.541 0.554\n0.496\nDialogue Le\nvel\nCoh 0.649 0.724\n0.700\nEng 0.674 0.705\n0.725\nDiv 0.473 0.516\n0.492\nInf 0.557 0.568\n0.610\nOvr 0.674 0.707\n0.686\nAv\nerage 0.605 0.644\n0.643\nTable 6: Performance of the ensemble vs proprietary models\nfor each dimension at the turn and dialogue levels.\ncan observe that the ensemble approach yields strong cor-\nrelations with overall human judgments in general. Espe-\ncially for Chimera-inst-chat-13B and Baize-v2-13B at the\ndialogue level, the ensemble approach provides gains of\nmore than 10% than direct prompting. We also observe\nthat the scores assigned by Tulu-13B, ChatGPT, and Palm-\n2 Bison to different dimensions are highly similar while\nChimera-inst-chat-13B and Baize-v2-13B provide more di-\nverse scores when evaluating different dimensions. This may\nexplain why the ensemble of different dimension-specific\nscores of Chimera-inst-chat-13B and Baize-v2-13B leads to\nmore significant improvements than other LLMs.\nModel-Wise Ensemble For the model-wise ensemble, we\naverage the scores of the top 3 open-source models for each\ndimension as indicated in Table 2. We can observe that\nthe ensemble achieves comparable performance to ChatGPT\nand outperforms Palm-2 Bison at the dialogue level. At the\nturn level, the ensemble’s performance is worse than that of\nboth ChatGPT and Palm-2 Bison for dimensions other than\nrelevance and interestingness. The simple ensemble show-\ncases the potential benefits of combining multiple models to\nboost evaluation performance. Future research might delve\ndeeper into optimal ways of ensembling, such as how to best\ncombine models, which models to include in the ensemble,\nand how to weigh individual model outputs.\nRobustness of the LLM Evaluators\nMotivated by prior works on applying perturbation strate-\ngies for metric robustness probing (Sai et al. 2021; Khalid\nand Lee 2022), we analyze the robustness of LLM evaluators\nwith a series of adversarial strategies and table 7 presents the\ndata sources and statistics of our perturbation test suit11.\n11The detailed definitions of each perturbation strategy are out-\nlined in the Appendix.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19520\nLev\nel Source #Data Dims θ\nRR Turn\nDD & PC 1894 Rel 0.3\nRP Turn\nDD & PC 2919 Rel 0.2\nRNE Turn\nDD & PC 712 Rel 0.2\nCon Turn\nDD & PC 1094 Rel 0.2\nRep Turn\nDD & PC 1993 Und 0.2\nUP Turn\nDD & PC 369 Und 0.2\nDul Turn\nDD & PC 2811 Int/Spe 0.2\nOS Dial FED-Dial\n200 Coh 0.2\nUR Dial FED-Dial\n200 Coh 0.1\nSC Dial DECODE\n200 Eng 0.1\nUD Dial FED-Dial\n200 Eng 0.1\nRO Dial FED-Dial\n200 Eng 0.1\nGU Dial FED-Dial\n200 Eng/Inf 0.2\nCR Dial FED-Dial\n200 Inf 0.2\nTable 7: Adversarial Perturbation Data Statistics. DD & PC\nrefer to DailyDialog & PersonaChat Respectively. RR, RP,\nRNE, Con, Rep, UP, Dul, OS, UR, SC, UD, RO, GU, and\nCR refer to Random Response, Replace Pronoun, Replace\nNamed Entity, Contradiction, Repetition, Unnatural Para-\nphrase, Dullness, Order Shuffle, Utterance Replacement,\nSelf-Contradiction, Utterance Duplication, Repeating Oth-\ners, Generic Utterance, and Content Reduction respectively.\nWe focus on negative adversarial perturbations that di-\nminish the quality of the original response or dialogue. For-\nmally, let qdim represent the score assigned by LLMs for a\nhigh-quality dialogue/response specific to a certain dimen-\nsion. Conversely,pdim denotes the score given by the LLMs\nwhen a particular negative perturbation, targeting that di-\nmension, is applied to the response or dialogue. The LLMs’\nrobustness against that particular perturbation can be deter-\nmined by R = 1\nN ∑y. N is the number of data instances\ngenerated with that particular perturbation and y is calcu-\nlated as\ny = {1 if qdim − pdim > θ\n0 otherwise\nwhere θ is a positive threshold value determining the ex-\ntent of quality reduction introduced by the perturbation.\nIt’s worth noting that some perturbation strategies result in\ngreater quality degradation than others. A larger R signifies\ngreater robustness. As that robustness analysis is only mean-\ningful when applied to strong automatic metrics, we limit\nthe analysis to ChatGPT, Palm-2 Bison, Tulu-13B, Chimera-\ninst-chat-13B, Baize-v2-13B, WizardLM-13B-V1.2, and\nLLaMA-2-Chat-13B.\nImpact of Different Strategies For the ”random re-\nsponse” perturbation at the turn level, Palm-2 Bison, the\ntop-performing LLM, scores the original response over 0.3\nhigher than the perturbed response in 62.3% of cases. The\nother LLMs manage this less than half the time. Notably,\nBaize-v2-13B and WizardLM-13B-V1.2 frequently fail to\nrecognize the ”random response” perturbation. The “replace\npronoun” and ”replace named entity” perturbations are more\nchallenging than ”random response” as the robustness ratio,\nR, of all the LLMs drops to below 40%. These two strate-\ngies require a more fine-grained understanding of the se-\nmantics of the dialogue context and the response. For the\n“contradiction” perturbation, Palm-2 Bison and three other\nopen-source LLMs, Tulu-13B, Chimera-13B, and LLaMA-\n2-Chat-13B achieve a robustness ratio of more than 50%,\nsignificantly outperforming ChatGPT.\nIn general, all the LLMs perform poorly on “repetition”,\n“unnatural paraphrase”, and “dullness” perturbations. The\nobservation is in line with their weaker correlations in the\ninterestingness and understandability dimensions than in the\nrelevance dimension as shown in Table 2. Notably, ChatGPT\nperforms much better than other LLMs in handling the “un-\nnatural paraphrase” perturbation.\nThe proprietary models are more adept at handling the\nperturbations targeting dialogue-level coherence than the\nopen-source ones. For example, ChatGPT achieves a robust-\nness ratio of 0.535 and 0.730 for “Order Shuffle” and “Utter-\nance Replacement” respectively. Most LLMs struggle with\nperturbations targeting dialogue-level engagingness and in-\nformativeness, such as “Utterance Duplication”, “Repeat-\ning Others”, and “Generic Utterance”, except for LLaMA-2-\nChat-13B, suggesting that future research on LLMs for auto-\nmatic dialogue evaluation should prioritize a deeper compre-\nhension of multi-turn dialogues, such as the depth of topic\nengagement, speaker sentiments, interactivity & proactivity\namong the speakers, etc., which goes beyond mere surface-\nlevel coherence assessments.\nProprietary vs Open-Source LLMsAs illustrated in Ta-\nble 8, Palm-2 Bison exhibits superior robustness at the turn\nlevel, whereas LLaMA-2-Chat-13B performs the best at the\ndialogue level. Palm-2 Bison can handle most of the ad-\nversarial perturbations. Notably, it excels in identifying de-\nclines in response relevance, interestingness, specificity, and\ndialogue coherence. ChatGPT is capable of dealing with\nadversarial perturbations targeting response understandabil-\nity and dialogue coherence. Surprisingly, it performs poorly\nin handling other types of perturbations. Among the open-\nsource LLMs, Tulu-13B and LLaMA-2-Chat-13B perform\nsimilarly on average at the turn level. They are better than\nthe other three open-source models. At the dialogue level,\nLLaMA-2-Chat-13B performs exceptionally well and out-\nperforms Palm-2 Bison by 3.7% and ChatGPT by 16.4% on\naverage. It demonstrates consistent strength in dealing with\nall the perturbations except those targeting response under-\nstandability and dialogue coherence. In contrast, both Baize-\n13B and WizardLM-13B struggle to handle negative pertur-\nbations at both the turn and dialogue levels.\nIn general, we can observe that none of the LLMs are ro-\nbust against all the adversarial perturbations. The variance\nin performances underscores the inherent complexity of di-\nalogues and the challenges in creating a universally robust\nautomatic dialogue evaluator. Future work should prioritize\nbuilding on these findings to improve the robustness and\nadaptability of LLMs across diverse perturbations.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19521\nTurn\nLevel\nPerturbations Palm-2\nChatGPT Tulu Chimera Baize WizardLM L2-Chat\nRandom Response\n(↓Rel) 0.623 0.391 0.419\n0.307 0.043 0.125 0.429\nReplace Pronoun (↓Rel) 0.388 0.189 0.156\n0.249 0.041 0.083 0.272\nReplace Named Entity (↓Rel) 0.391 0.160 0.274\n0.301 0.097 0.156 0.337\nContradiction (↓Rel) 0.529 0.102\n0.523 0.535 0.214 0.282 0.529\nRepetition (↓Und) 0.127 0.139 0.060 0.068\n0.003 0.002 0.016\nUnnatural Paraphrase (↓Und) 0.307 0.451 0.182 0.252\n0.014 0.008 0.065\nDullness (↓Int) 0.433 0.107 0.260\n0.122 0.000 0.128 0.245\nDullness (↓Spe) 0.411 0.264 0.191\n0.092 0.000 0.023 0.166\nAv\nerage 0.401 0.225 0.258\n0.241 0.052 0.101 0.257\nDialogue Le\nvel\nOrder Shuf\nfle (↓Coh) 0.440 0.535 0.135 0.000\n0.000 0.000 0.155\nUtterance Replacement (↓Coh) 0.490 0.730 0.130 0.000\n0.000 0.000 0.075\nGeneric Utterance (↓Eng) 0.295 0.145\n0.110 0.085 0.000 0.050 0.620\nSelf-Contradiction (↓Eng) 0.365 0.075 0.645 0.455\n0.050 0.155 0.535\nUtterance Duplication (↓Eng) 0.230 0.095\n0.235 0.150 0.000 0.030 0.665\nRepeating Others (↓Eng) 0.285 0.090\n0.120 0.255 0.025 0.060 0.445\nContent Reduction (↓Inf) 0.355 0.040 0.235\n0.170 0.000 0.055 0.230\nGeneric Utterance (↓Inf) 0.330 0.065\n0.210 0.155 0.000 0.050 0.365\nAv\nerage 0.349 0.222\n0.228 0.159 0.009 0.050 0.386\nTable 8: Percentage of cases when the LLMs successfully detect a perturbation (Robustness Ratio R). The best ratio for each\nperturbation is highlighted in bold. All the open-source LLMs are the 13B variants and L2-Chat refers to LLaMA-2-Chat-13B.\nRelated Work\nHuynh et al. (2023) conduct a comprehensive analysis of\nthe dialogue evaluation capability of LLMs with varying\nmodel types, sizes, choices of training data, etc. Their study\nis limited to correlation analysis on a few LLMs, which\nare mainly vanilla models without instruction-tuning, such\nas BLOOM (Scao et al. 2023) and OPT (Zhang et al.\n2022b). With the increasing popularity of API-based propri-\netary instruction-following LLMs, such as OpenAI’s Chat-\nGPT and Anthropic Claude (Bai et al. 2022). Several re-\ncent works (Chen et al. 2023a; Liu et al. 2023; Lin and\nChen 2023) study the dialogue evaluation capability of these\nLLMs via prompting and show that such LLMs exhibit\nstrong zero-shot correlations with human evaluation. Yet,\ntheir study is constrained in terms of the number of meta-\nevaluation datasets, mode of assessment, and number of\nLLMs examined. Our research addresses these constraints,\nproviding profound insights into the use of various LLMs\nfor automatic dialogue evaluation. We conduct an extensive\nmulti-dimensional correlation analysis involving 30 of the\nlatest popular LLMs and introduce a range of perturbation\nstrategies to assess their robustness.\nConclusion\nIn summary, we’ve analyzed the multi-dimensional evalua-\ntion abilities of 30 recent LLMs, covering coherence, engag-\ningness, and more at dialogue and turn levels. To facilitate\nthe analysis, we enrich 12 existing meta-evaluation datasets\nwith new annotations, which will be publicly available for\nbenchmarking new metrics. Beyond correlation analysis, we\nintroduce various adversarial strategies to test LLM robust-\nness, a perspective not explored in existing works. Lastly,\nwe also examine the impact of dimension-wise and model-\nwise ensembles on dialogue evaluation in our work. The key\ninsights are summarized as follows:\n1. Instruction-tuned models align better with human evalu-\nations than vanilla foundation models.\n2. Proprietary models, especially GPT-4, have superior\nevaluation abilities compared to open-source LLMs.\n3. Model size and instruction data are vital for evaluation.\nOnly the ensemble of strong open-source models per-\nforms on par with ChatGPT and PaLM-2 Bison.\n4. LLMs excel more in evaluating coherence, relevance,\nand overall quality than specificity and diversity. Using\nan ensemble of their dimension-specific scores aligns\nbetter with the overall human evaluations than a direct\nassessment of the overall quality.\n5. None of the LLMs are robust against all the adversarial\nperturbations. Google’s Palm-2 Bison achieves the best\nrobustness at the turn level while Meta’s LLaMA-2-Chat-\n13B (Touvron et al. 2023b) tops at the dialogue level.\n6. LLMs show promise in automatic dialogue evaluation,\nbut it’s still an open problem, with even GPT-4 not ex-\ncelling in all dimensions (achieve correlations > 0.8).\nAcknowledgments\nWe thank the anonymous reviewers for their insight-\nful comments. This work is supported by Human Robot\nCollaborative AI under its AME Programmatic Funding\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19522\nScheme (Project No. A18A2b0046), the National Natu-\nral Science Foundation of China (Grant No. 62271432,\n62106038), Shenzhen Science and Technology Research\nFund (Fundamental Research Key Project Grant No.\nJCYJ20220818103001002), and the Internal Project Fund\nfrom Shenzhen Research Institute of Big Data under Grant\nNo. T00120220002. This work is also a result of the\nprojects: ASTOUND (101071191 - HORIZON-EIC-2021-\nPATHFINDERCHALLENGES-01) funded by the Euro-\npean Commission, BEWORD (PID2021-126061OB-C43)\nfunded by MCIN/AEI/10.13039/501100011033 and, as\nappropriate, by “ERDF A way of making Europe”,\nby the “European Union”, and the Research Grants\nfor Young Investigators from Universidad Polit ´ecnica\nde Madrid (GENIUS:APOYO-JOVENES-21-TAXTYC-32-\nK61X37) funded by Comunidad de Madrid.\nReferences\nAlmazrouei, E.; Alobeidli, H.; Alshamsi, A.; Cappelli, A.;\nCojocaru, R.; Debbah, M.; et al. 2023. Falcon-40B: an open\nlarge language model with state-of-the-art performance.\nBai, Y .; Kadavath, S.; Kundu, S.; Askell, A.; Kernion, J.;\nJones, A.; et al. 2022. Constitutional AI: Harmlessness from\nAI Feedback. arXiv:2212.08073.\nBiderman, S.; Schoelkopf, H.; Anthony, Q.; Bradley, H.;\nO’Brien, K.; Hallahan, E.; Khan, M. A.; Purohit, S.;\nPrashanth, U. S.; Raff, E.; Skowron, A.; Sutawika, L.; and\nvan der Wal, O. 2023. Pythia: A Suite for Analyzing\nLarge Language Models Across Training and Scaling.arXiv\npreprint arXiv: Arxiv-2304.01373.\nBrown, T.; Mann, B.; Ryder, N.; Subbiah, M.; Kaplan, J. D.;\nDhariwal, P.; et al. 2020. Language Models are Few-Shot\nLearners. In Larochelle, H.; Ranzato, M.; Hadsell, R.; Bal-\ncan, M.; and Lin, H., eds., Advances in Neural Information\nProcessing Systems, volume 33, 1877–1901. Curran Asso-\nciates, Inc.\nChen, Y .; Wang, R.; Jiang, H.; Shi, S.; and Xu, R. 2023a. Ex-\nploring the Use of Large Language Models for Reference-\nFree Text Quality Evaluation: A Preliminary Empirical\nStudy. arXiv preprint arXiv: Arxiv-2304.00723.\nChen, Z.; Jiang, F.; Chen, J.; Wang, T.; Yu, F.; Chen, G.;\nZhang, H.; Liang, J.; Zhang, C.; Zhang, Z.; Li, J.; Wan,\nX.; Wang, B.; and Li, H. 2023b. Phoenix: Democratizing\nChatGPT across Languages. arXiv preprint arXiv: Arxiv-\n2304.10453.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\net al. 2023. Vicuna: An Open-Source Chatbot Impressing\nGPT-4 with 90%* ChatGPT Quality.\nChowdhery, A.; Narang, S.; Devlin, J.; Bosma, M.; Mishra,\nG.; Roberts, A.; et al. 2022. PaLM: Scaling Language Mod-\neling with Pathways. arXiv:2204.02311.\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; et al. 2022. Scaling Instruction-Finetuned Lan-\nguage Models. arXiv preprint arXiv: Arxiv-2210.11416.\nFu, J.; Ng, S.-K.; Jiang, Z.; and Liu, P. 2023. GPTScore:\nEvaluate as You Desire. arXiv:2302.04166.\nGhazarian, S.; Hedayatnia, B.; Papangelis, A.; Liu, Y .; and\nHakkani-Tur, D. 2022. What is wrong with you?: Lever-\naging User Sentiment for Automatic Dialog Evaluation.\nIn Findings of the Association for Computational Linguis-\ntics: ACL 2022, 4194–4204. Dublin, Ireland: Association for\nComputational Linguistics.\nGilardi, F.; Alizadeh, M.; and Kubli, M. 2023. ChatGPT\nOutperforms Crowd-Workers for Text-Annotation Tasks.\narXiv preprint arXiv: 2303.15056.\nGupta, P.; Jiao, C.; Yeh, Y .-T.; Mehri, S.; Eskenazi, M.; and\nBigham, J. 2022. InstructDial: Improving Zero and Few-\nshot Generalization in Dialogue through Instruction Tuning.\nIn Proceedings of the 2022 Conference on Empirical Meth-\nods in Natural Language Processing, 505–525. Abu Dhabi,\nUnited Arab Emirates: Association for Computational Lin-\nguistics.\nHuynh, J.; Jiao, C.; Gupta, P.; Mehri, S.; Bajaj, P.; Chaud-\nhary, V .; and Eskenazi, M. 2023. Understanding the Effec-\ntiveness of Very Large Language Models on Dialog Evalu-\nation. In The 13th International Workshop on Spoken Dia-\nlogue Systems Technology.\nJi, T.; Graham, Y .; Jones, G.; Lyu, C.; and Liu, Q. 2022.\nAchieving Reliable Human Assessment of Open-Domain\nDialogue Systems. In Proceedings of the 60th Annual Meet-\ning of the Association for Computational Linguistics (Vol-\nume 1: Long Papers), 6416–6437. Dublin, Ireland: Associa-\ntion for Computational Linguistics.\nKarpinska, M.; Akoury, N.; and Iyyer, M. 2021. The Per-\nils of Using Mechanical Turk to Evaluate Open-Ended Text\nGeneration. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing, 1265–\n1285. Online and Punta Cana, Dominican Republic: Associ-\nation for Computational Linguistics.\nKhalid, B.; and Lee, S. 2022. Explaining Dialogue Eval-\nuation Metrics using Adversarial Behavioral Analysis. In\nProceedings of the 2022 Conference of the North American\nChapter of the Association for Computational Linguistics:\nHuman Language Technologies, 5871–5883. Seattle, United\nStates: Association for Computational Linguistics.\nLin, Y .-T.; and Chen, Y .-N. 2023. LLM-Eval: Unified\nMulti-Dimensional Automatic Evaluation for Open-Domain\nConversations with Large Language Models. In Proceed-\nings of the 5th Workshop on NLP for Conversational AI\n(NLP4ConvAI 2023), 47–58. Toronto, Canada: Association\nfor Computational Linguistics.\nLiu, C.-W.; Lowe, R.; Serban, I.; Noseworthy, M.; Charlin,\nL.; and Pineau, J. 2016. How NOT To Evaluate Your Di-\nalogue System: An Empirical Study of Unsupervised Eval-\nuation Metrics for Dialogue Response Generation. In Pro-\nceedings of the 2016 Conference on Empirical Methods in\nNatural Language Processing, 2122–2132. Austin, Texas:\nAssociation for Computational Linguistics.\nLiu, Y .; Iter, D.; Xu, Y .; Wang, S.; Xu, R.; and Zhu, C. 2023.\nG-Eval: NLG Evaluation using GPT-4 with Better Human\nAlignment. arXiv preprint arXiv: Arxiv-2303.16634.\nMehri, S.; Choi, J.; D’Haro, L. F.; Deriu, J.; Eskenazi, M.;\nGasic, M.; Georgila, K.; Hakkani-Tur, D.; Li, Z.; Rieser, V .;\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19523\nShaikh, S.; Traum, D.; Yeh, Y .-T.; Yu, Z.; Zhang, Y .; and\nZhang, C. 2022a. Report from the NSF Future Directions\nWorkshop on Automatic Evaluation of Dialog: Research Di-\nrections and Challenges.\nMehri, S.; and Eskenazi, M. 2020a. Unsupervised Evalua-\ntion of Interactive Dialog with DialoGPT. In Proceedings\nof the 21th Annual Meeting of the Special Interest Group on\nDiscourse and Dialogue, 225–235. 1st virtual meeting: As-\nsociation for Computational Linguistics.\nMehri, S.; and Eskenazi, M. 2020b. USR: An Unsupervised\nand Reference Free Evaluation Metric for Dialog Genera-\ntion. In Proceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, 681–707. Online:\nAssociation for Computational Linguistics.\nMehri, S.; Feng, Y .; Gordon, C.; Alavi, S. H.; Traum, D.; and\nEskenazi, M. 2022b. Interactive Evaluation of Dialog Track\nat DSTC9. In Proceedings of the Thirteenth Language Re-\nsources and Evaluation Conference, 5731–5738. Marseille,\nFrance: European Language Resources Association.\nOuyang, L.; and et al. 2022. Training language models to\nfollow instructions with human feedback. In Advances in\nneural information processing systems.\nPapineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002.\nBleu: a Method for Automatic Evaluation of Machine Trans-\nlation. In Proceedings of the 40th Annual Meeting of the As-\nsociation for Computational Linguistics, 311–318. Philadel-\nphia, Pennsylvania, USA: Association for Computational\nLinguistics.\nSai, A. B.; Dixit, T.; Sheth, D. Y .; Mohan, S.; and Khapra,\nM. M. 2021. Perturbation CheckLists for Evaluating NLG\nEvaluation Metrics. In Proceedings of the 2021 Confer-\nence on Empirical Methods in Natural Language Process-\ning, 7219–7234. Online and Punta Cana, Dominican Repub-\nlic: Association for Computational Linguistics.\nScao, T. L.; Fan, A.; Akiki, C.; Pavlick, E.; Ili´c, S.; Hesslow,\nD.; et al. 2023. BLOOM: A 176B-Parameter Open-Access\nMultilingual Language Model. arXiv:2211.05100.\nSee, A.; Roller, S.; Kiela, D.; and Weston, J. 2019. What\nmakes a good conversation? How controllable attributes af-\nfect human judgments. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Association for\nComputational Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), 1702–1723. Minneapo-\nlis, Minnesota: Association for Computational Linguistics.\nSmith, E.; Hsu, O.; Qian, R.; Roller, S.; Boureau, Y .-L.; and\nWeston, J. 2022. Human Evaluation of Conversations is an\nOpen Problem: comparing the sensitivity of various meth-\nods for evaluating dialogue agents. In Proceedings of the\n4th Workshop on NLP for Conversational AI, 77–97. Dublin,\nIreland: Association for Computational Linguistics.\nSvikhnushina, E.; Filippova, A.; and Pu, P. 2022. iEval:\nInteractive Evaluation Framework for Open-Domain Empa-\nthetic Chatbots. In Proceedings of the 23rd Annual Meeting\nof the Special Interest Group on Discourse and Dialogue ,\n419–431. Edinburgh, UK: Association for Computational\nLinguistics.\nTaori, R.; Gulrajani, I.; Zhang, T.; Dubois, Y .; Li, X.;\nGuestrin, C.; Liang, P.; and Hashimoto, T. B. 2023. Stanford\nAlpaca: An Instruction-following LLaMA model. https:\n//github.com/tatsu-lab/stanford\nalpaca.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; Rodriguez, A.; Joulin, A.; Grave, E.; and Lam-\nple, G. 2023a. LLaMA: Open and Efficient Foundation Lan-\nguage Models. ARXIV.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi,\nA.; Babaei, Y .; and other. 2023b. Llama 2: Open Founda-\ntion and Fine-Tuned Chat Models. arXiv preprint arXiv:\n2307.09288.\nWang, Y .; Ivison, H.; Dasigi, P.; Hessel, J.; Khot, T.; Chandu,\nK. R.; Wadden, D.; MacMillan, K.; Smith, N. A.; Beltagy,\nI.; and Hajishirzi, H. 2023. How Far Can Camels Go? Ex-\nploring the State of Instruction Tuning on Open Resources.\narXiv preprint arXiv: 2306.04751.\nWei, J.; Bosma, M.; Zhao, V .; Guu, K.; Yu, A. W.; Lester,\nB.; Du, N.; Dai, A. M.; and Le, Q. V . 2022a. Finetuned\nLanguage Models are Zero-Shot Learners. In International\nConference on Learning Representations.\nWei, J.; Wang, X.; Schuurmans, D.; Bosma, M.; brian ichter;\nXia, F.; Chi, E. H.; Le, Q. V .; and Zhou, D. 2022b. Chain\nof Thought Prompting Elicits Reasoning in Large Language\nModels. In Oh, A. H.; Agarwal, A.; Belgrave, D.; and Cho,\nK., eds., Advances in Neural Information Processing Sys-\ntems.\nXu, C.; Guo, D.; Duan, N.; and McAuley, J. 2023a. Baize:\nAn Open-Source Chat Model with Parameter-Efficient Tun-\ning on Self-Chat Data. arXiv preprint arXiv: 2304.01196.\nXu, C.; Sun, Q.; Zheng, K.; Geng, X.; Zhao, P.; Feng, J.; Tao,\nC.; and Jiang, D. 2023b. WizardLM: Empowering Large\nLanguage Models to Follow Complex Instructions. arXiv\npreprint arXiv: Arxiv-2304.12244.\nYeh, Y .-T.; Eskenazi, M.; and Mehri, S. 2021. A Compre-\nhensive Assessment of Dialog Evaluation Metrics. In The\nFirst Workshop on Evaluations and Assessments of Neural\nConversation Systems, 15–33. Online: Association for Com-\nputational Linguistics.\nZhang, C.; D’Haro, L. F.; Friedrichs, T.; and Li, H. 2022a.\nMDD-Eval: Self-Training on Augmented Data for Multi-\nDomain Dialogue Evaluation.Proceedings of the AAAI Con-\nference on Artificial Intelligence, 36(10): 11657–11666.\nZhang, S.; Roller, S.; Goyal, N.; Artetxe, M.; Chen, M.;\nChen, S.; et al. 2022b. OPT: Open Pre-trained Transformer\nLanguage Models. arXiv preprint arXiv: Arxiv-2205.01068.\nZhao, T.; Lala, D.; and Kawahara, T. 2020. Designing Pre-\ncise and Robust Dialogue Response Evaluators. In Proceed-\nings of the 58th Annual Meeting of the Association for Com-\nputational Linguistics, 26–33. Online: Association for Com-\nputational Linguistics.\nZheng, L.; Chiang, W.-L.; Sheng, Y .; Zhuang, S.; Wu, Z.;\nZhuang, Y .; et al. 2023. Judging LLM-as-a-Judge with\nMT-Bench and Chatbot Arena. In Thirty-seventh Confer-\nence on Neural Information Processing Systems Datasets\nand Benchmarks Track.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n19524",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.593045711517334
    },
    {
      "name": "Natural language processing",
      "score": 0.5104112029075623
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3707125186920166
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I88060688",
      "name": "Universidad Politécnica de Madrid",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I150229711",
      "name": "University of Electronic Science and Technology of China",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210116924",
      "name": "Chinese University of Hong Kong, Shenzhen",
      "country": "CN"
    }
  ]
}