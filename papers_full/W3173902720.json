{
  "title": "Positional Artefacts Propagate Through Masked Language Model Embeddings",
  "url": "https://openalex.org/W3173902720",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2593782505",
      "name": "Ziyang Luo",
      "affiliations": [
        "Uppsala University"
      ]
    },
    {
      "id": "https://openalex.org/A2759999012",
      "name": "Artur Kulmizev",
      "affiliations": [
        "Uppsala University"
      ]
    },
    {
      "id": "https://openalex.org/A2765646751",
      "name": "Xiaoxi Mao",
      "affiliations": [
        "NetEase (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631349028",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2963084773",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2988217457",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W3161869090",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2970550868",
    "https://openalex.org/W2950577311",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3033187248",
    "https://openalex.org/W3154229486",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2251861449",
    "https://openalex.org/W2133458109",
    "https://openalex.org/W2462305634",
    "https://openalex.org/W4214566146",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3037983807",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2759848268",
    "https://openalex.org/W2964165804",
    "https://openalex.org/W4245255589",
    "https://openalex.org/W2953273646",
    "https://openalex.org/W2099584159",
    "https://openalex.org/W2254842198",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W4288375898",
    "https://openalex.org/W2948771346",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4288351520",
    "https://openalex.org/W4288796528",
    "https://openalex.org/W2946359678",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W3099143320",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2126400076",
    "https://openalex.org/W2907252220",
    "https://openalex.org/W2905381038",
    "https://openalex.org/W3122515622",
    "https://openalex.org/W2949650786",
    "https://openalex.org/W2956105246",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W1614298861",
    "https://openalex.org/W2973154008",
    "https://openalex.org/W3017779903",
    "https://openalex.org/W2966610483",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2970727289",
    "https://openalex.org/W2964303116",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2912206855",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2971031791",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2152180407",
    "https://openalex.org/W3104350794"
  ],
  "abstract": "Ziyang Luo, Artur Kulmizev, Xiaoxi Mao. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 5312–5327\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n5312\nPositional Artefacts Propagate Through\nMasked Language Model Embeddings\nZiyang Luo1∗, Artur Kulmizev1, Xiaoxi Mao2\n1 Department of Linguistics and Philology, Uppsala University, Sweden\n2 Fuxi AI Lab, NetEase Inc., Hangzhou, China\nZiyang.Luo.9588@student.uu.se, artur.kulmizev@lingfil.uu.se\nmaoxiaoxi@corp.netease.com\nAbstract\nIn this work, we demonstrate that the contex-\ntualized word vectors derived from pretrained\nmasked language model-based encoders share\na common, perhaps undesirable pattern across\nlayers. Namely, we ﬁnd cases of persistent\noutlier neurons within BERT and RoBERTa’s\nhidden state vectors that consistently bear the\nsmallest or largest values in said vectors. In\nan attempt to investigate the source of this in-\nformation, we introduce a neuron-level anal-\nysis method, which reveals that the outliers\nare closely related to information captured by\npositional embeddings. We also pre-train the\nRoBERTa-base models from scratch and ﬁnd\nthat the outliers disappear without using posi-\ntional embeddings. These outliers, we ﬁnd, are\nthe major cause of anisotropy of encoders’ raw\nvector spaces, and clipping them leads to in-\ncreased similarity across vectors. We demon-\nstrate this in practice by showing that clipped\nvectors can more accurately distinguish word\nsenses, as well as lead to better sentence em-\nbeddings when mean pooling. In three super-\nvised tasks, we ﬁnd that clipping does not af-\nfect the performance.\n1 Introduction\nA major area of NLP research in the deep learn-\ning era has concerned the representation of words\nin low-dimensional, continuous vector spaces.\nTraditional methods for achieving this have in-\ncluded word embedding models such as Word2Vec\n(Mikolov et al., 2013), GloVe (Pennington et al.,\n2014), and FastText (Bojanowski et al., 2017).\nHowever, though inﬂuential, such approaches all\nshare a uniform pitfall in assigning a single, static\nvector to a word type. Given that the vast major-\nity of words are polysemous (Klein and Murphy,\n2001), static word embeddings cannot possibly rep-\nresent a word’s changing meaning in context.\n∗ Work partly done during internship at NetEase Inc..\nIn recent years, deep language models, like\nELMo (Peters et al., 2018), BERT (Devlin et al.,\n2019) and RoBERTa (Liu et al., 2019b), have\nachieved great success across many NLP tasks.\nSuch models introduce a new type of word vectors,\ndeemed the contextualized variety, where the repre-\nsentation is computed with respect to the context\nof the target word. Since these vectors are sensitive\nto context, they can better address the polysemy\nproblem that hinders traditional word embeddings.\nIndeed, studies have shown that replacing static\nembeddings (e.g. word2vec) with contextualized\nones (e.g. BERT) can beneﬁt many NLP tasks,\nincluding constituency parsing (Kitaev and Klein,\n2018), coreference resolution (Joshi et al., 2019)\nand machine translation (Liu et al., 2020).\nHowever, despite the major success in deploy-\ning these representations across linguistic tasks,\nthere remains little understanding about informa-\ntion embedded in contextualized vectors and the\nmechanisms that generate them. Indeed, an en-\ntire research area central to this core issue — the\ninterpretability of neural NLP models — has re-\ncently emerged (Linzen et al., 2018, 2019; Alishahi\net al., 2020). A key theme in this line of work\nhas been the use of linear probes in investigating\nthe linguistic properties of contextualized vectors\n(Tenney et al., 2019; Hewitt and Manning, 2019).\nSuch studies, among many others, show that con-\ntextualization is an important factor that sets these\nembeddings apart from static ones, the latter of\nwhich are unreliable in extracting features central\nto context or linguistic hierarchy. Nonetheless,\nmuch of this work likewise fails to engage with\nthe raw vector spaces of language models, pre-\nferring instead to focus its analysis on the trans-\nformed vectors. Indeed, the fraction of work that\nhas done the former has shed some curious insights:\nthat untransformed BERT sentence representations\nstill lag behind word embeddings across a variety\n5313\nof semantic benchmarks (Reimers and Gurevych,\n2019) and that the vector spaces of language mod-\nels are explicitly anisotropic (Ethayarajh, 2019;\nLi et al., 2020a). Certainly, an awareness of the\npatterns inherent to models’ untransformed vector\nspaces — even if shallow — can only beneﬁt the\ntransformation-based analyses outlined above.\nIn this work, we shed light on a persistent pattern\nthat can be observed for contextualized vectors pro-\nduced by BERT and RoBERTa. Namely, we show\nthat, across all layers, select neurons in BERT and\nRoBERTa consistently bear extremely large values.\nWe observe this pattern across vectors for all words\nin several datasets, demonstrating that these sin-\ngleton dimensions serve as major outliers to the\ndistributions of neuron values in both encoders’\nrepresentational spaces. With this insight in mind,\nthe contributions of our work are as follows:\n1. We introduce a neuron-level method for ana-\nlyzing the origin of a model’s outliers. Using\nthis, we show that they are closely related to\npositional information.\n2. In investigating the effects of clipping the out-\nliers (zeroing-out), we show that the degree\nof anisotropy in the vector space diminishes\nsigniﬁcantly.\n3. We show that after clipping the outliers, the\nBERT representations can better distinguish\nbetween a word’s potential senses in the\nword-in-context (WiC) dataset (Pilehvar and\nCamacho-Collados, 2019), as well as lead to\nbetter sentence embeddings when mean pool-\ning.\n2 Finding outliers\nIn this section, we demonstrate the existence of\nlarge-valued vector dimensions across nearly all\ntokens encoded by BERT and RoBERTa. To illus-\ntrate these patterns, we employ two well-known\ndatasets — SST-2 (Socher et al., 2013) and QQP 1.\nSST-2 (60.7k sentences) is a widely-employed sen-\ntiment analysis dataset of movie reviews, while\nQQP (727.7k sentences) is a semantic textual sim-\nilarity dataset of Quora questions, which collects\nquestions across many topics. We choose these\ndatasets in order to account for a reasonably wide\ndistributions of domains and topics, but note that\n1https://www.quora.com/q/quoradata/\nFirst-Quora-Dataset-Release-Question-Pairs\nFigure 1: Average vectors for each layer of BERT-base.\nFigure 2: Average vectors for each layer of RoBERTa-\nbase.\nany dataset would illustrate our ﬁndings well. We\nrandomly sample 10k sentences from the training\nsets of both SST-2 and QQP, tokenize them, and\nencode them via BERT-base and RoBERTa-base.\nAll models are downloaded from the Huggingface\nTransformers Library (Wolf et al., 2020), though\nwe replicated our results for BERT by loading the\nprovided model weights via our own loaders.\nWhen discounting the input embedding layers\nof each model, we are left with 3.68M and 3.59M\ncontextualized token embeddings for BERT-base\nand RoBERTa-base, respectively. In order to illus-\ntrate the outlier patterns, we average all subword\nvectors for each layer of each model.\nIn examining BERT-base, we ﬁnd that the mini-\nmum value of 96.60% of vectors lies in the 557th\ndimension. Figure 1 displays the averaged subword\nvectors for each layer of BERT-base, corroborat-\ning that these patterns exist across all layers. For\nRoBERTa-base, we likewise ﬁnd that the maximum\nvalue of all vectors is the 588th element. Interest-\ningly, theminimum element of 88.19% of vectors in\n5314\nRoBERTa-base is the77th element, implying that\nRoBERTa has two such outliers. Figure 2 displays\nthe average vectors for each layer of RoBERTa-\nbase.\nOur observations here reveal a curious pattern\nthat is present in the base versions of BERT and\nRoBERTa. We also corroborate the same ﬁndings\nfor the large and distilled (Sanh et al., 2020) vari-\nants of these architectures, which can be found in\nthe Appendix A. Indeed, it would be difﬁcult to\nreach any sort of conclusion about the represen-\ntational geometry of such models without under-\nstanding the outliers’ origin(s).\n3 Where do outliers come from?\nIn this section, we attempt to trace the source of the\noutlier dimensions in BERT-base and RoBERTa-\nbase (henceforth BERT and RoBERTa). Similarly\nto the previous section, we can corroborate the re-\nsults of the experiments described here (as well\nas in the remainder of the paper) for the large and\ndistilled varieties of each respective architecture.\nThus, for reasons of brevity, we focus our forth-\ncoming analyses on the base versions of BERT\nand RoBERTa and include results for the remain-\ning models in the Appendix B.2 for the interested\nreader.\nIn our per-layer analysis in §2, we report that\noutlier dimensions exist across every layer in each\nmodel. Upon a closer look at the input layer (which\nfeatures a vector sum of positional, segment, and\ntoken embeddings), we ﬁnd that the same outliers\nalso exist in positional embeddings. Figure 3 shows\nthat the 1st positional embedding of BERT has\ntwo such dimensions, where the 557th element is\nlikewise the minimum. Interestingly, this pattern\ndoes not exist in other positional embeddings, nor\nin segment or token embeddings. Furthermore,\nFigure 4 shows that the 4th positional embedding\nof RoBERTa has four outliers, which include the\naforementioned 77th and 588th dimensions. We\nalso ﬁnd that, from the 4th position to the ﬁnal po-\nsition, the maximum element of 99.8% positional\nembeddings is the 588th element.\nDigging deeper, we observe similar patterns in\nthe Layer Normalization (LN, Ba et al. (2016)) pa-\nrameters of both models. Recall that LN has two\nlearnable parameters — gain (γ) and bias (β) —\nboth of which are 768-dimension vectors (in the\ncase of the base models). These are designed as\nan afﬁne transformation over dimension-wise nor-\nFigure 3: The ﬁrst positional embedding of BERT-base.\nFigure 4: The fourth positional embedding of\nRoBERTa-base.\nmalized vectors in order to, like most normaliza-\ntion strategies, improve their expressive ability and\nto aid in optimization. Every layer of BERT and\nRoBERTa applies separate LNs post-attention and\npre-output. For BERT, the 557th element of the γ\nvector is always among the top-6 largest values for\nthe ﬁrst ten layers’ ﬁrst LN. Speciﬁcally, it is the\nlargest value in the ﬁrst three layers. For RoBERTa,\nthe 588th element of the ﬁrst LN’s β vector is al-\nways among the top-2 largest values for all layers —\nit is largest in the ﬁrst ﬁve layers. Furthermore, the\n77th element of the second LN’sγare among the\ntop-7 largest values from the second to the tenth\nlayer.\nIt is reasonable to conclude that, after the vector\nnormalization performed by LN, the outliers\nobserved in the raw embeddings are lost. We\nhypothesize that these particular neurons are\nsomehow important to the network, such that they\nretained after scaling the normalized vectors by the\nafﬁne transformation involving γ and β. Indeed,\nwe observe that, in BERT, only the 1st position’s\nembedding has such an outlier. However, it is\nsubsequently observed in every layer and token\n5315\nafter the ﬁrst LN is applied. Since LayerNorm\nis trained globally and is not token speciﬁc, it\nhappens to rescale every vector such that the\npositional information is retained. We corroborate\nthis by observing that all vectors share the same γ.\nThis effectively guarantees the presence of outliers\nin the 1st layer, which are then propagated upward\nby means of the Transformer’s residual connection\n(He et al., 2015). Also, it is important to note that,\nin the case of BERT, the ﬁrst position’s embedding\nis directly tied to the requisite[CLS] token, which\nis prepended to all sequences as part of the MLM\ntraining objective. This has been recently noted to\naffect e.g. attention patterns, where much of the\nprobability mass is distributed to this particular\ntoken alone, despite it bearing the smallest norm\namong all other vectors in a given layer and head\n(Kobayashi et al., 2020).\nNeuron-level analysis In order to test the extent\nto which BERT and RoBERTa’s outliers are related\nto positional information, we employ a probing\ntechnique inspired by Durrani et al. (2020). First,\nwe train a linear probe W ∈RM×N without bias\nto predict the position of a contextualized vector\nin a sentence. In Durrani et al. (2020), the weights\nof the classiﬁer are employed as a proxy for select-\ning the most relevant neurons to the prediction. In\ndoing so, they assume that, the larger the absolute\nvalue of the weight, the more important the corre-\nsponding neuron. However, this method disregards\nthe magnitudes of the values of neurons, as a large\nweights do not necessarily imply that the neuron\nhas high contribution to the ﬁnal classiﬁcation re-\nsult. For example, if the value of a neuron is close\nto zero, a large weight also leads to a small contri-\nbution. In order to address this issue, we deﬁne the\ncontribution of theith neuron as c(i) =abs(wi∗vi)\nfor i= 1,2,3,...,n , where wi is the ith weight and\nvi is the ith neuron in the contextualized word vec-\ntor. We name C = [c(1),c(2),...,c (n)] as a contri-\nbution vector. If a neuron has a high contribution,\nthis means that this neuron is highly relevant to the\nﬁnal classiﬁcation result.\nWe train, validate, and test our probe on the\nsplits provided in the SST-2 dataset (as mentioned\nin §2, we surmise that any dataset would be ade-\nquate for demonstrating this). The linear probe is\na 768 ×300 matrix, which we train separately for\neach layer. Since all SST-2 sentences are shorter\nthan 300 tokens in length, we set M = 300. We\nuse a batch size of 128 and train for 10 epochs\nwith a categorical cross-entropy loss, optimized by\nAdam (Kingma and Ba, 2017).\nFigure 5a shows that, while it is possible to\ndecode positional information from the lowest\nthree layers with almost perfect accuracy, much\nof this information is gradually lost higher up\nin the model. Furthermore, it appears that the\nhigher layers of RoBERTa contain more positional\ninformation than BERT. Looking at Figure 5b,\nwe see that BERT’s outlier neuron has a higher\ncontribution in position prediction than the average\ncontribution of all neurons. We also ﬁnd that\nthe contribution values of the same neuron are\nthe highest in all layers. Combined with the\naforementioned pattern of the ﬁrst positional\nembedding, we can conclude that the 557th neuron\nis related to positional information. Likewise,\nfor RoBERTa, Figure 5c shows that the 77th\nand 588th neurons have the highest contribution\nfor position prediction. We also ﬁnd that the\ncontribution values of the588th neurons are always\nlargest for all layers, which implies that these neu-\nrons are likewise related to positional information.2\nRemoving positional embeddings In order to\nisolate the relation between outlier neurons and\npositional information, we pre-train two RoBERTa-\nbase models (with and without positional embed-\ndings) from scratch using Fairseq (Ott et al., 2019).\nOur pre-training data is the English Wikipedia Cor-\npus3, where we train for 200k steps with a batch\nsize of 256, optimized by Adam. All models share\nthe same hyper-parameters, which are listed in the\nAppendix C.1. We use four NVIDIA A100 GPUs\nto pre-train each model, costing about 35 hours per\nmodel.\nWe ﬁnd that, without the help of positional em-\nbeddings, the validation perplexity of RoBERTa-\nbase is very high at 354.0, which is in line\nwith Lee et al. (2019)’s observation that the self-\nattention mechanism of Transformer Encoder is\norder-invariant. In other words, the removal of PEs\nfrom RoBERTa-base makes it a bag-of-word model,\nwhose outputs do not contain any positional infor-\nmation. In contrast, the perplexity of RoBERTa\nequipped with standard positional embeddings is\nmuch lower at 4.3, which is likewise expected.\n2We also use heatmaps to show the contribution values in\nAppendix B.1.\n3We randomly select 158.4M sentences for training and\n50k sentences for validation.\n5316\n(a) Accuracy of position prediction.\n (b) The contribution value of BERT-\nbase’s outlier neuron on position predic-\ntion.\n(c) The contribution value of RoBERTa-\nbase’s outlier neurons on position predic-\ntion.\nIn examining outlier neurons, we employ the\nsame datasets detailed in §2. For the RoBERTa-\nbase model with PEs, we ﬁnd that the maximum\nelement of 82.56% of all vectors is the 81st dimen-\nsion4, similarly to our ﬁndings above. However, we\ndo not observe the presence of such outlier neurons\nin the RoBERTa-base model without PEs, which\nindicates that the outlier neurons are tied directly\nto positional information. Similar to §2, we display\nthe averaged subword vectors for each layer of our\nmodels in Appendix C.2, which also corroborate\nour results.\n4 Clipping the outliers\nIn §3, we demonstrated that outlier neurons are re-\nlated to positional information. In this section, we\ninvestigate the effects of zeroing out these dimen-\nsions in contextualized vectors, a process which we\nrefer to as clipping.\n4.1 Vector space geometry\nAnisotropy Ethayarajh (2019) observe that con-\ntextualized word vectors are anisotropic in all non-\ninput layers, which means that the average cosine\nsimilarity between uniformly randomly sampled\nwords is close to 1. To corroborate this ﬁnding, we\nrandomly sample 2000 sentences from the SST-2\ntraining set and create 1000 sentence-pairs. Then,\nwe randomly select a token in each sentence, dis-\ncarding all other tokens. This effectively sets the\ncorrespondence between the two sentences to two\ntokens instead. Following this, we compute the\ncosine similarity between these two tokens to mea-\nsure the anisotropy of contextualized vectors.\nIn the left plot of Figure 6, we can see that con-\ntextualized representations of BERT and RoBERTa\nare more anisotropic in higher layers. This is espe-\n4Different initializations make our models have different\noutlier dimensions.\ncially true for RoBERTa, where the average cosine\nsimilarity between random words is larger than 0.5\nafter the ﬁrst non-input layer. This implies that the\ninternal representations in BERT and RoBERTa\noccupy a narrow cone in the vector space.\nSince outlier neurons tend to be valued higher\nor lower than all other contextualized vector\ndimensions, we hypothesize that they are the main\nculprit behind the degree of observed anisotropy.\nTo verify our hypothesis, we clip BERT and\nRoBERTa’s outliers by setting each neuron’s value\nto zero. The left plot in Figure 6 shows that, after\nclipping the outliers, their vector spaces become\nclose to isotropic.\nSelf-similarity In addition to remarking upon the\nanisotropic characteristics of contextualized vector\nspaces, Ethayarajh (2019) introduce several mea-\nsures to gauge the extent of “contextualization” in-\nherent models. One such metric is self-similarity,\nwhich the authors employ to compare the similar-\nity of a word’s internal representations in different\ncontexts. Given a word wand ndifferent sentences\ns1,s2,...,s n which contain such word,fi\nl (w) is the\ninternal representation of w in sentence si in the\nlth layer. The average self-similarity of win the lth\nlayer is then deﬁned as:\nSelfSiml(w) =\n∑n\ni=1\n∑n\nj=i+1 cos\n(\nfi\nl (w),fj\nl (w)\n)\nn(n−1)\n(1)\nIntuitively, a self-similarity score of 1 indicates\nthat no contextualization is being performed by the\nmodel (e.g. static word embeddings), while a score\nof 0 implies that representations for a given word\nare maximally different given various contexts.\nTo investigate the effect of outlier neurons on a\nmodel’s self-similarity, we sample 1000 different\nwords from SST-2 training set, all of which appear\nat least in 10 different sentences. We then com-\n5317\nFigure 6: Left: anisotropy measurement of contextualized word vectors in BERT and RoBERTa before and after\nclipping the outlier dimensions. Right: self-similarity measurement of BERT and RoBERTa before and after\nclipping.\npute the average self-similarity of these words as\ncontextualized by BERT and RoBERTa — before\nand after clipping the outliers. To adjust for the\neffect of anisotropy, we subtract the self-similarity\nfrom each layer’s anisotropy measurement, as in\nEthayarajh (2019).\nThe right plot in Figure 6 shows that, similarly\nto the ﬁndings in (Ethayarajh, 2019), a word’s self-\nsimilarity is highest in the lower layers, but de-\ncreases in higher layers. Crucially, we also observe\nthat, after clipping the outlier dimensions, the self-\nsimilarity increases, indicating that vectors become\ncloser to each other in the contextualized space.\nThis bears some impact on studies attempting to\ncharacterize the vector spaces of models like BERT\nand RoBERTa, as it is clearly possible to overstate\nthe degree of “contextualization” without address-\ning the effect of positional artefacts.\n4.2 Word sense\nBearing in mind the ﬁndings of the previous sec-\ntion, we now turn to the question of word sense, as\ncaptured by contextualized embeddings. Suppose\nthat we have a target wordw, which appears in two\nsentences. whas the same sense in these two sen-\ntences, but its contextualized representations are\nnot identical due to the word appearing in (perhaps\nslightly) different contexts. In the previous few sec-\ntions, we showed that outlier neurons are related\nto positional information and that clipping them\ncan make a word’s contextualized vectors more\nsimilar. Here, we hypothesize that clipping such\ndimensions can likewise aid in intrinsic semantic\ntasks, like differentiating senses of a word.\nTo test our hypothesis, we analyze contextu-\nalized vectors using the word-in-context (WiC)\ndataset (Pilehvar and Camacho-Collados, 2019),\nwhich is designed to identify the meaning of words\nModel Layer Threshold Accuracy\nBaseline - - 50.0%\nBefore clipping\nBERT 7 0.7 67.5%\nRoBERTa 10 0.9 69.0%\nAfter clipping\nBERT-clip 10 0.5 68.4%\nRoBERTa-clip 11 0.6 69.9%\nTable 1: The best accuracy scores on WiC dataset.\nBold indicates that the best result increases after clip-\nping.\nin different contexts. WiC is a binary classiﬁcation\ntask, where, given a target word and two sentences\nwhich contain it, models must determine whether\nthe word has the same meaning across the two sen-\ntences.\nIn order to test how well we can identify differ-\nences in word senses using contextualized vectors,\nwe compute the cosine similarity between contex-\ntualized vectors of target words across pairs of sen-\ntences, as they appear in the WiC dataset. If the\nsimilarity value is larger than a speciﬁed threshold,\nwe assign the true label to the sentence pair; other-\nwise, we assign the false label. We use this method\nto compare the accuracy of BERT and RoBERTa\non WiC before and after clipping the outliers. Since\nthis method does not require any training, we test\nour models on the WiC training dataset.5 We com-\npare 9 different thresholds from 0.1 to 0.9, as well\nas a simple baseline model that assigns the true\nlabels to all samples.\nTable 1 shows that after clipping outliers, the\nbest accuracy scores of BERT and RoBERTa in-\ncrease about 1%.6 This indicates that these neurons\n5The WiC test set does not provide labels and the size\nof validation set is too small (638 sentences pairs). We thus\nchoose to use the training dataset (5428 sentences pairs).\n6The thresholds are different due to the fact that the cosine\n5318\nDataset STS-B SICK-R STS-12 STS-13 STS-14 STS-15 STS-16\nBaseline\nAvg. GloVe 58.02 53.76 55.14 70.66 59.73 68.25 63.66\nBefore clipping\nBERT 58.61(3) 60.78(2) 48.00(1) 61.19(12) 50.10(12) 61.15(1) 62.38(12)\nRoBERTa 56.60(11) 64.68(11) 40.00(1) 58.33(11) 49.79(8) 64.39(9) 64.82(11)\nAfter clipping\nBERT-clip 63.06(2) 61.74(2) 50.40(1) 61.44(1) 54.52(2) 67.00(2) 64.18(2)\nRoBERTa-clip 60.61(11) 64.82(11) 43.44(1) 59.72(11) 51.92(3) 66.15(3) 67.14(11)\nTable 2: Experimental results on semantic textual similarity, where the baselines results are published in Reimers\nand Gurevych (2019). We show the best Spearman rank correlation between sentence embeddings’ cosine simi-\nlarity and the golden labels. The results are reported as r×100. The number in the parenthesis denotes that this\nresult belongs to the speciﬁc layer. Bold indicates that the best result increases after clipping.\nare less related to word sense information and can\nbe safely clipped for this particular task (if per-\nformed in an unsupervised fashion).\n4.3 Sentence embedding\nVenturing beyond the word-level, we also hypothe-\nsize that outlier clipping can lead to better sentence\nembeddings when relying on the cosine similar-\nity metric. To test this, we follow Reimers and\nGurevych (2019) in evaluating our models on 7\nsemantic textual similarity (STS) datasets, includ-\ning the STS-B benchmark (STS-B) (Cer et al.,\n2017), the SICK-Relatedness (SICK-R) dataset\n(Bentivogli et al., 2016) and the STS tasks 2012-\n2016 (Agirre et al., 2012, 2013, 2014, 2015, 2016).\nEach sentence pair in these datasets is annotated\nwith a relatedness score on a 5-point rating scale,\nas obtained from human judgments. We load each\ndataset using the SentEval toolkit (Conneau and\nKiela, 2018).\nIndeed, the most common approach for com-\nputing sentence embeddings from contextualized\nmodels is simply averaging all subword vectors that\ncomprise a given sentence (Reimers and Gurevych,\n2019). We follow this method in obtaining embed-\ndings for each pair of sentences in the aforemen-\ntioned tasks, between which we compute the cosine\nsimilarity. Given a set of similarity and gold relat-\nedness scores, we then calculate the Spearman rank\ncorrelation. As a comparison, we also consider\naveraged GloVe embeddings as our baseline.\nTable 2 shows that, after clipping the outliers, the\nbest Spearman rank correlation scores for BERT\nand RoBERTa increase across all datasets, some\nby a large margin. This indicates that clipping the\noutlier neurons can lead to better sentence embed-\ndings when mean pooling. However, like Li et al.\nsimilarity is inﬂated in the presence of outlier neurons.\nModel SST-2 IMDB SST-5\nBefore clipping\nBERT 85.9%(12) 86.8%(10) 46.2%(10)\nRoBERTa 88.4%(8) 91.5%(9) 46.9%(7)\nAfter clipping\nBERT-clip 85.4%(12) 86.4%(10) 46.1%(12)\nRoBERTa-clip 88.7%(8) 91.6%(9) 47.0%(7)\nTable 3: The best accuracy scores on different super-\nvised tasks. The number in the parenthesis denotes that\nthis result belongs to the speciﬁc layer.\n(2020b), we also notice that averaged GloVe em-\nbeddings still manage outperform both BERT and\nRoBERTa on all STS 2012-16 tasks. This implies\nthat the post-clipping reduction in anisotropy is\nonly a partial explanation for why contextualized,\nmean-pooled sentence embeddings still lag behind\nstatic word embeddings in capturing the semantics\nof a given sentence.\n4.4 Supervised tasks\nIn the previous sections, we analyzed the effects of\nclipping outlier neurons on various intrinsic seman-\ntic tasks. Here, we explore the effects of clipping in\na supervised scenario, where we hypothesize that\na model will learn to discard outlier information\nif it is not needed for a given task. We consider\ntwo binary classiﬁcation tasks, SST-2 and IMDB\n(Maas et al., 2011), and a multi-class classiﬁcation\ntask, SST-5, which is a 5-class version of SST-2.\nFirst, we freeze all the parameters of the pre-trained\nmodels and use the same method in §4.3 to get the\nsentence embedding of each sentence. Then, we\ntrain a simple linear classiﬁer W ∈R768×N for\neach layer, where N is the number of classes. We\nuse different batch sizes for different tasks, 768 for\nSST-2, 128 for IMDB and 1536 for SST-5. Then we\ntrain for 10 epochs with a categorical cross-entropy\nloss, optimized by Adam.\n5319\nTable 3 shows that there is little difference in\nemploying raw vs. clipped vectors in terms of task\nperformance. This indicates that using vectors with\nclipped outliers does not drastically affect classiﬁer\naccuracy when it comes to these common tasks.\n5 Discussion\nThe experiments detailed in the previous sections\npoint to the dangers of relying on metrics like co-\nsine similarity when making observations about\nmodels’ representational spaces. This is particu-\nlarly salient when the vectors being compared are\ntaken off-the-shelf and their composition is not\nwidely understood. Given the presence of model\nidiosyncracies like the outliers highlighted here,\nmean-sensitive, L2 normalized metrics (e.g. cosine\nsimilarity or Pearson correlation) will inevitably\nweigh the comparison of vectors along the highest-\nvalued dimensions. In the case of positional arte-\nfacts propagating through the BERT and RoBERTa\nnetworks, the basis of comparison is inevitably\nsteered towards whatever information is captured in\nthose dimensions. Furthermore, since such outlier\nvalues show little variance across vectors, proxy\nmetrics of anisotropy like measuring the average\ncosine similarity across random words (detailed\nin §4.1) will inevitably return an exceedingly high\nsimilarity, no matter what the context. When cosine\nsimilarity is viewed primarily as means of seman-\ntic comparison between word or sentence vectors,\nthe prospect of calculating cosine similarity for\na benchmark like WiC or STS-B becomes erro-\nneous. Though an examination of distance metrics\nis outside the scope of this study, we acknowledge\nsimilar points as having been addressed in regards\nto static word embeddings (Mimno and Thomp-\nson, 2017) as well as contextualized ones (Li et al.,\n2020b). Likewise, we would like to stress that our\nmanual clipping operation was performed for il-\nlustrative purposes and that interested researchers\nshould employ more systematic post-hoc normal-\nization strategies, e.g. whitening (Su et al., 2021),\nwhen working with hidden states directly.\nRelatedly, the anisotropic nature of the vector\nspace that persists even after clipping the outliers\nsuggests that positional artefacts are simply part of\nthe explanation. Per this point, Gao et al. (2019)\nprove that, in training any sort of model with likeli-\nhood loss, the representations learned for tokens be-\ning predicted will be naturally be pushed away from\nmost other tokens in order to achieve a higher like-\nlihood. They relate this observation to the Zipﬁan\nnature of word distributions, where the vast major-\nity of words are infrequent. Li et al. (2020a) extend\nthis insight speciﬁcally to BERT and show that,\nwhile high frequency words concentrate densely,\nlow frequency words are much more sparsely dis-\ntributed. Though we do not attempt to dispute\nthese claims with our ﬁndings, we do hope our\nexperiments will highlight the important role that\npositional embeddings play in the representational\ngeometry of Transformer-based models. Indeed, re-\ncent work has demonstrated that employing relative\npositional embeddings and untying them from the\nsimultaneously learned word embeddings has lead\nto impressive gains for BERT-based architectures\nacross common benchmarks (He et al., 2020; Ke\net al., 2020). It remains to be seen how such pro-\ncedures affect the representations of such models,\nhowever.\nBeyond this, it is clear that LayerNorm is the\nreason positional artefacts propagate though model\nrepresentations in the ﬁrst place. Indeed, our exper-\niments show that the outlier dimension observed for\nBERT is tied directly to the [CLS] token, which\nalways occurs at the requisite 1st position —- de-\nspite having no linguistic bearing on the sequence\nof observed tokens being modeled. However, the\nfact that RoBERTa (which employs a similar de-\nlimiter) retains outliers originating from different\npositions’ embeddings implies that the issue of\nartefact propagation is not simply a relic of task\ndesign. It is possible that whatever positional id-\niosyncrasies contribute to a task’s loss are likewise\nretained in their respective embeddings. In the case\nof BERT, the outlier dimension may be granted a\nlarge negative weight in order to differentiate the\n(privileged) 1st position between all others. This\ninformation being reconstructed by the LayerNorm\nparameters, which are shared for all positions in the\nsequence length, and then propagated up through\nthe Transformer network is a phenomenon worthy\nof further attention.\n6 Related work\nIn recent years, an explosion of work focused on\nunderstanding the inner workings of pretrained neu-\nral language models has emerged. One line of\nsuch work investigates the self-attention mecha-\nnism of Transformer-based models, aiming to e.g.\ncharacterize its patterns or decode syntactic struc-\nture (Raganato and Tiedemann, 2018; Vig, 2019;\n5320\nMareˇcek and Rosa, 2018; V oita et al., 2019; Clark\net al., 2019; Kobayashi et al., 2020). Another line\nof work analyzes models’ internal representations\nusing probes. These are often linear classiﬁers that\ntake representations as input and are trained with\nsupervised tasks in mind, e.g. POS-tagging, de-\npendency parsing (Tenney et al., 2019; Liu et al.,\n2019a; Lin et al., 2019; Hewitt and Manning, 2019;\nZhao et al., 2020). In such work, high probing\naccuracies are often likened to a particular model\nhaving “learned” the task in question.\nMost similar to our work, Ethayarajh (2019) in-\nvestigate the extent of “contextualization” in mod-\nels like BERT, ELMo, and GPT-2 (Radford et al.,\n2019). Mainly, they demonstrate that the contextu-\nalized vectors of all words are non-isotropic across\nall models and layers. However, they do not indi-\ncate why these models have such properties. Also\nrelevant are the studies of Dalvi et al. (2018), who\nintroduce a neuron-level analysis method, and Dur-\nrani et al. (2020), who use this method to analyze\nindividual neurons in contextualized word vectors.\nSimilarly to our experiment, Durrani et al. (2020)\ntrain a linear probe to predict linguistic information\nstored in a vector. They then employ the weights\nof the classiﬁer as a proxy to select the most rele-\nvant neurons to a particular task. In a similar vein,\nCoenen et al. (2019) demonstrate the existence of\nsyntactic and semantic subspaces in BERT repre-\nsentations.\n7 Conclusion\nIn this paper, we called attention to sets of out-\nlier neurons that appear in BERT and RoBERTa’s\ninternal representations, which bear consistently\nlarge values when compared to the distribution of\nvalues of all other neurons. In investigating the\norigin of these outliers, we employed a neuron-\nlevel analysis method which revealed that they are\nartefacts derived from positional embeddings and\nLayer Normalization. Furthermore, we found that\noutliers are a major cause for the anisotrophy of\na model’s vector space (Ethayarajh, 2019). Clip-\nping them, consequently, can make the vector space\nmore directionally uniform and increase the similar-\nity between words’ contextual representations. In\naddition, we showed that outliers can distort results\nwhen investigating word sense within contextual-\nized representations as well as obtaining sentence\nembeddings via mean pooling, where removing\nthem leads to uniformly better results. Lastly, we\nﬁnd that “clipping” does not affect models’ perfor-\nmance on three supervised tasks.\nIt is important to note that the exact dimensions\nat which the outliers occur will vary pending dif-\nferent initializations and training procedures (as\nevidenced by our own RoBERTa model). As such,\nfuture work will aim at investigating strategies for\nmitigating the propagation of these artefacts when\npretraining. Furthermore, given that both BERT\nand RoBERTa are masked language models, it will\nbe interesting to investigate whether or not similar\nartefacts occur in e.g. autoregressive models like\nGPT-2 (Radford et al., 2019) or XLNet (Yang et al.,\n2019). Per the insights of Gao et al. (2019), it is\nvery likely that the representational spaces of such\nmodels are anisotropic, but it is important to gauge\nthe extent to which this can be traced to positional\nartefacts.\nAuthors’ Note We would like to mention Koval-\neva et al. (2021)’s contemporaneous work, which\nlikewise draws attention to BERT’s outlier neurons.\nWhile our discussion situates outliers in the con-\ntext of positional embeddings and vector spaces,\nKovaleva et al. (2021) offer an exhaustive analy-\nsis of LayerNorm parameterization and its impact\non masked language modeling and ﬁnetuning. We\nrefer the interested reader to that work for a thor-\nough discussion of LayerNorm’s role in the outlier\nneuron phenomenon.\nAcknowledgments We would like to thank\nJoakim Nivre and Daniel Dakota for fruitful dis-\ncussions and the anonymous reviewers for their\nexcellent feedback.\nReferences\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, I˜nigo Lopez-Gazpio, Montse Maritxalar, Rada\nMihalcea, German Rigau, Larraitz Uria, and Janyce\nWiebe. 2015. SemEval-2015 task 2: Semantic tex-\ntual similarity, English, Spanish and pilot on inter-\npretability. In Proceedings of the 9th International\nWorkshop on Semantic Evaluation (SemEval 2015),\npages 252–263, Denver, Colorado. Association for\nComputational Linguistics.\nEneko Agirre, Carmen Banea, Claire Cardie, Daniel\nCer, Mona Diab, Aitor Gonzalez-Agirre, Weiwei\nGuo, Rada Mihalcea, German Rigau, and Janyce\nWiebe. 2014. SemEval-2014 task 10: Multilingual\nsemantic textual similarity. In Proceedings of the\n8th International Workshop on Semantic Evaluation\n5321\n(SemEval 2014), pages 81–91, Dublin, Ireland. As-\nsociation for Computational Linguistics.\nEneko Agirre, Carmen Banea, Daniel Cer, Mona Diab,\nAitor Gonzalez-Agirre, Rada Mihalcea, German\nRigau, and Janyce Wiebe. 2016. SemEval-2016\ntask 1: Semantic textual similarity, monolingual\nand cross-lingual evaluation. In Proceedings of the\n10th International Workshop on Semantic Evalua-\ntion (SemEval-2016), pages 497–511, San Diego,\nCalifornia. Association for Computational Linguis-\ntics.\nEneko Agirre, Daniel Cer, Mona Diab, and Aitor\nGonzalez-Agirre. 2012. SemEval-2012 task 6: A\npilot on semantic textual similarity. In *SEM 2012:\nThe First Joint Conference on Lexical and Compu-\ntational Semantics – Volume 1: Proceedings of the\nmain conference and the shared task, and Volume\n2: Proceedings of the Sixth International Workshop\non Semantic Evaluation (SemEval 2012), pages 385–\n393, Montr ´eal, Canada. Association for Computa-\ntional Linguistics.\nEneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-\nAgirre, and Weiwei Guo. 2013. *SEM 2013 shared\ntask: Semantic textual similarity. In Second Joint\nConference on Lexical and Computational Seman-\ntics (*SEM), Volume 1: Proceedings of the Main\nConference and the Shared Task: Semantic Textual\nSimilarity, pages 32–43, Atlanta, Georgia, USA. As-\nsociation for Computational Linguistics.\nAfra Alishahi, Yonatan Belinkov, Grzegorz Chrupała,\nDieuwke Hupkes, Yuval Pinter, and Hassan Saj-\njad, editors. 2020. Proceedings of the Third Black-\nboxNLP Workshop on Analyzing and Interpreting\nNeural Networks for NLP. Association for Compu-\ntational Linguistics, Online.\nJimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hin-\nton. 2016. Layer normalization.\nLuisa Bentivogli, Raffaella Bernardi, Marco Marelli,\nStefano Menini, Marco Baroni, and Roberto Zam-\nparelli. 2016. Sick through the semeval glasses: Les-\nson learned from the evaluation of compositional\ndistributional semantic models on full sentences\nthrough semantic relatedness and textual entailment.\nLanguage Resources and Evaluation, 50.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nDaniel Cer, Mona Diab, Eneko Agirre, I ˜nigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1–14, Vancouver,\nCanada. Association for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of BERT’s attention. In Pro-\nceedings of the 2019 ACL Workshop BlackboxNLP:\nAnalyzing and Interpreting Neural Networks for\nNLP, pages 276–286, Florence, Italy. Association\nfor Computational Linguistics.\nAndy Coenen, Emily Reif, Ann Yuan, Been Kim,\nAdam Pearce, Fernanda Vi´egas, and Martin Watten-\nberg. 2019. Visualizing and measuring the geometry\nof bert. arXiv preprint arXiv:1906.02715.\nAlexis Conneau and Douwe Kiela. 2018. SentEval: An\nevaluation toolkit for universal sentence representa-\ntions. In Proceedings of the Eleventh International\nConference on Language Resources and Evaluation\n(LREC 2018), Miyazaki, Japan. European Language\nResources Association (ELRA).\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, Anthony Bau, and James Glass. 2018.\nWhat is one grain of sand in the desert? analyzing\nindividual neurons in deep nlp models.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nNadir Durrani, Hassan Sajjad, Fahim Dalvi, and\nYonatan Belinkov. 2020. Analyzing individual neu-\nrons in pre-trained language models. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP), pages\n4865–4880, Online. Association for Computational\nLinguistics.\nKawin Ethayarajh. 2019. How contextual are contex-\ntualized word representations? comparing the geom-\netry of BERT, ELMo, and GPT-2 embeddings. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 55–65,\nHong Kong, China. Association for Computational\nLinguistics.\nJun Gao, Di He, Xu Tan, Tao Qin, Liwei Wang, and Tie-\nYan Liu. 2019. Representation degeneration prob-\nlem in training natural language generation models.\narXiv preprint arXiv:1907.12009.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Deep residual learning for image recog-\nnition.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2020. Deberta: Decoding-enhanced\nbert with disentangled attention. arXiv preprint\narXiv:2006.03654.\n5322\nJohn Hewitt and Christopher D. Manning. 2019. A\nstructural probe for ﬁnding syntax in word repre-\nsentations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4129–4138, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMandar Joshi, Omer Levy, Luke Zettlemoyer, and\nDaniel Weld. 2019. BERT for coreference reso-\nlution: Baselines and analysis. In Proceedings of\nthe 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5803–5808, Hong Kong,\nChina. Association for Computational Linguistics.\nGuolin Ke, Di He, and Tie-Yan Liu. 2020. Rethink-\ning the positional encoding in language pre-training.\narXiv preprint arXiv:2006.15595.\nDiederik P. Kingma and Jimmy Ba. 2017. Adam: A\nmethod for stochastic optimization.\nNikita Kitaev and Dan Klein. 2018. Constituency pars-\ning with a self-attentive encoder. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 2676–2686, Melbourne, Australia. Associa-\ntion for Computational Linguistics.\nDevorah E. Klein and Gregory L. Murphy. 2001. The\nrepresentation of polysemous words. Journal of\nMemory and Language, 45(2):259 – 282.\nGoro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and\nKentaro Inui. 2020. Attention is not only a weight:\nAnalyzing transformers with vector norms. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7057–7075, Online. Association for Computa-\ntional Linguistics.\nOlga Kovaleva, Saurabh Kulshreshtha, Anna Rogers,\nand Anna Rumshisky. 2021. Bert busters: Outlier\nlayernorm dimensions that disrupt bert.\nJuho Lee, Yoonho Lee, Jungtaek Kim, Adam Ko-\nsiorek, Seungjin Choi, and Yee Whye Teh. 2019.\nSet transformer: A framework for attention-based\npermutation-invariant neural networks. In Proceed-\nings of the 36th International Conference on Ma-\nchine Learning, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 3744–3753. PMLR.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020a. On the sen-\ntence embeddings from pre-trained language models.\narXiv preprint arXiv:2011.05864.\nBohan Li, Hao Zhou, Junxian He, Mingxuan Wang,\nYiming Yang, and Lei Li. 2020b. On the sentence\nembeddings from pre-trained language models. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 9119–9130, Online. Association for Computa-\ntional Linguistics.\nYongjie Lin, Yi Chern Tan, and Robert Frank. 2019.\nOpen sesame: Getting inside bert’s linguistic knowl-\nedge.\nTal Linzen, Grzegorz Chrupała, and Afra Alishahi, ed-\nitors. 2018. Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP. Association for Computa-\ntional Linguistics, Brussels, Belgium.\nTal Linzen, Grzegorz Chrupała, Yonatan Belinkov, and\nDieuwke Hupkes, editors. 2019. Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP. Association\nfor Computational Linguistics, Florence, Italy.\nNelson F. Liu, Matt Gardner, Yonatan Belinkov,\nMatthew E. Peters, and Noah A. Smith. 2019a. Lin-\nguistic knowledge and transferability of contextual\nrepresentations. In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1073–1094, Minneapolis, Minnesota.\nAssociation for Computational Linguistics.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising\npre-training for neural machine translation.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analy-\nsis. In Proceedings of the 49th Annual Meeting of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 142–150, Port-\nland, Oregon, USA. Association for Computational\nLinguistics.\nDavid Mareˇcek and Rudolf Rosa. 2018. Extracting syn-\ntactic trees from transformer encoder self-attentions.\nIn Proceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 347–349, Brussels, Belgium.\nAssociation for Computational Linguistics.\nTomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey\nDean. 2013. Efﬁcient estimation of word represen-\ntations in vector space.\nDavid Mimno and Laure Thompson. 2017. The strange\ngeometry of skip-gram with negative sampling. In\nEmpirical Methods in Natural Language Process-\ning.\n5323\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nJeffrey Pennington, Richard Socher, and Christopher\nManning. 2014. GloVe: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing (EMNLP) , pages 1532–1543, Doha,\nQatar. Association for Computational Linguistics.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long Papers), pages\n2227–2237, New Orleans, Louisiana. Association\nfor Computational Linguistics.\nMohammad Taher Pilehvar and Jose Camacho-\nCollados. 2019. WiC: the word-in-context dataset\nfor evaluating context-sensitive meaning represen-\ntations. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 1267–1273, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nAlessandro Raganato and J ¨org Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n287–297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for\nComputational Linguistics.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2020. Distilbert, a distilled version of\nbert: smaller, faster, cheaper and lighter.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631–1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nJianlin Su, Jiarun Cao, Weijie Liu, and Yangyiwen Ou.\n2021. Whitening sentence representations for bet-\nter semantics and faster retrieval. arXiv preprint\narXiv:2103.15316.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Sam Bowman, Dipanjan Das,\nand Ellie Pavlick. 2019. What do you learn from\ncontext? probing for sentence structure in contextu-\nalized word representations. In International Con-\nference on Learning Representations.\nJesse Vig. 2019. Visualizing attention in transformer-\nbased language representation models.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural\nInformation Processing Systems, volume 32, pages\n5753–5763. Curran Associates, Inc.\nMengjie Zhao, Philipp Dufter, Yadollah\nYaghoobzadeh, and Hinrich Sch¨utze. 2020. Quanti-\nfying the contextualization of word representations\nwith semantic class probing.\n5324\nFigure 7: Average vectors for each layer of BERT-\ndistil.\nFigure 8: Average vectors for each layer of RoBERTa-\ndistil.\nA Outliers of distilled and large models\nFor BERT-distil, Figure 7 shows the patterns of\nBERT-distil across all layers. The 557th element\nis an outlier. For RoBERTa-distil, Figure 8 shows\nthe patterns of RoBERTa-distil across all layers.\nthe 77th and 588th elements are two outliers. For\nBERT-large, Figure 9 shows the patterns of BERT-\nlarge across all layers. From the ﬁrst layer to the\ntenth layer, the 896th element is an outlier. From\nthe tenth layer to the seventeenth layer, the 678th\nelement is an outlier. From the sixteenth layer to\nthe nineteenth layer, the122nd element is an outlier.\nFrom the nineteenth layer to the twenty-third layer,\nthe 928th element is an outlier. The ﬁnal layer\ndoes not have outliers. For RoBERTa-large, Figure\nFigure 9: Average vectors for each layer of BERT-\nlarge.\nFigure 10: Average vectors for each layer of RoBERTa-\nlarge.\n10 shows the patterns of RoBERTa-large across\nall layers. From the ﬁrst layer to the twenty-third\nlayer, the 673rd element is an outlier. From the\nﬁfteenth layer to the ﬁnal layer, the 631st element\nis an outlier. From the ﬁrst layer to the sixth layer,\nthe 981st element is an outlier.\nB Neuron-level analysis\nB.1 Heatmaps of base models\nFigure 11 and 12 show the heatmaps of the outlier\nneurons and the highest non-outlier contribution\nvalues.\nB.2 Distilled and large models\nFigure 13 show the accuracy scores of position\nprediction of distilled and large models.\nDistil-models Figure 14 shows the contribution\nvalue of distilled models’ outlier neurons on\nposition prediction.\nLarge-models Figure 15 shows the contribution\nvalue of large models’ outlier neurons on position\nprediction.\nC Our Pre-training Models\nC.1 Hyper-parameters\nTable 4 shows the hyper-parameters of pre-training\nour RoBERTa-base models.\n5325\nFigure 11: Up: contribution values heatmap of the out-\nlier neuron of BERT-base. Down: the highest non-\noutlier contribution value of BERT-base.\nFigure 12: Up: contribution values heatmap of the 77th\ndimension of RoBERTa-base. Mid: contribution val-\nues heatmap of the 588th dimension of RoBERTa-base.\nDown: the highest non-outlier contribution value of\nRoBERTa-base.\nFigure 13: Up: accuracy of position prediction of dis-\ntilled models. Down: accuracy of position prediction\nof large models.\nFigure 14: The contribution value of distilled models’\noutlier neurons on position prediction.\n5326\nFigure 15: The contribution value of large models out-\nlier neurons on position prediction.\nHyper-parameter Our RoBERTa-base\nNumber of Layers 12\nHidden size 768\nFNN inner hidden size 3072\nAttention Heads 12\nAttention Head size 64\nDropout 0.1\nWarmup Steps 10k\nMax Steps 200k\nLearning Rates 1e-4\nBatch Size 256\nWeight Decay 0.01\nLearning Rate Decay Polynomial\nAdam (ϵ, β1, β2) (1e-6, 0.9, 0.98)\nGradient Clipping 0.5\nTable 4: Hyper-parameters for pre-training our\nRoBERTa-base models.\nFigure 16: Average vectors for each layer of our\nRoBERTa-base w/ or w/o PE.\nC.2 Average subword vectors\nFigure 16 show the average vectors for each of our\nmodels.\nD Clipping the outliers\nD.1 Geometry of vector space\nDistil-models Figure 17 shows the anisotropic\nmeasurement of distilled models and the self-\nsimilarity measurement of distilled models.\nLarge-models Figure 18 shows the anisotropic\nmeasurement of large models and Figure 19 shows\nthe self-similarity measurement of large models.\nWe “clip” different outlier neurons in different lay-\ners. For BERT-large, we zero-out the 896th neuron\nfrom the ﬁrst layer to the tenth layer, the 678th\nneuron from the tenth layer to the seventeenth\nlayer, the 122nd neuron from the sixteenth layer\nto the nineteenth layer and the 928th neuron from\nthe nineteenth layer to the twenty-third layer. For\nRoBERTa-large, we zero-out the673rd neuron for\nall non-input layers, the 981st neuron for the ﬁrst 9\nlayers and the 631st neuron for the last 10 layers.\n5327\nFigure 17: Up: average cosine similarity between ran-\ndom words of distil-models. Down: self-similarity\nmeasurement of BERT-distil and RoBERTa-distil (ad-\njusted by anisotropy) before and after “clipping the out-\nliers”.\nFigure 18: Average cosine similarity between random\nwords of large-models.\nFigure 19: Self-similarity measurement of BERT-large\nand RoBERTa-large (adjusted by anisotropy) before\nand after “clipping the outliers”.\nModel Layer Threshold Acc.\nBaseline - - 50.0%\nBefore clipping\nBERT-distil 5 0.9 66.5%\nRoBERTa-distil 5 0.9 63.7%\nBERT-large 12 0.7 70.2%\nRoBERTa-large 10 0.9 70.4%\nAfter clipping\nBERT-distil-clip 6 0.6 67.3%\nRoBERTa-distil-clip 5 0.6 66.7%\nBERT-large-clip 12 0.6 70.3%\nRoBERTa-large-clip 16 0.6 71.3%\nTable 5: The best accuracy scores on WiC dataset for\ndistilled and large models. Bold indicates that the best\nresult increases after clipping.\nDataset BERT\ndistil\nRoBERTa\ndistil\nBERT\ndistil\nclip\nRoBERTa\ndistil\nclip\nSTS-B 59.65(6) 56.06(5) 56.62(6) 58.47(5)\nSICK-R 62.64(6) 62.63(5) 62.42(6) 62.73(6)\nSTS-12 42.96(1) 40.19(1) 46.47(1) 42.36(1)\nSTS-13 59.33(1) 56.42(5) 55.74(1) 60.64(6)\nSTS-14 53.81(6) 49.59(6) 50.57(1) 52.51(2)\nSTS-15 61.40(6) 65.10(5) 61.48(1) 65.93(2)\nSTS-16 61.43(6) 62.90(5) 60.75(6) 64.49(5)\nTable 6: Experimental results on semantic textual simi-\nlarity of distilled models. The number in the parenthe-\nsis denotes that this result belongs to the speciﬁc layer.\nBold indicates that the best result increases after clip-\nping.\nD.2 Word sense\nTable 5 shows the accuracy scores of distill-models\nand large-models on WiC dataset before and after\n“clipping the outliers”.\nD.3 Sentence embedding\nTable 6 shows the results on semantic textual sim-\nilarity tasks of distilled models before and after\n“clipping the outliers”.\nTable 7 shows the results on semantic textual\nsimilarity tasks of large models before and after\n“clipping the outliers”.\nDataset BERT\nlarge\nRoBERTa\nlarge\nBERT\nlarge\nclip\nRoBERTa\nlarge\nclip\nSTS-B 62.56(1) 59.71(19) 66.43(3) 62.01(23)\nSICK-R 64.47(24) 63.08(14) 65.72(23) 63.50(16)\nSTS-12 54.05(1) 44.72(1) 56.44(3) 49.69(1)\nSTS-13 68.80(2) 61.68(8) 71.07(2) 62.82(10)\nSTS-14 60.46(1) 51.39(8) 63.35(1) 57.33(1)\nSTS-15 73.91(1) 65.98(7) 76.51(1) 69.71(1)\nSTS-16 66.35(17) 66.50(14) 71.41(3) 68.25(11)\nTable 7: Experimental results on semantic textual sim-\nilarity of large models. The number in the parenthe-\nsis denotes that this result belongs to the speciﬁc layer.\nBold indicates that the best result increases after clip-\nping.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7011620402336121
    },
    {
      "name": "Joint (building)",
      "score": 0.6072497367858887
    },
    {
      "name": "Natural language processing",
      "score": 0.5650608539581299
    },
    {
      "name": "Volume (thermodynamics)",
      "score": 0.5577857494354248
    },
    {
      "name": "Language model",
      "score": 0.49938273429870605
    },
    {
      "name": "Computational linguistics",
      "score": 0.4965975880622864
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44349756836891174
    },
    {
      "name": "Linguistics",
      "score": 0.43595850467681885
    },
    {
      "name": "Natural language",
      "score": 0.4272916913032532
    },
    {
      "name": "Engineering",
      "score": 0.1499183177947998
    },
    {
      "name": "Philosophy",
      "score": 0.07431873679161072
    },
    {
      "name": "Architectural engineering",
      "score": 0.06295424699783325
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I123387679",
      "name": "Uppsala University",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I4210091137",
      "name": "NetEase (China)",
      "country": "CN"
    }
  ],
  "cited_by": 22
}