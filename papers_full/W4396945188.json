{
  "title": "A Survey of Large Language Models for Graphs",
  "url": "https://openalex.org/W4396945188",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2380925004",
      "name": "Ren, Xubin",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A3034854067",
      "name": "Tang, Jiabin",
      "affiliations": [
        "University of Hong Kong"
      ]
    },
    {
      "id": "https://openalex.org/A2109288988",
      "name": "Yin Dawei",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2616285778",
      "name": "Chawla, Nitesh",
      "affiliations": [
        "University of Notre Dame"
      ]
    },
    {
      "id": "https://openalex.org/A2104312933",
      "name": "Huang Chao",
      "affiliations": [
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4393277515",
    "https://openalex.org/W4402670739",
    "https://openalex.org/W4322766882",
    "https://openalex.org/W3045200674",
    "https://openalex.org/W4392846385",
    "https://openalex.org/W2908230750",
    "https://openalex.org/W4379539462",
    "https://openalex.org/W4402727764",
    "https://openalex.org/W4391345293",
    "https://openalex.org/W2250844151",
    "https://openalex.org/W4390692489",
    "https://openalex.org/W2981852735",
    "https://openalex.org/W4396758712",
    "https://openalex.org/W4372279027",
    "https://openalex.org/W4393147025",
    "https://openalex.org/W4402683734",
    "https://openalex.org/W4393065402",
    "https://openalex.org/W4380136143",
    "https://openalex.org/W4392367398",
    "https://openalex.org/W4376864968",
    "https://openalex.org/W4380993339",
    "https://openalex.org/W2907492528",
    "https://openalex.org/W4385568277",
    "https://openalex.org/W4396722529"
  ],
  "abstract": "Graphs are an essential data structure utilized to represent relationships in\\nreal-world scenarios. Prior research has established that Graph Neural Networks\\n(GNNs) deliver impressive outcomes in graph-centric tasks, such as link\\nprediction and node classification. Despite these advancements, challenges like\\ndata sparsity and limited generalization capabilities continue to persist.\\nRecently, Large Language Models (LLMs) have gained attention in natural\\nlanguage processing. They excel in language comprehension and summarization.\\nIntegrating LLMs with graph learning techniques has attracted interest as a way\\nto enhance performance in graph learning tasks. In this survey, we conduct an\\nin-depth review of the latest state-of-the-art LLMs applied in graph learning\\nand introduce a novel taxonomy to categorize existing methods based on their\\nframework design. We detail four unique designs: i) GNNs as Prefix, ii) LLMs as\\nPrefix, iii) LLMs-Graphs Integration, and iv) LLMs-Only, highlighting key\\nmethodologies within each category. We explore the strengths and limitations of\\neach framework, and emphasize potential avenues for future research, including\\novercoming current integration challenges between LLMs and graph learning\\ntechniques, and venturing into new application areas. This survey aims to serve\\nas a valuable resource for researchers and practitioners eager to leverage\\nlarge language models in graph learning, and to inspire continued progress in\\nthis dynamic field. We consistently maintain the related open-source materials\\nat \\\\url{https://github.com/HKUDS/Awesome-LLM4Graph-Papers}.\\n",
  "full_text": "A Survey of Large Language Models for Graphs\nXubin Ren\nUniversity of Hong Kong\nHong Kong, China\nxubinrencs@gmail.com\nJiabin Tang\nUniversity of Hong Kong\nHong Kong, China\njiabintang77@gmail.com\nDawei Yin\nBaidu Inc.\nBeijing, China\nyindawei@acm.org\nNitesh Chawla\nUniversity of Notre Dame\nIndiana, USA\nnchawla@nd.edu\nChao Huang‚àó\nUniversity of Hong Kong\nHong Kong, China\nchaohuang75@gmail.com\nABSTRACT\nGraphs are an essential data structure utilized to represent rela-\ntionships in real-world scenarios. Prior research has established\nthat Graph Neural Networks (GNNs) deliver impressive outcomes\nin graph-centric tasks, such as link prediction and node classifica-\ntion. Despite these advancements, challenges like data sparsity and\nlimited generalization capabilities continue to persist. Recently,\nLarge Language Models (LLMs) have gained attention in natu-\nral language processing. They excel in language comprehension\nand summarization. Integrating LLMs with graph learning tech-\nniques has attracted interest as a way to enhance performance in\ngraph learning tasks. In this survey, we conduct an in-depth re-\nview of the latest state-of-the-art LLMs applied in graph learning\nand introduce a novel taxonomy to categorize existing methods\nbased on their framework design. We detail four unique designs:\ni) GNNs as Prefix, ii) LLMs as Prefix, iii) LLMs-Graphs Integra-\ntion, and iv) LLMs-Only, highlighting key methodologies within\neach category. We explore the strengths and limitations of each\nframework, and emphasize potential avenues for future research, in-\ncluding overcoming current integration challenges between LLMs\nand graph learning techniques, and venturing into new applica-\ntion areas. This survey aims to serve as a valuable resource for re-\nsearchers and practitioners eager to leverage large language models\nin graph learning, and to inspire continued progress in this dynamic\nfield. We consistently maintain the related open-source materials\nat https://github.com/HKUDS/Awesome-LLM4Graph-Papers.\nCCS CONCEPTS\n‚Ä¢ General and reference ‚ÜíSurveys and overviews ; ‚Ä¢ Informa-\ntion systems ‚ÜíData mining; Language models; ‚Ä¢ Mathemat-\nics of computing ‚ÜíGraph algorithms.\n‚àóCorresponding author\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\n¬© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0490-1/24/08.\nhttps://doi.org/10.1145/3637528.3671460\nKEYWORDS\nLarge Language Models, Graph Learning\nACM Reference Format:\nXubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, and Chao Huang. 2024.\nA Survey of Large Language Models for Graphs. In Proceedings of the 30th\nACM SIGKDD Conference on Knowledge Discovery and Data Mining (KDD\n‚Äô24), August 25‚Äì29, 2024, Barcelona, Spain. ACM, Barcelona, Spain, 11 pages.\nhttps://doi.org/10.1145/3637528.3671460\n1 INTRODUCTION\nGraphs, comprising nodes and edges that signify relationships, are\nessential for illustrating real-world connections across various do-\nmains. These include social networks [39, 52], molecular graphs [4],\nrecommender systems [23, 59], and academic networks [27]. This\nstructured data form is integral in mapping complex interconnec-\ntions relevant to a wide range of applications.\nIn recent years, Graph Neural Networks (GNNs) [79] have emerged\nas a powerful tool for a variety of tasks, including node classifica-\ntion [82] and link prediction [89]. By passing and aggregating infor-\nmation across nodes and iteratively refining node features through\nsupervised learning, GNNs have achieved remarkable results in\ncapturing structural nuances and enhancing model accuracy. To\naccomplish this, GNNs leverage graph labels to guide the learning\nprocess. Several notable models have been proposed in the litera-\nture, each with its own strengths and contributions. For instance,\nGraph Convolutional Networks (GCNs) [34] have been shown to\nbe effective in propagating embeddings across nodes, while Graph\nAttention Networks (GATs) [67] leverage attention mechanisms to\nperform precise aggregation of node features. Additionally, Graph\nTransformers [35, 86] employ self-attention and positional encod-\ning to capture global signals among the graph, further improving\nthe expressiveness of GNNs. To address scalability challenges in\nlarge graphs, methods such as Nodeformer [77] and DIFFormer [76]\nhave been proposed. These approaches employ efficient attention\nmechanisms and differentiable pooling techniques to reduce com-\nputational complexity while maintaining high levels of accuracy.\nDespite these advancements, current GNN methodologies still face\nseveral challenges. For example, data sparsity remains a signifi-\ncant issue, particularly in scenarios where the graph structure is\nincomplete or noisy [85]. Moreover, the generalization ability of\nGNNs to new graphs or unseen nodes remains an open research\nquestion, with recent works highlighting the need for more robust\nand adaptive models [17, 80, 93].\narXiv:2405.08011v3  [cs.LG]  11 Sep 2024\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang\nLarge Language Models (LLMs) [96], which show great gener-\nalization abilities for unseen tasks [ 12, 56, 72], have emerged as\npowerful tools in various research fields, including natural lan-\nguage processing [1], computer vision [ 43, 44], and information\nretrieval [26, 41, 99]. The advent of LLMs has sparked significant\ninterest within the graph learning community [29, 33, 37], prompt-\ning investigations into the potential of LLMs to enhance perfor-\nmance on graph-related tasks. Researchers have explored various\napproaches to leverage the strengths of LLMs for graph learning,\nresulting in a new wave of methods that combine the power of\nLLMs with graph neural networks. One promising direction is to\ndevelop prompts that enable LLMs to understand graph structures\nand respond to queries effectively. For instance, approaches such\nas InstructGLM [84] and NLGraph [68] have designed specialized\nprompts that allow LLMs to reason over graph data and generate\naccurate responses. Alternatively, other methods have integrated\nGNNs to feed tokens into the LLMs, allowing them to understand\ngraph structures more directly. For example, GraphGPT [63] and\nGraphLLM [5] use GNNs to encode graph data into tokens, which\nare then fed into the LLMs for further processing. This synergy\nbetween LLMs and GNNs has not only improved task performance\nbut also demonstrated impressive zero-shot generalization capabili-\nties, where the models can accurately answer queries about unseen\ngraphs or nodes.\nIn this survey, we offer a systematic review of the advancements\nin Large Language Models (LLMs) for graph applications, and we\nexplore potential avenues for future research. Unlike prior surveys\nthat categorize studies based on the role of LLMs [ 33, 37] or fo-\ncus primarily on integrating LLMs with knowledge graphs [ 53],\nour work highlights the model framework design, particularly the\ninference and training processes, to distinguish between existing\ntaxonomies. This perspective allows readers to gain a deeper under-\nstanding of how LLMs effectively address graph-related challenges.\nWe identify and discuss four distinct architectural approaches: i)\nGNNs as Prefix , ii) LLMs as Prefix , iii) LLMs-Graphs Integration , and\niv) LLMs-Only, each illustrated with representative examples. In\nsummary, the contributions of our work can be summarized as:\n‚Ä¢Comprehensive Review of LLMs for Graph Learning. We\noffer a comprehensive review of the current state-of-the-art Large\nLanguage Models (LLMs) for graph learning, elucidating their\nstrengths and pinpointing their limitations.\n‚Ä¢Novel Taxonomy for Categorizing Research. We introduce\na novel taxonomy for categorizing existing research based on\ntheir framework design, which provides a deeper insight into\nhow LLMs can be seamlessly integrated with graph learning.\n‚Ä¢Future Research Avenues. We also explore potential avenues\nfor future research, including addressing the prevalent challenges\nin merging LLMs with graph learning methods and venturing\ninto novel application areas.\n2 PRELIMINARIES AND TAXONOMY\nIn this section, we first provide essential background knowledge\non large language models and graph learning. Then, we present\nour taxonomy of large language models for graphs.\n2.1 Definitions\nGraph-Structured Data. In computer science, a graphG= (V, E)\nis a non-linear data structure that consists of a set of nodesV, and a\nset of edgesEconnecting these nodes. Each edgeùëí ‚ààE is associated\nwith a pair of nodes (ùë¢, ùë£), where ùë¢ and ùë£ are the endpoints of the\nedge. The edge may be directed, meaning it has a orientation from\nùë¢ to ùë£, or undirected, meaning it has no orientation. Furthermore,\nA Text-Attributed Graph (TAG) is a graph that assigns a sequential\ntext feature (i.e., sentence) to each node, denoted as tùë£, which is\nwidely used in the era of large language models. The text-attributed\ngraph can be formally represented as GùëÜ = (V, E, T), where Tis\nthe set of text features.\nGraph Neural Networks (GNNs) are deep learning architectures\nfor graph-structured data that aggregate information from neigh-\nboring nodes to update node embeddings. Formally, the update of a\nnode embedding hùë£ ‚ààRùëë in each GNN layer can be represented as:\nh(ùëô+1)\nùë£ = ùúì (ùúô ({h(ùëô )\nùë£‚Ä≤ : ùë£‚Ä≤‚ààN(ùë£)}), h(ùëô )\nùë£ ), (1)\nwhere ùë£‚Ä≤‚ààN(ùë£)denotes a neighbor node of ùë£, and ùúô (¬∑)and ùúì (¬∑)\nare aggregation and update functions, respectively. By stacking ùêø\nGNN layers, the final node embeddings can be used for downstream\ngraph-related tasks such as node classification and link prediction.\nLarge Language Models (LLMs) . Language Models (LMs) is a\nstatistical model that estimate the probability distribution of words\nfor a given sentence. Recent research has shown that LMs with bil-\nlions of parameters exhibit superior performance in solving a wide\nrange of natural language tasks (e.g., translation, summarization\nand instruction following), making them Large Language Models\n(LLMs). In general, most recent LLMs are built with transformer\nblocks that use a query-key-value (QKV)-based attention mecha-\nnism to aggregate information in the sequence of tokens. Based on\nthe direction of attention, LLMs can be categorized into two types\n(given a sequence of tokens x = [ùë•0, ùë•1, ..., ùë•ùëõ]):\n‚Ä¢Masked Language Modeling (MLM) . Masked Language Mod-\neling is a popular pre-training objective for LLMs that involves\nmasking out certain tokens in a sequence and training the model\nto predict the masked tokens based on the surrounding context.\nSpecifically, the model takes into account both the left and right\ncontext of the masked token to make accurate predictions:\nùëù(ùë•ùëñ |ùë•0, ùë•1, ..., ùë•ùëõ). (2)\nRepresentative models include BERT [12] and RoBERTa [47].\n‚Ä¢Causal Language Modeling (CLM) . Causal Language Modeling\nis another popular training objective for LLMs that involves\npredicting the next token in a sequence based on the previous\ntokens. Specifically, the model only considers the left context of\nthe current token to make accurate predictions:\nùëù(ùë•ùëñ |ùë•0, ùë•1, ..., ùë•ùëñ‚àí1) (3)\nNotable examples include the GPT (e.g., ChatGPT) and Llama [66].\n2.2 Taxonomy\nIn this survey, we present our taxonomy focusing on the model in-\nference pipeline that processes both graph data and text with LLMs.\nSpecifically, we summarize four main types of model architecture\ndesign for large language models for graphs, as follows:\nA Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nLarge Language Models for Graphs\nGNNs as Prefix\nNode-level Tokenization GprahGPT [63], HiGPT [64], GraphTranslator [88], UniGraph [25],\nGIMLET [92], XRec [51]\nGraph-level Tokenization GraphLLM [5], GIT-Mol [45], MolCA [48], InstructMol [4],\nG-Retriever [24], GNP [65]\nLLMs as Prefix\nEmbs. from LLMs for GNNs G-Prompt [30], SimTeG [14], GALM [81], OFA [42], TAPE [22], LLMRec [73]\nLabels from LLMs for GNNs OpenGraph [80], LLM-GNN [9], GraphEdit [21], RLMRec [58]\nLLMs-Graphs Integration\nAlignment between GNNs and LLMs MoMu [60], ConGraT [3], G2P2 [74], GRENADE [36], MoleculeSTM [46],\nTHLM [100], GLEM [94]\nFusion Training of GNNs and LLMs GreaseLM [90], DGTL [54], ENGINE [98], GraphAdapter [31]\nLLMs Agent for Graphs Pangu [19], Graph Agent [71], FUXI [18], Readi [10], RoG [49]\nLLMs-Only\nTuning-free NLGraph [68], GPT4Graph [20], Beyond Text [28], Graph-LLM [8], GraphText [95],\nTalk like a Graph [15], LLM4DyG [91], GraphTMI [11], Ai et al. [2]\nTuning-required InstructGLM [84], WalkLM [62], LLaGA [7], InstructGraph [69], ZeroG [38],\nGraphWiz [6], GraphInstruct [50], MuseGraph [61]\nFigure 1: The proposed taxonomy of Large Language Models (LLMs) for graphs, featuring representative works.\n‚Ä¢GNNs as Prefix . GNNs serve as the first component to process\ngraph data and provide structure-aware tokens (e.g., node-level,\nedge-level, or graph-level tokens) for LLMs for inference.\n‚Ä¢LLMs as Prefix . LLMs first process graph data with textual\ninformation and then provide node embeddings or generated\nlabels for improved training of graph neural networks.\n‚Ä¢LLMs-Graphs Integration. In this line, LLMs achieve a higher\nlevel of integration with graph data, such as fusion training or\nalignment with GNNs, and also build LLM-based agents to inter-\nact with graph information.\n‚Ä¢LLMs-Only. This line designs practical prompting methods to\nground graph-structured data into sequences of words for LLMs\nto infer, while some also incorporate multi-modal tokens.\n3 LARGE LANGUAGE MODELS FOR GRAPHS\n3.1 GNNs as Prefix\nIn this section, we discuss the application of graph neural networks\n(GNNs) as structural encoders to enhance the understanding of\ngraph structures by LLMs, thereby benefiting various downstream\ntasks, i.e., GNNs as Prefix . In these methods, GNNs generally play\nthe role of a tokenizer, encoding graph data into a graph token\nsequence rich in structural information, which is then input into\nLLMs to align with natural language. These methods can gener-\nally be divided into two categories: i) Node-level Tokenization: each\nnode of the graph structure is input into the LLM, aiming to make\nthe LLM understand fine-grained node-level structural informa-\ntion and distinguish relationships. ii) Graph-level Tokenization: the\ngraph is compressed into a fixed-length token sequence using a spe-\ncific pooling method, aiming to capture high-level global semantic\ninformation of the graph structure.\n3.1.1 Node-level Tokenization. For some downstream tasks in\ngraph learning, such as node classification and link prediction,\nthe model needs to model the fine-grained structural information\nat node level, and distinguish the semantic differences between\ndifferent nodes. Traditional GNNs usually encode a unique repre-\nsentation for each node based on the information of neighboring\nnodes, and directly perform downstream node classification or link\nprediction. In this line, the node-level tokenization method is uti-\nlized, which can retain the unique structural representation of each\nnode as much as possible, thereby benefiting downstream tasks.\nWithin this line, GraphGPT [63] proposes to initially align\nthe graph encoder with natural language semantics through text-\ngraph grounding, and then combine the trained graph encoder\nwith the LLM using a projector. Through the two-stage instruc-\ntion tuning paradigm, the model can directly complete various\ngraph learning downstream tasks with natural language, thus per-\nform strong zero-shot transferability and multi-task compatibil-\nity. The proposed Chain-of-Thought distillation method empowers\nGraphGPT to migrate to complex tasks with small parameter sizes.\nThen, HiGPT [64] proposes to combine the language-enhanced\nin-context heterogeneous graph tokenizer with LLMs, solving the\nchallenge of relation type heterogeneity shift between different\nheterogeneous graphs. Meanwhile, the two-stage heterogeneous\ngraph instruction-tuning injects both homogeneity and heterogene-\nity awareness into the LLM. And the Mixture-of-Thought (MoT)\nmethod combined with various prompt engineering further solves\nthe common data scarcity problem in heterogeneous graph learn-\ning. GIMLET [92], as a unified graph-text model, leverages natural\nlanguage instructions to address the label insufficiency challenge\nin molecule-related tasks, effectively alleviating the reliance on\nexpensive lab experiments for data annotation. It employs a gen-\neralized position embedding and attention mechanism to encode\nboth graph structures and textual instructions as a unified token\ncombination that is fed into a transformer decoder.GraphTransla-\ntor [88] proposes the use of a translator with shared self-attention\nto align both the target node and instruction, and employs cross\nattention to map the node representation encoded by the graph\nmodel to fixed-length semantic tokens. The proposed daul-phase\ntraining paradigm empowers the LLM to make predictions based\non language instructions, providing a unified solution for both\npre-defined and open-ended graph-based tasks. Instead of using\npre-computed node features of varying dimensions,UniGraph [25]\nleverages Text-Attributed Graphs for unifying node representations,\nfeaturing a cascaded architecture of language models and graph\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang\nGNNs\nGraphs\nWhat is this node?node\nLLMs\nThis node ‚Ä¶Node-level\nWhat is this graph?graph\nLLMs\nThis graph ‚Ä¶Graph-level\nFigure 2: GNNs as Prefix.\nneural networks as backbone networks. In recent research on rec-\nommendation systems, XRec [51] has been proposed as a method\nthat utilizes the encoded user/item embeddings from graph neural\nnetworks as collaborative signals. These signals are then integrated\ninto each layer of large language models, enabling the generation\nof explanations for recommendations, even in zero-shot scenarios.\n3.1.2 Graph-level Tokenization. On the other hand, to adapt\nto other graph-level tasks, models need to be able to extract global\ninformation from node representations, to obtain high-level graph\nsemantic tokens. In the method of GNN as Prefix , Graph-level tok-\nenization abstracts node representations into unified graph repre-\nsentations through various \"pooling\" operations, further enhancing\nvarious downstream tasks.\nWithin this domain, GraphLLM [5] utilizes a graph transformer\nthat incorporates the learnable query and positional encoding to en-\ncode the graph structure and obtain graph representations through\npooling. These representations are directly used as graph-enhanced\nprefix for prefix tuning in the LLM, demonstrating remarkable effec-\ntiveness in fundamental graph reasoning tasks. MolCA [48] with\nCross-Modal Projector and Uni-Modal Adapter is a method that\nenables a language model to understand both text- and graph-based\nmolecular contents through the proposed dual-stage pre-training\nand fine-tuning stage. It employs a cross-modal projector imple-\nmented as a Q-Former to connect a graph encoder‚Äôs representation\nspace and a language model‚Äôs text space, and a uni-modal adapter\nfor efficient adaptation to downstream tasks. InstructMol [4] in-\ntroduces a projector that aligns the molecular graph encoded by\nthe graph encoder with the molecule‚Äôs Sequential information and\nnatural language instructions, with the first stage of Alignment\nPretraining and the second stage of Task-specific Instruction Tun-\ning enabling the model to achieve excellent performance in vari-\nous drug discovery-related molecular tasks. GIT-Mol [45] further\nunifies the graph, text, and image modalities through interaction\ncross-attention between different modality encoders, and aligns\nthese three modalities, enabling the model to simultaneously per-\nform four downstream tasks: captioning, generation, recognition,\nand prediction. GNP [65] employs cross-modality pooling to in-\ntegrate the node representations encoded by the graph encoder\nwith the natural language tokens, resulting in a unified graph rep-\nresentation. This representation is aligned with the instruction\nthrough the LLM to demonstrate superiority in commonsense and\nbiomedical reasoning tasks. Recently, G-Retriever [24] utilizes\nretrieval-augmented techniques to obtain subgraph structures. It\ncompletes various downstream tasks in GraphQA (Graph Question\nAnswering) through the collaboration of graph encoder and LLMs.\n3.1.3 Discussion. The GNN as Prefix approach aligns the mod-\neling capability of GNNs with the semantic modeling capability\nof LLMs, demonstrating unprecedented generalization, i.e., zero-\nshot capability, in various graph learning downstream tasks and\nreal-world applications. However, despite the effectiveness of the\naforementioned approach, the challenge lies in whether the GNN\nas Prefix method remains effective for non-text-attributed graphs.\nAdditionally, the optimal coordination between the architecture\nand training of GNNs and LLMs remains an unresolved question.\n3.2 LLMs as Prefix\nThe methods presented in this section leverage the information\nproduced by large language models to improve the training of\ngraph neural networks. This information includes textual content,\nlabels, or embeddings derived from the large language models.\nThese techniques can be categorized into two distinct groups: i)\nEmbeddings from LLMs for GNNs , which involves using embeddings\ngenerated by large language models for graph neural networks, and\nii) Labels from LLMs for GNNs , which involves integrating labels\ngenerated by large language models for graph neural networks.\n3.2.1 Embeddings from LLMs for GNNs. The inference pro-\ncess of graph neural networks involves passing node embeddings\nthrough the edges and then aggregating them to obtain the next-\nlayer node embeddings. In this process, the initial node embeddings\nare diverse across different domains. For instance, ID-based em-\nbeddings in recommendation systems or bag-of-words embeddings\nin citation networks can be unclear and non-diverse. Sometimes,\nthe poor quality of initial node embeddings can result in subop-\ntimal performance of GNNs. Furthermore, the lack of a universal\ndesign for node embedders makes it challenging to address the\ngeneralization capability of GNNs in unseen tasks with different\nnode sets. Fortunately, the works in this line leverage the power-\nful language summarization and modeling capabilities of LLMs to\ngenerate meaningful and effective embeddings for GNNs‚Äô training.\nIn this domain, G-Prompt [30] adds a GNN layer at the end of\na pre-trained language models (PLMs) to achieve graph-aware fill-\nmasking self-supervised learning. By doing so, G-Prompt can gen-\nerate task-specific, explainable node embeddings for downstream\ntasks using prompt tuning. SimTeG [14] first leverages parameter-\nefficient fine-tuning on the text embeddings obtained by LLMs for\ndownstream tasks (e.g., node classification). Then, the node embed-\ndings are fed into GNNs for inference. Similarly,GALM [81] utilizes\nBERT as a pre-language model to encode text embeddings for each\nnode. Then, the model is pre-trained through unsupervised learning\ntasks, such as link prediction, to minimize empirical loss and find\noptimal model parameters, which enables GALM to be applied for\nvarious downstream tasks. Recently, OFA [42] leverages LLMs to\nunify graph data from different domains into a common embedding\nspace for cross-domain learning. It also uses LLMs to encode task-\nrelevant text descriptions for constructing prompt graphs, allowing\nthe model to perform specific tasks based on context. TAPE [22]\nuses customized prompts to query LLMs, generating both prediction\nand text explanation for each node. Then, DeBERTa is fine-tuned\nto convert the text explanations into node embeddings for GNNs.\nFinally, GNNs can use a combination of the original text features,\nexplanation features, and prediction features to predict node labels.\nA Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nTextual\nData\n‚äï\nGraphsLLMs Embedder\ninput\nEmbs. from LLMs\nfor GNNs\nLLMs Labeler\n Labels from LLMs\nfor GNNs\nLabelstrain\nFigure 3: LLMs as Prefix.\nIn the field of recommendation, LLMRec [73] achieves graph aug-\nmentation on user-item interaction data using GPT-3.5, which not\nonly filters out noise interactions and adds meaningful training\ndata, but also enriches the initial node embeddings for users and\nitems with generated rich textual profiles, ultimately improving the\nperformance of recommenders.\n3.2.2 Labels from LLMs for GNNs. Another approach leverages\nthe generated labels from large language models as supervision\nto improve the training of graph neural networks. Notably, the\nsupervised labels in this context are not limited to categorized\nlabels in classification tasks, but can take various forms such as\nembeddings, graphs, and more. The generated information from\nthe LLMs is not used as input to the GNNs, but rather forms the\nsupervision signals for better optimization, which enables GNNs\nto achieve higher performance on various graph-related tasks.\nFollow this line, OpenGraph [80] employs LLMs to generate\nnodes and edges, mitigating the issue of sparse training data. The\ngeneration process for nodes and edges is refined using the Gibbs\nsampling algorithm and a tree-of-prompt strategy, which is then\nutilized to train the graph foundation model. LLM-GNN [9] lever-\nages LLMs as annotators to generate node category predictions\nwith confidence scores, which serve as labels. Post-filtering is then\nemployed to filter out low-quality annotations while maintaining\nlabel diversity. Finally, the generated labels are used to train GNNs.\nGraphEdit [21] leverages the LLMs to build an edge predictor,\nwhich is used to evaluate and refine candidate edges against the\noriginal graph‚Äôs edges. In recommender systems, RLMRec [58]\nleverages LLMs to generate text descriptions of user/item prefer-\nences. These descriptions are then encoded as semantic embeddings\nto guide the representation learning of ID-based recommenders\nusing contrastive and generative learning techniques [57].\n3.2.3 Discussion. Despite the progress made by the aforemen-\ntioned methods in enhancing graph learning performance, a lim-\nitation persists in their decoupled nature, where LLMs are not\nco-trained with GNNs, resulting in a two-stage learning process.\nThis decoupling is often due to computational resource limitations\narising from the large size of the graph or the extensive parameters\nof LLMs. Consequently, the performance of the GNNs is heavily\ndependent on the pre-generated embeddings/labels of LLMs or even\nthe design of task-specific prompts.\n3.3 LLMs-Graphs Integration\nThe methods introduced in this section aim to further integrate\nlarge language models with graph data, encompassing various\nmethodologies that enhance not only the ability of LLMs to tackle\ngraph tasks but also the parameter learning of GNNs. These works\ncan be categorized into three types: i) Fusion Training of GNNs\nand LLMs , which aims to achieve fusion-co-training of the parame-\nters of both models; ii) Alignment between GNNs and LLMs , which\nfocuses on achieving representation or task alignment between\nthe two models; and iii) LLMs Agent for Graphs , which builds an\nautonomous agent based on LLMs to plan and solve graph tasks.\n3.3.1 Alignment between GNNs and LLMs. In general, GNNs\nand LLMs are designed to handle different modalities of data, with\nGNNs focusing on structural data and LLMs focusing on textual\ndata. This results in different feature spaces for the two models. To\naddress this issue and make both modalities of data more beneficial\nfor the learning of both GNNs and LLMs, several methods use tech-\nniques such as contrastive learning or Expectation-Maximization\n(EM) iterative training to align the feature spaces of the two models.\nThis enables better modeling of both graph and text information,\nresulting in improved performance on various tasks.\nWithin this topic, MoMu [60] is a multimodal molecular foun-\ndation model that includes two separate encoders, one for handling\nmolecular graphs (GIN) and another for handling text data (BERT).\nIt uses contrastive learning to pre-train the model on a dataset\nof molecular graph-text pairs. This approach enables MoMu to\ndirectly imagine new molecules from textual descriptions. Also\nin the bioinfo domain, MoleculeSTM [46] combines the chemi-\ncal structure information of molecules (i.e., molecular graph) with\ntheir textual descriptions ( i.e., SMILES strings), and uses a con-\ntrastive learning to jointly learn the molecular structure and textual\ndescriptions. It show great performance on multiple benchmark\ntests, including structure-text retrieval, text-based editing tasks,\nand molecular property prediction. Similarly, in ConGraT [3], a\ncontrastive graph-text pretraining technique is proposed to align\nthe node embeddings encoded by LMs and GNNs simultaneously.\nThe experiments are conducted on social networks, citation net-\nworks, and link networks, and show great performance on node\nand text classification as well as link prediction tasks. Furthermore,\nG2P2 [74, 75] enhances graph-grounded contrastive pre-training\nby proposing three different types of alignment: text-node, text-\nsummary, and node-summary alignment. This enables G2P2 to\nleverage the rich semantic relationships in the graph structure to\nimprove text classification performance in low-resource environ-\nments. GRENADE [36] is a graph-centric language model that\nproposes graph-centric contrastive learning and knowledge align-\nment to achieve both node-level and neighborhood-level alignment\nbased on the node embeddings encoded from GNNs and LMs. This\nenables the model to capture text semantics and graph structure\ninformation through self-supervised learning, even in the absence\nof human-annotated labels. In addition to contrastive learning,\nTHLM [100] leverages BERT and HGNNs to encode node embed-\ndings and uses a positive-negative classification task with negative\nsampling to improve the alignment of embeddings from two differ-\nent modalities. Recently, GLEM [94] adopts an efficient and effec-\ntive solution that integrates graph structure and language learning\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang\nanswer\nFusion\nModel\nGraphs\nStructure Info.\nTextual Info.\nGNNs\nLLMs\nalign\nAlignment between GNNs and LLMs\nGNNs\nLLMs\n‚äï\nGraphs\ntrain on\nFusion Training of GNNs and LLMs \nLLM\nAgent\nMemory\nTools\nGraph\nQuestions\n?\nplanning\nLLMs Agent for Graphs\nFigure 4: LLMs-Graphs Integration.\nthrough a variational expectation-maximization (EM) framework.\nBy iteratively using LMs and GNNs to provide labels for each other\nin node classification, GLEM aligns their capabilities in graph tasks.\n3.3.2 Fusion Training of GNNs and LLMs. Although align-\nment between the representations of GNNs and LLMs achieves\nco-optimization and embedding-level alignment of the two mod-\nels, they remain separate during inference. To achieve a higher\nlevel of integration between LLMs and GNNs, several works have\nfocused on designing a deeper fusion of the architecture of the\nmodules, such as transformer layers in LLMs and graph neural lay-\ners in GNNs. Co-training GNNs and LLMs can result in a win-win\nbi-directional benefit for both modules in graph tasks.\nAlong this line,GreaseLM [90] integrates transformer layers and\nGNN layers by designing a specific forward propagation layer that\nenables bidirectional information passing between LM and GNN\nthrough special interaction markers and interaction nodes. This\napproach allows language context representations to be grounded\nin structured world knowledge, while subtle linguistic differences\n(such as negation or modifiers) can affect the representation of\nthe knowledge graph, which enables GreaseLM to achieve high\nperformance on Question-Answering tasks. DGTL [54] proposes\ndisentangled graph learning to leverage GNNs to encode disentan-\ngled representations, which are then injected into each transformer\nlayer of the LLMs. This approach enables the LLMs to be aware of\nthe graph structure and leverage the gradient from the LLMs to fine-\ntune the GNNs. By doing so, DGTL achieves high performance on\nboth citation network and e-commerce graph tasks. ENGINE [98]\nadds a lightweight and tunable G-Ladder module to each layer of\nthe LLM, which uses a message-passing mechanism to integrate\nstructural information. This enables the output of each LLM layer\n(i.e., token-level representations) to be passed to the correspond-\ning G-Ladder, where the node representations are enhanced and\nthen used for downstream tasks such as node classification. More\ndirectly, GraphAdapter [31] uses a fusion module (typically a\nmulti-layer perceptrons) to combine the structural representations\nobtained from GNNs with the contextual hidden states of LLMs\n(e.g., the encoded node text). This enables the structural information\nfrom the GNN adapter to complement the textual information from\nthe LLMs, resulting in a fused representation that can be used for\nsupervision training and prompting for downstream tasks.\n3.3.3 LLMs Agent for Graphs. With the powerful capabilities of\nLLMs in understanding instructions and self-planning to solve tasks,\nan emerging research direction is to build autonomous agents based\non LLMs to tackle human-given or research-related tasks. Typically,\nan agent consists of a memory module, a perception module, and an\naction module to enable a loop of observation, memory recall, and\naction for solving given tasks. In the graph domain, LLMs-based\nagents can interact directly with graph data to perform tasks such\nas node classification and link prediction.\nIn this field, Pangu [19] pioneered the use of LMs to navigate\nKGs. In this approach, the agent is designed as a symbolic graph\nsearch algorithm, providing a set of potential search paths for the\nlanguage models to evaluate in response to a given query. The re-\nmaining path is then utilized to retrieve the answer. Graph Agent\n(GA) [71] converts graph data into textual descriptions and gen-\nerates embedding vectors, which are stored in long-term memory.\nDuring inference, GA retrieves similar samples from long-term\nmemory and integrates them into a structured prompt, which is\nused by LLMs to explain the potential reasons for node classification\nor edge connection. FUXI [18] framework integrates customized\ntools and the ReAct [83] algorithm to enable LLMs to act as agents\nthat can proactively interact with KGs. By leveraging tool-based\nnavigation and exploration of data, these agents perform chained\nreasoning to progressively build answers and ultimately solve com-\nplex queries efficiently and accurately. Readi [10] is another ap-\nproach that first uses in-context learning and chain-of-thought\nprompts to generate reasoning paths with multiple constraints,\nwhich are then instantiated based on the graph data. The instanti-\nated reasoning paths are merged and used as input to LLMs to gen-\nerate an answer. This method has achieved significant performance\nimprovements on KGQA (knowledge graph question answering)\nand TableQA (table question answering) tasks. Recently, RoG [49]\nis proposed to answer graph-retaled question in three steps: plan-\nning, retrieval, and reasoning. In the planning step, it generates\na set of associated paths based on the structured information of\nthe knowledge graph according to the problem. In the retrieval\nstep, it uses the associated paths generated in the planning stage to\nretrieve the corresponding reasoning paths from the KG. Finally,\nit uses the retrieved reasoning paths to generate the answer and\nexplanation for the problem using LLMs.\n3.3.4 Discussion. The integration of LLMs and graphs has shown\npromising progress in minimizing the modality gap between struc-\ntured data and textual data for solving graph-related tasks. By\ncombining the strengths of LLMs in language understanding and\nthe ability of graphs to capture complex relationships between\nentities, we can enable more accurate and flexible reasoning over\ngraph data. However, despite the promising progress, there is still\nroom for improvement in this area. One of the main challenges\nin integrating LLMs and graphs is scalability. In alignment and\nfusion training, current methods often use small language models\nor fix the parameters of LLMs, which limits their ability to scale to\nlarger graph datasets. Therefore, it is crucial to explore methods\nfor scaling model training with larger models on web-scale graph\ndata, which can enable more accurate and efficient reasoning over\nlarge-scale graphs. Another challenge in this area is the limited\ninteraction between graph agents and graph data. Current methods\nfor graph agents often plan and execute only once, which may not\nbe optimal for complex tasks requiring multiple runs. Therefore,\nA Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nGraphs\nDiverse\nFeatures\n‚äï\nInput Prompts\nTuning-free LLMs\nTuning-required LLMs\nor Answer\nFigure 5: LLMs-Only.\nit is necessary to investigate methods for agents to interact with\ngraph data multiple times, refining their plans and improving their\nperformance based on feedback from the graph. This can enable\nmore sophisticated reasoning over graph data and improve the ac-\ncuracy of downstream tasks. Overall, the integration of LLMs and\ngraphs is a promising research direction with significant potential\nfor advancing the state-of-the-art in graph learning. By address-\ning the aforementioned challenges and developing more advanced\nmethods for integrating LLMs and graphs, we can enable more\naccurate and flexible reasoning over graph data and unlock new\napplications in areas such as knowledge graph reasoning, molecular\nmodeling, and social network analysis.\n3.4 LLMs-Only\nIn this section, we will elaborate in detail on the direct application of\nLLMs for various graph-oriented tasks, namely the LLMs-Only cat-\negory. These methods aim to allow LLMs to directly accept graph\nstructure information, understand it, and perform inference for\nvarious downstream tasks in combination with this information.\nThese methods can mainly be divided into two broad categories: i)\nTuning-free methods aim to design prompts that LLMs can un-\nderstand to express graphs, directly prompting pre-trained LLMs\nto perform graph-oriented tasks; ii) Tuning-required approaches\nfocus on converting graphs into sequences in a specific way and\naligning graph token sequences and natural language token se-\nquences using fine-tuning methods.\n3.4.1 Tuning-free. Given the unique structured characteristics\nof graph data, two critical challenges arise: effectively construct-\ning a graph in natural language format and determining whether\nLarge Language Models (LLMs) can accurately comprehend graph\nstructures as represented linguistically. To address these issues,\ntuning-free approaches are being developed to model and infer\ngraphs solely within the text space, thereby exploring the potential\nof pre-trained LLMs for enhanced structural understanding.\nNLGraph [68], GPT4Graph [20] and Beyond Text [28] col-\nlectively examine the capabilities of LLMs in understanding and\nreasoning with graph data. NLGraph proposes a benchmark for\ngraph-based problem solving and introduces instruction-based ap-\nproaches, while GPT4Graph and Beyond Text investigate the pro-\nficiency of LLMs in comprehending graph structures and empha-\nsizes the need for advancements in their graph processing capa-\nbilities. And Graph-LLM [8] explores the potential of LLMs in\ngraph machine learning, focusing on the node classification task.\nTwo pipelines, LLMs-as-Enhancers and LLMs-as-Predictors, are in-\nvestigated to leverage LLMs‚Äô extensive common knowledge and\nsemantic comprehension abilities. Through comprehensive stud-\nies, it provides original observations and insights that open new\npossibilities for utilizing LLMs in learning on graphs. Meanwhile,\nGraphText [95] translates graphs into natural language by deriv-\ning a graph-syntax tree and processing it with an LLM. It offers\ntraining-free graph reasoning and enables interactive graph rea-\nsoning, showcasing the unexplored potential of LLMs. Talk like a\nGraph [15] conducts an in-depth examination of text-based graph\nencoder functions for LLMs, evaluating their efficacy in transform-\ning graph data into textual format to enhance LLMs‚Äô capabilities in\nexecuting graph reasoning tasks, and proposes the GraphQA bench-\nmark to systematically measure the influence of encoding strate-\ngies on model performance. And LLM4DyG [91] benchmarks the\nspatial-temporal comprehension of LLMs on dynamic graphs, intro-\nducing tasks that evaluate both temporal and spatial understanding,\nand suggests the Disentangled Spatial-Temporal Thoughts (DST2)\nprompting technique for improved performance. To facilitate the\nintegration of multimodality, GraphTMI [11] presents an innova-\ntive approach to integrating graph data with LLMs, introducing\ndiverse modalities such as text, image, and motif encoding to en-\nhance LLMs‚Äô efficiency in processing complex graph structures, and\nproposes the GraphTMI benchmark for evaluating LLMs in graph\nstructure analysis, revealing that the image modality outperforms\ntext and prior GNNs in balancing token limits and preserving essen-\ntial information. Ai et al. [2] introduces a multimodal framework\nfor graph understanding and reasoning, utilizing image encoding\nand GPT-4V‚Äôs advanced capabilities to interpret and process diverse\ngraph data, while identifying challenges in Chinese OCR and com-\nplex graph types, suggesting directions for future enhancements in\nAI‚Äôs multimodal interaction and graph data processing.\n3.4.2 Tuning-required. Due to the limitations of expressing graph\nstructural information using pure text, the recent mainstream ap-\nproach is to align graphs as node token sequences with natural lan-\nguage token sequences when inputting them to LLMs. In contrast\nto the aforementioned GNN as Prefix approach, the Tuning-required\nLLM-only approach discards the graph encoder and adopts a spe-\ncific arrangement of graph token sequences, along with carefully\ndesigned embeddings of graph tokens in prompts, achieving promis-\ning performances in various downstream graph-related tasks.\nInstructGLM [84] introduces an innovative framework for graph\nrepresentation learning that combines natural language instruc-\ntions with graph embeddings to fine-tune LLMs. This approach\nallows LLMs to effectively process graph structures without re-\nlying on specialized GNN architectures. WalkLM [62] integrates\nlanguage models with random walks to create unsupervised attrib-\nuted graph embeddings, focusing on the technical innovation of\ntransforming graph entities into textual sequences and utilizing\ngraph-aware fine-tuning. This technique captures both attribute\nsemantics and graph structures. Recently, LLaGA [7] has utilized\nnode-level templates to restructure graph data into organized se-\nquences, which are then mapped into the token embedding space.\nThis allows Large Language Models to process graph-structured\ndata with enhanced versatility, generalizability, and interpretability.\nInstructGraph [69] proposes a methodological approach to im-\nprove LLMs for graph reasoning and generation through structured\nformat verbalization, graph instruction tuning, and preference align-\nment. This aims to bridge the semantic gap between graph data and\ntextual language models, and to mitigate the issue of hallucination\nin LLM outputs.\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang\nZeroG [38] then leverages a language model to encode node\nattributes and class semantics, employing prompt-based subgraph\nsampling and lightweight fine-tuning strategies to address cross-\ndataset zero-shot transferability challenges in graph learning. Fur-\nthermore, GraphWiz [6] utilizes GraphInstruct, an instruction-\ntuning dataset, to augment language models for addressing var-\nious graph problems, employing Direct Preference Optimization\n(DPO) [55] to enhance the clarity and accuracy of reasoning pro-\ncesses. GraphInstruct [50] presents a comprehensive benchmark\nof 21 graph reasoning tasks, incorporating diverse graph genera-\ntion methods and detailed reasoning steps to enhance LLMs with\nimproved graph understanding and reasoning capabilities. And,\nMuseGraph [61] fuses the capabilities of LLMs with graph mining\ntasks through a compact graph description mechanism, diverse in-\nstruction generation, and graph-aware instruction tuning, enabling\na generic approach for analyzing and processing attributed graphs.\n3.4.3 Discussion. The LLMs-Only approach is an emerging re-\nsearch direction that explores the potential of pre-training Large\nLanguage Models specifically for interpreting graph data and merg-\ning graphs with natural language instructions. The main idea be-\nhind this approach is to leverage the powerful language understand-\ning capabilities of LLMs to reason over graph data and generate\naccurate responses to queries. However, effectively transforming\nlarge-scale graphs into text prompts and reordering graph token\nsequences to preserve structural integrity without a graph encoder\npresent significant ongoing challenges. These challenges arise due\nto the complex nature of graph data, which often contains intri-\ncate relationships between nodes and edges, as well as the limited\nability of LLMs to capture such relationships without explicit guid-\nance. As such, further research is needed to develop more advanced\nmethods for integrating LLMs with graph data and overcoming the\naforementioned challenges.\n4 FUTURE DIRECTIONS\nIn this section, we explore several open problems and potential\nfuture directions in the field of large language models for graphs.\n4.1 LLMs for Multi-modal Graphs\nRecent studies have demonstrated the remarkable ability of large\nlanguage models to process and understand multi-modal data [78],\nsuch as images [44] and videos [87]. This capability has opened up\nnew avenues for integrating LLMs with multi-modal graph data,\nwhere nodes may contain features from multiple modalities [40].\nBy developing multi-modal LLMs that can process such graph data,\nwe can enable more accurate and comprehensive reasoning over\ngraph structures, taking into account not only textual information\nbut also visual, auditory, and other types of data.\n4.2 Efficiency and Less Computational Cost\nIn the current landscape, the substantial computational expenses\nassociated with both the training and inference phases of LLMs\npose a significant limitation [ 13, 16], impeding their capacity to\nprocess large-scale graphs that encompass millions of nodes. This\nchallenge is further compounded when attempting to integrate\nLLMs with GNNs, as the fusion of these two powerful models\nbecomes increasingly arduous due to the aforementioned compu-\ntational constraints [94]. Consequently, the necessity to discover\nand implement efficient strategies for training LLMs and GNNs\nwith reduced computational costs becomes paramount. This is not\nonly to alleviate the current limitations but also to pave the way for\nthe enhanced application of LLMs in graph-related tasks, thereby\nbroadening their utility and impact in the field of data science.\n4.3 Tackling Different Graph Tasks\nThe prevailing methodologies LLMs have primarily centered their\nattention on conventional graph-related tasks, such as link predic-\ntion and node classification. However, considering the remarkable\ncapabilities of LLMs, it is both logical and promising to delve into\ntheir potential in tackling more complex and generative tasks, in-\ncluding but not limited to graph generation [97], graph understand-\ning, and graph-based question answering [32]. By expanding the\nhorizons of LLM-based approaches to encompass these intricate\ntasks, we can unlock a myriad of new opportunities for their appli-\ncation across diverse domains. For instance, in the realm of drug\ndiscovery, LLMs could facilitate the generation of novel molecular\nstructures; in social network analysis, they could provide deeper in-\nsights into intricate relationship patterns; and in knowledge graph\nconstruction, they could contribute to the creation of more com-\nprehensive and contextually accurate knowledge bases.\n4.4 User-Centric Agents on Graphs\nThe majority of contemporary LLM-based agents, specifically de-\nsigned to address graph-related tasks, are predominantly tailored for\nsingle graph tasks. These agents typically adhere to a one-time-run\nprocedure, aiming to resolve the provided question in a single at-\ntempt. Consequently, these agents are neither equipped to function\nas multi-run interactive agents, capable of adjusting their generated\nplans based on feedback or additional information, nor are they\ndesigned to be user-friendly agents that can effectively manage a\nwide array of user-given questions. An LLM-based agent [70] that\nembodies the ideal qualities should not only be user-friendly but\nalso possess the capability to dynamically search for answers within\ngraph data in response to a diverse range of open-ended questions\nposed by users. This would necessitate the development of an agent\nthat is both adaptable and robust, able to engage in iterative interac-\ntions with users and adept at navigating the complexities of graph\ndata to provide accurate and relevant answers.\n5 CONCLUSION\nIn this comprehensive survey, we delve into the current state of\nlarge language models specifically tailored for graph data, propos-\ning an innovative taxonomy grounded in the distinctive designs\nof their inference frameworks. We meticulously categorize these\nmodels into four unique framework designs, each characterized by\nits own set of advantages and limitations. Additionally, we provide\na detailed discussion on these characteristics, enriching our analysis\nwith insights into potential challenges and opportunities within\nthis field. Our survey not only serves as a critical resource for re-\nsearchers keen on exploring and leveraging large language models\nfor graph-related tasks but also aims to inspire and guide future\nresearch endeavors in this evolving domain. Through this work,\nwe hope to foster a deeper understanding and stimulate further\ninnovation in the integration of LLMs with graphs.\nA Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nREFERENCES\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2] Qihang Ai, Jianwu Zhou, Haiyun Jiang, Lemao Liu, and Shuming Shi. 2023. When\nGraph Data Meets Multimodal: A New Paradigm for Graph Understanding and\nReasoning. arXiv preprint arXiv:2312.10372 (2023).\n[3] William Brannon et al. 2023. Congrat: Self-supervised contrastive pretraining\nfor joint graph and text embeddings. arXiv preprint arXiv:2305.14321 (2023).\n[4] He Cao, Zijing Liu, Xingyu Lu, Yuan Yao, and Yu Li. 2023. Instructmol: Multi-\nmodal integration for building a versatile and reliable molecular assistant in\ndrug discovery. arXiv preprint arXiv:2311.16208 (2023).\n[5] Ziwei Chai, Tianjie Zhang, Liang Wu, Kaiqiao Han, Xiaohai Hu, Xuanwen\nHuang, and Yang Yang. 2023. Graphllm: Boosting graph reasoning ability of\nlarge language model. arXiv preprint arXiv:2310.05845 (2023).\n[6] Nuo Chen, Yuhan Li, Jianheng Tang, and Jia Li. 2024. GraphWiz: An Instruction-\nFollowing Language Model for Graph Problems. arXiv preprint arXiv:2402.16029\n(2024).\n[7] Runjin Chen, Tong Zhao, Ajay Jaiswal, Neil Shah, and Zhangyang Wang. 2024.\nLLaGA: Large Language and Graph Assistant. arXiv preprint arXiv:2402.08170\n(2024).\n[8] Zhikai Chen, Haitao Mao, Hang Li, Wei Jin, Hongzhi Wen, Xiaochi Wei,\nShuaiqiang Wang, Dawei Yin, Wenqi Fan, Hui Liu, et al. 2024. Exploring the\npotential of large language models (llms) in learning on graphs. ACM SIGKDD\nExplorations Newsletter 25, 2 (2024), 42‚Äì61.\n[9] Zhikai Chen, Haitao Mao, Hongzhi Wen, Haoyu Han, Wei Jin, Haiyang Zhang,\nHui Liu, and Jiliang Tang. 2023. Label-free node classification on graphs with\nlarge language models (llms). arXiv preprint arXiv:2310.04668 (2023).\n[10] Sitao Cheng, Ziyuan Zhuang, Yong Xu, Fangkai Yang, Chaoyun Zhang, Xiaoting\nQin, Xiang Huang, Ling Chen, Qingwei Lin, Dongmei Zhang, et al. 2024. Call Me\nWhen Necessary: LLMs can Efficiently and Faithfully Reason over Structured\nEnvironments. arXiv preprint arXiv:2403.08593 (2024).\n[11] Debarati Das, Ishaan Gupta, Jaideep Srivastava, and Dongyeop Kang. 2023.\nWhich Modality should I use‚ÄìText, Motif, or Image?: Understanding Graphs\nwith Large Language Models. arXiv preprint arXiv:2311.09862 (2023).\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert:\nPre-training of deep bidirectional transformers for language understanding.\narXiv preprint arXiv:1810.04805 (2018).\n[13] Ning Ding, Yujia Qin, Guang Yang, Fuchao Wei, Zonghan Yang, Yusheng Su,\nShengding Hu, Yulin Chen, Chi-Min Chan, Weize Chen, et al. 2023. Parameter-\nefficient fine-tuning of large-scale pre-trained language models.Nature Machine\nIntelligence 5, 3 (2023), 220‚Äì235.\n[14] Keyu Duan, Qian Liu, Tat-Seng Chua, Shuicheng Yan, Wei Tsang Ooi, Qizhe\nXie, and Junxian He. 2023. Simteg: A frustratingly simple approach improves\ntextual graph learning. arXiv preprint arXiv:2308.02565 (2023).\n[15] Bahare Fatemi, Jonathan Halcrow, and Bryan Perozzi. 2023. Talk like a graph:\nEncoding graphs for large language models. arXiv preprint arXiv:2310.04560\n(2023).\n[16] Ze-Feng Gao, Peiyu Liu, Wayne Xin Zhao, Zhong-Yi Lu, and Ji-Rong Wen. 2022.\nParameter-efficient mixture-of-experts architecture for pre-trained language\nmodels. arXiv preprint arXiv:2203.01104 (2022).\n[17] Vikas Garg, Stefanie Jegelka, and Tommi Jaakkola. 2020. Generalization and\nrepresentational limits of graph neural networks. In ICML. PMLR, 3419‚Äì3430.\n[18] Yu Gu et al. 2024. Middleware for LLMs: Tools Are Instrumental for Language\nAgents in Complex Environments. arXiv preprint arXiv:2402.14672 (2024).\n[19] Yu Gu, Xiang Deng, and Yu Su. 2022. Don‚Äôt Generate, Discriminate: A Proposal\nfor Grounding Language Models to Real-World Environments. arXiv preprint\narXiv:2212.09736 (2022).\n[20] Jiayan Guo, Lun Du, and Hengyu Liu. 2023. Gpt4graph: Can large language\nmodels understand graph structured data? an empirical evaluation and bench-\nmarking. arXiv preprint arXiv:2305.15066 (2023).\n[21] Zirui Guo, Lianghao Xia, Yanhua Yu, Yuling Wang, Zixuan Yang, Wei Wei, Liang\nPang, Tat-Seng Chua, and Chao Huang. 2024. GraphEdit: Large Language\nModels for Graph Structure Learning. arXiv preprint arXiv:2402.15183 (2024).\n[22] Xiaoxin He, Xavier Bresson, Thomas Laurent, Adam Perold, Yann LeCun, and\nBryan Hooi. 2023. Harnessing explanations: Llm-to-lm interpreter for enhanced\ntext-attributed graph representation learning. In The Twelfth International Con-\nference on Learning Representations .\n[23] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network for\nrecommendation. In Proceedings of the 43rd International ACM SIGIR conference\non research and development in Information Retrieval . 639‚Äì648.\n[24] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh V Chawla, Thomas Laurent, Yann Le-\nCun, Xavier Bresson, and Bryan Hooi. 2024. G-Retriever: Retrieval-Augmented\nGeneration for Textual Graph Understanding and Question Answering. arXiv\npreprint arXiv:2402.07630 (2024).\n[25] Yufei He and Bryan Hooi. 2024. UniGraph: Learning a Cross-Domain Graph\nFoundation Model From Natural Language. arXiv preprint arXiv:2402.13630\n(2024).\n[26] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,\nand Wayne Xin Zhao. 2024. Large language models are zero-shot rankers\nfor recommender systems. In European Conference on Information Retrieval .\nSpringer, 364‚Äì381.\n[27] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen\nLiu, Michele Catasta, and Jure Leskovec. 2020. Open graph benchmark: Datasets\nfor machine learning on graphs. Advances in neural information processing\nsystems 33 (2020), 22118‚Äì22133.\n[28] Yuntong Hu, Zheng Zhang, and Liang Zhao. 2023. Beyond Text: A Deep Dive\ninto Large Language Models‚Äô Ability on Understanding Graph Data. arXiv\npreprint arXiv:2310.04944 (2023).\n[29] Chao Huang, Xubin Ren, Jiabin Tang, Dawei Yin, and Nitesh Chawla. 2024.\nLarge Language Models for Graphs: Progresses and Directions. In Companion\nProceedings of the ACM on Web Conference 2024 . 1284‚Äì1287.\n[30] Xuanwen Huang, Kaiqiao Han, Dezheng Bao, Quanjin Tao, Zhisheng Zhang,\nYang Yang, and Qi Zhu. 2023. Prompt-based node feature extractor for few-shot\nlearning on text-attributed graphs. arXiv preprint arXiv:2309.02848 (2023).\n[31] Xuanwen Huang, Kaiqiao Han, Yang Yang, Dezheng Bao, Quanjin Tao, Ziwei\nChai, and Qi Zhu. 2024. Can GNN be Good Adapter for LLMs? arXiv preprint\narXiv:2402.12984 (2024).\n[32] Xiao Huang, Jingyuan Zhang, Dingcheng Li, and Ping Li. 2019. Knowledge\ngraph embedding based question answering. In Proceedings of the twelfth ACM\ninternational conference on web search and data mining . 105‚Äì113.\n[33] Bowen Jin, Gang Liu, Chi Han, Meng Jiang, Heng Ji, and Jiawei Han. 2023.\nLarge language models on graphs: A comprehensive survey. arXiv preprint\narXiv:2312.02783 (2023).\n[34] Thomas N Kipf and Max Welling. 2016. Semi-supervised classification with\ngraph convolutional networks. arXiv preprint arXiv:1609.02907 (2016).\n[35] Chaoliu Li, Lianghao Xia, Xubin Ren, Yaowen Ye, Yong Xu, and Chao Huang.\n2023. Graph transformer for recommendation. In SIGIR. 1680‚Äì1689.\n[36] Yichuan Li, Kaize Ding, and Kyumin Lee. 2023. GRENADE: Graph-Centric Lan-\nguage Model for Self-Supervised Representation Learning on Text-Attributed\nGraphs. arXiv preprint arXiv:2310.15109 (2023).\n[37] Yuhan Li, Zhixun Li, Peisong Wang, Jia Li, Xiangguo Sun, Hong Cheng, and\nJeffrey Xu Yu. 2023. A survey of graph meets large language model: Progress\nand future directions. arXiv preprint arXiv:2311.12399 (2023).\n[38] Yuhan Li, Peisong Wang, Zhixun Li, Jeffrey Xu Yu, and Jia Li. 2024. ZeroG:\nInvestigating Cross-dataset Zero-shot Transferability in Graphs. arXiv preprint\narXiv:2402.11235 (2024).\n[39] Zhonghang Li, Lianghao Xia, Jiabin Tang, Yong Xu, Lei Shi, Long Xia, Dawei\nYin, and Chao Huang. 2024. Urbangpt: Spatio-temporal large language models.\narXiv preprint arXiv:2403.00813 (2024).\n[40] Wanying Liang, Pasquale De Meo, Yong Tang, and Jia Zhu. 2024. A Survey of\nMulti-modal Knowledge Graphs: Technologies and Trends. Comput. Surveys\n(2024).\n[41] Xinyu Lin, Wenjie Wang, Yongqi Li, Shuo Yang, Fuli Feng, Yinwei Wei, and Tat-\nSeng Chua. 2024. Data-efficient Fine-tuning for LLM-based Recommendation.\narXiv preprint arXiv:2401.17197 (2024).\n[42] Hao Liu, Jiarui Feng, Lecheng Kong, Ningyue Liang, Dacheng Tao, Yixin Chen,\nand Muhan Zhang. 2023. One for all: Towards training one graph model for all\nclassification tasks. arXiv preprint arXiv:2310.00149 .\n[43] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2024. Improved base-\nlines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition . 26296‚Äì26306.\n[44] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual\ninstruction tuning. Advances in neural information processing systems 36 (2024).\n[45] Pengfei Liu, Yiming Ren, Jun Tao, and Zhixiang Ren. 2024. Git-mol: A multi-\nmodal large language model for molecular science with graph, image, and text.\nComputers in Biology and Medicine 171 (2024), 108073.\n[46] Shengchao Liu et al . 2023. Multi-modal molecule structure‚Äìtext model for\ntext-based retrieval and editing. Nature Machine Intelligence (2023).\n[47] Yinhan Liu et al. 2019. Roberta: A robustly optimized bert pretraining approach.\narXiv preprint arXiv:1907.11692 (2019).\n[48] Zhiyuan Liu et al . 2023. Molca: Molecular graph-language modeling with\ncross-modal projector and uni-modal adapter. arXiv preprint arXiv:2310.12798\n(2023).\n[49] Linhao Luo, Yuan-Fang Li, Gholamreza Haffari, and Shirui Pan. 2023. Reasoning\non graphs: Faithful and interpretable large language model reasoning. arXiv\npreprint arXiv:2310.01061 (2023).\n[50] Zihan Luo, Xiran Song, Hong Huang, Jianxun Lian, Chenhao Zhang, Jinqi\nJiang, Xing Xie, and Hai Jin. 2024. GraphInstruct: Empowering Large Language\nModels with Graph Understanding and Reasoning Capability. arXiv preprint\narXiv:2403.04483 (2024).\n[51] Qiyao Ma, Xubin Ren, and Chao Huang. 2024. XRec: Large Language Models\nfor Explainable Recommendation. arXiv preprint arXiv:2406.02377 (2024).\nKDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain Xubin Ren, Jiabin Tang, Dawei Yin, Nitesh Chawla, & Chao Huang\n[52] Seth A Myers, Aneesh Sharma, Pankaj Gupta, and Jimmy Lin. 2014. Informa-\ntion network or social network? The structure of the Twitter follow graph. In\nProceedings of the 23rd international conference on world wide web . 493‚Äì498.\n[53] Shirui Pan, Linhao Luo, Yufei Wang, Chen Chen, Jiapu Wang, and Xindong Wu.\n2024. Unifying large language models and knowledge graphs: A roadmap. IEEE\nTransactions on Knowledge and Data Engineering (2024).\n[54] Yijian Qin, Xin Wang, Ziwei Zhang, and Wenwu Zhu. 2023. Disentangled\nrepresentation learning with large language models for text-attributed graphs.\narXiv preprint arXiv:2310.18152 (2023).\n[55] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano\nErmon, and Chelsea Finn. 2024. Direct preference optimization: Your language\nmodel is secretly a reward model. Advances in Neural Information Processing\nSystems 36 (2024).\n[56] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang,\nMichael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text transformer. Journal of machine\nlearning research 21, 140 (2020), 1‚Äì67.\n[57] Xubin Ren, Wei Wei, Lianghao Xia, and Chao Huang. 2024. A Comprehen-\nsive Survey on Self-Supervised Learning for Recommendation. arXiv preprint\narXiv:2404.03354 (2024).\n[58] Xubin Ren, Wei Wei, Lianghao Xia, Lixin Su, Suqi Cheng, Junfeng Wang, Dawei\nYin, and Chao Huang. 2024. Representation learning with large language models\nfor recommendation. In Proceedings of the ACM on Web Conference 2024 . 3464‚Äì\n3475.\n[59] Xubin Ren, Lianghao Xia, Jiashu Zhao, Dawei Yin, and Chao Huang. 2023. Disen-\ntangled contrastive collaborative filtering. InProceedings of the 46th International\nACM SIGIR Conference on Research and Development in Information Retrieval .\n1137‚Äì1146.\n[60] Bing Su, Dazhao Du, Zhao Yang, et al. 2022. A molecular multimodal founda-\ntion model associating molecule graphs with natural language. arXiv preprint\narXiv:2209.05481 (2022).\n[61] Yanchao Tan, Hang Lv, Xinyi Huang, Jiawei Zhang, Shiping Wang, and Carl\nYang. 2024. MuseGraph: Graph-oriented Instruction Tuning of Large Language\nModels for Generic Graph Mining. arXiv preprint arXiv:2403.04780 (2024).\n[62] Yanchao Tan, Zihao Zhou, Hang Lv, Weiming Liu, and Carl Yang. 2024. Walklm:\nA uniform language model fine-tuning framework for attributed graph embed-\nding. Advances in Neural Information Processing Systems 36 (2024).\n[63] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Lixin Su, Suqi Cheng, Dawei Yin,\nand Chao Huang. 2023. Graphgpt: Graph instruction tuning for large language\nmodels. arXiv preprint arXiv:2310.13023 (2023).\n[64] Jiabin Tang, Yuhao Yang, Wei Wei, Lei Shi, Long Xia, Dawei Yin, and Chao\nHuang. 2024. HiGPT: Heterogeneous Graph Language Model. arXiv preprint\narXiv:2402.16024 (2024).\n[65] Yijun Tian, Huan Song, Zichen Wang, Haozhu Wang, Ziqing Hu, Fang Wang,\nNitesh V Chawla, and Panpan Xu. 2024. Graph neural prompting with large\nlanguage models. In Proceedings of the AAAI Conference on Artificial Intelligence ,\nVol. 38. 19080‚Äì19088.\n[66] Hugo Touvron et al . 2023. Llama: Open and efficient foundation language\nmodels. arXiv preprint arXiv:2302.13971 (2023).\n[67] Petar Veliƒçkoviƒá, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro\nLio, and Yoshua Bengio. 2017. Graph attention networks. arXiv preprint\narXiv:1710.10903 (2017).\n[68] Heng Wang, Shangbin Feng, Tianxing He, Zhaoxuan Tan, Xiaochuang Han, and\nYulia Tsvetkov. 2024. Can language models solve graph problems in natural\nlanguage? Advances in Neural Information Processing Systems 36 (2024).\n[69] Jianing Wang, Junda Wu, Yupeng Hou, et al . 2024. InstructGraph: Boosting\nLarge Language Models via Graph-centric Instruction Tuning and Preference\nAlignment. arXiv preprint arXiv:2402.08785 (2024).\n[70] Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang,\nZhiyuan Chen, Jiakai Tang, et al. 2024. A survey on large language model based\nautonomous agents. Frontiers of Computer Science 18, 6 (2024), 186345.\n[71] Qinyong Wang, Zhenxiang Gao, and Rong Xu. 2023. Graph Agent: Explicit\nReasoning Agent for Graphs. arXiv preprint arXiv:2310.16421 (2023).\n[72] Yizhong Wang, Hamish Ivison, Pradeep Dasigi, et al. 2023. How far can camels\ngo? exploring the state of instruction tuning on open resources. Advances in\nNeural Information Processing Systems 36 (2023), 74764‚Äì74786.\n[73] Wei Wei, Xubin Ren, Jiabin Tang, Qinyong Wang, Lixin Su, Suqi Cheng, Junfeng\nWang, Dawei Yin, and Chao Huang. 2024. Llmrec: Large language models\nwith graph augmentation for recommendation. In Proceedings of the 17th ACM\nInternational Conference on Web Search and Data Mining . 806‚Äì815.\n[74] Zhihao Wen and Yuan Fang. 2023. Augmenting low-resource text classification\nwith graph-grounded pre-training and prompting. In Proceedings of the 46th\nInternational ACM SIGIR Conference on Research and Development in Information\nRetrieval. 506‚Äì516.\n[75] Zhihao Wen and Yuan Fang. 2023. Prompt tuning on graph-augmented low-\nresource text classification. arXiv preprint arXiv:2307.10230 (2023).\n[76] Qitian Wu, Chenxiao Yang, Wentao Zhao, Yixuan He, David Wipf, and Junchi\nYan. 2023. Difformer: Scalable (graph) transformers induced by energy con-\nstrained diffusion. arXiv preprint arXiv:2301.09474 (2023).\n[77] Qitian Wu, Wentao Zhao, Zenan Li, David P Wipf, and Junchi Yan. 2022. Node-\nformer: A scalable graph structure learning transformer for node classification.\nAdvances in Neural Information Processing Systems 35 (2022), 27387‚Äì27401.\n[78] Shengqiong Wu, Hao Fei, Leigang Qu, Wei Ji, and Tat-Seng Chua. 2023. Next-gpt:\nAny-to-any multimodal llm. arXiv preprint arXiv:2309.05519 (2023).\n[79] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and\nS Yu Philip. 2020. A comprehensive survey on graph neural networks. IEEE\ntransactions on neural networks and learning systems 32, 1 (2020), 4‚Äì24.\n[80] Lianghao Xia, Ben Kao, and Chao Huang. 2024. OpenGraph: Towards Open\nGraph Foundation Models. arXiv preprint arXiv:2403.01121 (2024).\n[81] Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N Ioannidis, Xiang Song,\nQing Ping, Sheng Wang, Carl Yang, Yi Xu, et al. 2023. Graph-aware language\nmodel pre-training on a large graph corpus can help multiple graph applications.\nIn Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and\nData Mining . 5270‚Äì5281.\n[82] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. 2018. How powerful\nare graph neural networks? arXiv preprint arXiv:1810.00826 (2018).\n[83] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\nand Yuan Cao. 2022. React: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629 (2022).\n[84] Ruosong Ye, Caiqi Zhang, et al. 2023. Natural language is all a graph needs.\narXiv preprint arXiv:2308.07134 (2023).\n[85] Yuning You, Tianlong Chen, Yang Shen, and Zhangyang Wang. 2021. Graph\ncontrastive learning automated. InInternational Conference on Machine Learning .\nPMLR, 12121‚Äì12132.\n[86] Seongjun Yun, Minbyul Jeong, Raehyun Kim, Jaewoo Kang, and Hyunwoo J Kim.\n2019. Graph transformer networks. Advances in neural information processing\nsystems 32 (2019).\n[87] Hang Zhang, Xin Li, and Lidong Bing. 2023. Video-llama: An instruction-\ntuned audio-visual language model for video understanding. arXiv preprint\narXiv:2306.02858 (2023).\n[88] Mengmei Zhang et al. 2024. GraphTranslator: Aligning Graph Model to Large\nLanguage Model for Open-ended Tasks. arXiv preprint arXiv:2402.07197 (2024).\n[89] Muhan Zhang and Yixin Chen. 2018. Link prediction based on graph neural\nnetworks. Advances in neural information processing systems 31 (2018).\n[90] Xikun Zhang, Antoine Bosselut, Michihiro Yasunaga, Hongyu Ren, Percy Liang,\net al. 2022. Greaselm: Graph reasoning enhanced language models for question\nanswering. arXiv preprint arXiv:2201.08860 (2022).\n[91] Zeyang Zhang, Xin Wang, Ziwei Zhang, Haoyang Li, Yijian Qin, Simin Wu, and\nWenwu Zhu. 2023. LLM4DyG: Can Large Language Models Solve Problems on\nDynamic Graphs? arXiv preprint arXiv:2310.17110 (2023).\n[92] Haiteng Zhao, Shengchao Liu, Ma Chang, Hannan Xu, Jie Fu, Zhihong Deng,\nLingpeng Kong, and Qi Liu. 2024. Gimlet: A unified graph-text model for\ninstruction-based molecule zero-shot learning. Advances in Neural Information\nProcessing Systems 36 (2024).\n[93] Jianan Zhao, Hesham Mostafa, Michael Galkin, Michael Bronstein, Zhaocheng\nZhu, and Jian Tang. 2024. GraphAny: A Foundation Model for Node Classifica-\ntion on Any Graph. arXiv preprint arXiv:2405.20445 (2024).\n[94] Jianan Zhao, Meng Qu, Chaozhuo Li, Hao Yan, Qian Liu, Rui Li, Xing Xie, and\nJian Tang. 2022. Learning on large-scale text-attributed graphs via variational\ninference. arXiv preprint arXiv:2210.14709 (2022).\n[95] Jianan Zhao, Le Zhuo, Yikang Shen, Meng Qu, Kai Liu, Michael Bronstein,\nZhaocheng Zhu, and Jian Tang. 2023. Graphtext: Graph reasoning in text space.\narXiv preprint arXiv:2310.01089 (2023).\n[96] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou,\nYingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. 2023. A survey\nof large language models. arXiv preprint arXiv:2303.18223 (2023).\n[97] Yanqiao Zhu, Yuanqi Du, Yinkai Wang, Yichen Xu, Jieyu Zhang, Qiang Liu, and\nShu Wu. 2022. A survey on deep graph generation: Methods and applications.\nIn Learning on Graphs Conference . PMLR, 47‚Äì1.\n[98] Yun Zhu, Yaoke Wang, Haizhou Shi, and Siliang Tang. 2024. Efficient Tuning\nand Inference for Large Language Models on Textual Graphs. arXiv preprint\narXiv:2401.15569 (2024).\n[99] Yaochen Zhu, Liang Wu, Qi Guo, Liangjie Hong, and Jundong Li. 2023. Col-\nlaborative large language model for recommender systems. arXiv preprint\narXiv:2311.01343 (2023).\n[100] Tao Zou, Le Yu, Yifei Huang, Leilei Sun, and Bowen Du. 2023. Pretraining\nlanguage models with text-attributed heterogeneous graphs. arXiv preprint\narXiv:2310.12580 (2023).\n6 APPENDIX\nIn Table 1, we provide an overview of notable graph learning tech-\nniques that utilize large language models.\nA Survey of Large Language Models for Graphs KDD ‚Äô24, August 25‚Äì29, 2024, Barcelona, Spain\nTable 1: Summary of representative graph learning methods with large language models.\nCategory Method Pipeline Domain Venue Year\nGNNs as Prefix\nGraphGPT [63] Node-level Tokenization General Graph SIGIR 2024\nHiGPT [64] Node-level Tokenization Heterogeneous Graph KDD 2024\nGraphTranslator [88] Node-level Tokenization General Graph WWW 2024\nUniGraph [25] Node-level Tokenization General Graph arXiv 2024\nGIMLET [92] Node-level Tokenization Bioinformatics NeurIPS 2024\nXRec [51] Node-level Tokenization Recommendation arXiv 2024\nGraphLLM [5] Graph-level Tokenization Graph Reasoning arXiv 2023\nGIT-Mol [45] Graph-level Tokenization Bioinformatics Comput Biol Med 2024\nMolCA [48] Graph-level Tokenization Bioinformatics EMNLP 2023\nInstructMol [4] Graph-level Tokenization Bioinformatics arXiv 2023\nG-Retriever [24] Graph-level Tokenization Graph-based QA arXiv 2024\nGNP [65] Graph-level Tokenization Graph-based QA AAAI 2024\nLLMs as Prefix\nG-Prompt [30] Embs. from LLMs for GNNs General Graph arXiv 2023\nSimTeG [14] Embs. from LLMs for GNNs General Graph arXiv 2023\nGALM [81] Embs. from LLMs for GNNs General Graph KDD 2023\nOFA [42] Embs. from LLMs for GNNs General Graph ICLR 2024\nTAPE [22] Embs. from LLMs for GNNs General Graph ICLR 2024\nLLMRec [73] Embs. from LLMs for GNNs Recommendation WSDM 2024\nOpenGraph [80] Labels from LLMs for GNNs General Graph arXiv 2024\nLLM-GNN [9] Labels from LLMs for GNNs General Graph ICLR 2024\nGraphEdit [21] Labels from LLMs for GNNs General Graph arXiv 2023\nRLMRec [58] Labels from LLMs for GNNs Recommendation WWW 2024\nLLMs-Graphs Interaction\nMoMu [63] Alignment between GNNs and LLMs Bioinformatics arXiv 2022\nConGraT [64] Alignment between GNNs and LLMs General Graph arXiv 2023\nG2P2 [88] Alignment between GNNs and LLMs General Graph SIGIR 2023\nGRENADE [25] Alignment between GNNs and LLMs General Graph EMNLP 2023\nMoleculeSTM [92] Alignment between GNNs and LLMs Bioinformatics Nature MI 2023\nTHLM [51] Alignment between GNNs and LLMs Heterogeneous Graph EMNLP 2023\nGLEM [5] Alignment between GNNs and LLMs General Graph ICLR 2023\nGreaseLM [90] Fusion Training of GNNs and LLMs Graph-based QA ICLR 2022\nDGTL [54] Fusion Training of GNNs and LLMs General Graph arXiv 2023\nENGINE [98] Fusion Training of GNNs and LLMs General Graph arXiv 2024\nGraphAdapter [31] Fusion Training of GNNs and LLMs General Graph WWW 2024\nPangu [19] LLMs Agent for Graphs Graph-based QA ACL 2023\nGraph Agent [71] LLMs Agent for Graphs General Graph arXiv 2023\nFUXI [18] LLMs Agent for Graphs Graph-based QA arXiv 2024\nReadi [10] LLMs Agent for Graphs Graph-based QA arXiv 2024\nRoG [49] LLMs Agent for Graphs Graph-based QA ICLR 2024\nLLMs-Only\nNLGraph [68] Tuning-free Graph Reasoning NeurIPS 2024\nGPT4Graph [20] Tuning-free Graph Reasoning & QA arXiv 2023\nBeyond Text [28] Tuning-free General Graph arXiv 2023\nGraph-LLM [8] Tuning-free General Graph KDD Exp. News. 2023\nGraphText [95] Tuning-free General Graph arXiv 2023\nTalk like a Graph [15] Tuning-free Graph Reasoning arXiv 2023\nLLM4DyG [91] Tuning-free Dynamic Graph arXiv 2023\nGraphTMI [11] Tuning-free General Graph arXiv 2023\nAi et al. [2] Tuning-free Multi-modal Graph arXiv 2023\nInstructGLM [84] Tuning-required General Graph EACL 2024\nWalkLM [62] Tuning-required General Graph NeurIPS 2024\nLLaGA [7] Tuning-required General Graph ICML 2024\nInstructGraph [69] Tuning-required General Graph & QA & Reasoning arXiv 2024\nZeroG [38] Tuning-required General Graph arXiv 2024\nGraphWiz [6] Tuning-required Graph Reasoning arXiv 2024\nGraphInstruct [50] Tuning-required Graph Reasoning & Generation arXiv 2024\nMuseGraph [61] Tuning-required General Graph arXiv 2024",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.3971092998981476
    },
    {
      "name": "Linguistics",
      "score": 0.3529815673828125
    },
    {
      "name": "Philosophy",
      "score": 0.12020546197891235
    }
  ]
}