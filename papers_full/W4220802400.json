{
  "title": "Transformer-based molecular optimization beyond matched molecular pairs",
  "url": "https://openalex.org/W4220802400",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2108660797",
      "name": "Jiazhen He",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2317767365",
      "name": "eva nittinger",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A96058648",
      "name": "Christian Tyrchan",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A1467017110",
      "name": "Werngard Czechtizky",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A1452831760",
      "name": "Atanas Patronov",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2972792757",
      "name": "Esben Jannik Bjerrum",
      "affiliations": [
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A208626163",
      "name": "Ola Engkvist",
      "affiliations": [
        "Chalmers University of Technology",
        "AstraZeneca (Sweden)"
      ]
    },
    {
      "id": "https://openalex.org/A2108660797",
      "name": "Jiazhen He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2317767365",
      "name": "eva nittinger",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A96058648",
      "name": "Christian Tyrchan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1467017110",
      "name": "Werngard Czechtizky",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1452831760",
      "name": "Atanas Patronov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2972792757",
      "name": "Esben Jannik Bjerrum",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A208626163",
      "name": "Ola Engkvist",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2023818227",
    "https://openalex.org/W1977242738",
    "https://openalex.org/W2578240541",
    "https://openalex.org/W2765224015",
    "https://openalex.org/W2529996553",
    "https://openalex.org/W2963609389",
    "https://openalex.org/W4289436753",
    "https://openalex.org/W6610423178",
    "https://openalex.org/W2805002767",
    "https://openalex.org/W2793945656",
    "https://openalex.org/W2610148085",
    "https://openalex.org/W2736137960",
    "https://openalex.org/W2963445908",
    "https://openalex.org/W2956961449",
    "https://openalex.org/W2963028280",
    "https://openalex.org/W3025593963",
    "https://openalex.org/W3100358278",
    "https://openalex.org/W4245562513",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W1604884792",
    "https://openalex.org/W2562677145",
    "https://openalex.org/W2060531713",
    "https://openalex.org/W2900090807",
    "https://openalex.org/W2020859943",
    "https://openalex.org/W3097605476",
    "https://openalex.org/W2803615944",
    "https://openalex.org/W3173178613",
    "https://openalex.org/W2966357564",
    "https://openalex.org/W3098269892"
  ],
  "abstract": "Abstract Molecular optimization aims to improve the drug profile of a starting molecule. It is a fundamental problem in drug discovery but challenging due to (i) the requirement of simultaneous optimization of multiple properties and (ii) the large chemical space to explore. Recently, deep learning methods have been proposed to solve this task by mimicking the chemist’s intuition in terms of matched molecular pairs (MMPs). Although MMPs is a widely used strategy by medicinal chemists, it offers limited capability in terms of exploring the space of structural modifications, therefore does not cover the complete space of solutions. Often more general transformations beyond the nature of MMPs are feasible and/or necessary, e . g . simultaneous modifications of the starting molecule at different places including the core scaffold. This study aims to provide a general methodology that offers more general structural modifications beyond MMPs. In particular, the same Transformer architecture is trained on different datasets. These datasets consist of a set of molecular pairs which reflect different types of transformations. Beyond MMP transformation, datasets reflecting general structural changes are constructed from ChEMBL based on two approaches: Tanimoto similarity (allows for multiple modifications) and scaffold matching (allows for multiple modifications but keep the scaffold constant) respectively. We investigate how the model behavior can be altered by tailoring the dataset while using the same model architecture. Our results show that the models trained on differently prepared datasets transform a given starting molecule in a way that it reflects the nature of the dataset used for training the model. These models could complement each other and unlock the capability for the chemists to pursue different options for improving a starting molecule.",
  "full_text": "He et al. Journal of Cheminformatics           (2022) 14:18  \nhttps://doi.org/10.1186/s13321-022-00599-3\nRESEARCH ARTICLE\nTransformer-based molecular optimization \nbeyond matched molecular pairs\nJiazhen He1* , Eva Nittinger2, Christian Tyrchan2, Werngard Czechtizky2, Atanas Patronov1, \nEsben Jannik Bjerrum1 and Ola Engkvist1,3 \nAbstract \nMolecular optimization aims to improve the drug profile of a starting molecule. It is a fundamental problem in drug \ndiscovery but challenging due to (i) the requirement of simultaneous optimization of multiple properties and (ii) the \nlarge chemical space to explore. Recently, deep learning methods have been proposed to solve this task by mimick-\ning the chemist’s intuition in terms of matched molecular pairs (MMPs). Although MMPs is a widely used strategy by \nmedicinal chemists, it offers limited capability in terms of exploring the space of structural modifications, therefore \ndoes not cover the complete space of solutions. Often more general transformations beyond the nature of MMPs \nare feasible and/or necessary, e.g. simultaneous modifications of the starting molecule at different places including \nthe core scaffold. This study aims to provide a general methodology that offers more general structural modifications \nbeyond MMPs. In particular, the same Transformer architecture is trained on different datasets. These datasets consist \nof a set of molecular pairs which reflect different types of transformations. Beyond MMP transformation, datasets \nreflecting general structural changes are constructed from ChEMBL based on two approaches: Tanimoto similarity \n(allows for multiple modifications) and scaffold matching (allows for multiple modifications but keep the scaffold \nconstant) respectively. We investigate how the model behavior can be altered by tailoring the dataset while using the \nsame model architecture. Our results show that the models trained on differently prepared datasets transform a given \nstarting molecule in a way that it reflects the nature of the dataset used for training the model. These models could \ncomplement each other and unlock the capability for the chemists to pursue different options for improving a start-\ning molecule.\nKeywords: Molecular optimization, Matched molecular pairs, Transformer, Tanimoto similarity, Scaffold, ADMET\n© The Author(s) 2022. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco \nmmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article, unless otherwise stated in a credit line to the data.\nIntroduction\nMolecular optimization aims to improve the property \nprofile of a starting molecule. It plays an important role in \nthe drug discovery and development process. However, \nthis problem is challenging due to (i) the requirement \nof simultaneous optimization of multiple, often conflict -\ning properties, e.g. physicochemical properties, ADMET \n(absorption, distribution, metabolism, elimination and \ntoxicity) properties, safety and potency against its target \nand (ii) the large chemical space [1] to explore. Tradition-\nally, chemists use their knowledge, experience and intui -\ntion [2] to apply chemical transformations to the starting \nmolecule, to design improved molecules that have a bal -\nance of multiple properties. However, it heavily relies on \nchemist’s knowledge and is often impacted by individual‘s \nbiases. This can limit the design process and the oppor -\ntunities to find improved molecules within a reasonable \ntime scale.\nRecently, various deep learning methods have \nbeen used and proposed for de novo molecular \ndesign, e.g.  recurrent neural networks (RNNs)  [3 –5], \nOpen Access\nJournal of Cheminformatics\n*Correspondence:  jiazhen.he@astrazeneca.com\n1 Molecular AI, Discovery Sciences, R&D, AstraZeneca, Gothenburg, \nSweden\nFull list of author information is available at the end of the article\nPage 2 of 14He et al. Journal of Cheminformatics           (2022) 14:18 \nvariational autoencoders (VAEs)  [6 –11] and genera -\ntive adversarial networks (GANs) [12– 15]. To improve \nthe generated molecules towards desirable properties, \nreinforcement learning  [12, 13, 15, 16], adversarial \ntraining  [17– 19], transfer learning  [3 ] and different \noptimization techniques [6 , 20] have been used. Con -\nditional generative models  [8 , 11, 21, 22] have also \nbeen proposed where the desirable properties are \nincorporated as condition to directly control the gen -\nerating process. However, most of them focus on gen -\nerating molecules from scratch. There are only a few \nstudies on generating molecules with desirable prop -\nerties from a given starting molecule, which aim to \nsolve the molecular optimization task directly. Most \nof them use a set of molecular pairs for training. Jin \net al. [ 17, 23, 24] utilized molecular graph represen -\ntations and viewed the molecular optimization prob -\nlem as a graph-to-graph translation problem. He et al.  \n[25, 26] instead utilized the string-based representa -\ntion, the simplified molecular-input line-entry system \n(SMILES)  [27] and employed the machine transla -\ntion models  [28, 29] from natural language process -\ning (NLP). They trained machine translation models \n(Transformer and Seq2Seq) to mimic the chemist’s \napproach of using MMPs  [30, 31] where two mole -\ncules differ by a single chemical transformation. It was \nshown that the Transformer performs better than the \nSeq2Seq and HierG2G architectures [24].\nApplication of MMPs is a widely used design strat -\negy by medicinal chemists due to its interpretable \nand intuitive nature. However, MMPs are inherently \nlimited in terms of structural modifications relevant \nfor molecular optimization. From chemist’s perspec -\ntive, there could be need for transformations that \nextend beyond the reach and capabilities of MMPs, \nsuch as simultaneous modifications of the molecule \nat multiple points or modifications of the core scaf -\nfold. Moreover, such modifications are often needed \nto reach the optimization goals. In this study, the \nsame Transformer architecture is trained on different \ndatasets. These datasets consist of a set of molecu -\nlar pairs, and are prepared to reflect different types \nof transformations. To capture more general trans -\nformations beyond MMPs, two approaches are used \nto extract molecular pairs from ChEMBL: Tanimoto \nsimilarity (allows for multiple modifications) and scaf -\nfold matching  [32] (allows for multiple modifications \nbut keeps the scaffold constant) respectively. The goal \nof this study is not necessarily to benchmark against \nMMPs but instead to provide more general structural \nmodifications than only MMPs. This could unlock the \ncapability for the chemists to pursue different options \nfor improving a starting molecule.\nMethods\nFollowing [25], the SMILES representation of mol -\necule and the Transformer model from NLP are used \nin our study. The Transformer model is trained on a set \nof molecular pairs together with the property changes \nbetween source and target molecules. Figure  1 shows \nan example of source and target sequences which are \nfed into the Transformer model. The input consists of \nproperty constraint and source molecule’s SMILES. The \nproperty constraint specifies how to change the source \nmolecule.\nGiven a set of molecular pairs {(X, Y, Z)} where X rep-\nresents source molecule, Y represents target molecule, \nand Z represents the property change between source \nmolecule X and target molecule Y, the Transformer \nmodel will learn a mapping (X, Z) ∈X ×Z → Y ∈Y \nduring training where X × Z represents the input space \nand Y represents the target space. During testing, given \na new (X, Z) ∈ X × Z , the model will be expected to \ngenerate a diverse set of target molecules with desirable \nproperties [25].\nProperties optimized\nThree ADMET properties, logD, solubility and clearance \nwhich are important properties of a drug are selected \nto be optimized simultaneously. LogD is measured as \na compound’s distribution coefficient between octanol \nand water at pH 7.4, based on the shake flask approach. \nSolubility is measured by the generation of a saturated \nFig. 1 Input and output of the Transformer model (following [25]). \nThe input is the concatenation of property change tokens and the \nSMILES of the starting molecule. During training, the output is the \ntarget molecule with the desirable properties while during inference \nthe output is generated token by token and is expected to satisfy the \nproperty constraint in the input\nPage 3 of 14\nHe et al. Journal of Cheminformatics           (2022) 14:18 \n \nsolution of the compound, followed by assaying the solu -\ntion using high-performance liquid chromatography \n(HPLC) with ultra violet (UV) quantification and mass \nspectrometry (MS) identification. The measured unit of \nsolubility is µ M. For clearance, human liver microsome \nintrinsic clearance (HLM CLint) is measured, and the \nunit is µL/min/mg. The measured in-house property data \nwas used to build the property prediction models. These \nmodels were then applied to the processed molecules in \nChEMBL to derive the data used for training the Trans -\nformer model. They are also used to estimate the proper -\nties of the generated molecules from the model. Details \ncan be found in Section ADMET Property Prediction \nModel.\nTokenizing SMILES and property changes\nThe Transformer model takes a sequence of tokens as \ninput. Therefore the SMILES and property changes \nneed to be tokenized to be recognized by the model. The \nSMILES is tokenized based on a single character with the \nexception of two-character tokens (i.e.  , “Cl” and “Br”) \nand tokens between brackets (e.g.  “[nH]” and “[O-]”). \nThe tokenization was performed independently for each \ndataset.\nConsidering practical desirable criteria and experi -\nmental errors, solubility and clearance changes are \nencoded using three categories, while the change in \nlogD is encoded into range intervals, with each inter -\nval length= 0.2 except for the two open intervals on \nthe sides (Table  1). The threshold for low/high solubil-\nity is 50 µ M (1.7 in log10 scale), and the threshold for \nlow/high clearance is 20 µL/min/mg respectively (1.3 \nin log10 scale). These property change tokens can be \nderived from the given input molecule’s properties and \nthe target desirable properties. For example, if an input \nmolecule’s solubility value is 10 µ M and the target \ndesirable solubility value is 80 µ M, then the encoded \nproperty change token would be “Solubility_low → \nhigh” .\nThe vocabulary consists of all the tokens after per -\nforming the tokenization on all the SMILES and \nproperty changes of the molecular pairs in a dataset. \nAdditionally, special tokens, start  and end are added to \nsignal the beginning and ending of a sequence.\nTransformer neural network\nThe same Transformer neural network in [25, 29] is used \nin this study. The Transformer consists of an encoder \nand a decoder. The network takes a sequence of tokens \nas input. Each token is converted into an embedding vec -\ntor–a numerical representation of the token that can be \nprocessed by the network. The input tokens are fed into \nthe network simultaneously. To capture the order infor -\nmation of the input tokens, positional encoding is per -\nformed on the embedding vectors. The resulting vectors \nare then passed through the encoder. The encoder is a \nstack of encoder layers, which process their input itera -\ntively one layer after another. Each encoder layer converts \nits input (a sequence of vectors) into another sequence \nof vectors called encodings. These encodings are passed \nto the next encoder layer as input. The decoder is a stack \nof decoder layers of the same number as encoder. It does \nthe opposite of the encoder: convert the encoder encod -\nings into a sequence of tokens one token at a time. The \nattention mechanism is utilized in both encoder and \ndecoder to encode or decode a current vector consid -\nering the importance of other vectors in the sequence. \nMore details about the Transformer architecture can be \nfound in [25, 29].\nTable 1 Property change encoding\nProperty Measured unit Threshold Threshold in log10 scale Designed property change tokens\nLogD - - - LogD_change_(− inf, − 6.9]\n...\nLogD_change_(− 0.3, − 0.1]\nLogD_change_(− 0.1, 0.1]\nLogD_change_(0.1, 0.3]\n...\nLogD_change_(6.9, inf ]\nSolubility µM low: ≤50 low: ≤1.7 Solubility_low→high\nhigh: >50 high: >1.7 Solubility_high→low\nSolubility_no_change\nClearance µL/min/mg low: ≤20 low: ≤1.3 Clearance_low→high\nhigh: >20 high: >1.3 Clearance_high→low\nClearance_no_change\nPage 4 of 14He et al. Journal of Cheminformatics           (2022) 14:18 \nModel training and sampling\nThe same Transformer architecture was trained with \neach dataset. Each model was trained on a single GPU \n(either NVIDIA GeForce RTX 2080 Ti or NVIDIA Tesla \nK80). The hyperparameters were set the same as [25]. \nThe models were trained using a batch size of 128, Adam \noptimizer and the original learning rate schedule  [29] \nwith 4000 warmup steps. More details about the hyper -\nparameters can be found in Additional file 1: Table S1.\nAfter training, the model can be used to generate \nsequences given an input sequence. The sequence of \ntokens are generated one token at a time. At the first \ntime step, the decoder takes the start  token together \nwith the encoder outputs as input, and samples an out -\nput token from the produced probability distribution \nover all the tokens in the vocabulary. The next time step \nwill take all previous generated tokens and the encoder \noutputs as input. This process will continue until the end \ntoken is generated or a pre-defined maximum length of \nsequence is reached. To allow for the generation of multi-\nple sequences, multinomial sampling is used.\nData preparation\nThe datasets1 consist of a set of molecular pairs extracted \nfrom ChEMBL 28  [33]. In particular, the pairs were \nextracted from the molecules that are originated from \nthe same publication since the molecules are more likely \nto be in the same project. Therefore, the molecular pairs \nare more likely to reflect the chemist’s intuition. The mol-\necules, publications and molecular pairs are processed in \nthe following fashion,\nMolecule pre-processing\n• Standardization using MolVS  2: Keep uncharged \nversion of the largest fragment; Sanitize; Remov -\neHs; Disconnect metals; Apply normalization rules; \nReionize acids; Keep sterochemistry\n• 10 ≤ Number of heavy atoms ≤ 50\n• Number of rings > 0\n• AZFilter=“CORE” [34] to filter out low-quality com -\npounds\n• Substructure filters [35] for hit triaging with Severity-\nScore<10 3.\n• Each molecule’s property values are within 3 stand -\nard deviations of all molecules’ property values (pre -\ndicted)\nPublication pre-processing\n• Year ≥ 2000\n• 10 ≤ Number of molecules ≤ 60\nMolecular pair pre-processing\n• Remove duplicated pairs (keep the earliest reported)\n• Include reverse pairs\nThe resulting statistics on the data after performing the \nsteps above can be found in Additional file 1: Figure S1.\nConstructing molecular pairs\nTo capture different types of transformations, the follow-\ning criteria are considered for extracting the pairs from \ndifferent perspectives.\nMMP. The matched molecular pairs are two molecules \ndiffer by a single transformation, which has been widely \nused as a strategy by medicinal chemists to support \nmolecular optimization. Here, the MMPs are extracted \nusing mmpdb, an open-source matched molecular pair \ntool [36]. The ratio between the number of heavy atoms \n(non-hydrogen atoms) in the R-group and the number \nof heavy atoms in the entire molecule is not greater than \n0.33 [37].\nTo capture more general transformations (e.g. multiple \nmodifications), apart from single transformations, the \nfollowing criteria are used,\nTanimoto similarity. The Tanimoto similarity is com -\nputed based on Morgan Fingerprint with radius=2 \n(ECFP4) using RDKit. Figure  2 shows the distribution of \nTanimoto similarity between all the possible unique pairs \noriginating from the same publication. We extract the \nmolecular pairs based on the following thresholds,\nFig. 2 Tanimoto similarity distribution considering all the possible \nunique pairs with the same publication\n1 https:// doi. org/ 10. 5281/ zenodo. 63198 21.\n2 https:// molvs. readt hedocs. io/ en/ latest/.\n3 https:// github. com/ rdkit/ rdkit/ tree/ master/ Contr ib/ NIBRS ubstr uctur \neFilt ers.\nPage 5 of 14\nHe et al. Journal of Cheminformatics           (2022) 14:18 \n \n• Similarity ( ≥0.5) for similar molecules\n• Similarity ([0.5,0.7)) for medium similar molecules\n• Similarity ( ≥0.7) for highly similar molecules\nScaffold matching. For the molecules originating from \nthe same publication, if two molecules share the same \nscaffold then they are extracted as pairs. In particular, \nthe Murcko scaffold from RDKit which removes the side \nchains and the Murcko scaffold generic which converts \nall atom types to C and all bonds to single are used. The \ntop 20 frequently occurring scaffold and generic scaffold \ncan be found in Additional file 1: Figures S2 and S3.\nTable  2 shows the resulting datasets (all datasets \ninclude reverse pairs). The training, validation and test \nsets are split based on the year of the publications from \nwhich the pairs are extracted. The Transformer neural \nnetwork is trained on each dataset, and is expected to \ntransform the input molecule in a way that it reflects the \nnature of the dataset used for training the model.\nADMET property prediction model\nThe input of our Transformer model takes the property \nchanges of molecular pairs into account. The property \npredictive models were built by using a message passing \nneural network [38]. Since the public data in ChEMBL on \nthe properties of interest was scarce, we resorted to using \nin-house data instead. The solubility and clearance data \nare transformed to log10 scale. The resulting models were \nused as a source of ground truth for deriving the training \ndata. They were also used for evaluating the properties of \nthe output from the Transformer model. Experimental \nverification would have been an expensive alternative and \nfor the illustrative purposes of our work, we found that \na simulated alternative of a wet lab experiment would \nbe sufficient. Table  3 shows the train and test size, root-\nmean-square error (RMSE), normalized RMSE (NRMSE) \nand R2 for each property prediction model.\nExperimental settings\nFor each starting molecule in the test set, 10 unique valid \nmolecules, which are different from the starting mole -\ncule, were generated using multinomial sampling.\nEvaluation metrics\nThe models are evaluated in two main aspects,\n• Successful property constraints gives the per -\ncentage of generated molecules that fulfill the three \ndesirable properties specified by model input simul -\ntaneously. The ADMET property prediction model \nin Table 3 is used to compute the properties of gen -\nerated molecules. Following  [25], the model error \n(Test RMSE in Table  3) is considered to determine \nif a generated molecule satisfies its desirable prop -\nerties. For logD, the generated molecules with \n|logDgenerated − logDtarget|≤ 0.4 will be considered \nas satisfying desirable logD constraint. For solubility, \nthe threshold for low and high will be a range con -\nsidering the model error, i.e. 1.7±0.6. The generated \nmolecules with solubility≤ 2.3 will be considered as \nlow, and those with solubility≥ 1.1 will be consid -\nered as high. Similarly, for clearance , the threshold is \n1.3±0.35.\n• Successful structure constraints gives the percent -\nage of generated molecules that when comparing \nTable 2 Dataset\nDatasets Training (2000-2017) Validation (2018) Test (2019-2020)\nMMPs 2,287,588 143,978 166,582\nSimilarity ( ≥0.5) 6,543,684 418,180 475,070\nSimilarity ([0.5,0.7)) 4,543,472 286,682 327,606\nSimilarity ( ≥0.7) 2,000,212 131,498 147,464\nScaffold 2,850,180 171,914 199,786\nScaffold generic 4,127,058 255,580 289,034\nTable 3 Property prediction model performance on in-house \ndata\nLogD Solubility Clearance\nTrain size 186,575 197,988 155,652\nTrain RMSE 0.295 0.489 0.271\nTrain NRMSE 0.025 0.056 0.053\nTrain R2 0.942 0.775 0.760\nTest size 20,731 21,999 17,295\nTest RMSE 0.395 0.600 0.352\nTest NRMSE 0.038 0.076 0.091\nTest R2 0.897 0.659 0.555\nPage 6 of 14He et al. Journal of Cheminformatics           (2022) 14:18 \nwith their corresponding starting molecules, have the \nsame structure constraints as the pairs in the train -\ning set. This differs according to datasets, e.g.  for \nthe MMPs dataset, this metric gives the percentage \nof generated molecules that are matched molecular \npairs with their starting molecules while for the Simi-\nlarity ( ≥0.5) dataset, the structure constraint is that \nthe Tanimoto similarity between the generated mol -\necules and their corresponding starting molecules \nis between 0.5 and 1.0. This metric evaluates if the \nmodel has learned to use the type of transformation \nreflected in the training set to modify starting mol -\necules.\nBaselines\nWe compare our model Transformer with the following \nbaselines,\n• Transformer-U is the unconditional Transformer \narchitecture trained on molecular pairs but without \nany input property constraints.\n• Random randomly selects 10 molecules (for a direct \ncomparison with our Transformer model where \n10 molecules are generated) from the unique set of \nmolecules in the test set that have the same structure \nconstraint as the training set. For example, for the \nScaffold dataset, it randomly select 10 molecules that \nshare the same scaffold with the given starting mol -\necule. Since it is computationally expensive to evalu -\nate all the samples (each sample consist of a starting \nmolecule desirable property changes) in the test set, \nwe randomly select 1% of the test set, repeat 5 times \nwith different sampling seeds and report the aver -\nage results. Note the Random baseline will always \ngive 100% successful structure constraints due to its \nnature of fulfilling the structure constraints.\nResults and discussion\nData statistics\nFigure  3 shows the overlap of training molecular pairs \namong different datasets. Almost all the MMPs are in \nthe dataset of pairs with Similarity ( ≥0.5). The over -\nlap between the MMP dataset and the Similarity ( ≥0.7) \ndataset is bigger than the one between the MMP dataset \nand the Similarity ([0.5,0.7)) dataset. Exemplar molecu -\nlar pairs only in dataset Similarity ( ≥0.5) show that the \nscaffold is changed compared to pairs sharing generic \nFig. 3 Overlap of training molecular pairs among different datasets. Exemplar molecular pairs are shown for data only in dataset Similarity ( ≥0.5), \nscaffold generic and MMP respectively\nPage 7 of 14\nHe et al. Journal of Cheminformatics           (2022) 14:18 \n \nscaffold and are non-MMPs because of multiple modi -\nfications and/or big change in R-group. The molecular \npairs only in scaffold generic have Tanimoto similarity \nbelow 0.5. A tiny proportion of MMPs have Tanimoto \nsimilarity below 0.5 and change the scaffold.\nPerformance comparison with baselines\nTable 4 compares our Transformer model with the base -\nlines (Transformer-U and Random) in terms of successful \nproperty and structure constraints on different datasets. \nTransformer outperforms Transformer-U and Random \nin terms of successful property constraints, generating \nmore molecules with desirable properties on all datasets. \nFor the successful structure constraints, Transformer-U \nis comparable or better than Transformer. Transformer-\nU has learned to generate “similar” molecules to the given \ninput starting molecules. However, it generates much less \nmolecules with desirable properties compared to Trans -\nformer. It is mainly because Transformer-U was trained \nonly on molecular pairs, and does not include the prop -\nerty change of the pairs in the input, while Transformer \nhaving the property changes as additional input, allows \nfor more directed output generation. Both Transformer \nand Transformer-U outperform the Random baseline—\nfinding more molecules that satisfy desirable properties \nand structure constraint simultaneously.\nFigure  4 compares the Tanimoto similarity distribu -\ntion of the molecular pairs from the training set with \nthe one between the generated molecules and their \nstarting molecules from the test set for the Transformer \nmodel. It can be seen that the distribution of the gen -\nerated pairs align well with the pairs from the training \nset for most datasets. This indicates that the model has \nlearned to transform a given starting molecule in a way \nthat it reflects the nature of the training data. For the \ndatasets based on Tanimoto similarity, the alignment \nis worse, but the model systematically generates mole -\ncules that fulfil the successful property constraints. This \ncan be seen from the areas (lightcyan) that are outside \nthe constrains of the training set (red). This also indi -\ncates the model can extrapolate the learning beyond \nthe structure constraints defined by the training data. \nAdditionally, the overlap between the Tanimoto simi -\nlarity distribution of molecular pairs from the train -\ning set (red) and the one from the test set (yellow) for \nthe scaffold-based datasets is slightly worse than the \noverlap for the MMP dataset in Fig.  4. However, from \nTable  4, the models trained on scaffold-based data -\nsets perform better than the one trained on the MMP \ndataset in terms of fulfilling successful structure con -\nstraints. This might be because the structural changes \nwith MMPs are in general smaller than the ones with \nTable 4 Performance comparison of Transformer and baselines in terms of successful property constraints, successful structure \nconstraints and both metrics simultaneously\nThe results in bold indicate the best values; higher values are better\n Each model is trained on the corresponding dataset for that row\nDataset Model Successful property \nconstraints (%)\nSuccessful structure \nconstraints (%)\nSuccessful property and \nstructure constraints (%)\nMMP Transformer 61.90 91.55 58.09\nTransformer-U 33.67 93.25 31.85\nRandom 13.44±0.43 100 13.44±0.43\nSimilarity ( ≥0.5) Transformer 51.83 82.30 44.53\nTransformer-U 29.04 83.63 25.32\nRandom 15.17±0.27 100 15.17±0.27\nSimilarity ([0.5,0.7)) Transformer 46.75 68.09 32.96\nTransformer-U 26.23 69.13 18.72\nRandom 14.57±0.37 100 14.57±0.37\nSimilarity ( ≥0.7) Transformer 65.09 82.68 56.07\nTransformer-U 39.57 84.83 34.70\nRandom 11.48±0.29 100 11.48±0.29\nScaffold Transformer 61.53 95.32 59.69\nTransformer-U 37.16 95.69 36.26\nRandom 17.22±0.74 100 17.22±0.74\nScaffold generic Transformer 55.05 96.01 53.66\nTransformer-U 32.55 96.30 31.69\nRandom 16.48±0.41 100 16.48±0.41\nPage 8 of 14He et al. Journal of Cheminformatics           (2022) 14:18 \nscaffold-based pairs, which tends to keep the Tanimoto \nsimilarity higher. On the other hand, it is relatively easy \nfor the model trained on molecular pairs sharing the \nsame scaffold to maintain the same scaffold while intro -\nducing multiple modifications. For the model trained \non MMPs, the modification has to be a single and a \nsmall transformation in order to fulfill the successful \nstructure constraint.\nPerformance comparison of models trained on different \ntypes of molecular pairs\nWith the following experiments, we evaluate how the \nmodels trained on different types of molecular pairs per -\nform on the same test sets. Table  5 shows the results on \nthe restricted intersection test set which is the intersec -\ntion of MMP , Similarity ( ≥0.5) and Scaffold generic test \nFig. 4 Tanimoto similarity distribution for Similarity (≥ 0.5) dataset, Similarity ([0.5,0.7)) dataset, Similarity (≥ 0.7) dataset, MMP dataset, Scaffold \ndataset and Scaffold generic dataset. Legend Train for the molecular pairs from the training set; Generated desirable property for the pairs \nbetween the generated molecules that fulfil successful property constraints and their starting molecules from the test set; Generated desirable \nproperty+structure for the pairs between the generated molecules that fulfil both successful property and structure constraints and their starting \nmolecules from the test set; Generated desirable property¬structure for the pairs between the generated molecules that fulfil successful property \nbut not structure constraints and their starting molecules from the test set\nTable 5 Performance comparison of the Transformer models trained on different types of molecular pairs on the restricted \nintersection test set (numbers in bracket represent the absolute increase or decrease compared to the corresponding Transformer \nmodel performance on the original test set in Table 4)\nThe extremes (best/worst performance or largest/smallest change) are highlighted in bold\nTest set Type of molecular pairs where \nTransformer is trained\nSuccessful property \nconstraints (%)\nSuccessful structure \nconstraints (%)\nSuccessful property and \nstructure constraints (%)\nMMP 65.71 ( ↑ 3.81) 91.68 ( ↑ 0.13) 61.82 ( ↑ 3.73)\nSimilarity ( ≥0.5) 55.55 ( ↑ 3.72) 84.47 ( ↑ 2.17) 48.97 ( ↑ 4.44)\nRestricted Similarity ([0.5,0.7)) 50.17 ( ↑ 3.42) 68.66 ( ↑ 0.57) 35.28 ( ↑ 2.32)\nintersection Similarity ( ≥0.7) 65.39 ( ↑ 0.30) 81.49 ( ↓ 1.19) 55.55 ( ↓ 0.52)\nScaffold 62.91 ( ↑ 1.38) 94.42 ( ↓ 0.90) 60.70 ( ↓ 1.01)\nScaffold generic 59.07 ( ↑ 4.02) 96.14 ( ↑ 0.13) 57.68 ( ↑ 4.02)\nPage 9 of 14\nHe et al. Journal of Cheminformatics           (2022) 14:18 \n \nsets. Details about the test sets, and the results for other \ntest sets can be found in Additional file 1 (p.6-7).\nThe model trained on MMP dataset performs best in \nterms of successful property constraints, followed closely \nby the one trained on Similarity ( ≥0.7) dataset, while the \nmodel trained on Similarity ([0.5, 0.7)) dataset performs \nworst. This might be because the molecular pairs in the \nrestricted intersection test set have smaller structural \nchanges and desired property changes, and it is easier \nto achieve small desirable property changes by making \nsmall structural changes. It might also be because of the \nvarying performance of the models trained on differ -\nent types of molecular pairs in the beginning (Table  4). \nTherefore we also report the difference (numbers in \nbracket) compared to their performance on their original \ntest sets (Table 4). We can see that most models perform \nbetter compared to the performance on their own origi -\nnal test set, indicating this restricted intersection test set \nis an relative easy task. The performance change of the \nmodels trained on Similarity ( ≥0.7) and Scaffold are very \nsmall, indicating there is not much difference between \nthis restricted dataset and their own original test set in \nterms of difficulty.\nFigure 5a shows how the training molecular pairs from \ndifferent datasets correlate with each other. For example, \n40% of MMPs (row) are also pairs with Similarity ([0.5, \n0.7)) (column) but only 20% of pairs with Similarity ([0.5, \n0.7)) (row) are MMPs (column). Figure  5b shows that for \nthe restricted intersection test set, how the generated \nmolecules from models trained on different datasets sat -\nisfy different structure constraints. For example, among \nthe generated molecules (that satisfy the property con -\nstraints and structure constraints, i.e.  Similarity ([0.5, \n0.7))) from the model trained on Similarity ([0.5, 0.7)) \n(row), 22% of them are MMPs when comparing with \ntheir corresponding starting molecules. Compared to \nthe heatmap for the training set, the one for Restricted \nintersection test set basically follow the same pattern \n(similar patterns are found on other test sets), indicating \nthe models have learned to modify the starting molecules \nin the way that it reflects the nature of the training set. \nOverall, it is shown that there is no single model generat -\ning molecules that cover the ones from all other models. \nIt could be beneficial to use an ensemble of these mod -\nels which complement each other to provide different \n(a) Relationshipb etw een the trainingmolecular\npairsof diﬀerent datasets\n(b) Generated moleculesf romd iﬀerent mo dels\n(row) intermso fsatisfyingdiﬀerent structurecon-\nstraints (column )\nFig. 5 Comparison of heatmaps for training set and test set. The more similar, the better. a Relationship between the training molecular pairs \nof different datasets, e.g. the number 0.2 with Similarity ([0.5, 0.7)) as row and MMP as column from the training set represents 20% of the pairs \nwith Similarity ([0.5, 0.7)) are also MMPs. b Each row represents the model trained on the corresponding dataset, and each column represents the \ncorresponding structure constraints. The number 0.22 with Similarity ([0.5, 0.7)) as row and MMP as column from the Restricted intersection test \nset represents that when looking at the generated molecules using the Transformer model trained on Similarity ([0.5, 0.7)) dataset, among all the \nones fulfilling the the property constraints and structure constraints (i.e. Similarity ([0.5, 0.7))), 22% of them are MMPs. The diagonal for the Restricted \nintersection is always 1 because we only look at the generated molecules that already fulfil the property constraints and structure constraints\nPage 10 of 14He et al. Journal of Cheminformatics           (2022) 14:18 \noptions to transform a starting molecule towards desir -\nable properties.\nPerformance on test sets with large property changes \ndesired\nWith the following experiments, we evaluate how the \nmodels trained on different types of molecular pairs per -\nform on the test sets where large property changes (logD \nchange is above 1; solubility and clearance change is \neither low→high or high →low) are desired. The molec -\nular pairs in the original test sets where large property \nchanges are extracted and merged excluding duplicates. \nTable 6 shows that 4.6% (highest) of the Similarity ([0.5, \n0.7)) dataset has large property changes desired while \nSimilarity ( ≥0.7) dataset has the lowest, 2.3%. It is rea -\nsonable because it is less likely to have large property \nchanges while keeping higher structural similarity.\nTable 7 shows the results on the merged dataset (the \nresults on other datasets in Table  6 can be found in \nAdditional file  1: Table  S5). All models perform worse \ncompared to their performance on their original test \nset (Table 4). The reason is that only a small proportion \nof molecular pairs having large property changes in the \ntraining set (Additional file  1: Figure S4), therefore the \nmodels generalize less well on such pairs. Intuitively, it \nwould be expected that the model trained on Similarity \n([0.5, 0.7)) dataset would perform best since it has higher \npercentage of pairs with large property changes for train -\ning and have more freedom to modify the starting mol -\necule. However, it is observed that the model trained on \nMMPs performs best. This might be because it is easier \nto train the Transformer model for MMPs compared to \npairs with similarity ([0.5, 0.7)) (already seen in Table  4) \ndue to the smaller extrapolated space. Having that said, \nthe performance of the models trained on different \ntypes of molecular pairs differ less on this Merged test \nset where big property changes are desired compared to \nprevious test sets ( (Table  4 and Table  5). When looking \nat the numbers in bracket, we observed that the perfor -\nmance of model trained on Similarity ([0.5, 0.7)) drop the \nleast, while the one for Similarity ( ≥0.7) drop the most, \nfollowed by Scaffold and MMP .\nExample of diverse molecules generated using models \ntrained on different types of molecular pairs\nFigures 6 and 7 show an example of the generated mol -\necules that fulfill the desirable properties but modify the \nstarting molecule in different ways depending on the \ntraining data used for training the model. In particular, \nthe generated molecules in Fig.  6b make a single trans -\nformation to the starting molecule while the ones in \nFig. 7c and 7d allow for multiple modifications but keep \nthe scaffold or generic scaffold constant. The generated \nmolecules in Fig.  6c, 6d and 7b allow for multiple modi -\nfications and changes in scaffold, but the Tanimoto simi -\nlarity lies approximately [0.5, 1.0], [0.7, 1.0] and [0.5, 0.7) \nrespectively. Overall, this shows the flexibility of modify -\ning starting molecules to achieve desirable properties in \ndifferent ways by using the models trained on different \ntypes of molecular pairs.\nTable 6 Test sets where big property changes (logD change is \nabove 1; solubility and clearance change is either low →high or \nhigh→low) are desired\nSize indicates the number of data points where big property change are desired; \nPercentage indicates the fraction of the original test set in Table 2 with data \npoints that have big property changes, e.g. 6180/166582≈3.7%\nTest set Size Percentage \n(%)\nMMP 6,180 3.7\nSimilarity ( ≥0.5) 18,546 3.9\nSimilarity ([0.5, 0.7)) 15,130 4.6\nSimilarity ( ≥0.7) 3,416 2.3\nScaffold 6,252 3.1\nScaffold generic 10,514 3.6\nMerged 21,652 -\nTable 7 Performance comparison of Transformer models trained on different types of molecular pairs on the Merged dataset where \nbig property changes are desired (numbers in bracket represent the absolute increase/decrease compared to the corresponding \nTransformer model performance on the original test set in Table 4)\nThe extremes (best/worst performance or largest/smallest change) are highlighted in bold\nTest set Type of molecular pairs where \nTransformer is trained\nSuccessful property \nconstraints (%)\nSuccessful structure \nconstraints (%)\nSuccessful property and \nstructure constraints (%)\nMMP 40.82 ( ↓ 21.08) 83.89 ( ↓ 7.66) 36.12 ( ↓ 21.97)\nSimilarity ( ≥0.5) 39.81 ( ↓ 12.02) 75.00 ( ↓ 7.30) 30.70 ( ↓ 13.83)\nMerged Similarity ([0.5,0.7)) 38.33 ( ↓ 8.42) 66.64 ( ↓ 1.45) 25.94 ( ↓ 7.02)\nSimilarity ( ≥0.7) 36.14 ( ↓ 28.95) 68.57 ( ↓ 14.11) 25.58 ( ↓ 30.49)\nScaffold 36.50 ( ↓ 25.03) 89.17 ( ↓ 6.15) 33.60 ( ↓ 23.09)\nScaffold generic 37.78 ( ↓ 17.27) 91.30 ( ↓ 4.71) 35.26 ( ↓ 18.40)\nPage 11 of 14\nHe et al. Journal of Cheminformatics           (2022) 14:18 \n \nDiscussion\nVarying performance of models trained on different types \nof molecular pairs\nThe Transformer models trained on different datasets \nshow varying performance as shown in Table  4. For the \nMMP , scaffold and scaffold generic datasets, it is easier to \ngenerate molecules in terms of successful structure con -\nstrains (MMPs, sharing same scaffold) compared to the \ndatasets based on Tanimoto similarity split. This might \nbe because the pairs in the Tanimoto similarity based \ndatasets have more variations, and the models have more \nfreedom to extrapolate which makes it difficult to keep \nthe same structure constraints. It might also be due to \nthe hard Tanimoto similarity cutoff used for constructing \nthe training set (Fig.  4), which is difficult for the gener -\nated molecules from the Transformer model to follow on.\nIn terms of successful property constrains, Similar -\nity ( ≥0.7) dataset has the best performance, followed by \nMMP and scaffold, which are much better than Similar -\nity ([0.5,0.7)), Similarity ( ≥0.5) and scaffold generic. The \nreason might be that the extrapolated space is larger \nwhich makes it harder to find molecules with desirable \nproperties. It might also be because the molecular pairs \nare more similar and the property changes are smaller for \nSimilarity ( ≥0.7), MMP and scaffold dataset (Additional \nfile 1: Figure S4).\n(a) Starting moleculea nd desirablep roperties\n(b) Generated molecules from model trained on MMPs\n(c) Generated molecules from model trained on pairs with Similarity( ≥0.5)\n(d) Generated molecules from model trained on pairs with Similarity ([0.5,0.7))\nFig. 6 Example of diverse molecules with desirable properties generated by models trained on (b) MMPs (c) pairs with Similarity ( ≥0.5) (d) \npairs with Similarity ([0.5, 0.7)). The changes in the generated molecules compared with starting molecule are highlighted in red. Sim represents \nTanimoto similarity\nPage 12 of 14He et al. Journal of Cheminformatics           (2022) 14:18 \nVarying performance in terms of successful structure \nconstraints and successful property constraints\nIt is observed from Table  4 that the Transformer model’s \nperformance in terms of successful structure constraints \nis better than successful property constraints. This might \nbe because it is a relative easy task to keep the same \nstructure constraint as in the training set. While for suc -\ncessful property constraints, it is more restricted due to \nthe requirement of satisfying three properties simultane -\nously and the logD change is encoded at a higher level of \ngranularity (considering the practical use) compared to \nsolubility and clearance change which only have three \npossible changes (Table  1). This makes the input space \nmore complicated and bigger, which requires more data \nto build a good model and makes it harder to generalize \nwell.\nMolecular optimization beyond MMPs\nThe goal of this study is not necessarily to benchmark \nagainst MMPs, but instead to provide a general meth -\nodology that enables general structural changes beyond \nwhat MMPs are designed for. The application of MMPs \nis a useful concept, but it poses a limitation of explor -\ning a broader chemical space. Often structural modi -\nfications beyond the reach of MMPs are feasible and/\n(a)S tarting molecule and desirable properties\n(b) Generated molecules from model trainedo np airs withS imilarity( ≥0.7)\n(c) Generated moleculesf rom model trained on pairs sharing scaﬀold\n(d) Generated molecules fromm odel trainedo np airs sharing generic scaﬀold\nFig. 7 Example of diverse molecules with desirable properties generated by models trained on b pairs with Similarity ( ≥0.7) c pairs sharing \nscaffold and d pairs sharing generic scaffold. The changes in the generated molecules compared with starting molecule are highlighted in red. Sim \nrepresents Tanimoto similarity\nPage 13 of 14\nHe et al. Journal of Cheminformatics           (2022) 14:18 \n \nor needed to reach optimization goals. The presented \nmethod and results deliver the opportunity of exploring \na broader space of structural modifications for molec -\nular optimization. There is an observed tendency that \nit is more challenging for the model to learn from the \ndatasets with larger structural changes, i .e.  Similarity \n( ≥0.5), Similarity ([0.5, 0.7)) and Scaffold generic. The \nreason might be because the navigated chemical space \nis larger and it is hard to relate the large structural \nchanges to accurate property changes. Nevertheless, \nthese models provide alternatives to MMPs, which is \nuseful when MMPs are not adequate or feasible during \noptimization. This study shows how tailoring the train -\ning datasets can lead to the changes in the behaviour of \nthe resulting trained model. This concept can be extrap -\nolated to any user-specified structure modification.\nConclusions\nWe propose a general methodology to provide more gen-\neral structural transformations beyond MMPs for molecular \noptimization. This can be achieved by tailoring the dataset \naccordingly while using the same model architecture. Dif-\nferent types of dataset (molecular pairs) were extracted from \nChEMBL based on MMPs, Tanimoto similarity and scaf -\nfold matching which result in six datasets: MMPs, Similar-\nity ( ≥0.5), Similarity ([0.5, 0.7)), Similarity ( ≥0.7)), Scaffold \nand Scaffold generic. These datasets reflect different types \nof transformations, and the Transformer neural network \nwas trained on each dataset. Our results showed that it is \nrelatively easy to keep the structure constraints for MMP and \nScaffold-based datasets compared to Tanimoto similarity-\nbased datasets. Furthermore, the models trained on different \ntypes of molecular pairs transform a given starting molecule \nin a way that it reflects the nature of the dataset used for \ntraining the model, e.g. the model trained on MMPs modify \nthe starting molecules by a single transformation, the models \ntrained on similarity based molecular pairs allow for multi-\nple modifications but keep the Tanimoto similarity in certain \nranges, and the model trained on Scaffold-based molecular \npairs allow for multiple modifications but keep the scaffold \nor generic scaffold constant. These models could comple-\nment each other and unlock the capability for the chemists \nto pursue different options for improving a starting mole-\ncule, therefore accelerate the drug discovery process.\nAbbreviations\nMMPs:: Matched molecular pairs; ADMET:: Absorption, distribution, metabolism, \nelimination and toxicity; RNNs:: Recurrent neural networks; VAEs:: Variational autoen-\ncoders; GANs:: Generative adversarial networks; SMILES:: Simplified Molecular-Input \nLine-Entry System; NLP:: Natural language processing; Seq2Seq:: Sequence to \nsequence; HierG2G:: Hierachical graph encoder-decoder; HPLC:: High-performance \nliquid chromatography; UV:: Ultra violet; MS:: Mass spectrometry; HLM CLint:: \nHuman liver microsome intrinsic clearance; RMSE:: Root-mean-square error; \nNRMSE:: Normalized RMSE.\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s13321- 022- 00599-3.\n Additional file 1: Table S1: Hyperparameters for the Transformer model. \nTable S2. Training sets where big property changes (logD change is above \n1; solubility and clearance change is either low→high or high→low) \naredesired. Percentage indicates the fraction of training sets with data \npoints that have big property changes. Table S3. Test sets extracted for \nmodel comparison. Table S4. Performance comparison of the Trans-\nformer models trained on different types of molecular pairs on different \ntest sets (numbers in bracketrepresent the absolute increase or decrease \ncompared to the corresponding Transformer model performance on \nthe original test set in Table 4). Theextremes (best/worst performance or \nlargest/smallest change) are highlighted in bold. Table S5. Performance \ncomparison of Transformer models trained on different types of molecular \npairs on different test sets where big propertychanges are desired \n(numbers in bracket represent the absolute increase/decrease compared \nto the corresponding Transformer model performanceon the original test \nset in Table 4). The extremes (best/worst performance or largest/smallest \nchange) are highlighted in bold. Figure S1. Data statistics after perform-\ning the pre-processing steps (described in Data Preparation section) on \nthe molecules and the publicationsavailable in ChEMBL 28. Publications \nPer Year: the number of publications published per year; Molecules Per \nPublication: the number of moleculesthat are released per publication. \nFigure S2. Top 20 frequently occurring scaffolds in the Scaffold training \nset. Figure S3. Top 20 frequently occurring generic scaffolds in the \nScaffold generic training set. Figure S4. Property change distribution for \ndifferent training datasets. Each tick in the horizontal axis represents the \ncombination of logD, solubility and clint changes. For example, the first \ntick big change; high→ low; high→ low represents the logD change is \nbig change, solubility change ishigh→ low, and clint change is high→ \nlow. For logD change, no change includes (-0.1, 0.1]; small change \nincludes changes below 0.5; medium change includes between 0.5 and \n1; big change includes changes above 1. Figure S5. Overlap of molecular \npairs among different test sets, MMP ,Similarity (≥0.5), Scaffold generic \ndatasets, used for extracting test sets for model comparison.\nAcknowledgements\nJiazhen He thanks the Molecular AI group at AstraZeneca for useful discussion \nand the postdoc program at AstraZeneca.\nAuthors’ contributions\nJiazhen He performed the research. Christian Tyrchan, Werngard Czechtizky \nand Ola Engkvist proposed and supervised the project. All authors provided \nhelpful feedback on the datasets used, experiment and results on the project. \nJiazhen He wrote the manuscript, and all authors read and approved the final \nmanuscript.\nFunding\nNot applicable.\nAvailability of data and materials\nAll source code and datasets used to produce the reported results can be \nfound at https:// github. com/ Molec ularAI/ deep- molec ular- optim izati on/ tree/ \ngener al_ trans forma tion and https:// doi. org/ 10. 5281/ zenodo. 63198 21.\nDeclarations\nCompeting interests\nThe authors declare that they have no competing interests.\nAuthor details\n1 Molecular AI, Discovery Sciences, R&D, AstraZeneca, Gothenburg, Swe-\nden. 2 Medicinal Chemistry, Research and Early Development, Respiratory \nand Immunology (R&I), BioPharmaceuticals R&D, AstraZeneca, Gothenburg, \nSweden. 3 Department of Computer Science and Engineering, Chalmers \nUniversity of Technology, Gothenburg, Sweden. \nPage 14 of 14He et al. Journal of Cheminformatics           (2022) 14:18 \n•\n \nfast, convenient online submission\n •\n  \nthorough peer review by experienced researchers in your ﬁeld\n• \n \nrapid publication on acceptance\n• \n \nsupport for research data, including large and complex data types\n•\n  \ngold Open Access which fosters wider collaboration and increased citations \n \nmaximum visibility for your research: over 100M website views per year •\n  At BMC, research is always in progress.\nLearn more biomedcentral.com/submissions\nReady to submit y our researc hReady to submit y our researc h  ?  Choose BMC and benefit fr om: ?  Choose BMC and benefit fr om: \nReceived: 24 November 2021   Accepted: 11 March 2022\nReferences\n 1. Polishchuk PG, Madzhidov TI, Varnek A (2013) Estimation of the size of \ndrug-like chemical space based on gdb-17 data. J comput Aided Mol Des \n27(8):675–679\n 2. Topliss JG (1972) Utilization of operational schemes for analog synthesis \nin drug design. J Med Chem 15(10):1006–1011\n 3. Segler MH, Kogej T, Tyrchan C, Waller MP (2018) Generating focused \nmolecule libraries for drug discovery with recurrent neural networks. ACS \nCentral Sci 4(1):120–131\n 4. Gupta A, Müller AT, Huisman BJ, Fuchs JA, Schneider P , Schneider G (2018) \nGenerative recurrent networks for de novo drug design. Mol Inform \n37(1–2):1700111\n 5. Bjerrum EJ, Threlfall R (2017) Molecular generation with recurrent neural \nnetworks (RNNs). arXiv preprint arXiv: 1705. 04612\n 6. Gómez-Bombarelli R, Wei JN, Duvenaud D, Hernández-Lobato JM, \nSánchez-Lengeling B, Sheberla D, Aguilera-Iparraguirre J, Hirzel TD, \nAdams RP , Aspuru-Guzik A (2018) Automatic chemical design using a \ndata-driven continuous representation of molecules. ACS Central Sci \n4(2):268–276\n 7. Dai H, Tian Y, Dai B, Skiena S, Song L (2018) Syntax-directed variational \nautoencoder for molecule generation. In: Proceedings of the interna-\ntional conference on learning representations\n 8. Lim J, Ryu S, Kim JW, Kim WY (2018) Molecular generative model based \non conditional variational autoencoder for de novo molecular design. J \nCheminform 10(1):1–9\n 9. Jin W, Barzilay R, Jaakkola T (2018) Junction tree variational autoencoder \nfor molecular graph generation. In: International Conference on Machine \nLearning, pp. 2323–2332\n 10. Liu Q, Allamanis M, Brockschmidt M, Gaunt A (2018) Constrained graph \nvariational autoencoders for molecule design. In: Advances in neural \ninformation processing systems, pp. 7795–7804\n 11. Simonovsky M, Komodakis N (2018) Graphvae: Towards generation of \nsmall graphs using variational autoencoders. In: International conference \non artificial neural networks, pp. 412–422 . Springer\n 12. Guimaraes GL, Sanchez-Lengeling B, Outeiral C, Farias P .L.C., Aspuru-Guzik \nA (2017) Objective-reinforced generative adversarial networks (organ) for \nsequence generation models. arXiv preprint arXiv: 1705. 10843\n 13. Putin E, Asadulaev A, Ivanenkov Y, Aladinskiy V, Sanchez-Lengeling B, Asp-\nuru-Guzik A, Zhavoronkov A (2018) Reinforced adversarial neural com-\nputer for de novo molecular design. J Chem Inf Model 58(6):1194–1204\n 14. Putin E, Asadulaev A, Vanhaelen Q, Ivanenkov Y, Aladinskaya AV, Aliper A, \nZhavoronkov A (2018) Adversarial threshold neural computer for molecu-\nlar de novo design. Mol Pharm 15(10):4386–4397\n 15. De Cao N, Kipf T (2018) MolGAN: An implicit generative model for small \nmolecular graphs. In: ICML 2018 workshop on theoretical foundations \nand applications of deep generative models\n 16. Olivecrona M, Blaschke T, Engkvist O, Chen H (2017) Molecular de-novo \ndesign through deep reinforcement learning. J Cheminform 9(1):48\n 17. Jin W, Yang K, Barzilay R, Jaakkola T (2018) Learning multimodal graph-to-\ngraph translation for molecule optimization. In: International conference \non learning representations\n 18. Kadurin A, Nikolenko S, Khrabrov K, Aliper A, Zhavoronkov A (2017) \ndruGAN: an advanced generative adversarial autoencoder model for de \nnovo generation of new molecules with desired molecular properties in \nsilico. Mol Pharm 14(9):3098–3104\n 19. Blaschke T, Olivecrona M, Engkvist O, Bajorath J, Chen H (2018) Applica-\ntion of generative autoencoder in de novo molecular design. Mol Inform \n37(1–2):1700123\n 20. Winter R, Montanari F, Steffen A, Briem H, Noé F, Clevert D-A (2019) \nEfficient multi-objective molecular optimization in a continuous latent \nspace. Chem Sci 10(34):8016–8024\n 21. Li Y, Zhang L, Liu Z (2018) Multi-objective de novo drug design with \nconditional graph generative model. J Cheminform 10(1):33\n 22. Kotsias P-C, Arús-Pous J, Chen H, Engkvist O, Tyrchan C, Bjerrum EJ (2020) \nDirect steering of de novo molecular generation with descriptor condi-\ntional recurrent neural networks. Nat Mach Intell 2(5):254–265\n 23. Jin W, Barzilay R, Jaakkola T (2019) Hierarchical graph-to-graph translation \nfor molecules. arXiv, 1907\n 24. Jin W, Barzilay R, Jaakkola T (2020) Hierarchical generation of molecular \ngraphs using structural motifs. In: International conference on machine \nlearning, pp. 4839–4848 . PMLR\n 25. He J, You H, Sandström E, Nittinger E, Bjerrum EJ, Tyrchan C, Czechtizky W, \nEngkvist O (2021) Molecular optimization by capturing chemist’s intuition \nusing deep neural networks. J Cheminform 13(1):1–17\n 26. He J, Mattsson F, Forsberg M, Bjerrum E.J., Engkvist O, Tyrchan C, Czech-\ntizky W, et al. (2021) Transformer neural network for structure constrained \nmolecular optimization. In: ICLR 2021 workshop: machine learning for \npreventing and combating pandemics\n 27. Weininger D (1988) Smiles, a chemical language and information system. \n1. introduction to methodology and encoding rules. J Chem Inf Comput \nSci 28(1):31–36\n 28. Sutskever I, Vinyals O, Le Q.V. (2014) Sequence to sequence learning with \nneural networks. In: Advances in neural information processing systems, \npp. 3104–3112\n 29. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez A.N., Kaiser \nŁ, Polosukhin I (2017) Attention is all you need. In: Advances in neural \ninformation processing systems, pp. 5998–6008\n 30. Kenny PW, Sadowski J (2005) Structure modification in chemical data-\nbases. Chemoinform Drug Discov 23:271–285\n 31. Tyrchan C, Evertsson E (2017) Matched molecular pair analysis in short: \nalgorithms, applications and limitations. Comput Structl Biotechnol J \n15:86–90\n 32. Bemis GW, Murcko MA (1996) The properties of known drugs. 1. molecu-\nlar frameworks. J Med Chem 39(15):2887–2893\n 33. Mendez D, Gaulton A, Bento AP , Chambers J, De Veij M, Félix E, Magariños \nMP , Mosquera JF, Mutowo P , Nowotka M et al (2019) Chembl: towards \ndirect deposition of bioassay data. Nucl Acids Res 47(D1):930–940\n 34. Cumming JG, Davis AM, Muresan S, Haeberlein M, Chen H (2013) Chemi-\ncal predictive modelling to improve compound quality. Nat Rev Drug \nDiscov 12(12):948–962\n 35. Schuffenhauer A, Schneider N, Hintermann S, Auld D, Blank J, Cotesta \nS, Engeloch C, Fechner N, Gaul C, Giovannoni J et al (2020) Evolu-\ntion of Novartis’ small molecule screening deck design. J Med Chem \n63(23):14425–14447\n 36. Dalke A, Hert J, Kramer C (2018) mmpdb: an open-source matched \nmolecular pair platform for large multiproperty data sets. J Chem Inf \nModel 58(5):902–910\n 37. Gogishvili D, Nittinger E, Margreitter C, Tyrchan C (2021) Nonadditivity \nin public and inhouse data: implications for drug design. J Cheminform \n13(1):1–18\n 38. Yang K, Swanson K, Jin W, Coley C, Eiden P , Gao H, Guzman-Perez A, Hop-\nper T, Kelley B, Mathea M et al (2019) Analyzing learned molecular repre-\nsentations for property prediction. J Chem Inf Model 59(8):3370–3388",
  "topic": "Chemical space",
  "concepts": [
    {
      "name": "Chemical space",
      "score": 0.8565611839294434
    },
    {
      "name": "chEMBL",
      "score": 0.790611743927002
    },
    {
      "name": "Computer science",
      "score": 0.7502723336219788
    },
    {
      "name": "Virtual screening",
      "score": 0.5275090336799622
    },
    {
      "name": "Drug discovery",
      "score": 0.4997594356536865
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48051226139068604
    },
    {
      "name": "Transformer",
      "score": 0.47264164686203003
    },
    {
      "name": "Cheminformatics",
      "score": 0.4552934467792511
    },
    {
      "name": "Scaffold",
      "score": 0.4455491006374359
    },
    {
      "name": "Data mining",
      "score": 0.4373580813407898
    },
    {
      "name": "Intuition",
      "score": 0.4323369860649109
    },
    {
      "name": "Machine learning",
      "score": 0.34548622369766235
    },
    {
      "name": "Bioinformatics",
      "score": 0.2271149456501007
    },
    {
      "name": "Chemistry",
      "score": 0.15952110290527344
    },
    {
      "name": "Computational chemistry",
      "score": 0.08850905299186707
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210143795",
      "name": "AstraZeneca (Sweden)",
      "country": "SE"
    },
    {
      "id": "https://openalex.org/I66862912",
      "name": "Chalmers University of Technology",
      "country": "SE"
    }
  ],
  "cited_by": 72
}