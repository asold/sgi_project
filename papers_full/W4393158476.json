{
  "title": "AnomalyGPT: Detecting Industrial Anomalies Using Large Vision-Language Models",
  "url": "https://openalex.org/W4393158476",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2224565031",
      "name": "Zhaopeng Gu",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Automation",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2231461358",
      "name": "Bingke Zhu",
      "affiliations": [
        "Institute of Automation",
        "Shandong Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2099149257",
      "name": "Guibo Zhu",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2118530787",
      "name": "Ying-Ying Chen",
      "affiliations": [
        "Shandong Institute of Automation",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2108125403",
      "name": "Ming Tang",
      "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "University of Chinese Academy of Sciences",
        "Institute of Automation"
      ]
    },
    {
      "id": "https://openalex.org/A2097542353",
      "name": "Jin-qiao Wang",
      "affiliations": [
        "Institute of Automation",
        "Beijing Academy of Artificial Intelligence",
        "University of Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2224565031",
      "name": "Zhaopeng Gu",
      "affiliations": [
        "Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2231461358",
      "name": "Bingke Zhu",
      "affiliations": [
        "Institute of Automation",
        "ObjectVideo (United States)",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2099149257",
      "name": "Guibo Zhu",
      "affiliations": [
        "University of Chinese Academy of Sciences",
        "Institute of Automation",
        "Beijing Academy of Artificial Intelligence",
        "Chinese Academy of Sciences"
      ]
    },
    {
      "id": "https://openalex.org/A2118530787",
      "name": "Ying-Ying Chen",
      "affiliations": [
        "Chinese Academy of Sciences",
        "Institute of Automation",
        "ObjectVideo (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2108125403",
      "name": "Ming Tang",
      "affiliations": [
        "Chinese Academy of Sciences",
        "University of Chinese Academy of Sciences",
        "Institute of Automation",
        "Beijing Academy of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2097542353",
      "name": "Jin-qiao Wang",
      "affiliations": [
        "ObjectVideo (United States)",
        "Institute of Automation",
        "University of Chinese Academy of Sciences",
        "Chinese Academy of Sciences",
        "Beijing Academy of Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2948982773",
    "https://openalex.org/W3099658315",
    "https://openalex.org/W6853163053",
    "https://openalex.org/W3183588514",
    "https://openalex.org/W6840256930",
    "https://openalex.org/W4361193508",
    "https://openalex.org/W4281655026",
    "https://openalex.org/W4323557475",
    "https://openalex.org/W3156880769",
    "https://openalex.org/W6742348326",
    "https://openalex.org/W2432481613",
    "https://openalex.org/W6795092660",
    "https://openalex.org/W3169651898",
    "https://openalex.org/W4286380239",
    "https://openalex.org/W4292851291",
    "https://openalex.org/W3173538657",
    "https://openalex.org/W3038008725",
    "https://openalex.org/W6838461383",
    "https://openalex.org/W3092704883",
    "https://openalex.org/W4293518844",
    "https://openalex.org/W4386113288",
    "https://openalex.org/W4289433371",
    "https://openalex.org/W4287236896",
    "https://openalex.org/W3023868590",
    "https://openalex.org/W2962914239",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4312605624",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W4312570668",
    "https://openalex.org/W4378711593",
    "https://openalex.org/W4377164404",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W3159648608",
    "https://openalex.org/W4318718936",
    "https://openalex.org/W4386065385",
    "https://openalex.org/W4287887190",
    "https://openalex.org/W4312298392",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4281643792",
    "https://openalex.org/W2884561390",
    "https://openalex.org/W4318751475",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4366850747",
    "https://openalex.org/W4386071707",
    "https://openalex.org/W4378770571",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3106848223",
    "https://openalex.org/W4386075649"
  ],
  "abstract": "Large Vision-Language Models (LVLMs) such as MiniGPT-4 and LLaVA have demonstrated the capability of understanding images and achieved remarkable performance in various visual tasks. Despite their strong abilities in recognizing common objects due to extensive training datasets, they lack specific domain knowledge and have a weaker understanding of localized details within objects, which hinders their effectiveness in the Industrial Anomaly Detection (IAD) task. On the other hand, most existing IAD methods only provide anomaly scores and necessitate the manual setting of thresholds to distinguish between normal and abnormal samples, which restricts their practical implementation. In this paper, we explore the utilization of LVLM to address the IAD problem and propose AnomalyGPT, a novel IAD approach based on LVLM. We generate training data by simulating anomalous images and producing corresponding textual descriptions for each image. We also employ an image decoder to provide fine-grained semantic and design a prompt learner to fine-tune the LVLM using prompt embeddings. Our AnomalyGPT eliminates the need for manual threshold adjustments, thus directly assesses the presence and locations of anomalies. Additionally, AnomalyGPT supports multi-turn dialogues and exhibits impressive few-shot in-context learning capabilities. With only one normal shot, AnomalyGPT achieves the state-of-the-art performance with an accuracy of 86.1%, an image-level AUC of 94.1%, and a pixel-level AUC of 95.3% on the MVTec-AD dataset.",
  "full_text": "AnomalyGPT: Detecting Industrial Anomalies Using\nLarge Vision-Language Models\nZhaopeng Gu1,2*, Bingke Zhu1,3*, Guibo Zhu1,2† ,\nYingying Chen1,3†, Ming Tang1,2, Jinqiao Wang1,2,3\n1Foundation Model Research Center, Institute of Automation, Chinese Academy of Sciences, Beijing, China\n2School of Artificial Intelligence, University of Chinese Academy of Sciences, Beijing, China\n3Objecteye Inc., Beijing, China\nguzhaopeng2023@ia.ac.cn, {bingke.zhu, gbzhu, yingying.chen, tangm, jqwang}@nlpr.ia.ac.cn\nAbstract\nLarge Vision-Language Models (LVLMs) such as MiniGPT-4\nand LLaV A have demonstrated the capability of understand-\ning images and achieved remarkable performance in various\nvisual tasks. Despite their strong abilities in recognizing com-\nmon objects due to extensive training datasets, they lack spe-\ncific domain knowledge and have a weaker understanding of\nlocalized details within objects, which hinders their effective-\nness in the Industrial Anomaly Detection (IAD) task. On the\nother hand, most existing IAD methods only provide anomaly\nscores and necessitate the manual setting of thresholds to dis-\ntinguish between normal and abnormal samples, which re-\nstricts their practical implementation. In this paper, we ex-\nplore the utilization of LVLM to address the IAD problem\nand propose AnomalyGPT, a novel IAD approach based on\nLVLM. We generate training data by simulating anomalous\nimages and producing corresponding textual descriptions for\neach image. We also employ an image decoder to provide\nfine-grained semantic and design a prompt learner to fine-\ntune the LVLM using prompt embeddings. Our AnomalyGPT\neliminates the need for manual threshold adjustments, thus\ndirectly assesses the presence and locations of anomalies. Ad-\nditionally, AnomalyGPT supports multi-turn dialogues and\nexhibits impressive few-shot in-context learning capabilities.\nWith only one normal shot, AnomalyGPT achieves the state-\nof-the-art performance with an accuracy of 86.1%, an image-\nlevel AUC of 94.1%, and a pixel-level AUC of 95.3% on the\nMVTec-AD dataset.\nIntroduction\nLarge Language Models (LLMs) like GPT-3.5 (Ouyang\net al. 2022) and LLaMA (Touvron et al. 2023) have demon-\nstrated remarkable performance on a range of Natural Lan-\nguage Processing (NLP) tasks. More recently, novel meth-\nods including MiniGPT-4 (Zhu et al. 2023), BLIP-2 (Li et al.\n2023), and PandaGPT (Su et al. 2023) have further extended\nthe ability of LLMs into visual processing by aligning vi-\nsual features with text features, bringing a significant revolu-\ntion in the domain of Artificial General Intelligence (AGI).\nWhile LVLMs are pre-trained on amounts of data sourced\n*These authors contributed equally.\n†Corresponding author.\nCopyright © 2024, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nFigure 1: Comparison between our AnomalyGPT, existing\nIAD methods and existing LVLMs. Existing IAD methods\ncan only provide anomaly scores and need manually thresh-\nold setting, while existing LVLMs cannot detect anomalies\nin the image. AnomalyGPT can not only provide informa-\ntion about the image but also indicate the presence and loca-\ntion of anomaly.\nfrom the Internet, their domain-specific knowledge is rela-\ntively limited and they lack sensitivity to local details within\nobjects, which restricts their potentiality in IAD task.\nIAD task aims to detect and localize anomalies in indus-\ntrial product images. Due to the rarity and unpredictability\nof real-world samples, models are required to be trained only\non normal samples and distinguish anomalous samples that\ndeviate from normal samples. Current IAD methods (Jeong\net al. 2023; Huang et al. 2022; You et al. 2022) typically only\nprovide anomaly scores for test samples and require manu-\nally specification of thresholds to distinguish between nor-\nmal and anomalous instances for each class of items, which\nis not suitable for real production environments.\nAs illustrated in Figure 1 and Table 1, neither existing\nIAD methods nor LVLMs can address IAD problem well,\nso we introduce AnomalyGPT, a novel IAD approach based\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1932\nMethods Few-shot learning Anomaly score Anomaly localization Anomaly judgement Multi-turn dialogue\nTraditional IAD methods ✓ ✓\nFew-shot IAD methods ✓ ✓ ✓\nLVLMs ✓ ✓\nAnomalyGPT (ours) ✓ ✓ ✓ ✓ ✓\nTable 1: Comparison between our AnomalyGPT and existing methods across various functionalities. The “Tradi-\ntional IAD methods” in the table refers to “one-class-one-model” methods such as PatchCore (Roth et al. 2022), InTra (Pirnay\nand Chai 2022), and PyramidFlow (Lei et al. 2023). “Few-shot IAD methods” refers to methods that can perform few-shot\nlearning like RegAD (Huang et al. 2022), Graphcore (Xie et al. 2023), and WinCLIP (Wang et al. 2023). “LVLMs” represents\ngeneral large vision-language models like MiniGPT-4 (Zhu et al. 2023), LLaV A (Liu et al. 2023), and PandaGPT (Su et al.\n2023). “Anomaly score” in the table represents just providing scores for anomaly detection, while “Anomaly judgement” indi-\ncates directly assessing the presence of anomaly.\non LVLM. AnomalyGPT can detect the presence and loca-\ntion of anomalies without the need for manual threshold set-\ntings. Moreover, our method can provide information about\nthe image and allows for interactive engagement, enabling\nusers to ask follow-up questions based on their needs and\nthe provided answers. AnomalyGPT can also perform in-\ncontext learning with a small number of normal samples,\nenabling swift adaptation to previously unseen objects.\nSpecifically, we focus on fine-tuning the LVLM us-\ning synthesized anomalous visual-textual data, integrating\nIAD knowledge into the model. However, direct training\nwith IAD data presents numerous challenges. The first is\ndata scarcity. Methods like LLaV A (Liu et al. 2023) and\nPandaGPT (Su et al. 2023) are pre-trained on 160k images\nwith corresponding multi-turn dialogues. However, existing\nIAD datasets (Bergmann et al. 2019; Zou et al. 2022) contain\nonly a few thousand samples, rendering direct fine-tuning\neasy to overfitting and catastrophic forgetting. To address\nthis, we use prompt embeddings to fine-tune the LVLM in-\nstead of parameter fine-tuning. Additional prompt embed-\ndings are added after image inputs, introducing supplemen-\ntary IAD knowledge into the LVLM. The second challenge\nrelates to fine-grained semantic. We propose a lightweight,\nvisual-textual feature-matching-based decoder to generate\npixel-level anomaly localization results. The decoder’s out-\nputs are introduced to the LVLM along with the original\ntest images through prompt embeddings, which allows the\nLVLM to utilize both the raw image and the decoder’s out-\nputs to make anomaly determinations, improving the accu-\nracy of its judgments.\nExperimentally, we conduct extensive experiments on the\nMVTec-AD (Bergmann et al. 2019) and VisA (Zou et al.\n2022) datasets. With unsupervised training on the MVTec-\nAD dataset, we achieve an accuracy of 93.3%, an image-\nlevel AUC of 97.4%, and a pixel-level AUC of 93.1%. When\none-shot transferred to the VisA dataset, we achieve an ac-\ncuracy of 77.4%, an image-level AUC of 87.4%, and a pixel-\nlevel AUC of 96.2%. Conversely, after unsupervised training\non the VisA dataset, one-shot transferred to the MVTec-AD\ndataset result in an accuracy of 86.1%, an image-level AUC\nof 94.1%, and a pixel-level AUC of 95.3%.\nOur contributions are summarized as follows:\n• We present the pioneering utilization of LVLM for ad-\ndressing IAD task. Our method not only detects and lo-\ncates anomaly without manually threshold adjustments\nbut also supports multi-round dialogues. To the best of\nour knowledge, we are the first to successfully apply\nLVLM to the domain of industrial anomaly detection.\n• The lightweight, visual-textual feature-matching-based\ndecoder in our work addresses the limitation of the\nLLM’s weaker discernment of fine-grained semantic and\nalleviates the constraint of LLM’s restricted ability to\nsolely generate text outputs.\n• We employ prompt embeddings for fine-tuning and train\nour model concurrently with the data utilized during\nLVLM pre-training, thus preserving the LVLM’s inher-\nent capabilities and enabling multi-turn dialogues.\n• Our method retains robust transferability and is capa-\nble of engaging in in-context few-shot learning on new\ndatasets, yielding outstanding performance.\nRelated Work\nIndustrial Anomaly Detection Existing IAD methods\ncan be categorized into reconstruction-based and feature\nembedding-based approaches. Reconstruction-based meth-\nods primarily aim to reconstruct anomalous samples to their\ncorresponding normal counterparts and detect anomalies\nby calculating the reconstruction error. RIAD (Zavrtanik,\nKristan, and Sko ˇcaj 2021), SCADN (Yan et al. 2021), In-\nTra (Pirnay and Chai 2022) and AnoDDPM (Wyatt et al.\n2022) employ different reconstruction network architec-\ntures, ranging from autoencoder and Generative Adversarial\nNetwork (GAN) to Transformer and diffusion model.\nFeature embedding-based methods focus on modeling the\nfeature embeddings of normal samples. Approaches such as\nPatchSVDD (Yi and Yoon 2020) aim to find a hypersphere\nthat tightly encapsulates normal samples. Cflow-AD (Gu-\ndovskiy, Ishizaka, and Kozuka 2022) and PyramidFlow (Lei\net al. 2023) use normalizing flows to project normal samples\nonto a Gaussian distribution. PatchCore (Roth et al. 2022)\nand CFA (Lee, Lee, and Song 2022) establish a memory\nbank of patch embeddings from normal samples and detect\nanomalies by measuring the distance between a test sample\nembedding and its nearest normal embedding.\nThese methods typically follow the “one-class-one-\nmodel” learning paradigm, requiring plentiful normal sam-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1933\nples for each object class to learn its distribution, making\nthem impractical for novel object categories and less suit-\nable for dynamic production environments. In contrast, our\nmethod facilitates in-context learning for novel object cate-\ngories, enabling inference with only few normal samples.\nZero-/Few-Shot Industrial Anomaly Detection Recent\nefforts have focused on methods utilizing minimal normal\nsamples to accomplish IAD task. PatchCore (Roth et al.\n2022) constructs a memory bank using only a few normal\nsamples, resulting in a noticeable performance decline. Re-\ngAD (Huang et al. 2022) trained an image registration net-\nwork to align test images with normal samples, followed\nby similarity computation for corresponding patches. Win-\nCLIP (Jeong et al. 2023) leveraged CLIP (Radford et al.\n2021) to compute similarity between images and textual\ndescriptions representing normal and anomalous seman-\ntics, distinguishing anomalies based on their relative scores.\nHowever, these methods can only provide anomaly scores\nfor test samples during inference. To distinguish normal\nsamples from anomalous ones, it’s necessary to experimen-\ntally determine the optimal threshold on a test set, which\ncontradicts the original intent of IAD task that only uti-\nlize normal data. For instance, while PatchCore (Roth et al.\n2022) achieves an image-level AUC of 99.3% on MVTec-\nAD in unsupervised setting, its accuracy drops to 79.76%\nwhen using a unified threshold for inference. Our method, in\ncontrast, enables the LVLM to directly assess test samples\nfor the presence of anomalies and pinpoint their locations,\ndemonstrating enhanced practicality.\nLarge Vision-Language Models LLMs, traditionally\nsuccessful in NLP, are now explored for visual tasks. BLIP-\n2 (Li et al. 2023) leverages Q-Former to input visual features\nfrom Vision Transformer (Dosovitskiy et al. 2020) into the\nFlan-T5 (Chung et al. 2022) model. MiniGPT-4 (Zhu et al.\n2023) connects the image segment of BLIP-2 and the Vi-\ncuna (Chiang et al. 2023) model with a linear layer, perform-\ning a two-stage fine-tuning process using extensive image-\ntext data. PandaGPT (Su et al. 2023) establishes a connec-\ntion between ImageBind (Girdhar et al. 2023) and the Vi-\ncuna (Chiang et al. 2023) model via a linear layer, allowing\nfor multi-modal input. These approaches showcase the po-\ntential of LLM-based polymathic models.\nHowever, as mentioned earlier, these models are trained\non general data and lack domain-specific expertise. In this\npaper, through the utilization of simulated anomaly data, im-\nage decoder and prompt embeddings, AnomalyGPT is in-\ntroduced as an novel approach that achieves IAD task with-\nout the need for manually specified thresholds, while also\nenabling few-shot in-context learning. Table 1 illustrates a\ncomparison between AnomalyGPT and existing methods\nacross various functionalities.\nMethod\nAnomalyGPT is a novel conversational IAD vision-\nlanguage model, primarily designed for detecting anoma-\nlies in images of industrial artifacts and pinpointing their\npositions. We leverage a pre-trained image encoder and a\nLLM to align IAD images and their corresponding textual\ndescriptions via simulated anomaly data. We introduce a\ndecoder module and a prompt learner module to enhance\nIAD performance and achieve pixel-level localization out-\nput. Employing prompt tuning and alternate training with\npre-training data preserves the LLM’s transferability and\nprevents catastrophic forgetting. Our method exhibits robust\nfew-shot transfer capability, enabling anomaly detection and\nlocalization for previously unseen items with merely one\nnormal sample provided.\nModel Architecture\nFigure 2 illustrates the comprehensive architecture of\nAnomalyGPT. Given a query image x ∈ RH×W×C, the fi-\nnal features Fimg ∈ RC1 extracted by the image encoder\nare passed through the linear layer to obtain the image em-\nbedding Eimg ∈ RCemb , which is then fed into the LLM.\nIn unsupervised setting, the patch-level features extracted\nby intermediate layers of image encoder are fed into the\ndecoder together with text features to generate pixel-level\nanomaly localization results. In few-shot setting, the patch-\nlevel features from normal samples are stored in memory\nbanks and the localization result can be obtained by calcu-\nlating the distance between query patches and their most\nsimilar counterparts in the memory bank. The localization\nresults is subsequently transformed into prompt embeddings\nthrough the prompt learner, serving as a part of LLM input.\nThe LLM leverages image input, prompt embeddings, and\nuser-provided textual input to detect anomalies and identify\ntheir locations, thus generating responses for the user.\nDecoder and Prompt Learner\nDecoder To achieve pixel-level anomaly localization, we\nemploy a lightweight feature-matching-based image de-\ncoder that supports both unsupervised IAD and few-shot\nIAD. The design of the decoder is primarily inspired by\nPatchCore (Roth et al. 2022), WinCLIP (Jeong et al. 2023),\nand APRIL-GAN (Chen, Han, and Zhang 2023).\nAs illustrated in the upper part of Figure 2, we partition\nthe image encoder into 4 stages and obtain the intermedi-\nate patch-level features extracted by every stage Fi\npatch ∈\nRHi×Wi×Ci , where i indicates the i-th stage. Following\nthe idea from WinCLIP (Jeong et al. 2023), a natural ap-\nproach is to compute the similarity between Fi\npatch and\nthe text features Ftext ∈ R2×Ctext respectively represent-\ning normality and abnormality, such as A photo of a nor-\nmal bottle and A photo of an abnormal capsule. However,\nsince these intermediate features have not undergone the\nfinal image-text alignment, they cannot be directly com-\npared with text features. To address this, we introduce addi-\ntional linear layers to project these intermediate features to\n˜Fi\npatch ∈ RHi×Wi×Ctext , and align them with text features\nrepresenting normal and abnormal semantics. The localiza-\ntion result M ∈ RH×W can be obtained by Eq. (1):\nM = Upsample\n 4X\ni=1\nsoftmax( ˜Fi\npatchFT\ntext)\n!\n. (1)\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1934\nFigure 2: The architecture of AnomalyGPT. The query image is passed to the frozen image encoder and the patch-level features\nextracted from intermediate layers are fed into image decoder to compute their similarity with normal and abnormal texts to\nobtain localization result. The final features extracted by the image encoder are fed to a linear layer and then passed to the\nprompt learner along with the localization result. The prompt learner converts them into prompt embeddings suitable for input\ninto the LLM together with user text inputs. In few-shot setting, the patch-level features from normal samples are stored in\nmemory banks and the localization result can be obtained by calculating the distance between query patches and their most\nsimilar counterparts in the memory bank.\nFor few-shot IAD, as illustrated in the lower part of Fig-\nure 2, we utilize the same image encoder to extract inter-\nmediate patch-level features from normal samples and store\nthem in memory banks Bi ∈ RN×Ci , where i indicates the\ni-th stage. For patch-level features Fi\npatch ∈ RHi×Wi×Ci ,\nwe calculate the distance between each patch and its most\nsimilar counterpart in the memory bank, and the localization\nresult M ∈ RH×W can be obtained by Eq. (2):\nM = Upsample\n 4X\ni=1\n\u0010\n1 − max(Fi\npatch · BiT\n)\n\u0011!\n. (2)\nPrompt Learner To leverage fine-grained semantic from\nimages and maintain semantic consistency between LLM\nand decoder outputs, we introduce a prompt learner that\ntransforms the localization result into prompt embeddings.\nAdditionally, learnable base prompt embeddings, unrelated\nto decoder outputs, are incorporated into the prompt learner\nto provide extra information for the IAD task. Finally, these\nembeddings, along with the original image information, are\nfed into the LLM.\nAs illustrated in Figure 2, the prompt learner consists of\nthe learnable base prompt embeddings Ebase ∈ Rn1×Cemb\nand a convolutional neural network. The network converts\nthe localization result M ∈ RH×W into n2 prompt em-\nbeddings Edec ∈ Rn2×Cemb . Ebase and Edec form a set of\nn1 +n2 prompt embeddings Eprompt ∈ R(n1+n2)×Cemb that\nare combined with the image embedding into the LLM.\nData for Image-Text Alignment\nAnomaly Simulation We primarily adopt the approach\nproposed by NSA (Schl ¨uter et al. 2022) to simulate anoma-\nlous data. The NSA (Schl ¨uter et al. 2022) method builds\nupon the Cut-paste (Li et al. 2021) technique by incorpo-\nrating the Poisson image editing (P´erez, Gangnet, and Blake\n2003) method to alleviate the discontinuity introduced by\npasting image segments. Cut-paste (Li et al. 2021) is a com-\nmon technique in IAD domain for generating simulated\nanomaly images. This method involves randomly cropping\na block region from an image and then pasting it onto a ran-\ndom location in another image, thus creating a simulated\nanomalous portion. Simulated anomaly samples can signif-\nicantly enhance the performance of IAD models, but this\nprocedure often results in noticeable discontinuities, as il-\nlustrated in Figure 3. The Poisson editing method (P ´erez,\nGangnet, and Blake 2003) has been developed to seamlessly\nclone an object from one image into another image by solv-\ning the Poisson partial differential equations.\nQuestion and Answer Content To conduct prompt tun-\ning on the LVLM, we generate corresponding textual queries\nbased on the simulated anomalous images. Specifically, each\nquery consists of two components. The first part involves a\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1935\nFigure 3: Illustration of the comparison between cut-paste\nand poisson image editing. The results of cut-paste exhibit\nevident discontinuities and the results of poisson image edit-\ning are more natural.\ndescription of the input image, providing information about\nthe objects present in the image and their expected attributes,\nsuch as This is a photo of leather, which should be brown\nand without any damage, flaw, defect, scratch, hole or bro-\nken part. The second part queries the presence of anoma-\nlies within the object, namely Is there any anomaly in the\nimage? The LVLM firstly responds to whether anomalies\nare present. If anomalies are detected, the model continues\nto specify the number and location of the anomalous areas,\nsuch as Yes, there is an anomaly in the image, at the bottom\nleft of the image. or No, there are no anomalies in the im-\nage. We divide the image into a grid of3×3 distinct regions\nto facilitate the LVLM in verbally indicating the positions\nof anomalies, as shown in Figure 4. The descriptive con-\ntent about the image furnishes the LVLM with foundational\nknowledge of the input image, aiding in the model’s bet-\nter comprehension of the image contents. However, during\npractical applications, users may opt to omit this descriptive\ninput, and the model is still capable of performing IAD task\nbased solely on the provided image input.\nPrompts fed to the LLM typically follow the format:\n### Human: <Img>Eimg</Img>Eprompt[Image De-\nscription]Is there any anomaly in the image?###Assistant:\nEimg ∈ RCemb represents the image embedding be-\ning processed through the image encoder and linear layer,\nEprompt ∈ R(n1+n2)×Cemb refers to the prompt embeddings\ngenerated by the prompt learner, and [Image Description]\ncorresponds to the textual description of the image.\nLoss Functions\nTo train the decoder and prompt learner, we primarily\nemployed three loss functions: cross-entropy loss, focal\nloss (Lin et al. 2017), and dice loss (Milletari, Navab, and\nAhmadi 2016). The latter two are primarily utilized to en-\nhance the pixel-level localization accuracy of the decoder.\nCross-Entropy Loss Cross-entropy loss is commonly em-\nployed for training language models, which quantifies the\ndisparity between the text sequence generated by the model\nand the target text sequence. The formula is as follows:\nLce = −\nnX\ni=1\nyilog(pi), (3)\nwhere n is the number of tokens,yi is the true label for token\ni and pi is the predicted probability for token i.\nFigure 4: Illustration of the 3×3 grid of image, which is used\nto let LLM verbally indicate the abnormal position.\nFocal Loss Focal loss (Lin et al. 2017) is commonly used\nin object detection and semantic segmentation to address the\nissue of class imbalance, which introduces an adjustable pa-\nrameter γ to modify the weight distribution of cross-entropy\nloss, emphasizing samples that are difficult to classify. In\nIAD task, where most regions in anomaly images are still\nnormal, employing focal loss can mitigate the problem of\nclass imbalance. Focal loss can be calculated by Eq. (4):\nLfocal = −1\nn\nnX\ni=1\n(1 − pi)γlog(pi), (4)\nwhere n = H × W represents the total number of pixels,\npi is the predicted probability of the positive classes and γ\nis a tunable parameter for adjusting the weight of hard-to-\nclassify samples. In our implementation, we set γ to 2.\nDice Loss Dice loss (Milletari, Navab, and Ahmadi 2016)\nis a commonly employed loss function in semantic segmen-\ntation tasks. It is based on the dice coefficient and can be\ncalculated by Eq. (5):\nLdice = −\nPn\ni=1 yi ˆyi\nPn\ni=1 y2\ni + Pn\ni=1 ˆy2\ni\n, (5)\nwhere n = H × W, yi is the output of decoder and ˆyi is the\nground truth value.\nFinally, the overall loss function is defined as:\nL = αLce + βLfocal + δLdice, (6)\nwhere α, β, δare coefficients to balance the three loss func-\ntions, which are set to 1 by default in our experiments.\nExperiments\nDatasets We conduct experiments primarily on the\nMVTec-AD (Bergmann et al. 2019) and VisA (Zou et al.\n2022) datasets. The MVTec-AD dataset comprises 3629\ntraining images and 1725 testing images across 15 different\ncategories, making it one of the most popular datasets for\nIAD. The training images only consist of normal images,\nwhile the testing images contain both normal and anoma-\nlous images. The image resolutions vary from 700×700 to\n1024×1024. VisA, a newly introduced IAD dataset, con-\ntains 9621 normal images and 1200 anomalous images\nacross 12 categories, with resolutions approximately around\n1500×1000. Consistent with previous IAD methods, we\nonly use the normal data from these datasets for training.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1936\nSetup Method MVTec-AD VisA\nImage-AUC Pixel-AUC Accuracy Image-AUC Pixel-AUC Accuracy\n1-shot\nSPADE 81.0 ± 2.0 91.2 ± 0.4 - 79.5 ± 4.0 95.6 ± 0.4 -\nPaDiM 76.6 ± 3.1 89.3 ± 0.9 - 62.8 ± 5.4 89.9 ± 0.8 -\nPatchCore 83.4 ± 3.0 92.0 ± 1.0 - 79.9 ± 2.9 95.4 ± 0.6 -\nWinCLIP 93.1 ± 2.0 95.2 ± 0.5 - 83.8 ± 4.0 96.4 ± 0.4 -\nAnomalyGPT (ours) 94.1 ± 1.1 95.3 ± 0.1 86.1 ± 1.1 87.4 ± 0.8 96.2 ± 0.1 77.4 ± 1.0\n2-shot\nSPADE 82.9 ± 2.6 92.0 ± 0.3 - 80.7 ± 5.0 96.2 ± 0.4 -\nPaDiM 78.9 ± 3.1 91.3 ± 0.7 - 67.4 ± 5.1 92.0 ± 0.7 -\nPatchCore 86.3 ± 3.3 93.3 ± 0.6 - 81.6 ± 4.0 96.1 ± 0.5 -\nWinCLIP 94.4 ± 1.3 96.0 ± 0.3 - 84.6 ± 2.4 96.8 ± 0.3 -\nAnomalyGPT (ours) 95.5 ± 0.8 95.6 ± 0.2 84.8 ± 0.8 88.6 ± 0.7 96.4 ± 0.1 77.5 ± 0.3\n4-shot\nSPADE 84.8 ± 2.5 92.7 ± 0.3 - 81.7 ± 3.4 96.6 ± 0.3 -\nPaDiM 80.4 ± 2.5 92.6 ± 0.7 - 72.8 ± 2.9 93.2 ± 0.5 -\nPatchCore 88.8 ± 2.6 94.3 ± 0.5 - 85.3 ± 2.1 96.8 ± 0.3 -\nWinCLIP 95.2 ± 1.3 96.2 ± 0.3 - 87.3 ± 1.8 97.2 ± 0.2 -\nAnomalyGPT (ours) 96.3 ± 0.3 96.2 ± 0.1 85.0 ± 0.3 90.6 ± 0.7 96.7 ± 0.1 77.7 ± 0.4\nTable 2: Few-shot IAD results on MVTec-AD and VisA datasets. Results are listed as the average of 5 runs and the best-\nperforming method is in bold. The results for SPADE, PaDiM, PatchCore and WinCLIP are reported from (Jeong et al. 2023).\nMethod Image-AUC Pixel-AUC Accuracy\nPaDiM (Unified) 84.2 89.5 -\nJNLD (Unified) 91.3 88.6 -\nUniAD 96.5 96.8 -\nAnomalyGPT (ours) 97.4 93.1 93.3\nTable 3: Unsupervised anomaly detection results on MVTec-\nAD dataset. The best-performing method is in bold and the\nresults for PaDiM and JNLD are reported from (Zhao 2023).\nEvaluation Metrics Following existing IAD methods, we\nemploy the Area Under the Receiver Operating Charac-\nteristic (AUC) as our evaluation metric, with image-level\nand pixel-level AUC used to assess anomaly detection and\nanomaly localization performance, respectively. However,\nour approach uniquely allows for determining the presence\nof anomalies without the need for manually-set thresholds.\nTherefore, we also utilize the image-level accuracy to eval-\nuate the performance of our method.\nImplementation Details We utilize ImageBind-\nHuge (Girdhar et al. 2023) as the image encoder and\nVicuna-7B (Chiang et al. 2023) as the inferential LLM,\nconnected through a linear layer. We initialize our model\nusing pre-trained parameters from PandaGPT (Su et al.\n2023). We set the image resolution at 224×224 and feed\nthe outputs from the 8th, 16th, 24th, and 32nd layers of\nImageBind-Huge’s image encoder to the image decoder.\nTraining is conducted on two RTX-3090 GPUs over 50\nepochs, with a learning rate of 1e-3 and a batch size of 16.\nLinear warm-up and a one-cycle cosine learning rate decay\nstrategy are applied. We perform alternating training using\nboth the pre-training data of PandaGPT (Su et al. 2023) and\nour anomaly image-text data. Only the decoder and prompt\nlearner undergo parameter updates, while the remaining\nparameters are all kept frozen.\nQuantitative Results\nFew-Shot Industrial Anomaly Detection We compare\nour work with prior few-shot IAD methods, selecting\nSPADE (Cohen and Hoshen 2020), PaDiM (Defard et al.\n2021), PatchCore (Roth et al. 2022), and WinCLIP (Jeong\net al. 2023) as the baselines. The results are presented in\nTable 2. Across both datasets, our method notably outper-\nforms previous approaches in terms of image-level AUC and\nachieves competitive pixel-level AUC and good accuracy.\nUnsupervised Industrial Anomaly Detection In the set-\nting of unsupervised training with a large number of nor-\nmal samples, given that our method trains a single model\non samples from all classes within a dataset, we selected\nUniAD (You et al. 2022), which is trained under the same\nsetup, as a baseline for comparison. Additionally, we com-\npare our model with PaDiM (Defard et al. 2021) and\nJNLD (Zhao 2022) using the same unified setting. The re-\nsults on MVTec-AD dataset are presented in Table 3.\nQualitative Examples\nFigure 5 illustrates the performance of our AnomalyGPT\nin unsupervised anomaly detection, and Figure 6 showcases\nthe results in the 1-shot in-context learning. Our model is\ncapable of indicating the presence of anomalies, pinpointing\ntheir locations, and providing pixel-level localization results.\nUsers can engage in multi-turn dialogues related to image\ncontent. In the 1-shot in-context learning setting, due to the\nabsence of training, the model’s localization performance is\nslightly lower than the unsupervised setting.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1937\nDecoder Prompt learner LLM LoRA MVTec-AD (unsupervised) VisA (1-shot)\nImage-AUC Pixel-AUC Accuracy Image-AUC Pixel-AUC Accuracy\n✓ - - 72.2 - - 56.5\n✓ ✓ - - 73.4 - - 56.6\n✓ ✓ - - 79.8 - - 63.4\n✓ ✓ 97.1 90.9 72.2 85.8 96.2 56.5\n✓ ✓ ✓ 97.1 90.9 84.2 85.8 96.2 64.7\n✓ ✓ ✓ ✓ 96.0 88.1 83.9 85.8 96.5 72.7\n✓ 97.1 90.9 90.3 85.8 96.2 75.4\n✓ ✓ ✓ 97.4 93.1 93.3 87.4 96.2 77.4\nTable 4: Results of ablation studies. The ✓ in “Decoder” and “Prompt learner” columns indicate module inclusion. The ✓ in\n“LLM” column denotes whether use LLM for inference and the ✓ in “LoRA” column denotes whether use LoRA to fine-tune\nLLM. In settings without LLM, the maximum anomaly score from normal samples is used as the classification threshold. In\nsettings without decoder, due to the sole textual output from the LLM, we cannot compute image-level and pixel-level AUC.\nFigure 5: Qualitative example of AnomalyGPT in the un-\nsupervised setting. AnomalyGPT is capable of detecting\nanomaly, pinpointing its location, providing pixel-level lo-\ncalization results and answering questions about the image.\nAblation Studies\nTo prove the efficacy of each proposed module, extensive\nablation experiments are conducted on both the MVTec-AD\nand VisA datasets. We primarily focus on four aspects: the\ndecoder, prompt learner, the usage of LLM for inference,\nand the utilization of LoRA to fine-tune the LLM. The prin-\ncipal results are presented in Table 4. Unsupervised training\nand testing are carried out on the MVTec-AD dataset, while\nthe one-shot performance is evaluated on the visa dataset.\nIt can be observed that the decoder demonstrates impressive\npixel-level anomaly localization performance. Compared to\nmanually-set thresholds, the LLM exhibits superior infer-\nence accuracy and provides additional functionality. Further-\nmore, prompt tuning outperforms LoRA in terms of accu-\nracy and transferability.\nConclusion\nWe introduce AnomalyGPT, a novel conversational IAD\nvision-language model, leveraging the powerful capabili-\nties of LVLM. AnomalyGPT can determine whether an im-\nage contains anomalies and pinpoint their locations with-\nout the need for manually specified thresholds. Further-\nmore, AnomalyGPT enables multi-turn dialogues focused\nFigure 6: Qualitative example of AnomalyGPT in the one-\nnormal-shot setting. The localization performance is slightly\nlower compared to the unsupervised setting due to the ab-\nsence of parameter training.\non anomaly detection and demonstrates remarkable perfor-\nmance in few-shot in-context learning. The effectiveness of\nAnomalyGPT is validated on two common datasets. Our\nwork delves into the potential application of large visual lan-\nguage models in anomaly detection, offering fresh ideas and\npossibilities for the field of industrial anomaly detection.\nAcknowledgements\nThis work was supported in part by the National Key R&D\nProgram of China (No.2022ZD0160601), National Natu-\nral Science Foundation of China (No.62276260, 62076235,\n61976210, 62006230), Beijing Municipal Science and Tech-\nnology Project (Z231100007423004), sponsored by Zhe-\njiang Lab (No.2021KH0AB07).\nReferences\nBergmann, P.; Fauser, M.; Sattlegger, D.; and Steger, C.\n2019. MVTec AD–A comprehensive real-world dataset\nfor unsupervised anomaly detection. In Proceedings of\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1938\nthe IEEE/CVF conference on computer vision and pattern\nrecognition, 9592–9600.\nChen, X.; Han, Y .; and Zhang, J. 2023. A Zero-/Few-\nShot Anomaly Classification and Segmentation Method for\nCVPR 2023 V AND Workshop Challenge Tracks 1&2: 1st\nPlace on Zero-shot AD and 4th Place on Few-shot AD.arXiv\npreprint arXiv:2305.17382.\nChiang, W.-L.; Li, Z.; Lin, Z.; Sheng, Y .; Wu, Z.; Zhang, H.;\nZheng, L.; Zhuang, S.; Zhuang, Y .; Gonzalez, J. E.; et al.\n2023. Vicuna: An open-source chatbot impressing gpt-4\nwith 90%* chatgpt quality. See https://vicuna. lmsys. org\n(accessed 14 April 2023).\nChung, H. W.; Hou, L.; Longpre, S.; Zoph, B.; Tay, Y .; Fe-\ndus, W.; Li, E.; Wang, X.; Dehghani, M.; Brahma, S.; et al.\n2022. Scaling instruction-finetuned language models. arXiv\npreprint arXiv:2210.11416.\nCohen, N.; and Hoshen, Y . 2020. Sub-image anomaly de-\ntection with deep pyramid correspondences. arXiv preprint\narXiv:2005.02357.\nDefard, T.; Setkov, A.; Loesch, A.; and Audigier, R.\n2021. Padim: a patch distribution modeling framework for\nanomaly detection and localization. In International Con-\nference on Pattern Recognition, 475–489. Springer.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; et al. 2020. An image is worth 16x16\nwords: Transformers for image recognition at scale. arXiv\npreprint arXiv:2010.11929.\nGirdhar, R.; El-Nouby, A.; Liu, Z.; Singh, M.; Alwala, K. V .;\nJoulin, A.; and Misra, I. 2023. Imagebind: One embedding\nspace to bind them all. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n15180–15190.\nGudovskiy, D.; Ishizaka, S.; and Kozuka, K. 2022. Cflow-\nad: Real-time unsupervised anomaly detection with localiza-\ntion via conditional normalizing flows. InProceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, 98–107.\nHuang, C.; Guan, H.; Jiang, A.; Zhang, Y .; Spratling, M.;\nand Wang, Y .-F. 2022. Registration based few-shot anomaly\ndetection. In European Conference on Computer Vision,\n303–319. Springer.\nJeong, J.; Zou, Y .; Kim, T.; Zhang, D.; Ravichandran, A.;\nand Dabeer, O. 2023. Winclip: Zero-/few-shot anomaly\nclassification and segmentation. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 19606–19616.\nLee, S.; Lee, S.; and Song, B. C. 2022. Cfa: Coupled-\nhypersphere-based feature adaptation for target-oriented\nanomaly localization. IEEE Access, 10: 78446–78454.\nLei, J.; Hu, X.; Wang, Y .; and Liu, D. 2023. Pyramid-\nFlow: High-Resolution Defect Contrastive Localization us-\ning Pyramid Normalizing Flow. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 14143–14152.\nLi, C.-L.; Sohn, K.; Yoon, J.; and Pfister, T. 2021. Cutpaste:\nSelf-supervised learning for anomaly detection and localiza-\ntion. In Proceedings of the IEEE/CVF conference on com-\nputer vision and pattern recognition, 9664–9674.\nLi, J.; Li, D.; Savarese, S.; and Hoi, S. 2023. Blip-2:\nBootstrapping language-image pre-training with frozen im-\nage encoders and large language models. arXiv preprint\narXiv:2301.12597.\nLin, T.-Y .; Goyal, P.; Girshick, R.; He, K.; and Doll ´ar, P.\n2017. Focal loss for dense object detection. In Proceedings\nof the IEEE international conference on computer vision ,\n2980–2988.\nLiu, H.; Li, C.; Wu, Q.; and Lee, Y . J. 2023. Visual instruc-\ntion tuning. arXiv preprint arXiv:2304.08485.\nMilletari, F.; Navab, N.; and Ahmadi, S.-A. 2016. V-net:\nFully convolutional neural networks for volumetric medical\nimage segmentation. In 2016 fourth international confer-\nence on 3D vision (3DV), 565–571. Ieee.\nOuyang, L.; Wu, J.; Jiang, X.; Almeida, D.; Wainwright, C.;\nMishkin, P.; Zhang, C.; Agarwal, S.; Slama, K.; Ray, A.;\net al. 2022. Training language models to follow instructions\nwith human feedback. Advances in Neural Information Pro-\ncessing Systems, 35: 27730–27744.\nP´erez, P.; Gangnet, M.; and Blake, A. 2003. Poisson image\nediting. In ACM SIGGRAPH 2003 Papers, 313–318.\nPirnay, J.; and Chai, K. 2022. Inpainting transformer for\nanomaly detection. In International Conference on Image\nAnalysis and Processing, 394–406. Springer.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from nat-\nural language supervision. In International conference on\nmachine learning, 8748–8763. PMLR.\nRoth, K.; Pemula, L.; Zepeda, J.; Sch ¨olkopf, B.; Brox,\nT.; and Gehler, P. 2022. Towards total recall in indus-\ntrial anomaly detection. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n14318–14328.\nSchl¨uter, H. M.; Tan, J.; Hou, B.; and Kainz, B. 2022. Nat-\nural synthetic anomalies for self-supervised anomaly detec-\ntion and localization. In European Conference on Computer\nVision, 474–489. Springer.\nSu, Y .; Lan, T.; Li, H.; Xu, J.; Wang, Y .; and Cai, D. 2023.\nPandagpt: One model to instruction-follow them all. arXiv\npreprint arXiv:2305.16355.\nTouvron, H.; Lavril, T.; Izacard, G.; Martinet, X.; Lachaux,\nM.-A.; Lacroix, T.; Rozi `ere, B.; Goyal, N.; Hambro, E.;\nAzhar, F.; et al. 2023. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971.\nWang, W.; Chen, Z.; Chen, X.; Wu, J.; Zhu, X.; Zeng, G.;\nLuo, P.; Lu, T.; Zhou, J.; Qiao, Y .; et al. 2023. Visionllm:\nLarge language model is also an open-ended decoder for\nvision-centric tasks. arXiv preprint arXiv:2305.11175.\nWyatt, J.; Leach, A.; Schmon, S. M.; and Willcocks, C. G.\n2022. Anoddpm: Anomaly detection with denoising diffu-\nsion probabilistic models using simplex noise. In Proceed-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1939\nings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 650–656.\nXie, G.; Wang, J.; Liu, J.; Zheng, F.; and Jin, Y . 2023. Push-\ning the limits of fewshot anomaly detection in industry vi-\nsion: Graphcore. arXiv preprint arXiv:2301.12082.\nYan, X.; Zhang, H.; Xu, X.; Hu, X.; and Heng, P.-A. 2021.\nLearning semantic context from normal samples for unsu-\npervised anomaly detection. In Proceedings of the AAAI\nconference on artificial intelligence, volume 35, 3110–3118.\nYi, J.; and Yoon, S. 2020. Patch svdd: Patch-level svdd for\nanomaly detection and segmentation. In Proceedings of the\nAsian conference on computer vision.\nYou, Z.; Cui, L.; Shen, Y .; Yang, K.; Lu, X.; Zheng, Y .; and\nLe, X. 2022. A unified model for multi-class anomaly detec-\ntion. Advances in Neural Information Processing Systems,\n35: 4571–4584.\nZavrtanik, V .; Kristan, M.; and Sko ˇcaj, D. 2021. Recon-\nstruction by inpainting for visual anomaly detection.Pattern\nRecognition, 112: 107706.\nZhao, Y . 2022. Just noticeable learning for unsupervised\nanomaly localization and detection. In 2022 IEEE Interna-\ntional Conference on Multimedia and Expo (ICME), 01–06.\nIEEE.\nZhao, Y . 2023. OmniAL: A unified CNN framework for\nunsupervised anomaly localization. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3924–3933.\nZhu, D.; Chen, J.; Shen, X.; Li, X.; and Elhoseiny, M.\n2023. Minigpt-4: Enhancing vision-language understand-\ning with advanced large language models. arXiv preprint\narXiv:2304.10592.\nZou, Y .; Jeong, J.; Pemula, L.; Zhang, D.; and Dabeer, O.\n2022. Spot-the-difference self-supervised pre-training for\nanomaly detection and segmentation. In European Confer-\nence on Computer Vision, 392–408. Springer.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n1940",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.5078607201576233
    },
    {
      "name": "Computer science",
      "score": 0.5004932880401611
    },
    {
      "name": "Computer vision",
      "score": 0.359180212020874
    },
    {
      "name": "Natural language processing",
      "score": 0.354364812374115
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210112150",
      "name": "Institute of Automation",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210165038",
      "name": "University of Chinese Academy of Sciences",
      "country": "CN"
    }
  ]
}