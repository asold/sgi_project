{
  "title": "Document-Level Machine Translation with Large Language Models",
  "url": "https://openalex.org/W4389520065",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2155672199",
      "name": "Longyue Wang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2964479149",
      "name": "Chenyang Lyu",
      "affiliations": [
        "Mohamed bin Zayed University of Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2896377289",
      "name": "Tianbo Ji",
      "affiliations": [
        "Dublin City University"
      ]
    },
    {
      "id": "https://openalex.org/A2520512037",
      "name": "Zhirui Zhang",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099188023",
      "name": "Dian Yu",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2105557964",
      "name": "Shuming Shi",
      "affiliations": [
        "Tencent (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2126985900",
      "name": "Zhaopeng Tu",
      "affiliations": [
        "Tencent (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3173691187",
    "https://openalex.org/W3105214104",
    "https://openalex.org/W2971287409",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2903193068",
    "https://openalex.org/W3100501376",
    "https://openalex.org/W4298771022",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3164747806",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2799051177",
    "https://openalex.org/W4307694778",
    "https://openalex.org/W4385574121",
    "https://openalex.org/W4321177597",
    "https://openalex.org/W4367860087",
    "https://openalex.org/W4404780783",
    "https://openalex.org/W4384644509",
    "https://openalex.org/W2951604644",
    "https://openalex.org/W3093345276",
    "https://openalex.org/W3104976898",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W4294214983",
    "https://openalex.org/W4385572225",
    "https://openalex.org/W2062989416",
    "https://openalex.org/W3203909556",
    "https://openalex.org/W2971347700",
    "https://openalex.org/W2952446148",
    "https://openalex.org/W3107826490",
    "https://openalex.org/W4321472057",
    "https://openalex.org/W3153290692",
    "https://openalex.org/W3211978535",
    "https://openalex.org/W4385567303",
    "https://openalex.org/W4377164428",
    "https://openalex.org/W2149327368",
    "https://openalex.org/W4385574203",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4389519421",
    "https://openalex.org/W4322760121",
    "https://openalex.org/W4300428972",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W2783419700",
    "https://openalex.org/W2964093087",
    "https://openalex.org/W4386566643",
    "https://openalex.org/W4320167623",
    "https://openalex.org/W2963842551",
    "https://openalex.org/W136732505",
    "https://openalex.org/W2963194310",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4376312026",
    "https://openalex.org/W2340762547",
    "https://openalex.org/W4285121328",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2980462515"
  ],
  "abstract": "Large language models (LLMs) such as ChatGPT can produce coherent, cohesive, relevant, and fluent answers for various natural language processing (NLP) tasks. Taking document-level machine translation (MT) as a testbed, this paper provides an in-depth evaluation of LLMs’ ability on discourse modeling. The study focuses on three aspects: 1) Effects of Context-Aware Prompts, where we investigate the impact of different prompts on document-level translation quality and discourse phenomena; 2) Comparison of Translation Models, where we compare the translation performance of ChatGPT with commercial MT systems and advanced document-level MT methods; 3) Analysis of Discourse Modelling Abilities, where we further probe discourse knowledge encoded in LLMs and shed light on impacts of training techniques on discourse modeling. By evaluating on a number of benchmarks, we surprisingly find that LLMs have demonstrated superior performance and show potential to become a new paradigm for document-level translation: 1) leveraging their powerful long-text modeling capabilities, GPT-3.5 and GPT-4 outperform commercial MT systems in terms of human evaluation; 2) GPT-4 demonstrates a stronger ability for probing linguistic knowledge than GPT-3.5. This work highlights the challenges and opportunities of LLMs for MT, which we hope can inspire the future design and evaluation of LLMs (We release our data and annotations at https://github.com/longyuewangdcu/Document-MT-LLM).",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 16646–16661\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nDocument-Level Machine Translation with Large Language Models\nLongyue Wang1∗ Chenyang Lyu2∗ Tianbo Ji3∗ Zhirui Zhang1∗\nDian Yu1 Shuming Shi1 Zhaopeng Tu1\n1Tencent AI Lab 2MBZUAI 3Dublin City University\n{vinnylywang, jackzrzhang, shumingshi, zptu}@tencent.com\nchenyang.lyu@mbzuai.ac.ae, tianbo.ji2@mail.dcu.ie\nAbstract\nLarge language models (LLMs) such as Chat-\nGPT can produce coherent, cohesive, relevant,\nand fluent answers for various natural language\nprocessing (NLP) tasks. Taking document-\nlevel machine translation (MT) as a testbed, this\npaper provides an in-depth evaluation of LLMs’\nability on discourse modeling. The study fo-\ncuses on three aspects: 1) Effects of Context-\nAware Prompts, where we investigate the im-\npact of different prompts on document-level\ntranslation quality and discourse phenomena; 2)\nComparison of Translation Models, where we\ncompare the translation performance of Chat-\nGPT with commercial MT systems and ad-\nvanced document-level MT methods; 3) Anal-\nysis of Discourse Modelling Abilities , where\nwe further probe discourse knowledge encoded\nin LLMs and shed light on impacts of training\ntechniques on discourse modeling. By evalu-\nating on a number of benchmarks, we surpris-\ningly find that LLMs have demonstrated supe-\nrior performance and show potential to become\na new paradigm for document-level translation:\n1) leveraging their powerful long-text modeling\ncapabilities, GPT-3.5 and GPT-4 outperform\ncommercial MT systems in terms of human\nevaluation;1 2) GPT-4 demonstrates a stronger\nability for probing linguistic knowledge than\nGPT-3.5. This work highlights the challenges\nand opportunities of LLMs for MT, which we\nhope can inspire the future design and evalua-\ntion of LLMs.2\n1 Introduction\nIn the past several years, machine translation (MT)\nhas seen significant advancements with the intro-\nduction of pre-trained models such as BERT (De-\nvlin et al., 2018), GPT-2 (Radford et al., 2019), and\n∗Equal contribution.\n1The protocol employed in this work was approved by the\nTencent Institutional Review Board (IRB).\n2We release our data and annotations at https://github.\ncom/longyuewangdcu/Document-MT-LLM.\nFigure 1: An example of translating a document-\nlevel text from English to Chinese using GPT-4 (Date:\n2023.03.17). We highlight the discourse phenomena\nusing figures and lines, which are invisible to GPT-4.\nT5 (Raffel et al., 2020). These models have demon-\nstrated impressive performance on MT (Zhu et al.,\n2020; Guo et al., 2020; Xue et al., 2021). However,\nmost of the existing work has focused on sentence-\nlevel translation, which can result in translations\nthat lack coherence and context. Recent years have\nseen a growing interest in document-level transla-\ntion, which is a crucial task that involves translat-\ning entire documents (Wang et al., 2017; Bawden\net al., 2018; Wang, 2019; Zhang et al., 2022) while\nmodelling specific discourse phenomena (Wang\net al., 2016; V oita et al., 2018; Wang et al., 2018a,b,\n2019; V oita et al., 2019b; Wang et al., 2023b). The\nmost popular large language model (LLM) – Chat-\nGPT3 shows the ability of maintaining long-term\ncoherence and consistency in a conversation by\nconditioning on previous conversational turns. Ad-\n3https://chat.openai.com. All corresponding results\nwere obtained from GPT-3.5 and GPT-4 in March 2023. The\nreproducibility is discussed in Section Limitation.\n16646\nditionally, the model is trained on a large dialogue\ndataset, which allows it to learn the patterns and\nconventions of human communication, further im-\nproving its ability to document-level understanding\nand generation (as shown in Figure 1).\nIn this paper, we are particularly interested in\nhow LLMs such as ChatGPT perform for modeling\ndocument-level text, encompassing discourse phe-\nnomena such as entity consistency, referential ex-\npressions, and coherence. Taking document-level\nMT as a testbed, we conduct an empirical study\nfrom three in-depth perspectives:\n• Effects of Context-Aware Prompts: ChatGPT\nneeds a prompt as guidance to trigger its trans-\nlation ability. Thus, we enable prompts to guide\nChatGPT to consider document-level contexts as\nlong as possible. Jiao et al. (2023) has found that\nthe candidate prompts generally work well and\nshow minor performance differences on sentence-\nlevel translation. In this work, we further inves-\ntigate the effects of prompts on the translation\nquality and specific discourse phenomena.\n• Comparison of Advanced Translation Models:\nWhile ChatGPT has demonstrated remarkable\nabilities in long-text NLP tasks, we are specifi-\ncally interested in how it performs on document-\nlevel translation. Consequently, we conduct a\nsystematic comparison of commercial MT prod-\nucts and advanced document-level approaches,\nutilizing both automatic and human evaluations\nto assess their discourse awareness.\n• Analysis of Discourse Modelling Abilities: A\nmore challenging question is the extent to which\nChatGPT capture and utilize discourse knowl-\nedge. To answer this question, we introduce a\nprobing method through contrastive testing and\nexplanation. In addition, the impact of various\ntraining techniques on the ability of LLMs to\nmodel discourse has not been thoroughly investi-\ngated. We compare variant models of ChatGPT\nthat incorporate techniques such as code pretrain-\ning, supervised fine-tuning (SFT), and reinforce-\nment learning from human feedback (RLHF).\nHowever, this is not a strict comparison because\nthere are other confounding variables employed\nduring the evolution of ChatGPT. In general, we\nhope to pose this open question that stimulates\nreflection and sparks further investigation.\nWe conduct experiments on a variety of\ndocument-level translation benchmarks, cover-\ning three language pairs (i.e. Chinese ⇒English,\nEnglish⇒German and English ⇒Russian) and\nseven domains (i.e. news, social, fiction, Q&A,\nTED, Europarl, and subtitle). We adopt a com-\nprehensive set of evaluation methods to mea-\nsure the performance of the models on document-\nlevel translation, including general-purpose met-\nrics, discourse-specific metrics, and human evalua-\ntion. The main contributions are:\n• Our empirical study shows the superior capabil-\nities of LLMs over advanced MT systems and\nmethods on document-level translation, indicat-\ning their potential to form a new paradigm.\n• We establish a benchmark with a probing method\nto thoroughly assess the document-level transla-\ntion quality and the ability of learning discourse\nknowledge, which will be made available for fu-\nture research.\n• To facilitate future research on document MT, we\npublicly release the instruction-based benchmark,\nsystem outputs as well as human annotations.\n2 Experimental Step\n2.1 Dataset\nTable 1 shows statistics of document-level datasets\nused in our experiments. About Group #1, we\nutilized the latest datasets, mZPRT (Wang et al.,\n2022) and WMT2022 (Kocmi et al., 2022), for\nevaluation to ensure that the testing data had not\nbeen used in commercial systems (e.g. Google\nTranslate and ChatGPT) before. As seen, this cov-\ners four domains (i.e. news, social media, web\nfiction, and Q&A forum) in Chinese ⇒English.\nRegarding Group #2, we utilized four widely-\nused benchmarks to compare established document-\nlevel methods with GPT-like applications. This cov-\ners three domains (i.e. TED talks, news commen-\ntary, European Parliament) in Chinese ⇒English\nand English⇒German. In Group #3, we employed\nan English ⇒Russian contrastive testset (V oita\net al., 2019b) that specifically targets discourse phe-\nnomena, such as deixis, ellipsis, and lexical cohe-\nsion. We use this dataset to further exploit models’\ncapacity for modeling discourse knowledge.\nAs seen, we also report the average length of\na document (|W|/|D|), which can be considered a\nmeasure of the complexity of discourse modeling.\nAs the length of the document increases, it becomes\nmore challenging to model accurate cohesive de-\nvices and discourse structure. From this perspec-\ntive, the mZPRT Fiction and IWSLT TED datasets\npose a greater challenge compared to others.\n16647\nID Domain Source Language |D| |S| |W| |W|/|D|\n1\nNews WMT2022 Zh ⇒En 38 505 16.1K/18.5K 424\nSocial 25 478 16.4K/13.3K 656\nFiction mZPRT Zh ⇒En 12 857 17.1K/16.6K 1,425\nQ&A 182 1,171 15.0K/22.1K 82\n2\nTED IWSLT2015 Zh ⇒En 62 6,047 116.9K/101.5K 1,885\nIWSLT2017 En ⇒De 23 2,271 38.1K/33.8K 1,657\nNews News Commentary v11 En⇒De 155 2,999 56.8K/53.9K 366\nEuroparl Europarl v7 360 5,134 130.1K/120.9K 361\n3 Subtitle OpenSub2018 En ⇒Ru 6,000 24,000 187.8K/514.8K 31\nTable 1: Statistics of datasets for document-level translation and analysis. We select the four latest benchmarks\nand four commonly-used ones, covering diverse domains and languages. We count the number of documents |D|,\nsentences |S|, and words |W| in terms of source/target language. The average length of a document |W|/|D| can be\nconsidered a measure of the complexity of discourse modeling. Note taht, WMT2022 includes two references,\nwhereas others have only one.\nDiscussion on Data Contamination We con-\nducted ChatGPT in March 28 ∼31 2023 with the\nofficial notice “the training data is up to Septem-\nber 2021”. 4 Different from previous work that\nleaned on dated or single-type datasets to assess\nChatGPT’s capabilities (Jiao et al., 2023; Lu et al.,\n2023), we carefully chosen both lastst, public and\ndiverse testsets to mitigate the risks associated with\ndata contamination. Taking Probing Discourse\nKnowledge (Section 5.1) for example, while the\ncontrastive testset used for the prediction task orig-\ninated in 2019, the evaluation on the explanation\ntask remains devoid of public references. Our con-\nclusions are comprehensively made by considering\nboth prediction and explanation results, balancing\nout any potential data contamination concerns. De-\nspite precautions, there remains a risk of data con-\ntamination, given that publicly available datasets\nare easily incorporated into LLM training (e.g. pre-\ntraining, SFT, or RLHF). A better way is to con-\nsistently integrate and employ the latest datasets\nwhen evaluating LLMs.\n2.2 Evaluation Method\nTranslation Quality We evaluate different ap-\nproaches and systems using classical sentence-\nand document-level evaluation metrics. About\nsentence-level metrics, we employ the commonly-\nused sacreBLEU (Post, 2018) and TER (Snover\net al., 2006). Additionally, we utilize COMET (Rei\net al., 2020), which leverages pretrained language\n4https://platform.openai.com/docs/models/\ngpt-4.\nmodels to achieve high correlations with human\nquality judgments. About document-level metrics,\nwe report document-level sacreBLEU (d-BLEU)\n(Liu et al., 2020), which is computed by matching\nn-grams in the whole document. Note that all eval-\nuations are case-sensitive. To facilitate sentence-\nlevel evaluation of document outputs, we imple-\nmented automatic sentence splitting/alignment5 on\nthe output and then manually corrected errors.\nDiscourse Awareness To target specific dis-\ncourse phenomena, we utilized two targeted met-\nrics, namely, CTT and AZPT , which respectively\nevaluate the consistency of terminology transla-\ntion and accuracy of zero pronoun (ZP) translation.\nRegarding CTT, one repeated terminology should\nkeep the same translation throughout the whole\ndocument (Xiao et al., 2011). We adopt a lexical\ntranslation consistency metric (Lyu et al., 2021):\nCTT =\n∑\nt∈TT\n∑k\ni=1\n∑k\nj=i+1 1(ti=tj )\nC2\nk\n|TT| (1)\nfor each terminology word w ∈TT , the C2\nk de-\nnotes the size of the combination of translation\nset (t1, . . . , tk), and function 1(ti = tj) returns 1\nif ti is same as tj, otherwise 0. The metric illus-\ntrates how frequently translation pairs of w is same\nwithin a document. The higher the metric value is,\nthe more likely w is translated consistently.\nRegarding ZP, it is a discourse phenomenon that\nappears frequently in pronoun-dropping (pro-drop)\n5https://github.com/rsennrich/Bleualign.\n16648\nID BLEU↑ TER↓ COMET↑ d-BLEU↑ CTT↑ AZPT↑\nNews Fiction News Fiction News Fiction News Fiction Fiction Fiction\nBase 25.5 12.4 62.7 85.0 0.420 0.095 28.2 15.4 0.19 0.39\nP1 25.8 13.9 63.8 82.8 0.483 0.124 28.7 17.0 0.29 0.41\nP2 26.2 13.8 61.4 81.8 0.479 0.155 28.8 16.5 0.28 0.41\nP3 26.5 14.4 61.1 81.1 0.469 0.154 29.1 17.4 0.33 0.44\nTable 2: Ablation study of document-level prompts (detailed in Table 3) on Chinese ⇒English datasets using\nChatGPT. We use BLEU and d-BLEU to measure sentence- and document-level translation quality. We also\nconduct two targeted evaluation metrics to measure specific discourse phenomena: accuracy of zero pronoun\ntranslation (AZPT ) and consistency of terminology translation ( CTT). Base is a sentence-level baseline system\nusing InstructGPT API without any context-based chat box.\nID Prompt\nP1 Please provide theTGT translation for the\nsentence: S\nP2 Translate the followingSRC sentences into\nTGT: [S1], [S2] . . .\nP3 (Continue) Translate this document from\nSRC to TGT: S1 S2 . . .\nTable 3: The prompts suggested by ChatGPT for\ndocument-level translation. SRC and TGT denote source\nand target languages, respectively. Each document is\norderly processed in one “Chat Box” while each prompt\nis fed into one “Conversational Turn”. P1 represents\nseparately translating each sentenceS in a document. P2\nor P3 means translating a document w/wo a sentential\nboundary tag “[]”.\nlanguages such as Chinese and Japanese. Recover-\ning ZPs in a target language (non-pro-drop) needs\nan understanding of the discourse structure. We\nused the AZPT score to measure the accuracy of\nZP translation (Wang et al., 2022):\nAZPT =\n∑\nz∈ZP A(tz|z)\n|ZP| (2)\nwhere ZP is the list of zero pronouns in the source\nsentences, tz is the generated translation for the\nzero pronoun z, and A(tz|z) is a binary scorer to\njudge whether tz is the correct translation of z.\nHuman Evaluation To thoroughly validate our\nconclusions, we also conduct a human evaluation\n(see Section Limitation.). We establish two sets\nof evaluation criteria: 1) general quality, covering\naspects such as fluency and adequacy; 2)discourse-\naware quality, including factors such as consis-\ntency, word choice, and anaphora. The detailed\nscoring criteria are listed in Appendix§ A.2. Ac-\ncordingly, each output will be assigned two distinct\nscores (0∼5). For each domain subset, we assessed\n100 instances, with each instance containing out-\nputs from 5 different systems. This amounted to\nan evaluation of roughly 70K words in total. The\nscores were assigned to each window of neighbor-\ning sentences, taking into account the context pro-\nvided by the entire document. Our intent was for\nevaluators to consider discourse properties beyond\nsingle sentences, while also avoiding the difficult\ntask of evaluating an entire document. We em-\nployed two professional evaluators for our study.\nThe payment and background is detailed in Section\nEthical Considerations and Appendix§ A.2, respec-\ntively. Besides, our annotators were given practice\nitems, and the annotations reaches 0.86 Cohen’s\nkappa scores (McHugh, 2012), demonstrating that\nthe annotators work efficiently and consistently un-\nder this guideline.\n3 Effects of Context-Aware Prompts\n3.1 Motivation\nExisting document NMT methods can be\nmainly classified into two categories: multi-\nsentence (Wang et al., 2017; V oita et al., 2018; Tu\net al., 2018) and whole-document (Macé and Ser-\nvan, 2019; Bao et al., 2021) approaches. ChatGPT\nis capable of not only handling long text in a sin-\ngle conversational turn but also recalling the entire\ncontext in the chat box. Accordingly, we design\nprompts to trigger the document-level translation\nability of ChatGPT.\nThe prompt engineering is necessary to ensure\nChatGPT’s robust ability to interpret instructions\nand to model long-term dependencies. Our re-\nsearch confirms the neutrality and representative-\nness of various prompts, allowing other researchers\n16649\nModel Automatic (d-BLEU) Human (General/Discourse)\nNews Social Fiction Q&A Ave. News Social Fiction Q&A Ave.\nGoogle 27.7 35.4 16.0 12.0 22.8 1.9/2.0 1.2/1.3 2.1/2.4 1.5/1.5 1.7/1.8\nDeepL 30.3 33.4 16.1 11.9 22.9 2.2/2.2 1.3/1.1 2.4/2.6 1.6/1.5 1.9/1.9\nTencent 29.3 38.8 20.7 15.0 26.0 2.3/2.2 1.5/1.5 2.6/2.8 1.8/1.7 2.1/2.1\nGPT-3.5 29.1 35.5 17.4 17.4 24.9 2.8/2.8 2.5/2.7 2.8/2.9 2.9/2.9 2.8/2.8\nGPT-4 29.7 34.4 18.8 19.0 25.5 3.3/3.4 2.9 /2.9 2.6/2.8 3.1/3.2 3.0 /3.1\nTable 4: Comparison between commercial MT systems and LLM applications on Chinese⇒English datasets using\nboth automatic and human evaluation methods. The human evaluation based on a scale from 0∼5 encompasses\ntwo dimensions: general quality and discourse awareness (detailed in Table 12). The significant test is detailed in\nAppendix §A.1.\nto utilize them with confidence, unburdened by\nconcerns of unintended biases.\n3.2 Comparison of Different Prompts\nWe query ChatGPT itself for advice and obtain\na number of candidate prompts, and then refine\nthem into three document-level prompts as shown\nin Table 3. We utilize P1 to translate a document\nsentence by sentence, with each sentence placed\nin a single conversational turn and the entire docu-\nment contained within one chat box. This mainly\ntakes advantage of ChatGPT’s long-term modeling\nability in the chat box. P2 and P3 combine mul-\ntiple continuous sentences and translate them in\none conversational turn until the entire document\nis finished. This aims to maximize the length of\ndocument as input. The only difference is whether\nor not the sentential boundary tag “[]” is inserted\ninto each sentence.\nWe compare the three candidate prompts on\nthe Zh ⇒En translation task using two testsets,\nWMT2022 News and mZPRT Fiction. Table 2\nshows the translation quality in terms of a variety\nof automatic evaluation metrics. In general, Chat-\nGPT reliably performs well with three candidate\nprompts, showing only minor variations in perfor-\nmance. This aligns with prior findings in sentence-\nlevel translation with ChatGPT (Jiao et al., 2023).\nOut of the three prompts, the prompt involved\nmulti-turn contexts without sentence boundaries\n(P3) achieves the best scores in most evaluation\nmetrics, except for COMET. Regarding discourse\nphenomena, P3 outperforms other candidates with\nbetter consistency of terminology translation and\nhigher accuracy of ZP translation. Upon examin-\ning the output samples, we noticed that ChatGPT\nmay sometimes forget the sentential boundary tag\nof P2 and combine all sentences together. Take-\naway: (1) Despite translating a document sentence\nby sentence, ChatGPT’s ability to model long-term\ndependencies already exists within the chat box.(2)\nIncreasing document length as a input can further\nenhance translation quality and discourse aware-\nness. (3) ChatGPT tends to translate a document\nwithout adhering to strict sentential boundaries,\nmirroring a natural approach adopted by humans\nduring document translation, which doesn’t neces-\nsitate sentence-to-sentence translation.\n4 Comparison of Translation Models\nIn this section, we compare various systems and\nmethods for the document-level translation task. In\nthe following experiments, we use the P3 prompt\nfor ChatGPT and the same document-level window\nsize for MT models as the default setting.\n4.1 ChatGPT vs. Commercial Systems\nCommercial systems are known for their high ac-\ncuracy and efficiency in translation, making them a\nstrong contender for any machine translation evalu-\nation. By comparing with commercial systems, we\ncan gauge ChatGPT’s performance relative to the\nbest available MT technologies. We compare GPT-\n3.5/GPT-4 with three commercial translation prod-\nucts, including Google Translate, 6 DeepL Trans-\nlate,7 and Tencent TranSmart (Huang et al., 2021).8\nWe employ both automatic (d-BLEU) and human\nevaluation (general/discourse-aware quality) as de-\ntailed in Section 2.2.\nTable 4 shows the results. When evaluated using\nd-BLEU, commercial MT systems generally out-\n6https://translate.google.com.\n7https://www.deepl.com.\n8https://transmart.qq.com.\n16650\nModel\nZH⇒EN EN ⇒DE\nTED TED News Europarl\nBLEU d-BLEU BLEU d-BLEU BLEU d-BLEU BLEU d-BLEU\nMCN 19.1 25.7 25.1 29.1 24.9 27.0 30.4 32.6\nG-Trans - - 25.1 27.2 25.5 27.1 32.4 34.1\nSent2Sent 19.2 25.8 25.2 29.2 25.0 27.0 31.7 33.8\nMR-Doc2Sent 19.4 25.8 25.2 29.2 25.0 26.7 32.1 34.2\nMR-Doc2Doc - 25.9 - 29.3 - 26.7 - 34.5\nSent2Sent⋆ 21.9 27.9 27.1 30.7 27.9 29.4 32.1 34.2\nMR-Doc2Sent⋆ 22.0 28.1 27.3 31.0 29.5 31.2 32.4 34.5\nMR-Doc2Doc⋆ - 28.4 - 31.4 - 32.6 - 34.9\nChatGPT - 28.3 - 33.6 - 39.4 - 30.4\nTable 5: Comparison between document-level NMT methods and LLM applications on Chinese ⇒English and\nEnglish⇒German benchmarks using commonly-used BLEU and d-BLEU metrics. “⋆” indicates using additional\nsentence-level corpus for model pre-training.\nperform LLM-based systems, except for the Q&A\ndomain, which involves informal spoken language.\nWhile the difference in performance is not signif-\nicant in the news domain (e.g. the gap between\nDeepL and GPT-4 is only 0.6 points), it is consid-\nerable in the social media and web fiction domains\n(i.e. the gaps are 3.3 and 1.9 points). A surpris-\ning finding is that GPT-4 and GPT-3.5 perform\nsignificantly better than MT systems in terms of\nhuman evaluation. The potential reasons may be:\n(1) d-BLEU only measures the similarity of the\nn-grams between the MT output and the reference\ntranslations. However, human takes into account\nadditional factors such as coherence, fluency, and\nnaturalness of the translation, which may not neces-\nsarily correlate with d-BLEU scores. (2) ChatGPT\nand MT systems may have different strengths and\nweaknesses. For example, ChatGPT may be better\nat modeling long-term dependencies and capturing\ndiscourse-level information, which could result in\nhigher human evaluation. On the other hand, MT\nsystems may perform better in terms of word-level\naccuracy, which is reflected in d-BLEU. Note that,\nour findings is distinguished from Neubig and He\n(2023). Focusing on long-text translation, we com-\npare ChatGPT with MT systems, and underscore\nChatGPT’s enhanced capacity to model long-term\ndependencies in comparison to MT systems. On\nthe other hand, Neubig and He (2023) investigate\nthe varying performances of GPT models based\non sentence length. They found that GPT models\nperform better on shorter sentences while worse on\nlonger ones. Karpinska and Iyyer (2023) recently\nhighlighted that GPT-3.5 has the capability to uti-\nlize document-level context effectively for literary\ntranslation, yet it is not free from critical errors.\nWhile the Fiction testset in our work is categorized\nunder literary, we did not find obvious omission\nerrors in the output. A more detailed comparison\nis earmarked for future exploration. Karpinska\nand Iyyer (2023) recently pointed that GPT-3.5\ncan effectively leverage document-level context for\nliterary translation, but critical errors persist. Al-\nthough the Fiction subset belongs to literary, we\ndid not find omission errors in the output and we\nleave this fine-grained comparison for future work.\nTakeaway: (1) There is a certain degree of discrep-\nancy discrepancy between human and automatic\nevaluation, which potentially provide complemen-\ntary reference points when measuring the quality of\ndocument-level translations; (2) This discrepancy\nunderlines the complexity inherent in accurately\nevaluating the capabilities of such systems. We\nfurther explore evaluation methods in Section 5.1.\n4.2 ChatGPT vs. Document NMT Methods\nDocument NMT methods are specifically designed\nto handle part or entire documents, making them a\nrelevant point of comparison for evaluating Chat-\nGPT’s ability to model long-term dependencies\nand discourse phenomena. We compare with five\nadvanced document-level NMT models:\n• MCN (Zheng et al., 2020): A multi-channel net-\nwork that integrates a hierarchical encoder and a\nparallel decoder, which leverages the document\nstructure and semantics for translation.\n16651\nID Prompt\nP4 Given anSRC text:{D}. Which one is the\ncorrect TGT translation as follows: [T1],\n. . . , [Tm]. Why?\nTable 6: The prompt for probing discourse knowledge\nencoded in LLMs. SRC and TGT denote source and\ntarget languages, respectively. D represents a document\ncontains several sentences. T1 . . .Tm refer to the trans-\nlation candidates, where only one of them is a positive\ntranslation and the others are negative due to the modifi-\ncation of discourse-specific words.\n• G-Trans (Bao et al., 2021): A graph-based\ntransformer that incorporates document-level dis-\ncourse structure as a directed acyclic graph, en-\nhancing the representation of the context.\n• Sent2Sent: A superior sentence-level baseline\nthat employs a transformer architecture to trans-\nlate each sentence independently and then merges\nthe translations into a document-level output.\n• MR-Doc2Doc and MR-Doc2Sent: Sun et al.\n(2022) explore to resolve document transla-\ntion with the end-to-end, namely document-to-\ndocument (Doc2Doc) pattern, and utilize Multi-\nresolutional Training, which combines docu-\nments with shorter segments like sentences or\nparagraphs to improve translation quality (de-\nnoted as MR-Doc2Doc). Additionally, they re-\nproduce the document-to-sentence baseline (MR-\nDoc2Sent) that introduces extra model modules\nto capture contextual information.\nTo enable a fair comparison with previous work,\nwe use four widely used document-level translation\nbenchmarks: TED (ZH-EN and EN-DE), News\n(EN-DE), and Europarl (EN-DE). We adopt tok-\nenized case-insensitive BLEU and d-BLEU as the\nevaluation metrics. As MR-Doc2Doc and Chat-\nGPT generate document-level translations that are\ndifficult to separate into individual sentences, we\nonly report d-BLEU scores for these models.\nTable 5 lists the results. The MR-Doc2Doc\nwith extra model pre-training achieves the best\ndocument-level performance among previous mod-\nels. Thanks to the document-level LM pre-training,\nChatGPT easily outperforms MR-Doc2Doc ⋆ on\nTED (EN-DE) and News (EN-DE) datasets, obtain-\ning similar performance on TED (ZH-EN) dataset.\nSurprisingly, ChatGPT performs poorly on the Eu-\nroparl (EN-DE) dataset, even worse than Sent2Sent.\nWe suspect this phenomenon may be caused by\nModel deixis lex.c ell.infl ell.VP\nSent2Sent 51.1 45.6 55.4 27.4\nMR-Doc2Doc 64.7 46.3 65.9 53.0\nCADec 81.6 58.1 72.2 80.0\nDocRepair 91.8 80.6 86.4 75.2\nGPT-3.5 57.9 44.4 75.0 71.6\nGPT-4 ∗85.9 ∗72.4 69.8 ∗81.4\nTable 7: Accuracy [%] of translation prediction for spe-\ncific contextual phenomena (deixis, lexical consistency,\nellipsis (inflection), and VP ellipsis) between different\nmodels on the English⇒Russian contrastive testset. “*”\nindicates a significant difference (p <0.001) between\nGPT-4 and GPT-3.5.\nthe domain distribution bias of the training data.\nMoreover, we find that ChatGPT is unstable, and\nits translation results sometimes exhibit omissions\nand obvious copying behaviors. Note that, the\ncommonly-used datasets were created between\n2012 and 2017, a time frame that raises the pos-\nsibility of these datasets being incorporated into\nthe training data of newer language models. Take-\naway: (1) ChatGPT has exhibited superior perfor-\nmance and may become a new promising paradigm\nfor document-level NMT ; (2) It is still debatable\nwhether these benchmarks can be considered as ap-\npropriate measures for evaluating document-level\ntranslation methods. We advocate for greater trans-\nparency from model developers regarding their\ntraining datasets. Additionally, this highlights the\nimportance of designing innovative evaluation tech-\nniques that can reliably assess model capabilities\nwhile sidestepping concerns related to data con-\ntamination.\n5 Analysis of Large Language Models\nWe analyze the ability of LLMs to capture dis-\ncourse knowledge from two perspectives: (1) prob-\ning the discourse knowledge encoded in LLMs,\nand (2) examining the impact of different training\ntechniques on discourse modeling.\n5.1 Probing Discourse Knowledge in LLM\nIn order to verify whether LLMs truly learn to uti-\nlize the context to resolve discourse inconsistencies,\nwe adopt the contrastive test sets proposed by V oita\net al. (2019b). This dataset includes deixis, lexi-\ncon consistency, ellipsis (inflection), and ellipsis\n(verb phrase) for evaluating discourse phenomena\nin English-Russian translations. Each instance has\n16652\nSubset GPT-3.5 GPT-4\nPrediction Explanation rϕ Prediction Explanation rϕ\ndeixis 58.0 18.0 0.293 ∗89.0 ∗93.0 0.279\nlex.c 42.0 11.0 0.089 ∗72.0 ∗86.0 0.293\nell.infl 75.0 58.0 0.398 71.0 ∗91.0 0.184\nell.VP 74.0 75.0 0.448 82.0 ∗94.0 0.539\nTable 8: Human evaluation results of GPT-3.5 and GPT-4 on contrastive test sets. For each test set, we randomly\nselect 100 examples and ask annotators to assess whether the responses generated by the models include the correct\nprediction and explanation, respectively. We count the accuracy (%) of prediction and explanation for GPT-3.5 and\nGPT-4, based on which the Phi coefficient (rϕ) is calculated to measure the association between two binary variables\n(i.e., prediction and explanation). “*” indicates a significant difference (p <0.001) between GPT-4 and GPT-3.5.\na positive translation and a few negative ones that\ndiffer by only one specific word. The goal is to\ndetermine if a model is more likely to generate a\ncorrect translation than incorrect variations. In this\nexperiment, we compare GPT-3.5/GPT-4 with ad-\nvanced methods, such as Sent2Sent, MR-Doc2Doc,\nCADec (V oita et al., 2019b) and DocRepair (V oita\net al., 2019a), where CADec and DocRepair intro-\nduce context-aware post-editing modules to refine\nthe sentence-level translations. For these baselines,\nwe adopt force decoding to generate scores for all\ntranslation candidates in each instance. If the score\nof the positive translation is the highest, then this\ninstance is counted as correct. For ChatGPT, we\nquery them with the prompt P4 in Table 6 to obtain\nresponses and correspondent explanations for each\ninstance. Then some heuristic rules and manual\nverification are used to calculate final performance.\nEvaluation on Prediction As shown in Table 7,\nGPT-3.5 performs worse than DocRepair (discouse-\nenhanced method) across all discourse phenomena,\nwith particularly significant gaps present in deixis\nand lexical consistency tests. These results show\nthat it is difficult to handle deixis and lexical consis-\ntency phenomena with large-scale document-level\npre-training. GPT-4 exhibits significant improve-\nments in these areas, but it still lags behind DocRe-\npair in deixis, lexical consistency, and ellipsis (in-\nflection) phenomena. Takeaway: (1) GPT-3.5\ndemonstrates lower accuracy in contrastive pre-\ndiction compared to conventional translation mod-\nels, whereas GPT-4 exhibits significant improve-\nment. (2) As there is no detailed technical report\navailable for GPT-4, we argue that its significant\nimprovements are likely due to the use of super-\nvised data and RLHF . We further explore this in\nSection 5.2.\nEvaluation on Explanation We conduct human\nevaluations to assess the quality of LLM-generated\nexplanations. This provides an additional way to\nexplore the discourse knowledge contained within\nLLMs. As illustrated in Table 8, we randomly\nselect 100 examples for each contrastive test set\nand request native speakers to evaluate whether\nthe models’ responses contain the correct predic-\ntion and explanation, respectively. Then the Phi\ncoefficient (rϕ) is further calculated to better mea-\nsure the correlation between two binary variables\n(i.e., prediction and explanation). We can observe\nthat the accuracy of explanation is often not re-\nflective of the accuracy of prediction, indicating\na mismatch in utilizing discourse knowledge for\nprediction and explanation. In addition, GPT-3.5 is\nnot good at explaining the reason for selecting the\ncorrect translation, while GPT-4 exhibits high per-\nformance in this aspect and brings better accuracy\nof prediction. Takeaway: (1) GPT-4 demonstrates\na strong ability to explain discourse knowledge.\n(2) Despite GPT-4’s superior performance in pre-\ndiction and explanation, the correlation between\nprediction and explanation does not appear to be\nsignificantly improved compared to GPT-3.5.\n5.2 Potential Impacts of Training Techniques\nLLMs have become the foundation of natural lan-\nguage processing research (Brown et al., 2020),\nwith recent advancements such as learning from\nsource code (Chen et al., 2021) and RLHF showing\npromise in improving LLM performance (Ouyang\net al., 2022). To investigate the potential impacts of\nthese approaches on discourse modelling, we con-\nduct experiments on Chinese⇒English Fiction and\nEnglish⇒Russian datasets using different variants\nof LLMs trained with distinct techniques (detailed\nin §A.3). Accordingly, we use P3 and P4 prompts.\n16653\nModel ZH⇒EN EN ⇒RU Training Techniques in LLMs\nAutomatic Human Probing\nGPT-3 3.3 n/a n/a\nInstructGPT\n+ SFT 7.1 n/a n/a\n+ FeedME-1 14.1 2.2/2.5 30.5/28.6\nCodexGPT\n+ FeedME-2 16.1 2.2/2.3 34.4/30.1\n+ PPO 17.2 2.6/2.7 58.0/39.4\nGPT-3.5 17.4 2.8/2.9 62.3/40.5\nGPT-4 18.8 2.6/2.8 78.5/91.0\nTable 9: Translation quality on Chinese⇒English Fiction (automatic d-BLEU and human general/discourse) and\nprobing performance on English⇒Russian (constrative prediction and explanation). The figure on the left illustrates\nLLMs along with their training methods (from ChatGPT reports and API notes). ‘n/a’ skips low performances.\nTable 9 shows the results. When SFT with high-\nquality demonstration examples, the translation per-\nformance can achieve 14.1 d-BLEU, which reaches\nto an acceptable level (InstructGPT +FeedME-1\nvs. +SFT). Moreover, code pretraining can im-\nprove the document translation quality by 2 d-\nBLEU points and the discourse knowldge probing\nby 4/1.5 (CodexGPT +FeedME-2 vs. InstructGPT\n+FeedME-1). When further adding PPO method,\nit outperforms all other combination strategies on\ntranslation quality, discourse awareness and dis-\ncourse knowledge probing (CodexGPT +FeedME-\n2 +PPO vs. others). This shows that RLHF strongly\nenhances LLM’s capability of translation. Lastly,\nGPT-3.5 and GPT-4 excel in d-BLEU and human\nevaluation scores as well as probing performance,\ndemonstrating the importance of contextual infor-\nmation for complex, lengthy document translation.\nTakeaway: (1) Methods like code pretraining, SFT\nand RLHF appear to enhance the performance\nof document translation and discourse modeling;\n(2) However, it is quite challenging to explore\nthe non-open source systems due to various con-\nfounding factors introduced during their develop-\nment. Therefore, we advocate for organizations\nlike OpenAI to provide greater transparency to aid\nresearchers in such explorations.\n6 Conclusion and Future Work\nWe provide a comprehensive evaluation of LLMs\n(such as GPT-3.5 and GPT-4) for document-level\nmachine translation. Our evaluation covers three\nmain aspects: (1) the effects of discourse-aware\nprompts, (2) comparison of advanced translation\nmodels, and (3) analysis of discourse modelling\nabilities. With the release of the GPT-4 model,\nthe discourse-aware performance has been signifi-\ncantly improved, making it a promising paradigm\nfor document-level translation. Despite its prowess\nin generative tasks, it struggles with discerning sub-\ntle distinctions for ranking.\nIn our future work, we plan to explore more\ndocument-level evaluation method (Castilho, 2021;\nJiang et al., 2023; Kocmi and Federmann, 2023),\nlatest long-text benchmarks (Wang et al., 2023a;\nThai et al., 2022; Wang et al., 2023c) and other\nMT scenarios (Ghazvininejad et al., 2023; Guer-\nreiro et al., 2023; Lyu et al., 2023). Furthermore,\nwe intend to delve into a more detailed analysis\nand comparison in future work. For instance, we\nwill employ appropriate significant test methods\nto account for multiple comparisons (e.g. non-\nparametric Kruskal-Wallis test, Bonferroni correc-\ntion) and conduct a power analysis (Card et al.,\n2020; Graham et al., 2020; Vilar et al., 2022; Hendy\net al., 2023). About annotation consistency, we fur-\nther apply the Krippendorff’s alpha coefficient and\ncheck the confidence interval (Krippendorff, 2011).\nLimitations\nWe list the main limitations of this work as follows:\n• Potential Inaccuracy of Conclusions. Our con-\nclusions are derived from experiments conducted\non a limited set of datasets, which may not guar-\nantee accuracy or applicability across all contexts.\nThese limitations might inadvertently introduce\n16654\nbias or overlook certain phenomena, potentially\nimpacting the comprehensiveness of our findings.\nIn response to this concern, we strive to use an\nextensive array of the most recent datasets, span-\nning various language pairs and domains. This\nbroad coverage aims to encompass a wide range\nof linguistic and contextual variations, thereby\nenhancing the generalizability of our findings.\n• Model Updates in ChatGPT and Reproducibil-\nity. When the underlying model or its parame-\nters of ChatGPT are updated or changed, the\nconclusions derived from prior evaluations may\nno longer be valid or entirely accurate. To mit-\nigate this issue, this paper has tried utmost to\nensure the reproducibility of our findings: (1)\nWe release all system outputs accompanied by\nexact timestamps and change logs. This ensures\nthat researchers can reliably reproduce and vali-\ndate our results. (2) We evaluated all systems at\ntwo distinct points: in March and August 2023.\nWhile there were minor variations in the exact\nperformance figures between these two evalua-\ntions, our overarching conclusions and core find-\nings remained unchanged and consistent.\n• Criteria of Human Evaluation and Refine-\nment. The design of the criteria still has room\nfor improvement. For example, in the \"Discourse\nAwareness\" category, there is only a slight dif-\nference between Scores 5 and 4, but a more sig-\nnificant gap between Scores 3 and 2. Given the\nscarcity of benchmark standards on discourse\nevaluation from past research, we published the\ndetailed scores for further analysis and highlight\nthis area as an opportunity for refinement.\nEthical Considerations\n• Annotation Process and Annotator Profile. A\none-week trial annotation phase was conducted\nfor bidding (five companies participated) on our\nenterprise-level annotation platform. The authors\nanswered questions posted by the annotators of\nthese companies and updated annotation guide-\nlines accordingly. The Q&A history is recorded\nand updated in the formal annotation phase. After\nevaluating the trial annotations based on accuracy\nand consistency, we selected a professional lan-\nguage service company (large enterprise9) head-\nquartered in Beijing, China. Their annotators\nwere experts in both source and target languages,\n9It is based on the information provided by https://www.\nkanzhun.com/firm.\nwith a background in translation and linguistics\n(detailed in Table11). To understand any poten-\ntial biases, annotators were inquired about their\nfamiliarity with the specific translation models\nunder evaluation and any affiliations with AI or\ntranslation companies. We ensured that none\nof the annotators had conflicts of interest, and\ntheir annotations were routinely cross-checked\nfor consistency. In terms of compensation, an-\nnotators received an hourly wage of $37.4. This\nrate aligns closely with the mean hourly wages\nobserved for U.S. interpreters/translators and for-\neign language teachers.10\n• Annotator Consent and IRB Review. Prior to\nthe commencement of the study, all participating\nannotators gave their informed consent, confirm-\ning their understanding of the study’s objectives\nand the intended research use of their annotations.\nAn exhaustive IRB review was undertaken and\nfinalized before the project’s onset (IRB Protocol\nNumber: IRB-2023-00067 and Approval Date:\n01/12/2023). The protocol employed in this work\nwas approved by the Tencent Institutional Review\nBoard. We remain steadfast in our commitment\nto uphold and adhere to the institution’s highest\nethical and professional benchmarks throughout\nour research endeavors.\n• Reproducibility Challenges and Mitigation\nStrategies. The evolving nature of closed com-\nmercial platforms indeed presents challenges to\nthe reproducibility of research. As these plat-\nforms continue to upgrade and improve, previ-\nous versions of models may be retired or modi-\nfied, which can make replication efforts problem-\natic. To tackle this issue, we have made several\nadditions to our paper: (1) Documentation of\nSpecifics: We have included exact versions of\nmodels we evaluated, along with the precise date\nof evaluation and other pertinent details. This\nallows for a clear record of the conditions under\nwhich our study was conducted. (2) Release of\nSystem Outputs: We release all system outputs,\nwhich ensures that researchers can reliably re-\nproduce and validate our results. (3) Advocacy\nfor Archiving Historical Versions: We emphasize\nthe importance for both the AI community and\ncommercial organizations to maintain archives\nof previous model iterations. By doing this, re-\n10It is based on the information provided by https://www.\nbls.gov/oes/current/oes273091.htm and https://www.\nbls.gov/oes/current/oes251124.htm.\n16655\nsearchers can readily access and evaluate past\nversions, ensuring continuity in analysis even as\nnew model versions emerge.\nAcknowledgements\nWe are grateful to the anonymous reviewers, area\nchairs and ethics committee for their insightful\ncomments and suggestions which will serve to im-\nprove the paper considerably. Their insights are in-\nvaluable, not only in helping us present our findings\nmore cautiously, but also in educating us beyond\nthe scope of this single paper.\nReferences\nGuangsheng Bao, Yue Zhang, Zhiyang Teng, Boxing\nChen, and Weihua Luo. 2021. G-transformer for\ndocument-level machine translation. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers).\nRachel Bawden, Rico Sennrich, Alexandra Birch, and\nBarry Haddow. 2018. Evaluating discourse phenom-\nena in neural machine translation. In Proceedings of\nthe 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 1 (Long Pa-\npers).\nWelch Bl. 1947. The generalization of ‘student’s’ prob-\nlem when several different population varlances are\ninvolved. Biometrika.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems.\nDallas Card, Peter Henderson, Urvashi Khandelwal,\nRobin Jia, Kyle Mahowald, and Dan Jurafsky. 2020.\nWith little power comes great responsibility. arXiv\npreprint arXiv:2010.06595.\nSheila Castilho. 2021. Towards document-level human\nmt evaluation: On the issues of annotator agreement,\neffort and misevaluation. Association for Computa-\ntional Linguistics.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nMarjan Ghazvininejad, Hila Gonen, and Luke Zettle-\nmoyer. 2023. Dictionary-based phrase-level prompt-\ning of large language models for machine translation.\narXiv preprint arXiv:2302.07856.\nYvette Graham, Barry Haddow, and Philipp Koehn.\n2020. Statistical power and translationese in ma-\nchine translation evaluation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing.\nNuno M. Guerreiro, Elena V oita, and André Martins.\n2023. Looking for a needle in a haystack: A com-\nprehensive study of hallucinations in neural machine\ntranslation. In Proceedings of the 17th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics.\nJunliang Guo, Zhirui Zhang, Linli Xu, Hao-Ran Wei,\nBoxing Chen, and Enhong Chen. 2020. Incorpo-\nrating bert into parallel sequence decoding with\nadapters. Advances in Neural Information Process-\ning Systems.\nAmr Hendy, Mohamed Abdelrehim, Amr Sharaf,\nVikas Raunak, Mohamed Gabr, Hitokazu Matsushita,\nYoung Jin Kim, Mohamed Afify, and Hany Hassan\nAwadalla. 2023. How good are gpt models at ma-\nchine translation? a comprehensive evaluation. arXiv\npreprint arXiv:2302.09210.\nGuoping Huang, Lemao Liu, Xing Wang, Longyue\nWang, Huayang Li, Zhaopeng Tu, Chengyan Huang,\nand Shuming Shi. 2021. Transmart: A practical in-\nteractive machine translation system. arXiv preprint\narXiv:2105.13072.\nYuchen Eleanor Jiang, Tianyu Liu, Shuming Ma, Dong-\ndong Zhang, Mrinmaya Sachan, and Ryan Cotterell.\n2023. Discourse centric evaluation of machine trans-\nlation with a densely annotated parallel corpus. arXiv\npreprint arXiv:2305.11142.\nWenxiang Jiao, Wenxuan Wang, Jen-tse Huang, Xing\nWang, and Zhaopeng Tu. 2023. Is chatgpt a good\ntranslator? a preliminary study. arXiv preprint\narXiv:2301.08745.\nMarzena Karpinska and Mohit Iyyer. 2023. Large lan-\nguage models effectively leverage document-level\ncontext for literary translation, but critical errors per-\nsist. arXiv preprint arXiv:2304.03245.\nTom Kocmi, Rachel Bawden, Ond ˇrej Bojar, Anton\nDvorkovich, Christian Federmann, Mark Fishel,\nThamme Gowda, Yvette Graham, Roman Grund-\nkiewicz, Barry Haddow, Rebecca Knowles, Philipp\nKoehn, Christof Monz, Makoto Morishita, Masaaki\nNagata, Toshiaki Nakazawa, Michal Novák, Martin\nPopel, and Maja Popovi´c. 2022. Findings of the 2022\nconference on machine translation (WMT22). In\nProceedings of the Seventh Conference on Machine\nTranslation.\n16656\nTom Kocmi and Christian Federmann. 2023. Large\nlanguage models are state-of-the-art evaluators of\ntranslation quality. arXiv preprint arXiv:2302.14520.\nKlaus Krippendorff. 2011. Computing krippendorff’s\nalpha-reliability.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transactions\nof the Association for Computational Linguistics.\nHongyuan Lu, Haoyang Huang, Dongdong Zhang, Hao-\nran Yang, Wai Lam, and Furu Wei. 2023. Chain-\nof-dictionary prompting elicits translation in large\nlanguage models. arXiv preprint arXiv:2305.06575.\nChenyang Lyu, Jitao Xu, and Longyue Wang. 2023.\nNew trends in machine translation using large lan-\nguage models: Case examples with chatgpt. arXiv\npreprint arXiv:2305.01181.\nXinglin Lyu, Junhui Li, Zhengxian Gong, and Min\nZhang. 2021. Encouraging lexical translation consis-\ntency for document-level neural machine translation.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing.\nValentin Macé and Christophe Servan. 2019. Using\nwhole document context in neural machine transla-\ntion. In Proceedings of the 16th International Confer-\nence on Spoken Language Translation, Hong Kong.\nAssociation for Computational Linguistics.\nMary L McHugh. 2012. Interrater reliability: the kappa\nstatistic. Biochemia medica.\nGraham Neubig and Zhiwei He. 2023. Zeno gpt ma-\nchine translation report.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Gray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. In Advances in Neural\nInformation Processing Systems.\nMatt Post. 2018. A call for clarity in reporting BLEU\nscores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. Journal of Machine Learning Research.\nRicardo Rei, Craig Stewart, Ana C Farinha, and Alon\nLavie. 2020. COMET: A neural framework for MT\nevaluation. In Proceedings of the 2020 Conference\non Empirical Methods in Natural Language Process-\ning. Association for Computational Linguistics.\nMatthew Snover, Bonnie Dorr, Rich Schwartz, Linnea\nMicciulla, and John Makhoul. 2006. A study of trans-\nlation edit rate with targeted human annotation. In\nProceedings of the 7th Conference of the Association\nfor Machine Translation in the Americas: Technical\nPapers.\nZewei Sun, Mingxuan Wang, Hao Zhou, Chengqi Zhao,\nShujian Huang, Jiajun Chen, and Lei Li. 2022. Re-\nthinking document-level neural machine translation.\nIn Findings of the Association for Computational\nLinguistics: ACL 2022.\nKatherine Thai, Marzena Karpinska, Kalpesh Krishna,\nBill Ray, Moira Inghilleri, John Wieting, and Mohit\nIyyer. 2022. Exploring document-level literary ma-\nchine translation with parallel paragraphs from world\nliterature. arXiv preprint arXiv:2210.14250.\nZhaopeng Tu, Yang Liu, Shuming Shi, and Tong Zhang.\n2018. Learning to remember translation history with\na continuous cache. Transactions of the Association\nfor Computational Linguistics.\nDavid Vilar, Markus Freitag, Colin Cherry, Jiaming Luo,\nViresh Ratnakar, and George Foster. 2022. Prompt-\ning palm for translation: Assessing strategies and\nperformance. arXiv preprint arXiv:2211.09102.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019a.\nContext-aware monolingual repair for neural ma-\nchine translation. In Proceedings of the 2019 Con-\nference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing.\nElena V oita, Rico Sennrich, and Ivan Titov. 2019b.\nWhen a good translation is wrong in context: Context-\naware machine translation improves on deixis, ellip-\nsis, and lexical cohesion. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics.\nElena V oita, Pavel Serdyukov, Rico Sennrich, and Ivan\nTitov. 2018. Context-aware neural machine trans-\nlation learns anaphora resolution. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers).\nLongyue Wang. 2019. Discourse-aware neural machine\ntranslation. Ph.D. thesis, Ph. D. thesis, Dublin City\nUniversity, Dublin, Ireland.\nLongyue Wang, Zefeng Du, Donghuai Liu, Cai Deng,\nDian Yu, Haiyun Jiang, Yan Wang, Leyang Cui,\nShuming Shi, and Zhaopeng Tu. 2023a. Disco-bench:\nA discourse-aware evaluation benchmark for lan-\nguage modelling. arXiv preprint arXiv:2307.08074.\nLongyue Wang, Siyou Liu, Mingzhou Xu, Linfeng\nSong, Shuming Shi, and Zhaopeng Tu. 2023b. A\nsurvey on zero pronoun translation. arXiv preprint\narXiv:2305.10196.\n16657\nLongyue Wang, Zhaopeng Tu, Yan Gu, Siyou Liu, Dian\nYu, Qingsong Ma, Chenyang Lyu, Liting Zhou, Chao-\nHong Liu, Yufeng Ma, Weiyu Chen, Yvette Graham,\nBonnie Webber, Philipp Koehn, Andy Way, Yulin\nYuan, and Shuming Shi. 2023c. Findings of the\nWMT 2023 shared task on discourse-level literary\ntranslation. In Proceedings of the Eighth Conference\non Machine Translation (WMT).\nLongyue Wang, Zhaopeng Tu, Shuming Shi, Tong\nZhang, Yvette Graham, and Qun Liu. 2018a. Trans-\nlating pro-drop languages with reconstruction mod-\nels. In Proceedings of the 2018 AAAI Conference on\nArtificial Intelligence, volume 32.\nLongyue Wang, Zhaopeng Tu, Xing Wang, and Shum-\ning Shi. 2019. One model to learn both: Zero pro-\nnoun prediction and translation. In Proceedings of\nthe 2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing.\nLongyue Wang, Zhaopeng Tu, Andy Way, and Qun\nLiu. 2017. Exploiting cross-sentence context for\nneural machine translation. In Proceedings of the\n2017 Conference on Empirical Methods in Natural\nLanguage Processing.\nLongyue Wang, Zhaopeng Tu, Andy Way, and Qun\nLiu. 2018b. Learning to jointly translate and predict\ndropped pronouns with a shared reconstruction mech-\nanism. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing.\nLongyue Wang, Zhaopeng Tu, Xiaojun Zhang, Hang Li,\nAndy Way, and Qun Liu. 2016. A novel approach\nto dropped pronoun translation. In Proceedings of\nthe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nLongyue Wang, Mingzhou Xu, Derek F. Wong, Hongye\nLiu, Linfeng Song, Lidia S. Chao, Shuming Shi, and\nZhaopeng Tu. 2022. GuoFeng: A benchmark for\nzero pronoun recovery and translation. In Proceed-\nings of the 2022 Conference on Empirical Methods\nin Natural Language Processing.\nRF Woolson. 2007. Wilcoxon signed-rank test. Wiley\nencyclopedia of clinical trials.\nTong Xiao, Jingbo Zhu, Shujie Yao, and Hao Zhang.\n2011. Document-level consistency verification in ma-\nchine translation. In Proceedings of Machine Trans-\nlation Summit XIII: Papers.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies.\nBiao Zhang, Ankur Bapna, Melvin Johnson, Ali Dabir-\nmoghaddam, Naveen Arivazhagan, and Orhan Firat.\n2022. Multilingual document-level translation en-\nables zero-shot transfer from sentences to documents.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers).\nZaixiang Zheng, Xiang Yue, Shujian Huang, Jiajun\nChen, and Alexandra Birch. 2020. Toward making\nthe most of context in neural machine translation.\nArXiv.\nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao Qin,\nWengang Zhou, Houqiang Li, and Tie-Yan Liu. 2020.\nIncorporating bert into neural machine translation.\narXiv preprint arXiv:2002.06823.\n16658\nA Appendix\nA.1 Significance Testing\nMachine Translation For automatic evaluations,\nwe used the non-parametric one-tailed Wilcoxon\nsigned-rank test (Woolson, 2007). About results\nin Table 4, the significance test contrasting GPT-\n3.5/GPT-4 with others yields a p-value of less than\n0.05, indicating they do significantly boosts trans-\nlation quality. For human evaluation, we employ\nWelch’s t-test (Bl, 1947). Table 10 shows the over-\nall significance test by combining all datasets, and\nthe results for each domain are also consistent.\np Automatic Human\nGPT-3.5 GPT-4 GPT-3.5 GPT-4\nGoogle 0.0000 0.0000 0.0000 0.0000\nDeepL 0.0000 0.0000 0.0000 0.0000\nTencent 0.0309 0.0001 0.0000 0.0000\nGPT-3.5 n/a 0.0022 n/a 0.0028\nGPT-4 n/a n/a n/a n/a\nTable 10: The results of significance test in Table 4.\nProbing Task We performed a Welch’s t-test (Bl,\n1947) with unequal variances to verify the signifi-\ncance between GPT-3.5 and GPT-4 in Table 7 and\n8. We find that the corresponding two-tailed p-\nvalue is smaller than 0.001, which indicates the\nsignificance between them.\nA.2 Human Evaluation Guidelines\nGuidelines Table 12 presents the human evalu-\nation criteria for document-level translation, with\nscores ranging from 0 to 5. A score of 5 indicates\nexcellent overall translation quality, with no gram-\nmatical errors, accurate word choice, consistent key\nterms, and consistent context and tone throughout\nthe passage. A score of 0 indicates poor overall\ntranslation quality, with more than half of the trans-\nlation being mistranslated or missing, inconsistent\nkey terms, and poor fluency and clarity. In between\nscores reflect varying degrees of translation quality,\nwith factors such as fluency, accuracy, consistency\nof key terms, and context and tone consistency af-\nfecting the score.\nHuman Evaluators We employed two profes-\nsional evaluators for our study through Tencent’s\ndesignated supplier. Table 11 detailed their back-\nground related to this task.\nLevel Evaluator A (Zh-En)\nPosition lecture at an international university\nEducation Ph.D degree in Translation Studies, in-\nternational university\nCertificationCATTI Translation Level 1\nExperience Translator for the academic journal;\nParticipant in the Construction and Ap-\nplication Research of the bilingual ter-\nminology knowledge base, a National\nSocial Science Fund project.\nLevel Evaluator B (Zh-En)\nPosition manager of quality control at a famous\ntranslation company\nEducation Master in English, international univer-\nsity\nCertificationTEM8\nExperience Translator for the National Internet In-\nformation Center; Translator and proof-\nreader for top company.\nLevel Evaluator C (Ru-En)\nPosition interpreter at an import&export trading\ncompany\nEducation Master in Russian Written Translation,\ninternational university\nCertificationRussian Professional Level 8, English\nCET6\nExperience Interpreter in several import&export\ntrading companies.\nLevel Evaluator D (Ru-En)\nPosition translator at a translation company\nEducation Master in Russian Written Translation,\ninternational university\nCertificationRussian Professional Level 8, English\nCET6\nExperience Work in several translation companies.\nTable 11: The basic background of human annotators.\nA.3 Training Methods in LLMs\n• GPT-3 (Brown et al., 2020): A LLM with 175B\nparameters pre-trained on large-scale web cor-\npora (approximately 400B tokens) We used Ope-\nnAI API davinci.\n• InstructGPT ( SFT) (Ouyang et al., 2022): A\nGPT-3 model trained with supervised fine-tuning\non human demonstrations similar to Ouyang et al.\n(2022).11\n• InstructGPT (FeedME-1) (Ouyang et al., 2022):\nAn improved version of GPT-3 with supervised\n11https://platform.openai.com/docs/\nmodel-index-for-researchers .\n16659\nfine-tuning on human-written demonstrations\nand model-generated examples rated by humans\nwith high quality.\n• InstructGPT (FeedME-2) (Ouyang et al., 2022):\nAn improved version of Codex (Chen et al., 2021)\nwith supervised fine-tuning on human-written\ndemonstrations and human-rated examples with\nhigh quality.12\n• InstructGPT (PPO) (Ouyang et al., 2022): An\nimproved version of InstructGPT (FeedME-2)\nwith extra training of RLHF, which is trained\nwith a reward model learned from human com-\nparisons.13\n• ChatGPT: A further improved version of In-\nstrucGPT that can perform tasks via dialogue\nwith users, which is able to take contextual infor-\nmation in dialogue into consideration.\n12https://openai.com/blog/openai-codex.\n13https://openai.com/blog/\ninstruction-following.\n16660\nScore General Quality Discourse Awareness\n5 Translation passes quality control; the overall\ntranslation is excellent. Translation is very\nfluent with no grammatical errors and has\nbeen localized to fit target language. Word\nchoice is accurate with no mistranslations.\nThe translation is a 100% true to the source\ntext.\nNo inconsistency relating to key terms such\nas names, organization, etc. Linking words\nor expressions between sentences keeps the\nlogic and language of the passage clear and\nfluent. Context and tone are consistent\nthroughout. The style of the text conforms to\nthe culture and habit of the target language.\n4 Translation passes quality control; the overall\ntranslation is very good. Translation is fluent.\nAny errors that may be present does not affect\nthe meaning or comprehension of the text.\nMost word choice is accurate, but some may\ncause ambiguity. Key terms are consistent.\nInconsistency is limited to non-key terms.\nLogical and language is clear and fluent.\nSome sentences lack transition but does not\naffect contextual comprehension. Topic is\nconsistent. Tone and word choice may be in-\nconsistent, but comprehension is not affected.\nTranslation conforms to the culture and habit.\n3 Translation passes quality control; the overall\ntranslation is ok. Translation is mostly flu-\nent but there are many sections that require\nrereading due to language usage. Some word\nchoice is inaccurate or errors but meaning of\nthe sentence can be inferred from context.\nSome key terms may be inconsistent. Most\nsentences translation smoothly and logically\nbut some sentences that may seem abrupt due\nto lack of linkage. Topic is consistent. Tone\nand word choice is inconsistent, noticeably\naffecting the accuracy of reading comprehen-\nsion.\n2 Translation does not pass quality control;\nthe overall translation is poor. Meaning is\nunclear or disjointed. Even with multiple\nrereading, passage may still be incomprehen-\nsible. Translation is not accurate to the source\ntext or is missing in large quantities, causing\nthe translation to deviate from the source text.\nMany key terms are inconsistent, needing\nmultiple rereading to understand context of\nthe passage. Some linkages are present but\noverall, the passage lacks fluency and clar-\nity, causing trouble with comprehension. The\ntopic or tone is different from the other pas-\nsages, affecting reading comprehension.\n1 Translation does not pass quality control; the\noverall translation is very poor. More than\nhalf of the translation is mistranslated or miss-\ning.\nKey terms are inconsistent, causing great\ntrouble with comprehension. Some linkages\nare present but overall, the passage lacks\nfluency and clarity, heavily interfering with\ncomprehension. The topic or tone is different\nfrom the other passages, heavily interfering\nwith comprehension.\n0 Translation output is unrelated to the source\ntext.\nOutput is unrelated to previous or following\nsections.\nTable 12: Human evaluation criteria on document-level translation.\n16661",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6868280172348022
    },
    {
      "name": "Machine translation",
      "score": 0.5721551179885864
    },
    {
      "name": "Natural language processing",
      "score": 0.5599537491798401
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5460202693939209
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4872201979160309
    },
    {
      "name": "Translation (biology)",
      "score": 0.44041404128074646
    },
    {
      "name": "Testbed",
      "score": 0.42871636152267456
    },
    {
      "name": "World Wide Web",
      "score": 0.2010161578655243
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}