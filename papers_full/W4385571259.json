{
  "title": "How Well Do Large Language Models Perform on Faux Pas Tests?",
  "url": "https://openalex.org/W4385571259",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A1966808150",
      "name": "Natalie Shapira",
      "affiliations": [
        "Bar-Ilan University"
      ]
    },
    {
      "id": null,
      "name": "Guy Zwirn",
      "affiliations": [
        "Hadassah Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2144962531",
      "name": "Yoav Goldberg",
      "affiliations": [
        "Bar-Ilan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2787845769",
    "https://openalex.org/W2946609015",
    "https://openalex.org/W4385565410",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W2970536767",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4295992949",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385572854",
    "https://openalex.org/W3102187933",
    "https://openalex.org/W2318994447",
    "https://openalex.org/W2970062726",
    "https://openalex.org/W646514108",
    "https://openalex.org/W2996908057",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4226367199",
    "https://openalex.org/W4327644573",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4221143736",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W3173699110",
    "https://openalex.org/W2139188400"
  ],
  "abstract": "Motivated by the question of the extent to which large language models \"understand\" social intelligence, we investigate the ability of such models to generate correct responses to questions involving descriptions of faux pas situations. The faux pas test is a test used in clinical psychology, which is known to be more challenging for children than individual tests of theory-of-mind or social intelligence. Our results demonstrate that, while the models seem to sometimes offer correct responses, they in fact struggle with this task, and that many of the seemingly correct responses can be attributed to over-interpretation by the human reader (\"the ELIZA effect\"). An additional phenomenon observed is the failure of most models to generate a correct response to presupposition questions. Finally, in an experiment in which the models are tasked with generating original faux pas stories, we find that while some models are capable of generating novel faux pas stories, the stories are all explicit, as the models are limited in their abilities to describe situations in an implicit manner.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 10438–10451\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nHow Well Do Large Language Models Perform on Faux Pas Tests?\nNatalie Shapira1 Guy Zwirn2 Yoav Goldberg1,3\n1Bar-Ilan University, Ramat Gan, Israel\n2Hadassah University Medical Center, Jerusalem, Israel\n3Allen Institute for AI, Tel Aviv, Israel\nnd1234@gmail.com\nAbstract\nMotivated by the question of the extent to\nwhich large language models “understand” so-\ncial intelligence, we investigate the ability of\nsuch models to generate correct responses to\nquestions involving descriptions of faux pas\nsituations. The faux pas test is a test used in\nclinical psychology, which is known to be more\nchallenging for children than individual tests of\ntheory-of-mind or social intelligence. Our re-\nsults demonstrate that, while the models seem\nto sometimes offer correct responses, they in\nfact struggle with this task, and that many of the\nseemingly correct responses can be attributed\nto over-interpretation by the human reader (“the\nELIZA effect”). An additional phenomenon ob-\nserved is the failure of most models to generate\na correct response to presupposition questions.\nFinally, in an experiment in which the models\nare tasked with generating original faux pas\nstories, we find that while some models are ca-\npable of generating novel faux pas stories, the\nstories are all explicit, as the models are lim-\nited in their abilities to describe situations in an\nimplicit manner.\n1 Introduction\nTheory of Mind (ToM) is the ability or skill to iden-\ntify, evaluate or attribute mental states—beliefs,\nintents, desires, pretending, knowledge, etc.—to\noneself and others and to understand that others\nhave perspectives that are different from one’s own\n(Wimmer and Perner, 1983). A social skill is any\ncompetence facilitating interaction and communi-\ncation with others (Dowd and Tierney, 2005). Ide-\nally, automated agents that interact with people\nshould possess such social common sense abilities\n(Choi, 2022), and indeed, a recent trend in the field\nof AI aims to address challenges related to social\nskills and commonsense (Sakaguchi et al., 2021;\nLe et al., 2019; Talmor et al., 2022; Sap et al., 2019;\nZellers et al., 2019; Hessel et al., 2022; Lin et al.,\n2020; Shapira et al., 2023).\nFigure 1: A faux pas story from (Baron-Cohen et al.,\n1999) and sample answers of large language models.\nWhile ChatGPT’s answer is incorrect according to hu-\nman nature response (lack of theory-of-mind), it offers\nrelevant details to the question that causes the ELIZA\neffect. Other models’ responses in the example (GPT-J,\nGPT2) are vague, incoherent, and out of context.\nTo what extent do Large Language Models\n(LLMs; Brown et al., 2020; Bommasani et al.,\n2021; Zhao et al., 2023)—models that were trained\non massive amounts of both supervised and un-\nsupervised language data, and which constitute\nthe current state of the art in language-based rea-\nsoning and communication—possess the ability\nto effectively reason about implicit social situa-\ntions, that may not be explicitly discussed in texts?\nSap et al. (2022) examine zero-shot theory-of-mind\nabilities in LLMs (GPT-3-Davinci; Brown et al.,\n2020) and show that the models struggle with ToM-\nbased tasks. Since then, ChatGPT,1 a new model\ntrained on additional supervised data and in particu-\nlar human-dialog data, suggests improved abilities\nat such tasks.\nWe propose to push beyond the current theory-\nof-mind tests and consider the task of “recognition\nof faux pas” , an established task in the clinical\npsychology domain (Baron-Cohen et al., 1999).\nThe faux pas task combines the SocialIQa (Sap\n1https://openai.com/blog/chatgpt/\n10438\net al., 2019) and the ToMi (Le et al., 2019) tasks\nmentioned in (Sap et al., 2022) and is considered\nto be more difficult for children than any of the\nindividual tasks on their own. We show that the\ntask is also challenging for state-of-the-art LLMs.\nWe describe two studies, examining different as-\npects related to the recognition of faux pas within\nLLMs.2 In the first study (§3) we evaluate, together\nwith a clinical psychologist with diagnosis exper-\ntise, the faux pas test results on LLMs. At the first\nstage (§3.1) we perform a qualitative analysis of\nthe responses of the models and propose a new an-\nnotation method that tries to capture quantitatively\npart of “the ELIZA effect” (Weizenbaum, 1976)\na phenomenon where an individual may attribute\nunderstanding to a machine based on its ability to\nrespond in a seemingly intelligent manner, even\nif the response does not fully answer the question.\nIn the second stage, the models were restricted to\nclosed-ended questions by requiring a yes or no\nanswer or without explanations (§3.2). The results\nshow that while the models seem to sometimes\noffer correct responses, they in fact struggle with\nthis task and that many of the seemingly correct\nresponses can be attributed to over-interpretation\nby the human reader.\nAn additional phenomenon observed is that most\nof the models failed to generate a correct response\nto “What did they say that they should not have\nsaid?” when the question was based on a false\nassumption and there was no problematic statement\nin the text.\nIn the second study (§4) we instruct models to\ngenerate 20 original faux pas stories which we man-\nually evaluate, showing that while the best models\ncan generate some faux pas stories, they can only\ndo it in an explicit manner, and struggle with the\nimplicit aspects, which are central to the ToM.\n2 Recognition of Faux Pas\nFaux Pas (French for “false step”) is defined as\n“when a speaker says something without consider-\ning if it is something that the listener might not\nwant to hear or know, and which typically has neg-\native consequences that the speaker never intended”\n(Baron-Cohen et al., 1999).\nOne example of a faux pas situation is when a\nguest tells their hosts that they “like cakes except\nfor apple pie”, without realizing that the hosts have\n2The original clinical test and our research were done in\nEnglish.\nmade an apple pie for them. The complexity of\nthe situation depends not only on the content of\nthe statement (“except for apple pie”) but also on\nthe context in which it was made (e.g., the host\nhad made an apple pie and the guest was unaware).\nFaux pas is the \"uhoh!\" emotion most people would\nfeel when they reveal the reality of the context. In\nthe mentioned example, the statement may not be\nproblematic if the hosts had made a cheesecake\ninstead.\nIn the original test,3 the subject is told 10 stories\nthat contain faux pas. At the end of each story, the\nsubject is asked 4 questions:\n• Q1 - Faux Pas Detection Question - In the\nstory did someone say something that they\nshould not have said?\n• Q2 - Identification Question - What did they\nsay that they should not have said?\n• Q3 - Comprehensive Question(this question\nis different for each story)\n• Q4 - False Belief Question . Did they\nknow/remember that? (this question is dif-\nferent for each story)\nEach faux pas story that is answered correctly (i.e.,\nall four questions are correct) scored 1 point. In a\nclinical trial, the average score for 9- to 11-year-old\nchildren is 8.2 (SD=1.56) out of 10 faux pas stories\n(Baron-Cohen et al., 1999).\nWe note that the faux pas test was initially devel-\noped to diagnose autism or Asperger syndrome in\nchildren. Here, we do not diagnose models.\nFaux Pas as a task can be viewed as a composi-\ntion of the two tasks that were presented separately\nby Sap et al. (2022): (1) SocialIQa (Sap et al., 2019)\nthat is related to analyzing and understanding so-\ncial situations such as reasoning about motivations\n(e.g., Why would someone accidentally push some-\none in a narrow elevator? to enter the elevator),\nwhat happens next (e.g., What would one want to\ndo after food spilled on the floor? mop up) and\nemotional reaction (e.g, How would others feel af-\nter a scene where the hero is struggling with the\nvillain? hope that the hero will win). (2) ToMi (Le\net al., 2019) that is related to the ability to perceive\nthe existence of different perspectives for different\nagents (e.g., Sally puts a marble in a basket and\n3Table 4 in the Appendix contains an example of a full test\n- a faux pas and control stories with questions and the expected\nanswers.\n10439\nleft the room. Anne moves the marble to a closet.\nWhere will Sally look for the marble?).\nThe compositionality between the data sets is\ncurrently at the essence level and not at the prac-\ntical level. Faux-pas test is based on mental state\ninference and the ability to recognize false beliefs\n(Korman et al., 2017). The SocialIQa includes\nquestions about reasoning about motivation and\nemotional reactions i.e., “mental state”. The ToMi\naims to assess the recognition of false beliefs. For\nexample in the story mentioned in Figure 1, the\nreader is expected to infer (1) When someone is\ntold “I never liked that object” when the object is a\ngift from that person, they may be hurt/feel disre-\nspected (mental state). (2) Under the assumption\nof good intentions, a reasonable possible interpre-\ntation is that Richard did not remember/know that\nJames brought him the gift although the reader\nknows this fact (false belief).\nWhile most ToM clinical tests are designed for\nsubjects with a mental age of 4-6 years, according\nto the literature, faux pas detection is a ToM clinical\ntest designed to recognize Asperger Syndrome or\nHigh-Functioning Autism in children ages 7-11\n(Baron-Cohen et al., 1999). This may suggest the\ndifficulty of the test.\nFor the purposes of this study, we will use 20\nexamples (10 containing faux pas and 10 control\nexamples) as they appear in the original test in\nthe literature. In addition, aided by LLMs we at-\ntempted to generate additional 20 original faux pas\nstories and annotate their quality (§4)\n3 Study 1: Faux Pas Test Performance\nIn this study the first and second authors of the\npaper (an NLP researcher and a clinical psychol-\nogist trained in diagnostics) manually evaluated\nthe responses to the faux pas tests generated by 14\ndifferent language models that were run in a zero-\nshot manner (Liu et al., 2021) (for technical details\nregarding prompts and parameters see Appendix\nA.1).4\nThe annotation included 3 phases. Phase 1: A\nresearcher (NLP researcher in study/table 1 and\nClinical Psychology Diagnostician in studies/tables\n2-3) labeled the data and produced simple guide-\nlines. Phase 2: the other researcher labeled the data\naccording to guidelines. Phase 3: both researchers\n4Annotated data of study 1 and generated stories of study 2\ncould be found at https://github.com/NatalieShapira/\nFauxPasEAI.\nFigure 2: Example of ChatGPT response demonstrates\nthe dimensions of illusion.\ndiscussed the final decision in cases where there\nwas disagreement. The agreement between the two\nannotators was 82-100% and after reconciliation\n100%.\nFirst, the models were given a story and an\nopen-ended question “In the story did someone\nsay something that they should not have said?”\n(§3.1). The results of this task raised concerns\nabout ELIZA effect. To accommodate for that, in\nthe second stage the models were given the com-\nplete test (4 questions) with closed-ended ques-\ntions requiring a yes or no answer or without expla-\nnations “In the story did someone say something\nthat they should not have said? Answer with “Yes”\nor “No” only, without explanations. In case of\ndoubt, answer according to the most probable an-\nswer. ”(§3.2).5\nWe found the open-ended version of the ques-\ntions (Table 2) both expensive (manually by ex-\nperts) and problematic from the perspective of the\nELIZA effect i.e., some responses contain the cor-\nrect answer but at the same time also suggest the\nwrong answer, in a persuasive way, without a clear-\ncut final answer (§3.1 and Figure 2). The restricted\nyes/no version of the questions (§3.2 and Table 1)\nis clear-cut and could be done automatically.\n3.1 Assessing the ELIZA Effect in Responses\nWe assess the quality of the Q1 responses as an\nopen-end question, on several quality factors. The\ngoal is to appraise whether the response provides an\nELIZA effect, giving an illusion of understanding\n(see Figure 2).\nThe annotation of the response consists of the\nfollowing factors:\nCorrect: Contains the correct answer (even if not\nthe full answer or there are also wrong parts in the\nresponse).\n5Table 5 in the Appendix lists all questions versions.\n10440\nFaux Pas Control\nModel Q1 Q2 Q3 Q4 Final Q1 Q2 Q3 Q4 Final\nChatGPT 0.6 0.7 1.0 0.7 0.3 1.0 1.0 1.0 0.9 0.9\nGPT3 0.5 0.8 1.0 0.6 0.3 1.0 0.0 1.0 0.7 0.0\nFlan-T5-xxl 0.5 0.7 1.0 0.7 0.4 0.8 0.0 1.0 0.8 0.0\nFlan-T5-xl 0.5 0.9 1.0 0.7 0.4 0.6 0.0 0.8 0.7 0.0\nFlan-T5-large 0.9 0.5 0.9 0.4 0.2 0.0 0.0 0.8 0.5 0.0\nT5-11b 0.8 0.7 0.8 0.5 0.1 0.0 0.0 0.4 0.8 0.0\nTable 1: Accuracy of the responses to the 20 stories (10 faux pas and 10 control) by different models on the 4 faux\npas questions. The final test result is correct when all 4 sub-questions are marked as correct. Models with a final\nscore of 0 were left out of the table (GPT2, GPT-J, Flan-T5{base, small}, T5{3b, large, base, small}). Compared to\naverage recognition rate (M=0.82, SD=0.156) of normally developed children,all models fail on the faux pas task.\nCor- Cohe- Persu- Equi- Personi-\nModel rect rent asive vocal fication\nChatGPT 18 20 20 20 20\nGPT3 16 16 20 0 0\nFlan-T5-xxl 11 16 13 0 0\nFlan-T5-xl 10 15 18 0 1\nFlan-T5-large2 5 10 0 0\nT5-11b 10 19 20 0 0\nTable 2: The \"ELIZA effect\" - assessment of tested\nlanguage models on their responses to the 20 control\nand faux pas stories. The scores are the number of\nstories that meet the criteria. A high score indicates an\nillusion of understanding.\nCoherent: Correct grammar, in-context response,\nthe response makes sense, the discourse flows (e.g.,\nthere is grounding, full-long answer, finished sen-\ntence, there is an answer to the question asked). We\nignored unnecessary dots or question marks.\nPersuasive: Providing information beyond \"Yes\"\nor \"No\" that supports decision such as: (A) Par-\ntial knowledge of the situation, e.g., the ability to\nanswer some other questions related to the situ-\nation correctly i.e., providing information about\nQ4 as a response to Q1 “scratch points” even if\nthey were not asked about the information in the\ncurrent question. (B) Wrong but logical answers\n(e.g., a scenario in low probability but not zero)\nor contains general world knowledge (e.g., “it ex-\npresses negative feelings towards people who work\nas ... ’’, “possibly to avoid any further discomfort\nor embarrassment”).\nEquivocal: Providing non-decisive wrong answers\n(“difficult to say for sure”, “might still have been\nperceived”, “but it’s not necessarily”, “possible”).\nPersonification: Speaking in a human-like manner\n(“It doesn’t seem like”, “I think”).\nTable 2 summarizes the assessment annotation.\nAs seen, a few language models provide responses\nthat appear to demonstrate a good understanding,\nhowever, we will next show that this is often indeed\nan illusion.\n3.2 Results on the Faux Pas Closed-Task\nAs indicated in Table 1, the performance of the\nmodels on faux pas tests is inadequate. The highest\nscore achieved by any of the evaluated models is\n0.4, by Flan-T5-xxl and Flan-T5-xl, which is sig-\nnificantly lower than the average recognition rate\nof 0.82 (SD=0.156) reported for normally develop-\ning 9- to 11-year-old children (Baron-Cohen et al.,\n1999).\nAnother noteworthy result is that all models (ex-\ncept ChatGPT)6 performed poorly in Q2 of the\ncontrol stories, achieving a score of 0. In the faux\npas stories, question Q2 “What did they say that\nthey should not have said?” is asking for a specific\nproblematic statement that was made in the story,\nwhereas in the control stories (which are neutral\nstories that do not contain any problematic state-\nments), question Q2 is based on a false assumption,\nthat there is a problematic statement in the text. The\nmodels’ responses were either picking an arbitrary\nutterance from the story or generating delusional\ntext (compared to ChatGPT which simply responds\nwith \"There doesn’t seem to be anything inappro-\npriate or disrespectful said in the story.\"). This is\ndespite the fact that some of the models even recog-\nnized that there was no problematic statement in the\nstory and answered the first question correctly. The\ndifficulty of models with presupposition questions\nis a well-known phenomenon in the QA domain,\nas reported in previous research (Yu et al., 2022;\nKim et al., 2021; Rajpurkar et al., 2018).\n6At the paper submission time, the way to access ChatGPT\nwas through the web. In later tests with direct access to the API\n(gpt-3.5-turbo-0301), it turns out that the advantage was due\nto the history of the messages that helped keep the responses\nconsistent.\n10441\nFull Explicit\nModel CoherentFaux PasFaux PasControl\nChatGPT 20 0 8 10*\nGPT3 20 0 0 10*\nFlan-T5-xxl 12 0 0 0\nFlan-T5-xl 0 0 0 0\nTable 3: Assessment of the 20 stories generated by\nlanguage models (10 control and 10 faux pas).\n* Too simplistic; only clear positive/neutral attitude.\n4 Study 2: Generation Abilities\nIn this study, we developed instructions for creating\nfaux pas stories, which included a definition of\nfaux pas, examples of two stories that contain faux\npas, and two corresponding control stories. The\ninstructions also highlighted potential pitfalls and\nasked to generate 20 new diverse stories (for the\nfull instructions see Appendix A.3).\nA model’s (ChatGPT, GPT3-text-davinci-003,\nFlanT5-xxl and FlanT5-xl) output was evaluated\nby the first and second authors, experts in NLP and\nin clinical psychology. The results are summarized\nin Table 3.\nChatGPT generated 8 faux pas stories (with cor-\nresponding control stories). However, the stories\nhad a limitation in that they were all explicit, and\nfailed to create implicit situations where one of the\ncharacters lacks information (e.g., explicitly men-\ntioning “not realizing that the woman was one of\nthe guests at the dinner party”).7,8 Additionally, all\ncontrol stories were too simplistic and contained\nclear positive/neutral attitudes.\nGPT3 generated 10 stories with corresponding\ncontrol stories, however, none of the stories were\nfaux pas. Although some of the stories contained\nsomething offensive, the offense was not caused\nby a lack of information. E.g., a bad faux pas\nstory: Sara and her friends were at the mall. They\nwere looking at clothes when one of her friends,\nEmily, said \"I love this dress, but I don’t think I\ncan afford it.\" Sara then said \"You don’t have to\nworry about money, your parents are rich.\" Emily\n7In another experiment, where the task was to correct the\nstories by changing the explicit statement and describing it\nin an implicit manner, two outcomes were observed: either\nthe model left the story unchanged or the explicit statement\nwas removed completely, resulting in an unclear situation. For\nexample, when the story specifically stated that the speaker\nmade a faux pas because she was unaware that the person she\nwas talking about was present in the room, after the removal\nof this sentence, the speaker was gossiping about someone,\nand also the reader does not know that someone is in the room.\n8We manually fixed part of the stories and released them\nwith the annotated data.\nwas embarrassed because she had forgotten that\nher parents were wealthy. In this story, Sara said\nsomething that is considered a bit rude and also\ncaused Emily to feel embarrassed, but it wasn’t\na result of Sara’s false belief (it did not happen\nbecause she didn’t know something). In addition,\npeople do not usually forget their parents are rich,\nand the embarrassment emotion is bizarre in this\ncontext (it is not indicated that Sara is poor).\nIn addition, the stories had other problems, such\nas non-coherent-emotions issues (i.e., not using the\nappropriate emotion to describe situations). E.g.,\na non-coherent emotion story: John and his fam-\nily were visiting his grandmother for the weekend.\nHis grandmother asked him how school was going\nand he said “It’s okay, but I’m not doing very well\nin math. ” His grandmother then said “Oh, that’s\ntoo bad. Your father was never very good at math\neither. ” John was embarrassed because he had\nforgotten that his father had struggled with math\nin school. Besides that it is definitely not a faux\npas story, there is another problem with the emo-\ntional coherence - why does the fact that John had\nforgotten that his father had struggled with math\nin school make him embarrassed? This is not the\nappropriate emotion here.\nLike ChatGPT’s control stories, the control sto-\nries generated by GPT3 were also too simplistic.\nFlan-T5-xxl barely succeeded in creating stories\nand failed to create faux pas or control stories. Flan-\nT5-xl failed to create stories at all (See Appendix\nA.4 for examples and issues).\n5 Conclusion and Future Work\nIn conclusion, the results of this study demonstrate\nthat large language models struggle with correctly\nidentifying and responding to faux-pas situations.\nThis suggests that these models do not possess\na strong notion of social intelligence and theory\nof mind. Additionally, the phenomenon of the\n“ELIZA effect” was observed, where seemingly\ncorrect responses were found to be attributed to\nover-interpretation by the human reader. Further-\nmore, when the models were tasked with generat-\ning original faux pas stories, it was found that they\nwere limited in their abilities to describe situations\nin an implicit manner. Future work will look for\nmore clinical tests that challenge today’s LLMs and\ndevelop large-scale datasets and methods to crack\nthe challenge.\n10442\nLimitations\nIt is important to note that the study is based on a\nlimited set of examples and although it is enough to\ngive a signal if a system is struggling or not in faux\npas tests, the number of stories is not sufficient for\nstatistically significant ranking between systems.\nEthical Statement\nThe study’s scope did not include the representation\nof harm toward specific populations. The narratives\nwere evaluated by a clinical psychologist to ensure\nthat they did not contain offensive content. How-\never, it is important to acknowledge the potential\nvalue of further research on the representation of\nharm in relation to culturally sensitive and socially\ncontroversial topics.\nAcknowledgements\nWe would like to thank Vered Shwartz, Ori Shapira,\nOsnat Baron Singer, Tamar Nissenbaum Putter,\nMaya Sabag, Arie Cattan, Uri Katz, Mosh Levy,\nAya Soffer, David Konopnicki, and IBM-Research\nstaff members for helpful discussions and contribu-\ntions, each in their own way. We thank the anony-\nmous reviewers for their insightful comments and\nsuggestions. This project was partially funded by\nthe European Research Council (ERC) under the\nEuropean Union’s Horizon 2020 research and in-\nnovation program, grant agreement No. 802774\n(iEXTRACT); and by the Computer Science De-\npartment of Bar-Ilan University.\nReferences\nSimon Baron-Cohen, Michelle O’riordan, Valerie Stone,\nRosie Jones, and Kate Plaisted. 1999. Recognition\nof faux pas by normally developing children and\nchildren with asperger syndrome or high-functioning\nautism. Journal of autism and developmental disor-\nders, 29(5):407–418.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. arXiv preprint\narXiv:2108.07258.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYejin Choi. 2022. The curious case of commonsense\nintelligence. Daedalus, 151(2):139–155.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nTom P Dowd and Jeff Tierney. 2005. Teaching social\nskills to youth: A step-by-step guide to 182 basic to\ncomplex skills plus helpful teaching techniques. Boys\nTown Press.\nJack Hessel, Ana Marasovi ´c, Jena D Hwang, Lillian\nLee, Jeff Da, Rowan Zellers, Robert Mankoff, and\nYejin Choi. 2022. Do Androids laugh at electric\nsheep? Humor \"Understanding\" benchmarks from\nthe new yorker caption contest. arXiv preprint\narXiv:2209.06293.\nNajoung Kim, Ellie Pavlick, Burcu Karagol Ayan, and\nDeepak Ramachandran. 2021. Which linguist in-\nvented the lightbulb? Presupposition verification for\nquestion-answering. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3932–3945, Online. Association\nfor Computational Linguistics.\nJoanna Korman, Tiziana Zalla, and Bertram F Malle.\n2017. Action understanding in high-functioning\nautism: The faux pas task revisited. In CogSci.\nMatthew Le, Y-Lan Boureau, and Maximilian Nickel.\n2019. Revisiting the evaluation of theory of mind\nthrough question answering. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5872–5877.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\n10443\nPranav Rajpurkar, Robin Jia, and Percy Liang. 2018.\nKnow what you don’t know: Unanswerable ques-\ntions for SQuAD. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 2: Short Papers), pages 784–789,\nMelbourne, Australia. Association for Computational\nLinguistics.\nKeisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavat-\nula, and Yejin Choi. 2021. Winogrande: An adver-\nsarial winograd schema challenge at scale. Commu-\nnications of the ACM, 64(9):99–106.\nMaarten Sap, Ronan Le Bras, Daniel Fried, and Yejin\nChoi. 2022. Neural theory-of-mind? On the limits of\nsocial intelligence in large LMs. In Proceedings of\nthe 2022 Conference on Empirical Methods in Nat-\nural Language Processing, pages 3762–3780, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nMaarten Sap, Hannah Rashkin, Derek Chen, Ronan\nLe Bras, and Yejin Choi. 2019. Social IQa: Com-\nmonsense reasoning about social interactions. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 4463–\n4473, Hong Kong, China. Association for Computa-\ntional Linguistics.\nNatalie Shapira, Oren Kalinsky, Alex Libov, Chen\nShani, and Sofia Tolmach. 2023. Evaluating humor-\nous response generation to playful shopping requests.\nIn Advances in Information Retrieval: 45th Euro-\npean Conference on Information Retrieval, ECIR\n2023, Dublin, Ireland, April 2–6, 2023, Proceedings,\nPart II, pages 617–626. Springer.\nAlon Talmor, Ori Yoran, Ronan Le Bras, Chan-\ndra Bhagavatula, Yoav Goldberg, Yejin Choi, and\nJonathan Berant. 2022. CommonsenseQA 2.0:\nExposing the limits of AI through gamification.\nhttps://openreview.net/forum?id=qF7FlUT5dxa.\nJoseph Weizenbaum. 1976. Computer power and hu-\nman reason: From judgment to calculation.\nHeinz Wimmer and Josef Perner. 1983. Beliefs about\nbeliefs: Representation and constraining function of\nwrong beliefs in young children’s understanding of\ndeception. Cognition, 13(1):103–128.\nXinyan Velocity Yu, Sewon Min, Luke Zettlemoyer,\nand Hannaneh Hajishirzi. 2022. CREPE: Open-\nDomain Question Answering with False Presupposi-\ntions. arXiv preprint arXiv:2211.17257.\nRowan Zellers, Ari Holtzman, Yonatan Bisk, Ali\nFarhadi, and Yejin Choi. 2019. HellaSwag: Can a ma-\nchine really finish your sentence? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4791–4800, Florence,\nItaly. Association for Computational Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Beichen\nZhang, Junjie Zhang, Zican Dong, Yifan Du, Chen\nYang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang,\nRuiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu,\nPeiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. A\nsurvey of large language models.\n10444\nA Appendices\nA.1 Generative LMs\nA.1.1 Prompts\nAs input to the LLMs, we used the 20 stories with\nthe 4 questions (Q1-Q4) as appeared in (Baron-\nCohen et al., 1999). For each question we created\n3 versions:\nQi: The original question Qi\nQi-Elaborate: Qi + Explain your answer.\nQi-Restricted: Qi +:\n• Q1: Answer with “Yes” or “No” only, with-\nout explanations. In case of doubt, answer\naccording to the most probable answer.\n• Q2: Answer with a quote only, without ex-\nplanations.\n• Q3: Answer the question only, without ex-\nplanations.\n• Q4: Answer with “Yes” or “No” only, with-\nout explanations. In case of doubt, answer\naccording to the most probable answer.\nThe prompt for ChatGPT, GPT3, FlanT5, GPT-J,\nand GPT2 were simply story with a question (one\nat a time). The prompt for T5 was a story with a\nquestion with the suffix Answer:[MASK]9\nA.1.2 Parameters\nGPT-2 (Radford et al., 2019). Python\npackage transformers implementation\n(TFGPT2LMHeadModel, GPT2Tokenizer);\ntensorflow random set seed 0; Generation by gen-\nerate function; do_sample=True; max_length=50;\ntop_k=50; top_p=0.95;\nGPT-J.10 Python package transformers im-\nplementation (AutoModelForCausalLM, Au-\ntoTokenizer); torch; Generation by generate\nfunction; do_sample=True; max_new_tokens=100;\ntemperature=0.9; num_return_sequences=1;\npad_token_id=50256; eos_token_id=50256\nT5 (Raffel et al., 2020) . Python package\ntransformers implementation (T5Tokenizer,\nT5Config, T5ForConditionalGeneration); torch;\nGeneration by generate function; num_beams=10,\nnum_return_sequences=10, max_length=20,\n9[MASK] is a necessary part of the syntax and the addition\nof \"Answer\" is the result of an initial experiment that showed\nthat this addition helps to reduce the incoherent texts.\n10https://arankomatsuzaki.wordpress.com/2021/\n06/04/gpt-j/\neos_token_id=32098, pad_token_id=32098;\nforced_eos_token_id=32098; from_pretrained:t5-\nsmall, t5-base, t5-large, t5-3b, t5-11b\nFlanT5 (Chung et al., 2022). Python pack-\nage transformers implementation (AutoMod-\nelForSeq2SeqLM, AutoTokenizer); torch; Gen-\neration by generate function; do_sample=True;\nmax_length=50000, from_pretrained:google/flan-\nt5-small, google/flan-t5-base, google/flan-t5-large,\ngoogle/flan-t5-xl, google/flan-t5-xxl\nGPT3 (Brown et al., 2020). Python package ope-\nnai model=text-davinci-003; Generation by Com-\npletion.create function; For the detection test - tem-\nperature=0, max_tokens=50 For the generation test\n- temperature=0,0.1 and 0.2; max_tokens=3390\nChatGPT.11 The default values within the website.\nA.1.3 The sampling method\nA single sample (the first) was selected from each\nmodel for the analysis of the stories.\nA.1.4 Hyperparameter\nHyperparameters were chosen to minimize random-\nness and bring the most probable answer (i.e., low\ntemperature, sampling method) and to be sufficient\nfor the expected tokens.\nA.2 Faux Pas Task and Responses\nTable 4 describes examples of faux pas and control\nstories with questions and ground truth responses.\nTable 5 describes examples of different question\nversions.\nFigure 2 demonstrates the different dimensions\non an example response\nA.3 Faux Pas Generation Instructions\nThe exact instructions as they were given appear\non the next page (all page).\nA.4 Faux Pas Generation Examples\nTable 7 describes generation examples.\n11https://chat.openai.com/chat\n10445\nA Faux Pas Story\nJames bought Richard a toy airplane for his birthday. A few months later, they were playing\nwith it, and James accidentally dropped it. “Don’t worry” said Richard, “I never liked it\nanyway. Someone gave it to me for my birthday.”\nQ1. In the story did someone say something that they should not have said?\nYes\nQ2. What did they say that they should not have said?\n“I never liked it anyway. Someone gave it to me for my birthday. ”\nQ3. What did James give Richard for his birthday?\nA toy airplane\nQ4. Did Richard remember James had given him the toy airplane for his birthday?\nNo\nA Control Story\nSimon bought Robert a toy car for his birthday. A few months later, they were playing with it,\nand Simon dropped it. “Don’t worry,” said Robert, “it was only an accident”.\nQ1. In the story did someone say something that they should not have said?\nNo\nQ2. What did they say that they should not have said?\n(Not relevant)\nQ3. What did Simon give Robert for his birthday?\nA toy car\nQ4. Did Simon know Robert had given him the toy car for his birthday?\nYes\nTable 4: Ground Truth. Example of a faux pas and corresponding control stories and questions with ground truth\nresponses from (Baron-Cohen et al., 1999)\n10446\nQ1 In the story did someone say something that they should not have said?\nQ1-Elaborate In the story did someone say something that they should not have said?\nExplain your answer.\nQ1-Restricted In the story did someone say something that they should not have said?\nAnswer with “Yes” or “No” only, without explanations. In case of doubt, answer\naccording to the most probable answer.\nQ2 What did they say that they should not have said?\nQ2-Elaborate What did they say that they should not have said?\nExplain your answer.\nQ2-Restricted What did they say that they should not have said?\nAnswer with a quote only, without explanations.\nQ3 What did James give Richard for his birthday?\nQ3-Elaborate What did James give Richard for his birthday?\nExplain your answer.\nQ3-Restricted What did James give Richard for his birthday?\nAnswer the question only, without explanations.\nQ4 Did Richard remember James had given him the toy airplane for his birthday?\nQ4-Elaborate Did Richard remember James had given him the toy airplane for his birthday?\nExplain your answer.\nQ4-Restricted Did Richard remember James had given him the toy airplane for his birthday?\nAnswer with “Yes” or “No” only, without explanations. In case of doubt, answer\naccording to the most probable answer.\nTable 5: Example of question versions for the story described in Table 4.\n10447\nFaux Pas Generation Instructions\nFaux Pas is defined as \"when a speaker says something without considering if it is something that the listener\nmight not want to hear or know, and which typically has negative consequences that the speaker never intended\"\nHere are two stories that contains faux pas:\nStory 1:\nAll of the class took part in a story competition. Emma really wanted to win. Whilst she was away from school,\nthe results of the competition were announced: Alice was the winner. The next day, Alice saw Emma and said\n\"I’m sorry about your story.\" \"What do you mean?\" said Emma. \"Oh nothing,\" said Alice.\nIn the story did someone say something that they should not have said?\nYes\nWhat did they say that they should not have said?\n\"I’m sorry about your story.\"\nDid Alice realize that Emma hadn’t heard the results of the competition?\nNo\nStory 2:\nRobert had just started at a new school. He said to his new friend, Andrew, \"My Mum is a dinner lady at this\nschool.\" Then Claire came over and said, \"I hate dinner ladies. They’re horrible.\" \"Do you want to come and\nplay rounders?\" Andrew asked Claire. \"No\" she replied \"I’m not feeling very well.\"\nIn the story did someone say something that they should not have said?\nYes\nWhat did they say that they should not have said?\n\"I hate dinner ladies. They’re horrible.\"\nDid Clare know that Robert’s Mum was a dinner lady?\nNo\nAnd here are two control stories (i.e., that do not contains faux pas):\nControl Story 1:\nAll of the class took part in a poetry competition. Jane really wanted to win. While she was away, the\nresults of the competition were announced: Mary was the winner. The next day, Jane bumped into Mary. Mary said\n\"How are you feeling?\". \"Fine thanks?\" said Jane, \"Oh good\" said Mary.\nIn the story did someone say something that they should not have said?\nNo\nControl Story 2:\nDavid had just started at a new school. He said to his new friend, Mike, \"My Mum is a teacher in this school.\"\nThen Jeff came over. \"I hate school\" he told them, \"It’s so small.\" \"Do you want to come and play rounders?\"\nMike asked Jeff. \"No\" he replied \"I’m not feeling very well.\"\nIn the story did someone say something that they should not have said?\nNo\n—\nCreate new 10 faux pas stories and 10 control stories, make sure that (1) the stories are diverse with different\ntopics, places, names, situations, emotions (2) the stories contain conversation (at least 2 different quotes),\n(3) in the faux pas stories there is a sentence to cite which could be the answer to \"What did they say that they\nshould not have said?\" (4) the faux pas story contain a character that didn’t know/remember something and that’s\nwhat causes the faux pas. (5) the control stories are similar to the faux pas stories, and that contains some\nconversation that might confuse like as if it contains a faux pas, but without faux pas\nTable 6: Faux pas generation instructions\n10448\nModel Faux pas Control Issues\nChatGPT At a dinner party, Clair was\ntelling a story about a trip she\ntook. She mentioned a woman in\nthe story and, not realizing that\nthe woman was one of the guests\nat the dinner party, said, “She\nwas the most annoying person\nI’ve ever met”\nAt a dinner party, Clair\nwas telling a story about\na trip she took. She men-\ntioned a woman in the\nstory and said, “She was\nvery helpful during my\ntrip”\nFail to describe situations in\nan implicit manner “not re-\nalizing that the woman was\none of the guests at the dinner\nparty” explaining explicitly\nthe ToM perspective of the\ncharacter to the reader. The\ncontrol story is too simplistic.\nGPT3-\ntext-\ndavinci-\n003\nJohn and his family were visit-\ning his grandmother for the week-\nend. His grandmother asked him\nhow school was going and he\nsaid “It’s okay, but I’m not doing\nvery well in math. ” His grand-\nmother then said “Oh, that’s too\nbad. Your father was never very\ngood at math either. ” John was\nembarrassed because he had for-\ngotten that his father had strug-\ngled with math in school.\nJohn and his family were\nvisiting his grandmother\nfor the weekend. His\ngrandmother asked him\nhow school was going and\nhe said \"It’s okay, but\nI’m not doing very well in\nmath.\" His grandmother\nthen said \"Oh, that’s too\nbad. Have you tried study-\ning with a tutor?\"\nDefinitely not a faux pas story.\nIn addition, there is a problem\nwith the emotion coherent in\nthe story - why does the fact\nthat John had forgotten that\nhis father had struggled with\nmath in school make him em-\nbarrassed? This is not the ap-\npropriate emotion here. Here\ntoo (like in ChatGPT’s con-\ntrol story), the control story is\ntoo simplistic.\nFlan-\nT5-xxl\nAs soon as she got to the school,\nSusan told her friend, Amanda\nabout the dinner for her family.\nBob is very tall and strong,\nhis workplace is surpris-\ning when I see him run\nfrom one point to another.\nOnly a sentence that lacks the\nbasic elements of faux pas\nstory (e.g., conversation). No\nmatch between faux pas and\ncontrol story.\nFlan-\nT5-xl\nA: “I’m sorry about your story”\nB: “That’s terrible. ” C: “I don’t\nknow if that’ll work.\n- Not even a story\nTable 7: Faux pas generation examples. A faux pas and corresponding control story by ChatGPT. Not a faux pas\nstory by GPT3. Barely a story by FlanT5-xxl. Not even a story by FlanT5-xl\n10449\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nRight after the conclusion section (the limitation section is on page 5)\n□\u0013 A2. Did you discuss any potential risks of your work?\nIn the ethical section right after the limitation section\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nabstract right after the title and section 1 for the introduction summarize the paper’s main claims\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nI used chatGPT as a linguistic editor and improver in rephrasing\nB □\u0013 Did you use or create scientiﬁc artifacts?\nwe annotated LMM responses for stories and create new stories\n□\u0013 B1. Did you cite the creators of artifacts you used?\n1, 2\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\n3 For reasons of anonymity, we have not left a direct link. There is a note in the footnote that the\ndata will be published. It will be free to use.\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\n1,2 the data we used is for free use.\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nethical section\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\n3\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nsections 3,4\nC □\u0013 Did you run computational experiments?\n3,4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nA Appendices A.1 Generative LMs We ran systems in zero-shot mode on a relatively small cluster of\nstories. Running time was negligible.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n10450\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nA.1.4\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nA.1.3\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nA.1.2\nD □\u0013 Did you use human annotators (e.g., crowdworkers) or research with human participants?\n3,4 (as written in the paper, the author of the papers annotated the data)\n□\u0013 D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\n3,4\n□\u0013 D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\n3,4 (as written in the paper, the author of the papers annotated the data)\n□\u0013 D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\n3,4 (as written in the paper, the author of the papers annotated the data)\n□\u0017 D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nthe data is annotations of LLM. we discuss potential risks at the ethical section\n□\u0013 D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\n3,4 (as written in the paper, the author of the papers annotated the data)\n10451",
  "topic": "Presupposition",
  "concepts": [
    {
      "name": "Presupposition",
      "score": 0.6902117133140564
    },
    {
      "name": "Interpretation (philosophy)",
      "score": 0.6091793179512024
    },
    {
      "name": "Task (project management)",
      "score": 0.6077945828437805
    },
    {
      "name": "Computer science",
      "score": 0.5771802663803101
    },
    {
      "name": "Phenomenon",
      "score": 0.5312255024909973
    },
    {
      "name": "Test (biology)",
      "score": 0.5142862200737
    },
    {
      "name": "Cognitive psychology",
      "score": 0.4789170026779175
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4365695118904114
    },
    {
      "name": "Cognitive science",
      "score": 0.36062097549438477
    },
    {
      "name": "Psychology",
      "score": 0.3412852883338928
    },
    {
      "name": "Natural language processing",
      "score": 0.32489123940467834
    },
    {
      "name": "Epistemology",
      "score": 0.26401352882385254
    },
    {
      "name": "Philosophy",
      "score": 0.08421057462692261
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I13955877",
      "name": "Bar-Ilan University",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I2799899409",
      "name": "Hadassah Medical Center",
      "country": "IL"
    }
  ],
  "cited_by": 14
}