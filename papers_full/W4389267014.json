{
    "title": "A medical multimodal large language model for future pandemics",
    "url": "https://openalex.org/W4389267014",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2102522444",
            "name": "Fenglin Liu",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2109825558",
            "name": "Tingting Zhu",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A1967639284",
            "name": "Xian Wu",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2225906136",
            "name": "Bang Yang",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2678749619",
            "name": "Chenyu You",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2113694499",
            "name": "Chenyang Wang",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A1964836801",
            "name": "Lei Lu",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A3083395830",
            "name": "Zhangdaihong Liu",
            "affiliations": [
                "Suzhou Research Institute",
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2110607565",
            "name": "Yefeng Zheng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2107643647",
            "name": "Xu Sun",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2093908790",
            "name": "Yang Yang",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2121652955",
            "name": "Lei Clifton",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2057169223",
            "name": "David A Clifton",
            "affiliations": [
                "Suzhou Research Institute",
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2102522444",
            "name": "Fenglin Liu",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2109825558",
            "name": "Tingting Zhu",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A1967639284",
            "name": "Xian Wu",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2225906136",
            "name": "Bang Yang",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2678749619",
            "name": "Chenyu You",
            "affiliations": [
                "Yale University"
            ]
        },
        {
            "id": "https://openalex.org/A2113694499",
            "name": "Chenyang Wang",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A1964836801",
            "name": "Lei Lu",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A3083395830",
            "name": "Zhangdaihong Liu",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2110607565",
            "name": "Yefeng Zheng",
            "affiliations": [
                "Tencent (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2107643647",
            "name": "Xu Sun",
            "affiliations": [
                "Peking University"
            ]
        },
        {
            "id": "https://openalex.org/A2093908790",
            "name": "Yang Yang",
            "affiliations": [
                "Shanghai Jiao Tong University"
            ]
        },
        {
            "id": "https://openalex.org/A2121652955",
            "name": "Lei Clifton",
            "affiliations": [
                "University of Oxford"
            ]
        },
        {
            "id": "https://openalex.org/A2057169223",
            "name": "David A Clifton",
            "affiliations": [
                "University of Oxford"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3119527628",
        "https://openalex.org/W3138497854",
        "https://openalex.org/W2581082771",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4292947664",
        "https://openalex.org/W2770165365",
        "https://openalex.org/W6760721734",
        "https://openalex.org/W3174714208",
        "https://openalex.org/W3181252431",
        "https://openalex.org/W3095681026",
        "https://openalex.org/W3154493450",
        "https://openalex.org/W4318408278",
        "https://openalex.org/W3094894162",
        "https://openalex.org/W4310266197",
        "https://openalex.org/W4285531589",
        "https://openalex.org/W2515682654",
        "https://openalex.org/W2040246121",
        "https://openalex.org/W2857887762",
        "https://openalex.org/W2789303506",
        "https://openalex.org/W3074741277",
        "https://openalex.org/W6986955575",
        "https://openalex.org/W3008627141",
        "https://openalex.org/W3008985036",
        "https://openalex.org/W3006082171",
        "https://openalex.org/W4230649743",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W4312533035",
        "https://openalex.org/W3213233983",
        "https://openalex.org/W4296027312",
        "https://openalex.org/W3094948156",
        "https://openalex.org/W2963466845",
        "https://openalex.org/W2995225687",
        "https://openalex.org/W3135057764",
        "https://openalex.org/W3139487216",
        "https://openalex.org/W3160840375",
        "https://openalex.org/W3033814865",
        "https://openalex.org/W3095828274",
        "https://openalex.org/W3189279073",
        "https://openalex.org/W3035524453",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3129922404",
        "https://openalex.org/W2159583324",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W3013507463",
        "https://openalex.org/W3017855299",
        "https://openalex.org/W3114062942",
        "https://openalex.org/W4247493843",
        "https://openalex.org/W3104609094",
        "https://openalex.org/W3212101459",
        "https://openalex.org/W4312420429",
        "https://openalex.org/W3177048142",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W1956340063",
        "https://openalex.org/W1889081078",
        "https://openalex.org/W2611650229",
        "https://openalex.org/W2914203365",
        "https://openalex.org/W1904878066",
        "https://openalex.org/W3201906559",
        "https://openalex.org/W4384071683",
        "https://openalex.org/W4377009978",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4318069287",
        "https://openalex.org/W4318925155",
        "https://openalex.org/W4312220150",
        "https://openalex.org/W3129875423",
        "https://openalex.org/W3016836174",
        "https://openalex.org/W3092603779",
        "https://openalex.org/W4362603432",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3132450979",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W4385573131",
        "https://openalex.org/W3046375318",
        "https://openalex.org/W2911489562",
        "https://openalex.org/W2963716420",
        "https://openalex.org/W2971258845",
        "https://openalex.org/W2177066871",
        "https://openalex.org/W6804012510",
        "https://openalex.org/W2122402213",
        "https://openalex.org/W2778310824",
        "https://openalex.org/W2963967185",
        "https://openalex.org/W2980282514",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W2964744899",
        "https://openalex.org/W2963373823",
        "https://openalex.org/W3162351260",
        "https://openalex.org/W3014561994",
        "https://openalex.org/W3136933888",
        "https://openalex.org/W3101156210",
        "https://openalex.org/W3164323420",
        "https://openalex.org/W3098325931",
        "https://openalex.org/W3134475970"
    ],
    "abstract": "Abstract Deep neural networks have been integrated into the whole clinical decision procedure which can improve the efficiency of diagnosis and alleviate the heavy workload of physicians. Since most neural networks are supervised, their performance heavily depends on the volume and quality of available labels. However, few such labels exist for rare diseases (e.g., new pandemics). Here we report a medical multimodal large language model (Med-MLLM) for radiograph representation learning, which can learn broad medical knowledge (e.g., image understanding, text semantics, and clinical phenotypes) from unlabelled data. As a result, when encountering a rare disease, our Med-MLLM can be rapidly deployed and easily adapted to them with limited labels. Furthermore, our model supports medical data across visual modality (e.g., chest X-ray and CT) and textual modality (e.g., medical report and free-text clinical note); therefore, it can be used for clinical tasks that involve both visual and textual data. We demonstrate the effectiveness of our Med-MLLM by showing how it would perform using the COVID-19 pandemic “in replay”. In the retrospective setting, we test the model on the early COVID-19 datasets; and in the prospective setting, we test the model on the new variant COVID-19-Omicron. The experiments are conducted on 1) three kinds of input data; 2) three kinds of downstream tasks, including disease reporting, diagnosis, and prognosis; 3) five COVID-19 datasets; and 4) three different languages, including English, Chinese, and Spanish. All experiments show that our model can make accurate and robust COVID-19 decision-support with little labelled data.",
    "full_text": "ARTICLE OPEN\nA medical multimodal large language model for future\npandemics\nFenglin Liu 1 ✉, Tingting Zhu 1, Xian Wu2, Bang Yang 3, Chenyu You 4, Chenyang Wang 1,L e iL u1, Zhangdaihong Liu1,5,\nYefeng Zheng 2, Xu Sun3, Yang Yang6, Lei Clifton7 and David A. Clifton1,5 ✉\nDeep neural networks have been integrated into the whole clinical decision procedure which can improve the efﬁciency of diagnosis\nand alleviate the heavy workload of physicians. Since most neural networks are supervised, their performance heavily depends on\nthe volume and quality of available labels. However, few such labels exist for rare diseases (e.g., new pandemics). Here we report a\nmedical multimodal large language model (Med-MLLM) for radiograph representation learning, which can learn broad medical\nknowledge (e.g., image understanding, text semantics, and clinical phenotypes) from unlabelled data. As a result, when encountering\na rare disease, our Med-MLLM can be rapidly deployed and easily adapted to them with limited labels. Furthermore, our model\nsupports medical data across visual modality (e.g., chest X-ray and CT) and textual modality (e.g., medical report and free-text clinical\nnote); therefore, it can be used for clinical tasks that involve both visual and textual data. We demonstrate the effectiveness of our\nMed-MLLM by showing how it would perform using the COVID-19 pandemic“in replay”. In the retrospective setting, we test the\nmodel on the early COVID-19 datasets; and in the prospective setting, we test the model on the new variant COVID-19-Omicron. The\nexperiments are conducted on 1) three kinds of input data; 2) three kinds of downstream tasks, including disease reporting,\ndiagnosis, and prognosis; 3)ﬁve COVID-19 datasets; and 4) three different languages, including English, Chinese, and Spanish. All\nexperiments show that our model can make accurate and robust COVID-19 decision-support with little labelled data.\nnpj Digital Medicine          (2023) 6:226 ; https://doi.org/10.1038/s41746-023-00952-2\nINTRODUCTION\nRecently, the rapid development of deep neural networks has\nenabled their wide applications in clinics\n1,2. To process clinical\ndata of different modalities, different neural networks have been\nemployed accordingly. For processing visual data such as\ndermoscopy images, Convolutional Neural Network (CNN) based\nframeworks\n3 have been applied to classify the type of skin lesion4;\nFor textual input such as Electronic Medical Record (EMR),\nTransformer based frameworks\n5 have been be applied to estimate\nthe mortality or re-hospitalisation probabilities6; For multi-modal\ndata such as radiology image-report pairs, the encoder-decoder\nbased frameworks\n7–11 have been applied to generate textual\nreports from medical images.\nDeep neural networks can assist physicians in the diagnosis\nprocess and relieve their heavy burden. Most deep neural\nnetworks exploit supervised training, and therefore their perfor-\nmance heavily relies on the volume and quality of labelled data.\nHowever, the labelling process of clinical data is usually costly and\ntime-consuming. For rare diseases, it is difﬁcult to collect and label\nsufﬁcient data in a timely manner to train a deep learning model\n(with some studies taking over one year to collect suf ﬁcient\ndata\n12,13), thus delaying the rapid deployment of deep learning\nmodels needed for combating rare diseases promptly.\nTake the recent pandemic SARS-CoV-2/COVID-19 for example,\nwhich not only leads to multi-organ failures and death but also\nthreatens to affect global health for the foreseeable future 14.\nAlthough early COVID-19 incurred a high mortality rate, its most\nrecent variants are not life-threatening for the young healthy\npopulation. It is still uncertain whether a new variant in the future\nwould pose a life-threatening risk again. Considering the large\nvolume of the vulnerable population for COVID-19, three common\ntypes of AI-based decision-support tools can be developed to\nsupport accurate diagnosis and prognosis:\n● COVID-19 radiology reporting: Given radiology images,\nphysicians need to write textual reports to address the clinical\nﬁndings\n7,11,15–17. Given the large number of COVID-19\npatients, writing medical reports is a heavy burden for\nphysicians who could otherwise concentrate on patient\ncare\n18,19. The overly-heavy workload of physicians is well-\ndocumented20,21, and using deep learning methods to\nautomatically generate reports that can be modi ﬁed and\napproved by physicians can partly automate routine\ntasks\n1,2,22,23.\n● COVID-19 diagnosis: Currently, the Reverse Transcription\nPolymerase Chain Reaction (RT-PCR) is recognised as the gold\nstandard for COVID-19 diagnosis\n24. Due to the high false-\nnegative rate of RT-PCR and shortage of equipment 25,26,\ndifferent diagnosis models that use medical data across\ndifferent modalities1,27 to generate more timely results than\nRT-PCR can work as an alternative in COVID-19 diagnosis.\n● COVID-19 prognosis: A prognosis model2 can support better\ntriage on who to admit to the hospital or intensive care, who\nto isolate, predicting whom and when to recover, and who is\nat the highest risk of deterioration.\nTraining common neural networks for the above three tasks\nrequires labels on visual, textual and multi-modal data. However,\ncollecting labelled data for a rare disease is expensive and\n1Institute of Biomedical Engineering, Department of Engineering Science, University of Oxford, Oxford, UK.2Jarvis Research Center, Tencent YouTu Lab, Beijing, China.3School of\nComputer Science, Peking University, Beijing, China.4Yale University, New Haven, CT, USA.5Oxford-Suzhou Centre for Advanced Research, Suzhou, China.6School of Public\nHealth, Shanghai Jiao Tong University School of Medicine, Shanghai, China.7Nufﬁeld Department of Population Health, University of Oxford, Oxford, UK.\n✉email: fenglin.liu@eng.ox.ac.uk; david.clifton@eng.ox.ac.uk\nwww.nature.com/npjdigitalmed\nPublished in partnership with Seoul National University Bundang Hospital\n1234567890():,;\ntime-consuming. To this end, inspired by the great success of\nlarge-scale pre-training28–31, as shown in Fig. 1, we present the\nMedical Multimodal Large Language Model(Med-MLLM) framework\nfor radiograph representation learning31–34. Our framework deals\nwith the situation where labelled data are scarce, and shortens the\ntime-frame of model deployment, allowing rapid response to rare\ndiseases in the future.\nAs shown in Fig.2, our framework adopts multimodal medical\ndata across visual and textual modalities to learn the following\ncomprehensive thorax knowledge. 1) Visual data: for medical\nimages such as Chest X-rays (CXR) and Computed Tomography\n(CT), we pre-train an image encoder with two types of losses:\npatient-level contrastive learning loss and image-level contrastive\nloss. 2) Textual data: for medical texts such as medical reports and\nclinical notes, we pre-train a text encoder with three types of\nlosses: masked language modelling loss, sentence reconstruction\nloss, and ﬁndings-impression alignment loss. 3) Multi-modal data:\nfor unpaired radiology images and reports, we introduce a soft\nimage-text alignment loss to further pre-train the visual encoder\nand text encoder. In this manner, Med-MLLM handles visual,\ntextual and multi-modal input, and therefore can be applied to\nCOVID-19 reporting (i.e., medical report generation), diagnosis (i.e.,\ndisease classiﬁcation), and prognosis (i.e., survival prediction) tasks\nwith limited labels for training\n1,2,12,13,15,27,35.\nThe retrospective and prospective experiments across different\nmodalities, languages, and regions assess the effectiveness of our\nMed-MLLM for clinical decision-making when using limited\nlabelled data. Besides COVID-19, the framework can be readily\napplied to other 14 common thorax diseases and tuberculosis as\nwell with 1% labelled data, demonstrating the scalability of our\nframework in assisting physicians when encountering a rare\ndisease.\nOverall, the contributions of our work are as follows:\n● With the goal of quick deployment of tools for rapid response\nto rare diseases, we present the medical multimodal large\nlanguage model (Med-MLLM) framework. We evaluate the\neffectiveness of Med-MLLM using the COVID-19 pandemic“in\nreplay”, showing that Med-MLLM is able to accomplish\naccurate COVID-19 decision-support tasks with limited\nlabelled data. In contrast, existing efforts usually require\nthousands, or even more, labelled data to achieve similar\nperformance.\n● Med-MLLM is able to handle image-only, text-only, and image-\ntext data, addressing multiple medical tasks including\nreporting, diagnosis, and prognosis. To demonstrate the\neffectiveness of Med-MLLM, we conduct both retrospective\nand prospective (i.e., pre-training model from the early COVID-\n19 and making a prediction for COVID-19-Omicron) experi-\nments across different modalities, languages, and regions.\n● To evaluate the scalability of Med-MLLM, we investigate other\n14 common thorax diseases and tuberculosis. Our results\nshow that Med-MLLM achieves competitive performances\nw.r.t. previous works with 1% of the labelled training data, and\ncomparable performance when the full training set is used.\nOVERALL FRAMEWORK\nAs shown in Fig. 1, we develop a Medical Multimodal Large\nLanguage Model (Med-MLLM) for rare diseases to deal with the\nsituation where the labelled data is scarce. An example is the early\nstages of a new pandemic, for which we will have very little data.\nMed-MLLM (i) adopts the unlabelled medical image data from\nexisting public image datasets, e.g., chest radiology images\n36,37,\nCOVID chest X-ray images38–42, and COVID CT images40,42–44 to\nperform image-only pre-training45,46 to learn visual characteristics,\ncapturing the rich diagnostic information in medical images1,2,27;\n(ii) adopts the unlabelled medical text data from existing public\ntext datasets, e.g., PubMed47, MIMIC-CXR medical reports37, and\nMIMIC-III clinical notes48, to perform text-only pre-training49–51 to\nlearn text semantics and clinical ﬁndings in medical texts52; (iii)\nadopts an existing large knowledge base, i.e., Uni ﬁed Medical\nLanguage System (UMLS)53, to perform image-text pre-training54\nCOVID-19 Decision-Support Tasks\nMedical TextsMedical Images\nX-Ray Images CT Images \nMIMIC-CXR\nReports\nMedical Multimodal Data\nLarge-scale Pre-training\nCOVID-19 Reporting\n(medical report generation)\nCOVID-19 Diagnosis\n(disease classification)\nCOVID-19 Prognosis\n(survival prediction)\nText-only\nLarge-scale Pre-training\nMIMI-III\nClinical Database\nImage-only \nLarge-scale Pre-training\nPatient-level contrastive\nImage augmentation\nRegularisation\nImage-Text \nLarge-scale Pre-training\nKnowledge base\nPre-training objectives\nData Augmentation\nRadiology-specific vocab\nPre-training objectives\nText augmentation\nFig. 1 Flowchart. Our presented medical multimodal large language model (Med-MLLM) for COVID-19 reporting, diagnosis and prognosis.\nF. Liu et al.\n2\nnpj Digital Medicine (2023)   226 Published in partnership with Seoul National University Bundang Hospital\n1234567890():,;\nto unify the learned knowledge from unpaired images and texts,\ncapturing accurate disease phenotypes and clinical presentations.\nFigure 2 shows the detailed structure of the Med-MLLM\nframework. For a fair comparison, we adopt the ResNet-5055 as\nthe image encoder and the Transformer5 as the text encoder/\ndecoder. In detail, Med-MLLM (i) adopts contrastive learning46,56\nto perform image-only pre-training, which is improved by a\npatient-level contrastive learning, image augmentation, and\nregularisation; (ii) builds a large language model (LLM)\n49, which\nadopts self-supervised learning 49,50, to perform text-only pre-\ntraining. The LLM is further improved by the radiology-speciﬁc\nvocabulary, two pre-training objectives, and a text augmentation\nmethod; (iii) adopts contrastive learning54 to perform image-text\npre-training, improved by the UMLS knowledge base53 and a pre-\ntraining objective. In this way, our framework could capture\ncomprehensive medical knowledge to provide a solid basis for the\ndiagnosis of rare diseases, including COVID-19 and its\nvariant–Omicron. As a result, our framework can be taken as a\n“warm start\" algorithm to provide an accurate and ef ﬁcient\ndiagnosis of rare diseases using limited labels. Our extensive\nexperiments show that the framework yields encouraging\nperformance for a wide range of downstream tasks.\nFine-tuning\nFigure 3 illustrates the details of ﬁne-tuning the Med-MLLM for\ndownstream COVID-19 decision-support tasks. (i) We adopt the\nimage encoder and an additional text decoder toﬁne-tune (cross-\nentropy optimisation) the pre-trained Med-MLLM on the COVID-19\nreporting (medical report generation) task. (ii) For the task of\nCOVID-19 diagnosis (disease classiﬁcation), we add a classiﬁcation\nlayer on the output of image and/or text encoders, and the Med-\nMLLM is ﬁne-tuned using a binary cross-entropy loss. (iii) For the\ntask of COVID-19 prognosis (survival prediction), we adopt the\nsame ﬁne-tuning strategy as the COVID-19 diagnosis task above,\nbecause these two tasks differ solely in the output results. Both\ntasks can accept three types of input medical data: image-only,\ntext-only, and image-text.\nRESULTS\nIn this section, we conduct experiments on COVID-19 reporting,\ndiagnosis, and prognosis tasks. We ﬁrst describe ﬁve COVID-19\ndatasets used for the experiments. Then, we present the results of\nour framework on COVID-19 decision-support across modalities,\nlanguages, and regions, using limited labels (e.g. 1% labelled\ndata).\nDatasets\nWe evaluate the performance of our framework onﬁve COVID-19\ndatasets across different modalities, languages, and regions, i.e.,\nCOVIDx-CXR-2 dataset\n40, COVID-CXR dataset 39,41, COVID-19 CT\ndataset44, BIMCV-COVID-19 dataset42, and COVID-HCH dataset16.\nThe COVIDx-CXR-2 dataset includes 29,986 medical images of\n16,648 patients from 51 countries; The COVID-CXR dataset\ncontains over 900 chest X-rays of 412 patients from 26 countries,\nwhere 361 patients have survival/death labels. The COVID-19 CT\ndataset contains 1104 medical images associated with 368\nmedical reports in Chinese from 96 patients. The dataset was\ncollected from the First Af ﬁliated Hospital of Jinan University\nGuangzhou and the Fifth Af ﬁliated Hospital of Sun Yat-sen\nUniversity, Zhuhai, China. The BIMCV-COVID-19 dataset is a large\ndataset consisting of over 20k CXR and CT images from over 1000\nCOVID-19 patients along with their radiographic reports in\nSpanish. The COVID-HCH dataset includes 5115 COVID-19 records\nand 4112 non-COVID-19 records of viral and bacterial pneumonia\nfrom 91 patients, resulting in a total of 9227 records associated\nSITA Loss\n(b) Text-only Large-scale Pre-Training\nText\nDecoder\n(c) Image-Text Large-scale Pre-Training\n′\nMLM Loss\n[Impression]:\nNo acute cardiopulm-\nonary abnormality…\nFIA Loss\n[I]\nSR Loss\n[F]\n[Impression]:\nNo acute cardiopulm-\nonary abnormality…\n[Findings]:\n… a right pleural \neffusion. Heart size is \nenlarged. No evidence \nof pneumothorax …\nMLM Loss [Findings]:\n… a right pleural \neffusion. Heart size is \nenlarged. No evidence \nof pneumothorax …SR Loss\nKnowledge\nBase\nText\nEncoder\n[Medical Tags]:\n1. Pleural Effusion\nText\nEncoder\nImage\nEncoder\nImage\nEncoder\n(a) Image-only\nLarge-scale Pre-Training\n… a right pleural \neffusion. Heart size is \nenlarged. No evidence \nof pneumothorax …\n(Predicted)\nSimilarity Score\n[Medical Tags]:\n1. Pleural Effusion\n2. Cardiomegaly\n(Target)\nSimilarity Score\nPCL Loss\n… a right pleural \neffusion. Heart size is \nenlarged. No evidence \nof pneumothorax …\nFig. 2 Structure of the presented Med-MLLM framework. It consists of three main components: a Image-only pre-training which\nincorporates the patient-level contrastive learning (PCL);b Text-only pre-training which incorporates three training objectives: the masked\nlanguage modelling (MLM), the sentence reconstruction (SR) loss, and theﬁndings-impression alignment (FIA) loss; andc Image-text pre-\ntraining which incorporates a knowledge base and a pre-training objective: soft image-text alignment (SITA).\nF. Liu et al.\n3\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   226 \nwith radiographic reports in Chinese. Speciﬁcally, the 5115 COVID-\n19 records are composed of 3577 COVID-19-Delta records and\n1538 COVID-19-Omicron records. Meanwhile, we invite clinical\nprofessionals to translate 100 reports into English. Each English\nreport is associated with multiple (> 10) medical images, and\ndifferent images serve as different samples. We adopt the\nOmicron data to perform simulated prospective studies. In detail,\nwe pre-train the model on Delta data andﬁne-tune the model on\nOmicron data.\nTo pre-process the datasets, we randomly split them into\ntraining, validation and test sets with a ratio of 8:1:1, respectively.\nThe training, validation, and test sets are used to train the model,\nselect the optimal modules and hyper-parameters, and evaluate\nthe performance, respectively. All protected health information\n(e.g., patient name and date of birth) was de-identiﬁed for all\ndatasets used in our experiments. Several previous works\n57–60\nconstruct a balanced test set to minimise the effect of dataset bias\non model performance. The reason is that a balanced test set\nprovides a genuine reﬂection of the models’ ability to correctly\ndistinguish between positive and negative cases, i.e., their\ncapability to accurately identify COVID-19 cases. Thus, the models\nare prevented from exploiting biases in the data distribution to\nachieve high overall performance. To this end, we constructed\nbalanced validation and test sets by randomly sampling 10% of\nthe dataset, with 5% from the positive cases and the other 5%\nfrom the negative cases (i.e. the ratio of COVID-19 records to non-\nCOVID records is 1:1). The remaining 80% samples are used as the\ntraining set. Therefore, our models are trained on the unbalanced\nset, but validated and tested on the balanced set. For all\nexperiments, we conduct multiple runs with different seeds and\nreport the average performances for baselines and our model.\nExperimental settings\nIn our work, we conduct both prospective and retrospective\nstudies. In the retrospective studies, we perform the experiments\nby directly pre-training and evaluating the model on the COVID-19\ndata. For the prospective studies, we perform the experiments by\npre-training the model from early COVID-19 and making predic-\ntions for COVID-19-Omicron. For example, we have observed the\nDelta variant but have no data for Omicron, so our prospective\nstudies can test Med-MLLM to see how it adapts to the new\nvariant (i.e., Omicron) from the old variant (i.e., Delta).\nCOVID-19 reporting\nOur COVID-19 reporting task aims to automatically generate a\ncomprehensive and coherent medical report of a given medical\nimage. In clinical practice, writing reports for numerous images\nfrom routine imaging exams can be time-consuming and tedious\nfor even experienced radiologists\n7. Given the large volume of\nmedical images, automatically generating reports can improve\ncurrent clinical practice in diagnostic radiology and assist\nradiologists in clinical decision-making. Therefore, automatic\nreport generation is receiving remarkable attention in both\ncommunities of arti ﬁcial intelligence and clinical medi-\ncine\n7,11,15,16,61–64. To measure the performance of COVID-19\nreporting, we select the widely-used natural language generation\nmetrics, including BLEU-2, -3, -465, ROUGE-L66, and CIDEr67, which\nare computed by a standard evaluation toolkit68 automatically.\nThese metrics measure the match between the generated reports\nand reference reports annotated by professional physicians.\nRetrospective studies. We further select existing methods, includ-\ning R2Gen61, KGAE62, and XProNet63, for comparison. We conduct\nretrospective studies on the COVID-19-CT dataset in Chinese and\nthe BIMCV-COVID-19 dataset in Spanish. We randomly select 1%\nlabelled data for training. The results in Table1 show that with 1%\nof training data, our method achieves competitive performance\nw.r.t. the previous models trained on the full training set across\nChinese and Spanish. It shows that our approach can be efﬁciently\ntrained and deployed with limited labels to combat rare diseases\npromptly. Using the full training set as used in previous methods,\nour method achieves the best results across different languages\nCOVID-19 Prognosis\nText\nDecoder\nImage\nEncoder\n[Image-Text] Image + Report\n[Radiology Image]\nImage Encoder\nText Encoder[Text-only] Medical Report\n[Image-only] Radiology Image\nImage Encoder\nText Encoder\n[Survival Prediction]\nSurvival/Death\n[Medical Report]:\nBorderline cardiac enlargement.\nEnlarged calcified thoracic aorta.\nEmphysema. No acute pulmonary\nabnormality. Mild spondylosis.\n[Survival Prediction]\n[Survival Prediction]\nCOVID-19 Reporting\nCOVID-19 Diagnosis\n[Image-Text] Image + Report\nImage Encoder\nText Encoder[Text-only] Medical Report\n[Image-only] Radiology Image\nImage Encoder\nText Encoder\n[Disease Classification]\n1. COVID-19/Non-COVID\n2. Omicron/Delta\n[Disease Classification]\n[Disease Classification]\nFig. 3 Illustration of ﬁne-tuning our Med-MLLM on downstream COVID-19 decision-support tasks: COVID-19 reporting, diagnosis, and\nprognosis.\nF. Liu et al.\n4\nnpj Digital Medicine (2023)   226 Published in partnership with Seoul National University Bundang Hospital\nand regions. In detail, our framework outperforms previous best\nresults by up to 4.3%/3.8% in BLEU-4, 9.1%/4.3% in ROUGE-L, and\n10.9%/9.8% in CIDEr scores in Chinese/Spanish scenarios. The\nimprovement demonstrates the effectiveness of our framework in\nproviding a solid basis for COVID-19 reporting.\nProspective studies . We perform prospective studies on the\nCOVID-19-Omicron data from the COVID-HCH dataset. Speciﬁcally,\nwe adopt the Delta data for pre-training the model and adopt the\nOmicron data for evaluation. As shown in Table1, our method\nMed-MLLM outperforms previous methods trained on full training\ndata on most metrics. Compared with retrospective studies, our\nmethod achieves better results on COVID-19-Omicron reporting.\nThe results of prospective studies evaluated on COVID-19-Omicron\ndata show that our method shortens the time for data acquisition,\nallowing us to respond quickly in future to rare diseases across\ndifferent languages and regions. We further validate it on the\nfollowing COVID-19 diagnosis and prognosis tasks. It is worth\nnoting that the performance of our method can be further\nimproved by using more training data, achieving improved\nperformances when it is trained with the full training set as used\nin previous methods.\nCOVID-19 diagnosis\nIn the retrospective setting, the COVID-19 diagnosis task (i.e.,\ndisease classiﬁcation) aims to distinguish COVID-19 from non-\nCOVID-19 cases. In the prospective setting, the aim is to identify\nCOVID-19-Omicron. We conduct retrospective studies on the\nCOVIDx-CXR-2 and COVID-19-Delta data and conduct prospective\nstudies on the COVID-19-Omicron data. In our experiments, we\nreport the widely-used AUC for assessing the diagnosis accuracy.\nRetrospective studies. We utilise the COVIDx-CXR-2 dataset to\nperform the image-only COVID-19 diagnosis task, and adopt the\nCOVID-19-Delta data labelled in English to perform the text-only\nand image-text medical diagnosis tasks. We further select self-\nsupervised learning and contrastive learning methods for\ncomparison, i.e., CLIP\n54, ConVIRT34, and BioViL30. Since previous\nmodels had not attempted to deal with image-only, text-only and\nimage-text tasks simultaneously, we re-implement these methods\nfor evaluation.\nTable 2 shows the diagnosis accuracy of our framework and the\nprevious methods on COVID-19 classi ﬁcation, where our Med-\nMLLM achieves superior performance on all tasks and datasets. It\nnot only achieves competitive results compared to previous\nmethods with 1% training data, but also outperforms them when\nusing 100% training data. The results demonstrate the validity of\nour method in relaxing the dependency on the high quality of\nlabelled data for training, while making an accurate COVID-19\ndiagnosis.\nProspective studies. We pre-train the model on Delta data and\nﬁne-tune the model on Omicron data. As shown in Table2, with\n1% of Omicron data, our method can outperform several previous\nworks (e.g., CLIP). More encouragingly, with 100% training labels,\nMed-MLLM surpasses the previous method by up to 10.6%, 3.6%,\nand 5.9% in diagnosis accuracy on image-only, text-only, and\nimage-text classiﬁcation tasks, respectively. The performance of\nprospective studies assesses the good generalisation capability of\nTable 1. Results of the COVID-19 reporting task: an image-text multimodal task aiming to automatically generate the medical reports of given\nmedical images, on three datasets across Chinese, Spanish and English.\nMethods Year Ratio of training data Retrospective studies\nDataset: COVID-19-CT (Chinese) Dataset: BIMCV-COVID-19 (Spanish)\nBLEU-2 BLEU-3 BLEU-4 ROUGE-L CIDEr BLEU-2 BLEU-3 BLEU-4 ROUGE-L CIDEr\nR2Gen\n61 2020 1% 35.9 33.2 31.3 41.7 57.6 28.8 24.3 21.0 37.4 42.7\nKGAE62 2021 1% 43.6 39.1 36.7 50.2 72.4 35.5 29.8 27.8 45.4 58.0\nXProNet63 2022 1% 38.4 35.0 33.5 44.8 60.7 32.2 27.0 25.1 40.6 51.6\nMed-MLLM Ours 1% 54.3 (2.7) 47.5(2.1) 42.1(1.7) 57.2(1.9) 85.3(2.5) 47.6(3.0) 42.0(2.3) 38.1(1.8) 55.4(1.6) 73.4(2.8)\nR2Gen61 2020 100% 53.3 45.1 39.4 54.5 80.4 43.2 37.8 33.2 52.9 67.2\nKGAE62 2021 100% 56.4 48.6 44.3 60.3 83.7 47.0 40.6 36.8 53.2 71.3\nXProNet63 2022 100% 57.7 49.0 44.4 59.4 84.5 48.3 41.1 38.4 54.0 70.9\nMed-MLLM Ours 100% 64.2(2.1) 55.0(1.6) 48.7(1.3) 68.5(1.2) 95.4(2.0) 55.6(2.4) 46.4(1.8) 42.2(1.4) 58.3(1.2) 80.7(2.3)\nMethods Year Ratio of training data Prospective studies\nDataset: COVID-19-Omicron (Chinese) Dataset: COVID-19-Omicron (English)\nBLEU-2 BLEU-3 BLEU-4 ROUGE-L CIDEr BLEU-2 BLEU-3 BLEU-4 ROUGE-L CIDEr\nR2Gen61 2020 1% 57.0 52.9 49.2 55.7 74.8 20.3 18.5 14.2 30.7 35.6\nKGAE62 2021 1% 62.9 56.0 51.3 58.6 83.4 25.7 22.4 18.8 34.6 41.1\nXProNet63 2022 1% 63.0 57.2 52.2 58.8 84.1 26.0 21.9 18.7 35.0 42.8\nMed-MLLM Ours 1% 70.1 (1.7) 64.6(1.4) 60.1(1.0) 64.3(1.5) 95.2(1.1) 32.4(2.6) 28.7(1.7) 24.9(1.5) 46.7(2.1) 53.0(1.9)\nR2Gen61 2020 100% 63.1 57.5 52.1 59.2 85.3 31.7 26.2 20.2 43.1 47.8\nKGAE62 2021 100% 67.3 58.7 53.0 60.4 89.2 34.2 27.5 24.4 46.0 50.3\nXProNet63 2022 100% 66.5 59.2 53.3 61.1 90.4 33.0 28.0 24.8 47.8 51.9\nMed-MLLM Ours 100% 74.5(1.4) 67.8(1.0) 63.2(0.9) 67.2(0.8) 97.5(0.5) 40.1(1.7) 34.4(1.2) 29.0(1.0) 51.6(1.3) 62.6(1.4)\nWe report the mean and standard deviation(STD) of performance. Higher is better for all metrics. The best results are in bold. 100% denotes that the models are\ntrained on the full training set.\nF. Liu et al.\n5\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   226 \nour approach in dealing with situations where the training data\nare scarce. Therefore our Med-MLLM is suitable for new\npandemics caused by rapidly developing pathogens, improving\nthe practical value of AI-based decision-support tools in clinical\npractice.\nCOVID-19 prognosis\nThe COVID-19 prognosis task aims at predicting the survival of\nCOVID-19 patients, i.e., predicting whether the patients will survive\nafter treatment in the hospital. In this experiment, we evaluate the\nperformance of prognosis on COVID-CXR and COVID-HCH\ndatasets.\nRetrospective studies. We conduct the image-only task on the\nCOVID-CXR dataset and conduct the text-only and image-text\ntasks on the COVID-19-Delta data from the COVID-HCH dataset.\nSimilar to the COVID-19 diagnosis task, we also re-implement the\nexisting methods for COVID-19 prognosis. The results of COVID-19\nprognosis are reported in Table3, showing that our Med-MLLM is\ncomparable to the previous approaches with 1% training data.\nUsing the full training data, our method outperforms previous\nmethods by up to 4.6%, 1.1%, and 1.5% in AUC on image-only,\ntext-only, and image-text COVID-19 prognosis tasks, respectively.\nProspective studies. We adopt the Omircon data to report the\nresults of prospective studies. In implementations, we pre-train\nthe model on Delta and predict for Omicron. The results illustrated\nin Table 3 indicate that when it comes to COVID-19 Omircon\nprognosis, with 1% of data for ﬁne-tuning, our Med-MLLM\nsurpasses existing methods by substantial margins demonstrating\nthe effectiveness of our method in making an accurate and fast\nCOVID-19 diagnosis with limited labelled data. With 100% training\ndata, our method surpasses existing self-supervised learning and\ncontrastive learning methods, which is in accordance with the\nresults of COVID-19 reporting and diagnosis.\nDISCUSSION\nIn addition to COVID-19, our Med-MLLM can be readily applied to\nother chest/respiratory diseases. Table4 shows the performances\nof Med-MLLM on the CheXpert\n36, NIH ChestX-ray 69, RSNA\nPneumonia70, SIIM-ACR Pneumothorax71, and Shenzhen Tubercu-\nlosis72 benchmark datasets for common disease classi ﬁcation\ntasks. We follow previous works 30–32,34,73 to pre-process the\ndatasets and perform the evaluation. As we can see from Table4,\nwith limited labels (i.e., 1% of CheXpert, NIH ChestX-ray, RSNA\nPneumonia, SIIM-ACR Pneumothorax datasets, and 10% of\nShenzhen Tuberculosis), our method can achieve competitive\nresults with previous fully-supervised methods trained on full\nlabels. In particular, our Med-MLLM with 1% training data\noutperforms previous methods trained with 100% data on the\nCheXpert and RSNA datasets. Then, in Table5, we further evaluate\nthe performance of our method on 14 common thorax diseases.\nThe t-tests between the results from Med-MLLM and the best-\nperforming baseline REFERS indicate that the improvement is\nsigniﬁcant with p < 0.01. As we can see, our approach Med-MLLM\n(1%) achieves up to 0.4%, 0.5%, 0.1%, and 0.2% absolute\nimprovements upon the current best results trained with full data\nfor diseases–consolidation, effusion, inﬁltration, and pneumonia,\nrespectively. More encouragingly, with all training labels as in\nTable 2. The diagnosis accuracy (AUC) of COVID-19 image-only, text-only and image-text disease classiﬁcation experiments.\nMethods Year Ratio of training data Retrospective studies Prospective studies\nImage-only Text-only Image-text Image-only Text-only Image-text\nCLIP54 2021 1% 87.5 75.6 88.6 58.7 65.3 69.9\nConVIRT34 2022 1% 88.1 86.4 88.8 59.6 66.4 71.5\nBioViL30 2022 1% 90.4 89.7 91.0 60.9 68.8 73.0\nMed-MLLM Ours 1% 95.3 (0.3) 93.8(0.5) 95.9(0.4) 64.8(1.1) 72.9(0.8) 78.2(0.7)\nCLIP54 2021 100% 95.7 83.3 89.0 63.5 68.8 75.2\nConVIRT34 2022 100% 97.6 94.5 97.7 70.4 77.6 82.1\nBioViL30 2022 100% 97.4 94.5 98.2 66.7 80.5 84.4\nMed-MLLM Ours 100% 98.4(0.2) 96.3(0.4) 98.7(0.2) 81.0(0.4) 84.1(0.5) 90.3(0.3)\nAll values are reported in percentage (%). The best results are in bold.\nTable 3. AUC values of COVID-19 prognosis experiments, which aim to predict the survival of COVID-19 patients.\nMethods Year Ratio of training data Retrospective studies Prospective studies\nImage-only Text-only Image-text Image-only Text-only Image-text\nCLIP54 2021 1% 70.4 84.3 89.5 66.9 76.7 81.3\nConVIRT34 2022 1% 75.3 88.1 92.6 70.6 81.2 85.4\nBioViL30 2022 1% 77.1 89.0 92.9 70.8 82.1 85.7\nMed-MLLM Ours 1% 82.8 (0.5) 92.1(0.3) 95.7(0.3) 81.2(0.7) 88.3(0.8) 92.0(0.5)\nCLIP54 2021 100% 79.5 91.7 93.2 70.6 84.0 88.3\nConVIRT34 2022 100% 83.4 93.8 95.4 77.5 88.7 90.1\nBioViL30 2022 100% 83.5 94.2 95.1 77.0 87.9 89.8\nMed-MLLM Ours 100% 88.1(0.2) 95.3(0.2) 96.6(0.1) 85.7(0.4) 92.8(0.2) 94.9(0.2)\nAll values are reported in percentage (%). The best results are in bold.\nF. Liu et al.\n6\nnpj Digital Medicine (2023)   226 Published in partnership with Seoul National University Bundang Hospital\nprevious works, our Med-MLLM (100%) can outperform these\nmethods across all datasets and diseases. The promising results\nassess the generalisation capabilities of our approach.\nTo further evaluate the effectiveness of our framework for rare\ndiseases, we assess the diagnosis performances of existing LLMs,\ni.e., GPT-2, GPT-3, ChatGPT (GPT-3.5 version), and GPT-4\n28,74 that\nare released by OpenAI. Since LLMs only accept the text as input,\nwe perform the text-only COVID-19 diagnosis task, which aims to\ndistinguish COVID-19 from non-COVID-19 cases. To obtain the\ndiagnosis accuracy (i.e., disease classiﬁcation performance) from\nthe LLMs, we take the following text as input:‘Original Clinical\nText’ + ‘Is this a COVID-19 case? ’. Then, we sample the\nprobabilities of ‘Yes’ (P\nyes) and ‘No’ (Pno) from the next predicted\ntoken by GPT. Finally, ifPyes > Pno, we take the‘Yes’as the output\nof LLMs; ifPyes < Pno, we take the‘No’as the output of LLMs. In this\nway, we can obtain the COVID-19 diagnosis accuracy of LLMs. For\nthe ChatGPT and GPT-4, we follow previous works\n75,76 to\nincorporate the few-shot prompting 28 and chain-of-thought\nprompting77 strategies. It means that we incorporate ﬁve\nexamples, which cover both COVID-19 and non-COVID-19 cases,\nand instructions as input to request them to generate the\nresponse. Therefore, the full input is:\nThis is just a text classiﬁcation test. Analyze the reportﬁrst, then\nprovide the ﬁnal answer here based on the following examples,\nwhich must be either“ Yes\" or “ No\".\nReport: ‘Original Clinical Text’;\nQuestion: Is this a COVID-19 case?\nAnswer: Provide theﬁnal answer here, which must be either“ Yes\"\nor “ No\".\nAt last, due to the potential variation in output from ChatGPT,\nwe conduct ﬁve runs for each enquiry and select the answer that\nappears most frequently as theﬁnal answer. In addition, it can also\nbe considered as an ensemble approach to achieve better results.\nTable 6 reports the performances of our method and existing\nstrong LLMs. As we can observe, our approach performs better\nthan several strong LLMs, i.e., GPT-2, GPT-3, and ChatGPT, and\nachieves a competitive result w.r.t. GPT-4. It is worth noting that\nalthough these LLMs have shown great success in natural text\nunderstanding, we cannot directly adopt the results provided by\nChatGPT in the medical domain\n78,79.\nWe perform a robustness analysis to examine whether our\nmethod can aid in the COVID-19 diagnosis of new regions by\npredicting the COVID-19 cases in new regions. To this end, we\nconduct a cross-region prediction by training the methods on\npatient data from one region and evaluating the methods on\npatient data from other regions. In implementations, the BIMCV-\nCOVID-19 dataset collected in Spain, the COVID-HCH dataset\ncollected in China, and the COVID-CXR dataset collected in over 20\ncountries (excluding Spain and China) are used for the validation.\nThe image-only COVID-19 diagnosis accuracy of our method and\nprevious methods are summarised in Table7. It shows that our\napproach consistently outperforms previous methods and\nachieves solid performances in COVID-19 diagnosis in new\nregions. In particular, when transferring our approach trained on\npatient data from Spain to China, we observe an encouraging\nperformance, i.e., 90.1% AUC, which is competitive with the\nregion-speciﬁc results of previous works CLIP (80.7% AUC) and\nBioViL (90.4% AUC), which were obtained by training and testing\non the data collected from the same region. Similarly, the cross-\nregion performance of the Spain region (84.8% AUC) of our\nmethod, which is trained on China, is competitive with the region-\nspeciﬁc result of CLIP (85.4% AUC). These results highlight the\ntransferability and robustness of our approach, leading to a\nhigher-quality diagnosis of rare diseases in new regions than the\ncurrent methods.\nTo further assess the effectiveness of our approach in diagnosis,\nwe present to use more labels to conduct continuous learning to\ntrain the model continuously. It can evaluate whether the model\ncan continue to be improved when more labelled data are\ncollected as the disease evolves. It is particularly useful in real-\nworld settings. To this end, in Fig.4, we evaluate the performance\nof Med-MLLM with respect to the increasing quantity of training\nlabels. Speciﬁcally, we evaluate the results on the BIMCV-COVID-19\nand COVID-Omicron data for COVID-19 reporting, diagnosis, and\nprognosis tasks across modalities, languages, and regions. For\ncomparison, we also re-implement the state-of-the-art (SOTA)\nmodels (i.e., XProNet\n63 for reporting and ConVIRT34 for diagnosis\nand prognosis) using the same training labels to better under-\nstand the strengths of our method. We conduct multiple runs with\ndifferent seeds and report the average performance. As we can\nsee in Fig. 4, for different COVID-19 decision-support tasks, our\nmethod Med-MLLM consistently outperforms SOTA with the\ndifferent numbers of training labels. With more training labels,\nour method can be continuously improved. It is worth noting that,\nunder the low label setting, e.g., 1% of training labels, our\napproach surpasses the SOTA by large margins, up to 21.8%, 6.7%,\nand 6.6% absolute improvements on COVID-19 reporting,\ndiagnosis, and prognosis tasks, respectively. More importantly,\nwith 10% labelled data for training, our method can outperform\nprevious SOTA methods trained with 100% training data. It\ndemonstrates the effectiveness of our approach in relaxing the\nreliance on the annotations to provide a solid basis for COVID-19\ndecision-support, which is particularly useful for rare diseases,\nwhere the labels are scarce at the early stage.\nWe provide two intuitive examples to illustrate our approach.\nFigure 5 shows that our method Med-MLLM can simultaneously\ngenerate useful and informative reports across different lan-\nguages. More importantly, Med-MLLM is able to accurately report\nimportant abnormalities, e.g., ‘multiple patchy-like ground glass\ndensity shadow’ in the ﬁrst example, and‘a lamellar ground glass\nTable 4. The diagnosis accuracy of different methods on various diseases across CheXpert, NIH ChestX-ray, RSNA Pneumonia, SIIM-ACR\nPneumothorax, and Shenzhen Tuberculosis datasets.\nMethods Year Ratio of training data CheXpert NIH ChestX-ray RSNA SIIM-ACR Tuberculosis\nConVIRT34 2022 1% / 10% 87.0 66.2 88.8 71.3 93.7\nBioViL30 2022 1% / 10% 86.8 69.5 88.1 69.5 95.0\nREFERS32 2022 1% / 10% 87.2 76.7 89.4 76.6 95.8\nMed-MLLM Ours 1% / 10% 88.9 (0.5) 83.3(0.9) 93.4(0.5) 87.5(0.7) 96.7(0.4)\nConVIRT34 2022 100% 88.1 81.3 92.7 90.0 96.4\nBioViL30 2022 100% 87.9 82.5 89.1 86.9 97.1\nREFERS32 2022 100% 88.2 84.7 92.7 89.3 98.0\nMed-MLLM Ours 100% 89.5(0.2) 88.1(0.3) 95.3(0.2) 94.0(0.4) 98.6(0.1)\nAll values are reported in percentage (%). The best results are in bold.\nF. Liu et al.\n7\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   226 \nshadow is seen in the lower lobe of the left lung’ in the second\nexample. It is encouraging that our approach can accurately report\nabnormalities. Overall, with limited labels, our Med-MLLM can\ngenerate informative and “believable” reports for different\nlanguages, demonstrating its capability for combating rare\ndiseases.\nWe further detect the hallucinations and missing facts in the\ngenerated reports. To successfully assist physicians and reduce\ntheir workloads of writing medical reports, it is important to\ngenerate accurate reports (faithfulness or precision), such that the\nmodel does not generate hallucinations that“do not exist”.I ti s\nalso necessary to provide comprehensive facts (comprehensiveness\nor recall), i.e., the model does not leave out the trueﬁndings. To\nthis end, we ﬁrst employ a medical natural language processing\n(NLP) tool from the work of CheXpert\n36, to label the ground truth\nreports, e.g., [Abnormality_A, Abnormality_B]. Then, we again\nemploy the NLP tool to label the generated reports, e.g.,\n[Abnormality_B, Abnormality_C]. We can ﬁnd that the model\ngenerates a hallucination, i.e., [Abnormality_C], and misses a fact,\ni.e., [Abnormality_A]. Therefore, we can use this method to\ncalculate the ‘Precision’ and ‘Recall’ scores to preliminary detect\nthe hallucinations and missing facts, respectively. At last, we\nfurther calculate the F1 score to obtain the overall performance.\nSince the NLP tool can extract abnormalities from the English text,\nwe conduct the evaluation on English report generation. For\ncomparison, we also calculate the Precision, Recall, and F1 scores\nof previous methods, i.e., R2Gen\n61, KGAE62, and XProNet63. For a\nfair comparison, both previous methods and our method are\ntrained on 100% of training data. The results are reported in Table8,\nTable 5. The diagnosis accuracy on 14 common thorax diseases from the NIH ChestX-ray dataset.\nMethods Year Ratio of\ntraining\ndata\nAtelectasis Cardiomegaly Consolidation Edema Effusion Emphysema Fibrosis Hernia In ﬁltration Mass Nodule Pleural\nthickening\nPneumonia Pneumothorax\nNIH69 2017 1% 73.3 69.6 76.0 81.7 80.5 67.1 64.9 64.8 65.8 67.0 62.3 65.7 65.0 74.0\nContext\nRestoration105\n2019 1% 69.1 64.4 73.2 73.8 78.1 70.0 62.1 70.2 65.2 62.4 59.1 65.0 62.2 73.8\nC2L83 2020 1% 75.1 67.1 77.6 75.1 83.4 71.5 66.8 70.0 63.8 70.1 66.2 68.1 65.7 74.4\nModel Genesis82 2021 1% 72.1 67.1 75.8 76.1 80.6 72.6 64.8 73.5 65.7 65.2 62.2 67.6 64.8 76.2\nTransVW81 2021 1% 74.5 68.9 76.7 79.8 81.1 67.9 68.7 68.2 66.8 66.5 66.2 68.5 68.8 75.0\nREFERS32 2022 1% 77.5 85.6 78.6 84.9 85.4 79.5 72.3 77.1 67.5 76.2 66.5 71.6 69.3 81.7\nMed-MLLM Ours 1% 80.1 88.2 82.5 89.4 89.2 90.4 80.0 87.8 74.2 81.6 75.9 77.9 77.2 87.3\nNIH69 2017 100% 78.3 89.3 77.6 87.9 85.9 87.4 78.5 88.8 65.9 79.9 70.7 74.5 71.0 84.7\nContext\nRestoration105\n2019 100% 75.8 82.9 76.4 86.6 84.8 88.2 78.6 83.0 70.0 79.6 69.5 73.2 69.4 84.0\nC2L83 2020 100% 81.1 90.2 81.0 88.1 88.0 88.3 80.8 86.8 72.0 82.7 74.1 76.2 75.3 85.9\nModel Genesis82 2021 100% 78.8 84.5 79.2 87.8 86.6 89.7 81.0 85.2 71.1 81.9 73.2 75.8 73.0 85.6\nTransVW81 2021 100% 79.8 85.0 80.0 88.2 87.1 90.1 81.8 85.9 72.3 82.6 74.4 76.6 74.0 86.1\nREFERS32 2022 100% 83.0 92.3 82.1 90.2 88.7 91.4 83.9 93.3 74.1 85.5 76.7 78.5 77.0 89.1\nMed-MLLM Ours 100% 85.4 94.1 84.7 91.3 90.2 95.0 88.2 94.6 76.9 88.7 79.3 82.8 79.1 89.9\nAll values are reported in percentage (%). The best results are in bold.\nTable 6. Comparison with existing large language models (LLMs), i.e.,\nGPT-2, GPT-3, ChatGPT (GPT-3.5), and GPT-4.\nMethods GPT-2 GPT-3 ChatGPT GPT-4 Med-MLLM\nCOVID-19 diagnosis 87 91 93 98 97\nWe perform the text-only COVID-19 diagnosis task to compare the\nusefulness of our approach with that of the strong LLMs in the medical\ndomain. All values are reported in percentage (%).\nTable 7. Robustness analysis aims to examine whether our framework\ncan provide COVID-19 decision support for new regions.\nTraining regions Methods Year Testing Regions\nSpain China\nSpain (BIMCV-COVID-19) CLIP 54 2021 85.4 75.4\nConVIRT34 2022 92.7 85.6\nBioViL30 2022 92.8 83.3\nMed-MLLM Ours 95.2 90.1\nChina (COVID-HCH) CLIP 54 2021 70.5 80.7\nConVIRT34 2022 79.9 91.7\nBioViL30 2022 77.0 90.4\nMed-MLLM Ours 84.8 93.9\n>20 Countries (COVID-CXR CLIP 54 2021 63.3 61.5\nexcl. Spain & China) ConVIRT 34 2022 71.2 69.0\nBioViL30 2022 69.6 70.4\nMed-MLLM Ours 78.2 74.8\nWe perform cross-region prediction by training on patient data from one\nregion and evaluating on patient data with different phenotypes from\nother regions. All values are reported in percentage (%). The best results\nare in bold.\nF. Liu et al.\n8\nnpj Digital Medicine (2023)   226 Published in partnership with Seoul National University Bundang Hospital\nshowing that our Med-MLLM method surpasses previous methods\non all metrics by 5.5%, 3.6%, and 4.6% in terms of Precision, Recall,\nand F1 scores, respectively. It shows that our approach can\ngenerate more faithful reports (i.e., fewer hallucinations) and more\ncomprehensive reports (i.e., fewer missing facts) than previous\nmethods, demonstrating that our method can better assist\nphysicians in reducing their workload.\nTo better understand the effectiveness of each introduced\ncomponent, we provide a thorough ablation study of our Med-\nMLLM in Table9. It shows that all of our introduced components\ncan bring improvements to downstream tasks. In detail, as image-\nonly pre-training can enable the model to learn broad thorax\nknowledge, e.g., the diagnostic information, from visual images,\nremoving it would impair the performances (i.e., 73.4%→57.8% in\nCIDEr on reporting, 78.2% →69.4% in AUC on diagnosis,\n92.0% →82.3% AUC on prognosis). The impaired performances\nassess the effectiveness of learning the important visual char-\nacteristics from medical images to support accurate diagnosis and\nprognosis. Besides, we ﬁnd that removing the patient-level\ncontrastive learning (PCL) impairs performance across all tasks.\nBy comparing settings (c-e), we notice that, among the introduced\nthree modules in text-only pre-training, the sentence reconstruc-\ntion module (SR), which can help the model efﬁciently learn to\ngenerate reports, brings the most improvements on reporting. In\ncontrast, the other two modules, MLM and FIA, result in more\nimprovements on diagnosis and prognosis. The image-text pre-\ntraining aims to unify the learned medical knowledge from\nmedical images and text. The performance across all tasks\ndecreases when it is removed, showing that unifying visual and\ntextual information can boost the representation of medical data.\nOverall, the ablation study demonstrates the effectiveness of the\nMed-MLLM, where all the components can contribute to\nperformance.\nAt last, to explore the effect of scaling up the number of model\nparameters, we introduce a larger version of the language model\n(i.e., Med-MLLM-Large) with 8.9 billion parameters initialized with\nGatorTron\n80, where the number of layers is 56, the number of\nattention heads is 56, and the dimensionality is 3584. For\ncomparison, we perform the evaluation on the text-only COVID-\n19 diagnosis and prognosis tasks to evaluate the performance of\ndifferent language models. The results in Table10 show that the\nMed-MLLM-Large has better performance than the Med-MLLM-\nBase by 1.7 ~ 3.6 in AUC values. It not only shows that more model\nparameters can lead to further improvements, but also demon-\nstrates the potential of LLM that can be further improved in the\nfuture by directly scaling up the models.\nFig. 4 Results of Med-MLLM and state-of-the-art (SOTA) methods with respect to the increasing quantity of training labels.The margins\nin different ratios are shown with the polyline. As we can see, our method can be continuously improved using more training labels which\nmay be available as the disease evolves.\nMed-MLLM\nEnglish: The thorax is symmetrical on both sides. A lamellar ground glass shadow is \nseen in the lower lobe of the left lung. There is mild cardiomegaly. The bilateral pleura \nis not thick, and there is no sign of effusion in the bilateral pleural cavity. There is no\npleural effusion. \nSpanish: Persiste vidrio deslustrado periférico en pulmón izquierdo. Áreas parcheadas\nintersticio-alveolares bilaterales de districbución periférica en relación con probable \nneumonía atípica/vírica. Ni masas o derrame pleural. Cardiomegalia.\nMed-MLLM\nEnglish: Multiple patchy-like ground glass density shadows can be seen in both lungs. \nThere is no pleural effusion. The heart is normal in size. The mediastinum is \nunremarkable. No obvious swollen lymph nodes were seen in the mediastinum.\nSpanish: Aumento de densidad periférico en lóbulo inferior izquierdo. Hallazgos\nsugestivos de covid-19. Mediastino no engrosado. Silueta cardiaca normal. Hilios de \nmorfología, densidad y situación dentro de la normalidad. No derrame pleural o\npinzamiento.\nFig. 5 The examples of COVID-19 reports generated by our Med-MLLM framework for different languages, i.e., English, Spanish, and\nChinese. As we can see, Med-MLLM can generate accurate and informative reports across different languages to relieve the heavy burden of\nphysicians and could support them in clinical decision-making.\nTable 8. We detect the hallucinations and missing facts in reports\ngenerated by different methods.\nMethods Year Precision Recall F1\nR2Gen61 2020 71.8 82.0 76.6\nKGAE62 2021 70.5 79.8 74.9\nXProNet63 2022 73.6 84.7 78.8\nMed-MLLM Ours 79.1 88.3 83.4\nHigher precision and recall indicate fewer hallucinations and missing facts,\nrespectively. Therefore, higher is better for all metrics. All values are\nreported in percentage (%). The best results are in bold.\nF. Liu et al.\n9\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   226 \nMETHODS\nIn this section, we describe in detail the three main components of\nour deep learning model.\nImage-only pre-training\nWe ﬁrst introduce Patient-level Contrastive Learning (Fig.2a) and\nthen present the image augmentation and regularisation.\nPatient-level contrastive learning . We conduct image-only pre-\ntraining to learn medical knowledge from the large-scale\nunlabelled image-only data. Several existing works based on\nself-supervised learning or contrastive learning\n81–83 have shown\nthe effectiveness of training models on large-scale image-only\nmedical data. In this work, inspired by the success of contrastive\nlearning in natural images 45,46,56, we introduce Image-level\nContrastive Learning (ICL) and Patient-level Contrastive Learning\n(PCL) for medical image understanding.\nIn implementations, for a fair comparison, we choose ResNet-\n50\n55 as our basic model to perform the image-only training, while\nseveral works84 are based on more powerful models, i.e., Vision\nTransformer (ViT)85. During training, weﬁrst sample a mini-batch\nof N medical images. Then, for each input medical image, we\nrandomly select the image augmentation functions, e.g., afﬁne\ntransformations (shearing and rotation), colour jittering (contrast\nand brightness), and random Gaussian blurring 30,34,45,46,56,t o\ntransform the current medical image into two correlated views of\nthe same image, encoded by ResNet-50 asV\ni and Vj, which we\nconsider as a positive pair. As a result, we can obtain a training\nbatch with 2N images. We treat the other 2(N − 1) augmented\nimages as negative examples to Vi. The image-level contrastive\nlearning aims to minimise the distance between positive\nexamples, e.g., V\ni and Vj, while maximising the distance between\nnegative examples, e.g., Vi and Vk (k ≠ i, j). To this end, we adopt\nthe ICL loss to train our approach, deﬁned as follows:\nℓICL ¼\nX\nði;jÞ\n/C0 log exp hVi; Vj i=τ\n/C0/C1\nP\nk≠i exp hVi; Vk i=τðÞ ; (1)\nwhere the 〈 ⋅ , ⋅ 〉 denotes the cosine similarity and τ is a\ntemperature hyperparameter46.\nWhile conventional (image-level) contrastive learning can\nenable the model to understand the input medical images by\ntraining the model to distinguish whether the inputted medical\nimages are from the same image or not, it is plausible that this\ncould result in a model that is primarily learning to distinguish\nimages based on the appearance of images, instead of the\npathology. To incorporate the characteristics of medical images,\nwe further introduce patient-level contrastive learning (PCL)\n86,87.\nIn detail, PCL takes two images with completely different views,\ni.e., Anteroposterior (AP) and Posteroanterior (PA), as input. This\napproach prevents the model from distinguishing the input\nimages by learning to capture the appearance. Instead, it forces\nthe model to pay more attention to capturing the pathologies in\nmedical images.\nIn implementations, PCL considers two medical images, e.g., AP\nand PA views, which are encoded by ResNet-50 asV\n0\ni and V0\nj from\nthe same patient as a positive pair, and the remaining images\nfrom other patients in the mini-batch as negative examples. The\nPCL is deﬁned as follows:\nℓ\nPCL ¼\nX\nði;jÞ\n/C0 log\nexpðhV0\ni ; V0\nj i=τÞP\nk≠i expðhV0\ni ; V0\nk i=τÞ : (2)\nThe full training objective of image-only pre-training is deﬁned as:\nℓImage = ℓPCL + ℓICL. As we can see, the training of our method\ndoes not rely on labelled data, thus, the image-only pre-training\ncould be unsupervised. During training, we exploit the image-only\ndata from several public datasets, including CheXpert\n36, MIMIC-\nCXR37, COVID-CXR 39,41, COVID-19-CT-CXR 43, COVIDx-CXR-2 40,\nBIMCV-COVID-1942, RSNA Pneumonia70, and COVID-19 CT 44,t o\nconduct the image-only pre-training. As a result, we can learn\ncomprehensive thorax knowledge from image-only data. In\nparticular, when we evaluate the Med-MLLM on a dataset, we\nwill exclude it from the pre-training set.\nImage augmentation and regularisation. Since the size of medical\nimage datasets is usually smaller than the size of natural image\ndatasets, such as ImageNet\n88, we adopt image augmentation\nstrategies to further improve the performance and robustness of\nour framework. In implementations, we apply random cropping,\nrotation (−10 to 10 degrees), brightness and contrast adjustment\nwith ratios randomly sampled from [0.8, 1.2], horizontalﬂipping\nwith 50% probability, and Gaussian blurring withσ ∈ [0.1, 3.0], as\nTable 9. Ablation study of the proposed components in three pre-training settings: image-only, text-only, and image-text.\nSettings Methods Reporting: Spanish Reporting: English Diagnosis Prognosis\nBLEU-2 BLEU-3 BLEU-4 ROUGE-L CIDEr BLEU-2 BLEU-3 BLEU-4 ROUGE-L CIDEr Image-Text Image-Text\nFull Med-MLLM 47.6 42.0 38.1 55.4 73.4 32.4 28.7 24.9 46.7 53.0 78.2 92.0\n(a) w/o image-only (PCL) 45.8 39.9 34.7 54.3 70.9 31.2 27.5 23.0 45.3 51.8 75.7 90.2\n(b) w/o image-only 39.5 35.4 32.2 48.7 57.8 28.1 24.0 19.3 42.1 45.9 69.4 82.3\n(c) w/o text-only (MLM) 45.7 39.6 34.2 54.7 71.1 31.3 27.2 21.8 44.9 50.7 73.3 86.7\n(d) w/o text-only (SR) 44.2 38.5 33.6 54.1 70.3 30.2 26.7 21.5 44.0 49.4 76.3 89.1\n(e) w/o text-only (FIA) 46.0 40.1 36.5 54.2 70.6 31.8 27.5 23.2 45.5 51.6 75.4 88.3\n(f) w/o image-text 42.4 38.3 34.2 53.7 66.5 29.8 25.6 20.7 44.5 48.9 74.1 85.4\nWe perform the analysis on COVID-19 reporting, diagnosis, and prognosis. All values are reported in percentage (%).\nTable 10. The COVID-19 diagnosis and prognosis accuracy (AUC) of\ndifferent sizes of our Med-MLLM, which are trained on full training\ndata.\nMethods Year Retrospective studies Prospective studies\nDiagnosis Prognosis Diagnosis Prognosis\nClinicalBERT94 2019 93.2 93.4 74.8 87.5\nBioBERT93 2020 85.9 92.2 71.2 84.6\nPubMedBERT92 2022 95.7 93.9 76.7 88.4\nMed-MLLM-Base Ours 96.3 95.3 84.1 92.8\nMed-MLLM-Large Ours 98.0\n(+1.7) 97.2(+1.9) 87.7(+3.6) 94.9(+2.1)\nAll values are reported in percentage (%). The best results are in bold.\nF. Liu et al.\n10\nnpj Digital Medicine (2023)   226 Published in partnership with Seoul National University Bundang Hospital\nused in previous works 30,32,34,89. Besides augmentation, we\nintroduce several regularisation methods into our framework.\nFirst, we re-write the full training loss of the image-only pre-\ntraining as follows:\nℓImage ¼ λℓPCL þð 1 /C0 λÞℓICL; (3)\nwhere λ ∈ [0, 1] is the hyperparameter that controls the regular-\nisation. We set λ = 0.2 according to the performance on the\nvalidation set. Meanwhile, the global batch normalisation46, layer\nnormalisation90 and dropout91 are used in regularisation. The\nexperiments show that all the introduced modules contribute to\nimproved performances.\nText-only pre-training\nAs shown in Fig.1, we adopt a specialised medical large language\nmodel (LLM) with a radiology-speciﬁc vocabulary. Meanwhile, as\nshown in Fig. 2b, we present two training objectives and a text\naugmentation method to enhance the performance of our LLM.\nLarge language model (LLM) . In recent years, several\nefforts75,76,80,92 have been invested to build medical large\nlanguage models, which have shown great success in processing\nmedical text, such as BioBERT93, ClinicalBERT94, BlueBERT95, and\nPubMedBERT92. In detail, BioBERT is pre-trained on PubMed 47,\nClinicalBERT is pre-trained on MIMIC-III48, while BlueBERT com-\nbines both corpora for pre-training. All these methods use a\nvocabulary deﬁned on open-domain text (i.e., Wiki+ Books) as in\noriginal BERT49. For comparison, PubMedBERT is pre-trained on\nPubMed47 with a medical vocabulary designed on medical text\nfrom PubMed.\nAs we can see, among the above models, only PubMedBERT\ndesigned a domain-speciﬁc vocabulary for training; none of the\nexisting LLMs designed a radiology-speci ﬁc vocabulary. For\nexample, the radiology-speciﬁc term‘cardiomegaly’will be broken\ninto multiple sub-words (word pieces), i.e.,‘card-io-me-gal-y’ and\n‘cardio-me-gal-y’ in the ClinicalBERT and PubMedBERT, respec-\ntively. Since most sub-words have no medical relevance, it hinders\nthe LLMs from accurately understanding the radiology-speci ﬁc\nmedical terms30.\nTo resolve this, we introduce a radiology-speciﬁc vocabulary30\nbased on the medical texts from PubMed 47, MIMIC-III clinical\nnotes48, and MIMIC-CXR medical reports37. Based on the designed\nradiology-speciﬁc vocabulary that includes the whole-word\nradiology-speciﬁc terms (e.g., ‘cardiomegaly’), we perform pre-\ntraining of our model on the text-only data from PubMed +\nMIMIC-III + MIMIC-CXR corpora. In the following, we will introduce\nthe training objectives of our framework in detail.\nTraining objectives. This section introduces the training objec-\ntives used in our method. In implementations, we adopt three\ntraining objectives, i.e., Masked Language Modelling (MLM),\nSentence Reconstruction (SR), and medical-report-speci ﬁc\nFindings-Impression Alignment (FIA).\nMasked Language Modelling (MLM) . Given a mini-batch of N\nmedical text sequences, following conventional BERT\n49,50, for each\nmedical text sequence, we randomly mask out the input words\nwith 15% probability, resulting inN sequences of masked words\nand unmasked words (wm, w\\m). The training objective of MLM is\nto predict the randomly masked words wm based on the\nremaining unmasked words w\\m. Therefore, the MLM loss is\ndeﬁned as:\nℓMLM ¼/C0 1\nN\nP\nðwm;wnmÞ\nlog pw mjwnm\n/C0/C1/C0/C1\n; (4)\nwhere p denotes the predicted probability. The masked tokens are\npredicted as a classiﬁcation problem by selecting one token from\nthe vocabulary.\nSentence Reconstruction (SR). We further introduce a training\nobjective, sentence reconstruction, to boost the understanding\nand generation of medical text. As shown in Fig.2b, we introduce\nan additional text decoder to reconstruct the input medical text in\nthe auto-encoding pipeline. It means that the decoder takes the\ninput medical text as the ground truth, i.e.,W = {w\n1, w2, … , wM}, for\nsentence reconstruction. Therefore, the sentence reconstruction\nloss is deﬁned as:\nℓSR ¼/C0 1\nN\nX\nW\nXM\nt¼1\nlog pw tjw1:t/C0 1ðÞðÞ : (5)\nThe training objective is to reconstruct the same input sentence,\nand it is straightforward for our model to be trained51,62,96 to learn\nthe necessary domain knowledge from the unlabelled\nmedical texts.\nFindings-Impression Alignment (FIA). We observe that a medical\nreport contains rich structural information. Typically, it contains a\nsection for “ﬁndings” and another section for“impression”, where\nthe former is a paragraph of multiple sentences describing both\nthe normal and abnormal ﬁndings in detail, and the latter\nsummarizes a diagnostic conclusion from theﬁndings section. We\ntherefore introduce the training objective FIA\n30 to exploit the\nstructural information of medical reports.\nIn implementations, we adopt self-supervised learning and\ncontrastive loss46.W e ﬁrst sample a batch ofN medical reports,\nincluding N pairs of“Findings” and “Impression” sections. Then, we\ndenote the encoded “Findings” and “Impression” sections of the\nith input medical report as (TF\ni ; TI\ni ), which we consider as a positive\npair. “Findings” and “Impression” from different medical reports\nare used as negative pairs. The training loss of FIA is deﬁned as\nfollows:\nℓðF!IÞ\ni ¼/C0 log\nexp TF\ni ;TI\nihi =τðÞPN\nj¼1 exp TF\ni ;TI\njhi =τðÞ ;\nℓðI!FÞ\ni ¼/C0 log\nexp TI\ni ;TF\nihi =τðÞPN\nj¼1 exp TI\ni ;TF\njhi =τðÞ ;\n(6)\nwhere the 〈 ⋅ , ⋅ 〉 denotes the cosine similarity and τ is a\ntemperature hyperparameter46. We note that the numerators in\nboth two losses are equal, representing the similarity betweenTF\ni\nand TI\ni for the ith positive pair of “Findings” and “Impression”.\nHowever, their denominators differ. For the ﬁrst loss ℓðF!IÞ\ni , the\ndenominator measures the similarity between theith “Findings”\nTF\ni\n/C0/C1\nand all other “Impressions”. For ℓðI!FÞ\ni , the denominator\nmeasures the similarity between theith “Impression” TI\ni\n/C0/C1\nand all\nother “Findings”. Therefore, the two Equations are distinct and\nrespectively re ﬂect the similarity of “Findings” relative to\n“Impression” (F →I) and“Impression” relative to“Findings” (I →F).\nFinally, we obtain the full training objective of FIA by combining\nthe ℓðF!IÞ\ni and ℓðI!FÞ\ni , as follows:\nℓFIA ¼ 1\nN\nXN\ni¼1\nℓðF!IÞ\ni þ ℓðI!FÞ\ni\n/C16/C17\n: (7)\nThrough the above operation, our method exploits the structural\ninformation to improve the understanding of medical texts, and\nthus boost the performance.\nText augmentation and regularisation . To further improve the\nperformance of our method, we present a text augmentation\nmethod and several regularisation methods.\nFor the text augmentation, we observe that each medical text is\ncomposed of multiple sentences, which are usually permutation-\ninvariant\n97. Therefore, we can randomly shufﬂe the sentences to\naugment the medical texts to boost performance.\nF. Liu et al.\n11\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   226 \nMeanwhile, we introduceα and β f o rb e t t e rr e g u l a r i s a t i o n .T h ef u l l\ntraining objective of text-only pre-trainingℓText is deﬁned as follows:\nℓText ¼ ℓFIA þ αℓSR þ βℓMLM: (8)\nIn implementations, theα and β are set to 0.5 and 0.1, respectively,\naccording to the performances on the validation set. In detail, our\nframework is ﬁrst trained using MLM (ℓ\nMLM), then is trained using\nthe combination of MLM and FIA, andﬁnally is trained on the full\ntraining objective ℓText.\nImage-text pre-training\nMost recently, several image-text pre-training methods30,32,34,44\nhave been proposed to demonstrate the importance of unifying\nthe images and texts to improve the understanding of medical\ndata. However, all existing methods mainly adopt supervised\ntraining and heavily rely on large-scale coupled image-report pairs\nfor training, while collecting labelled and paired medical data\nacross different modalities is typically very costly and time-\nconsuming. To this end, we introduce the image-text pre-training\nto relax the reliance on the labelled image-text pairs\n89.\nSoft image-text alignment (SITA) . As shown in Fig. 2 (c), we\nincorporate a knowledge base and a pre-training objective, i.e., Soft\nImage-Text Alignment (SITA)89,98. In particular, given a mini-batch of\nN randomly sampled pairs of images and texts, we adopt\nMetaMap99 to extract entities de ﬁned in the Uni ﬁed Medical\nLanguage System (UMLS)53 from the ith medical text. Following\nprevious works 36,37,62,89,100,101, we focus on the 14 common\nradiographic entities (Atelectasis, Cardiomegaly, Consolidation,\nEdema, Enlarged Cardiomediastinum, Fracture, Lung Lesion, Lung\nOpacity, No Finding, Pleural Effusion, Pleural Other, Pneumonia,\nPneumothorax, Support Devices). As a result, given the medical\ntext, e.g.,\"A right pleural effusion. Heart size is enlarged. No evidence\nof pneumothorax” , we can extract two entities,pleural effusionand\ncardiomegaly. Then, we construct a multi-hot vector H\nT\ni of\ndimension 14 from the extracted entities, where 1/0 denotes the\npresence/absence of the radiographic entity. Similarly, for thejth\nmedical image with diagnosis labels, we again adopt MetaMap99 to\nextract radiographic entities by mapping the raw diagnosis labels of\nmedical images to UMLS concepts, e.g.,“Normal” will be mapped to\n“No Findings”. As a result, the images and the texts can share the\nsame radiographic entities. Then, we can construct a multi-hot\nvector HV\nj of dimension 14 for the image. At last, we calculate the\ncosine similarity ofHT\ni and HV\nj to measures the similarity of theith\ntext and the jth image. In this way, we measure the similarity\nbetween any text and image. The target similarity score sðT!VÞ\nij\nbetween theith text and thejth image is calculated as:\nsðT!VÞ\nij ¼\nexp HT\ni ; HV\nj\nDE .\nτ\n/C16/C17\nPN\nk¼1 exp HT\ni ; HV\nk\n/C10/C11 .\nτ\n/C16/C17 ; (9)\nwhere 〈 ⋅ , ⋅ 〉 denotes the cosine similarity andτ is a temperature\nparameter. Similarly, we can obtain the target similarity score\nsðV!TÞ\nji between the jth image and theith text:\nsðV!TÞ\nji ¼\nexp HV\nj ; HT\ni\nDE .\nτ\n/C16/C17\nPN\nk¼1\nexp HV\nj ; HT\nk\nDE .\nτ\n/C16/C17 : (10)\nsðT!VÞ\nij and sðV!TÞ\nji are used as the soft target labels of image-text\nalignment in the image-text pre-training, which will be introduced\nas follows.\nTo perform the image-text pre-training, weﬁrst use the BERT49 and\nResNet-5055 to encode theith text andjth image, resulting inTi and Vj,\nrespectively. Therefore, the predicted similarity scores0\nij\nðT!VÞ between\nthe ith text and the jth image and the predicted similarity score\ns0\nji\nðV!TÞ between thejth image and theith text are calculated by:\ns0\nij\nðT!VÞ ¼ expðhTi ;Vj i=τÞ\nPN\nk¼1\nexpðhTi ;Vk i=τÞ\n;\ns0\nji\nðV!TÞ ¼ expðhVj ;Ti i=τÞ\nPN\nk¼1\nexpðhVj ;Tk i=τÞ\n:\n(11)\nAt last, the soft image-text alignment (SITA) loss is implemented\nby the cross entropy loss:\nℓT!V\ni ¼/C0 PN\nj¼1\nsðT!VÞ\nij log s0\nij\nðT!VÞ;\nℓV!T\nj ¼/C0 PN\ni¼1\nsðV!TÞ\nji log s0\nji\nðV!TÞ;\nℓSITA ¼ 1\nN\nPN\nk¼1\nℓT!V\nk þ ℓV!T\nk\n/C0/C1\n:\n(12)\nThrough the SITA, our method performs image-text pre-training to\nexploit unpaired medical images and texts to ef ﬁciently and\naccurately align medical data across modalities89.\nData augmentation and regularisation . Similarly, we introduce\nimage augmentation in image-only pre-training and text aug-\nmentation in text-only pre-training to further boost the robustness\nand thus improve the performance of our method.\nMore importantly, during the regularisation, we incorporate the\nMLM loss for joint training, resulting in the full training objective of\nimage-text pre-training as follows:\nℓImage/C0 Text ¼ ℓSITA þ γℓMLM: (13)\nIn implementations, γ controls the regularisation and is set to 2,\naccording to the performances on the validation set. Our\npreliminnarly experiments show the effectiveness of performing\ncontinuous MLM optimisation.\nExperiment settings\nFor a fair comparison, we adopt the ResNet-50\n55 as the image\nencoder and the BERT5,102 as the text encoder. The number of\nencoder layers is set to 6 and the dimension of the latent states is\n768 unless otherwise stated. Meanwhile, we also explored a larger\nversion of the language model\n49,80 with 8.9 billion parameters,\nwhere the number of layers is 56, the number of attention heads is\n56, and the dimensionality of the latent states is 3584. We adopt\nthe AdamW optimiser103 for training. We train our model in the\norder of image-only, text-only, and image-text pre-training. During\nimage-only/text-only/image-text pre-training: the hyper-\nparameter τ is set to 0.5/0.5/0.1 according to the average\nperformances on the validation sets; we use a learning rate of\n10\n−3/2 × 10−5/5 × 10−5 and a batch size of 256/256/100. During\nﬁne-tuning, we use a batch size of 32/64/16 and a learning rate of\n10−4 for parameter optimisation on the COVID-19 reporting/\ndiagnosis/prognosis task. Our code is implemented in PyTorch104.\nDuring testing, we add a text decoder, i.e., Transformer 5,t o\nperform the reporting task, and add a fully connected layer to\nperform the diagnosis and prognosis tasks.\nEthical considerations . Our study was conducted on thirteen\ndatasets, in which all Protected Health Information (PHI), e.g.,\npatient name, sex, gender, and date of birth, is of ﬁcially de-\nidentiﬁed for all datasets used in our experiments. It means that the\ndeletion of PHI from structured data sources (e.g., databaseﬁelds\nthat provide age, genotypic information, past and current diagnosis\nand treatment categories) is performed in compliance with the\nHealth Insurance Portability and Accountability Act (HIPAA)\nstandards in order to facilitate public access to the datasets.\nF. Liu et al.\n12\nnpj Digital Medicine (2023)   226 Published in partnership with Seoul National University Bundang Hospital\nRecruitment statement . We do not recruit any new human\nresearch participants for this study. For the public data, all\nnecessary patient/participant consent has been obtained and the\nappropriate institutional forms have been ofﬁcially archived.\nReporting summary\nFurther information on research design is available in the Nature\nResearch Reporting Summary linked to this article.\nDATA AVAILABILITY\nThe data used in our work may be available for research purposes from the\ncorresponding authors upon reasonable request.\n1) CheXpert is available athttps://stanfordmlgroup.github.io/competitions/chexpert/.\n2) COVIDx-CXR-2 is available athttps://alexswong.github.io/COVID-Net/.\n3) MIMIC-CXR is available athttps://physionet.org/content/mimic-cxr/2.0.0/.\n4) COVID-19-CT-CXR is available athttps://github.com/ncbi-nlp/COVID-19-CT-CXR.\n5) COVID-19 CT is available athttps://covid19ct.github.io/.\n6) COVID-CXR is available athttps://github.com/ieee8023/covid-chestxray-dataset.\n7) BIMCV-COVID-19 is available at https://bimcv.cipf.es/bimcv-projects/bimcv-\ncovid19/.\n8) PubMed is available athttps://pubmed.ncbi.nlm.nih.gov/download/.\n9) MIMIC-III is available athttps://physionet.org/content/mimiciii/1.4/.\n10) SIIM-ACR is available at https://www.kaggle.com/c/siim-acr-pneumothorax-\nsegmentation.\n11) RSNA is available at https://www.kaggle.com/c/rsna-pneumonia-detection-\nchallenge.\n12) NIH ChestX-ray is available athttps://nihcc.app.box.com/v/ChestXray-NIHCC.\n13) Shenzhen Tuberculosis is available at: https://www.kaggle.com/raddar/\ntuberculosis-chest-xrays-shenzhen.\nCODE AVAILABILITY\nThe code that supports theﬁndings of this study is available from the corresponding\nauthors upon reasonable request.\nReceived: 8 May 2023; Accepted: 24 October 2023;\nREFERENCES\n1. Roberts, M. et al. Common pitfalls and recommendations for using machine\nlearning to detect and prognosticate for covid-19 using chest radiographs and\nct scans. Nat. Mach. Intell.3, 199–217 (2021).\n2. Driggs, D. et al. Machine learning for covid-19 diagnosis and prognostication:\nlessons for amplifying the signal while reducing the noise.Radiol. Artif. Intell.3,\ne210011 (2021).\n3. Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classiﬁcation with deep\nconvolutional neural networks. In Advances in Neural Information Processing\nSystems (Curran Associates, Inc., 2012).\n4. Esteva, A. et al. Dermatologist-level classiﬁcation of skin cancer with deep neural\nnetworks. Nature 542, 115–118 (2017).\n5. Vaswani, A. et al. Attention is all you need. InAdvances in Neural Information\nProcessing Systems (Curran Associates, Inc., 2017).\n6. Lyu, W. et al. A multimodal transformer: Fusing clinical notes with structured\nEHR data for interpretable in-hospital mortality prediction. InAmerican Medical\nInformatics Association Annual Symposium(AMIA, 2022).\n7. Jing, B., Xie, P. & Xing, E. P. On the automatic generation of medical imaging\nreports. In Annual Meeting of the Association for Computational Linguistics\n(Association for Computational Linguistics, 2018).\n8. Liu, G. et al. Clinically accurate chest x-ray report generation. In Machine\nLearning for Healthcare Conference(PMLR, 2019).\n9. Li, Y., Liang, X., Hu, Z. & Xing, E. P. Hybrid retrieval-generation reinforced agent\nfor medical image report generation. In Advances in Neural Information Pro-\ncessing Systems (Curran Associates, Inc., 2018).\n10. Liu, F., Ge, S. & Wu, X. Competence-based multimodal curriculum learning for\nmedical report generation. In Annual Meeting of the Association for Computa-\ntional Linguistics (Association for Computational Linguistics, 2021).\n11. Liu, F., Wu, X., Ge, S., Fan, W. & Zou, Y. Exploring and distilling posterior and prior\nknowledge for radiology report generation. In IEEE Conference on Computer\nVision and Pattern Recognition(IEEE, 2021).\n12. Bhattacharya, S. et al. Deep learning and medical image processing for cor-\nonavirus (covid-19) pandemic: a survey.Sustain. Cities Soc.65, 102589 (2021).\n13. Soomro, T. A. et al. Artiﬁcial intelligence (ai) for medical imaging to combat\ncoronavirus disease (covid-19): a detailed review with direction for future\nresearch. Artif. Intell. Rev.55, 1409–1439 (2022).\n14. El-Sadr, W. M., Vasan, A. & El-Mohandes, A. Facing the new covid-19 reality.N.\nEngl. J. Med.388, 385–387 (2023).\n15. Carlile, M. et al. Deployment of artiﬁcial intelligence for radiographic diagnosis\nof covid-19 pneumonia in the emergency department.J. Am. Coll. Emerg. Phys.\nOpen 1, 1459–1464 (2020).\n16. Wu, X. et al. Deltanet: Conditional medical report generation for COVID-19\ndiagnosis. InInternational Conference on Computational Linguistics(International\nCommittee on Computational Linguistics, 2022).\n17. You, D. et al. Aligntransformer: Hierarchical alignment of visual regions and\ndisease tags for medical report generation. InMedical Image Computing and\nComputer Assisted Intervention,7 2–82 (Springer, 2021).\n18. Sinsky, C. et al. Allocation of physician time in ambulatory practice: a time and\nmotion study in 4 specialties.Ann. Internal Med.165, 753–760 (2016).\n19. Weiner, M. & Biondich, P. The inﬂuence of information technology on patient-\nphysician relationships. J. General Internal Med.21,3 5–39 (2006).\n20. Tawﬁk, D. S. et al. Physician burnout, well-being, and work unit safety grades in\nrelationship to reported medical errors. InMayo Clinic Proceedings, 1571–1580\n(Elsevier, 2018).\n21. West, C. P., Dyrbye, L. N. & Shanafelt, T. D. Physician burnout: contributors,\nconsequences and solutions.J. Internal Med.283, 516–529 (2018).\n22. Zhou, S. K. et al. A review of deep learning in medical imaging: imaging traits,\ntechnology trends, case studies with progress highlights, and future promises.\nProc. IEEE 109, 820–838 (2021).\n23. Liu, F. et al. Retrieve, reason, and reﬁne: Generating accurate and faithful patient\ninstructions. In Advances in Neural Information Processing Systems (Curran\nAssociates, Inc., 2022).\n24. Zu, Z. Y. et al. Coronavirus disease 2019 (covid-19): a perspective from china.\nRadiology 296, E15–E25 (2020).\n25. Fang, Y. et al. Sensitivity of chest ct for covid-19: comparison to rt-pcr.Radiology\n296, E115–E117(2020).\n26. Ng, M.-Y. et al. Imaging proﬁle of the covid-19 infection: radiologicﬁndings and\nliterature review. Radiology: Cardiothoracic Imaging 2, e200034 (2020).\n27. Rubin, G. D. et al. The role of chest imaging in patient management during the\ncovid-19 pandemic: a multinational consensus statement from theﬂeischner\nsociety. Radiology 296, 172–180 (2020).\n28. Brown, T. et al. Language models are few-shot learners. InAdvances in Neural\nInformation Processing Systems(Curran Associates, Inc., 2020).\n29. OpenAI. Gpt-4 technical report. Preprint at https://arxiv.org/abs/2303.08774\n(2023).\n30. Boecking, B. et al. Making the most of text semantics to improve biomedical\nvision–language processing. In European Conference on Computer Vision,1 –21\n(Springer, 2022).\n31. Zhou, H.-Y., Lian, C., Wang, L. & Yu, Y. Advancing radiograph representation\nlearning with masked record modeling. InThe Eleventh International Conference\non Learning Representations(OpenReview.net, 2023).\n32. Zhou, H. et al. Generalized radiograph representation learning via cross-\nsupervision between images and free-text radiology reports.Nat. Mach. Intell.4,\n32–40 (2022).\n33. Tiu, E. et al. Expert-level detection of pathologies from unannotated chest x-ray\nimages via self-supervised learning.Nat. Biomed. Eng.6, 1399–1406 (2022).\n34. Zhang, Y., Jiang, H., Miura, Y., Manning, C. D. & Langlotz, C. P. Contrastive\nlearning of medical visual representations from paired images and text. In\nMachine Learning for Healthcare Conference,2 –25 (2022).\n35. Desai, S. B., Pareek, A. & Lungren, M. P. Deep learning and its role in covid-19\nmedical imaging. Intell. Based Med.3, 100013 (2020).\n36. Irvin, J. et al. Chexpert: A large chest radiograph dataset with uncertainty labels\nand expert comparison. In Association for the Advancement of Artiﬁcial Intelli-\ngence (AAAI Press, 2019).\n37. Johnson, A. E. et al. Mimic-cxr, a de-identiﬁed publicly available database of\nchest radiographs with free-text reports.Sci. Data 6, 317 (2019).\n38. Rahman, T. et al. Exploring the effect of image enhancement techniques on\nCOVID-19 detection using chest x-ray images.Comput. Biol. Med. 132, 104319\n(2021).\n39. Cohen, J. P. et al. Covid-19 image data collection: prospective predictions are\nthe future. Mach. Learn. Biomed. Imaging1,1 –10 (2020).\n40. Pavlova, M. et al. Covid-net cxr-2: an enhanced deep convolutional neural\nnetwork design for detection of covid-19 cases from chest x-ray images.Front.\nMed. 9, 861680 (2022).\n41. Cohen, J. P., Morrison, P. & Dao, L. COVID-19 image data collection. Preprint at\nhttps://arxiv.org/abs/2003.11597 (2020).\nF. Liu et al.\n13\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   226 \n42. Vayá, M. D. L. I. et al. Bimcv covid-19+: a large annotated dataset of rx and ct\nimages from covid-19 patients. Preprint at https://arxiv.org/abs/2006.01174\n(2020).\n43. Peng, Y. et al. COVID-19-CT-CXR: A freely accessible and weakly labeled chest\nx-ray and CT image collection on COVID-19 from biomedical literature. IEEE\nTrans. Big Data7,3 –12 (2021).\n44. Liu, G. et al. Medical-vlbert: Medical visual language BERT for COVID-19 CT\nreport generation with alternate learning.IEEE Trans. Neural Networks Learn. Syst.\n32, 3786–3797 (2021).\n45. He, K., Fan, H., Wu, Y., Xie, S. & Girshick, R. B. Momentum contrast for unsu-\npervised visual representation learning. InIEEE Conference on Computer Vision\nand Pattern Recognition(IEEE, 2020).\n46. Chen, T., Kornblith, S., Norouzi, M. & Hinton, G. E. A simple framework for\ncontrastive learning of visual representations. In International Conference on\nMachine Learning (PMLR, 2020).\n47. National Institutes of Health. PubMed Corpora, https://\npubmed.ncbi.nlm.nih.gov/download/ (2022).\n48. Johnson, A. E. W. et al. MIMIC-III, a freely accessible critical care database.Sci.\nData 3 160035 (2016).\n49. Devlin, J., Chang, M., Lee, K. & Toutanova, K. BERT: pre-training of deep bidir-\nectional transformers for language understanding. InConference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Lan-\nguage Technologies (Association for Computational Linguistics, 2019).\n50. Liu, Y. et al. Roberta: A robustly optimized BERT pretraining approach. Preprint\nat https://arxiv.org/abs/1907.11692 (2019).\n51. Tschannen, M., Bachem, O. & Lucic, M. Recent advances in autoencoder-based\nrepresentation learning. Preprint athttps://arxiv.org/abs/1812.05069 (2018).\n52. Casey, A. et al. A systematic review of natural language processing applied to\nradiology reports. BMC Med. Inf. Decis. Mak.21, 179 (2021).\n53. Bodenreider, O. The uniﬁed medical language system (UMLS): integrating bio-\nmedical terminology. Nucleic Acids Res. 32, D267–D270 (2004).\n54. Radford, A. et al. Learning transferable visual models from natural language\nsupervision. In International conference on machine learning, 8748–8763 (PMLR,\n2021).\n55. He, K., Zhang, X., Ren, S. & Sun, J. Deep residual learning for image recognition.\nIn IEEE Conference on Computer Vision and Pattern Recognition(IEEE, 2016).\n56. Chen, X., Fan, H., Girshick, R. B. & He, K. Improved baselines with momentum\ncontrastive learning. Preprint athttps://arxiv.org/abs/2003.04297 (2020).\n57. Narin, A., Kaya, C. & Pamuk, Z. Automatic detection of coronavirus disease\n(covid-19) using x-ray images and deep convolutional neural networks.Pattern\nAnal. Appl. 24, 1207–1220 (2021).\n58. Ozturk, T. et al. Automated detection of covid-19 cases using deep neural\nnetworks with x-ray images.Comput. Biol. Med.121, 103792 (2020).\n59. Irmak, E. A novel deep convolutional neural network model for covid-19 disease\ndetection. In Medical Technologies Congress,1 –\n4 (IEEE, 2020).\n60. Hall, L. O., Paul, R., Goldgof, D. B. & Goldgof, G. M. Finding covid-19 from chest\nx-rays using deep learning on a small dataset. Preprint athttps://arxiv.org/abs/\n2004.02060 (2020).\n61. Chen, Z., Song, Y., Chang, T. & Wan, X. Generating radiology reports via memory-\ndriven transformer. In Conference on Empirical Methods in Natural Language\nProcessing (Association for Computational Linguistics, 2020).\n62. Liu, F. et al. Auto-encoding knowledge graph for unsupervised medical report\ngeneration. In Advances in Neural Information Processing Systems (Curran\nAssociates, Inc., 2021).\n63. Wang, J., Bhalerao, A. & He, Y. Cross-modal prototype driven network for radi-\nology report generation. InEuropean Conference on Computer Vision(Springer,\n2022).\n64. Liu, F. et al. Contrastive attention for automatic chest x-ray report generation. In\nFindings of the Association for Computational Linguistics: ACL-IJCNLP 2021, 269-\n280 (Association for Computational Linguistics, 2021).\n65. Papineni, K., Roukos, S., Ward, T. & Zhu, W. BLEU: a method for automatic\nevaluation of machine translation. In Annual Meeting of the Association for\nComputational Linguistics (Association for Computational Linguistics, 2002).\n66. Lin, C.-Y. ROUGE: A package for automatic evaluation of summaries. InAnnual\nMeeting of the Association for Computational Linguistics (Association for Com-\nputational Linguistics, 2004).\n67. Vedantam, R., Zitnick, C. L. & Parikh, D. Cider: Consensus-based image descrip-\ntion evaluation. InIEEE Conference on Computer Vision and Pattern Recognition\n(IEEE, 2015).\n68. Chen, X. et al. Microsoft COCO captions: data collection and evaluation server.\nPreprint at https://arxiv.org/abs/1504.00325 (2015).\n69. Wang, X. et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks\non weakly-supervised classiﬁcation and localization of common thorax diseases.\nIn IEEE Conference on Computer Vision and Pattern Recognition(IEEE, 2017).\n70. Shih, G. et al. Augmenting the national institutes of health chest radiograph\ndataset with expert annotations of possible pneumonia.Radiol. Artif. Intell. 1\ne180041 (2019).\n71. Society for Imaging Informatics in Medicine (SIIM). Siim-acr pneumothorax\nsegmentation. In Kaggle, https://www.kaggle.com/c/siim-acr-pneumothorax-\nsegmentation (2019).\n72. Jaeger, S. et al. Two public chest x-ray datasets for computer-aided screening of\npulmonary diseases. Quant. Imaging Medm Surg.4, 475 (2014).\n73. Huang, S., Shen, L., Lungren, M. P. & Yeung, S. Gloria: A multimodal global-local\nrepresentation learning framework for label-efﬁcient medical image recogni-\ntion. In International Conference on Computer Vision, 3922–3931 (IEEE, 2021).\n74. Radford, A. et al. Language models are unsupervised multitask learners.OpenAI\nBlog 1, 9 (2019).\n75. Singhal, K. et al. Large language models encode clinical knowledge.Nature 620,\n172–180 (2023).\n76. Singhal, K. et al. Towards expert-level medical question answering with large\nlanguage models. Preprint athttps://arxiv.org/abs/2305.09617 (2023).\n77. Wei, J. et al. Chain-of-thought prompting elicits reasoning in large language\nmodels. Adv. Neural Inf. Proces. Syst.35, 24824–24837 (2022).\n78. Shen, Y. et al. Chatgpt and other large language models are double-edged\nswords. Radiology 307, e230163 (2023).\n79. Kitamura, F. C. Chatgpt is shaping the future of medical writing but still requires\nhuman judgment. Radiology 307, 230171 (2023).\n80. Yang, X. et al. A large language model for electronic health records.NPJ Digital\nMed. 5, 194 (2022).\n81. Haghighi, F., Taher, M. R. H., Zhou, Z., Gotway, M. B. & Liang, J. Transferable visual\nwords: Exploiting the semantics of anatomical patterns for self-supervised\nlearning. IEEE Trans. Med. Imaging40, 2857–2868 (2021).\n82. Zhou, Z., Sodha, V., Pang, J., Gotway, M. B. & Liang, J. Models genesis.Med. Image\nAnal. 67, 101840 (2021).\n83. Zhou, H. et al. Comparing to learn: Surpassing imagenet pretraining on radio-\ngraphs by comparing image representations. In International Conference on\nMedical Image Computing and Computer Assisted Intervention , 398 –407\n(Springer, 2020).\n84. Shamshad, F. et al. Transformers in medical imaging: a survey.Medical Image\nAnal. 88, 102802 (2023).\n85. Dosovitskiy, A. et al. An image is worth 16x16 words: transformers for image\nrecognition at scale. In International Conference on Learning Representations\n(OpenReview.net, 2021).\n86. Vu, Y. N. T. et al. Medaug: Contrastive learning leveraging patient metadata\nimproves representations for chest x-ray interpretation. InMachine Learning for\nHealthcare Conference, 755–769 (PMLR, 2021).\n87. Kiyasseh, D., Zhu, T. & Clifton, D. A. Clocs: Contrastive learning of cardiac signals\nacross space, time, and patients. InInternational Conference on Machine Learn-\ning, 5606–5615 (PMLR, 2021).\n88. Deng, J. et al. Imagenet: a large-scale hierarchical image database. In IEEE\nConference on Computer Vision and Pattern Recognition(IEEE, 2009).\n89. Wang, Z., Wu, Z., Agarwal, D. & Sun, J. Medclip: contrastive learning from\nunpaired medical images and text. InConference on Empirical Methods in Natural\nLanguage Processing , 3876–3887 (Association for Computational Linguistics,\n2022).\n90. Ba, L. J., Kiros, R. & Hinton, G. E. Layer normalization. Preprint athttps://arxiv.org/\nabs/1607.06450 (2016).\n91. Srivastava, N., Hinton, G. E., Krizhevsky, A., Sutskever, I. & Salakhutdinov, R.\nDropout: a simple way to prevent neural networks from overﬁtting. J. Mach.\nLearn. Res. 15, 1929–1958 (2014).\n92. Gu, Y. et al. Domain-speciﬁc language model pretraining for biomedical natural\nlanguage processing. ACM Trans. Comput. Heal.3, 2:1–2:23 (2022).\n93. Lee, J. et al. Biobert: a pre-trained biomedical language representation model\nfor biomedical text mining.Bioinform. 36, 1234–1240 (2020).\n94. Alsentzer, E. et al. Publicly available clinical BERT embeddings. InProceedings of\nthe 2nd Clinical Natural Language Processing Workshop\n(Association for Com-\nputational Linguistics, 2019).\n95. Peng, Y., Yan, S. & Lu, Z. Transfer learning in biomedical natural language\nprocessing: an evaluation of BERT and elmo on ten benchmarking datasets. In\nBioNLP@Annual Meeting of the Association for Computational Linguistics,5 8–65\n(Association for Computational Linguistics, 2019).\n96. Wang, Y., Yao, H. & Zhao, S. Auto-encoder based dimensionality reduction.\nNeurocomputing 184, 232–242 (2016).\n97. Preechakul, K. et al. Set prediction in the latent space. InAdvances in Neural\nInformation Processing Systems, 25516–25527 (Curran Associates, Inc., 2021).\n98. Liu, F., Liu, Y., Ren, X., He, X. & Sun, X. Aligning visual regions and textual\nconcepts for semantic-grounded image representations. In Adv. Neural Inf.\nProces. Syst. (Curran Associates, Inc., 2019).\nF. Liu et al.\n14\nnpj Digital Medicine (2023)   226 Published in partnership with Seoul National University Bundang Hospital\n99. Aronson, A. R. & Lang, F.-M. An overview of metamap: historical perspective and\nrecent advances. J. Am. Med. Inf. Assoc.17, 229–236 (2010).\n100. Peng, Y. et al. Negbio: a high-performance tool for negation and uncertainty\ndetection in radiology reports.AMIA Summits Transl. Sci. Proc.2018, 188 (2018).\n101. Wang, X., Peng, Y., Lu, L., Lu, Z. & Summers, R. M. Tienet: Text-image embedding\nnetwork for common thorax disease classiﬁcation and reporting in chest x-rays.\nIn IEEE Conference on Computer Vision and Pattern Recognition(IEEE, 2018).\n102. Wolf, T. et al. Huggingface ’s transformers: State-of-the-art natural language\nprocessing. Preprint athttps://arxiv.org/abs/1910.03771 (2019).\n103. Loshchilov, I. & Hutter, F. Decoupled weight decay regularization. InInterna-\ntional Conference on Learning Representations(OpenReview.net, 2019).\n104. Paszke, A. et al. Pytorch: An imperative style, high-performance deep learning\nlibrary. In Advances in Neural Information Processing Systems(Curran Associates,\nInc., 2019).\n105. Chen, L. et al. Self-supervised learning for medical image analysis using image\ncontext restoration. Med. Image Anal.58, 101539 (2019).\nACKNOWLEDGEMENTS\nDAC was supported by an NIHR Research Professorship and a Royal Academy of\nEngineering Research Chair; the NIHR Oxford Biomedical Research Centre; the Hong\nKong Centre for Cerebro-cardiovascular Engineering (COCHE); and the Pandemic\nSciences Institute, University of Oxford, Oxford, UK. F.L. gratefully acknowledges\nfunding from the Clarendon Fund and the Magdalen Graduate Scholarship. T.Z. was\nsupported by the Royal Academy of Engineering under the Research Fellowship\nscheme. C.W. gratefully acknowledges funding from the Clarendon Fund and the\nSloane Robinson Scholarship. Y.Y. was supported by the Science and Technology\nCommittee of Shanghai Municipality (23ZR1436400). We sincerely thank all the\nanonymous reviewers and editors for their constructive comments and suggestions\nthat substantially improved this paper. D.A.C. and F.L. are the corresponding authors\nof this paper.\nAUTHOR CONTRIBUTIONS\nD.A.C. conceived the project. F.L. conceived and designed the study, performed the\ndata analysis, and prepared the manuscript. T.Z., X.W. and B.Y. contributed to the\nimplementations, experiments, and results interpretation. All authors contributed to\nthe ﬁnal manuscript preparation.\nCOMPETING INTERESTS\nThe authors declare no competing interests.\nADDITIONAL INFORMATION\nSupplementary information The online version contains supplementary material\navailable at https://doi.org/10.1038/s41746-023-00952-2.\nCorrespondence and requests for materials should be addressed to Fenglin Liu or\nDavid A. Clifton.\nReprints and permission information is available at http://www.nature.com/\nreprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims\nin published maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly\nfrom the copyright holder. To view a copy of this license, visit http://\ncreativecommons.org/licenses/by/4.0/.\n© The Author(s) 2023\nF. Liu et al.\n15\nPublished in partnership with Seoul National University Bundang Hospital npj Digital Medicine (2023)   226 "
}