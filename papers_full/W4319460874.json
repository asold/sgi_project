{
  "title": "How Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language Models for Medical Education and Knowledge Assessment",
  "url": "https://openalex.org/W4319460874",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3134629398",
      "name": "Aidan Gilson",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2791482975",
      "name": "Conrad W. Safranek",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2101979092",
      "name": "Thomas Huang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2787252827",
      "name": "Vimig Socrates",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A1966855885",
      "name": "Ling Chi",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2396393586",
      "name": "Richard Andrew Taylor",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2313261013",
      "name": "David Chartash",
      "affiliations": [
        "University College Dublin",
        "National University of Ireland",
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A3134629398",
      "name": "Aidan Gilson",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2791482975",
      "name": "Conrad W. Safranek",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2101979092",
      "name": "Thomas Huang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2787252827",
      "name": "Vimig Socrates",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A1966855885",
      "name": "Ling Chi",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2396393586",
      "name": "Richard Andrew Taylor",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2313261013",
      "name": "David Chartash",
      "affiliations": [
        "University College Dublin",
        "Yale University",
        "National University of Ireland"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3081747209",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W4285124505",
    "https://openalex.org/W3090073303",
    "https://openalex.org/W4281252097",
    "https://openalex.org/W3191249430",
    "https://openalex.org/W4296415471",
    "https://openalex.org/W4286985375",
    "https://openalex.org/W61072347",
    "https://openalex.org/W2952752960",
    "https://openalex.org/W2972522091",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W2987501933",
    "https://openalex.org/W2113671972",
    "https://openalex.org/W1995903131",
    "https://openalex.org/W2058046738",
    "https://openalex.org/W2211637316",
    "https://openalex.org/W2132053012"
  ],
  "abstract": "Background Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing model that can generate conversation-style responses to user input. Objective This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical Licensing Examination (USMLE) Step 1 and Step 2 exams, as well as to analyze responses for user interpretability. Methods We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to Step 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also provides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National Board of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language models, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical justification of the answer selected, presence of information internal to the question, and presence of information external to the question. Results Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved accuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT by 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant decrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical justification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the question was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and 27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P&lt;.001) and NBME-Free-Step2 (P=.001) data sets, respectively. Conclusions ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question answering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves the equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT’s capacity to provide logic and informational context across the majority of answers. These facts taken together make a compelling case for the potential applications of ChatGPT as an interactive medical education tool to support learning.",
  "full_text": "Original Paper\nHow Does ChatGPT Perform on the United States Medical\nLicensing Examination (USMLE)? The Implications of Large\nLanguage Models for Medical Education and Knowledge\nAssessment\nAidan Gilson1,2, BS; Conrad W Safranek1, BS; Thomas Huang2, BS; Vimig Socrates1,3, MS; Ling Chi1, BSE; Richard\nAndrew Taylor1,2*, MD, MHS; David Chartash1,4*, PhD\n1Section for Biomedical Informatics and Data Science, Yale University School of Medicine, New Haven, CT, United States\n2Department of Emergency Medicine, Yale University School of Medicine, New Haven, CT, United States\n3Program of Computational Biology and Bioinformatics, Yale University, New Haven, CT, United States\n4School of Medicine, University College Dublin, National University of Ireland, Dublin, Dublin, Ireland\n*these authors contributed equally\nCorresponding Author:\nDavid Chartash, PhD\nSection for Biomedical Informatics and Data Science\nYale University School of Medicine\n300 George Street\nSuite 501\nNew Haven, CT, 06511\nUnited States\nPhone: 1 203 737 5379\nEmail: david.chartash@yale.edu\nRelated Articles:\nThis is a corrected version. See correction statement in: https://mededu.jmir.org/2024/1/e57594\nComment in: https://mededu.jmir.org/2023/1/e46876/\nComment in: https://mededu.jmir.org/2023/1/e46885/\nComment in: https://mededu.jmir.org/2023/1/e48305\nComment in: https://mededu.jmir.org/2023/1/e50336\nAbstract\nBackground: Chat Generative Pre-trained Transformer (ChatGPT) is a 175-billion-parameter natural language processing\nmodel that can generate conversation-style responses to user input.\nObjective: This study aimed to evaluate the performance of ChatGPT on questions within the scope of the United States Medical\nLicensing Examination (USMLE) Step 1 and Step 2 exams, as well as to analyze responses for user interpretability.\nMethods: We used 2 sets of multiple-choice questions to evaluate ChatGPT’s performance, each with questions pertaining to\nStep 1 and Step 2. The first set was derived from AMBOSS, a commonly used question bank for medical students, which also\nprovides statistics on question difficulty and the performance on an exam relative to the user base. The second set was the National\nBoard of Medical Examiners (NBME) free 120 questions. ChatGPT’s performance was compared to 2 other large language\nmodels, GPT-3 and InstructGPT. The text output of each ChatGPT response was evaluated across 3 qualitative metrics: logical\njustification of the answer selected, presence of information internal to the question, and presence of information external to the\nquestion.\nResults: Of the 4 data sets, AMBOSS-Step1, AMBOSS-Step2, NBME-Free-Step1, and NBME-Free-Step2, ChatGPT achieved\naccuracies of 44% (44/100), 42% (42/100), 64.4% (56/87), and 57.8% (59/102), respectively. ChatGPT outperformed InstructGPT\nby 8.15% on average across all data sets, and GPT-3 performed similarly to random chance. The model demonstrated a significant\ndecrease in performance as question difficulty increased (P=.01) within the AMBOSS-Step1 data set. We found that logical\njustification for ChatGPT’s answer selection was present in 100% of outputs of the NBME data sets. Internal information to the\nJMIR Med Educ 2023 | vol. 9 | e45312 | p. 1https://mededu.jmir.org/2023/1/e45312\n(page number not for citation purposes)\nGilson et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nquestion was present in 96.8% (183/189) of all questions. The presence of information external to the question was 44.5% and\n27% lower for incorrect answers relative to correct answers on the NBME-Free-Step1 (P<.001) and NBME-Free-Step2 (P=.001)\ndata sets, respectively.\nConclusions: ChatGPT marks a significant improvement in natural language processing models on the tasks of medical question\nanswering. By performing at a greater than 60% threshold on the NBME-Free-Step-1 data set, we show that the model achieves\nthe equivalent of a passing score for a third-year medical student. Additionally, we highlight ChatGPT’s capacity to provide logic\nand informational context across the majority of answers. These facts taken together make a compelling case for the potential\napplications of ChatGPT as an interactive medical education tool to support learning.\n(JMIR Med Educ 2023;9:e45312) doi: 10.2196/45312\nKEYWORDS\nnatural language processing; NLP; MedQA; generative pre-trained transformer; GPT; medical education; chatbot; artificial\nintelligence; education technology; ChatGPT; conversational agent; machine learning; USMLE\nIntroduction\nChat Generative Pre-trained Transformer (ChatGPT) [1] is a\n175-billion-parameter natural language processing model that\nuses deep learning algorithms trained on vast amounts of data\nto generate human-like responses to user prompts [2]. As a\ngeneral purpose dialogic agent, ChatGPT is designed to be able\nto respond to a wide range of topics, potentially making it a\nuseful tool for customer service, chatbots, and a host of other\napplications. Since its release, it has garnered significant press\nfor both seemingly incredible feats such as automated generation\nof responses in the style of Shakespearean sonnets while also\nfailing to answer simple mathematical questions [3-5].\nChatGPT is the latest among a class of large language models\n(LLMs) known as autoregressive language models [6].\nGenerative LLMs believed to be similar to ChatGPT are trained\nusing the decoder component of a transformer model [7], tasked\nwith predicting the next token in a sequence on large corpora\nof text [8-10]. Such foundation models are often fine-tuned on\ntask-specific data to improve performance. However, the\nintroduction of OpenAI’s GPT-3 presented the first in a line of\nhighly scaled LLMs that achieve state-of-the-art performance\nwith little fine-tuning required [6]. ChatGPT builds on OpenAI’s\nprevious GPT-3.5 language models with the addition of both\nsupervised and reinforcement learning techniques [1]. ChatGPT\nis a direct descendant of InstructGPT, a fine-tuned version of\nGPT-3.5 trained on human-derived responses to prompts\nsubmitted to the OpenAI application programming interface\n(API) Playground. InstructGPT was developed by first being\ntasked to generate a set of responses to a particular prompt and\nhaving human annotators label the preferred answer. These\npreferences are then maximized in a reward model trained using\nProximal Policy Optimization, a reinforcement learning\nalgorithm, to tune InstructGPT. ChatGPT is reported to be\nspecifically trained on conversational prompts to encourage\ndialogic output.\nWithin the medical domain, LLMs have been investigated as\ntools for personalized patient interaction and consumer health\neducation [11,12]. Although demonstrating potential, these\nmodels have had limited success testing clinical knowledge\nthrough (generative) question-answering tasks [13,14]. ChatGPT\ncould represent the first in a new line of models that may better\nrepresent the combination of clinical knowledge and dialogic\ninteraction. ChatGPT’s interface that produces unique narrative\nreplies allows for novel use cases, such as acting as a simulated\npatient, a brainstorming tool providing individual feedback, or\na fellow classmate to simulate small group–style learning. For\nthese applications to be useful, however, ChatGPT must perform\ncomparably to humans on assessments of medical knowledge\nand reasoning such that users have sufficient confidence in its\nresponses.\nIn this paper, we aimed to quantify ChatGPT’s performance on\nexaminations that seek to assess the primary competency of\nmedical knowledge—established and evolving biomedical,\nclinical, epidemiological, and social-behavioral science\nknowledge—and a facet of its application to patient care through\nthe use of 2 data sets centered around knowledge tested in the\nUnited States Medical Licensing Examination (USMLE) Step\n1 and Step 2 Clinical Knowledge exams. Step 1 focuses on\nfoundational sciences and their relation to the practice of\nmedicine, whereas Step 2 focuses on the clinical application of\nthose foundational sciences. USMLE Step 3 was excluded as\nit is intended to assess skills and capacity for independent\ngeneralist medical practice rather than foundational knowledge.\nWe also compared the performance of ChatGPT on these\nexaminations to the performances of 2 previously mentioned\nLLMs, GPT-3 and InstructGPT. In addition, to further assess\nthe ability of ChatGPT to serve as a simulated medical tutor,\nwe qualitatively examined the integrity of ChatGPT’s responses\nwith regard to logical justification and the use of intrinsic and\nextrinsic information.\nMethods\nMedical Education Data Sets\nWe created 2 pairs of data sets to examine ChatGPT’s\nunderstanding of medical knowledge related to Step 1 and Step\n2. We first selected a subset of 100 questions from AMBOSS,\na widely used question bank that contains over 2700 Step 1 and\n3150 Step 2 questions [15]. The existing performance statistics\nfrom previous AMBOSS users allows us to determine the\nrelative performance of the model. We call these data sets\nAMBOSS-Step1 and AMBOSS-Step2. AMBOSS provides users\nwith an Attending Tip when they have difficulty with a question,\nas well as a difficulty rating (1-5). We included a second\ninstance of each question including these tips in our data set to\nJMIR Med Educ 2023 | vol. 9 | e45312 | p. 2https://mededu.jmir.org/2023/1/e45312\n(page number not for citation purposes)\nGilson et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\ndetermine if the additional context provided by the tip improves\nperformance.\nWe also used the list of 120 free Step 1 and Step 2 Clinical\nKnowledge questions developed by the National Board of\nMedical Examiners (NBME), which we call NBME-Free-Step1\nand NBME-Free-Step2, respectively, to evaluate ChatGPT’s\nperformance on the questions most closely aligned with those\nfrom the true licensure exams.\nPrompt Engineering\nDue to the significant impact that prompt engineering has been\nshown to have on generative LLM output, we standardized the\ninput formats of the AMBOSS and NBME data sets [16]. First,\nwe removed any questions that include an image, as ChatGPT\nonly accepts textual input. Next, we removed questions where\nthe answer was formatted as a table. This was done so that the\naccuracy of ChatGPT’s answers was solely dependent on its\nability to synthesize medical knowledge within narrative text\nrather than parsing complicated text inputs. Questions were\nformatted with the question text followed by the direct question\nseparated by a new line. In the AMBOSS data sets, the Attending\nTip was inserted as a separate instance of the question.\nFollowing the question text and direct question, the\nmultiple-choice answers were provided, separated again by a\nnew line. An example question prompt and response is shown\nin Figure 1.\nFigure 1. Template of question posed to each large language model (LLM), including both AMBOSS Attending Tip and the response from Chat\nGenerative Pre-trained Transformer (ChatGPT). The correct answer to this question is “E. Zidovudine (AZT).” In the case of GPT-3, prompt engineering\nwas necessary, with: \"Please answer this multiple choice question:\" + question as described previously + \"Correct answer is.\" As GPT-3 is inherently\na nondialogic model, this was necessary to reduce model hallucinations and force a clear answer [17].\nModel Testing\nWe first recorded all correct answers as they appeared in the\nAMBOSS and NBME data sets. All model testing was performed\non the December 15, 2022, version of ChatGPT by manually\nentering questions into the ChatGPT website. The OpenAI API\nwas used to query GPT-3 and InstructGPT using the davinci\nand text-davinci-003 models, respectively. We then prompted\nthe models with the standardized questions. We also further\nprompted ChatGPT with questions including the Attending Tip.\nAll responses were directly copied into a shared spreadsheet\nfor review. Due to the nature of each model’s output, we\nmanually reviewed each answer to determine which answer\nfrom the multiple-choice question was selected, if any.\nWe then qualified the ChatGPT responses for each question\nusing 3 binary variables characteristic of narrative coherence\n[18]. Without deeper linguistic analysis, these variables provide\na crude metric, assessing the following:\n1. Logical reasoning: The response clearly identifies the logic\nin selecting between answers given the information\npresented in the response.\n2. Internal information: The response uses information internal\nto the question, including information about the question\nin the response.\nJMIR Med Educ 2023 | vol. 9 | e45312 | p. 3https://mededu.jmir.org/2023/1/e45312\n(page number not for citation purposes)\nGilson et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\n3. External information: The response uses information\nexternal to the question, including but not limited to\nqualifying the answers given or the stem.\nFinally, for each question answered incorrectly, we labeled the\nreason for the incorrect answer as one of the following options:\n• Logical error: The response adequately found the pertinent\ninformation but did not properly convert the information\nto an answer.\n• Example: Identifies that a young woman has been\nhaving difficulty with taking pills routinely and still\nrecommends oral contraceptives over an intrauterine\ndevice.\n• Information error: ChatGPT either did not identify a key\npiece of information, whether present in the question stem\nor through external information, that would be considered\nexpected knowledge.\n• Example: Recommends antibiotics for sinusitis\ninfection, believing most cases to be of bacterial\netiology even when the majority are viral.\n• Statistical error: An error centered around an arithmetic\nmistake. This includes explicit errors, such as stating “1 +\n1 = 3,” or indirect errors, such as an incorrect estimation\nof disease prevalence.\n• Example: Identifies underlying nephrolithiasis but\nmisclassifies the prevalence of different stone types.\nAll authors who performed qualitative analysis of the responses\n(AG, CWS, RAT, and DC) worked collaboratively, and all\nuncertain labels were reconciled.\nData Analysis\nAll analysis was conducted in Python software (version 3.10.2;\nPython Software Foundation). Unpaired chi-square tests were\nused to determine whether question difficulty significantly\naffected ChatGPT’s performance on the AMBOSS-Step1 and\nAMBOSS-Step2 data sets. Similarly, unpaired chi-square tests\nwere also used to evaluate the distribution of logical reasoning,\ninternal information, and external information between correct\nand incorrect responses in the NBME-Free-Step1 and\nNBME-Free-Step2 data sets.\nResults\nOverall Performance\nTable 1 shows the performance of 3 LLMs: ChatGPT, GPT-3,\nand InstructGPT, on the 4 data sets tested. Scores for AMBOSS\nmodels are shown when the Attending Tip was not used.\nChatGPT performed more accurately on Step 1 related questions\ncompared to Step 2 questions on both the NBME and AMBOSS\ndata sets: 64.4% (56/87) versus 57.8% (59/102) and 44%\n(44/100) versus 42% (42/100), respectively. Furthermore, the\nmodel performed better on NBME questions when compared\nto AMBOSS questions, for both Step 1 and Step 2: 64.4% (56/87)\nversus 44% (44/100) and 57.8% (59/102) versus 42% (42/100),\nrespectively. ChatGPT outperformed both GPT-3 and\nInstructGPT on all data sets. InstructGPT was outperformed by\n8.15% on average, whereas GPT-3 performed similarly to\nrandom chance on all question sets.\nTable 1. The performance of the 3 large language models (LLMs) on the 4 outlined data sets.\nAMBOSS-Step2 (n=100),\nn (%)\nAMBOSS-Step1 (n=100),\nn (%)\nNBME-Free-Step2\n(n=102), n (%)\nNBMEa-Free-Step1\n(n=87), n (%)\nLLM, response\nChatGPTb\n42 (42)44 (44)59 (57.8)56 (64.4)Correct\n58 (58)56 (56)43 (42.2)31 (35.6)Incorrect\nInstructGPT\n35 (35)36 (36)54 (52.9)45 (51.7)Correct\n65 (65)64 (64)48 (47.1)42 (48.3)Incorrect\nGPT-3\n17 (17)20 (20)19 (18.6)22 (25.3)Correct\n83 (83)80 (80)83 (81.4)65 (74.7)Incorrect\naNBME: National Board of Medical Examiners.\nbChatGPT: Chat Generative Pre-trained Transformer.\nQuestion Difficulty and Model Accuracy\nFrom Table 2, relative to AMBOSS users as reported on the\nafter-test summary, ChatGPT was in the 30th percentile on Step\n1 questions without the Attending Tip and the 66th percentile\non Step 1 questions with the Attending Tip. On the Step 2\nAMBOSS data set with and without the Attending Tip, the model\nperformed at the 20th and 48th percentiles, respectively. On\nStep 1 questions without the Attending Tip, ChatGPT had a\nsignificant decrease in accuracy as the AMBOSS-reported\ndifficulty increased (P=.01), falling from 64% (9/14) accuracy\non level 1 questions to 0% (0/9) accuracy on level 5 questions.\nThe remaining groups were monotonically decreasing in\naccuracy as question difficulty increased, except for questions\nwith difficulty 2 versus 3 for Step 1 with the Attending Tip and\nJMIR Med Educ 2023 | vol. 9 | e45312 | p. 4https://mededu.jmir.org/2023/1/e45312\n(page number not for citation purposes)\nGilson et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nquestions with difficulty 4 versus 5 for Step 2 without the Attending Tip.\nTable 2. ChatGPT’sa performance on AMBOSS-Step1 and AMBOSS-Step2 data sets by question.\nP valueQuestion difficulty, n (%)Overall, n (%)Step, tip, response\n54321\nStep 1 (overall: n=100; difficulty 1: n=14; difficulty 2: n=27; difficulty 3: n=32; difficulty 4: n=18; difficulty 5: n=9)\nWithout Attending Tip\n.010 (0)6 (33.3)13 (40.6)16 (59.3)9 (64.3)44 (44)Correct\n9 (100)12 (66.7)19 (59.4)11 (40.7)5 (35.7)56 (56)Incorrect\nWith Attending Tip\n.062 (22.2)7 (38.9)21 (65.6)16 (59.3)10 (71.4)56 (56)Correct\n7 (77.8)11 (61.1)11 (34.4)11 (40.7)4 (28.6)44 (44)Incorrect\nStep 2 (overall: n=100; difficulty 1: n=25; difficulty 2: n=23; difficulty 3: n=27; difficulty 4: n=16; difficulty 5: n=9)\nWithout Attending Tip\n.133 (33.3)3 (18.8)11 (40.7)10 (43.5)15 (60)42 (42)Correct\n6 (66.7)13 (81.2)16 (59.3)13 (56.5)10 (40)58 (58)Incorrect\nWith Attending Tip\n.082 (22.2)7 (43.8)12 (44.4)15 (65.2)17 (68)53 (53)Correct\n7 (77.8)9 (56.2)15 (55.6)8 (34.8)8 (32)47 (47)Incorrect\naChatGPT: Chat Generative Pre-Trained Transformer.\nQualitative Breakdown of Responses\nFinally, in Table 3, we evaluated ChatGPT’s answer quality\nacross 3 metrics as outlined above: presence of logical\nreasoning, internal information, and external information. We\nfound that every response provided by ChatGPT provided a\nlogical explanation of its answer selection, independent of the\ncorrectness of the response. Additionally, across both\nNBME-Free-Step1 and NBME-Free-Step2 data sets, for both\ncorrect and incorrect responses, ChatGPT used information\ninternal to the question in 96.8% (183/189) of questions. There\nwas no significant difference between the presence of internal\ninformation between correct or incorrect responses for either\nStep 1 or Step 2 data sets (P=.25 and P=.07, respectively).\nFinally, information external to the question was used in 92.9%\n(52/56) of correct responses and 48.4% (15/31) of incorrect\nresponses for the Step 1 data set (difference of 44.5%; P<.001).\nFor the Step 2 data set, external information was used in 89.8%\n(53/59) of correct answers and 62.8% (27/43) of incorrect\nanswers (difference of 27%; P=.001). For both Step 1 and Step\n2, logical errors were the most common, followed by\ninformation errors. Few statistical errors were present for either\ndata set.\nJMIR Med Educ 2023 | vol. 9 | e45312 | p. 5https://mededu.jmir.org/2023/1/e45312\n(page number not for citation purposes)\nGilson et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nTable 3. Qualitative analysis of ChatGPT’sa response quality for NBMEb-Free-Step1 and NBME-Free-Step2.\nNBME-Free-Step2NBME-Free-Step1Metric\nIncorrect\n(n=43), n (%)\nCorrect (n=59),\nn (%)\nOverall\n(n=102), n (%)\nIncorrect\n(n=31), n (%)\nCorrect (n=56),\nn (%)\nOverall (n=87),\nn (%)\nLogical reasoning\n43 (100)59 (100)102 (100.0)31 (100)56 (100)87 (100)True\n0 (0)0 (0)0 (0)0 (0)0 (0)0 (0)False\nInternal information\n40 (93)59 (100)99 (97.1)29 (93.5)55 (98.2)84 (96.6)True\n3 (7)0 (0)3 (2.9)2 (6.5)1 (1.8)3 (3.4)False\nExternal information\n27 (62.8)53 (89.8)80 (78.4)15 (48.4)52 (92.9)67 (77)True\n16 (37.2)6 (10.2)22 (21.6)16 (51.6)4 (7.1)20 (23)False\nReason for incorrect answer\n16 (37.2)——13 (41.9)——cLogical error\n13 (30.2)——7 (22.6)——Information error\n1 (2.3)——2 (6.5)——Statistical error\n13 (30.2)——9 (29)——Logical and information errors\naChatGPT: Chat Generative Pre-Trained Transformer.\nbNBME: National Board of Medical Examiners.\ncNot applicable.\nDiscussion\nPrincipal Findings\nOne of the key features touted by the advancement of ChatGPT\nis its ability to understand context and carry on a conversation\nthat is coherent and relevant to the topic at hand. In this paper,\nwe have shown that this extends into the medical domain by\nevaluating ChatGPT on 4 unique medical knowledge\ncompetency data sets, framing conversation as question\nanswering. We found that the model is capable of correctly\nanswering up to over 60% of questions representing topics\ncovered in the USMLE Step 1 and Step 2 licensing exams. A\nthreshold of 60% is often considered the benchmark passing\nstandards for both Step 1 and Step 2, indicating that ChatGPT\nperforms at the level expected of a third-year medical student.\nAdditionally, our results demonstrate that even in the case of\nincorrect answers, the responses provided by the model always\ncontained a logical explanation for the answer selection, and\ngreater than 90% of the time, this response directly included\ninformation contained in the question stem. Correct answers\nwere found to contain information external to the question stem\nsignificantly more frequently (given a threshold of P<.001 [19])\nthan incorrect responses, indicating that the ability of the model\nto correctly answer a question may be related to its ability to\nrelate the prompt to data within its armamentarium.\nPrior work in the field of medical question answering research\nhas often been focused on more specific tasks with the intent\nof improving model performance at the expense of\ngeneralizability. For example, Jin et al [20] achieved a 68.1%\naccuracy with their model that answers yes-or-no questions\nwhose answers may be found in the corpus of PubMed-available\nabstracts. Attempts at more generalizable models have been\nmet with more challenges. A different Jin et al [21] achieved\nan accuracy of 36.7% on a data set of 12,723 questions derived\nfrom Chinese medical licensing exams. Similarly, in 2019, Ha\net al [22] reported only a 29% accuracy on 454 USMLE Step\n1 and Step 2 questions. Expanding beyond simple\nquestion-answering tasks, ChatGPT therefore represents a\nsignificant step forward on 3 distinct fronts. First is\ngeneralizability, as ChatGPT is capable of responding to any\nquestion that can be formatted with text alone; the scope of\npossible questions is limited only by what can be submitted by\nthe user. The second front is accuracy. We have shown that\nChatGPT equals or outperforms prior models on questions of\nsimilar difficulty and content. Finally, ChatGPT marks the\ngreatest jump forward in user interpretability due to its\nconversational interface. Each response has some level of\nreasoning as we have demonstrated, and the ability to ask\nfollow-up questions allows the user to gain a larger perspective\non the concept being addressed in the question, rather than just\nan answer output alone.\nThis dialogic nature is what separates ChatGPT from previous\nmodels in its ability to act as an educational tool. InstructGPT\nperformed at an accuracy above random chance, although still\nbelow ChatGPT on all data sets. However, even if InstructGPT\nperformed at an accuracy equal to ChatGPT, the responses\nInstructGPT provided were not as conducive to student\neducation. InstructGPT’s responses were frequently only the\nselected answer with no further explanation, and it is impossible\nto ask follow-up questions to gain more context. As InstructGPT\nJMIR Med Educ 2023 | vol. 9 | e45312 | p. 6https://mededu.jmir.org/2023/1/e45312\n(page number not for citation purposes)\nGilson et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nis not formatted as a dialogic system, the model will often\ncontinue the prompt rather than provide a distinct answer. For\nexample, a prompt ending in “G) Delirium” will be extended\ninto “tremens B) Dislodged otoliths” before an answer is\nprovided. GPT-3 suffers from similar fallbacks and requires\nmore prompt engineering to generate the desired output [17].\nAdditionally, the model performed far below both ChatGPT\nand InstructGPT on all data sets.\nOne potential use case to highlight for the use of ChatGPT is\nas an adjunct or surrogate for small (peer) group education.\nSmall group education has been shown to be a highly efficacious\nmethod of teaching [23,24]. Specific examples of facilitating\nsmall group discourse in medical education include clinical\nproblem-solving by working through case presentations [25].\nSuch an approach to education is useful and independent of the\nknowledge of the students, as evidenced by small group\neducation starting as early as the first week after matriculation\nwithin the Yale System of Medical Education [26]. Rees et al\n[27] also demonstrated that students taught by peers do not have\nsignificantly different outcomes than students taught by faculty.\nAn aspect of small group education that is often beneficial is\nthe ability of students to test ideas off of each other and receive\nfeedback. With its dialogic interface, ChatGPT is able to provide\nmany of these same benefits for students when they are studying\nindependently. Students could use the tool to ask questions\nabout specific medical concepts, diagnoses, or treatments and\nreceive accurate and personalized responses to help them better\nstructure their knowledge around each concept. For example,\nauthor CWS provides the following reflection on his use of\nChatGPT while reviewing particularly challenging problems\nfrom a recent virology midterm. He found value in plugging\nquestions into ChatGPT and engaging with follow-up dialogue,\nbecause it could unearth context relevant to the question and\neffectively trigger recall for specific lectures that taught the\nmaterial relevant to the problem. This suggests that the context\nthat ChatGPT provides in an initial answer could open the door\nfor further questioning that naturally digs into the foundational\nknowledge required to justify the given underlying medical\nreasoning. Further studies are needed to evaluate the specific\nefficacy of ChatGPT for the simulation of small group\neducation, as well as other use cases that may be beneficial\n(such as the process of reflective learning) [28]. As the\ntechnology is further explored and improved, it is also possible\nthat novel educational methods may be developed that fully use\nthe capabilities of a tool such as ChatGPT.\nLimitations\nThis study has several limitations. First, ChatGPT was first\ntrained on a corpus that was created from data produced on or\nbefore 2021. This limits the model’s prompts to contain only\ninformation found prior to that date. Second, due to the closed\nnature of this model and the lack of a public API, we are unable\nto fine-tune this model on task-specific data and examine the\nbreadth of its inherent stochasticity. However, this work\ninvestigates ChatGPT’s in-context performance on USMLE\nStep 1 and 2 exams, so these limitations did not hamper our\nanalysis. Third, ChatGPT is receiving regular updates, believed\nto be the result of training on inputs as they are provided by\nusers. The version of ChatGPT used in this study was not the\nmost up-to-date model at the time of publication. All said, it is\nreasonable to hypothesize that each new iteration of the model\nwill not cause a significant reduction in model performance on\nthe task we have outlined and, in fact, may cause an increase\nin performance.\nConclusions\nIn conclusion, our results suggest that ChatGPT performs at a\nlevel expected of a third-year medical student on the assessment\nof the primary competency of medical knowledge. Furthermore,\nthe tool has potential as an innovation within the context of\nsmall group education in medicine (such as for problem-based\nlearning or clinical problem-solving). ChatGPT’s responses to\nquestions provide interpretable context to justify the written\nresponse of the model in most cases, suggesting narrative\ncoherence. By providing sufficiently accurate dialogic responses\nakin to human learners, the model may facilitate the creation\nof an on-demand, interactive learning environment for students,\nwhich has the potential to support problem-solving and\nexternally supported reflective practice.\nTo provide additional value to the reader as to the functioning\nof ChatGPT, a second conclusion to this paper was written by\nthe tool using the prompt “Write a conclusion for the following\nmanuscript:” with the entire text of the manuscript up to but\nexcluding the handwritten conclusion included in the prompt:\nIn conclusion, this study aimed to assess the\nperformance of the ChatGPT language model on\nexaminations which evaluate medical knowledge and\nreasoning, as well as its ability to serve as a virtual\nmedical tutor. The study used two pairs of datasets,\nAMBOSS-Step1 and AMBOSS-Step2, and\nNBME-Free-Step1 and NBME-Free-Step2, to examine\nChatGPT’s understanding of medical knowledge\nrelated to USMLE Step 1 and Step 2 Clinical\nKnowledge exams. The results of the study showed\nthat ChatGPT’s performance on the medical\nknowledge examination was superior to that of GPT-3\nand InstructGPT, and that the additional context\nprovided by the Attending Tip improved its\nperformance. Additionally, the study found that\nChatGPT’s responses were coherent in terms of\nlogical reasoning, use of internal information, and\nuse of external information. Overall, this study\nsuggests that ChatGPT has the potential to be used\nas a virtual medical tutor, but more research is\nneeded to further assess its performance and usability\nin this context.\nAcknowledgments\nThis publication was made possible by the Yale School of Medicine Fellowship for Medical Student Research. Research reported\nin this publication was supported by the National Institute of Diabetes and Digestive and Kidney Diseases of the National Institutes\nJMIR Med Educ 2023 | vol. 9 | e45312 | p. 7https://mededu.jmir.org/2023/1/e45312\n(page number not for citation purposes)\nGilson et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\nof Health under award number T35DK104689. The content is solely the responsibility of the authors and does not necessarily\nrepresent the official views of the National Institutes of Health.\nData Availability\nThe data sets analyzed during this study are available in Multimedia Appendix 1.\nConflicts of Interest\nNone declared.\nMultimedia Appendix 1\nSpreadsheet of all questions, annotations, and ChatGPT responses for all four datasets.\n[XLSX File (Microsoft Excel File), 677 KB-Multimedia Appendix 1]\nReferences\n1. OpenAI. ChatGPT: optimizing language models for dialogue. OpenAI. Nov 30, 2022. URL: https://openai.com/blog/chatgpt/\n[accessed 2022-12-22]\n2. Scott K. Microsoft teams up with OpenAI to exclusively license GPT-3 language model. The Official Microsoft Blog. Sep\n22, 2020. URL: https://blogs.microsoft.com/blog/2020/09/22/\nmicrosoft-teams-up-with-openai-to-exclusively-license-gpt-3-language-model/ [accessed 2022-12-19]\n3. Bowman E. A new AI chatbot might do your homework for you. but it's still not an A+ student. NPR. Dec 19, 2022. URL:\nhttps://www.npr.org/2022/12/19/1143912956/chatgpt-ai-chatbot-homework-academia [accessed 2022-12-19]\n4. How good is ChatGPT? The Economist. Dec 8, 2022. URL: https://www.economist.com/business/2022/12/08/\nhow-good-is-chatgpt [accessed 2022-12-20]\n5. Chambers A. Can Artificial Intelligence (Chat GPT) get a 7 on an SL Maths paper? IB Maths Resources from\nIntermathematics. Dec 11, 2022. URL: https://ibmathsresources.com/2022/12/11/\ncan-artificial-intelligence-chat-gpt-get-a-7-on-an-sl-maths-paper/ [accessed 2022-12-20]\n6. Brown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, et al. Language models are few-shot learners. arXiv..\nPreprint posted online on May 28, 2020. [doi: 10.48550/arXiv.2005.14165]\n7. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. arXiv.. Preprint posted\nonline May 17, 2017. [doi: 10.48550/arXiv.1706.03762]\n8. Dai Z, Yang Z, Yang Y, Carbonell J, Le QV, Salakhutdinov R. Transformer-XL: attentive language models beyond a\nfixed-length context. arXiv.. Preprint posted online June 2, 2019. [doi: 10.48550/arXiv.1901.02860]\n9. Radford A, Narasimhan K, Salimans T, Sutskever I. Improving language understanding by generative pre-training. Amazon\nAWS. URL: https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/\nlanguage_understanding_paper.pdf [accessed 2022-12-19]\n10. Keskar NS, McCann B, Varshney LR, Xiong C, Socher R. CTRL: a conditional transformer language model for controllable\ngeneration. arXiv.. Preprint posted online on September 20, 2019. [doi: 10.48550/arXiv.1909.05858]\n11. Das A, Selek S, Warner AR, Hu Y, Keloth VK, Li J, et al. Conversational bots for psychotherapy: a study of generative\ntransformer models using domain-specific dialogues. In: Proceedings of the 21st Workshop on Biomedical Language\nProcessing. Association for Computational Linguistics; 2022. Presented at: ACL 2022; May 26, 2022:285-297; Dublin,\nIreland. [doi: 10.18653/v1/2022.bionlp-1.27]\n12. Savery M, Abacha AB, Gayen S, Demner-Fushman D. Question-driven summarization of answers to consumer health\nquestions. Scientific Data. Oct 02, 2020;7(1):322. [FREE Full text] [doi: 10.1038/s41597-020-00667-z] [Medline: 33009402]\n13. Gutiérrez BJ, McNeal N, Washington C, Chen Y, Li L, Sun H, et al. Thinking about GPT-3 in-context learning for biomedical\nIE? think again. arXiv.. Preprint posted online on November 5, 2022. [doi: 10.48550/arXiv.2203.08410]\n14. Logé C, Ross E, Dadey DYA, Jain S, Saporta A, Ng AY, et al. Q-Pain: a question answering dataset to measure social bias\nin pain management. arXiv.. Preprint posted online on August 3, 2021. [doi: 10.48550/arXiv.2108.01764]\n15. The smarter way to learn and practice medicine. AMBOSS. URL: https://www.amboss.com/ [accessed 2022-12-21]\n16. Chen Y, Zhao C, Yu Z, McKeown K, He H. On the relation between sensitivity and accuracy in in-context learning. arXiv..\nPreprint posted online on September 16, 2022. [doi: 10.48550/arXiv.2209.07661]\n17. Moradi M, Blagec K, Haberl F, Samwald M. GPT-3 models are poor few-shot learners in the biomedical domain. arXiv..\nPreprint posted online on September 6, 2021. [doi: 10.48550/arXiv.2109.02555]\n18. Trabasso T. The development of coherence in narratives by understanding intentional action. Advances in Psychology.\n1991;79:297-314. [doi: 10.1016/s0166-4115(08)61559-9]\n19. Colquhoun D. The reproducibility of research and the misinterpretation of -values. R Soc Open Sci. Dec 2017;4(12):171085.\n[FREE Full text] [doi: 10.1098/rsos.171085] [Medline: 29308247]\nJMIR Med Educ 2023 | vol. 9 | e45312 | p. 8https://mededu.jmir.org/2023/1/e45312\n(page number not for citation purposes)\nGilson et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX\n20. Jin Q, Dhingra B, Liu Z, Cohen WW, Lu X. PubMedQA: a dataset for biomedical research question answering. arXiv..\nPreprint posted online on September 13, 2019. [doi: 10.48550/arXiv.1909.06146]\n21. Jin D, Pan E, Oufattole N, Weng WH, Fang H, Szolovits P. What disease does this patient have? a large-scale open domain\nquestion answering dataset from medical exams. Applied Sciences. Jul 12, 2021;11(14):6421. [doi: 10.3390/app11146421]\n22. Ha LA, Yaneva V. Automatic question answering for medical MCQs: can it go further than information retrieval? In:\nProceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019). 2019.\nPresented at: RANLP 2019; September 2-4, 2019:418-422; Varna, Bulgaria. [doi: 10.26615/978-954-452-056-4_049]\n23. Springer L, Stanne ME, Donovan SS. Effects of small-group learning on undergraduates in science, mathematics, engineering,\nand technology: a meta-analysis. Rev Educ Res. Jun 23, 2016;69(1):21-51. [doi: 10.3102/00346543069001021]\n24. Neville AJ, Norman GR. PBL in the undergraduate MD program at McMaster University: three iterations in three decades.\nAcad Med. Apr 2007;82(4):370-374. [doi: 10.1097/ACM.0b013e318033385d] [Medline: 17414193]\n25. Anspach RR. Notes on the sociology of medical discourse: the language of case presentation. J Health Soc Behav. Dec\n1988;29(4):357-375. [Medline: 3253326]\n26. Wang DC. The Yale System at 100 Years. Yale J Biol Med. Aug 31, 2020;93(3):441-451. [FREE Full text] [Medline:\n32874151]\n27. Rees EL, Quinn PJ, Davies B, Fotheringham V. How does peer teaching compare to faculty teaching? a systematic review\nand meta-analysis. Med Teach. Aug 2016;38(8):829-837. [doi: 10.3109/0142159X.2015.1112888] [Medline: 26613398]\n28. Sandars J. The use of reflection in medical education: AMEE Guide No. 44. Med Teach. Aug 2009;31(8):685-695. [doi:\n10.1080/01421590903050374] [Medline: 19811204]\nAbbreviations\nAPI: application programming interface\nChatGPT: Chat Generative Pre-trained Transformer\nLLM: large language model\nNBME: National Board of Medical Examiners\nUSMLE: United States Medical Licensing Examination\nEdited by T Leung; submitted 23.Dec.2022; peer-reviewed by I Wilson, C Meaney, B Meskó, K Roberts; comments to author 24.Jan.2023;\nrevised version received 27.Jan.2023; accepted 29.Jan.2023; published 08.Feb.2023\nPlease cite as:\nGilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, Chartash D\nHow Does ChatGPT Perform on the United States Medical Licensing Examination (USMLE)? The Implications of Large Language\nModels for Medical Education and Knowledge Assessment\nJMIR Med Educ 2023;9:e45312\nURL: https://mededu.jmir.org/2023/1/e45312\ndoi: 10.2196/45312\nPMID: 36753318\n©Aidan Gilson, Conrad W Safranek, Thomas Huang, Vimig Socrates, Ling Chi, Richard Andrew Taylor, David Chartash.\nOriginally published in JMIR Medical Education (https://mededu.jmir.org), 08.Feb.2023. This is an open-access article distributed\nunder the terms of the Creative Commons Attribution License (https://creativecommons.org/licenses/by/4.0/), which permits\nunrestricted use, distribution, and reproduction in any medium, provided the original work, first published in JMIR Medical\nEducation, is properly cited. The complete bibliographic information, a link to the original publication on https://mededu.jmir.org/,\nas well as this copyright and license information must be included.\nJMIR Med Educ 2023 | vol. 9 | e45312 | p. 9https://mededu.jmir.org/2023/1/e45312\n(page number not for citation purposes)\nGilson et alJMIR MEDICAL EDUCATION\nXSL•FO\nRenderX",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.642863929271698
    },
    {
      "name": "United States Medical Licensing Examination",
      "score": 0.5854576230049133
    },
    {
      "name": "Computer science",
      "score": 0.5590518712997437
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5363966226577759
    },
    {
      "name": "Medical education",
      "score": 0.3753146529197693
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36788302659988403
    },
    {
      "name": "Medical school",
      "score": 0.28754937648773193
    },
    {
      "name": "Medicine",
      "score": 0.26776158809661865
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}