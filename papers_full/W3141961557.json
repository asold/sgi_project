{
  "title": "Innovative Bert-Based Reranking Language Models for Speech Recognition",
  "url": "https://openalex.org/W3141961557",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4227459726",
      "name": "Chiu, Shih-Hsuan",
      "affiliations": [
        "National Taiwan Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2752626035",
      "name": "Chen, Berlin",
      "affiliations": [
        "National Taiwan Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6631362777",
    "https://openalex.org/W6678809451",
    "https://openalex.org/W3024308166",
    "https://openalex.org/W6770528390",
    "https://openalex.org/W1566877644",
    "https://openalex.org/W3014539864",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6713098461",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3015267417",
    "https://openalex.org/W2160451571",
    "https://openalex.org/W2747917286",
    "https://openalex.org/W2748092010",
    "https://openalex.org/W2889152503",
    "https://openalex.org/W1999965501",
    "https://openalex.org/W2134731454",
    "https://openalex.org/W2005733316",
    "https://openalex.org/W3015970933",
    "https://openalex.org/W1992475611",
    "https://openalex.org/W2158195707",
    "https://openalex.org/W3035317046",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W6607333740",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W4256161595",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2160815625",
    "https://openalex.org/W6712868603",
    "https://openalex.org/W2936202865",
    "https://openalex.org/W2747236259",
    "https://openalex.org/W3015484572",
    "https://openalex.org/W2891628540",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W2890817076",
    "https://openalex.org/W2125336414",
    "https://openalex.org/W2990391581",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2396464458",
    "https://openalex.org/W2399550240",
    "https://openalex.org/W330298975",
    "https://openalex.org/W179875071",
    "https://openalex.org/W2402268235",
    "https://openalex.org/W3013669377",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963341956"
  ],
  "abstract": "More recently, Bidirectional Encoder Representations from Transformers (BERT)\\nwas proposed and has achieved impressive success on many natural language\\nprocessing (NLP) tasks such as question answering and language understanding,\\ndue mainly to its effective pre-training then fine-tuning paradigm as well as\\nstrong local contextual modeling ability. In view of the above, this paper\\npresents a novel instantiation of the BERT-based contextualized language models\\n(LMs) for use in reranking of N-best hypotheses produced by automatic speech\\nrecognition (ASR). To this end, we frame N-best hypothesis reranking with BERT\\nas a prediction problem, which aims to predict the oracle hypothesis that has\\nthe lowest word error rate (WER) given the N-best hypotheses (denoted by\\nPBERT). In particular, we also explore to capitalize on task-specific global\\ntopic information in an unsupervised manner to assist PBERT in N-best\\nhypothesis reranking (denoted by TPBERT). Extensive experiments conducted on\\nthe AMI benchmark corpus demonstrate the effectiveness and feasibility of our\\nmethods in comparison to the conventional autoregressive models like the\\nrecurrent neural network (RNN) and a recently proposed method that employed\\nBERT to compute pseudo-log-likelihood (PLL) scores for N-best hypothesis\\nreranking.\\n",
  "full_text": "INNOVATIVE BERT-BASED RERANKING LANGUAGE MODELS FOR SPEECH RECOGNITION   Shih-Hsuan Chiu and Berlin Chen  National Taiwan Normal University, Taipei, Taiwan {shchiu, berlin}@ntnu.edu.tw   ABSTRACT  More recently, Bidirectional Encoder Representations from Transformers (BERT) was proposed and has achieved impressive success on many natural language processing (NLP) tasks such as question answering and language understanding, due mainly to its effective pre-training then fine-tuning paradigm as well as strong local contextual modeling ability. In view of the above, this paper presents a novel instantiation of the BERT-based contextualized language models (LMs) for use in reranking of N-best hypotheses produced by automatic speech recognition (ASR). To this end, we frame N-best hypothesis reranking with BERT as a prediction problem, which aims to predict the oracle hypothesis that has the lowest word error rate (WER) given the N-best hypotheses (denoted by PBERT). In particular, we also explore to capitalize on task-specific global topic information in an unsupervised manner to assist PBERT in N-best hypothesis reranking (denoted by TPBERT). Extensive experiments conducted on the AMI benchmark corpus demonstrate the effectiveness and feasibility of our methods in comparison to the conventional autoregressive models like the recurrent neural network (RNN) and a recently proposed method that employed BERT to compute pseudo-log-likelihood (PLL) scores for N-best hypothesis reranking.  Index Terms‚Äî automatic speech recognition, language models, BERT, N-best hypotheses reranking  1. INTRODUCTION  Due to the dramatic progress in automatic speech recognition (ASR) with various sophisticated deep neural network (DNN) modeling techniques, alongside the availability of large amounts of training data and powerful computational resources, there has been widespread adoption of ASR solutions in many daily life applications such as virtual assistants, smart speakers, interactive voice responses (IVR) and among others [1][2]. However, the performance of ASR \nin many real-world scenarios is still far from perfect [2][3][4]. To remedy this problem, a lightweight treatment is to rerank (or rescore) the N-best hypotheses generated by ASR with more elaborate autoregressive language models, such as high-order n-gram language models [5][6][7], the recurrent neural network (RNN) and its improvements with long short-term memory (LSTM) units [8][9][10][11], as well as extra linguistic or acoustic information clues. By doing so, we can avoid any modification of the modules of an ASR system and thus have fast experimental turnover. Notably, the family of RNN-based language models have shown significant and consistent superiority across many ASR hypothesis reranking tasks in relation to the conventional high-order n-gram language models, partly attributed to their better ability of capturing longer dependence relationships among words. In the recent past, a novel neural network-based contextualized language model, viz. Bidirectional Encoder Representations from Transformers (BERT) [12], has been proposed and achieved unprecedented success in a wide array of natural language processing (NLP) tasks such as question answering [13] and language understanding [14]. This can be attributed to its flexible pre-training then fine-tuning paradigm as well as excellent local contextual modeling ability. Nevertheless, as far as we are concerned, BERT has not been sufficiently and systematically studied in the context of ASR N-best hypothesis reranking. Building on these observations, this paper presents a novel instantiation of BERT-based contextualized language models for use in reranking of N-best hypotheses produced by ASR. To this end, we formulate N-best hypothesis reranking with BERT as a prediction problem, which aims to predict the oracle hypothesis that has the lowest word error rate (WER) given the N-best hypotheses as the input (denoted by PBERT). Notably, we also explore to leverage task-specific topic information in an unsupervised manner to supplement PBERT in the reranking process (denoted by TPBERT). To the best of our knowledge, TPBERT is the first to integrate unsupervised topic modeling, which captures the global topic information about the task of interest, into a BERT-based method for ASR hypothesis reranking. \n 2. RELATED WORK  In this section, we briefly review some representative approaches to N-best hypothesis reranking for ASR. Recently, along with the booming of various DNN modeling techniques developed in ASR, the RNN-based LM and its extensions (e.g., LSTM and bidirectional-LSTM [15][16]) have emerged as the de facto standard for ASR N-best hypothesis reranking [17][18]. In a follow-up study [19], the authors sought to perform on-the-fly adaptation of the RNN-based LM in an autoregressive manner by augmenting its input at each time stamp with a context-aware vector inferred from the partial word sequence before the current time stamp. In a similar vein, the work in [20] explored a topic modeling approach to extract topic features as additional inputs to the RNN-based LM for genre and topic adaptation on a multi-genre broadcast transcription task. Among other things, there have also been some research efforts devoted so as to modify the model training strategy [21], retrofit the internal model components of an LSTM-based LM [22], and perform ensemble modeling with a mixture of LSTM-based LMs [23]. More recently, an approach analogous to [23] was explored in [24], which replaced LSTM with vanilla Transformer for bidirectionally contextual modeling.  Although the aforementioned LM methods performs well in ASR N-best hypothesis reranking, their objectives of model training are all centered around next word prediction (namely, reading a list of words sequentially and predict an upcoming word at each time stamp) but are not closely linked to the reranking accuracy optimization. In this light, a discriminative modeling framework for N-best hypothesis reranking was proposed in [25], which consists of an LSTM-based encoder network followed by a binary classification network built on a fully-connected feedforward neural network (FNN).  \nWith the above background, in this paper we put forward a novel N-best hypothesis reranking framework, which has at least two noteworthy extensions to the existing state-of-the-art, mainstream methods. First, we leverage BERT as an encoder to simultaneously generate the semantic embeddings of all N-best hypotheses, instead of using LSTM to generate the embedding of each hypothesis in a one-by-one or pairwise manner. This way, the embedding of each hypothesis is a function of the entire N-best list, with both intra- and inter-hypothesis word-dependence relations (as well as sentence structures) taken into consideration. Second, the embeddings of all N-best hypotheses are then fed into a fully-connected FNN at the same time (instead of in a pairwise manner as [25]), while these embeddings can be further augmented with their corresponding task-specific topic representations as extra features to achieve better prediction of the hypothesis out from the N-best list that has the lowest WER.  3. PROPOSED RERANKING FRAMEWORK  3.1. Fundamentals of BERT  BERT [12] is a neural contextualized language model, which makes effective use of bi-directional self-attention (also called the Transformer [26]) to capture both short- and long-span contextual interaction between the tokens in its input sequences, usually in the form of words or word pieces. In contrast to the traditional embedding methods, the advantage of BERT is that it can produce different context-aware representations for the same word at different locations by considering bi-directional dependence relations of words across sentences. The training of BERT consists of two stages: pre-training and fine-tuning. At the pre-training stage, its model parameters can be estimated on huge volumes of unlabeled training data over different tasks such as the masked language model task and the next (relevant) sentence prediction task. At the fine-tuning stage, the pre-trained BERT model, stacked with an additional single- or multi-layer FNN, can be fine-tuned to work well on many NLP-related tasks when only a very limited amount of supervised task-specific training data is made available.  A bit more terminology: for the masked language model task conducted at the pre-training stage, given a token (e.g., word) sequence ùê±=[ùë•%,‚Ä¶,ùë•(], BERT constructs ùê±* by randomly replacing a proper portion of tokens in ùê± with a special symbol [MASK] for each of them, and designates the masked tokens collectively be ùê±+. Let ùêª- denotes a Transformer which maps a length-T token sequence x into a sequence of hidden vectors ùêª-(ùê±)=[ùêª-(ùê±)%,ùêª-(ùê±)0 ‚Ä¶\t,ùêª-(ùê±)(], then the pre-training objective function of BERT can be expressed by [12][27] max-log\tùëÉ9:;<(ùê±+|ùê±*,ùúÉ)‚âà@ùëöBlog(\nBC% ùëÉ9:;<(ùë•B|ùê±*,ùúÉ)  \n(1)  \n   Figure 1: A schematic depiction of PBERT for ASR N-best hypothesis reranking. \nButit‚ÄôsusefulBERTis usefulBirdis full\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm BERT\n[CLS] [SEP]\nFNN + softmax\n0.20.5‚Ä¶ ‚Ä¶ 0.1\nhyp1\nhyp2...hypN\n‚Ä¶\nPredicted Scores of Hypotheses\nwhere ùëöB is an indicator of whether the token at position\tùë° is masked or not.  3.2. BERT for ASR N-best Hypothesis Reranking  In this paper, we explore a novel use of BERT in the context of ASR N-best hypothesis reranking (denoted by PBERT), which the purpose of predicting a hypothesis that would have the lowest WER from an ASR N-best hypothesis list. More specifically, PBERT consists of two model components, namely BERT stacked with an additional fully-connected feedforward neural network (FNN), as depicted in Figure 1. For the list of N-best hypotheses that is fed into the BERT component, we add [CLS] at the beginning of each hypothesis (regarded as a sentence here) and [SEP] at the end of the hypothesis. In addition, as with the seminal paper of BERT, words involved in each hypothesis will be additionally encoded with a position vector to indicate their positions in the hypothesis, as well as a segment vector to indicate the change of hypotheses if needed. The intermediate goal of PBERT is to use each respective [CLS] vector output from the last layer of the BERT component as the semantic representation of a hypothesis, while all of these [CLS] vectors will be then fed in parallel into the FNN component to output a prediction score (with the softmax normalization) for each hypothesis. Given a set of training utterances, each of which is equipped with an N-best hypothesis list generated by ASR and the indication of the oracle hypothesis that has the lowest WER, we can train the FNN component and fin-tune the BERT component as well.  As a side note, the acoustic model score, language model score or their combination score for each hypothesis, obtained from ASR, can be concatenated together with the corresponding [CLS] vector of the hypothesis for feature augmentation.  \n3.3. Incorporation of Task-specific Topic Information into PBERT  To make the proposed PBERT more task-aware, we seek to add an additional model component to PBERT to empower it to further capture task-specific topic characteristics of N-best hypotheses, apart from the both short- and long-span word-level contextual interaction relations that has been already encoded by the BERT component. To this end, we employ probabilistic latent topic analysis (PLSA) [28], a celebrated unsupervised topic modeling method, to extract hypothesis-level topic representation pertaining to the ASR task of interest. In more precise terms, PLSA is first trained in an unsupervised manner with the transcripts of the training utterances so as to globally estimate a set of latent topic distributions that are relevant to the ASR task of interest. At test time, the task-specific topic representation of each hypothesis can be obtained through a simple folding-in process with the set of PLSA-based latent topic distributions estimated beforehand. Thereafter, we refer to this enhanced modeling method of PBERT as TPBERT, whose model structure is schematically depicted in Figure 2. Our TPBERT bears some resemblance to a recently proposed BERT-based model for semantic similarity detection [29]. We remark here that there have been some very recent studies that applied a vanilla BERT-based LM pretrained on a large dataset to score a sentence in terms of the so-called pseudo-log-likelihood (PLL) score, which can be in turn used in conjunction with other scores or embeddings to perform \n   Figure 3: A schematic depiction of MBERT for ASR N-best hypothesis reranking.   Table 1: Basic Statistics of the AMI Corpus.  Training Set Development Set Evaluation Set Hours 78 8.71 8.97 # Utterances 108,104 13,059 12,612  \n[MASK]is usefulBERT[MASK]usefulBERTis [MASK]\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm\nTrm BERT\nFNN + softmax\n[CLS] [SEP]\nBERTis useful\nŒ£\tlog\nPLL\tScore \n    Figure 2: A schematic depiction of TPBERT for ASR N-best hypothesis reranking.  \n\nreranking of the N-best hypotheses generated by machine translation (MT) or ASR [30][31]. The PLL score of a given sentence (or hypothesis) ùê±=[ùë•%,‚Ä¶,ùë•(]\tis defined by  PLL(ùê±)=‚àëlogùëÉ9:;<(ùë•B|ùê±\\IJ,ùúÉ\t)(BC%       (2) where ùê±\\IJ denotes the input sentence ùê± that has its word at time stamp ùë° be replaced with the special symbol [MASK]. We denote such a BERT-based scoring method as MBERT, which will be compared to our methods for ASR N-best hypothesis reranking in the next section.  4. EMPIRICAL EXPERIMENTS  4.1. Experimental setup  We evaluate our proposed approach on the AMI meeting transcription database and task [32], while all experiments are conducted using Kaldi toolkit [33]. For the AMI database, the speech corpus consisted of utterances collected with the individual headset microphones (IHM), while a pronunciation lexicon of 50K words was used. Table 1 shows some basic statistics of the AMI corpus for our experiments.  The ASR system employed in this paper was built on the hybrid DNN-HMM paradigm. For acoustic modeling, our recipe was to first estimate a GMM-HMM acoustic model on the training utterances with their corresponding orthographic transcripts, and in turn used the prior distribution of the senones obtained from the GMM components, in conjunction with the LF-MMI training objective, to estimate the time-delay neural network (TDNN) acoustic model [34], which was taken as the seed model. The speech feature vectors were 40 MFCC coefficients extracted from frames of 25 msec length and 10 msec shift, spliced with 100-dimensional i-vectors for speaker adaptation of TDNN. On the other hand, the language model of the baseline ASR system was a trigram language model, which was trained on the transcripts of the AMI training utterances with Kneser-Ney (KN) [5] smoothing.  4.2. Experimental Results  In the first set of experiments, we assess the effectiveness of our proposed PBERT and TPBERT methods for ASR N-best reranking with two different settings, in comparison to the conventional LSTM and bidirectional-LSTM methods (denoted by LSTM and BLSTM for short, respectively) [15][16]. In Setting (I), we augment the embedding of each hypothesis with the ASR decoding score (a log-liner combination of the acoustic model and language model probability scores). In Setting (II), we first disentangle the decoding score into acoustic model probability score and language model probability score, and then augment the embedding of each hypothesis with these two scores as two additional dimensions. The corresponding results are shown \nin Table 2, where the WER result of the baseline ASR system is listed for reference. Inspection of Table 2 reveals three noteworthy points. First, PBERT(I) seems to perform at the same level as the LSTM, but does not compete with BLSTM. Second, TPBERT(I) stands out in performance, leading to a relative WER reduction of 2.3% over BLSTM. This also validates the utility of additional incorporation of task-specific topic into PBERT. Third, Setting (II), which disentangles the ASR decoding score into the acoustic model and language model probability scores, appears to worsen the performance when compared Setting (I). In the second set of experiments, we evaluate the performance of the recently proposed MBERT method [30] [31], so as to further confirm the superiority of our proposed TPBERT method. Note again here that, MBERT treats the vanilla BERT as a masked language model to calculate the PLL score of each hypothesis (cf. Section 3.3). Our implementation of MBERT follows the model configuration proposed by [31], with three different training settings: in Setting (A), MBERT simply makes use a publicly-available BERT model that were pretrained on 3.3-billion word tokens [12]; in Setting (B), the BERT model of Setting (A) is further fine-tuned with the ground-truth (reference) transcripts of training utterances; and in Setting (C), the BERT model of Setting (A) is further fine-tuned with the oracle hypotheses of training utterances. Three observations can be drawn from Table 3. First, MBERT(A) delivers a worse WER result than TPBERT(I) and performs slightly better than BLSTM. Second, MBERT(B) and MBERT(C) yield the same WER result, being on par with BLSTM. Third, fine-tuning of the \nTable 2: The WER results obtained by the proposed PBERT and TPBERT N-best reranking methods in comparison to that of the LSTM- and BLSTM-based reranking methods. Method WER (%) Baseline ASR 22.79 LSTM 21.33 BLSTM 20.98 PBERT(I) 21.27 PBERT(II) 21.69 TPBERT(I) 20.49 TPBERT(II) 21.02  Table 3: The WER results obtained by variants of the MBERT N-best reranking method. Method WER (%) MBERT(A) 20.88 MBERT(B) 20.98 MBERT(C)  20.98   \nBERT model bring no benefit to BERT. The above observations do verify the efficacy of TPBERT, which frames the ASR N-best rescoring as a prediction problem rather than a language model rescoring problem, such as MBERT that exploits the PLL scores.  5. CONCLUSION AND FUTURE WORK  In this paper, we have presented a novel and effective BERT-based method, namely TPBERT, for ASR N-best hypothesis reranking, which can additionally leverage task-specific topic information clues to assist the BERT model component in achieving better embeddings of N-best hypotheses. A series of empirical evaluations on a benchmark meeting transcription task indeed demonstrate the utility of TPBERT in comparison to some top-of-the-line methods. As to future work, we envisage to explore more sophisticated techniques for task-specific topic or knowledge modeling that can be tightly coupled with our method for use in ASR N-best hypothesis reranking, document ranking for spoken document retrieval, and among others [27][35]. We also plan to optimize the model components of our method with some feasible discriminative training criteria [36][37].  6. ACKNOWLEDGMENT   This research is supported in part by the Ministry of Science and Technology (MOST), Taiwan, under Grant Number MOST 109-2634-F-008-006- through Pervasive Artificial Intelligence Research (PAIR) Labs, Taiwan, and Grant Numbers MOST 108-2221-E-003-005-MY3 and MOST 109-2221-E-003- 020-MY3. Any findings and implications in the paper do not necessarily reflect those of the sponsors.  7. REFERENCES  [1] G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-R. Mohamed, N. Jaitly, A. Senior, V. Vanhoucke, P . Nguyen, T.N. Sainath, and B. Kingsbury, ‚ÄúDeep neural networks for acoustic modeling in speech recognition: The shared views of four research groups,‚Äù IEEE Signal Processing Magazine, vol. 29, no. 6, pp. 82‚Äì97, 2012. [2] D. Y u and L. Deng, Automatic speech recognition: A deep learning approach, Springer-Verlag: London, 2015. [3] J. Li, L. Deng, Y . Gong, and R. Haeb-Umbach, ‚ÄúAn overview of noise-robust automatic speech recognition,‚Äù IEEE/ACM Transactions on Audio, Speech, and Language Processing, vol. 22, no. 4, pp. 745‚Äì777, 2014. [4] T. Hori, S. Araki, T. Yoshioka, M. Fujimoto, S. Watanabe, T. Oba, A. Ogawa, K. Otsuka, D. Mikami, K. Kinoshita, T. Nakatani, A. Nakamura, and J. Yamato, ‚ÄúLow-latency real-time meeting recognition and understanding using distant microphones and omni-directional camera,‚Äù IEEE Transactions on Audio, Speech, and Language Processing, vol. 20, no. 2, pp. 499‚Äì513, 2012. \n[5] R. Kneser and H. Ney, ‚ÄúImproved backing-off for m-gram language modeling,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 181-184, 1995.  [6] S. F. Chen and J. Goodman, ‚ÄúAn empirical study of smoothing techniques for language modeling,‚Äù Computer Speech & Language, vol. 13, no. 4, pp. 359‚Äì394, 1999.  [7] J. T. Goodman, ‚ÄúA bit of progress in language modeling,‚Äù Computer Speech & Language, vol. 15, no. 4, pp. 403‚Äì434, 2001.  [8] T. Mikolov, M. Karafiat, L. Burget, F. ¬¥ Cernock Àá y, and S. Khu- ` danpur, ‚ÄúRecurrent neural network based language model,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2010. [9] Toma¬¥s Mikolov, Stefan Kombrink, Luk Àá a¬¥s Burget, Jan Àá Cernock Àá y, and ` Sanjeev Khudanpur, ‚ÄúExtensions of recurrent neural network language model,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5528‚Äì5531, 2011. [10] S. Hochreiter and J. Schmidhuber, ‚ÄúLong short-term memory,‚Äù Neural Computing, vol. 9, no. 8, pp. 1735‚Äì1780, 1997. [11] M. Sundermeyer, R. Schluter, and H. Ney, ‚ÄúLSTM neural networks for language modeling,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2012. [12] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. ‚ÄúBERT: Pre-training of deep bidirectional transformers for language understanding. In in Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2019. [13] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, ‚ÄúSquad: 100,000+ questions for machine comprehension of text,‚Äù arXiv preprint, arXiv:1606.05250, 2016. [14] C.-W. Huang and Y.-N. Chen, ‚ÄúLearning ASR-robust contextualized embeddings for spoken language understanding,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.  [15] E. Arisoy, A. Sethy, B. Ramabhadran, and S. Chen, ‚ÄúBidirectional recurrent neural network language models for automatic speech recognition,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5421‚Äì5425, 2015. [16] X. Chen, A. Ragni, X. Liu, and M. Gales, ‚ÄúInvestigating bidirectional recurrent neural network language models for speech recognition.,‚Äù in Proc. ICSA INTERSPEECH, 2017. [17] Mittul Singh, Youssef Oualil, and Dietrich Klakow, ‚ÄúApproximated and domain-adapted lstm language models for first-pass decoding in speech recognition.,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2017.  [18] K. Li, H. Xu, Y. Wang, D. Povey, and S. Khudanpur, ‚ÄúRecurrent neural network language model adaptation for conversational \nspeech recognition.,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2018. [19] Tomas Mikolov and Geoffrey Zweig, ‚ÄúContext dependent recurrent neural network language model,‚Äù in Proceedings of the IEEE Spoken Language Technology Workshop (SLT), 2012. [20] X. Chen, T. Tan, X. Liu, P. Lanchantin, M. Wan, M. Gales, and P. C. Woodland, ‚ÄúRecurrent neural network language model adaptation for multi-genre broadcast speech recognition,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2015. [21] M. Ma, M. Nirschl, F. Biadsy, and S. Kumar, ‚ÄúApproaches for neural-network language model adaptation.,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2017. [22] M. W. Y . Lam, X. Chen, S. Hu, J. Yu, X. Liu, and H. Meng, ‚ÄúGaussian process LSTM recurrent neural network language models for speech recognition,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 7235‚Äì7239, 2019. [23] K. Irie, S. Kumar, M. Nirschl, and H. Liao, ‚ÄúRADMM: Recurrent adaptive mixture model with applications to domain robust language modeling,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2018. [24] K. Li, Z. Liu, T. He, H. Huang, F. Peng, D. Povey, S. Khudanpur, ‚ÄúAn Empirical Study of Transformer-Based Neural Language Model Adaptation‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. [25] A. Ogawa, M. Delcroix, S. Karita, and T. Nakatani, ‚ÄúRescoring N-best speech recognition list based on one-on-one hypothesis comparison using encoder-classifier model,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 6099‚Äì6103, 2018. [26] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, ‚ÄúAttention is all you need,‚Äù Proceedings of the International Conference on Neural Information Processing Systems (NIPS), pp. 6000‚Äì6010, 2017. [27] S. W. Fan-Jiang, T. H. Lo, B.Chen, ‚ÄúSpoken document retrieval leveraging BERT-based modeling and query reformulation,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.  [28] T. Hoffmann, ‚ÄúUnsupervised Learning by Probabilistic Latent Semantic Analysis,‚Äù Machine Learning, vol. 42, pp. 177‚Äì196, 2001. [29] N.Peinelt, D. Nguyen, and M. Liakata, ‚ÄútBERT: Topic models and BERT joining forces for semantic similarity detection,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association (ACL), pp. 7047‚Äì7055, 2020. [30] J. Shin, Y. Lee, and K. Jung, ‚ÄúEffective sentence scoring method using BERT for speech recognition,‚Äù in Proceedings of The Asian Conference on Machine Learning (ACML), pp. 1081‚Äì1093, 2019. \n[31] J. Salazar, D. Liang, T. Q Nguyen, and K. Kirchhoff, ‚ÄúMasked language model scoring,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association (ACL), pp. 2699‚Äì2712, 2020. [32] J. Carletta, S. Ashby, and S. Bourban et al., ‚ÄúThe AMI meeting corpus: A pre-announcement,‚Äù in Proceedings of the International Workshop on Machine Learning for Multimodal Interaction, pp. 28‚Äì39, 2005.  [33] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek, N. Goel, M. Hannemann, P . Motlicek, Y . Qian, P . Schwarz, J. Silovsky, G. Stemmer, and K. Vesely. The Kaldi Speech Recognition Toolkit. in Proceedings of the IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU), 2011.  [34] D. Povey, V. Peddinti, D. Galvez, P. Ghahrmani, V. Manohar, X. Na, Y. Wang, S. Khudanpur, ‚ÄúPurely sequence-trained neural networks for ASR Based on Lattice-Free MMI,‚Äù in Proceedings of the Annual Conference of the International Speech Communication Association (Interspeech), 2016 [35] S.-H. Liu, K.-Y. Chen, B. Chen, ‚ÄúEnhanced language modeling with proximity and sentence relatedness information for extractive broadcast news summarization,\" ACM Transactions on Asian and Low-Resource Language Information Processing, vol. 19, no. 3, Article 46: 1-19, 2020. [36] Y.  Ta c hioka and S. Watanabe, ‚ÄúA discriminative method for recurrent neural network language models,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5386‚Äì5389, 2015.  [37] T. Hori, C. Hori, S. Watanabe, and J. R. Hershey, ‚ÄúMinimum word error training of long short-term memory recurrent neural network language models for speech recognition,‚Äù in Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pp. 5990‚Äì5994, 2016. ",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8442381620407104
    },
    {
      "name": "Language model",
      "score": 0.8152899146080017
    },
    {
      "name": "Oracle",
      "score": 0.6486882567405701
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5983330607414246
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5863651037216187
    },
    {
      "name": "Word error rate",
      "score": 0.5258280038833618
    },
    {
      "name": "Transformer",
      "score": 0.512748122215271
    },
    {
      "name": "Natural language processing",
      "score": 0.5109283328056335
    },
    {
      "name": "Encoder",
      "score": 0.49889540672302246
    },
    {
      "name": "Task (project management)",
      "score": 0.4708544611930847
    },
    {
      "name": "Natural language understanding",
      "score": 0.46330079436302185
    },
    {
      "name": "Speech recognition",
      "score": 0.45359840989112854
    },
    {
      "name": "Question answering",
      "score": 0.45137232542037964
    },
    {
      "name": "Frame (networking)",
      "score": 0.4138130247592926
    },
    {
      "name": "Natural language",
      "score": 0.3980730175971985
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Software engineering",
      "score": 0.0
    }
  ],
  "topic": "Computer science",
  "institutions": [
    {
      "id": "https://openalex.org/I134161618",
      "name": "National Taiwan Normal University",
      "country": "TW"
    }
  ],
  "cited_by": 36
}