{
    "title": "Re-ViLM: Retrieval-Augmented Visual Language Model for Zero and Few-Shot Image Captioning",
    "url": "https://openalex.org/W4389520529",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2222316594",
            "name": "Zhuolin Yang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1964369109",
            "name": "Wei Ping",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2111492569",
            "name": "Zihan Liu",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Vijay Korthikanti",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2162609287",
            "name": "Weili Nie",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2634375912",
            "name": "De-An Huang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2267218070",
            "name": "Linxi Fan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2131575858",
            "name": "Zhiding Yu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2095617962",
            "name": "Shiyi Lan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2045831236",
            "name": "Bo Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1842149474",
            "name": "Mohammad Shoeybi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2341459302",
            "name": "Ming-Yu Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2128865582",
            "name": "Yuke Zhu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2688031072",
            "name": "Bryan Catanzaro",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2778191936",
            "name": "Chaowei Xiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3164401859",
            "name": "Anima Anandkumar",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4225323055",
        "https://openalex.org/W3176641147",
        "https://openalex.org/W4287113019",
        "https://openalex.org/W4292945941",
        "https://openalex.org/W4312605942",
        "https://openalex.org/W3170863103",
        "https://openalex.org/W4385573483",
        "https://openalex.org/W4320458302",
        "https://openalex.org/W2886641317",
        "https://openalex.org/W4309953147",
        "https://openalex.org/W2109586012",
        "https://openalex.org/W4312920106",
        "https://openalex.org/W4307106676",
        "https://openalex.org/W3195680250",
        "https://openalex.org/W3173220247",
        "https://openalex.org/W3154766321",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W3193402170",
        "https://openalex.org/W4296406182",
        "https://openalex.org/W2998702515",
        "https://openalex.org/W4320235125",
        "https://openalex.org/W3104279398",
        "https://openalex.org/W3167118264",
        "https://openalex.org/W3169064633",
        "https://openalex.org/W2745461083",
        "https://openalex.org/W1861492603",
        "https://openalex.org/W4229042118",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W4226182655",
        "https://openalex.org/W3099700870",
        "https://openalex.org/W4226279206",
        "https://openalex.org/W2997591391",
        "https://openalex.org/W1905882502",
        "https://openalex.org/W3007672467",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4312922092",
        "https://openalex.org/W2964616647"
    ],
    "abstract": "Zhuolin Yang, Wei Ping, Zihan Liu, Vijay Korthikanti, Weili Nie, De-An Huang, Linxi Fan, Zhiding Yu, Shiyi Lan, Bo Li, Mohammad Shoeybi, Ming-Yu Liu, Yuke Zhu, Bryan Catanzaro, Chaowei Xiao, Anima Anandkumar. Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 11844–11857\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nRe-ViLM: Retrieval-Augmented Visual Language Model for\nZero and Few-Shot Image Captioning\nZhuolin Yang*‡1 Wei Ping*2 Zihan Liu2 Vijay Korthikanti2 Weili Nie2\nDe-An Huang2 Linxi Fan2 Zhiding Yu2 Shiyi Lan2 Bo Li1 Mohammad Shoeybi2\nMing-Yu Liu2 Yuke Zhu2,3 Bryan Catanzaro†2 Chaowei Xiao†2,4 Anima Anandkumar†2,5\nAbstract\nAugmenting pretrained language models (LMs)\nwith a vision encoder (e.g., Flamingo) has\nobtained the state-of-the-art results in image-\nto-text generation. However, these models\nstore all the knowledge within their parame-\nters, thus often requiring enormous model pa-\nrameters to model the abundant visual concepts\nand very rich textual descriptions. Addition-\nally, they are inefficient in incorporating new\ndata, requiring a computational-expensive fine-\ntuning process. In this work, we introduce a\nRetrieval-augmented Visual Language Model,\nRe-ViLM, built upon the Flamingo, that sup-\nports retrieving the relevant knowledge from\nthe external database for zero and in-context\nfew-shot image-to-text generations. By stor-\ning certain knowledge explicitly in the external\ndatabase, our approach reduces the number of\nmodel parameters and can easily accommodate\nnew data during evaluation by simply updating\nthe database. We also construct an interleaved\nimage and text data that facilitates in-context\nfew-shot learning capabilities. We demonstrate\nthat Re-ViLM significantly boosts performance\nfor image-to-text generation tasks, especially\nfor zero-shot and few-shot generation in out-of-\ndomain settings with 4×less parameters com-\npared with baseline methods.\n1 Introduction\nImage-to-text generation, also known as image cap-\ntioning (e.g., Karpathy and Fei-Fei, 2015), plays a\nvital role in understanding visual information and\nenhancing human-AI interaction. This task has a\nwide range of practical applications such as gam-\ning, virtual reality, and robotics (Luo et al., 2019;\nZhao et al., 2021; Liu et al., 2021). To address this\n*Equal contribution. ‡Work done during an internship\nat NVIDIA. 1UIUC. 2NVIDIA. 3UT Austin. 4University\nof Wisconsin–Madison. 5California Institute of Technol-\nogy. †Equal advising. Correspondence to: Zhuolin Yang\n<zhuolin5@illinois.edu>, Wei Ping <wping@nvidia.com>\nproblem, numerous methods have been proposed\nrecently and obtained great success (e.g., Alayrac\net al., 2022; Wang et al., 2022c; Hu et al., 2022;\nChen et al., 2022b,a; Wang et al., 2022b).\nAmong them, visual language models (LMs)\nbuild on top of pretrained autoregressive LMs (e.g.,\nGPT-3, Brown et al., 2020), and inherit its powerful\ntext generation ability. In particular, the pretrained\nLM parameters are usually frozen and only some\ntrainable layers (e.g., adaptor) are added into the\nlarge LM during multimodal pretraining (Eichen-\nberg et al., 2022; Mokady et al., 2021; Tsimpoukelli\net al., 2021; Alayrac et al., 2022). This frozen LM\nstrategy can avoid catastrophic forgetting when the\nvisual LM is trained on ⟨image, text⟩data, where\nthe text quality is usually lower than the text-only\ncorpus to pretrain LM. In addition, it enables the\ncompelling zero-shot or few-shot capability of pre-\ntrained LM (e.g., Flamingo, Alayrac et al., 2022).\nHowever, such methods have two major lim-\nitations: 1) They store all acquired knowledge\nwithin the model parameters, making them param-\neter inefficient in modeling the abundant visual\nconcepts (e.g., uncommon objects) and rich textual\ndescriptions (e.g., alternative descriptions for the\nsame scene). 2) They are inefficient in incorporat-\ning new data, typically requiring computationally\nexpensive fine-tuning (Chen et al., 2022a) or per-\ntaining on increasingly more parameters and inter-\nleaved ⟨image, text⟩data (Alayrac et al., 2022).\nIn the past few years, retrieval-augmented\nLMs (Guu et al., 2020; Lewis et al., 2020;\nKarpukhin et al., 2020; Borgeaud et al., 2022)\nhave shown notable success in improving accu-\nracy while reducing model parameters by retrieving\nlarge-scale text corpus. Despite their success, there\nare several issues to be addressed before we apply\nretrieval technique to visual LMs for image-to-text\ngeneration: i) The visual LM needs to seamlessly\nretrieve and encode external knowledge at the be-\nginning of multi-modal pretraining. Otherwise, the\n11844\nFlamingo: \nA brown and black animal in a pool of water.\nRe-ViLM: \nA sea otter sitting on the beach.\nSea otter (Enhydra lutris) floating in \nwater holding a starfish.\nA sea otter and its young swimming \ntogether on their backs.\nThe small spiny crayfish Euastacus \ndangadi from the Coffs Harbour region.\nA critically endangered obey crayfish\n(Cambarus obeyensis) collected near \nMonterey. \nFlamingo: \nA picture of a couple of different types of \nanimals\nRe-ViLM: \nA picture of two crayfish with different colors.\nFlamingo: \nA cake sitting on top of a white plate.\nRe-ViLM: \nA strawberry cake sitting on a white plate.\nThe top view of the layered strawberry\ncake with strawberries on top of the \nwhipped cream topping in a pattern.\nHome bakery christmas cake with \nstrawberry on top.\nQuery Samples Generated Captions Retrieved Evidence\nFigure 1: Examples of input images and output captions from 2.1B Flamingo (re-implemented) and 2.4B Re-ViLM.\nRe-ViLM can utilize the retrieved captions to generate more informative and accurate captions.\npowerful pretrained autoregressive LM tends to ig-\nnore the poorly encoded external knowledge. ii) In\nmulti-modal datasets, there are cases where multi-\nple captions describing the same image (e.g., from\ndifferent annotators (Lin et al., 2014)), and multiple\nimages having the same caption (e.g., see images\nin Figure 3 in Appendix). Thus, simply performing\nstandard nearest neighbor retrieval tends to make\nthe model take a shortcut and copy-paste retrieval\nexamples during training. iii) Training the model\non large-scale interleaved ⟨image, text⟩dataset fa-\ncilitates few-shot learning capability (e.g., M3W in\nAlayrac et al. (2022)), but it is really expensive to\ncollect such dataset.\nIn this work, we propose a Retrieval-augmented\nVisual Language Model, Re-ViLM, which en-\nhances the state-of-the-art visual LM, Flamingo\nfor zero-shot and in-context few-shot image cap-\ntioning,1 by seamlessly incorporating a multimodal\nretriever and retrieval-augmented LM layers that\ncross-attend to a text encoder (see selected sam-\nples in Figure 1, model framework in Figure 2).\nSpecifically, we make the following contributions:\n1. In contrast to previous work, we initialize\nRe-ViLM with RETRO, a pretrained retrieval-\naugmented LM (Borgeaud et al., 2022), thus it\ncan seamlessly integrate the retrieval capability\n1Note that, no official implementation ofFlamingo is avail-\nable, which is trained on large-scale in-house dataset (Alayrac\net al., 2022). We re-implement the model on public available\ndataset.\nat the beginning of multimodal pretraining and\nresult in improved performance.\n2. We investigate the retrieval strategy to build the\nmultimodal retriever. At multimodal pretrain-\ning, we find the best performance is obtained by\nretrieving k-nearest neighbor captions based on\ncosine similarity between image CLIP (Radford\net al., 2021) embeddings, while circumventing\n“copy-and-paste” behavior in training by filtering\nout retrieved candidates with the same caption\nas the training instance.\n3. We construct both pretraining and evaluation\ndatasets consisting of interleaved ⟨image, text⟩\npairs for multimodal pretraining, using exist-\ning public datasets. This facilitates in-context\nlearning where few-shot examples are given as\ninterleaved ⟨image, text⟩pairs.\n4. We conduct extensive experiments for image-\nto-text generation under zero-shot, few-shot,\nand fine-tuning settings on various bench-\nmarks including MSCOCO (Lin et al., 2014),\nFlickr30k (Plummer et al., 2015), and No-\nCaps (Agrawal et al., 2019). Re-ViLM consis-\ntently outperforms the baseline Flamingo model\nacross all settings. The improvements are partic-\nularly notable in zero-shot and few-shot settings,\ne.g., our Re-ViLM can outperform the Flamingo\nmodel containing even 4×more parameters in\nzero-shot evaluation.\nWe organize the rest of the paper as follows. In § 2,\n11845\nwe discuss related work. We introduce Re-ViLM\nmodel in § 3 and multimodal dataset for pretraining\nand retrieval in § 4. We present our experimental\nresults in § 5 and conclude the paper in § 6.\n2 Related Work\nVisual Language Models Many recent work\ntackle the problem of generating text captions for\ngiven images (e.g., Wang et al., 2022c; Alayrac\net al., 2022; Aghajanyan et al., 2022; Wang et al.,\n2022b; Hu et al., 2022; Li et al., 2022; Chen et al.,\n2022b; Yu et al., 2022). Among these work, vi-\nsual language models (Tsimpoukelli et al., 2021;\nAlayrac et al., 2022) directly augment pretrained\nLMs with visual component, achieving strong re-\nsults in both zero-shot and few-shot generation.\nRetrieval-augmented ModelsRetrieval has been\nsuccessfully applied in various NLP tasks, in-\ncluding question answering (Guu et al., 2020;\nKarpukhin et al., 2020), autoregressive language\nmodeling (Borgeaud et al., 2022), and other\nknowledge-intensive tasks (e.g., Lewis et al., 2020).\nIn computer vision, retrieval has also been applied\nfor image recognition with long-tail distribution of\nclasses (Long et al., 2022). In this work, we apply\nretrieval for image captioning.\nRA-CM3 (Yasunaga et al., 2022) augments the\nCM3 backbone (Aghajanyan et al., 2022) with re-\ntrieval, which can perform both image-to-text gen-\neration and text-to-image synthesis. In contrast to\nour ReViLM, there are the following differences:\n1) We investigate the “copy-and-paste” behavior of\nretrieval-augmented model during training, and pro-\npose a simple filtering strategy during retrieval. In\ncontrast, RA-CM3 proposes a query-dropout strat-\negy that drops some tokens of the query caption\nused in retrieval. In our ablation study, we find our\nsimple strategy works better than the dropout reg-\nularization as shown in § 5.5.1. 2) Our Re-ViLM\nuses retrieval-augmented LM decoder layer with\ncross-attention module to attend to the retrieved\nsimilar captions. In contrast, RA-CM3 appends\nthe retrieved captions as the prefix context on the\ndecoder side. In our ablation study, we find ours\nare more effective, as shown in § 5.5.2. 3) For\nimage-to-text generation, RA-CM3 only provides\nthe result of 2-shot in-context image captioning\non MSCOCO (Lin et al., 2014). In contrast, we\nperform extensive evaluations of our Re-ViLM on\nvarious benchmarks under zero-shot, few-shot and\nfine-tuning settings.\n3 Re-ViLM Architecture\nIn this section, we begin by outlining the frame-\nwork of Re-ViLM. After that, we delve into the\ndetails of each component in depth.\n3.1 Framework\nWe illustrate our Re-ViLM framework for image\ncaptioning in Figure 2. It consists of three essential\ncomponents:\n• Image encoder begins with a pretrained vision\ntransformer from CLIP (Radford et al., 2021)\nto extract visual features from the input images.\nThese features are subsequently fed into a train-\nable perceiver resampler (Jaegle et al., 2021) to\nunify the image features into the textual repre-\nsentation used in the retrieval-augmented LM.\n• Retrieval-augmented LM is initialized with a\npretrained RETRO model (Borgeaud et al.,\n2022) to generate corresponding captions based\non the image features and retrieved evidence.\nAmong some LM layers, the module cross-\nattend the hidden representations from the im-\nage encoder or the shared text encoder. The\nbidirectional text encoder encodes the retrieved\ncaptions obtained from the multimodal retriever.\n• Multimodal retriever consists of a retrieval\ndatabase storing ⟨image, text ⟩pairs indexed\nby Faiss, a fast similarity search library (John-\nson et al., 2019) using their CLIP embeddings.\nGiven a query image, the retriever extracts its\nembedding using the CLIP-ViT module within\nthe image encoder. It then returns the top- k\n⟨image, text⟩pairs, measured by the cosine sim-\nilarity of embeddings between the query im-\nage and the retrieved images. After that, the\nretrieved captions are encoded by the LM to\ngenerate relevant captions.\nIn the following subsections, we provide more de-\ntails about each component of our model.\n3.2 Image Encoder\nTo fully leverage the existing pretrained model, we\ninitialize the image encoder with CLIP-ViT (Rad-\nford et al., 2021), a pretrained vision transformer\nthat processes images by dividing them into a grid\nof patches and then processing each patch with\na transformer encoder. In our experiments, we\nused two different sizes of the CLIP-ViT model:\nViT-B/32 and ViT-L/14. We freeze the CLIP-ViT\npart during multimodal pretraining to avoid catas-\ntrophic forgetting while making it trainable during\n11846\n3\nRetrieval\tdatabase\nText Encoder\nCLIP ViT\nEMBCAFFW[Q]\n[KV]\nATTNCA[Q]\n[KV]\nMultimodal Retriever\nPerceiver Resampler\nImage Encoder\nPrompt: A picture of ... \n FFW\nKey:Imageembedding\nValue:\n…\nGATED XTTN-DENSELM DecoderLayerRetrieval-Augmented LM\nInputImage\nxN\nFigure 2: The framework of Re-ViLM. The model first extracts CLIP embedding of the input image, and use it to\nretrieve similar image-text pairs from the database. Within some predetermined layers, the retrieval-augmented LM\nwill cross-attend the visual representation from the image encoder, and the textual representation from text encoder,\nwhich encodes the retrieved captions.\nfine-tuning for better results. We then use a per-\nceiver resampler to obtain fixed-length hidden rep-\nresentations from the CLIP-ViT token embeddings.\nThe perceiver is trainable during both the multi-\nmodal pretraining and future fine-tuning, allowing\nit to adapt the visual representations for the text\ndecoder and connects the two modalities.\n3.3 Multimodal Retriever\nBuilding Database. The retrieval database is\nbuilt upon a image-text paired dataset and is struc-\ntured as a key-value map, where the keys are CLIP\nViT-B/32 image embeddings, and the values are\nthe corresponding text descriptions. The database\nis indexed by Faiss library (Johnson et al., 2019).\nRetrieval Given an input query image I, we\nperform k-nearest neighbor retrieval with cosine\nsimilarity of embeddings between query image and\ndatabase images.2 The retrieval results are denoted\nas R(I) = {(i1, c1), ··· , (ik, ck)}, where ij and\ncj with j ∈[1, k] represents the retrieved image\nand caption, respectively.\nFiltering strategy There can be multiple cap-\ntions for the same image from different annota-\ntors in some datasets (e.g., MSCOCO (Lin et al.,\n2014)). In this case, these retrieved captions, which\nare highly correlated (even near-duplicate) with the\nground-truth caption, could give a false sense of\nretrieved evidence quality to the model, resulting\n2We also tried to use embeddings of query image and\ndatabase captions with the same CLIP model, and the empiri-\ncal results are very similar.\nin potentially degraded test performance due to\nthe discrepancy between the training and evalua-\ntion setting. To avoid it, we filter out the retrieved\nimage-text pairs if the retrieved image is identical\nto the query image I (e.g., i1 = I) during both\ntraining and inference.\nFurthermore, in the image-text datasets (e.g.,\nConceptual Captions (Sharma et al., 2018; Chang-\npinyo et al., 2021)), multiple images can have iden-\ntical captions. For example, one annotator may\nprovide the same caption or alt-text for similar im-\nages (see Figure 3 in Appendix for examples). If\nthe retrieved caption from database is the same as\nthe ground-truth caption during training, it will en-\ncourage the model to take a short path and simply\ncopy-and-paste the retrieved caption to the model\noutput, hindering the training of Re-ViLM. To ad-\ndress this issue, we employ a filtering strategy that\nfilters out the retrieved image-text pair if its text\nis identical to the training image’s caption that is\nused as the teacher-forced input at the LM decoder\nlayer. Thus, if the corresponding ground-truth cap-\ntion of I is C, the filtered retrieval results R(I) =\n{(i1, c1), ··· , (ik, ck) | cj ̸= C, ij ̸= I, ∀j ∈\n[1, k]}. In § 5.5.1, we show that Re-ViLM can be\nlargely improved with this simple filtering strategy.\n3.4 Retrieval-augmented LM\nTo facilitate the model using the retrieved captions,\nthe visual LM needs to seamlessly retrieve and en-\ncode the external knowledge at the beginning of\n11847\nmultimodal pretraining. Thus, we initialize our\ntext encoder and LM decoder layer with pretrained\nRETRO (Borgeaud et al., 2022), a state-of-the-\nart retrieval-augmented LM. In our experiments,\nwe use three different sizes of RETRO models:\nRETRObase with 148M parameters, RETROmedium\nwith 405M, and RETROlarge with 1.5B parameters.\nTo generate captions conditioned on visual input,\nwe interleave the LM decoder layers with gated\ncross-attention dense layers (gated xttn-dense) as in\nFlamingo (Alayrac et al., 2022), which take the out-\nput of perceiver resampler as the key and value for\ncross-attention. To incorporate the retrieved cap-\ntions as evidence, we interleave the LM decoder\nlayers with retrieval-augmentation layers, which\ntake text encoder output as key and value for cross-\nattention. Note that, the text encoder is shared\nacross retrieval-augmentation layers. We freeze the\nretrieval-augmented LM at multimodal pretraining,\nand make it trainable at fine-tuning.\nText encoderGiven the retrieved k-nearest neigh-\nbor captions, we use a transformer-based bidirec-\ntional encoder model, to obtain the hidden repre-\nsentations. Specifically, our text encoder shares\nthe subword embedding table with the LM de-\ncoder. We concatenate k embeddings along with\nthe length dimension to form the retrieval raw em-\nbedding tensor E ∈R(k×m)×d, where m is se-\nquence length, d is hidden dimension. After ap-\nplying transformer layers, the encoder output is\ncross-attended by the LM decoder layers. We ini-\ntialize the text encoder with a pretrained RETRO\nmodel (Borgeaud et al., 2022) instead of training\nfrom scratch to improve model performance.\n3.5 Trainable Modules at Pretraining and\nFinetuning\nSimilar to Flamingo, our training objective is to\nmaximize the conditional likelihood of the cap-\ntions given the images. At pretraining, we fol-\nlow Flamingo’s strategy by freezing the pretrained\ncomponents, including CLIP-ViT and retrieval-\naugmented LM, training only the perceiver resam-\npler from scratch. At finetuning, we unfreeze all\nthe pretrained components and increase image res-\nolution from 224 ×224 to 480 ×480, as suggested\nin (Alayrac et al., 2022). This has been shown to\nimprove overall performance.\n4 Multimodal Data for Pretraining and\nRetrieval\n4.1 Image-Text Pair Data\nIn this work, we pretrain our models using two\nmulti-modal datasets: 1) CC3M + CC12M +\nSBU, which consists of overall 15 million high-\nquality image-text pairs from the Conceptual\nDataset (Sharma et al., 2018; Changpinyo et al.,\n2021) and SBU Captions (Ordonez et al., 2011);\n2) COYO-700M (Byeon et al., 2022), which con-\ntains 747 million image-text pairs after filtering out\nlow-quality samples from a collection of 10 billion\nweb image-text sources. In our experiment, we find\nthat the high-quality captions are essential for pre-\ntraining in image captioning task. Thus, we further\nfilter out instances with irregular textual tokens or\nlow CLIP similarity scores between image and text,\nand obtain 104M high-quality image-text pairs. For\nsimplicity, we refer to the CC3M + CC12M + SBU\ndataset as CCS and the COYO-104M dataset as\nCOYO. At retrieval, we useCCS and COYOas the\nmain sources for our retrieval database, and utilize\nthe Faiss library (Johnson et al., 2019) to support\nfast similarity-based retrieval. It takes 241GB for\nstoring faiss index file and for each query, it takes\naround 50ms performing retrieval on our database.\n4.2 Interleaved Image-Text Data\nIn this subsection, we discuss our proposed pre-\ntraining method for enhancing the in-context few-\nshot ability of our model. In this scenario, the\nmodel needs to be highly conditioned on the pre-\nvious few-shot samples (image-text pairs) to effec-\ntively generate captions of test images. However,\nexisting multimodal models generally do not use\nmultiple image-text pairs as inputs for pretrain-\ning (Tsimpoukelli et al., 2021; Yasunaga et al.,\n2022). This makes the in-context few-shot learning\nat inference time challenging, as there is no such\nsupervision during pretraining. While Alayrac et al.\n(2022) built an in-house large-scale multimodal cor-\npora with interleaved images and text, collecting\nsuch a dataset is expensive.\nWe construct our image-text interleaved datasets\nusing publicly available image-text pair datasets.\nWe make the image-text pairs in each interleaved\nsample relevant, in order to explicitly teach the\nmodel how to condition on previous data samples\nfor generating the caption of the current image.\nOur interleaved dataset is constructed by using\nCCS. For each image-text pair (query) in CCS, we\n11848\nselect four relevant data pairs from the same corpus\nto construct each interleaved sample, which results\nin five data pairs for each interleaved sample. The\ndata selection process for each query consists of\ntwo steps. Step-1: We use L2 metric to measure\nthe distances between the CLIP embeddings of the\nquery image and the rest of images in CCS, and se-\nlect data pairs where the images have a normalized\ndistance score between 0.4 and 0.6 to the query\nimage.3 Step-2: To ensure the captions in an in-\nterleaved sample are similar, we further use CLIP\nembeddings to calculate the distances between the\nquery caption and the captions from the selected\ndata pairs. We pick the top-4 data pairs where the\ncaptions are the most similar to the query caption.\n5 Experiments\nIn this section, we evaluate the performance of Re-\nViLM under three different settings: zero-shot, few-\nshot and fine-tuning, on various image captioning\nbenchmarks. We aim to demonstrate the superiority\nof our retrieval augmentation technique in improv-\ning the quality and relevance of generated captions\nthrough retrieving relevant knowledge from exter-\nnal databases. We compare our results to several\nwidely-used image captioning models such as Enc-\nDec (Changpinyo et al., 2021), SimVLM (Wang\net al., 2022c), and Flamingo (Alayrac et al., 2022).\nThrough extensive evaluation of Re-ViLM, we con-\nclude that Re-ViLM is compelling under zero-shot\nand few-shot settings.\n5.1 Experimental setup\nEvaluation Dataset.We conduct our image cap-\ntioning evaluation on three multi-modal datasets:\n1) MSCOCO (Lin et al., 2014) is a dataset for\nimage captioning, object detection, and segmen-\ntation. We use the Karpathy split (Karpathy and\nFei-Fei, 2015), with 82k/5k/5k images for training,\nvalidation, and testing respectively. Each image\nis annotated with at most 5 human-generated cap-\ntions. 2) Flickr30k (Plummer et al., 2015) is a\nstandard benchmark for sentence-based image cap-\ntioning, which includes 29k/1k/1k images in its\nKarpathy split. 3) NoCapscontains 15k images\ncontaining nearly 400 additional novel classes to\n3Note that we eliminate the most similar images since\nthere are lots of near-duplicate images, e.g., they have very\nfew differences in terms of resizing, cropping, color, rotation,\nwatermark. In practice, we set thresholds [0.4, 0.6] to filter\nout these cases, increase the diversity of the data pairs in one\ninterleaved sample, and still make sure the selected images\nare relevant.\noriginal MSCOCO, which can be used to evaluate\nnovel object captioning performance after finetun-\ning on MSCOCO. For zero-shot setting, we focus\non the evaluation under MSCOCO and Flickr30k\ndatasets. For fine-tuning setting, we evaluate on\nMSCOCO, Flickr30k and NoCaps datasets. We\nconduct our few-shot experiments on MSCOCO\ndataset only to assess Re-ViLM’s generalization\nand adaptability. Throughout our experiments, we\nreport BLEU@4, CIDEr, and SPICE scores (Lin\net al., 2014) to measure the quality and relevance\nof the generated captions given input images.\nImplementation. We develop Re-ViLM with\ndifferent scales based on different size of\nCLIP-ViT and RETRO, named Re-ViLM base\n(ViT-B/32, RETRO-148M), Re-ViLMmedium(ViT-\nL/14, RETRO-410M) and Re-ViLMlarge(ViT-B/32,\nRETRO-1.5B). Compared to Flamingo model with\nthe same CLIP-ViT and comparable GPT-3 configu-\nration (Brown et al., 2020), Re-ViLM introduces up\nto 16% additional parameters while largely boost-\ning the performance. We build our model with\nMegatron-LM infrastructure to support large visual\nLM training and evaluation. We set global batch\nsize as 256 and use Adam optimizer at training. We\nuse beam search with beam size as 3, maximum\ngeneration length as 10 for inference. In our ex-\nperiments, we set the number of retrieved captions\nk = 2for Re-ViLM. We also evaluate Re-ViLM\nperformance with larger k = 5, 10, which indicates\ninsignificant improvements. Details can be found\nin Appendix B.\n5.2 Zero-shot Evaluation\nWe conduct zero-shot evaluation on MSCOCO and\nFlickr30k datasets. During pretraining, we include\nboth CCS and COYO as our retrieval database\nand report the best number among all different set-\ntings. Results are shown in Table 1. We find that,\nRe-ViLM could achieve significant boosts (around\n10.0 on CIDer score) compared to the Flamingo\nmodel, by introducing up to 16% additional param-\neters. Even Re-ViLMbase outperforms the largest\nSimVLM by a large margin. We leave the full\nresults containing Re-ViLM’s performance under\ndifferent pretraining and retrieval database combi-\nnation in Appendix A.\n5.3 Few-shot Evaluation\nWe evaluate the few-shot learning capability of\nRe-ViLM by pretraining it on the constructed in-\nterleaved CCS dataset, and evaluating it under the\n11849\nTable 1: Zero-shot evaluation results on MSCOCO, Flickr30k benchmarks, compared with different image captioning\nbaselines. We report BLEU@4, CIDer, SPICE scores for different methods. Note that MSCOCO, Flickr30k were\nexcluded from pretraining set in the following MSCOCO and Flickr30k results. We replicate Flamingo models with\nthe same image encoder and text decoder as Re-ViLM based on original paper.\nMethod Total Trainable MSCOCO karpathy testFlickr30k karpathy test\nparams. params. BLEU@4 CIDer CIDer SPICE\nVL-T5 (Cho et al., 2021) 224M 224M - 4.9 2.6 2.0\nUnfied VLP (Zhou et al., 2020) 122M 122M - - 24.9 7.2\nSimVLMbase(Wang et al., 2022c) - - 9.5 24.0 - -\nSimVLMlarge - - 10.5 24.9 - -\nSimVLMhuge ∼1.4B ∼1.4B 11.2 32.2 - -\nFlamingobase(re-impl) 364M 102M 12.4 39.6 42.2 7.9\nFlamingomedium(re-impl) 894M 233M 15.6 44.3 43.2 8.8\nFlamingolarge(re-impl) 2.1B 489M 16.5 49.2 46.4 9.4\nRe-ViLMbase 420M 158M 17.0 51.2 45.2 9.2\nRe-ViLMmedium 1.0B 347M 17.9 53.6 52.0 9.8\nRe-ViLMlarge 2.4B 806M 18.6 60.8 52.1 10.0\nTable 2: Few-shot evaluation results on MSCOCO benchmarks, compared with vanilla Flamingo models as our\nbaseline. We report BLEU@4, CIDer scores for different methods. We pretrain our Re-ViLM on constructed\nCCS interleaved dataset and evaluate on constructed COCO interleaved dataset respectively. We adoptCCS as our\nretrieval set during both pretraining and evaluate stage.\nMethod Total Trainable 2 shots 4 shots 8 shots\nparams. params. BLEU@4 CIDerBLEU@4 CIDerBLEU@4 CIDer\nFlamingo-3B (Alayrac et al., 2022)3.2B 1.3B - - - 85.0 - -\nFlamingo-9B 9.3B 1.6B - - - 93.1 - -\nFlamingobase(re-impl) 364M 102M 13.7 53.9 19.5 66.0 22.1 71.8\nRe-ViLMbase 420M 158M 14.8 60.1 20.8 72.2 21.8 72.6\nFlamingomedium(re-impl) 894M 233M 17.9 69.0 23.3 80.2 23.1 76.8\nRe-ViLMmedium 1.0B 347M 18.2 73.6 24.0 84.5 24.1 81.0\nFlamingolarge(re-impl) 2.1B 489M 18.2 71.6 25.7 89.2 26.3 89.1\nRe-ViLMlarge 2.4B 806M 18.4 77.2 25.5 90.5 26.2 90.2\nTable 3: Finetuning evaluation results on MSCOCO, Flickr30k, and NoCaps benchmarks, compared with different\nimage captioning baselines. Note that, for NoCaps, we finetune on MSCOCO karpathy train, following prior\nworks (Li et al., 2022), while some work mentioning this setting as zero-shot evaluation. We finetune our Re-\nViLM on MSCOCO/Flickr30k karpathy train split respectively for MSCOCO and Flick30k evaluation. We report\nBLEU@4, CIDer, SPICE scores for different methods.\nMethod Total MSCOCO karpathy testFlickr30k karpathy testNoCaps validation\nparams.BLEU@4 CIDer BLEU@4 SPICE CIDer SPICE\nEnc-Dec (Changpinyo et al., 2021)- - 110.9 - - 90.2 12.1\nVinVL (Zhang et al., 2021) - 38.2 129.3 - - 92.5 13.1\nVL-T5 (Cho et al., 2021) 172M 34.6 116.1 - - 4.4 5.3\nMetaLM (Hao et al., 2022) 545M 37.6 126.6 - - 58.7 8.6\nUnfied VLP (Zhou et al., 2020)122M 36.5 116.9 30.1 17.0 - -\nBUTD (Anderson et al., 2018) - 36.2 113.5 27.3 16.0 - -\nNBT (Lu et al., 2018) - 34.7 107.2 27.1 15.6 - -\nSimVLMhuge(Wang et al., 2022c)∼1.4B 40.6 143.3 - - 110.3 14.5\nBLIP (Li et al., 2022) 252M 38.6 129.7 - - 105.1 14.4\nBLIPCapFilt-L(Li et al., 2022) 252M 40.4 136.7 - - 113.2 14.8\nFlamingobase(re-impl) 364M 37.0 128.0 30.4 16.5 102.8 14.0\nFlamingomedium(re-impl) 894M 37.4 129.0 30.7 17.2 105.6 14.4\nFlamingolarge(re-impl) 2.1B 38.2 129.4 31.2 17.4 109.2 14.5\nRe-ViLMbase 420M 37.8 129.1 30.6 17.3 105.2 14.2\nRe-ViLMmedium 1.0B 38.2 131.2 31.0 17.5 106.8 14.4\nRe-ViLMlarge 2.4B 39.4 134.2 31.6 18.0 109.5 14.7\n11850\ninterleaved MSCOCO dataset, constructed by the\nsame process described in § 4.2, with {2, 4, 8}-\nshots. Results are shown in Table 2. While the sig-\nnificant improvements on {2, 4}-shots setting com-\npared with the comparable size Flamingo model are\nclearly observed, we notice that the retrieval aug-\nmentation benefits becomes less when the number\nof shots increases (i.e., 8-shot). This is not sur-\nprising as the few-shot in-domain examples from\nMSCOCO has more useful information to boost the\nmodel performance on MSCOCO, than the out-of-\ndomain samples from our retrieval database, CCS\nand COYO. As the number of in-domain exam-\nples increases, the benefit of retrieval from out-of-\ndomain examples becomes marginal.\n5.4 Fine-tuning Evaluation\nWe conduct fine-tuning evaluation of Re-ViLM on\nMSCOCO, Flickr30K and NoCaps benchmarks.\nFor evaluation on MSCOCO and Flickr30k, we\nfine-tune our pretrained Re-ViLM with smaller\nlearning rate and early-stop strategy on MSCOCO\nand Flickr30k dataset respectively. For NoCaps\nevaluation, we fine-tune our model on MSCOCO\ndataset, following prior works (Li et al., 2022).\nResults are shown in Table 3. We observe that Re-\nViLM still consistently outperforms Flamingo, al-\nthough the relative improvements becomes smaller\ncompared to the zero-shot and few-shot settings.\nWe leave the full results containing Re-ViLM’s per-\nformance under different pretraining and retrieval\ndatabase combination in Appendix A.\n5.5 Ablation Study\n5.5.1 Filtering during Retrieval\nThere could exist two different types of dupli-\ncation scenarios in multi-modal datasets: Same\nimage with multiple captions, which is com-\nmonly found in MSCOCO, Flickr30k and NoCaps\ndatasets, could lead to label leakage during training.\nMultiple images with identical caption, which\nis common in multimodal datasets such as Con-\nceptual Captions, as shown in Figure 3 in Ap-\npendix C. 4 Both of these duplication can lead to\na severe issue that Re-ViLM can simply copy and\npaste the retrieved captions to achieve100% match\nto the ground-truth captions. See Appendix C for\nmore in-depth discussion.\nTo mitigate these above issues, we develop a\n4We find that the ratio of identical captions in Conceptual\nCaptions can be as high as 15.7%.\nsimple filtering strategy that discards retrieved\nsamples that matches the training query image\nI or its caption C at training (i.e. R(I) =\n{(i1, c1), ··· , (ik, ck) | cj ̸= C, ij ̸= I, ∀j ∈\n[1, k]}). We notice that another concurrent work\nRA-CM3, has also proposed the query-dropout\nstrategy which mitigates such duplication issue by\nrandomly dropping out retrieved caption tokens\nbased on their similarity to the query image and text.\nWe conduct ablation study to compare our simple\nfiltering method with the query-dropout method.\nThe results, as shown in Table 4, indicates that\nour simple filtering strategy leads to consistent im-\nprovement in the performance of Re-ViLM, while\nthe query-dropout strategy achieves competitive but\nslightly worse results than simple filtering strategy.\nTable 4: Comparison between ReViLM with simple fil-\ntering strategy, query-dropout strategy, and without any\nfiltering method during retrieval. Models are pretrained\non CCS dataset, and evaluated on MSCOCO, Flickr30k\nunder zero-shot setting. We report B@4: BLEU@4, C:\nCIDer, S: SPICE scores for different methods.\nMethod MSCOCOFlickr30k\nB@4 C C S\nRe-ViLMbase[No filtering] 12.3 35.5 41.4 8.1\nRe-ViLMbase[Query-dropout]16.5 48.6 43.4 9.1\nRe-ViLMbase 17.0 51.2 45.2 9.2\nRe-ViLMmedium[No filtering]12.3 35.5 41.4 8.1\nRe-ViLMmedium[Query-dropout]17.5 52.1 50.5 9.6\nRe-ViLMmedium 17.9 53.6 52.0 9.8\n5.5.2 Retrieval Augmentation as In-Context\nPrepending\nOur Re-ViLM incorporate retrieved captions\nthrough the cross attention between the bidirec-\ntional text encoder and LM decoder layers. A con-\ncurrent work, RA-CM3, proposed an alternative\nretrieval augmentation method by appending the\nretrieved captions as the prefix context on decoder\nside as a simpler way to utilize retrieved evidence\nwithout introducing additional parameters. We in-\nvestigate this prompt-like augmentation method,\nand replicate it by appending the top 2 retrieved\nevidence as prefix during pretraining and inference\nof Flamingo model. We compare this retrieval aug-\nmentation method with our retrieval-augmented\nLM layer approach. The results are shown in in Ta-\nble 5. We can observe that our retrieval-augmented\ndesign has better zero-shot captioning performance\nthan the retrieval augmentation method in Ya-\nsunaga et al. (2022). It reveals the importance of\nour retrieval-based architecture design.\n11851\nTable 5: Comparison between different retrieval aug-\nmentation methods: retrieval-augmented LM layers (Re-\nViLM) and in-context prepending as prompt (Flamingo\n+ prepend), along with vanilla Flamingo model. Mod-\nels are pretrained on CCS dataset, and evaluated on\nMSCOCO, Flickr30k under zero-shot setting. We re-\nport B@4: BLEU@4, C: CIDer, S: SPICE scores for\ndifferent methods.\nMethod MSCOCOFlickr30k\nB@4 C C S\nFlamingobase 12.4 39.6 42.2 7.9\nFlamingobase+ prepend 13.4 43.4 43.5 8.2\nRe-ViLMbase 17.0 51.2 45.2 9.2\nFlamingomedium 15.6 44.3 43.2 8.8\nFlamingomedium+ prepend16.4 45.6 46.6 9.2\nRe-ViLMmedium 17.9 53.6 52.0 9.8\n6 Conclusion\nIn this work, we propose Re-ViLM, a retrieval-\naugmented image-to-text model, with strong zero-\nshot and few-shot image captioning results. Re-\nViLM, which is built on Flamingo, provides sub-\nstantial reduction in the number of parameters\nwhile obtaining compelling results across different\nsettings, as it does not need to store all knowledge\nwithin the parameters. We also propose a simple\nyet effective filtering strategy at retrieval to circum-\nvent the “copy-and-paste” behavior of retrieval-\naugmented model. Furthermore, we construct an in-\nterleaved image-text dataset for pretraining, which\nis crucial for in-context few-shot learning. Ex-\ntensive experiments on diverse image-captioning\ndatasets shows that Re-ViLM consistently outper-\nform the baseline Flamingo model across all set-\ntings. Additionally, we conduct experiments on\nfine-tuning settings and show promising results.\n7 Limitations\nIn this paper, we focus on exploring emergent\nzero-shot and in-context few-shot image caption-\ning. To achieve this, we designed our retrieval\naugmented model mainly based on the Flamingo\nframework (Alayrac et al., 2022), and leave the\napplication of our retrieval design to other image-\nto-text frameworks (Bao et al., 2021; Chen et al.,\n2022b; Wang et al., 2022a) as future work. Further-\nmore, since there is no official implementation of\nFlamingo and its training datasets, our framework\nis based on our reimplemented Flamingo, trained\non publicly available datasets and manually crafted\ninterleaved image-text datasets. Also from the scal-\ning perspective, comparable to GPT in the text-only\ndomain, one of the most important advantages of\nthe Flamingo-like model is scaling. In this study,\nwe haven’t been able to further scale Re-ViLM to\n80B to address the benefits from retrieval on large\nscale visual language models.\nReferences\nArmen Aghajanyan, Bernie Huang, Candace Ross,\nVladimir Karpukhin, Hu Xu, Naman Goyal, Dmytro\nOkhonko, Mandar Joshi, Gargi Ghosh, Mike Lewis,\net al. 2022. Cm3: A causal masked multi-\nmodal model of the internet. arXiv preprint\narXiv:2201.07520.\nHarsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen,\nRishabh Jain, Mark Johnson, Dhruv Batra, Devi\nParikh, Stefan Lee, and Peter Anderson. 2019. No-\ncaps: Novel object captioning at scale. In Proceed-\nings of the IEEE/CVF International Conference on\nComputer Vision, pages 8948–8957.\nJean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-\ntoine Miech, Iain Barr, Yana Hasson, Karel Lenc,\nArthur Mensch, Katie Millican, Malcolm Reynolds,\net al. 2022. Flamingo: A visual language model for\nfew-shot learning. In NeurIPS.\nPeter Anderson, Xiaodong He, Chris Buehler, Damien\nTeney, Mark Johnson, Stephen Gould, and Lei Zhang.\n2018. Bottom-up and top-down attention for image\ncaptioning and visual question answering. In Pro-\nceedings of the IEEE conference on computer vision\nand pattern recognition, pages 6077–6086.\nHangbo Bao, Li Dong, Songhao Piao, and Furu Wei.\n2021. Beit: Bert pre-training of image transformers.\narXiv preprint arXiv:2106.08254.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In ICML.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. NeurIPS.\nMinwoo Byeon, Beomhee Park, Haecheon Kim,\nSungjun Lee, Woonhyuk Baek, and Saehoon Kim.\n2022. Coyo-700m: Image-text pair dataset. https:\n//github.com/kakaobrain/coyo-dataset.\nSoravit Changpinyo, Piyush Sharma, Nan Ding, and\nRadu Soricut. 2021. Conceptual 12M: Pushing web-\nscale image-text pre-training to recognize long-tail\nvisual concepts. In CVPR.\nJun Chen, Han Guo, Kai Yi, Boyang Li, and Mohamed\nElhoseiny. 2022a. Visualgpt: Data-efficient adapta-\ntion of pretrained language models for image caption-\ning. In CVPR.\n11852\nXi Chen, Xiao Wang, Soravit Changpinyo, AJ Pier-\ngiovanni, Piotr Padlewski, Daniel Salz, Sebastian\nGoodman, Adam Grycner, Basil Mustafa, Lucas\nBeyer, et al. 2022b. Pali: A jointly-scaled mul-\ntilingual language-image model. arXiv preprint\narXiv:2209.06794.\nJaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. 2021.\nUnifying vision-and-language tasks via text genera-\ntion. In International Conference on Machine Learn-\ning, pages 1931–1942. PMLR.\nConstantin Eichenberg, Sidney Black, Samuel Wein-\nbach, Letitia Parcalabescu, and Anette Frank.\n2022. Magma–multimodal augmentation of gener-\native models through adapter-based finetuning. In\nEMNLP.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Mingwei Chang. 2020. Realm: Retrieval\naugmented language model pre-training. In ICML.\nYaru Hao, Haoyu Song, Li Dong, Shaohan Huang,\nZewen Chi, Wenhui Wang, Shuming Ma, and Furu\nWei. 2022. Language models are general-purpose\ninterfaces. arXiv preprint arXiv:2206.06336.\nXiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang,\nZicheng Liu, Yumao Lu, and Lijuan Wang. 2022.\nScaling up vision-language pre-training for image\ncaptioning. In Proceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition,\npages 17980–17989.\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol\nVinyals, Andrew Zisserman, and Joao Carreira. 2021.\nPerceiver: General perception with iterative attention.\nIn ICML.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep visual-\nsemantic alignments for generating image descrip-\ntions. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 3128–\n3137.\nVladimir Karpukhin, Barlas O˘guz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In EMNLP.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive NLP tasks. In NeurIPS.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation. In ICML.\nTsung-Yi Lin, Michael Maire, Serge Belongie, James\nHays, Pietro Perona, Deva Ramanan, Piotr Dollár,\nand C Lawrence Zitnick. 2014. Microsoft coco:\nCommon objects in context. In European confer-\nence on computer vision, pages 740–755. Springer.\nAn-An Liu, Yingchen Zhai, Ning Xu, Weizhi Nie, Wen-\nhui Li, and Yongdong Zhang. 2021. Region-aware\nimage captioning via interaction learning. IEEE\nTransactions on Circuits and Systems for Video Tech-\nnology.\nAlexander Long, Wei Yin, Thalaiyasingam Ajanthan,\nVu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair,\nChunhua Shen, and Anton van den Hengel. 2022.\nRetrieval augmented classification for long-tail vi-\nsual recognition. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recog-\nnition, pages 6959–6969.\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh.\n2018. Neural baby talk. In Proceedings of the IEEE\nconference on computer vision and pattern recogni-\ntion, pages 7219–7228.\nRen C Luo, Yu-Ting Hsu, Yu-Cheng Wen, and Huan-\nJun Ye. 2019. Visual image caption generation for\nservice robotics and industrial applications. In 2019\nIEEE International Conference on Industrial Cyber\nPhysical Systems (ICPS), pages 827–832. IEEE.\nRon Mokady, Amir Hertz, and Amit H Bermano. 2021.\nClipcap: Clip prefix for image captioning. arXiv\npreprint arXiv:2111.09734.\nVicente Ordonez, Girish Kulkarni, and Tamara Berg.\n2011. Im2text: Describing images using 1 million\ncaptioned photographs. Advances in neural informa-\ntion processing systems, 24.\nBryan A Plummer, Liwei Wang, Chris M Cervantes,\nJuan C Caicedo, Julia Hockenmaier, and Svetlana\nLazebnik. 2015. Flickr30k entities: Collecting\nregion-to-phrase correspondences for richer image-\nto-sentence models. In Proceedings of the IEEE\ninternational conference on computer vision, pages\n2641–2649.\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\net al. 2021. Learning transferable visual models from\nnatural language supervision. In ICML.\nPiyush Sharma, Nan Ding, Sebastian Goodman, and\nRadu Soricut. 2018. Conceptual captions: A cleaned,\nhypernymed, image alt-text dataset for automatic im-\nage captioning. In ACL.\nMaria Tsimpoukelli, Jacob L Menick, Serkan Cabi,\nSM Eslami, Oriol Vinyals, and Felix Hill. 2021. Mul-\ntimodal few-shot learning with frozen language mod-\nels. Advances in Neural Information Processing Sys-\ntems, 34:200–212.\n11853\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie\nLi, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. 2022a. Git: A generative image-to-text\ntransformer for vision and language. arXiv preprint\narXiv:2205.14100.\nWenhui Wang, Hangbo Bao, Li Dong, Johan\nBjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal,\nOwais Khan Mohammed, Saksham Singhal, Subhojit\nSom, et al. 2022b. Image as a foreign language: Beit\npretraining for all vision and vision-language tasks.\narXiv preprint arXiv:2208.10442.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai,\nYulia Tsvetkov, and Yuan Cao. 2022c. SimVLM:\nSimple visual language model pretraining with weak\nsupervision. In ICLR.\nMichihiro Yasunaga, Armen Aghajanyan, Weijia Shi,\nRich James, Jure Leskovec, Percy Liang, Mike Lewis,\nLuke Zettlemoyer, and Wen-tau Yih. 2022. Retrieval-\naugmented multimodal language modeling. arXiv\npreprint arXiv:2211.12561.\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Ye-\nung, Mojtaba Seyedhosseini, and Yonghui Wu. 2022.\nCoca: Contrastive captioners are image-text founda-\ntion models. arXiv preprint arXiv:2205.01917.\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei\nYang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jian-\nfeng Gao. 2021. Vinvl: Revisiting visual representa-\ntions in vision-language models. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and\nPattern Recognition, pages 5579–5588.\nRui Zhao, Zhenwei Shi, and Zhengxia Zou. 2021. High-\nresolution remote sensing image captioning based\non structured attention. IEEE Transactions on Geo-\nscience and Remote Sensing, 60:1–14.\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu,\nJason Corso, and Jianfeng Gao. 2020. Unified vision-\nlanguage pre-training for image captioning and vqa.\nIn Proceedings of the AAAI Conference on Artificial\nIntelligence, volume 34, pages 13041–13049.\n11854\nAppendix\nA Re-ViLM with Increasing Size of Pretraining and Retrieval Data\nIn this section, we present detailed results of Re-ViLM in both zero-shot and fine-tuning settings, as shown\nin Table 6, Table 7, and Table 8. The performance of Re-ViLM continually improves with the increasing\nsize of the pretraining set and retrieval database. As the retrieval database grows, Re-ViLM has a higher\nchance of obtaining more relevant evidence from the retriever, resulting in better performance.\nB Re-ViLM with Different Number of Retrieved Captions\nIn this section, we present detailed results of Re-ViLM with different number of retrieved captionsk in\nboth zero-shot and fine-tuning settings on MSCOCO dataset, as shown in Table 9. We can see that, while\nthe number of retrieved examples increases, there is no additional benefit in further improving our results\n(sometimes even worse). We hypothesis that this is due to i) the original RETRO LM (used to initialize\nRe-ViLM) is pretrained with k = 2, and ii) larger k may return captions that are less related to the test\nimage.\nC Multiple Images with The Same Caption\nIn this section, we delve into the issue of data duplication, where multiple images have the identical\ncaption. As illustrated in Figure 3, there are three distinct scenarios: #1) different views of the same event,\n#2) identical scenes with different poses, and #3) different scenes with the same description. In scenario\n1), the provided caption describes an event with a series of pictures taking at the same time, but it is not\nwell related to a specific picture, e.g., the first picture in this scenario. As a result, it is undesired to directly\ncopy the retrieved caption as the model output at both training and test stage. If the retrieved similar\nimage-text pair belongs to scenario 2) and 3), the caption can be useful even with direct copy-and-paste\nat test time. Specifically, for scenario 2), images depict the same scene (e.g. a woman holding a cell\nphone in an office) and should have highly correlated or even identical caption. However, if the large\npart of training set consists of such examples 5, the retrieval-augmented models are overly encouraged to\ncopy-and-paste the retrieved caption as the output, which can undermine their generalization ability at test\nstage, especially for out-of-domain settings.\nTable 6: Full zero-shot evaluation results on MSCOCO dataset. CCS refers to CC3M+CC12M+SBU dataset and\nCOYO the COYO104M. We also conduct experiments on showing “in-domain” retrieval by using MSCOCO\ndataset as retrieval base during inference. This brings further performance improvements. We also present the\nbaseline results by directly evaluating with the retrieved captions from CCS+COYO database. Results indicate that\nthe retrieved caption itself cannot be directly used for captioning generation.\nMethod Pretraining Evaluation\nTraining SetRetrieval DatabaseRetrieval DatabaseBLEU@4 CIDer\nRetrieved Captions - - CCS+COYO 0.0 3.6\nRe-ViLMbase CCS CCS CCS 17.6 49.4\nRe-ViLMbase CCS+COYO CCS+COYO CCS+COYO 17.0 51.2\nRe-ViLMbase CCS+COYO CCS+COYO MSCOCO 17.4 55.2\nRe-ViLMmedium CCS CCS CCS 17.3 52.8\nRe-ViLMmedium CCS+COYO CCS+COYO CCS+COYO 17.9 53.6\nRe-ViLMbase CCS+COYO CCS+COYO MSCOCO 18.2 57.4\nRe-ViLMlarge CCS CCS CCS 19.2 59.6\nRe-ViLMlarge CCS+COYO CCS+COYO CCS+COYO 18.6 60.8\nRe-ViLMbase CCS+COYO CCS+COYO MSCOCO 18.8 65.4\n5For example, we find the ratio of training examples that have the same captions as others can be as high as 15.1% in\nConceptual Captions (Sharma et al., 2018).\n11855\nTable 7: Full zero-shot evaluation results on Flickr30k dataset. CCS refers to CC3M+CC12M+SBU dataset and\nCOYO the COYO104M.\nMethod Pretraining Evaluation\nTraining SetRetrieval DatabaseRetrieval DatabaseCIDer SPICE\nRe-ViLMbase CCS CCS CCS 45.0 9.2\nRe-ViLMbase CCS+COYO CCS+COYO CCS+COYO 45.2 9.2\nRe-ViLMmedium CCS CCS CCS 50.8 9.5\nRe-ViLMmedium CCS+COYO CCS+COYO CCS+COYO 52.0 9.8\nRe-ViLMlarge CCS CCS CCS 51.5 9.7\nRe-ViLMlarge CCS+COYO CCS+COYO CCS+COYO 52.1 10.0\nTable 8: Full fine-tuning evaluation results on MSCOCO dataset. CCS refers to CC3M+CC12M+SBU dataset and\nCOYO the COYO104M.\nMethod Pretraining Evaluation\nTraining SetRetrieval DatabaseRetrieval DatabaseBLEU@4 CIDer\nRe-ViLMbase CCS CCS CCS 37.0 127.5\nRe-ViLMbase CCS CCS CCS+COYO 37.3 128.0\nRe-ViLMbase CCS+COYO CCS CCS 37.2 128.0\nRe-ViLMbase CCS+COYO CCS CCS+COYO 37.4 128.6\nRe-ViLMbase CCS+COYO CCS+COYO CCS+COYO 37.5 128.2\nRe-ViLMbase CCS+COYO CCS+COYO CCS+COYO+COCO 37.8 129.1\nRe-ViLMmedium CCS CCS CCS+COYO 37.5 129.0\nRe-ViLMmedium CCS+COYO CCS CCS+COYO 37.7 129.1\nRe-ViLMmedium CCS+COYO CCS+COYO CCS+COYO 38.0 129.9\nRe-ViLMmedium CCS+COYO CCS+COYO CCS+COYO+COCO 38.2 131.2\nRe-ViLMlarge CCS CCS CCS 38.4 129.8\nRe-ViLMlarge CCS CCS CCS+COYO 38.1 128.4\nRe-ViLMlarge CCS+COYO CCS+COYO CCS+COYO+COCO 39.4 134.2\nTable 9: Re-ViLM zero-shot evaluation and finetuning evaluation on MSCOCO dataset with different number of\nretrieved samples k. While the number of retrieved examples increases, there is no additional benefit in further\nimproving our results.\nMethod Zero-shot Evaluation Finetuning Evaluation\nBLEU@4 CIDer BLEU@4 CIDer\nRe-ViLMbase\nk = 2 17.0 51.2 37.8 129.1\nk = 5 17.0 51.4 37.5 128.4\nk = 10 16.5 49.8 37.5 128.2\nRe-ViLMmedium\nk = 2 17.9 53.6 38.2 131.2\nk = 5 17.6 52.2 37.8 128.6\nk = 10 17.2 50.4 37.6 128.6\nRe-ViLMlarge\nk = 2 18.6 60.8 39.4 134.2\nk = 5 18.5 57.2 38.4 129.2\nk = 10 18.2 55.8 38.2 128.8\n11856\nScenario #2: Identical scenes with different poses\nScenario #1: Different views from the same event\nScenario #3: Different scenes with the same description\nCaption: Hanging out near the flower \nmarket bloemenmarkt.\nCaption: throwing rocks in the water.\nCaption: Women holding a cell phone \nin the office.\nFigure 3: Set of images from conceptual captions dataset with the same captions under(Top)Scenario #1): Different\nviews from the same event. (Middle) #2): Identical scenes with different poses. (Bottom) Scenario #3): Different\nscenes with the same description. The example of images are from Conceptual Captions (Sharma et al., 2018)\n11857"
}