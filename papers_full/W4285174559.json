{
  "title": "Buy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in Pre-trained Language Models",
  "url": "https://openalex.org/W4285174559",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5005388902",
      "name": "Chengyu Chuang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5005421447",
      "name": "Yi Yang",
      "affiliations": [
        "Hong Kong University of Science and Technology",
        "University of Hong Kong"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2003297894",
    "https://openalex.org/W3037252472",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2963078909",
    "https://openalex.org/W3198300102",
    "https://openalex.org/W3173610337",
    "https://openalex.org/W2054240155",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3122280770",
    "https://openalex.org/W2989344603",
    "https://openalex.org/W3122987879",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2893425640",
    "https://openalex.org/W2022792389",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2117000114"
  ],
  "abstract": "Pretrained language models such as BERT have achieved remarkable success in several NLP tasks. With the wide adoption of BERT in real-world applications, researchers begin to investigate the implicit biases encoded in the BERT. In this paper, we assess the implicit stock market preferences in BERT and its finance domain-specific model FinBERT. We find some interesting patterns. For example, the language models are overall more positive towards the stock market, but there are significant differences in preferences between a pair of industry sectors, or even within a sector. Given the prevalence of NLP models in financial decision making systems, this work raises the awareness of their potential implicit preferences in the stock markets. Awareness of such problems can help practitioners improve robustness and accountability of their financial NLP pipelines",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 100 - 105\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nBuy Tesla, Sell Ford: Assessing Implicit Stock Market Preference in\nPre-trained Language Models\nCheng Yu Chuang1 Yi Yang2\n1 Department of Mathematics and Economics,\n2 Department of Information Systems and Operations Management,\nHong Kong University of Science and Technology\ncychuangab@connect.ust.hk, imyiyang@ust.hk\nAbstract\nPretrained language models such as BERT have\nachieved remarkable success in several NLP\ntasks. With the wide adoption of BERT in real-\nworld applications, researchers begin to investi-\ngate the implicit biases encoded in the BERT. In\nthis paper, we assess the implicit stock market\npreferences in BERT and its finance domain-\nspecific model FinBERT. We find some interest-\ning patterns. For example, the language mod-\nels are overall more positive towards the stock\nmarket, but there are significant differences in\npreferences between a pair of industry sectors,\nor even within a sector. Given the prevalence\nof NLP models in financial decision making\nsystems, this work raises the awareness of their\npotential implicit preferences in the stock mar-\nkets. Awareness of such problems can help\npractitioners improve robustness and account-\nability of their financial NLP pipelines 1.\n1 Introduction\nPre-trained language models (PLM) have achieved\nsuperior performance on many NLP tasks (Devlin\net al., 2018; Liu et al., 2019; Radford et al., 2019).\nThey have also been integrated into real-world NLP\nsystems for automated decision-making. Recently,\na burgeoning body of literature has studied the\nhuman-like bias encoded in the PLMs. For ex-\nample, in the mask token prediction task, BERT\nfill-in the [MASK] in the sentence “He/she works\nas a [MASK]” with “doctor/nurse”, reflecting gen-\nder stereotype biased associations (Garimella et al.,\n2021; May et al., 2019). Such biases in the PLMs\nmay further propagate to downstream applications\nwith unintended societal and economic impact.\nIn this work, we investigate and assess the im-\nplicit preference encoded in the PLMs, in the con-\ntext of the financial market. We examine if the\nPLMs prefer one company over the other compa-\nnies. We also examine if such implicit preference\n1Code and data for this work are available at https://\ngithub.com/MattioCh/Buy-Tesla-Sell-Ford\nSentence\nBERT Tesla stock share is going to float.\nFord stock share is going to collapse.\nFinBERT Tesla stock share is going to increase.\nFord stock share is going to decrease.\nTable 1: Masked token predictions.\nin individual stocks also manifests at the industry\nsector level. Our core idea is based on the assump-\ntion that an NLP system designed to be widely\napplicable should ideally produce scores that are\nindependent of the identities of name entities men-\ntioned in the text (Prabhakaran et al., 2019).\nTable 1 illustrates the potential stock market im-\nplicit preferences in the BERT (Devlin et al., 2018)\nand its finance-domain specific variation FinBERT\n(Yang et al., 2020). Clearly, we see a favor of Tesla\nover Ford in both PLMs. This implicit association\nmay be rooted in the training data: While BERT\nis trained on fairly neutral corpora, FinBERT is\ntrained on financial communication corpora, in-\ncluding earnings conference calls and analyst re-\nports. If a company’s name is often mentioned in\nnegative contexts (such as losses, disruptions), a\ntrained model might inadvertently associate nega-\ntivity to that name, resulting in biased predictions\non sentences with that name.\nWe quantitatively assess the implicit preferences\nin the PLMs, using a sample of nearly 3,000 ma-\njor U.S. market stocks. Our analysis reveals that\nthe language models are overall more positive to-\nwards the stock market, but there are significant\ndifferences in preferences between a pair of in-\ndustry sectors, or even within a sector. Given the\nwide adoption of PLMs in the financial applica-\ntions, we hope our work raises awareness of their\npotential stock market implicit preferences of com-\npany names. Moreover, care needs to be taken to\nensure that the unintended preference does not af-\nfect downstream applications. Awareness of such\nmatters can help practitioners to build more robust\n100\nand accountable financial NLP systems.\n2 Background\nHumans have (irrational) preferences in the\nstock markets. Humans are irrational (Becker,\n1962). Human decision-makers are often in-\nfluenced by emotion, biases, and cognitive er-\nrors. Human (irrational) preferences in the stock\nmarkets are well documented in behavioral fi-\nnance/economics literature. For example, the\nhome-bias refers to investors’ strong preference for\ndomestic stocks or concentrated exposure to their\nemployer’s stock (French and Poterba, 1991; Tesar\nand Werner, 1995). Bhattacharya et al. (2018) find\nthat the Mandarin-speaking individual investors\nsubmit disproportionately more limit orders at 8\nthan at 4, because of the belief that the number 8\nis lucky and the number 4 is unlucky — and those\nsuperstitious investors lose money.\nWhy is the implicit stock market preference\nin PLMs an issue?Automated NLP technique\nfor financial decision making is expected to mini-\nmize human irrationality. However, PLMs that are\ntrained with a human-written corpus may inherit\nsuch human preferences (we do find it is the case).\nThis resembles the allocational harms that “arise\nwhen an automated system allocates resources or\nopportunities unfairly to different social groups”\n(Blodgett et al., 2020). In the financial markets, the\ndisproportional allocation of resources, i.e., capital,\nalso has unintended consequences. First, the strong\nfavoritism to a stock can attract more investors to in-\nvest in the stock and increase the company’s capital\nvalue, which helps the company’s growth and de-\nvelopment (Beck and Levine, 2002). This implies\nthat less favored companies may struggle with cap-\nital access. Second, the disproportional resource\nallocation may result in high trading activities and\nincreased volatility of certain stocks, which creates\nuncertainty and instability in the market.\n3 Data and PLMs\nData: We choose Russell 3000 constituent firms\nas our target companies because of their impor-\ntance and tractability. This index includes the 3,000\nlargest publicly held companies incorporated in the\nUnited States as measured by total market cap-\nitalization, and it represents approximately 98%\nof the U.S. public equity market. We also obtain\nan industry sector label for each firm in our sam-\nple, based on the Global Industry Classification\nStandard (GICS). GICS is a widely used industry\nclassification for market analysis, and it consists\nof 11 sectors. For example, company Apple (NAS-\nDAQ:AAPL) is in the Information Technology sec-\ntor, while the company Walmart (NASDAQ:WMT)\nis in the Consumer Staples sector. The GICS sector\nallows us to examine the implicit preference at the\nindustry sector level. The total number of stocks\nin our sample is 2,653. The detailed breakdown of\nGICS sectors in our sample is presented in Table 2.\nGICS Sector Number of stocks\nFinancials 495\nIndustrials 391\nHealth Care 379\nInformation Technology 351\nConsumer Discretionary 310\nReal Estate 162\nEnergy 144\nMaterials 136\nCommunication Services 110\nConsumer Staples 104\nUtilities 71\nTable 2: Sample stocks GICS breakdown.\nPLM: We choose two BERT-based pre-trained lan-\nguage models in our analysis: BERT and FinBERT.\nBERT is one of the most widely used PLMs that\nis trained on Wikipedia and BookCorpus (Devlin\net al., 2018). In addition to BERT, we choose Fin-\nBERT, which is a domain-specific BERT model\nthat is pre-trained on financial communications text,\nincluding annual reports, analyst reports, and earn-\nings conference call transcripts (Yang et al., 2020).\nThe vocabulary of FinBERT is different from the\nBERT model as it contains finance-domain specific\nterms, including company names. It has shown\nto outperform the general-domain BERT (Huang\net al., 2020) on financial downstream tasks. We\nload both base-uncased BERT and FinBERT from\nthe transformers library (Wolf et al., 2020).\n4 Assessing Implicit Preference in\nMasked Token Prediction\nSince BERT and FinBERT use a masked language\nmodeling objective, we directly probe the model\nusing the masked token prediction task, using cloze-\nstyle prompts. Prior work also uses this approach\nto assess the social biases (May et al., 2019), or\nthe knowledge learned by PLMs (Petroni et al.,\n2019). For each firm, we create a simple tem-\n101\nplate containing the attribute word for which we\nwant to measure the preference (e.g. buy or sell)\nand the company name as the target word (e.g.,\nMicrosoft). We then mask the attribute words\nand target words accordingly, to get the condi-\ntional probability of producing the buy or sell\ntoken. Specifically, for firm i, we use the tem-\nplate sentence “We should [MASK] the {name}\nstock” and query the probability of masked token:\nPi,buy = P([MASK ] = buy|name = i), and\nPi,sell = P([MASK ] =sell|name = i). We then\nnormalize the two conditional probabilities.\n4.1 Implicit preferences in the market\nOur first evaluation simply assesses if the PLM is\nlean more towards buy or sell across companies.\nWe obtain the normalized conditional probability\nPi,buy for each firm i, and we plot the boxplot of\nPi,buy in Figure 1. An ideal model would have\na conditional probability close to 0.5 for all firms.\nClearly, it is not the case in the BERT and FinBERT.\nFigure 1 shows that the mean value of Pi,buy is sig-\nnificantly different from 0.5. FinBERT’s average\nbuy probability is even higher than 0.9, indicating\nas a stronger preference for predicting buy token\nover the sell token. This tendency could be ex-\nplained by two reasons. First, prior literature shows\nthat there is a universal positive bias in the human\nlanguage (Dodds et al., 2015). Second, compared\nto BERT which is trained on a fairly neutral corpus,\nFinBERT is trained on financial communication\ncorpora such as analyst reports. Therefore, the\nhigher buy probability may imply that the overall\nmarket sentiment over the years is positive.\nFigure 1: Boxplot of Pi,buy (normalized with Pi,sell)\nfor BERT and FinBERT. It shows strong positive prefer-\nences in company names.\n4.2 Implicit preferences between industries\nIt may not be surprising that the PLMs are overall\npositive. Therefore, we examine if certain industry\nsectors are more favored than the other industries.\nWe use a univariate regression analysis. For firm\ni, we use the Pi,buy, the probability of predicting\nthe masked token “buy”, as the response variable,\nand we use the firm’s sector Xi as the dummy in-\ndependent variable, i.e., Xi is 1 if stock i belong\nto the sector j, otherwise 0. Since we have a total\nof 11 sectors, we set up 11 univariate regression\nmodels and examine the relationship between the\nprobability of “buy” and the dummy industry sec-\ntor variable. The univariate regression is specified\nas follows, and ϵ is the error term.\nFor sector j: yj = βjxj\ni + ϵ (1)\nThe univariate regression results are presented\nin Table 3. We can see that both models have pref-\nerences of one sector over the other sectors. For\nexample, both BERT and FinBERT find companies\nin the Financial sectors less preferred in terms of\npredicting the buy token, as seen from the negative\nβ value and significant p-values. From Table 2, we\ncan see that the most preferred sectors in BERT are\nMaterials, Consumer Staples, and Utilities; while\nfor FinBERT, the most preferred sector is Materi-\nals and Industrials. Moreover, we find that, while\nFinBERT has a stronger buy preference across all\ncompanies than BERT, it has less preference when\ncomparing to the industry sector level, as we see\nthere is a fewer number of sectors with significant\np-values. In other words, FinBERT has positive\npreference across most of sectors, while BERT has\npositive preference only in certain sectors.\nWe further compare the implicit preference be-\ntween a pair of sectors. To do so, we conduct\nCohen’s d test and calculate the effect size of the\ndistributions of pair of industry A and industry B.\nSpecifically, Cohen’s d determines the mean dif-\nference between industry A and B in terms of the\nprobability Pi,buy. A positive value indicates that\nthe PLM has a stronger buy preference for indus-\ntry A than for industry B. We plot the heatmap\nbetween pairs of industries in Figure 2. The figure\nshows that both models have an implicit preference\nbetween sectors. Consistently, Financial is the least\npreferred industry sector.\n102\nGICS Sector BERT FinBERT\nFinancials -0.88*** -0.83***\nIndustrials 0.43 0.40***\nHealth Care 0.00*** 0.10\nInformation Technology 0.12*** 0.7\nConsumer Discretionary 0.17** -0.94\nReal Estate -1.88*** 0.07*\nEnergy 0.15 0.72*\nMaterials 2.22*** 1.09**\nCommunication Services -0.07 -0.98\nConsumer Staples 0.73** -0.30\nUtilities 0.61*** 1.34\nTable 3: Value of β (×10−2) using BERT and Fin-\nBERT model. Asterisk indicates statistical significance\np-value: * p < .1, ** p < .05, *** p < .01\n(a) BERT\n(b) FinBERT\nFigure 2: Heatmap of Cohen’s d test between a pair of\nsectors. Higher value (red) indicates a stronger prefer-\nence in predicting the buy token from one sector on the\nvertical axis to another sectors on the horizontal axis.\n5 Assessing Implicit Preferences within\nan Industry Sector\nThe masked token prediction is only one way of\nprobing the PLMs. Recent NLP literature has pro-\nposed the word association tests to measure the\nhuman-like biases in the static word embedding\n(Bolukbasi et al., 2016; Caliskan et al., 2017) or\ncontextualized word embedding (May et al., 2019).\nThe word association test in the contextualized\nembedding model is called Sentence Encoder As-\nsociation Test (SEAT). Essentially, SEAT evalu-\nates whether the contextualized representations for\nwords from an attribute word set tend to be more\nclosely associated with the contextualized represen-\ntations for words from a target word set. Templates\nsuch as “this is a [word]” are used to obtain the\nword contextualized representations.\nIn this work, we create a template sentence\n“{name} is a stock” where {name} is a stock’s com-\npany name, and we obtain the [CLS] embedding\nas its embedding. For preference words buy and\nsell, we create a template “We should buy/sell a\nstock\", and we obtain the [CLS] embedding as its\nembedding. Let simi,buy and simi,sell be the co-\nsine similarity between the embedding of company\ni’s name and the embedding of buy/sell. Given\nan industry sector S containing a set of stocks,\nwe calculate the SEAT association effect-size as:\nd = meani∈S(simi,buy)−meani∈S(simi,sell)\nstd_devi∈S{simi,buy,simi,sell} . An ef-\nfect size with absolute value closer to 0 indicates\nlower implicit preference. We present the individ-\nual sector’s SEAT score in Table 4, which leads to\nthe following observations. First, we see consistent\nimplicit preferences within individual sectors. For\nexample, both BERT and FinBERT regard Finan-\ncials as the least preferred sector (negative effect\nsize). Since this is a within-in sector study, it im-\nplies that some Financial stocks are preferred over\nthe other Financial stocks. Second, we see that the\nmajority of the sectors have a positive effect size,\nindicating that both PLMs exhibit a positive bias\nwithin the sector.\n6 Conclusion\nIn this paper, we study the implicit stock market\npreference in PLMs. Motivated by recent literature\nin implicit social bias, we apply the masked token\nprediction and sentence embedding association test\n(SEAT) to the PLMs. We find that there is a con-\nsistent implicit preference of the stock market in\nthe PLMs, and the preferences exist at the whole-\n103\nGICS Sector BERT FinBERT\nFinancials -0.65 -0.15\nIndustrials 0.19 0.34\nHealth Care 0.06 -0.03\nInformation Technology 0.44 0.06\nConsumer Discretionary 0.25 0.26\nReal Estate 0.00 0.29\nEnergy -0.56 0.44\nMaterials -0.15 0.15\nCommunication Services 0.10 -0.06\nConsumer Staples -0.08 0.18\nUtilities -0.19 0.12\nTable 4: Within-sector implicit preferences using SEAT.\nValue close to zero indicates lower implicit preference.\nmarket, between-industry,and within-industry level.\nGiven the wide adoption of PLMs in real-world\nfinancial systems, we hope that this work raises the\nawareness of potential implicit stock preferences,\nso that practitioners and researchers can build more\nrobust and accountable financial NLP systems. Fu-\nture work can investigate whether the implicit pref-\nerences are driven by some financial factors such as\nmarket value or stock returns, and examine how the\npreferences over stocks/industries in PLMs affect\ndownstream financial NLP applications, such as\nsentiment analysis, or stock movement prediction.\nAcknowledgement\nThis work was supported by HKUST-Kaisa Group\nSeed Project on Fintech “HKJRI3A-057”.\nReferences\nThorsten Beck and Ross Levine. 2002. Industry growth\nand capital allocation:: does having a market-or bank-\nbased system matter? Journal of financial economics,\n64(2):147–180.\nGary S Becker. 1962. Irrational behavior and economic\ntheory. Journal of political economy, 70(1):1–13.\nUtpal Bhattacharya, Wei-Yu Kuo, Tse-Chun Lin, and\nJing Zhao. 2018. Do superstitious traders lose\nmoney? Management Science, 64(8):3772–3791.\nSu Lin Blodgett, Solon Barocas, Hal Daumé III, and\nHanna M. Wallach. 2020. Language (technology)\nis power: A critical survey of \"bias\" in NLP. In\nProceedings of the 58th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2020,\nOnline, July 5-10, 2020, pages 5454–5476. Associa-\ntion for Computational Linguistics.\nTolga Bolukbasi, Kai-Wei Chang, James Y . Zou,\nVenkatesh Saligrama, and Adam Tauman Kalai. 2016.\nMan is to computer programmer as woman is to\nhomemaker? debiasing word embeddings. In Ad-\nvances in Neural Information Processing Systems 29:\nAnnual Conference on Neural Information Process-\ning Systems 2016, December 5-10, 2016, Barcelona,\nSpain, pages 4349–4357.\nAylin Caliskan, Joanna J Bryson, and Arvind Narayanan.\n2017. Semantics derived automatically from lan-\nguage corpora contain human-like biases. Science,\n356(6334):183–186.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nPeter Sheridan Dodds, Eric M Clark, Suma Desu,\nMorgan R Frank, Andrew J Reagan, Jake Ryland\nWilliams, Lewis Mitchell, Kameron Decker Harris,\nIsabel M Kloumann, James P Bagrow, et al. 2015.\nHuman language reveals a universal positivity bias.\nProceedings of the national academy of sciences ,\n112(8):2389–2394.\nKenneth R French and James M Poterba. 1991. Investor\ndiversification and international equity markets. The\nAmerican Economic Review, 81(2):222–226.\nAparna Garimella, Akhash Amarnath, Kiran Kumar,\nAkash Pramod Yalla, N Anandhavelu, Niyati Chhaya,\nand Balaji Vasan Srinivasan. 2021. He is very intel-\nligent, she is very beautiful? on mitigating social\nbiases in language modelling and generation. In\nFindings of the Association for Computational Lin-\nguistics: ACL-IJCNLP 2021, pages 4534–4545.\nAllen Huang, Hui Wang, and Yi Yang. 2020. Finbert—a\ndeep learning approach to extracting textual informa-\ntion. Available at SSRN 3910214.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nChandler May, Alex Wang, Shikha Bordia, Samuel R\nBowman, and Rachel Rudinger. 2019. On measuring\nsocial biases in sentence encoders. arXiv preprint\narXiv:1903.10561.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases? arXiv preprint arXiv:1909.01066.\nVinodkumar Prabhakaran, Ben Hutchinson, and Mar-\ngaret Mitchell. 2019. Perturbation sensitivity analy-\nsis to detect unintended model biases. InProceedings\nof the 2019 Conference on Empirical Methods in Nat-\nural Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 5740–5745.\n104\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nLinda L Tesar and Ingrid M Werner. 1995. Home bias\nand high turnover. Journal of international money\nand finance, 14(4):467–492.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45.\nYi Yang, Mark Christopher Siy Uy, and Allen Huang.\n2020. Finbert: A pretrained language model\nfor financial communications. arXiv preprint\narXiv:2006.08097.\n105",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6230263710021973
    },
    {
      "name": "Stock market",
      "score": 0.5807554125785828
    },
    {
      "name": "Financial market",
      "score": 0.5731383562088013
    },
    {
      "name": "Stock (firearms)",
      "score": 0.5463970899581909
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.5377322435379028
    },
    {
      "name": "Preference",
      "score": 0.5186754465103149
    },
    {
      "name": "Language model",
      "score": 0.5001587867736816
    },
    {
      "name": "Artificial intelligence",
      "score": 0.45549190044403076
    },
    {
      "name": "Accountability",
      "score": 0.4472179114818573
    },
    {
      "name": "Machine learning",
      "score": 0.335868239402771
    },
    {
      "name": "Financial economics",
      "score": 0.32176876068115234
    },
    {
      "name": "Finance",
      "score": 0.29925036430358887
    },
    {
      "name": "Economics",
      "score": 0.27528077363967896
    },
    {
      "name": "Microeconomics",
      "score": 0.2615475058555603
    },
    {
      "name": "Engineering",
      "score": 0.08373579382896423
    },
    {
      "name": "History",
      "score": 0.08006995916366577
    },
    {
      "name": "Context (archaeology)",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I200769079",
      "name": "Hong Kong University of Science and Technology",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I889458895",
      "name": "University of Hong Kong",
      "country": "HK"
    }
  ]
}