{
    "title": "Improving hierarchical product classification using domain-specific language modelling",
    "url": "https://openalex.org/W3187798969",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2130680807",
            "name": "Alexander Brinkmann",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A247353998",
            "name": "Christian Bizer",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2970771982",
        "https://openalex.org/W3046054871",
        "https://openalex.org/W2167277498",
        "https://openalex.org/W3102616888",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3115961023",
        "https://openalex.org/W2945883855",
        "https://openalex.org/W3116169343",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2150766729",
        "https://openalex.org/W2970574105",
        "https://openalex.org/W3097384126"
    ],
    "abstract": "In order to deliver a coherent user experience, product aggregators such as market places or price portals integrate product offers from many web shops into a single product categorization hierarchy. Recently, transformer models have shown remarkable performance on various NLP tasks. These models are &#13;\\npre-trained on huge cross-domain text corpora using self-supervised learning and fine-tuned afterwards for specific downstream tasks. Research from other application domains indicates that additional selfsupervised pre-training using domain-specific text corpora can further increase downstream performance without requiring additional task-specific training data. In this paper, we first show that transformers outperform a more traditional fastText-based classification technique on the task of assigning product offers from different web shops into a product hierarchy. Afterwards, we investigate whether it is possible to improve the performance of the transformer models by performing additional self-supervised pretraining using different corpora of product offers, which were extracted from the Common Crawl. Our experiments show that by using large numbers of related product offers for masked language modelling, it is possible to increase the performance of the transformer models by 1.22% in wF1 and 1.36% in hF1 reaching a performance of nearly 89% wF1.",
    "full_text": null
}