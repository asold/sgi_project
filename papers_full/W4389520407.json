{
  "title": "Explicit Planning Helps Language Models in Logical Reasoning",
  "url": "https://openalex.org/W4389520407",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5101910721",
      "name": "Hongyu Zhao",
      "affiliations": [
        "Toyota Technological Institute at Chicago",
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5046827792",
      "name": "Kangrui Wang",
      "affiliations": [
        "Toyota Technological Institute at Chicago",
        "University of Chicago"
      ]
    },
    {
      "id": "https://openalex.org/A5102025157",
      "name": "Mo Yu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5070368916",
      "name": "Hongyuan Mei",
      "affiliations": [
        "Toyota Technological Institute at Chicago"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2769099080",
    "https://openalex.org/W4206633687",
    "https://openalex.org/W3104499181",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W4385573616",
    "https://openalex.org/W2996848635",
    "https://openalex.org/W3172267148",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2584683887",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W4296415442",
    "https://openalex.org/W2964072618",
    "https://openalex.org/W2962886429",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2942128719",
    "https://openalex.org/W2980536612",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4281250694",
    "https://openalex.org/W2970155250",
    "https://openalex.org/W4287084089",
    "https://openalex.org/W2132174274",
    "https://openalex.org/W2029249040",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3015770160",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3159156888",
    "https://openalex.org/W4320858367",
    "https://openalex.org/W4302305823",
    "https://openalex.org/W3183138634",
    "https://openalex.org/W4318719086",
    "https://openalex.org/W3173805051",
    "https://openalex.org/W4226288657",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2516014563",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4385767671",
    "https://openalex.org/W2623293810",
    "https://openalex.org/W3105516974",
    "https://openalex.org/W4287758766",
    "https://openalex.org/W4385573938",
    "https://openalex.org/W3200178276",
    "https://openalex.org/W4205870266",
    "https://openalex.org/W2890487780",
    "https://openalex.org/W41554520",
    "https://openalex.org/W4385573762",
    "https://openalex.org/W2962833140",
    "https://openalex.org/W3204886703",
    "https://openalex.org/W4295302007",
    "https://openalex.org/W2087451659",
    "https://openalex.org/W2075780421",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W3089102176",
    "https://openalex.org/W2953163841",
    "https://openalex.org/W2125297418",
    "https://openalex.org/W2950246755",
    "https://openalex.org/W1503159390",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "Language models have been shown to perform remarkably well on a wide range of natural language processing tasks. In this paper, we propose LEAP, a novel system that uses language models to perform multi-step logical reasoning and incorporates explicit planning into the inference procedure. Explicit planning enables the system to make more informed reasoning decisions at each step by looking ahead into their future effects. Moreover, we propose a training strategy that safeguards the planning process from being led astray by spurious features. Our full system significantly outperforms other competing methods on multiple standard datasets. When using small T5 models as its core selection and deduction components, our system performs competitively compared to GPT-3 despite having only about 1B parameters (i.e., 175 times smaller than GPT-3). When using GPT-3.5, it significantly outperforms chain-of-thought prompting on the challenging PrOntoQA dataset. We have conducted extensive empirical studies to demonstrate that explicit planning plays a crucial role in the system's performance.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 11155–11173\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nExplicit Planning Helps Language Models in Logical Reasoning\nHongyu Zhao∗1,3 Kangrui Wang1,3 Mo Yu2 Hongyuan Mei3\n1University of Chicago 2WeChat AI 3Toyota Technological Institute at Chicago\n{hzhao,hongyuan}@ttic.edu\nAbstract\nLanguage models have been shown to perform\nremarkably well on a wide range of natural lan-\nguage processing tasks. In this paper, we pro-\npose LEAP, a novel system that uses language\nmodels to perform multi-step logical reasoning\nand incorporates explicit planning into the in-\nference procedure. Explicit planning enables\nthe system to make more informed reasoning\ndecisions at each step by looking ahead into\ntheir future effects. Moreover, we propose a\ntraining strategy that safeguards the planning\nprocess from being led astray by spurious fea-\ntures. Our full system significantly outper-\nforms other competing methods on multiple\nstandard datasets. When using small T5 mod-\nels as its core selection and deduction compo-\nnents, our system performs competitively com-\npared to GPT-3 despite having only about 1B\nparameters (i.e., 175 times smaller than GPT-\n3). When using GPT-3.5, it significantly out-\nperforms chain-of-thought prompting on the\nchallenging PrOntoQA dataset. We have con-\nducted extensive empirical studies to demon-\nstrate that explicit planning plays a crucial role\nin the system’s performance.\n1 Introduction\nLogical reasoning is one of the most important and long-\nstanding problems in artificial intelligence (Russell and\nNorvig, 2010). A logical reasoning system is able to\ndraw new facts by applying known rules to known facts\nand determine the truth value of a given hypothesis;\nsee Figure 1 for an example. For decades, research in\nbuilding reasoning systems has heavily relied on for-\nmal logic. Since the surge of pretrained large language\nmodels (LMs), there have been efforts that harness the\npower of pretrained LMs and directly handle natural\nlanguage statements to perform multi-step logical rea-\nsoning; see section 5 for a summary. In this paper, we\npropose LEAP, the first LM-based logical reasoning\nsystem that performs explicit planning during inference.\nWhile determining the truth value of a statement, our\nsystem searches over the known facts for those which\n∗Work done during internship at TTI-Chicago.\nx1\nx6\nx1\nx2\nx3\nx4\nx5 x7\nx6\nx0\nx2\nx3 x4\nx5\nx0: Eagles are carnivores.\nx2: Eagles eat rabbits.\nx1: Carnivores only eat animals.\nx3: Rabbits are animals.\nx4: Eagles do not eat plants.\nx7: Eagles are carnivores.x6: Eagles only eat animals.\nx5: Eagles eat animals.\ngoaltheory logical reasoning process\n(3 steps of selection and deduction)\nx1x2\nx3\nx4\nstep 1\nselection step 1\ndeduction step 2\nselection \nstep 2\ndeduction \nstep 3\nselection step 3\ndeduction \nx5\nx7\nx6\nadded after step 1\nadded after step 2\nadded after step 3\nFigure 1: An example of theoryTand goal x0 as well as\na human-annotated multi-step logical reasoning process\nthat proves the goal based on the theory.\nare relevant and performs multiple rounds of deduction\nto reach the conclusion. At each round, the planning\nprocess looks ahead into the future outcomes of each\npossible reasoning decision (i.e., which to select and\nwhat to deduce), examining which of them is more likely\nto discover a valid proof for the given statement.\nWhy planning? Planning is a fundamental property\nof intelligent behavior: it uses foresight to anticipate\nfuture outcomes of each possible decision and informs\nthe process of decision making to achieve desirable\nend results. This concept has influenced the develop-\nment of various methods in the field of artificial intel-\nligence. Minimax-style game playing evaluates each\npossible move by anticipating replies and counterreplies\nbetween the player and the opponent (while assuming\nthat both play optimally) (Russell and Norvig, 2010).\nModel-based reinforcement learning uses environment\nmodels to simulate responses to actions and then uses\nthe simulated experiences to help learn value functions\n(e.g., Dyna, Monte-Carlo tree search) (Sutton and Barto,\n2018). In natural language processing, planning has\nbeen used to help language models generate utterances\nthat satisfy complex constraints (Lu et al., 2022a).\nPlanning is important for logical reasoning. By ex-\namining the future outcomes of each possible decision,\na planning-based system will be able to focus on the\nactually useful (given and deduced) facts at early steps,\nthus enjoying a high chance of success. In addition, a\nplanning-based reasoning system tends to be more in-\nterpretable, thus more useful in user-centric and safety-\ncritical scenarios. For example, at each round of deduc-\ntion, planning will explicitly show “what will happen\nafter—and that is also why—I select these known facts\n11155\nand deduce this particular new fact from them”, which\nis more informative than only saying “I select these and\ndeduce this.” However, none of the previous LM-based\nsystems use explicit planning during inference.\nWhy is it challenging? During planning, a verifica-\ntion mechanism is in need to determine the quality of\neach possible proof. In reality, the verification has to\nbe performed by a model (like in model-based rein-\nforcement learning), and models are imperfect due to\narchitectural biases and finite training data. As a con-\nsequence, the reasoning system faces the problem of\nmodel exploitation: any model mistake may misguide\nthe planning such that it favors a seemingly promising\ndecision over the actually correct one. For example,\nthe model may incorrectly think a statement proves the\nhypothesis, just because of a significant lexical over-\nlap, causing the planning to favor a decision that helps\ndeduce that statement and lead to the wrong conclusion.\nOur contributions. We first propose a logical reason-\ning system along with a beam-search-style inference\nalgorithm (section 3.1): the system utilizes pretrained\nLMs and mimics human-like step-by-step reasoning.\nThen we integrate explicit planning into the inference\nalgorithm (section 3.2) and significantly improve the\nperformance of the system. We empirically demonstrate\nthat planning encounters the issue of model exploita-\ntion: when the given hypothesis is false, planning may\nfind out an incorrect proof that fools the system to be-\nlieve that the hypothesis is true. Finally, we develop a\ntraining strategy that effectively mitigates the issue of\nmodel exploitation (section 3.3). Our training strategy\nis adversarial: for each training theory, we synthesize\na non-provable hypothesis but call the planning-based\ninference method to find a highly-scored proof for it;\nthen we refine the verification model such that the score\nit assigns to that proof is suppressed; at the same time,\nwe force the verification model to preserve its scores\non the correct proofs of the provable hypothesises. Our\nexperiments show that this strategy further significantly\nimproves the performance of our system.\n2 Problem Formulation\nWe consider the problem of logical reasoning. Given a\nhypothesis (or, in other words, a goal) x0 and a theory\nT = {x1,..., xN }, we are interested in determining\nthe truth value of x0, i.e., whether x0 can be logically\nproved by T. If the goal x0 is provable, we are inter-\nested in discovering the reasoning process that proves it.\nBelow is an example theory T\n{\n“Richard is a King.” “John is also a King.”\n“John is greedy.” “A greedy King is evil.”\n}\nFor the goal “John is evil.”, humans can easily verify\nthat it is provable by figuring out the following reason-\ning path: we can select the two premises about “John”\nand deduce “John is a greedy King.” by combining\nthem; we then pick the premise about “greedy King”\nand conclude “John is evil.” by combining it with the\nprevious deduction. In this paper, we build an automatic\nsystem that is able to perform this kind of human-like\nlogical reasoning.\n3 Our LEAP Framework\nWe propose LEAP, an LM-based logical reasoning sys-\ntem that performs explicit planning. Pretrained LMs are\nexcellent at understanding natural languages as well as\nfluently generating them.1 Our LEAP system harnesses\nsuch abilities to simulate step-by-step reasoning pro-\ncesses that resembles how humans do logical reasoning.\nIn this section, we will incrementally build up our full\nsystem, starting from a base system (section 3.1) to how\nexplicit planning is integrated (sections 3.2–3.3).\n3.1 Base System\nOur base system consists of a selection model psel, a de-\nduction model pded, and a verification model pver. They\nwork together in an iterative fashion to perform multi-\nstep reasoning like shown in Figure 1. At each step, the\nselection model psel selects a couple of premises from\nthe current theory. For example, at step-1 in Figure 1,\nit selects “eagles eat rabbits” and “rabbits are animals”\nfrom the original theory of four premises. Then the\ndeduction model pded reads the selected premises and\noutputs a new statement that is logically plausible given\nthe selection. For example, at step-1 in Figure 1, it de-\nduces “eagles eat animals”. The new statement is then\nadded to the theory (whose size increases by one) and\nit may be selected by psel at a later step. The procedure\nstops if the max number of reasoning steps has been\nreached; otherwise, it starts a new iteration of selection\nand deduction. This procedure gives a reasoning path\nas shown in Figure 1.\nWe define the proof score of the reasoning path to be\nf(T,x0)\ndef\n= max\nn=1,...,N\npver(x0 |xn) ∈(0,1) (1)\nwhere theory T has been extended to include all the\nnew deductions obtained through the reasoning process.\nEach pver(x0 |xn) is given by the verification model\nand measures how likely the statementxn will prove the\ngoal: e.g., “eagles only eat animals” (x6) should have\na lower score than “eagles are carnivores” ( x7) since\nthe latter means the same as the goal. The proof score\nf(T,x0) can be regarded as the system’s belief that the\ntheory proves the goal.\nHow do we define the verification score pver(x0 |\nxn)? We utilize a pretrained DeBERTa model (He et al.,\n2021) that was fine-tuned on the standard MNLI lan-\nguage inference dataset (Williams et al., 2018). For a\nstatement xn and goal x0, we define the verification\nscore pver(x0 |xn) to be the DeBERTa probability that\nxn entails x0. It is a reasonable estimate for the proba-\nbility that xn proves x0.\n1We use “language model” broadly to refer to multiple\ntypes of language representation models including encoder-\nonly, decoder-only, and encoder-decoder models.\n11156\nOur system is general: the selection and deduction\nmodels can be any pretrained decoder-only or encoder-\ndecoder models, including the small models whose pa-\nrameters we could update and the huge models that we\ncould only use as blackboxes. In section 4, we will\ndiscuss some specific model choices as well as how to\ntransfer them to our logical reasoning problem. Gener-\nally, we only require that\n• the selection model psel can propose multiple multi-\npremise selections given the theory T and assign a\nscore to each of them. For a multi-premise selec-\ntion s (e.g., s = x2x3), we denote the score to be\npsel(s |T ,x0), or psel(s) for short.\n• the deduction model pded can draw multiple deduc-\ntions given a selection s and assign a score to each\nof them. For a deduction x, we denote its score to be\npded(x |s).\nSo far, we have been assuming that we select the highest\nscored selection and deduction at each step (e.g., in Fig-\nure 1 and at the beginning of this section). But this kind\nof one-best decoding tends to be short-sighted: there\nmay be multiple possible reasoning paths to proving\nthe goal; some may be better than the others (e.g., they\nare shorter) but they may not appear to be promising at\nthe early steps; such reasoning paths may be missed by\none-best decoding. Therefore, we develop an improved\ndecoding method that resembles beam search (Jurafsky\nand Martin, 2000).\nBeam-search-style inference. We maintain a bufferB\nof maximum size Bwhich can host at most Bongoing\nreasoning paths, which we think are the most promising\nand will eventually prove the goal. Each of ongoing path\ntracks its proof score f as well as its log-probability g\nunder our system. Both f and gget updated as the path\nprogresses, which we will explain shortly. It also tracks\nits initial theory as well as its selections and deductions;\nthe initial theory and the deductions form the extended\n(or current) theory. As long as we haven’t reached the\nmaximum number of steps, we keep expanding each\nongoing path in the buffer. Each step of expansion\nincludes a selection step followed by a deduction step.\nAt the selection step, we do the following:\n• For each ongoing path, we find its top Bmost proba-\nble selections (u1,s1),..., (uB,sB) where ub is the\nlog-probability log psel(sb). Each selection expands\nits ongoing path and updates itsgscore by g←g+ub.\n• Now we have B2 extended paths and let the buffer B\nonly keep Bof them which are most probable under\nthe system (i.e., those with the highest g).\nAt the deduction step, we follow a similar procedure:\n• For each ongoing path, we draw its top Bmost proba-\nble deductions (v1,y1),..., (vB,yB) conditioned on\nthe most recent selection s; vb is the log-probability\npded(yb |s) under deduction model pded. Each deduc-\ntion expands the ongoing path: it updates the scores\nby g←g+ vb and f ←max{f,pver(x0 |yb)}.\n• Now we end up withB2 extended paths and only keep\nBof them which have the highest g.\nIn the end, we return the reasoning path with the highest\nproof score f: intuitively, among all the choices that\nare probable under the selection and deduction models,\nwe’d like to pick what’s most likely to actually prove\nthe goal. This method becomes one-best decoding if we\nset B = 1.\nAppendix B.1 has more details of the base system,\nincluding pseudocode for inference (Algorithms 1–3).\nRelations to formal logic systems. Our base system\nresembles a rule-based system and the inference method\nis like a combination of the forward and backward chain-\ning algorithms (Russell and Norvig, 2010). Each deduc-\ntion step extends the theory by deducing new facts from\nthe existing facts and rules, which resembles the forward\nchaining algorithm. Each selection step is conditioned\non the goal, which resembles the backward chaining\nalgorithm. However, the forward and backward algo-\nrithms can not handle the theories that have non-definite\nclauses like “Either John or Richard is evil.”; our method\ndoesn’t have that limitation.\n3.2 Improvement-A: Inference with Planning\nThe inference method in section 3.1 lacks planning.\nWhile expanding each ongoing path, the selections and\ndeductions are ranked by their scores uand vthat are\nonly conditioned on the previous selections and deduc-\ntions. However, the selections and deductions that ap-\npear to be promising may not actually lead to the future\nsteps that are able to prove the goal. In this section, we\npropose an improved inference method that ranks the\nselections and deductions by explicit planning. We refer\nto the improved version as System-A.\nPlanning for selection. At each selection step, we\nexpand each ongoing reasoning path with Bselections\ngiven by the no-planning method, and let the buffer B\nkeep Bof the B2 extended paths with the highest scores.\nThe key improvement is: we redefine the score such that\nit reflects not only the probability of the selection under\nthe model psel but also the quality of the future steps\nthat the selection leads to.\nPrecisely, we redefineu= log psel(s)+α∆uwhere α\nis a tunable hyperparameter and ∆uis a future-specific\ncorrection term that we can compute after rolling out\nsome imaginary future deductions. For a possible se-\nlection s, we call the base one-best decoding method\n(section 3.1) to roll out D steps of future deductions\n˜y1,..., ˜yD. Then we obtain pver(x0 | ˜yd)—which\nevaluates how likely each rolled-out deduction may\nentail the goal—and compute the correction term by\n∆u\ndef\n= maxd log pver(x0 |˜yd). Note that ∆uis the log-\narithm of the proof score defined on the rolled-out future\n11157\nx0x1\nx2\ngoal\nx3\nx4\nx1\nx2\nx3\nx4\nx5\n…\nx4\nx5 …\npossible \nselections\ncurrent\ntheory\n2 steps of roll-out\n(i.e., roll-out 2 steps of deduction)\nproof score = 0.65\nproof score = 0.57\nproof score = 0.93\nx4x5 has highest score, thus selected\n…\n…\n…\n…\n(a) Planning for selection.\nx0\ngoal\nx1\nx2\nx3\nx4\nx5\nx4\nx5\ncurrent \nselection\ncurrent\ntheory\n1 steps of roll-out\n(i.e., roll-out 1 step of deduction)\nproof score = 0.38\nproof score = 0.92\nproof score = 0.71 \nthis has highest score, thus chosen as deduction\n…\n…\n…\n…\npossible\ndeductions (b) Planning for deduction.\nFigure 2: An illustration of explicit planning at the 2nd selection and deduction step of the full procedure in Figure 1.\nreasoning path. Intuitively, a higher ∆umeans that this\nfuture reasoning path is more likely to prove the goal.\nIn the end, we obtain Bselections with updated scores\n(u1,s1),..., (uB,sB) for each ongoing path.\nThis improved subroutine is illustrated in Figure 2a.\nIts pseudocode is Algorithm 10 in Appendix B.5.\nPlanning for deduction. At each deduction step, we\nexpand each ongoing reasoning path with Bdeductions\ngiven by the no-planning method, and let the buffer B\nkeep B of the extended paths with the highest scores.\nSimilar to the planning-based selection step, the key\nimprovement is the refined definition of the score, which\nreflects not only the probability of the deduction under\nthe model pded but also the quality of its future steps.\nPrecisely, we first draw Bmost probable deductions\n(v1,y1),..., (vB,yB) under the model pded. Then we\nedit the score vb ← vb + β∆vb where β is a tun-\nable hyperparameter and ∆v is a future-specific cor-\nrection similar to ∆u. For each possible deduction yb,\nwe call the no-planning one-best decoding method to\nroll out D steps of future deductions ˜yb,1,..., ˜yb,D.\nThen we compute ∆vb\ndef\n= maxd log pver(x0 |˜yb,d). In\nthe end, we obtain B deductions with updated scores\n(v1,y1),..., (vB,yB) for each ongoing path.\nThis improved subroutine is illustrated in Figure 2b.\nIts pseudocode is Algorithm 11 in Appendix B.5.\nThe full method. Except for the score definitions, the\nplanning-based inference method looks the same as the\nno-planning method: the top selections and deductions\nwill expand their ongoing paths and update their scores\nf and g; the buffer will only keep B paths with the\nhighest g. But the planning-based method will tend to\nend up with a different set of reasoning paths than the\nno-planning method since the scores have been affected\nby the roll-outs. The full inference algorithm is Algo-\nrithm 1 in Appendix B.1: when D≥1, it does explicit\nplanning; when D = 0, it doesn’t roll out future steps\nand becomes the no-planning method.\nSystem 1 vs. System 2 reasoning. According to the\n“dual process” theories of reasoning (Evans, 2003), hu-\nman cognition can be thought of as an interplay between\na fast and intuitive “System 1” and a slow but analytical\n“System 2”. Given enough time, System 2 can analyze\nthe default behavior of System 1 and override it if neces-\nsary. In analogy to this process, our base system can be\nconsidered as System 1, while the advanced planning-\nbased system is like System 2, which requires more\ncomputation but performs more deliberative reasoning.\nPrecisely, at each step of reasoning, the no-planning\nbase system needs 3B operations (i.e., select, deduce,\nand verify). In contrast, the planning-based inference\nneeds 3B+ 3B2D+ 3B2Doperations: for each ongo-\ning reasoning path in the buffer, we need to examine its\nBpossible expansions (selection or deduction), and roll\nout Dfuture steps (via one-best decoding) for each ex-\npansion. Overall, the planning-based system consumes\n1 + 2BD times of computation. Fortunately, our imple-\nmentation is efficient because of careful tensorization\nand parallelism; please see section 6.1 for an analysis\nof its actual walk-clock time.\n3.3 Improvement-B: Refined Verification Model\nThe key limitation of the planning method is that it\nmay exploit the pretrained verification model pver such\nthat the final proof score f(theory,goal) is inflated: this\nmethod keeps ongoing paths that have high pver(goal |\npossible future deductions). This will result in a high\nrate of false positive: even when the goal is not provable,\nexplicit planning will still try its best to find out the\nreasoning paths that have high proof scores; a high\nproof score will then fool the system itself to believe\nthat this goal is provable. This issue is illustrated in\nour experiments (see Figure 5c and related analysis in\nsection 6.1). In this section, we propose to resolve this\nissue by refining our verification model. We refer to this\nversion of our LEAP system as System-B.\nOur method is to tune the verification modelpver such\nthat pver(goal |deduction) is low when the deduction\ncan not prove the goal. Technically, given a theory T\nand a non-provable goal ¯x0, we first call our planning-\nbased method to find a reasoning path that tries to prove\n¯x0, and then make pver(¯x0 | ¯y) to be low for each\ndeduction ¯y in the reasoning path. Precisely, we locally\nminimize ℓ:\nlog pver(¯x0 |¯y) −log (pver(¯x0 |¯y) + pver(x0 |y))\n(2)\nwhere x0 is a provable goal and y is a deduction in a\nreasoning path that actually proves x0. This objective\nℓ is a typical contrastive learning objective (Ma and\nCollins, 2018). In our setting, it means: if we are given\n11158\nx0\nprovable goal\nx1\nx2\nx3\nx4\ncurrent\ntheory\nground-truth reasoning path\nതx0\nnon-provable goal\ny\nmodel-proposed incorrect reasoning path\n… …\n… …\nതy\npver(തx0 | തy)\npver(തx0 | തy)        +    pver(x0 | y)\nminimize    log\nFigure 3: Illustration of our contrastive learning frame-\nwork for refining verification model.\na non-provable goal ¯x0 paired with a model-proposed\nreasoning path as well as a provable goalx0 paired with\na correct reasoning path, our verification model pver\nshould learn to correctly judge that “¯x0 proved by path\nof ¯y” is less likely than “x0 proved by path of y”. This\nframework is illustrated in Figure 3.\nAdditionally, we augment the loss ℓwith\nΩ = −p−\nver(x0 |y) logpver(x0 |y) (3a)\n−\n(\n1 −p−\nver(x0 |y)\n)\nlog (1−pver(x0 |y))\n(3b)\nwhere p−\nver is the pretrained verification model used in\nsections 3.1 and 3.2. It is the KL-divergence (minus\nH(p−\nver), which is a constant wrt. model parameters) be-\ntween the pretrained and tuned verification models, and\nminimizing it aims to prevent the tuned model from de-\nviating too much from the pretrained. This is desirable\nsince the pretrained model already enjoys a high rate of\ntrue positive for provable goals; see results in Figure 5b\nand relevant analysis in section 6.1.\nTechnical details (including visualization) about the\nverification model are in Appendix B.2.\n4 Small and Large Model Versions\nNow we introduce two specific versions of our proposed\nframework: the small language model (SLM) version\nthat uses pretrained T5 (Raffel et al., 2020) and the large\nlanguage model (LLM) version that utilizes GPT-3.5.\n4.1 SLM Version\nOur SLM version adapts pretrained T5 models (Raffel\net al., 2020) to be the selection and deduction models.\nWe use the T5-small instance (from Huggingface) that\nhas only 60M parameters because we would like to\ninvestigate how well a very small system will work in\npractice. Shortly in section 6, we will see that this small\nsystem works very well.\nGiven a theory T and a goal x0, the selection T5\nmodel reads them as input and produces the probability\npsel(xn |T ,x0) that each premise xn is selected in the\nattempt to prove the goal x0. Then we can use these\nprobabilities to compute the probability psel(s |T ,x0)\nthat a multi-premise combination s (e.g., s = x2x4) is\nT5 \nEncoder\nT5 \nDecoder\nh\npsel( xi | x1 x2 x3 x4 x5 x0 ) = σ(hTwi) = \nx1 x4SP1 SP4SP0 x0 …ENC DECx5SP5\nspecial tokens with trainable embeddings: ENC SP0 SP1 … SP5 DEC\nSP1 SP4 SP5…\n0.15 0.75 0.83…\nw1 w4 w5…\nhidden state\nembeddings of special tokens\ncurrent theory goal selection turns out to be x4 x5 (high psel)\n(a) A selection step. The T5 encoder reads special to-\nkens, the goal x0, and the theory T. The decoder com-\nputes psel(xn |T ,x0)\ndef\n= σ(h⊤wn) where wn is the\nembedding of special token SPn.\nx6 :  Eagles            only       ?\nT5 \nEncoder T5 Decoder\nENC DEC\nx4: Eagles do not eat plants. x5: Eagles eat animals.\nx6 :  Eagles only\nx6: Eagles only eat animals. T5 vocabulary\nx4 x5\ncontent of selection\nground-truth deduction\na the …… …eat…\nsoftmax prob 0.01 0.03 …… …0.17…\nspecial tokens with trainable embeddings: ENC DEC\n(b) A deduction step. The T5 encoder reads special\ntokens and the selection s = x4 x5 and generates a\ndeduction autoregressively. It is currently trying to find\nthe token after “only”, and “eat” wins.\nFigure 4: An illustration of how the SLM selection and\ndeduction models in the example procedure of Figure 1.\nselected:2\n∏\nn:xn∈s\npsel(xn |T ,x0)\n∏\nn:xn /∈s\n(1 −psel(xn |T ,x0))\nThen finding the most probable selection is to choose\nthe premises xn that have psel(xn |T ,x0) >0.5.3 This\nprocedure is illustrated in Figure 4a.\nGive a selection s, the deduction T5 model reads\ns and produces a logical deduction y one token after\nanother. The probability ofy under the model ispded(y |\ns). Figure 4b shows a deduction step.\nTraining the SLM version requires a corpus of theo-\nries and goals as well as their ground-truth reasoning\npaths. The selection steps are training examples for psel;\nthe deduction steps are training examples for pded. Tak-\ning Figure 1 as an example, the selection training data\n(green background) is\n• T = {x1,x2,x3,x4} and s = x2 x3\n• T = {x1,x2,x3,x4,x5} and s = x4 x5\n• T = {x1,x2,x3,x4,x5,x6}and s = x1 x6\nand the deduction training data (blue background) is\n• s = x2 x3 and new statement y = x5\n2We treat each xn independently.\n3For each xn, if psel >0.5, we will have psel >1 −psel.\nThat is, including it in s will increase the probability of s.\n11159\n• s = x4 x5 and new statement y = x6\n• s = x1 x6 and new statement y = x7\nThe training objectives for the selection model psel\nand deduction model pded are log psel(s |T ,x0) and\nlog pded(y |s), respectively.\nAppendix B.3 includes more details about the SLM\nversion (e.g., pseudocode for training and inference).\n4.2 LLM Version\nOur LLM uses GPT-3.5-turbo as the selection and de-\nduction models. GPT-3.5 is the current largest and state-\nof-the-art language model that we have access to. We\ninstruct GPT-3.5 to perform selection and deduction by\nfew-shot prompting; please see Appendices B.4 and C.6\nfor technical details and the prompts used in our ex-\nperiments. This is similar to the selection-inference\nframework proposed by Creswell et al. (2023) except\nthat we request GPT-3.5 to propose multiple possible\nselections and deductions at each step. This design al-\nlows us to perform explicit planning for each possible\nselection and deduction and then choose the best option\nbased on planning. Since GPT-3.5 doesn’t give the val-\nues of the probabilities psel and pded, we set u= v= 0\nin the inference methods, conditioning the selection and\ndeduction entirely on the planning signals. The proof\nscore fis still given by the DeBERTa verification model\nthat we introduced in section 3.\n5 Related Work\nReasoning has been a long-standing research topic in\nnatural language processing. For a long time, the ma-\njority of research in this direction has been focused\non simple tasks such as single-sentence language infer-\nence (Bernardi, 2002; Zamansky et al., 2006; MacCart-\nney and Manning, 2009; Angeli et al., 2016; Hu et al.,\n2020; Chen et al., 2021) and single-step commonsense\ninference (Rajani et al., 2019; Latcinnik and Berant,\n2020; Shwartz et al., 2020).\nRecently, there has been an increasing research inter-\nest in the more complex problem of multi-step logical\nreasoning, which we study in this paper. Saha et al.\n(2020), to the best of our knowledge, is the first to pro-\npose an interpretable LM-based model for this problem.\nThey and Tafjord et al. (2021) work on synthesized data\nof limited language variability. The LM-based system\nproposed by Bostrom et al. (2022) has an architecture\nsimilar to the SLM version of our base system except\nthat their inference is one-best decoding without plan-\nning and their deduction model is trained with extra\ndata collected by Bostrom et al. (2021). The selection-\ninference system of Creswell et al. (2023) is similar to\nthe LLM version of our base system but their selection\nand deduction models are few-shot-prompted GPT-3;\nwe compare with them in section 6.3. Liu et al. (2022)\nalso use a similar architecture which they train by rein-\nforcement learning. Weir and Van Durme (2022) embed\nLMs into a backward chaining framework, achieving\nstrong performance in scientific reasoning. Our main\ncontribution is complementary to the previous work: we\nintegrate explicit planning into LM-based reasoning sys-\ntems and design a training method to mitigate the model\nexploitation issue that arises in planning. Our system is\na kind of general model programs (Dohan et al., 2022)—\nespecially those with verification models (Cobbe et al.,\n2021)—which use language models inside as probabilis-\ntic programs and apply disparate inference algorithms\nto the models. Other kinds of approaches to use LMs for\nreasoning include training discriminative models (Clark\net al., 2020; Picco et al., 2021; Ghosal et al., 2022;\nZhang et al., 2023), prompting GPT-3 with spelled-out\nreasoning procedure (Wei et al., 2022; Talmor et al.,\n2020), and distilling GPT-3.5 (Fu et al., 2023).\nAnother straightforward approach for text-based logi-\ncal reasoning is to first translate natural language state-\nments into formal logic expressions and then use a for-\nmal logic inference engine (Weber et al., 2019; Lev-\nkovskyi and Li, 2021; Nye et al., 2021; Lu et al., 2022b;\nBetz and Richardson, 2022). We tried this approach in\nour experiments; please see Appendix C.3 for details.\nAnother research area related to multi-step logical\nreasoning is to reason over graph-structured data. A pop-\nular kind of graph is knowledge graphs, i.e., relational\ngraphs over symbolic tuples (Lao and Cohen, 2010;\nWang et al., 2013; Neelakantan et al., 2015; Cohen et al.,\n2017; Xiong et al., 2017; Chen et al., 2018; Das et al.,\n2018). Another kind of graph is built by linking texts via\nlexical overlap or hyperlink connections (Welbl et al.,\n2018; Yang et al., 2018; Khot et al., 2020, 2021). Meth-\nods in this area involve multi-step navigation through\ngraphs. But they rely on pre-defined symbolic and re-\nlational structures, thus not directly applicable to our\nsetting. Additionally, recent research (Chen and Durrett,\n2019; Min et al., 2019) shows that optimizing the perfor-\nmance on these datasets is not well aligned to improving\nthe models’ fundamental reasoning abilities.\n6 Experiments\nWe carried out a diverse set of experiments that can\ndemonstrate the effectiveness of our proposed methods.\nWe implemented our methods with PyTorch (Paszke\net al., 2019) and Transformers (Wolf et al., 2020). Our\ncode is at https://github.com/cindermond/leap.\n6.1 SLM Experiments on Entailment Bank\nWe first trained and evaluated our SLM version on\nthe standard benchmark Entailment Bank (Dalvi et al.,\n2021) dataset. This dataset is a corpus of human-\nannotated (theory, provable goal, reasoning path) tuples,\nincluding the example in Figure 1. It uses informal lan-\nguage, which closely aligns with how humans engage in\nlogical reasoning during everyday conversations. This\ndataset has two versions: in Version-I, for each pair of\ntheory and goal, all the premises have to be used to\nprove the goal; in Version-II, each theory includes a\nfew distractors that are not useful for proving the goal.\n11160\n0.0 0.2 0.4 0.6 0.8 1.0\nfalse positive rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0true positive rate\nBaseline-T5\nBase System\nSystem A\nSystem B\n(a) ROC curves.\n0.0 0.2 0.4 0.6 0.8 1.0\nthreshold\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy\nBaseline-T5\nBase System\nSystem A\nSystem B (b) Acc curves on positive examples.\n0.0 0.2 0.4 0.6 0.8 1.0\nthreshold\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy\nBaseline-T5\nBase System\nSystem A\nSystem B (c) Acc curves on negative examples.\nFigure 5: Test results with 95% bootstrap confidence intervals (CFs) on Entailment Bank Version-I.\nMETHOD AUROC AUACC POS AUACCNEG F1\nBASELINE -T5 0.67 (0.63, 0.71) 0.53 (0.49, 0.57) 0.75 (0.72, 0.78) 0.62 (0.59, 0.64)\nBASE SYSTEM 0.56 (0.51, 0.60) 0.42 (0.38, 0.47) 0.78 (0.76, 0.81) 0.67 (0.67, 0.67)\nSYSTEM A 0.87 (0.84, 0.89) 0.86 (0.84, 0.89) 0.54 (0.50, 0.57) 0.82 (0.80, 0.84)\nSYSTEM B 0.94 (0.92, 0.95) 0.87 (0.84, 0.89) 0.82 (0.79, 0.85) 0.89 (0.87, 0.91)\nRULE TAKER 0.90 (0.88, 0.93) 0.91 (0.88, 0.94) 0.73 (0.69, 0.77) 0.84 (0.83, 0.86)\nNEURAL UNIF 0.72 (0.68, 0.76) 0.56 (0.56, 0.57) 0.49 (0.48, 0.50) 0.72 (0.71, 0.74)\nGPT-3 (0- SHOT ) - - - 0.89\nTable 1: Test results with 95% bootstrap CFs on Entailment Bank Version-I.\nWe trained the models on Version-I training data, but\nevaluated them on both Version-I and Version-II test\ndata. Experiment details are in Appendix C, includ-\ning data statistics (Table 5) and training details (e.g.,\nhyperparameter tuning in Appendix C.2).\nEvaluation-I: binary classification. We evaluated\nthe abilities of the systems to classify provable and\nnon-provable goals. For this purpose, we gave a non-\nprovable goal to each dev and test theory by selecting it\nfrom other (theory, goal, reasoning path) samples. The\nselection is adversarial: we tuned a pretrained T5 model\nto generate a provable goal given a theory; for each\ntheory T, we looped over all the goals in the dataset that\nare guaranteed to be not provable under T, and chose\nthe one that the T5 thinks is the most probable given T\n(see details in Appendix C).\nFor each given theory T and goal x0, we let the\nsystem generate a reasoning path that tries to prove the\ngoal, and obtain the proof score f(T,x0) of the path.\nGiven a threshold τ ∈(0,1), we say “x0 is provable” if\nf(T,x0) ≥τ and “x0 is not provable” otherwise. For a\nsystematic investigation, we varied τ and plot a receiver\noperating characteristic (ROC) curve for each system;\nthe larger the area under ROC curve (AUROC) is, the\nbetter the system is.\nThe ROC curves are shown in Figure 5a: our LEAP\nSystem-A and System-B substantially and significantly\noutperform the base system and a T5 model (trained\non generating goals given theories); System-B further\nsignificantly outperforms System-A. Surprisingly, our\nbase system underperforms the T5 model even though\nit has learned to spell out its reasoning steps which we\nexpect to help the classification.\nFigure 5b and Figure 5c show the results broken\ndown into the accuracies on the provable goals and\nnon-provable goals, respectively. On provable goals, the\naccuracy is the number of true positive divided by the\ntotal number of test cases; on non-provable goals, the ac-\ncuracy is the number of true negative divided by the total\nnumber of test cases. As we can see, System-A works\nvery well on the provable goals, but performs poorly\non the non-provable goals. That is because System-A\nexploits the verification model by explicit planning: as\nwe have discussed in section 3.3, the proof scores given\nby System-A tend to be high, thus yielding a high rate\nof false positive. System-B works well on both provable\nand non-provable goals: the refined verification model\npver successfully avoided being exploited by planning.\nActual values of the areas under curves are shown in\nTable 1: AUACCpos and AUACCneg correspond to the\ncurves in Figure 5b and Figure 5c, respectively. The\nF1 numbers were computed as follows: we chose an\noptimal threshold τ by maximizing the F1 score on the\ndevelopment set, and then computed F1 on the test set\naccording to the chosen τ.\nFor a comprehensive evaluation, we also compared\nwith three other kinds of methods: GPT-3-davinci with\n0-shot prompting, RuleTaker (Clark et al., 2020), and\nNeural Unification (Picco et al., 2021). GPT-3 achieves\na strong F1 of 0.89, and our System-B performs as\nwell as this strong model. RuleTaker is a discrimina-\ntive method, training a RoBERTa (Liu et al., 2019) to\nperform logical reasoning as binary classification (prov-\nable or not). Neural Unification is also a discriminative\nmethod but has a different architecture than RuleTaker.\nIt requires more sophisticated annotation and prepara-\ntion of the training data than RuleTaker and our methods.\nNeither of them spells out a reasoning process. For these\nmethods, we matched their numbers of trainable param-\neters with our methods for a fair comparison. Overall,\nRuleTaker performs better than our System-A but worse\nthan System-B. Neural Unification performs worse than\n11161\n0.0 0.2 0.4 0.6 0.8 1.0\nfalse positive rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0true positive rate\nBaseline-T5\nBase System\nSystem A\nSystem B\n(a) ROC curves.\n0.0 0.2 0.4 0.6 0.8 1.0\nthreshold\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy\nBaseline-T5\nBase System\nSystem A\nSystem B (b) Acc curves on positive examples.\n0.0 0.2 0.4 0.6 0.8 1.0\nthreshold\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy\nBaseline-T5\nBase System\nSystem A\nSystem B (c) Acc curves on negative examples.\nFigure 6: Test results with 95% bootstrap CFs on Entailment Bank Version-II.\nRuleTaker and our System-A. Note that these results are\northogonal to our main finding that explicit planning is\nhelpful for text-based multi-step logical reasoning.\nAnalysis-I: robustness to size of training data. We\nalso trained the models with (randomly sampled) 50%\nof the training data, and evaluated them on the same\ntest set. It turns out that our System-B still performs\nthe best; see Figure 8 (which looks boringly similar to\nFigure 5) in Appendix C.4 for details.\nAnalysis-II: About the regularization in equation (3).\nWe compared the system B with and without the regu-\nlarization term Ω: without Ω, System-B only achieves\nAUROC = 0.79 (AUROCpos = 0.68 and AUROCneg =\n0.65), worse than System-A. We also evaluated the\ntuned verification models on the MNLI dataset (on\nwhich they were fine-tuned) and found that: the model\ntuned without Ω only achieved 62.0% accuracy; the\nmodel tuned with Ω achieved 91.4% accuracy, almost\nas good as it originally was (91.7%). It means that the\nregularization term indeed helps the verification model\npreserve its ability to judge the entailment relationship.\nAnalysis-III: Robustness to distractors. We inves-\ntigated the robustness of the systems to distractors by\nevaluating them on Version-II test data. Note that they\nwere only trained on Version-I training data. As shown\nin Figure 6, all the systems perform worse than they\ndid on Version-I test data, but the performance drop of\nour systems is much smaller than that of the T5 model.\nIt means that our systems are more robust to the dis-\ntractors. That is perhaps because our systems explicitly\nspell out their reasoning steps and explicit planning can\nhelp the systems (A and B) focus on the premises that\nare actually relevant to the goal at each selection step.\nAnalysis-IV: About model size and denoising. To\nexamine the effect of model size, we reran the main\nexperiments with T5-small (60M) replaced by T5-base\n(220M): using a larger model achieved a consistently\nstronger performance; our planning-based systems still\nsignificantly outperform the base system. We also ex-\nperimented with denoising training of the selection and\ndeduction models: every time we used a training exam-\nple, we randomly permuted the input statements. The\ndenoising training led to a better generalization to the\nMETHOD VERSION -I V ERSION -II\nBASELINE -T5 0.60 (0.55, 0.65) 0.20 (0.16, 0.24)\nBASE SYSTEM 0.46 (0.41, 0.52) 0.29 (0.25, 0.34)\nSYSTEM A 0.80 (0.76, 0.84) 0.44 (0.39, 0.49)\nSYSTEM B 0.88 (0.85, 0.92) 0.63 (0.58, 0.68)\nRULE TAKER 0.83 (0.79, 0.87) 0.73 (0.68, 0.77)\nNEURAL UNIF 0.62 (0.55, 0.69) 0.62 (0.57, 0.67)\nGPT-3 (0- SHOT ) 0.72 0.20\nGPT-3 (5- SHOT ) 0.97 0.96\nGPT-3 (COT) 0.98 0.98\nTable 2: Test accuracy with 95% bootstrap CFs in\nmultiple-choice QA. Accuracy of random guess is 25%.\nevaluation settings with distractors. We also found that\ntraining with distractors (i.e., using Verstion-II training\ndata) significantly improved the results. Detailed results\nand analysis are in Table 6 and Table 7 of Appendix C.4.\nAnalysis-V: About buffer size. The buffer size B\nis a tunable hyperparameter. In our experiments, we\nchose B = 5, a common choice in text generation. A\npilot experiment with B ∈{2,3,5,10}showed that:\na smaller Btends to slightly decrease the accuracy on\npositive samples, but increase it on negative samples;\na larger B tends to slightly increase the accuracy on\npositive samples, but decreases it on negative samples;\noverall, there are only tiny changes in AUROC, which\ndepends on accuracies on both kinds of samples.\nAnalysis-VI: Computation Cost. In our experiments,\nwe used B = 5 and D = 2, i.e., a buffer size of 5 and\na roll-out depth of 2. According to the theoretical anal-\nysis in section 3.2, the planing-based inference should\nbe 1 + 2BD = 21 times slower than the no-planning\nmethod. In practice, it takes an average of 2.8 seconds\nfor the no-planning method to work on a theory-goal\npair from Entailment Bank. For the planning-based in-\nference, it takes an average of31 seconds, only 11 times\nslower. The implementation is faster than the theoretical\nanalysis thanks to tensorization and parallelism.\nEvaluation-II: Multiple-Choice QA. We further\nevaluated the systems in a multiple-choice question an-\nswering (QA) setting. Particularly, given a theory T\nin Entailment Bank, each system is asked to select the\nprovable goal from four choices {x(1)\n0 ,x(2)\n0 ,x(3)\n0 ,x(4)\n0 }:\none of them is the ground-truth provable goal while the\n11162\nMETHOD ACC\nBASE SYSTEM 0.68 (0.65, 0.71)\nSYSTEM A 0.84 (0.82, 0.87)\nSYSTEM B 0.85 (0.83, 0.87)\nTable 3: Dev accuracy with 95%\nbootstrap CFs on QASC.\nMETHOD DEPTH =1 D EPTH =3 D EPTH =5\nCOT 0.71 (0.62, 0.80) 0.57 (0.47, 0.67) 0.52 (0.42, 0.62)\nSI 0.88 (0.81, 0.94) 0.51 (0.41, 0.61) 0.45 (0.35, 0.55)\nSYSTEM A 0.90 (0.84, 0.95) 0.55 (0.45, 0.65) 0.55 (0.45, 0.65)\nTable 4: Accuracy with 95% bootstrap confidence intervals on PrOntoQA.\nThe “depth” denotes the number of ground-truth reasoning steps.\nothers are negative choices selected by a tuned T5.\nWe took the systems trained in section 6.1 and eval-\nuated them on the Version-I and Version-II of this\nmultiple-choice task: in the Version-II setting, each the-\nory has a few distractors, so it is more challenging than\nVersion-I. For each theory, a system tries to prove each\nchoice x(c)\n0 , ranks the four choices by their proof scores\nf(T,x(c)\n0 ), and then chooses the one with the highest\nscore. The systems were evaluated by accuracy. As\nshown in Table 2, the systems behave similarly as they\ndo on the binary classification: in both Version-I and\nVersion-II settings, System-A and System-B perform\nsignificantly better than the baselines, and System-B\nsignificantly outperforms System-A.\nWe also evaluated GPT-3-davinci with 0-shot, 5-shot,\nand chain-of-thought (COT) prompting (Brown et al.,\n2020; Wei et al., 2022). The COT prompts include\nthe ground-truth reasoning paths of the correct choices;\nexamples are in Appendix C.5. Our full system outper-\nforms 0-shot GPT-3, but underperforms 5-shot and COT\nGPT-3. Interestingly, 0-shot GPT-3 works worse than\nrandom guess when theories have distractors, which\nindicates the difficulty of this problem. In addition,\nwe evaluated RuleTaker and Neural Unification, with\ntheir numbers of trainable parameters matched with our\nmethods. In the Version-I setting, they both perform\nworse than our System-B and Neural Unification per-\nforms even worse than System-A. Interestingly, they\nseem to be more robust to distractors: in the Versition-II\nsetting, Neural Unification performs competitive to our\nSystem-B, and RuleTaker performs significantly better\nthan System-B. However, these methods do not generate\ninterpretable reasoning processes.\n6.2 SLM Experiments on QASC\nWe also trained and evaluated the systems on the QASC\ndataset (Khot et al., 2020), a multiple-choice question\nanswering dataset where each question has eight candi-\ndate answers. Each training QA pair has two premises\nand a deduction, which can be used to train our de-\nduction model. Each development QA pair has two\npremises so the reasoning system only needs to do a\nstep of deduction but no selection. Test QA pairs have\nno premises given and one has to search through a pool\nof millions of statements to find the relevant premises,\nwhich is not the focus of this paper. So we only evalu-\nated the systems on the development set. The results are\nin Table 3. Although this data only requires one step of\nreasoning, the planning-based systems still significantly\noutperform the base system, suggesting that explicit\nplanning is indeed helpful for LM-based reasoning.\n6.3 LLM Experiments on PrOntoQA\nWe evaluated the LLM version on the “fictional” version\nof the PrOntoQA dataset (Saparov and He, 2023). It is\na binary classification task like Entailment Bank (sec-\ntion 6.1), but it is more challenging to large language\nmodels such as GPT-3.5 since its logical statements are\nabout fictional characters (e.g., wumpus), meaning that\na large model can not bypass the reasoning and draw\ncorrect conclusions by commonsense or memorization.\nThe main results are shown in Table 4. In all cases,\nour planning-based system outperforms the selection-\ninference (SI) method, meaning that explicit planning is\nconsistently helpful. In most cases, our planning-based\nsystem performs better than the strong chain-of-thought\n(COT) prompting. We also experimented with the De-\nBERTa model tuned on the Entailment Bank training\ndata (sections 3.3 and 6.1) and found that it couldn’t\nimprove the performance on PrOntoQA. Appendix C.6\nincludes more details about this set of experiments as\nwell as more results.\n7 Conclusion\nIn this paper, we presented LEAP, an LM-based logi-\ncal reasoning system that integrates explicit planning\ninto the inference method. We also proposed a method\nthat learns to prevent the explicit planning from being\nmisguided. Our proposed methods exhibit intriguing\ntechnical connections to other reasoning systems and\ncan be likened to the deliberative System 2 in “dual\nprocess” theories of reasoning. In our experiments,\nour planning-based system outperforms strong baseline\nmethods including the selection-inference method and\nchain-of-thought prompting. We will discuss several ex-\nciting avenues for further improvements in Appendix A.\nAcknowledgments\nThis work was supported by a research gift to the last\nauthor by Adobe Research. We thank the anonymous\nEMNLP reviewers and meta-reviewer for their construc-\ntive feedback. We thank our colleagues at UChicago\nand TTIC for helpful discussion. We also thank Hao Tan\nat Adobe Research, Yisi Sang at Apple, Benjamin Van\nDurme at Johns Hopkins University, and David Dohan\nat OpenAI for their helpful comments.\nLimitations\nThe main limitation of our proposed framework is that\nit requires more computation than the baseline methods\nthat do not perform explicit planning. As discussed\nin section 3.2, the no-planning methods are like the\n11163\nintuitive and fast System 1 (Evans, 2003) while our\nmethods are like the analytical and slow System 2: after\nall, more analysis consumes more computation and thus\nour framework is less energy-efficient. This limitation\nhas inspired us to explore new methods such as bandit\nlearning to switch between two types of systems and\nmore efficient planning (see Appendix A).\nEthics Statement\nOur work complies with the ACL Ethics Policy. It aims\nto build more intelligent language-based logical reason-\ning systems which would have a broad positive impact\nto the society. For example, in our daily life, an intelli-\ngent logical reasoning system may help us verify facts\nand identify fake news; in legal domain, it may work as\nan automatic paralegal and assist lawyers with their doc-\nument processing and decision making; in education,\nit may help students reason about their mistakes and\nimprove learning experience. Meanwhile, our methods\nshare the same risks as other machine learning methods,\nsuch as misusage, containing data bias, and suffering\nfrom adversarial attacks. However, this paper is orthog-\nonal to the research efforts to mitigate these issues.\nReferences\nGabor Angeli, Neha Nayak, and Christopher D. Man-\nning. 2016. Combining natural logic and shallow\nreasoning for question answering. In Proceedings of\nthe Annual Meeting of the Association for Computa-\ntional Linguistics (ACL).\nRaffaella Anna Bernardi. 2002. Reasoning with polarity\nin categorial type logic. Ph.D. thesis.\nGregor Betz and Kyle Richardson. 2022. DeepA2: A\nmodular framework for deep argument analysis with\npretrained neural Text2Text language models. In\nProceedings of the 11th Joint Conference on Lexical\nand Computational Semantics.\nKaj Bostrom, Zayne Sprague, Swarat Chaudhuri, and\nGreg Durrett. 2022. Natural language deduction\nthrough search over statement compositions. In Find-\nings of the Conference on Empirical Methods in Nat-\nural Language Processing (Findings of EMNLP).\nKaj Bostrom, Xinyu Zhao, Swarat Chaudhuri, and Greg\nDurrett. 2021. Flexible generation of natural lan-\nguage deductions. In Proceedings of the Conference\non Empirical Methods in Natural Language Process-\ning (EMNLP).\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in Neural Information Processing\nSystems (NeurIPS).\nJifan Chen and Greg Durrett. 2019. Understanding\ndataset design choices for multi-hop reasoning. In\nProceedings of the Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics (NAACL).\nWenhu Chen, Wenhan Xiong, Xifeng Yan, and William\nWang. 2018. Variational knowledge graph reasoning.\nIn Proceedings of the Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics (NAACL).\nZeming Chen, Qiyue Gao, and Lawrence S. Moss. 2021.\nNeuralLog: Natural language inference with joint\nneural and logical reasoning. In Proceedings of\n*SEM 2021: The Tenth Joint Conference on Lexical\nand Computational Semantics.\nPeter Clark, Oyvind Tafjord, and Kyle Richardson. 2020.\nTransformers as soft reasoners over language. In\nProceedings of the International Joint Conference on\nArtificial Intelligence (IJCAI).\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavarian,\nMark Chen, Heewoo Jun, Lukasz Kaiser, Matthias\nPlappert, Jerry Tworek, Jacob Hilton, Reiichiro\nNakano, et al. 2021. Training verifiers to solve math\nword problems. arXiv preprint arXiv:2110.14168.\nWilliam W Cohen, Fan Yang, and Kathryn Rivard\nMazaitis. 2017. Tensorlog: Deep learning meets\nprobabilistic dbs. Journal of Artificial Intelligence\nResearch (JAIR).\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2023. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. In Pro-\nceedings of the International Conference on Learning\nRepresentations (ICLR).\nBhavana Dalvi, Peter Jansen, Oyvind Tafjord, Zheng-\nnan Xie, Hannah Smith, Leighanna Pipatanangkura,\nand Peter Clark. 2021. Explaining answers with en-\ntailment trees. In Proceedings of the Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP).\nRajarshi Das, Shehzaad Dhuliawala, Manzil Zaheer,\nLuke Vilnis, Ishan Durugkar, Akshay Krishnamurthy,\nAlex Smola, and Andrew McCallum. 2018. Go for\na walk and arrive at the answer: Reasoning over\npaths in knowledge bases using reinforcement learn-\ning. In Proceedings of the International Conference\non Learning Representations (ICLR).\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Jacob\nAustin, David Bieber, Raphael Gontijo Lopes, Yuhuai\nWu, Henryk Michalewski, Rif A Saurous, Jascha\nSohl-Dickstein, et al. 2022. Language model cas-\ncades. arXiv preprint arXiv:2207.10342.\nJonathan St BT Evans. 2003. In two minds: dual-\nprocess accounts of reasoning. Trends in cognitive\nsciences.\nYao Fu, Hao Peng, Litu Ou, Ashish Sabharwal, and\nTushar Khot. 2023. Specializing smaller language\nmodels towards multi-step reasoning. In Proceedings\nof the International Conference on Machine Learning\n(ICML).\n11164\nDeepanway Ghosal, Navonil Majumder, Rada Mihal-\ncea, and Soujanya Poria. 2022. Two is better than\nmany? binary classification as an effective approach\nto multi-choice question answering. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention. In Proceedings of\nthe International Conference on Learning Represen-\ntations (ICLR).\nHai Hu, Qi Chen, Kyle Richardson, Atreyee Mukher-\njee, Lawrence S. Moss, and Sandra Kuebler. 2020.\nMonaLog: a lightweight system for natural language\ninference based on monotonicity. In Proceedings of\nthe Society for Computation in Linguistics.\nDaniel Jurafsky and James H. Martin. 2000. Speech\nand Language Processing: An Introduction to Natu-\nral Language Processing, Computational Linguistics,\nand Speech Recognition.\nTushar Khot, Peter Clark, Michal Guerquin, Peter\nJansen, and Ashish Sabharwal. 2020. Qasc: A dataset\nfor question answering via sentence composition. In\nProceedings of the AAAI Conference on Artificial\nIntelligence (AAAI).\nTushar Khot, Daniel Khashabi, Kyle Richardson, Pe-\nter Clark, and Ashish Sabharwal. 2021. Text mod-\nular networks: Learning to decompose tasks in the\nlanguage of existing models. In Proceedings of the\nConference of the North American Chapter of the\nAssociation for Computational Linguistics – Human\nLanguage Technologies (NAACL HLT).\nDiederik P Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In Proceedings\nof the International Conference on Learning Repre-\nsentations (ICLR).\nNi Lao and William W Cohen. 2010. Relational re-\ntrieval using a combination of path-constrained ran-\ndom walks. Machine Learning.\nVeronica Latcinnik and Jonathan Berant. 2020. Explain-\ning question answering models through text genera-\ntion. arXiv preprint arXiv:2004.05569.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt tun-\ning. In Proceedings of the Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nOleksii Levkovskyi and Wei Li. 2021. Generating pred-\nicate logic expressions from natural language. In\nSoutheastCon.\nTengxiao Liu, Qipeng Guo, Xiangkun Hu, Yue Zhang,\nXipeng Qiu, and Zheng Zhang. 2022. RLET: A re-\ninforcement learning based approach for explainable\nQA with entailment trees. In Proceedings of the Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP).\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A\nrobustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nXiming Lu, Sean Welleck, Peter West, Liwei Jiang,\nJungo Kasai, Daniel Khashabi, Ronan Le Bras, Lian-\nhui Qin, Youngjae Yu, Rowan Zellers, Noah A. Smith,\nand Yejin Choi. 2022a. NeuroLogic a*esque de-\ncoding: Constrained text generation with lookahead\nheuristics. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics (NAACL).\nXuantao Lu, Jingping Liu, Zhouhong Gu, Hanwen Tong,\nChenhao Xie, Junyang Huang, Yanghua Xiao, and\nWenguang Wang. 2022b. Parsing natural language\ninto propositional and first-order logic with dual re-\ninforcement learning. In Proceedings of the 29th\nInternational Conference on Computational Linguis-\ntics.\nZhuang Ma and Michael Collins. 2018. Noise con-\ntrastive estimation and negative sampling for condi-\ntional models: Consistency and statistical efficiency.\nIn Proceedings of the Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nBill MacCartney and Christopher D. Manning. 2009.\nAn extended model of natural logic. In Proceedings\nof the Eight International Conference on Computa-\ntional Semantics.\nSewon Min, Eric Wallace, Sameer Singh, Matt Gardner,\nHannaneh Hajishirzi, and Luke Zettlemoyer. 2019.\nCompositional questions do not necessitate multi-hop\nreasoning. In Proceedings of the Annual Meeting of\nthe Association for Computational Linguistics (ACL).\nArvind Neelakantan, Benjamin Roth, and Andrew Mc-\nCallum. 2015. Compositional vector space models\nfor knowledge base inference. In Proceedings of the\nAAAI Conference on Artificial Intelligence (AAAI).\nMaxwell Nye, Michael Tessler, Josh Tenenbaum, and\nBrenden M Lake. 2021. Improving coherence and\nconsistency in neural sequence models with dual-\nsystem, neuro-symbolic reasoning. In Advances in\nNeural Information Processing Systems (NeurIPS).\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch: An\nimperative style, high-performance deep learning li-\nbrary. In Advances in Neural Information Processing\nSystems (NeurIPS). Curran Associates, Inc.\nGabriele Picco, Hoang Thanh Lam, Marco Luca Sbodio,\nand Vanessa Lopez Garcia. 2021. Neural unification\nfor logic reasoning over natural language. InFindings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (Findings of EMNLP).\n11165\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, Peter J Liu, et al. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research\n(JMLR).\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain yourself!\nleveraging language models for commonsense rea-\nsoning. In Proceedings of the Annual Meeting of the\nAssociation for Computational Linguistics (ACL).\nKyle Richardson and Ashish Sabharwal. 2020. What\ndoes my QA model know? devising controlled probes\nusing expert knowledge. Transactions of the Associa-\ntion for Computational Linguistics (TACL).\nStuart Russell and Peter Norvig. 2010. Artificial Intelli-\ngence: A Modern Approach. Prentice Hall.\nSwarnadeep Saha, Sayan Ghosh, Shashank Srivastava,\nand Mohit Bansal. 2020. PRover: Proof generation\nfor interpretable reasoning over rules. In Proceedings\nof the Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP).\nAbulhair Saparov and He He. 2023. Language models\nare greedy reasoners: A systematic formal analysis of\nchain-of-thought. In Proceedings of the International\nConference on Learning Representations (ICLR).\nVered Shwartz, Peter West, Ronan Le Bras, Chandra\nBhagavatula, and Yejin Choi. 2020. Unsupervised\ncommonsense question answering with self-talk. In\nProceedings of the Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nRichard S Sutton and Andrew G Barto. 2018. Reinforce-\nment learning: An introduction.\nOyvind Tafjord, Bhavana Dalvi, and Peter Clark. 2021.\nProofWriter: Generating implications, proofs, and\nabductive statements over natural language. In Find-\nings of the Annual Meeting of the Association for\nComputational Linguistics (Findings of ACL).\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge. In Advances in Neural\nInformation Processing Systems (NeurIPS).\nWilliam Yang Wang, Kathryn Mazaitis, and William W\nCohen. 2013. Programming with personalized pager-\nank: a locally groundable first-order probabilistic\nlogic. In Proceedings of the ACM International Con-\nference on Information & Knowledge Management\n(CIKM).\nLeon Weber, Pasquale Minervini, Jannes Münchmeyer,\nUlf Leser, and Tim Rocktäschel. 2019. NLProlog:\nReasoning with weak unification for question answer-\ning in natural language. In Proceedings of the Annual\nMeeting of the Association for Computational Lin-\nguistics (ACL).\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. Advances in Neural Information\nProcessing Systems (NeurIPS).\nNathaniel Weir and Benjamin Van Durme. 2022. Dy-\nnamic generation of interpretable inference rules in\na neuro-symbolic expert system. arXiv preprint\narXiv:2209.07662.\nJohannes Welbl, Pontus Stenetorp, and Sebastian Riedel.\n2018. Constructing Datasets for Multi-hop Read-\ning Comprehension Across Documents. Transac-\ntions of the Association for Computational Linguis-\ntics (TACL).\nAdina Williams, Nikita Nangia, and Samuel R. Bow-\nman. 2018. A broad-coverage challenge corpus for\nsentence understanding through inference. In Pro-\nceedings of the Conference of the North American\nChapter of the Association for Computational Lin-\nguistics – Human Language Technologies (NAACL\nHLT).\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pierric\nCistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nWenhan Xiong, Thien Hoang, and William Yang Wang.\n2017. Deeppath: A reinforcement learning method\nfor knowledge graph reasoning. In Proceedings of\nthe Conference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP).\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the Conference on Empirical Meth-\nods in Natural Language Processing (EMNLP).\nMo Yu, Shiyu Chang, Yang Zhang, and Tommi Jaakkola.\n2019. Rethinking cooperative rationalization: Intro-\nspective extraction and complement control. In Pro-\nceedings of the Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nAnna Zamansky, Nissim Francez, and Yoad Winter.\n2006. A ‘natural logic’inference system using the\nlambek calculus. Journal of Logic, Language and\nInformation.\nHonghua Zhang, Liunian Harold Li, Tao Meng, Kai-Wei\nChang, and Guy Van den Broeck. 2023. On the para-\ndox of learning to reason from data. In Proceedings\nof the International Joint Conference on Artificial\nIntelligence (IJCAI).\n11166\nA Future Extensions\nOur experiments have inspired us to explore several\nexciting avenues for further improvements.\nThe first is to jointly refine the selection, deduction,\nand verification models. In this paper, we have already\nshown that adversarially refining the verification model\nwill significantly improve the performance. So a natural\nnext step is to adversarially refine the selection and de-\nduction models in response to the updated verification\nmodel. Allowing components of a system to adversari-\nally refine one another has been shown useful in natural\nlanguage processing (Yu et al., 2019).\nThe second is to developimplicit planning methods to\nimprove inference efficiency. In reinforcement learning,\nexplicit planning is often only used to help learn a value\nfunction during training; during inference, calling a\nvalue function is like planning implicitly but faster than\nexplicit planning. This kind of methods can apply to\nour setting. Another way to improve efficiency is to\nlearn a bandit that could cleverly switch between the no-\nplanning “System 1” and our planning-based “System 2”\nsuch that we only spend more computation in the more\ndifficult cases.\nAnother direction is to leverage unlabeled data, i.e.,\ndata without human-annotated reasoning paths. Such\ndata is less expensive to collect. An LM-based reasoning\nsystem may be able to benefit from (the indirect training\nsignals of) such data by self-supervised learning.\nB Method Details\nIn this section, we give details of our methods.\nB.1 Reasoning Process Details\nAlgorithm 1 gives a detailed explanation for how our\ninference method works. When D = 0, it is the naive\nmethod. When D≥1, it is the inference with explicit\nplanning. During selection, we constrain the model to\nonly select two premises for a more controllable behav-\nior. When we compute the proof score we only consider\nthe newly generated deductions for convenience. Its\neffect to results is negligible since later deductions tend\nto more directly prove the goal.\nAlgorithm 2 is designed to select a set of statements\nfrom the current theory T, with the goal of inferring x0.\nWe fix the size of the selection set to 2 in our experi-\nments, but in principle this restriction can be removed.\nAlgorithm 3 draws Bded new deductions. Their SLM\nversions are Algorithms 5 and 6 and the LLM versions\nare Algorithms 8 and 9.\nB.2 Details of Tuning the Verification Model\nWe use the soft prompt tuning method (Lester et al.,\n2021): we augment the input with a few special tokens\nand the only trainable parameters are the embeddings\nof those tokens; it is illustrated in Figure 7.\nWhere do we get x0, y, ¯x0, and ¯y? Recall that we\nhave a training corpus of theories and goals as well\nAlgorithm 1 Reasoning (Inference) with Our System\nHyperparam: max number of inference steps M;\ndepth of planning D(D= 0 means “no planning”);\ninference beam size Binf\nInput: theory T = {x1,x2,..., xN }and goal x0;\nselection model psel, deduction model pded;\nverification model pver\nOutput: reasoning path Rwith proof score f\n1: procedure INFERENCE (T,x0,psel,pded,pver)\n2: ▷has access to M, Binf, D\n3: B← PriorityQueue(Binf)\n4: ▷max size is Binf ; priority is first element of tuple\n5: B.add((0,∅,T,−∞))\n6: ▷init with empty path and current theory\n7: for m= 1 to M :\n8: ▷do inference at each step\n9: ▷selection at step m\n10: Bold ←B; B← PriorityQueue(Binf)\n11: for gb,Rb,Tb,fb in Bold :\n12: ▷gis log-prob of path and f is its proof score\n13: Sb ←SELECT (Tb,x0,psel)\n14: if D> 0 :\n15: ▷rank selections based on D-step roll-outs\n16: Sb ←PLAN S(Tb,x0,Sb,psel,pded,pver)\n17: for uk,sk in Sb :\n18: ▷add expanded path into priority queue\n19: ▷priority score changes by uk\n20: B.add((gb + uk,Rb + {sk},Tb,fb))\n21: ▷Bhas a fixed size Binf: if |B|>Binf\n22: ▷auto-delete lowest-priority element\n23: ▷deduction at step m\n24: Bold ←B; B← PriorityQueue(Binf)\n25: for gb,Rb,Tb,fb in Bold :\n26: sb ←the most recent selection in Rb\n27: Yb ←DEDUCE (sb,pded)\n28: if D> 0 :\n29: ▷rank deductions based on D-step roll-outs\n30: Yb ←PLAN D(Tb,x0,Yb,psel,pded,pver)\n31: for vk,yk in Yb :\n32: B.add((gb+vk,Rb+{yk},Tb+{yk},fb))\n33: for gb,Rb,Tb,fb in B :\n34: yb ←the most recent deduction in Rb\n35: ▷if yb entails x0 better than any prev deduction\n36: ▷update proof score of path Rb\n37: if pver(x0 |yb) >fb : fb ←pver(x0 |yb)\n38: ▷choose reasoning path with highest proof score\n39: bmax ←argmaxb fb\n40: return Rbmax ,fbmax\nx0 : Eagles are carnivores.\npretrained DeBERTa with classification head\nVER x6 :  Eagles only eat animals.\nspecial tokens with trainable embeddings: VER\nCLS\npver( x0 | x6 ) \nFigure 7: The structure of the verification model.\n11167\nAlgorithm 2 Selection Subroutine\nHyperparam: selection beam size Bsel\nInput: current theory T = {x1,..., xN+m}and goal\nx0; selection model psel\nOutput: selections with their scores {(uk,sk)}\n1: procedure SELECT (T,x0,psel)\n2: ▷generic method for illustration only\n3: ▷in practice, we call the SLM or LLM version\n4: ▷see Algorithm 5 for SLM version\n5: ▷see Algorithm 8 for LLM version\n6: ▷has access to Bsel\n7: ▷return list Swhich contains Bsel scored selections\n8: ▷each scored selection is (u,s)\n9: ▷score uis defined in section 3.2\n10: return S\n11: procedure ONEBEST SELECT (T,x0,psel)\n12: ▷only keeps selection with highest score\n13: S← SELECT (T,x0,psel)\n14: (u,s) ←highest-scored element in S\n15: return s\nAlgorithm 3 Deduction Subroutine\nHyperparam: deduction beam size Bded\nInput: current selection s of statements;\ndeduction model pded\nOutput: deductions with their scores {(vk,yk)}\n1: procedure DEDUCE (s, pded)\n2: ▷generic method for illustration only\n3: ▷in practice, we call the SLM or LLM version\n4: ▷see Algorithm 6 for SLM version\n5: ▷see Algorithm 9 for LLM version\n6: ▷has access to Bded\n7: ▷return list Ywhich contains Bded scored deductions\n8: ▷each scored deduction is (v,y)\n9: ▷score vis defined in section 3.2\n10: return Y\n11: procedure ONEBEST DEDUCE (s, pded)\n12: ▷only keeps deduction with highest score\n13: Y← DEDUCE (s,pded)\n14: (v,y) ←element in Ywith highest v\n15: return y\nas their ground-truth reasoning paths. For each pair\nof theory T and provable goal x0, we could randomly\nsample a deduction y from its ground-truth reasoning\npath. We use the goal of another training example as our\nnon-provable goal ¯x0, call the planning-based inference\nmethod to get a reasoning path, and sample a deduction\nfrom the reasoning path as our ¯y.\nAlgorithm 4 shows how we refine the verification\nmodel using the contrastive loss with regularization.\nAlgorithm 4 Refining Verification Model\nInput: provable goal x0 and gold reasoning path R;\nnon-provable goal ¯x0 and model-generated path ¯R;\nverification model pver\nOutput: updated verification model pver\n1: procedure REFINE (x0,R,¯x0, ¯R,pver)\n2: ▷refining procedure\n3: p−\nver ←a copy of pretrained pver\n4: ▷sample deductions from reasoning paths\n5: randomly draw y from deductions in R\n6: randomly draw ¯y from deductions in ¯R\n7: ▷refine verification model\n8: ℓ←LOSS VER(x0,y,¯x0,¯y,pver,p−\nver)\n9: compute ∇ℓwrt. trainable parameters θver of pver\n10: update θver with chosen optimization method\n11: return pver\n12: procedure LOSS VER(x0,y,¯x0,¯y,pver,p−\nver)\n13: ▷contrastive loss\n14: ℓ←log pver(¯x0|¯y)\npver(¯x0|¯y)+pver(x0|y)\n15: ▷compute regularization\n16: ℓ−= p−\nver(x0 |y) logpver(x0 |y)\n17: ℓ−= (1 −p−\nver(x0 |y)) log(1−pver(x0 |y))\n18: return ℓ\nB.3 SLM Details\nWe give SLM details in this section.\nSelection model. The selection model psel uses a\npretrained encoder-decoder model T5 (Raffel et al.,\n2020). The encoder reads a context string concatenat-\ning the goal x0 and the premises x1,..., xN of cur-\nrent theory T; the decoder computes the probabilities\npsel(xn |T ,x0) that each premise xn is selected in the\nattempt to prove the goal x0. It is illustrated in Fig-\nure 4a: besides the statements, T5 also reads a few\nspecial tokens (ENC,SP0,SP1,..., SPN ,DEC); its de-\ncoder gives a hidden state h, which is involved in com-\nputing psel(xn |T ,x0)\ndef\n= σ(h⊤wn) where wn is the\nembedding of SPn. For training and inference efficiency,\nwe keep the pretrained T5 frozen so the only trainable\nparameters of the selection modelpsel—denoted as θsel—\nare the embeddings of the special tokens. The pseu-\ndocode of using it for inference is in Algorithm 5.\nDeduction model. Given the selection s, the deduc-\ntion model pded produces a logical deduction y by com-\nbining the premises in s. The new statement y is added\nto the theory T whose size is then increased by one;\n11168\ntherefore, for a theory of size N, we also denote y\nas xN+1. The deduction model pded uses another pre-\ntrained T5. As shown in Figure 4b, its encoder reads an\ninput string concatenating the selected premises along\nwith a few special tokens; its autoregressive decoder pro-\nduces a deduction one token after another. Its trainable\nparameters θded are the embeddings of the special to-\nkens. The pseudocode of deploying it is in Algorithm 6.\nAlgorithm 5 Selection Subroutine for SLM\nHyperparam: selection beam size Bsel\nInput: current theory T = {x1,..., xN+m}and goal\nx0; prompted encoder-decoder language model psel\nOutput: selections with their scores {(uk,sk)}\n1: procedure SELECT (T,x0,psel)\n2: ▷has access to Bsel\n3: ▷build context by concatenating hypothesis and theory\n4: c = SP0+x0+SP1+x1+... +SPN+m+xN+m\n5: for i= 1 to N + m:\n6: ▷compute prob that each statement is selected\n7: pi ←psel(SPi|c)\n8: S← PriorityQueue(Bsel)\n9: ▷max size is Bsel; priority is first element of tuple\n10: for i= 1 to N + m:\n11: for j = i+ 1 to N + m:\n12: sk ←xi + xj\n13: uk ←log pi +log pj +∑\nℓ̸=i,ℓ̸=j log(1−pℓ)\n14: S.add((uk,sk))\n15: ▷if Bis larger than Bsel, element with\n16: ▷lowest priority will be automatically deleted\n17: return S\nAlgorithm 6 Deduction Subroutine for SLM\nHyperparam: deduction beam size Bded\nInput: current selection s of statements;\nprompted encoder-decoder language model pded\nOutput: deductions with their scores {(vk,yk)}\n1: procedure DEDUCE (s, pded)\n2: ▷has access to Bded\n3: ▷has access to standard beam search implementation\n4: Y← BEAM SEARCH (pded,Bded,s)\n5: ▷assume:\n6: ▷BEAM SEARCH gives a list of tuples {(vk,yk)}\n7: ▷text string yk sorted in descending order of vk\n8: return Y\nTraining. Algorithm 7 elaborates how the SLM selec-\ntion and deduction models are trained. We use prompt-\nlearning because we do not want to distort the pretrained\nweights too much. It is well known that pretrained lan-\nguage models have already captured substantial amounts\nof commonsense knowledge such as hypernymy (A is\na type of B) and meronymy (A is part of B) (Richard-\nson and Sabharwal, 2020); we would like to keep such\nknowledge to benefit our settings.\nAlgorithm 7 Training for SLM\nInput: theory T = {x1,..., xN }and goal x0;\nreasoning path R; verification model pver\nselection model psel, deduction model pded\nOutput: updated models psel and pded\n1: procedure TRAIN (R,T,x0,psel,pded,pver)\n2: ▷training method for selection and deduction models\n3: ▷init extended theory that will include deduction\n4: ˜T ←T\n5: for m= 1 to |R|/2 :\n6: ▷loop over each step of selection and deduction\n7: sm ←mth selection ▷i.e., (2m−1)th entry in R\n8: ym ←mth deduction ▷i.e., (2m)th element in R\n9: ▷train selection model\n10: ℓ←LOSS SEL( ˜T,sm,psel)\n11: compute ∇ℓwrt. trainable params θsel of psel\n12: update θsel with chosen optimization method\n13: ▷train deduction model\n14: ℓ←LOSS DED(sm,ym,pded)\n15: compute ∇ℓwrt. trainable params θded of pded\n16: update θded with chosen optimization method\n17: ▷extend theory with new deduction\n18: ˜T ←˜T+ {ym}\n19: return psel,pded\n20: procedure LOSS SEL(T,s,psel)\n21: ▷construct context for selecting statements from theory\n22: c ←SP0+x0+SP1+x1+... +SPN+m+xN+m\n23: ℓ←0 ▷loss is negative log-likelihood of selection\n24: for i= 1 to N + m:\n25: pi ←psel(SPi|c) ▷prob that xi is included in s\n26: if xi in s : ∆ℓ←log pi else ∆ℓ←log(1 −pi)\n27: ℓ←ℓ−∆ℓ▷ update ℓwith minus log-probability\n28: return ℓ\n29: procedure LOSS DED(s,y,pded)\n30: ▷loss is negative log-prob of deduction under model\n31: ℓ←−log pded(y |s)\n32: ▷log pded(y |s) sums log-probabilities of tokens in y\n33: return ℓ\n11169\nB.4 LLM Details\nWe give LLM details in this section. For selection, we\nuse a large language model as a black box and prompt\nit to choose several different multi-premise selections\nfrom the given theory T. The pseudocode is in Algo-\nrithm 8. Below is the prompt template:\n# few−shot examples to demonstrate selection\n# see Appendix C.6 for an example\n...\n# end of demonstration\nPlease refer to these examples, select four\npairs of indexes from the theory (e.g. 12 and\n3 / 12 and 6 / 12 and 7 / 12 and 11) that\ncan potentially help us answer the question,\nno need to say anything else.\nYou must choose four pairs even if there are\nno valid selections.\n# theory and question/goal of interest\nFor deduction, we also use a large language model\nas a black box and prompt it to draw new deductions\nconditioned on a given selection s. The pseudocode is\nin Algorithm 9. The prompt template is as follows:\n# few−shot examples to demonstrate deduction\n# see Appendix C.6 for an example\n...\n# end of demonstration\nPlease refer to these examples and generate\nthe inference.\n# selection of statements of interest\nAlgorithm 8 Selection Subroutine for LLM\nHyperparam: selection beam size Bsel\nInput: current theory T = {x1,..., xN+m}and goal\nx0; selection model psel\nOutput: selections with their scores {(uk,sk)}\n1: procedure SELECT (T,x0,psel)\n2: ▷has access to Bsel\n3: prompt LLM to select Bsel different multi-\npremise selections s from the theory T\n4: ▷prompt templates are in Appendix B.4\n5: each selection s is assigned a score u= 0\n6: construct list Sto contain the multiple (u,s)\n7: return S\nB.5 Details of Planning-Based Methods\nAlgorithm 10 illustrates the details of how we use ex-\nplicit planning for selection. The method considers how\neach selection could affect future in Dsteps. One-best\nsearch is applied in the roll-out process to simplify the\nAlgorithm 9 Deduction Subroutine for LLM\nHyperparam: deduction beam size Bded\nInput: current selection s of statements;\ndeduction model pded\nOutput: deductions with their scores {(vk,yk)}\n1: procedure DEDUCE (s, pded)\n2: ▷has access to Bded\n3: prompt LLM to draw Bded new deductions\n4: ▷prompt templates are in Appendix B.4\n5: each deduction y is assigned a score v= 0\n6: construct list Yto contain the multiple (v,y)\n7: return Y\nAlgorithm 10 Planning for Selection\nHyperparam: deduction beam width Bded;\ndepth of planning D; planning scale α\nInput: current theory T = {x1,..., xN+m}and goal\nx0; verification model pver\nselection candidates at current stepS= {(uk,sk)};\nselection model psel and deduction model pded\nOutput: selections with updated scores {(uk,sk)}\n1: procedure PLAN S(T,x0,S,psel,pded,pver)\n2: ▷has access to Bded, D, α\n3: ▷init hypothetical extended theory\n4: for uk,sk in S: ˜Tk ←T\n5: for uk,sk in S:\n6: ▷iterate over all candidate selections\n7: ▷find hypothetical next-step deduction\n8: ˜yk ←ONEBEST DEDUCE (sk,pded)\n9: ▷extend theory with new deduction\n10: ˜Tk ←˜Tk + {˜yk}\n11: ▷planning with roll-outs\n12: ▷what’s given by ROLL OUT is ∆uin section 3.2\n13: uk ←uk + αROLL OUT\n14: sort Sin descending order of updated uk\n15: return S\n16: procedure ROLL OUT\n17: ▷roll outDsteps of imaginary selection and deduction\n18: ▷make in-place edits to ˜sk,˜yk, ˜Tk\n19: f ←−∞ ▷init score of roll-out\n20: for d= 1 to D: ▷step-by-step roll-out\n21: ˜sk ←ONEBEST SELECT ( ˜Tk,x0,psel)\n22: ˜yk ←ONEBEST DEDUCE (˜sk,pded)\n23: ˜Tk ←˜Tk + {˜yk}\n24: if pver(x0 |˜yk) >f : f ←pver(x0 |˜yk)\n25: return log f\n11170\nAlgorithm 11 Planning for Deduction\nHyperparam: deduction beam width Bded;\ndepth of planning D; planning scale β\nInput: current theory T = {x1,..., xN+m}and goal\nx0; deduction candidates at current step Y =\n{(vk,yk)}; selection model psel and deduction\nmodel pded; verification model pver\nOutput: deductions with updated scores {(vk,yk)}\n1: procedure PLAN D(T, x0, Y, psel, pded, pver)\n2: ▷has access to Bded, D, β\n3: ▷init hypothetical extended theory\n4: for vk,yk in Y: ˜Tk ←T\n5: for vk,yk in Y:\n6: ▷iterate over all candidate deductions\n7: ˜Tk ←˜Tk + {yk}▷extend theory with deduction\n8: vk ←vk + β ROLL OUT\n9: ▷ROLL OUT is in Algorithm 10\n10: ▷what’s given by ROLL OUT is ∆vin section 3.2\n11: sort Yin descending order of updated vk\n12: return Y\nplanning. Intuitively, a higher log f means that the fu-\nture reasoning path conditioned on this selection is more\nlikely to prove the goal. Similar to Algorithm 10, Algo-\nrithm 11 measures how the newly generated deduction\ncould affect the future reasoning path in Dsteps, and\nhonors the deduction which improves the possibility of\nproving the goal in the future.\nC Experiment Details\nWe present experiment details in this section.\nC.1 Data Statistics\nThe data statistics of Entailment Bank is shown in Ta-\nble 5. In Version-I of Entailment Bank, there is one\nsample in the test set that has a theory with a single\nstatement. We ignore this sample since it can not be\ndealt by our system in the normal way. The dataset\ncan be downloaded from https://allenai.org/data/\nentailmentbank.\nSPLIT # OF SAMPLES MAX STEPS AVG STEPS\nTRAIN 1313 17 3.2\nDEV 187 15 3.2\nTEST 340 11 3.3\nTable 5: Data statistics of Entailment Bank.\nC.2 Hyperparameters\nFor SLM experiments, we use “t5-small” in the Hug-\ngingface transformers (Wolf et al., 2020) library for the\nselection and deduction models. We use “deberta-v2-\nxlarge-mnli” for the verification model. We prompt tune\nthese models, with a prompt length of 4 for the selection\nand deduction models, and a prompt length of 32 for the\nverification model. Note that for T5 models, the prompt\nis added to the beginning of both the encoder and the\ndecoder (weight not shared). For the selection model, a\nlayernorm is added before the sigmoid operation.\nIn training, we use the Adam (Kingma and Ba, 2015)\noptimizer with β1 = 0.9,β2 = 0.999,ϵ = 1e−8,λ =\n0. We use learning rate γ = 0.1 for the T5 models, and\nγ = 0.01 for the verification model. We use a batch size\nof 16. We set a very large epoch number like 1000 and\nuse the validation set to do early stopping. In practice,\nthe best epoch is often within 100.\nFor LLM experiments, we use GPT-3.5-turbo model\nprovided by OpenAI. We set the temperature to reduce\nrandomness. We keep the default role “system” with the\nmessage “You are an AI assistant that speaks English.”\nWe used a fixed random seed for all our data gen-\neration and training, so that our results can be easily\nreproduced with our codes.\nDuring inference, we set Binf = Bded = 5 and retain\nthe selections formed by 4 top-scored statements. We\nset α= 10 and β = 0.5 to roughly match the scale of\nthe beam score. We roll out 3 steps for selection and\n2 steps for deduction. We set the maximum step to be\nM = 20.\nWe do not tune hyperparameters except the learning\nrate, and we only tune it in our first training of every\nmodel. We try [0.1, 0.01, 0.001, 0.0001] and choose the\none that yields the best dev set performance.\nOur experiments were run on 8 A6000 GPUs. Train-\ning takes about 1 hour. Time for inference is discussed\nin section 6.1.\nC.3 Details of FOL Translations\nThe classical approach of logical reasoning is to use\nformal logic systems. So we also evaluated the perfor-\nmance of a first-order-logic (FOL) system. Because the\nEntailment Bank dataset does not have human-annotated\nFOL translations for the natural language statements, we\ntranslated all the statements into FOL expressions using\na T5 model trained on the corpus of (natural language,\nFOL) pairs collected by Levkovskyi and Li (2021), and\nthen used a FOL engine to perform reasoning. This\napproach failed because the FOL translations are mostly\nof very poor quality. Here is a summary of the errors:\n• inconsistency in variable naming. The FOL transla-\ntions often use inconsistent variable naming, making\nit difficult to pattern-match relevant expressions.\n• incorrect translations. Some FOL translations inaccu-\nrately represent the original sentences, resulting in a\nfailure to capture the intended meaning. For example,\n“driving is a kind of skill” is incorrectly translated into\n“∃x.(driving(x)&∃y.(vehicle(y)kind(x,y))).\n• syntax errors. Some FOL translations contain syntax\nerrors, making them difficult to be process.\n• missing or incomplete information. In several in-\nstances, the FOL translations do not capture all rel-\n11171\n0.0 0.2 0.4 0.6 0.8 1.0\nfalse positive rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0true positive rate\nBaseline-T5\nBase System\nSystem A\nSystem B\n(a) ROC curves.\n0.0 0.2 0.4 0.6 0.8 1.0\nthreshold\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy\nBaseline-T5\nBase System\nSystem A\nSystem B (b) Acc curves on positive examples.\n0.0 0.2 0.4 0.6 0.8 1.0\nthreshold\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0accuracy\nBaseline-T5\nBase System\nSystem A\nSystem B (c) Acc curves on negative examples.\nFigure 8: Test results with 95% bootstrap CFs on Entailment Bank Version-I under 50% training data.\nMETHOD AUROC AUACC POS AUACCNEG F1\nBASE SYSTEM (T5-BASE ) 0.73 (0.69, 0.77) 0.61 (0.56, 0.65) 0.81 (0.79, 0.84) 0.68 (0.66, 0.71)\nSYSTEM A (T5-BASE ) 0.91 (0.89, 0.93) 0.90 (0.88, 0.92) 0.62 (0.58, 0.65) 0.85 (0.83, 0.87)\nSYSTEM B (T5-BASE ) 0.94 (0.93, 0.96) 0.89 (0.87, 0.91) 0.84 (0.80, 0.87) 0.90 (0.88, 0.91)\nBASE SYSTEM (DENOISE ) 0.55 (0.50, 0.59) 0.39 (0.35, 0.43) 0.83 (0.80, 0.85) 0.67 (0.67, 0.67)\nSYSTEM A (DENOISE ) 0.88 (0.85, 0.90) 0.87 (0.85, 0.89) 0.55 (0.52, 0.59) 0.83 (0.81, 0.85)\nSYSTEM B (DENOISE ) 0.93 (0.91, 0.95) 0.83 (0.80, 0.86) 0.88 (0.85, 0.90) 0.85 (0.84, 0.86)\nTable 6: Test results with 95% bootstrap CFs on Entailment Bank Version-I.\nMETHOD VERSION -I V ERSION -II\nBASE SYSTEM (T5-BASE ) 0.65 (0.60, 0.70) 0.26 (0.22, 0.31)\nSYSTEM A (T5-BASE ) 0.88 (0.85, 0.91) 0.45 (0.39 ,0.50)\nSYSTEM B (T5-BASE ) 0.91 (0.88, 0.94) 0.55 (0.50, 0.60)\nBASE SYSTEM (DENOISE ) 0.46 (0.40, 0.52) 0.27 (0.22, 0.32)\nSYSTEM A (DENOISE ) 0.83 (0.80, 0.87) 0.40 (0.35, 0.46)\nSYSTEM B (DENOISE ) 0.90 (0.87, 0.93) 0.67 (0.62,0.72)\nBASE SYSTEM (VERSION -II) - 0.49 (0.44, 0.54)\nSYSTEM A (VERSION -II) - 0.73 (0.69, 0.78)\nSYSTEM B (VERSION -II) - 0.80 (0.76, 0.84)\nTable 7: Test accuracy with 95% bootstrap CFs in multiple-choice QA. A random guess gives 25% accuracy. The\nsystems in the third block were trained on Version-II training data.\nevant information from the original sentences. For\nexample, it may leave out an entity or quantifier.\nThis analysis reveals a fundamental need for tools that\nwork directly with natural language statements for rea-\nsoning like ours.\nC.4 Results of Ablation Studies\nFigure 8 shows the results of the systems trained on 50%\ntraining data. Some results of ablation studies described\nin section 6.1 are shown in Table 6 and Table 7.\nC.5 Examples of Prompts for GPT-3\nIn section 6.1, we used three kinds of prompts for GPT-\n3: 0-shot, 5-shot and COT. In this section, we provide\nsome examples of these prompts.\nAn in-context demonstration is\nBased on the statements that:\nthe earth rotating on its axis causes stars /\nthe moon to appear to move across the sky at\nnight.\ndiurnal motion is when objects in the sky\nappear to move due to earth 's rotation on\nits axis.\nstars appear to move relative to the horizon\nduring the night.\nWhich of the following conclusions can be\ninferred?\n0. earth rotating on its axis causes horizon\nof stars and night on earth.\n1. earth 's horizon on its rotating axis\ncauses stars to occur in new york night.\n2. the earth revolving around the axis causes\nstars to appear in different night in the\nsky at different horizon of year.\n3. the earth rotating on its axis causes\nstars to appear to move relative to the\nhorizon during the night.\nA: 3.\nFor COT prompting, we used the ground-truth reason-\ning path for the correct choice as the “chain-of-thought”,\nso the last line of the (say) above example will be:\nReason: diurnal motion is when objects in the\n11172\nMETHOD DEPTH =1 D EPTH =3 D EPTH =5\nSYSTEM A WITH MODIFIED PROOF SCORE 0.90 (0.84, 0.95) 0.55 (0.45, 0.65) 0.55 (0.45, 0.65)\nSYSTEM A WITH ORIGINAL PROOF SCORE 0.85 (0.78, 0.92) 0.64 (0.54, 0.74) 0.5 (0.41, 0.6)\nSYSTEM B TRAINED ON ENTAILMENT BANK 0.87 (0.80, 0.94) 0.54 (0.44, 0.64) 0.46 (0.36, 0.56)\nTable 8: Results of ablation studies on PrOntoQA with 95% bootstrap CFs.\nsky appear to move due to earth 's rotation\non its axis & stars appear to move relative\nto the horizon during the night −> int1:\nstars appearing to move relative to the\nhorizon during the night is an example of\ndiurnal motion; int1 & the earth rotating on\nits axis causes stars / the moon to appear to\nmove across the sky at night −> the earth\nrotating on its axis causes stars to appear\nto move relative to the horizon during the\nnight.\nA:3.\nC.6 Experiment Details on PrOntoQA\nThe PrOntoQA data has three subsets of different\n“depths”. The “depth” denotes the number of ground-\ntruth reasoning steps so a “deeper” subset is harder. For\neach depth, we draw (using the released data generation\ncode of Saparov and He (2023)) 5 training examples\nand 100 test examples.\nFor the experiments on PrOntoQA, our final verifica-\ntion is performed by a few-shot-prompted GPT-3.5: it\nreads the extended theory and judges whether the given\ngoal is proved. By doing this, we do not need to tune a\nthreshold for the proof scores given by the verification\nmodel (although those scores are still very important\nin the process of explicit planning). In this dataset, the\nnon-provable goals are often definitively disapprovable.\nSo we would like the explicit planning to favor not only\nthe future steps that have large proof scores but also\nthose of large contradiction scores. Therefore, we re-\nplace the proof score f in the planning procedure by the\ngeneralized score gdefined below\ng(T,x0)\ndef\n= max\nn\nmax(pver(x0 |xn),pcon(x0 |xn))\n(4)\nwhere pcon(x0 |xn) is the probability of “ xn contra-\ndicts x0” given by the pretrained DeBERTa. Table 8\nshows how much this modification helps: if we do not\nuse g, the average performance doesn’t change but we\nwill suffer a higher variance. Table 8 also shows the\nresults of using System B trained on Entailment Bank\ndata. We did this to see if the verification model could\ngeneralize to out-of-domain data. In this experiment, it\nhurts the performance.\nIn this section, we also show the prompts for GPT-3.5\nused in the experiments in section 6.3. For selection and\ndeduction, we employed 5-shot prompting to enhance\nthe model’s comprehension. An in-context training ex-\nample for 5-shot selection prompt is\nContexts:\n0.Every tumpus is not earthy.\n1.Wumpuses are not red.\n2.Wumpuses are vumpuses.\n3.Each vumpus is bitter.\n4.Vumpuses are zumpuses.\n5.Every zumpus is cold.\n6.Zumpuses are numpuses.\n7.Numpuses are aggressive.\n8.Numpuses are dumpuses.\n9.Dumpuses are opaque.\n10.Dumpuses are yumpuses.\n11.Yumpuses are not small.\n12.Each yumpus is a rompus.\n13.Every rompus is earthy.\n14.Each rompus is a jompus.\n15.Jompuses are metallic.\n16.Each jompus is an impus.\n17.Alex is a dumpus.\nQuestion:True or false: Alex is not earthy.\nSelection:17 and 10 / 17 and 1 / 17 and 13 /\n17 and 15.\nAlex is a dumpus and dumpuses are yumpuses.\nAlex is a dumpus and wumpuses are not red.\nAlex is a dumpus and every rompus is earthy.\nAlex is a dumpus and jompuses are metallic.\nAn in-context example for deduction prompt is\nWe know that: Sally is a tumpus and each\ntumpus is hot.\nInference: Sally is hot.\nNote that we didn’t let GPT to propose multiple de-\nductions in this experiment because the no-planning\ndeduction is almost always correct as long as the selec-\ntion is correct.\n11173",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8071597814559937
    },
    {
      "name": "Inference",
      "score": 0.6724609136581421
    },
    {
      "name": "Spurious relationship",
      "score": 0.6446730494499207
    },
    {
      "name": "Process (computing)",
      "score": 0.5484642386436462
    },
    {
      "name": "Artificial intelligence",
      "score": 0.539631724357605
    },
    {
      "name": "Natural language",
      "score": 0.4847704768180847
    },
    {
      "name": "Core (optical fiber)",
      "score": 0.44444915652275085
    },
    {
      "name": "Natural language understanding",
      "score": 0.4334200322628021
    },
    {
      "name": "Machine learning",
      "score": 0.3963141143321991
    },
    {
      "name": "Natural language processing",
      "score": 0.34340858459472656
    },
    {
      "name": "Programming language",
      "score": 0.22973278164863586
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    }
  ]
}