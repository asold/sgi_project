{
    "title": "MViT: Mask Vision Transformer for Facial Expression Recognition in the wild.",
    "url": "https://openalex.org/W3167651619",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2129177411",
            "name": "Hanting Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3167894644",
            "name": "Mingzhe Sui",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1233410411",
            "name": "Feng Zhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221504550",
            "name": "Zheng-Jun Zha",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097876706",
            "name": "Feng Wu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2896277673",
        "https://openalex.org/W3175546442",
        "https://openalex.org/W2799041689",
        "https://openalex.org/W3112113890",
        "https://openalex.org/W2738672149",
        "https://openalex.org/W3128609017",
        "https://openalex.org/W3166513219",
        "https://openalex.org/W2108113956",
        "https://openalex.org/W3035574324",
        "https://openalex.org/W1573257727",
        "https://openalex.org/W3164024107",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W3019410392",
        "https://openalex.org/W3132890542",
        "https://openalex.org/W2143238378",
        "https://openalex.org/W3003720578",
        "https://openalex.org/W2481681431",
        "https://openalex.org/W2798553619",
        "https://openalex.org/W2787524669",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W1710476689",
        "https://openalex.org/W2948217907",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W3101998545",
        "https://openalex.org/W2436394355",
        "https://openalex.org/W2904483377",
        "https://openalex.org/W2161969291",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2745497104",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3034552680",
        "https://openalex.org/W2126556638",
        "https://openalex.org/W3089911443",
        "https://openalex.org/W2106390385",
        "https://openalex.org/W2074867602",
        "https://openalex.org/W2164641162",
        "https://openalex.org/W2894217452",
        "https://openalex.org/W2031402837",
        "https://openalex.org/W3116489684",
        "https://openalex.org/W3141497777",
        "https://openalex.org/W2108598243"
    ],
    "abstract": "Facial Expression Recognition (FER) in the wild is an extremely challenging task in computer vision due to variant backgrounds, low-quality facial images, and the subjectiveness of annotators. These uncertainties make it difficult for neural networks to learn robust features on limited-scale datasets. Moreover, the networks can be easily distributed by the above factors and perform incorrect decisions. Recently, vision transformer (ViT) and data-efficient image transformers (DeiT) present their significant performance in traditional classification tasks. The self-attention mechanism makes transformers obtain a global receptive field in the first layer which dramatically enhances the feature extraction capability. In this work, we first propose a novel pure transformer-based mask vision transformer (MVT) for FER in the wild, which consists of two modules: a transformer-based mask generation network (MGN) to generate a mask that can filter out complex backgrounds and occlusion of face images, and a dynamic relabeling module to rectify incorrect labels in FER datasets in the wild. Extensive experimental results demonstrate that our MVT outperforms state-of-the-art methods on RAF-DB with 88.62%, FERPlus with 89.22%, and AffectNet-7 with 64.57%, respectively, and achieves a comparable result on AffectNet-8 with 61.40%.",
    "full_text": "MVT: MASK VISION TRANSFORMER FOR FACIAL EXPRESSION RECOGNITION\nIN THE WILD\nHanting Li, Mingzhe Sui, Feng Zhao‚àó, Zhengjun Zha, and Feng Wu\nUniversity of Science and Technology of China, Hefei 230027, China\n{ab828658, sa20 }@mail.ustc.edu.cn, {fzhao956, zhazj, fengwu}@ustc.edu.cn\nABSTRACT\nFacial Expression Recognition (FER) in the wild is an ex-\ntremely challenging task in computer vision due to variant\nbackgrounds, low-quality facial images, and the subjective-\nness of annotators. These uncertainties make it difÔ¨Åcult for\nneural networks to learn robust features on limited-scale\ndatasets. Moreover, the networks can be easily distributed\nby the above factors and perform incorrect decisions. Re-\ncently, vision transformer (ViT) and data-efÔ¨Åcient image\ntransformers (DeiT) present their signiÔ¨Åcant performance in\ntraditional classiÔ¨Åcation tasks. The self-attention mechanism\nmakes transformers obtain a global receptive Ô¨Åeld in the\nÔ¨Årst layer which dramatically enhances the feature extrac-\ntion capability. In this work, we Ô¨Årst propose a novel pure\ntransformer-based mask vision transformer (MVT) for FER\nin the wild, which consists of two modules: a transformer-\nbased mask generation network (MGN) to generate a mask\nthat can Ô¨Ålter out complex backgrounds and occlusion of face\nimages, and a dynamic relabeling module to rectify incorrect\nlabels in FER datasets in the wild. Extensive experimental re-\nsults demonstrate that our MVT outperforms state-of-the-art\nmethods on RAF-DB with 88.62%, FERPlus with 89.22%,\nand AffectNet-7 with 64.57%, respectively, and achieves a\ncomparable result on AffectNet-8 with 61.40%.\nIndex Terms‚Äî Transformers, FER in the wild, Genera-\ntive adversarial network\n1. INTRODUCTION\nFacial expressions are the most natural and direct way for\nhumans to express emotions [1]. Understanding human emo-\ntional state is a fundamental premise for many computer\nvision tasks including human-robot interaction (HRI), driver\nfatigue monitoring, health-care, etc [2, 3]. Therefore, many\nresearchers are working on building more robust and stable\nmodels to recognize facial expressions. Computer vision\ntasks usually need large-scale datasets, however, building\nlarge-scale laboratorial facial expression datasets requires\ngreat costs. Thus, researchers have made signiÔ¨Åcant progress\n‚àóThe corresponding author is Feng Zhao\nRAF-DBFERPlusAffectNet\nFig. 1. Samples from RAF-DB, FERPlus, and AffectNet. Variant\nhead poses, occlusion, image quality and backgrounds can bring\ntroubles to FER.\non building FER datasets in the wild, such as RAF-DB [4],\nFERPlus [5], AffectNet [6], EmotionNet [7], etc.\nNonetheless, as illustrated in Fig. 1, FER datasets in the\nwild collected in natural environments are extremely hard to\nannotate, which is mainly on account of the uneven quality of\nimages from the Internet, as well as the complex backgrounds\nand occlusion interference. These uncertainties often lead to\nincorrect labeling and inconsistent labeling standards which\nwill seriously affect the quality of datasets and bring difÔ¨Åcul-\nties to network training.\nBefore the rise of deep learning, traditional FER meth-\nods are mainly based on hand-crafted features (i.e., LBP\n[8], LDA [9], PCA [10], and SIFT [11]) following by the\nclassiÔ¨Åers such as SVM [12]. In recent years, with the de-\nvelopment of parallel computing hardware, methods based\non convolutional neural networks (CNNs) have gradually\nreplaced the traditional methods and achieve state-of-the-art\nperformance in FER tasks. However, the performance of\nthese CNN-based approaches can be easily affected by the\nquality of the datasets and often lacks robustness, which is\ncaused by the different backgrounds, image quality, head\nposes, and occlusion interference of the images. Especially,\ndirectly recognizing expressions on non-frontal faces caused\nby variant head poses is a big challenge [13].\nIn the past few years, the transformer has become the\narXiv:2106.04520v2  [cs.CV]  10 Jul 2021\nOriginalMasked\nFig. 2. Samples from RAF-DB, FERPlus, and AffectNet. The top\nrow shows the original images in the wild, while the bottom row\nshows the corresponding masked images. The black area represents\nthe masked regions.\nmodel of choice in natural language processing (NLP) [14].\nMotivated by the success of transformer in NLP tasks, ViT\nachieves huge success in image classiÔ¨Åcation tasks by pre-\ntraining ViT on a large-scale dataset and then Ô¨Åne-tuning on a\nsmaller dataset [15]. DeiT further reduces the training costs of\nViT by utilizing data enhancement and knowledge distillation\n[16]. Back to the aforementioned FER problem in the wild,\nstudies have shown that ViT has strong robustness against se-\nvere occlusion and disturbance [17]. Therefore, transformer\nis particularly suitable to deal with FER in the wild.\nIn this paper, we propose a convolutional-free model for\nFER termed as Mask Vision Transformer (MVT). To the\nbest of our knowledge, it is the Ô¨Årst pure transformer-based\nframework to solve the FER problems. By generating a mask\nfor each facial image and improved relabeling strategy [18],\nour MVT effectively reduces the uncertainties and the impact\nof incorrect labels on the classiÔ¨Åcation in FER, and achieves\nstate-of-the-art or comparable performance on RAF-DB,\nFERPlus and AffectNet datasets. The proposed MVT con-\nsists of two crucial modules: dynamic relabeling and mask\ngeneration. Given a face image, a lightweight transformer-\nbased network called mask generation network (MGN) is\nused to generate the mask for each image, which is employed\nto Ô¨Ålter out the areas that are not relevant to expression recog-\nnition. As we can see in Fig. 2, the mask can effectively wipe\noff the useless background areas and occlusion interference\nwhich are harmful to FER. The training process of MGN is\nsimilar to generative adversarial network (GAN) [19], which\nwill be described in detail in Section 3. Then the masked im-\nage is split into several patches, linearly embed each of them,\nadd position embeddings, and feed the resulting sequence\nof vectors to a ViT. After learning the relationship between\npatches with global self-attention mechanism, we utilize the\nclass token to decide the expression. The second module\nis a relabeling module that relabels the suspected incorrect\nlabels by comparing the maximum predicted probabilities to\nthose of the given labels. Different from the Ô¨Åxed threshold\nin the relabeling module in SCN [18], we propose a dynamic\nrelabeling module where the threshold varies with the proba-\nbilities of the given labels. Through the experiment, we Ô¨Ånd\nthat the relabeling strategy with a Ô¨Åxed threshold can some-\ntimes lead to erratic training because triggering the relabeling\noperation is usually caused by the incomplete Ô¨Åtting of the\nnetwork instead of the incorrect label when the probabilities\nof the given labels are relatively high. Therefore, the rela-\nbeling with the Ô¨Åxed weights is easy to lead to the instability\nin training process. The proposed dynamic relabeling effec-\ntively makes the training process more stable and improves\nthe performance on public datasets.\nIn summary, this paper has the following contributions:\n‚Ä¢We propose a pure transformer-based model called\nMVT. To the best of our knowledge, MVT is the Ô¨Årst\nFER framework built entirely based on transformer.\nOur MVT achieves state-of-the-art results on RAF-DB,\nFERPlus, AffectNet-7 datasets and a comparable result\non AffectNet-8 dataset.\n‚Ä¢We propose a mask generation network (MGN) in-\nspired by GAN, which is totally different from most\ntraditional GANs used to generate real-world images.\nThe mask generated by our MGN can effectively Ô¨Ålter\nout the backgrounds and interference of face images,\nretaining the expression information parts and boosting\nthe classiÔ¨Åcation accuracy. In addition, we propose a\nnovel variance loss function to train our MGN.\n‚Ä¢We propose a new relabeling strategy, which makes\nthe relabeling more accurate and the training process\nmore stable, thus improving the performance on FER\ndatasets in the wild.\n2. RELATED WORK\n2.1. Facial Expression Recognition in the Wild\nIn general, the existing FER methods are generally composed\nof two parts, face alignment and expression recognition. In\nthe face alignment stage, MTCNN [20] and RetinaFace [21]\nare used to locate the face. The detected faces can be further\naligned. In the expression recognition stage, hand-crafted\nfeatures occupied the mainstream in FER at the beginning\nincluding SIFT [11], HOG [22], LBP [8] and NMF [23, 24].\nDeep learning based methods gradually replaced hand-crafted\nfeatures in recent years. CNNs have a natural inductive bias\nfor image processing. By making use of this advantage,\nmany CNN-based methods have shown promising perfor-\nmance on lab-collected FER datasets [25‚Äì28]. However, the\nCNN-based models can be easily distributed by complex\nbackgrounds, variant head poses, occlusion, etc. Wang et al.\n[18], proposed a self-cure Network (SCN) to suppress the\nuncertainties for FER in the wild. Recent studies have shown\nthat ViT has strong robustness against severe occlusion and\ndisturbance [17] which is the reason why we choose ViT as\nthe backbone of our MVT.\nLinear Projection of Flattened Patches\nPatch + Position Embedding\nTransformer encoder\nLinear Projection to one dimension\n1 2 3 4 5 6 7 8 9\nSigmoid\nMask\n1  -\nMask Generation Network\nReshape\nSplit\nLinear Projection of Flattened \nPatches\n1 2 3 4 5 6 7 8 9*0\nPatch + Position \nEmbedding\n* Extra learnable \n[class] embedding\nTransformer encoder\nMLP \nHead\nSad\nHappy\nAngry\n‚Ä¶\nMask Vision Transformer\nElement-wise \nMultiplication\nForward  \nReshape\nDynamic Relabeling Module\nùëÉ1\nùëÉ1\n.\n.\n.\nùëÉùëõ\nprobabilities\nùëÉùëöùëéùë• ‚àíùëÉùëîùë° > ùëì ùëÉùëîùë° +Œ¥\nangry happy\nFig. 3. The pipeline of the pure transformer-based Mask Vision Transformer. It can be divided into three parts, mask generation network,\nmask vision transformer, dynamic relabeling module. We choose DeiT-S [16] pre-trained on ImageNet as our classiÔ¨Åer. The face images are\nfed to MGN to generate the corresponding masks which can measure the importance of the different areas and Ô¨Ålter out complex backgrounds\nand occlusion. Then we utilize MVT to determine the expression of the masked images. During the training process, we design a dynamic\nrelabeling module to rectify the suspected incorrect labels.\n2.2. Vision Transformer\nTransformers were proposed by Vaswani et al. [14] for\nmachine translation, and have become the state-of-the-art\nmethod in many NLP tasks. Inspired by the success of\ntransformers, several researchers have tried to invest trans-\nformers in computer vision tasks, such as image classiÔ¨Åcation\n[15, 16], object detection [29], segmentation [30], etc. ViT is\nthe Ô¨Årst work to apply a vanilla transformer to image classiÔ¨Å-\ncation. By pre-training ViT on a large dataset (e.g. ImageNet\n[31], ImageNet-21k [32]) and then Ô¨Åne-tuning on a smaller\ndataset (e.g. CIFAR-10/100 [33]), ViT sets new state-of-the-\nart on several classiÔ¨Åcation datasets. DeiT further reduces\nthe training costs of ViT by utilizing data enhancement and\na distillation token [16]. Both ViT and DeiT show superior\nperformance compared with CNN-based methods. Inspired\nby ViT and DeiT, we Ô¨Årst propose a pure transformer-based\nmodel for FER in the wild.\n2.3. Transformer based Generative Adversarial Network\nDeep generative models of images are neural networks trained\nto output synthetic imagery. At Ô¨Årst, GAN can only generate\nlow-resolution faces or numbers [19]. The latest generation\nmodels already can generate sample images that are hard for\nhumans to distinguish from real-world images [34]. Inspired\nby ViT, scholars recently began to build generative networks\nbased on transformers [35, 36]. Instead of generating real-\nworld images, we propose a mask generation network based\non transformer that generates masks for face images.\n3. METHOD\nTo learn robust facial expression features with uncertainties,\nwe propose a pure transformer-based framework Mask Vision\nTransformer (MVT). In order to fully demonstrate the advan-\ntages of ViT for visual modeling tasks, we retain the entire in-\nternal structure of ViT [15]. Since the training of ViT often re-\nquires large-scale data, we directly use the DeiT-S pretrained\non ImageNet [31] in [16]. Firstly, we provide an overview of\nthe MVT, and then present its two main modules.\n3.1. Overview of Mask Vision Transformer\nOur MVT is built upon pure transformer and composed of\ntwo crucial modules: i) mask generation network, ii) dynamic\nrelabeling, as depicted in Fig. 3.\nFor a given face image Iin with the size of H√óW√óC, we\nreshape Iin into a sequence of Ô¨Çattened 2D patches I p with\nthe size of N√ó(P2 √óC), where (H,W ) is the resolution of\nthe original image, C is the number of channels, (P,P ) is\nthe resolution of each image patch, and N = H √óW/P2 is\nthe number of patches which also serves as the effective in-\nput sequence length for our MGN. The MGN uses constant\nLinear Projection of Flattened Patches\nTransformer encoder\nLinear Projection to one dimension\n1 2 3 4 5 6 7 8 9\nSigmoid\nPatch + Position Embedding\nGnerator\nùêª√óùëä √óùê∂\nReshape\nMask\nùêª√óùëä √óùê∂\nDisciminator\nGenerated \nmasked image\nRandom \nmasked image\nSplit\nLinear Projection of Flattened \nPatches\nTransformer encoder\nMLP \nHead\nSad\nHappy\nAngry\n‚Ä¶\nùêøùê∫ = ùë£ùëéùëü ùë†ùëúùëìùë°ùëöùëéùë•(ùëôùëúùëîùëñùë°ùë†ùëö) +|ùëÄùëéùë£ùëî ‚àíùëö|\nùêøùê∑ = ùêøùê∂ùê∏ ùëôùëúùëîùëñùë°ùë†ùëö +ùêøùê∂ùê∏ ùëôùëúùëîùëñùë°ùë†ùëüùëö\nElement-wise \nMultiplication\nForward \nReshape \nPatch + Position \nEmbedding\nFig. 4. The pipeline of the pure transform-based generator and discriminator of MGN. Here H = W = 224and C = 3.\nlatent vector size D through all of its layers, so we Ô¨Çatten\nthe patches and map them to vector Z p with D dimensions\nthrough a trainable linear projection. Then trainable position\nembeddings Epos with the size of N √óD are added to the\nembedded patches Z p to retain positional information. The\nembedding vectors with the size of N √óDare fed to MGN.\nIn this paper, D = 384 , H = W = 224 and P = 16 .\nMGN outputs N vectors with the size of D after modeling\nthe relationship between embedded patches. The outputs of\nMGN are then mapped to N vectors with 1 dimension Mpi\nranging from 0 to 1 with another trainable linear projection\nand sigmoid function. These values represent the importance\nof the corresponding patches to FER. SpeciÔ¨Åcally, the higher\nthe value is, the more unimportant this patch is. Since these\nvalues are negatively correlated with the importance of the\npatches, we need to subtractMp from 1 to get the patch masks\nwhich can Ô¨Ålter out the backgrounds and occlusion of the face\nimage. Then Mpi is copied to P √óP √óC to match the size\nof input patches. These patch masks are restored to ‚Äùimages‚Äù\nin the spatial dimension based on the original positions to get\nthe Ô¨Ånal mask M f with the size of H√óW√óC, which is the\nsame as the input facial image I in. We then do a dot prod-\nuct between M f and Iin to get the masked image I M , then\nIM is split into patches and fed to MVT to obtain the prob-\nabilities of each expression. It should be noted that only the\nparameters of MVT are optimized during the training, while\nthe parameters of MGN are frozen. To further improve MVT,\nthe dynamic relabeling module is added to rectify the sam-\nples with suspected incorrect label. This relabeling operation\naims to hunt more clean samples and then enhance the Ô¨Ånal\nmodel. Different from the relabeling module from SCN [18],\nour relabeling threshold changes dynamically with the prob-\nabilities of given labels which can make the training process\nmore stable and the module more robust.\n3.2. Mask Generation Network\nInspired by the success of generative adversarial networks\n[36], we propose a pure transformer-based mask generation\nnetwork to generate a mask that can Ô¨Ålter out the complex\nbackgrounds and interference for each image. Instead of gen-\nerating a real-world image, MGN generates the mask that acts\nas a Ô¨Ålter which is different from most traditional GANs. To\ntrain our mask generation network, we need an extra ViT as a\ndiscriminator. As can be seen in Fig. 4, similar to the tradi-\ntional GAN training mode, we train our MGN by alternately\ntraining generator and discriminator. When training the dis-\ncriminator, we Ô¨Åx the parameters of MGN. Images with the\nmask generated by MGN and random masked images are fed\nto the discriminator for inference, and the loss function of the\ndiscriminator is formulated as:\nLD = LCE (logitsgm) + LCE (logitsrm) (1)\nwhere LCE is the cross-entropy loss commonly used in clas-\nsiÔ¨Åcation tasks; logitsrm and logitsgm are the output of MLP\nhead of random masked image and generated masked image,\nrespectively. By minimizing LD, the discriminator is capa-\nble of recognizing expressions under masks. SpeciÔ¨Åcally, the\ndiscriminator can learn the importance and relevance of dif-\nferent regions of the image to expression recognition by learn-\ning from random mask images which correspond to the real-\nworld images in traditional GAN. Correspondingly, the gen-\nerated masked images correspond to the generated image in\nthe traditional GAN.\nWhen training the generator of MGN, we Ô¨Åx the param-\neters of the discriminator. Only images masked by MGN are\nfed to the discriminator for inference. To train this special\n‚ÄùGAN‚Äù we propose a novel generator loss function, formu-\nlated as:\nLG = var(softmax(logitsgm)) + |Mavg ‚àím|, (2)\nwith\nvar(x) =\nK‚àí1‚àë\ni=0\n(xi ‚àí¬Øx)2\nK‚àí1 ,softmax(x)j = exj\n‚àëK‚àí1\ni=0 exi\n(3)\nand\nMavg =\nN‚àí1‚àë\ni=0\nMpi\nN , (4)\nwhere ¬Øxis the mean of the elements of vector x, xi donates\nto the i-th element of the vector xwith K dimensions, Mavg\nis is the mean value of the generated patch masks Mpi , and\nmis the expected mask area size. We assume that when the\ngenerated masks can block the region related to the FER in-\nformation, the discriminator will not be able to distinguish\nthe expression of images. In other words, the discriminator\nwill output each expression with equal probability. Therefore,\nby minimizing the variance of the expression probability, the\nMGN can learn the ability of measuring the importance of\ndifferent areas of face images. The constant m restricts the\narea of the generated mask, which can be Ô¨Çexibly adjusted\naccording to different datasets. For the datasets with large\nbackgrounds or interference areas, it can be set to a higher\nvalue. On the contrary, it can be reduced accordingly.\nBy training the generator and discriminator alternately, we\ncan Ô¨Ånally get a MGN that can generate a mask that covering\nthe crucial regions for each face. In the inference stage, we\nonly need to exploit the complement of the generated masks\nto effectively Ô¨Ålter out the backgrounds and interference, so\nas to preserve the crucial areas needed for classiÔ¨Åcation.\n3.3. Dynamic Relabeling\nInspired by the relabeling module in SCN [18], we proposed\nthe dynamic relabeling module to correct the mislabeled sam-\nples in FER datasets in the wild to improve the quality of the\ntraining sets. The original relabeling module chooses con-\nstant Œ¥as the threshold to relabel the samples, that is to say, a\nsample is assigned to a new pseudo label if the maximum pre-\ndiction probability is higher than the probability of the given\nlabel with a constant threshold Œ¥. The origin relabeling mod-\nule is deÔ¨Åned as\nlnew =\n{ lmax ifPmax ‚àíPgt >Œ¥,\nlorg otherwise, (5)\nwhere lnew denotes the new label, Œ¥is the threshold, Pmax is\nthe maximum predicted probability, and Pgt is the predicted\nprobability of the given label. lorg and lmax are the origi-\nnally given label and the index of the maximum prediction,\nrespectively. We consider that it is unfair to use the same\nthreshold as the criterion for relabeling when the probability\nof the given label is different. SpeciÔ¨Åcally, when Pgt is rela-\ntively high (e.g. Pgt = 0.3), it may harm to relabel the sam-\nple with the index of the maximum prediction. By observing\nthe relabeled samples in the experiment, we Ô¨Ånd that when\nPgt is relatively high, both the non-convergence of network\nand incorrect label can trigger a relabeling operation. When\nthe samples are relabeled because the network does not con-\nverge, it is likely to change the correct label to the wrong one\nwhich can be extremely harmful to the network training. So\nwe propose an improved relabeling module called dynamic\nrelabeling. Formally, the module can be deÔ¨Åned as,\nlnew =\n{ lmax ifPmax ‚àíPgt >f (Pgt) + Œ¥,\nlorg otherwise, (6)\nwhere fis a nonnegative monotone increasing function ofPgt\nand Œ¥is the lower limit of the threshold. This means that the\nrelabeling threshold increases asPgt boosts, which effectively\navoids the erroneous relabeling operation and makes the con-\nvergence more stable.\n4. EXPERIMENTS\nTo verify the effectiveness of the proposed method, we con-\nduct the experiments on three popular in-the-wild facial ex-\npression datasets (i.e., RAF-DB [4], FERPlus [5] and Affect-\nNet [6]). In this section, we Ô¨Årst introduce the FER datasets\nused in our experiments and implementation details. We\nthen explore the impact of each component of MVT on these\ndatasets. Subsequently, we compare the proposed method\nwith several state-of-the-art approaches.\n4.1. Datasets\nRAF-DB [4] contains 29,672 facial images annotated with\nbasic or compound expressions by 40 independent annotators.\nConsistent with most of the previous work, only images of\nseven prototypical expressions, such as neutral, happy, sad,\nsurprised, fear, disgust, and anger, are used. Among them,\n12271 images are used for training and 3068 images are for\ntesting. The original size of the images in RAF-DB is 100 √ó\n100. Both training images and test images have imbalanced\ndistribution. The accuracy of the overall samples is executed\nfor performance measurement.\nFERPlus [5] is extended from FER2013 employed in the\nICML 2013 Challengesin representation learning. FERPlus\ncontains 28,709 training images, 3,589 validation images\nand 3,589 test images, which are all collected by the Google\nsearch engine. All the images are grayscale and have been\nTable 1. Evaluation of the mask ratio m on FERPlus and RAF-DB.\nThe best results are in bold.\nDatasets Mask ratio (%) Accuracy (%)\n0 (Baseline) 86.7\nRAF-DB 10 87.52\n15 87.91\n20 87.45\n0 (Baseline) 87.92\nFERPlus 10 88.45\n15 88.58\n20 88.88\nTable 2. Comparisons between different dynamic relabel functions.\nThe best results are in bold.\nDatasets f Accuracy (%)\nConstant (Baseline) 86.86\nRAF-DB Linear 87.09\nQuadratic 87.2\nSigmoid 87.45\nConstant (Baseline) 88.22\nFERPlus Linear 88.32\nQuadratic 88.68\nSigmoid 88.52\nresized to 48 √ó48. Each face image in FERPlus is anno-\ntated by 10 annotators. Apart from seven basic expressions\nas RAF-DB, contempt is included which leads to 8 expres-\nsion classes. We report the accuracy of all samples under the\nsupervision of majority voting for performance evaluation.\nAffectNet [6] is a large facial expression datasets with\nmore than 1,000,000 face images gathered from the Internet.\nWhile only about 450, 000 images have been annotated man-\nually with 11 expression categories. The seven expression\ncategories denoted by AffectNet-7 involves neutrality, hap-\npiness, sadness, surprise, fear, disgust, and anger, while the\neight expression categories denoted by AffectNet-8 with the\nadditional contempt. It should be noted that AffectNet-7 and\nAffectNet-8 both have an imbalanced training set and a bal-\nanced test set. For AffectNet-7, there are 283,901 images as\ntraining data and 3,500 images for testing, and for AffectNet-\n8, there are 287,568 images as training data and 4,000 images\nas test data. We mainly present the mean class accuracy on\nthe test set for performance measurement and fair compar-\nisons with other methods.\nOcclusion and Pose Variant Datasets is used to verify\nthe performance of the FER model under real-world occlusion\nand variant head poses conditions. Wanget al.[27] built three\nsubsets Occlusion-RAF-DB and Pose-RAF-DB from the test\nset of RAF-DB. The pose variations can be divided into two\ntypes, which are poses larger than 30 degrees and those larger\nthan 45 degrees. We perform the accuracy of the overall sam-\nples for evaluation.\n4.2. Implementation Details\nIn our experiments, images on all the datasets are resized to\nthe size of 224√ó224. Then the backgrounds and occlusion of\nthe images are masked by the corresponding masks generated\nby our MGN. It is worth noting that we do not perform any\nextra face alignment on all FER in the wild datasets. The\nrandom erasing, horizontal Ô¨Çipping, and color jittering are\nemployed to avoid over-Ô¨Åtting. Since the transformer-based\nmodel beneÔ¨Åts from pre-training on large-scale datasets, we\nuse the Deit-S in [16] pre-trained on ImageNet [31] as the\nbackbone of both MGN and the classiÔ¨Åer. In particular, we\nuse the Ô¨Årst six layers of DeiT-S with the pre-trained param-\neters as the backbone of MGN and the entire DeiT-S as the\nbackbone of MVT. In the inference stage, We use AdamW\n[37] to optimize MVT with a batch size of 32 while keeping\nthe parameters of MGN frozen. For RAF-DB and FERPlus,\nthe learning rate is initialized to 0.00009, decreased at an ex-\nponential rate. For AffectNet-7 and AffectNet-8, the learn-\ning rate is initialized to 0.00001 with the same decay rate as\nRAF-DB and FERPlus. All the experiments are conducted on\na single NVIDIA RTX 3070 card with Pytorch toolbox.\n4.3. Ablation Studies\nAs shown in Fig. 3, our MVT mainly consists of mask gen-\neration network (MGN) and dynamic relabeling module. To\nshow the effectiveness of our MVT, we conduct ablation stud-\nies to evaluate the inÔ¨Çuence of the key parameters and com-\nponents on the Ô¨Ånal performance on RAF-DB and FERPlus\ndatasets.\nInÔ¨Çuence of the mask ratio. We evaluate the recognition\nperformance of MGN with different mask ratio min Eq. (2),\nas displayed in Table 1.\nWe can observe that the optimal mask ratio is various for\ndifferent datasets. SpeciÔ¨Åcally, the optimal mfor RAF-DB is\n15% and is 20% for FERplus. This is caused by the different\nproportions of backgrounds and occlusion areas in different\ndatasets. As we aforementioned in Section 3, it is essential to\ntrain our MGN with various mask ratios according to different\ndatasets. In the following, we set the values of m to 15%\nfor RAF-DB, AffectNet-7, and AffectNet-8, while 20% for\nFERPlus.\nInÔ¨Çuence of Dynamic relabeling threshold function.\nAs we mentioned in Section 3, the dynamic threshold rela-\nbeling strategy can make the training process more stable,\nthus improving the performance. We Ô¨Årst give three basic\ndynamic threshold functions of linear, quadratic, and sigmoid\nas pictured in Fig. 5(a). SpeciÔ¨Åcally, we Ô¨Åx Œ¥ = 0.2 as Wang\net al. did in SCN [18]. We also give the convergence curves\nof different f on the RAF-DB test set with adding dynamic\nrelabeling module and constant threshold relabeling module\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0 0.1 0.2 0.3\nConstant\nQuadratic\nLinear\nSigmoid\nRelabel threshold\nùëÉùëîùë°\n(a)\n83\n83.5\n84\n84.5\n85\n85.5\n86\n86.5\n87\n87.5\n88\nConstant\nLinear\nQuadratic\nSigmoid\nEpochs\nAccuracy(%) (b)\nFig. 5. Three dynamic relabeling threshold functions (Left: threshold function curves; Right: convergence curves of diferent f on the\nRAF-DB test set).\n(a) Confusion Matrix of MVT on RAF-DB.\n (b) Confusion Matrix of CVT on FERPlus.\n (c) Confusion Matrix of CVT on AffectNet-7.\nFig. 6. The confusion matrices of our method on RAF-DB, FERPlus and AffectNet. (from left to right: RAF-DB, FERPlus, and AffectNet-7).\nin 10-th epoch to prove the stability of the dynamic relabeling\nstrategy as illustrated in Fig. 5(b).\nFrom Fig. 5(b), it can be seen that the dynamic relabel-\ning threshold is signiÔ¨Åcantly better than the constant threshold\nin terms of both performance and convergence stability. This\nphenomenon is predictable because when relabeling threshold\nis the monotonically increasing function of Pgt, the problem\nof false relabeling caused by the non-convergence of the net-\nwork can be effectively avoided. We then evaluate the recog-\nnition performance of dynamic relabeling with different types\nof f in Eq. (6), as shown in Table 2.\nWe can Ô¨Ånd that the performance with different dynamic\nrelabeling threshold functions is better than that of the con-\nstant threshold relabeling strategy in SCN on both RAF-DB\nand FERPlus datasets. In particular, it can be seen from that\nthe optimal f for RAF-DB is of the sigmoid type while for\nFERPlus is of the quadratic type from Table 2. We consider\nthat this is because the images in FERPlus are grayscale im-\nages, which provide less information and lead to a smaller dis-\ntance between classes of different expressions, a lower thresh-\nold is more suitable for FERPlus. In the following, we choose\nthe sigmoid type f for RAF-DB and the quadratic type f for\nFERPlus.\nEffectiveness of Each Component in MVT. To verify\nthe validity of MGN and dynamic relabeling module in our\nmethod, we conduct ablation studies on RAF-DB, FERPlus,\nand AffectNet-7 datasets. SpeciÔ¨Åcally, the pre-trained DeiT-S\n[16] is employed as the baseline in experiments, then MGN\nand dynamic relabeling module are added to the baseline, re-\nspectively. Experimental results are reported in Table 3.\nWe can see that after Ô¨Åltering out the backgrounds and oc-\nclusion by MGN, the result exceeds the baselines by 1.24%,\n0.96%, and 0.78% on RAF-DB, FERPlus and AffectNet-7\ndatasets, respectively, which indicates the importance and va-\nTable 3. Evaluation of the MGN and dynamic relabeling modules.\nThe best results are in bold.\nMGN D-Relabel RAF-DB FERPlus AffectNet-7\n% % 86.67 87.92 63.79\n\" % 87.91 88.88 64.57\n% \" 87.45 88.68 ‚Äì\n\" \" 88.62 89.22 ‚Äì\nlidity of wiping of the occlusion and backgrounds for FER in\nthe wild datasets. Moreover, by employing the dynamic re-\nlabeling module during the training process, our method also\noutperforms the baseline.\nIn particular, on account of the large amount of data in\ntraining set and the imbalanced data distribution in AffectNet,\nthe training accuracy should not be too high (i.e. 65% for Af-\nfectNet), otherwise it will lead to over-Ô¨Åtting. However, the\nrelabeling module can only be added when the training accu-\nracy is high enough (e.g. 90%) so as to utilize features learned\nby the network to help correct the wrong labels. Therefore,\nwe do not add the dynamic relabeling module in the training\nof AffectNet.\n4.4. Comparison with State-of-the-arts\nWe compare our method with previous state-of-the-art meth-\nods on RAF-DB, FERPlus, AffectNet-7, and AffectNet-8.\nTo our knowledge, we achieve new state-of-the-art results\non RAF-DB, FERPlus, AffectNet-7, and a comparable re-\nsult on AffectNet-8. SpeciÔ¨Åcally, MVT outperforms recent\nstate-of-the-art methods with 88.62%, 89.22%, and 64.57%\non RAF-DB, FERPlus, and AffectNet-7, respectively as pre-\nsented in Table 4. We also give the confusion matrices on\nRAF-DB, FERPlus, and AffectNet-7 to show the superiority\nof our method as shown in Fig. 6.\nResults on RAF-DB: Comparisons with other state-of-\nthe-art methods are listed in Table 4(a). Among them, FSN\n[38] propose a feature selection mechanism to improve the\nperformance of a CNN-based model. gACNN [26] lever-\nages a patch-based attention network and a global network.\nSPWFA-SE [40] also does not report speciÔ¨Åc expression\nrecognition accuracy, it provides the confusion matrix on\nRAF-DB. Therefore, we borrow the accuracy results from\nits confusion matrix for comparison. CVT [41] is the Ô¨Årst to\nintroduce transformer into FER in the wild task even though\nthey use CNNs to extract features. EfÔ¨ÅcientFace [28] is a\nlightweight network with label distribution training. Our\nproposed method achieves 88.62% on RAF-DB. From Table\n4(a), it can be witnessed that the proposed method outper-\nforms all of these state-of-the-art methods.\nResults on FERPlus: Comparison with other state-of-\nthe-art methods are shown in Table 4(b). Among them,\nSHCNN [43], ResNet+VGG [42], LDR [44], RAN [27], and\nTable 4 . Comparisons with the state-of-the-art results. ‚ó¶Extra\nface alignment is used. ‚Ä†These results test on AffecNet-7.\n‚àóOversampling is used since the train set of AffectNet is imbalanced.\n(a) Comparisons with state-of-the-art methods on RAF-DB. The best re-\nsults are in bold.\nMethod Year Accuracy(%)\nFSN [38] 2018 81.14\ngACNN [26] 2018 85.07\nRAN [27] 2020 86.90\nSCN [18] 2020 87.03\nDSAN-VGG-RACE [39] 2020 85.37\nSPWFA-SE [40] 2020 86.31\nCVT [41] 2021 88.14\nEfÔ¨ÅcientFace [28] 2021 88.36\nOurs 2021 88.62\n(b) Comparisons with state-of-the-art methods on FERPlus. The best re-\nsults are in bold.\nMethod Year Accuracy(%)\nResNet+VGG [42] 2017 87.4\nSHCNN [43] 2019 86.54\nLDR [44] 2020 87.6\nRAN [27] 2020 87.85\nRAN‚ó¶[27] 2020 88.55\nSCN [18] 2020 88.01\nCVT [41] 2021 88.81\nOurs 2021 89.22\n(c) Comparisons with state-of-the-art methods on AffectNet-7 and\nAffectNet-8. The best results are in bold.\nMethod Year Accuracy(%)\nIPA2LT‚Ä†[45] 2018 55.11\ngACNN‚Ä†[26] 2018 58.78\nSPWFA-SE‚Ä†[40] 2020 59.23\nDDA-Loss‚Ä†‚àó[46] 2020 62.34\nEfÔ¨ÅcientFace‚Ä†‚àó[28] 2021 63.7\nRAN [27] 2020 52.97\nRAN‚àó[27] 2020 59.5\nSCN‚àó[18] 2020 60.23\nCVT‚àó[41] 2021 61.85\nEfÔ¨ÅcientFace‚àó[28] 2021 59.89\nOurs‚àó 2021 61.40\nOurs‚Ä†‚àó 2021 64.57\nTable 5. Comparisons with other methods on Occlusion-RAF-DB\nand Pose-RAF-DB datasets\nMethod Occlusion Pose(30) Pose(45)\nBaseline [27] 80.19 84.04 83.15\nRAN [27] 82.72 86.74 85.2\nCVT [41] 83.95 87.97 88.35\nEfÔ¨ÅcientFace [28] 83.24 88.13 86.92\nOurs 85.17 87.99 88.40\nSCN [18] are CNN-based method. Especially, RAN ‚àómeans\noriginal RAN with extra face alignment. CVT [41] utilize\nboth CNNs and multi-layer transformer encoder to recognize\nexpressions. Even without face alignment in our experiment\nsettings, MVT still achieve the state-of-the-art results 89.22%\non FERPlus.\nResults on AffectNet: We compare MVT with several\nstate-of-the-art methods on AffectNet-7 and AffectNet-8, re-\nspectively in Table 4(c). For AffectNet-8, our MVT achieves\na comparable result of 61.40% with CVT [41]. It can be seen\nthat there is a large gap between the results of AffectNet-7\nand AffectNet-8 which is mainly caused by adding expres-\nsion of contempt based on AffectNet-7. As illustrated in [28],\nthere exists lots of noise in the eighth expression categories,\nwhich can seriously deteriorate the accuracy. Our MVT out-\nperforms state-of-the-art method [28] by 0.87%, which uses\nthe label distribution for training.\nResults on Occlusion and Pose Variant Datasets: To\nverify the robustness of the MGN to occlusion and variant\nhead poses, we conduct several experiments on Occlusion-\nRAF-DB and Pose-RAF-DB which are collected by [27] to\nexamine methods under real-world scenario. Table 5 shows\nthe accuracy on Occlusion-RAF-DB and Pose-RAF-DB with\nthe same experimental setting as RAF-DB.\nFrom Table 5, We can see that our MVT obtains the\ngains of 1.93% and 1.22% over EfÔ¨ÅcientFace and CVT on\nOcclusion-RAF-DB, which fully proves the effectiveness of\nour method for Ô¨Åltering out occlusion. In addition, we also\nachieves comparable results with state-of-the-art methods on\nPose-RAF-DB.\n5. CONCLUSION\nIn this paper, we Ô¨Årstly propose a pure transformer-based\nframework Mask Vision Transformer (MVT) for FER in the\nwild. SpeciÔ¨Åcally, we present the mask generation network\n(MGN) which can Ô¨Ålter out the complex backgrounds and\nocclusion to provide clean data for the inference. In the infer-\nence stage, based on the original constant threshold relabel-\ning strategy, a novel dynamic relabeling module is proposed\nto hunt more clean labeled samples and enhance the Ô¨Ånal\nmodel. Extensive experiments on three public datasets in the\nwild demonstrate that our MVT achieves new state-of-the-art\nresults and is robust to real-world occlusion.\n6. REFERENCES\nReferences\n[1] Y-I Tian, T. Kanade, and J. F. Cohn, ‚ÄúRecognizing ac-\ntion units for facial expression analysis,‚Äù IEEE Trans.\nPattern Anal. Mach. Intell., vol. 23, no. 2, pp. 97‚Äì115,\n2001.\n[2] F. Zhang, T. Zhang, Q. Mao, and C. Xu, ‚ÄúJoint pose and\nexpression modeling for facial expression recognition,‚Äù\nin Proc. IEEE Conf. Comput. Vis. Pattern Recog., 2018,\npp. 3359‚Äì3368.\n[3] S. Li and W. Deng, ‚ÄúDeep facial expression recognition:\nA survey,‚Äù IEEE Trans. Affect. Comput., 2020.\n[4] S. Li, W. Deng, and J. Du, ‚ÄúReliable crowdsourcing and\ndeep locality-preserving learning for expression recog-\nnition in the wild,‚Äù in Proc. IEEE Conf. Comput. Vis.\nPattern Recog., 2017, pp. 2852‚Äì2861.\n[5] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang,\n‚ÄúTraining deep networks for facial expression recogni-\ntion with crowd-sourced label distribution,‚Äù in Proc.\nACM Int. Conf. Multimodal Interact, 2016, pp. 279‚Äì283.\n[6] A. Mollahosseini, B. Hasani, and M. H. Mahoor, ‚ÄúAf-\nfectNet: A database for facial expression, valence, and\narousal computing in the wild,‚Äù IEEE Trans. Affect.\nComput., vol. 10, no. 1, pp. 18‚Äì31, 2017.\n[7] C. Fabian Benitez-Quiroz, R. Srinivasan, and A. M.\nMartinez, ‚ÄúEmotioNet: An accurate, real-time algo-\nrithm for the automatic annotation of a million facial\nexpressions in the wild,‚Äù in Proc. IEEE Conf. Comput.\nVis. Pattern Recog., 2016, pp. 5562‚Äì5570.\n[8] C. Shan, S. Gong, and P. W. McOwan, ‚ÄúRobust facial\nexpression recognition using local binary patterns,‚Äù in\nProc. IEEE Int. Conf. Inf. Process.IEEE, 2005, vol. 2,\npp. 367‚Äì370.\n[9] H. Deng et al., ‚ÄúA new facial expression recognition\nmethod based on local gabor Ô¨Ålter bank and PCA plus\nLDA,‚Äù International Journal of Information Technol-\nogy, vol. 11, no. 11, pp. 86‚Äì96, 2005.\n[10] M. R. Mohammadi, E. Fatemizadeh, and M. H. Ma-\nhoor, ‚ÄúPCA-based dictionary building for accurate fa-\ncial expression recognition via sparse representation,‚Äù\nJournal of Visual Communication and Image Represen-\ntation, vol. 25, no. 5, pp. 1082‚Äì1092, 2014.\n[11] P. C. Ng and S. Henikoff, ‚ÄúSIFT: Predicting amino acid\nchanges that affect protein function,‚Äù Nucleic acids re-\nsearch, vol. 31, no. 13, pp. 3812‚Äì3814, 2003.\n[12] John Platt, ‚ÄúSequential minimal optimization: A fast\nalgorithm for training support vector machines,‚Äù 1998.\n[13] W. Zheng et al., ‚ÄúEmotion recognition from non-frontal\nfacial images,‚Äù Emotion Recognition: A Pattern Analy-\nsis Approach, vol. 1, pp. 183‚Äì213, 2014.\n[14] A. Vaswani et al., ‚ÄúAttention is all you need,‚Äù arXiv\npreprint arXiv:1706.03762, 2017.\n[15] A. Dosovitskiy et al., ‚ÄúAn image is worth 16x16 words:\nTransformers for image recognition at scale,‚Äù arXiv\npreprint arXiv:2010.11929, 2020.\n[16] H. Touvron et al., ‚ÄúTraining data-efÔ¨Åcient image trans-\nformers & distillation through attention,‚ÄùarXiv preprint\narXiv:2012.12877, 2020.\n[17] M. Naseer et al., ‚ÄúIntriguing properties of Vision Trans-\nformers,‚Äù 2021.\n[18] K. Wang et al., ‚ÄúSuppressing uncertainties for large-\nscale facial expression recognition,‚Äù in Proc. IEEE\nConf. Comput. Vis. Pattern Recog., 2020, pp. 6897‚Äì\n6906.\n[19] I. J. Goodfellow et al., ‚ÄúGenerative adversarial net-\nworks,‚Äù arXiv preprint arXiv:1406.2661, 2014.\n[20] K. Zhang, Z. Zhang, Z. Li, and Y . Qiao, ‚ÄúJoint face\ndetection and alignment using multitask cascaded con-\nvolutional networks,‚Äù IEEE Signal Process. Lett., vol.\n23, no. 10, pp. 1499‚Äì1503, 2016.\n[21] J. Deng et al., ‚ÄúRetinaface: Single-shot multi-level face\nlocalisation in the wild,‚Äù in Proc. IEEE Conf. Comput.\nVis. Pattern Recog., 2020, pp. 5203‚Äì5212.\n[22] N. Dalal and B. Triggs, ‚ÄúHistograms of oriented gradi-\nents for human detection,‚Äù inProc. IEEE Conf. Comput.\nVis. Pattern Recog.Ieee, 2005, vol. 1, pp. 886‚Äì893.\n[23] I. Buciu and I. Pitas, ‚ÄúApplication of non-negative and\nlocal non negative matrix factorization to facial expres-\nsion recognition,‚Äù in Proc. Int. Conf. on Pattern Recog.\nIEEE, 2004, vol. 1, pp. 288‚Äì291.\n[24] L. Zhao, G. Zhuang, and X. Xu, ‚ÄúFacial expression\nrecognition based on pca and nmf,‚Äù in 2008 7th World\nCongress on Intelligent Control and Automation. IEEE,\n2008, pp. 6826‚Äì6829.\n[25] S. Li, W. Deng, and J. Du, ‚ÄúReliable crowdsourcing and\ndeep locality-preserving learning for expression recog-\nnition in the wild,‚Äù in Proc. IEEE Conf. Comput. Vis.\nPattern Recog., 2017, pp. 2852‚Äì2861.\n[26] Y . Li, J. Zeng, S. Shan, and X. Chen, ‚ÄúOcclusion aware\nfacial expression recognition using CNN with attention\nmechanism,‚Äù IEEE Trans. Image Process., vol. 28, no.\n5, pp. 2439‚Äì2450, 2018.\n[27] K. Wang et al., ‚ÄúRegion attention networks for pose and\nocclusion robust facial expression recognition,‚Äù IEEE\nTrans. Image Process., vol. 29, pp. 4057‚Äì4069, 2020.\n[28] Z. Zhao, Q. Liu, and F. Zhou, ‚ÄúRobust lightweight facial\nexpression recognition network with label distribution\ntraining,‚Äù in Proc. Conf. Arti. Intel., 2021.\n[29] N. Carion et al., ‚ÄúEnd-to-end object detection with\ntransformers,‚Äù in Proc. Eur. Conf. Comput. Vis.\nSpringer, 2020, pp. 213‚Äì229.\n[30] W. Wang et al., ‚ÄúPyramid vision transformer: A ver-\nsatile backbone for dense prediction without convolu-\ntions,‚Äù arXiv preprint arXiv:2102.12122, 2021.\n[31] A. Krizhevsky, I. Sutskever, and G. E. Hinton, ‚ÄúIma-\ngenet classiÔ¨Åcation with deep convolutional neural net-\nworks,‚Äù Advances in neural information processing sys-\ntems, vol. 25, pp. 1097‚Äì1105, 2012.\n[32] J. Deng et al., ‚ÄúImagenet: A large-scale hierarchical im-\nage database,‚Äù inProc. IEEE Conf. Comput. Vis. Pattern\nRecog. Ieee, 2009, pp. 248‚Äì255.\n[33] A. Krizhevsky et al., ‚ÄúLearning multiple layers of fea-\ntures from tiny images,‚Äù 2009.\n[34] T. Karras et al., ‚ÄúAnalyzing and improving the image\nquality of stylegan,‚Äù in Proc. IEEE Conf. Comput. Vis.\nPattern Recog., 2020, pp. 8110‚Äì8119.\n[35] D. Wodajo and S. Atnafu, ‚ÄúDeepfake video detection\nusing convolutional vision transformer,‚Äù arXiv preprint\narXiv:2102.11126, 2021.\n[36] Y . Jiang, S. Chang, and Z. Wang, ‚ÄúTransgan: Two\ntransformers can make one strong gan,‚Äù arXiv preprint\narXiv:2102.07074, 2021.\n[37] I. Loshchilov and F. Hutter, ‚ÄúFixing weight decay regu-\nlarization in Adam,‚Äù 2018.\n[38] S. Zhao et al., ‚ÄúFeature selection mechanism in CNNs\nfor facial expression recognition.,‚Äù in BMVC, 2018, p.\n317.\n[39] Y . Fan, V . Li, and J. CK. Lam, ‚ÄúFacial expression recog-\nnition with deeply-supervised attention network,‚Äù IEEE\nTrans. Affect. Comput., 2020.\n[40] Y . Li et al., ‚ÄúFacial expression recognition in the wild\nusing multi-level features and attention mechanisms,‚Äù .\n[41] F. Ma, B. Sun, and S. Li, ‚ÄúRobust facial expres-\nsion recognition with convolutional visual transform-\ners,‚Äù arXiv preprint arXiv:2103.16854, 2021.\n[42] C. Huang, ‚ÄúCombining convolutional neural networks\nfor emotion recognition,‚Äù in 2017 IEEE MIT Un-\ndergraduate Research Technology Conference (URTC).\nIEEE, 2017, pp. 1‚Äì4.\n[43] S. Miao, H. Xu, Z. Han, and Y . Zhu, ‚ÄúRecognizing\nfacial expressions using a shallow convolutional neural\nnetwork,‚Äù IEEE Access, vol. 7, pp. 78000‚Äì78011, 2019.\n[44] X. Fan et al., ‚ÄúLearning discriminative representation\nfor facial expression recognition from uncertainties,‚Äù in\nProc. IEEE Int. Conf. Inf. Process.IEEE, 2020, pp. 903‚Äì\n907.\n[45] J. Zeng, S. Shan, and X. Chen, ‚ÄúFacial expression recog-\nnition with inconsistently annotated datasets,‚Äù in Proc.\nEur. Conf. Comput. Vis., 2018, pp. 222‚Äì237.\n[46] A. H. Farzaneh and X. Qi, ‚ÄúDiscriminant distribution-\nagnostic loss for facial expression recognition in the\nwild,‚Äù in Proc. IEEE Conf. Comput. Vis. Pattern Recog.\nWorkshops, 2020, pp. 406‚Äì407."
}