{
  "title": "Probing Neural Language Models for Human Tacit Assumptions",
  "url": "https://openalex.org/W3034912286",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226269064",
      "name": "Weir, Nathaniel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224933226",
      "name": "Poliak, Adam",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222221224",
      "name": "Van Durme, Benjamin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2570058081",
    "https://openalex.org/W1965580172",
    "https://openalex.org/W2106425986",
    "https://openalex.org/W2890073522",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2962843521",
    "https://openalex.org/W2078894097",
    "https://openalex.org/W2251625530",
    "https://openalex.org/W2148946889",
    "https://openalex.org/W3004346089",
    "https://openalex.org/W2985797697",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W90502410",
    "https://openalex.org/W2108783283",
    "https://openalex.org/W2101745498",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2963583512",
    "https://openalex.org/W2910243263",
    "https://openalex.org/W62088058",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2799124508",
    "https://openalex.org/W3155610677"
  ],
  "abstract": "Humans carry stereotypic tacit assumptions (STAs) (Prince, 1978), or propositional beliefs about generic concepts. Such associations are crucial for understanding natural language. We construct a diagnostic set of word prediction prompts to evaluate whether recent neural contextualized language models trained on large text corpora capture STAs. Our prompts are based on human responses in a psychological study of conceptual associations. We find models to be profoundly effective at retrieving concepts given associated properties. Our results demonstrate empirical evidence that stereotypic conceptual representations are captured in neural models derived from semi-supervised linguistic exposure.",
  "full_text": "Probing Neural Language Models for Human Tacit Assumptions\nNathaniel Weir, Adam Poliak, and Benjamin Van Durme\nDepartment of Computer Science, Johns Hopkins University\n{nweir,azpoliak,vandurme}@jhu.edu\nAbstract\nHumans carry stereotypic tacit assumptions (STAs) (Prince,\n1978), or propositional beliefs about generic concepts. Such\nassociations are crucial for understanding natural language.\nWe construct a diagnostic set of word prediction prompts to\nevaluate whether recent neural contextualized language mod-\nels trained on large text corpora capture STAs. Our prompts\nare based on human responses in a psychological study of con-\nceptual associations. We ﬁnd models to be profoundly effec-\ntive at retrieving concepts given associated properties. Our re-\nsults demonstrate empirical evidence that stereotypic concep-\ntual representations are captured in neural models derived from\nsemi-supervised linguistic exposure.\nKeywords: language models; deep neural networks; concept\nrepresentations; norms; semantics\nIntroduction\nRecognizing generally accepted properties about con-\ncepts is key to understanding natural language (Prince,\n1978). For example, if one mentions a bear, one does\nnot have to explicitly describe the animal as having teeth\nor claws, or as being a predator or a threat. This phe-\nnomenon reﬂects one’s held stereotypic tacit assump-\ntions (STAs), i.e. propositions commonly attributed to\n“classes of entities” (Prince, 1978). STAs, a form of\ncommon knowledge (Walker, 1991), are salient to cog-\nnitive scientists concerned with how human representa-\ntions of knowledge and meaning manifest.\nAs “studies in norming responses are prone to re-\npeated responses across subjects” (Poliak et al., 2018),\ncognitive scientists demonstrate empirically that hu-\nmans share assumptions about properties associated\nwith concepts (McRae et al., 2005). We take these con-\nceptual assumptions as one instance of STAs and ask\nwhether recent contextualized language models trained\non large text corpora capture them. In other words, do\nmodels correctly distinguish concepts associated with a\ngiven set of properties? To answer this question, we de-\nsign ﬁll-in-the-blank diagnostic tests (Figure 1) based\non existing data of concepts with corresponding sets of\nhuman-elicited properties.\nBy tracking conceptual recall from prompts of itera-\ntively concatenated conceptual properties, we ﬁnd that\nthe popular neural language models, BERT (Devlin et\nal., 2019) and R OBERTA (Liu et al., 2019), capture\nSTAs. We observe that ROBERTA consistently outper-\nforms BERT in correctly associating concepts with their\nPrompt Model Predictions\nA has fur. dog, cat, fox, ...\nA has fur, is big, and has claws. cat, bear, lion, ...\nA has fur, is big, has claws, has\nteeth, is an animal, eats, is brown,\nand lives in woods.\nbear, wolf, cat, ...\nFigure 1: The concept bear as a target emerging as the highest\nranked predictions of the neural LM ROBERTA-L (Liu et al., 2019)\nwhen prompted with conjunctions of the concept’s human-produced\nproperties.\ndeﬁning properties across multiple metrics; this perfor-\nmance discrepancy is consistent with many other lan-\nguage understanding tasks (Wang et al., 2018). We also\nﬁnd that models associate concepts with perceptual cat-\negories of properties (e.g. visual) worse than with non-\nperceptual ones (e.g. encyclopaedic or functional).\nWe further examine whether STAs can be extracted\nfrom the models by designing prompts akin to those\nshown to humans in psychological studies (McRae et\nal., 2005; Devereux et al., 2014). We ﬁnd signiﬁcant\noverlap between model and human responses, but with\nnotable differences. We provide qualitative examples in\nwhich the models’ predictive associations differ from\nhumans’, yet are still sensible given the prompt. Such\nresults highlight the difﬁculty of constructing word pre-\ndiction prompts that elicit particular forms of reasoning\nfrom models optimized purely to predict co-occurrence.\nUnlike other work analyzing linguistic meaning cap-\ntured in sentence representations derived from language\nmodels (Conneau et al., 2018; Tenney et al., 2019), we\ndo not ﬁne-tune the models to perform any task; we in-\nstead ﬁnd that the targeted tacit assumptions “fall out”\npurely from semi-supervised masked language model-\ning. Our results demonstrate that exposure to large cor-\npora alone, without multi-modal perceptual signals or\ntask-speciﬁc training cues, may enable a model to sufﬁ-\nciently capture STAs.\nBackground\nContextualized Language Models Language models\n(LMs) assign probabilities to sequences of text. They\nare trained on large text corpora to predict the probabil-\nity of a new word based on its surrounding context. Uni-\narXiv:2004.04877v2  [cs.CL]  16 Jun 2020\ndirectional models approximate for any text sequence\nw = [w1,w2, . . .wN] the factorized left-context probabil-\nity p(w) =∏N\ni=1 p(wi |w1 . . .wi−1). Recent neural bi-\ndirectional language models estimate the probability\nof an intermediate ‘masked out’ token given both left\nand right context; this task is colloquially “masked lan-\nguage modelling” (MLM). Training in this way pro-\nduces a probability model that, given input sequence w\nand an arbitrary vocabulary word predicts the distribu-\ntion p(wi = v |w1, . . .wi−1,wi+1, . . .wn). When neural\nbi-directional LMs trained for MLM are subsequently\nused as contextual encoders, 1 performance across a\nwide range of language understanding tasks greatly im-\nproves.\nWe investigate two recent neural LMs: Bi-\ndirectional Encoder Representations from Transformers\n(BERT) (Devlin et al., 2019) and Robustly optimized\nBERT approach (ROBERTA) (Liu et al., 2019). In ad-\ndition to the MLM objective, BERT is trained with an\nauxiliary objective of next-sentence prediction. BERT\nis trained on a book corpus and English Wikipedia.\nUsing an identical neural architecture, R OBERTA is\ntrained for purely MLM (no next-sentence prediction)\non a much larger dataset with words masked out of\nlarger input sequences. Performance increases ubiqui-\ntously on standard NLU tasks when BERT is replaced\nwith ROBERTA as an off-the-shelf contexual encoder.\nProbing Language Models via Word Prediction Re-\ncent research employs word prediction tests to ex-\nplore whether contextualized language models cap-\nture a range of linguistic phenomena, e.g. syn-\ntax (Goldberg, 2019), pragmatics, semantic roles, and\nnegation (Ettinger, 2020). These diagnostics have psy-\ncholinguistic origins; they draw an analogy between the\n“ﬁll-in-the-blank” word predictions of a pre-trained lan-\nguage model and distribution of aggregated human re-\nsponses in cloze tests designed to target speciﬁc sen-\ntence processing phenomena. Similar tests have been\nused to evaluate how well these models capture sym-\nbolic reasoning (Talmor et al., 2019) and relational\nfacts (Petroni et al., 2019).\nStereotypic Tacit Assumptions Recognizing associa-\ntions between concepts and their deﬁning properties is\nkey to natural language understanding and plays “a crit-\nical role in language both for the conventional meaning\nof utterances, and in conversational inference” (Walker,\n1991). Tacit assumption (TAs) are commonly accepted\nbeliefs about speciﬁc entities ( Alice has a dog ) and\nstereotypic TAs (STAs) pertain to a generic concept,\nor a class of entity ( people have dogs ) (Prince, 1978).\nWhile held by individuals, STAs are generally agreed\nupon and are vital for reﬂexive reasoning and pragmat-\n1That is, when used to obtain contextualized representations of\nwords and sequences.\nics; Alice might tell Bob ‘I have to walk my dog!,’ but\nshe does not need to say “I am a person, and people\nhave dogs, and dogs need to be walked, so I have to\nwalk my dog!” Comprehending STAs allows for gen-\neralized recognition of new categorical instances, and\nfacilitates learning new categories (Lupyan et al., 2007),\nas shown in early word learning by children (Hills et al.,\n2009). STAs are not explicitly facts. 2 Rather, they are\nsufﬁciently probable assumptions to be associated with\nconcepts by a majority of people. A partial inspiration\nfor this work was the observation by Van Durme (2010)\nthat the concept attributes most supported by peoples’\nsearch engine query logs (Pasca & Van Durme, 2007)\nwere strikingly similar to examples of STAs listed by\nPrince. That is, there is strong evidence that the beliefs\npeople hold about particular conceptual attributes (e.g.\n“countries have kings”), are reﬂected in the aggregation\nof their most frequent search terms (“what is the name\nof the king of France?”).\nOur goal is to determine whether contextualized lan-\nguage models exposed to large corpora encode associa-\ntions between concepts and their tacitly assumed prop-\nerties. We develop probes that speciﬁcally test a model’s\nability to recognize STAs. Previous works (Rubinstein\net al., 2015; Sommerauer & Fokkens, 2018; Da & Ka-\nsai, 2019) have tested for similar types of stereotypic\nbeliefs; they use supervised training of probing classi-\nﬁers (Conneau et al., 2018) to identify concept/attribute\npairs. In contrast, our word prediction diagnostics ﬁnd\nthat these associations fall out of semi-supervised LM\npretraining. In other words, the neural LM inducts STAs\nas a byproduct of learning co-occurrence without receiv-\ning explicit cues to do so.\nProbing for Stereotypic Tacit Assumptions\nDespite introducing the notion of STAs, Prince (1978)\nprovides only a few examples. We therefore draw\nfrom other literature to create diagnostics that evalu-\nate how well a contexualized language model captures\nthe phenomenon. Semantic feature production norms,\ni.e. properties elicited from human subjects regarding\ngeneric concepts, fall under the category of STAs. Inter-\nested in determining “what people know about different\nthings in the world,”3 McRae et al. (2005) had human\nsubjects list properties that they associated with individ-\nual concepts. When many people individually attribute\nthe same properties to a speciﬁc concept, collectively\nthey provide STAs. We target the elicited properties that\nwere most often repeated across the subjects.\nPrompt Design We construct prompts for evaluating\nSTAs in LMs by leveraging the CSLB Concept Prop-\nerty Norms (Devereux et al., 2014), a large extension\n2E.g., “countries have presidents” does not apply toall countries.\n3 Wording taken from instruction shown to participants—as\nshown in Appendix B of McRae et al. (2005)\nFigure 2: Results from neural LM concept retrieval diagnostic. Mean reciprocal rank and assigned probability of correct concept word sharply\nincrease with the number of conjunctive properties in the prompt.\nof the McRae study that contains 638 concepts each\nlinked with roughly 34 associated properties. The ﬁll-\nin-the-blank prompts are natural language statements in\nwhich the target concept associated with a set of human-\nprovided properties is the missing word . If LMs accu-\nrately predict the missing concept, we posit that they en-\ncode the given STA set. We iteratively grow prompts by\nappending conceptual properties into a single compound\nverb phrase (Figure 1) until the verb phrase contains 10\nproperties. Since we test for 266 concepts, this pro-\ncess creates a total of 2 ,660 prompts.4 Devereux et al.\n(2014) record production frequencies (PF) enumerating\nhow many people produced each property for a given\nconcept. For each concept, we select and append the\nproperties with the highest PF in decreasing order. It-\neratively growing prompts enables a gradient of perfor-\nmance - we observe concept retrieval given few “clue”\nproperties and track improvements as more are given.\nProbing Method Prompts are fed as toknized se-\nquences to the neural LM encoder with the concept to-\nken replaced with a [MASK]. A softmax is taken over\nthe ﬁnal hidden vector extracted from the model at the\nindex of the masked token to obtain a probability dis-\ntribution over the vocabulary of possible words. Fol-\nlowing Petroni et al. (2019), we use a pre-deﬁned, case-\nsensitive vocabulary of roughly 21K tokens to control\nfor the possibility that a model’s vocabulary size inﬂu-\nences its rank-based performance. 5 We use this proba-\nbility distribution to obtain a ranked list of words that\n4Because LMs are highly sensitive to the ‘a/an’ determiner pre-\nceding a masked word e.g. LMs far prefer to complete “A\nbuzzes, ”with “bee,” but prefer e.g. “insect” to complete “An\nbuzzes. ”, a task issue noted by Ettinger (2020). We remove exam-\nples containing concepts that begin with vowel sounds. A prompt\nconstruction that simultaneously accepts words that start with both\nvowels and consonants is left for future work.\n5The vocabulary is constructed from the uniﬁed intersection of\nthose used to train BERT and R OBERTA. We omit concepts that\nare not contained within this intersection.\nthe model believes should be the missing t token. We\nevaluate the BASE (-B) and LARGE (-L) cased models\nof BERT and R OBERTA.\nEvaluation Metrics We use mean reciprocal rank\n(MRR), or 1 /rankLM(target concept), a metric more\nsensitive to ﬁne-grained differences in rank than other\ncommon retrieval metrics such as recall. We track the\npredicted rank of a target concept from relatively low\nranks given few ‘clue’ properties to much higher ranks\nas more properties are appended. MRR above 0 .5 for\na test set indicates that a model’s top 1 prediction is\ncorrect in a majority of examples. We also report the\noverall probability the LM assigns to the target concept\nregardless of rank. This allows us to measure model\nconﬁdence beyond empirical task performance.\nResults\nFigure 2 displays the results. When given just one prop-\nerty, ROBERTA-L achieves a MRR of 0 .23, indicating\nthat the target concept appears on average in the model’s\ntop-5 ﬁll-in predictions (over the whole vocabulary).\nThe increase in MRR and model conﬁdence (y-axis) as\nproperties are iteratively appended to prompts (increas-\ning x-axis) demonstrates that the LMs more accurately\nretrieve the missing concept when given more associ-\nated properties. MRR steeply increases for all models as\nproperties are added to a prompt, but we ﬁnd less stark\nimprovements after the ﬁrst four or ﬁve. The LARGE\nmodels consistently outperform their BASE variants un-\nder both metrics, as do R OBERTAs over the BERTs\nof the same size. R OBERTA-B and BERT-L per-\nform interchangeably. Notably, R OBERTA-L achieves\na higher performance on both metrics when given just\n4 ‘clue’ properties than any other model when pro-\nvided with all 10. R OBERTA-L assigns double the\ntarget probability at 10 properties than that of the next\nbest model (ROBERTA-B). Thus, ROBERTA-L is pro-\n(a)\n (b)\nFigure 3: (a) Comparison of R OBERTA-L’s performance given only properties from each category versus all combined. (b) R OBERTA-L\nperformance given the property sets with the top vs bottom production frequencies (PF) ordered in increasing vs decreasing PF. Plotted against\na randomly sampled and reordered baseline.\nfoundly more conﬁdent in its correct answers than any\nother model. However, that all models achieve at least\nbetween .5 and .85 MRR conditioned on 10 properties\nillustrates the effectiveness of all considered models in\nidentifying concepts given STA sets.\nQualitative Analysis Examples of prompts and cor-\nresponding model predictions are shown in Appendix\nFigure 4. We ﬁnd that model predictions are nearly al-\nways grammatical and semantically sensible. Highly-\nranked incorrect answers generally apply to a subset of\nthe conjunction of properties, or are correct at an inter-\nmediate iteration but become precluded by subsequently\nappended properties. 6 We note that an optimal per-\nformance may not be perfect; not all prompts uniquely\nidentify the target concept, even at 10 properties.7 How-\never, models still perform nearly as well as could be ex-\npected given the ambiguity.\nProperties Grouped by Category To measure\nwhether the the type of property affects the ability of\nLMs to retrieve a concept, we create additional prompts\nthat only contain properties of speciﬁc categories as\ngrouped by Devereux et al. (2014): visual perceptual\n(bears have fur), functional (eat ﬁsh), and encyclopaedic\n(are found in forests).8\nFigure 3a shows that R OBERTA-L performs inter-\nchangeably well given just encyclopedic or functional\ntype properties. BERT (not shown) shows a similar\noverall pattern, but it performs slightly better given en-\ncyclopedic properties than functional. Perceptual prop-\nerties are overall less helpful for models to distinguish\n6E.g. tiger and lion are correct for ‘A has fur, is big, and has\nclaws’but reveal to be incorrect with the appended ‘lives in woods’\n7E.g. the properties of buffalo do not distinguish it from cow.\n8We omit the categories “other perceptual” (bears growl) and\n“taxonomic” (bears are animals), as few concepts have more than\n2-3 such properties.\nconcepts than non-perceptual. This may be the prod-\nuct of category speciﬁcity; while perceptual properties\nare produced by humans nearly as frequently as non-\nperceptual, the average perceptual property is assigned\nto nearly twice as many CSLB concepts as the average\nnon-perceptual (6 to 3). However, the empirical ﬁnding\ncoheres with previous conclusions that models that learn\nfrom language alone lack knowledge of perceptual fea-\ntures (Collell & Moens, 2016; Lucy & Gauthier, 2017).\nSelecting and Ordering Prompts When designing the\nprobes, we selected and appended the 10 properties with\nthe highest production frequencies (PF) in decreasing\norder. To investigate whether these selection and or-\ndering choices affect a model’s performance in the re-\ntrieval task, we compare the top-PF property selection\nmethod with an alternative selection criterion using the\nbottom-PF properties. For both selection methods, we\ncompare the decreasing-PF ordering with a reversed,\nincreasing-PF order. We compare the resulting 4 eval-\nuations against a random baseline that measures per-\nformance using a random permutation of a randomly-\nselected set of properties.9\nFigure 3b compare the differences in performance.\nRegardless of ordering, the selection of the top\n(bottom)-PF features improves (reduces) model perfor-\nmance relative to the random baseline. Ordering by\ndecreasing PF improves performance over the oppo-\nsite direction by up to 0 .2 for earlier sizes of property\nconjunction, but the two strategies converge in perfor-\nmance for larger sizes. This indicates that the selection\nand ordering criteria of the properties may matter when\nadding them to prompts. The properties with lower\nPF are correspondingly less beneﬁcial for model per-\nformance. This suggests that assumptions that are less\n9 The random baseline’s performance is averaged over 5 random\npermutations of 5 random sets for each concept.\nstereotypic—that is, highly salient to fewer humans—\nare less well captured by the LMs.\nEliciting Properties from Language Models\nWe have found that neural language models capture to\na high degree the relationship between human-produced\nsets of stereotypic tacit assumptions and their associated\nconcepts. Can we use the LMs toretrieve the conceptual\nproperties under the same type of setup used for human\nelicitation? We design prompts to replicate the “linguis-\ntic ﬁlter” (McRae et al., 2005) through which the human\nsubjects conveyed conceptual assumptions.\nIn the human elicited studies, subjects were asked\nto list properties that would complete “ {concept}\n{relation}...” prompts in which the relation could take\non10 one of four ﬁxed phrases: is, has, made of , and\ndoes. We mimic this protocol using the ﬁrst three rela-\ntions11 and compare the properties predicted by the LMs\nto the corresponding human response sets. Examples of\nthis protocol are shown in Table 1.\nComparing LM Probabilities with Humans We can\nconsider the listed properties as samples from a fuzzy\nnotion of a human STA distribution conditioned on the\nconcept and relation. These STAs reﬂect how humans\ncodify their probabilistic beliefs about the world. What\na subject writes down about the ‘dog’ concept reﬂects\nwhat that subject believes from their experience to be\nsufﬁciently ubiquitous, i.e. extremely probable, for all\n‘dog’ instances. The dataset also portrays a distribu-\ntion over listed STAs. Not all norms are produced\nby all participants given the same concepts and rela-\ntion prompts; this reﬂects how individuals hold differ-\nent sets of STAs about the same concept. Through\neither of these lenses, we can speculate that the hu-\nman subject produces the sample e.g. ‘fur’ from some\np(STA |concept = bear, relation = has). 12 We can\nconsider our protocol to be sampling from a LM approx-\nimation of such a conditional distribution.\nLimits to Elicitation Asking language models to list\nproperties via word prediction is inherently limiting,\nas the models are not primed to speciﬁcally produce\nproperties beyond whatever cues we can embed in\nthe context of a sentence. In contrast, human sub-\njects were asked directly “What are the properties of\nX?” (Devereux et al., 2014). This is a highly semanti-\ncally constraining question that cannot be directly asked\nof an off-the-shelf language model.\n10Selected at the discretion of the subject via a drop-down menu.\n11We do not investigate the does relation or the open-ended “... ”\nrelation, because the resulting human responses are not easily com-\nparable with LM predictions using template-based prompts. We\nconstruct prompts using is a and has a for broader dataset coverage.\n12This formulation should be taken with a grain of salt; the subject\nis given all relation phrases at once and has the opportunity to ﬁll out\nas many (or few) completions as she deems salient, provided that in\ncombination there are at least 5 total properties listed.\nContext Human ROBERTA-L\nResponse PF Response pLM\n(Everyone\nknows that) a\nbear has .\nfur 27 teeth .36\nclaws 15 claws .18\nteeth 11 eyes .05\ncubs 7 ears .03\npaws 7 horns .02\n(Everyone\nknows that) a\nladder is made\nof .\nmetal 25 wood .33\nwood 20 steel .08\nplastic 4 metal .07\naluminum 2 aluminum .03\nrope 2 concrete .03\nTable 1: Example concept/relation prompts with resulting human\nand R OBERTA-L responses (and corresponding production fre-\nquencies and LM probabilities, resp.). Portions of context prompts\nencased in () were only shown to the model, not human.\nThe phrasing of the question to humans also has im-\nplications regarding salience: when describing a dog,\nhumans would rarely, if never, describe a dog as being\n“larger than a pencil” , even though humans are “ca-\npable of verifying” this property (McRae et al., 2005).\nEven if they do produce a property as opposed to an al-\nternative lexical completion, it may be unfair to expect\nlanguage models to replicate how human subjects pre-\nfer to list properties that distinguish and are salient to\na concept (e.g. ‘goes moo’) as opposed to listing prop-\nerties that apply to many concepts (e.g. ‘has a heart’).\nThus, comparing properties elicited by language models\nto those elicited by humans is a challenging endeavour.\nAnticipating this issue, we prepend the phrase ‘Every-\none knows that’ to our prompts. They therefore take\nthe form shown in the left column of Table 1. For\nthe sake of comparability, we evaluate the models’ re-\nsponses against only the human responses that ﬁt the\nsame syntax. We also remove human-produced proper-\nties with multiple words following the relation (e.g. ‘ is\nfound in forests’) since the contextualized LMs under\nconsideration can only predict a single missing word.\nOur method produces a set of between 495 and 583\nprompts for each of the relations considered.\nResults We use the information retrieval metric mean\naverage precision (mAP) for ranked sequences of pre-\ndictions in which there are multiple correct answers. We\ndeﬁne mAP here given n test examples:\nmAP = 1\nn\nn\n∑\ni=1\n|vocab|\n∑\nj=1\nPi( j)∆ri( j)\nwhere Pi( j) = precision@ j and ∆ri( j) is the change in\nrecall from item j −1 to j for example i. We report\nmAP on prediction ranks over a LM’s entire vocabu-\nlary (mAPVOCAB ), but also over a much smaller vocab-\nulary (mAPSENS ) comprising the set of human comple-\ntions that ﬁt the given prompt syntax for all concepts in\nRelation |Data| Metric Bb Bl Rb Rl\nis 583 mAPVOCAB .081 .080 .078 .190\nmAPSENS .131 .132 .105 .212\nρHuman PF .062 .100 .062 .113\nis a 506 mAPVOCAB .253 .318 .266 .462\nmAPSENS .393 .423 .387 .559\nρHuman PF .226 .389 .385 .386\nhas 564 mAPVOCAB .098 .043 .151 .317\nmAPSENS .171 .138 .195 .367\nρHuman PF .217 .234 .190 .316\nhas a 537 mAPVOCAB .202 .260 .136 .263\nmAPSENS .272 .307 .208 .329\nρHuman PF .129 .153 .174 .209\nmade of 495 mAPVOCAB .307 .328 .335 .503\nmAPSENS .324 .339 .347 .533\nρHuman PF .193 .182 .075 .339\nTable 2: Mean average precision and Spearmanρ with human PF for\nLM prediction of properties given concept/relation pairs. B and R\nindicate BERT and ROBERTA, b and l indicate -BASE and -LARGE .\nthe study. This follows the intuition that responses given\nfor a set of concepts are likely not attributes of the other\nconcepts, and models should be sensitive to this discrep-\nancy. While mAP measures the ability to distinguish\nthe set13 of correct responses from incorrect responses,\nwe also evaluate probability assigned among the correct\nanswers by computing average Spearman’s ρ between\nhuman production frequency and LM probability.\nResults using these metrics are displayed in Table 2.\nWe ﬁnd that R OBERTA-L outperforms the other mod-\nels by up to double mAP. No model’s rank ordering of\ncorrect answers correlates particularly strongly with hu-\nman production frequencies. When we narrow the mod-\nels’ vocabulary to include only the property words pro-\nduced by humans for a given syntax, we ﬁnd that per-\nformance (mAPSENS ) increases ubiquitously.\nQualitative Analysis Models generally provide coher-\nent and grammatically acceptable completions. Most\noutputs fall under the category of ‘veriﬁable by hu-\nmans,’ which as noted by McRae et al. could be listed\nby humans given sufﬁcient instruction. We observe\nproperties that apply to the concept but are not in the\ndataset 14 and properties that apply to senses of a con-\ncept that were not considered in the human responses.15\nWe ﬁnd that some prompts are not sufﬁcently syntacti-\ncally constraining, and license non-nominative comple-\ntions. The relation has permits past participle comple-\ntions (e.g. ‘has arrived’) along with the targeted nomina-\ntive attributes (‘has wheels’). We also ﬁnd that models\nidiosyncratically favor speciﬁc words regardless of the\n13Invariant to order of correct answers.\n14E.g. ‘hamsters are real’ and ‘motorcycles have horsepower’.\n15While human subjects list only properties of the object anchor,\nLMs also provide properties of a television anchor.\nPrince Example ROBERTA -L\nA person has parents, sib-\nlings, relatives, a home, a pet,\na car, a spouse, a job. ,\nperson [.73], child [.1], hu-\nman [.04], family [.03], kid\n[.02]\nA country has a leader, a\nduke, borders, a president,\na queen, citizens, land, a\nlanguage, and a history.\nconstitution [.23], history\n[.07], culture [.07], soul\n[.04], budget [.03], border\n[.03], leader [.03], currency\n[.02], population [.02]\nTable 3: ROBERTA-L captures Prince’s own exemplary STAs (tar-\nget completions bolded), as shown by predictions of both concept\nand properties (associated probability in brackets).\nconcept, which can lead to unacceptable completions.16\nWe provide examples predictions produced by models\nin Appendix Figure 5.\nEffect of Prompt Construction We investigate the\nextent to which our choice of lexical framing impacts\nmodel performance by ablating the step in which “ev-\neryone knows that” is prepended to the prompt. We ﬁnd\na relatively wide discrepancy in effects; with the less-\nened left context, models perform on average .05 and\n.1 mAP worse on the is and has relations respectively,\nbut perform .06 and .01 mAP better on is a and has\na. Notably, R OBERTA-L sees a steep drop in perfor-\nmance on the has relation, losing nearly .3 mAP. We\nobserve that models exhibit highly varying levels of in-\nstability given the choice of context. This highlights the\ndifﬁculty in constructing prompts that effectively target\nthe same type of lexical response from any arbitrary bi-\ndirectional LM.\nCapturing Prince’s STAs\nWe return to Prince (1978) to investigate whether neu-\nral language models, which we have found to capture\nSTAs elicited from humans by McRae, do so as well\nfor what she envisioned. Prince lists some of her own\nSTAs about the concepts country and person. We ap-\nply the methodologies of the previous experiments and\nshow the resulting conceptual recall and feature produc-\ntions in Table 3. We ﬁnd signiﬁcant overlap in both di-\nrections of prediction. Thus, the exact examples of basic\ninformation about the world that Prince considers core\nto discourse and language processing are clearly cap-\ntured by the neural LMs under investigation.\nConclusion\nWe have explored whether the notion owing to Prince\n(1978) of the stereotypic tacit assumption (STA), a\ntype of background knowledge core to natural language\nunderstanding, is captured by contexualized language\n16ROBERTA-B often blindly produces ‘has legs’, the two BERT\nmodels predict that nearly all concepts are ‘ made of wood,’ and all\nmodels except ROBERTA-L often produce ‘is dangerous.’\nmodeling. We developed diagnostic experiments de-\nrived from human subject responses to a psychologi-\ncal study of conceptual representations and observed\nthat recent contextualized LMs trained on large cor-\npora may indeed capture such important information.\nThrough word prediction tasks akin to human cloze\ntests, our results provide a lens of quantitative and qual-\nitative exploration of whether BERT and R OBERTA\ncapture concepts and associated properties. We illus-\ntrate that the conceptual knowledge elicited from hu-\nmans by Devereux et al. (2014) is indeed contained\nwithin an encoder; when a speaker may mention some-\nthing that ‘ﬂies’and ‘has rotating blades, ’the LM can\ninfer the description is of a helicopter. We hope that our\nwork serves to further research in exploring the extent\nof semantic and linguistic knowledge captured by con-\ntextualized language models.\nAcknowledgements\nThis work was supported in part by DARPA KAIROS\n(FA8750-19-2-0034). The views and conclusions con-\ntained in this work are those of the authors and should\nnot be interpreted as representing ofﬁcial policies or en-\ndorsements of DARPA or the U.S. Government.\nReferences\nCollell, G., & Moens, M.-F. (2016). Is an image worth\nmore than a thousand words? on the ﬁne-grain seman-\ntic differences between visual and linguistic represen-\ntations. In COLING.\nConneau, A., Kruszewski, G., Lample, G., Barrault, L.,\n& Baroni, M. (2018). What you can cram into a sin-\ngle $&!#* vector: Probing sentence embeddings for\nlinguistic properties. In ACL.\nDa, J., & Kasai, J. (2019). Cracking the Contextual\nCommonsense Code: Understanding Commonsense\nReasoning Aptitude of Deep Contextual Representa-\ntions. In First Workshop on Commonsense Inference\nin Natural Language Processing.\nDevereux, B. J., Tyler, L. K., Geertzen, J., & Randall, B.\n(2014). The centre for speech, language and the brain\n(CSLB) concept property norms. Behavior Research\nMethods, 46(4).\nDevlin, J., Chang, M.-W., Lee, K., & Toutanova, K.\n(2019). BERT: Pre-training of deep bidirectional\ntransformers for language understanding. In NAACL.\nEttinger, A. (2020). What BERT is not: Lessons from a\nnew suite of psycholinguistic diagnostics for language\nmodels. TACL, 8, 34–48.\nGoldberg, Y . (2019). Assessing BERT’s syntactic abili-\nties. arXiv preprint arXiv:1901.05287.\nHills, T. T., Maouene, M., Maouene, J., Sheya, A., &\nSmith, L. (2009). Categorical Structure among Shared\nFeatures in Networks of Early-learned Nouns. Cogni-\ntion, 112(3), 381–396.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen,\nD., . . . Stoyanov, V . (2019). RoBERTa: A robustly\noptimized BERT pretraining approach. arXiv preprint\narXiv:1907.11692.\nLucy, L., & Gauthier, J. (2017). Are distributional rep-\nresentations ready for the real world? evaluating word\nvectors for grounded perceptual meaning. In First\nWorkshop on Language Grounding for Robotics.\nLupyan, G., Rakison, D. H., & McClelland, J. L. (2007).\nLanguage is not just for talking: Redundant labels fa-\ncilitate learning of novel categories. Psychological\nScience, 18(12), 1077-1083.\nMcRae, K., Cree, G. S., Seidenberg, M. S., & McNor-\ngan, C. (2005). Semantic feature production norms\nfor a large set of living and nonliving things. Behav-\nior Research Methods, 37(4), 547–559.\nPasca, M., & Van Durme, B. (2007). What you seek\nis what you get: Extraction of class attributes from\nquery logs. In IJCAI.\nPetroni, F., Rockt ¨aschel, T., Riedel, S., Lewis, P.,\nBakhtin, A., Wu, Y ., & Miller, A. (2019). Language\nmodels as knowledge bases? In EMNLP.\nPoliak, A., Naradowsky, J., Haldar, A., Rudinger, R., &\nVan Durme, B. (2018). Hypothesis only baselines in\nnatural language inference. In Starsem.\nPrince, E. F. (1978). On the function of existential pre-\nsupposition in discourse. In Chicago Linguistic Soci-\nety (V ol. 14, pp. 362–376).\nRubinstein, D., Levi, E., Schwartz, R., & Rappoport, A.\n(2015). How well do distributional models capture\ndifferent types of semantic knowledge? In ACL.\nSommerauer, P., & Fokkens, A. (2018). Firearms and\ntigers are dangerous, kitchen knives and zebras are\nnot: Testing whether word embeddings can tell. In\nBlackboxNLP .\nTalmor, A., Elazar, Y ., Goldberg, Y ., & Berant, J.\n(2019). oLMpics – On what Language Model Pre-\ntraining Captures. arXiv preprint arXiv:1912.13283.\nTenney, I., Xia, P., Chen, B., Wang, A., Poliak, A., Mc-\nCoy, R. T., . . . Pavlick, E. (2019). What do you learn\nfrom context? probing for sentence structure in con-\ntextualized word representations. In ICLR.\nVan Durme, B. (2010). Extracting Implicit Knowledge\nfrom Text. Unpublished doctoral dissertation, Univer-\nsity of Rochester.\nWalker, M. A. (1991). Common Knowledge: A Survey.\nUniversity of Pennsylvania.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O., &\nBowman, S. (2018). GLUE: A multi-task bench-\nmark and analysis platform for natural language un-\nderstanding. In EMNLP.\nAppendix\nThe following tables show qualitative results of our ex-\nperiments. Figure 4 shows BERT-L and R OBERTA-\nL’s predicted concepts with associated log probabilities\ngiven iteratively longer conjunctions of human-elicited\nproperties. Figure 5 shows examples of property pro-\nduction given concept/relation prompts; they are chosen\nas notable failure cases that exhibit shortcomings of the\nelicitation and evaluation protocol.\nConnection to Web-Extracted Class Attributes\nThis work shows that neural contextualized LMs en-\ncode concept/property pairs commonly held among peo-\nple as hypothesized by Prince (1978). Their ubiquity\nis reﬂected in the frequency by which they were pro-\nduced by subjects of the CSLB property norms study.\nPasca and Van Durme (2007), also concerned with con-\ncepts and their attributes, show that these pairs are re-\nﬂected in the logs of people’s web searches. Their work,\nwhich proposes automatic concept/attribute extraction\nbased on frequency of occurrence in web logs, can be\nviewed as additional support for Prince’s STAs; people\nhold the beliefs that concepts have particular attributes\n(e.g. “countries have kings”), and then reﬂect such be-\nliefs in their queries (“who is the king of France?”).\nAs such, we examine whether the neural LMs under\ninvestigation capture a sample of the concept/attribute\nsets documented in Pasca and Van Durme (2007). Re-\nsults shown in Figure 6 show the signiﬁcant degree to\nwhich these sets are captured by ROBERTA -L.\nContext BERT-L R OBERTA-L\nA bus has wheels. car [-2.4], wheel [-2.9], wagon [-3.2], horse [-3.3] , vehicle\n[-3.9]\ncar [-1.8], bus [-1.9], train [-2.4], bicycle [-2.6] , horse [-3.4]\nA bus has wheels, is made of metal, carries,\nhas a driver, is red, and transports people.\ncar [-1.6], cart,[-2.1], bus [-2.1], truck,[-2.7] , wagon[-2.9] bus [-0.6], car [-1.7], train [-2.7], cab [-3.6] , taxi [-3.7]\nA bus has wheels, is made of metal, carries,\nhas a driver, is red, transports people, has\nseats, is transport, is big, and has windows.\ncar [-1.1], bus [-1.5], truck [-2.6], vehicle [-3.0], tram [-3.2] bus [-0.8], car [-0.9], train [-3.2], truck [-3.6], vehicle [-3.9]\nA cake is tasty. bite [-3.1], meal [-3.3], duck [-3.7], little [-3.9], steak [-4.0] lot [-3.8], steak [-4.1], meal [-4.6], pizza [-4.6], duck [-4.8]\nA cake is tasty, is eaten, is made of sugar, is\nmade of ﬂour, and is made of eggs.\ncake [-2.4], dish [-3.2], sweet [-3.6], pie [-3.8], dessert [-3.9] cookie [-1.2], cake [-1.2], pie [-2.8], meal [-2.9], banana [-\n3.4]\nA cake is tasty, is eaten, is made of sugar, is\nmade of ﬂour, is made of eggs, has icing, is\nbaked, is sweet, is a kind of pudding, and is\nfor special occasions.\ncake [-.7], pie [-3.0], dessert [-3.1], jam [-4.0], dish [-4.1] cake [-.1], pie [-2.6], cookie [-3.9], dessert [-4.3], cream [-6.5]\nA buffalo has horns. lion [-2.9], horse [-3.3], goat [-3.6], man [-3.6], bull [-3.9] bull [-2.8], wolf [-2.9], horse [-3.0], goat [-3.1], cow [-3.3]\nA buffalo has horns, is hairy, is an animal,\nis big, and eats grass.\ngoat [-2.6], man [-2.7], horse [-3.1], bear [-3.3], lion [-3.5] bull [-1.6], cow [-1.8], lion [-2.4], goat [-2.4], horse [-3.1]\nA buffalo has horns, is hairy, is an animal,\nis big, eats grass, lives in herds, is a mam-\nmal, is brown, eats, and has four legs.\nman [-1.8], person [-2.0], goat [-2.9], human [-3.3], horse [-\n3.3]\ncow [-1.1], lion [-2.3], bear [-2.5], deer [-2.6], bull [-2.6]\nA tiger has stripes. number [-4.2], line [-4.2], stripe [-4.3], lot [-4.7], color [-4.8] tiger [-2.4], dog [-3.4], cat [-3.6], lion [-3.7], bear [-3.7]\nA tiger has stripes, is a cat, is orange, is\nbig, and has teeth.\ncat [-1.1], tiger [-2.5], dog [-2.6], person [-3.1], man [-3.6] tiger [-.5], cat [-1.9], lion [-2.8], dog [-3.7], bear [-3.7]\nA tiger has stripes, is a cat, is orange, is\nbig, has teeth, is black, is endangered, is a\nbig cat, is an animal, and is a predator.\ncat [-.4], tiger [-2.7], person [-3.5], lion [-4.2], dog [-4.3] cat [-.3], tiger [-1.6], lion [-3.5], fox [-4.4], bear [-4.5]\nA book has pages. page [-0.9], book [-1.2], ﬁle [-3.8], chapter [-4.1], word [-4.5] book [-0.3], diary [-2.2], novel [-2.8], journal [-3.8], notebook\n[-3.8]\nA book has pages, is made of paper, has a\ncover, is read, and has words.\nbook [-0.06], novel [-4.7], manuscript [-4.7], Bible [-5.4], dic-\ntionary [-5.5]\nbook [-0.01], novel [-4.8], newspaper [-6.0], dictionary [-6.7],\njournal [-7.0],\nA book has pages, is made of paper, has\na cover, is read, has words, is found in li-\nbraries, is used for pleasure, has pictures,\nhas information, and has a spine.\nbook [-0.0] , novel [-4.9], manuscript [-5.4], journal [-5.4],\ndictionary [-5.9]\nbook [-0.0], novel [-4.3], dictionary [-6.1], paperback [-6.3],\njournal [-6.4]\nA helicopter ﬂies. moth [-1.8], bird [-2.3], ﬂy [-2.7], crow [-3.0], bee [-3.0] bird [-2.2], bee [-2.4], butterﬂy [-2.7], bat [-2.9], moth [-3.0]\nA helicopter ﬂies, is made of metal, has ro-\ntors, has a pilot, and is noisy.\nhelicopter [-.9], bird [-3.1], drone [-3.3], plane [-3.8], rotor\n[-3.9]\nplane [-.2], helicopter [-2.1], bird [-4.5], jet [-4.8], airplane\n[-5.9]\nA helicopter ﬂies, is made of metal, has ro-\ntors, has a pilot, is noisy, has blades, has\na propeller, is a form of transport, has an\nengine, and carries people.\nhelicopter [-.3], plane [-3.0], bird [-3.7], vehicle [-4.2], car\n[-4.5]\nplane [-.2], helicopter [-2.1], bird [-4.7], airplane [-5.5], air-\ncraft [-5.8]\nA taxi is expensive. car [-2.5], house [-3.5], divorce [-4.1], ticket [-4.1], horse [-\n4.7]\ncar [-3.0], house [-4.0], lot [-4.1], life [-4.5], horse [-4.6]\nA taxi is expensive, is yellow, is black, is a\ncar, and is for transport.\ncar [-1.0], bicycle [-3.0], vehicle [-3.4], horse [-3.5], bus [-4.1] Mercedes [-1.8], taxi [-1.9], bus [-2.4], Bentley [-3.0], Jaguar\n[-3.0]\nA taxi is expensive, is yellow, is black, is a\ncar, is for transport, is made of metal, has\na meter, has wheels, has passengers, and is\nuseful.\ncar [-.7], bicycle [-2.3], vehicle [-3.0], horse [-3.8],taxi [-4.2] taxi [-1.3], bus [-1.5], car [-2.0], bicycle [-2.8], train [-3.5]\nA telephone is made of plastic. shield [-4.4], chair [-4.4], helmet [-4.5], mask [-4.7], cap [-\n4.7]\ncar [-3.1], condom [-3.7], banana [-3.7], toy [-3.8], toilet [-\n4.0]\nA telephone is made of plastic, is used for\ncommunication, has a speaker, rings, and\nallows you to make calls.\nphone [-.5], telephone [-1.2], mobile [-4.6], receiver [-4.8],\ncell [-5.1]\nphone [-.4], telephone [-1.8], bell [-5.0], radio [-5.3], mobile\n[-5.3]\nA telephone is made of plastic, is used for\ncommunication, has a speaker, rings, al-\nlows you to make calls, has a receiver, has a\nwire, is mobile, has buttons, and has a dial.\nphone [-.8], telephone [-.8], radio [-4.6], mobile [-4.9], re-\nceiver [-5.3]\nphone [-.6], telephone [-1.3], radio [-5.2], mobile [-6.1], bell\n[-6.8]\nFigure 4: Examples of models’ predicted completions with 1, 5, and 10 ‘clue’ features provided. Associated log probability included in\nsquare brackets.\nContext Human BERT-B BERT-L ROBERTA-B ROBERTA-L\nEveryone knows that a\nhamster is .\nsmall, alive, cute,\nwhite, black\ndangerous, good,\nright, funny\ndead, real, dan-\ngerous, involved\ndangerous, evil,\nbad, dead\ncute, adorable,\nharmless, alive\nEveryone knows that a\nbucket is a .\ncontainer, vessel,\ncylinder\nbucket, toilet,\nproblem, mess\nbucket, toilet,\nweapon, tank\nbucket, toilet,\nbomb, hat\ntoilet, bucket,\ntool, container\nEveryone knows that a mo-\ntorcycle has .\nwheels, seats,\nlights, brakes,\ngears\nwheels, arrived,\nescaped, tires\ncrashed, arrived,\ndied, power\nlegs, wings,\nwheels, power\nwheels, brakes,\nhorsepower,\npower\nEveryone knows that an an-\nchor has a .\nchain, cable,\nrope, point\nproblem, story,\nweakness, cam-\nera\npurpose, life,\nweakness, soul\npoint, voice,\npulse, personal-\nity\njob, personality,\nvoice, story\nEveryone knows that a sock\nis made of .\ncotton, fabric,\ncloth, material,\nwool\nwood, leather,\nsteel, iron, metal\ncotton, rubber,\nwool, leather,\nplastic\nrubber, wood,\nmetal, plastic,\nbones\ncotton, wool, fab-\nric, rubber, mate-\nrial\nFigure 5: Examples of models’ predicted completions to concept/relation prompts targeting the production of properties. Predictions are over\nthe full vocabulary intersection.\nPasca and Van Durme (2007) Example ROBERTA -L\nConcept from Attributes Attributes from Concept\nA company has a CEO, a future, a president,\na competitors, a mission statement, an owner,\na website, an organizational structure, a logo,\nand a market share.\ncompany [0.695] , business [0.23],\ncorporation [0.03], startup [0.02],\nbrand [0.01]\nCEO [0.15], culture [0.1], mis-\nsion [0.04], price [0.03], hierarchy\n[0.03], strategy [0.03]\nA country has a capital, a population a presi-\ndent, a map, a capital city , a currency, a cli-\nmate, a ﬂag, a culture, and a leader.\ncountry [0.72], nation [0.25], state\n[0.03], republic [0.002], govern-\nment [0.001]\nconstitution [0.23], history [0.07],\nculture [0.07], soul [0.04], budget\n[0.03], border [0.03]\nA drug has a side effect, a cost, structure, a ben-\neﬁt, a mechanism, overdose, use, a price, and a\npharmacology.\ndrug [0.9], medicine [0.02], prod-\nuct [0.02], medication [0.02], sub-\nstance [0.01]\neffect [0.1], risk [0.1], dependency\n[0.06], potential [0.05], cost [0.04]\nA painter has paintings, works, a portrait, a\ndeath, a style, a artwork, a bibliography, a bio,\nand a childhood.\nperson [0.21], painter [0.2], writer\n[0.14], poet [0.05], book [0.04]\nstyle [0.15], voice [0.1], vision\n[0.07], technique [0.03], palette\n[0.03], soul [0.03]\nFigure 6: R OBERTA-L captures the concept/attribute pairs automatically extracted by Pasca & Van Durme (2007) based on web log fre-\nquency (target completions bolded), as shown by predictions of both concept and properties (associated probability in brackets).",
  "topic": "Tacit knowledge",
  "concepts": [
    {
      "name": "Tacit knowledge",
      "score": 0.7641311883926392
    },
    {
      "name": "Human language",
      "score": 0.4765877425670624
    },
    {
      "name": "Computer science",
      "score": 0.4659823775291443
    },
    {
      "name": "Linguistics",
      "score": 0.28016823530197144
    },
    {
      "name": "Knowledge management",
      "score": 0.24762925505638123
    },
    {
      "name": "Philosophy",
      "score": 0.10293227434158325
    }
  ]
}