{
  "title": "Transformer and GAN-Based Super-Resolution Reconstruction Network for Medical Images",
  "url": "https://openalex.org/W4386041524",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2944680276",
      "name": "Weizhi Du",
      "affiliations": [
        "Washington University in St. Louis"
      ]
    },
    {
      "id": "https://openalex.org/A2258639997",
      "name": "Shihao Tian",
      "affiliations": [
        "Cornell University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2894343975",
    "https://openalex.org/W1976598894",
    "https://openalex.org/W2963037581",
    "https://openalex.org/W2963470893",
    "https://openalex.org/W2883996939",
    "https://openalex.org/W1991867157",
    "https://openalex.org/W2157466038",
    "https://openalex.org/W2093008421",
    "https://openalex.org/W2964042923",
    "https://openalex.org/W2963814095",
    "https://openalex.org/W6735913928",
    "https://openalex.org/W6741832134",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2133665775",
    "https://openalex.org/W3114166611",
    "https://openalex.org/W3005969576",
    "https://openalex.org/W6754089586",
    "https://openalex.org/W2963372104",
    "https://openalex.org/W2919046835",
    "https://openalex.org/W2554753600",
    "https://openalex.org/W2990661479",
    "https://openalex.org/W783608400",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2557668825",
    "https://openalex.org/W2739748921",
    "https://openalex.org/W2888285221",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4320013936",
    "https://openalex.org/W4295521014"
  ],
  "abstract": "Super-resolution reconstruction in medical imaging has become more demanding due to the necessity of obtaining high-quality images with minimal radiation dose, such as in low-field magnetic resonance imaging (MRI). However, image super-resolution reconstruction remains a difficult task because of the complexity and high textual requirements for diagnosis purpose. In this paper, we offer a deep learning based strategy for reconstructing medical images from low resolutions utilizing Transformer and generative adversarial networks (T-GANs). The integrated system can extract more precise texture information and focus more on important locations through global image matching after successfully inserting Transformer into the generative adversarial network for picture reconstruction. Furthermore, we weighted the combination of content loss, adversarial loss, and adversarial feature loss as the final multi-task loss function during the training of our proposed model T-GAN. In comparison to established measures like peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM), our suggested T-GAN achieves optimal performance and recovers more texture features in super-resolution reconstruction of MRI scanned images of the knees and belly.",
  "full_text": "\rC The author(s) 2024. The articles published in this open access journal are distributed under the terms of the\nCreative Commons Attribution 4.0 International License (http://creativecommons.org/licenses/by/4.0/).\n1 Introduction\nComputers have fueled our reliance on images. From the\nﬁrst X-rays of a tumor to the latest magnetic resonance\nimaging (MRI) scans, images have become integral to\nthe practice of every ﬁeld in medicine. The process\nof image acquisition is affected, and often limited, by\nmany aspects, such as the equipment, environment, and\ncost. For instance, to reduce the radiation exposed on\nhuman body, computed tomography (CT) is required\nto decrease its beam’s energy, resulting in scanned\nimages with lower spatial resolution. In the medical\n\u000fWeizhi Du is with the Arts & Sciences College, Washington\nUniversity in St. Louis, St. Louis, MO 63130, USA. E-mail:\nd.weizhi@wustl.edu.\n\u000fShihao Tian is with the Department of Electric and Computing\nEngineering, Cornell University, Ithaca, NY 14850, USA. E-\nmail: st689@cornell.edu.\n\u0003To whom correspondence should be addressed.\nManuscript received: 2022-10-17; revised: 2022-12-21;\naccepted: 2022-12-29\ndiagnostic process, low-quality images can affect the\npathological assessment by clinical experts and auxiliary\ncomputers, among others[1–4]. Calciﬁcation, for example,\nis a common symptom of most breast cancers, but\ncalciﬁcation is small and difﬁcult to detect. As a\nresult, the low intensity variation between pathological\ntissue and healthy areas makes the diagnostic process\ncumbersome. In the diagnostic retinal images of fundus,\nmany lesions cover extremely tiny areas and can be\nshown as microaneurysms or hemorrhages. Also, there\nare parts that may not be clearly visible such as soft\nexudates, certain neointima formation, etc.[5] Therefore,\nsuper-resolution reconstruction for medical images has\nbecome an essential role in clinical applications.\nThere were two main types of image super-resolution\nreconstruction techniques: single image super-resolution\n(SISR), where a high-resolution image was acquired\nfrom a single low-resolution image, and reference-\nbased image super-resolution (RefSR), where a high-\nresolution image was synthesized from multiple low-\nTSINGHUA SCIENCE AND TECHNOLOGY\nISSN ll1007-0214 16/22 pp197–206\nDOI: 1 0 . 2 6 5 9 9 / T S T . 2 0 2 2 . 9 0 1 0 0 7 1\nVolume 29, Number 1, February 2024\nTransformer and GAN-Based Super-Resolution Reconstruction\n  Network for Medical Images\nWeizhi Du and Shihao Tian\u0003\nAbstract: Super-resolution reconstruction in medical imaging has become more demanding due to the necessity of \nobtaining high-quality images with minimal radiation dose, such as in low-ﬁeld magnetic resonance imaging (MRI).\nHowever, image super-resolution reconstruction remains a difﬁcult task because of the complexity and high textual \nrequirements for diagnosis purpose. In this paper, we offer a deep learning based strategy for reconstructing medical \nimages from low resolutions utilizing Transformer and generative adversarial networks (T -GANs). The integrated \nsystem can extract more precise texture information and focus more on important locations through global image \nmatching after successfully inserting Transformer into the generative adversarial network for picture reconstruction.\nFurthermore, we weighted the combination of content loss, adversarial loss, and adversarial feature loss as the ﬁnal \nmulti-task loss function during the training of our proposed model T -GAN. In comparison to established measures like \npeak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM), our suggested T -GAN achieves \noptimal performance and recovers more texture features in super-resolution reconstruction of MRI scanned images \nof the knees and belly.\nKey words: super-resolution; image reconstruction; Transformer; generative adversarial network (GAN)\n198 Tsinghua Science and Technology, February2024, 29(1): 197–206\nresolution images. Among them, the goal of SISR\noften required optimizing the mean square error between\nhigh-resolution (HR) and SR pixels, however, the\nuse of mean put error often led to edge blurring\ndue to the uncomfortable nature of super-resolution\n(illposed). The main reason was that the high-resolution\ntexture of a single image was often over corrupted\nand a large amount of information was lost, leading\nto unrecoverable textures[6]. While generating a large\nnumber of adversarial samples from images based on\ngenerative adversarial networks could alleviate such\nproblems[7], the resulting hallucinations and artifacts\nposed a greater challenge to image super-resolution\ntasks.\nRefSR had been shown to be promising in providing\nreference (Ref) images with similar content to the\nlowresolution (LR) input to recover high-resolution (HR)\nimage details[8]. A large number of RefSR methods\nhad produced visually more pleasing results compared\nto SISR methods. Currently, RefSR is mainly used\nto make full use of the Ref image information by\nmethods such as image aligning and “patch matching”.\nReferences [ 9–11] aligned LR and Ref images from\ndifferent perspectives.\nIn Ref. [9], landmark aligned LR and Ref by a global\nregistration while minimizing energy; In Ref. [10], LR\nand Ref images needed to be pre-aligned ﬁrst, however,\nnon-uniform warping operation was used to enhance\nRef images by matching LR and Ref feature maps to\nobtain super resolution; In Ref. [ 11], the method used\noptical ﬂow to align LR and Ref pictures at different\nscales and connected them to the decoder’s relevant\nlayers. However, the quality of the alignment between\nLR and Ref had a signiﬁcant impact on the performance\nof these approaches. In addition, alignment methods\nsuch as optical ﬂow required a large computational cost,\nmaking it difﬁcult to be popularized in practical. On the\nother hand, Refs. [6, 8, 12, 13] used a “patch matching”\napproach to search for suitable reference information\nin the Ref image to complement the information and\nthus obtained super resolution. In Ref. [12], the gradient\nfeatures in the downsampled Ref were searched to match\nthe LR patch; in Ref. [8], the features in the CNN were\nused instead of gradient features to match the patch of\nRef and LR, while the LR image was expanded using the\nSISR method; In Ref. [13], features in VGG were used\nto match the patch of Ref and LR, and super-resolution\nwas obtained by swapping similar texture features. In\nRef. [ 6], a texture transformer network was used to\nfeature-match Ref and LR, where the low-resolution\n(LR) and reference (Ref) images were represented as\nqueries and keywords in the Transformer, respectively.\nThis setup allowed the LR and the Ref images to learn\nfeatures together, i.e., deep feature correspondences\nthat could improve accuracy in texture can be detected\nthrough an attention mechanism. However, when the\nreference image was less sharp, the quality of the RefSR\nmight receive a devastating impact, resulting in impaired\nperformance of the algorithm.\nIn this paper, our objective is to unleash the potential\nof RefSR by generating 71 reference images with more\ntexture details through generative adversarial networks,\nand to discover deep feature correspondences by using\nthe Transformer framework to perform joint feature\nlearning between LR and reference (Ref) images. The\ncomparing result could be found in Fig. 1, which\nFig. 1 Schematic diagram of the model framework.\nWeizhi Du et al.: 199\ndemonstrate the effectiveness of our method on medical\nimages. The main contributions of this paper are as\nfollows:\n\u000fPropose a generative adversarial network (GAN)\nframework to recover the detailed information of the\noriginal photo from the severely downsampled (low-\nresolution) image to obtain a high-resolution image using\nthe powerful generative power of GAN networks.\n\u000f Introduce Transformer architecture to extract\nlearnable texture features.\n2 Model Description\nIn this section, we would review previous classical\nalgorithms for building single image super-resolution\n(SISR) for GAN-based framework in a single image\nand classical methods for RefSR in the Transformer\nframework for reference-based image super-resolution,\nwhich are the most relevant to our work.\n2.1 GANs in image reconstruction\nIn image super-resolution reconstruction tasks, generative\nadversarial networks (GANs) [7] had emerged as an\neffective method for enhancing the perceptual quality of\nupsampled images[7, 14–16]. GANs were an effective min-\nmax two-player game[7]. The generator G captured the\ndata distribution, while the discriminator D continuously\ndistinguishes whether the samples were from the training\ndataset or not. GANs could produce more aesthetically\nattractive images without supervised input using this\npowerful method. However, due to their intrinsic\ninstability, the initial GANs were difﬁcult to train.\nWasserstein GANs (WGANs) used weight clipping to\nensure that D is in the space of 1-Lipschit [17]; the\nimproved training of Wasserstein GANs (WGANGP)[18]\ncould use gradient penalties to encourage D to learn\nsmoother decision boundaries; and Gulrajani et al. [19]\nproposed a weighted combination of WGANs and\nWGAN-GP loss terms to form a complex loss function\nthat facilitates the model to generate. However, the above\nGANs were only applied on SISR while limited to\nimage datasets with single-scale upsampling at relatively\nlow target resolution[20]. A multi-scale GAN-enhanced\nSISR approach was proposed in Ref. [ 8], which was\nprogressive in both architecture and training, similar\nto what was done in course learning, simulating the\nlearning process from easy to hard; Lim et al. [21]\nproposed a framework for recovering its ﬁne texture\ndetails when super-resolution at magniﬁcation factors.\nThis framework proposed a perceptual loss function,\nwhich consisted of an adversarial loss and a content\nloss. The calculation of adversarial loss allowed the\nreconstructed image to be pushed towards the natural\nimage stream shape, while constructing a discriminator\nnetwork for distinguishing the super-resolution image\nfrom the original photo-realistic image.\nThe traditional reference learning based image\nsuperresolution reconstruction model (RefSR) took the\nhighresolution image as the reference (Ref), so that the\nrelevant texture was transferred to the low-resolution\n(LR) image. Currently, RefSR mainly made full use of\nthe image information of Ref by methods such as image\naligning and “patch matching”. However, most of the\ntraditional methods fed all swapped features equally into\nthe main network, neglecting to transfer high-resolution\ntextures from the reference image using attention\nmechanisms, thus limiting the application of these\nmethods in challenging situations[6, 8]. Following that,\nRef. [6] proposed a new texture transformation network\nfor image super-resolution (TTSR), in which the LR and\nRef pictures were represented as queries and keywords\nin the transformation, respectively. A learnable texture\nextractor for deep neural networks (DNNs), a relevance\nembedding module, a hard-attention module for texture\ntransfer, and a soft-attention module for texture synthesis\nwere all part of TTSR, which was tuned for the image\nproduction task. This approach supported cooperative\nfeature learning across LR and Ref pictures, allowing\nattention to uncover deep feature correspondences and\nproduce accurate texture features. The suggested texture\nconverter could also be stacked in a cross-scale manner,\nallowing texture recovery at multiple magniﬁcation\nlevels (e.g., from 1 \u0002to 4 \u0002). The method, however,\nrelied on a high-resolution image as a reference (Ref),\nwhile in practice, a huge number of low-resolution\nimages were frequently obtained. Therefore, we used\nGANs to enhance the quality of low-resolution images\nfrom the perspective of low-resolution images as a\nreference, used the enhanced images for information\ncomplementation, and used the attention mechanism\nto transfer the effective features of different images to\nLR images to achieve RefSR reconstruction in complex\nenvironments.\n2.2 Deep-learning models based on Transformer\nand GAN\nIn fact, SISR aims to learn the non-linear mapping\nrelations between LR and HR images. In general, this\nnon-linear mapping can be expressed as\nTransformer and GAN-Based Super-Resolution Reconstruction Network for Medical Images\n200 Tsinghua Science and Technology, February2024, 29(1): 197–206\nyD\u001e.k \u0002xCn/ (1)\nwhere \u001e is the non-linear compression operator, k\nrepresents the convolution operation, nrepresents the\nrandom noise, yrepresents the degenerated LR image,\nand x is real HR image. In general, Eq. (1) can be\nsimpliﬁed as\nyDDx (2)\nwhere D is the degeneracy matrix representing the\ndown-sampling operation. Since the conditions for the\ndiscomfort inverse issue expressed in single image\nsuperresolution are not sufﬁcient, xcan not be recovered\nin the simple way that\nxDD\u00001y (3)\nFortunately, deep learning based models have made\na huge success in image processing ﬁelds. Many\nresearchers have applied these deep learning models to\nreconstruct HR images from LR images, which actually\nlearns an implicit mapping between LR and HR.\nOxDF.y/ (4)\nin which Oxrepresents the reconstructed high resolution\nimage corresponding to the ground-truth image x.\nTechnicality, deep neural network models minimize the\noptimization objective mainly by training a network F\nas\n1\nN\nNX\niD1\n.F.yi/\u0000xi/2 (5)\nwhere N is the number of training samples. In general,\nthis type of deep learning model can be represented as\nOxDFd.\u0001\u0001\u0001F3.F2.F1.y//// (6)\nwhere d denotes the number of layers of the deep\nnetwork (number of convolutional layers).\n2.3 Proposed framework\nRather than just increasing the network depth, the key\ngoal is to increase the performance of SISR neural\nnetworks by selecting optimal internal mechanisms. The\ngenerative network and the adversarial network are the\ntwo key components of our proposed model, as depicted\nin Fig. 1. The residual learning channel and the texture\nTransformer channel are two elements of the generative\nnetwork.\nEventually, we intend to train a generative function\nG that estimates its corresponding HR image from a\ngiven LR input image. To achieve this, we propose a\ngenerative network consisting of two channels, residual\nlearning and texture Transformer. Here we use \u0012G to\ndenote all parameters of the generative network as well\nas the bias term that is learned by optimizing a particular\nSR reconstruction loss lSR. Speciﬁcally, for a given\ntraining HR image IHR\nn and its corresponding LR image\nILR\nn , the objective of the generative network is\nO\u0012G Darg min 1\nNlSR.G\u0012.ILR\nn /;IHR\nn / (7)\nMultiple residual learning blocks and deconvolution\nblocks make up the residual learning channel (as\nshown in Fig. 1). Because of their success in\nimage classiﬁcation, convolutional operations are now\nfrequently utilized in deep learning, and several studies\nhave transferred CNNs to SISR. These CNN-based SR\ntechniques, on the other hand, rarely consider whether\nconvolutional processes are appropriate for the SISR\nmechanism. The majority of them just apply CNN\nmodels to SISR from image classiﬁcation tasks. The\nmain objective of SISR is to ﬁgure out how LR and\nHR images are related. For the mapping relationship\nbetween LR and HR images, it can be represented by a\nsimple linear degenerate model as follows:\nyDx\u0002k (8)\nThe convolution theorem states that spatial convolution\ncan be converted to frequency-domain multiplication.\nF.y/DF.x/\u0001F.k/ (9)\nwhere F.\u0001/ is the Fourier transform and \u0001 is the\ncorresponding element multiplication. Thus, in the\nFourier domain, xcan be expressed as\nxDF\u00001.F.y/=F.k//DF\u00001.1=F.k//\u0003y (10)\nwhere F\u00001 denotes the inverse Fourier transform and \u0003\ndenotes the convolution operation. Thus, the true HR\nimage can be recovered from the low-resolution image\nyby a pseudo-inverse calculation, i.e.,\nxDk\u0003y;\nwhere \u0003denotes the deconvolution operation.\nUsually, the deconvolution kernel k is hard to\nobtain. Therefore, we construct multiple residual\nlearning blocks and a deconvolution block to implement\nthe deconvolution operation. Speciﬁcally, we use a\nconvolution kernel with a small size 3 \u00023 and 64\nfeature mappings as the convolution layer followed\nby a batch normalization layer, while employing the\nReLU function as the activation function. A residual\nlearning mechanism is introduced (constant mapping) in\norder to avoid information loss and also to eliminate\nthe gradient disappearance and gradient explosion\nphenomena. Finally we use the deconvolution layer\n(step size D0:5/proposed by Zheng et al.[13] to improve\nthe resolution of the input image.\nFor the texture Transformer channel, similar to the\nsetup in Ref. [ 6] (shown in Fig. 2), LR, LR\", and Ref\nWeizhi Du et al.: 201\nFig. 2 Schematic diagram of the texture Transformer strategy.\ndenote the input image, the 4\u0002double-triple upsampling\ninput image, and the reference image, respectively. We\napply the double triple downsampling and upsampling\nin turn, using the same factor 4 \u0002on Ref to obtain\nRef \"#, with the domain consistent with LR \". The\ntexture converter accepts the LR features generated by\nRef, Ref \"#, and LR \"trunk and outputs a synthetic\nfeature map which is further used to generate HR\npredictions. The texture converter consists of four\ncomponents: a learnable texture extractor (LTE), a\ncorrelation embedding module (CEM), a hard attention\nmodule (HA) for feature transfer, and a soft attention\nmodule (SA) for feature synthesis.\nLTE mainly uses an end-to-end model to train the\nlearning parameters such that the images of LR and\nRef are able to perform joint feature learning and\ntherefore capture more accurate texture features. LTE\nmainly extracts the texture features of the following\nthree images and notates them as Q (query), K (key),\nand V (value): QDLTE.LR \"/; KDLTE.Ref#\"/;\nand V DLTE.Ref/, where LTE ./denotes the output\nof the learnable texture extractor. After extracting\nthe texture features, the RE establishes the matching\nrelationship between LR and Ref images by estimating\nthe similarity between Q and K. First, Q and K are\nexpanded into a number of patches (patches), which are\nused to compute normalized inner products to obtain the\ncorrelation between each patch; similarly, HA transfers\nfeatures for the most relevant positions in each Q and\nV. As a technique to fully merge LR and Ref related\ninformation, SA employs a soft attention mechanism in\nwhich relevant texture transfers are ampliﬁed and less\nrelevant texture transfers are avoided. In conclusion, the\ntexture converter can effectively convert key HR texture\ncharacteristics in the reference image to LR texture\nfeatures, allowing for more accurate texture production.\n2.4 Loss function\nThe perceptual loss function lSR deﬁnition guides the\noptimization direction of the generative network and is\ncritical to the performance of the model. We use mean\nsquared error (MSE) to lSR modeling and express the\nperceptual loss as a weighted sum of content loss and\nadversarial loss components using features extracted\nfrom the texture Transformer channel, as follows:\nlSR Dl.XSR/C10\u00003l.GenSR/ (11)\nwhere l.XSR/ is content loss, and l.GenSR/ is the\nadversarial loss.\n2.4.1 Content loss\nTraditional content loss is often based on pixel-wise\nMSE loss, e.g.,\nlSR\nMSE D 1\nr2WH\nrWX\nxD1\nrHX\nyD1\n.IHR\nx;y \u0000G\u0012G .ILR/x;y/2 (12)\nwhere W and H refer to the width and height of the\ninput image, respectively.\nHowever this loss tends to make the model ignore\nthe high frequency content information during training,\nmaking the solution to the problem of overly smooth\ntextures (OST) not ideal. Instead of relying on pixel loss,\nTransformer and GAN-Based Super-Resolution Reconstruction Network for Medical Images\n202 Tsinghua Science and Technology, February2024, 29(1): 197–206\nour proposed model is based on the difference between\nthe features extracted by the texture Transformer channel,\nand then we deﬁne the texture-based Transformer loss\nas the difference between the reconstructed image\nG\u0012G .ILR/and the reference image IHR as the Euclidean\ndistance between the feature representations of the\nreconstructed image and the reference image.\nlSR\nVGG=i;j D 1\nWi;jHi;j\nWi;jX\nxD1\nHi;jX\nyD1\n.\u001ei;j.IHR/x;y\u0000\n\u001ei;j.G\u0012G .ILR//x;y/2 (13)\n2.4.2 Adversarial loss\nThe content loss describes the difference between\nthe reconstructed image and the reference image,\nwhile we also need to consider the loss incurred\nwhen reconstructing the image using GAN. When\nreconstructing an image using GAN, we need to cheat\nthe discriminator network to obtain a more insurgent\nimage, while generating a loss based on the probability\nthat the discriminator produces a natural sample over all\ntraining samples deﬁned as\nlSR\nGen D\nNX\nnD1\n\u0000log R\u0012R;n .G\u0012G .ILR// (14)\nwhere R\u0012R;n .G\u0012G .ILR// denotes the reconstructed\nimage G\u0012G .ILR/ is the estimated probability of the\nnatural HR image.\n3 Experiment\nIn this section, the proposed model is analyzed in\ncomparison with bicubic interpolation and some typical\ndeep CNN based image super-resolution reconstruction\nmodel frameworks, including enhanced deep residual\nnetworks for single image super-resolution (EDSR)[21],\nand wide activation for efﬁcient and accurate image\nsuper-resolution (WDSR)[22]. EDSR and WDSR won\nthe international competitions NTIRE 2017 and NTIRE\n2018 image high-resolution competitions, respectively.\n3.1 Validation indicator\nWe conducted experiments on a number of benchmark\nmedical image datasets. For a fair quantitative\ncomparison, we use peak signal-to-noise ratio (PSNR)\nand structural similarity index measure (SSIM) [23] for\nSR framework assessment, and the evaluation indicators\nof PSNR and SSIM are calculated as follows:\nMSE D 1\nM2\nMX\ni\u00001\nMX\nj\u00001\n.a.i;j/ \u0000b.i;j// 2 (15)\nPSNR D10\u0001log10.MAX2\nMSE / (16)\nˇSIM D .2\u0016a\u0016b Cc1/.2\u001bab Cc2/\n.\u00162a C\u00162\nb Cc1/.\u001ba C\u001bb Cc2/ (17)\nwhere a is generated image, b is ground truth image,\nM is image size, and MAX is gray scale’s maximum\nvalue; \u0016 and \u001b denote the mean and variance,\nrespectively, and \u001bab denotes the covariance of the\ntwo images; two constants c1 D.0:01\u0000MAX/2 and\nc2 D.0:03\u0000MAX/2 were calculated according to the\nSSIM convention.\n3.2 Dataset and implementation details\nTo validate the effectiveness of our model in real medical\nimages, we selected separate datasets of MRI scans of\nthe knee and abdomen datasets  for comparison tests.\nMRI imaging methods are completely different from CT\nimages and natural images in general, and each pixel’s\nvalue in MRI images has no particular physical meaning.\nBefore training and testing, zero-mean normalization had\nto be applied to each MRI image (i.e., the normalization\ncalculation is to use each value to subtract the mean and\nthen divide by standard deviation). The low-resolution\nMRI image slices were obtained by averaging the 4\u00024\npooling over the original high-resolution MRI image\nslices. We set \u00151 D5\u000210\u00002;\u00152 D5\u000210\u00003\nand \u0015L1 D\n10\u00002 for training the loss function in the proposed super-\nresolution model and iterative optimization using the\nAdam optimizer[24] with ˇ1 D0:9and ˇ2 D0:999, and\nthe initial learning rate is set to 10\u00004.\nIt should be noted that we used knee MRI images,\nabdominal MRI images, and chest CT images [25] as\nthe training set. To enlarge the training set, we crop\nthe single original image into multiple small images\nof the same size while the downsampling factor is set\nto 4 to obtain low-resolution input images. Following\nthat, the suggested depth model is trained using the\nobtained lowresolution dataset as well as the original\nhigh-resolution dataset.\n3.3 Super-resolution reconstruction results of MRI\nimages\nFirst we choose the MRI images of knee and abdomen\nfor testing. After the reconstruction process, the\nPNSR/SSIM test results for the knee MRI test images\nfor all comparing methods are shown in Table 1. The\nPNSR/SSIM test results for the abdominal MRI images\nare shown in Table 2. It is worth mentioning that all\nhttp://mridata.org/about\n203\nTable 1 Reconstruction results of each comparing algorithm on MRI images of the knee.\nImage No. T-GANWDSREDSRBicubic\nSSIMPSNR SSIMPSNR SSIMPSNR SSIMPSNR\n0.903332.921 0.940534.56 0.946834.68 0.952635.26\n0.880231.582 0.936933.26 0.940534.12 0.950235.03\n0.890231.883 0.935433.38 0.937833.87 0.951435.16\n0.868228.564 0.921532.24 0.928933.18 0.945334.17\n0.872231.215 0.932433.12 0.946534.14 0.948734.98\n0.882831.23Average 0.933333.312 0.940133.998 0.949634.92\nTable 2 Reconstruction results of each comparing algorithm on abdominal MRI images.\nImage No. T-GANWDSREDSRBicubic\nSSIMPSNR SSIMPSNR SSIMPSNR SSIMPSNR\n0.884231.281 0.902633.86 0.924234.23 0.932735.13\n0.890131.342 0.915834.15 0.938434.98 0.931834.62\n0.880730.783 0.900532.98 0.927633.86 0.929434.76\n0.898632.164 0.932734.87 0.948734.98 0.940135.13\n0.887230.845 0.912432.57 0.930533.74 0.938934.28\n0.880429.476 0.911832.12 0.931633.68 0.939134.19\n0.886930.98Average 0.912633.43 0.933534.25 0.935334.69\nmetrics were calculated on cropped photos in order\nto eliminate the impact of non-subject areas. The\nquantitative results show that for knee MRI images, our\nproposed T-GAN model achieves the best performance\non the PSNR/SSIM metrics. For abdominal MRI images,\nour model essentially achieves optimal performance,\nwith individual image WDSR slightly outperforming our\nmodel. The experimental results proves that our model\nis more suitable for medical image super-resolution\nreconstruction than the existing deep learning based\nimage super-segmentation models.\nWe likewise give the visualization comparison results\nfor each comparison algorithm, as shown in Figs. 3\nand 4. It can be seen that the reconstructed images\nbased on bicubic interpolation and deep learning\nbased EDSR and WDSR both show oversmoothing\nphenomenon. In contrast, our T-GAN performs better\nfor the reconstruction of detail information due to\nthe texture Transformer structure. Also, Figs. 3 and 4\nclearly show that our proposed T-GAN provides the\nbest reconstruction of details, with very low amount of\nartifacts and noise. The reconstructed images based on\nbicubic interpolation and deep learning based EDSR\nboth exhibit some loss of detail information due to the\nloss of some salient image features during the ﬁltering\nprocess.\n3.4 Super-resolution reconstruction of low-dose\nCT images\nInstead of typical MRI images, the proposed image\n(a) Original HD image\n (b) Bicubic\n(c) EDSR\n (d) WDSR\n(e) T-GAN\nFig. 3 Reconstruction results of each algorithm for MRI\nimages of the knee.\nWeizhi Du et al.: Transformer and GAN-Based Super-Resolution Reconstruction Network for Medical Images\n204 Tsinghua Science and Technology, February2024, 29(1): 197–206\n(a) Original HD image\n (b) Bicubic\n(c) EDSR\n (d) WDSR\n(e) T-GAN\nFig. 4 Reconstruction results of each algorithm for\nabdominal MRI images.\nreconstruction algorithm can also be applied to other\nmedical pictures, such as X-ray scans and computed\ntomography (CT) scans. X-ray and CT scans are widely\nused in clinical applications such as noninvasive illness\ndetection, anatomical imaging, and treatment planning.\nThese imaging approaches, however, have some serious\nlimitations and disadvantages. Because it requires\nhigh energy electromagnetic wave to pass through\nhuman body during imaging process, the radiation\ndamage is unavoidable and high image precision often\nrequires greater energy from the scanner. Low-dose\nCT (LDCT) is currently the clinically recommended\nstrategy for preventing irreversible radiation harm to\nthe body, however, it comes at the cost of getting CT\npictures with low resolution or noise contamination. The\nspatial resolution is generally coarser than typical CT\nimaging which has a high signal-to-noise ratio. As a\nresult, obtaining high-resolution scanned images with a\nlowdose CT scanner would be signiﬁcantly beneﬁcial to\nboth the doctors and patients for diagnosis purpose.\nIn this section, we selected chest CT images of\nCOVID-19 patients in an actual hospital [25] for our\nexperiments. The visualization results of the experiments\nare shown in Figs. 5 and 6. The experimental results\nshow that our proposed T-GAN is also applicable to\nthe super-resolution reconstruction of low-dose CT\nimages, and the high-resolution images obtained by our\nmodel have more detailed information compared with\nthe baseline algorithm.\n4 Conclusion\nIn this paper, we present a super-resolution model\n(TGAN) for medical pictures based on Transformer and\ngenerative adversarial network (GAN), with Tansformer\napproach and residual learning as two generator channels.\nThe results suggest that our proposed T-GAN model\ncan be employed directly for super-resolution MRI\nimage reconstruction, and that our reconstruction\nmethods preserve more texture information than generic\nimage reconstruction algorithms. The ﬁndings of the\nexperiments suggest that using the super-resolution\nreconstruction model to recover more picture details\n(a) Original HD image\n (b) Bicubic\n(c) EDSR\n (d) WDSR\n(e) T-GAN\nFig. 5 Reconstruction results of each algorithm for low-doze\nchest CT images: Case 1.\nWeizhi Du et al.: 205\n(a) Original HD image\n (b) Bicubic\n(c) EDSR\n (d) WDSR\n(e) T-GAN\nFig. 6 Reconstruction results of each algorithm for low-doze\nchest CT images: Case 2.\nfrom clinically collected low-resolution images is\npossible (e.g., LDCT, low-ﬁeld MRI, and MRI spectral\nimaging).\nReferences\n[1] X. Lu, Z. Huang, and Y . Yuan, MR image super-resolution\nvia manifold regularized sparse learning, Neurocomputing,\nvol. 162, pp. 96–104, 2015.\n[2] A. Rueda, N. Malpica, and E. Romero, Single-image\nsuper-resolution of brain MR images using overcomplete\ndictionaries, Med. Image Anal., vol. 17, no. 1, pp. 113–132,\n2013.\n[3] Y . Zhang, Z. Dong, P. Phillips, S. Wang, G. Ji, and J.\nYang, Exponential wavelet iterative shrinkage thresholding\nalgorithm for compressed sensing magnetic resonance\nimaging, Inf. Sci., vol. 322, pp. 115–132, 2015.\n[4] G. Zheng, G. Han, and N. Q. Soomro, An inception module\nCNN classiﬁers fusion method on pulmonary nodule\ndiagnosis by signs, Tsinghua Science and Technology, vol.\n25, no. 3, pp. 368–383, 2020.\n[5] X. Yang, S. Zhan, C. Hu, Z. Liang, and D. Xie, Super-\nresolution of medical image using representation learning,\nin Proc. 2016 8th Int. Conf. Wireless Communications &\nSignal Processing (WCSP), Yangzhou, China, 2016, pp.\n1–6.\n[6] F. Yang, H. Yang, J. Fu, H. Lu, and B. Guo, Learning\ntexture transformer network for image super-resolution, in\nProc. 2020 IEEE/CVF Conf. Computer Vision and Pattern\nRecognition (CVPR), Seattle, WA, USA, 2020, pp. 5790–\n5799.\n[7] I. J. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu,\nD. Warde-Farley, S. Ozair, A. Courville, and Y . Bengio,\nGenerative adversarial nets, in Proc. 27th Int. Conf. on\nNeural Information Processing Systems, Montreal, Canada,\n2014, pp. 2672–2680.\n[8] Z. Zhang, Z. Wang, Z. Lin, and H. Qi, Image super-\nresolution by neural texture transfer, in Proc. 2019\nIEEE/CVF Conf. Computer Vision and Pattern Recognition\n(CVPR), Long Beach, CA, USA, 2019, pp. 7974–7983.\n[9] Y . Wang, Y . Liu, W. Heidrich, and Q. Dai, The light ﬁeld\nattachment: Turning a DSLR into a light ﬁeld camera using\na low budget camera ring,IEEE Trans. Vis. Comput. Graph.,\nvol. 23, no. 10, pp. 2357–2364, 2016.\n[10] H. Yue, X. Sun, J. Yang, and F. Wu, Landmark image super-\nresolution by retrieving web images, IEEE Trans. Image\nProcess., vol. 22, no. 12, pp. 4865–4878, 2013.\n[11] H. Zheng, M. Ji, H. Wang, Y . Liu, and L. Fang, CrossNet:\nAn end-to-end reference-based super resolution network\nusing cross-scale warping, in Proc. European Conference\non Computer Vision, Munich, Germany, 2018, pp. 87–104.\n[12] V . Boominathan, K. Mitra, and A. Veeraraghavan,\nImproving resolution and depth-of-ﬁeld of light ﬁeld\ncameras using a hybrid imaging, in Proc. 2014 IEEE Int.\nConf. on Computational Photography(ICCP), Santa Clara,\nCA, USA, 2014, pp. 1–10.\n[13] H. Zheng, M. Ji, L. Han, Z. Xu, H. Wang, Y . Liu, and\nL. Fang, Learning cross-scale correspondence and patch-\nbased synthesis for reference-based super-resolution, in\nProc. British Machine Vision Conference, London, UK,\n2017, pp. 1–13.\n[14] C. Ledig, L. Theis, F. Husz´ar, J. Caballero, A. Cunningham,\nA. Acosta, A. Aitken, A. Tejani, J. Totz, Z. Wang, et\nal., Photo-realistic single image super-resolution using\na generative adversarial network, in Proc. 2017 IEEE\nConf. Computer Vision and Pattern Recognition(CVPR),\nHonolulu, HI, USA, 2017, pp. 105–114.\n[15] M. S. M. Sajjadi, B. Sch¨olkopf, and M. Hirsch, EnhanceNet:\nSingle image super-resolution through automated texture\nsynthesis, in Proc. 2017 IEEE Int. Conf. Computer Vision\n(ICCV), Venice, Italy, 2017, pp. 4501–4510.\n[16] X. Wang, K. Yu, C. Dong, and C. C. Loy, Recovering\nrealistic texture in image super-resolution by deep spatial\nfeature transform, in Proc. 2018 IEEE/CVF Conf. Computer\nVision and Pattern Recognition, Salt Lake City, UT, USA,\n2018, pp. 606–615.\n[17] Y . Wang, F. Perazzi, B. McWilliams, A. Sorkine-Hornung,\nO. Sorkine-Hornung, and C. Schroers, A fully progressive\napproach to single-image super-resolution, in Proc. 2018\nIEEE/CVF Conf. Computer Vision and Pattern Recognition\nWorkshops (CVPRW), Salt Lake City, UT, USA, 2018, pp.\n977–97709.\n[18] M. Arjovsky, S. Chintala, and L. Bottou, Wasserstein\ngenerative adversarial networks, in Proc. 34th Int. Conf.\nMachine Learning, Sydney, Australia, 2017, pp. 214–223.\nTransformer and GAN-Based Super-Resolution Reconstruction Network for Medical Images\n206 Tsinghua Science and Technology, February2024, 29(1): 197–206\n[19] I. Gulrajani, F. Ahmed, M. Arjovsky, V . Dumoulin, and A.\nCourville, Improved training of Wasserstein GANs, inProc.\n31st Int. Conf. on Neural Information Processing Systems,\nLong Beach, CA, USA, 2017, pp. 5769–5779.\n[20] X. Zhu, L. Zhang, L. Zhang, X. Liu, Y . Shen, and S. Zhao,\nGAN-based image super-resolution with a novel quality\nloss, Math. Probl. Eng., vol. 2020, p. 5217429, 2020.\n[21] B. Lim, S. Son, H. Kim, S. Nah, and K. M. Lee, Enhanced\ndeep residual networks for single image super-resolution,\nin Proc. 2017 IEEE Conf. Computer Vision and Pattern\nRecognition Workshops (CVPRW), Honolulu, HI, USA,\n2017, pp. 1132–1140.\n[22] J. Yu, Y . Fan, J. Yang, N. Xu, Z. Wang, X. Wang, and T.\nHuang, Wide activation for efﬁcient and accurate image\nsuper-resolution, arXiv preprint arXiv: 1808.08718, 2018.\n[23] Z. Wang, A. C. Bovik, H. R. Sheikh, and E. P. Simoncelli,\nImage quality assessment: From error visibility to structural\nsimilarity, IEEE Trans. Image Process., vol. 13, no. 4, pp.\n600–612, 2004.\n[24] D. P. Kingma and L. J. Ba, Adam: A method for\nstochastic optimization, presented at Int. Conf. on Learning\nRepresentations, San Diego, CA, USA, 2015.\n[25] H. Gunraj, L. Wang, and A. Wong, COVIDNet-CT: A\ntailored deep convolutional neural network design for\ndetection of COVID-19 cases from chest CT images, Front.\nMed. (Lausanne), vol. 7, p. 608525, 2020.\nShihao Tianreceived the PhD degree from\nCornell University, USA in 2019, the MS\ndegree from Cornell University, USA in\n2015, and the BS degree from University\nof Virginia, USA in 2012. He is currently\ninterested in applying AI to physics and\nscientiﬁc research. Also, he is exploring\nthe potential of employing NLP technology\nto improve STEM education and facilitate the research progress\nof younger students. He is a member of IEEE and AAPT, and he\nserves as the judge of ISEF, CONRAD, and PUPC competitions.\nWeizhi Du is a rising freshman at\nWashington University in St. Louis, St.\nLouis, MO, USA. He is interested in\nlearning computer science and taking\ndigital artwork.",
  "topic": "Artificial intelligence",
  "concepts": [
    {
      "name": "Artificial intelligence",
      "score": 0.7005037665367126
    },
    {
      "name": "Computer science",
      "score": 0.682279646396637
    },
    {
      "name": "Computer vision",
      "score": 0.5398015975952148
    },
    {
      "name": "Generative adversarial network",
      "score": 0.5378619432449341
    },
    {
      "name": "Transformer",
      "score": 0.5372495651245117
    },
    {
      "name": "Deep learning",
      "score": 0.5284005999565125
    },
    {
      "name": "Generative grammar",
      "score": 0.48290109634399414
    },
    {
      "name": "Adversarial system",
      "score": 0.4713135361671448
    },
    {
      "name": "Iterative reconstruction",
      "score": 0.4547651410102844
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4417412281036377
    },
    {
      "name": "Image resolution",
      "score": 0.42382827401161194
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39215996861457825
    },
    {
      "name": "Image (mathematics)",
      "score": 0.297148197889328
    },
    {
      "name": "Engineering",
      "score": 0.09216752648353577
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}