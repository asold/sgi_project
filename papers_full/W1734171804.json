{
    "title": "Information Extraction Using the Structured Language Model",
    "url": "https://openalex.org/W1734171804",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4293488235",
            "name": "Chelba, Ciprian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2230404914",
            "name": "Mahajan Milind",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2100578078",
        "https://openalex.org/W1598003989",
        "https://openalex.org/W1989705153",
        "https://openalex.org/W2049633694",
        "https://openalex.org/W2116502454",
        "https://openalex.org/W2101461290",
        "https://openalex.org/W2096175520"
    ],
    "abstract": "The paper presents a data-driven approach to information extraction (viewed as template filling) using the structured language model (SLM) as a statistical parser. The task of template filling is cast as constrained parsing using the SLM. The model is automatically trained from a set of sentences annotated with frame/slot labels and spans. Training proceeds in stages: first a constrained syntactic parser is trained such that the parses on training data meet the specified semantic spans, then the non-terminal labels are enriched to contain semantic information and finally a constrained syntactic+semantic parser is trained on the parse trees resulting from the previous stage. Despite the small amount of training data used, the model is shown to outperform the slot level accuracy of a simple semantic grammar authored manually for the MiPad --- personal information management --- task.",
    "full_text": "arXiv:cs/0108023v1  [cs.CL]  29 Aug 2001\nInformation Extraction Using the Structured Language Model\nCiprian Chelba and Milind Mahajan\nMicrosoft Research\nMicrosoft Corporation\nOne Microsoft Way, Redmond, WA 98052\n{chelba,milindm}@microsoft.com\nAbstract\nThe paper presents a data-driven approach to infor-\nmation extraction (viewed as template ﬁlling) using\nthe structured language model (SLM) as a statistical\nparser. The task of template ﬁlling is cast as con-\nstrained parsing using the SLM. The model is auto-\nmatically trained from a set of sentences annotated\nwith frame/slot labels and spans. Training pro-\nceeds in stages: ﬁrst a constrained syntactic parser\nis trained such that the parses on training data meet\nthe speciﬁed semantic spans, then the non-terminal\nlabels are enriched to contain semantic information\nand ﬁnally a constrained syntactic+semantic parser\nis trained on the parse trees resulting from the pre-\nvious stage. Despite the small amount of training\ndata used, the model is shown to outperform the\nslot level accuracy of a simple semantic grammar\nauthored manually for the MiPad — personal infor-\nmation management — task.\n1 Introduction\nInformation extraction from text can be character-\nized as template ﬁlling (Jurafsky and Martin, 2000):\na given template or frame contains a certain num-\nber of slots that need to be ﬁlled in with segments\nof text. Typically not all the words in text are rele-\nvant to a particular frame. Assuming that the seg-\nments of text relevant to ﬁlling in the slots are non-\noverlapping contiguous strings of words, one can rep-\nresent the semantic frame as a simple semantic parse\ntree for the sentence to be processed. The tree has\ntwo levels: the root node is tagged with the frame\nlabel and spans the entire sentence; the leaf nodes\nare tagged with the slot labels and span the strings\nof words relevant to the corresponding slot.\nConsider the semantic parse S for a sentence W\npresented in Fig. 1. CalendarTask is the frame tag,\n(CalendarTask schedule meeting with\n(ByFullName*Person megan hokins) about\n(SubjectByWildCard*Subject internal lecture)\nat ( PreciseTime*Time two thirty p.m.))\nFigure 1: Sample sentence and semantic parse\nspanning the entire sentence; the remaining ones are\nslot tags with their corresponding spans.\nIn the MiPad scenario (Huang et al., 2000) — es-\nsentially a personal information management (PIM)\ntask — there is a module that is able to convert\nthe information extracted according to the semantic\nparse into speciﬁc actions. In this case the action is\nto schedule a calendar appointment.\nWe view the problem of information extraction as\nthe recovery of the two-level semantic parse S for a\ngiven word sequence W .\nWe propose a data driven approach to information\nextraction that uses the structured language model\n(SLM) (Chelba and Jelinek, 2000) as an automatic\nparser. The parser is constrained to explore only\nparses that contain pre-set constituents — spanning\na given word string and bearing a tag in a given\nset of semantic tags. The constraints available dur-\ning training and test are diﬀerent, the test case con-\nstraints being more relaxed as explained in Section 4.\nThe main advantage of the approach is that it\ndoesn’t require any grammar authoring expertise.\nThe approach is fully automatic once the annotated\ntraining data is provided; it does assume that an\napplication schema — i.e. frame and slot structure\n— has been deﬁned but does not require seman-\ntic grammars that identify word-sequence to slot or\nframe mapping. However, the process of convert-\ning the word sequence coresponding to a slot into\nactionable canonical forms — i.e. convert half past\ntwo in the afternoon into 2:30 p.m. — may require\ngrammars. The design of the frames — what infor-\nmation is relevant for taking a certain action, what\nslot/frame tags are to be used, see (Wang, 1999) —\nis a delicate task that we will not be concerned with\nfor the purposes of this paper.\nThe remainder of the paper is organized as fol-\nlows: Section 2 reviews the structured language\nmodel (SLM) followed by Section 3 which describes\nin detail the training procedure and Section 4 which\ndeﬁnes the operation of the SLM as a constrained\nparser and presents the necessary modiﬁcations to\nthe model. Section 5 compares our approach to oth-\ners in the literature, in particular that of (Miller et\nal., 2000). Section 6 presents the experiments we\nhave carried out. We conclude with Section 7.\n2 Structured Language Model\nWe proceed with a brief review of the structured\nlanguage model (SLM); an extensive presentation\nof the SLM can be found in (Chelba and Jelinek,\n2000). The model assigns a probability P (W, T )\nto every sentence W and its every possible binary\nparse T . The terminals of T are the words of W\nwith POStags, and the nodes of T are annotated\nwith phrase headwords and non-terminal labels. Let\n(<s>, SB)   .......   (w_p, t_p) (w_{p+1}, t_{p+1}) ........ (w_k, t_k) w_{k+1}.... </s>\nh_0 = (h_0.word, h_0.tag)h_{-1}h_{-m} = (<s>, SB)\nFigure 2: A word-parse k-preﬁx\nW be a sentence of length n words to which we\nhave prepended the sentence beginning marker <s>\nand appended the sentence end marker </s> so that\nw0 =<s> and wn+1 =</s>. Let Wk = w0 . . . w k be\nthe word k-preﬁx of the sentence — the words from\nthe begining of the sentence up to the current po-\nsition k — and WkTk the word-parse k-preﬁx. Fig-\nure 2 shows a word-parse k-preﬁx; h_0 .. h_{-m}\nare the exposed heads, each head being a pair (head-\nword, non-terminal label), or (word, POStag) in the\ncase of a root-only tree. The exposed heads at a\ngiven position k in the input sentence are a function\nof the word-parse k-preﬁx.\n2.1 Probabilistic Model\nThe joint probability P (W, T ) of a word sequence W\nand a complete parse T can be broken into:\nP (W, T ) =\n∏ n+1\nk=1 [P (wk/W k− 1Tk− 1) · P (tk/W k− 1Tk− 1, w k) ·\n∏ Nk\ni=1 P (pk\ni /W k− 1Tk− 1, w k, t k, p k\n1 . . . p k\ni− 1)]\nwhere:\n• Wk− 1Tk− 1 is the word-parse ( k − 1)-preﬁx\n• wk is the word predicted by WORD-\nPREDICTOR\n• tk is the tag assigned to wk by the TAGGER\n• Nk −1 is the number of operations the PARSER\nexecutes at sentence position k before passing\ncontrol to the WORD-PREDICTOR (the Nk-th\noperation at position k is the null transition);\nNk is a function of T\n• pk\ni denotes the i-th PARSER operation carried\nout at position k in the word string; the opera-\ntions performed by the PARSER are illustrated\nin Figures 3-4 and they ensure that all possible\nbinary branching parses with all possible head-\nword and non-terminal label assignments for the\nw1 . . . w k word sequence can be generated. The\npk\n1 . . . p k\nNk sequence of PARSER operations at\nposition k grows the word-parse ( k − 1)-preﬁx\ninto a word-parse k-preﬁx.\n...............\nT'_0\nT_{-1} T_0<s> T'_{-1}<-T_{-2}\nh_{-1} h_0\nh'_{-1} = h_{-2}\nT'_{-m+1}<-<s>\nh'_0 = (h_{-1}.word, NTlabel)\nFigure 3: Result of adjoin-left under NTlabel\n............... T'_{-1}<-T_{-2} T_0\nh_0h_{-1}\n<s>\nT'_{-m+1}<-<s>\nh'_{-1}=h_{-2}\nT_{-1}\nh'_0 = (h_0.word, NTlabel)\nFigure 4: Result of adjoin-right under NTlabel\nOur model is based on three probabilities, each\nestimated using deleted interpolation and parame-\nterized (approximated) as follows:\nP (wk/W k− 1Tk− 1) .= P (wk/h 0, h − 1)\nP (tk/w k, W k− 1Tk− 1) .= P (tk/w k, h 0, h − 1)\nP (pk\ni /W kTk) .= P (pk\ni /h 0, h − 1)\nIt is worth noting that if the binary branching struc-\nture developed by the parser were always right-\nbranching and we mapped the POStag and non-\nterminal label vocabularies to a single type then\nour model would be equivalent to a trigram lan-\nguage model. Since the number of parses for a\ngiven word preﬁx Wk grows exponentially with k,\n|{Tk}| ∼ O(2k), the state space of our model is huge\neven for relatively short sentences, so we had to use\na search strategy that prunes it. Our choice was a\nsynchronous multi-stack search algorithm which is\nvery similar to a beam search.\nThe language model probability assignment for\nthe word at position k + 1 in the input sentence is\nmade using:\nP (wk+1/W k) =\n∑\nTk ∈ Sk\nP (wk+1/W kTk) · ρ(WkTk),\nρ(WkTk) = P (WkTk)/\n∑\nTk∈ Sk\nP (WkTk) (1)\nwhich ensures a proper probability over strings W ∗ ,\nwhere Sk is the set of all parses present in our stacks\nat the current stage k.\n2.2 Model Parameter Estimation\nEach model component — WORD-PREDICTOR,\nTAGGER, PARSER — is initialized from a set\nof parsed sentences after undergoing headword\npercolation and binarization. Separately for each\nmodel component we:\n• gather counts from “main” data — about 90%\nof the training data\n• estimate the interpolation coeﬃcients on counts\ngathered from “check” data — the remaining\n10% of the training data.\nAn N-best EM (Dempster et al., 1977) variant is\nthen employed to jointly re-estimate the model pa-\nrameters such that the likelihood of the training data\nunder our model is increased.\n3 Training Procedure\nThis section describes the training procedure for\nthe SLM when applied to information extraction\nand introduces the modiﬁcations that need to be\nmade to the SLM operation.\nThe training of the model proceeds in four stages:\n1. initialize the SLM as a syntactic parser for the\ndomain we are interested in. A general purpose\nparser (such as NLPwin (Heidorn, 1999)) can\nbe used to generate a syntactic treebank from\nwhich the SLM parameters can be initialized.\nAnother possibility for initializing the SLM is\nto use a treebank for out-of-domain data (such\nas the UPenn Treebank (Marcus et al., 1993))\n— see Section 6.1.\n2. train the SLM as a matched constrained parser .\nAt this step the parser is going to propose a set\nof N syntactic binary parses for a given word\nstring (N-best parsing), all matching the con-\nstituent boundaries speciﬁed by the semantic\nparse: a parse T is said to match the seman-\ntic parse S, denoted T ∋ S, if and only if the\nset of un-labeled constituents that deﬁne S is\nincluded in the set of constituents that deﬁne\nT .\nAt this time only the constituent span informa-\ntion in S is taken into account.\n3. enrich the non-terminal and pre-terminal labels\nof the resulting parses with the semantic tags\n(frame and slot) present in the semantic parse,\nthus expanding the vocabulary of non-terminal\nand pre-terminal tags used by the syntactic\nparser to include semantic information along-\nside the usual syntactic tags.\n4. train the SLM as a L(abel)-matched constrained\nparser to explore only the semantic parses for\nthe training data. This time the semantic con-\nstituent labels are taken into account too, which\nmeans that a parse P — containing both syn-\ntactic and semantic information — is said to\nL(abeled)-match S if and only if the set of la-\nbeled semantic constituents that deﬁnes S is\nidentical to the set of semantic constituents that\ndeﬁnes P . If we let SEM (P ) denote the func-\ntion that maps a tree P containing both syntac-\ntic and semantic information to the tree con-\ntaining only semantic information, referred to\nas the semantic projection of P , then all the\nparses Pi, ∀i < N , proposed by the SLM for a\ngiven sentence W , L-match S and thus satisfy\nSEM (Pi) = S, ∀i < N .\nThe semantic tree S has a two level structure\nso the above requirement can be satisﬁed only if\nthe parses SEM (P ) proposed by the SLM are\nalso on two levels, frame and slot level respec-\ntively. We have incorporated this constraint\ninto the operation of the SLM — see Section 4.2.\nThe model thus trained is then used to parse\ntest sentences and recover the semantic parse us-\ning S = SEM (arg maxPi P (Pi, W )). In principle,\none should sum over all the parses P that yield\nthe same semantic parse S and then choose S =\narg maxS\n∑\nPis.t. SEM(Pi)=S P (Pi, W ).\nA few iterations of the N-best EM variant — see\nSection 2 — were run at each of the second and\nfourth step in the training procedure. The con-\nstrained parser operation makes this an EM variant\nwhere the hidden space — the possible parse trees\nfor a given sentence — is a priori limited by the se-\nmantic constraints to a subset of the hidden space\nof the unrestricted model. At test time we wish to\nrecover the most likely subset of the hidden space\nconsistent with the constraints imposed on the sen-\ntence.\nTo be more speciﬁc, during the second training\nstage, the E-step of the reestimation procedure will\nonly explore syntactic trees (hidden events) that\nmatch the semantic parse; the fourth stage E-steps\nwill consider hidden events that are constrained even\nfurther to L-match the semantic parse. We have no\nproof that this procedure should lead to better per-\nformance in terms of slot/frame accuracy but intu-\nitively one expects it to place more and more proba-\nbility mass on the desirable trees — that is, the trees\nthat are consistent with the semantic annotation.\nThis is conﬁrmed experimentally by the fact that\nthe likelihood of the training word sequence (observ-\nable) — calculated by Eq. (1) where the sum runs\nover the parse trees that match/L-match the seman-\ntic constraints — does increase 1 at every training\nstep, as presented in Section 6, Table 1. However,\n1Equivalent to a decrease in perplexity\nthe increase in likelihood is not always correlated\nwith a decrease in error rate on the training data,\nsee Tables 2 and 3 in Section 6.\n4 Constrained Parsing Using the\nStructured Language Model\nWe now detail the constrained operation of the SLM\n— matched and L-matched parsing — used at the\nsecond and fourth steps of the training procedure\ndescribed in the previous section.\nA semantic parse S for a given sentence W con-\nsists of a set of constituent boundaries along with\nsemantic tags. When parsing the sentence using the\nstandard formulation of the SLM, one obtains binary\nparses that are not guaranteed to match the seman-\ntic parse S, i.e. the constituent proposed by the\nSLM may cross semantic constituent boundaries; for\nthe constituents matching the semantic constituent\nboundaries, the labels proposed may not be the de-\nsired ones.\nTo ﬁx terminology, we deﬁne a constrained con-\nstituent — or simply a constraint — c to be a span\ntogether with a set2 of allowable tags for the span:\nc =< l, r, Q > where l is the left boundary of the\nconstraint, r is the right boundary of the constraint\nand Q is the set of allowable non-terminal tags for\nthe constraint.\nA semantic parse can be viewed as a set of con-\nstraints; for each constraint the set of allowable non-\nterminal tags Q contains a single element, respec-\ntively the semantic tag for each constituent. An ad-\nditional fact to be kept in mind is that the semantic\nparse tree consists of exactly two levels: the frame\nlevel (root semantic tag) and the slot level (leaf se-\nmantic tags).\nDuring training, we wish to constrain the SLM op-\neration such that it considers only parses that match\nthe constraints ci, i = 1 . . . C as it proceeds left to\nright through a given sentence W . In light of the\ntraining procedure sketched in the introduction, we\nconsider two ﬂavors of constrained parsing, one in\nwhich we only generate parses that match the con-\nstraint boundaries and another in which we also en-\nforce that the proposed tag for every matching con-\nstituent is among the constrained set of non-terminal\ntags ci.Q — L(abeled)-match constrained parsing.\nThe only constraints available for the test sen-\ntences are:\n• the semantic tag of the root node — which\nspans the entire sentence — must be in the\nset of frame tags. If it were a test sen-\ntence the example in Figure 1 would have\nthe following semantic parse (constraints):\n({CalendarTask,ContactsTask,MailTask}\n2The set of allowable tags must contain at least one ele-\nment\nschedule meeting with megan hokins about\ninternal lecture to two thirty p.m.)\n• the semantic projection of the trees proposed\nby the SLM must have exactly two levels; this\nconstraint is built in the operation of the L-\nmatch parser.\nThe next section will describe the constrained\nparsing algorithm. Section 4.2 will describe fur-\nther changes that the algorithm uses to produce\nonly parses P whose semantic projection SEM (P )\nhas exactly two levels, frame (root) and slot (leaf)\nlevel, respectively — only in the L-match case. We\nconclude with Section 4.3 explaining how the con-\nstrained parsing algorithm interacts with the prun-\ning of the SLM search space for the most likely parse.\n4.1 Match and L-match SLM Parsing\nThe trees produced by the SLM are binary trees.\nThe tags annotating the nodes of the tree are purely\nsyntactic — during the second training stage — or\nsyntactic+semantic — during the last training stage\nor at test time. It can be proved that satisfying\nthe following two conditions at each position k in\nthe input sentence ensures that all the binary trees\ngenerated by the SLM parsing algorithm match the\npre-set constraints ci, i = 1 . . . C as it proceeds left to\nright through the input sentence W = w0 . . . w n+1.\n• for a given word-parse k-preﬁx WkTk (see Sec-\ntion 2) accept an adjoin transition if and only\nif:\n1. the resulting constituent does not violate 3\nany of the constraints ci, i = 1 . . . C\n2. L-match parsing only: if the seman-\ntic projection of the non-terminal tag\nSEM (NT tag ) proposed by the adjoin op-\neration is non-void then the newly created\nconstituent must L-match an existing con-\nstraint, ∃ ci s.t. SEM (NT tag ) ∈ ci.Q .\n• for a given word-parse k-preﬁx WkTk (see Sec-\ntion 2) accept the null transition if and only\nif all the constraints ci whose right boundary\nis equal to the current word index k, ci.r = k,\nhave been matched. If these constraints remain\nun-matched they will be broken at a later time\nduring the process of completing the parse for\nthe current sentence W : there will be an adjoin\noperation involving a constituent to the right\nof the current position that will break all the\nconstraints ending at the current position k.\n4.2 Semantic Tag Layering\nThe two-layer structure of the semantic trees need\nnot be enforced during training, simply L-matching\n3A constraint is violated by a constituent if the span of\nthe constituent crosses the span of the constraint.\nthe semantic constraints will implicitly satisfy this\nconstraint. As explained above, for test sentences\nwe can only specify the frame level constraint, leav-\ning open the possibility of generating a tree whose\nsemantic projection would contain more than two\nlevels — nested slot level constituents. In order\nto avoid this, each tree in a given word-parse has\ntwo bits that describe whether the tree already con-\ntains a constituent whose semantic projection is a\nframe/slot level tag, respectively. An adjoin opera-\ntion proposing a tag that violates the correct layer-\ning of frame/slot level tags can now be detected and\ndiscarded.\n4.3 Interaction with Pruning\nIn the absence of pruning the search for the most\nlikely parse satisfying the constraints for a given\nsentence becomes computationally intractable 4. In\npractice, we are forced to use pruning techniques in\norder to limit the size of the search space. However,\nit is possible that during the left to right traversal of\nthe sentence, the pruning scheme will keep alive only\nparses whose continuation cannot meet constraints\nthat we have not encountered yet and no complete\nparse for the current sentence can be returned. In\nsuch cases, we back-oﬀ to unconstrained parsing —\nregular SLM usage. In our experiments, we noticed\nthat this was necessary for very few training sen-\ntences (1 out of 2,239) and relatively few test sen-\ntences (31 out of 1,101).\n5 Comparison with Previous Work\nThe use of a syntactic parser augmented with se-\nmantic tags for information information from text is\nnot a novel idea. The basic approach we described\nis very similar to the one presented in (Miller et al.,\n2000) however there are a few major diﬀerences:\n• in our approach the augmentation of the syn-\ntactic tags with semantic tags is straightforward\ndue to the fact that the semantic constituents\nare matched exactly 5. The approach in (Miller\net al., 2000) needs to insert additional nodes\nin the syntactic tree to account for the seman-\ntic constituents that do not have a correspond-\ning syntactic one. We believe our approach en-\nsures tighter coupling between the syntactic and\nthe semantic information in the ﬁnal augmented\ntrees.\n• our constraint deﬁnition allows for a set of se-\nmantic tags to be matched on a given span.\n4It is assumed that the constraints for a given sentence are\nconsistent, namely there exists at least one parse that meets\nall of them.\n5This is a consequence of the fact that the SLM generates\nbinary trees\n• the two-level layering constraint on semantic\ntrees is a structural constraint that is embed-\nded in the operation of the SLM and thus can\nbe guaranteed on test sentences.\nThe semantic annotation required by our task is\nmuch simpler than that employed by (Miller et al.,\n2000). One possibly beneﬁcial extension of our work\nsuggested by (Miller et al., 2000) would be to add\nsemantic tags describing relations between entities\n(slots), in which case the semantic constraints would\nnot be structured strictly on the two levels used in\nthe current approach, respectively frame and slot\nlevel. However, this would complicate the task of\ndata annotation making it more expensive.\nThe same constrained EM variant employed for\nreestimating the model parameters has been used\nby (Pereira and Schabes, 1992) for training a purely\nsyntactic parser showing increase in likelihood but\nno improvement in parsing accuracy.\n6 Experiments\nWe have evaluated the model on manually annotated\ndata for the MiPad (Huang et al., 2000) task. We\nhave used 2,239 sentences (27,119 words) for train-\ning and 1,101 sentences (8,652 words) for test. There\nwere 2,239/5,431 semantic frames/slots in the train-\ning data and 1,101/1,698 in the test data, respec-\ntively.\nThe word vocabulary size was 1,035, closed over\nthe test data. The slot and frame vocabulary\nsizes were 79 and 3, respectively. The pre-terminal\n(POStag) vocabulary sizes were 64 and 144 for train-\ning steps 2 and 4 (see Section 3), respectively; the\nnon-terminal (NTtag) vocabulary sizes were 61 and\n540 for training steps 2 and 4 (see Section 3), re-\nspectively. We have used the NLPwin (Heidorn,\n1999) parser to obtain the MiPad syntactic treebank\nneeded for initializing the SLM at training step 1.\nTraining Perplexity\nStage It Training Set Test set\n2 (matched) 0 9.27 34.81\n2 (matched) 1 5.81 31.25\n2 (matched) 2 5.51 31.41\n4 (L-matched) 0 4.71 24.39\n4 (L-matched) 1 4.61 24.73\n4 (L-matched) 2 4.56 24.88\nTable 1: Likelihood Evolution during Training\nAlthough not guaranteed theoretically, the N-best\nEM variant used for the SLM parameter reestima-\ntion increases the likelihood of the training data\nwith each iteration when the parser is run in both\nmatched (training step 2) and L-matched (training\nstep 4) constrained modes. Table 1 shows the evo-\nlution of the training and test data perplexities (cal-\nculated using the probability assignment in Eq. 1)\nduring the constrained training steps 2 and 4.\nThe training data perplexity decreases monoton-\nically during both training steps whereas the test\ndata perplexity doesn’t decrease monotonically in ei-\nther case. We attribute this discrepancy between the\nevolution of the likelihood on the training and test\ncorpora to the diﬀerent constrained settings for the\nSLM.\nThe most important performance measure is the\nslot/frame error rate. To measure it, we use man-\nually created parses which consist of frame-level la-\nbels and slot-level labels and spans as reference. A\nframe-level error is caused by a frame label of the\nhypothesis parse which is diﬀerent from the frame\nlabel of the reference. In order to calculate the slot-\nlevel errors, we create a set of slot label and slot\nspan pairs for the reference and hypothesis parse,\nrespectively. The number of slot errors is then the\nminimum edit distance between these 2 sets using\nthe substitution, insertion and deletion operations\non the elements of the set.\nTable 2 shows the error rate on training and test\ndata at diﬀerent stages during training. The last col-\numn of test data results (Test-L1) shows the results\nobtained by assuming that the user has speciﬁed the\nidentity of the frame — and thus the frame level con-\nstraint contains only the correct semantic tag. This\nis a plausible scenario if the user has the possibility\nto choose the frame using a diﬀerent input modality\nsuch as a stylus. The error rates on the training data\nwere calculated by running the model with the same\nconstraint as on the test data — constraining the set\nof allowable tags at the frame level. This could be\nseen as an upper bound on the performance of the\nmodel (since the model parameters were estimated\non the same data).\nOur model signiﬁcantly outperforms the baseline\nmodel — a simple semantic context free grammar\nauthored manually for the MiPad task — in terms\nof slot error rate (about 35% relative reduction in\nslot error rate) but it is outperformed by the latter\nin terms of frame error rate. When running the mod-\nels from training step 2 on test data one cannot add\nany constraints; only frame level constraints can be\nused when evaluating the models from training step\n4 on test data. N-best reestimation at either train-\ning stage (2 or 4) doesn’t improve the accuracy of\nthe system, although the results obtained by intial-\nizing the model using the reestimated stage 2 model\n— iteration 2- {0,1,2} models tend to be slightly bet-\nter than their 0- {0,1,2} counterparts. Constraining\nthe frame level tag to have the correct value doesn’t\nsigniﬁcantly reduce the slot error rate in either ap-\nproach, as can be seen from the Test-L1 column of\nresults6.\n6.1 Out-of-domain Initial Statistics\nRecent results (Chelba, 2001) on the portability of\nsyntactic structure within the SLM framework show\nthat it is possible to initialize the SLM parameters\nfrom a treebank for out-of-domain text and main-\ntain the same language modeling performance. We\nhave repeated the experiment in the context of in-\nformation extraction.\nSimilar to the approach in (Miller et al., 2000) we\ninitialized the SLM statistics from the UPenn Tree-\nbank parse trees (about 1Mwds of training data) at\nthe ﬁrst training stage, see Section 3. The remain-\ning part of the training procedure was the same as\nin the previous set of experiments.\nThe word, slot and frame vocabulary were the\nsame as in the previous set of experiments. The\npre-terminal (POStag) vocabulary sizes were 40 and\n204 for training steps 2 and 4 (see Section 3), respec-\ntively; the non-terminal (NTtag) vocabulary sizes\nwere 52 and 434 for training steps 2 and 4 (see Sec-\ntion 3), respectively.\nThe results are presented in Table 3, showing im-\nproved performance over the model initialized from\nin-domain parse trees. The frame accuracy increases\nsubstantially, almost matching that of the baseline\nmodel, while the slot accuracy is just slightly in-\ncreased. We attribute the improved performance of\nthe model initialized from the UPenn Treebank to\nthe fact that the model explores a more diverse set\nof trees for a given sentence than the model initial-\nized from the MiPad automatic treebank generated\nusing the NLPwin parser.\n6.2 Impact of Training Data Size on\nPerformance\nWe have also evaluated the impact of the training\ndata size on the model performance. The results are\npresented in Table 4, showing a strong dependence\nof both the slot and frame error rates on the amount\nof training data used. This, together with the high\naccuracy of the model on training data (see Table 3),\nsuggests that we are far from saturation in perfor-\nmance and that more training data is very likely to\nimprove the model performance substantially.\n6.3 Error Trends\nAs a summary error analysis, we have investigated\nthe correlation between the semantic frame/slot er-\nror rate and the number of semantic slots in a sen-\ntence. We have binned the sentences in the test set\naccording to the number of slots in the manual an-\n6The frame error rate in this column should be 0; in prac-\ntice this doesn’t happen because some test sentences could\nnot be L-match parsed using the pruning strategy employed\nby the SLM, see Section 4.3\nTraining It Error Rate (%)\nTraining Test Test-L1\nStage 2 Stage 4 Slot Frame Slot Frame Slot Frame\nBaseline 43.41 7.20 57.36 14.90 57.30 6.90\n0 0 9.78 1.65 37.87 21.62 37.46 0.64\n0 1 10.36 1.20 39.16 21.80 38.28 0.64\n0 2 9.42 1.05 39.75 22.25 38.63 0.82\n2 0 8.92 1.25 38.04 22.07 37.81 0.91\n2 1 9.01 0.95 37.51 21.89 37.28 0.91\n2 2 9.47 0.90 38.99 21.89 38.57 0.82\nTable 2: Training and Test Data Slot/Frame Error Rates\nTraining It Error Rate (%)\nTraining Test Test-L1\nStage 2 Stage 4 Slot Frame Slot Frame Slot Frame\nBaseline 43.41 7.20 57.36 14.90 57.30 6.90\n0, MiPad/NLPwin 0 9.78 1.65 37.87 21.62 37.46 0.64\n1, UPenn Trbnk 0 8.44 2.10 36.93 16.08 36.34 0.91\n1, UPenn Trbnk 1 7.82 1.70 36.98 16.80 36.22 0.82\n1, UPenn Trbnk 2 7.69 1.50 36.98 16.80 36.22 1.00\nTable 3: Training and Test Data Slot/Frame Error Rates, UPenn Tre ebank initial statistics\nTraining Training It Error Rate (%)\nCorpus Training Test Test-L1\nSize Stage 2 Stage 4 Slot Frame Slot Frame Slot Frame\nBaseline 43.41 7.20 57.36 14.90 57.30 6.90\nall 1, UPenn Trbnk 0 8.44 2.10 36.93 16.08 36.34 0.91\n1/2 all 1, UPenn Trbnk 0 — — 43.76 18.44 43.40 0.45\n1/4 all 1, UPenn Trbnk 0 — — 49.47 22.98 49.53 1.82\nTable 4: Performance Degradation with Training Data Size\nnotation and evaluated the frame/slot error rate in\neach bin. The results are shown in Table 5.\nThe frame/slot accuracy increases with the num-\nber of slots per sentence — except for the 5+ bin\nwhere the frame error rate increases — showing that\nslot co-ocurence statistics improve performance; sen-\ntences containing more semantic slots tend to be less\nambiguous from an information extraction point of\nview.\nError Rate (%)\nNo. slots/sent Slot Frame No. Sent\n1 43.97 18.01 755\n2 39.23 16.27 209\n3 26.44 5.17 58\n4 26.50 4.00 50\n5+ 21.19 6.90 29\nTable 5: Frame/Slot Error Rate versus Slot Density\n7 Conclusions and Future Directions\nWe have presented a data-driven approach to infor-\nmation extraction that, despite the small amount\nof training data used, is shown to outperform the\nslot level accuracy of a simple semantic grammar\nauthored manually for the MiPad — personal infor-\nmation management — task.\nThe performance of the baseline model could be\nimproved with more authoring eﬀort, although this\nis expensive.\nThe big diﬀerence in performance between train-\ning and test and the fact that we are using so\nlittle training data, makes improvements by using\nmore training data very likely, although this may\nbe expensive. A framework which utilizes the vast\namounts of text data collected once such a system\nis deployed would be desirable. Statistical model-\ning techniques that make more eﬀective use of the\ntraining data should be used in the SLM, maximum\nentropy (Berger et al., 1996) being a good candidate.\nAs for using the SLM as the language understand-\ning component of a speech driven application, such\nas MiPad, it would be interesting to evaluate the im-\npact of incorporating the semantic constraints on the\nword-level accuracy of the system. Another possible\nresearch direction is to modify the framework such\nthat it ﬁnds the most likely semantic parse given\nthe acoustics — thus treating the word sequence as\na hidden variable.\nReferences\nA. L. Berger, S. A. Della Pietra, and V. J. Della\nPietra. 1996. A maximum entropy approach to\nnatural language processing. Computational Lin-\nguistics, 22(1):39–72, March.\nCiprian Chelba and Frederick Jelinek. 2000. Struc-\ntured language modeling. Computer Speech and\nLanguage, 14(4):283–332, October.\nCiprian Chelba. 2001. Portability of syntactic struc-\nture for language modeling. In Proceedings of\nICASSP, page to appear. Salt Lake City, Utah.\nA. P. Dempster, N. M. Laird, and D. B. Rubin. 1977.\nMaximum likelihood from incomplete data via the\nEM algorithm. In Journal of the Royal Statistical\nSociety, volume 39 of B, pages 1–38.\nGeorge Heidorn. 1999. Intelligent writing assis-\ntance. In R. Dale, H. Moisl, and H. Somers, ed-\nitors, Handbook of Natural Language Processing .\nMarcel Dekker, New York.\nX. Huang, A. Acero, C. Chelba, L. Deng, D. Duch-\nene, J. Goodman, H. Hon, D. Jacoby, L. Jiang,\nR. Loynd, M. Mahajan, P. Mau, S. Meredith,\nS. Mughal, S. Neto, M. Plumpe, K. Wang, and\nY. Wang. 2000. MiPad: A next generation PDA\nprototype. In ICSLP’00, Proceedings, Beijing,\nChina.\nDaniel Jurafsky and James H. Martin, 2000. An In-\ntroduction to Natural Language Processing, Com-\nputational Linguistics, and Speech Recognition ,\npages 577–583. Prentice Hall.\nM. Marcus, B. Santorini, and M. Marcinkiewicz.\n1993. Building a large annotated corpus of En-\nglish: the Penn Treebank. Computational Lin-\nguistics, 19(2):313–330.\nScott Miller, Heidi Fox, Lance Ramshaw, and Ralph\nWeischedel. 2000. A novel use of statistical pars-\ning to extract information from text. In Proceed-\nings of ANLP-NAACL , pages 226–233. Seattle,\nWashington.\nFernando Pereira and Yves Schabes. 1992. Inside-\noutside reestimation from partially bracketed cor-\npora. ACL, 30:128–135.\nY.-Y. Wang. 1999. A robust parser for spoken lan-\nguage understanding. In Eurospeech’99 Proceed-\nings, Budapest, Hungary."
}