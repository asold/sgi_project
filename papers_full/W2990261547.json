{
  "title": "Red Dragon AI at TextGraphs 2019 Shared Task: Language Model Assisted Explanation Generation",
  "url": "https://openalex.org/W2990261547",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A5087486426",
      "name": "Yew Ken Chia",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5077074805",
      "name": "Sam Witteveen",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5102739303",
      "name": "Martin Andrews",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963318894",
    "https://openalex.org/W4298110152",
    "https://openalex.org/W2786395785",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2794325560",
    "https://openalex.org/W3119370638",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963339923",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2790767126",
    "https://openalex.org/W2968447920",
    "https://openalex.org/W2983719617"
  ],
  "abstract": "The TextGraphs-13 Shared Task on Explanation Regeneration asked participants\\nto develop methods to reconstruct gold explanations for elementary science\\nquestions. Red Dragon AI's entries used the language of the questions and\\nexplanation text directly, rather than a constructing a separate graph-like\\nrepresentation. Our leaderboard submission placed us 3rd in the competition,\\nbut we present here three methods of increasing sophistication, each of which\\nscored successively higher on the test set after the competition close.\\n",
  "full_text": "Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13), pages 85–89\nHong Kong, November 4, 2019.c⃝2019 Association for Computational Linguistics\n85\nRed Dragon AI at TextGraphs 2019 Shared Task:\nLanguage Model Assisted Explanation Generation\nYew Ken Chia\nRed Dragon AI\nSingapore\nken@reddragon.ai\nSam Witteveen\nRed Dragon AI\nSingapore\nsam@reddragon.ai\nMartin Andrews\nRed Dragon AI\nSingapore\nmartin@reddragon.ai\nAbstract\nThe TextGraphs-13 Shared Task on Explana-\ntion Regeneration (Jansen and Ustalov, 2019)\nasked participants to develop methods to re-\nconstruct gold explanations for elementary sci-\nence questions. Red Dragon AI’s entries used\nthe language of the questions and explanation\ntext directly, rather than a constructing a sep-\narate graph-like representation. Our leader-\nboard submission placed us 3rd in the compe-\ntition, but we present here three methods of in-\ncreasing sophistication, each of which scored\nsuccessively higher on the test set after the\ncompetition close.\n1 Introduction\nThe Explanation Regeneration shared task asked\nparticipants to develop methods to reconstruct\ngold explanations for elementary science ques-\ntions (Clark et al., 2018), using a new corpus\nof gold explanations (Jansen et al., 2018) that\nprovides supervision and instrumentation for this\nmulti-hop inference task.\nEach explanation is represented as an “explana-\ntion graph”, a set of atomic facts (between 1 and\n16 per explanation, drawn from a knowledge base\nof 5,000 facts) that, together, form a detailed ex-\nplanation for the reasoning required to answer and\nexplain the resoning behind a question.\nLinking these facts to achieve strong perfor-\nmance at rebuilding the gold explanation graphs\nrequires methods to perform multi-hop inference -\nwhich has been shown to be far harder than infer-\nence of smaller numbers of hops (Jansen, 2018),\nparticularly for the case here, where there is con-\nsiderable uncertainty (at a lexical level) of how\nindividual explanations logically link somewhat\n‘fuzzy’ graph nodes.\nData Python Scala Python Leaderboard\nsplit Baseline Baseline Baseline 1e9 Submission\nTrain 0.0810 0.2214 0.4216\nDev 0.0544 0.2890 0.2140 0.4358\nTest 0.4017\nTable 1: Base MAP scoring - where the Python\nBaseline1e9 is the same as the original Python Baseline,\nbut with the evaluate.py code updated to assume\nmissing explanations have rank of 109\n1.1 Dataset Review\nThe WorldTree corpus (Jansen et al., 2018) is a\nnew dataset is a comprehensive collection of ele-\nmentary science exam questions and explanations.\nEach explanation sentence is a fact that is related\nto science or common sense, and is represented\nin a structured table that can be converted to free-\ntext. For each question, the gold explanations have\nlexical overlap (i.e. having common words), and\nare denoted as having a speciﬁc explanation role\nsuch as CENTRAL (core concepts); GROUNDING\n(linking core facts to the question); andLEXICAL\nGLUE (linking facts which may not have lexical\noverlap).\n1.2 Problem Review\nAs described in the introduction, the general task\nbeing posed is one of multi-hop inference, where\na number of ‘atomic fact’ sentences must be com-\nbined to form a coherent chain of reasoning to\nsolve the elementary science problem being posed.\nThese explanatory facts must be retrieved from\na semi-structured knowledge base - in which the\nsurface form of the explanation is represented as a\nseries of terms gathered by their functional role in\nthe explanation.\nFor instance, for the explanation “Grass snakes\nlive in grass” is encoded as “[Grass snakes] [live\nin] [grass]”, and this explanation is found in a\n86\nPROTO-HABITATS table. However, in the same\ntable there are also more elaborate explanations,\nfor example : “Mice live in in holes in the ground\nin ﬁelds / in forests.” is expressed as : “[mice]\n[live in] [in holes in the ground] [in ﬁelds OR in\nforests]”. And more logically complex : “Most\npredators live in/near the same environment as\ntheir prey.” being expressed as : “[most] [preda-\ntors] [live in OR live near] [the same environment\nas their prey]”.\nSo, whereas the simpler explanations ﬁt in the\nusual Knowledge-Base triples paradigm, the more\ncomplex ones are much more nuanced about what\nactually constitutes a node, and how reliable the\narcs are between them. Indeed, there is also a col-\nlection of if/then explanations, including ex-\namples such as : “[if] [something] [has a] [posi-\ntive impact on] [something else] [then] [increas-\ning] [the] [amount of] [that something] [has a]\n[positive impact on] [that something else]” - where\nthe explanation has meta-effect on the graph itself,\nand includes ‘unbound variables’.1\n2 Preliminary Steps\nIn this work, we used the pure textual form of each\nexplanation, problem and correct answer, rather\nthan using the semi-structured form given in the\ncolumn-oriented ﬁles provided in the dataset. For\neach of these we performed Penn-Treebank to-\nkenisation, followed by lemmatisation using the\nlemmatisation ﬁles provided with the dataset, and\nthen stop-word removal.2\nConcerned by the low performance of the\nPython Baseline method (compared to the Scala\nBaseline, which seemed to operate using an al-\ngorithm of similar ‘strength’), we identiﬁed an\nissue in the organizer’s evaluation script where\npredicted explanations that were missing any of\nthe gold explanations were assigned a MAP score\nof zero. This dramatically penalised the Python\nBaseline, since it was restricted to only returning\n10 lines of explanation. It also effectively forces\nall submissions to include a ranking over all ex-\nplanations - a simple ﬁx (with the Python Baseline\nrescored in Table 1) will be submitted via GitHub.\nThis should also make the upload/scoring process\nfaster, since only the top ∼1000 explanation lines\nmeaningfully contribute to the rank scoring.\n1The PROTO-IF-THEN explanation table should have\nbeen annotated with a big red warning sign\n2PTB tokenisation and stopwords from the NLTK pack-\nage)\n3 Model Architectures\nAlthough more classic graph methods were ini-\ntially attempted, along the lines of Kwon et al.\n(2018), where the challenge of semantic drift in\nmulti-hop inference was analysed and the effec-\ntiveness of information extraction methods was\ndemonstrated, the following 3 methods (which\nnow easily surpass the score of our competition\nsubmission) were ultimately pursued due to their\nsimplicity/effectiveness.\nData Optimised Iterated BERT\nsplit TF-IDF TF-IDF Re-ranking\nTrain 0.4525 0.4827 0.6677\nDev 0.4581 0.4966 0.5089\nTest 0.4274 0.4576 0.4771\nTime 0.02 46.97 92.96\nTable 2: MAP scoring of new methods. The timings\nare in seconds for the whole dev-set, and the BERT\nRe-ranking ﬁgure includes the initial Iterated TF-IDF\nstep.\n3.1 Optimized TF-IDF\nAs mentioned above, the original TF-IDF imple-\nmentation of the provided Python baseline script\ndid not predict a full ranking, and was penalized\nby the evaluation script. When this issue was\nremedied, its MAP score rose to 0.2140.\nHowever, there are three main steps that signif-\nicantly improve the performance of this baseline:\n1. The original question text included all the an-\nswer choices, only one of which was correct\n(while the others are distractors). Removing\nthe distractors resulted in improvement;\n2. The TF-IDF algorithm is very sensitive to\nkeywords. Using the provided lemmatisation\nset and NLTK for tokenisation helped to align\nthe different forms of the same keyword and\nreduce the vocabulary size needed;\n3. Stopword removal gave us approximately\n0.04 MAP improvement throughout - remov-\ning noise in the texts that was evidently ‘dis-\ntracting’ for TF-IDF.\nAs shown in Table 2, these optimisation steps\nincreased the Python Baseline score signiﬁcantly,\nwithout introducing algorithmic complexity.\n87\n3.2 Iterated TF-IDF\nWhile graph methods have shown to be effective\nfor multi-hop question answering, the schema in\nthe textgraphs dataset is unconventional (as illus-\ntrated earlier). To counter this, the previous TF-\nIDF method was extended to simulate jumps be-\ntween explanations, inspired by graph methods,\nbut without forming any actual graphs:\n1. TF-IDF vectors are pre-computed for all\nquestions and explanation candidates;\n2. For each question, the closest explanation\ncandidate by cosine proximity is selected,\nand their TF-IDF vectors are aggregated by\na max operation;\n3. The next closest (unused) explanation is se-\nlected, and this process was then applied it-\neratively up to maxlen=128 times3, with\nthe current TF-IDF comparison vector pro-\ngressively increasing in expressiveness. At\neach iteration, the current TF-IDF vector was\ndown-scaled by an exponential factor of the\nlength of the current explanation set, as this\nwas found to increase development set results\nby up to +0.0344.\nBy treating the TF-IDF vector as a representa-\ntion of the current chain of reasoning, each succes-\nsive iteration builds on the representation to accu-\nmulate a sequence of explanations.\nThe algorithm outlined above was additionally\nenhanced by adding a weighting factor to each\nsuccessive explanation as it is added to the cumu-\nlative TF-IDF vector. Without this factor, the ef-\nfectiveness was lower because the TF-IDF repre-\nsentation itself was prone to semantic drift away\nfrom the original question. Hence, each succes-\nsive explanation’s weight was down-scaled, and\nthis was shown to work well.4\n3.3 BERT Re-ranking\nLarge pretrained language models have been\nproven effective on a wide range of downstream\ntasks, including multi-hop question answering,\nsuch as in Liu et al. (2019) on the RACE dataset,\n3 This maxlen value was chosen to minimise computa-\ntion time, noting that explanation ranks below approximately\n100 have negligible impact on the ﬁnal score.\n4Full, replicable code is available on GitHub for all\n3 methods described here, at https://github.com/\nmdda/worldtree_corpus/tree/textgraphs\nand Xu et al. (2019) which showed that large ﬁne-\ntuned language models can be beneﬁcial for com-\nplex question answering domains (especially in a\ndata-constrained context).\nInspired by this, we decided to adapt BERT\n(Devlin et al., 2018) - a popular language model\nthat has produced competitive results on a variety\nof NLP tasks - for the explanation generation task.\nFor our ‘BERT Re-ranking’ method, we attach a\nregression head to a BERT Language Model. This\nregression head is then trained to predict a rele-\nvance score for each pair of question and explana-\ntion candidate. The approach is as follows :\n1. Calculate a TF-IDF relevance score for every\ntokenised explanation against the tokenised\n‘[Problem] [CorrectAnswer] [Gold explana-\ntions]’ in the training set. This will rate the\ntrue explanation sentences very highly, but\nalso provide a ‘soft tail’ of rankings across\nall explanations;\n2. Use this relevance score as the prediction\ntarget of the BERT regression head, where\nBERT makes its predictions from the original\n‘[Problem] [CorrectAnswer]’ text combined\nwith each potential Explanation text in turn\n(over the training set);\n3. At prediction time, the explanations are\nranked according to their relevance to ‘[Prob-\nlem] [CorrectAnswer]’ as predicted by the\nBERT model’s output.\nWe cast the problem as a regression task (rather\nthan a classiﬁcation task), since treating it as a task\nto classify which explanations are relevant would\nresult in an imbalanced dataset because the gold\nexplanation sentences only comprise a small pro-\nportion of the total set. By using soft targets (given\nto us by the TF-IDF score against the gold answers\nin the training set), even explanations which are\nnot designated as “gold” but have some relevance\nto the gold paragraph can provide learning signal\nfor the model.\nDue to constraints in compute and time, the\nmodel is only used to rerank the topn = 64 pre-\ndictions made by the TF-IDF methods.\nThe BERT model selected was of “Base” size\nwith 110M parameters, which had been pretrained\non BooksCorpus and English Wikipedia. We\ndid not further ﬁnetune it on texts similar to the\nTextGraphs dataset prior to regression training. In\n88\n1 2 3 4 5 6 7 8 9 10+\nGold explanation lengths\n0.0\n0.2\n0.4\n0.6\n0.8\nMean MAP score\nMean MAP score against Gold explanation lengths\nOptimizedTFIDF\nIterativeTFIDF\nIterativeTFIDF + BERT\nFigure 1: Mean MAP score against gold explanation\nlengths\nother tests, we found that the “Large” size model\ndid not help improve the ﬁnal MAP score.\n4 Discussion\nThe authors’ initial attempts at tackling the Shared\nTask focussed on graph-based methods. However,\nas identiﬁed in (Jansen, 2018), the uncertainty in-\nvolved with interpreting each lexical representa-\ntion, combined with the number of hops required,\nmeant that this line of enquiry was put to one side5.\nWhile the graph-like approach is clearly attrac-\ntive from a reasoning point of view (and will be the\nfocus of future work), we found that using purely\nthe textual aspects of the explanation database\nbore fruit more readily. Also. the complexity of\nthe resulting systems could be minimised such that\nthe description of each system could be as consise\nas possible.\nSpeciﬁcally, we were able to optimise the TF-\nIDF baseline to such an extent that our ‘Opti-\nmised TF-IDF’ would now place 2nd in the sub-\nmission rankings, even though it used no special\ntechniques at all.6\nThe Iterated TF-IDF method, while more algo-\nrithmically complex, also does not need any train-\ning on the data before it is used. This shows how\neffective traditional text processing methods can\nbe, when used strategically.\nThe BERT Re-ranking method, in contrast, does\nrequire training, and also applies one of the more\nsophisticated Language Models available to ex-\ntract more meaning from the explanation texts.\nFigure 1 illustrates how there is a clear trend to-\n5Having only achieved 0.3946 on the test set\n6Indeed, our Optimized TF-IDF, scoring 0.4581 on the\ndev set, and 0.4274 on the test set, could be considered a new\nbaseline for this corpus, given its simplicity.\nExplanation Optimised Iterated BERT\nrole TF-IDF TF-IDF Re-ranking\nGROUNDING 0.1373 0.1401 0.0880\nLEX-GLUE 0.0655 0.0733 0.0830\nCENTRAL 0.4597 0.5033 0.5579\nBACKGROUND 0.0302 0.0285 0.0349\nNEG 0.0026 0.0025 0.0022\nROLE 0.0401 0.0391 0.0439\nTable 3: Contribution of Explanation Roles - Dev-Set\nMAP per role (computed by ﬁltering explanations of\nother roles out of the gold explanation list then com-\nputing the MAP as per normal)\nwards being able to build longer explanations as\nour semantic relevance methods become more so-\nphisticated.\nThere are also clear trends across the data in Ta-\nble 3 that show that the more sophisticated meth-\nods are able to bring moreCENTRAL explanations\ninto the mix, even though they are more ‘textually\ndistant’ from the original Question and Answer\nstatements. Surprisingly, this is at the expense of\nsome of the GROUNDING statements.\nSince these methods seem to focus on different\naspects of solving the ranking problem, we have\nalso explored averaging the ranks they assign to\nthe explanations (essentially ensembling their de-\ncisions). Empirically, this improves performance7\nat the expense of making the model more obscure.\n4.1 Further Work\nDespite our apparent success with less sophis-\nticated methods, it seems clear that more ex-\nplicit graph-based methods appears will be re-\nquired to tackle the tougher questions in this\ndataset (for instance those that require logical de-\nductions, as illustrated earlier, or hypothetical sit-\nuations such as some ‘predictor-prey equilibrium’\nproblems). Even some simple statements (such as\n‘Most predators ...’) present obstacles to existing\nKnowledge-Base representations.\nIn terms of concrete next steps, we are ex-\nploring the idea of creating intermediate forms of\nrepresentation, where textual explanations can be\nlinked using a graph to plan out the logical steps.\nHowever these grander schemes suffer from being\n7The combination of ‘Iterated TF-IDF’ and ‘BERT Re-\nranking’ scoring 0.5195 on the dev set, up from their scores\nof 0.4966 and 0.5089 respectively\n89\nincrementally less effective than ﬁnding additional\n‘smart tricks’ for existing methods!\nIn preparation, we have begun to explore doing\nmore careful preprocessing, notably :\n1. Exploiting the structure of the explanation\ntables individually, since some columns are\nknown to be relationship-types that would be\nsuitable for labelling arcs between nodes in a\ntypical Knowledge Graph setting;\n2. Expanding out the conjunction elements\nwithin the explanation tables. For instance\nin explanations like “[coral] [lives in the]\n[ocean OR warm water]”, the different sub-\nexplanations “(Coral, LIVES-IN, Ocean)”\nand “(Coral, LIVES-IN, WarmWater)” can\nbe generated, which are far closer to a ‘graph-\nable’ representation;\n3. Better lemmatisation : For instance ‘ice cube’\ncovers both ‘ice’ and ‘ice cube’ nodes. We\nneed some more ‘common sense’ to cover\nthese cases.\nClearly, it is early days for this kind of multi-\nhop inference over textual explanations. At this\npoint, we have only scratched the surface of the\nproblem, and look forward to helping to advance\nthe state-of-the-art in the future.\nAcknowledgments\nThe authors would like to thank Google for ac-\ncess to the TFRC TPU program which was used\nin training and ﬁne-tuning models during experi-\nmentation for this paper.\nReferences\nPeter F. Clark, Isaac Cowhey, Oren Etzioni, Tushar\nKhot, Ashish Sabharwal, Carissa Schoenick, and\nOyvind Tafjord. 2018. Think you have solved ques-\ntion answering? Try ARC, the AI2 Reasoning Chal-\nlenge. ArXiv, arXiv:1803.05457.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training\nof deep bidirectional transformers for language\nunderstanding. Computing Research Repository,\narXiv:1810.04805.\nPeter Jansen. 2018. Multi-hop inference for sentence-\nlevel textgraphs: How challenging is meaningfully\ncombining information for science question answer-\ning? In Proceedings of the Twelfth Workshop on\nGraph-Based Methods for Natural Language Pro-\ncessing (TextGraphs-12), pages 12–17.\nPeter Jansen and Dmitry Ustalov. 2019. TextGraphs\n2019 Shared Task on Multi-Hop Inference for Ex-\nplanation Regeneration. In Proceedings of the Thir-\nteenth Workshop on Graph-Based Methods for Nat-\nural Language Processing (TextGraphs-13), Hong\nKong. Association for Computational Linguistics.\nPeter Jansen, Elizabeth Wainwright, Steven Mar-\nmorstein, and Clayton Morrison. 2018. WorldTree:\nA Corpus of Explanation Graphs for Elementary\nScience Questions supporting Multi-hop Inference.\nIn Proceedings of the Eleventh International Confer-\nence on Language Resources and Evaluation (LREC\n2018), Miyazaki, Japan. European Language Re-\nsources Association (ELRA).\nHeeyoung Kwon, Harsh Trivedi, Peter Jansen, Mi-\nhai Surdeanu, and Niranjan Balasubramanian. 2018.\nControlling information aggregation for complex\nquestion answering. In European Conference on In-\nformation Retrieval, pages 750–757. Springer.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nDongfang Xu, Peter Jansen, Jaycie Martin, Zheng-\nnan Xie, Vikas Yadav, Harish Tayyar Madabushi,\nOyvind Tafjord, and Peter Clark. 2019. Multi-\nclass hierarchical question classiﬁcation for mul-\ntiple choice science exams. arXiv preprint\narXiv:1908.05441.",
  "topic": "Sophistication",
  "concepts": [
    {
      "name": "Sophistication",
      "score": 0.7854217290878296
    },
    {
      "name": "Computer science",
      "score": 0.6676055788993835
    },
    {
      "name": "Task (project management)",
      "score": 0.607598602771759
    },
    {
      "name": "Competition (biology)",
      "score": 0.525807797908783
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5126765966415405
    },
    {
      "name": "Natural language processing",
      "score": 0.5078416466712952
    },
    {
      "name": "Graph",
      "score": 0.4894358217716217
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.47052064538002014
    },
    {
      "name": "Representation (politics)",
      "score": 0.43838974833488464
    },
    {
      "name": "Language model",
      "score": 0.4368087947368622
    },
    {
      "name": "Theoretical computer science",
      "score": 0.2239713966846466
    },
    {
      "name": "Programming language",
      "score": 0.13253426551818848
    },
    {
      "name": "Art",
      "score": 0.10413223505020142
    },
    {
      "name": "Engineering",
      "score": 0.06794103980064392
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Aesthetics",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ]
}