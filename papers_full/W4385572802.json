{
  "title": "Revisiting Pre-trained Language Models and their Evaluation for Arabic Natural Language Processing",
  "url": "https://openalex.org/W4385572802",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2135504139",
      "name": "Abbas Ghaddar",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2226411450",
      "name": "Yimeng Wu",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2612506034",
      "name": "Sunyam Bagga",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2109353700",
      "name": "Ahmad, Rashid",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A3156606986",
      "name": "Khalil Bibi",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2258670661",
      "name": "Mehdi Rezagholizadeh",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2128161365",
      "name": "Chao Xing",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2111728759",
      "name": "Ya-sheng Wang",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2104466519",
      "name": "Xinyu Duan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129700595",
      "name": "Zhefeng Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3035468489",
      "name": "Baoxing Huai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097242334",
      "name": "Xin Jiang",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2109590494",
      "name": "Qun Liu",
      "affiliations": [
        "Huawei Technologies (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A2666580798",
      "name": "Phillippe Langlais",
      "affiliations": [
        "University of Milan"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1485311222",
    "https://openalex.org/W3106445907",
    "https://openalex.org/W4225591000",
    "https://openalex.org/W3106433641",
    "https://openalex.org/W4382246105",
    "https://openalex.org/W2951583236",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2155069789",
    "https://openalex.org/W3114950584",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3171654528",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W340195604",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2890225082",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W2970814728",
    "https://openalex.org/W4287370853",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4206256378",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W3176169354",
    "https://openalex.org/W3134155512",
    "https://openalex.org/W3086966320",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2963625095",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W4287757832",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4301239768",
    "https://openalex.org/W3113459090",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W3154100567",
    "https://openalex.org/W3135175488",
    "https://openalex.org/W4200319795",
    "https://openalex.org/W2971871542",
    "https://openalex.org/W4287025617",
    "https://openalex.org/W3115462295",
    "https://openalex.org/W4287628388",
    "https://openalex.org/W4301368518",
    "https://openalex.org/W3182414949",
    "https://openalex.org/W4385573073",
    "https://openalex.org/W3172021172",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3105424285",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W4385681388",
    "https://openalex.org/W3040573126",
    "https://openalex.org/W3118440692",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W4297663785",
    "https://openalex.org/W3041687854",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3104196571",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W3116641301",
    "https://openalex.org/W3191780119",
    "https://openalex.org/W4200634402",
    "https://openalex.org/W3182414670",
    "https://openalex.org/W4287042819",
    "https://openalex.org/W3158631574",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3095771422",
    "https://openalex.org/W3091355780",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W3127512039",
    "https://openalex.org/W3035503910"
  ],
  "abstract": "Abbas Ghaddar, Yimeng Wu, Sunyam Bagga, Ahmad Rashid, Khalil Bibi, Mehdi Rezagholizadeh, Chao Xing, Yasheng Wang, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Xin Jiang, Qun Liu, Phillippe Langlais. Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3135–3151\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nRevisiting Pre-trained Language Models and their Evaluation for Arabic\nNatural Language Processing\nAbbas Ghaddar1,∗ Yimeng Wu1,∗ Sunyam Bagga1 Ahmad Rashid1 Khalil Bibi1\nMehdi Rezagholizadeh1 Chao Xing1 Yasheng Wang1 Duan Xinyu2\nZhefeng Wang2 Baoxing Huai2 Xin Jiang1 Qun Liu1 and Philippe Langlais3\n1 Huawei Technologies Co., Ltd.\n2 Huawei Cloud Computing Technologies Co., Ltd\n3 RALI/DIRO, Université de Montréal, Canada\n{abbas.ghaddar,yimeng.wu,sunyam.bagga}@huawei.com\n{ahmad.rashid,khalil.bibi,mehdi.rezagholizadeh}@huawei.com\n{xingchao.ml,wangyasheng,duanxinyu}@huawei.com\n{wangzhefeng,huaibaoxing,jiang.xin,qun.liu}@huawei.com\nfelipe@iro.umontreal.ca\nAbstract\nThere is a growing body of work in recent years\nto develop pre-trained language models (PLMs)\nfor the Arabic language. This work addresses\ntwo major problems in existing Arabic PLMs\nthat limit the progress of the Arabic NLU and\nNLG fields. First, existing Arabic PLMs are\nnot well-explored and their pre-training can be\nimproved significantly using a more methodical\napproach. Second, there is a lack of systematic\nand reproducible evaluation of these models in\nthe literature. We revisit both the pre-training\nand evaluation of Arabic PLMs. In terms of\npre-training, we explore the impact of the qual-\nity of the pretraining data, the size of the model\nand the incorporation of character-level infor-\nmation to Arabic PLMs. As a result, we release\nthree new Arabic BERT-style models (JABER,\nChar-JABER, and SABER), and two T5-style\nmodels (AT5S and AT5B). In terms of eval-\nuation, we conduct a comprehensive empiri-\ncal study to systematically evaluate the perfor-\nmance of existing state-of-the-art models on\nALUE, a leaderboard-powered benchmark for\nArabic NLU tasks, and on a subset of Arabic\ngenerative tasks. We show that our models\nsignificantly outperform existing Arabic PLMs\nand achieve a new state-of-the-art performance\non both discriminative and generative tasks.\n1 Introduction\nPre-trained language models (PLMs) such as\nBERT (Devlin et al., 2018), GPT (Radford et al.,\n2018), and T5 (Raffel et al., 2019) have become\nthe default standard architectures for modern natu-\nral language understanding (NLU) systems in both\nacademic (Kalyan et al., 2021; Min et al., 2021)\nand industrial settings (Chakravarti et al., 2020;\n∗ Equal contribution\nTunstall et al., 2022; Li et al., 2021). On the evalu-\nation side, the community has widely adopted the\nleaderboard paradigm1 as a reliable and fair tool\nto track the progress on various NLP tasks (Mehri\net al., 2020; Wang et al., 2018, 2019).\nRecent years have seen tremendous efforts to\ndevelop language-specific PLMs (Le et al., 2020;\nChan et al., 2020; Canete et al., 2020; Ul ˇcar and\nRobnik-Šikonja, 2020) and leaderboards (Xu et al.,\n2020, 2021; Shavrina et al., 2020; Wilie et al.,\n2020) for languages other than English. These\nlanguage-specific PLMs have proven to be more\naccurate than multilingual ones in monolingual\nevaluation settings (Martin et al., 2019; Wei et al.,\n2019; Safaya et al., 2020). Moreover, creating\nhigh-quality human-curated benchmarks is consid-\nered to be of utmost importance for reliable evalua-\ntion (DeYoung et al., 2020; Kiela et al., 2021).\nFor some high-resource languages like Chinese,\nthe community has been able to be on par with En-\nglish NLU in terms of developing PLMs (Sun et al.,\n2019, 2020, 2021; Zeng et al., 2021) and evaluat-\ning them on publicly available leaderboards (Xu\net al., 2020). However, we find that the NLP com-\nmunity is unfortunately lagging behind for other\nlanguages like Arabic. Despite the wide availabil-\nity of Arabic PLMs (Abdul-Mageed et al., 2021;\nAntoun et al., 2020; Nagoudi et al., 2022; Inoue\net al., 2021) and datasets (Zeroual et al., 2019; El-\nKhair, 2016; Nagoudi et al., 2020), there are two\nmajor issues that constrain the progress of Arabic\nNLU field.\nFirst, we observe that the latest techniques for\nimproving pre-training (Brown et al., 2020; Clark\net al., 2022; Di Liello et al., 2021) are under-\n1We use the same definition of leaderboard as Ethayarajh\nand Jurafsky (2020).\n3135\nexplored in the context of Arabic PLMs. In this\nwork, we investigate three ways to improve on the\nexisting Arabic PLMs: quality of the pre-training\ndata, size of the model, and morphology. We pro-\npose JABER, a BERT-base model pre-trained on\nhigh-quality filtered data, that significantly outper-\nforms the best Arabic PLM baseline by 1.5% on\nALUE (Seelawi et al., 2021), a newly proposed\nbenchmark with a leaderboard for sequence clas-\nsification Arabic tasks. 2 We also explore two\nother variants of JABER and report further gains\nin performance: (i) Char-JABER which exploits\ncharacter-level information and (ii) SABER which\ninvolves a BERT-large model.\nSecond, there is a lack of systematic and repro-\nducible evaluation. As a matter of fact, most of the\nexisting work on Arabic PLMs does not follow the\nrecommended evaluation protocols (Pineau, 2020;\nChen et al., 2022) which include extensive hyper-\nparameter tuning, performing multiple runs on the\ndevelopment set, and reporting performance on\nhidden test sets. To address this issue, we system-\natically compare five popular BERT-based Arabic\nPLMs by carefully assessing their performance on\nthe ALUE leaderboard. We find that the perfor-\nmance ranking of models drastically changes when\nmeasured on dev sets as compared to the leader-\nboard test sets, thereby calling for caution when\ncomparing models without a leaderboard setting.\nFurthermore, we extend our work to T5 encoder-\ndecoder models and Arabic generative tasks. We\npre-train two T5 small and base models for Arabic:\nAT5S and AT5B. AT5B achieves state-of-the-art re-\nsults on several generative tasks (Naous et al., 2020;\nLadhak et al., 2020) by outperforming the recently\nproposed AraT5-base model (Nagoudi et al., 2022)\nboth on automatic and human evaluations. We fur-\nther observe that T5-based Arabic PLMs perform\nworse than the BERT-based models on the ALUE\nbenchmark which is in contrast to the powerful per-\nformance of T5-models on English language tasks\n(Raffel et al., 2019). We conclude with a set of\nsuggestions and directions to explore for pushing\nprogress forward in the Arabic NLU community.\n2 Related Work\nThere have been several efforts to improve on the\npre-training paradigm by scaling up the model\nsize (Lepikhin et al., 2021; Brown et al., 2020)\nand data size (Liu et al., 2019), exploring new\n2The Arabic equivalent of GLUE (Wang et al., 2018).\npre-training tasks (Di Liello et al., 2021; Panda\net al., 2021) and model architectures (Lan et al.,\n2019; V oita et al., 2019), and support for long in-\nput sequences (Choromanski et al., 2021; Beltagy\net al., 2020). In this work, we use the original set-\nting of BERT (Devlin et al., 2018) and T5 (Raffel\net al., 2019) models to pre-train our Arabic encoder-\nonly and encoder-decoder models respectively. The\nbroader goal is to be fairly and directly comparable\nwith other existing Arabic PLMs discussed below.\nTable 1 shows the configuration used by popular\npublicly available Arabic BERT models as well\nas those of JABER and SABER. AraBERT (An-\ntoun et al., 2020) and Arabic-BERT (Safaya et al.,\n2020) were amongst the first to pre-train 12-layer\nBERT-base models specifically for Arabic. Abdul-\nMageed et al. (2021) proposed two BERT-based\nmodels: ARBERT which is tailored for Modern\nStandard Arabic (MSA) NLU tasks and MAR-\nBERT dedicated to tasks that include Arabic di-\nalects (especially tweets). ARBERT and MAR-\nBERT are pre-trained on 61GB and 128GB of MSA\nand tweets data respectively. Inoue et al. (2021) go\none step further and pre-train a single BERT-base\nmodel called CAMeLBERT, on 167GB of MSA, di-\nalect and classic Arabic data. The major difference\nbetween JABER and these existing Arabic PLMs\nis that JABER is pre-trained on a high-quality and\nstrictly filtered dataset (115GB out of 514GB).\nA wide range of methods have been proposed\nlately to enrich PLMs with character-level infor-\nmation, as it has been shown to be beneficial for\nmorphologically rich languages like Arabic (Kim\net al., 2016; Gerz et al., 2018; Clark et al., 2022).\nMa et al. (2020) proposed Noisy Language Mod-\neling, a new unsupervised pre-training objective\nfor learning character representations. Pinter et al.\n(2021) proposed their XRayEmb method that in-\nvolves adding character-level information to exist-\ning PLMs without the need for pretraining them\nfrom scratch. CharacterBERT (El Boukkouri et al.,\n2020) uses a character-CNN module to learn repre-\nsentations for entire words by consulting the char-\nacters of each token, thus avoiding to recourse\nto word-pieces (Wu et al., 2016). Our character-\nenhanced BERT-base model, Char-JABER, uses a\nsimple and efficient method to inject character-level\nrepresentations alongside the sub-tokens represen-\ntations only at the input layer of BERT, with min-\nimal additional parameters and no computational\noverhead.\n3136\nModel Arabic-BERT AraBERT CAMeLBERT ARBERT MARBERTJABER SABER\n#Params (w/o emb)110M (85M) 135M (85M) 108M (85M) 163M (85M) 163M (85M)135M (85M) 369M (307M)\nVocab Size 32k 64k 30k 100k 100k 64k 64k\nTokenizer WordPiece WordPiece WordPiece WordPiece WordPiece BBPE BBPE\nNormalization ✘ ✓ ✓ ✘ ✘ ✓ ✓\nData Filtering ✘ ✘ ✘ ✘ ✘ ✓ ✓\nTextual Data Size95GB 27GB 167GB 61GB 128GB 115GB 115GB\nDuplication Factor 3 10 10 - - 3 3\nTraining epochs 27 27 2 42 36 15 5\nTable 1: Configuration of publicly available Arabic BERT models and our JABER and SABER models. AraBERT\nand MARBERT did not provide their data duplication factor. Char-JABER has the same characteristics as JABER.\nRecent efforts have also been made to develop\nbenchmarks for Arabic NLU tasks. Abdul-Mageed\net al. (2021) proposed the ARLUE benchmark\nwhich is a collection of 42 discriminative classi-\nfication tasks. Nagoudi et al. (2022) proposed the\nARGEN benchmark which consists of 19 datasets\nfor generative tasks. However, both benchmarks\nhave certain limitations which make it challeng-\ning to meaningfully evaluate Arabic PLMs. For\nmany tasks, the authors use their own train-dev-\ntest splits which are not made publicly available,\nas of May 10, 2022. In addition, the access to\nsome datasets is not available free of cost. Further-\nmore, none of the tasks include privately-held test\ndata which is important to ensure that a benchmark\nis used fairly (Wang et al., 2018). Therefore, we\nadopt the ALUE benchmark (Seelawi et al., 2021)\nfor evaluating our models on classification tasks\nbecause this benchmark has a public leaderboard\nand includes privately-held test sets for many tasks.\nFor evaluating our Arabic T5 models, we select a\nsubset of generative tasks from the ARGEN bench-\nmark whose results are freely reproducible (see\nSection 4.1).\n3 Pre-training\n3.1 Data Collection and Processing\nWe collect our pre-training corpus from the follow-\ning four sources:\nCommon Crawl (CC): We use 10 shards of\nCommon Crawl 3 data from March to De-\ncember 2020. After removing non-Arabic\ntext, this dataset is 444GB in size. Addi-\ntionally, we use the monthly shard of CC\nfrom November 2018 provided by the OS-\nCAR project (Suárez et al., 2019). We down-\nload the unshuffled version (31GB) from Hug-\ngingFace Datasets (Lhoest et al., 2021).\n3https://commoncrawl.org\nNEWS: We use the links provided in the Open\nSource International Arabic News Corpus\n(Zeroual et al., 2019) to collect 21GB of tex-\ntual data from 19 popular Arabic news web-\nsites.\nEL-KHAIR: We use the 1.5 billion words Arabic\nCorpus (El-Khair, 2016) which is a collection\nof newspaper articles published by 10 Arabic\nnews sources between 2002-2014.\nWIKI: We use the Arabic Wikipedia dump4 from\nJune 2021 and extract the text of articles using\nWikiExtractor (Attardi, 2015).\nRecent studies have highlighted the importance\nof cleaning up raw pre-training data for achieving\nbetter performance on downstream tasks (Raffel\net al., 2019; Brown et al., 2020). We developed a\nset of heuristics for cleaning our Arabic corpora\nthat is able to filter out gibberish, noisy and dupli-\ncated texts (see Appendix A.1).\nSource Original Clean\nCC 475GB 87GB (18%)\nNEWS 21GB 14GB (67%)\nEL-KHAIR 16GB 13GB (82%)\nWIKI 1.6GB 1GB (63%)\nTotal 514GB 115GB (22%)\nTable 2: Size of our pre-training corpus before and\nafter applying the data cleaning methods. Parentheses\nindicate the proportion of the remaining data.\nTable 2 shows the size of our pre-training cor-\npora before and after data pre-processing. The\nfinal pre-training dataset represents only 22% of\nthe original corpus and is 115GB in size. Al-\nthough our approach seemingly filters out a large\nproportion of the dataset, our corpus size is com-\nparable with other models such as Arabic-BERT\n4https://dumps.wikimedia.org/\n3137\nTask |Train| |Dev| |Test| Metric |Classes| Domain Lang Seq. Len.\nSingle-Sentence Classification\nMDD 42k 5k 5k F1-macro 26 Travel DIAL 7 ±3.7\nOOLD 7k 1k 1k F1-macro 2 Tweet DIAL 21 ±13.3\nOHSD 7k 1k 1k F1-macro 2 Tweet DIAL 21 ±13.3\nFID 4k - 1k F1-macro 2 Tweet DIAL 23 ±11.7\nSentence-Pair Classification\nMQ2Q 12k - 4k F1-macro 2 Web MSA 13 ±2.9\nXNLI 5k - 3k Accuracy 3 Misc MSA 27 ±9.6\nMulti-label Classification\nSEC 2k 600 1k Jaccard 11 Tweet DIAL 18 ±7.8\nRegression\nSVREG 1k 138 1k Pearson 1 Tweet DIAL 18 ±7.9\nTable 3: Task descriptions and statistics of the ALUE benchmark. Test sets in bold use labels that are publicly\navailable. The average sequence length and standard deviations are computed based on the word count of the\ntokenized text of the training set.\n(95GB) and MARBERT (128GB). Moreover, as\nwe will discuss in Section 4, our models are able\nto significantly outperform other models that used\nlight pre-processing (Safaya et al., 2020; Abdul-\nMageed et al., 2021). We also utilise the Arabic\ntext-normalization procedure of AraBERT5 which\ninvolves removing emojis, tashkeel, tatweel, and\nHTML markup (Antoun et al., 2020).\n3.2 Our Models\nWe pre-train both BERT- and T5-style models.\nJABER and SABER stand for Junior (12-layer)\nand Senior (24-layer) Arabic BERT models re-\nspectively. They follow the default configura-\ntion of BERT-base and BERT-large (Devlin et al.,\n2018) respectively. We also enhance JABER with\ncharacter-level representations at the input layer,\nwhich we refer to as the Char-JABER model.\nFor Char-JABER, each word is represented as\na sequence of characters, and we use a m-layer\nCNN encoder (Chiu and Nichols, 2016; Lee et al.,\n2018) to obtain a continuous vector of character-\nlevel representation for each word. The final input\nrepresentation is obtained by adding those vectors\nto the original BERT input representations (token,\nsegment, and position). Note that all sub-tokens\nof the same word share the same character-level\nrepresentation of that word.\nAT5B and AT5S use the same encoder-decoder\narchitecture and configuration of T5-base and T5-\nsmall (Raffel et al., 2019) respectively. AT5B is di-\n5https://github.com/aub-mind/arabert/blob/\nmaster/preprocess.py\nrectly comparable with AraT5-base (Nagoudi et al.,\n2022), the state-of-the-art model for Arabic genera-\ntive tasks. The configurations and implementation\ndetails of our models are listed in Appendix A.2\nand A.3.\n4 Experimental Protocol\n4.1 Datasets\nWe evaluate all models on the newly proposed\nALUE benchmark (Seelawi et al., 2021). ALUE\nis a collection of eight Arabic NLU tasks: four\nsingle-sentence, two sentence-pair, and one multi-\nlabel classification task, as well as one regression\ntask. Five of the eight ALUE tasks are sourced\nfrom Twitter data whereas six tasks involve dialec-\ntal Arabic. We refer the readers to Seelawi et al.\n(2021) for a detailed description of each task.\nThe final score is computed as the unweighted\naverage over those tasks. ALUE is powered by a\nleaderboard6 with privately-held test sets and we\npresent a brief summary of the ALUE tasks in Ta-\nble 3.\nWe note three potential limitations with the\nALUE benchmark: (1) the size of training data and\naverage sequence lengths across tasks are smaller\nwhen compared with GLUE (Wang et al., 2018),\n(2) the test set labels are public for three tasks: FID,\nXNLI, and MDD, and (3) development sets are not\navailable for three tasks: FID, XNLI and MQ2Q.\nFollowing Seelawi et al. (2021), we use the avail-\nable test set as the development set for FID and\n6https://www.alue.org/leaderboard\n3138\nXNLI. In order to create the development set for\nMQ2Q, we use a simple approach: (i) we convert\nthe development set of another task called QQP 7\nfrom English to Arabic using an online translation\nservice, (ii) we pick a random sample of 2k positive\nand 2k negative instances. We only pick sentence\npairs that do not contain any English letters to cre-\nate a high-quality development set.\nUnfortunately, there is no equivalent of the\nALUE benchmark for Arabic generative tasks\nwhich has a leaderboard with a fixed train/dev/test\nsplit and privately-held test set. Therefore, we\nevaluate encoder-decoder models on a selected\nset of generative tasks from the ARGEN bench-\nmark (Nagoudi et al., 2022): Text Summarization\n(TS), Question Generation (QG), and Question\nAnswering (QA), where the latter is treated as a\nsequence-to-sequence generative task. In addition,\nwe experiment on a single-turn dialogue task using\nthe Empathetic Dialogue (EMD) dataset (Naous\net al., 2021). See Appendix B.1 for a detailed de-\nscription of the datasets, splits, and evaluation met-\nrics.\n4.2 Baselines\nOn one hand, we compare our JABER, Char-\nJABER and SABER models with the popular Ara-\nbic PLMs: Arabic-BERT (Safaya et al., 2020),\nAraBERT (Antoun et al., 2020), CAMeLBERT (In-\noue et al., 2021), ARBERT and MARBERT\n(Abdul-Mageed et al., 2021). On the other hand, we\nevaluate our AT5S and AT5B models against the re-\ncently proposed AraT5-base (Nagoudi et al., 2022)\nand AraB2B (Naous et al., 2021) models. The lat-\nter is an encoder-decoder model initialized with the\nweights of AraBERT. CAMeLBERT and AraT5-\nbase refer to CAMeLBERT-MIX and AraT5 mod-\nels in (Inoue et al., 2021) and (Nagoudi et al., 2022)\nrespectively. These models were pretrained on a\nmix of MSA and tweets (the largest possible cor-\npus) and achieve the best overall performance in\ntheir respective papers.\n4.3 Implementation Details\nIn order to ensure a fair comparison amongst ex-\nisting models, we define a systematic evaluation\nprotocol following the recommendations of Pineau\net al. (2021). The following four-step approach is\napplied to every model (including the baselines) for\n7https://www.quora.com/q/quoradata/\nFirst-Quora-Dataset-Release-Question\neach of the ALUE and generative tasks:(1) We con-\nduct extensive hyperparameter-search experiments\n(e.g. 60 for BERT models) to find the best combi-\nnation of batch size, learning rate, and dropout rate;\n(2) We use the best found hyperparameter-setting to\nperform 5 runs with different random seeds;(3) We\nreport the average and the standard deviation on the\ndevelopment set; (4) We use the best-performing\nmodels of the development set experiments for the\nALUE leaderboard submissions, as well as for re-\nporting the test-set scores of the encoder-decoder\nmodels.\nFor BERT-style models, we use the\nAdamW (Loshchilov and Hutter, 2017) opti-\nmizer with a learning rate decay. Fixing the\nnumber of epochs to 30, we perform grid search\nwith multiple runs to find the best hyperparameters:\nlearning rate from {7e-6, 2e-5, 5e-5}, batch-size\nfrom {8, 16, 32, 64, 128}, hidden dropout from\n{0.1, 0.2, 0.3, 0.4}. For encoder-decoder models,\nwe use the Adafactor (Shazeer and Stern, 2018)\nwith inverse square root decay and pick a learning\nrate from {1e-3, 1e-4, 1e-5}. The fine-tuning\ncode is based on the PyTorch (Paszke et al., 2019)\nversion of the Transformers library (Wolf et al.,\n2020). We run all experiments on a single NVIDIA\nTesla V100 GPU. The best hyperparameters for\nthe generative and ALUE tasks can be found in\nTable 12 and Table 13 respectively (Appendix B).\n5 Results of BERT-Style Models\n5.1 ALUE Dev\nThe performance of all BERT-based models in-\ncluding the baselines on the development set of\nALUE tasks is presented in Table 4. We report\nthe average and standard deviation of 5 runs with\ndifferent random seeds. We observe that the vari-\nance in performances of the multiple runs is low\nand is approximately the same on average for all\nBERT-base models, with the exception of OHSD\nwhere all models exhibit higher variance. Inter-\nestingly, Char-JABER and SABER report a lower\nvariance across the five runs when compared to the\nBERT-base models.\nIt can be seen that Arabic-BERT and AraBERT\nhave comparable performances (average score of\n72.4% and 72.6% respectively). This could be due\nto the similar size of training data used by both\nmodels: Arabic-BERT was pre-trained on 95GB\nof text data that was duplicated 3 times (285GB),\nwhile AraBERT was pre-trained on 27GB dupli-\n3139\nModel MQ2Q* MDD SVREG SEC FID OOLD XNLI OHSD Avg.\nBaselines\nCAMeLBERT68.9±1.1 62.9±0.1 86.7±0.1 45.4±0.5 84.9±0.6 91.3±0.4 55.7±1.2 81.1±0.7 72.1±0.6\nArabic-BERT73.3±0.6 61.9±0.2 83.6±0.8 42.4±0.4 83.9±0.6 88.8±0.5 66.0±0.6 79.3±1.0 72.4±0.6\nAraBERT 73.5±0.5 61.1±0.3 82.3±0.9 42.2±0.6 85.2±0.2 89.7±0.4 67.2±0.4 79.9±1.8 72.6±0.6\nMARBERT 69.1±0.9 63.2±0.3 88.0±0.4 47.6±0.9 84.7±0.4 91.8±0.3 63.3±0.7 83.8±1.4 73.9±0.7\nARBERT 74.7±0.1 62.5±0.2 83.5±0.6 43.9±0.6 85.3±0.3 90.5±0.5 70.8±0.5 81.9±2.0 74.1±0.6\nOurs\nJABER 75.1±0.3 65.7±0.3 87.4±0.7 46.8±0.8 84.8±0.3 92.2±0.5 72.4±0.7 85.0±1.6 76.2±0.7\nChar-JABER 76.8±0.2 67.3±0.2 87.5±0.3 47.8±0.4 85.7±0.2 93.3±0.1 72.7±0.3 86.4±0.5 77.2±0.3\nSABER 77.7±0.4 67.4±0.2 89.3±0.3 49.0±0.5 86.1±0.3 93.4±0.4 75.9±0.3 88.9±0.3 78.5±0.3\nTable 4: DEV performances and standard deviations over 5 runs on the ALUE benchmark. Bold entries describe the\nbest results among all models, while underlined entries show best results among BERT-base models. * indicates\nthat the results are on our own MQ2Q dev set.\ncated 10 times (270GB). While CAMeLBERT out-\nperforms the other baseline models on certain tasks,\nit achieves the lowest average score of 72.1. This\nis due to its poor performance on MQ2Q (68.9)\nand XNLI (55.7), both of which are sentence-pair\nclassification tasks and involve MSA data.\nARBERT achieves the highest average score of\n74.1% closely followed by MARBERT (73.9%).\nMARBERT was pre-trained on a large corpus of\nArabic tweets and we observe that it performs well\non tasks that involve tweet-data. The opposite holds\ntrue for ARBERT.\nOur JABER model significantly outperforms the\nbest existing baseline model (ARBERT) by 2.3%\non the average ALUE score. While MARBERT\nperforms marginally better on the SVREG and\nSEC tasks, JABER significantly outperforms MAR-\nBERT on all other tasks, particularly the MSA tasks\n– XNLI and MQ2Q – where it achieves gains of\n+9.1% and +6.0% respectively.\nWe see further improvements when the JABER\nmodel is enhanced with character representations at\nthe input level. Char-JABER performs better than\nJABER on all ALUE tasks resulting in a one point\njump in the average ALUE score. Moreover, it can\nbe seen that Char-JABER outperforms MARBERT\non all tasks (except on SVREG) that involve tweets\nand dialect data, despite not being pre-trained on\ntweet corpora.\nCharacter-level information can be crucial for\nmorphologically rich languages like Arabic, where\nmany less frequent dialect words share the same\nroot and meaning as more frequent MSA words.\nWe integrate this information in an unsupervised\nmanner into both pretraining and fine-tuning stages.\nWe do so without adding any computational over-\nhead and without requiring massive amounts of\nTwitter data (unlike MARBERT) which can be dif-\nficult to obtain for a large section of the research\ncommunity.\nAs expected, our large SABER model outper-\nforms all the BERT-base models on all ALUE tasks\n(including MARBERT on SVREG), achieving a\n2.3% and 1.3% improvements on ALUE average\nover JABER and Char-JABER respectively. In our\nstudy, it seems that increasing the model capacity\nis more important than adding character level infor-\nmation for modelling low frequent dialect words.\nNevertheless, combining both techniques may fur-\nther improve performance, which we leave for fu-\nture work.\n5.2 ALUE Test\nTable 5 shows the test performance of all BERT-\nbased models on the ALUE leaderboard. The top\ntwo rows correspond to the baselines provided by\nthe ALUE authors and the values are directly taken\nfrom the leaderboard. The middle and the bottom\nsections display the performances of our competi-\ntors’ baselines and our own models respectively.\nWe keep the baseline results private8 since we are\nnot the owners of these models. Figure 1 in Ap-\npendix includes a screenshot of the leaderboard\nfrom June 2022.\nInterestingly, we observe that our private sub-\nmission of the Arabic-BERT model achieves an\naverage ALUE score of 69.3% which is 2.2 per-\ncentage points higher than the one available on the\nALUE leaderboard. This can directly be attributed\nto our extensive fine-tuning protocol (described\n8We contacted the owners of the ALUE leaderboard to\nsubmit the other baseline models in private mode.\n3140\nModel MQ2Q MDD SVREG SEC FID OOLD XNLI OHSD Avg. DIAG\nALUE Baselines\nmBERT 83.2 61.3 33.9 14.0 81.6 80.3 63.1 70.5 61.0 19.0\nArabic-BERT 85.7 59.7 55.1 25.1 82.2 89.5 61.0 78.7 67.1 19.6\nOur Private Submissions of Baselines\nAraBERT 89.2 58.9 56.3 24.5 85.5 88.9 67.4 76.8 68.4 23.5\nArabic-BERT 89.7 59.7 58.0 26.5 84.3 89.1 67.0 80.1 69.3 19.0\nCAMeLBERT 89.4 61.3 69.5 30.3 85.5 90.3 56.1 80.6 70.4 11.8\nARBERT 89.3 61.2 66.8 30.3 85.4 89.5 70.7 78.2 71.4 24.3\nMARBERT 83.3 61.9 75.9 36.0 85.3 92.1 64.3 78.9 72.2 12.3\nOurs\nJABER 93.1 64.1 70.9 31.7 85.3 91.4 73.4 79.6 73.7 24.4\nChar-JABER 92.0 66.1 74.5 34.7 86.0 92.3 73.1 83.5 75.3 26.7\nSABER 93.3 66.5 79.2 38.8 86.5 93.4 76.3 84.1 77.3 26.2\nTable 5: Leaderboard test results (as of 24/06/2022) of experiments on ALUE tasks and their diagnostic dataset\n(DIAG). Bold entries describe the best results among all models, while underlined entries show best results among\nBERT-base models.\nin Section 4.3). Specifically, the proper tuning of\nthe hyperparameters for our version of the Arabic-\nBERT model resulted in an overall improvement.\nSurprisingly, we also observe that the relative\nranks of the baseline models have changed drasti-\ncally as compared to the dev set (Table 4). CAMeL-\nBERT had the lowest average ALUE score of\n72.1% on the dev set, but it now outperforms\nAraBERT and Arabic-BERT on the leaderboard\ntest-set. Similarly, MARBERT outperforms AR-\nBERT by 0.8% on the leaderboard while being\n0.3% behind on the dev set. This happens despite\nour extensive hyperparameter tuning protocol and\nthe fact that we perform multiple runs. This ob-\nservation underscores the importance of having\nseparate privately-held test sets to determine the\nactual state-of-the-art rankings for Arabic PLMs.\nWe observe that our models consistently rank at\nthe top for both ALUE dev and test sets. JABER\noutperforms all other existing Arabic language\nmodels achieving an average score of 73.7%. Char-\nJABER outperforms JABER with a 1.6% increase\nin the average ALUE score. SABER expectedly\nfurther boosts the average score by 2% compared\nto JABER, achieving the new state-of-the-art score\nof 77.3% on the ALUE benchmark.\nIt is interesting to note that Char-JABER is\nable to outperform the much larger SABER model\n(by 0.5%) on ALUE’s diagnostic data (DIAG), a\ndataset which is designed to capture the complex\nlinguistic phenomena of Arabic (Seelawi et al.,\n2021). Moreover, it performs better than JABER\non all the ALUE tasks (except MQ2Q and XNLI).\nTherefore, we argue that augmenting language\nmodels with character information is a worthy pur-\nsuit for Arabic NLU.\n6 Results of Encoder-Decoder Models\nTable 6 shows the performance of our T5 models\n(AT5S and AT5B) and AraT5-base (Nagoudi et al.,\n2022) on the development split of the ALUE tasks.\nExpectedly, the smaller variant AT5S achieves a\nlower average score. The performance of our AT5B\nmodel is very similar to that of AraT5-base with\nboth models slightly outperforming each other on\nfour tasks each.\nTask name AT5S AT5B AraT5-base\nMQ2Q⋆ 73.0±0.1 73.7±0.1 70.5±2.1\nOOLD 88.4±0.2 90.0 ±0.4 90.5±0.4\nOHSD 81.0±1.8 81.2±2.1 78.3±1.4\nSVREG 75.6±1.6 78.1 ±2.4 80.8±1.3\nSEC 41.3±0.5 43.8 ±0.7 44.0±0.6\nFID 82.1±0.6 83.1±0.5 82.3±0.4\nXNLI 67.9±0.3 72.2 ±0.4 72.5±1.5\nMDD 63.1±0.3 64.7±0.2 63.6±0.2\nAvg 71.5±0.7 73.3±0.9 73.0±1.0\nTable 6: ALUE scores of Arabic T5-style models on the\ndevelopment set. Results on our own MQ2Q dev set are\nmarked by a ⋆.\nMoreover, comparing Table 4 and Table 6, we\nobserve that T5-style Arabic PLMs perform worse\nthan the BERT-based models on the same ALUE\n3141\nbenchmark. This is in contrast to the powerful per-\nformance of T5-models on English language tasks\n(Raffel et al., 2019). This observation requires fur-\nther investigations, and therefore we did not submit\nour T5 models to the ALUE leaderboard.\nDev Test\nModel EM F1 EM F1\nAT5S 36.8±0.4 57.5 ±0.3 29.2 65.1\nAT5B 40.8±0.7 61.6 ±1.1 31.6 67.2\nAraT5-base 40.2±0.4 61.4 ±0.8 31.2 65.7\nAraB2B 27.3±2.5 47.9 ±1.6 22.7 54.0\nTable 7: F1-score and Exact Match (EM) of T5-style\nmodels on the Question Answering task.\nQG EMD\nModel Dev Test Dev Test\nAT5S 7.8 ±0.4 15.6 2.1 ±0.1 1.9\nAT5B 8.1±0.1 17.0 2.3±0.1 2.0\nAraT5-base 6.7 ±0.1 13.5 2.0 ±0.0 1.8\nAraB2B 4.7 ±0.3 11.7 2.0 ±0.0 1.8\nTable 8: BLEU score of T5-style models on the Ques-\ntion Generation and Empathetic Dialogue tasks.\nRouge1 Rouge2 RougeL\nWikiLingua Dev\nAT5S 24.3 ±1.3 9.5 ±0.6 21.6 ±1.0\nAT5B 26.1 ±2.8 10.5 ±1.6 23.2 ±2.5\nAraT5-base 25.0 ±0.2 10.0 ±0.0 22.4 ±0.2\nWikiLingua Test\nAT5S 25.2 9.9 22.4\nAT5B 27.8 11.5 24.8\nAraT5-base 25.1 10.2 22.5\nEASC Test\nAT5S 11.3 2.7 10.1\nAT5B 12.6 3.5 11.3\nAraT5-base 10.7 2.7 9.3\nTable 9: T5-style models’ performances on the Text\nSummarization task.\nIn order to perform a more meaningful evalu-\nation, we also evaluate the Arabic T5 models on\nfour other tasks: Empathetic Dialogue (EMD), Text\nSummarization (TS), Question Answering (QA)\nand Question Generation (QG). We present the per-\nformances of all T5-based models on QA in Table 7,\non QG and EMD in Table 8 and on TS in Table 9.\nNote that we do not experiment with AraB2B on\nTS as BERT model is constrained by a maximum\ninput length of 512.\nOur AT5B model significantly outperforms\nAraT5-base on Question Generation and WikiLin-\ngua summarization tasks by 3.5 points and 2.7\npoints respectively. On the remaining QA and\nEMD tasks, the performance of the two mod-\nels is similar with our AT5B model performing\nmarginally better. Moreover, we observe in Table 8\nthat even our smaller AT5S model is able to outper-\nform the bigger AraT5-base on QG and EMD tasks\nwhile achieving comparable scores on TS and QA\ntasks. This can be very useful for the community\nfor operating in a low latency setting.\nFinally, we observe from Table 7 and Table 8\nthat the performance of AraB2B model is worse\nthan all other T5-based models. We believe that the\nBERT2BERT approach for Arabic response gen-\neration adopted in (Naous et al., 2021) is not well\nsuited for such generation tasks, and it is preferable\nto pre-train the model from scratch compared to\ninitializing the encoder-decoder architecture with\npre-trained weights.\nAcceptable Best\nQG\nAT5B 68%±10 56% ±12\nAraT5-base 37%±11 19% ±14\nAraB2B 40%±12 25% ±02\nEMD\nAT5B 53%±08 50% ±07\nAraT5-base 50%±12 37% ±10\nAraB2B 27%±04 13% ±05\nTS\nAT5B 74%±08 66% ±05\nAraT5-base 61%±12 34% ±04\nTable 10: Human evaluation performances on 3 genera-\ntive tasks.\nThe ideal way to measure performance on lan-\nguage generation tasks is to ask humans to evaluate\nthe models’ outputs (Sai et al., 2022). Thus, we\nevaluate our T5-based and AraB2B models on the\nthree generation tasks of QG, EMD and TS us-\ning human annotators. Each of the three models’\noutputs was evaluated by four annotators. We per-\nform both absolute and relative comparison of the\nthree models. Specifically, we asked the annotators\nto label a hundred outputs from each model and\neach task for two scenarios: (1) Acceptable: each\nmodel output is labeled for whether it is acceptable\n(not strictly perfect) to the annotator or not, and\n(2) Best: where the annotator must pick exactly\none best output out of the three ones. In order to\n3142\nmitigate annotation biases, we randomly shuffle,\nanonymize and rename the three models’ outputs.\nThe results of our human evaluation study for\nboth Acceptable and Best scenarios are presented\nin Table 10. First, we assert that the reported values\nare reliable as the standard deviation is low (approx-\nimately 10%) across all tasks. Second, we observe\nthat the scores obtained in the human evaluation\nstudy are much higher than what the correspond-\ning BLEU and ROUGE scores reported in Table 8\nwould suggest. On EMD, for example, our AT5B\nmodel achieves a score of 53% for the Acceptable\nscenario as compared to the previously reported\nBLEU score of 2.0. This is possible because the\ndialogue generated by the model could be convey-\ning the same tone and emotion as the reference\ndialogue which led the annotators to mark it as Ac-\nceptable, despite having a low n-gram overlap with\nthe reference dialogue.\nFinally, we can conclude from Table 10 that our\nAT5B model was preferred by the annotators for\nboth scenarios on each of the three tasks. The im-\nprovement over AraT5-base is considerably large\nfor QG and TS tasks as compared to the empa-\nthetic dialogues task. On EMD, we observe that\nonly a fraction of all of the models’ responses are\nconsidered acceptable by the annotators. However,\neven in that scenario, the annotators pick our AT5B\nmodel as the best-performing one since it is able to\nproduce the most syntactically correct and coherent\nresponses. One reason for the overall low perfor-\nmance on these tasks is the quality of the datasets\navailable for Arabic NLP: the data is not originally\nin Arabic and the datasets were created via auto-\nmatic translation from English datasets. Therefore,\nin order to make meaningful progress in Arabic\nNLP, we argue that the community needs to curate\nhigh-quality datasets dedicatedly for Arabic.\n7 Conclusion\nIn this work, we revisit the pre-training and eval-\nuation of Arabic PLMs. We introduced five new\nArabic language models using both BERT- and T5-\nstyle pre-training schemes. Our models outperform\nall existing Arabic models on the generative tasks\nas well as on the ALUE benchmark, with SABER\nsetting a new state-of-the-art on ALUE.\nIn order to accelerate the progress in Ara-\nbic NLU, we advocate for the creation of more\nleaderboard-based benchmarks with privately-held\nevaluation sets that cover a wide array of tasks.\nMoreover, we strongly recommend researchers fol-\nlow a systematic approach similar to the one we\npropose when evaluating Arabic models, with ex-\ntensive hyperparameter tuning and multiple runs\nwith different random seeds.\nIn the future, our research will mainly focus\non scaling up Arabic PLMs to tens (and hun-\ndreds) of billions of parameters in an energy-\nefficient manner (Du et al., 2021; Chowdh-\nery et al., 2022) as well as scaling up with\nhigh-quality pre-training data (Hoffmann et al.,\n2022). Having met all the other conditions in\nthe Reproducibility Checklist (Pineau, 2020), we\nmake our source code and models freely avail-\nable at https://github.com/huawei-noah/Pretrained-\nLanguage-Model/tree/master/JABER-PyTorch.\nLimitations\nWhile we evaluated our models on a diverse set of\nclassification and generative tasks, there are sev-\neral NLP tasks that were not accounted for in our\nstudy. It would be worthwhile to explore other\ntasks such as named-entity recognition (Benajiba\nand Rosso, 2007) or coreference resolution (Prad-\nhan et al., 2012). Also, there are other Arabic\nPLMs (Talafha et al., 2020; Lan et al., 2020) that\nwere not used in our evaluation study. Those mod-\nels have been reported to underperform the PLMs\nwe have considered as baselines in our study. How-\never, there is a small chance that including them\nmight change the performance ranking in our eval-\nuation.\nAs the focus of this study is on overall bench-\nmark performances, we did not assess the robust-\nness of our models on out-of-domain datasets. Fi-\nnally, our study lacks a qualitative exploration of\nthe datasets and models’ error analyses, which we\nleave for future work. In particular, we wish to ex-\nplore the impressive performance of Char-JABER\non ALUE’s diagnostic data.\nAcknowledgments\nWe thank Mindspore,9 a new deep learning comput-\ning framework, for the partial support of this work.\nWe are also thankful to the anonymous reviewers\nfor their insightful comments.\n9https://www.mindspore.cn/\n3143\nReferences\nMuhammad Abdul-Mageed, AbdelRahim Elmadany,\nand El Moatez Billah Nagoudi. 2021. ARBERT &\nMARBERT: Deep bidirectional transformers for Ara-\nbic. In Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n7088–7105, Online. Association for Computational\nLinguistics.\nWissam Antoun, Fady Baly, and Hazem Hajj. 2020.\nArabert: Transformer-based model for arabic lan-\nguage understanding. In LREC 2020 Workshop Lan-\nguage Resources and Evaluation Conference 11–16\nMay 2020, page 9.\nGiusepppe Attardi. 2015. Wikiextractor. https://\ngithub.com/attardi/wikiextractor.\nIz Beltagy, Matthew E. Peters, and Arman Cohan. 2020.\nLongformer: The long-document transformer.\nYassine Benajiba and Paolo Rosso. 2007. Anersys 2.0:\nConquering the ner task for the arabic language by\ncombining the maximum entropy with pos-tag infor-\nmation. In IICAI, pages 1814–1823.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nJosé Canete, Gabriel Chaperon, Rodrigo Fuentes, and\nJorge Pérez. 2020. Spanish pre-trained bert model\nand evaluation data. PML4DC at ICLR, 2020.\nRishav Chakravarti, Anthony Ferritto, Bhavani Iyer, Lin\nPan, Radu Florian, Salim Roukos, and Avi Sil. 2020.\nTowards building a robust industry-scale question\nanswering system. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics:\nIndustry Track, pages 90–101, Online. International\nCommittee on Computational Linguistics.\nBranden Chan, Stefan Schweter, and Timo Möller.\n2020. German’s next language model. arXiv preprint\narXiv:2010.10906.\nYanran Chen, Jonas Belouadi, and Steffen Eger. 2022.\nReproducibility issues for bert-based evaluation met-\nrics. arXiv preprint arXiv:2204.00004.\nJason P. C. Chiu and Eric Nichols. 2016. Named en-\ntity recognition with bidirectional lstm-cnns. Trans.\nAssoc. Comput. Linguistics, 4:357–370.\nKrzysztof Marcin Choromanski, Valerii Likhosherstov,\nDavid Dohan, Xingyou Song, Andreea Gane, Tamas\nSarlos, Peter Hawkins, Jared Quincy Davis, Afroz\nMohiuddin, Lukasz Kaiser, David Benjamin Be-\nlanger, Lucy J Colwell, and Adrian Weller. 2021.\nRethinking attention with performers. In Interna-\ntional Conference on Learning Representations.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. arXiv preprint\narXiv:2204.02311.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2022. Canine: Pre-training an efficient\ntokenization-free encoder for language representa-\ntion. Trans. Assoc. Comput. Linguistics, 10:73–91.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nJay DeYoung, Sarthak Jain, Nazneen Fatema Rajani,\nEric Lehman, Caiming Xiong, Richard Socher, and\nByron C. Wallace. 2020. ERASER: A benchmark\nto evaluate rationalized NLP models. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, ACL 2020, Online,\nJuly 5-10, 2020, pages 4443–4458. Association for\nComputational Linguistics.\nLuca Di Liello, Matteo Gabburo, and Alessandro Mos-\nchitti. 2021. Efficient pre-training objectives for\ntransformers.\nNan Du, Yanping Huang, Andrew M. Dai, Simon\nTong, Dmitry Lepikhin, Yuanzhong Xu, Maxim\nKrikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat,\nBarret Zoph, Liam Fedus, Maarten Bosma, Zong-\nwei Zhou, Tao Wang, Yu Emma Wang, Kellie Web-\nster, Marie Pellat, Kevin Robinson, Kathy Meier-\nHellstern, Toju Duke, Lucas Dixon, Kun Zhang,\nQuoc V Le, Yonghui Wu, Zhifeng Chen, and Claire\nCui. 2021. Glam: Efficient scaling of language mod-\nels with mixture-of-experts.\nHicham El Boukkouri, Olivier Ferret, Thomas Lavergne,\nHiroshi Noji, Pierre Zweigenbaum, and Jun’ichi Tsu-\njii. 2020. CharacterBERT: Reconciling ELMo and\nBERT for word-level open-vocabulary representa-\ntions from characters. In Proceedings of the 28th\nInternational Conference on Computational Linguis-\ntics, pages 6903–6915, Barcelona, Spain (Online).\nInternational Committee on Computational Linguis-\ntics.\nMahmoud El-Haj, Udo Kruschwitz, and Chris Fox.\n2010. Using mechanical turk to create a corpus of\narabic summaries.\nIbrahim Abu El-Khair. 2016. 1.5 billion words Arabic\nCorpus. arXiv preprint arXiv:1611.04033.\nKawin Ethayarajh and Dan Jurafsky. 2020. Utility is in\nthe eye of the user: A critique of nlp leaderboards.\narXiv preprint arXiv:2009.13888.\nDaniela Gerz, Ivan Vulic, Edoardo Maria Ponti, Roi\nReichart, and Anna Korhonen. 2018. On the relation\nbetween linguistic typology and (limitations of) mul-\ntilingual language modeling. In Proceedings of the\n3144\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, Brussels, Belgium, October 31\n- November 4, 2018, pages 316–327. Association for\nComputational Linguistics.\nJordan Hoffmann, Sebastian Borgeaud, Arthur Men-\nsch, Elena Buchatskaya, Trevor Cai, Eliza Ruther-\nford, Diego de Las Casas, Lisa Anne Hendricks,\nJohannes Welbl, Aidan Clark, et al. 2022. Train-\ning compute-optimal large language models. arXiv\npreprint arXiv:2203.15556.\nGo Inoue, Bashar Alhafni, Nurpeiis Baimukan, Houda\nBouamor, and Nizar Habash. 2021. The interplay\nof variant, size, and task type in arabic pre-trained\nlanguage models. In Proceedings of the Sixth Arabic\nNatural Language Processing Workshop, WANLP\n2021, Kyiv, Ukraine (Virtual), April 9, 2021, pages\n92–104. Association for Computational Linguistics.\nKatikapalli Subramanyam Kalyan, Ajit Rajasekharan,\nand Sivanesan Sangeetha. 2021. AMMUS: A survey\nof transformer-based pretrained models in natural\nlanguage processing.\nDouwe Kiela, Max Bartolo, Yixin Nie, Divyansh\nKaushik, Atticus Geiger, Zhengxuan Wu, Bertie Vid-\ngen, Grusha Prasad, Amanpreet Singh, Pratik Ring-\nshia, et al. 2021. Dynabench: Rethinking benchmark-\ning in nlp. In Proceedings of the 2021 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 4110–4124.\nYoon Kim, Yacine Jernite, David A. Sontag, and Alexan-\nder M. Rush. 2016. Character-aware neural language\nmodels. In Proceedings of the Thirtieth AAAI Confer-\nence on Artificial Intelligence, February 12-17, 2016,\nPhoenix, Arizona, USA , pages 2741–2749. AAAI\nPress.\nFaisal Ladhak, Esin Durmus, Claire Cardie, and Kath-\nleen McKeown. 2020. WikiLingua: A new bench-\nmark dataset for cross-lingual abstractive summariza-\ntion. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 4034–4048,\nOnline. Association for Computational Linguistics.\nWuwei Lan, Yang Chen, Wei Xu, and Alan Ritter. 2020.\nAn empirical study of pre-trained transformers for\narabic information extraction. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 4727–4734.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nHang Le, Loïc Vial, Jibril Frej, Vincent Segonne, Max-\nimin Coavoux, Benjamin Lecouteux, Alexandre Al-\nlauzen, Benoit Crabbé, Laurent Besacier, and Didier\nSchwab. 2020. Flaubert: Unsupervised language\nmodel pre-training for french. In Proceedings of The\n12th Language Resources and Evaluation Confer-\nence, pages 2479–2490.\nChanhee Lee, Young-Bum Kim, Dongyub Lee, and\nHeui-Seok Lim. 2018. Character-level feature ex-\ntraction with densely connected networks. In Pro-\nceedings of the 27th International Conference on\nComputational Linguistics, pages 3228–3239.\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu,\nDehao Chen, Orhan Firat, Yanping Huang, Maxim\nKrikun, Noam Shazeer, and Zhifeng Chen. 2021.\nGshard: Scaling giant models with conditional com-\nputation and automatic sharding. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nQuentin Lhoest, Albert Villanova del Moral, Yacine\nJernite, Abhishek Thakur, Patrick von Platen, Suraj\nPatil, Julien Chaumond, Mariama Drame, Julien Plu,\nLewis Tunstall, et al. 2021. Datasets: A community\nlibrary for natural language processing. In Proceed-\nings of the 2021 Conference on Empirical Methods\nin Natural Language Processing: System Demonstra-\ntions, pages 175–184.\nGuangjun Li, Xianzhi Wang, and Minxi Li. 2021. A\nreview of recent trends and industry prospects for\nartificial intelligence technologies. In 2021 8th In-\nternational Conference on Behavioral and Social\nComputing (BESC), pages 1–7. IEEE.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nIlya Loshchilov and Frank Hutter. 2017. Decou-\npled weight decay regularization. arXiv preprint\narXiv:1711.05101.\nWentao Ma, Yiming Cui, Chenglei Si, Ting Liu, Shijin\nWang, and Guoping Hu. 2020. CharBERT: Character-\naware pre-trained language model. In Proceedings\nof the 28th International Conference on Computa-\ntional Linguistics, pages 39–50, Barcelona, Spain\n(Online). International Committee on Computational\nLinguistics.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz\nSuárez, Yoann Dupont, Laurent Romary, Éric Ville-\nmonte de la Clergerie, Djamé Seddah, and Benoît\nSagot. 2019. Camembert: a tasty french language\nmodel. arXiv preprint arXiv:1911.03894.\nS. Mehri, M. Eric, and D. Hakkani-Tur. 2020.\nDialoglue: A natural language understanding\nbenchmark for task-oriented dialogue. ArXiv,\nabs/2009.13570.\n3145\nBonan Min, Hayley Ross, Elior Sulem, Amir\nPouran Ben Veyseh, Thien Huu Nguyen, Oscar Sainz,\nEneko Agirre, Ilana Heinz, and Dan Roth. 2021. Re-\ncent advances in natural language processing via\nlarge pre-trained language models: A survey. arXiv\npreprint arXiv:2111.01243.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany, and\nMuhammad Abdul-Mageed. 2022. AraT5: Text-to-\ntext transformers for Arabic language generation. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 628–647, Dublin, Ireland. Asso-\nciation for Computational Linguistics.\nEl Moatez Billah Nagoudi, AbdelRahim Elmadany,\nMuhammad Abdul-Mageed, and Tariq Alhindi. 2020.\nMachine generation and detection of Arabic manipu-\nlated and fake news. In Proceedings of the Fifth Ara-\nbic Natural Language Processing Workshop, pages\n69–84, Barcelona, Spain (Online). Association for\nComputational Linguistics.\nTarek Naous, Wissam Antoun, Reem Mahmoud, and\nHazem Hajj. 2021. Empathetic BERT2BERT conver-\nsational model: Learning Arabic language generation\nwith little data. In Proceedings of the Sixth Arabic\nNatural Language Processing Workshop, pages 164–\n172, Kyiv, Ukraine (Virtual). Association for Compu-\ntational Linguistics.\nTarek Naous, Christian Hokayem, and Hazem Hajj.\n2020. Empathy-driven Arabic conversational chatbot.\nIn Proceedings of the Fifth Arabic Natural Language\nProcessing Workshop, pages 58–68, Barcelona, Spain\n(Online). Association for Computational Linguistics.\nSubhadarshi Panda, Anjali Agrawal, Jeewon Ha, and\nBenjamin Bloch. 2021. Shuffled-token detection for\nrefining pre-trained RoBERTa. In Proceedings of\nthe 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nStudent Research Workshop, pages 88–93, Online.\nAssociation for Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311–318.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, et al. 2019. Pytorch: An imperative style,\nhigh-performance deep learning library. Advances\nin neural information processing systems, 32:8026–\n8037.\nJoelle Pineau. 2020. Machine learning reproducibil-\nity checklist v2.0. https://www.cs.mcgill.ca/\n~jpineau/ReproducibilityChecklist.pdf.\nJoelle Pineau, Philippe Vincent-Lamarre, Koustuv\nSinha, Vincent Larivière, Alina Beygelzimer, Flo-\nrence d’Alché Buc, Emily Fox, and Hugo Larochelle.\n2021. Improving reproducibility in machine learning\nresearch: a report from the neurips 2019 reproducibil-\nity program. Journal of Machine Learning Research,\n22.\nYuval Pinter, Amanda Stent, Mark Dredze, and Jacob\nEisenstein. 2021. Learning to look inside: Augment-\ning token-based encoders with character-level infor-\nmation.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nOlga Uryupina, and Yuchen Zhang. 2012. Conll-\n2012 shared task: Modeling multilingual unrestricted\ncoreference in ontonotes. In Joint Conference on\nEMNLP and CoNLL-Shared Task, pages 1–40.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nHannah Rashkin, Eric Michael Smith, Margaret Li, and\nY-Lan Boureau. 2019. Towards empathetic open-\ndomain conversation models: A new benchmark and\ndataset. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 5370–5381.\nAli Safaya, Moutasem Abdullatif, and Deniz Yuret.\n2020. Kuisail at semeval-2020 task 12: Bert-cnn\nfor offensive speech identification in social media. In\nProceedings of the Fourteenth Workshop on Semantic\nEvaluation, pages 2054–2059.\nAnanya B. Sai, Akash Kumar Mohankumar, and\nMitesh M. Khapra. 2022. A survey of evaluation\nmetrics used for nlg systems. ACM Comput. Surv.,\n55(2).\nHaitham Seelawi, Ibraheem Tuffaha, Mahmoud Gzawi,\nWael Farhan, Bashar Talafha, Riham Badawi, Zyad\nSober, Oday Al-Dweik, Abed Alhakim Freihat, and\nHussein Al-Natsheh. 2021. Alue: Arabic language\nunderstanding evaluation. In Proceedings of the\nSixth Arabic Natural Language Processing Workshop,\npages 173–184.\nAlexander Sergeev and Mike Del Balso. 2018. Horovod:\nfast and easy distributed deep learning in tensorflow.\narXiv preprint arXiv:1802.05799.\nTatiana Shavrina, Alena Fenogenova, Anton Emelyanov,\nDenis Shevelev, Ekaterina Artemova, Valentin Ma-\nlykh, Vladislav Mikhailov, Maria Tikhonova, Andrey\nChertok, and Andrey Evlampiev. 2020. Russiansu-\nperglue: A russian language understanding evalua-\ntion benchmark. arXiv preprint arXiv:2010.15925.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\n3146\nPedro Javier Ortiz Suárez, Benoît Sagot, and Laurent\nRomary. 2019. Asynchronous pipeline for process-\ning huge corpora on medium to low resource infras-\ntructures. In 7th Workshop on the Challenges in the\nManagement of Large Corpora (CMLC-7). Leibniz-\nInstitut für Deutsche Sprache.\nYu Sun, Shuohuan Wang, Shikun Feng, Siyu Ding,\nChao Pang, Junyuan Shang, Jiaxiang Liu, Xuyi Chen,\nYanbin Zhao, Yuxiang Lu, et al. 2021. Ernie 3.0:\nLarge-scale knowledge enhanced pre-training for lan-\nguage understanding and generation. arXiv preprint\narXiv:2107.02137.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019. Ernie: Enhanced represen-\ntation through knowledge integration. arXiv preprint\narXiv:1904.09223.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Hao\nTian, Hua Wu, and Haifeng Wang. 2020. Ernie 2.0:\nA continual pre-training framework for language un-\nderstanding. In Proceedings of the AAAI Conference\non Artificial Intelligence, pages 8968–8975.\nBashar Talafha, Mohammad Ali, Muhy Eddin Za’ter,\nHaitham Seelawi, Ibraheem Tuffaha, Mostafa Samir,\nWael Farhan, and Hussein Al-Natsheh. 2020. Multi-\ndialect arabic bert for country-level dialect identifi-\ncation. In Proceedings of the Fifth Arabic Natural\nLanguage Processing Workshop, pages 111–118.\nLewis Tunstall, Leandro von Werra, and Thomas Wolf.\n2022. Natural Language Processing with Transform-\ners. \" O’Reilly Media, Inc.\".\nMatej Ulˇcar and Marko Robnik-Šikonja. 2020. Finest\nbert and crosloengual bert: less is more in multilin-\ngual models. arXiv preprint arXiv:2006.07890.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, pages 5797–5808, Florence, Italy.\nAssociation for Computational Linguistics.\nAlex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-\npreet Singh, Julian Michael, Felix Hill, Omer Levy,\nand Samuel R Bowman. 2019. SuperGLUE: A stick-\nier benchmark for general-purpose language under-\nstanding systems. arXiv preprint arXiv:1905.00537.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In Proceedings of\nthe 2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP, pages\n353–355.\nJunqiu Wei, Qun Liu, Yinpeng Guo, and Xin Jiang.\n2021. Training multilingual pre-trained language\nmodel with byte-level subwords. arXiv preprint\narXiv:2101.09469.\nJunqiu Wei, Xiaozhe Ren, Xiaoguang Li, Wenyong\nHuang, Yi Liao, Yasheng Wang, Jiashu Lin, Xin\nJiang, Xiao Chen, and Qun Liu. 2019. Nezha: Neural\ncontextualized representation for chinese language\nunderstanding. arXiv preprint arXiv:1909.00204.\nBryan Wilie, Karissa Vincentio, Genta Indra Winata,\nSamuel Cahyawijaya, X. Li, Zhi Yuan Lim, S. Sole-\nman, R. Mahendra, Pascale Fung, Syafri Bahar, and\nA. Purwarianti. 2020. Indonlu: Benchmark and re-\nsources for evaluating indonesian natural language\nunderstanding. In Proceedings of the 1st Conference\nof the Asia-Pacific Chapter of the Association for\nComputational Linguistics and the 10th International\nJoint Conference on Natural Language Processing.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le,\nMohammad Norouzi, Wolfgang Macherey, Maxim\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, et al.\n2016. Google’s neural machine translation system:\nBridging the gap between human and machine trans-\nlation. arXiv preprint arXiv:1609.08144.\nLiang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie\nCao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu,\nCong Yu, et al. 2020. Clue: A chinese language\nunderstanding evaluation benchmark. arXiv preprint\narXiv:2004.05986.\nLiang Xu, Xiaojing Lu, Chenyang Yuan, Xuanwei\nZhang, Hu Yuan, Huilin Xu, Guoao Wei, Xiang\nPan, and Hai Hu. 2021. Fewclue: A chinese few-\nshot learning evaluation benchmark. arXiv preprint\narXiv:2107.07498.\nWei Zeng, Xiaozhe Ren, Teng Su, Hui Wang, Yi Liao,\nZhiwei Wang, Xin Jiang, ZhenZhang Yang, Kaisheng\nWang, Xiaoda Zhang, et al. 2021. Pangu-α: Large-\nscale autoregressive pretrained chinese language\nmodels with auto-parallel computation. arXiv\npreprint arXiv:2104.12369.\nImad Zeroual, Dirk Goldhahn, Thomas Eckart, and Ab-\ndelhak Lakhouaja. 2019. Osian: Open source interna-\ntional arabic news corpus-preparation and integration\ninto the clarin-infrastructure. In Proceedings of the\nFourth Arabic Natural Language Processing Work-\nshop, pages 175–182.\n3147\nA Pretraining Details\nA.1 Filtering Heuristics\n1. Remove sentences with HTML or Javascript\ncode (Raffel et al., 2019).\n2. Remove sentences if it has less than 70% Ara-\nbic characters.\n3. Remove sentences with less than 8 words.\n4. Remove sentences with more than 3 succes-\nsive punctuation marks (excluding dot).\n5. Remove documents with less than 64 words.\n6. Remove long spans of non-Arabic text (mostly\nEnglish) inside a sentence. We observe that\nmost of these sentences were gibberish/noisy\ntext and not related to the original content.\n7. Represent each sentence by the concatenation\nof the first and last 3 words. We only con-\nsider words that did not include any digits and\nwere longer than 3 characters. Then, we de-\nduplicate the corpus by only keeping the first\noccurrence of sentences with the same key.\n8. Discard a document if more than 30% of\nits sentences are removed in our filtering\npipeline.\nA.2 BERT-style Models\nFor tokenization, we use the byte-level Byte Pair\nEncoding (BBPE) (Wei et al., 2021) training\nmethod which considers the text as a byte se-\nquence. This method improves the learning of\nthe representations of rare words and eliminates\nthe out-of-vocabulary problem. We use a vocab-\nulary size of 64K which is comparable to that\nof AraBERT, twice the size of Arabic-BERT and\nCAMeLBERT, and 36% smaller than ARBERT\nand MARBERT. Our JABER and SABER models\nuse the same architecture as that of BERT-base and\nBERT-large (Devlin et al., 2018) respectively. The\nformer is a stack of 12 Transformer-encoder layers\n(768 hidden units) while the latter consists of 24\nTransformer-encoder layers (1024 hidden units).\nFor Char-JABER, we first randomly initialize a\ncharacter embedding lookup table with a vocab size\nof 160 characters (induced from the pre-training\ncorpus) and a 768 hidden size. Each word is split\ninto a sequence of characters with a maximum char-\nacter sequence length of 10. We use two 1-D CNN\nlayers with each layer having a filter size of 348\nand a sliding window of size 3. Note that we ap-\nply a maxpooling layer with a window size of 5\nafter the first CNN layer. After the second CNN\nlayer, a linear layer is used to map the final rep-\nresentation to the 768 hidden size. Although this\narchitecture adds an additional 700K parameters to\nChar-JABER, this has a negligible computational\noverhead on JABER.\nFollowing Devlin et al. (2018), we pre-train our\nmodels on two unsupervised tasks: Masked Lan-\nguage Modelling (MLM) and Next Sentence Pre-\ndiction (NSP). Specifically for MLM, we use whole\nword masking with a probability of 15%. The orig-\ninal tokens are replaced 80% of the time with the\nspecial [MASK] token, 10% of the time by a ran-\ndom token, and remains unchanged 10% of the\ntime. We choose a duplication factor of 3: each\ninput sequence generates 3 random sets of masked\ntokens.\nWe pre-train our JABER and SABER models on\n16 servers for 15 and 5 epochs respectively. Each\nserver constitutes 8 NVIDIA Tesla V100 GPUs\nwith 32GB of memory. The distributed training is\nachieved through Horovod (Sergeev and Del Balso,\n2018) with full precision. We use the AdamW\n(Loshchilov and Hutter, 2017) optimizer with a\nlearning rate decay setting the initial learning rate\nto 1e-4 with 10,000 warm-up steps. We train with\na maximum sequence length of 128, and set the\nper-GPU batch size to 64 for JABER and 32 for\nSABER. It takes approximately 16 and 32 hours\nto conclude one epoch for JABER and SABER\nrespectively. Finally, the pre-training setting of our\nChar-JABER model is identical to that of JABER\nwith the exception of using a smaller initial learning\nrate of 5e-5.\nA.3 Encoder-Decoder Models\nOur text-to-text Transformer models AT5B and\nAT5S use the same encoder-decoder architecture as\nT5-base and T5-small (Raffel et al., 2019) respec-\ntively. The encoder and decoder components of\nT5-base have similar configuration as that of BERT-\nbase (12 layers) while T5-small is a smaller model\nwith only 6 layers and 8-headed attention. We\nuse the self-supervised denoising objective (Raf-\nfel et al., 2019) to pre-train our models. Specif-\nically, 15% of tokens are randomly dropped-out\nfrom the input and all consecutive spans of such\ntokens are replaced by a single sentinel token. The\n3148\nexpected output is a sequence of these dropped-out\ntokens separated by the corresponding sentinel to-\nken. We train our T5-style models using the same\nvocabulary and pre-training corpus as that of our\nBERT-style models.\nThe models are pre-trained on 64 GPU clusters\nfor 200k steps. The pre-training code is based on\nthe PyTorch (Paszke et al., 2019) version of the\nTransformers library (Wolf et al., 2020). The dis-\ntributed training is achieved by PyTorch’s native\ndistributed training capabilities. We use the Adafac-\ntor optimizer (Shazeer and Stern, 2018) with an\ninitial learning rate of 1 and inverse square-root\ndecay until the end of pre-training.\nFor both AT5S and AT5B, the maximum se-\nquence length is set to 512 for the encoder and\n114 for the decoder. We use a per-GPU batch size\nof 56 and 16 for AT5S and AT5B respectively (the\nmaximum batch size that can fit on a single GPU).\nIt is important to note that most of our implemen-\ntation choices (learning rate, optimizer, etc.) are\nadopted from Raffel et al. (2019) and Nagoudi et al.\n(2022). We only differ from AraT5 through the use\nof a different pre-training corpus and vocabulary.\nB Fine-tuning Details\nB.1 Generative Tasks Datasets\nWhile there is no equivalent for ALUE for gen-\nerative tasks, Nagoudi et al. (2022) recently in-\ntroduced the ARGEN benchmark for Arabic natu-\nral language generation composed of 7 tasks and\n19 datasets. Besides the lack of a public leader-\nboard and private test sets, there are certain issues\nwith this benchmark. Some datasets are not avail-\nable publicly (e.g. ARGEN NTG ), and in some\ncases, the exact data split is not made public (e.g.\nARGENTS ). Therefore, we only consider three\nARGEN tasks in our evaluation: Question Gener-\nation (QG), Question Answering (QA), and Text\nSummarization (TS). We did not include any tasks\nthat involved non-Arabic text (e.g. translation)\nsince we restrict the scope of this work to a mono-\nlingual setting.\nFurthermore, we also evaluate our models on the\nEMpathetic Dialogues (EMD) dataset (Naous et al.,\n2020), which is an Arabic conversational dataset of\nempathetic conversations curated by translating its\nEnglish counterpart (Rashkin et al., 2019). Table 11\nshows the number of instances in the train/dev/test\nsplits for each dataset. The data collection process\nand evaluation metrics are adopted from (Nagoudi\net al., 2022; Naous et al., 2021). Specifically, we\nuse ROUGE (Lin, 2004) to evaluate models on the\nTS task and BLEU (Papineni et al., 2002) for QA,\nQG and EMD tasks.\nTask |Train| |Dev| |Test|\nTS 23.4k 2.9k 2.9k/153\nEMD 19.5k 2.8K 2.5k\nQG/QA 101.6k 517 11.6k\nTable 11: Train/Dev/test sizes of the datasets used to\nevaluate encoder-decoder models. Note that the test\nset for TS consists of 2.9K articles from WikiLingua\n(Ladhak et al., 2020) and 153 articles from Essex Arabic\nSummaries Corpus (EASC) (El-Haj et al., 2010).\nWe adopt the generative format for QA where\nthe input is a pair of passage and question text,\nand the model is expected to generate the answer.\nFollowing Nagoudi et al. (2022), we use the same\ndataset for QG as QA except that the input now is a\npair of passage and answer text, and the model must\ngenerate the corresponding question. Note that for\nthe summarization task, Nagoudi et al. (2022) did\nnot publish the exact split they used on WikiLingua\n(Ladhak et al., 2020). We create our own splits by\nfirst randomly shuffling the dataset (with seed =\n42) and then splitting with the same proportions of\n80% train, 10% dev and 10% test. We will make\nthe code publicly available to reproduce our splits\nand empirical results.\nIt is important to mention that the performance\nscores obtained from our re-implementations on\nTS and EMD tasks are significantly lower than the\noriginal scores reported in (Nagoudi et al., 2022)\nand (Naous et al., 2021). This is due to errors in\nthe original implementations. For TS, we found\na major error in the calculation of the ROUGE\nscore as the ROUGE tool used by the authors was\nincompatible with Arabic. For EMD, we found the\noriginal BLEU scores to be inflated as the authors\ncompute it on segmented text and not at the word-\nlevel (after de-segmentation).\n3149\nFigure 1: Screenshot of ALUE leaderboard as by 13/10/2022. Red buttons indicate our private submission baselines\nwhich are not visible to the public.\nModel QA QG EMD TS\nAraB2B\nbatch size 16 16 32 -\nhidden dropout 0.1 0.2 0.1 -\nlearning rate 1e-03 1e-03 1e-03 -\nAraT5-base\nbatch size 32 8 8 4\nhidden dropout 0.2 0.2 0.1 0.1\nlearning rate 1e-03 1e-03 1e-03 1e-03\nAT5S\nbatch size 32 16 32 4\nhidden dropout 0.1 0.2 0.1 0.1\nlearning rate 1e-03 1e-03 1e-03 1e-03\nAT5B\nbatch size 16 32 16 4\nhidden dropout 0.1 0.1 0.1 0.2\nlearning rate 1e-03 1e-03 1e-03 1e-03\nTable 12: Best hyperparameters for Arabic encoder-\ndecoder models on the generative tasks.\n3150\nModel MQ2Q MDD SVREG SEC FID OOLD XNLI OHSD\nArabic-BERT\nbatch size 64 16 16 16 32 32 64 16\nhidden dropout 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\nlearning rate 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05\nAraBERT\nbatch size 128 32 8 8 8 32 32 16\nhidden dropout 0.1 0.1 0.2 0.1 0.1 0.1 0.3 0.1\nlearning rate 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05\nCAMeLBERT\nbatch size 16 8 8 32 8 128 32 8\nhidden dropout 0.2 0.2 0.2 0.1 0.2 0.1 0.1 0.1\nlearning rate 5e-05 2e-05 2e-05 5e-05 2e-05 2e-05 2e-05 2e-05\nARBERT\nbatch size 64 16 32 8 32 128 32 32\nhidden dropout 0.1 0.1 0.3 0.3 0.1 0.1 0.1 0.3\nlearning rate 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 7e-06\nMARBERT\nbatch size 64 64 16 8 64 64 64 64\nhidden dropout 0.3 0.2 0.1 0.3 0.1 0.2 0.2 0.1\nlearning rate 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05\nJABER\nbatch size 64 32 8 16 32 128 16 32\nhidden dropout 0.3 0.2 0.1 0.1 0.1 0.2 0.1 0.3\nlearning rate 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 2e-05 7e-06\nChar-JABER\nbatch size 64 32 32 16 8 32 64 16\nhidden dropout 0.1 0.2 0.1 0.2 0.2 0.2 0.2 0.1\nlearning rate 7e-06 2e-05 2e-05 2e-05 2e-05 7e-06 2e-05 7e-06\nSABER\nbatch size 32 32 8 8 32 32 32 32\nhidden dropout 0.1 0.1 0.2 0.2 0.3 0.2 0.2 0.1\nlearning rate 7e-06 2e-05 7e-06 2e-05 2e-05 7e-06 7e-06 7e-06\nAT5S\nbatch size 16 32 8 16 16 16 8 32\nhidden dropout 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\nlearning rate 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03\nAT5B\nbatch size 8 16 16 16 8 16 8 64\nhidden dropout 0.2 0.2 0.1 0.1 0.1 0.1 0.1 0.1\nlearning rate 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03\nAraT5-base\nbatch size 64 64 16 64 32 64 32 8\nhidden dropout 0.1 0.1 0.1 0.1 0.1 0.1 0.1 0.1\nlearning rate 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03 1e-03\nTable 13: Best Hyperparameters for Arabic BERT-based and T5-based models on all ALUE tasks.\n3151",
  "topic": "Arabic",
  "concepts": [
    {
      "name": "Arabic",
      "score": 0.6904650926589966
    },
    {
      "name": "Natural language processing",
      "score": 0.6666154861450195
    },
    {
      "name": "Computer science",
      "score": 0.6183735728263855
    },
    {
      "name": "Artificial intelligence",
      "score": 0.548993706703186
    },
    {
      "name": "Linguistics",
      "score": 0.5472308397293091
    },
    {
      "name": "Natural language",
      "score": 0.5010144710540771
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.43827003240585327
    },
    {
      "name": "Philosophy",
      "score": 0.25428664684295654
    },
    {
      "name": "History",
      "score": 0.19402220845222473
    },
    {
      "name": "Archaeology",
      "score": 0.07607772946357727
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210160618",
      "name": "Huawei Technologies (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I70931966",
      "name": "Université de Montréal",
      "country": "CA"
    }
  ],
  "cited_by": 14
}