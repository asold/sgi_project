{
  "title": "Benchmarking large language models for biomedical natural language processing applications and recommendations",
  "url": "https://openalex.org/W4409190866",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2115254388",
      "name": "Qing-Yu Chen",
      "affiliations": [
        "United States National Library of Medicine",
        "Yale University",
        "National Institutes of Health"
      ]
    },
    {
      "id": "https://openalex.org/A2096506771",
      "name": "Yan Hu",
      "affiliations": [
        "The University of Texas Health Science Center",
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2952239724",
      "name": "Xue-Qing Peng",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2126157100",
      "name": "Qianqian Xie",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2091663374",
      "name": "Qiao Jin",
      "affiliations": [
        "National Institutes of Health",
        "United States National Library of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A3134629398",
      "name": "Aidan Gilson",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A5119192569",
      "name": "Maxwell B. Singer",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A4366102695",
      "name": "Xuguang Ai",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A4221678393",
      "name": "Po-Ting Lai",
      "affiliations": [
        "United States National Library of Medicine",
        "National Institutes of Health"
      ]
    },
    {
      "id": "https://openalex.org/A2121606350",
      "name": "Zhizheng Wang",
      "affiliations": [
        "United States National Library of Medicine",
        "National Institutes of Health"
      ]
    },
    {
      "id": "https://openalex.org/A4327705837",
      "name": "Vipina K Keloth",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2189196551",
      "name": "Kalpana Raja",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2098794577",
      "name": "Jimin Huang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2097683557",
      "name": "Huan He",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2480796555",
      "name": "Fongci Lin",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2343127410",
      "name": "Jingcheng Du",
      "affiliations": [
        "The University of Texas Health Science Center",
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "University of Minnesota System",
        "University of Minnesota Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2167803262",
      "name": "W Jim Zheng",
      "affiliations": [
        "The University of Texas Health Science Center",
        "The University of Texas Health Science Center at Houston"
      ]
    },
    {
      "id": "https://openalex.org/A2066736790",
      "name": "Ron A. Adelman",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2150237561",
      "name": "Zhiyong Lu",
      "affiliations": [
        "National Institutes of Health",
        "United States National Library of Medicine"
      ]
    },
    {
      "id": "https://openalex.org/A2105258892",
      "name": "Hua Xu",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2115254388",
      "name": "Qing-Yu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096506771",
      "name": "Yan Hu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2952239724",
      "name": "Xue-Qing Peng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2126157100",
      "name": "Qianqian Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2091663374",
      "name": "Qiao Jin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3134629398",
      "name": "Aidan Gilson",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5119192569",
      "name": "Maxwell B. Singer",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4366102695",
      "name": "Xuguang Ai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221678393",
      "name": "Po-Ting Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121606350",
      "name": "Zhizheng Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4327705837",
      "name": "Vipina K Keloth",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2189196551",
      "name": "Kalpana Raja",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098794577",
      "name": "Jimin Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097683557",
      "name": "Huan He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2480796555",
      "name": "Fongci Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2343127410",
      "name": "Jingcheng Du",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2167803262",
      "name": "W Jim Zheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2066736790",
      "name": "Ron A. Adelman",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2150237561",
      "name": "Zhiyong Lu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105258892",
      "name": "Hua Xu",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4308834638",
    "https://openalex.org/W4308616343",
    "https://openalex.org/W4310775156",
    "https://openalex.org/W3017463390",
    "https://openalex.org/W2005958938",
    "https://openalex.org/W4296777576",
    "https://openalex.org/W2944400536",
    "https://openalex.org/W2896016608",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W6689189152",
    "https://openalex.org/W4297253404",
    "https://openalex.org/W4223492536",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W2940542551",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4385682300",
    "https://openalex.org/W4389524025",
    "https://openalex.org/W4381587418",
    "https://openalex.org/W4395703766",
    "https://openalex.org/W4391301614",
    "https://openalex.org/W2735784619",
    "https://openalex.org/W4365511667",
    "https://openalex.org/W4200445293",
    "https://openalex.org/W4394782456",
    "https://openalex.org/W4393157213",
    "https://openalex.org/W6930615351",
    "https://openalex.org/W4390573130",
    "https://openalex.org/W4385573954",
    "https://openalex.org/W4402528490",
    "https://openalex.org/W3198080531",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4206445734",
    "https://openalex.org/W4385381606",
    "https://openalex.org/W4224325974",
    "https://openalex.org/W2914171828",
    "https://openalex.org/W4244393794",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W3035320939",
    "https://openalex.org/W2900758626",
    "https://openalex.org/W2174775663",
    "https://openalex.org/W3106969213",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W2963926728",
    "https://openalex.org/W3166664235",
    "https://openalex.org/W4385567129",
    "https://openalex.org/W4402671278",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W3185341429",
    "https://openalex.org/W4391292768",
    "https://openalex.org/W4391644728",
    "https://openalex.org/W4385573087",
    "https://openalex.org/W4393094733",
    "https://openalex.org/W3003257820",
    "https://openalex.org/W4394581301",
    "https://openalex.org/W4366595110",
    "https://openalex.org/W4294974822",
    "https://openalex.org/W4205977363",
    "https://openalex.org/W4224315024",
    "https://openalex.org/W4307003748",
    "https://openalex.org/W4389519915",
    "https://openalex.org/W2335791510",
    "https://openalex.org/W4392297694",
    "https://openalex.org/W4283830961",
    "https://openalex.org/W4295077729",
    "https://openalex.org/W2998813011",
    "https://openalex.org/W3103145119",
    "https://openalex.org/W4386530347",
    "https://openalex.org/W3104059174",
    "https://openalex.org/W4391126287"
  ],
  "abstract": null,
  "full_text": "Article https://doi.org/10.1038/s41467-025-56989-2\nBenchmarking large language models for\nbiomedical natural language processing\napplications and recommendations\nQingyu Chen 1,2,Y a nH u3, Xueqing Peng1, Qianqian Xie 1, Qiao Jin 2,\nAidan Gilson4,M a x w e l lB .S i n g e r4,X u g u a n gA i1,P o - T i n gL a i2, Zhizheng Wang2,\nVipina K. Keloth1,K a l p a n aR a j a1, Jimin Huang1,H u a nH e1, Fongci Lin1,\nJingcheng Du 3,R u iZ h a n g5,6, W. Jim Zheng3, Ron A. Adelman4,\nZhiyong Lu 2,7 &H u aX u1,7\nThe rapid growth of biomedical literature poses challenges for manual\nknowledge curation and synthesis. Biomedical Natural Language Processing\n(BioNLP) automates the process. While Large Language Models (LLMs) have\nshown promise in general domains, their effectiveness in BioNLP tasks remains\nunclear due to limited benchmarks and practical guidelines. We perform a\nsystematic evaluation of four LLMs— GPT and LLaMA representatives— on\n12 BioNLP benchmarks across six applications. We compare their zero-shot,\nfew-shot, andﬁne-tuning performance with the traditionalﬁne-tuning of BERT\nor BART models. We examine inconsistencies, missing information, halluci-\nnations, and perform cost analysis. Here, we show that traditionalﬁne-tuning\noutperforms zero- or few-shot LLMs inmost tasks. However, closed-source\nLLMs like GPT-4 excel in reasoning-related tasks such as medical question\nanswering. Open-source LLMs still requireﬁne-tuning to close performance\ngaps. Weﬁnd issues like missing information and hallucinations in LLM out-\nputs. These results offer practical insights for applying LLMs in BioNLP.\nBiomedical literature presents direct obstacles to curation, inter-\npretation, and knowledge discovery due to its vast volume and\ndomain-speciﬁc challenges. PubMed alone sees an increase of\napproximately 5000 articles every day, totaling over 36 million as of\nMarch 2024\n1.I ns p e c i a l i z e dﬁelds such as COVID-19, roughly 10,000\ndedicated articles are added each month, bringing the total to over 0.4\nmillion as of March 2024\n2. In addition to volume, the biomedical\ndomain also poses challenges with ambiguous language. For example,\na single entity such as Long COVID can be referred to using 763 dif-\nferent terms\n3. Additionally, the same term can describe different\nentities, as seen with the termAP2, which can refer to a gene, a\nchemical, or a cell line4. Beyond entities, identifying novel biomedical\nrelations and capturing semantics in biomedical literature present\nfurther challenges5,6.\nTo overcome these challenges, biomedical natural language pro-\ncessing (BioNLP) techniques are used to assist with manual curation,\ninterpretation, and knowledge discovery. Biomedical language models\nare considered as the backbone of BioNLP methods; they leverage\nmassive amounts of biomedical literature and capture biomedical\nsemantic representations in an unsupervised or self-supervised man-\nner. Early biomedical language models are non-contextual embed-\ndings (e.g., word2vec and fastText) that use fully connected neural\nReceived: 17 November 2023\nAccepted: 7 February 2025\nCheck for updates\n1Department of Biomedical Informatics and Data Science, Yale School of Medicine, Yale University, New Haven, CT, USA.2National Library of Medicine,\nNational Institutes of Health, Bethesda, MD, USA.3McWilliams School of Biomedical Informatics, University of Texas Health Science at Houston, Houston, TX,\nUSA. 4Department of Ophthalmology and Visual Science, Yale School of Medicine, Yale University, New Haven, CT, USA.5Division of Computational Health\nSciences, Department of Surgery, Medical School, University of Minnesota, Minneapolis, MN, USA.6Center for Learning Health System Sciences, University of\nMinnesota, Minneapolis, MN 55455, USA.7These authors contributed equally: Zhiyong Lu, Hua Xu.e-mail: zhiyong.lu@nih.gov; hua.xu@yale.edu\nNature Communications|         (2025) 16:3280 1\n1234567890():,;\n1234567890():,;\nnetworks such as BioWordVec and BioSentVec4,7,8. Since the inception\nof transformers, biomedical language models have adopted their\narchitecture, and can be categorized into (1) encoder-based, masked\nlanguage models using the encoder from the transformer architecture\nsuch as the biomedical bidirectional encoder representations from\ntransformers (BERT) family including BioBERT and PubMedBERT\n9–11,\n(2) decoder-based, generative language models using the decoder\nfrom the transformer architecture such as the generative pre-trained\ntransformer (GPT) family including BioGPT and BioMedLM\n12,13,a n d( 3 )\nencoder-decoder-based, using both encoders and decoders such as\nBioBART and Sciﬁve\n14,15.B i o N L Ps t u d i e sﬁne-tuned those language\nm o d e l sa n dd e m o n s t r a t e dt h a tt h e ya c h i e v e dt h eS O T Ap e r f o r m a n c e\nin various BioNLP applications\n10,16, and those models have been suc-\ncessfully employed in PubMed-scale downstream applications such as\nbiomedical sentence search\n17 and COVID-19 literature mining2.\nRecently, the latest closed-source GPT models, including GPT-3\nand, more notably, GPT-4, have made signiﬁcant strides and garnered\nconsiderable attention from society. A key characteristic of these\nmodels is the exponential growth of their parameters. For instance,\nGPT-3 has ~175 billion parameters, which is hundreds larger than GPT-\n2. Models of this magnitude are commonly referred to as Large Lan-\nguage Models (LLMs)\n18. Moreover, the enhancement of LLMs is\nachieved through reinforcement learning with human feedback,\nthereby aligning text generation with human preferences\n19.F o r\ninstance, GPT-3.5 builds upon the foundation of GPT-3 using reinfor-\ncement learning techniques, resulting in signiﬁcantly improved per-\nformance in natural language understanding\n20. The launch of ChatGPT\n— a chatbot using GPT-3.5 and GPT-4— has marked a milestone in gen-\nerative artiﬁcial intelligence. It has demonstrated strong capabilities in\nthe tasks that its predecessors fail to do; for instance, GPT-4 passed\nover 20 academic and professional exams, including the Uniform Bar\nExam, SAT Evidence-Based Reading & Writing, and Medical Knowledge\nSelf-Assessment Program\n21. The remarkable advancements have\nsparked extensive discussions among society, with excitement and\nconcerns alike. In addition to closed-source LLMs, open-source LLMs,\nsuch as LLaMA\n22 and Mixtral23 have been widely adopted in down-\nstream applications and also used as the basis for continuous pre-\ntraining domain-speciﬁc resources. In the biomedical domain, PMC\nLLaMA (7B and 13B) is one of theﬁrst biomedical domain-speciﬁc LLMs\nthat continuously pre-trained LLaMA on 4.8 M biomedical papers and\n30 K medical textbooks\n24. Meditron (7B and 70B), a more recent bio-\nmedical domain-speciﬁc LLM, employed a similar continuous pre-\ntraining strategy on LLaMA 2.\nPioneering studies have conducted early experiments on LLMs in\nthe biomedical domain and reported encouraging results. For\ninstance, Bubeck et al. studied the ability of GPT-4 in a wide spectrum,\nsuch as coding, mathematics, and interactions with humans. This early\nstudy reported biomedical-related results, indicating that GPT-4\nachieved an accuracy of approximately 80% in the US Medical Licen-\nsing Exam (Step 1, 2, and 3), along with an example of using GPT-4 to\nverify claims in a medical note. Lee et al. also demonstrated use cases\nof GPT-4 for answering medical questions, generating summaries from\npatient reports, assisting clinical decision-making, and creating edu-\ncational materials\n24. Wong et al. conducted a study on GPT-3.5 and\nGPT-4 for end-to-end clinical trial matching, handling complex elig-\nibility criteria, and extracting complex matching logic\n25. Liu et al.\nexplored the performance of GPT-4 on radiology domain-speciﬁcu s e\ncases26. Nori et al. further found that general-domain LLMs with\nadvanced prompt engineering can achieve the highest accuracy in\nmedical question answering withoutﬁne-tuning\n27.R e c e n tr e v i e w sa l s o\nsummarize related studies in detail28–30.\nThese results demonstrate the potential of using LLMs in BioNLP\napplications, particularly when minimal manually curated gold stan-\ndard data is available andﬁne-tuning or retraining for every new task is\nnot required. In the biomedical domain, a primary challenge is the\nlimited availability of labeled datasets, which have a signiﬁcantly lower\nscale than those in the general domain (e.g., a biomedical sentence\nsimilarity dataset only has 100 labeled instances in total\n31)32,33.T h i s\nchallenges theﬁne-tuning approach because (1) modelsﬁne-tuned on\nlimited labeled datasets may not be generalizable, and (2) it becomes\nmore challenging toﬁne-tune the models with a larger size.\nMotivated by the early experiments, it is important to system-\natically assess the effectiveness of LLMs in BioNLP tasks and compre-\nhend their impact on BioNLP method development and downstream\nusers. Table1 provides a detailed comparison of representative studies\nin this context. While our primary focus is on the biomedical domain,\nspeciﬁcally the evaluation of LLMs using biomedical literature, we have\nalso included two representative studies in the clinical domain (eval-\nuating LLMs using clinical records) for reference. There are several\nprimary limitations. First, most evaluation studies primarily assessed\nGPT-3 or GPT-3.5, which may not provide a full spectrum of repre-\nsentative LLMs from different categories. For instance, few studies\nevaluated more advanced closed-source LLMs such as GPT-4, LLM\nrepresentatives from the general domain such as LLaMA\n22,a n db i o -\nmedical domain-speciﬁc LLMs such as PMC-LLaMA34.S e c o n d ,t h e\nexisting studies mostly assessed extraction tasks where the gold\nstandard isﬁxed. Few of these studies evaluated generative tasks such\nas text summarization and text simpliﬁcation where the gold standard\nis free-text. Arguably, existing transformer models have demonstrated\nsatisfactory performance in extractive tasks, while generative tasks\nremain a challenge in terms of achieving similar levels of proﬁciency.\nTherefore, it is imperative to assess how effective LLMs are in the\ncontext of generative tasks in BioNLP, examining whether they can\ncomplement existing models. Third, most existing studies only\nreported quantitative assessments such as the F1-score, with limited\nemphasis on qualitative evaluations. However, conducting qualitative\nevaluations (e.g., assessing the quality of LLM-generated text and\ncategorizing inconsistent or hallucinated responses) to understand of\nthe errors and impacts of LLMs on downstream applications in the\nbiomedical domain are arguably more critical than mere quantitative\nmetrics. For instance, studies on LLMs found a relatively low correla-\ntion between human judgments and automatic measures, such as\nROUGE-L, commonly applied to text summarization tasks in the clin-\nical domain\n35. Finally, it is worth noting that several studies did not\nprovide public access to their associated data or codes. For example,\nfew studies have made the prompts or selected examples for few-shot\nlearning available. This hinders reproducibility and also presents\nchallenges in evaluating new LLMs using the same setting for a fair\ncomparison.\nIn this study, we conducted a comprehensive evaluation of LLMs\nin BioNLP applications to examine their great potentials as well as their\nlimitations and errors. Our study has three main contributions.\nFirst, we performed comprehensive evaluations on four repre-\nsentative LLMs: GPT-3.5 and GPT-4 (representatives from closed-\nsource LLMs), LLaMA 2 (a representative from open-sourced LLMs),\nand PMC LLaMA (a representative from biomedical domain-speciﬁc\nLLMs). We evaluated them on 12 BioNLP datasets across six applica-\ntions: (1) named entity recognition, which extracts biological entities of\ninterest from free-text, (2) relation extraction, which identiﬁes rela-\ntions among entities, (3) multi-label document classiﬁcation, which\ncategorizes documents into broad categories, (4) question answering,\nwhich provides answers to medical questions, (5) text summarization,\nwhich produces a coherent summary of an input text, and (6) text\nsimpliﬁcation, which generates understandable content of an input\ntext. The models were evaluated under four settings: zero-shot, static\nfew-shot, dynamic K-nearest few-shot, andﬁne-tuning where applic-\nable. We compared these models against the state-of-the-art (SOTA)\napproaches that useﬁne-tuned, domain-speciﬁcB E R To rB A R Tm o d -\nels. Both BERT and BART models are well-established in BioNLP\nresearch.\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 2\nTable 1 | A comparison of key elements from representative studies assessing large language models (LLMs) in the biomedical and clinical domains as ofMarch 2024\nStudy Domain Model Tasks Evaluation scope a Evaluation setting Evaluation measures Availability\nExtractive/\nClassiﬁcation\nGenerative Zero/\nFew-shot\nFine-tuningc Quantitative Qualitative d Cost analysis\n80 Clinical T5,\nGPT-3\nClinical language inference,\nRadiology question answering,\nDischarge summary classiﬁcation\nYN Y N Y N N Y\n76 Clinical GPT-3 Clinical sense disambiguation,\nBiomedical evidence extraction,\nCoreference resolution,\nMedication status extraction,\nMedication attribute extraction\nYN Y N Y N N N\n43 Biomedical BERT,\nGPT-3\nNamed entity recognition,\nRelation extraction,\nYN Y N Y N N Y\n44 Biomedical BERT,\nGPT-3.5\nRelation extraction Y N Y N Y N N N\n27 Biomedical Med-PaLM,\nGPT-4\nQuestion answering N Y Y N Y Y N N\n81 Biomedical BERT,\nGPT-3.5,\nGPT-4\nBiomedical reasoning,\nDocument classiﬁcation\nYN Y N Y N N N\n82 Biomedical BERT,\nGPT-3.5\nNamed entity recognition,\nRelation extraction,\nDocument classiﬁcation,\nQuestion answering\nYN Y N Y N N N\nOurs Biomedical BERT, BART,\nLLaMA 2,\nPMC LLaMA,\nGPT-3.5,\nGPT-4\nNamed entity recognition,\nRelation extraction,\nDocument classiﬁcation,\nQuestion answering,\nText summarization,\nText simpliﬁcation\nYY Y Y Y Y Y Y\nThe table categorizes each study by its domain of focus (Biomedical or Clinical), the models evaluated, the evaluation scope including extractive tasks such as named entity recognition (NER) and generative tasks such as text summarization, the evaluation\nmeasures including quantitative evaluation metrics (such as the F1-score), qualitative evaluation metrics (such as the completeness in a scale of 1— 5), and the accessibility of data, prompts, and codes to the public.aExtractive or classiﬁcation: the tasks where the\ngold standard isﬁxed, e.g., relation extraction.bGenerative: text summarization and text simpliﬁcation tasks where the gold standard is free-text.cFine-tuning: an LLM is further tuned on speciﬁc datasets.dQualitative: tasks such as manual validations on the quality\nof LLM-generated text.\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 3\nOur results suggest that SOTA ﬁne-tuning approaches out-\nperformed zero- and few-shot LLMs in most of the BioNLP tasks. These\napproaches achieved a macro-average approximately 15% higher than\nthe best zero- and few-shot LLM performance across 12 benchmarks\n(0.65 vs. 0.51) and over 40% higher in information extraction tasks,\nsuch as relation extraction (0.79 vs. 0.33). However, closed-source\nLLMs such as GPT-3.5 and GPT-4 demonstrated better zero- and few-\nshot performance in reasoning-related tasks such as medical question\nanswering, where they outperformed the SOTAﬁne-tuning approa-\nches. In addition, they exhibited lower-than-SOTA but reasonable\nperformance in generation-related tasks such as text summarization\nand simpliﬁcation, showing competitive accuracy and readability, as\nwell as showing potential in semantic understanding tasks such as\ndocument-level classiﬁcation. Among the LLMs, GPT-4 showed the\noverall highest performance, especially due to its remarkable reason-\ning capability. However, it comes with a trade-off, being 60 to 100\ntimes more expensive than GPT-3.5. In contrast, open-sourced LLMs\nsuch as LLaMA 2 did not demonstrate robust zero- and few-shot per-\nformance– they still requireﬁne-tuning to bridge the performance gap\nfor BioNLP applications.\nSecond, we conducted a thorough manual validation on collec-\ntively over hundreds of thousands of sample outputs from the LLMs.\nFor extraction and classiﬁcation tasks where the gold standard isﬁxed\n(e.g., relation extraction and multi-label document classiﬁcation), we\nexamined (1) missing output, when LLMs fail to provide the requested\noutput, (2) inconsistent output, when LLMs produce different outputs\nfor similar instances, and (3) hallucinated output, when LLMs fail to\naddress the user input and may contain repetitions and misinforma-\ntion in the output\n36. For text summarization tasks, two healthcare\nprofessionals performed manual evaluations assessing Accuracy,\nCompleteness, and Readability. The results revealed prevalent cases of\nmissing, inconsistent, and hallucinated outputs, especially for LLaMA 2\nunder the zero-shot setting. For instance, it had over 102 hallucinated\ncases (32% of the total testing instances) and 69 inconsistent cases\n(22%) for a multi-label document classiﬁcation dataset.\nFinally, we provided recommendations for downstream users on\nthe best practice to use LLMs in BioNLP applications. We also noted\ntwo open problems. First, the current data and evaluation paradigms in\nBioNLP are tailored to supervised methods and may not be fair to\nLLMs. For instance, the results showed that automatic metrics for text\nsummarization may not align with manual evaluations. Also, the\ndatasets that speciﬁcally target tasks where LLMs excel, such as rea-\nsoning, are limited in the biomedical domain. Revisiting data and\nevaluation paradigms in BioNLP are key to maximizing the beneﬁts of\nLLMs in BioNLP applications. Second, addressing errors, missing\ninformation, and inconsistencies is crucial to minimize the risks asso-\nciated with LLMs in biomedical and clinical applications. We strongly\nencourage a community effort toﬁnd better solutions to mitigate\nthese issues.\nWe believe that theﬁndings of this study will be beneﬁcial for\nBioNLP downstream users and will also contribute to further enhan-\ncing the performance of LLMs in BioNLP applications. The established\nbenchmarks and baseline performance could serve as the basis for\nevaluating new LLMs in the biomedical domain. To ensure reprodu-\ncibility and facilitate benchmarking, we have made the relevant data,\nmodels, and results publicly accessible throughhttps://doi.org/10.\n5281/zenodo.14025500\n37.\nResults\nQuantitative evaluations\nTable 2 illustrates the primary evaluation metric results and their\nmacro-averages of the LLMs under zero/few-shot (static one- andﬁve-\nshot) and ﬁne-tuning settings over the 12 datasets. The results on\nspeciﬁc datasets were consistent with those independently reported\nby other studies, such as an accuracy of 0.4462 and 0.7471 on MedQA\nfor GPT-3.5 zero-shot and GPT-4 zero-shot, respectively (0.4988 and\n0.7156 in our study, respectively)38. Similarly, a micro-F1 of 0.6224 and\n0.6720 on HoC and LitCovid for GPT-3.5 zero-shot was reported,\nrespectively (0.6605 and 0.6707 in our study, respectively)\n39.A n\naccuracy of 0.7790 on PubMedQA was also reported for theﬁne-tuned\nPMC LLaMA 13B (combined multiple question answering datasets for\nﬁne-tuning)\n34; our study also reported a similar accuracy of 0.7680\nusing the PubMedQA training set only. We further summarized\ndetailed results in Supplementary Information S2 Quantitative eva-\nluation results, including secondary metric results in S2.2, perfor-\nmance mean, variance, and conﬁdence intervals in S2.3, statistical test\nresults in S2.4, and dynamic K-nearest few-shot results in S2.5.\nSOTA vs. LLMs. The results of SOTAﬁne-tuning approaches for\ncomparison are provided in Table2. Recall that the SOTA approaches\nutilized ﬁne-tuned (domain-speciﬁc) language models. For the\nextractive and classiﬁcation tasks, the SOTA approachesﬁne-tuned\nbiomedical domain-speciﬁc BERT models such as BioBERT and Pub-\nMedBERT. For text summarization and simpliﬁcation tasks, the SOTA\napproachesﬁne-tuned BART models.\nAs demonstrated in Table2,t h eS O T Aﬁne-tuning approaches had\na macro-average of 0.6536 across the 12 datasets, whereas the best\nLLM counterparts were 0.4561, 0.4750, 0.4862, and 0.5131 under zero-\nshot, one-shot,ﬁve-shot, andﬁne-tuning settings, respectively. It out-\nperformed the zero- and few-shot of LLMs in 10 out of the 12 datasets.\nIt had much higher performance especially in information extraction\ntasks. For instance, for NCBI Disease, the SOTA approach achieved an\nentity-level F1-score of 0.9090, whereas the best results of LLMs (GPT-\n4) under zero- and one-shot settings were 30% lower (0.5988). The\nperformance of LLMs is closer under theﬁne-tuning setting, with\nLLaMA 2 13B achieving an entity-level F1-score of 0.8682, but it is still\nlower. Notably, the SOTA ﬁne-tuning approaches are very strong\nbaselines – they were much more sophisticated than simpleﬁne-\ntuning over a foundation model. Continuing with the example of NCBI\nDisease, the SOTAﬁne-tuning approach generated large-scale weak\nlabeled examples and used contrastive learning to learn a general\nrepresentation.\nIn contrast, the LLMs outperformed the SOTA ﬁne-tuning\napproaches in question answering. For MedQA, the SOTA approach\nhad an accuracy of 0.4195. GPT-4 under the zero-shot setting had\nalmost 30% higher accuracy in absolute difference (0.7156), and GPT-\n3.5 also had approximately 8% higher accuracy (0.4988) under the\nzero-shot setting. For PubMedQA, the SOTA approach had an accuracy\nof 0.7340. GPT-4 under the one-shot setting had a similar accuracy\n(0.7100) and showed higher accuracy with more shots (0.7580 under\nthe ﬁve-shot setting), as we will show later. Both LLaMA 2 13B and PMC\nLLaMA 13B also had higher accuracy under theﬁne-tuning setting\n(0.8040 and 0.7680, respectively). In this case, GPT-3.5 did not achieve\nhigher accuracy over the SOTA approach, but it already had a com-\npetitive accuracy (0.6950) under theﬁve-shot setting.\nComparisons among the LLMs. Comparing among the LLMs,\nunder zero/few-shot settings, the results demonstrate that GPT-4\nconsistently had the highest performance. Under the zero-shot setting,\nthe macro-average of GPT-4 was 0.4561, which is approximately 7%\nhigher than GPT-3.5 (0.3814) and almost double than LLaMA 2 13B\n(0.2362). It achieved the highest performance in nine out of the 12\ndatasets, and its performance was also within 3% of the best result for\nthe remaining three datasets. The one-shot andﬁve-shot settings\nshowed very similar patterns.\nIn addition, LLaMA2 13B exhibited substantially lower perfor-\nmance than GPT-3.5 (15% lower and 10% lower) and GPT-4 (22% lower\nand 17% lower) under zero- and one-shot settings. It had up to six times\nlower performance in speciﬁc datasets compared to the best LLM\nr e s u l t s ;f o re x a m p l e ,0 . 1 2 8 6v s .0 . 7 1 0 9f o rH o Cu n d e rt h ez e r o - s h o t\nsetting. These results suggest that LLaMA2 13B still requiresﬁne-tuning\nto achieve similar performance and bridge the performance gap. Fine-\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 4\ntuning improved LLaMA 2 13B’s macro-average from 0.2837 to 0.5131.\nNotably, its performance under theﬁne-tuning setting is slightly higher\nthan the zero- and few-shot performance of GPT-4. Fine-tuning LLaMA\n2 13B generally improved its performance in all tasks except text\nsummarization and text simpliﬁcation. A key reason for its perfor-\nmance limitation is that the datasets have much longer input context\nthan its allowed input tokens (4096) such thatﬁne-tuning did not help\nin this case. This observation also motivates further research efforts on\nextending LLMs’context window\n40,41.\nUnder theﬁne-tuning setting, the results also indicate that PMC\nLLaMA 13B, as a continuously pretrained biomedical domain-speciﬁc\nLLM, did not achieve an overall higher performance than LLaMA 2 13B.\nFine-tuned LLaMA 2 13B had better performance than that of PMC\nLLaMA 13B in 10 out of the 12 datasets. As mentioned, we reproduced\nsimilar results reported in PMC LLaMA study\n34. For instance, it repor-\nted an accuracy of 0.7790 on PubMedQA withﬁne-tuning multiple\nquestion answering datasets together. We got a very similar accuracy\nof 0.7680 whenﬁne-tuning PMC LLaMA 13B on the PubMedQA dataset\nonly. However, we also found that directlyﬁne-tuning of LLaMA 2 13B\nusing the exact same setting resulted in better or at least similar\nperformance.\nFew-shot and cost analysis. Figure 1 further illustrates the perfor-\nmance of the dynamic K-nearest few-shot and the associated cost with\nthe increasing number of shots. The detailed results are also provided\nin Supplementary Information S2. Dynamic K-nearest few-shot was\nconducted for K values of one, two, andﬁve. For comparison, we also\nprovided the zero-shot and static one-shot performance in theﬁgure.\nThe results suggest that dynamic K-nearest few-shot is most effective\nfor multi-label document classiﬁcation and question answering. For\ninstance, for the LitCovid dataset, GPT-4 had a macro-F1 of 0.5901\nunder the static one-shot setting; in contrast, its macro-F1 under\nTable 2 | Quantitative evaluations of the LLMs on the 12 benchmarks under zero/few-shot (including static one- andﬁve-shot))\nand ﬁne-tuned settings\nSOTA results\nbefore the\nLLMs\n(Foundation\nmodel)\nZero/Few-shot Fine-tuned\nZero-shot One-shot Five-shot\nGPT-\n3.5\nGPT-4 LLaMA\n21 3 B\nGPT-\n3.5\nGPT-4 LLaMA\n2\n13B\nGPT-\n3.5\nGPT-4 LLaMA\n2\n13B\nb\nLLaMA 2\n13B\nPMC\nLLaMA\n13B\nNamed entity recognition\nBC5CDR-\nchemical\nEntity F1 0.9500 83\n(PubMedBERT)\n0.6274 0.7993 0.3944 0.7133 0.8327* 0.6276 0.7228 0.7979 0.5530 0.9149 0.9063\nNCBI Disease Entity F1 0.9090 83\n(PubMedBERT)\n0.4060 0.5827 0.2211 0.4817 0.5988 0.3811 0.4309 0.6389* 0.4847 0.8682* 0.8353\nRelation extraction\nChemProt Macro F1 0.7344 84\n(BioBERT)\n0.1345 0.3250 0.1392 0.1280 0.3391 0.0718 0.1758 0.3756 0.0967 0.4612* 0.3111\nDDI2013 Macro F1 0.7919 85\n(BioBERT)\n0.2004 0.2968 0.1305 0.2126 0.3312 0.1779 0.1706 0.3276 0.1663 0.6218 0.5700\nMulti-label document classiﬁcation\nHoC Macro F1 0.8882 86\n(BioBERT)\n0.6722 0.7109 0.1285 0.6671 0.7093 0.3072 0.6994 0.7099 0.1797 0.6957* 0.4221\nLitCovid Macro F1 0.8921 86\n(BioBERT)\n0.5967 0.5883 0.3825 0.6009 0.5901 0.4808 0.6179 0.6077 0.3305 0.5725* 0.4273\nQuestion answering\nMedQA (5-\nOption)\nAccuracy 0.4195 a87\n(BioLinkBERT)\n0.4988 0.7156 0.2522 0.5161 0.7439 0.2899 0.5208 0.7651* 0.3504 0.4462* 0.3975\nPubMedQA Accuracy 0.7340 87\n(BioLinkBERT)\n0.6560 0.6280 0.5520 0.4600 0.7100 0.2660 0.6920 0.7580* 0.6000 0.8040* 0.7680\nText summarization\nPubMed Rouge-L 0.4316 42\n(BART)\n0.2274 0.2419 0.1190 0.2351 0.2427 0.0989 0.2423 0.2444 0.1629 0.1857* 0.1684\nMS^2 Rouge-L 0.2080 50\n(BART)\n0.0889 0.1224 0.0948 0.1132 0.1248 0.0320 0.1013 0.1218 0.1205 0.0934* 0.0059\nText simpliﬁcation\nCochrane\nPLS\nRouge-L 0.4476 88\n(BART)\n0.2365 0.2375 0.2081 0.2447 0.2385 0.2207 0.2470 0.2469 0.2283 0.2355 0.2370\nPLOS Rouge-L 0.4368 70\n(BART)\n0.2323 0.2253 0.2121 0.2449* 0.2386 0.1836 0.2416 0.2409 0.1656 0.2583 0.2577\nMacro-\naverage\n0.6536 0.3814 0.4561 0.2362 0.3848 0.4750 0.2614 0.4052 0.4862 0.2866 0.5131 0.4422\nThe primary metric results are reported. State-of-the-art (SOTA) results, representing the reported best performance of studies usingﬁne-tuned (domain-speciﬁc) language models before the LLMs\nand their backbone models, are also provided. The SOTA results are directly extracted from the studies.athe study reported accuracy on MedQA (4-option); we applied the released model for\ninference on MedQA (5-option).bThe inputs for LLaMA 2 were truncated for question answering, text summarization, and text simpliﬁcation tasks under theﬁve-shot setting due to its input token\nlength limit detailed in Supplementary Information S1 Prompt engineering. The highest performance under either zero/few-shot orﬁne-tuned settings is marked in bold. For instance, GPT-4 one-shot\nachieved the highest performance under the zero/few-shot setting, and LLaMA 2 13Bﬁne-tuned achieved the highest performance under theﬁne-tuned setting in the BC5CDR-chemical dataset. A\ntwo-tailed Wilcoxon rank-sum test with bootstrapping, using a subsample size of 30 and 100 repetitions at a 95% conﬁdence interval, was conducted for both zero/few-shot andﬁne-tuning settings.\nAn asterisk (*) indicates if the P-value of the best performance is less than 0.05 for all the models under either the zero/few-shot orﬁne-tuned settings. Continuing with the BC5CDR-chemical\nexample, the P-value of GPT-4 one-shot was less than 0.05 for all others under the zero/few-shot setting, whereas LLaMA 2 13Bﬁne-tuned was not under theﬁne-tuned setting. Detailed results,\nincluding performance mean and variance, statistical test results, dynamic few-shot, and secondary metrics are provided in Supplementary Information S2 Quantitative evaluation results S2.4.\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 5\nFig. 1 | Dynamic K-nearest few-shot results (K = 1, 2, and 5) shown in line charts,\nwith associated costs (dollars per 100 instances) depicted in bar charts for each\nbenchmark.The input and output types for each benchmark are displayed at the\nbottom of each subplot. Detailed methods for the few-shot and cost analysis are\nsummarized in the Data and Methods section. Dynamic K-nearest few-shot\ninvolves selecting the K closest training instances as examples for each testing\ninstance. Additionally, the performance of static one-shot (using the same one-\nshot example for each testing instance) is shown as a dashed horizontal line for\ncomparison. Detailed performance in digits is also provided in Supplementary\nInformation S2.\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 6\ndynamic one-nearest shot was 0.6500 and further increased to 0.7055\nwith ﬁve-nearest shots. Similarly, GPT-3.5 exhibited improvements,\nwith its macro-F1 under the static one-shot setting at 0.6009, com-\npared to 0.6364 and 0.6484 for dynamic one-shot andﬁve-shot,\nrespectively. For question answering, the improvement was not as high\nas for multi-label document classiﬁcation, but the overall trend\nshowed a steady increase, especially considering that GPT-4 already\nhad similar or higher performance than SOTA approaches with zero-\nshot. For instance, its accuracy on PubMedQA was 0.71 with a static\none-shot; the accuracy increased to 0.72 and 0.75 under dynamic one-\nshot andﬁve-shot, respectively.\nIn contrast, the results show that dynamic K-nearest few-shot was\nless effective for other tasks. For instance, the dynamic one-shot per-\nformance is lower than the static one-shot performance for both GPT\nmodels on the two named entity recognition datasets, and by\nincreasing the number of dynamic shots does not help either. Similar\nﬁndings are also observed in relation extraction. For text summariza-\ntion and text simpliﬁcation tasks, the dynamic K-nearest few-shot\nperformance was slightly higher in two datasets, but in general, it was\nvery similar to the static one-shot performance. In addition, the results\nalso suggest that increasing the number of shots does not necessarily\nimprove the performance. For instance, GPT-4 with dynamicﬁve-shot\ndid not have the highest performance in eight out of the 12 datasets.\nSimilar ﬁndings were reported in other studies, where the perfor-\nmance of GPT-3.5 withﬁve-shot learning was lower than that of zero-\nshot learning for natural language inference tasks\n39.\nFigure 1 further compares the costs per 100 instances of using\nGPT-3.5 and GPT-4. The cost is calculated based on the number of\ninput and output tokens with unit price. We used gpt-4-0613 for\nextractive tasks and gpt-4-32k-0613 for generative tasks because the\ninput and output context are much longer especially with more shots.\nGPT-4 generally exhibited the highest performance, as shown in both\nTable2 and Fig.1; however, the cost analysis results also demonstrate a\nclear trade-off, with GPT-4 being 60 to 100 times more expensive. For\nextractive and classiﬁcation tasks, the actual cost per 100 instances of\nGPT-4 forﬁve-shots ranges from approximately $2 for sentence-level\ninputs to around $10 for abstract-level inputs. This cost is 60 to 70\ntimes higher than that of GPT-3.5, which costs approximately $0.03 for\nsentence-level inputs and around $0.16 for abstract-level inputs with\nﬁve-shots. For generative tasks, the cost difference is even more pro-\nnounced, scaling to 100 times or more expensive. One reason is that\nGPT-4 32 K has a higher unit price, and tasks like text summarization\ninvolve much longer input and output tokens. Taking the PubMed Text\nSummarization dataset as an example, GPT-4 cost $84.02 per 100\ninstances with ﬁve-shots, amounting to approximately $5600 to\ninference the entire testing set. In comparison, GPT-3 only cost $0.71\nper 100 instances forﬁve-shots, totaling around $48 for the entire\ntesting set.\nBased on both performance and cost results, it indicates that the\ncost difference does not necessarily scale to the performance differ-\nence, except for question answering tasks. GPT-4 exhibited 20% to 30%\nhigher accuracy than GPT-3.5 in question-answering tasks, and higher\nthan the SOTA approaches; for other tasks, the performance differ-\nence is much smaller with a signiﬁcantly higher cost. For instance, the\nperformance of GPT-4 on both text simpliﬁcation tasks was within 2%\nof that of GPT-3.5, but the actual cost was more than 100 times higher.\nQualitative evaluations\nError analysis on named entity recognition.F i g u r e2A further shows\nan error analysis on the named entity recognition benchmark NCBI\nDisease, where the performance of LLMs under zero- and few-shot\nsettings was substantially lower than SOTA results (e.g., the LLaMA 2\n13B zero-shot performance is almost 70% lower). Recall that named\nentity recognition extracts entities from free text, and the benchmarks\nevaluate the accuracy of these extracted entities. We examined all the\npredictions on full test sets and categorized into four types: (1) correct\nentities, where the predicted entities are correct with both text spans\nand entity types, (2) wrong entities, where the predicted entities are\nincorrect, (3) missing entities, where the true entities are not pre-\ndicted, and (4) boundary issues, where the predicted entities are cor-\nrect but with different text spans than the gold standard, as shown in\nFig. 2A. The results reveal that the LLMs can predict up to 512 entities\ncorrectly out of 960 in total, explaining the low F1-score. As the SOTA\nmodel is not publicly available, we used an alternateﬁne-tuned Bio-\nBERT model on NCBI Disease from an independent study (https://\nhuggingface.co/ugaray96/biobert_ncbi_disease_ner), which had an\nentity-level F1-score of 0.8920 for comparison. It predicted 863 enti-\nties out of 960 correctly. The wrong entities, missing entities, and\nboundary issues were 111, 97, and 269, respectively.\nIn addition, Fig.2A also shows that GPT-4 had the lowest number\nof wrong entities, whereas other categories have a similar prevalence\nto GPT-3.5, which explains its higher F1-score overall. Furthermore,\nproviding one shot did not alter the errors for GPT-3.5 and GPT-4\ncompared to their zero-shot settings, but it dramatically changed the\nresults for LLaMA 2 13B. Under one-shot, LLaMA 2 13B had 449 cor-\nrectly predicted entities, compared to 148 under zero-shot. Addition-\nally, its missing entities also reduced from 812 to 511 with one-shot, but\nit also had a trade-off of more boundary issues and wrong entities.\nEvaluations on inconsistencies, missing information, and halluci-\nnations.F i g u r e2B–D present the qualitative evaluation results on\nChemProt, HoC, and MedQA, respectively. Recall that we categorized\ninconsistencies, missing information, and hallucinations on the tasks\nwhere the gold standard is aﬁxed classiﬁcation type or a multiple-\nchoice option. Table3 also provides detailed examples. Theﬁndings\nshow prevalent inconsistent, missing, or hallucinated responses, par-\nticularly in LLaMA 2 13B zero-shot responses. For instance, it exhibited\n506 hallucinated responses (~3% out of the total 16,943 instances) and\n2376 inconsistent responses (14%) for ChemProt. In the case of HoC,\nthere were 102 (32%) hallucinated responses and 69 (22%) inconsistent\nresponses. Similarly, for MedQA, there were 402 (32%) inconsistent\nresponses. In comparison, GPT-3.5 and GPT-4 exhibited substantially\nfewer cases. GPT-3.5 showed a small number of inconsistent responses\nfor ChemProt and HoC, and a few missing responses for MedQA. On\nthe other hand, GPT-4 did not exhibit any such cases for ChemProt and\nHoC, while displaying a few missing responses for MedQA.\nIt is worth noting that inconsistent responses do not necessarily\nimply that they fail to address the prompts; rather, the responses\nanswer the prompt but in different formats. In contrast, hallucinated\ncases do not address the prompts and may repeat the prompts or\ncontain irrelevant information. All such instances pose challenges for\nautomatic extraction or postprocessing and may require manual\nreview. As a potential solution, we observed that adding just one shot\ncould signiﬁcantly reduce such cases, especially for LLaMA 2 13B,\nwhich exhibited prevalent instances in zero-shot. As illustrated in\nFig. 2B, LLaMA 2 13B one-shot dramatically reduced these cases in\nChemProt and MedQA. Similarly, its hallucinated responses decreased\nfrom 102 to 0, and inconsistent cases decreased from 69 to 23 in HoC\nwith one-shot. Another solution isﬁne-tuning, which we did notﬁnd\nany such cases during the manual examination, albeit with a trade-off\nof computational resources.\nEvaluations on accuracy, completeness, and readability.F i g u r e3\npresents the qualitative evaluation results on the PubMed Text Sum-\nmarization dataset. In Fig.3A, the overall results in accuracy, com-\npleteness, and readability for the four models on 50 random samples\nare depicted. The evaluation results in digits are further demonstrated\nin Table4 for complementary. Detailed results with statistical analysis\nand examples are available in Supplementary Information S3. Theﬁne-\ntuned BART model used in the SOTA approach\n42, serving as the\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 7\nFig. 2 | Qualitative evaluation results on inconsistency, missing information,\nand hallucinations. AError analysis on the named entity recognition benchmark\nNCBI Disease. Correct entities: the predicted entities are correct with both text\nspans and entity types; Wrong entities: the predicted entities are incorrect; Missing\nentities: true entities are not predicted; and Boundary issues: the predicted entities\nare correct but with different text spans than the gold standard.B–D Qualitative\nevaluation on ChemProt, HoC, and MedQA where the gold standard is aﬁxed\nclassiﬁcation type or multiple-choice option. Inconsistent responses: the responses\nare in different formats; Missingness: the responses are missing; and Hallucina-\ntions, where LLMs fail to address the prompt and may contain repetitions and\nmisinformation in the output.\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 8\nbaseline, achieved an accuracy of 4.76 (out of 5), a completeness of\n4.02, and a readability of 4.05. In contrast, both GPT-3.5 and GPT-4\ndemonstrated similar and slightly higher accuracy (4.79 and 4.83,\nrespectively) and statistically signiﬁcantly higher readability than the\nﬁne-tuned BART model (4.66 and 4.73), but statistically signiﬁcantly\nlower completeness (3.61 and 3.57) under the zero-shot setting. The\nLLaMA 2 13B zero-shot performance is substantially lower in all three\naspects.\nFigure 3B further compares GPT-4 to GPT-3.5 and theﬁne-tuned\nBART model in detail. In the comparison between GPT-4 and GPT-3.5,\nGPT-4 had a slightly higher number of winning cases in the three\naspects (4 winning cases vs. 1 losing case for accuracy, 17 vs. 13 for\ncompleteness, and 13 vs. 6 for readability). Most of the cases resulted\nin a tie. When comparing GPT-4 to theﬁne-tuned BART model, GPT-4\nhad signiﬁcantly more winning cases for readability (34 vs. 1) with\nmuch fewer winning cases for completeness (9 vs. 22).\nDiscussions\nFirst, the SOTAﬁne-tuning approaches outperformed zero- and few-\nshot performance of LLMs in most of BioNLP applications. As\ndemonstrated in Table2, it had the best performance in 10 out of the\n12 benchmarks. In particular, it outperformed zero- and few-shot\nLLMs by a large margin in information extraction and classiﬁcation\ntasks such as named entity recognition and relation extraction, which\nis consistent to the existing studies\n43,44. In contrast to, other tasks\nsuch as medical question answering, named entity recognition, and\nrelation extraction require limited reasoning and extract information\ndirectly from inputs at the sentence-level. Zero- and few-shot learn-\ning may not be appropriate or sufﬁcient for these conditions. For\nthose tasks, arguably, ﬁne-tuned biomedical domain-speci ﬁc\nlanguage models are still theﬁrst choice and have already set a high\nbar, according to the literature\n32.\nIn addition, closed-source LLMs such as GPT-3.5 and GPT-4\ndemonstrated reasonable zero- and few-shot capabilities for three\nBioNLP tasks. The most promising task that outperformed the SOTA\nﬁne-tuning approaches is medical question answering, which involves\nreasoning\n45.A ss h o w ni nT a b l e2 and Fig. 1, GPT-4 already out-\nperformed previous ﬁne-tuned SOTA approaches in MedQA and\nPubMedQA with zero- or few-shot learning. This is also supported by\nthe existing studies on medical question answering\n38,46. The second\npotential use case is text summarization and simpliﬁcation. As shown\nin Table2, those tasks are still less favored by the automatic evaluation\nmeasures; however, manual evaluation results show both GPT-3.5 and\nGPT-4 had higher readability and competitive accuracy compared to\nthe SOTA ﬁne-tuning approaches. Other studies reported similar\nﬁndings regarding the low correlation between automatic and manual\nevaluations35,47. The third possible use case – though still under-\nperformed by previousﬁne-tuned SOTA approaches– document-level\nclassiﬁcation, which involves semantic understanding. As shown in\nFig. 1, GPT-4 achieved over a 0.7 F1-score with dynamic K-nearest shot\nfor both multi-label document-level classiﬁcation benchmarks.\nIn addition to closed-source LLMs, open-source LLMs such as\nLLaMA 2 do not demonstrate strong zero- and few-shot capabilities.\nWhile there are other open-source LLMs available, LLaMA 2 remains\nas a strong representative\n48. Results in Table1 suggest that its overall\nzero-shot performance is 15% and 22% lower than that of GPT-3.5 and\nGPT-4, respectively, and up to 60% lower in speciﬁc BioNLP tasks.\nNot only does it exhibit suboptimal performance, but the results\nin Fig. 2 also demonstrate that its zero-shot responses frequently\ncontain inconsistencies, missing elements, and hallucinations,\nTable 3 | Examples of inconsistent, missing, and hallucinated responses\nType Example\n1. Inconsistency Example 1. GPT-3 zero-shot on ChemProt\nRequired output: one of the six relation types: CPR:3, CPR:4, CPR:5, CPR:6, CPR:9, or false\nActual and expected output: CPR:9 [the expected output]\nInconsistent output 1: SUBSTRATE [it might refer to CPR:9]\nInconsistent output 2: SUBSTRATE PRODUCT OF [it might refer to CPR:9]\nExample 2. LLaMA 2 zero-shot on MedQA\nRequired output: a multiple-choice option from A to E\nActual and somewhat expected output: I’m happy to help! Based on the information provided, the correct answer is E. Age above 50 years.\nAspirin is contraindicated… [it directly provided the option at the start, which is similar to the expected output]\nInconsistent output 1: I’m happy to help! Based on the patient’s symptoms and physical examinationﬁndings, the most likely diagnosis is pyloric\nstenosis. … [it did not provide the answer option]\nInconsistent output 2: Great! Let’s break down the question and the options. The question is asking about an enzyme deﬁciency that is causing the\nsymptoms described in the scenario.… Therefore, option E,… , is the correct answer. [it went through all the options and provided the answer\noption at last]\nInconsistent output 3: Great question! To conﬁrm the most likely diagnosis of this patient, we need to consider the following options: Options A,\nC, and D… Therefore, the correct answer is: E.… It is important to consider all possible causes…\n2. Missingness Example 1. GPT-3.5 one-shot on MedQA\nRequired output: a multiple-choice option from A to E\nActual and expected output: C [the expected output]\nMissing output 1: [no answer]\n3. Hallucinations Example 1. LLaMA 2 zero-shot on HoC\nRequired output: one or more document topic labels out of 10 in total separated by semicolons\nActual and somewhat expected output: Based on the input article, the following topics are relevant: * Sustaining proliferative signaling * Resisting\ncell death * Enabling replicative immortality * Activating invasion and metastasis * Genomic instability and mutation * Tumor promoting\ninﬂammation * Cellular energetics * Avoiding immune destruction Please note that the article is related to multiple topics, and the output is a\nsemicolon-separated list of relevant topics. [it provides the topics like the expected format though not separated by semicolons]\nHallucinated output 1: Please provide the list of topics that are relevant to the input article.\nExample 2. LLaMA 2 zero-shot on MedQA\nRequired output: a multiple-choice option from A to E\nActual and expected out example is provided above\nHallucinated output 1: Great! You have selected the correct answer. Let me explain why.…\nHallucinated output 2: That’s correct! Tetralogy of Fallot is a congenital heart defect…\nHallucinated output 3: Great question! Based on the patient’s symptoms and physical examinationﬁndings, the most likely impaired structure is\nthe ________________.… [it asks toﬁll in the blank]\nHallucinated output 4: Please select one of the options from A to E.\nText in square brackets represents annotated explanations. Unnecessary detail is omitted due to space constraints.\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 9\naccounting for up to 30% of the full testing set instances. Therefore,\nﬁne-tuning open-source LLMs for BioNLP tasks is still necessary to\nbridge the gap. Only throughﬁne-tuning LLaMA 2, its overall per-\nformance is slightly higher than the one-shot GPT-4 (4%). However, it\nis worth noting that the model sizes of LLaMA 2 and PMC LLaMA are\nsigniﬁcantly smaller than those of GPT-3.5 and GPT-4, making it\nchallenging to evaluate them on the same level. Additionally, open-\nsource LLMs have the advantage of continued development and local\ndeployment.\nAnother primaryﬁnding on open-source LLMs is that the results\ndo not indicate signiﬁcant performance improvement from con-\ntinuously biomedical pre-trained LLMs (PMC LLaMA 13B vs. LLaMA 2\n13B). As mentioned, our study reproduced similar results reported in\nPMC LLaMA 13B; however, we also found that directlyﬁne-tuning\nL L a M A2y i e l d e db e t t e ro ra tl e a s ts i m i l a rp e r f o r m a n c e— and this is\nconsistent across all 12 benchmarks. In the biomedical domain,\nrepresentative foundation LLMs such as PMC LLaMA used 32 A100\nGPUs\n34, and Meditron used 128 A100 GPUs to continuously pretrain\nfrom LLaMA or LLaMA 249. Our evaluation did notﬁnd signiﬁcant\nperformance improvement for PMC LLaMA; the Meditron study also\nonly reported ~3% improvement itself and only evaluated on question\nanswering datasets. At a minimum, the results suggest the need for a\nmore effective and sustainable approach to developing biomedical\ndomain-speciﬁc LLMs.\nThe automatic metrics for text summarization and simpliﬁcation\ntasks may not align with manual evaluations. As the quantitative results\non text summarization and generation demonstrated, commonly used\nautomatic evaluations such as Rouge, BERT, and BART scores con-\nsistently favored theﬁne-tuned BART’s generated text, while manual\nevaluations show different results, indicating that GPT-3.5 and GPT-4\nhad competitive accuracy and much higher readability even under the\nzero-shot setting. Existing studies also reported that the automatic\nmeasures on LLM-generated text may not correlate to human\npreference\n35,47. The MS^2 benchmark used in the study also discussed\nthe limitation of automatic measures, speci ﬁcally for text\nsummarization50. Additionally, the resultshighlight that completeness\nis a primary limitation when adapting GPT models to biomedical text\ngeneration tasks despite its competitive accuracy and readability\nscores.\nLast, our evaluation on both performance and cost demonstrates\na clear trade-off when using LLMs in practice. GPT-4 had the overall\nbest performance in the 12 benchmarks, with an 8% improvement over\nGPT-3.5 but also at a higher cost (60 to 100 times higher than GPT-3.5).\nNotably, GPT-4 showed signiﬁcantly higher performance, particularly\nin question-answering tasks that involve reasoning, such as over 20%\nimprovement in MedQA compared to GPT-3.5. This observation is\nconsistent withﬁndings from other studies\n27,38. Note that newer ver-\nsions of GPT-4, such as GPT-4 Turbo, may further reduce the cost of\nusing GPT-4.\nTheseﬁndings lead to recommendations for downstream users to\napply LLMs in BioNLP applications, summarized in Fig.4.I tp r o v i d e s\nsuggestions on which BioNLP applications are recommended (or not)\nfor LLMs, categorized by conditions (e.g., the zero/few-shot setting\n(A) (B)\n(C)\nFig. 3 | Qualitative evaluation results on accuracy, completeness, and read-\nability. AThe overall results of theﬁne-tuned BART, GPT-3.5 zero-shot, GPT-4 zero-\nshot, and LLaMA 2 zero-shot models on a scale of 1 to 5, based on random 50 testing\ninstances from the PubMed Text Summarization dataset.B and C display the\nnumber of winning, tying, and losing cases when comparing GPT-4 zero-shot to\nGPT-3.5 zero-shot and GPT-4 zero-shot to theﬁne-tuned BART model, respectively.\nTable 4 shows the results in digits for complementary. Detailed results, including\nstatistical tests and examples, are provided in Supplementary Information S3.\nTable 4 | Qualitative evaluation results on accuracy, com-\npleteness, and readability of the generated text for theﬁne-\ntuned BART, GPT-3.5 zero-shot, GPT-4 zero-shot, and LLaMA\n2 zero-shot models on a scale of 1 to 5, based on random 50\ntesting instances from the PubMed Text Summarization\ndataset, to complement Fig.3\nFine-\ntuned BART\nGPT-3.5\nzero-shot\nGPT-4\nzero-shot\nLLaMA 2\nzero-shot\nAccuracy 4.76 4.79 4.83 3.42\nCompleteness 4.02 3.61 3.57 2.20\nReadability 4.05 3.57 4.73 2.53\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 10\nwhen computational resources are limited) and additional tips (e.g.,\nwhen advanced prompt engineering is more effective).\nWe also recognize the following two open problems and encou-\nrage a community effort for better usage of LLMs in BioNLP\napplications.\nAdapting both data and evaluation paradigms is essential to\nmaximize the beneﬁts of LLMs in BioNLP applications. Arguably, the\ncurrent datasets and evaluation settings in BioNLP are tailored to\nsupervised (ﬁne-tuning) methods and is not fair for LLMs. Those issues\nchallenge the direct comparison between theﬁne-tuned biomedical\ndomain-speciﬁc language models and zero/few shot of LLMs. The\ndatasets for the tasks where LLMs excel are also limited in the bio-\nmedical domain. Further, the manual measures on biomedical text\nsummarization also showed different results than that of all three\nautomatic measures. These collectively suggest the current BioNLP\nevaluation frameworks have limitations when they are applied to\nLLMs\n35,51. They may not be able to accurately assess the full beneﬁts of\nLLMs in biomedical applications, calling for the development of new\nevaluation datasets and methods for LLMs in bioNLP tasks.\nAddressing inconsistencies, missingness, and hallucinations pro-\nduced by LLMs is critical. The prevalence of inconsistencies, missing-\nness, and hallucinations generated by LLMs is of concern, and we argue\nthat they must be addressed for deployment. Our results demonstrate\nthat providing just one shot could signiﬁcantly reduce the occurrence\nof such issues, offering a simple solution. However, thorough exam-\nination in real-world scenario validations is still necessary. Addition-\nally, more advanced approaches for validating LLMs’responses are\nexpected for further improvement of their reliability and usability\n47.\nThis study also has several limitations that should be acknowl-\nedged. While this study examined strong LLM representatives from\neach category (closed-source, open-source, and biomedical domain-\nspeciﬁc), it is important to note that there are other LLMs, such as\nBARD52 and Mistral53, that have demonstrated strong performance in\nthe literature. Additionally, while we investigated zero-shot, one-shot,\ndynamic K-nearest few-shot, andﬁne-tuning techniques, each of them\nhas variations, and there are also new approaches\n54. Given the rapidly\ngrowing nature of this area, our study cannot cover all of them.\nInstead, our aim is to establish baseline performance on the main\nBioNLP applications using commonly used LLMs and methods as\nrepresentatives, and to make the datasets, methods, codes, and results\npublicly available. This enables downstream users to understand when\nand how to apply LLMs in their own use cases and to compare new\nLLMs and associated methods on the same benchmarks. In the future,\nwe also plan to assess LLMs in real-world scenarios in the biomedical\ndomain to further broaden the scope of the study.\nMethods\nEvaluation tasks, datasets, and metrics\nTable 5 presents a summary of the evaluation tasks, datasets, and\nmetrics. We benchmarked the models on the full testing sets of the\ntwelve datasets from six BioNLP applications, which are BC5CDR-\nchemical and NCBI-disease for Named Entity Recognition, ChemProt\nand DDI2013 for relation extraction, HoC and LitCovid for multi-label\ndocument classiﬁcation, and MedQA and PubMedQA for question\nanswering, PubMed Text Summarization and MS^2 for text summar-\nization, and Cochrane PLS and PLOS Text Simpliﬁcation for text sim-\npliﬁcation. These datasets have been widely used in benchmarking\nbiomedical text mining challenges\n55–57 and evaluating biomedical lan-\nguage models9–11,16. The datasets are also available in the repository. We\nevaluated the datasets using the ofﬁcial evaluation metrics provided\nby the original dataset description papers, as well as commonly used\nmetrics for method development or applications with the datasets, as\ndocumented in Table5. Note that it is challenging to have a single one-\nsize-ﬁts-all metric, and some datasets and related studies used multiple\nevaluation metrics. Therefore, we also adopted secondary metrics for\nadditional evaluations. A detailed description is below.\nNamed entity recognition. Named entity recognition is a task that\ninvolves identifying entities of interest from free text. The biomedical\nentities can be described in various ways, and resolving the ambi-\nguities is crucial\n58. Named entity recognition is typically a sequence\nQuestion answering\nSummarization\nDocument -level\nclassification\nExtraction\nHighly recommend\nRecommend\nLess recommend\nGood to try\nZero/few-shot\n Fine-tuning General\nRecommendationsTop-choice:\nGPT-4\nGood-choice:\nClosed-source LLMs only\n(e.g., starting with GPT-3.5)\nAdvanced\nPrompt\nEngineering\n+ Open-source LLMs\nGood-choice:\nClosed-source LLMs only\n(e.g., starting with GPT-3.5)\nStrong baseline to try first:\nfine-tuned BART models\nOpen-source LLMs:\nif input context length fits\nGood-choice:\nClosed-source LLMs only\n(e.g., starting with GPT-3.5)\nLess recommended\nStrong baseline to try first:\nfine-tuned BERT models\nOpen-source LLMs:\nif input context length fits\nTop-choice:\nfine-tuned BERT models\nOpen-source LLMs:\nif input context length fits\nAdvanced Prompt Engineering\n+\n1. Stay aware of inconsistent,\nmissing, and hallucinated\nresponses; providing even one\nexample could reduce such\ncases; manual review is\nessential\n2. GPT-3.5 is a reliable\nbaseline option given its\nperformance and cost -\neffectiveness; apply GPT -4\nespecially for tasks requiring\nadvanced reasoning abilities\n3. Apply advanced prompt\nengineering especially for\ntasks requiring reasoning and\nsemantic understanding\nReasoning -related\nSimplification\nGeneration -related\nSemantic understanding -\nrelated\nExtractive tasks\nFig. 4 | Recommendations for using LLMs in BioNLP applications.It presents speciﬁc task-based recommendations across different settings and offers general guidance\non effectively applying LLMs in BioNLP.\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 11\nlabeling task, where each token is classiﬁed into a speciﬁce n t i t yt y p e .\nBC5CDR-chemical59 and NCBI-disease60 are manually annotated named\nentity recognition datasets for chemicals and diseases mentioned in\nbiomedical literature, respectively. The exact match (that is, the pre-\ndicted tokens must have the same text spans as the gold standard) F1-\nscore was used to quantify the model performance.\nRelation extraction. Relation extraction involves identifying the\nrelationships between entities, which is important for drug repurpos-\ning and knowledge discovery\n61. Relation extraction is typically a multi-\nclass classiﬁcation problem, where a sentence or passage is given with\nidentiﬁed entities and the goal is to classify the relation type between\nthem. ChemProt55 and DDI201362 are manually curated relation\nextraction datasets for protein-protein interactions and drug-drug\ninteractions from biomedical literature, respectively. Macro and micro\nF1-scores were used to quantify the model performance.\nMulti-label document classiﬁcation. Multi-label document classi-\nﬁcation identiﬁes semantic categories at the document-level. The\nsemantic categories are effective for grasping the main topics and\nsearching for relevant literature in the biomedical domain\n63. Unlike\nmulti-class classiﬁcation, which assigns only one label to an instance,\nmulti-label classiﬁcation can assign up to N labels to an instance. HoC64\nand LitCovid56 are manually annotated multi-label document classiﬁ-\ncation datasets for hallmarks of cancer (10 labels) and COVID-19 topics\n(7 labels), respectively. Macro and Micro F1 scores were used as the\nprimary and secondary evaluation metrics, respectively.\nQuestion answering. Question answering evaluates the knowl-\nedge and reasoning capabilities of a system in answering a given bio-\nmedical question with or without associated contexts\n45. Biomedical QA\ndatasets such as MedQA and PubMedQA have been widely used in the\nevaluation of language models\n65. The MedQA dataset is collected from\nquestions in the United States Medical License Examination (USMLE),\nwhere each instance contains a question (usually a patient description)\nand ﬁve answer choices (e.g.,ﬁve potential diagnoses)\n66.T h eP u b -\nMedQA dataset includes biomedical research questions from PubMed,\nand the task is to use yes, no, or maybe to answer these questions with\nthe corresponding abstracts\n67. Accuracy and macro F1-score are used\nas the primary and secondary evaluation metrics, respectively.\nText summarization. Text summarization produces a concise and\ncoherent summary of a longer documents or multiple documents\nwhile preserving its essential content. We used two primary biomedical\ntext summarization datasets: the PubMed text summarization\nbenchmark\n68 and MS^250. The PubMed text summarization benchmark\nfocuses on single document summarization where the input is a full\nPubMed article, and the gold standard output is its abstract. M2^2 in\ncontrast, focuses on multi-document summarization where the input\nis a collection of PubMed articles, and the gold standard output is the\nabstract of a systematic review study that cites those articles. Both\nbenchmarks used the ROUGE-L score as the primary evaluation metric;\nBERT score and BART score were used as secondary evaluation\nmetrics.\nText simpliﬁcation. Text simpliﬁcation rephrases complex texts\ninto simpler language while maintaining the original meaning, making\nthe information more accessible to a broader audience. We used two\nprimary biomedical text simpliﬁcation datasets: Cochrane PLS\n69 and\nthe PLOS text simpliﬁcation benchmark70. Cochrane PLS consists of\nthe medical documents from the Cochrane Database of Systematic\nReviews and the corresponding plain-language summary (PLS) written\nby the authors. The PLOS text simpliﬁcation benchmark consists of\narticles from PLOS journals and the corresponding technical summary\nand PLS written by the authors. The ROUGE-L score was used as the\nprimary evaluation metric. Flesch-Kincaid Grade Level (FKGL) and\nDale-Chall Readability Score (DCRS), two commonly used evaluation\nmetrics on readability\n71 were used as the secondary evaluation metrics.\nBaselines\nFor each dataset, we reported the reported SOTAﬁne-tuning result\nbefore the rise of LLMs as the baseline. The SOTA approaches involved\nﬁne-tuning (domain-speciﬁc) language models such as PubMedBERT\n16,\nBioBERT9,o rB A R T72 as the backbone. Theﬁne-tuning still requires\nscalable manually labeled instances, which is challenging in the bio-\nmedical domain32. In contrast, LLMs may have the advantage when\nminimal manually labeled instances are available, and they do not\nrequire ﬁne-tuning or retraining for every new task through zero/few-\nshot learning. Therefore, we used the existing SOTA results achieved\nTable 5 | Evaluation datasets, dataset size, and evaluation metrics\nTraining Validation Testing Primary metrics Secondary metrics\nNamed entity recognition\nBC5CDR-chemical59 4560 4581 4797 Entity-level F1 59,89\nNCBI-disease60 5424 923 940 Entity-level F1 16,60\nRelation extraction\nChemProt55 19,460 11,820 16,943 Macro F1 90 Micro F155,90\nDDI201362 18,779 7244 5761 Macro F1 62,85 Micro F116\nMulti-label document classiﬁcation\nHoC64 1108 157 315 Macro F1 64,86 Micro F186\nLitCovid56 24,960 6239 2500 Macro F1 56 Micro F156\nQuestion answering\nMedQA 5-option66 10,178 1272 1273 Accuracy 66 Macro F191\nPubMedQA67 190,142 21,127 500 Accuracy 67 Macro F191\nText summarization\nPubMed Text Summarization\na68 117,108 6631 6658 Rouge-L 68 BERT Score92,B A R TS c o r e93\nMS^2b50 14,188 2021 - Rouge-L 50 BERT Score94,B A R TS c o r e28\nText simpliﬁcation\nCochrane PLS69 3568 411 480 Rouge-L 69 FKGL95,D C R S96\nPLOS Text Simpliﬁcation70 26,124 1000 1000 Rouge-L 70 FKGL70,D C R S70\nThe related studies using the metrics are also provided.aWe ﬁltered the noisy instances with less than 50 words for the training and validation sets and kept the testing set untouched.bThe gold\nstandard of the testing set of MS^2 is not publicly available; we used the validation set instead.\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 12\nby theﬁne-tuning approaches to quantify the beneﬁts and challenges\nof LLMs in BioNLP applications.\nLarge language models\nRepresentative LLMs and their versions. Both GPT-3.5 and GPT-4 have\nbeen regularly updated. For reproducibility, we used the snapshots\ngpt-3.5-turbo-16k-0613 and gpt-4-0613 for extractive tasks, and gpt-4-\n32k-0613 for generative tasks, considering their input and output\ntoken sizes. Regarding LLaMA 2, it is available in 7B, 13B, and 70B\nversions. We evaluated LLaMA 2 13B based on the computational\nresources required forﬁne-tuning, which is arguably the most com-\nmon scenario applicable to BioNLP downstream applications. For PMC\nLLaMA, both 7B and 13B versions are available. Similarly, we used PMC\nLLaMA 13B, speciﬁcally evaluating it under theﬁne-tuning setting– the\nsame setting used in its original study\n34.I nt h eo r i g i n a ls t u d y ,P M C\nLLaMA was only evaluated on medical question answering tasks,\ncombining multiple question answering datasets forﬁne-tuning. In our\ncase, weﬁne-tuned each dataset separately and reported the results\nindividually.\nPrompts. To date, prompt design remains an open research\nproblem73–75. We developed a prompt template that can be used across\ndifferent tasks based on existing literature74–77. An annotated prompt\nexample is provided in Supplementary Information S1 Prompt engi-\nneering, and we have made all the prompts publicly available in the\nrepository. The prompt template contains (1) task descriptions (e.g.,\nclassifying relations), (2) input speciﬁcations (e.g., a sentence with\nlabeled entities), (3) output speciﬁcations (e.g., the relation type), (4)\ntask guidance (e.g., detailed descriptions or documentations on rela-\ntion types), and (5) example demonstrations if examples from training\nsets are provided. This approach aligns with previous studies in the\nbiomedical domain, which have demonstrated that incorporating task\nguidance into the prompt leads to improved performance\n74,76 and was\nalso employed and evaluated in our previous study, speciﬁcally\nfocusing on named entity recognition77.W ea l s oa d a p t e dt h eS O T A\nexample selection approach in the biomedical domain described\nbelow\n27.\nZero-shot and static few-shot. We comparatively evaluated the\nzero-shot, one-shot, andﬁve-shot learning performances. Only a few\nstudies have made the selected examples available. For reproducibility\nand benchmarking, weﬁrst randomly selected the required number of\nexamples in training sets, used the same selected examples for few-\nshot learning, and made the selected examples publicly available.\nDynamic K-nearest few-shot. In addition to zero- or static few-shot\nlearning whereﬁxed instructions are used for each instance, we further\nevaluated the LLMs under a dynamic few-shot learning setting. The\ndynamic few-shot learning is based on the MedPrompt approach, the\nSOTA method that demonstrated robust performance in medical\nquestion answering tasks withoutﬁne-tuning\n27. The essence is to use K\ntraining instances that are most similar to the test instance as the\nselected examples. We denote this setting as dynamic K-nearest few-\nshot, as the prompts for different test instances differ. Speciﬁcally, for\neach dataset, we used the SOTA text embedding model text-\nembedding-ada-002\n54 to encode the instances and used cosine simi-\nlarity as the metric forﬁnding similar training instances to a testing\ninstance. We tested dynamic K-nearest few-shot prompts with K equals\nto one, two, andﬁve.\nParameters for prompt engineering. For zero-, one-, and few-\nshot approaches, we used a temperature parameter of 0 to minimize\nvariance for both GPT and LLaMA-based models. Additionally,\nfor LLaMA models, we maintained other parameters unchanged, set\nthe maximum number of generated tokens per task, and truncated\nthe instances due to the input length limit for theﬁve-shot setting.\nFurther details are provided in Supplementary Information S1\nPrompt engineering, and the related codes are available in the\nrepository.\nFine-tuning. We further conducted instructionﬁne-tuning on\nLLaMA 2 13B and PMC-LLaMA 13B. For each dataset, weﬁne-tuned\nLLaMA 2 13B and PMC- LLaMA 13B using its training set. The goal of\ninstruction ﬁne-tuning is de ﬁned by the objective function:\nargmax\nθ\nP\nxi , yiðÞ ϵðX, Y Þ logpðyijxi; θÞ,w h e r exi represents the input\ninstruction,yi is the ground truth response, andθ is the parameter set\nof the model. This function aims to maximize the likelihood of accu-\nrately predicting responses based on the given instructions. Theﬁne-\ntuning is performed on eight H100 80G GPUs, over three epochs with a\nlearning rate of 1e−5, a weight decay of 1e−5, a warmup ratio of 0.01,\nand Low-Rank Adaptation (LoRA) for parameter-effective tuning\n78.\nOutput parsing. For extractive and classi ﬁcation tasks, we\nextracted the targeted predictions (e.g., classi ﬁcation types or\nmultiple-choice options) from the raw outputs of LLMs with a com-\nbination of manual and automatic processing. We manually reviewed\nthe processed outputs. Manual review showed that LLMs provided\nanswers in inconsistent formats in some cases. For example, when\npresenting multiple-choice option C, the raw output examples inclu-\nded variations such as:“Based on the information provided, the most\nlikely … is C. The thyroid gland is a common site for metastasis, and\n…” , “Great! Let’s go through the options. A.… B. … Therefore, the most\nlikely diagnosis is C.”,a n d“I’m happy to help! Based on the patient’s\nsymptoms and examinationﬁndings, … Therefore, option A is incor-\nrect. … , so option D is incorrect. The correct answer is option C.”\n(adapted from real responses with unnecessary details omitted). In\nsuch cases, automatic processing might overlook the answer, poten-\ntially lowering LLM accuracy. Thus, we manually extracted outputs in\nthese instances to ensure fair credit. Additionally, we qualitatively\nevaluated the prevalence of such cases (providing responses in\ninconsistent formats), which will be introduced below.\nEvaluations\nQuantitative evaluations. We summarized the evaluation metrics in\nTable 5 under zero-shot, static few-shot, dynamic K-nearest few-shot,\nand ﬁne-tuning settings. The metrics are applicable to the entire test-\ning sets of 12 datasets. We further conducted bootstrapping using a\nsubsample size of 30 and repeated 100 times at a 95% conﬁdence\ninterval to report performance variance and performed a two-tailed\nWilcoxon rank-sum test using SciPy\n79. Further details are provided in\nSupplementary Information S2 Quantitative evaluation results (S2.1.\nResult reporting).\nQualitative evaluations on inconsistency, missing information,\nand hallucinations. For the tasks where the gold standard isﬁxed, e.g.,\na classiﬁcation type or multiple-choice option, we conducted qualita-\ntive evaluations on collectively hundreds of thousands of raw outputs\nof the LLMs (the raw outputs from three LLMs under zero- and one-\nshot conditions across three benchmarks) to categorize errors beyond\ninaccurate predictions. Speciﬁcally, we examined (1) inconsistent\nresponses, where the responses are in different formats, (2) missing-\nness, where the responses are missing, and (3) hallucinations, where\nLLMs fail to address the prompt and may contain repetitions and\nmisinformation in the output\n36. We evaluated and reported the results\nin selected datasets: ChemProt, HoC, and MedQA.\nQualitative evaluations on accuracy, completeness, and read-\nability. For the tasks with free-text gold standards, such as summaries,\nwe conducted qualitative evaluations on the quality of generated text.\nSpeciﬁcally, one senior resident and one junior resident evaluated four\nmodels: theﬁne-tuned BART model reported in the SOTA approach,\nGPT-3.5 zero-shot, GPT-4 zero-shot, and LLaMA 2 13B zero-shot on 50\nrandom samples from the PubMed Text Summarization benchmark.\nEach annotator was provided with 600 annotations. To mitigate\npotential bias, the model outputs were all lowercased, their orders\nwere randomly shufﬂed, and the annotators were unaware of the\nmodels being evaluated. They assessed three dimensions on a scale of\n1— 5: (1) accuracy, does the generated text contain correct information\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 13\nfrom the original input, (2) completeness, does the generated text\ncapture the key information from the original input, and (3) read-\nability, is the generated text easy to read. The detailed evaluation\nguideline is provided in Supplementary Information S3 Qualitative\nevaluation on the PubMed Text Summarization Benchmark.\nCost analysis. We further conducted a cost analysis to quantify the\ntrade-off between cost and accuracy when using GPT models. The cost\nof GPT models is determined by the number of input and output\ntokens. We tracked the tokens in the input prompts and output com-\npletions using the ofﬁcial model tokenizers provided by OpenAI\n(https://cookbook.openai.com/examples/how_to_count_tokens_with_\ntiktoken) and used the pricing table (https://azure.microsoft.com/en-\nus/pricing/details/cognitive-services/openai-service/) to compute the\noverall cost.\nReporting summary\nFurther information on research design is available in the Nature\nPortfolio Reporting Summary linked to this article.\nData availability\nAll data supporting theﬁndings of this study, including source data,\nare available in the article and Supplementary Information, and can be\naccessed publicly via https://doi.org/10.5281/zenodo.14025500\n37.\nAdditional data or requests for data can also be obtained from the\ncorresponding authors upon request. Source data are provided with\nthis paper.\nCode availability\nThe codes are publicly available viahttps://doi.org/10.5281/zenodo.\n1402550037.\nReferences\n1. Sayers, E. W. et al. Database resources of the National Center for\nBiotechnology Information in 2023.Nucleic Acids Res.51,\nD29–D38 (2023).\n2. Chen, Q. et al. LitCovid in 2022: an information resource for the\nCOVID-19 literature.Nucleic Acids Res.51, D1512–D1518 (2023).\n3. Leaman, R. et al. Comprehensively identifying long COVID\narticles with human-in-the-loop machine learning.Patterns 4,\n100659 (2023).\n4. Chen, Q. et al. BioConceptVec: creating and evaluating literature-\nbased biomedical concept embeddings on a large scale.PLoS\nComput. Biol.16,e 1 0 0 7 6 1 7( 2 0 2 0 ) .\n5. Blake, C. Beyond genes, proteins, and abstracts: Identifying scien-\ntiﬁc claims from full-text biomedical articles.J. Biomed. Inform.43,\n173–189 (2010).\n6. Su, Y. et al. Deep learning joint models for extracting entities and\nrelations in biomedical: a survey and comparison.Brief. Bioinforma.\n23, bbac342 (2022).\n7. Zhang, Y., Chen, Q., Yang, Z., Lin, H., & Lu, Z. BioWordVec,\nimproving biomedical word embeddings with subword information\nand MeSH.Sci. Data.6,1 –9 (2019).\n8. Chen, Q., Peng, Y. & Lu, Z. BioSentVec: creating sentence embed-\ndings for biomedical texts.In2019 IEEE International Conference on\nHealthcare Informatics (ICHI)1–5( I E E E ,2 0 1 9 ) .\n9. Lee, J. et al. BioBERT: a pre-trained biomedical language repre-\nsentation model for biomedical text mining.Bioinformatics36,\n1234–1240 (2020).\n10. Peng, Y., Yan, S., & Lu, Z. Transfer learning in biomedical natural\nlanguage processing: an evaluation of BERT and ELMo on ten\nbenchmarking datasets. InProc. 18th BioNLP Workshop and Shared\nTask,5 8–65 (Association for Computational Linguistics, Florence,\nItaly, 2019).\n11. Fang, L., Chen, Q., Wei, C.-H., Lu, Z. & Wang, K. Bioformer: an efﬁ-\ncient transformer language model for biomedical text mining, arXiv\npreprint arXiv:2302.01588 (2023).\n12. Luo, R. et al. BioGPT: generative pre-trained transformer for bio-\nmedical text generation and mining.Brief. Bioinforma.23,\nbbac409 (2022).\n1 3 . V e n i g a l l a ,A . ,F r a n k l e ,J . ,&C a r b i n ,M .B i o m e d l m :ad o m a i n - s p e c iﬁc\nlarge language model for biomedical text, MosaicML. Accessed:\nDec, 23 (2022).\n14. Yuan, H. et al. BioBART: Pretraining and evaluation of a biomedical\ngenerative language model. InProc. 21st Workshop on Biomedical\nLanguage Processing,9 7–109 (2022).\n15. Phan, L.N. et al. Sciﬁve: a text-to-text transformer model for bio-\nmedical literature, arXiv preprint arXiv:2106.03598 (2021).\n16. Gu, Y. et al. Domain-speciﬁc language model pretraining for bio-\nmedical natural language processing.ACM Trans. Comput. Healthc.\nHEALTH, 3,1 –23 (2021).\n17. Allot, A. et al. LitSense: making sense of biomedical literature at\nsentence level.Nucleic Acids Res.47,W 5 9 4–W599 (2019).\n18. Zhao, W. X. et al. A survey of largel a n g u a g em o d e l s ,a r X i vp r e p r i n t\narXiv:2303.18223 (2023).\n19. Ouyang, L. et al. Training language models to follow instructions\nwith human feedback.Adv. Neural Inf. Process. Syst.35,\n27730–27744 (2022).\n20. Chen, X. et al. How Robust is GPT-3.5 to Predecessors? A Com-\nprehensive Study on Language Understanding Tasks, arXiv preprint\narXiv:2303.00293 (2023).\n21. OpenAI, GPT-4 Technical Report, ArXiv, abs/2303.08774, (2023).\n22. Touvron, H. et al. Llama 2: Open foundation andﬁne-tuned chat\nmodels, arXiv preprint arXiv:2307.09288 (2023).\n2 3 . J i a n g ,A .Q .e ta l .M i x t r a lo fe x p e r t sa r X i vp r e p r i n ta r X i v : 2 4 0 1 . 0 4 0 8 8 ,\n2024.\n2 4 . L e e ,P ,G o l d b e r g ,C .&K o h a n e ,I .T h eA Ir e v o l u t i o ni nm e d i c i n e :G P T -\n4 and beyond (Pearson, 2023).\n2 5 . W o n g ,C .e ta l .S c a l i n gc l i n i c a ltrial matching using large language\nmodels: A case study in oncology. InMachine Learning for Health-\ncare Conference846–862 (PMLR, 2023).\n26. Liu, Q. et al. Exploring the Boundaries of GPT-4 in Radiology. InProc.\nof the 2023 Conference on Empirical Methods in Natural Language\nProcessing14414–14445 (2023).\n27. Nori, H. et al. Can generalist foundation models outcompete\nspecial-purpose tuning? Case study in medicine, arXiv preprint\narXiv:2311.16452 (2023).\n28. Tian, S. et al. Opportunities and challenges for ChatGPT and large\nlanguage models in biomedicine and health.Brief. Bioinforma.25,\nbbad493 (2024).\n29. He, K. et al. A survey of large language models for healthcare: from\ndata, technology, and applications to accountability and ethics,\narXiv preprint arXiv:2310.05694 (2023).\n30. Omiye, J. A., Gui, H., Rezaei, S. J., Zou, J. & Daneshjou, R. Large\nlanguage models in medicine: the potentials and pitfalls: a narrative\nreview. Ann. Intern. Med.177,2 1 0–220 (2024).\n31. So ğancıoğlu, G., Öztürk, H. & Özgür, A. BIOSSES: a semantic sen-\ntence similarity estimation system for the biomedical domain.\nBioinformatics33,i 4 9–i58 (2017).\n32. Tinn, R. et al. Fine-tuning large neural language models for\nbiomedical natural language processing.Patterns. 4,\n100729 (2023).\n3 3 . C h e n ,Q . ,R a n k i n e ,A . ,P e n g ,Y . ,A g h a a r a b i ,E .&L u ,Z .B e n c h m a r k i n g\neffectiveness and efﬁciency of deep learning models for semantic\ntextual similarity in the clinical domain: validation study.JMIR Med.\nInform.\n9, e27386 (2021).\n34. Wu, C. et al. PMC-LLaMA: toward building open-source language\nmodels for medicine,J .A m .M e d .I n f o r m .A s s o c i a t. ocae045 (2024).\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 14\n35. Fleming, S. L. et al. MedAlign: A clinician-generated dataset for\ninstruction following with electronic medical records. InProc.\nAAAI Conference on Artiﬁcial IntelligenceVol. 38\n22021–22030 (2023).\n36. Zhang, Y. et al. Siren’s song in the AI ocean: a survey on halluci-\nnation in large language models. arXiv preprint arXiv:2309.01219\n(2023).\n37. Chen, Q. et al. A systematic evaluation of large language models for\nbiomedical natural language processing: benchmarks, baselines,\nand recommendations.https://doi.org/10.5281/zenodo.14025500\n(2024).\n38. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E. Cap-\nabilities of gpt-4 on medical challenge problems. arXiv preprint\narXiv:2303.13375 (2023).\n39. Labrak, Y., Rouvier, M. & Dufour, R. A zero-shot and few-shot study\nof instruction-ﬁnetuned large language models applied to clinical\nand biomedical tasks. InProc. 2024 Joint International Conference\non Computational Linguistics, Language Resources and Evaluation\n(LREC-COLING 2024)2049–2066 (ELRA and ICCL, 2024).\n40. Jin, H. et al. Llm maybe longlm: Self-extend llm context window\nwithout tuning. InProc. of Machine Learning Research, 235\n22099–22114 (2024).\n41. Ding, Y. et al. LongRoPE: Extending LLM Context Window Beyond 2\nMillion Tokens, arXiv preprint arXiv:2402.13753 (2024).\n42. Xie, Q., Huang, J., Saha, T. & Ananiadou, S. Gretel: Graph contrastive\ntopic enhanced language model for long document extractive\nsummarization. InProc. 29th International Conference on Compu-\ntational Linguistics,6 2 5 9–6269 (International Committee on Com-\nputational Linguistics, 2022).\n43. Jimenez Gutierrez, B. et al. Thinking about GPT-3 in-context learning\nfor biomedical IE? Think again. InFindings of the Association for\nComputational Linguistics: EMNLP 2022,4 4 9 7–4512 (Association\nfor Computational Linguistics, 2022).\n44. Rehana, H. et al. Evaluation of GPT and BERT-based models on\nidentifying protein-protein interactions in biomedical text, arXiv\npreprint arXiv:2303.17728 (2023).\n45. Jin, Q. et al. Biomedical questionanswering: a survey of approaches\nand challenges.ACM Comput. Surv. (CSUR)55,1 –36 (2022).\n46. Singhal, K. et al. Large language models encode clinical knowl-\nedge, Nature 620,1 –9( 2 0 2 3 ) .\n47. Chang, Y. et al. A survey on evaluation of large language models,\nACM Trans. Intell. Syst. Technol.( 2 0 2 3 ) .\n48. Minaee, S. et al. Large language models: A survey, arXiv preprint\narXiv:2402.06196 (2024).\n49. Chen, Z. et al. Meditron-70b: Scaling medical pretraining for large\nlanguage models, arXiv preprint arXiv:2311.16079 (2023).\n50. DeYoung, J., Beltagy, I., van Zuylen, M., Kuehl, B. & Wang, L. L. Ms2:\nMulti-document summarization of medical studies. InProc. 2021\nConference on Empirical Methods in Natural Language Processing,\n7494–7513 (2021).\n51. Wornow, M. et al. The shaky foundations of large language models\nand foundation models for electronic health records.npj Digit. Med.\n6, 135 (2023).\n52. Manyika, J. An overview of Bard: an early experiment with gen-\nerative AI.https://ai.google/static/documents/google-about-bard.\npdf (2023).\n53. Jiang, A. Q. et al. Mistral 7B, arXiv preprint arXiv:2310.06825, (2023).\n54. Neelakantan, A. et al. Text and code embeddings by contrastive\npre-training, arXiv preprint arXiv:2201.10005 (2022).\n55. Krallinger, M. et al. Overview of the BioCreative VI chemical-protein\ninteraction Track. InProc. of the sixth BioCreative challenge eva-\nluation workshopVol. 1, 141–146 (2017).\n56. Chen, Q. et al. Multi-label classiﬁcation for biomedical literature: an\noverview of the BioCreative VII LitCovid Track for COVID-19 litera-\nture topic annotations,Database 2022, baac069 (2022).\n57. Islamaj Doğan, R. et al. Overview of the BioCreative VI Precision\nMedicine Track: mining protein interactions and mutations for\nprecision medicine,Database2019, bay147 (2019).\n58. International Society for Biocuration, Biocuration: Distilling data into\nknowledge,Plos Biol., 16, e2002846 (2018).\n59. Li, J. et al. BioCreative V CDR task corpus: a resource for chemical\ndisease relation extraction,Database,2 0 1 6( 2 0 1 6 ) .\n60. Doğa n ,R .I . ,L e a m a n ,R .&L u ,Z .N C B Id i s e a s ec o r p u s :ar e s o u r c ef o r\ndisease name recognition and concept normalization.J. Biomed.\nInform. 47,1 –10 (2014).\n61. Li, X., Rousseau, J. F., Ding, Y., Song, M. & Lu, W. Understanding\ndrug repurposing from the perspective of biomedical entities and\ntheir evolution: Bibliographic research using aspirin.JMIR Med.\nInform. 8, e16739 (2020).\n6 2 . S e g u r a - B e d m a r ,I . ,M a r t í n e z ,P .&H e r r e r o - Z a z o ,M .S e m e v a l - 2 0 1 3\ntask 9: extraction of drug-drug interactions from biomedical texts\n(ddiextraction 2013). InSecond Joint Conference on Lexical and\nComputational Semantics (* SEM), Volume 2: Proc. Seventh Inter-\nnational Workshop on Semantic Evaluation (SemEval 2013)341–350\n(Association for Computational Linguistics, 2013).\n63. Du, J. et al. ML-Net: multi-label classiﬁcation of biomedical texts\nwith deep neural networks.J. Am. Med. Inform. Assoc.26,\n1279–1285 (2019).\n64. Baker, S. et al. Automatic semantic classiﬁcation of scientiﬁcl i t -\nerature according to the hallmarks of cancer.Bioinformatics32,\n432–440 (2016).\n6 5 . K a d d a r i ,Z . ,M e l l a h ,Y . ,B e r r i c h ,J . ,B o u c h e n t o u f ,T .&B e l k a s m i ,M .G .\nBiomedical question answering: A survey of methods and datasets.\nIn 2020 Fourth International Conference On Intelligent Computing\nin Data Sciences (ICDS)1–8 (IEEE, 2020).\n6 6 . J i n ,D .e ta l .W h a td i s e a s ed o e st h is patient have? A large-scale open\ndomain question answering dataset from medical exams.Appl. Sci.\n11, 6421 (2021).\n67. Jin, Q., Dhingra, B., Liu, Z., Cohen, W. & Lu, X. Pubmedqa: A dataset\nfor biomedical research question answering. InProc. 2019 Con-\nference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Pro-\ncessing,2 5 6 7–2577 (EMNLP-IJCNLP, 2019).\n68. Cohan, A. et al. A discourse-aware attention model for\nabstractive summarization of long documents.In Proc. 2018\nConference of the North American Chapter of the Association\nfor Computational Linguistics: Human Languag Technologies\nVol. 2, 615–621 (2018).\n69. Devaraj, A., Wallace, B. C., Marshall, I. J. & Li, J. J. Paragraph-level\nsimpliﬁcation of medical texts. InProc. 2021 Conference of the\nNorth American Chapter of the Association for Computational Lin-\nguistics: Human Language Technologies4972–4984 (Association\nfor Computational Linguistics, 2021).\n70. Luo, Z., Xie, Q., & Ananiadou, S. Readability controllable biomedical\ndocument summarization. InFindings of the Association for Com-\nputational Linguistics: EMNLP,4 6 6 7–4680 (2022).\n71. Goldsack, T. et al. Overview of the biolaysumm 2024 shared task on\nlay summarization of biomedical research articles. InProc. 23rd\nWorkshop on Biomedical Natural Language Processing\n122–131 (2024).\n72. Lewis, M. et al. Bart: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and comprehension. In\nProc. 58th Annual Meeting of the Association for Computational\nLinguistics, 7871–7880 (2020).\n73. Liu, P. et al. Pre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.ACM Comput.\nSurv. 55,1 –35 (2023).\n74. Hu, Y. et al. Improving large language models for clinical named\nentity recognition via prompt engineering,J. Am. Med. Inform.\nAssoc. 31, ocad259 (2024).\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 15\n75. Wang, L. et al. Investigating the impact of prompt engineering on\nthe performance of large language models for standardizing\nobstetric diagnosis text: comparative study.JMIR Format Res.8,\ne53216 (2024).\n76. Agrawal, M., Hegselmann, S., Lang, H., Kim, Y. & Sontag, D. Large\nlanguage models are few-shot clinical information extractors. In\nProc. 2022 Conference on Empirical Methods in Natural Language\nProcessing,1 9 9 8–2022 (2022).\n77. Keloth, V. K. et al. Advancing entity recognition in biomedicine via\ninstruction tuning of large language models.Bioinformatics40,\nbtae163 (2024).\n78. Hu, E. J. et al. Lora: Low-rank adaptation of large language models,\narXiv preprint arXiv:2106.09685 (2021).\n79. Virtanen, P. et al. SciPy 1.0: fundamental algorithms for scientiﬁc\ncomputing in Python.Nat. Methods17,2 6 1–272 (2020).\n80. Lehman, E. et al. Do we still need clinical language models?\nIn Conference on health, inference, and learning,5 7 8–597\n(PMLR, 2023).\n81. Chen, S. et al. Evaluating the ChatGPT family of models for bio-\nmedical reasoning and classiﬁcation.J. Am. Med. Inform. Assoc.31,\nocad256 (2024).\n82. Chen, Q. et al. A comprehensive benchmark study on biomedical\ntext generation and mining with ChatGPT,bioRxiv, pp. 2023.04.\n19.537463 (2023).\n83. Zhang, S., Cheng, H., Gao, J. & Poon H. Optimizing bi-encoder\nfor named entity recognition via contrastive learning. InProc. 11th\nInternational Conference on Learning Representations,\n(ICLR, 2023).\n84. He, J. et al. Chemical-protein relation extraction with pre-trained\nprompt tuning.Proc IEEE Int. Conf. Healthc. Inform. 2022,\n608–609 (2022).\n85. Mingliang, D., Jijun, T. & Fei, G. Document-level DDI relation\nextraction with document-entity embedding. pp. 392–397.\n8 6 . C h e n ,Q . ,D u ,J . ,A l l o t ,A .&L u ,Z .L i t M C - B E R T :t r a n s f o r m e r - b a s e d\nmulti-label classiﬁcation of biomedical literature with an applica-\ntion on COVID-19 literature curation,IEEE/ACM Trans. Comput. Biol.\nBioinform. 19,2 5 8 4–2595 (2022).\n87. Yasunaga, M. et al. Deep bidirectional language-knowledge graph\npretraining.Adv. Neural Inf. Process. Syst.35,3 7 3 0 9–37323 (2022).\n8 8 . F l o r e s ,L .J .Y . ,H u a n g ,H . ,S h i ,K . ,C h h e a n g ,S .&C o h a n ,A .M e d i c a l\ntext simpliﬁcation: optimizing for readability with unlikelihood\ntraining and reranked beam search decoding. InFindings of the\nAssociation for Computational Linguistics: EMNLP, 4859–4873\n(2023).\n89. Wei, C.-H. et al. Assessing the state of the art in biomedical relation\nextraction: overview of the BioCreative V chemical-disease relation\n(CDR) task.Database2016,b a w 0 3 2( 2 0 1 6 ) .\n90. He, J. et al. Prompt tuning in biomedical relation extraction,J.\nHealthcare Inform. Res.8,1 –19 (2024).\n91. Guo, Z., Wang, P., Wang, Y. & Yu, S. Improving small language\nmodels on PubMedQA via Generative Data Augmentation,arXiv,\n12 (2023).\n92. Koh, H. Y., Ju, J., Liu, M. & Pan, S. An empirical survey on long\ndocument summarization: Datasets, models, and metrics.ACM\nComput. Surv.55,1 –35 (2022).\n93. Bishop, J. A., Xie, Q. & Ananiadou, S. LongDocFACTScore: Evalu-\nating the factuality of long document abstractive summarisation.\nIn Proc. of the 2024 Joint International Conference on Computational\nLinguistics, Language Resources and Evaluation (LREC-COLING\n2024) 10777–10789 (2024).\n94. Wang, L. L, DeYoung, J. & Wallace, B. Overview of MSLR2022: A\nshared task on multidocument summarization for literature reviews.\nIn Proc. Third Workshop on Scholarly Document Processing175–180\n(Association for Computational Linguistics, 2022).\n95. Ondov, B., Attal, K. & Demner-Fushman, D. A survey of automated\nmethods for biomedical text simpliﬁcation. J .A m .M e d .I n f o r m .\nAssoc. 29,1 9 7 6–1988 (2022).\n96. Stricker, J., Chasiotis, A., Kerwer, M. & Günther, A. Scientiﬁc\nabstracts and plain language summaries in psychology: A com-\nparison based on readability indices.PLoS One15,\ne0231160 (2020).\nAcknowledgements\nThis study is supported by the following National Institutes of Health\ngrants: 1R01LM014604 (Q.C., R.A.A., and H.X), 4R00LM014024 (Q.C.),\nR01AG078154 (R.Z., and H.X), 1R01AG066749 (W.J.Z), W81XWH-22-1-\n0164 (W.J.Z), and the Intramural Research Program of the National\nLibrary of Medicine (Q.C., Q.J., P.L., Z.W., and Z.L).\nAuthor contributions\nQ.C., Z.L., and H.X. designed the research. Q.C., Y.H., X.P., Q.X., Q.J.,\nA.G., M.B.S., X.A., P.L., Z.W., V.K.K., K.P., J.H., H.H., F.L., and J.D. per-\nformed experiments and data analysis. Q.C., Z.L., and H.X. wrote and\nedited the manuscript. All authors contributed to the discussion and\nmanuscript preparation.\nFunding\nOpen access funding provided by the National Institutes of Health.\nCompeting interests\nDr. Jingcheng Du and Dr. Hua Xu have research-relatedﬁnancial inter-\nests at Melax Technologies Inc. The remaining authors declare no\ncompeting interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41467-025-56989-2.\nCorrespondenceand requests for materials should be addressed to\nZhiyong Lu or Hua Xu.\nPeer review informationNature Communicationsthanks the anon-\nymous reviewers for their contribution to the peer review of this work. A\npeer reviewﬁle is available.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jur-\nisdictional claims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as\nlong as you give appropriate credit to the original author(s) and the\nsource, provide a link to the Creative Commons licence, and indicate if\nchanges were made. The images or other third party material in this\narticle are included in the article’s Creative Commons licence, unless\nindicated otherwise in a credit line to the material. If material is not\nincluded in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the copyright\nholder. To view a copy of this licence, visithttp://creativecommons.org/\nlicenses/by/4.0/.\nThis is a U.S. Government work and not under copyright protection in the\nUS; foreign copyright protection may apply 2025\nArticle https://doi.org/10.1038/s41467-025-56989-2\nNature Communications|         (2025) 16:3280 16",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7948997616767883
    },
    {
      "name": "Biomedical text mining",
      "score": 0.7152633666992188
    },
    {
      "name": "Benchmarking",
      "score": 0.6964409947395325
    },
    {
      "name": "Data science",
      "score": 0.45486822724342346
    },
    {
      "name": "Process (computing)",
      "score": 0.4431045353412628
    },
    {
      "name": "Natural language processing",
      "score": 0.37339842319488525
    },
    {
      "name": "Artificial intelligence",
      "score": 0.34830066561698914
    },
    {
      "name": "Text mining",
      "score": 0.22799381613731384
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1299303238",
      "name": "National Institutes of Health",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800548410",
      "name": "United States National Library of Medicine",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I919571938",
      "name": "The University of Texas Health Science Center at Houston",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130238516",
      "name": "University of Minnesota",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2800403580",
      "name": "University of Minnesota System",
      "country": "US"
    }
  ]
}