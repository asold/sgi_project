{
  "title": "TransCT: Dual-path Transformer for Low Dose Computed Tomography",
  "url": "https://openalex.org/W3134839993",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1962833114",
      "name": "Zhang Zhi-cheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2746028584",
      "name": "Yu Lequan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2696984509",
      "name": "Liang Xiaokun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1978283486",
      "name": "Zhao Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2101745635",
      "name": "Xing Lei",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2979294594",
    "https://openalex.org/W3002137088",
    "https://openalex.org/W2044774090",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2803224943",
    "https://openalex.org/W2094366314",
    "https://openalex.org/W2804078698",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2796256498",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3098281398",
    "https://openalex.org/W3130411087",
    "https://openalex.org/W2131635146",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W2911459672",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2171697262",
    "https://openalex.org/W2762996341",
    "https://openalex.org/W2584483805",
    "https://openalex.org/W2007919187",
    "https://openalex.org/W3103372211",
    "https://openalex.org/W2402144811",
    "https://openalex.org/W2046119925",
    "https://openalex.org/W2466541558",
    "https://openalex.org/W2611467245",
    "https://openalex.org/W3112965401",
    "https://openalex.org/W2086777366",
    "https://openalex.org/W1964231221",
    "https://openalex.org/W3087119976",
    "https://openalex.org/W2739134101",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2119259145",
    "https://openalex.org/W2476548250",
    "https://openalex.org/W3171125843"
  ],
  "abstract": "Low dose computed tomography (LDCT) has attracted more and more attention in routine clinical diagnosis assessment, therapy planning, etc., which can reduce the dose of X-ray radiation to patients. However, the noise caused by low X-ray exposure degrades the CT image quality and then affects clinical diagnosis accuracy. In this paper, we train a transformer-based neural network to enhance the final CT image quality. To be specific, we first decompose the noisy LDCT image into two parts: high-frequency (HF) and low-frequency (LF) compositions. Then, we extract content features (X_{L_c}) and latent texture features (X_{L_t}) from the LF part, as well as HF embeddings (X_{H_f}) from the HF part. Further, we feed X_{L_t} and X_{H_f} into a modified transformer with three encoders and decoders to obtain well-refined HF texture features. After that, we combine these well-refined HF texture features with the pre-extracted X_{L_c} to encourage the restoration of high-quality LDCT images with the assistance of piecewise reconstruction. Extensive experiments on Mayo LDCT dataset show that our method produces superior results and outperforms other methods.",
  "full_text": "TransCT: Dual-path Transformer for Low Dose\nComputed Tomography\nZhicheng Zhang1( \f), Lequan Yu1,2, Xiaokun Liang1, Wei Zhao1, and Lei Xing1\n1 Department of Radiation Oncology, Stanford University, Stanford, USA.\nzzc623@stanford.edu\n2 Department of Statistics and Actuarial Science, The University of Hong Kong\nAbstract. Low dose computed tomography (LDCT) has attracted more\nand more attention in routine clinical diagnosis assessment, therapy plan-\nning, etc., which can reduce the dose of X-ray radiation to patients.\nHowever, the noise caused by low X-ray exposure degrades the CT im-\nage quality and then aï¬€ects clinical diagnosis accuracy. In this paper, we\ntrain a transformer-based neural network to enhance the ï¬nal CT image\nquality. To be speciï¬c, we ï¬rst decompose the noisy LDCT image into two\nparts: high-frequency (HF) and low-frequency (LF) compositions. Then,\nwe extract content features (XLc ) and latent texture features (XLt ) from\nthe LF part, as well as HF embeddings (XHf ) from the HF part. Further,\nwe feed XLt and XHf into a modiï¬ed transformer with three encoders\nand decoders to obtain well-reï¬ned HF texture features. After that, we\ncombine these well-reï¬ned HF texture features with the pre-extracted\nXLc to encourage the restoration of high-quality LDCT images with the\nassistance of piecewise reconstruction. Extensive experiments on Mayo\nLDCT dataset show that our method produces superior results and out-\nperforms other methods.\n1 Introduction\nComputed tomography (CT) system, as noninvasive imaging equipment, has\nbeen widely used for medical diagnosis and treatment [18,16]. However, concerns\nabout the increase of X-ray radiation risk have become an unavoidable problem\nfor all CT vendors and medical institutions [2]. Since x-ray imaging is mainly\nbased on a photon-noise-dominated process [27], lowering the X-ray dose will\nresult in degraded CT images. Therefore, on the premise of ensuring CT image\nquality, how to reduce the X-ray radiation dose as far as possible becomes a\npromising and signiï¬cant research topic [2].\nCompared to sparse or limited-view CT [33] and other hardware-based strate-\ngies [34], lowering single X-ray exposure dose [11,22] is the most convenient and\naï¬€ordable method. To obtain high-quality LDCT images, previous works can\nbe mainly classiï¬ed into two categories: model-based and data-driven methods.\nThe key to model-based methods is to use a mathematical model for the de-\nscription of each process of CT imaging: noise characteristics in the sinogram\ndomain [15,30], image prior information in the image domain, such as sparsity\narXiv:2103.00634v4  [eess.IV]  5 Jul 2021\n2 Z. Zhang et al.\nin gradient domain [13] and low rank [3], as well as defects in CT hardware\nsystems [32]. This kind of methods are independent of a large training dataset,\nwhile the accuracy of the model depiction limits its performance.\nWith the development of deep learning in medical image reconstruction and\nanalysis [29], many data-driven works have been proposed to reconstruct LDCT\nimages with convolution neural network (CNN) [25]. Kang et al. proposed a\nCNN-based neural network with the assistance of directional wavelets, suggest-\ning the potential of deep learning technique in LDCT. Similarly, Chen et al.\nemployed residual learning to extract noise in the LDCT images and obtain su-\nperior performance [5]. However, these methods need FBP-reconstructed LDCT\nimages as the inputs, which belong to image post-processing. To get rid of the\ninï¬‚uence of traditional analytic algorithms (e.g. FBP), Zhu et al.suggested that\nâ€˜AUTOMAPâ€™ was a direct reconstruction method from the measurement data to\nthe ï¬nal image [35]. Then again, the ï¬rst fully-connected layer as domain trans-\nform has a huge memory requirement, which makes AUTOMAP unavailable for\nlarge-scale CT reconstruction [24]. Besides, many works with the combination of\niterative reconstruction and deep learning have been proposed as deep unrolled\napproaches. This kind of method used CNNs as special regularizations plugged\ninto conventional iterative reconstruction. They not only inherit the advantages\nof the convenient calculation of system matrix in conventional algorithms but\nalso get rid of the complicated manual design regularization [10,7,11].\nDespite the success of CNNs in LDCT reconstruction, CNN-based methods\nheavily rely on cascaded convolution layers to extract high-level features since\nthe convolution operation has its disadvantage of a limited receptive ï¬eld that\nonly perceives local areas. Moreover, this disadvantage makes it diï¬ƒcult for\nCNN-based methods to make full of the similarity across large regions [26,31],\nwhich makes CNN-based methods less eï¬ƒcient in modeling various structural\ninformation in CT images [14]. To overcome this limitation, Transformers [23],\nwhich solely depend on attention mechanisms instead, have emerged as a power-\nful architectures in many ï¬elds, such as natural language processing (NLP) [8],\nimage segmentation [6],image recognition [9], etc. In addition to these high-level\ntasks, Transformer has also been tentatively investigated for some lower-level\ntasks [28,4], which can model all pairwise interactions between image regions\nand capture long-range dependencies by computing interactions between any\ntwo positions, regardless of their positional distance.\nFor image denoising, noise is mainly contained in the high-frequency sub-\nband. Moreover, the remaining low-frequency sub-band not only contains the\nmain image content, but also contains the weakened image textures, which are\nnoise-free. These weakened image textures can be used to help noise removal\nin the high-frequency sub-band. Inspired by this observation, in this paper,\nwe present the ï¬rst work, TransCT, to explore the potential of transformers\nin LDCT imaging. Firstly, we decompose the noisy LDCT image into high-\nfrequency (HF) and low-frequency (LF) parts. To remove the image noise on the\npremise of retaining the image content, we extract the corresponding content\nfeatures (XLc ) and latent texture features ( XLt ) from the LF part. Simultane-\nTransCT 3\nMulti-Head\nAttention\nMulti-Head\nAttention\nMulti-Head\nAttention\n Feed\nForward\nFeed\nForward\nGaussian Filter\nConv+lrelu\nConv+lrelu\nConv+lrelu\nConv+lrelu\nConv+lrelu\nConv+lrelu\nConv+lrelu â€¦\nâ€¦\nâ€¦ â€¦\nFlatten\nConv+lrelu\nConv+lrelu\nConv+lrelu â€¦\nâ€¦\nâ€¦ â€¦\nFlatten Reshape\nConv+lrelu\nSub-Pixel\n3x\n3x\nn16s2 n32s2\nn64s2 n128s2 n256s2\nn64s2\nn256s2\nğ‘¯ğ‘¯Ã— ğ‘¾ğ‘¾\nğ‘¯ğ‘¯Ã— ğ‘¾ğ‘¾\nğ‘¯ğ‘¯Ã— ğ‘¾ğ‘¾\nğ‘¯ğ‘¯\nğŸ‘ğŸ‘ğŸ‘ğŸ‘ Ã— ğ‘¾ğ‘¾\nğŸ‘ğŸ‘ğŸ‘ğŸ‘ Ã—256\nğ‘¯ğ‘¯\nğŸğŸğŸğŸ Ã— ğ‘¾ğ‘¾\nğŸğŸğŸğŸ Ã—256 ğ‘¯ğ‘¯\nğŸğŸğŸğŸ Ã— ğ‘¾ğ‘¾\nğŸğŸğŸğŸ Ã—256\nğ‘¯ğ‘¯\nğŸğŸğŸğŸ Ã— ğ‘¾ğ‘¾\nğŸğŸğŸğŸ Ã—256\nğ‘¯ğ‘¯\nğŸ–ğŸ– Ã— ğ‘¾ğ‘¾\nğŸ–ğŸ– Ã—64\nn256s1\nn256s1\nn256s1\nn256s1\nn64s1\nn64s1\nLow-frequency, ğ‘‹ğ‘‹ğ¿ğ¿\nHigh-frequency , ğ‘‹ğ‘‹ğ»ğ»\nğ‘‹ğ‘‹ğ¿ğ¿ğ‘ğ‘ğ‘\nğ‘‹ğ‘‹ğ¿ğ¿ğ‘ğ‘ğ‘\nğ‘‹ğ‘‹ğ¿ğ¿ğ‘¡ğ‘¡\nğ‘‹ğ‘‹ğ»ğ»ğ‘“ğ‘“\nğ‘†ğ‘†ğ¿ğ¿\nğ‘†ğ‘†ğ»ğ»\nConv+lrelu\nConv+lrelu\nSub-Pixel\nConv+lrelu\nn256s1\nTransformer\nPiecewise\nReconstruction\nğ‘¯ğ‘¯W\nğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ğŸ‘ Ã—256\nğ‘¯ğ‘¯W\nğŸğŸğŸğŸğŸ‘ğŸ‘ Ã—256\nFig. 1.The overall architecture of the proposed TransCT. â€˜ n64s2â€™ means the convolu-\ntion layer has 64 kernels with stride 2. Sub-Pixel layer is the upsampling layer [21].\nously, we extract the corresponding embeddings (XHf ) from the HF part. Since\ntransformers can only use sequences as input, we then convert XLt and XHf\ninto separated sequences as the input of transformer encoder and decoder, re-\nspectively. To preserve the ï¬ne details of the ï¬nal LDCT images, we integrate\nthe output of the transformer decoder and some speciï¬c features from the LF\npart and then piecewise reconstruct high-quality and high-resolution LDCT im-\nages by stages. Extensive experiments on Mayo LDCT dataset demonstrate the\nsuperiority of our method over other methods.\n2 Method\nFig 1 illustrates the overview of our proposed framework. For image denoising,\nan intuitive solution is to decompose the noisy image into HF and LF parts, and\nthen the noise is mainly left in the HF part, which also contains plenty of image\ntextures. However, noise removal only in the HF part breaks the relationship\nbetween the HF and LF parts since there are also weakened latent textures in\nthe LF part with reduced noise. Therefore, we can remove the noise in the HF\npart with the assistance of the latent textures from the LF part. In this work,\ngiven the noisy LDCT image X with the size of HÃ—W, we ï¬rst use a Gaussian\nï¬lter with a standard deviation of 1 .5 to decompose the LDCT image into two\ncompositions: HF part XH and LF part XL.\nX = XH + XL (1)\nTo use the latent textures in XL, we ï¬rstly extract the corresponding con-\ntent fetatures XLc and texture features XLt from XL using shallow two CNNs.\nFurther, we use these texture features and embeddings fromXH to train a trans-\nformer and get high-level features of XH, combined with content features from\nXL to reconstruct the ï¬nal high-quality LDCT image.\n4 Z. Zhang et al.\n2.1 TransCT\nSequence Similar with what other works have done [6], we ï¬rstly employ two\nconvolution layers with stride 2 to obtain low-resolution features from XL, and\nthen set two paths to extract content features XLc1 (H\n8 Ã—W\n8 Ã—64), XLc2 ( H\n16 Ã—\nW\n16 Ã—256) and latent texture feature XLt ( H\n32 Ã—W\n32 Ã—256), respectively. For XH,\nwe employ sub-pixel layer to make XH to be low-resolution images ( H\n16 Ã—W\n16 Ã—\n256), and ï¬nal high-level features XHf can be obtained with three convolution\nlayers. The goal is to get a sequence of moderate dimensions eventually. To take\nadvantage of the characteristic of long-range dependencies of transformers, we\nperform tokenization by reshaping XLt and XHf into two sequences SL, SH,\nrespectively.\nTransformer In this work, we employ a modiï¬ed transformer with three en-\ncoders and three decoders, each encoder includes a multi-head attention module\n(MHSA) and a feed-forward layer (MLP) and each decoder consists of two multi-\nhead attention modules and a feed-forward layer, as can be seen in Fig 1. For\ntransformer encoder, we use SL (WH\n1024 Ã—256) as the input token, followed by a\nmulti-head attention module to seek the global relationship across large regions,\nand then we use two fully-connected layers (whose number of the node are 8 c\nand c, respectively. c is the dimension of the input sequence) to increase the\nexpressive power of the entire network.\nZ = MHSA(Siâˆ’1\nL ) + Siâˆ’1\nL\nSi\nL = MLP(Z) + Z\ns.t. i âˆˆ{1,2,3}\n(2)\nAfter acquiring the latent texture features S3\nL from XL, we feed SH (WH\n256 Ã—\n256) into the ï¬rst multi-head attention module and treatS3\nL as the key and value\nof each transformer decoder in the second multi-head attention module.\nZ = MHSA(Siâˆ’1\nH ) + Siâˆ’1\nH\nZ = MHSA(Z,S3\nL,S3\nL) + Z\nSi\nH = MLP(Z) + Z\ns.t. i âˆˆ{1,2,3}\n(3)\nPiecewise Reconstruction Since the transformer only output features Y, we\ncombine Y with XLc1 , XLc2 to piecewise reconstruct the ï¬nal high-quality LDCT\nimages. In our work, the output of the transformer has the size of H\n16 Ã—W\n16 Ã—256.\nHere, we reconstruct the high-resolution LDCT image piecewise. In the ï¬rst\nstep, we add Y and XLc2 and then feed the output into a ResNet with two\nâ€˜Conv2d + Leaky-ReLU(lrelu)â€™ layers, followed by a sub-pixel layer which results\nin higher-resolution features with size of H\n8 Ã—W\n8 Ã—64. Similarly, we add these\nhigher-resolution features and XLc1 . After another ResNet with two â€˜Conv2d\nTransCT 5\n+ lreluâ€™ layers and sub-pixel layer, we can get the ï¬nal output with the size of\nHÃ—W\n2.2 Loss Function\nThe MSE measures the diï¬€erence between the output and normal dose CT\nimages (NDCT), which reduces the noise in the input LDCT images. Formally,\nthe MSE is deï¬ned as follows:\nmin\nÎ¸\nL= ||IND âˆ’FÎ¸(ILD)||2\n2 (4)\nWhere IND is the NDCT image and ILD is the LDCT image, F is the\nproposed model and Î¸ denotes the network parameters.\n2.3 Implementation\nIn this work, the proposed framework was implemented in python based on\nTensorï¬‚ow [1] library. We used the Adam [12] optimizer to optimize all the pa-\nrameters of the framework. We totally trained 300 epochs with a mini-batch\nsize of 8. The learning rate was set as 0.0001 in the ï¬rst 180 epochs and then\nreduced to 0.00001 for the next 120 epochs. The conï¬guration of our com-\nputational platform is Intel(R) Core(Tm) i7-7700K CPU @4.20GHZ, 32 GB\nRAM, and a GeForce GTX TITAN X GPU with 12 GB RAM. We initialized\nall the variations with xavier initialization. Our code is publicly available at\nhttps://github.com/zzc623/TransCT\n3 Experiments\nDatasets In this work, we used a publicly released dataset for the 2016 NIH-\nAAPM-Mayo Clinic Low-Dose CT Grand Challenge3 [17]. In this dataset, normal-\ndose abdominal CT images of 1 mm slice thickness were taken from 10 anony-\nmous patients and the corresponding quarter-dose CT images were simulated by\ninserting Poisson noise into the projection data. To better train the proposed\nTransCT, we divided the original 10 training patient cases into 7/1/2 cases,\nrelated to the training/validation/testing datasets, respectively. Before network\ntraining, we converted CT value of each pixel into its corresponding attenuation\nvalue under the assumption that the x-ray source was monochromatic at 60 keV.\nComparison with other methods We compared our method with baseline\nmethods: Non-local Mean (NLM), RED-CNN [5], MAP-NN [19], which are the\nhigh-performance LDCT methods. NLM can be found in the scikit-image li-\nbrary4. Since there is no public well-trained model for RED-CNN and MAP-NN,\nwe re-train these methods with the same dataset.\n3 https://www.aapm.org/GrandChallenge/LowDoseCT/\n4 https://scikit-image.org/\n6 Z. Zhang et al.\n(A) (B) (C)\n(D) (E) (F)\nFig. 2.Visual comparisons from Mayo testing dataset. (A) NDCT, (B) LDCT, (C)\nNLM, (D) RED-CNN, (E) MAP-NN, (F) TransCT. The display window is [-160, 240]\nHU .\nFig 2 shows the results randomly selected from the testing dataset. As com-\npared to LDCT (Fig 2 (B)), NLM and all the DL-based methods can remove\nnoise to a certain extent, while our proposed TransCT is more close to NDCT.\nBy investigating the local region in Fig 3, we can see that the blood vessels (red\narrows) are not obvious with NLM in (Fig 3 (C)). RED-CNN and MAP-NN gen-\nerate, more or less, some additional light tissues (yellow arrow in (Fig 3 (D)))\nand shadows (green arrow in (Fig 3 (E))), respectively.\nQuantitative Analysis To quantitatively compare all the related methods,\nwe conducted 5-fold cross-validation for all methods on Mayo dataset and em-\nployed Root Mean Square Error (RMSE), Structural Similarity (SSIM), and\nVisual Information Fidelity (VIF) [20] as image quality metrics. Among the\nthree metrics, RMSE and SSIM mainly focus on pixel-wise similarity, and VIF\nuses natural statistics models to evaluate psychovisual features of the human\nvisual system. From table 1, we can see that all the related methods improve\nthe image quality on all three metrics. To be speciï¬c, Red-CNN is superior to\nMAP-NN at the pixel-wise level while inferior to MAP-NN in terms of VIF. As\ncompared to LDCT, our TransCT can decrease RMSE by 40.5%, improve SSIM\nby 12.3%, and VIF by 93.7%. For clinical evaluation, limited by clinical ethics,\nwe evaluated all the methods on clinical CBCT images from a real pig head.\nTransCT 7\n(A) (B) (C)\n(D) (E) (F)\nFig. 3.The zoomed regions marked by the red box in Fig. 2 (A). (A) NDCT, (B)\nLDCT, (C) NLM, (D) RED-CNN, (E) MAP-NN, (F) TransCT. The display window\nis [-160, 240] HU .\nThe tube current was: 80mAfor NDCT and 20mAfor LDCT. From table 1, our\nmethod outperforms others with superior robustness.\n3.1 Ablation study\nOn the Inï¬‚uence of Piecewise Reconstruction In this work, after the\noutput of transformer decoder, we used two resnet blocks and two sub-pixel\nlayers to piecewise reconstruct the high-quality high-resolution LDCT image.\nThe goal is to restore image detail more ï¬nely. To evaluate the inï¬‚uence of\npiecewise reconstruction, we modiï¬ed the proposed TransCT and removed the\npiecewise reconstruction. After the output of the third transformer decoder, we\nused a sub-pixel layer to directly reconstruct the noise-free high-resolution HF\ntexture, and then we added this HF texture and XL to obtain the ï¬nal LDCT\nimage. Speciï¬cally, we have removed six convolution layers, including the path of\ncontent extraction (XLc1 and XLc2 ) and four convolution layers in the ï¬nal two\nTable 1.Quantitative results (MEAN Â±SDs) associated with diï¬€erent methods. Red\nand blue indicate the best and the second-best results, respectively.\nDataset LDCT NLM RED-CNN MAP-NN TransCT\nMayo\nRMSE 37.167Â±7.245 25.115 Â±4.54 22.204 Â±3.89 22.492 Â±3.897 22.123Â±3.784\nSSIM 0.822 Â±0.053 0.908 Â±0.031 0.922 Â±0.025 0.921 Â±0.025 0.923 Â±0.024\nVIF 0.079 Â±0.032 0.133 Â±0.037 0.152 Â±0.037 0.150 Â±0.038 0.153 Â±0.039\nPig\nRMSE 50.776 Â±3.7 42.952 Â±5.971 37.551Â±5.334 37.744Â±4.883 36.999 Â±5.25\nSSIM 0.701 Â±0.02 0.799 Â±0.043 0.861 Â±0.03 0.86 Â± 0.027 0.87 Â±0.029\nVIF 0.023 Â±0.002 0.040 Â±0.004 0.066 Â±0.006 0.063 Â±0.006 0.069 Â±0.007\n8 Z. Zhang et al.\n20\n25\n30\n35\n40\n45\n50\n55\n60\n65\n70\n3 30 57 84 111 138 165 192 219 246 273 300\nRMSE(HU)\nWith Piecewise Reconstruction\nWithout Piecewise Reconstruction\n20\n22\n24\n26\n28\n30\n32\n34\n36\n38\n40\n3 30 57 84 111 138 165 192 219 246 273 300\nRMSE(HU)\nc 2c 4c 8c\n(a) (b)\nFig. 4.RMSE results on the validation dataset during the network trainings.\nresnet blocks. Fig 4 (a) shows the RMSE value on the validation dataset at each\nepoch. We can see that in about the ï¬rst 20 epochs, the RMSE from modiï¬ed\nTransCT decreases faster since its model scale is smaller than our TransCT, while\nthe convergence was inferior to our TransCT with piecewise reconstruction.\nOn the Inï¬‚uence of Model Size Generally, larger network size will lead to\nstronger neural network learning ability. In terms of each transformer encoder\nand decoder, which includes a two-layer feed-forward network, respectively, when\nthe dimension of the input sequence is ï¬xed, the dimension of the hidden layer\nin the feed-forward network will determine the network size. Here, we adjusted\nthe dimension of the hidden layer {c,2c,4c}to investigate the inï¬‚uence of model\nsize. From Fig 4 (b), we can see that the smaller the dimension of the hidden\nlayer is, the larger the ï¬‚uctuation of the convergence curve is, the larger the ï¬nal\nconvergent value will be. Therefore, we conclude that larger model results in a\nbetter performance. In this work, we set the dimension of the hidden layer in\nthe feed-forward network at 8c.\nAblation studies on Transformer Module and Dual-path Module To\ninvestigate the eï¬€ectiveness of the transformer module and dual-path mod-\nule, we conducted two additional experiments. First, we used a revised module\n(â€Conv+3Ã—ResNet blocksâ€) to replace the transformer module. We concate-\nnated XHf and the output from the fourth Conv layer (n128s2, before XLt ) and\nthen inputted it into the revised module. As for the dual-path module, we dis-\ncarded the HF path and inputted the XLc2 into 3 transformer encoders, whose\noutput will be combined with XLc1 and XLc2 in the piecewise reconstruction\nstage. The results on the validation dataset were shown in table 2, we can see\nthat our TransCT with transformer module and dual-path module can obtain\nbetter performance.\n4 Conclusion\nInspired by the internal similarity of the LDCT image, we present the ï¬rst\ntransformer-based neural network for LDCT, which can explore large-range de-\npendencies between LDCT pixels. To ease the impact of noise on high-frequency\nTransCT 9\nTable 2.Ablation studies on transformer module and dual-path module conducted on\nthe validation dataset.\nRMSE SSIM VIF\nw/o transformer module 22.62 Â±2.068 0.927Â±0.013 0.13 Â±0.023\nw/o dual-path module 21.711 Â±1.997 0.931Â±0.012 0.14 Â±0.025\nTransCT 21.199 Â±2.054 0.933Â±0.012 0.144Â±0.025\ntexture recovery, we employ a transformer encoder to further excavate the low-\nfrequency part of the latent texture features and then use these texture features\nto restore the high-frequency features from noisy high-frequency parts of LDCT\nimage. The ï¬nal high-quality LDCT image can be piecewise reconstructed with\nthe combination of low-frequency content and high-frequency features. In the\nfuture, we will further explore the learning ability of TransCT and introduce\nself-supervised learning to lower the need for the training dataset.\nAcknowledgements. This work was partially supported by NIH (1 R01CA227713)\nand a Faculty Research Award from Google Inc.\nReferences\n1. Abadi, M., Barham, P., et al.: Tensorï¬‚ow: A system for large-scale machine learn-\ning. In: OSDI. pp. 265â€“283 (2016)\n2. Brenner, D.J., Hall, E.J.: Computed tomographyâ€”an increasing source of radiation\nexposure. N. Engl. J. Med. 357(22), 2277â€“2284 (2007)\n3. Cai, J.F., Jia, X., et al.: Cine cone beam ct reconstruction using low-rank matrix\nfactorization: algorithm and a proof-of-principle study. IEEE Trans. Med. Imag.\n33(8), 1581â€“1591 (2014)\n4. Chen, H., Wang, Y., et al.: Pre-trained image processing transformer. In: CVPR.\npp. 12299â€“12310 (2021)\n5. Chen, H., Zhang, Y., et al.: Low-dose ct with a residual encoder-decoder convolu-\ntional neural network. IEEE Trans. Med. Imag. 36(12), 2524â€“2535 (2017)\n6. Chen, J., Lu, Y., et al.: Transunet: Transformers make strong encoders for medical\nimage segmentation. arXiv:2102.04306 (2021)\n7. Chun, I.Y., Zheng, X., et al.: Bcd-net for low-dose ct reconstruction: Acceleration,\nconvergence, and generalization. In: MICCAI. pp. 31â€“40. Springer (2019)\n8. Devlin, J., Chang, M.W., et al.: Bert: Pre-training of deep bidirectional transform-\ners for language understanding. arXiv:1810.04805 (2018)\n9. Dosovitskiy, A., Beyer, L., et al.: An image is worth 16x16 words: Transformers for\nimage recognition at scale. arXiv:2010.11929 (2020)\n10. Gupta, H., Jin, K.H., et al.: Cnn-based projected gradient descent for consistent\nct image reconstruction. IEEE Trans. Med. Imag. 37(6), 1440â€“1453 (2018)\n11. He, J., Yang, Y., et al.: Optimizing a parameterized plug-and-play admm for iter-\native low-dose ct reconstruction. IEEE Trans. Med. Imag. 38(2), 371â€“382 (2018)\n12. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization.\narXiv:1412.6980 (2014)\n10 Z. Zhang et al.\n13. LaRoque, S.J., Sidky, E.Y., Pan, X.: Accurate image reconstruction from few-view\nand limited-angle data in diï¬€raction tomography. JOSA A25(7), 1772â€“1782 (2008)\n14. Li, M., Hsu, W., et al.: Sacnn: self-attention convolutional neural network for low-\ndose ct denoising with self-supervised perceptual loss network. IEEE Trans. Med.\nImag. 39(7), 2289â€“2301 (2020)\n15. Manduca, A., Yu, L., et al.: Projection space denoising with bilateral ï¬ltering and\nct noise modeling for dose reduction in ct. Med. phys. 36(11), 4911â€“4919 (2009)\n16. Mathews, J.P., Campbell, Q.P., et al.: A review of the application of x-ray com-\nputed tomography to the study of coal. Fuel 209, 10â€“24 (2017)\n17. McCollough, C.H., Bartley, A.C., et al.: Low-dose ct for the detection and classiï¬-\ncation of metastatic liver lesions: Results of the 2016 low dose ct grand challenge.\nMed. phys. 44(10), e339â€“e352 (2017)\n18. Seeram, E.: Computed tomography: physical principles, clinical applications, and\nquality control. Elsevier Health Sciences (2015)\n19. Shan, H., Padole, A., et al.: Competitive performance of a modularized deep neural\nnetwork compared to commercial algorithms for low-dose ct image reconstruction.\nNat. Mach. Intell. 1(6), 269â€“276 (2019)\n20. Sheikh, H.R., Bovik, A.C.: Image information and visual quality. IEEE Trans.\nImage Process. 15(2), 430â€“444 (2006)\n21. Shi, W., Caballero, J., et al.: Real-time single image and video super-resolution\nusing an eï¬ƒcient sub-pixel convolutional neural network. In: CVPR. pp. 1874â€“\n1883 (2016)\n22. Tian, Z., Jia, X., et al.: Low-dose ct reconstruction via edge-preserving total vari-\nation regularization. Phys. Med. Biol. 56(18), 5949 (2011)\n23. Vaswani, A., Shazeer, N., et al.: Attention is all you need. arXiv:1706.03762 (2017)\n24. Wang, G., Ye, J.C., et al.: Image reconstruction is a new frontier of machine learn-\ning. IEEE Trans. Med. Imag. 37(6), 1289â€“1296 (2018)\n25. Wang, G., Ye, J.C., De Man, B.: Deep learning for tomographic image reconstruc-\ntion. Nat. Mach. Intell. 2(12), 737â€“748 (2020)\n26. Wang, X., Girshick, R., et al.: Non-local neural networks. In: CVPR. pp. 7794â€“7803\n(2018)\n27. Xu, Q., Yu, H., et al.: Low-dose x-ray ct reconstruction via dictionary learning.\nIEEE Trans. Med. Imag. 31(9), 1682â€“1697 (2012)\n28. Yang, F., Yang, H., et al.: Learning texture transformer network for image super-\nresolution. In: CVPR. pp. 5791â€“5800 (2020)\n29. Yu, L., Zhang, Z., et al.: Deep sinogram completion with image prior for metal\nartifact reduction in ct images. IEEE Trans. Med. Imag. 40(1), 228â€“238 (2020)\n30. Yu, L., Manduca, A., et al.: Sinogram smoothing with bilateral ï¬ltering for low-dose\nct. In: Medical Imaging 2008: Physics of Medical Imaging. vol. 6913, p. 691329\n31. Zhang, H., Goodfellow, I., et al.: Self-attention generative adversarial networks. In:\nICML. pp. 7354â€“7363 (2019)\n32. Zhang, Z., Yu, L., et al.: Modularized data-driven reconstruction framework for\nnon-ideal focal spot eï¬€ect elimination in computed tomography. Med. Phys. (2021)\n33. Zhang, Z., Liang, X., et al.: A sparse-view ct reconstruction method based on\ncombination of densenet and deconvolution. IEEE Trans. Med. Imag.37(6), 1407â€“\n1417 (2018)\n34. Zhang, Z., Yu, S., et al.: A novel design of ultrafast micro-ct system based on\ncarbon nanotube: a feasibility study in phantom. Phys. Med. 32(10), 1302â€“1307\n(2016)\n35. Zhu, B., Liu, J.Z., et al.: Image reconstruction by domain-transform manifold learn-\ning. Nature 555(7697), 487â€“492 (2018)",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5750919580459595
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5720750093460083
    },
    {
      "name": "Computed tomography",
      "score": 0.5693572163581848
    },
    {
      "name": "Computer science",
      "score": 0.4826217591762543
    },
    {
      "name": "Encoder",
      "score": 0.4500475525856018
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.432525634765625
    },
    {
      "name": "Image quality",
      "score": 0.42364710569381714
    },
    {
      "name": "Medicine",
      "score": 0.3443618416786194
    },
    {
      "name": "Image (mathematics)",
      "score": 0.2510783076286316
    },
    {
      "name": "Radiology",
      "score": 0.24745357036590576
    },
    {
      "name": "Physics",
      "score": 0.21796128153800964
    },
    {
      "name": "Voltage",
      "score": 0.177543044090271
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "topic": "Transformer",
  "institutions": []
}