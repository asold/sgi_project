{
  "title": "Decision Transformer: Reinforcement Learning via Sequence Modeling",
  "url": "https://openalex.org/W3169291081",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1908124851",
      "name": "Chen LiLi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4227136231",
      "name": "Lu, Kevin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221975916",
      "name": "Rajeswaran, Aravind",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3010572687",
      "name": "Lee, Kimin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223065690",
      "name": "Grover, Aditya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3041522122",
      "name": "Laskin, Michael",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743294920",
      "name": "Abbeel, Pieter",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287473691",
      "name": "Srinivas, Aravind",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221404522",
      "name": "Mordatch, Igor",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2992155277",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W3022566517",
    "https://openalex.org/W3132797628",
    "https://openalex.org/W2993398598",
    "https://openalex.org/W3098108114",
    "https://openalex.org/W2990376820",
    "https://openalex.org/W2914261249",
    "https://openalex.org/W3034767611",
    "https://openalex.org/W2970705602",
    "https://openalex.org/W3085190015",
    "https://openalex.org/W3111981703",
    "https://openalex.org/W1757796397",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W3132593015",
    "https://openalex.org/W2798363023",
    "https://openalex.org/W3016525976",
    "https://openalex.org/W2947137205",
    "https://openalex.org/W2145339207",
    "https://openalex.org/W2947150733",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2785379783",
    "https://openalex.org/W3034731451",
    "https://openalex.org/W3102848167",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3122690883",
    "https://openalex.org/W2964268978",
    "https://openalex.org/W2991355586",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2950541952",
    "https://openalex.org/W3103780890",
    "https://openalex.org/W2978455699",
    "https://openalex.org/W2995706821",
    "https://openalex.org/W2970277495",
    "https://openalex.org/W3113184484",
    "https://openalex.org/W2963757175",
    "https://openalex.org/W3122543654",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3127407414",
    "https://openalex.org/W2996125406",
    "https://openalex.org/W2991993771",
    "https://openalex.org/W3034607397",
    "https://openalex.org/W2735574368",
    "https://openalex.org/W2963438456",
    "https://openalex.org/W2948036864",
    "https://openalex.org/W3011120880",
    "https://openalex.org/W2964222296",
    "https://openalex.org/W3035435378",
    "https://openalex.org/W3101192004",
    "https://openalex.org/W2962770929",
    "https://openalex.org/W1491843047",
    "https://openalex.org/W3099551617",
    "https://openalex.org/W3210184670",
    "https://openalex.org/W2990032740",
    "https://openalex.org/W3034786558",
    "https://openalex.org/W3096964654",
    "https://openalex.org/W2971262355",
    "https://openalex.org/W2997876358",
    "https://openalex.org/W2907502844",
    "https://openalex.org/W2741986794",
    "https://openalex.org/W3165994454",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W2963704132"
  ],
  "abstract": "We introduce a framework that abstracts Reinforcement Learning (RL) as a sequence modeling problem. This allows us to draw upon the simplicity and scalability of the Transformer architecture, and associated advances in language modeling such as GPT-x and BERT. In particular, we present Decision Transformer, an architecture that casts the problem of RL as conditional sequence modeling. Unlike prior approaches to RL that fit value functions or compute policy gradients, Decision Transformer simply outputs the optimal actions by leveraging a causally masked Transformer. By conditioning an autoregressive model on the desired return (reward), past states, and actions, our Decision Transformer model can generate future actions that achieve the desired return. Despite its simplicity, Decision Transformer matches or exceeds the performance of state-of-the-art model-free offline RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.",
  "full_text": "Decision Transformer: Reinforcement\nLearning via Sequence Modeling\nLili Chen∗,1, Kevin Lu∗,1, Aravind Rajeswaran2, Kimin Lee1,\nAditya Grover2, Michael Laskin1, Pieter Abbeel1, Aravind Srinivas†,1, Igor Mordatch†,3\n∗equal contribution †equal advising\n1UC Berkeley 2Facebook AI Research 3Google Brain\n{lilichen, kzl}@berkeley.edu\nAbstract\nWe introduce a framework that abstracts Reinforcement Learning (RL) as a se-\nquence modeling problem. This allows us to draw upon the simplicity and scalabil-\nity of the Transformer architecture, and associated advances in language modeling\nsuch as GPT-x and BERT. In particular, we present Decision Transformer, an\narchitecture that casts the problem of RL as conditional sequence modeling. Un-\nlike prior approaches to RL that ﬁt value functions or compute policy gradients,\nDecision Transformer simply outputs the optimal actions by leveraging a causally\nmasked Transformer. By conditioning an autoregressive model on the desired\nreturn (reward), past states, and actions, our Decision Transformer model can gen-\nerate future actions that achieve the desired return. Despite its simplicity, Decision\nTransformer matches or exceeds the performance of state-of-the-art model-free\nofﬂine RL baselines on Atari, OpenAI Gym, and Key-to-Door tasks.\nR s a\na\ns\na\na\ncausal transformer\nemb. + pos. enc.\nlinear decoder\n. . .\n21\n \nreturn state action\n. . .\n^ R^\nFigure 1: Decision Transformer architecture 1. States, actions, and returns are fed into modality-\nspeciﬁc linear embeddings and a positional episodic timestep encoding is added. Tokens are fed into\na GPT architecture which predicts actions autoregressively using a causal self-attention mask.\n1Our code is available at: https://sites.google.com/berkeley.edu/decision-transformer\narXiv:2106.01345v2  [cs.LG]  24 Jun 2021\nContents\n1 Introduction 3\n2 Preliminaries 4\n2.1 Ofﬂine reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n2.2 Transformers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\n3 Method 4\n4 Evaluations on Ofﬂine RL Benchmarks 6\n4.1 Atari . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\n4.2 OpenAI Gym . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7\n5 Discussion 8\n5.1 Does Decision Transformer perform behavior cloning on a subset of the data? . . . 8\n5.2 How well does Decision Transformer model the distribution of returns? . . . . . . 8\n5.3 What is the beneﬁt of using a longer context length? . . . . . . . . . . . . . . . . . 9\n5.4 Does Decision Transformer perform effective long-term credit assignment? . . . . 9\n5.5 Can transformers be accurate critics in sparse reward settings? . . . . . . . . . . . 10\n5.6 Does Decision Transformer perform well in sparse reward settings? . . . . . . . . 10\n5.7 Why does Decision Transformer avoid the need for value pessimism or behavior\nregularization? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n5.8 How can Decision Transformer beneﬁt online RL regimes? . . . . . . . . . . . . . 11\n6 Related Work 11\n6.1 Ofﬂine reinforcement learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11\n6.2 Supervised learning in reinforcement learning settings . . . . . . . . . . . . . . . . 11\n6.3 Credit assignment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n6.4 Conditional language generation . . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n6.5 Attention and transformer models . . . . . . . . . . . . . . . . . . . . . . . . . . 12\n7 Conclusion 12\nA Experimental Details 18\nA.1 Atari . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA.2 OpenAI Gym . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA.2.1 Decision Transformer . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18\nA.2.2 Behavior Cloning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA.3 Graph Shortest Path . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nB Atari Task Scores 20\n2\n1 Introduction\nRecent work has shown transformers [ 1] can model high-dimensional distributions of semantic\nconcepts at scale, including effective zero-shot generalization in language [2] and out-of-distribution\nimage generation [3]. Given the diversity of successful applications of such models, we seek to\nexamine their application to sequential decision making problems formalized as reinforcement\nlearning (RL). In contrast to prior work using transformers as an architectural choice for components\nwithin traditional RL algorithms [ 4, 5], we seek to study if generative trajectory modeling – i.e.\nmodeling the joint distribution of the sequence of states, actions, and rewards – can serve as a\nreplacement for conventional RL algorithms.\nWe consider the following shift in paradigm: instead of training a policy through conventional\nRL algorithms like temporal difference (TD) learning [ 6], we will train transformer models on\ncollected experience using a sequence modeling objective. This will allow us to bypass the need for\nbootstrapping for long term credit assignment – thereby avoiding one of the “deadly triad” [6] known\nto destabilize RL. It also avoids the need for discounting future rewards, as typically done in TD\nlearning, which can induce undesirable short-sighted behaviors. Additionally, we can make use of\nexisting transformer frameworks widely used in language and vision that are easy to scale, utilizing a\nlarge body of work studying stable training of transformer models.\nIn addition to their demonstrated ability to model long sequences, transformers also have other\nadvantages. Transformers can perform credit assignment directly via self-attention, in contrast to\nBellman backups which slowly propagate rewards and are prone to “distractor” signals [7]. This can\nenable transformers to still work effectively in the presence of sparse or distracting rewards. Finally,\nempirical evidence suggest that a transformer modeling approach can model a wide distribution of\nbehaviors, enabling better generalization and transfer [3].\nWe explore our hypothesis by consideringofﬂine RL, where we will task agents with learning policies\nfrom suboptimal data – producing maximally effective behavior from ﬁxed, limited experience. This\ntask is traditionally challenging due to error propagation and value overestimation [8]. However, it is\na natural task when training with a sequence modeling objective. By training an autoregressive model\non sequences of states, actions, and returns, we reduce policy sampling to autoregressive generative\nmodeling. We can specify the expertise of the policy – which “skill” to query – by selecting the\ndesired return tokens, acting as a prompt for generation.\nIllustrative example. To get an intuition for our proposal, consider the task of ﬁnding the shortest\npath on a directed graph, which can be posed as an RL problem. The reward is 0 when the agent is\nat the goal node and −1 otherwise. We train a GPT [9] model to predict next token in a sequence\nof returns-to-go (sum of future rewards), states, and actions. Training only on random walk data –\nwith no expert demonstrations – we can generate optimal trajectories at test time by adding a prior\nto generate highest possible returns (see more details and empirical results in the Appendix) and\nsubsequently generate the corresponding sequence of actions via conditioning. Thus, by combining\nthe tools of sequence modeling with hindsight return information, we achieve policy improvement\nwithout the need for dynamic programming.\n...\ngoal\n-2\n-1\n-3\n-4\n-1\n-3\n-4\n-∞\n-∞ -∞\n-∞\ngraph training dataset (random walks)\ngoal\nstart -3\n-1\ngeneration\n-20 0\n-∞\nFigure 2: Illustrative example of ﬁnding shortest path for a ﬁxed graph (left) posed as reinforcement\nlearning. Training dataset consists of random walk trajectories and their per-node returns-to-go\n(middle). Conditioned on a starting state and generating largest possible return at each node, Decision\nTransformer sequences optimal paths.\n3\nMotivated by this observation, we propose Decision Transformer, where we use the GPT architecture\nto autoregressively model trajectories (shown in Figure 1). We study whether sequence modeling\ncan perform policy optimization by evaluating Decision Transformer on ofﬂine RL benchmarks in\nAtari [10], OpenAI Gym [11], and Key-to-Door [12] environments. We show that – without using\ndynamic programming– Decision Transformer matches or exceeds the performance of state-of-the-art\nmodel-free ofﬂine RL algorithms [13, 14]. Furthermore, in tasks where long-term credit assignment\nis required, Decision Transformer capably outperforms the RL baselines. With this work, we aim to\nbridge sequence modeling and transformers with RL, and hope that sequence modeling serves as a\nstrong algorithmic paradigm for RL.\n2 Preliminaries\n2.1 Ofﬂine reinforcement learning\nWe consider learning in a Markov decision process (MDP) described by the tuple (S, A, P, R). The\nMDP tuple consists of states s ∈S, actions a ∈A, transition dynamics P(s′|s,a), and a reward\nfunction r = R(s,a). We use st, at, and rt = R(st,at) to denote the state, action, and reward\nat timestep t, respectively. A trajectory is made up of a sequence of states, actions, and rewards:\nτ = (s0,a0,r0,s1,a1,r1,...,s T ,aT ,rT ). The return of a trajectory at timestep t, Rt = ∑T\nt′=t rt′ ,\nis the sum of future rewards from that timestep. The goal in reinforcement learning is to learn a policy\nwhich maximizes the expected return E\n[∑T\nt=1 rt\n]\nin an MDP. In ofﬂine reinforcement learning,\ninstead of obtaining data via environment interactions, we only have access to some ﬁxed limited\ndataset consisting of trajectory rollouts of arbitrary policies. This setting is harder as it removes the\nability for agents to explore the environment and collect additional feedback.\n2.2 Transformers\nTransformers were proposed by Vaswani et al.[1] as an architecture to efﬁciently model sequential\ndata. These models consist of stacked self-attention layers with residual connections. Each self-\nattention layer receives nembeddings {xi}n\ni=1 corresponding to unique input tokens, and outputs\nn embeddings {zi}n\ni=1, preserving the input dimensions. The i-th token is mapped via linear\ntransformations to a key ki, query qi, and value vi. The i-th output of the self-attention layer is given\nby weighting the values vj by the normalized dot product between the query qi and other keys kj:\nzi =\nn∑\nj=1\nsoftmax({⟨qi,kj′ ⟩}n\nj′=1)j ·vj. (1)\nAs we shall see later, this allows the layer to assign “credit” by implicitly forming state-return\nassociations via similarity of the query and key vectors (maximizing the dot product). In this work,\nwe use the GPT architecture [ 9], which modiﬁes the transformer architecture with a causal self-\nattention mask to enable autoregressive generation, replacing the summation/softmax over the n\ntokens with only the previous tokens in the sequence ( j ∈[1,i]). We defer the other architecture\ndetails to the original papers.\n3 Method\nIn this section, we present Decision Transformer, which models trajectories autoregressively with\nminimal modiﬁcation to the transformer architecture, as summarized in Figure 1 and Algorithm 1.\nTrajectory representation. The key desiderata in our choice of trajectory representation are that\nit should enable transformers to learn meaningful patterns and we should be able to conditionally\ngenerate actions at test time. It is nontrivial to model rewards since we would like the model to\ngenerate actions based on future desired returns, rather than past rewards. As a result, instead of\nfeeding the rewards directly, we feed the model with the returns-to-go ˆRt = ∑T\nt′=t rt′ . This leads to\nthe following trajectory representation which is amenable to autoregressive training and generation:\nτ =\n(\nˆR1,s1,a1, ˆR2,s2,a2,..., ˆRT ,sT ,aT\n)\n. (2)\n4\nAt test time, we can specify the desired performance (e.g. 1 for success or 0 for failure), as well as\nthe environment starting state, as the conditioning information to initiate generation. After executing\nthe generated action for the current state, we decrement the target return by the achieved reward and\nrepeat until episode termination.\nArchitecture. We feed the last Ktimesteps into Decision Transformer, for a total of 3Ktokens (one\nfor each modality: return-to-go, state, or action). To obtain token embeddings, we learn a linear\nlayer for each modality, which projects raw inputs to the embedding dimension, followed by layer\nnormalization [15]. For environments with visual inputs, the state is fed into a convolutional encoder\ninstead of a linear layer. Additionally, an embedding for each timestep is learned and added to each\ntoken – note this is different than the standard positional embedding used by transformers, as one\ntimestep corresponds to three tokens. The tokens are then processed by a GPT [ 9] model, which\npredicts future action tokens via autoregressive modeling.\nTraining. We are given a dataset of ofﬂine trajectories. We sample minibatches of sequence length\nKfrom the dataset. The prediction head corresponding to the input token st is trained to predict at –\neither with cross-entropy loss for discrete actions or mean-squared error for continuous actions – and\nthe losses for each timestep are averaged. We did not ﬁnd predicting the states or returns-to-go to\nimprove performance, although it is easily permissible within our framework (as shown in Section\n5.4) and would be an interesting study for future work.\nAlgorithm 1 Decision Transformer Pseudocode (for continuous actions)\n# R, s, a, t: returns -to -go , states , actions , or timesteps\n# transformer : transformer with causal masking ( GPT )\n# embed_s , embed_a , embed_R : linear embedding layers\n# embed_t : learned episode positional embedding\n# pred_a : linear action prediction layer\n# main model\ndef DecisionTransformer (R, s, a, t):\n# compute embeddings for tokens\npos_embedding = embed_t (t) # per - timestep ( note : not per - token )\ns_embedding = embed_s (s) + pos_embedding\na_embedding = embed_a (a) + pos_embedding\nR_embedding = embed_R (R) + pos_embedding\n# interleave tokens as (R_1 , s_1 , a_1 , ... , R_K , s_K )\ninput_embeds = stack ( R_embedding , s_embedding , a_embedding )\n# use transformer to get hidden states\nhidden_states = transformer ( input_embeds = input_embeds )\n# select hidden states for action prediction tokens\na_hidden = unstack ( hidden_states ). actions\n# predict action\nreturn pred_a ( a_hidden )\n# training loop\nfor (R, s, a, t) in dataloader : # dims : ( batch_size , K, dim )\na_preds = DecisionTransformer (R, s, a, t)\nloss = mean (( a_preds - a )**2) # L2 loss for continuous actions\noptimizer . zero_grad (); loss . backward (); optimizer . step ()\n# evaluation loop\ntarget_return = 1 # for instance , expert - level return\nR, s, a, t, done = [ target_return ], [ env . reset ()] , [] , [1] , False\nwhile not done : # autoregressive generation / sampling\n# sample next action\naction = DecisionTransformer (R, s, a, t )[ -1] # for cts actions\nnew_s , r, done , _ = env . step ( action )\n# append new tokens to sequence\nR = R + [R[ -1] - r] # decrement returns -to -go with reward\ns, a, t = s + [ new_s ], a + [ action ], t + [ len (R)]\nR, s, a, t = R[-K:] , ... # only keep context length of K\n5\nAtari OpenAI Gym Key-To-Door\n50\n100\nPerformance\nDecision Transformer (Ours) TD Learning Behavior Cloning\nFigure 3: Results comparing Decision Transformer (ours) to TD learning (CQL) and behavior\ncloning across Atari, OpenAI Gym, and Minigrid. On a diverse set of tasks, Decision Transformer\nperforms comparably or better than traditional approaches. Performance is measured by normalized\nepisode return (see text for details).\n4 Evaluations on Ofﬂine RL Benchmarks\nIn this section, we investigate the performance of Decision Transformer relative to dedicated ofﬂine\nRL and imitation learning algorithms. In particular, our primary points of comparison are model-\nfree ofﬂine RL algorithms based on TD-learning, since our Decision Transformer architecture is\nfundamentally model-free in nature as well. Furthermore, TD-learning is the dominant paradigm in\nRL for sample efﬁciency, and also features prominently as a sub-routine in many model-based RL\nalgorithms [16, 17]. We also compare with behavior cloning and variants, since it also involves a\nlikelihood based policy learning formulation similar to ours. The exact algorithms depend on the\nenvironment but our motivations are as follows:\n• TD learning: most of these methods use an action-space constraint or value pessimism, and will\nbe the most faithful comparison to Decision Transformer, representing standard RL methods. A\nstate-of-the-art model-free method is Conservative Q-Learning (CQL) [14] which serves as our\nprimary comparison. In addition, we also compare against other prior model-free RL algorithms\nlike BEAR [18] and BRAC [19].\n• Imitation learning: this regime similarly uses supervised losses for training, rather than Bellman\nbackups. We use behavior cloning here, and include a more detailed discussion in Section 5.1.\nWe evaluate on both discrete (Atari [10]) and continuous (OpenAI Gym [ 11]) control tasks. The\nformer involves high-dimensional observation spaces and requires long-term credit assignment, while\nthe latter requires ﬁne-grained continuous control, representing a diverse set of tasks. Our main results\nare summarized in Figure 3, where we show averaged normalized performance for each domain.\n4.1 Atari\nThe Atari benchmark [10] is challenging due to its high-dimensional visual inputs and difﬁculty of\ncredit assignment arising from the delay between actions and resulting rewards. We evaluate our\nmethod on 1% of all samples in the DQN-replay dataset as per Agarwal et al. [13], representing 500\nthousand of the 50 million transitions observed by an online DQN agent [ 20] during training; we\nreport the mean and standard deviation of 3 seeds. We normalize scores based on a professional\ngamer, following the protocol of Hafner et al. [21], where 100 represents the professional gamer\nscore and 0 represents a random policy.\nWe compare to CQL [14], REM [13], and QR-DQN [22] on four Atari tasks (Breakout, Qbert, Pong,\nand Seaquest) that are evaluated in Agarwal et al. [13]. We use context lengths of K = 30 for\nDecision Transformer (except K = 50for Pong). We also report the performance of behavior cloning\n(BC), which utilizes the same network architecture and hyperparameters as Decision Transformer\nbut does not have return-to-go conditioning2. For CQL, REM, and QR-DQN baselines, we report\nnumbers directly from the CQL and REM papers. We show results in Table 1. Our method is\ncompetitive with CQL in 3 out of 4 games and outperforms or matches REM, QR-DQN, and BC on\nall 4 games.\n2We also tried using an MLP with K = 1as in prior work, but found this was worse than the transformer.\n6\nGame DT (Ours) CQL QR-DQN REM BC\nBreakout 267.5 ± 97.5 211.1 17 .1 8 .9 138 .9 ± 61.7\nQbert 15.4 ± 11.4 104.2 0.0 0 .0 17 .3 ± 14.7\nPong 106.1 ± 8.1 111.9 18.0 0 .5 85 .2 ± 20.0\nSeaquest 2.5 ± 0.4 1.7 0 .4 0 .7 2 .1 ± 0.3\nTable 1: Gamer-normalized scores for the 1% DQN-replay Atari dataset. We report the mean\nand variance across 3 seeds. Best mean scores are highlighted in bold. Decision Transformer (DT)\nperforms comparably to CQL on 3 out of 4 games, and outperforms other baselines in most games.\n4.2 OpenAI Gym\nIn this section, we consider the continuous control tasks from the D4RL benchmark [23]. We also\nconsider a 2D reacher environment that is not part of the benchmark, and generate the datasets using\na similar methodology to the D4RL benchmark. Reacher is a goal-conditioned task and has sparse\nrewards, so it represents a different setting than the standard locomotion environments (HalfCheetah,\nHopper, and Walker). The different dataset settings are described below.\n1. Medium: 1 million timesteps generated by a “medium” policy that achieves approximately\none-third the score of an expert policy.\n2. Medium-Replay: the replay buffer of an agent trained to the performance of a medium policy\n(approximately 25k-400k timesteps in our environments).\n3. Medium-Expert: 1 million timesteps generated by the medium policy concatenated with 1 million\ntimesteps generated by an expert policy.\nWe compare to CQL [14], BEAR [18], BRAC [19], and AWR [24]. CQL represents the state-of-\nthe-art in model-free ofﬂine RL, an instantiation of TD learning with value pessimism. Score are\nnormalized so that 100 represents an expert policy, as per Fu et al. [23]. CQL numbers are reported\nfrom the original paper; BC numbers are run by us; and the other methods are reported from the\nD4RL paper. Our results are shown in Table 2. Decision Transformer achieves the highest scores in a\nmajority of the tasks and is competitive with the state of the art in the remaining tasks.\nDataset Environment DT (Ours) CQL BEAR BRAC-v A WR BC\nMedium-Expert HalfCheetah 86.8 ± 1.3 62.4 53 .4 41 .9 52 .7 59 .9\nMedium-Expert Hopper 107.6 ± 1.8 111.0 96.3 0 .8 27 .1 79 .6\nMedium-Expert Walker 108.1 ± 0.2 98.7 40 .1 81 .6 53 .8 36 .6\nMedium-Expert Reacher 89.1 ± 1.3 30.6 - - - 73.3\nMedium HalfCheetah 42.6 ± 0.1 44 .4 41 .7 46.3 37.4 43 .1\nMedium Hopper 67.6 ± 1.0 58.0 52 .1 31 .1 35 .9 63 .9\nMedium Walker 74.0 ± 1.4 79 .2 59 .1 81.1 17.4 77 .3\nMedium Reacher 51.2 ± 3.4 26.0 - - - 48.9\nMedium-Replay HalfCheetah 36.6 ± 0.8 46 .2 38 .6 47.7 40.3 4 .3\nMedium-Replay Hopper 82.7 ± 7.0 48.6 33 .7 0 .6 28 .4 27 .6\nMedium-Replay Walker 66.6 ± 3.0 26.7 19 .2 0 .9 15 .5 36 .9\nMedium-Replay Reacher 18.0 ± 2.4 19 .0 - - - 5.4\nAverage (Without Reacher) 74.7 63.9 48 .2 36 .9 34 .3 46 .4\nAverage (All Settings) 69.2 54.2 - - - 47.7\nTable 2: Results for D4RL datasets 3. We report the mean and variance for three seeds. Decision\nTransformer (DT) outperforms conventional RL algorithms on almost all tasks.\n3Given that CQL is generally the strongest TD learning method, for Reacher we only run the CQL baseline.\n7\n5 Discussion\n5.1 Does Decision Transformer perform behavior cloning on a subset of the data?\nIn this section, we seek to gain insight into whether Decision Transformer can be thought of as\nperforming imitation learning on a subset of the data with a certain return. To investigate this, we\npropose a new method, Percentile Behavior Cloning (%BC), where we run behavior cloning on only\nthe top X% of timesteps in the dataset, ordered by episode returns. The percentile X% interpolates\nbetween standard BC (X = 100%) that trains on the entire dataset and only cloning the best observed\ntrajectory (X →0%), trading off between better generalization by training on more data with training\na specialized model that focuses on a desirable subset of the data.\nWe show full results comparing %BC to Decision Transformer and CQL in Table 3, sweeping over\nX ∈[10%,25%,40%,100%]. Note that the only way to choose the optimal subset for cloning is to\nevaluate using rollouts from the environment, so %BC is not a realistic approach; rather, it serves to\nprovide insight into the behavior of Decision Transformer. When data is plentiful – as in the D4RL\nregime – we ﬁnd %BC can match or beat other ofﬂine RL methods. On most environments, Decision\nTransformer is competitive with the performance of the best %BC, indicating it can hone in on a\nparticular subset after training on the entire dataset distribution.\nDataset Environment DT (Ours) 10%BC 25%BC 40%BC 100%BC CQL\nMedium HalfCheetah 42.6 ± 0.1 42 .9 43 .0 43 .1 43 .1 44.4\nMedium Hopper 67.6 ± 1.0 65.9 65 .2 65 .3 63 .9 58 .0\nMedium Walker 74.0 ± 1.4 78 .8 80.9 78.8 77 .3 79 .2\nMedium Reacher 51.2 ± 3.4 51 .0 48 .9 58 .2 58.4 26.0\nMedium-Replay HalfCheetah 36.6 ± 0.8 40 .8 40 .9 41 .1 4 .3 46.2\nMedium-Replay Hopper 82.7 ± 7.0 70.6 58 .6 31 .0 27 .6 48 .6\nMedium-Replay Walker 66.6 ± 3.0 70.4 67.8 67 .2 36 .9 26 .7\nMedium-Replay Reacher 18.0 ± 2.4 33.1 16.2 10 .7 5 .4 19 .0\nAverage 56.1 56.7 52.7 49 .4 39 .5 43 .5\nTable 3: Comparison between Decision Transformer (DT) and Percentile Behavior Cloning (%BC).\nIn contrast, when we study low data regimes – such as Atari, where we use 1% of a replay buffer\nas the dataset – %BC is weak (shown in Table 4). This suggests that in scenarios with relatively\nlow amounts of data, Decision Transformer can outperform %BC by using all trajectories in the\ndataset to improve generalization, even if those trajectories are dissimilar from the return conditioning\ntarget. Our results indicate that Decision Transformer can be more effective than simply performing\nimitation learning on a subset of the dataset. On the tasks we considered, Decision Transformer either\noutperforms or is competitive to %BC, without the confound of having to select the optimal subset.\nGame DT (Ours) 10%BC 25%BC 40%BC 100%BC\nBreakout 267.5 ± 97.5 28.5 ± 8.2 73 .5 ± 6.4 108 .2 ± 67.5 138 .9 ± 61.7\nQbert 15.4 ± 11.4 6 .6 ± 1.7 16 .0 ± 13.8 11 .8 ± 5.8 17.3 ± 14.7\nPong 106.1 ± 8.1 2.5 ± 0.2 13 .3 ± 2.7 72 .7 ± 13.3 85 .2 ± 20.0\nSeaquest 2.5 ± 0.4 1.1 ± 0.2 1 .1 ± 0.2 1 .6 ± 0.4 2 .1 ± 0.3\nTable 4: %BC scores for Atari. We report the mean and variance across 3 seeds. Decision Transformer\n(DT) outperforms all versions of %BC in most games.\n5.2 How well does Decision Transformer model the distribution of returns?\nWe evaluate the ability of Decision Transformer to understand return-to-go tokens by varying the\ndesired target return over a wide range – evaluating the multi-task distribution modeling capability of\ntransformers. Figure 4 shows the average sampled return accumulated by the agent over the course of\nthe evaluation episode for varying values of target return. On every task, the desired target returns\nand the true observed returns are highly correlated. On some tasks like Pong, HalfCheetah and\nWalker, Decision Transformer generates trajectories that almost perfectly match the desired returns\n8\n(as indicated by the overlap with the oracle line). Furthermore, on some Atari tasks like Seaquest, we\ncan prompt the Decision Transformer with higher returns than the maximum episode return available\nin the dataset, demonstrating that Decision Transformer is sometimes capable of extrapolation.\n0 100 200 300\n0\n100\n200\n300\nPerformance\nBreakout\n0 20 40 60 80 100\n0\n50\n100\nQbert\n0 50 100 150\n0\n50\n100\n150\nPong\n0 1 2 3\n0\n1\n2\n3\nSeaquest\n0 10 20 30 40 50\nTarget Return (Normalized)\n0\n20\n40\nPerformance\nHalfCheetah\n0 25 50 75 100\nTarget Return (Normalized)\n0\n50\n100\nHopper\n0 25 50 75 100\nTarget Return (Normalized)\n0\n50\n100\nWalker\n0 5 10 15 20 25\nTarget Return (Normalized)\n0\n10\n20\nReacher\n0.04\n 0.02\n 0.00 0.02 0.04\n0.05\n0.00\n0.05\nDecision Transformer Oracle Best Trajectory in Dataset\nFigure 4: Sampled (evaluation) returns accumulated by Decision Transformer when conditioned on\nthe speciﬁed target (desired) returns. Top: Atari. Bottom: D4RL medium-replay datasets.\n5.3 What is the beneﬁt of using a longer context length?\nTo assess the importance of access to previous states, actions, and returns, we ablate on the context\nlength K. This is interesting since it is generally considered that the previous state (i.e. K = 1) is\nenough for reinforcement learning algorithms when frame stacking is used, as we do. Table 5 shows\nthat performance of Decision Transformer is signiﬁcantly worse when K = 1, indicating that past\ninformation is useful for Atari games. One hypothesis is that when we are representing a distribution\nof policies – like with sequence modeling – the context allows the transformer to identify which\npolicy generated the actions, enabling better learning and/or improving the training dynamics.\nGame DT (Ours) DT with no context ( K = 1)\nBreakout 267.5 ± 97.5 73.9 ± 10\nQbert 15.1 ± 11.4 13.6 ± 11.3\nPong 106.1 ± 8.1 2.5 ± 0.2\nSeaquest 2.5 ± 0.4 0.6 ± 0.1\nTable 5: Ablation on context length. Decision Transformer (DT) performs better when using a longer\ncontext length (K = 50for Pong, K = 30for others).\n5.4 Does Decision Transformer perform effective long-term credit assignment?\nTo evaluate long-term credit assignment capabilities of our model, we consider a variant of the\nKey-to-Door environment proposed in Mesnard et al. [12]. This is a grid-based environment with a\nsequence of three phases: (1) in the ﬁrst phase, the agent is placed in a room with a key; (2) then, the\nagent is placed in an empty room; (3) and ﬁnally, the agent is placed in a room with a door. The agent\nreceives a binary reward when reaching the door in the third phase, butonly if it picked up the key\nin the ﬁrst phase. This problem is difﬁcult for credit assignment because credit must be propagated\nfrom the beginning to the end of the episode, skipping over actions taken in the middle.\nWe train on datasets of trajectories generated by applying random actions and report success rates\nin Table 6. Furthermore, for the Key-to-Door environment we use the entire episode length as the\ncontext, rather than having a ﬁxed content window as in the other environments. Methods that use\nhighsight return information: our Decision Transformer model and %BC (trained only on successful\nepisodes) are able to learn effective policies – producing near-optimal paths, despite only training\non random walks. TD learning (CQL) cannot effectively propagate Q-values over the long horizons\ninvolved and gets poor performance.\n9\nDataset DT (Ours) CQL BC %BC Random\n1K Random Trajectories 71.8% 13 .1% 1 .4% 69 .9% 3 .1%\n10K Random Trajectories 94.6% 13 .3% 1 .6% 95.1% 3 .1%\nTable 6: Success rate for Key-to-Door environment. Methods using hindsight (Decision Transformer,\n%BC) can learn successful policies, while TD learning struggles to perform credit assignment.\n5.5 Can transformers be accurate critics in sparse reward settings?\nIn previous sections, we established that decision transformer can produce effective policies (actors).\nWe now evaluate whether transformer models can also be effective critics. We modify Decision\nTransformer to output return tokens in addition to action tokens on the Key-to-Door environment.\nAdditionally, the ﬁrst return token is not given, but it is predicted instead (i.e. the model learns the\ninitial distribution p( ˆR1)), similar to standard autoregressive generative models. We ﬁnd that the\ntransformer continuously updates reward probability based on events during the episode, shown in\nFigure 5 (Left). Furthermore, we ﬁnd the transformer attends to critical events in the episode (picking\nup the key or reaching the door), shown in Figure 5 (Right), indicating formation of state-reward\nassociations as discussed in Raposo et al. [25] and enabling accurate value prediction.\nkey room distractor room door room\nEpisode time\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Predicted reward probability\nnot pick up key\npick up key and reach door\npick up key and not reach door\npick up key reach door\nEpisode time\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Attention weight\nFigure 5: Left: Averages of running return probabilities predicted by the transformer model for three\ntypes of episode outcomes. Right: Transformer attention weights from all timesteps superimposed\nfor a particular successful episode. The model attends to steps near pivotal events in the episode, such\nas picking up the key and reaching the door.\n5.6 Does Decision Transformer perform well in sparse reward settings?\nA known weakness of TD learning algorithms is that they require densely populated rewards in order\nto perform well, which can be unrealistic and/or expensive. In contrast, Decision Transformer can\nimprove robustness in these settings since it makes minimal assumptions on the density of the reward.\nTo evaluate this, we consider a delayed return version of the D4RL benchmarks where the agent\ndoes not receive any rewards along the trajectory, and instead receives the cumulative reward of the\ntrajectory in the ﬁnal timestep. Our results for delayed returns are shown in Table 7. Delayed returns\nminimally affect Decision Transformer; and due to the nature of the training process, while imitation\nlearning methods are reward agnostic. While TD learning collapses, Decision Transformer and %BC\nstill perform well, indicating that Decision Transformer can be more robust to delayed rewards.\nDelayed (Sparse) Agnostic Original (Dense)\nDataset Environment DT (Ours) CQL BC %BC DT (Ours) CQL\nMedium-Expert Hopper 107.3 ± 3.5 9.0 59.9 102 .6 107.6 111 .0\nMedium Hopper 60.7 ± 4.5 5 .2 63.9 65.9 67.6 58 .0\nMedium-Replay Hopper 78.5 ± 3.7 2.0 27.6 70 .6 82.7 48 .6\nTable 7: Results for D4RL datasets with delayed (sparse) reward. Decision Transformer (DT) and\nimitation learning are minimally affected by the removal of dense rewards, while CQL fails.\n10\n5.7 Why does Decision Transformer avoid the need for value pessimism or behavior\nregularization?\nOne key difference between Decision Transformer and prior ofﬂine RL algorithms is that we do\nnot require policy regularization or conservatism to achieve good performance. Our conjecture is\nthat TD-learning based algorithms learn an approximate value function and improve the policy by\noptimizing this value function. This act of optimizing a learned function can exacerbate and exploit\nany inaccuracies in the value function approximation, causing failures in policy improvement. Since\nDecision Transformer does not require explicit optimization using learned functions as objectives, it\navoids the need for regularization or conservatism.\n5.8 How can Decision Transformer beneﬁt online RL regimes?\nOfﬂine RL and the ability to model behaviors has the potential to enable sample-efﬁcient online\nRL for downstream tasks. Works studying the transition from ofﬂine to online generally ﬁnd that\nlikelihood-based approaches, like our sequence modeling objective, are more successful [ 26, 27].\nAs a result, although we studied ofﬂine RL in this work, we believe Decision Transformer can\nmeaningfully improve online RL methods by serving as a strong model for behavior generation. For\ninstance, Decision Transformer can serve as a powerful “memorization engine” and in conjunction\nwith powerful exploration algorithms like Go-Explore [28], has the potential to simultaneously model\nand generative a diverse set of behaviors.\n6 Related Work\n6.1 Ofﬂine reinforcement learning\nTo mitigate the impact of distribution shift in ofﬂine RL, prior algorithms either (a) constrain the\npolicy action space [ 29, 30, 31] or (b) incorporate value pessimism [ 29, 14], or (c) incorporate\npessimism into learned dynamics models [ 32, 33]. Since we do not use Decision Transformers\nto explicitly learn the dynamics model, we primarily compare against model-free algorithms in\nour work; in particular, adding a dynamics model tends to improve the performance of model-free\nalgorithms. Another line of work explores learning wide behavior distribution from an ofﬂine dataset\nby learning a task-agnostic set of skills, either with likelihood-based approaches [34, 35, 36, 37] or by\nmaximizing mutual information [38, 39, 40]. Our work is similar to the likelihood-based approaches,\nwhich do not use iterative Bellman updates – although we use a simpler sequence modeling objective\ninstead of a variational method, and use rewards for conditional generation of behaviors.\n6.2 Supervised learning in reinforcement learning settings\nSome prior methods for reinforcement learning bear more resemblance to static supervised learning,\nsuch as Q-learning [41, 42], which still uses iterative backups, or likelihood-based methods such as\nbehavior cloning, which do not (discussed in previous section). Recent work [ 43, 44, 45] studies\n“upside-down” reinforcement learning (UDRL), which are similar to our method in seeking to model\nbehaviors with a supervised loss conditioned on the target return. A key difference in our work is the\nshift of motivation to sequence modeling rather than supervised learning: while the practical methods\ndiffer primarily in the context length and architecture, sequence modeling enables behavior modeling\neven without access to the reward, in a similar style to language [9] or images [46], and is known to\nscale well [2]. The method proposed by Kumar et al. [44] is most similar to our method with K = 1,\nwhich we ﬁnd sequence modeling/long contexts to outperform (see Section 5.3). Ghosh et al. [47]\nextends prior UDRL methods to use state goal conditioning, rather than rewards, and Paster et al.\n[48] further use an LSTM with state goal conditioning for goal-conditoned online RL settings.\nConcurrent to our work, Janner et al. [49] propose Trajectory Transformer, which is similar to\nDecision Transformer but additionally uses state and return prediction, as well as discretization,\nwhich incorporates model-based components. We believe that their experiments, in addition to\nour results, highlight the potential for sequence modeling to be a generally applicable idea for\nreinforcement learning.\n11\n6.3 Credit assignment\nMany works have studied better credit assignment via state-association, learning an architecture\nwhich decomposes the reward function such that certain “important” states comprise most of the credit\n[50, 51, 12]. They use the learned reward function to change the reward of an actor-critic algorithm to\nhelp propagate signal over long horizons. In particular, similar to our long-term setting, some works\nhave speciﬁcally shown such state-associative architectures can perform better in delayed reward\nsettings [52, 7, 53, 25]. In contrast, we allow these properties to naturally emerge in a transformer\narchitecture, without having to explicitly learn a reward function or a critic.\n6.4 Conditional language generation\nVarious works have studied guided generation for images [ 54] and language [ 55, 56]. Several\nworks [57, 58, 59, 60, 61, 62] have explored training or ﬁne-tuning of models for controllable text\ngeneration. Class-conditional language models can also be used to learn disciminators to guide\ngeneration [63, 55, 64, 65]. However, these approaches mostly assume constant “classes”, while in\nreinforcement learning the reward signal is time-varying. Furthermore, it is more natural to prompt\nthe model desired target return and continuously decrease it by the observed rewards over time, since\nthe transformer model and environment jointly generate the trajectory.\n6.5 Attention and transformer models\nTransformers [1] have been applied successfully to many tasks in natural language processing\n[66, 9] and computer vision [67, 68]. However, transformers are relatively unstudied in RL, mostly\ndue to differing nature of the problem, such as higher variance in training. Zambaldi et al. [5]\nshowed that augmenting transformers with relational reasoning improve performance in combinatorial\nenvironments and Ritter et al. [69] showed iterative self-attention allowed for RL agents to better\nutilize episodic memories. Parisotto et al. [4] discussed design decisions for more stable training of\ntransformers in the high-variance RL setting. Unlike our work, these still use actor-critic algorithms\nfor optimization, focusing on novelty in architecture. Additionally, in imitation learning, some\nworks have studied transformers as a replacement for LSTMs: Dasari and Gupta [70] study one-\nshot imitation learning, and Abramson et al. [71] combine language and image modalities for\ntext-conditioned behavior generation.\n7 Conclusion\nWe proposed Decision Transformer, seeking to unify ideas in language/sequence modeling and\nreinforcement learning. On standard ofﬂine RL benchmarks, we showed Decision Transformer can\nmatch or outperform strong algorithms designed explicitly for ofﬂine RL with minimal modiﬁcations\nfrom standard language modeling architectures.\nWe hope this work inspires more investigation into using large transformer models for RL. We\nused a simple supervised loss that was effective in our experiments, but applications to large-scale\ndatasets could beneﬁt from self-supervised pretraining tasks. In addition, one could consider more\nsophisticated embeddings for returns, states, and actions – for instance, conditioning on return\ndistributions to model stochastic settings instead of deterministic returns. Transformer models\ncan also be used to model the state evolution of trajectory, potentially serving as an alternative to\nmodel-based RL, and we hope to explore this in future work.\nFor real-world applications, it is important to understand the types of errors transformers make in\nMDP settings and possible negative consequences, which are underexplored. It will also be important\nto consider the datasets we train models on, which can potentially add destructive biases, particularly\nas we consider studying augmenting RL agents with more data which may come from questionable\nsources. For instance, reward design by nefarious actors can potentially generate unintended behaviors\nas our model generates behaviors by conditioning on desired returns.\n12\nAcknowledgements\nThis research was supported by Berkeley Deep Drive, Open Philanthropy, and the National Science\nFoundation under NSF:NRI #2024675. Part of this work was completed when Aravind Rajeswaran\nwas a PhD student at the University of Washington, where he was supported by the J.P. Morgan PhD\nFellowship in AI (2020-21). We also thank Luke Metz and Daniel Freeman for valuable feedback and\ndiscussions, as well as Justin Fu for assistance in setting up D4RL benchmarks, and Aviral Kumar for\nassistance with the CQL baselines and hyperparameters.\n13\nReferences\n[1] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Informa-\ntion Processing Systems, 2017.\n[2] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. arXiv preprint arXiv:2005.14165, 2020.\n[3] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea V oss, Alec Radford, Mark\nChen, and Ilya Sutskever. Zero-shot text-to-image generation. arXiv preprint arXiv:2102.12092,\n2021.\n[4] Emilio Parisotto, Francis Song, Jack Rae, Razvan Pascanu, Caglar Gulcehre, Siddhant Jayaku-\nmar, Max Jaderberg, Raphael Lopez Kaufman, Aidan Clark, Seb Noury, et al. Stabilizing\ntransformers for reinforcement learning. In International Conference on Machine Learning,\n2020.\n[5] Vinicius Zambaldi, David Raposo, Adam Santoro, Victor Bapst, Yujia Li, Igor Babuschkin, Karl\nTuyls, David Reichert, Timothy Lillicrap, Edward Lockhart, et al. Deep reinforcement learning\nwith relational inductive biases. In International Conference on Learning Representations ,\n2018.\n[6] Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT Press,\n2018.\n[7] Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico\nCarnevale, Arun Ahuja, and Greg Wayne. Optimizing agent behavior over long time scales by\ntransporting value. Nature communications, 10(1):1–12, 2019.\n[8] Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. Ofﬂine reinforcement learning:\nTutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643, 2020.\n[9] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language\nunderstanding by generative pre-training. 2018.\n[10] Marc G Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning\nenvironment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence\nResearch, 47:253–279, 2013.\n[11] Greg Brockman, Vicki Cheung, Ludwig Pettersson, Jonas Schneider, John Schulman, Jie Tang,\nand Wojciech Zaremba. Openai gym. arXiv preprint arXiv:1606.01540, 2016.\n[12] Thomas Mesnard, Théophane Weber, Fabio Viola, Shantanu Thakoor, Alaa Saade, Anna\nHarutyunyan, Will Dabney, Tom Stepleton, Nicolas Heess, Arthur Guez, et al. Counterfactual\ncredit assignment in model-free reinforcement learning. arXiv preprint arXiv:2011.09464,\n2020.\n[13] Rishabh Agarwal, Dale Schuurmans, and Mohammad Norouzi. An optimistic perspective on\nofﬂine reinforcement learning. In International Conference on Machine Learning, 2020.\n[14] Aviral Kumar, Aurick Zhou, George Tucker, and Sergey Levine. Conservative q-learning for\nofﬂine reinforcement learning. In Advances in Neural Information Processing Systems, 2020.\n[15] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[16] Richard S. Sutton. Integrated architectures for learning, planning, and reacting based on\napproximating dynamic programming. In ICML, 1990.\n[17] Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model:\nModel-based policy optimization. In Advances in Neural Information Processing Systems ,\npages 12498–12509, 2019.\n14\n[18] Aviral Kumar, Justin Fu, George Tucker, and Sergey Levine. Stabilizing off-policy q-learning\nvia bootstrapping error reduction. arXiv preprint arXiv:1906.00949, 2019.\n[19] Yifan Wu, George Tucker, and Oﬁr Nachum. Behavior regularized ofﬂine reinforcement\nlearning. arXiv preprint arXiv:1911.11361, 2019.\n[20] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G\nBellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al.\nHuman-level control through deep reinforcement learning. nature, 518(7540):529–533, 2015.\n[21] Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with\ndiscrete world models. arXiv preprint arXiv:2010.02193, 2020.\n[22] Will Dabney, Mark Rowland, Marc Bellemare, and Rémi Munos. Distributional reinforcement\nlearning with quantile regression. In Conference on Artiﬁcial Intelligence, 2018.\n[23] Justin Fu, Aviral Kumar, Oﬁr Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for\ndeep data-driven reinforcement learning. arXiv preprint arXiv:2004.07219, 2020.\n[24] Xue Bin Peng, Aviral Kumar, Grace Zhang, and Sergey Levine. Advantage-weighted regression:\nSimple and scalable off-policy reinforcement learning. arXiv preprint arXiv:1910.00177, 2019.\n[25] David Raposo, Sam Ritter, Adam Santoro, Greg Wayne, Theophane Weber, Matt Botvinick,\nHado van Hasselt, and Francis Song. Synthetic returns for long-term credit assignment. arXiv\npreprint arXiv:2102.12425, 2021.\n[26] Yang Gao, Huazhe Xu, Ji Lin, Fisher Yu, Sergey Levine, and Trevor Darrell. Reinforcement\nlearning from imperfect demonstrations. arXiv preprint arXiv:1802.05313, 2018.\n[27] Ashvin Nair, Murtaza Dalal, Abhishek Gupta, and Sergey Levine. Accelerating online rein-\nforcement learning with ofﬂine datasets. arXiv preprint arXiv:2006.09359, 2020.\n[28] Adrien Ecoffet, Joost Huizinga, Joel Lehman, Kenneth O Stanley, and Jeff Clune. Go-explore:\na new approach for hard-exploration problems. arXiv preprint arXiv:1901.10995, 2019.\n[29] Scott Fujimoto, David Meger, and Doina Precup. Off-policy deep reinforcement learning\nwithout exploration. In International Conference on Machine Learning, 2019.\n[30] Aviral Kumar, Justin Fu, Matthew Soh, George Tucker, and Sergey Levine. Stabilizing off-policy\nq-learning via bootstrapping error reduction. In Advances in Neural Information Processing\nSystems, 2019.\n[31] Noah Y Siegel, Jost Tobias Springenberg, Felix Berkenkamp, Abbas Abdolmaleki, Michael\nNeunert, Thomas Lampe, Roland Hafner, and Martin Riedmiller. Keep doing what worked:\nBehavioral modelling priors for ofﬂine reinforcement learning. In International Conference on\nLearning Representations, 2020.\n[32] Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, and Thorsten Joachims. Morel:\nModel-based ofﬂine reinforcement learning. In Advances in Neural Information Processing\nSystems, 2020.\n[33] Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea\nFinn, and Tengyu Ma. Mopo: Model-based ofﬂine policy optimization. In Advances in Neural\nInformation Processing Systems, 2020.\n[34] Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and Oﬁr Nachum. Opal: Of-\nﬂine primitive discovery for accelerating ofﬂine reinforcement learning. arXiv preprint\narXiv:2010.13611, 2020.\n[35] Víctor Campos, Alexander Trott, Caiming Xiong, Richard Socher, Xavier Giro-i Nieto, and\nJordi Torres. Explore, discover and learn: Unsupervised discovery of state-covering skills. In\nInternational Conference on Machine Learning, 2020.\n[36] Karl Pertsch, Youngwoon Lee, and Joseph J Lim. Accelerating reinforcement learning with\nlearned skill priors. arXiv preprint arXiv:2010.11944, 2020.\n15\n[37] Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine.\nParrot: Data-driven behavioral priors for reinforcement learning. In International Conference\non Learning Representations, 2021.\n[38] Benjamin Eysenbach, Abhishek Gupta, Julian Ibarz, and Sergey Levine. Diversity is all you\nneed: Learning skills without a reward function. In International Conference on Learning\nRepresentations, 2019.\n[39] Kevin Lu, Aditya Grover, Pieter Abbeel, and Igor Mordatch. Reset-free lifelong learning with\nskill-space planning. arXiv preprint arXiv:2012.03548, 2020.\n[40] Archit Sharma, Shixiang Gu, Sergey Levine, Vikash Kumar, and Karol Hausman. Dynamics-\naware unsupervised discovery of skills. In International Conference on Learning Representa-\ntions, 2020.\n[41] Christopher Watkins. Learning from delayed rewards. 01 1989.\n[42] V olodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan\nWierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint\narXiv:1312.5602, 2013.\n[43] Rupesh Kumar Srivastava, Pranav Shyam, Filipe Mutz, Wojciech Ja ´skowski, and Jürgen\nSchmidhuber. Training agents using upside-down reinforcement learning. arXiv preprint\narXiv:1912.02877, 2019.\n[44] Aviral Kumar, Xue Bin Peng, and Sergey Levine. Reward-conditioned policies. arXiv preprint\narXiv:1912.13465, 2019.\n[45] Acting without rewards. 2019. URL https://ogma.ai/2019/08/\nacting-without-rewards/.\n[46] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya\nSutskever. Generative pretraining from pixels. In International Conference on Machine\nLearning, pages 1691–1703. PMLR, 2020.\n[47] Dibya Ghosh, Abhishek Gupta, Justin Fu, Ashwin Reddy, Coline Devin, Benjamin Eysenbach,\nand Sergey Levine. Learning to reach goals without reinforcement learning. arXiv preprint\narXiv:1912.06088, 2019.\n[48] Keiran Paster, Sheila A McIlraith, and Jimmy Ba. Planning from pixels using inverse dynamics\nmodels. arXiv preprint arXiv:2012.02419, 2020.\n[49] Michael Janner, Qiyang Li, and Sergey Levine. Reinforcement learning as one big sequence\nmodeling problem. arXiv preprint arXiv:2106.02039, 2021.\n[50] Johan Ferret, Raphaël Marinier, Matthieu Geist, and Olivier Pietquin. Self-attentional credit\nassignment for transfer in reinforcement learning. arXiv preprint arXiv:1907.08027, 2019.\n[51] Anna Harutyunyan, Will Dabney, Thomas Mesnard, Mohammad Azar, Bilal Piot, Nicolas\nHeess, Hado van Hasselt, Greg Wayne, Satinder Singh, Doina Precup, et al. Hindsight credit\nassignment. arXiv preprint arXiv:1912.02503, 2019.\n[52] Jose A Arjona-Medina, Michael Gillhofer, Michael Widrich, Thomas Unterthiner, Johannes\nBrandstetter, and Sepp Hochreiter. Rudder: Return decomposition for delayed rewards. arXiv\npreprint arXiv:1806.07857, 2018.\n[53] Yang Liu, Yunan Luo, Yuanyi Zhong, Xi Chen, Qiang Liu, and Jian Peng. Sequence mod-\neling of temporal credit assignment for episodic reinforcement learning. arXiv preprint\narXiv:1905.13420, 2019.\n[54] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative\nadversarial networks. In Conference on Computer Vision and Pattern Recognition, 2019.\n[55] Marjan Ghazvininejad, Xing Shi, Jay Priyadarshi, and Kevin Knight. Hafez: an interactive\npoetry generation system. In Proceedings of ACL, System Demonstrations, 2017.\n16\n[56] Lilian Weng. Controllable neural text generation. lilianweng.github.io/lil-\nlog, 2021. URL https://lilianweng.github.io/lil-log/2021/01/02/\ncontrollable-neural-text-generation.html .\n[57] Jessica Ficler and Yoav Goldberg. Controlling linguistic style aspects in neural language\ngeneration. arXiv preprint arXiv:1707.02633, 2017.\n[58] Zhiting Hu, Zichao Yang, Xiaodan Liang, Ruslan Salakhutdinov, and Eric P Xing. Toward\ncontrolled generation of text. In International Conference on Machine Learning, 2017.\n[59] Nazneen Fatema Rajani, Bryan McCann, Caiming Xiong, and Richard Socher. Explain yourself!\nleveraging language models for commonsense reasoning. arXiv preprint arXiv:1906.02361,\n2019.\n[60] Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial\nnets with policy gradient. In AAAI conference on artiﬁcial intelligence, 2017.\n[61] Daniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B Brown, Alec Radford, Dario Amodei,\nPaul Christiano, and Geoffrey Irving. Fine-tuning language models from human preferences.\narXiv preprint arXiv:1909.08593, 2019.\n[62] Nitish Shirish Keskar, Bryan McCann, Lav R Varshney, Caiming Xiong, and Richard Socher.\nCtrl: A conditional transformer language model for controllable generation. arXiv preprint\narXiv:1909.05858, 2019.\n[63] Sumanth Dathathri, Andrea Madotto, Janice Lan, Jane Hung, Eric Frank, Piero Molino, Jason\nYosinski, and Rosanne Liu. Plug and play language models: A simple approach to controlled\ntext generation. arXiv preprint arXiv:1912.02164, 2019.\n[64] Ari Holtzman, Jan Buys, Maxwell Forbes, Antoine Bosselut, David Golub, and Yejin Choi.\nLearning to write with cooperative discriminators. arXiv preprint arXiv:1805.06087, 2018.\n[65] Ben Krause, Akhilesh Deepak Gotmare, Bryan McCann, Nitish Shirish Keskar, Shaﬁq Joty,\nRichard Socher, and Nazneen Fatema Rajani. Gedi: Generative discriminator guided sequence\ngeneration. arXiv preprint arXiv:2009.06367, 2020.\n[66] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805,\n2018.\n[67] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In European Conference on\nComputer Vision, 2020.\n[68] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[69] Sam Ritter, Ryan Faulkner, Laurent Sartran, Adam Santoro, Matt Botvinick, and David Raposo.\nRapid task-solving in novel environments. arXiv preprint arXiv:2006.03662, 2020.\n[70] Sudeep Dasari and Abhinav Gupta. Transformers for one-shot visual imitation. arXiv preprint\narXiv:2011.05970, 2020.\n[71] Josh Abramson, Arun Ahuja, Iain Barr, Arthur Brussee, Federico Carnevale, Mary Cassin,\nRachita Chhaparia, Stephen Clark, Bogdan Damoc, Andrew Dudzik, et al. Imitating interactive\nintelligence. arXiv preprint arXiv:2012.05672, 2020.\n[72] Thomas Wolf, Julien Chaumond, Lysandre Debut, Victor Sanh, Clement Delangue, Anthony\nMoi, Pierric Cistac, Morgan Funtowicz, Joe Davison, Sam Shleifer, et al. Transformers: State-of-\nthe-art natural language processing. In Conference on Empirical Methods in Natural Language\nProcessing: System Demonstrations, 2020.\n[73] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint\narXiv:1711.05101, 2017.\n17\nA Experimental Details\nCode for experiments can be found in the supplementary material.\nA.1 Atari\nWe build our Decision Transformer implementation for Atari games off of minGPT ( https://\ngithub.com/karpathy/minGPT), a publicly available re-implementation of GPT. We use most\nof the default hyperparameters from their character-level GPT example ( https://github.com/\nkarpathy/minGPT/blob/master/play_char.ipynb). We reduce the batch size (except in Pong),\nblock size, number of layers, attention heads, and embedding dimension for faster training. For\nprocessing the observations, we use the DQN encoder from Mnih et al. [20] with an additional linear\nlayer to project to the embedding dimension.\nFor return-to-go conditioning, we use either 1×or 5×the maximum return in the dataset, but more\npossibilities exist for principled return-to-go conditioning. In Atari experiments, we use Tanh instead\nof LayerNorm (as described in Section 3) after embedding each modality, but did this does not make\na signiﬁcant difference in performance. The full list of hyperparameters can be found in Table 8.\nTable 8: Hyperparameters of DT (and %BC) for Atari experiments.\nHyperparameter Value\nNumber of layers 6\nNumber of attention heads 8\nEmbedding dimension 128\nBatch size 512 Pong\n128 Breakout, Qbert, Seaquest\nContext length K 50 Pong\n30 Breakout, Qbert, Seaquest\nReturn-to-go conditioning 90 Breakout (≈ 1× max in dataset)\n2500 Qbert (≈ 5× max in dataset)\n20 Pong (≈ 1× max in dataset)\n1450 Seaquest (≈ 5× max in dataset)\nNonlinearity ReLU, encoder\nGeLU, otherwise\nEncoder channels 32, 64, 64\nEncoder ﬁlter sizes 8 × 8, 4 × 4, 3 × 3\nEncoder strides 4, 2, 1\nMax epochs 5\nDropout 0.1\nLearning rate 6 ∗ 10−4\nAdam betas (0.9, 0.95)\nGrad norm clip 1.0\nWeight decay 0.1\nLearning rate decay Linear warmup and cosine decay (see code for details)\nWarmup tokens 512 ∗ 20\nFinal tokens 2 ∗ 500000 ∗ K\nA.2 OpenAI Gym\nA.2.1 Decision Transformer\nOur code is based on the Huggingface Transformers library [72]. Our hyperparameters on all OpenAI\nGym tasks are shown below in Table 9. Heuristically, we ﬁnd using larger models helps to model the\ndistribution of returns, compared to standard RL model sizes (which learn one policy). For reacher\nwe use a smaller context length than the other environments, which we ﬁnd to be helpful as the\nenvironment is goal-conditioned and the episodes are shorter. We choose return targets based on\nexpert performance for each environment, except for HalfCheetah where we ﬁnd 50% performance\nto be better due to the datasets containing lower relative returns to the other environments. Models\nwere trained for 105 gradient steps using the AdamW optimizer [73] following PyTorch defaults.\n18\nTable 9: Hyperparameters of Decision Transformer for OpenAI Gym experiments.\nHyperparameter Value\nNumber of layers 3\nNumber of attention heads 1\nEmbedding dimension 128\nNonlinearity function ReLU\nBatch size 64\nContext length K 20 HalfCheetah, Hopper, Walker\n5 Reacher\nReturn-to-go conditioning 6000 HalfCheetah\n3600 Hopper\n5000 Walker\n50 Reacher\nDropout 0.1\nLearning rate 10−4\nGrad norm clip 0.25\nWeight decay 10−4\nLearning rate decay Linear warmup for ﬁrst 105 training steps\nA.2.2 Behavior Cloning\nAs brieﬂy mentioned in Section 4.2, we found previously reported behavior cloning baselines to be\nweak, and so run them ourselves using a similar setup as Decision Transformer. We tried using a\ntransformer architecture, but found using an MLP (as in previous work) to be stronger. We train for\n2.5 ×104 gradient steps; training more did not improve performance. Other hyperparameters are\nshown in Table 10. The percentile behavior cloning experiments use the same hyperparameters.\nTable 10: Hyperparameters of Behavior Cloning for OpenAI Gym experiments.\nHyperparameter Value\nNumber of layers 3\nEmbedding dimension 256\nNonlinearity function ReLU\nBatch size 64\nDropout 0.1\nLearning rate 10−4\nWeight decay 10−4\nLearning rate decay Linear warmup for ﬁrst 105 training steps\nA.3 Graph Shortest Path\nWe give details of the illustrative example discussed in the introduction. The task is to ﬁnd the\nshortest path on a ﬁxed directed graph, which can be formulated as an MDP where reward is 0 when\nthe agent is at the goal node and −1 otherwise. The observation is the integer index of the graph\nnode the agent is in. The action is the integer index of the graph node to move to next. The transition\ndynamics transport the agent to the action’s node index if there is an edge in the graph, while the\nagent remains at the past node otherwise. The returns-to-go in this problem correspond to negative\npath lengths and maximizing them corresponds to generating shortest paths.\nIn this environment, we use the GPT model as described in Section 3 to generate both actions\nand return-to-go tokens. This makes it possible for the model it generate its own (realizable)\nreturns-to-go ˆR. Since we require a return prompt to generate actions and we do assume knowl-\nedge of the optimal path length upfront, we use a simple prior over returns that favors shorter\npaths: Pprior( ˆR = k) ∝T + 1−k, where T is the maximum trajectory length. Then, it is com-\nbined with the return probabilities generated by the GPT model: P( ˆRt|s0:t,a0:t−1, ˆR0:t−1) =\n19\n1 2 3 4 5 6 7 8 9\n# of steps to goal\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7proportion of paths\nshortest path\ntransformer\nrandom walk\nFigure 6: Histogram of steps to reach the goal node for random walks on the graph, shortest possible\npaths to the goal, and attempted shortest paths generated by the transformer model. ∞indicates the\ngoal was not reached during the trajectory.\nPGPT( ˆRt|s0:t,a0:t−1, ˆR0:t−1) ×Pprior( ˆRt)10. Note that the prior and return-to-go predictions are\nentirely computable by the model, and thus avoids the need for any external or oracle information like\nthe optimal path length. Adjustment of generation by a prior has also been used for similar purposes\nin controllable text generation in prior work [65].\nWe train on a dataset of 1,000 graph random walk trajectories of T = 10steps each with a random\ngraph of 20 nodes and edge sparsity coefﬁcient of 0.1. We report the results in Figure 6, where we\nﬁnd that transformer model is able to signiﬁcantly improve upon the number of steps required to\nreach the goal, closely matching performance of optimal paths.\nThere are two reasons for the favorable performance on this task. In one case, the training dataset\nof random walk trajectories may contain a segment that directly corresponds to the desired shortest\npath, in which case it will be generated by the model. In the second case, generated paths are entirely\noriginal and are not subsets of trajectories in the training dataset - they are generated from stitching\nsub-optimal segments. We ﬁnd this case accounts for 15.8% of generated paths in the experiment.\nWhile this is a simple example and uses a prior on generation that we do not use in other experiments\nfor simplicity, it illustrates how hindsight return information can be used with generation priors to\navoid the need for explicit dynamic programming.\nB Atari Task Scores\nTable 11 shows the normalized scores used for normalization used in Hafner et al.[21]. Tables 12\nand 13 show the raw scores corresponding to Tables 1 and 4, respectively. For %BC scores, we use\nthe same hyperparameters as Decision Transformer for fair comparison. For REM and QR-DQN,\nthere is a slight discrepancy between Agarwal et al. [13] and Kumar et al. [14]; we report raw data\nprovided to us by REM authors.\nGame Random Gamer\nBreakout 2 30\nQbert 164 13455\nPong −21 15\nSeaquest 68 42055\nTable 11: Atari baseline scores used for normalization.\n20\nGame DT (Ours) CQL QR-DQN REM BC\nBreakout 76.9 ± 27.3 61.1 6 .8 4 .5 40 .9 ± 17.3\nQbert 2215.8 ± 1523.7 14012.0 156.0 160 .1 2464 .1 ± 1948.2\nPong 17.1 ± 2.9 19.3 −14.5 −20.8 9 .7 ± 7.2\nSeaquest 1129.3 ± 189.0 779.4 250 .1 370 .5 968 .6 ± 133.8\nTable 12: Raw scores for the 1% DQN-replay Atari dataset. We report the mean and variance across\n3 seeds. Best mean scores are highlighted in bold. Decision Transformer performs comparably to\nCQL on 3 out of 4 games, and usually outperforms other baselines.\nGame DT (Ours) 10%BC 25%BC 40%BC 100%BC\nBreakout 76.9 ± 27.3 10.0 ± 2.3 22 .6 ± 1.8 32 .3 ± 18.9 40 .9 ± 17.3\nQbert 2215.8 ± 1523.7 1045 ± 232.0 2302 .5 ± 1844.1 1674 .1 ± 776.0 2464.1 ± 1948.2\nPong 17.1 ± 2.9 −20.3 ± 0.1 −16.2 ± 1.0 5 .2 ± 4.8 9 .7 ± 7.2\nSeaquest 1129.3 ± 189.0 521.3 ± 103.0 549 .3 ± 96.2 758 ± 169.1 968 .6 ± 133.8\nTable 13: %BC scores for Atari. We report the mean and variance across 3 seeds. Decision\nTransformer usually outperforms %BC.\n21",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7977776527404785
    },
    {
      "name": "Reinforcement learning",
      "score": 0.7919539213180542
    },
    {
      "name": "Computer science",
      "score": 0.6893815994262695
    },
    {
      "name": "Scalability",
      "score": 0.5854198932647705
    },
    {
      "name": "Architecture",
      "score": 0.46523135900497437
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4581988453865051
    },
    {
      "name": "Autoregressive model",
      "score": 0.4520610570907593
    },
    {
      "name": "Machine learning",
      "score": 0.3569129705429077
    },
    {
      "name": "Engineering",
      "score": 0.21466520428657532
    },
    {
      "name": "Voltage",
      "score": 0.1470687985420227
    },
    {
      "name": "Mathematics",
      "score": 0.12520670890808105
    },
    {
      "name": "Electrical engineering",
      "score": 0.089667409658432
    },
    {
      "name": "Econometrics",
      "score": 0.0
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    }
  ]
}