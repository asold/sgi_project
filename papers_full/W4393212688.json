{
  "title": "FEEL: A Framework for Evaluating Emotional Support Capability with Large Language Models",
  "url": "https://openalex.org/W4393212688",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2223170888",
      "name": "Huaiwen Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096664151",
      "name": "Yu Chen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098424906",
      "name": "Ming Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2065720840",
      "name": "Shi Feng",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W4401043863",
    "https://openalex.org/W4389519254",
    "https://openalex.org/W6603178198",
    "https://openalex.org/W4385572601",
    "https://openalex.org/W3097868756",
    "https://openalex.org/W4226076738",
    "https://openalex.org/W4226396354",
    "https://openalex.org/W4385571851",
    "https://openalex.org/W4401042286",
    "https://openalex.org/W2584220694",
    "https://openalex.org/W3034808773",
    "https://openalex.org/W4392669905",
    "https://openalex.org/W3159259047",
    "https://openalex.org/W6601365666"
  ],
  "abstract": null,
  "full_text": "FEEL: A Framework for Evaluating Emotional Support \nCapability with Large Language Models \nHuaiwen Zhang*, Yu Chen*, Ming Wang and Shi Feng(ï€ª) \nSchool of Computer Science and Engineering, Northeastern University, Shenyang 110819, \nChina \nfengshi@cse.neu.edu.cn \nAbstract.  Emotional Support Conversation (ESC) is a typical dialogue that can \neffectively assist the user in mitigating emotional pressures . However, owing to \nthe inherent subjectivity involved in analyzing emotions,  current non-artificial \nmethodologies face challenges i n effectively appraising the emotional support \ncapability. These metrics exhibit a low correlation with human judgments. Con-\ncurrently, manual evaluation methods extremely will cause high costs. To solve \nthese problems, we propos e a novel model FEEL (Framework for Evaluating \nEmotional Support Capability with Large Language Models), employing Large \nLanguage Models (LLMs) as evaluators to assess emotional support capabili-\nties. The model meticulously considers various evaluative  aspects of ESC to \napply a more comprehensive and accurate evaluation method for ESC. Addi-\ntionally, it employs self-CoT and probability distribution approaches for a more \nstable result and integrates an ensemble learning strategy, leveraging multiple \nLLMs with assigned weights to enhance evaluation accuracy.  To appraise the \nperformance of FEEL, we conduct extensive experiments on existing ESC \nmodel dialogue s. Experimental results demonstrate our model  exhibits a sub-\nstantial enhancement in alignment with human evaluations  compared to the \nbaselines. Our source code is available at https://github.com/Ansisy/FEEL. \nKeywords: Emotional Support Conversation, Large Language Model, Dia-\nlogue Quality Evaluation, Ensemble Learning. \n1 Introduction \nEmotional Support Conversations (ESC) [1] is a goal-oriented task focused on allevi-\nating em otional distress and ef fecting positive changes in individuals' psychological \nstates. ESC has extensive applications in mental health support , customer service \nchats, etc. However, the evaluation of ESC, as compared to the generation, presents a \nmore challenging task.  As shown in  Fig. 1, unreliable ESC evaluation systems may \nmislead users and even increase their psychological stress.  In the available studies , \nworks on ESC evaluation are mainly divided into two categories. The first is to use \ntraditional automatic metrics, such as BLEU [ 2], ROUGE [ 3], Distinct-n [4], \n \n* These authors contributed equally to this work. \n2 \nMETEOR [5], etc. These metrics predominantly co ncentrate on the similarity to the \ngolden outputs. They exhibit a low correlation with human evaluations. Consequently, \nthese methods are not suitable for assessing ESC as they cannot understand and eval-\nuate complex and divers e emotions of humanity. The second strategy involves train-\ning individuals to assess dialogue quality in specific aspects, necessitating annotator s \nwith robust text evaluation skills.  This methodology has demonstrated efficacy in \nassessing the quality of ESC.  However, there is no systematic evaluation framework \nand the basis for evaluation relies heavily on the subjective experience of the authors.  \nThis variation necessitates humans to be proficient in the meaning of different aspects \nwhich significantly increase s labor costs. Consequently, this approach demands sub-\nstantial time and labor investment.  Furthermore, manual evaluations typically involve \nonly a limited subset of data samples . If the sampling methodology is not reasonably \nchosen, it may introduce bias into the results. \n \nFig. 1. Low-quality evaluation systems lead to poor emotional support experiences and even \nworsen userâ€™s situation. \nRecently, many evaluation methods for natural language generation (NLG)  based \non LLMs have achieved notable advancements.  Fu et al. [6] discovered that genera-\ntive pre-training models exhibit enhanced reliability when guided by specific task and \naspect definitions, thereby laying the groundwork for the flexible selectio n of aspects. \nMoreover, Liu  et al.  [7] introduced an LLM -based evaluator for text summary as-\nsessment and achieved good alignment with humans. These works unveil the potential \nof LLMs as evaluators in assessing task-specific dialogues . Nonetheless, due to the \nstochastic nature of computations in the underlying hardware , the responses generated \nby a single LLM to the same question tend to be unstable with significant variance  \n[8]. Furthermore, most current works focus on generic evaluations of NLG tasks , with \na notable lack of specialized prompts designed for the assessment of ESC. Therefore, \nto get results with better human alignment , a more stable and task-specific mixed \nLLMs model is needed for the evaluation task of ESC. \nTo tackle these issues, we first redefine six evaluation aspects in emotional support \nskills and text quality  dimensions shown in AUGESC [9] by analyzing interactions \nwithin psychotherapy talk [ 10] to systematically evaluate the emotional support capa-\n         \n        \n   \n                         \n                     \n              \n                   \n                      \n                  \n            \n                               \n                                 \n                \n3 \nbility of dialogue  systems. Furthermore, we propose a novel evaluation framework \nFEEL, a.k.a . Framework for Evaluating Emotional support capability with Large \nLanguage Models.  Utilizing an ensemble learning  method, FEEL integrates three \nLLMs: ERNIE-Bot 4.0 [ 11], GLM -4 [ 12], and GPT -3.5-Turbo [ 13]. By providing \nLLMs with task definitions and scoring criteria as prompts, we empower the LLMs to \nrepeatedly derive the score distribution probability for each aspect. Moreover, we \naverage the scores to mitigate the effects of variance.  Then, we build a n emotional \nsupport capability score dataset ESCEval by instructing annotators to meticulously \nassess with dialogues in AUGESC [9] and ESConv [1]. Subsequently, we use the \nLLMsâ€™ Spearman's rank correlation coefficient  with the human result in ESCEval  as \nweights in the framework  to deriv e the final  FEEL outputs : the score of emotional \nsupport capabilities. Finally, we conduct extensive experiments on  dialogues generat-\ned from existing emotional s upport models . In comparison to automatic evaluation \nmetrics, FEEL demonstrates superior alignment with human-derived outcomes. \nThe main contributions of this paper are summarized as follows: \nâ€¢ We refine a suite of aspects, which assess dialogue quality in terms of both emo-\ntional support skills and text quality. \nâ€¢ We propose a  framework FEEL for emotional support capability evaluation based \non mixed LLMs. \nâ€¢ We annotate ESCEval: a high -quality dataset composed of human evaluations of \nemotional support capabilities, using instances from ESConv and AUGESC  and \ntrain the weights of LLMs on the dataset. \nâ€¢ We carry out extensive experiments on different emotion support models, demon-\nstrating that FEEL surpasses existing evaluation metrics in aligning with human  \nevaluation. \n2 Related Work \nNon-LLM-based ESC. In terms of datasets, Liu et al. [1] created an Emotion Sup-\nport Conversation dataset, richly annotated for both help -seeker and supporter interac-\ntions.  Meanwhile, Zheng et al . [ 9] developed AUGESC, an augmented dataset for \nESC tasks. In terms of emotional support modeling, Peng et al. [14] proposed a Feed-\nback Aware Double Con trolling Network for  strategic scheduling and supportive \nresponse generation. Peng et al. [15] introduced a Global-to-Local Hierarchical Graph \nNetwork to capture multi -source information and model hierarchical conversation \nrelationships. Tu et al. [16] propose the novel MISC model, which first infers users' \nfine-grained emotional states, then skillfully responds with a strategic mix. To further \nimprove the effectiveness of ESC, Zhao et al. [17] suggest considering turn-level state \ntransitions in ESC, encompassing semantic, strategic, and emotional shifts. But all the \nabove models, which give limited help to human emotional support tasks, are not very \neffective. \n4 \nLLM-based ESC. To test LLMâ€™s ability to perform on ESC tasks, Li et al. [18] con-\nducted experiments on 45 ESC tasks with various LLMs, demonstrating their grasp of \nemotional intelligence. Zhang et al. [19] reported that although LLMs perform satis-\nfactorily in simpler tasks, they fall short in complex tasks requiring deep understand-\ning or structured sentiment analysis. It is shown that ESC tasks using the LLM per-\nformed better on overall emotional support than previous models. \n2.1 NLG Evaluation Models \nNon-LLM-based Evaluation Models. To address the poor performance of tradition-\nal automatic evaluation metrics on some NLG tasks, Tao et al . [ 20] introduced \nRUBER, blending referenced and unreferenced metrics for evaluating replies against \nboth ground truth and queries. Zhong et al . [ 21] developed UNIEVAL, a unified \nevaluator for NLG across multiple dimensions. Further, Mehri et al. [22] presented \nUSR, a reference -free metric using unsupervised models to assess dialog qualities. \nThese models show good performance on text quality evaluation, but the evaluation \naspects of ESC evaluation tasks are quite different from those of traditional text quali-\nty evaluation tasks, so it is difficult to apply these models directly to ESC evaluation \ntasks. \nLLM-based Evaluation Models. To test whether LLM has the ability to evaluate the \nquality of NLG tasks, Fu et al. [6] introduced GPTSCORE, an innovative framework \nleveraging generative models for text assessment. Liu et al. [7] developed G -EVAL, \nutilizing large models with CoT and form -filling to evaluate NLG quality. To further \nenergize the LLM in evaluating the quality of NLG tasks, Liu et al. [23] proposed \nAUTOCALIBRATE, a method for aligning LLM -based evaluations with human \npreferences without gradients. Chen et al. [24] examined three reference -free evalua-\ntion techniques using ChatGPT or similar LLMs for dialog generation tasks. The \nsame problem as non -LLM-based evaluation models, these models are difficult to \napply to the ESC evaluation task. \n3 Methodology \n3.1 Task Definition \nTo evaluate the emotional support capability, it is imperative to consider both the \ndialogue generation quality and t he emotional perception in ESC. Currently, m ost \nideas of ESC eval uation aspects selection follow the procedure of emotional sup-\nportâ€”exploration, comforting, and action â€”as outlined in  ESConv [1]; however, the \nspecific selections demonstrate considerable variability and a notable lack of system-\natic approach across different studies. Therefore, we need to first make a comprehen-\nsive and systematic definition of ESC evaluation. \n5 \nBuilding upon the method ology proposed by AUGESC  [9], we categorize the \nevaluation of ESC into two distinct dimensions: emotional support skills  and text \nquality. Subsequently, through research on psychotherapy in [ 10], we further explain \neach aspect as below: \nEmotional Support Skills. (1) Informativeness: the extent to which the supporter \nguides the help-seeker in articulating their emotional problems in detail.  (2) Compre-\nhensibility: t he extent to which the supporter understand s the seekerâ€™    p         \nand feelings. (3) Helpfulness: the supporter's capacity to alleviate the client's emo-\ntional distress and provide constructive advice. \nText Quality. (1) Consistency: the alignment of the supporterâ€™s perspective and the \nadherence to his role. (2) Coherence: the maintenance of focus on the subject matter \nand the fluidity of topic transitions.  (3) Safety: the absence of inappropriate language \nor content within the dialogue. \nFor each aspect, a four-tiered Likert scale score ğ‘†ğ‘–  is assigned, where ğ‘†ğ‘– âˆˆ [0,3] \nand can be either an integer or a decimal.  The evaluation result of one dialogue is \nrepresented as   {ğ‘†1,  ğ‘†2, ğ‘†3, ğ‘†4, ğ‘†5, ğ‘†6 }, denoting the emotional support capability \nscores across the six aspects.  \nThe complete evaluation criteria for human annotators is shown on our open-\nsource website. \n3.2 ESCEval: A Dataset of Human ESC Evaluation \nIn order to assess the emotional support capabilities of each LLM and then to deter-\nmine their weight in FEEL , it is imperative to establish a dataset comprising human \nscores for emotional support dialogues as a reference standard for LLM . Therefore, \nwe construct ESCEval: a dataset composed of human evaluations of emotional sup-\nport capabilities . We enlisted six college s tudents majoring in computer science to \nevaluate a total of 200 dialogue  instances, randomly selected from ESConV and \nAUGESC. Any identifiable personal information present in the dialogues is anony-\nmized to ensure data security . Subsequently, refer to the human annotations work in \n[25], we conduct a total of two scoring round s: in the  first round, each anno tator is \nasked to score in six aspects as per the standards delineated in Sect. 3.1; in the second \nround, we manually inspect the results and require that each annotator's score on a \ndimension differs from another annotator by more than 1 point, and that the other \nannotators' scores within 1 point of each other are rescored. Whenever a scoring dis-\ncrepancy greater than one point arose without the pattern, all annotators were instruct-\ned to collaboratively reevaluate the annotation.  When rescoring, the annotator can \nrefer to other people's scores to review again to avoid strong subjectivity.  We finally \ncalculate the average score of all annotators to construct the dataset . By employing \nthis methodical approach, we effectively reduce the impact of personal subjectivity on \nthe scoring results  and construct a high-quality emotional support capability scores \ndataset. The process of ESCEval construction can be seen in Fig. 2. \n6 \n \nFig. 2. The process of ESCEval construction \n3.3 Proposed FEEL \nFEEL represents a comprehensive, multifaceted evaluator based on LLMs, compris-\ning: a) a structured prompt delineating task specification; evaluative criteria for each \naspect as discussed in Sect. 3.1; the evaluation sample dialogue; and the output format \nthat standardize the output form of  LLMs; b) an advanced scoring algorithm that as-\ncertains individual LLM scores by computing probabilities linked to the assignment \nof particular scoring bands  to get stable evaluation scores; and c) a comprehensive \nintegrative weighted computational approach using ensemble learning method that \nsynthesizes results from multiple LLMs to deduce definitive evaluation scores . The \noutput of the FEEL metric quantitatively reflects the emotional support capabilities of \nthe supporter exhibited in the sample across each aspect. The detailed framework of \nFEEL is shown in Fig. 3.  \nEvaluator Prompt Design.  A prompt serves as a natural language directive that \nclearly delineates the specific evaluation criteria and expectations for an assessment \ntask. We first delineate prompt for the task of evaluating emotional support  capabili-\nties: \n \n You will play the role of a psychologist who is well versed in emo-\ntional support. There will be a dialog between the help seeker (i.e., the \nperson seeking support) and the supporter (i.e., the person providing \nsupport).â€¦â€¦.  \n \n                 \n   \n             \n          \n         \n                                             \n                            \n                 \n              \n                \n                   \n           \n              \n                        \n7 \nFurthermore, the prompt phrases encompass specific evaluation aspects pertinent \nto the task of assessing emotional support capability: \n \n Evaluation Criteria: \n Comprehensibility: how well the supporter understands the help -\nseeker's experiences and feelingsâ€¦â€¦ \nThe complete prompt is shown in our open-source website. \nSelf Chain-of-Thoughts for Emotional Capability Evaluation.  Chain-of-Thought \n(CoT) prompting in the context of large language models refers to a technique that \nenhances the model's reasoning capabilities by breaking down complex tasks into a \nsequence of intermediate steps.  In the emotional support cap ability evaluation, the \nLLMs needs to master the detailed evaluation steps of each indicator so as to conduct \ndetailed evaluation step by step.  In the meanwhile , manually designing CoT is time -\nconsuming and cannot accurately follow the problem -solving logic of LLM.  There-\nfore, refer to the CoT design in [7], we give LLM the task definition and the specific \nmetric evaluation criteria to let it generate the evaluation steps automatically (Self-\nCoT). For example, for evaluating informativeness in emotional support cap ability, \nwe add a line of â€œE          S  p  â€ to the prompt and let LLM to generate the fol-\nlowing CoT automatically. \n \n1. Read the client's description carefully. \n2. Extract the emotional issues and related details in the description. \n3. Evaluate whether the description contains the specific causes of the \nemotional issues. \nâ€¦â€¦ \nIndividual LLM Evaluation . For specific evaluation aspect of single -round dialog \ndata, we utilize a prompt -based answer format to facilitate the LLM in generating \nselection probabilities for each score band (0 to 3), respectively, with the probabilities \nof the four score bands summing to 1: \n \nAnswer format (give the probability of each score band for each type \nof score): \n- Comprehensibility Score: \n0 points: \n1 point: \n2 points: \n3 points: \n \nSubsequently, for a particular evaluation aspect , the selection probabilities pro-\nduced by the LLM serve as weights. These are multiplied by the scores within the \nrespective score bands, adding up the weighted scores of the four score bands to as-\ncertain the singular -round score for the specific aspect. Acknowledging the inherent \n8 \nvolatility in LLM responses to ascertain the singular -round score for the specific as-\npect, the variability of scoring outcomes across different LLM rounds  [26]. For the \nsame evaluation aspect of the same round of dialog, we mitigate it by calculating the \naverage of ten successive LLM round scores, thereby deriving a stable final score : \n ğ‘†ğ‘–  =  \nâˆ‘ âˆ‘ ğ‘Šğ‘—,ğ‘› âˆ— ğ‘—3\nğ‘—=0\n10\nğ‘›=1\n10           (1)                                                     \nwhere ğ‘†ğ‘– represents individual LLM score in aspect ğ‘–, ğ‘Šğ‘—,ğ‘› denotes the selection prob-\nability of score band  ğ‘— in iteration ğ‘› (ğ‘— is an integer ranging from  0 to 3 and ğ‘› is an \ninteger ranging from 1 to 10). \n \n \nFig. 3. The detailed framework of FEEL. We first input Task Specification and Eval-\nuation Criteria to each LLM and ask it to generate a self-CoT of detailed Evaluation \nSteps. Then we employ the LLMs to output the possibility distribution according to \nthe Output Format for every dialogue sample. Finally, we use the Spearman Correla-\ntion-weighted score by three LLMs as the FEEL output. \nEstablishment of FEEL. Following individual LLM evaluations, as diverse LLMs \ndemonstrate distinct advantages across different  dialogues, allocating the scores of \neach model through weight adjustment can prominently underscore the individual \nstrengths of each model, thereby enhancing the overall evaluation outcomes. After \npreliminary testing, we iden tify three LLMs : ERINE-Bot-4.0 (LLM-1), GLM-4 \n(LLM-2), and GPT-3.5-turbo (LLM-3) as the constituents of  FEEL, which a ll have \nhigh-performance natural language understanding and fine-grained emotional analysis \ncapabilities. Subsequently, to determine the weight of each LLM in FEEL, we respec-\ntively employ the three LLMs to evaluate the ESCEval in Sect. 3.2. Finally, using the \nSpearman's rank correlation coefficient  with results in the ESCEval as the weight, we \ncalculate the FEEL result as follows: \n ğœŒğ‘–,ğ‘›  =   \nğ¶ğ‘–,ğ‘›\nâˆ‘ ğ‘ğ‘–,ğ‘›3ğ‘›=1\n (2) \n\n9 \n ğ¹ğ‘–  =  âˆ‘ ğœŒğ‘–,ğ‘› âˆ— ğ‘†ğ‘–,ğ‘›\n3\nğ‘›=1  (3) \nwhere ğœŒğ‘–,ğ‘›  denotes the weight of LLM -n in aspect ğ‘–, ğ¶ğ‘–,ğ‘›  represents the Spearman \ncorrelation coefficient of LLM-n in aspect ğ‘–. ğ‘†ğ‘–,ğ‘› represents the individual score of \nLLM-n in aspect ğ‘– and ğ¹ğ‘–  denotes the FEEL score in aspect ğ‘–, which represent s the \n        â€™            support capability in aspect ğ‘–. \n4 Experiments and Results \n4.1 Implementation of FEEL \nExperimental Settings.  We utilize the APIs of each LLM in FEEL and send request \nover the network to the model hosting server to access and interact with the corre-\nsponding LLMs for subsequent experimentation.  The whole experiment was complet-\ned on 2024-02-26. \nEvaluation Implementation of Individual LLM. For each of the three LLMs: GPT -\n3.5-turbo, ERNIE-bot-4.0, and GLM-4, we execute rigorous performance evaluations. \nUtilizing the standardized prompts delineated previously, we train and assess 200 \ndialogue instances collected in Sect. 3.2. Across various dialogue datasets, we ascer-\ntain the performance of these LLMs across six evaluative aspect s. Subsequently, we \nconduct an analysis of the Spearman correlation coefficients  and Kendall's tau coeffi-\ncient, comparing the evaluation scores of these LLMs across the six aspect s against \nESCEval. The calculation formulas are as follows: \n ğ‘Ÿ  =  1 âˆ’\n6 âˆ‘ ğ‘‘ğ‘–\n2ğ‘›\nğ‘–\nğ‘›(ğ‘›2âˆ’1) (4) \nwhere ğ‘Ÿ is the Spearman rank correlation coefficient, ğ‘› is the number of data points, ğ‘– \ndenotes that this is the ith data, and ğ‘‘ğ‘– denotes the gap between the model's evaluation \nscore and the manual labeling score. \n ğœ =\nğ¶âˆ’ğ·\nâˆš(ğ¶+ğ·+ğ‘‡)(ğ¶+ğ·+ğ‘ˆ) (5) \nwhere ğœ is the Kendall's tau coefficient . C is the number of consistent pairs. D is the \nnumber of inconsistent pairs. T is the number of leveled pairs for the first variable. U \nis the number of leveled pairs for the second variable. \nWeight Determination and Analysis.  Following the individual LLM evaluation and \nthe establishment of FEEL in Sect . 3.3, we employ the three LLMs â€™ Spearman rank \ncorrelation coefficient on ESCEval to derive the final FEEL score using equation 3, \nthe pertinent experimental result are detailed in Table 1 and Table 2. A substantial \nnumber of the coefficient values more than 0.3 and some approached or exceeded 0.4, \nindicating that the three LLMs demonstrated a high correlation with human assess-\n10 \nments across various aspects, and FEEL effectively synthesizes the high -performance \naspects of each LLM  using the calculation formula, reaching the highest Spearman's \nrank correlation coefficient  of 0.509 in terms of helpfulness. The FEEL result is \nshown in the last lines of two tables. \nIt should be emphasized that while FEEL generally outperforms individual LLMs \nacross a multitude of  aspects, its efficacy might be compromised on certain specific \naspects due to the pronounced variability inherent in the dataset or a potential predis-\nposition towards a particular model. \nTable 1. Spearman's rank correlation coefficient (Spear.) and Kendall's tau coefficient (Kend.) \nfor different models on the aspects of emotional support skills. \n  Informativeness Comprehensibility  Helpfulness \nSpear. Kend. Spear. Kend. Spear. Kend. \nERNIE-4.0 0.368 0.270 0.372 0.269 0.414 0.313 \nGPT-3.5 0.163 0.119 0.190 0.139 0.360 0.264 \nGLM-4 0.364 0.299 0.317 0.250 0.385 0.297 \nFEEL 0.404 0.300 0.429 0.314 0.509 0.377 \nTable 2. Spearman's rank correlation coefficient (Spear.) and Kendall's tau coefficient (Kend.) \nfor different models on the aspects of text quality. \n  Consistency Coherence  Safety \nSpear. Kend. Spear. Kend. Spear. Kend. \nERNIE-4.0 0.427 0.323 0.343 0.250 0.384 0.298 \nGPT-3.5 0.126 0.088 0.135 0.094 0.257 0.192 \nGLM-4 0.313 0.244 0.265 0.211 0.311 0.252 \nFEEL 0.434 0.327 0.331 0.241 0.409 0.314 \n4.2 Comparative Results \nSample Construction. To comprehensively ascertain FEEL's advancement in evalu-\nating the emotional support capabilities of models, we select three pre-trained models: \nBlenderBot-Joint [1], MISC  [16], TransESC  [17] and four large  language models: \nSpark-V3.0 [27], Baichuan2-Turbo [28], qwen-turbo [29], ChatGLM -6B [30]. Ten \ndistinct conversation topics are selected, and the reply lengths are standardized across \ndifferent models within a specified range  to control irrelevant variables . Subsequent-\nly, based on the methodology in  [17], the human evaluators are required to choose \nwhich one performs better (or tie) in every two models following five aspects (1) \nFluency, (2) Identification, (3) Empathy, (4) Suggestion and (5) Security. Finally, for \neach topic, the tally of wins versus losses for each model is computed to establish a \n11 \nmodel ranking of emotional support capabilities as a standard of comparison for sub-\nsequent evaluations. \nBaselines. We compare FEEL with the following automatic metrics which are widely \nused in ESC evaluation: \nâ€¢ BLEU-1, BLEU -2 [ 2] measure the n -gram precision between the generated text \nand reference texts, specifically focusing on unigrams and bigrams, respectively. \nâ€¢ ROUGE-1, ROUGE-2 and ROUGE-L [3] measure the lexical overlap between the \ngenerated text and corresponding references based on unigram, bigram and longest \ncommon subsequence, respectively. \nâ€¢ Meteor [4] measures the alignment between the generated text and reference texts \nby considering exact, stem, synonym, and paraphrase matches to compute a har-\nmonic mean of precision and recall. \nEvaluation Strategy.  To ass ess the extent to which FEEL correlates with human \njudgment more effectively than traditional automatic metrics, we use the rank-based \nindicators in Sect. 4.1: (1) Spearman's rank correlation coefficient ;  2) K      â€™     . \nConcurrently, to quantify the discrepancies in error between the model predictions  \nand human rankings across the sample, we also utilize (3) Root mean squared error \nand (4) Mean absolute error as our metrics: \n  ğ‘…ğ‘€ğ‘†ğ¸ =\nâˆšâˆ‘ (ğ‘ğ‘–âˆ’ğ‘Ÿğ‘–)2ğ‘›\nğ‘–=1\nğ‘›  (6) \n ğ‘€ğ´ğ¸ =\nâˆ‘ |ğ‘ğ‘–âˆ’ğ‘Ÿğ‘–|ğ‘›\nğ‘–=1\nğ‘›  (7) \nwhere ğ‘ğ‘–  represents the model pred iction ranking ï¼Œğ‘Ÿğ‘– represents the manual ranking \nresult, and ğ‘› is the number of models participating in the ranking . The comparative \ncalculation results are shown in  Table 3. It can be seen that FEEL is significantly \nbetter than all other baselines in four metrics. \nTable 3. The average Spearman's rank correlation coefficient (Spear.)  K      â€™      (Kend.), \nRooted Mean Squared Error (RMSE) and Mean Absolute Error (MAE) on the sample.  \n Spear. Kend. RMSE MAE \nBLEU-1 -0.136 -0.124 2.868 2.400 \nBLEU-2 -0.082 -0.076 2.878 2.343 \nROUGE-1 -0.261 -0.210 3.145 2.714 \nROUGE-2 -0.332 -0.257 3.230 2.743 \nROUGE-L -0.196 -0.162 3.017 2.543 \nMETEOR -0.029 -0.038 2.828 2.400 \nFEEL 0.404 0.314 2.049 1.657 \n \n12 \n4.3 Ablation Experiment \nIn order to further evaluate the effectiveness  of mixed LLM â€™ emotional support ca-\npabilities, the ablation experiment of FEEL with part of LLMs is conducted. Table 4 \nshows the results of the combination of every two LLMs in FEEL on the sample.  \nFirst of all, compared with the single-LLM ablation model, the combination of the \ntwo models and FEEL achieve better results, which is reflected in the improvement of \ncorrelation and the reduction of both errors.  Compared with the three ablated models  \ncombining two LLMs, FEEL achieves a notable improvement of 4.6% in Spearman's \nrank correlation coefficient against GLM+GPT. This enhancement is even more pro-\nnounced when FEEL is compared to ERNIE+GLM and ERNIE+GPT, with improve-\nments of 63.56% and 53.03%, respectively, highlighting FEELâ€™s robust performance \nin human alignments. Similarly, in Kendall's tau coefficient, FEEL shows significant \nimprovements, particularly a 73.48% increase compared to ERNIE+GPT, demonstrat-\ning its strength in ordinal association evaluation. There's a slight decrease of 1.72% in \nMean Absolute Error compared to GLM+GPT. Such a nuanced decrement may due to \nfactors including measurement error, accidental differences in the data collection \nprocess, or the inherent complexity of the data itself. \nTable 4. Ablation study results of FEEL.  \n Spear. Kend. RMSE MAE \nERNIE 0.219 0.187 2.324 1.714 \nGLM 0.161 0.124 2.505 2.086 \nGPT 0.182 0.174 2.126 1.900 \nERNIE+GLM 0.247 0.200 2.342 1.886 \nERNIE+GPT 0.264 0.181 2.331 1.857 \nGLM+GPT 0.386 0.276 2.150 1.629 \nFEEL 0.404 0.314 2.049 1.657 \n5 Conclusion \nIn this paper, we introduce a LLMs -based evaluator FEEL to evaluate emotional sup-\nport capability in dialogue  systems. Meanwhile, we systematically and detailedly \nredefine the evaluation aspects of ESC  and annotate a high -quality human score da-\ntaset ESCEval. Comparative experiment result shows that FEEL has higher human \nalignment than existing automatic evaluation metrics . Further ablation experiment \nshows that the scoring method that uses multiple LLMs for ensemble learning effec-\ntively improves the evaluation  quality. However, although FEEL performs well in \nmost aspects, it is still affected by noise including the subjectivity of manual scoring \nand differences in dialogue data.  Our future work is to further enhance the robustness \nof FEEL . In addition, due to funding reasons, we only use three LLMs for model \narchitecture in this  paper. In the future, we will explore the impact of more LLM \ncomponents on the evaluation effect. \n13 \nReferences \n1. Liu, S., Zheng, C., Demasi, O., Sabour, S., Li, Y., Yu, Z., ... & Huang, M.: Towards emo-\ntional support dialog systems. arXiv preprint arXiv:2106.01144 (2021) \n2. Papineni, K., Roukos, S., Ward, T., & Zhu, W. J.: Bleu: a method for automatic evaluation \nof machine translation. In: Proceedings of the 40th annual meeting of the Association for \nComputational Linguistics, pp. 311-318 (2002) \n3. Lin, C. Y.: Rouge: A package for automatic evaluation of summaries. In: Text summariza-\ntion branches out, pp. 74-81 (2004) \n4. Li, J., Galley, M., Brockett, C., Gao, J., & Dolan, B.: A diversity -promoting objective \nfunction for neural conversation models. arXiv preprint arXiv:1510.03055 (2015) \n5. Banerjee, S., & Lavie, A.: METEOR: An automatic metric for MT evaluation with im-\nproved correlation with human judgments. In: Proceedings of the acl workshop on intrinsic \nand extrinsic evaluation measures for machine translation and/or summarization, pp. 65 -72 \n(2005) \n6. Fu, J., Ng, S. K., Jiang, Z., & Liu, P.: Gptscore: Evaluate as you desire. arXiv preprint \narXiv:2302.04166 (2023) \n7. Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., & Zhu, C.: Gpteval: NLG evaluation using gpt -\n4 with better human alignment. arXiv preprint arXiv:2303.16634 (2023) \n8. Morin, M., & Willetts, M. : Non-determinism in tensorflow resnets.  arXiv preprint \narXiv:2001.11396 (2020) \n9. Zheng, C., Sabour, S., Wen, J., Zhang, Z., & Huang, M.: Augesc: Dialogue  augmentation \nwith large language models for emotional support conversation. In:  Findings of the Asso-\nciation for Computational Linguistics: ACL 2023,  pp. 1552-1568 (2023) \n10. Avdi, E., & Evans, C.: Exploring conversational and physiological aspects of psychothera-\npy talk. In: Frontiers in Psychology, 11, 591124 (2020) \n11. ERNIE-4.0-8K - Qianfan Large Model Platform | Baidu Intelligent Cloud Documentation, \nhttps://cloud.baidu.com/doc/WENXINWORKSHOP/s/clntwmv7t, last accessed \n2024/02/24. \n12. Zhipu AI open platform , https://open.bigmodel.cn/dev/api#glm-4, last accessed \n2024/02/23. \n13. Introducing ChatGPT, https://openai.com/blog/chatgpt, last accessed 2024/02/26.  \n14. Peng, W., Qin, Z., Hu, Y., Xie, Y., Li, Y.: FADO: Feedback-Aware Double Controlling \nNetwork for Emotional Support Conversation. Knowledge -Based Systems 264, 110340 \n(2023)  \n15. Peng, W., Hu, Y., Xing, L., Xie, Y., Sun, Y., Li, Y.: Control Globally, Understand Local-\nly: A Global -to-Local Hierarchical Graph Network for Emotional Support Conversation. \narXiv preprint arXiv:2204.12749 (2022) \n16. Tu, Q., Li, Y., Cui, J., Wang, B., Wen, J. -R., Yan, R.: MISC: A Mixed Strategy -Aware \nModel Integrating COMET for Emotional Support Conversation. arXiv preprint \narXiv:2203.13560 (2022) \n17. Zhao, W., Zhao, Y., Wang, S., Qin, B.: TransESC: Smoothing Emotional Support Conver-\nsation via Turn-Level State Transition. arXiv preprint arXiv:2305.03296 (2023) \n18. Li, C., Wang, J., Zhang, Y., Zhu, K., Hou, W., Lian, J., Luo, F., Yang, Q., Xie, X.: Large \nLanguage Models Understand and Can Be Enhanced by Emotional Stimuli.  arXiv preprint \narXiv:2307.11760 (2023) \n19. Zhang, W., Deng, Y., Liu, B., Pan, S.J., Bing, L.: Sentiment Analysis in the Era of Large \nLanguage Models: A Reality Check. arXiv preprint arXiv:2305.15005 (2023) \n14 \n20. Tao, C., Mou, L., Zhao, D., Yan, R.: RUBER: An Unsupervised Method for Automatic \nEvaluation of Open-Domain Dialog Systems. arXiv preprint arXiv:1701.03079 (2017) \n21. Zhong, M., Liu, Y., Yin, D., Mao, Y., Jiao, Y., Liu, P., Zhu, C., Ji, H., Han, J.: Towards a \nUnified Multi -Dimensional Evaluator for Text Generation. In: Proceedings of the 2022 \nConference on Empirical Methods in Natural Language Processing, pp. 2023 â€“2038. Asso-\nciation for Computational Linguistics (2022) \n22. Mehri, S., Eskenazi, M.: USR: An Unsupervised and Reference Free Evaluation Metric for \nDialog Generation. arXiv preprint arXiv:2005.00456 (2020) \n23. Liu, Y., Yang, T., Huang, S., Zhang, Z., Huang, H., Wei, F., Deng, W., Sun, F., Zhang, Q.: \nCalibrating LLM-Based Evaluation. arXiv preprint arXiv:2309.13308 (2023) \n24. Chen, Y., Wang, R., Jiang, H., Shi, S., Xu, R.: Exploring the Use of Large Language Mod-\nels for Reference -Free Text Quality Evaluation: A Preliminary Empirical Study.  arXiv \npreprint arXiv:2304.00723 (2023) \n25. F       A  R   K  Å›  Å„     W   M       B   **         S       R   & R      D   \nSummeval: Re -evaluating summarization evaluation.  Transactions of the Association for \nComputational Linguistics, 9, 391-409. (2021) \n26. Lin, Z., Trivedi, S., & Sun, J.: Generating with Confidence: Uncertainty Quantification for \nBlack-box Large Language Models. arXiv perprint arXiv:2305.19187. (2023) \n27. Spark Cognitive Large Model Web API Documentation | iFlytek Open Platform Docu-\nmentation Center, https://www.xfyun.cn/doc/spark/Web.html#_1-\n%E6%8E%A5%E5%8F%A3%E8%AF%B4%E6%98%8E, last accessed 2024/02/26. \n28. Baichuan Large Model - Gathering the world's knowledge and creating wonderful works - \nBaichuan Intelligence,  https://platform.baichuan-ai.com/docs/api, last accessed \n2024/02/26. \n29. How to quickly start Tongyi Qianwen_Model Service Lingji (DashScope) -Alibaba Cloud \nHelp Center , https://help.aliyun.com/zh/dashscope/developer-reference/quick-\nstart?spm=a2c4g.11186623.0.0.42951c83bCtJwX, last accessed 2024/02/25. \n30. THUDM/ChatGLM-6B: ChatGLM -6B: An Open Bilingual Dialogue Language Model | \nOpen Bilingual Dialogue Language Model , https://github.com/THUDM/ChatGLM-6B, \nlast accessed 2024/02/26. ",
  "topic": "Psychology",
  "concepts": [
    {
      "name": "Psychology",
      "score": 0.5111971497535706
    },
    {
      "name": "Computer science",
      "score": 0.4491550326347351
    },
    {
      "name": "Cognitive psychology",
      "score": 0.3219575881958008
    }
  ],
  "institutions": []
}