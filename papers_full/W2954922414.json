{
  "title": "Text Generation from Knowledge Graphs with Graph Transformers",
  "url": "https://openalex.org/W2954922414",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4286977836",
      "name": "Koncel-Kedziorski, Rik",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287077772",
      "name": "Bekal, Dhanush",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2353587976",
      "name": "Luan Yi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221535453",
      "name": "Lapata, Mirella",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4200857851",
      "name": "Hajishirzi, Hannaneh",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1902237438",
    "https://openalex.org/W2739046565",
    "https://openalex.org/W2250937897",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W1980287119",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2962905474",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2963718112",
    "https://openalex.org/W2964235962",
    "https://openalex.org/W2798749466",
    "https://openalex.org/W2963592583",
    "https://openalex.org/W2270627573",
    "https://openalex.org/W2100833569",
    "https://openalex.org/W2963374482",
    "https://openalex.org/W2798726039",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2116716943",
    "https://openalex.org/W2962950136",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2133459682",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W1828163288",
    "https://openalex.org/W2952913664",
    "https://openalex.org/W2964116568",
    "https://openalex.org/W2410539690",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963091658",
    "https://openalex.org/W2964321699",
    "https://openalex.org/W2801930304",
    "https://openalex.org/W2003170434"
  ],
  "abstract": "Generating texts which express complex ideas spanning multiple sentences requires a structured representation of their content (document plan), but these representations are prohibitively expensive to manually produce. In this work, we address the problem of generating coherent multi-sentence texts from the output of an information extraction system, and in particular a knowledge graph. Graphical knowledge representations are ubiquitous in computing, but pose a significant challenge for text generation techniques due to their non-hierarchical nature, collapsing of long-distance dependencies, and structural variety. We introduce a novel graph transforming encoder which can leverage the relational structure of such knowledge graphs without imposing linearization or hierarchical constraints. Incorporated into an encoder-decoder setup, we provide an end-to-end trainable system for graph-to-text generation that we apply to the domain of scientific text. Automatic and human evaluations show that our technique produces more informative texts which exhibit better document structure than competitive encoder-decoder methods.",
  "full_text": "Text Generation from Knowledge Graphs with Graph Transformers\nRik Koncel-Kedziorski1, Dhanush Bekal1, Yi Luan1, Mirella Lapata2, and Hannaneh Hajishirzi1,3\n1University of Washington\n{kedzior,dhanush,luanyi,hannaneh}@uw.edu\n2University of Edinburgh\nmlap@inf.ed.ac.uk\n3Allen Institute for Artiﬁcial Intelligence\nAbstract\nGenerating texts which express complex ideas\nspanning multiple sentences requires a struc-\ntured representation of their content (docu-\nment plan), but these representations are pro-\nhibitively expensive to manually produce. In\nthis work, we address the problem of gener-\nating coherent multi-sentence texts from the\noutput of an information extraction system,\nand in particular a knowledge graph. Graph-\nical knowledge representations are ubiquitous\nin computing, but pose a signiﬁcant challenge\nfor text generation techniques due to their\nnon-hierarchical nature, collapsing of long-\ndistance dependencies, and structural variety.\nWe introduce a novel graph transforming en-\ncoder which can leverage the relational struc-\nture of such knowledge graphs without impos-\ning linearization or hierarchical constraints.\nIncorporated into an encoder-decoder setup,\nwe provide an end-to-end trainable system\nfor graph-to-text generation that we apply to\nthe domain of scientiﬁc text. Automatic and\nhuman evaluations show that our technique\nproduces more informative texts which ex-\nhibit better document structure than competi-\ntive encoder-decoder methods. 1\n1 Introduction\nIncreases in computing power and model capac-\nity have made it possible to generate mostly-\ngrammatical sentence-length strings of natural\nlanguage text. However, generating several sen-\ntences related to a topic and which display over-\nall coherence and discourse-relatedness is an open\nchallenge. The difﬁculties are compounded in do-\nmains of interest such as scientiﬁc writing. Here\nthe variety of possible topics is great (e.g. top-\nics as diverse as driving, writing poetry, and pick-\ning stocks are all referenced in one subﬁeld of\n1Data and code available at https://github.com/\nrikdz/GraphWriter\nOur Modeloutperforms HMM modelsby 15% on this data.\nused-for\ncomparison\nWe present a CRF Modelfor Event Detection. CRF Model\nEvent Detection\nSemEval2011Task 11\nused-for\nWe evaluate this modelon SemEval2010 Task 11\nevaluate-for\nevaluate-for\nevaluate\n-for\nevaluate-for\nHMM Models\ncomparison\nTitle: Event Detection with Conditional Random Fields\nAbstract Graph\nFigure 1: A scientiﬁc text showing the annotations of\nan information extraction system and the correspond-\ning graphical representation. Coreference annotations\nshown in color. Our model learns to generate texts from\nautomatically extracted knowledge using a graph en-\ncoder decoder setup.\none scientiﬁc discipline). Additionally, there are\nstrong constraints on document structure, as sci-\nentiﬁc communication requires carefully ordered\nexplanations of processes and phenomena.\nMany researchers have sought to address these\nissues by working with structured inputs. Data-to-\ntext generation models (Konstas and Lapata, 2013;\nLebret et al., 2016; Wiseman et al., 2017; Pudup-\npully et al., 2019) condition text generation on\ntable-structured inputs. Tabular input representa-\ntions provide more guidance for producing longer\ntexts, but are only available for limited domains\nas they are assembled at great expense by manual\nannotation processes.\nThe current work explores the possibility of us-\ning information extraction (IE) systems to auto-\nmatically provide context for generating longer\ntexts (Figure 1). Robust IE systems are avail-\nable and have support over a large variety of tex-\ntual domains, and often provide rich annotations\nof relationships that extend beyond the scope of\narXiv:1904.02342v3  [cs.CL]  24 Mar 2022\na single sentence. But due to their automatic na-\nture, they also introduce challenges for generation\nsuch as erroneous annotations, structural variety,\nand signiﬁcant abstraction of surface textual fea-\ntures (such as grammatical relations or predicate-\nargument structure).\nTo effect our study, we use a collection of ab-\nstracts from a corpus of scientiﬁc articles (Ammar\net al., 2018). We extract entity, coreference, and\nrelation annotations for each abstract with a state-\nof-the-art information extraction system (Luan\net al., 2018), and represent the annotations as a\nknowledge graph which collapses co-referential\nentities. An example of a text and graph are shown\nin Figure 1. We use these graph/text pairs to train a\nnovel attention-based encoder-decoder model for\nknowledge-graph-to-text generation. Our model,\nGraphWriter, extends the successful Transformer\nfor text encoding (Vaswani et al., 2017) to graph-\nstructured inputs, building on the recent Graph\nAttention Network architecture (Veliˇckovi´c et al.,\n2018). The result is a powerful, general model\nfor graph encoding which can incorporate global\nstructural information when contextualizing ver-\ntices in their local neighborhoods.\nThe main contributions of this work include:\n1. We propose a new graph transformer encoder\nthat applies the successful sequence trans-\nformer to graph structured inputs.\n2. We show how IE output can be formed\nas a connected unlabeled graph for use in\nattention-based encoders.\n3. We provide a large dataset of knowledge-\ngraphs paired with scientiﬁc texts for further\nstudy.\nThrough detailed automatic and human evalua-\ntions, we demonstrate that automatically extracted\nknowledge can be used for multi-sentence text\ngeneration. We further show that structuring and\nencoding this knowledge as a graph leads to im-\nproved generation performance compared to other\nencoder-decoder setups. Finally, we show that\nGraphWriter’s transformer-style encoder is more\neffective than Graph Attention Networks on the\nknowledge-graph-to-text task.\n2 Related Work\nOur work falls under the larger scope of concept-\nto-text generation. Barzilay and Lapata (2005) in-\ntroduced a collective content selection model for\ngenerating summaries of football games from ta-\nbles of game statistics. Liang et al. (2009) jointly\nlearn to segment and align text with records, re-\nducing the supervision needed for learning. Kim\nand Mooney (2010) improve this technique by\nlearning a semantic parse to logical forms. Kon-\nstas and Lapata (2013) focus on the generation\nobjective, jointly learning planning and generat-\ning using a rhetorical (RST) grammar induction\napproach.\nThese earlier works often focused on smaller\nrecord generation datasets such as WeatherGov\nand RoboCup, but recently Mei et al. (2016)\nshowed how neural models can achieve strong re-\nsults on these standards, prompting researchers\nto investigate more challenging domains such as\nours.\nLebret et al. (2016) tackles the task of generat-\ning the ﬁrst sentence of a Wikipedia entry from the\nassociated infobox. They provide a large dataset\nof such entries and a language model conditioned\non tables. Our work focuses on a multi-sentence\ntask where relations can extend beyond sentence\nboundaries.\nWiseman et al. (2017) study the difﬁculty of ap-\nplying neural models to the data-to-text task. They\nintroduce a large dataset where a text summary of\na basketball game is paired with two tables of rel-\nevant statistics and show that neural models strug-\ngle to compete with template based methods over\nthis data. We propose generating from graphs\nrather than tables, and show that graphs can be ef-\nfectively encoded to capture both local and global\nstructure in the input.\nWe show that modeling knowledge as a graph\nimproves generation results, connecting our work\nto other graph-to-text tasks such as generating\nfrom Abstract Meaning Representation (AMR)\ngraphs. Konstas et al. (2017) provide the ﬁrst neu-\nral model for this task, and show that pretrain-\ning on a large dataset of noisy automatic parses\ncan improve results. However, they do not di-\nrectly model the graph structure, relying on lin-\nearization and sequence encoding instead. Cur-\nrent works improve this through more sophisti-\ncated graph encoding techniques. Marcheggiani\nand Perez-Beltrachini (2018) encode input graphs\ndirectly using a graph convolution encoder (Kipf\nand Welling, 2017). Our model extends the graph\nattention networks of Veli ˇckovi´c et al. (2018), a\ndirect descendant of the convolutional approach\nwhich offers more modeling power and has been\nTitle Abstract KG\nV ocab 29K 77K 54K\nTokens 413K 5.8M 1.2M\nEntities - - 518K\nAvg Length 9.9 141.2 -\nAvg #Vertices - - 12.42\nAvg #Edges - - 4.43\nTable 1: Data statistics of our AGENDA dataset. Aver-\nages are computed per instance.\nshown to improve performance. Song et al. (2018)\nuses a graph LSTM model to effect information\npropagation. At each timestep, a vertex is rep-\nresented by a gated combination of the vertices\nto which it is connected and the labeled edges\nconnecting them. Beck et al. (2018) use a sim-\nilar gated graph neural network. Both of these\ngated models make heavy use of label information,\nwhich is much sparser in our knowledge graphs\nthan in AMR. Generally, AMR graphs are denser,\nrooted, and connected, whereas the knowledge our\nmodel works with lacks these characteristics. For\nthis reason, we focus on attention-based models\nsuch as Veli ˇckovi´c et al. (2018), which impose\nfewer constraints on their input.\nFinally, our work is related to Wang et al.\n(2018) who offer a method for generating sci-\nentiﬁc abstracts from titles. Their model uses a\ngated rewriter network to write and revise sev-\neral draft outputs in several sequence-to-sequence\nsteps. While we operate in the same general do-\nmain as this work, our task setup is ultimately dif-\nferent due to the use of extracted information as in-\nput. We argue that our setup improves the task de-\nﬁned in Wang et al. (2018), and our more general\nmodel can be applied across tasks and domains.\n3 The AGENDA Dataset\nWe consider the problem of generating a text from\nautomatically extracted information ( knowledge).\nIE systems can produce high quality knowledge\nfor a variety of domains, synthesizing information\nfrom across sentence and even document bound-\naries. Generating coherent text from knowledge\nrequires a model which considers global charac-\nteristics of the knowledge as well as local charac-\nteristics of each entity. This feature of the task mo-\ntivates our use of graphs for representing knowl-\nedge, where neighborhoods localize important in-\nformation and paths through the graph build con-\nnections between distant nodes through interme-\ndiate ones. An example knowledge graph can be\nseen in Figure 1.\nWe formulate our problem as follows: given the\ntitle of a scientiﬁc article and a knowledge graph\nconstructed by an automatic information extrac-\ntion system, the goal is to generate an abstract that\na) is appropriate for the given title and b) expresses\nthe content of the knowledge graph in natural lan-\nguage text. To evaluate how well a model accom-\nplishes this goal, we introduce the Abstract GEN-\neration DAtaset (AGENDA), a dataset of knowl-\nedge graphs paired with scientiﬁc abstracts. Our\ndataset consists of 40k paper titles and abstracts\nfrom the Semantic Scholar Corpus taken from the\nproceedings of 12 top AI conferences (Ammar\net al., 2018).\nFor each abstract, we create a knowledge graph\nin two steps. First, we apply the SciIE system\nof Luan et al. (2018), a state-of-the-art science-\ndomain information extraction system. This sys-\ntem provides named entity recognition for scien-\ntiﬁc terms, with entity types Task, Method, Metric,\nMaterial, or Other Scientiﬁc Term. The model also\nproduces co-reference annotations as well as seven\nrelations that can obtain between different enti-\nties (Compare, Used-for, Feature-of, Hyponym-\nof, Evaluate-for, and Conjunction). For exam-\nple, in Figure 1, the node labeled “SemEval 2011\nTask 11” is of type ‘Task’, “HMM Models” is of\ntype ‘Model’, and there is a ‘Evaluate-For’ rela-\ntion showing that the models are evaluated on the\ntask.\nWe form these annotations into knowledge\ngraphs. We collapse co-referential entities into a\nsingle node associated with the longest mention\n(on the assumption that these will be the most in-\nformative). We then connect nodes to one another\nusing the relation annotations, treating these as la-\nbeled edges in the graph. The result is a possibly\nunconnected graph representation of the SciIE an-\nnotations for a given abstract.\nStatistics of the AGENDA dataset are available\nin Table 1. We split the AGENDA dataset into\n38,720 training, 1000 validation, and 1000 test\ndatapoints. We offer standardized data splits to fa-\ncilitate comparison.\n4 Model\nFollowing most work on neural generation we\nadopt an encoder-decoder architecture, shown in\nFigure 2: Converting disconnected labeled graph to\nconnected unlabeled graph for use in attention-based\nencoder. vi refer to vertices, Rij to relations, and Gis\na global context node.\nFigure 3, which we call GraphWriter. The input\nto GraphWriter is a title and a knowledge graph\nwhich are encoded respectively with a bidirec-\ntional recurrent neural network and a novel Graph\nTransformer architecture (to be discussed in Sec-\ntion 4.1). At each decoder time step, we attend on\nencodings of the knowledge graph and document\ntitle using the decoder hidden state ht ∈Rd. The\nresulting vectors are used to select output wt ei-\nther from the decoder’s vocabulary or by copying\nan entity from the knowledge graph. Details of our\ndecoding process are described in Section 4.2. The\nmodel is trained end-to-end to minimize the neg-\native log likelihood of the mixed copy and vocab-\nulary probability distribution and the human au-\nthored text.\n4.1 Encoder\nThe AGENDA dataset contains a knowledge\ngraph for each datapoint, but our model requires\nunlabeled, connected graphs as input. To encode\nknowledge graphs with this model, we restructure\neach graph as an unlabeled connected graph, pre-\nserving label information by the method described\nbelow and sketched in Figure 2.\nGraph Preparation We convert each graph to\nan unlabeled connected bipartite graphs following\na similar procedure to Beck et al. (2018). In this\nprocess, each labeled edge is replaced with two\nvertices: one representing the forward direction\nof the relation and one representing the reverse.\nThese new vertices are then connected to the en-\ntity vertices so that the directionality of the former\nedge is maintained. This restructures the original\nknowledge graph as an unlabeled directed graph\nwhere all vertices correspond to entities and rela-\ntions in the SciIE annotations without loss of infor-\nGraph \nTransformer Title Encoder\n“T ext Generation From Knowledge Graphs”\nAttention Layers\n=\nCopy \nMechanism\nVocab \nSoftmax\nwt\nht\nht+1\nwt-1\nct\n… …\nFigure 3: GraphWriter Model Overview\nmation. To promote information ﬂow between dis-\nconnected parts of the graph, we add a global ver-\ntex which connects all entity vertices. This global\nvertex will be used to initialize the decoder, analo-\ngously to the ﬁnal encoder hidden state in a tra-\nditional sequence to sequence model. The ﬁnal\nresult of these restructuring operations is a con-\nnected, unlabeled graph G = (V,E), where V is\na list of entities, relations, and a global node and\nE is an adjacency matrix describing the directed\nedges.\nGraph Transformer Our model is most sim-\nilar to the Graph Attention Network (GAT) of\nVeliˇckovi´c et al. (2018), which computes the\nhidden representations of each node in a graph\nby attending over its neighbors following a self-\nattention strategy. The use of self-attention in\nGAT addresses the shortcomings of prior meth-\nods based on graph convolutions (Defferrard et al.,\n2016; Kipf and Welling, 2017), but limits vertex\nupdates to information from adjacent nodes. Our\nmodel allows for a more global contextualization\nof each vertex through the use of a transformer-\nstyle architecture. The recently proposed Trans-\nformer (Vaswani et al., 2017) addresses the inher-\nent sequential computation shortcoming of recur-\nrent neural networks, enabling efﬁcient and par-\nalleled computation by invoking a self-attention\nmechanism for global context modeling. These\nmodels have shown promising results in a variety\nof text processing tasks (Radford et al., 2018).\nOur Graph Transformer encoder starts with self-\nNorm & Add\nNorm & Add\nFeedforward Network\nGraph Attention\nInput: V \u0002\u0001-1\nVˆ\nOutput: Ṽ\u0002\nVʹ\n×L\nBlock Network\nvi\u0002-1\nNi\n!1ij\nvi\u0002-1\n!2ij\nvi\u0002-1\n!Nij\n…\nGraph Attention \nHead 1\nHead 2\nHead N\nFigure 4: Graph Transformer\nattention of local neighborhoods of vertices; the\nkey difference with GAT is that our model in-\ncludes additional mechanisms for capturing global\ncontext. This additional modeling power allows\nthe Graph Transformer to better articulate how a\nvertex should be updated given the content of its\nneighbors, as well as to learn global patterns of\ngraph structure relevant to the model’s objective.\nSpeciﬁcally, V is embedded in a dense contin-\nuous space by the embedding process described at\nthe end of this section, resulting in matrix V0 =\n[vi],vi ∈Rd which will serve as input to the graph\ntransformer model shown in Figure 4. Each ver-\ntex representation vi is contextualized by attend-\ning over the other vertices to whichvi is connected\nin G. We use an N-headed self attention setup,\nwhere Nindependent attentions are calculated and\nconcatenated before a residual connection is ap-\nplied:\nˆ vi = vi +\nNn\nn=1\n∑\nj∈Ni\nαn\nijWn\nV vj (1)\nαn\nij = an(vi,vj) (2)\nHere, ∥denotes the concatenation of the N atten-\ntion heads, Ni denotes the neighborhood of vi in\nG, Wn\nV ∈ R( d\nN )×d, and where an are attention\nmechanisms parameterized per head. In this work,\nwe use attention functions of the following form:\na(qi,kj) = exp((WKkj)⊤WQqi)∑\nz∈Ni exp((WKkz)⊤WQqi) (3)\nEach a learns independent transformations\nWQ,WK ∈ R( d\nN )×d of q and k respectively,\nand the resulting product is normalized across all\nconnected edges. To reduce the tendency of these\ndot products to impede gradient ﬂow, we scale\nthem by 1√\nd, following Vaswani et al. (2017).\nThe Graph Transformer then augments these\nmulti-headed attention layers withblock networks.\nEach block applies the following transformations:\n˜ vi = LayerNorm(v′\ni + LayerNorm(ˆ vi))(4)\nv′\ni = FFN(LayerNorm(ˆ vi)) (5)\nWhere FFN(x) is a two layer feedforward network\nwith a non-linear transformation f between layers\ni.e. f(xW1 + b1)W2 + b2.\nStacking multiple blocks allows information to\npropagate through the graph. Blocks are stacked\nLtimes, with the output of layer l−1 taken as the\ninput to layer l, so that vl\ni = ˜ vl−1\ni . The resulting\nvertex encodings VL = [vL\ni ] represent entities,\nrelations, and the global node contextualized by\ntheir relationships in the graph structure. We refer\nto the resulting encodings as graph contextualized\nvertex encodings.\nEmbedding Vertices, Encoding TitleAs stated\nabove, the vertices of our graph correspond to\nentities and relations from the SciIE annotations.\nBecause each relation is represented as both a\nforward- and backward-looking vertex, we learn\ntwo embeddings per relation as well as an ini-\ntial embedding for the global node. Entities\ncorrespond to scientiﬁc terms which are often\nmulti-word expressions. To produce a single d-\ndimensional embedding per phrase, we use the last\nhidden state of a bidirectional RNN run over em-\nbeddings of each word in the entity phrase, i.e.\nBiRNN(x1 ... xm) for dense embeddings x and\nphrase length m. The output of our embedding\nstep is a collection V0 of d-dimensional vectors\nrepresenting each vertex in V.\nThe title input is also a short string, and so we\nencode it with another BiRNN to produce T =\nBiRNN(x′\n1 ...x ′\nm) for title word embedding x′.\n4.2 Decoder\nWe decode with an attention-based decoder with\na copy mechanism for copying input from the\nknowledge graph and title. At each decoding\ntimestep twe use decoder hidden state ht to com-\npute context vectors cg and cs for the graph and\ntitle sequence respectively. cg is computed using\nmulti-headed attention contextualized by ht:\ncg = ht +\nNn\nn=1\n∑\nj∈V\nαn\nj Wn\nGvLj (6)\nαj = a(ht,vLj) (7)\nfor a as described in Equation (1) by attending\nover the graph contextualized encodings VL. cs\nis computed similarly, attending over the title en-\ncoding T. We then construct the ﬁnal context vec-\ntor by concatenation, ct = [cg∥cs]. We use an\ninput-feeding decoder (Luong et al., 2015) where\nboth ht and ct are passed as input to the next RNN\ntimestep.\nWe compute a probabilitypof copying from the\ninput using ht and ct in a fashion similar to See\net al. (2017), that is:\np= σ(Wcopy[ht∥ct] +bcopy) (8)\nThe ﬁnal next-token probability distribution is:\np∗αcopy + (1−p) ∗αvocab, (9)\nWhere the probability distribution αcopy over en-\ntities and input tokens is computed as αcopy\nj =\na([ht∥ct],xj) for xj ∈V∥T. The remaining 1−p\nprobability is given to αvocab, which is calculated\nby scaling [ht∥ct] to the vocabulary size and tak-\ning a softmax.\n5 Experiments\nEvaluation Metrics We evaluate using a com-\nbination of human and automatic evaluations. For\nhuman evaluation, participants were asked to\ncompare abstracts generated by various models\nand those written by the authors of the scien-\ntiﬁc articles. We used Best-Worst Scaling (BWS;\n(Louviere and Woodworth, 1991; Louviere et al.,\n2015)), a less labor-intensive alternative to paired\ncomparisons that has been shown to produce more\nreliable results than rating scales (Kiritchenko and\nMohammad, 2016). Participants were presented\nwith two or three abstracts and asked to decide\nwhich one was better and which one was worse\nin order of grammar and ﬂuency (is the abstract\nwritten in well-formed English?), coherence (does\nthe abstract have an introduction, state the prob-\nlem or task, describe a solution, and discuss eval-\nuations or results?), and informativeness (does the\nabstract relate to the provided title and make use\nof appropriate scientiﬁc terms?). We provided ex-\namples of good and bad abstracts and explain how\nthey succeed or fail to meet the deﬁned criteria.\nBecause our dataset is scientiﬁc in nature, eval-\nuations must be done by experts and we can only\ncollect a limited number of these high quality dat-\napoints.2 The study was conducted by 15 experts\n(i.e. computer science students) who were famil-\niar with the abstract writing task and the content\nof the abstracts they judged. To supplement this,\nwe also provideautomatic metrics. We use BLEU\n(Papineni et al., 2002), an n-gram overlap measure\npopular in text generation tasks, and METEOR\n(Denkowski and Lavie, 2014), a machine transla-\ntion with paraphrase and language-speciﬁc consid-\nerations.\nComparisons We compare our GraphWriter\nagainst several strong baselines. In GAT, we\nreplace our Graph Transformer encoder with a\nGraph Attention Network of (Veli ˇckovi´c et al.,\n2018). This encoder consists of PReLU activa-\ntions stacked between 6 self-attention layers. To\ndetermine the usefulness of including graph re-\nlations, we compare to a model which uses only\nentities and title (EntityWriter). Finally, we com-\npare with the gated rewriter model of Wang et al.\n(2018) (Rewriter). This model uses only the docu-\nment title to iteratively rewrite drafts of its output.\n3\nImplementation Details Our models are trained\nend-to-end to minimize the negative joint log like-\nlihood of the target text vocabulary and the copied\nentity indices. We use SGD optimization with mo-\nmentum (Qian, 1999) and “warm restarts”, a cycli-\ncal regiment that reduces the learning rate from\n0.25 to 0.05 over the course of 5 epochs, then re-\nsets for the following epoch. Models are trained\nfor 15 epochs with early stopping (Prechelt, 1998)\nbased on the validation loss, with most models\nstopping between 8 and 13 epochs. We use single-\nlayer LSTMs (Hochreiter and Schmidhuber, 1997)\nas recurrent networks. We use dropout (Srivas-\ntava et al., 2014) in self attention layers set to\n0.3. Hidden states and embedding dimensions\nare ﬁxed at 500 and attentions learn 500 dimen-\n2Attempts to crowd source this evaluation failed.\n3Due to the larger size and greater variety of our dataset\nand accompanying vocabularies compared to theirs, we were\nunable to train this model with the reported batch size of 240.\nWe use batch size 24 instead, which is partially responsible\nfor the lower performance.\nBLEU METEOR\nGraphWriter 14.3 ± 1.01 18.8 ± 0.28\nGAT 12.2 ± 0.44 17.2 ± 0.63\nEntityWriter 10.38 16.53\nRewriter 1.05 8.38\nTable 2: Automatic Evaluations of Generation Sys-\ntems.\nsional projections. In Block layers, the feedfor-\nward network has an intermediate size of 2000,\nand we use a PReLU activation function (He et al.,\n2015). GraphWriter and GAT use L = 6 lay-\ners. The number of attention heads is set to 4. In\nall models, for both inputs and output, we replace\nwords occurring fewer than 5 times with <unk>\ntokens. In each abstract, we replace all mentions\nin a coreference chain in the abstract with the\ncanonical mention used in the graph. We decode\nwith beam search (Graves, 2012; Sutskever et al.,\n2014) with a beam size of 4. A post-processing\nstep deletes repeated sentences and repeated coor-\ndinated clauses.\n5.1 Results\nA comparison of all systems in terms of automatic\nmetrics is shown in Table 2. Our GraphWriter\nmodel outperforms other methods. We see that\nmodels which leverage title, entities, and relations\n(GraphWriter and GAT) outperform models which\nuse less information (EntityWriter and Rewriter).\nWe see that GraphWriter outperforms GAT\nacross metrics, indicating that the global contextu-\nalization provided by GraphWriter improves gen-\neration. To verify the performance gap between\nGraphWriter and GAT, we report the average test\nmetrics for 4 training runs of each model along\nwith their variances. We see that the variance of\nthe different models is non-overlapping, and in\nfact all training runs of GraphWriter outperformed\nall runs of GAT on these metrics.\nDoes Knowledge Help? To evaluate the value\nof knowledge in the generation task we compare\nour GraphWriter model to a model which does\nnot generate from knowledge. We provide expert\nannotators with 50 randomly-selected paper titles\nfrom the test set and ask them for a single judg-\nment according to the criteria described in Sec-\ntion 5. We pair each paper title with the generated\nabstracts produced by GraphWriter (a knowledge-\ninformed modes), Rewriter (a knowledge-agnostic\nmodel), and the gold abstract (with canonicalized\nBest Worst\nRewriter (No knowledge) 12% 64%\nGraphWriter (Knowledge) 24% 36%\nHuman Authored 64% 0%\nTable 3: Does knowledge improve generation? Human\nevaluations of best and worst abstract.\nWin Lose Tie\nStructure 63% 17% 20%\nInformativeness 43% 23% 33%\nGrammar 63% 23% 13%\nOverall 63% 17% 20%\nTable 4: Human Judgments of GraphWriter and Enti-\ntyWriter models.\ncoreferential mentions).\nResults of this comparison can be seen in Ta-\nble 3. We see that GraphWriter is selected as\n“Best” more often than Rewriter, and is less of-\nten selected as “Worst”, attesting to the value of\nincluding knowledge in the text generation pro-\ncess. We see that sometimes generated texts are\npreferred to human authored text, which is due in\npart to the disﬂuencies introduced by canonical-\nization of entity mentions.\nTo further understand the advantages of using\nknowledge graphs, we provide a more detailed\ncomparison of the GraphWriter and EntityWriter\nmodels. We select 30 additional test datapoints\nand ask experts to provide per-criterion judgments\nof the outputs of the two systems. Since both mod-\nels make use of extracted entities, we show this list\nalong with the title for each datapoint, and mod-\nify the description of Informativeness to include\n“making use of the provided entities”. Results of\nthis evaluation are shown in Table 4. Here we see\nthat including structured knowledge in the form of\na graph improves abstract generation compared to\ngenerating from an unstructured collection of en-\ntities. The largest gains are made in terms of doc-\nument structure and grammar, indicating that the\nstructure of the input knowledge is being trans-\nlated into the surface form.\nGenerating from Title The Rewriter model\n(Wang et al., 2018) considers the task of gener-\nating an abstract with only the paper’s title as in-\nput. We compare against this model because it is\namong the ﬁrst end-to-end systems to attempt to\nwrite scientiﬁc abstracts. However, the task setup\nused in Wang et al. (2018) differs signiﬁcantly\nfrom the task introduced in this work. In order\nTitle Block and Group Regularized Sparse Modeling for Dictionary Learning\nKnowledge (dictionary learning, CONJUNCTION, sparse coding) ; (optimization problems, USED-FOR, dictionary\nlearning) ; (optimization problems, USED-FOR, sparse coding). . .\nGraphWriter Sparse representations have recently been shown to be effective in many optimization problems. However,\nexisting dictionary learning methods are limited in the number of dictionary blocks, which can be expensive\nto obtain. In this paper, we propose a novel approach to dictionary learning based on sparse coding . . .\nGAT In this paper, we consider the problem of dictionary learning in well-known datasets. In particular, we\nconsider the problem of dictionary learning, where the goal is to ﬁnd a set of dictionary blocks that maximize\nthe likelihood of a given set of dictionary blocks . . .\nEntityWriter We propose a novel dictionary learning framework for reconstructed block/group sparse coding schemes.\nThe dictionary learning framework is based on the descent, which is a block structure of the group structure\n. . .\nRewriter This paper presents a new approach to the k-means of the algorithm. The proposed approach is based on the\nbasis of the stationarity algorithm. The algorithm is based on the fact that the number of bits is a constant\nof the base of the base of the input . . .\nGold This paper proposes a dictionary learning framework that combines the proposed block/group (BGSC) or\nreconstructed block/group (R-BGSC) sparse coding schemes with the novel Intra-block Coherence Suppres-\nsion Dictionary Learning algorithm. An important and distinguishing feature of the proposed framework is\nthat all dictionary blocks are trained simultaneously . . .\nTitle Image Interpolation with Directionlets\nKnowledge (directionally adaptive image interpolation USED-FOR edge information) ; (numeric and visual quality,\nHYPONYM-OF, directionally adaptive image interpolation) ; (directionlets, EV ALUATE-FOR, multiple-\ndirection wavelet transform) . . .\nGraphWriter In this paper, we propose a novel directionally adaptive image interpolation based on the multiple-direction\nwavelet transform, called directionlets, which can be used as a directionlets to improve the numeric and\nvisual quality of the directionally adaptive image interpolation . . .\nGAT In this paper, we propose a novel directionally adaptive image interpolation, called directionally adaptive\nimage interpolation, for directionally adaptive image interpolation , which is based on the multiple-direction\nwavelet transform . . .\nEntityWriter We present a novel directionally adaptive image interpolation for numeric and visual quality. The wavelet\ntransform is based on the wavelet transform between the low-resolution image and the interpolated image.\nThe high-resolution image is represented by a wavelet transform . . .\nRewriter We present a new method for ﬁnding topic-speciﬁc data sets. The key technical contributions of our ap-\nproach is to be a function of the terrestrial distributed memory. The key idea is to be a function of the page\nthat seeks to be ranked the buckets of the data. The basic idea is a new tool for the embedded space . . .\nGold We present a novel directionally adaptive image interpolation based on a multiple-direction wavelet trans-\nform, called directionlets. The directionally adaptive image interpolation uses directionlets to efﬁciently\ncapture directional features and to extract edge information along different directions from the low-\nresolution image . . .\nTable 5: Example outputs of various systems versus Gold.\nto make a fair comparison, we construct a variant\nof our model which is only provided with a title\nas input. We develop a model that predicts entities\nfrom the title, and then uses our knowledge-aware\nmodel to generate the abstract. For this compari-\nson we use the EntityWriter model with a collec-\ntion of entities inferred from the title alone (Infer-\nEntityWriter).\nTo infer relevant entities, we learn to embed ti-\ntles and entities extracted from the corresponding\nabstract in a shared dense vector space by min-\nimizing their cosine distance. We use negative\nsampling to provide deﬁnition to this vector space.\nAt test time, we use the title embedding to infer the\nK = 12 closest entities to feed into the InferEn-\ntityWriter model. Results are shown in Table 6,\nwhich shows that InferEntityWriter achieves bet-\nBLEU METEOR\nRewriter 1.05 8.38\nInferEntityWriter 3.60 12.2\nTable 6: Comparison of generation without knowledge\nand with Inferred Knowledge (InferEntityWriter)\nter results than Rewriter, indicating that the inter-\nmediate entity prediction step is helpful in abstract\ngeneration.\n5.2 Analysis\nTable 5 shows examples of various system outputs\nfor a particular test instance.We see that Graph-\nWriter makes use of more entities from the input,\narranged with more articulated textual context.\nIt demonstrates less repetition than GAT. Both\nGraphWriter and GAT show much better coher-\nence than EntityWriter, which copies entities from\nthe input into unreasonable contexts. Rewriter,\nwhile ﬂuent and grammatical, jumps from topic\nto topic, failing to relate as strongly to the input as\nthe knowledge-aware models.\nTo determine the shortcomings of our model,\nwe calculate rough error statistics over the out-\nputs of the GraphWriter on the test set. We no-\ntice that 40% of entities in the knowledge graphs\ndo not appear in the generated text. Future work\nshould address this coverage problem, perhaps\nthrough modiﬁcations to the inference procedure\nor a coverage loss (Tu et al., 2016) modiﬁed to\nthe speciﬁcs of this task. We ﬁnd that 18% of all\nsentences generated by our model repeat sentences\nor clauses and are subjected to the post-processing\npruning mentioned in Section 5. While this step is\na simple solution to improve generated outputs, a\nmore advanced solution is required.\n6 Conclusion\nWe have studied the problem of generating multi-\nsentence text from the output of automatic infor-\nmation extraction systems, and have shown that\nincorporating knowledge as graphs improves per-\nformance. We introduced GraphWriter, featuring\na new attention model for graph encoding, and\ndemonstrated its utility through human and au-\ntomatic evaluation compared to strong baselines.\nLastly, we provide a new resource for the genera-\ntion community, the AGENDA dataset of abstracts\nand knowledge. Future work could address the\nproblem of repetition and entity coverage in the\ngenerated texts.\nAcknowledgments\nThis research was supported by the Ofﬁce of Naval\nResearch under the MURI grant N00014-18-1-\n2670, NSF (IIS 1616112, III 1703166), Allen Dis-\ntinguished Investigator Award, Samsung GRO and\ngifts from Allen Institute for AI, Google, Amazon,\nand Bloomberg. We gratefully acknowledge the\nsupport of the European Research Council (Lap-\nata; award number 681760). We also thank the\nanonymous reviewers and the UW-NLP group for\ntheir helpful comments.\nReferences\nWaleed Ammar, Dirk Groeneveld, Chandra Bhagavat-\nula, Iz Beltagy, Miles Crawford, Doug Downey, Ja-\nson Dunkelberger, Ahmed Elgohary, Sergey Feld-\nman, Vu Ha, Rodney Kinney, Sebastian Kohlmeier,\nKyle Lo, Tyler Murray, Hsu-Han Ooi, Matthew Pe-\nters, Joanna Power, Sam Skjonsberg, Lucy Lu Wang,\nChris Wilhelm, Zheng Yuan, Madeleine van Zuylen,\nand Oren Etzioni. 2018. Construction of the Litera-\nture Graph in Semantic Scholar. In NAACL.\nRegina Barzilay and Mirella Lapata. 2005. Collective\nContent Selection for Concept-to-Text Generation.\nIn EMNLP, pages 331–338. Association for Com-\nputational Linguistics.\nDaniel Edward Robert Beck, Gholamreza Haffari, and\nTrevor Cohn. 2018. Graph-to-Sequence Learning\nusing Gated Graph Neural Networks. In ACL.\nMicha¨el Defferrard, Xavier Bresson, and Pierre Van-\ndergheynst. 2016. Convolutional Neural Networks\non Graphs with Fast Localized Spectral Filtering. In\nNeurIPS.\nMichael J. Denkowski and Alon Lavie. 2014. Meteor\nUniversal: Language Speciﬁc Translation Evalua-\ntion for Any Target Language. In Workshop on Sta-\ntistical Machine Translation.\nAlex Graves. 2012. Sequence Transduction with\nRecurrent Neural Networks. arXiv preprint\narXiv:1211.3711.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving Deep into Rectiﬁers: Surpass-\ning Human-Level Performance on ImageNet Classi-\nﬁcation. In Proceedings of the IEEE international\nconference on computer vision, pages 1026–1034.\nSepp Hochreiter and J¨urgen Schmidhuber. 1997. Long\nShort-Term Memory. Neural Comput., 9(8):1735–\n1780.\nJoohyun Kim and Raymond J Mooney. 2010. Gen-\nerative Alignment and Semantic Parsing for Learn-\ning from Ambiguous Supervision. In Proceedings\nof the 23rd International Conference on Computa-\ntional Linguistics: Posters, pages 543–551.\nThomas N. Kipf and Max Welling. 2017. Semi-\nSupervised Classiﬁcation with Graph Convolutional\nNetworks. In ICLR.\nSvetlana Kiritchenko and Saif Mohammad. 2016. Cap-\nturing Reliable Fine-Grained Sentiment Associa-\ntions by Crowdsourcing and Best-Worst Scaling. In\nNAACL-HLT.\nIoannis Konstas, Srinivasan Iyer, Mark Yatskar, Yejin\nChoi, and Luke S. Zettlemoyer. 2017. Neural AMR:\nSequence-to-Sequence Models for Parsing and Gen-\neration. In ACL.\nIoannis Konstas and Mirella Lapata. 2013. Inducing\nDocument Plans for Concept-to-Text Generation. In\nEMNLP, pages 1503–1514.\nR´emi Lebret, David Grangier, and Michael Auli. 2016.\nNeural Text Generation from Structured Data with\nApplication to the Biography Domain. In EMNLP.\nPercy Liang, Michael I. Jordan, and Dan Klein. 2009.\nLearning Semantic Correspondences with Less Su-\npervision. In ACL/AFNLP, pages 91–99.\nJordan J Louviere, Terry N Flynn, and Anthony Al-\nfred John Marley. 2015. Best-Worst Scaling: The-\nory, Methods and Applications. Cambridge Univer-\nsity Press.\nJordan J Louviere and George G Woodworth. 1991.\nBest-Worst Scaling: A Model for the Largest Dif-\nference Judgments. University of Alberta: Working\nPaper.\nYi Luan, Luheng He, Mari Ostendorf, and Han-\nnaneh Hajishirzi. 2018. Multi-Task Identiﬁcation\nof Entities, Relations, and Coreference for Scientiﬁc\nKnowledge Graph Construction. In EMNLP, pages\n3219–3232.\nThang Luong, Hieu Pham, and Christopher D. Man-\nning. 2015. Effective Approaches to Attention-\nbased Neural Machine Translation. In EMNLP.\nDiego Marcheggiani and Laura Perez-Beltrachini.\n2018. Deep Graph Convolutional Encoders for\nStructured Data to Text Generation. INLG.\nHongyuan Mei, Mohit Bansal, and Matthew R Walter.\n2016. What to talk about and how? Selective Gener-\nation using LSTMs with Coarse-to-Fine Alignment.\nIn NAACL-HLT, pages 720–730.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a Method for Automatic\nEvaluation of Machine Translation. In ACL.\nLutz Prechelt. 1998. Early Stopping - but when? In\nNeural Networks: Tricks of the Trade, pages 55–69,\nLondon, UK, UK. Springer-Verlag.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2019.\nData-to-Text Generation with Content Selection and\nPlanning. In AAAI.\nNing Qian. 1999. On the momentum term in gradient\ndescent learning algorithms. Neural networks : the\nofﬁcial journal of the International Neural Network\nSociety, 12 1:145–151.\nAlec Radford, Karthik Narasimhan, Tim Salimans,\nand Ilya Sutskever. 2018. Improving Language Un-\nderstanding by Generative Pre-Training. Accessed\nat https://s3-us-west-2.amazonaws.\ncom/openai-assets/research-covers/\nlanguage-unsupervised/language_\nunderstanding_paper.pdf.\nAbigail See, Peter J Liu, and Christopher D Man-\nning. 2017. Get to the Point: Summarization\nwith Pointer-Generator Networks. arXiv preprint\narXiv:1704.04368.\nLinfeng Song, Yue Zhang, Zhiguo Wang, and Daniel\nGildea. 2018. A Graph-to-Sequence Model for\nAMR-to-Text Generation. In ACL.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A Simple Way to Prevent Neural Net-\nworks from Overﬁtting. J. Mach. Learn. Res. ,\n15(1):1929–1958.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to Sequence Learning with Neural Net-\nworks. In NeurIPS, pages 3104–3112.\nZhaopeng Tu, Zhengdong Lu, Yang Liu, Xiaohua\nLiu, and Hang Li. 2016. Modeling Coverage\nfor Neural Machine Translation. arXiv preprint\narXiv:1601.04811.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In NeurIPS, pages 5998–6008.\nPetar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova,\nAdriana Romero, Pietro Li `o, and Yoshua Bengio.\n2018. Graph Attention Networks. In ICLR.\nQingyun Wang, Zhihao Zhou, Lifu Huang, Spencer\nWhitehead, Boliang Zhang, Heng Ji, and Kevin\nKnight. 2018. Paper Abstract Writing through Edit-\ning Mechanism. In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers) , pages 260–\n265, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nSam Wiseman, Stuart M Shieber, and Alexander M\nRush. 2017. Challenges in Data-to-document Gen-\neration. In EMNLP.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7879303693771362
    },
    {
      "name": "Encoder",
      "score": 0.6071921586990356
    },
    {
      "name": "Text generation",
      "score": 0.5735020637512207
    },
    {
      "name": "Knowledge graph",
      "score": 0.5618614554405212
    },
    {
      "name": "Transformer",
      "score": 0.5112357139587402
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.4925850033760071
    },
    {
      "name": "Theoretical computer science",
      "score": 0.48628461360931396
    },
    {
      "name": "Graph",
      "score": 0.45265501737594604
    },
    {
      "name": "Sentence",
      "score": 0.43790119886398315
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4366548955440521
    },
    {
      "name": "Natural language processing",
      "score": 0.380951464176178
    },
    {
      "name": "Information retrieval",
      "score": 0.33613821864128113
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I201448701",
      "name": "University of Washington",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I98677209",
      "name": "University of Edinburgh",
      "country": "GB"
    }
  ],
  "cited_by": 45
}