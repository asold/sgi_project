{
  "title": "Debiased Fine-Tuning for Vision-Language Models by Prompt Regularization",
  "url": "https://openalex.org/W4382240281",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2888240725",
      "name": "Beier Zhu",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A2551411703",
      "name": "Yulei Niu",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2517051981",
      "name": "Saeil Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2121220932",
      "name": "Min-Hoe Hur",
      "affiliations": [
        "Hyundai Motor Group (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2141833608",
      "name": "Han-wang Zhang",
      "affiliations": [
        "Nanyang Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2771951981",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2893749619",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W6735463952",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W2968474690",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W6745111092",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W6811668061",
    "https://openalex.org/W6780793664",
    "https://openalex.org/W3098528040",
    "https://openalex.org/W6779283709",
    "https://openalex.org/W6756577627",
    "https://openalex.org/W2890952061",
    "https://openalex.org/W104184427",
    "https://openalex.org/W2969862959",
    "https://openalex.org/W3087788237",
    "https://openalex.org/W6734871034",
    "https://openalex.org/W3009961661",
    "https://openalex.org/W6756234464",
    "https://openalex.org/W2946299408",
    "https://openalex.org/W6791190519",
    "https://openalex.org/W6782868315",
    "https://openalex.org/W3134873017",
    "https://openalex.org/W4226427271",
    "https://openalex.org/W2763549966",
    "https://openalex.org/W2131953535",
    "https://openalex.org/W4309811444",
    "https://openalex.org/W3177934633",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2952716587",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W4297795751",
    "https://openalex.org/W4312310776",
    "https://openalex.org/W3195577433",
    "https://openalex.org/W4289421997",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W4386790226",
    "https://openalex.org/W3204250462",
    "https://openalex.org/W3104182862",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W2955124656",
    "https://openalex.org/W3122855191",
    "https://openalex.org/W4281987380",
    "https://openalex.org/W3198675127",
    "https://openalex.org/W2963644680",
    "https://openalex.org/W4229453513",
    "https://openalex.org/W3088715381",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W2593768305",
    "https://openalex.org/W4287281411",
    "https://openalex.org/W2981720610",
    "https://openalex.org/W2980113592",
    "https://openalex.org/W3107803166",
    "https://openalex.org/W3033161486",
    "https://openalex.org/W3016211260",
    "https://openalex.org/W3173220247",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W3014611590"
  ],
  "abstract": "We present a new paradigm for fine-tuning large-scale vision-language pre-trained models on downstream task, dubbed Prompt Regularization (ProReg). Different from traditional fine-tuning which easily overfits to the downstream task data, ProReg uses the prediction by prompting the pretrained model to regularize the fine-tuning. The motivation is: by prompting the large model “a photo of a [CLASS]”, the fill-in answer is only dependent on the pretraining encyclopedic knowledge while independent of the task data distribution, which is usually biased. Specifically, given a training sample prediction during fine-tuning, we first calculate its Kullback-Leibler loss of the prompt prediction and Cross-Entropy loss of the ground-truth label, and then combine them with a proposed sample-wise adaptive trade- off weight, which automatically adjusts the transfer between the pretrained and downstream domains. On various out-of-distribution benchmarks, we show the consistently strong performance of ProReg compared with conventional fine-tuning, zero-shot prompt, prompt tuning, and other state-of-the-art methods.",
  "full_text": "Debiased Fine-Tuning for Vision-Language Models by Prompt Regularization\nBeier Zhu1, Yulei Niu2*, Saeil Lee3, Minhoe Hur4, Hanwang Zhang1\n1 Nanyang Technological University\n2 Columbia University\n3 HMGICS AIR Center\n4 AIRS Company, Hyundai Motor Group\nbeier002@e.ntu.edu.sg, yn.yuleiniu@gmail.com, saeil.lee@hmgics.com, minhoe.hur@hyundai.com,\nhanwangzhang@ntu.edu.sg\nAbstract\nWe present a new paradigm for fine-tuning large-scale vision-\nlanguage pre-trained models on downstream task, dubbed\nPrompt Regularization (ProReg). Different from traditional\nfine-tuning which easily overfits to the downstream task\ndata, ProReg uses the prediction by prompting the pretrained\nmodel to regularize the fine-tuning. The motivation is: by\nprompting the large model “a photo of a [CLASS]”, the fill-\nin answer is only dependent on the pretraining encyclopedic\nknowledge while independent of the task data distribution,\nwhich is usually biased. Specifically, given a training sample\nprediction during fine-tuning, we first calculate its Kullback-\nLeibler loss of the prompt prediction and Cross-Entropy loss\nof the ground-truth label, and then combine them with a pro-\nposed sample-wise adaptive trade-off weight, which automat-\nically adjusts the transfer between the pretrained and down-\nstream domains. On various out-of-distribution benchmarks,\nwe show the consistently strong performance of ProReg\ncompared with conventional fine-tuning, zero-shot prompt,\nprompt tuning, and other state-of-the-art methods.\nIntroduction\nPlease think about it: when you want to train a vision model\nfor a task, what is the first thing first? Most likely, you will\ndownload an off-the-shelf foundation model (Bommasani\net al. 2021), e.g., ResNet (He et al. 2016) or CLIP (Radford\net al. 2021), pretrained on a large-scale dataset such as Ima-\ngeNet (Deng et al. 2009) or image-text pairs collected from\nthe Internet (Radford et al. 2021); then, remove its classi-\nfier head, plug in your own task-specific head to the penul-\ntimate layer, and finally fine-tune on your own task data.\nSuch “pretrain, fine-tune” paradigm has become a nearly\nubiquitous standard for CV community—from classifica-\ntion to generation (Viazovetskyi, Ivashkin, and Kashin 2020;\nBrock, Donahue, and Simonyan 2018), regions to pixels (He\net al. 2017), and single modality to multi-modality (Ander-\nson et al. 2018; Lu et al. 2019). The underlying empirical\nprinciple is that the pretrained model as initialization plays\na role in regularization which reduces the variance of the\nfine-tuning model (Sutskever et al. 2013).\n*Corresponding author. Work started when at NTU.\nCopyright © 2023, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nDespite the beneficial regularization, the pretraining\nknowledge has a negative impact, especially when the down-\nstream task data is limited or biased (Wang et al. 2019; Yang\net al. 2021): the early exposed encyclopedic or generic fea-\ntures from the pretrained model may mislead the fine-tuning\nto focus on the task-unrelated attributes, resulting in a biased\nfine-tuned model. Figure 1 shows three types of biases: Fig-\nure 1(a) contextual bias: images of training and test set con-\ntain distinct backgrounds, e.g., if most of the training “bird”\nimages are in “sky” background, the fine-tuning will mis-\nclassify “bird on ground” as “dog”; Figure 1(c) image style\ngap: test image style is unseen during training, e.g., if train-\ning images are from art painting, cartoon and real domain,\nthe fine-tuned model is able to classify the in-distribution\nart painting “dog” but is confused when testing on a sketch\n“dog”; Figure 1(e) language bias: for every question type,\ntrain and test sets have different prior distributions of an-\nswers, e.g., if most VQA training “bananas” images are “yel-\nlow”, it mistakes the answer “yellow” for question “what\ncolor are the bananas?”, given an image of green bananas.\nRecently, NLP community presents a tuning-free\nparadigm called “pretrained, prompt, predict” (Liu et al.\n2021a) and it has been quickly migrated to CV tasks (Zhou\net al. 2021; Tsimpoukelli et al. 2021; Yao et al. 2021) by\nusing a pretrained multi-modal model (Kim, Son, and Kim\n2021; Radford et al. 2021; Zhang et al. 2021). For example,\nimage classification can be cast into the cloze prompt: “a\nphoto of a [CLASS]”, where the prediction can be found\nas the class word whose fill-in sentence is most similar to\nthe query image. The similarity can be calculated directly\nfrom the pretrained model in a zero-shot learning fashion.\nAs prompt is a rule-based query that has nothing to do with\nthe downstream statistics, the prediction is expected to be\nindependent to downstream domain and faithfully respects\nthe pretrained knowledge.\nYet, relying too much on the general knowledge also hurts\nthe domain-specific generalization. For example, as shown\nin Figure 1(b), although prompt can correctly focus on the\nforeground object, it is less discriminative to distinguish be-\ntween “rat” and “cat” in domain-specific animal images; in\nFigure 1(d), prompt’s prediction confidence is not as dis-\ncriminative as that of fine-tuning; in Figure 1(f), prompt-\nbased VQA is too general to perform the downstream task\nof counting “bananas”. To this end, “prompt tuning” is pro-\nThe Thirty-Seventh AAAI Conference on Artificial Intelligence (AAAI-23)\n3834\nTraining Stage\nA: yellow\n(e) Fine-tune Results\nInference Stage\n(f) Zero-shot Results\nPretext Task: MLM\nQ: How many bananas?\nPrompt\nThere are [MASK] bananas.\n [MASK] bananas are  \nhanging under sunlight. \n[MASK] = many\nthere are [MASK]  \ngreen bananas.\n[MASK] = many\n A:yellow\nGT: green\n(a) Fine-tune Results\n(b) Zero-shot Results\nRat\n Sheep Cat\nCat Bear Rat\nBird\nCatRat\n(c) Fine-tune Results\n(d) Zero-shot Results\nTraining stage\nOOD ID\n“a photo of a [CLASS]”\nFine-tune Logits Prompt Logits\nDog\nDog\nContextual Bias Image Style Gap Language Bias\nInference Stage\nGiraﬀePred.\nPred.Label Image Image Attention\nDog\nTraining stage Inference Stage\nImage & \nAttention\nLabel\nImage\n Logits\n7.5 15 22.5 30\n7.5 15 22.5 30\n7.5 15 22.5 30\n7.5 15 22.5 30\nDog\nQ: What color are the bananas?\n A:many\nGT: 7\nInference Stage\nFigure 1: Examples of the contextual bias (He, Shen, and Cui 2020; Nam et al. 2020), image style bias (Li et al. 2017) and\nlanguage bias (Goyal et al. 2017; Agrawal et al. 2018) caused by only relying on fine-tuning. (a&c&e): If the downstream data\nis biased, fine-tuning is biased. (b&d&f): Pretraining knowledge is too general to perform domain-specific downstream tasks.\nposed to fine-tune the token embeddings in a prompt using\nthe task data (Liu et al. 2021a,b; Han et al. 2021; Zhou\net al. 2021). For example, the prefix “a photo of a” be-\nfore the cloze blank [CLASS]can be replaced with a set of\ntunable parameters. Prompt tuning is essentially fine-tuning\nwith fixed backbone and tunable head (i.e., the prompt pre-\nfix). Therefore, it still inherits the above drawbacks of the\nbiased fine-tuning.\nIn this paper, we present a new fine-tuning paradigm,\ncalled Prompt Regularization (ProReg), for a just-right\nknowledge transfer from pretrained model to fine-tuned\nmodel. As expected, ProReg can fine-tune the resultant\nmodel, neither biased towards the pretrained knowledge\nnor towards the downstream knowledge. We formulate the\ndownstream knowledge as the ground-truth annotations in\ndownstream tasks, and represent the pretrained knowledge\nwith the “soft” label of the downstream data generated by\nprompting the pre-trained model. We then proposed the\nProReg loss to enable learning from both of the knowledge.\nIt is worth noting that different from traditional knowledge\ndistillation that using a constant weight λ ∈ (0, 1) to trade-\noff the contribution of the knowledge:Lkd = (1−λ) ·Lce +\nλ·Lkl, we propose a sample-wise adaptive weight to achieve\na good trade-off between the two knowledge (Section ). The\nproposed weight inspects whether the task-specific knowl-\nedge or the pre-trained general knowledge dominates the\noptimization process for each fine-tuning sample, which in-\ndeed requires a different ratio of the two knowledge types.\nWe show that the estimation of the ratio evolves during the\ntraining process and can be automatically calculated on-the-\nfly.\nWe implement ProReg on top of two off-the-shelf large-\nscale pretrained models: CLIP (Radford et al. 2021) and\nViLT (Kim, Son, and Kim 2021), which demonstrates that\nProReg is applicable to different vision-language models\nthat adopt masked language modeling or contrastive learn-\ning as pretraining tasks. We conduct extensive evaluations\nfor ProReg on various out-of-distribution benchmarks, in-\ncluding BAR (Nam et al. 2020), NICO (He, Shen, and Cui\n2020), PACS (Li et al. 2017) and DomainNet (Peng et al.\n2019) for image classification tasks and VQA-CP (Agrawal\net al. 2018) for visual question answering tasks. We demon-\nstrate that: 1) ProReg consistently outperforms zero-shot\nprompt, conventional fine-tuning, and prompt tuning on all\nthe datasets, 2) ProReg achieves compelling performance in\nboth out-of-distribution and in-distribution settings. Thus,\nreaders can feel free to use ProReg regardless of the train-\ning and testing distribution discrepancy.\nRelated Work\nVision-Language Models (VLM).Most existing VLMs use\n1) Masked language modeling (Kim, Son, and Kim 2021;\nLi et al. 2019; Lu et al. 2019), 2) Image text pairing (Tan\nand Bansal 2019; Huang et al. 2020), and 3) Contrastive\nlearning (Radford et al. 2021) as their pretraining objectives.\nRecently there is a line of adapting the existing VLMs to\nthe downstream tasks. Conventional fine-tuning paradigm\nadds an additional classifier on top of the visual backbone\n(Linear Probe (Radford et al. 2021)) or additional feature\nadapter, (CLIP-Adapter (Gao et al. 2021)). Prompt-based\nlearning that tunes the prompt to maximize the ground-\ntruth token has gained its popularity,e.g., CoOp (Zhou et al.\n2021) and CoCoOp (Zhou et al. 2022). ProGrad (Zhu et al.\n2022) bridges generalization gap by matching the gradient\nof prompt to the general knowledge. ProDA (Lu et al. 2022)\nintroduces prompt distribution learning to adapts VLMs to\ndownstream classification tasks. As discussed in Section ,\nboth of these two fine-tune paradigms may result in biased\ndownstream models, in this work, we aims to fine-tune a de-\nbiased VLM for downstream tasks.\nOOD Generalization. In real world, test distribution may\nshift from the training distribution, such as domain gener-\nalization (Ben-David et al. 2007; Tzeng et al. 2017), long-\ntailed classification (Menon et al. 2020; Tang, Huang, and\nZhang 2020), contextual bias (Nam et al. 2020; He, Shen,\nand Cui 2020), and language bias (Wu and Mooney 2019;\nRamakrishnan, Agrawal, and Lee 2018; Cadene et al. 2019;\nNiu et al. 2021). WiSE (Wortsman et al. 2022) shares the\nsame goal to improve OOD performance, which ensembles\n3835\nthe weights of zero-shot and fine-tuned models. However,\nit requires the fine-tuned model and the pre-trained model to\nhave the same architecture. Differently, our ProReg is free of\nthis requirement and allows the modification of architecture.\nThe ProReg Pipeline\nPre-trained Model\nIn this paper, we adopt two state-of-the-art VLM mod-\nels as our backbones, i.e., Constrastive Language-Image\nPre-training model (CLIP (Radford et al. 2021) for image\nclassification tasks and Vision-and-Language Transformer\n(ViLT (Kim, Son, and Kim 2021))) for VQA tasks.\nCLIP collects large amounts of images with natural lan-\nguage description and is trained in a contrastive manner.\nThe associated image and text pairs are regarded as posi-\ntive pairs, while the mis-matched image and text pair are\nregarded as negative ones. The contrastive loss aims to max-\nimize the cosine similarity of the positive pairs in the batch\nwhile minimize the cosine similarity of the negative pairs.\nViLT consists of stacked blocks that include a multi-\nheaded self-attention and a multi-layer perceptron layer. It\nis pretrained with two pretext tasks: Image Text Matching\n(ITM) and Masked Language Modeling (MLM). For ITM,\nViLT selects half of the sentences and replaces each of them\nwith a mismatched sentence. Then, a binary classifier is\nadded to predict whether the image and the sentence match\nwith each other. MLM first randomly replace 15% of input\ntext tokens with masked tokens, e.g., [MASK], and then the\nmodel is optimized to predict the masked tokens, given other\nnon-masked tokens and image feature.\nFor classification tasks, CLIP based models show supe-\nrior performance over the ViLT one, thus we only report the\nCLIP results in the main paper while the image classifica-\ntion results of ViLT can be found in Appendix. For VQA\ntasks, the input text varies sample-wisely, it is difficult for\nCLIP model to infer and optimize. As a result, we only im-\nplement the ProReg on ViLT models. Note that our approach\nis applicable to broader vision-language models whose pre-\ntrained pretext tasks include ITM and MLM objectives or\ncontrastive objective.\nPrompt-based Zero-shot Inference\nProper prompt can adapt the pre-trained model to down-\nstream tasks without fine-tuning. In this section, we illustrate\nhow CLIP performs zero-shot inference for image classifi-\ncation tasks and how ViLT can leverage carefully designed\nprompt to perform zero-shot prediction for VQA tasks.\nImage Classification. We formulate image classification as\nan image-text matching problem where the text is designed\nby a simple static prompt template. Take the action recog-\nnition dataset BAR (Nam et al. 2020) in Figure 2(a) as an\nexample. We first compute the text embeddingwi of “a per-\nson is [CLASS].”, where the “[CLASS]” is replaced by\ni-th class name. Then the probability distribution of the im-\nage feature x over K classes is given by:\nf(y = i|x; θ) = exp(< x, wi >/τ )\nPK\nj=1 exp(< x, wj >/τ )\n(1)\nDowns.\nT\nask\nPretext\nTask Prompt Example\nImage\nCLS Contra. • the\nperson is [CLASS].\n• a photo of a [CLASS].\nVQA\nMLM\n• ho\nw many hats are there?\nQ2S\n− − →there are [MASK]hats.\n• what color is the shirt?\nQ2S\n− − →the color of the shirt is [MASK].\nITM\n• does\nthis fruit grow on vines?\nQ2S\n− − →this fruit grow on vines.\n• is the zebra sleeping?\nQ2S\n− − →the zebra is sleeping.\nTable 1: Prompt Examples. For VQA, the prompt is gener-\nated according to the question type. If the question is open-\nended, we convert the question to a statement with masked\ntoken and use MLM head to predict the answer. If the ques-\ntion is closed-ended, we convert the question to a statement\nand use ITM head to predict “yes” or “no”. For image clas-\nsification, we adopt a simple static template and use cosine\nsimilarity to predict the class.\nwhere τ is the temperature learned by CLIP.\nVisual Question Answering (VQA). The design of prompts\nfor VQA task is less straightforward because the text in-\nputs in VQA (i.e., questions) and pretraining tasks (i.e., cap-\ntions) have different forms. Notice that VQA dataset con-\nsists of two types of questions: open-ended questions (e.g.,\n“what color is the cake?”) and closed-ended questions (e.g.,\n“any brown apples in the picture?”). We convert the ques-\ntion to a statement and name such design as Question-to-\nStatement (Q2S) prompt. Table 1 shows some prompt ex-\namples. Specifically, we use ITM for closed-ended ques-\ntions and MLM for open-ended questions. For example, for\na closed-ended question like “any brown apples in the pic-\nture?”, the prompt statement is generated as “some brown\napples in the picture.”. The text embedding together with\nthe visual feature are fed into ITM head to predict whether\nthey match with each other. A high match score corresponds\nto the answer “yes” and a low score to “no”. For an open-\nended question like “what color is the cake?”, the prompt is\ngenerated as “the color of the cake is [MASK].”. Similar to\nEq. (1), we search the candidate answer set to find the one\nhas the highest probability.\nSample-wise Adaptive Trade-off Weight\nIn this section, we illustrate how to implement ProReg for\nCLIP models, the ProReg for ViLT models can be imple-\nmented analogously or refer to Appendix. Figure 2 (b) and\n(c) show the comparison of current fine-tuning works. Fig-\nure 2 (b) shows the pipeline of “pretrain, prompt, fine-tune”\nwhere the classification head is generated by the optimizing\nthe continuous prompt. We name the “pretrain, fine-tune”\nparadigm in Figure 2(c) as “Conventional Fine-tuning”,\nwhich adds a classification head for prediction. The clas-\nsification head can be randomly initialized or initialized by\n3836\n(a) Zero-shot Prompt Inference\nclimbing\ndriving\nﬁshing\nvaul8ng\nracing\nthrowing\nThe person \nis [CLASS].\n(b) Prompt Tuning\n[CLASS].…[V]1 [V]2 [V]M\n(c) Conven8onal Fine-tuning\nx\nCLS Head\n̂ y\npredic8on\nCE Loss\ny\nlabel\nprompt  \npredic8on\n̂ yzs\n(d) ProReg Fine-tuning\nx\nw1 w2 w6\nsimilarity \n…\n…\nText  \nEncoder\n❄\nImage  \nEncoder\n❄\n̂ ypredic8on\nCE Loss\ny\nlabel\n\" tunable  \n\"\nProReg \n Loss\n\"\nImage  \nEncoder\nx\nw1 w2 w6…\n…\nText  \nEncoder\n❄\nImage  \nEncoder\n❄\ny\nlabel\nx\nCLS Head\n̂ y\npredic8on\n\"\nImage  \nEncoder\n̂ yzs\nprompt \npredic8on\n\"\n\"\n❄\nTunable\nFixed\nFigure 2: Comparisons of frameworks based on CLIP models for image classification. (a) Zero-shot Prompt Inference, i.e.,\n“pretrain, prompt”, generates the classification weights by a text encoder thought prompting. (b) Prompt Tuning, i.e., “pre-\ntrain, prompt, fine-tune”, learns continuous prompt by minimizing the cross-entropy loss. (c) Conventional Fine-tuning, (i.e.,\n“pre-train, fine-tune”), adds classification head on top of the visual backbone and is optimized by cross-entropy loss. The classi-\nfication head can be randomly initialized or initialized by the text encoder thought prompting . (d) ProReg fine-tuning. Different\nfrom conventional fine-tuning, the supervisions come from two sources: the ground-truth target from the downstream domain,\nand the zero-shot prompt prediction from the pretraining domain. The classification head of ProReg is initialized by prompting.\nfeeding the hand-craft prompt to the text encoder. The com-\nmon characteristic is that both of the two paradigms only use\nthe ground-truth labels as supervision. As discussed in Sec-\ntion , both of these two fine-tune paradigms may result in\nbiased downstream models, especially when the task data is\nlimited and biased (Yue et al. 2020; Wang et al. 2019; Yang\net al. 2021).\nWe present a new fine-tuning paradigm, dubbed Prompt\nRegularization (ProReg), to transfer the knowledge from\nthe pretraining domain to the downstream domain. ProReg\nconsiders two types of supervisions for optimization: task-\nspecific downstream knowledge and task-agnostic general\nknowledge.\nTask-specific downstream knowledge. The ground-truth\nlabels serve as the task-specific knowledge and allows the\nmodel f(; θ) to be adapted to downstream task. The cross-\nentropy Lce of an image x is obtained as:\nLce(θ) =−\nX\ni\nyi log fi(x; θ), (2)\nwhere y denotes the one-hot ground-truth vector, and the\nsubscript i denotes the i-th label; f(; θ) is the classification\nmodel initialized by pretrained model.\nTask-agnostic general knowledge. Compared to large-\nscale datasets for pre-training, the task-specific data for\ndownstream task may be limited or even biased. As a result,\nonly taking the ground-truth annotations as supervision may\nlead to biased fine-tuning. In order to achieve debiased fine-\ntuning, we also use task-agnostic pre-training knowledge as\nregularization. We take the zero-shot prompt prediction of\nthe pre-trained model yzs as the regularization knowledge,\nand use kullback-leibler divergence (L kl) between the fine-\ntuned model prediction and the zero-shot prompt prediction\nas the regularization term:\nLkl(θ) =−\nX\ni\nyzs\ni log fi(x; θ)\nyzs\ni\n, (3)\nTo fine-tune the model neither biased towards the\ndownstream domain nor the pre-trained knowledge do-\nmain, a straightforward intuition is knowledge distillation\n(KD) (Hinton, Vinyals, and Dean 2015), to combine the two\nterms with a constant weight λ ∈ (0, 1):\nLkd = (1− λ) · Lce + λ · Lkl, (4)\nwhere we omit θ for simplicity. However, this simple solu-\ntion overlooks that the trade-off of the two knowledge varies\nsample-wisely and evolves during training. In the next part,\nwe will analyze the trade-off from the view of task-agnostic\nknowledge decomposition and propose a dynamic weight to\nbalance the two loss terms for each training sample.\nDecomposition of task-agnostic general knowledge. We\ndecompose the regularization term Lkl in Eq. (3) as Lkl =\nLce + (Lkl − Lce). The first term Lce aims to learn task-\nspecific knowledge. The second term (Lkl − Lce) is the key\ncomponent to learn supplementary knowledge that is pro-\nvided by the task-agnostic general knowledge but not in-\ncluded in the task-specific downstream knowledge. To better\nunderstand the contribution of the two knowledge, we cal-\nculate their gradients w.r.tthe logit z of the model f on the\nclass t as:\n∇zt Lce = ft − yt, (5)\n∇zt (Lkl − Lce) = (ft − yzs) − (ft − yt) =yt − yzs\nt . (6)\nWe have the following observations: First, the two gradi-\nents always have opposite directions, learning one knowl-\nedge well will inevitably lead to the deterioration of the\nother knowledge. If t is the true class, i.e., yt = 1, then\n∇zt Lce = ft − 1 < 0 while ∇zt (Lkl − Lce) = 1− yzs\nt > 0.\nIf t is the false class, i.e., yt = 0, ∇zt Lce = ft > 0 while\n∇zt (Lkl − Lce) = −yzs\nt < 0. The first observation reveals\nthe conflict between task-specific knowledge learning and\nsupplementary knowledge learning, which motivates us to\nbalance the learning process of the two knowledge.\nSecond, the gradient yt − yzs\nt for learning supplementary\nknowledge is constant in different optimization steps, and\n3837\nthe magnitude varies sample-wisely. As a comparison, the\ngradient ft − yt for learning task-specific knowledge keeps\nupdated during the training process (f t changes after each\noptimization step). The above analysis motivates us to trade-\noff the two knowledge in a sample-wise and dynamic man-\nner, dubbed ProReg:\nLProReg = Lce +w·(Lkl −Lce) = (1−w)·Lce +w·Lkl. (7)\nThe sample-wise trade-off weight w aims to prevent the\nmodel from biasing to either task-specific knowledge and\nsupplementary knowledge. There are two typical cases that\naffect the determination of w.\nIf |∇zt Lce| is much larger than|∇zt (Lkl −Lce)|, the task-\nspecific knowledge will dominate the overall optimization\ndirection of this sample. We assign a larger weight w to\nthe term (Lkl − Lce) to emphasis the supplementary knowl-\nedge and prevent the model from biasing to the downstream\nknowledge. On the contrary, if |∇zt (Lkl − Lce)| is much\nlarger than |∇zt Lce|, the task-specific downstream knowl-\nedge might be neglected. We assign a smaller weight w\nto (Lkl − Lce) to guarantee the learning of downstream\nknowledge. From the above analysis, the trade-off weightw\nshould be proportional to |∇zt Lce|/|∇zt (Lkl − Lce)| to bal-\nance the two knowledge. From Eq. (5) and Eq. (6), we have\n|∇zt Lce|/|∇zt (Lkl − Lce)| = yt−ft\nyt−yzs\nt\n. For simplifying anal-\nysis, we consider the binary classification case and assume\nthe ground-truth label is t, i.e., yt = 1. For true class t, we\nhave yt−ft\nyt−yzs\nt\n= 1−ft\n1−yzs\nt\n∝ yzs\nt\nft\n. For the false class (1 − t), we\nhave y1−t−f1−t\ny1−t−yzs\n1−t\n= 0−(1−ft)\n0−(1−yzs\nt ) ∝ yzs\nt\nft\n. From Eq. (7), we also\nexpect w ∈ [0, 1] to guarantee the positive sign of Lce and\nLkl. Therefore, we design the trade-off weight as\nw = ft\nft + yzs\nt\n∝ yzs\nt\nft\n, (8)\nwhere t is the ground-truth label.\nIn our implementation, in addition to the sample-wisely\ntrade-off weight w, we further introduce a hyper-parameter\nα >0 on Lkl in Eq. (7) to control the strength of trade-off\nglobally. Our ProReg is formulated as:\nLProReg = (1− w) · Lce + α · w · Lkl (9)\nExperiments\nDatasets and Implementation Details\nBAR (Nam et al. 2020) is a real-world image dataset for ac-\ntion classification, where the action is biased to the place.\nExamples of “climbing” class are shown in Figure 3(a),\nwhere most training background are rocks, while the one of\ntesting images are snow.\nNICO (He, Shen, and Cui 2020) dataset is designed for\nOut-of-Distribution (OOD) image classification, which has\n2 subsets (animal and vehicle), 19 classes and 188 contexts.\nFor each class, we select 3 different contexts for both train-\ning and test set,e.g., the training contexts for “sheep” are “in\nwater”, “on road” and “on snow”, while the test contexts are\n“at sunset”, “on grass” and “in forset”. Please kindly refer to\nApprendix for more details of our setting.\nTraining Set TesKng Set\nClimb\n(a) Examples of BAR Dataset \n(c) Answer DistribuKon of ``What color?’’ on VQA-CP Dataset\nwhite red blue green (others)\nblack pink gray ye. (others)br.\nGuitar\nTraining Set\n TesKng Set\n(b) Examples of PACS Dataset\nTraining Set Answer DistribuKon\nTe\nsKng Set Answer DistribuKon\nFigure 3: Examples of OOD benchmarks.\nPACS (Li et al. 2017) covers photo, sketch, cartoon and\npainting domains. Figure 3(b) shows some examples. The\nmodel is trained and validated on any three seen domains\nthen tested on the rest unseen domain.\nDomainNet (Peng et al. 2019) consists of images from\n345 classes covering the “sktech”, “real”, “clipart” and\n“painting” domains. Here, we use the “sktech” domain as\nID dataset, and use “clipart” and “painting” as the OOD\ndatasets. Please kindly refer to Appendix for more details.\nVQA-CP (Agrawal et al. 2018) is proposed to examine the\ngeneralizability of VQA models when the language prior\nvaries significantly between training and testing splits. Fig-\nure 3(c) shows some answer distribution of the training and\ntest set, e.g., most of “what color ” questions are answered\nas “white” in training set while “black ” in the test set.\nExperimental Details. For ViLT-based models, we fol-\nlowed the original fine-tuning settings in (Kim, Son,\nand Kim 2021), which adopt the ViLT-B/32 model with\nAdamW (Loshchilov and Hutter 2018) optimizer for 10\nepochs for all datasets. For CLIP-based models, we used\nthe ViT-B/32 backbone and adopted the ViLT fine-tuning\nsettings, including the training epoch, optimizer, warmup\nschedule and image pre-processing, etc. α is set to 2 for all\nexperiments. See Appendix for more details.\nFine-tuning Baselines. For CLIP-based models, we com-\npared ProReg with 6 baselines. (1) zero-shot CLIP (Rad-\nford et al. 2021); (2) linear probe (Radford et al. 2021);\n(3) prompt tuning, i.e., CoOp (Zhou et al. 2021); (4) an ad-\nvance CLIP fine-tuning: CLIP-adapter (Gao et al. 2021); (5)\nconventional fine-tuning with randomly initialized classifi-\ncation head, which is denoted as FT; (6) conventional fine-\ntuning with classification head initialized from a text en-\ncoder thought prompting, which is denoted as FT++;\nFor ViLT-based models, we compared ViLT-ProReg with\nthree baseline methods. (1) zero-shot ViLT. (2) conventional\nfine-tuning with randomly initialized classification head,\nwhich is denoted as FT. (3) conventional fine-tuning with\nclassification head initialized from a text encoder thought\n3838\nMethod Acc.\nZero-shot 86.7\nLinear\nProbe 89.5\nCoOp 90.2\nCLIP-Adapter 90.4\nFT 90.5\nFT++ 90.9\nProReg 92.1\nTable\n2: Accuracy (%) on\nBAR.\nMethod ID\nOOD HM\nZero-shot 58.2\n63.7 60.8\nLinear Probe 70.4 52.9 60.4\nCoOp 70.9 56.6 62.9\nCLIP-Adapter 71.2 51.8 60.0\nFT 75.9 55.8 64.3\nFT++ 75.8 60.6 67.4\nProReg 75.6 64.4 69.2\nTable\n3: Accuracy (%) on DomainNet\ndataset.\nMethod Animal Subset\nVehicle Subset\nID OOD\nHM ID OOD HM\nZero-shot 93.3\n92.5 92.9 95.8 95.9 95.8\nLinear Probe 99.0 87.0 92.6 99.4 87.5 93.1\nCoOp 99.3 86.1 92.2 99.4 87.6 93.1\nCLIP-Adapter 99.0 81.9 89.6 99.4 83.7 90.9\nFT 98.7 85.3 91.5 99.6 88.9 93.9\nFT++ 98.9 91.7 95.2 99.4 90.8 94.9\nProReg 98.5 94.4 96.4 98.7 94.1 96.3\nTable 4: Accuracy (%) on NICO dataset.\nprompting, which is denoted as FT++. Please see Appendix\nfor more details.\nEvaluation Metrics. To evaluate the unbiasedness, we also\nreport the in-domain (ID) accuracy, out-of-domain (OOD)\naccuracy and their harmonic mean for NICO and Domain-\nNet datasets. Specifically, ID test set has the same distri-\nbution with the training set while the distribution of OOD\ntest set is different from the training one. A debiased model\nshould have high performance on both ID and OOD testing,\nas a result, it should also have the highest harmonic mean.\nMain Results\nImage Classification. Results are shown in Table 2, Table 4,\nTable 5 and Table 3. Due to the powerful pretraining knowl-\nedge and hand-crafted prompt, the zero-shot CLIP model\nachieves strong performance. In particular, on NICO Vehi-\ncle subset (Table 4), the zero-shot CLIP exhibits harmonic\nmean of 95.8%. After training on the downstream data, we\nobserved that ProReg demonstrates clear advantages over\nother fine-tuning methods on the OOD test sets, e.g., on\nBAR test set, the ProReg outperforms other counterparts by\nat least 1.2%. Not surprisingly, on NICO and DomainNet\ndatasets, we observed that other fine-tuning methods gain\nsignificant improvements on in-distribution accuracies but at\na cost of performance loss on out-of-distribution accuracies\ncompared to zero shot performance, e.g., recently proposed\nCoOp (Zhou et al. 2021) increases the ID accuracies from\n93.3% to 99.3% on NICO Animal subset (from 58.2% to\n70.9% on DomainNet) but the OOD accuracies decreases\nfrom 92.5% to 86.1% (from 63.7% to 56.6% on Domain-\nNet). As a comparison, our ProReg obtains a good trade-off\nbetween on ID and OOD performance, thus achieving the\nbest performance on harmonic means.\nTable 5 reported the results on PACS. The zero-shot CLIP\nMethod OOD Domain A\nvgA C\nP S\nZero-shot 89.8\n96.3 99.4 85.8 92.8\nLinear Probe 84.3 91.7 83.1 80.3 84.9\nCoOp 92.6 96.1 98.1 85.0 93.0\nCLIP-Adapter 93.6 96.6 97.7 84.4 93.1\nFT 95.3 95.6 99.7 86.8 94.4\nFT++ 95.6 97.0 99.3 86.8 94.7\nProReg 96.2 98.6 99.8 89.4 96.0\nTable 5: Accuracy (%) on PACS.\nMethods All Y/N Num.\nOther\nViL\nT\nQ2S Zero-shot\n43.62 77.70 12.02 33.93\nFT 45.68 44.16 14.44 55.12\nFT++ 46.34 73.00 16.29 40.23\nProReg 54.89 73.06 14.85 55.58\nTable 6: Evaluations (Accurarcy%) on VQA-CP.\nshows strong performance on Photo (P) and Cartoon (C) Do-\nmains, e.g., zero-shot CLIP achieves 99.4% on photo do-\nmain. Besides the best performance in average accuracy, our\nProReg significantly outperforms other fine-tuning methods\non difficult unseen domains Sketch (S) and Cartoon (C),e.g.,\nProReg gains 1.6% and 2.6% on sketch and cartoon domain\ncompared to FT++ method. These results show the power of\nour ProReg to overcome diverse domain biases.\nVisual Question Answering.Considering that the input text\nfor VQA tasks varies sample-wisely, it is difficult for CLIP\nmodel to infer and optimize. We only implement the ProReg\nbased on ViLT models and compared with zero-shot prompt,\nFT and FT++.\nAlthough zero-shot prompt models have no access to the\ntraining data, thanks to the pretraining knowledge and our\nproposed question-to-statement (Q2S) prompt design, our\nzero-shot prompt achieves impressive results with 43.62%\naccuracy, which is a strong baseline. The results show that\nour ProReg framework shows strong performance under\nlanguage bias, e.g., ProReg achieves an overall accuracy\nof 54.89% with an impressive improvement +8.55% com-\npared to conventional fine-tuning with classification initial-\nized from prompt (FT++). More interestingly, conventional\npromptless fine-tune (FT) failed on “Yes/No” questions,\nwhile FT++ failed on “Other”. As a comparison, ProReg\nperforms relatively well on all the three questions types.\nAblation Studies\nWe further conduct ablation studies to answer the following\nquestions.\nQ1: What is the effect of the hyper-parameter α? We con-\nducted the experiments on DomainNet dataset by varying α\nin Eq. (9), the results are shown in Figure 4. As expected,\nas α increases, the fine-tuned model is encouraged to learn\nmore from pre-trained knowledge. As a result, the OOD per-\nformance will increase while the ID accuracy drops.\n3839\nFigure 4: Effect of hyper-parameter α on DomainNet.\n(a) (b)\nFigure 5: (a) KD results (ID, OOD and harmonic %) on\nNICO animal subset.(b) KD results (overall accuracy %) on\nVQA-CP with different weight λ.\nQ2: Can we blend the knowledge using conventional dis-\ntillation that use constant trade-off weight? No, its perfor-\nmance is worse than ProReg fine-tune. We implemented\ntraditional knowledge distillation strategy by varying the\ntrade-off weight λ as described in Eq. (4). Figure 5 shows\nthe results on NICO Vehicle and VQA-CP dataset. We\nobserved that our ProReg model achieves superior per-\nformance than traditional knowledge distillation, e.g., for\nVQA-CP datasets, even with the optimal λ, traditional KD\n(blue line) achieves the best performance of 53.15% with\n−1.74% gap to ProReg fine-tune (red line).\nQ3: Can we directly ensemble the zero-shot model and tra-\nditional fine-tune model? No, it not only achieves worse\nperformance than ProReg, but also doubles the inference\ntime and the number of parameters. In Figure 6, we in-\nvestigate whether combining the knowledge by ensembling\nthe conventional fine-tuning and the zero-shot CLIP model\ncan perform superior results. Specifically, given the pre-\ndiction of fine-tuning model yft and the one from zero-\nshot model yzs, the ensembled prediction is formulated as\nyens = (1 − λ) · yft + λ · yzs, where λ ∈ [0, 1]. Empir-\nical results on VQA-CP dataset and NICO vehicle dataset\nare plotted in Figure 6(a), where the highest harmonic mean\nwiht λ = 0.5 (blue line) still underperforms our ProReg (red\nline). For VQA-CP dataset, ensemble model reaches its op-\ntimal accuracy 53.48% with λ = 0.5 (blue line). The highest\naccuracy of ensemble is still surpassed by our ProReg result\n(54.89%, red line). Moreover, the ensemble model doubles\nthe inference time and the number of parameters.\nQualitative results. We visualized a sports related question,\n(a) (b)\nFigure 6: (a) Ensemble results (harmonic mean%) on NICO\nvehicle subset. (b) Ensemble results (overall accuracy%) on\nVQA-CP with different weight λ.\n!\"#$%&'()\n*+,$-./01)$2&3('.4$5&64$*&,$7\"$2&3('.4$5&64\n!\"#$5(30+\n89:;\n89<\n89=;\n89>\n%&'() 5(30+\n89>\n-./01) 73?4@)'?4\n-./A4B\n'\n 4\n89:\n89C\n89<\n89=\n%&'() 5(30+\n89=\n-./01) 73?4@)'?4\n-./A4B\n/A4\n./\n 4\nFigure 7: Qualitative examples of zero-shot prompt, conven-\ntional fine-tune and ProReg fine-tune on BAR dataset.\nwhich is too domain-specific, the general knowledge learned\nby zero-shot prompt model is hard to answer such ques-\ntion. In the meanwhile, our ProReg inherits the knowledge\nfrom both domain and gives the right answer. Figure 7(a)\nand (b) show some failure cases of conventional fine-tune\nmodel and zero-shot ViLT model for BAR dataset. In Figure\n7 (a), a “climb” image is mis-classified as “vault” by conven-\ntional fine-tune model, which can be attributed to the dataset\nbias, where the context of the “climb” training images are\nmost rocks. In Figure 7 (b), a “vault” image is recognized\nas “dive” by the zero-shot ViLT model, we conjecture that\nthe reason is that the knowledge of pole vaulting is not com-\nmon in the pretrained domain. In both cases, ProReg predicts\nright answers by inheriting knowledge from the downstream\ndata and the pretraining knowledge.\nConclusion\nWe presented an effective novel fine-tuning strategy: Prompt\nRegularization (ProReg), for deploying VLMs in down-\nstream tasks. As its name implies, ProReg is motivated to\nmake the best of the two worlds: pretrained general knowl-\nedge and task-specific knowledge, where the former is ac-\nquired by using prompt. The highlight of ProReg is the\nproposed sample-wise adaptive weight that trades off the\ntraining losses from the two worlds. Note that this weight\nis theoretically justified. Therefore, we believe that ProReg\nhas a great potential for helping practitioners to fine-tune\ntheir own models efficiently. By extensive evaluations on\nthree types of challenging OOD benchmarks, ProReg signif-\nicantly outperformed zero-shot prompt, prompt tuning, con-\nventional fine-tuning and other state-of-the-art methods.\n3840\nAcknowledgments\nThe authors would like to thank the reviewers for their\ncomments that help improve the manuscript. This research\nis supported by the National Research Foundation, Singa-\npore under its AI Singapore Programme (AISG Award No:\nAISG2-PhD-2021-01-002 and AISG2-RP-2021-022).\nReferences\nAgrawal, A.; Batra, D.; Parikh, D.; and Kembhavi, A. 2018.\nDon’t just assume; look and answer: Overcoming priors for\nvisual question answering. In CVPR.\nAnderson, P.; He, X.; Buehler, C.; Teney, D.; Johnson, M.;\nGould, S.; and Zhang, L. 2018. Bottom-up and top-down at-\ntention for image captioning and visual question answering.\nIn CVPR.\nBen-David, S.; Blitzer, J.; Crammer, K.; Pereira, F.; et al.\n2007. Analysis of representations for domain adaptation.\nNeurIPS, 19: 137.\nBommasani, R.; Hudson, D. A.; Adeli, E.; Altman, R.;\nArora, S.; von Arx, S.; Bernstein, M. S.; Bohg, J.; Bosselut,\nA.; Brunskill, E.; et al. 2021. On the opportunities and risks\nof foundation models. arXiv preprint arXiv:2108.07258.\nBrock, A.; Donahue, J.; and Simonyan, K. 2018. Large Scale\nGAN Training for High Fidelity Natural Image Synthesis. In\nICLR.\nCadene, R.; Dancette, C.; Cord, M.; Parikh, D.; et al. 2019.\nRubi: Reducing unimodal biases for visual question answer-\ning. NeurIPS.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-\nFei, L. 2009. ImageNet: A large-scale hierarchical image\ndatabase. In CVPR, 248–255.\nGao, P.; Geng, S.; Zhang, R.; Ma, T.; Fang, R.; Zhang, Y .;\nLi, H.; and Qiao, Y . 2021. CLIP-Adapter: Better Vision-\nLanguage Models with Feature Adapters. arXiv preprint\narXiv:2110.04544.\nGoyal, Y .; Khot, T.; Summers-Stay, D.; Batra, D.; and\nParikh, D. 2017. Making the v in vqa matter: Elevating the\nrole of image understanding in visual question answering.\nIn CVPR.\nHan, X.; Zhao, W.; Ding, N.; Liu, Z.; and Sun, M. 2021.\nPTR: Prompt Tuning with Rules for Text Classification.\narXiv preprint arXiv:2105.11259.\nHe, K.; Gkioxari, G.; Doll´ar, P.; and Girshick, R. 2017. Mask\nr-cnn. In ICCV, 2961–2969.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep residual\nlearning for image recognition. In CVPR.\nHe, Y .; Shen, Z.; and Cui, P. 2020. Towards Non-IID Image\nClassification: A Dataset and Baselines. PR, 107383.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. stat, 1050: 9.\nHuang, Z.; Zeng, Z.; Liu, B.; Fu, D.; and Fu, J. 2020. Pixel-\nbert: Aligning image pixels with text by deep multi-modal\ntransformers. arXiv preprint arXiv:2004.00849.\nKim, W.; Son, B.; and Kim, I. 2021. ViLT: Vision-and-\nLanguage Transformer Without Convolution or Region Su-\npervision. In ICML.\nLi, D.; Yang, Y .; Song, Y .-Z.; and Hospedales, T. M. 2017.\nDeeper, broader and artier domain generalization. In ICCV.\nLi, L. H.; Yatskar, M.; Yin, D.; Hsieh, C.-J.; and Chang, K.-\nW. 2019. Visualbert: A simple and performant baseline for\nvision and language. arXiv preprint arXiv:1908.03557.\nLiu, P.; Yuan, W.; Fu, J.; Jiang, Z.; Hayashi, H.; and Neubig,\nG. 2021a. Pre-train, prompt, and predict: A systematic sur-\nvey of prompting methods in natural language processing.\narXiv preprint arXiv:2107.13586.\nLiu, X.; Zheng, Y .; Du, Z.; Ding, M.; Qian, Y .; Yang, Z.;\nand Tang, J. 2021b. GPT Understands, Too. arXiv preprint\narXiv:2103.10385.\nLoshchilov, I.; and Hutter, F. 2018. Decoupled Weight De-\ncay Regularization. In ICLR.\nLu, J.; Batra, D.; Parikh, D.; and Lee, S. 2019. ViLBERT:\npretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. In NeurIPS.\nLu, Y .; Liu, J.; Zhang, Y .; Liu, Y .; and Tian, X. 2022. Prompt\nDistribution Learning. In CVPR.\nMenon, A. K.; Jayasumana, S.; Rawat, A. S.; Jain, H.; Veit,\nA.; and Kumar, S. 2020. Long-tail learning via logit adjust-\nment. In ICLR.\nNam, J.; Cha, H.; Ahn, S.-S.; Lee, J.; and Shin, J. 2020.\nLearning from Failure: De-biasing Classifier from Biased\nClassifier. In NeurIPS.\nNiu, Y .; Tang, K.; Zhang, H.; Lu, Z.; Hua, X.-S.; and Wen,\nJ.-R. 2021. Counterfactual vqa: A cause-effect look at lan-\nguage bias. In CVPR.\nPeng, X.; Bai, Q.; Xia, X.; Huang, Z.; Saenko, K.; and Wang,\nB. 2019. Moment matching for multi-source domain adap-\ntation. In ICCV.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\net al. 2021. Learning transferable visual models from natural\nlanguage supervision. In ICML.\nRamakrishnan, S.; Agrawal, A.; and Lee, S. 2018. Over-\ncoming Language Priors in Visual Question Answering with\nAdversarial Regularization. In NeurIPS.\nSutskever, I.; Martens, J.; Dahl, G.; and Hinton, G. 2013.\nOn the importance of initialization and momentum in deep\nlearning. In ICML.\nTan, H.; and Bansal, M. 2019. LXMERT: Learning Cross-\nModality Encoder Representations from Transformers. In\nEMNLP-IJCNLP, 5100–5111.\nTang, K.; Huang, J.; and Zhang, H. 2020. Long-Tailed Clas-\nsification by Keeping the Good and Removing the Bad Mo-\nmentum Causal Effect. NeurIPS.\nTsimpoukelli, M.; Menick, J.; Cabi, S.; Eslami, S. A.;\nVinyals, O.; and Hill, F. 2021. Multimodal Few-Shot Learn-\ning with Frozen Language Models. In NeurIPS.\nTzeng, E.; Hoffman, J.; Saenko, K.; and Darrell, T. 2017.\nAdversarial discriminative domain adaptation. In CVPR.\nViazovetskyi, Y .; Ivashkin, V .; and Kashin, E. 2020. Style-\ngan2 distillation for feed-forward image manipulation. In\nECCV. Springer.\n3841\nWang, Z.; Dai, Z.; P´oczos, B.; and Carbonell, J. 2019. Char-\nacterizing and avoiding negative transfer. In CVPR.\nWortsman, M.; Ilharco, G.; Kim, J. W.; Li, M.; Kornblith,\nS.; Roelofs, R.; Lopes, R. G.; Hajishirzi, H.; Farhadi, A.;\nNamkoong, H.; et al. 2022. Robust fine-tuning of zero-shot\nmodels. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 7959–7971.\nWu, J.; and Mooney, R. 2019. Self-Critical Reasoning for\nRobust Visual Question Answering. NeurIPS.\nYang, X.; Zhang, H.; Qi, G.; and Cai, J. 2021. Causal atten-\ntion for vision-language tasks. In CVPR.\nYao, Y .; Zhang, A.; Zhang, Z.; Liu, Z.; Chua, T.-S.; and Sun,\nM. 2021. Cpt: Colorful prompt tuning for pre-trained vision-\nlanguage models. arXiv preprint arXiv:2109.11797.\nYue, Z.; Zhang, H.; Sun, Q.; and Hua, X.-S. 2020. Interven-\ntional Few-Shot Learning. NeurIPS.\nZhang, P.; Li, X.; Hu, X.; Yang, J.; Zhang, L.; Wang, L.;\nChoi, Y .; and Gao, J. 2021. Vinvl: Revisiting visual repre-\nsentations in vision-language models. In CVPR.\nZhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2021. Learn-\ning to prompt for vision-language models. arXiv preprint\narXiv:2109.01134.\nZhou, K.; Yang, J.; Loy, C. C.; and Liu, Z. 2022. Conditional\nPrompt Learning for Vision-Language Models. In CVPR.\nZhu, B.; Niu, Y .; Han, Y .; Wu, Y .; and Zhang, H. 2022.\nPrompt-aligned gradient for prompt tuning. arXiv preprint\narXiv:2205.14865.\n3842",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7606374621391296
    },
    {
      "name": "Regularization (linguistics)",
      "score": 0.7371612787246704
    },
    {
      "name": "Fine-tuning",
      "score": 0.6095266342163086
    },
    {
      "name": "Language model",
      "score": 0.5517560243606567
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5350666046142578
    },
    {
      "name": "Task (project management)",
      "score": 0.5300001502037048
    },
    {
      "name": "Cross entropy",
      "score": 0.5127895474433899
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.4984934329986572
    },
    {
      "name": "Kullback–Leibler divergence",
      "score": 0.44966283440589905
    },
    {
      "name": "Ground truth",
      "score": 0.4262336492538452
    },
    {
      "name": "One shot",
      "score": 0.4249950051307678
    },
    {
      "name": "Machine learning",
      "score": 0.3919413089752197
    },
    {
      "name": "Principle of maximum entropy",
      "score": 0.3659055233001709
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I172675005",
      "name": "Nanyang Technological University",
      "country": "SG"
    },
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I197312522",
      "name": "Hyundai Motor Group (South Korea)",
      "country": "KR"
    }
  ]
}