{
  "title": "Detecting Anomalies in System Logs With a Compact Convolutional Transformer",
  "url": "https://openalex.org/W4387609326",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5019843025",
      "name": "René Larisch",
      "affiliations": [
        "Chemnitz University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5016814228",
      "name": "Julien Vitay",
      "affiliations": [
        "Chemnitz University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5089469300",
      "name": "Fred H. Hamker",
      "affiliations": [
        "Chemnitz University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6795062860",
    "https://openalex.org/W2107263349",
    "https://openalex.org/W2476891002",
    "https://openalex.org/W6756358366",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3214635173",
    "https://openalex.org/W4205965165",
    "https://openalex.org/W2934012051",
    "https://openalex.org/W6637373629",
    "https://openalex.org/W3183619936",
    "https://openalex.org/W6620707391",
    "https://openalex.org/W6687483927",
    "https://openalex.org/W1966716734",
    "https://openalex.org/W4309703472",
    "https://openalex.org/W3116286104",
    "https://openalex.org/W3010945647",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W4280493558",
    "https://openalex.org/W2972810968",
    "https://openalex.org/W4256669726",
    "https://openalex.org/W2945044133",
    "https://openalex.org/W2736848882",
    "https://openalex.org/W3199174176",
    "https://openalex.org/W3127712067",
    "https://openalex.org/W6803927403",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3095840026",
    "https://openalex.org/W3092557781",
    "https://openalex.org/W6631943919",
    "https://openalex.org/W6675354045",
    "https://openalex.org/W2886020981",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W3023371261",
    "https://openalex.org/W2106681456",
    "https://openalex.org/W1976526581",
    "https://openalex.org/W6840157977",
    "https://openalex.org/W3134079112",
    "https://openalex.org/W2583874385",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W2555780822",
    "https://openalex.org/W6680785567",
    "https://openalex.org/W2795516651",
    "https://openalex.org/W2606697812",
    "https://openalex.org/W2808079449",
    "https://openalex.org/W2387314750",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W639708223",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2143220335",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3214665350",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W4322588869",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W4285596585",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W3083891030"
  ],
  "abstract": "Computer systems play an important role to ensure the correct functioning of critical systems such as train stations, power stations, emergency systems, and server infrastructures. To ensure the correct functioning and safety of these computer systems, the detection of abnormal system behavior is crucial. For that purpose, monitoring log data (mirroring the recent and current system status) is very commonly used. Because log data consists mainly of words and numbers, recent work used Transformer-based networks to analyze the log data and predict anomalies. Despite their success in fields such as natural language processing and computer vision, the main disadvantage of Transformers is the huge amount of trainable parameters, leading to long training times. In this work, we use a Compact Convolutional Transformer to detect anomalies in log data. Using convolutional layers leads to a much smaller number of trainable parameters and enable the processing of many consecutive log lines. We evaluate the proposed network on two standard datasets for log data anomaly detection, Blue Gene/L (BGL) and Spirit. Our results demonstrate that the combination of convolutional processing and self-attention improves the performance for anomaly detection in comparison to other self-supervised Transformer-based approaches, and is even on par with supervised approaches.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identiﬁer 10.1109/ACCESS.2017.DOI\nDetecting anomalies in system logs with\na compact convolutional transformer\nRENÉ LARISCH1, JULIEN VITAY1, AND FRED H. HAMKER,1\n1Department of Computer Science, Artiﬁcial Intelligence, University of Technology, Chemnitz, 09111 Chemnitz, Germany\nCorresponding author: Fred H. Hamker (e-mail: fred.hamker@informatik.tu-chemnitz.de).\nThis work has been funded by the Federal Ministry of Education and Research (BMBF 16KIS1199).\nABSTRACT Computer systems play an important role to ensure the correct functioning of critical systems\nsuch as train stations, power stations, emergency systems, and server infrastructures. To ensure the correct\nfunctioning and safety of these computer systems, the detection of abnormal system behavior is crucial. For\nthat purpose, monitoring log data (mirroring the recent and current system status) is very commonly used.\nBecause log data consists mainly of words and numbers, recent work used Transformer-based networks\nto analyze the log data and predict anomalies. Despite their success in ﬁelds such as natural language\nprocessing and computer vision, the main disadvantage of Transformers is the huge amount of trainable\nparameters, leading to long training times. In this work, we use a Compact Convolutional Transformer\nto detect anomalies in log data. Using convolutional layers leads to a much smaller number of trainable\nparameters and enable the processing of many consecutive log lines. We evaluate the proposed network\non two standard datasets for log data anomaly detection, Blue Gene/L (BGL) and Spirit. Our results\ndemonstrate that the combination of convolutional processing and self-attention improves the performance\nfor anomaly detection in comparison to other self-supervised Transformer-based approaches, and is even on\npar with supervised approaches.\nINDEX TERMS anomaly detection, deep learning, self-supervised learning, transformer\nI. INTRODUCTION1\nComputer systems, such as cyber-physical systems (CPS),2\nindustry control systems (ICS), server systems, IoT services3\nor supercomputers create every day a huge amount of data4\nabout the current status of the system, processes, network5\ncommunication, or critical events, recorded in log data. Along6\nwith the direct monitoring of network trafﬁc or process data,7\nthe automated evaluation of the log data to ﬁnd anomalies8\nor faulty processing in the systems became an important9\nchallenge in the last years in order to ensure an error-less10\noperation of the system. Log data are mainly created by11\nfollowing a speciﬁc syntax to create templates of words and ﬁll12\nthem with numbers, reﬂecting the status of the corresponding13\nsystem [1, 2]. Thus, log data consists of a number of log lines14\nor log sequences, where each log line consists of multiple (or a15\nsequence) of items (words or numbers). The task of analyzing16\nthe log data can thus be compared to text understanding,17\na ﬁeld of natural language processing where Transformer18\nnetworks, a recent architecture for artiﬁcial neural networks19\n[3], allowed for impressive applications in the last years [4, 5].20\nThis success lead to a certain popularity of Transformer-based21\nneural networks for log data analysis [6, 7, 8, 9, 10, 11].22\nDespite their success, Transformer networks require large23\namounts of data for training [ 12]. This is mainly due to the24\nhuge number of trainable parameters (weights and biases) in25\nthe encoder blocks, which contain the self-attention heads:26\naround 110 million parameters for the BERT model [ 4]27\nand around 175 billion parameters for GPT-3 [ 13]. As a28\nconsequence, training a Transformer model like BERT can29\ntake several days 1. To avoid this issue, some researchers used30\na pre-trained Transformer to create a latent representation of31\nthe log data and feed this into an additional classiﬁcation or32\nprediction layer [10]. Another problem with the Transformer33\napproach is the limitation of processing only one line of the log34\ndata in one learning step of the network [7, 8], which can lead35\nto a loss of information about possible correlations between36\nthe different lines of the log dataset. Previous research37\naddressed this problem by concatenating successive log items38\ninto one sequence of speciﬁc length, allowing to capture the39\ncontextual information about different log entries [6, 8, 10].40\nHowever, the amount of information preserved across multiple41\nconsecutive log entries can vary, due to the different lengths42\n1https://huggingface.co/blog/bert-101\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nof single log entries and the resulting variation in the number43\nof items from multiple log entries in one sequence.44\nMost log data are built in a semi-structured way, consisting of45\nelements on ﬁxed positions (like timestamps or process ids)46\nand elements with varying size and position in a log entry47\n(like free-text written by a developer or automatically created48\nstrings via templates or syntax rules) [1, 2]. Log data can then49\nvary from a structured [14] to a more unstructured format [15].50\nRegardless of whether the log data consists of more structured51\nor unstructured content, a standard preprocessing step for52\nanalyzing is to perform a pre-parsing step to identify templates53\nalong the data and their corresponding parameters [6, 7, 8, 9].54\nIt has been argued that splitting the dataset into templates and55\nparameters can destroy the context between consecutive log56\nsequences and that falsely created templates and parameter57\nvalues lead to out-of-vocabulary word errors [10]. These are58\nwords that appear only rarely or not at all in the training set59\nand can lead to a higher false positive rate, by ignoring the60\nbigger context in which an anomaly can happen [10]. Further,61\nthe resulting templates and parameters depend mainly on the62\nused pre-parsing approach [2], leading to different template63\nparameter pairs and making it necessary to identify the best64\nﬁtting pre-parsing approach for the respective dataset.65\nTo overcome these limitations, we designed an anomaly de-66\ntection network using a Compact Convolutional Transformer67\n[12] (CCT). The usage of convolutional sub-networks for68\nposition embedding together with sequence pooling in the69\nCCT leads to a compact architecture. While there exists a70\nbroad corpus of models and methods in anomaly detection71\nin log data (see for a review He et al. [16]), we focus here72\non Transformer-based solutions, as they have been shown73\nto outperform other methods. Additionally, most of the74\nrecent Transformer-based approaches utilize a self-supervised75\nlearning scheme, not requiring labels during the training76\nprocess. As our approach also uses a self-supervised learning77\nscheme, supervised methods, which are not Transformer-78\nbased, are not considered.79\nThe compactness of the CCT enables the training of the com-80\nplete network on two logﬁle datasets from high-performance81\ncomputing systems: Blue Gene/L (BGL) and Spirit [ 15].82\nDespite the fact that 2D-convolutional neural networks are83\nvery common in object recognition [ 17, 18] and object84\ndetection [ 19], it has been shown that they are useful for85\nother domains, like anomaly detection in sensor data [20] or86\nin analyzing network trafﬁc [21]. Based on these results, we87\nassume that the two-dimensional kernel will be able to detect88\ncorrelations between multiple rows and columns and encode89\nlocal information. The self-attention mechanism [ 3] of the90\nTransformer encoder block in the model [12] can preserve the91\ncontextual information between the latent representation from92\nthe convolutional sub-network and thereby encode global93\ninformation.94\nThe log data analysis is performed on the unstructured data95\nwithout any pre-parsing to identify templates and parameters,96\nas it has already been shown that pre-parsing is not necessary97\nto achieve comparable performances [7, 9, 10].98\nIn this study, we evaluate the capability of the CCT to99\nencode the context within the datasets by a systematical100\nvariation of the 2D-convolutional kernel sizes, the decision101\nthreshold, and other hyperparameters. Our results show that102\na bigger kernel (allowing the parallel processing of multiple103\nlog sequences) can increase the performance of the network,104\nindicating the importance of contextual information for105\nanomaly detection. We demonstrate in this paper how the106\ncombination of convolutional processing in the early stages of107\nthe CCT and the attention-based processing in the Transformer108\nblocks improves anomaly detection in comparison to previous109\npublished Transformer-based approaches.110\n111\nThe contributions of this paper are as follows:112\n1) By transferring the Compact Convolutional Transformer113\n(CCT) [12] from object recognition to a task closer to114\nnatural language processing, namely detecting anoma-115\nlies in supercomputer log data, we demonstrate how116\nthe extension of a Transformer-encoder architecture by117\na 2D-convolutional neural network improves anomaly118\ndetection in unstructured log data.119\n2) We demonstrate how bigger convolutional kernel sizes120\nincrease the performance of the network, by incorporat-121\ning information from multiple log lines and encapsulat-122\ning contextual information between the convolutional123\nencoding through self-attention.124\n3) The combination of 2D convolutional neural networks125\nwith Transformer-encoder blocks leads to a network126\nwith an overall lower number of trainable parame-127\nters than previously published Transformer-based ap-128\nproaches, showing comparable or better performances.129\nII. METHODS130\nThe network was implemented in Python 3.7 with the Tensor-131\nﬂow (v2.6) [22] and Keras (v2.6) [23] libraries. The Compact132\nConvolutional Transformer is based on the implementation by133\nSayak Paul in the ofﬁcal Keras documentation 2 and modiﬁed134\nfor the needs of this work. The source code of the proposed135\nCCT architecture is available online 3.136\nA. DATASET137\nTo evaluate our approach, we use the open Blue Gene/L138\n(BGL) dataset and Spirit datasets. The ﬁrst one contains139\nalert and non-alert messages recorded from the Blue Gene/L140\nsupercomputer at the Lawrence Livermore National Labs141\n(LLNL) in Livermore, California[15, 24]. The log entries are142\nmanaged by a Machine Management Control System (MMCS)143\nand provides error messages and warnings from hardware144\nas well from software for individual chips and computer145\nnodes, errors about inter-processor communication over the146\nnetwork, and temperature emergencies (for example through147\na dysfunctional fan) [25, 15]. Only warnings and errors which148\ncorrespond to a faulty behavior of the system (like a software149\n2https://keras.io/examples/vision/cct/\n3https://github.com/hamkerlab/Larisch2023_Detecting_Anomalies\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\ncrash) are marked as an alert in the dataset and they are the150\nanomalies that we aim to detect in our study. Messages in the151\ndataset are collected with a period of around one millisecond152\n[25, 15]. In this study, we use the version from the Loghub153\ncollection [24], consisting of 4,747,963 messages in total and154\nwith 348,460 messages marked as anomalies.155\nThe Spirit dataset contains log messages from the super-156\ncomputer installed at the Sandia National Labs (SNL) in157\nAlbuquerque, New Mexico [15]. We use 1GB of log messages158\nfrom the Spirit dataset as provided by Le and Zhang [10],159\nconsisting of 7,983,345 log messages in total with 768,142160\nmessages marked as anomalies.161\nB. COMPACT CONVOLUTIONAL TRANSFORMER162\nIntroduced by Hassani et al. [12], the Compact Convolutional163\nTransformer (CCT) is a subsequent development of the Vision164\nTransformer (ViT) model proposed by Dosovitskiy et al. [26].165\nA sketch of the complete network is illustrated in Fig. 1. In166\nthe original ViT, the input image is split into non-overlapping167\npatches, similarly to the tokenization of vocabularies in natural168\nlanguage processing. A patch and position embedding is used169\nto retain information about the spatial order of the patches.170\nDue to this, only images that can be divided equally along171\nthe height and width can be used, if neither cropping nor172\npadding are possible. In the case of log sequence analysis,173\ncropping could lead to a loss of contextual information and174\npadding increases the number of irrelevant items in the175\ninput. The sequence of patches is passed to a Transformer176\nencoder, consisting of multi-head attention layers and multi-177\nlayer perceptron (MLP) blocks, extracting the input for the178\nclassiﬁcation head.179\nIn contrast to the original ViT, the CCT uses a convolu-180\ntional block instead of separated image patches, enabling the181\nprocessing of inputs with a non-quadratic spatial resolution182\n[12]. The convolution operation with a set of K kernels183\nW = {W1,...,W K}and biases {b1,...,b K}on an input184\ntensor X is deﬁned pixel-wise by Eq. 1:185\n(W ∗X)[m,n,k ] =\n∑\ni\n∑\nj\nWk[i,j] X[m−i,n −j] (1)\nThe convolutional layer applies a non-linear activation186\nfunction element-wise on this tensor, after adding a bias bk187\nfor each ﬁlter Wk (Eq. 2). We use the ReLU (rectiﬁed linear188\nunit) activation function for all convolutional layers.189\nY = ReLU(W ∗X+ b) = max(0,W ∗X+ b) (2)\nEach convolutional layer is followed by a max-pooling190\nlayer, performing a subsampling operation that only keeps the191\nhighest value in a 2x2 region. The output of a convolutional192\nblock (yl) is shown in Eq. 3.193\nZ = MaxPool(ReLU(Conv2D(X))) (3)\nThe usage of a convolutional subnetwork allows to change194\nthe number of log sequences and the length of each log195\nsequence independently from each other. Furthermore, the196\nconvolutional block can preserve local spatial information,197\nwhat could make the prepositional embedding obsolete. In fact,198\nHassani et al. [12] showed that the accuracy of the CCT was199\nonly weakly affected by removing the positional embedding,200\nwhat we can conﬁrm (see Fig. S1). The representations from201\nthe convolutional blocks are send into one or more Trans-202\nformer blocks. Each Transformer block starts by normalizing203\nthe representation of each sample, followed by the multi-head204\nself-attention with hself-attention heads to capture different205\ncontextual aspects. Following Vaswani et al.[3], the output of206\none attention head Ai is the scaled dot-product attention as207\nshown in Eq. 4.208\nAi = Attention(Qi,Ki,Vi) = Softmax(Qi ×KT\ni√dk\n) ×Vi\n(4)\nwhere Qi = X×WQ\ni , Ki = X×WK\ni , Vi = X×WV\ni209\nare called the query, key and value, respectively, withXbeing210\nthe input representation. WQ\ni , WK\ni and WV\ni ∈RdX×dk are211\nthe weight matrices of the i-thattention head, dk being the212\nembedding dimensionality and dX the dimensionality of the213\ninput. Each attention head Ai ∈[1,h] follows Eq. 4and has214\nits own WQ\ni , WK\ni and WV\ni matrices. The multi-head attention215\nis a mixing of the houtputs of the attention heads ( Eq. 5),216\nusing the mixing matrix WO ∈Rh dk×dk .217\nMultiHead(Q,K,V ) = Concat(A1,...,A h) ×WO (5)\nAfter the output of the multi-head attention block is218\nconcatenated with the input (skip connections) and layer-219\nnormalized, a multi-layer perceptron block, consisting of220\ndense layers with the ReLU activation and a dropout rate221\nof 0.1 (as suggested by Vaswani et al. [3]), calculates the222\noutput of the encoder block, also using skip connections.223\nFinally, a Sequence Pooling mechanism is applied on the224\noutput of the Transformer blocks [ 12], by pooling over the225\nsequence dimension before the latent representation is feed226\ninto a classiﬁcation head. The output of the last encoder-block227\n(xE) is passed to a linear layer (g()) and a softmax function228\nis applied on it (see Eq. 6).229\nxS = Softmax(g(xE)T ) (6)\nApplying the resulting xS on the encoder output ( xP =230\nxS ×xE) scales it depending on the importance of the task.231\nIn the original CCT model, the resulting xP is then sent to a232\nclassiﬁcation layer [12].233\nIn the BERT model, as well as in the original ViT, an234\nadditional [class] token is added to the input sequence.235\nThis additional token accumulates information about the236\nactual input sequence due to the self-attention [ 4] and the237\ncorresponding hidden state is used for classiﬁcation [ 4, 26].238\nBy pooling over the sequences, this additional token is not239\nnecessary with CCT [12].240\nThe version of CCT used in our work consists of two241\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nFigure 1: Schematic view on our network architectureThe Compact Convolutional Transformer (CCT) performs a\nconvolutional processing on the log window input and sends the corresponding representation into the Transformer encoder to\ncreate a latent representation. The convolutional kernels are represented by colored matrices. The corresponding representations\nare represented by the colored squares. Positional embedding does not inﬂuence the order of the representations. The CCT uses\nthe standard Transformer encoder architecture (image on the right). The latent representations from the Transformer encoder are\nused by the MLP head to create the prediction for the last log sequence in the input window. The softmax activation function in\nthe prediction layer gives a probability score for each item in the sequence and for each possible token in the vocabulary.\n2D-convolutional layers with 128 feature maps in the ﬁrst242\nconvolutional layer and 64 in the second, an embedding243\ndimensionality ( dk) of 64, 5 Transformer blocks, and 5244\nattention heads. For the purpose of this work, we send the245\noutput of the Sequence Pooling layer ( xP ) to an additional246\nMLP block with 1,800 neurons in the hidden layer and247\na softmax output layer. The softmax function predicts a248\nprobability score for each item in the log-sequence and for249\neach possible word in the vocabulary. The network predicts250\na n×vdimensional matrix, where nis the sequence length251\nand vthe vocabulary size of the tokenization. This leads for252\nexample to 2,571,803 trainable parameters for a convolutional253\nkernel size of 4 ×4. In the experiments, we vary the kernel254\nsize of both 2D-convolutional layer to investigate if a bigger255\nspatial resolution, and thus more contextual information, can256\nlead to a better performance.257\nC. PRE-PROCESSING258\nPrevious work advised not to perform the classical log parsing259\nof the dataset in order to avoid log parsing errors by out-of-260\nvocabulary (OOV) words and losing contextual information261\nbetween consecutive log messages [ 10, 9, 11]. Contrary to262\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nthis advice, the raw data in our work is pre-processed in263\nthe classical way for natural language processing: Special264\ncharacters and numbers are deleted, and all words are set to265\nlowercase [27, 28, 10]. It has to be mentioned that deleting266\nnumbers in log data could disturb the detection of anomalies,267\ndepending on the system that should be analyzed. However,268\ncontinuous numbers in particular can lead to a high number269\nof tokens, where each token has only a low frequency of270\nappearance, so they are removed.271\nAfterwards, single sequences are mapped to numbers (tokens).272\nIn this work, log data will refer to the complete log ﬁle, a log273\nsequence to a single line (a single message) inside of the data,274\na log item is a single token after the tokenization, and a log275\nwindow consists of a number of consecutive log sequences.276\nTo perform the mapping, a pre-trained BERT Tokenizer from277\nthe Hugging Face library 4 is used. It performs a WordPiece278\n[29] tokenization of the words [4], with 451 unique tokens in279\nthe BlueGene/L dataset and 920 unique tokens in the Spirit280\ndataset. Additionally, at the beginning and end of each log281\nsequence, a start token ( 101) and an end token ( 102) are282\nadded to signal the start and the end of each log sequence to283\nthe Transformer. By calculating the length of each sequence284\n(determined by the start and end tokens), we observed that285\nmost of the log sequences in both datasets are shorter than 30286\nitems (see Fig. S2aand Fig. S2b), so we use a ﬁxed sequence287\nsize of 30 tokens. If a sequence contains less than the 30288\nitems, the ﬁrst padded zero is replaced with the end token. If289\na sequence contains more than the 30 items, the last token290\nis replaced with the end token. After mapping the complete291\ndataset to tokens, we create log windows with a window size292\nof 15 consecutive log sequences for the BlueGene/L dataset293\nand of 20 consecutive log sequences for the Spirit dataset. The294\nbeginning of the next log-window is shifted by one, so each295\nlog sequence in the training set can be the last log sequence296\nin a window (except the ﬁrst 14 sequences in the very ﬁrst297\nlog window). In this way, two-dimensional input matrices298\nare created, which preserve the contextual information of299\none log sequence and also between multiple consecutive log300\nsequences.301\nD. SPLIT INTO TRAINING, VALIDATION AND TEST SET302\nDue to the self-supervised nature of our approach, we use two303\nadditional hyperparameters to decide whether a data sample304\nis an anomaly or not. The ﬁrst one is a decision threshold (th),305\napplied to the prediction probability of each single token to306\ndecide if it is a valid token or an anomalous token. The second307\none is the number of anomalous tokens (h) in a data sample308\nuntil it is detected as an anomaly. Together with the size309\nof the convolutional kernel, there are three hyperparameters310\nwhose inﬂuence the anomaly detection performance. To tune311\nthese hyperparameters, an additional validation and test set312\nare required to measure the performance values on unseen313\ndata [30, 31]. Due to the fact that neither the BGL dataset nor314\nthe Spirit dataset provides a predeﬁned training, validation, or315\n4https://huggingface.com/\ntest set, we have to split the dataset by ourselves.316\nAfter the complete dataset is pre-processed, it is shufﬂed317\nand we use 60% for the training set and the remaining 40%318\nto create the validation and test set. A similar split is used319\nin previous studies [ 10, 7]. To ensure learning only on log320\nsequences representing the normal behavior of the system, we321\ndelete from the training set every log window containing a322\nsingle log sequence originally marked as an anomaly. Due323\nto the pre-processing and the frequent presence of similar324\nentries in the log ﬁles, it can be assumed that the dataset is325\nhighly redundant and the subsets contain identical samples. To326\navoid data leakage caused by this [32] and to ensure that the327\nvalidation and test set only contains unseen samples, we sort328\nout every sample that also exists in the training set. After that,329\nwe split the remaining dataset further into a validation and a330\ntest set, with 20% used for the validation set and 80% used331\nfor the test set. We sort out data samples from the test set if332\nthey also exist in the validation to guarantee that only unseen333\ndata is used for the test set. A schematic overview about how334\nwe split the datasets is shown in Fig. 2.335\nDue to the fact that trainable parameters in deep neural336\nnetworks are initialized randomly, we create ten different337\ntraining, validation and test sets for the BGL and Spirit338\ndatasets.The median number of samples, together with the339\nstandard deviation, in the training set, validation set, and test340\nset are shown in Tab. 1.341\nE. EVALUATION AND TESTING342\nTo evaluate the inﬂuence of the different hyperparameters343\non the anomaly detection performance, we vary them sys-344\ntematically. To do so, we vary the kernel size from 2 ×2 to345\n6 ×6, testing ﬁve ﬁxed values for the decision threshold (th),346\ndecreasing decrementally from 1 ×10−3 to 1 ×10−5, and347\nthree values for the hparameter from one to three. We report348\nhere the performance values for the different hyperparameter349\nconﬁgurations obtained on the validation set.350\nTo allow for a fair comparison among different approaches,351\nwe report the performance values obtained on the unseen352\ntest set, which is not involved in training nor hyperparameter353\noptimization. As the decision threshold depends on the354\npredicted occurrence probability of the tokens, we assume355\nthat the performance of anomaly detection depends on the356\nexact adjustment of the threshold. While an incremental357\nchange of the threshold is only a rough estimation for the358\nbest threshold value, we determine the decision threshold359\nautomatically using the Precision-Recall curve [33, 34]. Due360\nto the fact that the network predicts for each token in the last361\nlog line an occurrence probability, we assign the minimum362\noccurrence probability as a sample threshold for the Precision-363\nRecall curve. After calculating the F1-Score for the different364\nthreshold values, we choose the obtained threshold with the365\nhighest F1-Score as the decision threshold for the evaluation366\nset. To calculate the precision-recall curve, we use the scikit-367\nlearn package [35]. While we change the determination of the368\nbest threshold value, the other two hyperparameters are tested369\nas before. The hyperparameters which lead to the highest370\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nFigure 2: Schematic view on the data split process.We split the dataset into a training, validation, and test set. To do so, after\nshufﬂing the dataset, 60% has been cleaned up from anomaly data and used for training. To create the validation, the remaining\n40% of the dataset has been cleaned up by identical samples from the training set and 20% are used for the validation set. To\ncreate the test set, the remaining samples have been cleaned up from identical samples from the validation set.\nTable 1: The number of log windows in the complete dataset, the training set, the validation set, and the test set. The table shows\nthe median value of the remaining log windows ±the standard deviation.\nDataset # log windows original training set validation set test set\nnormal anomaly normal anomaly\nBGL 4,747,943 2,609,888 ± 367 21,515 ± 133 27,926 ± 125 84,037 ± 157 6,624 ± 56\nSpirit 7,983,325 3,809,825 ± 12,055 184,628 ± 19,843 61,425 ± 165 725,975 ± 78,927 176,730 ± 16,985\nF1-Score on the validation set are used and the performance371\nvalues obtained on the test set will be reported.372\nF. TRAINING373\nIn the training phase, the network should learn the context374\nin which a token appears in a self-supervised manner. To375\nrealize that, we adapt the masked training task originally used376\nfor the BERT Transformer [4]. We mask 20% of the tokens377\nin the last sequence (except the start, end, and zero tokens)378\nin each log window with the masking token ( 103) (Fig. 3379\nshows a simpliﬁed illustration of the training) and the training380\nobjective is to predict the masked tokens. This procedure is381\nconsidered as a self-supervised training scheme as it uses382\nautomatically generated labels and does not rely on human383\nannotations [36]. It has been suggested by other Transformer-384\nbased approaches that masked learning is sufﬁcient to learn385\ncontextual information [8, 9, 11].386\nThe network is trained with the Adam optimizer with weight387\ndecay [37] and using the cross-entropy loss as the objective388\nfunction. To deal with the imbalance in the appearance389\nfrequency of sequence length (see Fig. S2), we weight the390\ntraining samples depending on the appearance frequency of391\nthe length of the last log sequence. The weight of one sample392\n(Ws) is described in Eq. 7, where N is the number of all393\nsamples in the training set, Ls is the number of samples with394\nthe same sequence length as the current sample, and Lis the395\nnumber of all different sequence lengths in the training set.396\nSample weighting is only used during training. A pseudocode397\ndescription of the training is shown in Algorithm 1. We398\ntrained the network for 100 epochs due to the saturated loss-399\nvalues (see Fig. S3and Fig. S4) .400\nWs = ln(Ls\nN )\nln( 1\nL) (7)\nThe initialization of the weights and biases in a deep neural401\nnetwork can inﬂuence the ﬁnal performance of the network,402\nas well as the speed of performance convergence [ 38]. To403\nenable a fast convergence, we initialized the weights in the404\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nAlgorithm 1Training algorithm\nInput: Training data Xtrain = x1,x2,x3,..xm with m\nsamples\n1: preprocess Xtrain\n2: remove windows containing anomaly log lines\n3: calculate sample weighting (WS)\n4: for e= 0 : Epochsdo\n5: Xtrain = shufﬂe(Xtrain)\n6: WS = shufﬂe(WS)\n7: for b= 0 : Batchesdo\n8: Xb ∈Xtrain\n9: Yb = ∅\n10: WSb ∈WS\n11: for i= 0 : Samples in batch do\n12: Yb.append(last logline of xi ∈Xb )\n13: mask 20% randomly chosen tokens in the last\nlogline of xi ∈Xb\n14: end for\n15: Pb = CCT(Xb)\n16: Lossb = CrossEntropy(Yb,Pb)\n17: optimize CCT based on Lossb and WSb\n18: end for\n19: end for\nconvolutional subnetwork with randomly chosen weights from405\na normal distribution based on the initialization proposed by406\nHe et al. [39] and all other parameters are chosen randomly407\nfrom a uniform distribution as proposed by Glorot and408\nBengio [40]. To ensure that the observed performance is not409\nonly caused by the chosen weights, we train each network410\nconﬁguration multiple times with different initialized weights411\nand biases. The network is trained directly on the log window412\nsamples as described above. Neither pre-training nor ﬁne-413\ntuning was used.414\nG. ANOMALY DETECTION415\nTo identify an anomaly, we assume that the network will416\nlearn the context in which a single token of the log sequence417\nis appearing. If a normal token appears in the right context, as418\nit is under normal system conditions, the network should be419\nable to predict its appearance with a high certainty. If a token420\nappears in the wrong context, as it is the case for an anomaly,421\nthe network should not be able to predict it. Therefore, we422\nuse the softmax output of the network to determine the423\nprobability that the appearance of a token is correct or not. We424\npresent the log windows of the respective dataset and receive425\na probability score for each token in the last sequence. To426\ncreate the predicted log sequence, we initialize the predicted427\nlog sequence with zeros and iterate through the probabilities428\nof each token in the last sequence. If the corresponding429\nprobability is below a speciﬁc threshold th, the token is430\nconsidered as an anomalous token and is set to -1. Otherwise,431\nthe original token is set in the prediction sequence. To decide if432\na complete log sequence is an anomaly, there must be hmany433\nanomalous tokens as minimum in the predicted sequence (Fig.434\nAlgorithm 2Anomaly detection algorithm\nInput: Input data Xdata = x1,x2,x3,..xm with msamples;\nth; h\n1: preprocess Xdata\nEvaluate data :\n2: L= length(Xdata)\n3: for l= 0 : Ldo\n4: xl ∈Xdata\n5: ¯xl = last logline of xl\n6: predictionl = CCT(xl)\n7: T = length(¯xl)\n8: for t= 0 : T do\n9: if predictiont\nl <th then\n10: ¯xt\nl = −1\n11: end if\n12: end for\n13: if sum(¯xl[== −1]) >h then\n14: return(Anomaly)\n15: else\n16: return(OK)\n17: end if\n18: end for\n4 depicts the evaluation process). A pseudocode description435\nof the evaluation is shown in Algorithm 2.436\na: Evaluation metrics437\nAfter each sequence is classiﬁed as an anomaly or not, we438\nevaluate the performance of the network by using the common439\nmetrics, Recall , Precision and F1-Score [41]. These metrics440\nare computed as followed:441\nPrecision = TP\nTP + FP (8)\nRecall = TP\nTP + FN (9)\nF1-Score = 2 ·Precision ·Recall\nPrecision + Recall (10)\nwhere TP refers to the number of true positive samples,442\nFP to the number of false positive samples, and FN to the443\nnumber of false negative samples. Due to the strong imbalance444\nbetween normal and abnormal samples, we report the values445\nfor both classes (normal and anomaly).446\nTo support our observations on the inﬂuence of different447\nconvolutional kernel sizes on the performance, we conduct448\nthe Friedman test [ 42] on the models with different kernel449\nsizes for the F1-Score, Recall, and Precision values. Due to450\nthe ongoing discussion about correct thresholds to determine451\nstatistical signiﬁcance [43, 44, 45], we did not deﬁne a speciﬁc452\nthreshold and only report the pure pand χ2 values.453\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nFigure 3: Training with masked predictions.The created log windows with a size sare tokenized and 20% of the tokens in\nthe last sequence in each window are swapped with the [MASK] token. The original tokens of the last sequence have to be\npredicted as a training task. Training happens only on normal log data.\nFigure 4: Detection of an anomaly.Sequence windows of the dataset are given to the network to predict the last sequence in the\nwindow. The soft-max output for each token in the last sequence is interpreted as a probability value. If the probability value\nfor a token is below the decision threshold (th), we assume that the token appears in the wrong context and set to −1. If the\nnumbers of −1 in one sequence are higher than the value h, the sequence is marked as an anomaly\nIII. RESULTS454\nWe present the performance of the CCT on the log anomaly455\ndetection task for two datasets: Blue Gene/L (BGL) and Spirit456\n[15]. We evaluate the ability to process the information of457\nneighboring tokens by the convolutional kernel, by varying458\nthe kernel size from a 2 ×2 kernel to a 6 ×6 kernel. Further,459\nwe investigate how the recognition performance is inﬂuenced460\nby the detection threshold ( th) and the minimum number461\nof anomalous tokens (h). We report ﬁnally the performance462\nvalues on the test set, obtained via hyperparameter tuning on463\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nthe validation set.464\nA. EVALUATION OF ANOMALY DETECTION465\n1) Varying decision threshold and kernel sizes466\nTo evaluate the inﬂuence of convolutional kernel size, decision467\nthreshold (th), and number of anomalous tokens ( h) on the468\nanomaly detection of the CCT, we varied them systematically469\nand evaluate the different combinations on the validation set.470\nOur results on ﬁve ﬁxed thresholds show that with a kernel471\nsize of 3 ×3, the network achieves already overall good472\nperformances on the BGL dataset, with a higher standard473\ndeviation of the Recall values for very low thresholds ( Fig.474\n5). A smaller kernel size of 2 ×2 leads to a decrease of the475\nPrecision, but also for the Recall values. This suggests that the476\nsmaller kernel size does not encapsulate enough contextual477\ninformation. Bigger kernel sizes (4 ×4, 5 ×5, and 6 ×6) also478\nlead to promising results but with higher standard deviations479\nand number of outliers. Especially at the lowest threshold480\nvalue (th = 10 −5), the standard deviation for the Recall481\nvalues increases, indicating an increase in the false negative482\nrate. While a lower threshold reduces the probability for483\nan anomaly to be detected, the number of falsely detected484\nanomalies decreases (leading to a high Precision score) but485\nalso increases the number of not detected anomalies (higher486\nnumber of false negatives).487\nHowever, on the Spirit dataset, the model with the4×4 kernel488\nshows the best performance in comparison to all other models,489\nregardless of the kernel size (Fig. 6). This suggests that, for the490\nbigger kernels, misleading context information is processed by491\nthe network. This demonstrates that the amount of contextual492\ninformation can vary from dataset to dataset. Regarding the493\nperformance on the BGL dataset, a lower threshold also leads494\nto a decrease of the Recall value. If the threshold is too low,495\nthe probability score of a token must be lower to be classiﬁed496\nas a false token, reducing the number of false positives and497\nincreasing the Precision value, but also increasing the number498\nof false negatives and decreasing the Recall value.499\nBecause a single token with a low probability is not always an500\nindicator for an abnormal behavior, we varied the minimum501\nnumber of tokens in a single log sequence which can have502\na low probability (the hvalue). By increasing this number,503\nthe false positive rate should decrease. Our results show a504\nsimilar behavior for all convolutional kernel sizes and for505\nboth datasets. With an higher h, the number of false positives506\ndecreases and the Precision increases. However, the number507\nof false negatives increases, leading to a decrease in the Recall508\nand F1-score ( Fig. 7). The Precision, Recall and F1-Score509\nvalues for all thand hpairs can be found in the supplementary510\nmaterial (see Fig. S5and Fig. S6).511\n2) Evaluation scores512\nOverall, the best evaluation scores are achieved with th =513\n0.505 ×10−3, h= 1 and all bigger kernel sizes for the BGL514\ndataset (see Tab. 2and Supplementary material for a deeper515\ncomparison of all values of thand h). As mentioned above,516\nthe model with a small 2 ×2 kernel leads to a low Precision517\nscore for the anomaly class ( 86.65), while a kernel size of518\n3 ×3 leads to an increase of the Precision score to99.26, with519\na low standard deviation and the overall highest Precision520\nscore. Due to one heavy outlier in this model, the standard521\ndeviation for this kernel size is quite high.522\nIn contrast, the median Recall score is over 99.75 for all four523\nmodels with a bigger kernel size, with the highest Recall value524\npresented by the 4 ×4 model and the 6 ×6 model (99.96).525\nThe overall highest F-1 score is achieved by the 6 ×6 model526\n(99.58). We see that the performance levels for the networks527\nwith the 3 ×3, 4 ×4, 5 ×5, and 6 ×6 kernels are not that528\ndifferent from each other, suggesting that for the BGL dataset529\nit is enough to encode the context information shared over 3530\nconsecutive log lines. Due to the training of the networks with531\nthe same ten training sets, we assume that the high standard532\ndeviation values for the 5 ×5 and 6 ×6 model are caused533\nmainly by the random initialization. Applying the Friedman534\ntest for different kernel sizes in the convolutional subnetwork535\nresults in p< 0.0007 (χ2 = 19.44) for F1-Score, p< 0.0001536\n(χ2 = 24.64) for Recall, and p <0.1955 (χ2 = 6.05) for537\nPrecision.538\nFor the Spirit dataset, the best result is achieved with th=539\n0.7525 ×10−3, h= 1 and a 4 ×4 kernel size with a F1-score540\nof 97.69 (see Tab. 3). In contrast to the results on the BGL541\ndataset, the lowest Precision value of 67.282 for the anomaly542\nsamples occurs with a 3 ×3 kernel while the Precision value543\nfor 2 ×2 kernel is only a bit lower than on the BGL dataset544\n(90.94). Another difference is the decrease in performance for545\nthe bigger kernel sizes 5 ×5 and 6 ×6 (F1-score of 85.89 and546\n94.02, respectively). We assume that the lower performance547\nis caused by irrelevant information which is processed by the548\nbigger kernel. Applying the Friedman test for different kernel549\nsizes in the convolutional subnetwork results in p <0.0161550\n(χ2 = 12 .18) for F1-Score, p <0.0026 (χ2 = 16 .27) for551\nRecall, and p< 0.0010 (χ2 = 18.40) for Precision.552\nB. COMPARISON WITH OTHER TRANSFORMER-BASED553\nMODELS554\nAfter evaluating the inﬂuence of the single hyperparam-555\neters, we want to compare the performance of the CCT556\nwith previously published Transformer-based approaches for557\nanomaly detection. In order to avoid a bias in choosing the558\nbest hyperparameters for the particular dataset and enable559\ncomparability with other approaches, we used the validation560\nset to ﬁnd the best-performing hyperparameters. While the561\ndecision threshold was obtained automatically, we report here562\nthe kernel size and hparameter, which achieves the highest563\nF1-score on the validation set and the obtained performance564\nvalues on the test set.565\nFor the BGL dataset, we observed the highest F1-Score566\non the validation set with a kernel size of 6 ×6 and h = 1,567\nresulting in an F1-Score of 97.0 on the test set (Tab. 4). Due568\nto this, our approach shows a slightly lower F1-Score than the569\nbest-performing supervised approach from Le and Zhang[10],570\nwhich have used 80% of the dataset for training and 20% for571\ntesting. Further, our approach shows a higher F1-Score than all572\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nFigure 5: Precision, Recall and F1-Score on the Blue Gene/L dataset for different threshold valuesColumns indicate the\nkernel size from a 2 ×2 to a 6 ×6 convolution kernel. Rows indicate the Precision (blue), Recall (red), and F1-score (green),\nrespectively. The X-axis represents the changing detection threshold. The median is indicated by a black line and a bigger\ndot, the smaller dots indicating outliers, and the box 50% of the datapoints. A bigger kernel leads to better performance, by\nintegrating more context information. Lower values for the detection threshold lead to an increase in Precision due to a smaller\nnumber of false positives. When the threshold becomes too small, the number of false negatives increases. The hvalue is one.\nFigure 6: Precision, Recall and F1-Score on the Spirit dataset for different threshold values.Columns indicate the kernel\nsize from a 2×2 to a 6×6 convolution kernel. Rows indicate the Precision (blue), Recall (red), and F1-score (green), respectively.\nThe X-axis represents the changing detection threshold. The median is indicated by a black line and a bigger dot, the smaller dots\nindicating outliers, and the box 50% of the datapoints. With a 4 ×4 kernel, the network performs best, while both bigger kernels\nshow higher standard deviations. Lower values for the detection threshold lead to an increase in Precision due to a smaller\nnumber of false positives. When the threshold becomes too small, the number of false negatives increases. The hvalue is one.\nTable 2: Median of Precision, Recall, and F1-Score on the on the validation set of the BGL dataset with different sizes of the\nconvolutional kernel. A ﬁxed detection threshold th= 0.505 ×10−3, and number of anomalous tokens of one (h=1).\nKernel size Precision Recall F1-Score\nnormal anomaly normal anomaly normal anomaly\n2 × 2 99.51 ± 19.02 86.65 ± 6.45 84.32 ± 6.51 99.71 ± 20.05 88.68 ± 11.32 92.72 ± 13.18\n3 × 3 99.66 ± 0.12 99.26 ± 0.32 99.04 ± 0.42 99.77 ± 0.09 99.36 ± 0.24 99.51 ± 0.18\n4 × 4 99.95 ± 0.16 98.68 ± 0.57 98.25 ± 0.76 99.96 ± 0.12 98.99 ± 0.43 99.23 ± 0.32\n5 × 5 99.89 ± 10.90 99.18 ± 0.2 98.99 ± 0.18 99.92 ± 13.06 99.43 ± 6.61 99.55 ± 8.34\n6 × 6 99.95 ± 14.58 99.22 ± 0.39 99.05 ± 0.40 99.96 ± 17.55 99.45 ± 8.86 99.58 ± 11.25\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\n(a)\n (b)\nFigure 7: Higher hleads to lower Recall values.Columns indicate the kernel size from a 2 ×2 to a 6 ×6 convolution kernel.\nRows indicate the Precision (blue), Recall (red), and F1-score (green), respectively. On the right are shown the values for the\nBGL validation set and on the left for the Spirit validation set. A higher value for hleads to an increase in the Recall value, due\nto a higher number of false negatives. The value for the detection threshold this 0.505 ×10−3.\nTable 3: Median of Precision, Recall, and F1-Score on the on the validation set of the Spirit dataset with different sizes of the\nconvolutional kernel. A ﬁxed detection threshold th= 0.7525 ×10−3, and number of anomalous tokens of one(h=1).\nKernel size Precision Recall F1-Score\nnormal anomaly normal anomaly normal anomaly\n2 × 2 99.97 ± 0.07 90.94 ± 11.37 96.78 ± 10.60 99.93 ± 0.21 98.36 ± 6.59 95.23 ± 7.45\n3 × 3 99.86 ± 3.38 67.28 ± 19.90 84.31 ± 16.23 99.58 ± 11.47 91.48 ± 10.55 79.01 ± 12.35\n4 × 4 99.84 ± 2.92 97.34 ± 3.45 99.16 ± 1.29 99.53 ± 10.04 99.26 ± 1.53 97.69 ± 5.95\n5 × 5 96.32 ± 3.67 85.42 ± 13.00 94.47 ± 6.95 90.98 ± 12.83 95.53 ± 3.89 85.89 ± 9.06\n6 × 6 97.26 ± 4.17 96.73 ± 15.75 98.98 ± 8.27 91.30 ± 12.32 98.15 ± 5.71 94.02 ± 12.78\nTable 4: Precision, Recall, and F1-Score on the BGL dataset in comparison with previously published methods. Score values of\nour network (CCT) obtained on the test set. Due to different splits of the dataset into training, validation and test set, reported\nscores from the literature have to be treated with caution, as detailed in the discussion.\nModel Learning type Precision Recall F1-Score\nHuang et al. [6] supervised 96.4 88.6 92.3\nLe and Zhang [10] supervised 98.0 98.0 98.0\nNedelkoski et al. [7] self-supervised 52.0 87.0 65.0\nGuo et al. [8] self-supervised 89.4 92.3 90.8\nLee et al. [9] self-supervised - - 87.5\nCCT_6 × 6 self-supervised 95.87 ± 4.83 99.12 ± 0.75 97.0 ± 2.49\nother self-supervised approaches, and the highest Recall value573\nof 99.12 regardless if supervised or self-supervised training574\nwas used. A lower Precision value of 96.39 indicates a lower575\nnumber of false negative samples (or missed anomalies), but576\na higher number of false positive samples (or false alarms), in577\ncomparison to other methods.578\nWith a convolutional kernel size of 4 ×4 and h = 1, the579\nCCT achieves on the Spirit dataset an F1-Score of 97.69 on580\nthe test set, what is a higher F1-Score then the works of Le and581\nZhang [10] and Nedelkoski et al. [7] (Tab. 5). While the CCT582\nachieves the highest Recall score (99.39), the Precision score583\n(96.0) is again lower. Considering the fact that Le and Zhang584\n[10] trained their network in a supervised fashion, this shows585\nnevertheless the potential of our self-supervised approach.586\nC. ANOMALY INTERPRETATION587\nSince the development of deep neural networks, the un-588\nderstanding and interpretability of the outcome of neural589\nnetworks have become very important [ 46]. This becomes590\neven more important for deep networks analyzing log data591\nof computer systems in critical infrastructures and cyberse-592\ncurity related domains [47]. In the presented approach, it is593\npossible to visualize the input log sequence with the predicted594\nprobability to identify which part of the log sequence is the595\nprobable cause for an anomaly (see Fig. 8).596\nThis visualization can either help to identify the anomaly in597\nthe system faster or to verify the correctness of the prediction598\nthrough an expert.599\nIV. DISCUSSION600\nWe have demonstrated how the Compact Convolutional601\nTransformer (CCT) [12] can be used to detect anomalies in602\nlog data using data from two HPC systems, Blue Gene/L603\n(BGL) and Spirit. Furthermore, we have demonstrated how604\nthe detection beneﬁts from the combination of convolutional605\ninput processing to capture local information and the ability606\nVOLUME 4, 2016 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nTable 5: Precision, Recall and, F1-Score on the Spirit dataset in comparison with previously published methods. Score values of\nour network (CCT) obtained on the test set. Due to different splits of the dataset into training, validation and test set, reported\nscores from the literature have to be treated with caution, as detailed in the discussion.\nModel Learning type Precision Recall F1-Score\nLe and Zhang [10] supervised 98.0 96.0 97.0\nNedelkoski et al. [7] self-supervised 69.0 56.0 62.0\nCCT_4 × 4 self-supervised 96.0 ± 2.29 99.39 ± 2.08 97.69 ± 2.05\n(a)\n(b)\nFigure 8: Visualization the prediction probability score for\nlog sequences, which are detected correctly as normal (a) or\nas anomaly (b). Green indicates a high probability, yellow a\nmoderate probability, and red a probability below the detection\nthreshold ( th = 0 .505 ×10−3). The red-colored words\nsuggests, which word in the log sequence causes the prediction\nas an anomaly. Example sequences are taken from the Blue\nGene/L test set.\nof Transformers to encode more global context information.607\nOur results show that a small kernel size of 3 ×3 raises the608\nperformances of the network to a level comparable with previ-609\nous Transformer-based approaches for the Precision score and610\neven outperforms them at the Recall score. This highlights the611\nimportance of the contextual information between different612\nlog sequences and how processing them in parallel improves613\nthe anomaly detection.614\nOn the BGL dataset, the median performance of all kernel615\nsizes, except the 2 ×2 kernel, shows only small differences.616\nThis suggests that a bigger kernel could be used, enabling617\nthe processing of more complex information and a more618\ncompact representation, making it possible to reduce the size619\nof the Transformer encoder part (by reducing the number of620\nencoder blocks, attention heads, or the input dimensionality)621\nand leading to a reduced training time. However, our results622\nobtained on the Spirit dataset suggests that the correct kernel623\nsize depends on the dataset, as the highest F1-Score is624\nachieved with a 4 ×4 kernel size and both smaller and bigger625\nkernel sizes lead to a decrease in the performance and to a626\nhigher standard deviation between multiple network instances,627\nsuggesting a less robust detection performance.628\na: Recent Work629\nSince the growing success of Transformer networks in natural630\nlanguage processing (NLP), they became more interesting631\nfor tasks which are similar to NLP, like anomaly detection632\nin log data [ 6, 7, 8, 9, 10]. However, the different model633\napproaches make different pre-processing steps necessary,634\nsuch as log-data pre-parsing, converting strings into tokens,635\nand sorting out redundant data (to name a few). Due to this, the636\ncomparison with other approaches can be difﬁcult. However,637\nin the following section, we discuss other Transformer-based638\nmodels for anomaly detection in system log data.639\nWith the HitAnomaly network, Huang et al. [6] presented one640\nof the ﬁrst approaches using a Transformer-based network.641\nThey used a pre-parsing mechanism to create templates and642\nparameters out of the log data. To process the templates and643\nthe corresponding parameters, the network is composed of644\ntwo encoders. Each encoder consists of Transformer blocks645\nto create a latent representation of the templates and the646\nparameters. Both representations together are used by a647\nclassiﬁer to predict normal and abnormal log data. They648\nachieved an F1-Score of 92.1 on the BGL dataset [ 6] with649\nthis supervised model (see Tab. 4). With a Recall of 90.0,650\ntheir network has more false negative predictions than our651\napproach, and a slightly lower Precision value of 95.0.652\nNedelkoski et al. [7] proposed the Logsy network. For the653\nnetwork to distinguish between normal and abnormal log654\nsequences, they used two different types of inputs for training.655\nThe ﬁrst type are normal log sequences from the original656\ndataset that should be analyzed. The second type are auxiliary657\nsequences from a completely different dataset. By labeling the658\nﬁrst type as \"normal\" and the second type as \"anomaly\", they659\ntrain the network in a self-supervised manner. A spherical660\nloss function is used to maximize the difference in the latent661\nrepresentation between \"normal\" and \"anomaly\" messages.662\nAn interesting feature of the usage of such labels is that it663\nmakes it possible to train later the network with real \"anomaly\"664\ndata, labeled by experts Nedelkoski et al. [7]. However, a F1-665\nScore of 44.0 on the BGL dataset is much lower than our666\napproach by a similar training/test split ( 60% training and667\n12 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\n40% test), which is mainly caused by the very low Precision668\nvalue, indicating a high rate of false positives. For the Spirit669\ndataset, the authors report a F1-Score of 56.0 with the same670\nsplit. With a training to test split of 80% to 20%, the authors671\nreport an increase of the Precision but a slight decrease of672\nthe Recall. This leads to a F1-Score of 65.0 and 62.0 for the673\nBGL and Spirit dataset (respectively). If the authors add 2.1%674\nof correctly labeled anomaly data into the training set, the675\nPrecision increases to 89.0 and the F1-Score to 80.0 on the676\nBGL dataset (data on the Spirit dataset are not reported). This677\nshows the impact of even a small number of correctly labeled678\ndata on the performance of the network.679\nGuo et al. [8] introduced the LogBERT network. They use680\npre-parsing to identify the templates in the log messages and681\nfeed the tokenized representation into a Transformer encoder.682\nThey conserve correlated information between consecutive log683\nmessages by creating log sequences, where one log-sequence684\ncontains templates of multiple log messages. Similarly to the685\nBERT Transformer [4], LogBERT is trained on a masked key686\nprediction task, where some randomly chosen tokens in one687\nsequence are masked and the network must learn to predict688\nthem correctly. Additionally, they used an additional loss689\nfunction to minimize the distance of the representations of the690\nnormal log sequences to the center of a hypersphere. With this,691\nthe encoder representations of normal log sequences should692\nbe closer to the center of the sphere, while the representations693\nof anomaly log sequences should have a greater distance and694\nresult in a higher loss. To identify an anomaly, they mask695\na random number of tokens in a sequence and predict the696\nmasked one. For each masked item, a candidate set is created697\nand if the corresponding input token is not under the candidate698\nset, the token is considered as an anomaly. If there are too699\nmany anomaly tokens in one sequence, the complete sequence700\nis considered as an anomaly. With the proposed LogBERT,701\nthe authors achieved on the BGL dataset a Precision score of702\n89.4 and a Recall value of 92.3.703\nA similar approach was used by Lee et al. [9] for the704\nLAnoBERT network, which is also based on a BERT-like705\nTransformer and trained on the masked key prediction task.706\nIn contrast to the previously mentioned work, they did707\nnot perform pre-parsing on the unstructured log data but708\nlearned directly on the log sequences, which resemble more709\nsingle log messages rather than a sequence of consecutive710\nmessages. To detect anomalies in the test set, every log key711\nin one sequence is masked one after the other to predict712\nthe corresponding masked key. For this, they also created a713\ncandidate set where they consider the six possible tokens with714\nthe highest probability score. Assuming that the distribution715\nof the resulting probabilities are different for normal and716\nabnormal log sequences, they calculated an abnormal score,717\nbased on the probability values of the candidate set. If the718\nabnormal score of a log sequence receives a certain value, the719\nsequence will be classiﬁed as an anomaly. Additionally, they720\ncalculated a second error score, based on the loss value of the721\ncandidate set and compared which of both score values are722\nbetter to detect anomalies. They achieved their highest F1-723\nScore of 87.49 with the abnormal score on the BGL dataset.724\nA supervised trained model, with the usage of a pre-trained725\nBERT model, was presented by [ 10]. They used the BERT726\nmodel to create semantic vectors and learn a binary classiﬁca-727\ntion task on it. With that, they achieved98 for the F1-Score on728\nthe BGL dataset and a F1-Score of 97.9 on the Spirit dataset.729\nThis demonstrates the advantage of learning in a supervised730\nfashion. Despite that, creating labels for datasets regarding731\nto a speciﬁc real-world task is very time consuming, what732\nmakes self-supervised or unsupervised learning methods more733\ninteresting for real world scenarios.734\nAnother stumbling stone in comparing different approaches735\nwith each other is the splitting of the dataset and the usage736\nfor training, validation, and test set. To measure the ﬁnal737\nperformance of an approach and to avoid a bias in the738\nperformance, the performance should be measured on new (or739\nunseen) data, which has not been used either for parameter740\noptimization or to tune additional hyperparameters (like a741\ndecision threshold) [ 30, 31]. This data is called the test742\nset, while the subset used to optimize the weights is called743\ntraining set. The validation set is used to tune additional744\nhyperparameter. If the used dataset did not provide an explicit745\ntraining, validation, and test set, the subsets must be created746\nby hand, as in our study. One way to create the three subsets747\nis to assign samples randomly to one of the three sets. To748\nensure that the obtained high performance is not caused by749\nluck in assigning the samples to the subsets, we ﬁrst shufﬂed750\nthe complete dataset ten times and split it into a training,751\nvalidation, and test set. An alternative approach would be k-752\nfold cross-validation [41].753\nMost model approaches with a self-supervised learning754\nscheme require one or more additional hyperparameters to755\ndetect an anomaly (such as a decision threshold). In the self-756\nsupervised model papers to which we compare our approach,757\nonly Guo et al. [8] mentioned the usage of a validation set758\nto determine the hyperparameters, without clarifying how759\nthe validation set was obtained. Huang et al. [6] and Le760\nand Zhang [10] mention the division of the dataset into a761\ntraining and test set, but not the usage of a validation set.762\nBy splitting the dataset into a train, validation, and test set,763\nit is necessary to avoid data leakage to guarantee that only764\nunseen data exists in the test set. In Le and Zhang [10], the765\nauthors sorted the dataset samples in chronological order and766\ntook the ﬁrst 80% for training and the remaining 20% for767\nthe test set. A similar procedure was done by Nedelkoski768\net al. [7] by sorting the data samples chronologically and769\nevaluating the split-ratio between training and test set, without770\nany validation set. While they motivated that this division771\nwould guarantee only unseen data in the test set, it must be772\ntaken into account that entries in log data can be written with773\na speciﬁc frequency. As a consequence, the same sample774\ncould be in the training and test set. Due to this, we sorted775\nduplicated samples between the training, validation and test776\nset out. How sorting out duplicated samples can inﬂuence the777\nperformance is observable by comparing the Recall, Precision,778\nand F1-Score between the validation set and the test set.779\nVOLUME 4, 2016 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nOn the BGL dataset, the Precision decreases from 99.18780\non the validation set to 96.39 on the test set, with a kernel781\nsize of 5 ×5, indicating that the ratio between true positive782\nsamples and false positive samples shifted. We assume that783\nsorting out reduces the number of detectable anomaly samples,784\ndecreasing the number of true positive samples in relation785\nto false positive samples. It has also to be mentioned that786\nsorting out duplicated samples can introduce a new imbalance787\nbetween the number of normal and anomalous samples. This788\nis, for example, observable for the BGL dataset, where the789\nnumber of anomalous samples is roughly 5% of the number790\nof normal samples in the test dataset, while the validation791\ndataset is much more balanced. This supports how important792\nit is to correctly split the dataset for the ﬁnal evaluation of the793\nmodel. The different ways to obtain a training, validation, and794\ntest set complicate the comparability of different approaches,795\nquestioning the numbers reported in the studies, leading to796\nproblems in reproducing study results [ 32], and making it797\ndifﬁcult to choose the right method in a practical situation798\n[30].799\nIn the original publication of the other Transformer-based800\napproaches, no information about the number of Transformer801\nblocks or trainable parameters could be found. Considering802\nthe BERTBASE model [4], BERT consists of 11 Transformer803\nblocks and at least 110 million parameters, what can be804\nconsidered the size of the Transformer-based approaches805\ndiscussed here. In contrast to that, the CCT used in this806\nwork consists of two convolutional layers with 128 and 64807\nfeature maps (respectively), 5 Transformer blocks, and 5808\nattention heads. In total with the additional prediction head,809\nthe complete network consists of only 2.6 million parameters.810\nAlthough our network only used a fraction of their parameters,811\nthe proposed network can outperform all other self-supervised812\napproaches.813\nb: Possible improvements814\nCritical metrics for the correct evaluation for an anomaly815\ndetection system are the Recall and the Precision values for816\nthe normal and anomaly class. With regard to the CCT in817\nthis work, the number of convolutional layers, Transformer818\nblocks and MLP blocks were found experimentally. However,819\nfurther improvements could be obtained with a more extensive820\nhyperparameter search using optimization frameworks such821\nas Optuna [48] or skorch [49].822\nOur results on the BGL dataset show that, for a detection823\nthreshold (th) from 1.0×10−3 to 0.505×10−3 and a detection824\nthreshold of one ( h = 1), the Recall value for the anomaly825\nclass is over99.7 for the 3×3 and 4×4 models. If the threshold826\ndecreases further, the Recall score drops, while the Precision827\nscore increases. A similar behavior is also observable for the828\nother model variants (see Fig. 5). This shows how the balance829\nbetween Recall and Precision is determined by the detection830\nthreshold. We assume that with a higher detection threshold831\nthe Precision would be decreased, due to a higher number of832\nfalsely detected anomalies.833\nDifferent methods have been proposed to ﬁnd a suitable834\ndecision threshold automatically, using optimization methods835\nsuch as Grid-Search or Bayesian optimization [50]. If a deep836\nneural network is used in anomaly detection, the loss on the837\nvalidation set can be used to determine the decision threshold838\nautomatically [9]. We tested a method to obtain the decision839\nthreshold automatically by using the Precision-Recall curve,840\ninstead of using the Receiver Operating Characteristic (ROC)841\ncurve, which is a common method to determine the quality842\nin a binary classiﬁcation task. It has been argued that the843\nPrecision-Recall curve leads to a more robust evaluation of a844\nbinary classiﬁcation task on an unbalanced dataset [51].845\nTo classify a log sequence as an anomaly, the number of tokens846\npredicted as an anomaly in a sequence must be above the value847\nh. With a threshold of one, only one anomalous token will848\nlead the prediction as an anomaly. This can lead to a high849\nnumber of false positives, for example, if a single word is not850\nincluded in the training set. With a hof 2, a slight increase of851\nthe Precision values for the Spirit dataset is notable, but also a852\nstrong decrease of the Recall value, as well as an increase of853\nthe standard deviation (see Fig. 7).854\nIn masked learning, a part of the input is masked out and855\nthe network should learn to predict either only the masked856\nparts or the complete (unmasked) input sample. It has been857\nshown that, for 2D images, a masking ratio of ∼75% can be858\nused if the complete unmasked image is predicted [52], due859\nto the high information redundancy in an image. Despite the860\nfact that the created log windows in our work are similar to861\n2D images, our network predicts only the last log sequence862\nin a log window (not the complete log window), and the863\ntokens in the last log sequence are not very redundant (as864\nin a language prediction task). To evaluate whether a higher865\nmasking ratio leads to better performance, we trained the CCT866\nwith a 4 ×4 convolutional kernel and a 50% masking ratio on867\nthe Blue Gene/L dataset. With the higher masking ratio, the868\nnetwork obtained a Precision value of 94.52, a Recall value869\nof 57.87, and an F1-Score of 74.27 on average over 5 model870\nrepetitions and at a decision threshold of th= 0.505 ×10−3871\n(see Fig. S7). In comparison to a CCT trained with an identical872\nconvolutional kernel size and a masking ratio of20%, all three873\nperformance values decrease.874\nOur results show that lower Precision and Recall scores are875\npaired with a higher standard deviation. We hypothesize the876\nfollowing reasons for that: The weights and biases in neural877\nnetworks are initialized randomly, so the starting point for878\nthe optimization is different for every model run. To train the879\nmodel, the Adam optimizer with weight decay was used [37]880\nwith a learning rate of 0.0001 and a weight decay of 0.00001.881\nThis was mainly done to improve the regularization of the882\nnetwork, but the values for the learning rate and weight decay883\nwere found experimentally and could be ﬁne tuned further.884\nIt also has been suggested that techniques like \"warmup885\nlearning\" can increase the stability of the network training886\n[53].887\nAs in previous work, we understand anomaly detection in888\nlog data as a natural language processing task [ 10]. We889\nperformed natural language pre-processing by removing890\n14 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\nspecial characters and numbers, as it is done for example in891\nsentiment analysis [54]. Despite the good performance of the892\nproposed method, it is still unclear whether numbers should893\nbe removed or not for anomaly detection in log data: removing894\nthe information on how much free space is left on a hard drive895\nor how high is the workload of the CPU can lead to a high896\nnumber of false negatives, as critical system state caused by897\nvery high CPU workload can not be detected. On the other898\nside, it can be assumed that ﬂoating point numbers with a low899\nappearance frequency in the original data will lead to a high900\namount of tokens. To reduce the number of possible tokens,901\nwe suggest treading the numbers before and after the decimal902\npoint as individual elements.903\nc: Conclusion904\nIn this study, we presented a log anomaly detection network905\nbased on the Compact Convolutional Transformer [12]. Our906\nresults show that the use of 2D-convolutional layers leads907\nto the successful learning of contextual correlations between908\nlog messages without further pre-parsing. This conserves the909\ncontextual information and leads, to the best of our knowledge,910\nto the highest F1-Score a Transformer-based network has911\nachieved so far on the Spirit dataset, comparable to the best912\nsupervised trained approaches. Our study demonstrates how913\nthe combination of processing spatial information with a 2D-914\nconvolutional layer and the encoding of broader contextual915\ninformation via attention can be used for log data analysis.916\nACKNOWLEDGMENT917\nThe authors thank Franka Schuster and Andreas Paul for their918\nhelpful comments and fruitful discussions.919\nReferences920\n[1] Min Du and Feifei Li. Spell: Streaming parsing of921\nsystem event logs. In 2016 IEEE 16th International922\nConference on Data Mining (ICDM) , pages 859–864,923\n2016. .924\n[2] Sasho Nedelkoski, Jasmin Bogatinovski, Alexander925\nAcker, Jorge Cardoso, and Odej Kao. Self-supervised log926\nparsing. In Yuxiao Dong, Dunja Mladeni ´c, and Craig927\nSaunders, editors, Machine Learning and Knowledge928\nDiscovery in Databases: Applied Data Science Track ,929\npages 122–138, Cham, 2021. Springer International930\nPublishing. ISBN 978-3-030-67667-4.931\n[3] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob932\nUszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz933\nKaiser, and Illia Polosukhin. Attention is all you934\nneed. In I. Guyon, U. V on Luxburg, S. Bengio,935\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-936\nnett, editors, Advances in Neural Information Pro-937\ncessing Systems, volume 30. Curran Associates, Inc.,938\n2017. URL https://proceedings.neurips.cc/paper/2017/939\nﬁle/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.940\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and941\nKristina Toutanova. Bert: Pre-training of deep bidirec-942\ntional transformers for language understanding, 2018.943\nURL https://arxiv.org/abs/1810.04805.944\n[5] Anthony Gillioz, Jacky Casas, Elena Mugellini, and945\nOmar Abou Khaled. Overview of the transformer-based946\nmodels for nlp tasks. In 2020 15th Conference on947\nComputer Science and Information Systems (FedCSIS),948\npages 179–183, 2020. .949\n[6] Shaohan Huang, Yi Liu, Carol Fung, Rong He, Yining950\nZhao, Hailong Yang, and Zhongzhi Luan. Hitanomaly:951\nHierarchical transformers for anomaly detection in952\nsystem log. IEEE Transactions on Network and Service953\nManagement, 17(4):2064–2076, 2020. .954\n[7] Sasho Nedelkoski, Jasmin Bogatinovski, Alexander955\nAcker, Jorge Cardoso, and Odej Kao. Self-attentive956\nclassiﬁcation-based anomaly detection in unstructured957\nlogs. In 2020 IEEE International Conference on Data958\nMining (ICDM), pages 1196–1201, 2020. .959\n[8] Haixuan Guo, Shuhan Yuan, and Xintao Wu. Logbert:960\nLog anomaly detection via bert. In 2021 International961\nJoint Conference on Neural Networks (IJCNN), pages962\n1–8, 2021. .963\n[9] Yukyung Lee, Jina Kim, and Pilsung Kang. Lanobert964\n: System log anomaly detection based on bert masked965\nlanguage model, 2021. URL https://arxiv.org/abs/2111.966\n09564.967\n[10] Van-Hoang Le and Hongyu Zhang. Log-based anomaly968\ndetection without log parsing. In 2021 36th IEEE/ACM969\nInternational Conference on Automated Software Engi-970\nneering (ASE), pages 492–504, 2021. .971\n[11] Zhenfei Zhao, Weina Niu, Xiaosong Zhang, Runzi972\nZhang, Zhenqi Yu, and Cheng Huang. Trine: Syslog973\nanomaly detection with three transformer encoders in974\none generative adversarial network.Applied Intelligence,975\nNov 2021. ISSN 1573-7497. . URL https://doi.org/10.976\n1007/s10489-021-02863-9.977\n[12] Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu978\nAbuduweili, Jiachen Li, and Humphrey Shi. Escaping979\nthe big data paradigm with compact transformers, 2021.980\nURL https://arxiv.org/abs/2104.05704.981\n[13] Tom Brown, Benjamin Mann, Nick Ryder, Melanie982\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind983\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda984\nAskell, Sandhini Agarwal, Ariel Herbert-V oss, Gretchen985\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,986\nDaniel Ziegler, Jeffrey Wu, Clemens Winter, Chris987\nHesse, Mark Chen, Eric Sigler, Mateusz Litwin,988\nScott Gray, Benjamin Chess, Jack Clark, Christo-989\npher Berner, Sam McCandlish, Alec Radford, Ilya990\nSutskever, and Dario Amodei. Language models991\nare few-shot learners. In H. Larochelle, M. Ran-992\nzato, R. Hadsell, M.F. Balcan, and H. Lin, editors,993\nAdvances in Neural Information Processing Systems ,994\nvolume 33, pages 1877–1901. Curran Associates, Inc.,995\n2020. URL https://proceedings.neurips.cc/paper/2020/996\nﬁle/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.997\n[14] Alexander D. Kent. Cyber security data sources for998\nVOLUME 4, 2016 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\ndynamic network research, volume V olume 1 ofSecurity999\nScience and Technology, pages 37–65. WORLD SCIEN-1000\nTIFIC (EUROPE), Dec 2016. ISBN 978-1-78634-074-0.1001\n. URL https://doi.org/10.1142/9781786340757_0002. 0.1002\n[15] Adam Oliner and Jon Stearley. What supercomputers1003\nsay: A study of ﬁve system logs. In 37th Annual1004\nIEEE/IFIP International Conference on Dependable1005\nSystems and Networks (DSN’07), pages 575–584, 2007.1006\n.1007\n[16] Shilin He, Pinjia He, Zhuangbin Chen, Tianyi Yang,1008\nYuxin Su, and Michael R. Lyu. A survey on automated1009\nlog analysis for reliability engineering. ACM Comput.1010\nSurv., 54(6), jul 2021. ISSN 0360-0300. . URL https:1011\n//doi.org/10.1145/3460345.1012\n[17] Karen Simonyan and Andrew Zisserman. Very deep con-1013\nvolutional networks for large-scale image recognition,1014\n2014. URL https://arxiv.org/abs/1409.1556.1015\n[18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian1016\nSun. Deep residual learning for image recognition, 2015.1017\nURL https://arxiv.org/abs/1512.03385.1018\n[19] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian1019\nSun. Faster r-cnn: Towards real-time object detec-1020\ntion with region proposal networks. In C. Cortes,1021\nN. Lawrence, D. Lee, M. Sugiyama, and R. Gar-1022\nnett, editors, Advances in Neural Information Pro-1023\ncessing Systems, volume 28. Curran Associates, Inc.,1024\n2015. URL https://proceedings.neurips.cc/paper/2015/1025\nﬁle/14bfa6bb14875e45bba028a21ed38046-Paper.pdf.1026\n[20] Zhenyu Wu, Yang Guo, Wenfang Lin, Shuyang Yu, and1027\nYang Ji. A weighted deep representation learning model1028\nfor imbalanced fault diagnosis in cyber-physical systems.1029\nSensors, 18(4), 2018. ISSN 1424-8220. . URL https:1030\n//www.mdpi.com/1424-8220/18/4/1096.1031\n[21] Wei Wang, Ming Zhu, Xuewen Zeng, Xiaozhou Ye,1032\nand Yiqiang Sheng. Malware trafﬁc classiﬁcation using1033\nconvolutional neural network for representation learn-1034\ning. In 2017 International Conference on Information1035\nNetworking (ICOIN), pages 712–717, 2017. .1036\n[22] Martín Abadi, Ashish Agarwal, Paul Barham, Eugene1037\nBrevdo, Zhifeng Chen, Craig Citro, Greg S. Corrado,1038\nAndy Davis, Jeffrey Dean, Matthieu Devin, Sanjay Ghe-1039\nmawat, Ian Goodfellow, Andrew Harp, Geoffrey Irving,1040\nMichael Isard, Yangqing Jia, Rafal Jozefowicz, Lukasz1041\nKaiser, Manjunath Kudlur, Josh Levenberg, Dandelion1042\nMané, Rajat Monga, Sherry Moore, Derek Murray, Chris1043\nOlah, Mike Schuster, Jonathon Shlens, Benoit Steiner,1044\nIlya Sutskever, Kunal Talwar, Paul Tucker, Vincent1045\nVanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol1046\nVinyals, Pete Warden, Martin Wattenberg, Martin Wicke,1047\nYuan Yu, and Xiaoqiang Zheng. TensorFlow: Large-1048\nscale machine learning on heterogeneous systems, 2015.1049\nURL https://www.tensorﬂow.org/. Software available1050\nfrom tensorﬂow.org.1051\n[23] François Chollet et al. Keras. https://keras.io, 2015.1052\n[24] Shilin He, Jieming Zhu, Pinjia He, and Michael R.1053\nLyu. Loghub: A large collection of system log datasets1054\ntowards automated log analytics, 2020. URL https:1055\n//arxiv.org/abs/2008.06448.1056\n[25] Y . Liang, Y . Zhang, A. Sivasubramaniam, R.K. Sahoo,1057\nJ. Moreira, and M. Gupta. Filtering failure logs for a1058\nbluegene/l prototype. In 2005 International Conference1059\non Dependable Systems and Networks (DSN’05), pages1060\n476–485, 2005. .1061\n[26] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,1062\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,1063\nMostafa Dehghani, Matthias Minderer, Georg Heigold,1064\nSylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An1065\nimage is worth 16x16 words: Transformers for image1066\nrecognition at scale, 2020. URL https://arxiv.org/abs/1067\n2010.11929.1068\n[27] Zhao Jianqiang. Pre-processing boosting twitter sen-1069\ntiment analysis? In 2015 IEEE International Confer-1070\nence on Smart City/SocialCom/SustainCom (SmartCity),1071\npages 748–753, 2015. .1072\n[28] Symeon Symeonidis, Dimitrios Effrosynidis, and Avi1073\nArampatzis. A comparative evaluation of pre-1074\nprocessing techniques and their interactions for twit-1075\nter sentiment analysis. Expert Systems with Appli-1076\ncations, 110:298–310, 2018. ISSN 0957-4174. .1077\nURL https://www.sciencedirect.com/science/article/pii/1078\nS0957417418303683.1079\n[29] Yonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V .1080\nLe, Mohammad Norouzi, Wolfgang Macherey, Maxim1081\nKrikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff1082\nKlingner, Apurva Shah, Melvin Johnson, Xiaobing1083\nLiu, Łukasz Kaiser, Stephan Gouws, Yoshikiyo Kato,1084\nTaku Kudo, Hideto Kazawa, Keith Stevens, George1085\nKurian, Nishant Patil, Wei Wang, Cliff Young, Jason1086\nSmith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg1087\nCorrado, Macduff Hughes, and Jeffrey Dean. Google’s1088\nneural machine translation system: Bridging the gap1089\nbetween human and machine translation, 2016. URL1090\nhttps://arxiv.org/abs/1609.08144.1091\n[30] D. Heinke and F.H. Hamker. Comparing neural net-1092\nworks: a benchmark on growing neural gas, growing1093\ncell structures, and fuzzy artmap. IEEE Transactions on1094\nNeural Networks, 9(6):1279–1291, 1998. .1095\n[31] Daniel Arp, Erwin Quiring, Feargus Pendlebury, Alexan-1096\nder Warnecke, Fabio Pierazzi, Christian Wressnegger,1097\nLorenzo Cavallaro, and Konrad Rieck. Dos and don’ts of1098\nmachine learning in computer security. In 31st USENIX1099\nSecurity Symposium (USENIX Security 22), pages 3971–1100\n3988, Boston, MA, August 2022. USENIX Association.1101\nISBN 978-1-939133-31-1. URL https://www.usenix.org/1102\nconference/usenixsecurity22/presentation/arp.1103\n[32] Sayash Kapoor and Arvind Narayanan. Leakage and the1104\nreproducibility crisis in ml-based science, 2022. URL1105\nhttps://arxiv.org/abs/2207.07048.1106\n[33] Jesse Davis and Mark Goadrich. The relationship1107\nbetween precision-recall and roc curves. In Proceedings1108\nof the 23rd International Conference on Machine Learn-1109\ning, ICML ’06, page 233–240, New York, NY , USA,1110\n16 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\n2006. Association for Computing Machinery. ISBN1111\n1595933832. . URL https://doi.org/10.1145/1143844.1112\n1143874.1113\n[34] Sheraz Naseer, Yasir Saleem, Shehzad Khalid, Muham-1114\nmad Khawar Bashir, Jihun Han, Muhammad Munwar1115\nIqbal, and Kijun Han. Enhanced network anomaly1116\ndetection based on deep neural networks. IEEE Access,1117\n6:48231–48246, 2018. .1118\n[35] F. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,1119\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,1120\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos, D. Cour-1121\nnapeau, M. Brucher, M. Perrot, and E. Duchesnay. Scikit-1122\nlearn: Machine learning in Python. Journal of Machine1123\nLearning Research, 12:2825–2830, 2011.1124\n[36] Longlong Jing and Yingli Tian. Self-supervised visual1125\nfeature learning with deep neural networks: A survey,1126\n2019. URL https://arxiv.org/abs/1902.06162.1127\n[37] Ilya Loshchilov and Frank Hutter. Decoupled weight1128\ndecay regularization, 2017. URL https://arxiv.org/abs/1129\n1711.05101.1130\n[38] Celso A. R. de Sousa. An overview on weight initial-1131\nization methods for feedforward neural networks. In1132\n2016 International Joint Conference on Neural Networks1133\n(IJCNN), pages 52–59, 2016. .1134\n[39] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian1135\nSun. Delving deep into rectiﬁers: Surpassing human-1136\nlevel performance on imagenet classiﬁcation. In 20151137\nIEEE International Conference on Computer Vision1138\n(ICCV), pages 1026–1034, 2015. .1139\n[40] Xavier Glorot and Yoshua Bengio. Understanding the1140\ndifﬁculty of training deep feedforward neural networks.1141\nIn Yee Whye Teh and Mike Titterington, editors, Pro-1142\nceedings of the Thirteenth International Conference1143\non Artiﬁcial Intelligence and Statistics , volume 9 of1144\nProceedings of Machine Learning Research, pages 249–1145\n256, Chia Laguna Resort, Sardinia, Italy, 13–15 May1146\n2010. PMLR. URL https://proceedings.mlr.press/v9/1147\nglorot10a.html.1148\n[41] Claude Sammut and Geoffrey I. Webb, editors. Encyclo-1149\npedia of Machine Learning and Data Mining. Springer1150\nReference. Springer, New York, 2 edition, 2017. ISBN1151\n978-1-4899-7685-7. .1152\n[42] Hassan Ismail Fawaz, Benjamin Lucas, Germain1153\nForestier, Charlotte Pelletier, Daniel F. Schmidt,1154\nJonathan Weber, Geoffrey I. Webb, Lhassane Idoumghar,1155\nPierre-Alain Muller, and François Petitjean. Incep-1156\ntiontime: Finding alexnet for time series classiﬁcation.1157\nData Mining and Knowledge Discovery , 34(6):1936–1158\n1962, Nov 2020. ISSN 1573-756X. . URL https:1159\n//doi.org/10.1007/s10618-020-00710-y.1160\n[43] Daniel J. Benjamin, James O. Berger, Magnus Johannes-1161\nson, Brian A. Nosek, E.-J. Wagenmakers, Richard Berk,1162\nKenneth A. Bollen, Björn Brembs, Lawrence Brown,1163\nColin Camerer, David Cesarini, Christopher D. Cham-1164\nbers, Merlise Clyde, Thomas D. Cook, Paul De Boeck,1165\nZoltan Dienes, Anna Dreber, Kenny Easwaran, Charles1166\nEfferson, Ernst Fehr, Fiona Fidler, Andy P. Field, Mal-1167\ncolm Forster, Edward I. George, Richard Gonzalez,1168\nSteven Goodman, Edwin Green, Donald P. Green,1169\nAnthony G. Greenwald, Jarrod D. Hadﬁeld, Larry V .1170\nHedges, Leonhard Held, Teck Hua Ho, Herbert Hoijtink,1171\nDaniel J. Hruschka, Kosuke Imai, Guido Imbens, John1172\nP. A. Ioannidis, Minjeong Jeon, James Holland Jones,1173\nMichael Kirchler, David Laibson, John List, Roder-1174\nick Little, Arthur Lupia, Edouard Machery, Scott E.1175\nMaxwell, Michael McCarthy, Don A. Moore, Stephen L.1176\nMorgan, Marcus Munafó, Shinichi Nakagawa, Brendan1177\nNyhan, Timothy H. Parker, Luis Pericchi, Marco Pe-1178\nrugini, Jeff Rouder, Judith Rousseau, Victoria Savalei,1179\nFelix D. Schönbrodt, Thomas Sellke, Betsy Sinclair,1180\nDustin Tingley, Trisha Van Zandt, Simine Vazire, Dun-1181\ncan J. Watts, Christopher Winship, Robert L. Wolpert,1182\nYu Xie, Cristobal Young, Jonathan Zinman, and Valen E.1183\nJohnson. Redeﬁne statistical signiﬁcance. Nature Hu-1184\nman Behaviour, 2(1):6–10, Jan 2018. ISSN 2397-3374.1185\n. URL https://doi.org/10.1038/s41562-017-0189-z.1186\n[44] Andrade Chittaranjan. The p value and statistical signiﬁ-1187\ncance: Misunderstandings, explanations, challenges, and1188\nalternatives. Indian Journal of Psychological Medicine,1189\n41(3):210–2150, 2019. . URL https://doi.org/10.4103/1190\nIJPSYM.IJPSYM_193_19.1191\n[45] Giovanni Di Leo and Francesco Sardanelli. Statistical1192\nsigniﬁcance: p value, 0.05 threshold, and applications1193\nto radiomics—reasons for a conservative approach.1194\nEuropean Radiology Experimental, 4(1):18, Mar 2020.1195\nISSN 2509-9280. . URL https://doi.org/10.1186/1196\ns41747-020-0145-y.1197\n[46] Pantelis Linardatos, Vasilis Papastefanopoulos, and1198\nSotiris Kotsiantis. Explainable ai: A review of machine1199\nlearning interpretability methods. Entropy, 23(1), 2021.1200\nISSN 1099-4300. . URL https://www.mdpi.com/1201\n1099-4300/23/1/18.1202\n[47] Mayra Macas, Chunming Wu, and Walter Fuertes. A1203\nsurvey on deep learning for cybersecurity: Progress,1204\nchallenges, and opportunities. Computer Net-1205\nworks, 212:109032, 2022. ISSN 1389-1286. .1206\nURL https://www.sciencedirect.com/science/article/pii/1207\nS1389128622001864.1208\n[48] Takuya Akiba, Shotaro Sano, Toshihiko Yanase, Takeru1209\nOhta, and Masanori Koyama. Optuna: A next-generation1210\nhyperparameter optimization framework. InProceedings1211\nof the 25rd ACM SIGKDD International Conference on1212\nKnowledge Discovery and Data Mining, 2019.1213\n[49] Marian Tietz, Thomas J. Fan, Daniel Nouri, Benjamin1214\nBossan, and skorch Developers. skorch: A scikit-learn1215\ncompatible neural network library that wraps PyTorch,1216\nJuly 2017. URL https://skorch.readthedocs.io/en/stable/.1217\n[50] Hyun-Su Kang, Yun-Seok Choi, Jun-Sang Yu, Sung-1218\nWook Jin, Jung-Min Lee, and Youn-Jea Kim. Hyper-1219\nparameter tuning of oc-svm for industrial gas turbine1220\nanomaly detection. Energies, 15(22), 2022. ISSN 1996-1221\n1073. . URL https://www.mdpi.com/1996-1073/15/22/1222\nVOLUME 4, 2016 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/\nLarisch et al.: Preparation of Papers for IEEE ACCESS\n8757.1223\n[51] Takaya Saito and Marc Rehmsmeier. The precision-1224\nrecall plot is more informative than the roc plot when1225\nevaluating binary classiﬁers on imbalanced datasets.1226\nPLOS ONE , 10(3):1–21, 03 2015. . URL https:1227\n//doi.org/10.1371/journal.pone.0118432.1228\n[52] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li,1229\nPiotr Dollár, and Ross Girshick. Masked autoencoders1230\nare scalable vision learners. In Proceedings of the1231\nIEEE/CVF Conference on Computer Vision and Pattern1232\nRecognition (CVPR), pages 16000–16009, June 2022.1233\n[53] Akhilesh Gotmare, Nitish Shirish Keskar, Caiming1234\nXiong, and Richard Socher. A closer look at deep learn-1235\ning heuristics: Learning rate restarts, warmup and distil-1236\nlation, 2018. URL https://arxiv.org/abs/1810.13243.1237\n[54] Mariam Khader, Arafat Awajan, and Ghazi Al-Naymat.1238\nThe effects of natural language processing on big data1239\nanalysis: Sentiment analysis case study. In 2018 Inter-1240\nnational Arab Conference on Information Technology1241\n(ACIT), pages 1–7, 2018. .1242\nRENÉ LARISCHgraduated in computer science1243\nfrom the Chemnitz University of Technology, Ger-1244\nmany, where he is currently pursuing a Ph.D. de-1245\ngree in the lab of artiﬁcial intelligence. He is a1246\nresearch assistant there since 2020. His research1247\ninterests include machine learning, deep learning,1248\napplication- based research, and spiking neural1249\nnetworks.1250\n1251\nJULIEN VITAYreceived an M.Sc. degree in micro-1252\nelectronics from the Université of Rennes, France,1253\nand an engineering degree in signal processing1254\nand microelectronics from the École Supérieure1255\nd’Électricité (Supélec), France, in 2002. He ob-1256\ntained 2006 a Ph.D. degree in computer sci-1257\nence from the Université Henri Poincaré, Nancy,1258\nFrance, and was later a postdoctoral fellow at1259\nthe Department of Psychology of the Westfälis-1260\nche Wilhelms-University, Münster, Germany. He1261\nis since 2011 researcher and lecturer at the Chemnitz University of Tech-1262\nnology, Germany. He has authored/coauthored many papers in top-ranking1263\nconferences and journals. His research interests include computational neu-1264\nroscience (basal ganglia, hippocampus) and reinforcement learning.1265\nFRED H. HAMKERgraduated in electrical engi-1266\nneering at the University of Paderborn, Germany,1267\nin 1994, and received his Ph.D. from the Faculty1268\nof Computer Science at the Technical University1269\nof Ilmenau, Germany, in 1999. He was a Postdoc1270\nat the J.W. Goethe University of Frankfurt and the1271\nCalifornia Institute of Technology, Pasadena, CA,1272\nUSA. In 2008, he received his venia legendi from1273\nthe Department of Psychology at the Westfälische1274\nWilhelms- University, Münster, Germany. Since1275\n2009, he has been a Professor of Artiﬁcial Intelligence in the Department1276\nof Computer Science at the Chemnitz University of Technology, Chemnitz,1277\nGermany. He has authored/coauthored more than 70 international journals1278\nand conference papers, including IEEE. His main research interests include1279\nthe development of neurocomputational models to understand brain function1280\nand to demonstrate their performance in technical systems.1281\n1282\n1283\n18 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3323252\nThis work is licensed under a Creative Commons Attribution 4.0 License. For more information, see https://creativecommons.org/licenses/by/4.0/",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7666171193122864
    },
    {
      "name": "Anomaly detection",
      "score": 0.6332412958145142
    },
    {
      "name": "Transformer",
      "score": 0.617343008518219
    },
    {
      "name": "Mirroring",
      "score": 0.4775986671447754
    },
    {
      "name": "Data mining",
      "score": 0.4610665440559387
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4315565824508667
    },
    {
      "name": "Machine learning",
      "score": 0.37425610423088074
    },
    {
      "name": "Engineering",
      "score": 0.08674359321594238
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Sociology",
      "score": 0.0
    },
    {
      "name": "Communication",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "cited_by": 6
}