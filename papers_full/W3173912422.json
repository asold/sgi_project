{
  "title": "Transformer-based Spatial-Temporal Feature Learning for EEG Decoding",
  "url": "https://openalex.org/W3173912422",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2381823921",
      "name": "Song Yonghao",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Jia, Xueyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2134036308",
      "name": "Yang Lie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2355536679",
      "name": "Xie Longhan",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971518519",
    "https://openalex.org/W3086573485",
    "https://openalex.org/W2925836809",
    "https://openalex.org/W2792724009",
    "https://openalex.org/W3046051368",
    "https://openalex.org/W3021582344",
    "https://openalex.org/W2963283402",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3015530815",
    "https://openalex.org/W2896120927",
    "https://openalex.org/W2077746856",
    "https://openalex.org/W2010371409",
    "https://openalex.org/W1969878365",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3140416091",
    "https://openalex.org/W2899435621",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W2946811830",
    "https://openalex.org/W3102455230",
    "https://openalex.org/W2613375858",
    "https://openalex.org/W3087480840",
    "https://openalex.org/W2948669902",
    "https://openalex.org/W3161343553",
    "https://openalex.org/W3000231660",
    "https://openalex.org/W2971075653",
    "https://openalex.org/W2181785117",
    "https://openalex.org/W2742329239",
    "https://openalex.org/W3004827935",
    "https://openalex.org/W2914609207",
    "https://openalex.org/W3040552008",
    "https://openalex.org/W2944066407",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W2794345050",
    "https://openalex.org/W2954214015",
    "https://openalex.org/W3130174577",
    "https://openalex.org/W3038344852",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3121905318",
    "https://openalex.org/W3088256290",
    "https://openalex.org/W3080222908",
    "https://openalex.org/W2132360759",
    "https://openalex.org/W2971432438",
    "https://openalex.org/W2741907166",
    "https://openalex.org/W3123356826",
    "https://openalex.org/W2766309231",
    "https://openalex.org/W2124101897",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2566239798",
    "https://openalex.org/W1799366690",
    "https://openalex.org/W3092342532",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2075647286",
    "https://openalex.org/W3081155232"
  ],
  "abstract": "At present, people usually use some methods based on convolutional neural networks (CNNs) for Electroencephalograph (EEG) decoding. However, CNNs have limitations in perceiving global dependencies, which is not adequate for common EEG paradigms with a strong overall relationship. Regarding this issue, we propose a novel EEG decoding method that mainly relies on the attention mechanism. The EEG data is firstly preprocessed and spatially filtered. And then, we apply attention transforming on the feature-channel dimension so that the model can enhance more relevant spatial features. The most crucial step is to slice the data in the time dimension for attention transforming, and finally obtain a highly distinguishable representation. At this time, global averaging pooling and a simple fully-connected layer are used to classify different categories of EEG data. Experiments on two public datasets indicate that the strategy of attention transforming effectively utilizes spatial and temporal features. And we have reached the level of the state-of-the-art in multi-classification of EEG, with fewer parameters. As far as we know, it is the first time that a detailed and complete method based on the transformer idea has been proposed in this field. It has good potential to promote the practicality of brain-computer interface (BCI). The source code can be found at: \\textit{https://github.com/anranknight/EEG-Transformer}.",
  "full_text": "1\nTransformer-based Spatial-Temporal Feature\nLearning for EEG Decoding\nYonghao Song, Xueyu Jia, Lie Yang, and Longhan Xie, Member, IEEE\nAbstract—At present, people usually use some methods based\non convolutional neural networks (CNNs) for Electroencephalo-\ngraph (EEG) decoding. However, CNNs have limitations in per-\nceiving global dependencies, which is not adequate for common\nEEG paradigms with a strong overall relationship. Regarding this\nissue, we propose a novel EEG decoding method that mainly relies\non the attention mechanism. The EEG data is ﬁrstly preprocessed\nand spatially ﬁltered. And then, we apply attention transforming\non the feature-channel dimension so that the model can enhance\nmore relevant spatial features. The most crucial step is to slice\nthe data in the time dimension for attention transforming, and\nﬁnally obtain a highly distinguishable representation. At this\ntime, global averaging pooling and a simple fully-connected\nlayer are used to classify different categories of EEG data.\nExperiments on two public datasets indicate that the strategy of\nattention transforming effectively utilizes spatial and temporal\nfeatures. And we have reached the level of the state-of-the-art\nin multi-classiﬁcation of EEG, with fewer parameters. As far\nas we know, it is the ﬁrst time that a detailed and complete\nmethod based on the transformer idea has been proposed in\nthis ﬁeld. It has good potential to promote the practicality of\nbrain-computer interface (BCI). The source code can be found\nat: https://github.com/anranknight/EEG-Transformer.\nIndex Terms—Electroencephalograph (EEG), attention, trans-\nformer, brain-computer interface (BCI), motor imagery (MI).\nI. I NTRODUCTION\nB\nRAIN-computer interface (BCI) is a technology that\nprovides the possibility of establishing direct connections\nbetween the brain and external devices [1]. Researchers extract\nbrain signals to obtain the user’s intention, which is further\nused to control some equipment such as wheelchairs, robots\nand automatic vehicles [2]–[4]. With BCI, it is easier to per-\nform many tasks well without too much manpower. Especially\nit could help the disabled or paralyzed patients to get rid of\nthe predicament that they have to rely on caregivers all the\nday [5].\nFor the above applications, one of the most widely studied\nBCI patterns are the sensorimotor rhythm stimulated by motor\nimagery (MI) on the motor cortex [6]. Different categories of\nMI, such as right-hand movement and left-hand movement\ncould be decoded with Electroencephalograph (EEG), an eco-\nnomical and convenient method for measuring information in\nthe brain [7]. Then the results are used as commands to control\nassistive devices and support the user to do corresponding\nmovement. Rehabilitation training based on MI-BCI has been\n(corresponding author: Longhan Xie)\nYonghao Song, Xueyu Jia, Lie Yang and Longhan Xie are with the\nShien-Ming Wu School of Intelligent Engineering, South China University\nof Technology, Guangzhou 510460, China (e-mail: eeyhsong@gmail.com,\nxielonghan@gmail.com).\nproven to be effective in helping patients suffering from\nstrokes recover [8], [9]. Obviously, these BCI scenarios and\napplicable groups all require sufﬁcient stability and reliability,\nwhich means that the decoding of brain signals should be\naccurate and robust.\nResearchers have tried a lot of decoding methods to classify\ndifferent EEG signals. The two main concerns are classiﬁca-\ntion and feature extraction. In the beginning, some traditional\nmachine learning methods such as linear discriminant anal-\nysis (LDA), support vector machine (SVM), are applied to\nanalyze feature distribution and ﬁnd a projection or hyper-\nplane to separate different categories [10], [11]. From another\nperspective, multi-layer perceptron (MLP) is also used to ﬁt\nthe input-output relationship with several hidden layers. The\ninput is EEG signal, and the output is the category label\n[12]. Although these methods are quite efﬁcient, with the\nadvancement of acquisition equipment, the large-scale data we\nobtain is difﬁcult to handle properly. The ensuring amounts of\nirrelevant noise also challenge their generalization ability [13].\nSubsequently, deep learning methods have achieved re-\nmarkable results in computer vision and neural language\nprocessing. Features in multiple small ﬁelds are well perceived\nby the convolutional neural networks (CNNs) to obtain deep\nrepresentation for classiﬁcation. People also use CNNs in BCI\nto establish end-to-end EEG decoding models and achieve\nleading performance [14], [15]. Some skillful network struc-\ntures use different scales of convolution calculations to extract\nfeatures in various domains [16], [17]. However, CNN is very\ndependent on the selection of kernels. The large kernel hinders\nits exploration of deep feature, while the small kernel limits the\nreceptive ﬁeld [18]. It loses part of time-series information and\nis difﬁcult to perceive a wide range of internal relationships of\nthe signal without a fairly deep structure, which may cause a\nlot of computations. Various recurrent neural networks (RNNs)\nare proposed to learn the temporal feature of EEG signal and\nhave gained advantages in some scenarios [19]. For instance,\nlong short-term memory networks (LSTMs) employ gates for\ncontrolling the state of information ﬂow to learn long-term\ndependencies of EEG signal. Nevertheless, these methods are\nstill not enough to deal with more extended data, and less\nefﬁcient due to RNN steps cannot be parallelized [20]. Another\nproblem is RNNs only act on the previous memory and current\nstate. But we know that the action of a trial of EEG is\ncoherent. In other words, every part of the action is related,\nincluding past and present, present and future [21]. In this\ncase, neither CNNs nor RNNs are enough to perceive global\ndependencies of EEG well. Recently, there is a method called\nself-attention applied in machine translation, which calculates\narXiv:2106.11170v1  [eess.SP]  11 Jun 2021\n2\nthe representation of a sequence with dependencies between\ndifferent positions [22]. It seems that the attention mechanism\nmight help us decode EEG signals more reasonably.\nAnother concern that cannot be ignored is feature extraction.\nEEG is a non-invasive brain data acquisition mode, which\nintroduces a lot of noise caused by large impedance, ﬁeld\npotential interference, and so on [23]. Even those excellent\nend-to-end classiﬁcation methods are difﬁcult to obtain good\nperformance without well-designed modules for feature ex-\ntraction directly. Therefore, people are also trying to ﬁnd\nmore distinguishing features to facilitate EEG decoding. Fast\nFourier transform (FFT) is used to convert the original EEG\nsignal to a frequency representation, and continuous wavelet\ntransform (CWT) is used for time-frequency features [24],\n[25]. Among these strategies, common spatial pattern (CSP),\nwhich focus on spatial features, is the most widely recognized\n[26]. In this way, the original signal is ﬁltered to a new space\nwith larger differences between two categories. Earlier, small-\nscale feature vectors are calculated for classiﬁcation, and later\nthe ﬁltered signal with temporal feature retained is employed\nas the input of CNN and other networks [27]. But there is\noften a shortcoming that for multi-classiﬁcation tasks, these\nmethods simply stack feature channels obtained by multiple\nresults of one-versus-rest (OVR) processing, and then use a\nconvolution kernel with a size corresponding to the feature\nchannels to handle these spatial features. This integration\nstrategy largely neglects the importance of different feature\nchannels, so collaborative optimization is not well performed.\nEspecially for some data with minor category differences, it\nmay causes serious mutual interference.\nTo address the above problems, in this paper, we apply\nthe attention mechanism to construct a tiny transformer for\nEEG decoding, which means mainly depends on attention to\ntransform the input into a more distinguishable representation.\nThis model is called Spatial-Temporal Tiny Transformer (S3T)\nbecause it focuses on capturing spatial and temporal features.\nFirstly, the original EEG signal is ﬁltered following the idea\nof CSP, and the outputs of multiple OVRs are stacked to\nform the input of S3T. Secondly, a feature-channel attention\nblock gives weights to different channels so that the model\ncould selectively pay attention to more relevant channels and\nignore others. After that, all channels are compressed to reduce\ncomputation cost. Thirdly, the data is divided into many small\nslices in the time dimension. The attention mechanism is used\nto obtain a representation suitable for classiﬁcation, by per-\nceiving global temporal features. Finally, we get the category\nlabel with a simple fully-connected layer after global average\npooling. Besides, we innovatively use a convolution-based\nlayer to retain the position information of the sample. During\ntraining, only cross-entropy is employed as the constrain for\noptimization.\nThe major contributions of this article can be summarized\nas follows.\n1) We propose a tiny framework named S3T for EEG\ndecoding that mainly relies on the attention mechanism to\nlearn the spatial and temporal features of EEG signals. It has\nthe potential as a new backbone to classify EEG, like CNNs.\n2) We present a strategy to weight feature channels, thereby\nimproving the limitation of previous methods that neglect the\nimportance of different feature channels.\n3) Detailed experiments on public datasets prove the com-\npetitiveness of our method, reaching the level of the state-of-\nthe-art with fewer parameters.\nThe remainder of this paper is organized as follows. Some\nrelated works are given in Section II. Details of our framework\nS3T is explained in Section III. We introduce the datasets, ex-\nperiment settings, performance evaluation, comparative results\nin Section IV . A careful discussion is in Section V . Finally, we\ncome to a conclusion in Section VI.\nII. R ELATED WORKS\nThe development of EEG decoding methods could be sum-\nmarized in two stages, the ﬁrst of which is to ﬁnd some\nrepresentative patterns and use classical machine learning for\nclassiﬁcation. We will not go into details since the previous\narticles already have pretty good descriptions [28], [29]. And it\ncomes to the second stage when the development of hardware\npromotes deep learning. The CNN-based models have a great\nability to perceive feature dependence within a speciﬁc range\nand deepen it layer by layer. Some methods have obtained\ngood EEG decoding results with the help of CNNs. Sakhavi\net al. spatially ﬁltered EEG signal and sampled it in time\nto get the input of a CNN, where two-scale kernels were\nused to capture information in different domains [30]. Zhao\net al. transformed the data into a 3-dimensional (3D) form\nas the spatial distribution of the channels to cooperate with\na 3D CNN to better use the original characteristics [31].\nSome special objective functions are also used to improve\nthe difference of different categories [32]. We can see that\ndifferent kernels have an essential impact on the learning of\nthe features. And a kernel with a limited length is not easy to\nmeasure the relationship between long-distance features, even\nif there are many layers. Moreover, the amount of parameters\nto be calculated is usually considerable. On the other hand,\nsome RNN-based methods also yield good results leveraging\ntime-series information. Wang et al. used it to deal with a one\ndimension-aggregate approximation representation of the EEG\nsignal for MI classiﬁcation [33]. Ma et al. employed LSTM\nto learn the spatial features and temporal features separately\n[34]. Zhang et al. tried to concatenate CNN and LSTM to fuse\nthe advantage of the two [35].\nIn recent year, new methods different from CNNs and\nRNNs called attention mechanism has appeared in some ﬁelds.\nVaswani et al. creatively constructed an encoder and decoder\nmainly composed of attention modules to convert a sequence\nof sentences in one language to another [22]. This strategy\ncontains this idea shows a great potential to dig out more\nexpressive global dependencies. Dosovitskiy et al. divided the\npicture into multiple patches and used attention modules to\nobtain a token related to category information [36]. Some\npeople have also partly introduced the attention mechanism to\nEEG decoding. Zheng et al. replaced the forget gate of LSTM\nand weighted the output with attention modules [37]. Tao et\nal. integrated attention with CNN and RNN to explore more\ndiscriminative information of EEG for emotion recognition\n3\n[38]. Zhang et al. respectively processed the sliced EEG signal\nand a graph with CNN and LSTM then utilized attention\nmodule to synthesize all slices for classiﬁcation [39], [40].\nThese remarkable methods have demonstrated some capability\nof the attention mechanism. Therefore, we try to further\nexplore its ability to perceive features and use it as a trunk\nto construct S3T.\nIII. M ETHODS\nIn this paper, we propose an EEG decoding method named\nSpatial-Temporal Tiny Transformer (S3T). Just like a Rubik’s\nCube, S3T relies on the attention mechanism to transform the\ndata into a highly distinguishing representation with the help of\nspatial and temporal information. The global dependencies of\nthe signal and the importance of different feature channels are\nwell perceived for classiﬁcation tasks. The cost effectiveness\nof this method is also worth noting.\nThe overall framework is shown in Fig. 1, including four\nparts. The original EEG signal is ﬁrstly preprocessed, and the\nspatial ﬁlter is calculated to improve the difference between\ndifferent categories. Then in the spatial transforming part,\nwe apply the feature-channel attention to weight the feature-\nchannels so that the model is able to focus on more relevant\nchannels and ignore irrelevant ones. Thirdly, in the temporal\ntransforming part, we use a layer of convolution to encode the\nposition information in the time domain. After that, the data\nis compressed in a single channel to reduce computation cost.\nMeanwhile, it is divided into multiple small slices. The same\nsize representation is inputted to the classiﬁer after learning\nthe global dependencies with temporal attention. Finally, all\nthe slices are merged by global average pooling, and the result\nis obtained through a simple fully connected structure. Next,\nwe will present the details of the four parts in S3T.\nA. Preprocessing\nThe preprocessing of raw EEG data includes segmenting,\nband-pass ﬁltering, and standardization. Some approaches of\ncalibration and artifact removal have been omitted to reduce\nexcessive manual work.\nThe collected data is segmented into trials according to a\nwhole MI process. The shape of a trial is expressed as Ceeg ×\nT, where Ceeg is the number of EEG channels and T is the\nsample points. Then we band-pass ﬁlter the data to [4, 40]\nHz to remove high and low-frequency noise, during which the\nsensorimotor rhythms is disentangled, retaining µand β band\n[41]. The z-score standardization is employed to relieve the\nﬂuctuation and nonstationarity as\nX = x−µ√\nσ2 (1)\nwhere X ∈ RCeeg×T and x ∈ RCeeg×T represent the\nstandardized and input signal. µand σ2 denote the mean value\nand variance of training data. After standardization, signals\nbecome normally distributed with a mean of 0 and a standard\ndeviation of 1. The mean and variance will be used directly\nin the test.\nB. Spatial Filter\nIn this section, we give a feasible way based on CSP usage\n[26] to improve the spatial difference of the original signal\nand maintain the temporal information. Besides, we introduce\na one-versus-rest (OVR) strategy to handle multi-classiﬁcation\ntasks, since the traditional CSP is suitable for only two\ncategories. OVR refers to dividing a multi-classiﬁcation into\nN bi-classiﬁcation tasks of one category and the remaining\ncategory, where N equals the number of categories. The sub-\nﬁlters gained by each OVR are combined to form the spatial\nﬁlter at last.\nIn one OVR, we calculate the covariance matrix of each trial\nas C(X), where C() denotes calculate the covariance matrix.\nX ∈RCeeg×T is the preprocessed EEG data.\nAfter that, we average all the covariance matrix of one\ncategory to obtain R1 ∈RCeeg×Ceeg of the ’one’ and R2 ∈\nRCeeg×Ceeg of the ’rest’. In this way, spatial characteristics\nof the two categories are depicted because each position of\nthe covariance matrix measures the jointly variability of two\nchannels. Afterward, a common space R ∈ RCeeg×Ceeg is\nobtained by\nR= R1 + R2 (2)\nAnd eigendecomposition is performed as\nR= UΛUT (3)\nwhere U and Λ denote the eigenvectors and the eigenvalues,\nrespectively. Λ is arranged in descending order. Then the\nwhitening matrix P of R is obtained:\nP =\n√\nΛ−1UT (4)\nApplying the matrix P to R1 and R2, we get\nS1 = PR1PT\nS2 = PR2PT\n(5)\nThe orthogonal diagonalization of S2 is obtained as\nS2 = BΛSBT (6)\nwhere B and ΛS are the eigenvectors and eigenvalues of S1.\nΛS is arranged in ascending order. Because of the orthogonal-\nity, there is\nBT B = I (7)\nI = BT √\nΛ−1UT UΛUT (\n√\nΛ−1UT )T B\n= BT PRPT B\n= BT PR1PT B+ BT PR2PT B\n= BT PR1PT B+ ΛS\n(8)\nwhere BT PR1PT B is a diagonal matrix, denoted as Λ′\nS with\na value of I−ΛS, which means that BT P diagonalizes both\nR1 and R2. So the values of Λ′\nS become larger when the\nvalues of ΛS becomes smaller. BT P is treated as a spatial\nﬁlter, with which the difference between the ’one’ and the\n’rest’ is maximized.\nWe obtain N outputs here, and the ﬁrst S rows of each\nare taken out as a sub-ﬁlter corresponding to the largest four\n4\nFig. 1. The overall framework of Spatial-Temporal Tiny Transformer (S3T). After preprocessing and spatial ﬁlter, the method contains spatial transforming\nand temporal transforming built with the attention mechanism. There is a simple classiﬁer consisting of global average pooling and a fully-connected layer.\neigenvalues of Λ′\nS, to reduce the computational complexity.\nThen the data is ﬁltered as\nZ = WX (9)\nwhere Z is the ﬁltered data and X is the preprocessed data.\nW ∈RNS×Ceeg is the ﬁnal spatial ﬁlter made by stacking N\nsub-ﬁlters.\nC. Spatial Transforming\nIn previous methods, people rarely paid attention to the im-\nportance of different feature channels, leading to inefﬁciency\nand mutual interference of features. Therefore, we present a\nmethod of feature channel weighting inspired by scaled dot-\nproduct attention [22]. The dependence of each element on\nother elements is learned to obtain the importance score for\nweighting the values of data. Speciﬁcally, we use dot product\nto evaluate the correlation between one feature channel and\nothers as Fig. 2. The input data is ﬁrst linearly transformed into\nvectors, queries (Q) and keys (K) of dimension dk, and values\n(V) of dimension dv, along the spatial feature dimension. Q\nrepresents each channel that will be used to match with K\nrepresents all the other channels using dot product. Then the\nresult is divided by a scaling factor of √dk to ensure that the\nSoftmax function has a good perception ability. The output\nweight score is assigned to V for the ﬁnal representation using\ndot product. The whole process can be expressed as\nAttention(Q,K,V ) = Softmax(QKT\ndk\n)V (10)\nwhere Attention(Q,K,V ) is the weighted representation. Q,\nK and V are matrices packed by vectors for simultaneous\ncalculation. Besides, the residual connection of the input and\nthe output is used to help gradients of the framework ﬂow\n[42].\nD. Temporal Transforming\nHere is the most important part of this article, that is,\nperceiving global temporal dependencies of EEG signals using\nFig. 2. The calculation process of spatial feature-channel attention.\nthe attention mechanism. We know that a behavior driven by\nthe brain is a complete process, so our method could be more\neffective to utilize the relationship between any two parts\nof a trial. In the beginning, the data is compressed to one\ndimension ( 1 ×T) to reduce the computational complexity,\nsince the feature channels have been weighted in the previous\nstep. Through the observation of EEG signal, a segment\nreﬂects the trend better than a single sample point, which is\nmore likely to have abnormal values. Therefore, we divide\nthe data into multiple slices with a shape of 1 ×d for\nattention training. Different from spatial transforming, multi-\nhead attention (MHA) [22] is employed to allow the model\nto learn the dependencies from different angles. In this way,\nthe input is split into hsmaller parts named heads, which will\nperform attention in parallel. And the outputs of each part are\nconcatenated and linearly transformed to obtain the original\nsize. The process can be expressed as\nMHA(XQ,XK,XV ) = [head0; ...; headh−1]Wo (11)\nheadi = Attention(XQWQ\ni ,XKWK\ni ,XV WV\ni ) (12)\nwhere [.; .] represents a concatenation operation. WQ\ni ∈\nRd×\ndk\nh , WK\ni ∈ Rd×\ndk\nh and WV\ni ∈ Rd×dv\nh denote linear\n5\ntransformations to obtain queries, keys and values matrices\nof each head. Wo ∈Rdv×d denotes the linear transformation\nto obtain the ﬁnal output. A feed-forward (FF) block contains\ntwo fully-connected layers and the activation function GeLU\n[43] is connected behind the MHA, to enhance the perception\nand non-linear learning capabilities of the model. The input\nand output sizes of the FF block are the same, and the inner\nsize is expanded to Nf times. Layer normalization [44] is set\nbefore MHA and FF block. The residual connection is also\nused for better training. The module with MHA and FF block\nis repeated Na = 3 times for a ensemble effect. Although\nthe temporal transforming measure the dependencies between\ndifferent slices well, it ignores the position information, which\nis the sequence relationship between the EEG sample points.\nWe creatively use a convolutional layer on time dimension\nwith a kernel size of kc and stride of 1 to encode position\ninformation before compressing and slicing.\nE. Classiﬁer\nAfter the above process, the data is transformed into a\nnew representation by learning the spatial and temporal de-\npendencies. Now we just need to apply a global pooling to\naverage all the slices in the temporal transforming part. And\nthe pooling result is connected to a fully-connected layer after\nlayer normalization. The number of output neurons is equal to\nthe number of categories. Then, the Softmax function is used\nto obtain the predicted probability. The objective function is\nthe classiﬁcation loss achieved by cross-entropy as\nL= −1\nM\nN∑\nn=1\nM∑\nm=1\nyn\nmlog( ˆym\nn) (13)\nwhere M is the number of trials and N is the number of\ncategories. ym\nn denotes the real label for the m-th trial, and\nˆym\nn represents the predicted probability of the m-th trial for\nthe category n.\nIV. E XPERIMENTS AND RESULTS\nA. Datasets\nTo evaluate the proposed S3T model, we conducted detailed\nexperiments on two public datasets, datasets 2a and 2b of BCI\ncompetition IV .\n1) Datasets 2a of BCI Competition IV: The datasets [45]\nprovided by Graz University of Technology were used for our\nexperimental study. The MI EEG data of nine subjects were\nacquired with twenty-two Ag/AgCl electrodes at a sampling\nrate of 250 Hz, and band-pass ﬁltered between 0.5 Hz and\n100 Hz. There were four different tasks, including imagination\nof moving left hand, right hand, both feet, and tongue. Two\nsessions of data were collected for each subject, and each\nsession contained 288 trials (72 for each task). The temporal\nsegment of [2, 6] second was used in our experiments.\n2) Datasets 2b of BCI Competition IV: The datasets [46]\nrecorded the MI data of nine subjects on three bipolar elec-\ntrodes. The data were band-pass ﬁltered between 0.5 Hz and\n100 Hz with a sampling rate of 250 Hz. The imagination\nof moving left hand and right hand was applied to be the\nparadigm. Two sessions without visual feedback and three\nsessions with visual feedback were obtained for every subject.\nWe used 120 trials in each session, due to the number of trials\nfor several sessions was slightly different. The segment of [3,\n7] second was maintained as one trial in the experiments.\nB. Experiment details\nOur method was implemented with Python 3.6 and PyTorch\nlibrary on a Geforce 2080Ti GPU. The electrooculogram\nchannels of the data were removed directly, and there was\nno additional artifact removal except for band-pass ﬁltering.\nWe trained subject-speciﬁc models according to the usual\nimplementation. Ten-fold cross-validation was used to evaluate\nthe ﬁnal results. The slice size, h, kc, and Nf were 10, 5, 51\nand 4, respectively. For the ﬁrst datasets, category number N\nwas 4, and the row number S applied in the spatial ﬁlter was\n4. In the second dataset, there was N = 2 and just one sub-\nﬁlter was obtained with S = 3. In the model, Adam [47], with\na learning rate of 0.0002, was utilized to optimize the training\nprocess. β1, β2, and batch size were 0.5, 0.9, 50 separately. We\nintroduced a dropout of 0.5 in temporal transforming and 0.3\nin spatial transforming to avoid overﬁtting. Wilcoxon Signed-\nRank Test was used for statistical analysis.\nC. Scoring performance\nFig. 3. The classiﬁcation results on BCI competition IV datasets 2a and BCI\ncompetition IV datasets 2b shown in confusion matrices.\nTABLE I\nSCORING PERFORMANCE OF S3T.\nlabel Accuracy Precision Recall Speciﬁcity F-score\n2a\nc0 91.30 83.33 75.60 95.72 79.28\nc1 91.48 81.88 84.33 93.84 83.09\nc2 92.03 87.84 83.87 95.32 85.81\nc3 90.37 77.40 85.61 91.91 81.30\n2b c0 84.26 83.09 85.87 82.66 84.46\nc1 84.26 85.50 82.66 85.87 84.06\nIn the beginning, we give an overview with several metrics\nto show the performance of S3T. Confusion matrices of the\nclassiﬁcation results on two datasets are displayed in Fig. 3,\nwhere the horizontal axis represents the predicted label and\nthe vertical axis represents the true label. We further evaluate\nthe method with ﬁve indicators for each category, accuracy,\nprecision, recall, speciﬁcity, and F-score, as given in Table I.\n6\nThese matrices are calculated in OVR mode, considering one\ncategory as the positive and the other as the negative. F-score\ncould be calculated as\nFscore = (2 ×Recall)/(Precision + Recall) (14)\nIt can be seen that the overall performance of our method\nis trustworthy, with good classiﬁcation ability for different\ncategories and not much bias.\nD. Statistical Analysis with Baselines\nThen we conduct comparative experiments and perform\nsigniﬁcance analysis with some recent representative baselines.\nBrief descriptions are given as follows.\n• FBCSP [48] is one of the most widely accepted methods\nthat applies the ﬁlter bank strategy to cooperate with CSP.\n• ConvNet [49] presents a remarkable demonstration to use\nCNN-based deep learning models for EEG decoding.\n• EEGNet [50] designs a compact and practical CNN with\ndepthwise and separable convolutions to classify EEG.\n• C2CM [30] introduces a temporal representation with\nCSP and utilize a CNN architecture for classiﬁcation.\n• CNN+LSTM [51] segments the original signal for OVR-\nCSP processing and extracts features with CNN, and the\noutput is passed through an LSTM for temporal features.\n• DFL [32] uses a well-designed feature separation strategy\nin CNN to improve EEG decoding performance.\nThe accuracy of each subject, average accuracy, standard\ndeviation (std), parameter amount (Params) on two datasets are\nlisted in Table II. P-values are used to measure the signiﬁcance\nof our method. We can see that the accuracy of our S3T relies\non attention mechanism has obvious superiority with a limited\namount of parameters on the two datasets, and the std is at a\nlow level, which means that the method is effective and robust\nfor EEG decoding.\nTo be speciﬁc, in datasets 2a, FBCSP makes good use of\nCSP, but the accuracy is lower because it compresses the\ntime domain and uses traditional machine learning classiﬁers.\nConvNet and EEGNet show the power of CNN-based deep\nnetworks in EEG classiﬁcation. Although the convolutions of\ndifferent scales are used dexterously to extract spatial domain\nand temporal domain features, the performance is still inef-\nﬁcient due to the lack of perception of global dependencies.\nC2CM combines the advantages of CSP and CNNs but has\nsimilar limitations. CNN+LSTM further introduces LSTM to\ndeal with temporal dependencies, and DFL employs additional\nobjective functions to expand feature differences. However,\nexcept for the weak results on Subject 05, our method still has\na signiﬁcant improvement than CNN+LSTM ( p = 0.0210) and\nDFL (p = 0.0104). Another important issue is that the methods\nthat rely on CNNs usually have a large number of parameters,\nsuch as ConvNet, C2CM, and DFL. Fewer parameters of\nour method reduce the computational complexity and cost to\na large extent. EEGNet has few parameters but insufﬁcient\nperformance. The comparison in datasets 2b also shows a\nconsistent trend, and the std of our method is lower than all\nothers.\nThe above results illustrate that our method has quite great\nperformance and is more cost-effective.\nE. Ablating Study\nFig. 4. The results of the ablation study on datasets 2a.\nThe core idea of the proposed S3T method is to transform\nthe EEG signal into a distinguishable representation by cap-\nturing spatial and temporal features. Therefore, we present an\nablation study on BCI competition datasets 2a to show the\neffectiveness of each part in S3T as Fig. 4.\nWe ﬁrst remove the temporal transforming and spatial\ntransforming module from the complete S3T, respectively. It\ncan be seen from Fig. 4 that the results are in line with our\nexpectations. Temporal transforming is the most important in\nthe model, and its absence makes the mean accuracy have\na signiﬁcant drop of 35.93%. The contribution of spatial\ntransforming has also been proven to be sufﬁcient, with the\nmean accuracy dropping 3.33% when we remove it. Besides,\nspatial transforming does have a distinct improvement for the\ndata with poor discrimination, such as subject 5 and 6.\nBesides, we employ a convolutional layer as a position\nencoding block and a feed forward (FF) block after each MHA\nin the temporal transforming module. So these two blocks are\ndiscarded separately to test their contribution to the model.\nAs shown in Fig. 4, position encoding largely compensates\nfor the use of only attention mechanism with an improvement\nof 11.11%. It also demonstrates that position information is\nvaluable for EEG decoding. FF blocks also give the model\na more robust learning capability like spatial transforming.\nAnd the mean accuracy increases by 2.22% after adding FF\nblocks. The above experiment explains that the main parts in\nS3T have the evident effect, where the temporal transforming\nis the trunk.\nF . Parameter Sensitivity\nIt should be noted that two parameters may affect the\nperformance of the model. They are the kernel size of position\nencoding and slice size in temporal transforming. We perform\nsensitivity analysis by varying these two parameters in a wide\nrange and testing the impact on classiﬁcation accuracy. The\ncomparison results on the data of the ﬁrst subject in two\ndatasets are depicted in Fig. 5, separately. We can see that\nthe results drop sharply when the slice size is small. In fact,\nwe choose to slice, on the one hand, to reduce computation;\nanother more important reason is to conduct attention on many\nshort sequences that better reﬂect the state of the signal, just\n7\nTABLE II\nCOMPARISON WITH REPRESENTATIVE METHODS ON DATASETS 2A AND 2B OF BCI COMPETITION IV.\ndatasets methods S01 S02 S03 S04 S05 S06 S07 S08 S09 Average std signiﬁcance Params\n2a\nFBCSP [48] 76.00 56.50 81.25 61.00 55.00 45.25 82.75 81.25 70.75 67.75 12.94 p < 0.01 —\nConvNet [49] 76.39 55.21 89.24 74.65 56.94 54.17 92.71 77.08 76.39 72.53 13.42 p < 0.01 295.25k\nEEGNet [50] 85.76 61.46 88.54 67.01 55.90 52.08 89.58 83.33 86.81 74.50 14.36 p < 0.01 1.46k\nC2CM [30] 87.50 65.28 90.28 66.67 62.5 45.49 89.58 83.33 79.51 74.46 14.45 p < 0.01 36.68k\nCNN+LSTM [51] 85.00 54.00 87.00 78.00 77.00 66.00 95.00 83.00 90.00 80.00 11.97 p = 0.0961 8.57k\nDFL [32] 91.31 71.62 92.32 78.38 80.10 61.62 92.63 90.30 78.38 81.85 10.15 p = 0.0774 30.69k\nOurs 91.67 71.67 95.00 78.33 61.67 66.67 96.67 93.33 88.33 82.59 12.52 — 8.68k\n2b\nFBCSP [48] 70.00 60.36 60.94 97.50 93.12 80.63 78.13 92.50 86.88 80.00 13.06 p < 0.05 —\nConvNet [49] 76.56 50.00 51.56 96.88 93.13 85.31 83.75 91.56 85.62 79.37 16.27 p < 0.05 295.23k\nEEGNet [50] 68.44 57.86 61.25 90.63 80.94 63.13 84.38 93.13 83.13 75.88 12.57 p < 0.01 1.15k\nMSCNN [52] 80.56 65.44 65.97 99.32 89.19 86.11 81.25 88.82 86.81 82.61 10.44 p < 0.05 24.99k\nOurs 81.67 68.33 66.67 98.33 88.33 90.00 85.00 93.33 86.67 84.26 10.03 – 6.50k\nFig. 5. The results of parameter sensitivity tests. The position encoding kernel\nand the slice size are independently changed to test the performance of our\nmodel on the ﬁrst subject of datasets 2a and datasets 2b.\nlike CNNs do. Smaller slices are also more susceptible to some\nnoise or outliers.\nFor position encoding, the smaller kernel size reduces\nthe performance expressly. We introduce position encoding\nto improve the perception of sequence position information,\nbecause the attention mechanism directly focuses on global\ndependencies. Therefore, when the kernel size is too small,\nit is hard to capture valuable information in a wide enough\nrange.\nAfter the position kernel and slice size taking larger values,\nthe result ﬂuctuates slightly. So our method is conﬁrmed to be\nstable enough when the parameters change.\nG. Interpretability and Visualization\nThe learning process is visualized with t-SNE [53] to\ninterpret the S3T model further. We ﬁrst train the model to be\nstable and then visualize the data of one epoch in four stages,\nafter spatial transforming and after each of the three temporal\ntransforming layers as Fig. 6. The data used to display is\nfrom subject 1 in BCI competition IV datasets 2a. The four\ncolors in the ﬁgure represent four categories of EEG. Fig.\n6(a) is the output of the spatial transforming. We can see\nthat the four categories have been mildly distinguished, but\nnot very obvious, and there are two categories completely\nmixed together. According to Fig. 6(b)-(d), after each layer\nof temporal transforming is processed, the distinction among\nthe four categories gradually becomes larger, until these data\ncan be clearly distinguished in the end.\nV. D ISCUSSION\nBetter decoding is a necessary issue to improve the ap-\nplication of BCI. In recent years, CNN-based deep learning\nmethods plays a dominant role in EEG analysis. But CNNs\nhave a limitation on perceiving global dependencies of long\nsequences. Especially for EEG signal like motor or motor\nimagery, a trial is inseparable. Therefore, we propose a method\nthat uses attention mechanism to deal with global dependen-\ncies of EEG signal. Several layers of attention operations\ntransform the original input into a distinguishable represen-\ntation for classiﬁcation. Besides, we also perform attention on\nthe feature channels of spatially ﬁltered EEG, which helps us\navoid aliasing of channels. After detailed experiments, we ﬁnd\nthat this transforming idea has the potential to compete with\nusual CNN and LSTM models in EEG decoding.\nAccording to the ablation study in Fig. 4, temporal trans-\nforming is the backbone of our method. We slice the data\ninto many small segments for attention, which is inspired by\nthe method in computer vision to divide the image into many\npatches [36]. Refer to Fig. 5, this measure is more effective and\nwith lower computation cost than directly processing sample\npoints. The inﬂuence of some outliers is alleviated by observ-\ning a few continuous sample points. In this way, after several\nrounds of attention weighting, the model focuses on slices that\nare more related to category information with the constraints\nof only classiﬁcation loss. So the ﬁnal representation is easily\nseparated as Fig. 6(d).\nIt should be noted that we don’t completely abandon con-\nvolution as ”attention is all you need” [22]. Although the\nattention has well perceived the global dependencies of the\nEEG signal, the position encoding realized by a layer of\nconvolution still plays an important role, as shown in Fig.\n4. The application of attention by slicing ignores the sequence\nrelationship of the signal to a certain extent, and the convolu-\ntion of a nearby area just makes up for this problem. Besides\nremoving the convolution in ablation study, we can also see\n8\nFig. 6. Visualization with t-SNE. (a) Data distribution after spatial transforming. (b) Data distribution after the ﬁrst layer of temporal transforming. (c) Data\ndistribution after the second layer of temporal transforming. (d) Data distribution after the third layer of temporal transforming, also the input of the classiﬁer.\nfrom Fig. 5 that too small convolution kernel also reduces the\nperformance, proving that the position encoding implemented\nin this way is effective. New methods appear every day, and\nwe are not going to dismiss the previous methods but try to\ncombine the advantages of different strategies.\nIn temporal transforming, we compress the signal from the\nfeature-channel dimension in 1 channel to reduce calculation.\nThis is because the feature-channels have been weighted in\nspatial transforming with attention mechanism, inspired by\n[54]. One explanation for its meaning is that OVR strategy\nand spatial ﬁlter are employed to process the original EEG\nsignal like [30]. In this case, stacked feature-channels obtained\nfrom different subspaces may cause interference. So spatial\ntransforming in advance could increase the model’s attention\nto more relevant channels. Experiments display that it does\nenhance the overall performance, especially with a signiﬁcant\nimprovement on ’A05’ and ’A06’ in Fig. 4, whose data are\nnot easily distinguishable.\nThe scale of our model is an issue that cannot be ignored.\nSome methods have achieved impressive results with a large\nnumber of parameters, but require considerable computing\npower and training time. So we strive to control the number\nof parameters of S3T at a relatively low level, on the premise\nof good accuracy. This mainly due to two reasons. One is\nthat we compress the feature-channels. An operation with the\nsimilar idea is also used after temporal transforming, which is\nto average all slices as pooling as [55], instead of using some\nfully-connected layers as usual. This trick could be used with\na small-size fully-connected layer to classify, own to the good\nrepresentation provided by spatial and temporal transforming.\nAlthough S3T achieves very nice performance on EEG\ndecoding tasks, the limitation of this work should also be\nconsidered. Only subject-speciﬁc experiments are conducted\nas the common way, and the cross-subject ability has not\nbeen well explored. Another limitation is that we choose the\nhyper-parameters of the model, such as dropout and batch size,\njust by some preliminary experiments, so the model may not\nreach the optimal state. In future work, we will verify the\ngeneralization ability of the method with cross-subject tests,\nand try to use some data augmentation strategies to improve\nthe performance.\nVI. C ONCLUSION\nIn this paper, we propose S3T, a tiny method relies on atten-\ntion mechanism to perceive the spatial and temporal features\nof EEG signal for decoding tasks. Firstly, we use attention to\nweight the preprocessed and spatially ﬁltered data along the\nfeature-channel dimension, enhancing more relevant channels.\nIt is also employed along the time dimension to perceive\nglobal dependencies. Experiments show that our method is\nrobust enough to classify multiple categories of EEG, reaching\nthe state-of-the-art level with much fewer parameters. The\nstability of the method and the effectiveness of each module\nhave also been proven. We further utilize visualization tools to\nconﬁrm that it is meaningful to transform data through spatial\nand temporal features learning. In summary, our method has\ngood potential to complement commonly used deep learning\nmethods for EEG decoding.\nACKNOWLEDGMENT\nThis work was supported in part by the National Nat-\nural Science Foundation of China (Grant No. 52075177),\nJoint Fund of the Ministry of Education for Equipment Pre-\nResearch (Grant No. 6141A02033124), Research Foundation\nof Guangdong Province (Grant No. 2019A050505001 and\n2018KZDXM002), Guangzhou Research Foundation (Grant\nNo. 202002030324 and 201903010028), Zhongshan Research\nFoundation (Grant No.2020B2020), and Shenzhen Institute of\nArtiﬁcial Intelligence and Robotics for Society (Grant No.\nAC01202005011).\nREFERENCES\n[1] F. R. Willett, D. T. Avansino, L. R. Hochberg, J. M. Henderson,\nand K. V . Shenoy, “High-performance brain-to-text communication via\nhandwriting,” Nature, vol. 593, no. 7858, pp. 249–254, 2021.\n[2] A. Cruz, G. Pires, A. Lopes, C. Carona, and U. J. Nunes, “A Self-Paced\nBCI With a Collaborative Controller for Highly Reliable Wheelchair\nDriving: Experimental Tests With Physically Disabled Individuals,”\nIEEE Transactions on Human-Machine Systems, vol. 51, no. 2, pp. 109–\n119, Apr. 2021.\n[3] A. Schwarz, M. K. H ¨oller, J. Pereira, P. Ofner, and G. R. M ¨uller-Putz,\n“Decoding hand movements from human EEG to control a robotic arm\nin a simulation environment,” Journal of Neural Engineering , vol. 17,\nno. 3, p. 036010, May 2020.\n[4] Y . Song, W. Wu, C. Lin, G. Lin, G. Li, and L. Xie, “Assistive Mobile\nRobot with Shared Control of Brain-Machine Interface and Computer\nVision,” in 2020 IEEE 4th Information Technology, Networking, Elec-\ntronic and Automation Control Conference (ITNEC) . Chongqing,\nChina: IEEE, Jun. 2020, pp. 405–409.\n9\n[5] E. Tidoni, M. Abu-Alqumsan, D. Leonardis, C. Kapeller, G. Fusco,\nC. Guger, C. Hintermuller, A. Peer, A. Frisoli, F. Tecchia, M. Bergam-\nasco, and S. M. Aglioti, “Local and Remote Cooperation With Virtual\nand Robotic Agents: A P300 BCI Study in Healthy and People Living\nWith Spinal Cord Injury,” IEEE Transactions on Neural Systems and\nRehabilitation Engineering, vol. 25, no. 9, pp. 1622–1632, Sep. 2017.\n[6] G. Pfurtscheller, C. Brunner, A. Schl ¨ogl, and F. Lopes da Silva,\n“Mu rhythm (de)synchronization and EEG single-trial classiﬁcation of\ndifferent motor imagery tasks,”NeuroImage, vol. 31, no. 1, pp. 153–159,\nMay 2006.\n[7] A. Al-Saegh, S. A. Dawwd, and J. M. Abdul-Jabbar, “Deep learning for\nmotor imagery EEG-based classiﬁcation: A review,” Biomedical Signal\nProcessing and Control, vol. 63, p. 102172, Jan. 2021.\n[8] R. Foong, K. K. Ang, C. Quek, C. Guan, K. S. Phua, C. W. K. Kuah,\nV . A. Deshmukh, L. H. L. Yam, D. K. Rajeswaran, N. Tang, E. Chew,\nand K. S. G. Chua, “Assessment of the Efﬁcacy of EEG-Based MI-\nBCI With Visual Feedback and EEG Correlates of Mental Fatigue for\nUpper-Limb Stroke Rehabilitation,” IEEE Transactions on Biomedical\nEngineering, vol. 67, no. 3, pp. 786–795, Mar. 2020.\n[9] R. Mane, T. Chouhan, and C. Guan, “BCI for stroke rehabilitation: motor\nand beyond,” Journal of Neural Engineering , vol. 17, no. 4, p. 041001,\nAug. 2020.\n[10] R. Fu, Y . Tian, T. Bao, Z. Meng, and P. Shi, “Improvement Motor\nImagery EEG Classiﬁcation Based on Regularized Linear Discriminant\nAnalysis,” Journal of Medical Systems, vol. 43, no. 6, p. 169, Jun. 2019.\n[11] Yinxia Liu, Weidong Zhou, Qi Yuan, and Shuangshuang Chen, “Auto-\nmatic Seizure Detection Using Wavelet Transform and SVM in Long-\nTerm Intracranial EEG,” IEEE Transactions on Neural Systems and\nRehabilitation Engineering, vol. 20, no. 6, pp. 749–755, Nov. 2012.\n[12] O. W. Samuel, Y . Geng, X. Li, and G. Li, “Towards Efﬁcient Decoding\nof Multiple Classes of Motor Imagery Limb Movements Based on EEG\nSpectral and Time Domain Descriptors,” Journal of Medical Systems ,\nvol. 41, no. 12, p. 194, Dec. 2017.\n[13] Jie Xu, Yuan Yan Tang, Bin Zou, Zongben Xu, Luoqing Li, and Yang\nLu, “The Generalization Ability of Online SVM Classiﬁcation Based\non Markov Sampling,” IEEE Transactions on Neural Networks and\nLearning Systems, vol. 26, no. 3, pp. 628–639, Mar. 2015.\n[14] C.-T. Lin, C.-H. Chuang, Y .-C. Hung, C.-N. Fang, D. Wu, and Y .-K.\nWang, “A Driving Performance Forecasting System Based on Brain\nDynamic State Analysis Using 4-D Convolutional Neural Networks,”\nIEEE Transactions on Cybernetics , pp. 1–9, 2020.\n[15] S. U. Amin, M. Alsulaiman, G. Muhammad, M. A. Mekhtiche, and\nM. Shamim Hossain, “Deep Learning for EEG motor imagery classi-\nﬁcation based on multi-layer CNNs feature fusion,” Future Generation\nComputer Systems, vol. 101, pp. 542–554, Dec. 2019.\n[16] G. Dai, J. Zhou, J. Huang, and N. Wang, “HS-CNN: a CNN with hybrid\nconvolution scale for EEG motor imagery classiﬁcation,” Journal of\nNeural Engineering, vol. 17, no. 1, p. 016025, Jan. 2020.\n[17] Y . Ding, N. Robinson, Q. Zeng, and C. Guan, “TSception: Capturing\nTemporal Dynamics and Spatial Asymmetry from EEG for Emotion\nRecognition,” arXiv:2104.02935 [cs] , Apr. 2021, arXiv: 2104.02935.\n[Online]. Available: http://arxiv.org/abs/2104.02935\n[18] J. He, L. Zhao, H. Yang, M. Zhang, and W. Li, “HSI-BERT: Hyperspec-\ntral Image Classiﬁcation Using the Bidirectional Encoder Representation\nFrom Transformers,” IEEE Transactions on Geoscience and Remote\nSensing, vol. 58, no. 1, pp. 165–178, Jan. 2020.\n[19] T. Zhang, W. Zheng, Z. Cui, Y . Zong, and Y . Li, “Spatial–Temporal\nRecurrent Neural Network for Emotion Recognition,”IEEE Transactions\non Cybernetics, vol. 49, no. 3, pp. 839–847, Mar. 2019.\n[20] N. Zhang, “Learning Adversarial Transformer for Symbolic Music\nGeneration,” IEEE Transactions on Neural Networks and Learning\nSystems, pp. 1–10, 2020.\n[21] C. S. Zandvoort, J. H. van Die ¨en, N. Dominici, and A. Daffertshofer,\n“The human sensorimotor cortex fosters muscle synergies through\ncortico-synergy coherence,”NeuroImage, vol. 199, pp. 30–37, Oct. 2019.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, L. Kaiser, and I. Polosukhin, “Attention Is All You Need,”\narXiv:1706.03762 [cs] , Dec. 2017, arXiv: 1706.03762. [Online].\nAvailable: http://arxiv.org/abs/1706.03762\n[23] S. K. Goh, H. A. Abbass, K. C. Tan, A. Al-Mamun, C. Wang, and\nC. Guan, “Automatic EEG Artifact Removal Techniques by Detecting\nInﬂuential Independent Components,” IEEE Transactions on Emerging\nTopics in Computational Intelligence , vol. 1, no. 4, pp. 270–279, Aug.\n2017.\n[24] M. Li and W. Chen, “FFT-based deep feature learning method for EEG\nclassiﬁcation,” Biomedical Signal Processing and Control , vol. 66, p.\n102492, Apr. 2021.\n[25] P. Kant, S. H. Laskar, J. Hazarika, and R. Mahamune, “CWT Based\nTransfer Learning for Motor Imagery Classiﬁcation for Brain computer\nInterfaces,” Journal of Neuroscience Methods, vol. 345, p. 108886, Nov.\n2020.\n[26] Kai Keng Ang, Zhang Yang Chin, Haihong Zhang, and Cuntai Guan,\n“Filter Bank Common Spatial Pattern (FBCSP) in Brain-Computer\nInterface,” in 2008 IEEE International Joint Conference on Neural\nNetworks (IEEE World Congress on Computational Intelligence). Hong\nKong, China: IEEE, Jun. 2008, pp. 2390–2397.\n[27] J. Chen, Z. Yu, Z. Gu, and Y . Li, “Deep Temporal-Spatial Feature\nLearning for Motor Imagery-Based Brain–Computer Interfaces,” IEEE\nTransactions on Neural Systems and Rehabilitation Engineering, vol. 28,\nno. 11, pp. 2356–2366, Nov. 2020.\n[28] F. Lotte, M. Congedo, A. L ´ecuyer, F. Lamarche, and B. Arnaldi,\n“A review of classiﬁcation algorithms for EEG-based brain–computer\ninterfaces,” Journal of Neural Engineering , vol. 4, no. 2, pp. R1–R13,\nJun. 2007.\n[29] F. Lotte, L. Bougrain, A. Cichocki, M. Clerc, M. Congedo, A. Rako-\ntomamonjy, and F. Yger, “A review of classiﬁcation algorithms for EEG-\nbased brain–computer interfaces: a 10 year update,” Journal of Neural\nEngineering, vol. 15, no. 3, p. 031005, Jun. 2018.\n[30] S. Sakhavi, C. Guan, and S. Yan, “Learning Temporal Information for\nBrain-Computer Interface Using Convolutional Neural Networks,” IEEE\nTransactions on Neural Networks and Learning Systems, vol. 29, no. 11,\npp. 5619–5629, Nov. 2018.\n[31] X. Zhao, H. Zhang, G. Zhu, F. You, S. Kuang, and L. Sun, “A\nMulti-Branch 3D Convolutional Neural Network for EEG-Based Motor\nImagery Classiﬁcation,” IEEE Transactions on Neural Systems and\nRehabilitation Engineering, vol. 27, no. 10, pp. 2164–2177, Oct. 2019.\n[32] L. Yang, Y . Song, K. Ma, and L. Xie, “Motor Imagery EEG Decoding\nMethod Based on a Discriminative Feature Learning Strategy,” IEEE\nTransactions on Neural Systems and Rehabilitation Engineering, vol. 29,\npp. 368–379, 2021.\n[33] P. Wang, A. Jiang, X. Liu, J. Shang, and L. Zhang, “LSTM-Based EEG\nClassiﬁcation in Motor Imagery Tasks,” IEEE Transactions on Neural\nSystems and Rehabilitation Engineering, vol. 26, no. 11, pp. 2086–2095,\nNov. 2018.\n[34] X. Ma, S. Qiu, C. Du, J. Xing, and H. He, “Improving EEG-Based\nMotor Imagery Classiﬁcation via Spatial and Temporal Recurrent Neural\nNetworks,” in 2018 40th Annual International Conference of the IEEE\nEngineering in Medicine and Biology Society (EMBC) . Honolulu, HI:\nIEEE, Jul. 2018, pp. 1903–1906.\n[35] R. Zhang, Q. Zong, L. Dou, X. Zhao, Y . Tang, and Z. Li, “Hybrid\ndeep neural network using transfer learning for EEG motor imagery\ndecoding,” Biomedical Signal Processing and Control , vol. 63, p.\n102144, Jan. 2021.\n[36] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly,\nJ. Uszkoreit, and N. Houlsby, “An Image is Worth 16x16 Words:\nTransformers for Image Recognition at Scale,” arXiv:2010.11929\n[cs], Oct. 2020, arXiv: 2010.11929. [Online]. Available: http:\n//arxiv.org/abs/2010.11929\n[37] X. Zheng and W. Chen, “An Attention-based Bi-LSTM Method for\nVisual Object Classiﬁcation via EEG,” Biomedical Signal Processing\nand Control, vol. 63, p. 102174, Jan. 2021.\n[38] W. Tao, C. Li, R. Song, J. Cheng, Y . Liu, F. Wan, and X. Chen,\n“EEG-based Emotion Recognition via Channel-wise Attention and Self\nAttention,” IEEE Transactions on Affective Computing , pp. 1–1, 2020.\n[39] D. Zhang, L. Yao, K. Chen, and J. Monaghan, “A Convolutional Re-\ncurrent Attention Model for Subject-Independent EEG Signal Analysis,”\nIEEE Signal Processing Letters, vol. 26, no. 5, pp. 715–719, May 2019.\n[40] D. Zhang, K. Chen, D. Jian, and L. Yao, “Motor Imagery Classiﬁcation\nvia Temporal Attention Cues of Graph Embedded EEG Signals,” IEEE\nJournal of Biomedical and Health Informatics , vol. 24, no. 9, pp. 2570–\n2579, Sep. 2020.\n[41] J. S. Kirar and R. K. Agrawal, “Relevant Frequency Band Selection\nusing Sequential Forward Feature Selection for Motor Imagery Brain\nComputer Interfaces,” in 2018 IEEE Symposium Series on Computa-\ntional Intelligence (SSCI) . Bangalore, India: IEEE, Nov. 2018, pp.\n52–59.\n[42] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning\nfor Image Recognition,” arXiv:1512.03385 [cs] , Dec. 2015, arXiv:\n1512.03385. [Online]. Available: http://arxiv.org/abs/1512.03385\n[43] D. Hendrycks and K. Gimpel, “Gaussian Error Linear Units (GELUs),”\narXiv:1606.08415 [cs] , Jul. 2020, arXiv: 1606.08415. [Online].\nAvailable: http://arxiv.org/abs/1606.08415\n10\n[44] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer Normalization,”\narXiv:1607.06450 [cs, stat] , Jul. 2016, arXiv: 1607.06450. [Online].\nAvailable: http://arxiv.org/abs/1607.06450\n[45] C. Brunner, R. Leeb, G. R. Muller-Putz, and A. Schlogl, “BCI Compe-\ntition 2008 – Graz data set A,” p. 6.\n[46] R. Leeb, C. Brunner, G. R. Muller-Putz, and A. Schlogl, “BCI Compe-\ntition 2008 – Graz data set B,” p. 6.\n[47] D. P. Kingma and J. Ba, “Adam: A Method for Stochastic Optimization,”\narXiv:1412.6980 [cs], Jan. 2017, arXiv: 1412.6980. [Online]. Available:\nhttp://arxiv.org/abs/1412.6980\n[48] K. K. Ang, Z. Y . Chin, C. Wang, C. Guan, and H. Zhang, “Filter Bank\nCommon Spatial Pattern Algorithm on BCI Competition IV Datasets 2a\nand 2b,” Frontiers in Neuroscience, vol. 6, 2012.\n[49] R. T. Schirrmeister, J. T. Springenberg, L. D. J. Fiederer, M. Glasstetter,\nK. Eggensperger, M. Tangermann, F. Hutter, W. Burgard, and T. Ball,\n“Deep learning with convolutional neural networks for EEG decoding\nand visualization: Convolutional Neural Networks in EEG Analysis,”\nHuman Brain Mapping , vol. 38, no. 11, pp. 5391–5420, Nov. 2017.\n[50] V . J. Lawhern, A. J. Solon, N. R. Waytowich, S. M. Gordon, C. P. Hung,\nand B. J. Lance, “EEGNet: a compact convolutional neural network for\nEEG-based brain–computer interfaces,” Journal of Neural Engineering ,\nvol. 15, no. 5, p. 056013, Oct. 2018.\n[51] R. Zhang, Q. Zong, L. Dou, and X. Zhao, “A novel hybrid deep learning\nscheme for four-class motor imagery classiﬁcation,” Journal of Neural\nEngineering, vol. 16, no. 6, p. 066004, Oct. 2019.\n[52] X. Tang, W. Li, X. Li, W. Ma, and X. Dang, “Motor imagery EEG\nrecognition based on conditional optimization empirical mode decom-\nposition and multi-scale convolutional neural network,” Expert Systems\nwith Applications, vol. 149, p. 113285, Jul. 2020.\n[53] L. van der Maaten and G. Hinton, “Visualizing data using t-sne,” Journal\nof Machine Learning Research , vol. 9, no. 86, pp. 2579–2605, 2008.\n[54] J. Hu, L. Shen, and G. Sun, “Squeeze-and-Excitation Networks,” in 2018\nIEEE/CVF Conference on Computer Vision and Pattern Recognition .\nSalt Lake City, UT: IEEE, Jun. 2018, pp. 7132–7141.\n[55] M. Lin, Q. Chen, and S. Yan, “Network In Network,” arXiv:1312.4400\n[cs], Mar. 2014, arXiv: 1312.4400. [Online]. Available: http://arxiv.org/\nabs/1312.4400",
  "topic": "Decoding methods",
  "concepts": [
    {
      "name": "Decoding methods",
      "score": 0.6482073664665222
    },
    {
      "name": "Transformer",
      "score": 0.5687938332557678
    },
    {
      "name": "Computer science",
      "score": 0.5509417057037354
    },
    {
      "name": "Electroencephalography",
      "score": 0.4586058557033539
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.43483343720436096
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.39445266127586365
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3909614682197571
    },
    {
      "name": "Speech recognition",
      "score": 0.3768211007118225
    },
    {
      "name": "Psychology",
      "score": 0.23953500390052795
    },
    {
      "name": "Engineering",
      "score": 0.1979789137840271
    },
    {
      "name": "Electrical engineering",
      "score": 0.17289263010025024
    },
    {
      "name": "Neuroscience",
      "score": 0.11308547854423523
    },
    {
      "name": "Telecommunications",
      "score": 0.09718465805053711
    },
    {
      "name": "Voltage",
      "score": 0.08409294486045837
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}