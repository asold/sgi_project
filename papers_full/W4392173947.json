{
  "title": "Optimal Semi-Fragile Watermarking Based on Maximum Entropy Random Walk and Swin Transformer for Tamper Localization",
  "url": "https://openalex.org/W4392173947",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4379217315",
      "name": "P. Aberna",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    },
    {
      "id": "https://openalex.org/A2462341815",
      "name": "L. Agilandeeswari",
      "affiliations": [
        "Vellore Institute of Technology University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3210301271",
    "https://openalex.org/W2797856256",
    "https://openalex.org/W3030928236",
    "https://openalex.org/W2028434600",
    "https://openalex.org/W1934028671",
    "https://openalex.org/W2031495735",
    "https://openalex.org/W2020800109",
    "https://openalex.org/W2804490505",
    "https://openalex.org/W2518005945",
    "https://openalex.org/W3081317704",
    "https://openalex.org/W2968281990",
    "https://openalex.org/W6849429852",
    "https://openalex.org/W2093122002",
    "https://openalex.org/W4308266459",
    "https://openalex.org/W4387570033",
    "https://openalex.org/W4297228992",
    "https://openalex.org/W3089007946",
    "https://openalex.org/W4379229435",
    "https://openalex.org/W4365448040",
    "https://openalex.org/W4312746849",
    "https://openalex.org/W2970419734",
    "https://openalex.org/W3006513159",
    "https://openalex.org/W6686257757",
    "https://openalex.org/W4220893579",
    "https://openalex.org/W4387388856",
    "https://openalex.org/W2391676209",
    "https://openalex.org/W3171557240",
    "https://openalex.org/W3214266794",
    "https://openalex.org/W1967468037",
    "https://openalex.org/W2793905823",
    "https://openalex.org/W2979505395",
    "https://openalex.org/W4308749835",
    "https://openalex.org/W4379207975",
    "https://openalex.org/W2965240382",
    "https://openalex.org/W2572561073",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4387389575",
    "https://openalex.org/W6849459525",
    "https://openalex.org/W6846529796",
    "https://openalex.org/W6798442695",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W4327522191",
    "https://openalex.org/W2887985146",
    "https://openalex.org/W2144270295",
    "https://openalex.org/W1910102824",
    "https://openalex.org/W2136116685",
    "https://openalex.org/W2128446554",
    "https://openalex.org/W2086090764",
    "https://openalex.org/W2019064215",
    "https://openalex.org/W2221477732",
    "https://openalex.org/W4367293029",
    "https://openalex.org/W2129276048",
    "https://openalex.org/W2035188902",
    "https://openalex.org/W3135318750",
    "https://openalex.org/W2586363962",
    "https://openalex.org/W2185261072",
    "https://openalex.org/W4321853583",
    "https://openalex.org/W4320168843",
    "https://openalex.org/W3099362990",
    "https://openalex.org/W3181925591"
  ],
  "abstract": "In the multimedia arena, image tampering is an uncontrollable process that necessitates content authentication and tamper detection in a variety of applications. One method that is recommended for meeting all of those needs in the multimedia arena is watermarking. Mobile cameras may now be used to effortlessly take high dynamic range (HDR) photographs, which increase the image&#x2019;s visual quality and realism. Watermark visibility and recognition algorithms built for standard images may be affected by this introduction of perceptual variations in the image relative to the source. In order to overcome those shortcomings, we introduced a novel, an optimal semi-blind watermarking technique that works for both colour and HDR compressed JPEG images. A unique quaternion dual-tree complex wavelet transform technique is used to extract the highly informative features from the original image. The optimal embedding region in the low frequency sub-band is determined using the maximal entropy random walk (MERW) algorithm. In order to detect tampering and to localize the tampered region a watermark is generated using the swin transformer model and watermark embedding is carried out in the selected optimal blocks. A dual scrambled image is encoded in the effective principal component coefficient values of the singular value decomposition (SVD) Transform in order to authenticate the watermarked image prior to watermark extraction. The semi-blind extraction process is intended to confirm the content&#x2019;s authenticity by comparing the recovered scrambled watermark with the regenerated original watermark. The process of extraction is merely the opposite of the process of embedding. When compared to previous research, the experimental results demonstrated good imperceptibility with an average PSNR of 65 dB and SSIM of 0.999 and strong robustness against attacks.",
  "full_text": "Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000.\nDigital Object Identifier 10.1 109/ACCESS.2023.0322000\nOptimal Semi-Fragile Watermarking based on\nMaximum Entropy Random Walk and Swin\nTransformer for Tamper Localization\nP ABERNA1, L AGILANDEESWARI2,\n1Research Scholar, School Of Computer Science Engineering and Information Systems, VIT, Vellore, TAMIL NADU, 632014 India (e-mail:\naberna.p2020@vitstudent.ac.in)\n2Professor, School Of Computer Science Engineering and Information Systems, VIT, Vellore, TAMIL NADU, 632014 India (e-mail: agila.l@vit.ac.in)\nCorresponding author: L. Agilandeeswari (e-mail: agila.l@vit.ac.in).\nABSTRACT In the multimedia arena, image tampering is an uncontrollable process that necessitates content\nauthentication and tamper detection in a variety of applications. One method that is recommended for\nmeeting all of those needs in the multimedia arena is watermarking. Mobile cameras may now be used\nto effortlessly take high dynamic range (HDR) photographs, which increase the image’s visual quality\nand realism. Watermark visibility and recognition algorithms built for standard images may be affected\nby this introduction of perceptual variations in the image relative to the source. In order to overcome those\nshortcomings, we introduced a novel, an optimal semi-blind watermarking technique that works for both\ncolour and HDR compressed JPEG images. A unique quaternion dual-tree complex wavelet transform\ntechnique is used to extract the highly informative features from the original image. The optimal embedding\nregion in the low frequency sub-band is determined using the maximal entropy random walk (MERW)\nalgorithm. In order to detect tampering and to localize the tampered region a watermark is generated using\nthe swin transformer model and watermark embedding is carried out in the selected optimal blocks. A\ndual scrambled image is encoded in the effective principal component coefficient values of the singular\nvalue decomposition (SVD) Transform in order to authenticate the watermarked image prior to watermark\nextraction. The semi-blind extraction process is intended to confirm the content’s authenticity by comparing\nthe recovered scrambled watermark with the regenerated original watermark. The process of extraction is\nmerely the opposite of the process of embedding. When compared to previous research, the experimental\nresults demonstrated good imperceptibility with an average PSNR of 65 dB and SSIM of 0.999 and strong\nrobustness against attacks.\nINDEX TERMS High dynamic image, Maximum Entropy Random Walk algorithm, Quaternion Dual-Tree\ncomplex wavelet Transform, Swin Transformer.\nI. INTRODUCTION\nT\nHE vulnerability of multimedia content has grown in\nrecent decades as its usage has grown. Due to the fast-\ndeveloping digital technology, it urges to secure multimedia\nimages against tampering applications. Watermarking is rec-\nommended as a highly concerned strategy for securing multi-\nmedia content [1]. Digital watermarking is the process where\nthe image visual quality is maintained by embedding the\nadditional information in the original image. The additional\ninformation used as a watermark is either an image or meta-\ndata or a binary logo. Much research has been done in the past\ntwo decades concentrating on copyright protection [2] and\ncontent authentication applications and currently, it focuses\non color images and high dynamic range (HDR) images for\ntamper detection applications. In the current trend due to the\nadvancement of photographic technology and the availability\nof less-cost mobile cameras, high quality HDR images are\nproduced with more color and detail pixel information.\nMost modern mobile cameras have an HDR mode or an\nAuto HDR feature in the camera settings to capture an HDR\nimage and compress the final HDR image into a JPEG for-\nmat. In the early stage, multi-exposure low dynamic images\n(LDR) are captured and combined using software resulting\nin HDR images. But in the current digital world, the JPEG\n(Joint Photographic Experts Group) is a widely used file\nformat for storing digital images, including HDR images.\nVOLUME 11, 2023 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nSome advanced camera apps or newer mobile devices may\noffer the option to save HDR images in other formats, such\nas HEIF (High-Efficiency Image Format) or DNG (Digital\nNegative). Thus, the rising trend of HDR images on real-time\napplications increases demand for securing multimedia data\nagainst intentional or unintentional attacks.\nFew studies have been done explicitly to analyse HDR images\nagainst tone mapping attacks; in particular, very few studies\nhave examined HDR images against image processing attacks\nemploying watermarking approaches. At the very first, in\nHDR based watermarking technique Yu C et al. [3] presented\na spatial domain-based distortion-free data concealing ap-\nproach. Using a secret key in the spatial domain homogeneity\nvalue of the superpixel, the secret message is directly placed\non the 32-bit RGBE image format. Since every HDR image\nhas a unique wide dynamic range and every pixel has an\ninfinite number of decimal values, it is difficult to directly\nalter the pixels in an HDR image [4]. To get over this problem,\nbilateral filtering techniques based on HDR images are used,\nfollowed by a logarithmic wavelet domain to classify the\nscalable high-frequency image [5]. Their objective is only to\ndetect watermarks using threshold values for which a unique\nmethod is used to compute them. The HDR image watermark-\ning approaches utilised in this literature, which processed\nthe images in three formats—RGBE [6], LogLuv [7], and\nOpenEXR format [8] —are mostly focused on tone map-\nping attacks. To defend against tone mapping attacks, Perez\nDaniel et al. [9] presented a spatial domain watermarking\ntechnique. The super-pixels in the Y channel are separated\naccording to their texture, colour, and semantic content. To\ninsert the binary visible watermark in the adaptable area,\na luma fluctuation tolerance threshold curve based on the\ntransfer function is utilised. Luminance sensitivity in each\nluma code area found in the Y-channel is used to incorporate\nthe watermark to accomplish invisible HDR watermarking.\nSeveral tone mapping operators and a few signal processing\nattacks were investigated in the experiment. Using Tucker de-\ncomposition, an auto-regressive robust watermarking model\nis presented in [10]. The auto-regressive model is used to\ncalculate the local similarity among the initial feature map\nof Tucker Decomposition and to remove the non-adaptive\nzone to embed the watermark in a suitable region. A spatial\ndomain image watermarking method for HDR images in the\nOpenEXR extension was presented by Lin et al. [8]. The HVS\nfeatures take advantage of an adaptive low luminance region,\nwhere the watermark bits are encoded in a 10-bit mantissa\nwith three channels. The outcomes demonstrated that there\nwere no perceived variations in the visual quality between\nthe generated watermarked HDR image and its tone-mapped\nlow dynamic range (LDR) images and that a high embedding\ncapacity was attained. It is evident from the description above\nthat distortion-free systems (i.e., virtually undetectable wa-\ntermarking systems), higher payload systems, and watermark\ndetecting systems are offered. However, the above solution is\nstill underperforming, because it does not take the robustness\nrequirements into account. As a solution, Yazdan Bakhsh et\nal. [7] proposed an artificial bee colony (ABC) optimisation\ntechnique to create a robust HDR image watermarking sys-\ntem. The watermark is embedded in the LH and HL sub-\nbands of the discrete wavelet (DWT) transform, and the best\nembedding block is chosen using an optimization technique.\nRecently, a saliency detection method was used to investigate\na trade-off watermarking system [11]. The saliency object is\ngenerated using a unique ResNet architecture on an HDR\nimage. The original and watermarked images’ foreground\nand background are divided into distinct sections based on\nthe saliency mask. The quantized index modulation approach\nis used to include the permuted random segment watermark\nblocks into the lifting wavelet transform (LWT) host image\nbit plane. Better results were obtained while using different\nmetrics to evaluate the experiment result in order to examine\nthe trade-off performance.\nThe challenges in HDR images are (i) Tone Mapping tech-\nniques that compress the wide dynamic range into a standard\ndynamic range by altering the image’s characteristics, includ-\ning color and contrast. (ii) When tone mapped and saved as\nJPEG the bit depth is reduced from 10 or 12-bit to 8-bit per\nchannel, potentially impact on watermark imperceptibility\nand robustness. (iii) Due to more visually appealing and\nrealistic representation of the HDR image produces higher\nperceptual differences than to the low dynamic image (LDR)\nimage, which affects the watermark visibility of the algo-\nrithms designed for standard images [5]. (iv) Research on\nHDR images is not exposed much in the case of robustness,\ncontent authentication, tamper detection, and localization ap-\nplications. This motivated us to attempt a tamper detection\nbased HDR image watermarking system by embedding the\nwatermark features.\nGenerally, the watermarking system is categorized into three\ntypes based on its robustness namely, Robust watermark-\ning [12], [13], Fragile watermarking, and Semi-fragile wa-\ntermarking [14], [15]. (i) Robust Watermarking: Notably, a\npowerful watermarking system that can endure attacks re-\nlated to signal and image processing operations, but it cannot\nwithstand original image manipulations or pinpoint the tam-\npered region, instead, it verifies whether the image has been\ntampered with or not is termed as “Robust watermarking”.\nIt is mostly used for scenarios where copyright protection is\ncrucial, such as in digital images, audio, and video broadcasts.\nThis system won’t be suitable for content authentication as\nthey are designed to resist alterations. (ii) Fragile Watermark-\ning: Eventhough the watermark fails to be robust against un-\nintentional attacks, it can effectively detect tampering, which\nis termed as “Fragile watermarking”. It is best suitable for\nsensitive and high-security documents like legal documents,\ngovernment records, scientific research papers and forensic\nimaging where integrity is paramount. This system won’t\nbe suitable for copyright protection due to its inability to\nwithstand common alterations like compression. (iii) Semi-\nFragile Watermarking: The Semi-fragile watermarking ad-\ndresses the flaws of both watermarking systems. It stays\nrobust by tolerating image processing attacks and fragile\n2 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nfor intentional attacks. As a result, the semi-fragile system\nhas been suggested for content authentication, and tamper\ndetection system. It is mainly for content integrity verification\nwhere the content might undergo some standard processing.\nThe threshold between intentional and unintentional attacks\ncan be a challenging one to define precisely.\nOn the other hand, based on the domain of embedding\nit is classified into two types as, spatial and transform\ndomain. Compared to spatial domain, the transform do-\nmain system was highly suggested because the spatial do-\nmain fails to provide robustness as well as imperceptibility.\nIn the literature, various transform domain techniques are\nproposed, namely Discrete Fourier Transform (DFT), Dis-\ncrete Cosine Transforms (DCT), Discrete Wavelet Trans-\nform [16], Integer Wavelet Transform (IWT) [17], Station-\nary Wavelet Transform (SWT) [18] and Dual-Tree Complex\nWavelet Transforms [19], Discrete Contourlet Transforms\n[4], Curvelet Transform [20], [21]. Mathematical tool trans-\nform are namely Singular Value Decomposition (SVD) [22],\nQR Decomposition [18], and hybrid of these transform such\nas, DWT-SVD [23], DCT-DWT-SVD [24], Hilbert-IWT [25],\nDWT-BAM [26]. These transform domain techniques are able\nto process grayscale and colour images with qualities like\nimperceptibility, resilience, and capacity. However, they are\nunable to handle colour information in an image effectively.\nThus, it’s critical to preserve the colour information prior\nto watermark insertion. To preserve the color information,\na color processing model has been suggested by many re-\nsearchers, where the luminance Y component is considered\nwhen embedding the watermark. However, the disadvantage\nof ignoring the correlation between the color channels makes\nthe system vulnerable towards color attacks. To avoid this\nproblem, quaternion form of transform domain techniques\nis suggested. They are Quaternion Fourier Transform (QFT)\n[27], Quaternion Discrete Cosine Transform [28], Quaternion\nWavelet Transform (QWT) [29], Quaternion Curvelet trans-\nforms [30], Quaternion Hadamard Transform [31], Quater-\nnion Singular Value Decomposition [22] and so on. Hence,\nto treat the color images in a holistic manner the Quater-\nnion Discrete Fourier Transform (QDFT) is introduced using\nquaternions algebraic form by Li L, et al [27] for copy-\nright protection system. An additional tensor decomposition\nis used after a third order tensor is created from the three\nimaginary frequencies of QFT. Odd-even quantization is the\ntechnique used to incorporate the watermark. To improve the\nextraction process accuracy, the geometric distortion is recti-\nfied by multiple output least squares support vector regression\n(MLS–SVR) network model and pseudo-Zernike moment\nfeatures. The experimental results shown better impercep-\ntibility and robustness for image-processing and geometric\nattacks.\nMulti-level and multi-region watermarking systems are pre-\nsented in [32]. The gray-scale invariant reversible feature is\nembedded as a watermark in the SURF keypoint in multiple\nregions. The multi-levels embedding is performed where M Ö\nN original image region M Ö N/2 payload watermark is em-\nbedded. Likewise, the second level of embedding is carried\nout by finding the SURF keypoint on the first level of the\nwatermarked image where M Ö N/4 payload watermark is em-\nbedded and the same exists for the third level of embedding.\nThe experimental results show better results, but selecting an\noptimal SURF keypoint is a challenge as it affects the texture\nregion during the extraction process.\nChen Y et al. [22] provided a strong dual-color watermarking\nthat protects the copyright by utilising quaternion singular\nvalue decomposition (QSVD). The QSVD transform’s highly\ncorrelated U coefficients contain the watermark incorporated\nin them. Using a quick structure-preserving technique to\nthe singular values from quaternion form is calculated. This\nscheme resulted in highly correlated color channels through\nquaternion form which not only has strong anti-attack perfor-\nmance but is also robust and imperceptible to some common\nattacks.\nIn addition to the above, an optimal system needs to be\nadapted to balance and enhance the performance of water-\nmark characteristics during the watermark embedding and\nextraction process. In order to embed the appropriate amount\nof watermark information, the adaptive system chooses the\nsuitable scaling factor and embedding region. When a water-\nmarking system is non-adaptive, its resilience performance\ndrops even in the absence of an attack [30]. The watermarking\ntechnique should incorporate an adaptive watermarking sys-\ntem as an essential factor to address this problem. In an effort\nto solve these issues, digital watermarking techniques have\nrecently been optimized through the use of nature-inspired\nalgorithms. The optimal scaling factor to integrate the water-\nmark is found in [24] by combining the JAYA Optimisation al-\ngorithm with the nature-inspired Particle Swam Optimisation\n(PSO) technique. DWT or LWT is applied primarily on the\nhost image to obtain the high and low-frequency sub-bands.\nFollowing that, the DCT-SVD transform is applied on the\nsub-bands to embed the watermark using the optimal scaling\nfactor. The proposed system was evaluated for various attacks\nand showed better robustness than the existing system.\nCheema A et al. [16], a efficient semi-blind watermarking\nhas been proposed to address important problems such as\nownership proof and copyright protection. The Singular Value\nDecomposition (SVD), Discrete Wavelet Transform (DWT),\nand Finite Ridgelet Transform (FRT) are used for the wa-\ntermark embedding. To incorporate the watermark into the\nappropriate original image single values, they selected the\nY component luminance channel. Prior to embedding, the\nprimary components are created using SVD techniques, and\nthe security is achieved by the Arnold scrambling method,\nwhich scrambles the Y component of the watermark image.\nUsing a scaling factor optimized by the Particle Swam Opti-\nmisation method (PSO), the singular primary component of\nthe watermark is embedded in the original singular values.\nHsu et al. [28] proposed a blind color image watermarking\nusing two AI techniques. Binary embedding is carried out in\nthe QDCT where the grey wolf optimizer (GWO) is used for\nperformance optimization. From the extracted watermark the\nVOLUME 11, 2023 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nAI-based denoising convolutional neural network (DnCNN)\nhas been utilised to visually recognize the extracted water-\nmark. The results show better robustness for all attacks except\nJPEG compression.\nIn watermarking techniques, watermark embedding, extrac-\ntion, and watermark generation are the major key aspects.\nMost of the researchers used a different image, logo, or partial\nimage feature as a watermark. For instance, different images\nlike RGB, Grayscale, or binary logo, and partial image fea-\ntures like transform domain feature, block-based average fea-\nture values are generated as watermarks. Many studies have\nbeen conducted in the tamper detection field in recent years,\nrevealing that, when compared to standard watermarking sys-\ntems, deep learning-based watermarking systems are more\nefficient in the watermark embedding and extraction process\ndue to their learnable traits [33]. As a result, watermark em-\nbedding and extraction are carried out using the CNN model\nto design a robust and blind watermarking system. Thirteen\nCNN layers have preprocessing, embedding, and extraction\nnetworks are designed. The binary watermark is properly\nembedded in the host images using an adaptive scaling factor.\nThe resultant showed high invisibility and robustness against\npixel-value change attacks and geometric attacks. Luo Y et\nal. [34] recently proposed a convolutional-neural-network\n(CNN)-based multiple residuals learning model for a robust\nmedian filter forensic approach using compressed JPEG and\nlower-size images. [35] presented a deep learning-based im-\nage forgery detection technique. Applications like as image\nsplicing and copy-move detection are the focus of the CNN-\nSRM hybrid model. A test image is used to extract features\nfrom a pre-trained CNN model. The resultant feature is then\ninput into an SVM classifier using a feature fusion method.\nAccording to the aforementioned study, content privacy and\ntamper detection challenges received more attention in the\nCNN model than tamper location and recovery.\nTo solve the above issue, Lee et al. [20] proposed an algo-\nrithm that generates the watermark from CNN features due\nto its traits. The VGG16 network is suggested to generate a\nmask decoder using the high-frequency image traits. The low-\nfrequency sub-band images and the root mean squared high-\nfrequency feature are provided as input to the VGG16 model\nfrom the stationary wavelet transform. Using two batch-\nnormalized inception-based mask deconvolutions followed\nby bilinear upsampling and two simple bilinear upsamplings,\nthe mask decoder model detects tamper localization.\nFor more than a decade, convolutional neural networks\n(CNN), have demonstrated persistent success in the field of\ncomputer vision. In recent times, the Transformer model has\ndemonstrated its dominance in numerous applications, such\nas text summarization and machine translation [21], [36].\nThe transformer model relies on self-attention rather than\nconvolution, which is a straightforward parallel processing\ntechnique that outperformed the CNN models performance.\nCurrently, Transformer models are intentionally designed for\nNLP applications to address long term dependency issue. The\nessential self-attention mechanism that picks up on global\ntraits, is what drives the evolution of the transformer model.\nAttention is the key advantage in transformer model, which\nis extended to deep learning models like convolution. By\nmarrying both the models advantage together, it is termed as\nconvolution-attention model. [37] presented a tamper detec-\ntion and recovery system using convolution attention model.\nLater from the transformer models the image classification\nmodel is proposed by only using the encoder block, which\nis a an image-specific model called Vision and Swin Trans-\nformer exists. Vision Transformers have recently demon-\nstrated ground-breaking performance in a number of different\ntasks, including image classification [38], object detection\n[39], and semantic segmentation [40]. A transformer-based\nwatermarking model was attempted by Aberna et al. [18] for\ntamper detection applications. Using the transformer model,\nmulti-image watermark features are generated. The DWT-QR\ndecomposition is used for embedding, and stationary wavelet\ntransform (SWT) techniques are then applied. For attacks\nlike image processing, better outcomes were obtained. While\nthe authentication key is embedded in the LH sub-band of\nthe singular value matrix, the most important six-bit vision\nfeature maps are embedded in the Schur decomposition ma-\ntrix. Because of its effective deep global features performance\nversus the tamper detection system, this system has demon-\nstrated improved outcomes in terms of robustness and better-\ntampered image recoverability. Later a compromised image\nrecovery system, where a vision transformer-based multi-\nwatermark feature map is used to detect localize tampering\nand to recover the region [17]. Embedding is done using schur\ndecomposition and singular value decomposition transforms.\nAs the feature learning process is limited to local patches,\na hierarchical extended vision transformer, also referred as\nSwin transformer, produces deeper global characteristics than\nvision through shifted window traits. Due to its strong po-\ntential, we incorporated the Swin transformer model in the\nproposed work to extract the watermark feature map.\nA. EXISTING SYSTEMS LIMITATIONS AND OUR\nCONTRIBUTIONS:\nFrom the above literature, the flaws (or) limitations of the\nexisting system are listed as,\n1) Most of the existing quaternion-based color image wa-\ntermarking systems are suggested only for robust wa-\ntermarking and not for tamper detection applications\n[22], [31].\n2) The majority of HDR image based research works are\ntested only for various Tone mapping attacks, with a\nvery few systems evaluated for unintentional attacks\nand none of the systems evaluated for intentional at-\ntacks [8].\n3) Robust global characteristics of the original image have\nnot been taken into consideration as a watermark by\nmany of the existing methods.\n4) The existing color image watermarking system will\nnot be efficient in handling HDR-compressed JPEG\nimages. [28], [32].\n4 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\n5) Lack of research on HDR images in the areas of tamper\ndetection, localization and content authentication [7].\nThe above research results are widely explored in hand-\ncrafted conventional watermark features generated through\ntransform domain techniques. These features make it difficult\nto express the characteristics of the entire host image. The\nkey factor for improving the robustness is by, generating a\nrobust global watermark feature instead of embedding an\nimage or a logo that depicts the characteristics of the host\nimage. Taking this factor into consideration, convolution\nwatermarking models were designed to embed and extract the\nwatermark, and few systems employed convolution features\nor convolution masks as watermarks due to their feature\nlearning traits. Thus, the deep learning-based watermarking\nsystem attained better results in terms of content privacy and\ntampered image prediction. The flaw with the convolution\nmodel is that the existing system learns local features using\nkernels that are connected locally. In contrast, the Trans-\nformer model learns features globally using self-attention for\nall the tokens. The difference between CNN and Transformer\nis the feature interacting mechanism [41]. The computational\ncomplexity increases when a large number of tokens are pro-\ncessed. To solve this issue, several recent works, such as Swin\nTransformer which works locally inside the patch window.\nWe intend to generate robust deep global watermark features\nthat express the characteristics of the original image. The\nSwin transformer model is effective in learning deep global\nfeatures by shifting and windowing processes. The Swin\ntransformer traits match our intention, which motivated us\nto incorporate the Swin-based optimal watermarking model\nin our proposed system.\nIn addition, to address the difficulties of HDR images,\nit’s important to consider watermarking techniques specif-\nically designed to handle the limitations and characteris-\ntics of JPEG and HDR-compressed JPEG images. Adaptive\nwatermarking algorithms, robust feature extraction meth-\nods, and careful consideration of compression parameters\nand quality settings can help improve the performance and\nresilience of watermarks in these challenging scenarios.\nThe contributions towards this work are listed below:\n1) Design an adaptive and robust feature extraction water-\nmarking techniques to handle both the color image and\ncompressed HDR images against tamper detection and\nlocalization application.\n2) Attempted Swin transformer-based watermark gener-\nation model for the first time on color and HDR im-\nages, which extracts invariant global feature maps as\nwatermarks to achieve robustness against intentional\nand unintentional attacks and achieved better results for\ntamper detection applications.\n3) The optimal region is determined using a novel maxi-\nmum entropy random walk algorithm.\n4) Quaternion dual complex tree wavelet transform tech-\nnique is derived to maintain the color information intact\nand also to embed the watermark, where the correlation\namong color channels is maintained.\n5) Payload is improved by embedding the watermark fea-\ntures repeatedly to enhance the tamper detection accu-\nracy.\n6) The effectiveness and efficiency of the Swin features is\nevaluated in terms of robustness and imperceptibility\nmetrics for various attacks on both RGB and HDR\nimages.\nII. MATERIALS AND METHODS\nA. SWIN TRANSFORMER\nSwim transformer [42] is a hierarchical mechanism that\nprocesses images using the concept of shifted windowing.\nFixed image patches are inappropriate attributes for image\nprocessing transformer models since it increases the number\nof tokens for high-resolution images, resulting in quadratic\ncomputational complexity. Swin transformers are built to\navoid such issues where a hierarchical dense feature maps\nare generated by combining image patches represented by\nwindow shifting traits, where the model’s performance is\nlimited by self-attention computation within the local win-\ndow as well as cross windowing connections. In addition,\nswin transformer has the advantages of hierarchy, locality,\ntranslation invariance and an inductive bias that suits for task\ntargeting [43]. Therefore, it is suitable for large datasets that\ngeneralize the process by learning features. The architecture\nselected in this paper, is the Swin Transformer Base, and its\nstructure is shown in Fig. (2).\nAccording to Swin transformer block as in Fig. (2), first the\noriginal image is divided into patches of size 4 Ö 4 through\nthe patch partition module where each patch ‘ Xi’ treated as\n“tokens”. The features of each channel are linearly embedded\nby flattening it with dimension H\n4 × W\n4 × C. After linear\nembedding four stages of Swin modules with self-attention\nare applied on each patch to construct different size feature\nmaps. So, the number of original image patches and the linear\nembedding layer together referred as “Stage 1”. Apart from\nStage 1, the last three stacked Swin modules are downsampled\nby patch merging layer which reduce the number of tokens\ninto halves. This first hierarchical patch merging layer com-\nbine the features into 2 Ö 2 patches and apply linear layer\nwhich downsample the number of tokens by 2 Ö C. The\nSwin transformer block is applied on resultant patch merged\ntokens with resolution of H\n8 ×W\n8 represented as “Stage 2”. The\nprocess was repeated twice in stage 3 and stage 4 with patch\ndimension of H\n16 × W\n16 and H\n32 × W\n32. The Swin transformer\nblocks comprises of four components namely, multilayer per-\nceptron (MLP), window multi-head self-attention (W-MSA),\nshifted window-based multi-head self-attention (SW-MSA),\nlayer normalization (LN) and Drop-Path. The W-MSA and\nSW-MSA are the key components of Swin used together for\neach Swin module. The LN will normalize the feature distri-\nbution data and Drop-path has regularization effect. Unlike\nmulti-head attention (MSA) layer, shifted-MSA is employed\nVOLUME 11, 2023 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nalong with two MLP layer where GELU activation function\nis induced between the MSA and MLP layer. The SA com-\nputation process is provided in Eq. (1), (2) and (3). The Key\n(K), Value (V), Query (Q) computed using Equation (1),\nQi = Wq Pi, Ki = Wk Pi, Vi = Wv Pi (1)\nwhere Wq, Wk , Wv are the trainable parameters, which will\nbe the same for all the input patch vector Xi:n sequence.\nN number of heads referred as self-attention mechanism is\ncombined together as multi-head attention (MHA) shown in\nEq (4).\nScalingDotproduct(Q, K, V ) = Ki:n\nT Qi√dK\n(2)\nSelf − Attention(SAi) = softmax\n\u0012Ki:n\nT Qi√dK\n+ B\n\u0013\nVi, (3)\nwhere i =1,2,...n\nMultiHeadAttention(MHA) = Concat(SA1 + SA2+, ...,SAn)\n(4)\nwhere B represents relative position code, d represents\nquery dimension. The W-MHA process is given in equation\n(5) and (6),\nbPl = Droppath(W − MHA(LN(Pl−1))) + P\nl−1\n(5)\nThe attention weight score is given as input MLP classifier to\nobtain the learned feature maps as in Eq.(6),\nPl = Droppath\n\u0010\nMLP\n\u0010\nLN\n\u0010bPl\n\u0011\u0011\u0011\n+ bPl , l = 1, 2... L\n(6)\nWithin W-MSA, the global interaction is limited as self-\nattention is calculated within the local window. The S-MHA\nis used to divide the L layer by moving half of the window\nto the L+1 layer and then dividing it again in order to prevent\nthis. The local window attention weight will interact globally\nas a result of doing this. The computation is given in the\nequation below,\ndPl+1 = Droppath\n\u0000\nMLP\n\u0000\nLN\n\u0000\nPl \u0001\u0001\u0001\n+ Pl (7)\nPl+1 = Droppath\n\u0010\nMLP\n\u0010\nLN\n\u0010 dPl+1\n\u0011\u0011\u0011\n+ dPl+1 (8)\nFrom the trained Swin Transformer model, robust features\nare obtained to enhance the robustness of the proposed wa-\ntermarking scheme.\nB. MAXIMUM ENTROPY RANDOM WALK (MERW)\nALGORITHM\nGraph emerged from the spectral graph theory concept,\nrepresenting the relation between objects or samples. Graph\nspectral theory is linked with signal processing from where it\nis stimulated to graph-based transform. Due to their ability to\nmathematically model the interconnections between pixels,\nthey have recently gained adoption in computer vision appli-\ncations. Applications like Citation analysis, social networks,\nand link-structure analysis have highly benefited from using\ngraph-based algorithms. Edge identification and segmenta-\ntion are a few more image processing challenges that have\nbeen solved using graph-based techniques [44].\nA graph algorithm determines a vertex’s importance, power,\nor energy inside a graph by considering global information\nrather than local vertex-specific information. Mihalcea. R et\nal., [45] has shown adapting graph-based ranking technique\nsuch as Kleinberg’s HITS algorithm (Kleinberg, 1999) or\nGoogle’s PageRank (Brin and Page, 1998) for the weighted\nand unweighted graph.\nRandom walks (RW) and maximal entropy random walk\n(MERW) are the suitable algorithms for the undirected im-\nage which has been successfully adopted for image seg-\nmentation [46], link prediction [47], and object detection\n[48], object localization [49], visual saliency region [50],\ntampering detection [51]. Since MERW assigns the uni-\nform probability distribution of all pathways in a given\ngraph by globally maximising entropy, it performs bet-\nter than Random Walk; Whereas, RW selects uniform\nprobability distribution for every vertex among its outgo-\ning edges which the maximizes the entropy rate locally.\nMERW is a popular random walk algorithm that works based\non the transition probabilities which are chosen according\nto the maximum entropy principle. The distribution of tran-\nsition probability on graph structure indicates the region’s\nimportance in terms of maximum entropy. In the proposed\nmodel, we utilised the MERW algorithm to determine the\nadaptive region due to its localization property on a graph.\nGraph is represented as G = {V , E}, where V & E represent\nvertices and edges, V = {v0, v1, . . . .vn}, where V ∈ Ri×j and\nE = {i, j}, where {i, j} ∈V . The graph method is extended\nto images by representing each image pixel as vertex V and\ninterconnection between pixels represented as edge E. The\nadjacency weight matrix ′Aij′ defined from the connectivity\nof two vertex through edges E (i.e., incoming and outgoing\nlinks between two vertices) which is computed as follows:\nAij =\n\u001a (aij, (i, j) ∈ E\n0, otherwise (9)\nThe random walk refers to a random walker ′Rn′, where n\n∈ 0, 1 . . .. The random walker moves in random transition\nfrom current node i ∈ V to neighbour node j ∈ V based on\ntransition probability ‘ P’ where Pij : P (Rn+1 = j | Rn =\ni).The MERW method hopping is a Markov process that\nhops longer m-steps from one node i to the other node j.\nAs a result, the walker distribution towards maximal entropy\nachieves the MERW global structure. The suggested model\ndiscovers the maximal entropy region based on the transition\nprobability distribution in a graph. The average entropy rate\nof the stochastic transition process can be calculated by taking\nthe probability of visiting each vertex and averaging it using\nShannon entropy equation:\nE = −\nnX\ni=1\nρ∗\ni\nnX\nj=1\nPij log Pij (10)\n6 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nThe transition probability matrix ′Pij′ is obtained by eigen\nvalue and eigen vector. From the real symmetric matrix L,\nnon-negative eigenvalues and eigenvector are generated as:\nL = φλφT =\nnX\ni\nφiλφT\ni (11)\nwhere λ = {λ1, λ2...λn} are eigenvalues and φi =\n{φ1, φ2, .......φn} are the corresponding eigenvector. The\ngraph laplacian matrix L is obtained by L= D − Aij, where\nD is a diagonal matrix, where ith diagonal element di is sum\nof all edge weights of vertices V. Then transition probability\nmatrix are given by,\nPij = Aijφi\nλφj\n(12)\nwhere λ represents eigenvalue of the weight matrix, and φi, φj\ndenote the ith and the jth node corresponding eigenvector.\nAccording to the Frobenius-Perron theorem, both the φ and\nλ are non-negative sign, so that one can choose φi > 0. The\nstationary probability distribution ρi(t), find the particle at\nnode i at time t generally through stationary state ρ∗\ni using\nthe equation:\nρ∗\ni =\nX\nj\nρ∗\nj Pji (13)\nDue to the non-negative weight matrix, all the eigen vectors\nare normalised to Pφi = 1, so the transition matrix are also\nnormalized. Therefore, the stationary distribution of finding\nthe walker at node i by the following equation:\nρ∗\ni = φ2\ni , (14)\nwhere ρ∗\ni : i ∈ V and P\ni ρ∗\ni = 1. Further the quantity of\ninterest in generating the probability P of trajectory δt\nv0vn of\nlength t passing through nodes (v0, v1, . . .vn).\nP\n\u0000\nδt\nv0vn\n\u0001\n= (Pv0v1 , Pv1v2 , . . . . . .Pvt−1vn ) (15)\nThe transition probability distribution of trajectory\n‘P\n\u0000\nδt\nv0vn\n\u0001\n’\nP\n\u0000\nδt\nv0vn\n\u0001\n= 1\nλt\nφi\nφj\n(16)\nStart the random walk process from a specific region and\niteratively perform the random walk using the transition prob-\nabilities. At each step, the random walk can transition to\nneighbouring regions based on the probabilities defined in\nthe adjacency matrix. After several iterations, the random\nwalk process will converge, and you can compute the MERW\nscores for each region. The MERW scores represent the\nadaptive high-energy values of the regions, indicating their\nsuitability for watermark embedding. Based on the MERW\nscores, identify the regions with the highest scores, as these\nregions are considered adaptive high-energy regions which\nare suitable for embedding watermarks.\nC. QUATERNION FORM\nFor color image watermarking systems, most of the system\nutilised Quaternion form of representation mathematical no-\ntion suggested by William Rowan Hamilton in 1843 [52].\nSpecifically in watermarking, the frequency domain tech-\nnique is the first preprocessing step in acquiring more infor-\nmation to further process the data, to embed the watermark\neither in a single or in a RGB channel. When the watermark is\nembedded either in single color channel or an entire color im-\nage, it losses the correlation among the channels which affects\nthe performance of robustness. To avoid this, the quaternion\nform of image representation is taken as a preprocessing\nstep before embedding the watermark. Quaternion form of an\nimage is suggested as a solution where it converts it to a vector\nfield. Generally, the quaternion form is expressed as a sum of\nreal and vector form,\nq = + qii + qjj + qk k (17)\nwhere µ represents real part (qreal), and qii + qjj + qk k as\nimaginary part, which satisfies i2 = j2 = k2 = ijk = −1.\nThe pure quaternion form expressed as q = qii + qjj + qk k,\nwhen qreal = 0 , if q has unit norm q=1, then q is unit\nquaternion form.\nq =\np\nq.q∗ =\np\nq2r +\np\nq2i +\np\nq2j +\np\nq2k (18)\nFor color image the quaternion form is expressed as,\nq = µ + qR(x, y)i + qG(x, y)j + qB(x, y)k (19)\nwhere qR (x, y) , qG (x, y) , qB(x, y) represent the red,\ngreen, and blue channels pixels respectively. The inverse\nquaternion form is just the conjugate of general quaternion\nform which can be represented as,\nq = µ − qii − qjj − qk k (20)\nColor image inverse quaternion form is represented as,\nq = µ − qR(x, y)i − qG(x, y)j − qB(x, y)k (21)\nD. DUAL-TREE COMPLEX WAVELET TRANSFORM (DTCWT)\nAmong those transform domain techniques, discrete wavelet\ntransform has been suggested the most in the existing works\ndue to its efficiency. However, it fails to achieve the shift-\ninvariance and directionality properties resulting in perfor-\nmance degradation in some cases. To address this issue,\na Dual-Tree Complex Wavelet Transform is developed by\nKingsbury [53] having many advantages like shift-invariance,\nstrong directional selectivity, limited redundancy, and perfect\nreconstruction property with less computing complexity. The\nDTCWT complex values are obtained from DWT and com-\nplex wavelet transform. The inverse DTCWT is performed\nwhere the real and imaginary parts are conjugated to form real\nsignal. By averaging the two real signals the final reconstruc-\ntion is attained. First, the translation and dilation sequence is\nperformed on image I(x, y) using complex scaling function\nVOLUME 11, 2023 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nφ(x, y) resulting six complex wavelet function Ψk , lθ(x, y).\nFor an image I(x ,y) the DTCWT is expressed as,\nI(x, y) = lϵZ Rk0,l ΨΘ\nk,l (x, y) + ΣΘΣk≥k0 ΣlϵZ2 W Θ\nk,l ψθ\nk,l (x, y)\n(22)\nwhere Θ represents six complex wavelet directionalities,\nΘ ∈ (±15◦, ±45◦, ±75◦), Z represents natural number,\nk and l are shift and dilation, Wk,l represents complex\nwavelet coefficients with complex ϑk0,l (x) = ϑreal\nk0,l (x) +q\n−1ϑimg\nk0,l (x), and ϑk,l (x) = ϑreal\nk,l (x) +\nq\n−1ϑimg\nk,l (x). So,\nthe DTCWT applied on the image I(x, y) produce two com-\nplex low frequency sub-band and six direction high frequency\nsub-bands expressed as shown in Eq. (23) and (24)\nDTCWT (I (u, v)) = Low frequency(L1)+ High frequency(H1)\n(23)\nDTCWT (I (u, v)) = x(∀, L1, u, v) + x(γ, Hθ\n1 , u, v) (24)\nThe above Eq. (24) is elaborated as,\nx(∀, L1, u, v) = R(x(∀, L1, u, v)) + kImg(x(∀, L1, u, v))\n(25)\nx(γ, Hθ\n1 , u, v) = R(x(γ, Hθ\n1 , u, v)) + kImg(x(γ, Hθ\n1 , u, v))\n(26)\nwhere R and Img stands for real and imaginary parts, ∀ rep-\nresents number of decomposition levels, here it is 1, L1 rep-\nresents Low frequency sub-bands, u,v represents coefficients\nsub-band locations where the values range between 0 to N\n2∗ -1.\nThus, the result of DTCWT has shown good shift-invariance,\nselection strong directionality, with less redundancy and also\nimproved the efficiency of imperceptibility than any other\nwavelet transforms.\nE. QUATERNION DUAL-TREE COMPLEX WAVELET\nTRANSFORM\nThe advantage of both quaternion and dual-tree complex\nwavelet transform are combined as expressed in the given\nequation from (27) – (32). The color image ‘ i’ RGB chan-\nnel is denoted as qR, qG, qB. The QDTCWT is obtained by\nsubstituting Eq. (24) in Eq. (19) which is expressed as,\nQDTCWT (q) = I(m, n) = µ + DTCWT (qR(m, n)i\n+qG(m, n)j + qB(m, n)k) (27)\nQDTCWT (q) = µ + DTCWT (qR(m, n))i + DTCWT\n(qG(m, n))j + DTCWT (qB(m, n))k) (28)\nNow substitute µ in Eq. (28),\nQDTCWT =\n[re(DTCWT (qR(m, n))) + Img(DTCWT (qR(m, n)))]i\n+ [re(DTCWT (qG(m, n))) + Img(DTCWT (qG(m, n)))]j\n+ [re(DTCWT (qB(m, n))) + Img(DTCWT (qB(m, n)))]k\n(29)\nwhere µ = α∗i+β∗j+γ∗k denotes a unit quaternion subject\nto the constraint that µ2 where i, j, k are real numbers. The\nabove Eq. (29) is same as in [30] and [54], so we have used\nthe four-vector space quaternion dual-tree complex wavelet\ntransform which is expressed as,\nQDTCWT (q(m, n)) = Q0(m, n) + Q1(m, n)i + Q2(m, n)j\n+ Q3(m, n)k\n(30)\nwhere\nQ0(m, n) = −αImg(DTCWT (qR)) − βImg(DTCWT (qG))\n+ γImg(DTCWT (qB)),\nQ1(m, n) = re(DTCWT (qR))\n+ γImg(DTCWT (qG)) − βImg(DTCWT (qB)),\nQ2(m, n) = re(DTCWT (qG)) + αImg(DTCWT (qB))\n− γImg(DTCWT (qR)),\nQ3(m, n) = re(DTCWT (qB)) + βImg(DTCWT (qR))\n− αImg(DTCWT (qG))\nHere, re(x) denotes the real part of the usual com-\nplex number x, and Img(x) denotes the imaginary com-\nponent. The conventional DTCWT matrix of the red,\ngreen, and blue channels is represented by the symbols\nDTCWT (qR), DTCWT (qG), DTCWT (qB), respectively. For\nthe sample image the quaternion dual-tree complex wavelet\ntransform of red channel is visualized in Fig. (1). The in-\nverse quaternion dual-tree complex tree wavelet transform\n(IQDTCWT) can be expressed as,\nIQDTCWT (q(m, n)) =\nre(IDTCWT (Q0(m, n)) + µImg(IDTCWT (Q0(m, n))\n+ [re(IDTCWT (Q1(m, n)) + µImg(IDTCWT (Q1(m, n))]i\n+ [re(IDTCWT (Q2(m, n)) + µImg(IDTCWT (Q2(m, n))]j\n+ [re(IDTCWT (Q3(m, n)) + µImg(DTCWT (Q3(m, n))]k\n(31)\nIQDTCWT (q(m, n)) = f (Q0)(m, n) + f(Q1)(m, n)i+\nf(Q2)(m, n)j + f(Q3)(m, n)k (32)\nwhere,\nfQ0 (m, n) =re(IDTCWT (Q0)(m, n))) − αImg(IDTCWT\n(Q1)(m, n)) − βImg(IDTCWT (Q2)(m, n))\n− γImg(IDTCWT (Q3)(m, n)),\nfQ1 (m, n) =re(IDTCWT (Q1)(m, n)) + αImg(IDTCWT\n(Q0)(m, n)) + γImg(IDTCWT (Q2)(m, n))\n− βImg(IDTCWT (Q3)(m, n))),\nfQ2 (m, n) =re(IDTCWT (Q2)(m, n)) + βImg(IDTCWT\n(Q0)(m, n)) + αImg(IDTCWT (Q3)(m, n))\n− γImg(IDTCWT (Q1)(m, n)),\nfQ3 (m, n) =re(IDTCWT (Q3)(m, n)) + γImg(IDTCWT\n(Q0)(m, n)) + βImg(IDTCWT (Q1)(m, n))\n− αImg(IDTCWT (Q2)(m, n))\n8 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\n(A)\n(B)\nFIGURE 1. Red Channel of Quaternion Dual-Tree Complex Wavelet\nTransform (A) CASIA dataset images - Image 7, (B) LVZ-TMO HDR images -\nImage 7\nIII. PROPOSED MODEL\nIn this work, a novel optimal image watermarking scheme\nusing a Swin transformer and maximum entropy random walk\nalgorithm is suggested for HDR image-based tamper detec-\ntion applications. Normally, the HDR image is composed of\ndetailed pixel information in terms of luminance, brightness,\nand texture features. The HDR images are compressed to\nJPEG format by a mobile camera. The existing color im-\nage watermarking system will not be efficient in handling\nHDR-compressed JPEG images. As a solution, quaternion\nDTCWT and SVD transform model. Apart from embedding\na watermark in an optimal region without influencing im-\nperceptibility, generating a robust watermark is also a major\ncriterion in the watermarking technique. The proposed model\nattempted the Swin base transformer model which is highly\ninvariant to translation and produces dense global watermark\nfeatures of an original image, that enhance the system to\nattain high robustness against various attacks than the existing\ntraditional watermark generation techniques. Apart from a\nrobust watermark, embedding the watermark in an optimal\nregion is determined by the maximum entropy random walk\n(MERW) region.\nA. DETAILED PROCESS\nThe proposed model embedding process is clearly illustrated\nin Fig. (2) and the extraction process is depicted in Fig.\n(3). The original image is pre-processed by converting the\npixel to quaternion form which can maintain the correlation\namong the color channels, followed by that Dual-tree com-\nplex wavelet transform is applied on the quaternion vector\ncoefficient to produce two low frequency ( L1R, L1Img) and\n(H1, H2, H3, H4, H5, H6) six high-frequency sub-bands. To\nembed the watermark in an optimal region, a novel maximum\nentropy random walk algorithm is employed to determine\nthe low-sensitive region. The random walker ’ Rn’ search the\nmaximum entropy region by the transition probability score\nof each pixel. Once the high MERW determines the optimal\nregion ’ ORn’ through stochastic distribution score, those re-\ngions are bounded to further embed the watermark ’ OBn’. The\nrobust global deep feature watermark is generated from the\nSwin transformer model where the dataset is trained, to learn\nthe feature. From the pre-trained model, the original image\nfeatures are obtained through class labels. Before embedding\nthe watermark, QSVD is used as in [22], where the Swin fea-\ntures are partitioned according to the optimal block size and\nembedded in the principal component of QSVD transform\nusing the scaling factor. In the proposed work α =0.063 is\nfound to be an appropriate embedding factor. Inverse SVD\nis performed on the modified low-frequency sub-band L′\n1R.\nFollowing that, the secured content authentication watermark\nis obtained from the dual gyrator scrambled image which\nis embedded in the imaginary low frequency sub-bands.\nThe extraction process in the reverse of embedding process\nwhere the gyrator scrambled watermark is recovered from\nthe watermarked. The regenertaed authentication watermark\nis compared to the extrcated authentication watermark; if\nthey match, the extraction operation can continue; otherwise,\nextraction process not allowed.\nTo detect the tampering of the watermarked image, the prin-\ncipal component EB′′\nOBn is extracted from low frequency\nsub-band of QDTCWT from the tampered image and the\noriginal principal components EB′\nOBn are extracted from the\nwatermarked image. By comparing both applied to extract\nthe principal components tamper localization is detected and\ntampered region is recovered using swin features.\nAlgorithm\nProcess 1 : Watermark embedding process\n1) Apply quaternion dual-tree complex wavelet transform\non input image Ii, and the red channel quaternion dual-\ntree complex wavelet transform is expressed as,\n[L1R, L1Img, (H11R, H11Img, H12R, H12Img, H13R,\nH13Img, H14R, H14Img, H15R, H15Img,\nH16R, H16Img)] = QDTCWT (qR(m, n))\n(33)\nwhere L1, H1 refers low and high frequency sub-bands,\nR and Img refers real and imaginary part coefficient\n2) Determination of suitable embedding region using\nMERW on each channel of LL sub-band:\nVOLUME 11, 2023 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 2. Proposed Embedding Process\na) Graph Construction: Construct a graph represen-\ntation for L1 sub-band image, G ( L1R) = V, E\nrepresent the graph, where V = v0, v1, v2 . . .vn,\nvnϵ I(m,n), E = E0, E1 . . .En, Enϵ V\nb) Determine the energy value: Energy value re-\nferred as E( vi) computed by Eq. (10) where E( vi)\nrepresent scalar function that map node vi to its\nenergy value in the graph.\n10 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 3. Proposed Extraction Process\nc) Compute the transition probabilities ’ Pij’ of each\nnode vi computed using Eq. (12).\nd) Normalize the transition probabilities ’ Pij’ and\nconstruct the transition matrix T, Pijϵ I(m,n)\nT (i, j) = Pij\nΣ P(vi, Aij(vi)) (34)\nhere Aij denotes the adjacent matrix value node\nof vi. ‘ T ’ matrix size depends on the number of\nnodes in the graph.\ne) Initialize random walker Rn and perform random\nwalk process from a seed pixel or set of seed\npixels.\nf) The stationary probability pi(t) from node v0 to\nnode vn is determined as explained in II(B).\ng) After several iterations of the random walk, the\nMERW scores for each pixel.\nh) Let S( vi) represent the MERW score of node vn.\nThe MERW score can be calculated using the\nprobability values obtained from the random walk\nprocess:\nS(vi) = −log(T (vi)) (35)\nThe negative logarithm of the probability repre-\nsents the entropy or uncertainty associated with\nbeing at node vi.\ni) Apply a threshold to the MERW scores to de-\ntermine the block boundaries. Pixels with high\nMERW scores are considered part of the optimal\nregion ORb, while pixels with low scores are con-\nsidered high sensitive region.\nVOLUME 11, 2023 11\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nORn = MS (vi, vj) ≥ t, i, jϵϵI(m, n) (36)\nwhere t represent average threshold values of\nMERW score\nj) Embedding blocks are selected as ’ OBn’ from the\noptimal region ’ ORn’\n3) Robust swin watermark feature generation\na) Train the dataset images Ii on the Swin trans-\nformer.\nb) Divide the image into an even number of n-sized\npatches.\nc) Perform linear embeddings by flattening the im-\nage patches.\nd) The linear input layer is inputted to stage1 Swin\ntransformer module.\ne) In stage 2, all the patches are combined in patch\nmerging layers followed by swin module.\nf) Hierarchical four stages of features are learnt and\nrobust feature maps are extracted from the last\nlayer fi from the pre-trained model.\n4) Select the optimal embedding block OBn and embed the\nwatermark fi in selected optimal embedding block OBn\ncoefficients,\ndLOBn\n1Ri = dLOBn\n1R + α × fi (37)\nwhere i represent number of selected blocks where the\nfeatures blocks are embedded accordingly.\n5) Preprocessing watermark dual biometric image b1 and\nb2\n6) Dual owner biometric image b1, b2 scrambled with\ngyrator scrambling method.\nScramble1 = gyrator(b1) (38)\nScramble2 = gyrator(b2) (39)\n7) Final authentication watermark is generated using XOR\nmethod.\nAuwm = XOR(Scramble1, Scramble2) (40)\n8) The authentication watermark is embedded only in the\nred channel L1R sub-band principal component coeffi-\ncient of SVD transform\n[U cL1R\n, S cL1R\n, V T\ncL1R\n] = SVD( cL1R) (41)\n9) Modified L′\n1R coefficient is obtained by embedding the\nwatermark ’ Auwm’ in low frequency imaginary sub-\nband and inverse SVD transform applied as expressed\nas,\nEB′\ncL1R\n= (S cL1R\n× V T\ncL1R\n) + αAuwm (42)\nL′\n1R = ISVD[U cL1R\n, EB′\ncL1R\n, V T\ncL1R\n] (43)\n10) The embedding process from step (1 – 4) is carried out\nfor all the three channels and the modified QDTCWT\nis obtained.\nIQDTCWT [( cL′\n1R, L1Img, (H11R, H11Img, H12R, H12Img,\nH13R, H13Img, H14R, H14Img, H15R, H15Img,\nH16R, H16Img))] = ModifiedC oefficient(q′\nR(m, n))\n(44)\n11) Finally, the watermarked image ‘WM’ is obtained using\ninversing the QDTCWT as expressed in Eq (32).\nProcess 2: Watermark extraction process\n1) Apply QDTCWT on the received watermarked image\n‘WM’ as performed similarly in the embedding pro-\ncess, where the below Eq. (45) express the QDTCWT\nof red channel,\n[L1RW , L1ImgW , (H11RW , H11ImgW , H12RW , H12Imgw ,\nH13RW , H13ImgW , H14RW , H14ImgW , H15RW , H15ImgW ,\nH16RW , H16ImgW )] = QDTCWT (WM(qRed (x, y))\n(45)\n2) To verify the content authentication by extracting the\nwatermark from the principal component of low fre-\nquency imaginary sub-band L′\n1Img\n[U′\nL1Rw\n, S′\nL1Rw\n, V T ′\nL1Rw\n] = SVD(L1Rw ) (46)\nEAu′\nwm = S′\nL1Rw\n× V T ′\nL1Rw\n(47)\n3) Original authentication watermark is regenerated from\nthe owner’s original biometric image by repeating the\nembedding process steps from (5-7)\nA = Compare(EAu′\nwm, Auwm) (48)\nIf A == true\nContent authenticated\nDo\nExtraction process\nElse\nWatermarked image is unauthenticated and no extrac-\ntion process\n4) To extract the tamper detection and recovery water-\nmark, the optimal region ’ OBn’ is determined by per-\nforming MERW algorithm in ’ L1Rw ’ sub-band as in step\n2 of embedding process algorithm.\n5) Swin features are extracted from the optimal region\nblocks L′dOBn\n1Ri from the low frequency sub-band L1RW ,\nL′dOBn\n1RWi\n= MERW (L1RW ) (49)\nTDOBn\ni = Compare\n\u0010\nL\ndOBn\n1RWi\n, dLOBn\n1Ri\n\u0011\n(50)\n12 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nIf TDOBn\ni = =0\nImage Not tampered, extraction process not required\nElse\nTamper detected and extract watermark\n6) Extract the watermark from the watermarked low fre-\nquency band to localize\nEfi = L′dOBn\n1RWi\n− dLOBn\n1Ri / ∝ (51)\n7) Extracted swin features are combined to produce ex-\ntracted watermark which has the ability to recover the\ntampered region.\nIV. EXPERIMENTAL RESULTS\nThe performance of the proposed algorithm against various\nattacks where the watermarking characteristics efficiency is\nevaluated in terms of PSNR, SSIM, HDR-VDP, and NCC.\nThis section deals with experimental setup, dataset descrip-\ntion, evaluation metrics, performance analysis and compara-\ntive analysis.\nA. EXPERIMENTAL SETUP\nThe experiment is carried out in Windows 11th Gen Intel(R)\nCore(TM) i5-1155G7 with processor speed @ 2.50GHz 2.40\nGHz and 8GB capacity of RAM with 64-bit processor. The\nsoftware used for simulation purpose is MATLAB R2021a. In\norder to generate watermark, the transformer model is trained\nin google colab platform with T4 GPU hardware accelerator.\nB. DATASET DESCRIPTION\nTo evaluate the performance of the proposed algorithm, the\nexperiment is conducted on the selected dataset LVZ-TMO\n(HDR images) [55], CASIA [56] and benchmark dataset\n(Color images). So, the LVZ-TMO dataset [55] contains\n457 HDR images with four classes namely, indoors with\n71 images, nature with 173 images, nighttime with 80 im-\nages, and river-side sunset with 133 images, with dimension\n1024×1024 in compressed JPEG format, CASIA1 dataset has\n1721 images belongs to two classes with image dimension\n384 × 256 and the benchmark images like crown, boat, lena,\naeroplane, and Peppers with dimension of 256 × 256, and\n512 × 512. The original images and the augmented images\ntotally 10,990 images are inputted, in order to train and to\nextract features from the swin transformer model.\nTo authenticate the watermarked image, biometric images\nare utilized from FVC2002 [57] and MMU iris dataset [58]\n(Biometric images). The FVC2002 dataset contains 880 im-\nages of size 288×384 (108 Kpixels), and iris datasets named\nMultimedia University (MMU1) [58] contains merely 460\nimages with dimension 320 × 240 in ’.bmp’ format respec-\ntively. The sample test images from LVZ-TMO and CASIA1\ndataset is shown in Figure (4). Figure (5)(A) shows a sample\nbiometric image from FVC2002- DB2B and MMU iris dataset\nthat are listed in Figure (5)(B).\n(A)\n(B)\n(C)\nFIGURE 4. Sample Test Images (A) LVZ- TMO HDR images (B) Benchmark\nImages (C) CASIA dataset images\n(A)\n(B)\nFIGURE 5. Owners Sample biometric images (A) FVC-DB2004 (B) MMU\nIris dataset\nC. EVALUATION METRICS\nTo protect HDR and RGB images from tampering, the opti-\nmal semi-fragile watermarking technique is designed using\nthe swin transformer model. The performance analysis was\nconducted with respect to robustness, imperceptibility, and\nauthenticity. Common metrics to assess image quality in\nterms of imperceptibility are the Peak Signal-to-Noise Ratio\n(PSNR) and the Structural Similarity Index Measure (SSIM).\nBit Error Rate (BER) and Normalised Correlation Coefficient\n(NCC) are the most widely used measures to quantify robust-\nVOLUME 11, 2023 13\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nness.\n1) Imperceptibility\nGenerally, the embedded watermark shouldn’t degrade the\nquality of the original image, which is why it has impercepti-\nbility properties. In assessing the imperceptibility The metrics\nused to assess the degree of resemblance between the original\nand watermarked image are the Peak Signal-to-Noise Ratio\nand the Structural Similarity Index Measure (SSIM).\n1) Peak Signal-to-Noise Ratio (PSNR) [30]: The PSNR\nmetric’s performance is determined by the degree of\nsimilarity between the original and watermarked im-\nages, with a minimum value of 30 dB, indicating a sig-\nnificant range. Lesser than 30 dB is considered as less\nimperceptible which means the embedded watermark\nhas influenced the watermarked image more. Whereas\nSSIM range between 0 to 1, closer to 1 is of acceptable\nrange. The PSNR and SSIM metrics are computed\nthrough the below given equations,\nPSNR = 10 log 10\n\n 2552\nhP\nM,N (I(m, n) − WM( m, n)2] 1\nM∗N\n\n\n(52)\nwhere M, N are the image dimension, original image rep-\nresented as ‘ I’ and watermarked image as ‘ WM’\n2) Structural Similarity Index Measure (SSIM) [30]: The\nimperceptibility performance is determined by luminance,\ncontrast and correlation between original and watermarked\nimage which ranges between 0 to 1. The value range near to 1\nis considered to be of acceptable quality which is represented\nas,\nSSIM = (2µI µWM )\n\u0000\n2σ(I,WM)\n\u0001\n(µ2\nI + µ2\nWM ) (σ2\nI + σ2\nWM ) (53)\nwhere µOI and µWI represent the mean of I and WI (i.e.,\nimage luminance), \" σI \" and \" σWM \" represent the standard\ndeviation of I and WI (i.e. image contrast), \" σI,WM \" represent\nthe correlation coefficient of original ‘ I’ and watermarked\nimage ‘WM’\n3) Visual Dynamic Predictor (VDP) [10]: In addition to the\nabove metrics, the performance of the HDR image in terms of\nimperceptibility is determined by VDP which is defined as:\nHDR−VDP = 1\nF.O\nFX\nf =1\nOX\no=1\nwf log\n \n1\nM\nMX\nm=1\nD2\np [f , o](m) + ε\n!\n(54)\nWhere wf represent vector of per-band pooling weights de-\ntermined by maximizing correlations with subjective opinion\nscores, m is the pixel index, M is the total number of pixels,\nDp denotes the noise-normalized difference the f th spatial\nfrequency (f = 1toF) band and (f = 1 to F) band and oth\norientation (o = 1 to O) of the steerable pyramid of the\noriginal and watermarked image, ε = 10−5 is a constant to\navoid singularities when Dp is close to 0.\n2) Robustness\nUsing Normalised Correlation Coefficient (NCC) and Bit\nError Rate (BER), which have a range of 0 to 1, are used\nto calculate the robustness performance of the embedded\nwatermark.\n1) Normalized Correlation coefficient [30] This metric\nmeasures the correlation among the extracted water-\nmark and the original watermark. When the values are\ncloser to 1, then the images are highly correlated (i.e.,\nrobust) else, it is uncorrelated (i.e., Less robust). NCC\nmetric is defined as:\nNCC =\n(ΣnL\ni=1ΣnK\nj=1(|fi + Efi |⧸2)\nnMN\n(55)\nwhere W and EW represents the original and extracted\nwatermark and nMN dimension, (i, j) represent the po-\nsition of a pixel in the image.\n2) Bit error rate (BER) [30]: The BER performance is\ncalculated by dividing the total number of embedded\nwatermark bits by the number of watermark bits that\nwere extracted incorrectly. There won’t be an error if it\nranges closer to 0, otherwise numbers will be closer to\n1 which is represented as,\nBER = NE rr\nNBit\n× 100 (56)\nD. PERFORMANCE ANALYSIS\nUnauthorized users may intentionally or unintentionally tam-\nper with the watermarked image ’ WM ’ at the receiver side\nduring an unsecured network transmission. The quality degra-\ndation between the recovered image and the watermarked\nimage on the sample test images for various attacks, such as\n(i) Zero or No attack, (ii) Unintentional attack, such as Noise\nattacks, geometrical attacks, and image processing operations\nlike histogram equalization, sharpening attack, and JPEG\ncompression, is used to assess the watermark features ability\nof the Swin transformer-based watermark. (iii) Copy-move\nforgeries, image splicing, and content removal attacks are\ninstances of intentional attacks.\n1) Zero or No attack\nThe imperceptibility metric is used to calculate the efficiency\nof the suggested approach in terms of quality deterioration\nwhen the watermark is embedded for the zero attacked water-\nmarked image. The watermarked images are measured using\nPSNR and SSIM metric, and also the HDR watermarked\nimage similarity is additionally measured using HDR-VDP2\n(High dynamic range-Visual Difference Predictor). Figure.\n(6) clears that the highest PSNR value achieved is 68.75\ndB, the maximum SSIM value obtained is 0.999, whereas\nmaximum VDP-quality achieved is 94.76.\n2) Image Processing attacks\nThe performance of the proposed approach for HDR and\nRGB watermarked images evaluated for various image pro-\n14 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 6. Performance evaluation of Zero attack on watermarked test\nimages using PSNR, SSIM and HDR-VDP\ncessing attacks in order to estimate imperceptibility and ro-\nbustness efficiency for various image tampering ratios.\nImage processing attacks are namely: (1) Noise attacks, (2)\nFiltering attacks, and (3) Common attacks. The noise attacks\nare categorized as salt and pepper noise (S& P), Gaussian\nnoise (GN), and Speckle noise (SN) tested for various density\nand variance values of 0.02, 0.05, 0.1. Filtering attacks such as\nGaussian filter, Median filter, and average filter are evaluated\nfor various filter size of 3 Ö 3, 5 Ö 5, 7 Ö 7 and 9 Ö 9. The\ncommon attacks with various parameters namely sharpening\nattack with tampering ratio of 10%, 20%, 40%, and 50%,\nhistogram attacks with tampering ratio of 10%, 50% and the\nJPEG compression attacks with varying quality factor of 10%,\n50%, 70%, and 90%. The imperceptibility performance of the\nproposed algorithm for unintentional attack is evaluated by\nPSNR, SSIM, and HDR-VDP metrics (between Watermarked\nand Recovered image) and the results are shown in Figures\n(7), (8) and (9).\nIt is observed from the Fig. (7), (8) and (9) that, the per-\nformance of noise attack for color image as achieved good\nresult than the HDR images with a maximum PSNR value\nof 67.99 dB and HDR-VDP resulting in 90.38. Whereas\nthe filtering, histogram, sharpening and JPEG compression\nattack is evaluated on sample test image via various filter\nsizes, yielding high fidelity with average PSNR values of 64\ndB and HDR-VDP of 85.\n3) Geometric Attacks\nThe proposed systems performance against RST attacks is ex-\namined, to evaluate the robustness of the watermark in terms\nof image recovery. Intentional geometrical attacks are namely\nrotational attacks with tampering ratios of 10◦, 20◦, 40circ,\nand 60◦, scaling attack with tampering ratios of 25%, 50%,\n75%, and 125%, and translation attacks with different axis\nranges of Tx=-25, Ty=20, Tx =50, Ty= -50, Tx=45, Ty=-25,\nand Tx= -50, Ty=50 respectively. The watermarked image\nis tampered intentionally using MATLAB command, where\nthe performance is computed using multiple metrics and the\nresults are depicted in Fig. (10). The system shown better\nimperceptibility performance with the highest PSNR values\nof 63.5 dB, SSIM value of 0.999, and an average HDR-VDP\nof 92.\n4) Intentional attacks\nTamper detection and localization efficiency for the proposed\nmodel is evaluated by intentionally tampering the water-\nmarked image. The intentional attacks are namely content\nremoval, copy-move forgery, content removal, and Image\nSplicing. From Fig. (11) it is inferred that the proposed\nwork is highly efficient in tamper localization and the deep\nglobal swin features maps are highly capable in recovering\nthe image. The proposed system performed better for content\nremoval and copy-move attack. For image splicing attack,\nthe fidelity performance of HDR image achieved better result\nwhen compared with RGB image. From figure (6) - (11), it\nis inferred that the proposed algorithm significantly outper-\nforms for various attacks.\nE. ROBUSTNESS ANALYSIS\nThe swin watermark robustness efficiency is evaluated for\nvarious attacks where the performance of the extracted water-\nmark from the tampered image is calculated using Normal-\nized correlation coefficient (NCC) and Bit error rate (BER)\nmetric and the results obtained is mentioned in Fig. (12) and\n(13). In the proposed work, swin features maps are embed-\nded as watermark in quaternion transform of RGB channel\nwhich not only guarantee the imperceptibility of watermarked\nimage and also achieve higher robustness. Moreover, in the\nadaptive less sensitive region the watermark information are\neffectively dispersed in all the three RGB channels of the\nhost image, thus the robustness as well as the payload of the\nproposed model is improved.\nV. COMPARATIVE ANALYSIS\nThis section conducts a comparative analysis between the\nsuggested algorithm and the state-of-the-art watermarking\nsystems to demonstrate the endurance of the proposed algo-\nrithm. For a fair comparison, the performance of the sug-\ngested (i) color image watermarking was evaluated using\nReversible multi-watermarking for color images by Ying Sun\net al. 2023 [32], (ii) HDR image watermarking using Artificial\nbee colony and tucker decomposition by Yazdan et al. 2018\n[7] and Mei et al. 2019 [10], (iii) tamper detection and re-\ncovery using convolutional attention-based turtle shell matrix\nby Aberna et al 2024 [37] and QDFT and tamper ranking\nby Junlin et al 2023 [14], and (iv) optimization using LWT-\nDCT-SVD and DWTDCT-SVD based watermarking schemes\nusing jaya and particle swarm optimization by Divyanshu et\nal. 2022 [24].\nTable (1) and Table (2) depict the comparative results of\nthe proposed Vs state-of-the-art systems in terms of BER and\nNCC values. When compared to state-of-the-art systems, the\nproposed model attained a greater performance in terms of\nNCC and BER values because of the adaptive embedding\nregion and global deep watermark features. It is also observed\nthat the proposed system lags robustness for fewer attacks\nthan the systems [32] and [24] in Table (1) and Table (2)\nVOLUME 11, 2023 15\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 7. Performance evaluation of Noise attacks on watermarked test images using PSNR, SSIM and HDR-VDP\nrespectively.\nAdditionally, to validate the efficiency of the suggested\nwork on HDR images, a fair comparison is conducted using\nbenchmark HDR images by replicating the existing Tucker\ndecomposition technique [10] and artificial bee colony ap-\nproach [7] for various intentional and unintentional attacks. In\n[10] extensive evaluations on several HDR images made by\nencoding two widely-used transfer functions (TFs) confirm\nthe strong HVS-imperceptibility capabilities of the method,\nas well as the robustness of the embedded watermarks to tone\nmapping, lossy compression, and common signal processing\noperations. Table (3) depicts the robustness performance of\nthe Proposed HDR image against conventional systems [7],\n[10] in terms of BER. From Table (3), we infer that the\nproposed system shows better BER performance than the\n16 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFigure 8:Contd....\nstate-of-the-art methods except for image 3.\nThe payload characteristics of the proposed and the state-of-\nthe-art systems are measured as a number of watermark bits\nembedded in the original image pixel. In addition to that, the\ntrade-off between imperceptibility and robustness for various\npayload values is measured using PSNR Vs. payload and\nit is tabulated in Table 4. It is inferred from Table (4) that\nthe performance of the proposed system shows better PSNR\nresults for various payload values.\nVI. CONCLUSION AND FUTURE WORK\nThe growth of photographic technology has increased in real-\nworld scenarios that resulted in high-quality images. Also, the\nuncontrollable multimedia image manipulation insists on the\nneed for secured content authentication and tamper detection\nsystems. Much research suggested a successful model for\ncolor image content authentication and tamper detection sys-\ntems, but limited research has been carried out on HDR im-\nages for tampering attacks. Considering the above-mentioned\ndifficulties, we proposed an adaptive semi-blind watermark-\nVOLUME 11, 2023 17\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nTABLE 1. BER comparison of Proposed Vs Existing Watermarking Systems for color images\nAttacks & Index Image BER (in %)\nYing Sun et al.\n[32]\nAberna et al.\n[37]\nJunlin O et al.\n[14]\nAwasthi D et al.\n[24] Proposed\nSalt and Pepper (density=0.02)\n3\n0.0031 0.072 0.161 0.002 0.016\nSalt and Pepper (density=0.05) 0.061 0.068 0.106 0.219 0.0065\nGaussian noise (variance=0.02) 0.0281 0.81 0.141 0.015 0.082\nGaussian noise (variance=0.05) 0.0682 0.126 0.019 0.085 0.082\nSpeckle Noise (density=0.02) 0.112 0.012 0.120 0.1108 0.102\nSpeckle Noise (density=0.05) 0.013 0.167 0.1142 0.22 0.241\nMedian filter size =3*3 0.072 0.03 0.133 0.146 0.0129\nMedian filter Size=5*5 0.0186 0.28 0.170 0.191 0.0891\nAverage filter size = 3*3 0.150 1.02 0.148 0.221 0.0183\nAverage filter size = 5*5 0.342 1.03 0.0127 0.075 0.159\nGaussian filter size = 3*3 0.618 0.812 0.074 0.028 0.0216\nGaussian filter size = 5*5\n7\n1.024 0.09 0.16 0.0059 0.240\nJPEG with QF = 20 0.341 0.0254 0.0463 0.18 1.09\nRotation _10º 1.062 0.64 0.084 0.12 0.053\nCopy-move_20% 0.24 0.607 0.022 0.0259 0.051\nScaling_25% 0.36 0.093 0.0124 0.0821 0.0124\nImage Splicing _20% 0.012 0.580 0.146 0.1406 0.098\nContent Removal 40% 0.0286 0.35 0.33 0.144 0.1012\nTABLE 2. NCC comparison of Proposed Vs Existing watermarking Systems for color images\nAttacks & Index Image NCC\nYing Sun et al.\n[32]\nAberna et al.\n[37]\nJunlin O et al.\n[14]\nAwasthi D et al.\n[24] Proposed\nSalt and Pepper (density=0.02)\n3\n0.9958 0.999 0.9992 0.9926 0.9999\nSalt and Pepper (density=0.05) 0.9711 0.9998 0.9899 0.9884 0.9923\nGaussian noise (variance=0.02) 0.9621 0.99 0.9882 0.9795 0.9996\nGaussian noise (variance=0.05) 0.9479 0.9981 0.9806 0.9681 0.9902\nSpeckle Noise (density=0.02) 0.9845 0.986 0.9782 0.9601 0.9969\nSpeckle Noise (density=0.05) 0.9120 0.972 0.9786 0.9542 0.9892\nMedian filter size =3*3 0.9918 0.973 0.9726 0.9654 0.9899\nMedian filter Size=5*5 0.8788 0.97 0.9733 0.9542 0.9810\nAverage filter size = 3*3 0.9917 0.98 0.9814 0.9588 0.9827\nAverage filter size = 5*5 0.9482 0.98 0.9891 0.9782 0.9791\nGaussian filter size = 3*3\n7\n0.9925 0.9854 0.9790 0.9706 0.9848\nGaussian filter size = 5*5 0.9867 0.9802 0.9709 0.9786 0.9809\nJPEG with QF = 20 0.9735 0.9781 0.99811 0.9980 0.9857\nRotation _10º 0.9975 0.98 0.9789 0.9706 0.9932\nCopy-move_20% 0.9587 0.991 0.9621 0.9647 0.9981\nScaling_25% 0.9969 0.9862 0.9601 0.9768 0.9862\nImage Splicing _20% 0.9862 0.9813 0.9962 0.9752 0.9847\nContent Removal 40% 0.9721 0.9887 0.9893 0.9626 0.9821\nTABLE 3. BER comparison of Proposed Vs Existing systems on HDR images\nIndex Attacks Image 3 Image 5 Image 7\n[7] [10] Proposed [7] [10] Proposed [7] [10] Proposed\n[1] Salt and Pepper (density=0.02) 0.35 0.082 0.006 0.26 0.068 0.012 0.47 0.0921 0.0182\n[3] Gaussian noise (variance=0.02) 0.91 0.0629 0.0028 0 0.0436 0.028 0.68 0.0726 0.0355\n[5] Speckle Noise (density=0.02) 0.026 0.105 0.021 2.81 0.0242 0.0281 0.21 0.0802 0.0926\n[7] Median filter size =3*3 1.18 0.011 0.0129 3.41 0.019 0.016 0.94 0.079 0.056\n[9] Average filter size = 3*3 0.98 0.121 0.0183 0.71 0.13 0.019 0.65 0.093 0.034\n[11] Gaussian filter size = 3*3 0.78 0.160 0.211 1.04 0.056 0.092 3.82 0.069 0.102\n[13] JPEG with QF 20% 0.7 0.142 1.09 2.79 0.1702 1.009 0.8 0.18 0.189\n[16] Rotation (20º ) 0.66 0.0689 0.056 1.13 0.0569 0.049 0.09 0.069 0.0759\n[19] Scaling (50%) 2.97 0.0094 0.0128 1.72 0.0183 0.0093 2.07 0.0742 0.0261\n[21] Translation (Tx=-25,Ty= 20) 0.47 0.0897 0.0142 0.71 0.0249 0.0136 1.19 0.028 0.0138\n[23] Copy-move (20%) 0.23 0.7056 0.8204 0.35 0.9321 0.81 2.42 0.83 0.829\n[25] Image Splicing (20%) 0.28 0.0567 0.0114 0.62 0.0142 0.914 0.92 0.1114 0.95\n[27] Content Removal (20%) 0.6 0.027 0.416 0.18 0.087 0.426 0.48 0.0917 0.421\n18 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 8. Performance evaluation of filtering attacks on watermarked test images using PSNR, SSIM and HDR-VDP\nTABLE 4. Comparative Analysis of PSNR Vs Payload among Proposed and State-of-the-art Systems\nImages Ying sun et al., [32]\nMax 17,000 bits\nAbernal et al., [37]\nNearly 65,000 bits\nAwasthi D et al., [24]\nNearly 65,000 bits\nProposed\nNearly 786,432 bits\n1 38.8817 45.5728 60.32 64.72\n2 36.0018 42.15 61.09 63.18\n3 37.881 43.817 59.98 65.94\n4 34.2896 39.98 62.01 64.801\n5 35.872 43.0124 62.41 64.92\n6 36.8859 42.897 60.106 63.64\n12 36.461 36.292 59.806 62.07\n13 34.6567 44.87 58.82 63.81\n14 33.2016 38.6015 623.3 63.21\n15 34.5178 40.869 61 66.12\n16 34.102 40.415 60.9899 64.84\n17 34.8523 42.783 61.29 62.013\n18 33.7719 43.689 60.77 61.69\ning system for HDR and RGB images using quaternion Dual-\nTree complex wavelet transform for content authentication,\nproof of ownership, and tamper detection application using\nthe Swin transformer model. This work focuses mainly on a\nrobust watermark and an optimal secured authentic system\nfor various intentional and unintentional attacks. For the first\ntime, we have evaluated the Swin transformer-based feature\ngeneration model for the first time on the HDR and RGB\nimage datasets.\nWith the use of a pixel correlation-based graph entropy sys-\ntem, the optimal embedding region is identified using the\nMaximal Entropy Random Walk approach, which strikes a\nbalance between robustness and imperceptibility. The less\nsensitive Quaternion Dual-Tree Complex Wavelet Transform\ncoefficient in all three channels, which not only demonstrated\nhigh robustness because of its shift-invariance and strong\ndirectionality property but also preserved the pixel correla-\ntion, was another way to improve system performance. Fur-\nthermore, the novel Quaternion Dual-Tree Complex Wavelet\nTransform (QGBT) yields superior tamper detection and lo-\ncalization results because of its strong perceptual features. In\naddition, to confirm the content authenticity, the dual scram-\nbled biometric watermark is integrated into the principal com-\nponents of the singular value decomposition matrix. Utilising\na gyrator transform to jumble the owner’s biometric image,\nthe protected authentication system is achieved while prevent-\ning outside parties from detecting or extracting the watermark\nfrom the image. Extraction of the watermark is possible, only\nVOLUME 11, 2023 19\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 9. Performance evaluation of common attacks on watermarked test images using PSNR, SSIM and HDR-VDP\nwhen the owner’s dual biometric image matches with the\nextracted watermark. To authenticate and detect tampering\nowner’s biometric image and the original watermarked image\nare required which is known to be a semi-blind watermarking\nsystem.\nThe suggested algorithm’s performance is assessed for a\nrange of intentional and unintentional attacks. The proposed\nalgorithm imperceptibility and robustness performance for\neach attack with varying tampering ratios is evaluated using\nquality metrics like PSNR, NCC, BER, SSIM, and HDR-\nVDP. With a maximum PSNR value of approximately 65 dB\nand an NCC of 0.999, it is evident from the result analysis\nthat, the suggested model outperformed the other state-of-\nthe-art systems in terms of imperceptibility and robustness.\nThe VDP metric was also used to measure the HDR quality\nvisualisation, and it achieved the highest value of 94.76, re-\n20 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 10. Performance evaluation of Geometric attacks on test images using PSNR, HDR-VDP and SSIM\nspectively. An adaptive, content authentication, tamper detec-\ntion, tamper localization and recovery, and proof of ownership\napplication were all successfully obtained by the suggested\nsystem. Also, the trade-off between watermarking properties\nis well balanced by an adaptable system. In the future, this\nsystem can be extended for real-time and high dynamic range\nvideos.\nREFERENCES\n[1] Mehmet Zeki Konyar and Serdar Solak. Efficient data hiding method\nfor videos based on adaptive inverted lsb332 and secure frame selection\nwith enhanced vigenere cipher. Journal of Information Security and\nApplications, 63:103037, 2021.\n[2] Mehmet Zeki Konyar and S Ozturk. Reed solomon coding-based medical\nimage data hiding method against salt and pepper noise. Symmetry,\n12(6):899, 2020.\n[3] Chung-Min Yu, Kuo-Chen Wu, and Chung-Ming Wang. A distortion-free\ndata hiding scheme for high dynamic range images. Displays, 32(5):225–\nVOLUME 11, 2023 21\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 11. Performance evaluation of intentional attacks on watermarked test images using PSNR, SSIM and HDR-VDP\n236, 2011.\n[4] Loganathan Agilandeeswari and K Ganesan. A robust color video water-\nmarking scheme based on hybrid embedding techniques. Multimedia Tools\nand Applications, 75:8745–8780, 2016.\n[5] Xinwei Xue, Masahiro Okuda, and Satoshi Goto. Bilateral filtering based\nwatermarking for high dynamic range image. In 2011 International\nSymposium on Intelligent Signal Processing and Communications Systems\n(ISPACS), pages 1–5. IEEE, 2011.\n[6] Chin-Chen Chang, Thai-Son Nguyen, and Chia-Chen Lin. A new\ndistortion-free data embedding scheme for high-dynamic range images.\nMultimedia Tools and Applications , 75:145–163, 2016.\n[7] F Yazdan Bakhsh and Mohsen Ebrahimi Moghaddam. A robust hdr images\nwatermarking method using artificial bee colony algorithm. Journal of\nInformation Security and Applications , 41:12–27, 2018.\n[8] Yun-Te Lin, Chung-Ming Wang, Wei-Sung Chen, Fang-Pang Lin, and\nWoei Lin. A novel data hiding algorithm for high dynamic range images.\nIEEE Transactions on Multimedia , 19(1):196–211, 2016.\n[9] Karina Ruby Perez-Daniel, Francisco Garcia-Ugalde, and Victor\nSanchez. Watermarking of hdr images in the spatial domain with\nhvs-imperceptibility. IEEE Access, 8:156801–156817, 2020.\n[10] Mei Yu, Yang Wang, Gangyi Jiang, Yongqiang Bai, and Ting Luo. High\ndynamic range image watermarking based on tucker decomposition. IEEE\nAccess, 7:113053–113064, 2019.\n[11] Ahmed Khan, Minoru Kuribayashi, KokSheik Wong, and Vishnu\nMonn Baskaran. Hdr image watermarking using saliency detection and\nquantization index modulation. arXiv e-prints, pages arXiv–2302, 2023.\n[12] Junlin Ouyang, Gouenou Coatrieux, Beijing Chen, and Huazhong Shu.\nColor image watermarking based on quaternion fourier transform and im-\nproved uniform log-polar mapping. Computers & Electrical Engineering ,\n46:419–432, 2015.\n[13] Sezgin Kacar, Mehmet Zeki Konyar, and Ünal Çavuşoğlu. 4d chaotic\nsystem-based secure data hiding method to improve robustness and embed-\n22 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 12. Robustness performance in terms of NCC and BER against various attacks on HDR Images\nVOLUME 11, 2023 23\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nFIGURE 13. Robustness performance in terms of NCC and BER against various attacks on RGB Images\n24 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\nding capacity of videos. Journal of Information Security and Applications ,\n71:103369, 2022.\n[14] Junlin Ouyang, Jingtao Huang, and Xingzi Wen. A semi-fragile reversible\nwatermarking method based on qdft and tamper ranking. Multimedia Tools\nand Applications, pages 1–24, 2023.\n[15] Junlin Ouyang, Jingtao Huang, Xingzi Wen, and Zhuhong Shao. A semi-\nfragile watermarking tamper localization method based on qdft and multi-\nview fusion. Multimedia Tools and Applications , 82(10):15113–15141,\n2023.\n[16] Adnan Mustafa Cheema, Syed Muhammad Adnan, and Zahid Mehmood.\nA novel optimized semi-blind scheme for color image watermarking. IEEE\nAccess, 8:169525–169547, 2020.\n[17] P Aberna, L Agilandeeswari, and A Bansal. Vision transformer-based\nwatermark generation for authentication and tamper detection using schur\ndecomposition and hybrid transforms. Int J Comput Inf Syst Ind Manag\nAppl, 15:107–121, 2023.\n[18] Aberna Palani and Agilandeeswari Loganathan. Multi-image feature map-\nbased watermarking techniques using transformer. Int J Electr Electron\nRes, 11:339–344, 2023.\n[19] L Agilandeeswari, M Prabukumar, and Farhan A Alenizi. A robust\nsemi-fragile watermarking system using pseudo-zernike moments and dual\ntree complex wavelet transform for social media content authentication.\nMultimedia Tools and Applications , pages 1–53, 2023.\n[20] Sang In Lee, Jun Young Park, and Il Kyu Eom. Cnn-based copy-move\nforgery detection using rotation-invariant wavelet feature. IEEE Access ,\n10:106217–106229, 2022.\n[21] Yang Liu and Mirella Lapata. Text summarization with pretrained en-\ncoders. arXiv preprint arXiv:1908.08345 , 2019.\n[22] Yong Chen, Zhigang Jia, Yan Peng, and Yaxin Peng. Robust dual-color\nwatermarking based on quaternion singular value decomposition. IEEE\nAccess, 8:30628–30642, 2020.\n[23] Loganathan Agilandeeswari and Kumaravel Muralibabu. A robust video\nwatermarking algorithm for content authentication using discrete wavelet\ntransform (dwt) and singular value decomposition (svd). International\nJournal of Security and Its Applications , 7(4):145–158, 2013.\n[24] Divyanshu Awasthi and Vinay Kumar Srivastava. Lwt-dct-svd and dwt-\ndct-svd based watermarking schemes with their performance enhancement\nusing jaya and particle swarm optimization and comparison of results under\nvarious attacks. Multimedia Tools and Applications , 81(18):25075–25099,\n2022.\n[25] L Agilandeeswari and K Ganesan. An efficient hilbert and integer wavelet\ntransform based video watermarking. Journal of Engineering Science and\nTechnology, 11(3):327–345, 2016.\n[26] Agilandeeswari Loganathan and Ganesan Kaliyaperumal. An adaptive\nhvs based video watermarking scheme for multiple watermarks using\nbam neural networks and fuzzy inference system. Expert Systems with\nApplications, 63:412–434, 2016.\n[27] Li Li, Rui Bai, Jianfeng Lu, Shanqing Zhang, and Ching-Chun Chang. A\nwatermarking scheme for color image using quaternion discrete fourier\ntransform and tensor decomposition. Applied Sciences, 11(11):5006, 2021.\n[28] Ling-Yuan Hsu and Hwai-Tsu Hu. Qdct-based blind color image water-\nmarking with aid of gwo and dncnn for performance improvement. IEEE\nAccess, 9:155138–155152, 2021.\n[29] Ming Yin, Wei Liu, Jun Shui, Jiangmin Wu, et al. Quaternion wavelet\nanalysis and application in image denoising. Mathematical Problems in\nEngineering, 2012, 2012.\n[30] Loganathan Agilandeeswari and K Ganesan. Rst invariant robust video\nwatermarking algorithm using quaternion curvelet transform. Multimedia\nTools and Applications , 77:25431–25474, 2018.\n[31] K Prabha, MJ Vaishnavi, and I Shatheesh Sam. Quaternion hadamard\ntransform and qr decomposition based robust color image watermarking.\nIn 2019 3rd International Conference on Trends in Electronics and Infor-\nmatics (ICOEI), pages 101–106. IEEE, 2019.\n[32] Ying Sun, Xiaochen Yuan, Xingrun Wang, and Jianqing Li. Reversible\nmulti-watermarking for color images with grayscale invariance. Multime-\ndia Tools and Applications , 82(11):16323–16342, 2023.\n[33] P Aberna and L Agilandeeswari. Digital image and video watermarking:\nmethodologies, attacks, applications, and future directions. Multimedia\nTools and Applications , pages 1–61, 2023.\n[34] Luo Yu, Yujin Zhang, Hua Han, Lijun Zhang, and Fei Wu. Robust median\nfiltering forensics by cnn-based multiple residuals learning. IEEE Access,\n7:120594–120602, 2019.\n[35] Yuan Rao and Jiangqun Ni. A deep learning approach to detection of\nsplicing and copy-move forgeries in images. In 2016 IEEE international\nworkshop on information forensics and security (WIFS) , pages 1–6. IEEE,\n2016.\n[36] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion\nJones, Aidan N Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention\nis all you need. Advances in neural information processing systems , 30,\n2017.\n[37] Aberna Palani and Agilandeeswari Loganathan. Semi-blind watermarking\nusing convolutional attention-based turtle shell matrix for tamper detec-\ntion and recovery of medical images. Expert Systems with Applications ,\n238:121903, 2024.\n[38] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weis-\nsenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, et al. An image is worth\n16x16 words: Transformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[39] Michael Yang. Visual transformer for object detection. arXiv preprint\narXiv:2206.06323, 2022.\n[40] Bowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chun-\nhua Shen, et al. Segvit: Semantic segmentation with plain vision transform-\ners. Advances in Neural Information Processing Systems , 35:4971–4982,\n2022.\n[41] Jinpeng Li, Yichao Yan, Shengcai Liao, Xiaokang Yang, and Ling Shao.\nLocal-to-global self-attention in vision transformers. arXiv preprint\narXiv:2107.04735, 2021.\n[42] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen\nLin, and Baining Guo. Swin transformer: Hierarchical vision transformer\nusing shifted windows. In Proceedings of the IEEE/CVF international\nconference on computer vision , pages 10012–10022, 2021.\n[43] Baoru Han, Han Wang, Dawei Qiao, Jia Xu, and Tianyu Yan. Application\nof zero-watermarking scheme based on swin transformer for securing\nthe metaverse healthcare data. IEEE Journal of Biomedical and Health\nInformatics, 2023.\n[44] Francesco Verdoja and Marco Grangetto. Graph laplacian for image\nanomaly detection. Machine Vision and Applications , 31(1-2):11, 2020.\n[45] Rada Mihalcea. Graph-based ranking algorithms for sentence extraction,\napplied to text summarization. In Proceedings of the ACL interactive poster\nand demonstration sessions , pages 170–173, 2004.\n[46] Wei Ju, Deihui Xiang, Bin Zhang, Lirong Wang, Ivica Kopriva, and Xinjian\nChen. Random walk and graph cut for co-segmentation of lung tumor\non pet-ct images. IEEE Transactions on Image Processing , 24(12):5854–\n5867, 2015.\n[47] Rong-Hua Li, Jeffrey Xu Yu, and Jianquan Liu. Link prediction: The\npower of maximal entropy random walk. page 1147–1156. Association\nfor Computing Machinery, 2011.\n[48] Viswanath Gopalakrishnan, Yiqun Hu, and Deepu Rajan. Random walks\non graphs for salient object detection in images. IEEE Transactions on\nImage Processing, 19(12):3232–3242, 2010.\n[49] Liantao Wang, Ji Zhao, Xuelei Hu, and Jianfeng Lu. Weakly supervised\nobject localization via maximal entropy random walk. In 2014 IEEE\nInternational Conference on Image Processing (ICIP) , pages 1614–1617,\n2014.\n[50] Jin-Gang Yu, Ji Zhao, Jinwen Tian, and Yihua Tan. Maximal entropy\nrandom walk for region-based visual saliency. IEEE Transactions on\nCybernetics, 44(9):1661–1672, 2014.\n[51] Paweł Korus and Jiwu Huang. Improved tampering localization in digital\nimage forensics based on maximal entropy random walk. IEEE Signal\nProcessing Letters, 23(1):169–173, 2016.\n[52] Chaoyan Huang, Juncheng Li, and Guangwei Gao. Review of quaternion-\nbased color image processing methods. Mathematics, 11(9), 2023.\n[53] Nick Kingsbury. Complex wavelets for shift invariant analysis and filtering\nof signals. Applied and Computational Harmonic Analysis , 10(3):234–\n253, 2001.\n[54] Beijing Chen, Gouenou Coatrieux, Gang Chen, Xingming Sun, Jean Louis\nCoatrieux, and Huazhong Shu. Full 4-d quaternion discrete fourier trans-\nform based watermarking for color images. Digital Signal Processing ,\n28:106–119, 2014.\n[55] Karen Panetta, Landry Kezebou, Victor Oludare, Sos Agaian, and Zehua\nXia. Tmo-net: A parameter-free tone mapping operator using generative\nadversarial network, and performance benchmarking on large scale hdr\ndataset. IEEE Access, 9:39500–39517, 2021.\n[56] Casia dataset, https://www.kaggle.com/datasets/sophatvathana/casia-\ndataset. 2023.\n[57] Fvc2002 - second international fingerprint verification competition,\nhttp://bias.csr.unibo.it/fvc2002/databases.asp. 2023.\nVOLUME 11, 2023 25\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nP Abernaet al.: Maximum Entropy Random Walk and Swin Transformer based Semi-Fragile Watermarking\n[58] Mmu iris dataset,https://www.kaggle.com/datasets/naureenmohammad/mmu-\niris-dataset. 2023.\nP ABERNAis pursuing her Ph.D. at the School of\nInformation Technology and Engineering, Vellore\nInstitute of Technology Vellore. She completed\nher M.Tech in Software Engineering from Vellore\nInstitute of Technology in the year 2018. Her re-\nsearch interest includes Digital Image Watermark-\ning, Image and Video Processing, Multimedia Se-\ncurity, Digital forensics, Machine Learning, and\nDeep Learning.\nL AGILANDEESWARI completed her Ph.D. and\nworking as a Professor in the Department of Soft-\nware Systems and Engineering, School of Com-\nputer Science Engineering and Information Sys-\ntems (SCORE), VIT Vellore. She received her\nBachelor’s degree in Information Technology and\nMaster’s in Computer Science and Engineering\nfrom Anna University in 2005 and 2009 respec-\ntively. She has around 19 years of teaching expe-\nrience and published 60+ papers in peer-reviewed\nreputed journals. Her reputed publications include research articles in peer-\nreviewed journals namely Expert Systems with Applications, Multimedia\nTools and Applications, International Journal of Remote Sensing, IEEE\nAccess, Journal of Ambient Intelligence and Humanized Computing, and\nJournal of Applied Remote Sensing indexing at Thomson Reuters with an\naverage impact factor of 5. She is a peer reviewer in journals including\nNeural Computing and Applications, IEEE Access, Pattern Recognition,\nInternational Journal of Remote Sensing, Array, Artificial Intelligence Re-\nview, Informatics in Medicine Unlocked, Neurocomputing, Computers, and\nElectrical Engineering, Journal of King Saud University– Computer and\nInformation Sciences, IET Review, and Journal of Engineering Science\nand Technology (JESTEC). She also published 20+ engineering books as\nper Anna University Syllabus. She is a lifetime member of the Computer\nSociety of India. She produced two Ph.D. candidates. Her areas of interest\ninclude Image and Video watermarking, Image and Video processing, Neural\nnetworks, Cryptography, Fuzzy logic, Machine learning, IoT, Information-\ncentric networks, and Remote sensing. She can be contacted by email:\nagila.l@vit.ac.in.\n26 VOLUME 11, 2023\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2024.3370411\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/",
  "topic": "Random walk",
  "concepts": [
    {
      "name": "Random walk",
      "score": 0.6354252696037292
    },
    {
      "name": "Digital watermarking",
      "score": 0.6302874088287354
    },
    {
      "name": "Computer science",
      "score": 0.5660747289657593
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.54267817735672
    },
    {
      "name": "Principle of maximum entropy",
      "score": 0.46578019857406616
    },
    {
      "name": "Transformer",
      "score": 0.41437828540802
    },
    {
      "name": "Artificial intelligence",
      "score": 0.31421616673469543
    },
    {
      "name": "Mathematics",
      "score": 0.21378052234649658
    },
    {
      "name": "Physics",
      "score": 0.1377100646495819
    },
    {
      "name": "Electrical engineering",
      "score": 0.12591847777366638
    },
    {
      "name": "Statistics",
      "score": 0.10784673690795898
    },
    {
      "name": "Engineering",
      "score": 0.1062520444393158
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}