{
    "title": "Few-Shot NLG with Pre-Trained Language Model",
    "url": "https://openalex.org/W2939363422",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1459615597",
            "name": "Chen Zhiyu",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Eavani, Harini",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2347345197",
            "name": "Chen Wenhu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2748213883",
            "name": "Liu Yin-yin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3091095485",
            "name": "Wang, William Yang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2127795553",
        "https://openalex.org/W2950726992",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2949547296",
        "https://openalex.org/W2463507112",
        "https://openalex.org/W2963217826",
        "https://openalex.org/W2604799547",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2785824987",
        "https://openalex.org/W2786660442",
        "https://openalex.org/W2143379389",
        "https://openalex.org/W2963592583",
        "https://openalex.org/W1710635016",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2159573465",
        "https://openalex.org/W2116716943",
        "https://openalex.org/W2523790121",
        "https://openalex.org/W2157812664",
        "https://openalex.org/W2915161943",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2946846126",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W2951714314"
    ],
    "abstract": "Neural-based end-to-end approaches to natural language generation (NLG) from structured data or knowledge are data-hungry, making their adoption for real-world applications difficult with limited data. In this work, we propose the new task of \\textit{few-shot natural language generation}. Motivated by how humans tend to summarize tabular data, we propose a simple yet effective approach and show that it not only demonstrates strong performance but also provides good generalization across domains. The design of the model architecture is based on two aspects: content selection from input data and language modeling to compose coherent sentences, which can be acquired from prior knowledge. With just 200 training examples, across multiple domains, we show that our approach achieves very reasonable performances and outperforms the strongest baseline by an average of over 8.0 BLEU points improvement. Our code and data can be found at \\url{https://github.com/czyssrs/Few-Shot-NLG}",
    "full_text": "Few-Shot NLG with Pre-Trained Language Model\nZhiyu Chen1, Harini Eavani2, Wenhu Chen1, Yinyin Liu2, and William Yang Wang1\n1University of California, Santa Barbara\n2Intel AI\n{zhiyuchen, wenhuchen, william}@cs.ucsb.edu, {harini.eavani, yinyin.liu}@intel.com\nAbstract\nNeural-based end-to-end approaches to natural\nlanguage generation (NLG) from structured\ndata or knowledge are data-hungry, making\ntheir adoption for real-world applications dif-\nﬁcult with limited data. In this work, we pro-\npose the new task offew-shot natural language\ngeneration. Motivated by how humans tend to\nsummarize tabular data, we propose a simple\nyet effective approach and show that it not only\ndemonstrates strong performance but also pro-\nvides good generalization across domains. The\ndesign of the model architecture is based on\ntwo aspects: content selection from input data\nand language modeling to compose coherent\nsentences, which can be acquired from prior\nknowledge. With just 200 training examples,\nacross multiple domains, we show that our ap-\nproach achieves very reasonable performances\nand outperforms the strongest baseline by an\naverage of over 8.0 BLEU points improvement.\nOur code and data can be found at https:\n//github.com/czyssrs/Few-Shot-NLG\n1 Introduction\nNatural language generation (NLG) from struc-\ntured data or knowledge (Gatt and Krahmer,\n2018) is an important research problem for vari-\nous NLP applications. Some examples are task-\noriented dialog, question answering (He et al.,\n2017; Ghazvininejad et al., 2018; Su et al., 2016;\nSaha et al., 2018; Yin et al., 2016) and interdis-\nciplinary applications such as medicine (Hasan\nand Farri, 2019; Cawsey et al., 1997) and health-\ncare (Hasan and Farri, 2019; DiMarco et al., 2007).\nThere is great potential to use automatic NLG sys-\ntems in a wide range of real-life applications. Re-\ncently, deep neural network based NLG systems\nhave been developed, such as those seen in the\nE2E challenge (Novikova et al., 2017), WEATHER -\nGOV (Liang et al., 2009), as well as more complex\nones such as WIKI BIO (Liu et al., 2018) and RO-\nTOWIRE (Wiseman et al., 2017). Compared to\ntraditional slot-ﬁlling pipeline approaches, such\nneural-based systems greatly reduce feature engi-\nneering efforts and improve text diversity as well\nas ﬂuency.\nAlthough they achieve good performance on\nbenchmarks such as E2E challenge (Novikova\net al., 2017) and WIKI BIO (Lebret et al., 2016),\ntheir performance depends on large training\ndatasets, e.g., 500k table-text training pairs for\nWIKI BIO (Lebret et al., 2016) in a single domain.\nSuch data-hungry nature makes neural-based NLG\nsystems difﬁcult to be widely adopted in real-world\napplications as they have signiﬁcant manual data\ncuration overhead. This leads us to formulate an\ninteresting research question:\n1. Can we signiﬁcantly reduce human\nannotation effort to achieve reasonable\nperformance using neural NLG models?\n2. Can we make the best of generative\npre-training, as prior knowledge, to gen-\nerate text from structured data?\nMotivated by this, we propose the new task of few-\nshot natural language generation : given only a\nhandful of labeled instances (e.g., 50 - 200 train-\ning instances), the system is required to produce\nsatisfactory text outputs (e.g., BLEU ≥20). To\nthe best of our knowledge, such a problem in NLG\ncommunity still remains under-explored. Herein,\nwe propose a simple yet very effective approach\nthat can generalize across different domains.\nIn general, to describe information in a table,\nwe need two skills to compose coherent and faith-\nful sentences. One skill is to select and copy fac-\ntual content from the table - this can be learned\nquickly by reading a handful of tables. The other\nis to compose grammatically correct sentences that\nbring those facts together - this skill is not re-\narXiv:1904.09521v3  [cs.CL]  19 Apr 2020\nInput Table\nAttribute (R) Value (V)\nName Walter ExtraNationality GermanOccupation Aircraft designerand manufacturer... ...\nTable encoder\nAttention weights\nWalter  Extra     is     ...\nPre-trained Language Model\nWalter   Extra   German \nname    name   nationaltily\ntable values\nattribute names\nposition information\n Walter  Extra     is        a     …\n ...\nThe swicth \npolicy\n name   name     --        --     ...\n                           --        --     ...\nMatching\nFigure 1: Overview of our approach: Under the base framework with switch policy, the pre-trained language model serves as\nthe generator. We follow the same encoder as in (Liu et al., 2018). The architecture is simple in terms of both implementation\nand parameter space that needs to be learned from scratch, which should not be large given the few-shot learning setting.\nstricted to any domain. One can think of a latent\n“switch” that helps us alternate between these two\nskills to produce factually correct and coherent\nsentences. To do this, we use the pre-trained lan-\nguage model (Chelba et al., 2013; Radford et al.,\n2019) as the innate language skill, which provides\nstrong prior knowledge on how to compose ﬂu-\nent and coherent sentences. The ability to switch\nand select/copy from tables can be learned success-\nfully using only a few training instances, freeing\nthe neural NLG model from data-intensive train-\ning. Previous best performing methods based on\nlarge training data, such as (Liu et al., 2018), which\ndoes not apply such switch mechanism but trains\na strong domain-speciﬁc language model, perform\nvery poorly under few-shot setting.\nSince we are operating under a highly data-\nrestricted few-shot regime, we strive for simplicity\nof model architecture. This simplicity also implies\nbetter generalizability and reproducibility for real-\nworld applications. We crawl multi-domain table-\nto-text data from Wikipedia as our training/test\ninstances. With just 200 training instances, our\nmethod can achieve very reasonable performance.\nIn a nutshell, our contributions are summarized\nas the following:\n•We propose the new research problem of few-\nshot NLG, which has great potential to beneﬁt\na wide range of real-world applications.\n•To study different algorithms for our proposed\nproblem, we create a multi-domain table-to-\ntext dataset.\n•Our proposed algorithm can make use of the\nexternal resources as prior knowledge to sig-\nniﬁcantly decrease human annotation effort\nand improve the baseline performance by an\naverage of over 8.0 BLEU on various do-\nmains.\n2 Related Work\n2.1 NLG from Structured Data\nAs it is a core objective in many NLP applications,\nnatural language generation from structured data/-\nknowledge (NLG) has been studied for many years.\nEarly traditional NLG systems follow the pipeline\nparadigm that explicitly divides generation into\ncontent selection, macro/micro planning and sur-\nface realization (Reiter and Dale, 1997). Such a\npipeline paradigm largely relies on templates and\nhand-engineered features. Many works have been\nproposed to tackle the individual modules, such\nas (Liang et al., 2009; Walker et al., 2001; Lu et al.,\n2009). Later works (Konstas and Lapata, 2012,\n2013) investigated modeling context selection and\nsurface realization in an uniﬁed framework.\nMost recently, with the success of deep neural\nnetworks, data-driven, neural based approaches\nhave been used, including the end-to-end meth-\nods that jointly model context selection and sur-\nface realization (Liu et al., 2018; Wiseman et al.,\n2018; Puduppully et al., 2018). Such data-driven\napproaches achieve good performance on several\nbenchmarks like E2E challenge (Novikova et al.,\n2017), WebNLG challenge (Gardent et al., 2017)\nand WIKI BIO (Lebret et al., 2016). However, they\nrely on massive amount of training data. ElSahar\net al. (2018) propose zero-shot learning for ques-\ntion generation from knowledge graphs, but their\nwork applies on the transfer learning setting for\nunseen knowledge base types, based on seen ones\nand their textual contexts, which still requires large\nin-domain training dataset. This is different from\nour few-shot learning setting. Ma et al. (2019)\npropose low-resource table-to-text generation with\n1,000 paired examples and large-scale target-side\nexamples. In contrast, in our setting, only tens to\nhundreds of paired training examples are required,\nmeanwhile without the need for any target exam-\nples. This is especially important for real-world use\ncases where such large target-side gold references\nare mostly hard to obtain. Therefore, our task is\nmore challenging and closer to real-world settings.\n2.2 Large Scale Pre-Trained Models\nMany of the current best-performing methods for\nvarious NLP tasks adopt a combination of pre-\ntraining followed by supervised ﬁne-tuning, using\ntask-speciﬁc data. Different levels of pre-training\ninclude word embeddings (Mikolov et al., 2013;\nPennington et al., 2014; Peters et al., 2018), sen-\ntence embeddings (Le and Mikolov, 2014; Kiros\net al., 2015), and most recently, language model-\ning based pre-training like BERT (Devlin et al.,\n2018) and GPT-2 (Radford et al., 2019). Such\nmodels are pre-trained on large-scale open-domain\ncorpora, and provide down-streaming tasks with\nrich prior knowledge while boosting their perfor-\nmance. In this paper, we adopt the idea of em-\nploying a pre-trained language model to endow\nin-domain NLG models with language modeling\nability, which cannot be well learned from few shot\ntraining instances.\n3 Method\n3.1 Problem Formulation\nWe are provided with semi-structured data: a table\nof attribute-value pairs {Ri : Vi}n\ni=1. Both Ri\nand Vi can be either a string/number, a phrase or a\nsentence. Each value is represented as a sequence\nof words Vi = {vj}m\nj=1. For each word vj, we have\nits corresponding attribute name Ri and position\ninformation of the word in the value sequence. The\ntarget is to generate a natural language description\nbased on the semi-structured data, provided with\nonly a handful of training instances.\n3.2 Base Framework with Switch Policy\nWe start with the ﬁeld-gated dual attention model\nproposed in (Liu et al., 2018), which achieves\nstate-of-the-art performance (BLEU) on WIKI BIO\ndataset. Their method uses an LSTM decoder with\ndual attention weights. We ﬁrst apply a switch pol-\nicy that decouples the framework into table content\nselection/copying and language model based gener-\nation. Inspired by the pointer generator (See et al.,\n2017), at each time step, we maintain a soft switch\npcopy to choose between generating from softmax\nover vocabulary or copying from input table val-\nues with the attention weights as the probability\ndistribution.\npcopy = sigmoid(Wcct + Wsst + Wxxt + b)\nWhere ct = ∑\ni ai\nthi, {hi}is the encoder hid-\nden states, xt,st,at is the decoder input, state\nand attention weights respectively at time step t.\nWc,Ws,Wx and bare trainable parameters.\nThe pointer generator learns to alternate between\ncopying and generating based on large training\ndata and shows its advantage of copying out-of-\nvocabulary words from input. In our task, the train-\ning data is very limited, and many of the table\nvalues are not OOV . We need to explicitly “teach”\nthe model where to copy and where to generate.\nTherefore, to provide the model accurate guidance\nof the behavior of the switch, we match the target\ntext with input table values to get the positions of\nwhere to copy. At these positions, we maximize the\ncopy probability pcopy via an additional loss term.\nOur loss function:\nL= Lc + λ\n∑\nwj∈m\nm∈{Vi}\n(1 −pj\ncopy)\nWhere Lc is the original loss between model out-\nputs and target texts. wj is the target token at po-\nsition j, {Vi}is the input table value list deﬁned\nin Section 3.1, and mmeans a matched phrase. λ\nis hyperparameter as the weight for this copy loss\nterm. We also concatenate the decoder input with\nits matched attribute name and position information\nin the input table as xt to calculate pcopy .\n3.3 Pre-Trained LM as Generator\nWe use a pre-trained language model as the genera-\ntor, serving as the “innate language skill”. Due\nto the vocabulary limitation of few training in-\nstances, we leave the pre-trained word embedding\nﬁxed while ﬁne-tuning other parameters of the pre-\ntrained language model, so that it can generalize\nwith tokens unseen during training.\nFigure 1 shows our model architecture. We use\nthe pre-trained language model GPT-21 proposed\nin (Radford et al., 2019), which is a 12-layer trans-\nformer. The ﬁnal hidden state of the transformer\nis used to calculate attention weights and the copy\n1https://github.com/openai/gpt-2\nDomain Humans Books Songs\n# of training instances - 50 100 200 500 - 50 100 200 500 - 50 100 200 500\nTemplate 16.3 - - - - 25.6 - - - - 30.1 - - - -\nBase-original - 2.2 3.7 4.9 5.1 - 5.8 6.1 7.4 6.7 - 9.2 10.7 11.1 11.3\nBase - 2.9 5.1 6.1 8.3 - 7.3 6.8 7.8 8.8 - 10.4 12.0 11.6 13.1\nBase + switch - 15.6 17.8 21.3 26.2 - 24.7 26.9 30.5 33.2 - 29.7 30.6 32.5 34.9\nBase + switch + LM-scratch - 6.6 11.5 15.3 18.6 - 7.1 9.2 14.9 21.8 - 11.6 16.2 20.6 23.7\nBase + switch + LM (Ours) -25.7 29.5 36.1 41.7 - 34.3 36.2 37.9 40.3 - 36.1 37.2 39.4 42.2\nTable 1: BLEU-4 results on three domains. Base-original: the original method in (Liu et al., 2018); Base: applies pre-trained\nword embedding; Base+switch: adds the switch policy; Base+switch+LM-scratch: makes the same architecture as our method,\nbut trains the model from scratch without pre-trained weights for the generator. Template: manually crafted templates\nswitch pcopy. We ﬁrst feed the embedded attribute-\nvalue list serving as the context for generation. In\nthis architecture, the generator is ﬁne-tuned from\npre-trained parameters while the encoder and atten-\ntion part is learned from scratch, the initial geom-\netry of the two sides are different. Therefore we\nneed to apply larger weight to the copy loss pcopy,\nto give the model a stronger signal to “teach” it to\ncopy facts from the input table.\n4 Experiment\n4.1 Datasets and Experiment Setup\nThe original WIKI BIO dataset (Lebret et al., 2016)\ncontains 700k English Wikipedia articles of well-\nknown humans, with the Wiki infobox serving as\ninput structured data and the ﬁrst sentence of the\narticle serving as target text. To demonstrate gen-\neralizability, we collect datasets from two new do-\nmains: Books and Songs by crawling Wikipedia\npages. After ﬁltering and cleanup, we end up with\n23,651 instances for Books domain and 39,450 in-\nstances for Songs domain2. Together with the Hu-\nmans domain of the original WIKI BIO dataset, for\nall three domains we conduct experiments by vary-\ning the training dataset size to 50, 100, 200 and\n500. The rest of data is used for validation (1,000)\nand testing. The weight λof the copy loss term is\nset to 0.7. Other parameter settings can be found\nin Appendix A. To deal with vocabulary limitation\nof few-shot training, for all models we adopt the\nByte Pair Encoding (BPE) (Sennrich et al., 2016)\nand subword vocabulary in (Radford et al., 2019).\nWe compare the proposed method with other\napproaches investigated in Section 3, serving as\nthe baselines - Base-original: the original model\n2Note that the target text sometimes contains informa-\ntion not in the infobox. This is out of the scope of the few-\nshot generation in this work. Therefore we further ﬁlter the\ndatasets and remove the ones with rare words out of infobox.\nCheck (Dhingra et al., 2019) for a related study of this issue\non the WikiBio dataset\nin (Liu et al., 2018); Base: uses the same ar-\nchitecture, but in addition applies the pre-trained\nword embedding and ﬁx it during training; Base\n+ switch: adds the switch policy; Base + switch\n+ LM-scratch: makes the architecture same as\nour method, except training the model from scratch\ninstead of using pre-trained weights for generator.\nTemplate: template-based non-neural approach,\nmanually crafted for each domain.\n4.2 Results and Analysis\nFollowing previous work (Liu et al., 2018), we\nﬁrst conduct automatic evaluations using BLEU-\n4, shown in Table 1. The ROUGE-4 (F-measure)\nresults follow the same trend with BLEU-4 results,\nwhich we show in Appendix B.\nAs we can see, the original model Base-\noriginal (Liu et al., 2018), which obtains the state-\nof-the-art result on WIKI BIO full set, performs\nvery poorly under few-shot setting. It generates\nall tokens from softmax over vocabulary, which re-\nsults in severe overﬁtting with limited training data,\nand the results are far behind the template-based\nbaseline. With the switch policy,Base+switch ﬁrst\nbrings an improvement of an average of over 10.0\nBLEU points. This indicates that the content se-\nlection ability is easier to be learned with a hand-\nful of training instances. However, it forms very\nlimited, not ﬂuent sentences. With the augmenta-\ntion of the pre-trained language model, our model\nBase+switch+LM brings one more signiﬁcant im-\nprovement of an average over 8.0 BLEU points.\nWe provide sample outputs of these methods using\n200 training instances in Table 2.\nTable 3 shows the effect of the copy switch loss\npcopy introduced in Section 3.2, giving the model a\nstronger signal to learn to copy from input table.\nMa et al. (2019) propose the Pivot model, for\nlow-resource NLG with 1,000 paired examples and\nlarge-scale target-side examples. We compare our\nAttribute Value Attribute Value\nname andri ibo fullname andri ibo\nbirth date3 april 1990 birth place sentani , jayapura ,indonesia\nheight 173 cm currentclub persipura jayapura\npositiondefender ...\nGold Reference: andri ibo( bornapril 3 , 1990) is anindonesianfoot-\nballer who currently plays forpersipura jayapurain the indonesia superleague .\nGenerated texts of different methods\nBase: vasco emanuel freitas ( born december 20 , 1992 in kong kong ) is ahong kussian football player and currently plays for hong kong ﬁrst divisionleague side tsw pegasus .\nBase+switch: andri iboandri ibo (3 april 1990) is a international crick-eter .\nBase+switch+LM (Ours): andri ibo( born 3 april 1990) is\nan indonesianfootballdefender, who currently plays forpersipura jayapura .\nTable 2: A sample input table and generated summaries from\nthe test set of Humans domain, using 200 training instances\n# of training instances 50 100 200 500\nBase + switch + LM25.7 29.5 36.1 41.7\n- w/o copy losspcopy 21.4 25.5 31.3 38.0\nTable 3: Ablation study: Effect of the copy loss term on\nHumans domain, measured by BLEU-4. The loss term brings\nan average improvement of over 4.0 BLEU points.\nmethod with the Pivot model in table 4. Note that\nhere we train and evaluate the models on the orig-\ninal WikiBio dataset used in their work, in order\nto maintain the size of the target side examples for\ntheir settings.\n# of paired training instances 50 100 200 500 1000\nPivot 7.0 10.2 16.8 20.3 27.3\nOurs 17.2 23.8 25.4 28.6 31.2\nTable 4: Comparison with the Pivot model (Ma et al., 2019).\nCompared to their method using additional large-scale target\nside examples, our method requires no additional target side\ndata, while achieving better performance.\nHuman Evaluation\nWe also conduct human evaluation studies using\nAmazon Mechanical Turk, based on two aspects:\nFactual correctnessand Language naturalness. We\nevaluate 500 samples. Each evaluation unit is as-\nsigned to 3 workers to eliminate human variance.\nThe ﬁrst study attempts to evaluate how well the\ngenerated text correctly conveys information in the\ntable, by counting the number of facts in the text\nsupported by the table, and contradicting with or\nmissing from the table. The 2nd and 3rd columns\nof Table 5 show the average number of supporting\nand contradicting facts for our method, comparing\nto the strongest baseline and the gold reference.\nThe second study evaluates whether the generated\ntext is grammatically correct and ﬂuent, regard-\nless of factual correctness. We conduct pairwise\ncomparison among all methods, and calculate the\naverage times each method is chosen to be better\nthan another, shown in the 4th column of Table 5.\nOur method brings a signiﬁcant improvement over\nthe strongest baseline (p <0.01 in Tukey’s HSD\ntest for all measures). The copy loss term further\nalleviates producing incorrect facts. The language\nnaturalness result of our method without the copy\nloss is slightly better, because this evaluation does\nnot consider factual correctness; thus the generated\ntexts with more wrong facts can still get high score.\nSee Appendix C for more details of our evaluation\nprocedure.\n# Supp. # Cont. Lan. ScoreGold Reference 4.25 0.84 1.85\nBase + switch 2.57 2.17 0.93\nBase + switch + LM (ours)3.64 1.12 1.59- w/o copy losspcopy 3.54 1.30 1.63\nTable 5: Human evaluation results: Average number of sup-\nporting facts (column 2, the larger the better), contradicting\nfacts (column 3, the smaller the better), and language natural-\nness score (column 4, the larger the better).\n5 Conclusion\nIn this paper, we propose the new research problem\nof few-shot natural language generation. Our ap-\nproach is simple, easy to implement, while achiev-\ning strong performance on various domains. Our\nbasic idea of acquiring language modeling prior\ncan be potentially extended to a broader scope of\ngeneration tasks, based on various input structured\ndata, such as knowledge graphs, SQL queries, etc.\nThe deduction of manual data curation efforts for\nsuch tasks is of great potential and importance for\nmany real-world applications.\nAcknowledgment\nWe thank the anonymous reviewers for their\nthoughtful comments. We thank Shuming Ma for\nreleasing the processed data and code for the Pivot\nmodel. This research was supported by the Intel\nAI Faculty Research Grant. The authors are solely\nresponsible for the contents of the paper and the\nopinions expressed in this publication do not reﬂect\nthose of the funding agencies.\nReferences\nAlison J Cawsey, Bonnie L Webber, and Ray B Jones.\n1997. Natural language generation in health care.\nCiprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge,\nThorsten Brants, Phillipp Koehn, and Tony Robin-\nson. 2013. One billion word benchmark for measur-\ning progress in statistical language modeling. arXiv\npreprint arXiv:1312.3005.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nBhuwan Dhingra, Manaal Faruqui, Ankur P. Parikh,\nMing-Wei Chang, Dipanjan Das, and William W.\nCohen. 2019. Handling divergent reference texts\nwhen evaluating table-to-text generation. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 4884–4895. Association for Computa-\ntional Linguistics.\nChrysanne DiMarco, HDominic Covvey, D Cowan,\nV DiCiccio, E Hovy, J Lipa, D Mulholland, et al.\n2007. The development of a natural language gener-\nation system for personalized e-health information.\nIn Medinfo 2007: Proceedings of the 12th World\nCongress on Health (Medical) Informatics; Building\nSustainable Health Systems, page 2339. IOS Press.\nHady ElSahar, Christophe Gravier, and Fr ´ed´erique\nLaforest. 2018. Zero-shot question generation from\nknowledge graphs for unseen predicates and entity\ntypes. In Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, NAACL-HLT 2018, New Orleans, Louisiana,\nUSA, June 1-6, 2018, Volume 1 (Long Papers), pages\n218–228.\nClaire Gardent, Anastasia Shimorina, Shashi Narayan,\nand Laura Perez-Beltrachini. 2017. The webnlg\nchallenge: Generating text from RDF data. In Pro-\nceedings of the 10th International Conference on\nNatural Language Generation, INLG 2017, Santi-\nago de Compostela, Spain, September 4-7, 2017 ,\npages 124–133.\nAlbert Gatt and Emiel Krahmer. 2018. Survey of the\nstate of the art in natural language generation: Core\ntasks, applications and evaluation. Journal of Artiﬁ-\ncial Intelligence Research, 61:65–170.\nMarjan Ghazvininejad, Chris Brockett, Ming-Wei\nChang, Bill Dolan, Jianfeng Gao, Wen-tau Yih, and\nMichel Galley. 2018. A knowledge-grounded neural\nconversation model. In Proceedings of the Thirty-\nSecond AAAI Conference on Artiﬁcial Intelligence,\n(AAAI-18), the 30th innovative Applications of Arti-\nﬁcial Intelligence (IAAI-18), and the 8th AAAI Sym-\nposium on Educational Advances in Artiﬁcial Intel-\nligence (EAAI-18), New Orleans, Louisiana, USA,\nFebruary 2-7, 2018, pages 5110–5117.\nSadid A Hasan and Oladimeji Farri. 2019. Clini-\ncal natural language processing with deep learning.\nIn Data Science for Healthcare , pages 147–171.\nSpringer.\nHe He, Anusha Balakrishnan, Mihail Eric, and Percy\nLiang. 2017. Learning symmetric collaborative dia-\nlogue agents with dynamic knowledge graph embed-\ndings. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics, ACL\n2017, Vancouver, Canada, July 30 - August 4, Vol-\nume 1: Long Papers, pages 1766–1776.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nRyan Kiros, Yukun Zhu, Ruslan Salakhutdinov,\nRichard S. Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. 2015. Skip-thought vec-\ntors. In Advances in Neural Information Processing\nSystems 28: Annual Conference on Neural Informa-\ntion Processing Systems 2015, December 7-12, 2015,\nMontreal, Quebec, Canada, pages 3294–3302.\nIoannis Konstas and Mirella Lapata. 2012. Unsuper-\nvised concept-to-text generation with hypergraphs.\nIn Human Language Technologies: Conference of\nthe North American Chapter of the Association of\nComputational Linguistics, Proceedings, June 3-8,\n2012, Montr´eal, Canada, pages 752–761.\nIoannis Konstas and Mirella Lapata. 2013. A global\nmodel for concept-to-text generation. J. Artif. Intell.\nRes., 48:305–346.\nQuoc V . Le and Tomas Mikolov. 2014. Distributed rep-\nresentations of sentences and documents. In Pro-\nceedings of the 31th International Conference on\nMachine Learning, ICML 2014, Beijing, China, 21-\n26 June 2014, pages 1188–1196.\nR´emi Lebret, David Grangier, and Michael Auli. 2016.\nNeural text generation from structured data with ap-\nplication to the biography domain. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 1203–1213.\nPercy Liang, Michael I. Jordan, and Dan Klein. 2009.\nLearning semantic correspondences with less super-\nvision. In ACL 2009, Proceedings of the 47th An-\nnual Meeting of the Association for Computational\nLinguistics and the 4th International Joint Confer-\nence on Natural Language Processing of the AFNLP ,\n2-7 August 2009, Singapore, pages 91–99.\nTianyu Liu, Kexiang Wang, Lei Sha, Baobao Chang,\nand Zhifang Sui. 2018. Table-to-text generation\nby structure-aware seq2seq learning. In Proceed-\nings of the Thirty-Second AAAI Conference on Ar-\ntiﬁcial Intelligence, (AAAI-18), the 30th innovative\nApplications of Artiﬁcial Intelligence (IAAI-18), and\nthe 8th AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence (EAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018 , pages 4881–\n4888.\nWei Lu, Hwee Tou Ng, and Wee Sun Lee. 2009. Nat-\nural language generation with tree conditional ran-\ndom ﬁelds. In Proceedings of the 2009 Conference\non Empirical Methods in Natural Language Process-\ning, EMNLP 2009, 6-7 August 2009, Singapore, A\nmeeting of SIGDAT, a Special Interest Group of the\nACL, pages 400–409.\nShuming Ma, Pengcheng Yang, Tianyu Liu, Peng Li,\nJie Zhou, and Xu Sun. 2019. Key fact as pivot: A\ntwo-stage model for low resource table-to-text gen-\neration. In Proceedings of the 57th Conference of\nthe Association for Computational Linguistics, ACL\n2019, Florence, Italy, July 28- August 2, 2019, Vol-\nume 1: Long Papers, pages 2047–2057. Association\nfor Computational Linguistics.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Gregory S.\nCorrado, and Jeffrey Dean. 2013. Distributed rep-\nresentations of words and phrases and their com-\npositionality. In Advances in Neural Information\nProcessing Systems 26: 27th Annual Conference on\nNeural Information Processing Systems 2013. Pro-\nceedings of a meeting held December 5-8, 2013,\nLake Tahoe, Nevada, United States. , pages 3111–\n3119.\nJekaterina Novikova, Ondrej Dusek, and Verena Rieser.\n2017. The E2E dataset: New challenges for end-\nto-end generation. In Proceedings of the 18th An-\nnual SIGdial Meeting on Discourse and Dialogue,\nSaarbr¨ucken, Germany, August 15-17, 2017 , pages\n201–206.\nJeffrey Pennington, Richard Socher, and Christopher D.\nManning. 2014. Glove: Global vectors for word\nrepresentation. In Proceedings of the 2014 Confer-\nence on Empirical Methods in Natural Language\nProcessing, EMNLP 2014, October 25-29, 2014,\nDoha, Qatar, A meeting of SIGDAT, a Special Inter-\nest Group of the ACL, pages 1532–1543.\nMatthew E. Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word rep-\nresentations. In Proceedings of the 2018 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, NAACL-HLT 2018, New Or-\nleans, Louisiana, USA, June 1-6, 2018, Volume 1\n(Long Papers), pages 2227–2237.\nRatish Puduppully, Li Dong, and Mirella Lapata. 2018.\nData-to-text generation with content selection and\nplanning. CoRR, abs/1809.00582.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nEhud Reiter and Robert Dale. 1997. Building applied\nnatural language generation systems. Natural Lan-\nguage Engineering, 3(1):57–87.\nAmrita Saha, Vardaan Pahuja, Mitesh M. Khapra,\nKarthik Sankaranarayanan, and Sarath Chandar.\n2018. Complex sequential question answering: To-\nwards learning to converse over linked question an-\nswer pairs with a knowledge graph. In Proceed-\nings of the Thirty-Second AAAI Conference on Ar-\ntiﬁcial Intelligence, (AAAI-18), the 30th innovative\nApplications of Artiﬁcial Intelligence (IAAI-18), and\nthe 8th AAAI Symposium on Educational Advances\nin Artiﬁcial Intelligence (EAAI-18), New Orleans,\nLouisiana, USA, February 2-7, 2018 , pages 705–\n713.\nAbigail See, Peter J. Liu, and Christopher D. Manning.\n2017. Get to the point: Summarization with pointer-\ngenerator networks. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics, ACL 2017, Vancouver, Canada, July 30 -\nAugust 4, Volume 1: Long Papers, pages 1073–1083.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics, ACL 2016, August 7-12, 2016, Berlin, Ger-\nmany, Volume 1: Long Papers.\nYu Su, Huan Sun, Brian M. Sadler, Mudhakar Sri-\nvatsa, Izzeddin Gur, Zenghui Yan, and Xifeng Yan.\n2016. On generating characteristic-rich question\nsets for QA evaluation. In Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2016, Austin, Texas,\nUSA, November 1-4, 2016, pages 562–572.\nMarilyn A. Walker, Owen Rambow, and Monica Ro-\ngati. 2001. Spot: A trainable sentence planner. In\nLanguage Technologies 2001: The Second Meeting\nof the North American Chapter of the Association\nfor Computational Linguistics, NAACL 2001, Pitts-\nburgh, PA, USA, June 2-7, 2001.\nSam Wiseman, Stuart M. Shieber, and Alexander M.\nRush. 2017. Challenges in data-to-document gen-\neration. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Processing,\nEMNLP 2017, Copenhagen, Denmark, September 9-\n11, 2017, pages 2253–2263.\nSam Wiseman, Stuart M. Shieber, and Alexander M.\nRush. 2018. Learning neural templates for text gen-\neration. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\nBrussels, Belgium, October 31 - November 4, 2018 ,\npages 3174–3187.\nJun Yin, Xin Jiang, Zhengdong Lu, Lifeng Shang,\nHang Li, and Xiaoming Li. 2016. Neural generative\nquestion answering. In Proceedings of the Twenty-\nFifth International Joint Conference on Artiﬁcial In-\ntelligence, IJCAI 2016, New York, NY, USA, 9-15\nJuly 2016, pages 2972–2978.\nAppendix A. Implementation Details\nWe use the Adam optimizer (Kingma and Ba, 2015)\nwith learning rate set to 0.0003. The mini-batch\nsize is set to 40 and the weight λof the copy loss\nterm to 0.7. The dimension of the position embed-\nding is set to 5. For attribute name with multiple\nwords, we average their word embeddings as the\nattribute name embedding. Refer to our released\ncode and data at https://github.com/czyssrs/\nFew-Shot-NLG for more details.\nAppendix B. ROUGE-4 Results\nFollowing previous work (Liu et al., 2018), we\nconduct automatic evaluations using BLEU-4 and\nROUGE-4 (F-measure)3. Table 6, 7 and 8 show\nthe ROUGE-4 results for three domains Humans,\nBooks and Songs, respectively.\nDomain Humans\n# of training instances - 50 100 200 500\nTemplate 5.1 - - - -\nBase-original - 0.1 0.4 0.5 0.6\nBase - 0.1 0.4 0.8 1.5\nBase+switch - 4.9 6.3 9.8 12.5\nBase+switch+LM-scratch - 1.0 2.8 4.7 7.1\nBase+switch+LM (Ours) - 14.1 16.2 22.1 28.3\nTable 6: ROUGE-4 results on Humans domain\nDomain Books\n# of training instances - 50 100 200 500\nTemplate 15.0 - - - -\nBase-original - 1.1 1.6 2.1 1.5\nBase - 1.7 1.5 2.1 2.4\nBase+switch - 12.8 15.0 18.1 20.7\nBase+switch+LM-scratch - 2.4 4.2 6.5 10.7\nBase+switch+LM (Ours) - 22.5 23.1 25.0 27.6\nTable 7: ROUGE-4 results on Books domain\nAppendix C. Human Evaluation Details\nWe conduct human evaluation studies using Ama-\nzon Mechanical Turk, based on two aspects: Fac-\ntual correctness and Language naturalness. For\nboth studies, we evaluate the results trained with\n200 training instances of Humans domain. We ran-\ndomly sample 500 instances from the test set, to-\ngether with the texts generated with different meth-\n3We use standard scripts NIST mteval-v13a.pl (for BLEU),\nand rouge-1.5.5 (for ROUGE)\nDomain Songs\n# of training instances - 50 100 200 500\nTemplate 24.5 - - - -\nBase-original - 3.4 4.2 4.7 4.8\nBase - 4.1 5.1 4.7 5.8\nBase+switch - 20.2 21.7 23.2 24.8\nBase+switch+LM-scratch - 5.4 8.0 12.0 15.0\nBase+switch+LM (Ours) - 26.2 28.6 30.1 32.6\nTable 8: ROUGE-4 results on Songs domain\nods. Each evaluation unit is assigned to 3 workers\nto eliminate human variance.\nThe ﬁrst study attempts to evaluate how well a\ngenerated text can correctly convey information\nin the table. Each worker is present with both the\ninput table and a generated text, and asked to count\nhow many facts in the generated text are supported\nby the table, and how many are contradicting with\nor missing from the table, similar as in (Wiseman\net al., 2017). The we calculate the average number\nof supporting and contradicting facts for the texts\ngenerated by each method.\nThe second study aims to evaluate whether the\ngenerated text is grammatically correct and ﬂuent\nin terms of language, regardless of factual correct-\nness. Each worker is present with a pair of texts\ngenerated from the same input table, by two dif-\nferent methods, then asked to select the better one\nonly according to language naturalness, or “Tied”\nif the two texts are of equal quality. The input table\nis not shown to the workers. Each time a generated\ntext is chosen as the better one, we assign score of\n1.0. If two texts are tied, we assign 0.5 for each. We\nthen calculate the average score for the texts gener-\nated by each method, indicating its superiority in\npairwise comparisons with all other methods.\nThe signiﬁcance test is conducted respectively\non all three measures: number of supporting facts\nand number of contradicting facts for the ﬁrst study;\nthe assigned score for the second study. We use the\nTukey HSD post-hoc analysis of an ANOV A with\nthe worker’s response as the dependent variable, the\nmethod and worker id as independent variables."
}