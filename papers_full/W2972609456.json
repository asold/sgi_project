{
  "title": "Question Generation by Transformers",
  "url": "https://openalex.org/W2972609456",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4297710379",
      "name": "Kriangchaivech, Kettip",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4297710380",
      "name": "Wangperawong, Artit",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2891946694",
    "https://openalex.org/W2427527485",
    "https://openalex.org/W244289328",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2963418779",
    "https://openalex.org/W2905270607",
    "https://openalex.org/W1555380324",
    "https://openalex.org/W2949615363",
    "https://openalex.org/W2114483840",
    "https://openalex.org/W2610986956",
    "https://openalex.org/W2942203175",
    "https://openalex.org/W1778447668",
    "https://openalex.org/W1520663121",
    "https://openalex.org/W2952913664",
    "https://openalex.org/W2109609717",
    "https://openalex.org/W2400976407",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2803933017"
  ],
  "abstract": "A machine learning model was developed to automatically generate questions from Wikipedia passages using transformers, an attention-based model eschewing the paradigm of existing recurrent neural networks (RNNs). The model was trained on the inverted Stanford Question Answering Dataset (SQuAD), which is a reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles. After training, the question generation model is able to generate simple questions relevant to unseen passages and answers containing an average of 8 words per question. The word error rate (WER) was used as a metric to compare the similarity between SQuAD questions and the model-generated questions. Although the high average WER suggests that the questions generated differ from the original SQuAD questions, the questions generated are mostly grammatically correct and plausible in their own right.",
  "full_text": "Question Generation by Transformers\nKettip Kriangchaivech1∗ and Artit Wangperawong2\n1kettipk@gmail.com\n2artit.wangperawong@usbank.com\nU.S. Bank\n1095 Avenue of the Americas\nNew York, NY 10036\nAbstract\nA machine learning model was developed to automatically\ngenerate questions from Wikipedia passages using transform-\ners, an attention-based model eschewing the paradigm of\nexisting recurrent neural networks (RNNs). The model was\ntrained on the inverted Stanford Question Answering Dataset\n(SQuAD), which is a reading comprehension dataset consist-\ning of 100,000+ questions posed by crowdworkers on a set\nof Wikipedia articles. After training, the question generation\nmodel is able to generate simple questions relevant to unseen\npassages and answers containing an average of 8 words per\nquestion. The word error rate (WER) was used as a metric\nto compare the similarity between SQuAD questions and the\nmodel-generated questions. Although the high average WER\nsuggests that the questions generated differ from the original\nSQuAD questions, the questions generated are mostly gram-\nmatically correct and plausible in their own right.\nIntroduction\nExisting question generating systems reported in the liter-\nature involve human-generated templates, including cloze\ntype (Hermann et al. 2015), rule-based (Mitkov and Ha\n2003; Rus et al. 2010), or semi-automatic questions ( ´Alvaro\nand ´Alvaro 2010; Rey et al. 2012; Liu and Lin 2014). On\nthe other hand, machine learned models developed recently\nhave used recurrent neural networks (RNNs) to perform se-\nquence transduction, i.e. sequence-to-sequence (Du, Shao,\nand Cardie 2017; Kim et al. 2019). In this work, we in-\nvestigated an automatic question generation system based\non a machine learning model that uses transformers instead\nof RNNs (Vaswani et al. 2017; Wangperawong 2018). Our\ngoal was to generate questions without templates and with\nminimal human involvement using machine learning trans-\nformers that have been demonstrated to train faster and bet-\nter than RNNs. Such a system would beneﬁt educators by\nsaving time to generate quizzes and tests.\nBackground and Related Work\nA relatively simple method for question generation is the\nﬁll-in-the-blank approach, which is also known as cloze\n∗To whom correspondence should be addressed.\ntasks. Such a method typically involves the sentence ﬁrst be-\ning tokenized and tagged for part-of-speech with the named\nentity or noun part of the sentence masked out. These gen-\nerated questions are an exact match to the one in the reading\npassage except for the missing word or phrase. Although ﬁll-\nin-the-blank questions are often used for reading compre-\nhension, answering such questions correctly may not neces-\nsarily indicate comprehension if it is too easy to match the\nquestion to the relevant sentence in the passage. To improve\nﬁll in the blank type questions, a prior study used a super-\nvised machine learning model to generate ﬁll-in-the-blank\ntype questions. The model paraphrases the sentence from\nthe passage with the missing word by anonymizing entity\nmarkers (Hermann et al. 2015).\nSemi-automatic methods can also be use for question\ngeneration. Semi-automatic question generation involves\nhuman-generated templates in combination with querying\nthe linked database repositories to complete the question\n( ´Alvaro and ´Alvaro 2010; Rey et al. 2012). The answer\nto the question is also extracted from the linked database.\nIf the question is to be answered selecting from multiple\nchoices, then distractors could also be selected from the\ndatabase and randomly generated as incorrect choices for\nthe answer. Another example of template-based question-\nand-answer generator using linked data is called Sherlock\nthat has been shown to generate questions with varying lev-\nels of difﬁculty (Liu and Lin 2014). However, designing\na large set of high quality questions using semi-automatic\nquestion generation methods can be cognitively demanding\nand time-consuming. The types of questions created are also\nconstrained to the templates. Generating a large dataset of\nquestions is therefore cumbersome.\nOther automatic question generators require human-made\nrules for the model to follow (Mitkov and Ha 2003; Rus et\nal. 2010). Educators are recruited to deﬁne the rules that\nwill convert declarative sentences into interrogative ques-\ntions (Wang, Hao, and Liu 2007; Adamson et al. 2013;\nHeilman and Smith 2010). The rules generated requires the\neducator to possess both linguistic knowledge and subject\nknowledge. As with the template-based methods described\nabove, this rules-based method can also be time-consuming\nand cognitively demanding. Moreover, the quality of the\narXiv:1909.05017v2  [cs.CL]  14 Sep 2019\nquestions is limited by the quality of the handcrafted rules,\nand rules-based approaches are not scalable beyond human\ncapacity.\nPerhaps the most automated method reported thus far uti-\nlizes RNNs as sequence transduction (seq2seq) models to\ngenerate questions from sentences or passages (Du, Shao,\nand Cardie 2017; Kim et al. 2019). In the most successful\nvariant of RNNs, the Long Short-Term Memory (LSTM)\nnetworks, the model reads from left to right and includes\nan encoder and a decoder (Keneshloo et al. 2018). The en-\ncoder takes the input and converts it to hidden vectors, while\nthe decoder takes the vectors from the encoder and creates\nits own hidden vector to predict the next word based on the\nprevious hidden vector (Keneshloo et al. 2018). The hidden\nvector to the decoder stores all of the information about the\ncontext. The components in between the encoder and the\ndecoder of the seq2seq model consists of attention, beam\nsearch, and bucketing. The attention mechanism takes the\ninput to the decoder and allows the decoder to analyze the\ninput sequence selectively. Beam search mechanism allows\nthe decoder to select the highest probability word occurrence\nbased on previous words. The bucketing mechanism allows\nthe length of sequences to vary based on what we designate\nthe bucket size to be. The decoder is then rewarded for cor-\nrectly predicting the next word and penalized for incorrect\npredictions.\nIn this study, we developed a seq2seq model to automati-\ncally generate questions from Wikipedia passages. Our goal\nis to produce plausible questions with minimal human inter-\nvention that can assist educators in developing their quizzes\nand tests. Our model is based on transformers instead of\nRNNs. Transformers can train faster than RNNs because it\nis more parallelizable, working well with large and limited\ndatasets (Vaswani et al. 2017; Wangperawong 2018).\nTransformers can also achieve better performance at a\nfraction of the training cost. Like the RNN approach, trans-\nformers have an encoder and a decoder. Transformers also\nincorporate the beam search and bucketing mechanisms.\nUnlike RNNs, transformers adopt multiple attention heads\nwithout requiring any recurrence, though recurrence can be\nadded. The self-attention mechanism used is the scaled dot-\nproduct attention according to\nAttention(K, Q, V) =softmax\n(QKT\n√\nd\n)\nV, (1)\nwhere d is the dimension (number of columns) of the input\nqueries Q, keys K, and values V . By using self-attention,\ntransformers can account for the whole sequence in its en-\ntirety and bidirectionally. For multi-head attention with h\nheads that jointly attend to different representation sub-\nspaces at different positions given a sequence of length m\nand the matrix H ∈ Rm×d, the result is\nMultiHead (H) =Concat (head1, ..., headh) WO,\nheadi = Attention\n(\nHW\ni , HK\ni , HV\ni\n)\n,\n(2)\nwhere the projections are learned parameter matrices\nHW\ni , HK\ni , HV\ni ∈ R(d×d)/h and WO ∈ R(d×d).\nModels utilizing transformers have achieved state-of-the-\nart performance on many NLP tasks, including question an-\nswering (Devlin et al. 2018; Yang et al. 2019). It is therefore\ninteresting to study how transformers might be used to gen-\nerate questions by training on the inverted SQuAD.\nExperimental Methods\nData. In this study, we used the Stanford Question An-\nswering Dataset (SQuAD). SQuAD is a reading compre-\nhension dataset consisting of 100,000+ questions posed by\ncrowdworkers on a set of Wikipedia articles, where the an-\nswer to each question is a segment of text from the corre-\nsponding reading passage (Rajpurkar et al. 2016). To gener-\nate the data for SQuAD, the top 10,000 English Wikipedia\narticles were ranked by Project Nayuki’s Wikipedia’s inter-\nnal PageRanks as high-quality. Paragraphs that are longer\nthan 500 characters were then extracted from the articles\nand partitioned into a training set (80%), a development set\n(10%), and a test set (10%). Only the former two datasets\nare publicly available. Crowdworkers were then employed\nto generate the questions and then the answers to the ques-\ntions based on the extracted paragraphs. Another subset of\ncrowdworkers were then asked to answer the questions that\nwere generated given the corresponding passage to compare\nthe model’s answer with human generated answers and pro-\nvide a benchmark for machine learning models.\nPre-processing. We used the publicly available data from\nSQuAD to train our model to generate the questions. We\nused SQuAD’s training and dev sets as our training and test\nsets, respectively. The reading passage, question, and answer\ndata were pre-processed as described in the next section. For\nthe test set, we provided the model with the pre-processed\nreading passages and answers that were never seen by the\nmodel. We inverted SQuAD by training a machine learning\nmodel to infer a question given a reading passage and an\nanswer separated by a special token (i.e., ‘*’) as input.\nFor pre-processing the reading passages, questions and\nanswers, spaCy was used for named entity recognition and\npart-of-speech tagging (Honnibal and Montani 2017), and\nWordPiece was used for tokenization (Wu et al. 2016). To\nensure intelligible outputs, stop words are removed from the\ncontext passages and answers but not the questions. After\nlowercasing, tokenizing and removing the stop words, the\nnamed entities are then replaced with their respective tags\nto better allow the model to generalize and learn patterns in\nthe data. We address a variety of named and numeric enti-\nties, including companies, locations, organizations and prod-\nucts, etc. (Table 1). To account for multiple occurrences of a\nnamed entity type in the context passage, we also included\nan index after the named entity tag. As an example, consider\nthe following context passage:\nSuper Bowl 50 was an American football game to de-\ntermine the champion of the National Football League\n(NFL) for the 2015 season. The American Football\nConference (AFC) champion Denver Broncos defeated\nthe National Football Conference (NFC) champion\nCarolina Panthers 2410 to earn their third Super Bowl\ntitle. The game was played on February 7, 2016, at\nTable 1: Named entity tags and the entities they encompass.\nBase Tag Description\nPERSON People, including ﬁctional.\nNORP Nationalities or religious or political groups.\nFAC Buildings, airports, highways, bridges, etc.\nORG Companies, agencies, institutions, etc.\nGPE Countries, cities, states.\nLOC Non-GPE locations, mountain ranges, bodies of water.\nPRODUCT Objects, vehicles, foods, etc. (Not services.)\nEVENT Named hurricanes, battles, wars, sports events, etc.\nWORK OF ART Titles of books, songs, etc.\nLAW Named documents made into laws.\nLANGUAGE Any named language.\nDATE Absolute or relative dates or periods.\nTIME Times smaller than a day.\nPERCENT Percentage, including “%”.\nMONEY Monetary values, including unit.\nQUANTITY Measurements, as of weight or distance.\nORDINAL “ﬁrst”, “second”, etc.\nCARDINAL Numerals that do not fall under another type.\nLevi’s Stadium in the San Francisco Bay Area at Santa\nClara, California. As this was the 50th Super Bowl, the\nleague emphasized the ”golden anniversary” with vari-\nous gold-themed initiatives, as well as temporarily sus-\npending the tradition of naming each Super Bowl game\nwith Roman numerals (under which the game would\nhave been known as ”Super Bowl L”), so that the logo\ncould prominently feature the Arabic numerals 50.\nApplying the pre-processing described above, including the\nindexed-named entity tag replacement but not yet removing\nstop words, would produce\nEVENT 0 DATE 0 was an NORP 0 football game to de-\ntermine the champion of ORG 0 ( ORG 1 ) for DATE 1\n. the NORP 0 football conference ( ORG 2 ) champion\nORG 3 defeated ORG 4 ( ORG 5 ) champion ORG 6\n24 10 to earn their ORDINAL 0 EVENT 0 title . the\ngame was played on DATE 2 , at FAC 0 in FAC 1 at\nGPE 0 , GPE 1 . as this was the ORDINAL 1 EVENT 0\n, the league emphasized the ” golden anniversary ” with\nvarious gold - themed initiatives , as well as temporar-\nily suspend ##ing the tradition of naming each EVENT\n0 game with LANGUAGE 0 nu ##meral ##s ( under\nwhich the game would have been known as ” EVENT\n0 l ” ) , so that the logo could prominently feature the\nLANGUAGE 1 nu ##meral ##s DATE 0 .\nNote that the index is separated from the named entity tag by\na space and therefore interpreted by the model as a separate\ntoken so that named entities of similar types can be associ-\nated and generalized from without sacriﬁcing the ability to\ndistinguish between different entities of the same type. This\nspacing is necessary since we do not employ character-level\nembedding.\nTo generate the sub-word embeddings, we used the pre-\ntrained WordPiece model from BERT, which has a 30,000\ntoken vocabulary. WordPiece is a statistical technique used\nto segment text into tokens at the sub-word level. The vocab-\nulary is initialized with all individual characters and itera-\ntively aggregates the most frequently and likely combination\nof symbols into a vocabulary. The generation of WordPieces\nallows the model to capture the meaning of commonly oc-\ncurring word segments, such as the root word suspend from\nsuspending in the example context passage above. This dis-\npenses with the need for the model to learn different variants\nor conjugations of a word.\nEach input to the model comprised of a concatenation\nof the pre-processed answer and context passage. The most\ncommonly agreed upon answer was chosen from among the\nthree plausible answers for each question in SQuAD. Unlike\nprior question generation studies using SQuAD by isolat-\ning the sentence containing the answer, here we include the\nentire passage because the answers can depend on the con-\ntext outside of the answer-containing sentence. Compared to\nRNNs used in prior studies, transformers allow us to more\nconveniently train and perform inference on longer sequence\nlengths. The model was developed with TensorFlow (Abadi\net al. 2015) and Tensor2Tensor (Vaswani et al. 2018), and\nthen trained with an Nvidia Tesla T4 GPU for 1 million\ntraining steps.\nTo evaluate and analyze the results, the generated ques-\ntions are post-processed by removing unnecessary spaces\nand consolidating the resulting WordPieces into a single co-\nherent word. In other words, the results were de-tokenized\nusing BERT’s pre-trained WordPiece model.\nResults and Discussion\nTo measure the model’s question formulating ability, we cal-\nculated the word error rate (WER) between the generated\nquestions and the corresponding questions from SQuAD.\nThe SQuAD questions were used as the reference questions,\nas they ideally ask for the answers provided. WER is also\nknown as the edit distance, which is a measure of similarity\nat the word level between generated questions and the target\nquestions from SQuAD.\nIn essence, WER is the Levenshtein distance applied to a\nsequence of words. Differences between the two sequences\ncan include sequence insertions, deletions and substitutions.\nWER can be calculated according to\nWER = (S + D + I)\nN = (S + D + I)\n(S + D + C) , (3)\nwhere S is the number of word substitutions, D is the num-\nber of word deletions, I is the number of word insertions, C\nis the number of correct words, andN is the total number of\nwords in the reference sequence (i.e., N = S + D + C).\nFigure 1: Word error rate (WER) between the SQuAD ques-\ntions and model-generated questions on our test set (SQuAD\ndev set).\nA low WER would indicate that a model-generated ques-\ntion is similar to the reference question, while a high WER\nmeans that the generated question differs signiﬁcantly from\nthe reference question. It should be noted that a high WER\ndoes not necessarily invalidate the generated question, as\ndifferent questions can have the same answers, and there\ncould be various ways of phrasing the same question. On the\nother hand, a situation with low WER of 1 could be due to\nthe question missing a pertinent word to convey the correct\nidea. Despite these shortcomings in using WER as a metric\nof success, the WER can reﬂect our model’s effectiveness\nin generating questions that are similar to those of SQuAD\nbased on the given reading passage and answer. WER can\nbe used for initial analyses that can lead to deeper insights\nas discussed further below.\nUsing the SQuAD dev set as our test set, our model gener-\nated 10,570 questions. The questions generated were mostly\ngrammatically correct and related to the topic of the con-\ntext passage. Fig. 1 shows the WER distribution, which has\na mean of 9.66 words. 0.05% of the total questions gener-\nated by the model were an exact match to the corresponding\nSQuAD questions. For our discussion and analysis, we ex-\namine generated questions with various different WERs to\nFigure 2: First word frequency of the model-generated ques-\ntions in descending order for our test set (SQuAD dev set).\nResults for all ﬁrst words are shown.\nFigure 3: First word frequency of the SQuAD questions in\ndescending order for the dev set. Only the 21 most frequent\nﬁrst words are shown. The ﬁrst words all capitalized, i.e.\nPERSON and ORG, are named entity tags assigned after pre-\nprocessing.\ngain insight into the quality of the questions. 9.94% of the\nmodel-generated questions have a WER less than or equal\nto 5, 56.38% have a WER between 6 and 10, 26.41% with\na WER between 11 and 15, 5.81% with a WER between 16\nand 20, and 1.45% of the generated questions have a WER\ngreater than 21 (Fig. 1). In addition to the WER, the number\nof words in the questions (Figs. 4 and 5) and the ﬁrst word\nof the questions (Figs. 2 and 3) are also considered below.\nAs shown in Fig. 1, our model was able to generate the\nexact questions as those corresponding to SQuAD with a\nWER of 0 for a small portion of instances. These types of\nquestions tend to be relatively simpler and shorter, which\nTable 2: Select SQuAD and model-generated questions with various word error rates (WERs). The model-generated questions\nhave been de-tokenized using BERT’s pre-trained WordPiece model.\nSQuAD Model-generated WER\nwhere was PERSON 8 born? where was PERSON 8 born? 0\nhow many museums are in GPE 0? how many museums are in GPE 0? 0\nwhat is the largest city of GPE 1? what is the largest area of GPE 1? 1\nwhere did PERSON 2 die? when did PERSON 2 die? 1\nwhere did PERSON 4 die? how did PERSON 4 die? 1\nwhere was ORG 0 located? where was ORG 2 located? 1\nwhere is ORG 0 based? where is ORG 0 located? 1\nwho is the president of GPE 3? who was the president of GPE 3? 1\nwhen did the launches of boilerplate csms occur in orbit? when was the ORDINAL 0 satellite launched?8\nby what century did researchers see that they could liquefy air? in what century did water begin? 9\nby what means were scientists able to liquefy air? what is the name of the process of water? 9\nwhat other european NORP 4 leader was educated at ORG 1? who was the leader of GPE 5? 10\nwhat type of engines became popular for power generation after piston steam engines?what type of electric motors have? 10\nwhat was an example of a type of warship that required high speed? what was the name of the NORP 0 ships? 10\nwhat happens to the gdp growth of a country if the income share of the top what is the average amount of gdp? 22\nPERCENT 0 increases,according to imf staff economists?\nif the average GPE 1 worker were to complete an additional year of school, what was the average income rate of GPE 0?23\nwhat amount of growth would be generated over 5 years?\nFigure 4: Word count histogram of the model-generated\nquestions for our test set (SQuAD dev set).\nare easier to learn as apparent in select examples from Ta-\nble 1. The model was also able to learn about and utilize\nsynonyms, as apparent in the following examples where the\nWER is 1. Instead of using based as in the target question\nfrom SQuAD, “where is ORG 0 based?”, the model used\nlocated to generate “where is ORG 0 located?”. Although\nthe term area can encompass many meanings, city and area\ncan be synonymous depending on the context and therefore\n“what is the largest area of GPE 1?” generated by the model\nhas the same meaning as “what is the largest city of GPE\n1?” from SQuAD. The ability to capture relative meaning\nbetween words indicates that applying a pre-trained contex-\ntualized language model can improve performance in future\nstudies.\nBeyond exact matches, a low WER does not guarantee\nthat a model-generated question has the same meaning as\nthe target SQuAD question. Consider the following exam-\nples where the WERs are all 1, but the meanings differ be-\nFigure 5: Word count histogram of the SQuAD questions for\nour test set (SQuAD dev set).\ntween the generated and target questions. Sometimes the\nmodel produced a question in past tense when the target\nquestion from SQuAD is in present tense, e.g. “who was the\npresident of GPE 3?” generated by the model versus “who\nis the president of GPE 3?” from SQuAD. In some cases,\nthe named entity type matched, but the index did not, e.g.\n“where was ORG 2 located?” generated by the model ver-\nsus “where was ORG 0 located?” from SQuAD. This case\nof swapping the index of a given named entity type could\nbe a consequence of the pre-processing employed. Despite\nthe different meanings, the questions generated are plausi-\nble questions that could be reasonably asked given the same\ncontext passages.\nSince the questions generated by our model on the\nSQuAD dev set have an average WER of 9.66, we examined\nthe questions with a WER of 8 to 10 (Table 2) to see whether\nthe questions are structured properly and whether they main-\ntain the same meaning as the target SQuAD questions. As\nshown in Table 2, it was challenging for the model to pro-\nduce questions as complex as those from SQuAD, which\nresulted in a large WER. The average word count for the\ngenerated questions is 8 words per question (Fig. 4), while\nmost of the questions from SQuAD are more detailed and\ncomplex with a total average word count of 12 (Fig. 5). The\nstructure of the questions generated by the model are simpler\nand less detailed than those from SQuAD, but most are nev-\nertheless plausible questions that are grammatically correct.\nThe model-generated questions are relevant to the topic of\nthe context passage and has the correct type of asking words\nto start the question. For example, although the target ques-\ntion from SQuAD is “by what means were scientists able\nto liquefy air?”, the model can generate “what is the name\nof the process of water?”. The questions have a WER of 9\nand both can be interpreted as referring to the transforma-\ntion process of water; more speciﬁcally, the condensation\nprocess. The model may have not sufﬁciently learned about\nthe term liquefy but was still able to ask a similar question\ngiven limitations in the dataset used for training.\nWe next examined questions in the high WER regime, i.e.\nWER of 20 or more. Questions generated by the model still\nreﬂect the answer and context passage of interest. For ex-\nample, given inputs for the target SQuAD question “who\nwas responsible for the authorship of a paper published on\nreal time - computations?”, the model generated “what did\nPERSON 3 write?”. Although both questions ask about au-\nthorship, the model’s question is asking for a different type\nof answer, as indicated by the asking word of the questions\n(i.e., who vs. what).\nTo understand how the model chooses the asking word,\nwe plot the ﬁrst-word frequencies in descending order for\nSQuAD and model-generated questions (Figs. 2 and 3).\nQuestions from SQuAD predominantly involvewhat, which\nreﬂects the ﬁrst-word distribution of the training set as well.\nAs the ﬁrst-word is usually the asking word, training data\nimbalance most likely caused the model to be biased to-\nwards generating what questions. While the questions from\nSQuAD involve over 21 different words that initiate the\nquestions (Figs. 3), our model only uses 10 different words\nto initiate questions (Figs. 2). The lack of diversity demon-\nstrates that our model is not as well versed in asking ques-\ntions as the crowdworkers in SQuAD and forms less elabo-\nrate types of questions. After all, our model generates an av-\nerage of 8 words per question (Fig. 4), whereas the SQuAD\nquestions have an average of 12 words per question (Fig. 5).\nConclusion and Future Work\nWe demonstrate that a transformer model can be trained\nto generate questions with correct grammar and relevancy\nto the context passage and answers provided. WER anal-\nyses was applied to diagnose shortcomings and guide fu-\nture improvements. We observed that a low WER could be\ndue to syntactic similarity but semantic disagreement, while\ntwo questions with syntactic divergence but similar mean-\ning could result in a high WER. Since our results does not\nexhibit issues relating to contextual and syntactic roles of\nwords within a generated question, other popular metrics\n(BLEU, ROUGE, F1-score, etc.) would lead to similar ﬁnd-\nings (He, Deng, and Acero 2011). Perhaps a better approach\nto evaluating question generation models is to apply state-\nof-the-art question answering models from SQuAD’s leader-\nboard to measure how many answers agree.\nTo improve the model, more and balanced data can be\nprovided to train the model to reduce the asking word bias.\nOne method that can be used to obtain more data is through\ndata augmentation by back-translation (Xie et al. 2019). The\noriginal SQuAD can be translated into another language\nsuch as French. The translated text could then be translated\nback into English to generate a variation of the context, ques-\ntion and answers that provide more training data for the\nmodel. Another data augmentation method is to paraphrase\nSQuAD’s data to get another variation of the text (See, Liu,\nand Manning 2017), but one would have to ensure that perti-\nnent information is not sacriﬁced in the summarization. The\naugmented data should then be sampled to reduce bias as\nmuch as possible. Other pre-processing variations can be\nconsidered. We tried including stopwords and removing the\nanswer form the context passages but did not see improve-\nments. Recent advancements in pre-trained bidirectionally\ncontextualized language models can also be incorporated\n(Devlin et al. 2018; Yang et al. 2019), which would require\na decoder to be added to the pre-trained model.\nReferences\n[Abadi et al. 2015] Abadi, M.; Agarwal, A.; Barham, P.;\nBrevdo, E.; Chen, Z.; Citro, C.; Corrado, G. S.; Davis, A.;\nDean, J.; Devin, M.; Ghemawat, S.; Goodfellow, I.; Harp,\nA.; Irving, G.; Isard, M.; Jia, Y .; Jozefowicz, R.; Kaiser, L.;\nKudlur, M.; Levenberg, J.; Man´e, D.; Monga, R.; Moore, S.;\nMurray, D.; Olah, C.; Schuster, M.; Shlens, J.; Steiner, B.;\nSutskever, I.; Talwar, K.; Tucker, P.; Vanhoucke, V .; Vasude-\nvan, V .; Vi´egas, F.; Vinyals, O.; Warden, P.; Wattenberg, M.;\nWicke, M.; Yu, Y .; and Zheng, X. 2015. TensorFlow: Large-\nscale machine learning on heterogeneous systems. Software\navailable from tensorﬂow.org.\n[Adamson et al. 2013] Adamson, D.; Bhartiya, D.; Gujral,\nB.; Kedia, R.; Singh, A.; and Ros ´e, C. P. 2013. Automati-\ncally generating discussion questions. In AIED.\n[ ´Alvaro and ´Alvaro 2010] ´Alvaro, G., and ´Alvaro, J. 2010.\nA linked data movie quiz: the answers are out there, and so\nare the questions.\n[Devlin et al. 2018] Devlin, J.; Chang, M.; Lee, K.; and\nToutanova, K. 2018. BERT: pre-training of deep bidi-\nrectional transformers for language understanding. CoRR\nabs/1810.04805.\n[Du, Shao, and Cardie 2017] Du, X.; Shao, J.; and Cardie, C.\n2017. Learning to ask: Neural question generation for read-\ning comprehension. arXiv preprint arXiv:1705.00106.\n[He, Deng, and Acero 2011] He, X.; Deng, L.; and Acero, A.\n2011. Why word error rate is not a good metric for speech\nrecognizer training for the speech translation task? In 2011\nIEEE International Conference on Acoustics, Speech and\nSignal Processing (ICASSP), 5632–5635.\n[Heilman and Smith 2010] Heilman, M., and Smith, N. A.\n2010. Good question! statistical ranking for question gen-\neration. 609–617. Los Angeles, California: Association for\nComputational Linguistics.\n[Hermann et al. 2015] Hermann, K. M.; Kocisky, T.; Grefen-\nstette, E.; Espeholt, L.; Kay, W.; Suleyman, M.; and Blun-\nsom, P. 2015. Teaching machines to read and compre-\nhend. In Advances in neural information processing systems,\n1693–1701.\n[Honnibal and Montani 2017] Honnibal, M., and Montani, I.\n2017. spacy 2: Natural language understanding with bloom\nembeddings, convolutional neural networks and incremental\nparsing. https://spacy.io.\n[Keneshloo et al. 2018] Keneshloo, Y .; Shi, T.; Ramakrish-\nnan, N.; and Reddy, C. K. 2018. Deep reinforcement\nlearning for sequence to sequence models. arXiv preprint\narXiv:1805.09461.\n[Kim et al. 2019] Kim, Y .; Lee, H.; Shin, J.; and Jung, K.\n2019. Improving neural question generation using answer\nseparation. In Proceedings of the AAAI Conference on Arti-\nﬁcial Intelligence, volume 33, 6602–6609.\n[Liu and Lin 2014] Liu, D., and Lin, C. 2014. Sherlock: a\nsemi-automatic quiz generation system using linked data. In\nInternational Semantic Web Conference (Posters & Demos),\n9–12. Citeseer.\n[Mitkov and Ha 2003] Mitkov, R., and Ha, L. A. 2003.\nComputer-aided generation of multiple-choice tests. In Pro-\nceedings of the HLT-NAACL 03 workshop on Building ed-\nucational applications using natural language processing ,\n17–22.\n[Rajpurkar et al. 2016] Rajpurkar, P.; Zhang, J.; Lopyrev, K.;\nand Liang, P. 2016. Squad: 100,000+ questions for machine\ncomprehension of text. arXiv preprint arXiv:1606.05250.\n[Rey et al. 2012] Rey, G. ´A.; Celino, I.; Alexopoulos, P.;\nDamljanovic, D.; Damova, M.; Li, N.; and Devedzic, V .\n2012. Semi-automatic generation of quizzes and learning\nartifacts from linked data.\n[Rus et al. 2010] Rus, V .; Wyse, B.; Piwek, P.; Lintean, M.;\nStoyanchev, S.; and Moldovan, C. 2010. The ﬁrst question\ngeneration shared task evaluation challenge.\n[See, Liu, and Manning 2017] See, A.; Liu, P. J.; and Man-\nning, C. D. 2017. Get to the point: Summariza-\ntion with pointer-generator networks. arXiv preprint\narXiv:1704.04368.\n[Vaswani et al. 2017] Vaswani, A.; Shazeer, N.; Parmar, N.;\nUszkoreit, J.; Jones, L.; Gomez, A. N.; Kaiser, L.; and\nPolosukhin, I. 2017. Attention is all you need. CoRR\nabs/1706.03762.\n[Vaswani et al. 2018] Vaswani, A.; Bengio, S.; Brevdo, E.;\nChollet, F.; Gomez, A. N.; Gouws, S.; Jones, L.; Kaiser, L.;\nKalchbrenner, N.; Parmar, N.; Sepassi, R.; Shazeer, N.; and\nUszkoreit, J. 2018. Tensor2tensor for neural machine trans-\nlation. CoRR abs/1803.07416.\n[Wang, Hao, and Liu 2007] Wang, W.; Hao, T.; and Liu, W.\n2007. Automatic question generation for learning evalua-\ntion in medicine. In International conference on web-based\nlearning, 242–251. Springer.\n[Wangperawong 2018] Wangperawong, A. 2018. Attend-\ning to mathematical language with transformers. CoRR\nabs/1812.02825.\n[Wu et al. 2016] Wu, Y .; Schuster, M.; Chen, Z.; Le, Q. V .;\nNorouzi, M.; Macherey, W.; Krikun, M.; Cao, Y .; Gao, Q.;\nMacherey, K.; Klingner, J.; Shah, A.; Johnson, M.; Liu, X.;\nKaiser, L.; Gouws, S.; Kato, Y .; Kudo, T.; Kazawa, H.;\nStevens, K.; Kurian, G.; Patil, N.; Wang, W.; Young, C.;\nSmith, J.; Riesa, J.; Rudnick, A.; Vinyals, O.; Corrado, G.;\nHughes, M.; and Dean, J. 2016. Google’s neural machine\ntranslation system: Bridging the gap between human and\nmachine translation. CoRR abs/1609.08144.\n[Xie et al. 2019] Xie, Q.; Dai, Z.; Hovy, E. H.; Luong, M.;\nand Le, Q. V . 2019. Unsupervised data augmentation.CoRR\nabs/1904.12848.\n[Yang et al. 2019] Yang, Z.; Dai, Z.; Yang, Y .; Carbonell,\nJ. G.; Salakhutdinov, R.; and Le, Q. V . 2019. Xlnet: General-\nized autoregressive pretraining for language understanding.\nCoRR abs/1906.08237.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.7992748022079468
    },
    {
      "name": "Computer science",
      "score": 0.7009673714637756
    },
    {
      "name": "Natural language processing",
      "score": 0.6140629649162292
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6049280166625977
    },
    {
      "name": "Comprehension",
      "score": 0.5592918992042542
    },
    {
      "name": "Metric (unit)",
      "score": 0.485087513923645
    },
    {
      "name": "Reading comprehension",
      "score": 0.43314746022224426
    },
    {
      "name": "Artificial neural network",
      "score": 0.4320046603679657
    },
    {
      "name": "Word error rate",
      "score": 0.4219244122505188
    },
    {
      "name": "Question answering",
      "score": 0.41827964782714844
    },
    {
      "name": "Machine learning",
      "score": 0.3262735605239868
    },
    {
      "name": "Linguistics",
      "score": 0.22826668620109558
    },
    {
      "name": "Reading (process)",
      "score": 0.21128153800964355
    },
    {
      "name": "Philosophy",
      "score": 0.07037264108657837
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ]
}