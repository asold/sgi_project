{
  "title": "Large Language Models for Arabic Sentiment Analysis and Machine Translation",
  "url": "https://openalex.org/W4409173725",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4305293499",
      "name": "Mohamed Zouidine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2144089384",
      "name": "Mohammed Khalil",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6861581687",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4411630323",
    "https://openalex.org/W4382619177",
    "https://openalex.org/W4389518288",
    "https://openalex.org/W4404978690",
    "https://openalex.org/W2787146684",
    "https://openalex.org/W4389519409",
    "https://openalex.org/W4389518309",
    "https://openalex.org/W6860710830",
    "https://openalex.org/W4392822465",
    "https://openalex.org/W6680762825",
    "https://openalex.org/W2770803436",
    "https://openalex.org/W2184135559",
    "https://openalex.org/W4321649710",
    "https://openalex.org/W3008110149",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W4379117885",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W6761205521"
  ],
  "abstract": "Large Language Models (LLMs) have recently demonstrated outstanding performance in a variety of Natural Language Processing (NLP) tasks. Although many LLMs have been developed, only a few models have been evaluated in the context of the Arabic language, with a significant focus on the ChatGPT model. This study assessed three LLMs on two Arabic NLP tasks: sentiment analysis and machine translation. The capabilities of LLaMA, Mixtral, and Gemma under zero- and few-shot learning were investigated, and their performance was compared against State-Of-The-Art (SOTA) models. The experimental results showed that, among the three models, LLaMA tends to have better comprehension abilities for the Arabic language, outperforming Mixtral and Gemma on both tasks. However, except for the Arabic-to-English translation, where LLaMA outperforms the transformer model by 4 BLEU points, in all cases, the performance of the three LLMs fell behind that of the SOTA model.",
  "full_text": null,
  "topic": "Machine translation",
  "concepts": [
    {
      "name": "Machine translation",
      "score": 0.7488715052604675
    },
    {
      "name": "Arabic",
      "score": 0.7438194751739502
    },
    {
      "name": "Natural language processing",
      "score": 0.7082831263542175
    },
    {
      "name": "Sentiment analysis",
      "score": 0.6598700284957886
    },
    {
      "name": "Translation (biology)",
      "score": 0.648659348487854
    },
    {
      "name": "Computer science",
      "score": 0.6275817155838013
    },
    {
      "name": "Artificial intelligence",
      "score": 0.607925295829773
    },
    {
      "name": "Linguistics",
      "score": 0.36279726028442383
    },
    {
      "name": "Chemistry",
      "score": 0.09319612383842468
    },
    {
      "name": "Philosophy",
      "score": 0.05516684055328369
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ],
  "institutions": []
}