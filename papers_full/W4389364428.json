{
    "title": "A Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and the Ugly",
    "url": "https://openalex.org/W4389364428",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2550181736",
            "name": "Yao Yifan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3171422757",
            "name": "Duan, Jinhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2746937008",
            "name": "Xu Kaidi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352779664",
            "name": "Cai Yuanfang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2360476107",
            "name": "Sun Zhibo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1920224989",
            "name": "Zhang Yue",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4388747979",
        "https://openalex.org/W4385261653",
        "https://openalex.org/W4386043398",
        "https://openalex.org/W2779578326",
        "https://openalex.org/W4384304728",
        "https://openalex.org/W4387596201",
        "https://openalex.org/W3047947484",
        "https://openalex.org/W4380267320",
        "https://openalex.org/W4327918021",
        "https://openalex.org/W4388857114",
        "https://openalex.org/W4387156851",
        "https://openalex.org/W4380353960",
        "https://openalex.org/W4286750487",
        "https://openalex.org/W4401213616",
        "https://openalex.org/W4389072658",
        "https://openalex.org/W4402671641",
        "https://openalex.org/W4285224048",
        "https://openalex.org/W4385565281",
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4387074929",
        "https://openalex.org/W4385187421",
        "https://openalex.org/W4388212383",
        "https://openalex.org/W2938571053",
        "https://openalex.org/W4391724813",
        "https://openalex.org/W4389245454",
        "https://openalex.org/W3035204084",
        "https://openalex.org/W2884280357",
        "https://openalex.org/W4387947606",
        "https://openalex.org/W4389209096",
        "https://openalex.org/W4388333192",
        "https://openalex.org/W4321368061",
        "https://openalex.org/W4361866125",
        "https://openalex.org/W4365211596",
        "https://openalex.org/W4388184878",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4386273017",
        "https://openalex.org/W3213748123",
        "https://openalex.org/W4388788688",
        "https://openalex.org/W4385354242",
        "https://openalex.org/W3170901302",
        "https://openalex.org/W4285429195",
        "https://openalex.org/W4388850876",
        "https://openalex.org/W4383605161",
        "https://openalex.org/W4386081319",
        "https://openalex.org/W4382239944",
        "https://openalex.org/W4386501873",
        "https://openalex.org/W4281481109",
        "https://openalex.org/W4386794817",
        "https://openalex.org/W4391724785",
        "https://openalex.org/W4375959406",
        "https://openalex.org/W4386556183",
        "https://openalex.org/W3173769540",
        "https://openalex.org/W4404518472",
        "https://openalex.org/W4379958452",
        "https://openalex.org/W4386144957",
        "https://openalex.org/W4386272880",
        "https://openalex.org/W4385734162",
        "https://openalex.org/W4387192026",
        "https://openalex.org/W4386794464",
        "https://openalex.org/W4385374045",
        "https://openalex.org/W4293846201",
        "https://openalex.org/W3178659068",
        "https://openalex.org/W4367701241",
        "https://openalex.org/W4402042086",
        "https://openalex.org/W3109409894",
        "https://openalex.org/W4226142937",
        "https://openalex.org/W4385755118",
        "https://openalex.org/W4387806231",
        "https://openalex.org/W4387624038",
        "https://openalex.org/W4380558484",
        "https://openalex.org/W4285228085",
        "https://openalex.org/W4377297670",
        "https://openalex.org/W4387724119",
        "https://openalex.org/W4365396120",
        "https://openalex.org/W3177765786",
        "https://openalex.org/W2563351168",
        "https://openalex.org/W4387800058",
        "https://openalex.org/W4386148536",
        "https://openalex.org/W4378509432",
        "https://openalex.org/W4387427437",
        "https://openalex.org/W4386212285",
        "https://openalex.org/W4362466275",
        "https://openalex.org/W4380320358",
        "https://openalex.org/W4386302153",
        "https://openalex.org/W4387355380",
        "https://openalex.org/W4380353722",
        "https://openalex.org/W2984693664",
        "https://openalex.org/W4297900086",
        "https://openalex.org/W2887995258",
        "https://openalex.org/W4319773014",
        "https://openalex.org/W3195374612",
        "https://openalex.org/W4389072024",
        "https://openalex.org/W4385570581",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4313294616",
        "https://openalex.org/W4362598291",
        "https://openalex.org/W4387427818",
        "https://openalex.org/W4389519421",
        "https://openalex.org/W4388483595",
        "https://openalex.org/W4378227790",
        "https://openalex.org/W4389911182",
        "https://openalex.org/W4388569860",
        "https://openalex.org/W4381982883",
        "https://openalex.org/W4313447595",
        "https://openalex.org/W3007685714",
        "https://openalex.org/W4389518968",
        "https://openalex.org/W4386119501",
        "https://openalex.org/W4319166710",
        "https://openalex.org/W3098049952",
        "https://openalex.org/W4391579642",
        "https://openalex.org/W4385572293",
        "https://openalex.org/W4385195544",
        "https://openalex.org/W4385474633",
        "https://openalex.org/W4387308274",
        "https://openalex.org/W4385714464",
        "https://openalex.org/W4387075795",
        "https://openalex.org/W3174136778",
        "https://openalex.org/W4288057780",
        "https://openalex.org/W4292956935",
        "https://openalex.org/W3207429447",
        "https://openalex.org/W4402157666",
        "https://openalex.org/W4402263672",
        "https://openalex.org/W4205758343",
        "https://openalex.org/W4389519269",
        "https://openalex.org/W3170572542",
        "https://openalex.org/W4387323401",
        "https://openalex.org/W3174828871",
        "https://openalex.org/W4285276168",
        "https://openalex.org/W3177468621",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W4387838318",
        "https://openalex.org/W4386081573",
        "https://openalex.org/W4387561041",
        "https://openalex.org/W4385775119",
        "https://openalex.org/W4383737134",
        "https://openalex.org/W4393179680",
        "https://openalex.org/W4387210439",
        "https://openalex.org/W3100675173",
        "https://openalex.org/W4376122390",
        "https://openalex.org/W4388537645",
        "https://openalex.org/W4389524573",
        "https://openalex.org/W4385889721",
        "https://openalex.org/W4389158474",
        "https://openalex.org/W4378942268",
        "https://openalex.org/W287184503",
        "https://openalex.org/W4383860750",
        "https://openalex.org/W4387928594",
        "https://openalex.org/W4381797997",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W4391116688",
        "https://openalex.org/W3035367371",
        "https://openalex.org/W4386185470",
        "https://openalex.org/W4367623529",
        "https://openalex.org/W4376311878",
        "https://openalex.org/W4287553002",
        "https://openalex.org/W4287813862",
        "https://openalex.org/W4327810158",
        "https://openalex.org/W4372283945",
        "https://openalex.org/W4387390444",
        "https://openalex.org/W4385302156",
        "https://openalex.org/W4388511408",
        "https://openalex.org/W4387635545",
        "https://openalex.org/W2963612262",
        "https://openalex.org/W4386874846",
        "https://openalex.org/W4385507608",
        "https://openalex.org/W4388652756",
        "https://openalex.org/W4377234252",
        "https://openalex.org/W4366389138",
        "https://openalex.org/W4366420437",
        "https://openalex.org/W4385570888",
        "https://openalex.org/W3024386862",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4387876242",
        "https://openalex.org/W4280637719",
        "https://openalex.org/W4205923021",
        "https://openalex.org/W4385573004",
        "https://openalex.org/W4321855128",
        "https://openalex.org/W4380994471",
        "https://openalex.org/W4392904073",
        "https://openalex.org/W4387874118",
        "https://openalex.org/W4386081818",
        "https://openalex.org/W4387436631",
        "https://openalex.org/W4385621806",
        "https://openalex.org/W4385468994",
        "https://openalex.org/W4362656212",
        "https://openalex.org/W2972535098",
        "https://openalex.org/W4386436506",
        "https://openalex.org/W3183900321",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4387323716",
        "https://openalex.org/W4319663047",
        "https://openalex.org/W3096738375",
        "https://openalex.org/W4321521234",
        "https://openalex.org/W4385971619",
        "https://openalex.org/W4320560161",
        "https://openalex.org/W4385452929",
        "https://openalex.org/W4387322765",
        "https://openalex.org/W4385573569",
        "https://openalex.org/W4386655753",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W4288333985",
        "https://openalex.org/W4385750097",
        "https://openalex.org/W4229641819",
        "https://openalex.org/W4379468930",
        "https://openalex.org/W4384007380",
        "https://openalex.org/W4387559075",
        "https://openalex.org/W4387355106",
        "https://openalex.org/W4387946871",
        "https://openalex.org/W4321782661",
        "https://openalex.org/W4383473937",
        "https://openalex.org/W2969695741",
        "https://openalex.org/W4386302825",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4385570951",
        "https://openalex.org/W4388275954",
        "https://openalex.org/W3106673115",
        "https://openalex.org/W4379958578",
        "https://openalex.org/W4307225507",
        "https://openalex.org/W4387559087",
        "https://openalex.org/W4388328860",
        "https://openalex.org/W4404782637",
        "https://openalex.org/W4388497846",
        "https://openalex.org/W4385571830",
        "https://openalex.org/W4385573947",
        "https://openalex.org/W4386907292",
        "https://openalex.org/W4388108510",
        "https://openalex.org/W4307123345",
        "https://openalex.org/W4386080900",
        "https://openalex.org/W4308627220",
        "https://openalex.org/W4385270173",
        "https://openalex.org/W4385152093",
        "https://openalex.org/W4221158828",
        "https://openalex.org/W4385570698",
        "https://openalex.org/W4385849010",
        "https://openalex.org/W4285199616",
        "https://openalex.org/W4385236616",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4376653418",
        "https://openalex.org/W4385570369",
        "https://openalex.org/W2989539713",
        "https://openalex.org/W3174318528",
        "https://openalex.org/W4384025135",
        "https://openalex.org/W4386148639",
        "https://openalex.org/W4362515116",
        "https://openalex.org/W4387355711",
        "https://openalex.org/W4321605350",
        "https://openalex.org/W4380993995",
        "https://openalex.org/W4385571637",
        "https://openalex.org/W4386081444",
        "https://openalex.org/W2091409162",
        "https://openalex.org/W4384816574",
        "https://openalex.org/W4387806067",
        "https://openalex.org/W4380033094",
        "https://openalex.org/W2185499427",
        "https://openalex.org/W3206487987",
        "https://openalex.org/W3150395569",
        "https://openalex.org/W4315498228",
        "https://openalex.org/W4385848840",
        "https://openalex.org/W3173465197",
        "https://openalex.org/W3046764764",
        "https://openalex.org/W2157151973",
        "https://openalex.org/W87210284",
        "https://openalex.org/W4385750020",
        "https://openalex.org/W4387432528",
        "https://openalex.org/W4386114340",
        "https://openalex.org/W4320854935",
        "https://openalex.org/W4320843360",
        "https://openalex.org/W4384304865",
        "https://openalex.org/W4385571453",
        "https://openalex.org/W4391724817",
        "https://openalex.org/W4386977562",
        "https://openalex.org/W4386572529",
        "https://openalex.org/W4378474356",
        "https://openalex.org/W4321354332",
        "https://openalex.org/W4384390290",
        "https://openalex.org/W4362597819",
        "https://openalex.org/W4386651406",
        "https://openalex.org/W4321277276",
        "https://openalex.org/W4388143984",
        "https://openalex.org/W4383468784",
        "https://openalex.org/W4319240918",
        "https://openalex.org/W4387688022",
        "https://openalex.org/W3213881810",
        "https://openalex.org/W4367365458",
        "https://openalex.org/W1679907412",
        "https://openalex.org/W3155981360",
        "https://openalex.org/W4388581500",
        "https://openalex.org/W4387528238",
        "https://openalex.org/W4367694420",
        "https://openalex.org/W4388049829",
        "https://openalex.org/W4225108562",
        "https://openalex.org/W4386033770",
        "https://openalex.org/W4386436218",
        "https://openalex.org/W4383045326",
        "https://openalex.org/W4388110106",
        "https://openalex.org/W3156498837",
        "https://openalex.org/W4387724711",
        "https://openalex.org/W4387389711",
        "https://openalex.org/W4281391846",
        "https://openalex.org/W4389983699",
        "https://openalex.org/W4378469471",
        "https://openalex.org/W4307412413",
        "https://openalex.org/W4376167433",
        "https://openalex.org/W4386721335",
        "https://openalex.org/W4384915881",
        "https://openalex.org/W4387724821",
        "https://openalex.org/W4281774243",
        "https://openalex.org/W4388926496",
        "https://openalex.org/W4387500346",
        "https://openalex.org/W4387192100",
        "https://openalex.org/W4366208482",
        "https://openalex.org/W4387838452",
        "https://openalex.org/W4386302907"
    ],
    "abstract": "Large Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language understanding and generation. They possess deep language comprehension, human-like text generation capabilities, contextual awareness, and robust problem-solving skills, making them invaluable in various domains (e.g., search engines, customer support, translation). In the meantime, LLMs have also gained traction in the security community, revealing security vulnerabilities and showcasing their potential in security-related tasks. This paper explores the intersection of LLMs with security and privacy. Specifically, we investigate how LLMs positively impact security and privacy, potential risks and threats associated with their use, and inherent vulnerabilities within LLMs. Through a comprehensive literature review, the paper categorizes the papers into \"The Good\" (beneficial LLM applications), \"The Bad\" (offensive applications), and \"The Ugly\" (vulnerabilities of LLMs and their defenses). We have some interesting findings. For example, LLMs have proven to enhance code security (code vulnerability detection) and data privacy (data confidentiality protection), outperforming traditional methods. However, they can also be harnessed for various attacks (particularly user-level attacks) due to their human-like reasoning abilities. We have identified areas that require further research efforts. For example, Research on model and parameter extraction attacks is limited and often theoretical, hindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development, requires more exploration. We hope that our work can shed light on the LLMs' potential to both bolster and jeopardize cybersecurity.",
    "full_text": "A Survey on Large Language Model (LLM) Security and Privacy:\nThe Good, the Bad, and the Ugly\nYifan Yao, Jinhao Duan, Kaidi Xu, Yuanfang Cai, Zhibo Sun and Yue Zhang\naDrexel University, 3675 Market St., Philadelphia, PA, 19104, USA\nARTICLE INFO\nKeywords:\nLarge Language Model (LLM), LLM\nSecurity,LLMPrivacy,ChatGPT,LLM\nAttacks, LLM Vulnerabilities\nABSTRACT\nLarge Language Models (LLMs), such as ChatGPT and Bard, have revolutionized natural language\nunderstanding and generation. They possess deep language comprehension, human-like text genera-\ntioncapabilities,contextualawareness,androbustproblem-solvingskills,makingtheminvaluablein\nvarious domains (e.g., search engines, customer support, translation). In the meantime, LLMs have\nalsogainedtractioninthesecuritycommunity,revealingsecurityvulnerabilitiesandshowcasingtheir\npotentialinsecurity-relatedtasks.ThispaperexplorestheintersectionofLLMswithsecurityandpri-\nvacy.Specifically,weinvestigatehowLLMspositivelyimpactsecurityandprivacy,potentialrisksand\nthreatsassociatedwiththeiruse,andinherentvulnerabilitieswithinLLMs.Throughacomprehensive\nliterature review, the paper categorizes the papers into “The Good” (beneficial LLM applications),\n“The Bad” (offensive applications), and “The Ugly” (vulnerabilities of LLMs and their defenses).\nWe have some interesting findings. For example, LLMs have proven to enhance code security (code\nvulnerability detection) and data privacy (data confidentiality protection), outperforming traditional\nmethods.However,theycanalsobeharnessedforvariousattacks(particularlyuser-levelattacks)due\nto their human-like reasoning abilities. We have identified areas that require further research efforts.\nFor example, Research on model and parameter extraction attacks is limited and often theoretical,\nhindered by LLM parameter scale and confidentiality. Safe instruction tuning, a recent development,\nrequiresmoreexploration.WehopethatourworkcanshedlightontheLLMs’potentialtobothbolster\nand jeopardize cybersecurity.\n1. Introduction\nAlargelanguagemodelisthelanguagemodelwithmassive\nparameters that undergoes pretraining tasks (e.g., masked\nlanguage modeling and autoregressive prediction) to un-\nderstand and process human language, by modeling the\ncontextualized text semantics and probabilities from large\namounts of text data. A capable LLM should have four\nkey features [323]: (i) profound comprehension of natural\nlanguage context; (ii) ability to generate human-like text;\n(iii)contextualawareness,especiallyinknowledge-intensive\ndomains; (iv) strong instruction-following ability which is\nuseful for problem-solving and decision-making.\nThere are a number of LLMs that were developed and\nreleased in 2023, gaining significant popularity. Notable\nexamples include OpenAI’s ChatGPT [203], Meta AI’s\nLLaMA [4], and Databricks’ Dolly 2.0 [50]. For instance,\nChatGPT alone boasts a user base of over 180 million [69].\nLLMsnowofferawiderangeofversatileapplicationsacross\nvarious domains. Specifically, they not only provide techni-\ncal support to domains directly related to language process-\ning (e.g., search engines [352, 13], customer support [259],\ntranslation [327, 138]) but also find utility in more general\nscenarios such as code generation [118], healthcare [274],\nfinance [310], and education [186]. This showcases their\nadaptability and potential to streamline language-related\ntasks across diverse industries and contexts.\nyy566@drexel.edu (Y. Yao);jd3734@drexel.edu (J. Duan);\nkx46@drexel.edu (K. Xu);yfcai@cs.drexel.edu (Y. Cai);zs384@drexel.edu\n(Z. Sun);yz899@drexel.edu (Y. Zhang)\nORCID(s):\nLLMs are gaining popularity within the security com-\nmunity. As of February 2023, a research study reported that\nGPT-3uncovered213securityvulnerabilities(only4turned\nout to be false positives) [141] in a code repository. In con-\ntrast, one of the leading commercial tools in the market de-\ntected only 99 vulnerabilities. More recently, several LLM-\npoweredsecuritypapershaveemergedinprestigiousconfer-\nences.Forinstance,inIEEES&P2023,HammondPearceet\nal. [211] conducted a comprehensive investigation employ-\ning various commercially available LLMs, evaluating them\nacross synthetic, hand-crafted, and real-world security bug\nscenarios. The results are promising, as LLMs successfully\naddressedallsyntheticandhand-craftedscenarios.InNDSS\n2024, a tool named Fuzz4All [313] showcased the use of\nLLMs for input generation and mutation, accompanied by\nan innovative autoprompting technique and fuzzing loop.\nTheseremarkableinitialattemptspromptustodelveinto\nthree crucial security-related research questions:\n• RQ1. How do LLMs make a positive impact on se-\ncurity and privacy across diverse domains, and what\nadvantages do they offer to the security community?\n• RQ2.Whatpotentialrisksandthreatsemergefromthe\nutilizationofLLMswithintherealmofcybersecurity?\n• RQ3. What vulnerabilities and weaknesses within\nLLMs, and how to defend against those threats?\nFindings. To comprehensively address these questions, we\nconducted a meticulous literature review and assembled\na collection of 281 papers pertaining to the intersection\nYifan Yao et al.:Preprint submitted to Elsevier Page 1 of 24\narXiv:2312.02003v3  [cs.CR]  20 Mar 2024\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nof LLMs with security and privacy. We categorized these\npapersintothreedistinctgroups:thosehighlightingsecurity-\nbeneficial applications (i.e., the good), those exploring ap-\nplications that could potentially exert adverse impacts on\nsecurity (i.e., the bad), and those focusing on the discus-\nsion of security vulnerabilities (alongside potential defense\nmechanisms) within LLMs (i.e., the ugly). To be more\nspecific:\n• TheGood(§4): LLMshaveapredominantlypositive\nimpactonthesecuritycommunity,asindicatedbythe\nmost significant number of papers dedicated to en-\nhancing security. Specifically, LLMs have made con-\ntributions to both code security and data security and\nprivacy. In the context of code security, LLMs have\nbeen used for the whole life cycle of the code (e.g.,\nsecure coding, test case generation, vulnerable code\ndetection, malicious code detection, and code fixing).\nIn data security and privacy, LLMs have been ap-\npliedtoensuredataintegrity,dataconfidentiality,data\nreliability, and data traceability. Meanwhile, Com-\npared to state-of-the-art methods, most researchers\nfound LLM-based methods to outperform traditional\napproaches.\n• TheBad(§5): LLMsalsohaveoffensiveapplications\nagainst security and privacy. We categorized the at-\ntacks into five groups: hardware-level attacks (e.g.,\nside-channel attacks), OS-level attacks (e.g., analyz-\ning information from operating systems), software-\nlevel attacks (e.g., creating malware), network-level\nattacks(e.g.,networkphishing),anduser-levelattacks\n(e.g., misinformation, social engineering, scientific\nmisconduct). User-level attacks, with 32 papers, are\nthe most prevalent due to LLMs’ human-like rea-\nsoning abilities. Those attacks threaten both security\n(e.g., malware attacks) and privacy (e.g., social engi-\nneering). Nowadays, LLMs lack direct access to OS\nand hardware-level functions. The potential threats of\nLLMs could escalate if they gain such access.\n• The Ugly (§6): We explore the vulnerabilities and\ndefenses in LLMs, categorizing vulnerabilities into\ntwo main groups: AI Model Inherent Vulnerabili-\nties (e.g., data poisoning, backdoor attacks, training\ndata extraction) and Non-AI Model Inherent Vul-\nnerabilities (e.g., remote code execution, prompt in-\njection, side channels). These attacks pose a dual\nthreat, encompassing both security concerns (e.g.,\nremote code execution attacks) and privacy issues\n(e.g.,dataextraction).DefensesforLLMsaredivided\ninto strategies placed in the architecture, and applied\nduring the training and inference phases. Training\nphase defenses involve corpora cleaning, and opti-\nmization methods, while inference phase defenses\ninclude instruction pre-processing, malicious detec-\ntion, and generation post-processing. These defenses\ncollectively aim to enhance the security, robustness,\nand ethical alignment of LLMs. We found that model\nextraction, parameter extraction, and similar attacks\nhave received limited research attention, remaining\nprimarily theoretical with minimal practical explo-\nration.ThevastscaleofLLMparametersmakestradi-\ntional approaches less effective, and the confidential-\nity of powerful LLMs further shields them from con-\nventional attacks. Strict censorship of LLM outputs\nchallenges even black-box ML attacks. Meanwhile,\nresearch on the impact of model architecture on LLM\nsafety is scarce, partly due to high computational\ncosts. Safe instruction tuning, a recent development,\nrequires further investigation.\nContributions. Our work makes a dual contribution. First,\nwe are pioneers summarizing the role of LLMs in security\nand privacy. We delve deeply into the positive impacts of\nLLMs on security, their potential risks and threats, vulner-\nabilities in LLMs, and the corresponding defense mecha-\nnisms. Other surveys may focus on one or two specific as-\npects,suchasbeneficialapplications,offensiveapplications,\nvulnerabilities, or defenses. To the best of our knowledge,\nour survey is the first to cover all three key aspects related\nto security and privacy for the first time. Second, we have\nmade several interesting discoveries. For instance, our re-\nsearch reveals that LLMs contribute more positively than\nnegatively to security and privacy. Moreover, we observe\nthat most researchers concur that LLMs outperform state-\nof-the-art methods when employed for securing code or\ndata.Concurrently,itbecomesevidentthatuser-levelattacks\nare the most prevalent, largely owing to the human-like\nreasoning abilities exhibited by LLMs.\nRoadmap.Therestofthepaperisorganizedasfollows.We\nbegin with a brief introduction to LLM in §2. §3 presents\nthe overview of our work. In §4, we explore the beneficial\nimpacts of employing LLMs. §5 discusses the negative im-\npactsonsecurityandprivacy.In§6,wediscusstheprevalent\nthreats, vulnerabilities associated with LLMs as well as the\ncountermeasures to mitigate these risks. §7 discuss LLMs\nin other security related topics and possible directions. We\nconclude the paper in §9.\n2. Background\n2.1. Large Language Models (LLMs)\nLarge Language Models (LLMs) [347] represents an evolu-\ntion from language models. Initially, language models were\nstatistical in nature and laid the groundwork for compu-\ntational linguistics. The advent of transformers has signif-\nicantly increased their scale. This expansion, along with\nthe use of extensive training corpora and advanced pre-\ntrainingtechniquesispivotalinareassuchasAIforscience,\nlogical reasoning, and embodied AI. These models undergo\nextensive training on vast datasets to comprehend and pro-\nduce text that closely mimics human language. Typically,\nLLMsareendowedwithhundredsofbillions,orevenmore,\nYifan Yao et al.:Preprint submitted to Elsevier Page 2 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nparameters, honed through the processing of massive tex-\ntual data. They have spearheaded substantial advancements\nin the realm of Natural Language Processing (NLP) [82]\nand find applications in a multitude of fields (e.g., risk\nassessment [202], programming [26], vulnerability detec-\ntion [118], medical text analysis [274], and search engine\noptimization [13]).\nBased on Yang’s study [323], an LLM should have\nat least four key features. First, an LLM should demon-\nstrate a deep understanding and interpretation of natural\nlanguagetext,enablingittoextractinformationandperform\nvarious language-related tasks (e.g., translation). Second,\nit should have the capacity to generate human-like text\n(e.g., completing sentences, composing paragraphs, and\neven writing articles) when prompted. Third, LLMs should\nexhibit contextual awareness by considering factors such\nas domain expertise, a quality referred to as “Knowledge-\nintensive”. Fourth, these models should excel in problem-\nsolvinganddecision-making,leveraginginformationwithin\ntext passages to make them invaluable for tasks such as\ninformation retrieval and question-answering systems.\n2.2. Comparison of Popular LLMs\nAs shown in Table 1 [276, 235], there is a diversity of\nproviders for language models, including industry leaders\nsuch as OpenAI, Google, Meta AI, and emerging players\nsuch as Anthropic and Cohere. The release dates span from\n2018to2023,showcasingtherapiddevelopmentandevolu-\ntionoflanguagemodelsinrecentyears.Newermodelssuch\nas “gpt-4” have emerged in 2023, highlighting the ongoing\ninnovation in this field. While most of the models are not\nopen-source,itisinterestingtonotethatmodelslikeBERT,\nT5, PaLM, LLaMA, and CTRL are open-source, which can\nfacilitate community-driven development and applications.\nLarger models tend to have more parameters, potentially\nindicating increased capabilities but also greater computa-\ntional demands. For example, “PaLM” stands out with a\nmassive 540 billion parameters. It can also be observed\nthat LLMs tend to have more parameters, potentially indi-\ncating increased capabilities but also greater computational\ndemands. The “Tunability” column suggests whether these\nmodels can be fine-tuned for specific tasks. In other words,\nit is possible to take a large, pre-trained language model\nand adjust its parameters and training on a smaller, domain-\nspecific dataset to make it perform better on a particular\ntask. For instance, with tunability, one can fine-tune BERT\non a dataset of movie reviews to make it highly effective at\nsentiment analysis.\n3. Overview\n3.1. Scope\nOurpaperendeavorstoconductathoroughliteraturereview,\nwith the objective of collating and scrutinizing existing\nresearchandstudiesabouttherealmsofsecurityandprivacy\nin the context of LLMs. The effort is geared towards both\nestablishing the current state of the art in this domain and\nTable 1\nComparison of Popular LLMs\nModel Date Provider Open-Source Params Tunability\ngpt-4 [64] 2023.03 OpenAI ✗ 1.7T ✗\ngpt-3.5-turbo 2021.09 OpenAI ✗ 175B ✗\ngpt-3 [24] 2020.06 OpenAI ✗ 175B ✗\ncohere-medium [170] 2022.07 Cohere ✗ 6B ✓\ncohere-large [170] 2022.07 Cohere ✗ 13B ✓\ncohere-xlarge [170] 2022.06 Cohere ✗ 52B ✓\nBERT [61] 2018.08 Google ✓ 340M ✓\nT5 [225] 2019 Google ✓ 11B ✓\nPaLM [198] 2022.04 Google ✓ 540B ✓\nLLaMA [4] 2023.02 Meta AI ✓ 65B ✓\nCTRL [229] 2019 Salesforce ✓ 1.6B ✓\nDolly 2.0 [50] 2023.04 Databricks ✓ 12B ✓\npinpointing gaps in our collective knowledge. While it is\ntrue that LLMs wield multifaceted applications extending\nbeyond security considerations (e.g., social and financial\nimpacts), our primary focus remains steadfastly on matters\nof security and privacy. Moreover, it is noteworthy that\nGPT models have attained significant prominence within\nthis landscape. Consequently, when delving into specific\ncontent and examples, we aim to employ GPT models as\nillustrative benchmarks.\n3.2. The Research Questions\nLLMs have carried profound implications across diverse\ndomains. However, it is essential to recognize that, as with\nany powerful technology, LLMs bear a significant respon-\nsibility. Our paper delves deeply into the multifaceted role\nof LLMs in the context of security and privacy. We intend\nto scrutinize their positive contributions to these domains,\nexplorethepotentialthreatstheymayengender,anduncover\nthe vulnerabilities that could compromise their integrity. To\naccomplishthis,ourstudywillconductathoroughliterature\nreview centered around three pivotal research questions:\n• The Good (§4):How do LLMs positively contribute\nto security and privacy in various domains, and what\nare the potential benefits they bring to the security\ncommunity?\n• TheBad(§5): Whatarethepotentialrisksandthreats\nassociated with the use of LLMs in the context of cy-\nbersecurity? Specifically, how can LLMs be used for\nmalicious purposes, and what types of cyber attacks\ncan be facilitated or amplified using LLMs?\n• The Ugly (§6):What vulnerabilities and weaknesses\nexist within LLMs, and how do these vulnerabilities\npose a threat to security and privacy?\nMotivated by these questions, we conducted a search on\nGoogle Scholar and compiled papers related to security and\nprivacy involving LLMs. As shown in Figure 1, we gath-\nered a total of 83 “good” papers that highlight the positive\ncontributionsofLLMstosecurityandprivacy.Additionally,\nwe identified 54 “bad” papers, in which attackers exploited\nLLMs to target users, and 144 “ugly” papers, in which\nauthorsdiscoveredvulnerabilitieswithinLLMs.Mostofthe\nYifan Yao et al.:Preprint submitted to Elsevier Page 3 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nJan\nFeb\nMar\nAprMay\nJun\nJulAug\nSep\nOct\nNov\nDec\nJan\nFeb\nMar\nApr May\nJun\nJul\nAug Sep\nOct\nNov\nDec\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nGood: 83\nBad: 54\nUgly: 144\nFigure 1:An overview of our collected papers.\npaperswerepublishedin2023,withonly82ofthemreleased\nin between 2007 and 2022. Notably, there is a consistent\nupward trend in the number of papers released each month,\nwith October reaching its peak, boasting the highest num-\nber of papers published (38 papers in total, accounting for\n15.97% of all the collected papers). It is conceivable that\nmore security-related LLM papers will be published in the\nnear future.\nFinding I. In terms of security-related applications\n(i.e., the “good” and the “bad” parts), it is evident that\nthe majority of researchers are inclined towards using\nLLMs to bolster the security community, such as in\nvulnerability detection and security test generation,\ndespite the presence of some vulnerabilities in LLMs\nat this stage. There are relatively few researchers\nwho employ LLMs as tools for conducting attacks.\nIn summary, LLMs contribute more positively than\nnegatively to the security community.\n4. Positive Impacts on Security and Privacy\nInthissection,weexplorethebeneficialimpactsofemploy-\ning LLMs. In the context of code or data privacy, we have\nopted to use the term “privacy” to characterize scenarios\nin which LLMs are utilized to ensure the confidentiality of\neither code or data. However, given that we did not come\nacross any papers specifically addressing code privacy, our\ndiscussion focuses on code security (§4.1) as well as both\ndata security and privacy (§4.2).\n4.1. LLMs for Code Security\nAsshowninTable2,LLMshaveaccesstoavastrepository\nof code snippets and examples spanning various program-\nming languages and domains. They leverage their advanced\nlanguage understanding and contextual analysis capabilities\nto thoroughly examine code and code-related text. More\nspecifically, LLMs can play a pivotal role throughout the\nentirecodesecuritylifecycle,includingcoding(C),testcase\ngeneration (TCG), execution, and monitoring (RE).\nSecureCoding(C). WefirstdiscusstheuseofLLMsinthe\ncontextofsecurecodeprogramming[75](orgeneration[63,\n285, 199, 90]). Sandoval et al. [234] conducted a user study\n(58 users) to assess the security implications of LLMs, par-\nticularly OpenAI Codex, as code assistants for developers.\nThey evaluated code written by student programmers when\nassisted by LLMs and found that participants assisted by\nLLMs did not introduce new security risks: the AI-assisted\ngroup produced critical security bugs at a rate no greater\nthan10%higherthanthecontrolgroup(non-assisted).Heet\nal.[98,99]focusedonenhancingthesecurityofcodegener-\natedbyLLMs.Theyproposedanovelmethodcalled SVEN,\nwhich leverages continuous prompts to control LLMs in\ngenerating secure code. With this method, the success rate\nimproved from 59.1% to 92.3% when using the CodeGen\nLM.Mohammedetal.introduce SALLM [254],aframework\nconsisting of a new security-focused dataset, an evaluation\nenvironment, and novel metrics for systematically assessing\nLLMs’ ability to generate secure code. Madhav et al. [197]\nevaluatethesecurityaspectsofcodegenerationprocesseson\ntheChatGPTplatform,specificallyinthehardwaredomain.\nTheyexplorethestrategiesthatadesignercanemploytoen-\nable ChatGPT to provide secure hardware code generation.\nTest Case Generating (TCG).Several papers [33, 6, 238,\n316, 156, 253, 335] discuss the utilization of LLMs for\ngenerating test cases, with our particular emphasis on those\naddressing security implications. Zhang et al. [343] demon-\nstrated the use of ChatGPT-4.0 for generating security tests\nto assess the impact of vulnerable library dependencies on\nsoftwareapplications.TheyfoundthatLLMscouldsuccess-\nfully generate tests that demonstrated various supply chain\nattacks,outperformingexistingsecuritytestgenerators.This\napproach resulted in 24 successful attacks across 55 appli-\ncations.Similarly, Libro [136],aframeworkthatusesLLMs\nto automatically generate test cases to reproduce software\nsecurity bugs.\nIn the realm of security, fuzzing stands [325, 109,\n337, 345, 272] out as a widely employed technique for\ngeneratingtestcases.Dengetal.introduced TitanFuzz [56],\nan approach that harnesses LLMs to generate input pro-\ngrams for fuzzing Deep Learning (DL) libraries. TitanFuzz\ndemonstrates impressive code coverage (30.38%/50.84%)\nand detects previously unknown bugs (41 out of 65) in\npopular DL libraries. More recently, Deng et al. [58, 57]\nrefined LLM-based fuzzing (named FuzzGPT), aiming to\ngenerate unusual programs for DL library fuzzing. While\nYifan Yao et al.:Preprint submitted to Elsevier Page 4 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nTable 2\nLLMs for Code Security and Privacy\nWork\nLife Cycle\nLLM(s) Domain When compared to\nSOTA ways?Coding (C) Test Case\nGenerating\n(TCG)\nRunning and Executing (RE)\nBug\nDetecting\nMalicious\nCode Detecting\nVulnerability\nDetecting Fixing\nSandoval et al. [234] ○ ○␣ ○␣ ○␣ ○␣ ○␣ Codex - Negligible risks\nSVEN [98] ○ ○␣ ○␣ ○␣ ○␣ ○␣ CodeGen - More faster/secure\nSALLM [254] ○ ○␣ ○␣ ○␣ ○␣ ○␣ ChatGPT etc. - -\nMadhav et al. [197] ○ ○␣ ○␣ ○␣ ○␣ ○␣ ChatGPT Hardware -\nZhang et al. [343] ○␣ ○ ○␣ ○␣ ○ ○␣ ChatGPT Supply chain More valid cases\nLibro [136] ○␣ ○ ○␣ ○␣ ○ ○␣ LLaMA - Higher FP/FN\nTitanFuzz [56] ○␣ ○ ○ ○␣ ○ ○␣ Codex DL libs Higher coverage\nFuzzGPT [57] ○␣ ○ ○ ○␣ ○ ○␣ ChatGPT DL libs Higher coverage\nFuzz4All [313] ○␣ ○ ○ ○␣ ○ ○␣ ChatGPT Languages Higher coverage\nWhiteFox [321] ○␣ ○ ○ ○␣ ○ ○␣ GPT4 Compiler High-quality tests\nZhang et al. [337] ○␣ ○ ○ ○␣ ○ ○␣ ChatGPT API -\nCHATAFL [190] ○␣ ○ ○ ○␣ ○ ○␣ ChatGPT Protocol Higher coverage\nHenrik [105] ○␣ ○␣ ○␣ ○ ○␣ ○␣ ChatGPT - Higer FP/FN\nApiiro [74] ○␣ ○␣ ○␣ ○ ○␣ ○␣ N/A - -\nNoever [201] ○␣ ○␣ ○␣ ○␣ ○ ○ ChatGPT - 4X faster\nBakhshandeh et al. [15] ○␣ ○␣ ○␣ ○␣ ○ ○␣ ChatGPT - Low FP/FN\nMoumita et al. [218] ○␣ ○␣ ○␣ ○␣ ○ ○␣ ChatGPT - Higher FP/FN\nCheshkov et al. [41] ○␣ ○␣ ○␣ ○␣ ○ ○␣ ChatGPT - No better\nLATTE [174] ○␣ ○␣ ○␣ ○␣ ○ ○␣ GPT - Cost effective\nDefectHunter [296] ○␣ ○␣ ○␣ ○␣ ○ ○␣ Codex - -\nChen et al. [37] ○␣ ○␣ ○␣ ○␣ ○ ○␣ ChatGPT Blockchain -\nHu et al. [110] ○␣ ○␣ ○␣ ○␣ ○ ○␣ ChatGPT Blockchain -\nKARTAL [233] ○␣ ○␣ ○␣ ○␣ ○ ○␣ ChatGPT Web apps Less manual\nVulLibGen [38] ○␣ ○␣ ○␣ ○␣ ○ ○␣ LLaMa Libs Higher accuracy/speed\nAhmad et al. [3] ○␣ ○␣ ○␣ ○␣ ○ ○ Codex Hardware Fix more bugs\nInferFix [125] ○␣ ○␣ ○ ○␣ ○ ○ Codex - CI Pipeline\nPearce et al. [211] ○␣ ○␣ ○ ○␣ ○ ○␣ Codex etc. - Zero-shot\nFu et al. [83] ○␣ ○␣ ○ ○␣ ○ ○ ChatGPT APR Higher accuracy\nSobania et al. [257] ○␣ ○␣ ○␣ ○␣ ○␣ ○ ChatGPT etc. APR Higher accuracy\nJiang et al. [123] ○␣ ○␣ ○␣ ○␣ ○␣ ○ ChatGPT APR Higher accuracy\nTitanFuzz leverages LLMs’ ability to generate ordinary\ncode, FuzzGPT addresses the need for edge-case testing\nby priming LLMs with historical bug-triggering program.\nFuzz4All[313]leveragesLLMsasinputgeneratorsandmu-\ntation engines, creating diverse and realistic inputs for vari-\nous languages (e.g., C, C++), improving the previous state-\nof-the-art coverage by 36.8% on average.WhiteFox [321],\na novel white-box compiler fuzzer that utilizes LLMs to\ntestcompileroptimizations,outperformsexistingfuzzers(it\ngenerates high-quality tests for intricate optimizations, sur-\npassing state-of-the-art fuzzers by up to 80 optimizations).\nZhang et al. [337] explore the generation of fuzz drivers for\nlibrary API fuzzing using LLMs. Results show that LLM-\nbased generation is practical, with 64% of questions solved\nentirelyautomaticallyandupto91%withmanualvalidation.\nCHATAFL[190]isanLLM-guidedprotocolfuzzerthatcon-\nstructs grammars for message types and mutates messages\nor predicts the next messages based on LLM interactions,\nachieving better state and code coverage compared to state-\nof-the-art fuzzers (e.g., AFLNET [217], NSFUZZ [222]).\nVulnerable Code Detecting (RE).Noever [201] explores\nthe capability of LLMs, particularly OpenAI’s GPT-4, in\ndetecting software vulnerabilities. This paper shows that\nGPT-4 identified approximately four times the number of\nvulnerabilities compared to traditional static code analyzers\n(e.g.,SnykandFortify).Parallelconclusionshavealsobeen\ndrawn in other efforts [141, 15]. However, Moumita et\nal. [218] applied LLMs for software vulnerability detection,\nexposing a noticeable performance gap when compared to\nconventional static analysis tools. This disparity primarily\narises from the relatively higher occurrence of false alerts\ngenerated by LLMs. Similarly, Cheshkov et al. [41] point\nout that the ChatGPT model performed no better than a\ndummy classifier for both binary and multi-label classifi-\ncation tasks in code vulnerability detection. Wang et al.\nintroduce DefectHunter [296], a novel model that employs\nLLM-driven techniques for code vulnerability detection.\nThey demonstrate the potential of combining LLMs with\nadvancedmechanisms(e.g.,Conformer)toidentifysoftware\nvulnerabilities more effectively. This combination shows an\nimprovement in effectiveness, approximately from 14.64%\nto 20.62%, compared with Pongo-70B.LATTE [174] is a\nnovelstaticbinarytaintanalysismethodpoweredbyLLMs.\nLATTE surpasses existing state-of-the-art techniques (e.g.,\nEmtaint, Arbiter, and Karonte), demonstrating remarkable\neffectivenessinvulnerabilitydetection(37newbugsinreal-\nworld firmware) with lower cost.\nEfforts in leveraging LLMs for vulnerability detection\nextend to specialized domains (e.g.,blockchain [110, 37],\nkernel [104] mobile [303]). For instance, Chen et al. [37]\nand Hu et al. [110] focus on the application of LLMs\nin identifying vulnerabilities within blockchain smart con-\ntracts. Sakaoglu’s study introduces KARTAL [233], a pio-\nneering approach that harnesses LLMs for web application\nvulnerability detection. This method achieves an accuracy\nYifan Yao et al.:Preprint submitted to Elsevier Page 5 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nTable 3\nLLMs for Data Security and Privacy\nWork Prop. Model Domain Compared to\nSOTA ways?\nI C R T\nFang [294] ○ ○␣ ○ ○␣ChatGPT Ransomware -\nLiu et al. [187] ○ ○␣ ○ ○␣ChatGPT Ransomware -\nAmine et al. [73] ○ ○ ○ ○␣ChatGPT Semantic Aligned w/ SOTA\nHuntGPT [8] ○ ○ ○ ○␣ChatGPT Network More effective\nChris et al. [71] ○ ○ ○ ○␣ChatGPT Log Less manual\nAnomalyGPT [91] ○ ○ ○ ○␣ChatGPT Video Less manual\nLogGPT [221] ○ ○ ○ ○␣ChatGPT Log Less manual\nArpita et al. [286] ○␣ ○ ○␣ ○␣BERT etc. - -\nTakashi et al. [142]○␣ ○␣ ○ ○␣ChatGPT Phishing High precision\nFredrik et al. [102]○␣ ○␣ ○ ○␣ChatGPT etc Phishing Effective\nIPSDM [119] ○␣ ○␣ ○ ○␣BERT Phishing -\nKwon et al. [149] ○␣ ○ ○␣ ○␣ChatGPT - Non-exp friendly\nScanlon et al. [237]○␣ ○␣ ○␣ ○ChatGPT Forensic More effective\nSladić et al. [255] ○␣ ○␣ ○␣ ○ChatGPT Honeypot More realistic\nWASA [297] ○␣ ○␣ ○ ○ - Watermark More effective\nREMARK [340] ○␣ ○␣ ○ ○ - Watermark More effective\nSWEET [154] ○␣ ○␣ ○ ○ - Watermark More effective\nof up to 87.19% and is capable of conducting 539 pre-\ndictions per second. Additionally, Chen et al. [38] make\na noteworthy contribution with VulLibGen, a generative\nmethodologyutilizingLLMstoidentifyvulnerablelibraries.\nAhmad et al. [3] shift the focus to hardware security. They\ninvestigate the use of LLMs, specifically OpenAI’s Codex,\nin automatically identifying and repairing security-related\nbugs in hardware designs. PentestGPT [55], an automated\npenetrationtestingtool,usesthedomainknowledgeinherent\nin LLMs to address individual sub-tasks of penetration\ntesting, improving task completion rates significantly.\nMalicious Code Detecting (RE). Using LLM to detect\nmalware is a promising application. This approach lever-\nages the natural language processing capabilities and con-\ntextual understanding of LLMs to identify malicious soft-\nware. In experiments with GPT-3.5 conducted by Henrik\nPlate[105],itwasfoundthatLLM-basedmalwaredetection\ncan complement human reviews but not replace them. Out\nof 1800 binary classifications performed, there were both\nfalse-positives and false-negatives. The use of simple tricks\ncould also deceive the LLM’s assessments. More recently,\nthereareafewattemptshavebeenmadeinthisdirection.For\nexample, Apiiro [74] is a malicious code analysis tool using\nLLMs.Apiiro’sstrategyinvolvesthecreationofLLMCode\nPatterns(LCPs)torepresentcodeinvectorformat,makingit\neasiertoidentifysimilaritiesandclusterpackagesefficiently.\nIts LCP detector incorporates LLMs, proprietary code anal-\nysis, probabilistic sampling, LCP indexing, and dimension-\nality reduction to identify potentially malicious code.\nVulnerable/BuggyCodeFixing(RE). Severalpapers[123,\n211, 314] has focused on evaluate the performance of\nLLMs trained on code in the task of program repair. Jin\net al. [125] proposedInferFix, a transformer-based program\nrepairframeworkthatworksintandemwiththecombination\nofcutting-edgestaticanalyzerwithtransformer-basedmodel\nto address and fix critical security and performance issues\nwith accuracy between 65% to 75%. Pearce et al. [211]\nobserved that LLMs can repair insecure code in a range of\ncontextsevenwithoutbeingexplicitlytrainedonvulnerabil-\nity repair tasks.\nChatGPTisnotedforitsabilityincodebugdetectionand\ncorrection.Fuetal.[83]assessedChatGPTinvulnerability-\nrelated tasks like predicting and classifying vulnerabilities,\nseverity estimation, and analyzing over 190,000 C/C++\nfunctions. They found that ChatGPT’s performance was\nbehind other LLMs specialized in vulnerability detection.\nHowever, Sobania et al. [257] found ChatGPT’s bug fix-\ning performance competitive with standard program repair\nmethods, as demonstrated by its ability to fix 31 out of\n40 bugs. Xia et al. [315] presentedChatRepair, leveraging\npre-trained language models (PLMs) for generating patches\nwithout dependency on bug-fixing datasets, aiming to en-\nhance performance to generate patches without relying on\nbug-fixing datasets, aiming to improve ChatGPT’s code-\nfixing abilities using a mix of successful and failure tests.\nAsaresult,theyfixed162outof337bugsatacostof$0.42\neach.\nFindingII. AsshowninTable2,acomparisonwithstate-\nof-the-artmethodsrevealsthatthemajorityofresearchers\n(17 out of 25) have concluded that LLM-based methods\noutperform traditional approaches (advantages include\nhighercodecoverage,higherdetectingaccuracy,lesscost\netc.). Only four papers argue that LLM-based methods\ndo not surpass the state-of-the-art appoarches. The most\nfrequently discussed issue with LLM-based methods is\ntheir tendency to produce both high false negatives and\nfalse positives when detecting vulnerabilities or bugs.\n4.2. LLMs for Data Security and Privacy\nAs demonstrated in Table 3, LLMs make valuable contri-\nbutions to the realm of data security, offering multifaceted\napproaches to safeguarding sensitive information. We have\norganized the research papers into distinct categories based\non the specific facets of data protection that LLMs enhance.\nThese facets encompass critical aspects such as data in-\ntegrity (I), which ensures that data remains uncorrupted\nthroughout its life cycle; data reliability (R), which ensures\ntheaccuracyofdata;dataconfidentiality(C),whichfocuses\non guarding against unauthorized access and disclosure\nof sensitive information; and data traceability (T), which\ninvolves tracking and monitoring data access and usage.\nData Integrity (I).Data Integrity ensures that data remains\nunchanged and uncorrupted throughout its life cycle. As of\nnow, there are a few works that discuss how to use LLMs to\nprotect data integrity. For example, ransomware usually en-\ncrypts a victim’s data, making the data inaccessible without\nadecryptionkeythatisheldbytheattacker,whichbreaksthe\ndata integrity. Wang Fang’s research [294] examines using\nLLMsforransomwarecybersecuritystrategies,mostlytheo-\nreticallyproposingreal-timeanalysis,automatedpolicygen-\neration, predictive analytics, and knowledge transfer. How-\never, these strategies lack empirical validation. Similarly,\nYifan Yao et al.:Preprint submitted to Elsevier Page 6 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nLiu et al. [187] explored the potential of LLMs for creating\ncybersecurity policies aimed at mitigating ransomware at-\ntacks with data exfiltration. They compared GPT-generated\nGovernance, Risk and Compliance (GRC) policies to those\nfrom established security vendors and government cyberse-\ncurity agencies. They recommended that companies should\nincorporate GPT into their GRC policy development.\nAnomaly detection is a key defense mechanism that\nidentifiesunusualbehavior.Whileitdoesnotdirectlyprotect\ndata integrity, it identifies abnormal or suspicious behavior\nthat can potentially compromise data integrity (as well as\ndataconfidentialityanddatareliability).Amineetal.[73]in-\ntroducedanLLM-basedmonitoringframeworkfordetecting\nsemantic anomalies in vision-based policies and applied it\nto both finite state machine policies for autonomous driving\nand learned policies for object manipulation. Experimental\nresults demonstrate that it can effectively identify semantic\nanomalies, aligning with human reasoning.HuntGPT [8]\nis an LLM-based intrusion detection system for network\nanomalydetection.Theresultsdemonstrateitseffectiveness\nin improving user understanding and interaction. Chris et\nal. [71] andLogGPT [221] explore ChatGPT’s potential for\nlog-basedanomalydetectioninparallelfilesystems.Results\nshow that it addresses the issues in traditional manual\nlabeling and interpretability.AnomalyGPT [91] uses Large\nVision-Language Models to detect industrial anomalies. It\neliminates manual threshold setting and supports multi-turn\ndialogues.\nData Confidentiality (C).Data confidentiality refers to the\npractice of protecting sensitive information from unautho-\nrized access or disclosure, a topic extensively discussed in\nLLMprivacydiscussions[214,242,286,1].However,most\nof these studies concentrate on enhancing LLMs through\nstate-of-the-art Privacy Enhancing Techniques (e.g., zero-\nknowledgeproofs[224],differentialprivacy(e.g.,[242,184,\n166], and federated learning [145, 122, 78]). There are only\na few attempts that utilize LLMs to enhance user privacy.\nFor example, Arpita et al. [286] use LLMs to preserve\nprivacy by replacing identifying information in textual data\nwith generic markers. Instead of storing sensitive user in-\nformation, such as names, addresses, or credit card num-\nbers, the LLMs suggest substitutes for the masked tokens.\nThis obfuscation technique helps to protect user data from\nbeing exposed to adversaries. By using LLMs to generate\nsubstitutes for masked tokens, the models can be trained on\nobfuscateddatawithoutcompromisingtheprivacyandsecu-\nrityoftheoriginalinformation.Similarideashavealsobeen\nexplored in other studies [1, 262]. Hyeokdong et al. [149]\nexplore implementing cryptography with ChatGPT, which\nultimately protects data confidentiality. Despite the lack\nof extensive coding skills or programming knowledge, the\nauthors were able to successfully implement cryptographic\nalgorithms through ChatGPT. This highlights the potential\nfor individuals to utilize ChatGPT for cryptography tasks.\nDataReliability(R). Inourcontext,datareliabilityrefersto\ntheaccuracyofdata.Itisameasureofhowwelldatacanbe\ndepended upon to be accurate, and free from errors or bias.\nTakashi et al. [142] proposed to use ChatGPT for the de-\ntection of sites that contain phishing content. Experimental\nresultsusingGPT-4showpromisingperformance,withhigh\nprecision and recall rates. Fredrik et al. [102] assessed the\nability of four large language models (GPT, Claude, PaLM,\nand LLaMA) to detect malicious intent in phishing emails,\nandfoundthattheyweregenerallyeffective,evensurpassing\nhuman detection, although occasionally slightly less accu-\nrate. IPSDM [119] is a model fine-tuned from the BERT\nfamily to identify phishing and spam emails effectively.\nIPSDM demonstrates superior performance in classifying\nemails, both in unbalanced and balanced datasets.\nData Traceability (T).Data traceability is the capability\nto track and document the origin, movement, and history\nof data within a single system or across multiple systems.\nThis concept is particularly vital in fields such as incident\nmanagement and forensic investigations, where understand-\ning the journey and transformations of events to resolving\nissuesandconductingthoroughanalyses.LLMshavegained\ntractioninforensicinvestigations,offeringnovelapproaches\nfor analyzing digital evidence. Scanlon et al. [237] explored\nhow ChatGPT assists in analyzing OS artifacts like logs,\nfiles,cloudinteractions,executablebinaries,andinexamin-\ning memory dumps to detect suspicious activities or attack\npatterns.Additionally,Sladićetal.[255]proposedthatgen-\nerative models like ChatGPT can be used to create realistic\nhoneypots to deceive human attackers.\nWatermarking involves embedding a distinctive, typ-\nically imperceptible or hard-to-identify signal within the\noutputs of a model. Wang et al. [297] discusses concerns\nregardingtheintellectualpropertyoftrainingdataforLLMs\nand proposedWASA framework to learn the mapping be-\ntweenthetextsofdifferentdataproviders.Zhangetal.[340]\ndevelopedREMARK-LLM that focused on monitor the uti-\nlization of their content and validate their watermark re-\ntrieval. This helps protect against malicious uses such as\nspamming and plagiarism. Furthermore, identifying code\nproduced by LLMs is vital for addressing legal and ethical\nissues concerning code licensing, plagiarism, and malware\ncreation.Similarly,Lietal.[169]proposethefirstwatermark\ntechnique to protect large language model-based code gen-\neration APIs from remote imitation attacks. Lee et al. [154]\ndeveloped SWEET, a tool that implements watermarking\nspecifically on tokens within programming languages.\nFinding III. Likewise, it is noticeable that LLMs\nexcel in data protection, surpassing current solutions and\nrequiringfewermanualinterventions.Table2andTable3\nrevealthatChatGPTisthepredominantLLMextensively\nemployed in diverse security applications. Its versatility\nand effectiveness make it a preferred choice for various\nsecurity-related tasks, further reinforcing its position as\na go-to solution in the field of artificial intelligence and\ncybersecurity.\nYifan Yao et al.:Preprint submitted to Elsevier Page 7 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nHardware Attacks\nOS Attacks\nSoftware Attacks\nNetwork Attacks\nUser-based Attacks\nCyber \nAttacks\nHardware \nAttacks\nOS\nAttacks\nNetwork Attacks\n User Attacks\nHardware\nSide-Channel \nAttacks\nMalicious \nFirmware\nEvil Maid \nAttacks\nFault \nInjection\nPrivilege \nEscalation\nRootkit\nOS RCE\nMemory \nAttacks\nOS DoS\nSoftware\nExploitation\nFingerprinting\nAttacks\nOS\nSide-Channel \nAttacks\nSoftware\nDoS\nSoftware\nSide-Channel \nAttacks\nRace \nConditions\nBOF Attacks\nSoftware\nAttacks\nPhishing \nAttacks\nMalware \nAttacks\nKeylogger\nWorm\nRansomware\nBrute-force\nFileless\n CAPTCHA \nBreaking\nSQL Injection\nXSS Attacks\nCSRF Attacks\nWeb DoS\nWeb Side-\nChannel\nAttacks\nCookie Theft\nDirectory \nTraversal\nSocial \nEngineering\nFraud\nMisinformation \nSpear Phishing\nCredential \nStuffing\nTailgating\nScientific \nmisconduct\nImpersonation\nFigure 2: Taxonomy of Cyberattacks. The colored boxes rep-\nresent attacks that have been demonstrated to be executable\nusing LLMs, whereas the gray boxes indicate attacks that\ncannot be executed with LLMs.\nHardware OS Software Network User\nFigure 3:Prevalence of the existing attacks\n5. Negative Impacts on Security and Privacy\nAs shown in Figure 2, we have categorized the attacks into\nfive groups based on their respective positions within the\nsysteminfrastructure.Thesecategoriesencompasshardware-\nlevelattacks,OS-levelattacks,software-levelattacks,network-\nlevel attacks, and user-level attacks. Additionally, we have\nquantified the number of associated research papers pub-\nlished for each group, as illustrated in Figure 3.\nHardware-Level Attacks.Hardware attacks typically in-\nvolvephysicalaccesstodevices.However,LLMscannotdi-\nrectly access physical devices. Instead, they can only access\ninformation associated with the hardware. Side-channel at-\ntack[260,107,189]isoneattackthatcanbepoweredbythe\nLLMs. Side-channel attacks typically entail the analysis of\nunintentionalinformationleakagefromaphysicalsystemor\nimplementation,suchasacryptographicdeviceorsoftware,\nwith the aim of inferring secret information (e.g., keys).\nYaman [319] has explored the application of LLM tech-\nniques to develop side-channel analysis methods. The re-\nsearchevaluatestheeffectivenessofLLM-basedapproaches\nin analyzing side-channel information in two hardware-\nrelated scenarios: AES side-channel analysis and deep-\nlearning accelerator side-channel analysis. Experiments are\nconducted to determine the success rates of these methods\nin both situations.\nOS-LevelAttacks. LLMsoperateatahighlevelofabstrac-\ntion and primarily engage with text-based input and output.\nThey lack the necessary low-level system access essential\nforexecutingOS-levelattacks[114,288,128].Nonetheless,\nthey can be utilized for the analysis of information gath-\nered from operating systems, thus potentially aiding in the\nexecution of such attacks. Andreas et al. [94] establish a\nfeedback loop connecting LLM to a vulnerable virtual ma-\nchinethroughSSH,allowingLLMtoanalyzethemachine’s\nstate, identify vulnerabilities, and propose concrete attack\nstrategies, which are then executed automatically within\nthe virtual machine. More recently, they [95] introduced an\nautomatedLinuxprivilege-escalationbenchmarkusinglocal\nvirtual machines and an LLM-guided privilege-escalation\ntool to assess various LLMs and prompt strategies against\nthe benchmark.\nSoftware-Level Attacks.Similar to how they employ LLM\nto target hardware and operating systems, there are also\ninstances where LLM has been utilized to attack software\n(e.g., [343, 209, 212, 32]). However, the most prevalent\nsoftware-level use case involves malicious developers uti-\nlizing LLMs to create malware. Mika et al. [17] present\na proof-of-concept in which ChatGPT is utilized to dis-\ntribute malicious software while avoiding detection. Yin et\nal.[207]investigatethepotentialmisuseofLLMbycreating\na number of malware programs (e.g., ransomware, worm,\nkeylogger, brute-force malware, Fileless malware). Anto-\nnio Monje et al. [194] demonstrate how to trick ChatGPT\ninto quickly generating ransomware. Marcus Botacin [22]\nexplores different coding strategies (e.g., generating entire\nmalware, creating malware functions) and investigates the\nLLM’s capacities to rewrite malware code. The findings re-\nvealthatLLMexcelsinconstructingmalwareusingbuilding\nblock descriptions. Meanwhile, LLM can generate multiple\nversions of the same semantic content (malware variants),\nwithvaryingdetectionratesbyVirustotalAV(rangingfrom\n4% to 55%).\nNetwork-Level Attacks.LLMs can also be employed for\ninitiatingnetworkattacks.Aprevalentexampleofanetwork-\nlevel attack utilizing LLM is phishing attacks [18, 43].\nFredriketal. [102]comparedAI-generatedphishingemails\nYifan Yao et al.:Preprint submitted to Elsevier Page 8 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nusing GPT-4 with manually designed phishing emails cre-\nated using the V-Triad, alongside a control group exposed\nto generic phishing emails. The results showed that per-\nsonalized phishing emails, whether generated by AI or de-\nsignedmanually,hadhigherclick-throughratescomparedto\ngenericones.Tysonetal.[151]investigatedhowmodifying\nChatGPT’s input can affect the content of the generated\nemails, making them more convincing. Julian Hazell [97]\ndemonstratedthescalabilityofspearphishingcampaignsby\ngeneratingrealisticandcost-effectivephishingmessagesfor\nover 600 British Members of Parliament using ChatGPT. In\nanother study, Wang et al. [295] discuss how the traditional\ndefensesmayfailintheeraofLLMs.CAPTCHAchallenges,\ninvolvingdistortedlettersanddigits,struggletodetectchat-\nbots relying on text and voice. However, LLMs may break\nthechallenges,astheycanproducehigh-qualityhuman-like\ntext and mimic human behavior effectively. There is one\nstudy that utilizes LLM for deploying fingerprint attacks.\nArmin et al. [236] employed density-based clustering to\ncluster HTTP banners and create text-based fingerprints\nfor annotating scanning data. When these fingerprints are\ncompared to an existing database, it becomes possible to\nidentify new IoT devices and server products.\nUser-Level Attacks.Recent discussions have primarily fo-\ncused on user-level attacks, as LLM demonstrates its ca-\npability to create remarkably convincing but ultimately de-\nceptive content, as well as establish connections between\nseemingly unrelated pieces of information. This presents\nopportunities for malicious actors to engage in a range of\nnefarious activities. Here are a few examples:\n• Misinformation. Overreliance on content generated\nbyLLMswithoutoversightisraisingseriousconcerns\nregarding the safety of online content [206]. Numer-\nous studies have focused on detecting misinformation\nproduced by LLMs. Several study [35, 308, 324]\nreveal content generated by LLMs are harder to de-\ntect and may use more deceptive styles, potentially\ncausing greater harm. Canyu Chen et al. [35] pro-\npose a taxonomy for LLM-generated misinformation\nandvalidatemethods.Countermeasuresanddetection\nmethods [308, 280, 40, 267, 36, 341, 19, 155, 263]\nhave also been developed to address these emerging\nissues.\n• SocialEngineering. LLMsnotonlyhavethepotential\nto generate content from training data, but they also\noffer attackers a new perspective for social engineer-\ning.Work fromStabbetal. [261]highlightsthecapa-\nbilityofwell-trainedLLMstoinferpersonalattributes\nfrom text, such as location, income, and gender. They\nalso reveals how these models can extract personal\ninformation from seemingly benign queries. Tong et\nal. [275] investigated the content generated by LLMs\nmayincludeuserinformation.Moreover,PolraVictor\nFalade [76] stated the exploitation by LLM-driven\nsocial engineers involves tactics such as psychologi-\ncal manipulation, targeted phishing, and the crisis of\nauthenticity.\n• Scientific Misconduct. Irresponsible use of LLMs\ncan result in issues related to scientific misconduct,\nstemming from their capacity to generate original,\ncoherenttext.Theacademiccommunity[45,265,215,\n46, 179, 72, 200, 223, 87, 139, 226], encompass-\ning diverse disciplines from various countries, has\nraised concerns about the increasing difficulties in\ndetecting scientific misconduct in the era of LLMs.\nConcerns arise from LLMs’ ability to generate co-\nherent and original content, including complete pa-\npers from unreliable sources [283, 287, 232]. Re-\nsearchers are also actively engaged in the effort to\ndetect such misconduct. For example, Kavita Ku-\nmari et al. [146, 147] proposedDEMASQ, a precise\nChatGPT-generated content detector.DEMASQ con-\nsiders biases in text composition and evasion tech-\nniques, achieving high accuracy across diverse do-\nmains in identifying ChatGPT-generated content.\n• Fraud.Cybercriminalshavedevisedanewtoolcalled\nFraudGPT[76,10],whichoperateslikeChatGPTbut\nfacilitates cyberattacks. It lacks the safety controls of\nChatGPT and is sold on the dark web and Telegram\nfor $200 per month or $1,700 annually. FraudGPT\ncan create fraud emails related to banks, suggesting\nmalicious links’ placement in the content. It can\nalso list frequently targeted sites or services, aiding\nhackers in planning future attacks. WormGPT [52],\na cybercrime tool, offers features such as unlimited\ncharacter support and chat memory retention. The\ntool was trained on confidential datasets, with a focus\non malware-related and fraud-related data. It can\nguide cybercriminals in executing Business Email\nCompromise (BEC) attacks.\nFindingIV. AsillustratedinFigure3,whencomparedto\nother attacks, it becomes apparent that user-level attacks\nare the most prevalent, boasting a significant count of 33\npapers. This dominance can be attributed to the fact that\nLLMs have increasingly human-like reasoning abilities,\nenabling them to generate human-like conversations and\ncontent (e.g., scientific misconduct, social engineering).\nPresently,LLMsdonotpossessthesamelevelofaccessto\nOS-level or hardware-level functionalities. This observa-\ntion remains consistent with the attack observed in other\nlevels as well. For instance, at the network level, LLMs\ncan be abused to create phishing websites and bypass\nCAPTCHA mechanisms.\n6. Vulnerabilities and Defenses in LLMs\nIn the following section, we embark on an in-depth ex-\nploration of the prevalent threats and vulnerabilities asso-\nciated with LLMs (§6.1). We will examine the specific\nYifan Yao et al.:Preprint submitted to Elsevier Page 9 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nrisks and challenges that arise in the context of LLMs. In\naddition to discussing these challenges, we will also delve\ninto thecountermeasures and strategiesthat researchersand\npractitioners have developed to mitigate these risks (§6.2).\nFigure 4 illustrates the relationship between the attacks and\ndefenses.\n6.1. Vulnerabilities and Threats in LLMs\nIn this section, we aim to delve into the potential vulner-\nabilities and attacks that may be directed towards LLMs.\nOur examination seeks to categorize these threats into two\ndistinctgroups:AIModelInherentVulnerabilitiesandNon-\nAI Model Inherent Vulnerabilities.\n6.1.1. AI Inherent Vulnerabilities and Threats\nThesearevulnerabilitiesandthreatsthatstemfromthevery\nnatureandarchitectureofLLMs,consideringthatLLMsare\nfundamentally AI models themselves. For example, attack-\ners may manipulate the input data to generate incorrect or\nundesirable outputs from the LLM.\n(A1) Adversarial Attacks.Adversarial attacks in machine\nlearningrefertoasetoftechniquesandstrategiesusedtoin-\ntentionally manipulate or deceive machine learning models.\nThese attacks are typically carried out with malicious intent\nand aim to exploit vulnerabilities in the model’s behavior.\nWe only focus on the most extensively discussed attacks,\nnamely, data poisoning and backdoor attacks.\n• Data Poisoning.Data poisoning stands for attackers\ninfluencing the training process by injecting mali-\nciousdataintothetrainingdataset.Thiscanintroduce\nvulnerabilities or biases, compromising the security,\neffectiveness,orethicalbehavioroftheresultingmod-\nels [206]. Various study [148, 290, 289, 2, 291, 239]\nhave demonstrated that pre-trained models are vul-\nnerable to compromise via methods such as using\nuntrusted weights or content, including the insertion\nof poisoned examples into their datasets. By their\ninherent nature as pre-trained models, LLMs are sus-\nceptibletodatapoisoningattacks[227,251,245].For\nexample,Alexanderetal.[290]showedthatevenwith\njust 100 poison examples, LLMs can produce consis-\ntentlynegativeresultsorflawedoutputsacrossvarious\ntasks.Largerlanguagemodelsaremoresusceptibleto\npoisoning, and existing defenses like data filtering or\nmodel capacity reduction offer only moderate protec-\ntion while hurting test accuracy.\n• BackdoorAttacks. Backdoorattacksinvolvethema-\nlicious manipulation of training data and model pro-\ncessing, creating a vulnerability where attackers can\nembed a hidden backdoor into the model [322]. Both\nbackdoor attacks and data poisoning attacks involve\nmanipulating machine learning models, which can\nincludemanipulationofinputs.However,thekeydis-\ntinction is that backdoor attacks specifically focus on\nCyber \nAttacks\nHardware \nAttacks\nOS\nAttacks\nNetwork Attacks\n User Attacks\nHardware\nSide-Channel \nAttacks\nMalicious \nFirmware\nEvil Maid \nAttacks\nFault \nInjection\nPrivilege \nEscalation\nRootkit\nOS RCE\nMemory \nAttacks\nOS DoS\nSoftware\nExploitation\nFingerprinting\nAttacks\nOS\nSide-Channel \nAttacks\nSoftware\nDoS\nSoftware\nSide-Channel \nAttacks\nRace \nConditions\nBOF Attacks\nSoftware\nAttacks\nPhishing \nAttacks\nMalware \nAttacks\nKeylogger\nWorm\nRansomware\nBrute-force\nFileless\n CAPTCHA \nBreaking\nSQL Injection\nXSS Attacks\nCSRF Attacks\nWeb DoS\nWeb Side-\nChannel\nAttacks\nCookie Theft\nDirectory \nTraversal\nSocial \nEngineering\nFraud\nMisinformation \nSpear Phishing\nCredential \nStuffing\nTailgating\nScientific \nmisconduct\nImpersonation\nAI-Inherent Vulnerabilities and Threats \nExtraction Attacks\nBias and Unfairness \nExploitation\nAdversarial Attacks\nData Poisoning\nBackdoor Attacks\nInference Attacks\nAttribute Inference\nMembership Inferences\nInstruction Tuning Attacks\nJailbreaking\nPrompt Injection\nRemote Code Execution\nSide Channel\nSupply Chain \nVulnerabilities\nNon-AI-Inherent Vulnerabilities and Threats \nDefense Strategies in LLM Training\nModel Architecture\nLLM capacity\nLLM Sparsity\nCognitive Architectures\nKnowledge Graph\nCorpora Cleaning\nLanguage Identification\nDetoxicification\nDebiasing\nDe-identification\nDeduplication\nOptimization Methods\nAdversarial Training\nAdversarial Fine-tuning\nSafe Instruction-Tuning\nDifferential Privacy\nDefense Strategies in LLM Inference\nInstruction Processing \n(Pre-process)\nInstruction Manipulation\nInstruction Purification\nDefensive Demonstrations\nMalicious Detection \n(In-process)\nConfidence-based Detection\nEntropy-based Detection\nConsistency Check\nGradient-based Detection\nOutlier Detection\nGeneration Processing \n(Post-Processing)\nMajority Vote\n(Self-)Critique\nDefenses for Non-AI-Inherent Threats\n(Out of Scope)\nVulnerabilities and Threats Defenses\nDefense in Architecture\nFigure 4: Taxonomy of Threats and the Defenses. The line\nrepresents a defense technique that can defend against either\na specific attack or a group of attacks.\nintroducing hidden triggers into the model to ma-\nnipulate specific behaviors or responses when the\ntrigger is encountered. LLMs are subject to backdoor\nattacks[161,331,167].Forexample,Yaoetal.[329]a\nbidirectionalbackdoor,whichcombinestriggermech-\nanisms with prompt tuning.\n(A2) Inference Attacks.Inference attacks in the context\nof machine learning refer to a class of attacks where an\nadversary tries to gain sensitive information or insights\nabout a machine learning model or its training data by\nmakingspecificqueriesorobservationstothemodel.These\nattacks often exploit unintended information leakage from\nthe responses.\n• AttributeInferenceAttacks. AttributeinferenceAt-\ntack [208, 181, 133, 258, 183, 160] is a type of threat\nYifan Yao et al.:Preprint submitted to Elsevier Page 10 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nwhere an attacker attempts to deduce sensitive or\npersonalinformationofindividualsorentitiesbyana-\nlyzingthebehaviororresponsesofamachinelearning\nmodels. It works against the LLMs as well. Robin\net al. [261] presented the first comprehensive exam-\nination of pretrained LLMs’ ability to infer personal\ninformation from text. Using a dataset of real Reddit\nprofiles, the study demonstrated that current LLMs\ncan accurately infer a variety of personal information\n(e.g., location, income, sex) with high accuracy.\n• Membership Inferences.Membership inference At-\ntack is a specific type of inference attack in the field\nofdatasecurityandprivacythatdeterminingwhether\na data record was part of a model’s training dataset,\ngiven white-/black-box access to the model and the\nspecificdatarecord[250,68,143,85,84,191,112].A\nnumber of research studies have explored the concept\nof membership inference, each adopting a unique\nperspective and methodology. These studies have\nexploredvariousmembershipinferenceattacksbyan-\nalyzingthelabel[42],determiningthethreshold[120,\n28, 96], developing a generalized formulation [278],\namong other methods. Mireshghallah et al. [192]\nfound that fine-tuning the head of the model exhibits\ngreater susceptibility to attacks when compared to\nfine-tuning smaller adapters.\n(A3) Extraction Attacks.Extraction attacks typically refer\nto attempts by adversaries to extract sensitive information\nor insights from machine learning models or their asso-\nciated data. Extraction attacks and inference attacks share\nsimilarities but differ in their specific focus and objectives.\nExtraction attacks aim to acquire specific resources (e.g.,\nmodel gradient, training data) or confidential information\ndirectly.Inferenceattacksseektogainknowledgeorinsights\naboutthemodelordata’scharacteristics,oftenbyobserving\nthe model’s responses or behavior. Various types of data\nextraction attacks exist, including model theft attacks [130,\n137], gradient leakage [158], and training data extraction\nattacks [29]. As of the current writing, it has been observed\nthat training data extraction attacks may be effective against\nLLMs. Training data extraction [29] refers to a method\nwhereanattackerattemptstoretrievespecificindividualex-\namples from a model’s training data by strategically query-\ning the machine learning models. Numerous research [344,\n210, 326] studies have shown that it is possible to extract\ntraining data from LLMs, which may include personal and\nprivateinformation[113,339].Notably,theworkbyTruong\net al. [279] stands out for its ability to replicate the model\nwithout accessing the original model data.\n(A4) Bias and Unfairness Exploitation.Bias and unfair-\nness in LLMs pertain to the phenomenon where these mod-\nels demonstrate prejudiced outcomes or discriminatory be-\nhaviors. While bias and fairness issues are not unique to\nLLMs, they have received more attention due to the ethical\nand societal concerns. That is, the societal impact of LLMs\nhas prompted discussions about the ethical responsibilities\nof organizations and researchers developing and deploying\nthesemodels.Thishasledtoincreasedscrutinyandresearch\nonbiasandfairness.Concernsofbiaswereraisedfromvar-\nious fields, encompassing gender and minority groups [65,\n144, 81, 244], the identification of misinformation, political\naspects. Multiple studies [269, 281] revealed biases in the\nlanguage used while querying LLMs. Moreover, Urman et\nal.[282]discoveredthatbiasesmayarisefromadherenceto\ngovernmentcensorshipguidelines.Biasinprofessionalwrit-\ning [292, 263, 79] involving LLMs is also a concern within\nthe community, as it can significantly damage credibility.\nThe biases of LLMs may also lead to negative side effects\ninareasbeyondtext-basedapplications.Daietal.[47]noted\nthat content generated by LLMs might introduce biases in\nneural retrieval systems, and Huang et al. [111] discovered\nthat biases could also be present in LLM generated code.\n(A5) Instruction Tuning Attacks.Instruction tuning, also\nknownasinstruction-basedfine-tuning,isamachine-learning\ntechnique used to train and adapt language models for\nspecific tasks by providing explicit instructions or examples\nduring the fine-tuning process. In LLMs, instruction-tuning\nattacks refer to a class of attacks or manipulations that\ntarget instruction-tuned LLMs. These attacks are aimed at\nexploiting vulnerabilities or limitations in LLMs that have\nbeen fine-tuned with specific instructions or examples for\nparticular tasks.\n• Jailbreaking.Jailbreaking in LLMs involves bypass-\ning security features to enable responses to other-\nwise restricted or unsafe questions, unlocking capa-\nbilities usually limited by safety protocols. Numerous\nstudies have demonstrated various methods for suc-\ncessfully jailbreaking LLMs [159, 271, 248]. Wei et\nal. [301] emphasized that the alignment capabilities\nof LLMs can be influenced or manipulated through\nin-context demonstrations. In addition to this, several\nresearches [300, 132] also demonstrated similar ma-\nnipulation using various approaches, highlighting the\nversatility of methods that can jailbreaking LLMs.\nMore recently,MASTERKEY [54] employed a time-\nbased method for dissecting defenses, and demon-\nstratedproof-of-conceptattacks.Itautomaticallygen-\nerates jailbreak prompts with a 21.58Moreover, di-\nverse methods have been employed in jailbreaking\nLLMs, such as conducting fuzzing [328], implement-\ning optimized search strategies [353], and even train-\ning LLMs specifically to jailbreak other LLMs [53,\n353].Meanwhile,Caoetal.[27]developedRA-LLM,\namethodtolowersthesuccessrateofadversarialand\njailbreaking prompts without needing of retraining or\naccess to model parameters.\n• Prompt Injection.Prompt injection attack describes\na method of manipulating the behavior of LLMs to\nelicit unexpected and potentially harmful responses.\nThis technique involves crafting input prompts in a\nYifan Yao et al.:Preprint submitted to Elsevier Page 11 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nway that bypasses the model’s safeguards or trig-\ngers undesirable outputs. A substantial amount of\nresearch[177,332,135,299,173,124]hasalreadyau-\ntomated the process of identifying semantic preserv-\ning payload in prompt injections with various focus.\nFacilitatedbythecapabilityforfine-tuning,backdoors\nmay be introduced through prompt attacks [12, 133,\n346, 243]. Moreover, Greshake et al. [89] expressed\nconcerns about the potential for new vulnerabilities\narisingfromLLMsinvokingexternalresources.Other\nstudies have also demonstrated the ability to take\nadvantage of prompt injection attacks, such as un-\nveiling guide prompts [342], virtualizing prompt in-\njection [320], and integrating applications [178]. He\net al. [100, 101] explored a shift towards leveraging\nLLMs, trained on extensive datasets, for mitigating\nsuch attacks.\n• Denial of Service. A Denial of Service (DoS) at-\ntack is a type of cyber attack that aims to exhaust\ncomputationalresources,causinglatencyorrendering\nresources unavailable. Due to the nature of LLMs\nrequire significant amount of resources, attackers use\ndeliberatelyconstructpromptstoreducetheavailabil-\nity of models [59]. Shumailov et al. [252] proved the\npossibility of conducting sponge attacks in the field\nof LLMs, specifically designed to maximize energy\nconsumption and latency (by a factor of 10 to 200).\nThis strategy aims to draw the community’s attention\nto their potential impact on autonomous vehicles, as\nwellasscenariosrequiringmakingdecisionsintimely\nmanner.\nFinding V.Currently, there is limited research on model\nextraction attacks [68], parameter extraction attacks, or\nthe extraction of other intermediate esults [279]. While\nthere are a few mentions of these topics, they tend to re-\nmainprimarilytheoretical(e.g.,[172]),withlimitedprac-\nticalimplementationorempiricalexploration.Webelieve\nthat the sheer scale of parameters in LLMs complicates\nthesetraditionalapproaches,renderingthemlesseffective\noreveninfeasible.Additionally,themostpowerfulLLMs\nare privately owned, with their weights, parameters, and\nother details kept confidential, further shielding them\nfrom conventional attack strategies. Strict censorship of\noutputs generated by these LLMs challenges even black-\nbox traditional ML attacks, as it limits the attackers’\nability to exploit or analyze the model’s responses.\n6.1.2. Non-AI Inherent Vulnerabilities and Threats\nWe also need to consider non-AI Inherent Attacks, which\nencompass external threats and new vulnerabilities (which\nhave not been observed or investigated in traditional AI\nmodels) that LLMs might encounter. These attacks may\nnot be intricately linked to the internal mechanisms of the\nAI model, yet they can present significant risks. Illustrative\ninstances of non-AI Inherent Attacks involve system-level\nvulnerabilities (e.g., remote code execution).\n(A6) Remote Code Execution (RCE).RCE attacks typ-\nically target vulnerabilities in software applications, web\nservices,orserverstoexecutearbitrarycoderemotely.While\nRCE attacks are not typically applicable directly to LLMs,\nif an LLM is integrated into a web service (e.g.,https:\n//chat.openai.com/) and if there are RCE vulnerabilities\nin the underlying infrastructure or code of that service, it\ncould potentially lead to the compromise of the LLM’s\nenvironment. Tong et al. [175] identified 13 vulnerabilities\nin six frameworks, including 12 RCE vulnerabilities and 1\narbitrary file read/write vulnerability. Additionally, 17 out\nof 51 tested apps were found to have vulnerabilities, with\n16 being vulnerable to RCE and 1 to SQL injection. These\nvulnerabilities allow attackers to execute arbitrary code on\napp servers through prompt injections.\n(A7) Side Channel.While LLMs themselves do not typi-\ncallyleakinformationthroughtraditionalsidechannelssuch\naspowerconsumptionorelectromagneticradiation,theycan\nbe vulnerable to certain side-channel attacks in practical\ndeployment scenarios. For example, Edoardo et al. [51]\nintroduce privacy side channel attacks, which are attacks\nthat exploit system-level components (e.g., data filtering,\noutput monitoring) to extract private information at a much\nhigher rate than what standalone models can achieve. Four\ncategories of side channels covering the entire ML lifecycle\nare proposed, enabling enhanced membership inference at-\ntacks and novel threats (e.g., extracting users’ test queries).\nFor instance, the research demonstrates how deduplicating\ntraining data before applying differentially-private training\ncreatesasidechannelthatcompromisesprivacyguarantees.\n(A8) Supply Chain Vulnerabilities.Supply Chain Vulner-\nabilities refer to the risks in the lifecycle of LLM appli-\ncations that may arise from using vulnerable components\nor services. These include third-party datasets, pre-trained\nmodels, and plugins, any of which can compromise the\napplication’s integrity [206]. Most research in this field is\nfocused on the security of plugins. An LLM plugin is an\nextension or add-on module that enhances the capabilities\nof an LLM. Third-party plug-ins have been developed to\nexpand its functionality, enabling users to perform various\ntasks, including web searches, text analysis, and code exe-\ncution. However, some of the concerns raised by security\nexperts [206, 25] include the possibility of plug-ins being\nused to steal chat histories, access personal information, or\nexecute code on users’ machines. These vulnerabilities are\nassociatedwiththeuseofOAuthinplug-ins,awebstandard\nfor data sharing across online accounts. Umar et al. [115]\nattempted to address this problem by designing a frame-\nwork. The framework formulates an extensive taxonomy of\nattacks specific to LLM platforms, taking into account the\ncapabilities of plugins, users, and the LLM platform itself.\nByconsideringtherelationshipsbetweenthesestakeholders,\ntheframeworkhelpsidentifypotentialsecurity,privacy,and\nsafety risks.\nYifan Yao et al.:Preprint submitted to Elsevier Page 12 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\n6.2. Defenses for LLMs\nIn this section, we examine the range of existing defense\nmethods against various attacks and vulnerabilities associ-\nated with LLMs1.\n6.2.1. Defense in Model Architecture\nModel architectures determine how knowledge and con-\nceptsarestored,organized,andcontextuallyinteractedwith,\nwhich is crucial in the safety of Large Language Models.\nThere have been a lot of works [165, 351, 168, 333] delved\ninto how model capacities affect the privacy preservation\nand robustness of LLMs. Li et al. [165] revealed that lan-\nguage models with larger parameter sizes can be trained\nmore effectively in the differential privacy manner using\nappropriate non-standard hyper-parameters, in comparison\nto smaller models. Zhu et al. [351] and Li et al. [168]\nfound that LLMs with larger capacities, such as those with\nmore extensive parameter sizes, generally show increased\nrobustnessagainstadversarialattacks.Thiswasalsoverified\nin the Out-of-distribution (OOD) robustness scenarios by\nYuan et al. [333]. Beyond the architecture of LLMs them-\nselves, studies have focused on improving LLM safety by\ncombiningthemwithexternalmodulesincludingknowledge\ngraphs [39] and cognitive architectures (CAs) [150, 11].\nRomero et al. [231] proposed improving AI robustness by\nincorporating various cognitive architectures into LLMs.\nZafar et al. [336] aimed to build trust in AI by enhancing\nthe reasoning abilities of LLMs through knowledge graphs.\n6.2.2. Defenses in LLM Training and Inference\nDefense Strategies in LLM Training.The core compo-\nnents of LLM training include model architectures, training\ndata, and optimization methods. Regarding model architec-\ntures,weexaminetrustworthydesignsthatexhibitincreased\nrobustness against malicious use. For training corpora, our\ninvestigation focuses on methods aimed at mitigating un-\ndesired properties during the generation, collection, and\ncleaning of training data. In the context of optimization\nmethods, we review existing works that developed safe and\nsecure optimization frameworks.\n• Corpora Cleaning.LLMs are shaped by their train-\ningcorpora,fromwhichtheylearnbehavior,concepts,\nand data distributions [302]. Therefore, the safety\nof LLMs is crucially influenced by the quality of\nthe training corpora [86, 204]. However, it has been\nwidelyacknowledgedthatrawcorporacollectedfrom\nthewebarefullofissuesoffairness[14],toxicity[88],\nprivacy [208], truthfulness [171], etc. A lot of efforts\nhavebeenmadetocleanrawcorporaandcreatehigh-\nqualitytrainingcorporaforLLMs[129,306,152,307,\n213,277].Ingeneral,thesepipelinesconsistofthefol-\nlowing steps: language identification [129, 9], detox-\nification [88, 48, 180, 195], debiasing [188, 21, 16],\n1Please be aware that we will not delve into solutions for non-AI\ninherentvulnerabilitiesastheytendtobehighlyspecifictoindividualcases.\nde-identification (personally identifiable information\n(PII)) [264, 284], and deduplication [153, 134, 106,\n157]. Debiasing and detoxification aimed to remove\nundesirable content from training corpora.\n• Optimization Methods.Optimization objectives are\ncrucial in directing how LLMs learn from training\ndata, influencing which behaviors are encouraged or\npenalized. These objectives affect the prioritization\nofknowledgeandconceptswithincorpora,ultimately\nimpacting the overall safety and ethical alignment of\nLLMs. In this context, robust training methods like\nadversarial training [176, 293, 350, 330, 163] and\nrobust fine-tuning [66, 121] have shown resilience\nagainst perturbation-based text attacks. Drawing in-\nspiration from traditional adversarial training in the\nimagefield[182],Ivgietal.[116]andYooetal.[330]\napplied adversarial training to LLMs by generating\nperturbations concerning discrete tokens. Wang et\nal. [293] extended this approach to the continuous\nembedding space, facilitating more practical conver-\ngence, as followed by subsequent research [176, 350,\n163]. Safety alignments [205], an emerging learning\nparadigm, guide LLM behavior using well-aligned\nadditional models or human annotations, proving ef-\nfective for ethical alignment. Efforts to align LLMs\nwith other LLMs [334] and LLMs themselves [268].\nIn terms of human annotations, Zhou et al. [349] and\nShi et al. [249] emphasized the importance of high-\nqualitytrainingcorporawithcarefullycuratedinstruc-\ntions and outputs for enhancing instruction-following\ncapabilities in LLMs. Bianchi et al. [20] highlighted\nthatthesafetyofLLMscanbesubstantiallyimproved\nby incorporating a limited percentage (e.g., 3%) of\nsafe examples during fine-tuning.\nDefenseStrategiesinLLMInference. WhenLLMsarede-\nployed as cloud services, they operate by receiving prompts\nor instructions from users and generating completed sen-\ntences in response. Given this interaction model, the im-\nplementation of test-time LLM defense becomes a neces-\nsary and critical aspect of ensuring safe and appropriate\noutputs. Generally, test-time defense encompasses a range\nof strategies, including the pre-processing of prompts and\ninstructionstofilterormodifyinputs,thedetectionofabnor-\nmal events that might signal misuse or problematic queries,\nand the post-processing of generated responses to ensure\nthey adhere to safety and ethical guidelines. Test-time LLM\ndefensesareessentialtomaintaintheintegrityandtrustwor-\nthiness of LLMs in real-time applications.\n• Instruction Processing (Pre-Processing). Instruc-\ntion pre-processing applies transformations over in-\nstructions sent by users, in order to destroy potential\nadversarial contexts or malicious intents. It plays a\nvital role as it blocks out most malicious usage and\nYifan Yao et al.:Preprint submitted to Elsevier Page 13 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nprevents LLMs from receiving suspicious instruc-\ntions. In general, instruction pre-processing methods\ncan be categorized as instruction manipulation [246,\n230, 140, 117, 318], purification [164], and defensive\ndemonstrations [172, 193, 301]. Jain et al. [117]\nand Kirchenbauer et al. [140] evaluated multiple\nbaseline preprocessing methods against jailbreaking\nattacks, including retokenization and paraphrase. Li\net al. [164] proposed to purify instructions by first\nmasking the input tokens and then predicting the\nmaskedtokenswithotherLLMs.Thepredictedtokens\nwillserveasthepurifiedinstructions.Weietal.[301]\nand Mo et al. [193] demonstrated that inserting pre-\ndefined defensive demonstrations into instructions\neffectively defends jailbreaking attacks of LLMs.\n• Malicious Detection (In-Processing).Malicious de-\ntection provides in-depth examinations of LLM inter-\nmediate results, such as neuron activation, regarding\nthegiveninstructions,whicharemoresensitive,accu-\nrate,andspecifiedformalicioususage.Sunetal.[266]\nproposedtodetectbackdooredinstructionswithback-\nward probabilities of generations. Xi et al. [312] dif-\nferentiatednormalandpoisonedinstructionsfromthe\nperspective of mask sensitivities. Shao et al. [246]\nidentified suspicious words according to their tex-\ntual relevance. Wang et al. [298] detected adversar-\nial examples according to the semantic consistency\namongmultiplegenerations,whichhasbeenexplored\nin the uncertainty quantification of LLMs by Duan et\nal. [67]. Apart from the intrinsic properties of LLMs,\nthere have been works leveraging the linguistic statis-\ntic properties, such as detecting outlier words [220],\n• Generation Processing (Post-Processing).Genera-\ntion post processing refers to examining the proper-\nties (e.g., harmfulness) of the generated answers and\napplyingmodificationsifnecessary,whichisthefinal\nstep before delivering responses to users. Chen et\nal. [34] proposed to mitigate the toxicity of genera-\ntions by comparing with multiple model candidates.\nHelbling et al. [103] incorporated individual LLMs\nto identify the harmfulness of the generated answers,\nwhich shared similar ideas as Xiong et al. [317] and\nKadavath et al. [131] where they revealed that LLMs\ncan be prompted to answer the confidences regarding\nthe generated responses.\nFinding VI.For defense in LLM training, there’s a no-\ntable scarcity of research examining the impact of model\narchitecture on LLM safety, which is likely due to the\nhighcomputationalcostsassociatedwithtrainingorfine-\ntuning large language models. We observed that safe\ninstruction tuning is a relatively new development that\nwarrants further investigation and attention.\n7. Discussion\n7.1. LLM in Other Security Related Topics\nLLMs in Cybersecurity Education.LLMs can be used in\nsecurity practices and education [80, 162, 270]. For exam-\nple, in a software security course, students are tasked with\nidentifyingandresolvingvulnerabilitiesinawebapplication\nusingLLMs.Jingyueetal.[162]investigatedhowChatGPT\ncan be used by students for these exercises. Wesley Tann et\nal. [270] focused on the evaluation of LLMs in the context\nofcybersecurityCapture-The-Flag(CTF)exercises(partici-\npants find “flags” by exploiting system vulnerabilities). The\nstudy first assessed the question-answering performance of\nthese LLMs on Cisco certifications with varying difficulty\nlevels, then examined their abilities in solving CTF chal-\nlenges. Jin et al. [126] conducted a comprehensive study on\nLLMs’understandingofbinarycodesemantics[127]across\ndifferentarchitecturesandoptimizationlevels,providingkey\ninsights for future research in this area.\nLLMs in Cybersecurity Laws, Policies and Compliance.\nLLMs can assist in drafting security policies, guidelines,\nand compliance documentation, ensuring that organizations\nmeet regulatory requirements and industry standards. How-\never,it’simportanttorecognizethattheutilizationofLLMs\ncanpotentiallynecessitatechangestocurrentcybersecurity-\nrelated laws and policies. The introduction of LLMs may\nraisenewlegalandregulatoryconsiderations,asthesemod-\nelscanimpactvariousaspectsofcybersecurity,dataprotec-\ntion, and privacy. Ekenobi et al. [273] examined the legal\nimplications arising from the introduction of LLMs, with\na particular focus on data protection and privacy concerns.\nIt acknowledges that ChatGPT’s privacy policy contains\ncommendable provisions for safeguarding user data against\npotential threats. The paper also advocated for emphasizing\nthe relevance of the new law.\n7.2. Future Directions\nWehavegleaned valuablelessonsthatwebelievecan shape\nfuture directions.\n• Using LLMs for ML-Specific Tasks. We noticed\nthatLLMscaneffectivelyreplacetraditionalmachine\nlearningmethodsandinthiscontext,iftraditionalma-\nchine learning methods can be employed in a specific\nsecurityapplication(whetheroffensiveordefensivein\nnature),itishighlyprobablethatLLMscanalsobeap-\npliedtoaddressthatparticularchallenge.Forinstance,\ntraditionalmachinelearningmethodshavefoundutil-\nity in malware detection, and LLMs can similarly be\nharnessed for this purpose. Therefore, one promising\navenue is to harness the potential of LLMs in secu-\nrity applications where machine learning serves as a\nfoundationalorwidelyadoptedtechnique.Assecurity\nresearchers, we are capable of designing LLM-based\napproachestotacklesecurityissues.Subsequently,we\ncan compare these approaches with state-of-the-art\nmethods to push the boundaries.\nYifan Yao et al.:Preprint submitted to Elsevier Page 14 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\n• Replacing Human Efforts.It is evident that LLMs\nhave the potential to replace human efforts in both\noffensive and defensive security applications. For in-\nstance, tasks involving social engineering, tradition-\nally reliant on human intervention, can now be effec-\ntivelyexecutedusingLLMtechniques.Therefore,one\npromising avenue for security researchers is to iden-\ntify areas within traditional security tasks where hu-\nman involvement has been pivotal and explore oppor-\ntunities to substitute these human efforts with LLM\ncapabilities.\n• Modifying Traditional ML Attacks for LLMs.we\nhave observed that many security vulnerabilities in\nLLMs are extensions of vulnerabilities found in tra-\nditional machine-learning scenarios. That is, LLMs\nremainaspecializedinstanceofdeepneuralnetworks,\ninheritingcommonvulnerabilitiessuchasadversarial\nattacks and instruction tuning attacks. With the right\nadjustments(e.g.,thethreatmodel),traditionalMLat-\ntackscanstillbeeffectiveagainstLLMs.Forinstance,\nthejailbreakingattackisaspecificformofinstruction\ntuning attack aimed at producing restricted texts.\n• Adapting Traditional ML Defenses for LLMs.The\ncountermeasures traditionally employed for vulnera-\nbilitymitigationcanalsobeleveragedtoaddressthese\nsecurity issues. For example, there are existing ef-\nforts that utilize traditional Privacy-Enhancing Tech-\nnologies(e.g.,zero-knowledgeproofs,differentialpri-\nvacy, and federated learning [304, 305] ) to tackle\nprivacy challenges posed by LLMs. Exploring addi-\ntional PETs techniques, whether they are established\nmethods or innovative approaches, to address these\nchallenges represents another promising research di-\nrection.\n• Solving Challenges in LLM-Specific Attacks.As\npreviously discussed, there are several challenges as-\nsociated with implementing model extraction or pa-\nrameter extraction attacks (e.g., vast scale of LLM\nparameters, private ownership and confidentiality of\npowerful LLMs). These novel characteristics intro-\nduced by LLMs represent a significant shift in the\nlandscape, potentially leading to new challenges and\nnecessitating the evolution of traditional ML attack\nmethodologies.\n8. Related Work\nTherehavealreadybeenanumberofLLMsurveysreleased\nwith a variety of focuses (e.g., LLM evolution and taxon-\nomy [31, 347, 309, 93, 311, 23, 348], software engineer-\ning [77, 108], and medicine [274, 44]). In this paper, our\nprimary emphasis is on the security and privacy aspects\nof LLMs. We now delve into an examination of the ex-\nisting literature pertaining to this particular topic. Peter J.\nCaven [30] specifically explores how LLMs (particularly,\nChatGPT) could potentially alter the current cybersecurity\nlandscape by blending technical and social aspects. Their\nemphasis leans more towards the social aspects. Muna et\nal. [5] and Marshall et al. [185] discussed the impact of\nChatGPT in cybersecurity, highlighting its practical appli-\ncations (e.g., code security, malware detection). Dhoni et\nal.[62]demonstratedhowLLMscanassistsecurityanalysts\nindeveloping securitysolutionsagainst cyberthreats. How-\never, their work does not extensively address the potential\ncybersecurity threats that LLM may introduce. A number\nof surveys (e.g., [92, 59, 247, 49, 60, 228, 240, 241, 7])\nhighlight the threats and attacks against LLMs. In com-\nparison to our work, they do not dedicate as much text to\nthe vulnerabilities that the LLM may possess. Instead, their\nprimary focus lies in the realm of security applications, as\nthey delve into utilizing LLMs for launching cyberattacks.\nAttia Qammar et al. [219] and Maximilian et al. [196]\ndiscussed vulnerabilities exploited by cybercriminals, with\na specific focus on the risks associated with LLMs. Their\nworks emphasized the need for strategies and measures\nto mitigate these threats and vulnerabilities. Haoran Li et\nal. [166] analyzed current privacy concerns on LLMs, cate-\ngorizingthembasedonadversarycapabilities,andexplored\nexisting defense strategies. Glorin Sebastian [242] explored\ntheapplicationofestablishedPrivacy-EnhancingTechnolo-\ngies(e.g.,differentialprivacy[70],federatedlearning[338],\nand data minimization [216]) for safeguarding the privacy\nof LLMs. Smith et al. [256] also discussed the privacy\nrisks of LLMs. Our study comprehensively examined both\nthe security and privacy aspects of LLMs. In summary,\nour research conducted an extensive review of the literature\non LLMs from a three-fold perspective: beneficial security\napplications (e.g., vulnerability detection, secure code gen-\neration), adverse implications (e.g., phishing attacks, social\nengineering), and vulnerabilities (e.g., jailbreaking attacks,\nprompt attacks), along with their corresponding defensive\nmeasures.\n9. Conclusion\nOur work represents a pioneering effort in systematically\nexamining the multifaceted role of LLMs in security and\nprivacy. On the positive side, LLMs have significantly con-\ntributed to enhancing code and data security, while their\nversatile nature also opens the door to malicious applica-\ntions.Wealsodelvedintotheinherentvulnerabilitieswithin\nthese models, and discussed defense mechanisms. We have\nilluminated the path forward for harnessing the positive\naspects of LLMs while mitigating their potential risks. As\nLLMs continue to evolve and find their place in an ever-\nexpanding array of applications, it is imperative that we\nremainvigilantinaddressingsecurityandprivacyconcerns,\nensuringthatthesepowerfulmodelscontributepositivelyto\nthe digital landscape.\nYifan Yao et al.:Preprint submitted to Elsevier Page 15 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nAcknowledgement\nWe thank the anonymous reviewers and Xin Jin from The\nOhio State University for their invaluable feedback. This\nresearch was supported partly by the NSF award FMitF-\n2319242. Any opinions, findings, conclusions, or recom-\nmendations expressed are those of the authors and not nec-\nessarily of the NSF.\nReferences\n[1] M.Abbasian,I.Azimi,A.M.Rahmani,andR.Jain,“Conversational\nhealthagents:Apersonalizedllm-poweredagentframework,”2023.\n[2] H. Aghakhani, W. Dai, A. Manoel, X. Fernandes, A. Kharkar,\nC. Kruegel, G. Vigna, D. Evans, B. Zorn, and R. Sim, “Trojan-\npuzzle:Covertlypoisoningcode-suggestionmodels,” arXivpreprint\narXiv:2301.02344, 2023.\n[3] B. Ahmad, S. Thakur, B. Tan, R. Karri, and H. Pearce,\n“Fixing hardware security bugs with large language models,”\narXiv preprint arXiv:2302.01215 , 2023. [Online]. Available:\nhttps://doi.org/10.48550/arXiv.2302.01215\n[4] M. AI, “Introducing llama: A foundational, 65-billion-\nparameter language model,” https://ai.meta.com/blog/\nlarge-language-model-llama-meta-ai/, feb 2023, accessed:\n2023-11-13.\n[5] M.Al-Hawawreh,A.Aljuhani,andY.Jararweh,“Chatgptforcyber-\nsecurity: practical applications, challenges, and future directions,”\nCluster Computing, vol. 26, no. 6, pp. 3421–3436, 2023.\n[6] S. Alagarsamy, C. Tantithamthavorn, and A. Aleti, “A3test:\nAssertion-augmented automated test case generation,” arXiv\npreprint arXiv:2302.10352, 2023.\n[7] M. Alawida, B. A. Shawar, O. I. Abiodun, A. Mehmood, A. E.\nOmolara et al., “Unveiling the dark side of chatgpt: Exploring\ncyberattacks and enhancing user awareness,” 2023.\n[8] T. Ali and P. Kostakos, “Huntgpt: Integrating machine learning-\nbased anomaly detection and explainable ai with large language\nmodels (llms),”arXiv preprint arXiv:2309.16021, 2023.\n[9] E. Ambikairajah, H. Li, L. Wang, B. Yin, and V. Sethu, “Language\nidentification: A tutorial,”IEEE Circuits and Systems Magazine,\nvol. 11, no. 2, pp. 82–108, 2011.\n[10] Z. Amos, “What is fraudgpt?” https://hackernoon.com/\nwhat-is-fraudgpt, 2023.\n[11] J.R.AndersonandC.J.Lebiere, Theatomiccomponentsofthought .\nPsychology Press, 2014.\n[12] Anonymous, “On the safety of open-sourced large language\nmodels: Does alignment really prevent them from being misused?”\nin Submitted to The Twelfth International Conference on\nLearning Representations, 2023, under review. [Online]. Available:\nhttps://openreview.net/forum?id=E6Ix4ahpzd\n[13] B. B. Arcila, “Is it a platform? is it a search engine? it’s chatgpt! the\neuropeanliabilityregimeforlargelanguagemodels,” J.FreeSpeech\nL., vol. 3, p. 455, 2023.\n[14] A. H. Bailey, A. Williams, and A. Cimpian, “Based on billions of\nwords on the internet, people= men,”Science Advances, vol. 8,\nno. 13, p. eabm2463, 2022.\n[15] A. Bakhshandeh, A. Keramatfar, A. Norouzi, and M. M.\nChekidehkhoun, “Using chatgpt as a static application security\ntesting tool,”arXiv preprint arXiv:2308.14434, 2023.\n[16] S.Barikeri,A.Lauscher,I.Vulić,andG.Glavaš,“Redditbias:Areal-\nworld resource for bias evaluation and debiasing of conversational\nlanguage models,”arXiv preprint arXiv:2106.03521, 2021.\n[17] M. Beckerich, L. Plein, and S. Coronado, “Ratgpt: Turning online\nllms into proxies for malware attacks,” 2023.\n[18] S.Ben-Moshe,G.Gekker,andG.Cohen,“Opwnai:Aithatcansave\nthe day or hack it away. check point research (2022),” 2023.\n[19] A.-R.BhojaniandM.Schwarting,“Truthandregret:Largelanguage\nmodels, the quran, and misinformation,” pp. 1–7, 2023.\n[20] F. Bianchi, M. Suzgun, G. Attanasio, P. Röttger, D. Jurafsky,\nT. Hashimoto, and J. Zou, “Safety-tuned llamas: Lessons from im-\nprovingthesafetyoflargelanguagemodelsthatfollowinstructions,”\narXiv preprint arXiv:2309.07875, 2023.\n[21] S.BordiaandS.R.Bowman,“Identifyingandreducinggenderbias\nin word-level language models,”arXiv preprint arXiv:1904.03035,\n2019.\n[22] M. Botacin, “Gpthreats-3: Is automatic malware generation a\nthreat?” in 2023 IEEE Security and Privacy Workshops (SPW).\nIEEE, 2023, pp. 238–254.\n[23] S.R.Bowman,“Eightthingstoknowaboutlargelanguagemodels,”\narXiv preprint arXiv:2304.00612, 2023.\n[24] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhari-\nwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\nA. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh,\nD. M. Ziegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler,\nM. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish,\nA. Radford, I. Sutskever, and D. Amodei, “Language models are\nfew-shot learners,” 2020.\n[25] M. Burgess, “Chatgpt has a plug-in problem,” https://www.wired.\ncom/story/chatgpt-plugins-security-privacy-risk/, 2023.\n[26] Y. Cai, S. Mao, W. Wu, Z. Wang, Y. Liang, T. Ge, C. Wu, W. You,\nT. Song, Y. Xiaet al., “Low-code llm: Visual programming over\nllms,”arXiv preprint arXiv:2304.08103, 2023.\n[27] B. Cao, Y. Cao, L. Lin, and J. Chen, “Defending against alignment-\nbreaking attacks via robustly aligned llm,” 2023.\n[28] N. Carlini, S. Chien, M. Nasr, S. Song, A. Terzis, and F. Tramer,\n“Membership inference attacks from first principles,” in2022 IEEE\nSymposium on Security and Privacy (SP). IEEE, 2022, pp. 1897–\n1914.\n[29] N. Carlini, F. Tramer, E. Wallace, M. Jagielski, A. Herbert-Voss,\nK. Lee, A. Roberts, T. Brown, D. Song, U. Erlingssonet al., “Ex-\ntractingtrainingdatafromlargelanguagemodels,”in 30thUSENIX\nSecurity Symposium (USENIX Security 21), 2021, pp. 2633–2650.\n[30] P.Caven,“Amoreinsecureecosystem?chatgpt’sinfluenceoncyber-\nsecurity,”ChatGPT’s Influence on Cybersecurity (April 30, 2023),\n2023.\n[31] Y. Chang, X. Wang, J. Wang, Y. Wu, K. Zhu, H. Chen, L. Yang,\nX. Yi, C. Wang, Y. Wanget al., “A survey on evaluation of large\nlanguage models,”arXiv preprint arXiv:2307.03109, 2023.\n[32] P. V. S. Charan, H. Chunduri, P. M. Anand, and S. K. Shukla,\n“Fromtexttomitretechniques:Exploringthemalicioususeoflarge\nlanguage models for generating cyber attack payloads,” 2023.\n[33] B. Chen, F. Zhang, A. Nguyen, D. Zan, Z. Lin, J.-G. Lou, and\nW. Chen, “Codet: Code generation with generated tests,”arXiv\npreprint arXiv:2207.10397, 2022.\n[34] B. Chen, A. Paliwal, and Q. Yan, “Jailbreaker in jail: Mov-\ning target defense for large language models,” arXiv preprint\narXiv:2310.02417, 2023.\n[35] C. Chen and K. Shu, “Can llm-generated misinformation be de-\ntected?” 2023.\n[36] ——, “Combating misinformation in the age of llms: Opportunities\nand challenges,”arXiv preprint arXiv:2311.05656, 2023.\n[37] C. Chen, J. Su, J. Chen, Y. Wang, T. Bi, Y. Wang, X. Lin, T. Chen,\nand Z. Zheng, “When chatgpt meets smart contract vulnerability\ndetection:Howfararewe?” arXivpreprintarXiv:2309.05520 ,2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2309.05520\n[38] T. Chen, L. Li, L. Zhu, Z. Li, G. Liang, D. Li,\nQ. Wang, and T. Xie, “Vullibgen: Identifying vulnerable\nthird-party libraries via generative pre-trained model,” arXiv\npreprint arXiv:2308.04662 , 2023. [Online]. Available: https:\n//doi.org/10.48550/arXiv.2308.04662\n[39] X.Chen,S.Jia,andY.Xiang,“Areview:Knowledgereasoningover\nknowledge graph,”Expert Systems with Applications, vol. 141, p.\n112948, 2020.\n[40] Y. Chen, A. Arunasalam, and Z. B. Celik, “Can large language\nmodels provide security & privacy advice? measuring the ability of\nllms to refute misconceptions,” 2023.\nYifan Yao et al.:Preprint submitted to Elsevier Page 16 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\n[41] A. Cheshkov, P. Zadorozhny, and R. Levichev, “Evaluation\nof chatgpt model for vulnerability detection,” arXiv preprint\narXiv:2304.07232, 2023. [Online]. Available: https://doi.org/10.\n48550/arXiv.2304.07232\n[42] C. A. Choquette-Choo, F. Tramer, N. Carlini, and N. Papernot,\n“Label-onlymembershipinferenceattacks,”in Internationalconfer-\nence on machine learning. PMLR, 2021, pp. 1964–1974.\n[43] M. Chowdhury, N. Rifat, S. Latif, M. Ahsan, M. S. Rahman, and\nR.Gomes,“Chatgpt:Thecuriouscaseofattackvectors’supplychain\nmanagementimprovement,”in 2023IEEEInternationalConference\non Electro Information Technology (eIT), 2023, pp. 499–504.\n[44] J.Clusmann,F.R.Kolbinger,H.S.Muti,Z.I.Carrero,J.-N.Eckardt,\nN. G. Laleh, C. M. L. Löffler, S.-C. Schwarzkopf, M. Unger, G. P.\nVeldhuizenetal.,“Thefuturelandscapeoflargelanguagemodelsin\nmedicine,”Communications Medicine, vol. 3, no. 1, p. 141, 2023.\n[45] D. R. Cotton, P. A. Cotton, and J. R. Shipway, “Chatting and cheat-\ning: Ensuring academic integrity in the era of chatgpt,”Innovations\nin Education and Teaching International, pp. 1–12, 2023.\n[46] G. M. Currie, “Academic integrity and artificial intelligence: is\nchatgpt hype, hero or heresy?” inSeminars in Nuclear Medicine.\nElsevier, 2023.\n[47] S.Dai,Y.Zhou,L.Pang,W.Liu,X.Hu,Y.Liu,X.Zhang,andJ.Xu,\n“Llms may dominate information access: Neural retrievers are bi-\nasedtowardsllm-generatedtexts,” arXivpreprintarXiv:2310.20501 ,\n2023.\n[48] D. Dale, A. Voronov, D. Dementieva, V. Logacheva, O. Kozlova,\nN.Semenov,andA.Panchenko,“Textdetoxificationusinglargepre-\ntrained neural models,”arXiv preprint arXiv:2109.08914, 2021.\n[49] B. Dash and P. Sharma, “Are chatgpt and deepfake algorithms\nendangering the cybersecurity industry? a review,”International\nJournal of Engineering and Applied Sciences, vol. 10, no. 1, 2023.\n[50] Databricks, “Free dolly: Introducing the world’s\nfirst open and commercially viable instruction-tuned\nllm,” https://www.databricks.com/blog/2023/04/12/\ndolly-first-open-commercially-viable-instruction-tuned-llm,\n2023, accessed: 2023-11-13.\n[51] E. Debenedetti, G. Severi, N. Carlini, C. A. Choquette-Choo,\nM. Jagielski, M. Nasr, E. Wallace, and F. Tramèr, “Privacy\nside channels in machine learning systems,” arXiv preprint\narXiv:2309.05610, 2023.\n[52] D. Delley, “Wormgpt – the generative ai tool cybercriminals are\nusingtolaunchbusinessemailcompromiseattacks,”https://shorturl.\nat/iwFL7, 2023.\n[53] G.Deng,Y.Liu,Y.Li,K.Wang,Y.Zhang,Z.Li,H.Wang,T.Zhang,\nand Y. Liu, “Jailbreaker: Automated jailbreak across multiple large\nlanguage model chatbots,”arXiv preprint arXiv:2307.08715, 2023.\n[54] ——, “Masterkey: Automated jailbreaking of large language model\nchatbots,” in Proceedings of the 31th Annual Network and Dis-\ntributed System Security Symposium (NDSS’24), 2024.\n[55] G.Deng,Y.Liu,V.Mayoral-Vilches,P.Liu,Y.Li,Y.Xu,T.Zhang,\nY.Liu,M.Pinzger,andS.Rass,“Pentestgpt:Anllm-empoweredau-\ntomatic penetration testing tool,”arXiv preprint arXiv:2308.06782,\n2023.\n[56] Y. Deng, C. S. Xia, H. Peng, C. Yang, and L. Zhang, “Fuzzing\ndeep-learning libraries via large language models,”arXiv preprint\narXiv:2212.14834, 2022.\n[57] Y. Deng, C. S. Xia, C. Yang, S. D. Zhang, S. Yang, and L. Zhang,\n“Large language models are edge-case fuzzers: Testing deep learn-\ning libraries via fuzzgpt,”arXiv preprint arXiv:2304.02014, 2023.\n[58] ——, “Large language models are edge-case generators: Crafting\nunusual programs for fuzzing deep learning libraries,” in 2024\nIEEE/ACM46thInternationalConferenceonSoftwareEngineering\n(ICSE), 2024, pp. 830–842.\n[59] E. Derner, K. Batistič, J. Zahálka, and R. Babuška, “A secu-\nrity risk taxonomy for large language models,” arXiv preprint\narXiv:2311.11415, 2023.\n[60] E. Derner and K. Batistič, “Beyond the safeguards: Exploring the\nsecurity risks of chatgpt,” 2023.\n[61] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntrainingofdeepbidirectionaltransformersforlanguageunderstand-\ning,” 2019.\n[62] P. Dhoni and R. Kumar, “Synergizing generative ai and cyberse-\ncurity: Roles of generative ai entities, companies, agencies, and\ngovernment in enhancing cybersecurity,” 2023.\n[63] H. Ding, V. Kumar, Y. Tian, Z. Wang, R. Kwiatkowski, X. Li,\nM. K. Ramanathan, B. Ray, P. Bhatia, S. Senguptaet al., “A static\nevaluation of code completion by large language models,”arXiv\npreprint arXiv:2306.03203, 2023.\n[64] X. Ding, L. Chen, M. Emani, C. Liao, P.-H. Lin, T. Vanderbruggen,\nZ. Xie, A. Cerpa, and W. Du, “Hpc-gpt: Integrating large\nlanguage model for high-performance computing,” inProceedings\nof the SC ’23 Workshops of The International Conference on\nHigh Performance Computing, Network, Storage, and Analysis,\nser. SC-W 2023. ACM, Nov. 2023. [Online]. Available:\nhttp://dx.doi.org/10.1145/3624062.3624172\n[65] X. Dong, Y. Wang, P. S. Yu, and J. Caverlee, “Probing explicit and\nimplicit gender bias through llm conditional text generation,”arXiv\npreprint arXiv:2311.00306, 2023.\n[66] X.Dong,A.T.Luu,M.Lin,S.Yan,andH.Zhang,“Howshouldpre-\ntrained language models be fine-tuned towards adversarial robust-\nness?”AdvancesinNeuralInformationProcessingSystems ,vol.34,\npp. 4356–4369, 2021.\n[67] J. Duan, H. Cheng, S. Wang, C. Wang, A. Zavalny, R. Xu,\nB. Kailkhura, and K. Xu, “Shifting attention to relevance: Towards\ntheuncertaintyestimationoflargelanguagemodels,” arXivpreprint\narXiv:2307.01379, 2023.\n[68] J.Duan,F.Kong,S.Wang,X.Shi,andK.Xu,“Arediffusionmodels\nvulnerable to membership inference attacks?” inProceedings of\nthe 40th International Conference on Machine Learning, 2023, pp.\n8717–8730.\n[69] F. Duarte, “Number of chatgpt users (nov 2023),” https://\nexplodingtopics.com/blog/chatgpt-users, 2023, accessed: 2023-11-\n13.\n[70] C. Dwork, “Differential privacy,” inInternational colloquium on\nautomata,languages,andprogramming . Springer,2006,pp.1–12.\n[71] C. Egersdoerfer, D. Zhang, and D. Dai, “Early exploration of using\nchatgpt for log-based anomaly detection on parallel file systems\nlogs,” 2023.\n[72] D.O.Eke,“Chatgptandtheriseofgenerativeai:threattoacademic\nintegrity?” Journal of Responsible Technology, vol. 13, p. 100060,\n2023.\n[73] A. Elhafsi, R. Sinha, C. Agia, E. Schmerling, I. A. Nesnas, and\nM. Pavone, “Semantic anomaly detection with large language mod-\nels,”Autonomous Robots, pp. 1–21, 2023.\n[74] S. Eli and D. Gil, “Self-enhancing pattern detection with llms:\nOur answer to uncovering malicious packages at scale,” https:\n//apiiro.com/blog/llm-code-pattern-malicious-package-detection/,\n2023, accessed: 2023-11-13.\n[75] T.EspinhaGasiba,K.Oguzhan,I.Kessba,U.Lechner,andM.Pinto-\nAlbuquerque, “I’m sorry dave, i’m afraid i can’t fix your code:\nOn chatgpt, cybersecurity, and secure coding,” in4th International\nComputer Programming Education Conference (ICPEC 2023) .\nSchloss-Dagstuhl-Leibniz Zentrum für Informatik, 2023.\n[76] P. V. Falade, “Decoding the threat landscape: Chatgpt, fraudgpt,\nand wormgpt in social engineering attacks,”International Journal\nof Scientific Research in Computer Science, Engineering and\nInformationTechnology,p.185–198,Oct.2023.[Online].Available:\nhttp://dx.doi.org/10.32628/CSEIT2390533\n[77] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta,\nS. Yoo, and J. M. Zhang, “Large language models for software\nengineering: Survey and open problems,” 2023.\n[78] T. Fan, Y. Kang, G. Ma, W. Chen, W. Wei, L. Fan, and Q. Yang,\n“Fate-llm:Aindustrialgradefederatedlearningframeworkforlarge\nlanguage models,”arXiv preprint arXiv:2310.10049, 2023.\n[79] X. Fang, S. Che, M. Mao, H. Zhang, M. Zhao, and X. Zhao, “Bias\nof ai-generated content: An examination of news produced by large\nYifan Yao et al.:Preprint submitted to Elsevier Page 17 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\nlanguage models,”arXiv preprint arXiv:2309.09825, 2023.\n[80] J. C. Farah, B. Spaenlehauer, V. Sharma, M. J. Rodríguez-Triana,\nS. Ingram, and D. Gillet, “Impersonating chatbots in a code review\nexercisetoteachsoftwareengineeringbestpractices,”in 2022IEEE\nGlobal Engineering Education Conference (EDUCON). IEEE,\n2022, pp. 1634–1642.\n[81] V. K. Felkner, H.-C. H. Chang, E. Jang, and J. May, “Winoqueer:\nA community-in-the-loop benchmark for anti-lgbtq+ bias in large\nlanguage models,”arXiv preprint arXiv:2306.15087, 2023.\n[82] S.Y.Feng,V.Gangal,J.Wei,S.Chandar,S.Vosoughi,T.Mitamura,\nand E. Hovy, “A survey of data augmentation approaches for nlp,”\narXiv preprint arXiv:2105.03075, 2021.\n[83] M. Fu, C. Tantithamthavorn, V. Nguyen, and T. Le, “Chatgpt for\nvulnerability detection, classification, and repair: How far are we?”\n2023.\n[84] W.Fu,H.Wang,C.Gao,G.Liu,Y.Li,andT.Jiang,“Practicalmem-\nbership inference attacks against fine-tuned large language models\nvia self-prompt calibration,” 2023.\n[85] ——, “A probabilistic fluctuation based membership inference at-\ntack for diffusion models,” 2023.\n[86] P. Ganesh, H. Chang, M. Strobel, and R. Shokri, “On the impact\nofmachinelearningrandomnessongroupfairness,”in Proceedings\nof the 2023 ACM Conference on Fairness, Accountability, and\nTransparency, 2023, pp. 1789–1800.\n[87] C. A. Gao, F. M. Howard, N. S. Markov, E. C. Dyer, S. Ramesh,\nY. Luo, and A. T. Pearson, “Comparing scientific abstracts gener-\nated by chatgpt to original abstracts using an artificial intelligence\noutput detector, plagiarism detector, and blinded human reviewers,”\nBioRxiv, pp. 2022–12, 2022.\n[88] S.Gehman, S.Gururangan, M.Sap,Y. Choi,and N.A.Smith, “Re-\naltoxicityprompts:Evaluatingneuraltoxicdegenerationinlanguage\nmodels,”arXiv preprint arXiv:2009.11462, 2020.\n[89] K. Greshake, S. Abdelnabi, S. Mishra, C. Endres, T. Holz, and\nM. Fritz, “More than you’ve asked for: A comprehensive analysis\nof novel prompt injection threats to application-integrated large\nlanguage models,”arXiv preprint arXiv:2302.12173, 2023.\n[90] Q. Gu, “Llm-based code generation method for golang compiler\ntesting,” 2023.\n[91] Z. Gu, B. Zhu, G. Zhu, Y. Chen, M. Tang, and J. Wang, “Anoma-\nlygpt: Detecting industrial anomalies using large vision-language\nmodels,”arXiv preprint arXiv:2308.15366, 2023.\n[92] M. Gupta, C. Akiri, K. Aryal, E. Parker, and L. Praharaj, “From\nchatgpt to threatgpt: Impact of generative ai in cybersecurity and\nprivacy,”IEEE Access, 2023.\n[93] M. U. Hadi, R. Qureshi, A. Shah, M. Irfan, A. Zafar, M. Shaikh,\nN. Akhtar, J. Wu, and S. Mirjalili, “A survey on large language\nmodels: Applications, challenges, limitations, and practical usage,”\nTechRxiv, 2023.\n[94] A.HappeandJ.Cito,“Gettingpwn’dbyai:Penetrationtestingwith\nlarge language models,”arXiv preprint arXiv:2308.00121, 2023.\n[95] A. Happe, A. Kaplan, and J. Cito, “Evaluating llms for privilege-\nescalation scenarios,” 2023.\n[96] J. Hayes, L. Melis, G. Danezis, and E. De Cristofaro, “Logan:\nMembership inference attacks against generative models,”arXiv\npreprint arXiv:1705.07663, 2017.\n[97] J. Hazell, “Large language models can be used to effectively scale\nspear phishing campaigns,” 2023.\n[98] J. He and M. Vechev, “Large language models for code: Security\nhardening and adversarial testing,”ICML 2023 Workshop Deploy-\nableGenerativeAI, 2023, keywords: large language models, code\ngeneration, security, prompt tuning.\n[99] ——, “Large language models for code: Security hardening and\nadversarial testing,” in Proceedings of the 2023 ACM SIGSAC\nConference on Computer and Communications Security, 2023, pp.\n1865–1879.\n[100] X.He,S.Zannettou,Y.Shen,andY.Zhang,“Youonlypromptonce:\nOn the capabilities of prompt learning on large language models to\ntackle toxic content,”arXiv preprint arXiv:2308.05596, 2023.\n[101] ——,“Youonlypromptonce:Onthecapabilitiesofpromptlearning\non large language models to tackle toxic content,” in2024 IEEE\nSymposium on Security and Privacy (SP), 2024.\n[102] F.Heiding,B.Schneier,A.Vishwanath,andJ.Bernstein,“Devising\nand detecting phishing: Large language models vs. smaller human\nmodels,” 2023.\n[103] A.Helbling,M.Phute,M.Hull,andD.H.Chau,“Llmselfdefense:\nBy self examination, llms know they are being tricked,” arXiv\npreprint arXiv:2308.07308, 2023.\n[104] R.HelmkeandJ.vomDorp,“Checkforextendedabstract:Towards\nreliable and scalable linux kernel cve attribution in automated static\nfirmware analyses,” inDetection of Intrusions and Malware, and\nVulnerability Assessment: 20th International Conference, DIMVA\n2023, Hamburg, Germany, July 12–14, 2023, Proceedings, vol.\n13959. Springer Nature, 2023, p. 201.\n[105] P.Henrik,“Llm-assistedmalwarereview:Aiandhumansjoinforces\ntocombatmalware,”https://shorturl.at/loqT4,2023,accessed:2023-\n11-13.\n[106] D.Hernandez,T.Brown,T.Conerly,N.DasSarma,D.Drain,S.El-\nShowk, N. Elhage, Z. Hatfield-Dodds, T. Henighan, T. Humeet al.,\n“Scaling laws and interpretability of learning from repeated data,”\narXiv preprint arXiv:2205.10487, 2022.\n[107] B. Hettwer, S. Gehrer, and T. Güneysu, “Applications of machine\nlearning techniques in side-channel attacks: a survey,”Journal of\nCryptographic Engineering, vol. 10, pp. 135–162, 2020.\n[108] X. Hou, Y. Zhao, Y. Liu, Z. Yang, K. Wang, L. Li, X. Luo,\nD. Lo, J. Grundy, and H. Wang, “Large language models for soft-\nware engineering: A systematic literature review,”arXiv preprint\narXiv:2308.10620, 2023.\n[109] J. Hu, Q. Zhang, and H. Yin, “Augmenting greybox fuzzing with\ngenerative ai,”arXiv preprint arXiv:2306.06782, 2023.\n[110] S. Hu, T. Huang, F. İlhan, S. F. Tekin, and L. Liu, “Large language\nmodel-powered smart contract vulnerability detection: New\nperspectives,” arXiv preprint arXiv:2310.01152, 2023, 10 pages.\n[Online]. Available: https://doi.org/10.48550/arXiv.2310.01152\n[111] D. Huang, Q. Bu, J. Zhang, X. Xie, J. Chen, and H. Cui, “Bias\nassessment and mitigation in llm-based code generation,”arXiv\npreprint arXiv:2309.14345, 2023.\n[112] H. Huang, W. Luo, G. Zeng, J. Weng, Y. Zhang, and A. Yang,\n“Damia: leveraging domain adaptation as a defense against mem-\nbership inference attacks,”IEEE Transactions on Dependable and\nSecure Computing, vol. 19, no. 5, pp. 3183–3199, 2021.\n[113] J. Huang, H. Shao, and K. C.-C. Chang, “Are large pre-trained\nlanguagemodelsleakingyourpersonalinformation?” arXivpreprint\narXiv:2205.12628, 2022.\n[114] V. M. Igure and R. D. Williams, “Taxonomies of attacks and vul-\nnerabilities in computer systems,”IEEE Communications Surveys\n& Tutorials, vol. 10, no. 1, pp. 6–19, 2008.\n[115] U. Iqbal, T. Kohno, and F. Roesner, “Llm platform security: Apply-\ning a systematic evaluation framework to openai’s chatgpt plugins,”\n2023.\n[116] M.IvgiandJ.Berant,“Achievingmodelrobustnessthroughdiscrete\nadversarial training,”arXiv preprint arXiv:2104.05062, 2021.\n[117] N. Jain, A. Schwarzschild, Y. Wen, G. Somepalli, J. Kirchenbauer,\nP.-y. Chiang, M. Goldblum, A. Saha, J. Geiping, and T. Goldstein,\n“Baseline defenses for adversarial attacks against aligned language\nmodels,”arXiv preprint arXiv:2309.00614, 2023.\n[118] R. Jain, N. Gervasoni, M. Ndhlovu, and S. Rawat, “A code centric\nevaluation of c/c++ vulnerability datasets for deep learning based\nvulnerability detection techniques,” inProceedings of the 16th In-\nnovations in Software Engineering Conference, 2023, pp. 1–10.\n[119] S. Jamal and H. Wimmer, “An improved transformer-based model\nfor detecting phishing, spam, and ham: A large language model\napproach,” 2023.\n[120] B. Jayaraman, L. Wang, K. Knipmeyer, Q. Gu, and D. Evans, “Re-\nvisiting membership inference under realistic assumptions,”arXiv\npreprint arXiv:2005.10881, 2020.\nYifan Yao et al.:Preprint submitted to Elsevier Page 18 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\n[121] H. Jiang, P. He, W. Chen, X. Liu, J. Gao, and T. Zhao, “Smart:\nRobust and efficient fine-tuning for pre-trained natural language\nmodelsthroughprincipledregularizedoptimization,” arXivpreprint\narXiv:1911.03437, 2019.\n[122] J.Jiang,X.Liu,andC.Fan,“Low-parameterfederatedlearningwith\nlarge language models,”arXiv preprint arXiv:2307.13896, 2023.\n[123] N. Jiang, K. Liu, T. Lutellier, and L. Tan, “Impact of code language\nmodels on automated program repair,” 2023.\n[124] S. Jiang, X. Chen, and R. Tang, “Prompt packer: Deceiving\nllms through compositional instruction with hidden attacks,”arXiv\npreprint arXiv:2310.10077, 2023.\n[125] M. Jin, S. Shahriar, M. Tufano, X. Shi, S. Lu, N. Sundaresan, and\nA. Svyatkovskiy, “Inferfix: End-to-end program repair with llms,”\n2023.\n[126] X. Jin, J. Larson, W. Yang, and Z. Lin, “Binary code summariza-\ntion:Benchmarkingchatgpt/gpt-4andotherlargelanguagemodels,”\n2023.\n[127] X. Jin, K. Pei, J. Y. Won, and Z. Lin, “Symlm: Predicting func-\ntion names in stripped binaries via context-sensitive execution-\naware code embeddings,” inProceedings of the 2022 ACM SIGSAC\nConference on Computer and Communications Security, 2022, pp.\n1631–1645.\n[128] C. Joshi, U. K. Singh, and K. Tarey, “A review on taxonomies of\nattacksandvulnerabilityincomputerandnetworksystem,” Interna-\ntional Journal, vol. 5, no. 1, 2015.\n[129] A. Joulin, E. Grave, P. Bojanowski, M. Douze, H. Jégou, and\nT. Mikolov, “Fasttext. zip: Compressing text classification models,”\narXiv preprint arXiv:1612.03651, 2016.\n[130] M. Juuti, S. Szyller, S. Marchal, and N. Asokan, “Prada: protecting\nagainst dnn model stealing attacks,” in2019 IEEE European Sym-\nposiumonSecurityandPrivacy(EuroS&P) . IEEE,2019,pp.512–\n527.\n[131] S.Kadavath,T.Conerly,A.Askell,T.Henighan,D.Drain,E.Perez,\nN.Schiefer,Z.Hatfield-Dodds,N.DasSarma,E.Tran-Johnson etal.,\n“Language models (mostly) know what they know,”arXiv preprint\narXiv:2207.05221, 2022.\n[132] N. Kandpal, M. Jagielski, F. Tramèr, and N. Carlini, “Backdoor\nattacksforin-contextlearningwithlanguagemodels,” arXivpreprint\narXiv:2307.14692, 2023.\n[133] N.Kandpal,K.Pillutla,A.Oprea,P.Kairouz,C.A.Choquette-Choo,\nandZ.Xu,“Userinferenceattacksonlargelanguagemodels,”2023.\n[134] N. Kandpal, E. Wallace, and C. Raffel, “Deduplicating training\ndata mitigates privacy risks in language models,” inInternational\nConference on Machine Learning. PMLR, 2022, pp. 10697–\n10707.\n[135] D.Kang,X.Li,I.Stoica,C.Guestrin,M.Zaharia,andT.Hashimoto,\n“Exploiting programmatic behavior of llms: Dual-use through stan-\ndard security attacks,”arXiv preprint arXiv:2302.05733, 2023.\n[136] S. Kang, J. Yoon, and S. Yoo, “Llm lies: Hallucinations are not\nbugs,butfeaturesasadversarialexamples,”in 2023IEEE/ACM45th\nInternational Conference on Software Engineering (ICSE). IEEE,\n2023.\n[137] S. Kariyappa, A. Prakash, and M. K. Qureshi, “Maze: Data-free\nmodel stealing attack using zeroth-order gradient estimation,” in\nProceedings of the IEEE/CVF Conference on Computer Vision and\nPattern Recognition, 2021, pp. 13814–13823.\n[138] M. Karpinska and M. Iyyer, “Large language models effectively\nleverage document-level context for literary translation, but critical\nerrors persist,”arXiv preprint arXiv:2304.03245, 2023.\n[139] M. Khalil and E. Er, “Will chatgpt get you caught? rethinking of\nplagiarism detection,”arXiv preprint arXiv:2302.04335, 2023.\n[140] J.Kirchenbauer,J.Geiping,Y.Wen,M.Shu,K.Saifullah,K.Kong,\nK. Fernando, A. Saha, M. Goldblum, and T. Goldstein, “On the\nreliability of watermarks for large language models,”arXiv preprint\narXiv:2306.04634, 2023.\n[141] C.Koch,“Iusedgpt-3tofind213securityvulnerabilitiesinasingle\ncodebase,” http://surl.li/ncjvo, 2023.\n[142] T.Koide,N.Fukushi,H.Nakano,andD.Chiba,“Detectingphishing\nsites using chatgpt,”arXiv preprint arXiv:2306.05816, 2023.\n[143] F. Kong, J. Duan, R. Ma, H. Shen, X. Zhu, X. Shi, and K. Xu,\n“An efficient membership inference attack for the diffusion model\nby proximal initialization,”arXiv preprint arXiv:2305.18355, 2023.\n[144] H. Kotek, R. Dockum, and D. Sun, “Gender bias and stereotypes\nin large language models,” inProceedings of The ACM Collective\nIntelligence Conference, 2023, pp. 12–24.\n[145] W. Kuang, B. Qian, Z. Li, D. Chen, D. Gao, X. Pan, Y. Xie,\nY.Li,B.Ding,andJ.Zhou,“Federatedscope-llm:Acomprehensive\npackageforfine-tuninglargelanguagemodelsinfederatedlearning,”\narXiv preprint arXiv:2309.00363, 2023.\n[146] K. Kumari, A. Pegoraro, H. Fereidooni, and A.-R. Sadeghi,\n“Demasq: Unmasking the chatgpt wordsmith,” arXiv preprint\narXiv:2311.05019, 2023.\n[147] ——,“Demasq:Unmaskingthechatgptwordsmith,”in Proceedings\nofthe31thAnnualNetworkandDistributedSystemSecuritySympo-\nsium (NDSS’24), 2024.\n[148] K. Kurita, P. Michel, and G. Neubig, “Weight poisoning attacks on\npre-trained models,”arXiv preprint arXiv:2004.06660, 2020.\n[149] H. Kwon, M. Sim, G. Song, M. Lee, and H. Seo, “Novel approach\nto cryptography implementation using chatgpt,” Cryptology ePrint\nArchive, Paper 2023/606, 2023, https://eprint.iacr.org/2023/606.\n[Online]. Available: https://eprint.iacr.org/2023/606\n[150] J. E. Laird, C. Lebiere, and P. S. Rosenbloom, “A standard model\nof the mind: Toward a common computational framework across\nartificialintelligence,cognitivescience,neuroscience,androbotics,”\nAi Magazine, vol. 38, no. 4, pp. 13–26, 2017.\n[151] T. Langford and B. Payne, “Phishing faster: Implementing chatgpt\nintophishingcampaigns,”in ProceedingsoftheFutureTechnologies\nConference. Springer, 2023, pp. 174–187.\n[152] H. Laurençon, L. Saulnier, T. Wang, C. Akiki, A. Villanova del\nMoral, T. Le Scao, L. Von Werra, C. Mou, E. González Ponferrada,\nH. Nguyenet al., “The bigscience roots corpus: A 1.6 tb composite\nmultilingual dataset,”Advances in Neural Information Processing\nSystems, vol. 35, pp. 31809–31826, 2022.\n[153] K. Lee, D. Ippolito, A. Nystrom, C. Zhang, D. Eck, C. Callison-\nBurch,andN.Carlini,“Deduplicatingtrainingdatamakeslanguage\nmodels better,”arXiv preprint arXiv:2107.06499, 2021.\n[154] T.Lee,S.Hong,J.Ahn,I.Hong,H.Lee,S.Yun,J.Shin,andG.Kim,\n“Who wrote this code? watermarking for code generation,” 2023.\n[155] J. A. Leite, O. Razuvayevskaya, K. Bontcheva, and C. Scarton,\n“Detectingmisinformationwithllm-predictedcredibilitysignalsand\nweak supervision,”arXiv preprint arXiv:2309.07601, 2023.\n[156] C. Lemieux, J. P. Inala, S. K. Lahiri, and S. Sen, “Codamosa:\nEscaping coverage plateaus in test generation with pre-trained large\nlanguagemodels,”in Internationalconferenceonsoftwareengineer-\ning (ICSE), 2023.\n[157] J.Leskovec,A.Rajaraman,andJ.D.Ullman, Miningofmassivedata\nsets. Cambridge university press, 2020.\n[158] C. Li, Z. Song, W. Wang, and C. Yang, “A theoretical insight\ninto attack and defense of gradient leakage in transformer,”arXiv\npreprint arXiv:2311.13624, 2023.\n[159] H.Li,D.Guo,W.Fan,M.Xu,andY.Song,“Multi-stepjailbreaking\nprivacyattacksonchatgpt,” arXivpreprintarXiv:2304.05197 ,2023.\n[160] H. Li, Y. Song, and L. Fan, “You don’t know my favorite color:\nPreventingdialoguerepresentationsfromrevealingspeakers’private\npersonas,” in Proceedings of the 2022 Conference of the North\nAmericanChapteroftheAssociationforComputationalLinguistics:\nHuman Language Technologies, M. Carpuat, M.-C. de Marneffe,\nand I. V. Meza Ruiz, Eds. Seattle, United States: Association\nfor Computational Linguistics, Jul. 2022, pp. 5858–5870. [Online].\nAvailable: https://aclanthology.org/2022.naacl-main.429\n[161] J. Li, Y. Yang, Z. Wu, V. Vydiswaran, and C. Xiao, “Chatgpt as an\nattacktool:Stealthytextualbackdoorattackviablackboxgenerative\nmodel trigger,”arXiv preprint arXiv:2304.14475, 2023.\n[162] J. Li, P. H. Meland, J. S. Notland, A. Storhaug, and J. H. Tysse,\n“Evaluatingtheimpactofchatgptonexercisesofasoftwaresecurity\nYifan Yao et al.:Preprint submitted to Elsevier Page 19 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\ncourse,” 2023.\n[163] L.LiandX.Qiu,“Token-awarevirtualadversarialtraininginnatural\nlanguageunderstanding,”in ProceedingsoftheAAAIConferenceon\nArtificial Intelligence, vol. 35, no. 9, 2021, pp. 8410–8418.\n[164] L.Li,D.Song,andX.Qiu,“Textadversarialpurificationasdefense\nagainstadversarialattacks,” arXivpreprintarXiv:2203.14207 ,2022.\n[165] X. Li, F. Tramer, P. Liang, and T. Hashimoto, “Large language\nmodels can be strong differentially private learners,”arXiv preprint\narXiv:2110.05679, 2021.\n[166] Y. Li, Z. Tan, and Y. Liu, “Privacy-preserving prompt tuning for\nlarge language model services,”arXiv preprint arXiv:2305.06212,\n2023.\n[167] Y. Li, S. Liu, K. Chen, X. Xie, T. Zhang, and Y. Liu, “Multi-target\nbackdoor attacks for code pre-trained models,” 2023.\n[168] Z. Li, B. Peng, P. He, and X. Yan, “Evaluating the instruction-\nfollowing robustness of large language models to prompt\ninjection,” 2023. [Online]. Available: https://api.semanticscholar.\norg/CorpusID:261048972\n[169] Z. Li, C. Wang, S. Wang, and C. Gao, “Protecting intellectual\nproperty of large language model-based code generation apis via\nwatermarks,” inProceedings of the 2023 ACM SIGSAC Conference\non Computer and Communications Security, 2023, pp. 2336–2350.\n[170] P.Liang,R.Bommasani,T.Lee,D.Tsipras,D.Soylu,M.Yasunaga,\nY. Zhang, D. Narayanan, Y. Wu, A. Kumar, B. Newman, B. Yuan,\nB. Yan, C. Zhang, C. Cosgrove, C. D. Manning, C. Ré, D. Acosta-\nNavas,D.A.Hudson,E.Zelikman,E.Durmus,F.Ladhak,F.Rong,\nH. Ren, H. Yao, J. Wang, K. Santhanam, L. Orr, L. Zheng, M. Yuk-\nsekgonul, M. Suzgun, N. Kim, N. Guha, N. Chatterji, O. Khattab,\nP.Henderson,Q.Huang,R.Chi,S.M.Xie,S.Santurkar,S.Ganguli,\nT. Hashimoto, T. Icard, T. Zhang, V. Chaudhary, W. Wang, X. Li,\nY. Mai, Y. Zhang, and Y. Koreeda, “Holistic evaluation of language\nmodels,” 2023.\n[171] S.Lin,J.Hilton,andO.Evans,“Truthfulqa:Measuringhowmodels\nmimic human falsehoods,”arXiv preprint arXiv:2109.07958, 2021.\n[172] B. Liu, B. Xiao, X. Jiang, S. Cen, X. He, W. Douet al., “Adver-\nsarial attacks on large language model-based system and mitigating\nstrategies: A case study on chatgpt,”Security and Communication\nNetworks, vol. 2023, 2023.\n[173] C. Liu, F. Zhao, L. Qing, Y. Kang, C. Sun, K. Kuang, and F. Wu,\n“A chinese prompt attack dataset for llms with evil content,”arXiv\npreprint arXiv:2309.11830, 2023.\n[174] P.Liu,C.Sun,Y.Zheng,X.Feng,C.Qin,Y.Wang,Z.Li,andL.Sun,\n“Harnessingthepowerofllmtosupportbinarytaintanalysis,”2023.\n[175] T. Liu, Z. Deng, G. Meng, Y. Li, and K. Chen, “Demystifying rce\nvulnerabilities in llm-integrated apps,” 2023.\n[176] X. Liu, H. Cheng, P. He, W. Chen, Y. Wang, H. Poon, and J. Gao,\n“Adversarial training for large neural language models,” arXiv\npreprint arXiv:2004.08994, 2020.\n[177] X.Liu,N.Xu,M.Chen,andC.Xiao,“Autodan:Generatingstealthy\njailbreakpromptsonalignedlargelanguagemodels,” arXivpreprint\narXiv:2310.04451, 2023.\n[178] Y. Liu, G. Deng, Y. Li, K. Wang, T. Zhang, Y. Liu, H. Wang,\nY.Zheng,andY.Liu,“Promptinjectionattackagainstllm-integrated\napplications,”arXiv preprint arXiv:2306.05499, 2023.\n[179] C.K.Lo,“Whatistheimpactofchatgptoneducation?arapidreview\nof the literature,”Education Sciences, vol. 13, no. 4, p. 410, 2023.\n[180] V. Logacheva, D. Dementieva, S. Ustyantsev, D. Moskovskiy,\nD. Dale, I. Krotova, N. Semenov, and A. Panchenko, “Paradetox:\nDetoxificationwithparalleldata,”in Proceedingsofthe60thAnnual\nMeeting of the Association for Computational Linguistics (Volume\n1: Long Papers), 2022, pp. 6804–6818.\n[181] L. Lyu, X. He, and Y. Li, “Differentially private representation\nfor NLP: Formal guarantee and an empirical study on\nprivacy and fairness,” in Findings of the Association for\nComputational Linguistics: EMNLP 2020 , T. Cohn, Y. He,\nand Y. Liu, Eds. Online: Association for Computational\nLinguistics, Nov. 2020, pp. 2355–2365. [Online]. Available:\nhttps://aclanthology.org/2020.findings-emnlp.213\n[182] A. Madry, A. Makelov, L. Schmidt, D. Tsipras, and A. Vladu,\n“Towards deep learning models resistant to adversarial attacks,”\narXiv preprint arXiv:1706.06083, 2017.\n[183] S. Mahloujifar, H. A. Inan, M. Chase, E. Ghosh, and\nM. Hasegawa, “Membership inference on word embedding and\nbeyond,” ArXiv, vol. abs/2106.11384, 2021. [Online]. Available:\nhttps://api.semanticscholar.org/CorpusID:235593386\n[184] J.Majmudar,C.Dupuy,C.Peris,S.Smaili,R.Gupta,andR.Zemel,\n“Differentially private decoding in large language models,”arXiv\npreprint arXiv:2205.13621, 2022.\n[185] J. Marshall, “What effects do large language models have on cyber-\nsecurity,” 2023.\n[186] A. B. Mbakwe, I. Lourentzou, L. A. Celi, O. J. Mechanic, and\nA. Dagan, “Chatgpt passing usmle shines a spotlight on the flaws\nof medical education,” p. e0000205, 2023.\n[187] T.McIntosh,T.Liu,T.Susnjak,H.Alavizadeh,A.Ng,R.Nowrozy,\nand P. Watters, “Harnessing gpt-4 for generation of cybersecurity\ngrc policies: A focus on ransomware attack mitigation,”Computers\n& Security, vol. 134, p. 103424, 2023.\n[188] N. Meade, E. Poole-Dayan, and S. Reddy, “An empirical survey of\nthe effectiveness of debiasing techniques for pre-trained language\nmodels,”arXiv preprint arXiv:2110.08527, 2021.\n[189] M.MéndezRealandR.Salvador,“Physicalside-channelattackson\nembedded neural networks: A survey,”Applied Sciences, vol. 11,\nno. 15, p. 6790, 2021.\n[190] R. Meng, M. Mirchev, M. Böhme, and A. Roychoudhury, “Large\nlanguage model guided protocol fuzzing,” inProceedings of the\n31th Annual Network and Distributed System Security Symposium\n(NDSS’24), 2024.\n[191] F. Mireshghallah, K. Goyal, A. Uniyal, T. Berg-Kirkpatrick, and\nR. Shokri, “Quantifying privacy risks of masked language models\nusing membership inference attacks,” 2022.\n[192] F. Mireshghallah, A. Uniyal, T. Wang, D. Evans, and T. Berg-\nKirkpatrick, “An empirical analysis of memorization in fine-\ntuned autoregressive language models,” in Proceedings of the\n2022 Conference on Empirical Methods in Natural Language\nProcessing, Y. Goldberg, Z. Kozareva, and Y. Zhang, Eds. Abu\nDhabi, United Arab Emirates: Association for Computational\nLinguistics, Dec. 2022, pp. 1816–1826. [Online]. Available:\nhttps://aclanthology.org/2022.emnlp-main.119\n[193] W.Mo,J.Xu,Q.Liu,J.Wang,J.Yan,C.Xiao,andM.Chen,“Test-\ntime backdoor mitigation for black-box large language models with\ndefensive demonstrations,”arXiv preprint arXiv:2311.09763, 2023.\n[194] A.Monje,A.Monje,R.A.Hallman,andG.Cybenko,“Beingabad\ninfluence on the kids: Malware generation in less than five minutes\nusing chatgpt,” 2023.\n[195] D. Moskovskiy, D. Dementieva, and A. Panchenko, “Exploring\ncross-lingual text detoxification with large multilingual language\nmodels.” inProceedings of the 60th Annual Meeting of the Asso-\nciationforComputationalLinguistics: StudentResearchWorkshop ,\n2022, pp. 346–354.\n[196] M. Mozes, X. He, B. Kleinberg, and L. D. Griffin, “Use of llms for\nillicit purposes: Threats, prevention measures, and vulnerabilities,”\n2023.\n[197] M. Nair, R. Sadhukhan, and D. Mukhopadhyay, “Generating secure\nhardware using chatgpt resistant to cwes,” Cryptology ePrint\nArchive, Paper 2023/212, 2023, https://eprint.iacr.org/2023/212.\n[Online]. Available: https://eprint.iacr.org/2023/212\n[198] S. Narang and A. Chowdhery, “Pathways language\nmodel (palm): Scaling to 540 billion parameters for\nbreakthrough performance,” https://blog.research.google/2022/\n04/pathways-language-model-palm-scaling-to.html, apr 2022,\naccessed: 2023-11-13.\n[199] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. Wang, and\nX. V. Lin, “Lever: Learning to verify language-to-code generation\nwith execution,” inInternational Conference on Machine Learning.\nPMLR, 2023, pp. 26106–26128.\nYifan Yao et al.:Preprint submitted to Elsevier Page 20 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\n[200] S. Nikolic, S. Daniel, R. Haque, M. Belkina, G. M. Hassan,\nS. Grundy, S. Lyden, P. Neal, and C. Sandison, “Chatgpt versus\nengineering education assessment: a multidisciplinary and multi-\ninstitutional benchmarking and analysis of this generative artificial\nintelligencetooltoinvestigateassessmentintegrity,” EuropeanJour-\nnal of Engineering Education, pp. 1–56, 2023.\n[201] D. Noever, “Can large language models find and fix vulnerable\nsoftware?” arXiv preprint arXiv:2308.10345 , 2023. [Online].\nAvailable: https://doi.org/10.48550/arXiv.2308.10345\n[202] C. Novelli, F. Casolari, A. Rotolo, M. Taddeo, and L. Floridi,\n“Taking ai risks seriously: a new assessment model for the ai act,”\nAI & SOCIETY, pp. 1–5, 2023.\n[203] OpenAI,“Gpt-4technicalreport,”https://arxiv.org/abs/2303.08774,\n2023.\n[204] N. Ousidhoum, X. Zhao, T. Fang, Y. Song, and D.-Y. Yeung,\n“Probing toxic content in large pre-trained language models,” in\nProceedingsofthe59thAnnualMeetingoftheAssociationforCom-\nputational Linguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long Papers), 2021,\npp. 4262–4274.\n[205] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.Wainwright,P.Mishkin,\nC. Zhang, S. Agarwal, K. Slama, A. Rayet al., “Training language\nmodels to follow instructions with human feedback,”Advances in\nNeuralInformationProcessingSystems ,vol.35,pp.27730–27744,\n2022.\n[206] OWASP. (2023, Oct) OWASP Top 10 for\nLLM. [Online]. Available: https://owasp.org/\nwww-project-top-10-for-large-language-model-applications/\nassets/PDF/OWASP-Top-10-for-LLMs-2023-v1_1.pdf\n[207] Y. M. Pa Pa, S. Tanizaki, T. Kou, M. Van Eeten, K. Yoshioka, and\nT. Matsumoto, “An attacker’s dream? exploring the capabilities of\nchatgpt for developing malware,” inProceedings of the 16th Cyber\nSecurity Experimentation and Test Workshop, 2023, pp. 10–18.\n[208] X. Pan, M. Zhang, S. Ji, and M. Yang, “Privacy risks of general-\npurpose language models,” in2020 IEEE Symposium on Security\nand Privacy (SP). IEEE, 2020, pp. 1314–1331.\n[209] S. Paria, A. Dasgupta, and S. Bhunia, “Divas: An llm-based end-\nto-end framework for soc security analysis and policy-based protec-\ntion,”arXiv preprint arXiv:2308.06932, 2023.\n[210] R. Parikh, C. Dupuy, and R. Gupta, “Canary extraction in natural\nlanguageunderstandingmodels,” arXivpreprintarXiv:2203.13920 ,\n2022.\n[211] H. Pearce, B. Tan, B. Ahmad, R. Karri, and B. Dolan-Gavitt, “Ex-\namining zero-shot vulnerability repair with large language models,”\nin 2023 IEEE Symposium on Security and Privacy (SP), 2023, pp.\n2339–2356.\n[212] H. Pearce, B. Tan, P. Krishnamurthy, F. Khorrami, R. Karri, and\nB. Dolan-Gavitt, “Pop quiz! can a large language model help with\nreverse engineering?” 2022.\n[213] G. Penedo, Q. Malartic, D. Hesslow, R. Cojocaru, A. Cappelli,\nH. Alobeidli, B. Pannier, E. Almazrouei, and J. Launay, “The re-\nfinedwebdatasetforfalconllm:outperformingcuratedcorporawith\nweb data, and web data only,”arXiv preprint arXiv:2306.01116,\n2023.\n[214] C.Peris,C.Dupuy,J.Majmudar,R.Parikh,S.Smaili,R.Zemel,and\nR. Gupta, “Privacy in the time of language models,” inProceedings\nof the Sixteenth ACM International Conference on Web Search and\nData Mining, 2023, pp. 1291–1292.\n[215] M. Perkins, “Academic integrity considerations of ai large language\nmodels in the post-pandemic era: Chatgpt and beyond,”Journal of\nUniversityTeaching&LearningPractice ,vol.20,no.2,p.07,2023.\n[216] A. Pfitzmann and M. Hansen, “A terminology for talking about\nprivacy by data minimization: Anonymity, unlinkability, unde-\ntectability, unobservability, pseudonymity, and identity manage-\nment,” 2010.\n[217] V.-T. Pham, M. Böhme, and A. Roychoudhury, “Aflnet: a greybox\nfuzzer for network protocols,” in2020 IEEE 13th International\nConferenceonSoftwareTesting,ValidationandVerification(ICST) .\nIEEE, 2020, pp. 460–465.\n[218] M. D. Purba, A. Ghosh, B. J. Radford, and B. Chu, “Software\nvulnerability detection using large language models,” in2023 IEEE\n34th International Symposium on Software Reliability Engineering\nWorkshops (ISSREW), 2023, pp. 112–119.\n[219] A. Qammar, H. Wang, J. Ding, A. Naouri, M. Daneshmand, and\nH. Ning, “Chatbots to chatgpt in a cybersecurity space: Evolution,\nvulnerabilities, attacks, challenges, and future recommendations,”\n2023.\n[220] F. Qi, Y. Chen, M. Li, Y. Yao, Z. Liu, and M. Sun, “Onion: A\nsimpleandeffectivedefenseagainsttextualbackdoorattacks,” arXiv\npreprint arXiv:2011.10369, 2020.\n[221] J. Qi, S. Huang, Z. Luan, C. Fung, H. Yang, and D. Qian, “Loggpt:\nExploring chatgpt for log-based anomaly detection,”arXiv preprint\narXiv:2309.01189, 2023.\n[222] S. Qin, F. Hu, Z. Ma, B. Zhao, T. Yin, and C. Zhang, “Nsfuzz:\nTowards efficient and state-aware network service fuzzing,”ACM\nTransactions on Software Engineering and Methodology, 2023.\n[223] M.A.Quidwai,C.Li,andP.Dube,“Beyondblackboxai-generated\nplagiarism detection: From sentence to document level,” arXiv\npreprint arXiv:2306.08122, 2023.\n[224] M. Raeini, “Privacy-preserving large language models (ppllms),”\nAvailable at SSRN 4512071, 2023.\n[225] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\nY. Zhou, W. Li, and P. J. Liu, “Exploring the limits of transfer\nlearning with a unified text-to-text transformer,” 2023.\n[226] M. M. Rahman and Y. Watanobe, “Chatgpt for education and\nresearch: Opportunities, threats, and strategies,”Applied Sciences,\nvol. 13, no. 9, p. 5783, 2023.\n[227] J. Rando and F. Tramèr, “Universal jailbreak backdoors from poi-\nsoned human feedback,”arXiv preprint arXiv:2311.14455, 2023.\n[228] K. Renaud, M. Warkentin, and G. Westerman,From ChatGPT to\nHackGPT:MeetingtheCybersecurityThreatofGenerativeAI . MIT\nSloan Management Review, 2023.\n[229] S. A. Research, “Introducing a conditional transformer language\nmodel for controllable generation,” https://shorturl.at/azQW6, apr\n2023, accessed: 2023-11-13.\n[230] A.Robey,E.Wong,H.Hassani,and G.J.Pappas,“Smoothllm:De-\nfending large language models against jailbreaking attacks,”arXiv\npreprint arXiv:2310.03684, 2023.\n[231] O. J. Romero, J. Zimmerman, A. Steinfeld, and A. Tomasic, “Syn-\nergistic integration of large language models and cognitive archi-\ntectures for robust ai: An exploratory analysis,” arXiv preprint\narXiv:2308.09830, 2023.\n[232] R. J. Rosyanafi, G. D. Lestari, H. Susilo, W. Nusantara, and F. Nu-\nraini, “The dark side of innovation: Understanding research mis-\nconduct with chat gpt in nonformal education studies at universitas\nnegeri surabaya,”Jurnal Review Pendidikan Dasar: Jurnal Kajian\nPendidikan dan Hasil Penelitian, vol. 9, no. 3, pp. 220–228, 2023.\n[233] S. Sakaoglu, “Kartal: Web application vulnerability hunting using\nlarge language models,” Master’s thesis, Master’s Programme in\nSecurity and Cloud Computing (SECCLO), August 2023. [Online].\nAvailable: http://urn.fi/URN:NBN:fi:aalto-202308275121\n[234] G. Sandoval, H. Pearce, T. Nys, R. Karri, S. Garg, and\nB. Dolan-Gavitt, “Lost at c: A user study on the security\nimplications of large language model code assistants,” in\nUSENIX Security 2023, 2023, for associated dataset see [this\nURL](https://arxiv.org/abs/2208.09727). 18 pages, 12 figures. G.\nSandoval and H. Pearce contributed equally to this work. [Online].\nAvailable: https://arxiv.org/abs/2208.09727\n[235] Sapling, “Llm index,” https://sapling.ai/llm/index, 2023.\n[236] A. Sarabi, T. Yin, and M. Liu, “An llm-based framework for fin-\ngerprinting internet-connected devices,” inProceedings of the 2023\nACM on Internet Measurement Conference, 2023, pp. 478–484.\n[237] M. Scanlon, F. Breitinger, C. Hargreaves, J.-N. Hilgert, and\nJ. Sheppard, “Chatgpt for digital forensic investigation: The good,\nthe bad, and the unknown,”Forensic Science International: Digital\nInvestigation, vol. 46, p. 301609, 2023. [Online]. Available: https:\nYifan Yao et al.:Preprint submitted to Elsevier Page 21 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\n//www.sciencedirect.com/science/article/pii/S266628172300121X\n[238] M.Schäfer,S.Nadi,A.Eghbali,andF.Tip,“Adaptivetestgeneration\nusing a large language model,”arXiv preprint arXiv:2302.06527,\n2023.\n[239] R. Schuster, C. Song, E. Tromer, and V. Shmatikov, “You autocom-\nplete me: Poisoning vulnerabilities in neural code completion,” in\n30thUSENIXSecuritySymposium(USENIXSecurity21) ,2021,pp.\n1559–1575.\n[240] L. Schwinn, D. Dobre, S. Günnemann, and G. Gidel, “Adversarial\nattacksanddefensesinlargelanguagemodels:Oldandnewthreats,”\n2023.\n[241] G.Sebastian,“Dochatgptandotheraichatbotsposeacybersecurity\nrisk?: An exploratory study,”International Journal of Security and\nPrivacy in Pervasive Computing (IJSPPC), vol. 15, no. 1, pp. 1–11,\n2023.\n[242] ——, “Privacy and data protection in chatgpt and other ai chat-\nbots: Strategies for securing user information,”Available at SSRN\n4454761, 2023.\n[243] M. A. Shah, R. Sharma, H. Dhamyal, R. Olivier, A. Shah, D. Al-\nharthi, H. T. Bukhari, M. Baali, S. Deshmukh, M. Kuhlmann\net al., “Loft: Local proxy fine-tuning for improving transferability\nof adversarial attacks against large language model,”arXiv preprint\narXiv:2310.04445, 2023.\n[244] O. Shaikh, H. Zhang, W. Held, M. Bernstein, and D. Yang, “On\nsecondthought,let’snotthinkstepbystep!biasandtoxicityinzero-\nshot reasoning,”arXiv preprint arXiv:2212.08061, 2022.\n[245] S.Shan,W.Ding,J.Passananti,H.Zheng,andB.Y.Zhao,“Prompt-\nspecific poisoning attacks on text-to-image generative models,”\narXiv preprint arXiv:2310.13828, 2023.\n[246] K. Shao, J. Yang, Y. Ai, H. Liu, and Y. Zhang, “Bddr: An effective\ndefense against textual backdoor attacks,”Computers & Security,\nvol. 110, p. 102433, 2021.\n[247] E. Shayegani, M. A. A. Mamun, Y. Fu, P. Zaree, Y. Dong, and\nN.Abu-Ghazaleh,“Surveyofvulnerabilitiesinlargelanguagemod-\nels revealed by adversarial attacks,” 2023.\n[248] X.Shen,Z.Chen,M.Backes,Y.Shen,andY.Zhang,“\"doanything\nnow\": Characterizing and evaluating in-the-wild jailbreak prompts\nonlargelanguagemodels,” arXivpreprintarXiv:2308.03825 ,2023.\n[249] T. Shi, K. Chen, and J. Zhao, “Safer-instruct: Aligning lan-\nguage models with automated preference data,” arXiv preprint\narXiv:2311.08685, 2023.\n[250] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, “Membership\ninference attacks against machine learning models,” in2017 IEEE\nsymposium on security and privacy (SP). IEEE, 2017, pp. 3–18.\n[251] M. Shu, J. Wang, C. Zhu, J. Geiping, C. Xiao, and T. Gold-\nstein, “On the exploitability of instruction tuning,”arXiv preprint\narXiv:2306.17194, 2023.\n[252] I. Shumailov, Y. Zhao, D. Bates, N. Papernot, R. Mullins, and\nR. Anderson, “Sponge examples: Energy-latency attacks on neural\nnetworks,” 2021.\n[253] M. L. Siddiq, J. Santos, R. H. Tanvir, N. Ulfat, F. A. Rifat, and\nV. C. Lopes, “Exploring the effectiveness of large language models\nin generating unit tests,”arXiv preprint arXiv:2305.00418, 2023.\n[254] M. L. Siddiq and J. C. S. Santos, “Generate and pray: Using sallms\nto evaluate the security of llm generated code,” 2023, 16 pages.\n[Online]. Available: https://arxiv.org/abs/2311.00889\n[255] M. Sladić, V. Valeros, C. Catania, and S. Garcia, “Llm in the shell:\nGenerative honeypots,” 2023.\n[256] V.Smith,A.S.Shamsabadi,C.Ashurst,andA.Weller,“Identifying\nand mitigating privacy risks stemming from language models: A\nsurvey,” 2023.\n[257] D. Sobania, M. Briesch, C. Hanna, and J. Petke, “An analysis of the\nautomatic bug fixing performance of chatgpt,” 2023.\n[258] C. Song and A. Raghunathan, “Information leakage in embedding\nmodels,” inProceedings of the 2020 ACM SIGSAC conference on\ncomputer and communications security, 2020, pp. 377–390.\n[259] S. E. Spatharioti, D. M. Rothschild, D. G. Goldstein, and\nJ. M. Hofman, “Comparing traditional and llm-based search\nfor consumer choice: A randomized experiment,”arXiv preprint\narXiv:2307.03744, 2023.\n[260] R. Spreitzer, V. Moonsamy, T. Korak, and S. Mangard, “Systematic\nclassification of side-channel attacks: A case study for mobile de-\nvices,”IEEEcommunicationssurveys&tutorials ,vol.20,no.1,pp.\n465–488, 2017.\n[261] R. Staab, M. Vero, M. Balunović, and M. Vechev, “Beyond memo-\nrization: Violating privacy via inference with large language mod-\nels,” 2023.\n[262] K. Stephens, “Researchers test large language model that preserves\npatient privacy,”AXIS Imaging News, 2023.\n[263] J. Su, T. Y. Zhuo, J. Mansurov, D. Wang, and P. Nakov, “Fake\nnews detectors are biased against texts generated by large language\nmodels,”arXiv preprint arXiv:2309.08674, 2023.\n[264] N. Subramani, S. Luccioni, J. Dodge, and M. Mitchell, “Detecting\npersonal information in training corpora: an analysis,” inProceed-\ningsofthe3rdWorkshoponTrustworthyNaturalLanguageProcess-\ning (TrustNLP 2023), 2023, pp. 208–220.\n[265] M. Sullivan, A. Kelly, and P. McLaughlan, “Chatgpt in higher edu-\ncation: Considerations for academic integrity and student learning,”\n2023.\n[266] X. Sun, X. Li, Y. Meng, X. Ao, L. Lyu, J. Li, and T. Zhang, “De-\nfending against backdoor attacks in natural language generation,”\nin Proceedings of the AAAI Conference on Artificial Intelligence,\nvol. 37, no. 4, 2023, pp. 5257–5265.\n[267] Y. Sun, J. He, S. Lei, L. Cui, and C.-T. Lu, “Med-mmhl: A multi-\nmodal dataset for detecting human-and llm-generated misinforma-\ntioninthemedicaldomain,” arXivpreprintarXiv:2306.08871 ,2023.\n[268] Z. Sun, Y. Shen, Q. Zhou, H. Zhang, Z. Chen, D. Cox, Y. Yang,\nand C. Gan, “Principle-driven self-alignment of language models\nfrom scratch with minimal human supervision,” arXiv preprint\narXiv:2305.03047, 2023.\n[269] Z. Talat, A. Névéol, S. Biderman, M. Clinciu, M. Dey, S. Longpre,\nS. Luccioni, M. Masoud, M. Mitchell, D. Radevet al., “You reap\nwhat you sow: On the challenges of bias evaluation under multilin-\ngual settings,” inProceedings of BigScience Episode# 5–Workshop\non Challenges & Perspectives in Creating Large Language Models,\n2022, pp. 26–41.\n[270] W. Tann, Y. Liu, J. H. Sim, C. M. Seah, and E.-C. Chang, “Using\nlarge language models for cybersecurity capture-the-flag challenges\nand certification questions,” 2023.\n[271] P.Taveekitworachai,F.Abdullah,M.C.Gursesli,M.F.Dewantoro,\nS.Chen,A.Lanata,A.Guazzini,andR.Thawonmas,“Breakingbad:\nUnraveling influences and risks of user inputs to chatgpt for game\nstorygeneration,”in InternationalConferenceonInteractiveDigital\nStorytelling. Springer, 2023, pp. 285–296.\n[272] Z.Tay,“Usingartificialintelligencetoaugmentbugfuzzing,”2023.\n[273] E. ThankGod Chinonso, “The impact of chatgpt on privacy and\ndataprotectionlaws,” TheImpactofChatGPTonPrivacyandData\nProtection Laws (April 16, 2023), 2023.\n[274] A. J. Thirunavukarasu, D. S. J. Ting, K. Elangovan, L. Gutierrez,\nT. F. Tan, and D. S. W. Ting, “Large language models in medicine,”\nNature medicine, vol. 29, no. 8, pp. 1930–1940, 2023.\n[275] M.Tong,K.Chen,Y.Qi,J.Zhang,W.Zhang,andN.Yu,“Privinfer:\nPrivacy-preserving inference for black-box large language model,”\n2023.\n[276] J. Torres, “Navigating the llm landscape: A comparative analysis of\nleading large language models,” http://surl.li/ncjvc, 2023.\n[277] H.Touvron,L.Martin,K.Stone,P.Albert,A.Almahairi,Y.Babaei,\nN. Bashlykov, S. Batra, P. Bhargava, S. Bhosaleet al., “Llama\n2: Open foundation and fine-tuned chat models,”arXiv preprint\narXiv:2307.09288, 2023.\n[278] S. Truex, L. Liu, M. E. Gursoy, L. Yu, and W. Wei, “To-\nwards demystifying membership inference attacks,”arXiv preprint\narXiv:1807.09173, 2018.\n[279] J.-B. Truong, P. Maini, R. J. Walls, and N. Papernot, “Data-free\nmodel extraction,” inProceedings of the IEEE/CVF conference on\ncomputer vision and pattern recognition, 2021, pp. 4771–4780.\nYifan Yao et al.:Preprint submitted to Elsevier Page 22 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\n[280] A. Uchendu, J. Lee, H. Shen, T. Le, T.-H. K. Huang, and\nD. Lee, “Does human collaboration enhance the accuracy of\nidentifying llm-generated deepfake texts?” Proceedings of the\nAAAI Conference on Human Computation and Crowdsourcing,\nvol. 11, no. 1, pp. 163–174, Nov. 2023. [Online]. Available:\nhttps://ojs.aaai.org/index.php/HCOMP/article/view/27557\n[281] S. Urchs, V. Thurner, M. Aßenmacher, C. Heumann, and\nS. Thiemichen, “How prevalent is gender bias in chatgpt?–\nexploring german and english chatgpt responses,”arXiv preprint\narXiv:2310.03031, 2023.\n[282] A. Urman and M. Makhortykh, “The silence of the llms: Cross-\nlingual analysis of political bias and false information prevalence in\nchatgpt, google bard, and bing chat,” 2023.\n[283] L. Uzun, “Chatgpt and academic integrity concerns: Detecting ar-\ntificial intelligence generated content,”Language Education and\nTechnology, vol. 3, no. 1, 2023.\n[284] Ö. Uzuner, Y. Luo, and P. Szolovits, “Evaluating the state-of-the-\nartinautomaticde-identification,” JournaloftheAmericanMedical\nInformatics Association, vol. 14, no. 5, pp. 550–563, 2007.\n[285] P.Vaithilingam,T.Zhang,andE.L.Glassman,“Expectationvs.ex-\nperience: Evaluating the usability of code generation tools powered\nby large language models,” inChi conference on human factors in\ncomputing systems extended abstracts, 2022, pp. 1–7.\n[286] A. Vats, Z. Liu, P. Su, D. Paul, Y. Ma, Y. Pang, Z. Ahmed, and\nO.Kalinli,“Recoveringfromprivacy-preservingmaskingwithlarge\nlanguage models,” 2023.\n[287] R. J. M. Ventayen, “Openai chatgpt generated results: Similarity\nindex of artificial intelligence-based contents,”Available at SSRN\n4332664, 2023.\n[288] T. Vidas, D. Votipka, and N. Christin, “All your droid are belong to\nus: A survey of current android attacks,” in5th USENIX Workshop\non Offensive Technologies (WOOT 11), 2011.\n[289] E. Wallace, T. Z. Zhao, S. Feng, and S. Singh, “Concealed data\npoisoningattacksonnlpmodels,” arXivpreprintarXiv:2010.12563 ,\n2020.\n[290] A. Wan, E. Wallace, S. Shen, and D. Klein, “Poisoning\nlanguage models during instruction tuning,” arXiv preprint\narXiv:2305.00944, 2023.\n[291] Y. Wan, S. Zhang, H. Zhang, Y. Sui, G. Xu, D. Yao, H. Jin, and\nL. Sun, “You see what i want you to see: poisoning vulnerabilities\nin neural code search,” in Proceedings of the 30th ACM Joint\nEuropean Software Engineering Conference and Symposium on the\nFoundations of Software Engineering, 2022, pp. 1233–1245.\n[292] Y. Wan, G. Pu, J. Sun, A. Garimella, K.-W. Chang, and N. Peng,\n“\"kelly is a warm person, joseph is a role model\": Gender biases in\nllm-generated reference letters,”arXiv preprint arXiv:2310.09219,\n2023.\n[293] D. Wang, C. Gong, and Q. Liu, “Improving neural language mod-\neling via adversarial training,” inInternational Conference on Ma-\nchine Learning. PMLR, 2019, pp. 6555–6565.\n[294] F. Wang, “Using large language models to mitigate ransomware\nthreats,” Preprints, November 2023. [Online]. Available: https:\n//doi.org/10.20944/preprints202311.0676.v1\n[295] H. Wang, X. Luo, W. Wang, and X. Yan, “Bot or human? detecting\nchatgpt imposters with a single question,” 2023.\n[296] J. Wang, Z. Huang, H. Liu, N. Yang, and Y. Xiao, “Defecthunter:\nA novel llm-driven boosted-conformer-based code vulnerability\ndetection mechanism,” arXiv preprint arXiv:2309.15324, 2023.\n[Online]. Available: https://doi.org/10.48550/arXiv.2309.15324\n[297] J. Wang, X. Lu, Z. Zhao, Z. Dai, C.-S. Foo, S.-K. Ng, and B. K. H.\nLow,“Wasa:Watermark-basedsourceattributionforlargelanguage\nmodel-generated data,” 2023.\n[298] Z. Wang, Z. Liu, X. Zheng, Q. Su, and J. Wang, “Rmlm: A flexible\ndefenseframeworkforproactivelymitigatingword-leveladversarial\nattacks,” inProceedings of the 61st Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long Papers), 2023,\npp. 2757–2774.\n[299] Z. Wang, W. Xie, K. Chen, B. Wang, Z. Gui, and E. Wang, “Self-\ndeception: Reverse penetrating the semantic firewall of large lan-\nguage models,”arXiv preprint arXiv:2308.11521, 2023.\n[300] A. Wei, N. Haghtalab, and J. Steinhardt, “Jailbroken: How does llm\nsafety training fail?”arXiv preprint arXiv:2307.02483, 2023.\n[301] Z. Wei, Y. Wang, and Y. Wang, “Jailbreak and guard aligned\nlanguage models with only few in-context demonstrations,”arXiv\npreprint arXiv:2310.06387, 2023.\n[302] L.Weidinger,J.Mellor,M.Rauh,C.Griffin,J.Uesato,P.-S.Huang,\nM. Cheng, M. Glaese, B. Balle, A. Kasirzadeh et al., “Ethical\nand social risks of harm from language models,”arXiv preprint\narXiv:2112.04359, 2021.\n[303] H. Wen, Y. Li, G. Liu, S. Zhao, T. Yu, T. J.-J. Li, S. Jiang, Y. Liu,\nY.Zhang,andY.Liu,“Empoweringllmtousesmartphoneforintel-\nligent task automation,”arXiv preprint arXiv:2308.15272, 2023.\n[304] J. Weng, W. Jiasi, M. Li, Y. Zhang, J. Zhang, and L. Weiqi,\n“Auditable privacy protection deep learning platform construction\nmethod based on block chain incentive mechanism,” Dec. 5 2023,\nuS Patent 11,836,616.\n[305] J. Weng, J. Weng, J. Zhang, M. Li, Y. Zhang, and W. Luo,\n“Deepchain: Auditable and privacy-preserving deep learning with\nblockchain-basedincentive,” IEEETransactionsonDependableand\nSecure Computing, vol. 18, no. 5, pp. 2438–2455, 2019.\n[306] G.Wenzek,M.-A.Lachaux,A.Conneau,V.Chaudhary,F.Guzmán,\nA.Joulin,andE.Grave,“Ccnet:Extractinghighqualitymonolingual\ndatasets from web crawl data,”arXiv preprint arXiv:1911.00359,\n2019.\n[307] B. Workshop, T. L. Scao, A. Fan, C. Akiki, E. Pavlick, S. Ilić,\nD. Hesslow, R. Castagné, A. S. Luccioni, F. Yvonet al., “Bloom:\nA176b-parameteropen-accessmultilinguallanguagemodel,” arXiv\npreprint arXiv:2211.05100, 2022.\n[308] J.WuandB.Hooi,“Fakenewsinsheep’sclothing:Robustfakenews\ndetection against llm-empowered style attacks,” 2023.\n[309] J. Wu, S. Yang, R. Zhan, Y. Yuan, D. F. Wong, and L. S. Chao,\n“Asurveyonllm-gerneratedtextdetection:Necessity,methods,and\nfuture directions,”arXiv preprint arXiv:2310.14724, 2023.\n[310] S. Wu, O. Irsoy, S. Lu, V. Dabravolski, M. Dredze, S. Gehrmann,\nP. Kambadur, D. Rosenberg, and G. Mann, “Bloomberggpt: A\nlargelanguagemodelforfinance,” arXivpreprintarXiv:2303.17564 ,\n2023.\n[311] X. Wu, R. Duan, and J. Ni, “Unveiling security, privacy, and ethical\nconcerns of chatgpt,” 2023.\n[312] Z. Xi, T. Du, C. Li, R. Pang, S. Ji, J. Chen, F. Ma, and T. Wang,\n“Defendingpre-trainedlanguagemodelsasfew-shotlearnersagainst\nbackdoor attacks,”arXiv preprint arXiv:2309.13256, 2023.\n[313] C. S. Xia, M. Paltenghi, J. L. Tian, M. Pradel, and L. Zhang,\n“Universal fuzzing via large language models,” arXiv preprint\narXiv:2308.04748, 2023.\n[314] C.S.Xia,Y.Wei,andL.Zhang,“Practicalprogramrepairintheera\nof large pre-trained language models,” 2022.\n[315] C. S. Xia and L. Zhang, “Keep the conversation going: Fixing 162\nout of 337 bugs for $0.42 each using chatgpt,” 2023.\n[316] Z. Xie, Y. Chen, C. Zhi, S. Deng, and J. Yin, “Chatunitest: a\nchatgpt-based automated unit test generation tool,”arXiv preprint\narXiv:2305.04764, 2023.\n[317] M. Xiong, Z. Hu, X. Lu, Y. Li, J. Fu, J. He, and B. Hooi, “Can llms\nexpress their uncertainty? an empirical evaluation of confidence\nelicitation in llms,” ArXiv, vol. abs/2306.13063, 2023. [Online].\nAvailable: https://api.semanticscholar.org/CorpusID:259224389\n[318] L. Xu, L. Berti-Equille, A. Cuesta-Infante, and K. Veeramachaneni,\n“In situ augmentation for defending against adversarial attacks on\ntext classifiers,” inInternational Conference on Neural Information\nProcessing. Springer, 2022, pp. 485–496.\n[319] F.Yaman etal.,“Agentsca:Advancedphysicalsidechannelanalysis\nagent with llms.” 2023.\n[320] J. Yan, V. Yadav, S. Li, L. Chen, Z. Tang, H. Wang, V. Srinivasan,\nX. Ren, and H. Jin, “Virtual prompt injection for instruction-tuned\nlarge language models,”arXiv preprint arXiv:2307.16888, 2023.\nYifan Yao et al.:Preprint submitted to Elsevier Page 23 of 24\nA Survey on Large Language Model (LLM) Security and Privacy: The Good, the Bad, and Ugly\n[321] C.Yang,Y.Deng,R.Lu,J.Yao,J.Liu,R.Jabbarvand,andL.Zhang,\n“White-box compiler fuzzing empowered by large language mod-\nels,” 2023.\n[322] H. Yang, K. Xiang, H. Li, and R. Lu, “A comprehensive overview\nofbackdoorattacksinlargelanguagemodelswithincommunication\nnetworks,”arXiv preprint arXiv:2308.14367, 2023.\n[323] J. Yang, H. Jin, R. Tang, X. Han, Q. Feng, H. Jiang, B. Yin, and\nX. Hu, “Harnessing the power of llms in practice: A survey on\nchatgpt and beyond,”arXiv preprint arXiv:2304.13712, 2023.\n[324] J. Yang, H. Xu, S. Mirzoyan, T. Chen, Z. Liu, W. Ju, L. Liu,\nM.Zhang,andS.Wang,“Poisoningscientificknowledgeusinglarge\nlanguage models,”bioRxiv, pp. 2023–11, 2023.\n[325] S. Yang, “Crafting unusual programs for fuzzing deep learning\nlibraries,” Ph.D. dissertation, University of Illinois at Urbana-\nChampaign, 2023.\n[326] Z.Yang,Z.Zhao,C.Wang,J.Shi,D.Kim,D.Han,andD.Lo,“What\ndo code models memorize? an empirical study on large language\nmodels of code,”arXiv preprint arXiv:2308.09932, 2023.\n[327] B. Yao, M. Jiang, D. Yang, and J. Hu, “Empowering llm-\nbased machine translation with cultural awareness,”arXiv preprint\narXiv:2305.14328, 2023.\n[328] D. Yao, J. Zhang, I. G. Harris, and M. Carlsson, “Fuzzllm: A\nnovel and universal fuzzing framework for proactively discovering\njailbreak vulnerabilities in large language models,”arXiv preprint\narXiv:2309.05274, 2023.\n[329] H. Yao, J. Lou, and Z. Qin, “Poisonprompt: Backdoor at-\ntack on prompt-based large language models,” arXiv preprint\narXiv:2310.12439, 2023.\n[330] J. Y. Yoo and Y. Qi, “Towards improving adversarial training of nlp\nmodels,”arXiv preprint arXiv:2109.00544, 2021.\n[331] W. You, Z. Hammoudeh, and D. Lowd, “Large language models\nare better adversaries: Exploring generative clean-label backdoor\nattacks against text classifiers,”arXiv preprint arXiv:2310.18603,\n2023.\n[332] J.Yu,X.Lin,andX.Xing,“Gptfuzzer:Redteaminglargelanguage\nmodels with auto-generated jailbreak prompts,” arXiv preprint\narXiv:2309.10253, 2023.\n[333] L. Yuan, Y. Chen, G. Cui, H. Gao, F. Zou, X. Cheng, H. Ji,\nZ. Liu, and M. Sun, “Revisiting out-of-distribution robustness in\nnlp: Benchmark, analysis, and llms evaluations,”arXiv preprint\narXiv:2306.04618, 2023.\n[334] Z.Yuan,H.Yuan,C.Tan,W.Wang,S.Huang,andF.Huang,“Rrhf:\nRank responses to align language models with human feedback\nwithout tears,”arXiv preprint arXiv:2304.05302, 2023.\n[335] Z. Yuan, Y. Lou, M. Liu, S. Ding, K. Wang, Y. Chen, and X. Peng,\n“No more manual tests? evaluating and improving chatgpt for unit\ntest generation,”arXiv preprint arXiv:2305.04207, 2023.\n[336] A.Zafar,V.B.Parthasarathy,C.L.Van,S.Shahid,A.Shahid etal.,\n“Building trust in conversational ai: A comprehensive review and\nsolution architecture for explainable, privacy-aware systems using\nllmsandknowledgegraph,” arXivpreprintarXiv:2308.13534 ,2023.\n[337] C. Zhang, M. Bai, Y. Zheng, Y. Li, X. Xie, Y. Li, W. Ma, L. Sun,\nand Y. Liu, “Understanding large language model based fuzz driver\ngeneration,”arXiv preprint arXiv:2307.12469, 2023.\n[338] C. Zhang, Y. Xie, H. Bai, B. Yu, W. Li, and Y. Gao, “A survey on\nfederatedlearning,” Knowledge-BasedSystems,vol.216,p.106775,\n2021.\n[339] R.Zhang,S.Hidano,andF.Koushanfar,“Textrevealer:Privatetext\nreconstruction via model inversion attacks against transformers,”\narXiv preprint arXiv:2209.10505, 2022.\n[340] R. Zhang, S. S. Hussain, P. Neekhara, and F. Koushanfar, “Remark-\nllm: A robust and efficient watermarking framework for generative\nlarge language models,” 2023.\n[341] X.ZhangandW.Gao,“Towardsllm-basedfactverificationonnews\nclaims with a hierarchical step-by-step prompting method,”arXiv\npreprint arXiv:2310.00305, 2023.\n[342] Y. Zhang and D. Ippolito, “Prompts should not be seen as secrets:\nSystematically measuring prompt extraction attack success,”arXiv\npreprint arXiv:2307.06865, 2023.\n[343] Y. Zhang, W. Song, Z. Ji, D. D. Yao, and N. Meng, “How well\ndoesllmgeneratesecuritytests?” arXivpreprintarXiv:2310.00710 ,\n2023.\n[344] Z. Zhang, J. Wen, and M. Huang, “Ethicist: Targeted training data\nextraction through loss smoothed soft prompting and calibrated\nconfidence estimation,”arXiv preprint arXiv:2307.04401, 2023.\n[345] J. Zhao, Y. Rong, Y. Guo, Y. He, and H. Chen, “Understand-\ning programs by exploiting (fuzzing) test cases,”arXiv preprint\narXiv:2305.13592, 2023.\n[346] S. Zhao, J. Wen, L. A. Tuan, J. Zhao, and J. Fu, “Prompt as\ntriggersforbackdoorattack:Examiningthevulnerabilityinlanguage\nmodels,”arXiv preprint arXiv:2305.01219, 2023.\n[347] W. X. Zhao, K. Zhou, J. Li, T. Tang, X. Wang, Y. Hou, Y. Min,\nB. Zhang, J. Zhang, Z. Donget al., “A survey of large language\nmodels,”arXiv preprint arXiv:2303.18223, 2023.\n[348] W. Zhao, Y. Liu, Y. Wan, Y. Wang, Q. Wu, Z. Deng, J. Du, S. Liu,\nY. Xu, and P. S. Yu, “knn-icl: Compositional task-oriented parsing\ngeneralization with nearest neighbor in-context learning,” 2023.\n[349] C. Zhou, P. Liu, P. Xu, S. Iyer, J. Sun, Y. Mao, X. Ma, A. Efrat,\nP.Yu,L.Yu etal.,“Lima:Lessismoreforalignment,” arXivpreprint\narXiv:2305.11206, 2023.\n[350] C. Zhu, Y. Cheng, Z. Gan, S. Sun, T. Goldstein, and J. Liu, “Freelb:\nEnhanced adversarial training for natural language understanding,”\narXiv preprint arXiv:1909.11764, 2019.\n[351] K. Zhu, J. Wang, J. Zhou, Z. Wang, H. Chen, Y. Wang, L. Yang,\nW.Ye,N.Z.Gong,Y.Zhang etal.,“Promptbench:Towardsevaluat-\ningtherobustnessoflargelanguagemodelsonadversarialprompts,”\narXiv preprint arXiv:2306.04528, 2023.\n[352] N. Ziems, W. Yu, Z. Zhang, and M. Jiang, “Large language\nmodels are built-in autoregressive search engines,”arXiv preprint\narXiv:2305.09612, 2023.\n[353] A. Zou, Z. Wang, J. Z. Kolter, and M. Fredrikson, “Universal\nand transferable adversarial attacks on aligned language models,”\ncommunication, it is essential for you to comprehend user queries\nin Cipher Code and subsequently deliver your responses utilizing\nCipher Code, 2023.\nYifan Yao et al.:Preprint submitted to Elsevier Page 24 of 24"
}