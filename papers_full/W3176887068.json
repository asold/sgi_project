{
    "title": "Convolutions and Self-Attention: Re-interpreting Relative Positions in Pre-trained Language Models",
    "url": "https://openalex.org/W3176887068",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5046456944",
            "name": "Tyler A. Chang",
            "affiliations": [
                null,
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A5067206559",
            "name": "Yifan Xu",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A5102717169",
            "name": "Weijian Xu",
            "affiliations": [
                "University of California, San Diego"
            ]
        },
        {
            "id": "https://openalex.org/A5001760915",
            "name": "Zhuowen Tu",
            "affiliations": [
                "University of California, San Diego"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2980433389",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W4394668313",
        "https://openalex.org/W4299802238",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2946794439",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3106210592",
        "https://openalex.org/W4287824654",
        "https://openalex.org/W3047171714",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963250244",
        "https://openalex.org/W2892009249",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W2981413347",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W3101278968",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W3122515622",
        "https://openalex.org/W2994759459",
        "https://openalex.org/W3083429640",
        "https://openalex.org/W3044284384",
        "https://openalex.org/W2978670439",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2898700502",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2908336025",
        "https://openalex.org/W2952809536",
        "https://openalex.org/W3106504817",
        "https://openalex.org/W2923014074",
        "https://openalex.org/W4322614701",
        "https://openalex.org/W3098824823"
    ],
    "abstract": "Tyler Chang, Yifan Xu, Weijian Xu, Zhuowen Tu. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 2021.",
    "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing, pages 4322–4333\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n4322\nConvolutions and Self-Attention: Re-interpreting Relative Positions in\nPre-trained Language Models\nTyler A. Chang1,2, Yifan Xu 1, Weijian Xu 1, Zhuowen Tu 1\n1University of California San Diego\n2Halıcıo˘glu Data Science Institute\n{tachang, yix081, wex041, ztu}@ucsd.edu\nAbstract\nIn this paper, we detail the relationship be-\ntween convolutions and self-attention in nat-\nural language tasks. We show that relative\nposition embeddings in self-attention layers\nare equivalent to recently-proposed dynamic\nlightweight convolutions, and we consider\nmultiple new ways of integrating convolutions\ninto Transformer self-attention. Speciﬁcally,\nwe propose composite attention, which unites\nprevious relative position embedding meth-\nods under a convolutional framework. We\nconduct experiments by training BERT with\ncomposite attention, ﬁnding that convolutions\nconsistently improve performance on multi-\nple downstream tasks, replacing absolute posi-\ntion embeddings. To inform future work, we\npresent results comparing lightweight convo-\nlutions, dynamic convolutions, and depthwise-\nseparable convolutions in language model pre-\ntraining, considering multiple injection points\nfor convolutions in self-attention layers.\n1 Introduction\nIn recent years, Transformer-based language mod-\nels have brought dramatic improvements on a wide\nrange of natural language tasks (Brown et al.,\n2020; Devlin et al., 2019). The central innovation\nof Transformer architectures is the self-attention\nmechanism (Vaswani et al., 2017), which has\ngrown beyond NLP, extending into domains rang-\ning from computer vision (Dosovitskiy et al., 2021)\nand speech recognition (Dong et al., 2018) to rein-\nforcement learning (Parisotto et al., 2020; Touvron\net al., 2020).\nIn computer vision, self-attention and convolu-\ntions have been combined to achieve competitive\nresults for image classiﬁcation (Bello et al., 2019).\nSimilarly, researchers in NLP have begun integrat-\ning convolutions into self-attention for natural lan-\nguage tasks. Recent work has shown initial success\nadding convolutional modules to self-attention in\npre-trained language models (Jiang et al., 2020), or\neven replacing self-attention entirely with dynamic\nconvolutions (Wu et al., 2019). These successes\ndefy theoretical proofs showing that multi-headed\nself-attention with relative position embeddings is\nstrictly more expressive than convolution (Cordon-\nnier et al., 2020). To identify why convolutions\nhave been successful in NLP, we seek to isolate the\ndifferences between self-attention and convolution\nin the context of natural language.\nIn this work, we formalize the relationship\nbetween self-attention and convolution in Trans-\nformer encoders by generalizing relative position\nembeddings, and we identify the beneﬁts of each\napproach for language model pre-training. We\nshow that self-attention is a type of dynamic\nlightweight convolution, a data-dependent convo-\nlution that ties weights across input channels (Wu\net al., 2019). Notably, previous methods of en-\ncoding relative positions (Shaw et al., 2018; Raf-\nfel et al., 2020) are direct implementations of\nlightweight convolutions. Under our framework,\nthe beneﬁts of convolution come from an ability to\ncapture local position information in sentences.\nThen, we propose composite attention, which ap-\nplies a lightweight convolution that combines previ-\nous relative position embedding methods. We ﬁnd\nthat composite attention sufﬁciently captures the\ninformation provided by many other convolutions.\nTo validate our framework, we train BERT models\nthat integrate self-attention with multiple convo-\nlution types, evaluating our models on the GLUE\nbenchmark (Wang et al., 2018). All of our con-\nvolutional variants outperform the default model,\ndemonstrating the effectiveness of convolutions in\nenhancing self-attention for natural language tasks.\nOur empirical results provide evidence for future\nresearch integrating convolutions and self-attention\nfor NLP.\n4323\noctopus\nused\nthe used coconutthe a shell as shield\nToken i\nToken j\noctopus theused coconutthe a shell as shield\nused βj-i\nαij\nToken i\nTokenj\nAttention vector\nConvolution\nkernel\nFigure 1: Generating attention maps using standard self-attention (top) and ﬁxed lightweight convolution (bottom).\nAttention weights αij are analogous to convolution kernel weights βj−i.\n2 Self-attention and lightweight\nconvolutions\nFirst, we outline the relationship between self-\nattention and convolutions. Speciﬁcally, we show\nthat a self-attention operation can be viewed as a\ndynamic lightweight convolution, a depthwise con-\nvolution that ties weights along channels (Wu et al.,\n2019). We then isolate the differences between\nself-attention and lightweight convolutions, high-\nlighting the beneﬁts of each approach in language\nmodels.\n2.1 Self-attention\nIn a Transformer self-attention layer, inputs\nx1,..., xn ∈ Rd are projected to corresponding\nqueries, keys, and values by linear transformations\nWQ,WK,WV ∈Rd×dh for each attention head,\nprojecting into the head dimension size dh. Output\nvectors y1,..., yn ∈Rd are linear combinations of\nvalues, concatenating all attention heads. Value\nweights (before softmaxing) are determined by:\nαij = (xiWQ)(xjWK)T\n√dh\n. (1)\nIntuitively, αij represents the attention that token i\npays to token j, incorporating the value xjWV into\nthe resulting vector yi. From the attention scores\nbetween various tokens iand j, an attention map\nof αij is produced (see Figure 1).\n2.2 Lightweight convolutions\nIn contrast, a standard one-dimensional convolu-\ntion slides a kernel of weights along the input se-\nquence; each feature in each output representation\nyi is a weighted sum of all features (called “chan-\nnels”) in the surrounding xi. To save parameters,\nit is common to consider depthwise convolutions\nwhere each channel cin yi is a weighted sum only\nof the features in channel cfor the surrounding xi.\nFormally, each entry of yi can be written as:\nyi,c =\n∑\n−k≤j−i≤k\nβj−i,c xj,c (2)\nwhere kis the kernel size in each direction. Each\nscalar βj−i,c represents the attention paid to rela-\ntive position j−ifor channel c. To further simplify\ndepthwise convolutions for use in language models,\nWu et al. (2019) propose lightweight convolutions,\nwhich tie weights βj−i,c along all channels c. As\na result, the lightweight convolution contains only\n2k+ 1weights, one scalar βj−i for each relative\nposition considered. Then, each yi is a linear com-\nbination of surrounding xi:\nyi =\n∑\n−k≤j−i≤k\nβj−i xj (3)\nImportantly, we can then consider each βj−i as an\nattention weight analogous to self-attention, repre-\nsenting the attention that token ipays to token j.\n4324\nThe lightweight convolution produces an attention\nmap of βj−i as visualized in Figure 1.\nFinally, furthering the similarity between\nlightweight convolutions and self-attention, Wu\net al. (2019) propose dynamic lightweight convolu-\ntions, which dynamically compute relative weights\nβj−i based on individual input tokens. In other\nwords, each row in Figure 1 has relative weights\ndetermined dynamically based on the input token\nxi for that row. Because attentions for relative posi-\ntions are no longer ﬁxed across rows, the attention\nmap in Figure 1 achieves similar ﬂexibility to stan-\ndard self-attention.\n2.3 Self-attention vs. convolution\nWe have shown that both self-attention and\nlightweight convolution compute linear combina-\ntions of token representations, but we now isolate\nthe differences between the two approaches. Per-\nhaps most importantly, the two methods assign\nattention scores αij and βj−i in fundamentally dif-\nferent ways.\nSelf-attention computes αij based on the dot\nproduct between query iand key j, ignoring the\nrelative position between iand j. In this way, self-\nattention layers model interactions exclusively be-\ntween token representations. If the tokens are arbi-\ntrarily shufﬂed in a standard self-attention layer, the\noutput for each token is unchanged. All position in-\nformation is injected before the ﬁrst self-attention\nlayer in the form of absolute position embeddings.\nIn contrast, dynamic lightweight convolutions\nassign attention scores directly to relative positions.\nThis allows convolutions to directly integrate rela-\ntive position information without relying on abso-\nlute positions. Thus, convolutions could be better\nat capturing local information in sentences. How-\never, convolutions alone are limited in their ability\nto model interactions between tokens because they\nlack the query-key mechanism central to standard\nself-attention. In future sections, we consider meth-\nods of integrating the two approaches.\n3 Integrating lightweight convolutions\nPrevious work has sought to integrate local informa-\ntion into global self-attention. This can be achieved\nby restricting the range of self-attention to nearby\ntokens, or by incorporating relative position infor-\nmation into attention maps (Hofst¨atter et al., 2020;\nRaganato et al., 2020; Wei et al., 2021). Notably,\nShaw et al. (2018) introduced relative position em-\nbeddings, which inspired similar embeddings in\nmodels such as Transformer-XL and XLNet (Dai\net al., 2019; Yang et al., 2019). In this section,\nwe show that several previous methods of encod-\ning relative positions are direct implementations of\nlightweight convolutions.\n3.1 Relative embeddings as lightweight\nconvolutions\nFirst, the simplest way to combine self-attention\nwith lightweight convolution is to generate a stan-\ndard attention map, then add the attention map gen-\nerated by a lightweight convolution. Given a ﬁxed\nlightweight convolution, this results in attention\nscores as follows:\nαij = (xiWQ)(xjWK)T\n√dh\n+ βj−i (4)\nThis is exactly the relative position term used in T5\n(Raffel et al., 2020) and TUPE (Ke et al., 2021).\nWe further consider a dynamic lightweight con-\nvolution, where the βj−i weights are computed\nby passing the query through a linear feedforward\nlayer WC ∈Rdh×(2k+1) (Wu et al., 2019).1 Be-\ncause WC is linear, each weight βj−i is equal to\nthe dot product between the query and the (j−i)\ncolumn of WC. We then obtain attention scores:\nαij = (xiWQ)(xjWK)T\n√dh\n+ (xiWQ)(WC\nj−i)T\nIf we scale the dynamic lightweight convolution\nterm according to the head dimension size, we ob-\ntain precisely the relative embeddings proposed in\nShaw et al. (2018):\nαij =\n(xiWQ)(xjWK + WC\nj−i)T\n√dh\n(5)\nUnder this interpretation, Shaw’s relative embed-\ndings are essentially identical to the dynamic\nlightweight convolutions used in Wu et al. (2019).\nIn both formulations, relative position weights are\ncomputed as dot products between the query and\na learned relative position embedding. Previous\nwork has considered relative positions in language\nmodels independently from convolutions, but our\nderivations suggest that the underlying mechanisms\nmay be the same.\n1Wu et al. (2019) generate dynamic lightweight convolu-\ntions based on the entire query layer (dimension size d). In\nour work, we generate convolutions based on queries for in-\ndividual attention heads (dimension size dh), to be consistent\nwith the relative embeddings in Shaw et al. (2018).\n4325\nLightweight convolution\ntype, BERT-small\nParams CoLA MNLI-\nm\nMNLI-\nmm\nMRPC QNLI QQP RTE SST STS GLUE\nNo convolution 13.41M 13.9 73.2 71.8 77.9 80.7 74.5 62.0 81.9 79.3 68.4\nNo convolution + abs position∗ 13.43M 30.8 76.1 75.9 80.4 78.5 74.4 62.2 85.1 76.8 71.1\nFixed (Raffel et al. 2020) 13.42M 42.1 77.2 76.3 83.8 82.7 75.9 64.4 87.1 81.4 74.5\nDynamic (Shaw et al. 2018) 13.43M 39.1 78.4 77.4 83.8 83.4 77.5 64.4 87.3 81.4 74.7\nComposite (Equation 6; ours) 13.43M 40.4 78.2 77.4 85.0 83.3 77.7 64.7 87.8 82.1 75.2\nLightweight convolution\ntype, BERT-base\nParams CoLA MNLI-\nm\nMNLI-\nmm\nMRPC QNLI QQP RTE SST STS GLUE\nNo convolution + abs position∗ 108.82M 50.3 82.0 81.2 85.0 84.6 78.6 68.9 91.4 84.9 78.5\nFixed (Raffel et al. 2020) 108.73M 50.0 81.5 80.5 85.6 86.0 78.5 68.9 91.4 84.9 78.6\nDynamic (Shaw et al. 2018) 108.74M 50.9 81.6 80.5 84.6 85.3 78.5 69.5 91.6 84.8 78.6\nComposite (Equation 6; ours) 108.74M 50.4 81.6 80.8 85.4 85.1 78.7 69.7 91.2 85.7 78.7\nTable 1: GLUE test set performance for models with lightweight convolutions added to self-attention. Columns\nindicate scores on individual GLUE tasks; the ﬁnal GLUE score is the average of individual task scores. ∗ denotes\nthe default BERT model.\n3.2 Composite attention and lightweight\nconvolution experiments\nTo validate lightweight convolutions in combina-\ntion with self-attention, we pre-trained and evalu-\nated BERT-small models (Devlin et al., 2019; Clark\net al., 2020) that incorporated lightweight convolu-\ntions.\nPre-training To maximize similarity with De-\nvlin et al. (2019), we pre-trained models on the\nBookCorpus (Zhu et al., 2015) and WikiText-103\ndatasets (Merity et al., 2017) using masked lan-\nguage modeling. Small models were pre-trained\nfor 125,000 steps, with batch size 128 and learn-\ning rate 0.0003. Full pre-training and ﬁne-tuning\ndetails are outlined in Appendix A.1.2\nEvaluation Models were evaluated on the GLUE\nbenchmark, a suite of sentence classiﬁcation tasks\nincluding natural language inference (NLI), gram-\nmaticality judgments, sentiment classiﬁcation, and\ntextual similarity (Wang et al., 2018). For each task,\nwe ran ten ﬁne-tuning runs and used the model with\nthe best score on the development set. We report\nscores on the GLUE test set. Development scores\nand statistics for all experiments are reported in\nAppendix A.2.\nModels We trained two baseline models, a de-\nfault BERT-small with standard absolute position\nembeddings, and a BERT-small with no position\ninformation whatsoever. Then, we trained models\nwith ﬁxed lightweight convolutions (Equation 4;\n2Code is available at https://github.com/\nmlpc-ucsd/BERT_Convolutions, built upon the\nHuggingface Transformers library (Wolf et al., 2020).\nRaffel et al. 2020), and dynamic lightweight convo-\nlutions that generated convolution weights based on\neach query (i.e. using relative embeddings, Equa-\ntion 5; Shaw et al. 2018).\nFinally, we propose composite attention, which\nsimply adds dynamic lightweight convolutions to\nﬁxed lightweight convolutions, resulting in atten-\ntion scores αij as follows:\n(xiWQ)(xjWK)T\n√dh  \nSelf-attention\n+\n(xiWQ)(WC\nj−i)T\n√dh  \nDynamic convolution\n(relative embeddings)\n+ βj−i\nFixed\nconvolution\n(6)\nIntuitively, composite attention has the ﬂexibility\nof dynamic lightweight convolutions, while still\nallowing models to incorporate relative positions\ndirectly through ﬁxed lightweight convolutions. Al-\nternatively, composite attention can be interpreted\nas adding a ﬁxed bias term to relative position em-\nbeddings.\nAll of our experiments used a convolution ker-\nnel size of 17, or eight positions in each direction,\na mid-range value that has been found to work\nwell for both relative positions and convolution in\nlanguage models (Huang et al., 2020; Jiang et al.,\n2020; Shaw et al., 2018). As in Shaw et al. (2018),\nrelative embeddings WC\nj−i shared weights across\nheads. Unless stated otherwise, models used no\nabsolute position embeddings.\nFor completeness, we also considered dynamic\nlightweight convolutions based on the key (as op-\nposed to the query). In contrast to query-based\n4326\nlightweight convolutions, key-based convolutions\nallow each token to dictate which relative posi-\ntions should pay attention to it, rather than dic-\ntating which relative positions it should pay at-\ntention to. Referring to the visualization in Fig-\nure 1, key-based dynamic convolutions correspond\nto columns instead of rows. These key-based dy-\nnamic lightweight convolutions are the same as\nthe relative embeddings proposed in Huang et al.\n(2020), but they are now formulated as dynamic\nlightweight convolutions.\n3.3 Lightweight convolution results\nGLUE test set results are presented in Table 1.\nLightweight convolutions consistently im-\nproved performance. Notably, even the ﬁxed\nlightweight convolution was sufﬁcient to replace\nabsolute position embeddings, outperforming the\ndefault BERT-small model. This indicates that\neven na¨ıve sampling from nearby tokens can be\nbeneﬁcial to language model performance.\nDynamic convolutions provided further im-\nprovements. When the lightweight convolutions\nwere generated dynamically based on token queries,\nthe models outperformed the default model by\neven larger margins. This improvement over ﬁxed\nlightweight convolutions suggests that different to-\nkens ﬁnd it useful to generate different lightweight\nconvolutions, paying attention to different relative\npositions in a sentence.\nComposite attention performed the best.\nCombining ﬁxed lightweight convolutions with dy-\nnamic lightweight convolutions proved an effective\nstrategy for encoding relative positions. Although\ncomposite attention is simply a combination of\nShaw et al. (2018) and Raffel et al. (2020)’s relative\nposition embeddings, it validates convolution as\na viable method of encoding relative positions in\nself-attention.\nKey-based dynamic convolutions provided no\nadditional beneﬁt. When we generated an ad-\nditional lightweight convolution based on keys, the\nmodel performed worse than composite attention\nalone (GLUE 74.0 compared to 75.2). This result\nclariﬁes the ﬁndings of Huang et al. (2020), who\nreported only small improvements from query and\nkey-based relative position embeddings for a subset\nof the GLUE tasks.\nFigure 2: Learned convolution kernel weights βj−i for\nthe ﬁxed lightweight convolution (Equation 4).\nGrammaticality judgments were particularly\nsensitive to position information. On the CoLA\ntask (the corpus of linguistic acceptability;\nWarstadt et al. 2019), there was a dramatic per-\nformance drop when absolute position embed-\ndings were removed. However, when any type of\nlightweight convolution was added, performance\nimproved even over the baseline established by ab-\nsolute positions. The pronounced effects of local\nposition information on the CoLA task support the\nintuitive hypothesis that local dependencies are par-\nticularly important for grammaticality judgments.\nThis result also suggests that convolutions could\nbe beneﬁcial to more local tasks (e.g. token-level\ntasks) along with sentence classiﬁcation tasks.\n3.4 Interpreting lightweight convolutions\nTo better understand how lightweight convolu-\ntions improve language models, we visualized the\nlearned lightweight convolution kernel weights in\nFigure 2. Qualitatively, the kernels exhibited spe-\nciﬁc types of patterns:\n• Paying particular attention to the previous or\nnext token.\n• Paying graded attention either to past or future\ntokens, dictated by how far the target token is\nfrom the present token.\nThese observations support the assumption that\nnearby tokens are relevant to the interpretation of\nthe current token. They also align with the ﬁndings\n4327\nof V oita et al. (2019), who identiﬁed “positional”\nattention heads that focus primarily on the next or\nprevious token. From this perspective, lightweight\nconvolutions allow language models to explicitly\nrepresent nearby tokens’ positions.\nInterestingly, we also found that some kernels\npaid fairly uniform attention to all tokens, even\ndecreasing attention to nearby and adjacent tokens.\nIt is likely that these attention heads focused on\nmore global information, relying on the query-key\nattention mechanism rather than the convolution.\n3.5 BERT-base models\nTo thoroughly assess the impact of composite at-\ntention on pre-trained language models, we trained\nfull-sized BERT models for 1M steps each, repli-\ncating our BERT-small experiments. Pre-training\ndetails are outlined in Appendix A.1.\nResults are presented in Table 1. Differences be-\ntween models decreased substantially for full sized\nmodels, and the relative performances of different\napproaches varied across tasks. Our results suggest\nthat relative position information is more useful\nfor smaller or more data-limited models; extending\nthe beneﬁts of convolutions robustly from small\nmodels to larger models is an important direction\nfor future research. That said, even in the larger\nmodels, composite attention slightly outperformed\nthe other position embedding methods in overall\nGLUE score. Our results demonstrate that convo-\nlutions can perform at least on par with absolute\nposition embeddings even in larger models.\n4 Non-lightweight convolutions\nThe previous section found that lightweight convo-\nlutions consistently improved pre-trained language\nmodel performance. Next, we investigated whether\nthe additional ﬂexibility of non-lightweight convo-\nlutions could provide additional beneﬁts. Speciﬁ-\ncally, we considered convolutions that were ﬁxed\nbut non-lightweight. In other words, convolution\nweights were ﬁxed regardless of the input query,\nbut weights were not tied across channels, equiv-\nalent to a standard depthwise convolution. We\nonly considered ﬁxed depthwise convolutions be-\ncause under existing frameworks, dynamic depth-\nwise convolutions would introduce large numbers\nof parameters.\nTo implement depthwise convolutions, we added\na convolution term identical to the ﬁxed lightweight\nconvolution in Equation 4, except that βj−i was\nFigure 3: Learned convolution kernel weights βj−i,c\n(Equation 7) for the depthwise convolution in the deep-\nest attention layer. Channels correspond to the 256 fea-\ntures in each token representation. Channels are sorted\nsuch that kernels differentiating the previous and next\ntoken are grouped together.\nlearned separately for each feature channel:3\nαij,c = (xiWQ)(xjWK)T\n√dh\n+ βj−i,c (7)\nThis is equivalent to adding a depthwise convo-\nlution of the token values to the standard self-\nattention output.\n4.1 Non-lightweight convolution experiments\nWe ran experiments using the same setup as the\nlightweight convolution experiments in Section\n3.2. To compare the effects of dynamic lightweight\nconvolutions (e.g. composite attention) and non-\nlightweight (depthwise) convolutions, we trained\nmodels using each possible combination of the two\nconvolutions. Results are presented in Table 2.\nDepthwise convolutions were less effective than\nlightweight convolutions. As with lightweight\nconvolutions, the depthwise convolutions effec-\ntively replaced absolute position embeddings, out-\nperforming the default model. However, ﬁxed\ndepthwise convolutions performed worse than ﬁxed\nlightweight convolutions on the majority of tasks.\nThis indicates that ﬂexibility across channels is not\ncritical to the success of convolutions in language\nmodels.\n3For computational efﬁciency, we applied the softmax\nto the attention scores prior to adding the convolution term\nβj−i,c, to avoid computing softmax scores separately for each\nindividual channel. Softmax is not commonly applied in depth-\nwise convolutions.\n4328\nConvolutions Params CoLA MNLI-\nm\nMNLI-\nmm\nMRPC QNLI QQP RTE SST STS GLUE\nNo convolution + abs position∗ 13.43M 30.8 76.1 75.9 80.4 78.5 74.4 62.2 85.1 76.8 71.1\nComposite (Equation 6) 13.43M 40.4 78.2 77.4 85.0 83.3 77.7 64.7 87.8 82.1 75.2\nFixed depthwise 13.47M 36.9 77.6 76.1 80.6 81.9 76.4 64.5 87.5 79.7 73.5\nFixed depthwise + composite 13.48M 38.0 77.4 76.3 82.8 83.7 77.7 65.3 87.3 82.3 74.5\nTable 2: GLUE test set performance for BERT-small models with added depthwise convolutions and composite\nattention. ∗ denotes the default BERT-small model.\nNo composite attention\nQuery/Key Value Params GLUE\nLinear Linear 13.43M ∗71.1\nConvolution Linear 13.53M 71.9\nLinear Convolution 13.47M 73.4\nConvolution Convolution 13.58M 72.0\n+Composite attention\nQuery/Key Value Params GLUE\nLinear Linear 13.43M 75.2\nConvolution Linear 13.54M 74.5\nLinear Convolution 13.48M 73.9\nConvolution Convolution 13.59M 74.0\nTable 3: BERT-small performance on the GLUE test set when replacing queries, keys, and values with depthwise-\nseparable convolutions for half of the attention heads. ∗ denotes the use of absolute position embeddings in the\ndefault BERT-small model.\nComposite attention already provided the nec-\nessary ﬂexibility. Composite attention outper-\nformed the ﬁxed depthwise convolutions; even\nwhen composite attention was combined with\ndepthwise convolutions, there was no overall im-\nprovement over composite attention alone. This\nsuggests that in the context of language, dynamic\nlightweight convolutions efﬁciently encode any lo-\ncal position information provided by depthwise\nconvolutions.\nDepthwise convolutions differentiated previous\nand next tokens. In previous sections, we found\nthat lightweight convolution kernels often pay at-\ntention speciﬁcally to adjacent tokens. As can be\nseen in Figure 3, this result was even more pro-\nnounced in depthwise convolutions, with individ-\nual channels focusing on the previous or next token.\nInterestingly, other channels speciﬁcally directed\nattention away from adjacent tokens. This indicates\nthat the relevant information about next and previ-\nous tokens can be compressed into a subset of the\nfeature channels, freeing other channels to consider\nmore distant or position-independent information.\n5 Convolutional queries, keys, and values\nImprovements over the non-convolutional base-\nlines indicate that convolutions are beneﬁcial to lan-\nguage model pre-training, serving as replacements\nfor absolute position embeddings. Our previous\nexperiments applied different types of convolutions\nto self-attention values. To take this result one step\nfurther, we replaced the linear query, key, and value\nprojections themselves with convolutional layers.\nIntuitively, applying convolutions before self-\nattention induces even more mixing of token rep-\nresentations. If convolutions are built into every\nquery, key, and value, then it becomes impossible\nfor a token ito pay attention to a single token j\nwithout also incorporating information about to-\nkens surrounding token j.\n5.1 Convolutional Q, K, V experiments\nAs in Sections 3.2 and 4.1, we ran experiments on\nBERT-small. We replaced the query, key and value\nprojections with depthwise-separable convolutions\nin half of the self-attention heads. 4 This aligns\nwith previous work in which only half of the output\ndimensions for each token were generated using\nconvolutions (Jiang et al., 2020). Indeed, our initial\nexplorations found that it was more effective to\nreplace the linear projections in only half, not all,\nthe attention heads.\nThen, we considered whether convolutions from\nprevious experiments provided additional beneﬁts\nover convolutional queries, keys, and values. To\ntest this, we trained BERT-small models with com-\nposite attention (Equation 6), adding convolutional\nqueries, keys, and values.\n4Depthwise-separable convolutions are a common way\nto save convolution parameters. A depthwise convolution is\napplied ﬁrst, applying an independent convolution for each\nchannel. Then, a pointwise convolution (i.e. a feedforward\nlayer) mixes the channels to produce the ﬁnal output.\n4329\n5.2 Convolutional Q, K, V results\nResults are presented in Table 3. Similar to our pre-\nvious convolution experiments, all convolutional\nreplacements successfully outperformed the default\nmodel. These results strongly support the conclu-\nsion that convolutions are a viable method of en-\ncoding positional information for language tasks.\nHowever, all convolutional replacements for\nqueries, keys, and values slightly decreased the\nperformance of models using composite attention.\nConvolutional values in particular were effective\nin models without composite attention, but they\nslightly decreased performance in models that al-\nready incorporated such lightweight convolutions.\nWe conclude that although convolutions can beneﬁt\nmodels by adding local position information, there\nis a limit to how much local mixing should be done.\nIt is sufﬁcient to apply convolutions to token values\non top of self-attention; additional convolutional\nlayers applied before the self-attention map enforce\nunnecessary mixing of token representations.\n6 Discussion\nOur results demonstrate that convolutions provide\nconsistent beneﬁts to pre-trained language models.\nOur proposed composite attention mechanism com-\nbines previous relative position embedding meth-\nods, showing that convolutions can effectively com-\npensate for the lack of local position information\nin Transformer models.\n6.1 Related work\nOur work unites and builds upon previous work\nusing convolutions and relative positions in Trans-\nformers. We adopted the relative embeddings\nfrom Shaw et al. (2018) and Huang et al. (2020),\nshowing that these embeddings are equivalent to\nthe dynamic lightweight convolutions in Wu et al.\n(2019). Combining these dynamic lightweight\nconvolutions with ﬁxed lightweight convolutions\n(equivalent to the relative position terms in Raffel\net al. 2020), we studied relative embeddings under\nthe framework of convolution integrated with self-\nattention. As far as we are aware, our work is the\nﬁrst to holistically compare relative positions, con-\nvolutions, and self-attention in language models.\nBuilding upon dynamic lightweight convolu-\ntions, recent work has incorporated both depthwise-\nseparable and dynamic lightweight convolutions in\npre-trained language models. Jiang et al. (2020)\nproposed ConvBERT, which adds a convolutional\nmodule alongside the standard self-attention mech-\nanism in BERT. ConvBERT’s convolutional mod-\nule consists of a depthwise-separable convolution\ncombining with a query to generate a dynamic\nlightweight convolution. Under our integrated\nframework, this is analogous to the model which\nuses depthwise-separable convolutions for queries\nand keys, using composite attention as a query-\nbased dynamic lightweight convolution (see Table\n3). To make this comparison concrete, we trained\na ConvBERT-small model using the same setup as\nour experiments. Indeed, the analogous model un-\nder our framework outperformed ConvBERT-small\n(GLUE score 74.5 compared to 70.3). Details for\nthe ConvBERT comparison can be found in Ap-\npendix A.3.\nFinally, recent work has proved theoretical rela-\ntionships between self-attention and convolution.\nCordonnier et al. (2020) showed that given enough\nself-attention heads, self-attention weights can ex-\npress any convolution; in fact, they showed that\nself-attention layers often learn such convolutional\nstructures when trained on vision tasks. How-\never, this theoretical equivalence does not explain\nconvolution-based improvements for Transformers\nin language tasks. To clarify the relationship be-\ntween self-attention and convolution in language,\nour work characterizes self-attention as a type of\ndynamic lightweight convolution. By establishing\na per-parameter equivalence between relative po-\nsition embeddings and Wu’s dynamic lightweight\nconvolutions, we provide a concrete foundation\nwhere self-attention and convolution are used to-\ngether in practice.\n7 Conclusion\nIn this work, we formalized the relationship be-\ntween self-attention and convolution. We proposed\ncomposite attention, which combines self-attention\nwith lightweight convolution, uniting previous ap-\nproaches to relative positions. Our formulation and\nempirical results demonstrate that convolutions can\nimprove self-attention by providing local position\ninformation in sentences, capable of replacing ab-\nsolute position embeddings entirely.\nOur ﬁndings provide a solid foundation from\nwhich to study convolutions and self-attention in\nlanguage tasks. The spatially-oriented nature of\nconvolutional neural networks translates directly\ninto positional information in language. As vision\nand language researchers strive towards common\n4330\ndeep learning architectures, it is important to rec-\nognize how architectures for vision tasks can be\nadapted to linguistic domains.\nAcknowledgments\nThis work is funded by NSF IIS-1717431.\nZhuowen Tu is also funded under the Qualcomm\nFaculty Award. Tyler Chang is partially supported\nby the UCSD HDSI graduate fellowship.\nReferences\nIrwan Bello, Barret Zoph, Ashish Vaswani, Jonathon\nShlens, and Quoc Le. 2019. Attention augmented\nconvolutional networks. In International Confer-\nence on Computer Vision.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, Sandhini Agarwal, Ariel Herbert-\nV oss, Gretchen Krueger, Tom Henighan, Rewon\nChild, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,\nClemens Winter, Chris Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nProceedings of the 34th Conference on Neural Infor-\nmation Processing Systems.\nKevin Clark, Minh-Thang Luong, Quoc Le, and\nChristopher Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In Proceedings of the International Con-\nference on Learning Representations.\nJean-Baptiste Cordonnier, Andreas Loukas, and Mar-\ntin Jaggi. 2020. On the relationship between self-\nattention and convolutional layers. In Proceedings\nof the International Conference on Learning Repre-\nsentations.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nLinhao Dong, Shuang Xu, and Bo Xu. 2018. Speech-\ntransformer: a no-recurrence sequence-to-sequence\nmodel for speech recognition. In IEEE International\nConference on Acoustics, Speech and Signal Pro-\ncessing, pages 5884–5888.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image is\nworth 16x16 words: Transformers for image recog-\nnition at scale. In Proceedings of the International\nConference on Learning Representations.\nSebastian Hofst ¨atter, Hamed Zamani, Bhaskar Mitra,\nNick Craswell, and Allan Hanbury. 2020. Local\nself-attention over long text for efﬁcient document\nretrieval. In Proceedings of the 43rd International\nACM SIGIR Conference on Research and Develop-\nment in Information Retrieval, New York, NY , USA.\nAssociation for Computing Machinery.\nZhiheng Huang, Davis Liang, Peng Xu, and Bing Xi-\nang. 2020. Improve transformer models with bet-\nter relative position embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 3327–3335, Online. Association for\nComputational Linguistics.\nZihang Jiang, Weihao Yu, Daquan Zhou, Yunpeng\nChen, Jiashi Feng, and Shuicheng Yan. 2020. Con-\nvBERT: Improving BERT with span-based dynamic\nconvolution. In Proceedings of the 34th Conference\non Neural Information Processing Systems.\nGuolin Ke, Di He, and Tie-Yan Liu. 2021. Rethinking\npositional encoding in language pre-training. In Pro-\nceedings of the International Conference on Learn-\ning Representations.\nTaku Kudo and John Richardson. 2018. SentencePiece:\nA simple and language independent subword tok-\nenizer and detokenizer for neural text processing. In\nProceedings of the 2018 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 66–71, Brussels, Belgium.\nAssociation for Computational Linguistics.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture mod-\nels. In Proceedings of the Fifth International Confer-\nence on Learning Representations.\nEmilio Parisotto, Francis Song, Jack Rae, Razvan Pas-\ncanu, Caglar Gulcehre, Siddhant Jayakumar, Max\nJaderberg, Rapha ¨el Lopez Kaufman, Aidan Clark,\nSeb Noury, Matthew Botvinick, Nicolas Heess, and\nRaia Hadsell. 2020. Stabilizing transformers for re-\ninforcement learning. In Proceedings of the Interna-\ntional Conference on Machine Learning.\nJason Phang, Thibault F ´evry, and Samuel Bowman.\n2018. Sentence encoders on STILTs: Supple-\nmentary training on intermediate labeled-data tasks.\narXiv preprint arXiv:1811.01088.\n4331\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring\nthe limits of transfer learning with a uniﬁed text-to-\ntext transformer. Journal of Machine Learning Re-\nsearch, 21(140):1–67.\nAlessandro Raganato, Yves Scherrer, and J ¨org Tiede-\nmann. 2020. Fixed encoder self-attention patterns\nin transformer-based machine translation. In Find-\nings of the Association for Computational Linguis-\ntics: EMNLP 2020, pages 556–568, Online. Associ-\nation for Computational Linguistics.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers), pages 464–468,\nNew Orleans, Louisiana. Association for Computa-\ntional Linguistics.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Fran-\ncisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. 2020. Training data-efﬁcient image trans-\nformers and distillation through attention. arXiv\npreprint arXiv:2012.12877.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Proceedings of the 31st Conference on\nNeural Information Processing Systems.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797–5808, Florence,\nItaly. Association for Computational Linguistics.\nAlex Wang, Amanpreet Singh, Julian Michael, Fe-\nlix Hill, Omer Levy, and Samuel Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In Pro-\nceedings of the 2018 EMNLP Workshop Black-\nboxNLP: Analyzing and Interpreting Neural Net-\nworks for NLP, pages 353–355, Brussels, Belgium.\nAssociation for Computational Linguistics.\nAlex Warstadt, Amanpreet Singh, and Samuel R. Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nWei Wei, Zanbo Wang, Xianling Mao, Guangyou Zhou,\nPan Zhou, and Sheng Jiang. 2021. Position-aware\nself-attention based neural sequence labeling. Pat-\ntern Recognition, 110.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nFelix Wu, Angela Fan, Alexei Baevski, Yann Dauphin,\nand Michael Auli. 2019. Pay less attention with\nlightweight and dynamic convolutions. In Proceed-\nings of the Seventh International Conference on\nLearning Representations.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ Salakhutdinov, and Quoc Le. 2019.\nXLNet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems, volume 32.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. In 2015 IEEE International Con-\nference on Computer Vision, pages 19–27.\nHyperparameter Small Base\nLayers 12 12\nHidden size 256 768\nIntermediate hidden size 1024 3072\nAttention heads 4 12\nAttention head size 64 64\nEmbedding size 128 768\nV ocab size 30004 30004\nMax sequence length 128 128\nMask proportion 0.15 0.15\nLearning rate decay Linear Linear\nWarmup steps 10000 10000\nLearning rate 3e-4 1e-4\nAdam ϵ 1e-6 1e-6\nAdam β1 0.9 0.9\nAdam β2 0.999 0.999\nAttention dropout 0.1 0.1\nDropout 0.1 0.1\nWeight decay 0.01 0.01\nBatch size 128 256\nTrain steps 125K 1M\nTable 4: Pre-training hyperparameters.\nA Appendix\nA.1 Pre-training and ﬁne-tuning details\nBERT models (Devlin et al. 2019; Clark et al.\n2020) were pre-trained on the BookCorpus (Zhu\net al., 2015) and WikiText-103 datasets (Merity\n4332\nHyperparameter Value\nLearning rate decay Linear\nWarmup steps 10% of total\nLearning rate 1e-4 for QNLI or base-size\n3e-4 otherwise\nAdam ϵ 1e-6\nAdam β1 0.9\nAdam β2 0.999\nAttention dropout 0.1\nDropout 0.1\nWeight decay 0\nBatch size 128 for MNLI/QQP\n32 otherwise\nTrain steps 10 epochs for RTE/STS\n4 epochs for MNLI/QQP\n3 epochs otherwise\nTable 5: Fine-tuning hyperparameters. We used inter-\nmediate task training for RTE, STS, and MRPC, initial-\nizing from a checkpoint ﬁne-tuned on the MNLI task\n(Clark et al. 2020; Phang et al. 2018).\net al., 2017) using masked language modeling. Pre-\ntraining examples were formatted as sentence pairs\nwithout the next sentence prediction objective. In\ntotal, our dataset consisted of 31M unique sentence\npairs.5 Sentences were tokenized by training an un-\ncased SentencePiece tokenizer (Kudo and Richard-\nson, 2018), and input and output token embeddings\nwere tied during pre-training. Models were evalu-\nated on the GLUE benchmark (Wang et al., 2018).\nIncluding ten ﬁne-tuning runs for each GLUE task,\neach BERT-small model took about 24 hours to\ntrain on two Titan Xp GPUs. Each BERT-base\nmodel took about 16 days to train on 8 GPUs. Pre-\ntraining hyperparameters are listed in Table 4, and\nﬁne-tuning hyperparameters are listed in Table 5.\nHyperparameters are based on those used in Clark\net al. (2020) and Devlin et al. (2019).\nA.2 GLUE development results\nResults for each model on the GLUE development\nset are reported in Table 6. We report averages\nover ten ﬁne-tuning runs for each task, including\nstandard errors of the mean. Each overall GLUE\nscore was computed as the average of individual\ntask scores; we computed GLUE score averages\nand standard errors over ten GLUE scores, cor-\nresponding to the ten ﬁne-tuning runs. We note\nthat development scores were generally higher than\ntest scores due to differences between the test and\n5Because BERT-small models were only trained for\n125,000 steps with batch size 128, small models were trained\non 16M sentence pairs.\ntraining distributions (Wang et al., 2018).\nA.3 Detailed ConvBERT comparison\nConvBERT adds a convolutional module alongside\nthe standard self-attention mechanism in BERT\n(Jiang et al., 2020). ConvBERT uses half the num-\nber of standard self-attention heads, using convolu-\ntional modules for the other half. In each convolu-\ntional module, a depthwise-separable convolution\nis multiplied pointwise with the query in the cor-\nresponding self-attention head. This convolutional\nquery is fed into a linear layer to generate a dy-\nnamic lightweight convolution.\nUnder our framework, the analogous model re-\nplaces half of the queries and keys with depthwise-\nseparable convolutions and uses composite atten-\ntion (a query-based dynamic lightweight convolu-\ntion; see Table 3 in the full paper). In both models\n(ConvBERT and our own), half of the attention\nheads use a convolutional query. Additionally, in\nboth models, the convolutional query is used to\ngenerate a dynamic lightweight convolution.\nHowever, in our model, the dynamic lightweight\nconvolution (in this case, composite attention) is\nused for all attention heads, not just the convolu-\ntional heads. Furthermore, our convolutional heads\nstill use a self-attention mechanism along with the\ndynamic lightweight convolutions, by generating\nconvolutional keys. In this way, our model adds\nconvolutions to ConvBERT’s self-attention heads,\nand adds self-attention to ConvBERT’s convolu-\ntional heads.\nThen, we investigated whether the separate self-\nattention and convolutional modules in ConvBERT\nprovide any beneﬁt over our integrated convolu-\ntion and self-attention. We trained a ConvBERT-\nsmall model using the same pre-training setup as\nour BERT-small experiments, comparing perfor-\nmance to the analogous model under our frame-\nwork. Results are shown in Table 7. Indeed,\nintegrated convolutions and self-attention outper-\nformed ConvBERT-small, using only 3% more pa-\nrameters.\n4333\nConvolution type, BERT-small Params CoLA MNLI-m MNLI-mm MRPC QNLI\nNo convolution 13.41M 7.0 ± 2.4 73 .0 ± 0.1 73 .0 ± 0.1 80 .9 ± 0.4 80 .1 ± 0.2\nNo convolution + abs position∗ 13.43M 33.5 ± 0.4 75 .8 ± 0.1 76 .1 ± 0.1 83 .3 ± 0.4 78 .2 ± 0.3\nFixed lightweight (Raffel et al. 2020) 13.42M 38.3 ± 0.8 77 .2 ± 0.1 77 .2 ± 0.1 84 .0 ± 0.5 82 .1 ± 0.1\nDynamic lightweight (Shaw et al. 2018) 13.43M 38.4 ± 0.7 77.9 ± 0.1 77 .6 ± 0.1 85 .6 ± 0.5 82 .8 ± 0.1\nComposite (Equation 6) 13.43M 40.9 ± 0.7 77.9 ± 0.1 78 .0 ± 0.1 86 .2 ± 0.3 83 .0 ± 0.1\nComposite + key-based dynamic 13.44M 40.0 ± 0.6 77.9 ± 0.1 77 .7 ± 0.1 86.3 ± 0.3 83 .3 ± 0.1\nFixed depthwise 13.47M 38.0 ± 0.6 76 .9 ± 0.0 76 .8 ± 0.1 82 .8 ± 0.5 81 .9 ± 0.1\nComposite + ﬁxed depthwise 13.48M 40.4 ± 0.7 77 .2 ± 0.1 77 .4 ± 0.1 85 .0 ± 0.3 83 .3 ± 0.1\nConvolutional QK 13.53M 33.4 ± 0.4 76 .3 ± 0.1 76 .4 ± 0.1 83 .3 ± 0.2 81 .3 ± 0.2\nConvolutional value 13.47M 34.7 ± 0.9 76 .2 ± 0.0 76 .6 ± 0.1 83 .4 ± 0.4 82 .4 ± 0.1\nConvolutional QKV 13.58M 31.9 ± 0.7 76 .3 ± 0.1 76 .3 ± 0.1 83 .7 ± 0.4 80 .4 ± 0.2\nComposite + convolutional QK 13.54M 39.3 ± 0.8 77 .4 ± 0.1 77 .2 ± 0.1 85 .4 ± 0.3 81 .9 ± 0.1\nComposite + convolutional value 13.48M 37.9 ± 0.7 77 .8 ± 0.1 78.1 ± 0.1 85 .6 ± 0.4 83.6 ± 0.1\nComposite + convolutional QKV 13.59M 38.2 ± 1.0 77 .4 ± 0.1 77 .3 ± 0.1 85 .3 ± 0.4 82 .8 ± 0.1\nConvBERT 13.09M 33.3 ± 1.5 76 .7 ± 0.1 76 .8 ± 0.1 83 .9 ± 0.5 77 .1 ± 0.8\nConvolution type, BERT-base\nNo convolution + abs position∗ 108.82M 57.6 ± 0.6 82.0 ± 0.1 81.9 ± 0.1 88.4 ± 0.2 84 .7 ± 0.3\nFixed lightweight (Raffel et al. 2020) 108.73M 58.9 ± 0.5 81 .9 ± 0.1 81 .6 ± 0.1 87 .7 ± 0.3 86.2 ± 0.1\nDynamic lightweight (Shaw et al. 2018) 108.74M 58.4 ± 0.5 81 .8 ± 0.1 81 .8 ± 0.1 86 .7 ± 0.4 85 .6 ± 0.2\nComposite (Equation 6) 108.74M 58.5 ± 0.5 81 .9 ± 0.1 81 .6 ± 0.1 86 .0 ± 1.2 85 .0 ± 0.3\nConvolution type, BERT-small QQP RTE SST STS GLUE\nNo convolution 84.4 ± 0.1 61 .0 ± 0.5 80 .9 ± 0.9 83 .7 ± 0.1 69 .3 ± 0.3\nNo convolution + abs position∗ 84.9 ± 0.0 64 .4 ± 0.5 85 .0 ± 0.2 82 .4 ± 0.1 73 .7 ± 0.1\nFixed lightweight (Raffel et al. 2020) 86.2 ± 0.0 64 .7 ± 0.9 86 .9 ± 0.2 85 .2 ± 0.1 75 .7 ± 0.2\nDynamic lightweight (Shaw et al. 2018) 87.2 ± 0.0 65 .1 ± 0.9 86 .8 ± 0.2 85 .6 ± 0.1 76 .3 ± 0.1\nComposite (Equation 6) 87.3 ± 0.0 66 .1 ± 0.7 86 .9 ± 0.1 85 .9 ± 0.1 76.9 ± 0.1\nComposite + key-based dynamic 87.4 ± 0.0 66.3 ± 0.4 86 .5 ± 0.3 86 .1 ± 0.2 76 .8 ± 0.1\nFixed depthwise 86.1 ± 0.1 64 .2 ± 0.7 87 .2 ± 0.2 84 .4 ± 0.1 75 .4 ± 0.1\nComposite + ﬁxed depthwise 87.3 ± 0.0 63 .5 ± 0.8 87 .1 ± 0.2 86 .1 ± 0.1 76 .4 ± 0.1\nConvolutional QK 85.1 ± 0.1 63 .0 ± 1.0 86 .1 ± 0.2 84 .5 ± 0.1 74 .4 ± 0.1\nConvolutional value 86.6 ± 0.0 65 .2 ± 0.7 87 .2 ± 0.3 85 .0 ± 0.1 75 .2 ± 0.1\nConvolutional QKV 84.6 ± 0.2 66 .1 ± 0.9 86 .4 ± 0.1 84 .4 ± 0.1 74 .4 ± 0.1\nComposite + convolutional QK 86.7 ± 0.0 64 .0 ± 0.9 87.5 ± 0.2 85 .7 ± 0.1 76 .1 ± 0.1\nComposite + convolutional value 87.5 ± 0.0 65 .1 ± 0.5 87.5 ± 0.1 86.4 ± 0.1 76 .6 ± 0.1\nComposite + convolutional QKV 87.0 ± 0.0 64 .9 ± 0.8 86 .9 ± 0.1 85 .9 ± 0.1 76 .2 ± 0.2\nConvBERT 85.1 ± 0.1 64 .6 ± 0.5 86 .3 ± 0.3 84 .0 ± 0.2 74 .2 ± 0.3\nConvolution type, BERT-base\nNo convolution + abs position∗ 88.7 ± 0.0 69 .9 ± 0.5 90 .4 ± 0.1 88.4 ± 0.1 81 .0 ± 0.2\nFixed lightweight (Raffel et al. 2020) 88.8 ± 0.0 70 .9 ± 0.7 90 .8 ± 0.1 88 .1 ± 0.1 81.3 ± 0.2\nDynamic lightweight (Shaw et al. 2018) 88.7 ± 0.0 70 .6 ± 0.6 91.1 ± 0.1 87 .7 ± 0.3 81 .1 ± 0.2\nComposite (Equation 6) 88.7 ± 0.0 71.0 ± 0.7 90 .5 ± 0.1 88.4 ± 0.1 81 .2 ± 0.2\nTable 6: GLUE development set scores for each model described in the main paper, reporting averages and standard\nerrors of the mean over ten ﬁne-tuning runs for each task. ∗ denotes the default BERT model.\nModel, BERT-small Params CoLA MNLI-\nm\nMNLI-\nmm\nMRPC QNLI QQP RTE SST STS GLUE\nConvBERT 13.1M 25.5 75.4 73.9 79.7 76.0 74.7 64.3 85.6 77.9 70.3\nIntegrated convolutions and\nself-attention (ours)\n13.5M 37.9 77.5 76.6 83.7 83.1 76.6 65.3 88.7 81.1 74.5\nTable 7: Comparison between ConvBERT-small and the analogous model under our framework, reporting GLUE\ntest set results."
}