{
  "title": "TransMIL: Transformer based Correlated Multiple Instance Learning for Whole Slide Image Classication",
  "url": "https://openalex.org/W3193968034",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2999765235",
      "name": "Zhuchen Shao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2166263376",
      "name": "Hao Bian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2066193536",
      "name": "Chen Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2103854278",
      "name": "Yifeng Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2099156760",
      "name": "Jian Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2189497957",
      "name": "Xiangyang Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2120521483",
      "name": "Yong-Bing Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971376088",
    "https://openalex.org/W3135547872",
    "https://openalex.org/W3091730259",
    "https://openalex.org/W3099458507",
    "https://openalex.org/W2995426144",
    "https://openalex.org/W2964069537",
    "https://openalex.org/W3130695101",
    "https://openalex.org/W3089090082",
    "https://openalex.org/W1975103383",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W3034158323",
    "https://openalex.org/W3176719058",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2776507577",
    "https://openalex.org/W2970803838",
    "https://openalex.org/W2979818848",
    "https://openalex.org/W2986297814",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3034447539",
    "https://openalex.org/W3013098034",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3100084586",
    "https://openalex.org/W2963420686",
    "https://openalex.org/W3095422700",
    "https://openalex.org/W3137165224",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W3035022492",
    "https://openalex.org/W3093416812",
    "https://openalex.org/W2531897166",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W3109319753",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2949611920",
    "https://openalex.org/W2884585870",
    "https://openalex.org/W3033382446",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2956228567"
  ],
  "abstract": "Multiple instance learning (MIL) is a powerful tool to solve the weakly supervised classification in whole slide image (WSI) based pathology diagnosis. However, the current MIL methods are usually based on independent and identical distribution hypothesis, thus neglect the correlation among different instances. To address this problem, we proposed a new framework, called correlated MIL, and provided a proof for convergence. Based on this framework, we devised a Transformer based MIL (TransMIL), which explored both morphological and spatial information. The proposed TransMIL can effectively deal with unbalanced/balanced and binary/multiple classification with great visualization and interpretability. We conducted various experiments for three different computational pathology problems and achieved better performance and faster convergence compared with state-of-the-art methods. The test AUC for the binary tumor classification can be up to 93.09% over CAMELYON16 dataset. And the AUC over the cancer subtypes classification can be up to 96.03% and 98.82% over TCGA-NSCLC dataset and TCGA-RCC dataset, respectively.",
  "full_text": "TransMIL: Transformer based Correlated Multiple\nInstance Learning for Whole Slide\nImage Classiﬁcation\nZhuchen Shao∗ ∗,1, Hao Bian∗,1, Yang Chen∗,1, Yifeng Wang2, Jian Zhang3, Xiangyang Ji4\nYongbing Zhang††,2\n1Tsinghua Shenzhen International Graduate School, Tsinghua University\n2Harbin Institute of Technology (Shenzhen)\n3School of Electronic and Computer Engineering, Peking University\n4Department of Automation, Tsinghua University\nAbstract\nMultiple instance learning (MIL) is a powerful tool to solve the weakly supervised\nclassiﬁcation in whole slide image (WSI) based pathology diagnosis. However, the\ncurrent MIL methods are usually based on independent and identical distribution\nhypothesis, thus neglect the correlation among different instances. To address\nthis problem, we proposed a new framework, called correlated MIL, and provided\na proof for convergence. Based on this framework, we devised a Transformer\nbased MIL (TransMIL), which explored both morphological and spatial infor-\nmation. The proposed TransMIL can effectively deal with unbalanced/balanced\nand binary/multiple classiﬁcation with great visualization and interpretability. We\nconducted various experiments for three different computational pathology prob-\nlems and achieved better performance and faster convergence compared with\nstate-of-the-art methods. The test AUC for the binary tumor classiﬁcation can\nbe up to 93.09 % over CAMELYON16 dataset. And the AUC over the cancer\nsubtypes classiﬁcation can be up to 96.03 % and 98.82% over TCGA-NSCLC\ndataset and TCGA-RCC dataset, respectively. Implementation is available at:\nhttps://github.com/szc19990412/TransMIL.\n1 Introduction\nThe advent of whole slide image (WSI) scanners, which convert the tissue on the biopsy slide into a\ngigapixel image fully preserving the original tissue structure [1], provides a good opportunity for the\napplication of deep learning in the ﬁeld of digital pathology [ 2, 3, 4]. However, the deep learning\nbased biopsy diagnosis in WSI has to face a great challenges due to the huge size and the lack of\npixel-level annotations[5]. To address this problem, multiple instance learning (MIL) is usually\nadopted to take diagnosis analysis as a weakly supervised learning problem.\nIn deep learning based MIL, one straightforward idea is to perform pooling operation [ 6, 7] on\ninstance feature embeddings extracted by CNN. Ilse et al.[8] proposed an attention based aggregation\noperator, giving each instance additional contribution information through trainable attention weights.\nIn addition, Li et al. [9] introduced non-local attention into the MIL problem. By calculating the\nsimilarity between the highest-score instance and the others, each instance is given different attention\n∗Contributed equally: {shaozc0412,h2495067728,sky374263410}@gmail.com.\n†Corresponding author: ybzhang08@hit.edu.cn.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.\narXiv:2106.00908v2  [cs.CV]  31 Oct 2021\nFigure 1: Decision-making process. MIL Attention Mechanism: follow the i.i.d. assumption.\nSelf-attention Mechanism: under the correlated MIL framework.\nweight and the interpretable attention map can be obtained accordingly. There were also other\npioneering works [10, 11, 12, 13, 14] in weakly supervised WSI diagnosis.\nHowever, all these methods are based on the assumption that all the instances in each bag are\nindependent and identically distributed (i.i.d.). While achieving some improvements in many tasks,\nthis i.i.d. assumption was not entirely valid [15] in many cases. Actually, pathologists often consider\nboth the contextual information around a single area and the correlation information between different\nareas when making a diagnostic decision. Therefore, it would be much desirable to consider the\ncorrelation between different instances in MIL diagnosis.\nAt present, Transformer is widely used in many vision tasks [ 16, 17, 18, 19, 20, 21] due to the\nstrong ability of describing correlation between different segments in a sequence (tokens) as well as\nmodelling long distance information. As shown in Figure 1, different from bypass attention network\nin the existing MIL, the Transformer adopts self-attention mechanism, which can pay attention to\nthe pairwise correlation between each token within a sequence. However, traditional Transformer\nsequences are limited by their computational complexity and can only tackle shorter sequences (e.g.,\nless than 1000) [22]. Therefore, it is not suitable for large size images such as WSIs.\nTo address these challenges mentioned above, we proposed a correlated MIL framework, including\nthe convergence proof and a generic three-step algorithm. In addition, a Transformer based MIL\n(TransMIL) was devised to explore both morphological and spatial information between different\ninstances. Great performance over various datasets demonstrate the validity of the proposed method.\n2 Related Work\n2.1 Application of MIL in WSI classiﬁcation\nThe application of MIL in WSIs can be divided into two categories. The ﬁrst one is instance-level\nalgorithms [7, 23, 24, 25, 26], where a CNN is ﬁrst trained by assigning each instance a pseudo-label\nbased on the bag-level label, and then the top-k instances are selected for aggregation. However, this\nmethod requires a large number of WSIs, since only a small number of instances within each slide\ncan actually participate in the training. The second category is embedding-level algorithms, where\neach patch in the entire slide is mapped to a ﬁxed-length embedding, and then all feature embeddings\nare aggregated by an operator (e.g., max-pooling). To improve the performance, the MIL attention\nbased method [8, 10, 11, 12, 13] assigns the contribution of each instance by introducing trainable\nparameters. In addition, the feature clustering methods [14, 27, 28] calculated the cluster centroids of\nall the feature embeddings and then the representative feature embeddings was employed to make the\nﬁnal prediction. Recently, non-local attention [9] was also adopted in MIL to pay more attention to\nthe correlation between the highest-score instance and all the remaining instances.\n2\n2.2 Attention and Self-attention in Deep Learning\nAttention was initially used to extract important information about sentences in machine translation\n[29]. Then the attention mechanism was gradually applied to computer vision tasks, including giving\ndifferent weights to feature channels [30] or spatial distribution [31], or giving different weights to\ntime series in video analysis [32]. Recently, attention was also applied in MIL analysis [10, 11, 12, 13].\nHowever, all these methods did not consider the correlation between different instances.\nThe most typical self-attention application was the Transformer based NLP framework proposed\nby Google [33]. Recently, Transformer was also applied in many computer vision tasks, including\nobject detection [16, 17], segmentation [18, 19], image enhancement [20, 21] and video processing\n[34]. In this paper, for the ﬁrst time, we proposed a Transformer based WSI classiﬁcation, where the\ncorrelations among different instances within the same bag are comprehensively considered.\n3 Method\n3.1 Correlated Multiple Instance Learning\nProblem formulation Take binary MIL classiﬁcation as an example, we want to predict a target\nvalue Yi ∈{0,1}, given a bag of instances {xi,1,xi,2,..., xi,n}with Xi, for i= 1,...,b , that ex-\nhibit both dependency and ordering among each other. The instance-level labels {yi,1,yi,2,...,y i,n}\nare unknown, and the bag-level label is Yi, for i = 1,...,b . A binary MIL classiﬁcation can be\ndeﬁned as:\nYi =\n{\n0,iff\n∑\nyi,j = 0 yi,j ∈{0,1},j = 1 ...n\n1,otherwise\n(1)\nˆYi = S(Xi), (2)\nwhere S is a scoring function, ˆYi represents the prediction. bis the total number of bags, nis the\nnumber of instances in ith bag, and the number of ncan vary for different bags.\nCompared to the MIL framework proposed by Ilse et al. [8], we further introduce the correlation\nbetween different instances. Theorem 1 and Inference give an arbitrary approximation form of the\nscoring function S(X), and Theorem 2 provides the advantage of correlated MIL.\nTheorem 1. Suppose S : X →R is a continuous set function w.r.t Hausdorff distance dH(·,·).\n∀ε> 0, for any invertible map P : X→ Rn,∃function σand g, such that for any X ∈X:\n|S(X) −g( P\nX∈X\n{σ(x) : x ∈X})|<ε. (3)\nThat is: a Hausdorff continuous function S(X) can be arbitrarily approximated by a function in the\nform g( P\nX∈X\n{σ(x) : x ∈X}).\nProof. By the continuity of S, we take ∀ε> 0,∃δε, so that |S(X)−S(X′)|<ε for any X,X′∈X,\nif dH (X,X′) <δε.\nDeﬁne K = ⌈1\nδε\n⌉and deﬁne an auxiliary function: σ(x) = ⌊Kx⌋\nK . Let ˜X = {σ(x) : x ∈X}, then:\n|S(X) −S( ˜X)|<ε, (4)\nbecause dH(X,˜X) < 1\nK ≤δε.\nLet P : X →Rn be any invertible map, its inverse mapping is expressed as P−1: Rn →X. Let\ng= S\n(\nP−1)\n, then:\nS\n(\nP−1( P\nX∈X\n({σ(x) : x ∈X}))\n)\n= S\n(\nP−1( P\n˜X∈X\n( ˜X))\n)\n= S( ˜X). (5)\nBecause |S(X) −S( ˜X)|<ε and S( ˜X) = S\n(\nP−1(P( ˜X))\n)\n= g(P( ˜X)), we have:\n|S(X) −g( P\nX∈X\n{σ(x) : x ∈X})|<ε. (6)\nThis completes the proof.\n3\nFigure 2: The difference between different Pooling Matrix P. Suppose there are 5 instances sampled\nfrom WSI in (a), P ∈R5×5 is the corresponding Pooling Matrix, where the values in the diagonal\nline indicate the attention weight for itself and the rest indicate correlation between different instances.\n(b,c,d) all neglect the correlation information, hence the P is diagonal matrix. In (b), the ﬁrst instance\nwas chosen by Max-pooling operator, so there is only one non-zero value in the ﬁrst diagonal position.\nIn (c), all the values within diagonal line are the same due to the Mean-pooling operator. In (d), the\nvalues within diagonal line can be varied due to the introduction of bypass attention. (e) obeys the\ncorrelation assumption, so there are non-zero values in off-diagonal position indicating correlation\nbetween different instances.\nInference Suppose S : X →R is a continuous set function w.r.t Hausdorff distance dH(·,·).\n∀ε> 0, for any function f and any invertible map P : X→ Rn,∃function hand g, such that for\nany X ∈X:\n|S(X) −g( P\nX∈X\n{f(x) + h(x) : x ∈X})|<ε. (7)\nThat is: a Hausdorff continuous function S(X) can be arbitrarily approximated by a function in the\nform g( P\nX∈X\n{f(x) + h(x) : x ∈X}).\nProof. The proof is in the Appendix A.\nTheorem 2. The Instances in the bag are represented by random variables Θ1,Θ2,..., Θn,\nthe information entropy of the bag under the correlation assumption can be expressed as\nH(Θ1,Θ2,..., Θn), and the information entropy of the bag under the i.i.d. (independent and\nidentical distribution) assumption can be expressed as ∑n\nt=1 H(Θt), then we have:\nH(Θ1,Θ2,..., Θn) =\nn∑\nt=2\nH(Θt |Θ1,..., Θt−1) + H(Θ1) ≤\nn∑\nt=1\nH(Θt) . (8)\nProof. The proof is in the Appendix A.\nTheorem 2 proved the correlation assumption has smaller information entropy, which may reduce\nthe uncertainty and bring more useful information for the MIL problem. Motivated by the Inference\nand Theorem 2, a generic three-step method like Algorithm1 was developed. The main difference\nbetween the proposed algorithm and existing methods is shown in Figure 2.\nAlgorithm 1: A generic three-step approach under the correlated MIL\nInput: The bag of instances Xi = {xi,1,xi,2 ..., xi,n}\nOutput: Bag-level predicted label ˆYi\n1) Extracting morphological and spatial information of all the instances by f and h,\nrespectively;\nXf ←f(Xi), Xh ←h(Xi), Xfh ←Xf + Xh, where Xf,Xh,Xfh ∈Rn×d;\n2) Aggregating the extracted information for all instances by Pooling Matrix P;\nXP ←PXfh, where P ∈Rn×n;\n3) Transforming XP to obtain the predicted bag-level label by g;\nˆYi ←g(XP).\n4\nFigure 3: Overview of our TransMIL. Each WSI is cropped into patches (background is discarded),\nand embedded in feature vectors by ResNet50. Then the sequence is processed with the TPT module:\n1) Squaring of sequence; 2) Correlation modelling of the sequence; 3) Conditional position encoding\nand local information fusion; 4) Deep feature aggregation; 5) Mapping of T →Y.\n3.2 How to apply Transformer to correlated MIL\nThe Transformer uses a self-attention mechanism to model the interactions between all tokens in\na sequence, and the adding of positional information further increases the use of sequential order\ninformation. Therefore it’s a good idea to introduce the Transformer into the correlated MIL problem\nwhere the function hencodes the spatial information among instances, and the Pooling Matrix P uses\nself-attention for information aggregation. To make this clear, we further give a formal deﬁnition.\nTransformer based MIL. Given a set of bags {X1,X2,..., Xb}, and each bag Xi contains\nmultiple instances {xi,1,xi,2,..., xi,n}and a corresponding label Yi. The goal is to learn the\nmappings: X →T →Y,where X is the bag space, T is the Transformer space and Yis the label\nspace. The speciﬁc mapping form of X →T and T →Y are available in the Appendix B.\n3.3 TransMIL for Weakly Supervised WSI Classiﬁcation\nTo better describe the mapping of X →T, we design a TPT module with two Transformer layers\nand a position encoding layer, where Transformer layers are designed for aggregating morphological\ninformation and Pyramid Position Encoding Generator (PPEG) is designed for encoding spatial\ninformation. The overview of proposed Transformer based MIL (TransMIL) is shown in Figure 3.\nLong Instances Sequence Modelling with TPT. The sequences are from the feature embeddings\nin each WSI. The processing steps of the TPT module are shown in Algorithm 2, whereMSA denotes\nMulti-head Self-attention, MLP denotes Multilayer Perceptron, and LN denotes Layer Norm.\nAlgorithm 2: TPT module processing ﬂow\nInput: A bag of feature embeddings Hi = {hi,1,..., hi,n}, where hi,j ∈R1×d is the\nembedding of the jth instance, Hi ∈Rn×d\nOutput: Bag-level predicted label ˆYi\n1) Squaring of sequence;√\nN ←⌈√n⌉, M ←N −n, HS ←Concat (hi,class, Hi, (hi,1,..., hi,M)), where\nhi,class ∈R1×d represents class token, HS ∈R(N+1)×d;\n2) Correlation modelling of the sequence;\nHℓ\nS ←MSA (HS), where ℓdenotes the layer index of the Transformer, Hℓ\nS ∈R(N+1)×d;\n3) Conditional position encoding and local information fusion;\nHP\nS ←PPEG\n(\nHℓ\nS\n)\n, where HP\nS ∈R(N+1)×d;\n4) Deep feature aggregation;\nHℓ+1\nS ←MSA\n(\nHP\nS\n)\n, where Hℓ+1\nS ∈R(N+1)×d;\n5) Mapping of T →Y;\nˆYi ←MLP\n(\nLN\n(\n(Hℓ+1\nS )\n(0)))\n, where (Hℓ+1\nS )\n(0)\n∈R1×d represents class token.\n5\nFor most cases, the softmax used in Transformer for vision tasks such as [17, 18, 35] is a row-by-row\nsoftmax normalization function. The standard self-attention mechanism requires the calculation of\nsimilarity scores between each pair of tokens, resulting in both memory and time complexity of\nO(n2). To deal with the long instances sequence problem in WSIs, the softmax in TPT adopts the\nNystrom Method proposed in [22]. The approximated self-attention form ˆS can be deﬁned as:\nˆS = softmax\n(\nQ ˜KT\n√\ndq\n)(\nsoftmax\n(˜Q ˜KT\n√\ndq\n))+\nsoftmax\n(˜QKT\n√\ndq\n)\n, (9)\nwhere ˜Q and ˜K are the mselected landmarks from the original ndimensional sequence of Q and\nK, and A+ is a Moore-Penrose pseudoinverse of A. The ﬁnal computational complexity is reduced\nfrom O(n2) to O(n). By doing this, the TPT module with approximation processing can satisfy the\ncase where a bag contains thousands of tokens as input.\nPosition encoding with PPEG. In WSIs, the number of tokens in the corresponding sequence\noften varies due to the inherently variable size of the slide and tissue area. In [36] it is shown that the\nadding of zero padding can provide an absolute position information to convolution. Inspired by this,\nwe designed the PPEG module accordingly. The overview is shown in Figure 4, and the pseudo-code\nfor the processing is shown in Algorithm 3.\nFigure 4: Pyramid Position Encoding Generator. 1) The sequence is divided into patch tokens and\nclass token; 2) Patch tokens are reshaped into 2-D image space; 3) Different sized convolution kernels\nare used to encode spatial information; 4) Different spatial information are fused together; 5) Patch\ntokens are ﬂattened into sequence; 6) Connect class token and patch tokens.\nOur PPEG module has more advantages over the method proposed in [37]: (1) PPEG module uses\ndifferent sized convolution kernels in the same layer, which can encode the positional information\nwith different granularity, enabling high adaptability of PPEG. (2) Taking advantage of CNN’s ability\nto aggregate context information, the tokens in the sequence is able to obtain both global information\nand context information, which enriches the features carried by each token.\n4 Experiments and Results\nTo demonstrate the superior performance of the proposed TransMIL, various experiments were\nconducted over three public datasets: CAMELYON16, The Caner Genome Atlas (TCGA) non-small\ncell lung cancer (NSCLC), as well as the TCGA renal cell carcinoma (RCC).\nDataset. CAMELYON16 is a public dataset for metastasis detection in breast cancer, including\n270 training sets and 130 test sets. After pre-processing, a total of about 3.5 million patches at ×20\nmagniﬁcation, in average about 8,800 patches per bag were obtained.\nTCGA-NSCLC includes two subtype projects, i.e., Lung Squamous Cell Carcinoma (TGCA-LUSC)\nand Lung Adenocarcinoma (TCGA-LUAD), for a total of 993 diagnostic WSIs, including 507 LUAD\n6\nAlgorithm 3: PPEG processing ﬂow\nInput: A bag of feature embeddings Hℓ\nS after correlation modelling, where Hℓ\nS ∈R(N+1)×d.\nOutput: The feature embeddings HP\nS after conditional position encoding and local information\nfusion, where HP\nS ∈R(N+1)×d.\n1) Split: Hℓ\nS is divided into patch tokens Hf and class token Hc;\nHf,Hc ←Split\n(\nHℓ\nS\n)\n, where Hf ∈RN×d,Hc ∈R1×d;\n2) Spatial Restore: patch tokens Hf are reshaped to Hf\nS in the 2-D image space;\nHf\nS ←Restore (Hf), where Hf\nS ∈R\n√\nN×\n√\nN×d;\n3) Group Convolution: using a set of group convolutions with kernel kand k−1\n2 zero\npaddings(k= 3,5,7) to obtain Hf\nt,t = 1,2,3;\nHf\nt ←Conv\n(\nHf\nS\n)\n, where Hf\nt ∈R\n√\nN×\n√\nN×d,t = 1,2,3;\n4) Fusion: Hf\nS and the Hf\nt,t = 1,2,3 obtained from the convolution block processing are\nadded together to obtain HF\nS;\nHF\nS ←Hf\nS + Hf\n1 + Hf\n2 + Hf\n3 , where HF\nS ∈R\n√\nN×\n√\nN×d;\n5) Flatten: HF\nS are ﬂattened into sequence Hse;\nHse ←Flatten\n(\nHF\nS\n)\n, where Hse ∈RN×d;\n6) Concat: connect Hse and class token Hc to obtain HP\nS;\nHP\nS ←Concat (Hse,Hc), where HP\nS ∈R(N+1)×d.\nslides from 444 cases and 486 LUSC slides from 452 cases. After pre-processing, the mean number\nof patches extracted per slide at ×20 magniﬁcation is 15371.\nTCGA-RCC includes three subtype projects, i.e., Kidney Chromophobe Renal Cell Carcinoma\n(TGCA-KICH), Kidney Renal Clear Cell Carcinoma (TCGA-KIRC) and Kidney Renal Papillary Cell\nCarcinoma (TCGA-KIRP), for a total of 884 diagnostic WSIs, including 111 KICH slides from 99\ncases, 489 KIRC slides from 483 cases, and 284 KIRP slides from 264 cases. After pre-processing,\nthe mean number of patches extracted per slide at ×20 magniﬁcation is 14627.\nExperiment Setup and Evaluation Metrics. Each WSI is cropped into a series of 256 ×256 non-\noverlapping patches, where the background region (saturation<15) is discarded. In CAMELYON16\nwe trained on the ofﬁcial training set after splitting the 270 WSIs into approximately 90% training\nand 10% validation, and tested on the ofﬁcial test set. For TCGA datasets, we ﬁrst ensured that\ndifferent slides from one patient do not exist in both the training and test sets, and then randomly\nsplit the data in the ratio of training:validation:test = 60:15:25. For the evaluation metrics, we used\naccuracy and area under the curve (AUC) scores to evaluate the classiﬁcation performance, where the\naccuracy was calculated with a threshold of 0.5 in all experiments. For the AUC, the ofﬁcial test set\nAUC was used on the CAMELYON16 dataset, the average AUC was used on the TCGA-NSCLC\ndataset, and the average one-versus-rest AUC (macro-averaged) was used on the TCGA-RCC dataset.\nAll the results over TCGA datasets are obtained by 4-fold cross-validation.\nImplementation Details. In the training step, cross-entropy loss was adopted, and the Lookahead\noptimizer [38] was employed with a learning rate of 2e-4 and weight decay of 1e-5. The size of\nmini-batch Bis 1. As in [13], the feature of each patch is embedded in a 1024-dimensional vector\nby a ResNet50 [39] model pre-trained on ImageNet. During training, the dimension of each feature\nembedding is reduced from 1024 to 512 by a fully connected layer. Finally, the feature embedding of\neach bag can be represented as Hi ∈Rn×512. In the inference step, the softmax is used to normalize\nthe predicted scores for each class. All experiments are done with a RTX 3090.\nBaseline. The baselines we chose include deep models with traditional pooling operators such as\nmean-pooling, max-pooling and the current state-of-the-art deep MIL models [8, 9, 13, 23, 40], the\nattention based pooling operator ABMIL [8] and PT-MTA [40], non-local attention based pooling\noperator DSMIL [9], single-attention-branch CLAM-SB[13], multi-attention-branch CLAM-MB[13],\nand recurrent neural network(RNN) based aggregation MIL-RNN [23].\n7\nTable 1: Results on CAMELYON16, TCGA-NSCLC and TCGA-RCC.\nCAMELYON16 TCGA-NSCLC TCGA-RCC\nAccuracy AUC Accuracy AUC Accuracy AUC\nMean-pooling 0.6389 0.4647 0.7282 0.8401 0.9054 0.9786\nMax-pooling 0.8062 0.8569 0.8593 0.9463 0.9378 0.9879\nABMIL [8] 0.8682 0.8760 0.7719 0.8656 0.8934 0.9702\nPT-MTA [40] 0.8217 0.8454 0.7379 0.8299 0.9059 0.9700\nMIL-RNN [23] 0.8450 0.8880 0.8619 0.9107 \\ \\\nDSMIL [9] 0.7985 0.8179 0.8058 0.8925 0.9294 0.9841\nCLAM-SB [13] 0.8760 0.8809 0.8180 0.8818 0.8816 0.9723\nCLAM-MB [13] 0.8372 0.8679 0.8422 0.9377 0.8966 0.9799\nTransMIL 0.8837 0.9309 0.8835 0.9603 0.9466 0.9882\n4.1 Results on WSI classiﬁcation\nWe will present the results of both binary and multiple classiﬁcation. The binary classiﬁcation tasks\ncontain positive/negative classiﬁcation over CAMELYON16 and LUSC/LUAD subtypes classiﬁcation\nover TCGA-NSCLC. The multiple classiﬁcation refers to TGCA-KICH/TCGA-KIRC/TCGA-KIRP\nsubtypes classiﬁcation over TCGA-RCC. All the results are provided in Table 1.\nIn CAMELYON16, only a small portion of each positive slide contains tumours (averagely total\ncancer area per slide <10%), resulting in the presence of a large number of negative regions disturbing\nthe prediction of positive slide. The bypass attention-based methods and proposed TransMIL all\noutperform the traditional pooling operators. However in AUC score, TransMIL was at least 5 %\nhigher than ABMIL, PT-MTA and CLAM which neglect the correlation between instances, and do\nnot consider the spatial information between patches. DSMIL only considers the relationship between\nthe highest scoring instance and others, leading to limited performance.\nIn TCGA-NSCLC, positive slides contain relatively large areas of tumour region (averagely total\ncancer area per slide >80 %), consequently the pooling operator can achieve better performance\nthan in CAMELYON16. Again, TransMIL performed better than all the other competing methods,\nachieving 1.40% higher in AUC and 2.16% in accuracy, compared with the second best method.\nIn TCGA-RCC, as MIL-RNN did not consider the multi-classiﬁcation problem, it was not included\nin this comparison result. The TCGA-RCC is unbalanced distributed in cancer subtypes and has large\nareas of tumour region in the positive slides (averagely total cancer area per slide >80%). However,\nTransMIL is equally applicable to multi-class problems with unbalanced data. It can be observed that\nTransMIL achieves best results in both accuracy and AUC score.\n4.2 Ablation Study\nTo further determine the contribution of the PPEG module and the conditional position encoding\nfor the performance, we have conducted a series of ablation studies. Since the high classiﬁcation\naccuracy of most methods over TCGA-RCC is not obvious, all ablation study experiments are based\non the CAMELYON16 and the TCGA-NSCLC dataset. All experiments were evaluated by AUC.\n4.2.1 Effects of PPEG\nThe position encoding of the Transformer typically explores absolute position encoding (e.g., sinu-\nsoidal encoding, learnable absolute encoding) as well as conditional position encoding. However,\nlearnable absolute encoding is commonly used in problems with ﬁxed length sequences, and does\nnot meet the requirement for variable length of input sequences in WSI analysis, so it is not taken\ninto account in this paper. Here, we compared the effect of sinusoidal encoding and PPEG module\nwhich represents multi-level conditional position encoding. The same experiments are performed\nover CAMELYON16 and TCGA-NSCLC dataset, and the results are shown in Table 2. It should be\nnoted that sinusoidal encoding is added to the original sequence with a multiplication of 0.001 as\ndescribed in [33].\n8\nTable 2: Effects of PPEG.\nModel Params Camelyon16 NSCLC\nw/o 2.625M 0.8416 0.9287\nsin-cos 2.625M 0.8941 0.9374\n3×3 2.630M 0.8913 0.9355\n7×7 2.651M 0.9015 0.9336\nboth 2.669M 0.9059 0.9402\nPPEG 2.669M 0.9309 0.9603\nFigure 5: Effects of Positional Encoding.\nCompared with the model without position encoding, it can be seen that both sinusoidal encoding and\nconditional position encoding can improve the classiﬁcation performance, and conditional position\ninformation encoded by PPEG can be more effective in diagnosis analysis. In contrast to the 3 ×3\nand 7 ×7 convolutional block, adding different sized convolution kernels in the same layer allows for\nmulti-level positional encoding and adds more context information to each token.\n4.2.2 Effects of Conditional Position Encoding\nHere, by disrupting the order of the input sequences, we explore actual improvements for conditional\nposition encoding. The performance of the model under different conﬁgurations is shown in Figure 5,\nwhere order represents sequential data input and w/o represents random and disordered data input.\nIt can be seen that conditional position information did enhance the model performance, e.g., the\nimprovement can be up to 0.9% over CAMELYON16 and 0.61% over TCGA-NSCLC in AUC.\nCompared with the model without position encoding or with sinusoidal encoding, conditional position\ninformation encoded by PPEG can be more effective in diagnosis analysis. Compared with the results\ntrained over the sequential and disordered training sets, conditional position information did enhance\nthe model performance, e.g., the improvement can be up to 0.9% over CAMELYON16 and 0.61%\nover TCGA-NSCLC in AUC.\n4.3 Interpretability and Attention Visualization\nHere, we will further show the interpretability of TransMIL. As shown in Figure 6(a), the area\nwithin the blue curve annotation is the cancer region, which was provided by Gao et al. [41] over the\nTCGA-RCC dataset. In Figure 6(b), attention scores from TransMIL were visualised as a heatmap to\ndetermine the ROI and interpret the important morphology used for diagnosis, and Figure 6(c) is a\nzoomed-in view of the black square in Figure 6(b). Obviously, there is a high consistency between\nﬁne annotation area and heatmap, illustrating great interpretability and attention visualization of the\nproposed TransMIL.\nFigure 6: Interpretability and visualization.\n9\nFigure 7: The convergence comparison of TransMIL and the competing methods.\n4.4 Fast Convergence\nTraditional MIL methods as well as the latest MIL methods such as ABMIL, DSMIL and CLAM\nusually require a large number of epochs to converge. Different from these methods, TransMIL\nmakes use of the morphological and spatial information among instances, leading to approximately\ntwo to three times fewer training epochs. As shown in Figure 7, TransMIL has better performance in\nterms of convergence and validation AUC than other MIL methods.\n5 Conclusion\nIn this paper, we have developed a novel correlated MIL framework that is consistent with the behavior\nof pathologists considering both the contextual information around a single area and the correlation\nbetween different areas when making a diagnostic decision. Based on this framework, a Transformer\nbased MIL (TransMIL) was devised to explore both morphological and spatial information in weakly\nsupervised WSI classiﬁcation. We also design a PPEG for position encoding as well as a TPT module\nwith two Transformer layers and a position encoding layer. The TransMIL network is easy to train,\nand can be applied to unbalanced/balanced and binary/multiple classiﬁcation with great visualization\nand interpretability. Most importantly, TransMIL outperforms the state-of-the-art MIL algorithms\nin terms of both AUC and accuracy over three public datasets. Currently, all the experiments were\nconducted over the dataset with ×20 magniﬁcation, however the WSIs with higher magniﬁcation\nwill result in longer sequence and inevitably pose great challenges in terms of both computational\nand memory requirements, and we will explore this issue in the follow-up work.\nBroader Impact Our proposed approach shows greater potential for MIL application to real-world\ndiagnosis analysis, particularly in problems that require more correlated information such as survival\nanalysis and cancer cell spread detection. In the short term, the beneﬁt of this work is to provide a\nmodel with better performance, faster convergence and clinical interpretability. In the long term, the\nproposed TransMIL network is more applicable to real situations, and it is hoped that it will provide\nmore novel and effective ideas about further applications of deep MIL to diagnosis analysis.\nAcknowledgment This work was supported in part by the National Natural Science Founda-\ntion of China (61922048&62031023), in part by the Shenzhen Science and Technology Project\n(JCYJ20200109142808034), and in part by Guangdong Special Support (2019TX05X187).\nReferences\n[1] Lei He, L Rodney Long, Sameer Antani, and George R Thoma. Histology image analysis\nfor carcinoma detection and grading. Computer methods and programs in biomedicine, pages\n538–556, 2012.\n[2] Anant Madabhushi. Digital pathology image analysis: opportunities and challenges. Imaging\nin medicine, pages 7–10, 2009.\n[3] Chen Li, Xintong Li, Md Rahaman, Xiaoyan Li, Hongzan Sun, Hong Zhang, Yong Zhang,\nXiaoqi Li, Jian Wu, Yudong Yao, et al. A comprehensive review of computer-aided whole-slide\n10\nimage analysis: from datasets to feature extraction, segmentation, classiﬁcation, and detection\napproaches. arXiv preprint arXiv:2102.10553, 2021.\n[4] Xiaomin Zhou, Chen Li, Md Mamunur Rahaman, Yudong Yao, Shiliang Ai, Changhao Sun,\nQian Wang, Yong Zhang, Mo Li, Xiaoyan Li, et al. A comprehensive review for breast\nhistopathology image analysis using classical and deep neural networks. IEEE Access, pages\n90931–90956, 2020.\n[5] Chetan L Srinidhi, Ozan Ciga, and Anne L Martel. Deep neural network models for computa-\ntional histopathology: A survey. Medical Image Analysis, 2020.\n[6] Xinggang Wang, Yongluan Yan, Peng Tang, Xiang Bai, and Wenyu Liu. Revisiting multiple\ninstance neural networks. Pattern Recognition, pages 15–24, 2018.\n[7] Fahdi Kanavati, Gouji Toyokawa, Seiya Momosaki, Michael Rambeau, Yuka Kozuma, Fumi-\nhiro Shoji, Koji Yamazaki, Sadanori Takeo, Osamu Iizuka, and Masayuki Tsuneki. Weakly-\nsupervised learning for lung carcinoma classiﬁcation using deep learning. Scientiﬁc reports,\npages 1–11, 2020.\n[8] Maximilian Ilse, Jakub M. Tomczak, and M. Welling. Attention-based deep multiple instance\nlearning. In International Conference on Machine Learning, pages 2127–2136, 2018.\n[9] Bin Li, Yin Li, and Kevin W Eliceiri. Dual-stream multiple instance learning network for whole\nslide image classiﬁcation with self-supervised contrastive learning. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, 2021.\n[10] Naofumi Tomita, Behnaz Abdollahi, Jason Wei, Bing Ren, A. Suriawinata, and S. Hassanpour.\nAttention-based deep neural networks for detection of cancerous and precancerous esophagus\ntissue on histopathological slides. JAMA Network Open, 2019.\n[11] Noriaki Hashimoto, Daisuke Fukushima, Ryoichi Koga, Yusuke Takagi, Kaho Ko, Kei Kohno,\nMasato Nakaguro, Shigeo Nakamura, Hidekata Hontani, and Ichiro Takeuchi. Multi-scale\ndomain-adversarial multiple-instance cnn for cancer subtype classiﬁcation with unannotated\nhistopathological images. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 3852–3861, 2020.\n[12] Nikhil Naik, Ali Madani, Andre Esteva, Nitish Shirish Keskar, Michael F Press, Daniel Ru-\nderman, David B Agus, and Richard Socher. Deep learning-enabled breast cancer hormonal\nreceptor status determination from base-level h&e stains. Nature communications, pages 1–8,\n2020.\n[13] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and\nFaisal Mahmood. Data-efﬁcient and weakly supervised computational pathology on whole-slide\nimages. Nature Biomedical Engineering, pages 1–16, 2021.\n[14] Yash Sharma, Aman Shrivastava, Lubaina Ehsan, Christopher A. Moskaluk, Sana Syed, and\nDonald E. Brown. Cluster-to-conquer: A framework for end-to-end multi-instance learning for\nwhole slide image classiﬁcation. arXiv preprint arXiv:2103.10626, 2021.\n[15] Ming Tu, Jing Huang, Xiaodong He, and Bowen Zhou. Multiple instance learning with graph\nneural networks. In International Conference on Machine Learning, 2019.\n[16] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and\nSergey Zagoruyko. End-to-end object detection with transformers. In Proceedings of the\nEuropean Conference on Computer Vision, pages 213–229, 2020.\n[17] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable\ndetr: Deformable transformers for end-to-end object detection. In International Conference on\nLearning Representations, 2021.\n[18] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan Adeli, Yan Wang, Le Lu, Alan L\nYuille, and Yuyin Zhou. Transunet: Transformers make strong encoders for medical image\nsegmentation. arXiv preprint arXiv:2102.04306, 2021.\n[19] Yundong Zhang, Huiye Liu, and Qiang Hu. Transfuse: Fusing transformers and cnns for\nmedical image segmentation. arXiv preprint arXiv:2102.08005, 2021.\n[20] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yiping Deng, Zhenhua Liu, Siwei Ma,\nChunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. arXiv preprint\narXiv:2012.00364, 2020.\n11\n[21] Fuzhi Yang, Huan Yang, J. Fu, Hongtao Lu, and B. Guo. Learning texture transformer network\nfor image super-resolution. In Proceedings of the IEEE Conference on Computer Vision and\nPattern Recognition, pages 5790–5799, 2020.\n[22] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li,\nand Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention.\nIn Proceedings of the AAAI Conference on Artiﬁcial Intelligence, 2021.\n[23] Gabriele Campanella, Matthew G Hanna, Luke Geneslaw, Allen Miraﬂor, Vitor Werneck Krauss\nSilva, Klaus J Busam, Edi Brogi, Victor E Reuter, David S Klimstra, and Thomas J Fuchs.\nClinical-grade computational pathology using weakly supervised deep learning on whole slide\nimages. Nature medicine, pages 1301–1309, 2019.\n[24] G. Xu, Zhigang Song, Zhuo Sun, Calvin Ku, Z. Yang, C. Liu, S. Wang, Jianpeng Ma, and\nW. Xu. Camel: A weakly supervised learning framework for histopathology image segmentation.\nIn Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages\n10681–10690, 2019.\n[25] Marvin Lerousseau, Maria Vakalopoulou, Marion Classe, Julien Adam, Enzo Battistella, Alexan-\ndre Carré, Théo Estienne, Théophraste Henry, Eric Deutsch, and Nikos Paragios. Weakly\nsupervised multiple instance learning histopathological tumor segmentation. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 470–479,\n2020.\n[26] P. Chikontwe, Meejeong Kim, S. Nam, H. Go, and S. Park. Multiple instance learning with\ncenter embeddings for histopathology classiﬁcation. In International Conference on Medical\nImage Computing and Computer-Assisted Intervention, pages 519–528, 2020.\n[27] Xi Wang, Hao Chen, Caixia Gan, Huangjing Lin, Qi Dou, Efstratios Tsougenis, Qitao Huang,\nMuyan Cai, and Pheng-Ann Heng. Weakly supervised deep learning for whole slide lung cancer\nimage analysis. IEEE Transactions on cybernetics, pages 3950–3962, 2019.\n[28] Chensu Xie, Hassan Muhammad, Chad M Vanderbilt, Raul Caso, Dig Vijay Kumar Yarla-\ngadda, Gabriele Campanella, and Thomas J Fuchs. Beyond classiﬁcation: Whole slide tissue\nhistopathology analysis by end-to-end part learning. In Medical Imaging with Deep Learning,\npages 843–856, 2020.\n[29] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. In International Conference on Learning Representations, 2015.\n[30] Jie Hu, L. Shen, and G. Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE\nConference on Computer Vision and Pattern Recognition, pages 7132–7141, 2018.\n[31] S. Woo, Jongchan Park, Joon-Young Lee, and In-So Kweon. Cbam: Convolutional block\nattention module. In Proceedings of the European Conference on Computer Vision, pages 3–19,\n2018.\n[32] Yongming Rao, Jiwen Lu, and J. Zhou. Attention-aware deep reinforcement learning for video\nface recognition. In IEEE International Conference on Computer Vision, pages 3931–3940,\n2017.\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nundeﬁnedukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, pages 5998–6008, 2017.\n[34] Yanhong Zeng, Jianlong Fu, and Hongyang Chao. Learning joint spatial-temporal transforma-\ntions for video inpainting. In Proceedings of the European Conference on Computer Vision,\npages 528–543, 2020.\n[35] A. Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas\nUnterthiner, M. Dehghani, Matthias Minderer, G. Heigold, S. Gelly, Jakob Uszkoreit, and\nN. Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In\nInternational Conference on Learning Representations, 2021.\n[36] Md Amirul Islam, Sen Jia, and Neil D. B. Bruce. How much position information do convo-\nlutional neural networks encode? In International Conference on Learning Representations,\n2020.\n12\n[37] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and\nChunhua Shen. Conditional positional encodings for vision transformers. arXiv preprint\narXiv:2102.10882, 2021.\n[38] Michael Ruogu Zhang, James Lucas, Geoffrey E. Hinton, and Jimmy Ba. Lookahead optimizer:\nk steps forward, 1 step back. In Advances in Neural Information Processing Systems, pages\n9597–9608, 2019.\n[39] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for im-\nage recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, pages 770–778, 2016.\n[40] Weijian Li, Viet-Duy Nguyen, Haofu Liao, Matt Wilder, Ke Cheng, and Jiebo Luo. Patch\ntransformer for multi-tagging whole slide histopathology images. In International Conference\non Medical Image Computing and Computer-Assisted Intervention, pages 532–540, 2019.\n[41] Zeyu Gao, Pargorn Puttapirat, Jiangbo Shi, and Chen Li. Renal cell carcinoma detection\nand subtyping with minimal point-based annotation in whole-slide images. In International\nConference on Medical Image Computing and Computer-Assisted Intervention, pages 439–448,\n2020.\n13\nA Appendix A\nInference Suppose S : X →R is a continuous set function w.r.t Hausdorff distance dH(·,·).\n∀ε> 0, for any function f and any invertible map P : X→ Rn,∃function hand g, such that for\nany X ∈X:\n|S(X) −g( P\nX∈X\n{f(x) + h(x) : x ∈X})|<ε. (10)\nThat is: a Hausdorff continuous function S(X) can be arbitrarily approximated by a function in the\nform g( P\nX∈X\n{f(x) + h(x) : x ∈X}).\nProof. As shown in Theorem 1, ∀ε> 0, for any invertible map P : X→ Rn,∃function σ,g, such\nthat for any X ∈X:\n|S(X) −g( P\nX∈X\n{σ(x) : x ∈X})|<ε. (11)\nFor any function f(x), take h(x) = σ(x) −f(x), one can derive that:\n|S(X) −g( P\nX∈X\n{f(x) + h(x) : x ∈X})|<ε. (12)\nThis means S(X) can be arbitrarily approximated by a function in the form g( P\nX∈X\n{f(x) + h(x) :\nx ∈X}) .\nThis completes the proof.\nTheorem 2. The Instances in the bag are represented by random variables Θ1,Θ2,..., Θn,\nthe information entropy of the bag under the correlation assumption can be expressed as\nH(Θ1,Θ2,..., Θn), and the information entropy of the bag under the i.i.d. (independent and\nidentical distribution) assumption can be expressed as ∑n\nt=1 H(Θt), then we have:\nH(Θ1,Θ2,..., Θn) =\nn∑\nt=2\nH(Θt |Θ1,..., Θt−1) + H(Θ1) ≤\nn∑\nt=1\nH(Θt) . (13)\nProof. The Instance in the data source is represented by random variables Θ1,..., Θn and the joint\ndistribution function is p(θ1,...,θ n). The information entropy of data source under correlation\nassumption can be expressed as H(Θ1,..., Θn), then we have:\nH(Θ1,..., Θn) = −\n∑\nθ1,...,θn∈ϑn\np(θ1,...,θ n) logp(θ1,...,θ n)\n= −\n∑\nθ1,...,θn∈ϑn\np(θ1,...,θ n) log [p(θ1,...,θ n−1) p(θn |θ1,...,θ n−1)]\n= −\n∑\nθ1,...,θn∈ϑn\np(θ1,...,θ n) log\n{[ n∏\nt=2\np(θt |θ1,...,θ t−1)\n]\np(θ1)\n}\n= −\nn∑\nt=2\n\n ∑\nθ1,...,θn∈ϑn\np(θ1,...,θ t) logp(θt |θ1,...,θ t−1)\n\n−\n∑\nθ1∈ϑ\np(θ1) logp(θ1)\n=\nn∑\nt=2\nH(Θt |Θ1,..., Θt−1) + H(Θ1) ≤\nn∑\nt=1\nH(Θt)\n(14)\nHere\nn∑\nt=1\nH(Θt) is the information entropy of the data source under the i.i.d. assumption. Therefore,\nit is proved that the information source under the correlation assumption has smaller information\nentropy. In other words, correlation assumption reduces the uncertainty and brings more useful\ninformation.\nThis completes the proof.\n14\nB Appendix B\nTransformer based MIL. Given a set of bags {X1,X2,..., Xb}, and each bag Xi contains\nmultiple instances {xi,1,xi,2,..., xi,n}and a corresponding label Yi. The goal is to learn the\nmappings: X →T →Y,where X is the bag space, T is the Transformer space and Yis the label\nspace. The mapping of X →T can be deﬁned as:\nX0\ni = [xi,class; f(xi,1); f(xi,2); ... ; f(xi,n)] + Epos, X0\ni, Epos ∈R(n+1)×d (15)\nQℓ = Xℓ−1\ni WQ, Kℓ = Xℓ−1\ni WK, Vℓ = Xℓ−1\ni WV, ℓ = 1 ...L (16)\nhead = SA(Qℓ,Kℓ,Vℓ) = softmax\n(\nQℓ(\nKℓ)T\n√\ndq\n)\nVℓ, ℓ = 1 ...L (17)\nMSA(Qℓ,Kℓ,Vℓ) = Concat(head1,head2,..., headh)WO, ℓ = 1 ...L (18)\nXℓ\ni = MSA(LN(Xℓ−1\ni )) + Xℓ−1\ni , ℓ = 1 ...L (19)\nwhere WQ ∈Rd×dq , WK ∈Rd×dk , WV ∈Rd×dv ,WO ∈Rhdv×d, head ∈R(n+1)×dv , SA\ndenotes Self-attention layer, Lis the number of MSA block[ 35], his the number of head in each\nMSA block and Layer Normalization (LN) is applied before every MSA block.\nThe mapping of T →Y can be deﬁned as:\nYi = MLP(LN((XL\ni )\n(0)\n)), (20)\nwhere (XL\ni )\n(0)\nrepresents class token. The mapping of T →Y can be ﬁnished by using class token or\nglobal averaging pooling. Obviously, the key to Transformer based MIL is how to design the mapping\nof X →T. However, there are many difﬁculties to directly apply Transformer in WSI classiﬁcation,\nincluding the large number of instances in each bag and the large variation in the number of instances\nin different bags (e.g., ranging from hundreds to thousands). In this paper we focus on how to devise\nan efﬁcient Transformer to better model the instance sequence.\n15",
  "topic": "Interpretability",
  "concepts": [
    {
      "name": "Interpretability",
      "score": 0.9038518667221069
    },
    {
      "name": "Computer science",
      "score": 0.6617130637168884
    },
    {
      "name": "Binary classification",
      "score": 0.6194454431533813
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5648642182350159
    },
    {
      "name": "Machine learning",
      "score": 0.5024747848510742
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.5003266334533691
    },
    {
      "name": "Visualization",
      "score": 0.44738391041755676
    },
    {
      "name": "Data mining",
      "score": 0.37294915318489075
    },
    {
      "name": "Support vector machine",
      "score": 0.07821676135063171
    }
  ]
}