{
  "title": "Lifting the Curse of Multilinguality by Pre-training Modular Transformers",
  "url": "https://openalex.org/W4287890953",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2655736194",
      "name": "Jonas Pfeiffer",
      "affiliations": [
        "Technical University of Darmstadt",
        "New York University"
      ]
    },
    {
      "id": "https://openalex.org/A2507176056",
      "name": "Naman Goyal",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106187023",
      "name": "Xi LIN",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096862909",
      "name": "Xian Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1965196160",
      "name": "James Cross",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1976791985",
      "name": "Sebastian Riedel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2176075802",
      "name": "Mikel Artetxe",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971350322",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W3100198908",
    "https://openalex.org/W4293718192",
    "https://openalex.org/W3099771192",
    "https://openalex.org/W3108406411",
    "https://openalex.org/W3207937903",
    "https://openalex.org/W3103727211",
    "https://openalex.org/W3173788106",
    "https://openalex.org/W3173954987",
    "https://openalex.org/W3174418826",
    "https://openalex.org/W4287633642",
    "https://openalex.org/W4206178588",
    "https://openalex.org/W3214578205",
    "https://openalex.org/W2742113707",
    "https://openalex.org/W3190540921",
    "https://openalex.org/W4205712089",
    "https://openalex.org/W3096966601",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2972788276",
    "https://openalex.org/W2995549860",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W3034469191",
    "https://openalex.org/W3101286153",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2964114970",
    "https://openalex.org/W3112302586",
    "https://openalex.org/W3104723404",
    "https://openalex.org/W2963143606",
    "https://openalex.org/W3105421296",
    "https://openalex.org/W4322588812",
    "https://openalex.org/W3169244955",
    "https://openalex.org/W3166567197",
    "https://openalex.org/W3035547806",
    "https://openalex.org/W4230579319",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W3105190698",
    "https://openalex.org/W2995230342",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4286901653",
    "https://openalex.org/W3035497479",
    "https://openalex.org/W4205717708",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3100353583",
    "https://openalex.org/W3212651325",
    "https://openalex.org/W3099219382",
    "https://openalex.org/W2963165489",
    "https://openalex.org/W3098466758",
    "https://openalex.org/W3172698324",
    "https://openalex.org/W3206907172",
    "https://openalex.org/W3013840636",
    "https://openalex.org/W2970854433",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W3154311556",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W4236950558",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2962945654",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W3200245893"
  ],
  "abstract": "Jonas Pfeiffer, Naman Goyal, Xi Lin, Xian Li, James Cross, Sebastian Riedel, Mikel Artetxe. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3479 - 3495\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nLifting the Curse of Multilinguality\nby Pre-training Modular Transformers\nJonas Pfeiffer∗1,2,3, Naman Goyal 3, Xi Victoria Lin3, Xian Li 3,\nJames Cross3, Sebastian Riedel 3, Mikel Artetxe 3\n1New York University,2TU Darmstadt,\n3Meta AI\nAbstract\nMultilingual pre-trained models are known to\nsuffer from the curse of multilinguality, which\ncauses per-language performance to drop as\nthey cover more languages. We address this is-\nsue by introducing language-speciﬁc modules,\nwhich allows us to grow the total capacity of\nthe model, while keeping the total number of\ntrainable parameters per language constant. In\ncontrast with prior work that learns language-\nspeciﬁc components post-hoc, we pre-train the\nmodules of our Cross-lingual Modular ( X-\nMOD) models from the start. Our experiments\non natural language inference, named entity\nrecognition and question answering show that\nour approach not only mitigates the negative\ninterference between languages, but also en-\nables positive transfer, resulting in improved\nmonolingual and cross-lingual performance.\nFurthermore, our approach enables adding lan-\nguages post-hoc with no measurable drop in\nperformance, no longer limiting the model us-\nage to the set of pre-trained languages.\n1 Introduction\nRecent work on multilingual NLP has focused on\npre-training transformer-based models (Vaswani\net al., 2017) on concatenated corpora of a large\nnumber of languages (Devlin et al., 2019; Conneau\net al., 2020). These multilingual models have been\nshown to work surprisingly well in cross-lingual\nsettings, despite the fact that they do not rely on\ndirect cross-lingual supervision (e.g., parallel data\nor translation dictionaries; Pires et al., 2019; Wu\nand Dredze, 2019; Artetxe et al., 2020; Hu et al.,\n2020; K et al., 2020; Rust et al., 2021).\nHowever, recent work has uncovered fundamen-\ntal limitations of multilingual transformers. Con-\nneau et al. (2020) observe that pre-training a model\nwith a ﬁxed capacity on an increasing amount of\nlanguages only improves its cross-lingual perfor-\nmance up to a certain point, after which perfor-\n∗ Work done while interning at Meta AI.\nFeed\nForward\nAdd & Norm\nMulti-Head\nAttention\nAdd & Norm\nFF Down\nFF Up\nAdd & Norm\nFF Down\nFF Up\nLannguage n\nLannguage 1\n...\nFigure 1: A transformer layer of our proposed modular\narchitecture. The dark blue and green components illus-\ntrate the modular layers, which are language speciﬁc.\nThe Multi-Head Attention and Feed-Forward compo-\nnents are shared by all languages.\nmance drops can be measured—a phenomenon\nknown as the curse of multilinguality (Figure 2).\nAs such, prior work had to ﬁnd a trade-off between\nsupporting more languages and obtaining better\nperformance on a smaller set of languages.\nIn this work, we address this problem by in-\ntroducing language-speciﬁc, modular components\nduring pre-training (Figure 1). Our Cross-lingual,\nModular (X-M OD) language model shares the ma-\njority of the transformer parameters between all pre-\ntraining languages, while providing each language\nwith individual capacity to learn idiosyncratic in-\nformation without increasing the total number of\ntrainable parameters per language. While previous\nadapter-based approaches (Figure 3a) extend pre-\ntrained multilingual language models (LMs) with\nmodular components after pre-training, we add\nmodular components during pre-training, thereby\n3479\n10 20 30 40 50 60 70\n# Languages\n3.75\n4.00\n4.25Perplexity\nX-Mod shared\n(a) Mean Perplexity.\n10 20 30 40 50 60 70\n# Languages\n64\n65\n66Mean Score (F1/Acc)\nX-Mod shared (b) Mean Performance on XNLI and NER.\nFigure 2: Average (a) perplexity and (b) transfer performance on XNLI and NER across pre-trained languages\nwhen training on an increasing number of languages. Each model has seen the same amount of examplesin\neach language. Lower perplexity and higher downstream score indicate better performance. Refer to Figure 4 for\nper-task performance, and Appendix A for per-language performance.\npreparing the model to be extended to new lan-\nguages post-hoc. Our experiments on natural lan-\nguage inference (NLI), named entity recognition\n(NER), and question answering (QA) demonstrate\nthat our modular architecture not only is effective at\nmitigating interference between languages, but also\nachieves positive transfer, resulting in improved\nmonolingual and cross-lingual performance. In ad-\ndition, we show that X-M OD can be extended to\nunseen languages, with no measurable drop in per-\nformance, by learning its corresponding modules\nand leaving the shared parameters frozen. All in\nall, we propose a multilingual architecture that can\nscale to a large number of languages without any\nloss in performance, and can be further extended\nto new languages after pre-training.1\n2 Background and related work\nWe provide a background on multilingual and mod-\nular language modelling, as well as approaches that\nextend LMs to new languages.\n2.1 Multilingual transformers\nRecent LMs (Devlin et al., 2019; Conneau et al.,\n2020), based on transformer architectures (Vaswani\net al., 2017) and pre-trained on massive amounts\nof multilingual data, have surpassed (static) cross-\nlingual word embedding spaces (Ruder et al., 2019;\nGlavas et al., 2019) for cross-lingual transfer in\nNLP (Pires et al., 2019; Wu and Dredze, 2019;\nWu et al., 2020; Hu et al., 2020; K et al., 2020).\nTransformer-based models are 1) pre-trained on\ntextual corpora using Masked Language Modelling\n1Code and pre-trained models are available at:\nhttps://github.com/pytorch/fairseq/tree/main/examples/xmod.\nEmb\nHead\n...\nEmb\nHead\n...\nEmb\nHead ...\nEmb\nHead ...\n1 2 1 2\n(a) Adapter-based\nEmb\nHead\n...\nEmb\nHead\n...\nEmb\nHead ...\nEmb\nHead ...\n1 2 1 2 (b) X-M OD\nFigure 3: Our proposed architecture in comparison\nto adapter-based approaches. (a) Previous approaches\nx utilize non-modular pre-trained transformer models\nand y extend them with modular adapter components.\n(b) We x pre-train the transformer with modular units\nfrom the get-go, preparing the model to be y extended\nwith additional modular units later on. Yellow and\nlight blue components indicate standard Multi-Head\nAttention and Feed-Forward layers. The remaining\n(non-gray) components are bottleneck (modular) units.\nGrayed-out components are frozen.\n(MLM). They are then 2) ﬁne-tuned on labelled\ndata of a downstream task in asource language and\n3) directly applied to perform inference in a target\nlanguage (Hu et al., 2020).\n2.2 Modular language models\nModular approaches have a long standing history\nin NLP, preceding pre-trained models (Andreas\net al., 2016). They have recently re-gained in-\nterest for transformer-based models, where mix-\n3480\nture of experts (MoE; Shazeer et al., 2017) ap-\nproaches have enabled training trillion parame-\nters models in a distributed fashion (Fedus et al.,\n2021). More recently modular MoE approaches\nhave been shown to improve domain-speciﬁc pre-\ntraining of LMs (Gururangan et al., 2021). In a\nsimilar trend, ‘expert’ modules have been added\nto (non-modular) pre-trained LMs post-hoc, pre-\ndominantly referred to as adapters (Rebufﬁ et al.,\n2017, 2018; Houlsby et al., 2019). Next to being ex-\ntremely parameter (Houlsby et al., 2019; Mahabadi\net al., 2021a; He et al., 2022) and training efﬁcient\n(Pfeiffer et al., 2020a; Rücklé et al., 2021), these\nmodular approaches allow models to be extended\nto new data settings (Chen et al., 2019; Rücklé\net al., 2020), where newly learned knowledge can\nbe combined (Stickland and Murray, 2019; Wang\net al., 2021a; Pfeiffer et al., 2021a; Lauscher et al.,\n2020a; Mahabadi et al., 2021b; Poth et al., 2021),\nor stacked for combinatory cross-lingual (Pfeiffer\net al., 2020b, 2021b; Üstün et al., 2020; Vidoni\net al., 2020; Ansell et al., 2021b,a; Wang et al.,\n2021b) as well as NMT scenarios (Bapna and Fi-\nrat, 2019; Philip et al., 2020; Chronopoulou et al.,\n2020; Le et al., 2021; Üstün et al., 2021; Stickland\net al., 2021; Garcia et al., 2021).\n2.3 Weaknesses, improvements, and\nextensions of language models\nNext to the curse of multilinguality, recent works\nhave shown substantially reduced cross-lingual and\nmonolingual abilities of models for low-resource\nlanguages with smaller pre-training data (Wu and\nDredze, 2020; Hu et al., 2020; Lauscher et al.,\n2020b; Artetxe et al., 2020; Pfeiffer et al., 2020b,\n2021b; Chau et al., 2020b; Ponti et al., 2020).\nK et al. (2020); Artetxe et al. (2020) show that a\nshared vocabulary is not necessary for cross-lingual\ntransfer. Chung et al. (2021) demonstrate that de-\ncoupling the input embeddings from the predic-\ntion head improves the performance on a number\nof downstream tasks. Dufter and Schütze (2020)\nshow that the number of parameters and training\nduration is interlinked with the model’s multilin-\ngual capability. Chung et al. (2020); Rust et al.\n(2021) show that the tokenizer plays an important\nrole in the per-language downstream task perfor-\nmance, which Clark et al. (2022); Xue et al. (2022);\nTay et al. (2021) take to the extreme by proposing\ntokenizer-free approaches.\nTo extend a monolingual LM to other languages,\nArtetxe et al. (2020) train a new embedding layer\nwith a corresponding target-language tokenizer,\nwhile freezing the pre-trained transformer weights.\nTran (2020) extend a monolingual model to new\nlanguages using bilingual corpora. Wang et al.\n(2020); Chau et al. (2020a) extend the vocabu-\nlary of multilingual models with a small number\nof target-language tokens, to improve the perfor-\nmance in the target language. Muller et al. (2021)\npropose a transliteration based approach, Vernikos\nand Popescu-Belis (2021) propose subword map-\npings, and Pfeiffer et al. (2020b, 2021b); Vidoni\net al. (2020); Ansell et al. (2021b) propose adapter-\nbased approaches to extend multilingual models to\nunseen languages.\nWhile these approaches achieve considerable\nperformance gains over unseen languages, they are\noutperformed by standard full ﬁne-tuning methods\nfor seen languages. One can further argue that, as\nthe pre-trained models have already been cursed by\nmultilinguality, the adapter-based approaches build\nupon sub-optimal parameter initializations.2 In our\nwork, we consequently aim to 1) modularize the\nmodel from the start to prepare the model to be 2)\nextendable to new languages post-hoc.\n3 Proposed approach\nWe propose X-M OD, a modular multilingual archi-\ntecture that combines shared and language-speciﬁc\nparameters. In contrast to prior work, we pre-\ntrain modular models from the get-go. Our mod-\nels can be extended to new languages after pre-\ntraining, and used for cross-lingual transfer learn-\ning in downstream tasks.\nArchitecture. As illustrated in Figure 1, we\nextend the transformer-based architecture from\nmBERT (Devlin et al., 2019) and XLM-R (Con-\nneau et al., 2020) by incorporating language-\nspeciﬁc modules—bottleneck feed-forward layers—\nat every transformer layer. We learn a separate\nmodule for each language, whereas the attention\nand feed-forward components are shared. While\nthe total number of parameters of the model grows\nlinearly with the number of languages, the train-\ning and inference cost does not increase (as mea-\nsured in FLOPs), as only the module in the relevant\nlanguage is used for each input. Inspired by the\nadapter3 architecture of Pfeiffer et al. (2021a) we\n2We investigate this claim further in §6.2.\n3The term ‘adapter’ refers to newly introduced layers\nwithin a pre-trained (frozen) model. These layers adapt the\n3481\nplace our ‘modules’ after the LayerNorm of the\nfeed-forward transformer block, and the residual\nconnection is placed after the LayerNorm;4 the Lay-\nerNorm before and after the modular component is\nshared.5\nPre-training procedure.Similar to Conneau et al.\n(2020), we pre-train our model on MLM on com-\nbined monolingual corpora in multiple languages.\nExamples of each language are passed through\nthe shared embedding matrix as well as the multi-\nhead attention and feed-forward components at\neach layer. As each layer contains a language-\nspeciﬁc modular component, the examples are\nrouted through the respective designated modular\nbottleneck layer. Given that each example only\nrequires access to a single module, modules can\nbe efﬁciently stored on only a subset of GPUs in\ndistributed training.\nExtending to new languages.The modular de-\nsign of our model allows us to extend it to new\nlanguages after pre-training. To that end, we learn\nnew embeddings and adapter modules for the tar-\nget language through MLM, while the rest of the\ncomponents are frozen.6 Consequently, we are able\nto extend the model to a new language by learning\na small number of new parameters, without affect-\ning performance in the set of pre-trained languages.\nFollowing Pfeiffer et al. (2021b), we learn a new\nsubword vocabulary for the added languages, and\ninitialize the embeddings of lexically overlapping\ntokens from the original embedding matrix.\nFine-tuning on downstream tasks.To transfer\nthe models to cross-lingual downstream tasks, we\nﬁne-tune the shared weights only on the source\nlanguage data, while keeping the modular compo-\nnents and the embedding layer frozen. We follow\nthe standard ﬁne-tuning procedure of adding a pre-\ndiction head on top of the CLS token. We then\nreplace the source language modules (as well as\nembedding layer for added languages) with the tar-\nget language parameters, passing the text of the\ntarget language through the model.7\nrepresentations of the pre-trained mode; we train these mod-\nular components together with the transformer weights, and\ntherefore refer to them as modules.\n4We ﬁnd that the residual connection proposed by Pfeiffer\net al. (2021a) results in training instabilities when trained\ntogether with the transformer weights.\n5Preliminary results showed that sharing the LayerNorm\nresults in better cross-lingual transfer performance.\n6Following Artetxe et al. (2020) we train positional em-\nbeddings.\n7We initially also experimented with stacking adapters on\n4 Experimental design\nWe detail the baseline and models (§4.1), and their\ntraining (§4.2) and evaluation settings (§4.3).\n4.1 Model variants\nWe pre-train separate models for all combinations\nalong the following axes:\nX-M OD vs. SHARED . To evaluate the effective-\nness of our X-M OD model, we aim to compare\nourselves to a conventional non-modular architec-\nture. However, simply removing the modular com-\nponent would be unfair, as the number of FLOPs\nand trainable parameters per language would not\nbe the same—both in terms of pre-training, as\nwell as ﬁne-tuning. Consequently, for our base-\nline model—where all parameters should be fully\nshared between all languages—we include a single\nbottleneck layer right after the Feed-Forward com-\nponent. Effectively, this is the same architecture\nas our X-M OD model, just with a single module\nthat is shared by all languages. We refer to this\nas the SHARED model throughout this paper.8 To\nextend the SHARED model to unseen languages,\nwe follow Artetxe et al. (2020) and only learn a\nnew embedding layer, freezing the transformer pa-\nrameters. To ﬁne-tune the SHARED model on a\ndownstream task, we freeze the embedding layer,\nas well as the (single) module, thereby ﬁne-tuning\nan equal amount of parameters on the downstream\ntask as the X-M OD model.9\n13 vs. 30 vs. 60 vs. 75 languages.So as to under-\nstand how each approach is affected by the curse\nof multilinguality, we pre-train the X-M OD and\nSHARED models on 4 increasing sets of languages.\nWe start with an initial set of 13 typologically di-\nverse languages that we evaluate on, and add addi-\ntional languages for larger sets of 30, 60, and 75\nlanguages. In addition, we keep a set of 7 held-out\nlanguages that we extend the pre-trained models\nto. Table 1 lists the speciﬁc languages in each\ntop of the language modules similar to Pfeiffer et al. (2020b,\n2021b). While this approach is considerably more parameter\nefﬁcient, we ﬁnd that ﬁne-tuning all shared weights slightly\noutperformed the adapter-based approach.\n8Extending the total number of shared parameters would\nbe unfair, as X-M OD and SHARED would not have the same\nFLOPs nor the same number of trainable parameters when\nﬁne-tuning.\n9Adapter-based approach such as MAD-X (Pfeiffer et al.,\n2020b) would be an alternative. However, this would require\ntraining on languages twice—once during pre-training, and\nonce when adding adapters—which is not directly comparable\nto X-M OD. Nonetheless, we report results in §6.2.\n3482\nPre-trained\nlanguages\n13-LANGS en, ar, fr, hi, ko, ru, th, vi, ta, id, ﬁ, sw, ka\n30-LANGS 13-LANGS + cs, eu, hr, hu, hy, it, lt, ml, mn, ms, pl, ro, si, sk, sq, sv, tl\n60-LANGS 30-LANGS + af, am, be, bn, ca, cy, da, eo, et, fa, ga, gl, gu, ha, is, ku, la, lv, mk, ne, nl, no, ps,\npt, sa, sd, sl, so, sr, te\n75-LANGS 60-LANGS + as, br, bs, fy, gd, jv, kn, mg, mr, om, or, pa, su, xh, yi,\nAdded languages bg, de, el, es, tr, ur, zh,\nTable 1: Selection of languages.We pre-train different models on 4 sets of languages, and further extend them to\na set of held-out languages post-hoc. We evaluate on XNLI (languages in bold), NER (underlined languages) and\nXQuAD/MLQA (languages in italic). For more details about the language selection, see Appendix C.\ngroup. The selection and split of initial as well as\nadded languages is motivated by typological and\ngeographical diversity, as well as the availability of\ndownstream task evaluation data.\nControlling for total vs. per-language updates.\nConneau et al. (2020) investigated the effect of\nadding more languages during pre-training, while\ntraining on an equal number of update steps. How-\never, increasing the number of languages while\nkeeping the number of updates constant results in\nthe model seeing less data in each individual lan-\nguage. As such, it remains unclear if the curse of\nmultilinguality happens because of negative inter-\nference, or simply because the number of updates\nfor each speciﬁc language is smaller. So as to un-\nderstand this, we compare (1) training on an equal\nnumber of update steps and (2) training on an equal\nnumber of seen examples per language. We start\nwith the set of 13 languages (Table 1) and train the\nrespective models for 125k update steps. When\nadding more languages, we compare (1) training\nmodels on each set of languages for 125k update\nsteps, and (2) increasing the number of update steps\nsuch that the models are trained on the same num-\nber of examples in each of the initial 13 languages.\nFor the latter, this amounts to training for 195k,\n265k and 269k update steps, respectively.\n4.2 Training details\nData and hyperparameters. We sample lan-\nguages with α = 0.7 and train our models with\na batch size of 2048 across 64 V100 GPUs on\nthe CC100 dataset (Conneau et al., 2020) using\nfairseq (Ott et al., 2019). All our models extend the\nbase transformer architecture, with 12 layers and\n768 dimensions. Modules are implemented with\na bottleneck size of 384. The shared transformer\nweights account for 270M parameters, whereas\neach individual module accounts for 7M parame-\nters. We train our models with a linear learning\nrate decay peaking at 7e−4 during pre-training and\n1e−4 when adding languages.\nVocabulary. As we aim to identify the impact\nof modularity on the curse of multilinguality, we\ncontrol for consistent tokenization across the differ-\nent axes. We therefore tokenize using the XLM-R\nvocabulary for all our pre-training experiments.10\nHowever, for languages added post-hoc, we learn a\nnew SentencePiece tokenizer for each of the target\nlanguage,11 as the languages potentially use scripts\nunseen by the original tokenizer.\n4.3 Evaluation\nWe conduct experiments on NLI, NER, and QA.\nIn all cases, we ﬁne-tune the model on English\nand measure the zero-shot transfer performance in\nother languages. For NLI we train on MultiNLI\n(Williams et al., 2018) and evaluate on XNLI (Con-\nneau et al., 2018). For QA, we train on SQuAD\n(Rajpurkar et al., 2016) and evaluate on XQuAD\n(Artetxe et al., 2020) and MLQA (Lewis et al.,\n2020). For NER, we use WikiANN (Pan et al.,\n2017; Rahimi et al., 2019). We experiment with\nlearning rates 1e−4, 3e−4, and 5e−4 and train for\n3 or 5 epochs for QA and 5 or 10 epochs for NER\nand NLI. For NER and NLI we take the hyperpa-\nrameter setting performing best on the development\nsets, averaged across the pre-trained languages (Ta-\nble 1). For SQuAD we take the best performing\ncheckpoint evaluated on the English development\nset, and report the cross-lingual test set results. 12\nAll results are averaged across 5 random seed runs.\n10Rust et al. (2021) have previously demonstrated the im-\npact of the multilingual tokenizer on the downstream task\nperformance: languages underrepresented in the sub-word\nvocabulary exhibit considerable performance drops when com-\npared to vocabularies dedicated to the respective language.\n11We train the new tokenizers for a vocabulary size of 30k.\n12In contrast to NER and NLI, the cross-lingual evaluation\nbenchmarks of SQuAD do not provide a development set for\neach target language on the basis of which the best checkpoint\ncan be selected. Consequently, we select the checkpoint based\n3483\n10 20 30 40 50 60 70\nnumber of languages\n82.0\n82.5\n83.0\n83.5\n84.0Accuracy\n125k| 125k| 125k| 125k|\nSource Language (English)\n10 20 30 40 50 60 70\nnumber of languages\n71.0\n71.5\n72.0\n72.5\n73.0Accuracy\n125k| 125k| 125k| 125k|\nAverage Pre-Trained Languages\n10 20 30 40 50 60 70\nnumber of languages\n72.0\n72.5\n73.0\n73.5Accuracy\n125k| 125k| 125k| 125k|\nAverage Added Languages\nX-Mod\nshared\n10 20 30 40 50 60 70\nnumber of languages\n0.800\n0.805\n0.810\n0.815\n0.820F1\n125k\n|\n125k\n|\n125k\n|\n125k\n|\nSource Language (English)\n10 20 30 40 50 60 70\nnumber of languages\n0.55\n0.56\n0.57\n0.58\n0.59F1\n125k\n|\n125k\n|\n125k\n|\n125k\n|\nAverage Pre-Trained Languages\n10 20 30 40 50 60 70\nnumber of languages\n0.58\n0.60\n0.62\n0.64F1\n125k\n|\n125k\n|\n125k\n|\n125k\n|\nAverage Added Languages\nX-Mod\nshared\n(a) All models are trained for 125k update steps. Models trained on more languageshave seen less examplesin each language.\n10 20 30 40 50 60 70\nnumber of languages\n82.5\n83.0\n83.5\n84.0\n84.5Accuracy\n125k| 195k| 265k| 269k|\nSource Language (English)\n10 20 30 40 50 60 70\nnumber of languages\n71\n72\n73\n74Accuracy\n125k| 195k| 265k| 269k|\nAverage Pre-Trained Languages\n10 20 30 40 50 60 70\nnumber of languages\n72.5\n73.0\n73.5\n74.0\n74.5Accuracy\n125k| 195k| 265k| 269k|\nAverage Added Languages\nX-Mod\nshared\n10 20 30 40 50 60 70\nnumber of languages\n0.805\n0.810\n0.815\n0.820F1\n125k| 195k| 265k| 269k|\nSource Language (English)\n10 20 30 40 50 60 70\nnumber of languages\n0.56\n0.57\n0.58\n0.59\n0.60F1\n125k| 195k| 265k| 269k|\nAverage Pre-Trained Languages\n10 20 30 40 50 60 70\nnumber of languages\n0.550\n0.575\n0.600\n0.625\n0.650F1\n125k| 195k| 265k| 269k|\nAverage Added Languages\nX-Mod\nshared\n(b) Models trained on more languages are trained longer. All models have seen the same amount of examplesin each language.\nFigure 4: Test set results on XNLI (top) and NER (bottom) for models trained on different numbers of languages.\nSource Language (English) only includes scores of the source language. Average Pre-Trained Languagesincludes\nall evaluation languages that the model was pre-trained on. Average Added Languages includes all languages that\nwere added to the model after pre-training. Scores are averaged across all languages and random seeds.\n5 Results and discussion\nWe present results for pre-trained languages in §5.1\nand added languages in §5.2.\n5.1 Pre-trained languages\nIn Figure 4 we plot downstream task results of\nmodels pre-trained on different amounts of lan-\nguages. Table 2 reports the individual language per-\nformance for the models trained on 60 languages.\nThe Curse of Multilinguality. Conneau et al.\n(2020) showed that multilingual LMs trained on in-\ncreasing amounts of languages, while maintaining\nthe number of update steps, exhibit drops in down-\nstream task XNLI performance. We reproduce\nthese results, both in terms of language modelling\nperplexity (Figure 2a), 13 as well as downstream\non the best performance on the English development set.\n13For per-language perplexity see Appendix A.\ntask performance on XNLI and NER (Figure 4a).\nWe further ﬁnd that the curse of multilinguality\ndoes not only happen because the total number of\nupdate steps per language decreases, but also when\nall SHARED models are trained on the same num-\nber of examples per language (Figure 4b). This\nconﬁrms that fully shared architectures suffer from\nnegative interference.\nLifting the Curse.While for the SHARED model\nwe witness negative interference between lan-\nguages in terms of perplexity, theX-M OD model is\nable to maintain performance, and even improves\nfor a subset of languages. We observe similar\npatterns in the downstream task performance: In\nboth our experimental setups—(1) we control for\nthe number of update steps (Figure 4a); (2) we\ncontrol for the number of per-language seen ex-\namples (Figure 4b)—our X-M OD model—in con-\ntrast to the SHARED model—is able to maintain, or\n3484\nen ar fr hi ko ru th vi ta id ﬁ sw ka avg\nNER X-MOD 81.4 78.9 77.2 70.1 53.0 59.1 2.8 66.2 51.1 50.5 78.6 73.4 67.3 62.8\nSHARED 81.5 74.1 74.7 64.4 46.0 58.3 4.0 63.7 52.5 51.5 74.4 57.2 61.5 58.8\nXNLI X-MOD 84.4 71.2 77.6 68.3 - 74.1 71.7 73.4 - - - 66.9 - 73.5\nSHARED 82.8 69.2 75.6 66.6 - 73.2 68.5 72.5 - - - 62.1 - 72.5\nXQuAD X-MOD 85.1 68.1 - 67.5 - 75.0 66.3 74.9 - - - - - 72.8\nSHARED 83.8 64.6 - 65.8 - 72.7 63.0 72.6 - - - - - 70.4\nMLQA X-MOD 80.1 58.6 - 60.7 - - - 67.5 - - - - - 66.7\nSHARED 79.6 53.6 - 58.7 - - - 64.9 - - - - - 64.2\nTable 2: Pre-trained language results for the modular and shared model variants, pre-trained on the set of 60\nlanguages for 265k update steps. For NER and MLQA we report F1, for XNLI accuracy scores. Scores are\naveraged across all 5 random seeds of the best hyperparameter setting, evaluated on the development set.\nbg de el es tr ur zh avg\nNER X-MOD 77.6 75.1 75.2 71.9 72.6 54.7 21.664.1\nSHARED74.9 66.3 69.6 49.1 64.8 50.4 9.254.9\nXNLIX-MOD 77.4 75.4 76.2 78.5 72.4 64.9 73.874.1\nSHARED76.3 74.1 74.9 77.3 71.0 64.3 71.472.8\nMLQAX-MOD - 63.8 - 68.6 - - 61.764.8\nSHARED - 58.9 - 66.7 - - 56.5 60.7\nTable 3: Results for added languages, for models pre-\ntrained on the set of 60 languages for 265k update steps.\nWe report F1 and accuracy scores which are averaged\nacross all 5 random seeds of the best hyperparameter\nsetting on the development set.\neven outperform model variants trained on less lan-\nguages. These results demonstrate that the added\nper-language capacity is sufﬁcient for the model to\nadequately represent all languages.\nSurprisingly, X-M OD not only maintains per-\nformance, but actually slightly improves while we\nincrease the number of languages we pre-train on.\nThis is even the case for settings where the model\nsees less examples in the target language. This\nsuggests that increasing the language diversity can\nhave a positive impact on the model’s cross-lingual\nrepresentation capability.\nX-M OD vs SHARED . Overall, the X-M OD model\npre-trained on 60 languages achieves the best cross-\nlingual performance.14 Our results on XNLI, NER,\nMLQA, and XQuAD in Table 2 demonstrate con-\nsistent performance gains over the SHARED model\nfor every task and across (almost) all high- as well\nas low-resource languages.\n14We ﬁnd that the X-M OD model trained on 75 languages\nis less stable than the versions trained on less languages. We\nthink that this can be attributed to the 15 added languages\nbeing extremely low resource—we only train for an additional\n4k update steps—resulting in the respective randomly initial-\nized modules being updated very infrequently. This variance\ncould potentially be mitigated by training for longer.\n5.2 Extending to unseen languages\nWe further evaluate the cross-lingual performance\nof languages added in the second step; (1) on the\narchitectural side—comparing the SHARED with\nthe X-M OD modelling variant—and (2) by com-\nparing the performance when pre-training on the\nlanguage, vs. when adding the language post-hoc.\nModular vs Shared.We evaluate if the additional\nper-language capacity improves the extendability\nof the X-M OD model. On the right in Figure 4a\nwe plot the results for added languages on XNLI\n(top) and NER (bottom). Similarly, we plot the\nresults for the models where we control for the\nnumber of seen examples per target language in\nFigure 4b. We ﬁnd that the X-M OD model consis-\ntently outperforms the SHARED model, with a peak\nperformance when pre-training on 60 languages,\ndemonstrating that the language speciﬁc capacity\nis beneﬁcial for adding new languages post-hoc.\nWe report results for the 60 language versions in\nTable 3, demonstrating the consistent advantage of\nthe X-M OD over the SHARED model.\nPre-training vs Adding Languages.To evaluate\nif there is a measurable difference on downstream\nperformance for languages that we pre-train on vs.\nthose we add post-hoc, we train 2 models on differ-\nent initial sets of languages, adding the respectively\nmissing ones in the second step. So as to under-\nstand if the typological similarity of languages has\nimpact on the downstream task performance, we\nsplit the initial and added languages (Table 1) of\nour previous experiments into two parts. The ﬁrst\nsplit consists of languages where the model was\npre-trained on at least one language of the same\nlanguage family (e.g. English vs. German). The\nsecond split consists of languages that are part of\na unique language family, i.e. the model was not\n3485\nModel 1 pre-trained Model 2 pre-trained\nFigure 5: XNLI test set accuracy of X-M OD mod-\nels pre-trained on different languages in comparison to\nthose added post-hoc (Table 4).\nLanguage iso Family Script Model 1 Model 2\nEnglish en IE: GermanicLatin pre-train add\nGerman de IE: Germanic Latin add pre-train\nFrench fr IE: RomanceLatin pre-train add\nSpanish es IE: Romance Latin add pre-train\nRussian ru IE: Slavic Cyrillic pre-train add\nUkranian uk IE: Slavic Cyrillic add pre-train\nHindi hi IE: Iranian Devanagaripre-train add\nUrdu ur IE: Iranian Arabic add pre-train\nArabic ar Afro-AsiaticArabic pre-train add\nHebrew he Afro-Asiatic Hebrew add pre-train\nVietnamesevi Austro-AsiaticLatin pre-train add\nThai th Kra-Dai Thai pre-train add\nKorean ko Koreanic Korean pre-train add\nJapanese ja Japonic Japanese add pre-train\nGreek el IE: Hellenic Greek add pre-train\nTurkish tr Turkic Latin add pre-train\nTable 4: Selection of 2 sets of languages that we either\npre-train on, or add post-hoc. The last 6 languages in\nthe list are part of language families which are unique\nin the total list of languages we pre-train on (Table 1),\ni.e. none of our models was pre-trained on a language\nof the same family.\npre-trained on a language of the same family (Ta-\nble 4). Consequently, we pre-train two models on\ntwo sets of languages, adding the respective other\nset post-hoc.15\nOur XNLI results (Figure 5) demonstrate that\nthe per-language performance is on par when pre-\ntraining vs. when adding the language post-hoc.16\nWe also ﬁnd that the family does not have a measur-\nable effect on the performance of the language. Our\nresults therefore suggest that it is sufﬁcient to train\nX-M OD on only a subset of languages for which\nsufﬁcient pre-training data exists. Essentially, X-\n15In previous experiments, the modular model trained on\n60 languages achieved the best performance. Therefore, the\nmodels in these experiments are also trained on 60 languages.\nBoth models are trained on the same additional languages, i.e.\nthe 60-LANGS of Table 1, where only the 13-LANGS differ.\n16The models have seen an equal amount of examples in\nthe respective languages in each case.\n125k 250k\n83\n84\n85Accuracy\nEnglish\nX-Mod shared\n125k 250k\n72.0\n72.5\n73.0\n73.5\nPre-Trained Langs\nFigure 6: Results on XNLI when when pre-training on\n13 languages for 125k and 250k update steps.\nMOD has the potential to cover all languages of the\nworld, as the model has the capability to be adapted\nto new languages post-hoc.\n6 Further analysis\nWe further analyze the impact of the number of\nupdate steps on X-M OD (§6.1) and compare our\nmethod to adapter-based approaches (§6.2).\n6.1 The importance of update steps\nIn Figure 4 we have witnessed a slight edge of\nthe SHARED model over the X-M OD model, when\ntraining on only 13 languages and only training\nfor 125k update steps. Dufter and Schütze (2020)\nfound that it requires a large number of update steps\nfor a model pre-trained on multiple languages to\nbecome multilingual; with the added per-language\ncapacity we hypothesize that update steps also play\nan important role for modular models. We com-\npare the downstream task performance of mod-\nels pre-trained on 13 languages, when training for\n125k with 250k update steps in Figure 6. When\ntraining for longer we ﬁnd that the X-M OD model\nbegins to outperforms the SHARED model in the\nsource language, while almost closing the gap in\nthe cross-lingual setting. This supports the hypoth-\nesis that the X-M OD model requires more update\nsteps when training only on a small number of lan-\nguages, in order for modularity to “kick-in”.\n6.2 X-M OD vs. Adapters\nAs illustrated in Figure 3, from an architecture per-\nspective X-M OD is similar to previously proposed\nmultilingual Adapter-based methods ( MAD-X;\nPfeiffer et al., 2020b). MAD-X utilizes a pre-\ntrained massively multilingual transformer-based\nmodel and ﬁne-tunes newly introduced adapter\nweights on languages the model has seen during\npre-training, and ones the model has not been\n3486\n20 40 60\nnumber of languages\n81.5\n82.0\n82.5\n83.0\n83.5\n84.0Accuracy\n125k\n|\n125k\n|\n125k\n|\n125k\n|\nSource Language (English)\n20 40 60\nnumber of languages\n70.0\n70.5\n71.0\n71.5\n72.0\n72.5\n73.0Accuracy\n125k\n|\n125k\n|\n125k\n|\n125k\n|\nAverage Pre-Trained Languages\nX-Mod\nshared\nshared_nm\nAdapters\nFigure 7: Comparison on XNLI of X-M OD and shared\nmodels with an Adapter baseline, all models are pre-\ntrained for 125k update steps.\ntrained on. For a fair comparison in terms of seen\nexamples and number of update steps we train a\ntransformer model without module components\n(shared_nm) for 100k update steps on the respec-\ntive languages (Table 1). We subsequently train\nadapters on each of the target languages for an-\nother 25k update steps.17 We report results in com-\nparison to X-M OD in Figure 7, here results for\nshared_nm are for a model that was trained for\n125k update steps to instantiate a fair comparison.\nOur results demonstrate that the additional capac-\nity of adapters added after pre-training is not able\nto mitigate the curse of multilinguality which has al-\nready had a catastrophic impact on the shared trans-\nformer weights; the performance of the adapters\nstrongly correlates with the performance of the cor-\nresponding fully shared model shared_nm. Conse-\nquently, adding language-speciﬁc capacity during\npre-training is important, as the curse of multilin-\nguality cannot be lifted post-hoc.\n7 Conclusions\nIn this paper, we have evaluated the effectiveness\nof modular multilingual language modelling across\nmultiple axes. We have demonstrated that by\nproviding additional per-language capacity, while\nmaintaining the total number of trainable parame-\nters per language, we are not only able to mitigate\nnegative interference between languages, but ad-\nditionally achieve positive transfer. Our results\nsuggest that it is sufﬁcient to train our proposed\nX-M OD model only on a subset of languages for\nwhich sufﬁcient amounts of textual data is avail-\n17We follow Pfeiffer et al. (2020b) and train adapter weights\nwith a learning rate of 0.0001. While they have found that\ncross-lingual transfer performance of adapters converges at\n∼20k update-steps, we would like to stress that our experi-\nmental setup is only one of multiple different valid versions.\nA more thorough investigation to ﬁnd the optimal number of\nupdate steps for pre-training and subsequent adapter training\nis necessary, which was out of scope for this work.\nable. Unseen languages can be added post-hoc,\nwith no measurable drop in performance on XNLI.\nBy pre-training the model in a modular fashion, we\nthus mitigate negative interference of idiosyncratic\ninformation, while simultaneously preparing the\nmodel to be extendable to unseen languages.\nWhile in this work we have simulated language\nadding scenarios with a held out set of languages, in\nfuture work we aim to evaluate the performance on\ntruly low-resource languages such as MasakhaNER\n(Adelani et al., 2021) and AmericasNLI (Ebrahimi\net al., 2021). We further aim to evaluate the cross-\nlingual transfer performance from typologically\nmore diverse source languages, besides English.\nAcknowledgments\nWe thank Samuel Broscheit for insightful feedback\nand suggestions on a draft of this paper, as well\nas the ARR reviewers and meta-reviewers for their\nvaluable comments.\nReferences\nDavid Ifeoluwa Adelani, Jade Z. Abbott, Graham\nNeubig, Daniel D’souza, Julia Kreutzer, Constan-\ntine Lignos, Chester Palen-Michel, Happy Buza-\naba, Shruti Rijhwani, Sebastian Ruder, Stephen\nMayhew, Israel Abebe Azime, Shamsuddeen Has-\nsan Muhammad, Chris Chinenye Emezue, Joyce\nNakatumba-Nabende, Perez Ogayo, Aremu An-\nuoluwapo, Catherine Gitau, Derguene Mbaye, Jesu-\njoba O. Alabi, Seid Muhie Yimam, Tajuddeen Gwad-\nabe, Ignatius Ezeani, Rubungo Andre Niyongabo,\nJonathan Mukiibi, Verrah Otiende, Iroro Orife,\nDavis David, Samba Ngom, Tosin P. Adewumi, Paul\nRayson, Mofetoluwa Adeyemi, Gerald Muriuki,\nEmmanuel Anebi, Chiamaka Chukwuneke, Nkiruka\nOdu, Eric Peter Wairagala, Samuel Oyerinde,\nClemencia Siro, Tobius Saul Bateesa, Temilola\nOloyede, Yvonne Wambui, Victor Akinode, Deb-\norah Nabagereka, Maurice Katusiime, Ayodele\nAwokoya, Mouhamadane Mboup, Dibora Gebrey-\nohannes, Henok Tilaye, Kelechi Nwaike, Degaga\nWolde, Abdoulaye Faye, Blessing Sibanda, Ore-\nvaoghene Ahia, Bonaventure F. P. Dossou, Kelechi\nOgueji, Thierno Ibrahima Diop, Abdoulaye Diallo,\nAdewale Akinfaderin, Tendai Marengereke, and Sa-\nlomey Osei. 2021. MasakhaNER: Named Entity\nRecognition for African Languages. In Transac-\ntions of the Association for Computational Linguis-\ntics 2021.\nJacob Andreas, Marcus Rohrbach, Trevor Darrell, and\nDan Klein. 2016. Learning to compose neural net-\nworks for question answering. In NAACL HLT 2016,\nThe 2016 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\n3487\nHuman Language Technologies, San Diego Califor-\nnia, USA, June 12-17, 2016, pages 1545–1554. The\nAssociation for Computational Linguistics.\nAlan Ansell, Edoardo Maria Ponti, Anna Korhonen,\nand Ivan Vulic. 2021a. Composable sparse ﬁne-\ntuning for cross-lingual transfer. arXiv preprint.\nAlan Ansell, Edoardo Maria Ponti, Jonas Pfeiffer, Se-\nbastian Ruder, Goran Glavaš, Ivan Vuli´c, and Anna\nKorhonen. 2021b. MAD-G: Multilingual adapter\ngeneration for efﬁcient cross-lingual transfer. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 4762–4781, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nMikel Artetxe, Sebastian Ruder, and Dani Yogatama.\n2020. On the cross-lingual transferability of mono-\nlingual representations. In Proceedings of the 58th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 4623–4637, Online. Asso-\nciation for Computational Linguistics.\nAnkur Bapna and Orhan Firat. 2019. Simple, scal-\nable adaptation for neural machine translation. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019 , pages 1538–\n1548. Association for Computational Linguistics.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith.\n2020a. Parsing with multilingual BERT, a small cor-\npus, and a small treebank. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 1324–1334, Online. Association for Computa-\ntional Linguistics.\nEthan C. Chau, Lucy H. Lin, and Noah A. Smith.\n2020b. Parsing with multilingual bert, a small tree-\nbank, and a small corpus. In Proceedings of the\n2020 Conference on Empirical Methods in Natu-\nral Language Processing: Findings, EMNLP 2020,\nOnline Event, 16-20 November 2020 , pages 1324–\n1334.\nVincent S. Chen, Sen Wu, Alexander J. Ratner, Jen\nWeng, and Christopher Ré. 2019. Slice-based learn-\ning: A programming model for residual learning in\ncritical data slices. In Advances in Neural Infor-\nmation Processing Systems 32: Annual Conference\non Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver,\nBC, Canada, pages 9392–9402.\nAlexandra Chronopoulou, Dario Stojanovski, and\nAlexander Fraser. 2020. Reusing a Pretrained Lan-\nguage Model on Languages with Limited Corpora\nfor Unsupervised NMT. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 2703–2711, On-\nline. Association for Computational Linguistics.\nHyung Won Chung, Thibault Févry, Henry Tsai,\nMelvin Johnson, and Sebastian Ruder. 2021. Re-\nthinking embedding coupling in pre-trained lan-\nguage models. In 9th International Conference on\nLearning Representations, ICLR 2021, Virtual Event,\nAustria, May 3-7, 2021. OpenReview.net.\nHyung Won Chung, Dan Garrette, Kiat Chuan Tan, and\nJason Riesa. 2020. Improving multilingual models\nwith language-clustered vocabularies. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing, EMNLP 2020, On-\nline, November 16-20, 2020 , pages 4536–4546. As-\nsociation for Computational Linguistics.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2022. CANINE: pre-training an efﬁcient\ntokenization-free encoder for language representa-\ntion. Transactions of the Association for Computa-\ntional Linguistics, 10.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Conference of the Associ-\nation for Computational Linguistics, ACL 2020, Vir-\ntual Conference, July 6-8, 2020, pages 8440–8451.\nAlexis Conneau, Ruty Rinott, Guillaume Lample, Ad-\nina Williams, Samuel Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. XNLI: Evaluating\ncross-lingual sentence representations. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, pages 2475–2485,\nBrussels, Belgium. Association for Computational\nLinguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, NAACL-HLT 2019, Minneapolis, MN,\nUSA, June 2-7, 2019, Volume 1 (Long and Short Pa-\npers), pages 4171–4186.\nPhilipp Dufter and Hinrich Schütze. 2020. Identifying\nelements essential for BERT’s multilinguality. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 4423–4437, Online. Association for Computa-\ntional Linguistics.\nAbteen Ebrahimi, Manuel Mager, Arturo Oncevay,\nVishrav Chaudhary, Luis Chiruzzo, Angela Fan,\nJohn Ortega, Ricardo Ramos, Annette Rios, Ivan\nVladimir, Gustavo A. Giménez-Lugo, Elisabeth\nMager, Graham Neubig, Alexis Palmer, Rolando\nA. Coto Solano, Ngoc Thang Vu, and Katharina\nKann. 2021. AmericasNLI: Evaluating Zero-shot\nNatural Language Understanding of Pretrained Mul-\ntilingual Models in Truly Low-resource Languages.\narXiv preprint.\n3488\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2021.\nSwitch Transformers: Scaling to Trillion Parameter\nModels with Simple and Efﬁcient Sparsity. arXiv\npreprint.\nXavier Garcia, Noah Constant, Ankur Parikh, and\nOrhan Firat. 2021. Towards continual learning for\nmultilingual machine translation via vocabulary sub-\nstitution. In Proceedings of the 2021 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 1184–1192, Online. Association for\nComputational Linguistics.\nGoran Glavas, Robert Litschko, Sebastian Ruder, and\nIvan Vulic. 2019. How to (properly) evaluate cross-\nlingual word embeddings: On strong baselines, com-\nparative analyses, and some misconceptions. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 710–721.\nSuchin Gururangan, Mike Lewis, Ari Holtzman,\nNoah A. Smith, and Luke Zettlemoyer. 2021. Demix\nlayers: Disentangling domains for modular lan-\nguage modeling. arXiv preprint.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2022. Towards a\nuniﬁed view of parameter-efﬁcient transfer learning.\nIn 10th International Conference on Learning Rep-\nresentations, ICLR 2022, Virtual Conference, April\n25 - 29, 2022.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzkeb-\nski, Bruna Morrone, Quentin de Laroussilhe, An-\ndrea Gesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for NLP.\nIn Proceedings of the 36th International Conference\non Machine Learning, ICML 2019, 9-15 June 2019,\nLong Beach, California, USA, pages 2790–2799.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. XTREME: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalization. In Proceedings of the 37th International\nConference on Machine Learning, ICML 2020, 12-\n18 July 2020, Virtual Conference.\nKarthikeyan K, Zihan Wang, Stephen Mayhew, and\nDan Roth. 2020. Cross-lingual ability of multi-\nlingual BERT: an empirical study. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020.\nAnne Lauscher, Olga Majewska, Leonardo F. R.\nRibeiro, Iryna Gurevych, Nikolai Rozanov, and\nGoran Glavaš. 2020a. Common sense or world\nknowledge? investigating adapter-based knowledge\ninjection into pretrained transformers. In Proceed-\nings of Deep Learning Inside Out (DeeLIO): The\nFirst Workshop on Knowledge Extraction and Inte-\ngration for Deep Learning Architectures, pages 43–\n49, Online. Association for Computational Linguis-\ntics.\nAnne Lauscher, Vinit Ravishankar, Ivan Vuli ´c, and\nGoran Glavaš. 2020b. From zero to hero: On the\nlimitations of zero-shot language transfer with mul-\ntilingual Transformers. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4483–4499, On-\nline.\nHang Le, Juan Miguel Pino, Changhan Wang, Jiatao\nGu, Didier Schwab, and Laurent Besacier. 2021.\nLightweight adapter tuning for multilingual speech\ntranslation. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguis-\ntics and the 11th International Joint Conference on\nNatural Language Processing, ACL/IJCNLP 2021,\n(Volume 2: Short Papers), Virtual Event, August 1-\n6, 2021, pages 817–824. Association for Computa-\ntional Linguistics.\nPatrick Lewis, Barlas Oguz, Ruty Rinott, Sebastian\nRiedel, and Holger Schwenk. 2020. MLQA: Evalu-\nating cross-lingual extractive question answering. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7315–\n7330, Online. Association for Computational Lin-\nguistics.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021a. Compacter: Efﬁcient low-\nrank hypercomplex adapter layers. Advances in Neu-\nral Information Processing Systems 34: Annual Con-\nference on Neural Information Processing Systems\n2021, NeurIPS 2021.\nRabeeh Karimi Mahabadi, Sebastian Ruder, Mostafa\nDehghani, and James Henderson. 2021b. Parameter-\nefﬁcient multi-task ﬁne-tuning for transformers via\nshared hypernetworks. In Proceedings of the\n59th Annual Meeting of the Association for Com-\nputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing,\nACL/IJCNLP 2021, (Volume 1: Long Papers), Vir-\ntual Event, August 1-6, 2021, pages 565–576. Asso-\nciation for Computational Linguistics.\nBenjamin Muller, Antonios Anastasopoulos, Benoît\nSagot, and Djamé Seddah. 2021. When being un-\nseen from mBERT is just the beginning: Handling\nnew languages with multilingual language models.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 448–462, Online. Association for Computa-\ntional Linguistics.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. fairseq: A fast, extensible\ntoolkit for sequence modeling. In Proceedings of\n3489\nthe 2019 Conference of the North American Chap-\nter of the Association for Computational Linguistics\n(Demonstrations), pages 48–53, Minneapolis, Min-\nnesota. Association for Computational Linguistics.\nXiaoman Pan, Boliang Zhang, Jonathan May, Joel\nNothman, Kevin Knight, and Heng Ji. 2017. Cross-\nlingual name tagging and linking for 282 languages.\nIn Proceedings of the 55th Annual Meeting of the As-\nsociation for Computational Linguistics, ACL 2017,\nVancouver, Canada, July 30 - August 4, Volume 1:\nLong Papers, pages 1946–1958.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2021a.\nAdapterFusion: Non-destructive task composition\nfor transfer learning. In Proceedings of the 16th\nConference of the European Chapter of the Associ-\nation for Computational Linguistics: Main Volume ,\npages 487–503, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aish-\nwarya Kamath, Ivan Vuli ´c, Sebastian Ruder,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterHub: A Framework for Adapting Transform-\ners. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Process-\ning (System Demonstrations), EMNLP 2020, Virtual\nConference, 2020.\nJonas Pfeiffer, Ivan Vuli ´c, Iryna Gurevych, and Se-\nbastian Ruder. 2020b. MAD-X: An Adapter-Based\nFramework for Multi-Task Cross-Lingual Transfer.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 7654–7673, Online. Association for Computa-\ntional Linguistics.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2021b. UNKs Everywhere: Adapting\nMultilingual Language Models to New Scripts. In\nProceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2021, Online, November , 2021.\nJerin Philip, Alexandre Berard, Matthias Gallé, and\nLaurent Besacier. 2020. Monolingual adapters for\nzero-shot neural machine translation. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n4465–4470, Online. Association for Computational\nLinguistics.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert? In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 4996–5001.\nEdoardo Maria Ponti, Goran Glavaš, Olga Majewska,\nQianchu Liu, Ivan Vuli´c, and Anna Korhonen. 2020.\nXCOPA: A multilingual dataset for causal common-\nsense reasoning. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 2362–2376, Online. As-\nsociation for Computational Linguistics.\nClifton Poth, Jonas Pfeiffer, Andreas Rücklé, and Iryna\nGurevych. 2021. What to pre-train on? efﬁcient\nintermediate task selection. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 10585–10605. Association for Compu-\ntational Linguistics.\nAfshin Rahimi, Yuan Li, and Trevor Cohn. 2019.\nMassively multilingual transfer for NER. In Pro-\nceedings of the 57th Conference of the Association\nfor Computational Linguistics, ACL 2019, Florence,\nItaly, July 28- August 2, 2019, Volume 1: Long Pa-\npers, pages 151–164.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ Questions\nfor Machine Comprehension of Text. In Proceed-\nings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, EMNLP 2016,\nAustin, Texas, USA, November 1-4, 2016 , pages\n2383–2392.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. In Advances in Neural Infor-\nmation Processing Systems 30: Annual Conference\non Neural Information Processing Systems 2017, 4-\n9 December 2017, Long Beach, CA, USA , pages\n506–516.\nSylvestre-Alvise Rebufﬁ, Hakan Bilen, and Andrea\nVedaldi. 2018. Efﬁcient parametrization of multi-\ndomain deep neural networks. In 2018 IEEE Confer-\nence on Computer Vision and Pattern Recognition,\nCVPR 2018, Salt Lake City, UT, USA, June 18-22,\n2018, pages 8119–8127.\nAndreas Rücklé, Gregor Geigle, Max Glockner,\nTilman Beck, Jonas Pfeiffer, Nils Reimers, and Iryna\nGurevych. 2021. Adapterdrop: On the efﬁciency\nof adapters in transformers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, EMNLP 2021, Virtual Event\n/ Punta Cana, Dominican Republic, 7-11 November,\n2021, pages 7930–7946. Association for Computa-\ntional Linguistics.\nAndreas Rücklé, Jonas Pfeiffer, and Iryna Gurevych.\n2020. Multicqa: Zero-shot transfer of self-\nsupervised text matching models on a massive scale.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2020, Online, November 16-20, 2020 , pages 2471–\n2486. Association for Computational Linguistics.\nSebastian Ruder, Ivan Vuli ´c, and Anders Søgaard.\n2019. A survey of cross-lingual embedding models.\nJournal of Artiﬁcial Intelligence Research , 65:569–\n631.\n3490\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian\nRuder, and Iryna Gurevych. 2021. How good is\nyour tokenizer? on the monolingual performance of\nmultilingual language models. In Proceedings of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (Vol-\nume 1: Long Papers), pages 3118–3135, Online. As-\nsociation for Computational Linguistics.\nNoam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz,\nAndy Davis, Quoc V . Le, Geoffrey E. Hinton, and\nJeff Dean. 2017. Outrageously large neural net-\nworks: The sparsely-gated mixture-of-experts layer.\nIn 5th International Conference on Learning Rep-\nresentations, ICLR 2017, Toulon, France, April 24-\n26, 2017, Conference Track Proceedings . OpenRe-\nview.net.\nAsa Cooper Stickland, Alexandre Berard, and Vassilina\nNikoulina. 2021. Multilingual domain adaptation\nfor NMT: decoupling language and domain informa-\ntion with adapters. In Proceedings of the Sixth Con-\nference on Machine Translation, WMT@EMNLP\n2021, Online Event, November 10-11, 2021 , pages\n578–598. Association for Computational Linguis-\ntics.\nAsa Cooper Stickland and Iain Murray. 2019. BERT\nand pals: Projected attention layers for efﬁcient\nadaptation in multi-task learning. In Proceedings\nof the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 5986–5995. PMLR.\nYi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Prakash\nGupta, Hyung Won Chung, Dara Bahri, Zhen Qin,\nSimon Baumgartner, Cong Yu, and Donald Met-\nzler. 2021. Charformer: Fast character transform-\ners via gradient-based subword tokenization. arXiv\npreprint.\nKe M. Tran. 2020. From english to foreign languages:\nTransferring pre-trained language models. arXiv\npreprint.\nAhmet Üstün, Alexandre Berard, Laurent Besacier, and\nMatthias Gallé. 2021. Multilingual unsupervised\nneural machine translation with denoising adapters.\nIn Proceedings of the 2021 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2021, Virtual Event / Punta Cana, Dominican Re-\npublic, 7-11 November, 2021, pages 6650–6662. As-\nsociation for Computational Linguistics.\nAhmet Üstün, Arianna Bisazza, Gosse Bouma, and\nGertjan van Noord. 2020. UDapter: Language adap-\ntation for truly Universal Dependency parsing. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 2302–2315, Online. Association for Computa-\ntional Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, 4-9 Decem-\nber 2017, Long Beach, CA, USA, pages 5998–6008.\nGiorgos Vernikos and Andrei Popescu-Belis. 2021.\nSubword mapping and anchoring across languages.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2021, pages 2633–2647, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nMarko Vidoni, Ivan Vuli ´c, and Goran Glavaš. 2020.\nOrthogonal language and task adapters in zero-shot\ncross-lingual transfer. In arXiv preprint.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021a. K-adapter: Infusing\nknowledge into pre-trained models with adapters. In\nFindings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021 , volume ACL/IJCNLP 2021 of Findings\nof ACL, pages 1405–1418. Association for Compu-\ntational Linguistics.\nXinyi Wang, Yulia Tsvetkov, Sebastian Ruder, and Gra-\nham Neubig. 2021b. Efﬁcient test time adapter en-\nsembling for low-resource language varieties. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 730–737, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nZihan Wang, Karthikeyan K, Stephen Mayhew, and\nDan Roth. 2020. Extending multilingual BERT to\nlow-resource languages. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 2649–2656, Online. Association for Computa-\ntional Linguistics.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume\n1 (Long Papers) , pages 1112–1122, New Orleans,\nLouisiana. Association for Computational Linguis-\ntics.\nShijie Wu, Alexis Conneau, Haoran Li, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Emerging cross-\nlingual structure in pretrained language models. In\nProceedings of the 58th Conference of the Associa-\ntion for Computational Linguistics, ACL 2020, Vir-\ntual Conference, July 6-8, 2020, pages 6022–6034.\nShijie Wu and Mark Dredze. 2019. Beto, bentz, be-\ncas: The surprising cross-lingual effectiveness of\nBERT. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3491\n833–844, Hong Kong, China. Association for Com-\nputational Linguistics.\nShijie Wu and Mark Dredze. 2020. Are all languages\ncreated equal in multilingual BERT? InProceedings\nof the 5th Workshop on Representation Learning for\nNLP, pages 120–130, Online. Association for Com-\nputational Linguistics.\nLinting Xue, Aditya Barua, Noah Constant, Rami Al-\nRfou, Sharan Narang, Mihir Kale, Adam Roberts,\nand Colin Raffel. 2022. Byt5: Towards a token-free\nfuture with pre-trained byte-to-byte models. Trans-\nactions of the Association for Computational Lin-\nguistics 2022.\nA Additional results\nWe report MLQA and XQuAD results on pre-\ntrained languages in Tables 5 and 6, respectively,\nand MLQA results on added languages in Table 7.\nTable 8 report NER results on more languages.\nFigures 9, 10 and 11 report per-language results\nas we increase the amount of languages on lan-\nguage modeling perplexity, XNLI and NER, re-\nspectively.\nB Intermediate checkpoints\nOur results in §6.1 suggest that, when the number\nof languages is small, X-M OD becomes more com-\npetitive with SHARED as the number of training\nsteps increases. So as to understand if this behav-\nior also holds for models covering more languages,\nwe evaluate intermediate checkpoints for the 60-\nLANG model on XNLI. As shown in Figure 8,\nwe ﬁnd that the X-M OD model continuously out-\nperforms the SHARED model. This suggests that\nthe SHARED model immediately suffers from neg-\native interference between languages, while the\nadded, language-speciﬁc components of the X-\nMOD model are able to mitigate the curse of mul-\ntilinguality, resulting in considerable performance\ngains at all evaluated checkpoints.\nC Language selection\nWe provide more details about our selection of\nlanguages in Table 9.\n50k 100k 150k 200k 265k\n80\n81\n82\n83\n84Accuracy\nEnglish\nX-Mod shared\n50k 100k 150k 200k 265k\n68\n69\n70\n71\n72\n73\nPre-Trained Langs\nFigure 8: Results on XNLI using intermediate check-\npoints of the models trained on 60 languages.\nen ar hi vi avgF1/ EM F1/ EM F1/ EM F1/ EM F1/ EM\nX-MOD 80.1 / 66.9 58.6 / 38.9 60.7 / 42.4 67.5 / 46.166.7 / 48.6\nSHARED79.6 / 66.5 53.6 / 33.9 58.7 / 40.4 64.9 / 43.864.2 / 46.2\nTable 5: Average F 1 and Exact Match results for pre-\ntrained languages, on the test set of MLQA for the\nX-M OD and SHARED model variants, pre-trained on\nthe set of 60 languages for 265k update steps. Bold\nnumbers indicate better performance for the respective\nlanguage.\nen ar hi ru th vi avgF1/ EM F1/ EM F1/ EM F1/ EM F1/ EM F1/ EMF1/ EMX-MOD85.1 / 73.4 68.1 / 52.4 67.5 / 50.3 75.0 / 57.8 66.3 / 52.6 74.9 / 54.672.8 / 56.9SHARED83.8 / 72.1 64.6 / 48.5 65.8 / 48.3 72.7 / 54.5 63.0 / 48.0 72.6 / 52.170.4 / 53.9\nTable 6: Average F 1 and Exact Match results for pre-\ntrained languages, on the test set of XQuAD for the\nX-M OD and SHARED model variants, pre-trained on\nthe set of 60 languages for 265k update steps. Bold\nnumbers indicate better performance for the respective\nlanguage.\nde es zh avg\nF1 / EM F 1 / EM F 1 / EM F1 / EM\nX-MOD 63.8 / 48.9 68.8 / 50.3 61.7 / 36.464.8 / 45.2\nSHARED 58.9 / 44.1 66.7 / 48.3 56.5 / 32.260.7 / 41.5\nTable 7: Average F1 and Exact Match results foradded\nlanguages, on the test set of MLQA for the X-M OD\nand SHARED model variants, pre-trained on the set of\n60 languages for 265k update steps. Bold numbers in-\ndicate better performance for the respective language.\n3492\nen af ar bn et eu fa ﬁ fr hi hu id it ka ko ru sw ta th vi avg\nX-MOD 81.478.943.563.2 76.2 62.2 44.3 78.6 77.2 70.1 78.350.578.7 67.3 53.0 59.1 73.451.1 2.866.262.8\nSHARED81.574.144.262.4 70.7 58.1 40.3 74.4 74.7 64.4 74.251.575.5 61.5 46.0 58.3 57.252.5 4.063.759.5\nTable 8: Average F1 results for pre-trained languages, on the test set ofNER for the X-MOD and SHARED model\nvariants, pre-trained on the set of 60 languages. Bold numbers indicate better performance for the respective\nlanguage.\n25 50 75\n# Languages\n4.0\n4.2\n4.4Perplexity\nen\n25 50 75\n# Languages\n4.5\n5.0\n5.5\nar\n25 50 75\n# Languages\n2.8\n2.9\n3.0\n3.1\n3.2\nfr\n25 50 75\n# Languages\n3.5\n4.0\n4.5\nhi\n25 50 75\n# Languages\n3.0\n3.2\n3.4\nru\n25 50 75\n# Languages\n3.5\n4.0\n4.5\n5.0\nsw\n25 50 75\n# Languages\n4.5\n5.0\n5.5\nth\n25 50 75\n# Languages\n3.0\n3.2\n3.4\nvi\nX-Mod\nshared\nFigure 9: Perplexity when training on more languages. Each model has seen the same amount of examplesin\neach language. Lower perplexity indicates better performance.\n83.0\n83.5\n84.0Accuracy\nen\nX-Mod shared\n69.5\n70.0\n70.5\n71.0\nar\n75.5\n76.0\n76.5\n77.0\n77.5\nfr\n66\n67\n68\nhi\n10 20 30 40 50 60 70\nnumber of languages\n73\n74\n75Accuracy\nru\n10 20 30 40 50 60 70\nnumber of languages\n62\n64\n66\nsw\n10 20 30 40 50 60 70\nnumber of languages\n69\n70\n71\nth\n10 20 30 40 50 60 70\nnumber of languages\n72.5\n73.0\n73.5\nvi\n(a) Pre-Trained Languages\n76.0\n76.5\n77.0\n77.5Accuracy\nbg\nX-Mod shared\n74.0\n74.5\n75.0\n75.5\n76.0\nde\n74.5\n75.0\n75.5\n76.0\nel\n77.0\n77.5\n78.0\n78.5\nes\n10 20 30 40 50 60 70\nnumber of languages\n70.5\n71.0\n71.5\n72.0\n72.5Accuracy\ntr\n10 20 30 40 50 60 70\nnumber of languages\n63.0\n63.5\n64.0\n64.5\n65.0\nur\n10 20 30 40 50 60 70\nnumber of languages\n71.0\n71.5\n72.0\n72.5\n73.0\n73.5\nzh\n(b) Added Languages\nFigure 10: Testset results onXNLI of pre-trained (top) and added (bottom) languages trained on different numbers\nof languages. Models trained on more languages are trained for longer →all models have seen the same amount\nof examplesin each individual language. Scores are averaged across all random seeds.\n3493\n0.8050\n0.8075\n0.8100\n0.8125\n0.8150\n0.8175F1\nen\nX-Mod shared\n0.40\n0.42\n0.44\n0.46\n0.48\nar\n0.74\n0.75\n0.76\n0.77\n0.78\n0.79\nfi\n0.74\n0.76\n0.78\nfr\n0.64\n0.66\n0.68\n0.70F1\nhi\n0.46\n0.48\n0.50\nid\n0.62\n0.64\n0.66\nka\n0.46\n0.48\n0.50\n0.52\n0.54\nko\n0.64\n0.65\n0.66\n0.67F1\nvi\n0.54\n0.56\n0.58\n0.60\n0.62\nru\n0.60\n0.65\n0.70\nsw\n0.510\n0.515\n0.520\n0.525\nta\n10 20 30 40 50 60 70\nnumber of languages\n0.030\n0.035\n0.040\n0.045\n0.050F1\nth\n(a) Pre-Trained Languages\n0.75\n0.76\n0.77F1\nbg\nX-Mod shared\n0.675\n0.700\n0.725\n0.750\nde\n0.70\n0.72\n0.74\nel\n0.5\n0.6\n0.7\nes\n10 20 30 40 50 60 70\nnumber of languages\n0.66\n0.68\n0.70\n0.72F1\ntr\n10 20 30 40 50 60 70\nnumber of languages\n0.40\n0.45\n0.50\n0.55\nur\n10 20 30 40 50 60 70\nnumber of languages\n0.10\n0.15\n0.20\nzh\n(b) Added Languages\nFigure 11: Testset results on NER of pre-trained (top) and added (bottom) languages trained on different numbers\nof languages. Models trained on more languages are trained for longer →all models have seen the same amount\nof examplesin each individual language. Scores are averaged across all random seeds.\n3494\nLanguage iso Family Script 13 30 60 75\nAfrikaans af IE:Germanic Latin ✓ ✓Albanian sq IE:Albanian Latin ✓ ✓ ✓Amharic am Afro-Asiatic Amharic ✓ ✓Arabic ar Afro-Asiatic Arabic✓,(+)✓,(+)✓,(+)✓,(+)Armenian hy IE:Armenian Armenian✓ ✓ ✓Assamese as IE:Iranian Assamese ✓Basque eu Isolate Latin ✓ ✓ ✓Belarusian be IE:Slavic Cyrillic ✓ ✓Bengali bn IE:Iranian Bengali ✓ ✓Bosnian bs IE:Slavic Latin ✓Breton br IE:Celtic Latin ✓Bulgarian bg IE:Slavic Cyrillic + + + +Catalan ca IE:Romance Latin ✓ ✓Chinese zh Sino-Tibetan Chinese + + + +Croatian hr IE:Slavic Latin ✓ ✓ ✓Czech cs IE:Slavic Latin ✓ ✓ ✓Danish da IE:Germanic Latin ✓ ✓Dutch nl IE:Germanic Latin ✓ ✓English en IE:Germanic Latin✓,(+)✓,(+)✓,(+)✓,(+)Estonian et Uralic Latin ✓ ✓Esperanto eo Constructed Latin ✓ ✓Finnish ﬁ Uralic Latin ✓ ✓ ✓ ✓French fr IE:Romance Latin ✓,(+)✓,(+)✓,(+)✓,(+)Frisian fy IE:Germanic Latin ✓Galician gl IE:Romance Latin ✓ ✓Georgian ka Kartvelian Georgian✓ ✓ ✓ ✓German de IE:Germanic Latin +,(✓) +,(✓) +,(✓) +,(✓)Greek el IE:Hellenic Greek +,(✓) +,(✓) +,(✓) +,(✓)Gujarati gu IE:Iranian Gujarati ✓ ✓Hausa ha Afro-Asiatic Latin ✓ ✓Hebrew he Afro-Asiatic Hebrew +,(✓) +,(✓) +,(✓) +,(✓)Hindi hi IE:Iranian Devanagari✓,(+)✓,(+)✓,(+)✓,(+)Hungarian hu Uralic Latin ✓ ✓ ✓Icelandic is IE:Germanic Latin ✓ ✓Indonesian id Austronesian Latin✓ ✓ ✓ ✓Irish ga IE:Celtic Latin ✓ ✓Italian it IE:Romance Latin ✓ ✓ ✓Japanese ja Japonic Japanese +,(✓) +,(✓) +,(✓) +,(✓)Javanese jv Austronesian Latin ✓Kannada kn Dravidian Kannada ✓Korean ko Koreanic Korean ✓,(+)✓,(+)✓,(+)✓,(+)Kurdish ku IE:Iranian Latin ✓ ✓Latin la IE:Romance Latin ✓ ✓\nLanguage iso Family Script 13 30 60 75\nLatvian lv IE:Slavic Latin ✓ ✓Lithuanian lt IE:Slavic Latin ✓ ✓ ✓Macedonian mk IE:Slavic Cyrillic ✓ ✓Malagasy mg Austronesian Latin ✓Malay ms Austronesian Latin ✓ ✓ ✓Malayalam ml Dravidian Malayalam ✓ ✓ ✓Marathi mr IE:Iranian Devanagari ✓Mongolian mn Mongolian Cyrillic ✓ ✓ ✓Nepali ne IE:Iranian Devanagari ✓ ✓Norwegian no IE:Germanic Latin ✓ ✓Oriya or IE:Iranian Odia ✓Oromo om Afro-Asiatic Ge’ez ✓Pashto ps IE:Iranian Arabic ✓ ✓Persian fa IE:Iranian Arabic ✓ ✓Polish pl IE:Slavic Latin ✓ ✓ ✓Portuguese pt IE:Romance Latin ✓ ✓Punjabi pa IE:Iranian Gurmukhi ✓Romanian ro IE:Romance Latin ✓ ✓ ✓Russian ru IE:Slavic Cyrillic ✓,(+)✓,(+)✓,(+)✓,(+)Sanskrit sa IE:Iranian Devanagari ✓ ✓Scottish Gaelic gd IE:Germanic Latin ✓Serbian sr IE:Slavic Cyrillic ✓ ✓Sindhi sd IE:Iranian Arabic ✓ ✓Sinhala si IE:Iranian Sinhala ✓ ✓ ✓Slovak sk IE:Slavic Latin ✓ ✓ ✓Slovenian sl IE:Slavic Latin ✓ ✓Somali so Afro-Asiatic Latin ✓ ✓Spanish es IE:Romance Latin +,( ✓) +,(✓) +,(✓) +,(✓)Sundanese su Austronesian Latin ✓Swahili sw Niger-Congo Latin ✓ ✓ ✓ ✓Swedish sv IE:Germanic Latin ✓ ✓ ✓Tagalog tl Austronesian Latin ✓ ✓ ✓Tamil ta Dravidian Tamil ✓ ✓ ✓ ✓Telugu te Dravidian Telugu ✓ ✓Thai th Kra-Dai Thai ✓,(+)✓,(+)✓,(+)✓,(+)Turkish tr Turkic Latin +,( ✓) +,(✓) +,(✓) +,(✓)Ukrainian uk IE:Slavic Cyrillic +,(✓) +,(✓) +,(✓) +,(✓)Urdu ur IE:Iranian Arabic +,( ✓) +,(✓) +,(✓) +,(✓)Vietnamese vi Austroasiatic Latin✓,(+)✓,(+)✓,(+)✓,(+)Welsh cy IE:Celtic Latin ✓ ✓Xhosa xh Niger-Congo Latin ✓Yiddish yi IE:Germanic Hebrew ✓\nTable 9: List of languages we pre-train ✓on or add + in the different sets (13, 30, 60, 75). ( ·) indicates the\nrespectively different pre-training/added languages of models 1 and 2 as described in §5.2 and Table 4. IE stands\nfor Indo-European.\n3495",
  "topic": "Modular design",
  "concepts": [
    {
      "name": "Modular design",
      "score": 0.685261607170105
    },
    {
      "name": "Curse",
      "score": 0.6185147166252136
    },
    {
      "name": "Transformer",
      "score": 0.5524461269378662
    },
    {
      "name": "Computational linguistics",
      "score": 0.5271149277687073
    },
    {
      "name": "Computer science",
      "score": 0.4975939095020294
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33683064579963684
    },
    {
      "name": "Programming language",
      "score": 0.2962632179260254
    },
    {
      "name": "Engineering",
      "score": 0.29003340005874634
    },
    {
      "name": "Philosophy",
      "score": 0.258208692073822
    },
    {
      "name": "Electrical engineering",
      "score": 0.20748087763786316
    },
    {
      "name": "Theology",
      "score": 0.12386754155158997
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}