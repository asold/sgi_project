{
  "title": "Turning Low-Code Development Platforms into True No-Code with LLMs",
  "url": "https://openalex.org/W4403322492",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5109022115",
      "name": "Nathan Hagel",
      "affiliations": [
        "Karlsruhe Institute of Technology",
        "Institut polytechnique de Grenoble",
        "Laboratoire d'Informatique de Grenoble"
      ]
    },
    {
      "id": "https://openalex.org/A5018721774",
      "name": "Nicolas Hili",
      "affiliations": [
        "Universit√© Grenoble Alpes"
      ]
    },
    {
      "id": "https://openalex.org/A5013799481",
      "name": "Didier Schwab",
      "affiliations": [
        "Universit√© Grenoble Alpes"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3000642371",
    "https://openalex.org/W4200448945",
    "https://openalex.org/W3215068263",
    "https://openalex.org/W6930392341",
    "https://openalex.org/W4308730847",
    "https://openalex.org/W4300660073",
    "https://openalex.org/W3095441296",
    "https://openalex.org/W2787735108",
    "https://openalex.org/W4402860127",
    "https://openalex.org/W2042721203",
    "https://openalex.org/W4205883888",
    "https://openalex.org/W4361222198",
    "https://openalex.org/W1526112747",
    "https://openalex.org/W2125174515"
  ],
  "abstract": "International audience",
  "full_text": "Turning Low-Code Development Platforms into True No-Code\nwith LLMs\nNathan Hagel\nUniv. Grenoble Alpes, CNRS\nGrenoble INP, LIG\n38000 Grenoble, France\nKarlsruhe Institute of Technology\nKarlsruhe, Germany\nnathan@hagel.dev\nNicolas Hili\nUniv. Grenoble Alpes, CNRS\nGrenoble INP, LIG\n38000 Grenoble, France\nnicolas.hili@univ-grenoble-alpes.fr\nDidier Schwab\nUniv. Grenoble Alpes, CNRS\nGrenoble INP, LIG\n38000 Grenoble, France\ndidier.schwab@univ-grenoble-\nalpes.fr\nAbstract\nThe relevance of low-code / no-code development has grown sub-\nstantially in research and practice over the years to allow non-\ntechnical users to create applications and, therefore, democratise\nsoftware development. One problem in this domain still persists:\nmany platforms remain low-code as the underlying modeling layer\nstill requires professionals to write/design a model, often using\nDomain Specific Languages (DSLs). With the rise of generative\nAI and Large Language Models (LLMs) and their capabilities, new\npossibilities emerge on how Low Code Development Platforms\n(LCDPs) can be improved.\nThis paper shows how the capabilities of LLMs can be leveraged\nto turn DSL-based low-code platforms into true no-code. We an-\nalyzed how textual modeling can be replaced by generating the\nrequired model using LLMs. We conducted a user experiment to\ncompare textual modeling with the use of LLMs for that task. Our\nresults show that task completion time could be significantly im-\nproved, and the majority of users prefer using the LLM-aided mod-\neling. Based on these findings, we discuss the integration of these\ntechniques into an existing low-code platform to transform it into\ntrue no-code.\nCCS Concepts\n‚Ä¢ Software and its engineering ‚ÜíSoftware development tech-\nniques.\nKeywords\nLLM, AI, low-code development platform, meta-model, model-driven\nengineering, DSL\nThis is a preprint version. See ACM for the final official version.\nTo appear in the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems\n(MODELS Companion ‚Äô24), September 22‚Äì27, 2024, Linz, Austria.\nhttp://dx.doi.org/10.1145/3652620.3688334\n1 Introduction\nWith information‚Äôs increased value, developing software tools to\nmanipulate, store, and analyze it has become a challenging task.\nThe ability to quickly make decisions based on the collection of\nheterogeneous, possibly large amounts of data originating from\ndifferent sources in real-time requires complex representations and\nadvanced analysis tools that are now more and more integrated into\nlarge Information Systems (ISes), making their development even\nmore complex. In addition, due to the market pressure, companies\nmust reduce production cycle times. This has a negative impact on\ndevelopers, who have to constantly innovate to develop software\napplications faster while maintaining robustness.\nThis paper intends to address this challenge by combining two\nparadigms, namely Low Code Development Platforms (LCDPs) and\nLarge Language Models (LLMs). On the one hand, LCDPs promote\nthe development of fully functional and customizable applications,\nusing visual abstractions and graphical user interfaces while requir-\ning a small amount of code [3]. Leveraging LCDPs for developing\nsoftware systems is appealing to end-users with no programming\nskills who want to have a key role in the process of creating their\nsystems. However, transitioning from ‚Äúlow-code‚Äù to ‚Äúno-code‚Äù is\nstill a research challenge as code is still required to i) formally de-\nscribe the data entities to manipulate and their relationships; ii)\ndescribe how data is aggregated to be presented onto the screen;\nand iii) describe how an IS can be generated from a conceptual\nmodel. Several attempts (e.g., [13]) have been made using custom\nDomain Specific Languages (DSLs) while others rely on the UML\nand UML profiles [5]. In either case, knowledge of data modeling is\nstill a requirement for users of LCDPs.\nOn the other hand, the recent advancements in research and\ndevelopment of LLMs open new perspectives in terms of software\nproduction from textual specifications. Yet, as pointed out several\ntimes in the literature [25, 37], LLMs often produce suboptimal\nresults when used to generate code, forcing developers to spend\ntime reviewing the code generated by the LLMs, thus reducing\nthe possible benefits of using them. One reason that can explain\nthe lack of performance from LLMs is the high expressiveness\nthat general-purpose programming languages such as Java can\noffer. This limitation is, however, counterbalanced in the context of\nlow-code development as the expressiveness a LCDP can offer is\noften curbed by the number of available commercially off-the-shelf\nfunctionalities (e.g., widgets in the context of Web Information\nSystems (WISes)), along with their customization capabilities the\nLCDP‚Äôs User Interface (UI) can offer.\nResearch questions and contributions: The main research question\naddressed in this paper is:\nRQ1. How can existing LCDPs benefit from generative AI and\nLLMs?\nTo answer RQ1, this paper presents an approach to how LLMs\ncan be integrated into model-based LCDPs. The approach‚Äôs goal is\nto consume users‚Äô specifications in natural language and produce\nmodels conforming to textual notation. These models can then\nbe further processed in the respective LCDP to generate, e.g., the\napplication from it. The approach is independent of any of the\nchosen Model-Driven Engineering (MDE) technologies (EMF/Xtext\nHagel et al.\nor others) used in the running example and the case study, see\nSection 4.\nIn order to evaluate our approach, we present in this paper the\nresults of a user experiment where we compared the perceived us-\nability and task completion time of manually creating textual mod-\nels by hand with generating them from an LLM based on the user‚Äôs\nspecifications in natural language. We used for that the OpenAI\nAPI [23] with model version GPT-4o. The results of the user ex-\nperiment show that our approach could significantly improve the\ntask completion time in the chosen case study while not impacting\nusability. The results, logs, and implementation of our approach\ncan be found in the supplementary material [11].\nThis main research question led to the formalization of two\nsubsidiary questions:\nRQ2. Can LLMs completely remove the necessity for coding in\nexisting DSL-based LCDPs?\nTo address RQ2, we conducted an explorative study to verify that,\nalthough the task of generating code conforming to a DSL using an\nLLM that was not trained for it is not trivial, the correct definition\nof the prompt used to interact with the LLM significantly impacts\nthe capabilities of the LLM to produce models that are syntactically\nvalid. During the user experiment, we observed that using our\ndefined prompt template, all generated models were syntactically\nvalid. However, being syntactically valid does not mean that they\nmatch the user‚Äôs specification in natural language. In this paper, we\ndiscuss how our approach contributes to the semantic validation of\nthe generated models.\nRQ3. What are the required steps to transform model-based LCDPs\ninto true no-code by using the capabilities of LLMs?\nFinally, we discuss in this paper the opportunities and challenges\nin integrating generative AI in MDE processes and its benefits for\nLCDP end users and practitioners (RQ3). In particular, we discuss\nhow the integration can not only benefit the user of such platforms\nto describe applications in a no-code approach but also how devel-\nopers of such platforms can take advantage of LLMs to facilitate\nthe customization capabilities.\nPaper structure: the remainder of this paper is structured as fol-\nlows: Section 2 provides background; Section 3 presents the for-\nmulated approach; Section 4 details the user experiment; Section 5\ngives some pointers to turn low-code into no-code using LLMs;\nSection 6 discusses the current status of the implementation and\nfuture work; Section 7 presents related work; Section 8 concludes.\n2 Foundations\nThis section gives an overview of the relevant foundations and\nbackground required for the present work.\n2.1 Low-/No-Code Development\nLCDPs allow citizen developers (developers with little or no soft-\nware engineering background [22]) to rapidly deliver, set up, and\ndeploy applications, reducing the amount of hand-written code to\na minimum [29]. Therefore they are appealing to a larger group\nof stakeholders, as less or no software engineering knowledge is\nrequired to develop applications that meet the stakeholders specific\nneeds.\nOne relevant aspect of LCDP is the rapid creation, deployment,\nand release of new versions of the created application. As web-\nbased systems are often easier and cheaper to deploy than desktop\nones [31], they are more aligned with low-code development [28].\nLCDPs facilitate development through, e.g., drag‚Äôn‚Äôdrop-oriented\ninteractions or the composition of pre-built components or widgets.\nIdeally, the development process and editors are built into a single\napplication and UI, therefore further guiding the user through de-\nvelopment and sometimes even deployment. Further specification\nand customization can sometimes be added through textual speci-\nfication, or the whole composition/modeling process is based on\ntextual modeling. Business logic is usually described by workflows\nor processes using Business Process Model (BPM) and Notation\n(BPMN 2.0) [21] or equivalent languages. One important benefit of\nlow-code tools lies in their high potential in terms of customization\nof the UIs to fit end-users specific needs [29]. However, implement-\ning low-code customization capabilities in an end-user UI is not\ntrivial. MDE has the potential to facilitate that task.\n2.2 Model-driven Engineering (MDE)\nMDE is nowadays an established discipline for developing complex\nsoftware systems [19] by reducing platform complexity through\nadding a layer of abstraction on top of the programming languages\n[15, 30]. In MDE conceptualmodels are primary artifacts in software\ndevelopment processes, and techniques such as model transforma-\ntion are used to progressively refine abstract models into working\nsoftware solutions [15]. These system models can be represented\nby a variety of languages, such as UML or even custom DSLs, that\nare defined to model a specific problem.\nTherefore, a meta-model (aka. abstract syntax ) is the key ingre-\ndient to formalize models. A meta-model is a set of meta-classes\nwhich represent the concepts that define the domain and relations\nthat specify how the concepts can be bound together in a model.\nFurthermore, a set of well-formed rules restrict the way concepts\ncan be assembled to form a valid model.\nDifferent meta-modeling frameworks exist to specify meta-mo-\ndels, e.g., Eclipse Modeling Framework (EMF) [34] and MetaEdit+\n[14]. Dedicated to web-based technologies, new frameworks have\nemerged, e.g., EMF-REST [8], Ecore.js1, the JavaScript Modelling\nFramework (JSMF) [33], and FlexiMeta [12]. One or many concrete\nsyntaxes can then be specified for the same meta-model based on the\nabstract syntax defined using, e.g., the frameworks above. These can\nbe graphical or textual. Using Xtext [9] (a grammar-based approach),\nfor instance, the meta-model and a concrete textual syntax can be\ndefined at the same time [26]. The combination of a meta-model\nwith one or more concrete syntaxes is often also named a DSL and\npart of LCDPs.\nRunning Example DSL. The running example used in the user\nstudy in Section 4 uses a DSL to define web forms [13]. The meta-\nmodel in a graphical notation gives an overview of the available\nconcepts, see Figure 1. It allows the definition of web forms in a\nrow-based layout. Several widgets can be defined, e.g., a text or a\n1A JavaScript implementation of the Eclipse Meta-Object Facility (MOF).\nTurning Low-Code Development Platforms into True No-Code with LLMs\nLayout Rowrows\n[*]\nwidgets\n[1..*]\nimports[*]\nstylings\n[*]\nImport\nimportURI : String\nStyling\nname : String\nwidth : String\nheight : String\npadding : String\nborder : String\nfont-size : String\nfont-family : String\ncolor : String\nbackground-color : String\nborder-radius : String\nmargin : String\nWidget\nreferences : EStructuralFeature[]\nlabel : String\nicon : Stringstyling\n[0..1]\nTextInput\nplaceholder : String\nRichTextEditor\nplaceholder : String\nTextarea\nplaceholder : String\nCalendar Combobox\nDateInput TagsInput\nFigure 1: Running example: overview of the layout meta-model (extended version of [13]).\ncalendar input. This DSL can be used for both textual and graphical\nmodeling. However, for the running example, the meta-model is\ndefined using Xtext. This means that a textual concrete syntax for\ncreating models is already defined. An example model using this\nsyntax is provided in Listing 1.\nBesides defining the structure of the form and the type of widgets,\nthe DSL allows us to define to which data model attributes the\nwidget is linked using the references attribute. This means that\nanother input for the running example is a data model, which can\nalso be defined in a textual syntax and, therefore, generated by an\nLLM if required. The experiment used two data models, one for a\nconference event and one for a hair salon. Therefore, the widget\nentity features the attribute references of type EStructuralFeature in\ncase of a definition of the data model using Ecore. Lastly, a widget\nhas an icon and a label to customize and adjust the form. From\nthe layout created from Listing 1 we used MDE techniques (see\nSection 3) to generate the preview in Figure 2. Only the last two\nrows (4 widgets) are contained in the example DSL code in Listing 1\nfor space reasons. One of the contributions of this paper is to replace\nthe textual creation of this model with LLMs.\n2.3 Large Language Models (LLMs)\nLLMs can be used to generate text based on an input provided by\nthe user. This means, that given a sequence of input tokens, LLMs\ncan estimate the probability of the next output token. They can be\ncustomized for specific tasks using fine-tuning, which updates the\nweight of parameters and, therefore, influences the token genera-\ntion [27]. These capabilities, especially with the upcoming general\nlarge language models like ChatGPT [24], created a variety of use\ncases, from generating programming code or texts to proofreading\nor error detection and many more [36]. To extend these capabilities,\nseveral methods exist to adjust or improve the generated output.\nAs mentioned above, fine-tuning is one. However, it comes with a\nsignificant drawback as for effective fine-tuning big datasets are\nrequired. Creating them can be quite costly [20, 32].\n// missing row 1 (2 widgets) for space reasons\nrow {\ntext input {\nreferences Event.location\nlabel \"Location\"\nicon \"icon-location-pin\"\nplaceholder \"Enter the location\"\n}\ncombobox {\nreferences Event.type\nlabel \"Type of Event\"\nicon \"icon-options\"\nplaceholder \"Select the event type\"\n}\n}\nrow {\ncalendar {\nreferences Event.begin Event.end\nicon \"icon-calendar\"\nlabel \"Event Dates\"\nplaceholder \"Choose the event dates\"\n}\ntextarea {\nreferences Event.cfp\nlabel \"Call for Papers\"\nicon \"icon-doc\"\nplaceholder \"Enter the call for papers\"\n}\n}\nListing 1: Example of a model conforming to the layout meta-\nmodel given in Figure 1 in an arbitrary Xtext concrete syntax.\nHagel et al.\nFigure 2: Preview of the layout model given in Listing 1 (the\nfirst row is hidden in Listing 1 for space reasons).\nAn alternative method is prompt engineering. A prompt is the\ninformation provided to the LLM, which can, in particular, contain\ninstructions or a framing for a specific context. When generat-\ning code, for instance, these instructions could define a code style,\netc. [36]. Prompt engineering is the technique of defining/creating\nprompts for the required task. Using prompt engineering, competi-\ntive results compared to fine-tuning can be achieved with a usually\nlower effort [32]. Prompt engineering can reach from simply adding\nsmall instructions to generic prompts to providing sophisticated\ntemplates that structure the requests and add additional information\nor instructions to improve results [36].\n3 Approach\nIntegrating LLMs into LCDPs changes the way how the user in-\nteracts with the platform. This also depends on how the LCDP is\nstructured and how the development process already looks. We\nassume in this section that the production of an application, e.g., a\nWIS, in an LCDP is similar to the way an application is produced fol-\nlowing conventional MDE processes: i) the specifications of the user\nare captured; ii) models of the desired application are created; and\niii) an implementation is produced, either through code generation\nor model interpretation.\nIn this section, we propose an approach (see Figure 3) where the\nmodeling step is replaced with generating the required models using\nLLMs. The input of the approach is the user‚Äôs specifications of the\ndesired application in natural language (step 1). Then, the chosen\nLLM is queried (step 2), which generates the models (step 3). Finally,\nthe models are processed (step 4), and the desired application is\nproduced (step 5).\nThe proposed approach is platform-independent: it does not\nrequire a specific LLM to be used since it does not require a specific\ntraining phase; it does not depend on a specific modeling framework\nsince the heart of the approach only relies on models conforming\nto dedicated textual syntaxes that any kind of parsers could process.\nFinally, it does not restrict the way an application is produced from\nthe models, being, e.g., code generation or model interpretation. In\nour experiments, we used the OpenAI API [23] with model versions\nGPT-3.5 Turbo, GPT-4 Turbo, and GPT-4o for the user study as a\nthird party LLM, Xtext for the definition of the textual notations\nthat were used, and the LCDP presented in [13] to generate the\nresulting WIS.\nThe capabilities of the approach to generate correct models, i.e.,\nfrom both a syntactical and a semantic point of view, depends on\ntwo main factors: first, the user specification alone is not enough\nfor the LLM to generate syntactically verified models for DSLs it\nwas not trained for. To improve this step, we enriched the prompt\ncontaining not only the user‚Äôs specifications but also all relevant\ninformation an LLM requires to generate valid models. Second, due\nto the non-deterministic nature of an LLM to produce code from\na natural language specification, some validations must be done\nto ensure that the resulting models fulfill the user‚Äôs expectations.\nTo do so, we implemented a feedback loop (step 6) where the user\nhas the possibility to inspect the results (the generated model, the\napplication or both) and make changes.\n3.1 Prompt Engineering\nWhen using new, small, or proprietary LCDPs, it is quite likely\nthat no data, examples, or information at all was included in the\ntraining data sets these models were trained on. This limits the\napplicability of LLMs for LCDPs. One way to inform the LLM about\nthe LCDP‚Äôs internals and resources could be fine-tuning by creating\na big training data set and adjusting the model‚Äôs context. However,\nthis approach may be too expensive [20, 32]. As mentioned earlier,\nprompt engineering overcomes these drawbacks, as it does not\nrequire large training data sets and is less compute-intensive than\nfine-tuning. This technique is particularly suitable in the context\nof DSLs, which are reasonably small ‚Äì or at least smaller than\ntheir general-programming counterparts ‚Äì, dedicated languages\nfor specific domains.\nExample\nModels\nDesired System\n(Natural\nLanguage)\nUser\n(1) specifies\nIntegrated Prompt\nLLM\n(2) queries\n(3) generates\nModel\nLCDP processes\n(5) produces\nApplication\nLow-Code Development Platform\nGrammar\ninspects (and redefines) (6)\n(4) process\nFigure 3: Overview of the proposed model-based approach\nintegrating LLMs for developping no-code applications.\nTurning Low-Code Development Platforms into True No-Code with LLMs\nUsing the following information:\n[Additional Information]*\nAnd this Xtext grammar as the grammar for the DSL:\n[Xtext Grammar]\nAnd the following additional information:\n[Additional Information]*\n[I give you the following example: [example]]+\nPlease create a new example that conforms to the\ntext below:\n[User specification in natural language]\nThe content, naming of variables, etc., can be\nfreely chosen.\nNo additional text, explanation,etc.\nDo not add markdown tags or anything.\nListing 2: Prompt template with placeholders that can be\nused to generate textual models.\nThe structure of the prompt has an important impact on the\nLLM that may (positively or negatively) impact the outcomes of\nthe LLM. It has to be meticulously designed and tested to assess\nits efficiency, hence the notion of prompt engineering . This section\ndiscusses the design of the prompt template we used in the context\nof our approach.\nListing 2 details the template we defined for our approach. It\nconsists of predefined instructions (in black) and placeholders for\ninformation and resources (in blue). An arbitrary amount of ad-\nditional information, such as resources that should be referenced,\nother models (like the data model used in the running example and\nthe experiment in Section 4), etc., can be inserted at two locations\nif required. As the input of an LLM is a text string, it is important\nthat all information contained in the prompt can be serialized to\ntext. In MDE, a textual representation of models (and metamodels)\nis the norm. This representation can be a custom textual syntax or\nfor graphical DSLs a serialization into common formats, e.g., XMI,\nJSON [13, 4, 16].\nThe template proposed in Listing 2 assumes that the DSL‚Äôs syntax\nis defined with Xtext [9] and that the examples given are encoded\naccording to this grammar. However, we found out during our tests\nthat replacing the Xtext grammar with, e.g., Extended Backus-Naur\nForm (EBNF), and providing additional information like the data\nmodel required for our running example in different representa-\ntions (serialized Ecore models - XMI or textual representation of\nFleximeta models) also provided comparable results. Therefore,\nthe template proposed in this paper is not strictly dependent on\nthe technologies we used, and even though we did not further in-\nvestigate our findings, we are confident that alternative prompt\ntemplates would also work to generate the desired models. Com-\nparing different templates is outside the scope of this paper, and\nevaluating their impacts on the generated models is future work.\nDuring our experiments, we observed that only providing the\n(Xtext) grammar to the LLM is not enough to produce syntactically\nvalid models. We found out that providing at least one example\nmodel conforming to the DSL‚Äòs grammar considerably increased\nthe quality of the produced models, and we recommend providing\nenough examples to cover the different parts of the grammar. It\nis worth noting that one limit of LLMs is the context size, which\nrestricts the amount of data we can provide the LLM with. But,\nover the past years, the context size of mainstream LLMs such as\nChatGPT exponentially grew, making the context size no longer\na practical limit. However, mainstream LLMs and APIs, such as\nOpenAI API [23], rely on a financial model where the user is billed\naccording to the consumed number of (both input and output)\ntokens. In the future, we plan to explore other LLMs, including\nopen-source LLMs such as Mistral [18] or Llama 3 [17].\n3.2 Incremental Process\nProviding a good prompt template is enough for the LLM to produce\nsyntactically valid models with respect to the DSL definition, but\nit does not provide any guarantees that the produced models are\nsemantically correct with respect to the user‚Äôs specifications. This\nrequires a validation from the user. To do so, we implemented in\nour approach a feedback loop (see Fig. 3) where the user has the\npossibility to inspect the results and apply changes. In the tool\ndescribed in Section 4, the user‚Äôs feedback can be integrated in\ntwo different ways: either by making manual changes directly to\nthe generated models or by refining his/her demand in the prompt.\nObviously, the former is dedicated to MDE practitioners, while the\nlatter better fits the expectations of end users with no modeling\nbackground. This requires the LCDP to produce a preview of the\napplication produced from the models.\nThis has an impact on the prompt discussed in the previous sub-\nsection: not only the current prompt should be sent to the LLM, but\nalso information about the history of the conversation, similar to\nthe mechanisms applied when using tools like ChatGPT. Therefore,\nthe template presented in Listing 2 presents only the beginning\nof the conversation and is further completed with the user‚Äôs mes-\nsages refining the original request and responses from the LLM\ninterleaved with each other.\nIt is worth noting that this impacts the context size, as it will\ncontinuously grow with the number of user-requested changes. But,\nas mentioned earlier, recent technological advances in mainstream\nLLMs reduce the importance of this cap, and as soon as the context\nwindow‚Äòs limit is reached, the provided history could be reduced\n(e.g., by deleting the oldest messages).\n4 Experiment Design\nTo evaluate the effectiveness and usability of integrating LLMs into\nexisting LCDPs (RQ1), a user study was conducted. The user study‚Äôs\nfocus was the comparison of textual modeling with the creation\nof the desired model and, therefore, application using LLMs and\nnatural language specification.\nImplementation. For this comparison, we chose a simple low-\ncode approach that allows the definition of web forms that are\nconnected to a data model. We integrated a chat interface into\nthe UI, see Figure 4 that is similar to ChatGPT‚Äôs user experience.\nThe chat interface can connect to different LLM-versions like GPT-\n3.5-turbo or GPT-4o. For the experiment, we used only OpenAI‚Äôs\nmodels. However, the approach, in general, is not limited to Ope-\nnAI‚Äôs APIs. To improve the generated results, an engineered prompt\nas described in Subsection 3.1 was used. This means that the user\ninput was inserted at the correct location in the defined prompt\nbefore being sent to the API. The response from the LLM was then\ndirectly inserted into the low-code editor, see Figure 5 on the left,\nHagel et al.\nFigure 4: User Interface of the user experiment implementation (Condition 1) - LLM Chat Interface on the left, preview of the\ngenerated form on the right\nwhich resulted in the generation of the desired web form in our\nimplementation. Additionally, the response was displayed in the\ngreen chat box of the chat interface; see Figure 4 on the left. The\nweb form is only generated if the generated DSL code is syntax\nerror-free. If errors exist, they are displayed to the user in a similar\nway as in an IDE. The error detection was based on the grammar\nof the used Layout DSL in this experiment. The implementation\npreserved the history of the session, which means that users were\nable to reference previous generations or the current state of the\nmodel for error correction or improvements, see Figure 4.\nThe creation of web forms using the low-code approach and the\nchat interface was compared with the manual creation of the DSL\ncode / the models. To create the web forms manually, a code editor\nwas included in the UI, see Figure 5 on the left. Users could create\ntextual models, which immediately generated the preview on the\nright as soon as the model was syntactically correct.\nParticipants. We conducted the experiment with 18 participants\n(9 male, 9 female), aged 18-44 (M=26,78, SD=7,69). All participants\nwere part of computer science research groups in Grenoble Com-\nputer Science Laboratory (LIG). A minority (3 out of 18) had ex-\nperience in modeling and DSLs. The chosen low-code approach /\nDSL was unknown to every participant. Most of the participants\n(66.6%) stated that they use LLMs like ChatGPT Very Frequently or\nFrequently. The experiment was conducted as a supervised within-\nsubjects design. Each participant was exposed to both interactions\nand had to create forms using the chat interface and the manual\ntextual modeling. The participants were randomly split into two\ngroups. One group started with the chat interaction, whereas the\nother started with the manual modeling.\nExperiment Setup. The experiment was conducted in a lab un-\nder supervision. For every participant, the same technical setup\n(computer, monitor, etc.) was used. The main interaction happened\nthrough the keyboard and mouse. As the participants were inter-\nnational and, therefore, preferred different keyboard layouts, we\nprovided a suitable keyboard and settings as requested by the par-\nticipant. Every participant used both conditions (chat and manual\ncreation). At the beginning of the experiment we provided a general\nintroduction to the experiment, the used DSL / low-code platform,\nthe user interface etc. The participants had to create web forms for\ntwo data models, which were also provided in graphical (Ecore) and\ntextual (Fleximeta) notation. The data models were also explained\nto the participants. The grammar (Xtext) of the DSL was also pro-\nvided. Written scenarios described the web forms that had to be\ncreated, containing all the information required to create them and\nhow they had to look. The written scenarios were provided to the\nparticipants one by one for each task. As an additional resource,\nthe participants received documentation of the DSL and examples.\nFurthermore, an overview of the possible widgets and how they\ncould be configured was provided. If the participant had questions\nabout the mode, the experiment, or the resources provided, these\nquestions were answered. For each interaction, the participant re-\nceived another introduction about how the user interface works\nand what is expected. In total, each participant had to create 6 web\nforms, 3 for each data model and interaction. One task for each of\nthe two conditions was to get familiar with the low-code approach\nand how the tooling works. Therefore, the data for the results is\nbased only on two tasks for each condition. Each scenario had the\nsame size and expected the same amount and variety of widgets.\nAfter each condition (1 + 2 tasks), the participant had to answer\na questionnaire, including the System Usability Scale (SUS) ques-\ntions, to assess the perceived usability. In total, the duration of each\nexperiment was 70-90 minutes.\nTask. The task was to create web forms according to a speci-\nfication defined in a scenario. In total, 6 scenarios were defined,\nTurning Low-Code Development Platforms into True No-Code with LLMs\nFigure 5: User Interface of the user experiment implementation (Condition 2) - Low-Code Editor for the Layout DSL on the left,\npreview of the generated form on the right.\ntherefore 6 tasks, and had to be completed by each participant. The\nscenarios described the data model attributes the web forms had to\nask for. The different scenarios used two different data models. One\nis for a hair salon appointment/management tool, and one is for\na conference event management tool. Furthermore, the scenarios\ncontained a broad specification on the desired layout itself (e.g., use\nonly one widget per row to create a mobile-first form). The partic-\nipants were asked to read the scenario and ask questions if they\nhad any. Then, the participants had to create the form using the\nchat interaction or manual modeling. The layouts were not strictly\ndefined. A task/scenario was successfully completed as soon as all\nspecified widgets were correctly included in the form. Furthermore,\nthe labels and placeholders had to be defined appropriately for the\nsemantics of the widget‚Äôs data attribute. Also, the icon had to be\nset appropriately. A generated form from scenario 1 is visible in\nFigure 2. It is used to create a new conference event in a conference\nevent management tool. An example of a prompt that generated this\nform/the respective DSL code (see Listing 1) is visible in Listing 3.\nMetrics. During the experiment, we measured the following met-\nrics:\nCreate a form to create a new conference event.\nThe form should ask for the event's name, its\nacronym, the location and the type of the event.\nThe form must only allow possible choices for the\ntype of the event.\nLastly the form should ask for the events dates\nand the call for paper.\nUse two widgets per row.\nListing 3: Example prompt that generates a web form for\nscenario/task 1\n‚Ä¢(M1) Task completion time: Measured from the first key-\nstroke in the tool until the generated or created form ful-\nfilled the task‚Äôs requirements.\n‚Ä¢(M2) Personal preference: Evaluated through a question in\nthe questionnaire filled out by the participant after finishing\nall tasks.\n‚Ä¢(M3) Usability: Evaluated through an SUS [1] score ques-\ntionnaire that was filled out after each condition.\nThe decision why we started the timer of M1 at the first keystroke\nin the tool and not the moment when the user received the task\nwas made because each user had to read through the material, and\nsome had questions, which we did not want to measure in this\nexperiment. Rather, we decided to measure the time from when the\nuser understood the task until it was completed. We informed the\nparticipants that they should start with the task as soon as they\nunderstood the task and had no questions.\nHypotheses. We defined the following hypotheses prior to our\nexperiment:\n‚Ä¢(H1) We expect that using the chat interaction to generate\nthe models is significantly faster than creating the models\nmanually.\n‚Ä¢(H2) We expect that the majority of the participants will\nprefer the chat interaction to the manual creation of models.\n‚Ä¢(H3) We expect that the usability of interactions is consid-\nered good while the chat interaction will receive a better\nusability rating.\nEthical Considerations. Before the experiment, participants were\ninformed about the scope and purpose and potential ethical consid-\nerations. The risk was evaluated to not exceed the risk of living and\nusing computers. Every participant explicitly agreed to participate\nin the study and allowed the results and artifacts to be published\nHagel et al.\nfor research purposes. All data, logs, etc., are fully anonymized,\nensuring confidentiality.\nResults. This paragraph describes the results of the conducted\nuser experiment:\nTask completion time (M1): The results of the task completion time\nfor each scenario are depicted in the box plot in Figure 6. Scenarios 1,\n2, and 5 in yellow, orange, and red, respectively, were the scenarios\nfor the creation of models using the chat interaction. Scenarios 4, 6,\nand 3 in green, cyan, and blue, respectively, were the scenarios for\nmanually creating the models. The scenarios were also used in that\norder, meaning that depending on the condition with which the\nparticipant began, we started with scenarios 1 or 4 and continued\nwith 2 and 5, respectively, 6 and 3. This results in scenarios 1 and 4\nbeing the test or learning tasks. The task completion times for these\ntwo scenarios were also included in Figure 6 but are not included in\nthe further interpretation and evaluation of the results. We used a\npaired t-test to evaluate the impact of different systems on the task\ncompletion time. Results show a statistically significant difference\nbetween the two processes.\nùë° (17)= ‚àí8.55, ùëù< 0.001\nThe mean and the standard deviation of the task completion time\nfor the chat interaction and manual creation are the following\n(the values origin from the combination of the two scenarios per\nprocess):\n‚Ä¢ Chat: ùëã = 187.5, ùúéchat = 44.35\n‚Ä¢ Manual Creation: ùëã = 421.56, ùúémanualCreation = 32.49\nThe results indicate that using the chat interaction is significantly\nfaster than creating the models by hand.\nPersonal Preference (M2) and Perceived Usability (M3): We used a\npaired t-test to evaluate the impact of the two different processes\non the perceived usability using an SUS score questionnaire. Re-\nsults show no statistically significant difference between the two\nprocesses.\nùë° (17)= 1.0095, ùëù= 0.3268\nThe mean and standard deviation of the SUS score for chat interac-\ntion and manual creation are the following:\n‚Ä¢ Chat: ùëã = 80.0, ùúéchat = 10.74\n‚Ä¢ Manual Creation: ùëã = 75.83, ùúémanualCreation = 9.47\nWe can see that the average SUS score of both processes is be-\ntween good and excellent using the categorization of Bangor et al.\n[1] with the chat interaction having a slightly higher mean. The\nstandard deviation is almost the same. In literature, an average SUS\nscore of 70 is considered average and a passing criterion for user\ninteractions [1]. Both processes scored above that.\nThe questionnaire also assessed the personal preference of the par-\nticipants, as every participant was exposed to both conditions. 15\nout of 18 participants preferred the chat interaction.\nDiscussion. In the user study, the chat interaction to generate\nthe models performed significantly better in task completion time\ncompared to the manual modeling. The perceived usability, though\nhaving a higher mean, is not significantly better. However, par-\nticipants reported that they explicitly liked the direct feedback\nScenario 1(*)\nConference creation\nScenario 2\nSession chair registration\nScenario 3\nPaper submission\nScenario 4(*)\nHair salon creation\nScenario 5\nAppointment booking\nScenario 6\nService creation\n100\n200\n300\n400\n500\n600\n700\nTask completion time (s)\nFigure 6: Box Plot of the task completion time of scenario\n1 to 6. Results for test scenarios (*) are not accounted in the\npaired t-test evaluation. Scenarios in bold corresponds to\ntasks involving the LLM.\nprovided by the implemented UI when creating the models manu-\nally. Both ways of creating the desired models scored betweenGood\nand Excellent on the SUS scale/mapping to adjectives [1]. Therefore,\nour results are consistent with (H1) where we suspected that gener-\nating the models based on natural language is faster than manual\ntextual modeling. As 15 out of 18 participants preferred the chat\ninteraction, our results are also consistent with (H2). (H3) has to be\npartly rejected. Both ways of modeling received a rating of at least\ngood, though the chat interaction did not receive a significantly\nbetter usability score.\nIn conclusion, our results show that using LLMs with a suitable\nenriched and structured prompt as defined in Subsection 3.1 seems\nto be a good way to improve the efficiency of the modeling step\nand, therefore, the whole low-/no-code development process. Fur-\nthermore, a well-working integration does not reduce the perceived\nusability but rather adds a new way of using an existing LCDP. This\nanswers RQ1. However, we expect that the benefits that LLMs can\nhave on LCDPs are not limited to that.\n5 Transformation from low-code to true\nno-code\nBased on the conducted integration of ChatGPT into the layout\nweb-form LCDP as well as an ongoing integration into an extension\nof that [13], the following section describes a set of prerequisites,\nsteps, and challenges that we identified throughout our analysis\nand the conducted user study. These learnings and steps combined\nwith the results of the experiment answer RQ3.\nPrerequisites. For most LCDPs, we assume that available state-\nof-the art LLMs will not be able to generate correct models out-of-\nthe-box. As described in subsection 3.1, prompt engineering can be\nTurning Low-Code Development Platforms into True No-Code with LLMs\nused to enable the LLM to generate correct models. However, we\nexperienced that a set of resources seems to be crucial to program\nthe LLM appropriately for that task [36]. We identified that (i) the\nlanguage‚Äôs grammar and (ii) examples that cover most or ideally all\nfeatures of the modeling language should be included. This means\nthat these resources must be prepared and defined. For LCDPs\nwhich do not use a modeling language with a graphical syntax, this\ncan mean that defining it could be helpful. In many cases, this can\nbe easily achieved, for instance, using Xtext when using modeling\nlanguages from the EMF ecosystem [26]. In other cases, this step\ncould require more integration work, and Xtext could still be helpful\nin defining the language‚Äôs grammar.\nSteps. For properly using LLMs in LCDPs we think that it is\ncrucial to directly integrate them into the existing tools, similar to\nGithub Copilot [10] for IDEs. One indicator for this conclusion is\nthe results of the user experiment, see section 4, where the SUS\nscore was quite similar for using textual modeling and generating\nthe models using the chat interface. Without integration, this would\nmost likely mean additional steps for the user, which we expect\nwill reduce the perceived usability and efficiency. For a successful\nintegration, we therefore propose the following steps: (1) Define\nthe LLM that should be used; (2) Create or define the required re-\nsources and conduct tests until the generated results match the\nrequirements. This step could require several iterations. Then, (3)\nintegrate them into the prompt. Find and implement an appropriate\nuser interface within the existing LCDP. One example can be seen\nin Figure 4. This highly depends on the possibilities of the platform\nitself. (4) Directly integrate the generated models into the UI by\nadding a preview as in Figure 4 or a graphical or textual representa-\ntion of the generated model. (5) Use a stateful chat interface that is\nsession-based. This ensures that the user can reference the previous\ngeneration and interact with and redefine it.\nChallenges. We experienced challenges such as ensuring a high\nrate of correct generations, which is essential for a well-working\nuser experience. This also depends on the complexity of the used\nmodeling language. Using a prompt template as defined in Subsec-\ntion 3.1 or a modified version can help. However, examples should\nbe wisely chosen so as not to unnecessarily increase costs when\nusing billed APIs.\n6 Discussion\nCombining LLMs with MDE presents two benefits in the context of\nlow-code/no-code development. The first benefit targets end users\nwho can leverage generative AI to model their applications and\ncustomize the interfaces to represent the data on the screen accord-\ning to their preferences. By properly integrating LLMs, this entire\nprocess can be performed without modeling or coding experience\nby only using natural language. This can open LCDPs to more users.\nFurthermore, in the experiments we conducted, we observed that\nmodels can be successfully generated using LLMs like ChatGPT\nand the prompt template defined in Subsection 3.1. Implementing\nthis approach, the results of the conducted user study show that\nthe efficiency of the chosen LCDP could be significantly increased\nwithout impacting usability. Furthermore, 15 out of 18 participants\npreferred defining the system in natural language, which also high-\nlights the need to integrate these mechanisms into LCDPs. These\nimprovements were possible without fine-tuning a model but only\nby applying prompt engineering and the integration into the UI.\nRemoving the need for fine-tuning reduces not only the costs but\nalso the additional required environmental resources. However, the\nevaluation of the approach lacks the comparison if LLMs could also\nbe used to reduce the customization complexity of LCDPs, which\ncould be an extended application. In [13], we presented a platform\nwhere the graphical user interfaces can be easily modified by means\nof drag‚Äôn‚Äôdrop and by changing one widget by another. The experi-\nment should be extended to compare the two different modalities\n(standard mouse and click capability vs. the use of LLMs).\nThe previous observation leads to a second, less obvious benefit\nLCDP practitioners can take advantage of by extending the cus-\ntomization capabilities of LCDPs. The customization capabilities are\noften constrained by the set of, e.g., customizable widgets in the do-\nmain of WISes an LCDP has to offer, along with the user interfaces\nto customize the appearance and behavior of those widgets [2]. As\nexpectations about LCDPs grow, widgets become increasingly com-\nplex in size and customization capabilities, and LCDP developers\nmust make a trade-off between customization and the complexity\nof the platform. A commonly accepted compromise is to implement\ncoarse-grained customization capabilities available through no-\ncode user interfaces and fine-grained ones only through extending\nthe created code or additional textual settings. Hence, improving the\nusability of such interfaces for end-users LLM could play a relevant\nrole in facilitating the fine-grained customization of applications\nwhen no alternative UI capabilities are available or too much work\nto implement. Although we can‚Äôt answer RQ2 for all LCDPs, we\ncan for confidently say that it is indeed possible to eliminate cod-\ning entirely from DSL-based LCDPs by properly integrating the\ncapabilities of LLMs. However, the limitations of this approach will\nlikely co-develop with the limitations of state-of-the-art LLMs.\n7 Related Work\nThis section reviews related work on the topic focussing on the\napplication of LLMs for DSLs and LCDPs. Busch et al. [6] present\na Low-/No-Code approach to creating applications using first, a\ngraphical model created by the user and second, a prompt frame.\nThis prompt frame is then filled with a natural language descrip-\ntion of the user to extend the existing model with e.g., semantics.\nThe goal of the prompt is to generate code, which is added to the\ncode generated from the graphically defined model. Together, these\ntwo pieces create the final application. The approach was success-\nfully implemented in a running example to create a point-and-click\nadventure game. Grammar Prompting by Wang et al. focuses on\nthe generation of grammar-based DSLs in general (not only DSLs\nin the context of LCDPs) [35]. However, their approach could be\napplied and useful when integrating LLMs into existing LCDPs as it\ndescribes a way to create prompts that successfully generate DSLs\nby only providing parts of the grammar. The full grammar is only\nprovided for small DSLs. This approach, similar to subsection 3.1,\nalso requires examples in the prompt, which are in contrast to this\nproposal, labeled. They contain a query, a minimal provided gram-\nmar, and the result. Their prompt template requests the LLM to first\nHagel et al.\ngenerate a minimal grammar before generating the actual DSL.\nBesides these grammar and textual DSL based approaches, other\nwork focuses on the interpretation and translation of hand-drawn\nmodels into more formal representations like UML models [7].\nThese sketches can then be used as input for an LLM to generate\nUML models, which then form the input for LCDPs or a modelling\npipeline. Integrating this approach into LCDPs could just as well\nimprove the usability for MDE and low-code approaches.\n8 Conclusion\nIn this paper, we presented an approach to transform DSL based\nLCDPs into true no-code. The transformation is achieved by al-\nlowing users to define the desired application in natural language,\nfrom which first the models of the LCDP are generated, and the\nLCDP then creates the desired application. The approach is vali-\ndated through a user experiment with 18 participants, where results\nshow, that the task completion time could be significantly reduced\ncompared to manually using the LCDP without impacting the per-\nceived usability. We also presented a prompt template to generate\ntextual models/DSLs which can help when integrating LLMs into\nexisting LCDPs. In conclusion, LLMs can help to improve existing\nLCDPs. They can be used to either replace or aid the modeling step\nwhere the application is defined and improve the development time.\nAcknowledgments\nThis work was supported by MIAI@Grenoble Alpes, (ANR-19-P3IA-\n0003) and the pilot program Core Informatics at KIT (KiKIT) of the\nHelmholtz Association (HGF).\nReferences\n[1] Aaron Bangor, Philip Kortum, and James Miller. 2009. Determining what indi-\nvidual SUS scores mean: Adding an adjective rating scale. Journal of Usability\nStudies, 4, 3, 114‚Äì123.\n[2] Tina Beranic, Patrik Rek, and Marjan Hericko. 2020. Adoption and usability\nof low-code/no-code development tools. In Central European Conference on\nInformation and Intelligent Systems . Faculty of Organization and Informatics\nVarazdin, 97‚Äì103.\n[3] Alexander C Bock and Ulrich Frank. 2021. In search of the essence of low-\ncode: an exploratory study of seven development platforms. In 2021 ACM/IEEE\nInternational Conference on Model Driven Engineering Languages and Systems\nCompanion (MODELS-C) . IEEE, 57‚Äì66.\n[4] Alexandre Bragan√ßa, Isabel Azevedo, Nuno Bettencourt, Carlos Morais, Diogo\nTeixeira, and David Caetano. 2021. Towards supporting SPL engineering in\nlow-code platforms using a DSL approach. In Proceedings of the 20th ACM\nSIGPLAN International Conference on Generative Programming: Concepts and\nExperiences, 16‚Äì28.\n[5] Alan Bubalo and Nikola Tankovic. 2023. Modeling Low-Code Databases With\nExecutable UML. Human Systems Engineering and Design (IHSED 2023): Future\nTrends and Applications , 112, 112.\n[6] Daniel Busch, Gerrit Nolte, Alexander Bainczyk, and Bernhard Steffen. 2023.\nChatGPT in the loop: a natural language extension for domain-specific model-\ning languages. In International Conference on Bridging the Gap between AI and\nReality. Springer, 375‚Äì390.\n[7] Aaron Conrardy and Jordi Cabot. 2024. From image to uml: first results of image\nbased uml diagram generation using llms. arXiv preprint arXiv:2404.11376 .\n[8] Hamza Ed-Douibi, Javier Luis C√°novas Izquierdo, Abel G√≥mez, Massimo Tisi,\nand Jordi Cabot. 2015. Emf-rest: generation of restful apis from models. arXiv\npreprint arXiv:1504.03498 .\n[9] Sven Efftinge and Markus V√∂lter. 2006. OAW xText: a framework for textual\nDSLs. In Workshop on Modeling Symposium at Eclipse Summit number 118.\nVol. 32.\n[10] GitHub. 2024. GitHub Copilot ¬∑ Your AI pair programmer. https://github.com/f\neatures/copilot. Accessed: 2024-07-02. (2024).\n[11] Nathan Hagel, Nicolas Hili, and Didier Schwab. Supplementary Material for\n\"Turning Low-Code Development Platforms into True No-Code with LLMs\".\nZenodo, (July 2024). doi: 10.5281/zenodo.12733193.\n[12] Nicolas Hili. 2016. A Metamodeling Framework for Promoting Flexibility and\nCreativity Over Strict Model Conformance. InFlexible Model Driven Engineering\nWorkshop (Flexible Model Driven Engineering). Vol. 1694. Davide Di Ruscio\nand Juan de Lara and Alfonso Pierantonio. CEUR-WS, Saint-Malo, France, (Oct.\n2016), 2‚Äì11. https://hal.archives-ouvertes.fr/hal-01464800.\n[13] Nicolas Hili and Raquel Ara√∫jo de Oliveira. 2022. A light-weight low-code\nplatform for back-end automation. In Proceedings of the 25th International\nConference on Model Driven Engineering Languages and Systems: Companion\nProceedings, 837‚Äì846.\n[14] Panos Constantopoulos, John Mylopoulos, and Yannis Vassiliou, (Eds.) 1996.Ad-\nvanced Information Systems Engineering: 8th International Conference, CAiSE‚Äô96\nHeraklion, Crete, Greece, May 20‚Äì24, 1996 Proceedings . Springer Berlin Heidel-\nberg, Berlin, Heidelberg. Chap. MetaEdit+ A fully configurable multi-user and\nmulti-tool CASE and CAME environment, 1‚Äì21. isbn: 978-3-540-68451-0. doi:\n10.1007/3-540-61292-0_1.\n[15] Stuart Kent. 2002. Model driven engineering. In Integrated Formal Methods .\nMichael Butler, Luigia Petre, and Kaisa Sere, (Eds.) Springer Berlin Heidelberg,\nBerlin, Heidelberg, 286‚Äì298. isbn: 978-3-540-47884-3.\n[16] Faezeh Khorram, Jean-Marie Mottu, and Gerson Suny√©. 2020. Challenges &\nopportunities in low-code testing. In Proceedings of the 23rd ACM/IEEE In-\nternational Conference on Model Driven Engineering Languages and Systems:\nCompanion Proceedings , 1‚Äì10.\n[17] Meta. 2024. Llama 3. https://llama.meta.com/llama3. Accessed: 2024-07-05.\n(2024).\n[18] Mistral. 2024. Mistral AI. https://docs.mistral.ai/. Accessed: 2024-07-05. (2024).\n[19] Gunter Mussbacher et al. 2014. The relevance of model-driven engineering\nthirty years from now. In Model-Driven Engineering Languages and Systems .\nJuergen Dingel, Wolfram Schulte, Isidro Ramos, Silvia Abrah√£o, and Emilio\nInsfran, (Eds.) Springer International Publishing, Cham, 183‚Äì200. isbn: 978-3-\n319-11653-2.\n[20] Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar,\nMuhammad Usman, Nick Barnes, and Ajmal Mian. 2023. A comprehensive\noverview of large language models. arXiv preprint arXiv:2307.06435 .\n[21] Object Management Group. 2011. Business Process Model and Notation -\n(BPMN 2.0). (2011). http://www.omg.org/spec/BPMN/2.0.\n[22] Marten Oltrogge, Erik Derr, Christian Stransky, Yasemin Acar, Sascha Fahl,\nChristian Rossow, Giancarlo Pellegrino, Sven Bugiel, and Michael Backes. 2018.\nThe rise of the citizen developer: assessing the security impact of online app\ngenerators. In 2018 IEEE Symposium on Security and Privacy (SP) . IEEE, 634‚Äì647.\n[23] OpenAI. 2024. ChatGPT. https://platform.openai.com/docs/overview. Accessed:\n2024-07-02. (2024).\n[24] OpenAI. 2024. ChatGPT. https://chatgpt.com/. Accessed: 2024-06-12. (2024).\n[25] Shuyin Ouyang, Jie M Zhang, Mark Harman, and Meng Wang. 2023. LLM is\nLike a Box of Chocolates: the Non-determinism of ChatGPT in Code Generation.\narXiv preprint arXiv:2308.02828 .\n[26] Richard F. Paige, Dimitrios S. Kolovos, and Fiona A.C. Polack. 2014. A tutorial\non metamodelling for grammar researchers. Science of Computer Programming ,\n96, 396‚Äì416. doi: https://doi.org/10.1016/j.scico.2014.05.007.\n[27] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018.\nImproving language understanding by generative pre-training.\n[28] Clay Richardson and John R Rymer. 2016. Vendor landscape: the fractured,\nfertile terrain of low-code application platforms. FORRESTER, Janeiro .\n[29] Clay Richardson, John R Rymer, Christopher Mines, Alex Cullen, and Do-\nminique Whittaker. 2014. New development platforms emerge for customer-\nfacing applications. Forrester: Cambridge, MA, USA , 15.\n[30] Bran Selic. 2003. The pragmatics of model-driven development. IEEE software ,\n20, 5, 19‚Äì25.\n[31] Tao Shi, Hui Ma, Gang Chen, and Sven Hartmann. 2021. Cost-effective web\napplication replication and deployment in multi-cloud environment.IEEE Trans-\nactions on Parallel and Distributed Systems , 33, 8, 1982‚Äì1995.\n[32] Jiho Shin, Clark Tang, Tahmineh Mohati, Maleknaz Nayebi, Song Wang, and\nHadi Hemmati. 2023. Prompt engineering or fine tuning: an empirical assess-\nment of large language models in automated software engineering tasks. arXiv\npreprint arXiv:2310.10508 .\n[33] Jean-S√©bastien Sottet and Nicolas Biri. 2016. JSMF: a Javascript Flexible Mod-\nelling Framework. In Proceedings of the 2nd Workshop on Flexible Model Driven\nEngineering. Vol. 1694. CEUR Workshops Modeling, 42‚Äì51.\n[34] Dave Steinberg, Frank Budinsky, Marcelo Paternostro, and Ed Merks. 2009.\nEMF Eclipse Modeling Framework . The Eclipse Series . Addison Wesley.\n[35] Bailin Wang, Zi Wang, Xuezhi Wang, Yuan Cao, Rif A Saurous, and Yoon Kim.\n2024. Grammar prompting for domain-specific language generation with large\nlanguage models. Advances in Neural Information Processing Systems , 36.\n[36] Jules White, Quchen Fu, Sam Hays, Michael Sandborn, Carlos Olea, Henry\nGilbert, Ashraf Elnashar, Jesse Spencer-Smith, and Douglas C Schmidt. 2023.\nA prompt pattern catalog to enhance prompt engineering with chatGPT. arXiv\npreprint arXiv:2302.11382 .\n[37] Michael Wollowski. 2023. Using ChatGPT to produce code for a typical college-\nlevel assignment. AI Magazine , 44, 1, 129‚Äì130.",
  "topic": "Code (set theory)",
  "concepts": [
    {
      "name": "Code (set theory)",
      "score": 0.6636680960655212
    },
    {
      "name": "Computer science",
      "score": 0.5913999676704407
    },
    {
      "name": "Programming language",
      "score": 0.4328973889350891
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ]
}