{
  "title": "Language Modeling with Latent Situations",
  "url": "https://openalex.org/W4385572270",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3022823767",
      "name": "Belinda Z. Li",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2809388066",
      "name": "Maxwell Nye",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2558501541",
      "name": "Jacob Andreas",
      "affiliations": [
        "Massachusetts Institute of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2891012317",
    "https://openalex.org/W3197499505",
    "https://openalex.org/W2977843956",
    "https://openalex.org/W2888882903",
    "https://openalex.org/W2918996109",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W2810346659",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4225619898",
    "https://openalex.org/W4305028650",
    "https://openalex.org/W2001722583",
    "https://openalex.org/W4309204224",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2638659725",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W3169976744",
    "https://openalex.org/W3202187802",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W4310625358",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2964285770",
    "https://openalex.org/W3201531807",
    "https://openalex.org/W2951976932",
    "https://openalex.org/W2963983586",
    "https://openalex.org/W3173343821",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3176195078",
    "https://openalex.org/W3173798466",
    "https://openalex.org/W2951936329"
  ],
  "abstract": "Language models (LMs) often generate incoherent outputs: they refer to events and entity states that are incompatible with the state of the world described in inputs. We introduce SITUATIONSUPERVISION, a family of approaches for improving coherence in LMs by training them to construct and condition on explicit representations of entities and their states. SITUATIONSUPERVISION has two components: an *auxiliary situation modeling* task that trains models to predict entity state representations in context, and a *latent state inference* procedure that imputes these states from partially annotated training data. SITUATIONSUPERVISION can be applied via fine-tuning (by supervising LMs to encode state variables in their hidden representations) and prompting (by inducing LMs to interleave textual descriptions of entity states with output text). In both cases, it requires only a small number of state annotations to produce substantial coherence improvements (up to an 16% reduction in errors), showing that standard LMs can be efficiently adapted to explicitly model language and aspects of its meaning.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 12556–12571\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nLanguage Modeling with Latent Situations\nBelinda Z. Li Maxwell Nye ∗ Jacob Andreas\nMassachusetts Institute of Technology\n{bzl,mnye,jda}@mit.edu\nAbstract\nLanguage models (LMs) often generate inco-\nherent outputs: they refer to events and entity\nstates that are incompatible with the state of the\nworld described in inputs. We introduce SITU -\nATION SUPERVISION , a family of approaches\nfor improving coherence in LMs by training\nthem to construct and condition on explicit rep-\nresentations of entities and their states. SITU -\nATION SUPERVISION has two components: an\nauxiliary situation modeling task that trains\nmodels to predict entity state representations in\ncontext, and a latent state inference procedure\nthat imputes these states from partially anno-\ntated training data. SITUATION SUPERVISION\ncan be applied via fine-tuning (by supervising\nLMs to encode state variables in their hidden\nrepresentations) and prompting (by inducing\nLMs to interleave textual descriptions of entity\nstates with output text). In both cases, it re-\nquires only a small number of state annotations\nto produce substantial coherence improvements\n(up to an 16% reduction in errors), showing\nthat standard LMs can be efficiently adapted\nto explicitly model language and aspects of its\nmeaning.1\n1 Introduction\nRecent years have seen dramatic improvements in\nthe quality of text generated by neural language\nmodels (LMs; Brown et al., 2020; Raffel et al.,\n2020). Nevertheless, even the best LMs still suffer\nfrom failures of semantic coherence . Samples\nfrom LMs refer to entities that have not yet been\nmentioned, assert contradictory facts, or describe\nimpossible sequences of events (Marcus and Davis,\n2020). This paper introduces SITUATION SUPERVI -\nSION , a family of methods for efficiently mitigat-\ning incoherent language generation. SITUATION -\nSUPERVISION adapts pre-trained LMs to explicitly\n∗Work complete while MN was at MIT.\n1Code is available at https://github.com/belindal/\nsitsup.\nSam unzipped \nthe suitcase.\nSam put his \nclothes inside.\nThe suitcase \nis open.\nAnn spilled \ncoffee on her \nkeyboard.\nShe threw it \naway.\nContext T Situation S Text T/uni2032 \nThe \nkeyboard \nis broken.\nContext\nSituation\nText\nenc\ndec\ndec\nCtx1 Txt1 Ctx2 Txt2LM\n(a) Auxiliary situation modeling tasks\nstate prediction loss\nstate prediction prompt\nSit1 Sit2\n(b) Latent state  \n  inference\nFigure 1: Language modeling with SITUATION SUPER -\nVISION , which comprises two components: (a) An aux-\niliary situation modeling task: given contexts annotated\nwith explicit textual representations of the situations\nthey describe, we use them to adapt LMs through ei-\nther an auxiliary fine-tuning loss or a scratchpad-style\nprompt. (b) A latent state inference procedure, whereby\nmissing situation annotations are semi-supervisedly in-\nferred, enabling auxiliary situation modeling starting\nfrom a small number of seed situation annotations.\nmodel the situations they describeby tracking the\nproperties and relations of entities in generated text.\nThe core of this approach is an auxiliary situation\nmodeling task that trains LMs to predict textual\nrepresentations of entity state jointly with target\ntext. Unlike prior work in state tracking focused\npredominantly on reasoning (where the end task\nis to answer questions about the state, or to solve\nmath or coding problems), we focus on using state\ntracking to improve language generation.\nFor most generation tasks, state information is\nnot readily available: it must be manually annotated\n12556\nand is costly to collect. Thus, to make auxiliary\nsituation modeling for generation practical, we ad-\nditionally introduce a semi-supervised procedure\nfor inferring entity states in unannotated text, mak-\ning it possible to apply SITUATION SUPERVISION\nvery small number of initial annotations.\nModern LMs can be specialized to new tasks in a\nvariety of ways, including fine-tuning their parame-\nters and modifying their prompts. We develop ver-\nsions of SITUATION SUPERVISION suitable for both\nadaptation methods. For fine-tuned models, we\nintroduce an auxiliary state prediction loss that en-\ncourages models’ hidden representations to encode\nstate variables. For prompted models, we introduce\na scratchpad approach that instructs models to gen-\nerate explicit textual descriptions of world states\nprior to generating output text. Both approaches\nultimately yield ordinary LMs, compatible with\nstandard pre-training and decoding procedures.\nWe evaluate SITUATION SUPERVISION on two\nchallenging text generation tasks: the TextWorld\n(TW) task of generating acceptable next actions\nin a text-adventure game (Côté et al., 2018), and\nthe TRIP task of evaluating commonsense physi-\ncal plausibility of short (5-sentence) stories (Storks\net al., 2021). In experiments on fine-tuned BART\nLMs (Lewis et al., 2020), applying SITUATION -\nSUPERVISION with 500 seed annotations reduces\ncoherence errors by 5% on TW and 15% on TRIP.\nIn experiments on prompted GPT-3 models (Brown\net al., 2020), 12 seed annotations reduce coher-\nence errors by 9% on TW and 20 seed annota-\ntions reduce errors by 16% on TRIP. In both cases,\nit is far more sample-efficient to annotate entity\nstates in existing training samples than to augment\ntraining data with additional text-only samples: in\nfine-tuned models, SITUATION SUPERVISION with\n500 state annotations performs comparably to train-\ning on 9000 more text-only sentences, while in\nprompted models, devoting a fixed token budget to\nstate annotations rather than additional text samples\nyields a coherence improvement of up to 10%.\nAdditional experiments characterize the ingre-\ndients of a good situation representation, showing\nthat training LMs to predict causally relevant state\nvariables is essential for good performance. Be-\ncause the latent state inference objective favors\nrepresentations that improve LM predictions, SIT-\nUATION SUPERVISION discovers these variables\nautomatically, sometimes improving on human-\ndesigned state representations. In summary:\n1. We show that training LMs to build explicit\nrepresentations of entity state (via auxiliary\nlosses or scratchpad-based prompting) im-\nproves coherence in text generation tasks.\n2. We describe new algorithms for semi-\nsupervised learning of state representations,\nenabling auxiliary supervision and scratchpad\ntechniques to be applied with extremely small\nnumbers of annotations.\nOur results show that, even when LMs struggle to\ngenerate coherent continuations of input text, only\na small amount of supervision is needed to train\nthem to explicitly represent the situations that their\ninputs describe. Once predicted, these represen-\ntations in turn confer large improvements in LM\ncoherence itself.\n2 Background and Preliminaries\nA language model (LM) encodes a distribution\np(T′|T) over texts T′given contexts T (Fig. 1).\nToday, most LMs are implemented as deep neural\nnetworks trained on massive text corpora (Brown\net al., 2020). Sampling from them produces natu-\nralistic text that often resembles human-generated\nlanguage. However, LM generation is prone to sev-\neral failure modes, including generation of text that\nis incoherent, untruthful, or unreliable (Zhou et al.,\n2021; Maynez et al., 2020; Martindale et al., 2019).\nPast work has shown that some of these behaviors\nstem from models’ failure to build good represen-\ntations, both of entities’ default properties (Onoe\net al., 2021) and state changes in context (Zellers\net al., 2021). Humans’ ability to avoid these fail-\nure modes, and to generate truthful and coherent\ntext, is generally understood to rest upon explicit\nmental representations of the situations that lan-\nguage communicates. The nature and structure of\nthese representations remains an ongoing topic of\nresearch in linguistics and cognitive science, but\nexisting theories broadly agree that language users\nmaintain explicit beliefs about the properties of and\nrelations among entities mentioned in a discourse,\nupdating these beliefs in response to new observa-\ntions or new information conveyed in language (e.g.\nKratzer, 2007; Zwaan and Pecher, 2012).\nThese representational theories suggest that lan-\nguage models p(T′ |T) may also benefit from\nexplicit modeling of situation state. Given an input\ntext T, we wish to first represent the situation S\ndescribed by T before predicting a next sentence.\n12557\nInspired by models of situation semantics in the\nlinguistics literature (Barwise and Perry, 1981, in-\nter alia) we propose to model situations as sets of\npropositions si that are known or inferable about\nentities in a discourse. 2 Examples, with proposi-\ntions expressed as sentences in natural language,\nare shown in Fig. 1(b) and Fig. 2.\nHaving inferred S from T, we may then con-\ndition on it when sampling T′from p(T′ |S,T).\nPast work has proposed a number of language gen-\neration models that explicitly model the state of the\nworld, primarily by developing specialized predic-\ntion architectures that maintain internal state repre-\nsentations (Henaff et al., 2016; Gupta and Durrett,\n2019) or interact with outside simulation engines\n(Liu et al., 2022). While effective, these approaches\ncome at a cost—requiring complex training data\n(Mishra et al., 2018), limiting models to narrow,\npre-defined domains, and generally precluding the\nlarge-scale (text-only) pretraining responsible for\nmany of the greatest successes of current LMs.\nThe main question this paper seeks to answer is\nwhether the benefits of explicit world modeling\nmay be obtained entirely within the language mod-\neling paradigm itself, without specialized model\narchitectures or large amounts of specialized super-\nvision.\nWe do so by adapting pre-trained LMs to bet-\nter represent situations S. There are two standard\nframeworks for LM adaptation. In smaller mod-\nels, which are generally adapted by fine-tuning\nof model parameters, we develop auxiliary loss\nfunctions that encourage models’ hidden states to\ncontain the information required to generate tex-\ntual descriptions of state.3 In larger models, which\ncan also be prompted by prepending a task de-\nscription or set of examples to the input context,\nwe develop prompts that induce models to gen-\nerate textual state descriptions in LM output it-\nself. Our research builds on a large body of work\nthat uses auxiliary prediction tasks to shape model\nrepresentations, notably work using “scaffold” de-\ncoders to shape model representations of syntax\n(Swayamdipta et al., 2018; Wilcox et al., 2019), and\nand “scratchpad” or “chain-of-thought” approaches\nto perform intermediate computations in models’\n2This approach to modeling contrasts with approaches that\nimplicitly or explicitly represent the complete set of possible\nworlds consistent with a text.\n3Concurrent work by Richardson et al. (2022) also intro-\nduces a fine-tuning objective aimed at improving state rep-\nresentations, but focuses on state-tracking tasks rather than\ngeneration, and only examines a fully supervised setting.\noutput spaces (Camburu et al., 2018; Nye et al.,\n2021; Wei et al., 2022). In §3, we show how to\nadapt both techniques for a new class of text gener-\nation problems.\nAdapting LMs with auxiliary prediction tasks\nrequires a source of data for auxiliary supervision.\nThis kind of supervision is uniquely difficult to\nobtain for generation tasks. But the probabilistic\nframing described above makes it natural to for-\nmulate language modeling with explicit situations\nas a latent variable problem. At training time, we\nmay use context T and targets T′to guide infer-\nence of the unknown S from which T′ was pre-\ndicted. Once inferred, these states supervise the\nrepresentation-building model that predicts Sfrom\nT alone. As above, a great deal of past work has\nfocused on treating string-valued prompts or plans\nas latent variables (Sharma et al., 2021; Zelikman\net al., 2022; Sun et al., 2022). In §4, we generalize\nthese methods to support multi-step text genera-\ntion, and show that inferred states can be used to\nsupervise small models as well as prompt large\nones.\n3 Auxiliary Situation Modeling\nWe begin by assuming access to a pre-trained LM\nand two sources of supervision: a dataset XU of\ntext examples of the form (T,T ′), and a smaller\ndataset XA of examples (T,S,T ′) annotated with\ntextual situation descriptions S. Our full training\ndata Xis thus XU ∪XA. As depicted in Fig. 2, we\ntake these situation descriptions to consist of declar-\native sentences about the properties and relations\nof entities that are relevant to the text being gen-\nerated. In this section, we describe two auxiliary\nprediction schemes that use these annotations to\nimprove the LM’s ability to model the conditional\ntext distribution p(T′|T).\n3.1 Situation Modeling for Fine-tuning\nOur first approach uses a auxiliary decoding loss\nthat encourages context representations to directly\nencode entity state information. We focus on\nencoder–decoder models consisting of an encoder\nEand a decoderD, with D(E(T)) producing as out-\nput a probability distribution over next sentences\nT′. In standard training, parameters of Eand Dare\nchosen to maximize:\nL(T′|T) = log p(T′|T) = log D(T′|E(T)) (1)\nTo improve state representations, we add an aux-\niliary loss. This takes the form of an auxiliary\n12558\nT T/uni2032 \np(T/uni2032 /uni2223T)\nenc dec\nFine-tuning\nT T/uni2032 \np(T/uni2032 /uni2223T)\nenc dec\nS\nT/uni2032 \np(T/uni2032 /uni2223S)enc/dec\nT enc dec\ndec\np(T/uni2032 /uni2223T)\np(S /uni2223T)\nText-Only Loss Auxiliary Loss Latent State Inference\nPrompting\nText-Only Prompt\nT1 T/uni2032 \np(T/uni2032 /uni2223T)\nLMT/uni2032 \n1 T/uni22EFD\nScratchpad Prompt Latent State Inference\nT1 T/uni2032 \np(S /uni2223T)\nT/uni2032 \n1 T/uni22EFS1 SLM\np(T/uni2032 /uni2223S, T)\np(S1 /uni2223T1) p(T/uni2032 1 /uni2223S1, T1)\nD\nPrediction\nContext: You see a locked door \nand a chest. The chest holds a key. | \n> take key | You take the key. |\nText: > open \ndoor\nSituation: The door is \nlocked. The chest is \nempty. You hold the key.\nT T/uni2032 \np(T/uni2032 /uni2223T)\nenc dec\nS\np(S /uni2223T)\ndec\nMark a sentence \"Not OK\" if it contradicts the prior context. Mark it \"OK\" otherwise. \nSam unzipped the suitcase. | OK. | Known facts: The suitcase is open. | He put some clothes in it. | OK. \nAnn broke her keyboard. | \nT1 T/uni2032 \np(S /uni2223T)\nT/uni2032 \n1 T/uni22EFS1 SLM\np(T/uni2032 /uni2223S, T)\nD\nKnown facts: The keyboard is broken. | \nShe typed her paper with the keyboard. | \nNot OK.\nFigure 2: Fine-tuning (top) and prompting (bottom) with SITUATION SUPERVISION . For each, we show: (Left)\nText-only supervision, in which the LM is trained/prompted with text only samples and expected to produce next\nsentences T′ from contexts T. (Middle) Auxiliary situation modeling, in which the LM is given state descriptions\nS during training or in the prompt and expected to learn to encode it in its parameters or infer it in-context.\n(Right) Latent state inference, in which the LM must infer missing state descriptions in the training data or prompt\ndemonstrations. Finally, in fine-tuning, we discard the auxiliary decoder during inference, use the shared encoder\nand text-only decoder to infer the next sentence from the context.\ndecoder DS|T (distinct from the original decoder\nD) which is trained to predict state representations\nSfrom the encoded context E(T). We define:\nL(S|T) = log p(S|T) = log DS|T(S|E(T)) (2)\nand train parameters of the encoder (θE) and both\ndecoders (θD,θDT,S ) to maximize:\narg max\nθE,θD,θDT,S\n∑\nT,T′∈X\nL(T′|T)\n+\n∑\nT,S∈XA\nL(S|T) (3)\nIntuitively, to minimize this objective, the output of\nE(T) must encode information about the latent sit-\nuation S. Once encoded, this information is acces-\nsible to the original LM text decoderD. Eq. (3) is a\nstraightforward application of standard multi-task\ntraining approaches for deep networks; however,\nto the best of our knowledge it has not previously\nbeen used for state prediction tasks or shown to\nimprove LMs’ factual coherence.\n3.2 Situation Prediction for Prompting\nThe approach described above is general. But in\nLMs with very large numbers of parameters, it\nmight be costly to apply (or we may risk over-fitting\nif the fine-tuning dataset is too small). Thus, the\nsecond approach we describe is based on prompt-\ning models to construct better state representations.\nWe build on the observation in recent work that\nprompts can induce models to build better task\nrepresentations by writing these representations to\noutput: generating, then conditioning on, textual\nencodings of useful intermediate variables.\nTo induce language models to output textual situ-\nation descriptions, we construct prompts with three\ncomponents: a task description D, a set of task\ndemonstrations (“training set”) X, and an input\ncontext Tpred. The training set can include both\nunannotated and annotated examples: unannotated\nexamples are sequences Ti,T′\ni, while annotated\nexamples are sequences Ti,Si,T′\ni. Formally, we\nconstruct a prompt string:\nP= [D·PA ·PU ·Tpred] , where:\nPA = [T′\n0 ·S1 ·T′\n1 ···Sn ·T′\nn]x ∀x∈XA\nPU = [T′\n0 ·T′\n1 ···T′\nn]x ∀x∈X (4)\nwith ·denoting string concatenation. To enable\nthe model to predict annotations and text directly,\neach Sis prefixed with an appropriate control token\n12559\nthat informs the model that a situation description\nstring will come next. When predicting (or scor-\ning) a sentence T′\npred in context, we first prompt the\nmodel to generate a situation representation Spred,\nthen score T′\npred conditional on Tpred, Spred, and the\nentire preceding context. The bottom portion of\nFig. 2 shows a concrete example from the TRIP\ndomain. As above, this approach to prompting is\nclosely related to existing “scratchpad” and “chain-\nof-thought” methods used for question answering\nand formal reasoning tasks; our auxiliary situation\nmodeling approach applies this form of structured\nprompting to multi-sentence, open-ended text gen-\neration problems.\n4 Latent State Inference\nThe methods described in §3 applied SITUA -\nTION SUPERVISION only to examples for which\na ground-truth state annotation was provided. For\nthese methods to be effective, enough state annota-\ntions must be available to provide a useful training\nsignal in the auxiliary loss or to specify the auxil-\niary prediction task for the prompted model. But\nsuch state annotations are in general both hard to\ncollect and hard to design.\nIn this section we describe how to obtain them\nautomatically, without the need for large amounts\nof annotation. Below, we re-formulate the two\napproaches in §3 as latent variable models that\ncan infer and condition on state representations\neven for unannotated training documents. Intu-\nitively, this inference problem is easier at training\ntime than prediction time: knowing what text fol-\nlowed a context constrains the possible situations\nthe context could describe. Most work on semi-\nsupervised inference of auxiliary prediction targets\nhas focused on automatic optimization of prompts\nand reasoning chains (Zelikman et al., 2022; Sun\net al., 2022). To the best of our knowledge, in-\nferred latent variables have not been used to train\nauxiliary decoders or to design intermediate state\nrepresentation for multi-step text generation. The\ntechniques described below are quite general, and\nmight be productively employed beyond the gener-\nation applications we describe here.\n4.1 Latent State Inference for Fine-Tuning\nIntuitively, a good situation representation is one\nthat is both predictable from context, and useful\nfor predicting subsequent text. To guide inference\nof entity states for auxiliary prediction, we intro-\nduce another encoder-decoder into the model of\n§3.1: one which attempts to predict T′ from S.\nThis model now has two pathways for predicting\nT′: one that uses encoder representations to predict\nit directly from T, and another which generates\ntextual situation descriptions Sfrom decoder repre-\nsentations, then uses these to predict T′. We train\nthis model’s parameters and infer situation descrip-\ntion that maximize probability of next sentences\nunder both pathways, using information from both\nT and T′to infer situations S, then using these to\nsupervise the encoder.\nFormally, we optimize the complete likelihood:\narg max\nΘ,ˆS\n∑\nT,T′\n∈X\nL(T′|T)\n+\n∑\nT,T′,S\n∈XA\n(\nL(S|T) + L(T′|S,T)\n)\n+\n∑\nT,T′,ˆS\n∈XU\n(\nL( ˆS|T) + L(T′|ˆS,T )\n)\n. (5)\nEq. (5) extends auxiliary fine-tuning by concur-\nrently training an encoder-decoder MT′|S,T to\nmodel p(T′|S,T). We initialize θE,θD,θDS|T us-\ning Eq. (3), andθT′|S by fine-tuning to convergence\non XA. We then perform coordinate ascent (“hard\nEM”) by alternating between:\n1. E-step: Set ˆS ≈arg maxS p(S |T)p(T′|S)\nfor XU by sampling from p(S |T), then\nreranking according to p(S |T)p(T′|S).\n2. M-step: Using the new ˆS, train Θ to maximize\nEq. (5). Rather than training to convergence,\nwe perform SGD on Eq. (5) for five epochs.\nAs in auxiliary fine-tuning, E is shared the\np(T′|T) and p(S |T). Information about inferred\ndescriptions shapes text generation via the auxiliary\ndecoding objective.\n4.2 Latent State Inference for Prompting\nWork on few-shot prompting consistently finds\nbenefits from adding extra examples to prompts\n(Brown et al., 2020). As in §4.1, we produce extra\nexamples for a seed prompt by finding situation\ndescriptions Sthat are predictable from T and im-\nprove prediction of T′on unannotated examples.\nWe may do so using a very similar procedure to\nthe one in §4.1: now we choose prompts (but not\n12560\nmodel parameters) to maximize:\narg max\nˆS\n∑\nT,T′∈XU\np( ˆS |T) p(T′|T,S) (6)\nthen add these newly annotated examples to the\nprompt (which we may do during both training and\nevaluation). Algorithmically, we iterate incremen-\ntally over unannotated examples XA:\n1. E-step: set ˆS ≈arg maxSp(S |T) p(T′|S)\nfor each context-sentence pair (T,T ′) in XU\nby prompting the LM with [D·PA ·T], then\nreranking the candidate states according to\np(S |[D·PA ·T]) p(T′|[D·PA ·T ·S]) . (7)\n2. M-step: add [T ·ˆS·T′] to PA in Eq. (4).\nOnce all examples in XU have been annotated and\nadded to PA, we prompt with auxiliary supervision\nfor each context in the evaluation set using P=\n[D·PA·Tpred].\n5 Experimental Setup\nDatasets We evaluate SITUATION SUPERVISION\non English language modeling datasets. TW is\na set of 1368 transcripts (992 train / 376 evalua-\ntion) derived from TextWorld (Côté et al., 2018)\nWe generate a set of textual game transcripts where\nplayers navigate through a house, unlocking doors\nand containers to hunt for a target object. The\nLM is trained on these transcripts to generate next\nactions. As state supervision, we use the set of\nstate variables (given as entity-centric facts) that\nare known and relevant in the current context (see\n§7.1 for more details). TRIP (Storks et al., 2021)\nis a set of 1643 plausible and implausible five-\nsentence stories (1169 train / 474 evaluation) which\nrequire physical commonsense reasoning to disam-\nbiguate. Models are trained to generate judgments\nof whether or not a given sentence is acceptable in a\ncontext. The state is given by a set of attributes for\neach entity, which is updated after each sentence.4\nEach passage x ∈X comprises a sequence of\nchunks T′\n0,T′\n1,··· ,T′\nn. In TW, each chunk con-\nsists of a textual action description followed by a\ngame response. In TRIP, each chunk is a single\nsentence from the story followed by a plausibility\njudgment. We test coherence of generating each T′\nfrom its context T. For the annotated passages in\n4See Appendix A for state representation details.\nXA, each context Ti is annotated with correspond-\ning state information Si. Thus, passages in XU can\nbe broken down into (T,T ′) pairs, while passages\nin XA can be broken down into (T,S,T ′) triples.\nModels For fine-tuning experiments, we use\nBART-base (Lewis et al., 2020) as the language\nmodel and fine-tune it using the AdamW opti-\nmizer with learning rate 1e-5, stopping once val-\nidation accuracy has stopped improving for 10\nepochs. For prompting experiments, we use the\nGPT3 da-vinci-002 model (Brown et al., 2020).5\nMetrics To evaluate models on TW, we sample\nnext actions from the LM and compute the fraction\nof these that are semantically coherent using the\nTW simulator.67 For TRIP, we evaluate every story\npair by training models to predict the string OK or\nNot OK after each sentence depending on whether\nit is semantically acceptable within a given context.\nThe TRIP dataset contains human semantic accept-\nability judgments for each sentence of each stories;\nwe evaluate the accuracy with which models pre-\ndict these acceptability judgments (labeling a story\nas unacceptable if any sentence is predicted to be\nunacceptable).\nFor TW, we report sentence-wise metrics: we\nmeasure the fraction of next sentences which are\ngenerated to be coherent within the context. In\nTRIP, we report passage-wise metrics: we mea-\nsure the percent of complete passages for which\nevery sentence of the passageis labelled accurately.\nAs baselines in each domain, we compare to\nordinary fine-tuning and prompting. As far as we\nare aware, no prior work in these domains focus on\nevaluating generation coherence or accuracy.8\n5Further details can be found in Appendix C.\n6The simulator also returns game responses after each\naction (e.g. You entered the kitchen in response to > go\nwest). Game response coherence results can be found in Ap-\npendix B.1. Because coherence evaluation is less well-defined\nfor game responses, we do not report results in the main paper.\n7In Appendix B.2, we also evaluate generation diversity\namongst these actions using recall against the full set of\nground-truth possible next actions.\n8For TRIP, most prior work uses the evaluation procedure\nfrom Storks et al. (2021), which are focused not on evaluating\nacceptabilities of incrementally-generated stories, but instead\non post-hoc commonsense reasoning. For TW, prior work\nuses a much richer supervisory signal (environment reward) to\nselect optimal actions towards a goal, rather than modeling the\nfull set of plausible next actions (Ammanabrolu et al., 2021).\n12561\n|X| |XA| Method\nCoherence\nTW\n1k 0 Fine-tuning 79.4%±2.4%\n1k 500 S ITSUP 80.5%±1.8%\n1k 500 S ITSUP+ Latent 83.4%±1.4%\n1k 1k S ITSUP 81.5%±1.5%\n10k 0 Fine-tuning 83.6%±2.5%\nAccuracy\n1k 0 Fine-tuning 36.5%±3.5%\nTRIP 1k 500 S ITSUP 43.6%±1.0%\n1k 500 S ITSUP+ Latent 43.6%±1.0%*\n1k 1k S ITSUP 43.0%±1.7%\nTable 1: BART fine-tuning results on TW and TRIP,\nwhere |X|is the number of training examples, and |XA|\nis the total amount of state supervision. We report re-\nsults for standard LM training, SITUATION SUPERVI -\nSION with only auxiliary situation modeling, and SITU -\nATION SUPERVISION with both auxiliary modeling and\nthe latent state inference. The table shows and standard\nerrors over 8 random seeds. Training with any state\nsupervision helps over training with no state supervi-\nsion. With a comparable amount of state supervision,\nlatent inference sometimes gives further improvements.\n*Latent inference unable to improve beyond base auxiliary\nsituation modeling checkpoint\n6 Experiments\n6.1 Fine-Tuning\nOur experiments use 1000 training examples, vary-\ning the fraction of these examples for which we\nprovide state annotations ( |XA|= {0,500,1000\n}). For each choice of |XA|, we repeat experiments\nacross 8 random seeds, training on a different set\nof 1000 examples for each seed. We compare mod-\nels trained using ordinary language modeling tech-\nniques, Eq. (3), and Eq. (5). We evaluate using\nmetrics described in §5.\nResults Evaluation results are shown in Table 1.\nIn TW, using auxiliary supervision and latent\nstate inference, SITUATION SUPERVISION with 500\nstate annotations improves generation coherence by\n∼4% over a text-only baseline, giving comparable\nimprovements to training on 9,000 more text-only\nexamples. Results in Appendix B.2 show that these\nimprovements come at no cost to generation diver-\nsity. In fact, the latent procedure with 500 seed\nstates is able to outperform full auxiliary supervi-\nsion — possibly because latent state inference is\nable automatically discover usable state represen-\ntations, which are more useful for prediction than\nhuman-authored ones. In TRIP,SITUATION SUPER -\n|X | |X A| Method\nCoherence\n25 0 Text prompting 67.4%\nTW 25 12 S ITSUP 68.5%\n25 12 S ITSUP + Latent 75.6%\n25 25 S ITSUP 73.9%\nAccuracy\n80 0 Text prompting 59.5%\nTRIP 80 20 S ITSUP 58.2%\n80 20 S ITSUP + Latent 67.1%\n80 80 S ITSUP 70.7%\nTable 2: GPT3 prompting results on TW and TRIP, us-\ning text-only querying, SITUATION SUPERVISION with\nonly the auxiliary situation modeling component, and\nSITUATION SUPERVISION with both the auxiliary situa-\ntion modeling and the latent situation prediction compo-\nnents. Prompting with any state supervision helps over\nprompting with no state supervision. With a comparable\namount of ground-truth state supervision, latent infer-\nence significantly improves over only auxiliary situation\nmodeling.\nVISION with 500 seed states improves accuracy by\n∼7% over a text-only baseline. Note in this case\nthat the latent inference procedure was unable to\nimprove beyond auxiliary training. However, even\nadding in the remaining 500 ground-truth state an-\nnotations does not improve the LM, indicating that\nperhaps the 500 seed states were sufficient for the\nLM to learn everything it can from state supervi-\nsion.\n6.2 Prompting\nIn TW, we used 25 sentences (3 stories) in P.\nIn TRIP, we used 80 sentences (16 stories) in P.\nWhen evaluating latent supervision, we held out\nstate annotations on 13 sentences (2 stories) in TW,\nand 60 sentences (12 stories) in TRIP. We run each\nprompting experiment once.\nResults Results are reported in Table 2. Us-\ning SITUATION SUPERVISION with auxiliary sit-\nuation modeling where all passages are fully an-\nnotated with state (rows 4,8) dramatically im-\nproves performance compared to a text-only base-\nline (rows 1,5) in both domains. In TW, we see\na ∼6.5% improvement in generation coherence,9\nwhile in TRIP, we see a ∼11% improvement in\naccuracy of coherence judgments.\nNext, we examine the setting where certain state\nannotations are missing from the prompt, compar-\n9Results in Appendix B.2 shows that SITUATION SUPER -\nVISION also improves generation diversity.\n12562\nThe chest is open. \nThe chest is empty. \nThe chest is in the attic. \nYou have the old key. \nYou are in the kitchen. \nThe kitchen is east of the attic. \nThe red door is locked. \nThe old key matches the red door. \nThe living room is south of the kitchen. \nThe sofa is in the living room.\nLatent state S\n> unlock the red door with the \nold key\nContext T Next sent. T’\n-= Attic =- \nYou see an open chest. \nThe only thing in the chest is an old key. \nThere is an open green door leading east.  \n> pick up key \nYou pick up the key. \n> go east \n-= Kitchen =- \nYou enter the kitchen. \nThere is a locked red door leading south. \n> go west\n> drop the old key\nKnown \nstate\nCausally \nrelevant state\nFigure 3: Different choices of situation representation. We find that ideal representations consist of the intersection\nbetween known state and causally relevant state(highlighted in gray). The known state consists of all facts deducible\nfrom the prior context T (e.g. in TW, only facts about rooms or objects that the player has seen). The causally\nrelevant state consists of all facts causally relevant to any plausible next sentence T′ (e.g. in TW, only facts about\nthe currently accessible items, here the old key but not the chest).\ning SITUATION SUPERVISION with latent situation\nprediction (rows 3, 7) againstSITUATION SUPERVI -\nSION with only auxiliary situation modeling (rows\n2, 6). We find that incorporating generated latent\nstates into the prompt helps performance on both\nTW and TRIP, by 7.1% and 8.9% respectively.\n7 Analysis\n7.1 Choice of state is important\nIn this section, we explore the consequences of in-\ncluding/excluding various components of the state.\nTW We begin by conducting experiments in TW.\nBecause it is procedurally generated, the TW en-\nvironment is able to provide detailed ground-truth\nstate annotations for every entity that might be men-\ntioned in text. All experiments described in §6 use\nsituation representations that include only a subset\nof entities and properties: namely (1) only those\nthat are already known (i.e. those have been as-\nserted in the context), and (2) only those that are\ncausally relevant (i.e. those that, if changed, would\ninduce a different distribution over next sentences).\nSee Fig. 3 for examples.\nWe train with auxiliary supervision using the\nthree different choices of state: the full state, the\nknown state (facts that satisfy condition (1)), and\nthe relevant known state (facts that satisfy both\nconditions (1) and (2)). Results are shown in Ta-\nble 3. We find that the training with the full state is\nnot significantly better than simply training on text\nonly, and perhaps slightly worse. Training on the\nsubset of known facts outperforms training with the\nfull state, and training on the intersection of known\nstate and causally relevant state is even better.\nState Type Coherence\nNone 79.4%±2.4%\nFull state 78.0%±1.7%\nFull Known state 79.7%±1.5%\nRelevant Known state 81.5%±1.5%\nTable 3: Using different subsets of the state as auxiliary\nsupervision for TW fine-tuning yields varying amounts\nof coherence improvements. We report averages and\nstandard errors over 4 random seeds. Design of situ-\nation representations is consequential: using only the\nknown and causally relevant portions of the state (rele-\nvant known state) substantially outperforms using the\nfull state.\nTRIP Using the principles deduced from the pre-\nvious experiments in TW (the optimal state should\nbe both known from prior context and causally rel-\nevant to the next sentence), we optimize the design\nof TRIP state annotations.10 We used these state\nannotations for all experiments described above.\nIn this section, we demonstrate that this outper-\nforms using the original annotations provided in\nthe dataset. Specifically, we sample 12 training\nexamples to include in the prompt,11 and compare\ntext-only prompting against SITUATION SUPERVI -\nSION with the original states (Orig) and SITUA -\nTION SUPERVISION with handcrafted states (Ours).\nResults are reported in Table 4. By using our hand-\ncrafted states, we were able to achieve a much\nhigher accuracy than using the original states.\n10See Appendix A for details.\n11In previous sections we used 16 samples. However, be-\ncause the original states were much longer than our states, we\nwere only able to fit 12 candidates in context using the original\nstate annotations.\n12563\n|X | |X A| State Type Accuracy\n12 0 - 59.3%\nTRIP 12 12 Orig 62.8%\n12 12 Ours 68.1%\nTable 4: Using different types of state annotations for\nTRIP when prompting GPT3 yields various amounts of\nperformance improvements. We compare SITUATION -\nSUPERVISION using TRIP’s original state annotations\n(Orig) against SITUATION SUPERVISION using our own\nhandcrafted state annotations (Ours). Note that using\nthe original state annotation is only able to improve\n3.6% over a text-only baseline, while using our state\nannotations improves 8.8%.\nTW TRIP\nSITUATION SUPERVISION 75.6% 67.1%\nwithout state reranking 72.4% 65.8%\nTable 5: Ablating state reranking with p(T′ |S) when\ninferring the optimal latent state for prompting. In both\nTW and TRIP, SITUATION SUPERVISION works better\nwhen we sample multiple states from p(S |T) and\nrerank according to p(T′ |S), than when we simply\ntake the greedy optimal state from p(S |T).\n7.2 Explicit state inference outperforms\ngreedy next state prediction\nA simplification of our latent state inference proce-\ndure for prompting simply asks GPT3 to greedily\ngenerate the most likely state according to prior\ncontext (i.e., arg maxSp(S |T)), without consid-\nering p(T′|S) (as in chain-of-thought approaches;\nWei et al., 2022). We compare our currently latent\nstate procedure against this greedy state generation\nbaseline in Table 5. We find that it indeed helps to\nconsider p(T′|S) when generating states, improv-\ning next sentence coherence by 3.2% in TW and\nnext sentence accuracy by 1.3% in TRIP.\n7.3 For a fixed context window budget,\nincluding more state annotations\noutperforms including more text samples\nBecause the limiting factor in many current few-\nshot prompting methods is context window size\nrather than annotation effort, we study whether it is\nmore token-efficient to include additional state an-\nnotations or additional text examples in the prompt.\nWe compute the number of tokens in prompts an-\nnotated with state (PA), then formulate a text-only\nprompt (PT) by stripping the state annotations from\nPA, then appending randomly-selected text-only\nsamples from the remaining training data until the\n# toks |X | |X A| Metric\nCoherence\nTW 3199 54 0 56.7%*\nTW 3199 25 25 65.0%*\nAccuracy\nTRIP 3053 229 0 60.5%\nTRIP 3054 80 80 70.7%\nTable 6: When prompting with limited context-window\nsize, supplementing existing prompt demonstrations\nwith states is more token-efficient than providing more\ntext-only training examples.\n*Coherences of greedy next generations are reported in this\nexperiment for TW.\nnumber of tokens in the new prompt is equal (or\nnearly equal) to the number of tokens in PA.\nWe prompt the LM using either text-only prompt-\ning conditioned on PT, or auxiliary prompting\nconditioned on PA. The results are shown in Ta-\nble 6. (Due to limitations in annotation budget, for\nTW in this experiment, we report coherence of the\ngreedily-generated next actions rather than sam-\npling 5 actions.) We see that under a fixed context\ntoken budget, in both domains, it is more helpful\nto supplement existing examples with their state\nannotations rather than insert additional text-only\nexamples into the context window.\n8 Conclusion\nEffective generation of coherent text requires rea-\nsoning about the world that text describes. In this\nwork, we use entity states as auxiliary supervision\nto improve LMs ability to perform this reasoning\nunder both fine-tuning and prompting. We find that\nwhen either annotation budget (for fine-tuning) or\ncontext window size (for prompting) are limited, it\nis more sample- and token-efficient to increase the\namount of state supervision rather than text-only\nsupervision. However, since state annotations are\nharder to collect, we introduce latent supervision al-\ngorithms for sample-efficiently improving LM gen-\neration coherence, and demonstrate improvements\nin two domains. Our results point to a potentially\nbroad role for semantic supervision in LM train-\ning and prompting—even small amounts can yield\nlarge coherence improvements. This work more\ngenerally suggests that semantic state reasoning is\nstill challenging for even modern large language\nmodels, and but can be improved without funda-\nmental changes to the architecture of existing LMs.\n12564\n9 Limitations\nThe main limitation of SITUATION SUPERVISION\nis that situation annotations can often be expensive\nto curate and difficult to design (though we outline\nsome general principles for their design in §7). Fur-\nthermore, we conducted experiments on only two\ndatasets in this paper. Future work could explore\na wider genre of texts, more domains, and more\nlanguages.\n10 Impact Statement\nThis work introduces ways of using state super-\nvision for improving the coherence of language\nmodel generations. This can be used to reduce\nthe incidence of false or misleading generations\nfrom language models. Furthermore, we found that\nwe can bootstrap starting from small amounts of\nseed state supervision to achieve large coherence\ngains, meaning the method can be used with rela-\ntive ease without the need for extensive annotation.\nHowever, the methods described in this paper can\nalso be used maliciously to improve the coherence\nof automatically-generated misinformation, hate\nspeech, or other harmful content.\nReferences\nPrithviraj Ammanabrolu, Jack Urbanek, Margaret Li,\nArthur Szlam, Tim Rocktäschel, and Jason Weston.\n2021. How to motivate your dragon: Teaching goal-\ndriven agents to speak and act in fantasy worlds. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 807–833, Online. Association for Computa-\ntional Linguistics.\nJon Barwise and John Perry. 1981. Situations and atti-\ntudes. The Journal of Philosophy, 78(11):668–691.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens\nWinter, Chris Hesse, Mark Chen, Eric Sigler, Ma-\nteusz Litwin, Scott Gray, Benjamin Chess, Jack\nClark, Christopher Berner, Sam McCandlish, Alec\nRadford, Ilya Sutskever, and Dario Amodei. 2020.\nLanguage models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems ,\nvolume 33, pages 1877–1901. Curran Associates,\nInc.\nOana-Maria Camburu, Tim Rocktäschel, Thomas\nLukasiewicz, and Phil Blunsom. 2018. e-snli: Natu-\nral language inference with natural language expla-\nnations. Advances in Neural Information Processing\nSystems, 31.\nMarc-Alexandre Côté, Ákos Kádár, Xingdi (Eric) Yuan,\nBen Kybartas, Tavian Barnes, Emery Fine, James\nMoore, Matthew Hausknecht, Layla El Asri, Mah-\nmoud Adada, Wendy Tay, and Adam Trischler.\n2018. Textworld: A learning environment for text-\nbased games. In Computer Games Workshop at\nICML/IJCAI 2018, pages 1–29.\nAditya Gupta and Greg Durrett. 2019. Tracking discrete\nand continuous entity state for process understanding.\narXiv preprint arXiv:1904.03518.\nMikael Henaff, Jason Weston, Arthur Szlam, Antoine\nBordes, and Yann LeCun. 2016. Tracking the world\nstate with recurrent entity networks. arXiv preprint\narXiv:1612.03969.\nAngelika Kratzer. 2007. Situations in natural language\nsemantics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: Denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 7871–7880, Online. Association for Computa-\ntional Linguistics.\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.\nImplicit representations of meaning in neural lan-\nguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1813–1827, Online. Association for\nComputational Linguistics.\nRuibo Liu, Jason Wei, Shixiang Shane Gu, Te-Yen Wu,\nSoroush V osoughi, Claire Cui, Denny Zhou, and An-\ndrew M Dai. 2022. Mind’s eye: Grounded language\nmodel reasoning through simulation. arXiv preprint\narXiv:2210.05359.\nGary Marcus and Ernest Davis. 2020. Gpt-3, bloviator:\nOpenai’s language generator has no idea what it’s\ntalking about. [Online; posted 22-August-2020].\nMarianna Martindale, Marine Carpuat, Kevin Duh, and\nPaul McNamee. 2019. Identifying fluently inade-\nquate output in neural and statistical machine transla-\ntion. In Proceedings of Machine Translation Summit\nXVII: Research Track, pages 233–243, Dublin, Ire-\nland. European Association for Machine Translation.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\n12565\nBhavana Dalvi Mishra, Lifu Huang, Niket Tandon,\nWen-tau Yih, and Peter Clark. 2018. Tracking state\nchanges in procedural text: a challenge dataset and\nmodels for process paragraph comprehension. arXiv\npreprint arXiv:1805.06975.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, et al. 2021. Show your work: Scratch-\npads for intermediate computation with language\nmodels. arXiv preprint arXiv:2112.00114.\nYasumasa Onoe, Michael JQ Zhang, Eunsol Choi, and\nGreg Durrett. 2021. Creak: A dataset for com-\nmonsense reasoning over entity knowledge. arXiv\npreprint arXiv:2109.01653.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Köpf, Edward\nYang, Zach DeVito, Martin Raison, Alykhan Tejani,\nSasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie\nBai, and Soumith Chintala. 2019. PyTorch: An Im-\nperative Style, High-Performance Deep Learning Li-\nbrary. Curran Associates Inc., Red Hook, NY , USA.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nKyle Richardson, Ronen Tamari, Oren Sultan, Reut\nTsarfaty, Dafna Shahaf, and Ashish Sabharwal. 2022.\nBreakpoint transformers for modeling and tracking\nintermediate beliefs.\nPratyusha Sharma, Antonio Torralba, and Jacob An-\ndreas. 2021. Skill induction and planning with latent\nlanguage. arXiv preprint arXiv:2110.01517.\nShane Storks, Qiaozi Gao, Yichi Zhang, and Joyce Chai.\n2021. Tiered reasoning for intuitive physics: Toward\nverifiable commonsense language understanding. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2021 , pages 4902–4918, Punta\nCana, Dominican Republic. Association for Compu-\ntational Linguistics.\nTianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing\nHuang, and Xipeng Qiu. 2022. Black-box tuning\nfor language-model-as-a-service. arXiv preprint\narXiv:2201.03514.\nSwabha Swayamdipta, Sam Thomson, Kenton Lee,\nLuke Zettlemoyer, Chris Dyer, and Noah A Smith.\n2018. Syntactic scaffolds for semantic structures.\narXiv preprint arXiv:1808.10485.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed Chi, Quoc Le, and Denny Zhou. 2022.\nChain of thought prompting elicits reasoning in large\nlanguage models. arXiv preprint arXiv:2201.11903.\nEthan Wilcox, Peng Qian, Richard Futrell, Miguel\nBallesteros, and Roger Levy. 2019. Structural super-\nvision improves learning of non-local grammatical\ndependencies. arXiv preprint arXiv:1903.00943.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language processing.\nIn Proceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing: System\nDemonstrations, pages 38–45, Online. Association\nfor Computational Linguistics.\nEric Zelikman, Yuhuai Wu, and Noah D Goodman.\n2022. Star: Bootstrapping reasoning with reason-\ning. arXiv preprint arXiv:2203.14465.\nRowan Zellers, Ari Holtzman, Matthew Peters, Roozbeh\nMottaghi, Aniruddha Kembhavi, Ali Farhadi, and\nYejin Choi. 2021. PIGLeT: Language grounding\nthrough neuro-symbolic interaction in a 3D world.\nIn Proceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n2040–2050, Online. Association for Computational\nLinguistics.\nChunting Zhou, Graham Neubig, Jiatao Gu, Mona Diab,\nFrancisco Guzmán, Luke Zettlemoyer, and Marjan\nGhazvininejad. 2021. Detecting hallucinated content\nin conditional neural sequence generation. In Find-\nings of the Association for Computational Linguis-\ntics: ACL-IJCNLP 2021, pages 1393–1404, Online.\nAssociation for Computational Linguistics.\nRolf A Zwaan and Diane Pecher. 2012. Revisiting men-\ntal simulation in language comprehension: Six repli-\ncation attempts. PloS one, 7(12):e51382.\n12566\nA Constructing the State\nIn each domain, the state is a collection of facts\n(attributes and/or relations) about each entity. It is\nupdated each time there is a new action, instruc-\ntion, or sentence. We convert the state to natural\nlanguage to take advantage of existing linguistic un-\nderstanding in pre-trained models. Future work can\nexamine the effect of using non-natural-language\nforms of state.\nBelow, we discuss the details of this conversion\nfrom the available state annotations in each do-\nmains.\nTW In TW, the simulator gives us thefull state,\nor the full set of facts describing the state of the\nworld after executing each agent action. Facts are\neither entity properties (e.g. locked(door)), or\nrelations between two entities (e.g. is-in(key,\nchest)). However, since the agent has not ex-\nplored the full state at the start of each game, at\neach step, we compute a subset of the facts that the\nagent knows about. We call this the known state.\nWe further restrict this subset to only facts that are\ncausally relevant to any possible next action that\nthe agent can take, such that all possible next ac-\ntions can be inferred from just this set. We call this\nthe relevant known state.\nWe compute both these sets heuristically: the\nknown state consists of all facts about any currently\nor previously accessible entities that the agent has\nencountered. For the relevant known state, we dis-\ncard facts about previously accessible entities and\nkeep only facts about currently accessible entities.\nSpecifically, the relevant known state consists of\nfacts about: 1. player location, 2. all currently\naccessible items (i.e. in the current room or in the\ninventory), 3. which doorways are accessible from\nthe current room and/or which rooms neighbor the\ncurrent room.\nWe convert collections of facts to natural lan-\nguage following the same procedure as Li et al.\n(2021). Specifically, propositions p(o) are con-\nverted to “the {o}is {p}”, while relations r(o1,o2)\nare converted to “the {o1}is {r}{o2}”.\nTRIP In TRIP, we write out seed states for 16\nstories, consisting of facts known to hold true af-\nter each sentence of the story — then use GPT3\nto automatically infer states for the remaining sto-\nries in the training data. We aim to construct the\nstate in TRIP to capture the spirit of the relevant\nknown state in TW (which we know from §7.1 to\nbe the optimal state supervision), whereby we only\ninclude facts both known from the prior context\nand potentially causally relevant to the next sen-\ntence. However, though capturing known facts is\nstraightforward, because TRIP is a real dataset con-\nsisting of open-ended text, the set of plausible next\ngenerations is open-ended, meaning that the full set\nof causally relevant known facts cannot be always\nbe anticipated ahead of time. Instead, we use the\nground-truth acceptable completion as a minimal\nguarantee – we aim to include facts informative for\ngenerating at least the single ground-truth next sen-\ntence in the acceptable story (which isn’t always\nstraightforwardly derived from the known facts).\nOne example is as follows:\n• T = Tom packed his gloves in his suitcase.\nTom checked his suitcase in at the airport.\n• S = Tom’s gloves are in the suitcase.\nThe suitcase is checked in at the\nairport. Tom does not have his\nsuitcase. Tom does not have his\ngloves.\n• T′ = Tom boarded the plane without his\ngloves.\nNote that while Tom does not have his gloves\nis technically inferrable from Tom’s gloves are\nin the suitcase. The suitcase is checked\nin at the airport, including this fact explicitly\nin S reinforces the causal link between the next\nsentence T′and S.\nFor the analysis in §7.1, we compare against a\nstringified version of the originally-provided states.\nIn the original dataset, each sentence of a story is\nannotated with the state changes applied to each of\nthe (up to 15) attributes of that entity. The state an-\nnotations take the form of (entity,attribute,value)\ntriples. Each entity attribute is associated with a\nvalue indicating the direction of change for that\nattribute. For example, (shirt,cleanliness,true →\nfalse) indicates the shirt became dirty.\nBecause there are a finite set of (15) attributes\nand (8) values, we enumerate rules for converting\nall (attribute,value) pairs to natural language pred-\nicates VP. We then convert(entity,attribute,value)\ntriples into “the {entity}VP”.\n12567\n|X | |X A| Method Coherence\n1k 0 Fine-tuning 40.0%±0.7%\n1k 500 S ITSUP 40.0%±0.8%\n1k 500 S ITSUP + Latent 40.0%±0.4%\n1k 1k S ITSUP 42.9%±1.0%\nTable 7: TW game response generation coherence. We\nevaluate BART fine-tuning with and without compo-\nnents of SITUATION SUPERVISION .\nB Further TW Evaluations\nB.1 Game Response Coherence for BART\nFine-Tuning\nThe text of TW consists of alternating actions and\ngame responses. For example:\n> open door\nYou open the door\n> go west\n-= Kitchen =-\nYou arrive at a kitchen. You see a counter.\nOn the counter is an old key. [...]\nIn this example, lines starting with > are actions\nand all other lines are game responses.\nIn §6, we only evaluated coherence of generat-\ning actions in TW. Here, we evaluate coherence of\ngenerating game responsesas well. Due to quota re-\nstrictions, we evaluate game responses only for fine-\ntuning approaches and not prompting approaches.\nTable 7 reports coherence results for game re-\nsponses alone, and game responses and actions\ncombined. Unlike the set of acceptable actions,\nthe TW simulator does not provide us with a set\nof acceptable game responses. Instead, we can\nonly compare the ground-truth game response from\nthe simulator. This can result in over-penalization:\nwhen pieces of the underlying state are still un-\nknown, the LM will be falsely penalized, despite\ngenerating a game response coherent with the prior\ncontext. Thus the numbers reported in Table 7 are\nsimply a lower bound.\nB.2 Generation Diversity\nTo measure the diversity of LM outputs, we use\nrecall12 between the set of LM generations and\nthe full set of ground-truth valid sentences. This\nlatter set is provided to us by the TextWorld sim-\nulator. Note that this set is not entirely complete,\nas there will be generations that are consistent with\n12Because we sample at most 5 unique generations from the\nLM, there is a hard ceiling on maximum achievable “recall”\nin our case.\nModel |X | |X A| Method Recall\nBART\n1k 0 Fine-tuning 11.8%±0.3%\n1k 500 S ITSUP 11.8%±0.3%\n1k 500 S ITSUP + Latent 11.9%±0.2%\n1k 1k S ITSUP 12.6%±0.4%\nGPT3\n25 0 Text prompting 33.3%\n25 12 S ITSUP 40.1%\n25 12 S ITSUP + Latent 42.1%\n25 25 S ITSUP 40.9%\nTable 8: TW generation diversity for fine-tuning and\nprompting with and without components of SITUATION -\nSUPERVISION . We see that using SITUATION SUPER -\nVISION with fine-tuning does not harm the diversity of\ngenerated samples, while using SITUATION SUPERVI -\nSION with prompting actually increases diversity.\nthe known facts from the prior context but con-\ntradict an unknown fact, and is consequently not\naccepted by the simulator. However, recall against\nthe simulator-provided set of valid sentences re-\nmains a good heuristic for diversity.\nWe examine how training with SITUATION -\nSUPERVISION affects generation diversity. We\nuse the same models and training/prompting se-\ntups as in §6 and evaluate the diversity among the\ngenerated samples. Results are shown in Table 8.\nWe showed in §6 that SITUATION SUPERVISION\nimproves TW generation coherence in both the\nfine-tuning and prompting cases. As shown in Ta-\nble 8, SITUATION SUPERVISION does not sacri-\nfices diversity to achieve those coherence gains. In\nfact, prompting with SITUATION SUPERVISION im-\nproves diversity when compared against a text-only\nmodel, and doing latent inference appears to addi-\ntionally improve diversity beyond simply auxiliary\nsituation modeling.\nC Infrastructure and Reproducibility\nWe ran all fine-tuning experiments on a single\n32GB NVIDIA Tesla V100 GPU. We use a BART-\nbase model which has 6 Transformer layers each\nin its encoder and decoder, and 139M total param-\neters. Training time varies depending on domain\nand data size, but generally is not longer than a few\nhours. As a reference point: on 1000 TW examples,\ntraining takes ∼1 hour for text-only training, ∼1-2\nhours for training with auxiliary state supervision,\nand ∼1-3 hours for training with latent state super-\nvision. For prompting results, we use OpenAI’s\nGPT3 text-davinci-002 model. For sampling\nnext actions in TW, we use a generation temper-\nature of 0.7. When judging acceptability of each\n12568\nsentence in TRIP, we directly compare p(Not OK)\nagainst p(OK). When sampling states for latent\nstate inference, to encourage diversity, we use a\ngeneration temperature of 0.9.\nWe used PyTorch (Paszke et al., 2019) and Hug-\ngingface Transformers (Wolf et al., 2020) for im-\nplementing and training BART-base models. We\nuse OpenAI’s API13 for querying GPT3.\n13https://beta.openai.com/\n12569\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 9\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 10\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nAbstract, Section 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 5\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 5\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nCould not ﬁnd license\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nAll data was intended for research.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNone of the data used should contain identiﬁable or offensive information.\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 5\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 5\nC □\u0013 Did you run computational experiments?\nSection 6,7\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix D\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n12570\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5, Appendix D\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 6\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nAppendix D\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n12571",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8282017707824707
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.6471306085586548
    },
    {
      "name": "Language model",
      "score": 0.6010960340499878
    },
    {
      "name": "Inference",
      "score": 0.5872204303741455
    },
    {
      "name": "ENCODE",
      "score": 0.5481926202774048
    },
    {
      "name": "Construct (python library)",
      "score": 0.5477690696716309
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5416465401649475
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5184597373008728
    },
    {
      "name": "Natural language processing",
      "score": 0.5040720701217651
    },
    {
      "name": "Latent variable",
      "score": 0.5012269020080566
    },
    {
      "name": "State (computer science)",
      "score": 0.5002419948577881
    },
    {
      "name": "Task (project management)",
      "score": 0.4759499728679657
    },
    {
      "name": "Encoding (memory)",
      "score": 0.4212711751461029
    },
    {
      "name": "Algorithm",
      "score": 0.20533356070518494
    },
    {
      "name": "Mathematics",
      "score": 0.11169648170471191
    },
    {
      "name": "Statistics",
      "score": 0.07152050733566284
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I63966007",
      "name": "Massachusetts Institute of Technology",
      "country": "US"
    }
  ],
  "cited_by": 6
}