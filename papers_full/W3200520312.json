{
  "title": "SHAPE: Shifted Absolute Position Embedding for Transformers",
  "url": "https://openalex.org/W3200520312",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5066002127",
      "name": "Shun Kiyono",
      "affiliations": [
        "Preferred Networks (Japan)",
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5112876488",
      "name": "Sosuke Kobayashi",
      "affiliations": [
        "Preferred Networks (Japan)",
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5002182453",
      "name": "Jun Suzuki",
      "affiliations": [
        "Preferred Networks (Japan)",
        "Tohoku University"
      ]
    },
    {
      "id": "https://openalex.org/A5101815181",
      "name": "Kentaro Inui",
      "affiliations": [
        "Preferred Networks (Japan)",
        "Tohoku University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3134357720",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W3114271877",
    "https://openalex.org/W3121309507",
    "https://openalex.org/W3106531402",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970295111",
    "https://openalex.org/W3106210592",
    "https://openalex.org/W2941599692",
    "https://openalex.org/W2963011474",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W3035428952",
    "https://openalex.org/W3185923046",
    "https://openalex.org/W3133264589",
    "https://openalex.org/W4309793872",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2989156240",
    "https://openalex.org/W2963532001",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W3132607382",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W2963212250",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3210914950",
    "https://openalex.org/W3170261818",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W4288089799"
  ],
  "abstract": "Position representation is crucial for building position-aware representations in Transformers. Existing position representations suffer from a lack of generalization to test data with unseen lengths or high computational cost. We investigate shifted absolute position embedding (SHAPE) to address both issues. The basic idea of SHAPE is to achieve shift invariance, which is a key property of recent successful position representations, by randomly shifting absolute positions during training. We demonstrate that SHAPE is empirically comparable to its counterpart while being simpler and faster.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 3309–3321\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n3309\nSHAPE: Shifted Absolute Position Embedding for Transformers\nShun Kiyono♠,♥ Sosuke Kobayashi♥,♦ Jun Suzuki♥,♠ Kentaro Inui♥,♠\n♠RIKEN ♥Tohoku University ♦Preferred Networks, Inc.\nshun.kiyono@riken.jp, sosk@preferred.jp,\n{jun.suzuki, inui}@tohoku.ac.jp\nAbstract\nPosition representation is crucial for building\nposition-aware representations in Transformers.\nExisting position representations suffer from\na lack of generalization to test data with un-\nseen lengths or high computational cost. We\ninvestigate shifted absolute position embedding\n(SHAPE) to address both issues. The basic idea\nof SHAPE is to achieve shift invariance, which\nis a key property of recent successful position\nrepresentations, by randomly shifting absolute\npositions during training. We demonstrate that\nSHAPE is empirically comparable to its coun-\nterpart while being simpler and faster1.\n1 Introduction\nPosition representation plays a critical role in self-\nattention-based encoder-decoder models (Trans-\nformers) (Vaswani et al., 2017), enabling the\nself-attention to recognize the order of input se-\nquences. Position representations have two cat-\negories (Dufter et al., 2021): absolute position\nembedding (APE) (Gehring et al., 2017; Vaswani\net al., 2017) and relative position embedding\n(RPE) (Shaw et al., 2018). With APE, each po-\nsition is represented by a unique embedding, which\nis added to inputs. RPE represents the position\nbased on the relative distance between two tokens\nin the self-attention mechanism.\nRPE outperforms APE on sequence-to-sequence\ntasks (Narang et al., 2021; Neishi and Yoshinaga,\n2019) due to extrapolation, i.e., the ability to gen-\neralize to sequences that are longer than those\nobserved during training (Newman et al., 2020).\nWang et al. (2021) reported that one of the key\nproperties contributing to RPE’s superior perfor-\nmance is shift invariance2, the property of a func-\ntion to not change its output even if its input is\nshifted. However, unlike APE, RPE’s formulation\n1The code is available at https://github.com/\nbutsugiri/shape.\n2Shift invarianceis also known as translation invariance.\nMulti-Head\nAttention\nAdd & Norm\nFeed\nForward\nAdd & Norm\nInput\nEmbedding\n+\nJohn yelled at KevinPosition0\n1234\n k-1k\nk+1\nk+2\nk+3\nAbsolute Position Embedding (APE)(a) Shifted APE (SHAPE)(c)\nTransformerRelative Position Embedding (RPE)(b)\nRelative Distance John yelled at Kevin\n! ! \" #\n{ %&' , )*+,& } ! . \" #\n{ %&' , )*+,& }\n-2 -1 0 1 2\nKeyValue\nShifted by random offset k\nFigure 1: Overview of position representations. (a) APE\nand (c) SHAPE consider absolute positions in the input\nlayer, whereas (b) RPE considers the relative position\nof a given token pair in the self-attention mechanism.\nstrongly depends on the self-attention mechanism.\nThis motivated us to explore a way to incorporate\nthe beneﬁt of shift invariance in APE.\nA promising approach to achieving shift invari-\nance while using absolute positions is to randomly\nshift positions during training. A similar idea can\nbe seen in several contexts, e.g., computer vision\n(Goodfellow et al., 2016) and question-answering\nin NLP (Geva et al., 2020). APE is no exception;\na random shift should force Transformer to cap-\nture the relative positional information from ab-\nsolute positions. However, the effectiveness of a\nrandom shift for incorporating shift invariance in\nAPE is yet to be demonstrated. Thus, we formulate\nAPE with a random shift as a variant of position\nrepresentation, namely, Shifted Absolute Position\nEmbedding (SHAPE; Figure 1c), and conduct a\nthorough investigation. In our experiments, we ﬁrst\nconﬁrm that Transformer with SHAPE learns to be\nshift-invariant. We then demonstrate that SHAPE\nachieves a performance comparable to RPE in ma-\nchine translation. Finally, we reveal that Trans-\nformer equipped with shift invariance shows not\nonly better extrapolation ability but also better in-\nterpolation ability, i.e., it can better predict rare\nwords at positions observed during the training.\n3310\n2 Position Representations\nFigure 1 gives an overview of the position represen-\ntations compared in this paper. We denote a source\nsequence X as a sequence of I tokens, namely,\nX = (x1, . . . , xI). Similarly, let Y represent a\ntarget sequence of J tokens Y = (y1, . . . , yJ).\n2.1 Absolute Position Embedding (APE)\nAPE provides each position with a unique embed-\nding (Figure 1a). Transformer with APE computes\nthe input representation as the sum of the word em-\nbedding and the position embedding for each token\nxi ∈X and yj ∈Y .\nSinusoidal positional encoding (Vaswani et al.,\n2017) is a deterministic function of the position\nand the de factostandard APE for Transformer3.\nSpeciﬁcally, for the i-th token, the m-th element of\nposition embedding PE(i, m) is deﬁned as\nPE(i, m)=\n\n\n\nsin\n(\ni\n10000\n2m\nD\n)\nm is even\ncos\n(\ni\n10000\n2m\nD\n)\nm is odd\n, (1)\nwhere D denotes the model dimension.\n2.2 Relative Position Embedding (RPE)\nRPE (Shaw et al., 2018) incorporates position in-\nformation by considering the relative distance be-\ntween two tokens in the self-attention mechanism\n(Figure 1b). For example, Shaw et al. (2018)\nrepresent the relative distance between the i-th\nand j-th tokens with relative position embeddings\naKey\ni−j , aValue\ni−j ∈RD. These embeddings are then\nadded to key and value representations, respec-\ntively.\nRPE outperforms APE on out-of-distribution\ndata in terms of sequence length owing to its innate\nshift invariance(Rosendahl et al., 2019; Neishi and\nYoshinaga, 2019; Narang et al., 2021; Wang et al.,\n2021). However, the self-attention mechanism of\nRPE involves more computation than that of APE4.\nIn addition, more importantly, RPE requires the\nmodiﬁcation of the architecture, while APE does\nnot. Speciﬁcally, RPE strongly depends on the\nself-attention mechanism; thus, it is not necessar-\nily compatible with studies that attempt to replace\n3Learned position embedding (Gehring et al., 2017) is\nyet another variant of APE; however, we exclusively focus on\nsinusoidal positional encoding as its performance is compara-\nble (Vaswani et al., 2017).\n4Narang et al. (2021) reported that Transformer with RPE\nis up to 25% slower than that with APE.\nthe self-attention with a more lightweight alterna-\ntive (Kitaev et al., 2020; Choromanski et al., 2021;\nTay et al., 2020).\nRPE, which was originally proposed by Shaw\net al. (2018), has many variants in the litera-\nture (Dai et al., 2019; Raffel et al., 2020; Huang\net al., 2020; Wang et al., 2021; Wu et al., 2021).\nThey aim to improve the empirical performance or\nthe computational speed compared with the origi-\nnal RPE. However, the original RPE is still a strong\nmethod in terms of the performance. Narang et al.\n(2021) conducted a thorough comparison on multi-\nple sequence-to-sequence tasks and reported that\nthe performance of the original RPE is compara-\nble to or sometimes better than its variants. Thus,\nwe exclusively use the original RPE in our experi-\nments.\n2.3 Shifted Absolute Position Embedding\n(SHAPE)\nGiven the drawbacks of RPE, we investigate\nSHAPE (Figure 1c) as a way to equip Transformer\nwith shift invariance without any architecture mod-\niﬁcation or computational overhead on APE. Dur-\ning training, SHAPE shifts every position index of\nAPE by a random offset. This prevents the model\nfrom using absolute positions to learn the task and\ninstead encourages the use of relative positions,\nwhich we expect to eventually lead to the learning\nof shift invariance.\nLet k represent an offset drawn from a discrete\nuniform distribution U{0, K}for each sequence\nand for every iteration during training, where K ∈\nN is the maximum shift. SHAPE only replaces\nPE(i, m) of APE in Equation 1 with\nPE(i + k, m). (2)\nWe independently sample k for the source and tar-\nget sequence. SHAPE can thus be incorporated\ninto any model using APE with virtually no compu-\ntational overhead since only the input is modiﬁed.\nNote that SHAPE is equivalent to the original APE\nif we set K = 0; in fact, we set K = 0 during\ninference. Thus, SHAPE can be seen as a natural\nextension to incorporate shift invariance in APE.\nSHAPE can be interpreted in multiple view-\npoints. For example, SHAPE can be seen as a\nregularizer that prevents Transformer from over-\nﬁtting to the absolute position; such overﬁtting is\nundesirable not only for extrapolation (Neishi and\nYoshinaga, 2019) but also for APE with length\n3311\nconstraints (Takase and Okazaki, 2019; Oka et al.,\n2020, 2021). In addition, SHAPE can be seen as a\ndata augmentation method because the randomly\nsampled k shifts each instance into different sub-\nspaces during training.\n3 Experiments\nUsing machine translation benchmark data, we ﬁrst\nconﬁrmed that Transformer trained with SHAPE\nlearns shift invariance (Section 3.2). Then, we com-\npared SHAPE with APE and RPE to investigate its\neffectiveness (Section 3.3).\n3.1 Experimental Conﬁguration\nDataset We used the WMT 2016 English-\nGerman dataset for training and followed Ott et al.\n(2018) for tokenization and subword segmenta-\ntion (Sennrich et al., 2016). We used newstest2010-\n2013 and newstest2014-2016 as the validation and\ntest sets, respectively.\nOur experiments consist of the following three\ndistinct dataset settings:\n(i) VANILLA : Identical to previous stud-\nies (Vaswani et al., 2017; Ott et al., 2018).\n(ii) EXTRAPOLATE : Shift-invariant models are\ntypically evaluated in terms of extrapolation abil-\nity (Wang et al., 2021; Newman et al., 2020). We\nreplicated the settings of Neishi and Yoshinaga\n(2019); the training set excludes pairs whose source\nor target sequence exceeds 50 subwords, while the\nvalidation and test sets are identical to VANILLA .\n(iii) INTERPOLATE : We also evaluate the models\nfrom the viewpoint of interpolation, which we de-\nﬁne as the ability to generate tokens whose lengths\nare seen during training. Speciﬁcally, we evaluate\ninterpolation using long sequences since, ﬁrst, the\ngeneration of long sequences is an important re-\nsearch topic in NLP (Zaheer et al., 2020; Maruf\net al., 2021) and second, in datasets with long se-\nquences, the position distribution of each token\nbecomes increasingly sparse. In other words, to-\nkens in the validation and test sets become unlikely\nto be observed in the training set at corresponding\npositions; we expect that shift invariance is crucial\nfor addressing such position sparsity.\nIn this study, we artiﬁcially generate a long\nsequence by simply concatenating independent\nsentences in parallel corpus. Speciﬁcally, given\nten neighboring sentences of VANILLA , i.e.,\nX1, . . . ,X10 and Y1, . . . ,Y10, we concatenate\neach sentence with a unique token ⟨sep⟩. We also\nOriginal Swapped Performance Drop\nAPE 28.81 20.74 8.07\nSHAPE 28.51 27.06 1.45\nTable 1: BLEU score on the sub-sampled training\ndata of INTERPOLATE (10,000 pairs). In Original and\nSwapped, the order of input sequence is X1, . . . ,X10\nand X2, . . . ,X10, X1, respectively.\napply the same operation to the validation and test\nsets.\nEvaluation We evaluate the performance with\nsacreBLEU (Post, 2018). Throughout the experi-\nment, we apply the moses detokenizer to the system\noutput and then compute the detokenized BLEU5.\nModels We adopt transformer-base (Vaswani\net al., 2017) with APE, SHAPE, or RPE, re-\nspectively. Our implementations are based on\nOpenNMT-py (Klein et al., 2017). Unless other-\nwise stated, we use a ﬁxed value ( K = 500) for\nthe maximum shift of SHAPE to demonstrate that\nSHAPE is robust against the choice of K. We\nset the relative distance limit in RPE to 16 follow-\ning Shaw et al. (2018) and Neishi and Yoshinaga\n(2019)6.\n3.2 Experiment 1: Shift Invariance\nWe conﬁrmed that SHAPE learns shift invariance\nby comparing APE and SHAPE trained on INTER -\nPOLATE .\nQuantitative Evaluation: BLEU on Training\nData We ﬁrst evaluated if the model is robust\nto the order of sentences in each sequence. We\nused the sub-sampled training data (10k pairs) of\nINTERPOLATE to eliminate the effect of unseen\nsentences; in this way, we can isolate the effect\nof sentence order. Given a sequence in the origi-\nnal order (Original), X1, . . . ,X10, we generated a\nswapped sequence (Swapped) by moving the ﬁrst\nsentence to the end, i.e., X2, . . . ,X10, X1. The\nmodel then generates two sequences Y ′\n1 , . . . ,Y ′\n10\nand Y ′\n2 , . . . ,Y ′\n10, Y ′\n1 . Finally, we evaluated the\nBLEU score of Y ′\n1 . The result is shown in Ta-\nble 1. Here, SHAPE has a much smaller perfor-\nmance drop than APE when evaluated on different\nsentence ordering. This result indicates the shift\ninvariance property of SHAPE.\nQualitative Evaluation: Similarities of Repre-\nsentations We also qualitatively conﬁrmed the\n5Details of datasets and evaluation are in Appendix A.\n6See Appendix B for a list of hyperparameters.\n3312\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0\nFigure 2: Cosine similarities of the encoder hidden\nstates with different offsets k ∈ {0, 100, 250, 500}.\nOnly the representation of SHAPE is invariant with\nk.\nDataset Model Valid Test Speed\nVANILLA APE† 23.61 30.46 x1.00\nRPE† 23.67 30.54 x0.91\nSHAPE† 23.63 30.49 x1.01\nEXTRAPOLATE APE 22.18 29.22 x1.00\nRPE 22.97 29.86 x0.91\nSHAPE 22.96 29.80 x0.99\nINTERPOLATE APE 31.40 38.23 x1.00\nRPE∗ - - -\nSHAPE 32.50 39.09 x0.99\nTable 2: BLEU scores on newstest2010-2016. Valid is\nthe average of newstest2010-2013.Testis the average of\nnewstest2014-2016. The scores for individual newstests\nare available in Appendix D. †: the values are averages\nof ﬁve distinct trials with ﬁve different random seeds.\n∗: not available as the implementation was very slow.\nSpeed is the relative speed to APE (larger is faster).\nshift invariance as shown in Figure 2. The ﬁgure\nillustrates how the offsetk changes the encoder rep-\nresentations of trained models APE and SHAPE.\nGiven the two models and an input sequenceX, we\ncomputed the encoder hidden states of the given in-\nput sequence for each k ∈{0, 100, 250, 500}. For\neach position i, we computed the cosine similarity\n(sim) of the hidden states from two offsets, i.e.,\nhk1\ni , hk2\ni ∈RD, and computed its average across\nthe positions as\n1\nI\nI∑\ni=1\nsim(hk1\ni , hk2\ni ). (3)\nAs shown in Figure 2, SHAPE builds a shift-\ninvariant representation; regardless of the offset\nk, the cosine similarity is almost always 1.0. Such\ninvariance is nontrivial because the similarity of\nAPE does not show similar characteristics7.\n3.3 Experiment 2: Performance Comparison\nWe compared the overall performance of position\nrepresentations on the validation and test sets as\n7Additional ﬁgures are available in Appendix C.\n0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-\nSource Sequence Length (tokens)\n0\n5BLEU improvement\n(a) EXTRAPOLATE dataset\nAPE\nRPE\nSHAPE (K = 40)\nSHAPE (K = 500)\n0-99 100-199 200-299 300-399 400-499 500-599 600-\nSource Sequence Length (tokens)\n0\n1\n2BLEU improvement\n(b) INTERPOLATE dataset\nAPE\nSHAPE (K = 500)\nFigure 3: BLEU score improvement from APE on vali-\ndation and test sets with respect to the source sequence\nlength. The gray color means no training data.\nshown in Table 2. Figure 3 shows the BLEU im-\nprovement of RPE and SHAPE from APE with\nrespect to the source sequence length8.\nOn VANILLA , the three models show compa-\nrable results. APE being comparable to RPE is\ninconsistent with the result reported by Shaw et al.\n(2018); we assume that this is due to a difference\nin implementation. In fact, Narang et al. (2021)\nhave recently reported that improvements in Trans-\nformer often do not transfer across implementa-\ntions.\nOn EXTRAPOLATE , RPE (29.86) outperforms\nAPE (29.22) by approximately 0.6 BLEU points\non the test set; this is consistent with the result re-\nported by Neishi and Yoshinaga (2019). Moreover,\nSHAPE achieves comparable test performance to\nRPE (29.80). According to Figure 3a, both RPE\nand SHAPE have improved extrapolation ability,\ni.e., better BLEU scores on sequences longer than\nthose observed during training. In addition, Fig-\nure 3a shows the performance of SHAPE with the\nmaximum shift K = 40 that was chosen on the\nbasis of the BLEU score for the validation set. This\nmodel outperforms RPE, achieving BLEU scores\nof 23.12 and 29.86 on the validation and test sets,\nrespectively. These results indicate that SHAPE\ncan be a better alternative to RPE.\nOn INTERPOLATE , we were unable to train\nRPE because its training was prohibitively slow9.\n8The same graph with absolute BLEU is in Appendix D.\n9A single gradient step of RPE took about 5 seconds,\nwhich was 20 times longer than that of APE and SHAPE. We\nassume that the RPE implementation available in OpenNMT-\npy has difﬁculty in dealing with long sequences.\n3313\n1-10 11-20 21-30 31-40 41-50 51-60 61-70 71-\nDecoding Position\n1-5000\n5001-15000\n15001-25000\n25001-\nVocab. Frequency Rank: \n 1 is the most frequent\n0.52 0.53 0.53 0.53 0.58 0.76 0.84 0.79\n0.51 0.51 0.51 0.52 0.53 0.73 0.85 0.85\n0.50 0.50 0.52 0.51 0.57 0.75 0.84 0.81\n0.49 0.46 0.44 0.51 0.59 0.70 1.00 0.86\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(a) EXTRAPOLATE dataset\n1-100 101-200 201-300 301-400 401-500 501-\nDecoding Position\n1-5000\n5001-15000\n15001-25000\n25001-\nVocab. Frequency Rank: \n 1 is the most frequent\n0.52 0.53 0.53 0.53 0.52 0.52\n0.51 0.51 0.51 0.51 0.53 0.50\n0.51 0.53 0.52 0.51 0.50 0.51\n0.53 0.53 0.63 0.69 0.73 n/a\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n(b) INTERPOLATE dataset\nFigure 4: Tokenwise analysis on gold references: the\nvalue in each cell represents the ratio that SHAPE as-\nsigns a higher score to a gold token than APE.\nSimilarly to EXTRAPOLATE , SHAPE (39.09) out-\nperforms APE (38.23) on the test set. Figure 3b\nshows that SHAPE consistently outperformed APE\nfor every sequence length. From this result, we ﬁnd\nthat the shift invariance also improves the interpo-\nlation abilityof Transformer.\n4 Analysis\nThis section provides a deeper analysis of how the\nmodel with translation invariance improves the per-\nformance. We hereinafter exclusively focus on\nAPE and SHAPE because SHAPE achieves com-\nparable performance to RPE, and we were unable\nto train RPE on the INTERPOLATE dataset as ex-\nplained in footnote 9.\nAs discussed in Section 3.3, Figure 3 demon-\nstrated that SHAPE outperformed APE in terms\nof BLEU score. However, BLEU evaluates two\nconcepts simultaneously, that is, the token preci-\nsion via n-gram matching and the output length via\nthe brevity penalty (Papineni et al., 2002). Thus,\nthe actual source of improvement remains unclear.\nWe hereby exclusively analyzed the precision of\ntoken prediction. Speciﬁcally, we computed token-\nwise scores assigned for gold references, and we\nthen compared them across the models; given a se-\nquence pair (X, Y ) and a trained model, we com-\nputed a score (i.e., log probability) sj for each to-\nken yj in a teacher-forcing manner. Here, a higher\nscore to gold token means better model perfor-\nmance. We used the validation set for comparison.\nFigure 4 shows the ratio that SHAPE assigns a\nhigher score to a gold token than APE, compared\nacross for each position of the decoder.\nBetter extrapolation means better token preci-\nsion Figure 4a shows that SHAPE outperforms\nAPE, especially in the right part of the heat map.\nThis area corresponds to sequences longer than\nthose observed during training. This result indi-\ncates that better extrapolation in terms of BLEU\nscore means better token precision.\nInterpolation is particularly effective for rare\ntokens As shown in Figure 4b, SHAPE consis-\ntently outperforms APE and the performance gap is\nespecially signiﬁcant in the low-frequency region\n(bottom part). This indicates that SHAPE predicts\nrare words better than APE. One plausible expla-\nnation for this observation is that SHAPE carries\nout data augmentation in the sense that in each\nepoch, the same sequence pair is assigned a differ-\nent position depending on the offset k. Rare words\ntypically have sparse position distributions in train-\ning data and thus beneﬁt from the extra position\nassignment during training.\n5 Conclusion\nWe investigated SHAPE, a simple variant of APE\nwith shift invariance. We demonstrated that\nSHAPE is empirically comparable to RPE yet im-\nposes almost no computational overhead on APE.\nOur analysis revealed that SHAPE is effective at\nextrapolation to unseen lengths and interpolating\nrare words. SHAPE can be incorporated into the\nexisting codebase with a few lines of code and no\nrisk of a performance drop from APE; thus, we\nexpect SHAPE to be used as a drop-in replacement\nfor APE and RPE.\nAcknowledgements\nWe thank the anonymous reviewers for their in-\nsightful comments. We thank Sho Takase for valu-\nable discussions. We thank Ana Brassard, Ben-\njamin Heinzerling, Reina Akama, and Yuta Mat-\nsumoto for their valuable feedback. The work\nof Jun Suzuki was supported by JST Moonshot\nR&D Grant Number JPMJMS2011 (fundamen-\ntal research) and JSPS KAKENHI Grant Number\n19H04162 (empirical evaluation).\nReferences\nKrzysztof Choromanski, Valerii Likhosherstov, David\nDohan, Xingyou Song, Andreea Gane, Tamas Sar-\nlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin,\nŁukasz Kaiser, et al. 2021. Rethinking Attention\n3314\nwith Performers. In Proceedings of the 9th Inter-\nnational Conference on Learning Representations\n(ICLR 2021).\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive Language Models beyond\na Fixed-Length Context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL 2019), pages 2978–2988.\nMichael Denkowski and Graham Neubig. 2017.\nStronger Baselines for Trustable Results in Neural\nMachine Translation. In Proceedings of the First\nWorkshop on Neural Machine Translation, pages 18–\n27.\nPhilipp Dufter, Martin Schmitt, and Hinrich Schütze.\n2021. Position Information in Transformers: An\nOverview. arXiv preprint arXiv:2102.11090.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N. Dauphin. 2017. Convolutional\nSequence to Sequence Learning. In Proceedings\nof the 34th International Conference on Machine\nLearning (ICML 2017), pages 1243–1252.\nMor Geva, Ankit Gupta, and Jonathan Berant. 2020.\nInjecting Numerical Reasoning Skills into Language\nModels. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics\n(ACL 2020), pages 946–958.\nIan Goodfellow, Yoshua Bengio, and Aaron Courville.\n2016. Deep Learning, chapter 7.4. MIT Press.\nhttp://www.deeplearningbook.org.\nZhiheng Huang, Davis Liang, Peng Xu, and Bing Xi-\nang. 2020. Improve Transformer Models with Better\nRelative Position Embeddings. In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2020, pages 3327–3335.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The Efﬁcient Transformer. In\nProceedings of the 8th International Conference on\nLearning Representations (ICLR 2020).\nGuillaume Klein, Yoon Kim, Yuntian Deng, Jean Senel-\nlart, and Alexander Rush. 2017. OpenNMT: Open-\nSource Toolkit for Neural Machine Translation. In\nProceedings of ACL 2017, System Demonstrations\n(ACL 2017), pages 67–72.\nSameen Maruf, Fahimeh Saleh, and Gholamreza Haf-\nfari. 2021. A Survey on Document-Level Neural\nMachine Translation: Methods and Evaluation. ACM\nComputing Survey, 54(2).\nSharan Narang, Hyung Won Chung, Yi Tay, William\nFedus, Thibault Fevry, Michael Matena, Karishma\nMalkan, Noah Fiedel, Noam Shazeer, Zhenzhong\nLan, et al. 2021. Do Transformer Modiﬁcations\nTransfer Across Implementations and Applications?\narXiv preprint arXiv:2102.11972.\nMasato Neishi and Naoki Yoshinaga. 2019. On the Re-\nlation between Position Information and Sentence\nLength in Neural Machine Translation. In Proceed-\nings of the 23rd Conference on Computational Nat-\nural Language Learning (CoNLL 2019), pages 328–\n338.\nBenjamin Newman, John Hewitt, Percy Liang, and\nChristopher D. Manning. 2020. The EOS Decision\nand Length Extrapolation. In Proceedings of the\nThird BlackboxNLP Workshop on Analyzing and In-\nterpreting Neural Networks for NLP (BlackboxNLP\n2020), pages 276–291.\nNathan Ng, Kyra Yee, Alexei Baevski, Myle Ott,\nMichael Auli, and Sergey Edunov. 2019. Facebook\nFAIR’s WMT19 News Translation Task Submission.\nIn Proceedings of the Fourth Conference on Machine\nTranslation (WMT 2019), pages 314–319.\nYui Oka, Katsuki Chousa, Katsuhito Sudoh, and Satoshi\nNakamura. 2020. Incorporating Noisy Length Con-\nstraints into Transformer with Length-aware Posi-\ntional Encodings. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics\n(COLING 2020), pages 3580–3585.\nYui Oka, Katsuhito Sudoh, and Satoshi Nakamura. 2021.\nUsing Perturbed Length-aware Positional Encoding\nfor Non-autoregressive Neural Machine Translation.\narXiv preprint arXiv:2107.13689.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan,\nSam Gross, Nathan Ng, David Grangier, and Michael\nAuli. 2019. fairseq: A Fast, Extensible Toolkit for\nSequence Modeling. In Proceedings of the 2019\nConference of the North American Chapter of the\nAssociation for Computational Linguistics (Demon-\nstrations), pages 48–53.\nMyle Ott, Sergey Edunov, David Grangier, and Michael\nAuli. 2018. Scaling Neural Machine Translation. In\nProceedings of the Third Conference on Machine\nTranslation (WMT 2018), pages 1–9.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: a Method for Automatic\nEvaluation of Machine translation. In Proceedings\nof the 40th Annual Meeting of the Association for\nComputational Linguistics (ACL 2002), pages 311–\n318.\nMatt Post. 2018. A Call for Clarity in Reporting BLEU\nScores. In Proceedings of the Third Conference on\nMachine Translation: Research Papers (WMT 2018),\npages 186–191.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the Lim-\nits of Transfer Learning with a Uniﬁed Text-to-Text\nTransformer. Journal of Machine Learning Research\n(JMLR), 21(140):1–67.\n3315\nJan Rosendahl, Viet Anh Khoa Tran, Weiyue Wang, and\nHermann Ney. 2019. Analysis of Positional Encod-\nings for Neural Machine Translation. In Proceedings\nof 16th International Workshop on Spoken Language\nTranslation 2019 (IWSLT 2019).\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of the 54th\nAnnual Meeting of the Association for Computational\nLinguistics (ACL 2016), pages 1715–1725.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. 2018.\nSelf-Attention with Relative Position Representa-\ntions. In Proceedings of the 2018 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 2 (Short Papers) (NAACL 2018),\npages 464–468.\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJon Shlens, and Zbigniew Wojna. 2016. Rethink-\ning the Inception Architecture for Computer Vision.\nIn 2016 IEEE Conference on Computer Vision and\nPattern Recognition (CVPR 2016), pages 2818–2826.\nSho Takase and Naoaki Okazaki. 2019. Positional en-\ncoding to control output sequence length. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 3999–4004, Min-\nneapolis, Minnesota. Association for Computational\nLinguistics.\nYi Tay, Mostafa Dehghani, Dara Bahri, and Donald\nMetzler. 2020. Efﬁcient Transformers: A Survey.\narXiv preprint arXiv:2009.06732.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is All\nYou Need. In Advances in Neural Information Pro-\ncessing Systems 30 (NIPS 2017), pages 5998–6008.\nBenyou Wang, Lifeng Shang, Christina Lioma, Xin\nJiang, Hao Yang, Qun Liu, and Jakob Grue Simon-\nsen. 2021. On Position Embeddings in BERT. In\nProceedings of the 9th International Conference on\nLearning Representations (ICLR 2021).\nChuhan Wu, Fangzhao Wu, and Yongfeng Huang. 2021.\nDA-Transformer: Distance-aware Transformer. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies\n(NAACL 2021), pages 2059–2068.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, and Amr Ahmed. 2020. Big Bird: Trans-\nformers for Longer Sequences. In Advances in Neu-\nral Information Processing Systems 33 (NeurIPS\n2020), pages 17283–17297.\n3316\nA Summary of Datasets\nWe summarized the statistics, preprocessing, and\nevaluation metrics of datasets used in our experi-\nment in Table 3. The length statistics are in Fig-\nure 5.\nB Hyperparameters\nWe present the list of hyperparameters used in our\nexperiments in Table 4. Hyperparameters for train-\ning Transformer follow the recipe available in the\nofﬁcial documentation page of OpenNMT-py10.\nC Similarities of Representations\nIn Section 3.2, we presented Figure 2 to qual-\nitatively demonstrate that the representation of\nSHAPE is shift-invariant. We present ten addi-\ntional ﬁgures that we created from ten additional\ninstances in Figure 6. The characteristic of the ﬁg-\nures are consistent with that observed in Figure 2;\nthe representation of SHAPE is shift-invariant,\nwhereas the representation of APE is not.\nD Detailed BLEU Scores\nWe report the BLEU score on each of newstest2010-\n2016 in Table 51112 . In addition, we report the per-\nformance of APE, RPE, and SHAPE with respect\nto the source sequence lengths in Figure 7.\nE Learning Curve of Each Model\nWe present the learning curve of each model\n(APE, RPE, SHAPE) trained on different datasets\n(VANILLA , EXTRAPOLATE , INTERPOLATE ). Fig-\nures 8 and 9 present the validation perplexity\nagainst the number of gradient steps and wall clock,\nrespectively. From these ﬁgures, we made the fol-\nlowing observations:\nFirst, according to Figure 8, the speed of con-\nvergence is similar across the models in terms of\nthe number of gradient steps. In other words, in\nour experiment (Section 3), we never compare the\nmodels whose degree of convergence is different.\n10https://opennmt.net/OpenNMT-py/FAQ.\nhtml#how-do-i-use-the-transformer-model\n11SacreBLEU hash of VANILLA and EXTRAPOLATE is:\nBLEU+case.mixed+lang\n.en-de+numrefs.1+smooth.exp+\ntest.wmt{10,11,12,13,\n14/full,15,16}+tok.13a+version.1.5.0.\n12SacreBLEU hash of INTERPOLATE is\nBLEU+case.mixed+numrefs.1+smooth.exp+\ntok.13a+version.1.5.0.\nSecond, Figure 9 demonstrates that RPE re-\nquires more time to complete the training than\nAPE and SHAPE do. As explained in Section 2.2,\nRPE causes the computational overhead because it\nneeds to compute attention for relative position em-\nbeddings. The amount of time required to complete\nthe training is presented in Table 6.\nF Sanity Check of the Baseline\nPerformance\nBuilding a strong baseline is essential for trustable\nresults (Denkowski and Neubig, 2017). To con-\nﬁrm that our baseline model (i.e., Transformer with\nAPE) trained using OpenNMT-py (Klein et al.,\n2017) is strong enough, we compared its perfor-\nmance with that trained on Fairseq (Ott et al., 2019).\nFairseq is another state-of-the-art framework used\nby winning teams of WMT shared task (Ng et al.,\n2019). For training on Fairseq, we used the ofﬁcial\nrecipe available in the documentation13. The result\nis presented in Table 7. Here, the results are the\naverage of ﬁve distinct trials with different random\nseeds. From the table, we can conﬁrm that both\nmodels can achieve comparable results.\n13https://github.com/pytorch/fairseq/\ntree/master/examples/scaling_nmt\n3317\nDataset Name Training Data # of Sent.\nPairs in\nTraining\nData\nValidation Test Evaluation Metric\nVANILLA WMT 2016 English-German 4.5M newstest2010-\n2013\nnewsetst2014-\n2016\ndetokenized BLEU\nvia sacreBLEU\nEXTRAPOLATE WMT 2016 English-German.\nWe removed sequence pairs if\nthe length of the source or target\nsentence exceeds 50 subwords.\n3.9M newstest2010-\n2013\nnewsetst2014-\n2016\ndetokenized BLEU\nvia sacreBLEU\nINTERPOLATE WMT 2016 English-German.\nGiven neighboring ten sentence\nof VANILLA , i.e., X1,..., X10\nand Y1,..., Y10, we concate-\nnate each sentence with a spe-\ncial token ⟨sep⟩.\n450K newstest2010-\n2013. We concate-\nnated sentences as\nin training data.\nnewstest2014-\n2016. We concate-\nnated sentences as\nin training data.\ndetokenized BLEU\nvia sacreBLEU\nTable 3: Summary of statistics, preprocessing, and evaluation metric of datasets used in our experiment.\n0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-\nSource Sequence Length (tokens)\n0\n5\n10\n15\n20\n25\n30Percent\nTrain\nValid\nTest\n(a) VANILLA dataset\n0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-\nSource Sequence Length (tokens)\n0\n5\n10\n15\n20\n25\n30Percent\nTrain\nValid\nTest (b) EXTRAPOLATE dataset\n0-99 100-199 200-299 300-399 400-499 500-599 600-\nSource Sequence Length (tokens)\n0\n10\n20\n30\n40\n50\n60Percent\nTrain\nValid\nTest (c) INTERPOLATE dataset\nFigure 5: Distribution of source sequence length of each dataset.\nConﬁgurations Selected Value\nEncoder-Decoder Architecture transformer-base (Vaswani et al., 2017)\nOptimizer Adam ( β1 = 0.9,β2 = 0.98,ϵ = 1×10−8)\nLearning Rate Schedule “Noam” scheduler described in (Vaswani et al., 2017)\nWarmup Steps 8,000\nLearning Rate Scaling Factor† 2\nDropout 0.1\nGradient Clipping None\nBeam Search Width 4\nLabel Smoothing ϵls = 0.1 (Szegedy et al., 2016)\nMini-batch Size 112k tokens\nNumber of Gradient Steps 200,000\nAveraging Save checkpoint for every 5,000 steps and take an average of last 10 checkpoints\nMaximum Offset K(for SHAPE) We set K = 500for the most of the experiments. We manually tuned Kon validation\nBLEU for EXTRAPOLATE from following range: {10, 20, 30, 40, 100, 500}, and report\nthe score of K = 40 in addition to K = 500. We used a single random seed for the\ntuning.\nRelative Distance Limit (for RPE) 16 following (Neishi and Yoshinaga, 2019)\nGPU Hardware Used DGX-1 and DGX-2\nTable 4: List of hyperparameters. †: this corresponds to “learning rate” variable deﬁned in OpenNMT-py framework.\n3318\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0\n(a) Sequence ID: #1\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0 (b) Sequence ID: #2\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0\n(c) Sequence ID: #3\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0 (d) Sequence ID: #4\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0\n(e) Sequence ID: #5\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0 (f) Sequence ID: #6\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0\n(g) Sequence ID: #7\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0 (h) Sequence ID: #8\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0\n(i) Sequence ID: #9\n0\n100\n250\n500APE\nEmbeddingLayer 1 Layer 2 Layer 3 Layer 4 Layer 5 Layer 6\n0\n100\n250\n500\n0\n100\n250\n500SHAPE\nk\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0\n100\n250\n500\n0.8\n0.9\n1.0 (j) Sequence ID: #10\nFigure 6: Cosine similarities of encoder hidden states with different offsets k ∈{0, 100, 250, 500}. Only the\nrepresentation of SHAPE is invariant with k.\nModel 2010 2011 2012 2013 2014 2015 2016 Average Speed\nDataset: VANILLA\nAPE† 24.22 21.98 22.20 26.06 26.95 29.98 34.46 26.55 x1.00\nRPE† 24.29 22.05 22.22 26.13 27.00 30.00 34.61 26.61 x0.91\nSHAPE† 24.18 22.01 22.23 26.08 26.89 30.12 34.48 26.57 x1.01\nDataset: E XTRAPOLATE\nAPE 22.69 20.36 20.72 24.94 26.24 28.79 32.62 25.19 x1.00\nRPE 23.46 21.19 21.69 25.54 26.80 29.43 33.34 25.92 x0.91\nSHAPE 23.60 21.24 21.53 25.45 26.54 29.22 33.63 25.89 x0.99\nDataset: I NTERPOLATE ‡\nAPE 31.41 29.71 29.79 34.69 35.36 38.00 41.32 34.33 x1.00\nRPE∗ - - - - - - - - -\nSHAPE 32.71 30.77 30.96 35.54 35.72 39.18 42.37 35.32 x0.99\nTable 5: BLEU scores on newstest2010-2016. Average column shows the macro average of all newstests. †: the\nvalues are averages of ﬁve distinct trials with ﬁve different random seeds. ∗: not available as the implementation\nwas very slow. Speed is the relative speed to APE (larger is faster).\n3319\nModel Dataset Hardware Training Time (sec) Number of Parameters\nAPE V ANILLA DGX-1 97,073 61M\nRPE V ANILLA DGX-1 107,089 61M\nSHAPE V ANILLA DGX-1 96,439 61M\nAPE E XTRAPOLATE DGX-1 101,469 61M\nRPE E XTRAPOLATE DGX-1 111,246 61M\nSHAPE E XTRAPOLATE DGX-1 102,535 61M\nAPE I NTERPOLATE DGX-2 69,148 61M\nSHAPE I NTERPOLATE DGX-2 69,529 61M\nTable 6: Training time required to complete 200,000 gradient steps. RPE requires more time than APE and SHAPE\ndo. Figure 9 illustrates the corresponding learning curve.\nModel Implementation 2010 2011 2012 2013 2014 2015 2016 Average\nAPE Fairseq 24.24 22.10 22.40 26.38 27.11 29.58 34.34 26.59\nAPE OpenNMT-py 24.22 21.98 22.20 26.06 26.95 29.98 34.46 26.55\nTable 7: BLEU score on newstest2010-2016. We report average result of ﬁve distinct trials with different random\nseeds.\n3320\n0-9 10-19 20-29 30-39 40-49 50-59 60-69 70-\nSource Sequence Length (tokens)\n15\n20\n25\n30BLEU score\n(a) EXTRAPOLATE dataset\nAPE\nRPE\nSHAPE (K = 40)\nSHAPE (K = 500)\n0-99 100-199 200-299 300-399 400-499 500-599 600-\nSource Sequence Length (tokens)\n30\n40\n50BLEU score\n(b) INTERPOLATE dataset\nAPE\nSHAPE (K = 500)\nFigure 7: BLEU score on validation and test sets with\nrespect to the source sequence length. The gray color\nmeans no training data.\n3321\n0 50000 100000 150000 200000\nNumber of Gradient Steps\n4\n6\n8\n10\n12\n14Validation Perplexity\nAPE\nRPE\nSHAPE\n(a) VANILLA dataset\n0 50000 100000 150000 200000\nNumber of Gradient Steps\n4\n6\n8\n10\n12\n14Validation Perplexity\nAPE\nRPE\nSHAPE (b) EXTRAPOLATE dataset\n0 50000 100000 150000 200000\nNumber of Gradient Steps\n4\n6\n8\n10\n12\n14Validation Perplexity\nAPE\nSHAPE (c) INTERPOLATE dataset\nFigure 8: Learning curves for each position representation and dataset. We compare the speed of convergence in\nterms of number of gradient steps.\n0 20000 40000 60000 80000 100000\nWall Clock (Seconds)\n4\n6\n8\n10\n12\n14Validation Perplexity\nAPE\nRPE\nSHAPE\n(a) VANILLA dataset\n0 20000 40000 60000 80000 100000\nWall Clock (Seconds)\n4\n6\n8\n10\n12\n14Validation Perplexity\nAPE\nRPE\nSHAPE (b) EXTRAPOLATE dataset\n0 20000 40000 60000\nWall Clock (Seconds)\n4\n6\n8\n10\n12\n14Validation Perplexity\nAPE\nSHAPE (c) INTERPOLATE dataset\nFigure 9: Learning curves for each position representation and dataset. We compare the speed of convergence in\nterms of wall clock.",
  "topic": "Position (finance)",
  "concepts": [
    {
      "name": "Position (finance)",
      "score": 0.7354511022567749
    },
    {
      "name": "Embedding",
      "score": 0.7306351661682129
    },
    {
      "name": "Transformer",
      "score": 0.6756818294525146
    },
    {
      "name": "Computer science",
      "score": 0.535463273525238
    },
    {
      "name": "Representation (politics)",
      "score": 0.4863336682319641
    },
    {
      "name": "Generalization",
      "score": 0.48354244232177734
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4135719835758209
    },
    {
      "name": "Algorithm",
      "score": 0.3939092755317688
    },
    {
      "name": "Mathematics",
      "score": 0.34294557571411133
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3388800621032715
    },
    {
      "name": "Engineering",
      "score": 0.16966772079467773
    },
    {
      "name": "Mathematical analysis",
      "score": 0.1451663374900818
    },
    {
      "name": "Electrical engineering",
      "score": 0.08913359045982361
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210166566",
      "name": "Preferred Networks (Japan)",
      "country": "JP"
    },
    {
      "id": "https://openalex.org/I201537933",
      "name": "Tohoku University",
      "country": "JP"
    }
  ],
  "cited_by": 14
}