{
  "title": "SWEA: Updating Factual Knowledge in Large Language Models via Subject Word Embedding Altering",
  "url": "https://openalex.org/W4409348199",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A5100355078",
      "name": "Xiaopeng Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100376019",
      "name": "Shasha Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5010069764",
      "name": "Shezheng Song",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5080446056",
      "name": "Huijun Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5002881194",
      "name": "Bin Ji",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100442237",
      "name": "Xi Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100643102",
      "name": "Jun Ma",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100409840",
      "name": "Jie Yu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5031379854",
      "name": "Xiaodong Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100378420",
      "name": "Jing Wang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5100340717",
      "name": "Weimin Zhang",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2998363596",
    "https://openalex.org/W3154575616",
    "https://openalex.org/W3117576675",
    "https://openalex.org/W6839328737",
    "https://openalex.org/W6846320025",
    "https://openalex.org/W2757667583",
    "https://openalex.org/W2765939122",
    "https://openalex.org/W4294437885",
    "https://openalex.org/W4388585315",
    "https://openalex.org/W4377865177",
    "https://openalex.org/W4312048199",
    "https://openalex.org/W6851775633",
    "https://openalex.org/W6786251224"
  ],
  "abstract": "The general capabilities of large language models (LLMs) make them the infrastructure for various AI applications, but updating their inner knowledge requires significant resources. Recent model editing is a promising technique for efficiently updating a small amount of knowledge of LLMs and has attracted much attention. In particular, local editing methods, which directly update model parameters, are proven suitable for updating small amounts of knowledge. Local editing methods update weights by computing least squares closed-form solutions and identify edited knowledge by vector-level matching in inference, which achieve promising results. However, these methods still require a lot of time and resources to complete the computation. Moreover, vector-level matching lacks reliability, and such updates disrupt the original organization of the model's parameters. To address these issues, we propose a detachable and expandable Subject Word Embedding Altering (SWEA) framework, which finds the editing embeddings through token-level matching and adds them to the subject word embeddings in Transformer input. To get these editing embeddings, we propose optimizing then suppressing fusion method, which first optimizes learnable embedding vectors for the editing target and then suppresses the Knowledge Embedding Dimensions (KEDs) to obtain final editing embeddings. We thus propose SWEAOS method for editing factual knowledge in LLMs. We demonstrate the overall state-of-the-art (SOTA) performance of SWEAOS on the CounterFact and zsRE datasets. To further validate the reasoning ability of SWEAOS in editing knowledge, we evaluate it on the more complex RippleEdits benchmark. The results demonstrate that SWEAOS possesses SOTA reasoning ability.",
  "full_text": "SWEA: Updating Factual Knowledge in Large Language Models via Subject\nWord Embedding Altering\nXiaopeng Li, Shasha Li∗, Shezheng Song, Huijun Liu, Bin Ji, Xi Wang, Jun Ma*, Jie Yu∗,\nXiaodong Liu∗, Jing Wang, Weimin Zhang\nNational University of Defense Technology\nChangsha, Hunan 410073 China\n{xiaopengli, shashali, ssz614, jibin, liuhuijun, wx 23ndt, yj, majun, liuxiaodong, wangjing}@nudt.edu.cn,\nwmzhang104@139.com\nAbstract\nThe general capabilities of large language models (LLMs)\nmake them the infrastructure for various AI applications,\nbut updating their inner knowledge requires significant re-\nsources. Recent model editing is a promising technique for\nefficiently updating a small amount of knowledge of LLMs\nand has attracted much attention. In particular, local edit-\ning methods, which directly update model parameters, are\nproven suitable for updating small amounts of knowledge.\nLocal editing methods update weights by computing least\nsquares closed-form solutions and identify edited knowl-\nedge by vector-level matching in inference, which achieve\npromising results. However, these methods still require a lot\nof time and resources to complete the computation. More-\nover, vector-level matching lacks reliability, and such updates\ndisrupt the original organization of the model’s parameters.\nTo address these issues, we propose a detachable and ex-\npandable Subject Word Embedding Altering (SWEA) frame-\nwork, which finds the editing embeddings through token-\nlevel matching and adds them to the subject word embeddings\nin Transformer input. To get these editing embeddings, we\npropose optimizing then suppressing fusion method, which\nfirst optimizes learnable embedding vectors for the editing\ntarget and then suppresses the Knowledge Embedding Di-\nmensions (KEDs) to obtain final editing embeddings. We\nthus propose SWEA⊕OS method for editing factual knowl-\nedge in LLMs. We demonstrate the overall state-of-the-art\n(SOTA) performance of SWEA⊕OS on the C OUNTER FACT\nand zsRE datasets. To further validate the reasoning ability of\nSWEA⊕OS in editing knowledge, we evaluate it on the more\ncomplex RIPPLE EDITS benchmark. The results demonstrate\nthat SWEA⊕OS possesses SOTA reasoning ability.\n1 Introduction\nLarge language models (LLMs), with their rich reserve of\npre-trained knowledge, play a pivotal role in the current AI\nlandscape (Li et al. 2023a; Zhao et al. 2023). The knowl-\nedge pre-trained in LLMs is solidified in their parameters,\nmeaning that any outdated or incorrect knowledge within\nthe LLMs can only be updated through parameter updates.\nHowever, given that the training of LLMs relies heavily on\nGPUs and consumes a significant amount of electricity, re-\ntraining to update even small amounts of information can\n*Corresponding Author.\nCopyright © 2025, Association for the Advancement of Artificial\nIntelligence (www.aaai.org). All rights reserved.\nbe costly. Consequently, researchers have started to explore\nmodel editing methods (Yao et al. 2023; Meng et al. 2022a;\nMitchell et al. 2022; Zhang et al. 2024; Wang et al. 2024;\nTian et al. 2024) aiming to update a small amount of knowl-\nedge of LLMs more efficiently.\nThe purpose of model editing is to insert, update, and\ndelete target knowledge while avoiding editing non-target\nknowledge to preserve the original capabilities of LLMs.\nCurrent editing methods mainly edit model through three\napproaches (Wang et al. 2023): adding additional modules\n(Huang et al. 2023; Dong et al. 2022; Hartvigsen et al. 2022),\nglobal optimization (Zhu et al. 2020; Mitchell et al. 2022),\nand local editing (Meng et al. 2022a,b; Li et al. 2023b).\nMethods of adding additional modules involves incorpo-\nrating adapters within or external to the LLMs for storing\nedited instances, which increases the inference load. In con-\ntrast, global optimization and local editing methods write the\nediting information into the model weights, maintaining the\nsame inference cost as the original model. However, using\nglobal optimization methods for model editing is prone to\noverfitting (Meng et al. 2022b) because model editing often\nonly requires updating a small amount of knowledge. Local\nediting methods view the model editing as a least squares\nproblem, which is more suitable for updating a small amount\nof knowledge. Therefore, in this paper, we focus on local\nediting methods.\nLocal editing methods first select the critical layers that\nstore knowledge (Meng et al. 2022a), optimize the knowl-\nedge representation with the editing knowledge as the objec-\ntive, then calculate the keys of the editing knowledge and the\noriginal knowledge, and finally update the weights of criti-\ncal layers by solving the least squares problem. These meth-\nods have achieved remarkable results in model editing tasks.\nHowever, they still exsit three issues:(1) Lack of efficiency:\nthese methods need to spend a lot of time and resources to\ncompute all the vectors needed to solve the least squares\nproblem (Meng et al. 2022a,b; Li et al. 2023b); (2) Lack\nof reliability: in local editing methods, we observe that even\nwhen all the target knowledge representations are already\naligned with the editing goal, their editing success rate is\nstill far from expectation. Meanwhile, LLMs edited by these\nmethods are prone to misidentifying unedited knowledge as\nedited knowledge, reducing the usability of edited LLMs.\nThis might be due to the fact that using vector-level match-\nThe Thirty-Ninth AAAI Conference on Artificial Intelligence (AAAI-25)\n24494\nInput\nEmbedding\nMasked\nMulti-Head\nAttention\nAdd & Norm\nFeed Forwad\nInput\nAdd & Norm\nBlock k\nBlock k+1..N\nLinear\nSoftmax\nOutput\nPositional \nEncoding\nEdit\nBlock 1..k-1\nVector-level\nmatching\nAlter\nROME, MEMIT, PMET\nSWEA     OS (ours)\nCurrently, Nvidia's most ...\nToken-level\nmatching\nEdit\nFigure 1: Difference between our method and existing lo-\ncal editing methods. Our method focuses on altering the\nword embedding for the input via token-level matching,\nwhile existing local editing methods edit Feed Forward Net-\nwork (FFN) and identify editing knowledge by vector-level\nmatching. Mismatching is more likely to occur in vector-\nlevel matching, which leads to erroneous recognition of edit-\ning knowledge.\ning to identify the editing knowledge in the updated weights\nis not entirely reliable, since vector-level matching struggles\nto distinguish between two very similar vectors (Gionis et al.\n1999; Zhang et al. 2023); (3) Lack of protection: due to\nthe high complexity and incomplete transparency of LLMs\nthemselves, exactly updating their weights perfectly by solv-\ning the least squares problem is challenging. Consequently,\nthe original organization of the edited model’s parameters\nis disrupted (Li et al. 2023c), thereby affecting the general\napplications of LLMs (Gu et al. 2024).\nIn view of these issues, (1) we propose a novel model\nediting method, SWEA⊕OS, which alters subject word em-\nbedding by adding it with editing embeddings obtained\nby Ooptimizing then Suppressing (OS) fusion method\nin Subject Word Embedding Altering (SWEA) frame-\nwork. SWEA⊕OS only requires computing editing embed-\ndings, therefore it is more efficient. The difference between\nSWEA⊕OS and existing local editing methods is illustrated\nin Figure 1. (2) The SWEA framework identifies editing\nknowledge instances through token-level matching that is\nmore reliable than vector-level matching because it is sensi-\ntive to even single-character changes. The OS fusion method\nget the editing embeddings through: a) optimizing learn-\nable embedding vectors to achieve editing objectives, b)\nsuppressing the subject’s Knowledge Embedding Dimen-\nsions (KEDs) which are special dimensions related to spe-\ncific knowledge in word embeddings. The suppressing step\nis designed to mitigate the influence of the subject’s KEDs\non the expression of new knowledge. (3) Unlike local edit-\ning methods that directly modify weights, the SWEA frame-\nwork is detachable and embedded into the embedding layer\nof LLMs, which protects the original weights of LLMs. It\nis also expandable, which can be combined with different\nfusion methods for model editing. In addition, the SWEA\nframework edits knowledge by altering the subject word em-\nbedding, which ensures the same inference load as the orig-\ninal model.\nWe demonstrate our method is both efficient and effective\nin GPT-J (6B) (Wang and Komatsuzaki 2021) and Llama-\n2 (7B) (Touvron et al. 2023) across two datasets and one\nbenchmark. In detail, comparative experiments on GPT-J\nand Llama-2 show that the SWEA⊕OS method demon-\nstrates overall SOTA performance. On the C OUNTER FACT\ndataset, SWEA⊕OS increases the Score by 5.8% on GPT-J\nand 7.7% on Llama-2 compared to the most advanced base-\nline. The SWEA⊕OS method also shows the best reasoning\nperformance on the RippleEdits benchmark (Cohen et al.\n2023), indicating that the knowledge edited by the SWEA\nOS method has stronger consistency.\nOur contributions to the work are as follows:\n• We propose a detachable and expandable SWEA frame-\nwork, which can be combined with different fusion meth-\nods for model editing and ensures the same inference cost\nas the original model.\n• We introduce the OS fusion method. It optimizes learn-\nable embedding vectors for editing targets and then sup-\npresses KEDs of subject to alleviate the impact of KEDs\nof subject word embeddings on editing effects.\n• Combing the OS fusion method with SWEA, we pro-\npose SWEA⊕OS for editing factual knowledge in LLMs.\nWe demonstrate the overall superior performance of\nSWEA⊕OS on C OUNTER FACT and zsRE datasets and\na more complex RIPPLE EDITS benchmark.\n2 Related Work\n2.1 Model Editing\nModel editing is currently an emerging research hotspot,\nwith various model editing methods and benchmarks being\nproposed successively (Yao et al. 2023; Zhang et al. 2024;\nWang et al. 2023; Deng et al. 2024; Wang et al. 2024; Li\net al. 2024a). The model editing task was first proposed in\n(Zhu et al. 2020), where they proposed a constrained fine-\ntuning method for this task, which imposes a constraint on\nthe fine-tuning weights to reduce interference with original\nknowledge. Unlike constrained fine-tuning, recent methods\nutilize meta-learning to update weights (De Cao, Aziz, and\nTitov 2021; Mitchell et al. 2022; Tan, Zhang, and Fu 2023).\nThese methods train a hypernetwork that indirectly updates\nweights using global gradient information. However, since\nthe model editing task aims to correct a small portion of\nerrors within the model’s internal memories, the data for\nmodel editing is usually few, making methods that update\n24495\n...\nStage 1. Fusion\n...\nMinus\nStep1: Optimization\nOptimized Embedding Vector\n...Nvidia\n...KEDs\nStep2: Knowledge attribution\nNew Knowledge: Currently, Nvidia's most advanced GPU is the B200.\nEditing EmbeddingsStep3:\nFusion\nOld Knowledge: Currently, Nvidia 's most advanced GPU is the H100.\nTransformer\n...\nQuery: Currently, Nvidia 's most advanced GPU is the __\n...\nEmbeddingsEditing Embeddings\nStage 2. Inference\nOutput: Currently, Nvidia 's most advanced GPU is the B200.\nFigure 2: Overview of SWEA⊕OS. In fusion stage, we first optimize a learnable embedding vector for target knowledge\n“Currently, Nvidia’s most advanced GPU is the B200 .” Second, using knowledge attribution method, we find the KEDs of\n‘Nvidia’ regarding “its most advanced GPU”. Finally, we fuse the optimized embedding vector with these KEDs subtracted to\nobtain the editing embeddings. In inference stage, we add these editing embeddings to the embedding of the subject ‘Nvidia’\nfor inference.\nweights using global gradients prone to overfitting. Some\nmethods also add additional modules to perform model edit-\ning. They usually add a smaller model outside LLMs (Huang\net al. 2023), embed an adapter within LLMs (Hartvigsen\net al. 2022), or add editing information to the input for model\nediting. But these methods not only increase the inference\nburden but also add to the complexity of the system.\nIn contrast, local editing methods, from the perspective of\ninterpretability, directly update the Feed Forward Network\n(FFN) of key-value memories (Geva et al. 2021) using a\nclosed-form solution of least squares (Meng et al. 2022a,b;\nLi et al. 2023b), which is less prone to overfitting and more\nlightweight. However, these approaches still require a lot\nof time and resources to update weights, the vector-level\nmatching in model weights is not always reliable, and recent\nworks find that these approaches can cause irreversible dam-\nage to the model’s generalization capability due to the updat-\ning of model weights (Gu et al. 2024). Unlike existing local\nediting methods, SWEA⊕OS alters the subject word em-\nbeddings in the input through token-level matching through\nediting embeddings obtained by OS fusion method, making\nit more efficient and reliable.\n2.2 Explanation of Word Embedding\nWord embedding is a fundamental component in LLMs for\nprocessing natural language, where each token typically cor-\nresponds to a high-dimensional dense vector. Researchers\nhave sought to understand the interpretable concepts asso-\nciated with these dense vectors by categorizing the dimen-\nsions into specific concepts (S ¸enel et al. 2018; Balogh et al.\n2020) or by projecting word embeddings into more inter-\npretable vector spaces (Park, Bak, and Oh 2017; Simhi and\nMarkovitch 2023). (S ¸enel et al. 2018) introduced the SEM-\nCAT dataset and used statistical methods to classify differ-\nent dimensions of word embedding into 110 semantic cat-\negories. (Balogh et al. 2020) assigned common-sense con-\ncepts to each dimension of word embedding. (Simhi and\nMarkovitch 2023) mapped word embedding to a concept\nspace understandable by humans using vector similarity.\nThese works enhance our understanding of word embedding\nin terms of semantic concepts. However, they did not discuss\nthe relationship between the dimensions of word embedding\nand factual knowledge. In contrast to their methods, we use\nknowledge attribution methods to identify the correspond-\ning knowledge embedding dimensions (KEDs) for subject-\nspecific facts in word embedding dimensions and suppress\nthese KEDs to improve editing effects.\n3 Preliminaries\n3.1 Language Modeling\nLLMs process discrete text by firstly embedding it into con-\ntinuous vectors. After processing by Transformers, a proba-\nbility distribution on the vocabulary is finally obtained under\nthe action of the Softmax function. Formally, a discrete text\nT is first converted into token ids T = tok(T ) by the tok-\nenizer. Next, the embedding layerE of the LLMs maps each\ntoken id of T to a vector x ∈ R1×h, where h is the dimen-\nsion size of the Transformer’s hidden layer. Assuming the\nlength of the token ids is l, then E(T) =X ∈ Rl×h where\nX = [x1, x2, ..., xl] is the continuous vector of text T . Then\nthe Transformers of the LLMs process X layer by layer, fi-\nnally obtaining a probability distribution on the vocab:\nP(X) =Softmax(Transformers(X)) (1)\n24496\n3.2 Model Editing Task\nPrior work expresses factual knowledge as a triple (s, r, o)\n(Meng et al. 2022b), where s denotes the subject, r denotes\nthe relation, and o denotes the object. The purpose of model\nediting is:\n(s, r, o)→ (s, r, o′) (2)\nwhere o′ is another object different from o. At the same\ntime, model editing should protect other knowledge not be-\ning changed. For convenience, we express the factual knowl-\nedge with a pair (T , Y), where T is a sentence composed of\ns and r, and Y is o which is the continuation of the above\nsentence. Then the model editing task can be formally ex-\npressed as:\n(T , Y) → (T , Y′) (3)\nBatch Editing means editing n >1 factual knowledge at\nthe same time during a single run of the editing method:\nnX\n1\n(T , Y) →\nnX\n1\n(T , Y′) (4)\nSequential Editing means carrying out multiple consec-\nutive edits on a single model:\n(T , Y) → (T , Y′) → ... → (T , Y∗) (5)\nSequential Batch Editing means performing multiple\nconsecutive batch edits on a single model:\nnX\n1\n(T , Y) →\nnX\n1\n(T , Y′) → ... →\nnX\n1\n(T , Y∗) (6)\n4 Methodology\nIn this section, we explain what is the SWEA framework\nand how our proposed SWEA⊕OS method is used to up-\ndate the factual knowledge of LLMs. SWEA⊕OS consists\nof two stages: (1) Fusion: we use the OS fusion method to\ncompute the editing embeddings needed to update the fac-\ntual knowledge for the subject; (2) Inference: in the SWEA\nframework, the input embedding is altered with the matched\nediting embeddings to obtain the final input embeddings. We\ndetail these two stages in the subsections below, an overview\nof SWEA⊕OS is shown in Figure 2.\n4.1 Optimizing then Suppressing Fusion Method\nWord embeddings are dense continuous vectors (Zhao et al.\n2023). Some works show that their different dimensions\ncontain specific information (Li, Monroe, and Jurafsky\n2016; S ¸enel et al. 2018). Motivated by these, we assume\nthat certain dimensions of word embeddings of a subject\ncorrespond to specific factual knowledge about the subject\nin LLMs. For convenience, we name these dimensions as\nknowledge embedding dimensions (KEDs). For example,\nthe dimensions (26, 123, 336, 1024) of the word embedding\nof the subject “Nvidia” are KEDs that correspond to the fac-\ntual knowledge “Nvidia was founded by Jensen Huang.” Un-\nder this assumption, we aim to alter KEDs of the subject to\ncontrol the factual knowledge about the subject in LLMs.\nDue to word embeddings not being fully explained, di-\nrectly altering KEDs to update factual knowledge is very\ndifficult. We propose appending learnable embedding vec-\ntors to the subject’s word embeddings and optimizing these\nvectors to get optimized embeddings related to the editing\ntarget. During inference, simply adding the optimized em-\nbedding vectors to the subject’s word embeddings can up-\ndate factual knowledge. However, since the KEDs of the\nsubject’s word embeddings corresponding to factual knowl-\nedge still work, this may affect the knowledge expression\nof the optimized embedding vectors, leading to a decrease\nin editing effects. We thus suppress the KEDs of the origi-\nnal subject’s word embeddings. Therefore, we propose the\noptimizing then suppressing fusion method, which first op-\ntimizes learnable embedding vectors to achieve editing ob-\njectives, then suppresses the KEDs of the original subject’s\nword embeddings.\nFormally, supposeX is the text embedding ofT ; y and y′\nare all tokens ofY and Y′ respectively. To change the factual\nknowledge of the model from(T , Y) to (T , Y′), inspired by\nprevious work (Meng et al. 2022b; Li et al. 2023b), we add\nlearnable embedding vectors e to the representation of the\nsubject S in X to get ˆX, and use the following loss function\nto optimize and maximize the probability of y′:\nL(e) =αDKL\n\u0010\nP[y | X] ∥P[y | ˆX]\n\u0011\n+\nβ\nP\nPX\nj=1\n−log P\nh\ny′ | prefj ⊕ ˆX\ni (7)\nHere DKL is the KL divergence used to constrain the\nprobability distribution after adding the learnable embed-\nding vector e; to enhance the generalization of the learn-\nable embedding vectorse, we prependP prefixes (i.e., prefj)\ngenerated by the model to ˆX, where ⊕ indicates the concate-\nnation operation; α and β are two hyperparameters used to\nregulate the strength between preserving original knowledge\nand learning new knowledge during the optimization.\nWe use the knowledge attribution method (Dai et al. 2021)\nto find the KEDs of subject S. Let xz represent any one em-\nbedding vector in xS = [xS\ns , ..., xS\ne ] ∈ R|S|×h, the knowl-\nedge attribution of the embedding can be formally expressed\nas:\nAttr(xz) =x∗\nz\nn\nnX\nk=1\n∂P(y | X, xz := k\nnx∗\nz)\n∂xz\n(8)\nHere, x∗\nz represents the original value of the embedding\nvector; n is the number of steps for the Riemann integra-\ntion, and we follow (Dai et al. 2021) and set n = 20;\nP(y | X, xz := k\nnx∗\nz) represents the probability of the model\ngenerating y after replacing xz with k\nnx∗\nz. After obtaining\nthe attribution scores of all embedding dimensions of the\nsubject S, we retain those embedding dimensions that ex-\nceed t times the maximum attribution score as the KEDs\nKD. Finally, we subtract γ times the value of the original\nembedding vectors xS corresponding to KD from the opti-\nmized embedding vector e to obtain the final editing embed-\ndings eS:\neS = e − γO\\KD ⊙ xS (9)\n24497\nwhere O\\KD represents a vector with all positions as 0 ex-\ncept for the positions included inKD which are 1;⊙ denotes\nelement-wise multiplication.\n4.2 Subject Word Embedding Altering\nFramework\nSubject Word Embedding Altering (SWEA) framework\nmerges the editing embeddings eS = [eS\ns , ..., eS\ne ] ∈ R|S|×h\nabout the subject S with the subject embedding xS =\n[xS\ns , ..., xS\ne ] ∈ R|S|×h from the input text embedding X ∈\nRl×h. Here, |S| is the token length of the subject, and xS\ns\nand xS\ne represent the first and last token of the subject in the\ninput X, respectively. Therefore, in SWEA, the final input\nused by the model for inference is:\nX = [x0, ..., xS\ns + eS\ns , ..., xS\ne + eS\ne , ..., xl] (10)\nGiven that each subject’s token ids are unique, we use\nthese token ids as keys to index the editing embeddings.\nSpecifically, after obtaining the editing embeddingeS of the\nsubject S, we cacheeS using the token ids as key. ForS with\nonly one token id, we use this id as the key directly, and for\nS with multiple token ids, we concatenate the token ids of\nS using ’\n’. For example, the token ids of the subject ‘San\nFrancisco’ are [2986, 6033], so its key is ‘2986 6033’. For\nconvenience, we currently adopt the file caching method,\nwhich can be easily extended to a vector database. SWEA\ncan easily implement batch editing, it can obtaineS for mul-\ntiple subjects and then cache theseeS in editing embeddings\ncollection E. SWEA can also implement sequential editing\nand sequential batch editing. It caches past editing requests\nand recomputing eS for the subjects when the editing re-\nquests are updated. During the inference stage, we carry out\nthe longest continuous matching for the continuous combi-\nnation of the token ids of each input and the keys in E, and\nadd successful matched caches to matched tokens’s embed-\ndings using (10). The token-level matching and embedding\naltering algorithm of the above process can be found in Ap-\npendix A (See (Li et al. 2024b)). Note that some subjects\nmay have aliases. Currently, we are primarily focused on in-\ntroducing a new way for model editing, so SWEA currently\nonly considers cases where the subject is unique. However,\nthe SWEA framework can easily be adapted to include an\nalias list for each subject to identify them.\n5 Experiments\n5.1 Experimental Setup\nDatasets and Large Language Models We conducted\nedits on GPT-J (6B) (Wang and Komatsuzaki 2021) and\nLlama-2 (7B) (Touvron et al. 2023) on two datasets, COUN -\nTER FACT (Meng et al. 2022a), zsRE (De Cao, Aziz, and\nTitov 2021; Mitchell et al. 2022) and the R IPPLE ED-\nITS benchmark (Cohen et al. 2023). All metrics of the\nabove datasets and benchmark are described in Appendix B.\nCOUNTER FACT dataset is a completion task, which contains\na total of 21,919 counterfactual editing instances. MEMIT\n(Meng et al. 2022b) filtered out the counter-logical fact edit-\ning in this dataset. To ensure the same experimental set-\nting, we also only use the filtered 20,877 editing instances.\nScoreEff\nica\ncy\nGener\nalization\nSpec\nificity\nFluency\nConsistency\n0\n50\n100\nSWEA⊕OS\nScoreEff\nica\ncy\nGener\nalizat\nion\nSpec\nificityFluency\nConsistency\n0\n50\n100\nMEMIT\nScoreEfficacy\nGe\nner\naliza\ntion\nSpec\nificityFluency\nConsistenc\ny\n0\n50\n100\nPMET\nScoreEfficacy\nGener\nalization\nSpec\nificityFluen\ncy\nConsiste\nncy\n0\n50\n100\nROME\nbatch 1 batch 10 batch 100 batch 1000\nFigure 3: Results of sequential batch editing of SWEA⊕OS,\nPMET, MEMIT, and ROME. To better display the results,\nwe divide the fluency by the original fluency (i.e., 622.4)\nand then multiply by 100 to make it fall between 0 and 100.\nzsRE dataset is a QA task, for which we use 10,000 edit-\ning instances extracted from (Meng et al. 2022a) to con-\nduct editing. RippleEdits is a benchmark for testing the\nmulti-hop reasoning ability of post-edit models, including\nRECENT, RANDOM and POPULAR subsets. RECENT\nmainly evaluates the ability of the model’s editing method\nto insert knowledge, while the latter two mainly evaluate the\nability to edit knowledge. Since we currently only focus on\nupdating the knowledge of the model, we only use the two\nsubsets of rippleEdits, RANDOM and POPULAR.\nBaselines We compared SWEA⊕OS with the global op-\ntimization method Constrained Fine-Tuning (FT+W) (Zhu\net al. 2020), MEND (Mitchell et al. 2022), MALMEN (Tan,\nZhang, and Fu 2024), adding additional modules method\nGRACE (Hartvigsen et al. 2022), and the local editing meth-\nods ROME (Meng et al. 2022a), MEMIT (Meng et al.\n2022b), PMET (Li et al. 2023b) on the COUNTER FACT and\nzsRE datasets. On the RANDOM and POPULAR sub-\nsets of rippleEdits, we compared with local editing methods\nROME, MEMIT. Experimental details can be found in Ap-\npendix C.\n5.2 Experiments on C OUNTER FACT and zsRE\nDatasets\nWe first test the batch editing performance on the C OUN -\nTER FACT and zsRE datasets. We then test the scaling-up\nediting performance on the C OUNTER FACT dataset. Con-\nsidering that sequential editing is a subset of sequential\nbatch editing, we perform sequential batch editing directly\n24498\nEditor Score Efficacy Generalization Specificity Fluency Consistency\nGPT-J 22.4 15.2 (0.7) 17.7 (0.6) 83.5 (0.5) 622.4 (0.3) 29.4 (0.2)\nFT-W 67.6 99.4 (0.1) 77.0 (0.7) 46.9 (0.6) 293.9 (2.4) 15.9 (0.3)\nMEND 23.1 15.7 (0.7) 18.5 (0.7) 83.0 (0.5) 618.4 (0.3) 31.1 (0.2)\nROME 50.3 50.2 (1.0) 50.4 (0.8) 50.2 (0.6) 589.6 (0.5) 3.3 (0.0)\nMEMIT 85.8 98.9 (0.2) 88.6 (0.5) 73.7 (0.5) 619.9\n(0.3) 40.1 (0.2)\nGRACE 26.7 30.6 (0.9) 17.3 (0.6) 83.0 (0.5) 618.1 (0.3) 29.3 (0.2)\nPMET 86.2 99.5 (0.1) 92.8 (0.4) 71.4 (0.5) 620.0 (0.3) 40.6 (0.2)\nSWEA⊕OS 91.2 99.6 (0.1) 98.3 (0.2) 79.0 (0.5) 609.5 (0.7) 42.3 (0.2)\nLlama-2 20.5 14.8 (0.7) 15.0 (0.6) 82.4 (0.5) 604.3 (0.3) 25.4 (0.2)\nFT-W 65.4 99.8 (0.1) 84.9 (0.6) 41.5 (0.7) 546.9 (0.2) 20.0 (0.1)\nROME 50.5 51.3 (1.0) 50.0 (0.8) 50.2 (0.6) 488.1 (0.2) 2.6 (0.0)\nMEMIT 69.6 81.5 (0.8) 55.4 (0.8) 78.3 (0.5) 602.9 (0.2) 27.8 (0.2)\nGRACE 29.2 29.8 (0.9) 15.0 (0.6) 82.2 (0.5) 605.2 (0.3) 25.3 (0.2)\nPMET 83.2 97.1 (0.3) 87.8 (0.5) 69.5 (0.6) 599.4 (0.3) 34.7 (0.2)\nSWEA⊕OS 89.6 98.4 (0.2) 93.5 (0.4) 79.3 (0.5) 600.5 (0.5) 35.0 (0.2)\nTable 1: Results of 10,000 edits on GPT-J and Llama-2 on the C OUNTER FACT dataset. Within the parentheses is the 95%\nconfidence interval.\nEditor Score Efficacy\nGeneralization Specificity\nGPT-J 26.0 26.4\n(±0.6) 25.3 (±0.5) 26.8 (±0.5)\nFT-W 14.3 57.9\n(±0.7) 56.8 (±0.7) 5.7 (±0.5)\nMEND 20.0 19.4 (±0.5) 18.6 (±0.5) 22.4 (±0.5)\nMALMEN 37.3 76.1(±0.7) 72.3(±0.7) 18.6(±0.4)\nROME 1.1 9.2 (±0.8) 7.9 (±0.8) 0.4 (±0.2)\nMEMIT 50.2\n92.7 (±0.3) 86.7 (±0.5) 26.7 (±0.5)\nGRACE 31.3 47.8\n(±0.6) 26.5 (±0.5) 26.8 (±0.5)\nPMET 47.6 86.4 (±0.4) 81.5 (±0.5) 25.5 (±0.3)\nSWEA⊕OS 51.0 96.0 (±0.3) 89.7 (±0.2) 26.8 (±0.2)\nLlama-2 11.9 11.5( ±\n0.3) 11.1 (±0.3) 13.3 (±0.2)\nFT-W 11.7 13.8\n(±0.6) 13.1 (±0.5) 9.2 (±0.4)\nROME 4.3 3.9 (±0.8) 3.7 (±0.8) 5.8 (±0.3)\nMEMIT 23.1 45.6 (±0.4) 40.9 (±0.5) 12.0 (±0.5)\nGRACE 14.9 23.7 (±0.4) 11.8 (±0.6) 13.3 (±0.5)\nPMET 23.9\n48.1 (±0.3) 45.0 (±0.4) 12.1 (±0.3)\nSWEA⊕OS 25.5 50.7 (±0.4)\n44.0 (±0.3) 13.3 (±0.3)\nTable 2: Results of 10,000 edits on GPT-J and Llama-2\non the zsRE dataset. To ensure a fair comparison, we re-\nproduced baselines except MEND (non-reproducible) under\nthis setting.\non the COUNTER FACT dataset. We also compare the execu-\ntion time of the SWEA⊕OS method with that of baselines\nand analysis the inference latency introduced by the SWEA\nframework in Appendix D.\nBatch Editing Results The results of editing GPT-J and\nLlama-2 on the C OUNTER FACT and zsRE datasets are pre-\nsented in Tables 1 and 2, respectively. SWEA⊕OS achieved\nthe overall best results. Whether on the C OUNTER FACT\nor zsRE datasets, Efficacy, Generalization, Specificity, and\nConsistency of SWEA⊕OS shows substantial improvement\nover previous model editing methods. This indicates that\nSWEA⊕OS is a very effective editing method. FT-W has\na very high editing success rate, but the generalizability of\nthe edited knowledge is poor, and the model’s generative ca-\npability is severely compromised due to overfitting. When\nediting the GPT-J model, although MEND and GRACE ex-\nhibit the best specificity, their poor generalization affects\ntheir overall performance. When editing the Llama-2 model,\nGRACE shows similar results. Overall, we have demon-\nstrated through this set of experiments that SWEA⊕OS is a\nhighly effective model editing method. In addition, we show\nthe results of SWEA⊕OS and baselines performing 1, 10,\n100, 1000, and 10,000 edits respectively on the C OUNTER -\nFACT dataset in Appendix E.2. Additionally, in Appendix\nE.3, we present the qualitative results of the model generat-\ning facts after being edited on the COUNTER FACT dataset.\nSequential Batch Editing Results We use SWEA⊕OS,\nPMET, MEMIT, and ROME to perform sequential batch\nediting on the GPT-J model in the C OUNTER FACT dataset.\nThe number of sequences are 100, 20, 5, 2 with correspond-\ning batch sizes of 1, 10, 100, 1000, respectively. The results\nare shown in Figure 3, indicating that SWEA⊕OS performs\nthe most stable performance in sequential batch editing. The\nperformance of SWEA⊕OS in sequential batch editing only\nshow a slight decline as the batch size increased. When the\nediting batch is 1 and 1000, the scores of SWEA are 93.22\nand 93.01, respectively. In contrast, the performances of\nPMET, MEMIT, and ROME are very unstable. The score of\nPMET and ROME decreased by 43.08% and 42.74% from\nan editing batch 1 to an editing batch 1000, respectively.\n5.3 Experiments on R IPPLE EDITS\nSince RIPPLE EDITS tests the model’s ability to reason using\nedited knowledge, we first need to ensure that the model it-\nself has the corresponding reasoning ability. Essentially, we\nneed to edit the facts known to the model. To ensure this,\neach LLMs dataset needs to be filtered before RIPPLE EDITS\ntesting. We followed the filtering steps of RIPPLE EDITS , fi-\n24499\nDataset Editor LG CI CII SA RS Avg. LG CI CII SA RS Avg.\nGPT-J Llama-2\nRANDOM\nROME 0.58 0.52 0.24 1.0 0.44 0.56 0.57 0.41 0.29 1.0 0.52 0.56\nMEMIT 0.60 0.47 0.25 0.84 0.48 0.53 0.67 0.37 0.33 0.89 0.67 0.59\nPMET 0.70 0.46 0.26 0.88 0.34 0.53 0.62 0.47 0.18 1.0 0.49 0.55\nSWEA⊕OS 0.62 0.54 0.63 1.0 0.41 0.64 0.60 0.49 0.37 1.0 0.55 0.60\nPOPULAR\nROME 0.30 0.53 0.28 0.86 0.30 0.45 0.28 0.39 0.15 0.71 0.32 0.37\nMEMIT 0.30 0.44 0.19 1.0 0.33 0.45 0.28 0.45 0.09 0.96 0.56 0.47\nPMET 0.37 0.51 0.17 0.94 0.29 0.46 0.30 0.47 0.13 0.83 0.31 0.41\nSWEA⊕OS 0.32 0.56 0.53 1.0 0.29 0.54 0.30 0.49 0.16 0.81 0.37 0.43\nTable 3: Accuracy of RIPPLE EDITS on GPT-J and Llama-2.\nLLMs Dataset Editor LG CI CII SA RS Avg.\nGPT-J\nRANDOM SWEA⊕OS 0.62 0.54 0.63 1.0 0.41 0.64\nw/o suppressing 0.60 ↓0.02 0.47↓0.07 0.25↓0.38 0.84↓0.16 0.48↑0.07 0.53↓0.11\nPOPULAR SWEA⊕OS 0.60 0.53 0.23 1.0 0.38 0.54\nw/o suppressing 0.32 ↓0.28 0.53 0.19 ↓0.04 0.86↓0.14 0.28↓0.10 0.44 ↓0.1\nLlama-2\nRANDOM SWEA⊕OS 0.60 0.49 0.37 1.0 0.55 0.60\nw/o suppressing 0.59 ↓0.01 0.49 0.30 ↓0.07 1.0 0.54 ↓0.01 0.58 ↓0.02\nPOPULAR SWEA⊕OS 0.30 0.49 0.16 0.81 0.37 0.43\nw/o suppressing 0.29 ↓0.01 0.49 0.16 0.81 0.37 0.42 ↓0.01\nTable 4: Accuracy of RIPPLE EDITS on GPT-J and Llama-2 in ablation study.\nnally generating 2188 and 2186 editing instances for GPT-J\nand Llama-2, respectively. Since none of these editing in-\nstances contain data for testing Preservation, our results do\nnot include the Preservation metric.\nResults The accuracy of editing GPT-J and Llama-2 on\nthe RANDOM and POPULAR subdatasets of R IPPLE ED-\nITS is shown in Table 3. The results of GPT-J indicate\nthat SWEA⊕OS performs better than the baselines on CI,\nCII, and SA, suggesting that SWEA⊕OS’s ability to rea-\nson about edited knowledge surpasses existing baselines.\nFor Llama-2, except for the LG and RS on the RANDOM\ndataset and the SA and RS on the POPULAR dataset where\nSWEA⊕OS lags behind existing baselines, the results on\nother metrics are better than that of current baselines. From\nTable 3, it can be easily seen that SWEA⊕OS performs\npoorly on the RS, which tests the ability of the editing\nmethod to retain non-edited knowledge about the subject.\nA possible reason for this situation is that SWEA⊕OS in-\ntroduced unintended knowledge during the optimization of\nthe editing objectives. The metrics LG, CI and CII test the\nability of the model editing method in 2-hop reasoning, and\nexperimental results indicate that SWEAOS exhibits the best\nreasoning capability.\n5.4 Ablation Study\nTo verify that the suppressing step in the OS fusion method\nis effective to the expression of new knowledge, we remove\nthe suppressing step and test the results on the R IPPLE ED-\nITS benchmark. As shown in Table 4, after removing the\nsuppression step (i.e., w/o suppressing), the overwhelming\nmajority of performance of SWEA⊕OS in editing GPT-J\nand Llama-2 has declined, which indicates that our suppres-\nsion step effectively alleviated the effect brought by KEDs of\nsubject word embeddings. When editing GPT-J on the RAN-\nDOM and POPULAR datasets, the absence of suppression\nsteps led to an average reduction of 0.14 in all metrics. More-\nover, in the ablation experiment, the performance drop of\nGPT-J is more significant than that of Llama-2, which may\nbe due to the stronger robustness brought by more param-\neters of Llama-2. Overall, the introduction of suppression\nsteps in SWEA⊕OS effectively facilitated the expression of\nnew knowledge in LLMs.\n6 Conclusion\nWe propose SWEA⊕OS method for more effective and ef-\nficient knowledge editing. SWEA⊕OS consists of Subject\nWord Embedding Fusion (SWEA) framework and the op-\ntimizing then suppressing (OS) fusion method. The SWEA\nframework uses token-level matching to identify the edited\nsubject and adds the editing embeddings obtained from the\nOS fusion method to the subject embedding, ultimately al-\ntering the specific attributes of the subject to achieve knowl-\nedge editing. The OS fusion method employs an optimizing\nthen suppressing strategy to effectively express new knowl-\nedge in editing embeddings. SWEA⊕OS achieve overall\nstate-of-the-art (SOTA) results on the C OUNTER FACT and\nzsRE datasets, and it also shows SOTA performance in terms\nof reasoning ability on a more complex model editing bench-\nmark RIPPLE EDITS . Moreover, SWEA⊕OS also provide a\nnew insight to efficiently update knowledge of LLMs.\n24500\nEthical Statement\nThe purpose of this work is to provide a more efficient and\neffective approach for knowledge editing. While SWEA can\ncorrect incorrect or outdated knowledge in LLMs cooper-\nating with different fusion methods, it is important to rec-\nognize that SWEA is also susceptible to misuse, leading to\nthe corruption of correct and already aligned knowledge in\nLLMs. Given that LLMs can inherently produce hallucina-\ntions, we would remind readers not to overly trust LLMs.\nAcknowledgments\nThis work was partly supported by the Hunan Provincial\nNatural Science Foundation Projects (No. 2022JJ30668 and\nNo. 2022JJ30046), and also partly supported by the National\nKey R&D Program of China (No. 2024YFB4506200).\nReferences\nBalogh, V .; Berend, G.; Diochnos, D. I.; and Tur´an, G. 2020.\nUnderstanding the Semantic Content of Sparse Word Em-\nbeddings Using a Commonsense Knowledge Base. Pro-\nceedings of the AAAI Conference on Artificial Intelligence ,\n34(05): 7399–7406.\nCohen, R.; Biran, E.; Yoran, O.; Globerson, A.; and Geva,\nM. 2023. Evaluating the ripple effects of knowledge editing\nin language models. arXiv preprint arXiv:2307.12976.\nDai, D.; Dong, L.; Hao, Y .; Sui, Z.; Chang, B.; and Wei, F.\n2021. Knowledge neurons in pretrained transformers. arXiv\npreprint arXiv:2104.08696.\nDe Cao, N.; Aziz, W.; and Titov, I. 2021. Editing Fac-\ntual Knowledge in Language Models. In Proceedings of\nthe 2021 Conference on Empirical Methods in Natural Lan-\nguage Processing, 6491–6506. Online and Punta Cana, Do-\nminican Republic: Association for Computational Linguis-\ntics.\nDeng, J.; Wei, Z.; Pang, L.; Ding, H.; Shen, H.; and Cheng,\nX. 2024. UnKE: Unstructured Knowledge Editing in Large\nLanguage Models. arXiv preprint arXiv:2405.15349.\nDong, Q.; Dai, D.; Song, Y .; Xu, J.; Sui, Z.; and Li, L. 2022.\nCalibrating factual knowledge in pretrained language mod-\nels. arXiv preprint arXiv:2210.03329.\nGeva, M.; Schuster, R.; Berant, J.; and Levy, O. 2021. Trans-\nformer Feed-Forward Layers Are Key-Value Memories. In\nProceedings of the 2021 Conference on Empirical Methods\nin Natural Language Processing, 5484–5495. Online and\nPunta Cana, Dominican Republic: Association for Compu-\ntational Linguistics.\nGionis, A.; Indyk, P.; Motwani, R.; et al. 1999. Similarity\nsearch in high dimensions via hashing. In Vldb, volume 99,\n518–529.\nGu, J.-C.; Xu, H.-X.; Ma, J.-Y .; Lu, P.; Ling, Z.-H.; Chang,\nK.-W.; and Peng, N. 2024. Model Editing Can Hurt Gen-\neral Abilities of Large Language Models. arXiv preprint\narXiv:2401.04700.\nHartvigsen, T.; Sankaranarayanan, S.; Palangi, H.; Kim, Y .;\nand Ghassemi, M. 2022. Aging with GRACE: Lifelong\nModel Editing with Discrete Key-Value Adaptors. arXiv\npreprint arXiv:2211.11031.\nHuang, Z.; Shen, Y .; Zhang, X.; Zhou, J.; Rong, W.; and\nXiong, Z. 2023. Transformer-Patcher: One Mistake worth\nOne Neuron. arXiv preprint arXiv:2301.09785.\nLi, C.; Gan, Z.; Yang, Z.; Yang, J.; Li, L.; Wang, L.;\nand Gao, J. 2023a. Multimodal foundation models: From\nspecialists to general-purpose assistants. arXiv preprint\narXiv:2309.10020, 1(2): 2.\nLi, J.; Monroe, W.; and Jurafsky, D. 2016. Understand-\ning neural networks through representation erasure. arXiv\npreprint arXiv:1612.08220.\nLi, S.; Deng, Y .; Cai, D.; Lu, H.; Chen, L.; and Lam, W.\n2024a. Consecutive Model Editing with Batch alongside\nHooK Layers. arXiv preprint arXiv:2403.05330.\nLi, X.; Li, S.; Song, S.; Liu, H.; Ji, B.; Wang, X.; Ma, J.;\nYu, J.; Liu, X.; Wang, J.; and Zhang, W. 2024b. SWEA:\nUpdating Factual Knowledge in Large Language Models via\nSubject Word Embedding Altering. arXiv:2401.17809.\nLi, X.; Li, S.; Song, S.; Yang, J.; Ma, J.; and Yu, J.\n2023b. PMET: Precise Model Editing in a Transformer.\narXiv:2308.08742.\nLi, Z.; Zhang, N.; Yao, Y .; Wang, M.; Chen, X.; and Chen,\nH. 2023c. Unveiling the pitfalls of knowledge editing for\nlarge language models. arXiv preprint arXiv:2310.02129.\nMeng, K.; Bau, D.; Andonian, A.; and Belinkov, Y . 2022a.\nLocating and Editing Factual Associations in GPT. In\nKoyejo, S.; Mohamed, S.; Agarwal, A.; Belgrave, D.; Cho,\nK.; and Oh, A., eds., Advances in Neural Information Pro-\ncessing Systems, volume 35, 17359–17372. Curran Asso-\nciates, Inc.\nMeng, K.; Sharma, A. S.; Andonian, A.; Belinkov, Y .; and\nBau, D. 2022b. Mass-Editing Memory in a Transformer.\nMitchell, E.; Lin, C.; Bosselut, A.; Finn, C.; and Manning,\nC. D. 2022. Fast Model Editing at Scale. In International\nConference on Learning Representations.\nPark, S.; Bak, J.; and Oh, A. 2017. Rotated Word Vector\nRepresentations and their Interpretability. In Palmer, M.;\nHwa, R.; and Riedel, S., eds., Proceedings of the 2017 Con-\nference on Empirical Methods in Natural Language Pro-\ncessing, 401–411. Copenhagen, Denmark: Association for\nComputational Linguistics.\nS ¸enel, L. K.; Utlu, I.; Y¨ucesoy, V .; Koc, A.; and Cukur, T.\n2018. Semantic structure and interpretability of word em-\nbeddings. IEEE/ACM Transactions on Audio, Speech, and\nLanguage Processing, 26(10): 1769–1779.\nSimhi, A.; and Markovitch, S. 2023. Interpreting Embed-\nding Spaces by Conceptualization. In The 2023 Conference\non Empirical Methods in Natural Language Processing.\nTan, C.; Zhang, G.; and Fu, J. 2023. Massive Editing for\nLarge Language Models via Meta Learning. arXiv preprint\narXiv:2311.04661.\nTan, C.; Zhang, G.; and Fu, J. 2024. Massive Editing for\nLarge Language Models via Meta Learning. In The Twelfth\nInternational Conference on Learning Representations.\nTian, B.; Cheng, S.; Liang, X.; Zhang, N.; Hu, Y .; Xue,\nK.; Gou, Y .; Chen, X.; and Chen, H. 2024. InstructEdit:\n24501\nInstruction-based Knowledge Editing for Large Language\nModels. arXiv preprint arXiv:2402.16123.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; et al. 2023. Llama 2: Open foundation and fine-tuned\nchat models. arXiv preprint arXiv:2307.09288.\nWang, B.; and Komatsuzaki, A. 2021. GPT-J-6B: A 6 billion\nparameter autoregressive language model.\nWang, P.; Li, Z.; Zhang, N.; Xu, Z.; Yao, Y .; Jiang, Y .; Xie,\nP.; Huang, F.; and Chen, H. 2024. WISE: Rethinking the\nKnowledge Memory for Lifelong Model Editing of Large\nLanguage Models. arXiv preprint arXiv:2405.14768.\nWang, S.; Zhu, Y .; Liu, H.; Zheng, Z.; Chen, C.; and Li, J.\n2023. Knowledge Editing for Large Language Models: A\nSurvey. arXiv:2310.16218.\nYao, Y .; Wang, P.; Tian, B.; Cheng, S.; Li, Z.; Deng,\nS.; Chen, H.; and Zhang, N. 2023. Editing Large Lan-\nguage Models: Problems, Methods, and Opportunities. In\nBouamor, H.; Pino, J.; and Bali, K., eds., Proceedings of\nthe 2023 Conference on Empirical Methods in Natural Lan-\nguage Processing, 10222–10240. Singapore: Association\nfor Computational Linguistics.\nZhang, B.; Zheng, W.; Zhou, J.; and Lu, J. 2023. Bort: To-\nwards Explainable Neural Networks with Bounded Orthog-\nonal Constraint. In The Eleventh International Conference\non Learning Representations.\nZhang, N.; Yao, Y .; Tian, B.; Wang, P.; Deng, S.; Wang, M.;\nXi, Z.; Mao, S.; Zhang, J.; Ni, Y .; Cheng, S.; Xu, Z.; Xu, X.;\nGu, J.-C.; Jiang, Y .; Xie, P.; Huang, F.; Liang, L.; Zhang, Z.;\nZhu, X.; Zhou, J.; and Chen, H. 2024. A Comprehensive\nStudy of Knowledge Editing for Large Language Models.\narXiv:2401.01286.\nZhao, W. X.; Zhou, K.; Li, J.; Tang, T.; Wang, X.; Hou, Y .;\nMin, Y .; Zhang, B.; Zhang, J.; Dong, Z.; Du, Y .; Yang, C.;\nChen, Y .; Chen, Z.; Jiang, J.; Ren, R.; Li, Y .; Tang, X.; Liu,\nZ.; Liu, P.; Nie, J.-Y .; and Wen, J.-R. 2023. A Survey of\nLarge Language Models.\nZhu, C.; Rawat, A. S.; Zaheer, M.; Bhojanapalli, S.; Li, D.;\nYu, F.; and Kumar, S. 2020. Modifying Memories in Trans-\nformer Models.\n24502",
  "topic": "Subject (documents)",
  "concepts": [
    {
      "name": "Subject (documents)",
      "score": 0.769054651260376
    },
    {
      "name": "Word (group theory)",
      "score": 0.7066389918327332
    },
    {
      "name": "Computer science",
      "score": 0.6568526029586792
    },
    {
      "name": "Word embedding",
      "score": 0.5821825265884399
    },
    {
      "name": "Natural language processing",
      "score": 0.5762227177619934
    },
    {
      "name": "Embedding",
      "score": 0.5330537557601929
    },
    {
      "name": "Linguistics",
      "score": 0.48954662680625916
    },
    {
      "name": "Artificial intelligence",
      "score": 0.48749256134033203
    },
    {
      "name": "World Wide Web",
      "score": 0.1186673641204834
    },
    {
      "name": "Philosophy",
      "score": 0.08487048745155334
    }
  ],
  "institutions": [],
  "cited_by": 1
}