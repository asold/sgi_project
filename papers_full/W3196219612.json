{
    "title": "Improving 3D Object Detection with Channel-wise Transformer",
    "url": "https://openalex.org/W3196219612",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4286597740",
            "name": "Sheng, Hualian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2356369883",
            "name": "Cai, Sijia",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1996496358",
            "name": "Liu Yuan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2137906401",
            "name": "Deng Bing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2359370308",
            "name": "Huang, Jianqiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4209406217",
            "name": "Hua, Xian-Sheng",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4222113055",
            "name": "Zhao, Min-Jian",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3008105217",
        "https://openalex.org/W2897529137",
        "https://openalex.org/W2982363097",
        "https://openalex.org/W2415454270",
        "https://openalex.org/W2229637417",
        "https://openalex.org/W2150066425",
        "https://openalex.org/W3034602892",
        "https://openalex.org/W3107819843",
        "https://openalex.org/W2951517617",
        "https://openalex.org/W2963400571",
        "https://openalex.org/W2963121255",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3118341329",
        "https://openalex.org/W3116160554",
        "https://openalex.org/W2184393491",
        "https://openalex.org/W3035346742",
        "https://openalex.org/W2968296999",
        "https://openalex.org/W2886904239",
        "https://openalex.org/W3153465022",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2115579991",
        "https://openalex.org/W1644641054",
        "https://openalex.org/W3031752193",
        "https://openalex.org/W2963727135",
        "https://openalex.org/W2555618208",
        "https://openalex.org/W2981949127",
        "https://openalex.org/W3012573144",
        "https://openalex.org/W3035172746",
        "https://openalex.org/W2798965597",
        "https://openalex.org/W3113028524",
        "https://openalex.org/W2560609797",
        "https://openalex.org/W2970259716",
        "https://openalex.org/W2963927307",
        "https://openalex.org/W2949708697",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2899302124",
        "https://openalex.org/W2894705404",
        "https://openalex.org/W3042259089"
    ],
    "abstract": "Though 3D object detection from point clouds has achieved rapid progress in recent years, the lack of flexible and high-performance proposal refinement remains a great hurdle for existing state-of-the-art two-stage detectors. Previous works on refining 3D proposals have relied on human-designed components such as keypoints sampling, set abstraction and multi-scale feature fusion to produce powerful 3D object representations. Such methods, however, have limited ability to capture rich contextual dependencies among points. In this paper, we leverage the high-quality region proposal network and a Channel-wise Transformer architecture to constitute our two-stage 3D object detection framework (CT3D) with minimal hand-crafted design. The proposed CT3D simultaneously performs proposal-aware embedding and channel-wise context aggregation for the point features within each proposal. Specifically, CT3D uses proposal's keypoints for spatial contextual modelling and learns attention propagation in the encoding module, mapping the proposal to point embeddings. Next, a new channel-wise decoding module enriches the query-key interaction via channel-wise re-weighting to effectively merge multi-level contexts, which contributes to more accurate object predictions. Extensive experiments demonstrate that our CT3D method has superior performance and excellent scalability. Remarkably, CT3D achieves the AP of 81.77% in the moderate car category on the KITTI test 3D detection benchmark, outperforms state-of-the-art 3D detectors.",
    "full_text": "Improving 3D Object Detection with Channel-wise Transformer\nHualian Sheng1,2* Sijia Cai2 Yuan Liu2 Bing Deng2\nJianqiang Huang2 Xian-Sheng Hua2 Min-Jian Zhao1†\n1 College of Information Science and Electronic Engineering, Zhejiang University\n2DAMO Academy, Alibaba Group\nhlsheng@zju.edu.cn, {stephen.csj, alen.ly, dengbing.db}@alibaba-inc.com\njianqiang.jqh@gmail.com, xiansheng.hxs@alibaba-inc.com, mjzhao@zju.edu.cn\nAbstract\nThough 3D object detection from point clouds has\nachieved rapid progress in recent years, the lack of flexible\nand high-performance proposal refinement remains a great\nhurdle for existing state-of-the-art two-stage detectors. Pre-\nvious works on refining 3D proposals have relied on human-\ndesigned components such as keypoints sampling, set ab-\nstraction and multi-scale feature fusion to produce pow-\nerful 3D object representations. Such methods, however,\nhave limited ability to capture rich contextual dependencies\namong points. In this paper, we leverage the high-quality\nregion proposal network and a Channel-wise Transformer\narchitecture to constitute our two-stage 3D object detection\nframework (CT3D) with minimal hand-crafted design. The\nproposed CT3D simultaneously performs proposal-aware\nembedding and channel-wise context aggregation for the\npoint features within each proposal. Specifically, CT3D\nuses proposal’s keypoints for spatial contextual modelling\nand learns attention propagation in the encoding module,\nmapping the proposal to point embeddings. Next, a new\nchannel-wise decoding module enriches the query-key in-\nteraction via channel-wise re-weighting to effectively merge\nmulti-level contexts, which contributes to more accurate ob-\nject predictions. Extensive experiments demonstrate that\nour CT3D method has superior performance and excellent\nscalability. Remarkably, CT3D achieves the AP of 81.77%\nin the moderate car category on the KITTI test 3D detection\nbenchmark, outperforms state-of-the-art 3D detectors.\n1. Introduction\n3D object detection from point clouds is envisioned as\nan indispensable part of future Autonomous Vehicle (A V).\n*This work was done when the author was visiting Alibaba as a re-\nsearch intern. The code is available at https://github.com/hlsheng1/CT3D\n†Corresponding author.\nUnlike the developed 2D detection algorithms whose suc-\ncess is mainly due to the regular structure of image pix-\nels, LiDAR point clouds are usually sparse, unordered and\nunevenly distributed. This makes the CNN-like operations\nnot well suited to process unstructured point clouds directly.\nTo tackle these challenges, many approaches employ vox-\nelization or custom discretization for point clouds. Several\nmethods [28, 15] project point clouds to a birds-eye view\n(BEV) representation and apply the standard 2D convolu-\ntions, however, it will inevitably sacrifice certain geometric\ndetails which are vital for generating accurate localization.\nOther methods [3, 33] rasterize point clouds into a 3D voxel\ngrid and use regular 3D CNNs to perform computation in\ngrid space, but this category of methods suffers from com-\nputational bottleneck associated with making the grid finer.\nA major breakthrough in detection task on point clouds is\ndue to the effective deep architectures for point clouds rep-\nresentation such as volumetric convolution [33] and permu-\ntation invariant convolution [22].\nRecently, most state-of-the-art methods for 3D object\ndetection adopt a two-stage framework consisting of 3D\nregion proposal generation and proposal feature refine-\nment. Notice that the most popular region proposal network\n(RPN) backbone [33] has achieved over 95% recall rate on\nthe KITTI 3D Detection Benchmark, whereas this method\nonly achieves 78% Average Precision (AP). The reason for\nsuch a gap stems from the difficulty in encoding an object\nand extracting the robust feature from 3D proposals in cases\nof occlusion or long-range distance. Therefore, how to ef-\nfectively model geometric relationships among points and\nexploit accurate position information during the proposal\nfeature refinement stage is crucial for good performance.\nAn important family of models is PointNet [22] and its vari-\nants [23, 19, 25], which use a flexible receptive field to ag-\ngregate features by local regions and permutation-invariant\nnetwork. However, these methods have the drawback of\ninvolving plenty of hand-crafted designs, such as the neigh-\nbor ball radii and the grid size. Another family of models\nis the voxel-based methods [33, 27, 39] which use 3D con-\nvolutional kernels to gather information from neighboring\nvoxels. But the performance of such methods is not opti-\nmal caused by the voxel quantization and sensitive to hyper-\nparameters. Later studies [43, 24, 4, 10] further apply the\npoint-voxel mixed strategy to capture multi-scale features\nwhile retaining fine-grained localization but are strongly\ntied to the specific RPN architectures.\nIn this paper, we make two major contributions. First,\nwe propose a novel end-to-end two-stage 3D objection\ndetection framework called CT3D. Motivated by the re-\ncent Transformer-based 2D detection method DETR [1]\nthat uses CNN backbone to extract features and encoder-\ndecoder Transformer to enhance the RoI region features, we\ndesign our CT3D to generate 3D bounding boxes at the first\nstage, then learn per-proposal representation by incorporat-\ning a novel Transformer architecture with channel-wise re-\nweighting mechanism in decoder. The proposed framework\nexhibits very strong performance in terms of accuracy and\nefficiency, and thus can be conveniently combined with any\nhigh-quality RPN backbones.\nThe second contribution is the custom Transformer that\noffers several benefits over the traditional point/voxel-based\nfeature aggregation mechanism. Despite the point-wise or\nvoxel convolutions have the ability of local and global con-\ntext modelling, there still have been several limitations in\nincreasing receptive field and parameter optimization. In\naddition, point-cloud based 3D object detectors also have to\ndeal with the challenging missing/noisy detections such as\nocclusion and distancing patterns with a few points. Self-\nattention in Transformers has recently emerged as a basic\nbuilding block for capturing long-range interactions thus is\na natural choice in acquiring context information for en-\nriching the faraway objects or increasing the confidence of\nfalse negatives. Inspired by this idea, we initially intro-\nduce a proposal-to-point embedding to effectively encode\nthe RPN proposal information in the encoder module. Fur-\nthermore, we exploit a channel-wise re-weighting approach\nto augment the standard Transformer decoder in considera-\ntion of both global and local channel-wise features for the\nencoded points. The purpose is to scale the feature decod-\ning space where we can compute attention distribution over\neach channel dimension of key embeddings thus can en-\nhance the expressiveness of query-key interactions. Exten-\nsive experiments show that our proposed CT3D can out-\nperform the state-of-the-art published methods on both the\nKITTI dataset and the large-scale Waymo dataset.\n2. Related Work\nPoint Cloud Representations for 3d Object Detection.\nRecently, there has been a lot of progress on learning ef-\nfective representations for the raw LiDAR point clouds.\nA noticeable portion of efforts are PointNet series [22]\nwhich employed permutation invariant operations to ag-\ngregate the point features. F-PointNet [21] generated the\nregion-level features for point clouds within each 3D frus-\ntum. PointRCNN [25] used PointNet++ [23] to segment\nforeground 3D points and refine the proposals with the seg-\nmentation features. STD [37] further extended the pro-\nposal refinement by transferring sparse point features into\ndense voxel representation. Moreover, 3DSSD [36] im-\nproved the point-based approach with a new sampling strat-\negy based on feature distance. However, PointNet-like\narchitectures still present limited ability to capture local\nstructures for LiDAR data. Another category of meth-\nods [3, 13, 34, 35, 28, 15, 12, 16, 17] aimed to voxelize\nthe unstructured point clouds as a regular 2D/3D grid over\nwhich conventional CNNs can be easily applied. Pioneer\nwork [3] encoded the point clouds as 2D bird-view feature\nmaps to generate highly accurate 3D candidate boxes, moti-\nvating many efficient bird-view representation-based meth-\nods. V oxelNet [43] transformed the points to form a com-\npact feature representation. SECOND [33] introduced 3D\nsparse convolution for efficient 3D voxel processing. These\nvoxel-based methods are still focused on the subdivision of\na volume rather than adaptively modelling local geomet-\nric structure. Furthermore, various point-voxel based meth-\nods have been proposed for multi-scale feature aggregation.\nSA-SSD [10] presented an auxiliary network on the basis\nof 3D voxel CNN. PV-RCNN [24] and its variant V oxelR-\nCNN [4] adopted 3D voxel CNN as RPN to generate high-\nquality proposals and then utilize PointNet to aggregate the\nvoxel features around the grids. Nevertheless, these hybrid\nmethods require plenty of hand-crafted feature designs.\nTransformers for object detection.A new paradigm for\nobject detection has recently evolved due to the success of\nTransformers in many computer vision fields [1, 44, 5, 9, 6].\nSince Transformer models are very effective at learning lo-\ncal context-aware representations, DETR [1] viewed the de-\ntection as a set prediction problem and employed Trans-\nformer with parallel decoding to detect objects in 2D image.\nA variant of DETR [44] further developed a deformable\nattention module to employ cross-scale aggregation. For\npoint clouds, recent methods [9, 6] also explored to use self-\nattention for classification and segmentation tasks.\n3. CT3D for 3D Object Detection\nGiven proposals generated by the widely used RPN\nbackbones like 3D voxel CNN [33], current state-of-the-\nart proposal refinement approaches [24, 4] focus on refining\nthe intermediate multi-stage voxel features extracted by the\nconvolution layers, suffering the difficulties of extra hyper-\nparameter optimization and designing generalized models.\nWe believe that the raw points with precise position infor-\nmation are sufficient for refining the detection proposals.\nBearing this view in mind, we construct our CT3D frame-\nRPN \nProposal-to-point Self-attention Module Channel-wise Decoding Module \nDetect Head \nProposal-to-point Embedding \nChannel-wise Transformer \n3D Proposals \nand Raw Points FFN \nFFN \nConfidence \nBox Regression \nCls. \nBBox. \nProposal Box \n… \nDecoder Encoder \nPoint Cloud \nܰ×ܦTranspose \nSoftmax \nܰ×ܰ\n…\nܰ×ܦ\nSelf-attention Encoding \nRepeat ܰ×ܦ\nRe-weighting \n1 ×ܦ\n1 ×ܦ\n1 ×ܰH\nHadamard ෡K\n෡V\nݍQ\nK\nV\nPredicted Bounding Boxes \nTranspose \nSoftmax \nFigure 1. Overview of CT3D. The raw points are first fed into the RPN for generating 3D proposals. Then the raw points along with\nthe corresponding proposals are processed by the channel-wise Transformer composed of the proposal-to-point encoding module and the\nchannel-wise decoding module. Specifically, the proposal-to-point encoding module is to modulate each point feature with global proposal-\naware context information. After that, the encoded point features are transformed into an effective proposal feature representation by the\nchannel-wise decoding module for confidence prediction and box regression.\nwork by deploying a well-designed Transformer on top of\na RPN network to directly utilize the raw point clouds.\nSpecifically, the whole CT3D detection framework is com-\nposed of three parts, i.e., a RPN backbone for proposal gen-\neration, a channel-wise Transformer for proposal feature re-\nfinement and a detect head for object predictions. Figure 1\nillustrates an overview of our CT3D framework.\n3.1. RPN for 3D Proposal Generation\nStarting from the point clouds P with 3-dimension coor-\ndinates and C-dimension point features, the predicted 3D\nbounding box generated by RPN consists of center coordi-\nnate pc = [xc, yc, zc], length lc, width wc, height hc, and\norientation θc. In this paper, we adopt the 3D voxel CNN\nSECOND [33] as our default RPN due to its high efficiency\nand accuracy. Note that any high-quality RPN should be\nreadily replaceable in our framework and is amenable to\ntraining via an end-to-end manner.\n3.2. Proposal-to-point Encoding Module\nTo refine the generated RPN proposals, we adopt a two-\nstep strategy. Specifically, the first proposal-to-point em-\nbedding step maps the proposal to point features, then the\nsecond self-attention encoding step is to refine point fea-\ntures via modelling the relative relationships among points\nwithin the corresponding proposal.\nProposal-to-point Embedding. Given the proposals gen-\nerated by RPN, we delimit out a scaled RoI area in point\nclouds according to the proposal. This aims to compensate\nthe deviation between the proposal and the corresponding\nground-truth box by wrapping all object points as much as\npossible. Specifically, the scaled RoI area is a cylindrical\nwith unlimited height and a radius r = α\nq\n(lc\n2 )2 + (wc\n2 )2,\nwhere α is a hyper-parameter, and l, w denote the length\nand width of the proposal, respectively. Hereinafter, the\nrandomly sampled N = 256 points within the scaled RoIs\n(N = {p1, . . . ,pN } ) are taken out for further processing.\nAt first, we calculate the relative coordinates between\neach sampled point and the center point of the proposal\nfor unifying the input distance feature, denoted as ∆pc\ni =\npi − pc, ∀pi ∈ N. A straightforward thought is to directly\nconcatenate the proposal information into each point fea-\nture, i.e., [∆pc\ni , lc, wc, hc, θc, fr\ni ], where fr\ni is the raw point\nfeature such as reflection. However, the size-orientation\nrepresentation for proposal yields only modest performance\nas the Transformer encoder might be less effective to reori-\nent in accord with above-mentioned geometric information.\nIt is noteworthy that the keypoints usually offer more ex-\nplicit geometry property in detection tasks [41, 14], we pro-\npose a novel keypoints subtraction strategy to compute the\nrelative coordinates between each point and the eight corner\npoints of the corresponding proposal. The calculated rela-\ntive coordinates are ∆pj\ni = pi − pj, j= 1, . . . ,8, where\npj is the coordinate of the j-th corner point. Note that\nlc, wc, hc and θc disappear but are contained in different di-\nmensions of distance information. Through this way, the\nnewly generated relative coordinates ∆pj\ni can be viewed as\na better representation of proposal information. As shown\nin the left part of Figure 2, for each point pi, the proposal-\nguided point feature can be expressed as:\nfi = A([∆pc\ni , ∆p1\ni , . . . ,∆p8\ni , fr\ni ]) ∈ RD, (1)\nwhere A(·) is a linear projection layer to map point feature\ninto a high-dimensional embedding.\nSelf-attention Encoding. The embeded point features are\nthen fed into the multi-head self-attention layer, followed\n: Proposal Center and Corners \n: Subtraction Operator \nAdd & Norm \nMulti-Head Self-Attention \nFFN \nAdd & Norm \n3 灤\nQ K V\nProposal-to-point Embedding Self-attention Encoding \nOutput \nܰ× 28 ܰ× 256 \nFigure 2. Proposal-to-point encoding. The location features of raw\npoint clouds are first modulated by the proposal information (cen-\nter and corners) via subtraction operator. Then, the resulting point\nfeatures are refined by the proposal-aware encoding module with\nmulti-head self-attention mechanism.\nby a feed-forward network (FFN) with residual structure, to\nencode rich contextual relationships and point dependencies\nin proposal for refining point features. As shown in the right\npart of Figure 2, this self-attention encoding scheme shares\nalmost the same structure as the original NLP Transformer\nencoder, except for the position embedding since it is al-\nready included in the point features. Reader can refer to [31]\nfor more details. Denote X = [ fT\n1 , . . . ,fT\nN ]T ∈ RN×D\nas the embedded point features with the dimension D, we\nhave Q = WqX; K = WkX; V = WvX, where\nWq, Wk, Wv ∈ RN×N are linear projections, and Q, K\nand V are so-called query, key and value embeddings.\nThese three embeddings are then processed by multi-head\nself-attention mechanism. In a H-head attention situation,\nQ, K and V are further divided into Q = [Q1, . . . ,QH],\nK = [ K1, . . . ,KH], and V = [ V1, . . . ,VH], where\nQh, Kh, Vh ∈ RN×D′\n, ∀h = 1, . . . , H, and D′ = D\nH . The\noutput after multi-head self-attention is given by:\nS(att)(Q, K, V) =\n\u0014\nσ\n\u0000QhKT\nh√\nD′\n\u0001\n· Vh\n\u0015\n, h= 1, . . . , H,(2)\nwhere σ(·) is softmax function. Hereinafter, applying a sim-\nple FFN and residual operator, the result is as follows:\nS(emb)(X) = Z(F(Z(S(att)(Q, K, V)))), (3)\nwhere Z(·) denotes add and normalization operator, F(·)\ndenotes a FFN with two linear layers and one Relu activa-\ntion. We observe that a stack of 3 identical self-attention\nencoding modules is ideal for our CT3D framework.\n3.3. Channel-wise Decoding Module\nIn this subsection, we manage to decode all point fea-\ntures (i.e., ˆX) from the encoder module into a global repre-\nsentation, which is further processed by FFNs for the final\ndetection predictions. Different from the standard Trans-\nformer decoder, which transforms M multiple query em-\nbeddings using self- and encoder-decoder attention mecha-\nnism, our decoder only manipulates one query embedding\naccording to the following two facts:\n• M query embeddings suffer high memory latency, es-\npecially for processing with numbers of proposals.\n• M query embeddings are usually independently trans-\nformed into M words or objects, while our proposal\nrefinement model only needs one prediction.\nGenerally, the final proposal representation after decoder\ncan be regarded as a weighted sum of all point features, our\nkey motivation is to determine the decoding weights that are\ndedicated for each point. In below, we first analyze the stan-\ndard decoding scheme, and then develop an improved de-\ncoding scheme to acquire more effective decoding weights.\nStandard Decoding. The standard decoding scheme uti-\nlizes a learnable vector ( i.e., query embedding) of dimen-\nsion D to aggregate the point features across all channels.\nAs shown in Figure 3(a), the final decoding weight vector\nfor all point features in each attention head is:\nw(S)\nh = σ\n\u0000 ˆqh ˆKT\nh√\nD′\n\u0001\n, h= 1, . . . , H, (4)\nwhere ˆKh is the key embeddings of h-th head computed\nby the projection of encoder output, and ˆqh is the corre-\nsponding query embedding. Note that each value of vector\nˆqh ˆKT\nh can be viewed as the global aggregation for individ-\nual point ( i.e., each key embedding), and the subsequent\nsoftmax function assigns the decoding value for each point\naccording to the probability in the normalized vector. Con-\nsequently, the values in decoding weight vector are derived\nfrom simple global aggregation and lack the local channel-\nwise modelling, which is essential to learn 3D surface struc-\ntures of point clouds because different channels usually ex-\nhibit strong geometric relationships in point clouds.\nChannel-wise Re-weighting. In order to emphasize\nthe channel-wise information for key embeddings ˆKT\nh , a\nstraightforward solution is to compute the decoding weight\nvector for points based on all the channels of ˆKT\nh . That is,\nwe generate D different decoding weight vectors for each\nchannel to obtain D decoding values. Further, a linear pro-\njection is introduced for these D decoding values to form\na united channel-wise decoding vector. As shown in Fig-\nure 3(b), this new channel-wise re-weighting for decoding\nweight vector can be summarized as:\nw(C)\nh = s · ˆσ\n\u0000 ˆKT\nh√\nD′\n\u0001\n, h= 1, . . . , H, (5)\nwhere s is a linear projection that compressesD′ number of\ndecoding values into a re-weighting scalar, ˆσ(·) computes\nthe softmax along the N dimension. However, the decoding\nweights computed by ˆσ(·) are associated with each channel,\nand thus ignore the global aggregation of each point. There-\nfore, we can conclude that the standard decoding scheme\n(a) (b) \n(c) \n…\nmul \n…\nݍ௛ܭ௛\nsoftmax \n…\nmul \n…ݓ\n…\n…ܭ௛\n…\nsoftmax \nrepeat \n…\nmul \n…\nmul \n…\nݍ௛\n…\nܭ௛\nܭ௛\n…\n…\n…\nmul \n…ݓ\n…\nsoftmax \nFigure 3. Illustration of the different decoding schemes: (a)\nStandard decoding; (b) Channel-wise re-weighting; (c) Extended\nchannel-wise re-weighting.\nfocuses on global aggregation while the channel-wise re-\nweighting scheme concentrates on the channel-wise local\naggregation. To combine their characteristics, we propose\nan extended channel-wise re-weighting scheme as below.\nExtended Channel-wise Re-weighting. Specifically, we\nfirst repeat the matrix product of query embedding and\nkey embeddings to spread the spatial information into each\nchannel, and the output is then multiplied element-wise\nwith the key embeddings for keeping the channel differ-\nences. As illustrated in Figure 3 (c), this novel extended\nchannel-wise re-weighting scheme generates the following\ndecoding weight vector for all the points:\nw(EC)\nh = s · ˆσ\n\u0000ρ(ˆqh ˆKT\nh ) ⊙ ˆKT\nh√\nD′\n\u0001\n, h= 1, . . . , H,(6)\nwhere ρ(·) is a repeat operator makes R1×N → RD′×N .\nIn this way, we can not only maintain the global informa-\ntion as compared to the channel-wise re-weighting scheme,\nbut also enrich the local and detailed channel interactions\nas compared to the standard decoding scheme. Besides,\nthis extended channel-wise re-weighting only brings 1K+\n(Bytes) increase as compared to the other two schemes. As\na result, the final decoded proposal representation can be\ndescribed as follows:\ny = [w(EC)\n1 · ˆV1, . . . ,w(EC)\nH · ˆVH], (7)\nwhere the value embeddings ˆV is the linear projection ob-\ntained from ˆX.\n3.4. Detect head and Training Targets\nIn the previous steps, the input point features are sum-\nmarized into a D-dimension vector y, which is then fed into\ntwo FFNs for predicting the confidence and the box residu-\nals relative to the input 3D proposal, respectively.\nTo output the confidence, training targets are set as the\n3D IoU between the 3D proposals and their corresponding\nground-truth boxes. Given the IoU of the 3D proposal and\nits corresponding ground-truth box, we follow [11, 25, 24]\nto assign the confidence prediction target, which is shown\nas:\nct = min\n\u0012\n1, max\n\u0000\n0, IoU − αB\nαF − αB\n\u0001\u0013\n, (8)\nwhere αF and αB are the foreground and background IoU\nthresholds, respectively. Besides, regression targets (super-\nscript t) are encoded by proposals and their corresponding\nground-truth boxes (superscript g), given by:\nxt = xg − xc\nd , yt = yg − yc\nd , zt = zg − zc\nhc ,\nlt = log (lg\nlc ), wt = log (wg\nwc ), ht = log (hg\nhc ),\nθt = θg − θc, (9)\nwhere d =\np\n(lc)2 + (wc)2 is the diagonal of the base of\nthe proposal box.\n3.5. Training Losses\nWe adopt an end-to-end strategy to train CT3D. Hence,\nthe overall training loss is the summation of the RPN loss,\nthe confidence prediction loss, and the box regression loss,\nwhich is presented:\nL = LRPN + Lconf + Lreg. (10)\nHere, the binary cross entropy loss [11, 35] is exploited\nfor the predicted confidence c to compute the IoU-guided\nconfidence loss:\nLconf = −ct log (c) − (1 − ct) log (1− c). (11)\nMoreover, the box regression loss [35, 33] adopts:\nLreg = I(IoU ≥ αR)\nX\nµ∈x,y,z,l,w,h,θ\nLsmooth-L1(µ, µt), (12)\nwhere I(IoU ≥ αR) indicates that only proposals with\nIoU ≥ αR contribute to the regression loss.\n4. Experiments\nIn this section, we evaluate our CT3D on two public\ndatasets, KITTI [7] and Waymo [18, 42]. Furthermore, we\nconduct comprehensive ablation studies to verify the effec-\ntiveness of each module in CT3D.\n4.1. Dataset\nKITTI Dataset. KITTI dataset officially contains 7,481\ntraining LiDAR samples and 7,518 testing LiDAR samples.\nFollowing the previous work [2], we split the original train-\ning data into 3,712 training samples and 3,769 validation\nsamples for experimental studies.\nWaymo Dataset. Waymo dataset consists of 798 training\nsequences with around 158,361 LiDAR samples, and 202\nvalidation sequences with 40,077 LiDAR samples. This\nlarge-scale Waymo dataset detection task is more challeng-\ning due to its various autonomous driving scenarios [42].\nMethod Par. 3D Detection - Car\n(M) Easy Mod. Hard\nLiDAR & RGB\nMV3D, CVPR 2017 [3] - 74.97 63.63 54.00\nContFuse, ECCV 2018 [17] - 83.68 68.78 61.67\nA VOD-FPN,IROS 2018 [12] - 83.07 71.76 65.73\nF-PointNet, CVPR 2018 [21] 40 82.19 69.79 60.59\nUberATG-MMF,CVPR 2019 [16] - 88.40 77.43 70.22\n3D-CVF at SPA, ECCV 2020 [38] - 89.20 80.05 73.11\nCLOCs, IROS 2020 [20] - 88.94 80.67 77.15\nLiDAR only\nSECOND, Sensor 2018 [33] 20 83.34 72.55 65.82\nPointPillars, CVPR 2019 [13] 18 82.58 74.31 68.99\nSTD, ICCV 2019 [37] - 87.95 79.71 75.09\nPointRCNN, CVPR 2019 [25] 16 86.96 75.64 70.70\n3D IoU Loss, 3DV 2019 [40] - 86.16 76.50 71.39\nPart-A2, PAMI 2020 [26] 226 87.81 78.49 73.51\nSA-SSD, CVPR 2020 [10] 40.8 88.75 79.79 74.16\n3DSSD, CVPR 2020 [36] - 88.36 79.57 74.55\nPV-RCNN, CVPR 2020 [24] 50 90.25 81.43 76.82\nV oxel-RCNN,AAAI 2021 [4] 28 90.90 81.62 77.06\nCT3D (Ours) 30 87.83 81.77 77.16\nTable 1. Performance comparisons with state-of-the-art methods\non the KITTI test set. All results are reported by the average pre-\ncision with 0.7 IoU threshold and 40 recall positions.\n4.2. Implementation Details\nRPN. We adopt SECOND [33] as our RPN due to its\nhigh-quality proposals and fast speed of inference. For\nthe KITTI dataset, the X, Y, Zaxis ranges are set as\n(0, 70.4), (−40, 40), (−3, 1), and the voxel size is set\nas (0.05m, 0.05m, 0.1m) in (X-axis, Y-axis, Z-axis). For\nthe Waymo dataset, the corresponding axis ranges are\n(−75.2, 75.2), (−75.2, 75.2), (−2, 4), and the voxel size is\n(0.1m, 0.1m, 0.15m). LRPN consists of the Focal-Loss clas-\nsification branch and the Smooth-L1-Loss based regression\nbranch. Please refer to OpenPCDet [30] for more details\nsince we conduct our experiments with this toolbox.\nTraining Details.We use 8 V100 GPUs to train the entire\nnetwork with batch size 24 for the KITTI dataset and batch\nsize 16 for Waymo dataset. For the encoder and decoder\nmodules of channel-wise transformer, we set α = 1.2 and\nH = 4 . For training targets, we set αF = 0 .75, αB =\n0.25, αR = 0 .55, respectively. The whole CT3D frame-\nwork is trained end-to-end from scratch with ADAM op-\ntimizer for 100 epochs. We adopt cosine annealing learn-\ning rate strategy for our learning rate decay, and the max-\nimum of leaning rate is 0.001. In the training stage, only\n128 proposals are randomly selected to calculate the confi-\ndence loss while 64 (IoU ≥ αR) proposals are selected to\ncalculate the regression loss. In the inference stage, top-100\nproposals are selected for the final prediction.\n4.3. Detection Results on the KITTI Dataset\nWe compare our CT3D with state-of-the-art methods on\nboth the KITTI test and val sets with 0.7 IoU threshold. For\nMethod 3D Detection - Car\nEasy Mod. Hard\nLiDAR & RGB\nMV3D, CVPR 2017 [3] 71.29 62.68 56.56\nContFuse, ECCV 2018 [17] - 73.25 -\nA VOD-FPN,IROS 2018 [12] - 74.44 -\nF-PointNet, CVPR 2018 [21] 83.76 70.92 63.65\n3D-CVF at SPA, ECCV 2020 [38] 89.67 79.88 78.47\nLiDAR only\nSECOND, Sensor 2018 [33] 88.61 78.62 77.22\nPointPillars, CVPR 2019 [13] 86.62 76.06 68.91\nSTD, ICCV 2019 [37] 89.70 79.80 79.30\nPointRCNN, CVPR 2019 [25] 88.88 78.63 77.38\nSA-SSD, CVPR 2020 [10] 90.15 79.91 78.78\n3DSSD, CVPR 2020 [36] 89.71 79.45 78.67\nPV-RCNN, CVPR 2020 [24] 89.35 83.69 78.70\nV oxel-RCNN,AAAI 2021 [4] 89.41 84.52 78.93\nCT3D (Ours) 89.54 86.06 78.99\nTable 2. Performance comparisons with state-of-the-art methods\non the KITTI val set. All results are reported by the average preci-\nsion with 0.7 IoU threshold and 11 recall positions.\nIoU BEV Detection 3D Detection\nThr. Easy Mod. Hard Easy Mod. Hard\n0.7 96.14 91.88 89.63 92.85 85.82 83.46\nTable 3. Performance of our CT3D on the KITTI val set with AP\ncalculated by 40 recall positions for car category.\nIoU Pedestrian Cyclist\nThr. Easy Mod. Hard Easy Mod. Hard\n0.5 65.73 58.56 53.04 91.99 71.60 67.34\nTable 4. Performance for pedestrian and cyclist on the KITTI.\nour test submission, all the released training data is used\nto train the model. Following [25, 24, 4, 10], the average\nprecision (AP) for test set is calculated with 40 recall posi-\ntions, while the AP for val set is calculated with 11 recall\npositions when compared to the previous methods1.\nPerformance Comparisons.Table 1 illustrates the perfor-\nmance comparisons between our method and state-of-the-\nart methods on the official KITTI test server. It shows\nCT3D achieves the best performance on moderate and\nhard levels for car detection on both LiDAR only and Li-\ndar&RGB modalities, especially for the most important\nmoderate level [8]. Compared with the newest released PV-\nRCNN which shares the same RPN (i.e., SECOND) as ours,\nCT3D achieves better performance while requiring about\n1/3 times of parameters for refinement. Besides, as shown\nin Figure 4, CT3D presents much better visualization per-\nformance as compared to the PV-RCNN. This significant\nimprovement mainly comes from the fact that CT3D pro-\ncesses the raw points in refinement stage rather than relying\n1The setting of AP calculation is modified from 11 recall positions to\n40 recall positions on 08.10.2019. For fair comparison with previous meth-\nods, we exploit the 11 recall setting on val set.\nDifficulty Method 3D Detection - Vehicle BEV Detection - Vehicle\nOverall 0-30m 30-50m 50m-Inf Overall 0-30m 30-50m 50m-Inf\nLEVEL 1\nPointPillar, CVPR 2019 [13] 56.62 81.01 51.75 27.94 75.57 92.10 74.06 55.47\nMVF, CoRL 2020 [42] 62.93 86.30 60.02 36.02 80.40 93.59 79.21 63.09\nPillar-OD, arXiv 2020 [32] 69.80 88.53 66.50 42.93 87.11 95.78 84.87 72.12\nPV-RCNN, CVPR 2020 [24] 70.30 91.92 69.21 42.17 82.96 97.35 82.99 64.97\nV oxel-RCNN,AAAI 2021 [4] 75.59 92.49 74.09 53.15 88.19 97.62 87.34 77.70\nCT3D (Ours) 76.30 92.51 75.07 55.36 90.50 97.64 88.06 78.89\nLEVEL 2\nPV-RCNN, CVPR 2020 [24] 65.36 91.58 65.13 36.46 77.45 94.64 80.39 55.39\nV oxel-RCNN,AAAI 2021 [4] 66.59 91.74 67.89 40.80 81.07 96.99 81.37 63.26\nCT3D (Ours) 69.04 91.76 68.93 42.60 81.74 97.05 82.22 64.34\nTable 5. Performance comparisons with state-of-the-art methods on the Waymo dataset with 202 validation sequences (∼ 40k samples) for\nvehicle detection.\non human-specified designs and sub-optimal intermediate\nfeatures. Note the AP on easy level of our CT3D is com-\nparatively worse, there might be two reasons. First, we only\nsample 256 raw point within each proposal for all levels\neven the proposals in easy level usually have a much larger\nnumber of points. Second, we observe that KITTI exhibits\nlarge distribution differences between trainval and test sets.\nFor further validation, we conduct comparisons with pre-\nvious methods on the KITTIval set. It shows that our CT3D\noutperforms all the other methods with a large margin, lead-\ning the state-of-the-art method V oxel-RCNN by 1.54% on\nmoderate level, and achieves the competitive result on easy\nlevel. This improvement also verifies the effectiveness of\nour method, indicating our CT3D could better model the\ncontext information and dependencies as compared to the\nmethods based on multi-scale feature fusion. Our model\ncan also achieve strong performance on pedestrian and cy-\nclist detection. The car-BEV ,pedestrian-3D and cyclist-3D\nresults are presented in Table 3 and Table 4 for reference.\n4.4. Detection Results on the Waymo Dataset\nAs for the Waymo dataset, we train our model on the\ntraining set and evaluate it on the validation set. Like-\nwise, the mAP is calculated with 0.7 IoU threshold for ve-\nhicle detection. The data is split into two difficulty levels:\nLELVEL 1 denotes objects containing more than 5 points ,\nLELVEL 2 denotes objects containing 1 ∼ 5 points.\nPerformance Comparisons. In Table 5, we compare our\nCT3D with state-of-the-art methods based on official re-\nleased evaluation tools [29]. It can be seen that our method\noutperforms all previous methods with remarkable margins\non all distance ranges of interest in both LEVEL 1 and\nLEVEL 2. CT3D achieves 76.30% for the commonly used\nLEVEL 1 3D mAP evaluation metric, surpassing previous\nstate-of-the-art method V oxel-RCNN by 0.71% on 3D de-\ntection, and 2.31% on bird-view detection. This significant\nimprovement also verifies the effectiveness of our CT3D\napproach on large-scale point cloud feature representation.\nWe report the results of LEVEL 2 difficulty in Table 5, our\nmethod outperforms V oxel-RCNN significantly by 2.45%\non 3D detection. A contributing factor is that V oxel-RCNN\nlimits the feature interactions via dividing the RoI space into\ngrids, while our proposed CT3D has the obvious advantage\nof capturing long-range interactions among sparse points.\n4.5. Ablation Studies\nIn this section, we conduct comprehensive ablation stud-\nies for the CT3D to verify the effectiveness of each individ-\nual component. We report the 3D detection AP metric with\n40 recall positions on the KITTI val set.\nDifferent RPN Backbones.In Table 6, we validate the ef-\nfects of our refinement network with “SECOND RPN [33],”\nand “PointPillar RPN [13]”, respectively. It can be seen that\nthe detection performance boosts with +5.47% and +4.82%\nwhen compared to the RPN baselines. This benefits from\nthat our two-stage framework CT3D could be integrated on\nthe top of any RPNs to provide strong ability for proposal\nrefinement. We also provide the amount of parameters in\nTable 6 for reference.\nProposal-to-point Embedding.We investigate the impor-\ntance of the keypoints subtraction strategy by comparing\nit with the baseline size-orientation strategy adopted in the\nproposal-to-point embedding of Sec. 3.2. The 2nd and 3rd\nrows of Table 7 show that keypoints subtraction approach\nsignificantly improves the performance in all three difficulty\nlevels. The rationale behind this strategy is that the relative\ncoordinates between each point and the proposal keypoints\ncould provide more effective geometric information, form-\ning high-quality point location embeddings.\nSelf-attention Encoding.The 1st and 3rd rows of Table 7\nshow that removing the self-attention encoding drops per-\nformance a lot, which demonstrates that the self-attention\nenables better feature representation for each point by ag-\ngregating the global-aware context information and depen-\ndencies. Moreover, we visualize the attention maps of the\nlast self-attention layer of a trained model from different\nepoch checkpoints. As shown in Figure 5, the points on\ncars get more attention in epoch 80, even in an extremely\nsparse case as Figure 5 (c). On the contrary, the background\npoints get less attention with the training process. There-\nPV-RCNN CT3D \nFigure 4. Qualitative comparison results of 3D object detection on the KITTI test set. Our CT3D enables more reasonable and accurate\ndetection as compared to the PV-RCNN.\nAttention Maps LiDAR View \n(a) (b) (c) \nepoch 30 \nepoch 80 \na\nb\nc\n b\nc\nFigure 5. Attention maps generated by the self-attention layer. We\nvisualize the weights of at most 256 sampled points within 3 RoIs\n(red dotted line) as the 30-th and 80-th epochs.\nfore, CT3D pays more attention to foreground points, and\nthus achieves considerable performance.\nChannel-wise Decoding.As shown in the 3rd, 4th and 5th\nrows of Table 7, the extended channel-wise re-weighting\noutperforms both the standard decoding and channel-wise\nre-weighting with a large margin. This benefits from the\nintegration of the standard decoding and the channel-wise\nre-weighting for both global and channel-wise local aggre-\ngation, generating more effective decoding weights.\n5. Conclusion\nIn this paper, we present a two-stage 3D object detec-\ntion framework CT3D with a novel channel-wise Trans-\nformer architecture. Our method first encodes the proposal\ninformation into each raw point via an efficient proposal-\nPointPillar SECOND Two-stage Par. Moderate AP (%)RPN RPN refinement (M)\n✓ 18 79.26\n✓ ✓ 28 84.08\n✓ 20 80.35\n✓ ✓ 30 85.82\nTable 6. Ablation studies for different RPNs on the KITTI val set\nin terms of 3D detection AP metric with 40 recall positions.\nK. S. S. E. S. D. C. R. E. C. R Easy Mod. Hard\n✓ ✓ 90.29 79.20 74.59\n✓ ✓ 91.92 83.41 81.79\n✓ ✓ ✓ 92.09 85.10 82.98\n✓ ✓ ✓ 92.56 85.34 83.23\n✓ ✓ ✓ 92.85 85.82 83.46\nTable 7. Ablation studies for proposal-to-point embedding, self-\nattention encoding and channel-wise decoding on the KITTI val\nset. “K. S.” stands for the keypoints subtraction strategy, “S. E.”\nstands for the self-attention encoding, “S. D.”, “C. R.” and “E. C.\nR.” represent the standard decoding, channel-wise re-weighting,\nand our extended channel-wise re-weighting, respectively.\nto-point embedding, followed by self-attention to capture\nthe long-range interactions among points. Subsequently,\nwe transform the encoded point features into a global\nproposal-aware representation by an extended channel-wise\nre-weighting scheme which could obtain effective decod-\ning weights for all points. The CT3D provides a flexible\nand highly-effective framework which is particularly help-\nful for point cloud detection tasks. Experimental results on\nboth the KITTI dataset and the large-scale Waymo dataset\nalso verify that CT3D could achieve significant improve-\nment over the state-of-the-art methods.\nAcknowledgements\nThis work was supported by Alibaba Innovative Re-\nsearch (AIR) progamm and Major Scientifc Research\nProject of Zhejiang Lab (No. 2019DB0ZX01).\nReferences\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In Proceedings\nof the European Conference on Computer Vision (ECCV) ,\npages 213–229, 2020.\n[2] Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G\nBerneshawi, Huimin Ma, Sanja Fidler, and Raquel Urtasun.\n3d object proposals for accurate object class detection. Ad-\nvances in Neural Information Processing Systems (NIPS) ,\n28:424–432, 2015.\n[3] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia.\nMulti-view 3d object detection network for autonomous\ndriving. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR), pages 1907–\n1915, 2017.\n[4] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou,\nYanyong Zhang, and Houqiang Li. V oxel r-cnn: Towards\nhigh performance voxel-based 3d object detection. arXiv\npreprint arXiv:2012.15712, 2020.\n[5] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020.\n[6] Nico Engel, Vasileios Belagiannis, and Klaus Dietmayer.\nPoint transformer. arXiv preprint arXiv:2011.00931, 2020.\n[7] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel\nUrtasun. Vision meets robotics: The kitti dataset. The Inter-\nnational Journal of Robotics Research , 32(11):1231–1237,\n2013.\n[8] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we\nready for autonomous driving? the kitti vision benchmark\nsuite. In Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition (CVPR) , pages 3354–3361,\n2012.\n[9] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud\ntransformer. arXiv preprint arXiv:2012.09688, 2020.\n[10] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua,\nand Lei Zhang. Structure aware single-stage 3d object detec-\ntion from point cloud. In Proceedings of the IEEE Confer-\nence on Computer Vision and Pattern Recognition (CVPR) ,\npages 11873–11882, 2020.\n[11] Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, and Yun-\ning Jiang. Acquisition of localization confidence for accurate\nobject detection. In Proceedings of the European Conference\non Computer Vision (ECCV), pages 784–799, 2018.\n[12] Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh,\nand Steven L Waslander. Joint 3d proposal generation and\nobject detection from view aggregation. In 2018 IEEE/RSJ\nInternational Conference on Intelligent Robots and Systems\n(IROS), pages 1–8, 2018.\n[13] Alex H Lang, Sourabh V ora, Holger Caesar, Lubing Zhou,\nJiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders\nfor object detection from point clouds. In Proceedings of the\nIEEE Conference on Computer Vision and Pattern Recogni-\ntion (CVPR), pages 12697–12705, 2019.\n[14] Hei Law and Jia Deng. Cornernet: Detecting objects as\npaired keypoints. In Proceedings of the European Confer-\nence on Computer Vision (ECCV), pages 734–750, 2018.\n[15] Bo Li, Tianlei Zhang, and Tian Xia. Vehicle detection from\n3d lidar using fully convolutional network. arXiv preprint\narXiv:1608.07916, 2016.\n[16] Ming Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urta-\nsun. Multi-task multi-sensor fusion for 3d object detection.\nIn Proceedings of the IEEE Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 7345–7353, 2019.\n[17] Ming Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun.\nDeep continuous fusion for multi-sensor 3d object detection.\nIn Proceedings of the European Conference on Computer Vi-\nsion (ECCV), pages 641–656, 2018.\n[18] Jiquan Ngiam, Benjamin Caine, Wei Han, Brandon Yang,\nYuning Chai, Pei Sun, Yin Zhou, Xi Yi, Ouais Al-\nsharif, Patrick Nguyen, et al. Starnet: Targeted compu-\ntation for object detection in point clouds. arXiv preprint\narXiv:1908.11069, 2019.\n[19] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao\nHuang. 3d object detection with pointformer. arXiv preprint\narXiv:2012.11409, 2020.\n[20] Su Pang, Daniel Morris, and Hayder Radha. Clocs: Camera-\nlidar object candidates fusion for 3d object detection. In2018\nIEEE/RSJ International Conference on Intelligent Robots\nand Systems (IROS), 2020.\n[21] Charles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J\nGuibas. Frustum pointnets for 3d object detection from rgb-\nd data. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 918–\n927, 2018.\n[22] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas.\nPointnet: Deep learning on point sets for 3d classification\nand segmentation. In Proceedings of the IEEE conference\non computer vision and pattern recognition (CVPR) , pages\n652–660, 2017.\n[23] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J\nGuibas. Pointnet++: Deep hierarchical feature learning on\npoint sets in a metric space. Advances in Neural Information\nProcessing Systems (NIPS), 30:5099–5108, 2017.\n[24] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping\nShi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-\nvoxel feature set abstraction for 3d object detection. In Pro-\nceedings of the IEEE Conference on Computer Vision and\nPattern Recognition (CVPR), pages 10529–10538, 2020.\n[25] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-\ncnn: 3d object proposal generation and detection from point\ncloud. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 770–\n779, 2019.\n[26] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang,\nand Hongsheng Li. From points to parts: 3d object detection\nfrom point cloud with part-aware and part-aggregation net-\nwork. IEEE Transactions on Pattern Analysis and Machine\nIntelligence (PAMI), 2020.\n[27] Shuran Song and Jianxiong Xiao. Deep sliding shapes for\namodal 3d object detection in rgb-d images. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 808–816, 2016.\n[28] Hang Su, Subhransu Maji, Evangelos Kalogerakis, and Erik\nLearned-Miller. Multi-view convolutional neural networks\nfor 3d shape recognition. In Proceedings of the IEEE In-\nternational Conference on Computer Vision (ICCV) , pages\n945–953, 2015.\n[29] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien\nChouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou,\nYuning Chai, Benjamin Caine, et al. Scalability in perception\nfor autonomous driving: Waymo open dataset. In Proceed-\nings of the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 2446–2454, 2020.\n[30] OpenPCDet Development Team. Openpcdet: An open-\nsource toolbox for 3d object detection from point clouds.\nhttps://github.com/open-mmlab/OpenPCDet,\n2020.\n[31] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems (NIPS) , pages 5998–6008,\n2017.\n[32] Yue Wang, Alireza Fathi, Abhijit Kundu, David Ross, Car-\noline Pantofaru, Tom Funkhouser, and Justin Solomon.\nPillar-based object detection for autonomous driving. arXiv\npreprint arXiv:2007.10323, 2020.\n[33] Yan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embed-\nded convolutional detection. Sensors, 18(10):3337, 2018.\n[34] Bin Yang, Ming Liang, and Raquel Urtasun. Hdnet: Exploit-\ning hd maps for 3d object detection. In Conference on Robot\nLearning, pages 146–155, 2018.\n[35] Bin Yang, Wenjie Luo, and Raquel Urtasun. Pixor: Real-\ntime 3d object detection from point clouds. InProceedings of\nthe IEEE conference on Computer Vision and Pattern Recog-\nnition (CVPR), pages 7652–7660, 2018.\n[36] Zetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3dssd:\nPoint-based 3d single stage object detector. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 11040–11048, 2020.\n[37] Zetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Ji-\naya Jia. Std: Sparse-to-dense 3d object detector for point\ncloud. In Proceedings of the IEEE International Conference\non Computer Vision (ICCV), pages 1951–1960, 2019.\n[38] Jin Hyeok Yoo, Yecheol Kim, Ji Song Kim, and Jun Won\nChoi. 3d-cvf: Generating joint camera and lidar features us-\ning cross-view spatial feature fusion for 3d object detection.\narXiv preprint arXiv:2004.12636, 3, 2020.\n[39] Wu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, and Chi-\nWing Fu. Cia-ssd: Confident iou-aware single-stage object\ndetector from point cloud. arXiv preprint arXiv:2012.03015,\n2020.\n[40] Dingfu Zhou, Jin Fang, Xibin Song, Chenye Guan, Junbo\nYin, Yuchao Dai, and Ruigang Yang. Iou loss for 2d/3d ob-\nject detection. In 2019 International Conference on 3D Vi-\nsion (3DV), pages 85–94, 2019.\n[41] Xingyi Zhou, Jiacheng Zhuo, and Philipp Krahenbuhl.\nBottom-up object detection by grouping extreme and cen-\nter points. In Proceedings of the IEEE Conference on Com-\nputer Vision and Pattern Recognition (CVPR) , pages 850–\n859, 2019.\n[42] Yin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang\nGao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Va-\nsudevan. End-to-end multi-view fusion for 3d object detec-\ntion in lidar point clouds. In Conference on Robot Learning,\npages 923–932, 2020.\n[43] Yin Zhou and Oncel Tuzel. V oxelnet: End-to-end learning\nfor point cloud based 3d object detection. In Proceedings\nof the IEEE Conference on Computer Vision and Pattern\nRecognition (CVPR), pages 4490–4499, 2018.\n[44] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang\nWang, and Jifeng Dai. Deformable detr: Deformable trans-\nformers for end-to-end object detection. arXiv preprint\narXiv:2010.04159, 2020."
}