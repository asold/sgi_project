{
  "title": "German’s Next Language Model",
  "url": "https://openalex.org/W3114950584",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3093680596",
      "name": "Branden Chan",
      "affiliations": [
        "Bavarian State Library"
      ]
    },
    {
      "id": "https://openalex.org/A2944817359",
      "name": "Stefan Schweter",
      "affiliations": [
        "Bavarian State Library"
      ]
    },
    {
      "id": "https://openalex.org/A2100253249",
      "name": "Timo Möller",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2911227954",
    "https://openalex.org/W3032816972",
    "https://openalex.org/W4294152847",
    "https://openalex.org/W3013027210",
    "https://openalex.org/W2948902769",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2795038878",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W2802642435",
    "https://openalex.org/W2880875857",
    "https://openalex.org/W2986154550",
    "https://openalex.org/W4381683870",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3216234416",
    "https://openalex.org/W4308264370",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W4287760320",
    "https://openalex.org/W4287993739",
    "https://openalex.org/W2946119234",
    "https://openalex.org/W3009095382",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2989539713",
    "https://openalex.org/W630532510",
    "https://openalex.org/W2995647371",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W3102391739",
    "https://openalex.org/W2950018712",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2483215953"
  ],
  "abstract": "In this work we present the experiments which lead to the creation of our BERT and ELECTRA based German language models, GBERT and GELECTRA. By varying the input training data, model size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA performance across a set of document classification and named entity recognition (NER) tasks for both models of base and large size. We adopt an evaluation driven approach in training these models and our results indicate that both adding more data and utilizing WWM improve model performance. By benchmarking against existing German models, we show that these models are the best German models to date. All trained models will be made publicly available to the research community.",
  "full_text": "Proceedings of the 28th International Conference on Computational Linguistics, pages 6788–6796\nBarcelona, Spain (Online), December 8-13, 2020\n6788\nGerman’s Next Language Model\nBranden Chan∗†, Stefan Schweter∗‡, Timo M¨oller†\n†deepset\n{branden.chan, timo.moeller}@deepset.ai\n‡Bayerische Staatsbibliothek M¨unchen\nDigital Library/Munich Digitization Center\nstefan.schweter@bsb-muenchen.de\nAbstract\nIn this work we present the experiments which lead to the creation of our BERT and ELECTRA\nbased German language models, GBERT and GELECTRA. By varying the input training data,\nmodel size, and the presence of Whole Word Masking (WWM) we were able to attain SoTA\nperformance across a set of document classiﬁcation and named entity recognition (NER) tasks\nfor both models of base and large size. We adopt an evaluation driven approach in training these\nmodels and our results indicate that both adding more data and utilizing WWM improve model\nperformance. By benchmarking against existing German models, we show that these models\nare the best German models to date. Our trained models will be made publicly available to the\nresearch community.\n1 Introduction\nDeep transformer based language models have shown state-of-the-art results for various Natural Lan-\nguage Processing tasks like text classiﬁcation, NER and question answering (Devlin et al., 2019). They\nare pretrained, ﬁrst by feeding in large unlabeled text corpora before being ﬁne-tuned on the down-\nstream task. In this work we present a set of German BERT and ELECTRA models, the best of which,\nGELECTRALarge, signiﬁcantly improves upon state of the art performance on the GermEval18 hate\nspeech detection task by about +4% / +2.5% for the coarse and ﬁne variants of the task respectively.\nThis model also reaches SoTA on the GermEval14 NER task, outperforming the previous best by over\n+4%. While performant, such models are prohibitively large for many and so we also present a new\nGBERT model which matches deepset BERT, the previous best German BERT, in size but outperforms\nit by +2.23% F1 averaged over three tasks.\nIn the process of pretraining the language models, we also a) quantify the effect of increasing the\ntraining data by an order of magnitude and b) verify that whole word masking has a positive effect on\nBERT models.\nBecause of the computational expense of training large language models from scratch, we adopt a\ndownstream-oriented evaluation approach to ensure that we get the best performance from a limited\nnumber of runs. This involves regularly checkpointing the model over the course of pretraining, evalu-\nating these on a set of classiﬁcation and NER tasks and selecting as ﬁnal the checkpoint which shows\nthe best performance. This stands in contrast to approaches where the ﬁnal model is simply saved after a\nﬁxed number of steps. Our method is also an important tool in diagnosing pretraining and we hope that\nit will be of use to other teams looking to train effective language models on a budget.\n2 Related work\nModern language model architectures are trained to build word representations that take into consid-\neration the context around a given word. First versions such as ELMo (Peters et al., 2018), ULMFiT\nThis work is licensed under a Creative Commons Attribution 4.0 International Licence. Licence details:\nhttp://creativecommons.org/licenses/by/4.0/.\n∗Equal contribution.\n6789\n(Howard and Ruder, 2018) and F LAIR (Akbik et al., 2018) are LSTM based and these were able to\nset new performance benchmarks on downstream tasks like text classiﬁcation, PoS tagging and NER.\nMore recent approaches use Transformer-based (Vaswani et al., 2017) architectures and examples in-\nclude GPT-2 (Radford et al., 2019), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), ALBERT\n(Lan et al., 2020) and ELECTRA (Clark et al., 2020).\nIn this work we focus on BERT and ELECTRA models. BERT uses a masked language modeling\n(MLM) strategy to corrupt an input sentence by replacing some tokens with a [MASK] symbol. The\nmodel is then trained to re-construct the original token. However, this method of training is somewhat\nrestricted in that the model only learns from the masked out tokens which typically make up about 15%\nof the input tokens.\nELECTRA addresses this problem by introducing a new pretraining task called Replaced Token de-\ntection. Instead of masking out tokens, a subset of the input tokens are substituted by a synthetically\ngenerated token. The model is then trained to classify whether each input token is original or substituted,\nthus allowing for gradient updates at every input position. Practically speaking, this is achieved by hav-\ning a discriminator that performs the replaced token detection and a generator which provides plausible\ntoken substitutes. These two components are trained jointly and are both Transformer based.\nThe BERT model received an update when the original authors added Whole Word Masking1 whereby\nmasking one subword token requires that all other tokens in the word are also masked out. The authors\nreport that this method improves the training signal by removing the easiest cases and show that it im-\nproves performance in their tasks.\nThere is also a line of work that looks into bringing language modeling techniques that were ﬁrst\ndeveloped on English to other languages. These include but are not limited to monolingual models\nsuch as CamemBERT (Martin et al., 2020) and FlauBERT (Le et al., 2020) for French, Finnish BERT\n(Virtanen et al., 2019) and German BERTs by DBMDZ 2 and deepset3. For a more comprehensive list,\nsee (Nozza et al., 2020).\nSome models are also capable of supporting multiple languages such as multilingual BERT\n(mBERTBase) and XLM-RoBERTa (Conneau et al., 2019). Multilingual BERT is a multilingual model\nfor 104 different languages 4 trained on Wikipedia dumps. The XLM-RoBERTa model is trained on\n2.5TB of data from a cleaned Common Crawl corpus (Wenzek et al., 2020) for 100 different languages.\nIt is worth emphasizing here that systems trained on naturally occurring data will learn pre-existing\ncultural biases around gender (Bolukbasi et al., 2016), race and religion (Speer, 2017). Critical eval-\nuation of machine learning methods is more important than ever as NLP is gaining broader adoption.\nResearchers have been advocating for better documentation of decisions made during the construction of\na dataset (Gebru et al., 2018), explicit statements of a dataset’s “ingredients” (Holland et al., 2018) and\nrecognition of the dataset characteristics that may lead to exclusion, overgeneralisation and underexpo-\nsure (Bender and Friedman, 2018). These topics will be addressed in Section 3.1.\n3 Datasets\n3.1 Pretraining Data\nWe have available to us, a range of different German language corpora that we use in different combi-\nnations for our model pretraining. OSCAR (Ortiz Su ´arez et al., 2019) is a set of monolingual corpora\nextracted from Common Crawl. The Common Crawl texts are pre-processed (e.g. HTML entities are\nremoved) and a language classiﬁcation model is used to sort texts by language. We use the unshufﬂed\nversion of the German OSCAR corpus, resulting in 145GB of text. The Wikipedia dump for German is\npreprocessed with the WikiExtractor5 script forming a corpus of size 6GB. The OPUS project 6 (Tiede-\nmann, 2012) has collected texts from various domains such as movie subtitles, parliament speeches and\n1https://github.com/google-research/bert/commit/0fce551\n2https://github.com/dbmdz/berts\n3https://deepset.ai/german-bert\n4https://github.com/google-research/bert/blob/f39e88/multilingual.md\n5https://github.com/attardi/wikiextractor\n6http://opus.nlpl.eu\n6790\nDataset Size\nOSCAR 145\nOPUS 10\nWikipedia 6\nOpenLegalData 2.4\nTable 1: The size of each dataset in gigabytes.\nbooks and these comprise a collection of around 10GB. From Open Legal Data7 (Ostendorff et al., 2020)\nthere is a dataset of about 2.4GB of German court decisions. Table 1 shows an overview over all datasets.\nAs discussed in Section 2, our pretrained language models will learn pre-existing biases from the\ntraining datasets. The main portion (89%) of our training data, namely the OSCAR dataset, uses texts\nscraped from the internet, which is in some respects problematic. First off, this dataset contains a lot of\nexplicit and indecent material. While we ﬁltered out many of these documents through keyword match-\ning, we cannot guarantee that this method was successful in every case. Furthermore, many websites\ncontain unveriﬁed information and any dataset containing this kind of text can lead to a skewed model\nthat reﬂects commonly found lies and misconceptions. This includes gender, racial and religious biases\nwhich are found in textual data of all registers and so we advise that anyone using our model to recognise\nthat it will not always build true and accurate representation of real world concepts. We implore users\nof the model to seriously consider these issues before deploying it in a production setting, especially\nin situations where impartiality matter, such as journalism, and institutional decision making like job\napplications or insurance assessments.\n3.2 Downstream Data\n3.2.1 GermEval18\nFor text classiﬁcation we use GermEval18 (Coarse) and GermEval18 (Fine) which are both hate speech\nclassiﬁcation tasks (Wiegand et al., 2018). GermEval18 (Coarse) requires a system to classify a tweet\ninto one of two classes:OFFENSE if the tweet contains some form of offensive language, andOTHER if it\ndoes not. GermEval18 (Fine) extends the coarse-grained task and contains four classes: OTHER for non-\noffensive tweets as well as PROFANITY, INSULT and ABUSE which are all subclasses of OFFENSE\nfrom the coarse variant of the task.\n3.2.2 GermEval14\nFor NER, we use the GermEval14 (Benikova et al., 2014) shared task. The data is sampled from German\nWikipedia and News Corpora and contains over 31,000 sentences and 590,000 tokens. The dataset is\none of the largest NER datasets for German and features an advanced annotation schema that allows for\nnested annotations. The four main classes ( PERSON, ORGANISATION, LOCATION and OTHER) each\nhave part and derivative variants (e.g. LOCpart or PERderiv) resulting in 12 classes in total.\n4 Training\n4.1 Method\nTo train our German BERT and ELECTRA we use the Tensorﬂow training scripts from the ofﬁcial\nrepositories8. We train models that match the size of the original BERT Base, BERTLarge, ELECTRABase\nand ELECTRALarge. The hyperparameters used for training can be found in Table 2. The base models\nwere trained on single Google Cloud TPUs v3 (8 cores) while large models were trained on pods of 16\nTPUs v3 (128 cores).\n7http://openlegaldata.io/research/2019/02/19/court-decision-dataset.html\n8https://github.com/google-research/bert and https://github.com/google-research/\nelectra\n6791\nGBERTBase GBERTLarge GELECTRABase GELECTRALarge\nmax sequence length 512 512 512 512\nbatch size 128 2048 256 1024\nwarmup steps (k) 10 10 10 30\nlearning rate 1e-04 1e-04 2e-04 2e-4\ncheckpoint every (k) 100 100 76.6 100\nmax train steps (k) 4000 1000 766 1000\nlayers 12 24 12 24\nhidden states 768 1024 768 1024\nattention heads 12 16 12 16\nvocab size (k) 31 31 31 31\ntrain time (days) 7 11 8 7\nTable 2: Hyperparameters for language model pretraining.\n4.2 Models\nIn total, we trained 7 separate models with different combinations of data and model size as well as\nWhole Word Masking (WWM) for BERT models. The German DBMDZ BERT Base, is the same size\nas BERTBase and was trained using the OPUS and Wikipedia corpora. It serves as our baseline model.\nWe train four BERT variants of it, each referred to as GBERT, each using the same cased vocabulary as\nDBMDZ BERTBase. These match BERT Base in size unless they have the ”Large” sufﬁx, in which case\nthey match BERTLarge:\n•GBERTData - trained on all available data without Whole Word Masking\n•GBERTWWM - trained on the same data as DBMDZ BERTBase but uses Whole Word Masking\n•GBERTData + WWM - trained on all available data and uses Whole Word Masking\n•GBERTLarge - trained on all available data and uses Whole Word Masking\nWe also trained three ELECTRA variants of DBMDZ BERT Base, each referred to as GELECTRA\nmodels, which also match the size of the original ELECTRA Base unless they have the ”Large” sufﬁx in\nwhich case they match ELECTRALarge:\n•GELECTRA - trained on same data as DBMDZBase BERT\n•GELECTRAData - trained on all available data\n•GELECTRALarge - trained on all available data\nThe best models of each architecture and size are uploaded to the Hugging Face model hub 9 as\ndeepset/gbert-base, deepset/gbert-large, deepset/gelectra-base and deepset/gelectra-large.\n5 Evaluation\nIn our approach, models are evaluated continuously during pretraining. Model checkpoints are saved at\nregular intervals and converted into PyTorch models using Hugging Face’s Transformers library (Wolf\net al., 2019). Using the FARM framework 10, we evaluate the performance of each checkpoint on Ger-\nmEval18 (Coarse) and GermEval18 (Fine) which are both hate speech classiﬁcation tasks (Wiegand et\nal., 2018). Using Hugging Face’s Transformers we also evaluate on GermEval14 (Benikova et al., 2014)\nwhich is a NER task.\n9https://huggingface.co/models\n10https://github.com/deepset-ai/FARM\n6792\nGermEval18 (Coarse) GermEval18 (Fine) GermEval14\nType Classiﬁcation Classiﬁcation NER\nTrain Samples 4509 4509 24002\nDev Samples 501 501 2200\nTest Samples 3533 3533 5100\nClasses 2 4 12\nMax Epochs 5 5 3\nMax Train Steps 705 705 4500\nEvaluation Every 50 steps 50 steps 1500 steps\nLearning Rate 5e-06 5e-06 5e-05\nBatch Size 32 32 16\nMax Seq Len 150 150 128\nMetric F1 (macro) F1 (macro) F1 (micro)\nTable 3: Details of the downstream tasks and hyperparameters for model ﬁnetuning for all three tasks.\nIn BERT, the vector corresponding to the [CLS] token serves as a representation of the whole input\nsequence, while in ELECTRA, all word vectors are combined through a feed forward layer. In both\ncases, this input sequence representation is passed through a single layer Neural Network in order to\nperform prediction. In the NER task, each vector corresponding to the ﬁrst token in a word is passed\nthrough a single layer Neural Network and the resulting prediction is applied to the whole word.\nEach checkpoint is evaluated 3 times on each document classiﬁcation task since we observed signif-\nicant variance across different runs. Each of these runs is performed with early stopping and a differ-\nent seed each time. For NER, the model is evaluated just once without early stopping. The reported\nperformance is the average of the single best run for GermEval18 (Coarse), GermEval18 (Fine) and\nGermEval14. Table 3 summarizes the most important details and parameters of each task. For all exper-\niments, we use an Nvidia V100 GPU to accelerate training. For each model, we choose the checkpoint\nthat shows the best performance.\nFor comparison, we also run this evaluation pipeline on the two publicly available German BERT\nmodels (deepset German BERT Base and DBMDZ German BERT Base) as well as multilingual models\nsuch as mBERTBase and XLM-RoBERTaLarge.\n6 Results\nThe downstream performance graphs in Figure 1 show that the models are capable of learning with most\nof the gains being made in the ﬁrst phase of training and more incremental gains coming later. The best\ncheckpoints come at different points for different models as can be seen in Table 4.\nIn Table 5 are the evaluation results for each model’s best checkpoint for each of the three downstream\ntasks with comparison to benchmark models and previous SoTA results. For GermEval18, results from\nthe best-performing systems are reported (Wiegand et al., 2018). For GermEval14 we report the result\nthat can be achieved using the FLAIR framework (Akbik et al., 2019).\nSteps (k)\nGBERTData 3900\nGBERTWWM 1500\nGBERTData + WWM 2000\nGBERTLarge 900\nGELECTRA 766\nGELECTRAData 766\nGELECTRALarge 1000\nTable 4: Best checkpoint of each trained model.\n6793\nFigure 1: The F1 performance of each model averaged over the three downstream tasks over the course\nof language model pretraining.\nIn GermEval18 (Coarse), GBERTData + WWM, XLM-RobertaLarge, GBERTLarge and GELECTRALarge\nall improve upon the previous SoTA. GELECTRALarge does so with the largest margin reaching a score\nthat is +3.93% better. In GermEval18 (Fine), XLM-Roberta Large beats the previous best by +1.39% and\nGELECTRALarge sets a new SoTA that is better than the previous by +2.45%. In GermEval14, all 7\ntrained models exceed the previous SoTA, with GELECTRA Large showing a +4.3% improvement over\nthe previous best.\nThese results indicate that adding extra data gives a consistent but modest performance boost to our\nlanguage models. GBERT Data outperforms DBMDZ BERTBase by +0.25%, GBERTData + WWM outper-\nforms GBERTWWM by +0.93% and GELECTRA Data outperforms GELECTRA by +1.59%. For the\nBERT models, Whole Word Masking also shows a consistent positive impact with GBERTWWM outper-\nforming DBMDZ BERTBase by +1.70% and GBERTData + WWM outperforming GBERTData by +2.38%.\n7 Discussion\n7.1 Model Size\nThe large models that we train show much stronger performance than the base models. GBERTLarge out-\nperforms GBERTData + WWM by +2.33% averaged F1 and GELECTRALarge outperforms GELECTRAData\n6794\nParams GermEval18 (Coarse)GermEval18 (Fine) GermEval14 Averaged F1\nDBMDZ BERTBase 110m 75.23 47.39 87.90 69.72\ndeepset BERTBase 110m 74.7 48.8 86.87 70.12\nmBERTBase 172m 70.00 45.20 87.44 67.55\nXLM-RobertaLarge 550m 78.38 54.1 87.07 73.18\nGBERTData 110m 74.51 48.01 87.41 69.97\nGBERTWWM 110m 76.48 49.99 87.80 71.42\nGBERTData + WWM 110m 78.17 50.90 87.98 72.35\nGBERTLarge 335m 80.08 52.48 88.16 73.57\nGELECTRA 110m 76.02 42.22 86.02 68.09\nGELECTRAData 110m 76.59 46.28 86.02 69.63\nGELECTRALarge 335m 80.70 55.16 88.95 74.94\nPrevious SoTA 76.77 (TU Wien) 52.71 (uhhLT) 84.65 (FLAIR)\nTable 5: Downstream evaluation results for the best checkpoints of each GBERT and GELECTRA model\ncompared to a set of benchmark models. For GermEval18 we report scores for the best-performing\nsystems (Wiegand et al., 2018), and the result reported by F LAIR framework (Akbik et al., 2019) for\nGermEval14.\nby +5.31%. It must be noted however, that their differing training regimes mean that the large models\nare trained on many more tokens than their base counterparts. In future, we would also be interested in\ntraining larger models with less data in order to better quantify the gains that come from model size and\nthe gains that come from the extra data.\n7.2 Training Length\nFrom the downstream evaluation graphs in Figure 1, it is clear that the models gain most of their perfor-\nmance after a relatively short amount of training steps. GBERT WWM and GBERTData + WWM both show\nan upward trend in the second half of model training suggesting they could still beneﬁt from continuing\ntraining. There is also a clear upward trend over the course of GELECTRA and GELECTRAData’s train-\ning suggesting these models are undertrained. It should also be noted that none of the models exhibit any\nclear signs of overﬁtting or performance degradation and may improve with further training.\n7.3 ELECTRA Efﬁciency\nOne of the central claims of the ELECTRA paper is that it is capable of learning more efﬁciently\nthan MLM based Language Models. This is exempliﬁed by the comparison of GBERT Large and\nGELECTRALarge. By the end of their 1 million steps of training, GELECTRA Large has only seen half\nthe number of tokens that GBERT Large due to its smaller batch size and yet outperforms it by +1.47%\naveraged F1.\n7.4 Instabilities\nThe dip in performance around 2 million steps for the base sized GBERT models (See Figure 1) happens\nto coincide with our training regime whereby the model training is stopped, saved and then reloaded at 2\nmillion steps. While we suspect that these two events are related, it was beyond the scope of this project\nto investigate the exact reasons.\n8 Conclusion\nThe set of German models which we trained vary in terms of training regime and model architecture. We\nhope that the results that we present here will serve as important data points to other NLP practitioners\nwho are looking to train language models from scratch but are limited by compute. Our experiments\nshould give other teams a sense of the batch sizes and training lengths that make for efﬁcient model\ntraining. On top of this, we also present a set of GELECTRA and GBERT models which, according to\nour evaluations, set new SoTA performance for both large and base sized models on GermEval18 and\nGermEval14.\n6795\nAcknowledgements\nWe would like to thank the deepset team, especially Malte Pietsch and Tanay Soni for their regular\nsparring and their effort maintaining FARM. Thanks to Zak Stone, Jonathan Caton and everyone at the\nGoogle TensorFlow Research Cloud team for their advice and for providing us with the access to and\ncredits for the TPU pods that we used for pretraining. We would also like to thank Nikhil Dinesh from the\nAWS Activate program as well as Nvidia’s Inception program for providing us with the EC2 instances\nand credits that allowed us to do large scale evaluation of our models. Thanks also to Pedro Javier Ortiz\nSu´arez and the OSCAR corpus team for giving us access to their dataset. And thanks to Malte Ostendorff,\nco-founder of Open Justice e.V ., whose team created Open Legal Data.\nReferences\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018. Contextual string embeddings for sequence labeling. In\nCOLING 2018, 27th International Conference on Computational Linguistics, pages 1638–1649.\nAlan Akbik, Tanja Bergmann, Duncan Blythe, Kashif Rasul, Stefan Schweter, and Roland V ollgraf. 2019. FLAIR:\nAn easy-to-use framework for state-of-the-art NLP. In Proceedings of the 2019 Conference of the North Amer-\nican Chapter of the Association for Computational Linguistics (Demonstrations), pages 54–59, Minneapolis,\nMinnesota, June. Association for Computational Linguistics.\nEmily M. Bender and Batya Friedman. 2018. Data statements for natural language processing: Toward mitigating\nsystem bias and enabling better science. Transactions of the Association for Computational Linguistics, 6:587–\n604.\nDarina Benikova, Chris Biemann, Max Kisselew, and Sebastian Pad´o. 2014. Germeval 2014 named entity recog-\nnition: Companion paper. Proceedings of the KONVENS GermEval Shared Task on Named Entity Recognition,\nHildesheim, Germany, pages 104–112.\nTolga Bolukbasi, Kai-Wei Chang, James Y Zou, Venkatesh Saligrama, and Adam T Kalai. 2016. Man is to\ncomputer programmer as woman is to homemaker? debiasing word embeddings. In D. D. Lee, M. Sugiyama,\nU. V . Luxburg, I. Guyon, and R. Garnett, editors,Advances in Neural Information Processing Systems 29, pages\n4349–4357. Curran Associates, Inc.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and Christopher D. Manning. 2020. Electra: Pre-training text\nencoders as discriminators rather than generators. In International Conference on Learning Representations.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Unsupervised cross-\nlingual representation learning at scale.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirec-\ntional transformers for language understanding. In Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and\nShort Papers), pages 4171–4186, Minneapolis, Minnesota, June. Association for Computational Linguistics.\nTimnit Gebru, Jamie Morgenstern, Briana Vecchione, Jennifer Wortman Vaughan, Hanna Wallach, III Daum ´e,\nHal, and Kate Crawford. 2018. Datasheets for Datasets. arXiv e-prints, page arXiv:1803.09010, March.\nSarah Holland, Ahmed Hosny, Sarah Newman, Joshua Joseph, and Kasia Chmielinski. 2018. The Dataset Nu-\ntrition Label: A Framework To Drive Higher Data Quality Standards. arXiv e-prints, page arXiv:1805.03677,\nMay.\nJeremy Howard and Sebastian Ruder. 2018. Universal language model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long\nPapers), pages 328–339, Melbourne, Australia, July. Association for Computational Linguistics.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. 2020.\nAlbert: A lite bert for self-supervised learning of language representations. In International Conference on\nLearning Representations.\nHang Le, Lo¨ıc Vial, Jibril Frej, Vincent Segonne, Maximin Coavoux, Benjamin Lecouteux, Alexandre Allauzen,\nBenoit Crabb ´e, Laurent Besacier, and Didier Schwab. 2020. FlauBERT: Unsupervised language model pre-\ntraining for French. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 2479–\n2490, Marseille, France, May. European Language Resources Association.\n6796\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692.\nLouis Martin, Benjamin Muller, Pedro Javier Ortiz Su´arez, Yoann Dupont, Laurent Romary, ´Eric Villemonte de la\nClergerie, Djam´e Seddah, and Benoˆıt Sagot. 2020. Camembert: a tasty french language model. In Proceedings\nof the 58th Annual Meeting of the Association for Computational Linguistics.\nDebora Nozza, Federico Bianchi, and Dirk Hovy. 2020. What the [MASK]? Making Sense of Language-Speciﬁc\nBERT Models. arXiv e-prints, page arXiv:2003.02912, March.\nPedro Javier Ortiz Su´arez, Benoˆıt Sagot, and Laurent Romary. 2019. Asynchronous Pipeline for Processing Huge\nCorpora on Medium to Low Resource Infrastructures. In Piotr Ba ´nski, Adrien Barbaresi, Hanno Biber, Evelyn\nBreiteneder, Simon Clematide, Marc Kupietz, Harald L¨ungen, and Caroline Iliadi, editors, 7th Workshop on the\nChallenges in the Management of Large Corpora (CMLC-7), Cardiff, United Kingdom, July. Leibniz-Institut\nf¨ur Deutsche Sprache.\nMalte Ostendorff, Till Blume, and Saskia Ostendorff. 2020. Towards an open platform for legal information. In\nProceedings of the ACM/IEEE Joint Conference on Digital Libraries in 2020, JCDL ’20, page 385–388, New\nYork, NY , USA. Association for Computing Machinery.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and Luke Zettle-\nmoyer. 2018. Deep contextualized word representations. In Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 2227–2237, New Orleans, Louisiana, June. Association for Computational Linguistics.\nAlec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. Language models\nare unsupervised multitask learners.\nRobyn Speer. 2017. Conceptnet numberbatch 17.04: better, less-stereotyped word vectors.\nJ¨org Tiedemann. 2012. Parallel data, tools and interfaces in opus. In Nicoletta Calzolari (Conference Chair),\nKhalid Choukri, Thierry Declerck, Mehmet Ugur Dogan, Bente Maegaard, Joseph Mariani, Jan Odijk, and\nStelios Piperidis, editors, Proceedings of the Eight International Conference on Language Resources and Eval-\nuation (LREC’12), Istanbul, Turkey, may. European Language Resources Association (ELRA).\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Ł ukasz Kaiser,\nand Illia Polosukhin. 2017. Attention is all you need. In I. Guyon, U. V . Luxburg, S. Bengio, H. Wallach,\nR. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems 30,\npages 5998–6008. Curran Associates, Inc.\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma, Juhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough: BERT for Finnish. arXiv e-prints, page arXiv:1912.07076,\nDecember.\nGuillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm ´an, Armand\nJoulin, and Edouard Grave. 2020. CCNet: Extracting high quality monolingual datasets from web crawl\ndata. In Proceedings of The 12th Language Resources and Evaluation Conference, pages 4003–4012, Mar-\nseille, France, May. European Language Resources Association.\nMichael Wiegand, Melanie Siegel, and Josef Ruppenhofer. 2018. Overview of the germeval 2018 shared task on\nthe identiﬁcation of offensive language. In Proceedings of GermEval 2018, 14th Conference on Natural Lan-\nguage Processing (KONVENS 2018), Vienna, Austria – September 21, 2018, pages 1 – 10. Austrian Academy\nof Sciences, Vienna, Austria.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac,\nTim Rault, R’emi Louf, Morgan Funtowicz, and Jamie Brew. 2019. Huggingface’s transformers: State-of-the-\nart natural language processing. ArXiv, abs/1910.03771.",
  "topic": "German",
  "concepts": [
    {
      "name": "German",
      "score": 0.8826248049736023
    },
    {
      "name": "Computer science",
      "score": 0.7934581637382507
    },
    {
      "name": "Benchmarking",
      "score": 0.7892447710037231
    },
    {
      "name": "Language model",
      "score": 0.7860136032104492
    },
    {
      "name": "Masking (illustration)",
      "score": 0.6614813804626465
    },
    {
      "name": "Natural language processing",
      "score": 0.5905635952949524
    },
    {
      "name": "Training set",
      "score": 0.5821502804756165
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.5404502153396606
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5149605870246887
    },
    {
      "name": "Data modeling",
      "score": 0.4108770191669464
    },
    {
      "name": "Machine learning",
      "score": 0.3490162193775177
    },
    {
      "name": "Database",
      "score": 0.14791974425315857
    },
    {
      "name": "Linguistics",
      "score": 0.13692975044250488
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210138660",
      "name": "Bavarian State Library",
      "country": "DE"
    }
  ]
}