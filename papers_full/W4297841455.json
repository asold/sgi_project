{
  "title": "Exploring Capabilities of Monolingual Audio Transformers using Large\\n Datasets in Automatic Speech Recognition of Czech",
  "url": "https://openalex.org/W4297841455",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5085434299",
      "name": "Jan Lehečka",
      "affiliations": [
        "University of West Bohemia"
      ]
    },
    {
      "id": "https://openalex.org/A5072542443",
      "name": "Ján Švec",
      "affiliations": [
        "University of West Bohemia"
      ]
    },
    {
      "id": "https://openalex.org/A5010512605",
      "name": "Aleš Pražák",
      "affiliations": [
        "University of West Bohemia"
      ]
    },
    {
      "id": "https://openalex.org/A5057873612",
      "name": "Josef Psutka",
      "affiliations": [
        "University of West Bohemia"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2134800885",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3197652246",
    "https://openalex.org/W3030437843",
    "https://openalex.org/W3198429080",
    "https://openalex.org/W3119308075",
    "https://openalex.org/W3213029956",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W3036601975",
    "https://openalex.org/W3015356564",
    "https://openalex.org/W2127923419",
    "https://openalex.org/W2981910996",
    "https://openalex.org/W3207583491",
    "https://openalex.org/W3200506395",
    "https://openalex.org/W3207988762"
  ],
  "abstract": "In this paper, we present our progress in pretraining Czech monolingual audio\\ntransformers from a large dataset containing more than 80 thousand hours of\\nunlabeled speech, and subsequently fine-tuning the model on automatic speech\\nrecognition tasks using a combination of in-domain data and almost 6 thousand\\nhours of out-of-domain transcribed speech. We are presenting a large palette of\\nexperiments with various fine-tuning setups evaluated on two public datasets\\n(CommonVoice and VoxPopuli) and one extremely challenging dataset from the\\nMALACH project. Our results show that monolingual Wav2Vec 2.0 models are robust\\nASR systems, which can take advantage of large labeled and unlabeled datasets\\nand successfully compete with state-of-the-art LVCSR systems. Moreover, Wav2Vec\\nmodels proved to be good zero-shot learners when no training data are available\\nfor the target ASR task.\\n",
  "full_text": "arXiv:2206.07627v1  [cs.CL]  15 Jun 2022\nExploring Capabilities of Monolingual Audio T ransformersusing Large\nDatasets in Automatic Speech Recognition of Czech\nJan Lehe ˇcka, Jan ˇSvec, Aleˇ s Pra ˇz ´ak, Josef V . Psutka\nDepartment of Cybernetics, University of W est Bohemia Pils en, Czech Republic\n{jlehecka,honzas,aprazak,psutka j}@kky.zcu.cz\nAbstract\nIn this paper, we present our progress in pretraining Czech\nmonolingual audio transformers from a large dataset contai n-\ning more than 80 thousand hours of unlabeled speech, and sub-\nsequently ﬁne-tuning the model on automatic speech recog-\nnition tasks using a combination of in-domain data and al-\nmost 6 thousand hours of out-of-domain transcribed speech.\nW e are presenting a large palette of experiments with variou s\nﬁne-tuning setups evaluated on two public datasets (Common -\nV oice and V oxPopuli) and one extremely challenging dataset\nfrom the MALACH project. Our results show that monolingual\nW av2V ec 2.0 models are robust ASR systems, which can take\nadvantage of large labeled and unlabeled datasets and succe ss-\nfully compete with state-of-the-art L VCSR systems. Moreov er,\nW av2V ec models proved to be good zero-shot learners when no\ntraining data are available for the target ASR task.\nIndex T erms : speech recognition, audio transformers,\nW av2V ec\n1. Introduction\nSelf-supervised models recently became a very popular al-\nternative to large vocabulary continuous speech recogniti on\n(L VCSR) systems in automatic speech recognition (ASR)\ntasks. They can learn contextualized speech representatio ns\nfrom large-scale unlabeled audio datasets (pretraining se lf-\nsupervised phase), and consequently employ the knowledge\nin the ASR training from labeled data (ﬁne-tuning supervise d\nphase). One of the most studied self-supervised ASR model\narchitectures is W av2V ec 2.0 [1]. It is a deep neural network\npretrained to reconstruct the corrupted signals. The input raw\naudio signal is processed by a multi-layer convolutional ne ural\nnetwork into a sequence of latent-speech representations w hich\nare fed into a multi-layer Transformer [2]. The output of the\nTransformer is a sequence of frame-level contextualized sp eech\nrepresentations which are then processed by the connection ist\ntemporal classiﬁcation (CTC) layer [3, 4] decoding the most\nprobable sequence of graphemes.\nIn contrast with standard L VCSR systems, these end-to-end\napproaches alleviate the need for word pronunciation model ing\nand do not require any alignment of data.\nThe main focus of this paper is to explore the capabilities\nof monolingual W av2V ec-based models, their ability for zer o-\nshot transfer learning, performance dependency on the trai ning\nmetaparameters, scaling for large ASR datasets and additio nal\nlanguage models (LMs). W e also compare the end-to-end mod-\nels with state-of-the-art hybrid DNN-HMM L VCSR systems.\n2. Related work\nMonolingual W av2V ec models for languages other than En-\nglish are very rare [5]. For languages like Czech, there are\nnone. However, there are many multilingual pretrained mod-\nels of sizes from large [6] to extremely large [7]. These mode ls\ninclude also Czech. The common practice is to adopt a mul-\ntilingual pretrained model and ﬁne-tune it on the labeled AS R\ndata from the target language. Since we had access to large\nunlabeled datasets and were not satisﬁed with results from m ul-\ntilingual models, we decided to pretrain our own monolingua l\nW av2V ec model from scratch and released it to the public. W e\nare not aware of any similar model for Czech mentioned in the\nliterature.\n3. Pretraining\nSelf-supervised audio transformers are known to scale well with\nthe size of pretraining data, even with extremely huge datas ets\n[7]. Hence, we tried to gather as much public and in-house un-\nlabeled audio data as possible. T ogether, we were able to col lect\nmore than 80 thousand hours of Czech speech. W e are not aware\nof any similar collection of Czech speech data at this scale m en-\ntioned in the literature so far. The collection includes rec ord-\nings from radio (22k hours), unlabeled data from V oxPopuli\ndataset [8] (18.7k hours), TV shows (15k hours), shadow spea k-\ners (12k hours), sports (5k hours), telephone data (2k hours ),\nand a smaller amount of data from several other domains. W e\nused also raw audio ﬁles from all speech recognition dataset s\n(see Sec. 4).\nSince the feature extraction of the input signal is limited b y\nthe memory of GPUs in use, we sliced all records not to exceed\n30 s, which we found to be a reasonable input size for batching .\nW e followed the same pretraining steps as for the base\nW av2V ec 2.0 model in [1]. W e pretrained the model for 400\nthousand steps with a batch size of about 1.6 hours, corre-\nsponding to more than 11 epochs over the dataset. W e released\nour pretrained model under the nickname ClTRUS (abbreviation\nfor Czech language TRransformer from Unlabeled Speech) for\npublic non-commercial use 1 .\n4. Fine-tuning\nW e prepared all training and development speech recognitio n\ndata consistently for all datasets. W e sliced long training audio\nsignals on speech pauses not to exceed the length of 30 s, long er\nutterances were discarded. W e removed non-speech events an d\npunctuation from the transcripts and mapped texts into lowe r-\ncase. For each dataset, we carefully analyzed transcripts a nd\nﬁxed any data-speciﬁc deviation.\nIf not stated otherwise, we ﬁne-tuned all models with the\nsame setting as the base model in [1] using Fairseq tool2 .\n1 A vailable at https://huggingface.co/fav-kky/wav2vec2-base-cs-80k -ClTRUS\n2 https://github.com/pytorch/fairseq\n4.1. Speech recognition datasets\nW e were experimenting with three Czech speech recognition\ndatasets. Basic statistics are shown in T ab. 1.\nT able 1: Speech recognition datasets. W e are showing the num-\nber of hours, the number of words in transcripts (in thousand s),\nand the average length of records (in seconds).\nCommonV oice V oxPopuli MALACH\ntrain dev test train dev test train dev test\n# hours 32.2 8.1 8.1 52.3 3.0 3.1 87.2 19.2 9.0\n# words 183 46 45 404 23 23 615 137 63\navg-len 4.1 4.6 4.5 10.2 10.0 9.9 24.1 24.1 10.6\nThe CommonV oicedataset is a Czech portion of crowd-\nsourced project Mozilla Common V oice [9]. W e used corpus\nversion 7.0 containing 49 hours of validated speech. W e de-\ncided to keep also sentences reported as offensive and difﬁcult\npronunciation in our training data. All other reported sentences\n(e.g. incomplete transcription , different language etc.) were\nignored.\nThe V oxPopuli dataset [8] is a large-scale multilingual\nspeech corpus collected from 2009-2020 European Parliamen t\nevent recordings. The Czech portion contains 18.7 thousand\nunlabeled hours and 62 hours with transcription. W e ignored all\ntrain/dev records without the raw transcription, decreasi ng the\namount of transcribed data to 58.4 hours.\nThe MALACH data3 is a subset of the USC Shoah Foun-\ndation Visual History Archive of digitized interviews in 32 lan-\nguages from 52,000 survivors, liberators, rescuers, and wi t-\nnesses of the Nazi Holocaust. W e used the Czech portion [10]\nwhich is known to be a very difﬁcult and challenging speech\nrecognition task due to the strong emotional and heavily ac-\ncented speech of Holocaust survivors [11, 12].\nT o scale the ﬁne-tuning up, we used also a large additional\nin-house dataset, denoted as Extra, containing almost 6 thou-\nsand hours of transcribed Czech speech from various domains .\nThe dataset includes radio shows records (3.9 thousand hour s),\nTV sports recordings (645 hours), telephone recordings (44 0\nhours), and several other domains. The transcripts contain 43\nmillion words.\n5. Decoding\nW e studied two different decoding setups: (1) connectionis t\ntemporal classiﬁcation (CTC) [3], which is the training los s\nwe used during ﬁne-tuning of the models, and (2) CTC beam\nsearch decoder with an LM. CTC is an alignment-free method\nfor grouping audio frames belonging to the same output sym-\nbol in order to convert a sequence of audio frames into a much\nshorter sequence of characters. Thus, W av2V ec with the CTC\nis a grapheme-based lexicon-free speech recognizer withou t any\nlanguage constraints. The only orthography-related knowl edge\nthe model could learn is the training transcripts fed in duri ng\nthe ﬁne-tuning. Incorporating an LM into the CTC beam search\ndecoder usually improves the speech recognition accuracy b y\nbringing useful language information into the decoding pro cess\nand penalizing improbable outputs. On the other hand, it is a\nstep back from the idea of an end-to-end recognizer introduc -\ning once again problems known from L VCSR systems, such as\n3 https://malach.umiacs.umd.edu\nout-of-vocabulary words and the low-relevance or low-qual ity\nLMs.\nFor our experiments, we prepared 3 different word-based\nn-gram LMs. (1) LM-ASRSpec is a model trained speciﬁcally\nfor each speech recognition dataset only from in-domain tra in-\ning transcripts. (2) LM-ASRAll was trained from transcripts\nfrom all speech recognition training data described in Sec. 4.1,\nincluding the Extra dataset. It is a general domain-indepen dent\nLM trained from ASR transcripts with 44 million words. (3)\nLM-C5 is an LM trained from Czech Colossal Clean Crawled\nCorpus (C5) [13] which is a huge collection of cleaned and\ndeduplicated web pages from Common Crawl project 4 . Since\nthis text corpus contains almost 13 billion words (93 GB of\ncleaned text), we pruned all unigrams with counts lower than\n10 and higher-order n-grams with counts lower than 100.\nW e used implementation from Transformers [14] for\nCTC decoding and pyctcdecode5 decoder for CTC beam\nsearch decoder with n-gram LM. T o train LMs, we used KenLM\n[15] and to work with models of practical sizes, we limited th e\nmaximum order of models to 4-grams. W e trained all LMs in\nlowercase. The sizes of LM vocabularies were between 17 and\n22 thousand (LM-ASRSpec), 329 thousand (LM-ASRAll), and\n4.8 million words (LM-C5).\n6. Experiments\nW e evaluated speech recognition systems on test splits of in di-\nvidual datasets and compared word error rates (WER). During\nthe evaluation, we mapped reference texts into lowercase an d\nignored all punctuation.\n6.1. CTC beam search decoder with LM\nT o get a picture of how the recognition is improved when addin g\nLM into the decoder, we prepared data-speciﬁc W av2V ec mod-\nels (denoted as W2V -ASRSpec) by ﬁne-tuning the pretrained\nmodel only on in-domain (single-dataset) training data. Th en,\nwe evaluated the models with different LMs in the decoder.\nFrom results tabulated in T ab. 2, we can see that adding small\ndata-speciﬁc LM improves the recognition, adding domain-\nindependent LM from all ASR transcripts further improves th e\nrecognition, and switching to large-scale LM trained from C om-\nmon Crawl could be also beneﬁcial, especially for domains wi th\nspeciﬁc words mentioned several times somewhere on the In-\nternet (e.g. names of politicians for V oxPopuli or geograph ical\nnames for MALACH).\nT able 2: WER [%] for CTC beam search decoder with different\nLMs evaluated on three Czech datasets.\nCommonV oice V oxPopuli MALACH\nW2V -ASRSpec (no LM) 7.29 11.28 18.93\n+ LM-ASRSpec 6.12 10.51 18.45\n+ LM-ASRAll 5.34 9.78 16.67\n+ LM-C5 5.45 9.62 15.31\nIn the following sections, we experimented with all pre-\nsented LMs but observed very similar trends, so we decided\nto report only results with LM-C5 since it either improved th e\nmodel by correctly recognizing difﬁcult domain-speciﬁc wo rds\n4 https://commoncrawl.org\n5 https://github.com/kensho-technologies/pyctcdecode\nor – in the case of CommonV oice – insigniﬁcantly deteriorate d\nthe results when compared to LM-ASRAll.\n6.2. Zero-shot transfer learning\nIn the next experiment, we investigated the W av2V ec’s zero-\nshot transfer-learning ability between datasets, i.e. how well\nthe model can transfer knowledge from one or more domains to\nan unobserved target domain with zero additional training.\nFirst of all, we cross-evaluated models ﬁne-tuned on a sin-\ngle domain. These models were trained only from a single\ndataset. Then, we tried the opposite setting, i.e. to ﬁne-tu ne\nwith all training datasets except for the one the model was ev al-\nuated on. Results with these out-of-domain models (denoted\nas W2V -ood) correspond to a scenario where the model is ﬁne-\ntuned from a large-scale ASR dataset (6 thousand hours) and\nsubsequently used to recognize speech from an unobserved do -\nmain with zero additional training. The zero-shot transfer re-\nsults are shown in T ab. 3.\nT able 3: WER [%] for zero-shot transfer learning evaluated on\nthree Czech datasets. W e are reporting results of models ﬁne -\ntuned from single-domain training data (the ﬁrst 3 models, e ach\nwith and without LM, and all training data except for the in-\ndomain data (W2V-ood).\nCommonV oice V oxPopuli MALACH\nW2V -CommonV oice - 15.62 26.57\n+ LM-C5 - 13.81 22.33\nW2V -V oxPopuli 29.94 - 33.37\n+ LM-C5 22.57 - 28.04\nW2V -MALACH 19.21 16.89 -\n+ LM-C5 11.63 13.36 -\nW2V -ood 11.57 13.12 16.86\n+ LM-C5 6.40 11.22 13.55\nWhen comparing W2V -ood model with in-domain models\n(T ab. 2) for CommonV oice and V oxPopuli datasets, in-domain\nmodels are signiﬁcantly better (with relative improvement about\n15%), which is the expected result. However, we observed\na strange WER improvement when evaluating the W2V -ood\nmodel on the MALACH dataset. After some investigation,\nwe found out, that it was caused by a mix of formal and col-\nloquial Czech in transcripts. Formal Czech is the grammati-\ncally correct written form, while colloquial Czech is a spok en\n(common) form containing for example different sufﬁxes (e. g.\n”mlad ´ y” vs. ”mladej”) or shortened word forms (e.g. ”jsem” vs.\n”sem”). More details can be found in [16]. While in the traini ng\ntranscripts, the majority of the transcribed text is in coll oquial\nCzech (i.e. transcribed exactly as spoken), the opposite is true\nfor the test transcriptions. So, when the in-domain model wa s\nﬁne-tuned from the training MALACH data, it was forced to de-\ncode rather a colloquial Czech, which, however, conﬂicted w ith\nthe test transcripts, causing a lot of word-substitution er rors. For\nthis reason, the out-of-domain model can perform signiﬁcan tly\nbetter on the MALACH dataset than the in-domain model.\n6.3. General vs. in-domain models\nIn the next experiment, we compared in-domain models (W2V -\nASRSpec) with a general model ﬁne-tuned from all available\ntraining data (i.e. both in-domain and out-of-domain) at on ce.\nMoreover, we were experimenting with a 2-phase ﬁne-tuning\nprocedure, in which the pretrained model is ﬁrst ﬁne-tuned f rom\nmulti-domain data (same as the general model) and then ﬁne-\ntuned again using only the in-domain data. In this approach, the\nmodel can beneﬁt from large-scale out-of-domain data and, a t\nthe same time, the in-domain information is accentuated.\nT able 4: WER [%] for differently ﬁne-tuned models evaluated\non three Czech datasets. W e report results from models ﬁne-\ntuned on all ASR data, only in-domain data, and a combination\nof both within a 2-phase ﬁne-tuning procedure. W e report all\nresults without an LM and when decoded with LM-C5.\nCommonV oice V oxPopuli MALACH\nW2V -general 11.17 12.37 19.71\n+ LM-C5 6.10 10.13 13.64\nW2V -ASRSpec 7.29 11.28 18.93\n+ LM-C5 5.45 9.62 15.31\nW2V -general + ASRSpec 6.52 10.07 18.63\n+ LM-C5 4.74 8.80 15.19\nResults are shown in T ab. 4. Our suggested 2-phase ﬁne-\ntuning performed signiﬁcantly better than both the general and\nin-domain models. The only exception is the MALACH dataset,\nwhere in-domain models with LM again learned to decode col-\nloquial Czech contrasting with test transcripts.\n6.4. Scaling up a batch size and updates\nSo far, all models were ﬁne-tuned with the default setting [1 ],\ni.e. 80 thousand training steps with a batch size of about 27\nminutes of audio corresponding to more than 5 epochs over all\nspeech recognition data. Since we had large-scale ASR data,\nthe natural question we asked was: W ould it be beneﬁcial to\ntrain the general model for a longer time? And with a larger\nbatch?\nT o answer these questions, we ﬁne-tuned the general model\nfor a higher number of updates and/or with larger batch sizes ,\neffectively increasing the number of training epochs over d ata.\nW e ﬁne-tuned with learning rate 8 × 10− 5 which we found to\nbe better for large-scale datasets. Our results in T ab. 5 sho w\nthat W av2V ec speech recognizers scale very well with increa s-\ning the number of ﬁne-tuning epochs over large datasets. For\nexample, when we trained the model for 8x more epochs using a\nbatch size of 108 minutes (4x larger than the default model) a nd\n160 thousand update steps (2x more than default), the total e r-\nror rate of end-to-end ASR decreased from 15.47% to 13.33%.\nMoreover, additional in-domain ﬁne-tuning within the 2-ph ase\nprocedure as described in Sec. 6.3 (model denoted as 40 epoch s\n+ ASRSpec) further improved the recognition. In the second\nphase of ﬁne-tuning, we used the default settings since the u n-\nderlying in-domain data were again small.\nAlso, our results show that models trained for the same\nnumber of epochs performed about the same no matter we mul-\ntiplied the batch size or the number of updates. Results with\nthe MALACH dataset with LM were again affected by the in-\nconsistency of train and test texts leading sometimes to mod els\npreferring colloquial Czech over formal Czech and thus caus ing\na large number of recognition errors.\n6.5. Comparison with other models\nW av2V ec models are a very promising research area and we\nwanted to compare them also with existing ASR systems. As\nT able 5: WER [%] for scaling the ﬁne-tuning epochs up. W e show how the model pe rforms when increasing the batch size (BS)\nand/or the number of updates (UP) by multiplying the default values. F or each model, we show WER without LM and with LM-C5 f or\nindividual datasets, and total WER computed by aggregating numbers of words and errors over all 3 datasets.\nCommonV oice V oxPopuli MALACH TOT AL\nno LM LM-C5 no LM LM-C5 no LM LM-C5 no LM LM-C5\n5 epochs (default) 11.17 6.10 12.37 10.13 19.71 13.64 15.47 1 0.43\n10 epochs (2xUP) 9.30 5.04 11.00 9.42 18.62 13.33 14.07 9.79\n10 epochs (2xBS) 9.23 4.82 11.18 9.52 19.05 13.60 14.28 9.86\n20 epochs (2xBS, 2xUP) 8.49 4.59 10.75 9.30 17.99 12.98 13.45 9.44\n20 epochs (4xBS) 8.39 4.62 10.82 9.31 18.86 13.89 13.84 9.89\n40 epochs (4xBS, 2xUP) 7.68 4.29 10.23 8.81 18.52 13.73 13.33 9.61\n+ ASRSpec 5.41 3.80 10.07 8.80 17.65 14.51 12.10 9.82\nfor the state-of-the-art results, we did not ﬁnd any paper th at\nreported WER on Czech CommonV oice data. For V oxPopuli,\nthe best reported result is WER = 11.8% from [8] and for\nMALACH WER = 14.65% from [12].\nSince we did not develop an L VCSR system speciﬁcally\nfor public CommonV oice and V oxPopuli datasets, we arranged\nzero-shot transfer learning conditions, under which the co mpar-\nison can be made. W e used the L VCSR system developed for\nreal-time applications employed in live TV subtitling thro ugh\nrespeaking [17]. W e compared this model with the W2V -ood\nmodel. Thus, both systems were trained for other domains\nand did not see any training examples (except for unlabeled\ndata during pretraining of W av2V ec). W e slightly modiﬁed\nacronyms and multiwords in the reference to correspond with\nthe texts used in the L VCSR (so the results are not directly co m-\nparable with other results in this paper). For both systems, we\nused the same LM (3-gram from all ASR transcripts).\nT able 6: Comparison of LVCSR and W av2vec models on three\nCzech datasets in terms of WER [%].\nCommonV oice V oxPopuli MALACH\nL VCSR 8.62 11.83 14.35\nW av2V ec 5.75 9.01 12.93\nFor the MALACH dataset, we were comparing W av2V ec\nmodels with the state-of-the-art L VCSR models developed\nspeciﬁcally for this dataset. The L VCSR system was a CNN-\nTDNN LF-MMI with iV ectors and sMBR criterion and a care-\nfully curated 3-gram LM [12]. W e compared this model with\nour best-scoring W av2V ec model, which is a general model\nﬁne-tuned from all ASR data (including MALACH) for 20\nepochs. W e used the same LM when decoding the output. Our\nresults are summarized in T ab. 6.\n7. Discussion\nResults in T ab. 6 clearly show that W av2V ec with an LM-based\nbeam search decoder is a robust ASR system, which can take\nadvantage of large labeled and unlabeled datasets and succe ss-\nfully compete with state-of-the-art L VCSR systems.\nHowever, L VCSR systems have still some advantages over\ntransformer-based systems. T o mention a few of them: L VCSR\nsystem can be used for online real-time recognition with low\nlatency; L VCSR system can easily handle irregular and difﬁc ult\npronunciation of special words in a straightforward explic it way\nin the lexicon. Transformer-based ASR systems, on the other\nhand, are lexicon-free systems that can learn how to transcr ibe\nwords without the need for explicit phonetic transcription s, al-\nlowing any text (even huge corpora with vocabulary sizes in t he\norder of tens of million words) to be used for language model-\ning in the W av2V ec’s decoder without any additional manual o r\nengineering effort.\nResults from our paper showed that the performance of the\nW av2V ec-based ASR system can be signiﬁcantly improved by:\n• longer ﬁne-tuning over the large-scale dataset,\n• adding LM into the beam search CTC decoder; we found\nthe Common Crawl project to be a useful corpus for\nlarge-scale language modeling,\n• 2-phase ﬁne-tuning procedure - multi-domain ﬁne-\ntuning in the ﬁrst phase followed by a second phase of\nﬁne-tuning using only data from the target domain.\nOur best results with LM-C5 in T ab. 5 are – to our best knowl-\nedge – new state-of-the-art results in all three datasets.\n8. Conclusions\nIn this paper, we pretrained Czech monolingual audio trans-\nformers from a large dataset containing more than 80 thou-\nsand hours of unlabeled speech and released the model to the\npublic. W e presented a large palette of experiments with var -\nious ﬁne-tuning setups with large-scale datasets (up to 6 th ou-\nsand hours) evaluated on two public datasets (CommonV oice\nand V oxPopuli) and one extremely challenging dataset from\nthe MALACH project. Our results showed that monolingual\nW av2V ec2 models are robust ASR systems, which can take ad-\nvantage of large labeled and unlabeled datasets and success -\nfully compete with state-of-the-art L VCSR systems. Moreov er,\nour general transformer-based ASR systems proved to be good\nzero-shot learners when no training data are available for t he\ntarget domain. Our models scored new state-of-the-art resu lts\non all three tested datasets by a large margin.\n9. Acknowledgements\nComputational resources were supplied by the project ”e-\nInfrastruktura CZ” (e-INFRA CZ LM2018140 ) supported by\nthe Ministry of Education, Y outh and Sports of the Czech Re-\npublic.\n10. References\n[1] A. Baevski, Y . Zhou, A. Mohamed, and M. Auli, “wav2vec\n2.0: A framework for self-supervised learning of speech rep re-\nsentations, ” Advances in Neural Information Processing Systems ,\nvol. 33, pp. 12 449–12 460, 2020.\n[2] A. V aswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones , A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “ Attention is all you ne ed, ”\nAdvances in neural information processing systems , vol. 30, 2017.\n[3] A. Graves, S. Fern´ andez, F . Gomez, and J. Schmidhuber, “ Con-\nnectionist temporal classiﬁcation: labelling unsegmente d se-\nquence data with recurrent neural networks, ” in Proceedings of\nthe 23rd international conference on Machine learning , 2006, pp.\n369–376.\n[4] A. Baevski and A. rahman Mohamed, “Effectiveness of self -\nsupervised pre-training for asr, ” ICASSP 2020 - 2020 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Proces sing\n(ICASSP), pp. 7694–7698, 2020.\n[5] S. Evain, H. Nguyen, H. Le, M. Z. Boito, S. Mdhaffar, S. Ali samir,\nZ. T ong, N. T omashenko, M. Dinarelli, T . Parcollet et al. , “T ask\nagnostic and task speciﬁc self-supervised learning from sp eech\nwith lebenchmark, ” in Thirty-ﬁfth Conference on Neural Informa-\ntion Processing Systems Datasets and Benchmarks Track (Rou nd\n2), 2021.\n[6] A. Conneau, A. Baevski, R. Collobert, A. Mohamed, and M. A uli,\n“Unsupervised cross-lingual representation learning for speech\nrecognition, ” in Interspeech 2021, 22nd Annual Conference\nof the International Speech Communication Association, Br no,\nCzechia, 30 August - 3 September 2021 , H. Hermansky ,\nH. Cernock ´ y, L. Burget, L. Lamel, O. Scharenborg, and\nP . Motl´ ıcek, Eds. ISCA, 2021, pp. 2426–2430. [Online].\nA vailable: https://doi.org/10.21437/Interspeech.2021-329\n[7] A. Babu, C. W ang, A. Tjandra, K. Lakhotia, Q. Xu, N. Goyal,\nK. Singh, P . von Platen, Y . Saraf, J. Pino et al. , “Xls-r: Self-\nsupervised cross-lingual speech representation learning at scale, ”\narXiv preprint arXiv:2111.09296 , 2021.\n[8] C. W ang, M. Riviere, A. Lee, A. Wu, C. T alnikar, D. Haz-\niza, M. Williamson, J. Pino, and E. Dupoux, “V oxPopuli:\nA large-scale multilingual speech corpus for representati on\nlearning, semi-supervised learning and interpretation, ” in Pro-\nceedings of the 59th Annual Meeting of the Association\nfor Computational Linguistics and the 11th International\nJoint Conference on Natural Language Processing (V olume\n1: Long P apers) . Online: Association for Computational\nLinguistics, Aug. 2021, pp. 993–1003. [Online]. A vailable :\nhttps://aclanthology .org/2021.acl-long.80\n[9] R. Ardila, M. Branson, K. Davis, M. Henretty , M. Kohler,\nJ. Meyer, R. Morais, L. Saunders, F . M. T yers, and G. W eber,\n“Common voice: A massively-multilingual speech corpus, ” i n\nProceedings of the 12th Conference on Language Resources an d\nEvaluation (LREC 2020) , 2020, pp. 4211–4215.\n[10] J. Psutka, V . Radov´ a, P . Ircing, J. Matouˇ sek, and\nL. M ¨ uller, “USC-SFI MALACH Interviews and Tran-\nscripts Czech LDC2014S04, ” 2014. [Online]. A vailable:\nhttps://catalog.ldc.upenn.edu/LDC2014S04\n[11] J. V . Psutka, J. ˇSvec, and A. Praˇ z´ ak, “Cnn-tdnn-based architecture\nfor speech recognition using grapheme models in bilingual c zech-\nslovak task, ” in T ext, Speech, and Dialogue , K. Ekˇ stein, F . P ´ artl,\nand M. Konop´ ık, Eds. Cham: Springer International Publish ing,\n2021, pp. 523–533.\n[12] J. V . Psutka, A. Praˇ z´ ak, and J. V anˇ ek, “Recognition o f heavily ac-\ncented and emotional speech of english and czech holocaust s ur-\nvivors using various dnn architectures, ” in Speech and Computer ,\nA. Karpov and R. Potapova, Eds. Cham: Springer Internationa l\nPublishing, 2021, pp. 553–564.\n[13] J. Leheˇ cka and J. ˇSvec, “Comparison of czech transformers\non text classiﬁcation tasks, ” in Statistical Language and Speech\nProcessing, L. Espinosa-Anke, C. Mart´ ın-V ide, and I. Spasi´ c,\nEds. Cham: Springer International Publishing, 2021, pp. 27 –\n37.\n[14] T . W olf, L. Debut, V . Sanh, J. Chaumond, C. Delangue, A. M oi,\nP . Cistac, T . Rault, R. Louf, M. Funtowicz, J. Davison, S. Shl eifer,\nP . von Platen, C. Ma, Y . Jernite, J. Plu, C. Xu, T . L. Scao,\nS. Gugger, M. Drame, Q. Lhoest, and A. M. Rush, “Transformers :\nState-of-the-art natural language processing, ” in Proceedings of\nthe 2020 Conference on Empirical Methods in Natural Languag e\nProcessing: System Demonstrations . Online: Association for\nComputational Linguistics, Oct. 2020, pp. 38–45. [Online] . A vail-\nable: https://www .aclweb.org/anthology/2020.emnlp- demos.6\n[15] K. Heaﬁeld, “Kenlm: Faster and smaller language model q ueries, ”\nin Proceedings of the sixth workshop on statistical machine tr ans-\nlation, 2011, pp. 187–197.\n[16] W . Byrne, D. Doermann, M. Franz, S. Gustman, J. Hajic, D. Oard,\nM. Picheny , J. Psutka, B. Ramabhadran, D. Soergel, T . W ard, a nd\nW .-J. Zhu, “ Automatic recognition of spontaneous speech fo r ac-\ncess to multilingual oral history archives, ” IEEE Transactions on\nSpeech and Audio Processing , vol. 12, no. 4, pp. 420–435, 2004.\n[17] A. Praˇ z´ ak, Z. Loose, J. Psutka, V . Radov´ a, and J. Psut ka, “Live tv\nsubtitling through respeaking with remote cutting-edge te chnol-\nogy , ” Multimedia T ools and Applications , vol. 79, pp. 1–18, 01\n2020.",
  "topic": "Czech",
  "concepts": [
    {
      "name": "Czech",
      "score": 0.9307750463485718
    },
    {
      "name": "Computer science",
      "score": 0.7832835912704468
    },
    {
      "name": "Transformer",
      "score": 0.6381216049194336
    },
    {
      "name": "Speech recognition",
      "score": 0.6368356347084045
    },
    {
      "name": "Audio mining",
      "score": 0.5113984942436218
    },
    {
      "name": "Natural language processing",
      "score": 0.3917599320411682
    },
    {
      "name": "Acoustic model",
      "score": 0.3812706470489502
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38088172674179077
    },
    {
      "name": "Speech processing",
      "score": 0.31888067722320557
    },
    {
      "name": "Engineering",
      "score": 0.10354134440422058
    },
    {
      "name": "Linguistics",
      "score": 0.08405131101608276
    },
    {
      "name": "Electrical engineering",
      "score": 0.0594734251499176
    },
    {
      "name": "Voltage",
      "score": 0.04628026485443115
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}