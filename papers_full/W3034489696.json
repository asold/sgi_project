{
  "title": "Automatic Machine Translation Evaluation using Source Language Inputs and Cross-lingual Language Model",
  "url": "https://openalex.org/W3034489696",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2146426690",
      "name": "Kosuke TAKAHASHI",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2094290022",
      "name": "Katsuhito Sudoh",
      "affiliations": [
        "Japan Science and Technology Agency",
        "Nara Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2067869321",
      "name": "Satoshi Nakamura",
      "affiliations": [
        "Nara Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2759332014",
    "https://openalex.org/W2250484373",
    "https://openalex.org/W2891555348",
    "https://openalex.org/W2903376039",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2966292672",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W2963918774",
    "https://openalex.org/W2123301721",
    "https://openalex.org/W2963644595",
    "https://openalex.org/W2115259925",
    "https://openalex.org/W2294699749",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W658020064",
    "https://openalex.org/W2915756181",
    "https://openalex.org/W2786464815",
    "https://openalex.org/W2101105183"
  ],
  "abstract": "We propose an automatic evaluation method of machine translation that uses source language sentences regarded as additional pseudo references. The proposed method evaluates a translation hypothesis in a regression model. The model takes the paired source, reference, and hypothesis sentence all together as an input. A pretrained large scale cross-lingual language model encodes the input to sentence-pair vectors, and the model predicts a human evaluation score with those vectors. Our experiments show that our proposed method using Cross-lingual Language Model (XLM) trained with a translation language modeling (TLM) objective achieves a higher correlation with human judgments than a baseline method that uses only hypothesis and reference sentences. Additionally, using source sentences in our proposed method is confirmed to improve the evaluation performance.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3553–3558\nJuly 5 - 10, 2020.c⃝2020 Association for Computational Linguistics\n3553\nAutomatic Machine Translation Evaluation using\nSource Language Inputs and Cross-lingual Language Model\nKosuke Takahashi1, Katsuhito Sudoh1,,2, Satoshi Nakamura1\n1 Nara Institute of Science and Technology\n2 PRESTO, Japan Science and Technology Agency\n{takahashi.kosuke.th0, sudoh, s-nakamura}@is.naist.jp\nAbstract\nWe propose an automatic evaluation method of\nmachine translation that uses source language\nsentences regarded as additional pseudo refer-\nences. The proposed method evaluates a trans-\nlation hypothesis in a regression model. The\nmodel takes the paired source, reference, and\nhypothesis sentence all together as an input. A\npretrained large scale cross-lingual language\nmodel encodes the input to sentence-pair vec-\ntors, and the model predicts a human evalua-\ntion score with those vectors. Our experiments\nshow that our proposed method using Cross-\nlingual Language Model (XLM) trained with\na translation language modeling (TLM) objec-\ntive achieves a higher correlation with human\njudgments than a baseline method that uses\nonly hypothesis and reference sentences. Ad-\nditionally, using source sentences in our pro-\nposed method is conﬁrmed to improve the eval-\nuation performance.\n1 Introduction\nAutomatic machine translation evaluation (MTE)\nhas been studied to substitute human evaluation\nin machine translation development because it is\nlow-cost, handy, and stable to use. Popular auto-\nmatic MTE metrics such as BLEU (Papineni et al.,\n2002) calculate the evaluation score based on a\nsurface-level similarity of a paired 1-to-1 reference\nand translated hypothesis sentences. BLEU partic-\nularly evaluates the sentence similarity with the n-\ngram word matching rate between a reference and\nhypothesis. However, the evaluation score drops\nwhen a reference and hypothesis are dissimilar in\nthe surface even if they share the same meaning.\nTo counter this problem, METEOR (Banerjee\nand Lavie, 2005) is proposed to mitigate the word\nmatching of synonyms with a synonym dictionary.\nYet still, with mitigation of word matching, surface-\nlevel similarity cannot fully compensate for seman-\ntics, thus word representation instead of word sym-\nbols is used in Word Mover’s Distance (Kusner\net al., 2015) and bleu2vec (T¨attar and Fishel, 2017).\nBesides, sentence representation is known to be\nan efﬁcient feature instead of word representation\nbecause sentence vectors can represent more global\nmeanings. RUSE (Shimanaka et al., 2018) and\nBERT (Devlin et al., 2019) based MTE, BERT re-\ngressor (Shimanaka et al., 2019), utilized sentence\nrepresentation and performed well on WMT17 Met-\nric Shared Task (Bojar et al., 2017). The metrics\nmentioned above compare a hypothesis translation\nto a reference. However, a reference translation\nrepresents only one possible translation and those\nMTE metrics are unlikely to correctly evaluate all\ncandidates that share the same meanings of the\nreference or have fatally different meanings due\nto a few translation errors. This problem can be\nmitigated by the use of multiple reference transla-\ntions as argued by Dreyer and Marcu (2012) and\nQin and Specia (2015), but preparing such multiple\nreferences is costly.\nHereby, we propose a method to incorporate\nsource sentence into MTE as another pseudo\nreference, since the source and reference sen-\ntences should be semantically equivalent. The pro-\nposed method uses Cross-lingual Language Model\n(XLM) (Lample and Conneau, 2019) to handle\nsource and target languages in a shared sentence\nembedding space. The proposed method with\nXLM trained with a translation language model-\ning (TLM) objective showed a higher correlation\nwith human judgments than a baseline method us-\ning hypothesis and reference sentences.\n2 Related Work\nRecent advances in sentence-level embedding have\nbeen used in MTE. Shimanaka et al. (2018) pro-\nposed an MTE framework called RUSE (Regressor\nUsing Sentence Embeddings), which uses sentence-\n3554\nTable 1: Available corpus size annotated with human judgments in WMT-2017 Metrics Shared Task (to-English)\ncs-en de-en ﬁ-en lv-en ro-en ru-en tr-en zh-en {de,ru,tr,zh}-en all-en\nWMT-2015 500 500 500 - - 500 - - 1000 2000\nWMT-2016 560 560 560 - 560 560 560 - 1680 3360\nWMT-2017 560 560 560 560 - 560 560 560 2240 3920\nALL 1620 1620 1620 560 560 1620 1120 560 4920 9280\nlevel embeddings obtained by a large-scale pre-\ntrained model like InferSent (Conneau et al., 2017),\nQuick Thought (Logeswaran and Lee, 2018), and\nUniversal Sentence Encoder (Cer et al., 2018). Its\nregressor takes sentence vectors for a reference and\ntranslation hypothesis as inputs and returns a score,\nwhich is trained to correlate well with human eval-\nuation (Graham et al., 2015). RUSE achieved the\nbest correlation score with human judgments in\nthe WMT-2017 Metrics Shared Task (Bojar et al.,\n2017).\nBERT regressor (Shimanaka et al., 2019) is\na simple MTE metric based on BERT (Devlin\net al., 2019) encoder. It is composed of BERT\nencoder and a multi-layer perceptron (MLP) re-\ngressor attached to the last layer of BERT. This\nBERT encoder is a 12 layers bi-directional lan-\nguage model, referring to BERTbase(uncased)1,\ntrained with masked language model (MLM) and\nnext sentence prediction (NSP). BERT regressor\nsurpassed RUSE on the WMT-2017 data.\n3 Proposed method: Automatic\nevaluation using XLM\nWe propose an MTE method using source language\nsentences as additional pseudo references. We use\ncross-lingual language models called XLM (Lam-\nple and Conneau, 2019) to encode both source and\ntarget language sentences into an embedding vec-\ntor.\nXLM has three additional techniques to BERT:\nlanguage independent subword based on Byte Pair\nEncoding (Sennrich et al., 2016), a language em-\nbedding layer, and a translation language model-\ning (TLM) objective that predicts masked words\nfrom surrounding words or a paired translation.\nThe brief architecture of XLM is shown in Fig-\nure 1. (Lample and Conneau, 2019) reported that\nXLM trained with TLM objective obtains better\nperformance than multilingual BERT(Devlin et al.,\n2019) on the XNLI cross-lingual classiﬁcation\ntask(Conneau et al., 2018).\n1https://github.com/google-research/\nbert\nThe proposed method has two variants for the\nuse of source language sentences, as illustrated in\nFigure 2. The ﬁrst one called hyp+src/hyp+ref\nuses two sentence-pair vectors for hypothesis-\nsource and hypothesis-reference, encoded by a\ncross-lingual language model independently. These\nsentence-pair vectors are given to an MLP-based\nregression model to predict the human evaluation\nscores. This can be regarded as an ensemble model\nusing a monolingual vector based on the reference\nand a cross-lingual vector based on the source sen-\ntence. The other one called hyp+src+ref takes\na concatenation of hypotheses, source, and ref-\nerence sentences as an input to a cross-lingual\nlanguage model to obtain a sentence-pair vector.\nThis sentence-pair vector is expected to be directly\nlearned to represent the quality of the translation hy-\npothesis given two correct sentences aligned aside.\n4 Experiments\nWe conducted experiments to evaluate the perfor-\nmance of the proposed method in MTE by compar-\ning with some existing methods.\n4.1 Setting\nThe experiments were conducted with a corpus\nof all language pairs to English translation from\nsegment-level WMT2017 Metrics Shared Task (Bo-\njar et al., 2017). We split sentences in WMT15 and\nWMT16 to training and development data with the\nratio of 9:1 and whole sentences in WMT17 are\nused for evaluation of MTE methods. The corpus\nsize for each language pair is shown in Table 1.\nWe used two different models from all available\nXLM family models2: XLM15 pretrained by MLM\nand TLM, and XLM100 pretrained only by MLM.\nXLM15 is expected to perform better by the paired\nbilingual training of TLM, but the number of avail-\nable languages is limited. XLM15 is compatible\nwith only German, Russian, Turkish, and Chinese\nin the corpus, which conﬁnes the model to partial\naccess to the corpus. On the other hand, XLM100\n2https://github.com/facebookresearch/\nXLM\n3555\n++++ +++++ +++++ +++++ +\nn layer transformer encoder \nsentence-pair vector\n[/s]token1,1token1, hhhtoken1,n token2,1token2,hhhtoken2,m[/s][/s]\nV[/s] V1,1 V1, hhh V1,n V2,1 V2,hhh V2,mV[/s]V[/s]\npositional embedding\ninput tokens\nhidden vectors\nL1 L 1 L 1 L 1 L 2 L 2 L 2 L2L1language embedding\nPOS0 POS1 POShhh POSn POSn+3POShhhPOSn+m+2POSn+m+3POSn+1\n[/s]\nL2\nPOSn+2\nV[/s]\nFigure 1: The architecture of XLM sentence-pair encoder\nMLP\nevaluation score\nsentence-pair encoderhypothesis + source\nconcatenation vhyp+src, vhyp+src\nhypothesis + reference\nsentence-pair vector vhyp+srcsentence-pair vector vhyp+src\n(a) hyp+src/hyp+ref\nMLP\nevaluation score\nsentence-pair encoderhypothesis + source + reference\nsentence-pair vector vhyp+src+ref (b) hyp+src+ref\nFigure 2: Two variants in the proposed method\nis compatible with all language pairs in the corpus,\nwhile it lacks supervised bilingual pretraining.\nThus the experiments had two corpus settings;\nOne was a small corpus including {German (de),\nRussian (ru), Turkish (tr), and Chinese (zh) }to\nEnglish (en) language pairs, and the other was a\nwhole corpus including {Czech (cz), German (de),\nFinnish (ﬁ), Latvian (lv), Romanian (ro), Russian\n(ru), Turkish (tr), and Chinese (zh) }to English\nlanguage pairs. The evaluation was conducted with\nPearson’s correlation to human judgments in the\ntest set.\nWe compared the proposed methods with Sent-\nBLEU (Bojar et al., 2017), BERT regressor (Shi-\nmanaka et al., 2019) by our implementation. We\nalso conducted experiments using multilingual\nBERT, BERTmulti(cased), to contrast language\nmodels and experiments limiting the model’s in-\nput into source-hypothesis only and reference-\nhypothesis only to study the impact of adding\nsource sentences.\nThe ﬁne-tuning on the proposed methods and\nBERT regressor was based on Mean Squared Error\nTable 2: Pearson’s correlation scores in the small cor-\npus ({de,ru,tr,zh}-en)\nde-en ru-en tr-en zh-en avg\nSentBLEU 0.432 0.484 0.538 0.512 0.484\nBERT regressor0.729 0.757 0.770 0.702 0.740\nmulti-BERT\nhyp+src/hyp+ref0.661 0.739 0.768 0.735 0.726\nhyp+src+ref 0.625 0.713 0.725 0.691 0.689\nhyp+src 0.520 0.558 0.601 0.559 0.559\nhyp+ref 0.627 0.688 0.718 0.685 0.679\nXLM15\nhyp+src/hyp+ref0.753 0.795 0.771 0.763 0.771\nhyp+src+ref 0.729 0.769 0.767 0.725 0.747\nhyp+src 0.722 0.763 0.761 0.668 0.728\nhyp+ref 0.716 0.787 0.746 0.714 0.741\nXLM100\nhyp+src/hyp+ref0.643 0.722 0.725 0.712 0.701\nhyp+src+ref 0.635 0.695 0.715 0.661 0.677\nhyp+src 0.464 0.450 0.557 0.449 0.480\nhyp+ref 0.631 0.718 0.695 0.702 0.687\n(MSE) loss in the training set, back-propagated\nto both MLP and XLM in order. The hyper-\nparameters were selected through grid search for\nthe following parameters. Since models are af-\nfected by randomness in training, we ran ten exper-\niments for each of the settings and report results of\nthe average scores.\n•Optimizer : {Adam}\n•Learning rate : {3e-5, 1e-5, 9e-6, 7e-6}\n•Number of epochs : {1, ...,20}\n•Dropout rate: {0.1}\n•Batch size : {2, 4, 8, 16}\n4.2 Results\nThe results of each small corpus and whole corpus\nexperiments are shown in Tables 2 and 3, respec-\ntively. Note that XLM15 was not included in the\n3556\nTable 3: Pearson’s correlation scores in the whole corpus (all-en)\ncs-en de-en ﬁ-en lv-en ru-en tr-en zh-en avg\nSentBLEU 0.435 0.432 0.571 0.393 0.484 0.538 0.512 0.481\nBERT regressor 0.776 0.753 0.863 0.818 0.788 0.803 0.767 0.795\nmulti-BERT\nhyp+src/hyp+ref 0.743 0.688 0.824 0.812 0.772 0.796 0.751 0.769\nhyp+src+ref 0.714 0.670 0.802 0.774 0.754 0.758 0.722 0.742\nhyp+src 0.599 0.525 0.699 0.681 0.586 0.633 0.571 0.613\nhyp+ref 0.720 0.681 0.823 0.806 0.744 0.768 0.748 0.756\nXLM100\nhyp+src/hyp+ref 0.712 0.681 0.822 0.810 0.756 0.773 0.745 0.757\nhyp+src+ref 0.698 0.666 0.818 0.795 0.742 0.765 0.727 0.745\nhyp+src 0.510 0.531 0.672 0.662 0.543 0.602 0.537 0.580\nhyp+ref 0.692 0.666 0.813 0.788 0.743 0.746 0.714 0.738\nwhole corpus experiment due to its limited lan-\nguage coverage.\nPerformance of each language model As we\ncan see from Table 2, the proposed method using\nXLM15 with hyp+src/hyp+ref structure surpassed\nBERT regressor in the small corpus. However,\nXLM100 did not work well in the experiments;\nits results were much worse than the others in the\nsmall corpus condition, and it did not compete with\nBERT regressor in the whole corpus condition as\nshown in Table 3. One possible reason is the lack\nof TLM objective pretraining in XLM100. Since\nthe TLM task allows the model for learning seman-\ntically equivalent cross-lingual sentences directly,\nthe TLM task can be concluded to be important\nfor using source sentences in MTE. The results of\nmultilingual BERT are worse than BERT regressor\nand XLM15, but close to XLM100 or slightly bet-\nter in general. From this comparison of pretraining\nobjectives and language models, we report that our\nproposed method is inﬂuenced by the multilingual-\nism of a language model.\nhyp+src/hyp+ref VS hyp+src+ref The re-\nsults from Table 2 and Table 3 shows that\nhyp+src/hyp+ref structure is better than\nhyp+src+ref in most of the conditions, al-\nthough we expected hyp+src+ref to perform better\nbecause it can access 2 translation answers as\nreferences at the same time. This is probably\nbecause both of XLM and multilingual BERT was\nnot pretrained to handle 3 sentences in a sequence.\nHowever, it is perhaps possible that hyp+src+ref\nsurpasses hyp+src/hyp+ref when a ﬁne-tuning\ncorpus is large enough.\nTable 4: Pearson’s correlation score in the halved small\ncorpus {de,ru,tr,zh}-en\nde-en ru-en tr-en zh-en avg\nBERT regressor0.686 0.731 0.753 0.691 0.715\nmulti-BERT\nhyp+src/hyp+ref0.583 0.670 0.720 0.675 0.662\nhyp+src+ref 0.563 0.664 0.704 0.698 0.657\nhyp+src 0.384 0.509 0.629 0.482 0.501\nhyp+ref 0.574 0.651 0.722 0.693 0.660\nXLM15\nhyp+src/hyp+ref0.712 0.744 0.740 0.690 0.722\nhyp+src+ref 0.679 0.748 0.706 0.666 0.700\nhyp+src 0.570 0.635 0.654 0.616 0.619\nhyp+ref 0.682 0.707 0.708 0.700 0.699\nXLM100\nhyp+src/hyp+ref0.594 0.676 0.706 0.686 0.666\nhyp+src+ref 0.605 0.644 0.676 0.639 0.631\nhyp+src 0.321 0.408 0.447 0.431 0.402\nhyp+ref 0.559 0.631 0.675 0.668 0.633\nContribution of adding source sentences Ev-\nery model with hyp+src/hyp+ref achieved a better\nscore than both of hyp+src and hyp+ref, which\nindicates that source sentences contribute to the\nimprovement of evaluation.\n5 Analysis\nTraining data size We conducted another exper-\niment to see the effect of the training corpus size\nusing randomly halved {de, ru, tr, zh }-en small\ncorpus. From the results in Table 4, BERT regres-\nsor stably performed well even when the number\nof training data is about 1000 sentences, however,\nXLM15, XLM100, and multilingual BERT dete-\nriorated their performances. Since our proposed\nhyp+src/hyp+ref is an ensemble model and has a\nmore complex network structure than hyp+ref, the\n3557\nTable 5: Pearson’s correlation score for low and high human judgement score range in the small corpus\n({de,ru,tr,zh}-en)\nAll DA≥0.0 DA< 0.0 Reduction rate of Pearson’s score\nfrom DA≥0.0 to DA< 0.0 (%)\nBERT regressor 0.728 0.553 0.464 16.10\nmulti-BERT hyp+src/hyp+ref0.728 0.535 0.494 7.77\nmulti-BERT hyp+src+ref 0.686 0.512 0.423 17.51\nmulti-BERT hyp+src 0.539 0.339 0.316 6.88\nmulti-BERT hyp+ref 0.672 0.493 0.384 22.05\nXLM15 hyp+src/hyp+ref 0.768 0.580 0.529 8.68\nXLM15 hyp+src+ref 0.740 0.560 0.497 11.12\nXLM15 hyp+src 0.679 0.469 0.430 8.46\nXLM15 hyp+ref 0.735 0.534 0.458 14.20\nXLM100 hyp+src/hyp+ref 0.703 0.535 0.419 21.75\nXLM100 hyp+src+ref 0.662 0.501 0.389 22.29\nXLM100 hyp+src 0.522 0.337 0.292 13.42\nXLM100 hyp+ref 0.685 0.521 0.378 27.48\nuse of XLM and multi-BERT with the proposed\nmethod requires a certain amount of training data.\nTherefore, our proposed method deteriorated in the\nhalved small corpus setting. On the other hand,\nmonolingual BERT and hyp+ref beneﬁts from the\nlarge corpus because it has no language limitation\nother than English.\nEvaluation errors In order to see when mod-\nels make errors to evaluate hypothesis sentences,\nwe plot scatters of evaluation scores and human\njudgement scores (DA scores) in Figure 3(a), Fig-\nure 3(b), Figure 3(c), and Figure 3(d). Although\nin comparison, the evaluation scores of our best\nmodel XLM15 hyp+src/hyp+ref are set more lin-\nearly than the baseline BERT regressor, the scores\nof all models seem much dispersed in the low DA\narea (DA < 0.0). This indicates that all evaluation\nmodels listed here tend to miss-evaluate when a\nhypothesis is poor. Furthermore, we show Pear-\nson’s correlation score for each of high and low\nDA score range in Table 5. As we conﬁrmed in\nthe scatter ﬁgures, the correlation scores of low\nDA is low; evaluation models work poorly when\nhypotheses are poor. However, the reduction rate\nof Pearson’s scores from high DA to low DA is\nsmall with XLM15 hyp+src/hyp+ref and hyp+src.\nTherefore adding source sentences has an impact\nto stabilize the evaluation performance when hy-\npotheses are low-quality.\n6 Conclusion\nIn this paper, we proposed an MTE framework that\nutilizes source sentences using XLM. We show\n1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5\nDA score\n1.0\n0.5\n0.0\n0.5\n1.0\nevaluation score\n(a) BERT regressor\nr = 0.728\n1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5\nDA score\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nevaluation score (b) XLM15 hyp+src/hyp+ref\nr = 0.768\n1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5\nDA score\n1.0\n0.5\n0.0\n0.5\n1.0\nevaluation score\n(c) XLM15 hyp+src\nr = 0.679\n1.5\n 1.0\n 0.5\n 0.0 0.5 1.0 1.5\nDA score\n1.0\n0.5\n0.0\n0.5\n1.0\nevaluation score (d) XLM15 hyp+ref\nr = 0.735\nFigure 3: Scatter plots of human judgement scores (DA\nscores) and evaluation scores\nthat the proposed method with TLM-trained XLM\nshowed a higher correlation with human judgments\nthan the baseline method in the small corpus con-\ndition and stabilize the evaluation performance re-\ngardless of the quality of translation sentences by\nusing additional source sentences. We also inves-\ntigated why our proposed method worked poorly\nin the other conditions and found the importance\nof TLM training. In future work, we will work\naround the problem of evaluation errors in the low\nDA range.\nAcknowledgments\nThis work is supported by JST PRESTO (JP-\nMJPR1856).\n3558\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nAn Automatic Metric for MT Evaluation with Im-\nproved Correlation with Human Judgments. In Pro-\nceedings of the ACL Workshop on Intrinsic and Ex-\ntrinsic Evaluation Measures for Machine Transla-\ntion and/or Summarization, pages 65–72, Ann Ar-\nbor, Michigan. Association for Computational Lin-\nguistics.\nOndˇrej Bojar, Yvette Graham, and Amir Kamran. 2017.\nResults of the WMT17 Metrics Shared Task”. In\nProceedings of the Second Conference on Machine\nTranslation, pages 489–513, Copenhagen, Denmark.\nAssociation for Computational Linguistics.\nDaniel Cer, Yinfei Yang, Sheng-yi Kong, Nan Hua,\nNicole Limtiaco, Rhomni St. John, Noah Constant,\nMario Guajardo-Cespedes, Steve Yuan, Chris Tar,\nBrian Strope, and Ray Kurzweil. 2018. Universal\nsentence encoder for English. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 169–174, Brussels, Belgium. Association for\nComputational Linguistics.\nAlexis Conneau, Douwe Kiela, Holger Schwenk, Lo ¨ıc\nBarrault, and Antoine Bordes. 2017. Supervised\nLearning of Universal Sentence Representations\nfrom Natural Language Inference Data. In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 670–680,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nAlexis Conneau, Ruty Rinott, Guillaume Lample Ad-\nina Williams, Samuel R. Bowman, Holger Schwenk,\nand Veselin Stoyanov. 2018. Xnli: Evaluating cross-\nlingual sentence representations. pages 2475–2485.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing Asso-\nciation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMarkus Dreyer and Daniel Marcu. 2012. HyTER:\nMeaning-Equivalent Semantics for Translation Eval-\nuation. In Proceedings of the 2012 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, pages 162–171, Montr´eal, Canada. Association\nfor Computational Linguistics.\nYvette Graham, Timothy Baldwin, and Nitika Mathur.\n2015. Accurate Evaluation of Segment-level Ma-\nchine Translation Metrics. In Proceedings of the\n2015 Conference of the North American Chapter\nof the Association for Computational Linguistics:\nHuman Language Technologies, pages 1183–1191,\nDenver, Colorado. Association for Computational\nLinguistics.\nMatt J. Kusner, Yu Sun, Nicholas I. Kolkin, and Kil-\nian Q. Weinberger. 2015. From Word Embeddings\nto Document Distances. In Proceedings of the 32nd\nInternational Conference on International Confer-\nence on Machine Learning - Volume 37, pages 957–\n966.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. arXiv preprint,\nabs/1901.07291.\nLajanugen Logeswaran and Honglak Lee. 2018. An\nefﬁcient framework for learning sentence represen-\ntations. In Proceedings of the 6th International Con-\nference on Learning Representations (ICLR 2018).\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. B LEU : a Method for Automatic\nEvaluation of Machine Translation. In Proceed-\nings of the 40th Annual Meeting of the Associa-\ntion for Computational Linguistics, pages 311–318,\nPhiladelphia, Pennsylvania, USA. Association for\nComputational Linguistics.\nYing Qin and Lucia Specia. 2015. Truly Exploring\nMultiple References for Machine Translation Evalu-\nation. In Proceedings of the 18th Annual Conference\nof the European Association for Machine Transla-\ntion, pages 113–120, Antalya, Turkey.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural Machine Translation of Rare Words\nwith Subword Units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nHiroki Shimanaka, Tomoyuki Kajiwara, and Mamoru\nKomachi. 2018. RUSE: Regressor Using Sen-\ntence Embeddings for Automatic Machine Transla-\ntion Evaluation. In Proceedings of the Third Confer-\nence on Machine Translation: Shared Task Papers,\npages 751–758, Belgium, Brussels. Association for\nComputational Linguistics.\nHiroki Shimanaka, Tomoyuki Kajiwara, and Mamoru\nKomachi. 2019. Machine Translation Evalu-\nation with BERT Regressor. arXiv preprint ,\nabs/1907.12679.\nAndre T ¨attar and Mark Fishel. 2017. bleu2vec: the\nPainfully Familiar Metric on Continuous Vector\nSpace Steroids. In Proceedings of the Second Con-\nference on Machine Translation, pages 619–622,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8167257905006409
    },
    {
      "name": "Machine translation",
      "score": 0.7924066781997681
    },
    {
      "name": "Natural language processing",
      "score": 0.6434341073036194
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5606038570404053
    },
    {
      "name": "Translation (biology)",
      "score": 0.5252670645713806
    },
    {
      "name": "Example-based machine translation",
      "score": 0.4664100408554077
    },
    {
      "name": "Transfer-based machine translation",
      "score": 0.4663882255554199
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    }
  ]
}