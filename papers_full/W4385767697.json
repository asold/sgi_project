{
  "title": "Case-Based Reasoning with Language Models for Classification of Logical Fallacies",
  "url": "https://openalex.org/W4385767697",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4322412898",
      "name": "Zhivar Sourati",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A13399620",
      "name": "Filip Ilievski",
      "affiliations": [
        "University of Southern California"
      ]
    },
    {
      "id": "https://openalex.org/A4322412902",
      "name": "Hông-Ân Sandlin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2770699817",
      "name": "Alain Mermoud",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6678591960",
    "https://openalex.org/W2891163418",
    "https://openalex.org/W2946595845",
    "https://openalex.org/W1488351822",
    "https://openalex.org/W6863994431",
    "https://openalex.org/W2996035354",
    "https://openalex.org/W3006188107",
    "https://openalex.org/W2970487286",
    "https://openalex.org/W3029732686",
    "https://openalex.org/W4226418288",
    "https://openalex.org/W6758192945",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W4285602624",
    "https://openalex.org/W1994335990",
    "https://openalex.org/W6768795989",
    "https://openalex.org/W1492717421",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W6776225331",
    "https://openalex.org/W3044165763",
    "https://openalex.org/W2001110444",
    "https://openalex.org/W1525136198",
    "https://openalex.org/W3205808003",
    "https://openalex.org/W4287887517",
    "https://openalex.org/W6864014924",
    "https://openalex.org/W4308243058",
    "https://openalex.org/W6863071542",
    "https://openalex.org/W3207166518",
    "https://openalex.org/W6863631769",
    "https://openalex.org/W4229760088",
    "https://openalex.org/W1986285750",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3034830866",
    "https://openalex.org/W3037105888",
    "https://openalex.org/W4230019621",
    "https://openalex.org/W392511986",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2964611126",
    "https://openalex.org/W2126385963",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W2949729647",
    "https://openalex.org/W3104499181",
    "https://openalex.org/W3086744964",
    "https://openalex.org/W4385567117",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594"
  ],
  "abstract": "The ease and speed of spreading misinformation and propaganda on the Web motivate the need to develop trustworthy technology for detecting fallacies in natural language arguments. However, state-of-the-art language modeling methods exhibit a lack of robustness on tasks like logical fallacy classification that require complex reasoning. In this paper, we propose a Case-Based Reasoning method that classifies new cases of logical fallacy by language-modeling-driven retrieval and adaptation of historical cases. We design four complementary strategies to enrich input representation for our model, based on external information about goals, explanations, counterarguments, and argument structure. Our experiments in in-domain and out-of-domain settings indicate that Case-Based Reasoning improves the accuracy and generalizability of language models. Our ablation studies suggest that representations of similar cases have a strong impact on the model performance, that models perform well with fewer retrieved cases, and that the size of the case database has a negligible effect on the performance. Finally, we dive deeper into the relationship between the properties of the retrieved cases and the model performance.",
  "full_text": "Case-Based Reasoning with Language Models for Classification of Logical\nFallacies\nZhivar Sourati1,2 , Filip Ilievski1,2 , Hˆong- ˆAn Sandlin3 and Alain Mermoud3\n1Information Sciences Institute, University of Southern California, Marina del Rey, CA, USA\n2Department of Computer Science, University of Southern California, Los Angeles, CA, USA\n3Cyber-Defence Campus, armasuisse Science and Technology, Switzerland\n{souratih,ilievski}@isi.edu, {hongan.sandlin,alain.mermoud}@ar.admin.ch\nAbstract\nThe ease and speed of spreading misinformation\nand propaganda on the Web motivate the need to\ndevelop trustworthy technology for detecting fal-\nlacies in natural language arguments. However,\nstate-of-the-art language modeling methods exhibit\na lack of robustness on tasks like logical fallacy\nclassification that require complex reasoning. In\nthis paper, we propose a Case-Based Reasoning\nmethod that classifies new cases of logical fallacy\nby language-modeling-driven retrieval and adapta-\ntion of historical cases. We design four comple-\nmentary strategies to enrich input representation\nfor our model, based on external information about\ngoals, explanations, counterarguments, and argu-\nment structure. Our experiments in in-domain and\nout-of-domain settings indicate that Case-Based\nReasoning improves the accuracy and generaliz-\nability of language models. Our ablation studies\nsuggest that representations of similar cases have a\nstrong impact on the model performance, that mod-\nels perform well with fewer retrieved cases, and\nthat the size of the case database has a negligible\neffect on the performance. Finally, we dive deeper\ninto the relationship between the properties of the\nretrieved cases and the model performance.\n1 Introduction\nThe ease and speed of spreading misinformation [Wu et al.,\n2019; Allcott et al., 2019] and propaganda [Da San Martino\net al., 2019; Barr ´on-Cedeno et al., 2019 ] on the Web moti-\nvate the need to develop trustworthy technology for under-\nstanding novel arguments [Lawrence and Reed, 2020 ]. In-\nspired by centuries of philosophical theories[Aristotle, 1989;\nLocke, 1997; Copi, 1954; Barker, 1965], recent work has pro-\nposed the natural language processing (NLP) task of Logical\nFallacy Detection. Logical Fallacy Detection goes beyond\nprior work on binary detection of misinformation and fake\nnews classification, and aims to classify an argument into one\nof the dozens of fallacy classes. For instance, the argument\nThere is definitely a link between depression and drinking al-\ncoholic drinks. I read about it from Wikipedia belongs to the\nclass Fallacy of Credibility, as the validity of the argument is\nbased on the credibility of the source rather than the argument\nitself. Here, the focus is on informal fallacies that contain\nincorrect or irrelevant premises, as opposed to formal falla-\ncies, which have an invalid structure [Aristotle, 1989]. The\nidentification of informal fallacies is challenging for both hu-\nmans and machines as it requires complex reasoning and also\ncommon knowledge about the concepts involved in the fal-\nlacy [Hansen, 2020]. To predict the correct fallacy type, the\nmodel has to know what Wikipedia is and how it is used in\nsocietal discourse, the potential relationship between depres-\nsion and consuming alcoholic beverages, and also the causal\nlink between the first and second parts of the argument.\nThe currently dominant NLP paradigm of language mod-\nels (LMs) has been shown to struggle with reasoning over\nlogical fallacies [Jin et al., 2022 ] and similar tasks that\nrequire complex reasoning [Da San Martino et al., 2019;\nBarr´on-Cedeno et al., 2019 ]. As LMs are black boxes, at-\ntempts to improve their performance often focus on adapt-\ning their input data. Prior work has pointed to the need to\ninclude context [Vijayaraghavan and V osoughi, 2022], sim-\nplify the input structure [Jin et al., 2022], or perform special\ntraining that considers soft logic [Clark et al., 2021 ]. How-\never, these ideas have not been successful in classifying logi-\ncal fallacies yet. Alternatively, methods that leverage reason-\ning by example, e.g., based on Case-Based Reasoning (CBR),\nhave shown promise in terms of accuracy and explainabil-\nity for other tasks like question answering [Das et al., 2022],\nbut have not been applied to reason over logical fallacies to\ndate. We conclude that integrating such explainable methods\nwith generalizable LMs provides an unexplored opportunity\nto reason over logical fallacies.\nIn this paper, we pursue the question: Does reasoning over\nexamples improve the ability of language models to classify\nlogical fallacies? To answer this question, we develop a\nmethod based on the idea of CBR [Aamodt and Plaza, 1994].\nWe focus on the interpretive problem-solving variant of CBR,\nwhich aims to understand novel cases in terms of previous\nsimilar cases while not necessarily using the solutions from\nprevious cases directly [Leake, 2001]. We adapt this idea to\nthe task of classifying logical fallacies by using LMs as back-\nbones when retrieving and adapting prior similar cases. We\nmeasure the ability of our models in terms of accuracy and\ngeneralizability and also probe their explainability. The main\ncontributions of this paper are as follows:\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5188\n1. We design the first Case-Based Reasoning method for\nlogical fallacy classification to solve new cases based\non past similar cases. The framework implements the\ntheory of CBR with state-of-the-art (SOTA) techniques\nbased on language modeling and self-attention.\n2. We design four enriched case representations: Coun-\nterarguments, Goals, Explanations, and Structure of the\nargument to allow CBR to retrieve and exploit simi-\nlar cases based on implicit information, like argument\ngoals. To our knowledge, we are the first who inves-\ntigate the effect of these case representations on CBR\nperformance.\n3. We perform extensive experiments that investigate the\nimpact of CBR against Transformer LM baselines on in-\ndomain and out-of-domain settings. We perform abla-\ntions to provide insight into the sensitivity of our CBR\nmethod on its parameters and investigate the explana-\ntions extracted from the model.\nWe make our code and data available to support future re-\nsearch on logical fallacy classification. 1 For additional dis-\ncussion, please refer to longer version of the paper on Arxiv.\n2 Method\nCBR [Schank, 1983] is a method that reasons over new cases\nbased on similar past cases with a known label [Aamodt and\nPlaza, 1994 ]. Our CBR formulation (Figure 1) consists of\nthree steps: (1) given a new case, retrieve similar cases from\na case database, (2) adapt fetched similar cases based on\nthe current one, and (3) classify the new case based on the\nadapted exemplars. In this work, we use LMs in the retriever\nand the adapter because of their strong ability to encode and\ncompute similarity for any textual information.\nRetriever. Finding k similar cases Si (i ∈ {1, ..., k}) to\nthe new case C from a case database is retriever’s task. The\nretriever estimates the similarity betweenC and Si by encod-\ning each of them with the same LM encoder and computing\nthe cosine similarity of the resulting encodings. The retriever\nthen picks the k cases with top cosine similarities from the\ndatabase. The new case is concatenated to its similar cases,\ni.e., S = C ⊕ < SEP >⊕S1 ⊕ S2 ⊕ ... ⊕ Sk and is passed\nas input to the CBR adapter.\nAdapter. The framework’s middle part aims to prioritize\nthe most relevant information from S for reasoning over the\nnew case C. Based on the second step of the pipeline by\n[Aamodt and Plaza, 1994 ], after fetching similar cases, it\nmight be the case that only certain retrieved cases would be\nuseful, and therefore, they should be weighted according to\ntheir utility for approaching the new case. The fusion of the\ncurrent case with its previously seen similar problems would\ngive the model the chance to come up with a better represen-\ntation of the current problem, as well as better abstractions\nand generalizations for further uses. The adapter consists of\ntwo parts: an encoder and an attention component. The en-\ncoder is an LM that takes as an input C and S separately,\nthen outputs their respective embedding representations EC\n1https://github.com/zhpinkman/CBR\nFigure 1: Three stages of the CBR pipeline. Using the new case\nC, the retriever finds k similar cases {S1, S2, ..., Sk} and creates\nS = C ⊕ < SEP >⊕S1 ⊕ S2 ⊕ ... ⊕ Sk. The adapter processes\nboth the new case and fetched similar cases and tries to adapt S\nbased on the new case C, and extracts more abstract information\nfrom the fusion of the two. Finally, the classifier receives the adapted\ninformation and returns the probabilities associated with the new\nclass belonging to each fallacy type. In the example, k = 1.\nand ES. We use the hidden states of the last layer of the LM\nas the input embedding. A multi-headed attention component\n[Vaswani et al., 2017 ] with H heads selects the most use-\nful information from the similar cases embeddings ES given\nthe embedding of the new case EC. As commonly done in\nTransformer architectures, the Adapter generates Value and\nKey vectors from ES and Query vectors from EC. The dot\nproduct of the Query and Key vectors, fed through a soft-\nmax layer, results in an Attention vector, which indicates the\nimportance of each token in S when generating the adapted\nvector A. An adapted vector with adjusted attention on its\nelements is produced by the weighted sum of the Value vec-\ntors based on Attention weights. The output of the attention\ncomponent is A, the adjusted embedding of ES.\nClassifier. Last part of the framework predicts the final\nclass based on the adapter output A. The classifier is de-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5189\nsigned as a fully connected neural layer with a depth d and\nan activation function. The objective function of the classifier\nis the cross-entropy loss. The cross-entropy loss is computed\nover the probabilities that are extracted from C logits corre-\nsponding to each of the C classes. Also, during training, the\nretriever’s component weights are frozen while the rest of the\nframework is trained in an end-to-end fashion.\nOverall, our CBR architecture resembles a standard\n‘vanilla’ LM with a classification head but brings the addi-\ntional benefit of having access to prior relevant labeled cases\nweighed based on the attention mechanism. 2 We hypoth-\nesize that the CBR models bring two benefits over vanilla\nLMs: (1) the integration of similar labeled cases helps the\nmodel analyze the new fallacious argument better and clas-\nsify it more accurately, and (2) provides explicit insights into\nthe reasoning of the model by yielding similar cases to the\ncurrent one [Renkl, 2014].\n3 Case Representation\nMerely retrieving labeled cases may not be sufficient for rea-\nsoning on new cases, as it is unclear what dimensions of sim-\nilarity their relevance is based on. For instance, two cases\nmay be similar in terms of their explanation, structure, or the\ngoal behind the cases. As these dimensions are implicit and\nnot apparent from the plain text, we make them explicit by\nenriching the original text of the case with such information.\nWe consider four representations in which the case formula-\ntion is enriched with its counterargument, goal, explanation,\nand structure. As a baseline, we also include the original\ntext without any enrichments. Table 1 illustrates examples of\nthese representations for the sample case There was a thun-\nderstorm with rain therefore I did not finish my homework.\nEach of the enrichment strategies r modifies the case\nrepresentation by concatenating it with additional informa-\ntion, r(case). We introduce a case representation function\nR(case, r) that concatenates case with additional informa-\ntion r(case) resulting in case ⊕ r(case). These representa-\ntions modify both the new case C to R(C, r) and cases from\nthe database Si to R(Si, r), and change the cosine similarity\nto be computed between enriched cases instead of plain text.\nWe next describe the design of the enrichment strategies.\nCounterarguments. Counterarguments are common in\npersuasive writing, where they explain why one’s position is\nstronger than the counterargument and serve as a preemptive\naction to anticipate and remove any doubts about arguments\n[Harvey, 2009]. We hypothesize that counterarguments are\noften implicit in the arguments, and would therefore be useful\nto be provided directly to the model. For instance, in the argu-\nment presented in Table 1, although the plain text claims that\nthe reason for not finishing the homework is the heavy rain,\nthe counterargument points outother reasons for not finishing\nthe homework such as the person being too tired.\nGoals. Studies of argumentation often focus on the inter-\nplay between the goals that the writer is pursuing and their\nargumentations [Tracy, 2013]. Thus, when classifying logical\n2Our experiments using the framework without the attention\nmechanism consistently showed sub-optimal performance.\nfallacies, we expect that it is beneficial to take into account the\ngoals of the arguments. The goal may be entirely missing in\nthe argument’s text, or the argument may implicitly hint at the\ngoal. An example of the latter is shown in Table 1, where the\nphrase therefore I did not finish my homework alludes to the\nimplicit goal of the writer to justify not finishing their home-\nwork. As shown in this example, we include an explicit goal\nstatement to fill this gap.\nExplanations. By using explanations about logically falla-\ncious arguments, we aim to augment the arguments with a\nbroader notion of information that might be useful for classi-\nfying logical fallacies but is not already included in the orig-\ninal argument, such as reasoning steps getting from premises\nto conclusions of an argument [Barker, 1965]. As we do not\nimpose any restrictions on the explanations, their content may\noverlap with the previous two representations. Alternatively,\nexplanations may provide different complementary informa-\ntion. Such is the example in Table 1 that discusses the causal\nrelationship between two events that are not actually related.\nThus, the explanation acts as a general gap-filling mechanism\nthat can provide any relevant information that is missing in\nthe original argument.\nStructure. Tasks like logical fallacy classification involve\nhigher-order relation comprehension that is often based on\nthe structure rather than the content of the argument. In that\nsense, the semantics of specific entities and concepts in the ar-\ngument may be misleading to the model. Similarly to [Jin et\nal., 2022], we hypothesize that focusing on the logical struc-\nture of an argument rather than its content is beneficial for the\nmodel’s performance [Gabbay et al., 2004 ]. An example of\na structural simplification of an argument is presented in Ta-\nble 1. While this simplification may help the model grasp the\ncase structure more directly, the structure formulation may\nnot detect the implicit causal links between the thunderstorm\n(X) and the homework (Z).\nWe extract the enrichment information for a case using a\ncombination of few-shot and zero-shot prompting with two\nSOTA models: ChatGPT [OpenAI, 2022] and Codex [Chen\net al., 2021 ]. Given a representation strategy r, we prompt\nChatGPT to get the representations for a case for five differ-\nent examples using one template per representation. For in-\nstance, we use the template Express the goal of the argument\n{case} to retrieve the goals of the argument case. The five\nobtained examples per representation are used as demonstra-\ntions to prompt Codex in a few-shot manner. For a representa-\ntion strategy r, we use the same demonstrations together with\neach new case C from our task as input to the Codex model,\nwhich yields enrichment information r(case) per case. In\nthis manner, we combine the strong zero-shot ability of the\nclosed-source ChatGPT model with the few-shot generation\nstrength of the Codex model.\n4 Experimental Setup\nIn this section, we describe the evaluation data and metrics,\nthe baselines we compare to, and the implementation details.\nEvaluation dataset. We use two logical fallacy datasets\nfrom [Jin et al., 2022 ], called LOGIC and LOGIC Climate.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5190\nRepresentation Transformed Text\nGoals It’s possible that the goal is to explain why the speaker did not finish their homework. The speaker may be trying to\nconvince the listener that they did not finish their homework because of the thunderstorm.\nCounterarg. There are many factors that contribute to a person’s ability to complete their homework, and it’s not fair to suggest\nthat the thunderstorm was the only factor. It’s possible that the person did not finish their homework because they\nwere distracted by the thunderstorm or because they were tired.\nExplanations It presents a causal relationship between two events that might not be actually related.\nStructure There was an X with Y therefore I did not do Z.\nTable 1: One example of different representations for the case There was a thunderstorm with rain therefore I did not finish my homework.\nThe LOGIC dataset includes thirteen logical fallacy types\nabout common topics, namely: Ad Hominem, Ad Populum,\nAppeal to Emotion, Circular Reasoning, Equivocation, Fal-\nlacy of Credibility, Fallacy of Extension, Fallacy of Logic,\nFallacy of Relevance, False Causality, False Dilemma, Faulty\nGeneralization, and Intentional. LOGIC Climate dataset con-\nsists of more challenging examples for the same logical fal-\nlacy types on the climate change topic. We use LOGIC\nfor in-domain evaluation and LOGIC Climate for out-of-\ndomain evaluation. As LOGIC dataset is severely imbal-\nanced, we augment its train split using two techniques, i.e.,\nback-translation, and substitution of entities in the arguments\nwith their synonymous terms. This augmentation makes\nLOGIC dataset train split have 281 arguments for each fallacy\ntype. Note that we do not fine-tune our model on the LOGIC\nClimate dataset in our experiments to evaluate the general-\nizability of our framework. We model the classification task\nas a multi-class classification problem and use the customary\nmetrics of weighted precision, recall, and F1-score.\nBaselines. We consider three different LMs: BERT[Devlin\net al., 2018 ], RoBERTa [Liu et al., 2019 ], and ELECTRA\n[Clark et al., 2020]. We apply our CBR method (§2) on each\nof these models. As baselines, we use vanilla LMs without\na CBR extension. We also compare against Codex in a few-\nshot setting, with the prompt including all the possible classes\nas well as one example for each class resulting in thirteen la-\nbeled examples in the prompt. Finally, we include the results\nof a frequency-based predictor that predicts fallacy classes\nbased on the distribution of fallacy types in the training set.\nImplementation details. We use SimCSE [Gao et al.,\n2021], a transformer-based retriever that is optimized for cap-\nturing overall sentence similarity, to compute the similarity\nbetween cases (§2) and also use H = 8 heads for the multi-\nheaded attention component. The depth of our classifier is\nd = 2. It uses gelu [Hendrycks and Gimpel, 2016] as an ac-\ntivation function. We analyze the performance of our model\nusing k ∈ {1,2, 3, 4, 5}. To test the generalization of our\nmodel with sparser case databases, we experiment with vari-\nous ratios of the case database within {0.1, 0.4, 0.7, 1.0}.\n5 Results\nIn this section, we measure the effectiveness of CBR per\nmodel and case representation. We further provide ablations\nthat measure the sensitivity of the model to the size of the\ncase database and the number of cases. Finally, we present a\nLOGIC LOGIC\nClimate\nModel T\nype P R F1 P R F1\nFreq-based baseline\n0.094 0.094 0.093 0.120 0.079 0.080\nCodex few-shot 0.594 0.422 0.386 0.198 0.093 0.077\nELECTRA baseline\n0.614 0.602 0.599 0.276 0.229 0.217\nCBR 0.663 0.664 0.657 0.355 0.254 0.270\nRoBERT\na baseline 0.577 0.561 0.560 0.237 0.211 0.200\nCBR 0.631 0.619 0.619 0.379 0.248 0.245\nBERT\nbaseline 0.585 0.598 0.586 0.166 0.130 0.120\nCBR 0.613 0.616 0.611 0.359 0.204 0.200\nTable 2: Comparison of the best results of the CBR framework with\nvanilla LMs and two external baselines on two benchmarks focusing\non both in-domain (LOGIC) and out-of-domain (LOGIC Climate)\nsettings. The best results per model are boldfaced and the overall\nbest results are underlined.\nqualitative analysis of the explainability of CBR and a thor-\nough discussion about how retrieved cases help to classify\nnew ones.\nImpact of CBR. Table 2 shows the performance of the\nCBR framework and relevant baselines. For each model,\nwe present the results using the best case representation per\nmodel and using k = 1 while exploiting 10% of the case\ndatabase that we found to yield the best results among all pos-\nsible combinations. Overall, the CBR method brings a con-\nsistent and noticeable quantitative improvement in the clas-\nsification of logical fallacies by LMs. For each of the three\nLMs, CBR outperforms the vanilla baselines by 2.5 - 6 abso-\nlute F1 points on the in-domain dataset and up to 8 points on\nthe out-of-domain dataset. Furthermore, CBR outperforms\nCodex, which is utilized in a few-shot setting, despite it be-\ning a much larger model. Across the different LMs, ELEC-\nTRA is achieving the best score and benefits the most from\nthe CBR framework on the in-domain benchmark, which we\nattribute to its efficiency of pre-training [Clark et al., 2020].\nThe same pattern of the superiority of CBR over vanilla LMs\ncan be observed for the other two models with different pre-\ntraining procedures and varying numbers of internal param-\neters. The CBR method notably and consistently improves\nthe performance of the LMs on the out-of-domain (LOGIC\nClimate) benchmark as well, with ELECTRA performing the\nbest and BERT benefiting the most from CBR.3 We conclude\n3Per-class experiments demonstrated the ability of CBR model\nto improve the accuracy of baseline models for all fallacy types, es-\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5191\nthat CBR is a general framework that can be applied to any\nLM and can generalize well to unseen data and to various fal-\nlacy classes. The generalization of CBR is in line with prior\nwork that suggests its strong performance on tasks with data\nsparsity [Das et al., 2020].\nLOGIC LOGIC\nClimate\nModel Repr\nesentation P R F1 P R F1\nELECTRA Te\nxt 0.655 0.634 0.635 0.317 0.242 0.242\nCounterarg. 0.663 0.664 0.657 0.355 0.254 0.270\nGoals 0.646 0.622\n0.621 0.376 0.217 0.222\nStructure 0.634 0.625 0.618 0.375 0.254 0.269\nExplanations 0.605 0.580 0.578 0.314 0.242 0.237\nRoBERT\na Text 0.633 0.613 0.619 0.343 0.236 0.251\nCounterarg. 0.624 0.613 0.615 0.367 0.198 0.216\nGoals 0.632 0.613 0.619 0.351 0.242 0.263\nStructure 0.631 0.619 0.619 0.379 0.248 0.245\nExplanations 0.575 0.558\n0.559 0.359 0.192 0.181\nBERT T\next 0.595 0.604 0.596 0.311 0.192 0.204\nCounterarg. 0.607 0.613 0.603 0.342 0.217 0.228\nGoals 0.598 0.607 0.596 0.310 0.204 0.203\nStructure 0.613 0.616 0.611 0.359 0.204 0.200\nExplanations 0.540 0.531 0.532 0.274 0.217 0.190\nTable 3: Performance of the CBR framework using different case\nrepresentations. The best results per model are boldfaced and the\noverall best results are underlined.\nEffect of different representations. The results in Table\n3 confirm our expectation that the case representation plays\nan important role in the effectiveness of the CBR frame-\nwork. Depending on the LM used, the performance dif-\nference among different case representations ranges from 6\nto 8% F1-scores for the in-domain setting and 4 to 8% F1-\nscores for the out-of-domain setting. In general, we observe a\nboost in performance when enhancing the original represen-\ntation (text). Counterargument information yields the highest\nboost, though the impact of the representations varies across\nmodels. Using ELECTRA, the enrichment with counterar-\nguments helps the most, outperforming the model based on\nthe original text and the other enrichment strategies. With\nRoBERTa, goals and structure of the arguments perform on\npar with text, while with BERT, including information about\ncounterarguments and argument structure outperforms the\ntext representation. As the LMs have been trained with differ-\nent data and may optimize for different notions of similarity,\nit is intuitive that the impact of the case representations varies\nacross models. This finding is in line with theoretical work,\nwhich discusses that knowledge transfer is strictly guided\nby the similarity function of the reasoning model [Holyoak\nand Thagard, 1996]. Meanwhile, using a generic enrichment\nwith explanations performs consistently poorly and harms\nthe model performance, which suggests that the CBR mod-\nels benefit from more precise case representations.\nEffect of case database size. Next, we investigate the sen-\nsitivity of the best-performing CBR model based on ELEC-\nTRA to the size of the case database. Figure 2 (left) depicts\nthe performance of this model using different ratios of the\ncase database. The figure shows that the CBR framework\nconsistently outperforms the vanilla LM baseline (with 0%\npecially the ones having the least number of training examples.\nFigure 2: Performance of the CBR framework using different ratios\nof the case database (left) and different numbers of cases (right). The\nbaseline is outlined as the dotted line.\nof cases) on in- and out-of-domain settings. This trend stands\nregardless of the size of the case database, which indicates\nthe low sensitivity of the CBR model to the case database\nsize. However, we note that using 10% of the case database\nyields the best performance, which indicates that a limited\ncase database offers a better potential of abstraction to CBR.\nMoreover, comparing the performance of the model using\ndifferent ratios of the case database, we observe a continu-\nous decrease in the performance using higher percentages of\nthe case database. Having access to too much data makes\nthe model dependent and sensitive to the unnecessary and in-\nsignificant details of similar cases retrieved. These observa-\ntions point us to the data efficiency properties of the CBR\nframework [Das et al., 2020].\nEffect of different number of cases. The performance of\nthe best CBR model that uses ELECTRA with different num-\nbers of cases is illustrated in Figure 2 (right). In both in-\ndomain and out-of-domain settings, we observe a consistent\npattern of performance decrease when more cases are taken\ninto account in the reasoning process. The CBR framework\nreaches its peak performance using only one similar case\nwhile once more outperforming the vanilla LM (with 0 cases)\nin all the settings. This indicates that the models get easily\noverwhelmed with past information when considering a new\ncase. While intuitively, one would expect that a larger num-\nber of cases should help the model analyze a new case better,\nthe reasoner should have the capacity to process all of these\npast cases. Otherwise, as observed in this experiment, includ-\ning more cases can have an adverse effect on the reasoner by\ndistracting it rather than helping it.\nCase study on explainability. A key promise of the CBR\nframework is its native explainability by cases since its re-\ntrieval of similar cases and reasoning over them are integrated\ninto the CBR process. We perform a qualitative analysis of\nthe cases retrieved by CBR to develop a better intuition about\nits reasoning process. Table 4 illustrates four example cases\nthat the vanilla LM classifies incorrectly. For each case, we\nshow two CBR representations: one leading to a correct pre-\ndiction and one leading to an incorrect one. The first example\nshows the scenario where the original text of a retrieved case\ndoes not suffice for the model to reason correctly, despite its\ntopical surface similarity to the input case. In other words,\nthe high surface similarity of similar cases is confusing the\nmodel and forcing it to incorrectly predict the same class that\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5192\nInput Sentence Enriched Representation for Correct\nPrediction (representation)\nEnriched Representation for Wrong Predic-\ntion (representation) (predicted class)\nClass\nPeople who don’t sup-\nport the proposed mini-\nmum wage increase hate\nthe poor.\nThere are often multiple perspectives on\nan issue. It’s possible to have a nuanced or\nbalanced view that doesn’t align with any\nside completely. (Counterarg.)\nThat candidate wants to raise the minimum wage,\nbut they aren’t even smart enough to run a busi-\nness. (Text) (Ad Hominem)\nFallacy of\nExtension\nThe house is white;\ntherefore it must be big.\nX is y; therefore, it is z. (Structure) The sentence ”People who drive big cars hate\nthe environment” presents a generalization about\na group of people without sufficient evidence\nand it relies on oversimplification.(Explanations)\n(Faulty Generalization)\nFallacy of\nLogic\nStudent: You didn’t teach\nus this; we never learned\nthis. Teacher: So, you’re\neither lazy or unwilling\nto learn is that right?\nIt’s possible that the argument ”It’s pos-\nsible to pass the class without attending.\nso, you will pass even if you don’t attend”\nis trying to convince the listener that they\nwill pass the class even if they don’t at-\ntend. The speaker may be trying to per-\nsuade the listener to skip class. (Goals)\nThe sentence ”Teacher: You are receiving a zero\nbecause you didn’t do your homework. Students:\nAre you serious? You gave me a zero because\nyou hate me?” attacks the person making the ar-\ngument rather than the argument itself. (Expla-\nnations) (Fallacy of Extension)\nFalse\nDilemma\nOne day, Megan wore a\nDonald Duck shirt, and\nshe got an A on her test.\nNow she wears that shirt\nevery day to class.\nThere are many factors that contribute to a\nstudent’s grade, and it’s not fair to suggest\nthat the student’s past grades are the only\nfactor. It’s possible that the student failed\nthe test because they didn’t study, or be-\ncause they were sick. (Counterarg.)\nThe sentence ”Eating five candy bars and drink-\ning two sodas before a test helps me get better\ngrades. I did that and got an A on my last test\nin history” presents a causal relationship between\ntwo events without sufficient evidence to support\nthe claim. (Explanations) (Fallacy of Relevance)\nFalse\nCausality\nTable 4: Four examples from different classes in which the CBR model predicts the correct class. For each example, we show a representation\nthat leads to a correct prediction and a representation that still leads to predicting the wrong class. We also show the corresponding wrong\nclass predicted by the second variation of the model.\nis associated with the retrieved similar case. However, we\nsee that enriching the case with itscounterargument helps the\nCBR model, even though the counterargument is phrased in\nan abstract manner and is not similar to the new case on the\nsurface. We observe a similar situation with the explanations\nenrichment in the third example, having high surface simi-\nlarity between the retrieved case and the new one, where an-\nalyzing the argument goals instead helps the model. In the\nsecond example, the structure of the argument and the logical\ndepiction of the past cases help the most, while in the fourth\nexample, the counterarguments assist the reasoning of CBR.\nFrom the second and the fourth example, we observe that en-\nriching arguments with cases that are semantically far from\nthe new case is confusing for the CBR model, even if their\nreasoning would be helpful.\nLOGIC LOGIC Climate\nRepresentation ground\ntruth\noverlap\npredictions\noverlap\nground\ntruth\noverlap\npredictions\noverlap\nText 0.184 0.232 0.136 0.173\nCounterarg. 0.208 0.220 0.062 0.068\nGoals 0.178 0.196 0.130 0.124\nStructure 0.238 0.250 0.105 0.242\nExplanations 0.277 0.447 0.086 0.478\nTable 5: Overlap of retrieved cases’ labels with true labels and pre-\ndictions of the best CBR model (ELECTRA). We highlight the high-\nest overlaps in bold.\nIn summary, presented examples show that the retrieved\ncases help the model indirectly by providing CBR with high-\nlevel information (first example), symbolic abstractions (sec-\nond example), extensive analysis of the writer’s goal (third\nexample), and alternative possibilities (fourth example). This\nbrings up a natural question: does CBR performance correlate\nto class overlap between the current case and retrieved similar\ncases? In other words, can we label a new case solely based\non its k-nearest neighbors’ labels? To answer this question,\nwe compute the overlap of retrieved cases’ labels with both\nthe true and the predicted label for different case representa-\ntions (Table 5). We observe a low overlap of a maximum of\n27.7% between the retrieved cases’ labels and the true labels,\nwhich is only slightly better than a frequency-based predic-\ntion. Also, centering on the direct effect of retrieved cases on\nthe CBR predictions, the model with the highest class over-\nlap between the retrieved cases and the predicted classes also\nhas the lowest performance (explanations). Meanwhile, the\nbest CBR variants (e.g., counterarguments) do not directly\nreuse the labels of the retrieved cases. We conclude that while\nretrieving similar cases provides the CBR models with use-\nful information, this additional evidence influences the model\nreasoning indirectly and may have adverse effects otherwise.\nAlthough CBR, in its simplest form, can act as a k-nearest\nneighbors algorithm, our results suggest that the neighbors’\nlabels cannot be used blindly, and further reasoning step over\nthe retrieved cases is necessary. We believe that these findings\nopen exciting future research directions that investigate the\nrelationship between case similarity and CBR performance.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5193\n6 Related Work\nIn this section, we present prior research on logical fallacy\nclassification, CBR, and methods that prompt very large LMs.\nLogical fallacy. Prior computational work on logical fal-\nlacies has mostly focused on formal fallacies using rule-\nbased systems and theoretical frameworks [Nakpih and San-\ntini, 2020]. Nevertheless, recent work has switched attention\nto informal logical fallacies and natural language input. Jin\net al. propose the task of logical fallacy classification, con-\nsidering thirteen informal fallacy types and two benchmarks.\nThe authors gather a rich set of arguments containing vari-\nous logical fallacies from online resources and evaluate the\ncapabilities of large LMs in classifying logical fallacies both\nin in-domain and out-of-domain settings. Similarly, Goffredo\net al. present a dataset of political debates from U.S. Presi-\ndential Campaigns and use it to evaluate Transformer LMs.\nProcessing different parts of arguments, such as dialogue’s\ncontext, they create separate expert models for each part of\narguments and train all the models together, from which they\nreport the importance of discussion context in argument un-\nderstanding. Although LMs have been used to classify logi-\ncal fallacies, both independently and in an ensemble setting,\nto our knowledge, no prior work has tried to improve LMs’\ncapabilities to reason over previous cases of logical fallacies\nencountering a new case nor experimented with enriching the\nargument representation. We fill this gap by employing CBR\nwith LMs to reason over similar past cases to classify logical\nfallacies in new cases.\nCase-Based Reasoning. Case-Based Reasoning [Schank,\n1983] has been a cornerstone of interpretable models in many\nareas. For instance, researchers have applied CBR over past\nexperiences in mechanical engineering [Qin and Regli, 2003]\nand medical applications[Oyelade and Ezugwu, 2020]. Case-\nBased Reasoning has been also used in education, particu-\nlarly to teach students to recognize fallacies [Spensberger\net al., 2022 ]. Exploiting its interpretable properties, Walia\net al. use Case-Based Reasoning as a transparent model for\nWord Sense Disambiguation, Br ¨uninghaus and Ashley use\nCase-Based Reasoning for predicting legal cases an inter-\npretable pipeline, while Ford et al. use Case-Based Reason-\ning to enhance the transparency of classifications made on\nwritten digits [Lecun et al., 1998]. Inspired by its advantages,\nwe couple Case-Based Reasoning with LMs, leading to en-\nhanced accuracy and explainability of classifying logical fal-\nlacies. To our knowledge, this is the first work that combines\nCBR with LMs for complex tasks like logical fallacy clas-\nsification. Nevertheless, there are frameworks that are close\nto CBR that also have a notion of memory, but cannot serve\nas replacements, given their restrictions. Analogical reason-\ning [Gentner and Smith, 2012 ] methods typically focus on\nproportions between words or short text sequences and can-\nnot generalize well to unstructured text. K-nearest neighbor\nmethods are a simplified version of CBR that, given our ob-\nservations, can not perform as well as CBR. Our framework\ncan also be seen broadly as a memory-based model [Weston\net al., 2014 ], however, our proposed formulation that com-\nbines CBR and LMs has not been explored before for tasks\nlike logical fallacy classification.\nPrompting LMs. The behavior of LMs is dependent on the\nquality of their inputs. Aiming to create more comprehensive\ninputs for LMs and assist them in complex reasoning tasks,\nresearchers have attempted to transfer knowledge from very\nlarge LMs to smaller ones. Shwartz et al. show that LMs\ncan discover useful contextual information about the question\nthey answer, from another LM. Wang et al. propose an LM\npipeline that learns to faithfully reason over prompt-based ex-\ntracted rationales. Wei et al. explore how generating a series\nof intermediate reasoning steps using prompting can equip\nLMs with complex reasoning skills. Inspired by the ability of\nlarge LMs to provide relevant information for novel inputs, as\nwell as prior work that performs knowledge distillation from\nlarge to smaller LMs[West et al., 2021], we use prompting to\nenrich the arguments containing logical fallacies. According\nto [Barker, 1965], logical fallacies are created by transition\ngaps from premises to conclusions, and we try to enrich the\narguments using prompting to cover the gaps. Our method re-\nsembles retrieval-augmentation methods [Lewis et al., 2020],\nyet, our enrichment strategies are novel and have not been\nexplored on such complex tasks.\n7 Conclusions and Future Work\nIn this paper, we presented a novel method that uses Case-\nBased Reasoning with LMs to classify logical fallacies. The\nCBR method reasons over new cases by utilizing past experi-\nences. To do so, the method retrieves the most relevant past\ncases, adapts them to meet the needs of a new case, and fi-\nnally classifies the new case using the adjusted information\nfrom past cases. We devised four auxiliary case represen-\ntations that enrich the cases with implicit information about\ntheir counterarguments, goals, structure, and explanations.\nOur results showed that CBR can classify logical fallacies and\ncan leverage past experiences to fill the gaps in LMs. CBR\noutperformed the LM baselines in all settings and across all\nthirteen logical fallacy classes. CBR was able to generalize\nwell and transfer its knowledge to out-of-domain setting. The\nrepresentation of its cases played a key role: enriching cases\nwith counterarguments helped the most, while adding generic\nexplanations harmed the model’s performance. Furthermore,\nCBR models performed best when a small number of cases\nare provided, but showed low sensitivity to the size of the\ncase database. Finally, our qualitative analysis demonstrated\nthe value of CBR as an interpretable framework that benefits\nfrom past similar cases indirectly.\nSince our experiments showed that similar cases assist\nCBR indirectly, future research should further qualify the re-\nlationship between the information provided by the retrieved\ncases and the performance of the model. Moreover, future\nwork should focus on evaluating CBR on other natural lan-\nguage tasks that require abstraction, such as propaganda de-\ntection and dialogue modeling. For instance, given a task-\noriented dialogue about cooking a new meal, the model may\nbenefit from procedures for cooking similar meals. The ap-\nplication of CBR on such tasks might also inspire additional\ncase enrichment strategies, e.g., that describe the causal rela-\ntion between text chunks, and point to additional knowledge\ngaps that CBR needs to fill.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5194\nAcknowledgements\nZhivar Sourati has been supported by armasuisse Science and\nTechnology, Switzerland under contract No. 8003532866,\nand NSF under Contract No. IIS-2153546, while Filip\nIlievski is sponsored by the DARPA MCS program under\nContract No. N660011924033 with the US Office Of Naval\nResearch.\nReferences\n[Aamodt and Plaza, 1994] Agnar Aamodt and Enric Plaza.\nCase-based reasoning: Foundational issues, methodolog-\nical variations, and system approaches. AI Communica-\ntions, 7:39–59, 1994. 1.\n[Allcott et al., 2019] Hunt Allcott, Matthew Gentzkow, and\nChuan Yu. Trends in the diffusion of misin-\nformation on social media. Research & Politics,\n6(2):2053168019848554, 2019.\n[Aristotle, 1989] Aristotle. On sophistical refutations: On\nComin to be passing away - on the cosmos v. 3. Loeb\nClassical Library. LOEB, London, England, July 1989.\n[Barker, 1965] Stephen Francis Barker. The Elements of\nLogic. New York: Mcgraw-Hill, 1965.\n[Barr´on-Cedeno et al., 2019] Alberto Barr ´on-Cedeno, Israa\nJaradat, Giovanni Da San Martino, and Preslav Nakov.\nProppy: Organizing the news based on their propagan-\ndistic content. Information Processing & Management ,\n56(5):1849–1864, 2019.\n[Br¨uninghaus and Ashley, 2006] Stefanie Br ¨uninghaus and\nKevin D Ashley. Progress in textual case-based reason-\ning: predicting the outcome of legal cases from text. In\nAAAI, pages 1577–1580, 2006.\n[Chen et al., 2021] Mark Chen, Jerry Tworek, Heewoo Jun,\nQiming Yuan, Henrique Ponde de Oliveira Pinto, Jared\nKaplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. Evaluating large language models\ntrained on code. arXiv preprint arXiv:2107.03374, 2021.\n[Clark et al., 2020] Kevin Clark, Minh-Thang Luong,\nQuoc V . Le, and Christopher D. Manning. ELECTRA:\nPre-training text encoders as discriminators rather than\ngenerators. In ICLR, 2020.\n[Clark et al., 2021] Peter Clark, Oyvind Tafjord, and Kyle\nRichardson. Transformers as soft reasoners over language.\nIn Proceedings of the Twenty-Ninth International Joint\nConference on Artificial Intelligence, IJCAI’20, 2021.\n[Copi, 1954] Irving M. Copi. Introduction to logic. Philoso-\nphy, 29(110):271–271, 1954.\n[Da San Martino et al., 2019] Giovanni Da San Martino, Se-\nunghak Yu, Alberto Barr´on-Cedeno, Rostislav Petrov, and\nPreslav Nakov. Fine-grained analysis of propaganda in\nnews article. InProceedings of the 2019 conference on em-\npirical methods in natural language processing and the 9th\ninternational joint conference on natural language pro-\ncessing (EMNLP-IJCNLP), pages 5636–5646, 2019.\n[Das et al., 2020] Rajarshi Das, Ameya Godbole, Shehzaad\nDhuliawala, Manzil Zaheer, and Andrew McCallum. A\nsimple approach to case-based reasoning in knowledge\nbases. In Automated Knowledge Base Construction, 2020.\n[Das et al., 2022] Rajarshi Das, Ameya Godbole, Ankita\nNaik, Elliot Tower, Manzil Zaheer, Hannaneh Hajishirzi,\nRobin Jia, and Andrew McCallum. Knowledge base ques-\ntion answering by case-based reasoning over subgraphs.\nIn International Conference on Machine Learning, pages\n4777–4793. PMLR, 2022.\n[Devlin et al., 2018] Jacob Devlin, Ming-Wei Chang, Ken-\nton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805, 2018.\n[Ford et al., 2020] Courtney Ford, Eoin M. Kenny, and\nMark T. Keane. Play mnist for me! user studies on the\neffects of post-hoc, example-based explanations &; error\nrates on debugging a deep learning, black-box classifier.\narXiv preprint arXiv:2009.06349, 2020.\n[Gabbay et al., 2004] Dov M Gabbay, John Hayden Woods,\net al. Handbook of the History of Logic, volume 2009.\nElsevier North-Holland, 2004.\n[Gao et al., 2021] Tianyu Gao, Xingcheng Yao, and Danqi\nChen. SimCSE: Simple contrastive learning of sentence\nembeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Processing,\npages 6894–6910, Online and Punta Cana, Dominican Re-\npublic, November 2021. Association for Computational\nLinguistics.\n[Gentner and Smith, 2012] D. Gentner and L. Smith. Ana-\nlogical reasoning. In V .S. Ramachandran, editor,Encyclo-\npedia of Human Behavior (Second Edition), pages 130–\n136. Academic Press, San Diego, second edition edition,\n2012.\n[Goffredo et al., 2022] Pierpaolo Goffredo, Shohreh Had-\ndadan, V orakit V orakitphan, Elena Cabrio, and Serena Vil-\nlata. Fallacious argument classification in political de-\nbates. In Thirty-First International Joint Conference on\nArtificial Intelligence {IJCAI-22}, pages 4143–4149. In-\nternational Joint Conferences on Artificial Intelligence Or-\nganization, 2022.\n[Hansen, 2020] Hans Hansen. Fallacies. In Edward N. Zalta,\neditor, The Stanford Encyclopedia of Philosophy. Meta-\nphysics Research Lab, Stanford University, Summer 2020\nedition, 2020.\n[Harvey, 2009] Gordon Harvey. A brief guide to the ele-\nments of the academic essay. Harvard College Writing\nProgram, 2009.\n[Hendrycks and Gimpel, 2016] Dan Hendrycks and Kevin\nGimpel. Gaussian error linear units (gelus). arXiv preprint\narXiv:1606.08415, 2016.\n[Holyoak and Thagard, 1996] Keith J Holyoak and Paul\nThagard. Mental leaps: Analogy in creative thought. MIT\npress, 1996.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5195\n[Jin et al., 2022] Zhijing Jin, Abhinav Lalwani, Tejas Vaid-\nhya, Xiaoyu Shen, Yiwen Ding, Zhiheng Lyu, Mrinmaya\nSachan, Rada Mihalcea, and Bernhard Schoelkopf. Log-\nical fallacy detection. In Findings of the Association for\nComputational Linguistics: EMNLP 2022, pages 7180–\n7198, Abu Dhabi, United Arab Emirates, December 2022.\nAssociation for Computational Linguistics.\n[Lawrence and Reed, 2020] John Lawrence and Chris Reed.\nArgument Mining: A Survey. Computational Linguistics,\n45(4):765–818, 01 2020.\n[Leake, 2001] D.B. Leake. Problem solving and reasoning:\nCase-based. In Neil J. Smelser and Paul B. Baltes, edi-\ntors, International Encyclopedia of the Social & Behav-\nioral Sciences, pages 12117–12120. Pergamon, Oxford,\n2001.\n[Lecun et al., 1998] Y . Lecun, L. Bottou, Y . Bengio, and\nP. Haffner. Gradient-based learning applied to document\nrecognition. Proceedings of the IEEE, 86(11):2278–2324,\n1998.\n[Lewis et al., 2020] Patrick Lewis, Ethan Perez, Aleksan-\ndra Piktus, Fabio Petroni, Vladimir Karpukhin, Na-\nman Goyal, Heinrich K ¨uttler, Mike Lewis, Wen-tau Yih,\nTim Rockt ¨aschel, Sebastian Riedel, and Douwe Kiela.\nRetrieval-augmented generation for knowledge-intensive\nnlp tasks. In Proceedings of the 34th International Confer-\nence on Neural Information Processing Systems, NIPS’20,\nRed Hook, NY , USA, 2020. Curran Associates Inc.\n[Liu et al., 2019] Yinhan Liu, Myle Ott, Naman Goyal,\nJingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta:\nA robustly optimized bert pretraining approach. arXiv\npreprint arXiv:1907.11692, 2019.\n[Locke, 1997] John Locke. An Essay Concerning Human\nUnderstanding. Penguin classics. Penguin Classics, Lon-\ndon, England, June 1997.\n[Nakpih and Santini, 2020] Callistus Ireneous Nakpih and\nSimone Santini. Automated discovery of logical fallacies\nin legal argumentation. International Journal of Artificial\nIntelligence & Applications, 11(2):37–48, mar 2020.\n[OpenAI, 2022] OpenAI. Chatgpt. https://openai.com/blog/\nchatgpt, 2022. Accessed: April 30, 2023.\n[Oyelade and Ezugwu, 2020] Olaide N. Oyelade and Absa-\nlom E. Ezugwu. A case-based reasoning framework for\nearly detection and diagnosis of novel coronavirus. Infor-\nmatics in Medicine Unlocked, 20:100395, 2020.\n[Qin and Regli, 2003] Xiaoli Qin and William C. Regli. A\nstudy in applying case-based reasoning to engineering\ndesign: Mechanical bearing design. Artificial Intelli-\ngence for Engineering Design, Analysis and Manufactur-\ning, 17(3):235–252, 2003.\n[Renkl, 2014] Alexander Renkl. Toward an instructionally\noriented theory of example-based learning. Cognitive Sci-\nence, 38(1):1–37, 2014.\n[Schank, 1983] Roger C. Schank. Dynamic Memory: A The-\nory of Reminding and Learning in Computers and People.\nCambridge University Press, USA, 1983.\n[Shwartz et al., 2020] Vered Shwartz, Peter West, Ronan Le\nBras, Chandra Bhagavatula, and Yejin Choi. Unsupervised\ncommonsense question answering with self-talk. arXiv\npreprint arXiv:2004.05483, 2020.\n[Spensberger et al., 2022] Florian Spensberger, Ingo Kollar,\nand Sabine Pankofer. Effects of worked examples and\nexternal scripts on fallacy recognition skills: a random-\nized controlled trial. Journal of Social Work Education,\n58(4):622–639, 2022.\n[Tracy, 2013] Karen Tracy. Understanding face-to-face in-\nteraction: Issues linking goals and discourse. Routledge,\n2013.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki\nParmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁ ukasz Kaiser, and Illia Polosukhin. Attention is all you\nneed. In Advances in Neural Information Processing Sys-\ntems, volume 30. Curran Associates, Inc., 2017.\n[Vijayaraghavan and V osoughi, 2022] Prashanth Vija-\nyaraghavan and Soroush V osoughi. TWEETSPIN:\nFine-grained Propaganda Detection in Social Media\nUsing Multi-View Representations. In Proceedings of\nthe 2022 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human\nLanguage Technologies, pages 3433–3448, Seattle,\nUnited States, July 2022. Association for Computational\nLinguistics.\n[Walia et al., 2019] Himdweep Walia, Ajay Rana, and Vi-\nneet Kansal. Case based interpretation model for word\nsense disambiguation in gurmukhi. In 2019 9th Interna-\ntional Conference on Cloud Computing, Data Science &\nEngineering (Confluence), pages 359–364, 2019.\n[Wang et al., 2022] PeiFeng Wang, Aaron Chan, Filip\nIlievski, Muhao Chen, and Xiang Ren. PINTO: Faithful\nlanguage reasoning using prompt-generated rationales. In\nWorkshop on Trustworthy and Socially Responsible Ma-\nchine Learning, NeurIPS 2022, 2022.\n[Wei et al., 2022] Jason Wei, Xuezhi Wang, Dale Schuur-\nmans, Maarten Bosma, Ed Chi, Quoc Le, and Denny Zhou.\nChain of thought prompting elicits reasoning in large lan-\nguage models. arXiv preprint arXiv:2201.11903, 2022.\n[West et al., 2021] Peter West, Chandrasekhar Bhagavatula,\nJack Hessel, Jena D. Hwang, Liwei Jiang, Ronan Le Bras,\nXiming Lu, Sean Welleck, and Yejin Choi. Symbolic\nknowledge distillation: from general language models to\ncommonsense models. In North American Chapter of the\nAssociation for Computational Linguistics, 2021.\n[Weston et al., 2014] Jason Weston, Sumit Chopra, and An-\ntoine Bordes. Memory networks. arXiv preprint\narXiv:1410.3916, 2014.\n[Wu et al., 2019] Liang Wu, Fred Morstatter, Kathleen M.\nCarley, and Huan Liu. Misinformation in social media:\nDefinition, manipulation, and detection. SIGKDD Explor.\nNewsl., 21(2):80–90, nov 2019.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\n5196",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7750115394592285
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5680859088897705
    },
    {
      "name": "Natural language processing",
      "score": 0.5677498579025269
    },
    {
      "name": "Generalizability theory",
      "score": 0.5613998770713806
    },
    {
      "name": "Language model",
      "score": 0.5537435412406921
    },
    {
      "name": "Heuristics",
      "score": 0.523815929889679
    },
    {
      "name": "Natural language",
      "score": 0.454006165266037
    },
    {
      "name": "Domain adaptation",
      "score": 0.4392232298851013
    },
    {
      "name": "Representation (politics)",
      "score": 0.4368104338645935
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.432531476020813
    },
    {
      "name": "Case-based reasoning",
      "score": 0.4247547388076782
    },
    {
      "name": "Fallacy",
      "score": 0.4162415564060211
    },
    {
      "name": "Linguistics",
      "score": 0.19800010323524475
    },
    {
      "name": "Classifier (UML)",
      "score": 0.14337033033370972
    },
    {
      "name": "Psychology",
      "score": 0.09653103351593018
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Developmental psychology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ]
}