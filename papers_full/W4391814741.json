{
    "title": "Explainable hybrid vision transformers and convolutional network for multimodal glioma segmentation in brain MRI",
    "url": "https://openalex.org/W4391814741",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2515116846",
            "name": "Ramy A. Zeineldin",
            "affiliations": [
                "Menoufia University",
                "Friedrich-Alexander-Universität Erlangen-Nürnberg",
                "Reutlingen University"
            ]
        },
        {
            "id": "https://openalex.org/A4286554041",
            "name": "Mohamed E. Karar",
            "affiliations": [
                "Menoufia University"
            ]
        },
        {
            "id": "https://openalex.org/A3130304927",
            "name": "Ziad Elshaer",
            "affiliations": [
                "Universität Ulm"
            ]
        },
        {
            "id": "https://openalex.org/A2028170065",
            "name": "Jan Coburger",
            "affiliations": [
                "Universität Ulm"
            ]
        },
        {
            "id": "https://openalex.org/A2543132899",
            "name": "Christian R. Wirtz",
            "affiliations": [
                "Universität Ulm"
            ]
        },
        {
            "id": "https://openalex.org/A2706103262",
            "name": "Oliver Burgert",
            "affiliations": [
                "Reutlingen University"
            ]
        },
        {
            "id": "https://openalex.org/A4213566421",
            "name": "Franziska Mathis-Ullrich",
            "affiliations": [
                "Friedrich-Alexander-Universität Erlangen-Nürnberg"
            ]
        },
        {
            "id": "https://openalex.org/A2515116846",
            "name": "Ramy A. Zeineldin",
            "affiliations": [
                "Friedrich-Alexander-Universität Erlangen-Nürnberg",
                "Menoufia University",
                "Reutlingen University"
            ]
        },
        {
            "id": "https://openalex.org/A4286554041",
            "name": "Mohamed E. Karar",
            "affiliations": [
                "Menoufia University"
            ]
        },
        {
            "id": "https://openalex.org/A3130304927",
            "name": "Ziad Elshaer",
            "affiliations": [
                "Universität Ulm"
            ]
        },
        {
            "id": "https://openalex.org/A2028170065",
            "name": "Jan Coburger",
            "affiliations": [
                "Universität Ulm"
            ]
        },
        {
            "id": "https://openalex.org/A2543132899",
            "name": "Christian R. Wirtz",
            "affiliations": [
                "Universität Ulm"
            ]
        },
        {
            "id": "https://openalex.org/A2706103262",
            "name": "Oliver Burgert",
            "affiliations": [
                "Reutlingen University"
            ]
        },
        {
            "id": "https://openalex.org/A4213566421",
            "name": "Franziska Mathis-Ullrich",
            "affiliations": [
                "Friedrich-Alexander-Universität Erlangen-Nürnberg"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3110787789",
        "https://openalex.org/W3168322325",
        "https://openalex.org/W2802630259",
        "https://openalex.org/W2577801124",
        "https://openalex.org/W1641498739",
        "https://openalex.org/W2751069891",
        "https://openalex.org/W3014974815",
        "https://openalex.org/W3112701542",
        "https://openalex.org/W3018201087",
        "https://openalex.org/W3105078060",
        "https://openalex.org/W3111025719",
        "https://openalex.org/W3206476810",
        "https://openalex.org/W4220700457",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W2464708700",
        "https://openalex.org/W2996290406",
        "https://openalex.org/W6819060087",
        "https://openalex.org/W3090974769",
        "https://openalex.org/W3092444386",
        "https://openalex.org/W3148874463",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W6600882715",
        "https://openalex.org/W3182546273",
        "https://openalex.org/W3112557529",
        "https://openalex.org/W3189411510",
        "https://openalex.org/W2996061341",
        "https://openalex.org/W4280513507",
        "https://openalex.org/W3197956856",
        "https://openalex.org/W4205895239",
        "https://openalex.org/W2776220900",
        "https://openalex.org/W6629195898",
        "https://openalex.org/W3155555028",
        "https://openalex.org/W6834628200",
        "https://openalex.org/W3004572460",
        "https://openalex.org/W2785474956",
        "https://openalex.org/W4295938041",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W3026750579",
        "https://openalex.org/W2735601643",
        "https://openalex.org/W6819490522",
        "https://openalex.org/W6606960104",
        "https://openalex.org/W6638553918",
        "https://openalex.org/W114517082",
        "https://openalex.org/W2148347694",
        "https://openalex.org/W2734349601",
        "https://openalex.org/W4224318518",
        "https://openalex.org/W4313242282",
        "https://openalex.org/W3216843680",
        "https://openalex.org/W4225910659",
        "https://openalex.org/W4309474892"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports\nExplainable hybrid vision \ntransformers and convolutional \nnetwork for multimodal glioma \nsegmentation in brain MRI\nRamy A. Zeineldin 1,2,3*, Mohamed E. Karar 3, Ziad Elshaer 4, Jan Coburger 4, \nChristian R. Wirtz 4, Oliver Burgert 2 & Franziska Mathis‑Ullrich 1\nAccurate localization of gliomas, the most common malignant primary brain cancer, and its different \nsub‑region from multimodal magnetic resonance imaging (MRI) volumes are highly important \nfor interventional procedures. Recently, deep learning models have been applied widely to assist \nautomatic lesion segmentation tasks for neurosurgical interventions. However, these models are \noften complex and represented as “black box” models which limit their applicability in clinical practice. \nThis article introduces new hybrid vision Transformers and convolutional neural networks for accurate \nand robust glioma segmentation in Brain MRI scans. Our proposed method, TransXAI, provides \nsurgeon‑understandable heatmaps to make the neural networks transparent. TransXAI employs a \npost‑hoc explanation technique that provides visual interpretation after the brain tumor localization \nis made without any network architecture modifications or accuracy tradeoffs. Our experimental \nfindings showed that TransXAI achieves competitive performance in extracting both local and global \ncontexts in addition to generating explainable saliency maps to help understand the prediction of \nthe deep network. Further, visualization maps are obtained to realize the flow of information in the \ninternal layers of the encoder‑decoder network and understand the contribution of MRI modalities \nin the final prediction. The explainability process could provide medical professionals with additional \ninformation about the tumor segmentation results and therefore aid in understanding how the \ndeep learning model is capable of processing MRI data successfully. Thus, it enables the physicians’ \ntrust in such deep learning systems towards applying them clinically. To facilitate TransXAI model \ndevelopment and results reproducibility, we will share the source code and the pre‑trained models \nafter acceptance at https:// github. com/ razei neldin/ Trans XAI.\nIntra-axial brain tumors are among the ten most common malignancies leading to  death1. Although there \nare no screening or preventive examinations, effective diagnosis, and therapy influence the further course of \ngliomas. Neurosurgical intervention is the first and sometimes the only therapy for many types of  gliomas2. In \nparticular, the precise localization of pathological structures (lesions) within the brain anatomy is a major issue \nin neurosurgery. This challenge is related to the difficulty in visually delineating these pathological targets from \nthe healthy brain parenchyma.\nMagnetic resonance imaging (MRI) is the preferred modality for the evaluation of intra-axial, identification \nof normal brain structures, peritumoral edema, and detection of tumor-infiltrated  regions3. In particular, mul-\ntimodal MRI of the brain, including native T1-weighted (T1), post-contrast (T1Gd (Gadolinium)), T2-weighted \n(T2), and T2-weighted fluid-attenuated inversion recovery (FLAIR) sequences, is the gold standard to detect \nbrain gliomas including their sub-regions 4. The presence of peripheral contrast enhancement, central necrotic \nareas, intra-tumoral hemorrhages, ill-defined infiltration, and extensive perifocal edema is commonly seen in \naggressive lesions which raises the possibility of high-grade glioma (HGG) or glioblastoma (GBM) (WHO grade \nIV). However, non-enhancing tumor regions raise the possibility of low-grade gliomas (LGG). The Multimodal \nOPEN\n1Department Artificial Intelligence in Biomedical Engineering (AIBE), Friedrich-Alexander-University \nErlangen-Nürnberg (FAU), 91052 Erlangen, Germany. 2Research Group Computer Assisted Medicine (CaMed), \nReutlingen University, 72762 Reutlingen, Germany. 3Faculty of Electronic Engineering (FEE), Menoufia University, \nMinuf 32952, Egypt. 4Department of Neurosurgery, University of Ulm, 89312 Günzburg, Germany.  *email: \nramy.zeineldin@fau.de\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\nBrain Tumor Segmentation Challenge (BraTS) 2019  dataset5–7 provides a multi-institutional annotated MRI \ndataset aiming at performance evaluation of state-of-the-art (SOTA) automated deterministic solutions for the \nsegmentation of intra-axial brain lesions.\nRecent developments in deep learning, specifically convolutional neural networks (CNN) have achieved \nexcellent performance for processing and analyzing medical images, including those associated with brain tumor \n segmentation8,9, image  registration 10,11, and image  classification 12. In particular, the convolutional encoder-\ndecoder architectures, U-Net13,14, have revolutionized the medical field with outstanding feature representation \ncapabilities. In a typical U-shaped architecture, the encoder is a series of convolutional layers each followed by \ndown-sampling layers for feature representation learning with local receptive fields. The decoder aims at up-\nsampling the extracted deep feature maps to the same size as the original input. By using skip connections, the \nU-Net fuses the feature representations at different resolutions of the encoder with the corresponding layers at \nthe decoder to recover spatial information that is lost during down-sampling.\nFollowing this U-Net architecture, Zhou et al. proposed UNet++15 by redesigning skip connections to aggre-\ngate multiple-scale feature maps of an ensemble of U-Nets co-learning using deep supervision. Res-UNet16 was \nproposed to address small thin structures using a weighted attention mechanism and ResNet-based17 skip con-\nnection scheme. Similarly, KiU-Net18 employs an overcomplete convolutional architecture to effectively identify \nsmaller structures and achieve precise segmentation of boundary regions by restricting the expansion of the \nreceptive field size. It is worth mentioning that all these methods are based on CNNs, i. e. rely on the convolu -\ntional operation to capture local features by gathering information from neighborhood pixels. So, they lack the \nability to capture long-range dependency explicitly although there are some recent works trying to model global \ncontext for CNN such  as19,20 without providing satisfying results in modeling long dependencies.\nLately, Transformer has achieved tremendous success in the natural language processing (NLP)  field21. The \nself-attention mechanism in Transformer allows to model correlations among all the input tokens and hence is \nsuperior to CNN in handling long-range dependencies. Vision Transformer (ViT) 22 is a good example, which \nachieved SOTA on ImageNet classification. By reshaping input images into 2D patches with positional embed-\ndings, ViT achieved comparable performance with the CNN-based methods. Some approaches utilized Trans -\nformers as feature extractors or in the middle bottleneck  layers23,24.  TransUNet23 is the first attempt to combine \nTransformer with U-Net to establish self-attention for medical image segmentation. TransUNet uses a CNN \nencoder which generates feature maps to be fed into the Transformer using patch embedding in the bottleneck. \n TransAttUNet24 integrated multi-level guided attention U-Net with Transformer to enhance the performance of \nmedical image segmentation. Though achieving satisfying results, these methods heavily rely on a self-attention \nmechanism which would suffer from tremendous computation requirements at high-resolution volumes such \nas MRI images.\nIn addition, pure transformer-based architectures were proposed, such as  SwinUNet25 utilizes Swin Trans-\nformer as the building block for a U-shaped pure Transformer Encoder-Decoder architecture based on the \nshifted windows mechanism. The primary constraint for the use of pure Transformers is the need for huge train-\ning datasets (14M-300M images) which is not always available, especially in the medical field. This is because \nTransformers lack inductive biases, e. g. localized receptive fields, in contrast to the CNN models, and therefore \ndo not generalize well to test cases when trained on smaller data.\nNonetheless, most machine learning and/or deep learning techniques are under development for deployment \nin the clinical  field26,27. The primary reason behind that is the “black box” nature of the deep models which are \noften characterized by the lack of human-like explainable decisions. In addition, these models include a sub-\nstantial number (within millions) of extracted feature maps in each internal layer which are assumed to contain \nmeaningful information about the input problem and its possible solution. This makes fully understanding DL \nmethods highly problematic, even for professional experts. Thus, the application of such “black box” models in \nhighly sensitive medical applications is very  limited26,28. Recently, there is a growing interest in explainable AI \n(XAI) to address the justification of the decision-making process made by DL  models29. Though the explain-\nability provides no improvement in the accuracy of the deep learning model, XAI is important to guarantee \nsafety during clinical application and increase the trust of clinical end users, i.e., surgeons and radiologists. XAI \nprovides machine learning methods the ability to describe their “black box” nature in explainable or interpret -\nable terms to  humans26,28.\nIn previous studies, several interpretability methodologies have been introduced to explain the behavior \nof machine learning methods in medical applications, such as COVID-19  diagnosis30, retinal  imaging31, and \nskin  cancer32. Also, some research works have been conducted to generate explainable results for brain tumor \nsegmentation networks.  In33, Pereira et al. employed a joint Restricted Boltzmann Machine system (RBM) and \na Random Forest (RF) classifier to enhance the interpretability of a machine learning system. Inspired  by34, they \nprovided two levels of interpretation, i.e. local and global, allowing for an evaluation of the extracted task-specific \nfeatures and the voxel-level predictions, respectively. A key limitation of their mutual RBM-RF feature selection \nstrategy is the randomness of the input feature vector in each node which can be computationally expensive for \nmedical imaging tasks and, therefore, time-consuming.\nIn35, a method has been developed for visual explanations towards explaining the “black box” nature of CNNs. \nThis method extended Class Activation Mapping (CAM) 36 to extract explanations to interpret a segmentation \nnetwork for brain tumors in MRI. Moreover, they investigated how the input MRI modality perturbation affects \nthe prediction strategy of different brain lesion sub-regions. However, standard CAM approaches are restricted \nto a certain type of CNNs without including any multimodal input or fully connected layers CNNs.\nLi et al. developed an explainable ensemble Gaussian kernel (XEGK) to substitute for CNN in feature extrac-\ntion, in which they used a Gaussian kernel to capture characteristic features from relevant regions of the  input19. \nThey applied their method to mono-channel input and multi-channel inputs by leveraging the Gaussian mix-\nture model (GMM) and fusion of multiple GMMs, respectively. To interpret the experimental results, they \n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\nused Shapely additive explanations (SHAP)37 to reflect the features’ contribution. SHAP is a perturbation-based \napproach from the coalitional game theory which assigns a feature importance value for each class prediction. It \nis therefore inefficient in critical medical applications since the network must be run for the number of samples \nmultiplied by the number of features.\nNatekar et al. generated visual explanations of three deep neural networks (DNN) for the segmentation of \nbrain  tumors38. They applied Grad-CAM36 to explain the contribution of the internal layers of those segmenta-\ntion networks helping to understand “why” DNN achieved quantitively highly accurate tumor segmentations. \nThe experiments indicated that DNN follows a human-like hierarchical approach for localizing different parts \nof the brain tumor.\nOverall, the main focus of recent XAI research in medical image segmentation has been on integrating visual \ninterpretability without considering the clinical evaluation of the resultant visualizations. Besides, less attention \nhas been paid to the inclusion of medical knowledge into the decision approach made by AI-based models. \nMoreover, the decisions of these models must be consistent with the clinical knowledge to gain the trust of \nmedical professionals and encourage them to adopt AI-based systems.\nIn this work, TransXAI framework is proposed to leverage the power of CNN and Transformers as a hybrid \nmodel for explainable glioma segmentation. In designing our hybrid CNN-Transformer model, we carefully \nconsidered the specific challenges of glioma segmentation, opting for a fusion of architectures that harnesses the \nstrengths of both local and global feature extraction to provide a comprehensive understanding of MRI scans. \nIn particular, CNN is employed as an encoder to extract local image representations while a ViT is utilized to \nfurther the long-range dependency. The contributions of this study are divided into four-fold:\n• A hybrid CNN-Transformer architecture is proposed for the segmentation of brain tumors, which combines \nhigh-resolution local representations from CNN and the long-range dependency captured by Transformers.\n• An effective XAI diagnosis generator has been developed to extract explanations from the medical segmenta-\ntion network.\n• Evaluation of the proposed TransXAI framework on the multimodal brain tumor segmentation dataset \ndemonstrates its effectiveness, superiority, and robustness.\n• Explainability-driven evaluation by clinical experts showed that the proposed approach increases surgeons’ \ntrust in deep learning systems by providing evidence linked to the results of our TransXAI from the surgical \npoint of view.\nResults\nSegmentation results\nFigure 1 shows the visual segmentation results of the proposed TransXAI for three HGG and three LGG of the \nBraTS 2019 training set. In this Figure, the input MRI slices and the predicted segmentation maps overlaid on \nthe FLAIR MRI are presented in the Axial and Coronal views. The results demonstrate that our proposed model \nshows competitive performance, especially in detecting the brain glioma boundaries and its sub-regions. Further, \nthe statistical results reported by the BraTS evaluation  platform5 confirm this finding as Table 1 lists the average \nFigure 1.  Visual segmentation results of TransXAI on Axial and Coronal views along with the results of \npredictions for three BraTS 2019 Challenge LGG samples (left) and HGG samples (right). The tumor regions are \ncolor-coded, with the ET shown in green, the TC including both green and red regions, and the WT representing \nall the segmentation classes.\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\ndice similarity coefficient (DSC) and Hausdorff distance (95%) (HD95) for our TransXAI model on the BraTS \n2019 validation set.\nFurthermore, we have undertaken additional experiments to evaluate the robustness of our TransXAI model. \nThese experiments involved a meticulous assessment of the model’s performance across the five folds of cross-\nvalidation, which provides a comprehensive view of its consistency and resilience against variations in the train-\ning data. Table 1 showcases the 5-fold cross-validation ensemble results of the TransXAI method on the BraTS \n2019 validation dataset, along with the results of individual folds. (After the second BraTS external validation) \nThe consistent performance of TransXAI across different folds of cross-validation underscores its robustness, \nan essential attribute for clinical applications where variability in data is commonplace\nThe TransXAI method achieved a DSC of 0.745 for the enhancing tumor (ET) region, 0.782 for the tumor \ncore (TC) region, and 0.882 for the whole tumor (WT) region, with an average DSC of 0.803. Additionally, it \nachieved an HD95 of 4.31 mm for ET, 7.90 mm for TC, 6.36 mm for WT, and an average HD95 of 6.192 mm. \nThese results provide a comprehensive view of the method’s performance across different folds, emphasizing its \nconsistency and robustness in glioma sub-region segmentation.\nOur methodological choices, including the application of specific data augmentation techniques and the \nselection of a 5-fold cross-validation approach, were driven by the need to build a model that is not only accu -\nrate but also generalizable across different data distributions. Comparing these results with the SOTA methods, \nTransXAI method demonstrated competetive performance in various aspects. While it excels in certain metrics, \nsuch as TC DSC and HD95 across all three subregions, it closely aligns with other leading methods in terms of \nET and WT DSC, as indicated in Table  2. The average DSC of 0.803 and average HD95 of 6.19, while notable, \nreflect a competitive standing rather than a clear superiority.\nExternal multi‑site validation\nTo validate the generalizability of our TransXAI method, we have extended our evaluation to include external \ndatasets from the FeTS2022 Challenge, which is based on the BraTS2021 Challenge  dataset7,44,45. This data-\nset is a significant compendium of 1251 multi-modal brain MRI scans, inclusive of T1, T1ce (post-contrast \nT1-weighted), T2, and FLAIR sequences, each of a uniform size of 240 × 240 × 155 and an isotropic resolution \nof 1mm3 per voxel. Accompanying these images are multi-label tumor segmentation masks, distinguished into \nfour distinct categories: background, ET, TC, and WT. The dataset mirrors real-world diversity from different \nmulti-site institutions, thus providing a heterogeneous mix of imaging protocols and patient demographics. \nTable 1.  The fivefold cross-validation STAPLE ensemble results of the TransXAI on the BraTS 2019 \nvalidation, along with the results of single folds. Bold represents the best value within each metric column. \nThe metrics include Dice Similarity Coefficient (DSC), and Hausdorff Distance at 95% (HD95) across all sub-\nregions.\nModel\nDSC HD95\nET TC WT ET TC WT\nFold 0 0.725 0.767 0.883 6.81 9.66 6.35\nFold 1 0.730 0.770 0.869 9.11 11.37 7.68\nFold 2 0.720 0.777 0.876 6.08 8.23 5.52\nFold 3 0.746 0.758 0.868 4.93 8.76 6.73\nFold 4 0.734 0.756 0.874 5.05 8.67 6.62\nEnsemble 0.745 0.782 0.882 4.31 7.90 6.36\nTable 2.  Comparison of the segmentation results of TransXAI and SOTA on the BraTS 2019 validation set. \nBold and italic represent the best and second-best within each metric column, respectively.\nMethod\nDSC HD95\nET TC WT Avg ET TC WT Avg\nU-Net9 0.813 19.75\n3D U-Net14 0.709 0.725 0.874 0.769 5.062 8.719 9.43 7.74\nKiU-Net18 0.664 0.706 0.861 0.744 9.418 13.04 12.79 11.75\nRes U-Net16 0.667 0.706 0.853 0.742 7.270 9.57 8.55 8.46\nMS U-Net39 0.713 0.711 0.865 0.763 8.2465 12.65 9.42 10.11\nAttention U-Net40 0.7596 0.772 0.888 0.807 5.202 8.26 7.76 7.07\nmmFormer41 0.60 0.73 0.829 0.720\nV-Net42 0.739 0.766 0.887 0.797 6.131 8.71 6.26 7.03\nStarke et al.43 0.710 0.710 0.850 0.757 6.57 10.28 8.85 8.57\nTransXAI (Ours) 0.745 0.782 0.882 0.803 4.31 7.90 6.36 6.19\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\nThis feature enables the assessment of TransXAI in a simulated multi-site learning environment, which is a step \ncloser to its deployment in clinical practice.\nIn conducting our external validation, we have been meticulous in selecting data from the FeTS2022 Chal-\nlenge that minimizes overlap with our training set from BraTS 2019. Detailed attention was given to the case \ndistribution across institutions to ensure the independence of the validation datasets. Specifically, we have avoided \nusing cases from institutions that contributed to the BraTS 2019 dataset except for a controlled number from \nInstitute 1, which is sufficiently justified by the majority of new cases ensuring a valid assessment (with only \n129 overlapping cases out of 511). All other institutes (2, 18, 20, 21, 22) have no cases included in BraTS 2019, \nensuring the integrity of the validation process.\nFollowing the BraTS testing procedure, we have performed rigorous evaluations to assess the precision of our \nTransXAI model in delineating the ET, TC, and WT regions. The results of this evaluation are critical, as they \ndirectly inform the potential clinical utility of our model. The results are summarized in Table  3. By utilizing \nthe commonly utilized metrics, DSC and HD95, we were able to capture a comprehensive picture of TransXAI’s \nsegmentation performance. In particular, the results indicate high accuracy and robustness in identifying tumor \nboundaries and consistency across different tumor subregions. This external validation against FeTS2022 datasets \nunderscores the potential of TransXAI for clinical integration. The detailed analysis of our model’s performance \nhas identified key areas for future enhancement, particularly in addressing the variability across different imag-\ning protocols and patient demographics. These aspects are critical for developing more adaptable and robust AI \nsystems for real-world clinical use.\nRole of MRI in tumor detection\nTo better interpret the behavior of the CNN model, we performed a further experiment for generating visual \nexplanations of every tumor class using Grad-CAM. We experimented to infer TransXAI with a specified MRI \nmodality without involving other MRI sequences. This led to understanding the importance of each MRI input, \nnamely, T1, T1Gd, T2, and FLAIR in the process of different tumor label localization. Figure 2 outlines the visual \nrepresentation captured by the output convolutional layer of our TransXAI model with respect to the input MRI \nmodality. The results demonstrate that the detection of each tumor sub-region is related to one or more of the \ninput MRI volumes coherent with expert radiologists’ and raters’ observations in  reference6. For instance, T1Gd \nand T2 contribute most to the detection of the gross TC, including both label 1 (NC) and label 4 (ET), while \nthe edema and the WT region are predicted using FLAIR. Though, the visual explanations of T1 are the least \nimportant maps with very little contribution to the tumor sub-components segmentation and could, therefore, \nbe removed for computational performance advantage without model accuracy degradation.\nGrad‑CAM for different CNN layers\nIn this section, Grad-CAM has been applied to interpret the proposed TransXAI for tumor segmentation. Fig -\nure 3 shows saliency maps for the internal convolutional layers of the investigated CNN model. These visual \nexplanations provide details on the information flow inside individual filters of the network and how it learns \nsome meaningful concepts. In this hybrid network, the encoder typically consists of successive layers to capture \ncontextual information, Transformer blocks embedded in the bottleneck, and the expanding decoder path con-\ntains upsampling operators to enable high-resolution localization of the target tumor voxels.\nIt is important to observe that, internal layers of the deep neural network learn some implicit as well as explicit \nconcepts although the training stage included only explicit tumor labels. For example, Figure 3 (a) demonstrates \nhow the model implicitly differentiates white matter and gray matter region in encoder block 1 which the network \nhas not been trained to learn. Similarly, the network understands other implicit concepts such as initial and final \nnon-tumor boundaries in decoder block 3 and block 4, correspondingly. In addition, the CNN model learns \nexplicit brain tumor sub-regions which are labeled in the training dataset as depicted in Figure 3 (b).\nFurthermore, experimental results show that our proposed network follows a top-down approach for detect-\ning and segmenting brain glioma. First, the model starts with learning the entire brain tissue, followed by the \ninitial tumor boundaries, and finally, small objects and fine details are localized. In Figure 3 (b), some examples \nof finer segmentations are presented for the expansive path. Such filters outline the NC (label 1) in decoder block \n5, the ET (label 4) in decoder block 4, the TC region (label 1 and label 4) in decoder block 4, and the WT region \nTable 3.  External validation of TransXAI on FeTS2022 challenge datasets. DSC and HD95 are reported for \nenhancing tumor (ET), tumor core (TC), and whole tumor (WT), along with their average (Avg) values for \neach institute.\nInstitute Number of cases BraTS 2019 Overlap\nDSC HD95\nET TC WT Avg ET TC WT Avg\n1 511 129 0.710 0.891 0.739 0.780 4.68 14.95 7.88 9.17\n2 6 – 0.638 0.913 0.646 0.732 8.01 10.69 14.14 10.95\n18 382 – 0.687 0.854 0.718 0.753 6.20 16.77 9.04 10.67\n20 33 – 0.759 0.946 0.784 0.830 4.69 7.15 7.43 6.42\n21 35 – 0.804 0.931 0.800 0.845 6.46 9.12 8.68 8.09\n22 7 – 0.778 0.897 0.823 0.833 5.29 13.66 7.30 8.75\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\nFigure 2.  Impact of MRI input modality in the detection of different tumor labels. The first row shows the input \nMRI sequences and the ground truth annotations. The following rows correspond to label 1 (the necrotic tumor \ncore), label 2 (the peritumoral edema), and label 4 (the enhancing tumor). In the saliency maps, warmer regions \nrepresent a high score for the specified label detection.\nFigure 3.  Saliency maps for implicit concepts (left) and explicit concepts (right) learned by individual filters \nof the CNN model. It is interesting to note that there are no labels for implicit concepts in the training dataset. \nWarmer regions represent a high score for the specified concept in the prediction map. Note that EB and DB \ndenote the encoder and decoder block layers, individually.\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\n(all labels) in decoder block 5. Our findings are consistent with the top-down coupling approach that the human \nbrain follows for the comprehension of relevant visual features in global first then local  areas46.\nClinical feedback\nIn the medical domain, explainable and interpretable AI systems are crucial for many applications such as \nresearch, education, and clinical  treatments47. XAI systems can support medical professionals to understand \nthe prediction process followed by deep learning models, and thus, enhance human experts’ trust in the system’s \ndecisions. XAI systems can significantly enhance the capabilities of medical professionals in understanding the \nprediction process followed by deep learning models, thus fostering increased trust in the system’s decisions.\nTo quantitatively assess the clinical utility of our TransXAI model, structured interviews were conducted with \ntwo medical experts from the Department of Neurosurgery at Ulm University Hospital, each possessing extensive \nclinical experience—ten and seven years, respectively—to critically evaluate the practical utility of TransXAI. \nExperts were asked to assess the algorithm based on several criteria, including the accuracy of segmentation \nagainst known clinical cases, the clarity and clinical relevance of heatmaps, and the potential for the method to \nenhance current diagnostic and treatment planning procedures. Their evaluation involved a review of output \nfrom the TransXAI model in comparison with actual patient cases, discussions on the interpretability of the \nmodel’s decision-making process, and the alignment with their clinical experience.\nOur clinical collaborations revealed several potential benefits of our proposed hybrid CNN-Transformer \narchitecture for real-world clinical applications. One of the key advantages lies in the enhanced interpretability \nof the segmentation results through the application of the Grad-CAM technique. Medical experts found Grad-\nCAM to be a valuable tool for understanding the model’s decision-making process. This interpretability not \nonly enhances transparency in \"black box\" machine learning systems but also provides human-understandable \ninsights into the reasoning behind the model’s predictions.\nFurthermore, our method’s capability to generate spatial attention maps and implicit concept saliency maps \naligns well with the diagnostic practices of medical specialists. The logical systematic process exhibited by Tran-\nsXAI during the segmentation process mimics the cognitive approach used by clinicians to identify various \ntissue structures, such as neoplastic tissues and perifocal edema. This congruence between model behavior and \nclinical practice fosters a familiarity for medical specialists, facilitating more effective interaction with AI-assisted \nsegmentation results.\nHowever, it is essential to acknowledge certain limitations and considerations when applying our architecture \nin clinical scenarios. The performance of AI models can be influenced by factors such as dataset variability, acqui-\nsition protocols, and specific clinical contexts. While our approach achieved competitive segmentation results, \npotential inconsistencies in segmentation accuracy may arise due to the diversity of tumor characteristics and \nimaging conditions across different patient cohorts. Additionally, the reliance on specific MRI modalities for \naccurate segmentation raises the need for careful selection of imaging protocols to optimize the clinical utility \nof our method.\nIn conclusion, our hybrid CNN-Transformer architecture, in conjunction with the interpretability afforded \nby Grad-CAM and implicit concept saliency maps, holds promise for enhancing clinical decision-making and \nresearch efforts in glioma segmentation. The collaboration between AI systems and medical experts is key to \nmaximizing the benefits of such technologies while navigating their limitations. Further investigations and vali-\ndations within diverse clinical settings are essential to comprehensively assess the performance and robustness \nof our approach in real-world applications.\nDiscussion\nOur study has yielded comprehensive insights into the performance and interpretability of the proposed Tran-\nsXAI architecture for glioma sub-region segmentation in multimodal brain MRI scans. We have further clarified \nthe TransXAI model’s decision-making process, which is twofold: first, it employs CNNs for precise local feature \ndetection; then, it uses Transformers to contextualize these features globally, mirroring the holistic approach a \nsurgeon often takes when assessing MRI scans.\nThrough the segmentation results presented in Figure 1, we have demonstrated the outstanding performance \nof TransXAI in detecting brain glioma boundaries and their sub-regions. This is supported by statistical metrics \nin Table 1, which provides detailed insights into our TransXAI model’s performance metrics across the different \ncross-validation folds on the BraTS 2019 validation set. Additionally, a comparative analysis in Table 2 highlights \nthe competitive performance of TransXAI against SOTA methods, indicating its efficacy in accurately capturing \nthe intricate details of glioma sub-regions.\nThe robustness of the TransXAI model has been rigorously evaluated through five-fold cross-validation \nexperiments, demonstrating its consistent performance across different folds. The ensemble model’s results in \nTable 1 underscore the reliability of the method in glioma sub-region segmentation, with promising DSC and \nHD95 values across ET, TC, and WT regions. These results emphasize the robustness of the method and its \ncapability to generalize across varying data.\nAnother notable aspect of the proposed approach is its ability to interpret the behavior of the CNN model \nusing an XAI technique, namely the Grad-CAM. By inferring TransXAI with specific MRI modalities, we have \ndiscerned the importance of individual MRI inputs for different tumor label localizations. This analysis, outlined \nin Figure 2, provides insights into the distinct contributions of each MRI sequence, which align with the obser-\nvations of expert radiologists. Our findings emphasize the significance of utilizing FLAIR and T2 MRI scans \nfor precise estimation of perifocal edema and the importance of T1Gd for delineating high-grade intra-axial \nlesions. The implications for low-grade lesions are also highlighted, providing valuable guidance for selecting \nthe appropriate MRI sequences in clinical scenarios. This information holds significant importance for model \n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\ndevelopers, enabling them to assess the impact of selectively including or excluding specific sequences during \nthe training process and analyzing their effect on segmentation accuracy, and medical practitioners. It facilitates \na deeper understanding of the internal decision-making process of the DL model and its intricate interactions \nwith distinct imaging modalities.\nTo understand the model internal information workflow, we have investigated the interpretation of individual \nfilters through saliency maps. As depicted in Figure  3, our analysis shows that the model learns implicit and \nexplicit concepts. The presence of implicit concepts, such as differentiating white and gray matter, indicates the \nmodel’s capacity to discern meaningful features beyond its explicit training labels. This exploration of the CNN’s \nsystematic segmentation process has shown alignment with the clinical practice of identifying neoplastic tissues \nand differentiating between brain structures. The provision of activation maps from internal filters enhances \ntransparency and confidence in the model’s predictions. This is particularly valuable in generating human-\nunderstandable interpretations that can assist medical specialists in evaluating segmentations for clinical trials.\nTo ensure a robust clinical validation, the engagement with medical experts was structured around specific \nclinical use cases, with experts providing their assessment on a case-by-case basis. This included detailed discus-\nsions on the segmentation results for a range of glioma sub-types and complexities. The clinical implications of \nour work are evident in the feedback provided by medical professionals and the consensus reached on the utility \nof TransXAI in a clinical setting. Our approach to XAI aligns with the requirements of the medical domain, \nwhere interpretability and transparency are critical. Grad-CAM explanations have been well-received, as they \noffer intuitive visualizations that are easily understood from a surgical standpoint. The decision-making process \nwithin our TransXAI model is grounded in both data-driven insights and clinical interpretability. We provide an \nin-depth analysis of how the model interprets features aligning with clinical expectations and diagnostic criteria \nfor gliomas. This paves the way for improved collaboration between AI models and medical experts, fostering \ntrust and facilitating the integration of AI-based tools into clinical workflows.\nWithin the scope of our current investigation, our focus has centered on glioma segmentation utilizing 2D \naxial MRI slices. Notably, the selection of axial slices represents intrinsic clinical significance and methodological \nconsiderations. The choice of axial slices is well-grounded in both clinical relevance and practical considerations. \nAxial images are widely used in clinical practice for brain imaging due to their compatibility with anatomical \nlandmarks and consistent visualization of structures. Additionally, the spatial distribution of brain gliomas often \nfollows the axial plane. This alignment is particularly relevant for accurate tumor boundary delineation and \nunderstanding the extent of tumor involvement in adjacent regions. Furthermore, axial slices are commonly \navailable in medical datasets, including the BraTS dataset, simplifying data collection and preprocessing.\nNevertheless, a considerable interest lies in extending our methodology to accommodate 3D volumetric \ninputs. The transition to 3D data introduces a compelling dimension of exploration, with the potential to harness \ninherent spatial context and elevate the accuracy of glioma delineation. However, it is essential to acknowledge \nthat this transition comes accompanied by its constellation of challenges, including heightened computational \ndemands and the intricate integration of spatial information across multi-dimensional slices. This considera-\ntion steered our choice toward 2D slices, aligning with the pragmatic necessity to strike a symmetry between \ncomputational performance and model complexity.\nMaterials and methods\nThe overall proposed gradient-based justification hybrid CNN-Transformer architecture for explainable brain \nlesion segmentation is depicted in Figure 4. It is a two-step approach, which combines a deep network for tumor \nsegmentation, and an explainability generator. The first step is to segment the brain tumor boundaries from \nmultimodal MRI data using a combined neural network with Transformer. The second step is a justification \ngenerator that is employed to provide 2D visual feature explanations. Our decision to use 2D axial slices is rooted \nin the pragmatic need to balance computational efficiency with clinical efficacy. Axial slices are a staple in clini-\ncal practice, providing clear views of anatomical landmarks, which is crucial for glioma boundary delineation. \nThe following subsections describe the database used in our experiments as well as the detailed structure of the \ndeep model and the justification generator.\nData\nFor this study, we aim to apply our explainable TransXAI approach to segment glioma in brain MRI. In our \nexperiments, the BraTS 2019 challenge  dataset5–7 was used including 335 training and 125 validation subjects. \nFor every case, BraTS provides 3D pre-operative multimodal MRI scans including T1, T1Gd, T2, and FLAIR, \nas shown in Figure 5.\nGiven that the BraTS dataset was acquired from multiple institutes using various MRI scanners and following \ndifferent protocols, a pre-processing stage is crucial. Therefore, to perform the tumor boundaries prediction, \nBraTS organizers have followed typical data pre-processing  procedures5 including resampling to 1×1×1  mm3 \nvoxel resolution, reorientation to a common coordinate system, affine registration to the same anatomical volume, \nand skull-stripping. Subsequently, we deploy our pre-processing pipeline as follows: first, brain pixels of each \nMRI volume were extracted and non-brain voxels were assigned zero. This approach leads to a closer field of \nview (FOV) focused on the brain, using fewer image voxels and thus reducing resource consumption. Second, \nz-score data normalization has been applied to the resultant volume with the standard deviation, and the center \nwas cropped to 192 × 192 voxels.\nCNN‑transformer hybrid architecture\nA detailed pipeline of the proposed TransXAI approach is given in Fig.  4. Given an input MRI volume \nx∈ RH×W ×C where H × W is the spatial resolution and C number of channels (# of modalities), we first utilize \n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\nmodified 2D CNN, based on the widely used U-shaped encoder-decoder  architecture9,13,14, as shown in Fig. 6, \nto extract high-level feature representations capturing local spatial features. The CNN-based encoder blocks \nfirst utilize 2D 3 × 3 convolutional blocks to capture the spatial and depth information. Every CNN block has a \nbatch normalization (BN) layer between the convolution layers and ReLU  activation48,49. For downsampling, 2 × 2 \nmax-pooling is used to gradually extract spatial feature maps F∈ RK×H\n8 ×W\n8 ×C\n8 (K = 32), which is 1/8 of input \ndimensions of H and W . Then, the Transformer encoder blocks leverage to extract the long-distance dependencies \nFigure 4.  Overall proposed TransXAI pipeline for visual justification of glioma segmentation in brain MRI \nusing a hybrid CNN-Transformer architecture.\nFigure 5.  Glioma sub-regions in a sample scan from the BraTS 2019 challenge database. Image patches show \nthe different modalities of T1, T1Gd, T2, FLAIR, and annotated expert-labeled tumor segmentation. Ground \ntruth segmentation is provided for the enhancing tumor (blue) surrounding the non-enhancing necrotic tumor \ncore (green) visible in T1Gd, and (b) the peritumoral Edema (yellow) visible in the FLAIR, respectively.\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\nthrough the self-attention mechanism. The decoder is composed of 2 × 2 up convolutional layers that are applied \nto upscale the resultant encoded feature representation into the full-resolution segmentation maps of H × W . \nThis hybrid CNN-Transformer strategy allows to model local context information across spatial dimensions as \nwell as global context for volumetric segmentation.\nTransformer blocks\nFigure 6 shows a number of Transformer blocks embedded in the bottleneck of our TransXAI network. Each \nTransformer  block21 consists of two layers; a multi-head self-attention mechanism (MSA) and a multilayer \nperceptron network (MLP). A layer normalization (LN) is applied before each MSA and MLP layer in addition \nto employing residual connection around the output of each layer. Formally, the output zℓ  of a layer ℓ can be \ndefined as follows:\nwhere ℓε[1, 2, ... ,L] , and \n′\nzℓ is the encoded image representation.\nHowever, using a pure Transformer as an encoder would be impractical due to its computational complexity \nproportional to the number of input sequences. Therefore, we follow\nthe ViT  approach22 by splitting the x into fixed-size (P × P) patches image xp ∈ RP2C and then reshaping each \npatch into a token. Note that the input to the ViT blocks is the extracted image representations by the convolu-\ntional neural encoder blocks instead of raw input images.\nFeature restoration\nTo match the spatial resolution of the TransXAI decoder, we introduce a feature restoration module to decode \nthe resultant features. Specifically, the Transformer’s output sequence zℓ ∈ R\nHW\nP2 ×C is initially reshaped to \nH\nP × W\nP × C , but the direct usage of the low-resolution Transformer encoded data (compared with the original \nresolution H × W) may cause loss of low-level tumor region details. To compensate for such information loss, a \n1 × 1 convolutional layer is utilized which reduces the number of feature maps.\nUpsampling path\nTo gradually recover the abstract features and output the full-resolution segmentation map of H × W , we perform \nprogressive upsampling using 2 × 2 up convolutional operations. Inspired by U-Net13, low-level encoder details \nare fused with high-level decoder counterparts for finer semantic information with spatial details. Finally, a \nmulti-label softmax layer is used to estimate the final probability distribution for the output predictions.\n(1)′\nzℓ = MSA(LN(zℓ−1)) + zℓ−1\n(2)zℓ = MLP(LN(zℓ)) +\n′\nzℓ\nFigure 6.  The architecture of the hybrid CNN-Transformer brain segmentation network from mpMRI \nvolumes. The input is a 2D multimodal MRI of T1, T1Gd, T2, and FLAIR with a patch spatial resolution of \n192 × 192 × 4. The network has 8 convolution neural blocks (blue boxes), each consisting of two successive \nconvolutional layers 3 × 3, BN layer, and ReLU activation.\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\nExplainable CNN generator\nSince our main goal in this study is to investigate our hybrid CNN-based and Transformer model for brain \nsegmentation, we integrated an efficient post-hoc XAI technique. This means that all experiments are carried \nout after the inference of the model, i.e., at prediction time. Principally, we applied Grad-CAM to explore the \nspatial attention of the network predictions over internal input features based on our trials with neurosurgeons \nat Ulm University Hospital.\nGrad-CAM is a generalization of the local visualization approach Class Activation Mapping (CAM) 50 for \nidentifying discriminative features and addressing their shortcomings. Figure  7 shows an application of Grad-\nCAM to segmentation neural networks, which can be applied without any architectural modifications while the \nmodel’s output layer is differentiable with respect to its input feature neurons. By using the gradient information \nfrom the last convolutional layers of the CNN, Grad-CAM can highlight the regions responsible for a particular \nclass of interest.\nLet us define the Grad-CAM heatmap as Lc\nGCAM  which captures the important localization feature map k for \na certain class c  with respect to all N  pixels (indexed by x, y). Lc\nGCAM  is the linear combination of the forward \npass activation map Ak and the backpropagated gradient αc\nk with respect to the input activations followed by a \nReLU activation function.\nImplementation details\nThe 2D axial multimodal MRI images were fed into the hybrid CNN-Transformer network in randomly sam -\npled images of 192 × 192 pixels with batch sizes of 16. For the experiments, the CNN model was implemented \nin  TensorFlow51, using SGD  optimizer52 with a momentum of 0.9, a learning rate of 8e-3, trained on a single \nNvidia RTX2080Ti (11 GB) or RTX3060 (12 GB) GPU. The models were trained for 250 epochs for each fold, \ntotaling a training time of 5 days, facilitated by a multi-GPU setup. For our experiments, we utilized the Five-fold \ncross-validation approach on the BraTS dataset shuffling after each epoch. The ensemble of the predictions from \nthe five models was accomplished using the Simultaneous Truth and Performance Level Estimation (STAPLE) \n approach53, which leverages the expectation-maximization algorithm to attain comprehensive results. To allevi-\nate the class imbalance problem in the BraTS database, we use a combination of generalized dice (GD) 54 and \ncategorical entropy (CE) loss functions to train the network calculated by the following equations:\n(3)Lc\nGCAM = ReLU\n(∑\nl\nαc\nlAl\n)\n(4)αc\nl = 1\nN\n∑\nx\n∑\ny\n∂yc\n∂A lx,y\n(5)LOverall= LGD + LCE\n(6)LGD = 1 − 2 ∗ ∑ C\n1 W × ∑ N\n1 ys+ ε\n∑ C\n1 W ×\n(∑ N\n1 y+ s\n)\n+ ε\nFigure 7.  Applying grad-CAM to a sample glioma segmentation CNN model.\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\nwhere s represents the CNN softmax predictions, y is the expert-labeled annotation for every tumor label, and ∈ is \na regularization parameter. The GD loss is a multi-class version of the dice loss with an adaptive weight assigned \nW to each class. Finally, in order to address the class imbalance between tumor labels and brain-healthy tissue, a \nrange of on-the-fly spatial data augmentations have been integrated into the training process. This augmentation \npipeline encompasses geometric transformations, including Horizontal-Vertical Shift (HVS), Horizontal-Vertical \nFlip (HSF), and randomized rotation (within the range of 0 to 20 degrees), as well as adaptive zooming (up to \n20% variation). In addition, intensity augmentations, including controlled random brightness adjustments (with \na maximum deviation of 20%) and judiciously introduced Gaussian noise (with a standard deviation of 0.01), \nfurther enrich the augmentation strategy. This composition of transformations is meticulously applied to the \ninput of the multimodal MRI, enhancing the neural network’s capacity to generalize effectively and ensure robust \nperformance across diverse scenarios. For explainability experiments, we utilized the Grad-CAM implementa -\ntion from NeuroXAI  framework55.\nExperimental design and procedure\nIn encoder-decoder networks, like TransXAI, generated saliency maps for one of the last encoder layers are \nsmooth and do not capture feasible information in our segmentation problem. This is because these layers \ngenerate the smallest feature dimensions in the network and intensive upscaling is required to match the output \nprediction map. In contrast, selecting one of the last layers (e. g. the output layer) from the decoder network \nprovides a higher-resolution feature map showing detailed features of the segmentation process since these layers \nare combined with the encoder layers through concatenation. Moreover, by incorporating the output layer into \nthe explanation generation process, we solve the limitation of Grad-CAM for generating low-resolution heatmaps.\nFurthermore, our applied XAI generator is post-hoc in the sense that it provides explanations after obtaining \nthe model predictions, instead of being inserted into the network architecture itself. Therefore, all our explain -\nability experiments have been done after the training of the segmentation network. Pre-trained weights for the \nsegmentation were used for generating the heatmaps of the used XAI methods, i.e., Grad-CAM.\nSince segmentation networks pinpoint the localized region of brain tumor regions, providing visual saliency \nmaps of the output layer alone does not help in making the network transparent. Therefore, to better investigate \nthe behavior of the deep model and to determine how spatial information flows inside the internal layers, we \nconducted four main experiments to extend the explainability approach as follows:\n• Quantitative evaluation on the BraTS validation database and comparison with SOTA 2D and 3D methods.\n• Identifying the contribution of each MRI input modality in the final predicted tumor sub-components.\n• Interpreting the CNN layers using XAI to reveal how the network represents information in the internal \nfilters.\n• Clinical feedback on the proposed method from our clinical collaborators.TransUNetTiny2_wcross_loss\n• Detection of failure nodes of the TransXAI model and analysis of the reasons behind that.\nConclusion\nThis article demonstrated our successful TransXAI as a 2D generic explainability generator for interpreting the \nperformance of multimodal CNN for brain glioma segmentation using MRI scans. Our proposed TransXAI \nholds a competitive position among other SOTA methods by achieving mean dice scores of 0.88, 0.78, and 0.75 \non the WT, TC, and ET sub-regions. This balanced performance highlights its potential in clinical applications \nalongside other advanced methods. However, visual pixel-based representations are not enough to give meaning-\nful interpretable information, and therefore, we conducted extensive experiments to provide interpretability by \nevaluating their clinical significance. The obtained results supported our technical research work to realize that \ndeep neural models behave in a human-understandable manner and are consistent with the surgical experts’ \ndomain knowledge. The decision-making clarity provided by TransXAI’s explainability promotes trust among \nclinicians, ensuring that the model’s predictions are not only accurate but also understandable and aligned with \nclinical expertise.\nFor future work, the generalization architecture of our proposed TransXAI can be extended by adding new \nCNN models. Our future research will study the integration of a 3D architecture, with the aim of investigat-\ning its potential to further enhance performance and accuracy. The consideration of 3D volumetric data could \npotentially capture spatial relationships and contextual information that are inherently present in medical images, \npotentially leading to improved segmentation outcomes. Further studies should explore utilizing concept acti -\nvation maps and feeding them back to the neural network as on-demand deep supervision. That will provide \nadditional guidance to the network and thus enhance the overall accuracy of assisting the surgeons during \ninterventional procedures.\nData availability\nBRATS 2019 dataset analyzed during the current study is included in this article https:// doi. org/https:// doi. org/ \n10. 48550/ arXiv. 1811. 02629 and is available through the Image Processing Portal of the CBICA@UPenn (IPPipp.\ncbica.upenn.edu). This platform features downloading of the dataset, as well as the automatic evaluation of the \nsubmitted results.\n(7)LCE =− 1\nN\nN∑\n1\nC∑\n1\ny × log (s)\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\nCode availability\nThe source code and the pre-trained models for this study will be publicly available upon acceptance on GitHub \nat the following repository: https:// github. com/ razei neldin/ Trans XAI.\nReceived: 14 June 2023; Accepted: 9 February 2024\nReferences\n 1. Weller, M. et al. EANO guidelines on the diagnosis and treatment of diffuse gliomas of adulthood. Nat. Rev. Clin. Oncol. 18, \n170–186. https:// doi. org/ 10. 1038/ s41571- 020- 00447-z (2021).\n 2. Pala, A. et al. The impact of an ultra-early postoperative MRI on treatment of lower grade glioma. Cancers (Basel) https:// doi. org/ \n10. 3390/ cance rs131 22914 (2021).\n 3. Pope, W . B. & Brandal, G. Conventional and advanced magnetic resonance imaging in patients with high-grade glioma. Q. J. Nucl. \nMed. Mol. Imaging 62, 239–253 (2018).\n 4. Ellingson, B. M., Wen, P . Y . & Cloughesy, T. F . Modified criteria for radiographic response assessment in glioblastoma clinical \ntrials. Neurotherapeutics 14, 307–320. https:// doi. org/ 10. 1007/ s13311- 016- 0507-6 (2017).\n 5. Bakas, S. et al. Identifying the best machine learning algorithms for brain tumor segmentation, progression assessment, and overall \nsurvival prediction in the BRATS challenge. arXiv preprint arXiv: 1811. 02629 (2018).\n 6. Menze, B. H. et al. The multimodal brain tumor image segmentation benchmark (BRATS). IEEE Trans. Med. Imaging 34, 1993–\n2024. https:// doi. org/ 10. 1109/ TMI. 2014. 23776 94 (2015).\n 7. Bakas, S. et al. Advancing the cancer genome atlas glioma MRI collections with expert segmentation labels and radiomic features. \nSci. Data 4, 170117. https:// doi. org/ 10. 1038/ sdata. 2017. 117 (2017).\n 8. Isensee, F ., Jaeger, P . F ., Kohl, S. A. A., Petersen, J. & Maier-Hein, K. H. nnU-Net: a self-configuring method for deep learning-based \nbiomedical image segmentation. Nat. Methods 18, 203–211. https:// doi. org/ 10. 1038/ s41592- 020- 01008-z (2021).\n 9. Zeineldin, R. A., Karar, M. E., Coburger, J., Wirtz, C. R. & Burgert, O. DeepSeg: deep neural network framework for automatic \nbrain tumor segmentation using magnetic resonance FLAIR images. Int. J. Comput. Assist Radiol. Surg. 15, 909–920. https:// doi. \norg/ 10. 1007/ s11548- 020- 02186-z (2020).\n 10. Sedghi, A. et al. Image registration: Maximum likelihood, minimum entropy and deep learning. Med. Image Anal. 69, 101939. \nhttps:// doi. org/ 10. 1016/j. media. 2020. 101939 (2021).\n 11. Zeineldin, R. A. et al. iRegNet: Non-rigid registration of MRI to interventional US for brain-shift compensation using convolutional \nneural networks. Ieee Access 9, 147579–147590. https:// doi. org/ 10. 1109/ access. 2021. 31203 06 (2021).\n 12. Chatterjee, S., Nizamani, F . A., Nürnberger, A. & Speck, O. Classification of brain tumours in MR images using deep spatiospatial \nmodels. Sci. Rep. https:// doi. org/ 10. 1038/ s41598- 022- 05572-6 (2022).\n 13. Ronneberger, O., Fischer, P . & Brox, T. in Medical Image Computing and Computer-Assisted Intervention – MICCAI 2015 Lecture \nNotes in Computer Science Ch. Chapter 28, 234–241 (2015).\n 14. Çiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. & Ronneberger, O. in Medical Image Computing and Computer-Assisted Inter-\nvention – MICCAI 2016 Lecture Notes in Computer Science Ch. Chapter 49, 424–432 (2016).\n 15. Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N. & Liang, J. UNet++: Redesigning skip connections to exploit multiscale features in \nimage segmentation. IEEE Trans. Med. Imaging 39, 1856–1867. https:// doi. org/ 10. 1109/ TMI. 2019. 29596 09 (2020).\n 16. Xiao, X., Lian, S., Luo, Z. & Li, S. in 2018 9th International Conference on Information Technology in Medicine and Education (ITME) \n327–331 (2018).\n 17. He, K., Zhang, X., Ren, S. & Sun, J. in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 770–778.\n 18. Valanarasu, J. M. J., Sindagi, V . A., Hacihaliloglu, I. & Patel, V . M. KiU-Net: Overcomplete convolutional architectures for biomedical \nimage and volumetric segmentation. IEEE Trans. Med. Imaging 41, 965–976. https:// doi. org/ 10. 1109/ tmi. 2021. 31304 69 (2022).\n 19. Li, J. et al. Multigrained attention network for infrared and visible image fusion. IEEE Trans. Instrum. Meas. 70, 1–12. https:// doi. \norg/ 10. 1109/ tim. 2020. 30293 60 (2021).\n 20. Tomar, N. K. et al. FANet: A feedback attention network for improved biomedical image segmentation. IEEE Trans. Neural Netw. \nLearn Syst. https:// doi. org/ 10. 1109/ TNNLS. 2022. 31593 94 (2022).\n 21. Vaswani, A. et al. Attention is all you need. Advances in neural information processing systems 30 (2017).\n 22. Dosovitskiy, A. et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv: 2010. 11929 \n(2020).\n 23. Chen, J. et al. Transunet: Transformers make strong encoders for medical image segmentation. arXiv preprint arXiv: 2102. 04306 \n(2021).\n 24. Chen, B., Liu, Y ., Zhang, Z., Lu, G. & Zhang, D. Transattunet: Multi-level attention-guided u-net with transformer for medical \nimage segmentation. arXiv preprint arXiv: 2107. 05274 (2021).\n 25. Cao, H. et al. Swin-unet: Unet-like pure transformer for medical image segmentation. arXiv preprint arXiv: 2105. 05537 (2021).\n 26. Angelov, P . P ., Soares, E. A., Jiang, R., Arnold, N. I. & Atkinson, P . M. Explainable artificial intelligence: An analytical review. Wires \nData Min. Knowl. https:// doi. org/ 10. 1002/ widm. 1424 (2021).\n 27. Xie, X. et al. A survey on incorporating domain knowledge into deep learning for medical image analysis. Med. Image Anal. 69, \n101985. https:// doi. org/ 10. 1016/j. media. 2021. 101985 (2021).\n 28. Y ang, G., Y e, Q. & Xia, J. Unbox the black-box for the medical explainable AI via multi-modal and multi-centre data fusion: A \nmini-review, two showcases and beyond. Inf. Fusion 77, 29–52. https:// doi. org/ 10. 1016/j. inffus. 2021. 07. 016 (2022).\n 29. Du, M., Liu, N. & Hu, X. Techniques for interpretable machine learning. Commun. ACM 63, 68–77. https:// doi. org/ 10. 1145/ 33597 \n86 (2019).\n 30. Nguyen, D. Q. et al. BeCaked: An explainable artificial intelligence model for COVID-19 forecasting. Sci. Rep. https:// doi. org/ 10. \n1038/ s41598- 022- 11693-9 (2022).\n 31. Niu, Y ., Gu, L., Zhao, Y . & Lu, F . Explainable diabetic retinopathy detection and retinal image generation. IEEE J. Biomed. Health \nInform. 26, 44–55. https:// doi. org/ 10. 1109/ JBHI. 2021. 31105 93 (2022).\n 32. Mazoure, B., Mazoure, A., Bédard, J. & Makarenkov, V . DUNEScan: A web server for uncertainty estimation in skin cancer detec-\ntion with deep neural networks. Sci. Rep. https:// doi. org/ 10. 1038/ s41598- 021- 03889-2 (2022).\n 33. Pereira, S. et al. Enhancing interpretability of automatically extracted machine learning features: application to a RBM-random \nforest system on brain lesion segmentation. Med. Image Anal. 44, 228–244. https:// doi. org/ 10. 1016/j. media. 2017. 12. 009 (2018).\n 34. Ribeiro, M. T., Singh, S. & Guestrin, C. in Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery \nand data mining. 1135–1144.\n 35. Saleem, H., Shahid, A. R. & Raza, B. Visual interpretability in 3D brain tumor segmentation network. Comput. Biol. Med. 133, \n104410. https:// doi. org/ 10. 1016/j. compb iomed. 2021. 104410 (2021).\n 36. Selvaraju, R. R. et al. in Proceedings of the IEEE international conference on computer vision. 618–626.\n 37. Lundberg, S. M. & Lee, S.-I. in Proceedings of the 31st international conference on neural information processing systems. 4768–4777.\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:3713  | https://doi.org/10.1038/s41598-024-54186-7\nwww.nature.com/scientificreports/\n 38. Natekar, P ., Kori, A. & Krishnamurthi, G. Demystifying brain tumor segmentation networks: Interpretability and uncertainty \nanalysis. Front. Comput. Neurosci. 14, 6. https:// doi. org/ 10. 3389/ fncom. 2020. 00006 (2020).\n 39. Jesson, A. & Arbel, T. in Brainlesion: Glioma, multiple sclerosis, stroke and traumatic brain injuries lecture notes in computer science \nCh. Chapter 34, 392–402 (2018).\n 40. Oktay, O. et al. Attention U-Net: Learning where to look for the pancreas. arXiv: 1804. 03999 (2018). https:// ui. adsabs. harva rd. edu/ \nabs/ 2018a rXiv1 80403 999O.\n 41. Zhang, Y . et al. in Medical Image Computing and Computer Assisted Intervention – MICCAI 2022 Lecture Notes in Computer Science \nCh. Chapter 11, 107–117 (2022).\n 42. Milletari, F ., Navab, N. & Ahmadi, S.-A. in 2016 Fourth International Conference on 3D Vision (3DV) 565–571 (2016).\n 43. Starke, S. et al. in Brainlesion: Glioma, Multiple Sclerosis, Stroke and Traumatic Brain Injuries Lecture Notes in Computer Science \nCh. Chapter 35, 368–378 (2020).\n 44. Pati, S. et al. The Federated Tumor Segmentation (FeTS) Challenge. arXiv: 2105. 05874 (2021). https:// ui. adsabs. harva rd. edu/ abs/ \n2021a rXiv2 10505 874P.\n 45. Reina, G. A. et al. OpenFL: An open-source framework for Federated Learning. arXiv: 2105. 06413 (2021). https:// ui. adsabs. harva \nrd. edu/ abs/ 2021a rXiv2 10506 413R.\n 46. Dijkstra, N., Zeidman, P ., Ondobaka, S., van Gerven, M. A. J. & Friston, K. Distinct top-down and bottom-up brain connectivity \nduring visual perception and imagery. Sci. Rep. 7, 5677. https:// doi. org/ 10. 1038/ s41598- 017- 05888-8 (2017).\n 47. Holzinger, A., Biemann, C., Pattichis, C. S. & Kell, D. B. What do we need to build explainable AI systems for the medical domain? \narXiv preprint arXiv: 1712. 09923 (2017).\n 48. Srivastava, N., Hinton, G., Krizhevsky, A. & Salakhutdinov, R. in Journal of Machine Learning Research. 1929–1958.\n 49. Ioffe, S. & Szegedy, C. in 32nd International Conference on Machine Learning, ICML 2015 Vol. 1 448–456 (International Machine \nLearning Society (IMLS), 2015).\n 50. Zhou, B., Khosla, A., Lapedriza, A., Oliva, A. & Torralba, A. in Proceedings of the IEEE conference on computer vision and pattern \nrecognition. 2921–2929.\n 51. Abadi, M. et al. Tensorflow: Large-scale machine learning on heterogeneous distributed systems. arXiv preprint arXiv: 1603. 04467 \n(2016).\n 52. Bottou, L. in Proceedings of COMPSTAT’2010 Ch. Chapter 16, 177–186 (2010).\n 53. Warfield, S. K., Zou, K. H. & Wells, W . M. Simultaneous truth and performance level estimation (STAPLE): An algorithm for the \nvalidation of image segmentation. IEEE Trans. Med. Imaging 23, 903–921. https:// doi. org/ 10. 1109/ tmi. 2004. 828354 (2004).\n 54. Sudre, C. H., Li, W ., Vercauteren, T., Ourselin, S. & Jorge Cardoso, M. in Deep Learning in Medical Image Analysis and Multimodal \nLearning for Clinical Decision Support Lecture Notes in Computer Science Ch. Chapter 28, 240–248 (2017).\n 55. Zeineldin, R. A. et al. Explainability of deep neural networks for MRI analysis of brain tumors. Int. J. Comput. Assist Radiol. Surg. \n17, 1673–1683. https:// doi. org/ 10. 1007/ s11548- 022- 02619-x (2022).\nAcknowledgements\nWe would like to acknowledge the support of the German Academic Exchange Service (DAAD) for providing \nfunding to the first author during this study [scholarship number 91705803, 2018]. Their sponsorship signifi-\ncantly contributed to the successful completion of this research.\nAuthor contributions\nR.Z. conceived and conducted the experiments, analyzed the data, and drafted the manuscript; M.K. contributed \nto the study concept, the experimental design, funding acquisition, and revised the manuscript; Z. E. contributed \nto the medical data interpretation, investigation, and analysis; O. B. and F . M. supervised the project, provided \nproject resources, revised the manuscript; J. C. and C. W validated the data and the results; C. W provided project \nresources. All authors reviewed the manuscript.\nFunding\nOpen Access funding enabled and organized by Projekt DEAL.\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to R.A.Z.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}