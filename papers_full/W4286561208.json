{
    "title": "The interactive reading task: Transformer-based automatic item generation",
    "url": "https://openalex.org/W4286561208",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A1520572923",
            "name": "Yigal Attali",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2510101454",
            "name": "Andrew Runge",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2599899126",
            "name": "Geoffrey T. LaFlair",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286568438",
            "name": "Kevin Yancey",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102923473",
            "name": "Sarah Goodwin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2137073923",
            "name": "Yena Park",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A735471059",
            "name": "Alina A. von Davier",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1520572923",
            "name": "Yigal Attali",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2510101454",
            "name": "Andrew Runge",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2599899126",
            "name": "Geoffrey T. LaFlair",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4286568438",
            "name": "Kevin Yancey",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2102923473",
            "name": "Sarah Goodwin",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2137073923",
            "name": "Yena Park",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A735471059",
            "name": "Alina A. von Davier",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2006932483",
        "https://openalex.org/W1828457054",
        "https://openalex.org/W6752362974",
        "https://openalex.org/W2045892050",
        "https://openalex.org/W2088987187",
        "https://openalex.org/W4246329300",
        "https://openalex.org/W6786736537",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W4384158879",
        "https://openalex.org/W6760506910",
        "https://openalex.org/W2532524286",
        "https://openalex.org/W2044688197",
        "https://openalex.org/W6755207826",
        "https://openalex.org/W146206334",
        "https://openalex.org/W3188295510",
        "https://openalex.org/W2990045827",
        "https://openalex.org/W3210957772",
        "https://openalex.org/W2605073321",
        "https://openalex.org/W2084314409",
        "https://openalex.org/W2964304126",
        "https://openalex.org/W2970539732",
        "https://openalex.org/W2989613245",
        "https://openalex.org/W43048615",
        "https://openalex.org/W2229270207",
        "https://openalex.org/W4206268106",
        "https://openalex.org/W2102090982",
        "https://openalex.org/W2075201173",
        "https://openalex.org/W2107643868",
        "https://openalex.org/W3019082520",
        "https://openalex.org/W2079145130",
        "https://openalex.org/W1593930920",
        "https://openalex.org/W2135995390",
        "https://openalex.org/W6724651221",
        "https://openalex.org/W2980708516",
        "https://openalex.org/W4210738361",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2593853883",
        "https://openalex.org/W4205245217",
        "https://openalex.org/W2791246286",
        "https://openalex.org/W1993225241",
        "https://openalex.org/W2514166524",
        "https://openalex.org/W6948213869",
        "https://openalex.org/W2524369579",
        "https://openalex.org/W2024475444",
        "https://openalex.org/W6734015803",
        "https://openalex.org/W4248674065",
        "https://openalex.org/W4226163610",
        "https://openalex.org/W1514875575",
        "https://openalex.org/W282787389",
        "https://openalex.org/W2301128046"
    ],
    "abstract": "Automatic item generation (AIG) has the potential to greatly expand the number of items for educational assessments, while simultaneously allowing for a more construct-driven approach to item development. However, the traditional item modeling approach in AIG is limited in scope to content areas that are relatively easy to model (such as math problems), and depends on highly skilled content experts to create each model. In this paper we describe the interactive reading task, a transformer-based deep language modeling approach for creating reading comprehension assessments. This approach allows a fully automated process for the creation of source passages together with a wide range of comprehension questions about the passages. The format of the questions allows automatic scoring of responses with high fidelity (e.g., selected response questions). We present the results of a large-scale pilot of the interactive reading task, with hundreds of passages and thousands of questions. These passages were administered as part of the practice test of the Duolingo English Test. Human review of the materials and psychometric analyses of test taker results demonstrate the feasibility of this approach for automatic creation of complex educational assessments.",
    "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/two.tnum July /two.tnum/zero.tnum/two.tnum/two.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nOPEN ACCESS\nEDITED BY\nMichael Flor,\nEducational Testing Service,\nUnited States\nREVIEWED BY\nHollis Lai,\nUniversity of Alberta, Canada\nSusan Embretson,\nGeorgia Institute of Technology,\nUnited States\n*CORRESPONDENCE\nYigal Attali\nyigal@duolingo.com\nSPECIALTY SECTION\nThis article was submitted to\nNatural Language Processing,\na section of the journal\nFrontiers in Artiﬁcial Intelligence\nRECEIVED /two.tnum/three.tnum March /two.tnum/zero.tnum/two.tnum/two.tnum\nACCEPTED /two.tnum/nine.tnum June /two.tnum/zero.tnum/two.tnum/two.tnum\nPUBLISHED /two.tnum/two.tnum July /two.tnum/zero.tnum/two.tnum/two.tnum\nCITATION\nAttali Y, Runge A, LaFlair GT, Yancey K,\nGoodwin S, Park Y and von Davier AA\n(/two.tnum/zero.tnum/two.tnum/two.tnum) The interactive reading task:\nTransformer-based automatic item\ngeneration.\nFront. Artif. Intell./five.tnum:/nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/two.tnum Attali, Runge, LaFlair, Yancey,\nGoodwin, Park and von Davier. This is\nan open-access article distributed\nunder the terms of the\nCreative\nCommons Attribution License (CC BY) .\nThe use, distribution or reproduction\nin other forums is permitted, provided\nthe original author(s) and the copyright\nowner(s) are credited and that the\noriginal publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution\nor reproduction is permitted which\ndoes not comply with these terms.\nThe interactive reading task:\nTransformer-based automatic\nitem generation\nYigal Attali*, Andrew Runge, Geoﬀrey T. LaFlair, Kevin Yancey,\nSarah Goodwin, Yena Park and Alina A. von Davier\nDuolingo, Pittsburgh, PA, United States\nAutomatic item generation (AIG) has the potential to greatly e xpand the\nnumber of items for educational assessments, while simultaneo usly allowing\nfor a more construct-driven approach to item development. Howev er, the\ntraditional item modeling approach in AIG is limited in scope to content areas\nthat are relatively easy to model (such as math problems), and d epends on\nhighly skilled content experts to create each model. In this paper we describe\nthe interactive reading task, a transformer-based deep lang uage modeling\napproach for creating reading comprehension assessments. This approach\nallows a fully automated process for the creation of source passage s together\nwith a wide range of comprehension questions about the passages. Th e format\nof the questions allows automatic scoring of responses with hig h ﬁdelity (e.g.,\nselected response questions). We present the results of a lar ge-scale pilot\nof the interactive reading task, with hundreds of passages and tho usands of\nquestions. These passages were administered as part of the practi ce test of\nthe Duolingo English Test. Human review of the materials and p sychometric\nanalyses of test taker results demonstrate the feasibility o f this approach for\nautomatic creation of complex educational assessments.\nKEYWORDS\nautomatic item generation, reading assessment, language modeling, transformer\nmodels, psychometrics\nIntroduction\nAutomatic item generation\nThe advent of internet-based computerized assessment oﬀers many advant ages\ncompared to more traditional paper-based assessments, including support for inno vative\nitem types and alternative item formats (Sireci and Zenisky, 2006) , measurement\nof more complex knowledge, skills, and competencies (Bartram and Hambleton,\n2005), automated scoring (Shermis and Burstein, 2013) which also allows immediate\nfeedback to students (Attali and Powers, 2010) , and adaptive testing and testing\non-demand (van der Linden and Glas, 2010) . These advantages have resulted in\nincreased summative as well as formative testing and consequently the challe nge\nof much higher volume of item development (Downing and Haladyna, 2006) .\nFrontiers in Artiﬁcial Intelligence /zero.tnum/one.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nThis challenge may be addressed through automatic item\ngeneration (Irvine and Kyllonen, 2002; Gierl and Haladyna,\n2013). Automatic item generation (AIG) is usually based on the\nnotion of an item “model” (Bejar, 2002) , a schema or template\nfor a question with parameters that can be instantiated with\nspeciﬁc values. For example, the model X + Y = ?, where X and Y\ncan be any whole numbers in the range 0–9, has two parameters.\nAn item model can be instantiated, using a computer-based\nalgorithm, to display an actual item (in this case, a single-digit\naddition exercise). A more complex example is “How many\npieces of [fruit] will you have if you cut [5] whole [fruits] into\n[thirds]?”\n(Attali, 2018), where the text in parentheses represent\nparameters (numeric or text).\nAIG has been used to create items in diverse content areas\nand formats (Haladyna, 2013). From a practical standpoint, the\nuse of item models expands the potential number of items. From\na theoretical standpoint, item models provide an opportunity for\na construct-driven approach to item development\n(Embretson\nand Yang, 2006) because they can be tied to a mapping of\nthe construct through an analysis of the cognitive mechanisms\nrelated to the item solution and item features that call on these\nmechanisms\n(Whitely, 1983).\nHowever, an important limitation of the item model\napproach and its conventional implementation is that it is\nlimited in scope to content areas that are relatively easy to model\n(such as math). In addition, because the process depends on\nhighly skilled content experts to create the models, the AIG\nprocess is semi-automatic and can be costly in terms of the\nrequired resources.\nAs a result, the use of AIG as a technique to populate test\nor examination content is limited to relatively simple tasks.\nAn example of a complex type of task that is not amenable\nto AIG is a reading comprehension task, which is the most\ncommon method used for assessment of higher-level verbal\nskills and abilities. Setting aside the diﬃculty of generating\nthe reading passage, many reading comprehension questions\nwould be unique for every passage, and therefore could not be\nproduced from a simple item model. Other questions may have a\nstandard stem (e.g., “What is the main idea of the passage?”), but\nthe answer is of course unique to each reading passage, thereby\nshifting the burden to automatic scoring of possible responses\n(Shermis and Burstein, 2013) .\nA related literature in machine learning and natural\nlanguage processing concerns automated question generation\n(AQG) for educational purposes. A recent review of this\nliterature\n(Kurdi et al., 2020) classiﬁed AQG studies in terms\nof method of generation as template-based, rule-based, and\nstatistical methods. Templates are a diﬀerent term for models\nand are the most common approach used in the research\nreviewed. Rule-based approaches annotate sentences with\nsyntactic and/or semantic information and then manipulate\nthe input using suitable rules to create a question. For\nexample, to create a WH question from a sentence, the\nrules would specify how to select a suitable question type\n(a suitable wh-word) and how to convert the sentence into\na question. Template-based and rule-based methods similarly\nrequire manual development of the templates or rules\n(Kurdi\net al., 2020) . In contrast, with statistical methods the rules for\nquestion generation are implicitly learned through sophisticated\nlanguage models.\nModern language modeling\nLanguage modeling seeks to develop a probabilistic model\nof language that can be subsequently used both to determine\nhow likely a given text is and to stochastically generate plausible\ncontinuations to a given text. Language modeling has been\nat the core of natural language processing (NLP) research\nfor decades\n(Shannon, 1951; Ney et al., 1994) , but the most\nrecent advances based on neural transformer architectures\n(Vaswani et al., 2017; Devlin et al., 2019; Radford et al., 2019)\nhave made signiﬁcant progress on the goal of being able to\ngenerate long, coherent, and information-rich sequences of\ntext. What distinguishes transformer architectures from earlier\nneural network architectures is their ability to allow long-\ndistance lexical relationships in text to have a much more direct\ninﬂuence on the task of predicting the next token in a sequence,\nallowing the model to more eﬀectively leverage signiﬁcantly\nlonger contexts when making its predictions.\nWhile these models often require a tremendous amount of\ntraining data and computing power to train initially, the trained\nmodels can be used to generate representations of language that\ncan be subsequently used as inputs to train models for other\nlanguage-related tasks such as text classiﬁcation and question\nanswering\n(Sun et al., 2019; Yang et al., 2019) . These task-speciﬁc\nmodels typically need signiﬁcantly less labeled training data in\norder to achieve the same or better results as systems trained on\none or two orders of magnitude more data. However, they still\nrequire updating the model with a non-trivial amount of expert\nannotated data.\nOne of the most impressive recent examples has been\nOpenAI’s GPT-3 model\n(Brown et al., 2020) . This model has\ndemonstrated the ability to learn the format and style of a\ntext based on fewer than 10 examples in order to generate\nnovel, coherent content without the need to explicitly update\nthe underlying model. Because it mirrors the format as well as\nthe style of any natural text, the generated output of the model\ncan be natural text, enumerated lists, a paragraph paired with\nattributes or comprehension questions, or even well-formatted\nHTML code. As such, models such as GPT-3 represent a\ntremendous opportunity for innovations in AIG by allowing test\ndesigners to prototype and iterate on new item types without\nthe need for signiﬁcant expert-annotated data or lengthy model\ndevelopment and training processes.\nFrontiers in Artiﬁcial Intelligence /zero.tnum/two.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nEarly implementations for AIG\nvon Davier (2018) successfully demonstrated that non-\ncognitive personality items can be generated by training a\ntype of recurrent neural network known as long short-term\nmemory (LSTM) network on a set of established personality\nstatements.\nHommel et al. (2021) used a transformer-based\nmodel to produce similar items.\nKumar et al. (2018) generated question-answer pairs from\nsentences using an LSTM. The answers are one of the named\nentities in the sentence and a model selects the most appropriate\nentity as the pivotal answer, around which a question is then\ngenerated.\nKumar et al. (2019) present an extension of this work\nwith a web-based interface for test developers to select and ﬁlter\nquestions and answers generated from the model.\nSphinx\n(Khan et al., 2020) is a hybrid system that\nautomatically generates reading texts using advanced language\nmodeling techniques, but relies on human experts to generate\nreading comprehension questions, and item models to generate\nsimple grammatical questions, such as sentence fragment\ncorrection questions (\nKhan et al., 2020 , Figure 10).\nThe interactive reading task\nThe interactive reading (IR) task is a framework for AIG\nand automatic scoring of reading comprehension (RC) passages\nand a suite of questions associated with the passage. The IR\ntask requires test takers to sequentially interact with the text\nfor several purposes that underpin the construct of reading\n(Grabe and Stoller, 2013) . This task closely represents the activity\nof reading in a university setting and taps into established\npurposes for reading from the second language reading and\nlanguage assessment research literature\n(Grabe and Jiang, 2013) .\nThe IR task complements the psycholinguistic approach of the\nDuolingo English Test on assessing reading comprehension\nby focusing more on the product and consequences of\nreading comprehension.\nIR question types\nIR questions were designed to address several\nunique challenges:\n• The questions should cover a wide range of RC component\nabilities and academic purposes for reading\n• The questions should be able to support AIG and\nautomated scoring\n• The questions should be administered sequentially on the\nsame text to use limited testing time in an eﬃcient manner.\nThe following list provides an overview of the sequential\nIR questions:\n• Vocabulary in context (cloze). Only the ﬁrst part of a\ntext is presented, with several individual dispersed words\nblanked out throughout the text, and the test-taker is asked\nto complete the missing words.\n• Text completion. The ﬁrst part of the text is presented in\nfull, together with the second part of the text. However, a\nsentence is missing between the two parts and the test-taker\nis asked to complete the missing sentence.\n• Comprehension questions. The full text (two parts and\nmissing sentence) is presented in full and the test-taker is\nasked to answer comprehension questions about the text.\n• Main idea. The test-taker is asked to identify an idea\npresented in the text.\n• Possible title. The test-taker is asked to identify the best\npossible title for the text.\nFor examples of IR passages with associated questions, see\nthe Duolingo English Test guide for test takers\n(Duolingo, 2022)\nand the Duolingo English Test technical manual (Cardwell et al.,\n2022).\nConstruct coverage\nThe proposed questions cover major component abilities for\nreading comprehension (Grabe and Jiang, 2013) .\n• Vocabulary, morphological, and syntactic knowledge are\naddressed by the cloze question\n• Text-structure awareness and discourse organization are\naddressed by the text completion and main-idea questions\n• Main-idea comprehension is addressed by the main\nidea question\n• Inferences about text information and summarization\nabilities are addressed by the possible title question\n• Recall of relevant details and inferences about text\ninformation are addressed by the comprehension questions\n• Fluency, rapid word recognition, and search processes are\naddressed by all questions.\nThese questions also address multiple academic purposes\nfor reading\n(Grabe and Jiang, 2013) : Reading to search for\ninformation (scanning and skimming), reading for quick\nunderstanding (skimming), reading to learn, reading to use\ninformation, and reading for general comprehension.\nQuestion format and grading\nThe IR questions can be administered in multiple formats,\nincluding open-ended and selected-response formats. There\nis a sharp tradeoﬀ associated with these two general formats.\nOpen-ended questions are much easier to automatically\ngenerate (trivial for some of the questions) but very\ndiﬃcult to score automatically. For example, the c-rater\nFrontiers in Artiﬁcial Intelligence /zero.tnum/three.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\n(Leacock and Chodorow, 2003) short-answer automated-\nscoring engine requires hundreds of human-annotated training\nexamples for each question, and as a result has been used\nmostly in research studies\n(Attali et al., 2008; Liu et al.,\n2016), as opposed to operational high-stakes assessments.\nThe alternative selected-response format is trivial to score but\ncan be very diﬃcult to automatically generate the associated\noptions, both distractors and correct answers. In general the\nIR questions were designed as selected-response questions\nand option generation is at the heart of the AIG approach\noutlined below.\nOne exception for the current set of questions is the\ncomprehension questions that employ the format of text\nhighlighting where the test taker is asked to highlight (click\nand drag to select) the answer in the text to the question.\nAlthough this format is open-ended, responses are compared\nto a single correct answer (a particular part of the text).\nFor grading purposes, a text selection is deﬁned as a\npoint in the two-dimensional space for the location of the\nstart and end indexes of the selection. A continuous grade\nbetween 0 and 1 is then calculated based on the discrepancy\n(geometric distance) between the point representations of the\nresponse and the correct answer. This grading approach is\nmore nuanced than one which only considers the degree\nof overlap between a response and the correct answer. For\nexample, two responses might both have no overlap with\nthe correct answer, but one can be closer than the other\nto the correct answer (and therefore will be assigned a\nhigher grade).\nFuture questions\nIn addition to the core questions outlined above, other\nIR questions are being developed, including questions\nthat will address synthesis and critical reading skills. We\nare also developing questions with integrated modalities,\nsuch as reading-listening-writing (a question about the\ntext is spoken, rather than presented in text) and reading-\nspeaking (an answer to a question has to be spoken rather\nthan typed).\nPassage and item generation\nThe IR AIG framework is based on the use of a\nTransformer-based language model to create texts and\nassociated materials from which reading passages, questions,\ncorrect answers, distractors (for selected-response items),\nand other information necessary for automated scoring\nare extracted. For our experiments, we used the GPT-\n3\n(Brown et al., 2020) family of models, which allow\nfor few-shot conditioning of output without an explicit\nﬁne-tuning step.\nPassage generation\nWe start by generating a source passage using a\nTransformer-based Language Model by providing to the model:\n• A set of instructions, which are goals or general\ncharacteristics of the desired text output\n• A set of examples for use in “seeding” the model\n◦ Each example is a text and is associated with a desired\nformat, subject matter, and style\n• A set of attributes to control, or condition, the ﬁnal\ncharacteristics of the text output.\nThe examples are aligned with a particular format, subject,\nnarrative style, or other characteristics, which enables control\nover the qualities of the generated text. For example, the labels\ncan be used to control the topic or domain of the text, the\nregister, the format (i.e., dialogue vs. paragraph vs. enumerated\nlist), allowing a ﬂexible template for generating text with a range\nof qualities that could be useful for deﬁning items with diﬀerent\ngoals. The instructions are a human-readable description of the\ntype of content to generate, in some cases along with other basic\ndetails, such as “Generate short paragraphs from high school\ntextbooks on the speciﬁed topic.”\nQuestion and answer generation\nWe continue by generating questions and possible correct\nanswers in a similar way to generating the source passage, by\nproviding a set of instructions and examples (which consists of\na set of passages and the desired questions and correct answers)\ntogether with the source passage to condition the ﬁnal output of\nthe model.\nSpeciﬁcally, for the main-idea and title tasks, the examples\ninclude a text and its associated main-idea or title. We generate\nmultiple potential answers stochastically and evaluate them\nbased on their similarity to the passage and average negative\nlog likelihood as estimated by the language model. Similarity\nis computed by encoding the passage and each candidate\nanswer using the SentenceTransformers\n/one.tnumlibrary into a vector\nrepresentation and computing the cosine similarity between\nthem. We can also compute similarity between a candidate\ncorrect answer and each sentence in the passage to measure how\nwell an answer aligns with a particular section of the passage.\nThe average negative log likelihood is derived from the output\ndistribution of the large language model. The probability of each\ntoken in a candidate answer, conditioned on all prior tokens,\nis estimated by the language model. These are then converted\nto negative log likelihoods and averaged over each token in the\n/one.tnumhttps://www.sbert.net/\nFrontiers in Artiﬁcial Intelligence /zero.tnum/four.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nsentence to give a representation of how likely the answer is,\nbased on the examples the model has seen.\nFor comprehension questions, examples consist of passages\nwith multiple potential questions and their associated answers.\nThe model is then prompted to generate new questions and\ntheir answers for a source passage. To better guarantee that\nthe generated questions are answerable using the passage, we\nuse an external question answering model\n/two.tnumto predict the\noverall likelihood that the question can be answered. We ﬁlter\nout questions with a low answerability likelihood (determined\nthrough experimentation and manual review), questions that are\nextremely long ( >25 words), questions with answers that are\nvery short (1–2 words), and questions whose predicted answers\ndo not align well with text in the passage.\nFor the completion task, we identify candidate target\nsentences by estimating the likelihood of each sentence in the\ntext, averaged over the tokens in the sentence, using the language\nmodel. Candidates must have between 8 and 30 words and\ncannot be one of the ﬁrst two sentences or the last sentence in\nthe passage. The best candidates will have high likelihood, given\nthe ﬁrst part of the passage. In addition, the ﬁrst few sentences\nimmediately following the candidate should also have a high\nlikelihood, indicating that the candidate sentence ﬁts in well with\nboth its preceding and following contexts.\nTo generate a set of possible incorrect answers (distractors)\nfor the main-idea and title selected-response tasks, we generate\nalternative texts in addition to the original main source passage.\nThese alternative texts are generated using the same instructions,\nexamples, and conditioning attributes as the original text,\nthereby rendering them stylistically and topically similar to the\nsource passage, but they diﬀer in the exact content, making\nthem ideal for use in generating incorrect answers for questions\nto the original source passage. Accordingly, for each of the\nalternative passages, we generate a set of possible answers to each\nitem using the same instructions and examples for generating\npossible correct answers, but conditioning on the alternative\npassage instead of the original one. An example of automatically\nproduced alternative texts, as well as the main ideas and\ntitles associated with them, is presented in the\nAppendix. The\nmain ideas and titles for the original text would serve as the\ncorrect answers for the corresponding tasks, whereas those\nfor the alternative texts would be used as distractors for the\ncorresponding tasks.\nLastly, we compute a suite of NLP metrics for the pool\nof correct and incorrect option candidates in order to select\na single correct answer and several incorrect answers for the\nitem from these candidates. These include the average similarity\nof the answer to other correct answer candidates, similarity of\nthe answer to the source passage and to individual sentences\nin the source passage, and the model’s estimated probability of\ngenerating the candidate answer.\n/two.tnumhttps://huggingface.co/deepset/roberta-base-squad2\nVocabulary in context\nA diﬀerent process is used for generating vocabulary in\ncontext items. To select words for deletion, the language model\nis used to iteratively complete each word in the source passage.\nThe model computes likelihoods for each word in its vocabulary.\nCandidate words for deletion are then ﬁltered based on the\nlikelihood and rank order of the original word being suggested\nby the model (e.g., a word is more likely to be selected if it has\nthe highest likelihood), syntactic information about the word\n(e.g., nouns are more likely to be selected than adjectives),\nsemantic information about the word (e.g., the frequency of\nthe word is taken into account in the context of passage\ndiﬃculty), and the distance (i.e., the number of words that\nseparate consecutive deleted words) between the original word\nand the nearby successful candidates (e.g., the further a word\nis from other candidates, the more likely the word is to be\nselected for deletion). We use Spacy\n(Honnibal et al., 2020)\nfor part of speech tagging and lemmatization and the Corpus\nof Contemporary American English\n(Davies, 2009) for word\nfrequencies. Candidate distractors for deleted words are then\nselected from the model’s likelihood output for all other words\nin its vocabulary. Ideally, successful distractors have low, but not\ntoo low, likelihood, and have the same syntactic part-of-speech\nas the correct answer.\nHuman review\nAll materials, including source passages, questions, correct\nanswers, and distractors, are reviewed by subject-matter\nexperts (SME) with expertise in reading comprehension test\ndevelopment. These SMEs take into account editorial and test\ndesign considerations in reviewing the materials. Since most\npassages successfully pass this review process, any passage that\nis deemed to require more than a few minor edits is discarded.\nLastly, all ﬁnal materials go through fairness and bias review by\na separate group of reviewers.\nLarge-scale pilot\nThe IR task was developed through an iterative process\nthat included multiple pre-pilots that examined task design\nand psychometric issues. As such, this process exempliﬁes\nthe computational psychometrics approach\n(von Davier\net al., 2021) , which blends data- driven computer science\nmethods (machine learning and data mining, in particular)\nand theory-driven psychometrics and language assessment\nconsiderations in order to measure latent abilities in real\ntime. This blend is often instantiated as an iterative and\nadaptive process\n(von Davier, 2017) . The culmination of\nthis process was a large-scale pilot, described in this section,\nwhose purpose was to evaluate the quality of the AIG\nFrontiers in Artiﬁcial Intelligence /zero.tnum/five.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nprocesses described above, both from a human review and\npsychometric perspective.\nThe Duolingo English test\nThe development and large-scale pilot of the IR task\nwas carried out in the context of the Duolingo English Test\n(DET). The DET is a digital-ﬁrst English language proﬁciency\nassessment that is used by colleges and universities to make\nadmissions decisions. A digital-ﬁrst assessment is an assessment\nthat has been designed to be digital end-to-end, with automatic\ntools and theoretical frameworks ﬂuidly integrated for an\noptimal test taking experience (they are in contrast to digitized\nassessments that represent traditional assessments that have\nbeen moved online). A key consideration in the design,\ndevelopment, administration, and scoring of the test is the test\ntaker experience\n(Burstein et al., 2021) . The test is administered\nto test takers in the Duolingo English Test Desktop App via\nthe internet and can be taken anytime anywhere in the world.\nFurthermore, the test leverages advances in machine learning\nand computational psychometrics to create, score items, and\nadminister them adaptively. Its availability and 1-h duration\ncreates an improved experience for test takers over other\nstandardized tests, which take 3–4 h to complete and can only be\ntaken at a testing center, sometimes hundreds of miles away from\nthe test taker’s home. Test scores are reported within 48 h of test\ncompletion, and test takers can share their scores with as many\naccepting institutions as they need, free of charge (\nCardwell\net al., 2022 ).\nInitial machine-generation phase\nAs an initial step, over 14,000 passages were generated in\nthree text genres: news, expository, and narrative texts. We\nprovided 3–5 examples to GPT-3 of the given genre, with\neach example consisting of a topic, title, and passage. When\ngenerating new passages, we conditioned the output by just\nthe topic, with expository and narrative texts generated based\non 270 possible topics corresponding to common ﬁelds of\nuniversity study, while news texts were based on 45 common\nnews article categories.\nThis set of passages went through two rounds of automated\nﬁlters. The ﬁrst set ﬁltered the passage based on general desirable\nqualities of the text. We ﬁltered passages to be 100–175 words\nwith 5–20 sentences. We removed passages that contain repeated\n8-grams to avoid passages containing a high degree of repetition\nby the language model, as well as passages that contain profanity\nor other potentially oﬀensive content using a standard oﬀensive\nword list. We also ﬁltered passages in two ways based on the\nnegative log likelihood of each sentence as estimated by the\nlanguage model, averaged over the tokens: We imposed an\nupper bound on the maximum negative log likelihood that any\nsentence can have as a simple estimate of the expected coherence\nof the passage. If a sentence has extremely high negative log\nlikelihood, it is more likely to make less sense in the context of\nthe passage. Additionally, for the completion task we imposed an\neven stricter negative log likelihood threshold to identify viable\ncandidate sentences, as we wanted to ensure that the correct\nanswer is very likely to ﬁt in the context of the passage. These\nthresholds were identiﬁed through manual review of passages\nand potential candidate sentences.\nFor this pilot eﬀort, we sampled 800 texts from the set\nof passages that passed this ﬁrst round of ﬁlters. We then\ngenerated items for these texts and simultaneously ﬁltered\nthem based on the ability of our item generation processes to\ngenerate appropriate items with suﬃcient distractors according\nto the metric-based criteria described in the previous section.\nPassages for which items and distractors could not be generated\naccording to our speciﬁcations were removed from our set. We\nretained a set of 789 passages following the item generation\nand ﬁltering process. For each of these passages we generated\na vocabulary-in-context task (with 6.6 ﬁll-in-the-blank words on\naverage, and with four distractors), one text completion task, two\ncomprehension tasks, one main-idea task, and one possible title\ntask. All selected-response tasks (except for the vocabulary-in-\ncontext task) were generated with three distractors.\nHuman review\nContent and fairness review was conducted by 12\nexternal reviewers and six internal Duolingo team members.\nExternal reviewers had diverse backgrounds with regard\nto gender identity, age, and racial/ethnic background.\nAll had at least a bachelor’s degree (and in some cases a\nPh.D.) in linguistics, language studies, or a related ﬁeld.\nThey had expertise in teaching and assessing in a relevant\nlinguistic and cultural context. Internal team members\nhad in most cases a Ph.D. and considerable expertise in\nassessment development.\nEach passage and question went through multiple rounds\nof reviews, with a minimum of three content reviews and two\nfairness reviews. Content reviewers independently evaluated the\nappropriateness of the content and made edit suggestions. For\npassages, content reviewers evaluated the cohesion, clarity, and\nlogical consistency throughout the text. For questions, reviewers\njudged the viability of each option by ensuring that the correct\nanswer is correct and the distractors are incorrect. In cases\nwhere the evaluation recommended hefty edits to the passage\nor the questions, these edits were reviewed, adjudicated, and\nincorporated by additional reviewers.\nFollowing assessment fairness guidelines\n(Zieky, 2015) ,\nfairness reviewers reviewed passages and questions to ensure\nthey did not contain any content that was too culturally speciﬁc,\nFrontiers in Artiﬁcial Intelligence /zero.tnum/six.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nhad technical or ﬁeld-speciﬁc jargon, or could be potentially\nsensitive to test takers.\nIn summary, following all reviews and adjudication a ﬁnal\nset of 454 out of 789 passages (58%) were retained. The review\nprocess is estimated to have taken about 15 min per passage\n(including questions), across all rounds of review.\nAdministration\nThe pilot for the IR task was administered at the end of\nthe DET practice test (\nhttps://englishtest.duolingo.com/home),\nwhich is a shorter version of the operational DET and is freely\navailable. Similar to the operational test, the practice test is fully\nadaptive and test takers have the opportunity to respond to and\npractice all of the tasks that are included in the operational\ntest. At the end of the practice test, test takers were randomly\nassigned one of the 454 passages. The time limit for the IR\nsection was 8 min (determined based on previous pilots). The\npilot was active for 21 days, during which nearly 200 thousand\nIR sessions were completed.\nPsychometric results\nThis section summarizes results from various psychometric\nitem analyses. As a reminder, the pilot included 454 passages,\neach with a vocabulary-in-context task (with 6.6 ﬁll-in-the-\nblank words on average), one text completion task, two\ncomprehension tasks, one main-idea task, and one possible title\ntask. A total of 5,246 items (the term item is used in this section\nto refer to a measurement opportunity) were ﬁelded with an\naverage of 425 responses collected per item.\nResponse time\nOne of the purposes of the interactive nature of IR passages\nis to take advantage of earlier processing of the text to reduce\nthe time required for answering later questions. As expected\nfrom this design characteristic of the task,\nFigure 1 shows a sharp\ndecrease in response time from the ﬁrst task (cloze) to later tasks.\nEasiness and discrimination\nThe two primary psychometric indicators for items are item\neasiness and item discrimination. Item easiness (or diﬃculty),\nmeasured here as the mean score on the item (which is equal\nto percent correct for all tasks except the comprehension task),\nevaluates the degree that test takers successfully respond to the\nitems. Item discrimination evaluates how well an item is able to\ndistinguish between test takers who are knowledgeable and those\nwho are not. Item discrimination is measured here with item-\ntotal correlations, where total was deﬁned as the total practice\ntest score. A higher item-total correlation signiﬁes that test\ntakers with higher total scores were more likely to also answer\nthe item correctly.\nFigure 2 shows a wide distribution of item easiness (with\nan overall mean score of 70%) with earlier tasks (cloze and\ncompletion) less easy than later tasks.\nFigure 3 presents distributions of item-total correlations.\nNote that the practice test does not include reading items and\nis also quite short (up to 15 min), both of which would have\nthe eﬀect of moderating item-total correlations. Nevertheless,\nmost items show reasonably high discrimination (with an overall\naverage of 0.27), with small diﬀerences across tasks. In total,\nonly 6% of items had an item-total correlation lower than 0.1.\nHowever, even this number can be signiﬁcantly reduced through\na distractor analysis. By computing discrimination indices for\neach distractor (see\nAttali and Fraenkel, 2000 ) we can identify\ndistractors for which the average total score of test takers\nendorsing them is higher than the average total score of test\ntakers endorsing the correct answer. About 3% of all distractors\nare failing in this way. Removing these distractors (essentially\nadministering the item with one less distractor) reduces the\nnumber of items with item-total correlation lower than 0.1 to\nonly 2%.\nLocal item dependence\nAn important assumption for psychometric analysis of test\nitems is that the dependency between responses to any pair of\nitems is due only to the trait being measured. Pairs of items that\nviolate this assumption are said to exhibit local item dependency\n(LID). It is well-known (\nYen, 1993 ) that sets of items that are\nbased on a common stimulus, such as reading comprehension\nquestions, can result in local dependence that is due to the fact\nthat information used to answer diﬀerent items is interrelated\nin the stimulus. In the present context, the threat of LID is\neven greater since the items were automatically generated. In\nparticular, we expected that LID is more likely to be present\nbetween two deleted words of the same cloze task, as well as\nbetween two later tasks (e.g., the main-idea and the title task).\nA standard item response theory (IRT) approach for\ninvestigating LID between test items is to compute the\ncorrelation between residual responses—the diﬀerence between\nthe expected model-based score and the actual observed\nscore (\nYen, 1984 ). As an approximation of this approach,\nwe computed the partial correlations between pairs of items,\ncontrolling for total practice test score. In the analyses\nbelow, a common threshold of 0.3 (\nChristensen et al.,\n2017) was employed for categorizing residual correlations as\nindicating LID.\nFigure 4 presents distributions of these partial correlations\nfor the three types of pairs of items. Although pairs of items that\nare not of the same type (Other-Cloze) have slightly lower partial\ncorrelations (with an average of 0.07) than either Cloze-Cloze or\nFrontiers in Artiﬁcial Intelligence /zero.tnum/seven.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nFIGURE /one.tnum\nMedian response time distributions (in seconds).\nFIGURE /two.tnum\nMean score (easiness) distributions.\nOther-Other pairs of items (0.1 and 0.11, respectively), only 1%\nof all 9,437 Cloze-Cloze pairs and 3% of all 4,540 Other-Other\npairs exceed the threshold of 0.3.\nFuture directions\nL/two.tnum vs. L/one.tnum\nThe IR task can be used to assess reading skills of students\nfrom a variety of backgrounds and levels. However, in our study\nwe focused on readers whose ﬁrst language is not English. The\nsecond language (L2) readers who participated in this study are\ndiﬀerent from ﬁrst-language (L1) readers of English in many\nrespects, but also share some similarities, including cognitive\nand linguistic components such as word recognition, vocabulary\nknowledge, morphosyntactic knowledge, and world knowledge\n(Grabe and Jiang, 2013; Nassaji, 2014) . The two groups are\nalso similar in their advanced academic and professional jr\nreading strategies\n(Grabe and Stoller, 2013) . In addition, since\nboth groups are diverse and need to be able to read for a\nvariety of diﬀerent purposes, the IR task is potentially useful for\nboth L1 and L2 reading assessment. This can be the focus of\nfuture research.\nInteractive assessment\nProviding feedback regarding task performance is one of\nthe most frequently applied of all psychological interventions\n(Kulhavy and Stock, 1989; Shute, 2008) . Feedback helps\nlearners determine performance expectations, judge their level\nof understanding, and become aware of misconceptions. It may\nFrontiers in Artiﬁcial Intelligence /zero.tnum/eight.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nFIGURE /three.tnum\nItem-total correlations with total practice test score.\nFIGURE /four.tnum\nPartial correlations between item pairs.also provide clues about the best approaches for correcting\nmistakes and improving performance. However, feedback has\nhad almost no place in large-scale assessment\n(Attali and Powers,\n2010; Attali, 2015) .\nThe IR task has already incorporated interactivity with the\ngradual revelation of the passage: rather than presenting the\npassage in its entirety, the passage is sequentially revealed across\nthree diﬀerent item types. At the same time, a subtle form of\ncorrective feedback is provided, whereby along with the rest of\nthe passage, the answers to the previous items are also revealed.\nWe are exploring ways to enhance feedback. For example, test\ntakers could receive immediate feedback about the correctness\nof their responses and be asked to try again in the case of\nerrors. This type of multiple-try feedback has been shown to\ndecrease test anxiety and improve measurement accuracy by\nallowing partial credit grading\n(Attali and Powers, 2010; Attali,\n2011).\nModeling of task diﬃculty\nOur methods for passage and item generation allow us to\ngenerate texts with a range of diﬃculty levels by controlling\nthe register and domain of the generated text. Much prior\nwork has explored ways to estimate the diﬃculty of reading\npassages and comprehension questions based on a wide range\nof lexical, syntactic, and discourse properties\n(Xia et al., 2016;\nHuang et al., 2017; Settles et al., 2020) . Additionally, recent\nwork has demonstrated the ability to estimate the psychometric\nproperties of passage-based items using a featurized approach\nFrontiers in Artiﬁcial Intelligence /zero.tnum/nine.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nbased on the BERT transformer model (Devlin et al., 2019) ,\nallowing for the estimation of parameters of newly created items\nthat do not yet have any response data\n(McCarthy et al., 2021) .\nThis work primarily focuses on estimating the psychometric\nproperties of items that test vocabulary in context, so we plan\nto explore methods to adapt it to multiple choice and reading\ncomprehension questions about the passages. These estimates\ncan then be used to reﬁne our item generation processes\nin order to select passages, distractors and comprehension\nquestions that will result in the items having the desired\npsychometric properties.\nPsychometric modeling\nThe current set of IR questions include a mix of\nbinary graded questions (for selected-response questions) and\ncontinuous graded questions (between 0 and 1, for highlight\ncomprehension questions). This presents unique challenges in\nterms of psychometric modeling, since continuous response\nmodels are not often discussed in the context of IRT. For\nmodeling responses on the unit interval, the Beta distribution\nhas been proposed in the psychometrics literature\n(Noel and\nDauvier, 2007; Chen et al., 2019) . However, existing approaches\nare limited in diﬀerent ways. The approach by Chen et al. (2019)\nassumes that latent ability is constrained to the unit interval,\na constraint that is inconsistent with normal IRT assumptions.\nThe approach by\nNoel and Dauvier (2007) does not include\ndiscrimination parameters, a constraint that is also inconsistent\nwith other common IRT models, such as the 2PL model. The\nIR task would require innovative psychometric models that can\ncombine discrete and continuous grades, as well as support\nmodeling of item discrimination.\nOther types of texts and tasks\nThe IR task is a rich breeding ground for more innovations\nin terms of both assessment design and content generation.\nOther types of texts can be considered as candidates for\npassage generation; for instance, argumentative texts of multiple\nviewpoints would support additional tasks like synthesis.\nAnother possible avenue is diﬀerent response formats. The\ndigital format of IR makes it possible to accommodate multiple\ntypes of response formats, such as dragging and dropping, free\nresponse (both spoken and written), and interacting with other\nmedia such as a chart, a table, or a graph. The advantage\nof the digital format can be extended to the modality of the\ndelivery as well, where, for instance, comprehension questions\ncould be delivered aurally. All of these would allow for a fuller\napproximation of what is read, how it is read, and what comes\nafter reading in academic contexts.\nConclusions\nThis paper demonstrates how recent advances in\ncomputational language modeling can transform item\ndevelopment for complex tasks and assessments. A combination\nof task design, text generation techniques, and psychometric\nanalysis allows us to create reading passages and associated\nassessment tasks that can be automatically scored. This has not\nbeen possible with more traditional AIG approaches. These\nadvances in turn support increasingly complex digital-ﬁrst\nassessment systems that integrate a theoretical framework\nof domain expertise with AI tools, technology infrastructure\nand psychometrics.\nData availability statement\nThe original contributions presented in the study are\nincluded in the article/supplementary materials, further\ninquiries can be directed to the corresponding author.\nEthics statement\nEthical review and approval was not required for the study\non human participants in accordance with the local legislation\nand institutional requirements. The patients/participants\nprovided their written informed consent to participate in\nthis study.\nAuthor contributions\nAll authors listed have made a substantial, direct,\nand intellectual contribution to the work and approved it\nfor publication.\nConﬂict of interest\nYA, AR, GTL, KY, SG, YP , and AAD were employed by\nDuolingo.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed\nor endorsed by the publisher.\nFrontiers in Artiﬁcial Intelligence /one.tnum/zero.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nReferences\nAttali, Y. (2011). Immediate feedback and opportunity to revis e answers:\napplication of a graded response IRT model. Appl. Psychol. Meas. 35, 472–479.\ndoi: 10.1177/0146621610381755\nAttali, Y. (2015). Eﬀects of multiple-try feedback and question t ype during\nmathematics problem solving on performance in similar problems. Comput. Educ.\n86, 260–267. doi: 10.1016/j.compedu.2015.08.011\nAttali, Y. (2018). “Automatic item generation unleashed: An e valuation of a\nlarge-scale deployment of item models, ” in International Conference on Artiﬁcial\nIntelligence in Education (London), 17–29. doi: 10.1007/978-3-319-93843-1_2\nAttali, Y., and Fraenkel, T. (2000). The point-biserial as a dis crimination index\nfor distractors in multiplechoice items: Deﬁciencies in usage and an alternative. J.\nEduc. Measure. 37, 77–86. Available online at: https://www.nite.org.il/ﬁles/reports/\ne252.pdf\nAttali, Y., and Powers, D. (2010). Immediate feedback and oppor tunity\nto revise answers to open-ended questions. Educ. Psychol. Meas. 70, 22–35.\ndoi: 10.1177/0013164409332231\nAttali, Y., Powers, D., Freedman, M., Harrison, M., and Obetz , S. (2008).\nAutomated scoring of short-answer open-ended GRE R⃝ Subject Test items. ETS\nRes. Rep. Ser. 2008, i-22. doi: 10.1002/j.2333-8504.2008.tb02106.x\nBartram, D., and Hambleton, R. (2005). Computer-Based Testing and the\nInternet: Issues and Advances . Hoboken, NJ: John Wiley & Sons.\nBejar, I. I. (2002). “Generative testing: from conception to i mplementation, ”\nin Item Generation for Test Development , eds S. H. Irvine and P. C. Kyllonen\n(Mahwah, NJ: Erlbaum), 199–217.\nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhar iwal,\nP., et al. (2020). “Language models are few-shot learners, ” in Advances\nin Neural Information Processing Systems, Vol. 33 , eds H. Larochelle, M.\nRanzato, R. Hadsell, M. F. Balcan, and H. Lin (Curran Associates , Inc),\n1877–1901. Available online at: https://proceedings.neurips.cc/paper/2020/ﬁle/\n1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf\nBurstein, J., LaFlair, G. T., Kunnan, A. J., and von Davier, A. A . (2021).\nA Theoretical Assessment Ecosystem for a Digital-First Assessment -The Duolingo\nEnglish Test (Duolingo Research Report DRR-21-04) .\nCardwell, R., LaFlair, G. T., and Settles, B. (2022). Duolingo English Test:\nTechnical Manual . Available online at: https://duolingo-testcenter.s3.amazonaws.\ncom/media/resources/techinical_manual.pdf\nChen, Y., Silva Filho, T., Prudencio, R. B., Diethe, T., and Flac h, P. (2019). “ β 3-\nIRT: a new item response model and its applications, ” in The 22nd International\nConference on Artiﬁcial Intelligence and Statistics (Naha), 1013–1021.\nChristensen, K. B., Makransky, G., and Horton, M. (2017). Cri tical values for\nYen’s Q3: Identiﬁcation of local dependence in the Rasch model using residual\ncorrelations. Appl. Psychol. Measure. 41, 178–194. doi: 10.1177/0146621616677520\nDavies, M. (2009). The 385 + million word Corpus of Contemporary American\nEnglish (1990-2008 +): design, architecture, and linguistic insights. Int. J. Corpus\nLinguist. 14, 159–190. doi: 10.1075/ijcl.14.2.02dav\nDevlin, J., Chang, M.-W., Lee, K., and Toutanova, K. (2019). “ BERT: pre-training\nof deep bidirectional transformers for language understand ing, ” in Proceedings\nof the 2019 Conference of the North American Chapter of the Association fo r\nComputational Linguistics: Human Language Technologies, Vol. 1 (Minneapolis:\nLong and Short Papers), 4171–4186.\nDowning, S. M., and Haladyna, T. M. (2006). Handbook of Test Development .\nMahwah, NJ: Lawrence Erlbaum Associates Publishers.\nDuolingo (2022). Duolingo English Test: Oﬃcial Guide for Test Takers . Available\nonline at: https://englishtest.duolingo.com/guide\nEmbretson, S., and Yang, X. (2006). Automatic item generati on\nand cognitive psychology. Handb. Statist. Psychometr. 26, 747–768.\ndoi: 10.1016/S0169-7161(06)26023-1\nGierl, M. J., and Haladyna, T. M. (2013). Automatic Item Generation: Theory and\nPractice. New York, NY: Routledge.\nGrabe, W., and Jiang, X. (2013). “Assessing reading, ” in The Companion to\nLanguage Assessment , ed A. J. Kunnan (Hoboken, NJ: John Wiley & Sons, Ltd.),\n185–200.\nGrabe, W., and Stoller, F. L. (2013). Teaching and Researching: Reading, 3rd Edn .\nNew York, NY: Routledge.\nHaladyna, T. M. (2013). “Automatic item generation: a histor ical perspective, ”\nin Automatic Item Generation: Theory and Practice , eds M. J. Gierl and T. M.\nHaladyna (New York, NY: Routledge), 13–25.\nHommel, B. E., Wollang, F.-J. M., Kotova, V., Zacher, H., and Sch mukle,\nS. C. (2021). Transformer-based deep neural language modelin g for construct-\nspeciﬁc automatic item generation. Psychometrika 87, 1–24. doi: 10.31234/osf.io/\nqfvpe\nHonnibal, M., Montani, I., Van Landeghem, S., and Boyd, A. (20 20). spaCy:\nIndustrial-Strength Natural Language Processing in Python . Honolulu, HI: Zenodo.\nHuang, Z., Liu, Q., Chen, E., Zhao, H., Gao, M., Wei, S., et al. ( 2017). “Question\ndiﬃculty prediction for reading problems in standard tests, ” in Thirty-First AAAI\nConference on Artiﬁcial Intelligence (San Francisco, CA).\nIrvine, S. H., and Kyllonen, P. C. (2002). Item Generation for Test Development .\nMahwah, NJ: Erlbaum.\nKhan, S., Huang, Y., Pu, S., Tarasov, V., Andrade, A., Meisne r, R., et al. (2020).\n“Sphinx: an automated generation system for English reading c omprehension\nassessment, ” inInternational Conference on Learning Analytics and Knowledge .\nKulhavy, R. W., and Stock, W. A. (1989). Feedback in written\ninstruction: the place of response certitude. Educ. Psychol. Rev. 1, 279–308.\ndoi: 10.1007/BF01320096\nKumar, V., Boorla, K., Meena, Y., Ramakrishnan, G., and Li, Y.- F. (2018).\n“Automating reading comprehension by generating question an d answer pairs, ”\nin Paciﬁc-Asia Conference on Knowledge Discovery and Data Mining (Melbourne),\n335–348. doi: 10.1007/978-3-319-93040-4_27\nKumar, V., Muneeswaran, S., Ramakrishnan, G., and Li, Y.-F. ( 2019). Paraqg:\na system for generating questions and answers from paragraphs. ArXiv Preprint\nArXiv:1909.01642. doi: 10.18653/v1/D19-3030\nKurdi, G., Leo, J., Parsia, B., Sattler, U., and Al-Emari, S. (20 20). A systematic\nreview of automatic question generation for educational purpo ses. Int. J. Artiﬁcial\nIntell. Educ. 30, 121–204. doi: 10.1007/s40593-019-00186-y\nLeacock, C., and Chodorow, M. (2003). C-rater: automated sc oring of\nshort-answer questions. Comput. Hum. 37, 389–405. doi: 10.1023/A:10257796\n19903\nLiu, O. L., Rios, J. A., Heilman, M., Gerard, L., and Linn, M. C. ( 2016). Validation\nof automated scoring of science assessments. J. Res. Sci. Teach. 53, 215–233.\ndoi: 10.1002/tea.21299\nMcCarthy, A. D., Yancey, K. P., LaFlair, G. T., Egbert, J., Liao , M., and\nSettles, B. (2021). “Jump-starting item parameters for adaptiv e language tests, ”\nin Proceedings of the 2021 Conference on Empirical Methods in Natural Langua ge\nProcessing, 883–899.\nNassaji, H. (2014). The role and importance of lower-level proces ses in second\nlanguage reading. Lang. Teach. 47, 1–37. doi: 10.1017/S0261444813000396\nNey, H., Essen, U., and Kneser, R. (1994). On structuring prob abilistic\ndependences in stochastic language modelling. Comput. Speech Lang. 8, 1–38.\ndoi: 10.1006/csla.1994.1001\nNoel, Y., and Dauvier, B. (2007). A beta item response model\nfor continuous bounded responses. Appl. Psychol. Meas. 31, 47–73.\ndoi: 10.1177/0146621605287691\nRadford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskeve r, I., et al. (2019).\nLanguage models are unsupervised multitask learners. OpenAI Blog 1, 9.\nSettles, B., T., LaFlair, G., and Hagiwara, M. (2020). Machine le arning-\ndriven language assessment. Trans. Assoc. Comput. Linguist. 8, 247–263.\ndoi: 10.1162/tacl_a_00310\nShannon, C. E. (1951). Prediction and entropy of printed Englis h. Bell Syst. Tech.\nJ. 30, 50–64. doi: 10.1002/j.1538-7305.1951.tb01366.x\nShermis, M. D., and Burstein, J. (2013). Handbook of Automated Essay\nEvaluation: Current Applications and New Directions . New York, NY: Routledge.\nShute, V. J. (2008). Focus on formative feedback. Rev. Educ. Res. 78, 153–189.\ndoi: 10.3102/0034654307313795\nSireci, S. G., and Zenisky, A. L. (2006). “Innovative item for mats in computer-\nbased testing: in pursuit of improved construct representatio n, ” in Handbook of\nTest Development, eds S. M. Downing and T. M. Haladyna (Mahwah, NJ: Erlbaum),\n329–347.\nSun, C., Qiu, X., Xu, Y., and Huang, X. (2019). “How to ﬁne-tun e BERT\nfor text classiﬁcation?” in Chinese Computational Linguistics, Vol. 11856 , eds\nM. Sun, X. Huang, H. Ji, Z. Liu, and Y. Liu (Kunming: Springer In ternational\nPublishing), 194–206.\nvan der Linden, W. J., and Glas, C. A. (2010). Elements of Adaptive Testing . New\nYork, NY: Springer.\nFrontiers in Artiﬁcial Intelligence /one.tnum/one.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N.,\net al. (2017). Attention is all you need. Adv. Neural Inf. Process. Syst. 30, 1–5.\ndoi: 10.48550/arXiv.1706.03762\nvon Davier, A. A. (2017). Computational psychometrics in suppor t\nof collaborative educational assessments. J. Educ. Measure. 54, 3–11.\ndoi: 10.1111/jedm.12129\nvon Davier, A. A., Mislevy, R. J., and Hao, J. (2021). “Introdu ction to\ncomputational psychometrics: towards a principled integration of data science and\nmachine learning techniques into psychometrics, ” in Computational Psychometrics:\nNew Methodologies for a New Generation of Digital Learning and Asse ssment, eds\nA. A. von Davier, R. J. Mislevy, and J. Hao (Cham: Springer), 1–6 .\nvon Davier, M. (2018). Automated item generation with recur rent neural\nnetworks. Psychometrika 83, 847–857. doi: 10.1007/s11336-018-9608-y\nWhitely, S. E. (1983). Construct validity: construct represen tation versus\nnomothetic span. Psychol. Bull. 93, 179. doi: 10.1037/0033-2909.93.1.179\nXia, M., Kochmar, E., and Briscoe, T. (2016). “Text readabili ty assessment for\nsecond language learners, ” in Proceedings of the 11th Workshop on Innovative\nUse of NLP for Building Educational Applications (San Diego, CA), 12–22.\ndoi: 10.18653/v1/W16-0502\nYang, W., Xie, Y., Lin, A., Li, X., Tan, L., Xiong, K., et al. (20 19). “End-to-end\nopen-domain question answering with BERTserini, ” in Proceedings of the 2019\nConference of the North American Chapter of the Association for Computati onal\nLinguistics (Minneapolis: Demonstrations), 72–77.\nYen, W. M. (1984). Eﬀects of local item dependence on the ﬁt and e quating\nperformance of the threeparameter logistic model. Appl. Psychol. Measure.\n8, 125–145.\nYen, W. M. (1993). Scaling performance assessments: Strategi es for managing\nlocal item dependence. J. Educ. Measure. 30, 187–213.\nZieky, M. J. (2015). “Developing fair tests, ” in Handbook of Test Development , eds\nS. Lane, M. R. Ramond, and T. M. Haladyna (New York, NY: Routledg e), 97–115.\nFrontiers in Artiﬁcial Intelligence /one.tnum/two.tnum frontiersin.org\nAttali et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/two.tnum./nine.tnum/zero.tnum/three.tnum/zero.tnum/seven.tnum/seven.tnum\nAppendix\nAn example of alternative texts\nOriginal text\nFrom the largest animals to the tiniest, an ecosystem can\nbe organized by energy into a pyramid called the energy\npyramid. At the bottom of the pyramid are producers, which\nare organisms that make their own food, such as plants. Plants\nproduce food by taking in carbon dioxide from the air and\nwater from the soil. Plants use sunlight to create sugars from the\ncarbon dioxide molecules and water molecules. The plant moves\nsugars through its body and assembles these sugars into long\nchains of molecules called polysaccharides. The polysaccharides\nmay be used immediately, or stored for later use by the plant. A\nplant may store several million calories of energy in this manner,\nenough to live for several weeks or more without sun. Most of\nthis energy is glucose, a sugar that animals also need to survive.\nThe rest of the organisms in the pyramid rely on the energy that\ncomes from producers.\nMain idea: Producers are at the bottom of the energy\npyramid and are the source of food and energy for the other\norganisms in the ecosystem.\nTitle: Producers in an Ecosystem\nComprehension Questions: What do plants use\nto create energy? How can a plant survive for weeks\nwithout sunlight?\nAlternative text /one.tnum\nIn biology, the term energy pyramid refers to a graphical\nway of representing the position of energy-storing or energy-\nproviding organisms in an ecosystem. Organisms on the lower\nlevels of the pyramid—primary producers—can make their own\nfood from sunlight and inorganic substances, while those on\nthe upper levels—consumers—gain their nutrition from other\nliving things. The energy pyramid also shows how each level\nis dependent on the one below it. The food chain shows\nhow energy from the sun is passed along from one organism\nto another, where it is eventually lost when organisms die,\ndecompose, or are eaten.\nMain idea: The energy pyramid is a graphical representation\nof how energy is passed along from one organism to another in\nan ecosystem.\nTitle: Energy Pyramids in Ecosystems\nAlternative text /two.tnum\nThe energy pyramid is a model that shows the ﬂow of\nenergy in an ecosystem. The sun is the source of all energy\nin the ecosystem, and plants use this energy to create food.\nThe energy then ﬂows from the plants to the animals that\neat them. When animals eat plants, they only use a small\namount of the energy that the plants have stored. The rest\nof the energy is lost as heat. This is why there are fewer\nanimals at each level of the food chain. When animals eat\nother animals, they also use only a small amount of the\nenergy from the animal they are eating. The vast majority\nof the sun’s energy is lost as heat at each level of the\nfood chain.\nMain idea : The energy pyramid is a model that\nshows how energy ﬂows from the sun to plants to\nanimals, with most of the energy being lost as heat at\neach level.\nTitle: Energy and the Food Chain\nFrontiers in Artiﬁcial Intelligence /one.tnum/three.tnum frontiersin.org"
}