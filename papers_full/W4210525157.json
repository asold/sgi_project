{
  "title": "Tracker Meets Night: A Transformer Enhancer for UAV Tracking",
  "url": "https://openalex.org/W4210525157",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2120996495",
      "name": "Ye, Junjie",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A2013756148",
      "name": "Fu, Changhong",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A2743004273",
      "name": "Cao Ziang",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A2384721666",
      "name": "An Shan",
      "affiliations": [
        "Jingdong (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2256597320",
      "name": "Zheng Guangze",
      "affiliations": [
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A2014885863",
      "name": "Li, Bowen",
      "affiliations": [
        "Tongji University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3195152959",
    "https://openalex.org/W3192545136",
    "https://openalex.org/W3167454939",
    "https://openalex.org/W2963534981",
    "https://openalex.org/W3001584168",
    "https://openalex.org/W3035453691",
    "https://openalex.org/W3203510176",
    "https://openalex.org/W3214586131",
    "https://openalex.org/W3123853823",
    "https://openalex.org/W4205469118",
    "https://openalex.org/W3169909246",
    "https://openalex.org/W2566376500",
    "https://openalex.org/W3134649899",
    "https://openalex.org/W3121661546",
    "https://openalex.org/W3182700213",
    "https://openalex.org/W3174792937",
    "https://openalex.org/W3147590024",
    "https://openalex.org/W2470394683",
    "https://openalex.org/W3035326127",
    "https://openalex.org/W3170697543",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2076205488",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W4312812783",
    "https://openalex.org/W3000172657",
    "https://openalex.org/W3176820334",
    "https://openalex.org/W3114677757",
    "https://openalex.org/W3105335430",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W4389666313",
    "https://openalex.org/W4214493665",
    "https://openalex.org/W6754146604",
    "https://openalex.org/W6757817989",
    "https://openalex.org/W2518876086",
    "https://openalex.org/W2908510526",
    "https://openalex.org/W4293111169",
    "https://openalex.org/W2949187370",
    "https://openalex.org/W4302803934",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3156811085",
    "https://openalex.org/W3094502228"
  ],
  "abstract": "Most previous progress in object tracking is realized in daytime scenes with favorable illumination. State-of-the-arts can hardly carry on their superiority at night so far, thereby considerably blocking the broadening of visual tracking-related unmanned aerial vehicle (UAV) applications. To realize reliable UAV tracking at night, a spatial-channel Transformer-based low-light enhancer (namely SCT), which is trained in a novel task-inspired manner, is proposed and plugged prior to tracking approaches. To achieve semantic-level low-light enhancement targeting the high-level task, the novel spatial-channel attention module is proposed to model global information while preserving local context. In the enhancement process, SCT denoises and illuminates nighttime images simultaneously through a robust non-linear curve projection. Moreover, to provide a comprehensive evaluation, we construct a challenging nighttime tracking benchmark, namely DarkTrack2021, which contains 110 challenging sequences with over 100 K frames in total. Evaluations on both the public UAVDark135 benchmark and the newly constructed DarkTrack2021 benchmark show that the task-inspired design enables SCT with significant performance gains for nighttime UAV tracking compared with other top-ranked low-light enhancers. Real-world tests on a typical UAV platform further verify the practicability of the proposed approach. The DarkTrack2021 benchmark and the code of the proposed approach are publicly available at https://github.com/vision4robotics/SCT.",
  "full_text": "Tracker Meets Night: A Transformer Enhancer for UA V Tracking\nJunjie Ye1, Changhong Fu 1, Ziang Cao 2, Shan An 3, Guangze Zheng 1, and Bowen Li 1\nAbstract— Most previous progress in object tracking is re-\nalized in daytime scenes with favorable illumination. State-\nof-the-arts can hardly carry on their superiority at night so\nfar, thereby considerably blocking the broadening of visual\ntracking-related unmanned aerial vehicle (UA V) applications.\nTo realize reliable UA V tracking at night, a spatial-channel\nTransformer-based low-light enhancer (namely SCT), which\nis trained in a novel task-inspired manner, is proposed and\nplugged prior to tracking approaches. To achieve semantic-\nlevel low-light enhancement targeting the high-level task, the\nnovel spatial-channel attention module is proposed to model\nglobal information while preserving local context. In the en-\nhancement process, SCT denoises and illuminates nighttime\nimages simultaneously through a robust non-linear curve pro-\njection. Moreover, to provide a comprehensive evaluation, we\nconstruct a challenging nighttime tracking benchmark, namely\nDarkTrack2021, which contains 110 challenging sequences\nwith over 100 K frames in total. Evaluations on both the\npublic UA VDark135 benchmark and the newly constructed\nDarkTrack2021 benchmark show that the task-inspired design\nenables SCT with signiﬁcant performance gains for night-\ntime UA V tracking compared with other top-ranked low-\nlight enhancers. Real-world tests on a typical UA V platform\nfurther verify the practicability of the proposed approach.\nThe DarkTrack2021 benchmark and the code of the proposed\napproach are publicly available at https://github.com/\nvision4robotics/SCT.\nI. INTRODUCTION\nVisual tracking is a fundamental task in numerous\nunmanned aerial vehicle (UA V)-based applications, e.g.,\ntarget following [1], autonomous landing [2], and self-\nlocalization [3]. Given an initial position of an object, track-\ners are expected to estimate the location of the object in the\nfollowing period. Recent years have witnessed a huge leap\nforward in visual tracking. Continuously emerging tracking\napproaches [4]–[8] keep setting state-of-the-arts (SOTAs)\nin large-scale benchmarks. Nevertheless, previous progress\nis made on daytime sequences captured under favorable\nillumination conditions. In real-world applications of UA Vs,\nvision systems are always required to provide robust perfor-\nmance around the clock, while recent studies [9], [10] show\nthat SOTA trackers can hardly maintain their superiority\nin low-light conditions. Hence, the broadening of related\nUAV applications is impeded heavily by unstable tracking\nperformance at nighttime.\n*Corresponding Author\n1Junjie Ye, Changhong Fu, Guangze Zheng, and Bowen Li are with the\nSchool of Mechanical Engineering, Tongji University, Shanghai 201804,\nChina. changhongfu@tongji.edu.cn\n2Ziang Cao is with the School of Automotive Studies, Tongji University,\nShanghai 201804, China.\n3Shan An is with Tech & Data Center, JD.COM Inc., Beijing 100108,\nChina.\n0.36 0.38 0.40 0.42 0.44 0.46 0.48 0.50 0.52\nSuccess rate\n0.48\n0.51\n0.54\n0.57\n0.60\n0.63\n0.66\n0.69\nPrecision\nOverall performance on DarkTrack2021\nDiMP50 DiMP18 PrDiMP50 SiamRPN++ HiFT SiamAPN++\nBase tracker with SCT\nFig. 1. Overall performance of SOTA trackers [4]–[7], [11] with the\nproposed SCT enabled (markers in a dark color) or not (markers in a\nlight color) in the newly constructed nighttime UA V tracking benchmark—\nDarkTrack2021. SCT signiﬁcantly boosts the nighttime tracking perfor-\nmance of trackers in a plug-and-play manner.\nGenerally, images captured at night are always with dim\nbrightness and low-intensity contrast. In this case, feature ex-\ntractors trained with daytime images lose their effectiveness\nand lead to unsatisfying object tracking. Along with poor\nilluminance, the high-level noise also damages the structural\ndetails of images, further degrading tracking performance.\nWhat is worse, there are few public nighttime tracking\nbenchmarks with full annotations that are large enough to\nafford tracker training.\nIntuitively, one may turn to low-light enhancement ap-\nproaches and serve them as a preprocessing step. However,\nmost existing low-light enhancers [12]–[16] are with the\npurpose of facilitating human perception. The metrics in\nlow-light enhancement, e.g., the peak signal-to-noise ratio\n(PSNR) and the structural similarity (SSIM) are designed\nfor signal ﬁdelity, which is not fully aligned with that\nof high-level tasks. Therefore, their effectiveness in high-\nlevel tasks is unsatisfying, and there is still a large margin\nfor improvement [10], [17]. Another disadvantage that im-\npedes them from directly being deployed on UA Vs is the\ncomputation complexity. Due to the limited computational\nresource, algorithms with a high computational burden are\nunaffordable on UA Vs.\nIn visual tracking, the framework always consists of a\nbackbone network and a task head [4], [18]. The performance\nof object tracking is heavily dependent on the effective-\nness of feature extraction. Naturally, to carry on trackers’\nfavorable performance at daytime to low-light conditions, a\npromising solution is to narrow the gap between the features\narXiv:2303.10951v1  [cs.CV]  20 Mar 2023\nfrom low-light images and normal light ones. Therefore, we\nadopt a task-inspired perceptual loss. Adopting the tracking\nbackbone as a guideline, the objective of training a low-\nlight enhancer is to minimize the difference between features\nextracted from enhanced images and those from normal light\nimages. In this way, the low-light enhancer is trained to\nmeet the requirement of object tracking instead of human\nperception. Moreover, the proposed approach does not need\nany annotated night tracking sequences for training. Only a\nsmall set of paired low/normal light images is sufﬁcient to\nbring trackers to nighttime.\nRecent studies [19], [20] have shown the effectiveness of\nattention modules in image restoration tasks. Since low-light\nenhancement in this work devotes to facilitating the high-\nlevel perception task rather than solely pixel-level illumi-\nnance adjustment, the modeling of global information is rel-\natively more crucial. Due to the inherent locality of convolu-\ntion operations, convolutional neural networks (CNNs), how-\never, struggle to model long-range inter-dependencies [21].\nDrawing lessons from the robust performance of the global\nself-attention in long-range relation modeling [22]–[24], we\nconstruct our enhancer with a spatial-channel Transformer-\nbased attention module, dubbed SCT. To conserve local\ncontext in the meantime, the feed-forward network (FFN)\nof the standard Transformer is substituted by a residual\nconvolutional block. In contrast to image-to-image mapping,\nwe consider image enhancement as a non-linear curve projec-\ntion task following [13]. As we target real-world nighttime\nUA V applications, the inevitable noise frequently damages\nthe structural details of captured images. We consequently\ndesign a curve projection model with a noise term. An over-\nall performance comparison shown in Fig. 1 demonstrates\nthat SCT considerably assists nighttime tracking. Figure 2\nillustrates the workﬂow of the whole framework. Prior to the\nvisual tracking network, image enhancement is performed.\nThe proposed SCT ﬁrstly estimates illumination and noise\nparameter maps, and the robust curve projection formulation\nis then adopted to adjust the low-light template patch and\nsearch patches.\nMoreover, to facilitate the development of nighttime UA V\ntracking and provide a comprehensive evaluation, this work\nfurther collects and constructs a nighttime UA V tracking\nbenchmark, namely DarkTrack2021. Consisting of 110 well-\nannotated high deﬁnition sequences with over 100 K frames\nin total, DarkTrack2021 involves numerous objects and\nscenes.\nThe contributions of this paper lie in four-fold:\n• We propose a task-inspired low-light enhancer namely\nSCT. Trained in a task-tailored manner, SCT serves well\nin real-world nighttime UA V tracking.\n• We construct a low-light enhancer with a spatial-channel\nTransformer and a robust non-linear curve projection\nmodel to achieve favorable low-light image enhance-\nment.\n• We collect a nighttime UA V tracking benchmark,\nnamely DarkTrack2021, to provide comprehensive and\ndetailed evaluations for nighttime UA V tracking.\n• Evaluations on both the public UA VDark135 [9] and\nnewly constructed DarkTrack2021 benchmarks demon-\nstrate the effectiveness of SCT in facilitating nighttime\nUA V tracking. Real-world tests on a typical UA V plat-\nform further verify its efﬁciency and practicability.\nII. R ELATED WORK\nA. Low-Light Image Enhancement\nTowards meeting the requirement of human visual per-\nception, low-light image enhancement has attracted wide\nattention for a long time. C. Guo et al. [12] propose to\nestimate and reﬁne the illumination map for restoring the\ninput following the Retinex model [25]. To overcome the lack\nof sufﬁcient paired low/normal light data, Y . Jiang et al.[14]\ntrain a generative adversarial network (GAN) in an unsuper-\nvised manner. More recently, R. Liu et al.[16] introduce the\nneural architecture search into low-light enhancement to con-\nstruct a lightweight and effective structure. Reformulating the\nlow-light enhancement task as a non-linear curve projection\nproblem, DCE++ [13] estimates a parameter map from a low-\nlight input and adopts a curve projection model to illuminate\nlow-light images iteratively. However, such a curve projec-\ntion model focus on retouching illumination while neglecting\nthe noise that inevitably appears in real-world nighttime\nimaging. Despite the visually pleasing enhancement results,\nthe effectiveness of SOTA enhancers in facilitating nighttime\ntracking is still far from satisfying [10]. This can be partly\nattributed to the unalignment of their optimization goals with\nvisual tracking.\nB. Vision Transformers\nIn the past few years, Transformer [22] has shown ex-\ntraordinary performance in the ﬁeld of natural language\nprocessing (NLP). Due to the ability to effectively cap-\nture long-range inter-independencies, recent studies intro-\nduce Transfomer into computer vision tasks [7], [23], [24],\n[26]. However, Transformer for low-level tasks, e.g., image\nrestoration, is not well investigated so far. H. Chen et\nal. [27] pre-train a model comprised of several standard\nTransformer layers with multiple convolutional heads and\ntails to accomplish different tasks, while it suffers from\nhigh computational burn and large-scale training data. Z.\nWang et al. [28] construct a Transformer-based U-shaped\nmodel to realize several image restoration tasks, i.e., image\ndenoising, deraining, deblurring, and demoir´eing. In contrast,\nthis work introduces a Transformer-based spatial-channel\nattention module to model global inter-dependencies and\nexploit global context for effective low-light enhancement.\nC. Visual Perception at Nighttime\nVisual perception tasks at night have attracted a lot\nof attention. Several works [29]–[31] pertain to seman-\ntic segmentation focus on transferring daytime models to\nnighttime. In [32], a low-light enhancement model and\nan object detection model are merged to realize nighttime\nobject detection. However, object tracking at nighttime is\nnot well investigated relative to other computer vision tasks.\nFeature \nmaps\nI\nN\nC Channel \nTransformer \nlayer\nH×W Spatial \nTransformer \nlayer\nInput Loss\nSpatial-Channel Transformer-Based Low-Light Enhancer (SCT)\nTraining procedure\n Testing procedure UA V trackers\n…\n…\nBackbone\nTracker \nhead\nU AV\nU AV…\n…\nI Illumination curve map N Noise curve map\nP\nP Curve projection\nH\nW\nC\n…\n…\nFig. 2. Overview of our nighttime UA V tracking pipeline. The spatial-channel Transformer-based low-light enhancer (SCT) ﬁrstly estimates the illumination\nand noise curve maps from the input. Through the robust curve projection, low-light input is then illuminated. With a task-inspired perceptual loss, SCT is\ntrained to meet the requirement of the feature extractor in visual tracking. In the testing procedure, SCT is plugged prior to a tracking network to enhance\nthe template and searching patches, therefore bringing favorable gains for trackers.\nUnsatisfying tracking performance at nighttime still critically\nhinders the broadening of UA V applications up to the present.\nMotivated by the insight, B. Li et al. [9] incorporate a low-\nlight enhancer into a correlation ﬁlter-based tracker while is\nlack of ﬂexibility and failing to utilize robust deep features.\nJ. Ye et al. [10] attempt to boost nighttime tracking with\na Retinex-inspired enhancer, i.e., DarkLighter. Nevertheless,\nDarkLighter is designed intuitively and suffers from weak\ncollaboration with visual tracking. To this concern, this work\nproposes to learn an enhancer in a task-inspired manner, thus\nyielding promising effectiveness in nighttime tracking.\nIII. P ROPOSED METHOD\nThis work proposes a novel task-inspired low-light en-\nhancer to facilitate nighttime UA V tracking. As shown in\nFig. 2, the proposed SCT involves a Transformer-based\nspatial-channel attention module to estimate the illumination\nand noise curve maps of the input. A low-light image is\nthen enlightened through a robust non-linear projection. The\ntraining of SCT is guided by the feature extractor of object\ntracking to realize tracking-tailored low-light enhancement.\nIn the testing stage, SCT illuminates the template and search-\ning patches for trackers.\nA. Spatial-Channel Transformer-Based Low-Light Enhancer\nIn contrast to UNet [33] with pure convolutional layers,\nSCT adopts a U-shaped CNN-Transformer hybrid structure.\nThe introduction of Transformer-based spatial-channel at-\ntention enables SCT a better perception ability of global\ninformation. Given a low-light image Xi ∈ R3×Hi×Wi\nwith the resolution of Hi ×Wi pixels, K CNN encoders\nare utilized to generate the feature maps X ∈RC×H×W .\nEach encoder consists of two 3 ×3 convolutional layers\nwith a LeakyReLU activation function, followed by a down-\nsampling layer. After each encoder stage, the channels of\nfeature maps are doubled, and the spatial resolution is halved,\nexcept for the ﬁrst encoder that generates a feature map\nof 32 dimensions. Therefore, H = Hi\n2K , and W = Wi\n2K .\nTo achieve spatial-wise attention, we ﬂatten X in spatial\ndimension to Xs ∈R(H×W)×C and input it into the spatial\nTransformer layer. Xs can be regarded as H ×W feature\nvectors with a length of C. Each vector corresponds to a\nspeciﬁc spatial position of C feature channels. Next, the\nobtained feature maps are ﬂattened in the channel dimension,\nobtaining Xc ∈RC×(H×W). In this way, each feature vector\ninvolves the spatial information among all H ×W pixels\nof the corresponding channel. Through the spatial-channel\nattention module, the correlations among spatial positions\nand channels are captured. Following the attention module\nare K CNN decoders mirrored to the encoders. The last\nconvolutional layer is followed by a Tanh activation function,\nwhich outputs an illumination curve map I ∈ R3×Hi×Wi\nand a noise curve map N ∈ R3×Hi×Wi . Utilizing the\nproposed robust curve projection, the enlightened image\nXo ∈ R3×Hi×Wi is obtained. Below we introduce the\nTransformer layer and the robust curve projection in detail.\n1) Transformer layer: Since SCT serves as a preprocess-\ning step of nighttime tracking in resource-limited UA Vs, it is\nsupposed to be efﬁcient and effective. In consideration of the\nhigh computational burden of the vanilla Transformer [22],\nwe adopt the non-overlapping window-based multi-head self-\nattention (W-MSA) following [23], [24]. For instance, given\na ﬂattened feature F ∈RN×L, F is ﬁrst reshaped to 2D\nshape as F ∈RN×\n√\nL×\n√\nL, and then evenly partitioned to\nnon-overlapping windows with a size of M ×M. Next,\neach patch is ﬂattened and transposed to fj ∈RM2×N . The\nsubscript j denotes the j-th window. Therefore, F consists\nof L\nM2 patches:\nF =\n{\nf1, f2, ...,fj, ...,f L\nM2\n}\n. (1)\nFollowing Fig. 3 (a), self-attention is then performed on\neach patch. The computation of the Transformer layer can\nbe formulated as:\nˆF′=W-MSA(LN(F)) +F ,\nF′=MLP(LN(ˆF′)) +ˆF′ ,\n(2)\nwhere ˆF′and F′denote the output of W-MSA and the multi-\nlayer perception (MLP), respectively. LN is layer normaliza-\ntion. Inspired by [24], relative position bias B ∈RM2×M2\nis adopted into the self-attention. Therefore, the calculation\nof attention in each window can be represented as:\nAttention(Q, K, V) = SoftMax(QKT\n√\nd\n+ B)V , (3)\nLayer norm\nW-MSA\nLayer norm\nResFFN\nLinear transform\nVectors to maps\nConv\nDWConv\nMaps to vectors\nLinear transform\nLayer norm\nW-MSA\nLayer norm\nMLP\n(a) (b)\nFig. 3. Illustration of the original window-based Transformer layer (a)\nand our redesigned Transformer layer with ResFFN (b). The abbreviations\nMLP and W-MSA denote the multi-layer perception and the window-based\nmulti-head self-attention. Attributing to the introduction of ResFFN, local\ndetail information is enhanced.\nwhere Q, K, V ∈RM2×d represent the query, key, and value\nmatrixs. d is the dimension of the query.\nMoreover, previous studies [34], [35] show that standard\nTransformer presents drawbacks in leveraging local details,\nwhile local context is crucial to image enhancement tasks.\nHence, we replace MLP in the original Transformer layer\nwith a residual convolution feed-forward network (ResFFN),\naiming to make it up by convolution layers’ favorable\nlocal context perception ability. Figure 3 (b) illustrates the\nstructure of ResFFN, where the input gets through a linear\ntransform layer and is reshaped to a 2D shape, followed by\na residual operation involving a convolutional layer (Conv)\nand a depthwise convolutional layer (DWConv). The output\nis then generated by a ﬂattening operation and a linear\ntransform. In this way, the local context in the features is\npreserved and enhanced. Consequently, the attention opera-\ntion is reformulated as:\nˆF′= W-MSA(LN(F)) +F ,\nF′= ResFFN(LN(ˆF′)) +ˆF′ .\n(4)\n2) Robust curve projection: In [13], the low-light en-\nhancement task is reformulated as a non-linear curve pro-\njection problem. Estimating a light curve parameter map I\nfrom a low-light image Xi, the ﬁnal output of the enhanced\nimage Xo can be obtained following T iterative projections:\nXt\ni = Xt−1\ni + I ⊙Xt−1\ni ⊙(1 −Xt−1\ni ), t = 1, ..., T,\nXo =XT\ni , (5)\nwhere Xt\ni is the intermediate results of enhancing process,\nand X0\ni = Xi. ⊙denotes element-wise multiplication.\nHowever, in this curve projection model, noise is not taken\ninto consideration. To realize illuminance retouching for\nnormal pixels and denoising for noise pixels simultaneously,\nwe redesign the curve projection model and involve a noise\nterm:\nˆXt−1\ni = Xt−1\ni −N ,\nXt\ni = ˆXt−1\ni + I ⊙ˆXt−1\ni ⊙(1 −ˆXt−1\ni ), t = 1, ..., T,\nXo =XT\ni .\n(6)\nIn every iteration, the input Xt−1\ni ﬁrst subtracts the noise\nmap estimated by SCT to obtain ˆXt−1\ni for denoising, and\nthen undergoes the curve projection for illuminance retouch-\ning.\nRemark 1: The utilization of the noise term achieves dedi-\ncated denoise and makes the following curve projection focus\nmore on illuminance retouching, thus yielding a pleasant\nenhancement.\nB. Task-Inspired Training\nSince the purpose of low-light enhancement in this work\nis to promote UA V tracking, the proposed SCT enhancer is\nsupposed to meet the requirement of trackers. In light of\nthat feature extraction is essential for high-level tasks, this\nwork proposes to let the backbone Fin trackers guide the\ntraining process of low-light enhancement. Hence, a task-\ninspired loss Lis formulated as:\nL=\n∑\nm\n1\ncmhmwm\n∥Fm(Xo) −Fm(Y)∥2\n2 , (7)\nwhere cm, hm, wm represent corresponding dimensions of\nfeature maps from the m-th layer, and Y denotes the ground\ntruth normal light image. In our implementation, the widely\nadopted modiﬁed AlexNet [18] is employed as the loss\nbackbone F, for the reason that knowledge in a general\nbackbone can reﬂect the requirement of object tracking for\nfeatures and its relatively shallow structure ensures an easy\nconvergence. The 3rd, 4th, and 5th layers are used for\nloss calculation since trackers generally utilize features from\nthese layers. Thus, m = 3, 4, 5.\nRemark 2: Trained with this task-inspired loss only, the\nexperiments show that the learned SCT enhancer realizes\nfavorable gains in nighttime tracking compared to other\nSOTA low-light enhancers.\nC. SCT for Nighttime UAV Tracking\nAs shown in Fig. 2, SCT is plugged prior to track-\ning approaches. Template (or searching) patches are ﬁrstly\ncropped from newly captured low-light frames according\nto the initial position (or the estimated position of the\nlast frame). Before being fed into the feature extractor, the\ntemplate and searching patches are illuminated by SCT. Next,\nthe tracker head exploits features extracted by the backbone\nto calculate conﬁdence maps of the template patch and the\ncurrent searching patch, and update the position of the object.\nA visual comparison of conﬁdence maps generated from a\nbase tracker with SCT activated or not is illustrated in Fig. 4.\nIt can be seen clearly that with the enhancement of SCT\nto the template/searching patches, the base tracker yields\nsatisfying perception ability of objects in darkness.\nRemark 3: Through our manner, visual trackers are extended\nto the night without any need for nighttime tracking se-\nquences. Only a small set of paired low/normal light images\nis sufﬁcient for the training of SCT. Simply plug it in front of\nthe backbone network, SCT can shed light on the darkness\nand promote nighttime UA V tracking.\ntaxi_2\ncar_29\nFrames Base tracker Base tracker with SCT\nperson_14\nFig. 4. Visual comparison of conﬁdence maps generated from the\nbase tracker with SCT enabled or not. The red boxes in the ﬁrst column\nmark tracked objects. The images are from the proposed DarkTrack2021\nbenchmark, with sequence names displayed on the top left corner of the\noriginal frames. The base tracker loses the efﬁciency of locating objects in\nthe darkness, while SCT raises its perception ability favorably.\nIV. T HE DARK TRACK 2021 B ENCHMARK\nA nighttime UA V tracking benchmark, namely Dark-\nTrack2021, is constructed in this work for comprehensive\nevaluations. Compared with the existing benchmark [9] for\nnighttime UA V tracking in literature, scenes in the newly\ndeveloped benchmark are generally captured in complex\nurban from a higher altitude, where light conditions are more\ncluttered, frequently bringing severe illumination variation\nand overexposure/underexposure challenges. We hope the\ndeveloped DarkTrack2021 can promote the development of\nnighttime aerial tracking for the community.\nSequence collection All sequences are captured at nighttime\nin urban scenes by the authors, with utmost effort to design\nvarious challenging tracking scenarios. We adopt the DJI\nMavic Air 2 UA V1, with a frame rate of 30 frames/s (FPS).\nFinally, the benchmark comprises 110 challenging sequences\nwith 100,377 frames in total. The shortest, longest, and\nthe average length of sequences are respectively 92, 6579,\nand 913 frames. Figure 5 displays some ﬁrst frames of\nselected sequences. The objects in DarkTrack2021 contain\nperson, bus, car, truck, motor, dog, building, etc., covering\nabundant scenarios of real-world UA V nighttime tracking\ntasks. Large amounts of scenarios with various challenges,\nincluding viewpoint change, fast motion, large occlusion,\nlow resolution, low brightness, out-of-view, etc., are involved\nin DarkTrack2021. In that case, DarkTrack2021 provides\nextensive nighttime tracking performance evaluations.\nAnnotation Apart from sequence collection, delicate anno-\ntating is conducted for comprehensive evaluation. All frames\nin DarkTrack2021 are manually annotated by annotators\nfamiliar with object tracking, under the principle of tightly\nsurrounding objects’ edge pixels with bounding boxes. After\nthe ﬁrst annotating process, visual inspection by the authors\nand annotation reﬁnement by annotators are performed iter-\natively until all frames are annotated with high quality.\n1More information of the UA V can be found at https://www.dji.\ncom/cn/mavic-air-2.\n        \ncar_4\n          \nbus_2\n          \ncar_8\n          \nperson_7\n          \nskating_1\n        \nbuilding_1\n       \ntruck_12\n         motor_4\n        \ncar_38\n           \nbus_6\n        \nperson_17\n        \ndog_1\n          \nperson_24\n           \nperson_34\n           \ncar_6\n           \ncar_10\n        \ncar_36\n        \nperson_30\n           \nperson_1\n        \ntruck_2\nFig. 5. First frames of some selected sequences from DarkTrack2021.\nThe green boxes mark the tracking objects, while the top left corner of the\nimages displays sequence names. Low brightness makes it hard to identify\nobjects, which leads nighttime UA V tracking to an extremely challenging\ntask. Best viewed on-screen with high-resolution.\nV. E XPERIMENTS\nIn this section, a comprehensive ablation study is per-\nformed to verify components in SCT. A comparison of\nperformance in facilitating nighttime UA V tracking is then\nconducted among SCT and other SOTA low-light enhancers.\nTo testify the generalizability of SCT, it is further imple-\nmented on other SOTA trackers. Finally, real-world tests\nare conducted to verify the applicability of the proposed\napproach. The newly constructed DarkTrack2021 bench-\nmark, the source code of SCT, along with some demo\nvideos are made available at https://github.com/\nvision4robotics/SCT.\nA. Implementation Details\nThe total number K of CNN encoders/decoders is set\nto 4. Inspired by [13], the input images are ﬁrst scaled to\n128×128 to perform curve maps estimation for speeding\nup. There is respectively one Transformer layer for spatial\nattention and channel attention. The window size is 4 ×4.\nWe employ 485 paired low/normal light images from the\nLOL dataset [36] for training. The AdamW [37] is utilized\nas our optimizer. The initial learning rate is set to 0.0008,\nwith a weight decay of 0.02. The training is warmed up with\n5 epochs and lasts a total of 100 epochs. The batch size is\nset to 32. The training patch size is adopted as 256 ×256,\nwith a random crop operation on the original training image.\nExperiments are conducted on a PC with an Intel i9-9920X\nCPU, an NVIDIA TITAN RTX GPU, and a 32GB RAM.\nFinally, an NVIDIA Jetson AGX Xavier serves as the real-\nworld test platform to testify the practicability of SCT in\nnighttime UA V tracking.\nB. Evaluation Metrics\nSince we target nighttime UA V tracking, evaluation met-\nrics in visual tracking are adopted to rank the performance\ninstead of those in low-light enhancement. The experiments\nfollow the one-pass evaluation (OPE) [38], which involves\ntwo metrics, respectively precision and success rate. The\ncenter location error (CLE) of the predicted position and the\nground truth position of the object is utilized to calculate the\nprecision. The percentage of the frames with a CLE below\na given threshold is presented as the precision plot (PP).\nAs generally adopted, the threshold of 20 pixels is used to\nrank trackers. In addition, the success rate is measured by\nthe intersection over union (IoU) of the estimated bounding\nbox and the ground truth one. The percentage of the frames\nwhose IoU is greater than a preset maximum threshold makes\nup the success plot (SP). In general, the area-under-the-curve\n(AUC) on SP is used to rank the success rate of the trackers.\nC. Ablation Study\nThis subsection investigates the performance of different\nvariants of SCT. A typical Siamese tracker—SiamRPN++ [4]\nwith the AlexNet backbone, is utilized as the baseline. All\nvariants are trained in the same way. Tracking results on\nUA VDark135 [9] are reported in TABLE I. SA, CA, and\ndenoise denote the spatial attention, the channel attention,\nand the noise term, respectively. The bottom row indicates\nthe original baseline without enhancement, which shows an\nunsatisfying performance, while the introduction of complete\nSCT brings gains of 13.3% and 15.4% in success rate and\nprecision, respectively.\n1) Attention module: Ablating CA and SA respectively,\nthe gains of SCT bringing to tracking performance degraded\nto some extend, validating the effectiveness of both the\nspatial attention and the channel attention. As shown in\nthe fourth line of TABLE I, replacing the spatial-channel\nTransformer with a general CNN bottleneck, the pure CNN\nUNet enhancer brings an increase of 10.9% in success rate\nand 12.4% in precision, which is ∼2.5% lower than that the\ncomplete SCT brought. One can conclude that the proposed\nspatial-channel Transformer-based structure performs better\nthan the original CNN UNet.\n2) ResFF: As shown in the ﬁfth line of TABLE I,\nadopting MLP rather than our proposed ResFF as FFN,\nthe baseline only gains a performance of 8.0% in success\nrate and 8.5% in precision. SCT with ResFF surpasses that\nwith MLP in a large margin. The effectiveness of ResFF in\nTABLE I\nCOMPARISON OF SCT WITH DIFFERENT MODULES ENABLED ON\nUAVDARK 135. T HE BEST RESULTS ARE HIGHLIGHTED IN RED FONT .\n∆ REPRESENTS THE PERCENTAGES EXCEEDING THE BASELINE .\nSA CA ResFF Denoise Succ./Prec. ∆ (%)\n✓ ✓ ✓ ✓ 0.421/0.547 +13.3/+15.4\n✓ ✓ ✓ 0.415/0.536 +11.7/+13.0\n✓ ✓ ✓ 0.409/0.523 +10.1/+10.3\n✓ 0.412/0.533 +10.9/+12.4\n✓ ✓ ✓ 0.401/0.514 +8.0/+8.5\n✓ ✓ ✓ 0.393/0.505 +5.6/+6.6\n0.372/0.474 -/-\n0.372 0.474\nBaseline\nFig. 6. Tracking performance comparison with different low-light\nenhancers implemented. The bottom red line denotes baseline without any\nenhancement. The proposed SCT raises the baseline by a large margin.\npreserving and enhancing local context can be veriﬁed, which\nis crucial for both low-light enhancement and tracking.\n3) Noise term: Removing the noise term of the robust\ncurve projection model, the beneﬁt of introducing SCT is\nhalved, which shows the importance of appropriate denoising\nin real-world nighttime UA V tracking.\nRemark 4: Deactivating the key modules in SCT, the learned\nenhancer can still promote nighttime tracking in certain,\nwhich is attributed to the task-inspired training.\nD. Comparison with SOTA Low-Light Enhancers\nTo present the advantage of SCT in facilitating nighttime\nUA V tracking, it is further compared with other 6 SOTA low-\nlight enhancers, including DCE++ [13], EnlightenGAN [14],\nLIME [12], LLVE [15], RUAS [16], and DarkLighter [10].\nThe tracking performances of the baseline with different low-\nlight enhancement approaches enabled on the UA VDark135\nbenchmark are displayed in Fig. 6. The results demonstrate\nthat the task-speciﬁc design makes SCT perform surprisingly\nin facilitating nighttime tracking, while other general low-\nlight enhancers are at a similar suboptimal level. The pro-\nposed approach raises the baseline by over 13% and 15%\nin success rate and precision, respectively, far surpassing the\nsecond-best DCE++, which gains a promotion of around 8%.\nThough DarkLighter is designed with the motivation of UA V\ntracking, the beneﬁts it brings are inferior due to the weak\ncollaboration with the tracking task.\nE. SCT in SOTA Trackers\nTo testify the performance gain of SCT in different track-\ners, the proposed approach is further implemented in 6 SOTA\ntrackers, including AlexNet-based trackers (SiamRPN++ [4],\nSiamAPN++ [11], and HiFT [7]), ResNet18-based tracker\n(DiMP18 [5]), and ResNet50-based trackers (DiMP50 [5]\nand PrDiMP50 [6]). Figure 7 shows the overall performance\nof trackers on UA VDark135 and our newly constructed\nDarkTrack2021 with SCT enabled or not. The performance\ngains of each tracker are reported in the legend. The results\nshow that SCT signiﬁcantly boosts the nighttime tracking\nperformance of all involved trackers on both benchmarks. For\ninstance, in UA VDark135, with the facilitating of SCT, HiFT\nboosts 19.69% and 16.59% in precision and success rate,\nrespectively. In DarkTrack2021, SCT helps DiMP50 earn\n+7.20%\n+9.61%\n+4.53%\n+19.00%\n+15.37%\n+19.69%\n+5.62%\n+9.71%\n+4.15%\n+13.32%\n+13.33%\n+16.59%\n(a) Results on UA VDark135.\n+5.84%\n+8.64%\n+4.63%\n+9.66%\n+15.04%\n+13.81%\n+4.80%\n+10.36%\n+4.12%\n+8.22%\n+12.17%\n+14.29% (b) Results on DarkTrack2021.\nFig. 7. Overall performance of SOTA trackers with SCT activated (plots with deep colors) or not (plots with light colors). Trackers underlined in the\nlegend denote tracking performance with the activation of the proposed SCT, while the percentages report the performance gains brought by SCT. The\nresults show the proposed enhancement module considerably promotes the performance of all involved trackers.\n#00001 #00018 #00206 #00440\n000500\n#00001 #00395 #00500 #00871\n#00001 #00256 #00371 #00455\nDiMP50 DiMP18 PrDiMP50 SiamRPN++ HiFT SiamAPN++\nBase tracker Base tracker    SCT Target object\nFig. 8. Qualitative results of trackers with SCT activated (solid boxes)\nor not (dashed boxes). The images are globally enhanced by SCT for\nvisualization except for the ﬁrst column. From top to down, the sequences\nare person18, car23, and person24 from the newly constructed Dark-\nTrack2021. SCT boosts the nighttime tracking performance of trackers\nsigniﬁcantly.\npromotions of 5.84% and 4.80%, reaching 0.681 and 0.518 in\nprecision and success rate, respectively. In addition, we found\nthat the gains of SCT brought partly depend on the depth\nof backbones. The shallow ones earn more ( >13% in most\nAlexNet-based trackers), while the deep ones earn less (∼5%\nin ResNet50-based trackers). We conjecture that shallow\nbackbones lose feature extraction efﬁciency more severely\nat night, thus beneﬁting more from low-light enhancement.\nIn Fig. 8, some tracking screenshots are presented. Images\nare globally enhanced by SCT except for the ﬁrst column for\nvisualization, while only the template and search patches are\nenhanced in the practical tracking pipeline. We can conclude\nthat SCT beneﬁts the perception ability of trackers at night,\nthus raising tracking reliability considerably.\nRemark 5: Apart from favorable nighttime tracking perfor-\nmance gains, SCT also shows competitive image enhance-\nment results.\nF . Real-World Tests\nTo demonstrate the applicability of SCT in real-world\nnighttime UA V tracking applications, we further adopt it on a\ntypical embedded system, i.e., NVIDIA Jetson AGX Xavier.\n30 60 90 120 150 180 210 240 270 300 330 360\n \n0\n10\n20CLE\n60 120 180 240 300 360 420 480 540 600 660 720 780\n \n0\n10\n20CLE\nU AV\nTest 1 Test 2 Test 3\nTest 1\nTest 2\n25 50 75 100 125 150 175 200 225 250 275 300 325\nFrame (#)\n0\n10\n20CLE\nTest 3\nFig. 9. Real-world tests on a typical UA V platform. Red bounding boxes\ndenote the estimated positions. CLE curves between predictions and ground\ntruth are drawn below. The green dashed line locates a threshold of 20 pixels,\ntracking errors within which are normally regarded as satisfying. The base\ntracker realizes favorable nighttime tracking assisted by SCT.\nThe results demonstrate that SCT realizes a promising real-\ntime speed of ∼31.25 FPS without TensorRT acceleration.\nFurther, Fig. 9 shows some real-world nighttime tracking\ntests and CLE curves. The main challenges in the tests are\nlow brightness, illumination variation, and fast motion. The\nCLE curves show that the prediction errors are within 20\npixels, which can be regarded as reliable tracking. With the\nassistance of SCT, the base tracker is able to effectively\nextract discriminative features and realize robust object lo-\ncalization at night.\nVI. C ONCLUSION\nThis work proposes a task-inspired low-light enhance-\nment approach, namely SCT. Adopting a novel spatial-\nchannel Transformer module and a robust curve projection\nmodel, the proposed approach puts the requirement of visual\ntracking into great consideration, thus yielding a tracking-\ntailored enhancement performance. Working in a plug-and-\nplay manner, SCT can be utilized as a uniﬁed method to\nboost nighttime UA V tracking. Moreover, a well-annotated\nnighttime UA V tracking benchmark is constructed for com-\nprehensive tracking performance evaluation. The promising\nperformance of SCT in benchmarks and real-world tests\nveriﬁes its effectiveness and practicability, with a promising\nreal-time speed. To sum up, we strongly believe that both\nthe proposed tracking-customized low-light enhancer and the\nconstructed nighttime tracking benchmark will considerably\ncontribute to nighttime UA V tracking and expand related\nUA V applications at night.\nACKNOWLEDGMENT\nThis work was supported in part by the Natural Science\nFoundation of Shanghai under Grant 20ZR1460100 and in\npart by the National Natural Science Foundation of China\nunder Grant 62173249.\nREFERENCES\n[1] J. Li, H. Xie, K. H. Low, J. Yong, and B. Li, “Image-Based Visual\nServoing of Rotorcrafts to Planar Visual Targets of Arbitrary Orienta-\ntion,” IEEE Robot. Automat. Lett., vol. 6, no. 4, pp. 7861–7868, 2021.\n[2] J. Gonz ´alez-Trejo, D. Mercado-Ravell, I. Becerra, and R. Murrieta-\nCid, “On the Visual-Based Safe Landing of UA Vs in Populated Areas:\nA Crucial Aspect for Urban Deployment,” IEEE Robot. Automat. Lett.,\nvol. 6, no. 4, pp. 7901–7908, 2021.\n[3] J. Ye, C. Fu, F. Lin, F. Ding, S. An, and G. Lu, “Multi-regularized\ncorrelation ﬁlter for uav tracking and self-localization,” IEEE Trans.\nInd. Electron., vol. 69, no. 6, pp. 6004–6014, 2022.\n[4] B. Li, W. Wu, Q. Wang, F. Zhang, J. Xing, and J. Yan, “SiamRPN++:\nEvolution of Siamese Visual Tracking with Very Deep Networks,”\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2019, pp.\n4277–4286.\n[5] G. Bhat, M. Danelljan, L. Van Gool, and R. Timofte, “Learning\nDiscriminative Model Prediction for Tracking,” in Proc. IEEE/CVF\nInt. Conf. Comput. Vis., 2019, pp. 6181–6190.\n[6] M. Danelljan, L. Van Gool, and R. Timofte, “Probabilistic Regression\nfor Visual Tracking,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., 2020, pp. 7181–7190.\n[7] Z. Cao, C. Fu, J. Ye, B. Li, and Y . Li, “HiFT: Hierarchical Feature\nTransformer for Aerial Tracking,” in Proc. IEEE/CVF Int. Conf.\nComput. Vis., 2021, pp. 15 457–15 466.\n[8] X. Chen, B. Yan, J. Zhu, D. Wang, X. Yang, and H. Lu, “Transformer\nTracking,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\n2021, pp. 8122–8131.\n[9] B. Li, C. Fu, F. Ding, J. Ye, and F. Lin, “All-Day Object Tracking\nfor Unmanned Aerial Vehicle,” arXiv preprint arXiv:2101.08446, pp.\n1–13, 2021.\n[10] J. Ye, C. Fu, G. Zheng, Z. Cao, and B. Li, “DarkLighter: Light Up\nthe Darkness for UA V Tracking,” in Proc. IEEE/RSJ Int. Conf. Intell.\nRobots Syst., 2021, pp. 3079–3085.\n[11] Z. Cao, C. Fu, J. Ye, B. Li, and Y . Li, “SiamAPN++: Siamese\nAttentional Aggregation Network for Real-Time UA V Tracking,” in\nProc. IEEE/RSJ Int. Conf. Intell. Robots Syst., 2021, pp. 3086–3092.\n[12] X. Guo, Y . Li, and H. Ling, “LIME: Low-Light Image Enhance-\nment via Illumination Map Estimation,” IEEE Trans. Image Process.,\nvol. 26, no. 2, pp. 982–993, 2017.\n[13] C. Li, C. Guo, and C. L. Chen, “Learning to Enhance Low-Light Image\nvia Zero-Reference Deep Curve Estimation,” IEEE Trans. Pattern\nAnal. Mach. Intell., pp. 1–14, 2021.\n[14] Y . Jiang, X. Gong, D. Liu, Y . Cheng, C. Fang, X. Shen, J. Yang,\nP. Zhou, and Z. Wang, “EnlightenGAN: Deep Light Enhancement\nWithout Paired Supervision,” IEEE Trans. Image Process., vol. 30,\npp. 2340–2349, 2021.\n[15] F. Zhang, Y . Li, S. You, and Y . Fu, “Learning Temporal Consistency\nfor Low Light Video Enhancement from Single Images,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 4965–\n4974.\n[16] R. Liu, L. Ma, J. Zhang, X. Fan, and Z. Luo, “Retinex-inspired\nUnrolling with Cooperative Prior Architecture Search for Low-light\nImage Enhancement,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern\nRecognit., 2021, pp. 10 556–10 565.\n[17] J. Liang, J. Wang, Y . Quan, T. Chen, J. Liu, H. Ling, and Y . Xu,\n“Recurrent Exposure Generation for Low-Light Face Detection,”IEEE\nTrans. Multimedia, pp. 1–14, 2021.\n[18] L. Bertinetto, J. Valmadre, J. F. Henriques, V . Andrea, and P. H. S.\nTorr, “Fully-Convolutional Siamese Networks for Object Tracking,” in\nProc. Eur. Conf. Comput. Vis. Workshop, 2016, pp. 850–865.\n[19] K. Jiang, Z. Wang, P. Yi, C. Chen, B. Huang, Y . Luo, J. Ma, and\nJ. Jiang, “Multi-Scale Progressive Fusion Network for Single Image\nDeraining,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\n2020, pp. 8343–8352.\n[20] S. W. Zamir, A. Arora, S. Khan, M. Hayat, F. S. Khan, M.-H. Yang,\nand L. Shao, “Multi-Stage Progressive Image Restoration,” in Proc.\nIEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp. 14 816–\n14 826.\n[21] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local Neural\nNetworks,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit.,\n2018, pp. 7794–7803.\n[22] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention Is All You Need,” in\nProc. Adv. Neural Inf. Process. Syst., 2017, pp. 6000–6010.\n[23] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai\net al., “An Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale,” in Proc. Int. Conf. Learn. Representations,\n2021, pp. 1–12.\n[24] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo,\n“Swin Transformer: Hierarchical Vision Transformer using Shifted\nWindows,” in Proc. IEEE/CVF Int. Conf. Comput. Vis., 2021, pp.\n10 012–10 022.\n[25] E. H. Land, “The Retinex Theory of Color Vision,” Sci. Amer., vol.\n237, no. 6, pp. 108–129, 1977.\n[26] S. Zheng, J. Lu, H. Zhao, X. Zhu, Z. Luo, Y . Wang, Y . Fu, J. Feng,\nT. Xiang, P. H. Torr, and L. Zhang, “Rethinking Semantic Segmen-\ntation from a Sequence-to-Sequence Perspective with Transformers,”\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp.\n6877–6886.\n[27] H. Chen, Y . Wang, T. Guo, C. Xu, Y . Deng, Z. Liu, S. Ma, C. Xu,\nC. Xu, and W. Gao, “Pre-Trained Image Processing Transformer,”\nin Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit., 2021, pp.\n12 294–12 305.\n[28] Z. Wang, X. Cun, J. Bao, J. Liu, and H. Li, “Uformer: A Gen-\neral U-Shaped Transformer for Image Restoration,” arXiv preprint\narXiv:2106.03106, pp. 1–11, 2021.\n[29] C. Sakaridis, D. Dai, and L. Van Gool, “Guided Curriculum Model\nAdaptation and Uncertainty-Aware Evaluation for Semantic Nighttime\nImage Segmentation,” in Proc. IEEE/CVF Int. Conf. Comput. Vis.,\n2019, pp. 7373–7382.\n[30] X. Wu, Z. Wu, H. Guo, L. Ju, and S. Wang, “DANNet: A One-Stage\nDomain Adaptation Network for Unsupervised Nighttime Semantic\nSegmentation,” in Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recog-\nnit., 2021, pp. 15 764–15 773.\n[31] C. Sakaridis, D. Dai, and L. Van Gool, “Map-Guided Curriculum\nDomain Adaptation and Uncertainty-Aware Evaluation for Semantic\nNighttime Image Segmentation,” IEEE Trans. Pattern Anal. Mach.\nIntell., pp. 1–15, 2020.\n[32] Y . Sasagawa and H. Nagahara, “YOLO in the Dark - Domain\nAdaptation Method for Merging Multiple Models,” in Proc. Eur. Conf.\nComput. Vis., 2020, pp. 345–359.\n[33] O. Ronneberger, P. Fischer, and T. Brox, “U-Net: Convolutional\nNetworks for Biomedical Image Segmentation,” in Proc. Int. Conf.\nMed. Image Comput. Comput.-Assist. Interv., 2015, pp. 234–241.\n[34] Y . Li, K. Zhang, J. Cao, R. Timofte, and L. V . Gool, “Lo-\ncalViT: Bringing Locality to Vision Transformers,” arXiv preprint\narXiv:2104.05707, pp. 1–10, 2021.\n[35] H. Wu, B. Xiao, N. Codella, M. Liu, X. Dai, L. Yuan, and L. Zhang,\n“CvT: Introducing Convolutions to Vision Transformers,” in Proc.\nIEEE/CVF Int. Conf. Comput. Vis., 2021, pp. 22–31.\n[36] C. Wei, W. Wang, W. Yang, and J. Liu, “Deep Retinex Decomposition\nfor Low-Light Enhancement,” in Proc. Brit. Mach. Vis. Conf., 2018,\npp. 1–12.\n[37] I. Loshchilov and F. Hutter, “Decoupled Weight Decay Regulariza-\ntion,” in Proc. Int. Conf. Learn. Representations, 2019, pp. 1–11.\n[38] M. Mueller, N. Smith, and B. Ghanem, “A Benchmark and Simulator\nfor UA V Tracking,” inProc. Eur. Conf. Comput. Vis., 2016, pp. 445–\n461.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7956260442733765
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.6866159439086914
    },
    {
      "name": "Artificial intelligence",
      "score": 0.594252347946167
    },
    {
      "name": "Computer vision",
      "score": 0.5401486754417419
    },
    {
      "name": "Real-time computing",
      "score": 0.44110727310180664
    },
    {
      "name": "Transformer",
      "score": 0.42848652601242065
    },
    {
      "name": "Task (project management)",
      "score": 0.4260692596435547
    },
    {
      "name": "Code (set theory)",
      "score": 0.4162091314792633
    },
    {
      "name": "Context (archaeology)",
      "score": 0.41577285528182983
    },
    {
      "name": "Engineering",
      "score": 0.11969530582427979
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I116953780",
      "name": "Tongji University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210103986",
      "name": "Jingdong (China)",
      "country": "CN"
    }
  ]
}