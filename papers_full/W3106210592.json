{
  "title": "Improve Transformer Models with Better Relative Position Embeddings",
  "url": "https://openalex.org/W3106210592",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2113160865",
      "name": "Zhiheng Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2782013732",
      "name": "Davis Liang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1982578470",
      "name": "Peng Xu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2128398165",
      "name": "Bing Xiang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3016473712",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W854541894",
    "https://openalex.org/W3105163367",
    "https://openalex.org/W2613904329",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2891815651"
  ],
  "abstract": "The transformer model has demonstrated superior results on NLP tasks including machine translation and question answering. In this paper, we argue that the position information is not fully utilized in existing work. For example, the initial proposal of a sinusoid embedding is fixed and not learnable. In this paper, we first review the absolute position embeddings and existing relative position embedding methods. We then propose new methods to encourage increased interaction between query, key and relative position embeddings in the self-attention mechanism. Our most promising approach is a generalization of the absolute position embedding. Our method results in increased accuracy compared to previous approaches in absolute and relative position embeddings on the SQuAD1.1 dataset. In addition, we address the inductive property of whether a position embedding can be robust enough to handle long sequences. We demonstrate empirically that our relative embedding method can be reasonably generalized to and is robust in the inductive perspective. Finally, we show that our proposed method can be effectively and efficiently adopted as a near drop-in replacement for improving the accuracy of large models with little computational overhead.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3327–3335\nNovember 16 - 20, 2020.c⃝2020 Association for Computational Linguistics\n3327\nImprove Transformer Models with Better Relative Position Embeddings\nZhiheng Huang\nAWS AI\nzhiheng@amazon.com\nDavis Liang\nAWS AI\nliadavis@amazon.com\nPeng Xu\nAWS AI\npengx@amazon.com\nBing Xiang\nAWS AI\nbxiang@amazon.com\nAbstract\nTransformer architectures rely on explicit po-\nsition encodings in order to preserve a notion\nof word order. In this paper, we argue that ex-\nisting work does not fully utilize position in-\nformation. For example, the initial proposal\nof a sinusoid embedding is ﬁxed and not learn-\nable. In this paper, we ﬁrst review absolute\nposition embeddings and existing methods for\nrelative position embeddings. We then pro-\npose new techniques that encourage increased\ninteraction between query, key and relative po-\nsition embeddings in the self-attention mecha-\nnism. Our most promising approach is a gen-\neralization of the absolute position embedding,\nimproving results on SQuAD1.1 compared to\nprevious position embeddings approaches. In\naddition, we address the inductive property of\nwhether a position embedding can be robust\nenough to handle long sequences. We demon-\nstrate empirically that our relative position em-\nbedding method is reasonably generalized and\nrobust from the inductive perspective. Finally,\nwe show that our proposed method can be\nadopted as a near drop-in replacement for im-\nproving the accuracy of large models with a\nsmall computational budget.\n1 Introduction\nThe introduction of BERT (Devlin et al., 2018)\nhas lead to new state-of-the-art results on various\ndownstream tasks such as question answering and\npassage ranking. Variations of BERT, including\nRoBERTa (Liu et al., 2019b), XLNet (Yang et al.,\n2019), ALBERT (Lan et al., 2019) and T5 (Raffel\net al., 2019) have been proposed. At its core, BERT\nis non-recurrent and based on self-attention; in or-\nder to model the dependency between elements at\ndifferent positions in the sequence, BERT relies on\nposition embeddings. With BERT, the input em-\nbeddings are the sum of the token embeddings, seg-\nment embeddings, and position embeddings. The\nposition embedding encodes the absolute positions\nfrom 1 to maximum sequence length (usually 512).\nThat is, each position has a learnable embedding\nvector. The absolute position embedding is used\nto model how a token at one position attends to\nanother token at a different position.\nRecent work suggested removing the next sen-\ntence prediction (NSP) loss with training conducted\nsolely on individual chunks of text (Liu et al.,\n2019a). In this setup, the notion of absolute po-\nsitions can be arbitrary depending on chunk start\npositions. Therefore, the association of a token to\nan absolute position is not well justiﬁed. Indeed,\nwhat really matters is the relative position or dis-\ntance between two tokens ti and tj, which is j−i.\nThis phenomena has been realized and the relative\nposition representation has been proposed in Shaw\net al. (2018); Huang et al. (2018), in the context\nof encoder decoder machine translation and mu-\nsic generation respectively. Shaw et al. (2018) has\nbeen modiﬁed in transformer-XL (Dai et al., 2019)\nand adopted in XLNet (Yang et al., 2019). The rel-\native position embedding in (Shaw et al., 2018) has\nbeen proven to be effective and thus it is adopted\nin (Raffel et al., 2019; Song et al., 2020).\nIn this paper, we review the absolute position\nembedding from Devlin et al. (2018) and the rela-\ntive position embeddings in Shaw et al. (2018); Dai\net al. (2019). Our contributions are as follows.\n1. We argue that the relative position is not fully\nutilized in the existing work. We propose a\nnumber of relative position embeddings in this\npaper in order to encourage increased inter-\naction between the key, query, and position\nembeddings. We show that our proposed em-\nbeddings can outperform the widely used rel-\native position embedding (Shaw et al., 2018)\non SQuAD1.1.\n2. We discuss the inductive property: can BERT,\ntrained on short sequences, generalize to\nlonger sequences from the perspective of po-\nsition embeddings? We conduct ablation stud-\nies to show how the clipping value k(used to\nlimit the relative distance) affects the model\n3328\naccuracy. We demonstrate empirically that\nour relative embedding method is robust with\nrespect to this inductive property.\n3. We show that our novel position embedding\ntechnique can improve BERT-large perfor-\nmance with only a few epochs of ﬁne-tuning.\nAcquiring large gains with a small computa-\ntion budget.\n2 Related Work\nPreviously, Vaswani et al. (2017) introduced a po-\nsition embeddings with dimensions matching the\ntoken embeddings (so that they can be summed).\nSpeciﬁcally, they choose the sine and cosine func-\ntions at different frequencies:\nPE(pos,2i) = sin(pos/100002i/dmodel) (1)\nPE(pos,2i+1) = cos(pos/100002i/dmodel) (2)\nwhere posis the position and iis the embedding\ndimension. That is, each dimension of the position\nencoding corresponds to a sinusoid. The authors\nhypothesized that it would allow the model to easily\nlearn to attend via relative positions, since for any\nﬁxed offset k, PEpos+k can be represented as a\nlinear function of PEpos. They also experimented\nwith learned position embeddings (Gehring et al.,\n2017) and found that the two versions produced\nnearly identical results. BERT (Devlin et al., 2018)\nuses a learnable position embedding.\nPrevious work (Parikh et al., 2016) has intro-\nduced attention weights based on relative distance\nprior to BERT (Devlin et al., 2018). More recently,\nShaw et al. (2018) demonstrated the importance of\nrelative position representations. They presented\nan efﬁcient way of incorporating relative position\nrepresentations into the transformer self-attention\nlayer. They achieved signiﬁcant improvements in\ntranslation quality on two machine translation tasks.\nHuang et al. (2018) has proposed a similar idea\nto incorporate the relative distance explicitly but\nin the music generation domain. Transformer-XL\n(Dai et al., 2019) has modiﬁed (Shaw et al., 2018)\nto have the following two differences: 1) to intro-\nduce additional bias terms for queries; and 2) to re-\nintroduce the use of a sinusoid formulation, in the\nhope that a model trained on a memory of a certain\nlength can automatically generalize to a memory\nseveral times longer during evaluation1. The pro-\nposed relative position embedding has been used\nin transformer-XL (Dai et al., 2019) and XLNet\n(Yang et al., 2019). The relative position embed-\nding by Shaw et al. (2018) is proven to be effective\n1This was not rigorously veriﬁed in experiments.\nand it is validated in BERT variants model training\n(Raffel et al., 2019; Song et al., 2020).\nIn addition to the above work, Chorowski\net al. (2015) proposed a novel method of adding\nlocation-awareness to the attention mechanism in\nthe sequence to sequence framework for automatic\nspeech recognition (ASR). Their work is related to\nthis paper as both attempt to integrate a location\ninformation into the self-attention mechanism.\n3 Position Embeddings\nIn this section, we review the absolute position\nembedding used in the original BERT paper and\nthe relative position embedding proposed in (Shaw\net al., 2018; Dai et al., 2019). We then propose\na number of relative position embeddings, from\nsimpler ones to more complex ones. We analyze\nthe complexity of each embedding method.\n3.1 Self-Attention review\nThe BERT model consists of a transformer encoder\n(Vaswani et al., 2017) as shown in Figure 1.\nFigure 1: Transformer architectures with the original\nabsolute position embedding (left) and all other varia-\ntions of relative position embeddings (right).\nThe original transformer architecture uses mul-\ntiple stacked self-attention layers and point-wise\nfully connected layers for both the encoder and\ndecoder. Each self-attention sublayer consists of\nh attention heads. The result from each head\nare concatenated to form the sublayer’s output.\nEach attention head operates on an input sequence,\nx= (x1,...,x n) of nelements (maximum num-\nber of tokens allowed in model training,nis usually\n512 in default) where xi ∈Rdx, and computes a\nnew sequence z= (z1,...,z n) of the same length\nwhere zi ∈Rdz . Each output element, zi, is com-\nputed as weighted sum of linearly transformed in-\nput elements:\nzi =\nn∑\nj=1\nαij(xjWV ), (3)\n3329\nwhere αij is the weight which is computed by ap-\nplying a softmax function:\nαij = exp eij∑n\nk=1 exp eik\n, (4)\nwhere eij is the attention weight from position jto\ni, a scaled dotted product following a linear trans-\nformation:\neij = (xiWQ)(xjWK)T\n√dz\n. (5)\nThe scaling factor √dz is necessary to make the\ntraining stable. The dot product is chosen due to\nits simplicity and computational efﬁciency. Linear\ntransformation of the inputs add sufﬁcient expres-\nsive power. WQ, WK, WV ∈Rdx×dz are parame-\nter matrices. These parameter matrices are unique\nper layer and attention head.\n3.2 Absolute position embedding in BERT\nIn the self-attention scheme, the absolute position\nembedding is as follows.\nxi = ti + si + wi, (6)\nwhere xi, i ∈ {0,...,n −1}is the input em-\nbedding to the ﬁrst transformer layer, ti, si and\nwi ∈Rdx are the token embeddings, segment em-\nbeddings and absolute position embeddings respec-\ntively. Segment embedding indicates if a token is\nsentence Aor sentence B, which was originally in-\ntroduced in BERT (Devlin et al., 2018) to compute\nthe next sentence prediction (NSP) loss. Later work\n(Yang et al., 2019; Liu et al., 2019a; Raffel et al.,\n2019) suggested that the NSP loss does not help\nimprove accuracy. We therefore drop the segment\nembedding in this paper. Token embeddings ti and\nabsolute position embeddings wi, are learnable pa-\nrameters trained to maximize the log-likelihood of\nthe MLM task. Figure 2 depicts the absolute posi-\ntion embedding graphically, which is used in the\nﬁrst layer in Figure 1 left. The maximum length\nof a sequence nis required to be determined be-\nfore the training. Although it lacks the inductive\nproperty, this approach is found to be effective for\nmany NLP tasks, due to the fact that the maximum\nsequence length is enforced at inference anyway in\nmost cases.\n3.3 Shaw’s relative position embedding\nThe work of (Shaw et al., 2018) proposed the edge\nrepresentations, aij ∈Rdz , which is used to model\nhow much token ti attends to token tj. The equa-\ntion (5) can be revised as follows to consider the\nFigure 2: Absolute position embedding pi.\ndistance between token iand jin computing their\nattention.\neij = (xiWQ)(xjWK + aij)T\n√dz\n. (7)\nThey also introduced clipped value kwhich is\nthe maximum relative position distance allowed.\nThe authors hypothesized that the precise relative\nposition information is not useful beyond a certain\ndistance. Therefore, there are 2k+ 1unique edge\nlabels w = (w−k,...,w k) deﬁned as the follow-\ning.\naij = wclip(j−i,k) (8)\nclip(x,k) = max( −k,min(k,x)) (9)\nFigure 3 shows the edge representations aij graphi-\ncally, with k= 3.\nFigure 3: Relative position weights aij .\n3.4 XLNet’s relative position embedding\nTransformer-XL (Dai et al., 2019) and XLNet\n(Yang et al., 2019) also utilize the relative position\nembedding, with the equation (5) being revised as\nfollows\neij = (xiWQ + u)(xjWK)T + (xiWQ + v)(RijWR)T\n√dz\n,\n(10)\nwhere WR is a learnable parameter matrix and Rij\nis the sinusoid encoding vector between location i\nand j. Ris a sinusoid encoding matrix (Vaswani\net al., 2017) without learnable parameters, which\nessentially reﬂects the prior that only the relative\n3330\ndistance matters for where to attend. u∈Rdz and\nv ∈Rdz are trainable parameters to represent the\nquery bias for content-based (the ﬁrst term in nu-\nmerator) and location-based (the second term in\nnumerator) attentions respectively. The relative po-\nsition embedding deﬁned in equation (10) is similar\nto the work of Shaw et al. (2018) but with two dif-\nferences: 1) it introduces additional bias terms for\nqueries; and 2) it uses the sinusoid formulation pro-\nposed in the original transformer paper (Vaswani\net al., 2017).\nWe implemented this but found that the bias\nterms led to training instability. After removing the\nbias terms, keeping only the sinusoids, we found\nthat the accuracy is slightly worse than Shaw’s\nmethod (Shaw et al., 2018). We skip the compari-\nson to XLNet’s relative embedding while focusing\non the comparison to the Shaw’s method, which\nhas been widely used in the variants of BERTs due\nto its simplicity (Raffel et al., 2019; Song et al.,\n2020).\n3.5 Proposed position embeddings\nIn this section, we propose four variants of rela-\ntive position embedding to encourage increased\ninteractions between key, query, and position em-\nbedding in the self-attention mechanism. The de-\nsign choices include whether relative positions are\nsigned and whether they are scalars or vectors.\n3.5.1 Relative position embedding method 1\nThis method only considers the absolute distance\nof token iand j. That is, it does not distinguish the\nsign of the distance j−i. The distance embedding\ncan be written as follows.\naij = w|j−i|, (11)\nwhere w is scalar used to represent how token i\nattends to j with absolute distance |j −i|. We\ndo not apply the clipping value k in this method.\nThe learnable parameters arew= (w0,...,w n−1),\nwhere nis maximum sequence length. The equa-\ntion (5) can be revised as follows to consider the\ndistance between token iand jin computing their\nattention. As aij is a scalar, we use the multiplica-\ntive interaction between key, query and relative\nembedding, which is different from the additive\ninteraction in Shaw’s method.\neij = (xiWQ)(xjWK)T aij√dz\n. (12)\n3.5.2 Relative position embedding method 2\nAs with method 1, this method uses scalars to repre-\nsent relative position embeddings. However, it now\ndistinguishes the sign of the distance j−i. That\nis, it assumes that the future token has different at-\ntention weights from the previous one in attending\nto a token in the middle, despite that the absolute\ndistance is the same. The distance embedding can\nthus be written as follows.\naij = wj−i, (13)\nwhere wis scalars used to represent how token i\nattends to j. The learnable parameters are w =\n(w1−n,...,w 0,...,w n−1), where nis maximum\nsequence length. Similar to method 1, the equation\n(12) is used compute the attention scores.\n3.5.3 Relative position embedding method 3\nMethod 3 replaces the scalar relative position em-\nbeddings with vector embeddings. The distance\nembedding can thus be written as follows.\naij = wj−i, (14)\nwhere w ∈Rdz represents the embedding on how\ntoken iattends to j. The learnable parameters are\nw = (w1−n,..., w0,..., wn−1), where nis max-\nimum sequence length. The equation (5) can be\nrevised as follows.\neij = sum prod(xiWQ,xjWK,aij)√dz\n. (15)\nNote that the numerator is the sum over element-\nwise product of three vectors in dimension Rdz :\nquery vector, key vector and relative position em-\nbedding vector. This is a natural extension from\nmultiplication of scalars in method 2. The key\ndifference is the introduction of the multiplicative\ninteraction between key, query, and the relative po-\nsition vector, which was missing in all previous\nmethods (including absolute position embeddings\nand Shaw et al. (2018) and XLNet’s relative po-\nsition embeddings). For example, in Shaw et al.\n(2018), equation (5), the attention score has two\nfactors. The ﬁrst models the interaction between\nkey and query, (xiWQ)(xjWK)T , and the second\nmodels the interaction between query and relative\nposition embedding, (xiWQ)(aij)T . We hypothe-\nsize that the explicitly modeling of the interaction\nbetween query, key and relative position embed-\nding would have more expressive power. In this\nmethod, the relative position embedding serves as\na gate to ﬁlter out the dot product of query and key.\nThis gate would prevent a query from attending to\na similar key (content-wise) heavily if the query\nand key positions are far away from each other.\n3331\n3.5.4 Relative position embedding method 4\nWe identiﬁed that all previous relative position em-\nbeddings do not model the interaction of query,\nkey and relative position embeddings simultane-\nously. As a backoff from method 3 and also an\nextension to Shaw’s method, method 4 consists\nof modeling the dot product of all possible pairs\nof query, key, and relative position embeddings.\nAs with method 3, the learnable parameters are\nw = (w1−n,..., w0,..., wn−1). The equation\n(5) can be revised as follows.\neij = (xiWQ) ·(xjWK) + (xiWQ) ·aij + (xjWK) ·aij√dz\n.\n(16)\nThree factors in the numerator model the interac-\ntion of query and key, query and relative position\nembedding, and key and relative position embed-\nding, respectively. The interaction of query and\nkey is the conventional content attention, while the\nremaining two are for relative position discount of\nquery and key respectively. Shaw’s method (see\nequation 5) only contains the ﬁrst two factors. We\nnote that the embeddings are shared in factor 2 and\n3, the formulation in (16) empowers a more reli-\nable estimation of relative embeddings compared to\nShaw’s method, as we will see in the experiments.\nMethod 4 can be re-written as,\neij = (xiWQ + aij)(xjWK + aij)T −⟨aij,aij⟩√dz\n.\n(17)\nThe ﬁrst term is a generalized case to absolute posi-\ntion embeddings (see equation (6)), in which each\nabsolute position embedding vector is added to\nthe word embedding. Precisely, the assignment\nof aij = ai and aij = aj for the two entries of\naij in the ﬁrst factor and the drop of the bias term\n⟨aij,aij⟩make absolute position embeddings a spe-\nciﬁc case of method 4.\n3.6 Complexity Analysis\nWe analyze the storage complexity of various po-\nsition embedding methods in this section. For a\ntransformer model with mlayers, hattention heads\nper layer, and maximum sequence length of n, ta-\nble 1 lists the parameter size for various position\nembeddings and the runtime storage complexity. In\norder to have sufﬁcient expressive power, we allow\ndifferent embedding parameters at different layers\nfor all methods (see Figure 1 right) except absolute\nposition embedding2. For example, Shaw’s method\nintroduces relative position embedding parameters\nwith size of mh(2n−1)d. The parameters are\n2To be compatible to the original BERT implementation.\nused multiple times in equation (8) (also see Fig-\nure 3), leading to runtime storage complexity of\nO(mhn2d).\nMethod Parameter size Complexity\nAbsolute nd O(nd)\nShaw mh(2n −1)d O(mhn2d)\nmethod 1 mhn O(mhn2)\nmethod 2 mh(2n −1) O(mhn2)\nmethod 3 mh(2n −1)d O(mhn2d)\nmethod 4 mh(2n −1)d O(mhn2d)\nTable 1: Parameter sizes and runtime storage complex-\nities of various position embedding methods.\nAll position embedding methods introduce a\nsmall number of additional parameters to the BERT\nmodel. Precisely, Shaw, method 3 and 4 introduce\nmh(2n−1)d, 12 ∗12 ∗(2 ∗512 −1) = 147K\nparameters at maximum, which is negligible when\ncompare to the number of parameters in BERT\n(108M parameters). For simple methods 1 and 2,\nthey introduce even fewer parameters. We point\nout a caveat on method 3: despite the fact that it\nintroduces the same number of parameters as with\nmethod 4, it requires a signiﬁcantly higher mem-\nory footprint during training. This may be due to\nthe inefﬁcient GPU implementation of sum over\nelement-wise product of vectors in Equation (15)\ncompared to matrix multiplication. As a result, we\ncan only ﬁt 2 sequences in each GPU for method 3,\nas opposed to 20 sequences per GPU for all other\nmethods. In terms of training and inference speed,\nShaw’s method and proposed methods 1, 2 and 4\nare all similar to the absolute position embedding\nbaseline.\n4 Experiments\nWe leverage the same data used to pre-train BERT:\nBooksCorpus (800M words) (Zhu et al., 2015) and\nEnglish Wikipedia (2.5B words) (Wikipedia con-\ntributors, 2004; Devlin et al., 2018). Following\nthe setup from RoBERTa (Liu et al., 2019a), we\nleave out the next sentence prediction loss and only\nuse one segment instead of the two segments pro-\nposed in BERT (Devlin et al., 2018) during model\ntraining. We set the maximum input length to 512.\nSimilar to BERT, we use a vocabulary size of 30k\nwith wordpiece tokenization.\nWe generate the masked input from MLM targets\nusing whole word masking. The model updates use\na batch size of 160 and Adam optimizer with learn-\ning rate starting at 1e-4. Our maximum batch size\nis 160 on an Nvidia V100 instance (with 8GPUs).\nFollowing previous work (Devlin et al., 2018;\nYang et al., 2019; Liu et al., 2019a; Lan et al.,\n2019), we evaluate on the General Language Un-\n3332\nderstanding Evaluation (GLUE) benchmark (Wang\net al., 2018) and the Stanford Question Answering\nDataset (SQuAD1.1) (Rajpurkar et al., 2016).\n4.1 Models evaluation on SQuAD dataset\nWe run the pre-training experiments for different\nposition embedding methods on base settings only.\nWe omit the BERT-large experiments as they are\ncomputationally intensive. After pre-training, we\nﬁne-tune on SQuAD1.1. Table 2 shows the re-\nsults of SQuAD for absolute position embedding,\nShaw’s relative position embedding, and the four\nrelative position embeddings proposed in this paper.\nModel EM F1\nBERT (Devlin et al., 2018) 80.8 88.5\nAbsolute 81.58 88.59\n(Shaw et al., 2018) 82.38 89.37\nMethod 1 80.82 87.96\nMethod 2 81.44 88.86\nMethod 3 83.71 90.50\nMethod 4 83.63 90.53\nTable 2: SQuAD1.1 development results for various po-\nsition embeddings on BERT-base.\nWe reproduced compatible BERT baselines (De-\nvlin et al., 2018) (F1 score of 88.5) with absolute\nposition embedding (F1 score of 88.59). We show\nthat Shaw’s relative position embedding leads to\na higher accuracy (F1 score of 89.37) when com-\npared to the BERT default setting (absolute position\nembedding). Our proposed simple relative position\nembeddings method 1 results in F1 scores of 87.96,\nwhich is worse than the baseline of absolute po-\nsition embedding. When we consider the relative\ndistance sign (method 2), we obtain an improved\nF1 score of 88.86, which is similar to the BERT ab-\nsolute position embedding baseline. This shows the\neffectiveness of multiplicative interaction between\nquery, key and relative embedding directly, despite\nthat the relative embeddings are simple scalars. The\nmethod 3, which has vector representations for rel-\native position embeddings and also models the in-\nteraction between query, key and relative position\nembedding directly, leads to a higher F1 score of\n90.50. Finally, the method 4, which is backoff of\nmethod 3 (or extension of Shaw’s method), leads to\na similar F1 score of 90.53. Method 4 is the most\npromising method among four proposed methods\ndue to its high accuracy and computation efﬁciency.\n4.2 Model evaluation on GLUE datasets\nFollowing Devlin et al. (2018), we use a batch\nsize of 32 and 3-epoch ﬁne-tuning over the data\nfor GLUE tasks. For each task, we report the ac-\ncuracy on development dataset with learning rate\n3e-5. Table 3 shows the results of GLUE datasets\nfor absolute position embedding, Shaw’s relative\nposition embedding and the four proposed methods\nin this paper.\nFollowing the settings from BERT (Devlin et al.,\n2018), F1 scores are reported for QQP and MRPC,\nand accuracy scores are reported for MNLI and\nSST-2. There is no signiﬁcant accuracy differ-\nence between the absolute, Shaw and proposed\nmethods, except that the proposed method 3 leads\nto signiﬁcant lower F1 score (82.86) on MRPC\ndataset. While various position embeddings lead\nto different results on complex question answering\ndatasets like SQuAD, they are not as sensitive to\nGLUE tasks. Our hypothesis is that SQuAD re-\nquires hidden activations at all token positions, so\nthe relative position embedding plays a key role\nin modeling the interactions of tokens at different\npositions. The GLUE datasets, on the other hand,\nuse the ﬁrst token [CLS] only and thus the relative\nembeddings have limited impact. We do not know\nthe exact reason for the low accuracy of method 3\non MRPC dataset. One hypothesis is that the inter-\naction between query, key and position embedding\nintroduced in method 3 is unstable on this dataset.\n4.3 Models with variousk\nWe usually limit the maximum training sequence\nlength to 512 in BERT training in consideration\nof the memory footprint. It remains relatively un-\nexplored for the inductive property: can a BERT\nmodel trained on short sentences be generalized\nto handle longer sentences? This property is not\nthoroughly explored, partially because a maximum\nsequence length would be applied during infer-\nence anyway for practical considerations and thus\nthere is no consistency between training and test-\ning. Nevertheless, to fully address the question,\none can train BERT models with different settings\nof maximum sequence lengths and test on longer\nsequences. The inductive property is related to the\nposition embedding methods. One can try differ-\nent position embedding methods and test how they\naffect the inductive property. For example, if we\nset mand n, m < n, as the maximum sequence\nlengths for training and test respectively. The ﬁxed\nsinusoid, Shaw’s, and our proposed methods can\nbe directly employed while the absolute position\nmethod cannot as the position embeddings for po-\nsition [m+ 1,n] are not learned in training but are\nrequired during inference. The relative position\nembeddings are better choices as they are not sub-\nject to a maximum position value and learnable. In\n3333\nModel MNLI-(m/mm) QQP SST-2 MRPC\nBERT (Devlin et al., 2018) 84.6/83.4 71.2 93.5 88.9\nAbsolute 83.57/83.65 87.64 90.48 88.40\n(Shaw et al., 2018) 84.10/84.07 87.77 90.94 88.68\nMethod 1 83.84/84.06 87.52 91.97 88.65\nMethod 2 83.68/83.78 87.50 91.05 87.34\nMethod 3 84.81/84.68 87.11 91.39 82.86\nMethod 4 84.45/84.51 87.41 91.74 88.88\nTable 3: GLUE development results for different position embeddings on BERT base setting.\nthis section, we vary the clipping distance k(equa-\ntion 9) of the maximum relative distance to see\nhow it affects the model accuracy. A small value\nof kexplicitly models two tokens within this dis-\ntance. Any pairs of tokens greater than this would\nbe treated as if they are kpositions away.\nTable 4 shows the EM and F1 score of method\n4 on SQuAD dev dataset as a function of k. The\nPre-train Fine-tune\nk MaxSeqLen MaxSeqLen EM F1\n2 512 512 82.17 89.19\n4 512 512 82.44 89.37\n8 512 512 82.64 89.59\n16 512 512 83.42 90.18\n32 512 512 83.58 90.30\n64 512 512 83.40 90.21\n128 512 512 83.20 90.03\n256 512 512 83.59 90.54\n512 512 512 83.63 90.53\n256 512 576 83.80 90.71\n256 512 640 83.97 90.68\n256 512 704 83.44 90.32\nTable 4: SQuAD development results for differentkon\nBERT base setting.\nSQuAD dev data consists of 10570 question an-\nswer pairs. The average lengths of training and\ndevelopment sequences (questions and documents)\nare 130 and 133 respectively. We observe that\nthe accuracy on SQuAD dev remains similar with\nk ≥323. This suggest that the relative position\nembedding for token pairs are greater than k= 32\ncan only provide marginal information in BERT\nmodel training even the training sequences consist\nof 130 tokens in average. This observation ensures\nthat method 4 is robust and generalized from the\ninductive perspective.\nThe absolute position embedding used in BERT\ndoes not permit downstream ﬁne-tuning tasks train-\ning on sequences which have more tokens than the\nmaximum sequence length (512). This, however, is\nnot an issue for relative position embedding meth-\nods proposed in this paper. We hypothesize that\n3Note that in Shaw et al. (2018), they found that the BLEU\nscores remains the same when k ≥2 in encoder decoder\narchitecture.\nthis ﬂexibility may offer further accuracy boost on\ndownstream tasks. We ﬁne-tune the model, which\nwas pre-trained with k = 256and pre-train maxi-\nmum sequence length of 512, on SQuAD training\ndata but allowing increased maximum sequence\nlengths (576, 640 and 704 respectively)4. The bot-\ntom of Table 4 conﬁrms our hypothesis. For exam-\nple, setting ﬁne-tuning maximum sequence length\nto 576 results in the highest F1 score ( 90.71%).\nWe note that the gain mainly comes from the small\npercentage of SQuAD training and test data which\nhave more tokens than 512. We hypothesize that,\nfor a dataset which have a large percentage of se-\nquences with 512 or more tokens, the gain can be\nmore signiﬁcant.\n4.4 Relative position embeddings for large\nBERT models\nTraining BERT large models is computational ex-\npensive. To minimize the training cost, we test\nmethod 4 on a pre-trained BERT large model. In\nparticular, we load a pre-trained BERT large model,\nbert-large-uncased-whole-word-masking, from py-\ntorch transformer5 as the initial model and ﬁne-tune\nthe existing parameters and the new relative posi-\ntion embedding parameters for 3 epochs, staring\nwith a small learning rate of 5e−5. We do not\ndo the clipping of relative distance (thus k= 512).\nTable 5 shows that, with the near drop-in replace-\nment of absolute position embedding with relative\nposition embedding, method 4 boosts the F1 score\nof 93.15 to 93.55, with negligible increased num-\nber of parameters and inference latency. We hy-\npothesize the same ﬁne-tuning can be applied to\ndifferent BERT variants (e.g., RoBERTa) to boost\nnew state-of-the-art accuracy. In addition, we allow\nlarger maximum sequence lengths (576, 640 and\n704) in SQuAD ﬁne-tuning task but do not observe\nadditional gain in this case.\n4Around 3% of sequences have more tokens than maxi-\nmum sequence length of 512.\n5https://github.com/huggingface/transformers\n3334\nPre-train Fine-tune\nModel MaxSeqLen MaxSeqLen EM F1\nBERT (Devlin et al., 2018) 512 512 84.1 90.9\nPre-trained 512 512 86.91 93.15\nMethod 4 512 512 87.51 93.55\nMethod 4 512 576 87.40 93.48\nMethod 4 512 640 87.57 93.49\nMethod 4 512 704 87.16 93.47\nTable 5: SQuAD development results for various BERT models for large setting.\n4.5 Relative Position Visualization\nWe attempt to visualize relative position embed-\ndings in this section. We select method 4 for visu-\nalization as it is both the most efﬁcient and most\naccurate amongst our proposed methods.\nFigure 4 shows the embedding weights of the\nﬁrst head in the ﬁrst layer for method 4, which is\na 1023 ×64 matrix, with the ﬁrst dimension being\nthe relative distance between two tokens, and the\nsecond being the attention dimension. We choose\nto plot the relative position of [−50,50] which con-\ncentrates the proximity of two positions. We note\nthat the weights at relative position of zero have\nthe large absolute values, either positive (white) or\nnegative (dark blue). These large absolute values\nmay lead to largeeij values in equation (16), which\nindicates a token is likely to attend to another token\nwithin a close distance.\nFigure 5 shows the averaged attention weights\nover 12 heads on the ﬁrst transformer layer for\nmethod 4. We show the self-attention between\nthe ﬁrst 50 tokens. This clearly shows that tokens\nheavily attend to their neighbors (dark blue on the\ndiagonal) and has nearly zero attentions to tokens\nwhich are far away. This also explains why a small\nvalue of kis sufﬁcient for the relative position em-\nbedding as the attention weights beyond this range\nare close to zero. Note that tokens usually have near\nzero attention on themselves. This seems counter-\nintuitive but can be explained by the masked lan-\nguage model (MLM) task, in which the neighbors\nof a given token (as opposed to the token itself)\nprovide the most useful information for the task.\n5 Conclusion\nWe proposed new relative position embedding\nmethods to encourage more interactions between\nquery, key and relative position embeddings in self-\nattention mechanism. Our best proposed method\nis a generalization of the absolute position embed-\nding and it leads to higher accuracy than the abso-\nlute and previous relative position embeddings on\nSQuAD1.1. In addition, we demonstrated empir-\nically that our relative embedding method is rea-\n0 20 40 60\nHead attention dimension\n-50\n-40\n-30\n-20\n-10\n0\n10\n20\n30\n40 j-i\n1.5\n1.0\n0.5\n0.0\n0.5\n1.0\n1.5\nFigure 4: Relative position (from -50 to 50) embed-\nding weights on ﬁrst transformer layer and ﬁrst head\nfor method 4.\n0 10 20 30 40\nj\n0\n10\n20\n30\n40 i\n0.05\n0.10\n0.15\n0.20\n0.25\n0.30\nFigure 5: Averaged attention weights across 12 heads\non the ﬁrst transformer layer for method 4.\nsonably generalized and robust from the inductive\nperspective. Finally, we showed that our proposed\nmethod can be effectively and efﬁciently adopted\nas a drop-in replacement to boost the performance\nof large models with a small computational budget.\n3335\nReferences\nJan Chorowski, Dzmitry Bahdanau, Dmitriy Serdyuk,\nKyunghyun Cho, and Yoshua Bengio. 2015.\nAttention-based models for speech recognition.\narXiv:1506.07503.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V . Le, and Ruslan Salakhutdinov. 2019.\nTransformer-xl: Attentive language models beyond\na ﬁxed-length context. arXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv:1810.04805.\nJonas Gehring, Michael Auli, David Grangier,\nDenis Yarats, , and Yann N. Dauphin. 2017.\nConvolutional sequence to sequence learning.\narXiv:1705.03122v2.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Noam Shazeer, Curtis Hawthorne, An-\ndrewMDai, Matthew D Hoffman, and Douglas Eck.\n2018. An improved relative self-attention mecha-\nnism for transformer with application to music gen-\neration. arXiv:1809.04281.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv:1909.11942.\nXiaodong Liu, Pengcheng He, Weizhu Chen, and\nJianfeng Gao. 2019a. Multi-task deep neu-\nral networks for natural language understanding.\narXiv:1901.11504.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019b.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv:1907.11692.\nAnkur P. Parikh, Oscar T ¨ackstr¨om, Dipanjan Das,\nand Jakob Uszkoreit. 2016. A decomposable\nattention model for natural language inference.\narXiv:1606.01933.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions for\nmachine comprehension of text. Proceedings of the\n2016 Conference on Empirical Methods in Natural\nLanguage Processing.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswanii.\n2018. Self-attention with relative position represen-\ntations. arXiv:1803.02155.\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and\nTie-Yan Liu. 2020. Mpnet: Masked and per-\nmuted pre-training for language understanding.\narXiv:2004.09297.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. arXiv:1706.03762.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. A\nmulti-task benchmark and analysis platform for nat-\nural language understanding. 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP.\nWikipedia contributors. 2004. Plagiarism —\nWikipedia, the free encyclopedia. [Online; ac-\ncessed 22-July-2004].\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. arXiv:1906.08237.\nYukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhut-\ndinov, Raquel Urtasun, Antonio Torralba, and Sanja\nFidler. 2015. Aligning books and movies: Towards\nstory-like visual explanations by watching movies\nand reading books. IEEE international conference\non computer vision.",
  "topic": "Embedding",
  "concepts": [
    {
      "name": "Embedding",
      "score": 0.8247355818748474
    },
    {
      "name": "Computer science",
      "score": 0.6960733532905579
    },
    {
      "name": "Position (finance)",
      "score": 0.6720234155654907
    },
    {
      "name": "Transformer",
      "score": 0.670900821685791
    },
    {
      "name": "Generalization",
      "score": 0.5527881979942322
    },
    {
      "name": "Property (philosophy)",
      "score": 0.5104557275772095
    },
    {
      "name": "Relative value",
      "score": 0.416596382856369
    },
    {
      "name": "Algorithm",
      "score": 0.414652556180954
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4097445011138916
    },
    {
      "name": "Mathematics",
      "score": 0.23105579614639282
    },
    {
      "name": "Voltage",
      "score": 0.08887702226638794
    },
    {
      "name": "Engineering",
      "score": 0.0731135904788971
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Finance",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Epistemology",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 98
}