{
    "title": "Aligning Instruction Tasks Unlocks Large Language Models as Zero-Shot Relation Extractors",
    "url": "https://openalex.org/W4385569985",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A1988944048",
            "name": "Kai Zhang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3023642063",
            "name": "Bernal Jiménez Gutiérrez",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2107570501",
            "name": "Yu Su",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3164972323",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W4385573954",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W2996264288",
        "https://openalex.org/W2971136144",
        "https://openalex.org/W4385573991",
        "https://openalex.org/W2759211898",
        "https://openalex.org/W4378189609",
        "https://openalex.org/W3154945374",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3122890974",
        "https://openalex.org/W3034891697",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W3192405822",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4312205996",
        "https://openalex.org/W4309811444",
        "https://openalex.org/W3187984992",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W2181042685",
        "https://openalex.org/W3210277894",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W3198490223",
        "https://openalex.org/W2963924212",
        "https://openalex.org/W2475167333"
    ],
    "abstract": "Recent work has shown that fine-tuning large language models (LLMs) on large-scale instruction-following datasets substantially improves their performance on a wide range of NLP tasks, especially in the zero-shot setting. However, even advanced instruction-tuned LLMs still fail to outperform small LMs on relation extraction (RE), a fundamental information extraction task. We hypothesize that instruction-tuning has been unable to elicit strong RE capabilities in LLMs due to RE’s low incidence in instruction-tuning datasets, making up less than 1% of all tasks (Wang et al. 2022). To address this limitation, we propose QA4RE, a framework that aligns RE with question answering (QA), a predominant task in instruction-tuning datasets. Comprehensive zero-shot RE experiments over four datasets with two series of instruction-tuned LLMs (six LLMs in total) demonstrate that our QA4RE framework consistently improves LLM performance, strongly verifying our hypothesis and enabling LLMs to outperform strong zero-shot baselines by a large margin. Additionally, we provide thorough experiments and discussions to show the robustness, few-shot effectiveness, and strong transferability of our QA4RE framework. This work illustrates a promising way of adapting LLMs to challenging and underrepresented tasks by aligning these tasks with more common instruction-tuning tasks like QA.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 794–812\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nAligning Instruction Tasks Unlocks Large Language Models\nas Zero-Shot Relation Extractors\nKai Zhang Bernal Jiménez Gutiérrez Yu Su\nThe Ohio State University\n{zhang.13253, jimenezgutierrez.1, su.809}@osu.edu\nAbstract\nRecent work has shown that fine-tuning\nlarge language models (LLMs) on large-scale\ninstruction-following datasets substantially im-\nproves their performance on a wide range of\nNLP tasks, especially in the zero-shot set-\nting. However, even advanced instruction-\ntuned LLMs still fail to outperform small LMs\non relation extraction (RE), a fundamental in-\nformation extraction task. We hypothesize\nthat instruction-tuning has been unable to elicit\nstrong RE capabilities in LLMs due to RE’s\nlow incidence in instruction-tuning datasets,\nmaking up less than 1% of all tasks (Wang\net al., 2022). To address this limitation, we pro-\npose QA4RE, a framework that aligns RE with\nquestion answering (QA), a predominant task\nin instruction-tuning datasets. Comprehensive\nzero-shot RE experiments over four datasets\nwith two series of instruction-tuned LLMs (six\nLLMs in total) demonstrate that our QA4RE\nframework consistently improves LLM perfor-\nmance, strongly verifying our hypothesis and\nenabling LLMs to outperform strong zero-shot\nbaselines by a large margin. Additionally, we\nprovide thorough experiments and discussions\nto show the robustness, few-shot effectiveness,\nand strong transferability of our QA4RE frame-\nwork. This work illustrates a promising way of\nadapting LLMs to challenging and underrepre-\nsented tasks by aligning these tasks with more\ncommon instruction-tuning tasks like QA.1\n1 Introduction\nLarge language models (LLMs) (Brown et al.,\n2020; Chowdhery et al., 2022; Zhang et al., 2022)\nhave been shown to achieve impressive perfor-\nmance on many NLP tasks. Using the in-context\nlearning paradigm, without any parameter updating,\nLLMs are able to achieve comparable performance\nwith small language models (LMs) fine-tuned on\nthousands of examples (Liu et al., 2022; Min et al.,\n1Code and data are available at https://github.com/OSU-\nNLP-Group/QA4RE.\n/uni00000026/uni0000003e/uni00000004/uni00000045/uni00000372/uni00000064/uni000003f1\n/uni00000079/uni00000079/uni0000003e\n/uni00000027/uni00000057/uni00000064/uni00000372/uni000003ef/uni00000358/uni000003f1\n/uni0000019a/uni0000011e/uni000001c6/uni0000019a/uni00000372/uni000003ec/uni000003ec/uni000003ef\n/uni000003ef/uni000003ec\n/uni000003f0/uni000003ec\n/uni000003f1/uni000003ec\n/uni000003f2/uni000003ec/uni00000026/uni000003ed\n/uni00000004/uni000001c0/uni0000011e/uni0000018c/uni00000102/uni00000150/uni0000011e/uni00000003/uni0000005a/uni0000001c/uni00000003/uni00000057/uni0000011e/uni0000018c/uni00000128/uni0000017d/uni0000018c/uni00000175/uni00000102/uni00000176/uni00000110/uni0000011e\n/uni00000073/uni00000102/uni00000176/uni0000015d/uni0000016f/uni0000016f/uni00000102/uni00000003/uni0000005a/uni0000001c\n/uni00000059/uni00000004/uni000003f0/uni0000005a/uni0000001c\n/uni00000057/uni0000018c/uni0000015d/uni0000017d/uni0000018c/uni00000003/uni0000005e/uni0000017d/uni00000064/uni00000004\nFigure 1: Main finding: Strong instruction-tuned LLMs\nunderperform prior zero-shot RE methods using the stan-\ndard (vanilla) RE formulation. Our QA4RE framework\nenables models in two sets of instruction-tuned LLMs\n(FLAN-T5 and GPT-3.5) to surpass the prior SoTA on\n4 RE datasets by a large margin. Results are averaged\nover 4 RE datasets. We omit the word ‘davinci’ from\nthe GPT-3.5 model displayed for brevity.\n2022a; Liang et al., 2022). 2 More recently, fine-\ntuning LLMs on datasets containing thousands of\ndownstream tasks transformed into an instruction\nfollowing format (i.e., instruction-tuning) has been\nshown to improve LLMs considerably across the\nboard, especially in zero-shot setting (Iyer et al.,\n2022; Ouyang et al., 2022; Chung et al., 2022).\nWe examine the capability of LLMs in identi-\nfying the relationship between entities in a sen-\ntence, i.e., relation extraction (RE), a funda-\nmental task in information extraction. Recent\nwork (Jimenez Gutierrez et al., 2022) has found\nthat LLMs underperform fine-tuned small LMs for\nRE in the biomedical domain. Our results on gen-\neral domain RE in Fig. 1 reveal that even two of\nthe most advanced instruction-tuned LLMs, FLAN-\nT5 XXL (Chung et al., 2022) and text-davinci-003\n(Ouyang et al., 2022), fail to outperform the state-\nof-the-art (SoTA) zero-shot RE method based on\nsmall LMs (Sainz et al., 2021).\nWe hypothesize that the limited relation extrac-\ntion capability of instruction-tuned LLMs could be\n2We regard LMs with less than 1B params as small.\n794\na byproduct of the low incidence of RE tasks in\ninstruction-tuning datasets (Ouyang et al., 2022;\nSanh et al., 2022; Chung et al., 2022; Wang et al.,\n2022).3 To address the low incidence issue, we\npropose the QA4RE framework, which aligns RE\nwith multiple-choice question answering (QA), a\ntask that appears much more frequently in most\ninstruction-tuning datasets—around 12-15% of all\nthe tasks in both Wang et al. (2022) and Ouyang\net al. (2022). Specifically, by casting the input sen-\ntence as a question and possible relation types as\nmultiple-choice options (Fig. 2), LLMs are able to\nperform RE by selecting the option representing\nthe correct relation type.\nThorough evaluations on four real-world rela-\ntion extraction datasets and six instruction-tuned\nmodels from two different series (OpenAI GPT-\n3.5 and FLAN-T5 (Chung et al., 2022)) show that\nQA4RE brings significant gains over the standard\nRE formulation on, validating its effectiveness and\nour hypothesis concerning the low incidence of\nRE. More specifically, our framework enables text-\ndavinci-003 and FLAN-T5-XXLarge to achieve\nan average of 8.2% and 8.6% absolute improve-\nments in F1, respectively. For the first time, an\nLLM is able to outperform prior small-LM-based\nSoTA in the zero-shot setting by a large margin.\nIn-depth analyses further demonstrate the robust-\nness and few-shot effectiveness of QA4RE. More\nimportantly, our framework has been proven to be\neffectively transferable on instruction-tuned mod-\nels with various sizes, ranging from 80M to 175B.\nOur contributions are summarized as follows:\n(1) We systematically investigate instruction-tuned\nLLMs on four real-world relation extraction\ndatasets and note that their limited performance\non RE might stem from the low incidence of RE\ntasks in instruction-tuning datasets.\n(2) We reformulate RE as multiple-choice QA\nin an effort to appropriately leverage QA’s much\nhigher prevalence in instruction-tuning datasets\nand achieve significant improvements on six recent\ninstruction-tuned LLMs, significantly outperform-\ning previous SoTA zero-shot RE methods based on\nsmall LM for the first time.\n(3) In addition, we demonstrate our QA4RE\nmethod’s robustness to diverse prompt designs as\nwell as its promising results in the few-shot setting.\n(4) Finally, we show the effectiveness of QA4RE\n3RE-like tasks are <0.5% of the largest available instruc-\ntion dataset (Wang et al., 2022); see Appendix A for details.\nframework is transferable and consistent on various\ninstruction-tuned models with different sizes from\n80M to 175B. Our study illustrates the potential of\naligning infrequent and challenging tasks with fre-\nquent instruction-tuning tasks and can guide others\nin exploring this direction.\n2 Related Work\nInstruction Tuning. Large language models\noriginally obtained impressive zero and few-shot\nperformance by leveraging self-supervised next to-\nken prediction at massive scales. More recently,\nsupervised fine-tuning on a large number of down-\nstream tasks has been shown to improve LLM ac-\ncuracy, robustness, fairness, and generalization to\nunseen tasks (Ouyang et al., 2022; Iyer et al., 2022;\nWei et al., 2022a; Chung et al., 2022; Sanh et al.,\n2022). Several strategies have been developed\nto align LLMs to human instructions including\nReinforcement Learning from Human Feedback\n(RLHF) (Ouyang et al., 2022) as well as the more\nstandard language modeling objective, used to fine-\ntune LLMs on a wide range of tasks reformulated\nas instruction following tasks (Iyer et al., 2022; Wei\net al., 2022a; Chung et al., 2022; Sanh et al., 2022).\nEliciting LLM Abilities. The high cost and in-\ncreasingly private nature of LLM pre-training make\nit quite challenging to conclusively determine how\ndifferent pre-training techniques bring about dif-\nferent LLM capabilities. Many factors involved in\npre-training such as simple self-supervised scaling,\ncode or multi-lingual text pre-training (Chowdhery\net al., 2022; Chen et al., 2021; Chung et al., 2022)\nas well as the distinct versions of instruction-tuning\nmentioned above (Ouyang et al., 2022; Iyer et al.,\n2022; Wei et al., 2022a; Chung et al., 2022), can\ninteract in a wide variety of ways to unleash the\nabilities LLMs display. Nonetheless, Fu and Khot\n(2022) hypothesize that the use of code during pre-\ntraining seems to improve an LM’s reasoning abil-\nity, evidenced by the improved ability to leverage\nChain-of-Thought prompting (Wei et al., 2022b)\nby models trained partially on code such as PaLM\n(Chowdhery et al., 2022), code-davinci-002 (Chen\net al., 2021), and text-davinci-002/003 (Ouyang\net al., 2022), compared to text-only models like\ntext-davinci-001 and OPT-175B (Zhang et al.,\n2022). Additionally, instruction-tuning on a large\nset of tasks has been shown to improve generaliza-\ntion to unseen tasks, reduce the need for few-shot\nexamples and improve accuracy and robustness\n795\nVanilla REGiven a sentence, and two entities within the sentence, classify the relationship between the two entities based on the provided sentence. All possible relationships are listed below:-per:city_of_birth-per:city_of_death-per:cities_of_residence-no_relationSentence: Wearing jeans and a white blouse, Amanda Knox of Seattle is being cross-examined by prosecutors.Entity 1 : Amanda KnoxEntity 2 : SeattleRelationship:per:city_of_birth\nQA4REDetermine which option can be inferred from the given sentence.Sentence: Wearing jeans and a white blouse, Amanda Knox of Seattle is being cross-examined by prosecutors.Options:A. Amanda Knox was born in the city SeattleB. Amanda Knox died in the city SeattleC. Amanda Knox lives in the city SeattleD. Amanda Knox has no known relations to SeattleWhich option can be inferred from the given sentence?Option: C.\nNLI RE\nAmanda Knox lives in the city Seattle\nWearing jeans and a white blouse, Amanda Knox of Seattle is being cross-examined by prosecutors.\nENC\nNo Relation Threshold\nFigure 2: This figure shows a schematic of the SoTA NLI zero-shot framework in which each sentence must\nbe compared with each relation template (left), the vanilla formulation for prompting GPT-3 for RE as done in\nJimenez Gutierrez et al. (2022) (center) and our multiple-choice QA setting, in which each relation is transformed\ninto a template and GPT-3 is expected to predict only a single letter (right).\nacross many language tasks (Ouyang et al., 2022;\nIyer et al., 2022; Chung et al., 2022).\nLow-Resource Relation Extraction. Several re-\nformulations of standard RE have enabled small\nLMs to achieve fairly strong performance in the\nzero and few-shot settings. Sainz et al. (2021) uti-\nlize small LMs fine-tuned on natural language in-\nference (NLI) datasets to perform zero-shot RE by\nselecting the entity-filled relation template which\nis mostly entailed by the test sentence. Lu et al.\n(2022) frame RE as a summarization task and lever-\nage generative models to summarize the relation\nbetween target entities in the sentence. Other low-\nresource RE methods augment prompt-tuning by\nusing logical rules to create complex prompts from\nsub-prompts (Han et al., 2022) and injecting knowl-\nedge about entity types using learnable virtual to-\nkens (Chen et al., 2022). Our current work uses\nseveral relation templates designed in these studies.\nLLMs for Relation Extraction.In terms of ex-\nploring the RE capabilities of LLMs, most previous\nwork has focused on investigating biomedical RE.\nJimenez Gutierrez et al. (2022) report that LLMs\nunderperform standard small LMs fine-tuning in\nthe few-shot setting on a comprehensive set of\nbiomedical RE datasets and show evidence that the\npoor handling of the none-of-the-above (NoTA) re-\nlation category is one of the major culprits. Further-\nmore, although a few RE-like tasks were included\nin Super Natural Instruction (Wang et al., 2022),\nthese tasks constitute about 0.5% of the dataset and\nnone of them were selected for model evaluation.\n3 Methodology\nIn this section, we formally define the relation ex-\ntraction problem and describe our multi-choice QA\napproach for the problem in detail.\n3.1 Problem Statement\nRelation extraction (RE) aims to extract the rela-\ntionship between two given entities based on a\nspecific sentence. More concretely, one relation\nexample contains a sentence S as well as a head\nentity Eh and a tail entity Et within S. Given a re-\nlation example (S, Eh, Et), models are required to\nidentify the relation between Eh and Et expressed\nin the S from a set of pre-defined relation types.\n3.2 Relation Templates\nRecent low-resource RE approaches (Sainz et al.,\n2021; Lu et al., 2022; Han et al., 2022) utilize\nrelation-entailed templates as label verbalization\n(e.g., “per:city_of_birth” -> “{ Eh} was born in\nthe city {Et}”). As illustrated in Fig. 2 (left), the\ncurrent SoTA method for low-resource RE (Sainz\net al., 2021) utilizes manually constructed relation\ntemplates to reformulate the RE task as a natural\nlanguage inference (NLI) task.\nTo ensure a fair comparison, we utilize the same\ntemplates developed in previous studies (Sainz\net al., 2021; Lu et al., 2022) to generate answer op-\ntions within our QA4RE framework. Furthermore,\nin Sec. 6.2 we discuss the possibility of directly\napplying the NLI formulation for RE in LLMs.\n796\n3.3 QA4RE Framework\nAs shown in Fig. 2 (right), we reformulate the rela-\ntion extraction task as a multi-choice QA problem.\nBy integrating the given head and tail RE entities\n(Eh and Et) into the relation templates and using\nthem as multiple-choice options, LLMs are able\nto leverage extensive QA instruction fine-tuning\nwhich has dramatically improved recent models.\nAdditionally, our method allows LLM to generate\nonly an answer index instead of the verbalized rela-\ntion as in previous work (Jimenez Gutierrez et al.,\n2022), also shown in Fig. 2 (center).\nType-Constrained Answer Construction. To\ntransform RE into a multiple-choice question, for a\ngiven relation example (S, Eh, Et), we utilize sen-\ntence S as the context in standard QA and create op-\ntions composed of pre-defined templates filled with\nEh and Et entities. To fairly compare with previous\nwork, we apply type constraints (when applicable)\nto eliminate options for relation types that are not\ncompatible with the entity types of the head and\ntail entities. For instance, if the type of Eh is PER-\nSON, the relation “org:country_of_headquarters”\nwould be deemed invalid given that a person does\nnot have headquarters.\n4 Experiment Setup\n4.1 Datasets\nWe evaluate our methods on four RE datasets:\n(1) TACRED (Zhang et al., 2017), (2) RETA-\nCRED (Stoica et al., 2021), (3) TACREV (Alt et al.,\n2020), and (4) SemEval 2010 Task 8 (SemEval for\nbrevity) (Hendrickx et al., 2010). Following previ-\nous work (Sainz et al., 2021; Lu et al., 2022; Han\net al., 2022; Chen et al., 2022), we report the micro\naveraged F1 with the none-of-the-above relation ex-\ncluded. To keep OpenAI API costs under control,\nwe randomly sample 1,000 examples from each\ndataset’s test split as our test set.\n4.2 Baselines\nZero-Shot. For small LM-based models, we eval-\nuate two low-resource SoTA RE baselines: (1) As\nshown in Fig. 2 (left), NLI (Sainz et al., 2021)\nreformulates RE as a natural language inference\ntask and leverages several LMs fine-tuned on the\nMNLI dataset (Williams et al., 2018): BART-\nLarge (Lewis et al., 2020), RoBERTa-Large (Liu\net al., 2019), and DeBERTa-XLarge (He et al.,\n2021). This method holds the SoTA perfor-\nmance on both zero and few-shot RE. (2) Besides,\nSuRE (Lu et al., 2022) frames RE as a summa-\nrization task and utilizes generative LMs such as\nBART-Large (Lewis et al., 2020) and PEGASUS-\nLarge (Zhang et al., 2020), achieving competitive\nresults in few-shot and fully-supervised settings.\nFor the NLI approach (Sainz et al., 2021), we re-\nport performance using their own templates on TA-\nCRED and TACREV . As this method does not have\ntemplates for RETACRED and SemEval, we use\nthe templates from the follow-up work, SuRE (Lu\net al., 2022), on these two datasets instead. All\nthe zero-shot methods, including those on LLMs,\napply entity type constraints to reduce the relation\nlabel space. Since SemEval does not provide entity\ntypes, the above methods use all possible relations\nin every instance as the label space.\nFew-Shot. Though our main experiments focus\non zero-shot RE, we further explore our method’s\ncapabilities by comparing their few-shot perfor-\nmance against several competitive small LM-based\nmethods on the TACRED dataset.\nThe NLI baseline can be easily extended to the\nfew-shot setting.4 Furthermore, we add (1) stan-\ndard Fine-Tuning (Jimenez Gutierrez et al., 2022),\n(2) PTR (Han et al., 2022) using prompt-tuning\nwith logical rules, and (3) KnowPrompt (Chen\net al., 2022) using entity type knowledge via learn-\ning virtual tokens, all of which are initialized with\nRoBERTa-Large (Liu et al., 2019). For hyperpa-\nrameter details, please refer to Appendix B.1.\n4.3 QA4RE Implementation Details\nOur QA4RE framework utilizes the same tem-\nplates and type constraints developed by prior\nwork (Sainz et al., 2021; Lu et al., 2022). In par-\nticular, we use SuRE (Lu et al., 2022) templates\nfor our QA4RE approach on all 4 datasets since\nNLI (Sainz et al., 2021) templates were only de-\nsigned for TACRED. For prompt engineering, we\nexplore prompt formats and task instructions for\nvanilla RE and QA4RE in pilot experiments, using\ntext-davinci-002 on a 250-example subset of the\nTACRED dev set. We then use the same task in-\nstructions and prompt format for all four datasets\nand LLMs. Please refer to Appendix B.2 and B.3\nfor prompt format and relation verbalization tem-\nplate details, respectively.\n4SuRE can also be extended to the few-shot setting but we\nwere unable to replicate their results with the code provided.\n797\nMethods TACRED RETACRED TACREV SemEval Avg.\nP R F1 P R F1 P R F1 P R F1 F1\nBaselines\nNLIBART 42.6 65.0 51.4 59.5 34.9 44.0 44.0 74.6 55.3 21.6 23.7 22.6 43.3\nNLIRoBERTa 37.1 76.9 50.1 52.3 67.0 58.7 37.1 83.6 51.4 17.6 20.9 19.1 44.8\nNLIDeBERTa 42.9 76.955.1 71.7 58.3 64.3 43.3 84.6 57.2 22.0 25.7 23.7 50.1\nSuREBART 13.1 45.7 20.4 17.9 34.6 23.6 14.1 52.3 22.2 0.0 0.0 0.0 16.5\nSuREPEGASUS 13.8 51.7 21.8 16.6 34.6 22.4 13.5 54.1 21.6 0.0 0.0 0.0 16.4\nGPT-3.5 Series\nChatGPTVanilla32.1 74.8 44.9 45.4 61.3 52.1 30.3 79.6 43.9 18.2 20.8 19.4 40.1\nQA4RE32.8 68.0 44.2 (−0.7) 48.3 76.8 59.3 (+7.2) 34.7 79.1 48.2 (+4.3) 29.9 35.2 32.3 (+12.9) 46.0 (+5.9)\ncode-002Vanilla27.2 70.1 39.2 42.7 70.4 53.1 27.5 77.7 40.6 27.2 25.6 26.4 39.8\nQA4RE37.7 65.4 47.8 (+8.6) 48.0 74.0 58.2 (+5.1) 31.7 65.5 42.7 (+2.1) 25.2 29.2 27.0 (+0.6) 43.9 (+4.1)\ntext-002Vanilla31.2 73.1 43.7 44.1 76.3 55.9 30.2 76.8 43.3 31.4 28.8 30.1 43.2\nQA4RE35.6 68.4 46.8 (+3.1) 46.4 72.4 56.5 (+0.6) 35.7 76.8 48.8 (+5.4) 29.4 34.3 31.6 (+1.5) 45.9 (+2.7)\ntext-003Vanilla36.9 68.8 48.1 49.7 62.2 55.3 38.2 76.8 51.0 33.2 39.3 36.0 47.6\nQA4RE47.7 78.659.4(+11.3) 56.2 67.2 61.2 (+5.9) 46.0 83.6 59.4(+8.4) 41.7 45.043.3 (+7.3) 55.8(+8.2)\nFLAN-T5 Series\nXLarge Vanilla51.6 49.1 50.3 54.3 40.3 46.3 56.0 59.1 57.5 35.6 29.8 32.4 46.6\nQA4RE40.0 78.2 53.0 (+2.7) 57.1 79.7 66.5 (+20.2) 40.7 85.9 55.3 (−2.2) 45.1 40.1 42.5 (+10.1) 54.3 (+7.7)\nXXLargeVanilla52.1 47.9 49.9 56.6 54.0 55.2 52.6 50.9 51.7 29.6 28.8 29.2 46.5\nQA4RE40.6 82.9 54.5 (+4.6) 56.6 82.9 67.3(+12.1) 39.6 86.4 54.3 (+2.6) 41.0 47.844.1(+14.9)55.1 (+8.6)\nTable 1: Experimental results on four RE datasets (%). We omit the ‘davinci’ within the names of GPT-3.5 Series\nLLMs and ChatGPT refers to gpt-3.5-turbo-0301. We mark the best results in bold, the second-best underlined, and\nF1 improvement of our QA4RE over vanilla RE in green.\nTo systematically compare our QA4RE frame-\nwork with the vanilla RE formulation, we evaluate\nthem on two series of LLMs, resulting in seven\nmodels in total. In GPT-3.5 series LLMs, for LLMs\naccessible via Text Completion API (code-davinci-\n002, text-davinci-002, and text-davinci-003), we\nfollow previous work (Jimenez Gutierrez et al.,\n2022) and use the logit bias option to constrain\ntoken generation to relation labels for vanilla RE\nand option indices for QA4RE. Due to the fewer\navailable control options for LLMs in Chat Com-\npletion API (gpt-3.5-turbo-0301), we only set the\ntemperature as 0 and use the default system prompt.\nWe also examine open-sourced FLAN-T5 series\nLLMs (Chung et al., 2022) that are trained on a mix-\nture of tasks (Sanh et al., 2022; Wei et al., 2022a;\nWang et al., 2022). The 1,836 tasks utilized in\ntraining include less than 0.5% of RE-similar tasks,\nmaking FLAN-T5 series LLMs the ideal models\nfor verifying our hypothesis. Specifically, we use\nXLarge (3B) and XXLarge (11B) models and adopt\nthe same prompts and greedy decoding strategy as\nGPT-3.5 series LLMs to ensure a fair comparison.\n5 Results\n5.1 Zero-Shot Results\nOur main experimental results on four relation ex-\ntraction datasets can be found in Tab. 1. We have\nthe following observations from our results:\n(1) By reformulating RE as QA, our framework\nimproves upon the vanilla RE formulation on all\nthe LLMs and most datasets, making them much\nstronger zero-shot relation extractors. In particu-\nlar, text-davinci-003 and FLAN-T5 XL and XXL\nare able to outperform the prior SoTA, NLIDeBERTa,\nby a large margin. One thing worth noting is that\nQA4RE brings the largest gain on the best LLM in\neach series (text-davinci-003 and FLAN-T5 XXL),\nshowing that stronger LLMs may benefit more\nfrom our framework.\n(2) The two FLAN-T5 LLMs in Tab. 1 benefit sig-\nnificantly from our QA4RE framework. Moreover,\nconsistent and substantial improvements can also\nbe observed in other FLAN-T5 models and the full\ntest set, as discussed in Sec. 6.3 and Appendix C.\nConsidering that relation extraction tasks account\nfor less than 0.5% of the instruction tasks used\nto train FLAN-T5 models, these findings strongly\nsupport our hypothesis that aligning underrepre-\nsented tasks with more common instruction-tuning\ntasks, such as QA, unlocks LLMs’ ability to solve\nlow-frequency tasks.\n(3) The SemEval dataset poses a significant chal-\nlenge for all baselines given its lack of type-\nconstraints, particularly for SuRE (Lu et al., 2022).\nWith such a large search space, generative LMs\nwithout fine-tuning tend to summarize all exam-\nples into NoTA relation, resulting in its system-\natic failure. It should be noted that without type\nconstraints, the RE problem becomes a 19-choice\n798\nquestion answering task in our QA4RE framework.\nDespite this, our method still demonstrates sub-\nstantial improvements for LLMs, particularly for\ntext-davinci-003 and FLAN-T5 XXL.\n5.2 Robustness to Verbalization Templates\nFor our experiments, we utilize manually written\nrelation templates from previous work (Sainz et al.,\n2021; Lu et al., 2022). However, Lu et al. (2022)\nnote that model performance may vary significantly\nwith template design. Thus, to investigate the ro-\nbustness of models to different templates, thorough\nexperiments are conducted with four different tem-\nplates, described in detail in Appendix B.3, across\nall zero-shot methods on the TACRED dataset.\nTab. 2 shows results comparing these four tem-\nplates on all methods used in our main experiments,\nincluding vanilla RE as a template-free reference.\nMethods TEMP1 T EMP2 T EMP3 T EMP4\nNLIBART 51.4 49.7 4.4 42.0\nNLIRoBERTa 50.1 47.1 19.6 35.8\nNLIDeBERTa 55.0 49.4 17.1 36.6\nSuREBART 19.9 20.4 2.1 10.1\nSuREPEGASUS 20.5 21.8 6.2 19.3\ntext-003Vanilla 48.1\nQA4RE 56.6 59.4 48.7 50.1\nTable 2: F1 score on TACRED with four templates (%).\nThe best result using each template is marked in bold.\ntext-003 refers to text-davinci-003.\nFrom Tab. 2, we observe the following:\n(1) Our method consistently outperforms small LM\nbaselines and the vanilla RE framework, regardless\nof the template. It is worth noting that even with\ntemplates that are constructed with label name in-\nformation only and no expert knowledge (TEMP 3\nand TEMP 4), our QA framework still performs bet-\nter than vanilla RE, indicating the effectiveness and\nconsistency of our QA framework.\n(2) NLI and SuRE performance is largely template\ndependent. When using carefully crafted high-\nquality templates (T EMP 1 and T EMP 2), several\nLM-based NLI methods outperform text-davinci-\n003 with vanilla RE. However, when equipped\nwith templates created without expert knowledge\n(TEMP 3 and TEMP 4), the performance of both NLI\nand SuRE deteriorates dramatically. In contrast,\nQA4RE is more robust to variation in verbalization\ntemplates, reducing trial-and-error development ef-\nforts as well as making it more readily transferred\nto settings where obtaining quality templates may\nbe limited due to the high cost of expert annota-\ntions, such as the biomedical or financial domains.\n5.3 None-of-the-Above Relation Evaluation\nThe none-of-the-above (NoTA) relation (Gao et al.,\n2019; Sabo et al., 2021; Jimenez Gutierrez et al.,\n2022) is defined as the case where no rela-\ntion of interest exists between the given entities.\nJimenez Gutierrez et al. (2022) demonstrate that\nthe earlier inferior performance of LLMs on RE\ntasks can be largely attributed to their inability to\nhandle the NoTA relation. To evaluate the efficacy\nof zero-shot methods on NoTA relation, follow-\ning previous work (Fei and Liu, 2016; Shu et al.,\n2017; Sainz et al., 2021), we apply NoTA-included\nmacro F1 metric as well as micro and macro P vs.\nN (all positive relations vs. NoTA relation as binary\nclassification) F1 metrics.\nMethods Macro F1 Micro P vs. N Macro P vs. N\nNLIBART 49.8 75.9 71.1\nNLIRoBERTa 43.7 68.5 65.8\nNLIDeBERTa 55.0 75.6 72.3\nSuREBART 15.5 35.2 35.0\nSuREPEGASUS 14.9 32.4 31.5\ntext-003 Vanilla 45.3 72.8 69.5\nQA4RE 58.9 78.4 74.8\nTable 3: NoTA-included 42-class macro F1 as well\nas macro and micro P vs. N (all positive relations vs.\nNoTA) F1 on TACRED (%). The best result of each\nmetric is bolded. text-003 refers to text-davinci-003.\nMa and Mi are short for macro and micro, respectively.\nFrom Tab. 3, we observe that, when enhanced\nby our QA framework, text-davinci-003 achieves\nsignificant improvement in NoTA-included metrics,\noutperforming the small LM-based NLI methods.\nThis further demonstrates the effectiveness of our\nframework, even in handling the challenging NoTA\nrelation. It is worth noting that these superior re-\nsults are achieved by simply adding an entity-filled\nNoTA relation template as an answer option for\nQA, without the additional thresholding require-\nments of previous methods (Sainz et al., 2021; Lu\net al., 2022). This eliminates the need for additional\nhyperparameter searching, which can be tricky for\nlow-resource settings.\n5.4 Few-Shot Results\nWhile zero-shot RE is our main focus, we also\nevaluate our method under the few-shot setting.\nResults are shown in Tab. 4. Due to budget lim-\nitations, we restrict our case study to the 4-shot\nscenario (i.e., 4 labeled examples per relation) with\n799\nthe best-performing LLM in the zero-shot setting\n(text-davinci-003). After determining the optimal\nnumber of in-context examples searched on the dev\nset, we randomly select the examples with the same\nentity type constraints from the given train set.\nInterestingly, vanilla RE is unable to obtain any\nimprovement from labeled examples, suggesting\nthat it is also limited in the few-shot setting. The\nlimited performance shown by vanilla RE indicates\nthat few-shot demonstrations might bias the model\ntowards incorrect relations in the context rather\nthan helping it perform the task more accurately.\nMethods K=0 K=4 K=8 K=16 K=32\nFine-Tuning - 9.0 21.2 29.3 33.9\nPTR - 26.8 30.0 32.9 36.8\nKnowPrompt - 30.2 33.7 34.9 35.0\nNLIDeBERTa-TEMP1 55.0 64.2 64.7 58.7 65.7\nNLIDeBERTa-TEMP2 49.4 51.2 47.3 50.5 48.1\nVanilla 48.1 46.2 -\nQA4RE 59.4 62.0 -\nTable 4: Few-shot F1 on TACRED (%). All results are\naveraged over 3 different training subsets for each K.\nWe use text-davinci-003 for vanilla RE and QA4RE. For\nthe best-performing baseline (NLI) as well as vanilla\nRE and QA4RE, we mark the results in bold when they\nare improved over their zero-shot alternatives.\nEven employing our QA4RE framework, the\nfew-shot text-davinci-003 does not outperform the\nDeBERTa-based NLI method (Sainz et al., 2021)\nwhen using their own templates (T EMP 1). How-\never, fine-tuning the NLI model on RE data can\nbe brittle even with careful hyperparameter tuning,\nas evidenced by the unstable gains seen as more\ndata is added for both T EMP 1 and T EMP 2. Fur-\nthermore, we find that few-shot NLI results when\nusing TEMP 2 drop substantially from TEMP 1, sug-\ngesting that this approach also lacks robustness to\ntemplates in the few-shot setting. Thus, consider-\ning that our QA approach enables LLMs to obtain\nfew-shot improvements over zero-shot results using\nrandom in-context learning example selection, ob-\ntains only around 2% lower performance than the\nbest NLI model, and is robust to different template\ndesigns, our approach is competitive on few-shot\nRE and has the potential to achieve even stronger\nperformance with more exploration. We leave fur-\nther investigation on how to improve LLMs for\nfew-shot RE to future work.\nVanilla + Template REGiven a sentence, and two entities within the sentence, classify the relationship between the two entities based on the provided sentence. All possible relationships are listed below:-per:city_of_birth: Entity 1 was born in the city Entity 2-per:city_of_death: Entity 1 died in the city Entity 2-per:cities_of_residence: Entity 1 lives in the city Entity 2-no_relation: Entity 1 has no known relations to Entity 2Sentence: Wearing jeans and a white blouse, Amanda Knox of Seattle is being cross-examined by prosecutors.Entity 1 : Amanda KnoxEntity 2 : SeattleRelationship:per:city_of_birth\nFigure 3: The same example and templates as Fig. 2 but\nusing templates for relation explanations.\n6 Discussions\n6.1 Are Relation Templates All LLMs Need?\nWe conduct an ablation study to better understand\nhow relation templates contribute to the perfor-\nmance improvement obtained by QA4RE. As il-\nlustrated in Fig. 3, we fill the relation verbalization\ntemplates with markers Entity 1 and Entity 2 as\nrelation explanations, thereby presenting the expert\nknowledge from the templates to the LLM. Using\nthe same templates and type constraints, we com-\npare this framework (termed Vanilla+TEMP ) with\nvanilla RE and QA4RE on the TACRED dataset\nand GPT-3.5 series LLMs.\nAs shown in Tab. 5, introducing relation expla-\nnations using the same templates does not result\nin consistent or significant performance improve-\nment. In fact, adding extra information to the task\ninstruction might make it more challenging for the\nLLM to understand the task. In contrast, using\nour QA4RE framework, we do not need to sep-\narately specify the entities of interest or relation\nexplanations; they are both naturally embedded in\nthe answer options. These ablation results show\nthat the gains from QA4RE mainly come from the\nQA reformulation, not simply from the relation\nexplanations/templates.\n6.2 QA4RE vs. NLI4RE\nGiven the strong performance obtained by small\nLMs using the NLI reformulation of RE, we lever-\nage this same formulation (Sainz et al., 2021) for\nLLMs (termed NLI4RE). 5 More concretely, for\neach example, we use the LLM to predict whether\n5We follow the NLI format from ANLI (Wang et al., 2022).\n800\nMethods P R F1 ∆F1\ncode-002\nVanilla 27.2 70.1 39.2 -\nVanilla + TEMP 27.5 71.8 39.7 +0.5\nQA4RE 37.7 65.4 47.8 +8.6\ntext-002\nVanilla 31.2 73.1 43.7 -\nVanilla + TEMP 26.8 77.8 39.8 −3.9\nQA4RE 35.6 68.4 46.8 +3.1\ntext-003\nVanilla 36.9 68.8 48.1 -\nVanilla + TEMP 36.9 76.5 49.8 +1.7\nQA4RE 47.7 78.6 59.4 +11.3\nTable 5: Evaluation on TACRED regarding whether\nincorporating relation explanations based on the same\ntemplates into vanilla RE bridges its gap to QA4RE (%).\nthe given sentence (the premise) entails each an-\nswer option from the QA4RE formulation (the hy-\npothesis). We allow the LLM to generate entail-\nment, neutral, or contradiction for each sentence-\nrelation pair. If the maximum probability of entail-\nment among all possible positive relations is below\nthe threshold of 0.5, the example will be classified\nas NoTA, as done by Sainz et al. (2021).\nFormulation RED RERED REV Eval Avg.\nVanilla 48.1 55.3 51.0 36.0 47.6\nNLI4RE 41.7 36.8 39.2 22.4 35.0\nQA4RE 59.4 61.2 59.4 43.3 55.8\nTable 6: F1 of text-davinci-003 with different task for-\nmulations (%). RED, RERED, REV , and Eval are short\nfor TACRED, RETACRED, TACREV , and SemEval\ndatasets, respectively.\nAs shown in Tab. 6, when using the NLI formula-\ntion, text-davinci-003 surprisingly underperforms\nthe vanilla RE formulation. The reason for its poor\nperformance is two-fold: (1) The heuristically pre-\ndefined threshold 0.5 is not ideal for LLMs and thus\nmany positive predictions are classified as NoTA.\nHowever, it is also difficult to find a good threshold\nunder the zero-shot setting. (2) Under NLI4RE,\nunlike vanilla RE or QA4RE, an LLM is not seeing\nthe full relation space but assigning probabilities to\neach candidate hypothesis individually. The final\nprediction is thus more sensitive to the LLM’s bias\nover different relations.\nNLI4RE also requires multiple inference runs for\neach relation example to evaluate all the candidate\nrelations, incurring a significantly higher cost.\n6.3 QA4RE & Model Size\nTo verify the effectiveness and transferability of our\nQA4RE framework on smaller instruction-tuned\nmodels, we further evaluate the FLAN-T5 Small\nLMs Model Size Avg. F1\nVanilla QA4RE ∆\nGPT-3.5 Series\ntext-001 175B 22.3 14.9 −7.4\ncode-002 175B 39.8 43.9 +4.1\ntext-002 175B 43.2 45.9 +2.7\ntext-003 175B 47.6 55.8 +8.2\nFLAN-T5 Series\nSmall 80M 19.5 25.0 +5.6\nBase 250M 22.3 26.4 +4.2\nLarge 780M 34.8 41.8 +7.0\nXLarge 3B 46.6 54.3 +7.7\nXXLarge 11B 46.5 55.1 +8.6\nTable 7: Effectiveness of QA4RE on both the GPT-3.5\nseries and FLAN-T5 with different sizes. The results\nare averaged over four RE datasets.\n(80M), Base (250M), and Large (780M) on the\nfull test set over four RE datasets. Tab. 7 shows\nour QA4RE framework can still bring consider-\nable gains to instruction-tuned models with various\nsizes, even for the smallest one (80M). This demon-\nstrates the effectiveness of QA4RE is transferable\nacross various model sizes from 80M to 175B, con-\nsidering the consistent improvements of QA4RE\non several GPT-3.5 models.\nIn the FLAN-T5 series, larger models benefit\nmore from our framework. However, we note that\nthis trend does not continue when scaling up to\nmuch larger GPT-3.5 models. In fact, all GPT-\n3.5 models except for text-davinci-003 benefit less\nfrom QA4RE than FLAN-T5 models. The smaller\nimprovements of QA4RE on these models make\ntheir overall RE performance only comparable with\nmodels that are approximately 20 and 50 times\nsmaller. This indicates that the wide variety of\nalignment strategies used by the GPT-3.5 series\nmodels discussed in Sec. 2 might not be universally\nmore effective than standard instruction-tuning for\nimproving model generalization on low-incidence\ntasks even when aligned to high incidence ones.\nNevertheless, the strong improvement observed\nin the strongest models tested, text-davinci-003\nand FLAN-T5-XXL, demonstrates the potential\nfor QA4RE’s effectiveness to continue as models\nbecome even more capable in the future.\n7 Conclusions and Future Work\nIn this work, we first show that even the most recent\ninstruction-tuned LLMs underperform fine-tuned\nsmall LMs on the relation extraction (RE) task.\nTo address this limitation, we reformulate RE into\nmultiple-choice question answering (QA) with the\npurpose of leveraging a task that is widely cov-\n801\nered in instruction-tuning datasets like QA, instead\nof RE, which is barely present in these datasets.\nComprehensive experiments demonstrate that our\nQA4RE framework unlocks the power of LLMs as\nzero-shot relation extractors, especially for two re-\ncent LLMs (text-davinci-003 and FLAN-T5 XXL).\nWe also conduct thorough experiments to explore\nthe robustness and few-shot effectiveness of our\nmethod as well as study in what LLM training sce-\nnarios it is most effective.\nIn future work, we hope to explore additional\nunderrepresented tasks in instruction-tuning that\nmight be challenging for LLMs and could be\nsuccessfully aligned with more widely adopted\ninstruction-tuning tasks like QA. Additionally, we\nplan to continue exploring this line of work by\nleveraging our QA4RE framework for other LLMs\nsuch as the OPT-series (Zhang et al., 2022; Iyer\net al., 2022) and PaLM (Chowdhery et al., 2022),\nwhich are not included in this work due to the lim-\nited computational resources and/or access.\n8 Limitations\nEven though our method helps unleash the power\nof six recent strong LLMs as zero-shot relation ex-\ntractors, earlier LLMs without strong instruction\ntuning such as text-davinci-001 saw no improve-\nments from our framework. Additionally, although\nwe carry out comprehensive experiments on the\nzero-shot RE setting, our few-shot exploration is\nmore limited. It is still unclear from our investi-\ngation whether including even more training ex-\namples can improve LLM’s RE performance and\nto what extent the same trends seen across GPT-\n3 models in the zero-shot setting hold steady in\nthe few-shot setting. We leave answering these\nquestions for future work.\n9 Ethics Statement\nIn this work, we propose a method to improve LLM\nperformance on the important and fundamental task\nof relation extraction. We do not anticipate any\nethical issues regarding the topics of this research.\nAcknowledgements\nThe authors would like to thank Renze Lou, col-\nleagues from the OSU NLP group, and the anony-\nmous reviewers for their valuable feedback. The\nauthors would also like to thank Keming Lu for\ndiscussions and guidance on reproducing SuRE.\nThis research was supported in part by NSF OAC\n2112606, NIH R01LM014199, and Ohio Super-\ncomputer Center (Center, 1987).\nReferences\nChristoph Alt, Aleksandra Gabryszak, and Leonhard\nHennig. 2020. TACRED revisited: A thorough eval-\nuation of the TACRED relation extraction task. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 1558–\n1569, Online. Association for Computational Linguis-\ntics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nOhio Supercomputer Center. 1987. Ohio supercomputer\ncenter.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan,\nHenrique Ponde de Oliveira Pinto, Jared Kaplan,\nHarrison Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, Alex Ray, Raul Puri, Gretchen\nKrueger, Michael Petrov, Heidy Khlaaf, Girish Sas-\ntry, Pamela Mishkin, Brooke Chan, Scott Gray,\nNick Ryder, Mikhail Pavlov, Alethea Power, Lukasz\nKaiser, Mohammad Bavarian, Clemens Winter,\nPhilippe Tillet, Felipe Petroski Such, Dave Cum-\nmings, Matthias Plappert, Fotios Chantzis, Eliza-\nbeth Barnes, Ariel Herbert-V oss, William Hebgen\nGuss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie\nTang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,\nWilliam Saunders, Christopher Hesse, Andrew N.\nCarr, Jan Leike, Joshua Achiam, Vedant Misra, Evan\nMorikawa, Alec Radford, Matthew Knight, Miles\nBrundage, Mira Murati, Katie Mayer, Peter Welinder,\nBob McGrew, Dario Amodei, Sam McCandlish, Ilya\nSutskever, and Wojciech Zaremba. 2021. Evaluat-\ning large language models trained on code. CoRR,\nabs/2107.03374.\nXiang Chen, Ningyu Zhang, Xin Xie, Shumin Deng,\nYunzhi Yao, Chuanqi Tan, Fei Huang, Luo Si, and\nHuajun Chen. 2022. Knowprompt: Knowledge-\naware prompt-tuning with synergistic optimization\nfor relation extraction. In WWW ’22: The ACM Web\nConference 2022, Virtual Event, Lyon, France, April\n25 - 29, 2022, pages 2778–2788. ACM.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\n802\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways. CoRR, abs/2204.02311.\nHyung Won Chung, Le Hou, Shayne Longpre, Barret\nZoph, Yi Tay, William Fedus, Eric Li, Xuezhi Wang,\nMostafa Dehghani, Siddhartha Brahma, Albert Web-\nson, Shixiang Shane Gu, Zhuyun Dai, Mirac Suz-\ngun, Xinyun Chen, Aakanksha Chowdhery, Sharan\nNarang, Gaurav Mishra, Adams Yu, Vincent Y . Zhao,\nYanping Huang, Andrew M. Dai, Hongkun Yu, Slav\nPetrov, Ed H. Chi, Jeff Dean, Jacob Devlin, Adam\nRoberts, Denny Zhou, Quoc V . Le, and Jason Wei.\n2022. Scaling instruction-finetuned language models.\nCoRR, abs/2210.11416.\nGeli Fei and Bing Liu. 2016. Breaking the closed world\nassumption in text classification. In NAACL HLT\n2016, The 2016 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, San Diego\nCalifornia, USA, June 12-17, 2016, pages 506–514.\nThe Association for Computational Linguistics.\nHao Fu, Yao; Peng and Tushar Khot. 2022. How does\ngpt obtain its ability? tracing emergent abilities of\nlanguage models to their sources. Yao Fu’s Notion.\nTianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2019. Fewrel 2.0:\nTowards more challenging few-shot relation classi-\nfication. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Nat-\nural Language Processing, EMNLP-IJCNLP 2019,\nHong Kong, China, November 3-7, 2019, pages 6249–\n6254. Association for Computational Linguistics.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and\nMaosong Sun. 2022. Ptr: Prompt tuning with rules\nfor text classification. AI Open, 3:182–192.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: decoding-enhanced\nbert with disentangled attention. In 9th International\nConference on Learning Representations, ICLR 2021,\nVirtual Event, Austria, May 3-7, 2021 . OpenRe-\nview.net.\nIris Hendrickx, Su Nam Kim, Zornitsa Kozareva,\nPreslav Nakov, Diarmuid Ó Séaghdha, Sebastian\nPadó, Marco Pennacchiotti, Lorenza Romano, and\nStan Szpakowicz. 2010. SemEval-2010 task 8: Multi-\nway classification of semantic relations between pairs\nof nominals. In Proceedings of the 5th International\nWorkshop on Semantic Evaluation, pages 33–38, Up-\npsala, Sweden. Association for Computational Lin-\nguistics.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Daniel Simig, Ping Yu, Kurt Shuster,\nTianlu Wang, Qing Liu, Punit Singh Koura, Xian Li,\nBrian O’Horo, Gabriel Pereyra, Jeff Wang, Christo-\npher Dewan, Asli Celikyilmaz, Luke Zettlemoyer,\nand Ves Stoyanov. 2022. OPT-IML: scaling language\nmodel instruction meta learning through the lens of\ngeneralization. CoRR, abs/2212.12017.\nBernal Jimenez Gutierrez, Nikolas McNeal, Clayton\nWashington, You Chen, Lang Li, Huan Sun, and\nYu Su. 2022. Thinking about GPT-3 in-context learn-\ning for biomedical IE? think again. In Findings of\nEMNLP, pages 4497–4512, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871–7880.\nAssociation for Computational Linguistics.\nPercy Liang, Rishi Bommasani, Tony Lee, Dimitris\nTsipras, Dilara Soylu, Michihiro Yasunaga, Yian\nZhang, Deepak Narayanan, Yuhuai Wu, Ananya Ku-\nmar, Benjamin Newman, Binhang Yuan, Bobby Yan,\nCe Zhang, Christian Cosgrove, Christopher D. Man-\nning, Christopher R’e, Diana Acosta-Navas, Drew A.\nHudson, E. Zelikman, Esin Durmus, Faisal Ladhak,\nFrieda Rong, Hongyu Ren, Huaxiu Yao, Jue Wang,\nKeshav Santhanam, Laurel J. Orr, Lucia Zheng,\nMert Yuksekgonul, Mirac Suzgun, Nathan S. Kim,\nNeel Guha, Niladri S. Chatterji, O. Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael\nXie, Shibani Santurkar, Surya Ganguli, Tatsunori\nHashimoto, Thomas F. Icard, Tianyi Zhang, Vishrav\nChaudhary, William Wang, Xuechen Li, Yifan Mai,\nYuhui Zhang, and Yuta Koreeda. 2022. Holistic eval-\nuation of language models. ArXiv, abs/2211.09110.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022. What\nmakes good in-context examples for gpt-3? In Pro-\nceedings of Deep Learning Inside Out: The 3rd Work-\nshop on Knowledge Extraction and Integration for\nDeep Learning Architectures, DeeLIO@ACL 2022,\nDublin, Ireland and Online, May 27, 2022 , pages\n100–114. Association for Computational Linguistics.\n803\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nKeming Lu, I-Hung Hsu, Wenxuan Zhou,\nMingyu Derek Ma, and Muhao Chen. 2022.\nSummarization as indirect supervision for relation\nextraction. In Findings of EMNLP, pages 6575–6594,\nAbu Dhabi, United Arab Emirates. Association for\nComputational Linguistics.\nSewon Min, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. 2022a. Noisy channel language\nmodel prompting for few-shot text classification. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), ACL 2022, Dublin, Ireland, May 22-27,\n2022, pages 5316–5330. Association for Computa-\ntional Linguistics.\nSewon Min, Mike Lewis, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2022b. MetaICL: Learning to learn\nin context. In Proceedings of the 2022 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, pages 2791–2809, Seattle, United States.\nAssociation for Computational Linguistics.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul F. Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback. CoRR, abs/2203.02155.\nEthan Perez, Douwe Kiela, and Kyunghyun Cho. 2021.\nTrue Few-Shot Learning with Language Models.\nOfer Sabo, Yanai Elazar, Yoav Goldberg, and Ido Da-\ngan. 2021. Revisiting few-shot relation classification:\nEvaluation data and classification schemes. Trans.\nAssoc. Comput. Linguistics, 9:691–706.\nOscar Sainz, Oier Lopez de Lacalle, Gorka Labaka, An-\nder Barrena, and Eneko Agirre. 2021. Label verbal-\nization and entailment for effective zero and few-shot\nrelation extraction. In Proceedings of EMNLP, pages\n1199–1212, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Arun Raja, Manan Dey,\nM Saiful Bari, Canwen Xu, Urmish Thakker,\nShanya Sharma Sharma, Eliza Szczechla, Taewoon\nKim, Gunjan Chhablani, Nihal V . Nayak, Debajyoti\nDatta, Jonathan Chang, Mike Tian-Jian Jiang, Han\nWang, Matteo Manica, Sheng Shen, Zheng Xin Yong,\nHarshit Pandey, Rachel Bawden, Thomas Wang, Tr-\nishala Neeraj, Jos Rozen, Abheesht Sharma, An-\ndrea Santilli, Thibault Févry, Jason Alan Fries, Ryan\nTeehan, Teven Le Scao, Stella Biderman, Leo Gao,\nThomas Wolf, and Alexander M. Rush. 2022. Multi-\ntask prompted training enables zero-shot task gener-\nalization. In The Tenth International Conference on\nLearning Representations, ICLR 2022, Virtual Event,\nApril 25-29, 2022. OpenReview.net.\nLei Shu, Hu Xu, and Bing Liu. 2017. DOC: deep open\nclassification of text documents. In Proceedings of\nthe 2017 Conference on Empirical Methods in Nat-\nural Language Processing, EMNLP 2017, Copen-\nhagen, Denmark, September 9-11, 2017, pages 2911–\n2916. Association for Computational Linguistics.\nGeorge Stoica, Emmanouil Antonios Platanios, and\nBarnabás Póczos. 2021. Re-tacred: Addressing short-\ncomings of the TACRED dataset. In Thirty-Fifth\nAAAI Conference on Artificial Intelligence, AAAI\n2021, Thirty-Third Conference on Innovative Ap-\nplications of Artificial Intelligence, IAAI 2021, The\nEleventh Symposium on Educational Advances in Ar-\ntificial Intelligence, EAAI 2021, Virtual Event, Febru-\nary 2-9, 2021, pages 13843–13850. AAAI Press.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al. 2022.\nSuper-naturalinstructions:generalization via declara-\ntive instructions on 1600+ tasks. In EMNLP.\nJason Wei, Maarten Bosma, Vincent Y . Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M. Dai, and Quoc V . Le. 2022a. Finetuned\nlanguage models are zero-shot learners. In The Tenth\nInternational Conference on Learning Representa-\ntions, ICLR 2022, Virtual Event, April 25-29, 2022.\nOpenReview.net.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Ed H. Chi, Quoc Le, and Denny Zhou. 2022b.\nChain of thought prompting elicits reasoning in large\nlanguage models. CoRR, abs/2201.11903.\nAdina Williams, Nikita Nangia, and Samuel Bowman.\n2018. A broad-coverage challenge corpus for sen-\ntence understanding through inference. In Proceed-\nings of the 2018 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long Papers), pages 1112–1122. Association for\nComputational Linguistics.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Peter\nLiu. 2020. PEGASUS: Pre-training with extracted\ngap-sentences for abstractive summarization. In Pro-\nceedings of the 37th International Conference on\nMachine Learning, volume 119 of Proceedings of\nMachine Learning Research , pages 11328–11339.\nPMLR.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher\nDewan, Mona T. Diab, Xian Li, Xi Victoria Lin,\n804\nTodor Mihaylov, Myle Ott, Sam Shleifer, Kurt Shus-\nter, Daniel Simig, Punit Singh Koura, Anjali Srid-\nhar, Tianlu Wang, and Luke Zettlemoyer. 2022.\nOPT: open pre-trained transformer language mod-\nels. CoRR, abs/2205.01068.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D. Manning. 2017. Position-aware\nattention and supervised data improve slot filling. In\nProceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, EMNLP\n2017, Copenhagen, Denmark, September 9-11, 2017,\npages 35–45. Association for Computational Linguis-\ntics.\n805\nA Instruction Dataset Portion\n#Tasks %RE %QA\nT0 (Sanh et al., 2022) 62 0 27.4\nFLAN (Wei et al., 2022a) 62 0 21\nMetaICL (Min et al., 2022b) 142 0 28.9\nNaturalInstruct (Wang et al., 2022) 1731<0.5 >12\nTable 9: Popular instruction tuning datasets and propor-\ntion of RE and QA tasks in each.\nAs shown in Tab. 9, there is no RE task in T0 (Sanh\net al., 2022), FLAN (Wei et al., 2022a), and\nMetaICL (Min et al., 2022b) instruction tuning\ndatasets. Even in the largest available NaturalIn-\nstruct (Wang et al., 2022), RE tasks consist of only\nless than 0.5% of the total tasks. By contrast, QA\nis the most popular task format in all instruction\ntuning datasets. These observations indicate the\nlow incidence of RE tasks and the dominance of\nQA tasks in datasets used for instruction tuning.\nB Experimental Details\nB.1 Hyperparameters for Few-Shot Methods\nIn the few-shot setting, for each K, we randomly\nsample 3 times to obtain different training subsets,\neach of which will be used as in-context demonstra-\ntions for LLMs or used to train the small language\nmodels in baselines. Report results are averaged\nover the three subsets. To avoid over-estimating\nfew-shot performance with too many dev exam-\nples (Perez et al., 2021), we use 100 randomly\nselected examples of dev set for all the hyperpa-\nrameter searching.\nFor LLMs, we use the dev set to search for the\noptimal number of in-context examples as a hy-\nperparameter from {1,2,5}. Then we randomly\nselect the same type-constrained in-context exam-\nples from the given train set.\nFor all small LM-based baselines, we use their\npublicly available code and hyper-parameters for\ntraining. According to the original papers of\nNLI (Sainz et al., 2021) and SuRE (Lu et al., 2022),\nwe use the checkpoints available online and hyper-\nparameters reported for model training. Unfortu-\nnately, we were unable to reproduce SuRE results\nwith default hyperparameters. For standard Fine-\nTuning (Jimenez Gutierrez et al., 2022), PTR (Han\net al., 2022), and KnowPrompt (Chen et al., 2022),\nwe perform a grid search over hyperparameters on\ndev with the range shown in Tab. 10.\nWe use 8 NVIDIA GeForce RTX 2080 Ti and\n2 NVIDIA RTX A6000 to conduct all the experi-\nments. The total GPU hours used and the cost for\nOpenAI API are listed in Tab. 11.\nHyperparameter Search Space\nLearning Rate 1: { 1e−5, 3e−5}\nWeight Decay: { 0.01, 0.001}\nLearning Rate 2: { 5e−5, 2e−4}\nTable 10: Hyperparameters used for grid search of few-\nshot methods. Learning Rate 2 is used for training new\ntokens in PTR (Han et al., 2022) and virtual tokens in\nKnowPrompt (Chen et al., 2022).\nNum of Params Total GPU Total\n(Millions) Hours Cost\nRoBERTa-Large 354 284 -\nDeBERTa-XLarge 900 14 -\nBART-Large 406 2 -\nPegasus-Large 568 50 -\nFLAN-T5 S 80 <1 -\nFLAN-T5 M 250 <1 -\nFLAN-T5 L 780 1 -\nFLAN-T5 XL 3,000 2 -\nFLAN-T5 XXL 11,000 4 -\nOpenAI Text API175,000 - $ 835\nOpenAI Chat API ? - $ 4\nTable 11: Total GPU Hours for open sources LMs and\ncost for using OpenAI API (all version included).\nB.2 Prompts for LLMs\nAs shown in Tab. 12, we list all templates used\nin this paper including vanilla + T EMP in Tab. 5,\nNLI4RE in Tab. 6, and vanilla as well as QA4RE\nin all experiments.\nB.3 Relation Verbalization Templates\nIn the relation verbalization template robustness\nexperiment shown in Tab. 2, the differences be-\ntween four templates are described below using\nthe org:top_members/employees relation from TA-\nCRED benchmark as an example:\n1. Concrete Examples: {Eh} is a chairman/\npresident/director of {Et}\n2. Semantic Relationship: {Eh} is a high level\nmember of {Et}\n3. Straightforward: The relation between {Eh}\nand {Et} is top members or employees\n4. Word Translation: {Eh} organization top\nmembers or employees {Et}\n806\nMethods TACRED RETACRED TACREV SemEval Avg.\nP R F1 P R F1 P R F1 P R F1 F1\nSmall Vanilla9.5 40.9 15.4 22.8 50.2 31.3 9.1 41.9 15.0 10.0 11.8 10.8 18.1\nQA4RE13.8 52.2 21.8 (+6.4) 33.5 66.2 44.5 (+13.2) 13.7 55.2 22.0 (+7.0) 5.9 7.1 6.4 ( −4.4) 23.7 (+5.6)\nBase Vanilla14.1 31.1 19.4 21.1 26.8 23.6 14.1 33.3 19.8 14.9 17.9 16.2 19.8\nQA4RE17.1 54.7 26.0 (+6.6) 33.0 65.2 43.8 (+20.2) 17.2 58.5 26.6 (+6.8) 6.7 8.0 7.3 ( −8.9) 25.9 (+6.2)\nLarge Vanilla22.8 58.6 32.8 37.5 60.8 46.4 22.6 61.9 33.1 23.7 19.7 21.5 33.5\nQA4RE30.3 78.5 43.7 (+10.9) 44.5 72.6 55.2 (+8.8) 29.9 82.4 43.9 (+10.8) 24.8 15.8 19.3 (−2.2) 40.5 (+7.1)\nXLarge Vanilla48.8 49.0 48.9 55.8 39.8 46.4 52.0 55.7 53.8 34.9 29.6 32.0 45.3\nQA4RE37.6 78.650.9 (+2.0) 56.2 79.9 66.0 (+19.6) 38.2 84.7 52.7 (−1.1) 44.4 39.9 42.1 (+10.1)52.9 (+7.7)\nXXLargeVanilla48.2 45.3 46.7 56.1 53.7 54.9 50.6 50.6 50.6 29.2 28.1 28.6 45.2\nQA4RE38.1 82.952.2(+5.5) 55.9 82.0 66.5(+11.6) 38.3 88.153.4 (+2.8) 40.2 47.5 43.5(+14.9)53.9(+8.7)\nTable 8: FLAN-T5 results on full test set of four RE datasets (%). We mark the best results in bold, the second-best\nunderlined, and F1 improvement of our QA4RE over vanilla RE in green.\nThe first set of templates was written by Sainz et al.\n(2021), while the remaining three were explored\nby Lu et al. (2022). We use the templates from\ntheir official GitHub repositories.6 In addition, we\nfurther list relation verbalization templates used\nby all LLMs in our paper in Tab. 13, Tab. 14, and\nTab. 15.\nC Full Test Results on FLAN-T5\nWe present the full test set results of all four RE\ndatasets in Tab. 8. Our observations align with the\nfindings from experiments on 1,000 test examples:\n(1) Our QA4RE framework can bring consistent\nand significant improvements over all FLAN-T5\nseries models on the averaged results. Additionally,\nlarger models benefit more from our framework.\nThese two signals strongly demonstrate the effec-\ntiveness of QA4RE.\n(2) We notice that our QA4RE does not improve\nsmaller versions of FLAN-T5 on SemEval, a 19-\nchoice QA task. This may be due that these models\nhave difficulties in understanding long input fed by\nQA4RE.\n6Templates for Robustness Experiments:\nTEMP 1: https://github.com/osainz59/Ask2Transformers/blob/\nmaster/resources/predefined_configs/tacred.relation.config.json\nTEMP 3: https://github.com/luka-group/SuRE/blob/main/data\ntemplates/tacred/rel2temp_forward.json\nTEMP 4: https://github.com/luka-group/SuRE/blob/main/data\n/templates/tacred/rel2temp_raw_relation.json\n807\nFormulations Prompts\nVanilla RE\nGiven a sentence, and two entities within the sentence, classify the relationship between the two entities based\non the provided sentence. All possible Relationships are listed below:\n- [Possible Relation 1]\n- [Possible Relation 2]\n- [NoTA Relation]\nSentence: [SentenceS]\nEntity 1: [Head EntityEh]\nEntity 2: [Tail EntityEt]\nRelationship:\nVanilla + TEMP\nGiven a sentence, and two entities within the sentence, classify the relationship between the two entities based\non the provided sentence. All possible Relationships are listed below with explanations:\n- [Possible Relation 1]: [Relation 1 Template]\n- [Possible Relation 2]: [Relation 2 Template]\n- [NoTA Relation]: [NoTA Relation Template]\nSentence: [SentenceS]\nEntity 1: [Head EntityEh]\nEntity 2: [Tail EntityEt]\nRelationship:\nNLI4RE\nIn this task, you will be presented with a premise and a hypothesis sentence.\nDetermine whether the hypothesis sentence entails (implies), contradicts (opposes), or is neutral with respect\nto the given premise sentence. Please answer with \"Contradiction\", \"Neutral\", or \"Entailment\".\nPremise: [SentenceS]\nHypothesis: [Entities in Relation 1 Template]\nCategory:\nQA4RE\nDetermine which option can be inferred from the given Sentence.\nSentence: [SentenceS]\nOptions:\nA. [Entities in Relation 1 Template]\nB. [Entities in Relation 2 Template]\nC. [Entities in NoTA Relation Template]\nWhich option can be inferred from the given Sentence?\nOption:\nTable 12: Prompt Formats of frameworks for LLMs in this paper. We only demonstrate NLI4RE with 1 template for\nsimplicity.\n808\nRelation Template\nno_relation { Eh} has no known relations to {Et}\nper:stateorprovince_of_death { Eh} died in the state or province {Et}\nper:title { Eh} is a {Et}\norg:member_of { Eh} is the member of {Et}\nper:other_family { Eh} is the other family member of {Et}\norg:country_of_headquarters { Eh} has a headquarter in the country {Et}\norg:parents { Eh} has the parent company {Et}\nper:stateorprovince_of_birth { Eh} was born in the state or province {Et}\nper:spouse { Eh} is the spouse of {Et}\nper:origin { Eh} has the nationality {Et}\nper:date_of_birth { Eh} has birthday on {Et}\nper:schools_attended { Eh} studied in {Et}\norg:members { Eh} has the member {Et}\norg:founded { Eh} was founded in {Et}\nper:stateorprovinces_of_residence { Eh} lives in the state or province {Et}\nper:date_of_death { Eh} died in the date {Et}\norg:shareholders { Eh} has shares hold in {Et}\norg:website { Eh} has the website {Et}\norg:subsidiaries { Eh} owns {Et}\nper:charges { Eh} is convicted of {Et}\norg:dissolved { Eh} dissolved in {Et}\norg:stateorprovince_of_headquarters { Eh} has a headquarter in the state or province {Et}\nper:country_of_birth { Eh} was born in the country {Et}\nper:siblings { Eh} is the siblings of {Et}\norg:top_members/employees { Eh} has the high level member {Et}\nper:cause_of_death { Eh} died because of {Et}\nper:alternate_names { Eh} has the alternate name {Et}\norg:number_of_employees/members { Eh} has the number of employees {Et}\nper:cities_of_residence { Eh} lives in the city {Et}\norg:city_of_headquarters { Eh} has a headquarter in the city {Et}\nper:children { Eh} is the parent of {Et}\nper:employee_of { Eh} is the employee of {Et}\norg:political/religious_affiliation { Eh} has political affiliation with {Et}\nper:parents { Eh} has the parent {Et}\nper:city_of_birth { Eh} was born in the city {Et}\nper:age { Eh} has the age {Et}\nper:countries_of_residence { Eh} lives in the country {Et}\norg:alternate_names { Eh} is also known as {Et}\nper:religion { Eh} has the religion {Et}\nper:city_of_death { Eh} died in the city {Et}\nper:country_of_death { Eh} died in the country {Et}\norg:founded_by { Eh} was founded by {Et}\nTable 13: Templates for TACRED and TACREV datasets.\n809\nRelation Template\nno_relation { Eh} has no known relations to {Et}\nper:religion { Eh} has the religion {Et}\norg:country_of_branch { Eh} has a branch in the country {Et}\norg:stateorprovince_of_branch { Eh} has a branch in the state or province {Et}\norg:city_of_branch { Eh} has a branch in the city {Et}\norg:shareholders { Eh} has shares hold in {Et}\norg:top_members/employees { Eh} has the high level member {Et}\norg:members { Eh} has the member {Et}\norg:website { Eh} has the website {Et}\nper:parents { Eh} has the parent {Et}\norg:number_of_employees/members { Eh} has the number of employees {Et}\norg:political/religious_affiliation { Eh} has political affiliation with {Et}\nper:age { Eh} has the age {Et}\nper:origin { Eh} has the nationality {Et}\norg:alternate_names { Eh} is also known as {Et}\nper:other_family { Eh} is the other family member of {Et}\nper:identity { Eh} is the identity/pronoun of {Et}\nper:identity { Eh} and {Et} are the same person\nper:siblings { Eh} is the siblings of {Et}\norg:member_of { Eh} is the member of {Et}\nper:children { Eh} is the parent of {Et}\nper:employee_of { Eh} is the employee of {Et}\nper:spouse { Eh} is the spouse of {Et}\norg:dissolved { Eh} dissolved in {Et}\nper:schools_attended { Eh} studied in {Et}\nper:country_of_death { Eh} died in the country {Et}\nper:stateorprovince_of_death { Eh} died in the state or province {Et}\nper:city_of_death { Eh} died in the city {Et}\nper:date_of_death { Eh} died in the date {Et}\nper:cause_of_death { Eh} died because of {Et}\norg:founded { Eh} was founded in {Et}\norg:founded_by { Eh} was founded by {Et}\nper:countries_of_residence { Eh} lives in the country {Et}\nper:stateorprovinces_of_residence { Eh} lives in the state or province {Et}\nper:cities_of_residence { Eh} lives in the city {Et}\nper:country_of_birth { Eh} was born in the country {Et}\nper:stateorprovince_of_birth { Eh} was born in the state or province {Et}\nper:city_of_birth { Eh} was born in the city {Et}\nper:date_of_birth { Eh} has birthday on {Et}\nper:charges { Eh} is convicted of {Et}\nper:title { Eh} is a {Et}\nTable 14: Templates for RETACRED datasets.\nRelation Template\nOther {subj} has no known relations to {obj}\nComponent-Whole(e1,e2) {subj} is the component of {obj}\nComponent-Whole(e2,e1) {obj} is the component of {subj}\nInstrument-Agency(e1,e2) {subj} is the instrument of {obj}\nInstrument-Agency(e2,e1) {obj} is the instrument of {subj}\nMember-Collection(e1,e2) {subj} is the member of {obj}\nMember-Collection(e2,e1) {obj} is the member of {subj}\nCause-Effect(e1,e2) {subj} has the effect {obj}\nCause-Effect(e2,e1) {obj} has the effect {subj}\nEntity-Destination(e1,e2) {obj} is the destination of {subj}\nEntity-Destination(e2,e1) {subj} is the destination of {obj}\nContent-Container(e1,e2) {obj} contains {subj}\nContent-Container(e2,e1) {subj} contains {obj}\nMessage-Topic(e1,e2) {obj} is the topic of {subj}\nMessage-Topic(e2,e1) {subj} is the topic of {obj}\nProduct-Producer(e1,e2) {obj} produces {subj}\nProduct-Producer(e2,e1) {subj} produces {obj}\nEntity-Origin(e1,e2) {subj} origins from {obj}\nEntity-Origin(e2,e1) {obj} origins from {subj}\nTable 15: Templates for SemEval datasets.\n810\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 8.\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Our work helps LLM solve the relation extraction tasks, we don’t anticipate any\nrisks.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nBefore Section 1 (abstract) and Section 1 (introduction).\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nGrammarly. Grammar check for sections 1-8.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3, Section 4.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 4.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nFor artifacts we used in the paper, they have licenses in the public GitHub repos.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nFor artifacts we used in the paper, they have licenses in the public GitHub repos. We are following\nthe standard use of these artifacts. Our code will be released under the same license.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nThese datasets are widely used as relation extraction benchmarks in the research ﬁeld and as far as\nwe know, no previous work has reported offensive or sensitive content in these datasets.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nOur code focuses on using OpenAI API for a speciﬁc task, relation extraction, and we only have\ntested our code on standard English benchmarks for the relation extraction task.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 4.1\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n811\nC □\u0013 Did you run computational experiments?\nSections 5 and 6.\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix B.1\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 4.2 and Appendix B.1\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 4.2 Experimental Setup and Section 5.4. Averaged results over multiple runs are reported.\n□\u0017 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNo, we use the standard data format without extra processing and we use ofﬁcial GitHub repos for\nbaseline comparison.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n812"
}