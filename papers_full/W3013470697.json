{
  "title": "Actor-Transformers for Group Activity Recognition",
  "url": "https://openalex.org/W3013470697",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A4288956243",
      "name": "Gavrilyuk, Kirill",
      "affiliations": [
        "University of Amsterdam"
      ]
    },
    {
      "id": "https://openalex.org/A4288922426",
      "name": "Sanford, Ryan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4288922430",
      "name": "Javan, Mehrsan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222043054",
      "name": "Snoek, Cees G. M.",
      "affiliations": [
        "University of Amsterdam"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6725062358",
    "https://openalex.org/W6639102338",
    "https://openalex.org/W6759455113",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2016053056",
    "https://openalex.org/W1983364832",
    "https://openalex.org/W2963246338",
    "https://openalex.org/W2778252923",
    "https://openalex.org/W2047499569",
    "https://openalex.org/W6664757799",
    "https://openalex.org/W2940963663",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W1514535095",
    "https://openalex.org/W2605111198",
    "https://openalex.org/W2259801182",
    "https://openalex.org/W2895064504",
    "https://openalex.org/W2034014085",
    "https://openalex.org/W2927778007",
    "https://openalex.org/W289401892",
    "https://openalex.org/W6640754710",
    "https://openalex.org/W6743837088",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W1947746128",
    "https://openalex.org/W2963722382",
    "https://openalex.org/W2554408731",
    "https://openalex.org/W6730028046",
    "https://openalex.org/W2916798096",
    "https://openalex.org/W6751936687",
    "https://openalex.org/W2736442062",
    "https://openalex.org/W6724944384",
    "https://openalex.org/W2139857301",
    "https://openalex.org/W2791307013",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W6607974698",
    "https://openalex.org/W1972696612",
    "https://openalex.org/W6664172992",
    "https://openalex.org/W2962899219",
    "https://openalex.org/W2081314772",
    "https://openalex.org/W2796633859",
    "https://openalex.org/W2911109671",
    "https://openalex.org/W2269938945",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2508429489",
    "https://openalex.org/W2779380177",
    "https://openalex.org/W6679434410",
    "https://openalex.org/W2558630670",
    "https://openalex.org/W6732681475",
    "https://openalex.org/W6754693662",
    "https://openalex.org/W1744759976",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W6682864246",
    "https://openalex.org/W100367037",
    "https://openalex.org/W2964134613",
    "https://openalex.org/W2894669491",
    "https://openalex.org/W2963377215",
    "https://openalex.org/W6684983439",
    "https://openalex.org/W6640257725",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W6763509872",
    "https://openalex.org/W1912967058",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2964233791",
    "https://openalex.org/W2914120296",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2579154055",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2556782416",
    "https://openalex.org/W1923404803",
    "https://openalex.org/W1950788856",
    "https://openalex.org/W2952122856",
    "https://openalex.org/W2057067088",
    "https://openalex.org/W2950059857",
    "https://openalex.org/W2053619738",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2507009361",
    "https://openalex.org/W2963750390",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2510185399",
    "https://openalex.org/W2894087839",
    "https://openalex.org/W196214544",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2806331055",
    "https://openalex.org/W2599765304"
  ],
  "abstract": "This paper strives to recognize individual actions and group activities from videos. While existing solutions for this challenging problem explicitly model spatial and temporal relationships based on location of individual actors, we propose an actor-transformer model able to learn and selectively extract information relevant for group activity recognition. We feed the transformer with rich actor-specific static and dynamic representations expressed by features from a 2D pose network and 3D CNN, respectively. We empirically study different ways to combine these representations and show their complementary benefits. Experiments show what is important to transform and how it should be transformed. What is more, actor-transformers achieve state-of-the-art results on two publicly available benchmarks for group activity recognition, outperforming the previous best published results by a considerable margin.",
  "full_text": "Actor-Transformers for Group Activity Recognition\nKirill Gavrilyuk1∗ Ryan Sanford2 Mehrsan Javan2 Cees G. M. Snoek1\n1University of Amsterdam 2Sportlogiq\n{kgavrilyuk,cgmsnoek}@uva.nl {ryan.sanford, mehrsan}@sportlogiq.com\nAbstract\nThis paper strives to recognize individual actions and\ngroup activities from videos. While existing solutions for\nthis challenging problem explicitly model spatial and tem-\nporal relationships based on location of individual actors,\nwe propose an actor-transformer model able to learn and\nselectively extract information relevant for group activ-\nity recognition. We feed the transformer with rich actor-\nspeciﬁc static and dynamic representations expressed by\nfeatures from a 2D pose network and 3D CNN, respectively.\nWe empirically study different ways to combine these rep-\nresentations and show their complementary beneﬁts. Ex-\nperiments show what is important to transform and how it\nshould be transformed. What is more, actor-transformers\nachieve state-of-the-art results on two publicly available\nbenchmarks for group activity recognition, outperforming\nthe previous best published results by a considerable mar-\ngin.\n1. Introduction\nThe goal of this paper is to recognize the activity of an\nindividual and the group that it belongs to [11]. Consider\nfor example a volleyball game where an individual player\njumps and the group is performing a spike. Besides sports,\nsuch group activity recognition has several applications in-\ncluding crowd monitoring, surveillance and human behav-\nior analysis. Common tactics to recognize group activities\nexploit representations that model spatial graph relations\nbetween individual actors (e.g. [27, 45, 60]) and follow ac-\ntors and their movements over time (e.g. [28, 45, 48]). The\nmajority of previous works explicitly model these spatial\nand temporal relationships based on the location of the ac-\ntors. We propose an implicit spatio-temporal model for rec-\nognizing group activities.\nWe are inspired by progress in natural language process-\ning (NLP) tasks, which also require temporal modeling to\ncapture the relationship between words over time. Typi-\n∗This paper is the product of work during an internship at Sportlogiq.\nEmbedding\nEmbedding\nEmbedding\nTransformer\nGroup Activity\nFigure 1: We explore two complementary static and dy-\nnamic actor representations for group activity recognition.\nThe static representation is captured by 2D pose features\nfrom a single frame while the dynamic representation is ob-\ntained from multiple RGB or optical ﬂow frames. These\nrepresentations are processed by a transformer that infers\ngroup activity.\ncally, recurrent neural networks (RNN) and their variants\n(long short-term memory (LSTM) and gated recurrent unit\n(GRU)) were the ﬁrst choices for NLP tasks [8, 41, 52].\nWhile designed to model a sequence of words over time,\nthey experience difﬁculty modeling long sequences [14].\nMore recently, the transformer network [55] has emerged\nas a superior method for NLP tasks [15, 17, 33, 62] since it\nrelies on a self-attention mechanism that enables it to better\nmodel dependencies across words over time without a re-\ncurrent or recursive component. This mechanism allows the\nnetwork to selectively extract the most relevant information\nand relationships. We hypothesize a transformer network\ncan also better model relations between actors and com-\nbine actor-level information for group activity recognition\ncompared to models that require explicit spatial and tem-\nporal constraints. A key enabler is the transformer’s self-\nattention mechanism, which learns interactions between the\nactors and selectively extracts information that is important\nfor activity recognition. Therefore, we do not rely on any\na priori spatial or temporal structure like graphs [45, 60] or\n1\narXiv:2003.12737v1  [cs.CV]  28 Mar 2020\nmodels based on RNNs [16, 28]. We propose transformers\nfor recognizing group activities.\nBesides introducing the transformer in group activity\nrecognition, we also pay attention to the encoding of in-\ndividual actors. First, by incorporating simple yet effec-\ntive positional encoding [55]. Second, by explicit modeling\nof static and dynamic representations of the actor, which\nis illustrated in Figure 1. The static representation is cap-\ntured by pose features that are obtained by a 2D pose net-\nwork from a single frame. The dynamic representation is\nachieved by a 3D CNN taking as input the stacked RGB or\noptical ﬂow frames similar to [2]. This representation en-\nables the model to capture the motion of each actor without\nexplicit temporal modeling via RNN or graphical models.\nMeanwhile, the pose network can easily discriminate be-\ntween actions with subtle motion differences. Both types of\nfeatures are passed into a transformer network where rela-\ntions are learned between the actors enabling better recog-\nnition of the activity of the group. We refer to our approach\nas actor-transformers. Finally, given that static and dynamic\nrepresentations capture unique, but complimentary, infor-\nmation, we explore the beneﬁt of aggregating this informa-\ntion through different fusion strategies.\nWe make three contributions in this paper. First, we in-\ntroduce the transformer network for group activity recog-\nnition. It reﬁnes and aggregates actor-level features, with-\nout the need for any explicit spatial and temporal model-\ning. Second, we feed the transformer with a rich static and\ndynamic actor-speciﬁc representation, expressed by fea-\ntures from a 2D pose network and 3D CNN. We empiri-\ncally study different ways to combine these representations\nand show their complementary beneﬁts. Third, our actor-\ntransformers achieve state-of-the-art results on two publicly\navailable benchmarks for group activity recognition, the\nCollective [11] and V olleyball [28] datasets, outperforming\nthe previous best published results [2, 60] by a considerable\nmargin.\n2. Related Work\n2.1. Video action recognition\nCNNs for video action recognition.While 2D convolu-\ntional neural networks (CNN) have experienced enormous\nsuccess in image recognition, initially they could not be di-\nrectly applied to video action recognition, because they do\nnot account for time, which is vital information in videos.\nKarpathy et al. [31] proposed 2D CNNs to process individ-\nual frames and explored different fusion methods in an ef-\nfort to include temporal information. Simonyan and Zisser-\nman [49] employed a two-stream CNN architecture that in-\ndependently learns representations from input RGB image\nand optical ﬂow stacked frames. Wang et al. [57] proposed\nto divide the video into several segments and used a multi-\nstream approach to model each segment with their combi-\nnation in a learnable way. Many leveraged LSTMs to model\nlong-term dependencies across frames [18, 37, 42, 47]. Jiet\nal. [30] were the ﬁrst to extend 2D CNN to 3D, where time\nwas the third dimension. Tran et al. [53] demonstrated the\neffectiveness of 3D CNNs by training on a large collection\nof noisy labeled videos [31]. Carreira and Zisserman [7]\ninﬂated 2D convolutional ﬁlters to 3D, exploiting training\non large collections of labeled images and videos. The re-\ncent works explored leveraging feature representation of the\nvideo learned by 3D CNNs and suggesting models on top of\nthat representation [26, 59]. Wang and Gupta [59] explored\nspatio-temporal graphs while Hussein et al. [26] suggested\nmulti-scale temporal convolutions to reason over minute-\nlong videos. Similarly, we also rely on the representation\nlearned by a 3D CNN [7] to capture the motion and tempo-\nral features of the actors. Moreover, we propose to fuse this\nrepresentation with the static representation of the actor-\npose to better capture exact positions of the actor’s body\njoints.\nAttention for video action recognition.Originally pro-\nposed for NLP tasks [4] attention mechanisms have also\nbeen applied to image caption generation [61]. Several\nstudies explored attention for video action recognition by\nincorporating attention via LSTM models [37, 47], pool-\ning methods [22, 40] or graphs [59]. Attention can also be\nguided through different modalities, such as pose [5, 19]\nand motion [37]. More recently, transformer networks [55]\nhave received special recognition due to the self-attention\nmechanism that can better capture long-term dependencies,\ncompared to RNNs. Integrating the transformer network for\nvisual tasks has also emerged [21, 44]. Parmar et al. [44]\ngeneralized the transformer to an image generation task,\nwhile Girdhar et al. [21] created a video action transformer\nnetwork on top of a 3D CNN representation [7] for action\nlocalization and action classiﬁcation. Similarly, we explore\nthe transformer network as an approach to reﬁne and ag-\ngregate actor-level information to recognize the activity of\nthe whole group. However, we use representations of all\nactors to create query, key and values to reﬁne each individ-\nual actor representation and to infer group activity, while\n[21] used only one person box proposal for query and clip\naround the person for key and values to predict the person’s\naction.\nPose for video action recognition.Most of the human\nactions are highly related to the position and motion of body\njoints. This has been extensively explored in the literature,\nincluding hand-crafted pose features [29, 43, 56], skeleton\ndata [20, 25, 39, 46, 50], body joint representation [6, 8]\nand attention guided by pose [5, 19]. However, these ap-\nproaches were only trained to recognize an action for one\nindividual actor, which does not generalize well to inferring\ngroup activity. In our work we explore the fusion of the\n2\npose features with dynamic representations, following the\nmulti-stream approach [13, 54, 63] for action recognition,\nbut we leverage it to infer group activity.\n2.2. Group activity recognition\nGroup activity recognition has recently received more at-\ntention largely due to the introduction of the public Col-\nlective dataset [11] and V olleyball dataset [28]. Initially,\nmethods relied on hand-crafted features extracted for each\nactor, which were then processed by probabilistic graphi-\ncal models [1, 9, 10, 12, 23, 34, 35]. With the emergence\nof deep learning, the performance of group activity recog-\nnition has steadily increased. Some of the more successful\napproaches utilized RNN-type networks. Ibrahim et al. [28]\nused LSTM to model the action dynamics of individual ac-\ntors and aggregate the information to predict group activ-\nity. Deng et al. [16] integrated graphical models with RNN.\nShu et al. [48] used a two-level hierarchy of LSTMs that si-\nmultaneously minimized the energy of the predictions while\nmaximizing the conﬁdence. Bagautdinov et al. [3] jointly\ndetected every actor in a video, predicted their actions and\nthe group activity by maintaining temporal consistency of\nbox proposals with the help of RNN. Wang et al. [58] uti-\nlizes single person dynamics, intra-group and inter-group\ninteractions with LSTM-based model. Li and Chuah [36]\ntook an alternative approach, where captions were gener-\nated for every video frame and then were used to infer group\nactivity. Ibrahim and Mori [27] created a relational repre-\nsentation of each person which is then used for multi-person\nactivity recognition. Qi et al. [45] proposed an attentive se-\nmantic RNN that utilized spatio-temporal attention and se-\nmantic graphs to capture inter-group relationships. Lately,\nstudies have been moving away from RNNs. Azar et al. [2]\nused intermediate representations called activity maps, gen-\nerated by a CNN, to iteratively reﬁne group activity predic-\ntions. Wu et al. [60] built an actor relation graph using a\n2D CNN and graph convolutional networks to capture both\nthe appearance and position relations between actors. Like\nWu et al. [60] we also rely on actor-level representations\nbut differently, we utilize the self-attention mechanism that\nhas the ability to selectively highlight actors and group rela-\ntions, without explicitly building any graph. Moreover, we\nenrich actor features by using static and dynamic represen-\ntations. Similarly to [2] we build our dynamic representa-\ntion with a 3D CNN.\n3. Model\nThe goal of our method is to recognize group activity in\na multi-actor scene through enhancement and aggregation\nof individual actor features. We hypothesize that the self-\nattention mechanism provided by transformer networks is a\nﬂexible enough model that can be successfully used out-of-\nthe-box, without additional tricks or tweaks, for the infer-\nence of the activity of the whole group given the represen-\ntation of each actor.\nOur approach consists of three main stages presented in\nFigure 2: actor feature extractor, group activity aggregation\nand fusion. In brief, the input to our model is a sequence of\nvideo frames Ft,t = 1,..,T with N actor bounding boxes\nprovided for each frame where T is the number of frames.\nWe obtain the static and the dynamic representation of each\nactor by applying a 2D pose network on a single frame and\na 3D CNN on all input frames. The dynamic representa-\ntion can be built from RGB or optical ﬂow frames, which\nare processed by a 3D CNN followed by a RoIAlign [24]\nlayer. Next, actor representations are embedded into a sub-\nspace such that each actor is represented by a 1-dimensional\nvector. In the second stage, we apply a transformer network\non top of these representations to obtain the action-level fea-\ntures. These features are max pooled to capture the activity-\nlevel features. A linear classiﬁer is used to predict indi-\nvidual actions and group activity using the action-level and\ngroup activity-level features, respectively. In the ﬁnal stage\nwe introduce fusion strategies before and after the trans-\nformer network to explore the beneﬁt of fusing information\nacross different representations. We describe each stage in\nmore details in the following subsections.\n3.1. Actor feature extractor\nAll human actions involve the motion of body joints,\nsuch as hands and legs. This applies not only to ﬁne-grained\nactions that are performed in sports activities (e.g. spike and\nset in volleyball) but also to every day actions such aswalk-\ning and talking. This means that it is important to capture\nnot only the position of joints but their temporal dynamics\nas well. For this purpose, we utilize two distinct backbone\nmodels to capture both position and motion of joints and\nactors themselves.\nTo obtain joints positions a pose estimation model is ap-\nplied. It receives as input a bounding box around the ac-\ntor and predicts the location of key joints. Our approach\nis independent of the particular choice of the pose estima-\ntion model. We select the recently published HRNet [51]\nas our pose network as it has a relatively simple design,\nwhile achieving state-of-the-art results on pose estimation\nbenchmarks. We use the features from the last layer of\nthe network, right before the ﬁnal classiﬁcation layer, in all\nour experiments. Speciﬁcally, we use the smallest network\npose hrnet w32 trained on COCO key points [38], which\nshows good enough performance for our task as well.\nThe second backbone network is responsible for mod-\neling the temporal dynamics. Several studies have demon-\nstrated that 3D CNNs, with enough available data for train-\ning [53, 7], can build strong spatio-temporal representations\nfor action recognition. Accordingly, we utilize the I3D [7]\nnetwork in our framework since the pose network alone can\n3\nFigure 2: Overview of the proposed model.An input video with T frames and N actor bounding boxes is processed by\ntwo branches: static and dynamic. The static branch outputs an HRNet [51] pose representation for each actor bounding\nbox. The dynamic branch relies on I3D [7], which receives as input either stacked RGB or optical ﬂow frames. To extract\nactor-level features after I3D we apply a RoIAlign [24] layer. A transformer encoder ( E) reﬁnes and aggregates actor-level\nfeatures followed by individual action and group activity classiﬁers. Two fusion strategies are supported. For early fusion we\ncombine actor-level features of the two branches before E, in the late fusion we combine the classiﬁer prediction scores.\nnot capture the motion of the joints from a single frame.\nThe I3D network processes stacked Ft,t = 1,..,T frames\nwith inﬂated 3d convolutions. We consider RGB and optical\nﬂow representations as they can capture different motion as-\npects. As 3D CNNs are computationally expensive we em-\nploy a RoIAlign [24] layer to extract features for each actor\ngiven Nbounding boxes around actors while processing the\nwhole input frames by the network only once.\n3.2. Transformer\nTransformer networks were originally introduced for\nmachine translation in [55]. The transformer network con-\nsists of two parts: encoder and decoder. The encoder re-\nceives an input sequence of words (source) that is processed\nby a stack of identical layers consisting of a multi-head\nself-attention layer and a fully-connected feed-forward net-\nwork. Then, a decoder generates an output sequence (tar-\nget) through the representation generated by the encoder.\nThe decoder is built in a similar way as the encoder having\naccess to the encoded sequence. The self-attention mech-\nanism is the vital component of the transformer network,\nwhich can also be successfully used to reason about actors’\nrelations and interactions. In the following section, we de-\nscribe the self-attention mechanism itself and how the trans-\nformer architecture can be applied to the challenging task of\ngroup activity recognition in video.\nAttention Ais a function that represents a weighted sum\nof the values V. The weights are computed by matching\na query Q with the set of keys K. The matching func-\ntion can have different forms, most popular is the scaled\ndot-product [55]. Formally, attention with the scaled dot-\nproduct matching function can be written as:\nA(Q,K,V ) =softmax(QKT\n√\nd\n)V (1)\nwhere dis the dimension of both queries and keys. In the\nself-attention module all three representations ( Q, K, V)\nare computed from the input sequence S via linear projec-\ntions so A(S) =A(Q(S),K(S),V (S)).\nSince attention is a weighted sum of all values it over-\ncomes the problem of forgetfulness over time, which is\nwell-studied for RNNs and LSTMs [14]. In sequence-to-\nsequence modeling this mechanism gives more importance\nto the most relevant words in the source sequence. This\nis a desirable property for group activity recognition as well\nbecause we can enhance the information of each actor’s fea-\ntures based on the other actors in the scene without any spa-\ntial constraints. Multi-head attention Ah is an extension of\nattention with several parallel attention functions using sep-\narate linear projections hi of (Q, K, V):\nAh(Q,K,V ) =concat(h1,...,h m)W, (2)\n4\nhi = A(QWQ\ni ,KW K\ni ,VW V\ni ) (3)\nTransformer encoder layer E consists of multi-head atten-\ntion combined with a feed-forward neural network L:\nL(X) =Linear(Dropout(ReLU(Linear(X))) (4)\nˆE(S) =LayerNorm(S+ Dropout(Ah(S))) (5)\nE(S) =LayerNorm( ˆE(S) +Dropout(L( ˆE(S)))) (6)\nThe transformer encoder can contain several of such layers\nwhich sequentially process an input S.\nIn our case S is a set of actors’ features S = {si|i =\n1,..,N }obtained by actor feature extractors. As features si\ndo not follow any particular order, the self-attention mech-\nanism is a more suitable model than RNN and CNN for\nreﬁnement and aggregation of these features. An alterna-\ntive approach can be incorporating a graph representation\nas in [60] which also does not rely on the order of the si.\nHowever, the graph representation requires explicit model-\ning of connections between nodes through appearance and\nposition relations. The transformer encoder mitigates this\nrequirement relying solely on the self-attention mechanism.\nHowever, we show that the transformer encoder can beneﬁt\nfrom implicitly employing spatial relations between actors\nvia positional encoding ofsi. We do so by representing each\nbounding box bi of the respective actor’s featuressi with its\ncenter point (xi,yi) and encoding the center point with the\nsame function PE as in [55]. To handle 2D space we en-\ncode xi with the ﬁrst half of dimensions of si and yi with\nthe second half. In this work we consider only the encoder\npart of the transformer architecture leaving the decoder part\nfor future work.\n3.3. Fusion\nThe work by Simonyan and Zisserman [49] demon-\nstrated the improvements in performance that can be ob-\ntained by fusing different modalities that contain compli-\nmentary information. Following their example, we also in-\ncorporate several modalities into one framework. We ex-\nplore two branches, static and dynamic. The static branch\nis represented by the pose network which captures the static\nposition of body joints, while the dynamic branch is repre-\nsented by I3D and is responsible for the temporal features of\neach actor in the scene. As RGB and optical ﬂow can cap-\nture different aspects of motion we study dynamic branches\nwith both representations of the input video. To fuse static\nand dynamic branches we explore two fusion strategies:\nearly fusion of actors’ features before the transformer net-\nwork and late fusion which aggregates predictions of clas-\nsiﬁers, similar to [49]. Early fusion enables access to both\nstatic and dynamic features before inference of group ac-\ntivity. Late fusion separately processes static and dynamic\nfeatures for group activity recognition and can concentrate\non static or dynamic features, separately.\n3.4. Training objective\nOur model is trained in an end-to-end fashion to simul-\ntaneously predict individual actions of each actor and group\nactivity. For both tasks we use a standard cross-entropy loss\nfor classiﬁcation and combine two losses in a weighted sum:\nL= λgLg(yg,˜yg) +λaLa(ya,˜ya) (7)\nwhere Lg,La are cross-entropy losses,yg and ya are ground\ntruth labels, ˜yg and ˜ya are model predictions for group ac-\ntivity and individual actions, respectively. λg and λa are\nscalar weights of the two losses. We ﬁnd that equal weights\nfor individual actions and group activity perform best so we\nset λg = λa = 1 in all our experiments, which we detail\nnext.\n4. Experiments\nIn this section, we present experiments with our pro-\nposed model. First, we introduce two publicly available\ngroup activity datasets, the V olleyball dataset [28] and the\nCollective dataset [11], on which we evaluate our approach.\nThen we describe implementation details followed by abla-\ntion study of the model. Lastly, we compare our approach\nwith the state-of-the-art and provide a deeper analysis of the\nresults. For simplicity, we call our static branch as “Pose”,\nthe dynamic branch with RGB frames as “RGB” and the\ndynamic branch with optical ﬂow frames as “Flow” in the\nfollowing sections.\n4.1. Datasets\nThe Volleyball dataset[28] consists of clips from 55\nvideos of volleyball games, which are split into two sets:\n39 training videos and 16 testing videos. There are 4830\nclips in total, 3493 training clips and 1337 clips for test-\ning. Each clip is 41 frames in length. Available annotations\nincludes group activity label, individual players’ bounding\nboxes and their respective actions, which are provided only\nfor the middle frame of the clip. Bagautdinov et al. [3] ex-\ntended the dataset with ground truth bounding boxes for the\nrest of the frames in clips which we are also using in our\nexperiments. The list of group activity labels contains four\nmain activities (set, spike, pass, winpoint) which are divided\ninto two subgroups, left and right, having eight group activ-\nity labels in total. Each player can perform one of nine indi-\nvidual actions: blocking, digging, falling, jumping, moving,\nsetting, spiking, standing and waiting.\nThe Collective dataset[11] consists of 44 clips with\nvarying lengths starting from 193 frames to around 1800\n5\nframes in each clip. Every 10th frame has the annotation\nof persons’ bounding boxes with one of ﬁve individual ac-\ntions: ( crossing, waiting, queueing, walking and talking.\nThe group activity is determined by the action that most\npeople perform in the clip. Following [45] we use 32 videos\nfor training and 12 videos for testing.\n4.2. Implementation details\nTo make a fair comparison with related works we use\nT = 10frames as the input to our model on both datasets:\nmiddle frame, 5 frames before and 4 frames after. For the\nV olleyball dataset we resize each frame to720 ×1280 res-\nolution, for the Collective to 480 ×720. During training we\nrandomly sample one frame Ftp from T input frames for\nthe pose network. During testing we use the middle frame\nof the input sequence. Following the conventional approach\nwe are also using ground truth person bounding boxes for\nfair comparison with related work. We crop person bound-\ning boxes from the frame Ftp and resize them to 256 ×192,\nwhich we process with the pose network obtaining actor-\nlevel features maps. For the I3D network, we use features\nmaps obtained from Mixed 4f layer after additional average\npooling over the temporal dimension. Then we resize the\nfeature maps to 90 ×160 and use the RoIAlign [24] layer to\nextract features of size 5 ×5 for each person bounding box\nin the middle frame of the input video. We then embed both\npose and I3D features to the vector space with the same di-\nmension d = 128. The transformer encoder uses dropout\n0.1 and the size of the linear layer in the feed-forward net-\nwork Lis set to 256.\nFor the training of the static branch we use a batch size of\n16 samples and for the dynamic branch we use a batch size\nof 8 samples. We train the model for 20,000 iterations on\nboth datasets. On the V olleyball dataset we use an SGD op-\ntimizer with momentum 0.9. For the ﬁrst 10,000 iterations\nwe train with the learning rate 0.01 and for the last 10,000\niterations with the learning rate 0.001. On the Collective\ndataset, the ADAM [32] optimizer with hyper-parameters\nβ1 = 0.9, β2 = 0.999 and ϵ = e−10 is used. Initially, we\nset the learning rate to 0.0001 and decrease it by a factor of\nten after 5,000 and 10,000 iterations. The code of our model\nwill be available upon publication.\n4.3. Ablation study\nWe ﬁrst perform an ablation study of our approach on\nthe V olleyball dataset [28] to show the inﬂuence of all three\nstages of the model. We use group activity accuracy as an\nevaluation metric in all ablations.\nActor-Transformer.We start with the exploration of pa-\nrameters of the actor-transformer. We experiment with the\nnumber of layers, number of heads and positional encod-\ning. Only the static branch represented by the pose net-\nwork is considered in this experiment. The results are re-\n# Layers # Heads Positional\nEncoding\nGroup\nActivity\n1 1 \u0017 91.0\n1 1 \u0013 92.3\n1 2 \u0013 91.4\n2 1 \u0013 92.1\nTable 1: Actor-Transformer ablation on the V olleyball\ndataset using static actor representation. Positional encod-\ning improves the strength of the representation. Adding ad-\nditional heads and layers did not materialize due to limited\nnumber of available training samples.\nMethod Static Dynamic\nPose RGB Flow\nBase Model 89.9 89.0 87.8\nGraph [60] 92.0 91.1 89.5\nActivity Maps [2] - 92.0 91.5\nActor-Transformer (ours) 92.3 91.4 91.5\nTable 2: Actor Aggregation ablation of person-level\nfeatures for group activity recognition on the V olleyball\ndataset. Our actor-transformer outperforms a graph while\nmatching the results of activity maps.\nported in Table 1. Positional encoding is a viable part giv-\ning around 1.3% improvement. This is expected as group\nactivity classes of the V olleyball dataset are divided into\ntwo subcategories according to the location of which the\nactivity is performed: left or right. Therefore, explicitly\nadding information about actors’ positions helps the trans-\nformer better reason about this part of the group activity.\nTypically, transformer-based language models beneﬁt from\nusing more layers and/or heads due to the availability of\nlarge datasets. However, the V olleyball dataset has a rela-\ntively small size and the transformer can not fully reach its\npotential with a larger model. Therefore we use one layer\nwith one head in the rest of the experiments.\nActor Aggregation. Next, we compare the actor-\ntransformer with two recent approaches that combine in-\nformation across actors to infer group activity. We use\na static single frame (pose) and dynamic multiple frames\n(I3D) models as a baseline. It follows our single branch\nmodel without using the actor-transformer part, by directly\napplying action and activity classiﬁers on actor-level fea-\ntures from the pose and the I3D networks. The ﬁrst related\nmethod uses relational graph representation to aggregate in-\nformation across actors [60]. We use the authors’ publicly\navailable code for the implementation of the graph model.\nWe also use an embedded dot-product function for the ap-\n6\nMethod Pose + RGB Pose + Flow\nEarly - summation 91.2 88.5\nEarly - concatenation 91.8 89.7\nLate 93.5 94.4\nTable 3: Fusion ablation of static and dynamic representa-\ntions on the V olleyball dataset. The late fusion outperforms\nthe early fusion approaches.\npearance relation and distance masking for the position re-\nlation, which performed best in [60]. For fair comparison,\nwe replace the actor-transformer with a graph and keep the\nother parts of our single branch models untouched. The sec-\nond related method is based on multiple reﬁnement stages\nusing spatial activity maps [2]. As we are using the same\nbackbone I3D network, we directly compare with the results\nobtained in [2]. The comparisons are reported in Table 2.\nOur actor-transformer outperforms the graph for all back-\nbone networks with good improvement for optical ﬂow fea-\ntures without explicitly building any relationship represen-\ntation. We match the results of activity maps [2] on optical\nﬂow and having slightly worse results on RGB. However,\nwe achieve these results without the need to convert bound-\ning box annotations into segmentation masks and without\nmultiple stages of reﬁnement.\nFusion. In the last ablation, we compare different fusion\nstrategies to combine the static and dynamic representations\nof our model. For the late fusion, we set the weight for the\nstatic representation to be twice as large as the weight for\nthe dynamic representation. The results are presented in\nTable 3. The early fusion is not beneﬁcial for our model,\nperforming similar or even worse than single branch mod-\nels. Early fusion strategies require the actor-transformer\nto reason about both static and dynamic features. Due to\nthe small size of the V olleyball dataset, our model can not\nfully exploit this type of fusion. Concentrating on each of\ntwo representations separately helps the model to better use\nthe potential of static and dynamic features. Despite Flow\nonly slightly outperforming RGB ( 91.5% vs. 91.4%), fu-\nsion with static representation has a bigger impact ( 93.9%\nvs. 93.1%) showing that Flow captures more complemen-\ntary information to Pose than RGB.\n4.4. Comparison with the state-of-the-art\nVolleyball dataset. Next, we compare our approach\nwith the state-of-the-art models on the V olleyball dataset in\nTable 4 using the accuracy metrics for group activity and\nindividual action predictions. We present two variations of\nour model, late fusion of Pose with RGB (Pose + RGB)\nand Pose with optical ﬂow (Pose + Flow). Both variations\nsurpass all the existing methods with a considerable mar-\ngin: 0.5% and 1.4% for group activity, 2.7% and 2.9% for\nMethod Backbone Group\nActivity\nIndividual\nAction\nIbrahimet al. [28] AlexNet 81.9 -\nShuet al. [48] VGG16 83.3 -\nQiet al. [45] VGG16 89.3 -\nIbrahim and Mori [27] VGG19 89.5 -\nBagautdinovet al. [3] Inception-v3 90.6 81.8\nWuet al. [60] Inception-v3 92.5 83.0\nAzaret al. [2] I3D 93.0 -\nOurs (RGB + Flow) I3D 93.0 83.7\nOurs (Pose + RGB) HRNet + I3D 93.5 85.7\nOurs (Pose + Flow) HRNet + I3D 94.4 85.9\nTable 4: Volleyball dataset comparisonfor individual ac-\ntion prediction and group activity recognition. Our Pose +\nFlow model outperforms the state-of-the-art.\nMethod Backbone Group\nActivity\nLanet al. [35] None 79.7\nChoi and Salvarese [9] None 80.4\nDenget al. [16] AlexNet 81.2\nIbrahimet al. [28] AlexNet 81.5\nHajimirsadeghiet al. [23] None 83.4\nAzaret al. [2] I3D 85.8\nLi and Chuah [36] Inception-v3 86.1\nShuet al. [48] VGG16 87.2\nQi et al. [45] VGG16 89.1\nWuet al. [60] Inception-v3 91.0\nOurs (RGB + Flow) I3D 92.8\nOurs (Pose + RGB) HRNet + I3D 91.0\nOurs (Pose + Flow) HRNet + I3D 91.2\nTable 5: Collective dataset comparisonfor group activ-\nity recognition. Our Pose + RGB and Pose + Flow models\nachieve the state-of-the-art results.\nindividual action recognition. It supports our hypothesis\nthat the transformer-based model with the static and dy-\nnamic actor representations is beneﬁcial for the group ac-\ntivity task. Moreover, we also compare the late fusion of\nRGB with optical ﬂow representation (RGB + Flow) and\nachieve the same group activity accuracy as in [2] which\nalso uses a backbone I3D network. However, we achieve\nthese results with a much simpler approach and without re-\nquiring any segmentation annotation. Combination of all\nthree representations gives the same performance as Pose +\nFlow showing that only using one dynamic representation\nis essential.\nCollective dataset. We further evaluate our model on\nthe Collective dataset and provide comparisons with pre-\nvious methods in Table 5. We use only group activity ac-\ncuracy as a metric following the same approach as the re-\n7\nFigure 3: Example of each actor attentionobtained by\nactor-transformers. Most attention is concentrated on the\nkey actor (5) who performs setting action which helps to\ncorrectly predict left set group activity. Best viewed in the\ndigital version.\nright set\nright spike\nright pass\nright winpoint\nleft set\nleft spike\nleft pass\nleft winpoint\n90.6 2.1 5.7 0.0 1.0 0.5 0.0 0.0\n2.3 94.8 1.2 0.6 0.0 0.6 0.6 0.0\n1.4 0.5 97.1 0.0 0.0 0.0 0.5 0.5\n0.0 0.0 0.0 92.0 0.0 0.0 0.0 8.0\n0.6 0.6 0.0 0.0 94.0 1.2 3.0 0.6\n0.6 1.1 0.0 0.0 2.8 94.4 1.1 0.0\n0.0 1.3 2.2 0.0 0.9 0.0 95.6 0.0\n0.0 0.0 0.0 4.9 0.0 0.0 0.0 95.1\nFigure 4: Volleyball dataset confusion matrixfor group\nactivity recognition. Our model achieves over 90% accu-\nracy for each group activity.\nlated work. Interestingly, our individual branches on the\nCollective dataset have much more variation in their perfor-\nmance than on the V olleyball dataset: Flow - 83.8%, Pose\n- 87.9%, RGB - 90.8%. However, with both fused models,\nPose + RGB and Pose + Flow, we achieve the state-of-the-\nart results, slightly outperforming the best published results\nof [60]. We also explore the fusion of RGB and Flow rep-\nresentations and ﬁnd that this combination performs best\non the Collective dataset reaching 92.8% accuracy. We hy-\npothesize that Pose and RGB representations capture sim-\nilar information that is complementary to the optical ﬂow\nrepresentation as supported by the results of Pose + RGB\nmodel which is just slightly better than RGB representa-\ntion alone. We also try to combine all three representations\nwithout receiving any additional improvement over RGB\n+ Flow. It is worth noting that with the same backbone\nI3D network Azar et al. [2] achieve 85.8% accuracy which\nis 7.0% lower that our results showing the beneﬁts of the\ntransformer-based model over their activity maps approach.\n4.5. Analysis\nTo analyze the beneﬁts of our actor-transformer we il-\nlustrate the attention of the transformer in Figure 3. Each\nCrossing\nWaiting\nQueueing\nWalking\nTalking\n83.3 2.2 0.0 14.5 0.0\n0.0 96.1 0.0 3.9 0.0\n0.0 0.0 100.0 0.0 0.0\n9.6 1.4 0.9 88.1 0.0\n0.0 0.0 0.0 0.0 100.0\nFigure 5: Collective dataset confusion matrixfor group\nactivity recognition. Most confusion comes form distin-\nguishing crossing and walking.\nrow of the matrix on the right represents the distribution of\nattention Ah in equation 2 using the representation of the\nactor with the number of the row as a query. For most ac-\ntors the transformer concentrates mostly on the key actor\nwith number 5 of the left set group activity who performs\na setting action. To further understand the performance of\nour model we also present confusion matrices for group ac-\ntivity recognition on the V olleyball dataset in Figure 4 and\nthe Collective dataset in Figure 5. For every group activity\non the V olleyball dataset our model achieves accuracy over\n90% with the least accuracy forright set class (90.6%). The\nmost confusion emerges from discriminating set, spike and\npass between each other despite their spatial location, left\nor right. Also, the model struggles to distinguish between\nright winpoint and left winpoint. On the Collective dataset,\nour approach reaches perfect recognition for queueing and\ntalking classes. However, two activities, crossing and walk-\ning, lead to the most confusion for our model. Several\nworks [58, 2] argue that crossing and walking are naturally\nthe same activity as they only differ by the relation between\nperson and street. Integrating global scene-level informa-\ntion potentially can help to distinguish these two activities,\nwhich we leave for future work.\n5. Conclusion\nWe proposed a transformer-based network as a reﬁne-\nment and aggregation module of actor-level features for the\ntask of group activity recognition. We show that without\nany task-speciﬁc modiﬁcations the transformer matches or\noutperforms related approaches optimized for group activ-\nity recognition. Furthermore, we studied static and dy-\nnamic representations of the actor, including several ways to\ncombine these representations in an actor-transformer. We\nachieve the state-of-the-art on two publicly available bench-\nmarks surpassing previously published results by a consid-\nerable margin.\n8\nReferences\n[1] Mohammed Abdel Rahman Amer, Peng Lei, and Sinisa\nTodorovic. Hirf: Hierarchical random ﬁeld for collective ac-\ntivity recognition in videos. In ECCV, 2014. 3\n[2] Sina Mokhtarzadeh Azar, Mina Ghadimi Atigh, Ahmad\nNickabadi, and Alexandre Alahi. Convolutional relational\nmachine for group activity recognition. In CVPR, 2019. 2,\n3, 6, 7, 8\n[3] Timur M. Bagautdinov, Alexandre Alahi, Franois Fleuret,\nPascal Fua, and Silvio Savarese. Social scene understand-\ning: End-to-end multi-person action localization and collec-\ntive activity recognition. In CVPR, 2017. 3, 5, 7\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. In ICLR, 2014. 2\n[5] Fabien Baradel, Christian Wolf, and Julien Mille. Human\nactivity recognition with pose-driven attention to rgb. In\nBMVC, 2018. 2\n[6] Congqi Cao, Yifan Zhang, Chunjie Zhang, and Hanqing Lu.\nAction recognition with joints-pooled 3d deep convolutional\ndescriptors. In IJCAI, 2016. 2\n[7] Jo ˜ao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In CVPR,\n2017. 2, 3, 4\n[8] Guilhem Ch ´eron, Ivan Laptev, and Cordelia Schmid. P-cnn:\nPose-based cnn features for action recognition. In ICCV,\n2015. 1, 2\n[9] Wongun Choi and Silvio Savarese. A uniﬁed framework for\nmulti-target tracking and collective activity recognition. In\nECCV, 2012. 3, 7\n[10] Wongun Choi and Silvio Savarese. Understanding collec-\ntive activitiesof people from videos. IEEE Transactions on\nPattern Analysis and Machine Intelligence , 36:1242–1257,\n2014. 3\n[11] Wongun Choi, Khuram Shahid, and Silvio Savarese. What\nare they doing? : Collective activity classiﬁcation using\nspatio-temporal relationship among people. In ICCV Work-\nshops, 2009. 1, 2, 3, 5\n[12] Wongun Choi, Khuram Shahid, and Silvio Savarese. Learn-\ning context for collective activity recognition. In CVPR,\n2011. 3\n[13] Vasileios Choutas, Philippe Weinzaepfel, J ´erˆome Revaud,\nand Cordelia Schmid. Potion: Pose motion representation\nfor action recognition. In CVPR, 2018. 3\n[14] Jasmine Collins, Jascha Sohl-Dickstein, and David Sussillo.\nCapacity and trainability in recurrent neural networks. arXiv\npreprint arXiv:1611.09913, 2016. 1, 4\n[15] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime G. Carbonell,\nQuoc V . Le, and Ruslan Salakhutdinov. Transformer-xl: At-\ntentive language models beyond a ﬁxed-length context. In\nACL, 2019. 1\n[16] Zhiwei Deng, Arash Vahdat, Hexiang Hu, and Greg Mori.\nStructure inference machines: Recurrent neural networks for\nanalyzing relations in group activity recognition. In CVPR,\n2016. 2, 3, 7\n[17] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. In NAACL-HLT, 2019.\n1\n[18] Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Sub-\nhashini Venugopalan, Sergio Guadarrama, Kate Saenko, and\nTrevor Darrell. Long-term recurrent convolutional networks\nfor visual recognition and description. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 39:677–691,\n2014. 2\n[19] Wenbin Du, Yali Wang, and Yu Qiao. Rpan: An end-to-\nend recurrent pose-attention network for action recognition\nin videos. In ICCV, 2017. 2\n[20] Yong Du, Wei Wang, and Liang Wang. Hierarchical recur-\nrent neural network for skeleton based action recognition. In\nCVPR, 2015. 2\n[21] Rohit Girdhar, Jo ˜ao Carreira, Carl Doersch, and Andrew Zis-\nserman. Video action transformer network. In CVPR, 2019.\n2\n[22] Rohit Girdhar and Deva Ramanan. Attentional pooling for\naction recognition. In NIPS, 2017. 2\n[23] Hossein Hajimirsadeghi, Wang Yan, Arash Vahdat, and Greg\nMori. Visual recognition by counting instances: A multi-\ninstance cardinality potential kernel. In CVPR, 2015. 3, 7\n[24] Kaiming He, Georgia Gkioxari, Piotr Doll ´ar, and Ross B.\nGirshick. Mask r-cnn. In ICCV, 2017. 3, 4, 6\n[25] Yonghong Hou, Zhaoyang Li, Pichao Wang, and Wanqing\nLi. Skeleton optical spectra-based action recognition using\nconvolutional neural networks. IEEE Transactions on Cir-\ncuits and Systems for Video Technology, 28:807–811, 2018.\n2\n[26] Noureldien Hussein, Efstratios Gavves, and Arnold WM\nSmeulders. Timeception for complex action recognition. In\nCVPR, 2019. 2\n[27] Mostafa S. Ibrahim and Greg Mori. Hierarchical relational\nnetworks for group activity recognition and retrieval. In\nECCV, 2018. 1, 3, 7\n[28] Mostafa S. Ibrahim, Srikanth Muralidharan, Zhiwei Deng,\nArash Vahdat, and Greg Mori. A hierarchical deep temporal\nmodel for group activity recognition. In CVPR, 2016. 1, 2,\n3, 5, 6, 7\n[29] Hueihan Jhuang, Juergen Gall, Silvia Zufﬁ, Cordelia\nSchmid, and Michael J. Black. Towards understanding ac-\ntion recognition. In ICCV, 2013. 2\n[30] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolu-\ntional neural networks for human action recognition. IEEE\nTransactions on Pattern Analysis and Machine Intelligence,\n35:221–231, 2010. 2\n[31] Andrej Karpathy, George Toderici, Sanketh Shetty, Thomas\nLeung, Rahul Sukthankar, and Li Fei-Fei. Large-scale video\nclassiﬁcation with convolutional neural networks. In CVPR,\n2014. 2\n[32] D. P. Kingma and J. Ba. Adam: A method for stochastic\noptimization. In ICLR, 2015. 6\n[33] Guillaume Lample and Alexis Conneau. Cross-lingual lan-\nguage model pretraining. ArXiv, abs/1901.07291, 2019. 1\n9\n[34] Tian Lan, Leonid Sigal, and Greg Mori. Social roles in hi-\nerarchical models for human activity recognition. In CVPR,\n2012. 3\n[35] Tian Lan, Yang Wang, Weilong Yang, Stephen N. Robi-\nnovitch, and Greg Mori. Discriminative latent models for\nrecognizing contextual group activities. IEEE Transactions\non Pattern Analysis and Machine Intelligence , 34:1549–\n1562, 2012. 3, 7\n[36] Xin Li and Mooi Choo Chuah. Sbgar: Semantics based\ngroup activity recognition. In ICCV, 2017. 3, 7\n[37] Zhenyang Li, Kirill Gavrilyuk, Efstratios Gavves, Mihir Jain,\nand Cees GM Snoek. Videolstm convolves, attends and ﬂows\nfor action recognition. Computer Vision and Image Under-\nstanding, 166:41–50, 2018. 2\n[38] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, Lubomir D.\nBourdev, Ross B. Girshick, James Hays, Pietro Perona, Deva\nRamanan, C. Lawrence Zitnick, and Piotr Doll ´ar. Microsoft\ncoco: Common objects in context. In ECCV, 2014. 3\n[39] Jun Liu, Amir Shahroudy, Dong Xu, and Gang Wang.\nSpatio-temporal lstm with trust gates for 3d human action\nrecognition. In ECCV, 2016. 2\n[40] Xiang Long, Chuang Gan, Gerard de Melo, Jiajun Wu, Xiao\nLiu, and Shilei Wen. Attention clusters: Purely attention\nbased local feature integration for video classiﬁcation. In\nCVPR, 2018. 2\n[41] Tomas Mikolov, Stefan Kombrink, Luk ´as Burget, Jan\nˇCernock´y, and Sanjeev Khudanpur. Extensions of recurrent\nneural network language model. In ICASSP, 2011. 1\n[42] Joe Yue-Hei Ng, Matthew J. Hausknecht, Sudheendra Vi-\njayanarasimhan, Oriol Vinyals, Rajat Monga, and George\nToderici. Beyond short snippets: Deep networks for video\nclassiﬁcation. In CVPR, 2015. 2\n[43] Xiaohan Nie, Caiming Xiong, and Song-Chun Zhu. Joint\naction recognition and pose estimation from video. InCVPR,\n2015. 2\n[44] Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, ukasz\nKaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Im-\nage transformer. In ICML, 2018. 2\n[45] Mengshi Qi, Jie Qin, Annan Li, Yunhong Wang, Jiebo Luo,\nand Luc Van Gool. stagnet: An attentive semantic rnn for\ngroup activity recognition. In ECCV, 2018. 1, 3, 6, 7\n[46] Amir Shahroudy, Jun Liu, Tian-Tsong Ng, and Gang Wang.\nNtu rgb+d: A large scale dataset for 3d human activity anal-\nysis. In CVPR, 2016. 2\n[47] Shikhar Sharma, Ryan Kiros, and Ruslan Salakhutdinov. Ac-\ntion recognition using visual attention. In ICLR Workshops,\n2016. 2\n[48] Tianmin Shu, Sinisa Todorovic, and Song-Chun Zhu. Cern:\nConﬁdence-energy recurrent network for group activity\nrecognition. In CVPR, 2017. 1, 3, 7\n[49] Karen Simonyan and Andrew Zisserman. Two-stream con-\nvolutional networks for action recognition in videos. In\nNIPS, 2014. 2, 5\n[50] Sijie Song, Cuiling Lan, Junliang Xing, Wenjun Zeng, and\nJiaying Liu. An end-to-end spatio-temporal attention model\nfor human action recognition from skeleton data. In AAAI,\n2017. 2\n[51] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep\nhigh-resolution representation learning for human pose esti-\nmation. In CVPR, 2019. 3, 4\n[52] Ilya Sutskever, James Martens, and Geoffrey E. Hinton. Gen-\nerating text with recurrent neural networks. In ICML, 2011.\n1\n[53] Du Tran, Lubomir D. Bourdev, Rob Fergus, Lorenzo Torre-\nsani, and Manohar Paluri. Learning spatiotemporal features\nwith 3d convolutional networks. In ICCV, 2015. 2, 3\n[54] Zhigang Tu, Wei Xie, Qianqing Qin, Ronald Poppe,\nRemco C. Veltkamp, Baoxin Li, and Junsong Yuan. Multi-\nstream cnn: Learning representations based on human-\nrelated regions for action recognition. Pattern Recognition,\n79:32–43, 2018. 3\n[55] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In NIPS, 2017. 1, 2,\n4, 5\n[56] Chunyu Wang, Yizhou Wang, and Alan L. Yuille. An ap-\nproach to pose-based action recognition. In CVPR, 2013. 2\n[57] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua\nLin, Xiaoou Tang, and Luc Van Gool. Temporal segment\nnetworks: Towards good practices for deep action recogni-\ntion. In ECCV, 2016. 2\n[58] Minsi Wang, Bingbing Ni, and Xiaokang Yang. Recurrent\nmodeling of interaction context for collective activity recog-\nnition. In CVPR, 2017. 3, 8\n[59] Xiaolong Wang and Abhinav Gupta. Videos as space-time\nregion graphs. In ECCV, 2018. 2\n[60] Jianchao Wu, Limin Wang, Li Wang, Jie Guo, and Gang-\nshan Wu. Learning actor relation graphs for group activity\nrecognition. In CVPR, 2019. 1, 2, 3, 5, 6, 7, 8\n[61] Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,\nAaron C. Courville, Ruslan Salakhutdinov, Richard S.\nZemel, and Yoshua Bengio. Show, attend and tell: Neural\nimage caption generation with visual attention. In ICML,\n2015. 2\n[62] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Carbonell,\nRuslan Salakhutdinov, and Quoc V . Le. Xlnet: General-\nized autoregressive pretraining for language understanding.\nArXiv, abs/1906.08237, 2019. 1\n[63] Mohammadreza Zolfaghari, Gabriel L. Oliveira, Nima\nSedaghat, and Thomas Brox. Chained multi-stream networks\nexploiting pose, motion, and appearance for action classiﬁ-\ncation and detection. In ICCV, 2017. 3\n10",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8029603958129883
    },
    {
      "name": "Computer science",
      "score": 0.6197187900543213
    },
    {
      "name": "Artificial intelligence",
      "score": 0.531426191329956
    },
    {
      "name": "Machine learning",
      "score": 0.3541114926338196
    },
    {
      "name": "Engineering",
      "score": 0.19225305318832397
    },
    {
      "name": "Voltage",
      "score": 0.07022863626480103
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ]
}