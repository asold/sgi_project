{
  "title": "Bidirectional Language Model for Handwriting Recognition",
  "url": "https://openalex.org/W14230040",
  "year": 2012,
  "authors": [
    {
      "id": "https://openalex.org/A240551738",
      "name": "Volkmar Frinken",
      "affiliations": [
        "Computer Vision Center",
        "Centre de Recerca Matemàtica"
      ]
    },
    {
      "id": "https://openalex.org/A2155697977",
      "name": "Alicia Fornés",
      "affiliations": [
        "Computer Vision Center",
        "Centre de Recerca Matemàtica"
      ]
    },
    {
      "id": "https://openalex.org/A2152748675",
      "name": "Josep Lladós",
      "affiliations": [
        "Computer Vision Center",
        "Centre de Recerca Matemàtica"
      ]
    },
    {
      "id": "https://openalex.org/A2792753456",
      "name": "Jean-Marc Ogier",
      "affiliations": [
        "La Rochelle Université"
      ]
    },
    {
      "id": "https://openalex.org/A240551738",
      "name": "Volkmar Frinken",
      "affiliations": [
        "Centre de Recerca Matemàtica"
      ]
    },
    {
      "id": "https://openalex.org/A2155697977",
      "name": "Alicia Fornés",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2152748675",
      "name": "Josep Lladós",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2792753456",
      "name": "Jean-Marc Ogier",
      "affiliations": [
        "La Rochelle Université"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2168868236",
    "https://openalex.org/W2147393756",
    "https://openalex.org/W2122585011",
    "https://openalex.org/W1674813824",
    "https://openalex.org/W2092858021",
    "https://openalex.org/W2152928267",
    "https://openalex.org/W2142069714",
    "https://openalex.org/W2133552271",
    "https://openalex.org/W1984635093",
    "https://openalex.org/W1631260214",
    "https://openalex.org/W1528470941",
    "https://openalex.org/W10704533",
    "https://openalex.org/W1507680813",
    "https://openalex.org/W1904457459"
  ],
  "abstract": null,
  "full_text": "Bidirectional Language Model\nfor Handwriting Recognition\nVolkmar Frinken1, Alicia Forn´es1, Josep Llad´os1, and Jean-Marc Ogier2\n1 Computer Vision Center, Dept. of Computer Science\nEdiﬁci O, UAB, 08193 Bellaterra, Spain\n{vfrinken,afornes,josep}@cvc.uab.cat\n2 L3i Laboratory, Universit´e de La Rochelle\nAv. M. Cr´epeau, 17042 La Rochelle C´edex 1, France\njean-marc.ogier@univ-lr.fr\nAbstract. In order to improve the results of automatically recognized\nhandwritten text, information about the language is commonly included\nin the recognition process. A common approach is to represent a text line\nas a sequence. It is processed in one direction and the language infor-\nmation vian-grams is directly included in the decoding. This approach,\nhowever, only uses context on one side to estimate a word’s probability.\nTherefore, we propose a bidirectional recognition in this paper, using\ndistinct forward and a backward language models. By combining decod-\ning hypotheses from both directions, we achieve a signiﬁcant increase\nin recognition accuracy for the oﬀ-line writer independent handwriting\nrecognition task. Both language models are of the same type and can be\nestimated on the same corpus. Hence, the increase in recognition accu-\nracy comes without any additional need for training data or language\nmodeling complexity.\nKeywords: handwritingrecognition, language models, neuralnetworks.\n1 Introduction\nThe recognition of handwritten text is a very active research ﬁeld among re-\nsearchers on pattern recognition [12]. Promising approaches for handwriting\nrecognition are segmentation-free and learning-based, such as hidden Markov\nmodels (HMM) [2,13], neural networks (NN) [6], or combinations thereof [3].\nStill, the recognition of unconstrained text can not be considered a solved\nproblem. The main reason is the diﬃculty in dealing with the high variability\nencountered in diﬀerent handwriting styles. Often, a semantic understanding of\nthe text is necessary to be able to read a text. In case of automatic recogni-\ntion systems, contextual understanding is usually emulated by estimating word\nprobabilities, such asn-grams [5,7]. Yet, despite their simplicity and inability to\ncapture any long-term relationships between words,n-gram approaches perform\nremarkably well and are still state-of-the-art.\nCurrent handwriting recognition systems represent the text line as a sequence\nand perform the recognition usually in the direction of writing, i.e., left to right\nG.L. Gimel’ farb et al. (Eds.): SSPR & SPR 2012, LNCS 7626, pp. 611–619, 2012.\nc⃝ Springer-Verlag Berlin Heidelberg 2012\n612 V. Frinken et al.\nfor Roman scripts. This allows to directly includen-gram language model infor-\nmation in the decoding. In this form oflanguageprobabilityestimation, however,\nonly a limited context is used to estimate the occurrence probability of a word.\nAs a result, recognizers face the problem of error propagation. Correct word\nthat are required in a larger context might be dropped due to pruning. Instead a\nwrong hypotheses propagates wrong language model information to the follow-\ning words and may disturb their recognition, hence creating a form of decoding\ndirection dependent error.\nAsaconsequence,one-directionaldeco dingseemstobeanunnecessaryrestric-\ntion, especially when the input data areoﬀ-line text images. A word’sprobability\ncan be estimated more robustly by considering bothn-grams, the one consider-\ning the words on the left, and the one considering the words on the right side.\nThus, taking also the reversed decoding direction into account could reduce the\nrecognition error-propagation.\nIn this paper we propose the use of bi-directionaln-grams for improving the\nrecognition performance of unconstrained handwritten text. In order to do this,\nN-bestlistsarecreatedforbothdirectionsseparately,usingadistinctforwardand\na backward language model. Then, these lists are combined to produce the ﬁnal\nrecognition output. Note that the system used in this paper is based on Neural\nNetworks [6], but it could easily be extended to HMM-based approaches as well.\nThe rest of the paper is structured as follows. In Section 2 the proposed\nbidirectionallanguagemodel approachis introduced and explained in detail. The\nexperimental evaluation is presented in Section 3 and conclusions are drawn in\nSection 4.\n2 Bidirectional Language Models\nThe ambiguity of diﬀerent handwritten text and the huge variances in diﬀerent\nwriting styles require an integration of contextual information for an automatic\ntranscription.Thestandardwayofdoingthisistointegrateastatisticallanguage\nmodel in the decoding process. However, language modeling using bi-grams do\nnot capture the language suﬃciently well. One option is to increase the complex-\nity of the language model by using higher ordern-grams, however, the number\nof distinctn-grams increases exponentially withn. Hence, even in a large train-\ning corpus, many word combinations do not occur at all or they occur with a\nfrequency not high enough for a robust occurrence probability estimation.\nAnotherchallengetohandwritingrecognitionistheerrorpropagationofamis-\nrecognized word. As a sequential decoding problem, common recognition meth-\nods process the text line in one direction, left-to-right or right-to-left. Hence, any\nmis-recognition propagates in the direction of recognition due to the language\nmodel which takes the current recognition result to estimate the next word’s\nprobability.\nTo address both issues, the challenge to estimate sophisticated language mod-\nels on sparse data as well as the problem of error propagation, we propose in this\npaper to decode the text from both directions and combine the results. Forward\nBidirectional Language Models for Handwriting Recognition 613\nand backward decoding require to diﬀerent language models which can still be\nestimated on the same corpus and the combination can successfully increase the\nrecognition accuracy.\nThe proposed approach is a step towards holistic language models to better\ncapture syntactic and semantic information. While such models have been pro-\nposed for speech recognition in a sophisticated way [14], our approach does not\nincrease the language modeling and hence the computational complexity.\n2.1 Contribution\nFrom a mathematical point of view, continuous handwriting recognition systems\nmap a text image to a sequence of wordsw\nS\n1 = w1w2 ...w S .T h i si sd o n eb y\nusingboth,anobservationmodel ϑthat assignsaprobabilityvalue toacharacter\nsequence according to the observedimage and a language modelLM that assigns\na probability value to a given character sequence according to the language at\nhand. Thecharactersequencethatmaximi zesthe combined scoreis then selected\nas the ﬁnal output.\nIn this paper we focus on the language model probability score which can be\nfactorized as\np(wS\n1 )= p(w1)·p(w2|w1)··· p(wS|wS−1\n1 )( 1 )\n= p(w1)\nS∏\ni=2\np(wi|wi−1\n1 )( 2 )\n= p(wS)·p(wS−1|wS )··· p(w1|wS\n2 )( 3 )\n= p(wS)\nS∏\ni=1\np(wi|wS\ni+1) . (4)\nNote that we deﬁnewi = ε for i ≤ 0a n di>S to make the Equations more\nreadable. Following from the rules of probability, it does not matter whether the\nLMprobabilityisfactorizedsuchthattheprobabilityforaword wi isconditioned\non its left contextwi−1\n1 (see Eqn. 2) or its right contextwS\ni+1. However, keeping\ntrack of the entire context is unfeasible forrealword applications,hence state-of-\nart recognition systems usen-gram models which only take a limited number of\nwords into account. Usually this is done in the direction of text processing, i.e.,\nfor languages that are written and recognized from left-to-right, the left hand\nside context of a word is considered to estimate its probability. Here, we will\nindicate this withLM\n→ and call itforward LM\np→(wS\n1 |LM→)= p(w1|LM→)\nS∏\ni=2\np(wi|wi−1\ni−n+1,LM→) . (5)\nObviously, every text image can also be recognized in the reversed direction,\nrequiring diﬀerentn-grams, indicated here withLM← (backward LM )\np←(wS\n1 |LM←)= p(wS|LM←)\nS−1∏\ni=1\np(wi|wi+n−1\ni+1 ,LM←) . (6)\n614 V. Frinken et al.\n(a) Recognition lattice\n(b) Reversed recognition lattice\nFig. 1. In (a) the recognition lattice for the left-to-right decoding direction is given\nfor a sample text line. In (b) the reversed lattice with the modiﬁed language model\ninformation is shown. Note that all node labels but only one edge label is shown for\nthe sake of readability. In the left-to-right decoding, the bi-gram information that the\nword “This” occurs after the symbol “.” is used. In the right-to-left decoding, the\ncorresponding edge contains the probability of the symbol “.” occurring before the\nword “This”.\nAlthough the Equations (2) and (4) are a factorization of the same probability,\ntheir n-gram simpliﬁcation in Equations (5) and (6) is expected to produce dif-\nferent results. Yet, both can be estimated on the same corpus. Thus, we propose\nin this paper to exploit this fact. We show that a signiﬁcant improvement of the\nrecognition rate can be achieved by combining the recognition output of the two\nsystems using the forward LM and the backward LM of the samen-gram order.\n2.2 Approach\nWe propose to generate two diﬀerentN-best lists of recognition hypotheses,\none generated by a left-to-right and onegenerated by a right-to-left decoding.\nAfterwards, theseN-best lists can be combined to generate a new output.\nA straightforward way to build both lists is to use a recognizer for handwrit-\nten text that produces a recognition lattice, such as HMMs or BLSTM Neural\nNetworks in conjunction with a Token Passing algorithm. A recognition lattice\n(see Fig. 1), is a directed graph with node and edge labels and constitutes a\nBidirectional Language Models for Handwriting Recognition 615\ncomprehensive way of storing various decoding paths. From this,N-best lists\ncan easily be generated by searching the most likely paths across the lattice us-\ning A\n∗-search. The exact speciﬁcations, what information is stored in the nodes\nand labels may vary, but usually a node represents a word and an edge indicates\na transition between two words. In our approach, nodes are labeled with the\nposition where the word ends. Edges are labeled with two probability scores,\nthe bi-gram transition probability between the word at the starting node and\nthe word at the ending node and the observation probability of the word at the\nending node. From this, we generate theN-best lists of the forward direction.\nNext, we reverse the directions of the edges and adjust the bi-gram prob-\nabilities. That is, an edge e =( u,v) ∈ V × V from node u to v labeled\nwith p\n→(v|u,LM)a n dpobs(v) is changed into an edgee =( v,u) with labeling\np←(u|v,LM)a n dpobs(v). A path in the new lattice now represents a decoding\nusing the reversed bi-gram language model and anN-best list is also generated.\nNote that the word ordering of the hypotheses in this list is in reversed order\nand needs to be changed back.\nTomakeuseofhigherorder N-grams,thebi-gramwordtransitionprobabilities\non the edges are ignored. Instead, anA∗-search on the lattices is done using an\nexternal language model ﬁle to generate the forward and backwardN-best lists.\nFinally, the twoN-best lists can be combined using a generalizedrecognizer\noutput voting error reduction (ROVER) scheme [4,16]. In this system, theN-\nbest output word strings are ﬁrst aligned and then combined in a weighted\nvoting scheme. The weights of the word hypotheses in the combination are based\non their posterior probabilities which are estimated from the N-best lists of\nthe recognizers. The combination was done using the SRILM toolkit [15]. The\ntoolkit allows the use of diﬀerent weight parameters, which were optimized on\nthe validation set.\n3 Experimental Evaluation\n3.1 Setup\nFor the experiments, we have used the IAM oﬀ-line database [11], which contains\nforms of unconstrained handwritten English text. The database is composed of\n1,539 pages (13,353 text lines, 115,320 words) written by 657 writers. In our\nexperiments we have followed the benchmark deﬁned by the authors, which\nconsists in 6,161 lines in the training set, and 920 lines in the validation set, and\n2,781 lines in the test set.\nFirst of all, each text line has been binarized and normalized in order to cope\nwith diﬀerent handwriting styles. The normalization consists in correcting the\nskew and slant, and normalizing the size and width of the text. The result of\nthe text line normalization process can beseen in Fig. 2. Once the text lines are\nnormalized, a sliding window moving from left to right over the text image. At\neach column of width one pixel, the following nine features are extracted: the\n0th, 1st and 2nd moment of the black pixels’ distribution within the window,\nthe position of the top-most and bottom-most black pixel, the inclination of the\n616 V. Frinken et al.\n(a) The original text line image.\n(b) The normalized text line.\nFig. 2. T h et e x tl i n ep r e p r o c e s s i n g\ntop and bottom contour of the word at the actual window position, the number\nof vertical black/white transitions, and the average gray scale value between the\ntop-most and bottom-most black pixel. For a more detailed description of the\nnormalization and feature extraction, we refer to [10].\nAs a recognizer, a bidirectional LSTM neural network (BLSTM NN) is used,\ni.e., the sequence of feature vectors is fedinto the network from both directions,\nleft-to-right and right-to-left. The output layer consists of one node for each\npossible character. By normalizing the output activations, the result is a matrix\nofposteriorprobabilities for each letter and eachposition. Given that matrix and\na bi-gram language model, a token passing algorithm can be used to generate\nthe recognition lattices. For details about BLSTM networks and the CTC token\npassing algorithm, we refer to [6].\nBoth the forward and the backwardN-grams withN =2 ,3,4 are estimated\non the union of the Brown and Wellington corpus [1,8,9] as well as the part of\nthe LOB corpus not used in the validation or testing. The total amount of text\nis 3.34M words in 162.6K sentences.\nWe chose the dictionary to be the 20,000 most frequent English words. Since\nwe consider the open vocabulary recognition task, some words in the training,\nvalidation, and test set do not occur in the dictionary and can not be recognized.\nThis imposes an upper bound to the word recognition rate of 93.74%.\n3.2 Results\nIn Fig. 3, the impact of the bidirectional language model on the handwriting\nrecognition task can be seen. The solid line indicate the standard left-to-right\nlanguage model and it can be seen that the recognition accuracy increases from\n75.08% using a 2-gram LM, up to 75.47% (3-grams) and 75.50% (4-grams). The\nresults using bi-grams are comparable to the ones found in [6,3].\nUsing a right-to-left language model, the recognition rates are consistently\nhigher by reaching 75.25% (2-grams), 75.80% (3-grams), and 75.82% (4-grams).\nThe lack of signiﬁcant increase when switching from a 3-gram to the 4-gram LM\ncan be explained by the size of the language corpus. Obviously the limit of the\ngeneralization capability is reached.\nBidirectional Language Models for Handwriting Recognition 617\n75\n76\n77\n75.5\n76.6\n2-gram 3-gram 4-gram\nleft-to-right LM\nright-to-left LM\ncombined LM\nWord Recognition Accuracy\nFig. 3. Word level recognition accuracies of the diﬀerent systems\nThe proposed, combined language model, however, achieves a signiﬁcant in-\ncrease by combining the left-to-right and right-to-left models. With bi-gram\nmodel, a recognition accuracy of 76.08% is reached, outperforming even the\n4-gram recognition with an unidirectional model. The performance using the\n3-gram combined model is 76.33% and slightly better than using the 4-gram\n(76.29%)combined model. However,this diﬀerence is not statistically signiﬁcant,\nwhile all increases from the uni-directional models to the proposed bi-directional\nmodel are statistically signiﬁcant atα =0 .05 for everyN.\n4C o n c l u s i o n\nThe recognition of unconstrained handwritten text is still considered an open\nproblem mainly due to the high variability in the handwriting styles. Since state-\nof-the-art handwriting recognition systems decode a text line sequentially, the\ncontextual information used for solving ambiguities is only taken from one side\nof a word. To increase the robustness of estimating a word’s language model\nprobability one the one hand and to reduce the eﬀect of error-propagation of\nmis-recognized words, we propose bidirectional language models. In considering\ncontextual information from both sides of a word, our approach may be seen as\na step towards full sentence language models that capture the meaning of a text\nholistically.\nThe experimental results obtained with bidirectionaln-grams have shown a\nsigniﬁcant improvement over current state-of-the-art approaches. The improve-\nmenthasbeenachievedwithoutincreasingthe amountoftrainingdata,language\ncorpus, or the complexity of the language model.\nThus, we can conclude that bidirectional language models are promising ap-\nproaches.Therefore,furtherworkcouldbe focusedoninvestigatingholisticwhole\nsentence analysis with bidirectional grammars and context-free grammars.\n618 V. Frinken et al.\nAcknowledgements. The authors thank Alex Graves for kindly providing us\nwith the BLSTM Neural Network source code and Oriol Ramos Terrades for\ninsightful discussions. This work has been partially supported by the European\nprojects FP7-PEOPLE-2008-IAPP and ERC-2010-AdG-20100407-269796, the\nSpanish projects TIN2011-24631, TIN2009-14633-C03-03, Consolider-Ingenio\n2010 (CSD2007-00018), 2010 CONE3 00029, and the mobility research grant\n10 BE-1 00020.\nReferences\n1. Bauer, L.: Manual of Information to Accompany The Wellington Corpus of Writ-\nten New Zealand English. Technical report, Department of Linguistics, Victoria\nUniversity, Wellington, New Zealand (1993)\n2. Bunke, H., Bengio, S., Vinciarelli, A.: Oﬄine Recognition of Unconstrained Hand-\nwritten Texts using HMMs and Statistical Language Models. IEEE Transactions\non Pattern Analysis and Machine Intelligence 26(6), 709–720 (2004)\n3. Espana-Boquera, S., Castro-Bleda, M.J., Gorbe-Moya, J., Zamora-Mart´ınez, F.:\nImproving Oﬄine Handwritten Text Recognition with Hybrid HMM/ANN Models.\nIEEE Transactions on Pattern Analysis and Machine Intelligence 33(4), 767–779\n(2011)\n4. Fiscus, J.: A Post-processing System to Yield Reduced Word Error Rates: Rec-\nognizer Output Voting Error Reduction (ROVER). In: Workshop on Automatic\nSpeech Recognition and Understanding, pp. 347–354. IEEE (December 1997)\n5. Goodman, J.T.: A Bit of Progress in Language Modeling - Extended Version. Tech-\nnical Report MSR-TR-2001-72, Microsoft Research, One Microsoft Way Redmond,\nWA 98052, 8 (2001)\n6. Graves, A., Liwicki, M., Fern´andez, S., Bertolami, R., Bunke, H., Schmidhuber, J.:\nA novel Connectionist System for Unconstrained Handwriting Recognition. IEEE\nTransactions on Pattern Analysis and Machine Intelligence 31(5), 855–868 (2009)\n7. Jelinek, F.: Stochastic Analysis of Structured Language Modeling. In: Mathemati-\ncal Foundations of Speech and Language Processing, vol. 138,pp. 37–71.Springer,\n8. Johansson, S., Atwell, E., Garside, R., Leech, G.: The tagged lob corpus: Users’\nmanual. Technical report, The Norwegian Computing Centre for the Humanities\n(1986)\n9. Kucera, H., Francis, W.N.: Manual of Information to accompany A Standard Cor-\npus of Present-Day Edited American English, for use with Digital Computers.\nBrown University, Department of Linguistics, Providence, Rhode Island, 1964. Re-\nvised 1971. Revised and ampliﬁed (1979)\n10. Marti, U.-V., Bunke, H.: Using a Statistical Language Model to Improve the Per-\nformance of an HMM-Based Cursive Handwriting Recognition System. Int.Journal\nof Pattern Recognition and Artiﬁcial Intelligence 15, 65–90 (2001)\n11. Marti, U.V., Bunke, H.: The iam-database: An English Sentence Database for\nOﬄine Handwriting Recognition. Int’l Journal on Document Analysis and Recog-\nnition 5(1), 39–46 (2002)\n12. Plamondon, R., Srihari, S.N.: Online and Oﬀ-Line Handwriting Recognition: A\nComprehensive Survey. IEEE Transactions on Pattern Analysis and Machine In-\ntelligence 22(1), 63–84 (2000)\nBidirectional Language Models for Handwriting Recognition 619\n13. Pl¨otz, T., Fink, G.A.: Markov Models for Oﬄine Handwriting Recognition: A Sur-\nvey. Int’l Journal on Document Analysis and Recognition 12(4), 269–298 (2009)\n14. Rosenfeld, R., Chen, S.F., Zh, X.: Whole-Sentence Exponential Language Mod-\nels: A Vehicle for Linguistic-Statistical Integration. Computers, Speech and Lan-\nguage 15, 55–73 (2001)\n15. Stolcke, A.: SRILM: An Extensible Language Modeling Toolkit, pp. 901–904 (2002)\n16. Stolke, A., K¨onig, Y., Weintraub, M.: Explicit Word Error Minimization in N-Best\nList Rescoring. In: EUROSPEECH, pp. 163–166 (1997)",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9053913950920105
    },
    {
      "name": "Language model",
      "score": 0.7556003332138062
    },
    {
      "name": "Natural language processing",
      "score": 0.6208506226539612
    },
    {
      "name": "Handwriting recognition",
      "score": 0.6108391284942627
    },
    {
      "name": "Speech recognition",
      "score": 0.6095529794692993
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5971718430519104
    },
    {
      "name": "Decoding methods",
      "score": 0.5837568044662476
    },
    {
      "name": "Handwriting",
      "score": 0.518439531326294
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5181787610054016
    },
    {
      "name": "Task (project management)",
      "score": 0.5133301615715027
    },
    {
      "name": "Word (group theory)",
      "score": 0.4938672184944153
    },
    {
      "name": "Process (computing)",
      "score": 0.42140835523605347
    },
    {
      "name": "Named-entity recognition",
      "score": 0.4160997271537781
    },
    {
      "name": "Feature extraction",
      "score": 0.17843857407569885
    },
    {
      "name": "Algorithm",
      "score": 0.11229553818702698
    },
    {
      "name": "Linguistics",
      "score": 0.09974613785743713
    },
    {
      "name": "Programming language",
      "score": 0.06320586800575256
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4387153096",
      "name": "Computer Vision Center",
      "country": null
    },
    {
      "id": "https://openalex.org/I78744979",
      "name": "La Rochelle Université",
      "country": "FR"
    }
  ]
}