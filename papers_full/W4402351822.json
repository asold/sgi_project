{
  "title": "Towards Dataset-Scale and Feature-Oriented Evaluation of Text Summarization in Large Language Model Prompts",
  "url": "https://openalex.org/W4402351822",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5101313928",
      "name": "Sam Yu-Te Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A5107079522",
      "name": "Aryaman Bahukhandi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100436389",
      "name": "Dongyu Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3137427305",
      "name": "Kwan-Liu Ma",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4247105055",
    "https://openalex.org/W4396832834",
    "https://openalex.org/W3101017384",
    "https://openalex.org/W4387801427",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W4224223051",
    "https://openalex.org/W3022629704",
    "https://openalex.org/W4224280227",
    "https://openalex.org/W4226118367",
    "https://openalex.org/W2732547613",
    "https://openalex.org/W2963214037",
    "https://openalex.org/W2099813784",
    "https://openalex.org/W4225165463",
    "https://openalex.org/W4387801187",
    "https://openalex.org/W4396833505",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W2751731070",
    "https://openalex.org/W3213990450",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W2752332392",
    "https://openalex.org/W2150593711",
    "https://openalex.org/W2087388117",
    "https://openalex.org/W4401798709",
    "https://openalex.org/W2963672599",
    "https://openalex.org/W2565736599",
    "https://openalex.org/W6898505805",
    "https://openalex.org/W4366729084",
    "https://openalex.org/W2751298778",
    "https://openalex.org/W1916893613",
    "https://openalex.org/W1587559447",
    "https://openalex.org/W4205477024",
    "https://openalex.org/W4389520163",
    "https://openalex.org/W1971670344",
    "https://openalex.org/W2129286663",
    "https://openalex.org/W2752194699",
    "https://openalex.org/W6841665461",
    "https://openalex.org/W4389519552",
    "https://openalex.org/W4402227480",
    "https://openalex.org/W4387885999",
    "https://openalex.org/W2913615284",
    "https://openalex.org/W6853986894",
    "https://openalex.org/W6809646742",
    "https://openalex.org/W2753713840",
    "https://openalex.org/W4396851234",
    "https://openalex.org/W2128965734",
    "https://openalex.org/W4366548330",
    "https://openalex.org/W4391494845",
    "https://openalex.org/W2970785793",
    "https://openalex.org/W6852874933"
  ],
  "abstract": "Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization more accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation, especially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a dataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation. In response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our workflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead of using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides users in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics system that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a novel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We evaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design helps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our feature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate moving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.",
  "full_text": "© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nTowards Dataset-scale and Feature-oriented Evaluation of Text\nSummarization in Large Language Model Prompts\nSam Yu-Te Lee , Aryaman Bahukhandi, Dongyu Liu and Kwan-Liu Ma\nAbstract— Recent advancements in Large Language Models (LLMs) and Prompt Engineering have made chatbot customization\nmore accessible, significantly reducing barriers to tasks that previously required programming skills. However, prompt evaluation,\nespecially at the dataset scale, remains complex due to the need to assess prompts across thousands of test instances within a\ndataset. Our study, based on a comprehensive literature review and pilot study, summarized five critical challenges in prompt evaluation.\nIn response, we introduce a feature-oriented workflow for systematic prompt evaluation. In the context of text summarization, our\nworkflow advocates evaluation with summary characteristics (feature metrics) such as complexity, formality, or naturalness, instead\nof using traditional quality metrics like ROUGE. This design choice enables a more user-friendly evaluation of prompts, as it guides\nusers in sorting through the ambiguity inherent in natural language. To support this workflow, we introduce Awesum, a visual analytics\nsystem that facilitates identifying optimal prompt refinements for text summarization through interactive visualizations, featuring a\nnovel Prompt Comparator design that employs a BubbleSet-inspired design enhanced by dimensionality reduction techniques. We\nevaluate the effectiveness and general applicability of the system with practitioners from various domains and found that (1) our design\nhelps overcome the learning curve for non-technical people to conduct a systematic evaluation of summarization prompts, and (2) our\nfeature-oriented workflow has the potential to generalize to other NLG and image-generation tasks. For future works, we advocate\nmoving towards feature-oriented evaluation of LLM prompts and discuss unsolved challenges in terms of human-agent interaction.\n1 I NTRODUCTION\nPrompting is a new way of biasing Large Language Models (LLMs)\ntowards the desired output with natural language instructions [9]. Re-\ncently, OpenAI’s GPT Store opened the door for non-technical people\nto customize chatbots with prompts. The low technical barriers and high\ncustomizability of prompting democratize a wide range of tasks that\npreviously required programming skills [23], e.g., personalized reading\nand writing assistants [45] or visualization creation [21, 62, 69]. On\nthe other hand, composing a desired prompt (i.e., prompt engineering)\nis non-trivial. Design studies in prompt engineering [23, 25, 71] have\npointed out that evaluation in prompt engineering remains arduous be-\ncause of five challenges in current evaluation practices, i.e., evaluation\nis Opportunistic, Manual, Multi-criteria, Dynamic, and Unactionable.\nThese challenges exist even when the evaluation is done on only a\nfew test instances, and are amplified as the evaluation scales up. For\nexample, a prompt designed for text summarization on a news article\ndataset with one thousand articles requires an evaluation on a larger\ntest set than just a few instances to ensure its robustness. We refer\nto this kind of prompt evaluation as “dataset scale”, which is under-\nexplored yet challenging. In contrast to machine learning evaluation\nwhere quantitative metrics like F1 score are typically used, the quality\nof a summarization prompt output is hard to capture with quantitative\nmetrics. Traditional metrics such as ROUGE [30] have been criticized\nin many ways [10], especially their inability to differentiate between\nstate-of-the-art models, thus not suitable for evaluating summaries\ngenerated by LLM prompts. As a consequence, people choose the\ntest set opportunisticly, i.e., whatever they see first or the data points\nthat seem easy to evaluate, instead of choosing rigorous representatives\nof the whole dataset. Moreover, the test set is evaluated by manually\nscanning through the outputs. This introduces multiple and dynamic\ncriteria in prompt evaluation, where people change evaluation criteria\nas they see fit after the scanning. Most importantly, this evaluation\napproach does not necessarily lead to actionable insights, i.e., what\n• Sam Yu-Te Lee, Aryaman Bahukhandi, Dongyu Liu and Kwan-Liu Ma are\nwith University of California, Davis. Email: ytlee, abahukhandi, dyuliu,\nklma@ucdavis.edu\nManuscript received xx xxx. 201x; accepted xx xxx. 201x. Date of Publication\nxx xxx. 201x; date of current version xx xxx. 201x. For information on\nobtaining reprints of this article, please send e-mail to: reprints@ieee.org.\nDigital Object Identifier: xx.xxxx/TVCG.201x.xxxxxxx\nkind of instruction is needed, or how should the criteria be expressed to\ngenerate the desired output. Designers would still need to go through a\nhighly unpredictable trial-and-error prompt refinement process.\nIn this work, we target text summarization prompt refinement and\nattempt to explore the tasks that are involved in a systematic evaluation\nfor dataset-scale prompts and the visualization designs that foster ac-\ntionable insights. We target individuals who seek to tailor chatbots for\ntheir specific needs or work requirements using prompts and aspire to\nefficiently perform systematic evaluations of their prompts. Through\nthe lens of text summarization, we seek to generalize our findings to\nbroader real-world tasks that require dataset-scale evaluation. While\ntext summarization may not be representative of all possible prompt-\ning tasks that LLMs could perform, it presents challenges that are not\nstraightforward to solve and studying them could provide insights to-\nwards more generalizable solutions. First, summarization is extensively\nstudied yet NLP researchers can not agree on a robust quality metric\n(i.e., metrics like ROUGE [30] that seek to quantify the quality of\na summary) for evaluating state-of-the-art systems. This indicates\nthat quality metrics might have reached their limits in differentiating\nnuanced quality differences. Second, a significant cognitive load is\nrequired to manually evaluate summaries. Designers need to read the\nlengthy text and the summarized text to decide the prompt performance\nand refinement directions, making it infeasible at the dataset scale.\nThrough a pilot study, we found that feature metrics, i.e., computa-\ntional metrics that characterize the summary from different facets, are\nmore desirable than quality metrics for both technical and non-technical\nprompt designers. For example, formality and complexity might be\ntwo critical characteristics (features) of a summary if the summarization\ngoal is to produce academic-level writing. The key distinction of feature\nmetrics is that each metric evaluates one characteristic of the generated\nsummary, and the desired summary does not necessarily have a higher\nscore, e.g., a complexity score suitable for kids (lower scores) might\nbe more desirable than for professionals (higher scores). Compared to\nquality metrics, feature metrics provide a multi-faceted understanding\nof the generated summaries and a more concrete refinement direction.\nBased on this finding, we introduce a feature-oriented workflow\nthat involves four tasks: Feature Selection, Example Sourcing, Prompt\nRefinement, and Evaluation for Refinement. We develop Awesum, a V A\nsystem that implements the workflow for text summarization, incor-\nporating computational linguistics metrics as features and intelligent\nagents to support the tasks. It uses cluster visualization to provide an\noverview of the dataset and support the identification of ideal examples.\nIt provides prompt suggestions based on established prompting method-\nologies to support prompt refinement. Finally, a scatter plot inspired\n1\narXiv:2407.12192v3  [cs.HC]  9 Sep 2024\nLe vels of Pr ompt Editing Support\nL4: Pr ompt Suggestions\nUser pr ompt: Summarize the article in \na r eadable way . \nSuggestion: Be mor e specific about \nwhat ‘ r eadable ’ means.\nL3: Ke ywor d Replacement\nUser pr ompt: [Summarize] the article \nin a r eadable way . \nReplaced with: [Briefly describe] the \narticle in a r eadable way .\nL2: Sentence Replacement \nUser pr ompt: [Summarize the article \nin a r eadable way]. \nReplaced with: Use a tone for kids to \nsummarize the article\nL1: Pr ompt Replacement\nUser pr ompt: Summarize the article in \na r eadable way \nReplaced with: The user instructs you \nto [User pr ompt]. Strictly follo w the \nuser ’ s instruction.\nLe vels of Explanations for featur e metrics (comple xity) \nL3: Computational Metric\nThe F lesch Reading Ease I nde x is a \nr eadability formula that considers \nA ver age Sentence Length and \nA ver age W or d Syllables .\n\n\nC omple xity is thus the inverse of \nr eadability . H igher scor es means the \nte xt is very di ffi cult and vice versa.\nFormula:\nL2: Computational Linguistics \nC omple xity metrics aim to q uantify \nthe comple xity of a piece of writing \nb y considering various linguistic \nfeatur es ,  such as sentence length ,  \nsyllable count ,  familiarity and \ncommonality or semantic di ffi culty .\nL1: G ener al D escription\nC omple xity is the e ff ort a r eader \nneeds to understand a te xt based \non its content (vocabulary and \nsyntax). Lo wer comple xity in a te xt \neases r eading e ff ort and speed for \nthe gener al population of r eaders. \nEvery writer has a tar get audience \nthat the y have to ad j ust their \ncomple xity le vels to.\nFig. 1: Left: Levels of explanations for feature metrics (using complexity for demonstration). L1: a textual description that vaguely defines the\ncomplexity of a text. L2: a textual description but in the context of computational linguistics, outlining common linguistic features that are considered.\nL3: A specific computation metric for complexity, including a textual description and the computation formula. Right: Levels of prompt editing support.\nL1: the system inserts user prompts into a template prompt under the hood. L2: the system suggests a better sentence. L3: the system suggests a\nbetter keyword or noun phrase. L4: the system gives suggestions on the prompt based on established prompting methodology.\nby BubbleSets and enhanced with dimensional reduction techniques\nsupports users generate actionable insights for prompt refinement. We\nrecruit experts from various domains for practitioner review, from\nwhich we confirm the effectiveness of the system. We report the gener-\nalizability of the system to a broader range of tasks and discuss insights\ninto human-agent interaction. Our contributions are as follows:\n• We introduce a feature-oriented workflow to address challenges\nin supporting dataset-scale prompt evaluation, which we summa-\nrized from a literature review and a pilot study.\n• We develop a V A system, Awesum, that supports the feature-\noriented workflow for text summarization with computational\nlinguistics, intelligent agents, and interactive visualizations.\n• We evaluate the effectiveness and generalizability of the system\nwith a case study and interviews with practitioners from various\ndomains, and report implications for future directions in prompt\nevaluation and human-agent interaction.\n2 R ELATED WORKS\nIn this section, we first explore the technical challenges and solutions\nof text summarization evaluation. We then examine current interactive\nvisual interfaces and design studies for iterative prompt engineering,\nhighlighting the research gap in evaluating dataset-scale prompts.\n2.1 Text Summarization Evaluation\nAutomatic summarization is a classic natural language generation\n(NLG) task that converts a lengthy source text into a condensed text\ncontaining the most important information. However, the evaluation of\na summarization system has remained a persistent challenge. In the past,\npeople predominantly used n-gram overlap metrics like BLEU [44]\nor ROGUE [30] to assess the quality of a generated summary. Later,\nthey are enhanced with embeddings-based similarity measurements\nlike BERTScore [72] and MoverScore [75]. Still, studies have shown\nthat they cannot reliably quantify improvements if the difference is too\nsmall, because they are not sensitive enough to capture the subtle differ-\nences in high-quality summaries that humans can perceive [10]. This\nbecame a serious problem as the capability of NLG models advanced,\nand finally, these metrics became ineffective as LLMs demonstrated\nthe capability of generating human-level summaries.\nAnother problem with these metrics is that they all require a la-\nbeled dataset (i.e., references) to work with, essentially transforming\nthe summary quality evaluation into a similarity evaluation, where a\nhuman-written summary is assumed to be optimal. This is not practical\nin prompting as the practitioners can be non-technical and the outcome\nof prompting can easily exceed in quality that of any human-labeled\ndataset. Researchers have been calling for new metrics to assess the\nNLG quality of most recent models [7, 41]. While some reference-free\nmetrics that do not require labels are proposed, such as QA-QG met-\nrics [14, 50] or LLM-as-evaluators [65, 76], they do not yet show a\nconsistent correlation with human judgments [52], might be capturing\nspurious correlations [13], or might exhibit various biases [60]. We\nargue that metrics that attempt to capture the overall “quality” of a\nprompt output (i.e., quality metrics) are susceptible to misaligning with\nhuman judgment, as previous studies have shown that human judgment\nis multi-criteria and dynamic [25]. As an alternative, we introduce fea-\nture metrics that characterize the outputs to support sensemaking on the\nprompt performances. For summarization, we use feature metrics such\nas formality and naturalness to support dataset-scale prompt evaluation.\n2.2 Interactive Visualizations for Model Refinement\nFacilitating model refinement with interactive visualizations is an ex-\ntensive field [19]. Many works help model developers debug their\nmodels [32,46,56,67] by visualizing the data flows or training patterns,\nbut they are designed specifically for the underlying model architecture\nand are not applicable in prompt engineering. Works that focus on\nmodel tuning [3, 4, 16, 64] are more similar to the settings in prompt\nengineering, where visualizations such as line charts or parallel coor-\ndinates [16] are used to visualize the performance metrics and their\nrelations with hyperparameter settings.\nStill, new visualization techniques are needed in prompt engineer-\ning for three reasons. First, previous systems are designed for model\ndevelopers. In prompt engineering, the target users are extended to\nnon-technical people whose knowledge on machine learning can not\nbe assumed. Second, previous systems evaluate performances with\nquantitative metrics such as the F1 score, but these quantitative metrics\nare not applicable in most prompt evaluation scenarios. Third, previous\nsystems guide users by visualizing the relations between certain hyper-\nparameter settings and the performance metrics. In prompt engineering,\nthe search space is all possible text expressions that can not be enumer-\nated, and the non-deterministic nature of prompting introduces a high\nuncertainty in the outputs, making it hard to identify relations between\nprompts and performances. In our work, we explore the possibility of\nevaluating prompt performances with feature metrics, which are more\napproachable for non-technical users and can guide them in discovering\nthe relations between prompts and performances.\n2.3 Interactive Prompt Engineering and Design Studies\nGiven its uniqueness, several design studies have been conducted to ex-\nplore the challenges in prompt engineering [23, 71] and evaluation [25].\nZamfirescu et al. [71] found that prompt evaluation practices of non-\ntechnical users are opportunistic rather than systematic, due to the lack\nof experience in controlling automatic systems. Jiang et al. [23] found\nthe mental load of manually skimming large volumes of text introduces\na significant evaluation challenge. Moreover, it is hard to transform\nthe evaluation into prompt refinement, i.e., little actionable insight is\ngenerated. Kim et al. [25] focused on prompt evaluation and reported\ntwo additional challenges. First, evaluation is multi-criteria, i.e., the\nquality of the outputs could not be evaluated with a single criterion.\nSecond, evaluation is dynamic, i.e., designers expand or change their\ncriteria as they observe unexpected flaws in the outputs.\nMany works have attempted to support prompt engineering in an\ninteractive, code-free environment. PromptIDE [57] and PromptItera-\ntor [58] support the iterative experimentation process. As researchers\nand practitioners introduce more prompting techniques and best prac-\ntices [1,2], Kim et al. [24] and Arawjo et al. [6] propose to design chains\nof reusable blocks to separate prompt design, model selection, and eval-\nuation. Another line of work focuses on providing prompt suggestions,\n2\n© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nranging from word-level [39], sentence-level [8], to prompt-level [35],\nwhere users prompts are replaced with an expert prompt template.\nWe summarize the prompt evaluation challenges asOpportunistic,\nManual, Multi-criteria, Dynamic, and Unactionable. Previous design\nstudies and applications either focus on evaluation at the instance scale,\ni.e., the prompt is refined and tested on at most a few instances, or\nsupport only evaluation at the dataset scale in supervised settings,\nwhere performances can be measured with loss or accuracy with labeled\ndatasets. In our work, we explore the tasks and designs to support text\nsummarization evaluation at the dataset scale, which is under-explored\nand calls for new visualization techniques.\n3 P ILOT STUDY\nFrom the literature review, we have learned that quality metrics are in-\neffective in text summarization evaluation [7, 10, 41] and hypothesized\nthat feature metrics, such as complexity and naturalness, could be a new\nway of dataset-scale prompt evaluation. As prompting is a relatively\nnew research area and the typical workflows and challenges are not\nwell-studied, we conducted semi-structured interviews with 6 partici-\npants to verify findings from existing works [23, 25, 71], explore new\nchallenges that emerge at dataset scale, and confirm the feasibility of\nusing feature metrics for dataset-scale prompt evaluation. We recruited\nusers of LLMs from both technical and non-technical backgrounds\n(e.g., ChatGPT). Below, we report our study design and findings.\n3.1 Study design\nIn our interview, seek to answer the following research questions (RQs):\n• RQ1: What are the challenges in dataset-scale summarization\nprompt evaluation?\n• RQ2: Can feature metrics address the challenges in RQ1?\n• RQ3: What is the preferred level of explanation of feature metrics?\n• RQ4: What is the preferred way of supporting prompt optimization?\nTo provide context for the participants, we used summarization on\na news article dataset as a simulated scenario in the interview. For\nRQ1, participants were first asked to write an initial prompt that can\nsummarize a news article, and then brainstorm ways to evaluate it. Then,\nwe introduced a set of potential features to be used for evaluation, and\nthen asked participants to brainstorm ways of evaluation again (RQ2).\nTo answerRQ3 and RQ4, we prepared different levels of feature metric\nexplanations and prompt editing support, as shown in Figure 1, and\nasked the participants to choose levels that they prefer and explain why.\nParticipants We recruited participants with varying backgrounds:\ntwo data/visualization scientists (P1-2), two NLP researchers (P3-4),\nand two non-technical researchers with backgrounds in environmental\nscience ( P5-6). They also have varying levels of experience with\nprompts. The most experienced participant is an LLM researcher who\nhas worked on multiple related research projects and an internship.\nMiddle-level experienced participants (N=2) had designed prompts\nprogrammatically. The least experienced participants (N=3) had used\nChatGPT for preparation of presentations, or summarizing long texts.\nParticipants reported frequent usage of prompts for at least half a year.\nProcedure The 30-minute interview was conducted in a semi-\nstructured way. We started by asking about the participants’ back-\nground, experience with prompting, and their typical workflow. Then,\nwe introduced a simulated scenario in which they designed a summa-\nrization prompt for news articles, that takesone news article and outputs\none summary and emphasized that the prompt should be generalizable\nto 100 articles. Participants were not required to write actual prompts\nsince we were only interested in their thought process. Finally, we\nasked about their preferences on different levels of feature metrics and\nprompting support. All participants received a 5 USD compensation.\n3.2 Findings\nRegardless of their backgrounds, all participants reported a similar\nworkflow for prompting: first write an initial prompt to get a baseline\nresponse, and iteratively refine the prompt by skimming through the\nresponses. The reported challenges align with the five challenges we\nsummarized from the literature review. In addition, we identified several\nnew findings when prompts are evaluated at the dataset scale:\nF1 Dataset-scale evaluation is infeasible without support. The\nsimulated scenario revealed that some external support is necessary for\ndataset-scale evaluation (RQ1). In their current practice, participants\nwould randomly pick a few examples to see if the prompt works well.\nAll participants (P1-6) agree that such a way of evaluation becomes\ninfeasible as the evaluation scales up to a dataset as they would need to\nmanually skim through hundreds of summaries, far beyond the physical\ncognitive limit of human beings. Some (P3, P4) suggested automated\nmetrics as a possible solution, but “it is hard to find a metric that can\ngeneralize across the diverse tasks enabled by prompting”. The non-\ndeterministic nature of prompting makes it hard to guarantee that the\nprompts can generalize well without actually executing the prompts,\nwhich would be costly in time and slow down the iterative refinement.\nF2 Features are critical to dataset-scale evaluation. Decomposing\ngoals into features before the refinement presents opportunities to ad-\ndress the dynamic and unactionable challenge. When asked to evaluate\nprompts that generate an “academic” summary, participants were clue-\nless about the suitable evaluation criteria. However, after introducing\nthe feature metrics, all participants agreed that these metrics could be\nused as evaluation criteria to cover various summarization requirements\n(RQ2), e.g., “academic” could be expressed as complex, very formal,\nand having a neutral sentiment. Previous works have pointed out that\nprompt evaluation is dynamic in that designers frequently change the\nevaluation criteria and redefine “success” after finding unexpected flaws\nin the output, and that it is hard to gain actionable insights [25]. Our\ninterview revealed that not knowing which features constitute the in-\ntended goal is a major obstacle in evaluation. By assisting designers\nto systematically make sense of potential features and select the most\nappropriate ones, the evaluation criteria are less likely to change, and\ndesigners can identify weak aspects in the iterative refinement process.\nF3 Metrics are guidance, not target. When asked about the pre-\nferred explanation of feature metrics (RQ3), participants predominantly\n(P2-6) chose L2: textual definitions in computational linguistics (Fig-\nure 1). Most participants excluded L1 for being too generic and L3 for\nbeing too specific for the model to follow as an instruction. Moreover,\nsome (P2, 4, 6) had doubts about the reliability of the L3 computational\nformula, questioning that “it might not be a good representation of\ncomplexity”. P2 emphasizes that “(designers) should not optimize for\nthe formula, because LLMs can understand complexity more deeply”.\nThis observation echoes with GoodHart’s Law [55]: when a measure\nbecomes a target, it ceases to be a good measure, and is reflective of\nthe current situation in text summarization, where researchers are dis-\ncouraged from using metrics like ROGUE to evaluate LLM-generated\nsummaries. As a result, we assign each feature with semantically\nmeaningful categorizations and refrain from showing metric values.\nF4 Prompting methodology is more important. All participants\npreferred prompt suggestions over prompt replacements (RQ4), despite\nthe different levels of prompt editing support introduced in previous\nworks. P6 commented that “(receiving suggestions) is a more learnable\nexperience”. Other participants also expressed the desire to learn to\ndesign good prompts, i.e., the methodology of prompting [1, 2]. We\nalso found that even without knowledge of the established method-\nologies, participants developed similar methodologies through their\nown prompting experience, such as chain of thoughts (CoT) [66] or\nproviding examples. This suggests that the design of prompting support\nshould communicate the underlying prompting methodologies.\n4 R EQUIREMENT ANALYSIS\nCombining the literature review and the pilot study findings, we iden-\ntified four tasks that users must perform in a systematic evaluation\nworkflow: Feature Selection, Example sourcing, Prompt Refinement,\nand Evaluation for Refinement, as shown in Figure 2. Since the work-\nflow is guided by features, we refer to it as a feature-oriented workflow.\nNext, we introduce each task and discuss its necessity in detail.\n4.1 Systematic Workflow\nT1 Feature Selection: Designers need to make sense of the poten-\ntial features and their correlations, and then pick a subset that\nbest represents the intended goal. Designers should avoid fix-\nating on minor feature values F3 , and instead set target ranges\n3\nT4:  E v al uation f or Refinemen t\nIdentify featur es that \nar e not expr essed \ncorr ectly in the pr ompt\nActionable Insights\nInspect the featur es \nof ex ecution r esults \nEvaluation\nT3:  Pr omp t Refinemen t\nFollo w established \nmethodologies to \nwrite pr ompts\nMethodologies\nSample a validation \nset befor e \nr efinement starts\nV alidation set\nT2:  Ex amp l e S our cing\nInspect the examples \nto validate featur e \nconfigur ations\nV alidation\nUse featur e \nconfigur ations to \nfind ideal examples\nIdentifying Examples\nT1:  F eatur e S el e ction\nDecompose the goal \ninto featur es and \ntheir tar get le vels\nFeatur e Configur ation\nMak e sense of the  \nfeatur es and their \ncorr elations\nFeatur e S ensema k ing  \nR efine\nEvaluate\nChang e\nExamples\nP r ompt\nV alidate\nConfigur e\nFig. 2: The feature-oriented workflow for a systematic dataset-scale prompt evaluation. T1 : Feature Selection. Designers should make sense\nof potential features to use for evaluation and their correlations, and then select a feature configuration that consists of a subset of features and\ncorresponding target ranges. T2 : Example Sourcing. Designers should use the feature configuration to find ideal examples, which are used to\nvalidate (and refine) the configuration and in the prompt. T3 : Prompt Refinement. First, select a validation set before the refinement starts, and then\nfollow established prompting methodologies to write the prompt that expresses the feature configurations. Execute the prompt for validation when\nready. T4 : Evaluation for Refinement. Inspect the evaluation result on selected features, identify cases that do not fit the feature configuration, and\nrefine the prompt accordingly. We emphasize that features should be used as guidance, not targets.\n(configurations) of the features. As indicated by F2 , making\nFeature Selection explicit and systematic enables the subsequent\nevaluation to become less dynamic and more predictable.\nT2 Example Sourcing: Once the features are selected, designers\nshould find ideal examples that fit the features, which can be used\nto validate the feature configurations. Examples are also used in\nthe prompts or compare the outputs in subsequent tasks.\nT3 Prompt Refinement: To guarantee a systematic evaluation, de-\nsigners need to also refine prompts systematically. First, a repre-\nsentative validation set must be chosen statistically to guarantee\nits validity. Then, prompting should follow the methodologies\nproposed by prompt engineers and researchers to write better\nand more controllable prompts F4 . Finally, prompt drafts are\nexecuted on the validation set to generate insights for refinement.\nT4 Evaluation for Refinement: With previous tasks, we establish\na base to ensure that a systematic evaluation is possible. After\ngetting feedback from the selected features, we emphasize that\ndesigners should use features as guidance, not targets F3 . Once\nthe refinement on the validation set is completed, designers can\ntest the prompt on the whole dataset to confirm the performance.\nIdeally, designers would complete the tasks sequentially (Figure 2, yel-\nlow arrows). Nevertheless, the workflow supports an iterative process:\ndesigners can use the feedback from a later task to refine a previous\ntask (purple arrows). For example, if T3 shows that the examples are\nnot representative, designers can go back to T2 for better examples.\nSuch a systematic workflow presents a structured way of refining and\nevaluating prompts, allowing easy identification of failure points.\n4.2 Design Requirements\nA systematic evaluation with the above tasks is non-trivial for prompt\ndesigners to do properly F1 , especially for non-technical people. We\nsummarized four design requirements to support T1–4 :\nR1 Support sensemaking and recommendation of feature metrics.\nAs our target audience does not necessarily come from a technical\nbackground, selecting features introduces a steep learning curve.\nOur system should support an automatic recommendation of fea-\nture metrics based on the user’s goal and provide explanations.\nR2 Support overview and identification of ideal examples. Identi-\nfying and validating ideal examples at the dataset scale is cogni-\ntively demanding. To facilitate this process, our system should\nuse statistical analysis to guarantee the soundness of the examples\nand interactive visualizations to reduce the cognitive load.\nR3 Provide suggestions based on established methodologies.\nPrompting is a relatively new area and methodologies are con-\nstantly improving. As our users might not be aware of such\nadvancements, our system should be designed to enforce state-of-\nthe-art methodologies and give suggestions when necessary.\nR4 Support visual tracking of prompt refinement effects. It is\ncritical yet demanding for designers to track the evaluation results\nwith pure statistics. Our system should support such tracking\nthrough visualization and convey the performance of each prompt.\n5 Awesum: S YSTEM DESIGN\nBased on the design requirements, we developed Awesum1, a visual\nanalytics system that supports the feature-oriented workflow on dataset-\nscale summarization prompt refinement and evaluation (Figure 3). Next,\nwe introduce the details of each component in detail.\n5.1 Feature Computation\nAwesum characterizes summaries with six features and guides users\ntoward their goals. We use well-established metrics for complexity,\nformality, and sentiment, but no known metrics suit our needs for faith-\nfulness and naturalness, so we introduce new computations for them.\nFollowing R1 , we categorize each metric into easy-to-understand lev-\nels, encouraging users to not fixate on minor differences in metric\nvalues. Below, we briefly summarize their definitions. Computational\ndetails and categorizations are presented in the appendix.\nComplexity The complexity score characterizes the ease with\nwhich a reader can understand a written text. We use the Flesch Reading\nEase Index [26] to measure the complexity of a text based on sentence\ncount, word count, and syllable count. We simplify its original eight-\nlevel categorization into five levels: Elementary, Middle School, High\nSchool, College, and Professional, which indicates the knowledge level\nrequired for a reader to easily understand the text.\nFormality The formality score characterizes how formal a piece\nof text is in terms of linguistic structures, conventions, and vocabulary.\nWe use the measure of textual lexical diversity (MTLD) [37], which is\ncalculated from the ratio of unique word stems to the number of words.\nWe categorize it into Informal, Standard, Formaland Very Formal.\nSentiment The sentiment score characterizes the emotional tone\nexpressed in a piece of text. We use V ADER [22], a lexical-based\nsentiment analysis model to measure the sentiment of a text, which\ngenerates a score between [−1,1]. Then we categorize the sentiment\ninto Negative, Neutral, and Positive using a threshold of 0.3.\nFaithfulness Faithfulness characterizes the degree to which a gen-\nerated text is consistent with the input information in terms of semantic\nsimilarity, completeness, and accuracy [31]. We incorporate this feature\nin response to the “hallucination” issue that exists in most LLMs and\nis of the most concern to prompt designers. Although more advanced\napproaches exist, we calculate the faithfulness score based on Named\nEntity Recognition overlap (NER-overlap) [28] for its transparency,\nrobustness, fast computation speed, and reasonable alignment with\nhuman judgment. We categorize it into Bad, Low, Avg, and Good.\nNaturalness Naturalness characterizes how well a text reads like\nhuman-written. To the best of our knowledge, no metrics have been\nproposed to evaluate the naturalness of texts. Based on the insights from\nPu et al. [40] that LLM-generated text exhibits statistical differences\nin certain linguistic features, such as part-of-speech (POS) tags, we\nconducted experiments on a dataset [73] that contains both human-\nwritten and LLM-generated summaries to select differentiable linguistic\nfeatures and use them to compute the naturalness score with a weighted\nsum. The score is similarly categorized into Bad, Low, Avgand Good.\n1https://github.com/SamLee-dedeboy/Awesum\n4\n© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nT4T3\nT2T1\ng\nf\ne\nd1\nd\nc\nb\na\n(1) Ov er all per formance is bad\n(3) Refine Complexity and Length in the pr ompt\n(2) Complexity and Length ar e not satisfied \nh2\nh1\nFig. 3: Actions done by Alice in the case study. Alice uses Feature Selection View (a) to decompose the summarization goal “appeal to teenagers”\ninto feature configurations T1 . After setting the configuration, the system finds the closest cluster in (b) and highlights it in a green bubble. She uses\nRecommendation View (d) to validate the examples and adjust the feature configuration in (c) T2 . Then, she moves on to Prompt Editor View (e)\nand writes the first prompt according to the feature configuration T3 , with the help of the prompt suggestions given by a chatbot. She executes the\nfirst prompt and to compare it with the baseline prompt in Prompt Comparator View (f), which provides visual tracking of all the versions of prompts.\nFrom Bubble Plot (h1), she can see that the prompt is performing badly, as the dark and light gray bubbles are overlapping and not close to the green\nbubble. She goes to the dot plot (g) and finds that complexity and length are not satisfied, as many validation cases are falling out of the green bars.\nShe refines the prompt by adjusting how she expresses complexity and length T4 by taking prompting suggestions from the system, and the new\nprompt yields much better performance (h2), as the dark gray bubble overlaps significantly with the green bubble, and most of the curves are yellow.\nLength We use word count as the length of a text. Although a\nsimple feature, prompt designers and our pilot study participants have\nreported difficulties in controlling the length of the generated text. We\nthus include it and categorize it into Short, Mid, Long, and Very Long.\nFinally, we construct a feature vector F = (f1, f2, f3,... ) for each\nsummary, where fi is the numerical score of a feature calculated on the\ngenerated summary. Then for each feature, we compute its z score as\nthe value of each dimension in the feature vector. The feature vector of\neach summary is the basic computation unit in other parts of the system,\nnamely correlation analysis, clustering, and dimensionality reduction.\nWe chose the above six feature metrics because there is a clear\nsemantic meaning to each feature. Considering that our target users\ncould include non-technical people, the interpretability of the met-\nrics is essential to a systematic and rigorous evaluation. Moreover,\nwe refrain from using LLM-evaluators to support user-defined fea-\ntures [25], even though it might broaden the applicability of the system,\nfor three reasons. First, LLM evaluators have received criticism for\ntheir inconsistent alignment with human judgment [52] and potential\nbiases [60]. Second, we have conducted experiments to show that the\nnon-deterministic nature of LLMs makes them unreliable in dataset-\nscale evaluation. Experimentation details are presented in supplemental\nmaterials. Third, the goal of developing Awesum in this paper is to\nverify the effectiveness of the feature-oriented workflow on text sum-\nmarization task. Considering the above limitations of LLM evaluators,\nit might introduce unnecessary confounding factors in the evaluation.\n5.2 Feature Selection\nFeature Selection Viewsupports T1 with two sub-tasks: feature corre-\nlation analysis and feature recommendation R1 , which supports users\nto set feature configuration for their goals of the summarization prompt.\nFeature Correlation Matrix Feature correlation matrix shows the\nPearson correlation coefficients between all pairs of features (Figure 4-\na), which is designed to prevent users from selecting conflicting features\nthat are hard to accomplish simultaneously. For example, if the correla-\ntion matrix suggests that complexity and naturalness have a negative\ncorrelation, users should not pick a configuration that has both high\n(or low) complexity and naturalness. Visualizing the correlations as\na matrix allows users to avoid such conflicts at a glance R1 . The L2\nexplanations of each feature can be inspected by hovering over the\nfeature tags. Based on the result of the baseline prompt, the system\ncalculates all the feature vectors F on the generated summaries and\nthe Pearson correlation coefficient between all pairs of features. If the\ncoefficient exceeds a threshold, we use a square to indicate its signif-\nicance and encode the correlation strength with the length. Negative\nand positive correlations are encoded by red and green, respectively.\nFeatures omitted by the user in the configuration are shaded in stripes.\nFeature Recommendation In case the feature correlation matrix\ndoes not provide a concrete idea for the user, Feature Recommendation\nPanel (Figure 4-c and -d) integrates a chatbot to recommend feature\nconfigurations. Users can express their goals in natural language (e.g.,\ngenerate summaries suitable for academic writing), and the chatbot\nwill respond with a recommended configuration with explanations R1 .\nUnder the hood, we provide the L2 feature definitions and their catego-\nrizations to the chatbot to ensure reasonable responses. If significant\ncorrelations exist in the features, the chatbot would highlight the cor-\nrelations and give suggestions accordingly. The system automatically\nfills in the recommended configuration (Figure 4-b). Dropdown menus\nare provided to change feature levels manually.\n5.3 Example Sourcing\nExample Sourcing View supports T2 with Cluster Plot (Figure 3-b),\nwhich presents an overview of the dataset, and a side panel that can be\nswitched between Cluster Profiles (Figure 5), which visualizes cluster\nfeature characteristics; and Feature Distributions (Figure 3-c), which\nprovides finer control over feature ranges selection R2 .\nUsers can inspect Cluster Profiles or Feature Distributions and nar-\nrow down the target feature ranges (green bubble in Figure 3-b and\ngreen ranges in Figure 3-c) to identify ideal examples.\n5\nc\nb\na\n d\nL2 Explanation\nFill in\nho v er\nFig. 4: Feature Selection View is designed to support T1 . The feature\ncorrelation matrix (a) facilitates sensemaking of the feature definitions\nand their correlations. If the correlation exceeds a certain threshold, it is\nencoded by a red (negative) or green (positive) square. The rows and\ncolumns are sketched out in stripes if the user deems a feature irrelevant\nto the goal. Then, users can manually set the feature configuration\nin (b) using the dropdown menus, or use the feature recommendation\nchatbot (c) to get recommendations. The chatbot responds in (d) with\na recommended configuration by considering the L2 definition of each\nfeature and the user’s goal and automatically fills them in (b).\nCluster Analysis The system applies the OPTICS clustering al-\ngorithm [5] on the initial summaries using their feature vectors. The\nOPTICS algorithm has several benefits over other clustering algorithms\nsuch as KMeans [33]. First, OPTICS is more flexible as it automatically\ndetects the cluster densities to decide cluster numbers and thus does\nnot require prior knowledge of the dataset’s cluster shapes. Second, it\nensures that all generated clusters have low variance in feature vectors,\nmaking each cluster distinctive. Third, it removes “noises” that are not\nclose to any clusters, which is ideal for identifying examples as our\nusers might not have the cognitive power to identify examples from a\nlarge amount of data. These benefits make the OPTICS algorithm ideal\nfor our system. In addition, The clustering results are used to generate\nthe validation set by under-sampling [70], i.e., cluster centroids are\nassigned to the validation set, ensuring the diversity of the validation\nset for robust evaluation.\nCluster Plot In Cluster Plot, each initial summary is encoded as\na circle and colored by their corresponding cluster. Awesum applies\nKernel PCA [49] with cosine distance on the feature vectors of the\ninitial summaries to generate 2D coordinates, which plots clusters with\nsimilar feature characteristics closer to each other, forming regions in\n2D space that represent certain characteristics. The design considers\nmaintaining the visual continuity between example identification and\ntracking prompt refinement effects (subsection 5.6). Thus, we look\nfor parametric dimensionality reduction methods, where an explicit\nmapping function (i.e., projection to low-dimensional space) can be\nreused, excluding popular non-parametric methods like t-SNE [61],\nUMAP [38] or MDS [11]. Kernel PCA allows us to reuse the same\nprojection in subsequent steps where a visual tracking of the perfor-\nmances of different prompts is provided, while capturing the non-linear\nfeature relationships in the data. We additionally employ two tech-\nniques to improve visual clarity. First, we hide noise points deemed by\nthe OPTICS algorithm as they are not representative, and only show\nthem upon user toggling. Second, we apply collision detection and a\nforce-directed layout that attracts each point to its cluster’s centroid.\nAlthough this would affect point positions given by Kernel PCA pro-\njections, this is acceptable as the exact position of each point would not\naffect users identifying ideal examples T2 . Since our target audience\nis non-technical prompt designers, we emphasize clarity over accuracy.\nCluster Profiles Cluster Profiles show the feature ranges of each\ncluster in a scaled vertical bar chart (Figure 5). Considering R2 , we\nwant to highlight distinguishing feature characteristics among clus-\nters through Cluster Profiles. Inspired by the Difference Overlay\ndesign [53], we scale the bars by aligning the global mean of each\nmetric at the center of a profile. Then, we take the maximum of\n(global_max−global_mean,global_mean−global_min) as the range\nof half of the width and scale each bar accordingly. This way, users can\neasily identify the distinctive features of each cluster. Clicking on a\ncluster profile highlights its position in Cluster Plot (green bubble) and\ngl o b al_mean\nmax(\ngl o b al_max - gl o b al_mean, \ngl o b al_mean - gl o b al_min)\nFig. 5: Cluster Profiles visualize the feature ranges of each cluster with a\nscaled vertical bar chart. Each cluster is encoded by a unique color. The\nbars are aligned by the center vertical line, which encodes the global\nmean. The half of the profile width is encoded by max(global_max −\nglobal_mean,global_mean −global_min). This way, the bars are scaled\nto show the distinctiveness of each cluster. Hovering over a bar chart\nshows the ranges in values and categorization.\nautomatically sets the cluster points as ideal examples.\nFeature Distributions Feature Distribution Panel (Figure 3-c) sup-\nports users in identifying ideal examples with finer control over the\nfeature configurations R2 . It shows the unscaled ranges of each cluster\nfor each feature, which complement the scaled ranges in Cluster Profile\nwhile maintaining the same visual encoding for colors (cluster label),\nemphasizing visual continuity and simplicity. The cluster bars are or-\ndered from top to bottom by their mean value on the corresponding\nfeature. For each feature, users can click on a cluster bar to set its range\nas a target or drag the double-direction slider at the top to adjust the\ntarget range, indicated by a light green background. Supporting T2 at\na finer scale ensures the system adapts to diverse goals.\n5.4 Recommendation\nRecommendation View (Figure 3-d) supports validation of the target\nranges of the features T2 . It shows the content of the examples with\ntheir feature categorizations. After inspecting the examples, users can\nclick the star icon and add them to the prompt.\nThe design of Recommendation View considers two weaknesses\nin Example Sourcing View. First, it is hard for users to estimate the\nnumber of recommended examples in Cluster Plot, as users would need\nto estimate the region’s area to estimate the number. To complement,\nwe use a fixed height for each example in Recommendation View by\ncollapsing excessive content, ensuring as many examples are shown\nin the viewport as possible. This allows users to estimate the number\nof examples at a glance with the total height of the examples. Second,\nCluster Profiles are scaled so they do not strictly encode the range of\neach feature. This could mislead the users into choosing the wrong\nfeature configuration. To complement, we encode the feature values\nin horizontal green bars (Figure 3-d1). Users can skim through the\nexamples with awareness of the true ranges of each feature at a glance.\n5.5 Prompt Editor\nPrompt Editor View supports users with Prompt Refinement T3 . In-\nformed by state-of-the-art prompting methodologies [1, 2], we divide\na prompt into five blocks: Persona, Context, Constraints, Examples\nand Data. In each block, users have a clear goal for the content of\nthe section R3 . For example, Persona block should specify a role-\nbased identity for the AI to adopt. This explicit division of a prompt\nenforces the users to follow established prompting methodologies with\na minimum requirement for knowledge of the methodologies. Once a\nprompt is written, users can click the “Apply” button to test it on the\nvalidation set. In addition, users can hover over the block titles to learn\nthe purpose of the block and get suggestions specific to their feature\nconfiguration from a chatbot R3 , as shown in Figure 3-e.\n6\n© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nworse\nWith T r aject or y\nWith T r aject or y\nv3 traject ory + direction + bu bbles\nGoal: Move points towards green bubble\nOld\nNew\nBubbles:\nCloser\nF ar ther\nT r aject ories:\nFig. 6: Final design of Bubble Plot. The light green bubble represents the\ngoal. Light and dark gray bubbles (representing old and new prompts)\nallow users to estimate prompt performances by the distances from the\ngreen bubble. A visual pattern that indicates a “better” prompt (top) would\nhave the dark bubble moving closer to the light green bubble, and vice\nversa (bottom). Colored curves connect validation cases in old and new\nprompts, with color encoding changes in feature values (yellow for better,\npurple for worse, gray for insignificant). The importance of a validation\ncase is encoded by the circle size (and trajectory width).\n5.6 Prompt Comparator\nPrompt Comparator Viewsupports T4 with two components: Prompt\nTracking Panel (Figure 3-f) and Bubble Plot (Figure 6). Both compo-\nnents incorporate visualizations to reduce the cognitive load of keeping\ntrack of multiple iterations of prompts and their performances R4 .\nSince the refinement is conducted on the validation set, a “Test” button\nis provided to execute the prompt on the whole dataset once users are\nsatisfied with the prompt. Next, we introduce each component in detail.\nPrompt Tracking Prompt Tracking Panel (Figure 3-f) stores all\nthe prompts that the user has written, including the prompt content,\nthe examples, and their evaluation result using a horizontal dot plot\n(Figure 3-g). Tracking the history of prompts is essential as prompt\nrefinement and evaluation is a highly iterative process. To provide visual\ntracking R4 , each prompt snippet includes a dot plot that visualizes\nthe performance of the prompt in detail, where each row is a feature\nand each dot in a row is a validation case. Since the validation set is\ngenerated from the cluster centroids, we encode the dot size with its\ncorresponding cluster size to indicate its importance, i.e., larger dots\nare more important because they represent more points. We stroke the\nfeature configuration in light green on the dot plot to clearly present the\ngoal of the prompt: make all the dots fall into the light green bars.\nBubble Plot Bubble Plot (Figure 6) supports visual comparison\nbetween any two prompts and visualizes their “distances” from the\nideal examples to support T4 . As illustrated in subsection 5.3, we\nreuse the Kernel PCA projection in Cluster Plot for all points in the plot\nto ensure visual continuity. Ideal examples are highlighted in a light\ngreen bubble as an indication of the goal. Two iterations (old and new)\nof the validation cases are plotted as circles, where the circle radius\nencodes its corresponding cluster size to suggest importance, and are\nsurrounded in light and dark gray bubbles, respectively. This allows\nthe user to estimate the performance of a prompt through the distance\nbetween its bubble and the green bubble R4 . The expected visual\npattern for a “better” prompt is that the dark bubble is “moving towards”\nthe green bubble, and vice versa. To make the comparison more explicit,\nwe connect the same validation case from two versions of prompts with\nv2 traject ory + direction\nv1 s traight traject ory\nFig. 7: Previous designs of Bubble Plot.v1: Validation cases are encoded\nby cluster colors. Animated dotted straight lines connect validation cases\nfrom old and new prompts. It is cluttered and hard to gain any valuable\ninsight. v2: Cluttering is reduced by using curves to connect points,\nwhich are generated by sampling in high-dimensional space. Colors are\nbased on the directions (green for better, red for worse). The brightness\n(light to dark) encodes the connection direction (old to new prompt). The\nvisual pattern highlights comparison, but it is hard to make sense of the\nperformance of individual prompts.\ncurves generated with linear sampling in high-dimensional space. This\neffectively visualizes the “trajectory” of the validation case incurred by\nthe new prompt. Specifically, for each pair of feature vectors (F1,F2),\nwe linearly sample ε = 100 points between F1 and F2 and project each\npoint to 2D space, thus forming the curve and encoding the importance\nwith curve width.\nSince the projection is non-linear, the sampled trajectory appears to\nbe curved. Also, using a fixed number of sampling points ( ε) makes\nlong trajectories appear dotted. We use a color-blind-friendly color\npalette to indicate the direction of the trajectory: yellow if the validation\ncase is moving closer to the ideal examples, and vice versa, purple.\nValidation cases with non-significant changes (below a threshold) are\ncolored in gray. Users can click the “Select Comparison” button to\nselect any two iterations of prompts to compare.\nBubble Plot Design Iteration The design of Bubble Plot went\nthrough three iterations, as shown in Figure 7. In v1, we encoded the\ncluster of each validation case with color and connected validation cases\nfrom two different prompts with an animated straight dotted line to\nindicate their moving directions. However, the lines were cluttered, and\ninteractions were needed for further analysis. Barely any insights can\nbe generated at a glance. Inspired by Scheepens et al. [48], in v2, we\nmitigated the clutter issue by connecting validation cases with curves\ngenerated by linearly sampling in high-dimensional space and used\ngreen (closer) and red (further) to encode the moving direction from the\nvalidation cases. Users can now compare two prompts at a glance, but\nit was hard to estimate the performance of individual prompts, which\nis critical in R4 . This motivated us to surround the validation cases\nwith BubbleSets to indicate the relative distance from the target bubble,\nproviding a clear visual tracking of prompt performance.\n6 C ASE STUDY\nWe demonstrate how a non-technical prompt designer with no prior\nprompting experience can use Awesum to refine and evaluate prompts.\nAlice is a middle school teacher and she wants to customize a chatbot\nthat can summarize sports news in a way that appeals to teenagers. She\nloads the sports-related materials into Awesum and uses a baseline\nprompt to generate the initial summaries. The system executes the\nprompt and processes the summaries. Alice starts by selecting the\nfeatures that make a summary appeal to teenagers T1 . She asks in\nFeature Recommendation Panel: “I’m a middle school teacher. How to\nmake the summaries appeal to teenagers?”. The chatbot answers with a\nrecommended feature configuration with explanations. She checks the\ndefinitions of the features and adjusts “sentiment” to “positive” as she\nwants the summaries to be more energetic, as shown in Figure 3-a.\nAfter setting the feature configuration, she then proceeds to find\nideal examples that fit this configuration T2 . She clicks the arrow\nin the green background at the top right, which triggers the system\nto automatically find a cluster that best fits the feature configuration.\n7\nThe system highlights the purple cluster in a green background and\ndotted lines that connect to “recommendations” at the top (Figure 3-b).\nShe skims through the content and the feature ranges of the purple\ncluster in Recommendation View (Figure 3-d) to ensure that they fit\nher expectations. She is satisfied with the cluster and selects the two\nbest examples to use in the prompt (starred in green).\nAfter Feature Selection and Example Sourcing, Alice proceeds to\nwrite the prompt T3 . Even without prior prompting experience, she can\neasily understand what each block means and what should be written as\nthe blocks resemble interaction patterns with chatbots. She clicks the\n“Get Suggestions” button to brainstorm some ideas for Persona block\n(Figure 3-e), then decides to put “You are a middle school teacher who\nis trying to get students interested in sports news” in Persona block.\nShe repeats similarly for Context block and Constraints block and then\nexecutes the prompt on the validation set. The system executes the\nprompt and recalculates the features. The new prompt’s content and its\nevaluation result are presented in Prompt Tracking Panel (Figure 3-f),\nwith the new prompt colored in dark gray and the baseline prompt\ncolored in light gray. From Bubble Plot (Figure 3-h1), she can quickly\ntell that the new prompt has a bad overall performance as the gray\nbubble is not moving closer to the green bubble. She inspects the\ndot plot (Figure 3-g) and finds that Complexity and Length are not\nbeing satisfied. She thus goes back to the prompt and refines the parts\nwhere she describes complexity and length T4 , seeking the prompt\nsuggestion chatbot for help. After the refinement, the new prompt is\nmuch better as the dark gray bubble has significant overlaps with the\ngreen bubble with only a few exceptions. The trajectories show that\nmost validation cases are moving towards the green bubble, and only a\nfew do not have significant changes. Alice is satisfied with this prompt\nand she clicks the “Test” button to test it on the whole dataset, which\nshows that the prompt works as expected.\nThis case study shows that the system is effective in supporting\ndataset-scale prompt refinement and evaluation. Using the system, even\nnon-technical people with no prior prompting experience could write\na reasonably good prompt, evaluate the prompt, and figure out which\nparts need to be refined. By following the systematic feature-oriented\nworkflow behind the system, users overcome the first four challenges\n(opportunistic, manual, multi-criteria, and dynamic) in prompt eval-\nuation. Cluster Plot and Bubble plot allow users to overcome the\nunactionable challenge by visualizing the prompt performance.\n7 P RACTITIONER REVIEW AND DISCUSSION\nAwesum is evaluated through a practitioner review. Even though the\nsystem is designed for text summarization, we aim to explore a broader\nrange of tasks in people’s everyday work that can also be supported.\nWe recruit practitioners from various backgrounds who are interested\nin customizing LLMs to automate their professional workflow. We\nevaluate the effectiveness of the system with them and discuss its\ngeneralizability. In addition, we report challenges in prompt evaluation\nand human-agent interaction that arise in professional usage.\n7.1 Participants and Procedure\nFour practitioners from different backgrounds were recruited: C1 is\na correspondent for the New York Times specializing in scientific\ntopics; C2, C3, and C4 are researchers in ecology, NLP, and sociology,\nrespectively. C1, C2, and C4 came from non-technical backgrounds\nand had only prompted in ChatGPT. All participants expressed strong\ninterest in customizing LLMs with prompts to automate their workflow.\nThe review procedure consists of three sessions, 20 minutes each.\nIn the first session, we introduce the background and give a quick\nwalkthrough of the interface. In the second session, participants choose\na topic (from Politics, Sport, Technology, and Business), propose a\nsummarization goal, and then use Awesum to write prompts that fulfill\nthat goal. Finally, participants engage in a semi-structured interview.\nWe provide at least 25 US dollars in compensation for all participants.\n7.2 User Feedback\nOverall, the system received positive feedback on usability and user-\nfriendliness. Still, a minimum amount of training is needed to use the\nsystem. Below, we report the practitioner review on system design,\nprompting methodology, learning curve, and visualization literacy.\nSystem Design The system was highly praised for being easy to\nfollow despite its complexity. Participants commented that “I have\nno trouble understanding what I should do (C2)” and that “The or-\nganization makes a lot of sense (C1)” . All participants successfully\nwrote a well-performed prompt for their goals based on the feedback\nfrom the system. As C3 said, “You can tell (from Bubble Plot) that\ninitially my prompt wasn’t good, then I changed the prompt according\nto the features (L2 descriptions) and it’s giving exactly what I wanted. ”\nBubble Plot and dot plots facilitated participants to examine and locate\nunfulfilled features and the L2 descriptions helped them refine prompts.\nPrompting Methodology Providing suggestions supported by the\nestablished prompting methodologies inspired less experienced partici-\npants to explore more possibilities, going beyond our initial expectation\nthat simply facilitates prompt writing: “(C4): If I had just a blank text\nbox, I would not know how to write the first sentence . . . and I would\nnot know that this is something that you can do. ” Most inexperienced\nprompt designers find prompting challenging because they do not know\nwhat the model is capable of. Before using the system, C4 did not\nknow that designers could instruct the model to adopt a persona, and\nthat inspired her to explore what could be used as a persona. As op-\nposed to L1–3 prompt editing support (Figure 1), providing prompting\nsuggestions (L4) taught participants what to expect from the model.\nLearning Curve There is still a non-trivial learning curve to over-\ncome, despite being highly praised for its clarity. For C1, “without you\nto talk me through (the interface), it would have been hard to figure out,\n(because) there is a ton of information onto one page. ”This shows that\nfor people from non-technical backgrounds, engaging in a systematic\nworkflow is not something they are familiar with and could be challeng-\ning at the beginning. Still, the clarity of the workflow and visualization\ndesign helped smooth this process, as C1 and C4 both commented that\n“it was like playing a game”. By visualizing the prompt’s performance\nwith Bubble Plot, the system provided a clear goal and abundant visual\nhints for the participants to grasp their current status and how far away\nthey were from the goal, leading to a less unpredictable experience.\nVisualization Literacy For people unfamiliar with high-\ndimensional visualization, Cluster Plot could be confusing. As\nC2 mentioned, “I can understand that each region has its own\ncharacteristics and that the green bubble is my goal, but I would still\nwant to know how it works under the hood to really make sure I’m\nnot misunderstanding. ”C4 also raised a similar concern: “Figuring\nout what these functionalities do is one thing; figuring out what these\nfunctionalities mean, that’s another thing. ” This is a valid concern\nsince Kernal PCA introduces distortion in the two-dimensional scatter\nplot, which could mislead users who are not aware of it. Even though\nwe designed Bubble Plot with loose encodings to mitigate this issue\nand encouraged users to use metrics as guidance, users could still fixate\non absolute metric values without a minimum amount of training.\n7.3 Design Implications\nIn this section, we first discuss how the system can be generalized\nto a broader range of tasks. Then, we report three implications on\nhuman-agent interaction: the issue of trust, experimenting with LLM\ncapabilities, and using features as boundary objects.\nSupporting Information-Condensing Tasks Beyond summariza-\ntion, using the system to evaluate prompts for condensing information\nis a highly praised usage. When asked about how well the system could\nbe generalized to other tasks in their professional work, all participants\nagreed its strong potential in condensing information, e.g., generating\nlogical relations for multiple articles (C1), leading to the importance\nof proper evaluation, as C4 mentioned: “Say I take 2000 tweets, some-\nthing I might do with AI is to clean these tweets because not all tweets\nhave meaningful things according to my use. . . but I can’t use it unless\nI have a grasp of what the AI’s assumption is about ‘meaningful’. ”\nChallenges in evaluating information condensing tasks resemble\nthose in text summarization in many ways: there is a great amount\nof information (data); it is cognitively demanding to evaluate even\na small amount of outputs; and it is probably unreliable to evaluate\nall of them with quality metrics. Participants agreed that the current\nfeature metrics in Awesum most scenarios for their professional work,\n8\n© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nwhich is reasonable because the standards for “professional writing”\nare quite similar. In general, the system is shown to readily apply to\ninformation-condensing tasks.\nShift in Task Formulation We observe a shift in the formulation of\nresearchable tasks from practical tasks in the practitioner review. For\nexample, to understand a large amount of customer feedback quickly\n(practical), researchers in traditional machine learning formulate the\ntask as sentiment analysis or topic modeling task (researchable). In\nthe pilot study and practitioner review, non-technical participants can\ndesign prompts that directly solve the practical task at hand. In prompt\nengineering, it is less important to rigorously define a researchable\ntask given the high customizability of prompts. Therefore, while the\nsystem seems not applicable to many existing NLP tasks, such as data\nextraction [29], making it less advantageous for NLP researchers, it\nstill brings much value to non-technical practitioners.\nApplying to Image Generation Tasks C3 was very positive about\napplying the system to image generation tasks: “you would need some\nother metrics (for image generation) like the styles of the images, but\nthe other things are more or less the same. ”The system design makes\nit easy to extend feature metrics for a broader range of tasks. Given the\nright set of features, the rest of the system is agnostic of the underlying\nfeatures because OPTICS clustering and Kernel PCA both operate on\nfeature vectors. For image generation prompt evaluation, a potential\ndirection for computing feature metrics is to evaluate the style of an\nimage through image similarity analysis [43]. For example, one could\ncollect a dataset of cartoon images and calculate the similarity between\na generated image and the dataset to quantify how “cartoony” the image\nis. Such stylistic metrics could be used as feature metrics to evaluate\nimage generation prompts. In general, the system has the potential to\nbe extended for image generation tasks, but developing corresponding\nfeature metrics is out of scope and we leave it for future work.\nBeyond generalizability, participants also brought up topics related\nto human-agent interaction in their professional work.\nExperimenting with LLM Capabilities By observing the changes\nin features, designers can experiment with prompts to better understand\nthe capability of LLMs, as shown by two unexpected usages by C3 and\nC4 during the practitioner review. C4 tried paraphrasing, e.g., chang-\ning the persona from “reporter” to “editor”, and observing changes\nin relevant features, such as complexity and formality. For inexperi-\nenced prompt designers, coping with the ambiguity inherent in natural\nlanguage has been one of the reasons why prompting is challenging.\nExperimenting with prompts mitigates this issue as it guides designers\nin sorting through the ambiguity: “It’s actually not putting stuff in\nunambiguous form, but still less ambiguous enough that you can track\nwhere things are going. (C4)” C3 used the system for a more technical\nexperiment, where he tested the positional bias [63] of LLMs. C3 put\nconflicting instructions in different parts of the prompt, e.g., asking the\nmodel to adopt the persona of “an elementary school teacher writing for\nkids” but generate summaries “as academic as you can”, and observing\nwhich part of the instruction is followed based on the features. Both\nusages showed that evaluation through features has the potential for\nunderstanding and even diagnosing LLMs.\nFeatures as Boundary Objects The practitioner review revealed\nthe potential to use features as boundary objects [54], i.e., a common\nground that could be reached between humans and agents in prompting.\nC2 and C4 both felt that using natural language to communicate with\nAI agents is different from communication with humans. When asked\nwhy, C4 replied that “I know what I want, but how do I say it has to\ndo with context”, suggesting that context needs to be presented before\nproper communication can continue with each new conversation. C2\npointed out that the ability to sort through vague communication is\ncritical: “If the professor tells me to make something better, I’ll figure\nout myself what is ‘better’ and how to make it better. But with GPT,\nI have to say exactly what is better. ”We believe that this issue could\nbe mitigated by using features as boundary objects and reverting the\ninitiative in feature selection. Instead of a human selecting the feature\nconfiguration, the agent could use the features to make confirmations\nwith the human. For example, to generate an “academic level” summary,\nthe agent could confirm with the human what “academic level” means\nby asking for confirmations on the complexity level, formality level, and\nso on. Incorporating features as boundary objects presents a promising\ndirection for supporting better human-agent interaction.\nIssue of Trust As a professional journalist, C1 had serious trust\nissues with applying intelligent agents in journalism work: “Because I\nknow about GPT hallucinations, and if I am saying false things in this\nmaterial that I produce, that’s really bad for me. ”For C1, the level of\ntrust needed for the agent seems unreachable: “ (To trust it) I have to\ncheck every single thing that it says, and it defeats the purpose of having\na tool that increases my efficiency. ”In the feature-oriented evaluation\nworkflow, we do not consider the evaluation of the trustworthiness\nof the prompts as it can not be easily measured through any feature\nthat we know of. Supporting the evaluation of trustworthiness remains\nchallenging yet critical for many professional tasks.\n8 L IMITATIONS AND FUTURE WORK\nLimitations of the system are centered around features. First, the\nfeature metrics have their limitations. For example, the Faithfulness\nscore computation is confined to the token level, which could hinder its\naccuracy when the entities have different meanings across sentences.\nThe capability of Awesum to capture user intent accurately is thus\nlimited by the feature metrics. We provide a more thorough discussion\nof the limitations of feature metrics in the supplemental material.\nDeveloping or selecting suitable features that match the task at hand\nremains a challenge in the feature-oriented workflow, especially for\nnon-technical prompt designers. This includes the challenge to decom-\npose a complicated task into subtasks that are easier to evaluate, which\nremains hard even for technical designers with relevant training [68].\nOne possible solution is to maintain a repository of features and pro-\nvide recommendation support for each task, following the approach for\nfacilitating the selection of layouts for large graphs [27]. The practi-\ntioner review revealed that the evaluation criteria of most prompting\ntasks could be decomposed into a combination of features, making\nit possible to cover endlessly long-tail tasks with a finite amount of\nfeatures. This observation aligns with the findings of Kim et al. [25],\nthat task-specific considerations are often secondary to broader criteria.\nTechnical contributors can conduct meta evaluation on the features to\nguarantee their efficacy and maintain a repository of the results, thereby\nbenefiting non-technical prompt designers.\nReliance on the dataset hinders generalizability, as one is not always\navailable in every application scenario. A potential solution is LLM-\npowered data augmentation [51]. Given a small dataset, LLMs have\nthe capability to generate high-quality data points by retrieving and\nextrapolating from large datasets with sufficient contextual information.\nThe discussion with C3 revealed that data augmentation is a promising\ndirection to deal with the absence of a dataset.\nFinally, certain social factors might not be applicable to the feature-\noriented workflow, such as trustworthiness, privacy, ethics, and social\nbiases, which have a significant impact on the adoption of LLM-based\nintelligent agents deeper into our everyday lives. As mentioned in the\npractitioner review, the level of trust needed for an intelligent agent to\nconduct certain professional tasks might take a long time to be gained,\ndefeating the purpose of adopting them for efficiency in the first place.\nThis is the same for other social factors, in that intelligent agents can\nnot be recognized without a transition process that lasts for a significant\nperiod. We believe evaluating these social factors is critical before\ndeploying prompts for high-stake tasks, but it is out of the scope of our\nwork and we call for future work to continue on this issue.\n9 C ONCLUSION\nEvaluating human-level summarizations of LLMs requires capturing nu-\nanced quality differences, surpassing the capabilities of quality metrics.\nVia Awesum , we have shown that feature metrics have the potential\nto provide actionable insights in the summarization prompt evaluation\nsetting. While still limited, the practitioner review reveals positive di-\nrections to extend the feature-oriented workflow for broader prompting\ntasks. We call for more research to follow this direction to advance\nprompt evaluation via a human-in-the-loop approach.\n9\nACKNOWLEDGMENTS\nThis research is supported in part by the UC Climate Action Initiative.\nREFERENCES\n[1] Prompt engineering from openai. https://platform.openai.com/\ndocs/guides/prompt-engineering. Accessed: 2024-02-12. 2, 3, 6\n[2] Prompt engineering guide. https://www.promptingguide.ai/. Ac-\ncessed: 2024-02-12. 2, 3, 6\n[3] Mljar. https://mljar.com/, 2018. Accessed: 2024-06-12. 2\n[4] Sigopt. https://sigopt.com/, 2018. Accessed: 2024-06-12. 2\n[5] M. Ankerst, M. M. Breunig, H.-P. Kriegel, and J. Sander. Optics: ordering\npoints to identify the clustering structure. In SIGMOD ’99, p. 49–60.\nACM, New York, 1999. doi: 10.1145/304182.304187 6\n[6] I. Arawjo, C. Swoopes, P. Vaithilingam, M. Wattenberg, and E. L. Glass-\nman. Chainforge: A visual toolkit for prompt engineering and llm hypoth-\nesis testing. In CHI ’24. ACM, New York, 2024. doi: 10.1145/3613904.\n3642016 2\n[7] M. Bhandari, P. N. Gour, A. Ashfaq, P. Liu, and G. Neubig. Re-evaluating\nevaluation in text summarization. In B. Webber, T. Cohn, Y . He, and\nY . Liu, eds.,EMNLP ’20, pp. 9347–9359. ACL, Online, 2020. doi: 10.\n18653/v1/2020.emnlp-main.751 2, 3\n[8] S. Brade, B. Wang, M. Sousa, S. Oore, and T. Grossman. Promptify:\nText-to-image generation through interactive prompt exploration with\nlarge language models. In UIST ’23. ACM, New York, 2023. doi: 10.\n1145/3586183.3606725 3\n[9] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. Ziegler, J. Wu,\nC. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess,\nJ. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever, and\nD. Amodei. Language models are few-shot learners. In H. Larochelle,\nM. Ranzato, R. Hadsell, M. Balcan, and H. Lin, eds., Advances in Neural\nInformation Processing Systems, vol. 33, pp. 1877–1901. Curran Asso-\nciates, Inc., 2020. 1\n[10] D. Deutsch, R. Dror, and D. Roth. Re-examining system-level correlations\nof automatic summarization evaluation metrics. In M. Carpuat, M.-C.\nde Marneffe, and I. V . Meza Ruiz, eds.,Proceedings of the 2022 Confer-\nence of the North American Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pp. 6038–6052. ACL, Seattle,\n2022. doi: 10.18653/v1/2022.naacl-main.442 1, 2, 3\n[11] J. Douglas Carroll and P. Arabie. Chapter 3 - multidimensional scaling.\nIn M. H. Birnbaum, ed., Measurement, Judgment and Decision Mak-\ning, Handbook of Perception and Cognition (Second Edition), pp. 179–\n250. Academic Press, San Diego, 1998. doi: 10.1016/B978-012099975-0.\n50005-1 6\n[12] E. Durmus, H. He, and M. Diab. FEQA: A question answering evaluation\nframework for faithfulness assessment in abstractive summarization. In\nD. Jurafsky, J. Chai, N. Schluter, and J. Tetreault, eds.,Proceedings of the\n58th Annual Meeting of the Association for Computational Linguistics, pp.\n5055–5070. ACL, Online, 2020. doi: 10.18653/v1/2020.acl-main.454 12\n[13] E. Durmus, F. Ladhak, and T. Hashimoto. Spurious correlations in\nreference-free evaluation of text generation. In S. Muresan, P. Nakov,\nand A. Villavicencio, eds., Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers), pp.\n1443–1454. ACL, Dublin, 2022. doi: 10.18653/v1/2022.acl-long.102 2\n[14] A. Fabbri, C.-S. Wu, W. Liu, and C. Xiong. QAFactEval: Improved QA-\nbased factual consistency evaluation for summarization. In M. Carpuat,\nM.-C. de Marneffe, and I. V . Meza Ruiz, eds.,Proceedings of the 2022\nConference of the North American Chapter of the Association for Com-\nputational Linguistics: Human Language Technologies, pp. 2587–2601.\nACL, Seattle, 2022. doi: 10.18653/v1/2022.naacl-main.187 2\n[15] T. Falke, L. F. R. Ribeiro, P. A. Utama, I. Dagan, and I. Gurevych. Rank-\ning generated summaries by correctness: An interesting but challenging\napplication for natural language inference. In A. Korhonen, D. Traum,\nand L. Màrquez, eds., Proceedings of the 57th Annual Meeting of the As-\nsociation for Computational Linguistics, pp. 2214–2220. ACL, Florence,\n2019. doi: 10.18653/v1/P19-1213 12\n[16] D. Golovin, B. Solnik, S. Moitra, G. Kochanski, J. Karro, and D. Sculley.\nGoogle vizier: A service for black-box optimization. In KDD ’17, p.\n1487–1495. ACM, New York, 2017. doi: 10.1145/3097983.3098043 2\n[17] I. Groves, Y . Tian, and I. Douratsos. Treat the system like a human student:\nAutomatic naturalness evaluation of generated text without reference texts.\nIn E. Krahmer, A. Gatt, and M. Goudbeek, eds., Proceedings of the 11th\nInternational Conference on Natural Language Generation, pp. 109–118.\nACL, Tilburg University, 2018. doi: 10.18653/v1/W18-6512 13, 14\n[18] F. Heylighen and J.-M. Dewaele. Formality of language: definition, mea-\nsurement and behavioral determinants. Interner Bericht, Center “Leo\nApostel”, Vrije Universiteit Brüssel, 4(1), 1999. 12\n[19] F. Hohman, M. Kahng, R. Pienta, and D. H. Chau. Visual analytics in\ndeep learning: An interrogative survey for the next frontiers. IEEE TVCG,\n25(8):2674–2693, 2019. doi: 10.1109/TVCG.2018.2843369 2\n[20] M. Honnibal and I. Montani. spacy 2: Natural language understanding\nwith bloom embeddings, convolutional neural networks and incremental\nparsing. To appear, 7(1):411–420, 2017. 13\n[21] Q. Huang, M. Lu, J. Lanir, D. Lischinski, D. Cohen-Or, and H. Huang.\nGraphimind: Llm-centric interface for information graphics design, 2024.\ndoi: 10.48550/arXiv.2401.13245 1\n[22] C. Hutto and E. Gilbert. Vader: A parsimonious rule-based model for\nsentiment analysis of social media text. InProceedings of the international\nAAAI conference on web and social media, vol. 8, pp. 216–225, 2014. doi:\n10.1609/icwsm.v8i1.14550 4, 12\n[23] E. Jiang, K. Olson, E. Toh, A. Molina, A. Donsbach, M. Terry, and\nC. J. Cai. Promptmaker: Prompt-based prototyping with large language\nmodels. In CHI EA ’22, CHI EA ’22. ACM, New York, 2022. doi: 10.\n1145/3491101.3503564 1, 2, 3\n[24] T. S. Kim, Y . Lee, M. Chang, and J. Kim. Cells, generators, and lenses:\nDesign framework for object-oriented interaction with large language\nmodels. In UIST ’23. ACM, New York, 2023. doi: 10.1145/3586183.\n3606833 2\n[25] T. S. Kim, Y . Lee, J. Shin, Y .-H. Kim, and J. Kim. Evallm: Interactive\nevaluation of large language model prompts on user-defined criteria. In\nCHI ’24. ACM, New York, 2024. doi: 10.1145/3613904.3642216 1, 2, 3,\n5, 9, 18\n[26] J. P. Kincaid, R. P. Fishburne Jr, R. L. Rogers, and B. S. Chissom. Deriva-\ntion of new readability formulas (automated readability index, fog count\nand flesch reading ease formula) for navy enlisted personnel. 1975. doi:\n10.21236/ada006655 4, 12\n[27] O.-H. Kwon, T. Crnovrsanin, and K.-L. Ma. What would a graph look like\nin this layout? a machine learning approach to large graph visualization.\nIEEE TVCG, 24(1):478–488, 2018. doi: 10.1109/TVCG.2017.2743858 9\n[28] P. Laban, T. Schnabel, P. N. Bennett, and M. A. Hearst. SummaC: Re-\nvisiting NLI-based models for inconsistency detection in summarization.\nTransactions of the Association for Computational Linguistics, 10:163–\n177, 2022. doi: 10.1162/tacl_a_00453 4\n[29] B. Li, G. Fang, Y . Yang, Q. Wang, W. Ye, W. Zhao, and S. Zhang. Eval-\nuating chatgpt’s information extraction capabilities: An assessment of\nperformance, explainability, calibration, and faithfulness, 2023. doi: 10.\n48550/arXiv.2304.11633 9\n[30] C.-Y . Lin. ROUGE: A package for automatic evaluation of summaries. In\nText Summarization Branches Out, pp. 74–81. ACL, Barcelona, 2004. 1, 2\n[31] J. Liu, T. Yang, and J. Neville. Cliqueparcel: An approach for batching\nllm prompts that jointly optimizes efficiency and faithfulness, 2024. doi:\n10.48550/arXiv.2402.14833 4, 12\n[32] M. Liu, J. Shi, K. Cao, J. Zhu, and S. Liu. Analyzing the training processes\nof deep generative models. IEEE TVCG, 24(1):77–87, 2018. doi: 10.1109/\nTVCG.2017.2744938 2\n[33] S. Lloyd. Least squares quantization in pcm. IEEE TIT, 28(2):129–137,\n1982. doi: 10.1109/TIT.1982.1056489 6\n[34] Y . Lu, M. Bartolo, A. Moore, S. Riedel, and P. Stenetorp. Fantastically\nordered prompts and where to find them: Overcoming few-shot prompt\norder sensitivity. In S. Muresan, P. Nakov, and A. Villavicencio, eds.,\nProceedings of the 60th Annual Meeting of the Association for Computa-\ntional Linguistics (Volume 1: Long Papers), pp. 8086–8098. Association\nfor Computational Linguistics, Dublin, Ireland, May 2022. doi: 10.18653/\nv1/2022.acl-long.556 18\n[35] S. MacNeil, A. Tran, J. Kim, Z. Huang, S. Bernstein, and D. Mogil.\nPrompt middleware: Mapping prompts for large language models to ui\naffordances, 2023. doi: 10.48550/arXiv.2307.01142 3\n[36] J. Maynez, S. Narayan, B. Bohnet, and R. McDonald. On faithfulness and\nfactuality in abstractive summarization. In D. Jurafsky, J. Chai, N. Schluter,\nand J. Tetreault, eds., Proceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics, pp. 1906–1919. ACL, Online,\n2020. doi: 10.18653/v1/2020.acl-main.173 12\n[37] P. M. McCarthy and S. Jarvis. Mtld, vocd-d, and hd-d: A validation study\nof sophisticated approaches to lexical diversity assessment. Behavior\n10\n© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nresearch methods, 42(2):381–392, 2010. doi: 10.3758/BRM.42.2.381 4,\n12\n[38] L. McInnes, J. Healy, and J. Melville. Umap: Uniform manifold approxi-\nmation and projection for dimension reduction, 2020. doi: 10.48550/arXiv\n.1802.03426 6\n[39] A. Mishra, U. Soni, A. Arunkumar, J. Huang, B. C. Kwon, and C. Bryan.\nPromptaid: Prompt exploration, perturbation, testing and iteration using\nvisual analytics for large language models, 2023. doi: 10.48550/arXiv.\n2304.01964 3\n[40] A. Muñoz-Ortiz, C. Gómez-Rodríguez, and D. Vilares. Contrasting linguis-\ntic patterns in human and llm-generated text, 2023. doi: 10.48550/arXiv.\n2308.09067 4, 13\n[41] J. Novikova, O. Dušek, A. Cercas Curry, and V . Rieser. Why we need\nnew evaluation metrics for NLG. In M. Palmer, R. Hwa, and S. Riedel,\neds., EMNLP ’27, pp. 2241–2252. ACL, Copenhagen, 2017. doi: 10.\n18653/v1/D17-1238 2, 3, 12\n[42] S. Ouyang, J. M. Zhang, M. Harman, and M. Wang. Llm is like a box\nof chocolates: the non-determinism of chatgpt in code generation. arXiv\npreprint arXiv:2308.02828, 2023. 18, 19\n[43] G. Palubinskas. Image similarity/distance measures: what is really behind\nmse and ssim? International Journal of Image and Data Fusion, 8(1):32–\n53, 2017. doi: 10.1080/19479832.2016.1273259 9\n[44] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu. Bleu: a method for auto-\nmatic evaluation of machine translation. In P. Isabelle, E. Charniak, and\nD. Lin, eds., Proceedings of the 40th Annual Meeting of the Association\nfor Computational Linguistics, pp. 311–318. ACL, Philadelphia, 2002.\ndoi: 10.3115/1073083.1073135 2\n[45] S. Petridis, N. Diakopoulos, K. Crowston, M. Hansen, K. Henderson,\nS. Jastrzebski, J. V . Nickerson, and L. B. Chilton. Anglekindling: Support-\ning journalistic angle ideation with large language models. In CHI ’23.\nACM, New York, 2023. doi: 10.1145/3544548.3580907 1\n[46] N. Pezzotti, T. Höllt, J. Van Gemert, B. P. Lelieveldt, E. Eisemann, and\nA. Vilanova. Deepeyes: Progressive visual analytics for designing deep\nneural networks. IEEE TVCG, 24(1):98–108, 2018. doi: 10.1109/TVCG.\n2017.2744358 2\n[47] D. Pu and V . Demberg. ChatGPT vs human-authored text: Insights into\ncontrollable text summarization and sentence style transfer. In V . Padmaku-\nmar, G. Vallejo, and Y . Fu, eds.,Proceedings of the 61st Annual Meeting\nof the Association for Computational Linguistics (Volume 4: Student Re-\nsearch Workshop), pp. 1–18. ACL, Toronto, 2023. doi: 10.18653/v1/2023.\nacl-srw.1 12\n[48] R. Scheepens, C. Hurter, H. Van De Wetering, and J. J. Van Wijk. Visualiza-\ntion, selection, and analysis of traffic flows. IEEE TVCG, 22(1):379–388,\n2016. doi: 10.1109/TVCG.2015.2467112 7\n[49] B. Schölkopf, A. Smola, and K.-R. Müller. Kernel principal component\nanalysis. In W. Gerstner, A. Germond, M. Hasler, and J.-D. Nicoud, eds.,\nArtificial Neural Networks — ICANN’97, pp. 583–588. Springer Berlin\nHeidelberg, Berlin, 1997. doi: 10.1007/BFb0020217 6\n[50] T. Scialom, P.-A. Dray, S. Lamprier, B. Piwowarski, J. Staiano, A. Wang,\nand P. Gallinari. QuestEval: Summarization asks for fact-based evaluation.\nIn M.-F. Moens, X. Huang, L. Specia, and S. W.-t. Yih, eds.,EMNLP ’21,\npp. 6594–6604. ACL, Online and Punta Cana, 2021. doi: 10.18653/v1/\n2021.emnlp-main.529 2\n[51] M. Seo, J. Baek, J. Thorne, and S. J. Hwang. Retrieval-augmented data\naugmentation for low-resource domain tasks, 2024. doi: 10.48550/arXiv.\n2402.13482 9\n[52] C. Shen, L. Cheng, X.-P. Nguyen, Y . You, and L. Bing. Large language\nmodels are not yet human-level evaluators for abstractive summarization.\nIn H. Bouamor, J. Pino, and K. Bali, eds., Findings of the Association for\nComputational Linguistics: EMNLP 2023, pp. 4215–4233. ACL, Singa-\npore, 2023. doi: 10.18653/v1/2023.findings-emnlp.278 2, 5, 18\n[53] A. Srinivasan, M. Brehmer, B. Lee, and S. M. Drucker. What’s the\ndifference? evaluating variations of multi-series bar charts for visual\ncomparison tasks. In CHI ’18, p. 1–12. ACM, New York, 2018. doi: 10.\n1145/3173574.3173878 6\n[54] S. L. Star and J. R. Griesemer. Institutional ecology, ‘translations’ and\nboundary objects: Amateurs and professionals in berkeley’s museum of\nvertebrate zoology, 1907-39. Social Studies of Science, 19(3):387–420,\n1989. doi: 10.1177/030631289019003001 9\n[55] M. Strathern. ‘improving ratings’: audit in the british university system.\nEuropean Review, 5(3):305–321, 1997. doi: 10.1017/s1062798700002660\n3\n[56] H. Strobelt, S. Gehrmann, H. Pfister, and A. M. Rush. Lstmvis: A tool\nfor visual analysis of hidden state dynamics in recurrent neural networks.\nIEEE TVCG, 24(1):667–676, 2018. doi: 10.1109/TVCG.2017.2744158 2\n[57] H. Strobelt, A. Webson, V . Sanh, B. Hoover, J. Beyer, H. Pfister, and A. M.\nRush. Interactive and visual prompt engineering for ad-hoc task adaptation\nwith large language models. IEEE TVCG, 29(01):1146–1156, 2023. doi:\n10.1109/TVCG.2022.3209479 2\n[58] S. Suˇcik, D. Skala, A. Švec, P. Hraška, and M. Šuppa. Prompterator: Iter-\nate efficiently towards more effective prompts. In Y . Feng and E. Lefever,\neds., Proceedings of the 2023 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations, pp. 471–478. ACL,\nSingapore, 2023. doi: 10.18653/v1/2023.emnlp-demo.43 2\n[59] D. Temperley. Dependency-length minimization in natural and artificial\nlanguages*. Journal of Quantitative Linguistics, 15(3):256–282, 2008.\ndoi: 10.1080/09296170802159512 13, 14\n[60] L. Tjuatja, V . Chen, S. T. Wu, A. Talwalkar, and G. Neubig. Do llms\nexhibit human-like response biases? a case study in survey design, 2024.\ndoi: 10.48550/arXiv.2311.04076 2, 5\n[61] L. van der Maaten and G. Hinton. Visualizing data using t-sne. Journal of\nMachine Learning Research, 9(86):2579–2605, 2008. 6\n[62] C. Wang, J. Thompson, and B. Lee. Data formulator: Ai-powered concept-\ndriven visualization authoring. IEEE TVCG, 30(1):1128–1138, 2024. doi:\n10.1109/TVCG.2023.3326585 1\n[63] P. Wang, L. Li, L. Chen, Z. Cai, D. Zhu, B. Lin, Y . Cao, Q. Liu, T. Liu,\nand Z. Sui. Large language models are not fair evaluators, 2023. doi: 10.\n48550/arXiv.2305.17926 9\n[64] Q. Wang, Y . Ming, Z. Jin, Q. Shen, D. Liu, M. J. Smith, K. Veeramacha-\nneni, and H. Qu. Atmseer: Increasing transparency and controllability in\nautomated machine learning. In CHI ’19, p. 1–12. ACM, New York, 2019.\ndoi: 10.1145/3290605.3300911 2\n[65] Y . Wang, Z. Yu, Z. Zeng, L. Yang, C. Wang, H. Chen, C. Jiang, R. Xie,\nJ. Wang, X. Xie, W. Ye, S.-B. Zhang, and Y . Zhang. Pandalm: An\nautomatic evaluation benchmark for llm instruction tuning optimization.\nIn ICLR 2024, 2023. doi: 10.48550/arXiv.2306.05087 2, 18\n[66] J. Wei, X. Wang, D. Schuurmans, M. Bosma, B. Ichter, F. Xia, E. Chi,\nQ. Le, and D. Zhou. Chain-of-thought prompting elicits reasoning in large\nlanguage models, 2023. doi: 10.48550/arXiv.2201.11903 3\n[67] K. Wongsuphasawat, D. Smilkov, J. Wexler, J. Wilson, D. Mané, D. Fritz,\nD. Krishnan, F. B. Viégas, and M. Wattenberg. Visualizing dataflow graphs\nof deep learning models in tensorflow. IEEE TVCG, 24(1):1–12, 2018.\ndoi: 10.1109/TVCG.2017.2744878 2\n[68] T. Wu, H. Zhu, M. Albayrak, A. Axon, A. Bertsch, W. Deng, Z. Ding,\nB. Guo, S. Gururaja, T.-S. Kuo, J. T. Liang, R. Liu, I. Mandal, J. Milbauer,\nX. Ni, N. Padmanabhan, S. Ramkumar, A. Sudjianto, J. Taylor, Y .-J.\nTseng, P. Vaidos, Z. Wu, W. Wu, and C. Yang. Llms as workers in human-\ncomputational algorithms? replicating crowdsourcing pipelines with llms,\n2023. doi: 10.48550/arXiv.2307.10168 9\n[69] Y . Ye, J. Hao, Y . Hou, Z. Wang, S. Xiao, Y . Luo, and W. Zeng. Gener-\native ai for visualization: State of the art and future directions. Visual\nInformatics, 8(2):43–66, 2024. doi: 10.1016/j.visinf.2024.04.003 1\n[70] S.-J. Yen and Y .-S. Lee. Cluster-based under-sampling approaches for\nimbalanced data distributions. Expert Systems with Applications, 36(3,\nPart 1):5718–5727, 2009. doi: 10.1016/j.eswa.2008.06.108 6\n[71] J. Zamfirescu-Pereira, R. Y . Wong, B. Hartmann, and Q. Yang. Why\njohnny can’t prompt: How non-ai experts try (and fail) to design llm\nprompts. In CHI ’23. ACM, New York, 2023. doi: 10.1145/3544548.\n3581388 1, 2, 3\n[72] T. Zhang, V . Kishore, F. Wu, K. Q. Weinberger, and Y . Artzi. Bertscore:\nEvaluating text generation with bert, 2020. doi: 10.48550/arXiv.1904.\n09675 2\n[73] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B.\nHashimoto. Benchmarking Large Language Models for News Summa-\nrization. Transactions of the Association for Computational Linguistics,\n12:39–57, 2024. doi: 10.1162/tacl_a_00632 4\n[74] T. Zhang, F. Ladhak, E. Durmus, P. Liang, K. McKeown, and T. B.\nHashimoto. Benchmarking Large Language Models for News Summa-\nrization. Transactions of the Association for Computational Linguistics,\n12:39–57, 2024. doi: 10.1162/tacl_a_00632 13, 14, 18\n[75] W. Zhao, M. Peyrard, F. Liu, Y . Gao, C. M. Meyer, and S. Eger. Mover-\nScore: Text generation evaluating with contextualized embeddings and\nearth mover distance. In K. Inui, J. Jiang, V . Ng, and X. Wan, eds.,\nEMNLP-IJCNLP, pp. 563–578. ACL, Hong Kong, 2019. doi: 10.18653/\nv1/D19-1053 2\n[76] L. Zheng, W.-L. Chiang, Y . Sheng, S. Zhuang, Z. Wu, Y . Zhuang, Z. Lin,\n11\nZ. Li, D. Li, E. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging\nllm-as-a-judge with mt-bench and chatbot arena. In A. Oh, T. Neumann,\nA. Globerson, K. Saenko, M. Hardt, and S. Levine, eds., Advances in\nNeural Information Processing Systems, vol. 36, pp. 46595–46623. Curran\nAssociates, Inc., 2023. doi: 10.48550/arXiv.2306.05685 2, 18\nAppendices\nAppendix A presents computation details for the feature metrics used\nin Awesum, Appendix B presents the prompts for the intelligent agents\nthat provide feature recommendations and prompt suggestions, and\nAppendix C presents an experiment to assess the consistency of LLMs\nas evaluators. All LLM-based implementations use OpenAI’s “gpt-\n3.5-turbo” model. The prompt block definitions and L2 definitions of\neach feature and used both in the interface and the prompts are listed\nin Table 3 and Table 4.\nA F EATURE METRICS\nA.1 Complexity\nWe define complexity as the opposite of readability, which is defined\nas the ease with which a reader can understand a written text. Follow-\ning previous works [26, 41] in computational linguistics showing that\nreadability is closely associated with the lexical diversity and struc-\nture of text, we first calculate the readability of summaries using the\nFlesch Reading Ease Index [26], a readability metric that measures\nthe readability of a text based on sentence length and syllable count.\nThe resulting score r is a number between 0 and 100 that indicates the\napproximate educational level a person will need to be able to read a\nparticular text easily, as shown in Equation 1. Then we calculate the\ncomplexity score as 100 −r. The Flesch Reading Ease Index provides\nan eight-level breakdown. For simplicity, we merge similar levels and\ngenerate a five-level breakdown, as given in Table 1. Since we do not\nextend the formula, its limitation is also inherited: From Equation 1,\nwe observe that r relies on text length, and splitting long sentences into\nshorter ones can artificially decrease complexity. However, since LLMs\ndon’t inherently split sentences unless instructed, this limitation has\nminimal impact on our complexity calculation.\nr = 206.835 −1.015 ∗( Total _Words\nTotal _Sentences )−\n84.6 ∗(Total _Syllables\nTotal _Words )\n(1)\nScore Level\n90.0-100.0 Pro f essional\n50.0-90.0 College\n40.0-50.0 High School\n10.0-40.0 Middle School\n0.0-10.0 Elementary\nTable 1: Categorization levels of complexity score\nA.2 Formality\nFormality in language is defined as the degree of adherence to formal\nlinguistic structures, conventions, and vocabulary. In natural language\nprocessing, a more formal language is often associated with a broader\nrange of vocabulary [47]. This is further supported by Heylighen et\nal. [18] who associate formality with the explicitness of vocabulary.\nWe use the measure of textual lexical diversity (MTLD) [37] for\nformality calculation. MLTD measures the richness and diversity of vo-\ncabulary used in a text, considering both the frequency and the number\nof distinct words. It is calculated by averaging the length of consecu-\ntive word sequences in a text while ensuring they maintain a specific\ntype-token ratio. It evaluates the text sequentially in both forward and\nbackward passes to determine the average factor count, which repre-\nsents the text’s lexical variation. A higher MTLD score signifies a\nmore formal text. We classify the final score as Informal, Standard,\nFormal, and Very Formalbased on the quantiles they lie in. Since we\ndo not extend the formula, its limitations persist. MTLD’s sensitivity\nto text length can cause inaccuracies in very short summaries due to\nsignificant partial factors. Additionally, its sequential processing may\nnot fully capture the non-linear nature of human text comprehension.\nComputation Process The text is processed sequentially from\nthe beginning to the end. A Type-Token Ratio (TTR) is calculated\nincrementally as each word is added to the sequence. The TTR at\neach point is the ratio of the number of unique words (types) to the\ntotal number of words (tokens) up to that point. A factor is defined\nas a segment of the text where the TTR reaches a specified threshold,\nwhich is set at 0.72. When the TTR reaches the threshold, one factor is\ncounted, and the TTR calculation resets for the next segment. If the text\nends before the TTR reaches the threshold for the last segment, a partial\nfactor is calculated. Calculation of partial factors is given in Equation 2.\nThe total factor count is the sum of all complete and partial factors.\nThen the final MTLD score is given by Equation 3. We directly adopted\nthe implementation of the metric provided by the authors 2\nPartial_ f actor = 1.00 −T T R\n1.00 −0.72 (2)\nMT LD = Total number of words\nTotal Factor Count (3)\nA.3 Sentiment\nSentiment refers to the emotional tone or attitude expressed in the\ntext. We use V ADER (Valence Aware Dictionary and sEntiment Rea-\nsoner) [22], a lexicon and rule-based sentiment analysis tool that gener-\nates a sentiment score based on the words used, considering both the\npolarity (positive, negative, neutral) and strength of emotion. The cal-\nculation of V ADER adopts a human-centric approach, where a valence-\naware sentiment lexicon is constructed by human annotators. Also, it\nadopts five generalizable heuristics that reflect grammatical and syn-\ntactical patterns that humans use to express sentiment intensity. The\nfinal sentiment score is calculated by summing the valence scores of\neach word in the lexicon, adjusted according to the five heuristics.\nSince V ADER calculates sentiment for each word individually, it may\nnot accurately represent phrases, idioms, or other complex sentences\nand it’s heavy reliance on lexicon quality and coverage may over-\nlook misspellings, grammatical errors, and domain-specific words in\noverall sentiment calculation. However, we use V ADER over other\ndeep learning (DL) models because DL models are often fine-tuned\nto a domain-specific dataset and lack transparency and robustness. A\nrule-based sentiment analyzer allows fast computation with consis-\ntent results. V ADER also generalizes better, as it uses a number of\nwell-established lexicon word banks and captures human perception\nof sentiment very well. The sentiment score of a summary is given\nby Equation 4\nSentiment Score = ∑N\nn=1 Si\nN , (4)\nwhere Si is the sentiment score of the ith word in the text, and N is the\ntotal number of words.\nA.4 Faithfulness\nLiu et al. [31] define faithfulness in natural language generation (NLG)\nas the degree to which the generated text is consistent with the input\ninformation or the reference text in terms of semantic similarity, com-\npleteness, and accuracy. Although more advanced models exist, such as\nquestion generation and question answering (QG-QA) frameworks [12]\nand textual entailment models [15, 36], we useentity overlap to mea-\nsure faithfulness. It measures the overlap between the most important\n2https://github.com/kristopherkyle/lexical_diversity.git\n12\n© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nAlgorithm 1 Faithfulness score calculation\nInput: article a, summary s,similarity threshold ε\nOutput: Faithfulness score\n1: Es ← Entities in s\n2: Ea ← Entities in a\n▷ Similar entities in article and summary\n3: Ss ← SimilarEntites(Es,ε)\n4: Sa ← SimilarEntites(Ea,ε)\n5: Da ← Disjoint sets in Ss.\n6: Ds ← Disjoint sets in Sa.\n▷ Set of unique entities from each disjoint set\n7: Ua ← Unique entities in Da\n8: Us ← Unique entities in Ds\n9: T ← GetTopEntites(Ua,Us)\n▷ Top source entity match count with unique summary entities.\n10: M ← |{(e1,e2) | e1 ∈Us,e2 ∈ T,Fuzzy ratio(e1,e2) ≥ ε}|\n11: LT ← len(T )\n12: score ← M\nLT\nentities in the reference and generated texts. A high faithfulness score\nmeans the most important entities in the reference text are also present\nin the generated text. We use entity overlap instead of other state-of-\nthe-art metrics because they tend to have slower computations and\nmight introduce biases and randomness, with insignificant differences\nin performance. We prefer a robust metric devoid of randomness and\noffering fast computation speed for real-time analysis.\nComputation Process The implementations of the whole process\nare outlined in Algorithm 1. We use the pre-trained SpaCy tagger [20]\nto extract entities from the article and the summary, and further in-\ncorporate an entity disambiguation step that groups similar entities\nwith fuzzy string matching, as shown in Algorithm 2. We first create\na pair-wise similarity matrix between all entities. Next, we organize\nthese entities into disjoint sets, which are sets that do not share any\ncommon elements and have no intersection. Each disjoint set represents\nentities that have a fuzzy match ratio of at least ε within the set and\nless than ε when compared to entities in other disjoint sets. We set\nthe value of ε as 0.7 in all our implementations. After grouping all\nthe similar entities, we disambiguate the entities by only taking one\nunique entity from each disjoint set. Since not all entities in the article\nare important, Algorithm 3 extracts important entities based on their\nfrequency of occurrence using a fuzzy count. Although this is a naive\nranking approach, we avoid using other supervised ranking algorithms\nfor providing a fast and robust computation in real time. The most\nimportant entities extracted from the source are compared with the\ndisambiguated entities in the summary. We then count the number of\nmatches using the fuzzy match ratio. When there are very few (0–2)\nfrom the summary but many (more than 5) entities in the article, we\nadjust the score by comparing a fixed number of entities (Algorithm 3,\nLine 4). Finally, the score is calculated as the ratio of the number of\nmatches to the number of most important entities in the article. We\nclassify the final score as “Bad”, “Low”, “Avg” and “Good” based\non the quantile they lie in. One significant limitation of our approach\nis its reliance on token-level entity overlap, which disregards the se-\nmantic context of the entities within the article and summary. While\nour method accommodates entity and synonym matching, it fails to\nconsider paraphrasing and sentence-level similarity. To address this\nissue, we could employ deep learning-based sentence embedding mod-\nels to evaluate the similarity between sentences in the article and the\nsummary. However, in real-time applications, generating embeddings\nfor lengthy texts can be both resource-intensive and time-consuming.\nA.5 Naturalness\nThe naturalness of text is a measure of how well the text flows, sounds\nlike something a native speaker would produce, is easy to understand,\nand adheres to the rules of the language. To the best of our knowledge,\nno known metrics were proposed to measure the naturalness of a text.\nAlgorithm 2 SimilarEntites(E,ε)\nInput:List of all entities E, similarity threshold ε\nOutput:Sets of similar entities S\n1: Le ← len(E)\n2: S ← OLe×Le ▷ Zero matrix of size Le ×Le\n3: for i ∈ rows(S) & j ∈ columns(S) do\n4: Si j =\n(\n1 Fuzzy ratio (Ei,Ej) ≥ ε\n0 Fuzzy ratio (Ei,Ej) < ε\n5: end for\n6: return S\nAlgorithm 3 GetTopEntites(Ua,Us)\nInput:Article entities Ua, summary entities Us.\nOutput:Most important entities in the article.\n1: Sort(Ua), descending by their fuzzy occurrence in article.\n2: Ls ← len(Us)\n3: La ← len(Ua)\n4: if Ls < 3 and La > 4 then\n5: top _article_entities ←Ua[0 : 5]\n6: else if La < Ls or Ls = 0 then\n7: top _article_entities ←Ua\n8: else\n9: top _article_entities ←Ua[0 : Ls]\n10: end if\n11: return top _article_entities\nIn this work, we measure naturalness based on linguistic feature differ-\nences observed from human and LLM-generated text. All subsequent\nexperiments are conducted on the dataset released by Zhang et al. [74].\nIt is important to note that our approach, tested on general-domain\narticles, may not generalize to other tasks, domains, or languages. Its\napplicability in scientific terminology and languages with different\ngrammatical and semantic structures remains unexplored, where other\nlinguistic features might be more relevant. Additionally, this study does\nnot cover NLG tasks like question-answering and logical reasoning.\nHowever, similar methods could be applied to these tasks, domains, and\nlanguages, as analyzing a set of features has shown better alignment\nwith human judgments in various studies [17, 40, 59].\nA.5.1 Differentiating Linguistic Features\nInspired by the findings of previous works [17,40] that human and LLM-\ngenerated summaries exhibit different statistical patterns on certain\nlinguist features, we measure the naturalness of text by those that\ncan differentiate human and LLM-generated texts. As LLMs advance\nquickly, we sought to verify their findings on state-of-the-art LLMs\nand selected a set of linguistic features to experiment with, namely\naverage arc lengths, average subtree heights, average dependency-tree\nheights, average sentence lengths, and average word lengths. First,\nwe computed these features on all the summaries in the dataset and\nconducted PCA on these features, as shown in Table 2.\nTable 2: Importance assigned by PCA\nFeature Importance\navg_dependency_tree_heights 0.504439\navg_left_subtree_height 0.486318\navg_right_subtree_height 0.441374\naverage_sentence_lengths 0.427449\nnum_right_subtrees 0.252828\nnum_left_subtrees 0.158020\navg_total_arc_length 0.137703\navg_right_arc_length 0.128035\navg_left_arc_length -0.081986\navg_word_lengths -0.023296\n13\nFig. 8: This contour plot compares the average left vs right subtree heights of human (left) and llm-generated (right) summaries. The color signifies\nthe number of summaries with the corresponding subtree lengths, with lighter shades demonstrating more number of summaries. Human written\nsummaries have shorter average left, and right subtree heights as compared to LLM written summaries. This conclusion comes from the observation\nthat the densest point (yellow region) for the LLM summaries is closer to the lower left origin than human summaries. It is also observed that\nhuman-generated summaries consistently exhibit a higher frequency of balanced left and right subtrees compared to LLM summaries. Specifically,\nthere are 182 balanced human summaries and 133 balanced LLM summaries.\nTo better understand the PCA results, we generated visualizations\nto explain the differentiating capability of each feature. We observed\nthat the number of subtrees (right and left), average arc lengths (total,\nright, and left), and average word lengths do not show any significant\ndifference. Next, we present the observed differences in other features.\nSubtree Heights An analysis of the subtrees in the dependency\nparse trees of summaries shown in Figure 8 revealed that humans tend\nto produce text with balanced left and right subtrees more frequently as\ncompared to LLM summaries. This is also coherent with the works of\nTemperley et al. [59] which shows that sentences that have a balanced\ndepth on either side of the root in the dependency parse tree are judged\nto be more natural and well-worded by humans. We also note that\nhuman-written summaries tend to have shorter average left and right\nsub-tree heights more frequently than LLM summaries. These observa-\ntions indicate that texts with shorter and balanced sub-tree heights are\nmore likely to exhibit human-like naturalness.\nDependency Tree Heights We observe that humans tend to pro-\nduce summaries of shallower dependency tree heights more often than\nLLMs. In natural language, deeper dependency parse trees have more\nlinguistic and dependency complexity. Groves et al. [17] train classifiers\non word embeddings and linguistic features to weigh their importance\nin the automatic evaluation of naturalness. They find dependency tree\nheight to be one of the most important linguistic features. Following\nthis insight, we compare the dependency tree heights of LLM and\nhuman summaries in Figure 9-a. LLM tends to produce very few\nsummaries with short (<3.5) dependency tree heights, while human\nsummaries have less frequency of long (>3.5) dependency tree heights.\nThis indicates that text with shorter dependencies and lesser linguistic\ncomplexity correlates better with humans.\nSentence Length We further compare the average sentence lengths\nof human and LLM summaries in Figure 9-b which shows that humans\ntend to produce lesser summaries with longer average sentence lengths\nas compared to LLM summaries. Human texts also have shorter sen-\ntence lengths more frequently. Conversely, LLM tends to generate\nlonger sentences more frequently, which can be attributed to their ten-\ndency to hallucinate in some cases.\nA.5.2 Naturalness Score Calculation\nBased on previous works [17, 59] and the aforementioned experiments,\nwe identified a set of linguistic and lexical features that have been shown\nto exhibit statistical differences between human and LLM-generated\ntexts, consisting of average subtree height (right and left), average\ndependency tree height, and average sentence length. This feature set\nis used to train a random forest classifier. The classification task is\nto predict if a summary is written by a human or an LLM based on\nthe linguistic features. The classifier achieves an accuracy of 99.6%\non a test size of 300 human or LLM-generated texts [74]. The final\nweights assigned to these metrics by the classifier are used to calculate\nthe weighted average of the feature values in Equation 5:\n¯X = ∑N\nn=1 Wi ∗xi\nN\n(5)\nWi is the weight of the ith feature, xi is the feature value and N is the\nnumber of features. In Equation 6, we normalize the score and invert it\nto better align with human judgment (higher means more natural). We\nclassify the final score as “Bad”, “Low”, “Avg” and “Good” based on\nthe quantiles they lie in.\nnaturalness_score = 1 −\n¯X −min( ¯X)\nmax( ¯X)−min( ¯X) (6)\n14\n© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nFig. 9: (a) Average dependency tree heights vs sum of counts of summaries with the respective height. LLM tends to produce very few summaries\nwith short dependency tree heights with only 97 summaries out of 599 having a dependency tree height of 3.5 or lower as compared to 247 human\nsummaries. Human summaries have less frequency of average dependency tree heights beyond 3.5. This highlights that human text exhibits simpler\nand shorter dependencies more often. (b) Average sentence lengths vs frequency of summaries with the respective length. The blue line denotes\nhumans and the red denotes LLM summaries. It is observed that LLM summaries tend to have longer average sentence lengths beyond the length\nof 18. This indicates that human summaries have fewer instances with longer average sentence lengths as compared to LLM summaries.\n15\nB P ROMPTS\nThe intelligent agents for supporting prompt suggestions and feature\nrecommendation are implemented with a template prompt using Ope-\nnAI’s “gpt-3.5-turbo” model. Each template consists of two parts: a\nsystem prompt that describes the task and provides necessary contexts,\nsuch as feature definitions and prompt block definitions, and a user\nprompt, which is injected with the user’s input. Below, we provide the\ntemplates for prompt suggestions and feature recommendations.\nB.1 Prompt Suggestions\nThe prompt for prompt suggestions takes three inputs: block_name,\nblock_definition, as listed in Table 3 and user question (question):\nrole: system\nYou are a block recommendation assistant. \nYou are given a  block.\nBlock definition:  \nThe user has a simple version of  content. \nHe wants to know how to improve it. \nPlease give suggestions specific to their current \nprompt and intended goal.  \nPoint out what is missing or what can be improved \naccording to the  block definition. \nBe concise and reply with no more than 50 words.\nRespond with a coherent paragraph. Do not use bullet \npoints or lists. \n{block_name}\n{block_definition}\n{block_name}\n{block_name}\ncontent:\nrole: user\ncontent: {question}\nPr ompt Suggestions \nFig. 10: Prompt template for providing prompt suggestions. Grayed-out\nwords are variables to be injected.\nPrompt Block Persona Context Constraints Data\nDescription\nThe persona block should \nspecify a fictional or r ole-\nbased identity f or t he AI t o \nadopt when r esponding t o \npr ompt. The persona \nshould be tight ly r elat ed t o \nt he int ended task and goal. \nThe cont e xt block \nshould pr o vide \ne xt ernal inf ormation \nor additional cont e xt \nt hat can st eer t he \nmodel t o bett er \nr esponses.\nThe constraint block \nshould specify an y \nr equir ement s or \nlimitations f or t he AI t o \nf ollo w . Mak e sur e t he \nr equir ement s ar e v er y \nspecific t o t he \nint ended task and goal.\nThe data block should \nuse delimit ers lik e \ntriple quotation marks, \nXML tags, section \ntit les, et c. t o clearly \nindicat e which par t of \nt he pr ompt is data.\nTable 3: Prompt block definitions derived from prompting methodologies.\nB.2 Feature Recommendation\nThe prompt for feature recommendation takes two inputs: fea-\nture_descriptions as listed in Table 4 and user question ( question),\nand uses the JSON mode functionality to define a structured output:\nrole: system\nYou are an evaluation feature recommendation assistant. \nYou are given a list of features and their definitions.\nFeature definitions:  \nThe user will ask you to recommend the features that \nbest fit their needs.\nFirst, tell the user which features they should use.\nThen, tell them what level of the feature they should \naim for and briefly explain why. \nReply with the following JSON format: \n{ \"features\": [ \n\n\n\n\n        ... ]\n}\n\n\n \n{feature_definitions}\n{ \n    \"feature_name\": string, \n    \"level\": string (one of the provided), \n    \"explanation\": string \n}, \ncontent:\nrole: user\ncontent: {question}\nFeatur e Recommendation \nFig. 11: Prompt template for providing feature recommendations. Grayed-\nout words are variables to be injected.\n16\n© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nMetric Description Range Level Notes\nComplexity\nComplexity metrics aim to quantify the\nreadability of a piece of writing by con-\nsidering various linguistic features, such\nas sentence length, word length, syllable\ncount or semantic difficulty\n0, 10 Elementary Very easy to read. Easily under-\nstood by an 11-year-old student\n10, 40 Middle School\nPlain English. Easily under-\nstood by 13- to 15-year-old stu-\ndents.\n40, 50 High School Fairly difficult to read. Best un-\nderstood by high school students\n50, 90 College Very difficult to read. Best un-\nderstood by college students.\n90, 100 Professional\nExtremely difficult to read. Best\nunderstood by university gradu-\nates.\nFormality\nFormality measures how formal a piece\nof writing is.\n0, 60 Informal Casual and conversational lan-\nguage.\n60, 100 Standard Standard, neutral language.\n100, 200 Formal Professional or academic lan-\nguage\n200, -1 Very Formal Highly technical or legal lan-\nguage\nSentiment\nSentiment aims to determine the attitude\nof a writer with respect to the overall\ncontextual polarity.\n-1, -0.3 Negative Negative sentiment\n-0.3, 0.3 Neutral Neutral sentiment\n0.3, 1 Positive Positive sentiment\nFaithfulness\nFaithfulness measures how broad and\naccurate are the generated summary us-\ning the name entity overlap between\nthe summary and the article. Common\nnamed entities include persons, organi-\nzations, locations, or dates.\n0.0, 0.25 Bad\nAll or Almost all of the impor-\ntant entities from the article are\nmissing in the generated sum-\nmary\n0.25, 0.4 Low\nVery few of the important enti-\nties from the article are present\nin the generated summary\n0.4, 0.6 Avg\nFew of the important entities\nfrom the article are missing in\nthe generated summary\n0.6, 1.0 Good\nAll or almost all the impor-\ntant entities from the article are\npresent in the generated sum-\nmary\nNaturalness\nNaturalness of text is a measure of how\nwell the text flows and sounds like some-\nthing a native speaker would produce. It\nshould be easy to understand and should\nadhere to the rules of the language\n0.0, 0.435 Bad Summary is not natural or\nhuman-like\n0.435, 0.607 Low Summary is somewhat natural\nand human-like\n0.607, 0.715 Avg Summary is mostly natural and\nhuman-like\n0.715, 1.0 Good Summary is natural and human-\nlike\nLength Length measures the number of words\nin the summary.\n0, 100 Short Under 100 words.\n100, 300 Mid 100 to 300 words.\n300, 500 Long 300 to 500 words.\n500, -1 Very long Over 500 words.\nTable 4: Feature descriptions. In the prompt, we combine Metrics, Description, Levels and Notes with a string template.\n17\nFig. 12: x-axis: individual summaries (indexed from 0 to 599), y-axis: variance in truthfulness scores. For each summary, we used three levels of\ndefinitions (no definition, beginner, and expert) to generate a truthfulness score, and calculated the variance (y-axis). We varied the temperature\nsetting (0, 0.3, 0.7, 1.0) to ensure that the variance is persistent. At temperature 0.0 (left), we see that the majority of summaries have variance\nbelow 0.5. At temperature 1.0 (right), the variances are enlarged. The experiment shows that even at its most consistent setting (temperature=0), the\nLLM still shows variance in the generated scores when the metric definitions have varying levels of detail.\nFig. 13: x-axis: individual summary index (indexed from 0 to 599), y-axis: variance in sentiment scores. For each summary, we ran the same prompt\nten times and calculated the variance in the ten generated scores. We varied the temperature setting (0, 0.5, 1.0) to ensure that the variance is\npersistent. We observe that even at temperature 0.0 (left), a significant number of summaries have inconsistent scores, and the most inconsistent\nones have variance=1.0. At temperature 1.0 (right), the variances are enlarged.\nC E XPERIMENT : LLM S AS EVALUATORS\nRecently, LLM evaluators [25, 65, 76], where LLMs are prompted to\noutput a numerical evaluation score, have become more popular due\nto their low technical barrier and high customizability in defining an\nevaluation criterion. However, previous works [34, 42] have shown\nthat LLMs are non-deterministic and sensitive to slight changes in\nprompts in diverse NLP contexts, such as sentiment classification, code\ngeneration, and text summarization. Shen et al. [52] concluded that\nLLMs are not robust enough as human level evaluators. Building\nupon these works, our experiment is designed to explore, from the\nend-user’s perspective, how consistent LLMs are as evaluator in text\nsummarization. We seek to answer the following research questions:\n• RQ1: How do minor variations in the prompt defining an evalua-\ntion metric affect the variability of the metric score?\n• RQ2: Do LLMs maintain consistency in metric scores when the\nsame prompt defining an evaluation metric is applied repeatedly?\nProcedure To investigateRQ1, we chose three evaluation metrics:\nsentiment, readability, and truthfulness, ranging from 1 to 5. For each\nmetric, we created three prompts with varying levels of definition: (1)\nno definition, (2) a beginner-level, non-technical definition, and (3) an\nexpert-level, detailed definition. A prompt with no definition of eval-\nuation criterion represents prompt designers with no prior knowledge\nof natural language evaluation. The beginner-level prompt provides\na naive way of defining an evaluation metric representative of non-\ntechnical prompt designers. The expert-level prompt gives a detailed\ndefinition of the metric with breakdown of scores to better align the re-\nsponses according to the user’s intent, representing experienced prompt\ndesigners. These prompts were then used to evaluate all summaries in\nthe dataset. We calculated the variance between the three definitions to\ninspect the sensitivity of LLMs. The same procedure is repeated under\nfour temperature settings: 0.0, 0.3, 0.7, and 1.0.\nTo investigateRQ2, We chose two evaluation metrics, sentiment and\nreadability, each with a single prompt that includes a beginner-level\ndefinition of the metric, outputing a score ranging from 1 to 5. For\neach metric, we repeatedly applied the same prompt to the dataset 10\ntimes, and then examined the variance in the generated scores. The\nsame procedure is repeated under three temperature settings of 0.0,\n0.5, and 1.0. The specific prompts employed in these experiments are\ndocumented in Table 7.\nOur experiment utilizes the dataset released by Zhang et al. [74]\ncomprising 599 articles and corresponding summaries generated by\nLLMs. For sentiment and readability, we only assess the summaries.\nFor truthfulness, we take each pair of article and summary as input.\nFor the model, We utilized OpenAI’s “gpt-3.5-turbo” for generation\nof all metric scores. We do not use GPT-4 due to its cost of usage and\n18\n© 2024 IEEE. This is the author’s version of the article that has been published in IEEE Transactions on Visualization and\nComputer Graphics. The final version of this record is available at: https://doi.org/10.1109/TVCG.2024.3456398\nrate limit restrictions. Moreover, the non-determinism of GPT-4 as\ncompared to GPT-3.5-turbo remains to be persistent [42].\nConclusion and discussion We analyze the variance in metric\nscores across the three levels of definitions to address RQ1. From Fig-\nure 12, we observe the inconsistency in the truthfulness scores for three\ndifferent prompts defining a single metric. The temperature parameter\nbalances the consistency-creativity trade-off, with 0.0 temperature be-\ning their most consistent setting and 1.0 the most creative. However,\neven at 0.0 temperature, the variance across the three truthfulness scores\nassigned by the three definitions is non-zero for a significant amount of\nsummaries, highlighting the inconsistencies in scoring. The variance in-\ncreases as temperature is increased, with 1.0 temperature setting having\nthe highest variance. Similar patterns are observed for readability and\nsentiment. This indicates that variations in the evaluation prompts can\nlead to variance in scores. Consequently, allowing users to define new\nevaluation metrics can introduce more uncertainty to the system. The\nexact variance in scores for the three metrics across the three definitions\nfor the whole dataset is documented in Table 5.\nTo address RQ2, we analyze the variance of scores when the same\nprompt is applied multiple times to the same dataset. From Figure 13,\nwe observe that even at 0.0 temperature, the variance of sentiment\nscores is non-zero for a significant number of summaries. As the tem-\nperature is increased, we observe a higher variance in scores and more\nnumber of summaries with non-zero variance, with 1.0 temperature\nsetting being the most inconsistent. We observe similar patterns in\nreadability. This indicates that the application of an identical prompt\nto the same group of summaries can yield varying scores, leading to\nconcerns about the reliability of the results amongst users. The exact\nvariances observed are given in Table 6.\nMetric Temperature\n0.0 0.3 0.7 1.0\nSentiment 0.192 0.197 0.196 0.222\nReadability 0.104 0.103 0.131 0.160\nTruthfulness 0.110 0.138 0.207 0.265\nTable 5: Variance of Sentiment, Readability, and Truthfulness measured\nacross the scores generated by the three levels of definitions at different\ntemperatures.\nMetric Temperature\n0.0 0.5 1.0\nSentiment 0.057 0.174 0.334\nReadability 0.025 0.124 0.191\nTable 6: Variance of scores for Sentiment and Readability measured\nacross the ten iterations of the same dataset.\nLimitations Given the resource constraints, our experiment was\nconducted at a rather small scale, using a single dataset, 10 iterations,\nthree different metrics, and four temperature settings. For a more ro-\nbust conclusion, the experiment can be extended to cover more diverse\ndatasets and hyperparameter settings. Also, at a larger scale, a compre-\nhensive statistical analysis can be added to examine the robustness of\nthe experiment. For this experiment, we targeted the application sce-\nnario in the Awesumsystem to more robustly show the inapplicability of\nLLM evaluators, and leave a more comprehensive evaluation of LLM\nevaluators for future research.\n19\nMetric Prompt Definition Prompt\nSentiment\nNo Definition\nrole: system\ncontent: The user will provide a summary to be evaluated. Evaluate the sentiment of the summary. Reply strictly with a single digit score from\nthis list of scores: 1: Negative, 2: Slightly Negative, 3: Neutral, 4: Slightly Positive, 5: Positive.\nrole: user\ncontent: {Summary}\nBeginner\nrole: system\ncontent: The user will provide a summary to be evaluated. The metric to be evaluated is sentiment, which refers to the emotional tone or attitude\nexpressed within a given text. Evaluate the sentiment of the summary. Reply strictly with a single digit score from this list of scores: 1: Negative,\n2: Slightly Negative, 3: Neutral, 4: Slightly Positive, 5: Positive. role: user\ncontent: {Summary}\nExpert\nrole: system\ncontent: The user will provide a summary to be evaluated. Sentiment refers to the emotional tone or attitude expressed within a given text.\nEvaluate the sentiment of the summary. The possible scores of sentiment are:\n1: Negative, Strongly unfavorable or pessimistic tone. Expresses disapproval, dissatisfaction, or criticism.\n2: Mildly negative sentiment. Indicates some negativity, but not as intense as the “Negative” category.\n3: Neutral, Neither positive nor negative. Presents information objectively without emotional bias.\n4: Mildly positive sentiment. Conveys a favorable or optimistic tone, but not strongly so.\n5: Positive, Strongly favorable or enthusiastic. Expresses approval, satisfaction, or delight.\nReply strictly with a single digit score.\nrole: user\ncontent: {Summary}\nReadability\nNo definition\nrole: system\ncontent: The user will provide a summary to be evaluated. Evaluate the readability of the summary. You need to assign a score on a scale of 1 to 5.\nReply strictly with a single digit score.\nrole: user\ncontent: {Summary}\nBeginner\nrole: system\ncontent: The user will provide a summary to be evaluated. The metric to be evaluated is readability, which is defined as the ease with which a\nhuman can understand a written text. Evaluate the readability of the summary. You need to assign a score on a scale of 1 to 5. Reply strictly with\na single digit score.\nrole: user\ncontent: {Summary}\nExpert\nrole: system\ncontent: The user will provide a summary to be evaluated. Readability is defined as the ease with which a human can understand a written text.\nEvaluate the readability of the summary. You need to assign a score on a scale of 1 to 5. The breakdown of the scores is as follows:\n1: Very difficult to read, but can be read by professionals.\n2: Difficult to read, but can be read by well-read individuals.\n3: Fairly difficult to read but can be read by middle-aged students\n4: Easy to read, can be read by most people\n5: Very easy to read\nReply strictly with a single digit score.\nrole: user\ncontent: {Summary}\nTruthfulness\nNo definition\nrole: system\ncontent: The user will provide an article and a summary for that article. Evaluate the truthfulness of the summary, with the article as the ground\ntruth. You need to assign a score on a scale of 1 to 5. Reply strictly with a single digit score.\nrole: user\ncontent: {Article, Summary}\nBeginner\nrole: system\ncontent: The user will provide an article and a summary for that article. Evaluate the truthfulness of the summary with the article as the ground\ntruth. Truthfullness refers to how accurately the summary reflects the actual facts and content of the original article. You need to assign a score on\na scale of 1 to 5. Reply strictly with a single digit score.\nrole: user\ncontent: {Article, Summary}\nExpert\nrole: system\ncontent: The user will provide an article and a summary for that article. Evaluate the truthfulness of the summary, with the article as the ground\ntruth. You need to assign a score on a scale of 1 to 5. The breakdown of scores is as follows:\n1: Not Truthful: Information given is completely false or misleading, no element of truth.\n2: Slightly Truthful: Information has small elements of truth, but is largely flawed or inaccurate, significant omissions of relevant facts.\n3: Moderately Truthful: Information is a mix of truth and inaccuracies. Its truthfulness may depend on the interpretation or context.\n4: Mostly Truthful: high level of truth. May contain minor errors or omissions, or slightly misleading.\n5: Completely Truthful: The information presented is entirely accurate, factual, and free of any misleading elements or omissions.\nReply strictly with a single digit score.\nrole: user\ncontent: {Article, Summary}\nTable 7: Prompts with different levels of definitions for different feature metrics, each prompt used as a separate evaluation system for observing\nvariance in scores assigned by LLMs. Grayed-out words are inputs to the prompt.\n20",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8701999187469482
    },
    {
      "name": "Automatic summarization",
      "score": 0.7398156523704529
    },
    {
      "name": "Workflow",
      "score": 0.7241417765617371
    },
    {
      "name": "Natural language generation",
      "score": 0.49248751997947693
    },
    {
      "name": "Feature (linguistics)",
      "score": 0.46777597069740295
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4662576913833618
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4431939423084259
    },
    {
      "name": "Machine learning",
      "score": 0.38901349902153015
    },
    {
      "name": "Data science",
      "score": 0.36306315660476685
    },
    {
      "name": "Information retrieval",
      "score": 0.33490973711013794
    },
    {
      "name": "Natural language processing",
      "score": 0.3232153058052063
    },
    {
      "name": "Natural language",
      "score": 0.2996380925178528
    },
    {
      "name": "Database",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I84218800",
      "name": "University of California, Davis",
      "country": "US"
    }
  ],
  "cited_by": 11
}