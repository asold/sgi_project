{
  "title": "A Systematic Review of ChatGPT and Other Conversational Large Language Models in Healthcare",
  "url": "https://openalex.org/W4395688696",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2128372248",
      "name": "Leyao Wang",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2110339547",
      "name": "Zhiyu Wan",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A4208129010",
      "name": "Congning Ni",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2101427409",
      "name": "Qingyuan Song",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2079581369",
      "name": "Yang Li",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2553388713",
      "name": "Ellen Wright Clayton",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2145238237",
      "name": "Bradley A Malin",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2119903132",
      "name": "Zhijun Yin",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2128372248",
      "name": "Leyao Wang",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2110339547",
      "name": "Zhiyu Wan",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A4208129010",
      "name": "Congning Ni",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2101427409",
      "name": "Qingyuan Song",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2079581369",
      "name": "Yang Li",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2553388713",
      "name": "Ellen Wright Clayton",
      "affiliations": [
        "Vanderbilt University Medical Center"
      ]
    },
    {
      "id": "https://openalex.org/A2145238237",
      "name": "Bradley A Malin",
      "affiliations": [
        "Vanderbilt University Medical Center",
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2119903132",
      "name": "Zhijun Yin",
      "affiliations": [
        "Vanderbilt University Medical Center",
        "Vanderbilt University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384561707",
    "https://openalex.org/W4366591012",
    "https://openalex.org/W4376872703",
    "https://openalex.org/W4320920036",
    "https://openalex.org/W4379232324",
    "https://openalex.org/W4385346108",
    "https://openalex.org/W4385242971",
    "https://openalex.org/W4327946446",
    "https://openalex.org/W4323307381",
    "https://openalex.org/W4383346782",
    "https://openalex.org/W4294215472",
    "https://openalex.org/W4381838911",
    "https://openalex.org/W4361226605",
    "https://openalex.org/W4380995257",
    "https://openalex.org/W4380291180",
    "https://openalex.org/W4323050332",
    "https://openalex.org/W4381085670",
    "https://openalex.org/W4366124521",
    "https://openalex.org/W4379769651",
    "https://openalex.org/W4327941377",
    "https://openalex.org/W4383059305",
    "https://openalex.org/W4363676725",
    "https://openalex.org/W4365503932",
    "https://openalex.org/W4319334927",
    "https://openalex.org/W4380048615",
    "https://openalex.org/W4384407899",
    "https://openalex.org/W4367312135",
    "https://openalex.org/W4324129637",
    "https://openalex.org/W4385563413",
    "https://openalex.org/W4321167341",
    "https://openalex.org/W4377115988",
    "https://openalex.org/W4385156356",
    "https://openalex.org/W4376133327",
    "https://openalex.org/W4379511585",
    "https://openalex.org/W4380483612",
    "https://openalex.org/W4379231355",
    "https://openalex.org/W4353016766",
    "https://openalex.org/W4366462753",
    "https://openalex.org/W4380575774",
    "https://openalex.org/W4380997513",
    "https://openalex.org/W4381743186",
    "https://openalex.org/W4360610463",
    "https://openalex.org/W4379599010",
    "https://openalex.org/W4321459182",
    "https://openalex.org/W4382135134",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4384484700",
    "https://openalex.org/W4377157938",
    "https://openalex.org/W4368340908",
    "https://openalex.org/W4364378939",
    "https://openalex.org/W4381930847",
    "https://openalex.org/W4379877813",
    "https://openalex.org/W4320893927",
    "https://openalex.org/W4383749364",
    "https://openalex.org/W4366816003",
    "https://openalex.org/W4327681325",
    "https://openalex.org/W4385729889",
    "https://openalex.org/W4367060129",
    "https://openalex.org/W4366769280",
    "https://openalex.org/W4366603014",
    "https://openalex.org/W4312091558",
    "https://openalex.org/W4385264929",
    "https://openalex.org/W4384819947",
    "https://openalex.org/W4381682643",
    "https://openalex.org/W4366683734",
    "https://openalex.org/W4378783467",
    "https://openalex.org/W4375842592",
    "https://openalex.org/W4362521774",
    "https://openalex.org/W4327715333",
    "https://openalex.org/W4385847487",
    "https://openalex.org/W4321372824",
    "https://openalex.org/W4377010595",
    "https://openalex.org/W4379259189",
    "https://openalex.org/W4372231834",
    "https://openalex.org/W4385452929",
    "https://openalex.org/W4205170775",
    "https://openalex.org/W2105289139",
    "https://openalex.org/W3166254754",
    "https://openalex.org/W2984546022",
    "https://openalex.org/W2083384763",
    "https://openalex.org/W4384154780",
    "https://openalex.org/W4389917881",
    "https://openalex.org/W4386245555",
    "https://openalex.org/W4366330503",
    "https://openalex.org/W2973113902",
    "https://openalex.org/W246286872",
    "https://openalex.org/W2156098321"
  ],
  "abstract": "Abstract Background The launch of the Chat Generative Pre-trained Transformer (ChatGPT) in November 2022 has attracted public attention and academic interest to large language models (LLMs), facilitating the emergence of many other innovative LLMs. These LLMs have been applied in various fields, including healthcare. Numerous studies have since been conducted regarding how to employ state-of-the-art LLMs in health-related scenarios to assist patients, doctors, and public health administrators. Objective This review aims to summarize the applications and concerns of applying conversational LLMs in healthcare and provide an agenda for future research on LLMs in healthcare. Methods We utilized PubMed, ACM, and IEEE digital libraries as primary sources for this review. We followed the guidance of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRIMSA) to screen and select peer-reviewed research articles that (1) were related to both healthcare applications and conversational LLMs and (2) were published before September 1 st , 2023, the date when we started paper collection and screening. We investigated these papers and classified them according to their applications and concerns. Results Our search initially identified 820 papers according to targeted keywords, out of which 65 papers met our criteria and were included in the review. The most popular conversational LLM was ChatGPT from OpenAI (60), followed by Bard from Google (1), Large Language Model Meta AI (LLaMA) from Meta (1), and other LLMs (5). These papers were classified into four categories in terms of their applications: 1) summarization, 2) medical knowledge inquiry, 3) prediction, and 4) administration, and four categories of concerns: 1) reliability, 2) bias, 3) privacy, and 4) public acceptability. There are 49 (75%) research papers using LLMs for summarization and/or medical knowledge inquiry, and 58 (89%) research papers expressing concerns about reliability and/or bias. We found that conversational LLMs exhibit promising results in summarization and providing medical knowledge to patients with a relatively high accuracy. However, conversational LLMs like ChatGPT are not able to provide reliable answers to complex health-related tasks that require specialized domain expertise. Additionally, no experiments in our reviewed papers have been conducted to thoughtfully examine how conversational LLMs lead to bias or privacy issues in healthcare research. Conclusions Future studies should focus on improving the reliability of LLM applications in complex health-related tasks, as well as investigating the mechanisms of how LLM applications brought bias and privacy issues. Considering the vast accessibility of LLMs, legal, social, and technical efforts are all needed to address concerns about LLMs to promote, improve, and regularize the application of LLMs in healthcare.",
  "full_text": "  1 \n \nA Systematic Review of ChatGPT and Other Conversational Large \nLanguage Models in Healthcare \nAuthors \nLeyao Wang1,#, Zhiyu Wan2,#, Congning Ni1, Qingyuan Song1, Yang Li1, Ellen Wright Clayton3,4, Bradley \nA. Malin1,2,5, Zhijun Yin1,2,* \n \nAffiliations \n1Department of Computer Science, Vanderbilt University, Nashville, TN, USA, 37212 \n2Department of Biomedical Informatics, Vanderbilt University Medical Center, TN, USA, 37203 \n3Department of Pediatrics, Vanderbilt University Medical Center, Nashville, Tennessee, USA, 37203 \n4Center for Biomedical Ethics and Society, Vanderbilt University Medical Center, Nashville, Tennessee, \nUSA, 37203 \n5Department of Biostatistics, Vanderbilt University Medical Center, TN, USA, 37203 \n \n#These authors contributed equally. \n*Corresponding author. Email: zhijun.yin@vanderbilt.edu \n \nAbstract \nBackground: The launch of the Chat Generative Pre-trained Transformer (ChatGPT) in November 2022 \nhas attracted public attention and academic interest to large language models (LLM s), facilitating the \nemergence of many other innovative LLMs. These LLMs have been applied in various fields , including \nhealthcare. Numerous studies have since been conducted regarding how to employ state -of-the-art LLMs \nin health-related scenarios to assist patients, doctors, and public health administrators.  \nObjective: This review aims to summarize the applications and concerns of applying conversational LLMs \nin healthcare and provide an agenda for future research on LLMs in healthcare.  \nMethods: We utilized PubMed, ACM, and IEEE digital libraries as primary sources for this review. We \nfollowed the guidance of Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRIMSA) \nto screen and select peer-reviewed research articles that (1) were related to both healthcare applications and \nconversational LLMs and (2) were published before September 1st, 2023, the date when we started paper \ncollection and screening. We investigated these papers and classified them according to their applications \nand concerns.  \nResults: Our search initially identified 820 papers according to targeted keywords, out of which 65 papers \nmet our criteria and were included in the review.  The most popular conversational LLM was ChatGPT \nfrom OpenAI ( 60), followed by Bard from Google ( 1), Large Language Model Meta AI ( LLaMA) from \nMeta ( 1), and other LLMs ( 5). These papers were classified into four categories in terms of their \napplications: 1) summarization, 2) medical knowledge inquiry, 3) prediction, and 4) administration, and \nfour categories of concerns: 1) reliability, 2) bias, 3) privacy, and 4) public acceptability. There are 49 (75%) \nresearch papers using LLMs for summarization and/or medical knowledge inquiry, and 58 (89%) research \npapers express ing concerns about reliability and /or bias. We found that  conversational LLMs exhibit \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \nNOTE: This preprint reports new research that has not been certified by peer review and should not be used to guide clinical practice.\n  2 \n \npromising results in summarization and providing medical knowledge to patients with a relatively high \naccuracy. However, conversational LLMs like ChatGPT are not able to provide reliable answers to complex \nhealth-related tasks that require specialized domain expertise. Additionally, no experiments in our reviewed \npapers have been conducted to thoughtfully examine how conversational LLMs lead to bias or privacy \nissues in healthcare research. \nConclusions: Future studies should focus on improving the reliability of LLM applications in complex \nhealth-related tasks, as well as investigating the mechanisms of how LLM  applications brought bias and \nprivacy issues. Considering the vast accessibility of LLMs, legal, social, and technical efforts are all needed \nto address concerns about LLMs to promote, improve, and regularize the application of LLMs in healthcare. \n \nKeywords: large language model, ChatGPT, artificial intelligence, natural language processing, \nhealthcare, summarization, medical knowledge inquiry, reliability, bias, privacy.  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  3 \n \nIntroduction \nSince Chat Generative Pre-trained Transformer (ChatGPT) was released on November 30th, 2022, extensive \nattention has been d rawn to generative AI and large language models (LLMs) [1]. ChatGPT is a \nrepresentative conversational LLM that generates text based on its training on an extremely large amount \nof data from mostly the public domain [1]. Modern LLMs (such as GPT -4) incorporate in-text learning, \nwhich enables them to interpret and generalize user inputs in the form of natural language prompts that \nrequire little to no fine-tuning [2]. These LLMs have surpassed the limits of prior incarnations and are now \ncapable of performing various complex natural language processing (NLP) tasks, including translation and \nquestion-answering [3]. In comparison to traditional chatbots, the current array of conversational LLMs \ncan generate seemingly human-like coherent texts [3]. Moreover, since these models are trained on \npublications from online libraries such as Common Crawl and Wikipedia, they can generate seemingly \nscientific and competent answers [4]. \nDue to the high quality of their responses and the broad training database of modern LLMs, a growing body \nof studies has emerged regarding the applications of chatbots, particularly ChatGPT, in the domain of health \nand medicine [5]. However, most LLMs are not specially designed for healthcare and, as a result,  certain \npractical pitfalls may exist when they are put into practice  in that setting. Thus, there is a need to compile \nthe latest achievements in this domain so that potential issues and guidance for new research directions can \nbe laid out. Several reviews have been published to discuss the appropriateness of a particular application \nof LLMs in a specific aspect  [1,6-10] but none of them summarized the overall problems systematically  \n[8]. For example, Huang et al. summarized only the application of ChatGPT in dentistry without considering \nthe broader landscape of other subfields in healthcare  [6]. Mesko and  Topol only discussed regulatory \nchallenges, neglecting concerns about reliability in their application, such as their accuracy and consistency \nof responses [10]. Wang et al. discussed the ethical considerations of using ChatGPT in healthcare [7], they \ndid not consider other LLMs  for analysis,  account for other common challenges such as reliability , or \nmention detailed applications  of the models . While Snoswell et al. reviewed applications of LLMs in \nmedicine, they did not  conduct a systematic revie w [9].  Moreover, their work focused on LLMs’ \neducational and research applications  rather than their clinical usage. Although Sallam conducted a \nsystematic review [8], the articles Sallam considered were mostly editorials, letters to the editors, opinions, \ncommentaries, news articles, and preprints, as opposed to research articles. In addition, Sallam focused on \neducational and research applications of ChatGPT only. \nThis review focuses on peer -reviewed research articles on conversational LLMs that emerged after \nChatGPT, which was initially based on GPT-3, and their applications in healthcare. We aim to summarize \nthe applications of conversational LLMs in the field of healthcare with concrete experiments and identify \npotential concerns about using such LLMs in this field that need to be addressed in the future.   \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  4 \n \nMethods \nWe searched for papers that contain ed at least one word associated with LLMs {“ChatGPT”, “LLaMA”, \n“GPT-3”, “LaMDA” , “PalM”, “MT-NLG”, “GATO”, “BLOOM”, “Alpaca”, “Large Language Model” } \nand at least one word associated with healthcare { “health”, “diagnosis”, “intervention” , “patient”} \npublished before September 1 st, 2023 on PubMed,  Association for Computing Machinery (ACM) Digital \nLibrary, and Institute of Electrical and Electronics Engineers (IEEE) Xplore. This systematic review applied \nthe Preferred Reporting Items for Systematic Reviews and Meta -Analyses (PRIMSA) guidelines to steer \nthe paper search [11]. Relevant publications were gathered and downloaded on September 3 rd, 2023. For \nsimplicity, all the LLMs mentioned henceforth refer to conversational LLMs.  \nThe inclusion criteria for a paper are that 1) it was published as a peer-reviewed scientific research article \nbetween November 1 st, 2022 , and September 1 st, 2023 , and 2) it focuses on applications of LLMs in \naddressing a healthcare-related problem, which includes, but is not limited to, promotion of personal or \npublic health and well-being or the potential to alleviate the workload of healthcare providers. We excluded \na paper if it  was (1) not a peer-reviewed research article; (2) not related to healthcare applications (e.g., \nLLMs applied to preparing manuscripts for peer-review); (3) not accessible; (4) a duplicate of another paper \nconsidered; or (5) about LLMs released before GPT -3, such as BERT. We excluded BERT-related papers \nbecause this LLM, which was built upon the encoder of  a transformer, is mainly applied in fine-tuning \ndownstream machine-learning tasks. While the implementation of a chatbot based on BERT is feasible, it \nwaned in popularity as an LLM after the introduction of ChatGPT, which was built upon the decoder of a \ntransformer. The complete set of papers meeting the criteria were downloaded from the three digital libraries \nfor further screening. Specifically, five of the authors of this review (LW, ZW, CN, QS, and YL) participated \nin paper screening and summarization under the supervision of the corresponding author ZY.  A screening \nprotocol was created collectively after the team jointly reviewed 50 randomly selected papers. Each \nunreviewed paper was then screened by not fewer than two authors based on the protocol. All the papers in \nthe final collection were summarized by the co -authors according to their LLM applications in healthcare \nand the concerns raised.  \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  5 \n \nResults \nFigure 1  demonstrates the paper selection  process. The initial keyword search identified a total of 820 \narticles, with 736 articles from PubMed, 49 papers from ACM Digital Library, and 35 papers from IEEE \nXplore. The evaluation of the 820 articles was distributed among the authors  for screening the titles and \nabstracts. The inter -rater reliability was assessed by computing a Kappa score, yielding a value of 0.72.  \nAfter screening, we excluded 599 articles from PubMed, 46 articles from ACM Digital Library, and 33 \npapers from IEEE Xplore because they were either not relevant to the research topic or not research articles. \nNext, we extracted the full papers of the remaining 142 research articles and manually examined them for \nthe five excluding criteria  (See Methods). This led to a final set of 65 papers for full-paper review and \nsummarization - 63, 2, and 0 from PubMed, ACM Digital Library, and IEEE Xplore, respectively.  Among \nthese selected papers, 60 were related to ChatGPT from OpenAI, 1 was related to Large Language Model \nMeta AI (LLaMA) from Meta, 1 was related to Bard based on Language Model for Dialogue Applications \n(LaMDA) from Google, and 5 of them are related to other LLMs (See Supplemental Table 1). \n \nFigure 1. A flowchart of the article selection process based on the PRIMSA guidelines. PRIMSA: \nPreferred Reporting Items for Systematic Reviews and Meta-Analyses; ACM: Association for Computing \nMachinery; IEEE: Institute of Electrical and Electronics Engineers. \n \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  6 \n \n \nFigure 2. A summary of the applications and concerns about LLMs in healthcare as communicated by the \nreviewed papers. LLMs: large language models. \nFigure 2 illustrates the main topics of applications and concerns mentioned by the reviewed papers on \napplying LLMs in healthcare settings. The multifaceted applications of LLMs can be divided into four \nprimary categories: Summarization, Medical Knowledge Inquiry, Prediction, and Administration. \n• Summarization (25 papers): LLMs are potential tools for summarizing complex information or \ndocumentation in clinical domains.  \n• Medical Knowledge Inquiry (30 papers): LLMs demonstrate proficiency in answering a diverse \narray of medical question s and/or examinations, which enhance  public access to medical \nknowledge.  \n• Prediction (22 papers): LLMs demonstrate high diagnostic accuracy in multiple medical scenarios \n(15 papers), offer virtual assistance in diverse treatments (12 papers), and excel in predicting drug \ninteractions and synergies (1 paper).  \n• Administration (9 papers): LLMs streamline various tasks, including documentation (5 papers) and \ninformation collection (5 papers) to monitor the trend of public health.  \nThe concerns surrounding the application of LLMs in healthcare were varied, each  with nuanced \nconsiderations. \n• Reliability (55 papers): This includes accuracy (45 papers), or the correctness of the responses from \nLLMs; consistency (13 papers), whether LLMs produce the same response to the same questions \nwith different prompts; interpretability (5 papers), whether LLMs can explain their responses well \nand the data quality of the training dataset (16 papers).  \n• Bias (16 papers): The applications of LLMs may result in biased responses, which will exacerbate \ndisparity and inequality in healthcare, particularly in terms of financial costs (1 paper), readability \n(5 papers), and accessibility (3 papers).  \n• Privacy (6 papers): Training LLMs in healthcare settings requires a large number of health data \nwhich, however, is sensitive and may bring privacy issues.  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  7 \n \n• Public Acceptance (4 papers): Building trust  in LLMs from the public  is pivotal for widespread \nacceptance and usage of LLM-based healthcare applications.  \nApplications \nAll reviewed research papers demonstrated the usability or tested the capability of LLMs for healthcare \napplications in clinical or research domains , which can be further classified into the following four \ncategories: summarization, medical knowledge inquiry, prediction, and administration. \nSummarization \nChatGPT has been shown to be effective in summarizing medical documents for a diverse set of \napplications [13,14], including tasks such as adapting clinical guidelines  for diagnosis, treatment, and \ndisease management [15], summarizing medical notes [16-18], assisting in writing medical case reports \n[19-24], and generating and translating radiological reports [18,25]. Notably, efforts have been made to \nintegrate ChatGPT-41 with \"Link Reader\" for automating medical text synthesis [26], which boosted model \nperformance in providing answers according to clinical guidelines  [26]. Another study [23] explored \nChatGPT's role in supporting healthcare professionals in creating medical reports from real patient \nlaboratory results to offer treatment recommendations based on patients’ health conditions [23]. \nChatGPT prove d beneficial for summarizing research papers  as well  [27]. Notably, it demonstrate d \nimpressive performance in summarizing conference panels and recommendations [27], generating research \nquestions [28], extracting data from literature abstracts [29], drafting medical papers based on given \ndatasets [30], and generating references from medical articles [31].  ChatGPT was also utilized to evaluate \nthe quality and readability of online medical text regarding shockwave therapy for erectile dysfunction [32]. \nThese applications highlight ed the potential of LLMs to condense complex and extensive research \nmaterials, allowing for more accessible comprehension and utilization of information in healthcare. \nMedical Knowledge Inquiry \nChatGPT can be applied to  answer questions about healthcare, as evidenced by its excellent performance \nin various studies  [26,33-42]. For instance, ChatGPT has shown remarkable accuracy in reasoning \nquestions and medical exams [43,44], even successfully passing the Chinese Medical Licensing Exam [45] \nand the United States Medical Licensing Exam ( USMLE) [46]. It also perform ed well in addressing \nradiation oncology physics exam questions [47]. Likewise, “ChatGPT would have been at the 87th percentile \nof Bunting’s 2013 international cohort for the Cardiff Fertility Knowledge Scale and at the 95 th percentile \non the basis of Kudesia’s 2017 cohort for the Fertility and Infertility Treatment Knowledge Score” [48]. In \naddition, ChatGPT showed promising results in a simulated Ophthalmic Knowledge Assessment Program \n(OKAP) exam [49]. However, the average score of ChatGPT was 60.17% in the Membership of the Royal \nCollege of General Practitioners Applied Knowledge Test (AKT), which is below 70.4%, the mean passing \nthreshold in the last 2 years [50]. \nFurthermore, LLMs have been shown to be effective at making medical knowledge accessible to the public. \nIn particular, a fine -tuned chatbot based on LLaMA demonstrate d enhanced performance in identifying \npatients’ need s and providing informed suggestions  [51]. In the realm of medical advice, ChatGPT \ngenerated educational documents, answered questions about allergy and immunology [52], and countered \nvaccine conspiracy theories [53]. It can also answer the most frequently asked questions about the COVID-\n \n1 We use “ChatGPT-4” to represent ChatGPT Plus based on GPT-4 throughout this paper. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  8 \n \n19 pandemic. Its overall responses to queries related to cognitive decline were equivalent to and, at times, \nmore reliable than Google’s [54]. According to Bulck and Moons  [55], in comparison to Google search, \n40% of the 20 experts (19 nurses; 1 dietitian) considered answers from ChatGPT of greater value, 45% \nregarded them as equal value and 15% deemed them less valuable. Therefore, many experts predicted that \npatients will gradually rely more on LLMs (particularly ChatGPT) and less on Google searches due to the \nhigh quality and accessibility of the answers from LLMs. Regarding cancer myths and misconceptions , \n97% of expert reviews deemed answers from ChatGPT to be accurate [56]. In addition, Bird and Lotfi \noptimized a chatbot that could answer mental health -related questions with an accuracy of 89% [57]. \nOverall, LLM s, particularly  ChatGPT, demonstrate an impressive performance in public education in \nhealth. \nPrediction \nLLMs have been shown to have predictive capabilities in diagnosis, treatment recommendations, and drug \ninteractions and synergies. \nDiagnosis. ChatGPT has exhibited the potential to achieve high accuracy in diagnosing specific diseases  \n[14,58], providing diagnostic suggestions in simulated situations  [35,59] or using given lab reports for \ndiagnosis [60]. ChatGPT has been evaluated in dental [6], allergy [52], and mental disorders diagnoses [61]. \nParticularly, GPT-3 can be used to differentiate Alzheimer’s patients from healthy controls using speech \ndata [61]. Beyond ChatGPT, other generative AI frameworks , such as DR.BENCH [62], were employed \nfor clinical diagnostic reasoning tasks  [62]. Moreover, various pre -trained LLMs can extract microbe -\ndisease relationships from biomedical texts in zero -shot/few-shot contexts with high accuracy, with an \naverage F1 score, precision, and recall greater than 80%  [63]. In addition, ChatGPT was the best LLM \nwhen predicting high acuity cases than predicting low acuity cases according to emergency severity index \n(ESI), with a sensitivity of 76.2%, a specificity of 93.1%, compared to the overall sensitivity of 57.1%, \nspecificity of 34.5% [64]. \nFor example, Hirosawa and colleagues [4] obtained ChatGPT’s diagnostic response by describing a clinical \nscenario. The prompt began with “Tell me the top 10 suspected illnesses for the following symptoms”; \nThen, patients’ personal information (e.g., age and family history) was provided in this prompt along with \nother clinical data (e.g., symptoms, medication, and physical examination). According to the study, the top \nten suspected diseases generated by ChatGPT achieved a rate of 93% (28/30) in overall correctness. While \nsuch a level of performance is impressive, physicians still ma de a better prediction than ChatGPT . With \nrespect to the top five diagnoses, physicians achieved an accuracy of 98% while ChatGPT only achieve d \n83%. As to the top suspected disease, ChatGPT only had a correct rate of 53.3%, versus 98.3% achieved \nby physicians [4]. \nTreatment recommendations. LLMs can offer treatment recommendations while listing the side effects \nof these treatments [58]. They have been involved in the treatment of various diseases such as allergy and \nimmunology [52]. ChatGPT can identify guideline-based treatments for advanced solid tumors [65], such \nas breast tumor treatment  [66]. LLMs can also assist with treatment planning [67], and brain glioma \nadjuvant therapy decision -making [21]. Similarly, NYUTron, a large language model trained on \nunstructured clinical notes, has been applied for clinical predictive tasks in treatment s [19]. ChatGPT can \neffectively recommend breast tumor management strategies based on clinical information from ten patients \n[66], enhance clinical workflow, and assist in responsible decision -making in pediatrics [12]. In addition, \nChatGPT can recommend cancer screening given the radiology reports, with an accuracy of 88% [68]. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  9 \n \nOverall, ChatGPT performs well in certain scenarios of disease prevention and screening recommendations. \nDrug synergies. LLMs also demonstrate high utility when characterizing drug effects. Notably, ChatGPT \nwas employed to predict and explain drug-drug interactions [69]. In this study, the LLMs were asked about \npairing or interaction between drugs , and their re sponses are evaluated in terms of correctness and \nconclusiveness. Among the 40 pairs of Drug -Drug-Interactions, 39 responses are correct  for the first \nquestion, and among these 39 correct answers, 19 are conclusive while 20 are inconclusive. For the second \nquestion, 39 are correct among 40 pairs, with 17 answers conclusive and 22 answers inconclusive. \nAdministration \nLLMs can serve a multifaceted role in the realm of healthcare and administrative tasks.  Specifically, \nChatGPT proves instrumental in streamlining administrative processes by generating texts, thereby \nalleviating the associated workload [15]. Moreover, it can be used to track patients’ health status, \nparticularly those with chronic diseases  [70]. Through the analysis of social media slang, GPT -3 aided in \ndeveloping a drug abuse lexicon that was aimed at enhancing the monitoring of drug abuse trends [71]. \nNotably, an LLM-based Chatbot, called CLOV A CareCall built by NA VER AI [2], was applied as a health \ndata-collecting tool in South Korea. Designed for emotionally supporting socially isolated individuals, \nCareCall conducted periodic conversations, generating health reports with metrics like meals, sleep, and \nemergencies. Implemented in 20 cities by May 2022, it target ed solitary adults, notably those with lower \nincomes, and was proven effective in reducing loneliness. Social workers used the generated reports and \ncall recordings to monitor users’ health, resulting in positive feedback and a streamlined workload for public \nhealth workers. \n \nConcerns \nMost of the reviewed research papers pointed out technical and ethical concerns that people harbor with \nrespect to the application of LLMs in healthcare from several perspectives. This can generally be \ncategorized into four groups: 1) reliability, 2) bias, 3) privacy, and 4) public acceptance.  \nReliability \nThe reliability of LLMs is essential to their application in healthcare. It can be related to  accuracy, \nconsistency, and interpretability of LLM responses, and the quality of the training dataset. Specifically, \n100% of prediction -related studies, 7 2% of summarization -related studies, and 93% of studies related to \nmedical knowledge inquiries have reliability concerns (See Supplemental Table 1).  \nAccuracy. Several studies highlight ed that ChatGPT exhibit ed inaccuracies when asked to  respond to \ncertain questions [14,18,23,29,32,34,35,38,43,50,52,53,64,65,67,71,72]. For instance, ChatGPT could \nrespond with  incomplete information  or exhibit an inability to distinguish between truth and falsehood  \n[21,69]. The generative nature of the LLM algorithms will likely fabricate a fake reference to substantiate \nfalse claims  [31], a process that has been referred to as “hallucinations”  [59]. Additionally, such \nhallucinations can be communicated via persuasive prose [42], making it more likely to mislead patients. \nFor example, Jo et al. mentioned that LLMs (specifically CareCall based on NA VER AI in this paper) may \nmake ambitious or impractical promises to patients, which may add extra burden to therapists or cause a \ntrust crisis [2]. \nData Quality. The unreliability of LLMs may be attributed to limitations in data collection sources [58,49]. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  10 \n \nThere are concerns about the model’s limitation in medical knowledge [37] since the general-purpose nature \nof ChatGPT may affect its reliability in self-diagnosis [3]. Recent state-of-the-art LLMs are typically \nconstructed on texts from the Internet rather than verified resources about health and medicine [1]. \nOf greater concern is data availability. Healthcare institutions have shared no identifiable health information \nwith widely accessible LLMs like ChatGPT due to privacy concerns and legal compliances [7] and it is \narduous to collect new data for LLM training  [57]. ChatGPT, for example, was not trained on patients’ \nclinical data [4]. While a description of a clinical scenario without sensitive patient information can be fed \ninto ChatGPT through prompts, it may lead to inaccurate responses [4].  \nAnother contributing factor to inaccuracy is the outdated knowledge base used to train LLMs [21,25,30,41]. \nChatGPT based on GPT3.5 was pre-trained by using data collected until 2021 and does not support Internet \nconnection [49], making it unable to perform appropriately on questions regarding  events that happened \nafter 2021 [42]. \nConsistency. Many authors expressed concerns about the inconsistency of the responses from LLMs \n[21,25,30], where different answers  result from various prompts  of the same question  \n[17,32,39,58,59,64,67,72]. In addition, the output of ChatGPT to the same prompt may vary from user to \nuser [17]. This is because LLMs generate responses in a probabilistic manner [1]. Therefore, nuance in the \nprompts to the LLM may lead to a completely different answer [17].  \nInterpretability. Interpretability is another aspect regarding the reliability of the response. A study by \nCadamuro et al. [60] highlights two key issues with an LLM (particularly ChatGPT) in healthcare. Firstly, \nthe interpretation of some normal results, regarding suspected underlying diseases, was not fully correct . \nSecond, ChatGPT struggled to interpret all the coherent laboratory test s [60], generating superficial and \nincorrect responses.  Indeed, ChatGPT could generate overly general answers without citing original \nreferences [20,40,42].  \nBias \nIt has been noted that ChatGPT has issues with disparity  and bias among different populations . In other \nwords, because certain groups of people have financial, readability, and /or accessibility barriers using \nLLMs, their outcomes of using LLMs will be divergent from others . For example, ChatGPT may exert \nsome financial disparity on the users: u nlike previous versions like GPT -3.5, access to GPT-4 involves a \nmonthly fee [41]. These constraints potentially pose financial barriers, limiting widespread adoption and \nuse of the newer, more advanced models in healthcare applications. \nMoreover, the readability of an LLM’s response may further accentuate health disparity [54]. LLMs like \nChatGPT include texts from scientific websites (e.g. Wikipedia) as their training data, which makes their \nresponses sound professional and sophisticated. However, LLMs may produce biased results [6,52], making \nregulations to prevent bias necessary [27,53].  \nFurthermore, the training data can also be biased. Since recent LLMs are trained based on human-generated \ntexts from the Internet, they also tend to provide biased answers [4]. Besides, algorithms may reinforce \ncurrent health disparities and inequities [63]. Indeed, outputs from ChatGPT have been shown to be biased \nin terms of gender, race, and religion [4]. \nPrivacy \nPrivacy issues are important when training or using LLMs in healthcare settings [6,7,52,70]. All AI systems \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  11 \n \nincluding LLMs in health settings should comply with privacy regulations, including compliance with the \nHealth Insurance Portability and Accountability Act (HIPAA), and implement robust safeguards to ensure \nthe protection of sensitive patient information  [6,7,52]. Specifically, LLMs have three privacy problems. \nFirst, the responses from LLMs may embed training examples  directly, which breaches privacy if the \ntraining examples are identifiable. Second, LLMs may be susceptible to inferential disclosure. For example, \na patient’s membership in a dataset or sensitive attributes may be inferred from LLMs’ responses. Third, it \nmay not be clear whether text data is sufficiently de-identified for the anticipated recipients (which may be \nanyone in the world)  when training LLMs.  For instance, we may be able to de -identify text in a manner \nthat sufficiently thwarts people who are not incentivized to attack the system, but we may not be addressing \nrecipients who run machine-assisted attacks. \nPublic Acceptance \nPublic acceptance, the trust of the public in  the application of LLMs in healthcare, has been mentioned in \none study [3]. A cross-sectional survey-based study shows that 78% of a sample of 476 participants claim \nthat they trust ChatGPT’s diagnosis, most of whom possess a degree of bachelor ’s or even master ’s [3]. \nPeople are inclined to trust this new technique when using ChatGPT, partially due to the convenience of \nobtaining information and the patients’ inclination to search for information [3]. \n \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  12 \n \nDiscussion \nThis systematic review shows that LLMs have been applied to summarization, medical knowledge inquiry, \nprediction, and administration. At the same time, there are four major themes of concern when using these \nmodels in practice, including reliability, bias, privacy, and public acceptance. Specifically, the most popular \napplication (30 out of 65 papers) for LLMs was for medical knowledge inquiries , with the second most \npopular (25) being summarization, followed by prediction  (22), and then administration (9). At the same \ntime, most of the papers expressed concerns about reliability (55), followed by bias (16), then privacy (6), \nand finally public acceptance (4).  \nApplications \nAccording to our systematic review, LLMs were heavily applied in summarization and medical knowledge \ninquiry tasks. The former is probably due to the training method of LLMs, which focuses on its capability \nto summarize documents and paraphrase paragraphs. The latter is due to the inclusion of general medical \nknowledge in the training data. Specifically, in the category of summarization, summarizing medical notes \nis the type of task in which LLMs were applied the most. This is probably due to the simplicity of the task \nand the existence of redundancy in those notes. By contrast, in the genre of medical knowledge inquiry, \ntaking standard medical exams is the type of task in whi ch LLMs were applied the most. This is probably \ndue to the existence of medical questions and answers on the Internet that have been included in the training \ndata of some LLMs such as ChatGPT. \nLLMs were applied in prediction tasks as well. Specifically, in the category of prediction, diagnosis is the \ntype of task in which LLMs were applied but with the most reliability concerns. This is probably because \ndiagnosis is a complex process in comparison to summarization  and/or the current popular LLMs (e.g., \nChatGPT) used insufficient publicly available health datasets for model training . It might also be due to \npoorly constructed prompts without enough accurate information. Thus, LLMs are still not likely to be \nsuitable for generating reliable answers to uncommon questions. In the category of administration, LLMs \nwere applied equally heavily in various tasks such as appointment scheduling, information collection, and \ndocumentation. \nConcerns \nFor those applications of LLMs in healthcare, the two greatest concerns are reliability and bias (including \ndisparity and in equality). These concerns might eventually drive this application away from practical \nimplementation.  \nNotably, about 85% (55 of 65) of the reviewed studies emphasized concerns about the reliability of LLMs’ \nresponses given that it may impact a patient’s health-related behavior. The concerns of reliability arose \nmainly from two aspects: the quality of the training data in terms of data source and data timeliness , and \nthe models themselves. For example, GPT-3.5 was pretrained by using data collected by September 2021, \nand it also does not have access to private health records. Furthermore, most data that are used to train \nLLMs are crawled from the Internet rather than professionally validated sources. In addition, the generative \nnature of LLM may result in seemly professional writing but fabricating responses. However, according to \nShahsavar and Choudhury [3], people are inclined to trust this new technique, due partially to the \nconvenience of obtaining information and the patients’ inclination to search for information. \nThe issue of bias (or disparity) is mentioned in about 25% (16 of 65) of our included references. LLM \nbiases come from the training stage (e.g., biased training data and biased algorithms) and the application \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  13 \n \nstage (e.g., biased user base and biased outcomes). These papers discussed bias es mainly from three \ndifferent aspects: financial costs, readability, and accessibility. For example, Hirosawa et al. [4] pointed out \nthat the bias encoded in human-generated texts will make LLMs generate biased output ; Lee et al. [74] \nconcerned health disparity may result from low readability made by the sophistication of LLM wording; \nand Johnson et al. [56] noted that LLM algorithms tend to reinforce the health disparity and to prevent LLM \nalgorithms from exacerbating current disparity in health.   \nAnother concern that prevent s the wide application of LLMs in hea lthcare is privacy. When using third -\nparty LLMs such as ChatGPT, healthcare organizations face several privacy issues. Although no privacy \nbreach of LLMs regarding patient information has been reported, attacks for other types of private \ninformation targeting ChatGPT have been found [75]. For example, a breach led to the exposure of users’ \nconversations to unauthorized parties  [75]. As ChatGPT interacts with patients directly, it may gather \npersonal health information and so may breach their privacy  [7]. As a result, many medical centers do not \nallow researchers and healthcare providers to use raw patient data as inputs to ChatGPT and other LLMs \nor even ban their access to these services during work  [76]. Training or fine -tuning open-source LLMs \nrequires a large number of clinical data, which may lead to violations of patients’ privacy, perhaps \ninadvertently [6,14,52]. \nLimitations of the Reviewed Papers \nThe reviewed papers demonstrated two common limitations of their approaches. First of all, almost all the \nstudies relied on human experts to rate LLMs’ responses. This is problematic because  the score may be \nsubjective and/or more likely unrepresentative. Correspondingly, future works can focus on designing a \nformal and fair process to evaluate LLMs’ responses from a broad range of stakeholders, including \nresearchers, healthcare providers, patients , or any users  with diverse medical and sociodemographic \nbackgrounds. Second, some of the concerns mentioned in this review  (e.g., bias) are merely researchers’ \nspeculations of the potential risks that were included to provide directions for further work. However, the \nmechanisms of how the training of LLMs leads to such concerns have not been comprehensively examined \nthrough experiments. It is suggested the audience should be wary of taking these concerns for granted or as \nproven facts. \nOpportunities \nAmong all the included papers, few of them propose solutions to improve the reliability of LLMs. First, \nfuture research work should focus more on how to improve the accuracy of LLMs’ responses in the \nhealthcare domain . More specifically, domain -specific health data are demanded for training and fine -\ntuning of LLMs to improve the performance of LLMs in various tasks in the healthcare domain. Therefore, \ndata harmonization and consortia established for LLM training are potential direction s that can benefit the \nbroad research community. Qualified medical professionals can contribute to the creation of the dataset for \nLLM training. This, however, will be expensive in terms of time and effort [2]. Alternatively, using retrieval \naugmented generation (RAG) to augment LLM with external knowledge that is up to date might be a \nsolution for scenarios where accurate in -depth professorial knowledge is required. Second, to prevent the \nhallucination issue, LLMs should be limited to making responses based on validated references. Blockchain \ntechnology can be used in this process to provide validation and traceability. Moreover, a holistic system, \nor a keep-experts-in-the-loop framework, that efficiently facilitates the expert validation process becomes \nimportant in order to improve the accuracy and safety of health LLMs. Third, clinical trials based on health \noutcomes such as mortality and morbidity rates  should be conducted in clinical settings to validate the \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  14 \n \nutility of LLM applications formally [1]. \nHow conversational LLMs lead to bias or privacy issues in healthcare research was not thoughtfully \nexamined with experiments in our reviewed papers. Future studies should first focus on investigating the \nmechanisms of how LLMs caused bias and privacy issues with stringent experiments and then developing \npractical solutions. \nRegarding bias issues , it is suggested that systematic monitoring is necessary to ensure the impartial \nfunctioning of LLMs. However, all these sources discuss bias only with mere sentences and superficial \nsummaries without any experimental investigation. Hence, it is worth not ing that further work should also \nfocus more on conducting experiments to understand how bias impacts the responses of LLMs in \ninformation, diagnosis, recommendation, and surveillance. More specifically, all applications of LLMs in \nhealthcare should be tested regarding the exhibitions of bias, and the bias mitigation strategies such as data \naugmentation and targeted recruitment (e.g., the All of Us Research Program targets the collection of data \nfrom historically underrepresented populations [77]). \nRegarding privacy issues, t wo technical approaches to mitigate the privacy risk while training LLMs are \ndata anonymization [78] and synthetic data generation  [79]. For deep learning models, model inversion \nattacks can potentially infer training data giving model weights  [80]. Considering the exponentially \nincreased open-sourced LLMs with published model weights, a sensitive patient dataset needs to be de -\nidentified [81] or replaced with a synthetic dataset before being used to train or fine -tune an LLM. \nOtherwise, the patients with whom the data are associated should be informed about their participation in \nthe training or fine-tuning process [82]. To solve the privacy issues, legal, social, and technical protection \napproaches need to be implemented together to ensure the privacy and security of the whole process of \ntraining and using LLMs for healthcare applications. \nTo raise the public acceptance level of LLMs, explainable AI should be employed to address the \ninterpretability issues of LLMs by making the training data and model architecture transparent. More \nrigorous experimental studies using LLMs are encouraged in the “AI in medicine” research community to \ndemonstrate or improve  the reliability of LLM  applications in health care. Moreover, stakeholders and \ndecision-makers can propose new policies or regulations to manage the accountability and transparency of \nAI-generated content including the responses from LLMs. \nThere appears to be research that is beginning to address some of these raised issues. For example, Zack et \nal. assessed the potential of GPT -4 to perpetuate racial and gender biases in healthcare  [83]. Hanna et al. \nassessed racial and ethical bias of ChatGPT in healthcare-related text generation tasks [84]. However, more \nresearch studies in these directions are needed to validate these findings and conduct more comprehensive \nand transparent assessments. \nMoreover, almost all the research studies LLMs’ responses in one language . For example, 62 out of 65 \nstudy English, one focuses on Korean [2], one focuses on Chinese [45], and one focuses on Japanese [34]. \nTheir findings cannot be extrapolated to other languages directly. Considering that many patients or people \naround the world or even in the US do not speak English, it is necessary to guarantee that LLMs are usable \nuniversally or equitably and conduct more research to investigate the performance of LLMs in other \nlanguages. \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  15 \n \nLimitations of this Review \nDespite notable findings, this review has several limitations. Firstly, the review used PubMed, ACM Digital \nLibrary, and IEEE Xplore  as the primary source s for the papers. Other sources, such as Scopus, Web of \nScience, and ScienceDirect, may provide additional candidate papers regarding LLMs for Health. However, \nsince PubMed is the main digital library for medical publications, the research findings of this review should \nbe valuable to healthcare researchers or policymakers. Secondly, although this review intended to study the \napplication of state -of-the-art conversational LLMs in healthcare, most of the papers included are about \nChatGPT. This is because ChatGPT is still the most powerful conversational LLM. However, its closed-\nsource nature, which is against its company name - OpenAI, may hurdle its wide application in healthcare, \ndue primarily to the privacy concern when sharing sensitive patient information within prompts with \nOpenAI. Finally, only peer-reviewed papers published before Sep tember 2023 are included in our review. \nAs a result, on the one hand, the latest LLM application developments in this area are not included in this \nreview. Specifically, papers focused on LLMs other than ChatGPT, such as LLaMA, were very limited in \nour initial keyword search result s, and only a few of them are included in this review.  This is a problem \nbecause, while mono-modal conversational LLMs have been applied to many fields in healthcare, the multi-\nmodal LLMs that can process medical images, such as GPT -4, Large Language and Vision Assistant \n(LLaV A) [85] based on LLaMA, and LLaV A-Med [73] based on LLaV A, were just released before \nSeptember 2023 and are still being examined by researchers regarding their capabilities in healthcare \nresearch. As a result, no peer -reviewed research papers about applications of multi -modal LLMs in \nhealthcare have been published before September 2023. The main challenge of the application of multi -\nmodal LLMs in healthcare is that multi-modal LLMs are still not perfect either due to insufficient training \ndata or due to insufficient model parameters. Specifically, w ith the development of computing power, \nreduced computing cost, and reduced data access cost, LLMs can be applied to multimedia-based diagnosis \nand analysis in radiology and other departments. On the other hand, the latest studies addressing the \nconcerns are not included in this review. Although there is research that is beginning to address some of the \nissues raised in the systematic review [83,84], there may not have been sufficient time for all recent papers \nto be deposited into the repositories upon which this investigation relied yet. \nConclusions \nThis review summarize d applications of the state-of-the-art conversational LLMs in healthcare and the \nconcerns that need to be resolved in the future. According to the reviewed research articles, conversational \nLLMs perform well in summarizing health-related texts, answering general questions in healthcare, and \ncollecting information from patients. However, their performance is relatively less satisfying in making \ndiagnoses and offering recommendations based on patients’ symptoms and other information. Most authors \nwere concerned about the accuracy and consistency of the LLM responses, which should be the primary \nissues that researchers need to address in the near future . Nevertheless, other concerns regarding bias and \nprivacy issues also prevent conversational LLMs from being broadly applied in the healthcare domain. \nHowever, these concerns still receive insufficient attention: few studies examine the bias and privacy issues \nin LLMs health -related applications with rigorous scientific experiments. Future research should focus \nmore on conducting such research to investigate the mechanisms of how the training and application of \nconversational LLMs leads to such concerns, and to address these concerns that have been seen on any AI \ntools so that they can be safely applied in the healthcare domain.  \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  16 \n \nAcknowledgements \nLW, ZW, and ZY conceived and designed the study. LW, ZW, CN, QS, YL, and ZY participated in paper \nscreening and summarization . EWC, BAM, and ZY supervised the paper screening, summarization, and \ndiscussion. LW, ZW, and ZY wrote the original draft. All authors wrote the manuscript. All authors read \nand approved the final manuscript. \nThis research was funded, in part, by the following grants  from the National Institutes of Health: \nRM1HG009034 (to EWC and BAM) and R37CA237452 (to ZY).  \n \nConflicts of Interest \nNone declared. \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  17 \n \nReferences \n1. Thirunavukarasu AJ, Ting DSJ, Elangovan K, Gutierrez L, Tan TF, Ting DSW. Large language \nmodels in medicine. Nat Med. 2023;29(8). doi:10.1038/s41591-023-02448-8 \n2. Jo E, Epstein DA, Jung H, Kim YH. Understanding the Benefits and Challenges of Deploying \nConversational AI Leveraging Large Language Models for Public Health Intervention. In: \nConference on Human Factors in Computing Systems - Proceedings. ; 2023. \ndoi:10.1145/3544548.3581503 \n3. Shahsavar Y , Choudhury A. User Intentions to Use ChatGPT for Self-Diagnosis and Health-Related \nPurposes: Cross-sectional Survey Study. JMIR Hum Factors. 2023;10:e47564. doi:10.2196/47564 \n4. Hirosawa T, Harada Y , Yokose M, Sakamoto T, Kawamura R, Shimizu T. Diagnostic Accuracy of \nDifferential-Diagnosis Lists Generated by Generative Pretrained Transformer 3 Chatbot for Clinical \nVignettes with Common Chief Complaints: A Pilot Study. Int J Environ Res Public Health. \n2023;20(4). doi:10.3390/ijerph20043378 \n5. Anghelescu A, Firan FC, Onose G, et al. PRISMA Systematic Literature Review, including with \nMeta-Analysis vs. Chatbot/GPT (AI) regarding Current Scientific Data on the Main Effects of the \nCalf Blood Deproteinized Hemoderivative Medicine (Actovegin) in Ischemic Stroke. Biomedicines. \n2023;11(6). doi:10.3390/biomedicines11061623 \n6. Huang H, Zheng O, Wang D, et al. ChatGPT for shaping the future of dentistry: the potential of \nmulti-modal large language model. Int J Oral Sci. 2023;15(1):29. doi:10.1038/s41368-023-00239-y \n7. Wang C, Liu S, Yang H, Guo J, Wu Y , Liu J. Ethical Considerations of Using ChatGPT in Health \nCare. J Med Internet Res. 2023;25. doi:10.2196/48009 \n8. Sallam M. ChatGPT Utility in Healthcare Education, Research, and Practice: Systematic Review on \nthe Promising Perspectives and Valid Concerns. Healthcare (Switzerland). 2023;11(6). \ndoi:10.3390/healthcare11060887 \n9. Snoswell CL, Falconer N, Snoswell AJ. Pharmacist vs machine: Pharmacy services in the age of \nlarge language models. Res Social Adm Pharm. 2023;19(6):843-844. \ndoi:10.1016/j.sapharm.2023.03.006 \n10. Meskó B, Topol EJ. The imperative for regulatory oversight of large language models (or generative \nAI) in healthcare. NPJ Digit Med. 2023;6(1). doi:10.1038/s41746-023-00873-0 \n11. Moher D, Liberati A, Tetzlaff J, et al. Preferred reporting items for systematic reviews and meta-\nanalyses: The PRISMA statement. PLoS Med. 2009;6(7). doi:10.1371/journal.pmed.1000097 \n12. Kao HJ, Chien TW, Wang WC, Chou W, Chow JC. Assessing ChatGPT’s capacity for clinical \ndecision support in pediatrics: A comparative study with pediatricians using KIDMAP of Rasch \nanalysis. Medicine (United States). 2023;102(25). doi:10.1097/MD.0000000000034068 \n13. Kim HY . A Case Report on Ground-Level Alternobaric Vertigo Due to Eustachian Tube Dysfunction \nWith the Assistance of Conversational Generative Pre-trained Transformer (ChatGPT). Cureus. \n2023;15(3):e36830. doi:10.7759/cureus.36830 \n14. Liu J, Wang C, Liu S. Utility of ChatGPT in Clinical Practice. J Med Internet Res. 2023;25:e48568. \ndoi:10.2196/48568 \n15. Hamed E, Eid A, Alberry M. Exploring ChatGPT’s Potential in Facilitating Adaptation of Clinical \nGuidelines: A Case Study of Diabetic Ketoacidosis Guidelines. Cureus. 2023;15(5):e38784. \ndoi:10.7759/cureus.38784 \n16. Cascella M, Montomoli J, Bellini V , Bignami E. Evaluating the Feasibility of ChatGPT in \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  18 \n \nHealthcare: An Analysis of Multiple Clinical and Research Scenarios. J Med Syst. 2023;47(1):33. \ndoi:10.1007/s10916-023-01925-4 \n17. Robinson A, Aggarwal S. When Precision Meets Penmanship: ChatGPT and Surgery \nDocumentation. Cureus. 2023;15(6):e40546. doi:10.7759/cureus.40546 \n18. Bosbach WA, Senge JF, Nemeth B, et al. Ability of ChatGPT to generate competent radiology \nreports for distal radius fracture by use of RSNA template items and integrated AO classifier. Curr \nProbl Diagn Radiol. Published online April 17, 2023. doi:10.1067/j.cpradiol.2023.04.001 \n19. Jiang LY , Liu XC, Nejatian NP, et al. Health system-scale language models are all-purpose prediction \nengines. Nature. 2023;619(7969):357-362. doi:10.1038/s41586-023-06160-y \n20. Puthenpura V , Nadkarni S, DiLuna M, Hieftje K, Marks A. Personality Changes and Staring Spells in \na 12-Year-Old Child: A Case Report Incorporating ChatGPT, a Natural Language Processing Tool \nDriven by Artificial Intelligence (AI). Cureus. 2023;15(3):e36408. doi:10.7759/cureus.36408 \n21. Haemmerli J, Sveikata L, Nouri A, et al. ChatGPT in glioma adjuvant therapy decision making: \nready to assume the role of a doctor in the tumour board? BMJ Health Care Inform. 2023;30(1). \ndoi:10.1136/bmjhci-2023-100775 \n22. Guirguis CA, Crossley JR, Malekzadeh S. Bilateral V ocal Fold Paralysis in a Patient With \nNeurosarcoidosis: A ChatGPT-Driven Case Report Describing an Unusual Presentation. Cureus. \n2023;15(4):e37368. doi:10.7759/cureus.37368 \n23. Zhou Z. Evaluation of ChatGPT’s Capabilities in Medical Report Generation. Cureus. \n2023;15(4):e37589. doi:10.7759/cureus.37589 \n24. Akhter HM, Cooper JS. Acute Pulmonary Edema After Hyperbaric Oxygen Treatment: A Case \nReport Written With ChatGPT Assistance. Cureus. 2023;15(2):e34752. doi:10.7759/cureus.34752 \n25. Grewal H, Dhillon G, Monga V , et al. Radiology Gets Chatty: The ChatGPT Saga Unfolds. Cureus. \n2023;15(6):e40135. doi:10.7759/cureus.40135 \n26. Hamed E, Sharif A, Eid A, Alfehaidi A, Alberry M. Advancing Artificial Intelligence for Clinical \nKnowledge Retrieval: A Case Study Using ChatGPT-4 and Link Retrieval Plug-In to Analyze \nDiabetic Ketoacidosis Guidelines. Cureus. 2023;15(7):e41916. doi:10.7759/cureus.41916 \n27. Almazyad M, Aljofan F, Abouammoh NA, et al. Enhancing Expert Panel Discussions in Pediatric \nPalliative Care: Innovative Scenario Development and Summarization With ChatGPT-4. Cureus. \n2023;15(4):e38249. doi:10.7759/cureus.38249 \n28. Lahat A, Shachar E, Avidan B, Shatz Z, Glicksberg BS, Klang E. Evaluating the use of large \nlanguage model in identifying top research questions in gastroenterology. Sci Rep. 2023;13(1):4164. \ndoi:10.1038/s41598-023-31412-2 \n29. Chen X, Zhang X, Liu Y , Wang Z, Zhou Y , Chu M. RISK-GPT: Using ChatGPT to construct a \nreliable risk factor database for all known diseases. J Glob Health. 2023;13:03037. \ndoi:10.7189/jogh.13.03037 \n30. Macdonald C, Adeloye D, Sheikh A, Rudan I. Can ChatGPT draft a research article? An example of \npopulation-level vaccine effectiveness analysis. J Glob Health. 2023;13:01003. \ndoi:10.7189/jogh.13.01003 \n31. Bhattacharyya M, Miller VM, Bhattacharyya D, Miller LE. High Rates of Fabricated and Inaccurate \nReferences in ChatGPT-Generated Medical Content. Cureus. 2023;15(5):e39238. \ndoi:10.7759/cureus.39238 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  19 \n \n32. Golan R, Ripps SJ, Reddy R, et al. ChatGPT’s Ability to Assess Quality and Readability of Online \nMedical Information: Evidence From a Cross-Sectional Study. Cureus. 2023;15(7):e42214. \ndoi:10.7759/cureus.42214 \n33. Ali MJ. ChatGPT and Lacrimal Drainage Disorders: Performance and Scope of Improvement. \nOphthalmic Plast Reconstr Surg. 39(3):221-225. doi:10.1097/IOP.0000000000002418 \n34. Kusunose K, Kashima S, Sata M. Evaluation of the Accuracy of ChatGPT in Answering Clinical \nQuestions on the Japanese Society of Hypertension Guidelines. Circ J. 2023;87(7):1030-1033. \ndoi:10.1253/circj.CJ-23-0308 \n35. Altamimi I, Altamimi A, Alhumimidi AS, Altamimi A, Temsah MH. Snakebite Advice and \nCounseling From Artificial Intelligence: An Acute Venomous Snakebite Consultation With ChatGPT. \nCureus. 2023;15(6):e40351. doi:10.7759/cureus.40351 \n36. Lahat A, Shachar E, Avidan B, Glicksberg B, Klang E. Evaluating the Utility of a Large Language \nModel in Answering Common Patients’ Gastrointestinal Health-Related Questions: Are We There \nYet? Diagnostics (Basel). 2023;13(11). doi:10.3390/diagnostics13111950 \n37. Yeo YH, Samaan JS, Ng WH, et al. Assessing the performance of ChatGPT in answering questions \nregarding cirrhosis and hepatocellular carcinoma. Clin Mol Hepatol. 2023;29(3):721-732. \ndoi:10.3350/cmh.2023.0089 \n38. Wagner MW, Ertl-Wagner BB. Accuracy of Information and References Using ChatGPT-3 for \nRetrieval of Clinical Radiological Information. Can Assoc Radiol J. Published online April 20, \n2023:8465371231171125. doi:10.1177/08465371231171125 \n39. Nov O, Singh N, Mann D. Putting ChatGPT’s Medical Advice to the (Turing) Test: Survey Study. \nJMIR Med Educ. 2023;9:e46939. doi:10.2196/46939 \n40. Walker HL, Ghani S, Kuemmerli C, et al. Reliability of Medical Information Provided by ChatGPT: \nAssessment Against Clinical Guidelines and Patient Information Quality Instrument. J Med Internet \nRes. 2023;25:e47479. doi:10.2196/47479 \n41. Moshirfar M, Altaf AW, Stoakes IM, Tuttle JJ, Hoopes PC. Artificial Intelligence in Ophthalmology: \nA Comparative Analysis of GPT-3.5, GPT-4, and Human Expertise in Answering StatPearls \nQuestions. Cureus. 2023;15(6):e40822. doi:10.7759/cureus.40822 \n42. Cunningham AR, Behm HE, Ju A, Peach MS. Long-Term Survival of Patients With Glioblastoma of \nthe Pineal Gland: A ChatGPT-Assisted, Updated Case of a Multimodal Treatment Strategy Resulting \nin Extremely Long Overall Survival at a Site With Historically Poor Outcomes. Cureus. \n2023;15(3):e36590. doi:10.7759/cureus.36590 \n43. Hoch CC, Wollenberg B, Lüers JC, et al. ChatGPT’s quiz skills in different otolaryngology \nsubspecialties: an analysis of 2576 single-choice and multiple-choice board certification preparation \nquestions. Eur Arch Otorhinolaryngol. 2023;280(9):4271-4278. doi:10.1007/s00405-023-08051-4 \n44. Sinha RK, Deb Roy A, Kumar N, Mondal H. Applicability of ChatGPT in Assisting to Solve Higher \nOrder Problems in Pathology. Cureus. 2023;15(2):e35237. doi:10.7759/cureus.35237 \n45. Zhu Z, Ying Y , Zhu J, Wu H. ChatGPT’s potential role in non-English-speaking outpatient clinic \nsettings. Digit Health. 2023;9:20552076231184092. doi:10.1177/20552076231184091 \n46. Kung TH, Cheatham M, Medenilla A, et al. Performance of ChatGPT on USMLE: Potential for AI-\nassisted medical education using large language models. PLOS digital health. 2023;2(2):e0000198. \ndoi:10.1371/journal.pdig.0000198 \n47. Holmes J, Liu Z, Zhang L, et al. Evaluating large language models on a highly-specialized topic, \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  20 \n \nradiation oncology physics. Front Oncol. 2023;13:1219326. doi:10.3389/fonc.2023.1219326 \n48. Chervenak J, Lieman H, Blanco-Breindel M, Jindal S. The promise and peril of using a large \nlanguage model to obtain clinical information: ChatGPT performs strongly as a fertility counseling \ntool with limitations. Fertil Steril. 2023;120(3). doi:10.1016/j.fertnstert.2023.05.151 \n49. Antaki F, Touma S, Milad D, El-Khoury J, Duval R. Evaluating the Performance of ChatGPT in \nOphthalmology: An Analysis of Its Successes and Shortcomings. Ophthalmology science. \n2023;3(4):100324. doi:10.1016/j.xops.2023.100324 \n50. Thirunavukarasu AJ, Hassan R, Mahmood S, et al. Trialling a Large Language Model (ChatGPT) in \nGeneral Practice With the Applied Knowledge Test: Observational Study Demonstrating \nOpportunities and Limitations in Primary Care. JMIR Med Educ. 2023;9:e46599. doi:10.2196/46599 \n51. Li Y , Li Z, Zhang K, Dan R, Jiang S, Zhang Y . ChatDoctor: A Medical Chat Model Fine-Tuned on a \nLarge Language Model Meta-AI (LLaMA) Using Medical Domain Knowledge. Cureus. Published \nonline 2023. doi:10.7759/cureus.40895 \n52. Goktas P, Karakaya G, Kalyoncu AF, Damadoglu E. Artificial Intelligence Chatbots in Allergy and \nImmunology Practice: Where Have We Been and Where Are We Going? J Allergy Clin Immunol \nPract. 2023;11(9):2697-2700. doi:10.1016/j.jaip.2023.05.042 \n53. Sallam M, Salim NA, Al-Tammemi AB, et al. ChatGPT Output Regarding Compulsory Vaccination \nand COVID-19 Vaccine Conspiracy: A Descriptive Study at the Outset of a Paradigm Shift in Online \nSearch for Information. Cureus. 2023;15(2):e35029. doi:10.7759/cureus.35029 \n54. Hristidis V , Ruggiano N, Brown EL, Ganta SRR, Stewart S. ChatGPT vs Google for Queries Related \nto Dementia and Other Cognitive Decline: Comparison of Results. J Med Internet Res. \n2023;25:e48966. doi:10.2196/48966 \n55. Van Bulck L, Moons P. What if your patient switches from Dr. Google to Dr. ChatGPT? A vignette-\nbased survey of the trustworthiness, value, and danger of ChatGPT-generated responses to health \nquestions. European Journal of Cardiovascular Nursing. Published online 2023. \ndoi:10.1093/eurjcn/zvad038 \n56. Johnson SB, King AJ, Warner EL, Aneja S, Kann BH, Bylund CL. Using ChatGPT to evaluate \ncancer myths and misconceptions: artificial intelligence and cancer information. JNCI Cancer \nSpectr. 2023;7(2). doi:10.1093/jncics/pkad015 \n57. Bird JJ, Lotfi A. Generative Transformer Chatbots for Mental Health Support: A Study on \nDepression and Anxiety. In: ACM International Conference Proceeding Series. ; 2023. \ndoi:10.1145/3594806.3596520 \n58. Galido PV , Butala S, Chakerian M, Agustines D. A Case Study Demonstrating Applications of \nChatGPT in the Clinical Management of Treatment-Resistant Schizophrenia. Cureus. \n2023;15(4):e38166. doi:10.7759/cureus.38166 \n59. Liu S, Wright AP, Patterson BL, et al. Using AI-generated suggestions from ChatGPT to optimize \nclinical decision support. J Am Med Inform Assoc. 2023;30(7):1237-1245. \ndoi:10.1093/jamia/ocad072 \n60. Cadamuro J, Cabitza F, Debeljak Z, et al. Potentials and pitfalls of ChatGPT and natural-language \nartificial intelligence models for the understanding of laboratory medicine test results. An assessment \nby the European Federation of Clinical Chemistry and Laboratory Medicine (EFLM) Working Group \non Artificial Intelligence (WG-AI). Clin Chem Lab Med. 2023;61(7):1158-1166. doi:10.1515/cclm-\n2023-0355 \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  21 \n \n61. Agbavor F, Liang H. Predicting dementia from spontaneous speech using large language models. \nPLOS digital health. 2022;1(12):e0000168. doi:10.1371/journal.pdig.0000168 \n62. Sharma B, Gao Y , Miller T, Churpek MM, Afshar M, Dligach D. Multi-Task Training with In-\nDomain Language Models for Diagnostic Reasoning. Proc Conf Assoc Comput Linguist Meet. \n2023;2023(ClinicalNLP):78-85. doi: 10.18653/v1/2023.clinicalnlp-1.10 \n63. Karkera N, Acharya S, Palaniappan SK. Leveraging pre-trained language models for mining \nmicrobiome-disease relationships. BMC Bioinformatics. 2023;24(1):290. doi:10.1186/s12859-023-\n05411-z \n64. Sarbay İ, Berikol GB, Özturan İU. Performance of emergency triage prediction of an open access \nnatural language processing based chatbot application (ChatGPT): A preliminary, scenario-based \ncross-sectional study. Turk J Emerg Med. 2023;23(3):156-161. doi:10.4103/tjem.tjem_79_23 \n65. Schulte B. Capacity of ChatGPT to Identify Guideline-Based Treatments for Advanced Solid \nTumors. Cureus. 2023;15(4):e37938. doi:10.7759/cureus.37938 \n66. Sorin V , Klang E, Sklair-Levy M, et al. Large language model (ChatGPT) as a support tool for breast \ntumor board. NPJ Breast Cancer. 2023;9(1):44. doi:10.1038/s41523-023-00557-8 \n67. Kumari KS, K S A. An Esthetic Approach for Rehabilitation of Long-Span Edentulous Arch Using \nArtificial Intelligence. Cureus. 2023;15(5):e38683. doi:10.7759/cureus.38683 \n68. Haver HL, Ambinder EB, Bahl M, Oluyemi ET, Jeudy J, Yi PH. Appropriateness of Breast Cancer \nPrevention and Screening Recommendations Provided by ChatGPT. Radiology. 2023;307(4). \ndoi:10.1148/radiol.230424 \n69. Juhi A, Pipil N, Santra S, Mondal S, Behera JK, Mondal H. The Capability of ChatGPT in Predicting \nand Explaining Common Drug-Drug Interactions. Cureus. 2023;15(3):e36272. \ndoi:10.7759/cureus.36272 \n70. Montagna S, Ferretti S, Klopfenstein LC, Florio A, Pengo MF. Data Decentralisation of LLM-Based \nChatbot Systems in Chronic Disease Self-Management. In: ACM International Conference \nProceeding Series. ; 2023. doi:10.1145/3582515.3609536 \n71. Carpenter KA, Altman RB. Using GPT-3 to Build a Lexicon of Drugs of Abuse Synonyms for Social \nMedia Pharmacovigilance. Biomolecules. 2023;13(2). doi:10.3390/biom13020387 \n72. Lyu Q, Tan J, Zapadka ME, et al. Translating radiology reports into plain language using ChatGPT \nand GPT-4 with prompt learning: results, limitations, and potential. Vis Comput Ind Biomed Art. \n2023;6(1):9. doi:10.1186/s42492-023-00136-5 \n73. Li C, Wong C, Zhang S, et al. LLaV A-Med: Training a Large Language-and-Vision Assistant for \nBiomedicine in One Day. ArXiv. Preprint posted online on June 1, 2023. \ndoi:10.48550/arXiv.2306.00890 \n74. Lee TC, Staller K, Botoman V , Pathipati MP, Varma S, Kuo B. ChatGPT Answers Common Patient \nQuestions About Colonoscopy. Gastroenterology. 2023;165(2). doi:10.1053/j.gastro.2023.04.033 \n75. Gupta M, Akiri C, Aryal K, Parker E, Praharaj L. From ChatGPT to ThreatGPT: Impact of \nGenerative AI in Cybersecurity and Privacy. IEEE Access. 2023;11. \ndoi:10.1109/ACCESS.2023.3300381 \n76. Nelson F. Many Companies Are Banning ChatGPT. This Is Why. Published online on June 16, 2023. \nURL: https://www.sciencealert.com/many-companies-are-banning-chatgpt-this-is-why [accessed \n2024-04-20] \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  22 \n \n77. The “All of Us” Research Program. New England Journal of Medicine. 2019;381(7):668-676. \ndoi:10.1056/NEJMsr1809937 \n78. Emam K El, Rodgers S, Malin B. Anonymising and sharing individual patient data. BMJ (Online). \n2015;350. doi:10.1136/bmj.h1139 \n79. Chen RJ, Lu MY , Chen TY , Williamson DFK, Mahmood F. Synthetic data in machine learning for \nmedicine and healthcare. Nat Biomed Eng. 2021;5(6). doi:10.1038/s41551-021-00751-8 \n80. Zhang Y , Jia R, Pei H, Wang W, Li B, Song D. The Secret Revealer: Generative Model-Inversion \nAttacks Against Deep Neural Networks. ArXiv. Preprint posted online on April 18, 2020. \ndoi:10.48550/arXiv.1911.07135 \n81. El Emam K, Jonker E, Arbuckle L, Malin B. A systematic review of re-identification attacks on \nhealth data. PLoS One. 2011;6(12). doi:10.1371/journal.pone.0028071 \n82. Cohen IG. What Should ChatGPT Mean for Bioethics? American Journal of Bioethics. 2023;23(10). \ndoi:10.1080/15265161.2023.2233357 \n83. Zack T, Lehman E, Suzgun M, et al. Assessing the potential of GPT-4 to perpetuate racial and gender \nbiases in health care: a model evaluation study. Lancet Digit Health. 2024;6(1). doi:10.1016/S2589-\n7500(23)00225-X \n84. Hanna JJ, Wakene AD, Lehmann CU, Medford RJ. Assessing Racial and Ethnic Bias in Text \nGeneration for Healthcare-Related Tasks by ChatGPT. medRxiv. Preprint posted online on August \n28, 2023. doi:10.1101/2023.08.28.23294730 \n85. Liu H, Li C, Wu Q, Lee YJ. Visual Instruction Tuning. ArXiv. Preprint posted online on December \n11, 2023. doi: 10.48550/arXiv.2304.08485 \n  \n  \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint \n  23 \n \nAbbreviations \nChatGPT: Chat Generative Pre-trained Transformer \nLLMs: large language models \nNLP: natural language processing \nACM: Association for Computing Machinery \nIEEE: Institute of Electrical and Electronics Engineers \nPRIMSA: Preferred Reporting Items for Systematic Reviews and Meta-Analyses \nLLaMA: Large Language Model Meta AI \nLaMDA: Language Model for Dialogue Applications \nUSMLE: United States Medical Licensing Exam \nOKAP: Ophthalmic Knowledge Assessment Program \nAKT: Applied Knowledge Test \nESI: emergency severity index \nHIPAA: Health Insurance Portability and Accountability Act \nRAG: retrieval augmented generation \nLLaV A: Large Language and Vision Assistant \n . CC-BY 4.0 International licenseIt is made available under a \n is the author/funder, who has granted medRxiv a license to display the preprint in perpetuity. (which was not certified by peer review)\nThe copyright holder for this preprint this version posted April 27, 2024. ; https://doi.org/10.1101/2024.04.26.24306390doi: medRxiv preprint ",
  "topic": "Health care",
  "concepts": [
    {
      "name": "Health care",
      "score": 0.6322963237762451
    },
    {
      "name": "Systematic review",
      "score": 0.548852264881134
    },
    {
      "name": "Computer science",
      "score": 0.4688837230205536
    },
    {
      "name": "Psychology",
      "score": 0.3509710133075714
    },
    {
      "name": "Linguistics",
      "score": 0.34651774168014526
    },
    {
      "name": "Political science",
      "score": 0.22635412216186523
    },
    {
      "name": "MEDLINE",
      "score": 0.20751112699508667
    },
    {
      "name": "Philosophy",
      "score": 0.11355885863304138
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ]
}