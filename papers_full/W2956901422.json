{
    "title": "Hello, It’s GPT-2 - How Can I Help You? Towards the Use of Pretrained Language Models for Task-Oriented Dialogue Systems",
    "url": "https://openalex.org/W2956901422",
    "year": 2019,
    "authors": [
        {
            "id": null,
            "name": "Budzianowski, P",
            "affiliations": [
                "University of Cambridge"
            ]
        },
        {
            "id": null,
            "name": "Vulić, I",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2948110372",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W2972437240",
        "https://openalex.org/W2947480709",
        "https://openalex.org/W2915295540",
        "https://openalex.org/W2611029872",
        "https://openalex.org/W2984520708",
        "https://openalex.org/W2953039584",
        "https://openalex.org/W2798914047",
        "https://openalex.org/W2914204778",
        "https://openalex.org/W2581637843",
        "https://openalex.org/W1654173042",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2891732163",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2962717182",
        "https://openalex.org/W2963790827",
        "https://openalex.org/W2101105183",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2132339004",
        "https://openalex.org/W1948566616",
        "https://openalex.org/W2963026768",
        "https://openalex.org/W2950444459",
        "https://openalex.org/W3088987271",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2604698497",
        "https://openalex.org/W2898658996",
        "https://openalex.org/W2963706742",
        "https://openalex.org/W1975244201"
    ],
    "abstract": "Data scarcity is a long-standing and crucial challenge that hinders quick development of task-oriented dialogue systems across multiple domains: task-oriented dialogue models are expected to learn grammar, syntax, dialogue reasoning, decision making, and language generation from absurdly small amounts of task-specific data. In this paper, we demonstrate that recent progress in language modeling pre-training and transfer learning shows promise to overcome this problem. We propose a task-oriented dialogue model that operates solely on text input: it effectively bypasses explicit policy and language generation modules. Building on top of the TransferTransfo framework and generative model pre-training, we validate the approach on complex multi-domain task-oriented dialogues from the MultiWOZ dataset. Our automatic and human evaluations show that the proposed model is on par with a strong task-specific neural baseline. In the long run, our approach holds promise to mitigate the data scarcity problem, and to support the construction of more engaging and more eloquent task-oriented conversational agents.",
    "full_text": "Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 15–22\nHong Kong, China, November 4, 2019.c⃝2019 Association for Computational Linguistics\nwww.aclweb.org/anthology/D19-56%2d\n15\nHello, It’s GPT-2 - How Can I Help You?\nTowards the Use of Pretrained Language Models\nfor Task-Oriented Dialogue Systems\nPaweł Budzianowski1;2;3 and Ivan Vuli´c2;3\n1Engineering Department, Cambridge University, UK\n2Language Technology Lab, Cambridge University, UK\n3PolyAI Limited, London, UK\npfb30@cam.ac.uk, iv250@cam.ac.uk\nAbstract\nData scarcity is a long-standing and crucial\nchallenge that hinders quick development of\ntask-oriented dialogue systems across multiple\ndomains: task-oriented dialogue models are\nexpected to learn grammar, syntax, dialogue\nreasoning, decision making, and language gen-\neration from absurdly small amounts of task-\nspeciﬁc data. In this paper, we demonstrate\nthat recent progress in language modeling pre-\ntraining and transfer learning shows promise\nto overcome this problem. We propose a task-\noriented dialogue model that operates solely\non text input: it effectively bypasses ex-\nplicit policy and language generation modules.\nBuilding on top of the TransferTransfo frame-\nwork (Wolf et al. , 2019) and generative model\npre-training ( Radford et al. , 2019), we vali-\ndate the approach on complex multi-domain\ntask-oriented dialogues from the MultiWOZ\ndataset. Our automatic and human evaluations\nshow that the proposed model is on par with\na strong task-speciﬁc neural baseline. In the\nlong run, our approach holds promise to miti-\ngate the data scarcity problem, and to support\nthe construction of more engaging and more\neloquent task-oriented conversational agents.\n1 Introduction\nStatistical conversational systems can be roughly\nclustered into two main categories: 1) task-\noriented modular systems and 2) open-domain\nchit-chat neural models. The former typically con-\nsist of independently trained constituent modules\nsuch as language understanding, dialogue manage-\nment, and response generation. The main goal of\nsuch systems is to provide meaningful system re-\nsponses which are invaluable in building conversa-\ntional agents of practical value for restricted do-\nmains and tasks. However, data collection and\nannotation for such systems is complex, time-\nintensive, expensive, and not easily transferable\n(Young et al. , 2013). On the other hand, open-\ndomain conversational bots ( Li et al., 2017; Serban\net al. , 2017) can leverage large amounts of freely\navailable unannotated data ( Ritter et al. , 2010;\nHenderson et al. , 2019a). Large corpora allow\nfor training end-to-end neural models, which typ-\nically rely on sequence-to-sequence architectures\n(Sutskever et al. , 2014). Although highly data-\ndriven, such systems are prone to producing unre-\nliable and meaningless responses, which impedes\ntheir deployment in the actual conversational ap-\nplications (Li et al. , 2017).\nDue to the unresolved issues with the end-to-\nend architectures, the focus has been extended to\nretrieval-based models. Here, the massive datasets\ncan be leveraged to aid task-speciﬁc applications\n(Kannan et al. , 2016; Henderson et al. , 2017,\n2019b). The retrieval systems allow for the full\ncontrol over system responses, but the behaviour\nof the system is often highly predictable. It also\ndepends on the pre-existing set of responses, and\nthe coverage is typically insufﬁcient for a mul-\ntitude of domains and tasks. However, recent\nprogress in training high-capacity language mod-\nels (e.g., GPT, GPT-2) ( Radford et al. , 2018, 2019)\non large datasets reopens the question of whether\nsuch generative models can support task-oriented\ndialogue applications. Recently, Wolf et al. (2019)\nand Golovanov et al. (2019) showed that the GPT\nmodel, once ﬁne-tuned, can be useful in the do-\nmain of personal conversations. In short, their\napproach led to substantial improvements on the\nPersona-Chat dataset ( Zhang et al. , 2018), show-\ncasing the potential of exploiting large pretrained\ngenerative models in the conversational domain. 1\nIn this paper, we demonstrate that large gener-\native models pretrained on large general-domain\n1E.g., TransferTransfo ( Wolf et al. , 2019) yields gains in\nall crucial dialogue evaluation measures such as ﬂuency, con-\nsistency and engagingness on the Persona-Chat dataset.\n16\nFigure 1: Dialogue-context-to-text task.\ncorpora can support task-oriented dialogue appli-\ncations. We ﬁrst discuss how to combine a set\nof diverse components such as word tokenization,\nmulti-task learning, and probabilistic sampling to\nsupport task-oriented applications. We then show\nhow to adapt the task-oriented dialogue framework\nto operate entirely on text input, effectively by-\npassing an explicit dialogue management module\nand a domain-speciﬁc natural language generation\nmodule. The proposed model operates entirely\nin the sequence-to-sequence fashion, consuming\nonly simple text as input. The entire dialogue con-\ntext, which includes the belief state, the database\nstate and previous turns, is provided to the decoder\nas raw text. The proposed model follows the re-\ncently proposed TransferTransfo framework ( Wolf\net al., 2019), and relies on pretrained models from\nthe GPT family ( Radford et al. , 2018, 2019).\nOur results in the standard Dialogue-Context-to-\nText task (see Figure 1) on the multi-domain Multi-\nWOZ dataset ( Budzianowski et al., 2018b) suggest\nthat our GPT-based task-oriented dialogue model\nlearns to generate and understand domain-speciﬁc\ntokens, which in turn leads to a seamless adapta-\ntion to particular focused domains. While auto-\nmatic evaluation indicates that our framework still\nfalls slightly short of a strong task-speciﬁc neural\nbaseline, it also hints at the main advantage of our\nframework: it is widely portable and easily adapt-\nable to a large number of domains, bypassing the\nintricate modular design only at a small cost in per-\nformance. Furthermore, user-centered evaluations\nsuggest that there is no signiﬁcant difference be-\ntween the two models.\n2 From Unsupervised Pretraining to\nDialogue Modeling\nTask-oriented dialogue modeling requires substan-\ntial amounts of domain-speciﬁc manually labeled\ndata. A natural question to ask is: Can we leverage\ntransfer learning through generative pretraining on\nlarge unlabelled corpora to enable task-oriented di-\nalogue modeling. In this work, we rely on the stan-\ndard language modeling (LM) pretraining, where\nthe task is to predict the next word given the pre-\nceding word sequence ( Bengio et al. , 2003). The\nobjective maximizes the likelihood over the word\nsequence S = fw1; :::; wjSjg:\nL1(S) =\njSj∑\ni=1\nlog P(wijw0; w1; :::; wi\u00001): (1)\nTransfer learning based on such LM pretraining\ncombined with the Transformer decoder model\n(Vaswani et al. , 2017) resulted in signiﬁcant\nprogress across many downstream tasks ( Rei,\n2017; Howard and Ruder , 2018; Radford et al. ,\n2018, 2019).\n2.1 TransferTransfo Framework\nGolovanov et al. (2019) and Wolf et al. (2019)\nachieved a ﬁrst successful transfer of a genera-\ntive pretrained GPT model to an open-domain di-\nalogue task. The pretrained GPT model is ﬁne-\ntuned in a multi-task learning fashion following\nthe original work ( Radford et al. , 2018). The LM\nobjective from Eq. ( 1) is combined with the next\nutterance classiﬁcation task:\np(c; a) =softmax(hl \u0003 Wh): (2)\nc and a represent the context of the conversation\n(c) and a proposed answer ( a), hl is the last hidden\nstate of the transformer decoder, and Wh is learnt\nduring the ﬁne-tuning phase. The model signiﬁ-\ncantly improves upon previous baselines over all\nautomatic dialogue evaluation metrics as well as\nin evaluation with human subjects when evaluated\non the Persona-Chat dataset ( Zhang et al. , 2018).\nThe GPT input consists of token embeddings\nand positional embeddings. In order to move from\na single-speaker setting to a setting with two inter-\nlocutors, Wolf et al. (2019) introduced dialogue-\nstate embeddings. These embeddings inform the\nmodel whether the current token comes from an\nutterance of the ﬁrst speaker or an utterance of the\nsecond speaker. The dialogue-state embeddings\nare learned during the ﬁne-tuning phase.\n3 Domain Transfer for (Task-Oriented)\nDialogue Modeling\nWe now brieﬂy discuss several advances in model-\ning of natural language that facilitate applicability\n17\nFigure 2: The framework for modeling task-oriented conversations based on a pretrained GPT model which uses\nonly unstructured simple text as input. The context, belief state, and database state are joined together without\nexplicit standalone dialogue policy and generation modules. The token-level (i.e., dialogue-state) embeddings are\nlearned following Wolf et al. (2019).\nof pretrained generative models in task-oriented di-\nalogue modeling. To the best of our knowledge,\nthis work is ﬁrst to combine these existing compo-\nnents to enable task-oriented dialogue modeling.\n3.1 Domain Adaptation and Delexicalization\nDealing with out-of-vocabulary (OOV) words has\nbeen a long-standing challenge in dialogue mod-\neling, e.g., it is crucial for task-oriented genera-\ntion where the generated output is often delexical-\nized (Wen et al. , 2015). Delexicalization replaces\nslot values by their corresponding (generic) slot\ntokens and it allows learning value-independent\nparameters. Recently, owing to subword-level to-\nkenisation ( Sennrich et al. , 2016), language mod-\nels are now able to deal with OOVs and domain-\nspeciﬁc vocabularies more effectively ( Radford\net al., 2018).\n3.2 Simple Text-Only Input\nThere have been some empirical validations re-\ncently which suggest that posing NLP tasks in the\nform of simple text can yield improvements with\nunsupervised architectures ( Wolf et al., 2019; Rad-\nford et al. , 2019). For instance, in task-oriented\ndialogue modeling the Sequicity model ( Lei et al. ,\n2018) sees the classiﬁcation over the belief state\nas a generation problem. That way, the entire dia-\nlogue model pipeline is based on the sequence-to-\nsequence architecture: the output from one model\nis the input to the subsequent recurrent model. We\nfollow this approach by providing both the belief\nstate and the knowledge base state in a simple text\nformat to the generator. This signiﬁcantly simpli-\nﬁes the paradigm of building task-oriented mod-\nels: any new source of information can be simply\nadded to as another part of the text-only input pro-\nvided in “natural language”.\n3.3 Transferring Language Generation\nCapabilities\nTransformer architecture shows ability to learn\nnew (i.e., domain-speciﬁc) token embeddings in\nthe ﬁne-tuning phase ( Radford et al. , 2018; Wolf\net al., 2019). This means that the GPT models can\nadapt through special tokens to particular tasks.\nBy providing the input representation as text with\ndomain-speciﬁc tokens, we can use off-the-shelf\narchitectures and adapt to the domain-speciﬁc in-\nput without the need of training new dialogue sub-\nmodules. As mentioned in § 2.1, the token level\nlayer (Figure 2) informs the transformer decoder\nwhat part of the input comes from the system side\nor from the user side. In our framework, we create\ntwo task-oriented speciﬁc tokens ( System and\nUser tokens) that are learned during ﬁne-tuning.\n3.4 Generation Quality\nFinally, the long-standing problem of dull and\nrepetitive response generation ( Li et al. , 2017) has\nbeen in the focus of recent work ( Kulikov et al. ,\n2018; Holtzman et al. , 2019). Owing to new sam-\npling strategies, generative models are now able to\ncreate longer and more coherent sequence outputs.\nThis has been validated also for open-domain di-\nalogue modeling ( Wolf et al. , 2019; Golovanov\net al. , 2019). We experiment with standard de-\ncoding strategies as well as with the recently\nproposed nucleus sampling procedure ( Holtzman\net al. , 2019). A standard greedy samplingstrategy\n18\nchooses the most probable word as :\narg max\nwi\n= logP(wijw0; w1; :::; wi\u00001):\nOn the other hand, nucleus sampling is restricted\nonly to words from the p-th percentile of the dis-\ntribution during generation. The probabilities of\nwords for which the cumulative sum exceeds the\npercentile are rescaled and the sequence is sam-\npled from this subset. We probe the ability of such\nlarge pretrained models to generate more varied\nand semantically richer responses relying on nu-\ncleus sampling in lieu of greedy sampling without\nhurting the actual performance.\n4 Fine-Tuning GPT on MultiWOZ\nTo evaluate the ability of transferring the GPT gen-\neration capability to constrained/focused dialogue\ntasks and domains, we rely on the multi-domain\nMultiWOZ dataset ( Budzianowski et al. , 2018b).\nMultiWOZ consists of 7 domains and 10; 438 di-\nalogues and it is substantially larger than previ-\nous available datasets ( Wen et al. , 2017; El Asri\net al. , 2017). The conversations are natural as\nthey were gathered through human-human inter-\nactions. However, the dialogues are based on\ndomain-speciﬁc vocabulary such as booking IDs\nor telephone numbers that need to be delexicalized\nas they are entirely database-dependent.\nNatural Language as (the Only) Input. GPT\noperates solely on the text input. This is in oppo-\nsition to the standard task-oriented dialogue archi-\ntectures (Wen et al., 2017; Zhao et al. , 2017) where\nthe belief state and the database state are encoded\nin a numerical form. For example, the database\nstate is typically deﬁned as n-bin encodings repre-\nsenting a number of available entities at the current\nstate of the conversation ( Wen et al., 2017). There-\nfore, we transform the belief state and the knowl-\nedge base representation to a simple text represen-\ntation. The belief state takes the following form:\nDomain1 Slot1 Value1 Slot2 Value2\nDomain2 Slot1 ...\nand the database representation is provided as:\nDomain1 # of entities\nDomain2 # of entities ...\nThis is also similar in spirit to the Sequicity archi-\ntecture ( Lei et al. , 2018) where the second recur-\nrent model takes as input the belief state in the nat-\nural language (i.e., simple text-only) form. In this\nwork, we also transform the knowledge base state\nto a similar natural language format. These two\npieces of information are then concatenated with\nthe history of the conversation forming the full di-\nalogue context, see Figure 2. Following Wolf et al.\n(2019), we add new token embeddings for two par-\nties involved in the conversation to inform the at-\ntention layers what part of the context comes from\nthe user, and what part is related to the system. Fig-\nure 2 presents the ﬁnal architecture.\nTraining Details. We use the open-source im-\nplementation of the GPT architecture that provides\nboth GPT and GPT-2 ﬁne-tunable checkpoints. 2\nFollowing previous work ( Radford et al. , 2018;\nWolf et al. , 2019), we set the weight on the lan-\nguage model loss to be two times higher than the\none for the response prediction. The parameters\nfor the batch size ( 24), learning rate (1e-5) and the\nnumber of candidates per sequence ( 2) were cho-\nsen based on the grid search. 3\n5 Results and Analysis\nFollowing prior work ( Budzianowski et al. , 2018b;\nZhao et al. , 2019; Chen et al. , 2019), our evalu-\nation task is the dialogue-context-to-text genera-\ntion task (see Figure 1). Given a dialogue history,\nthe oracle belief state and the database state, the\nmodel needs to output the adequate response. By\nrelying on the oracle belief state, prior work has\nbypassed the possible errors originating from nat-\nural language understanding ( Budzianowski et al. ,\n2018b).\nThe main evaluation is based on the comparison\nbetween the following two models: 1) the base-\nline is a neural response generation model with an\noracle belief state obtained from the wizard anno-\ntations as in ( Budzianowski et al. , 2018a); 2) the\nmodel proposed in § 4 and shown in Figure 2 that\nworks entirely with text-only format as input (see\n§4). We test all three available pretrained GPT\nmodels - the original GPT model ( Radford et al. ,\n2018). and two GPT-2 models referred to as small\n(GPT2) and medium (GPT2-M) ( Radford et al. ,\n2019).\n2https://github.com/huggingface/\ntransfer-learning-conv-ai\n3We searched over the following values: learning rates 2\n{1-e4, 1-e5, 5-e6, 1-e6}, batch sizes 2 f8; 12; 16; 20; 24g and\ncandidate set sizes 2 f1; 2; 4; 6g.\n19\nBaseline GPT GPT2-S GPT2-M\nInform (%) 76.7 71.53 66.43 70.96\nSuccess (%) 64.63 55.36 55.16 61.36\nBLEU (%) 18.05 17.80 18.02 19.05\nTable 1: Evaluation on MultiWOZ with the greedy\nsampling procedure.\nBaseline GPT GPT2-S GPT2-M\nInform (%) 72.57 70.43 69.3 73.96\nSuccess (%) 57.63 51.0 54.93 61.20\nBLEU (%) 15.75 15.65 15.64 16.55\nTable 2: Evaluation on MultiWOZ with the nucleus\nsampling procedure.\n5.1 Evaluation with Automatic Measures\nWe report scores with three standard automatic\nevaluation measures. Two of them relate to the\ndialogue task completion: whether the system has\nprovided an appropriate entity ( Inform) and then\nanswered all requested attributes ( Success rate).\nFinally, ﬂuency is measured by the BLEU score\n(Papineni et al. , 2002).\nFirst, three versions of GPT were ﬁne-tuned on\nMultiWOZ and evaluated with greedy sampling.\nThe results are summarized in Table 1). They\nshow that the baseline obtains the highest score on\ntask-related metrics while the highest BLUE score\nwas achieved by GPT2-M. Although the results\nare lower for the GPT-based methods, we note the\ndesign simplicity of the GPT-based task-oriented\ndialogue models. Further, the gap in performance\nmight be partially attributed to the chosen greedy\nsampling procedure which puts too much focus\non the properties of the original pretraining phase\n(Holtzman et al. , 2019).\nTherefore, we also report the results with the\nnucleus sampling method in Table 2. The scores\nconﬁrm the importance of choosing the correct\nsampling method. The GPT2 models improve\nthe score on Inform and Success metrics. It is\nworth noting the consistent drop in BLUE scores\nacross all models. This comes from the fact that\nnucleus sampling allows for increased variability:\nthis might reduce the probability of generating\ndomain-speciﬁc tokens.\nWe have also qualitatively analyzed a sample\nof successful dialogues. Only around 50% of di-\nalogues are successful both with the baseline and\nwith the GPT-based models. Moreover, there are\nno clearly observed distinct patterns between suc-\ncessful dialogues for the two model types. This\nModel 1 vs Model 2\nGPT 59 % 41% Baseline\nGPT 46 % 54 % Target\nGPT2 46 % 54 % Target\nGPT2 45 % 55 % Baseline\nBaseline 43 % 57 % Target\nGPT2 51 % 49 % GPT\nTable 3: Human ranking of responses between all pairs\nof four analyzed models and the original responses.\nsuggests that they might be effectively ensembled\nusing a ranking model to evaluate the score of each\nresponse (Henderson et al. , 2019b). We will inves-\ntigate the complementarity of the two approaches\nalong with ensemble methods in future work.\n5.2 Human Evaluation\nIn another, now user-centered experiment, the goal\nwas to analyze the generation quality. Turkers, na-\ntive speakers of English, were asked to rate their\nbinary preference when presented with one-turn\nresponses from the baseline, GPT, GPT2-M and\nthe original dialogues ( Target). The turkers were\nrequired to choose what response they prefer when\npresented with two responses from two different\nmodels, resulting in more than 300 scores per each\nmodel pair.\nThe results are summarized in Table 3, while\nsome example dialogues with responses are pro-\nvided in Figure 3. As expected, the original re-\nsponses are ranked higher than all neural models\nwith the largest difference observed between the\noracle and the baseline model. Although the gen-\nerated output from the GPT is strongly preferred\nagainst the neural baseline, interestingly the oppo-\nsite is observed with the GPT2 model. These in-\nconclusive results call for further analyses in fu-\nture work, and also show that there are no sub-\nstantial differences in the quality of generated re-\nsponses when comparing the strong neural base-\nline and the GPT-based models.\n6 Conclusion\nIn this paper, we have made a ﬁrst step towards\nleveraging large pretrained generative models for\nmodeling task-oriented dialogue in multiple do-\nmains. The simplicity of the ﬁne-tuning proce-\ndure where all necessary information can be en-\ncoded as simple text enables a quick adaptation\nto constrained domains and domain-speciﬁc vo-\n20\ncabularies. We hope that this framework will in-\nform and guide future research in hope of simulta-\nneously improving and simplifying the design of\ntask-oriented conversational systems.\nReferences\nYoshua Bengio, Réjean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3(Feb):1137–1155.\nPaweł Budzianowski, Iñigo Casanueva, Bo-Hsiang\nTseng, and Milica Gaši ´c. 2018a. Towards end-\nto-end multi-domain dialogue modelling . Tech.\nRep. CUED/F-INFENG/TR.706, University of Cam-\nbridge, Engineering Department.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gasic. 2018b. MultiWOZ-A\nLarge-scale multi-domain wizard-of-oz dataset for\ntask-oriented dialogue modelling . In Proceedings of\nEMNLP, pages 5016–5026.\nWenhu Chen, Jianshu Chen, Pengda Qin, Xifeng\nYan, and William Yang Wang. 2019. Semantically\nconditioned dialog response generation via hierar-\nchical disentangled self-attention. arXiv preprint\narXiv:1905.12866.\nLayla El Asri, Hannes Schulz, Shikhar Sharma,\nJeremie Zumer, Justin Harris, Emery Fine, Rahul\nMehrotra, and Kaheer Suleman. 2017. Frames: A\ncorpus for adding memory to goal-oriented dialogue\nsystems. In Proceedings of SIGDIAL, pages 207–\n219.\nSergey Golovanov, Rauf Kurbanov, Sergey Nikolenko,\nKyryl Truskovskyi, Alexander Tselousov, and\nThomas Wolf. 2019. Large-scale transfer learning\nfor natural language generation . In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 6053–6058, Florence,\nItaly. Association for Computational Linguistics.\nMatthew Henderson, Rami Al-Rfou, Brian Strope, Yun-\nhsuan Sung, Laszlo Lukacs, Ruiqi Guo, Sanjiv Ku-\nmar, Balint Miklos, and Ray Kurzweil. 2017. Efﬁ-\ncient natural language response suggestion for smart\nreply. arXiv preprint arXiv:1705.00652.\nMatthew Henderson, Paweł Budzianowski, Iñigo\nCasanueva, Sam Coope, Daniela Gerz, Girish Ku-\nmar, Nikola Mrkši ´c, Georgios Spithourakis, Pei-\nHao Su, Ivan Vuli ´c, and Tsung-Hsien Wen. 2019a.\nA repository of conversational datasets . In Pro-\nceedings of the Workshop on NLP for Conversa-\ntional AI. Data available at github.com/PolyAI-\nLDN/conversational-datasets.\nMatthew Henderson, Ivan Vuli ´c, Daniela Gerz, Iñigo\nCasanueva, Paweł Budzianowski, Sam Coope,\nGeorgios Spithourakis, Tsung-Hsien Wen, Nikola\nMrksi’c, and Pei-Hao Su. 2019b. Training neural re-\nsponse selection for task-oriented dialogue systems .\nIn Proceedings of ACL.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degener-\nation. arXiv preprint arXiv:1904.09751.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation . In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339.\nAnjuli Kannan, Karol Kurach, Sujith Ravi, Tobias\nKaufmann, Andrew Tomkins, Balint Miklos, Greg\nCorrado, Laszlo Lukacs, Marina Ganea, Peter\nYoung, et al. 2016. Smart reply: Automated re-\nsponse suggestion for email . In Proceedings of the\n22nd ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining, pages 955–\n964. ACM.\nIlya Kulikov, Alexander H Miller, Kyunghyun Cho,\nand Jason Weston. 2018. Importance of a search\nstrategy in neural dialogue modelling . arXiv\npreprint arXiv:1811.00907.\nWenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun\nRen, Xiangnan He, and Dawei Yin. 2018. Sequicity:\nSimplifying task-oriented dialogue systems with sin-\ngle sequence-to-sequence architectures . In Proceed-\nings of ACL, pages 1437–1447.\nJiwei Li, Will Monroe, Tianlin Shi, Sébastien Jean,\nAlan Ritter, and Dan Jurafsky. 2017. Adversarial\nlearning for neural dialogue generation . In Proceed-\nings of the 2017 Conference on Empirical Methods\nin Natural Language Processing, pages 2157–2169.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. BLEU: A method for automatic\nevaluation of machine translation . In Proceedings\nof ACL, pages 311–318.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training .\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners . OpenAI\nBlog, 1(8).\nMarek Rei. 2017. Semi-supervised multitask learn-\ning for sequence labeling . In Proceedings of the\n55th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n2121–2130.\nAlan Ritter, Colin Cherry, and Bill Dolan. 2010. Un-\nsupervised modeling of twitter conversations . In\nHuman Language Technologies: The 2010 Annual\nConference of the North American Chapter of the As-\nsociation for Computational Linguistics, pages 172–\n180.\n21\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units . In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), volume 1, pages\n1715–1725.\nIulian Vlad Serban, Alessandro Sordoni, Ryan Lowe,\nLaurent Charlin, Joelle Pineau, Aaron Courville, and\nYoshua Bengio. 2017. A hierarchical latent variable\nencoder-decoder model for generating dialogues . In\nProceedings of AAAI, pages 3295–3301.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks .\nIn Proceedings of NeurIPS, pages 3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need . In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nTsung-Hsien Wen, Milica Gaši ´c, Nikola Mrkši ´c, Pei-\nHao Su, David Vandyke, and Steve Young. 2015. Se-\nmantically conditioned lstm-based natural language\ngeneration for spoken dialogue systems . In Proceed-\nings of the 2015 Conference on Empirical Methods\nin Natural Language Processing (EMNLP).\nTsung-Hsien Wen, David Vandyke, Nikola Mrkši ´c,\nMilica Gaši´c, Lina M Rojas-Barahona, Pei-Hao Su,\nStefan Ultes, and Steve Young. 2017. A network-\nbased end-to-end trainable task-oriented dialogue\nsystem. In Proceedings of EACL, pages 438–449.\nThomas Wolf, Victor Sanh, Julien Chaumond, and\nClement Delangue. 2019. Transfertransfo: A\ntransfer learning approach for neural network\nbased conversational agents . arXiv preprint\narXiv:1901.08149.\nSteve Young, Milica Gaši ´c, Blaise Thomson, and Ja-\nson D. Williams. 2013. POMDP-based statistical\nspoken dialog systems: A review . Proceedings of\nthe IEEE, 101(5):1160–1179.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2204–\n2213.\nTiancheng Zhao, Kaige Xie, and Maxine Eskenazi.\n2019. Rethinking action spaces for reinforcement\nlearning in end-to-end dialog agents with latent vari-\nable models . In Proceedings of the 2019 Confer-\nence of the North American Chapter of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, Volume 1 (Long and Short Pa-\npers), pages 1208–1218.\nTiancheng Zhao, Ran Zhao, and Maxine Eskenazi.\n2017. Learning discourse-level diversity for neural\ndialog models using conditional variational autoen-\ncoders. In Proceedings of ACL, pages 654–664.\n22\nFigure 3: The comparison of generated responses from the baseline model and GPT2-M."
}