{
  "title": "Improving across-dataset brain tissue segmentation for MRI imaging using transformer",
  "url": "https://openalex.org/W4309544075",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A3118824668",
      "name": "Vishwanatha M. Rao",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2053642436",
      "name": "Zihan Wan",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2223277419",
      "name": "Soroush Arabshahi",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4202170014",
      "name": "David J. Ma",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A4307878040",
      "name": "Pin-Yu Lee",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2096525004",
      "name": "Ye Tian",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2502127542",
      "name": "Xuzhe Zhang",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2111415633",
      "name": "Andrew F Laine",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2102843182",
      "name": "Jia Guo",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A3118824668",
      "name": "Vishwanatha M. Rao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2053642436",
      "name": "Zihan Wan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2223277419",
      "name": "Soroush Arabshahi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202170014",
      "name": "David J. Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4307878040",
      "name": "Pin-Yu Lee",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096525004",
      "name": "Ye Tian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2502127542",
      "name": "Xuzhe Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111415633",
      "name": "Andrew F Laine",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102843182",
      "name": "Jia Guo",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2621028221",
    "https://openalex.org/W6605779219",
    "https://openalex.org/W1966743212",
    "https://openalex.org/W2114459002",
    "https://openalex.org/W2147079711",
    "https://openalex.org/W1905429970",
    "https://openalex.org/W6600074859",
    "https://openalex.org/W2332001143",
    "https://openalex.org/W2793957891",
    "https://openalex.org/W2507603780",
    "https://openalex.org/W1998070036",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W2464708700",
    "https://openalex.org/W3092442943",
    "https://openalex.org/W2625430291",
    "https://openalex.org/W3010138595",
    "https://openalex.org/W2903554604",
    "https://openalex.org/W3177469079",
    "https://openalex.org/W2150606601",
    "https://openalex.org/W2127324977",
    "https://openalex.org/W1901260206",
    "https://openalex.org/W4212875960",
    "https://openalex.org/W1970488531",
    "https://openalex.org/W6743731764",
    "https://openalex.org/W4235770099",
    "https://openalex.org/W2944984718",
    "https://openalex.org/W6640001594",
    "https://openalex.org/W2898340770",
    "https://openalex.org/W140904458",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2888255030",
    "https://openalex.org/W2153366355",
    "https://openalex.org/W2031224239",
    "https://openalex.org/W3046999841",
    "https://openalex.org/W2042875269",
    "https://openalex.org/W2135365788",
    "https://openalex.org/W2884542984",
    "https://openalex.org/W2332066482",
    "https://openalex.org/W3123995491",
    "https://openalex.org/W1990288178",
    "https://openalex.org/W4221151536",
    "https://openalex.org/W2037403969",
    "https://openalex.org/W6639824700",
    "https://openalex.org/W2606521544",
    "https://openalex.org/W2157270343",
    "https://openalex.org/W2157848968",
    "https://openalex.org/W3202642596",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6791764970",
    "https://openalex.org/W2883617100",
    "https://openalex.org/W2036864541",
    "https://openalex.org/W6749954789",
    "https://openalex.org/W3038521198",
    "https://openalex.org/W2082526668",
    "https://openalex.org/W2048771405",
    "https://openalex.org/W2774320778",
    "https://openalex.org/W1865761",
    "https://openalex.org/W2612690371",
    "https://openalex.org/W2186222003",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W2052644075",
    "https://openalex.org/W2058161128",
    "https://openalex.org/W3104258355",
    "https://openalex.org/W4309544075"
  ],
  "abstract": "Brain tissue segmentation has demonstrated great utility in quantifying MRI data by serving as a precursor to further post-processing analysis. However, manual segmentation is highly labor-intensive, and automated approaches, including convolutional neural networks (CNNs), have struggled to generalize well due to properties inherent to MRI acquisition, leaving a great need for an effective segmentation tool. This study introduces a novel CNN-Transformer hybrid architecture designed to improve brain tissue segmentation by taking advantage of the increased performance and generality conferred by Transformers for 3D medical image segmentation tasks. We first demonstrate the superior performance of our model on various T1w MRI datasets. Then, we rigorously validate our model's generality applied across four multi-site T1w MRI datasets, covering different vendors, field strengths, scan parameters, and neuropsychiatric conditions. Finally, we highlight the reliability of our model on test-retest scans taken in different time points. In all situations, our model achieved the greatest generality and reliability compared to the benchmarks. As such, our method is inherently robust and can serve as a valuable tool for brain related T1w MRI studies. The code for the TABS network is available at: https://github.com/raovish6/TABS .",
  "full_text": "TYPE Original Research\nPUBLISHED /two.tnum/one.tnum November /two.tnum/zero.tnum/two.tnum/two.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nOPEN ACCESS\nEDITED BY\nSairam Geethanath,\nIcahn School of Medicine at Mount\nSinai, United States\nREVIEWED BY\nHamid Osman,\nTaif University, Saudi Arabia\nEvan Fletcher,\nUniversity of California, Davis,\nUnited States\n*CORRESPONDENCE\nJia Guo\njg/three.tnum/four.tnum/zero.tnum/zero.tnum@columbia.edu\nSPECIALTY SECTION\nThis article was submitted to\nNeuroimaging Analysis and Protocols,\na section of the journal\nFrontiers in Neuroimaging\nRECEIVED /one.tnum/nine.tnum August /two.tnum/zero.tnum/two.tnum/two.tnum\nACCEPTED /two.tnum/four.tnum October /two.tnum/zero.tnum/two.tnum/two.tnum\nPUBLISHED /two.tnum/one.tnum November /two.tnum/zero.tnum/two.tnum/two.tnum\nCITATION\nRao VM, Wan Z, Arabshahi S, Ma DJ,\nLee P-Y, Tian Y, Zhang X, Laine AF and\nGuo J (/two.tnum/zero.tnum/two.tnum/two.tnum) Improving across-dataset\nbrain tissue segmentation for MRI\nimaging using transformer.\nFront. Neuroimaging/one.tnum:/one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/two.tnum Rao, Wan, Arabshahi, Ma, Lee,\nTian, Zhang, Laine and Guo. This is an\nopen-access article distributed under\nthe terms of the\nCreative Commons\nAttribution License (CC BY) . The use,\ndistribution or reproduction in other\nforums is permitted, provided the\noriginal author(s) and the copyright\nowner(s) are credited and that the\noriginal publication in this journal is\ncited, in accordance with accepted\nacademic practice. No use, distribution\nor reproduction is permitted which\ndoes not comply with these terms.\nImproving across-dataset brain\ntissue segmentation for MRI\nimaging using transformer\nVishwanatha M. Rao /one.tnum, Zihan Wan /two.tnum, Soroush Arabshahi /one.tnum,\nDavid J. Ma /one.tnum, Pin-Yu Lee /one.tnum, Ye Tian /one.tnum, Xuzhe Zhang /one.tnum,\nAndrew F. Laine /one.tnumand Jia Guo /three.tnum,/four.tnum*\n/one.tnumDepartment of Biomedical Engineering, Columbia University, New York, NY, United States,\n/two.tnumDepartment of Applied Mathematics, Columbia University, New York, NY, United States,\n/three.tnumDepartment of Psychiatry, Columbia University, New York, NY, Uni ted States, /four.tnumMortimer B.\nZuckerman Mind Brain Behavior Institute, Columbia University , New York, NY, United States\nBrain tissue segmentation has demonstrated great utility i n quantifying MRI\ndata by serving as a precursor to further post-processing analys is. However,\nmanual segmentation is highly labor-intensive, and automate d approaches,\nincluding convolutional neural networks (CNNs), have struggl ed to generalize\nwell due to properties inherent to MRI acquisition, leaving a g reat need for an\neﬀective segmentation tool. This study introduces a novel CNN -Transformer\nhybrid architecture designed to improve brain tissue segmenta tion by\ntaking advantage of the increased performance and generality con ferred by\nTransformers for /three.tnumD medical image segmentation tasks. We ﬁrstdemonstrate\nthe superior performance of our model on various T/one.tnumw MRI datasets. Then, we\nrigorously validate our model’s generality applied across fo ur multi-site T/one.tnumw\nMRI datasets, covering diﬀerent vendors, ﬁeld strengths, scan parameters, and\nneuropsychiatric conditions. Finally, we highlight the reliab ility of our model\non test-retest scans taken in diﬀerent time points. In all situa tions, our model\nachieved the greatest generality and reliability compared to t he benchmarks. As\nsuch, our method is inherently robust and can serve as a valuable tool for brain\nrelated T/one.tnumw MRI studies. The code for the TABS network is available at: https://\ngithub.com/raovish/six.tnum/TABS.\nKEYWORDS\nMRI, transformer, deep learning, segmentation, investigatio n, brain tissue\nsegmentation\nIntroduction\nBrain tissue segmentation represents an important application of medical image\nprocessing, in which an MRI image of the brain is segmented into three classes :\ngray matter (GM), white matter (WM), and cerebrospinal ﬂuid (CSF). Bra in tissue\nsegmentation is a critical step in Voxel Based Morphometry (VBM), a me thod used\nto quantitatively analyze MRI scans. VBM presents the ability to highlight subtle\nstructural abnormalities by estimating diﬀerences in GM and WM brain tissue volum e.\nAs such, VBM has been prevalent for characterizing and monitoring conditions such\nas schizophrenia (\nWright et al., 1995 ), Alzheimer’s ( Hirata et al., 2005 ), Huntingon’s\nFrontiers in Neuroimaging /zero.tnum/one.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\n(Kassubek et al., 2004 ), and bipolar disorder ( Nugent et al.,\n2006). VBM has also been used as an integral preprocessing\ntool in machine learning and deep learning based disease\nclassiﬁcation pipelines (\nSalvador et al., 2017 ; Nemoto et al.,\n2021). Outside of VBM, brain tissue segmentation is useful for\ncharacterizing tissue volume in particular regions of interest.\nIt is often used with magnetic resonance spectroscopy to\nquantify metabolites by tissue type, and both techniques have\nbeen applied together to investigate morphological diﬀerences\nassociated with various disorders (\nAuer et al., 2001 ; Bagory et al.,\n2011) as well as correct for metabolite measurements based on\ndiﬀering tissue fractions ( Harris et al., 2015 ).\nDespite the demonstrated utility of brain tissue\nsegmentation, there is no universally accepted method\ncapable of segmenting accurately and eﬃciently across a\nwide variety of datasets. Manual segmentation of brain tissue\nis extremely labor intensive, often impractical given larger\ndatasets, and diﬃcult even for experts. Alternatively, automated\nsegmentation has proven challenging due to properties inherent\nto the MRI scans themselves. Changes in vendors or ﬁeld\nstrength have both been linked with increased variance in\nrepeated scan measures (\nHan et al., 2006 ), and scans acquired\nthrough diﬀerent imaging protocols tend to ﬂuctuate more in\nterms of volumetric brain measures (\nKruggel et al., 2010 ). Time\nof day as well as time between scans have been associated with\nvariable tissue volume estimation (\nKarch et al., 2019 ) while\nneuropsychiatric conditions such as schizophrenia have been\nlinked with subtle brain tissue anatomical changes (\nKoutsouleris\net al., 2015 ). Together, these inconsistencies make it diﬃcult\nfor brain tissue segmentation solutions to be applicable across\ndatasets of diﬀering vendors, collection parameters, time points,\nand neuropsychiatric condition.\nMany of the earlier proposed automated solutions have\ndepended on intensity thresholding (\nDora et al., 2017 ),\npopulation-based atlases ( Cabezas et al., 2011 ), clustering\n(Mahmood et al., 2015 ; Dora et al., 2017 ), statistical methods\n(Zhang and Brady, 2000 ; Marroquín et al., 2002 ; Greenspan\net al., 2006 ; Angelini et al., 2007 ), and standard machine learning\nalgorithms. Thresholding-based approaches often struggle to\nsegment low contrast input images with overlapping brain\ntissue intensity histograms. Alternatively, atlas-based algorithm\nperformance heavily depends on the quality of the population\nderived brain atlas. While machine learning algorithms such\nas support vector machine (SVM;\nBauer and Nolte, 2011 ),\nrandom forest ( Dadar and Collins, 2021 ), and neural networks\n(Amiri et al., 2013 ) have demonstrated reasonable segmentation\nperformance, their accuracy largely relies on the quality\nof manually extracted features. In general, many of these\nalgorithms require a priori information to properly segment\nbrain tissue, which is often not feasible to acquire for all\nnew scans segmented. FSL FAST is a popular statistical\nbrain tissue segmentation toolkit that combines Gaussian\nmixture models with hidden Markov random ﬁelds to achieve\nreliable segmentation performance across a variety of datasets\n(\nZhang and Brady, 2000 ). However, segmentation via FAST is\ntime consuming and therefore not ideal for many real-time\nsegmentation applications.\nConvolutional neural networks (CNNs) have recently\nemerged as a superior alternative to standard machine learning\nalgorithms for classiﬁcation-based brain segmentation given\ntheir feature-encoding capabilities (\nAkkus et al., 2017 ). CNNs\nhave been found to outperform machine learning algorithms\nsuch as random forest and SVM speciﬁcally for brain tissue\nsegmentation (\nZhang et al., 2015 ). Following their introduction,\nmany other CNN-based networks have been proposed for\nbrain tissue segmentation (\nMoeskops et al., 2016 ; Khagi and\nKwon, 2018 ) as well as brain tumor segmentation ( Beers et al.,\n2017; Mlynarski et al., 2019 ; Feng et al., 2020a ), including\nboth 2D and 3D approaches. Unet represents one popular\nsegmentation algorithm (\nRonneberger and Fischer, 2015 ; Çiçek\net al., 2016), which consists of symmetric encoding and decoding\nconvolutional operations that allows for the preservation of\nthe initial image resolution following segmentation. Variants\nof Unet have been successfully applied to brain tissue\nsegmentation achieving state of the art performance. For\nexample, one study achieved a DICE score of 0.988 using\n3D Unet, which even outperformed human experts (\nKolarík\net al., 2018 ). More recently, 2D patch-based Unet and Unet-\ninspired implementations have gained traction ( Lee et al., 2020 ;\nYamanakkanavar and Lee, 2020 ) to better preserve and account\nfor local details; such models have outperformed their non-\npatch-based variants.\nDespite the impressive performance CNNs have\ndemonstrated for brain tissue segmentation, they often\nstruggle to generalize well when presented with new datasets.\nMany prior brain tissue segmentation approaches only report\ntest performance on the same dataset upon which the model\nwas trained. While such metrics validate the generality of the\nproposed model on MRI scans from the same dataset, they\nfail to quantify model performance across diﬀerent datasets\nwhere changes in acquisition parameters can impact MRI\nimage features and thus decrease the model’s generality. Given\nthe importance of brain tissue segmentation in VBM and\npre-processing, it is not practical to retrain a CNN model every\ntime a scan is obtained diﬀerently. As such, model generality\nis especially imperative to developing a widely applicable\nautomated brain tissue segmentation solution.\nTransformers are an alternative to CNNs that have\nrecently demonstrated state-of-the-art results in natural\nimage segmentation. Emerging evidence suggests that\nTransformers coupled with CNNs may improve performance\nand generalization for medical image segmentation tasks\nincluding brain tissue segmentation (\nChen et al., 2021 ;\nHatamizadeh et al., 2021 ; Sun et al., 2021 ; Wang et al., 2021 ).\nIn this study, we sought to improve the traditional Unet\narchitecture using Transformers to not only achieve higher\nFrontiers in Neuroimaging /zero.tnum/two.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nFIGURE /one.tnum\nOverview of experimental pipeline. (A) Model performance test, where each model was trained and tested on individual datasets. (B) Model\ngenerality test, where models pre-trained on /three.tnumT DLBS/SALD datasets were tested on one another and on the /one.tnum./five.tnumT IXI dataset.(C) Model\nreliability test, where the best generalizing model to the COB RE dataset was compared to FAST based on similarity in segmentat ion outputs for\nrepeated scans.\nFIGURE /two.tnum\nData demographic and pre-processing visualization. (A–C) Age distribution by gender for train/validation/test groups of DLB S, SALD, and IXI,\nrespectively. (D) MRI pre-processing pipeline consisting of /one.tnum. Bias ﬁeld correction /two.tnum. Brain extraction /three.tnum. Aﬃne correction.\nbrain tissue segmentation performance, but also generalize\nbetter across diﬀerent datasets while remaining reliable. Here,\nwe propose Transformer-based Automated Brain Tissue\nSegmentation (TABS), a new 3D CNN-Transformer hybrid\ndeep learning architecture for brain tissue segmentation. In\ndoing so, we elucidate the beneﬁts of embedding a Transformer\nmodule within a CNN encoder-decoder architecture speciﬁcally\nfor brain tissue segmentation. Furthermore, after achieving\nimproved within dataset performance, we are the ﬁrst to\nrigorously demonstrate model generality and reliability across\nmultiple vendors, ﬁeld strengths, scan parameters, time points,\nand neuropsychiatric condition.\nMaterials and methods\nStudy design\nWe conducted three experiments to evaluate model\nperformance, generality, and reliability for brain tissue\nFrontiers in Neuroimaging /zero.tnum/three.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nTABLE /one.tnum MRI acquisition and demographic parameters for DLBS, SALD, IXI, and COBRE datasets.\nScan parameters DLBS SALD IXI COBRE\nScanner Philips achieva SIEMENS TrioTim Phillips Intera SIEMENS TrioTim\nField strength 3T 3T 1.5T 3T\nSequence MPRAGE MPRAGE MPRAGE MPRAGE\nVoxel size (mm) 1.0 × 1.0 × 1.0 1.0 × 1.0 × 1.0 1.0 × 1.0 × 1.0 1.0 × 1.0 × 1.0\nTR/TE (msec) 8.10/3.70 1,900/2.52 9.81/4.60 2,530/1.64\nFA (degrees) 12 90 8 7\nNumber of scans (Train/Validation/Test) 129/43/43 170/56/ 57 137/45/46 0/0/358 (179 pairs)\nFemale % 61.9 64.3 56.1 24.0\nAge Range (years) 20–89 21–80 21–86 18–66\nAge mean ± SD 56.4 ± 18.2 50.6 ± 13.5 51.1 ± 14.2 38.3 ± 12.6\nsegmentation. The experimental pipeline for these experiments\nis visualized in\nFigure 1. First, we trained and tested all of the\nmodels on three separate datasets (DLBS, SALD, and IXI) of\ndiﬀering acquisition parameters along with an aggregate total\ndataset containing all of the scans combined. We then evaluated\nmodel generality across ﬁeld strength and vendors; models\ntrained on 3T datasets were tested on the 1.5T dataset and\nmodels trained on 3T datasets from diﬀerent vendors were\ntested on one another. Finally, we extended our generalization\ntesting to an alternate dataset (COBRE) containing test-retest\nrepeated scans of both schizophrenia and healthy patients. We\napplied models pre-trained on the 3T SALD dataset to COBRE\nto give them the best chance of generalizing well, as SALD and\nCOBRE were collected using similar acquisition parameters.\nOnce conﬁrming that TABS generalized the best on this dataset,\nwe compared the reliability of TABS to that of the ground truth\nby evaluating the similarity of outputs on the test-retest repeated\nscans. Given that each pair of scans were acquired from the same\nsubject within a small time frame, we expected a more reliable\ntool to output very similar segmentation predictions across\nboth scans.\nWe compared TABS to three other benchmark CNN models\nin our experiments: vanilla Unet, Unet-SE, and ResUnet. We\nchose Unet given its prior state of the art performance in\n3D brain tissue segmentation (\nKolarík et al., 2018 ), and we\nalso compared to prior attempts at improving Unet including\nsqueeze-excitation (SE) blocks (\nHu et al., 2018 ) before each\ndownsampling operation (Unet-SE) and residual connections\n(ResUnet;\nZhang et al., 2018 ). Moreover, given that the model\narchitecture for TABS is identical to that of ResUnet except\nfor the Vision Transformer, comparing to ResUnet allowed us\nto highlight the speciﬁc beneﬁts conferred by the Transformer.\nAll of the tested models were the same depth and encoded\nthe same number of features. Finally, we also compared to\nFSL FAST, the tool used to generate the ground truths, in our\nreliability evaluation.\nStudy design\nWe collected MRI scans of healthy participants over a broad\nage range from three datasets for our ﬁrst two experiments:\nDLBS (\nRodrigue et al., 2012 ), SALD ( Wei et al., 2018 ), and\nIXI ( Biomedical Image Analysis Group et al., 2018 ). While they\nall use a MPRAGE sequence, the datasets vary in terms of\ntheir other acquisition parameters. Firstly, they diﬀer by ﬁeld\nstrength, where DLBS and SALD contain 3T scans and IXI\ncontains 1.5T scans. Moreover, all three datasets were acquired\nusing diﬀerent scanners, with the SALD dataset acquired\nusing a Siemens manufactured scanner as opposed to Phillips.\nLastly, the datasets diﬀer in terms of scan parameters such\nas repetition/echo time and ﬂip angle. We split each dataset\ninto 3:1:1 train/validation/test groups while maintaining a broad\nage distribution across each subsection. The age distributions\nacross these splits for each of these datasets are shown in\nFigures 2A–C. We also collected paired test-retest scans taken at\ndiﬀerent time points of healthy participants and schizophrenia\npatients from the COBRE dataset (\nBustillo et al., 2017 ) for our\nthird experiment. The demographic information and acquisition\nparameters for all four datasets are outlined in\nTable 1.\nWe followed the initial pre-processing protocol outlined by\nFeng et al. (2020b) for all of the datasets, which includes bias ﬁeld\ncorrection ( Sled et al., 1998 ), brain extraction using FreeSurfer\n(Ségonne et al., 2004 ), and aﬃne registration to the 1 mm 3\nisotropic MNI152 brain template with trilinear interpolation\nusing FSL FLIRT (\nJenkinson et al., 2002 ). After these steps, the\nDLBS/SALD/IXI MRI images were 182 × 218 × 182, and the\nCOBRE images were 193 × 229 × 193. We padded and cropped\nthe images to reach an input dimension of 192 × 192 × 192,\nusing a maximum intensity projection across all scans for each\ndataset to ensure that we did not remove important anatomical\ncomponents. Finally, we normalized the intensities for each scan\nto values between − 1 and 1. The pre-processing pipeline is\nvisualized in\nFigure 2D.\nFrontiers in Neuroimaging /zero.tnum/four.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nTABLE /two.tnum Performance results with each model trained and tested on individual datasets.\nProject Metrics TABS Unet Unet-SE ResUnet\nGray\nmatter\nWhite\nmatter\nCSF Gray\nmatter\nWhite\nmatter\nCSF Gray\nmatter\nWhite\nmatter\nCSF Gray\nmatter\nWhite\nmatter\nCSF\nDLBS DICE ↑ 0.932 ± 0.024 0.954 ± 0.013 0.964 ± 0.010 0.924 ± 0.027 0.947 ± 0.014 0.959 ± 0.009 0.925 ± 0.026 0.951 ± 0.014 0.956 ± 0.009 0.929 ± 0.026 0.951 ± 0.013 0.963 ± 0.008\nJaccard index ↑ 0.874 ± 0.041 0.913 ± 0.023 0.930 ± 0.018 0.859 ± 0.046 0.900 ± 0.025 0.922 ± 0.016 0.861 ± 0.045 0.907 ± 0.025 0.917 ± 0.017 0.868 ± 0.044 0.907 ± 0.023 0.928 ± 0.015\nPearson ↑ 0.965 ± 0.009 0.980 ± 0.006 0.984 ± 0.002 0.957 ± 0.012 0.980 ± 0.004 0.978 ± 0.002 0.961 ± 0.010 0.978 ± 0.007 0.979 ± 0.003 0.963 ± 0.010 0.979 ± 0.006 0.982 ± 0.002\nSpearman ↑ 0.930 ± 0.014 0.868 ± 0.016 0.844 ± 0.013 0.921 ± 0.017 0.869 ± 0.016 0.825 ± 0.015 0.922 ± 0.013 0.868 ± 0.015 0.839 ± 0.012 0.928 ± 0.014 0.866 ± 0.014 0.838 ± 0.013\nHD ↓ 9.179 ± 1.625 12.071 ± 2.142 10.795 ± 1.842 7.454 ± 1.184 14.278 ± 2.508 10.900 ± 1.841 7.788 ± 1.260 14.937 ± 2.444 11.763 ± 1.743 9.378 ± 1.547 12.107 ± 2.138 10.894 ± 2.077\nMSE ↓ 0.012 ± 0.002 0.011 ± 0.003 0.007 ± 0.001 0.018 ± 0.003 0.027 ± 0.005 0.011 ± 0.001 0.018 ± 0.002 0.015 ± 0.004 0.016 ± 0.001 0.013 ± 0.002 0.013 ± 0.004 0.009 ± 0.001\nSALD DICE ↑ 0.944 ± 0.017 0.959 ± 0.015 0.955 ± 0.014 0.939 ± 0.018 0.955 ± 0.015 0.950 ± 0.016 0.939 ± 0.018 0.956 ± 0.014 0.950 ± 0.016 0.941 ± 0.016 0.955 ± 0.013 0.954 ± 0.014\nJaccard index ↑ 0.895 ± 0.030 0.922 ± 0.065 0.914 ± 0.026 0.886 ± 0.031 0.914 ± 0.026 0.906 ± 0.029 0.885 ± 0.031 0.915 ± 0.026 0.904 ± 0.028 0.888 ± 0.028 0.914 ± 0.024 0.912 ± 0.026\nPearson ↑ 0.969 ± 0.007 0.983 ± 0.006 0.982 ± 0.004 0.967 ± 0.007 0.980 ± 0.007 0.980 ± 0.005 0.964 ± 0.006 0.981 ± 0.007 0.978 ± 0.005 0.968 ± 0.007 0.980 ± 0.007 0.979 ± 0.005\nSpearman ↑ 0.938 ± 0.007 0.864 ± 0.009 0.837 ± 0.015 0.937 ± 0.007 0.863 ± 0.009 0.837 ± 0.015 0.924 ± 0.010 0.864 ± 0.010 0.835 ± 0.015 0.938 ± 0.007 0.862 ± 0.009 0.832 ± 0.016\nHD ↓ 7.489 ± 1.557 11.737 ± 1.834 11.255 ± 1.891 7.294 ± 1.553 13.171 ± 2.919 11.241 ± 1.866 7.386 ± 1.509 13.733 ± 2.685 11.712 ± 1.678 8.197 ± 1.568 11.012 ± 2.251 11.092 ± 1.894\nMSE ↓ 0.011 ± 0.002 0.008 ± 0.003 0.007 ± 0.001 0.012 ± 0.002 0.010 ± 0.003 0.008 ± 0.001 0.014 ± 0.002 0.009 ± 0.003 0.011 ± 0.001 0.012 ± 0.002 0.010 ± 0.003 0.008 ± 0.001\nIXI DICE ↑ 0.942 ± 0.020 0.958 ± 0.017 0.962 ± 0.010 0.939 ± 0.021 0.955 ± 0.018 0.960 ± 0.012 0.938 ± 0.019 0.958 ± 0.016 0.957 ± 0.012 0.943 ± 0.021 0.960 ± 0.017 0.962 ± 0.011\nJaccard index ↑ 0.891 ± 0.034 0.920 ± 0.030 0.927 ± 0.018 0.885 ± 0.035 0.914 ± 0.010 0.924 ± 0.021 0.885 ± 0.032 0.919 ± 0.029 0.918 ± 0.021 0.892 ± 0.035 0.923 ± 0.029 0.926 ± 0.020\nPearson ↑ 0.969 ± 0.012 0.982 ± 0.009 0.984 ± 0.003 0.964 ± 0.012 0.982 ± 0.007 0.981 ± 0.002 0.961 ± 0.010 0.981 ± 0.009 0.980 ± 0.003 0.970 ± 0.011 0.984 ± 0.008 0.985 ± 0.003\nSpearman ↑ 0.938 ± 0.012 0.848 ± 0.010 0.854 ± 0.014 0.937 ± 0.013 0.850 ± 0.009 0.845 ± 0.014 0.907 ± 0.015 0.848 ± 0.009 0.852 ± 0.014 0.937 ± 0.012 0.850 ± 0.009 0.857 ± 0.015\nHD ↓ 8.785 ± 1.910 12.584 ± 2.361 10.715 ± 1.744 6.277 ± 1.040 14.159 ± 3.390 10.537 ± 1.722 7.479 ± 1.724 16.770 ± 3.034 11.426 ± 1.724 7.626 ± 2.042 11.993 ± 2.611 10.194 ± 1.729\nMSE ↓ 0.011 ± 0.004 0.009 ± 0.004 0.007 ± 0.001 0.016 ± 0.003 0.020 ± 0.004 0.009 ± 0.001 0.016 ± 0.003 0.009 ± 0.004 0.013 ± 0.002 0.011 ± 0.003 0.008 ± 0.004 0.007 ± 0.001\nTotal DICE ↑ 0.945 ± 0.020 0.961 ± 0.014 0.963 ± 0.012 0.941 ± 0.022 0.959 ± 0.014 0.959 ± 0.015 0.941 ± 0.020 0.960 ± 0.013 0.959 ± 0.014 0.944 ± 0.019 0.960 ± 0.012 0.963 ± 0.013\nJaccard index ↑ 0.896 ± 0.035 0.925 ± 0.026 0.929 ± 0.022 0.889 ± 0.037 0.921 ± 0.026 0.922 ± 0.027 0.889 ± 0.035 0.923 ± 0.024 0.921 ± 0.026 0.895 ± 0.033 0.924 ± 0.022 0.929 ± 0.023\nPearson ↑ 0.972 ± 0.009 0.984 ± 0.006 0.987 ± 0.003 0.970 ± 0.009 0.983 ± 0.006 0.984 ± 0.004 0.966 ± 0.009 0.983 ± 0.006 0.982 ± 0.0 04 0.971 ± 0.007 0.984 ± 0.005 0.985 ± 0.004\nSpearman ↑ 0.938 ± 0.009 0.861 ± 0.014 0.849 ± 0.018 0.935 ± 0.011 0.861 ± 0.014 0.848 ± 0.019 0.922 ± 0.012 0.861 ± 0.014 0.846 ± 0.017 0.937 ± 0.010 0.862 ± 0.014 0.848 ± 0.018\nHD ↓ 7.480 ± 1.824 12.273 ± 2.123 10.882 ± 1.7733 7.006 ± 1.506 12.423 ± 2.662 10.739 ± 1.816 7.114 ± 1.505 13.725 ± 3.041 11.078 ± 1.771 7.307 ± 1.800 11.714 ± 2.395 10.337 ± 1.814\nMSE ↓ 0.010 ± 0.003 0.007 ± 0.003 0.005 ± 0.001 0.011 ± 0.002 0.008 ± 0.003 0.006 ± 0.001 0.014 ± 0.002 0.008 ± 0.003 0.010 ± 0.001 0.010 ± 0.002 0.007 ± 0.002 0.006 ± 0.001\nBold text indicates superior mean performance, with equally performing m odel metrics both bolded. The up arrow indicates that higher numbe rs correspond to better performance and down arrow indicates that lower num bers correspond to better\nperformance.\nFrontiers in Neuroimaging /zero.tnum/five.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nFIGURE /three.tnum\nModel architecture for TABS, including a /five.tnum-layer encoder/decoder with a Vision Transformer between the encoder and decoder.\nModel architecture and implementation\nThe architecture of our proposed model is shown in\nFigure 3. TABS is a ResUnet ( Zhang et al., 2018 ) inspired model\nthat consists of a 5-layered 3D CNN encoder and decoder.\nTABS takes an input dimension of 192 × 192 × 192, and\nthe ﬁve encoder layers downsample the original image to\nf x12 × 12 × 12, where f represents the number of encoded\nfeatures. For this speciﬁc implementation, we chose a f value\nof 128. We followed the same “linear projection and learned\npositional embedding” operations introduced in\nWang et al.\n(2021) to convert the encoded feature tensor into 512 tokenized\nvectors that are sequentially fed into the Transformer module\nin the order determined by the learned positional embeddings.\nOur Transformer encoder consists of 4 layers and 8 heads\nfollowing the implementation initially described by\nVaswani\net al. (2017). The output of the Transformer is 512 × 1,728,\nwhich we then reshape to 512 × 12 × 12 × 12 and reduce\nthe feature dimensionality to f via convolution. The decoder\nportion of the network reconstructs the image to the original\ninput dimension, and a ﬁnal convolution operation is applied to\ngenerate a 3-channel output with each channel corresponding to\nan individual tissue type. We used a Softmax activation function\nto ensure that the probabilities for each voxel across the three\nchannels add up to 1.\nTraining protocol\nAll four models were trained using the same parameters\ndescribed below. We trained for 350 epochs with early stopping\nbased on validation loss. We selected pre-trained models based\non the best validation performance. We used FAST to generate\nground truth probability maps for each brain tissue type and\nstacked and cropped them to generate a three-channel image\nmatching the output shape of our models (3 × 192 × 192 × 192).\nThe models were trained on three 24 GB NVIDIA Quadro 6000\ngraphical processing units using mean-squared-error (MSE)\nloss with a batch size of 3. We used group normalization as\nopposed to batch normalization due to group normalization’s\nincreased stability for smaller batch sizes (\nWu and He, 2018 ). We\ntrained using Adam ( Kingma and Ba, 2014 ) as the optimization\nalgorithm with a learning rate of 1E-5 and weight decay set\nto 1E-6.\nEvaluation metrics\nAll evaluation metrics were only taken for the portion of\nthe outputs containing the brain, meaning that the background\nvoxels outside of the segmentation ﬁeld were not considered.\nAdditionally, all metrics were calculated individually for each\nbrain tissue type. Segmentation similarity using continuous\nprobability estimates was quantiﬁed using Pearson correlation,\nSpearman correlation, and MSE. Segmentation maps for each\ntissue type were then generated from the probability estimations\nby taking the argmax along the channel axis. We generated\nbinary maps for each tissue type based on the numerical value\nassigned to each voxel of the argmax output. Segmentation\nsimilarity between these binary maps was quantiﬁed using DICE\nScore, Jaccard Index, and Haussdorf Distance (HD;\nBeauchemin\net al., 1998 ).\nFrontiers in Neuroimaging /zero.tnum/six.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nFIGURE /four.tnum\nBox plots visualizing model performance with Wilcoxon pairwise comparisons. Each * indicates order of signiﬁcance.\nFIGURE /five.tnum\nVisualization of model performance for DLBS, SALD, and IXI. Segme ntation maps for the ground truth, TABS, and the three benchmark m odels\nare shown from left to right following the T/one.tnumw scan. Zoom in regions are included below each image.\nFrontiers in Neuroimaging /zero.tnum/seven.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nTABLE /three.tnum Generalization results across vendor, ﬁeld strength, and scanning parameters.\nProject Metrics TABS Unet Unet-SE ResUnet\nGray\nmatter\nWhite\nmatter\nCSF Gray\nmatter\nWhite\nmatter\nCSF Gray\nmatter\nWhite\nmatter\nCSF Gray\nmatter\nWhite\nmatter\nCSF\nDLBS → IXI DICE ↑ 0.947 ± 0.021 0.964 ± 0.014 0.964 ± 0.010 0.943 ± 0.019 0.966 ± 0.011 0.953 ± 0.012 0.940 ± 0.020 0.964 ± 0.012 0.952 ± 0.013 0.938 ± 0.022 0.961 ± 0.011 0.955 ± 0.013\nJaccard index ↑ 0.899 ± 0.036 0.931 ± 0.044 0.930 ± 0018 0.892 ± 0.033 0.935 ± 0.020 0.911 ± 0.023 0.887 ± 0.034 0.931 ± 0.022 0.908 ± 0.024 0.884 ± 0.038 0.925 ± 0.020 0.914 ± 0.023\nPearson ↑ 0.953 ± 0.018 0.978 ± 0.009 0.974 ± 0.007 0.937 ± 0.016 0.968 ± 0.007 0.958 ± 0.009 0.938 ± 0.020 0.978 ± 0.007 0.967 ± 0.008 0.937 ± 0.021 0.974 ± 0.008 0.965 ± 0.008\nSpearman ↑ 0.923 ± 0.015 0.849 ± 0.010 0.825 ± 0.020 0.916 ± 0.011 0.849 ± 0.011 0.751 ± 0.033 0.901 ± 0.019 0.849 ± 0.010 0.817 ± 0.020 0.913 ± 0016 0.849 ± 0.010 0.780 ± 0.026\nHD ↓ 8.416 ± 2.096 11.911 ± 2.384 11.437 ± 1.775 7.049 ± 1.455 13.218 ± 2.866 12.717 ± 1.786 7.484 ± 1.534 15.387 ± 2.957 13.099 ± 1.724 8.278 ± 1.866 11.334 ± 2.035 12.452 ± 1.721\nMSE ↓ 0.016 ± 0.005 0.011 ± 0.004 0.011 ± 0.003 0.024 ± 0.004 0.025 ± 0.003 0.017 ± 0.003 0.025 ± 0.005 0.014 ± 0.003 0.019 ± 0.003 0.022 ± 0.006 0.014 ± 0.003 0.015 ± 0.003\nSALD → IXI DICE ↑ 0.953 ± 0.019 0.970 ± 0.013 0.966 ± 0.010 0.950 ± 0.018 0.964 ± 0.015 0.968 ± 0.008 0.950 ± 0.018 0.967 ± 0.014 0.964 ± 0.008 0.949 ± 0.017 0.964 ± 0.013 0.967 ± 0.008\nJaccard index ↑ 0.910 ± 0.033 0.941 ± 0.033 0.935 ± 0.019 0.905 ± 0.031 0.932 ± 0.026 0.937 ± 0.015 0.905 ± 0.031 0.937 ± 0.025 0.930 ± 0.015 0.903 ± 0.029 0.931 ± 0.023 0.935 ± 0.015\nPearson ↑ 0.958 ± 0.015 0.982 ± 0.007 0.978 ± 0.006 0.964 ± 0.010 0.982 ± 0.006 0.981 ± 0.004 0.957 ± 0.013 0.983 ± 0.007 0.978 ± 0.004 0.962 ± 0.011 0.982 ± 0.006 0.978 ± 0.005\nSpearman ↑ 0.926 ± 0.014 0.851 ± 0.010 0.846 ± 0.012 0.931 ± 0.011 0.850 ± 0.010 0.851 ± 0.013 0.903 ± 0.019 0.851 ± 0.010 0.849 ± 0.012 0.931 ± 0.012 0.850 ± 0.010 0.843 ± 0.012\nHD ↓ 9.152 ± 1.958 11.607 ± 2.205 11.074 ± 1.685 6.850 ± 1.536 17.200 ± 3.451 10.596 ± 1.726 6.540 ± 1.307 17.429 ± 3.215 11.341 ± 1.744 9.591 ± 1.667 11.549 ± 2.151 10.952 ± 1.650\nMSE ↓ 0.014 ± 0.004 0.008 ± 0.003 0.009 ± 0.003 0.013 ± 0.003 0.009 ± 0.003 0.008 ± 0.002 0.016 ± 0.003 0.008 ± 0.003 0.012 ± 0.003 0.013 ± 0.003 0.009 ± 0.003 0.009 ± 0.003\nDLBS → SALD DICE ↑ 0.931 ± 0.019 0.947 ± 0.015 0.944 ± 0.020 0.942 ± 0.014 0.958 ± 0.012 0.947 ± 0.012 0.937 ± 0.014 0.955 ± 0.011 0.944 ± 0.012 0.936 ± 0.015 0.956 ± 0.010 0.945 ± 0.014\nJaccard index ↑ 0.871 ± 0.032 0.900 ± 0.067 0.894 ± 0.035 0.891 ± 0.024 0.920 ± 0.022 0.900 ± 0.022 0.882 ± 0.024 0.915 ± 0.020 0.894 ± 0.022 0.880 ± 0.026 0.915 ± 0.018 0.897 ± 0.025\nPearson ↑ 0.960 ± 0.012 0.976 ± 0.009 0.977 ± 0.007 0.951 ± 0.013 0.974 ± 0.006 0.959 ± 0.010 0.952 ± 0.013 0.978 ± 0.006 0.968 ± 0.008 0.949 ± 0.016 0.975 ± 0.007 0.966 ± 0.009\nSpearman ↑ 0.933 ± 0.010 0.858 ± 0.008 0.828 ± 0.017 0.926 ± 0.010 0.861 ± 0.010 0.754 ± 0.036 0.920 ± 0.014 0.861 ± 0.009 0.813 ± 0.018 0.924 ± 0.012 0.860 ± 0.009 0.784 ± 0.027\nHD ↓ 8.028 ± 1.603 12.200 ± 2.139 11.307 ± 1.897 7.817 ± 1.620 11.421 ± 2.367 13.463 ± 2.068 8.208 ± 1.516 13.226 ± 2.234 13.909 ± 1.939 8.631 ± 1.664 11.348 ± 1.946 13.158 ± 1.998\nMSE ↓ 0.015 ± 0.004 0.024 ± 0.004 0.009 ± 0.002 0.021 ± 0.003 0.026 ± 0.003 0.015 ± 0.003 0.022 ± 0.003 0.014 ± 0.003 0.018 ± 0.002 0.019 ± 0.005 0.014 ± 0.003 0.013 ± 0.003\nSALD → DLBS DICE ↑ 0.927 ± 0.040 0.953 ± 0.026 0.957 ± 0.013 0.905 ± 0.053 0.931 ± 0.038 0.950 ± 0.015 0.912 ± 0.047 0.937 ± 0.033 0.951 ± 0.016 0.921 ± 0.013 0.946 ± 0.026 0.957 ± 0.013\nJaccard index ↑ 0.866 ± 0.064 0.912 ± 0.045 0.919 ± 0.023 0.830 ± 0.081 0.874 ± 0.064 0.905 ± 0.027 0.841 ± 0.073 0.883 ± 0.056 0.907 ± 0.028 0.855 ± 0.064 0.899 ± 0.045 0.917 ± 0.023\nPearson ↑ 0.952 ± 0.023 0.975 ± 0.015 0.976 ± 0.005 0.950 ± 0.037 0.967 ± 0.024 0.979 ± 0.004 0.951 ± 0.029 0.969 ± 0.022 0.976 ± 0.005 0.952 ± 0.023 0.973 ± 0.016 0.976 ± 0.004\nSpearman ↑ 0.919 ± 0.020 0.861 ± 0.013 0.843 ± 0.014 0.920 ± 0.030 0.854 ± 0.014 0.848 ± 0.016 0.911 ± 0.021 0.856 ± 0.013 0.847 ± 0.005 0.919 ± 0.021 0.859 ± 0.014 0.840 ± 0.013\nHD ↓ 10.330 ± 1.288 12.812 ± 1.892 11.174 ± 1.825 8.026 ± 1.293 15.906 ± 2.058 10.349 ± 1.920 7.781 ± 1.351 15.881 ± 2.167 10.498 ± 1.351 10.279 ± 1.232 12.142 ± 2.158 10.895 ± 1.671\nMSE ↓ 0.017 ± 0.007 0.011 ± 0.007 0.010 ± 0.003 0.017 ± 0.010 0.015 ± 0.011 0.009 ± 0.002 0.018 ± 0.007 0.014 ± 0.010 0.013 ± 0.002 0.017 ± 0.007 0.013 ± 0.008 0.010 ± 0.002\nModels pre-trained on DLBS/SALD were applied to SALD/DLBS and IXI. Bold text indicates greater mean performance, with equally performing mode l metrics both. The up arrow indicates that higher numbers correspon d to better performance and\ndown arrow indicates that lower numbers correspond to better performan ce.\nFrontiers in Neuroimaging /zero.tnum/eight.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nFIGURE /six.tnum\nBox plots visualizing model generality with Wilcoxon pairwise c omparisons. Each * indicates order of signiﬁcance. (A) Model generality across\nﬁeld strengths. (B) Model generality across vendor.\nModel performance across each metric was compared using\npaired non-parametric Wilcoxon tests. Speciﬁcally, for each\ntissue type, TABS’s performance was compared pairwise with\neach of the benchmark models tested against. We used an α\nvalue of 0.05. Signiﬁcant diﬀerences are shown in box plots, with\neach the number of ∗ indicating order of signiﬁcance (ns, not\nsigniﬁcant, ∗p < 0.05, ∗∗p < 0.01, ∗∗∗ p < 0.001, ∗∗∗∗ p < 0.0001).\nResults\nModel generality\nThe performance results for each model trained and\ntested on DLBS, IXI, SALD, and Total datasets individually\nare reported in\nTable 2 and visualized in Figure 4. TABS\noutperformed ResUnet, Unet-SE, and Unet on all the datasets\nfor most metrics except for the 1.5T IXI dataset, where TABS\noutperformed Unet-SE and Unet while only performing slightly\nworse than ResUnet. TABS consistently achieves higher of\nDICE/Jaccard metrics across all tissue types along with higher\ncorrelation and lower MSE on most tissue types. In general,\nall models performed better on WM and CSF as opposed to\nGM.\nFigure 5 plots representative segmentation outputs for\nperformance testing for each of the datasets.\nModel generality—DLBS, IXI, and SALD\nThe generality results for all models trained on DLBS/SALD\nand applied to IXI as well as trained on DLBS/SALD and\napplied to SALD/DLBS are shown in\nTable 3 and visualized\nin Figure 6. TABS generalized better across datasets on most\nmetrics for the DLBS → IXI and SALD → DLBS tests, with\nhigher DICE/Jaccard and correlation metrics for at least two\ntissue types. Additionally, for the SALD → IXI generalization\ntest, TABS reached higher DICE/Jaccard metrics for both GM\nand WM. We observed that models trained on SALD performed\nbetter when applied to IXI than models trained on IXI itself.\nTABS also exhibited a similar increase in performance when pre-\ntrained on DLBS and applied to IXI compared to TABS trained\non IXI. Representative segmentation outputs for all models for\neach test scenario is shown in\nFigure 7.\nModel generality—COBRE\nWe extended our generalization testing to the COBRE\ndataset, consisting of healthy and schizophrenia test-retest\nrepeated scans. The generalization performance for all models is\nreported in\nTable 4 and visualized in Figure 8. TABS generalized\nbetter for GM and WM across the control, schizophrenia,\nand aggregate total dataset compared to the benchmark\nmodels for most metrics. Moreover, TABS also achieved higher\nDICE/Jaccard metrics for CSF for schizophrenia patients.\nCOBRE test-retest\nTABS showcased better reliability compared to FAST, the\ntool used to generate the ground truths. Similarity metrics\nFrontiers in Neuroimaging /zero.tnum/nine.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nFIGURE /seven.tnum\nVisualization of model generality across vendors, ﬁeld strength, and scanning parameters. Segmentation maps for the ground truth, TABS, and\nthe three benchmark models are shown from left to right following the T /one.tnumw scan. Zoom in regions are included below each image.between test-retest repeated images for both TABS and FAST\nare shown in\nTable 5 for the control, schizophrenia, and total\naggregate datasets and visualized in Figure 8. TABS proved\nconsistently more reliable across almost all metrics for GM\nand CSF. Moreover, TABS reached a higher Pearson correlation\nand lower MSE over all tissue types, and only performed\nslightly worse than FAST on WM DICE/Jaccard. Representative\nsegmentation outputs for paired repeated scans from both\ncontrol and schizophrenia datasets are visualized in\nFigure 9.\nDiscussion\nIn this study, we present TABS, a new Transformer-\nCNN hybrid deep learning architecture designed for brain\ntissue segmentation. TABS showcased superior performance\ncompared to prior state-of-the-art CNN implementations\nwhile also generalizing exceptionally well across datasets and\nremaining reliable between paired test-retest scans. These\ntraits are critical to developing a useful and more widely\napplicable brain tissue segmentation toolkit. Through TABS,\nwe also demonstrate the methodological utility using a Vision\nTransformer to improve the Unet architecture for brain\ntissue segmentation.\nOur experimental protocol was designed to elucidate\nthe real-world applicability of TABS compared to various\nbenchmark models. The datasets included in this study\nwere chosen with the goal of emulating the extreme\ndiﬀerences in MRI input a brain tissue segmentation\nalgorithm would receive in real-world applications;\nthe DLBS, SALD, and IXI datasets varied in terms of\nmanufacturer, ﬁeld strengths, and scanner parameters.\nFrontiers in Neuroimaging /one.tnum/zero.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nTABLE /four.tnum Generalization results for each model pre-trained on SALD and applied to COBRE.\nTest Metrics TABS Unet Unet-SE ResUnet\nGray\nmatter\nWhite\nmatter\nCSF Gray\nmatter\nWhite\nmatter\nCSF Gray\nmatter\nWhite\nmatter\nCSF Gray\nmatter\nWhite\nmatter\nCSF\nControl DICE ↑ 0.872 ± 0.032 0.910 ± 0.024 0.902 ± 0.040 0.858 ± 0.016 0.894 ± 0.010 0.900 ± 0.025 0.846 ± 0.015 0.870 ± 0.012 0.909 ± 0.024 0.832 ± 0.016 0.853 ± 0.017 0.904 ± 0.024\nJaccard index ↑ 0.774 ± 0.050 0.835 ± 0.042 0.824 ± 0.065 0.752 ± 0.025 0.809 ± 0.016 0.818 ± 0.041 0.733 ± 0.040 0.771 ± 0.018 0.833 ± 0.040 0.712 ± 0.023 0.744 ± 0.026 0.826 ± 0.040\nPearson ↑ 0.923 ± 0.025 0.952 ± 0.014 0.975 ± 0.006 0.918 ± 0.013 0.945 ± 0.007 0.975 ± 0.004 0.904 ± 0.016 0.923 ± 0.010 0.974 ± 0.005 0.882 ± 0.020 0.914 ± 0.014 0.973 ± 0.005\nSpearman ↑ 0.902 ± 0.019 0.898 ± 0.010 0.754 ± 0.019 0.901 ± 0.011 0.901 ± 0.003 0.757 ± 0.020 0.888 ± 0.014 0.886 ± 0.007 0.759 ± 0.020 0.869 ± 0.018 0.855 ± 0.015 0.757 ± 0.020\nHD ↓ 9.025 ± 1.559 9.672 ± 1.274 12.994 ± 2.524 8.488 ± 1.342 9.739 ± 1.209 13.619 ± 2.236 8.954 ± 1.430 10.400 ± 1.113 13.293 ± 2.378 11.023 ± 1.251 10.890 ± 1.087 12.797 ± 2.601\nMSE ↓ 0.026 ± 0.008 0.025 ± 0.008 0.007 ± 0.002 0.028 ± 0.004 0.031 ± 0.004 0.008 ± 0.009 0.032 ± 0.005 0.040 ± 0.005 0.011 ± 0.001 0.042 ± 0.007 0.049 ± 0.008 0.009 ± 0.001\nSchiz DICE ↑ 0.879 ± 0.033 0.919 ± 0.027 0.912 ± 0.040 0.850 ± 0.031 0.890 ± 0.012 0.903 ± 0.031 0.835 ± 0.018 0.865 ± 0.015 0.909 ± 0.029 0.820 ± 0.019 0.845 ± 0.019 0.907 ± 0.030\nJaccard index ↑ 0.786 ± 0.053 0.851 ± 0.045 0.841 ± 0.065 0.739 ± 0.027 0.802 ± 0.019 0.824 ± 0.051 0.718 ± 0.026 0.762 ± 0.023 0.835 ± 0.048 0.695 ± 0.027 0.731 ± 0.028 0.831 ± 0.050\nPearson ↑ 0.929 ± 0.026 0.956 ± 0.016 0.974 ± 0.007 0.914 ± 0.016 0.942 ± 0.008 0.975 ± 0.005 0.896 ± 0.020 0.918 ± 0.013 0.974 ± 0.005 0.873 ± 0.024 0.907 ± 0.016 0.973 ± 0.005\nSpearman ↑ 0.904 ± 0.019 0.902 ± 0.011 0.756 ± 0.022 0.896 ± 0.012 0.900 ± 0.004 0.763 ± 0.025 0.882 ± 0.017 0.883 ± 0.009 0.765 ± 0.026 0.861 ± 0.021 0.849 ± 0.016 0.763 ± 0.026\nHD ↓ 8.809 ± 1.718 9.715 ± 1.386 13.013 ± 2.686 8.654 ± 1.520 10.074 ± 1.290 13.384 ± 2.497 9.051 ± 1.701 10.597 ± 1.202 13.079 ± 2.477 11.204 ± 1.285 11.334 ± 1.070 12.425 ± 2.807\nMSE ↓ 0.024 ± 0.008 0.023 ± 0.008 0.008 ± 0.002 0.030 ± 0.005 0.033 ± 0.005 0.008 ± 0.001 0.035 ± 0.006 0.043 ± 0.007 0.011 ± 0.001 0.046 ± 0.008 0.054 ± 0.008 0.009 ± 0.001\nTotal DICE ↑ 0.875 ± 0.040 0.914 ± 0.026 0.907 ± 0.040 0.854 ± 0.018 0.892 ± 0.011 0.901 ± 0.028 0.841 ± 0.017 0.868 ± 0.013 0.909 ± 0.027 0.826 ± 0.018 0.849 ± 0.019 0.906 ± 0.027\nJaccard index ↑ 0.780 ± 0.052 0.843 ± 0.044 0.832 ± 0.066 0.746 ± 0.027 0.806 ± 0.018 0.821 ± 0.046 0.725 ± 0.026 0.766 ± 0.021 0.834 ± 0.045 0.703 ± 0.027 0.738 ± 0.028 0.829 ± 0.045\nPearson ↑ 0.926 ± 0.025 0.954 ± 0.015 0.974 ± 0.007 0.916 ± 0.015 0.943 ± 0.008 0.975 ± 0.004 0.900 ± 0.019 0.920 ± 0.012 0.974 ± 0.005 0.878 ± 0.023 0.910 ± 0.015 0.973 ± 0.005\nSpearman ↑ 0.903 ± 0.019 0.900 ± 0.011 0.755 ± 0.020 0.899 ± 0.023 0.901 ± 0.003 0.760 ± 0.023 0.885 ± 0.016 0.885 ± 0.008 0.762 ± 0.023 0.865 ± 0.020 0.852 ± 0.016 0.760 ± 0.023\nHD ↓ 8.917 ± 1.642 9.694 ± 1.330 13.003 ± 2.603 8.571 ± 1.435 9.907 ± 1.260 13.501 ± 2.370 9.003 ± 1.571 10.499 ± 1.162 13.185 ± 2.427 11.114 ± 1.270 11.114 ± 1.100 12.610 ± 2.709\nMSE ↓ 0.025 ± 0.008 0.024 ± 0.008 0.008 ± 0.002 0.029 ± 0.005 0.032 ± 0.005 0.008 ± 0.001 0.033 ± 0.006 0.042 ± 0.006 0.011 ± 0.001 0.044 ± 0.008 0.051 ± 0.009 0.009 ± 0.001\nBold text indicates superior model performance, with equally performing m odel metrics both bolded. The up arrow indicates that higher numbe rs correspond to better performance and down arrow indicates that lower num bers correspond to better\nperformance.\nFrontiers in Neuroimaging /one.tnum/one.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nFIGURE /eight.tnum\nBox plots visualizing model generality and test-retest reliab ility on the COBRE dataset with Wilcoxon pairwise comparisons. E ach * indicates\norder of signiﬁcance.\nMoreover, our test-retest dataset consisted of repeated\nscans from schizophrenia and healthy patients taken at\ndiﬀerent time points, presenting an even more challenging\nsegmentation task. Due to these factors, we believe our\nevaluation methodology accurately captures the versatility\nof TABS.\nWe ﬁrst found that TABS was the best performing model\nwhen trained and tested on the same dataset. While TABS\nachieved signiﬁcantly higher performance than both Unet\nand Unet-SE, we observed marginal performance beneﬁts\nover ResUnet. We hypothesize that the residual connections\nare responsible for the bulk of the performance gain over\nthe traditional Unet models, with the Transformer module\nproviding a small but consistent performance increase\nwithin datasets.\nThroughout our generality testing, TABS performed the\nbest on most datasets compared to the benchmark Unet\nmodels. The most signiﬁcant generalization diﬀerences we\nobserved were between TABS and ResUnet. Given that their\nmodel architectures are identical except for the Transformer,\nwe believe that the addition of the Transformer signiﬁcantly\nimproves model generality. CNNs are not well-suited to\ncapture long-range dependencies in the input image due\nto the local receptive ﬁelds of convolutional kernels. We\nbelieve that this property could make Transformer-based\nnetworks agnostic to dataset-speciﬁc variations and thus\nmore generalizable. The addition of the Transformer\nallows TABS to preserve and even improve the within\ndataset performance conferred by residual connections\nwhile also generalizing better than the vanilla Unet, where\nResUnet struggled.\nWe also noticed that all of the models tested improved\nin performance when trained on SALD and applied to\nIXI as opposed to training on IXI itself. This disparity\ncould be due to the diﬀerence in ﬁeld strength: the higher\nquality 3T MRI images from SALD may provide more\nglobally relevant features than the 1.5T MRI images from\nIXI. However, for TABS speciﬁcally, we observed this\nsame eﬀect when pre-trained on 3T DLBS scans. These\nresults indicate that TABS can potentially take better\nFrontiers in Neuroimaging /one.tnum/two.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nTABLE /five.tnum Test-retest reliability results across time-pointsand neuropsychiatric condition for TABS compared to FAST (ground t ruth) for control,\nschizophrenia, and aggregate total datasets from COBRE.\nTest Metrics TABS FAST\nGray matter White matter CSF Gray matter White matter CSF\nControl DICE ↑ 0.959 ± 0.015 0.977 ± 0.008 0.954 ± 0.012 0.951 ± 0.015 0.977 ± 0.006 0.948 ± 0.012\nJaccard index ↑ 0.922 ± 0.026 0.954 ± 0.016 0.912 ± 0.022 0.908 ± 0.026 0.955 ± 0.012 0.901 ± 0.021\nPearson ↑ 0.981 ± 0.010 0.994 ± 0.004 0.983 ± 0.007 0.968 ± 0.011 0.988 ± 0.005 0.867 ± 0.019\nSpearman ↑ 0.980 ± 0.009 0.982 ± 0.009 0.973 ± 0.017 0.967 ± 0.011 0.985 ± 0.005 0.886 ± 0.018\nHD ↓ 7.445 ± 1.513 8.066 ± 1.759 9.126 ± 1.942 7.489 ± 1.640 7.968 ± 1.507 10.624 ± 2.308\nMSE ↓ 0.005 ± 0.003 0.002 ± 0.001 0.003 ± 0.002 0.011 ± 0.004 0.005 ± 0.002 0.030 ± 0.005\nSchiz DICE ↑ 0.949 ± 0.020 0.972 ± 0.012 0.947 ± 0.016 0.941 ± 0.022 0.974 ± 0.009 0.942 ± 0.015\nJaccard index ↑ 0.904 ± 0.036 0.946 ± 0.021 0.899 ± 0.028 0.890 ± 0.038 0.949 ± 0.017 0.891 ± 0.026\nPearson ↑ 0.974 ± 0.016 0.992 ± 0.007 0.978 ± 0.010 0.961 ± 0.018 0.985 ± 0.008 0.856 ± 0.024\nSpearman ↑ 0.973 ± 0.014 0.978 ± 0.013 0.962 ± 0.028 0.959 ± 0.017 0.982 ± 0.008 0.875 ± 0.023\nHD ↓ 7.779 ± 1.453 8.078 ± 1.674 10.068 ± 2.551 7.552 ± 1.531 7.990 ± 1.542 10.424 ± 2.445\nMSE ↓ 0.007 ± 0.005 0.003 ± 0.002 0.004 ± 0.002 0.013 ± 0.006 0.006 ± 0.003 0.033 ± 0.007\nTotal DICE ↑ 0.954 ± 0.018 0.974 ± 0.010 0.950 ± 0.015 0.946 ± 0.019 0.975 ± 0.008 0.945 ± 0.014\nJaccard index ↑ 0.913 ± 0.033 0.950 ± 0.019 0.906 ± 0.026 0.899 ± 0.034 0.952 ± 0.015 0.896 ± 0.024\nPearson ↑ 0.978 ± 0.014 0.993 ± 0.006 0.981 ± 0.009 0.964 ± 0.016 0.986 ± 0.007 0.861 ± 0.022\nSpearman ↑ 0.976 ± 0.013 0.980 ± 0.011 0.967 ± 0.024 0.963 ± 0.015 0.983 ± 0.007 0.880 ± 0.022\nHD ↓ 7.613 ± 1.488 8.072 ± 1.712 9.600 ± 2.311 7.506 ± 1.582 7.979 ± 1.521 10.524 ± 2.374\nMSE ↓ 0.006 ± 0.004 0.002 ± 0.002 0.004 ± 0.002 0.012 ± 0.005 0.006 ± 0.003 0.032 ± 0.007\nBold text indicates superior model performance, with equally performing m odel metrics both bolded. The up arrow indicates that higher numbe rs correspond to better performance and\ndown arrow indicates that lower numbers correspond to better performan ce.\nFIGURE /nine.tnum\nVisualization of test-retest reliability results across tim e-points and neuropsychiatric condition. Segmentation maps for TA BS and FAST following\nthe T/one.tnumw scan are shown for each pair of repeated scans for control and schizophrenia groups. Zoom in regions are included below each image.\nadvantage of higher quality training data compared to the\nbenchmark models.\nFurthermore, we found that TABS generalized the best\non an alternate COBRE dataset consisting of both healthy\nFrontiers in Neuroimaging /one.tnum/three.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nand schizophrenia scans. Schizophrenia patients often reﬂect\nsubtle anatomical diﬀerences compared to healthy subjects,\nsuch as alterations in GM volume (\nKoutsouleris et al., 2015 ).\nThese changes make generalizing to the schizophrenia dataset\nan especially diﬃcult task. Additionally, the mean age of the\nCOBRE dataset was slightly lower than the datasets TABS was\noriginally trained on, making generalizing to COBRE potentially\neven more challenging. TABS generalized the best compared\nto the benchmark models on the overall COBRE dataset,\nwith even more pronounced diﬀerences for the schizophrenia\nportion. Therefore, we believe that TABS may excel in more\ndiﬃcult segmentation cases where standard Unet models\nyield errors.\nFinally, our test-retest experiment highlights the reliability\nof TABS, the best generalizing model on the COBRE dataset,\ncompared with the FAST, the algorithm used to generate the\nground truths. The test-retest repeated scans used in this\nstudy were taken from the same patient within a short time\nframe, meaning that we expected minimal diﬀerences in the\nsegmentation output. Through this test, we ﬁnd that TABS not\nonly generalizes well on the COBRE dataset, but also maintains\nthis performance more reliably than FAST.\nIn general, while traditional approaches such as FAST\nhave demonstrated compelling brain tissue segmentation\nperformance, there are several advantages to deep learning-\nbased alternatives such as TABS. First and foremost, the\nproduction times for segmented scans using FAST are\nsigniﬁcantly higher than that of TABS. For example, in\nour testing on the same machine, TABS could generate the\nsegmentations for an aggregate set of 146 T1w MRI scans 57x\nfaster, with an average time of 6.2 s per scan. In contrast, FAST\nrequired 353.7 s per scan. Deep learning algorithms also provide\nmore capacities for customization, as loaded models can be ﬁne-\ntuned and altered for particular tasks as well as directly built into\npost-processing pipelines.\nDespite the demonstrated advantages of TABS, there\nare certain limitations in our work that can be addressed\nin subsequent studies. 3D CNN models often require a\nlarge amount of computational power to eﬃciently train.\nWhile we were able to use full resolution MRI inputs for\nour model, we were limited to a batch size of 3 due to\nmemory constraints. Using a larger batch size may have\nresulted in better performance. Additionally, even though\nwe trained TABS on three large datasets, our model could\nbe further improved by increasing our sample size. An\nincrease in sample size could account for variations in MRI\nimage characteristics not captured in the four datasets we\ninvestigated. In fact,\nFletcher et al. (2021) required 8,000 train\nimages from 11 cohorts to develop and validate a suﬃciently\ngeneralized skull stripping model. Lastly, recent ﬁndings\nsuggest that patch-based 2D CNN approaches perform better\nthan non-patch-based variants for brain tissue segmentation\n(\nLee et al., 2020 ; Yamanakkanavar and Lee, 2020 ). As\nsuch, we believe that we could extend TABS to a patch\nbased 3D model in future studies to better capture local\ninformation that may be lost by processing the entire image\nat once.\nCode availability statement\nThe code used in this project is proprietary. The code for\nthe TABS model is available at https://github.com/raovish6/\nTABS, and the entire TABS package is available upon request\nof the corresponding author. The code for TABS is © 2021 The\nTrustees of Columbia University in the City of New York. This\nwork may be reproduced and distributed for academic non-\ncommercial purposes only. The data used in this study can be\nobtained from the following sources:\nRodrigue et al. (2012),\nBiomedical Image Analysis Group et al. (2018), Wei et al. (2018),\nand Bustillo et al. (2017).\nData availability statement\nThe original contributions presented in the study are\nincluded in the article/supplementary material, further inquiries\ncan be directed to the corresponding author/s.\nAuthor contributions\nVR: conceptualization, methodology, software, formal\nanalysis, investigation, data curation, writing—original draft,\nreview and editing, and visualization. ZW, SA, and DM:\nsoftware, formal analysis, visualization, and writing—review\nand editing. P-YL: formal analysis, visualization, and writing—\nreview and editing. YT: writing—original draft and review\nand editing. XZ: software, formal analysis, investigation, and\nwriting—review and editing. AL: writing—review and editing.\nJG: supervision, project administration, conceptualization, data\ncuration, visualization, and writing—original draft and review\nand editing. All authors contributed to the article and approved\nthe submitted version.\nAcknowledgments\nThe content of this manuscript has previously appeared\nonline as a preprint ( Rao et al., 2022 ).\nConﬂict of interest\nThe authors declare that the research was conducted in the\nabsence of any commercial or ﬁnancial relationships that could\nbe construed as a potential conﬂict of interest.\nFrontiers in Neuroimaging /one.tnum/four.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed\nor endorsed by the publisher.\nReferences\nAkkus, Z., Galimzianova, A., Hoogi, A., Rubin, D. L., and Eric kson, B. J. (2017).\nDeep learning for brain MRI segmentation: state of the art and future directions. J.\nDigit. Imaging30, 449–459. doi: 10.1007/s10278-017-9983-4\nAmiri, S., Movahedi, M. M., Kazemi, K., and Parsaei, H. (2013 ). An automated\nMR image segmentation system using multi-layer perceptron neur al network. J.\nBiomed. Phys. Eng.3:115.\nAngelini, E. D., Song, T., Mensh, B. D., and Laine, A. F. (2007) . Brain MRI\nsegmentation with multiphase minimal partitioning: a comparat ive study. Int. J.\nBiomed. Imaging2007:10526. doi: 10.1155/2007/10526\nAuer, D. P., Wilke, M., Grabner, A., Heidenreich, J. O., Bronis ch, T., and\nWetter, T. C. (2001). Reduced NAA in the thalamus and altered me mbrane\nand glial metabolism in schizophrenic patients detected by 1H-M RS and tissue\nsegmentation. Schizophr. Res.52, 87–99. doi: 10.1016/S0920-9964(01)00155-4\nBagory, M., Durand-Dubief, F., Ibarrola, D., Comte, J. C., Co tton, F., Confavreux,\nC., et al. (2011). Implementation of an absolute brain 1H-MRS quantiﬁcation\nmethod to assess diﬀerent tissue alterations in multiple scleros is. IEEE Trans.\nBiomed. Eng.59, 2687–2694. doi: 10.1109/TBME.2011.2161609\nBauer, S., Nolte, L. P. and Reyes, M. (2011). “Fully automatic se gmentation of\nbrain tumor images using support vector machine classiﬁcation in combination\nwith hierarchical conditional random ﬁeld regularization, ” in International\nConference on Medical Image Computing and Computer-Assisted Intervention\n(Berlin; Heidelberg: Springer), 354–361. doi: 10.1007/978-3 -642-23626-6_44\nBeauchemin, M., Thomson, K. P., and Edwards, G. (1998). On the Hausdorﬀ\ndistance used for the evaluation of segmentation results. Can. J. Remote Sensing24,\n3–8. doi: 10.1080/07038992.1998.10874685\nBeers, A., Chang, K., Brown, J., Sartor, E., Mammen, C. P., Ger stner, E., et al.\n(2017). Sequential 3d u-nets for biologically-informed brain t umor segmentation.\narXiv preprint arXiv:1709.02967. doi: 10.1117/12.2293941\nBiomedical Image Analysis Group, Imperial College London, and Cen tre for\nthe Developing Brain King’s College London (2018). Information eXtraction From\nImages. Available online at:\nhttps://brain-development.org/ixi-dataset/ (accessed\nDecember 15, 2021).\nBustillo, J. R., Jones, T., Chen, H., Lemke, N., Abbott, C., Qua lls, C., et al. (2017).\nGlutamatergic and neuronal dysfunction in gray and white matt er: a spectroscopic\nimaging study in a large schizophrenia sample. Schizophr. Bull. 43, 611–619.\ndoi: 10.1093/schbul/sbw122\nCabezas, M., Oliver, A., Lladó, X., Freixenet, J., and Cuadra, M. B. (2011). A\nreview of atlas-based segmentation for magnetic resonance b rain images. Comput.\nMethods Prog. Biomed.104, e158–e177. doi: 10.1016/j.cmpb.2011.07.015\nChen, J., Lu, Y., Yu, Q., Luo, X., Adeli, E., Wang, Y., et al. (202 1). Transunet:\nTransformers make strong encoders for medical image segmen tation. arXiv\n[Preprint]. arXiv:2102.04306. doi: 10.48550/arXiv.2102.04306\nÇiçek, Ö., Abdulkadir, A., Lienkamp, S. S., Brox, T. and Ronneb erger, O. (2016).\n“3D U-Net: learning dense volumetric segmentation from sparse annotation, ”\nin International Conference on Medical Image Computing and Computer-Assisted\nIntervention (Cham: Springer), 424–432. doi: 10.1007/978-3-319-46723- 8_49\nDadar, M., and Collins, D. L. (2021). BISON: brain tissue segme ntation pipeline\nusing T1-weighted magnetic resonance images and a random fo rest classiﬁer.\nMagn. Reson. Med.85, 1881–1894. doi: 10.1002/mrm.28547\nDora, L., Agrawal, S., Panda, R., and Abraham, A. (2017). Stat e-of-the-art\nmethods for brain tissue segmentation: a review. IEEE Rev. Biomed. Eng. 10,\n235–249. doi: 10.1109/RBME.2017.2715350\nFeng, X., Lipton, Z. C., Yang, J., Small, S. A., Provenzano, F. A. , Alzheimer’s\nDisease Neuroimaging Initiative and Frontotemporal Lobar D egeneration\nNeuroimaging Initiative (2020a). Estimating brain age bas ed on a uniform\nhealthy population with deep learning and structural magnetic re sonance imaging.\nNeurobiol. Aging91, 15–25. doi: 10.1016/j.neurobiolaging.2020.02.009\nFeng, X., Tustison, N. J., Patel, S. H., and Meyer, C. H. (2020b ). Brain tumor\nsegmentation using an ensemble of 3d u-nets and overall surviv al prediction\nusing radiomic features. Front. Comput. Neurosci.14:25. doi: 10.3389/fncom.2020.\n00025\nFletcher, E., Decarli, C., Fan, A. P., and Knaack, A. (2021). Co nvolutional\nneural net learning can achieve production-level brain segmen tation\nin structural magnetic resonance imaging. Front. Neurosci . 15:683426.\ndoi: 10.3389/fnins.2021.683426\nGreenspan, H., Ruf, A., and Goldberger, J. (2006). Constrained G aussian mixture\nmodel framework for automatic segmentation of MR brain imag es. IEEE Trans.\nMed. Imaging25, 1233–1245. doi: 10.1109/TMI.2006.880668\nHan, X., Jovicich, J., Salat, D., van der Kouwe, A., Quinn, B., Czanner,\nS., et al. (2006). Reliability of MRI-derived measurements of h uman cerebral\ncortical thickness: the eﬀects of ﬁeld strength, scanner upgr ade and manufacturer.\nNeuroimage 32, 180–194. doi: 10.1016/j.neuroimage.2006.02.051\nHarris, A. D., Puts, N. A., and Edden, R. A. (2015). Tissue cor rection for GABA-\nedited MRS: considerations of voxel composition, tissue seg mentation, and tissue\nrelaxations. J. Magn. Reson. Imaging42, 1431–1440. doi: 10.1002/jmri.24903\nHatamizadeh, A., Tang, Y., Nath, V., Yang, D., Myronenko, A. , Landman, B., et al.\n(2021). Unetr: transformers for 3d medical image segmentat ion. arXiv [Preprint].\narXiv:2103.10504. doi: 10.1109/WACV51458.2022.00181\nHirata, Y., Matsuda, H., Nemoto, K., Ohnishi, T., Hirao, K., Yamashita, F., et al.\n(2005). Voxel-based morphometry to discriminate early Alzheim er’s disease from\ncontrols. Neurosci. Lett.382, 269–274. doi: 10.1016/j.neulet.2005.03.038\nHu, J., Shen, L., and Sun, G. (2018). “Squeeze-and-excitation networks, ” in\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition\n(Beijing: IEEE), 7132–7141. doi: 10.1109/CVPR.2018.0074 5\nJenkinson, M., Bannister, P., Brady, M., and Smith, S. (2002 ). Improved\noptimization for the robust and accurate linear registration and motion correction\nof brain images. Neuroimage 17, 825–841. doi: 10.1006/nimg.2002.1132\nKarch, J. D., Filevich, E., Wenger, E., Lisofsky, N., Becker, M., Butler,\nO., et al. (2019). Identifying predictors of within-person var iance\nin MRI-based brain volume estimates. NeuroImage 200, 575–589.\ndoi: 10.1016/j.neuroimage.2019.05.030\nKassubek, J., Juengling, F. D., Kioschies, T., Henkel, K., Kar itzky, J., Kramer, B.,\net al. (2004). Topography of cerebral atrophy in early Huntington ’s disease: a voxel\nbased morphometric MRI study. J. Neurol. Neurosurg. Psychiatry75, 213–220.\nKhagi, B., and Kwon, G. R. (2018). Pixel-label-based segmentati on of cross-\nsectional brain MRI using simpliﬁed SegNet architecture-bas ed CNN. J. Healthc.\nEng. 2018:3640705. doi: 10.1155/2018/3640705\nKingma, D. P., and Ba, J. (2014). Adam: A method for stochasti c optimization.\narXiv [Preprint].arXiv:1412.6980. doi: 10.48550/arXiv.1412.6980\nKolarík, M., Burget, R., Uher, V. and Dutta, M. K. (2018). “3D d ense-\nU-net for MRI brain tissue segmentation, ” in 2018 41st International\nConference on Telecommunications and Signal Processing (Brno), 1–4\ndoi: 10.1109/TSP.2018.8441508\nKoutsouleris, N., Riecher-Rössler, A., Meisenzahl, E. M., Smi eskova, R., Studerus,\nE., Kambeitz-Ilankovic, L., et al. (2015). Detecting the psych osis prodrome across\nhigh-risk populations using neuroanatomical biomarkers. Schizophr. Bull. 41,\n471–482. doi: 10.1093/schbul/sbu078\nKruggel, F., Turner, J., Muftuler, L. T., and Alzheimer’s Disea se Neuroimaging\nInitiative (2010). Impact of scanner hardware and imaging pro tocol on image\nquality and compartment volume precision in the ADNI cohort. Neuroimage 49,\n2123–2133. doi: 10.1016/j.neuroimage.2009.11.006\nLee, B., Yamanakkanavar, N., and Choi, J. Y. (2020). Automat ic segmentation\nof brain MRI using a novel patch-wise U-net deep architecture . PLoS ONE\n15:e0236493. doi: 10.1371/journal.pone.0236493\nMahmood, Q., Chodorowski, A., and Persson, M. (2015). Autom ated MRI brain\ntissue segmentation based on mean shift and fuzzy c-means us ing a priori tissue\nprobability maps. IRBM 36, 185–196. doi: 10.1016/j.irbm.2015.01.007\nFrontiers in Neuroimaging /one.tnum/five.tnum frontiersin.org\nRao et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fnimg./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/three.tnum/four.tnum/eight.tnum/one.tnum\nMarroquín, J. L., Vemuri, B. C., Botello, S., Calderon, E., and Fe rnandez-\nBouzas, A. (2002). An accurate and eﬃcient Bayesian method f or automatic\nsegmentation of brain MRI. IEEE Trans. Med. Imaging 21, 934–945.\ndoi: 10.1109/TMI.2002.803119\nMlynarski, P., Delingette, H., Criminisi, A., and Ayache, N. ( 2019).\n3D convolutional neural networks for tumor segmentation usi ng\nlong-range 2D context. Comput. Med. Imaging Graph. 73, 60–72.\ndoi: 10.1016/j.compmedimag.2019.02.001\nMoeskops, P., Viergever, M. A., Mendrik, A. M., De Vries, L. S. , Benders,\nM. J., and Išgum, I. (2016). Automatic segmentation of MR bra in images with\na convolutional neural network. IEEE Trans. Med. Imaging 35, 1252–1261.\ndoi: 10.1109/TMI.2016.2548501\nNemoto, K., Sakaguchi, H., Kasai, W., Hotta, M., Kamei, R., N oguchi, T., et al.\n(2021). Diﬀerentiating dementia with lewy bodies and Alzheim er’s disease by deep\nlearning to structural MRI. J. Neuroimaging31, 579–587. doi: 10.1111/jon.12835\nNugent, A. C., Milham, M. P., Bain, E. E., Mah, L., Cannon, D.\nM., Marrett, S., et al. (2006). Cortical abnormalities in bipola r disorder\ninvestigated with MRI and voxel-based morphometry. Neuroimage 30, 485–497.\ndoi: 10.1016/j.neuroimage.2005.09.029\nRao, V. M., Wan, Z., Ma, D., Lee, P.-Y., Tian, Y., Laine, A. F., et al. (2022).\nImproving across-dataset brain tissue segmentation using t ransformer. arXiv\n[Preprint]. arXiv:2201.08741. doi: 10.48550/arXiv.2201.08741\nRodrigue, K. M., Kennedy, K. M., Devous, M. D., Rieck, J. R., H ebrank, A.\nC., Diaz-Arrastia, R., et al. (2012). β -Amyloid burden in healthy aging: regional\ndistribution and cognitive consequences. Neurology 78, 387–395.\nRonneberger, O., Fischer, P. and Brox, T. (2015). “U-net: co nvolutional networks\nfor biomedical image segmentation, ” in International Conference on Medical\nImage Computing and Computer-Assisted Intervention(Cham: Springer), 234–241.\ndoi: 10.1007/978-3-319-24574-4_28\nSalvador, R., Radua, J., Canales-Rodríguez, E. J., Solanes, A. , Sarró, S., Goikolea, J.\nM., et al. (2017). Evaluation of machine learning algorithms and structural features\nfor optimal MRI-based diagnostic prediction in psychosis. PLoS ONE12:e0175683.\ndoi: 10.1371/journal.pone.0175683\nSégonne, F., Dale, A. M., Busa, E., Glessner, M., Salat, D., Hahn, H. K., et al.\n(2004). A hybrid approach to the skull stripping problem in MRI. Neuroimage 22,\n1060–1075. doi: 10.1016/j.neuroimage.2004.03.032\nSled, J. G., Zijdenbos, A. P., and Evans, A. C. (1998). A nonparam etric method\nfor automatic correction of intensity nonuniformity in MRI data. IEEE Trans. Med.\nImaging 17, 87–97. doi: 10.1109/42.668698\nSun, Q., Fang, N., Liu, Z., Zhao, L., Wen, Y., and Lin, H. (2021 ). HybridCTrm:\nbridging CNN and transformer for multimodal brain image segm entation. J.\nHealthc. Eng.2021:7467261. doi: 10.1155/2021/7467261\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.\n(2017). “Attention is all you need, ” in Advances in Neural Information Processing\nSystems (San Francisco, CA), 5998–6008.\nWang, W., Chen, C., Ding, M., Yu, H., Zha, S. and Li, J. (2021). “Transbts:\nmultimodal brain tumor segmentation using transformer, ” in International\nConference on Medical Image Computing and Computer-Assisted Intervention\n(Cham: Springer), 109–119. doi: 10.1007/978-3-030-87193- 2_11\nWei, D., Zhuang, K., Ai, L., Chen, Q., Yang, W., Liu, W., et al. ( 2018). Structural\nand functional brain scans from the cross-sectional Southw est University adult\nlifespan dataset. Sci. Data5, 1–10. doi: 10.1038/sdata.2018.134\nWright, I. C., McGuire, P. K., Poline, J. B., Travere, J. M., Mur ray, R. M.,\nFrith, C. D., et al. (1995). A voxel-based method for the statis tical analysis of\ngray and white matter density applied to schizophrenia. Neuroimage 2, 244–252.\ndoi: 10.1006/nimg.1995.1032\nWu, Y., and He, K. (2018). “Group normalization, ” in Proceedings of\nthe European Conference on Computer Vision (Menlo Park, CA), 3–19.\ndoi: 10.1007/978-3-030-01261-8_1\nYamanakkanavar, N., and Lee, B. (2020). Using a patch-wise m- net convolutional\nneural network for tissue segmentation in brain mri images. IEEE Access 8,\n120946–120958. doi: 10.1109/ACCESS.2020.3006317\nZhang, W., Li, R., Deng, H., Wang, L., Lin, W., Ji, S., et al. (20 15). Deep\nconvolutional neural networks for multi-modality isointense infant brain image\nsegmentation. NeuroImage 108, 214–224. doi: 10.1016/j.neuroimage.2014.12.061\nZhang, Y., Brady, J. M. and Smith, S. (2000). “Hidden Markov r andom ﬁeld\nmodel for segmentation of brain MR image, ” in Medical Imaging 2000: Image\nProcessing, Vol. 3979 (Oxford: International Society for Optics and Photonics),\n1126–1137. doi: 10.1117/12.387617\nZhang, Z., Liu, Q., and Wang, Y. (2018). Road extraction by de ep residual u-net.\nIEEE Geosci. Remote Sens. Lett.15, 749–753. doi: 10.1109/LGRS.2018.2802944\nFrontiers in Neuroimaging /one.tnum/six.tnum frontiersin.org",
  "topic": "Segmentation",
  "concepts": [
    {
      "name": "Segmentation",
      "score": 0.6143194437026978
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5221036672592163
    },
    {
      "name": "Neuroimaging",
      "score": 0.4537492096424103
    },
    {
      "name": "Computer science",
      "score": 0.4512629806995392
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.38554614782333374
    },
    {
      "name": "Computer vision",
      "score": 0.35486656427383423
    },
    {
      "name": "Neuroscience",
      "score": 0.1823568344116211
    },
    {
      "name": "Psychology",
      "score": 0.1597295105457306
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    }
  ]
}