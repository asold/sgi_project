{
  "title": "Implementation Of The Swin Transformer and Its Application In Image Classification",
  "url": "https://openalex.org/W4389514562",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5079038963",
      "name": "Rasha Ali Dihin",
      "affiliations": [
        "University of Kufa"
      ]
    },
    {
      "id": "https://openalex.org/A5093455694",
      "name": "Ebtesam N. Al Shemmary",
      "affiliations": [
        "University of Kufa"
      ]
    },
    {
      "id": "https://openalex.org/A5082536766",
      "name": "Waleed A. Mahmoud Alâ€Jawher",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3197957534",
    "https://openalex.org/W3213783001",
    "https://openalex.org/W4411245096",
    "https://openalex.org/W4224250702",
    "https://openalex.org/W3161825146",
    "https://openalex.org/W4205281574",
    "https://openalex.org/W4229334058",
    "https://openalex.org/W6800689796",
    "https://openalex.org/W4220815323",
    "https://openalex.org/W4221163766",
    "https://openalex.org/W4220768647",
    "https://openalex.org/W6797737728",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W3157506437",
    "https://openalex.org/W3216853350",
    "https://openalex.org/W2750384547",
    "https://openalex.org/W2046759099",
    "https://openalex.org/W2188508910",
    "https://openalex.org/W1486674343",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W4285707640",
    "https://openalex.org/W3207918547",
    "https://openalex.org/W4286857019",
    "https://openalex.org/W4287062814",
    "https://openalex.org/W3180659574",
    "https://openalex.org/W4287324101",
    "https://openalex.org/W4312560592",
    "https://openalex.org/W3138516171"
  ],
  "abstract": "There are big differences between the field of view of the calculator and the field of natural languages, for example, in the field of vision, the difference is in the size of the object as well as in the accuracy of the pixels in the image, and this contradicts the words in the text, and this makes the adaptation of the transformers to see somewhat difficult.Very recently a vision transformer named Swin Transformer was introduced by the Microsoft research team in Asia to achieve state-of-the-art results for machine translation. The computational complexity is linear and proportional to the size of the input image, because the processing of subjective attention is within each local window separately, and thus results in processor maps that are hierarchical and in deeper layers, and thus serve as the backbone of the calculator's vision in image classification and dense recognition applications. This work focuses on applying the Swin transformer to a demonstrated mathematical example with step-by-step analysis. Additionally, extensive experimental results were carried out on several standardized databases from CIFAR-10, CIFAR-100, and MNIST. Their results showed that the Swin Transformer can achieve flexible memory savings. Test accuracy for CIFAR-10 gave a 71.54% score, while for the CIFAR-100 dataset the accuracy was 46.1%. Similarly, when the Swin transformer was applied to the MNIST dataset, the accuracy increased in comparison with other vision transformer results.",
  "full_text": " \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n318 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n  \nImplementation Of The Swin Transformer and Its Application In \nImage Classification \n1Rasha. A. Dihin,  2Ebtesam N. Al Shemmary, 3Waleed Ameen Al Jawher  \n1Department of Computer Science, University of Kufa, Najaf, Iraq, \n. 2IT Research and Development Center, University of Kufa, Najaf, Iraq \n3Collge of Engenering, Uruk University, Baghdad, Iraq \nrashaa.aljabry@uokufa.edu.iq. \n \n \n \n \n \n \n \n \n \n \n \nKeywords: Image classification, Object detection, Swin transformer ST, Vision transformer ViT.\n1. INTRODUCTION \nVision transformers (ViTs ) were first proposed for the \nmachine translation task in the Natural Language Processing \nNLP domain. The transformer -based methods have \naccomplished innovative performance in a variety of tasks \n[1]. The ViT's disadvantage is that it necessitates pre,-training \non its larg,e ,dataset, [2 -4]. Transformers have been widely \nused in numerous vision problems, especially for visual \nrecognition and detection [5].  \nThe pioneering work of Swin Transformer [6]  has two \noutstanding and effective contributions that distinguish it \nfrom other transformers : \na) The feature hierarchical scheme offers outstanding \nperformance in terms of computational complexity. It is \nlinear, and for this feature (ST) is the backbone of (CV) \ntasks. \nb) The second advantage of ST is that it proposes a design \nequipped with variable -size windows between layers for \nsuccessive attention, and this improves the power of the \nmodeling [7]. \nThe model proposed by Gu. Yeonghyeon et al  [8] \nSTHarDNet, which combines the blocks of Swin adapters \nwith U -Net, is light in weight because it contains. The \nproposed model was used in the segmentation of MRI images \ntaken to diagnose stroke, and it gave superior results. The \nSwin was used in the first communication layer in the \nencryption stage to preserve the features of the Swin \nhierarchical switch STHarDNet is similar to the CNN model \nin that it takes into account the constraints and completes the \ntask, therefore it is considered the best. \nLiao. Zhihao .el.at [9] The Swin-PANet model is proposed as \na window-dependent self,-attention mechanis.m using ST in \nthe intermediate supervision network, which has been called \nthe Prior Attention Network. In this case, there was an \nincrease in the improvement of the details in the limits, and \nthe proposed Swin- PANet was used to diagnose skin cancer, \nand it gave efficient results in improving the accuracy of \nsegmentation and outperformed the modern models, but \ndespite that, it still suffers from a set of limitations, for \nexample, the ability to transfer learning . \nL. Jingyun et al  [10] proposed a powerful basic image \nrecovery model based on the Swin Transformer named \nSwinIR. The proposed model consists of three parts, firstly \nextracting shallow features, secondly extracting deep \nfeatures, and finally creating HR modules for re -basis, \nSwinIR achieved superior image recovery performance in \nthese three tasks and six different settings. namely: â€œclassic \nimage SRâ€, grayscale image denoising, RGB image \nAbstract There are big differences between the field of view of the calculator and the field of natural \nlanguages, for example, in the field of vision, the difference is in the size of the object as well as in the \naccuracy of the pixels in the image, and this contradicts the words in the text, and this makes the adaptation \nof the transformers to see somewhat difficult .Very recently a vision transformer named Swin Transformer \nwas introduced by the Microsoft research team in Asia to achieve state -of-the-art results for machine \ntranslation. The computational complexity is linear and proportional to the size of the input image, because \nthe processing of subjective attention is within each local window separately, and thus results in processor \nmaps that are hierarchical and in deeper l ayers, and thus serve as the backbone of the calculator's vision in \nimage classification and dense recognition applications. This work focuses on applying the Swin \ntransformer to a demonstrated mathematical example with step -by-step analysis. Additionally,  extensive \nexperimental results were carried out on several standardized databases from CIFAR -10, CIFAR-100, and \nMNIST. Their results showed that the Swin Transformer can achieve flexible memory savings. Test \naccuracy for CIFAR -10 gave a 71.54% score, whil e for the CIFAR -100 dataset the accuracy was 46.1%. \nSimilarly, when the Swin transformer was applied to the MNIST dataset, the accuracy increased in \ncomparison with other vision transformer results.    \n \n10.36371/port.2023.4.2  \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n319 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n \n  \ndenoising, real -world image SR, light,weight image, and \nJPEG. \nH. Siyuan et al [11] proposed a new method for classifying \nremote sensing images called (TSTNet), which is a two -\nstream network using Swin   transformer, and which was used \nfor each stream and considered the basis for it. \n  Thus, when comparing the performance of the proposed \nnetwork with modern models, it was found that the network \ngives good performance by classifying on a difficult and \navailable data set. \nA. Hatamizadeh et al [12] reformulated the problem semantic \nsegmentation of 3D \"brain tumor\" as a Seq-to-Seq prediction \nissue by developing the Swin UNEt TRansformers (Swin \nUNETR) model. In this model, the input data of multi-modal \nwas converted onto a 1D embedding sequence and utilized as \nthe input to a hierarchical ST representing the encoder. In the \nvalidation phase, this model was listed among the top -\nperforming techniques, and in the t esting phase, it displayed \ncompetitive performance.  \nThis paper will cast as a specific case of extending the â€œself -\nattentio,n mechanismâ€ of the Swin Transformer by \ndemonstrating its performance on a given matrix. It will \nanalyze the intermediate results and explains the arbitrary \nrelations between any two e lements of the input during the \nimplementations. In addition, it will explore the work on \nmodeling labeled and directed graphs. On the three tasks, the \nSwin Transformer performance architecture was compared to \nthe past â€œstate -of-the-artâ€. Several experimen ts were \nconducted on â€œCIFAR -,10â€, â€œCIFAR -,100â€, and â€œMNISTâ€ \nfor image classification. The paper was organized into five \nsections: the second section covers the implantation of the \nSwin Transformer Block. ,Section 3 elucidates the \napplication of the Swin transform, s ectionâ€™ 4 states the \nresults, and the final section demonstrates several \nconclusions. \n2. BASIC CONCEPT OF A TRANSFORMER \nThe main structure of any Transformer [6-8] uses an encoder-\ndecoder structure where we note the use of layers in both \nencoder and decoder as shown in Figure 1 \nThe encryption layer also contains two important sub-layers: \nfirst, the self -attention, and second, the position -wise feed-\nforward layer. On the other hand, the decoding layer contains \nthree sub-layers:1) self-attention,layer, 2) â€˜encoderâ€™ -decoder \nattention, and 3) a â€˜positio,n -wise feedâ€™-,forward layer. The \ntransformers in general use a special type of connection \naround each of the sub -layers called residual connections. In \naddition, it uses an essential process known as  layer \nnormalization. One of the important stages in the structure of \ntransformers is the masking in the decoder. This masking \nwhich uses self -attention prevents a given output position \nfrom incorporating information about future output positions \nduring training. Both the input components of the  encoder \nand the decoder are locally encoded based on sinusoids at \ndifferent frequencies and all this is before entering the first \nlayer and therefore useful in making the model generalizable \nduring training, and also helps the model to learn by relative \nposition and when comparing this property with \nrepresentations Relative position is paradoxical because the \nrelative position of its representations is fixed and this, of \ncourse, helps spread the location information to other higher \nlayers. \n \nFigure 1: Transformer main blocks \n3. THE SHIFTED WINDOW \nTRANSFORMER (ST) \n     Shifted Window Transformer or Swin transformer (ST) \nconstructs hierarchical feature maps of the image by \ncombining their patches into deeper layers. The shifted \nwindowing scheme brings greater efficiency by limiting self-\nattention computation to non -overlapping local windows as \nwell as allowing the cross -window connection. The cost is \ncomputationally linear because the processing of self -\nattention within each local window is proportional to the size \nof the image entered. T hus, it generates feature maps with a \nsingle low resolution in comparison with earlier vision \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n320 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n  \ntransformers. Therefore, it can serve as a foundation for \nimage and texture categorization, and speech processing \nalgorithms. There are many Challenges in Vision \nApplications using the Shifted Window Transformer. \nTransformer-based models are all set tokens  in size and may \ntherefore be inappropriate for some CV applications \nThe resolution  of the pixels in the images is higher than the \nwords in the text segments, and this is another variation \nbetween CV and NLP. The last difference is that some CV \ntasks may require detailed predictions at the pixel level. For \nexample, semanti c segmentation is not possible in high -\nresolution images because it causes quadratic computational \ncomplexity. As a solution, the Microsoft research team in \nAsia presents the Swin transformer as a general -purpose \nTransformer foundation that creates deep network maps. The \nhierarchical diagram of the Swin Transformer is given in \nFigure 2. \nFigure 2 shows how ST builds a hierarchical representation \nby starting with tinypatches and then merging these \nneighboring patches to get deeper layers. \nWhere by using these feature maps that are hierarchical, the \ncomputational complexity is linear so that it divides the image \ninto non -overlapping windows, as well as the number of \ncorrections is constant, unlike other transformers, which are \ndistinguished maps with one precision and therefore the \ncomputational complexity is quadratic \n \nFigure 2: The Hierarchical of the Swin Transformer \n4. STRUCTURE DESIGN \nMoving the window between successive ST self -attention \nlevels switches to final layer windows. This key feature of the \nST greatly enhances modeling capabilities and because all \nquery patches that are inside the window share the same key \nset, this simplifie s access to device memory [6]. Figure 3 \nshows the architecture of the ST in its simplest form. The \ninput RGB image is split into several nonn -overlapping, \npatches using the patch splitter used in ViT,  where each patch \nis treated as a \"token\", where individual pixels are set to be a \nsequence of â€œRGBâ€ values. \nThe input image is routed via the patch partition layer and \nsplit into patchess of size 4x4 to create patch tokens with the \nshape (W/4, H/4, and 4x4 channel). In stage 1, the resulting \npatch tokens are subjected to linear embedding. They are then \nfed into two linked ST blocks to make tokens of (W/4, H/4, \nC), where C may be any dimension. Patch merging and Swin \ntransformer blocks are used in stages 2, 3, and 4, respectively. \nIn stages 2, 3, and 4, the shape of the tokens is (W1/81, H1/81, \n2C),(W1/161, H1/161 , 4C1), and (W1/321, H1/321, 8C1) \nrespectively. \nSwin Transformer can be created using transformed \nwindows. The normal multi -headed self -attention unit has \nbeen replaced. The rest of the layers remain the same. This \nlayer is followed by a non-linear unit of Gaussian error. Next, \nLayer normalization is applied before each multi-head self. -\nattention module. Finally, a residual connection is applied \nafter each of the above modules mentioned before.  \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n321 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n \n  \n \nFigure 3: The architecture of the Swin transformer \nThe following steps demonstrate how these module computations of the Swin transformer are implemented considering a given \nmatrix: \nStep 1: Give a matrix R of 8x8 pixels as shown in Figure 4. \n \nFigure 4: The matrix R. \nStep 2: Self-attention in non-overlapped blocks. \nSelf-attention In sub -layers use multiple attention heads  \nwhere the results in each heads  are sequential and a linear \ntransformation will be applied to the parameters in them. The \nfirst Swin Transformer (Swin-T) block module uses \n a regular window partitioning designing. The next block \nadopts a shifted window partitioning strategy, while the first \nregular block partitioning begins from the top -left pixel [3], \n[11]. Hence the 8x8 given feature map is evenly divided into \n2x2 windows with 4x4 items each, as illustrated in Figure 3.  \nEach patch is called a â€œtokenâ€ with a size of 4x4x3=48 pixels, \nwhere 3 is for the RGB channel and 4 is the height and width \nof the square patch, as shown in Figure 5. In patch partition, \nif M=4, if matrix 8*8, the patch=64. After patch partition, the \nsize of the image will become: \nSize of image = 2 x 2 x 48,   \nğ»\n4 Ã—\nğ‘Š\n4 Ã—  ğ¶ = \n8\n4 âˆ—\n8\n4 âˆ— 48 = 192 \nThen, the number of channels will be converted from 48 to C \nthrough a full link layer, also known as the Linear Embedding \nlayer. After finishing the patch, it will go through Linear \nEmbedding [13]. \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n322 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n  \n \nFigure 5: Patch partition diagram. \nStep 3:  The matrix R in this stage 1 is shown in Figure 6, \nwhere the number of symbols maintained in the transformer \nblocks is (\nğ»1\n41 Ã—\nğ‘Š1\n41 ). Figure 6 shows how each 4x4 pixel (3 \nchannels) is flattened into 1x1 patches (48 channels). The \nmatrix R with 8x8 pixels (3 channels) is processed in this \nway, then (2, 2) patches, 48 channels, and a feature map with \nthe size of (2, 2, 48) will be received, (see the second \nstereogram with the green part in Figure 6). Input features (W \n/ 8 Ã— H / 8 Ã— 8C1) apply a linear layer to it, so to increase the \ndimensions of this feature to 2 Ã— the original dimension   to \n((21Ã— the original dimension1)) (W/8Ã—H/8Ã—8C). To expand \nthe accuracy of the features of entering to (W,/8Ã—H1/8Ã—8C) \nand reduce the dimension to a quarter of the input dimension \n(W1/8Ã—H1/8Ã—2C â†’ W/4Ã—H/4Ã—C), the scientific order is \nrearranged.Patch partition and Linear Embedding are directly \ncombined into one, a convolution core with a size of 4x4, and \na stripe of 4 that is used directly to convert the number of \nchannels from 3 to C. \nStep 4: The matrix R in stage 2 is shown in Figure 7 Number \nof symbols reduced by a multiple of 2x21 = 4 (2x precision \ndownsampling).The output from it is set to 2C. feature \ntransformation used with Swin transformer blocks with the \nresolution remaining at:  \nğ»\n8 âˆ—\nğ‘Š\n8 âˆ— 2ğ¶, for matrix R  \n8\n8 âˆ—\n8\n8 âˆ— 2 âˆ—\n48 = 1 âˆ— 1 âˆ— 96 = 96. \nAssuming the input patch merging is an 8x8 single -channel \nfeature map (feature map), Patch Merging will divide each \n4x4 adjacent pixel into a patch, and then divide the same \nposition in each patch (the same Color) pixels into 4 feature \nmaps (putting them together). The depth of the feature map is \nchanged from C to 2C. This basic example shows that after \ngoing through the patch merging layer, the height and width \nof the feature map are halved, but the depth is doubled. To \nreduce the resolution, adjust the number of channels used, and \ncomplete the hierarchical design of the ST, merge correction \nwas used [14]. In this case, each downsample is two, and \nelements are chosen at every other point in the row and \ncolumn directions before being spliced together to expand, as \nseen in Figure 8. Next, apply the Swin transformer block to \nthe result. \n \nFigure 6: Stage 1 representation. \n \nFigure 7: Stage 2 representation \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n323 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n \n  \n \nFigure 8: Patch merging process. \nStep 5:  In the Sstage3, patch merge is applied which \naggregates 2 x 2 adjacent patches, then a linear layer with 4C-\nD is applied and by this process the number of tokens is \nreduced by multiples of (6 x precision downsampling) kept \nat  (\nğ»\n16 âˆ—\nğ‘Š\n16 âˆ— 4ğ¶) as shown in Figure 9 below, for matrix R  \n8\n16 âˆ—\n8\n16 âˆ— 4 âˆ— 48 = 4. \n \nFigure 9: Stage 3 representation \nStep 6: In stage 4, The patch merge layer aggregates the features of each set of patches that are contiguous ( 2x2 ) and then \napplies a linear layer (8C -D ) to the contiguous features. This decreases the number of â€œtokensâ€ by a factor of (2x resolution \ndownsampling) remaining at \nğ»\n32 âˆ—\nğ‘Š\n32 âˆ— 8ğ¶ as shown in Figure 10, for matrix R   \n8\n32 âˆ—\n8\n32 âˆ— 8 âˆ— 48 = 24. \n \nFigure 10: Stage 4 representation \nThe stages (1, 2, 3, and 4) work together to provide a \nhierarchical representation on the same resolution as the \nfeature map. \nStep 7: Generation matrix for 4 blocks (R1, R2, R3, R4) as \nshown in Figure 11, each block with a size of 4 Ã— 4 elements. \nWhere block R1 take the first element in the block \npartitioning [1, 2, 2, 2; 1, 1, 2, 2; 3, 3, 4, 4; 3, 3, 4, 4],  while \nblock R2 will take the second block from the block  \npartitioning [5, 5, 6, 6; 5, 5, 6, 6; 7, 7, 8, 8; 7, 7, 8, 8] , block \nR3 take the third block from the block  partitioning [9, 9, 10, \n10; 9, 9, 10, 10; 11, 11, 12, 12; 11, 11, 12, 12]  , and lastly, \nblock R4 takes a block from block partitioning  [13, 13, 14, \n14; 13, 13, 14, 14; 15, 15, 16, 16; 15, 15, 16, 16]. \n \nFigure 11: Self -attention within each block. \nStep 8: Compute the global bull attention mechanism (MSA) \nas in Equation 1.  Its computational complexity is the same as \n(hw) square correlation [15]. \nÎ© (M S A )=4 h w C2+2 ( h w )2C                  ( 1 ) \nWhere; \nâ€¢ h denotes the feature map height1. \nâ€¢ W1 is the 1feature map width1. \nâ€¢ C1 is the 1feature map depth1. \nâ€¢ M denotes the size of each window \n(Windows). \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n324 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n  \nTo implement on matrix R in Figure 2 with (H=8, W=8, \nC=48, M=4): \n               Î© (M S A )=4 h w C2+2 ( h w )2C                 ( 1 ) \n                                =4x8x8x(48)2+2(8x8)2x48 \n                                =983040 (quadratic complexity    ( \nThe standard transformer for vision conducts global self -\nattention, which has quadratic complexity concerning the \nnumber of tokens leading to intensive computational cost \n[16]. \nStep 9:  Compute window1 -based multi1 -head1 attention \nmechanism (W1 -MSA) as in Equation 2, because the \ncomputational complexity is huge in (M S A) [15]: \n               Î© (Wâˆ’M S A) =4 h w C2+2M 2 h w C           ( 2 ) \nTo implementation on matrix A in Figure 1 with (H=8, W=8, \nC=48, M=4). \n                 = 4x8x8x (48)2+2x42x8x8x48 \n                 =688128 (linear complexity) \nStep 10:  ShiftedR window 1partitioning in successive \nblocksS, see Figure 12. ST is constructed by substituting the \nconventional (MSAA) module in a Transformer block with a \nwindow-shifted moduleL. It adopts the configuration of \nwindows in the next module, in which each of the previous \nblocks is divided, resulting in new windows in the next block, \nThe self-attention computation in the new blocks crosses the \nboundaries of the previous blocks, providing connections \nacross blocks between neighboring non -overlapping \nwindows. Two consecutive Swin Transformer block \nalternates are used in both W-MSA and SW-MSA. \n \nFigure 12: Block partition. \nWhere shifted block partitioning generates more windows, \nthe windows increase by ( x 2.25). The number of a window \nin the Self -attention of non -overlapped blocks is (2Ã—2)=4 \nwindows while in this case is (3Ã—3)=4Ã—2.25=9 windows with \nsome of the blocks will be smaller. \nStep 11: The effective batch calculation for shifted \nconfiguration. Shifted window dividing generates more \nwindows, and some of the windows will be smaller. An \nefficient batch computing strategy based on cyclic shifting \ntowards the top -left side offers a better workaround. Figure \n13 shows the computing method of cyclic shifting \ntowards the top -left side. Figure 14 shows the \nimplementation of cyclic shifting on th e matrix R . Where \napply cyclic shifting by (-2, -2) to the matrix R to the outcome \nas illustrated in Figure 14. \n \nFigure 13: Cyclic shifting computation. \n \nFigure 14: The implementation of cyclic shifting on matrix R \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n325 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n \n  \n \nFigure 15: The implementation of cyclic shifting by (-2, -2) to the matrix R. \nStep 12: Masked-MSA. The batched block may be formed of \nmany sub-blocks that are not contiguous to the feature maps \nafter the cyclic shift; hence, Within each sub -block to \ndetermine to limit  \"self -attention\" a technique called \nMasking  is used  . The cyclic shift maintains the same \nnumbers of the batched blocks  as standard block splitting \nwhile still being computationally efficient, as seen in Figure \n16. Figure 17 illustrates how to find masked -MSA from the \nmatrix R. \n \nFigure 16: Masked-MSA mechanism. \nStep 13: Compute Shifted window partitioning in successive \nblocks.  \nST was built by adopting SW-MSA, where this unit relies on \ntransformed windows instead of (MSA) and the rest of the \nlayers were kept as they are, and this layer is followed by two \nlayers of  ) MLP( and \"GELU\" where this layer is non-linear. \nBoth the MSA layer and the MLP layer were preceded by the \nLN layer, and because this mechanism lacks connections \nacross windows, this means that its modeling ability is \nlimited  as in Figure 18. Equations (3 -6) state the \nmathematical expression of W -MSA and SW -MSA as \nfollows [15]: \n \nFigure 18: An example of the Swin transformer connection. \n \nğ‘§Ì‚ğ‘™ = ğ‘Š âˆ’ ğ‘€ğ‘†ğ´(ğ¿ğ‘(ğ‘ğ‘™âˆ’1))1 + ğ‘1ğ‘™âˆ’1       (3) \nğ‘§ğ‘™ = ğ‘€ğ¿ğ‘ƒ(ğ¿ğ‘(ğ‘ğ‘™âˆ’1)) + ğ‘§Ì‚ğ‘™Ø§Ø§                       4) \nğ‘§Ì‚ğ‘™+1 = ğ‘†ğ‘Š âˆ’ ğ‘€ğ‘†ğ´(ğ¿ğ‘(ğ‘§ğ‘™)) + ğ‘§ğ‘™              (5) \nğ‘ğ‘™+1 = ğ‘€ğ¿ğ‘ƒ(ğ¿ğ‘(ğ‘§Ì‚ğ‘™+1)) + ğ‘§Ì‚ğ‘™+1                 (6) \nWhere:  \nâ€¢ ğ‘§Ì‚ğ‘™: the item is in the current block. \nâ€¢ ğ‘ğ‘™âˆ’1: the item in the previous block.   \nâ€¢ L N: layer-norm.   \nâ€¢ M L P: multi-layer perceptron.  \nâ€¢ W-MSA: window self-attention.  \nâ€¢ SW-MSA: shift window self-attention \n1. To find ğ‘§Ì‚ğ‘™  apply Equation (3) as shown in Figure 19. \n \nFigure 19: The computation of the item in the current block. \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n326 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n  \nFirst, need to find the LN for ( ğ‘ğ‘™âˆ’1 ) where layer \nnormalization (LN) is a key part of the transformer for stable \ntraining and faster convergence. Equation 7 applies the LN \nover each sample z âˆˆ Rd, as shown in Figure 20 as follows \n[17],[18]: \nğ¿ğ‘(ğ‘§) =\nğ‘§âˆ’ğœ‡\nğ›¿ Â°ğ›¾ + ğ›½                   (7) \n \n \nFigure 20: The LN(z) Computation. \nWhere: \nâ€¢ Âµâ€™ âˆˆ R, Î´ âˆˆ R: is the feature's mean and standard \ndeviation, respectively.  \nâ€¢ (â—¦): is the â€œelement-wise dotâ€,  \nâ€¢ Î³' âˆˆ Rd, Î² âˆˆ Rd: are affine transform parameters that can \nbe learned, where Î³ refers to gain (scale factor) and Î² \nrefers to biases (offset) the value initialize Î² equals 0 \nvalue initialize Î³ equals 1 and if the affine transform = \ntrue.  \nâ€¢ Now, to compute ğœ‡ in LN apply the Equation 8 as \nfollows [18]: \n         ğœ‡ =\n1\nğ‘š âˆ‘ ğ‘§ğ‘–,ğ‘—\nğ‘š\nğ‘—=1                            (8)   \nâ€¢ To compute Î´ apply Equation 9 as follows [19]: \n         Ïƒ =\n1\nm\nâˆš(âˆ‘ zI,j âˆ’ Î¼I,j\nm\nj=1 )\n2\n           (9) \nAfter computing the (LN) for ğ‘ğ‘™âˆ’1 apply the Window -Multi-\nHead Self Attention to compute (LN) need to detect the \nnumber of channels, mini Batch Size, and the sequence \nLength (ğ‘Š âˆ’ ğ‘€ğ‘†ğ´ ) as shown in step 2 to find the item in the \ncurrent block (z ^ l). To implement matrix R take the block \nR1= [1 1 2 2; 1 1 2 2; 3 3 4 4; 3 3 4 4], the number of channels \n=3, min -Batch Size =4 and sequence Length=1 then the ğœ‡ =\n2.500 and Ïƒ = 1.1547, Î³ =1, Î²= 0. Now, compute the block \nR1 by applying Equation 7. The result =[-1, -1, -1,-1, -1, -1, -\n1, -1, 1, 1, 1, 1, 1, 1, 1, 1].The same thing applies to block R2, \nblock R3 and block R4, after that the result passed over the \nW-MSA as shown in step 3 where the result from W-MSA is \nthe summation with the item in  the previous block  (ğ‘ğ‘™âˆ’1).  \n2. Find ğ‘§ğ‘™ as shown in Figure 21. To compute  ğ‘§ğ‘™ need to find \nLN for the item in the previous block ( ğ‘ğ‘™âˆ’1) , to implement \nmatrix R take the block R1= [1 1 2 2; 1 1 2 2; 3 3 4 4; 3 3 4 \n4], the result LN( ğ‘ğ‘™âˆ’1)=  [-1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, \n1, 1, 1, 1, 1], then, as shown in Equation 10, MPL is used \nbetween self-,attention layers for feature transformation and \nnon-linearity [19]: \nMLP(X) = FC (Ïƒ(FC(X)))         (10) \n               FC(X) = XW + b            (11) \nWhere: \nâ€¢ W: the weight. \nâ€¢ b: the fully-connected layer's bias term. \nâ€¢ Ïƒ (Â·):  an activation function like GELU. \n \nFigure 21: Finding node in the hidden layer. \nTo apply Equation 11 needs to find the first FC. When using \nFC, a weight matrix must be calculated before adding a bias \nvector. Then, as indicated in Equation 12, use the activation \nfunction GELU (gaussian error linear unit) as an activation \nfunction on th e outcome of fully connected (FC) as follows \n[18],[20]: \nâˆ…(ğ‘¥1) = 0.51ğ‘¥(1 + tanh [âˆš2 ğœ‹ â„ (ğ‘¥ÙŠ+ 0. 7044715ğ‘¥3)]         \n(12) \nAfter that apply fully connect (FC) again on the result from \nMLP summation with  ğ‘§Ì‚ğ‘™ . \n3. Compute ğ‘§Ì‚ğ‘™+1 by applying Equation 5 as shown in Figure \n22. Finding the layer normalization (LN) first, and then \napplying Shift Window-Multi-head Self Attention SW-MSA \nto the result, as illustrated in steps (6-8). \n4. Compute   ğ‘ğ‘™+1 by applying Equation 6 as shown in Figure \n23. First, find the layer normalization (LN), and then apply \nthe Mulli Multi-Layer Perceptron (MLP) to the LN result.  \n \nFigure 23: Computation of ğ‘ğ‘™+1 \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n327 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n \n  \nStep 14: Reverse Cyclic shift. Figure 24 depicts the implementation of the reverse cyclic shift to retrieve the original matrix. \nFigure 25 shows the implementation of the reverse cyclic shift on the matrix resulting from the cyclic shift in step 8.  \n \nFigure 24: Reverse Cyclic shift implementation. \n \nFigure 25: Implementation of the reverse cyclic shift on the matrix from step 8. \n5. DATASET APPLICATION OF SWIN \nTRANSFORMER \nVision transducers have wide applications including image \nrecognition as well as image classification, object detection \nand segmentation. For the application of the Swin transducer, \nin this paper it is applied to two datasets.as follows: \n1. CIFAR-10 dataset, The project was funded by the \nCanadian Institute for Advanced Research, a collection of \n600 images from each of the 100 classes was gathered to be \nused in this work. This is referred to as the CIFARt -100b \ndataset. This dataset was collected using the same methods as \nCIFARn-10o. CIFAR-100 classesvv are mutually exclusive \nof CIFAR-10 classes, CIFAR-10 and CIFAR-100 are subsets \nof the 808 million annotated tiny image datasets [21]. \nCIFAR-10 and CIFAR -100n bdatasets consist of 502,000 \ntraining and 10j,000 testy images of 321Ã—327 resolution with \na total number of classes 10v and 100u, respectively \n[22],[21]. \n2. The MNIST dataset, the Modified National Institute \nof Standards and Technology  database introduced by LeCun \net al. in 1998, is a decent database for individuals consisting \nof 10 -class â€œhandwritten digitsâ€. The MNIST dataset has a \ntraining set of 60,000l instances and a testv set of 10,000c \nexamples, the image with a resolution of 28x28 and a total of \nx10 classes. MNIST's popularity stems from its small size \n[23]. \n6. RESULTS \nThe ST's shifted windowing technique improves performance \nby connecting via windows and by restricting the account for \nself-attention to local windows that are not overlapping.  \nMany parameters must be selected before conducting the \nSwin transform on a dataset, such as a patch size, the number \nof heads, the embedding dimension, the number of multi -\nlayer perceptrons, the size of the window, and the size of the \nshift. Where the para meter values in this work are ((2x2), 8, \n64, 256, 2, and 1), respectively.  In this paper, the Swin \nTransformer achieves strong performance on the CIFARx-10 \nand CIFAR-1001 datasets. Table1 and (Figure 26 Appendix) \nshows the performance of the Swin transform model on the \nCIFAR-10 datasets. Table2 and (Figure 27 Appendix) shows \nthe performance of the Swin transform model on the CIFAR-\n100 datasets. Table 3 and (Figure 28 Appendix) show the \nperformance of the Swin transform on the MNIST dataset. \nTable 1. The classification accuracy and loss on the CIFAR-10 \ndataset. \nEpoch Trainn- \nAccuracyb \nTarink -\nlossk \nValb - \nAccuracys \nVal-loss \n1 0.3325 1.9498 0.4152 1.7837 \n10 0.6328 1.6481 0.6254 1.3497 \n20 0.6852 1.2335 0.6692 1.2787 \n30 0.7141 1.1782 0.6946 1.2235 \n40 0.7288 1.1525 0.7080 1.2062 \n50 0.7438 1.1222 0.7272 1.1581 \n60 0.7561 1.0971 0.7272 1.1745 \n70 0.7628 1.0820 0.7112 1.1832 \n80 0.7718 1.0660 0.7262 1.1653 \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n328 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n  \n \nTable 2. Classification accuracy and loss on CIFAR-100 dataset. \nEpoch  Trainv- Accuracyx  Trainz -lossa Valq - Accuracyq Vals-lossz \n1 0.0798 4.1651 0.1288 3.9402 \n10 0.3442 2.9926 0.3338 3.0327 \n20 0.4055 2.7698 0.3896 2.8351 \n30 0.4380 2.6544 0.4078 2.7628 \n40 0.4616 2.5686 0.4184 2.7438 \n50 0.4788 2.5159 0.4318 2.7099 \n60 0.4960 2.4707 0.4362 2.6810 \n70 0.5048 2.4331 0.4356 2.6860 \n80 0.5122 2.4086 0.4452 2.6386 \nThrough Table 1 and Table 2, It can be noted that the Swin \ntransform performed well when applied to CIFAR -10 and \nCIFAR-100 datasets, and the accuracy increases in both \ntraining and validation as the number of epochs increases, \nwhile the loss is lower in bo th training and validation as the \nnumber of epochs increases. According to the results, the \nSwin transform produces better outcomes with CIFAR -10 \nthan with CIFAR-100. Whereas the test accuracy for CIFAR-\n10 was 71.54%, it was 46.1% for the CIFAR -100 dataset . \nTable 3 indicates that when the Swin transform is applied to \nthe MNIST dataset, the accuracy increases in both training \nand validation as the number of epochs increases.  \nTable 3. Classification accuracy and loss on MNIST dataset. \nEpoch Trainq- \nAccuracyw  \nTrainx -\nlossw \nVala - \nAccuracyc  \nValq-\nlossj \n1 0.0834 4.1474 0.1196 3.9148 \n10 0.3423 2.9891 0.3288 3.0287 \n20 0.4024 2.7730 0.3692 2.8723 \n30 0.4389 2.6501 0.3922 2.7989 \n40 0.4622 2.5711 0.4214 2.7222 \n50 0.4791 2.5199 0.4204 2.7206 \n60 0.4950 2.4699 0.4358 2.6942 \n70 0.5034 2.4313 0.4334 2.6717 \n80 0.5172 2.3921 0.4318 2.6806 \n6. CONCLUSION \nThe Swin Transform is (VT), where its computational \ncomplexity is proportional to the size of the input image, \nbecause its representation is a hierarchical \nrepresentation.Furthermore, ST beats prior best techniques in \nCOCOR objectQ identification and â€œADE20Kâ€ semantic \nsegmentation. The properties of ST make it suited for a wide \nrange of vision applications, including dense prediction tasks \nand image classification, such as â€œobject recognitionâ€ and \nâ€œsemantic segmentationâ€. According to the researchers, ST's \nexcellent performance on many vision challenges will \nsupport the integration of vision and language signal \nmodeling. The researchers proved that shifting Window -\nbased self-attention diversion is effective and appropriate for \nvisual problems and is an essential component of ST. This \nwork used the Swin transform on a mathematical example \nand detailed all of the transformation processes. It was also \nused to classify images into many classes using three different \ndatasets (CIFAR-10, CIFAR-100, and MNIST). The r esults \ndemonstrated that the Swin transform performs well in \nclassifying images with linear computational complexity as \ncompared to the ViT transformer's quadratic computational \ncomplexity. By the incorporation of Swin transformer with \ntransformation techn iques, the process of selecting, \ncombining, generating or adapting several features to \nefficiently solve accuracy and computation time problems. \nOne of the motivations for studying Swin transformer is to \nbuild systems which can handle classes of problems r ather \nthan solving just one problem [23-34]. \nREFERENCES  \n[1] H. Wang, P. Cao, J. Wang, and O. R. Zaiane, â€œUCTransNet: Rethinking the Skip Connections in U-Net from a Channel-\nwise Perspective with Transformer,â€ arXiv Prepr. arXiv2105.05537, 2021, [Online]. Available: http://arxiv.org/abs/2109.04335. \n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n329 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n \n  \n[2] W. Wang, E. Xie, X. Li, and D. -P. Fan, â€œPyramid Vision Transformer: A Versatile Backbone for Dense Prediction \nwithout Convolutions arXiv:2102.12122v2,â€ arXiv:2102.12122v2 [cs.CV], 2021. \n[3] J. Yang, C. Li, P. Zhang, X. Dai, and B. Xiao, â€œFocal Attention for Long -Range Interactions in Vision Transformers,â€ \nNeurIPS (Spotlight). pp. 1â€“21, 2021. \n[4] D. Lu, J. Wang, Z. Zeng, B. Chen, S. Wu, and S.-T. Xia, â€œSwinFGHash: Fine-grained Image Retrieval via Transformer-\nbased Hashing Network,â€ Bmvc. 2021. \n[5] H. Song, D. Sun, S. Chun, and V. Jampani, â€œAn Extendable, Efficient and Effective Transformer -based Object \nDetector,â€ arXiv:2204.07962v1, 2022. \n[6] Z. Liu, Y. Lin, Y. Cao, H. Hu, and Y. Wei, â€œSwin Transformer: Hierarchical Vision Transformer using Shifted Windows \narXiv:2103.14030v2,â€ arXiv:2103.14030v2, 2021. \n[7] L. Wang, R. Li, C. Duan, C. Zhang, and X. Meng, â€œA Novel Transformer based Semantic Segmentation Scheme for \nFine-Resolution Remote Sensing Images,â€ Geosci. Remote Sens. Lett., 2021. \n[8] Y. Gu, Z. Piao, and S. J. Yoo, â€œSTHarDNet: Swin Transformer with HarDNet for MRI Segmentation,â€ Appl. Sci., 2022. \n[9] Z. Liao, N. Fan, and K. Xu, â€œSwin Transformer Assisted Prior Attention Network for Medical Image Segmentation,â€ \nAppl. Sci., 2022. \n[10] J. Liang, J. Cao, G. Sun, and K. Zhang, â€œSwinIR: Image Restoration Using Swin Transformer,â€ arXiv:2108.10257v1, \n2021. \n[11] S. Hao, B. Wu, K. Zhao, and Y. Ye, â€œTwo -Stream Swin Transformer with Differentiable Sobel Operator for Remote \nSensing Image Classification,â€ Remote Sens., 2022. \n[12] A. Hatamizadeh, V. Nath, Y. Tang, D. Yang, H. Roth, and D. Xu, â€œSwin UNETR: Swin Transformers for Semantic \nSegmentation of Brain Tumors in MRI Images.â€ 2022, [Online]. Available: http://arxiv.org/abs/2201.01266.  \n[13] H. Wu, B. Xiao, N. Codella, and M. Liu, â€œCvT: Introducing Convolutions to Vision Transformers Haiping,â€ IEEE, \n2021. \n[14] L. Yan, J. Huang, H. Xie, P. Wei, and Z. Gao, â€œEfficient Depth Fusion Transformer for Aerial Image Semantic \nSegmentation,â€ Remote Sens., 2022. \n[15] Z. Liu et al., â€œVideo Swin Transformer.â€ 2021, [Online]. Available: http://arxiv.org/abs/2106.13230.  \n[16] W. Wang, L. Yao, L. Chen, and B. Lin, â€œCROSSFORMER: A VERSATILE VISION TRANSFORMER HINGING \nON CROSS-SCALE ATTENTION Wenxiao,â€ arXiv:2108.00154v2, 2021. \n[17] J. Xu, X. Sun, Z. Zhang, and G. Zhao, â€œUnderstanding and Improving Layer Normalization Jingjing,â€ Conf. Neural Inf. \nProcess. Syst., 2019. \n[18] J. L. Ba, J. R. Kiros, and G. E. Hinton, â€œLayer Normalization,â€ arXiv:1607.06450v1. 2016. \n[19] I. Tolstikhin et al. , â€œMLP -Mixer: An all -MLP Architecture for Vision.â€ 2021, [Online]. Available: \nhttp://arxiv.org/abs/2105.01601. \n[20] J. Guo, K. Han, H. Wu, and C. Xu, â€œCMT: Convolutional Neural Networks Meet Vision Transformers Jianyuan,â€ \narXiv:2107.06263v2, 2021. \n[21] J. Ahn, J. Hong, J. Ju, and H. Jung, â€œRethinking Query, Key, and Value Embedding in Vision Transformer under Tiny \nModel Constraints,â€ arXiv:2111.10017v1, 2021, [Online]. Available: http://arxiv.org/abs/2111.10017. \n[22] E. In, â€œLmsa: Low-Relation Mutil-Head Self- Attention Mechanism in Visual Transformer,â€ pp. 1â€“11, 2022. \n[23] H. Xiao, K. Rasul, and R. Vollgraf , â€œFashion-MNIST: a Novel Image Dataset for Benchmarking Machine Learning \nAlgorithms,â€ arXiv:1708.07747v2, 2017, [Online]. Available: http://arxiv.org/abs/1708.07747.  \n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n330 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n  \n[24] Mr Hamid M Hasan, Waleed A. Mahmoud Al - Jawher, Majid A Alwan â€œ3 -d face recognition using improved 3d mixed \ntransformâ€ Journal International Journal of Biometrics and Bioinformatics (IJBB), Volume 6, Issue 1, Pages 278 -290, 2012. \n[25] Waleed A. Mahmoud, MS Abdulwahab, HN Al-Taai â€œThe Determination of 3D Multiwavelet Transformâ€ IJCCCE, Volume \n2, Issue 4, pages 28-46 2005. \n[26] Waleed Ameen Mahmoud â€œA Smart Single Matrix Realization of Fast Walidlet Transformâ€ Journal International Journal \nof Research and Reviews in Computer Science, Volume 2, Issue, 1, Pages 144 -151, 2011. \n[27] Abbas Hasan Kattoush, Waleed Ameen Mahmoud Al -Jawher, Osama Q Al -Thahab â€œA radon-multiwavelet based OFDM \nsystem design and simulation under different channel conditionsâ€ Journal of Wireless personal communications, Volume 71, \nPages 857-871, 2013. \n[28] . Hadeel Al -Taai Walid Mahmoud, Mutaz Abdulwahab â€œNew fast method for computing multiwavelet coefficients from \n1D up to 3Dâ€ Proc. 1st Int. Conference on Digital Comm. & Comp. App., Jordan, Pages 412 -2 \n[29] Abbas H Kattoush, Waleed A Mahmoud, Ali Shaheen, Ahed Ghodayyah â€œThe performance of proposed one dimensional \nserial Radon based OFDM system under different channel conditionsâ€ The International Journal of Computers, Systems and \nSignals, Volume 9, Issue 2, Pages 412-422, 2008. \n[30] Walid A Mahmoud, Majed E Alneby, Wael H Zayer â€œ2D -multiwavelet transform 2D -two activation function wavelet \nnetwork-based face recognitionâ€ J. Appl. Sci. Res, vol. 6, issue 8, 1019-1028, 2010. \n[31] Waleed A Mahmoud, MR Shaker â€œ3D Ear Print Authentication using 3D Radon Transformâ€ proceeding of 2nd International \nConference on Information & Communication Technologies, Pages 1052 -1056, 2006. \n[32] Waleed A Mahmoud, Afrah Loay Mohammed Rasheed â€œ3D Image Denoising by Using 3D Multiwaveletâ€ AL-Mustansiriya \nJ. Sci, vol 21, issue 7, pp.  108-136, 2010. \n[33] AHM Al-Heladi, W. A.  Mahmoud, HA Hali, AF Fadhel â€œMultispectral Image Fusion using Walidlet Transformâ€ Advances \nin Modelling and Analysis B, vol 52, issue 1-2, pp. 1-20, 2009. \n[34] W. A. Mahmoud & I.K. Ibraheem â€œImage Denoising Using Stationary Wavelet Transformâ€ Signals, Inf. Patt. Proc. & \nClass., vol 46, issue 4, pp. 1-18, 2003. \nAppendix  \nThe performance of the Swin transform model on (CIFAR-10, CIFAR-100, and MNIST) datasets. \nFigures 26, 27, and 28 show the training and validation classification accuracy, and loss over 80 epochs for CIFAR-10, CIFAR-\n100, and MNIST datasets, respectively. \n \nFigure 26: Training and validation over epoch for CIFAR-10 dataset, (a) accuracy, (b) loss, (epochs=80). \n\n \n \nRasha. A. Dihin,  Ebtesam N. Al Shemmary, Waleed Ameen Al Jawher, 2023.  Implementation Of The Swin Transformer and Its Application In Image \nClassification. Journal port Science Research, 6(4), pp. 318-331. https://doi.org/10.36371/port.2023.4.2         \n331 \nJournal port Science Research \nAvailable online www.jport.co \nVolume 6, No:4. 2023 \n \n  \n \nFigure 27: Training and validation over epoch for CIFAR-100 dataset, (a) accuracy, (b) loss, (epochs=80). \n \nFigure 28: Training and validation over epoch for MNIST dataset, (a) accuracy, (b) loss, (epochs=80). \n \n",
  "topic": "MNIST database",
  "concepts": [
    {
      "name": "MNIST database",
      "score": 0.8595641851425171
    },
    {
      "name": "Transformer",
      "score": 0.7627716064453125
    },
    {
      "name": "Computer science",
      "score": 0.6673674583435059
    },
    {
      "name": "Calculator",
      "score": 0.6416958570480347
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5940684080123901
    },
    {
      "name": "Pixel",
      "score": 0.4384789764881134
    },
    {
      "name": "Computer vision",
      "score": 0.41514843702316284
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.4045775234699249
    },
    {
      "name": "Computer engineering",
      "score": 0.34317126870155334
    },
    {
      "name": "Machine learning",
      "score": 0.3373759984970093
    },
    {
      "name": "Engineering",
      "score": 0.20954015851020813
    },
    {
      "name": "Artificial neural network",
      "score": 0.19233942031860352
    },
    {
      "name": "Voltage",
      "score": 0.18618029356002808
    },
    {
      "name": "Electrical engineering",
      "score": 0.1015951931476593
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ]
}