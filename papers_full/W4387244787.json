{
    "title": "On the Question of Authorship in Large Language Models (LLMs)",
    "url": "https://openalex.org/W4387244787",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2791210400",
            "name": "Carlin Soos",
            "affiliations": [
                "University of California System"
            ]
        },
        {
            "id": "https://openalex.org/A3206320674",
            "name": "Levon Haroutunian",
            "affiliations": [
                "University of Washington"
            ]
        },
        {
            "id": "https://openalex.org/A2791210400",
            "name": "Carlin Soos",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3206320674",
            "name": "Levon Haroutunian",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3034723486",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W3203737321",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3125358881",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4205807230",
        "https://openalex.org/W6769430610",
        "https://openalex.org/W6756972089",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W2887315941",
        "https://openalex.org/W2882319491",
        "https://openalex.org/W4213090966",
        "https://openalex.org/W4323655724",
        "https://openalex.org/W6610489549",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W3095278674",
        "https://openalex.org/W4377823537",
        "https://openalex.org/W1614298861",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W3212464620",
        "https://openalex.org/W2981852735",
        "https://openalex.org/W6662048942",
        "https://openalex.org/W2029606495",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2159983530",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W1566289585",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2979401726",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W3094727374",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W289372308",
        "https://openalex.org/W4251427884",
        "https://openalex.org/W2332013542",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2953356739",
        "https://openalex.org/W4250234291"
    ],
    "abstract": "The adoption of pre-trained large language models (LLMs), like ChatGPT, across an increasingly diverse range of tasks and domains poses significant challenges for authorial attribution and other basic knowledge organization practices. This paper examines the theoretical and practical issues introduced by LLMs and describes how their use erodes the supposedly firm boundaries separating specific works and creators. Building upon the author-as-node framework proposed by Soos and Leazer (2020), we compare works created with and without the use of LLMs; ultimately, we argue that the issues associated with these novel tools are indicative of preexisting limitations within standard entity-relationship models. As the growing popularity of generative AI raises concerns about plagiarism, academic integrity, and intellectual property, we encourage a reevaluation of reductive work/creator associations and advocate for the adoption of a more expansive approach to authorship.",
    "full_text": "Carlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n1 \n \nCarlin Soos – University of California, Los Angeles (UCLA) \nLevon Haroutunian – University of Washington \nOn the Question of Authorship in Large Language \nModels (LLMs) \nAbstract \nThe adoption of pre-trained large language models (LLMs), like ChatGPT, across an increasingly diverse range of tasks \nand domains poses significant challenges for authorial attribution and other basic knowledge organization practices. This \npaper examines the theoretical and practical issues introduced by LLMs and describes how their use erodes the supposedly \nfirm boundaries separating specific works and creators. Building upon the author-as-node framework proposed by Soos \nand Leazer (2020), we compare works created with and without the use of LLMs; ultimately, we argue that the issues \nassociated with these novel tools are indicative of preexisting limitations within standard entity-relationship models. As the \ngrowing popularity of generative AI raises concerns about plagiarism, academic integrity, and intellectual property, we \nencourage a reevaluation of reductive work/creator associations and advocate for the adoption of a more expansive \napproach to authorship.  \n \n1. Introduction \nOpenAI’s release of ChatGPT in November 2022 triggered near-immediate concerns across \ncollege campuses. Perhaps still on guard from a reported rise in cheating attributed to the remote -\ninstruction phase of the COVID-19 pandemic (Jenkins et al. 2022; Dey 2022), administrators and \nfaculty quickly began speculating about widespread misuse of the chatbot. An abundance of news \ncoverage questioning ChatGPT’s ability to “replace humans” (Lock 2022) no doubt exacerbated \nthese anxieties, leading to concerns about “radical consequences for teaching and learning” (Dolan \n2023). As is common with these types of technological innovations, the panic subsided almost as \nquickly as it emerged, leaving in its wake a lingering malaise and ambivalence. Although concerns \npersist about the use of generative AI for cheating, university talking points now strike a balance \nof offensive disciplinary policies and practical recommendations (e.g. University of Washington \n2023; UCLA 2023; University of Wisconsin-Madison 2023).  \nAs the pedagogical value of ChatGPT and its competitors continues to be explored \n(Kasneci et al. 2023), educators are finding new ways to utilize the unique capabilities of these \nlarge language models (LLMs) without compromising the integrity of their clas srooms. While \nsome schools are attempting to outright ban all applications of generative AI, its utilization by \nstudents, staff, and faculty seems inevitable. As distinct entities, ChatGPT-like tools can be too \ntempting to resist, and their allure is only heightened by an increase in social, professional, and \neconomic pressures looming over learners and their instructors. Complicating matters further is \nthe embedding of these models into preexisting information retrieval systems, such as Microsoft’s \nuse of ChatGPT for Bing or Google’s new “collaborative AI service,” Bard (Elias 2023). As the \nline between “chatbot” and “search engine” is further blurred, determining where “researching” \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n2 \n \nends and “cheating” begins will likely become increasingly more difficult. Viewed from this \nperspective, initial warnings about how ChatGPT will “upend longstanding concepts of \nplagiarism, authorship, ownership, and learning” (McCarthy 2023) are not entire ly unfounded. \nHowever, with these new challenges comes an opportunity to revisit each of these concepts, \nquestion our preexisting assumptions about their conceptual validity, and develop new \nperspectives to match the current moment.  \n \n1.1. Paper Goals and Structure \nThis project seeks to address the implications of LLMs for authorial attribution and other \nknowledge organization (KO) practices. In the following section, we provide a technical overview \nof LLMs and introduce relevant literature from the field of natural language processing (NLP). \nNext, we review theories of authorship within KO, focusing primarily on Soos and Leazer’s \nconcept of the “author-as-node” (2020). Building upon this network theory, we proceed to discuss \nthe various authorship issues introduced by generative AI and describe how the nature of pre-\ntrained models—as well as their creative outputs—complicate the supposedly firm boundaries \nseparating specific works and creators. With these considerations in mind, we expand the author -\nas-node framework using the concept of “communicative intent” (Bender and Gebru et al. 2021). \nTo conclude, we reiterate and reaffirm previously acknowledged concerns about “the author” as a \ndistinct categorical entity while maintaining the importance of idea attribution for  personal \ndevelopment and community accountability.  \n \n2. Technical Overview of LLMs for KO  \nLanguage modeling is the task of computationally representing how humans use language. In \npractice, language models typically predict and generate a sequence of words given another \nsequence as context. These models are a useful component of nearly every ki nd of NLP system, \nfrom automatic speech recognition to machine translation to natural language generation.  \nLanguage models based on neural networks are far and away the most common types \nused today. The simplest type of neural language model is a feed-forward neural network (Bengio \net al. 2003), which is composed of a number of layers containing sets of units t ypically referred to \nas “neurons.” The first layer is an embedding layer, which converts the individual words of an \ninput into vectors of numeric values. Every unit—or neuron—of this embedding vector is \nconnected to every neuron in the next layer through a weight and a bias value. The first stage of \ncomputation applies those weights and biases to the initial vector; a nonlinear function (such as a \nsigmoid function) is then applied to the initial vector to determine the values of each neuron in the \nsecond layer. The neurons in the second layer are similarly connected to the neurons of the third \nlayer, and so on. More complex types of neural models incorporate different types of connections \nbetween neurons, which are necessary to account for the sequential nature of text data. Weights \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n3 \n \nand biases are generally referred to as “parameters,” and a model’s size is usually described by its \nnumber of parameters. \nDuring training,1 neural language models are optimized on token prediction tasks, where \nthey must predict output text based on input text. Input text is first split into a sequence of tokens, \nwhich are typically words or sub-word pieces. These tokens are then mapped to corresponding \nvectors, which are then run through the matrices that comprise the model’s parameters. The output \nof this computation is another sequence of vectors. This sequence of vectors can then be compared \nagainst the expected output, which is tokenized and mapped to a sequence of vectors in the same \nmanner the input was. The result of this comparison is a loss score, which determines the degree to \nwhich the model output differs from the expected output. Using the Chain Rule from multivariable \ncalculus, the training routine updates the model’s parameters in a way that reduces the loss score; \nin other words, the parameters are modified to push the output closer to the expected output. This \nprocess repeats for every input/output pair in the training data. Typ ically, training concludes after \nmany full passes (called “epochs”) over the training data. \nNeural language model training results in what is referred to as a parametric memory: \nlanguage models distill and “memorize” their training data in their parameters to produce output \nthat aligns with the observed data patterns. This parametric memory is th e sum total of the \n“knowledge” that a language model has; after training, a language model has no access to its \ntraining data or information from any other source. \nOnce a language model is trained, one can produce output from it by taking an input text, \nsplitting that text into a sequence of tokens, mapping those tokens into corresponding vectors, and \nrunning those vectors through the model’s parameters. The model produces output by iteratively \npredicting the vector of the next token in the sequence; in other words, it predicts the most likely \ncontinuation of the input, based on the information stored in its parametric memory.  \n \n2.1. Scaling Up: The Birth of Large Language Models \nIn 2017 (Vaswani et al.), the advent of a specialized type of neural network, called a Transformer, \ngave rise to a new era in language modeling. One of the first examples of a Large Language \nModel is BERT,  a Transformer-based language model that advanced the state of the art on many \ncommon NLP benchmark tasks (Devlin et al. 2019). \nThe shift to Transformer-based language models marked an increase in both the size of \nmodels and the data used to train them. BERT has approximately 110 million parameters —which \nis relatively massive compared to its contemporaries—and a similarly large training corpus: \nEnglish Wikipedia, which included 2.5 billion words in the version the authors used; and \nBookCorpus (Zhu et al. 2015), which contained around 800 million words pulled from \n \n1 This description is a simplified explanation of a typical training routine for a neural language \nmodel. For a more detailed overview, see Jurafsky and Martin (2023).  \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n4 \n \napproximately 11,000 unpublished books scraped from Smashwords. Following BERT was a \nflood of pre-trained language models, with the notable examples of ERNIE (Zhang et al. 2019), \nGPT-2 (Radford et al. 2019), XLNet (Yang et al. 2019), BART (Lewis et al. 202 0), T5 (Raffel et \nal. 2020), and GPT-3 (Brown et al. 2020).  \nThe creation of GPT-3 in 2020 marked the apex of increases to model size; it has a \nwhopping 175 billion parameters, almost 1,600 times larger than BERT. GPT -3’s gigantic scale \ncame along with, of course, a gigantic training set, which includes English Wiki pedia and \nBookCorpus along with the CommonCrawl dataset, which is a web crawl dataset consisting of the \ntext from billions of web pages. Like BERT before it, GPT-3 showed impressive performance \ngains on a variety of NLP tasks.  \nGPT-3 also marked the beginning of a new LLM paradigm. Previous LLMs were usually \nnot directly applied to specific tasks of interest; instead, researchers would download a pre -trained \nlanguage model like BERT and train its parameters further on a smaller set of task-specific data—\na process known as fine-tuning. Because GPT-3 was released closed-source, its users could not \nsimply download the model and train it further. However, GPT-3 achieved impressive \nperformance without being fine-tuned, through a method called in-context learning (ICL; Brown et \nal. 2020). To apply ICL, a user supplies a “prompt” to the model that includes a handful of in -\ncontext demonstrations (e.g. a few examples of English sentences paired with their French \ntranslations) along with their input to the model (e.g. a new English sentence), and the model is \nexpected to produce output in format given by the demonstrations (e.g. the French translation of \nthe input). In this way, GPT-3 functions as a general-purpose LLM; it is intended to be used on a \nwide variety of tasks, with no need (or option) to customize it. \n \n2.2. Data \nFor the reasons described above, massive corpora are a necessity to creating large language \nmodels: neural language models get their power from their parametric memory, and their \nparametric memory comes from the data the models ingest during training. Unfo rtunately, the \nsheer size of these corpora means that researchers who create them or use them cannot be fully \naware of what they contain (Paullada et al. 2020). The opacity of many of these large datasets is \ndue to what Bender and Gebru et al. (2021) call “documentation debt,” which is “a situation where \nthe datasets are both undocumented and too large to document post hoc” (615).  Numerous audits \nof large machine learning datasets have found that they contain non-trivial amounts of unwanted \ncontent (Dodge et al. 2021), copyright violations (Bandy and Vincent 2021), sexually explicit \nmaterial (Birhane et al., 2021), and hate speech (Gehman et al. 2020). For a more detailed critique \nof practices surrounding the collection and use of machine learning datasets, see Paullada et al. \n(2020). \nTake for example CommonCrawl, which is one of the datasets used to train GPT -3 and \nits successors. CommonCrawl is an effort by The Common Crawl Foundation to “[democratize] \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n5 \n \naccess to web information by producing and maintaining an open repository of web crawl data” \n(Common Crawl n.d.). As of April 2023, CommonCrawl contains 3.1 billion web pages (Nagel \n2023). An analysis by Luccioni and Viviano (2021) found that around 5% of the web pages \nincluded in CommonCrawl contain hate speech and slurs. There have been many efforts to filter \nCommonCrawl (most notably, C4; Raffel et al. 2020), including by Brown et al. (2020) in their \ncreation of GPT-3. However, it is virtually impossible to comprehensively filter or audit a data set \non the scale of CommonCrawl. Additionally, as Bender and Gebru et al. (2021) point out, the \nnature of Internet data means that datasets like CommonCrawl necessarily overrepresent the \nvoices of young, male Internet users in developed countries at the expense of marginalized people. \nWith language models as large and complex as GPT-3, it can be difficult to conceptualize \nthe links between the data it was trained on and the output it produces. However, an understanding \nof the training data used to train an LLM should be in the foreground of any attempts to determine \nthe source of its output. \n \n2.3. Where we are now: ChatGPT \nMost of OpenAI’s current state-of-the-art models are direct descendants of GPT-3,2 or more \nspecifically, of InstructGPT (Ouyang et al. 2022). What differentiates InstructGPT from the initial \nversion of GPT-3 is mainly two new phases of training called instruction tuning and reinforcement \nlearning from human feedback.  \nInstruction tuning is an extension of pre-training in which the model is trained on a \ndataset consisting of a set of instructions (such as writing prompts or math problems) and answers \nthat satisfy those instructions. The motivation behind this training is that it aligns the model to its \nlikely downstream usage: users will prompt the model with a description of the output that they \nwant, with the expectation that the model will provide a response conforming to their \nspecifications (ibid). To train InstructGPT, OpenAI collected a set of instructions and answers \nfrom human labelers, including users of GPT-3 and paid contractors (ibid). The resulting dataset \nhas not been released. Reinforcement Learning from Human Feedback (RLHF) is a stage of \ntraining following pre-training and instruction tuning, which may even continue once the model is \ndeployed (as is the case with ChatGPT (OpenAI 2022)). In this phase of training, human judges \nare presented with multiple model outputs for the same prompt and asked to rank t hem in order of \nquality (ibid). After being trained on this feedback, the model is more likely to produce output \nsimilar to the higher rated examples. \nThe motivation for using RLHF is what OpenAI has described as their aim “to make \nartificial general intelligence (AGI) aligned with human values and follow human intent” (Leike \n \n2 The exception to this is GPT-4, which is generally assumed to have far more parameters than \nthan the GPT-3 class. The exact number of parameters has not been released, but OpenAI CEO \nSam Altman has strongly suggested that it has fewer than 100 trillion (V incent 2023). \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n6 \n \n2022).  In practice, Ouyang et al. accomplish this by “[having] labelers evaluate whether an output \nis inappropriate in the context of a customer assistant, denigrates a protected class, or contains \nsexual or violent content” (2022, 10). \n \n2.4. Differences Between Human Language Production and LLM Text Generation \nChatGPT and its ilk are undoubtedly impressive technological feats. After a brief interaction with \nOpenAI’s chatbot, many users are surprised by its apparent mastery of the English language. \nHowever, Bender and Gebru et al. (2021) provide a cautionary reminder for interpreting LLM-\ngenerated text: \"coherence [is] in the eye of the beholder\" (616). LLMs might appear to \nunderstand human language and produce meaningful output in return, but that meaning is not \ncreated by the LLMs themselves—it is created by their human interlocutors (Bender and Koller \n2020).  \nHuman communication \"takes place between individuals who share common ground and \nare mutually aware of that sharing (and its extent), who have communicative intents which they \nuse language to convey, and who model each others’ mental states as they communi cate\" (Bender \nand Gebru et al., 2021, 616). Language models, having no experience of the world beyond the \ntokens in their training data, do not share common ground with their human users, nor do they \nhave communicative intent or mental states. When a person reads text generated by an LLM, it \nmay seem as though there is thought and intent behind the response. This is not the case, as it \nsimply isn't possible for an LLM to have thought or intent, and the illusion of communication \ncomes from our own human linguistic capabilities: \"our perception of natural language text, \nregardless of how it was generated, is mediated by our own linguistic competence and our \npredisposition to interpret communicative acts as conveying coherent meaning and intent, whether \nor not they do\" (ibid, 616).  \n \n3. Authorship Theory in KO \nLLMs process trillions of forms and learn to recognize statistically significant patterns in their \nusage, but this is not the same thing as understanding their meaning (Saussure 1959). Just as a \ncopy of Wuthering Heights is a representation of Emily Brontë's work and not the work itself, the \nforms used to pre-train an LLM are not intrinsically meaningful.  \n \n3.1. The Conceptual Structure of A Work \nSmiraglia explains that “works are core narratives in every part of human experience —from \nsacred texts to legal foundations to iconic structures to iconic novels” (2019, 311). While we tend \nto engage with these “mentefacts” (Gnoli 2018), or mental constructs, through physical artifacts, \n“a work is abstract at every level, from its creator’s conception of it, to its reception and inherence \nby its consumers” (Smiraglia 2019, 310). From an information retrieval perspective, these \nconceptual problems are typically circumvented through a fixation on the item-level object. \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n7 \n \nHypothetically speaking, identifying the title of a specific bibliographic object is a straightforward \nactivity. From there, assigning the author should be similarly easy.  \nWhile nice in theory, there are at least two factors that complicate the description of \nlinear author-work association. \n \n1. Different manifestations of a particular work can exhibit significant deviations from \nthe original expression.  \n2. Since works are abstract concepts, determining where the boundary of one ends and \nanother begins is a complex perceptual activity.  \n \nTo the first point, take Wuthering Heights. According to Resource Description and Access (RDA) \nguidelines, all versions of the novel are to be collocated under the same nominal authorized access \npoint (AAP) associated with the original manuscript: Emily Brontë. Editions published in 1848 \nand 1948 will likely have different covers and exhibit cosmetic editorial differences, but, by and \nlarge, few would deny both are versions of the same work. But how much can be changed before \nthe item is no longer Wuthering Heights? For example, under the entity-relationship model at the \ncore of the Functional Requirements for Bibliographic Records (FRBR), translations of a work \nshould be primarily associated with the original author. This means a Hebrew edition of the text \nwill be attributed to Brontë even though, at the time she penned her novel, the language had yet to \nbe revitalized and adapted for general use.  \nOn the one hand, this Hebrew translation will hopefully preserve the abstract work \nconcept intended by Brontë; as such, her creative labor deserves recognition. On the other hand, \nusing her name as the primary AAP “inevitably devalues the role of the trans lator and ignores the \ncreative license and labor required in the translation process” (Soos and Leazer 2020, 486). \nTranslating is not a one-to-one process in which individual words are simply swapped for identical \nones of another language. A talented translator will exhibit fluency in the source and target \nlanguages, possess a deep knowledge of the particular work, and utilize various linguistic tools to \narticulate its essence. So while the goal is the maintain both the semantic and affective qualities \nevoked by Brontë, a translator’s unique choices can severely alter a reader’s experience of her \nwork.  \n To the second point, as abstract concepts, works are subject to the same influences and \nfactors that impact all perceptive activities. While writing her book, Brontë built on her own \nexperiences—the things she read, the people she knew, the social context in which she lived—to \ncreate something new. Similarly, when a person is reading Wuthering Heights, their understanding \nof her text is inevitably influenced by their own unique experiences, and, having now interacted \nwith Brontë’s work, it is difficult to know how her ideas might impact their own creative \nproduction. In some kind of authorial butterfly effect, composer Jim Steinman might not have \nwritten “It's All Coming Back to Me Now” (popularized by Celine Dion) having not been inspired \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n8 \n \nby his own reading of Wuthering Heights. Still, his song is unanimously viewed as a distinct work \nexternal to the original text. \nWithin the FRBR model, the concept of a “super work” (Svenonius 2009, 38) seeks to \nsituate derivative works, like Steinman’s, as “ideational nodes within the set” (Smiraglia 2019, \n313). An influential “progenitor work” (Smiraglia 2007, 182), such as Wuthering Heights, can be \nviewed as a primary connective node within an “instantiation” (ibid) or “textual identity” (Leazer \nand Furner 1999) network, but it is intentionally positioned adjacent to the works it inspired. Yet \neven within these more robust webs of relationships, there exists the problem of determining \nwhere one work ends and another begins. Smiraglia arguably resolves this issue with his definition \nof a work, which he defines as “a deliberately created informing entity intended for \ncommunication” (2019, 308). “Deliberately” is the key term here, as the creator’s intention to \nproduce something distinct from the progenitor marks the beginning of a new work entity. This is \nan view supported in other disciplines, where creative genres like the readymade  and the parody \nuse a person’s ideation and intentionality to distinguish influence from theft.  \n \n3.2. Influence and Intention \nQuests for originality and authenticity can be equally liberatory as they are oppressive. While \nthere is undeniable value in personal expression, pressures that tie a person’s worth —be it \neconomic, professional, or social—to the originality of their creative output forces them to view \ntheir peers as competition rather than collaborators. In The Anxiety of Influence, Bloom argues that \nwriters are both limited and motivated by this desire to distinguish themselves from their \npredecessors,  \n \nFor the poet is condemned to learn his profoundest \nyearnings through an awareness of other selves. The \npoem is within him, yet he experiences the shame and \nsplendor of being found by poems —great poems —\noutside him. To lose freedom in this center is never to  \nforgive, and to learn the dread of threatened autonomy \nforever. (Bloom 1997, 26) \n \nIn an act of kenosis, the author seeks “discontinuity with the precursor” (ibid, 14), a response that \nparadoxically concedes power to the other’s influence. Moving away from something is as much a \nresponse as moving towards, and in rejecting the progenitor work a writer simply  reaffirms their \nplace within the creative continuum.  \nAlthough Bloom constructed his theory around poetic networks, the anxiety of influence \ntranscends genre and medium to gesture towards a broader humanistic desire for self -actualization. \nWhile this tendency is not inherently bad, the judgment of a work base d on its intellectual purity \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n9 \n \nsets a standard of originality almost impossible to achieve. Authors think and create surrounded by \nthe works of others, not within sterile incubators free from outside influence. So when the ultimate \ntest of intellectual autonomy rests upon someone’s ability to produce innovative work—poetic or \notherwise—completely detached from the works of others, anxiety is an entirely reasonable \nresponse to this unachievable expectation.  \nBuilding from Bloom and Foucault (1977), Soos and Leazer suggest that the author “as a \nlone and entirely detached figure simply does not exist,” arguing instead that “the complex nature \nof intellectual and creative production makes it impossible to draw a clear and distinct boundary \naround a particular work and attribute it to one unique individual” (2020, 487). Rather than \nviewing authors as “owners” of an idea, they suggest that an “author -as-node” approach better \npreserves the inherently collaborative nature of creative production. Just as work-based \ninstantiation networks connect individual items through a unifying progenitor node, this model \npositions an author as a singular entity within a sea of influential relationships.  \nThat being said, even Bloom rejected the claim that “no one ever had or ever will have a \nself of his or her own” as nothing more than an “unamiable fiction” (1997, xlvi). Yes, works are \ncreated within complex intellectual ecosystems, but, as individuals, the people that produce them \nhave unique perspectives and talents worthy of recognition. To borrow Smiraglia’s word, they \nhave intentionality.  \nAny KO theory of authorship inevitably reaches a seemingly contradictory impasse: \npeople are unique individuals with unique ideas and unique intentions—and, at the same time, \nthey are products of their environments. Authors are influenced by those who came  before them, \nthe people who inspire them, and the community that cares for them, but they also offer an \nessential quality that only they can provide. While nuanced discourse can simultaneously hold the \nimportance of relationality (Littletree, Belarde-Lewis, and Duarte 2020) and individuality, notions \nof authorship conveyed through standard ontological frameworks generally fail to capture this \nduality. FRBR extends authorship beyond individual persons to include families and corporate \nbodies, and the replacement of “author” with “contributor” in RDA perhaps better gestures to the \nexpansive nature of the work creation. However, the use of standardized AAPs in author \nattribution still removes a person, family, or corporate body from their broader context. In d oing \nso, we are essentially suggesting that influence is secondary to the intention it yields.  \nAlthough epistemically valuable, these influence networks are often too complex and \nmessy to visually represent through a basic KOS. At the end of the day, a student probably just \nwants to find Wuthering Heights in the stacks and finish their assignment, and they will likely do \nso by searching for “Emily Brontë,” not “Jim Steinman.” These authorial networks might help the \nuser contextualize Brontë’s work, but this is not typically the primary goal of most catalogs .  \nYet while presenting authors as “owners” of a work is the reasonable choice given user -\nwarranted practices, doing so defends particular ontological commitments that hide the social, \ncultural, economic, and professional “complexities that affect the product ion of new objects and \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n10 \n \nideas” (Soos and Leazer 2020, 486). And the consequences of these decisions extend far beyond \nany one user’s search query.  \n \n4. The Authorship of LLM Content \nMost universities have some kind of academic integrity policy. Cheating and other forms of \nacademic dishonesty are of primary concern, with plagiarism being one of the most vehemently \ncondemned. Learning to find, interpret, and cite sources are core skills needed for academic \nsuccess, and plagiarism—a spectrum of actions that ranges from an uncited paraphrase to the \nwholesale appropriation of another student’s writing—is largely viewed as antithetical to the ethos \nof the academy.  \nPlagiarism occurs when “somebody presents the work of others (data, words or theories) \nas if they were his/her own and without proper acknowledgment” (Wager and Kleinert 2012, 167). \nUnder the authorship concepts defended by RDA and FRBR, avoiding accusatio ns of plagiarism \nappears to be a straightforward task: you only need to indicate when you are referring to another \nperson’s work and never suggest their ideas are your own. Simple enough. We can debate the \nconceptual boundaries of works and authors, but, using the attribution protocols generally \naccepted across higher education, plagiarism is framed as an avoidable issue.  \nThe broader adoption of generative AI has revealed the limitations of this approach. \nFollowing the relatively quick adoption of ChatGPT by students and staff, many institutions \nformally declared the use of pre-trained language models to produce or enhance one’s work to be a \nviolation of academic integrity. Based on the above definition, asking ChatGPT to write your \nWuthering Heights essay seems to be a clear-cut case of plagiarism; the student did not produce \nthe content and is presenting it “as if they were his/her own and without proper acknowledgment.” \nBut who, or what, is being plagiarized?  \n \n4.1. Communicative Intent and Work Creation \nOpenAI has done a wonderful job of developing an application that appears to possess so -called \n“general intelligence.” But, as previously noted, while ChatGPT’s “human -like” responses can be \nquite convincing, the chatbot does not understand what it is saying—at least not in the typical \nsense in which people use those words. It also does not answer user queries in an intentional act of \ncommunication—again, at least not in the way implied by such a claim.  \nThat this lack of “communicative intent” (Bender and Gebru et al. 2021) marks the \nfundamental distinction between the way humans and LLMs utilize language. Within the context \nof Smiraglia’s definition, this inability to experience or express intention essentially disqualifies \nChatGPT from being able to produce a work. So, although a language model is capable of \nproducing information, it cannot make a work. Absent a work, it cannot be a victim of plagiarism.  \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n11 \n \n4.2. User Queries & Feedback \nThe model itself may be incapable of intentional action, but there are myriad other associated \nparties who are. The most obvious is the accused student.  \nFor all intent and purpose, there is nothing technically preventing this person from being \nnamed the creator of the Wuthering Heights essay. Entering a query into ChatGPT, copying the \ntext into a new document, adding their name, and submitting the file are all intentional acts \nfocused on recording and expressing a particular viewpoint. Sure, the student did not fabricate a \nmajority of the text, but the essay was deliberately created using their actions, knowledge, and \ncapabilities. \n \n4.3. Training Data  \nThis appears to be a victimless crime until one considers the broader context. The plethora of data \nused to train an LLM directly supports the parameters it uses to generate new responses. ChatGPT \nmay be incapable of “understanding,” but the millions of authors responsible for its immense \ntraining set probably are. Although they did not personally write the exact words used in this exact \nessay, the collective can be viewed as a “family or corporate body” responsible for this immense \nnetwork of data. Following this logic, one could argue that the generated essay paraphrases this \ncorpus of material, making the members of this family/corporate body targets of plagiarism.  \nWell, it’s an answer. But, as Dehouche argues, an accusation of plagiarism “appears \nrather inadequate when the ‘others’ in question consist in an astronomical number of authors, \nwhose work was combined and reformulated in unique ways” (2021, 21). While tho se individuals \nintentionally created the material that was used to train ChatGPT, and while they offer a wonderful \nmetaphor for how textual identity networks function, OpenAI was actually the one that developed \nthe GPT model that made the Wuthering Heights essay possible. \n In an interesting turn, OpenAI can now either be viewed as the victim of plagiarism (by \nthe student) or a perpetrator (towards the family/corporate body). Both the code used to create \nChatGPT and the parametric memory defined during its training are proprietary works \nintentionally created by those at OpenAI3. As the student failed to cite either, that can be viewed \nas an act of plagiarism. At the same time, ChatGPT’s parametric memory was constructed from \nbillions of other works that cannot be cited. Whether that memory constitutes a work on its own is \nanother matter entirely.  \n \n5. Plagiarism Revisited \nAll creative acts are forms of collaboration. New ideas and works develop within a broader social \ncontext that directly and indirectly contributes to their production, and any single author is but one \n \n3 Though, as we describe in Section 2.3, the datasets used to train ChatGPT–the sources of its \nparametric memory–largely are not the sole property of OpenAI. \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n12 \n \nnode in a vast network of influence. The ambiguous boundaries between specific authors and \nworks are further eroded by the innately diffused nature of LLMs.  \nOur failure to accommodate this generative content within preexisting notions of \nplagiarism reveals the conceptual limitations of an author-as-owner approach and highlights the \nimportance of networked attribution. “Plagiarism” is a semantic category that a llows for varying \ndegrees of membership. Its prototypical examples—for example, paying another person to write \nyour college assignment—generally support the validity of linear work-author relationships and, \ntherefore, reaffirm the validity of the class. However, the “internal structure” of this category \n(Rosch 1975) is much more stratified than standard use of the term suggests. The ambiguous \nnature of LLM-generated works just presents a more obvious challenge to the seemingly stable \nconcept. \nWhile fooder for an interesting philosophical discussion, we think debating whether \nChatGPT’s Wuthering Heights essay is an example of plagiarism—or a component of a bigger \nplagiarism racket—largely circumvents and obscures the actual issue. When real humans are being \nobviously plagiarized, holding the culprit responsible is often viewed as a way of minimizing the \nharm caused to this other party. But when the “other” is unidentifiable, what harm is being \ncaused? Why are so many people upset by the thought of a student getting an “A” on an essay \nproduced by an LLM?  \nPlagiarism is perhaps best viewed as an attempt to standardize a prescriptive claim about \nintellectual morality. In higher education, “plagiarism evokes deeply held emotions related to \ndeviance, credibility, and what it means to be outside the norm” (Rooks by quoted in McCarthy \n2023, 4). So even when a particular victim may be difficult to identify, submitting an essay you \ndid not write undermines the core tenets of an academic meritocracy: you should be assessed \nbased on what you know and how well you can articulate that knowledge. Yet this protective \nbarrier around “what you know” is deceptively precarious. Removed from the author -as-owner \nparadigm, the concept is nearly impossible to enforce.  \nTo be clear, this claim is not intended to defend violations of student conduct codes or \nrefute the importance of intellectual honesty. Quite the contrary. Simply asking people to not \n“steal” the “property” of others (i.e. their works and ideas) is a low bar that prevents us from \nhaving deeper discussions about what it means to think and live in relation to others. We should \ndemand more of those within a learning community, and reconsidering our views of solitary \nauthors with wholly distinct ideas provides an opportunity to explicitly acknowledge our reliance \non one another. When work production is reframed as a community activity rather than the mark \nof independent genius, the harm of plagiarism is no longer reduced to a localized interpersonal \nevent. \n \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n13 \n \n6. Conclusion: Reframing Accountability  \nThe moralization of technology is rarely beneficial. ChatGPT is a powerful tool with a number of \npromising pedagogical uses—at the same time, it can also facilitate non-learning and perpetuate \nharmful educational practices. The matriculation of LLM tools into different domains will \ncontinue to reveal possible benefits and risks, and our affective responses to these mis/applications \nshould be viewed as indicators of unfulfilled values.  \nUpset over the “plagiarizing” of ChatGPT’s content suggests a yearning for individual \nresponsibility and community accountability. Authorship is used to “confer credit” for a job well \ndone, but the connection between individuals and ideas additionally ensures “authors understand \ntheir role in taking responsibility and being accountable for what is published” (ICMJE 2023). The \nopaque webs of influence central to LLM writing tools complicate our ability to assign \nresponsibility and, consequently, challenge what it means to be held accountable to both oneself \nand one’s community. As the discussions prompted by these new technologies lead us to reflect on \nour values, we are provided with a meaningful opportunity to reaffirm those congruent with our \nviews and replace the ones that no longer serve us.  \n \nReferences \nBandy, John, and Nicholas Vincent. 2021. “Addressing ‘Documentation Debt’ in Machine \nLearning: A Retrospective Datasheet for BookCorpus.” In Proceedings of the Neural \nInformation Processing Systems Track on Datasets and Benchmarks 1. \nBender, Emily M., Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. \n“On the Dangers of Stochastic Parrots: Can Language Models Be Too Big?       .” In \nProceedings of the 2021 ACM Conference on Fairness, Accountability, and \nTransparency (FAccT ’21): 610-623. Online: ACM. \nBender, Emily M. and Alexander Koller. 2020. “Climbing towards NLU: On Meaning, Form, and \nUnderstanding in the Age of Data.” In Proceedings of the 58th Annual Meeting of the \nAssociation for Computational Linguistics, 5185–98. Online: Association for \nComputational Linguistics. doi:10.18653/v1/2020.acl-main.463. \nBengio, Yoshua, Réjean Ducharme, Pascal Vincent, and Christian Janvin. 2003. “A Neural \nProbabilistic Language Model.” Journal of Machine Learning Research 3: 1137-1155. \nBirhane, Abeba, Vinay Uday Prabhu, and Emmanuel Kahembwe. “Multimodal datasets: \nmisogyny, pornography, and malignant stereotypes.” Preprint, submitted October 2021. \nhttps://arxiv.org/abs/2110.01963. \nBloom, Harold. 1997. The Anxiety of Influence. 2nd ed. Oxford: Oxford University Press.  \nBrown, Tom B., Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, \nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n14 \n \nAriel Herbert-Voss, Gretchen Krueger, T. J. Henighan, Rewon Child, Aditya Ramesh, \nDaniel M. Ziegler, Jeff Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, \nMateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam \nMcCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. “Language Models are \nFew-Shot Learners.” Preprint, submitted July 2020. https://arxiv.org/abs/2005.14165. \nCommon Crawl. N.d. “About.” https://commoncrawl.org/about/. \nDehouche, Nassim. 2021. “Plagiarism in the age of massive Generative Pre -trained Transformers \n(GPT-3).” Ethics In Science And Environmental Politics 21: 17-23. doi: \n10.3354/esep00195. \nDey, Sneha. 2021. “Reports Of Cheating At Colleges Soar During The Pandemic.” NPR. \nhttps://www.npr.org/2021/08/27/1031255390/reports-of-cheating-at-colleges-soar-\nduring-the-pandemic. \nDevlin, Jacob, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. “BERT: Pre-Training of \nDeep Bidirectional Transformers for Language Understanding.” In Proceedings of the \n2019 Conference of the North American Chapter of the Association for Computational \nLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers) , 4171–\n86. Minneapolis, Minnesota: Association for Computational Linguistics, 2019. doi: \n10.18653/v1/N19-1423. \nDodge, Jesse, Maarten Sap, Ana Marasović, William Agnew, Gabriel Ilharco, Dirk Groeneveld, \nMargaret Mitchell, and Matt Gardner. “Documenting Large Webtext Corpora: A Case \nStudy on the Colossal Clean Crawled Corpus.” In Proceedings of the 2021 Conference on \nEmpirical Methods in Natural Language Processing, 1286–1305. Online and Punta \nCana, Dominican Republic: Association for Computational Linguistics, 2021. \ndoi:10.18653/v1/2021.emnlp-main.98. \nDolan, Jill. 2023. “Guidance on AI/ChatGPT: Memo to All Teaching Faculty - January 25, 2023.” \nThe McGraw Center for Teaching and Learning, Princeton University. \nhttps://mcgraw.princeton.edu/guidance-aichatgpt. \nElias, Jennifer. 2023. “Google execs tell employees in testy all-hands meeting that Bard A.I. isn’t \njust about search.” CNBC. https://www.cnbc.com/2023/03/03/google-execs-say-in-all-\nhands-meeting-bard-ai-isnt-all-for-search-.html. \nFirth, John Rupert. “A Synopsis of Linguistic Theory, 1930-1955.” In Studies in Linguistic \nAnalysis, 1-31. Oxford: Blackwell, 1957. \nFoucault, Michel. 1977. “What Is an Author?” In Language, Counter-Memory, Practice: Selected \nEssays and Interviews, ed. Donald F. Bouchard. Ithaca: Cornell University Press, 113 -38.  \nGehman, Samuel, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A. Smith. \n‘RealToxicityPrompts: Evaluating Neural Toxic Degeneration in Language Models’. \n2020. In Findings of the Association for Computational Linguistics: EMNLP 2020, 3356–\nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n15 \n \n69. Online: Association for Computational Linguistics, 2020. doi: \n10.18653/v1/2020.findings-emnlp.301. \nGnoli, Claudio. 2018. “Mentefacts as a missing level in theory of information science.” Journal of \nDocumentation 74(6): 1226-1242. doi: 10.1108/JD-04-2018-0054. \nHarris, Zellig. 1954. “Distributional Structure.” In WORD 10(2-3): 146-162. doi: \n10.1080/00437956.1954.11659520. \nInternational Committee of Medical Journal Editors (ICMJE). 2023. “Defining the Role of \nAuthors and Contributors: Why Authorship Matters.” \nhttps://www.icmje.org/recommendations/browse/roles-and-responsibilities/defining-the-\nrole-of-authors-and-contributors.html. \nJenkins, Baylee D., Jonathan M. Golding, Alexis M. Le Grand, Mary M. Levi, and Andrea M. \nPals. 2022. When Opportunity Knocks: College Students’ Cheating Amid the COVID -19 \nPandemic. Teaching of Psychology 0(0). doi: 10.1177/00986283211059067. \nJurafsky, Dan and James H. Martin. 2023. “Neural Networks and Neural Language Models.” In \nSpeech and Language Processing, 3rd edition. Preprint, submitted January 2023. \nhttps://web.stanford.edu/~jurafsky/slp3/. \nKasneci, Enkelejda, Kathrin Sessler, Stefan Küchemann, Maria Bannert, Daryna Dementieva, \nFrank Fischer, Urs Gasser, Georg Groh, Stephan Günnemann, Eyke Hüllermeier, Stepha \nKrusche, Gitta Kutyniok, Tilman Michaeli, Claudia Nerdel, Jürgen Pfeffer, Oleksandra  \nPoquet, Michael Sailer, Albrecht Schmidt, Tina Seidel, Matthias Stadler, Jochen Weller, \nJochen Kuhn, and Gjergji Kasneci. 2023. “ChatGPT for good? On opportunities and \nchallenges of large language models for education.” Learning and Individual Differences \n103. doi: 10.1016/j.lindif.2023.102274. \nLeazer, Gregory H. and Jonathan Furner. 1999. “Topological Indices of Textual Identity \nNetworks.” In Proceedings of the 62nd Annual Meeting of the American Society for \nInformation Science, 1999, ed. L. Woods. Medford, NJ: Information Today, 345-58. \nLeike, Jan, John Schulman, and Jeffrey Wu. 2022. “Our approach to alignment research.” OpenAi. \nhttps://openai.com/blog/our-approach-to-alignment-research. \nLewis, Mike, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer \nLevy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. “BART: Denoising Sequence -to-\nSequence Pre-Training for Natural Language Generation, Translation, and \nComprehension.” In Proceedings of the 58th Annual Meeting of the Association for \nComputational Linguistics, 7871–80. Online: Association for Computational Linguistics, \n2020. doi: 10.18653/v1/2020.acl-main.703. \nLittletree, Sandra, Miranda Belarde-Lewis, and Marisa Duarte. 2020. “Centering Relationality: A \nConceptual Model to Advance Indigenous Knowledge Organization Practices.” \nKnowledge Organization 47(5): 410-426. doi:10.5771/0943-7444-2020-5-410. \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n16 \n \nLock, Samantha. 2022. “What is AI chatbot phenomenon ChatGPT and could it replace humans?” \nThe Guardian. https://www.theguardian.com/technology/2022/dec/05/what-is-ai-chatbot-\nphenomenon-chatgpt-and-could-it-replace-humans. \nMcCarthy, Claudine. 2023. “ChatGPT use could change views on academic misconduct.” Dean & \nProvost 24(10): 1-4. doi: 10.1002/dap.31202. \nMikolov, Tomas, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. “Efficient Estimation of \nWord Representations in Vector Space.” International Conference on Learning \nRepresentations, 2013. \nNagel, Sebastian. 2023. “March/April 2023 crawl archive now available.” Common Crawl. \nhttps://commoncrawl.org/2023/04/mar-apr-2023-crawl-archive-now-available/. \nOpenAI. 2022. “Introducing ChatGPT.” https://openai.com/blog/chatgpt.  \nOuyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, \nChong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob \nHilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, \nPaul Francis Christiano, Jan Leike, and Ryan J. Lowe. 2022. “Training language models \nto follow instructions with human feedback.” Preprint, submitted March 2022. \nhttps://arxiv.org/abs/2203.02155. \nPaullada, Amandalynne, Inioluwa Deborah Raji, Emily M. Bender, Emily L. Denton and A. \nHanna. 2020. “Data and its (dis)contents: A survey of dataset development and use in \nmachine learning research.” Patterns 2(11). doi: 10.1016/j.patter.2021.100336. \nRadford, Alec, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. 2019. \n“Language Models are Unsupervised Multitask Learners.” Technical report, OpenAI.  \nRaffel, Colin, Noam M. Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, \nYanqi Zhou, Wei Li, and Peter J. Liu. 2020. “Exploring the Limits of Transfer Learning \nwith a Unified Text-to-Text Transformer.” Preprint, submitted July 2020. \nhttps://arxiv.org/abs/1910.10683. \nRosch, Eleanor. 1975. “Cognitive Representations of Semantic Categories.” Journal of \nExperimental Psychology: General 104(3): 192-233.  \nde Saussure, Ferdinand. 1959. Course in General Linguistics. Translated by Wade Baskin. New \nYork: The Philosophical Society. \nSmiraglia, Richard P. 2007. “The ‘Works’ Phenomenon and Best Selling Books.” Cataloging & \nClassification Quarterly 44(3-4): 179-195. doi: 10.1300/J104v44n03_02. \nSmiraglia, Richard P. 2019. “Work.” Knowledge Organization 46(4): 308-319. doi: 10.5771/0943-\n7444-2019-4-308. \nSoos, Carlin and Gregory H. Leazer. 2020. “Presentations of Authorship in Knowledge \nOrganization” Knowledge Organization 47(6): 486-500. doi: 10.5771/0943-7444-2020-6-\n486. \nCarlin Soos & Levon Haroutunian. 2023. On the Question of Authorship in Large Language \nModels (LLMs). NASKO, Vol. 9. pp. 1-17. \n17 \n \nSvenonius, Elaine. 2009. The Intellectual Foundations of Information Organization. Cambridge, \nMA: MIT Press.  \nUniversity of California, Los Angeles (UCLA). 2023. “ChatGPT and AI: Starting Points for \nDiscussion.” Online Teaching & Learning. https://online.ucla.edu/chatgpt -ai/. \nUniversity of Washington. 2023. “ChatGPT and other AI-based tools.” Center for Teaching and \nLearning. https://teaching.washington.edu/topics/preparing-to-teach/academic-\nintegrity/chatgpt/. \nUniversity of Wisconsin-Madison. 2023. “Considerations for Using AI in the Classroom.” L&S \nInstructional Design Collaborative. https://idc.ls.wisc.edu/guides/using -artificial-\nintelligence-in-the-classroom/. \nVaswani, Ashish, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Ł. \nUkasz Kaiser, and Illia Polosukhin. 2017. “Attention Is All You Need.” In Advances in \nNeural Information Processing Systems 30. \nVincent, James. 2023. “OpenAI CEO Sam Altman on GPT-4: ‘people are begging to be \ndisappointed and they will be.’” The Verge. https://www.theverge.com/23560328/openai -\ngpt-4-rumor-release-date-sam-altman-interview. \nWager E, Kleinert S. “Cooperation Between Research Institutions And Journals On Research \nIntegrity Cases: Guidance From The Committee On Publication Ethics (COPE).” ACTA \nInformatica Medica 20(3):136-40. doi: 10.5455/aim.2012.20.136-140. \nYang, Zhilin, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V. Le. \n2019. “XLNet: Generalized Autoregressive Pretraining for Language Understanding.” In \nProceedings of the 33rd International Conference on Neural Information Processing \nSystems. Red Hook, NY: 2019. \nZhang, Zhengyan, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. “ERNIE: \nEnhanced Language Representation with Informative Entities.” In Proceedings of the \n57th Annual Meeting of the Association for Computational Linguistics, 1441–51. \nFlorence, Italy: Association for Computational Linguistics, 2019. doi: 10.18653/v1/P19 -\n1139. \nZhu, Yukun, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio \nTorralba and Sanja Fidler. 2015. “Aligning Books and Movies: Towards Story -Like \nVisual Explanations by Watching Movies and Reading Books.” 2015 IEEE International \nConference on Computer Vision (ICCV): 19-27. \n \n \n \n \n "
}