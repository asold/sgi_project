{
  "title": "Ad Headline Generation using Self-Critical Masked Language Model",
  "url": "https://openalex.org/W3168282198",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A5005536808",
      "name": "Yashal Shakti Kanungo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5109167303",
      "name": "Sumit Negi",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5108532701",
      "name": "Aruna Rajan",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2555428947",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2970634364",
    "https://openalex.org/W2964352247",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W2994928925",
    "https://openalex.org/W2971274815",
    "https://openalex.org/W3006381853",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W2487501366",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2606974598",
    "https://openalex.org/W2953171463",
    "https://openalex.org/W2729046720",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2119717200"
  ],
  "abstract": "Yashal Shakti Kanungo, Sumit Negi, Aruna Rajan. Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies: Industry Papers. 2021.",
  "full_text": "Proceedings of NAACL HLT 2021: IndustryTrack Papers, pages 263–271\nJune 6–11, 2021. ©2021 Association for Computational Linguistics\n263\n1 \nAd Headline Generation using Self-Critical Masked Language Model \nYashal Shakti Kanungo Sumit Negi Aruna Rajan \nyashalk@amazon.com suminegi@amazon.com rajarna@amazon.com \nAbstract \nFor any E-commerce website it is a nontrivial \nproblem to build enduring advertisements that \nattract shoppers. It is hard to pass the creative \nquality bar of the website, especially at a large \nscale. We thus propose a programmatic solu-\ntion to generate product advertising headlines \nusing retail content. We propose a state of \nthe art application of Reinforcement Learning \n(RL) Policy gradient methods on Transformer \n(Vaswani et al., 2017) based Masked Language \nModels (Devlin et al., 2019). Our method cre-\nates the advertising headline by jointly con-\nditioning on multiple products that a seller \nwishes to advertise. We demonstrate that our \nmethod outperforms existing Transformer and \nLSTM + RL methods in overlap metrics and \nquality audits. We also show that our model-\ngenerated headlines outperform human sub-\nmitted headlines in terms of both grammar and \ncreative quality as determined by audits. \nIntroduction \nThere are a various types of ads. A set of exam-\nple ads that showcase products selected by sellers \nalong with headlines that advertise them are shown \nin Figure 1. Sellers create multiple ad campaigns \nfor multiple products, bid in an auction to advertise \nand pay for clicks on the ad. \nAn E-Commerce product catalog may have mil-\nlions of products which can be advertised. To ease \nthe ad headline writing process, humans resort to \nprogrammatically padding keywords, or repasting \nthe retail catalog content in the advertisement. \nTemplated creatives such as “Save Now on ...\" or \n“Buy more (product) of (brand)\" save the creative \neffort but fail to create any excitement or brand \nidentity in the minds of shoppers. High quality \nheadlines are more attractive to shoppers and of-\nfer better value proposition. In this paper, we de-\nscribe how we built a Natural Language Generation \n(NLG) system to generate instantaneous, attractive \nand brand identity building headlines for adver -\ntisements that intend to promote a wide range of \nproducts offered by a brand. \nThe content associated with a retail product has \nchallenging characteristics. Some product titles \nhave poor structure, grammatical issues, or par -\ntial phrases. The product titles also include vary-\ning number of product features such as “Hyper \nTough 18V Cordless Drill, 3/8 inch Chuck, Vari-\nable Speed, with 1.2Ah Nickel Cadmium Battery, \nCharger, Bit Holder LED Light\" along with titles \nsuch as “ZIPIT Grillz Backpack, Camo Grey\". \nThe generated headlines need to capture the in-\nformation present in the retail attributes and at the \nsame time be different and uniquely attractive. Ad-\nvertisers select multiple related products that are \nadvertised as part of a single ad campaign. The ad \ncampaign headline is then shared across all of these \nrelated products. Thus, the headline also needs to \ngeneralize the shared characteristics of the products \nand cannot be speciﬁc to a single product within \nthe campaign. \nThe key contributions of our work are: \n• We use Masked Language Model (MLM) for \nthe generation of advertisement headlines us-\ning multiple products at the same time. Ex-\ntensive test-set metrics, quality and grammar \naudits show that the proposed model out-\nperforms all the baselines and the human-\nsubmitted headlines in terms of quality and \ngrammar. \n• The novel usage of RL for the training of \nMLM allows us to directly optimize the MLM \nfor improved headline quality metrics with-\nout changing inference setup or latency. Our \nmethod can also be applied to any other NLG \ntask such as summarization, translation etc. \n• Our model reduces the extensive effort and \ntime that is required to manually create head-\nlines and has low latency. \n264\n2 \nFigure 1: Examples of different product ads from multiple websites across the internet. A variety of ad headlines \naccompany the products in these ads. \nRelated Work \nNatural Language Understanding (NLU) using Lan-\nguage Models (LM) has observed great leaps in \nrecent years. LMs have evolved from using word \nlevel models (Joulin et al., 2016) to to a variety \nof extensions to the Transformer (Vaswani et al., \n2017). The BERT (Devlin et al., 2019) employs \nTransformer in a pre-training setting and intro-\nduced the MLM training objective. \nRamachandran et al. (2016) ﬁrst demonstrated \ntextual generation by using auto-regressive predic-\ntion in a seq2seq architecture. Transformer based \nauto-regressive methods such as GPT2 (Radford \net al., 2019) and BART (Lewis et al., 2019) which \npredict one word at a time have also shown good \nresults. Zhu et al. (2020) concatenated BERT rep-\nresentations with the Encoder and Decoder layers \nof another LM to incorporate pre-trained LM. An-\nother model (Dong et al., 2019) combines BERT-\nbased Transformer Encoder with attention masking \nfrom the Transformer decoder. Rothe et al. (2019) \ncombined pre-trained BERT Encoder with GPT \ndecoder for NLG. \nRanzato et al. (2016) framed NLG as an RL prob-\nlem and the generation quality as a reward. The \nSelf-Critical Sequence Training (SCST) approach \n(Rennie et al., 2017) replaces the learned baseline \nfrom other approaches (Bahdanau et al., 2017) with \nthe model’s own inference time algorithm to nor-\nmalize the rewards. \nFor advertising, recent works (Xu et al., 2019; \nHughes et al., 2019) have combined LSTM based \npointer network (See et al., 2017) with RL methods \nto generate advertisement headlines. While these \nmethods improve the results, they fail to utilize \nextensive pre-training of Transformer based models \nand their various well-demonstrated advantages. \nOur method extends BERT based generation \n(Dong et al., 2019) by using Self-Critical policy \ngradient method (Rennie et al., 2017) and jointly \nconditioning the generated sentence on multiple \nproducts at the same time. This allows us to use \npre-trained BERT based LMs that can be trained \nto optimize various inference time metrics that are \ntypically non-differentiable such as BLEU, Rouge, \nReadability etc. \n3 Self-Critical Masked Language Model \n3.1 Masked Language Model \nThe BERT model takes an unlabeled input se-\nquence x = (x1, x2, ..., x|x|) and randomly masks \nsome positions Mx by replacing them with a spe-\ncial mask token [MASK], to produce a sequence \nlike (x1, [MASK], ..., x|x|). All the tokens are em-\nbedded and added to special positional embeddings. \nIt then uses N identical Transformer layers to gen-\nerate contextualized representation, with each layer \nemploying self-attention by taking in the output of \nthe previous layer. To compute self-attention, the \noutput of the previous layer is projected into triplets \nof vectors named Query, Key and Value (Q, K, V ) \nof dimensions d. The attention A is then given as: \nQKT \nA = softmax( √ )V (1)\nd \nAfter the ﬁnal Transformer layer the model uses \na feed forward layer followed by a softmax over \nthe vocabulary to predict the masked tokens. The \nMLM loss for the sequence x is then calculated as: \nY \nLMLM = − log p(xm|(xm0 ∈ x \\ Mx)) \nm∈Mx \n(2) \nwhere (xm0 ∈ x \\ Mx) represents all the tokens \nin x that are not masked and m ∈ Mx are all the \nmasked positions. \n265\nFigure 2: The sub-tokens from the product titles and \nheadline are embedded and added with other embed-\ndings that encode the positional and segment informa-\ntion. We also optionally add an embedding that repre-\nsents the category of the product. During training, the \nmasked tokens are predicted using Transformer layers \nand the cross-entropy (Eq. 2) loss and Self-Critical (Eq. \n9) gradient is used to optimize the model. During infer-\nence, we predict one word at a time (left-to-right) in an \nauto-regressive manner using Beam Search. \n3.2 Encoding multiple products and common \nheadline for Proposed MLM \nDuring training, for a given advertising campaign, \nhour model takes as input it’s headline x = \nh h(x1 , ..., x|xh|) and a set P of one or more prod-\nucts. Each product p is represented by its title \np ppx = (x1, ..., x ). The titles and the headline are |xp|\ntokenized to sub-word tokens. \nTo encode using the model that only accepts a \nsingle product, we simply append ‘ [EOS]’ ∈ V \nto both the title and the headline and concatenate \ntheir tokens. The entire concatenated sequence is \nprepended with ‘[SOS]’ ∈ V. \nWe encode multiple products by concatenating \nthe tokens from different products using a spe-\ncial token ‘\n[P_SEP]’ ∈ V. We replace a token \n‘[UNUSED_0]’ ∈ V that remains unused during \npre-training, with this special token during multi-\nproduct ﬁne-tuning. This makes a distinction be-\ntween different titles as well as the source and tar-\nget sub-sequences. It also yields individual embed-\ndings for each product for other tasks. \nhOnly the tokens from the headline x are ran-\ndomly masked with token ‘ [MASK]’ ∈ V. We \ndiscuss results for the model that additionally also \nmasks the source tokens in section 5.1. \nThe complete process for an example such that \nall products in the ad have two tokens and the head-\nline has 4 tokens is illustrated in Figure 2. \nWe also experimented with adding of category \nbased embeddings. The category labels for each \nproduct such as “Cell Phones and Accessories\" \nare tokenized to subword units, encoded using the \nsame embedding matrix as that of the title tokens, \naveraged and added to the title token embeddings. \n3.3 Generation using Self-Critical Masked \nLanguage Model \nThe BERT MLM framework with multi-directional \nattention discussed in Section 3.1 cannot be used \nfor auto-regressive generation directly. This is \nbecause, during training, the masked headline \nwords may condition on the future words which \nare not available during auto-regressive inference. \nFor MLM auto-regressive generation, we employ \nmasked attention (Dong et al., 2019) that modiﬁes \nthe attention from equation 1 as below: \nQKT \nAmasked = softmax( √ + Φij )V (3)\nd \nwhere Φij represents the attention mask between \nthe positions i and j. The elements are set to 0 if \nattention is allowed and −∞ if it is not allowed. \nFigure 3 illustrates the attention mask for headline \ngeneration using multiple input products. \nThe BERT MLM uses log-likelihood (Equation \n2) of masked words during training to optimize \nthe model parameters. The likelihood is predicted \nusing other ground-truth words during training \nand other predicted words during inference. This \ncauses exposure bias (Ranzato et al., 2016; Ren-\nnie et al., 2017) and accumulates error during in-\nference. Moreover, the training is optimized for \nlog-likelihood, while we actually care about other \nmore evolved measures of headline quality such as \noverlap metrics BLEU (Papineni et al., 2002) and \nROUGE (Lin, 2004). \nTo overcome these issues and improve the qual-\nity of the generated headlines, we frame the MLM \n266\nFigure 3: Masked attention partially restricts attention \nfor some token pairs. It prevents attention to headline \ntokens that would not be accessible during each step of \ngeneration during inference. \nas an RL problem. The model is an ‘agent’ that \ntakes the ‘action’ of predicting masked words \nand updates the ‘state’ such as the self-attention \nweights. The MLM follows a policy πθ deﬁned \nby the parameters θ of the model. It receives a \nreward that is proportional to the quality of the \ngenerated headline. This quality may either be the \noverlap with ground truth headlines that have been \napproved by internal subject-matter-experts or be \npredicted by another model. Our goal is to maxi-\nmize the reward corresponding to a generated head-\nline xˆh during training, with the tokens at some \nmasked positions M h sampled from the model. x \nWe thus minimize the negative expected reward \ndeﬁned by any reward function r(·) for headline \nquality r(ˆxh) as: \nLRL = −Exˆh∼πθ [r(ˆx h)] (4) \nWe can compute the gradient rθLRL using the \nREINFORCE algorithm (Williams, 1992). It is \ndeﬁned as: \nrθLRL = −Exˆh∼πθ [r(ˆx h)rθP ] (5) \nwhere, \nX h hP = log pθ(ˆx |(ˆx 0 ∈ xˆh \\ Mˆh ) (6)m m x \nm∈M hxˆ\nsuch that Mˆh are the masked positions and xˆh \\x \nMˆh are all the unmasked tokens. x \nTo reduce the variance without changing the ex-\npected gradient, the algorithm proposes to use a \nbaseline b that does not depend on the generated \nheadline xˆh . b is used to normalize the reward \nalong with P from equation 6 as: \nrθLRL = −Er(ˆxh)∼πθ [(r(ˆx h) − b)rθP ] (7) \nA single Monte-Carlo sample for each set of \nproducts and headline can be used to approximate \nthe gradient. Using the deﬁnition of P from equa-\ntion 6, we have the approximate gradient: \nrθLRL ≈−(r(ˆx h) − b)rθP (8) \nInstead of using other models to estimate the \nexpected baseline reward (Ranzato et al., 2016; \nBahdanau et al., 2017), we employ Self-Critical \ntraining (Rennie et al., 2017) that involves generat-\ning two headlines using the same underlying MLM. \nThe ﬁrst headline xˆh is generated by sampling from \nthe vocabulary distributions generated by the model \nfor the masked tokens. The second headline zˆh is \ngenerated using the inference time strategy, which \nuses the token with the maximum probability at \neach step rather than sampling. The difference in \nthe reward achieved by these two headlines is used \nto compute the gradient: \nrθLSC_MLM ≈−(r(ˆx h) − r(ˆz h))rθP (9) \nwhere P is deﬁned by equation 6. \nThus, this method maximizes both the reward \nof the headlines generated by MLM and the like-\nlihood of correct words by incorporating both the \nlikelihood and the reward in the loss function. \n3.4 Inference \nDuring inference, we generate the headline auto-\nregressively using beam search until we reach the \npredetermined max length or each beam generates \nthe end token. We have employed a modiﬁed ver-\nsion of Length Normalization (Wu et al., 2016) to \nbetter adapt to our headline lengths and training \nsetup. This is necessary as the default beam search \nsetup uses the log probability of each word to select \nthe best headline. However, this biases the results \nas longer headlines would have lower probability of \n267\ngeneration. We thus use the following normalized 4.2 Baseline \nscores for each word to select the best headline: \n(2 + 1)α \nh hscore(ˆxi ) = log-likelihood(ˆxi ) ∗ (10)(2 + i)α \nwhere α is the length normalization coefﬁcient \nhand xˆ is the ith word of the generated headline i \nin each beam. We also include additional Regular \nExpression based post-processing to remove extra \nspaces around various symbols such as ‘-,+()’ etc. \n4 Experiments \n4.1 Training and Inference \nWe used over 500,000 ad campaigns that were cre-\nated on Amazon by sellers who have signed-up for \nadvertising. Each campaign contains a set of re-\nlated products along with an ad headline. We only \nselected the campaigns that contained English head-\nlines and products with English titles. They were \nalso de-duplicated to only have unique products-\nheadline pairs. The mean product title length is \n19.6 words and the mean headline length is 6.16 \nwords. The entire dataset was divided into train \n(85%), validation (5%) and test (10%) sets. For \ntraining, we only selected the campaigns that com-\nply with ad policies as veriﬁed by internal experts. \nWe use the HuggingFace (Wolf et al., 2020) \nimplementation of the Transformer BERT ‘Large’ \nmodels as the base for our experiments. The mod-\nels are pre-trained on WikiPedia and BookCor -\npus (Devlin et al., 2019; Dong et al., 2019). We \nﬁrst ﬁne-tune the pre-trained model for up-to 15 \nepochs with early stopping using LMLM and Adam \n(Kingma and Ba, 2014). We then further ﬁne-tune \nthe model for another 15 epochs with early stop-\nping using Adam with rLSC_MLM (Equation 9). \nWe use the Rouge L F1 (Lin, 2004) overlap with \nthe approved headlines as the headline quality re-\nward. For a fair comparison, the MLM-only model \nis ﬁne-tuned for upto 30 epochs. \nThe model training is very time expensive with \na single ﬁne-tuning sub-experiment of 30 epochs \ntaking over 20 days on an Nvidia v100. We \nthus only performed the essential experiments that \nhelp to determine the contribution of different sub-\nexperiments and proposals. We estimated post-\nexperiment that a single ﬁne-tuning sub-experiment \nof 30 epochs would consume approximately 150 \nkWh of energy based on the GPU’s power draw. \nWe used a Pointer Network (See et al., 2017) based \nbi-LSTM with intra-decoder and temporal atten-\ntion. We also used Self-Critical training with the \nbi-LSTM, similar to other ad headline generation \nmethods (Xu et al., 2019; Hughes et al., 2019) meth-\nods for a fair comparison to Self-Critical MLM. \n4.3 Ablations \nWe trained a model with the same architecture, \nnumber of parameters and input as the proposed \nmodels but without MLM pre-training and sepa-\nrately without Self-Critical loss to study the impact \nof the proposals. \nWe also trained a model with MLM pre-training \nbut ﬁne-tuning only using the primary ﬁrst prod-\nuct from each campaign instead of using all the \nproducts. This is interesting since some of the cam-\npaigns are cohesive to a degree with similar prod-\nucts and using only one product improves training \ntime and inference latency. \nWe also report overlap metrics for model \nthat does not use length normalization and post-\nprocessing discussed in equation 10. We also in-\nclude results for model that uses BERT Base as the \nbase model instead of BERT Large. \n5 Results \n5.1 Overlap with Approved Headlines \nThe ﬁrst evaluation criterion we adopt is over -\nlap (Sharma et al., 2017) of model headlines with \nsubject-matter-experts approved human-submitted \nheadlines from the test set (Table 1). \nMasking the source product title words reduces \nthe performance as the titles and headlines do not \nfollow the same sentence structure and distribution. \nAdding product category embedding reduces per-\nformance and our hypothesis is that this is because \nthe base model cannot be pre-trained with these em-\nbeddings. Only using one title achieves lesser but \nrespectable performance, highlighting the efﬁcacy \nof multi-product conditioning. \n“No pre-training of MLM\" highlights the advan-\ntage of using non-pretrained Transformer based ar-\nchitecture over bi-LSTM. ‘Proposed MLM’ shows \nthe advantage of using pre-training, BERT Large \nand only masking the headline. ‘Proposed Self-\nCritical MLM’ achieves the best scores across all \nthe metrics and highlights the applicability of our \nproposed approach. \n268\nModel Rouge-L CIDEr BLEU-4 METEOR Avg. Cos. Sim. \nBaseline bi-LSTM Pointer Network model \nbi-LSTM - - - - -\nSelf Critical bi-LSTM 0.62 0.01 1.06 0.42 -4.31 \nMLM Baselines and Ablations (Single Product and No Self Critical Training) \nFirst Product Only 2.14 0.19 5.03 3.55 0.36 \nFirst Product and Category embedding 1.52 0.13 4.18 2.938 0.15 \nProposed MLM and Ablations (Multiple Products and No Self Critical Training) \nUsing BERT Base instead of BERT Large 2.85 0.22 4.96 3.58 1.53 \nNo pre-training of MLM (Training from scratch) 3.38 0.27 5.72 3.79 -0.04 \nAdditional Source Titles Masking 4.13 0.29 4.42 5.41 -2.09 \nProposed MLM 5.08 0.42 7.49 5.46 1.31 \nProposed Self-Critical MLM (SC-MLM) and Ablation \nNo beam search normalization and post-processing 5.37 0.43 7.81 5.61 1.96 \nProposed Self-Critical MLM 6.33 0.55 9.11 6.14 3.75 \nTable 1: Absolute improvement over baseline in terms of overlap measures with over 50,000 manually approved \nhuman-submitted headlines from the test set. We have reported the differences in the F1 of Rouge-L and BLEU-4 \nscores to the baseline bi-LSTM model. ‘Avg. Cos. Sim.’ is the average cosine similarity of model headlines to the \nhuman-submitted headlines measured using an independently pre-trained Language Model. \nSC- BILSTM MLM - S INGLE PRODUCT PROPOSED MLM PROPOSED SC-MLM \n% IMPROVEMENT IN MEAN RATING OVER HUMAN -SUBMITTED HEADLINES \n-9.87% 0.40% 1 .15% 2.07% \n% IMPROVEMENT IN NUMBER OF HEADLINES \nRATED ≥ 2 OUT OF 3 -4.99% \nRATED 3 OUT OF 3 -42.96% \n2.75% \n-0.06% \n2.42% \n1.22% \n2.37% \n6.53% \nTable 2: Comparison of model-generated headlines to human-submitted headlines on a 3-point scale quality audit \nof a random blind test set (N ≈ 5000). \n5.2 Quality and Grammar Audits \nWe also conducted large scale crowd-sourced eval-\nuation studies of the headlines with over 150,000 \njudgments. All headlines are shufﬂed and each \nheadline is rated by 3 random and double-blind \ncrowd-sourced auditors. The quality is judged on \na 3-point scale of [1. Incorrect or Irrelevant, 2. \nCorrect, 3. Correct and Attractive] and we use the \nmode of the 3 judgments. \nIn this double-blind audit, the auditors were \nnot aware of the source of the headlines and we \nwere not aware of the identity or demographics \nof any auditor. More details about the work-\nforce may be found in the platform documentation \n(Ground Truth, 2021). In order to determine the \ncompensation for the crowd-sourced workers, we \nused the guideline provided by the crowd-sourcing \nplatform to “choose a price consistent with the ap-\nproximate time it takes to complete a task\" (Visible \nin the Console while creating the Labeling (2021) \njob). We thus ﬁrst conducted an internal audit by \nvolunteers across our organization to determine the \ntime required to complete the task (average 21.59s) \nand then used the remuneration recommended for \nthe corresponding time range ($0.12 for 20s - 22s). \nTable 2 summarizes the quality audits. The \nSC-biLSTM model performed worse compared \nto human-submitted headlines. The proposed SC-\nMLM model achieves the highest average rating \nand the most number of perfectly rated headlines. \nUsing just a single product does produce correct \nheadlines with 8% faster inference latency but fails \nto produce attractive headlines due to lack of input \nfrom multiple products. \nWe also conducted Grammar speciﬁc audits (N \n≈ 10000) in which the grammar of the headlines \nis judged independently. 98.13% of SC-MLM and \n98.12% of MLM generated headlines were judged \nto have correct grammar against 93.14% of human \nsubmitted headlines. \nTable 3 shows a sample of headlines for cam-\npaigns in the blind test-set. Excessive keyword \nstufﬁng in source product titles does hamper head-\nline quality at times and post-ﬁltering using beam \n269\nOne of the source product’s title Human Submitted Headline Proposed MLM Proposed SC-MLM \nBEST Natural Hair Growth Oil for \nGUARANTEED Hair Strength, \nThickening, Hair Gro... \nNatural Hair Growth & Beard Care Protect Your Hair and Beard With All All Natural Hair care products Products Natural Oils \nRoyal 310DX Thermal Print Electronic \nCash Register \nAffordable Reliable Cash Management \nfrom Royal \nBlue Copper 5 Anti-Aging Products \nSoft & Hypoallergenic Twin Sheets \nShare the Love with this Classroom \nDecor \nPrint more for less with SuperTank \nprinters. Canon \nRoyal Cash Registers - Retail & Event \nSupplies \nSecure your cash with Royal Cash \nRegisters \nBlue Copper 5 Anti-Aging Body Lift, \nPregnancy Stretch Marks Prevention and \nRemoval Cream 5 Oz \nDiscover Osmotics Best Selling Products Say Goodbye to Stretch Marks \nCosy House Collection Twin Size Bed \nSheets - Cream Bedding Set - Deep \nPocket - Extra Soft Luxury... \nLuxury Twin Sheets - These Will Change \nYour Life. \nSoft Luxury Sheets - These Will Change \nYour Life. \nCarson Dellosa | Valentine’s Day Heart \nStickers | 1-inch x 1-inch, 216ct \nShow your Valentine some love this \nValentine’s Day Valentine’s Day Celebrations \nCanon GI-20 PGBK Ink Bottle, \nCompatible to PIXMA G6020 and \nG5020 MegaTank Printers \nAll-in-one solution for professional \ngrade prints. \nAll-in-one solution for professional \ngrade prints. \nLABILUS iPhone Xs MAX case, \n(Rugged Armor Series) TPU Soft Stripe \nDesigned Protective Cover Case... \n360° Protection Heavy Duty for iPhone \nXs Max \nRugged Protective Case for iPhone Xs Rugged Armor Protective Case for \nMAX iPhone Xs Max \nBiscotti Cookie Gift Basket, Gourmet \nGift Basket, Delicious Biscotti Artfully \nDecorated 18 Count... \nValentines Gifts Gourmet Chocolate Gift Baskets Gourmet Holiday Gift Baskets \nLe Angelique Tapered Curling Iron \nWand with Glove And 2 Clips - 3/4 to 1 \nInch (18-25mm) Conical ... \nTapered Curling Wands with Glove and 2 \nClips Le Angelique Tapered Curling Iron Wand Le Angelique Tapered Curling Iron Wand \nJUNK Brands London Fog-BBL London \nFog Big Bang Lite Headband Headbands for Every Adventure Headbands for Every Adventure BBL Headbands for Adventure \nJump&Go Portable Car Battery Jump \nStarter set -16,000mAh, 600A Peak, \nMini Automotive Power Boost... \nPortable Jump and go Jumpstarter Jump and Go Portable Car Jump Starter Jump and Go Portable Jump Starters \ndecanit Silver Metal Thin Edge 5x7 \nPicture Frames, Silver Thin Proﬁle Phot\nFrames 5 by 7 Inch,... \no Life-Style-Living Metal Thin Edge Picture Frames Thin Edge Picture Frames \nCARSYS Coating Thickness Gauge \nDPM-816 Extended Range Precision \nProbe Fe/NFe Paint Meter for Car... \nCoating Thickness Gauge - Range \nPrecision Coating Thickness Gauges Coating Thickness Gauge \nRich & Creamy Buttermilk Syrup \nOriginal Flavor by Uncle Bob’s Butter \nCountry 16 ﬂ oz/1 Pack \nRich and Creamy Buttermilk Syrup -Fresh and Premium Buttermilk Syrup Rich and Creamy Buttermilk Syrup Taste Great \nSesame Street Ernie Face Tee Funny \nHumor Pun Adult Mens Graphic T-Shirt \nApparel (Small), Orange \nSesame Street Tees for Adults Sesame Street Men’s Shirts Sesame Street Men’s Shirts \nAgvee Unbreakable End Tip [3 Pack 6ft] \n4A Heavy Duty USB Phone Charger AGVEE Fast Lightning Charging Cable \nfor iPhone \nAGVEE Fast iPhone 11 X 10s 10s XR \nCable \nAGVEE Heavy Duty iPhone 11 Xs XS \nXR Cable Cable, Durable Charging for iPhone 11 \nPro Max X XS XR, i-Phone 10x 10xs \n...... \nTable 3: Some samples of model generated headlines from subsets rated 3, 2 and 1. The frequency of headlines is \nnot indicative of true distribution of headline quality. \nsearch score helps to ﬁlter them out. \nWe do observe cases where both the models gen-\nerate the same headline. This is an artifact of the \nfact that both the models share the ﬁrst 15 epochs. \nThe SC-MLM model generates more descriptive \nheadlines and both models are able to abstract the \nproduct qualities. \nConclusion \nAd headline generation is a difﬁcult problem owing \nto the varying nature of retail product attributes. A \nlot of historical methods focus on template based \ncreation of ad headlines that are not very expres-\nsive. \nWe demonstrated a new NLG based method \nto generate headlines for multiple products. Our \nmethod achieves highest score in overlap metrics, \nquality audits and grammar audits compared to the \nbaselines and human-submitted headlines. \nMasked Language Models were relatively un-\nexplored for ad headline generation and we were \nable to demonstrate their utility. We further ex-\ntended the performance of the model by using Re-\ninforcement Learning. The method only changes \nthe training procedure without impacting inference \nlatency. Thus, our work contributes to both SOTA \nand practical business applications. \nThe approach can also be used for any other \nNLG task. \n6 \n270\nReferences \nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu, \nAnirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron \nCourville, and Yoshua Bengio. 2017. An \nActor-Critic Algorithm for Sequence Prediction. \narXiv:1607.07086 [cs]. ArXiv: 1607.07086. \nJacob Devlin, Ming-Wei Chang, Kenton Lee, and \nKristina Toutanova. 2019. BERT: Pre-training \nof Deep Bidirectional Transformers for Language \nUnderstanding. arXiv:1810.04805 [cs]. ArXiv: \n1810.04805. \nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou, \nand Hsiao-Wuen Hon. 2019. Uniﬁed Language \nModel Pre-training for Natural Language Under -\nstanding and Generation. arXiv:1905.03197 [cs]. \nArXiv: 1905.03197. \nUsing MTurk with Ground Truth. 2021. [link]. \nJ. Weston Hughes, Keng-hao Chang, and Ruofei Zhang. \n2019. Generating Better Search Engine Text Adver-\ntisements with Deep Reinforcement Learning. In \nProceedings of the 25th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data \nMining, KDD ’19, pages 2269–2277, Anchorage, \nAK, USA. Association for Computing Machinery. \nArmand Joulin, Edouard Grave, Piotr Bojanowski, and \nTomas Mikolov. 2016. Bag of tricks for efﬁcient text \nclassiﬁcation. arXiv preprint arXiv:1607.01759. \nDiederik Kingma and Jimmy Ba. 2014. Adam: A \nmethod for stochastic optimization. International \nConference on Learning Representations. \nGround Truth Labeling. 2021. Create a labeling job. \nMike Lewis, Yinhan Liu, Naman Goyal, Mar -\njan Ghazvininejad, Abdelrahman Mohamed, Omer \nLevy, Ves Stoyanov, and Luke Zettlemoyer. \n2019. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. arXiv:1910.13461 [cs, \nstat]. ArXiv: 1910.13461. \nChin-Yew Lin. 2004. ROUGE: A Package for Auto-\nmatic Evaluation of Summaries. In Text Summariza-\ntion Branches Out, pages 74–81, Barcelona, Spain. \nAssociation for Computational Linguistics. \nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic eval-\nuation of machine translation. In Proceedings of \nthe 40th Annual Meeting of the Association for Com-\nputational Linguistics, pages 311–318, Philadelphia, \nPennsylvania, USA. Association for Computational \nLinguistics. \nSponsored Advertising policies. [link]. \nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, \nDario Amodei, and Ilya Sutskever. 2019. Language \nmodels are unsupervised multitask learners. OpenAI \nBlog, 1(8):9. \nPrajit Ramachandran, Peter J. Liu, and Quoc V . Le. \n2016. Unsupervised pretraining for sequence to se-\nquence learning. arXiv preprint arXiv:1611.02683. \nMarc’Aurelio Ranzato, Sumit Chopra, Michael \nAuli, and Wojciech Zaremba. 2016. Sequence \nLevel Training with Recurrent Neural Networks. \narXiv:1511.06732 [cs]. ArXiv: 1511.06732. \nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh, \nJarret Ross, and Vaibhava Goel. 2017. Self-\ncritical Sequence Training for Image Captioning. \narXiv:1612.00563 [cs]. ArXiv: 1612.00563. \nSascha Rothe, Shashi Narayan, and Aliaksei Severyn. \n2019. Leveraging Pre-trained Checkpoints for Se-\nquence Generation Tasks. arXiv:1907.12461 [cs]. \nArXiv: 1907.12461. \nAbigail See, Peter J. Liu, and Christopher D. Man-\nning. 2017. Get To The Point: Summarization \nwith Pointer-Generator Networks. In Proceedings \nof the 55th Annual Meeting of the Association for \nComputational Linguistics (Volume 1: Long Papers), \npages 1073–1083, Vancouver, Canada. Association \nfor Computational Linguistics. \nShikhar Sharma, Layla El Asri, Hannes Schulz, and \nJeremie Zumer. 2017. Relevance of Unsupervised \nMetrics in Task-Oriented Dialogue for Evaluating \nNatural Language Generation. arXiv:1706.09799 \n[cs]. ArXiv: 1706.09799. \nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob \nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz \nKaiser, and Illia Polosukhin. 2017. Attention is All \nyou Need. In I. Guyon, U. V . Luxburg, S. Bengio, \nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998–6008. Curran Asso-\nciates, Inc. \nRonald J. Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine Learning, 8(3):229–256. \nThomas Wolf, Lysandre Debut, Victor Sanh, Julien \nChaumond, Clement Delangue, Anthony Moi, Pier -\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2020. HuggingFace’s Trans-\nformers: State-of-the-art Natural Language Process-\ning. arXiv:1910.03771 [cs]. ArXiv: 1910.03771. \nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V . \nLe, Mohammad Norouzi, Wolfgang Macherey, \nMaxim Krikun, Yuan Cao, Qin Gao, Klaus \nMacherey, Jeff Klingner, Apurva Shah, Melvin John-\nson, Xiaobing Liu, Łukasz Kaiser, Stephan Gouws, \nYoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith \nStevens, George Kurian, Nishant Patil, Wei Wang, \nCliff Young, Jason Smith, Jason Riesa, Alex Rud-\nnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, \nand Jeffrey Dean. 2016. Google’s Neural Machine \nTranslation System: Bridging the Gap between Hu-\nman and Machine Translation. arXiv:1609.08144 \n[cs]. ArXiv: 1609.08144. \n271\nPeng Xu, Chien-Sheng Wu, Andrea Madotto, and Pas-\ncale Fung. 2019. Clickbait? Sensational Headline \nGeneration with Auto-tuned Reinforcement Learn-\ning. In Proceedings of the 2019 Conference on \nEmpirical Methods in Natural Language Processing \nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages \n3065–3075, Hong Kong, China. Association for \nComputational Linguistics. \nJinhua Zhu, Yingce Xia, Lijun Wu, Di He, Tao \nQin, Wengang Zhou, Houqiang Li, and Tie-Yan \nLiu. 2020. Incorporating BERT into Neural Ma-\nchine Translation. arXiv:2002.06823 [cs]. ArXiv: \n2002.06823. ",
  "topic": "Headline",
  "concepts": [
    {
      "name": "Headline",
      "score": 0.9499796628952026
    },
    {
      "name": "Computer science",
      "score": 0.5996357798576355
    },
    {
      "name": "Computational linguistics",
      "score": 0.5278013944625854
    },
    {
      "name": "Natural language processing",
      "score": 0.47304481267929077
    },
    {
      "name": "Linguistics",
      "score": 0.45848721265792847
    },
    {
      "name": "Artificial intelligence",
      "score": 0.43231523036956787
    },
    {
      "name": "Philosophy",
      "score": 0.16362375020980835
    }
  ],
  "institutions": [],
  "cited_by": 13
}