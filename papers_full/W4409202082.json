{
  "title": "PBertKla: a protein large language model for predicting human lysine lactylation sites",
  "url": "https://openalex.org/W4409202082",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2319787471",
      "name": "Hongyan Lai",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2972053334",
      "name": "Diyu Luo",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2129031682",
      "name": "Mi Yang",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2089090647",
      "name": "Tao Zhu",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2124305921",
      "name": "Huan Yang",
      "affiliations": [
        "University of Electronic Science and Technology of China",
        "Quzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2342613933",
      "name": "Xinwei Luo",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2136267901",
      "name": "Yijie Wei",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2137284105",
      "name": "Sijia Xie",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A4380591323",
      "name": "Feitong Hong",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2553619133",
      "name": "Kunxian Shu",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A2886354550",
      "name": "Fu-Ying Dao",
      "affiliations": [
        "Nanyang Technological University"
      ]
    },
    {
      "id": "https://openalex.org/A1982262173",
      "name": "Hui Ding",
      "affiliations": [
        "University of Electronic Science and Technology of China"
      ]
    },
    {
      "id": "https://openalex.org/A2319787471",
      "name": "Hongyan Lai",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2972053334",
      "name": "Diyu Luo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2129031682",
      "name": "Mi Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2089090647",
      "name": "Tao Zhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2124305921",
      "name": "Huan Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2342613933",
      "name": "Xinwei Luo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136267901",
      "name": "Yijie Wei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2137284105",
      "name": "Sijia Xie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4380591323",
      "name": "Feitong Hong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2553619133",
      "name": "Kunxian Shu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2886354550",
      "name": "Fu-Ying Dao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1982262173",
      "name": "Hui Ding",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4315865958",
    "https://openalex.org/W4405624894",
    "https://openalex.org/W4391982099",
    "https://openalex.org/W4403045462",
    "https://openalex.org/W4403213599",
    "https://openalex.org/W4405525278",
    "https://openalex.org/W4401602316",
    "https://openalex.org/W2982216374",
    "https://openalex.org/W4392710536",
    "https://openalex.org/W4220670155",
    "https://openalex.org/W4378575828",
    "https://openalex.org/W4395962200",
    "https://openalex.org/W4388293998",
    "https://openalex.org/W4403044160",
    "https://openalex.org/W4403300181",
    "https://openalex.org/W2905629607",
    "https://openalex.org/W3096599651",
    "https://openalex.org/W4387016121",
    "https://openalex.org/W4312070890",
    "https://openalex.org/W4388571136",
    "https://openalex.org/W3192109225",
    "https://openalex.org/W4220742882",
    "https://openalex.org/W4404516238",
    "https://openalex.org/W4323066423",
    "https://openalex.org/W4392844021",
    "https://openalex.org/W4205773061",
    "https://openalex.org/W2141818629",
    "https://openalex.org/W4400269626",
    "https://openalex.org/W4200175423",
    "https://openalex.org/W4396661350",
    "https://openalex.org/W4384204069",
    "https://openalex.org/W4390014581",
    "https://openalex.org/W4402847507",
    "https://openalex.org/W4321003857",
    "https://openalex.org/W4406440058",
    "https://openalex.org/W4313454211",
    "https://openalex.org/W4309506674",
    "https://openalex.org/W2170747616",
    "https://openalex.org/W4380738334",
    "https://openalex.org/W4403434692",
    "https://openalex.org/W4390946589",
    "https://openalex.org/W6713134421",
    "https://openalex.org/W3177500196",
    "https://openalex.org/W4388080288",
    "https://openalex.org/W4403782367",
    "https://openalex.org/W4400061043",
    "https://openalex.org/W2972223935",
    "https://openalex.org/W4308370073"
  ],
  "abstract": "All results indicated that PBertKla excelled as an automatic predictor of human Kla sites, and it would advance the investigation of lactylation modifications and their significance in health and disease.",
  "full_text": "Lai et al. BMC Biology           (2025) 23:95  \nhttps://doi.org/10.1186/s12915-025-02202-1\nRESEARCH Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 \nInternational License, which permits any non-commercial use, sharing, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence, and indicate if \nyou modified the licensed material. You do not have permission under this licence to share adapted material derived from this article or \nparts of it. The images or other third party material in this article are included in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To \nview a copy of this licence, visit http://creativecommons.org/licenses/by-nc-nd/4.0/.\nBMC Biology\nPBertKla: a protein large language model \nfor predicting human lysine lactylation sites\nHongyan Lai1†, Diyu Luo1†, Mi Yang2, Tao Zhu1, Huan Yang3, Xinwei Luo4, Yijie Wei4, Sijia Xie4, Feitong Hong4, \nKunxian Shu1*, Fuying Dao5* and Hui Ding4* \nAbstract \nBackground Lactylation is a newly discovered type of post-translational modification, primarily occurring on lysine \n(K) residues of both histones and non-histones to exert diverse effects on target proteins. Research has shown \nthat lysine lactylation (Kla) modification is ubiquitous in different cells and participates in the determination of cell \nfunction and fate, as well as in the initiation and progression of various diseases. Precise identification of Kla sites \nis fundamental for elucidating their biological functions and uncovering their application potential.\nResults Here, we proposed a novel human Kla site predictor (named PBertKla) through curating a reliable bench-\nmark dataset with proper sample length and sequence identity threshold to train a protein large language model \nwith optimal hyperparameters. Extensive experimental results consistently demonstrated that our model possessed \nrobust human Kla site prediction ability, achieving an AUC (area under receiver operating characteristic curve) value \nof over 0.880 on the independent validation data. Feature visualization analysis further validated the effectiveness \nof in feature learning and representation from Kla sequences. Moreover, we benchmarked PBertKla against other \ncutting-edge models on an independent testing dataset from different sources, highlighting its superiority \nand transferability.\nConclusions All results indicated that PBertKla excelled as an automatic predictor of human Kla sites, and it would \nadvance the investigation of lactylation modifications and their significance in health and disease.\nKeywords Protein large language model, BERT, Transformer, Lysine lactylation site, Human\n†Hongyan Lai and Diyu Luo contributed equally to this work.\n*Correspondence:\nKunxian Shu\nshukx@cqupt.edu.cn\nFuying Dao\nfuying.dao@ntu.edu.sg\nHui Ding\nhding@uestc.edu.cn\n1 Chongqing Key Laboratory of Big Data for Bio Intelligence, Chongqing \nUniversity of Posts and Telecommunications, Chongqing 400065, China\n2 Clinical Hospital of Chengdu Brain Science Institute, School of Life \nScience and Technology, University of Electronic Science and Technology \nof China, Chengdu 610054, China\n3 Yangtze Delta Region Institute (Quzhou), University of Electronic Science \nand Technology of China, Quzhou, Zhejiang 324000, China\n4 Center for Informational Biology, School of Life Science and Technology, \nUniversity of Electronic Science and Technology of China, \nChengdu 611731, China\n5 School of Biological Sciences, Nanyang Technological University, \nSingapore 639798, Singapore\nPage 2 of 13Lai et al. BMC Biology           (2025) 23:95 \nBackground\nProtein post-translational modifications (PTMs) refer to \nthe enzyme-catalyzed covalent modification of amino \nacid residues after protein synthesis. PTMs do not alter \nthe original amino acid sequence but regulate the physic -\nochemical properties, structure, and stability of the pro -\nteins, thereby diversifying their functions [1]. At present, \na wide variety of PTMs have been identified and well-\nstudied, including phosphorylation, acetylation, ubiqui -\ntination, methylation, and glycosylation [2–4]. Different \nPTMs regulate various physiological and biochemical \nprocesses, influencing growth, development, and disease \nprogression [5–7].\nLysine lactylation (Kla), as a novel type of PTM [8], is \ncatalyzed by lactyltransferases (e.g., HAT p300) [9] and \ndelactylases (e.g., class I histone deacetylases, HDAC1-\n3) [10] in the lactate metabolism process. It dynamically \nregulates cellular metabolism, cell signaling, neuronal \nexcitation, immune homeostasis, and the maintenance \nof cardiovascular and cerebrovascular functions. Studies \nhave shown that Kla is closely associated with metabolic \ndisease [11], neurodegenerative diseases [12], cardiovas -\ncular diseases [13], and cancers [14–17], exhibiting func -\ntional heterogeneity. In many cancers, Kla demonstrates \ntumor-promoting properties. For instance, Zhou et  al. \n[18] found that upregulated histone Kla levels enhance \nliver metastasis of colorectal cancer. Conversely, Kla can \nalso exert tumor-suppressive effects. Longhitano et  al. \n[19] reported that lactate increases the levels of lactate-\ninduced histone H3K18la modification, significantly \nreducing the proliferation and migration of uveal mela -\nnoma cells.\nTherefore, accurately identifying Kla sites is crucial for \nunderstanding their biological functions and unraveling \nthe mechanism of Kla in various diseases. Liquid chro -\nmatography–mass spectrometry (LC–MS/MS) is a pow -\nerful technique for high-throughput and high-precision \nidentification of Kla sites. However, its application is \nconstrained by high sequencing costs and time con -\nsumption. To fill this gap, a series of computational mod-\nels have been developed [20]. For example, Jiang et  al. \n[21] proposed an ensemble deep learning (DL) model, \nFSL-Kla, to identify Kla sites, achieving an AUC value \nof 0.889. Additionally, Lv et  al. [22] first leveraged LC–\nMS/MS generated global lactylome profile to construct \nan attention-based Kla site prediction tool, DeepKla, \nwhich achieved an AUC value of 0.972 on independent \ndatasets, demonstrating robustness and transferability. \nBased on the same dataset, Guan et  al. [23] developed \nDeepKlapred to identify Kla sites in rice via multi-view \nfeature fusion. Similarly, tools such as Auto-Kla [24] and \nA/EBFF-Kla [25] were developed based on recently pub -\nlished Kla profiles to accurately predict human Kla sites. \nDespite significant progress in Kla site identification, cur-\nrent models still have room for improvement in general -\nizability and accuracy.\nIn this study, we introduced a novel tool, PBertKla, \nbased on a protein large language model for precise iden -\ntification of human Kla sites. We first curated a reliable \nhuman Kla site benchmark dataset with appropriate sam-\nple length and similarity threshold. Furthermore, we fine-\ntuned ProteinBERT, a deep language model pretrained on \nlarge-scale protein sequences and Gene Ontology (GO) \nannotation knowledges [26], on our Kla site dataset using \noptimal hyperparameter to build PBertKla. Comprehen -\nsive experimental results demonstrated that PBertKla \nachieved consistently high predictive performance (AUC \nvalue exceeding 0.880) across both training and inde -\npendent validation data. Feature visualization analysis \nconfirmed the effectiveness of PBertKla’s large-model \nstrategy for Kla site identification. To further validate its \nperformance, we compared PBertKla with other state-of-\nthe-art models, highlighting its superiority across inde -\npendent validation datasets from diverse sources. We \nbelieve that PBertKla will facilitate the overcoming of \nobstacles in identifying Kla sites, thereby advancing the \nin-depth exploration of its pivotal role in cellular func -\ntion regulation.\nResults\nCuration of human Kla benchmark dataset\nGiven that the bias of amino acids specific to their posi -\ntions relative to Kla site, it was of considerable impor -\ntance to select an appropriate window size for flanking \nsequences around Kla sites and to characterize Kla sam -\nples well. On the other hand, an unsuitable sequence \nsimilarity threshold of CD-HIT would compromise both \nsample distribution and model performance by either \nfailing to eliminate redundant sequences (high threshold) \nor reducing sequence diversity (low threshold). To create \na reliable and practical benchmark dataset and a strong \npredictive model for effectively identifying human Kla \nsites, it required determining the proper sample length \nand sequence similarity of human Kla peptide samples, \nwhich was implemented based on a preliminary analysis \nof their impact on prediction performance.\nThis thorough analysis experiment involved grid search \nover sample length ranging from 33 to 59 (with an incre -\nment of 2 amino acids) and sequence similarity thresh -\nold ranging from 30 to 70% (with a 10% increment). We \nanalyzed the predictive performance of each dataset with \nspecific sample length and sequence similarity using Pro -\nteinBERT model with default parameters. The prediction \nAUC values obtained on independent validation datasets \nwere summarized in Fig. 1 and Additional file 1: Table S1. \nAUC values of all datasets fluctuated between 0.765 and \nPage 3 of 13\nLai et al. BMC Biology           (2025) 23:95 \n \n0.822, indicating a marginal difference of approximately \n6%. Thereinto, datasets with sample length of 45 yielded \nAUCs ranging from 0.787 to 0.822, and AUCs of datasets \nwith 30% sequence similarity ranged from 0.765 to 0.822. \nAlthough the precise reasons for this variability remain \nunclear, it is evident that ill-suited data resulted in less \nstable model performance. To optimize computation effi-\nciency and facilitate comparison, the dataset with sample \nlength of 45 and 30% sequence similarity, including 9640 \nKla and non-Kla peptide samples and demonstrating \nsuperior performance over all other datasets, was thus \ndetermined as the human Kla benchmark dataset used \nfor all subsequent analyses in this study, unless otherwise \nspecified.\nTo intuitively examine the amino acid preferences \naround lactylated sites, we utilized Two-Sample-Logo \n[27] to generate sequence maps of fragments surround -\ning both Kla and non-Kla sites, thereby comparing the \namino acid distributions between positive and negative \nsamples. The analysis focused on 45-residue fragments \ncentered on Kla sites (positions − 22 to + 22 relative \nto the central lysine). As shown in Fig.  1D, the analysis \nrevealed an enrichment of lysine (K) and alanine (A) in \nboth the upstream and downstream regions of lactyla -\ntion sites, while glycine (G) exhibited a downstream-spe -\ncific preference. These findings indicate specific amino \nacid preferences around lactylation sites, supporting the \ninterpretation of these regions as contextually meaning -\nful (“semantic”) sequences.\nDetermination of PBertKla predictor\nIn view of parameter tuning, especially the learning rate \nand batch size hyperparameters, in DL models is critical \nfor achieving efficient convergence, stable training, and \nwell generalization performance [28]. Generally, to bal -\nance training speed and model accuracy, hyperparam -\neter fine-tuning could be effectively conducted by grid \nsearch. Here, to improve the predictive performance of \nthe pretrained protein language model for human lysine \nlactylation (Kla) site identification, we first conducted \nsystematic hyperparameter optimization through a grid \nsearch evaluating 36 combinations of learning rates (six \nvalues) and batch sizes (six values). Furthermore, to \nsystematically evaluate the impact of sequence length \nparameter on Kla identification task, we performed a \ncomprehensive analysis by varying sequence length and \nassessing model’s predictive capability on the validation \ndata. The specific search space of three parameters was \nenumerated in Additional file 1: Table S2. The Acc, MCC, \nand AUC evaluation metrics were employed to deter -\nmine the optimal parameter settings.\nThe human Kla site prediction performance of differ -\nent parameters on the validation data was demonstrated \nin Fig.  2. For different learning rates and batch sizes, \nFig. 1 The determination of optimal sample length and sequence similarity threshold for defining human Kla benchmark datasets. A The line \nchart showing each AUC value of each dataset with specific sample length and sequence similarity, in which x axis is sample length, different \ncolored lines correspond to different sequence similarity thresholds. B–C Two violin plots visualizing the AUC values of datasets generated based \non different sequence similarity thresholds of CD-HIT (corresponding to different colors) and different sample lengths (corresponding to different \ncolors), respectively. D Sequence characteristics of Kla and non-Kla sites in the training data\nPage 4 of 13Lai et al. BMC Biology           (2025) 23:95 \nthe values of Acc, MCC, and AUC metrics (as shown \nin Fig.  2A–C) were 70.3 ~ 78.8%, 0.407 ~ 0.576, and \n0.789 ~ 0.874, respectively. The optimization of these two \nhyperparameters would achieve about 9% increase in Acc \nand AUC, as well as an ~ 17% improvement in MCC. As \nillustrated in Fig.  2D, there was a gradual decline trend \nin model’s performance with the batch size increasing \nfrom 4 to 128. Referring to Fig.  2E, we observed an obvi -\nous positive trend in both Acc and MCC as well as AUC \nwith learning rate less than 0.005, and a downward trend \nwith learning rate more than 0.005. Regarding sequence \nlength (Fig. 2F), analysis results revealed that Acc, MCC, \nand AUC all improved as sequence lengths approach -\ning 256, but began to decline for sequences longer than \nFig. 2 The parameter optimization of the pretrained protein language model for accurately predicting human Kla sites. A–C The heatmap \nof learning rate (x-axis) and batch size (y-axis) optimization in terms of Acc, MCC, and AUC metrics, respectively. Brighter yellow blocks correspond \nto higher metric values in the heatmap. D–F representing the performance of different learning rates, batch sizes, and sequence length. Yellow, \ngreen, and red lines indicate Acc, MCC, and AUC metrics. G ROC (receiver operating characteristic) curve and H PRC (precision-recall) curve plotting \nthe overall prediction performance of PBertKla with the optimal parameter setting on the validation data (green line) and independent validation \ndata (red line), respectively\nPage 5 of 13\nLai et al. BMC Biology           (2025) 23:95 \n \n256. Finally, the optimal learning rate, batch size, and \nsequence length were set to 0.0005, 4, and 256, respec -\ntively, corresponding to the model with the highest AUC \nvalue of 0.883. Through the above exploration and analy -\nsis, the best parameter configuration was determined \nto construct PBertKla model for identifying human \nKla sites. On the independent validation data, PBert -\nKla achieved a prediction accuracy of 80.3%, sensitiv -\nity of 78.7%, specificity of 82.0%, and MCC, AUC, and \nAUPRC values of 0.607, 0.884, and 0.866, respectively \n(Fig. 2G–H).\nFeature visualization analysis of PBertKla training process\nTo intuitively illustrate the feature learning process on \nhuman Kla sequence samples and investigate the clas -\nsification ability of PBertKla, we further visualized the \ngradually changing feature maps of the training dataset \nin a 2-dimensional (2D) space using Uniform Manifold \nApproximation and Projection (UMAP) [29]. Figure  3 \nshows the sample distribution in each feature space dur -\ning the training process of PBertKla, in which each point \nrepresented each sample, Kla sites were labeled with red \ncolor while non-Kla sites with blue color.\nAs seen from Fig.  3A, all samples were mixed, indicat -\ning that model could not distinguish human Kla from \nnon-Kla samples based on the original features generated \nfrom embedding layer. When mapping the data to the \nlow-dimensional space on the basis of the output features \nof six transformer-like blocks, these two types of samples \nbecome more dispersed (Fig.  3B–G). After merging all \nlocal and global representations through a concatenation \nlayer, a relatively obvious boundary appeared between \nKla and non-Kla samples, with Kla samples predomi -\nnantly distributed on the right and non-Kla samples on \nthe left (Fig.  3H). This suggested that the key features \ndetermining PBertKla’s classification performance were \nlikely encoded in this layer, highlighting the effectiveness \nof hybrid DL architectures. Overall, these feature visu -\nalization observations further illustrated the capacity of \nPBertKla model in capturing potential information for \nthe identification of human Kla sites.\nPerformance of manual feature engineering on human Kla \nrepresentation\nFor comparison, this section delved into the exploration \nof Kla sample representation with manual feature engi -\nneering. As reported by literature [30], the fusion scheme \nof seven classical feature encodings could characterize \ndifferent types of PTM sites well. These comprehensive \nfeatures would be taken into consideration to represent \nhuman Kla sites. Specifically, we extracted the sequence \ninformation of our human Kla benchmark dataset using \nposition weight amino acid composition (PWAAC), \namino acid relative position composition (AARPC), com-\nposition of k-space amino acid pairs (CKSAAP), com -\nposition of physical and chemical properties (CPCP), \nFig. 3 The UMAP-based visualization of human Kla and non-Kla site sample distribution in 2D feature space. A The original feature map for input \nsequences originating from embedding transformation, B–G the feature maps originating from six transformer-like blocks, H the feature map \noriginating from the integration of all blocks. Red and blue dots denoting human Kla and non-Kla site samples, respectively\nPage 6 of 13Lai et al. BMC Biology           (2025) 23:95 \nencoding based on grouped weight (EBGW), composi -\ntion, transition and distribution (CTD), and cone fea -\nture space (CFS) feature descriptors. Meanwhile, a \nsimple end-to-end manner was adopted to integrate \nthese features for multi-information fusion. As a result, \n3080-dimensional feature vectors were produced for \nencoding Kla and non-Kla peptide samples. In this work, \nwe applied both the base (“_L1”) and stacking (“_L2”) \nmodes of PyTorch and fastai v1 neural network (NN) \nmodels to distinguish human Kla from non-Kla sites.\nThese models were learned on the training dataset and \nwere assessed using the independent validation dataset. \nPerformance evaluation results were presented in Fig.  4 \nand Additional file 1: Table S3. Based on the seven hand -\ncrafted features, NN models could identify human Kla \nsites with Acc of 72.0 ~ 79.6%, MCC of 0.445 ~ 0.595, and \nAUC of 0.808 ~ 0.882. As illustrated in Fig.  4, the stack -\ning NN models performed better than base NN models, \nthe Acc, MCC, and AUC values of NeuralNetFastAI_L2 \n(green) were higher than that of NeuralNetFastAI_L1 \n(yellow), and the same phenomenon was observed \nbetween NeuralNetTorch_L2 (red) and NeuralNetTorch_\nL1 (blue). The PyTorch models were more effective than \nfastai v1 models in identifying human Kla sites, and Neu -\nralNetTorch_L2 performed the best and achieved the \nhighest accuracy, MCC, and AUC of 79.6%, 0.595, and \n0.882, respectively. Overall, the multi-view feature fusion \nstrategy for distinguishing human Kla sites from non-\nKla sites achieved maximum values of 79.6% Acc, 0.595 \nMCC, and 0.882 AUC, which underperformed compared \nto PBertKla’s results of 80.3% Acc, 0.607 MCC, and 0.884 \nAUC (as shown in Fig.  4A–B). All analysis results indi -\ncated that this protein language model incorporating \nlocal and global representations constituted an effective \napproach for capturing Kla site-specific information.\nComparison of PBertKla with existing methods\nFor further assessing the effectiveness of our pro -\nposed PBertKla, we conducted a comparative analysis \nagainst state-of-the-art Kla prediction methods. Due to \nFig. 4 Performance evaluation of handcrafted features for representing human Kla sites based on neural network models. A The Acc, MCC, \nand AUC metrics, as well as B ROC and C PRC curves of the four neural network algorithms and our proposed PBertKla method (in red color)\nPage 7 of 13\nLai et al. BMC Biology           (2025) 23:95 \n \nchallenges in accessing or implementing FSL-Kla and \nA/EBFF-Kla, our comparison was limited to DeepKla \n[22], Auto-Kla [24], and DeepKlapred [23]. To ensure \nequitable comparison of algorithmic performance \ncharacteristics, the adoption of standardized training \nand validation protocols across all models on the same \ncurated benchmark datasets was methodologically \nimperative. Thus, we learned DeepKla, AutoKla, and \nDeepKlapred models on our human Kla training data to \npredict the independent data. Independent validation \nresults (Fig.  5 and Additional file  1: Table  S4) showed \nthat the human Kla site prediction accuracy, MCC, \nand AUC values of DeepKla were 76.1%, 0.533, and \n0.853, those of AutoKla and DeepKlapred were 72.5%, \n0.486, 0.837 and 70.9%, 0.417, 0.767. PBertKla demon -\nstrated consistent superiority over DeepKla, AutoKla, \nand DeepKlapred models across most evaluation met -\nrics, achieving relative improvements of approximately \n4–9% in accuracy, 7–19% in MCC, 3–12% in AUC, and \n2–11% in AUPRC, respectively (as depicted in Fig.  5A, \nC–D). Overall, our introduced PBertKla demonstrated \nan outstanding performance than other tools in pre -\ndicting human Kla sites.\nTo evaluate the robustness and generalization ability \nof a model, independent testing datasets were usually \nutilized to confirm the prediction performance. Con -\nsequently, we established another independent testing \ndataset covering 2488 human Kla peptides derived from \nOSCC (oral squamous cell carcinoma) and NA (nor -\nmal adjacent) tissues for further assessing the predictive \ncapability of the above tools. As expected, the superior \nperformance of PBertKla was sustained into the testing \nphase, and an apparent discrepancy in Kla identification \naccuracy presented in Fig. 5B. The PBertKla (in red color) \naccurately identified 2164 Kla sites, achieving the highest \nAcc value of 90.6% (2255/2488). Meanwhile, DeepKla (in \ngreen color), AutoKla (in yellow color), and DeepKlapred \n(in purple color) underperformed with accuracy of 79.1% \n(1968/2488), 65.5% (1608/2455), and 86.5% (2151/2488), \nrespectively. All consistent results highlighted the PBert -\nKla’s excellent prediction capability and robust transfer -\nability, and further affirmed the architectural superiority \nFig. 5 Performance comparison between PBertKla and existing tools in identifying human Kla sites. A, C–D The prediction Acc, MCC, and AUC \nmetrics, as well as ROC and PRC curves of PBertKla (in red color), DeepKla (in green color), AutoKla (in yellow color), and DeepKlapred (in purple \ncolor) on the independent validation dataset. B The human Kla site identification accuracy of each tool on the independent testing dataset\nPage 8 of 13Lai et al. BMC Biology           (2025) 23:95 \nof the pretrained protein large language model in pre -\ndicting human Kla sites.\nDiscussion\nLysine lactylation (Kla), an important type of protein \nPTMs, plays a key regulatory role in cellular metabolism, \ninflammatory response, and the onset and progression of \nvarious diseases. More and more researches pay atten -\ntion to explore the application potential of Kla modifica -\ntions as biomarkers for disease prognosis [31], therapy \n[32], drug-effect monitoring [33], and so on. However, \nthe accurate identification of lactylated proteins and \nexact localization of lactylated amino acid residues are \nessential for revealing the molecular mechanisms and \nfunctional diversity of Kla modifications. Currently, aside \nfrom time-consuming and labor-intensive experimental \nmethods, only a handful of computational tools are avail -\nable for the convenient and rapid detection of human Kla \nsites.\nGiven the current scarcity of effective models for auto -\nmatically identifying human Kla sites, we proposed a \nnovel human Kla site prediction approach in this work. \nA reliable benchmark dataset is essential for developing \nan efficient and stable classifier, serving as the founda -\ntion that significantly enhances the overall performance \nof the model. Therefore, we firstly conducted a prelimi -\nnary analysis to determine the optimal and representative \nhuman Kla benchmark dataset with appropriate sample \nlength of 45 and sequence identity of 30%. Secondly, we \nemployed a pretrained protein language model to learn \nfrom the above-mentioned human Kla peptide sequence \ndata, resulting in the development of the PBertKla model \nfor identifying human Kla sites. This model enabled the \ncapture of both local (character level) and global (whole \nsequence level) representations of human Kla peptides by \nincorporating the classic BERT and Transformer mod -\nules. It exhibited impressive and robust prediction per -\nformance on both independent validation and testing \ndatasets, achieving accuracies of about 80.3% and 90.6%, \nrespectively. Moreover, the comparative experiment find-\nings demonstrated that PBertKla exceeded the perfor -\nmance of current Kla predictors in identifying human Kla \nsites. Owning to the limitation of data and calculation, \nthe potential for our proposed method to identify Kla \nsites in other species remains unexplored in this research.\nFurthermore, the following aspects on automatically \npredicting human Kla site warrant further investiga -\ntion. First, with the growing popularity of MS/LC–MS \ntechnology and the significantly increasing focus on lac -\ntylation, there is a rapid expansion of human lactylation \ndata. To fully leverage these and information, a broad \nrange of data from databases and published literature can \nbe systematically collected in the future to construct a \nmore comprehensive and standardized dataset of human \nlactylation sites. Second, to date, no research has thor -\noughly examined the effectiveness of different traditional \nmachine learning (ML) algorithms and conventional \nfeature engineering based on amino acid sequences in \ndistinguishing human lactylation from non-lactylation \nsites. The potential of ML-based human Kla site predic -\ntors merit additional exploration. Third, it would be ben -\neficial to investigate the development of multi-scale deep \nprotein language learning models for the interpretable \nprediction of Kla sites. This model should incorporate \nprotein structural data, evolutionary conservation infor -\nmation, and other relevant features into advanced deep \nlearning approaches [34, 35], such as the recently pub -\nlished ESM3 model [36]. By integrating these diverse data \nsources, the model would provide a more comprehensive \nunderstanding of the biological context and improve the \naccuracy and interpretability of Kla site prediction. Lastly, \nto enhance accessibility for researchers in this field, we \npropose developing an advanced online platform. This \nplatform would offer tools for data exploration, facilitate \neasy downloads, and provide human (or even potentially \nacross various species) lactylation modification predic -\ntion services while also supporting comprehensive analy -\nsis of prediction results from different models.\nIn addition, extending studies beyond human data \nshould be performed to analyze lactylation in different \nspecies, which could reveal conserved mechanisms and \nbroaden our understanding of its evolutionary signifi -\ncance. It would be meaningful to investigate the cross -\ntalk between lactylation and other modifications (e.g., \nphosphorylation) to uncover potential synergistic or \nantagonistic effects on protein function and gain a more \ncomprehensive understanding of protein regulation.\nConclusions\nIn this study, we developed PBertKla, a novel deep learn -\ning-based predictor for accurately identifying human \nlysine lactylation (Kla) sites. By curating a reliable bench -\nmark dataset and leveraging a pretrained protein lan -\nguage model, PBertKla effectively captures both local \nand global sequence representations through BERT and \nTransformer modules. Our model demonstrated out -\nstanding predictive performance, achieving AUC and \nAUPRC values of 0.884 and 0.866, respectively, on the \nindependent validation data. Comparative analyses fur -\nther confirmed its superior accuracy, robustness, and \ngeneralization ability over existing Kla site predictors. \nThe source code of PBertKla is publicly available to offer \na powerful approach for advancing lactylation research \nand its potential implications in disease mechanisms and \ntherapeutic strategies.\nPage 9 of 13\nLai et al. BMC Biology           (2025) 23:95 \n \nThe development of PBertKla provides a useful com -\nputational tool to complement experimental strategies \nfor lactylation site detection, facilitating further explo -\nration of lactylation modifications in health and disease. \nDespite its strong performance, there remain areas for \nfuture improvement. Expanding the dataset with newly \nidentified lactylation sites, incorporating structural and \nevolutionary information, and exploring multi-scale deep \nlearning architectures could further enhance predictive \npower. Additionally, integrating Kla prediction tools into \na user-friendly online platform would improve accessibil-\nity for researchers, enabling broader applications across \nmultiple species and fostering deeper insights into the \nbiological significance of lactylation.\nMethods\nBenchmark dataset\nThe primary raw data of this work was collected from \nthe literature [37], including 9275 Kla sites 2437 proteins \nidentified by LC–MS/MS experiments on 110 tumor \nand adjacent liver samples from 52 hepatitis B virus-\nrelated hepatocellular carcinoma (HCC) patients. The \nsequences of all human proteins were downloaded from \nthe Uniprot (Universal Protein Resource) database [38]. \nAccording to the location information of Kla sites and the \ncorresponding annotated protein sequences, we obtained \nthe positive samples, namely Kla peptides with length of \n2n + 1 and lactylated lysine residue located at the center. \nThe Kla sites with less than n upstream or downstream \nresidues were not taken into consideration in this work. \nSimilarly, the negative samples, with center of normal \nlysine residue, were extracted from all proteins without \nKla sites. Then, the CD-HIT, a widely used bioinformat -\nics program for clustering and comparing large number \nof protein or nucleic acid sequences at different sequence \nidentity levels [39, 40], was utilized to remove redundant \nsequences and avoid model overfitting. As most proteins \ndo not undergo lactylation modification, a considerable \nnumber of negative samples were obtained. To balance \nthe positive and negative samples, the equal amount of \nnonredundant negative samples was randomly selected \nout to be non-Kla dataset.\nTo build a reliable and practical benchmark data -\nset of human Kla site, we conducted a preliminary \nanalysis for determining the optimal sample length \nand the optimal sequence identity threshold of CD-\nHIT. As a result, the dataset with sample length of 45 \nand sequence identity of 30% was selected as the final \nbenchmark dataset, consisting of 4820 Kla and 4820 \nnon-Kla sequence samples. Notably, this study divided \nthe human Kla benchmark dataset into a training sub -\nset for learning the prediction models, comprising 3856 \npositives and 3856 negatives, and a validation subset \nfor independently testing the prediction performance, \ncontaining 964 positives and 964 negatives (Table  1). \nThis strategy guarantees a consistent and compara -\ntive evaluation of the models’ effectiveness. For further \nvalidating the human Kla identification ability of mod -\nels, a smaller LC–MS/MS experiment dataset, includ -\ning 2488 Kla sites from oral squamous cell carcinoma \n(OSCC) tissues and normal adjacent tissues (NATs) \n[41], was preprocessed to be independent testing data.\nThe protein large language model\nIn this work, we mainly made full use of ProteinBERT, a \ndeep language model specifically designed for proteins \n[26, 42], to learn on Kla and non-Kla sequence data \nfor automatically and accurately predicting Kla sites \n(Fig.  6). In this section, we would briefly describe the \nproposal of PBertKla, including the pretraining process \non a huge number of protein sequences and relevant \nannotation knowledge, the critical fine-tuning process \non human Kla benchmark dataset, as well as the archi -\ntecture and methodology.\nEncoding of protein/peptide sequence and annotation\nDuring pretraining and fine-tuning, protein and pep -\ntide sequences were encoded using 26 distinct inte -\nger tokens to represent 20 standard amino acids, \nselenocysteine (U), an undefined amino acid (X), \nanother amino acid (OTHER), and three special tokens \n(START, END, PAD). Thereinto, START and END \ntokens marked the beginning and end of each sequence, \nwhile PAD token was used to extend shorter sequences \nto match the minibatch length. START and END tokens \nalso aided in modeling proteins longer than the chosen \nlength by selecting a random subsequence, excluding at \nleast one end. The absence of the START or END token \nindicated that only part of the sequence was provided. \nMeanwhile, GO annotations were encoded as a binary \nvector of 8943 elements, with zeros everywhere except \nfor entries corresponding to the protein’s GO terms. In \nscenarios of lacking GO annotation data, such as dur -\ning fine-tuning or evaluation on human Kla bench -\nmarks, it was set as a zero vector.\nTable 1 The benchmark dataset of human lysine lactylation site\nDataset Sample type Sample number\nTraining data (80%) Positive 3856\nNegative 3856\nIndependent data (20%) Positive 964\nNegative 964\nPage 10 of 13Lai et al. BMC Biology           (2025) 23:95 \nThe learning of PBertKla on human Kla benchmark dataset\nTo learn comprehensive protein representations, Pro -\nteinBERT [26], a DL framework modeled after the Bidi -\nrectional Encoder Representations from Transformers \n(BERT) architecture [43], was pretrained on approxi -\nmately 106  M UniRef90 protein amino acid sequences \nalong with their associated GO annotations. We directly \nobtained the pretrained protein language model frame -\nwork, implemented in TensorFlow’s Keras [44]. The Pro -\nteinBERT framework was subsequently fine-tuned on our \ncurated human Kla benchmark dataset to learn PBertKla.\nThe fine-tuning protocol of PBertKla involved three \nstages: in the initial phase, all layers of the pretrained \nmodel were kept frozen, with training restricted solely \nto a newly incorporated fully connected layer for a maxi -\nmum of 40 epochs. Subsequently, all layers were unfro -\nzen, and the model underwent further training for up to \nan additional 40 epochs. In the final step, the model was \ntrained for one last epoch using a larger sequence length \n(setting to 1024). In addition, hyperparameter optimi -\nzation was conducted on the learning rate, batch size, \nand sequence length (i.e., the number of tokens encod -\ning the input sequences), the search space of which was \nenumerated in Additional file 1: Table S2. We trained the \nmodel across different parameter settings and assessed \nits performance on the validation data. The best-per -\nforming parameter configuration was selected to final -\nize the human Kla prediction model, namely PBertKla. \nIt should be noted that throughout the fine-tuning and \nperformance evaluation phases, no GO annotation data \nwas utilized, namely the GO-annotation input remained \na fixed all-zero vector. The entire procedure described \nabove was executed on a NVIDIA RTX 3090 GPU.\nFramework overview of PBertKla\nReferring to the pretrained protein large language model \n(ProteinBERT) [26], when fine-tuning on the Kla bench -\nmark dataset, the model receives two types of input \ninformation: (1) peptide sequences, represented as amino \nacid token sequences, (2) GO annotations, encoded as \nfixed-size all-zero vectors (as lack of GO annotation \ndata). Architecturally, it features two quasi-parallel sub -\nnetworks: one dedicated to local representations and the \nother to global representations (Fig.  6). The local repre -\nsentations are three-dimensional tensors with the shape \nB × L × dlocal , where B represents the batch size, L is \nthe sequence length within a mini-batch, and dlocal is the \nnumber of channels for the local representations (set -\nting to 128). The global representations are two-dimen -\nsional tensors of shape B × dglobal (setting to 512). In \nthe model’s initial layers, an embedding layer with dlocal \noutput features transforms input sequences into a local-\nrepresentation 3D tensor. This embedding layer is applied \nuniformly across each position. Meanwhile, a fully con -\nnected layer with dglobal output features converts input \nannotations into a global-representation 2D tensor.\nFig. 6 Schematic diagram of the proposed PBertKla method for identifying human Kla sites\nPage 11 of 13\nLai et al. BMC Biology           (2025) 23:95 \n \nThe model’s core architecture comprises six trans -\nformer-inspired blocks designed to process the local \nand global representations generated by the initial lay -\ners. Each block refines the local representations using 1D \nconvolutional layers, which integrate both narrow and \nwide convolutional layers to capture contextual depend -\nencies at varying scales—ranging from local to distant \nsequence positions. These convolutional operations are \nfollowed by a fully connected layer. Simultaneously, the \nglobal representations are updated through two fully \nconnected layers within each block. Every hidden layer \nin the model, including both fully connected and con -\nvolutional layers, employs GELU (Gaussian Error Linear \nUnit) as the activation function.\nIn each block, the local and global representations \ninteract exclusively through broadcast fully connected \nlayers (from global to local) and global attention lay -\ners (from local to global). The broadcast layers are fully \nconnected layers that convert the global representa -\ntion’s dglobal features into the local representation’s dlocal \nfeatures, then duplicate this representation across all L \nsequence positions.\nWhen PBertKla was trained on labeled human Kla \ndatasets, a new fully connected layer was added to its \noutput, connected to the local representations. This \ninvolved concatenating all the local normalization lay -\ners (two per block) with the output sequence, which \nwere then passed into the final fully connected layer. \nThis layer included a dropout layer and a sigmoid acti -\nvation function for binary classification labels. Overall, \nthe innovative integration of local and global represen -\ntations, combined with dual-input pretraining strategy, \nthis robust framework would achieve efficient and stable \nlearning outcome on Kla peptide sequences. By synergis -\ntically integrating local residue-level patterns with global \nfunctional embeddings through a dual-modality pre -\ntraining strategy, this architecture would achieve robust \ngeneralization capabilities, enabling efficient and stable \nlearning on Kla peptide sequence prediction tasks.\nOther deep learning models\nTo evaluate PBertKla, we also trained and tested several \nother DL models on our human Kla benchmark dataset. \nWe firstly employed ProtBERT [45], another BERT-based \nlarge protein language model, to predict human Kla sites. \nMoreover, the research of Lv et al. [30] indicated that the \nfeature engineering based on amino acid composition \ninformation, physicochemical property information, and \nspatial mapping information was effective to represent \nPTM sites. Therefore, we encoded human Kla and non-\nKla samples using the seven feature descriptors proposed \nby them, including PWAAC, AARPC, CKSAAP , CPCP , \nEBGW, CTD, and CFS. These features were then fused to \nfeed into DL models to predict human Kla sites. Here, we \nutilized the program package provided by Lv et  al. [ 30] \nto perform the aforementioned feature extraction. The \nAutoGluon, an open source software with automated \nmachine learning (AutoML) capabilities, was developed \nby Amazon team [ 46]. They provided the convenient \nand effective implementation for a series of ML and DL \nmodels, including PyTorch neural networks and fastai v1 \nneural networks, which were employed in this work to \nclassify human Kla sites.\nAdditionally, we evaluated three existing Kla prediction \ntools (including DeepKla, AutoKla, and DeepKlapred) \nto predict human Kla sites. All of these tools were devel -\noped based on deep learning (DL) architectures. The \narchitecture of DeepKla consisted of four closely con -\nnected subnetworks, including a word embedding layer, \nconvolutional neural network (CNN), bidirectional gated \nrecurrent units (BiGRU), and attention mechanism layer \n[22]. DeepKlapred integrated six biochemical/evolution -\nary sequence descriptors into a BiGRU-Transformer \nframework, with a cross-attention fusion mechanism \ndynamically bridging sequence embeddings and descrip -\ntor features to capture their synergistic interactions [ 23]. \nThe Auto-Kla model was mainly composed of adaptive \nembedding module, Transformer encoder module, and \nmulti-layer perceptron classifier module [ 24]. Independ-\nent validation datasets were accordingly tested on them \nfor performance comparison among all models.\nPerformance evaluation\nThe performance of each Kla prediction model is evalu -\nated using following common metrics, including accu -\nracy (Acc), sensitivity (Sn), specificity (Sp), and Matthews \ncorrelation coefficient (MCC) [ 47–51]. The formulas for \ncomputing these metrics as follows:\nwhere TP (true positive) and FP (false positive) are the \nnumbers of sequences correctly and incorrectly pre -\ndicted as Kla samples, respectively. TN (true negative) \nand FN (false negative) are the numbers of sequences \ncorrectly and incorrectly classified as non-Kla samples. \nROC (receiver operating characteristic) curves and PR \n(precision-recall) curves are also employed to visually \nrepresent the overall predictive performance, and the \nareas under ROC curve (AUC) and PR curve (AUPRC) \nare further calculated to quantify performance. All indi -\ncator values range from [0, 1], higher values indicating \nbetter prediction performance.\n(1)\nAccuracy= TP +TN\nTP +TN +FP+FN\nSensitivity= TP\nTP +FN\nSpeciﬁcity= TN\nTN +FP\nMCC = TP ×TN −FP×FN√(TP +FP)(TP +FN )(TN +FP)(TN +FN )\nPage 12 of 13Lai et al. BMC Biology           (2025) 23:95 \nAbbreviations\nPTM  Post-translational modification\nKla  Lysine lactylation\nHAT  Histone acetyltransferase\nHDAC  Histone deacetylase\nLC–MS/MS  Liquid chromatography–mass spectrometry\nDL  Deep learning\nGO  Gene Ontology\nUMAP  Uniform Manifold Approximation and Projection\nNN  Neural network\nML  Machine learning\nOSCC  Oral squamous cell carcinoma\nUniprot  Universal Protein Resource\nPWAAC   Position weight amino acid composition\nAARPC  Amino acid relative position composition\nCKSAAP  Composition of k-space amino acid pairs\nCPCP  Composition of physical and chemical properties\nEBGW  Encoding based on grouped weight\nCTD  Composition, transition and distribution\nCFS  Cone feature space\nAcc  Accuracy\nSn  Sensitivity\nSp  Specificity\nMCC  Matthews correlation coefficient\nAUC   Area under receiver operating characteristic (ROC) curve\nAUPRC  Area under precision-recall (PRC) curve\nSupplementary Information\nThe online version contains supplementary material available at https:// doi. \norg/ 10. 1186/ s12915- 025- 02202-1.\nAdditional file 1: PBertKla: a protein large language model for predicting \nhuman lysine lactylation sites, Tables S1–S4. Table S1 AUC values of data-\nsets across different sample lengths and sequence similarities. Table S2 \nThe search space for the three parameters of the protein large language \nmodel. Table S3 Performance of feature descriptors based on neural \nnetworks. Table S4 Performance comparison of PBertKla with other deep \nlearning models as well as existing tools.\nAcknowledgements\nWe are grateful to the anonymous reviewers for their insightful and construc-\ntive comments, which will be instrumental in improving the presentation of \nthis paper.\nAuthors’ contributions\nH.L. and H.D. designed this study, and prepared the initial and final drafts of \nthe manuscript. D.L. collected and analyzed the data, performed the research. \nM.Y., T.Z., X.L., Y.W., S.X., and F.H. prepared the methodology. H.Y., K.S., and F.D. \ncritiqued drafts, and approved the final manuscript. All authors have read and \nagreed to the published version of the manuscript.\nFunding\nThe work was supported by the National Natural Science Foundation of \nChina (62271120), Natural Science Foundation of Chongqing (CSTB2024N-\nSCQ-MSX1241), Science and Technology Department of Sichuan Province \n(2024ZYD0039), Health Commission of Sichuan Province (24CXTD11), Sichuan \nMedical Association (S23012), Chengdu Science and Technology Bureau \n(2022-YF05-01867-SN), and Health Commission of Chengdu (2024141).\nAll data generated or analysed during this study are included in this published \narticle, its supplementary information files and publicly available repositories. \nThe human lysine lactylation sites were collected from the National Omics \nData Encyclopedia (NODE, https://www.biosino.org/node) database under \naccession number OEP002852 (Yang Z, Yan C, Ma J, Peng P , Ren X, Cai S, et al. \nLactylome analysis suggests lactylation-dependent mechanisms of metabolic \nadaptation in hepatocellular carcinoma) [37] and the Proteomics Identifica-\ntion Database (PRIDE, https://www.ebi.ac.uk/pride/) via PXD047535 (Jing F, \nZhu L, Zhang J, Zhou X, Bai J, Li X, et al. Multi-omics reveals lactylation-driven \nregulatory mechanisms promoting tumor progression in oral squamous cell \ncarcinoma) [41]. The core codes for implementing ProteinBERT [26], DeepKla \n[22], DeepKlapred [23], and AutoKla [24] models referred to https://github.\ncom/nadavbra/protein_bert, https://github.com/linDing-group/DeepKla, \nhttps://github.com/GGCL7/DeepKlapred, and https://github.com/tubic/\nAuto-Kla. The data and source code supporting PBertKla are available in the \nfollowing repositories: https://github.com/laihongyan/PBertKla and https:// \ndoi. org/ 10. 5281/ zenodo. 15107 500.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 16 January 2025   Accepted: 31 March 2025\nReferences\n 1. Lee JM, Hammarén HM, Savitski MM, Baek SH. Control of protein stability \nby post-translational modifications. Nat Commun. 2023;14(1):201. https:// \ndoi. org/ 10. 1038/ s41467- 023- 35795-8.\n 2. Dao F, Xie X, Zhang H, Guan Z, Wu C, Su W, et al. PlantEMS: a compre-\nhensive database of epigenetic modification sites across multiple plant \nspecies. Plant Commun. 2024:101228.\n 3. Lin H. Artificial intelligence with great potential in medical informatics: a \nbrief review. Medinformatics. 2024;1(1):2–9.\n 4. Chen Y, Gao Q, Wang D, Zou X, Li X, Ji J, et al. An overview of research \nadvances in oncology regarding the transcription factor ATF4. Curr Drug \nTargets. 2025;26(1):59–72.\n 5. Lin Y, Lin P , Lu Y, Zheng J, Zheng Y, Huang X, et al. Post-translational modi-\nfications of RNA-modifying proteins in cellular dynamics and disease \nprogression. Adv Sci (Weinh). 2024;11(44):e2406318.\n 6. Huang Z, Guo X, Qin J, Gao L, Ju F, Zhao C, et al. Accurate RNA velocity \nestimation based on multibatch network reveals complex lineage in \nbatch scRNA-seq data. BMC Biol. 2024;22(1):290.\n 7. Zhu H, Hao H, Yu L. Identification of microbe-disease signed associations \nvia multi-scale variational graph autoencoder based on signed message \npropagation. BMC Biol. 2024;22(1):172.\n 8. Zhang D, Tang Z, Huang H, Zhou G, Cui C, Weng Y, et al. Meta-\nbolic regulation of gene expression by histone lactylation. Nature. \n2019;574(7779):575–80.\n 9. Dong M, Zhang Y, Chen M, Tan Y, Min J, He X, et al. ASF1A-dependent \nP300-mediated histone H3 lysine 18 lactylation promotes atherosclerosis \nby regulating EndMT. Acta Pharm Sin B. 2024;14(7):3027–48.\n 10. Moreno-Yruela C, Zhang D, Wei W, Bæk M, Liu W, Gao J, et al. Class I \nhistone deacetylases (HDAC1–3) are histone lysine delactylases. Sci Adv. \n2022;8(3):eabi6696.\n 11. Wu X, Xu M, Geng M, Chen S, Little PJ, Xu S, et al. Targeting protein \nmodifications in metabolic diseases: molecular mechanisms and targeted \ntherapies. Signal Transduct Target Ther. 2023;8(1):220.\n 12. Wang X, Liu Q, Yu H-T, Xie J-Z, Zhao J-N, Fang Z-T, et al. A positive \nfeedback inhibition of isocitrate dehydrogenase 3β on paired-box gene \n6 promotes Alzheimer-like pathology. Signal Transduct Target Ther. \n2024;9(1):105.\n 13. Ouyang J, Wang H, Huang J. The role of lactate in cardiovascular diseases. \nCell Commun Signal. 2023;21(1):317.\n 14. Yang Y, Luo N, Gong Z, Zhou W, Ku Y, Chen Y. Lactate and lysine lactylation \nof histone regulate transcription in cancer. Heliyon. 2024;10(21):e38426.\n 15. Li H, Sun L, Gao P , Hu H. Lactylation in cancer: current understanding and \nchallenges. Cancer Cell. 2024;42(11):1803–7.\nPage 13 of 13\nLai et al. BMC Biology           (2025) 23:95 \n \n 16. Manayalan B, Basith S, Shin TH, Wei L, Lee G. mAHTPred: a sequence-\nbased meta-predictor for improving the prediction of anti-hypertensive \npeptides using effective feature representation. Bioinformatics. \n2019;35(16):2757–65.\n 17. Wei L, He W, Malik A, Su R, Cui L, Manavalan B. Computational predic-\ntion and interpretation of cell-specific replication origin sites from \nmultiple eukaryotes by exploiting stacking framework. Brief Bioinform. \n2021;22(4):bbaa275.\n 18. Zhou J, Xu W, Wu Y, Wang M, Zhang N, Wang L, et al. GPR37 promotes \ncolorectal cancer liver metastases by enhancing the glycolysis and \nhistone lactylation via Hippo pathway. Oncogene. 2023;42(45):3319–30.\n 19. Longhitano L, Giallongo S, Orlando L, Broggi G, Longo A, Russo A, et al. \nLactate rewrites the metabolic reprogramming of uveal melanoma cells \nand induces quiescence phenotype. Int J Mol Sci. 2022;24(1):24.\n 20. Liu M, Li C, Chen R, Cao D, Zeng X. Geometric deep learning for drug \ndiscovery. Expert Syst Appl. 2024;240:122498.\n 21. Jiang P , Ning W, Shi Y, Liu C, Mo S, Zhou H, et al. FSL-Kla: a few-shot \nlearning-based multi-feature hybrid system for lactylation site prediction. \nComput Struct Biotechnol J. 2021;19:4497–509.\n 22. Lv H, Dao F-Y, Lin H. DeepKla: an attention mechanism-based deep neural \nnetwork for protein lysine lactylation site prediction. Imeta. 2022;1(1):e11.\n 23. Guan J, Xie P , Dong D, Liu Q, Zhao Z, Guo Y, et al. DeepKlapred: a deep \nlearning framework for identifying protein lysine lactylation sites via \nmulti-view feature fusion. Int J Biol Macromol. 2024;283(Pt 3):137668.\n 24. Lai F-L, Gao F. Auto-Kla: a novel web server to discriminate lysine \nlactylation sites using automated machine learning. Brief Bioinform. \n2023;24(2):bbad070.\n 25. Yang Y-H, Yang J-T, Liu J-F. Lactylation prediction models based on \nprotein sequence and structural feature fusion. Brief Bioinform. \n2024;25(2):bbad539.\n 26. Brandes N, Ofer D, Peleg Y, Rappoport N, Linial M. ProteinBERT: a universal \ndeep-learning model of protein sequence and function. Bioinformatics. \n2022;38(8):2102–10.\n 27. Vacic V, Iakoucheva LM, Radivojac P . Two Sample Logo: a graphical repre-\nsentation of the differences between two sets of sequence alignments. \nBioinformatics. 2006;22(12):1536–7.\n 28. Liu Y, Zhou Z, Cao X, Cao D, Zeng X. Effective drug-target affinity predic-\ntion via generative active learning. Inf Sci. 2024;679:121135.\n 29. McInnes L, Healy J, Melville J. UMAP: uniform manifold approximation \nand projection for dimension reduction. arXiv preprint. 2020;arXiv: 1802. \n03426.\n 30. Lv H, Zhang Y, Wang J-S, Yuan S-S, Sun Z-J, Dao F-Y, et al. iRice-MS: an \nintegrated XGBoost model for detecting multitype post-translational \nmodification sites in rice. Brief Bioinform. 2022;23(1):bbab486.\n 31. Li F, Si W, Xia L, Yin D, Wei T, Tao M, et al. Positive feedback regulation \nbetween glycolysis and histone lactylation drives oncogenesis in pancre-\natic ductal adenocarcinoma. Mol Cancer. 2024;23(1):90.\n 32. Zhang N, Zhang Y, Xu J, Wang P , Wu B, Lu S, et al. α-Myosin heavy chain \nlactylation maintains sarcomeric structure and function and alleviates the \ndevelopment of heart failure. Cell Res. 2023;33(9):679–98.\n 33. Chen Y, Wu J, Zhai L, Zhang T, Yin H, Gao H, et al. Metabolic regula-\ntion of homologous recombination repair by MRE11 lactylation. Cell. \n2024;187(2):294–311.\n 34. Abdelkader GA, Kim JD. Advances in protein-ligand binding affinity \nprediction via deep learning: a comprehensive study of datasets, data \npreprocessing techniques, and model architectures. Curr Drug Targets. \n2024;25(15):1041–65.\n 35. Jiang Y, Wang R, Feng J, Jin J, Liang S, Li Z, et al. Explainable deep hyper-\ngraph learning modeling the peptide secondary structure prediction. \nAdv Sci (Weinh). 2023;10(11):e2206151.\n 36. Hayes T, Rao R, Akin H, Sofroniew NJ, Oktay D, Lin Z, et al. Simulat-\ning 500 million years of evolution with a language model. Science. \n2025;387(6736):850–8.\n 37. Yang Z, Yan C, Ma J, Peng P , Ren X, Cai S, et al. Lactylome analysis suggests \nlactylation-dependent mechanisms of metabolic adaptation in hepato-\ncellular carcinoma. Nat Metab. 2023;5(1):61–79.\n 38. The UC. UniProt: the universal protein knowledgebase in 2023. Nucleic \nAcids Res. 2023;51(D1):D523–31.\n 39. Fu L, Niu B, Zhu Z, Wu S, Li W. CD-HIT: accelerated for clustering the next-\ngeneration sequencing data. Bioinformatics. 2012;28(23):3150–2.\n 40. Meng Q, Guo F, Tang J. Improved structure-related prediction for insuf-\nficient homologous proteins using MSA enhancement and pre-trained \nlanguage model. Brief Bioinform. 2023;24(4):bbad217.\n 41. Jing F, Zhu L, Zhang J, Zhou X, Bai J, Li X, et al. Multi-omics reveals \nlactylation-driven regulatory mechanisms promoting tumor progression \nin oral squamous cell carcinoma. Genome Biol. 2024;25(1):272.\n 42. Zulfiqar H, Guo Z, Ahmad RM, Ahmed Z, Cai P , Chen X, et al. Deep-STP: a \ndeep learning-based approach to predict snake toxin proteins by using \nword embeddings. Front Med (Lausanne). 2023;10:1291352.\n 43. Devlin J, Chang M-W, Lee K, Toutanova K. BERT: Pre-training of Deep \nBidirectional Transformers for Language Understanding. arXiv preprint. \n2019;arXiv:1810.04805.\n 44. Abadi M, Barham P , Chen J, Chen Z, Davis A, Dean J, et al. TensorFlow: a \nsystem for large-scale machine learning. Proceedings of the 12th USENIX \nconference on operating systems design and implementation; Savannah, \nGA, USA: USENIX Association; 2016. p. 265–83.\n 45. Elnaggar A, Heinzinger M, Dallago C, Rehawi G, Wang Y, Jones L, et al. \nProtTrans: toward understanding the language of life through self-super-\nvised learning. IEEE Trans Pattern Anal Mach Intell. 2022;44(10):7112–27.\n 46. Erickson N, Mueller J, Shirkov A, Zhang H, Larroy P , Li M, et al. Autogluon-\ntabular: robust and accurate automl for structured data. arXiv preprint. \n2020;arXiv: 2003. 06505.\n 47. Zou X, Ren L, Cai P , Zhang Y, Ding H, Deng K, et al. Accurately identify-\ning hemagglutinin using sequence information and machine learning \nmethods. Front Med (Lausanne). 2023;10:1281880.\n 48. Wang Y, Zhai Y, Ding Y, Zou Q. SBSM-Pro: support bio-sequence machine \nfor proteins. SCIENCE CHINA Inf Sci. 2024;67(11):212106.\n 49. Ai C, Yang H, Liu X, Dong R, Ding Y, Guo F. MTMol-GPT: de novo multi-tar-\nget molecular generation with transformer-based generative adversarial \nimitation learning. Plos Comput Biol. 2024;20(6):e1012229.\n 50. Liu B, Gao X, Zhang H. BioSeq-Analysis 2.0: an updated platform for \nanalyzing DNA, RNA and protein sequences at sequence level and \nresidue level based on machine learning approaches. Nucleic Acids Res. \n2019;47(20):e127.\n 51. Yan K, Lv H, Guo Y, Peng W, Liu B. sAMPpred-GAT: prediction of antimicro-\nbial peptide by graph attention network and predicted peptide structure. \nBioinformatics. 2023;39(1):btac715.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Biology",
  "concepts": [
    {
      "name": "Biology",
      "score": 0.9317651391029358
    },
    {
      "name": "Lysine",
      "score": 0.7144222259521484
    },
    {
      "name": "Computational biology",
      "score": 0.4443509578704834
    },
    {
      "name": "Evolutionary biology",
      "score": 0.3828289210796356
    },
    {
      "name": "Genetics",
      "score": 0.26216575503349304
    },
    {
      "name": "Amino acid",
      "score": 0.1509191393852234
    }
  ]
}