{
  "title": "Large Language Models and Multimodal Retrieval for Visual Word Sense Disambiguation",
  "url": "https://openalex.org/W4389523913",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5093006051",
      "name": "Anastasia Kritharoula",
      "affiliations": [
        "National Technical University of Athens"
      ]
    },
    {
      "id": "https://openalex.org/A4380024070",
      "name": "Maria Lymperaiou",
      "affiliations": [
        "National Technical University of Athens"
      ]
    },
    {
      "id": "https://openalex.org/A2047892088",
      "name": "GIORGOS STAMOU",
      "affiliations": [
        "National Technical University of Athens"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4323557043",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W4387356107",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4389519449",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W4385571957",
    "https://openalex.org/W3126337491",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W4223974161",
    "https://openalex.org/W3141280416",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W4385570350",
    "https://openalex.org/W4385571124",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W4385570655",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W4378501037",
    "https://openalex.org/W4297801719",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W4319793302",
    "https://openalex.org/W2768348081",
    "https://openalex.org/W4379987198",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2080133951"
  ],
  "abstract": "Visual Word Sense Disambiguation (VWSD) is a novel challenging task with the goal of retrieving an image among a set of candidates, which better represents the meaning of an ambiguous word within a given context. In this paper, we make a substantial step towards unveiling this interesting task by applying a varying set of approaches. Since VWSD is primarily a text-image retrieval task, we explore the latest transformer-based methods for multimodal retrieval. Additionally, we utilize Large Language Models (LLMs) as knowledge bases to enhance the given phrases and resolve ambiguity related to the target word. We also study VWSD as a unimodal problem by converting to text-to-text and image-to-image retrieval, as well as question-answering (QA), to fully explore the capabilities of relevant models. To tap into the implicit knowledge of LLMs, we experiment with Chain-of-Thought (CoT) prompting to guide explainable answer generation. On top of all, we train a learn to rank (LTR) model in order to combine our different modules, achieving competitive ranking results. Extensive experiments on VWSD demonstrate valuable insights to effectively drive future directions.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13053–13077\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nLarge Language Models and Multimodal Retrieval for Visual Word Sense\nDisambiguation\nAnastasia Kritharoula\nanaskrith@gmail.com\nMaria Lymperaiou\nmarialymp@islab.ntua.gr\nArtificial Intelligence and Learning Systems Laboratory\nSchool of Electrical and Computer Engineering\nNational Technical University of Athens\nGiorgos Stamou\ngstam@cs.ntua.gr\nAbstract\nVisual Word Sense Disambiguation (VWSD) is\na novel challenging task with the goal of retriev-\ning an image among a set of candidates, which\nbetter represents the meaning of an ambigu-\nous word within a given context. In this paper,\nwe make a substantial step towards unveiling\nthis interesting task by applying a varying set\nof approaches. Since VWSD is primarily a\ntext-image retrieval task, we explore the lat-\nest transformer-based methods for multimodal\nretrieval. Additionally, we utilize Large Lan-\nguage Models (LLMs) as knowledge bases to\nenhance the given phrases and resolve ambigu-\nity related to the target word. We also study\nVWSD as a unimodal problem by converting\nto text-to-text and image-to-image retrieval, as\nwell as question-answering (QA), to fully ex-\nplore the capabilities of relevant models. To tap\ninto the implicit knowledge of LLMs, we exper-\niment with Chain-of-Thought (CoT) prompting\nto guide explainable answer generation. On top\nof all, we train a learn to rank (LTR) model in\norder to combine our different modules, achiev-\ning competitive ranking results. Extensive ex-\nperiments on VWSD demonstrate valuable in-\nsights to effectively drive future directions.\n1 Introduction\nVisual word sense disambiguation (VWSD) is a\nrecently introduced challenging task where an am-\nbiguous target word within a given context has to\nretrieve the proper image among competitive can-\ndidates (Raganato et al., 2023). For example, the\nphrase andromeda tree contains the ambiguous tar-\nget word andromeda accompanied by the context\ntree which resolves this ambiguity. Out of the 10\ncandidates presented in Fig. 1, a VWSD frame-\nwork attempts to retrieve the ground truth image,\ndenoted with colored border.\nEven though VWSD is essentially a text-image\nretrieval task, there are some fundamental differ-\nences. First of all, the context given for an ambigu-\nous word is minimal, most often limited to a single\nFigure 1: An example of the VWSD task.\nword, upon which a retrieval module should rely\nto retrieve the proper candidate. Additionally, the\ncandidate images themselves pose significant chal-\nlenges; for example, by observing the candidates\nof Fig. 1, which are related to several meanings\nof the ambiguous word andromeda (can be either\na constellation, fish species, tree, reptile etc), a\nsuccessful retrieval system should be fine-grained\nenough and highly sensitive to the contextualiza-\ntion of the ambiguous target word. In that case,\nthe context word tree should be prominent enough\n-from the perspective of the retrieval module- to\nresolve ambiguity. At the same time, this retrieval\nmodule should not excessively rely on the tree con-\ntext: images containing flowers and green grass\ninduce some expected visual bias in the retrieval\nprocess, thus the image having the highest probabil-\nity of containing a tree may be selected, ignoring\nthe ambiguous andromeda attribute. Of course,\nthere is also the possible scenario that a retrieval\nmodel has never been trained on the ambiguous\nword at all: the rarity of the concepts present in the\ntarget word vocabulary increases the probability\nof exclusively relying on the well-known context\n13053\nword, resulting in significant randomness during\nselection. To this end, VWSD trustworthiness also\narises as a critical point, raising the need for ex-\nplainable solutions.\nIn this work, we showcase a vast variety of imple-\nmentations for VWSD. Multiple experiments are\nconducted for each of the implemented approaches,\nachieving one of the first extensive contributions to\nthis interesting task:\n• We exploit Large Language Models (LLMs)\nas knowledge bases to enrich given full\nphrases, so that the target word is disam-\nbiguated by incorporating more context, ad-\ndressing even cases that the ambiguous word\nis unknown to the retrieval module.\n• We convert VWSD to a unimodal problem:\nretrieval (text-to-text and image-to-image) and\nquestion-answering (QA) to fully explore the\ncapabilities related models have to offer.\n• Features extracted from the aforementioned\ntechniques are used to train a learning to rank\nmodel, achieving competitive retrieval results.\n• Chain-of-Thought (CoT) prompting is lever-\naged to guide answer generation, while reveal-\ning intermediate reasoning steps that act as\nexplanations for retrieval.\nOur code can be found at\nhttps://github.com/anastasiakrith/multimodal-\nretrieval-for-vwsd/.\n2 Related work\nText-image retrieval has been revolutionized\nsince adopting the popular Transformer framework\n(Vaswani et al., 2017) to further incorporate the vi-\nsual modality. This transition towards multimodal-\nity was addressed by incorporating one additional\ntransformer stream to process images (Tan and\nBansal, 2019; Lu et al., 2019), extending the BERT\n(Devlin et al., 2019) architecture. Most recent ap-\nproaches (Kim et al., 2021; Huang et al., 2021;\nWang et al., 2021) improved upon those primordial\nworks by utilizing a single encoder for both images\nand language, therefore minimizing the number of\ntrainable parameters, and consequently improving\nthe performance of several VL tasks, including mul-\ntimodal retrieval. A significant milestone was the\nadoption of contrastive learning for text-image rep-\nresentations, a technique that is followed by CLIP\n(Radford et al., 2021) and ALIGN (Jia et al., 2021).\nVisual Word Sense Disambiguation (VWSD)\n(Raganato et al., 2023) is only recently introduced\nas part of the SemEval 2023 challenge. So far, the\nconcurrent work of Dadas (2023) can act as a mea-\nsure of comparison to ours. We adopt and extend\nsome ideas presented in their paper, while further\nexpanding the experimental suite to cover variable\napproaches to the task. Moreover, we step upon the\nusage of LLMs for VWSD as in Kritharoula et al.\n(2023) to address both performance improvement\nand explainability aspects.\nLLMs as knowledge bases is a core idea fol-\nlowed throughout our paper, as enriching the short\nphrases of the VWSD dataset can facilitate target\nword disambiguation, and thus improve retrieval.\nTraditionally, knowledge enhancement for disam-\nbiguation was performed via knowledge graphs\n(Feng et al., 2020; Nedelchev et al., 2020). The\nusage of knowledge graphs was also favored for\nknowledge enhancement of multimodal tasks (Lym-\nperaiou and Stamou, 2022). Nevertheless, Large\nLanguage Models (LLMs) as knowledge bases\n(LLM-as-KB) (Petroni et al., 2019; AlKhamissi\net al., 2022) is a novel paradigm, presenting some\ninteresting capabilities compared to traditional\nknowledge graphs. Knowledge retrieval from\nLLMs is implemented via prompting (Liu et al.,\n2021), which attempts to appropriately trigger the\nLLM in order to provide the fact requested. To this\nend, recent VL breakthroughs favor the LLM-as-\nKB paradigm for knowledge enhancement, even\nthough some unresolved shortcomings may be in-\nherited (Lymperaiou and Stamou, 2023). We opt\nfor incorporating the LLM-as-KB paradigm within\nour experimentation to investigate performance\ngains over knowledge-free baselines, following\nKritharoula et al. (2023).\n3 Method\nWe followed 6 approaches to investigate the VWSD\ntask from several different perspectives. All our\napproaches were tested exclusively on English.\n1. Image-Text similarity baseline We imple-\nment a simple multimodal (VL) retrieval baseline\nto evaluate the capabilities of existing pre-trained\nVL transformers on the VWSD task. VL transform-\ners provide joint embedding representations for text\nphrases t and candidate images i, and the image\nrepresentation achieving highest cosine similarity\nscore score(t, i) = max(sim(t, i)) with respect to\n13054\nthe text embedding is selected. We utilize CLIP\nwith ViT (Dosovitskiy et al., 2021) base encoder, as\nwell as with ViT large encoder (denoted as CLIP-\nL). ALIGN (Jia et al., 2021) is also used for text-\nimage retrieval. We also leverage several versions\nof BLIP (Li et al., 2022), namely BLIPC and BLIP-\nLC (pre-trained on COCO (Lin et al., 2015) and\nusing ViT base/ViT large as backbone encoders\nrespectively), as well as BLIPF and BLIP-LF (pre-\ntrained on Flickr30k (Young et al., 2014)). More\ndetails are provided in Appendix G. We also ex-\nperiment with incorporating the penalty factor p(i)\ndescribed in Dadas (2023) to modulate the retrieval\npreference of images that present high similarity\nscores sim(t, i) to multiple phrases t. In this case,\nthe similarity score obeys to the following:\nscore(t, i) =sim(t, i) − p(i) (1)\n2. LLMs for phrase enhancementWe employ\na variety of LLMs as knowledge bases to enhance\nthe short phrases t with more detail in a zero-shot\nfashion (Kritharoula et al., 2023) and thus facil-\nitate VL retrieval described in the previous para-\ngraph. All prompts to LLMs presented in Tab. 1 are\ndesigned upon manually crafted templates, based\non the intuition that instructively requesting spe-\ncific information from the model has been proven\nto be beneficial (Kojima et al., 2023). We select\nLLMs up to the scale that our hardware allows, or\notherwise accessible via public APIs; specifically,\nGPT2-XL (1.5B parameters) (Radford et al., 2019),\nBLOOMZ-1.7B & 3B (Muennighoff et al., 2023),\nOPT-2.7B & 6.7B (Zhang et al., 2022), Galactica\n6.7B (Taylor et al., 2022), and the 175B parameter\nGPT-3 (Brown et al., 2020) and GPT-3.5-turbo 1\ncomprise our experimental set for phrase enhance-\nment. The difference in the number of parameters\nis viewed as significant in order to indicate whether\nscale matters to elicit disambiguation capabilities\nof LLMs. We denote as te the LLM-enhanced\nphrases. Similar to the baseline case, a penalty\nfactor p(i) can be included, adjusting the LLM-\nenhanced retrieval score as:\nscore(te, i) =sim(te, i) − p(i) (2)\n3. Image captioning for text retrievalWe lever-\nage the merits of unimodal retrieval by exploiting\nstate-of-the-art image captioning transformers to\n1https://platform.openai.com/docs/models/gpt-3-5\nPrompt name Prompt template\nexact “<phrase> ”\nwhat_is “What is <phrase>?”\ndescribe “Describe <phrase>.”\nmeaning “What is the meaning of <phrase>?”\nTable 1: Prompts for phrase enhancement via LLMs.\nconvert images i to textual captions ci. Specifically,\nthe captioning models used are BLIP Captions (Li\net al., 2022) with ViT-base encoder (BLIP-L Cap-\ntions denotes building upon ViT-large), as well as\nGiT (Wang et al., 2022) (with ViT-base) and GiT-L\n(with ViT-large). For all BLIP and GiT variants we\nattempt both beam-search multinomial sampling\nwith 5 beams to obtain k=10 captions per image\ni, as well as greedy search. We symbolize as ck\ni\nthe k-th caption for an image i, as obtained from\nbeam search (greedy search returns a single cap-\ntion). In the case of beam search, the 10 captions\nare post-processed, as some of them are identical\nor substrings of longer ones.\nWe explore two options in obtaining embedding\nrepresentations for the captions ci and the phrases\nt. In the first case, embedding representations are\nobtained using the same VL transformers as in mul-\ntimodal retrieval. In the second case, we utilize a\nvariety of purely textual sentence transformers that\nare fine-tuned for semantic similarity (Reimers and\nGurevych, 2019). Then, for both cases, we use co-\nsine similarity or euclidean/manhattan 2 distance to\ncalculate the score(t, ck\ni ), thus retrieving the most\nsimilar caption embedding to each phrase embed-\nding. Experiments with and without LLM-based\nphrase enrichment were conducted.\n4. Wikipedia & Wikidata image retrieval\nImage-to-image retrieval is another way to ap-\nproach the VWSD task via unimodal represen-\ntations. For this reason, following the idea of\n(Dadas, 2023) we exploit the Wikipedia API to\nretrieve all relevant articles corresponding to the\ngiven phrase t, and we keep the primary image iw\nfrom each article. Consequently, we post-process\nthe retrieved image set by considering a maximum\nof k=10 Wikipedia images per t. The same process\nis repeated for Wikidata (Vrandeˇci´c and Krötzsch,\n2014). We obtain embedding representations for\nthe retrieved imagesiw, as well as for the candidate\nimages i, using the same VL transformers as in mul-\n2Distance metrics are also denoted as ’similarity metrics’\nthroughout this paper\n13055\ntimodal retrieval. Finally, we search for the embed-\ndings lying closer together in the embedding space\n(using cosine similarity or euclidean/manhattan dis-\ntance) according to score(iw, i).\n5. Learn to Rank Similarly to Dadas (2023),\nwe implement a lightweight Learning to Rank\n(LTR) model that harnesses features extracted from\nour aforementioned experiments. LGBMRanker\nwith lambdarank objective3, implemented upon the\nLightGBM gradient boosting framework (Ke et al.,\n2017), is selected as the LTR module.\nThe selected input features for the LTR model\nrepresent relationships between each given phrase\nand the candidate images extracted from the pre-\nvious 4 approaches. Specifically, the following\nsteps (a)-(e) are selected to craft features corre-\nsponding to the baseline case. In a similar way,\nthe steps a-e are repeated for score(te, i) (LLM\nenhancement), score(t, ck\ni )/score(te, ck\ni ) (caption-\nphrase retrieval/enhanced caption-phrase retrieval)\nand score(iw, i) (image retrieval). We train the\nLTR module on several combinations of the de-\nsigned features; also, different similarity (cosine)\nand distance (euclidean/manhattan) scores are at-\ntempted within these combinations, while the con-\ntribution of considering p(i) is evaluated, both in\nthe baseline VL retrieval (eq. 1), as well as in the\nLLM-enhanced VL retrieval module (eq. 2).\n(a) score(t, i)\n(b) max(score(t, i))\n(c) mean(score(t, i))\n(d) difference a-b\n(e) difference a-c\nIn order to further advance LTR performance, we\nattempt to combine features from enriched phrases\nte derived from different LLMs.\n6. Question-answering for VWSD and CoT\nprompting Based on the zero-shot question-\nanswering setting of Kritharoula et al. (2023), we\nincorporate the given phrase t within a question\ntemplate, providing 10 answer choices from A to\nJ that correspond to extracted captions ci from the\ncandidate images i = A, B, ..., Jfor each t. In the\ncase that beam search was used during captioning,\nall k=10 captions for each i are concatenated and\nseparated by comma to form each answer choice.\n3LGBMRanker docs\nMoreover, based on Kojima et al. (2023) we utilize\nchain-of-thought (CoT) prompts to obtain expla-\nnations regarding the reasoning process the LLM\nfollows when selecting an answer. All resulting QA\nprompts (Tab. 2) are provided to GPT-3.5-turbo.\n4 Experimental results\nThe VWSD dataset is comprised of 12869 training\nsamples and 463 test samples; each sample con-\ntains 10 images (Appendix A). All our approaches\nare evaluated on the VWSD test split using Accu-\nracy and Mean Reciprocal Rank (MRR) metrics.\nLLMs for phrase enhancementIn Tab. 3, we\npresent results regarding LLM-based phrase en-\nhancement involving all VL retrieval models (with\nand without penalty p(i)). Baselines refer to VL\nretrieval with non-enhanced phrases t. Additional\nquantitative results including more LLMs, as well\nas qualitative examples of how knowledge enhance-\nment benefits disambiguation and thus retrieval are\nprovided in Appendix C. We can easily observe\nthat different prompts contribute towards better re-\nsults per LLM, while BLIP-LF is the most success-\nful VL retriever; however, ALIGN (with penalty)\nachieves top performance, together with GPT-3 and\n\"meaning\" as the prompt. In general, large scale\nLLMs (GPT-3, GPT-3.5-turbo) are able to clearly\nsurpass the respective non-enhanced baselines in\nmost cases.\nInterestingly, for smaller LLMs (up to 6.7B pa-\nrameters), scaling-up does not necessarily imply\nadvanced performance: OPT-6.7B (Tab. 12) and\nGalactica-6.7B (Tab. 13) fall below their smaller\nBLOOMZ-3B competitor when the same prompts\nare used. Nevertheless, contrary to GPT-3 and GPT-\n3.5-turbo, the phrase enhancement that few-billion\nparameter LLMs can offer in total is only marginal\nin the best case, and sometimes even fail to compete\nagainst their non-enhanced baselines, indicating\nthat they do not contain the necessary knowledge\nto enrich rare target words with respect to their\ngiven context. Therefore, our LLM-enhancement\nanalysis reveals that the necessary enrichment for\nVWSD may be only achieved when employing\nlarge-scale LLMs, most probably being on par with\nother emergent LLM abilities (Wei et al., 2023).\nImage captioning In Tab. 4 we present results\non text retrieval between extracted image captions\nci and given phrases t, which are achieved us-\n13056\nPrompt name QA Prompt template\nthink (greedy) “Q: What is the most appropriate caption for the <context>? Answer choices: (A) <caption for image 1>\n(B) <caption for image 2> ... A: Let’s think step by step. ”\nthink (beam) “Q: What is the most appropriate group of captions for the <context>? Answer choices: (A) <captions\nfor image 1 (separated with comma)> (B) <captions for image 2> ... A: Let’s think step by step. ”\nCoT “<think_prompt> <response of llm with think prompt> Therefore, among A through J, the answer is ”\nno_CoT (greedy) “Q: What is the most appropriate caption for the <context>? Answer choices: (A) <caption for image 1>\n(B) <caption for image 2> ... A: ”\nno_CoT (beam) “Q: What is the most appropriate group of captions for the <context>? Answer choices: (A) <captions\nfor image 1> (B) <captions for image 2> ... A: ”\nTable 2: QA prompts for gpt-3.5-turbo containing given phrase and extracted image captions.\ning Manhattan distance as a similarity measure.\nALIGN and XLM distilroberta base (Reimers and\nGurevych, 2019) perform the caption-phrase re-\ntrieval, representing textual representations via VL\nmodels and purely linguistic semantic similarity\nmodels respectively. More text-to-text retrieval re-\nsults in Appendix D. The No-LLM row of Tab. 4\nrefers to the case that no phrase enhancement is\nperformed, while the rest of the cases correspond\nto prompts designed as per Tab. 1 towards en-\nhanced phrases te. In all results presented, GPT-3\nis selected as the LLM to be prompted, as it demon-\nstrated superior knowledge-enhancement perfor-\nmance. We observe that LLM-based enhancement\noffers performance boosts to text-to-text retrieval\ncompared to the No-LLM baseline in most cases;\nnevertheless, it still stays behind the best perfor-\nmance so far (acc 72.57%, MRR 82.29%) achieved\nby GPT-3-enhanced VL retrieval. We assume this\nis because of the information loss induced when\nconverting from the visual to the textual modality\nduring captioning.\nAnother observation is that VL transformers\nperform better in producing textual embeddings\ncompared to sentence similarity embeddings, even\nthough the latter have been explicitly fine-tuned\non semantic textual similarity. This observation is\ncross-validated by examining more purely textual\nsemantic similarity transformers in Appendix D.\nWikipedia & Wikidata image retrievalIn Tab.\n5 results regarding image-to-image retrieval be-\ntween candidates i and web retrieved images iw\nare presented. Out of the 463 samples of the test\nset, Wikipedia API and Wikidata API returned re-\nsults for 460 and 324 phrases respectively. Even\nbest results for image-to-image retrieval are not\ncompetent against our previous approaches; we\nassume that exclusively visual representations are\nnot expressive enough to distinguish fine-grained\ndetails between semantically related candidates.\nLearn to rank In Tab. 6 we exhibit results using\nALIGN as the VL retriever. The presented feature\ncombinations involve the following: (1) Baseline\nfeatures: the choice for incorporation (or not) of\npenalty p(i) in score(t, i) for the VL retrieval; (2)\nLLM-enhancement features: the prompt to produce\nenhanced phrases te (or an ensemble of prompts\nleading to multiple te) and the choice for incorpora-\ntion (or not) ofp(i) in score(te, i); (3) Text retrieval\nfeatures: the captioner to generate ci, together with\nthe text embedding model and the similarity mea-\nsure (cosine/euclidean/manhattan) for text-to-text\nretrieval, as well as the phrase (original t, or en-\nhanced te, or an ensemble of enhanced phrases te\nderived from different LLMs); (4) Image retrieval\nfeatures: image embedding model and similarity\nmeasure (cosine/euclidean/manhattan) for image-\nto-image retrieval. Additional results for the LTR\nare provided in Appendix E.\nFor all the experiments of Tab. 6 we\nused the following hyperparameter configuration:\nn_estimators: 500, early_stopping: 100, learning\nrate: 0.03, feature_fraction: 0.25, max_bin: 100,\nmin_child_samples: 50 and reg_alpha: 0.05. An\n80-20 train/validation split was followed, allocating\n2514 samples in the validation set.\nThe ablations of feature combinations presented\nin Tab. 6 are highly informative, indicating that\ndifferent features pose a varying influence on the\nfinal result, while the amount of incorporated fea-\ntures is also significant. In general, incorporating\nLLM-based phrase enhancement in LTR is highly\nbeneficial, offering optimal metric results com-\npared to other feature combinations, or our other\napproaches presented in Tab. 3, 4. Overall best\nresults are achieved when including all features\n(colored instances of Tab. 6). This is an interesting\nobservation since standalone text retrieval (Tab. 4)\n13057\nCLIP CLIP-L ALIGN BLIP C BLIP-LC BLIPF BLIP-LF\nacc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR\nWith penalty\nBaseline 63.28 76.27 62.85 76.24 68.90 80.00 60.90 74.33 64.58 77.51 60.47 73.87 69.76 80.42\nOPT-2.7B\nexact 62.85 76.00 62.85 75.93 68.68 79.89 61.12 74.46 64.58 77.41 60.26 73.73 69.76 80.36\nwhat_is 60.98 74.85 66.30 78.10 63.28 75.95 60.91 74.43 66.31 77.86 57.24 71.15 67.60 78.58\ndescribe 61.05 74.75 66.08 78.14 64.79 77.62 61.77 74.73 66.31 77.57 57.67 71.48 68.03 79.03\nmeaning 62.15 75.60 65.25 77.45 65.66 77.54 61.99 75.35 63.93 76.88 58.32 71.65 65.44 77.69\nBLOOMZ-3B\nexact 61.26 74.59 62.99 76.18 66.52 78.36 60.48 73.13 63.28 76.00 57.02 71.23 65.66 77.49\nwhat_is 64.36 76.82 68.25 79.82 67.39 78.72 61.34 74.94 66.95 78.47 59.61 73.35 68.47 79.58\ndescribe 62.01 75.38 65.28 78.07 66.09 78.60 62.85 75.65 67.39 78.71 57.24 71.72 67.82 79.20\nmeaning 65.58 77.96 67.32 78.76 68.47 79.14 63.71 76.52 66.31 78.55 59.40 73.60 68.03 79.26\nGPT-3.5\nexact 58.86 72.09 60.18 72.73 62.42 74.43 57.02 70.78 59.18 72.32 52.92 67.40 63.07 74.65\nwhat_is 66.52 78.81 69.35 80.51 70.41 81.42 67.60 78.56 68.47 79.67 60.91 74.30 71.71 82.02\ndescribe 67.32 78.95 69.28 80.31 73.22 82.73 69.33 79.90 70.41 80.80 59.83 73.65 70.63 81.29\nmeaning 67.76 79.76 69.06 80.55 70.41 81.38 66.52 78.59 66.52 79.16 58.53 73.31 69.98 81.46\nGPT-3\nexact 61.98 74.90 64.07 76.58 66.52 78.37 60.48 73.99 64.15 76.58 59.61 72.91 65.23 77.06\nwhat_is 67.92 79.27 70.73 81.57 71.71 82.27 68.25 78.93 68.90 79.91 60.48 74.24 69.11 80.25\ndescribe 68.25 79.40 68.72 80.26 72.57 82.52 64.58 76.75 68.25 79.35 61.34 74.03 69.33 80.47\nmeaning 68.07 80.08 69.84 81.56 74.95 84.09 66.74 78.37 71.71 81.55 62.63 75.55 72.35 82.28\nWithout penalty\nBaseline 59.18 72.94 60.69 74.42 65.66 77.48 57.24 72.07 61.34 75.88 57.67 71.96 65.01 77.86\nOPT-2.7B\nexact 58.96 72.77 60.26 74.15 65.66 77.48 57.45 72.19 61.12 75.77 57.24 71.68 65.01 77.90\nwhat_is 58.31 72.91 62.75 75.47 61.12 73.94 59.83 73.13 61.12 74.54 53.35 68.71 63.50 76.22\ndescribe 59.08 72.95 63.89 76.31 62.20 75.80 59.83 73.28 62.20 75.17 54.43 69.86 63.28 76.28\nmeaning 58.19 72.97 62.99 75.79 64.58 76.48 59.18 73.38 60.26 74.70 54.86 69.43 62.42 75.86\nBLOOMZ-3B\nexact 56.93 71.53 59.52 73.78 63.93 76.15 58.10 71.77 59.61 74.06 54.86 69.66 61.12 74.99\nwhat_is 62.20 75.39 65.66 77.88 62.85 75.51 61.34 74.35 65.01 77.32 57.24 71.85 68.03 79.12\ndescribe 60.04 73.83 62.88 76.11 63.50 76.35 60.48 73.87 62.85 76.06 54.86 70.48 65.66 77.64\nmeaning 61.69 75.51 64.94 77.17 66.31 77.62 61.77 74.92 62.42 76.27 57.02 71.79 65.23 77.21\nGPT-3.5\nexact 56.89 69.85 57.11 70.36 60.48 72.15 54.43 68.33 56.80 70.42 51.40 65.68 58.32 71.11\nwhat_is 65.00 77.11 65.87 78.11 67.82 79.52 64.15 75.91 65.87 77.78 58.10 72.32 68.03 79.36\ndescribe 65.80 77.26 66.67 78.42 70.84 81.16 65.44 77.57 69.11 80.20 58.96 72.66 67.60 79.47\nmeaning 65.14 77.61 67.10 79.07 68.47 79.87 63.93 77.05 65.66 78.33 63.93 72.23 68.25 80.17\nGPT-3\nexact 59.88 73.38 61.68 74.91 64.79 76.27 58.96 71.92 60.48 74.02 55.72 70.34 62.42 75.04\nwhat_is 66.51 77.62 68.15 79.38 69.55 80.22 63.28 75.56 65.01 77.40 56.59 71.54 67.82 79.03\ndescribe 67.30 78.50 68.25 79.81 71.27 81.21 63.93 75.81 66.31 77.74 58.96 72.62 67.17 78.93\nmeaning 66.52 78.32 68.96 80.26 72.57 82.29 65.87 77.56 69.55 80.26 60.26 74.26 70.41 81.09\nTable 3: Results for zero-shot LLM-based enhancement. Colored instances denote overall best results per metric,\nwhile bold numbers indicate best results for each LLM.\nand image retrieval (Tab. 5) experiments did not\nprovide competitive metric results; nevertheless,\nconsidering the respective features in LTR training\nbenefits performance. Moreover, ensembling of\nfeatures is highly beneficial. This applies on both\nensembling the LLM-enhanced prompt features\n(e.g. all prompts combines features from texact,\ntwhat_is, tdescribe, tmeaning), as well as ensembling\nphrase features for text-to-text retrieval ( all te +\nt refers to combining features from all the afore-\nmentioned 4 enhancements, plus the original given\nphrase t). As demonstrated in Tab. 6, most ensem-\nble feature combinations help surpassing baselines\nand other implementations (Dadas, 2023).\nThe implemented LTR module is computation-\nally efficient, as it only requires a CPU for training,\nwhile achieving state-of-the-art performance (Tab.\n9). The clear pattern that arises from the ablation\n13058\nGreedy Beam\nBLIP BLIP-L GiT GiT-L BLIP BLIP-L GiT GiT-L\nacc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR\nNo LLM 40.60 59.82 48.38 64.71 44.92 62.19 48.60 65.30 46.65 63.48 54.64 69.92 45.36 62.87 54.00 68.47\nManhattan distance - ALIGN\nexact 44.06 61.16 50.32 64.51 45.14 61.40 50.76 65.63 46.87 63.94 53.13 68.22 46.22 62.88 53.35 67.79\nwhat_is 47.73 64.65 50.32 66.29 47.95 64.32 54.64 69.27 49.68 66.77 61.12 74.14 51.40 66.93 57.67 71.49\ndescribe 47.08 64.31 51.40 66.87 46.87 63.98 54.43 69.11 52.48 67.89 59.83 73.42 52.27 68.00 58.75 72.02\nmeaning 50.54 66.93 53.78 68.79 50.54 66.38 57.02 70.92 49.89 67.42 62.42 75.67 55.51 69.99 59.61 73.21\nManhattan distance - XLM distilroberta base\nexact 38.66 56.78 38.88 57.05 36.93 54.85 42.33 59.19 42.55 59.35 42.98 60.78 41.04 58.83 48.16 63.82\nwhat_is 41.68 58.96 39.09 57.36 41.68 58.12 43.20 60.53 44.92 62.06 45.36 63.27 42.12 59.99 50.11 65.65\ndescribe 39.96 58.52 42.98 59.69 41.04 58.48 45.57 62.19 43.41 60.34 46.44 63.53 44.06 61.57 48.60 64.99\nmeaning 41.47 59.13 41.04 59.03 39.96 57.64 44.28 62.26 61.79 61.76 47.73 64.63 45.57 62.30 53.13 67.83\nTable 4: Results on phrase-caption retrieval (with and without GPT-3 enhancement) for different captioning models.\nCLIP\nSimilarity Image source acc. MRR\nCosine Wikidata Images 34.26 50.13\nWikipedia Images 53.26 68.14\nEuclidean Wikidata Images 33.64 49.24\nWikipedia Images 52.17 66.95\nManhattan Wikidata Images 33.02 48.75\nWikipedia Images 52.82 67.25\nALIGN\nCosine Wikidata Images 31.11 47.84\nWikipedia Images 53.26 68.44\nEuclidean Wikidata Images 30.83 47.52\nWikipedia Images 53.48 68.40\nManhattan Wikidata Images 31.11 47.66\nWikipedia Images 53.26 68.27\nTable 5: Image-to-image retrieval results\nprovides an explicit direction for potential perfor-\nmance improvements, without such endeavors be-\ning computationally prohibiting.\nCoT prompting results can reveal the internal\nsteps followed by an LLM to reach an answer. In\nFig. 2, we present the candidate images for the\nphrase t \"rowing dory\", with candidate C serving\nas the golden image. Captions are extracted using\nGiT-L captioner with greedy search, thus returning\none caption ci per candidate i. We then transform\n\"rowing dory\" in a question Q, following the think\nprompt template, as presented in Tab. 2, with cap-\ntions ci, i= A, ..., Jforming the answer choices.\nConsequently, we produce answers via GPT-3.5-\nturbo with and without CoT, as demonstrated in\nTab. 7. In the presented case, CoT encourages\nchoosing the correct answer C over J, which is\npreferred by GPT-3.5-turbo when CoT is omitted.\nFurthermore, CoT elicits the reasoning process of\nthe LLM, thus outputting a detailed explanation\nregarding the choice of a candidate against seman-\ntically similar candidates; for example, C (a small\nboat in the water with two oars) and J (a row boat\nwith people on it in the water) are indeed closely\nrelated and can be easily confused. As one of the\nbasic VWSD challenges is the fine-grained discrim-\nination between semantically similar candidates,\nwe consider such LLM-derived explanations to be\nhighly informative and accurate for discerning the\nsubtle details between them. More CoT/no_CoT\nexamples are presented in Appendix F.\n5 Discussion\nAfter the announcement of the Semeval-2023 pro-\nceedings, more works achieving state-of-the-art\nresults on VWSD for English have been released.\nZhang et al. (2023b) utilize online dictionaries and\nbiencoder architectures for disambiguation and a\nmatching module that selects the most appropriate\nmodality to guide final retrieval. Their approach\nsignificantly differs from ours, both in the disam-\nbiguation and the ranking part. Patil et al. (2023)\nrely on an ensemble approach involving fine-tuning\nCLIP and extracting a joint image-text representa-\ntion. In their paper they make no usage of knowl-\nedge sources but rather resort on more computa-\ntionally heavy approaches; instead, in our approach\nwe only train a lightweight ranking module, while\nthe usage of computationally demanding modules\nsuch as VL retrievers and LLMs is restricted to\n13059\nBaseline LLM-enhance Text retrieval features Image retrieval feat. Metrics\np(i) Prompt p(i) Captioner Embedding Similarity Phrase Embedding Similarity Acc. MRR\n- - - - - - - - - 63.93 76.33\n- - - - - - - - 68.90 80.04\nmeaning - - - - - - - 73.22 82.79\nmeaning - - - - - - 75.16 84.13\nexact - - - - - - 70.41 81.10\nwhat_is - - - - - - 71.71 81.52\ndescribe - - - - - - 73.00 82.84\nall prompts - - - - - - 73.87 83.96\nall-except\nexact\n- - - - - - 74.30 83.80\nmeaning +\ndescribe\n- - - - - - 74.30 83.86\nall-except\nexact\n- - - - ALIGN manhattan 76.09 85.36\nall-except\nexact\n- - - - ALIGN cosine 76.52 85.29\nall prompts - - - - ALIGN cosine 76.52 85.70\nall prompts BLIP-L-beam ALIGN cosine t ALIGN cosine 77.61 85.90\nall prompts BLIP-L-beam ALIGN cosine all te + t ALIGN cosine 77.17 86.08\nall prompts BLIP-L-beam ALIGN cosine tmeaning ALIGN cosine 76.52 85.63\nall prompts BLIP-L-greedy ALIGN cosine all te + t ALIGN cosine 78.48 86.65\nall prompts GiT-L-greedy ALIGN cosine t ALIGN cosine 77.83 86.30\nall prompts GiT-L-greedy ALIGN cosine tmeaning ALIGN cosine 77.39 85.92\nall prompts GiT-L-greedy ALIGN cosine all te + t ALIGN cosine 79.35 87.23\nall prompts GiT-L-greedy ALIGN cosine all te + t ALIGN euclidean 76.96 85.85\nall prompts GiT-L-greedy ALIGN cosine all te + t ALIGN manhattan 76.96 86.00\nall prompts GiT-L-beam ALIGN cosine all te + t ALIGN cosine 76.96 85.92\nLTR of Dadas (2023) (best results) 77.97 85.88\nSemEval organizers’ baseline 60.48 73.87\nTable 6: LTR results using feature combinations as extracted from our previous 4 approaches (baseline, LLM\nenhancement, text retrieval, image retrieval). ALIGN is employed as the VL retriever.Colored instances denote\nbest results overall, while bold instances highlight instances that outperform best results of Dadas (2023).\nA\n B\n C (golden image)\n D\n E\nF\n G\n H\n I\n J\nFigure 2: Candidate images for the phrase \"rowing dory\".\ninference. Yang et al. (2023) create a large-scale\nknowledge base for VWSD based on online dic-\ntionaries to acquire the different senses for each\nambiguous word. They also train a computation-\nally heavy model in a contrastive setting to tackle\nVWSD challenges. Overall, our current work de-\n13060\nQ: What is the most appropriate caption for the rowing\ndory? Answer Choices: (A) a church with a tall tower and\na hedge. (B) an old airplane sitting on top of a runway.\n(C) a small boat in the water with two oars. (D) the men’s\nrowing team are competing in the men’s single sculls. (E)\na large black ship in the water near a harbor. (F) a ship is\non the water with people on it. (G) a large brick building\nwith a sign on it. (H) a blue fish (sardines) at the australian\naquarium. (I) a painting of four fish in the water. (J) a row\nboat with people on it in the water.\nno_CoT\nQ: What is the most appropriate [...]\nA: The most appropriate caption for the rowing dory would\nbe (J) a row boat with people on it in the water. X\nCoT\nQ: What is the most appropriate [...]\nA: Let’s think step by step.\nFirst, we need to identify the subject of the caption, which\nis the rowing dory. Then, we need to choose a descriptive\nand informative caption that accurately represents the im-\nage. Out of all the answer choices, (C) \"a small boat in\nthe water with two oars\" would be the most appropriate\ncaption for the rowing dory as it accurately describes the\nimage and provides relevant information about the boat.\nAnswers (D), (F), and (J) also provide some information\nabout the boat and the people in it, but they either include\nirrelevant information or are too vague to be an appropriate\ncaption for the image.\nTherefore, among A through J, the answer is (C) \"a small\nboat in the water with two oars.\"\nTable 7: VWSD as QA problem example, with CoT\nprompting encouraging the selection of the correct can-\ndidate. Questions Q are presented truncated.\nviates from the aforementioned ones in two main\naspects. First, following the inclination of the NLP\ncommunity towards LLMs we showcase the per-\nformance advancements that can be achieved in\nVWSD via prompting, replacing the usage of on-\nline dictionaries for sense disambiguation. With\nthe constant improvement of LLM capabilities we\ncan also expect more advanced phrase enrichment,\nand thus further boost VWSD performance. Sec-\nond, we do not train or fine-tune heavy models\nfor the ranking stage: our LTR module requires\n20 minutes training on a CPU, thus being a very\ncomputationally affordable option for retrieval.\nA potential combination of techniques men-\ntioned in Zhang et al. (2023b); Patil et al. (2023);\nYang et al. (2023) and ours can yield even more\nimpressive results for VWSD, as long as related\ncomputational considerations are relaxed.\n6 Conclusion\nIn this work, we provided an extensive founda-\ntional experimentation and analysis on the novel\nVisual Word Sense Disambiguation (VWSD) task.\nSpecifically, we employed several state-of-the-art\nmodels for VL retrieval to build strong baselines,\nshowcased the merits of enhancing ambiguous\nphrases with external knowledge stored in LLMs\nand achieved competitive ranking results by train-\ning a lightweight retrieval module using features\nextracted from our independent experiments. More-\nover, we obtained useful explanations that unveil\nthe reasoning process behind VWSD via Chain-of-\nThought prompting. Our results surpassed concur-\nrent implementations and given baselines, while\ndemonstrating valuable insights that can drive fu-\nture state-of-the-art implementations. We plan to\nexpand our experimentation towards exploring the\nuse of soft prompting to advance knowledge en-\nhancement, and explore the possibility of harness-\ning large-scale knowledge graphs for further en-\nrichment of ambiguous phrases. Finally, we view\nexplainability aspects of VWSD as a critical direc-\ntion to be studied as future work.\nAcknowledgments\nThe research work was supported by the Hellenic\nFoundation for Research and Innovation (HFRI)\nunder the 3rd Call for HFRI PhD Fellowships (Fel-\nlowship Number 5537).\nLimitations\nOur current work is accompanied with certain limi-\ntations, some of which we plan to address as future\nwork. First of all, due to limited computational\nresources, we do not resort to very large LMs (>7B\nparameters) in our machines; however, scaling-up\nwould probably provide advanced knowledge en-\nhancement for short phrases, an assumption that\ncan be strengthened by the advanced results occur-\nring when incorporating the 175B GPT-3 in our\nexperimental pipelines. The usage of GPT-3 and\nGPT-3.5-turbo was focused on a targeted experi-\nmental subset due to high pricing. On the other\nhand, with respect to this limitation, we chose to\ndistribute our efforts across varying approaches\nfor VWSD rather than focusing to delve into a\nspecific direction (e.g. LLM-based knowledge en-\nhancement) and explore the contribution of larger\nLMs. Moreover, under this resource limitation,\nour results can drive researchers that have access\nto limited computational resources to replicate and\nextend our analysis, instead of restricting this exper-\nimentation to institutions or individual researchers\n13061\nwith higher budgets.\nOther than that, the LLM-enhancement tech-\nnique faces the risks associated with hallucinations\nand untruthful generation, which cannot be easily\ndetected or resolved, according to related state-\nof-the-art research (Bang et al., 2023; Mündler\net al., 2023; Azaria and Mitchell, 2023; Zheng et al.,\n2023; Zhang et al., 2023a). Such a shortcoming\ncould negatively impact our results, since we have\nnot investigated whether there are hallucinated or\nuntruthful enhancements, especially since some\nphrases may require specific domain knowledge to\nbe evaluated (for example,andromeda treeis not an\neveryday term, and even a human needs to consult\nan encyclopedia to evaluate if a related enhance-\nment is correct). To the best of our knowledge,\nthere has been no open-source tool to accurately\nand faithfully detect hallucinations, therefore as for\nnow, we cannot resolve this limitation. However,\nwe could possibly mitigate hallucinations and un-\ntruthful generation by combining LLM knowledge\nwith knowledge graphs, which can be regarded as\nmore reliable knowledge sources for enhancing VL\ntasks (Lymperaiou and Stamou, 2023). We will\ninvestigate such hybrid techniques in future work.\nFinally, we have focused our experimentation\non the English language in order to develop a va-\nriety of techniques for VWSD rather than testing\nan applicable subset of them on other languages.\nNevertheless, we plan to address this limitation as\nfuture work.\nEthics Statement\nOur work involves a newly introduced publicly\navailable dataset released under the CC-BY-NC\n4.0 license and can be accessed by any researcher.\nThroughout this work, we adhered to the fair data\nusage policy, as required by the dataset creators4.\nWe employ language models up to 6.7B parameters\nwhich were run on a machine with a 16 GB GPU.\nSuch computational resources are rather easily ac-\ncessible by most research institutions; therefore,\nthroughout this paper we promote fair and repro-\nducible research, eliminating the need for high-end\ncomputational budget. Accessing larger models\nsuch as GPT-3 and GPT-3.5-turbo was feasible via\ntheir APIs, which do not impose computational lim-\nitations from the user’s side. The task itself does\nnot involve any obvious risks, as it targets to expand\nthe field of multimodal retrieval. Harnessing lan-\n4https://raganato.github.io/vwsd/\nguage models as knowledge bases imposes the risk\nof retrieving false or inaccurate information, which\nhowever does not induce serious implications in its\ncurrent usage, given the non-critical nature of this\ndataset. Overall, we do not anticipate any ethical\nissues arising from our work.\nReferences\nBadr AlKhamissi, Millicent Li, Asli Celikyilmaz, Mona\nDiab, and Marjan Ghazvininejad. 2022. A review on\nlanguage models as knowledge bases.\nAmos Azaria and Tom Mitchell. 2023. The internal\nstate of an llm knows when its lying.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nSławomir Dadas. 2023. Opi at semeval 2023 task 1:\nImage-text embeddings and multimodal information\nretrieval for visual word sense disambiguation.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. ArXiv, abs/1810.04805.\nAlexey Dosovitskiy, Lucas Beyer, Alexander\nKolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias\nMinderer, Georg Heigold, Sylvain Gelly, Jakob\nUszkoreit, and Neil Houlsby. 2021. An image\nis worth 16x16 words: Transformers for image\nrecognition at scale. In International Conference on\nLearning Representations.\nZhifan Feng, Qi Wang, Wenbin Jiang, Yajuan Lyu, and\nYong Zhu. 2020. Knowledge-enhanced named en-\ntity disambiguation for short text. In Proceedings of\nthe 1st Conference of the Asia-Pacific Chapter of the\nAssociation for Computational Linguistics and the\n10th International Joint Conference on Natural Lan-\nguage Processing, pages 735–744, Suzhou, China.\nAssociation for Computational Linguistics.\n13062\nZhicheng Huang, Zhaoyang Zeng, Yupan Huang, Bei\nLiu, Dongmei Fu, and Jianlong Fu. 2021. Seeing\nout of the box: End-to-end pre-training for vision-\nlanguage representation learning.\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana\nParekh, Hieu Pham, Quoc V . Le, Yunhsuan Sung,\nZhen Li, and Tom Duerig. 2021. Scaling up vi-\nsual and vision-language representation learning with\nnoisy text supervision.\nGuolin Ke, Qi Meng, Thomas Finley, Taifeng Wang,\nWei Chen, Weidong Ma, Qiwei Ye, and Tie-Yan Liu.\n2017. Lightgbm: A highly efficient gradient boosting\ndecision tree. In Advances in Neural Information\nProcessing Systems, volume 30. Curran Associates,\nInc.\nWonjae Kim, Bokyung Son, and Ildoo Kim. 2021. Vilt:\nVision-and-language transformer without convolu-\ntion or region supervision.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nAnastasia Kritharoula, Maria Lymperaiou, and Giorgos\nStamou. 2023. Language models as knowledge bases\nfor visual word sense disambiguation.\nJunnan Li, Dongxu Li, Caiming Xiong, and Steven\nHoi. 2022. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding\nand generation.\nTsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir\nBourdev, Ross Girshick, James Hays, Pietro Perona,\nDeva Ramanan, C. Lawrence Zitnick, and Piotr Dol-\nlár. 2015. Microsoft coco: Common objects in con-\ntext.\nPengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang,\nHiroaki Hayashi, and Graham Neubig. 2021. Pre-\ntrain, prompt, and predict: A systematic survey of\nprompting methods in natural language processing.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee.\n2019. Vilbert: Pretraining task-agnostic visiolinguis-\ntic representations for vision-and-language tasks.\nMaria Lymperaiou and Giorgos Stamou. 2022. A survey\non knowledge-enhanced multimodal learning.\nMaria Lymperaiou and Giorgos Stamou. 2023. The\ncontribution of knowledge in visiolinguistic learning:\nA survey on tasks and challenges.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hai-\nley Schoelkopf, Xiangru Tang, Dragomir Radev, Al-\nham Fikri Aji, Khalid Almubarak, Samuel Albanie,\nZaid Alyafeai, Albert Webson, Edward Raff, and\nColin Raffel. 2023. Crosslingual generalization\nthrough multitask finetuning.\nNiels Mündler, Jingxuan He, Slobodan Jenko, and Mar-\ntin Vechev. 2023. Self-contradictory hallucinations\nof large language models: Evaluation, detection and\nmitigation.\nRostislav Nedelchev, Debanjan Chaudhuri, Jens\nLehmann, and Asja Fischer. 2020. End-to-end en-\ntity linking and disambiguation leveraging word and\nknowledge graph embeddings.\nRahul Patil, Pinal Patel, Charin Patel, and Mangal\nVerma. 2023. Rahul patil at SemEval-2023 task\n1: V-WSD: Visual word sense disambiguation. In\nProceedings of the 17th International Workshop on\nSemantic Evaluation (SemEval-2023), pages 1271–\n1275, Toronto, Canada. Association for Computa-\ntional Linguistics.\nFabio Petroni, Tim Rocktäschel, Patrick Lewis, An-\nton Bakhtin, Yuxiang Wu, Alexander H. Miller, and\nSebastian Riedel. 2019. Language models as knowl-\nedge bases?\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sas-\ntry, Amanda Askell, Pamela Mishkin, Jack Clark,\nGretchen Krueger, and Ilya Sutskever. 2021. Learn-\ning transferable visual models from natural language\nsupervision.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nAlessandro Raganato, Iacer Calixto, Asahi Ushio, Jose\nCamacho-Collados, and Mohammad Taher Pilehvar.\n2023. SemEval-2023 Task 1: Visual Word Sense\nDisambiguation. In Proceedings of the 17th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2023), Toronto, Canada. Association for Computa-\ntional Linguistics.\nNils Reimers and Iryna Gurevych. 2019. Sentence-bert:\nSentence embeddings using siamese bert-networks.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing. Associa-\ntion for Computational Linguistics.\nHao Tan and Mohit Bansal. 2019. Lxmert: Learning\ncross-modality encoder representations from trans-\nformers.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, volume 30. Curran Associates, Inc.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: A free collaborative knowledgebase. Commun.\nACM, 57(10):78–85.\n13063\nJianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie\nLi, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and\nLijuan Wang. 2022. Git: A generative image-to-text\ntransformer for vision and language.\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yu-\nlia Tsvetkov, and Yuan Cao. 2021. Simvlm: Simple\nvisual language model pretraining with weak super-\nvision.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nQihao Yang, Yong Li, Xuelin Wang, Shunhao Li, and\nTianyong Hao. 2023. TAM of SCNU at SemEval-\n2023 task 1: FCLL: A fine-grained contrastive\nlanguage-image learning model for cross-language\nvisual word sense disambiguation. In Proceedings of\nthe 17th International Workshop on Semantic Eval-\nuation (SemEval-2023) , pages 506–511, Toronto,\nCanada. Association for Computational Linguistics.\nPeter Young, Alice Lai, Micah Hodosh, and Julia Hock-\nenmaier. 2014. From image descriptions to visual\ndenotations: New similarity metrics for semantic in-\nference over event descriptions. Transactions of the\nAssociation for Computational Linguistics, 2:67–78.\nMuru Zhang, Ofir Press, William Merrill, Alisa Liu,\nand Noah A. Smith. 2023a. How language model\nhallucinations can snowball.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, Todor Mi-\nhaylov, Myle Ott, Sam Shleifer, Kurt Shuster, Daniel\nSimig, Punit Singh Koura, Anjali Sridhar, Tianlu\nWang, and Luke Zettlemoyer. 2022. Opt: Open pre-\ntrained transformer language models.\nXudong Zhang, Tiange Zhen, Jing Zhang, Yujin Wang,\nand Song Liu. 2023b. SRCB at SemEval-2023 task\n1: Prompt based and cross-modal retrieval enhanced\nvisual word sense disambiguation. In Proceedings of\nthe 17th International Workshop on Semantic Eval-\nuation (SemEval-2023) , pages 439–446, Toronto,\nCanada. Association for Computational Linguistics.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n2023. Why does chatgpt fall short in providing truth-\nful answers?\nA Dataset details\nIn Tab. 8 statistics for VWSD are presented. All\ntrain and test samples contain 10 image candidates.\nThe phrase length demonstrates negligible differ-\nences, with the vast majority of phrases comprised\nof 2 words. Data samples and official splits can be\nfound in https://raganato.github.io/vwsd/.\nSplit #Samples Phrase length\n1\nword\n2\nwords\n3\nwords\n4\nwords\nTrain 12869 0 12868 0 0\nTest 463 1 445 17 1\nTable 8: Dataset statistics\nB Computational resources\nIn Tab. 9 we analyze the resources used through-\nout our experiments, as well as the time needed\nfor inference on the entire test set of 463 sam-\nples. Regarding captioners, we demonstrate the\ntime needed for one batch of 1000 images. As for\nLTR, training time refers to the train split exclu-\nsively (12869 samples).\nC Additional LLM-enhancement results\nQuantitative results In Tab. 12, 13 we present\nthe continuation of Tab. 3 results. LLMs com-\nbined with VL retrievers without penalty. Since\nsmaller open-source LLMs do not involve pricing\nlimitations, we attempt to experiment with some\nadditional prompts, mostly paraphrasings of the\n\"describe\" prompt, as presented in Tab. 10. To this\nend, results for OPT-2.7B and BLOOMZ 3B are\nextended for these new prompts (Tab. 11).\nAs a general takeaway, we can verify the claim\nthat LLMs in the low-billion parameter scale do not\ncontain the appropriate knowledge to provide high-\nquality context to semantically enrich ambiguous\nphrases. For example, according to Tab. 12, OPT-\n6.7B enhancement leads to scores slightly below\nthe non-enhanced baselines in most cases, irrespec-\ntively of the incorporation of penalty in the VL\nretrieval module. The smaller models of GPT2-\nXL (1.5B) and BLOOMZ-1.7B showcase some im-\nprovements compared to the respective baselines\nwhen no penalty is used, but the results remain\nlow compared to the ones reported in Tab. 3. We\nalso observe an interesting variability regarding\nwhich prompts induce better enhancement results,\nwith different prompts performing better or worse\nacross models. This verifies that prompts are not\ntransferable, i.e. a prompt that performs well in\nconjunction with a certain model does not necessar-\nily perform equally well when inserted to another\nmodel. In total, all models in Tab. 12, 13 demon-\nstrate comparable performance to each other; even\nthough the reported metrics are not discouraging\nfrom the perspective of a potential real use case (e.g.\n13064\nModel Hardware Time (hours)\nVL Transformers for retrieval\nCLIP GPU - NVIDIA Tesla K40 12GB 00:10 h\nALIGN GPU - NVIDIA Tesla K40 12GB 00:08 h\nCLIP-L GPU - NVIDIA Tesla K40 12GB 00:15 h\nBLIP GPU - NVIDIA Tesla K40 12GB 00:20 h\nBLIP-L GPU - NVIDIA Tesla K40 12GB 00:45 h\nLLMs for phrase enhancement\nGPT2 XL 1.5B GPU - NVIDIA Tesla K40 12GB 00:30 h\nOPT 2.7B GPU - NVIDIA Tesla K40 12GB 01:45 h\nBLOOMZ 1.7B GPU - NVIDIA Tesla K40 12GB 00:15 h\nBLOOMZ 3B GPU 12.8GB 00:20 h\nOPT 6.7B 2 x GPU T4 14.8GB 02:00 h\nGalactica 6.7B 2 x GPU T4 14.8GB 02:15 h\nImage Captioners\nBLIP (batches with 1000 images each) GPU - NVIDIA Tesla K40 12GB ∼02:00 h / 1000 images\nBLIP-L (batches with 1000 images each) GPU - NVIDIA Tesla K40 12GB ∼03:00 h / 1000 images\nGiT (batches with 1000 images each) GPU - NVIDIA Tesla K40 12GB ∼03:00 h / 1000 images\nGiT-L GPU - NVIDIA Tesla K40 12GB ∼04:00 h / 1000 images\nSentence Transformers\nxlm-r-distilroberta NVIDIA TITAN Xp 12GB < 00:08 h\nstsb-roberta-base NVIDIA TITAN Xp 12GB < 00:08 h\nstsb-distilroberta-base NVIDIA TITAN Xp 12GB < 00:04 h\nstsb-mpnet-base NVIDIA TITAN Xp 12GB < 00:07 h\nall-MiniLM-L6 NVIDIA TITAN Xp 12GB < 00:03 h\nall-MiniLM-L12 NVIDIA TITAN Xp 12GB < 00:04 h\nall-mpnet-base NVIDIA TITAN Xp 12GB < 00:06 h\nmulti-QA-distilbert NVIDIA TITAN Xp 12GB < 00:06 h\nmulti-QA-MiniLM-L6 NVIDIA TITAN Xp 12GB < 00:04 h\nLTR\nLTR training CPU - 16GB RAM ∼00:20 h\nLTR prediction CPU - 16GB RAM < 00:01 h\nTable 9: Resources used for our experiments and time needed\nPrompt name Prompt template\nwould_say “To describe <phrase> I would say\nthat”\ncould_describe “I could describe <phrase> as ”\nwrite_description “Write a description of <phrase>.”\nTable 10: Prompts for phrase enhancement via LLMs.\nan image retrieval platform), especially given the\ndifficulty of VWSD, performance achieved using\nthe multi-billion parameter models of GPT3 and\nGPT-3.5 turbo set higher expectation for related\nVWSD implementations.\nQualitative results We showcase some LLM-\nenhancement examples on given phrases, accompa-\nnied by the label prediction ranking (the leftmost\nlabel is the top-1 choice of the VL model). In Fig.\n3 candidates corresponding to the phrase \"greeting\ncard\" are presented, with candidate C being the cor-\nrect ground truth answer. The baseline predicted\nlabel ranking from CLIP is: [’G’, ’C’, ’D’, ’E’, ’J’,\n’B’, ’I’, ’F’, ’H’, ’A’]; therefore, the golden label is\nranked second. We then create the enhancements\nfor \"greeting card\" presented in Tab. 14, accom-\npanied by their label predictions using CLIP. As\nconcluded by Tab. 14, enhancements can be ei-\nther beneficial (\"describe\" prompt enhancement),\nor not (enhancements apart from \"describe\") with\nrespect to the prediction. However, by qualitatively\nevaluating those enhancements, we view them as\nhighly sensible and informative, excluding the \"ex-\n13065\nCLIP CLIP-L ALIGN BLIP C BLIP-LC BLIPF BLIP-LF\nacc MRR acc MRR acc MRR acc MRR acc MRR acc MRR acc MRR\nWith penalty\nBaseline 63.28 76.27 62.85 76.24 68.90 80.00 60.90 74.33 64.58 77.51 60.47 73.87 69.76 80.42\nOPT\nwould_say. 62.53 76.07 65.17 78.02 64.15 76.92 63.07 75.22 61.99 74.91 57.02 70.74 65.44 77.33\ncould_desc. 59.83 73.70 56.99 72.31 68.68 80.41 65.23 77.44 66.09 77.44 57.45 71.88 68.25 79.57\nwrite_desc. 56.44 71.82 63.37 76.52 65.44 77.68 61.99 74.65 62.42 75.29 54.64 69.49 64.79 76.70\nBLOOMZ\nwould_say. 64.79 77.49 68.03 79.21 70.19 80.66 65.23 77.45 66.31 78.16 60.69 73.92 68.68 79.81\ncould_desc. 65.23 77.40 65.66 78.09 69.11 79.87 65.23 77.38 67.17 78.75 61.34 73.87 69.33 80.18\nwrite_desc. 65.73 77.16 66.81 79.16 68.68 80.07 62.85 76.23 64.36 77.61 58.53 72.21 67.39 79.17\nWithout penalty\nBaseline 59.18 72.94 60.69 74.42 65.66 77.48 57.24 72.07 61.34 75.88 57.67 71.96 65.01 77.86\nOPT\nwould_say. 61.21 74.61 62.27 75.92 60.69 74.45 58.75 72.16 58.75 72.69 53.56 68.09 61.56 75.02\ncould_desc. 56.77 71.43 54.37 69.93 66.74 78.48 61.56 75.25 62.42 75.56 54.43 69.98 65.01 77.61\nwrite_desc. 54.95 70.44 59.16 73.87 63.50 76.13 56.80 71.49 59.83 73.35 50.76 66.97 63.07 75.67\nBLOOMZ\nwould_say. 61.34 74.87 64.15 76.57 66.95 78.53 62.20 75.50 63.28 76.23 56.80 71.73 65.23 77.70\ncould_desc. 61.56 74.96 61.77 75.50 66.95 78.10 62.85 75.85 65.23 77.25 57.24 71.49 67.82 79.06\nwrite_desc. 62.26 74.85 63.56 77.02 65.87 77.75 60.91 74.43 62.42 75.68 55.51 70.35 65.01 77.40\nTable 11: Results for zero-shot LLM-based enhancement, as a continuation of results for OPT-2.7B and BLOOMZ-\n3B (Tab. 3). Bold numbers indicate best results for each LLM.\nact\" phrase enhancement which is truncated (we\ncannot easily predefine the optimal length for the\ngenerated text, and variations in length may result\nin varying enhancement results). This observation\nmotivates our exploration towards more explain-\nable solutions, such as the ones involving Chain-\nof-Thought prompting.\nAnother example regarding LLM-enhancement\nis exhibited in Fig. 4 regarding the ambiguous\nphrase \"suede chamois\". The baseline CLIP label\nprediction is [’A’, ’G’, ’C’, ’E’, ’I’, ’B’, ’F’, ’J’,\n’H’, ’D’], ranking the golden candidate G in the\nsecond position. Results after GPT-3 enhancement\nare presented in Tab. 15. We observe that enhance-\nments are highly relevant to the meaning of the\nambiguous phrase, and different prompts show sat-\nisfactory consistency regarding the enriched phrase\nthey provide. Nevertheless, retrieval results differ\nfor different prompts, when CLIP is used as the VL\nretriever between the enhanced phrase te and the\ncandidate images i. This is not an expected behav-\nior in terms of the VL retriever employed, since\nsemantically similar phrases should yield similar\n(or ideally identical) ranking results. Therefore,\nwe verify the need for retrieval explanations and\nwe conclude that the robustness of VL models -at\nleast for the task of multimodal retrieval- should be\nan issue of outmost importance, when such models\nare designed and deployed.\nD Additional text retrieval results\nIn Tab. 16 we present some additional results\nregarding phrase-caption retrieval using ALIGN\nas the model to obtain textual representations for\nphrases and captions. Cosine similarity and Eu-\nclidean distance are utilized as a measure of simi-\nlarity (we also regard distance as a measure of sim-\nilarity for narrative simplicity). More text-to-text\nretrieval results employing other VL transformers\nfor text embeddings are presented in Tab. 17 Addi-\ntionally, results involving purely textual sentence\ntransformers fine-tuned on semantic similarity for\nphrase and caption embeddings are presented in\nTab. 18. In general, the exclusively textual models\nof Tab. 18 fall behind the performance achieved\nwhen VL transformers (Tab. 16, 17), are employed\nfor the textual representations.\nE Additional LTR results\nAs a continuation of Tab. 6, LTR results are pre-\nsented in Tab. 19, which contains results when\nCLIP is exploited as the VL retriever in place of\nALIGN. In both cases, GPT-3 is used as the LLM\nfor phrase enhancement.\nAs in the case of ALIGN, the incorporation of\nLLM-enhancement features in the LTR training of-\nfers best results. Those results are boosted even fur-\nther when combining visual features, as occurring\n13066\nCLIP CLIP-L ALIGN BLIP C BLIP-LC BLIPF BLIP-LF\nacc MRR acc MRR acc MRR acc MRR acc MRR acc MRR acc MRR\nWith penalty\nBaseline 63.28 76.27 62.85 76.24 68.90 80.00 60.90 74.33 64.58 77.51 60.47 73.87 69.76 80.42\nGPT2-XL-1.5B\nexact 53.88 69.51 56.32 71.12 53.35 69.57 47.52 63.67 47.95 64.49 41.90 59.79 50.54 66.96\nwhat_is 61.22 74.89 61.44 75.83 63.93 76.33 57.02 70.42 57.67 71.78 51.62 67.78 61.34 74.42\ndescribe 57.58 72.38 60.82 74.82 58.10 72.67 54.43 68.78 53.78 69.38 47.73 65.08 57.24 71.80\nmeaning 60.82 75.00 65.58 77.55 64.15 76.65 58.75 72.64 59.18 73.27 52.92 68.27 63.07 75.96\nwould_say. 59.74 73.86 62.34 75.55 59.40 73.46 52.92 67.80 53.35 68.66 48.16 64.47 55.94 70.67\ncould_desc. 57.08 71.87 59.26 73.23 58.10 72.49 52.05 68.22 55.08 70.18 46.87 64.44 57.02 72.03\nwrite_desc. 58.90 72.87 62.42 75.63 60.26 74.37 50.76 67.19 54.21 69.57 48.60 65.12 60.04 73.43\nBLOOMZ-1.7B\nexact 61.44 74.50 64.92 77.42 65.87 77.60 64.58 76.28 65.66 77.13 59.18 72.70 67.39 78.67\nwhat_is 63.71 76.41 66.74 78.92 65.44 77.65 63.28 75.95 65.23 77.68 58.32 72.06 66.52 78.30\ndescribe 64.72 77.21 64.07 77.47 68.90 80.28 61.34 75.61 64.36 77.45 58.53 72.41 66.74 78.81\nmeaning 62.63 76.38 65.01 78.17 66.74 78.27 63.50 76.44 65.44 78.29 58.53 72.50 68.25 79.74\nwould_say. 63.20 76.36 66.45 78.20 70.41 80.70 64.15 76.77 66.95 78.66 57.88 72.20 68.25 79.81\ncould_desc. 64.86 77.13 63.99 77.10 69.33 79.96 62.42 75.65 65.87 78.17 58.10 72.47 68.47 79.89\nwrite_desc. 62.61 76.00 65.93 78.07 68.68 79.70 60.04 73.52 64.58 77.13 57.02 71.19 67.17 78.66\nOPT-6.7B\nexact 62.63 75.84 62.20 75.54 67.82 79.24 60.91 74.23 64.79 77.58 59.83 73.40 69.11 79.94\nwhat_is 61.79 75.70 64.63 77.68 64.79 77.23 61.77 75.01 63.07 76.16 57.88 71.79 65.87 77.77\ndescribe 64.43 76.91 65.73 78.24 65.23 77.89 61.12 74.67 63.93 77.07 56.16 71.30 66.09 78.38\nmeaning 62.17 75.84 63.61 77.19 66.74 78.47 63.28 75.93 65.44 77.43 59.83 72.96 68.03 78.75\nwould_say. 63.98 76.77 61.14 75.49 68.03 78.95 61.77 74.73 60.26 74.38 55.51 70.23 65.87 77.70\ncould_desc. 58.59 73.21 61.45 74.60 65.44 77.76 63.07 75.34 64.36 76.09 58.32 71.86 65.23 77.28\nwrite_desc. 60.38 74.46 60.61 75.19 64.58 77.04 57.02 71.13 61.34 74.24 53.13 68.91 64.58 76.70\nWithout penalty\nBaseline 59.18 72.94 60.69 74.42 65.66 77.48 57.24 72.07 61.34 75.88 57.67 71.96 65.01 77.86\nGPT2-XL 1.5B\nexact 49.45 66.24 53.66 69.09 51.19 67.22 44.28 61.19 45.57 62.60 35.85 55.50 47.52 64.43\nwhat_is 58.61 72.59 58.61 73.62 60.91 74.25 54.21 68.22 53.56 69.03 46.00 64.08 55.94 70.71\ndescribe 54.76 70.24 56.49 72.14 55.94 70.55 50.97 66.20 50.11 66.62 44.71 62.21 55.51 70.07\nmeaning 58.44 72.97 62.55 75.60 61.56 74.49 55.08 70.24 54.64 70.76 50.32 66.74 57.02 72.54\nwould_say. 54.51 69.99 59.34 73.54 57.45 71.77 47.52 64.76 48.16 65.10 45.36 62.79 56.16 70.50\ncould_desc. 51.85 68.28 55.77 70.84 54.64 69.73 48.81 65.20 51.19 66.97 42.76 61.44 53.13 68.74\nwrite_desc. 54.51 69.99 59.34 73.54 57.45 71.77 47.52 64.76 48.16 65.10 45.36 62.79 56.16 70.50\nBLOOMZ 1.7\nexact 58.82 72.23 61.66 75.05 63.28 75.56 59.18 72.96 62.85 74.99 56.37 70.60 63.50 76.20\nwhat_is 62.42 75.30 65.01 77.33 63.07 75.58 59.40 73.50 62.85 76.00 56.16 70.54 65.23 77.26\ndescribe 60.82 74.68 62.99 76.05 66.52 78.51 59.83 74.59 62.63 76.47 53.56 69.84 63.71 77.06\nmeaning 58.75 73.78 64.15 77.03 64.36 76.44 60.48 74.34 61.99 76.13 56.16 70.61 65.01 77.89\nwould_say. 59.96 73.99 63.64 76.37 68.03 78.94 61.56 74.87 64.79 77.09 55.94 70.84 66.31 78.34\ncould_desc. 60.74 74.63 60.52 74.64 67.17 78.20 60.69 74.14 63.50 76.46 55.72 70.94 66.31 78.54\nwrite_desc. 59.29 73.49 64.82 77.08 66.31 77.86 57.67 71.61 61.34 74.76 52.48 68.47 64.36 76.72\nOPT 6.7B\nexact 58.75 72.63 59.61 73.86 64.15 76.57 57.24 71.96 61.12 75.83 56.80 71.40 64.79 77.66\nwhat_is 60.48 74.10 62.45 75.89 61.77 75.18 57.88 72.27 61.77 74.89 52.92 68.83 61.99 75.23\ndescribe 60.74 74.28 63.12 76.19 63.28 76.26 59.40 73.03 58.96 73.86 52.92 69.18 62.63 76.13\nmeaning 59.28 73.77 62.17 76.04 63.71 76.31 52.92 74.37 61.99 75.47 55.94 70.67 65.01 77.27\nwould_say. 60.90 74.51 58.29 73.30 65.87 77.33 60.26 73.65 57.24 72.50 54.21 69.13 60.69 74.78\ncould_desc. 55.29 70.73 59.25 72.95 62.20 75.18 60.04 73.27 59.83 73.50 52.70 68.52 60.26 74.46\nwrite_desc. 56.60 71.61 57.31 72.84 62.85 75.52 55.08 69.45 58.32 71.93 49.68 66.35 60.48 74.51\nTable 12: Results for zero-shot LLM-based enhancement. Bold numbers indicate best results for each model.\n13067\nCLIP CLIP-L ALIGN BLIP C BLIP-LC BLIPF BLIP-LF\nacc MRR acc MRR acc MRR acc MRR acc MRR acc MRR acc MRR\nWith penalty\nBaseline 63.28 76.27 62.85 76.24 68.90 80.00 60.90 74.33 64.58 77.51 60.47 73.87 69.76 80.42\nGalactica 6.7B\nexact 49.66 66.59 56.24 71.44 52.48 68.45 43.41 60.82 50.32 66.95 41.90 59.68 54.00 70.23\nwhat_is 60.13 74.40 62.78 76.11 62.63 75.50 57.88 72.62 61.12 74.59 53.56 68.73 64.79 76.81\ndescribe 59.61 74.04 60.48 75.13 62.20 75.80 55.51 70.74 56.59 71.25 52.92 68.13 58.96 73.49\nmeaning 60.09 74.69 60.32 74.80 62.20 75.04 61.56 74.57 61.34 74.99 52.27 68.02 61.99 75.49\nwould_say. 57.78 73.32 59.75 74.52 56.16 71.45 55.29 70.42 55.72 70.92 51.19 66.89 60.26 74.05\ncould_desc. 62.45 75.98 63.97 76.62 58.10 72.89 55.51 70.00 56.59 71.38 49.24 66.15 60.48 74.29\nwrite_desc. 58.54 73.10 60.75 74.93 57.88 72.64 55.72 70.27 56.16 70.83 49.24 64.97 60.91 74.25\nWithout penalty\nBaseline 59.18 72.94 60.69 74.42 65.66 77.48 57.24 72.07 61.34 75.88 57.67 71.96 65.01 77.86\nGalactica 6.7B\nexact 45.35 63.57 53.97 69.58 49.89 66.30 38.66 57.14 47.08 64.40 38.01 56.83 50.76 67.94\nwhat_is 56.61 71.74 59.91 73.82 60.91 74.12 55.72 70.32 58.75 72.82 52.27 67.77 61.77 74.84\ndescribe 56.80 71.54 58.53 73.31 59.61 73.85 53.56 69.03 80.28 67.91 50.97 66.30 54.43 70.60\nmeaning 56.69 72.21 56.24 72.63 58.96 72.64 55.72 70.79 56.37 71.65 50.11 66.57 58.32 72.90\nwould_say. 54.32 70.73 56.30 72.15 52.05 68.27 51.40 67.87 50.97 67.77 46.87 63.77 54.21 70.28\ncould_desc. 60.26 74.07 62.23 75.20 54.86 69.99 54.21 68.60 54.21 69.24 45.57 63.44 58.32 72.09\nwrite_desc. 55.65 71.09 57.21 72.40 54.64 70.47 50.11 66.71 51.62 67.61 43.84 61.69 57.88 72.33\nTable 13: Results for zero-shot LLM-based enhancement for LLMs containing 6.7B parameters. Bold numbers\nindicate best results for each model.\nA\n B\n C (Gold image)\n D\n E\nF\n G\n H\n I\n J\nFigure 3: Candidate images for the phrase \"greeting card\".\nA\n B\n C\n D\n E\nF\n G (Gold image)\n H\n I\n J\nFigure 4: Candidate images for the phrase \"suede chamois\".\n13068\nPrompt name Enhanced phrase\nexact greeting card Dear [Name],I hope this card finds you in good health and spirits. I wanted to take a moment to\nlet you know how much I appreciate you and all that you do. You are an amazing person and I am so lucky\nto have you in my life.Thank you for always being there for me, for your kind words and support, and for the\ncountless\nCLIP predictions: [’G’, ’C’, ’B’, ’E’, ’H’, ’I’, ’J’, ’D’, ’F’, ’A’]\nwhat_is What is greeting card? A greeting card is a card with a message of celebration, congratulations, or other\nsentiment, typically sent to someone on a special occasion such as a birthday, anniversary, or holiday.\nGreeting cards often feature artwork, photographs, or illustrations, and are usually printed on high-quality\npaper stock.\nCLIP predictions: [’G’, ’D’, ’B’, ’J’, ’E’, ’H’, ’C’, ’F’, ’I’, ’A’]\ndescribe Describe greeting card. A greeting card is a small, decorated card that is sent to someone to express a\nsentiment or to celebrate a special occasion. Greeting cards typically feature artwork, a message, and\nsometimes a small gift such as a token or trinket. They are often sent on holidays, birthdays, anniversaries,\nand other special occasions.\nCLIP predictions: [’C’, ’G’, ’D’, ’B’, ’F’, ’E’, ’I’, ’H’, ’J’, ’A’]\nmeaning What is the meaning of greeting card? A greeting card is a card sent to someone to express congratulations,\nbest wishes, or other sentiments. It typically includes a message of celebration or goodwill, and may also\ninclude an image or design.\nCLIP predictions: [’G’, ’B’, ’D’, ’H’, ’E’, ’J’, ’C’, ’I’, ’F’, ’A’]\nTable 14: Enhanced phrases for \"greeting card\" using GPT-3.\nPrompt name Enhanced phrase\nexact suede chamois Suede chamois is a type of leather that is made from the hide of a sheep. It is known for\nits soft, velvety texture and is often used to make clothing, shoes, and other accessories. It is also used in\nupholstery and automotive detailing.\nCLIP predictions: [’A’, ’G’, ’C’, ’I’, ’E’, ’F’, ’J’, ’B’, ’D’, ’H’]\nwhat_is What is suede chamois? Suede chamois is a type of leather that has been treated to give it a soft, velvety\ntexture. It is often used for making clothing, shoes, and other accessories. It is also used for cleaning and\npolishing surfaces, such as cars and furniture.\nCLIP predictions: [’C’, ’G’, ’A’, ’I’, ’E’, ’F’, ’J’, ’B’, ’D’, ’H’]\ndescribe Describe suede chamois. Chamois is a type of soft, suede leather that is made from the hide of a chamois, a\ntype of mountain goat. It is known for its softness and absorbency, making it ideal for use as a cleaning cloth\nor for polishing leather goods. It is also used to make clothing, such as gloves and hats.\nCLIP predictions: [’F’, ’A’, ’G’, ’E’, ’C’, ’I’, ’H’, ’B’, ’D’, ’J’]\nmeaning What is the meaning of suede chamois? Suede chamois is a type of leather that has been buffed to create a\nsoft, velvety texture. It is often used to make clothing, shoes, and other accessories.\nCLIP predictions: [’G’, ’A’, ’C’, ’I’, ’F’, ’E’, ’B’, ’J’, ’H’, ’D’]\nTable 15: Enhanced phrases for \"suede chamois\" using GPT-3.\nbased on similarity scores between the representa-\ntions of the 10 candidates i and the representations\nof the Wikipedia/Wikidata images iw (both visual\nrepresentations are extracted using CLIP). In gen-\neral, the incorporation of more features compared\nto the VL-baseline only LTR case (first 2 rows\nof Tab. 19 is helpful, even though the standalone\nexperiments (text-to-text retrieval and image-to-\nimage retrieval) did not produce competitive results\n(Tab. 4, 5).\nF QA and Chain-of-Thought examples\nChain of Thought (CoT) prompting provided some\ninteresting insights regarding the explainability of\ndecisions and how it can guide the LLM towards\nthe correct reasoning path. In this section, we\nprovide some additional examples when convert-\ning VWSD to a QA problem, comparing prompts\nwith and without CoT. Our presented results high-\nlight particular cases, including the ones where\nCoT fails; we proceed with an analysis on why\nthis may happen and whether it is an absolutely\njustified behavior. All presented examples utilize\nthe GiT-L captioner with greedy search. There-\nfore, one caption per candidate is extracted, and the\nprompt is constructed appropriately, with each of\n13069\nGreedy Beam\nBLIP BLIP-L GiT GiT-L BLIP BLIP-L GiT GiT-L\nacc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR\nCosine similarity - ALIGN\nNo LLM 31.97 50.41 39.52 55.00 36.93 53.51 41.04 57.67 37.37 54.29 46.65 63.21 40.39 57.04 47.30 62.59\nexact 34.99 51.89 38.66 54.51 38.44 54.42 43.41 58.79 39.52 56.05 52.05 66.33 42.55 58.69 46.44 61.49\nwhat_is 38.23 55.02 41.68 56.75 41.25 57.19 46.44 61.45 40.60 57.80 56.16 70.00 47.52 62.39 51.19 65.76\ndescr. 39.74 56.14 42.33 57.87 43.20 58.92 48.60 63.49 42.33 59.06 57.02 70.66 50.11 64.65 52.92 67.18\nmeaning 36.93 53.93 40.82 56.19 41.68 57.33 44.28 60.39 41.90 58.05 54.64 69.22 48.60 63.34 50.11 65.20\nEuclidean distance - ALIGN\nNo LLM 38.88 57.17 44.28 60.60 43.84 61.30 44.92 62.67 45.14 61.17 50.76 67.62 44.92 62.49 50.97 66.23\nexact 43.63 59.57 44.06 59.99 45.14 60.86 51.19 65.38 46.22 62.40 54.86 69.37 47.08 62.81 50.32 65.78\nwhat_is 47.95 63.83 48.60 63.73 47.52 63.73 52.92 67.55 49.46 65.40 60.69 74.05 51.84 67.11 55.51 70.31\ndescr. 50.32 65.58 50.11 65.21 52.05 66.61 56.59 70.03 50.11 66.46 62.20 75.35 55.51 69.68 58.75 72.23\nmeaning 45.57 62.18 47.52 62.91 46.65 63.80 54.21 68.32 50.54 65.78 59.40 73.46 52.48 67.97 57.67 71.28\nTable 16: Continuation of Tab. 4. Additional results on phrase-caption retrieval (with and without GPT-3\nenhancement) for different captioning models using different ALIGN for text embeddings.\nthe answers for the 10 possible candidates forming\nanswer choices from A to J within the prompt.\nExample 1 In this example, CoT triggers the cor-\nrect reasoning path towards choosing the correct\ncaption choice (H) \"a close up of a metal plate\nwith a pattern of lines.\" of the corresponding image\n(Fig. 5), as demonstrated in Tab. 20. The no_CoT\nprompting case is easily misled by the semanti-\ncally similar caption (F) that includes the concept\n\"metal\" (\"a black piece of metal with a large black\nsquare in the middle\"). At the same time, CoT\nprompting elicit the appropriate behavior of GPT-\n3.5-turbo, guiding it to distinguish how the phrase\n\"metal steel\" differs from the semantically similar\none \"black piece of metal\". This fine-grained ca-\npacity is one of the crucial desiderata of VWSD\nsystems, and seems to be somehow connected with\nCoT reasoning.\nExample 2 There are cases when CoT fails to\nenhance the given phrase with appropriate context.\nIn the case of Fig. 6, the most relevant image out\nof the options of Fig. 6 to the phrase \"tender em-\nbrace\" fails to be returned: when GPT-3.5-turbo is\nprompted with Q, it has a hard time retrieving rele-\nvant knowledge, answering that it does not under-\nstand where the \"tender embrace\" phrase refers to,\nand that more information is needed. By observing\nthe captions comprising the answer choices of the\nquestion Q, we confirm that the extracted caption\nfor the golden candidate (C) is sensible and accu-\nrate (\"a student gets a hug from a student\"); thus,\nGPT-3.5-turbo cannot connect the phrase \"tender\nembrace\" with the correct caption (C), leading to\nfailure, as seen in Tab. 21. On the other hand, when\nno_CoT prompting is used, GPT-3.5-turbo success-\nfully returns the right answer. We can observe that\neven though the model does indeed understand the\nsense of the ambiguous phrase \"tender embrace\"\nwith respect to the answer choices, it cannot re-\ntrieve the appropriate choice under CoT prompting.\nTherefore, we mark this case as an inherent failure\nof CoT disambiguation.\nExample 3 Another case that CoT prompting in-\nherently fails, while no_CoT prompting succeeds is\npresented in Tab. 22 for the phrase \"edible sub\" and\nthe candidates of Fig. 7. Similarly to the previous\ncase, CoT prompting fails to retrieve the proper\ncaption choice, even though the caption itself is\nhighly relevant and accurate to the candidate im-\nage. Its reasoning path outputs some traces of cor-\nrect reasoning (it mentions the word \"sandwich\"),\neven though it cannot proceed further with disam-\nbiguation. At the same time, no_CoT prompting\nsucceeds in selecting the right choice (B). It also in-\ncreases the confidence in its answer by also stating\nthat \"This caption accurately describes the image\nand highlights the main focus of the picture\".\nExample 4 There are also cases where both\nno_CoT and CoT prompts fail to provide the right\nanswer. For example, given the phrase \"trotting\nappendix\" and the candidate images of Fig. 8, GPT-\n3.5-turbo fails to understand the context of the am-\n13070\n5\nGreedy\nBeam\nBLIP BLIP-L GiT GiT-L BLIP BLIP-L GiT GiT-L\nacc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR\nManhattan distance - CLIP\nNo LLM 36.50 56.58 41.25 58.18 36.93 55.57 39.09 56.98 41.04 58.45 44.71 61.70 41.68 59.16 43.84 60.14\nexact 45.21 63.28 47.60 63.26 43.41 60.58 45.51 62.14 47.90 64.16 50.60 67.08 48.20 63.93 49.10 64.70\nwhat_is 44.96 63.88 46.60 63.48 48.01 63.91 48.24 64.68 49.18 65.23 55.04 70.18 51.05 66.74 49.18 65.81\ndescribe 47.87 65.46 48.82 64.84 48.34 64.90 53.08 67.90 46.92 64.50 57.35 71.63 54.03 68.52 54.98 69.84\nmeaning 46.78 64.70 46.56 63.47 44.35 62.07 48.34 64.99 46.56 64.01 52.55 68.57 49.00 65.45 50.55 66.27\nCosine similarity - CLIP\nNo LLM 33.26 50.76 28.73 46.64 28.94 47.13 30.02 47.96 31.97 49.96 39.09 55.97 32.40 50.72 30.89 48.48\nexact 37.72 55.34 38.02 54.82 36.83 53.74 35.33 53.07 42.22 58.73 44.31 61.92 40.42 56.99 40.72 55.99\nwhat_is 37.24 55.91 37.24 54.50 38.41 55.03 38.17 55.40 41.45 58.42 46.14 62.39 41.92 57.88 38.17 55.33\ndescribe 39.34 57.90 45.02 60.39 44.08 59.27 41.23 57.73 40.76 58.20 50.24 66.27 46.45 62.07 41.71 58.13\nmeaning 38.14 56.35 34.37 52.59 35.25 53.03 39.02 55.48 39.02 56.91 44.79 61.57 42.79 58.42 38.14 54.77\nEuclidean distance - CLIP\nNo LLM 35.85 53.63 32.40 50.51 32.83 51.61 33.69 51.45 36.93 54.85 43.20 60.12 37.58 55.30 38.88 55.42\nexact 43.41 61.63 44.31 60.05 42.81 59.67 43.11 60.31 46.11 62.86 52.10 67.77 44.91 61.12 48.20 63.25\nwhat_is 44.26 62.13 44.50 60.57 43.79 60.37 44.26 61.15 47.78 63.67 55.04 69.60 48.95 64.45 46.60 62.91\ndescribe 48.34 64.95 47.39 62.68 47.87 63.17 49.76 64.50 42.65 61.87 58.29 72.28 53.55 67.52 51.66 67.28\nmeaning 44.57 62.38 43.02 59.86 40.80 58.94 44.57 61.05 45.90 62.72 52.11 67.36 48.56 64.52 45.90 62.39\nManhattan distance - CLIPL\nNo LLM 38.01 57.95 41.90 60.49 39.96 57.78 45.36 62.24 42.12 60.12 47.52 65.43 44.06 61.20 45.36 62.95\nexact 44.61 62.07 45.21 62.40 43.11 60.71 46.11 63.33 48.80 64.89 51.20 67.12 43.11 61.85 49.70 65.13\nwhat_is 43.09 61.97 47.31 64.83 44.73 62.32 50.59 66.57 49.41 65.88 54.10 70.17 49.18 65.85 51.05 67.35\ndescribe 45.97 63.01 48.34 65.36 46.45 63.40 49.29 65.05 46.45 63.57 54.50 70.82 48.82 65.37 52.61 68.64\nmeaning 45.01 63.42 47.45 64.70 45.68 62.73 50.33 67.02 47.89 65.37 52.55 69.29 45.90 64.44 51.22 67.35\nCosine similarity - CLIPL\nNo LLM 32.61 51.28 30.67 48.63 31.97 49.48 31.10 49.30 35.64 52.69 34.56 52.87 37.80 55.75 35.42 52.64\nexact 35.33 53.38 35.93 53.18 36.83 54.49 34.13 52.73 43.11 58.87 36.83 54.83 42.22 59.53 38.92 55.96\nwhat_is 36.53 55.41 37.00 54.44 38.41 55.33 37.24 55.40 41.22 57.77 39.34 56.80 44.03 61.02 37.70 55.53\ndescribe 37.91 55.86 39.81 56.24 38.86 57.39 40.76 57.42 40.28 57.13 43.60 59.62 47.39 64.94 40.76 58.52\nmeaning 38.14 56.00 36.36 53.56 36.81 54.32 36.14 54.69 41.24 57.85 37.25 55.44 42.79 60.02 37.25 54.78\nEuclidean distance - CLIPL\nNo LLM 35.85 54.44 32.40 52.91 32.83 54.65 33.69 55.50 36.93 57.62 43.20 62.20 37.58 59.01 38.88 59.30\nexact 43.41 58.78 44.31 57.62 42.81 57.93 43.11 60.52 46.11 62.19 52.10 64.91 44.91 60.55 48.20 62.52\nwhat_is 44.26 59.58 44.50 60.61 43.79 60.53 44.26 62.60 47.78 62.62 55.04 67.65 48.95 63.89 46.60 63.59\ndescribe 48.34 60.67 47.39 61.51 47.87 61.35 49.76 62.69 42.65 61.20 58.29 68.81 53.55 62.72 51.66 65.66\nmeaning 44.57 60.64 43.02 58.67 40.80 60.39 44.57 61.72 45.90 63.40 52.11 67.59 48.56 62.89 45.90 62.13\nTable 17: Additional results on phrase-caption retrieval (with and without GPT-3 enhancement) for different\ncaptioning models using different VL transformers for text embeddings.\nbiguous word with respect to the caption choices.\nDifferent answers are returned in each case; the\nno_CoT prompt confidently returns an unsuitable\nanswer ((B) instead of (G)), while the CoT prompt\nretains an uncertain narrative, stating that more con-\ntext is needed. Even in that case, CoT prompting is\nmore reliable, not only because it does not indicate\nan inappropriate candidate as the right answer, but\nalso because it better detects the reason why am-\nbiguity cannot be resolved: a \"trotting appendix\"\nhorse breed contains certain characteristics, which\nhowever were not captured by the captioner. Cap-\ntions of both images (B) and (G) refer to the seman-\ntic \"horse\" without further details about the animal,\n13071\nGreedy Beam\nBLIP BLIP-L GiT GiT-L BLIP BLIP-L GiT GiT-L\nacc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR acc. MRR\nManhattan distance - distilroberta base\nexact 39.52 56.71 39.09 57.43 38.66 56.94 41.04 58.99 39.52 57.34 46.22 62.29 37.37 56.04 44.49 60.81\nwhat_is 40.17 57.96 41.04 59.18 41.25 59.24 42.33 60.21 39.31 57.56 47.52 64.63 40.39 58.35 45.79 62.07\ndescribe 38.44 57.30 40.39 58.50 42.12 59.68 44.92 61.78 42.12 58.67 47.52 64.47 40.82 58.79 48.38 64.02\nmeaning 38.23 57.62 42.98 60.57 42.33 60.11 44.49 61.85 43.20 59.84 45.57 63.57 40.39 59.05 47.30 63.38\nManhattan distance - stsb roberta base\nexact 38.88 56.84 40.39 58.95 42.33 59.11 40.39 58.95 40.17 57.47 45.36 61.89 43.20 59.87 45.36 61.95\nwhat_is 40.60 58.51 44.06 61.87 39.74 58.31 44.06 61.87 42.98 59.92 44.71 63.28 44.06 61.24 47.73 64.56\ndescribe 42.55 60.03 40.17 57.58 42.12 59.63 45.14 62.17 42.98 60.04 47.95 65.14 45.14 62.48 47.73 64.66\nmeaning 45.14 61.37 40.17 58.30 41.04 59.02 46.65 63.33 43.63 61.02 49.46 66.34 43.20 61.61 49.89 66.24\nManhattan distance - stsb mpnet base\nexact 36.50 56.07 41.04 58.82 41.25 59.10 43.63 60.66 40.60 58.75 46.87 64.34 43.41 61.26 46.00 62.31\nwhat_is 40.39 58.92 42.33 60.80 42.55 60.90 45.14 62.40 43.41 61.05 49.68 66.47 45.14 62.24 49.46 65.19\ndescribe 42.12 60.14 43.41 60.90 44.06 61.67 49.89 65.66 43.20 61.66 47.08 65.61 47.52 64.19 50.11 66.02\nmeaning 41.25 59.56 43.84 62.06 44.49 62.28 50.76 66.00 44.06 61.86 50.11 67.09 47.52 64.08 50.76 66.71\nManhattan distance - all MiniLM-L6\nexact 42.55 59.89 45.36 62.33 41.04 59.53 45.14 61.81 42.98 60.78 49.24 65.45 43.41 61.58 49.24 64.92\nwhat_is 44.49 61.49 44.71 62.44 45.79 62.51 46.22 63.57 42.55 61.10 48.60 66.17 47.52 64.16 50.76 66.74\ndescribe 43.41 61.08 44.49 62.29 41.04 59.91 49.03 65.22 43.63 61.43 50.11 66.69 42.55 61.56 49.24 66.29\nmeaning 42.12 60.09 45.79 63.28 44.92 62.26 45.57 63.12 43.84 61.46 49.03 66.81 45.79 63.37 51.84 67.58\nManhattan distance - all MiniLM-L12\nexact 39.52 57.99 46.22 62.19 40.60 59.30 42.98 60.83 41.04 60.16 48.81 65.19 42.55 60.67 48.60 64.91\nwhat_is 40.39 59.02 43.63 61.88 41.68 60.14 46.00 63.36 42.76 61.36 48.81 66.42 44.71 61.97 49.46 66.10\ndescribe 40.60 59.33 44.28 61.94 41.90 60.48 47.73 64.36 42.12 61.47 48.38 66.11 44.49 62.40 51.40 67.73\nmeaning 40.60 59.37 43.41 62.00 43.20 61.37 47.52 64.20 42.76 61.46 50.54 67.42 46.44 63.33 49.46 66.37\nManhattan distance - all mpnet base\nexact 42.55 60.63 44.71 62.80 42.98 61.06 46.22 63.00 42.98 61.72 50.76 66.84 47.08 63.90 50.32 66.39\nwhat_is 42.55 61.56 46.22 64.69 45.79 63.20 49.24 66.13 43.41 62.80 54.43 70.45 46.65 64.33 50.97 67.57\ndescribe 42.76 61.40 46.00 64.07 43.63 61.87 48.60 65.58 44.06 62.54 52.70 68.71 45.36 63.52 52.27 68.36\nmeaning 43.84 62.12 49.03 66.06 45.57 62.82 48.81 65.87 44.92 63.55 54.43 70.37 48.38 65.30 50.54 67.68\nManhattan distance - multi-QA distilbert\nexact 40.60 59.86 43.41 60.81 42.33 60.13 42.55 60.73 42.98 61.53 49.03 65.71 42.98 61.51 49.24 65.35\nwhat_is 41.47 60.90 44.06 62.50 47.52 63.33 46.22 64.01 44.71 63.42 50.11 67.47 47.73 64.83 49.46 66.49\ndescribe 41.90 60.71 42.76 61.00 47.73 63.65 46.65 63.76 42.55 61.81 49.24 66.61 46.87 64.17 50.11 66.96\nmeaning 42.55 61.11 44.71 62.70 45.79 62.61 46.65 64.59 45.57 63.70 50.97 68.08 45.57 63.66 50.76 67.46\nManhattan distance - multi-QA MiniLM-L6\nexact 39.09 57.37 40.60 58.07 40.39 58.64 41.25 59.06 42.55 60.68 47.52 64.10 39.96 58.96 44.06 61.66\nwhat_is 40.82 59.40 42.76 60.14 40.82 59.49 43.84 61.65 44.28 62.08 49.24 66.57 41.90 61.13 46.87 64.42\ndescribe 38.23 57.35 42.33 59.58 39.74 58.84 45.14 62.05 41.68 60.51 48.38 65.32 40.82 59.65 47.30 64.50\nmeaning 40.39 59.04 42.33 60.46 41.68 60.05 45.57 62.92 42.33 61.35 48.60 65.93 42.33 61.26 49.89 66.55\nTable 18: Results on phrase-caption retrieval with GPT-3 enhancement for different captioning models using SBERT\nmodels for text embeddings.\neven though by visually observing the two image\ncandidates, there are obvious differences in terms\nof patterns and colors between the depicted horses.\nThe rest of the concepts mentioned in the captions\nsuch as \"field\", \"fence\", \"person\", \"building\" do\nnot provide any relevant context to the ambiguous\nword. Therefore, GPT-3.5-turbo correctly reaches\na tie condition where (B) and (G) captions equally\n13072\nBaseline LLM-enhance Text retrieval features Image retrieval feat. Metrics\np(i) Prompt p(i) Captioner Embedding Similarity Phrase Embedding Similarity Acc. MRR\n- - - - - - - - - 63.93 76.33\n- - - - - - - - 68.90 80.04\n- - - - - - - - 62.85 75.88\n- - - - - - CLIP cosine 70.87 81.36\n- - - - - - CLIP euclidean 70.22 81.09\n- - - - - - CLIP manhattan 69.78 80.95\n- - GiT-L-greedy CLIP cosine t - - 62.85 76.08\n- - GiT-L-beam CLIP cosine t - - 63.07 76.14\n- - GiT-L-beam CLIP euclidean t - - 62.85 75.85\n- - GiT-L-beam CLIP manhattan t - - 62.85 76.11\n- - blip-L-greedy CLIP cosine t - - 61.77 75.48\n- - blip-L-beam CLIP cosine t - - 62.85 75.94\nall - - - - - - 70.37 81.65\nmeaning - - - - - - - 65.85 78.67\nmeaning - - - - - - 66.52 79.21\nexact - - 65.57 78.25\nwhat_is - - - - - - 67.45 79.55\ndescribe - - - - - - 70.14 80.75\nall - - - - CLIP cosine 72.05 82.81\nall blip-L-beam CLIP cosine t CLIP cosine 72.05 82.61\nall GiT-L greedy CLIP cosine t CLIP cosine 70.81 82.28\nall GiT-L-greedy CLIP cosine all te+t CLIP cosine 73.91 83.53\nOur best 79.35 87.23\nLTR of Dadas (2023) (best results) 77.97 85.88\nSemEval organizers’ baseline 60.48 73.87\nTable 19: LTR results using feature combinations as extracted from our previous 4 approaches (baseline, LLM\nenhancement, text retrieval, image retrieval). CLIP is employed as the VL retriever.\nA\n B\n C\n D\n E\nF\n G\n H (Gold image)\n I\n J\nFigure 5: Example 1.Candidate images for the phrase \"metal steel\".\nmatch the ambiguous phrase, therefore failure to\nprovide an answer is totally justified.\nExample 5 Another example that CoT fails due\nto imperfections in captioning is the one corre-\nsponding to the phrase \"furry cimarron\" and the\ncandidates of Fig. 9. An official description 5 of\nthe term \"cimarron\" is \"wild sheep of mountainous\nregions of western North America having massive\ncurled horns\". However, none of the generated\ncaptions contains concepts such as \"wild sheep\"\nor \"massive curled horns\" which are descriptive\n5https://www.vocabulary.com/dictionary/cimarron\n13073\nA\n B\n C (Gold image)\n D\n E\nF\n G\n H\n I\n J\nFigure 6: Example 2.Candidate images for the phrase \"tender embrace\".\nA\n B (Gold image)\n C\n D\n E\nF\n G\n H\n I\n J\nFigure 7: Example 3.Candidate images for the phrase \"edible sub\".\nA\n B (Gold image)\n C\n D\n E\nF\n G\n H\n I\n J\nFigure 8: Example 4.Candidate images for the phrase \"trotting appendix\".\nExample 1\nQ: What is the most appropriate caption for the metal\nsteel? Answer Choices: (A) a chocolate bar with three\nsides (B) [unused0] and [unused0] at the concert in 2007\n(C) a guitar and a guitar are displayed in front of a speaker.\n(D) frosty patterns on a window (E) gold in the rocks - -\n(F) a black piece of metal with a large black square in the\nmiddle. (G) a jar of honey on a wooden table. (H) a close\nup of a metal plate with a pattern of lines. (I) a large white\nquartz rock with a clear base. (J) gold jewelry from the\nlate 19th century.\ncharacteristics of this animal; we assume that the\nselected captioner does not contain the knowledge\nneeded to fuse the appropriate details in the cap-\ntion. Therefore, GPT-3.5-turbo correctly avoids\nto return an answer. Nevertheless, since the con-\ncepts \"goat\" (caption (A)) and \"sheep\" (dictionary\ndescription for cimarron) are semantically related,\nthere should be an inclination towards choosing the\nanswer (A). Erroneously however, GPT-3.5.turbo\nhints about the answer choice (E) \"a rocky cliff face\nwith a body of water in the background\", pairing\n13074\nExample 1\nno_CoT\nQ: What is the most appropriate [...]\nA: The most appropriate caption for the metal steel would\nbe (F) a black piece of metal with a large black square in\nthe middle. X\nCoT\nQ: What is the most appropriate [...]\nA: Let’s think step by step.\nFirst, we need to understand what metal steel is and what\nits characteristics are. Steel is a hard and strong metal\nalloy made mainly of iron and carbon. It is often used in\nconstruction, machinery, and transportation. Based on this\ninformation, the most appropriate caption for metal steel\nwould be (H) a close up of a metal plate with a pattern of\nlines. This caption describes the texture and appearance of\nsteel, which is often characterized by its distinctive pattern\nof lines. The other options do not accurately describe steel\nor its unique qualities.\nTherefore, among A through J, the answer is (H) a close up\nof a metal plate with a pattern of lines.\"\nTable 20: VWSD as QA problem example, with CoT\nprompting encouraging the selection of the correct can-\ndidate. Questions Q are presented truncated.\nExample 2\nQ: What is the most appropriate caption for the tender\nembrace? Answer Choices: (A) a small boat sitting on top\nof a dock. (B) a group of people walking on a green hill.\n(C) a student gets a hug from a student. (D) a large fly\nlaying on a rock in the water. (E) the bus stop at the station\n(F) a train is parked at a station. (G) a crowd of people\nwatching a concert. (H) a train station with a sign on the\nside of it. (I) a black and red train on a track. (J) a man\nlaying in the sand on top of a surfboard.\nExample 2\nno_CoT\nQ: What is the most appropriate [...]\nA: The most appropriate caption for the tender embrace\nwould be: (C) a student gets a hug from a student.\nCoT\nQ: What is the most appropriate [...]\nA: Let’s think step by step.\nFirst, we need to understand what the tender embrace is\nreferring to. Once we know that, we can look at the answer\nchoices and find the one that best matches the subject of\nthe photo. Therefore, without further information about the\nphoto, it is not possible to choose the most appropriate cap-\ntion. Could you please provide more context or information\nabout the photo?\nTherefore, among A through J, the answer is not applicable\nwithout more information about the photo.\" X\nTable 21: VWSD as QA problem example, with CoT\nprompting encouraging the selection of the correct can-\ndidate. Questions Q are presented truncated.\nthe terms \"mountainous regions\" and \"rocky cliff\nface\" which are semantically related; the choice (E)\nExample 3\nQ: What is the most appropriate caption for the edible sub?\nAnswer Choices: (A) a ham ham with a few small spikes on\nit (B) a sandwich cut in half on a white paper. (C) candy\ncanes - the candy cane company (D) a white plate with\na bunch of peanuts. (E) a submarine on the water. (F) a\nplate of meats, sausages, and salad. (G) yellow flowers on\na green field (H) a basket filled with apples sitting on top\nof a table. (I) a man eating a hot dog in a restaurant. (J) a\nplate of sauerkraut with carrots and cheese.\nExample 3\nno_CoT\nQ: What is the most appropriate [...]\nA: The most appropriate caption for the edible sub would\nbe (B) \"A delicious sandwich cut in half on a white paper.\"\nThis caption accurately describes the image and highlights\nthe main focus of the picture.\nCoT\nQ: What is the most appropriate [...]\nA: Let’s think step by step.\nFirst, we need to understand what the \"edible sub\" is refer-\nring to. Is it a sandwich or something else? Once we have\nthis information, we can consider the answer choices and\ndetermine which one is the most appropriate caption for the\nedible sub. Can you provide more information about the\n\"edible sub\"?\nTherefore, among A through J, the answer is unclear without\nmore information about what the \"edible sub\" actually is.\"\nX\nTable 22: VWSD as QA problem example, with CoT\nprompting encouraging the selection of the correct can-\ndidate. Questions Q are presented truncated.\nExample 4\nQ: What is the most appropriate caption for the trotting\nappendix? Answer Choices: (A) a close up of a plant with\na green flower (B) a horse running in a field with a fence\nin the background. (C) a close up of a flower inside a plant\n(D) a close up of a fish (E) a bird standing on top of a grass\ncovered field. (F) a dog and a fox laying on the grass. (G)\na person holding a horse in front of a brick building. (H)\nthe flowers of the [ unused0 ] (I) a photo of a petri dish\nwith a yellow substance on it. (J) a blue jellyfish with a\nlong tail.\nthough does not even describe an animal. There-\nfore, despite the caption being deficient in properly\ndescribing the semantics of the image, the LLM\nas well seems incapable of inferring the related\nsemantics.\n13075\nA (Gold image)\n B\n C\n D\n E\nF\n G\n H\n I\n J\nFigure 9: Example 5.Candidate images for the phrase \"furry cimarron\".\nExample 4\nno_CoT\nQ: What is the most appropriate [...]\nA: The most appropriate caption for the trotting appendix\nwould be (B) - \"A horse running in a field with a fence in\nthe background.\" X\nCoT\nQ: What is the most appropriate [...]\nA: Let’s think step by step.\nFirst, we need to identify what a trotting appendix is. As\nan intelligent assistant, I do not have enough information to\nanswer that question. If you could provide more context or\nclarification, I would be happy to assist you in finding the\nappropriate caption.\nTherefore, among A through J, the answer is not applica-\nble.\" X\nTable 23: VWSD as QA problem example, with CoT\nprompting encouraging the selection of the correct can-\ndidate. Questions Q are presented truncated.\nExample 5\nQ: What is the most appropriate caption for the furry cimar-\nron? Answer Choices: (A) a goat laying on a rock in a\nfield. (B) a street that has a lot of buildings on it. (C) a\nlarge brick building with a sign on the front. (D) a ring\ntailed lemur in a zoo (E) a rocky cliff face with a body of\nwater in the background. (F) yucca trees in the desert (G)\na street with a person standing in front of it (H) a small\ntown in the mountains. (I) a river running through a field\nwith a fence in the background. (J) red fox furs hang from\nthe ceiling.\nG Model details\nWe provide the exact Huggingface model cards\nused in our implementations, or else APIs provided\nby the creators:\nVL retrieval models\n• CLIP (ViT-base)\nhttps://huggingface.co/openai/clip-vit-base-\npatch32\nExample 5\nno_CoT\nQ: What is the most appropriate [...]\nA: The most appropriate caption for the furry cimarron\nwould be missing from the answer choices. Could you\nplease provide some more information or context about\nthe furry cimarron that could help me suggest a suitable\ncaption?\" X\nCoT\nQ: What is the most appropriate [...]\nA: Let’s think step by step.\nFirst, we need to understand what the subject of the caption\nis. The subject is a furry cimarron. Next, we need to think\nabout what would be an appropriate description for the furry\ncimarron. The options provided do not seem to fit with the\nsubject, except for (E) a rocky cliff face with a body of\nwater in the background. However, this description still\ndoes not accurately capture the furry cimarron. Therefore,\nwe need to come up with a new option that specifically\ndescribes the furry cimarron. Here are some options: - \"A\nmajestic cimarron with a thick coat of fur, basking in the\nsun\" - \"A cute and cuddly cimarron taking a nap on a soft\nbed of grass\" - \"An independent and free-spirited cimarron\nexploring the great outdoors\" Overall, the most appropriate\ncaption would depend on the context and purpose of the\nimage.\nTherefore, among A through J, the answer is None of the\noptions are appropriate for describing a furry cimarron. We\nneed to come up with a new option that specifically de-\nscribes the furry cimarron.\" X\nTable 24: VWSD as QA problem example, with CoT\nprompting encouraging the selection of the correct can-\ndidate. Questions Q are presented truncated.\n• CLIP-L (ViT-large)\nhttps://huggingface.co/openai/clip-vit-large-\npatch14\n• ALIGN\nhttps://huggingface.co/kakaobrain/align-\nbase\n• BLIPC (BLIP with ViT base\nbackbone pre-trained on COCO)\n13076\nhttps://huggingface.co/Salesforce/blip-\nitm-base-coco\n• BLIP-LC (BLIP with ViT large\nbackbone trained on COCO)\nhttps://huggingface.co/Salesforce/blip-\nitm-large-coco\n• BLIPF (BLIP with ViT base back-\nbone pre-trained on Flickr30k)\nhttps://huggingface.co/Salesforce/blip-\nitm-base-flickr\n• BLIP-LF (BLIP with ViT large\nbackbone pre-trained on Flickr30k)\nhttps://huggingface.co/Salesforce/blip-\nitm-base-flickr\nLarge Language Models\n• GPT2-XL (1.5B)\nhttps://huggingface.co/gpt2-xl\n• BLOOM-1.7B\nhttps://huggingface.co/bigscience/bloom-\n1.7B\n• OPT 2.7B\nhttps://huggingface.co/facebook/opt-2.7b\n• BLOOMZ-3B\nhttps://huggingface.co/bigscience/bloomz-\n3b\n• OPT 6.7B\nhttps://huggingface.co/facebook/opt-6.7b\n• Galactica 6.7B\nhttps://huggingface.co/facebook/galactica-\n6.7b\n• GPT-3 text-davinci-003 175B (openai API)\n• gpt-3.5-turbo (openai API)\nImage captioning\n• BLIP Captions\nhttps://huggingface.co/Salesforce/blip-\nimage-captioning-base\n• BLIP-L Captions\nhttps://huggingface.co/Salesforce/blip-\nimage-captioning-large\n• GiT\nhttps://huggingface.co/microsoft/git-base\n• GiT-L\nhttps://huggingface.co/microsoft/git-large\nSentence Transformers Implementations re-\ngarding sentence similarity involve models from\nSentence Transformers https://www.sbert.net/.\n13077",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.850069522857666
    },
    {
      "name": "Natural language processing",
      "score": 0.6740571856498718
    },
    {
      "name": "Ambiguity",
      "score": 0.6112038493156433
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.6108344197273254
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5845111608505249
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.513139545917511
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.5101966857910156
    },
    {
      "name": "Question answering",
      "score": 0.5083102583885193
    },
    {
      "name": "Task (project management)",
      "score": 0.5024099349975586
    },
    {
      "name": "Transformer",
      "score": 0.49677616357803345
    },
    {
      "name": "Information retrieval",
      "score": 0.4952796995639801
    },
    {
      "name": "Language model",
      "score": 0.4925842881202698
    },
    {
      "name": "Context (archaeology)",
      "score": 0.48428744077682495
    },
    {
      "name": "Word (group theory)",
      "score": 0.4617912769317627
    },
    {
      "name": "Linguistics",
      "score": 0.14403992891311646
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I174458059",
      "name": "National Technical University of Athens",
      "country": "GR"
    }
  ],
  "cited_by": 3
}