{
  "title": "Multi-head or Single-head? An Empirical Comparison for Transformer Training",
  "url": "https://openalex.org/W3167588376",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5100657035",
      "name": "Liyuan Liu",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    },
    {
      "id": "https://openalex.org/A5101681241",
      "name": "Jialu Liu",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5019539533",
      "name": "Jiawei Han",
      "affiliations": [
        "University of Illinois Urbana-Champaign"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2970565456",
    "https://openalex.org/W2952180055",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W3103334733",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W3037932933",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W3034296505",
    "https://openalex.org/W2970389371",
    "https://openalex.org/W3007485446",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W2896060389",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2994689640",
    "https://openalex.org/W2963858333",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2952355681",
    "https://openalex.org/W2963323070",
    "https://openalex.org/W2950739196",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W2979636403",
    "https://openalex.org/W2948798935",
    "https://openalex.org/W2975067599"
  ],
  "abstract": "Multi-head attention plays a crucial role in the recent success of Transformer models, which leads to consistent performance improvements over conventional attention in various applications. The popular belief is that this effectiveness stems from the ability of jointly attending multiple positions. In this paper, we first demonstrate that jointly attending multiple positions is not a unique feature of multi-head attention, as multi-layer single-head attention also attends multiple positions and is more effective. Then, we suggest the main advantage of the multi-head attention is the training stability, since it has less number of layers than the single-head attention, when attending the same number of positions. For example, 24-layer 16-head Transformer (BERT-large) and 384-layer single-head Transformer has the same total attention head number and roughly the same model size, while the multi-head one is significantly shallower. Meanwhile, we show that, with recent advances in deep learning, we can successfully stabilize the training of the 384-layer Transformer. As the training difficulty is no longer a bottleneck, substantially deeper single-head Transformer achieves consistent performance improvements without tuning hyper-parameters.",
  "full_text": "Multi-head or Single-head? An Empirical\nComparison for Transformer Training\nLiyuan Liu\nUniversity of Illinois at Urbana-Champaign\nll2@illinois.edu\nJialu Liu\nGoogle Research\njialu@google.com\nJiawei Han\nUniversity of Illinois at Urbana-Champaign\nhanj@illinois.edu\nAbstract\nMulti-head attention plays a crucial role in the recent success of Transformer\nmodels, which leads to consistent performance improvements over conventional\nattention in various applications. The popular belief is that this effectiveness stems\nfrom the ability of jointly attending multiple positions. In this paper, we ﬁrst\ndemonstrate that jointly attending multiple positions is not a unique feature of\nmulti-head attention, as multi-layer single-head attention also attends multiple\npositions and is more effective. Then, we suggest the main advantage of the\nmulti-head attention is the training stability, since it has less number of layers\nthan the single-head attention, when attending the same number of positions. For\nexample, 24-layer 16-head Transformer (BERT-large) and384-layer single-head\nTransformer has the same total attention head number and roughly the same model\nsize, while the multi-head one is signiﬁcantly shallower. Meanwhile, we show that,\nwith recent advances in deep learning, we can successfully stabilize the training\nof the 384-layer Transformer. As the training difﬁculty is no longer a bottleneck,\nsubstantially deeper single-head Transformer achieves consistent performance\nimprovements without tuning hyper-parameters.\n1 Introduction\nTransformers [Vaswani et al., 2017] have led to a series of breakthroughs in various deep learning\ntasks [Devlin et al., 2019, Velickovic et al., 2018b]. One distinguishing characteristic of Transformer\nis that it does not contain any recurrent connections and can parallelize all computations in the\nsame layer, thus leads to better effectiveness, efﬁciency, and scalability. Without using recurrent\nconnections, Transformer purely relies on the attention mechanism to capture the dependency among\ninput tokens. Speciﬁcally, a novel multi-head attention module was proposed and used in Transformer\nto better capture the dependency among input tokens.\nThis multi-head attention module has been observed to be one major reason behind the success of the\nTransformer. For example, on machine translation benchmarks, Recurrent Neural Networks (RNNs)\ncan outperform Transformers when both are using the multi-head encoder-decoder attention, and\nwould underperform without using the multi-head attention [Chen et al., 2018]. Besides Transformer,\nmulti-head attention has also been incorporated into other models like RNNs [Chen et al., 2018],\nGraph Attention Network [Velickovic et al., 2018a], and Convolutional Neural Network [Xiao et al.,\n2020, Fang et al., 2019].\nPreprint. Word in progress.\narXiv:2106.09650v1  [cs.CL]  17 Jun 2021\n1-Layer 2-Head Transformer and 2-Layer 1-Head Transformer have the same \ntotal attention head number and roughly the same model size \n+ +\n+\n+\n+\nFeedforward \nModule\nSingle-Head \nAttention \nModule\n+\nMulti-Head Transformer Single-Head Transformer\nLN LN LN\nLN LN LN\nFigure 1: Left: with the same model components, both multi-head and multi-layer Transformer can\nattend multiple positions. Right: comparing to multi-head Transformer, multi-layer Transformer has\nthe potential to achieve a better performance, while its training is more challenging (without Admin,\nthe 48-layer transformer training diverged).\nAt a high level, it is widely believed that multi-head attention stands out by jointly attending multiple\npositions, while conventional attention module can only attend one position in one layer. Speciﬁcally,\nmulti-head attention projects the inputs into multiple different subspaces and conduct multiple\nattention computations in parallel.\nOur Contributions. Our point of start is demonstrating that attending multiple positions is not a\nunique feature of multi-head attention. In fact, stacking multiple conventional attentions can also\nattend multiple positions, and even could be more effective than the multi-head attention.\nSpeciﬁcally, as in Figure 1, a multi-head attention module can be viewed as an ensemble model, which\ncombines multiple single-head attention modules by calculating their average. Thus, by integrating\nthese modules differently, we can reconstruct a Transformer to be single-head 1 and substantially\ndeeper. These two networks can attend the same number of places (i.e., have the same total number of\nattention heads ), have roughly the same number of parameters and inference computation complexity,\nwhile the multi-head one is shallower and the single-head one is deeper.\nIn our experiments, we observe that, comparing to the shallower multi-head Transformer, the deeper\nsingle-head Transformer is more effective but harder to train, which matches the common wisdom\nthat model depth can increase model capacity at the cost of training difﬁculty. For example, the\n6-layer 6-head Transformer encoder-decoder model converges well, while the 36-layer single-head\nTransformer encoder-decoder model diverges. Fortunately, the recent advances in deep learning\nsuccessfully stabilize the Transformer training, and allows us to train substantially deeper models.\nSpeciﬁcally, with the adaptive model initialization (Admin), we are able to train the 36-layer single-\nhead Transformer, without changing any hyper-parameters [Liu et al., 2020b]. Meanwhile, after\nstabilizing the training of the 36-layer model, it converges faster and better than the 6-layer one (as in\nFigure 1). As elaborated in Section 5, we also reconstruct the 12-layer 12-head BERT-base model\ninto 144-layer single-head model, 24-layer 16-head BERT-large into 384-layer single-head model,\nand ﬁnd both performs better consistently across various tasks and domains.\n2 Related Work\nThere exist two aspects of related work regarding the topic here, which are Attention and Transformer.\n2.1 Attention and Multi-Head Structure\nAttention modules are ﬁrst proposed to capture the long-term dependency in sequence-to-sequence\nmodels [Graves et al., 2014, Bahdanau et al., 2015]. To calculate the output for a token in the target\nsequence, the attention module would calculate a weighted average of source token representations,\nwhile the weight is calculated by applying softmax on attention scores. Different variants of attention\n1We use single-head/multi-head Transformer to refer Transformer with single-head/multi-head Attention.\n2\nmodules calculate attention scores differently. For example, to calculate the attention score, Graves\net al. [2014] uses the cosine similarity, Bahdanau et al. [2015] uses the perception network, and\nLuong et al. [2015] uses dot product. While these modules can only attend one position in one layer,\nmulti-head attention is developed to improve the conventional attention module by allowing the\nmodule jointly attending multiple positions [Vaswani et al., 2017], which is identiﬁed as one major\nreason behind the success of Transformer [Chen et al., 2018]. Also, it has inspired several follow up\nwork to analysis the multi-head structure [Michel et al., 2019, Peng et al., 2020]. Speciﬁcally, Michel\net al. [2019] observes single-head Transformer performing better than multi-head Transformer for\nmodel pruning. Still, no study has been conducted on deep single-head Transformer training, due to\nits training difﬁculty.\n2.2 Transformer\nTransformer [Vaswani et al., 2017] has led to a series of breakthroughs in various domains [Devlin\net al., 2019, Velickovic et al., 2018b, Huang et al., 2019, Parmar et al., 2018, Ramachandran et al.,\n2019]. Meanwhile, Transformer training has been found to be more challenging and attracted lots of\nattention to analyze why Transformer is harder to train and how to stabilize Transformer training [Liu\net al., 2020a, Baevski and Auli, 2019, Nguyen and Salazar, 2019, Wang et al., 2019, Xiong et al.,\n2019, Liu et al., 2020b]. Many efforts have been made to improve Transformer, e.g., relative position\nencoding [Shaw et al., 2018] or replacing dot-product attention with locality-sensitive hashing [Kitaev\net al., 2020]. Here, we choose to focus our study on the original Transformer model as proposed in\nVaswani et al. [2017], and uses the initialization technique Admin to stabilize model training [Liu\net al., 2020b], since this method does not include any additional hyper-parameters and its ﬁnal model\nis equivalent to the original Transformer.\n3 Transformer Architecture\nThe Transformer architecture contains two types of sub-layers, i.e., Attention sub-layers and Feed-\nforward sub-layers. Each sub-layer is constructed with the shortcut connection and the Layer Norm.\nSpeciﬁcally, it calculates the output as xi+1 = fLN(xi + f(xi)), where xi is the input of layer i\nand the output of layer i−1 (top layers have larger indexes), fLN is the Layer Norm , and f(·)\nis multi-head attention fATT(·) or feedforward fFFN(·) for Attention sub-layers and Feedforward\nsub-layers respectively.\nLayer Norm. Layer norm [Ba et al., 2016] plays a vital role in the Transformer architecture. It is\ndeﬁned as fLN(x) =γ x−µ\nσ + ν, where µand σare the mean and standard deviation of x, γ and ν\nare learnable parameters.\nFeedforward. Transformers use two-layer perceptrons as feedforward networks, i.e., fFFN(x) =\nφ(xW(1))W(2), where W(·) are parameters, and φ(·) is the non-linear function. Speciﬁcally, the\noriginal Transformer ReLU as the activation function, while later study uses other types of activation\nfunction, e.g., BERT uses GELU as the activation function [Hendrycks and Gimpel, 2016].\nAttention. Transformers use the multi-head attention to capture the dependency among input\ntokens, which is based on the scaled dot-product attention. Scaled dot-product attention tries to\nquery information from the source sequence that is relevant to the target sequence. Speciﬁcally,\nassuming the length of the source sequence and the target sequence to be nand hidden dimension\nto be m, target sequence would be encoded as Q ∈Rn×m, source sequence would be encoded\nas K ∈Rn×m and V ∈Rn×m. The scaled dot-product attention would calculate the output as\nfScaled Dot-Product Attention(Q,K,V ) =softmax(QKT\n√m )V, where softmax(·) is the row-wise softmax.\nOne scaled dot-product attention is believed to attend only one position in each row (for each\ntarget token), since the output of softmax typically would have one dimension signiﬁcantly\nlarger than other dimensions in each row. Multi-head attention was proposed to jointly attend\nmultiple positions, which employs multiple scaled dot-product attention in parallel. Speciﬁ-\ncally, it calculates the output as fATT(Q,K,V ) = [ head1; ··· ; headh]W(O), where headi =\nfScaled Dot-Product Attention(QW(Q)\ni ,KW (K)\ni ,VW (V)\ni ), W(·) are learnable parameters, and his the num-\nber of heads.\n3\nTransformer.Transformer has two types of layer conﬁgurations when serving as the encoder and\nthe decoder respectively. Here, we use xi to refer the input of sub-layer i. Each Transformer\nencoder layer contains two sub-layers, i.e., one attention sub-layer in a self-attention manner and one\nfeedforward sublayer. Speciﬁcally, the attention sub-layer calculates outputs as x2i+1 = fLN(x2i +\nfATT(x2i,x2i,x2i)) and the feedforward sub-layer calculates outputs as x2i+2 = fLN(x2i+1 +\nfFFN(x2i+1). Notice that the attention sub-layer sets Q, K, and V as the same value x2i, capturing\nthe dependency among tokens within the same sequence, which is referred as self-attention.\nEach Transformer decoder layer contains three sub-layers, besides the self-attention sublayer and\nthe feedforward sublayer, it also includes a encoder-decoder attention sub-layer between them.\nSpeciﬁcally, the encoder-decoder attention sub-layer calculates outputs as x3i+2 = fLN(x3i+1 +\nfATT(x3i+1,h,h), where Kand V are set to the encoder output h.\n4 From Shallow Multi-Head To Deep Single-Head\nThe multi-head attention features the ability to jointly attend multiple positions in the same layer.\nHere, we ﬁrst show that the multi-head attention sub-layers and the feedforward sub-layers have\ninherent ensemble structures combining multiple smaller modules (e.g., outputs of 8-head attention is\nthe sum of 8 single-head attention). Then, we reconstruct the shallow multi-head Transformer into\nsingle-head multi-layer Transformer, which combines these modules in a more effective manner by\nallowing them to enhance each other.\n4.1 Inherent Ensemble Structure within Transformer\nAs in Figure 1, multi-head attention sub-layers and feedforward sub-layers have the inherent ensemble\nstructure, i.e., each of these sub-layers can be viewed as an ensemble of smaller models. Now let us\nproceed to introduce those parallel structure in details. Note that notations are introduced in Section 3.\nAttention. We split the weight matrix W(O) into h parts by rows, i.e., we mark W(O) =\n[W(O)T\n1 ; ··· ; W(O)T\nh ]T. Then, the multi-head attention calculates outputs as:\nfATT(Q,K,V ) = [head1; ··· ; headh]W(O)\n=\nh∑\ni=1\nheadiW(O)\ni =\nh∑\ni=1\nsoftmax(\nQW(Q)\ni W(K)T\nj KT\n√m )VW (V)\ni W(O)\ni . (1)\nNote that on the right side of Equation 1, each term can be viewed as a low-rank version of the general\nattention [Luong et al., 2015]. Thus, the multi-head attention can be viewed as jointly attending\nmultiple places by ensembling multiple conventional attention modules.\nSpeciﬁcally, the general attention module [Luong et al., 2015] calculates outputs as:\nfGeneral Attention(Q,K,V ) =softmax(QW1KT)VW2 (2)\nComparing Equation 2 and the term in Equation 1, we can ﬁnd their major difference is that the\nmulti-head attention decomposes the m×mmatrix W1 and W2 into W(Q)\ni W(K)T\ni√m and W(V)\ni W(O)\ni ,\nwhere W(Q)\ni ,W(K)\ni ,W(V)\ni ,W(O)T\ni ∈Rm×m\nh . With this low rank decomposition, the parameter\nnumber and computation complexity of the multi-head attention module would stay the same no\nmatter what the value of his (i.e., how many heads one layer has).\nFeedforward. Similar to the Attention module, we can also rewrite the Feedforward sub-layer\nas an ensemble of h modules.2 Speciﬁcally, we split the weight matrix W(1) into h parts by\nrows and W(2) into h parts by columns, i.e., we mark W(1) = [W(1)\n1 ; ··· ; W(1)\nh ] and W(2) =\n[W(2)T\n1 ; ··· ; W(2)T\nh ]T. Then, the feedforward sub-layer calculates outputs can be rewrote as:\nfFFN(x) =φ(xW(1))W(2) =\nh∑\ni=1\nφ(xW(1)\ni )W(2)\ni (3)\n2Note hhere is decided to be consistent with the Multi-Head Attention sub-layers.\n4\nTable 1: GLUE task descriptions and statistics. The second and fourth column denotes the number of\ntraining examples and the number of classes. Note that STS-B is an regression task.\nCorpus |Train| |Label| Task Metric(s) Domain\nSingle-Sentence Classiﬁcation\nCoLA 8.5k 2 acceptibility Matthews corr. misc.\nSST-2 67k 2 sentiment accuracy movie reviews\nSentence Similarity/Paraphrase\nMRPC 3.7k 2 paraphrase accuracy/F1 news\nSTS-B 5.7k - similarity Pearson/Spearman\ncorr.\nmisc.\nQQP 364k 2 similarity accuracy/F1 social QA questions\nNatural Language Inference (NLI)\nMNLI 393k 3 NLI (mis)matched acc. misc.\nQNLI 108k 2 QA/NLI accuracy Wikipedia\nRTE 2.5k 2 NLI accuracy misc.\nWNLI 634 2 coreference/NLI accuracy ﬁction books\nThus, the Feedforward sub-layer can be viewed as an ensemble of hsub-modules. Note that since the\nsum of the hsub-modules would be normalized by Layer Norm, their outputs are integrated in an\naveraging manner.\nAverage Ensemble. Each Transformer sub-layer calculates outputs as fLN(x + f(x)), where f(·)\ncould be fFFN(·) and fATT(·). Thus, the sum calculated in Equation 1 and 3 would be normalized by\nVar[x + f(x)]. In this way, the joint effect of layer norm and the sum would be similar to combining\nthese modules in an average ensemble manner.\n4.2 Shallow Multi-Head and Deep Single-Head as Module Integration Strategy\nIntuitively, with the same set of modules, no matter how these modules are integrated, the model can\nattend the same number of places. Still, some module integration strategy could be more effective\nintegrating modules.\nIn the original multi-head Transformer, modules in the same layer are combined in an ensemble\nmanner and cannot enhance each other. For example, as in Figure 1, when constructed in the multi-\nhead manner, the two attention heads would have the same input and are agnostic to each other. In\nthis way, the second attention head cannot leverage or beneﬁt from the information captured by the\nﬁrst attention head.\nIntuitively, it could be beneﬁcial to allow the second attention head standing on the shoulders of the\nﬁrst attention head. To this end, we integrate these modules differently, and reconstruct the shallow\nmulti-head Transformer into the deep single-head Transformer (As in Figure 1). Note that both\nmodels have the same total number of attention heads, roughly same model size, and roughly the\nsame inference computation complexity.\n5 Multi-Head or Single-Head? Empirical Comparisons\nHere, we conduct systematic empirical studies to compare the shallow multi-head attention and the\ndeep single-head attention. We ﬁrst show that, although the deep single-head attention is harder to\ntrain, the training difﬁculty is no longer an obstacle, when the recent advances of deep learning is\nemployed to stabilize model training. Then, we show that, the deep single-head attention also attends\nmultiple positions and is more effective than the shallow multi-head attention. More analyses are\nfurther conducted to verify our intuition.\n5\nTable 2: Deep single-head Transformer is harder to train than the shallow multi-head Transformer.\nTransformer-base BERT-base BERT-large\n8H-6L-6L 1H-48L-48L 12H-12L 1H-144L 16H-24L 1H-384L\nTraining ✓ ×/✓(w. Admin) ✓ ✓ ✓ ×/✓(w. Admin)\nTable 3: The model performance on the WMT’14 EN-DE dataset.\n8H-6L-6L 1H-48L-48L Evolved[So et al., 2019] 2D-CSANs[Yang et al., 2019] DARTSformer[Zhao et al., 2021]\nBLEU Param. BLEU Param. BLEU Param. BLEU Param. BLEU Param.\n27.90 63.2M 28.40 63.6M 28.4 64.1M 28.18 88.0M 28.4 65.2M\nTable 4: The model performance on dev sets of MNLI and SQuAD 2.0. The FLOPs are calculated\nfor the inference computation of one 512-length input sequence.\nFLOPs # Param. # MNLI Acc. SQuAD v2.0\nmatch mis-match exact match F1\n12H-12L BERTBASE 46.3B 109.5M 84.4 84.4 77.4 80.4\n1H-144L BERTBASE default 46.9B 110.0M 85.6 85.1 79.6 82.4\n1H-144L BERTBASE Admin 46.9B 110.0M 85.2 85.4 79.2 82.5\n16H-24L BERTLARGE 161.8B 335.1M 86.3 86.4 81.0 84.3\n1H-384L BERTLARGE Admin 164.1B 337.4M 87.7 87.5 82.6 85.7\n5.1 Experiment Setup\nTransformer Model Speciﬁcs. We conduct experiments with three Transformer models, i.e.,\nTransformer-base for the WMT’14 EN-DE translation task, BERT-base and BERT-large for the lan-\nguage model pre-training. Speciﬁcally, for machine translation, the original Transformer-base model\nis 8H-6L-6L3 Transformer encoder-decoder with 512-dimension word embedding, 64-dimension\nper-head attention output, and 2048-dimension feedforward network [Vaswani et al., 2017]. Here, we\ncompare it with 1H-48L-48L Transformer encoder-decoder with 512-dimension word embedding,\n64-dimension per-head attention output, and 256-dimension feedforward network. For language\nmodel pre-training, BERT-base model is 12H-12L Transformer encoder with 768-dimension word\nembedding, 64-dimension per-head attention output, and 3072-dimension feedforward network;\nBERT-large model is 16H-24L Transformer encoder with 1024-dimension word embedding, 64-\ndimension per-head attention output, and 4096-dimension feedforward network [Devlin et al., 2019].\nHere, we compare them with deep single-head BERT-base model (1H-144L Transformer encoder\nwith 768-dimension word embedding, single-head 64-dimension per-head attention output, and 256-\ndimension word embedding) and deep single-head BERT-large model (1H-384L Transformer encoder\nwith 768-dimension word embedding, 64-dimension per-head attention output, and 256-dimension\nword embedding). To stabilize 1H-48L-48L Transformer-base and 1H-384L BERT-large, we use the\nAdmin initialization [Liu et al., 2020b].\nTranslation. Here, we conduct experiments on WMT’14 EN-DE and evaluate model performance\nbased on their BLEU score on the test set and perplexity score on the development set. All hyper-\nparameter settings are adopted from Liu et al. [2020b], and are elaborated in the appendix.\nBERT. Here, we follow the training setting from Devlin et al. [2019] and evaluate pre-trained\nlanguage models on the SQuAD 2.0 [Rajpurkar et al., 2018] datasets for question answering, and\nthe GLUE benchmark [Wang et al., 2018], which includes 9 subtasks (as in Table 1). More detailed\nexperiment settings can be found in the appendix for reproduction.\n3We use “γH-αL(-βL)\" to denote that a model has γ-head α-layer encoder and γ-head β-layer decoder.\n6\nTable 5: The test performance on the GLUE benchmark with metrics described in Table 1.\nGLUE CoLA SST-2 MRPC SST-B QQP MNLI-m/mm QNLI RTE WNLI\n12H-12L 78.3 52.1 93.5 88.9/84.8 87.1/85.8 71.2/89.2 84.6/83.4 90.5 66.4 65.1\n1H-144L 79.4 59.2 94.2 89.3/85.4 84.3/83.5 70.9/88.9 85.1/84.3 91.0 69.0 65.1\n16H-24L 80.5 60.5 94.9 89.3/85.4 87.6/86.5 72.1/89.3 86.7/85.9 92.7 70.1 65.1\n1H-384L 81.3 62.7 95.1 90.5/87.2 86.9/86.3 71.3/89.1 87.4/86.5 93.3 72.7 65.1\n25 30 35 40 45 50 55 60 65\nModel Size (Millions of Parameters)\n23\n24\n25\n26\n27\n28BLEU Score on WMT14 En-De\nDeep Single-Head\nShallow Multi-Head\n30 40 50 60 70 80 90 100 110\nModel Size (Millions of Parameters)\n65.0\n67.5\n70.0\n72.5\n75.0\n77.5\n80.0Average Score on SQuAD 2.0\nShallow Multi-Head SQuAD 2.0\nDeep Single-Head SQuAD 2.0\n78\n79\n80\n81\n82\n83\n84\n85\n86\nAverage Accuracy on MNLI Matched & Mis-Matched\nShallow Multi-Head MNLI\nDeep Single-Head MNLI\nFigure 2: Performance Improvements under Different Model Size. Left: the performance of αH-6L-\n6L (α=1, 2, 4, 6, 8) and 1H-βL-βL (β=6,12,24,36,48), whose per-head dimension is the same with\nTransformer-base. Right: the performance of αH-12L (α=1, 3, 6, 12) and 1H-βL (β=12,36,72,144),\nwhose per-head dimension is the same with BERT-base.\n5.2 Stability Comparison\nAs in Table 2, after changing the shallow multi-head Transformer to the deep single-head Transformer,\nthe training fails to converge well for 2 out of 3 models. Note that, although the 1H-144L BERT-base\nmodel converges successfully, the model is sensitive to the choice of initialization. Speciﬁcally, the\nBERT-base model and BERT-large model are initialized with truncated normal distribution with\n0.02 variance, instead of following the common practice (e.g., using the Kaiming initialization [He\net al., 2015] or the Xavier initialization [Glorot and Bengio, 2010]). We observe that after changing\nthe variance of the initialization, or following the common practice, the training of the 1H-144L\nBERT-base model would also fail.\nMeanwhile, we show that, with the recent advances in deep learning, the training can be successfully\nstabilized by Adaptive Model Initialization (Admin), without changing any hyper-parameters Liu\net al. [2020b]. Also, after employing the Admin initializatioin, the 1H-144L BERT-base model can\nbe trained successfully when following the standard Xavier initialization. This shows that, although\nthe deep single-head Transformer is harder to train, the training difﬁculty is no longer an obstacle.\n5.3 Performance Comparison\nFor machine translation, we summarize the model performance in Table 3. With the same model size,\nthe deep single-head Transformer (1H-48L-48L) outperforms the shallow multi-head Transformer.\nAlso, the deep single-head Transformer achieves the same performance with the architecture search\nalgorithm the Evolved Transformer [So et al., 2019] and DARTSformer [Zhao et al., 2021], with\nthe same parameter number. Speciﬁcally, Evolved Transformer employs the evolution algorithm to\ndo neural architecture search on Transformer and DARTSformer employs the differentiable neural\narchitecture search algorithm, and both are based on multi-head attention. Our deep single-head\nTransformer achieves similar performance without hyper-parameter tuning, which further veriﬁes its\neffectiveness.\nWe summarized the model performance on the MNLI and SQuAD 2.0 in Table 4. Similar to machine\ntranslation, the deep single-head Transformer achieves consistent performance improvements over\nthe original shallow multi-head Transformer. Table 5 shows the test performance on the GLUE\nbenchmark. The deep single-head Transformer outperforms the shallow multi-head Transformer on 7\n7\nFigure 3: The Inference Speed of BERT-base with Different Batch Size and Sequence Length.\n50 100 150 200 250\nEpoch # (iterations over the training set)\n4.5\n4.6\n4.7\n4.8\n4.9\n5.0Development PPL on WMT14 En-De\n50 100 150 200 250 300 350\nGPU Hours\n4.5\n4.6\n4.7\n4.8\n4.9\n5.0\n6-Layer 8-Head Transformer\n48-Layer 1-Head Transformer\nFigure 4: Transformer Training Efﬁciency (GPU Hours are calculated for an idle RTX 3060).\nout of 9 tasks, and improves the average score (GLUE) by roughly 1 point. In the mean time, it is\nworth mentioning that, on 2 out of 3 sentence similarity/paraphrase tasks, the shallow multi-head\nTransformer achieves better performance. This indicates the deep single-head Transformer can be\nfurther improved, and we will further explore this in the future work.\nThese observations veriﬁed our intuition that the deep single-head Transformer could be more\neffective than the shallow multi-head Transformer. Together with previous analysis in Section 5.2,\nwe suggest that the main beneﬁt of the multi-head structure is the improved stability.\n5.4 Impact of Model Initialization\nPreviously, we showed that the initialization plays an important role in stabilizing model training (as\nin Table 2). Here, we aim to understand the impact of model initialization on the model performance.\nSpeciﬁcally, as the 1H-144L BERT-base model converges well with both the vanilla initialization\nand the Admin initialization, we not only conduct training with the Admin initialization, but also the\nvanilla initialization. As summarized in Table 5, the default initialization and the Admin initialization\nachieves similar performance. This observation supports our intuition that the major beneﬁts of the\nAdmin initialization is on training stability, and the performance improvements mostly come from\nthe change from shallow multi-head Transformer to deep single-head Transformer.\n5.5 Performance Improvements with Different Number of Heads\nIntuitively, the difference between deep single-head Transformer and shallow multi-head Transformer\nis proportional to the head number (e.g., the difference between 2H-6L and 1H-12L should be smaller\nthan the difference between 4H-6L and 1H-24L). Thus, we conduct experiments on Transformers\nwith different head numbers, i.e., αH-6L-6L (α=1, 2, 4, 6, 8 and feedforward network dimension\nis α·256) and 1H-βL-βL (β=6,12,24,36,48) for machine translation, αH-12L (α=1, 3, 6, 12 and\n8\nfeedforward network dimension is α·256) and 1H-βL (β=12,36,72,144) for BERT pre-training.\nNote that when αand βare set to the smallest value, the architectures of αH-6L-6L and 1H-βL-βL\n(or αH-12L and 1H-βL) are the same. The performance of deep single-head Transformer and shallow\nmulti-head Transformer are visualized in Figure 2, and veriﬁes our intuition that since the architecture\ndifference is between shallow multi-head Transformer and deep single-head Transformer is larger for\nmore number of heads, the performance improvement is also larger for more number of heads.\n5.6 GPU Inference Speed Comparison\nThe shallow multi-head Transformer and the deep single-head Transformer have roughly the same\nmodel size and the computation complexity (as in Table 3 and 4). Here, we try to verify that they also\nhave similar efﬁciency in practice.\nHere, we conduct experiments based on the Nvidia FasterTransformer implementation4. We calculated\nthe average inference speed on an idle RTX 3060 GPU, and summarized the results in Figure 3. It\nshows that the inference efﬁciency of the shallow multi-head Transformer and the deep single-head\nTransformer are roughly the same. Speciﬁcally, the shallow multi-head Transformer is slightly faster\nwhen the batch size and sequence length are smaller, and the deep single-head Transformer is slightly\nfaster when the batch size and the sequence length are larger.\n5.7 Performance v.s. Wall-Clock Time\nAlthough the deep single-head Transformer and the shallow multi-head Transformer have roughly\nthe same inference efﬁciency, we found the training computation of the multi-head Transformer is\nfaster in practice. We further conduct experiments to empirically compare the convergence speed of\nthe deep single-head Transformer and the shallow multi-head Transformer.\nAs in Figure 4, we can ﬁnd that the training computation speed of 1H-48L-48L Transformer is about\ntwo times slower than the 8H-6L Transformer. Meanwhile, the 8H-6L Transformer converges faster\nwith regard to epoch number, or GPU hours. This phenomenon veriﬁes our intuition that the network\ndepth of the 6-Layer Transformer has become a bottleneck of the model capacity, which restricts the\nmodel performance.\n6 Conclusion\nHere, we focus on understanding the effectiveness of the multi-head Transformer. We ﬁrst show that,\ndeep single-head Transformer also attends multiple positions and is more effective than the popular\nshallow multi-head Transformer. Then, we suggest the main advantage of the multi-head attention is\nthe training stability, since it has less number of layers than the single-head attention, when attending\nthe same number of positions. We also show that, with recent advances in deep learning, the training\nstability is no longer an obstacle and it can lead to consistent performance improvements by turning\nshallow single-head Transformer to deep multi-head Transformer.\nOur work opens up new possibilities to not only further push the state-of-the-art but understand\nthe effectiveness of Transformer better. It leads to various interesting future work. For example,\nintuitively, both shallow multi-head Transformer and deep single-head Transformer should not be the\noptimal architecture, and neural architecture search can be employed to ﬁnd a good balance between\nthe multi-head structure and the single-head structure.\nReferences\nJimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. ArXiv, abs/1607.06450,\n2016.\nAlexei Baevski and Michael Auli. Adaptive input representations for neural language modeling. In\nICLR, 2019.\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly\nlearning to align and translate. CoRR, abs/1409.0473, 2015.\n4We used the version 4.0 as in https://github.com/NVIDIA/FasterTransformer\n9\nMia Xu Chen, Orhan Firat, Ankur Bapna, Melvin Johnson, Wolfgang Macherey, George Foster, Llion\nJones, Niki Parmar, Michael Schuster, Zhi-Feng Chen, Yonghui Wu, and Macduff Hughes. The\nbest of both worlds: Combining recent advances in neural machine translation. In ACL, 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT, 2019.\nYong Fang, J. Gao, C. Huang, H. Peng, and R. Wu. Self multi-head attention-based convolutional\nneural networks for fake news detection. PLoS ONE, 14, 2019.\nXavier Glorot and Yoshua Bengio. Understanding the difﬁculty of training deep feedforward neural\nnetworks. In AISTATS, 2010.\nA. Graves, Greg Wayne, and Ivo Danihelka. Neural turing machines. ArXiv, abs/1410.5401, 2014.\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectiﬁers: Surpassing\nhuman-level performance on imagenet classiﬁcation. 2015 IEEE International Conference on\nComputer Vision (ICCV), pages 1026–1034, 2015.\nDan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv: Learning, 2016.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob Uszkoreit, Ian Simon, Curtis Hawthorne, Noam\nShazeer, Andrew M. Dai, Matthew D. Hoffman, Monica Dinculescu, and Douglas Eck. Music\ntransformer: Generating music with long-term structure. In ICLR, 2019.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. ArXiv,\nabs/2001.04451, 2020.\nLiyuan Liu, Haoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Jiawei\nHan. On the variance of the adaptive learning rate and beyond. In ICLR, 2020a.\nLiyuan Liu, X. Liu, Jianfeng Gao, Weizhu Chen, and J. Han. Understanding the difﬁculty of training\ntransformers. In EMNLP, 2020b.\nThang Luong, Hieu Pham, and Christopher D. Manning. Effective approaches to attention-based\nneural machine translation. ArXiv, abs/1508.04025, 2015.\nPaul Michel, Omer Levy, and Graham Neubig. Are sixteen heads really better than one? In NeurIPS,\n2019.\nToan Q. Nguyen and Julian Salazar. Transformers without tears: Improving the normalization of\nself-attention. In IWSLT, 2019.\nNiki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and\nDustin Tran. Image transformer. In ICML, 2018.\nHao Peng, Roy Schwartz, Dianqi Li, and Noah A. Smith. A mixture of h-1 heads is better than h\nheads. In ACL, 2020.\nPranav Rajpurkar, Robin Jia, and Percy Liang. Know what you don’t know: Unanswerable questions\nfor squad. In ACL, 2018.\nPrajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan Bello, Anselm Levskaya, and Jonathon\nShlens. Stand-alone self-attention in vision models. In NeurIPS, 2019.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations.\nIn NAACL-HLT, 2018.\nDavid So, Quoc Le, and Chen Liang. The evolved transformer. In International Conference on\nMachine Learning, pages 5877–5886. PMLR, 2019.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017.\nPetar Velickovic, Guillem Cucurull, A. Casanova, Adriana Romero, P. Lio’, and Yoshua Bengio.\nGraph attention networks. 2018a.\n10\nPetar Velickovic, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Liò, and Yoshua\nBengio. Graph attention networks. In ICLR, 2018b.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue:\nA multi-task benchmark and analysis platform for natural language understanding. In ICLR, 2018.\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S. Chao.\nLearning deep transformer models for machine translation. In ACL, 2019.\nXi Xiao, D. Zhang, Guangwu Hu, Y . Jiang, and Shutao Xia. Cnn-mhsa: A convolutional neural\nnetwork and multi-head self-attention combined approach for detecting phishing websites. Neural\nnetworks : the ofﬁcial journal of the International Neural Network Society, 125:303–312, 2020.\nRuibin Xiong, Yunchang Yang, Di He, Kai Zheng, Shu xin Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, Li-Wei Wang, and Tie-Yan Liu. On layer normalization in the transformer architecture.\nArXiv, abs/2002.04745, 2019.\nBaosong Yang, Longyue Wang, Derek Wong, Lidia S Chao, and Zhaopeng Tu. Convolutional\nself-attention networks. arXiv preprint arXiv:1904.03107, 2019.\nYuekai Zhao, Li Dong, Yelong Shen, Zhihua Zhang, Furu Wei, and Weizhu Chen. Memory-efﬁcient\ndifferentiable transformer architecture search. ArXiv, abs/2105.14669, 2021.\nAppendix\nImplementation Detail\nBesides the layer number and head number, we adopted all hyper-parameters from previous work.\nSpeciﬁcally, we followed [Liu et al., 2020b] for machine translation experiments and [Devlin et al.,\n2019] for language model pre-training experiments. It is worth mentioning that, in [Liu et al., 2020b],\nthe default initialization method is the Xavier initialization [Glorot and Bengio, 2010], which depends\non the size of the weight matrix. Here, to control variables, we ﬁx the initialization scale to be the same\nwith original multi-head shallow Transformer. Meanwhile, for language model pre-training, since\n[Devlin et al., 2019] ﬁxes the initialization scale for all models, we directly adopt the initialization\nstrategy without modiﬁcation.\nTraining Detail\nFor machine translation experiments, we followed [Liu et al., 2020b] to conduct data pre-processing,\nconduct model training on Nvidia GPUs (including Quadro RTX 8000, GeForce RTX 3060, and\nQuadro RTX A6000). As to language model pre-training experiments, we followed [Devlin et al.,\n2019] to conduct data pre-processing, conduct model training with Google TPU v3.\n11",
  "topic": "Head (geology)",
  "concepts": [
    {
      "name": "Head (geology)",
      "score": 0.6803551912307739
    },
    {
      "name": "Transformer",
      "score": 0.5771406888961792
    },
    {
      "name": "Computer science",
      "score": 0.48072078824043274
    },
    {
      "name": "Engineering",
      "score": 0.17790654301643372
    },
    {
      "name": "Electrical engineering",
      "score": 0.177018940448761
    },
    {
      "name": "Geology",
      "score": 0.10743334889411926
    },
    {
      "name": "Voltage",
      "score": 0.06836023926734924
    },
    {
      "name": "Geomorphology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157725225",
      "name": "University of Illinois Urbana-Champaign",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 23
}