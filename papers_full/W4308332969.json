{
    "title": "A deep ensemble learning-based automated detection of COVID-19 using lung CT images and Vision Transformer and ConvNeXt",
    "url": "https://openalex.org/W4308332969",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2135031755",
            "name": "Geng Tian",
            "affiliations": [
                "Hunan University of Technology",
                "Cipher Gene (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2103273906",
            "name": "Ziwei Wang",
            "affiliations": [
                "Hunan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098292663",
            "name": "Chang Wang",
            "affiliations": [
                "Hunan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2114343489",
            "name": "Jianhua Chen",
            "affiliations": [
                "Hunan Provincial Science and Technology Department"
            ]
        },
        {
            "id": "https://openalex.org/A2123439527",
            "name": "Guangyi Liu",
            "affiliations": [
                "Hunan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2100252417",
            "name": "He Xu",
            "affiliations": [
                "Hunan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2558360690",
            "name": "Yuankang Lu",
            "affiliations": [
                "Hunan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2530322981",
            "name": "Zhuoran Han",
            "affiliations": [
                "Northeast Normal University"
            ]
        },
        {
            "id": "https://openalex.org/A2121003022",
            "name": "Yubo Zhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2146048970",
            "name": "Zejun Li",
            "affiliations": [
                "Hunan Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2154075858",
            "name": "Xueming Luo",
            "affiliations": [
                "Hunan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2098689762",
            "name": "Lihong Peng",
            "affiliations": [
                "Hunan University of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2135031755",
            "name": "Geng Tian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2103273906",
            "name": "Ziwei Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098292663",
            "name": "Chang Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2114343489",
            "name": "Jianhua Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2123439527",
            "name": "Guangyi Liu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2100252417",
            "name": "He Xu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2558360690",
            "name": "Yuankang Lu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2530322981",
            "name": "Zhuoran Han",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2121003022",
            "name": "Yubo Zhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2146048970",
            "name": "Zejun Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2154075858",
            "name": "Xueming Luo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2098689762",
            "name": "Lihong Peng",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3173368240",
        "https://openalex.org/W3099905444",
        "https://openalex.org/W2765950793",
        "https://openalex.org/W3004906315",
        "https://openalex.org/W2963059730",
        "https://openalex.org/W4200119705",
        "https://openalex.org/W2524399695",
        "https://openalex.org/W3096831136",
        "https://openalex.org/W3005477624",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6725739302",
        "https://openalex.org/W2618530766",
        "https://openalex.org/W6682751323",
        "https://openalex.org/W4280587098",
        "https://openalex.org/W3159209990",
        "https://openalex.org/W3198147676",
        "https://openalex.org/W4220859562",
        "https://openalex.org/W6792155083",
        "https://openalex.org/W6810938606",
        "https://openalex.org/W2969790209",
        "https://openalex.org/W3131428626",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W3023180050",
        "https://openalex.org/W3138200812",
        "https://openalex.org/W1545528913",
        "https://openalex.org/W4285076853",
        "https://openalex.org/W6674914833",
        "https://openalex.org/W3016610966",
        "https://openalex.org/W6793164127",
        "https://openalex.org/W3127559895",
        "https://openalex.org/W3176797277",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W3096967708",
        "https://openalex.org/W4200110750",
        "https://openalex.org/W3008207212",
        "https://openalex.org/W4200566810",
        "https://openalex.org/W3200883733",
        "https://openalex.org/W3040149300",
        "https://openalex.org/W3020653337",
        "https://openalex.org/W3179572443",
        "https://openalex.org/W4236965008",
        "https://openalex.org/W3010096538",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2154579312",
        "https://openalex.org/W3138516171"
    ],
    "abstract": "Since the outbreak of COVID-19, hundreds of millions of people have been infected, causing millions of deaths, and resulting in a heavy impact on the daily life of countless people. Accurately identifying patients and taking timely isolation measures are necessary ways to stop the spread of COVID-19. Besides the nucleic acid test, lung CT image detection is also a path to quickly identify COVID-19 patients. In this context, deep learning technology can help radiologists identify COVID-19 patients from CT images rapidly. In this paper, we propose a deep learning ensemble framework called VitCNX which combines Vision Transformer and ConvNeXt for COVID-19 CT image identification. We compared our proposed model VitCNX with EfficientNetV2, DenseNet, ResNet-50, and Swin-Transformer which are state-of-the-art deep learning models in the field of image classification, and two individual models which we used for the ensemble (Vision Transformer and ConvNeXt) in binary and three-classification experiments. In the binary classification experiment, VitCNX achieves the best recall of 0.9907, accuracy of 0.9821, F1-score of 0.9855, AUC of 0.9985, and AUPR of 0.9991, which outperforms the other six models. Equally, in the three-classification experiment, VitCNX computes the best precision of 0.9668, an accuracy of 0.9696, and an F1-score of 0.9631, further demonstrating its excellent image classification capability. We hope our proposed VitCNX model could contribute to the recognition of COVID-19 patients.",
    "full_text": "TYPE Original Research\nPUBLISHED /zero.tnum/four.tnum November /two.tnum/zero.tnum/two.tnum/two.tnum\nDOI /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nOPEN ACCESS\nEDITED BY\nQi Zhao,\nUniversity of Science and Technology\nLiaoning, China\nREVIEWED BY\nGuangzhou Xiong,\nHuazhong University of Science and\nTechnology, China\nXueying Zeng,\nOcean University of China, China\n*CORRESPONDENCE\nZejun Li\nlzjfox@hnit.edu.cn\nXueming Luo\nlionver@hut.edu.cn\nLihong Peng\nplhhnu@/one.tnum/six.tnum/three.tnum.com\n†These authors have contributed\nequally to this work and share ﬁrst\nauthorship\nSPECIALTY SECTION\nThis article was submitted to\nSystems Microbiology,\na section of the journal\nFrontiers in Microbiology\nRECEIVED /two.tnum/one.tnum August /two.tnum/zero.tnum/two.tnum/two.tnum\nACCEPTED /one.tnum/six.tnum September /two.tnum/zero.tnum/two.tnum/two.tnum\nPUBLISHED /zero.tnum/four.tnum November /two.tnum/zero.tnum/two.tnum/two.tnum\nCITATION\nTian G, Wang Z, Wang C, Chen J, Liu G,\nXu H, Lu Y, Han Z, Zhao Y, Li Z, Luo X\nand Peng L (/two.tnum/zero.tnum/two.tnum/two.tnum) A deep ensemble\nlearning-based automated detection\nof COVID-/one.tnum/nine.tnum using lung CT images\nand Vision Transformer and ConvNeXt.\nFront. Microbiol./one.tnum/three.tnum:/one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum.\ndoi: /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nCOPYRIGHT\n© /two.tnum/zero.tnum/two.tnum/two.tnum Tian, Wang, Wang, Chen, Liu,\nXu, Lu, Han, Zhao, Li, Luo and Peng.\nThis is an open-access article\ndistributed under the terms of the\nCreative Commons Attribution License\n(CC BY)\n. The use, distribution or\nreproduction in other forums is\npermitted, provided the original\nauthor(s) and the copyright owner(s)\nare credited and that the original\npublication in this journal is cited, in\naccordance with accepted academic\npractice. No use, distribution or\nreproduction is permitted which does\nnot comply with these terms.\nA deep ensemble\nlearning-based automated\ndetection of COVID-/one.tnum/nine.tnum using\nlung CT images and Vision\nTransformer and ConvNeXt\nGeng Tian /one.tnum,/two.tnum†, Ziwei Wang /one.tnum†, Chang Wang /one.tnum, Jianhua Chen /three.tnum,\nGuangyi Liu /one.tnum, He Xu /one.tnum, Yuankang Lu /one.tnum, Zhuoran Han /four.tnum,\nYubo Zhao/five.tnum, Zejun Li /six.tnum*, Xueming Luo /one.tnum* and Lihong Peng /one.tnum,/seven.tnum*\n/one.tnumSchool of Computer Science, Hunan University of Technology, Zhuzhou, China, /two.tnumGeneis (Beijing)\nCo., Ltd., Beijing, China, /three.tnumHunan Storm Information Technology Co., Ltd., Changsha, China, /four.tnumHigh\nSchool Attached to Northeast Normal University, Changchun, China , /five.tnumNo. /two.tnum Middle School of\nShijiazhuang, Shijiazhuang, China, /six.tnumSchool of Computer Science, Hunan Institute of Technology,\nHengyang, China, /seven.tnumCollege of Life Sciences and Chemistry, Hunan University of Tec hnology,\nZhuzhou, China\nSince the outbreak of COVID-/one.tnum/nine.tnum, hundreds of millions of people have been\ninfected, causing millions of deaths, and resulting in a heavy impact on\nthe daily life of countless people. Accurately identifying pat ients and taking\ntimely isolation measures are necessary ways to stop the spread of COVID-\n/one.tnum/nine.tnum. Besides the nucleic acid test, lung CT image detection is also apath to\nquickly identify COVID-/one.tnum/nine.tnum patients. In this context, deep learning technology\ncan help radiologists identify COVID-/one.tnum/nine.tnum patients from CT images rapidly. In\nthis paper, we propose a deep learning ensemble framework called VitCNX\nwhich combines Vision Transformer and ConvNeXt for COVID-/one.tnum/nine.tnum CT image\nidentiﬁcation. We compared our proposed model VitCNX with Eﬃcie ntNetV/two.tnum,\nDenseNet, ResNet-/five.tnum/zero.tnum, and Swin-Transformer which are state-of-the-art deep\nlearning models in the ﬁeld of image classiﬁcation, and two indiv idual models\nwhich we used for the ensemble (Vision Transformer and ConvNeXt) in binary\nand three-classiﬁcation experiments. In the binary classiﬁcation experiment,\nVitCNX achieves the best recall of /zero.tnum./nine.tnum/nine.tnum/zero.tnum/seven.tnum, accuracy of /zero.tnum./nine.tnum/eight.tnum/two.tnum/one.tnum, F/one.tnum-score of\n/zero.tnum./nine.tnum/eight.tnum/five.tnum/five.tnum, AUC of /zero.tnum./nine.tnum/nine.tnum/eight.tnum/five.tnum, and AUPR of /zero.tnum./nine.tnum/nine.tnum/nine.tnum/one.tnum, which outperforms the other\nsix models. Equally, in the three-classiﬁcation experiment, VitCNX computes\nthe best precision of /zero.tnum./nine.tnum/six.tnum/six.tnum/eight.tnum, an accuracy of /zero.tnum./nine.tnum/six.tnum/nine.tnum/six.tnum, and an F/one.tnum-score of\n/zero.tnum./nine.tnum/six.tnum/three.tnum/one.tnum, further demonstrating its excellent image classiﬁcation capability. We\nhope our proposed VitCNX model could contribute to the recognit ion of\nCOVID-/one.tnum/nine.tnum patients.\nKEYWORDS\nCOVID-/one.tnum/nine.tnum, CT scan image, deep ensemble, Vision Transformer, ConvNeXt\nFrontiers in Microbiology /zero.tnum/one.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nIntroduction\nIn March 2020, the World Health Organization declared\nCOVID-19 as an international pandemic disease due to its\nrapid and strong transmission (\nCascella et al., 2022 ). Until\n22 April 2022, the pandemic has caused about 6.213 million\ndeaths worldwide, over 505.8 million people have been infected\nwith this virus, and there are up to ∼ 700 thousand new cases\nwithin 24 h of that time (Geneva:\nWorld Health Organization,\n2020; Wang et al., 2021 ). Diﬀerent from SARS, the new\ncoronavirus did not disappear quickly or cause limited losses\n(\nStadler et al., 2003 ). On the contrary, its Delta and Omicron\nvariants induced new pandemics worldwide after multiple\nmutations (\nVasireddy et al., 2021 ; V’kovski et al., 2021 ; Yu\net al., 2021 ; Del Rio et al., 2022 ). It has also caused a sustained\nimpact on the global economy. Long-term shutdowns left many\npeople unemployed. Many countries enforced lockdowns during\nperiodical outbreaks, which resulted in a global economic\nrecession (\nAlshater et al., 2021 ; Padhan and Prabheesh, 2021 ).\nAlthough vaccines have been researched and developed to\nprevent COVID-19 transmission to a certain extent, there is still\na need to adopt various methods to detect the virus and prevent\nits spread.\nAs a highly contagious respiratory disease, the clinical\nsymptoms of COVID-19 are similar to the common ﬂu and\ncommon pneumonia, for instance, coughing, dyspnea, dizziness,\nand some mild symptoms (\nZhang et al., 2020 ). But the patient\ninfected by the novel coronavirus may deteriorate into fatal acute\nrespiratory distress syndrome in a very short period of time\n(\nGuan, 2020 ). As a result, it greatly increases the diﬃculty of\nits early detection and places higher demands on the healthcare\nsystem for its treatment. Therefore, the eﬃcient and accurate\nidentiﬁcation of COVID-19 in patients has become a key to\npreventing its spread. The nucleic acid test is currently the most\nwidely used due to its high accuracy, simple operation, and low\ncost (\nTahamtan and Ardebili, 2020 ). But the paucity of standard\nlaboratory environments with specially trained staﬀ has limited\nthe entire testing process.\nAs an alternative, the non-invasive detection technology,\nComputed Tomography (CT) provides a new rapid detection\nmethod for detecting COVID-19. After the patient has\nundergone a lung CT scan, experienced radiologists can quickly\nﬁnd typical lesions in the patient’s lungs, such as ground-glass\nopacity, consolidation, and interlobular interstitial thickening by\nreading the CT images (\nChung et al., 2020 ; Xu et al., 2020 ). We\ncan also detect COVID-19 in a short time by combining patients’\nclinical symptoms and investigating recent social situations\nusing epidemiological survey methods. It can help medical\nworkers and epidemic management departments to quickly deal\nwith patients and deploy new prevention and control strategies,\nand thus intervene in the treatment of patients as early as\npossible to control its contagion.\nHowever, during the initial stage of the epidemic outbreak,\nthe massive inﬂux of patients often means medical staﬀ and\nhealthcare professionals have to work 24 h a day, which has a bad\neﬀect on the physical and mental health of doctors and aﬀects\nthe accuracy and eﬃciency of the medical diagnosis (\nZhan\net al., 2021 ). Alternatively, artiﬁcial intelligence technology\nis a quite eﬃcient strategy and obtains wide application in\nvarious ﬁelds (\nChen et al., 2019 ; Liu et al., 2021a , 2022a,b; Tang\net al., 2021 ; Wang et al., 2021 ; Zhang et al., 2021 ; Liang et al.,\n2022; Sun et al., 2022 ; Yang et al., 2022 ), and can be used to\ncomplement the work of radiologists. It can eﬃciently assist\nmedical staﬀ in judging symptoms, for example, pre-classifying\npathological images or predicting sampling results, and thus\ncan greatly reduce their working intensity. Particularly, deep\nlearning has achieved optimal performance in medical image\nprocessing (\nMunir et al., 2019 ). For instance, Sohail et al.\n(2021) used a modiﬁed deep residual neural network to detect\npathological tissue images of breast cancer and implemented\nautomated tumor grading by detecting cell mitosis. Similarly,\nCodella et al. (2017) introduced a deep ensemble model\nfor pathological image segmentation of skin cancer and the\ndetection of melanoma to improve the detection eﬃciency of\nskin cancer.\nDou et al. (2016) established a three-dimensional\nmulti-layer convolution model to detect pulmonary nodules in\nlung stereoscopic CT images, thereby reducing the false positive\nrate of automated pulmonary nodule detection.\nFarooq and\nHafeez (2020) proposed a ResNet-based COVID-19 screening\nsystem to assist radiologists to diagnose. Aslan et al. (2021)\ndeveloped a new type of COVID-19 infection detection system\nbased on convolutional neural networks (CNN) by combining\nthe long short-term memory (LSTM) network model. These\nmethods eﬀectively improved the identiﬁcation performance of\nCOVID-19-related CT images. In this paper, we propose a deep-\nlearning ensemble model by integrating Vision Transformer\n(\nDou et al., 2016 ) and ConvNeXt ( Liu et al., 2022c ) to eﬀectively\nimprove the prediction accuracy of COVID-19-related\nCT images.\nMaterials and methods\nMaterials\nWe constructed a comprehensive dataset by integrating and\nscreening data from three lung CT datasets ( Soares et al., 2020 ;\nYang et al., 2020 ). Dataset 1 contained a total of 4,171 images,\nwhere 2,167 images were from COVID-19 patients, 757 were\nfrom healthy people, and 1,247 were from other pneumonia\npatients. Dataset 2 contained a total of 2,481 images, where\n1,252 images were from COVID-19 patients, and 1,229 were\nfrom healthy people; both datasets 1 and 2 were from São Paulo,\nBrazil. Dataset 3 was from Wuhan, China, and included 746 CT\nFrontiers in Microbiology /zero.tnum/two.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nimages, of which 349 were from COVID-19 patients and 397\nwere from healthy people. Using these datasets we constructed\nan integrated dataset with a total of 7,398 CT images, which\nhad 3,768 CT images of COVID-19 patients, 2,383 healthy CT\nimages, and 1,247 CT images of other pneumonia patients.\nMethods\nWe investigated various CNN and transformer models and\nchose Vision Transformer and ConvNeXt as the basic classiﬁer\nof the ensemble model.\nVision transformer\nTransformers have been widely used in the natural language\nprocessing ﬁeld since it was proposed in 2017 (\nVaswani et al.,\n2017). It constructs basic decoder units by connecting the\nfeed-forward neural network and the self-attention mechanism\n(\nBahdanau et al., 2014 ), as well as adding an encoder-\ndecoder self-attention layer between the two network structures.\nIt creates a brand-new structure that diﬀers from CNN\nwhile obtaining relatively high accuracy. The self-attention\nmechanism used in the transformer ﬁrst converts the input text\ninto an embedding vector based on word embedding progress.\nNext, the obtained embedding vectors are used as inputs (named\nQueries, Keys, and Values) of the self-attention mechanism by a\nseries of multiplication operations. Finally, the output of the self-\nattention layer is computed using Equation (1) and is fed to the\nnext fully connected layer.\nAttention ( Q, K, V) = softmax\n(\nQKT\n√\ndk\n)\nV (1)\ndk = dim ( K)\nIn 2020, Dosovitskiy et al. built Vision Transformer for\nimage classiﬁcation. It achieved powerful classiﬁcation ability\ncomparable to the top CNN models on multiple datasets\n(CIFAR-100, ImageNet, etc.) (\nDosovitskiy et al., 2020 ).\nAs shown in Figure 1, the main architecture of the Vision\nTransformer model is mainly composed of three parts: First is\nthe embedding layer which is used to convert an image into\na vector that the transformer encoder can recognize. It also\nplays a role in embedding position information. The second\nis the transformer encoder layer which is used to extract\nfeatures. Finally, a multi-layer perceptron head is used to feature\ndimension reduction and classify images.\nThe embedding layer\nWe used Vision Transformer-B/16-224 to classify COVID-\n19-related images. The procedure for embedding the layer\nis shown in\nFigure 2. First, an original image is resized to\nthe following dimensions: 224 ∗224∗3. Second, the image is\nsegmented into blocks of 16 ∗16∗3 according to the VIT-B/16-\n224 conﬁguration, thereby generating 14 ∗14 = 196 (224/16 =\n14) blocks. Third, each block is mapped on a 768-dimensional\nvector through linear mapping. Finally, a matrix of 196 ∗768 size\nis obtained as the basic input token.\nIn the original transformer model, all vectors need to embed\nposition vectors to represent the spatiotemporal information\nof the original input. Similarly, Vision Transformer takes the\nlocation information as a trainable parameter and adds it to\nthe token after the image is converted into a vector. The token\nis extended by one dimension, and a trainable parameter that\nrepresents the class or label is added to this new dimension to\nrepresent the original class or label of the token for training. The\nobtained ﬁnal vector is input into the Transformer Encoder as\na token.\nTransformer encoder layer\nAs shown in\nFigure 3, the encoder layer mainly includes\nlayer normalization (LN), multi-head attention (MHA) block,\ndropout, and multi-layer perceptron (MLP) block. The core of\nthis structure is the parallel attention mechanism processing\nlayer called multi-head attention. First, the input token matrix is\nnormalized through layer normalization. Second, three matrices\nQ, K, and V are obtained by multiplying WQand WK , which\nare the same as the self-attention module. Third, Q, K, and\nV are divided into a matrix equal to the number of heads h\nby multiples of WQ\ni , WK\ni , WV\ni . The corresponding Qi, Ki, Vi\nmatrix of each head is then used to compute the respective\nattention score using Equation (2):\nheadi = Attention\n(\nQWQ\ni , KWK\ni , VWV\ni\n)\n(2)\nWQ\ni ∈ Rdmodel× dq , WK\ni ∈ Rdmodel× dk , WV\ni ∈ Rdmodel× dv ,\ndq = dk = dv = dmodel/ h\nFinally, the output of the MHA layer is obtained by\nconcatenating all heads and multiplying a matrix-like full\nconnection using Equation (3):\nMultiHead(Q, K, V) = Concat\n(\nhead1, . . ., headh\n)\nWo (3)\nWo ∈ Rhdv× dmodel\nThe output of the entire transformer encoder layer can be\nobtained through a residual connection both before and after the\nMHA and MLP layers. And the encoder layer of the entire model\nis usually formed by stacking multiple transformer encoders.\nMLP head\nThe main role of the MLP head is to obtain the high-\ndimensional features and obtain the ﬁnal classiﬁcation result.\nFrontiers in Microbiology /zero.tnum/three.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nFIGURE /one.tnum\nConcise structure of Vision Transformer.\nFIGURE /two.tnum\nStructure of embedding layer in Vision Transformer. The darke r green wider rectangles represent the ﬂattened feature vector of each block of an\nimage, while the pink wider rectangles represent the feature v ectors corresponding to classes, and the brown narrower rectangle represents the\nspatiotemporal information of the image.\nThe outputs of the transformer encoder layer (197 ∗768 in VIT\nB16/224) are used to compute the classiﬁcation probability of an\nimage. That is, the output of the transformer encoder layer is\na 197 ∗768 matrix, whose sizes are the same as the input of the\ntransformer encoder layer. Finally, only one 768-dimensional\nvector is used as the input for the MLP head to obtain the\nclassiﬁcation result of an image corresponding to the matrix.\nConvNeXt\nCNN is a classic neural network structure. Lenet was used\nfor handwritten digit recognition as the earliest convolutional\nneural network model (\nLeCun et al., 1989 ). Due to the\nlimitation of the lack of computer performance and the\ndiﬃculty of collecting large-scale datasets in the 1990s, CNN\ndid not achieve outstanding results in the 20 years that followed.\nIn 2012,\nKrizhevsky et al. (2012) proposed the AlexNet CNN\nmodel, which defeated all image classiﬁcation models at the\nILSVRC2012 competition (\nRussakovsky et al., 2015 ). The\nfollowing CNN models, for instance, VGGNet ( Simonyan\nand Zisserman, 2014 ) and GoogleNet ( Szegedy, 2015 ),\nhave become prevalent in many AI application ﬁelds. The\nconcept of residual and bottleneck layer proposed by the\nResNet (\nHe et al., 2016 ) model in 2015 again improved the\nperformance of CNN. It eﬀectively avoids the gradient problem\ncaused by deeper layers. The generative adversarial network\n(GAN) proposed by\nGoodfellow et al. (2014) divided the\nnetwork into two parts including generation and discriminator\nbased on game theory to achieve better performance through\niterative evolutions.\nSince the transformer structure came into being in 2020,\nCNN has not become obsolete. On the contrary, the ConvNeXt\nnetwork was introduced. ConvNeXt absorbs the advantages\nof multiple transformer structures in the network structure\nsetting and parameter selection. It outperformed the most\npowerful transformer model named swin-transformer (\nLiu\net al., 2021a ,b) on the ImageNet-1K dataset by adjusting\ntraining parameter settings, optimizer, and convolution\nkernel sizes.\nAs shown in\nFigure 4, ConvNeXt has a pretty concise\nstructure. Its performance is greatly improved to the original\nResNet although it is quite similar to ResNet. Moreover, it not\nFrontiers in Microbiology /zero.tnum/four.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nFIGURE /three.tnum\nStructure of encoder layer in Vision Transformer.\nFIGURE /four.tnum\nConcise structure of ConvNeXt.only demonstrates better performance than many classic CNN\nmodels but also outperforms many transformer models.\nFirst, ConvNeXt starts training ResNet-50 using techniques\nsimilar to training transformer models, such as better\noptimizers, more eﬃcient hyper-parameter settings, and\nnew data augmentation methods. Second, various new\noptimization strategies are gradually applied to optimize the\nmodel, for instance, setting new layer numbers and larger\nconvolution kernels. And eventually, ConvNeXt outperforms\nthe transformer model on the ImageNet-1K dataset.\nThe overall structure of ConvNeXt is very similar to ResNet-\n50. It includes the feature extraction layer of the head, the\nFrontiers in Microbiology /zero.tnum/five.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nmiddle layer where the bottleneck structure of four diﬀerent\ndimensions is separately stacked, and the ﬁnal high-dimensional\nfeature classiﬁcation layer. However, the strategy of stacking\nand the interior of each layer has undergone several changes.\nThe changes include: (i) In each stage of the original ResNet-\n50, the stacking number of each block is 3:4:6:3; in ConvNeXt\nthis has been revised to 3:3:9:3, which is similar to the block\nstacking of the transformer model. (ii) In the block of ResNet-\n50, the bottleneck design is to reduce the dimension ﬁrst, then\nfeature extraction, and ﬁnally increase the dimension. However,\nas shown in\nFigure 5, the bottleneck in ConvNeXt is designed\nto run feature extraction ﬁrst, then reduce the dimension, and\nﬁnally increase the dimension. (iii) It has modiﬁed the size of\nthe convolution kernel to 7 ∗7 from the ResNet 3 ∗3. (iv) Its\nactivation function has also been replaced from ReLU to GELU,\nand cut back the usage count of activation functions. (v) Its\nnormalization has changed to layer normalization from batch\nnormalization as well as reduced usage count of normalization.\nThe performance of ConvNeXt has gradually improved and\neven outperforms the VIT through the above ﬁve strategies\nand a few other settings including new parameters, structures,\nand functions.\nEnsemble\nAs shown in the pipeline in\nFigure 6, we can obtain the\nﬁnal classiﬁcation results by integrating the results of the\nVision Transformer and ConvNeXt based on the soft voting\nmechanism using Equation (4):\nSf = α Sv + ( 1 − α ) Sc (4)\nWhere S v and S c denote the classiﬁcation scores from\nVision Transformer and ConvNeXt for all images, respectively.\nFIGURE /five.tnum\nDiﬀerences between ConvNeXt and ResNet in bottleneck.\nFrontiers in Microbiology /zero.tnum/six.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nFIGURE /six.tnum\nPipeline of ViTCNX.\nFrontiers in Microbiology /zero.tnum/seven.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nResults\nExperimental evaluation and parameter\nsettings\nWe used six metrics to evaluate the performance of all\nclassiﬁcation models, that is, precision, recall, accuracy, F1-\nscore, AUC, and AUPR. These six evaluation metrics are deﬁned\nas follows:\nPrecision = TP\nTP + FP (5)\nRecall = TP\nTP + FN (6)\nAccuracy = TP + TN\nTP + TN + FP + FN (7)\nF1 − Score = 2∗Precision∗Recall\nPrecision + Recall (8)\nTPR ( Ture Positive Rate ) = TP\nTP + FN (9)\nFPR\n(\nFalse Positive Rate\n)\n= FP\nTN + FP (10)\nAUC is the area under the TPR-FPR curve. AUPR is the\narea under the precision-recall curve. For COVID-19-related\nimage binary classiﬁcation, precision means the proportion\nof images that are COVID-19-related images in the dataset\nand are predicted to be COVID-19-related among all the\npredicted COVID-19 images. Recall represents the proportion\nof images that are COVID-19-related images in the dataset\nand are predicted to be COVID-19-related among all COVID-\n19-related images in the dataset. Accuracy represents the\nproportion that is correctly predicted. F1-Score, AUC, and\nAUPR are comprehensive metrics that consider precision,\nrecall, and FPR.\nTo investigate the performance of our proposed ViTCNX\nmodel in diﬀerent classiﬁcation situations, we conducted\nexperiments under binary classiﬁcation and three-class\nclassiﬁcation, respectively. In the ViTCNX, the dataset\nwas randomly initialized with seed = 8. ConvNeXt uses\nConvNeXt_tiny to construct and initialize parameters,\nand its initial learning rate was set to 5e-4, and the initial\nweight adopted the convnext_tiny_1k_224_ema. The Vision\nTransformer uses vit_base_patch16 to construct and initialize\nparameters, and its initial learning rate was set to 1e-3. It\nadopted the initial weight vit_base_patch16_224_in21k. In\nall image classiﬁcation algorithms, the training epoch and\nthe batch size were set to 100 and 8, respectively. DenseNet,\nResNet-50, Swin Transformer, and EﬃcinetNetV2 used\ndensenet121, resnet50-pre, swin_tiny_patch4_window7_224,\nand pre_eﬃcientnetv2-s to initialize their weight parameters,\nrespectively. The corresponding learning rates were 1e-3, 1e-4,\n1e-4, and 1e-3, respectively. ViTCNX used the same parameter\nsettings as individual Vision Transformer and ConvNeXt.\nAfter comparing the image classiﬁcation ability under diﬀerent\nTABLE /one.tnum Performance of ViTCNX and the other six models under the\nbinary classiﬁcation.\nMetrics Precision Recall Accuracy F1-score AUC AUPR\nEﬃcientNetV2 0.9920 0.3293 0.5875 0.4945 0.9609 0.9738\nConvNeXt 0.9650 0.9894 0.9715 0.9770 0.9952 0.9968\nDenseNet 0.9788 0.9814 0.9756 0.9801 0.9973 0.9983\nSwin\nTransformer\n0.9587 0.9548 0.9471 0.9568 0.9911 0.9945\nResNet-50 0.9892 0.9695 0.9748 0.9792 0.9970 0.9979\nVision\nTransformer\n0.9815 0.9854 0.9797 0.9834 0.9985 0.9990\nViTCNX 0.9803 0.9907 0.9821 0.9855 0.9985 0.9991\nBold values means the highest score under this metric.\nvalues of α , we set α = 0.6 where ViTCNX computed the\nbest performance.\nBinary classiﬁcation for CT images\nUnder the binary classiﬁcation of images, there were a total\nof 6,151 CT images, including 3,768 CT images from COVID-\n19 patients and 2,383 CT images from healthy individuals. The\n6,151 images were divided into a ratio of 0.8:0.2. Consequently,\n4,922 images were used as the training set, including 3,015\nCOVID-19-related images and 1,907 CT images from healthy\nindividuals. The remaining 1,229 images were used as the\ntest set, including 753 COVID-19-related CT images and 476\nhealthy images. We compared our proposed ViTCNX model\nwith four state-of-the-art image classiﬁcation algorithms, that is,\nDenseNet (\nHuang et al., 2017 ), ResNet-50, Swin Transformer,\nand EﬃcinetNetV2 ( Tan and Le, 2021 ). In addition, ViTCNX\nwas also compared with the two individual models it was\ncomprised of, that is, Vision Transformer and ConvNeXt. The\nresults are shown in\nTable 1. The bold font in each column\nrepresents the best performance computed by the corresponding\nmethod among the above seven methods.\nTable 1 and Figure 7\nshow the precision, recall, accuracy, F1-score, AUC, and AUPR\nvalues and curves of these models.\nFrom\nTable 1 and Figure 7, we can ﬁnd that ViTCNX\nobtained the best recall, accuracy, F1-score, AUC, and\nAUPR, signiﬁcantly outperforming the other six methods.\nEﬃcientNetV2 achieved the best score of precision. This\nresult is consistent with the prediction results on the\nconfusion matrix. In the experiments, EﬃcientNetV2 computed\nhigher precision than ViTCNX. The reasons may be that\ndiﬀerent models perform very diﬀerently on diﬀerent parameter\nsettings, diﬀerent datasets, and diﬀerent sizes, which have a\nsigniﬁcant impact on the classiﬁcation performance of the\nmodel. In particular, ViTCNX outperforms its two individual\nmodels, Vision Transformer and ConvNeXt, demonstrating\nthat an ensemble of single classiﬁcation models can improve\nimage identiﬁcation performance.\nFigures 7B,C show the AUC\nFrontiers in Microbiology /zero.tnum/eight.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nFIGURE /seven.tnum\n(A) The performance comparison of VitCNX and six other models for COVID -/one.tnum/nine.tnum in binary classiﬁcation problems;(B,C) The AUC and AUPR\nvalues of VitCNX and six other models for COVID-/one.tnum/nine.tnum in binary classiﬁcation problems.\nand AUPR values obtained by the seven models. ViTCNX\noutperforms the other six models, elucidating that it can\neﬀectively classify related CT images as COVID-19-related\nor not.\nThree-classiﬁcation for CT images\nTo further investigate the performance of the seven models\nunder the three-classiﬁcation challenge, we considered a total\nof 7,398 CT images, including 3,768 images from COVID-\n19 patients, 2,383 from healthy individuals, and 1,247 from\nother pneumonia patients. The 7,398 images were divided in\na ratio of 0.8:0.2, resulting in 5,920 images in the training\nset and 1,478 images in the test set. The 5,920 images in the\ntraining set consisted of 3,015, 1,907, and 998 images from\nCOVID-19 patients, healthy individuals, and other pneumonia\npatients, respectively. The 1,478 images in the test set consisted\nof 753, 476, and 249 images from COVID-19 patients, healthy\nindividuals, and other pneumonia patients, respectively. We\nFrontiers in Microbiology /zero.tnum/nine.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\ntrained ViTCNX and the other comparable models using the\ntraining set and then evaluated their performance using the test\nset.\nTable 2 and Figure 8 show the precision, recall, accuracy, and\nF1-score values of ViTCNX and the other six models for the\nthree-classiﬁcation situation.\nFrom\nTable 2 and Figure 8, we can observe that ViTCNX\ncomputed the best precision, accuracy, and F1-score,\ngreatly outperforming the other six models. Although it\ncalculated a relatively lower recall of 0.9597 than Vision\nTransformer with a recall of 0.9599, the diﬀerence is very\nminor. Particularly, compared with Vision Transformer,\nConvNeXt, DenseNet, ResNet-50, Swin Transformer, and\nEﬃcientNetV2, ViTCNX computed a F1-score of 0.9631,\nbetter by 0.04, 1.58, 1.89, 5.32, 6.74, and 64.11% than\nthe six models, respectively. These results demonstrate\nthat ViTCNX can more accurately classify CT images\nfrom COVID-19, from other pneumonia cases, and\nhealthy individuals.\nTABLE /two.tnum Performance of ViTCNX and the other six models under\nthree classiﬁcation.\nMetrics Precision Recall Accuracy F1-Score\nEﬃcientNetV2 0.7783 0.4188 0.4526 0.3221\nConvNeXt 0.9562 0.9397 0.9574 0.9473\nDenseNet 0.9487 0.9402 0.9560 0.9442\nSwin Transformer 0.9259 0.8754 0.9127 0.8957\nResNet-50 0.9369 0.8936 0.9317 0.9100\nVision Transformer 0.9657 0.9599 0.9689 0.9627\nViTCNX 0.9668 0.9597 0.9696 0.9631\nBold values means the highest score under this metric.\nThe confusion matrix analysis\nWe further evaluated the number of true positives (TP), true\nnegatives (TN), false positives (FP), and false negatives (FN)\nobtained by Vision Transformer, ConvNeXt, DenseNet, ResNet-\n50, Swin Transformer, EﬃcientNetV2, and ViTCNX under\nbinary classiﬁcation.\nTable 3 and Figure 9 present the statistical\ndata of TP , TN, FP , and FN from the above seven models for\nbinary classiﬁcation. The importance of these four evaluation\nmetrics is not equal. For COVID-19 image recognition, TP\ndenotes the number of images that are COVID-19 images in\nthe dataset and are predicted to be COVID-19-related. FN\ndenotes the number of images that are COVID-19 images but\nare predicted to be non-COVID-19-related. FN denotes that\nthere are undetected COVID-19 patients, which may cause the\nspread of the pandemic. TP and FN are more important than the\nother two metrics. Higher TP and lower FN represent the better\nperformance of ViTCNX.\nTABLE /three.tnum Statistics of ViTCNX and other six models for binary\nclassiﬁcation.\nMetrics TP TN FP FN\nEﬃcientNetV2 248 474 2 505\nConvNeXt 745 449 27 8\nDenseNet 739 460 16 14\nSwin Transformer 719 445 31 34\nResNet-50 730 468 8 23\nVision Transformer 742 462 14 11\nViTCNX 746 461 15 7\nBold values means the highest score under this metric.\nFIGURE /eight.tnum\nThe performance of VitCNX and six other models for three-classiﬁ cation problem.\nFrontiers in Microbiology /one.tnum/zero.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nFIGURE /nine.tnum\nThe confusion matrix of results of VitCNX and six other models.\nFrontiers in Microbiology /one.tnum/one.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nFrom Table 3 and Figure 9, we can observe that our proposed\nViTCNX model screens the most TP , and the least FN compared\nto the other six models. Our proposed ViTCNX model computes\nthe highest TP of 746 and the lowest FN of 7 among 1,229 test\nsamples, demonstrating that it can most eﬃciently recognize\nCOVID-19-related images of COVID-19 patients.\nDiscussion and conclusion\nWith the rapid development of AI technology and high-\nperformance computing platforms, using deep learning models\nto detect COVID-19 through lung CT images has become\na research hotspot. Not only because this method has a\nhigher performance and faster speed, but also lower time and\neconomic cost. In this paper, we proposed an ensemble deep\nlearning model (ViTCNX) to recognize COVID-19-related CT\nimages by combining Vision Transformer and ConvNeXt. We\ncompared ViTCNX with six other state-of-the-art deep learning\nmodels (Vision Transformer, ConvNeXt, DenseNet, ResNet-\n50, Swin Transformer, and EﬃcientNetV2). We conducted a\nseries of comparative experiments to evaluate the performance\nof ViTCNX. The results show that ViTCNX computed the\nbest recall, accuracy, F1-score, AUC, and AUPR under binary\nclassiﬁcation and the best precision, accuracy, and F1-score\nunder three-classiﬁcation tests. Moreover, ViTCNX obtained\nthe highest TP and the lowest FN in binary classiﬁcation. The\nresults show that our proposed ViTCNX model has powerful\nCOVID-19-related image recognition ability.\nWe adopted several techniques to reduce over-ﬁtting. First,\nwe used three diﬀerent datasets of COVID-19 to evaluate the\nperformance of ViTCNX. The three datasets were collected from\ntwo diﬀerent places (Wuhan, China, and São Paulo, Brazil).\nWe integrated the three diﬀerent datasets into one dataset\nto increase the diﬀerences in datasets and further enhance\nthe generalization performance of ViTCNX. Additionally, we\nused techniques including layer normalization and dropout\nto prevent over-ﬁtting. The ensemble learning strategies also\nhelped to improve the model’s generalization ability and\nreduce over-ﬁtting.\nThere are two advantages of the proposed ViTCNX model:\nFirst, the variance is reduced through the ensemble of multiple\nmodels, thereby improving the robustness and generalization\nability of the model. Second, Vision Transformer and ConvNeXt\nare greatly diﬀerent in structure. An ensemble of them can\nlower their correlation and further reduce the classiﬁcation\nerror. Although ViTCNX obtains better performance, it does\nincrease a large number of training parameters, which increases\nthe training and testing time of the model and requires higher\ncomputational resources.\nIn the future, we will continuously update data to build\nlarger COVID-19 datasets to enhance the generalization ability\nof ViTCNX. We will also design a new deep learning\nframework, adopt eﬃcient training methods, and optimize\nparameter settings to improve the prediction ability of the\nmodel. Additionally, we will establish an automatic annotation\nmodel to autonomously label hot spots. We anticipate that our\nproposed ViTCNX model can contribute to the clinical detection\nof COVID-19.\nData availability statement\nThe original contributions presented in the study are\nincluded in the article/supplementary material, further inquiries\ncan be directed to the corresponding author/s.\nAuthor contributions\nGT, LP , ZW, XL, and ZL: conceptualization. GT, LP , ZW,\nCW, and ZL: methodology. ZW, GL, CW, ZH, YZ, and JC:\nsoftware. GT, LP , ZW, YL, HX, ZH, YZ, and GL: validation. GT,\nLP , XL, JC, and ZL: investigation. ZW, CW, GL, and HX: data\ncuration. LP and ZW: writing-original draft preparation and\nproject administration. GT and LP: writing-review and editing.\nGT, LP , and ZL: supervision and funding acquisition. All authors\nhave read and agreed to the published version of the manuscript.\nFunding\nZL was supported by the National Natural Science\nFoundation of China under Grant No. 62172158. LP was\nsupported by the National Natural Science Foundation of China\nunder Grant No. 61803151. GL and YL were supported by the\nInnovation and Entrepreneurship Training Program for College\nStudents of Hunan Province under Grant No. S202111535031\nand the Innovation and Entrepreneurship Training Program for\nCollege Students of the Hunan University of Technology under\nGrant No. 20408610119.\nConﬂict of interest\nAuthor GT was employed by the company Geneis (Beijing)\nCo., Ltd. Author JC was employed by Hunan Storm Information\nTechnology Co., Ltd.\nThe remaining authors declare that the research was\nconducted in the absence of any commercial or ﬁnancial\nrelationships that could be constructed as a potential conﬂict\nof interest.\nPublisher’s note\nAll claims expressed in this article are solely those of the\nauthors and do not necessarily represent those of their aﬃliated\norganizations, or those of the publisher, the editors and the\nreviewers. Any product that may be evaluated in this article, or\nclaim that may be made by its manufacturer, is not guaranteed\nor endorsed by the publisher.\nFrontiers in Microbiology /one.tnum/two.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nReferences\nAlshater, M.M., Atayah, O.F., and Khan, A. (2021). What do we k now about\nbusiness and economics research during COVID-19: a bibliome tric review. Econ.\nRes. 35, 1–29. doi: 10.1080/1331677X.2021.1927786\nAslan, M.F., Unlersen, M.F., Sabanci, K., and Durdu, A. (2021) . CNN-\nbased transfer learning–BiLSTM network: A novel approach for C OVID-19\ninfection detection. Appl. Soft Comput. 98, 106912. doi: 10.1016/j.asoc.2020.10\n6912\nBahdanau, D., Cho, K., and Bengio, Y. (2014). Neural machine translation by\njointly learning to align and translate. arXiv [Preprint]. arXiv: 1409.0473. Available\nonline at: https://arxiv.org/pdf/1409.0473.pdf\nCascella, M., Rajnik, M., Aleem, A., Dulebohn, S.C., and Di Napoli,\nR. (2022). Features, Evaluation, and Treatment of Coronavirus (COVID-19) .\nStatpearls.\nChen, X., Xie, D., Zhao, Q., and You, Z.H. (2019). MicroRNAs a nd complex\ndiseases: from experimental results to computational models. Brief. Bioinform.20,\n515–539. doi: 10.1093/bib/bbx130\nChung, M., Bernheim, A., Mei, X., Zhang, N., Huang, M., Zeng, X., et al. (2020).\nCT imaging features of 2019 novel coronavirus (2019-nCoV). Radiology. 295,\n202–207. doi: 10.1148/radiol.2020200230\nCodella, N.C., Nguyen, Q.B., Pankanti, S., Gutman, D.A., Helba, B., Halpern,\nA.C., et al. (2017). Deep learning ensembles for melanoma recogn ition\nin dermoscopy images. IBM J. Res. Dev. 61, 5. doi: 10.1147/JRD.2017.270\n8299\nDel Rio, C., Omer, S.B., and Malani, P.N. (2022). Winter of Omi cron—the\nevolving COVID-19 pandemic. JAMA 327, 319–320. doi: 10.1001/jama.2021.\n24315\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D. , Zhai, X., Unterthiner,\nT., et al. (2020). An image is worth 16x16 words: Transformers for image\nrecognition at scale. arXiv [Preprint]. arXiv: 2010.11929. Available online at:\nhttps://arxiv.org/pdf/2010.11929.pdf\nDou, Q., Chen, H., Yu, L., Qin, J., and Heng, P.A. (2016). Multi level\ncontextual 3-D CNNs for false positive reduction in pulmonary no dule detection.\nIEEE Transact. Biomed. Eng. 64, 1558–1567. doi: 10.1109/TBME.2016.261\n3502\nFarooq, M., and Hafeez, A. (2020). Covid-resnet: a deep learni ng framework\nfor screening of covid19 from radiographs. arXiv [Preprint]. arXiv: 2003.14395.\nAvailable online at: https://arxiv.org/pdf/2003.14395.pdf\nGoodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Fa rley, D., Ozair, S.,\net al. (2014). Generative adversarial nets. Adv. Neural Inf. Process. Syst.63, 139–144.\ndoi: 10.1145/3422622\nGuan, W.J., Ni, Z.Y., Hu, Y., Liang, W.H., Ou, C.Q., He, J.X., e t al. (2020). Clinical\ncharacteristics of 2019 novel coronavirus infection in Chi na. MedRxiv [Preprint].\ndoi: 10.1101/2020.02.06.20020974\nHe, K., Zhang, X., Ren, S., and Sun, J. (2016). “Deep residual learning for image\nrecognition, ” inProceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition (Las Vegas), 770–778.\nHuang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K.Q. (2 017). “Densely\nconnected convolutional networks, ” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition(Honolulu), 4700–4708.\nKrizhevsky, A., Sutskever, I., and Hinton, G.E. (2012). Imag enet classiﬁcation\nwith deep convolutional neural networks. Adv. Neural Inf. Process. Syst.60, 84–90.\ndoi: 10.1145/3065386\nLeCun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W.,\net al. (1989). Handwritten digit recognition with a back-propa gation network. Adv.\nNeural Inf. Process. Syst.2, 396–404.\nLiang, Y., Zhang, Z.Q., Liu, N.N., Wu, Y.N., Gu, C.L., and Wang , Y.L. (2022).\nMAGCNSE: predicting lncRNA-disease associations using multi- view attention\ngraph convolutional network and stacking ensemble model. BMC Bioinform.23,\n1–22. doi: 10.1186/s12859-022-04715-w\nLiu, H., Qiu, C., Wang, B., Bing, P., Tian, G., Zhang, X., et al. ( 2021a).\nEvaluating DNA methylation, gene expression, somatic mutatio n, and their\ncombinations in inferring tumor tissue-of-origin. Front. Cell Dev. Biol.9, 619330.\ndoi: 10.3389/fcell.2021.619330\nLiu, W., Jiang, Y., Peng, L., Sun, X., Gan, W., Zhao, Q., et al. ( 2022a).\nInferring gene regulatory networks using the improved Markov blanket discovery\nalgorithm. Interdiscipl. Sci. Comp. Life Sci.14, 168–181. doi: 10.1007/s12539-021-\n00478-9\nLiu, W., Lin, H., Huang, L., Peng, L., Tang, T., Zhao, Q., et al. (2022b).\nIdentiﬁcation of miRNA–disease associations via deep fore st ensemble learning\nbased on autoencoder. Brief. Bioinformat. 23, bbac104. doi: 10.1093/bib/bba\nc104\nLiu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., et al. (202 1b).\n“Swin transformer: Hierarchical vision transformer using shifted windows, ” in\nProceedings of the IEEE/CVF International Conference on Computer Vision,\n10012–10022.\nLiu, Z., Mao, H., Wu, C.Y., Feichtenhofer, C., Darrell, T., and Xie, S. (2022c). “A\nconvnet for the 2020s, ” in Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition(New Orleans), 11976–11986.\nMunir, K., Elahi, H., Ayub, A., Frezza, F., and Rizzi, A. (2019 ). Cancer\ndiagnosis using deep learning: a bibliographic review. Cancers 11, 235.\ndoi: 10.3390/cancers11091235\nPadhan, R., and Prabheesh, K.P. (2021). The economics of COV ID-19\npandemic: a survey. Econ. Anal. Policy 70, 220–237. doi: 10.1016/j.eap.2021.\n02.012\nRussakovsky, O., Deng, J., Su, H., Krause, J., Satheesh, S., Ma, S., et al. (2015).\nImagenet large scale visual recognition challenge. Int. J. Comput. Vis. 115, 211–252.\ndoi: 10.1007/s11263-015-0816-y\nSimonyan, K., and Zisserman, A. (2014). Very deep convolutio nal networks for\nlarge-scale image recognition. arXiv [Preprint]. arXiv: 1409.1556. Available online\nat: https://arxiv.org/pdf/1409.1556.pdf\nSoares, E., Angelov, P., Biaso, S., Froes, M. H., and Abe, D. K. (2020).\nSARSCoV-2 CT-scan dataset: a large dataset of real patients CT scans for\nSARS-CoV-2 identiﬁcation. MedRxiv [Preprint]. doi: 10.1101/2020.04.24.2007\n8584\nSohail, A., Khan, A., Wahab, N., Zameer, A., and Khan, S. (2021 ).\nA multi-phase deep CNN based mitosis detection framework for b reast\ncancer histopathological images. Sci. Rep. 11, 1–18. doi: 10.1038/s41598-021-\n85652-1\nStadler, K., Masignani, V., Eickmann, M., Becker, S., Abrign ani, S., Klenk, H.D.,\net al. (2003). SARS—beginning to understand a new virus. Nat. Rev. Microbiol.1,\n209–218. doi: 10.1038/nrmicro775\nSun, F., Sun, J., and Zhao, Q. (2022). A deep learning method fo r predicting\nmetabolite–disease associations via graph neural network. Brief. Bioinform. 23,\nbbac266. doi: 10.1093/bib/bbac266\nSzegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelo v, D., et al. (2015).\n“Going deeper with convolutions, ” in Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition(Boston, MA), 1–9.\nTahamtan, A., and Ardebili, A. (2020). Real-time RT-PCR in COV ID-19\ndetection: issues aﬀecting the results. Expert Rev. Mol. Diagn. 20, 453–454.\ndoi: 10.1080/14737159.2020.1757437\nTan, M., and Le, Q. (2021). “Eﬃcientnetv2: smaller models and fa ster training, ”\nin International Conference on Machine Learning(PMLR), 10096–10106.\nTang, X., Cai, L., Meng, Y., Xu, J., Lu, C., and Yang, J. (2021) . Indicator\nregularized non-negative matrix factorization method-bas ed drug repurposing\nfor COVID-19. Front. Immunol. 11, 603615. doi: 10.3389/ﬁmmu.2020.60\n3615\nVasireddy, D., Vanaparthy, R., Mohan, G., Malayala, S.V., and At luri, P. (2021).\nReview of COVID-19 variants and COVID-19 vaccine eﬃcacy: wh at the clinician\nshould know?. J. Clin. Med. Res.13, 317. doi: 10.14740/jocmr4518\nVaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., et al.\n(2017). Attention is all you need. Adv. Neural Inf. Process. Syst.30.\nV’kovski, P., Kratzel, A., Steiner, S., Stalder, H., and Thiel, V. (2021). Coronavirus\nbiology and replication: implications for SARS-CoV-2. Nat. Rev. Microbiol.19,\n155–170. doi: 10.1038/s41579-020-00468-6\nWang, C.C., Han, C.D., Zhao, Q., and Chen, X. (2021). Circular RNAs and\ncomplex diseases: from experimental results to computational mo dels. Brief.\nBioinform. 22, bbab286. doi: 10.1093/bib/bbab286\nWorld Health Organization (2020). WHO COVID-19 Dashboard. Geneva:\nWorld Health Organization. Available online at: covid19.who.int (accessed April\n25, 2022).\nXu, X., Yu, C., Qu, J., Zhang, L., Jiang, S., Huang, D., et al. (2 020).\nImaging and clinical features of patients with 2019 novel coro navirus SARS-\nCoV-2. Eur. J. Nucl. Med. Mol. Imaging47, 1275–1280. doi: 10.1007/s00259-020-\n04735-9\nFrontiers in Microbiology /one.tnum/three.tnum frontiersin.org\nTian et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/fmicb./two.tnum/zero.tnum/two.tnum/two.tnum./one.tnum/zero.tnum/two.tnum/four.tnum/one.tnum/zero.tnum/four.tnum\nYang, J., Ju, J., Guo, L., Ji, B., Shi, S., Yang, Z., et al. (2022) . Prediction of HER2-\npositive breast cancer recurrence and metastasis risk from h istopathological images\nand clinical information via multimodal deep learning. Comput. Struct. Biotechnol.\nJ. 20, 333–342. doi: 10.1016/j.csbj.2021.12.028\nYang, X., He, X., Zhao, J., Zhang, Y., Zhang, S., and Xie, P. (2 020). COVID-CT\ndataset: a CT scan dataset about COVID-19. arXiv [Preprint]. arXiv: 2003.13865.\nAvailable online at: https://arxiv.org/pdf/2003.13865.pdf\nYu, F., Lau, L.T., Fok, M., Lau, J.Y.N., and Zhang, K. (2021). COVID-19 Delta\nvariants—Current status and implications as of August 2021. Precis. Clin. Med.4,\n287–292. doi: 10.1093/pcmedi/pbab024\nZhan, H., Schartz, K., Zygmont, M.E., Johnson, J.O., and Kru pinski, E.A. (2021).\nThe impact of fatigue on complex CT case interpretation by radiolo gy residents.\nAcad. Radiol.28, 424–432. doi: 10.1016/j.acra.2020.06.005\nZhang, K., Liu, X., Shen, J., Li, Z., Sang, Y., Wu, X., et al. (20 20). Clinically\napplicable AI system for accurate diagnosis, quantitative measu rements, and\nprognosis of COVID-19 pneumonia using computed tomography. Cell 181,\n1423–1433. doi: 10.1016/j.cell.2020.04.045\nZhang, L., Yang, P., Feng, H., Zhao, Q., and Liu, H. (2021). Us ing network\ndistance analysis to predict lncRNA–miRNA interactions. Interdiscipl. Sci. 13,\n535–545. doi: 10.1007/s12539-021-00458-z\nFrontiers in Microbiology /one.tnum/four.tnum frontiersin.org"
}