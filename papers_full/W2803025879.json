{
  "title": "Retrospective Analysis of Clinical Performance of an Estonian Speech Recognition System for Radiology: Effects of Different Acoustic and Language Models",
  "url": "https://openalex.org/W2803025879",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A5020034964",
      "name": "Andrus Paats",
      "affiliations": [
        "Tallinn University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5023201445",
      "name": "Tanel Alumäe",
      "affiliations": [
        "Tallinn University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5102329333",
      "name": "Einar Meister",
      "affiliations": [
        "Tallinn University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5042591681",
      "name": "Ivo Fridolin",
      "affiliations": [
        "Tallinn University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2157951638",
    "https://openalex.org/W2409310431",
    "https://openalex.org/W1976253314",
    "https://openalex.org/W2043643141",
    "https://openalex.org/W1966843785",
    "https://openalex.org/W2745845505",
    "https://openalex.org/W1544392014",
    "https://openalex.org/W2014151772",
    "https://openalex.org/W38194800",
    "https://openalex.org/W150693643",
    "https://openalex.org/W1988595611",
    "https://openalex.org/W2140943358",
    "https://openalex.org/W2053115136",
    "https://openalex.org/W2963266252",
    "https://openalex.org/W2163680580",
    "https://openalex.org/W2155705765",
    "https://openalex.org/W289492037",
    "https://openalex.org/W336924564",
    "https://openalex.org/W2399924055"
  ],
  "abstract": null,
  "full_text": "Retrospective Analysis of Clinical Performance of an Estonian Speech\nRecognition System for Radiology: Effects of Different Acoustic\nand Language Models\nA. Paats 1,2 & T. Alumäe 3 & E. Meister 3 & I. Fridolin 1\nPublished online: 30 April 2018\n# The Author(s) 2018\nAbstract\nThe aim of this study was to analyze retrospectively the influence of different acoustic and language models in order to determine\nthe most important effects to the clinical performance of an Estonian language-based non-commercial radiology-oriented auto-\nmatic speech recognition (ASR) system. An ASR system was developed for Estonian language in radiology domain by utilizing\nopen-source software components (Kaldi toolkit, Thrax). The ASR system was trained with the real radiology text reports and\ndictations collected during development phases. The final version of the ASR system was tested by 11 radiologists who dictated\n219 reports in total, in spontaneous manner in a real clinical environment. The audio files collected in the final phase were used to\nmeasure the performance of different versions of the ASR system retrospectively. ASR system versions were evaluated by word\nerror rate (WER) for each speaker and modality and by WER difference for the first and the last version of the ASR system. Total\naverage WER for the final version throughout all material was improved from 18.4% of the first version (v1) to 5.8% of the last\n(v8) version which corresponds to relative improvement of 68.5%. WER improvement was strongly related to modality and\nradiologist. In summary, the performance of the final ASR system version was close to optimal, delivering similar results to all\nmodalities and being independent on user, the complexity of the radiology reports, user experience, and speech characteristics.\nKeywords Automatic speech recognition . Radiology . Estonian language . Spontaneous dictation . Word error rate\nIntroduction\nIn the modern healthcare system, computers and electronic\nhealthcare records are used extensively. Radiology is the most\ncomputerized specialty and a pioneer among other clinical\nfields using diagnostic workstations for image interpretation\nand radiology information systems for documenting findings.\nDemographic changes, including population aging, increase\nthe demand for healthcare services. This trend has also influ-\nenced radiology, where, for example, in Estonia, the number\nof radiology procedures, carried out between 2010 and 2015,\nhas increased by 30%. At the same time, the number of radi-\nologists has remained similar (190 in 2007, 188 in 2015) [ 1].\nThis demonstrates clearly the deficit of well-qualified radiol-\nogists in Estonian healthcare sector.\nTo be able to fulfill all patient needs with limited resources,\nthere is a necessity for radiologists to find a way for more\neffective image reporting. Currently, in Estonia, radiologists\nmanually type results of visual findings and quantitative mea-\nsurements of a study as a textual report. However, automatic\nspeech recognition (ASR) has shown to be a valid alternative,\nreplacing the traditional keyboard-based text entry in radiolo-\ngy reporting which can improve patient care and resource\nmanagement in the form of reduced report turnaround times,\nreduced staffing needs, and the efficient completion and dis-\ntribution of reports [ 2, 3].\nReporting by ASR is widely in use in countries where\nsoftware solutions for local languages are available. ASR\ntechnology has been commercially available for languages\nwith a large number of speakers (like English, French,\nGerman) already for several decades [ 3]. Estonia is a small\nmultinational country with about 1.4 million inhabitants,\namong those 70% speaking Estonian and 30% other\n* A. Paats\nandrus.paats@regionaalhaigla.ee\n1 Department of Health Technologies, Tallinn University of\nTechnology, Ehitajate tee 5, 19086 Tallinn, Estonia\n2 Medical Technology Division, North Estonia Medical Centre, J.\nSütiste tee 19, 13419 Tallinn, Estonia\n3 School of Information Technologies, Tallinn University of\nTechnology, Ehitajate tee 5, 19086 Tallinn, Estonia\nJournal of Digital Imaging (2018) 31:615–621\nhttps://doi.org/10.1007/s10278-018-0085-8\nlanguages, mainly Russian. Native language-supported ASR\nsystems for under-resourced and agglutinative languages are\noften not available [ 4, 5] which is also the case of Estonian\nlanguage. Apart from a preliminary attempt [6] and the system\npresented in this paper, no Estonian-based ASR systems exist\ncurrently in radiology.\nThe scientists from Tallinn University of Technology\n(TTÜ), in collaboration with radiologists from North\nEstonia Medical Centre (NEMC), Tallinn, Estonia, took a\nstep closer towards an ASR application in radiology for\nEstonian language by implementing an ASR prototype for\nEstonian language in radiology domain [ 7]. Due to the lack\nof resources and available commercial ASR system for\nEstonian language, open-source software components were\nutilized. Since ASR technology in its development phase has\nhigh frequency of transcription errors, necessitating careful\nproofreading and report editing, a profound understanding of\nthe errors and the frequency of errors is inevitable. Effective\nutilization of the ASR system could be hampered by high error\nrate [8, 9], low acceptance, and interest by the radiologists due\nto the issues related to the workflow or culture [ 10, 11]. In\norder to achieve similar performance to the commercial sys-\ntems in general, the first ASR system prototype developed for\nEstonian language in radiology domain was modified and\nimproved further, including the integration of domain-\nadapted deep neural network (DNN)-based acoustic models\n(AM), language model (LM) adaptation using real dictated\ntexts, smarter handling of sentence breaks, and spoken noises\nin the language model [ 12]. As a prerequisite for successful\nexploitation, modified software demand tests in clinical envi-\nronment to reveal the dictation error rates in finalized radiol-\nogy reports.\nThe aim of this study was to analyze retrospectively the\ninfluence of different acoustic and language models in order\nto determine the most important effects to the clinical perfor-\nmance of an Estonian language-based non-commercial\nradiology-oriented automatic speech recognition (ASR)\nsystem.\nMaterials and Methods\nThe ASR system was utilizing free- and open-source software\n(Kaldi toolkit,1 Thrax2)[ 7, 12] based on server-client platform\ndeveloped in TTÜ 3,4. System components in server side, re-\nsponsible for converting dictated speech into text, were avail-\nable for clients over network as reported earlier [ 13]. Client\nside system component, responsible for collecting the audio,\nconverting it into digital, and sending it to the server for pro-\ncessing, receiving and representing recognized text, was im-\nplemented as a Java application and was available for radiol-\nogists as a web-based tool.\nASR system was traditional, using an AM and a LM for\nspeech-to-text transformation [3]. Source textual information\nnecessary for preparation of the text corpus were collected\nand prepared in NEMC based on real radiology reports.\nNormalization of text corpus and training of LM was done as\ndescribed earlier [ 7, 12]. During the development, feedback\nwas collected from daily ASR system users [ 7]a n dt h eA S R\nsystem characteristics were modified in order to minimize er-\nrors through the enhancement of the AM or LM (Table 1)r e -\nported in detail earlier [ 12]. In the first version of the ASR\nsystem, the Gaussian mixture model (GMM)-based acoustic\nmodel was used as described elsewhere [ 7]. During later en-\nhancements, the AM was improved to include the integration of\nDNN technology [12].\nThe final version of the ASR system (v8) and the web-\nbased tool was used during routine reporting process in clin-\nical practice at the NEMC radiology department by 11 radiol-\nogists. This collected dataset was used to estimate retrospec-\ntively the performance of each ASR system version to evalu-\nate every model version in the similar conditions. Also, this\navoided the learning bias being built up in different develop-\nment phases of the ASR system.\nRadiologist ’s standard workplace consists of a PC\nequipped with four monitors. One monitor was used for com-\nposing a report in the Radiology Information System (RIS)\nand the others for visualization of images with PACS (Picture\nArchiving and Communication System) client (Agfa, Impax\n6.4). A web interface of the ASR prototype was implemented\ninto the same monitor as RIS in a way that the radiologist had\nvisual control of both systems at the same time. Every station\nwhere prototype was tested was equipped with a high-quality\nmicrophone headset (Logitech USB H340).\nRadiologists were supplied with written instructions of ex-\nperiments that explained how to select reports, connect and\nadjust microphone, start ASR client software, and spell punc-\ntuations and abbreviations. Dictations were marked with a\nunique code and stored by the web application, allowing to\nidentify every individual study and the modality during ana-\nlyzing process. Additional information about the radiologist\ncarrying out the dictation process was included.\nEvery radiologist reported approximately 20 radiological\nstudies in spontaneous dictation manner. For guaranteeing\nuniform distribution of report types, there was a recommen-\ndation to report eight computed tomography (CT), four mag-\nnet resonance tomography (MR), four X-ray (XR), and four\nultrasound (US) studies. The radiologists, specialized in cer-\ntain modalities (e.g., CT, MR), reported only those modalities\nand did not report other modalities. Distribution of dictated\n1 https://github.com/kaldi-asr/kaldi\n2 http://www.openfst.org/twiki/bin/view/GRM/Thrax\n3 https://github.com/alumae/kaldi-gstreamer-server\n4 https://github.com/alumae/gst-kaldi-nnet2-online\n616 J Digit Imaging (2018) 31:615 –621\nreports and modalities between radiologists is presented in\nTable 2. Average number of words per report for each modal-\nity was 126 (SD 66.6) for CT; 90.0 (SD 33.5) for MR; 37.2\n(SD 21.7) for XR; and 74.6 (SD 42.3) for US. The total num-\nber of words for each modality was CT 11083, MR 3778, XR\n1561, and US 3506.\nThe dictations recorded during the testing were analyzed.\nFor this purpose, every dictated audio file was carefully lis-\ntened and the content was transcribed into a text file as it was\nspoken. Every text file was used as a reference for comparing\noutput text produced by the ASR system from dictated audio.\nThe comparison revealed a difference between reference\nand recognition resulting in a number of incorrectly recog-\nnized words characterized with a number of substitutions\n(S), number of deletions (D), number of insertions (I), and\ncorrect words (C) in the synthesized text. Those variables\nwere used to calculate word error rate (WER) for each dictated\nreport with every ASR system model version as:\nWER ¼\nS þ D þ I\nN ¼ S þ D þ I\nS þ D þ C ð1Þ\nwhere N is a number of words in the reference texts [ 3, 14, 15].\nThe performance improvement of the last edition of the\nASR system (v8), compared to the first one (v1), was charac-\nterized by the WER difference between the systems:\nWERdifference ¼ WERv1−WERv8 ð2Þ\nwhere WERv1 and WERv8 are the WER values calculated for\nthe model versions v1 and v8, respectively.\nThe WER and the WER difference values for each dicta-\ntion were collected into a database to evaluate recognition\naccuracy of each ASR system version, modality, and radiolo-\ngist. Total WER, WER by radiologist, and WER by modality\nwere calculated and presented as mean percentage value to-\ngether with standard deviation (SD) for each ASR system\nversion. Additionally, WER difference by radiologist and mo-\ndality were determined and exhibited as median, maximum,\nminimum, first, and third quartile. The paired Student ’s t-test\nfor means was applied to compare means for WER, and the\nresulting p < 0.05 was considered significant.\nResults\nFigure 1 demonstrates changes in total WER over all dictated\nreports for all system versions. In the first version (v1) of the\nASR system with GMM acoustic model and language model\ntrained on 1-year reports, the total WER was 18.4% (SD 18.6).\nNext, version (v2) of ASR system with DNN acoustic model\nand a language model trained on 1-year reports reduced total\nWER significantly to 13.8% (SD 12.4, p <0 . 0 5 ) .I n t e r e s t i n g l y ,\nusing the language model trained on 5-year reports, the ASR\nversion v3 did not lower total WER compared to v2, but better\nTable 2 Distribution of dictated reports among radiologists as BTotal\nno. reports^ and modalities (XR X-ray, CT computed tomography, MR\nmagnetic resonance tomography, US ultrasound). The number of total\nwords per radiologist is given as BTotal no. wordsB\nRadiologist Total no. reports Modality Total no. words\nCT MR XR US\nNo. 1 19 8 3 4 4 2006\nNo. 2 19 7 4 4 4 1250\nNo. 3 22 9 13 2031\nNo. 4 22 10 4 4 4 2463\nNo. 5 19 8 9 2 1675\nNo. 6 20 8 10 2 1875\nNo. 7 20 8 8 4 2057\nNo. 8 20 8 4 8 1693\nNo. 9 19 6 13 1701\nNo. 10 19 8 7 4 1409\nNo. 11 20 8 8 4 1768\nTotal 219 88 42 42 47 19,928\nMean 19.9 8.0 7.0 5.3 5.2 1811\nSD 1.1 1.0 4.0 2.4 3.3 331\nTable 1 Different ASR system\ndevelopment versions Version number ASR system characteristics\nv1 GMM acoustic model, language model trained on 1-year reports\nv2 DNN acoustic model, language model trained on 1-year reports\nv3 + language model trained on 5-year reports\nv4 + better noise modeling in language model\nv5 + better modeling of sentence breaks\nv6 + less aggressive silence detection\nv7 + acoustic model adapted using in-domain data\nv8 + language model adapted using spoken data\nJ Digit Imaging (2018) 31:615 –621 617\nnoise modeling in the language model decreased WER from\n14.1% (SD 11.8) to 9.0% (SD 7.8, p < 0.05) in the case of v4.\nEnhanced modeling of sentence breaks in the ASR system\n(v5) and less aggressive silence detection (v6) did not generate\nlarge difference compared to v4, since the total WER de-\ncreased only to 7.9% (SD 7.4), but were still statistically sig-\nnificant (p < 0.05). The ASR system version v7 that incorpo-\nrated speaker-specific acoustic models adapted to audio files\nreduced errors even more, delivering a WER of 5.6% (SD 6.2,\np < 0.05). Adapting the language model with previously dic-\ntated texts (v8) did not have a big impact. WER stayed almost\nthe same with a small increase to 5.8% (SD 6.6, p =0 . 1 7 7 ) .\nLarge SD values are indicating heterogeneity of individual\ndictations and substantial differences in recognition accuracy.\nThe WER data by modality for each system version exhibit\ngenerally similar decreasing trend except for US, which has\nlow WER already for v1 (Fig. 2). WER was improved\nthroughout all model versions for different imaging modali-\nties. The system accuracy of the first system version was dif-\nferent for every modality, starting with mean WER 23.5% (SD\n21.9) for CT and 7.6% (SD 7.1) for US, and achieving the\nWER value of 5.3% (SD 4.8) for CT and 4.9% (SD 5.1) for\nUS in the final system version ( p <0 . 0 5 ) .\nFigure 3 shows the word error rates corresponding to indi-\nvidual radiologist for different system versions. A common\ntrend for each radiologist is similar: the last system model\nversion is giving generally better performance than the first.\nHowever, the difference in outcome of the system for an indi-\nvidual radiologist is clearly visible. Higher WER values for\nradiologists no. 1, no. 2, and no. 3 compared to others are\nseen. Moreover, the declining trend of WER is discontinued\nfor some middle system versions where the number of errors\nfor some radiologist (e.g., for radiologist no. 2, no. 4, no. 5, no.\n9, no. 11) increased.\nThe high value of standard deviation made it problematic\nto evaluate the recognition improvement between the first (v1)\nand final (v8) ASR system models by mean and SD. For this\nreason, the results in Figs. 4 and 5 are presented as median,\nquartiles, and minimum and maximum values.\nImprovement of the system as a difference of WER be-\ntween the first (v1) and last (v8) system versions for each\nradiologist is presented in Fig. 4. For most of the radiologists,\nthe ASR system’s performance improved with the system ver-\nsion v8, compared to that of the system version v1. However,\nthe improvement rate for individual radiologists was different.\nFig. 3 Word error rates corresponding t o individual radiologist for\ndifferent model versions\nFig. 2 Word error rates by modality for different model versions\nFig. 1 The total WER (mean, SD) for model versions 1 to 8\n618 J Digit Imaging (2018) 31:615 –621\nIn the best case (radiologist no. 3), the max WER improved by\n91%. The median WER difference ranged from 25.6% for\nradiologist no. 1 to 1.6% for radiologist no. 11.\nFigure 5 studies the model improvement for each modality,\npresenting the highest score for CT (median WER difference\n11.8%) and lowest for US (median WER difference 2.6%)\nbetween the first and the last versions. The ASR system ver-\nsion v8 gave higher recognition accuracy (WER difference >\n0) compared to the version v1 in 179 dictations from the total\namount of 219. Only 16 dictations (CT 2/88; XR 4/42, US 10/\n47) had better result with ASR system version v1 than with\nversion v8 (WER difference < 0). On 24 cases (CT 4/88; MR\n2/42; XR 8/42; US 10/47), the recognition accuracy was the\nsame for versions v1 and v8. Almost all dictated reports\nbenefited from the final system version for CT and MR.\nDespite the system improvement being the smallest for US,\nthe 82% of the dictated reports achieved better outcome in the\nfinal system version, compared to the first.\nDiscussion\nThe ASR system architecture utilized in our approach is sim-\nilar to the other ASR applications for radiology [ 13, 16].\nThe data on Fig. 1 shows that the system version v1 had\nsimilar WER as other GMM-based models (18.41%) used for\nradiology ASR systems reported by Miranda et al. 2008 [ 13].\nWhen increasing the number of training reports in the ASR\nsystem’s language model from 1-year data (v2) to 5-year data\n(v3), the changes were negligible ( p = 0.239). Even a small\ngain of error was detected as WER increased from 13.8% (SD\n12.4) to 14.1% (SD 11.8) indicating that system version v1\nhad sufficiently diverse dataset based on 1-year radiology re-\nports for language model training. Adding the dataset based\non 5-year reports did not improve the performance.\nComparing our open-source software-based ASR system\nperformance to commercially available products, the overall\nerror rate is in the same order or even lower. For example, the\noverall error rate in the study by the IBM MedSpeak ASR\nsystem was found to be 10.3% (SD 3.3) [ 17]. Using the\nASR system Nuance Gen, Nuance Med, and SRI Decipher\nfor interpreting spoken clinical questions resulted in a WER of\n68.1, 67.4, and 26.7%, respectively [18]. After all model mod-\nifications, our radiology domain-specific system performance\nimproved from 18.4% to a final WER of 5.8%, which yields\nrelative WER improvement of 68.5%. This behavior is similar\nto that of the SRI system improvement of 36% applied to\ngeneral clinical text [ 18]. The SD value of WER (18.7 and\n6.6% for system version v1 and v8, respectively) for our ASR\nsystem was somewhat higher than that in earlier studies [ 17],\nprobably explained by more heterogeneous report set in our\nstudy.\nFigures 2 and 3 reveal the impact of different system ver-\nsions to the imaging modalities and individual radiologists.\nThe implementation of the ASR system version v2 reduced\nWER for all modalities and all radiologists except for radiol-\nogist no. 11 (13 reports from 20 had higher WER in v2 than in\nv1). The possible cause was that GMM acoustic model in v1\nfitted well with the voice characteristics of the radiologist no.\n11, which was lost in the model v2 with a more general DNN\nacoustic model.\nThe ASR system version v3 had a small effect on recogni-\ntion errors being not statistically relevant for any of modalities\n(p > 0.05). WER drifted bi-directionally up and down (Figs. 2\nand 3)a n df o rs o m er a d i o l o g i s t s( r a d i o l o g i s tn o .2a n dn o .5 ) ,\nperformance worsened (p < 0.05). This could be explained by\na large number of rarely occurring words in language model\ndictionary based on the dataset of 5-year reports. In the reports\nfor modalities like US and XR with relatively simple\nFig. 4 Median word error rate improvement with maximum, minimum,\nfirst, and third quartile between the first (v1) and the last (v8) model\nversions corresponding to individual radiologist\nFig. 5 Median of word error rate improvement with maximum,\nminimum, first and third quartile between first (v1) and last (v8) model\nversions by modality\nJ Digit Imaging (2018) 31:615 –621 619\nvocabulary, many additional alternatives created a situation\nwhere probability to find correct word was more complex.\nImplementation of the ASR system version v4 affected all\nradiologists, except no. 4, and modalities in a similar way, the\nimprovement of WER was significant ( p <0 . 0 5 ) .T h i sc a nb e\nexplained by the reduction of filler non-stationary noises and\nenhanced background noise processing (e.g., elimination of\nsounds coming from keyboard, mouse, etc).\nThe ASR system versions v5 and v6 reduced WER in a\nsmall scale for all modalities, but influence to the radiologist\ndictations was different. For some radiologists, WER de-\ncreased and for others, it increased, because those models\nattempted to simulate specific work situations (e.g., long\npauses caused by performing measurements on image during\ndictation, a pause of thought) where users had different\nbehaviors.\nAdditional progress was detected after implementation\nof the ASR system version v7 as WER decreased for all\nradiologists and modalities, except for radiologists no. 1,\nno. 8, and no. 11 and for US modality ( p > 0.05). This can\nbe explained with better performance of the acoustic model\ntuned with audio files dictated by radiologists from previ-\nous tests.\nThe ASR system v8 had a relatively small, non-significant\n(p > 0.05) impact: WER decreased for CT and MR but in-\ncreased slightly for XR, for US, and for some radiologists\n(no. 2, no. 5, no. 8, no. 10). By adapting the language model,\nincluding content from earlier dictated reports, WER im-\nproved for more complicated CT and MR reports, but not\nfor more standardized US and XR reports.\nThe results in Figs. 4 and 5 characterize relative im-\nprovement in recognition accuracy for individual radiolo-\ngists and modalities between the first and last versions of\nthe ASR system as WER difference. The ASR system v8\ndisplayed significant improve ment in recognition accuracy\nto the most of the CT and MR report dictations, compared\nto the system version v1. For the XR and US reports, the\nimprovement was smaller, but enough to guarantee as high\nor even better detection accuracy than for CT and MR,\nr e f e r r i n gt or e l a t i v e l yg o o dr e c o g n i t i o na l g o r i t h m sa l r e a d y\nused in the ASR system version v1 for XR and US.\nGenerally, the ASR system provided the lowest WER for\nUS, in comparison to other moda lities. It can be explained\nby US reports having a more standardized structure than\nothers. For MR, the ASR did not reach the same perfor-\nmance level as for XR and CT (Fig. 2). Similar to our\nresults, Ramaswamy et al. [ 19] used the ASR system for\ndictation of MR reports and achieved an average WER of\n7.3%. Another study investigated CT and MR reports and\nindicated an average WER of 2.81% [ 11] and between 7.8 –\n11.5% and 9.3 –10.6% for CT and MR, respectively [ 17].\nChanges in different ASR system versions were made to\nincrease low recognition accuracy, mostly induced by the\nreports of complicated 3D modalities (CT and MR). This\ntask is fulfilled for all ASR system versions. At the same\ntime, modalities already having a good recognition accura-\ncy (XR, US) tend to suffer from this and WER increased\nslightly in some system versions compared to the previous\nones. The reason lies probably in a large number of rarely\noccurring words in language model dictionary based on the\ndataset of 5-year reports as explained above.\nIt is important that for the final version of the ASR\nsystem, the accuracy is similar for all modalities and\nWER is in the range of 4.2– 8.0%. However, there is no\npractical need to implement technology used in the ASR\nsystem version v8 for US and XR modalities since the\nmodel v7 assures the same result. According to the IBM\nWatson team [ 20], human accuracy as WER in English\nconversational speech recog nition was reported around\n5.1% and has been estimated to be even as low as 4%\n[21]. Our free- and open-source software-based ASR sys-\ntem approaches this number although the human transcrip-\ntion accuracy for dictated radiology reports is probably\nmuch lower than for conversational telephone calls.\nMoreover, an exact comparison is difficult, because WER\nvalues reported by different authors vary, probably caused\nby differences in methodology, study group, the complex-\nity of reported studies, etc.\nIn summary, the performance of the final ASR system\nversion was close to optimal, delivering similar results to\nall modalities and being independent on the user, the com-\nplexity of the radiology reports, user experience, and\nspeech characteristics. Even if some ASR system model\nversions did not give statistic ally significant improve-\nments, they cannot be ignored and should be considered\nfor implementation due to the fact that the effect was pres-\nent\nfor some radiologists.\nConclusions\nThis study contributes to the knowledge how different\ncharacteristics of the acoustic and language models of the\nASR system based on open-source software can improve\nASR system performance in radiology domain for a small\nlanguage as Estonian. Hopefully, this preserves native\nlanguage-based working environment in clinics under the\npressure of fast-developing t echnology and globalization.\nAcknowledgments The authors wish to thank all radiologists who par-\nticipated in the live experiments, especially Dr. Ä. Roose and Dr. P.\nRaudvere for normalization of the report corpus.\nFunding Information The work was supported by the European Union\nthrough the European Regional Development Fund, project\n3.2.1201.13-0010.\n620 J Digit Imaging (2018) 31:615 –621\nOpen Access This article is distributed under the terms of the Creative\nCommons Attribution 4.0 International License (http://\ncreativecommons.org/licenses/by/4.0/), which permits unrestricted use,\ndistribution, and reproduction in any medium, provided you give appro-\npriate credit to the original author(s) and the source, provide a link to the\nCreative Commons license, and indicate if changes were made.\nReferences\n1. National Institute for Health Development, Health Statistics and\nHealth Research Database, http://pxweb.tai.ee/PXWeb2015/\npxweb/en/04THressursid. Accessed 25.05.2017\n2. V oll K, Atkins S, Forster B: Improving the utility of speech recog-\nnition through error detection. J Digit Imaging 21:371 –377, 2008\n3. Ghai W, Singh N: Literature review on automatic speech recogni-\ntion. Int J Comput Appl 41:42 –50, 2012\n4. Arisoy E, Arslan LM: Turkish radiology dictation system.\nProceedings of SPECOM ’2004: 9th Conference Speech and\nComputer, St. Petersburg, Russia, September 20 –22, 2004\n5. Karpov A, Kipyatkova I, Ronzhin A: Speech recognition for East\nSlavic languages: the case of Russian. Proceedings 3rd\nInternational Workshop on Spoken Language Technologies for\nUnder-resourced Languages, SLTU’ 2012, Cape Town, South\nAfrica, pp 84-89, May7 –9, 2012\n6. Alumäe T, Meister E: Estonian large vocabulary speech recognition\nsystem for radiology. Proceedings Fourth International Conference\nHuman Language Technologies. The Baltic perspective, Baltic\nHLT 2010, Riga, Latvia, 219: 33 –38, 2010\n7. Paats A, Alumäe T, Meister E, Fridolin I: Evaluation of automatic\nspeech recognition prototype for Estonian language in radiology\ndomain: A pilot study. IFMBE Proceedings 16th Nordic-Baltic\nconference on biomedical engineering, Göteborg, Sweden, 48:96\n−99, 2015\n8. Basma S, Lord B, Jacks LM, Rizk M, Scaranelo AM: Error rates in\nbreast imaging reports: Comparison of automatic speech recogni-\ntion and dictation transcription. Am J Roentgenol 197:923 –927,\n2011\n9. Chang CA, Strahan R, Jolley D: Non-clinical errors using voice\nrecognition dictation software for radiology reports: A retrospective\naudit. J Digit Imaging 24:724 –728, 2011\n10. Talton D: Perspectives on speech recognition technology. Radiol\nManage 27:38–40 42–43, 2005\n11. Pezzullo JA, Tung GA, Rogg JM, Davis LM, Brody JM, Mayo-\nSmith WW: V oice recognition dictation: Radiologist as transcrip-\ntionist. J Digit Imaging 21:384 –389, 2008\n12. Alumäe T, Paats A, Meister E, Fridolin I: Implementation of a\nradiology speech recognition system for Estonian using Open\nSource Software. Proc. Interspeech 2017, Stockholm, Sweden, pp\n2168-2172, 2017\n13. Miranda J, Neto JP: A platform of distributed speech recognition\nfor the European Portuguese Language. Proceedings 8th\nInternational Conference, P ROPOR 2008, Computational\nProcessing of the Portuguese Language, Aveiro, Portugal, 5190:\n182-191, September 8-10, 2008\n14. Makhoul J, Schwartz R: State of the art in continuous speech rec-\nognition. Proc Natl Acad Sci USA 92:9956 –9963, 1995\n15. Morris AC, Maier V , Green P: From WER and RIL to MER and\nWIL: Improved evaluation measures for connected speech recogni-\ntion. Proceedings INTERSPEECH 2004 - ICSLP , 8th International\nConference on Spoken Language Processing, Jeju Island, Korea, pp\n2765-2768, October 4–8, 2004\n16. Angelini B, Antoniol G, Brugnara F, Cettolo M, Federico M,\nFiutem R, Lazzari G: Radiological reporting by speech recognition:\nThe A.Re.S. system. Proceedings ICSLP 94, International\nConference on Spoken Language Processing, Yokohama, Japan,\nSeptember 18–22, 1994\n17. Kanal KM, Hangiandreou NJ, Sykes AM, Eklund HE, Araoz PA,\nLeon JA, Erickson BJ: Initial evaluation of a continuous speech\nrecognition program for radiology. J Digit Imaging 14:30–37, 2001\n18. Liu F, Tur G, Hakkani-Tür D, Yu H: Towards spoken clinical-\nquestion answering: Evaluating and adapting automatic speech-\nrecognition systems for spoken clinical questions. J Am Med\nInform Assoc 18:625–630, 2011\n19. Ramaswamy MR, Chaljub G, Esch O, Fanning DD, vanSonnenberg\nE: Continuous speech recognition in MR imaging reporting:\nAdvantages, disadvantages, and impact. Am J Roentgenol 174:\n617–622, 2000\n20. Saon G, Kurata G, Sercu T, Audhkhasi K, Thomas S, Dimitriadis\nD, Cui X, Ramabhadran B, Picheny M, Lim LL, Roomi B, Hall P:\nEnglish conversational telephone speech recognition by humans\nand machines. Proceedings INTERSPEECH 2017, Stockholm,\nSweden, pp 132-136, August 20 –24, 2017\n21. Lippmann RP: Speech recognition by machines and humans.\nSpeech Commun 22:1–15, 1997\nJ Digit Imaging (2018) 31:615 –621 621",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7419381141662598
    },
    {
      "name": "Estonian",
      "score": 0.6871852874755859
    },
    {
      "name": "Modality (human–computer interaction)",
      "score": 0.5985445976257324
    },
    {
      "name": "Radiology information systems",
      "score": 0.49695470929145813
    },
    {
      "name": "Word error rate",
      "score": 0.48818522691726685
    },
    {
      "name": "Software",
      "score": 0.4795684218406677
    },
    {
      "name": "Speech recognition",
      "score": 0.467072069644928
    },
    {
      "name": "Natural language processing",
      "score": 0.41200700402259827
    },
    {
      "name": "Radiology",
      "score": 0.36572831869125366
    },
    {
      "name": "Medical physics",
      "score": 0.360983669757843
    },
    {
      "name": "Artificial intelligence",
      "score": 0.31191933155059814
    },
    {
      "name": "Medicine",
      "score": 0.22083353996276855
    },
    {
      "name": "Linguistics",
      "score": 0.15060630440711975
    },
    {
      "name": "Programming language",
      "score": 0.1217077374458313
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I111112146",
      "name": "Tallinn University of Technology",
      "country": "EE"
    }
  ],
  "cited_by": 9
}