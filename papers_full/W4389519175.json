{
  "title": "Self-prompted Chain-of-Thought on Large Language Models for Open-domain Multi-hop Reasoning",
  "url": "https://openalex.org/W4389519175",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2121028757",
      "name": "Jinyuan Wang",
      "affiliations": [
        "Shanghai Municipal Education Commission",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2117167659",
      "name": "Junlong Li",
      "affiliations": [
        "Shanghai Municipal Education Commission",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2112311038",
      "name": "Hai Zhao",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Shanghai Municipal Education Commission"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3101682885",
    "https://openalex.org/W4297899309",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W3027879771",
    "https://openalex.org/W2963339397",
    "https://openalex.org/W3007672467",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4226369848",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3190126809",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2953320089",
    "https://openalex.org/W4224247158",
    "https://openalex.org/W2964120615",
    "https://openalex.org/W4385572867",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W3115947671",
    "https://openalex.org/W4367000491",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W4389520103",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4311997167",
    "https://openalex.org/W4304194220",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W3156366114",
    "https://openalex.org/W4221161695"
  ],
  "abstract": "In open-domain question-answering (ODQA), most existing questions require single-hop reasoning on commonsense. To further extend this task, we officially introduce open-domain multi-hop reasoning (ODMR) by answering multi-hop questions with explicit reasoning steps in open-domain setting. Recently, large language models (LLMs) have found significant utility in facilitating ODQA without external corpus. Furthermore, chain-of-thought (CoT) prompting boosts the reasoning capability of LLMs to a greater extent with manual or automated paradigms. However, existing automated methods lack of quality assurance, while manual approaches suffer from limited scalability and poor diversity, hindering the capabilities of LLMs. In this paper, we propose Self-prompted Chain-of-Thought (SP-CoT), an automated framework to mass-produce high quality CoTs of LLMs, by LLMs and for LLMs. SP-CoT introduces an automated generation pipeline of high quality ODMR datasets, an adaptive sampler for in-context CoT selection and self-prompted inference via in-context learning. Extensive experiments on four multi-hop question-answering benchmarks show that our proposed SP-CoT not only significantly surpasses the previous SOTA methods on large-scale (175B) LLMs, but also nearly doubles the zero-shot performance of small-scale (13B) LLMs. Further analysis reveals the remarkable capability of SP-CoT to elicit direct and concise intermediate reasoning steps by recalling ~50% of intermediate answers on MuSiQue-Ans dataset.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 2717–2731\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nSelf-prompted Chain-of-Thought on Large Language Models for\nOpen-domain Multi-hop Reasoning\nJinyuan Wang1, 3 and Junlong Li2, 3 and Hai Zhao2, 3∗\n1SJTU-Paris Elite Institute of Technology, Shanghai Jiao Tong University\n2Department of Computer Science and Engineering, Shanghai Jiao Tong University\n3Key Laboratory of Shanghai Education Commission for Intelligent Interaction\nand Cognitive Engineering, Shanghai Jiao Tong University\n{steve_wang,lockonn}@sjtu.edu.cn, zhaohai@cs.sjtu.edu.cn\nAbstract\nIn open-domain question-answering (ODQA),\nmost existing questions require single-hop rea-\nsoning on commonsense. To further extend\nthis task, we officially introduce open-domain\nmulti-hop reasoning (ODMR) by answering\nmulti-hop questions with explicit reasoning\nsteps in open-domain setting. Recently, large\nlanguage models (LLMs) have found signifi-\ncant utility in facilitating ODQA without ex-\nternal corpus. Furthermore, chain-of-thought\n(CoT) prompting boosts the reasoning capabil-\nity of LLMs to a greater extent with manual\nor automated paradigms. However, existing\nautomated methods lack of quality assurance,\nwhile manual approaches suffer from limited\nscalability and poor diversity, hindering the ca-\npabilities of LLMs. In this paper, we propose\nSelf-prompted Chain-of-Thought (SP-CoT), an\nautomated framework to mass-produce high\nquality CoTs of LLMs, by LLMs and for\nLLMs. SP-CoT introduces an automated gener-\nation pipeline of high quality ODMR datasets,\nan adaptive sampler for in-context CoT selec-\ntion and self-prompted inference via in-context\nlearning. Extensive experiments on four multi-\nhop question-answering benchmarks show that\nour proposed SP-CoT not only significantly\nsurpasses the previous SOTA methods on large-\nscale (175B) LLMs, but also nearly doubles\nthe zero-shot performance of small-scale (13B)\nLLMs. Further analysis reveals the remarkable\ncapability of SP-CoT to elicit direct and con-\ncise intermediate reasoning steps by recalling\n∼50% of intermediate answers on MuSiQue-\nAns dataset.\n1 Introduction\nOpen-domain question-answering (ODQA) is a\nlongstanding and challenging task which addresses\nfactoid commonsense questions without specific\n∗Corresponding author. This paper was partially sup-\nported by the Joint Research Project of Yangtze River\nDelta Science and Technology Innovation Community (No.\n2022CSJGG1400).\ncontexts provided. While existing works in ODQA\nprimarily focus on resolving questions that mostly\nrequire single-hop reasoning, there is a burgeoning\ninterest in multi-hop question-answering (MHQA),\nwhich aims to derive the correct answer through\nmulti-step reasoning over a collection of candidate\narticles (Mavi et al., 2022). Yet, a significant dis-\nparity exists between such scenarios and real-world\napplications, since the latter often lacks an explicit\nset of candidate articles provided by users. In light\nof this, we officially introduce open-domain multi-\nhop reasoning (ODMR) as a progression task of\nODQA, which requires MHQA with explicit ratio-\nnales in open-domain setting.\nFor ODMR, an emerging approach is to lever-\nage large language models (LLMs) due to the vast\nknowledge stored within their numerous parame-\nters. In recent years, LLMs have shown powerful\nreasoning and instruction-following capabilities,\nsuch as GPT-3 (Brown et al., 2020), PaLM (Chowd-\nhery et al., 2022) and InstructGPT (Ouyang et al.,\n2022). After extensive training on vast corpora\nof textual data, LLMs prove to be zero-shot rea-\nsoners on complex reasoning tasks by breaking\ndown multi-step questions into intermediate ones\nfor step-by-step reasoning before producing the fi-\nnal answer (Kojima et al., 2023). Such series of\nintermediate reasoning steps is known as chain-of-\nthoughts (CoTs) (Wei et al., 2023). CoTs often\nserve as in-context demonstrations for in-context\nlearning (ICL) (Brown et al., 2020), which enables\nLLMs to generate outputs that are formally consis-\ntent with a target task via a few reference exam-\nples provided as prompt. Manual-CoT (Wei et al.,\n2023) adopt manually designed CoTs as in-context\ndemonstrations to improve the reasoning perfor-\nmance of LLMs. However, it demands delicate\nand meticulous design by humans, and the demon-\nstrations are the same for each question, which\nmay be sub-optimal. Zero-shot-CoT (Kojima et al.,\n2023) was proposed to trigger automated CoTs\n2717\nby certain specific prompting techniques, such as\n\"Let’s think step by step: \". Zhang et al.\n(2022) proposed Auto-CoT, an automated frame-\nwork to mass-produce CoTs and build in-context\ndemonstrations. However, previous works have\nnot fully leveraged the strong instruction-following\nand zero-shot reasoning capabilities of LLMs.\nIn this paper, we propose Self-prompted Chain-\nof-Thought (SP-CoT), an LLM-only framework\nto mass-produce high-quality CoTs for ODMR.\nIn general, SP-CoT introduces an automated gen-\neration pipeline of ODMR datasets, an adaptive\nsampler for CoT selection and self-prompted in-\nference via ICL. The automated ODMR datasets\nare MHQA datasets without candidate contexts,\nyet including multi-hop questions with six types of\ncomplex reasoning chains and step-by-step decom-\nposition. Each intermediate QA step is equipped\nwith a short explanation to justify the answer. By\nleveraging the ICL ability of LLMs, our method is\ngenerally effective on LLMs of different scales.\nWe evaluate our method on four MHQA datasets\nin an open-domain setting: ComplexWebQues-\ntions (CWebQ) (Talmor and Berant, 2018), Hot-\npotQA (Yang et al., 2018), 2WikiMultiHopQA\n(2Wiki) (Ho et al., 2020) and MuSiQue-Ans (MSQ)\n(Trivedi et al., 2022). Extensive experiments show\nthat our proposed SP-CoT not only significantly\nsurpasses the previous SOTA methods on large-\nscale (175B) LLMs, but also nearly doubles the\nzero-shot performance on small-scale (13B) LLMs\nin ODMR. Further analysis reveals the outstanding\ncapability of SP-CoT to elicit direct and concise\nintermediate reasoning steps by recalling ∼50% of\nintermediate answers on MSQ dataset.\nOur contributions can be summarized as follows:\n1. We introduce an automated pipeline to gen-\nerate high-quality ODMR datasets by LLMs,\nwhich include 2-4 hop questions with six types\nof complex reasoning chains.\n2. We propose SP-CoT, an automated framework\nto mass-produce CoTs while ensuring quality\nand diversity.\n3. We conduct extensive experiments to confirm\nthe effectiveness of SP-CoT on four ODMR\nbenchmarks. In ODMR setting, our approach\nsignificantly boosts the performance by elicit-\ning high-quality intermediate reasoning steps.\nOur code and datasets are publicly available at\nhttps://github.com/noewangjy/SP-CoT.\n2 Related Works\n2.1 Multi-Hop Dataset Creation\nCreating an annotated MHQA dataset manually\nrequires significant human resources. Therefore,\nsome researchers are dedicated to automating the\ngeneration of MHQA datasets. Jiang et al. (2020)\nelaborated the creation of a multi-hop fact veri-\nfication dataset from existing HotpotQA dataset.\nTrivedi et al. (2022) introduced a bottom-up pro-\ncess to build challenging multi-hop reading compre-\nhension QA dataset through meticulous selection\nand composition of single-hop questions derived\nfrom existing datasets. Press et al. (2023) proposed\nan automatically generated dataset with composi-\ntional 2-hop questions about celebrities. Neverthe-\nless, existing approaches are either only partially\nautomated, still requiring crowdsourcing, or they\nare limited to less complex 1-2 hop questions. In\nthis work, our proposed SP-CoT is capable of au-\ntomatically generating 2-4 hop questions with six\ndifferent types of reasoning chains (Figure 6 in\nAppendix).\n2.2 Chain-of-Thought Prompting\nRecent works on CoT prompting can be divided\ninto two research lines. The first is prompting\nLLMs step by step to leverage their comprehen-\nsion and reasoning abilities to answer questions.\nZero-shot-CoT (Kojima et al., 2023) adopts a two-\nstage design, which requires LLMs to first generate\nintermediate rationale and then produce an answer.\nWang et al. (2022) introduced iCAP, which iter-\natively prompts a fine-tuned small-scale LLM to\ngenerate CoTs and then combines the generated\nrationales to formulate answers. Least-to-Most\n(Zhou et al., 2023) requires LLMs to first decom-\npose a complex question into sub-questions and\nthen sequentially solve them to arrive at the final\nanswer.\nThe second research line focuses on designing\neffective CoT as demonstrations for ICL to re-\nlease more powerful reasoning abilities of LLMs.\nManual-CoT (Wei et al., 2023) was introduced\nto leverage manually designed CoT as in-context\ndemonstrations to solve arithmetic, commonsense,\nand symbolic problems. A recent work (Zelikman\net al., 2022) shed light on the practicality to auto-\nmate the generation of rationales by LLMs. Subse-\nquently, Self-Ask (Press et al., 2023) was proposed\nto construct step-by-step demonstrations with ex-\nplicit decision process and intermediate answers\n2718\n……\nSelf-prompted Inference via In-context LearningAdaptive CoT ConstructionGeneration Question:Whatisthebirthyearofthepersonwhoremixedthesong\"TakeonMe\"?Answer:Let'sthinkstepbystep:Step1:Thesong\"TakeonMe\"wasremixedbyJohnRatcliff.Step2:JohnRatcliff'sbirthdateisOctober7,1943.Therefore,thefinalanswerinjustoneentityis:October71943Question:WhatyeardidthepersonwhooriginallygroupedBrahmswithBachandBeethovenasthe\"ThreeBs\"ofmusicmakehisdebutasapianistafterstudyingwithFranzLiszt?Answer:Let'sthinkstepbystep:Step1:ThecommentgroupingBrahamswithBachandBeethovenasthe\"ThreeBs\"ofmusicwasoriginallymadebyHansvonBülow.Step2:HansvonBülowmadehisdebutasapianistin1848afterstudyingwithFranzLiszt.Therefore,thefinalanswerinjustoneentityis:1848\nDemo 1Demo x \nQuestion:Whosing\"HomeAloneTonight\"withtheperformerof\"countryMan\"?Answer:Let'sthinkstepbystep:\nTest SampleStep 1: The performer of \"Country Man\" is Luke Bryan.Step 2: Luke Bryan sings \"Home Alone Tonight\" with Karen Fairchild of Little Big Town.Therefore, the final answer in just one entity is: Karen FairchildOutput\nSelf-GeneratedODMR DatasetEncoding\n…\n…\n…\n… …\nClustering\nFigure 1: The overall framework of our proposed SP-CoT, including an automated generation of ODMR datasets,\nan adaptive sampler for CoT selection and self-prompted inference via ICL. Texts highlighted in purple refer to\nquestions, in red to previously generated CoTs, in orange to answers, and in green to newly generated contents.\nas CoT. Zhang et al. (2022) proposed Auto-CoT,\nwhich automatically constructs CoTs via LLMs\nand adopts clustering methods to dynamically build\ndemonstrations for each question.\nHowever, existing methods have two significant\nlimitations: 1) Over-reliance on the reasoning abil-\nities of LLMs. Most methods are reported effec-\ntive on large-scale LLMs like InstructGPT, while\nreproducing these methods on small-scale LLMs\nis quite challenging. 2) Over-confidence on the\nquality of intermediate results. When prompting\nLLMs step by step, defects in previous steps may\nlimit the performance of subsequent steps. Simi-\nlarly, while automatically constructing in-context\ndemonstrations, the effectiveness of ICL might be\ncompromised by the unstable quality of CoTs. Ad-\nmittedly, manually constructed CoTs can ensure\nquality, yet they face a trade-off between content\ndiversity and costs. To overcome the above draw-\nbacks, our proposed SP-CoT automates CoT gener-\nation with quality ensured by leveraging the strong\ninstruction-following capability of LLMs.\n2.3 Model Enhancement via LLM Generation\nWith the powerful capabilities of LLMs on con-\ntent generation and instruction-following, one re-\ncent research direction extensively leverages the\ncontent generated by LLMs to enhance smaller\nLLMs. Recent works such as GPTeacher, 1 Al-\n1https://github.com/teknium1/GPTeacher\npaca (Taori et al., 2023) and Vicuna (Chiang et al.,\n2023) collect the content generated by GPT-4 (Ope-\nnAI, 2023) and the corresponding prompts to train\nsmaller-scale LLMs to achieve comparable perfor-\nmance. Another research line aims to boost the\nperformance of large-scale LLMs to higher levels\nby leveraging the self-generated content. Some\nworks use the self-generation as contexts to assist\nthemselves in answering questions, such as elicit-\ning intermediate rationales as CoT (Kojima et al.,\n2023) or generating background articles for read-\ning comprehension (Yu et al., 2023). While oth-\ners instruct LLMs to generate demonstrations for\nICL during inference (Zhang et al., 2022), such as\nprompting LLMs to generate reliable QA pairs as\nself-prompted in-context demonstrations (Li et al.,\n2022). Our work is dedicated to extending the\nsecond research line to ODMR by leveraging the\nautomated self-generated CoT as in-context demon-\nstrations. Compared to previous works (Kojima\net al., 2023; Zhang et al., 2022; Li et al., 2022),\nour work taps into the potential of self-prompting\nLLMs with more complicated framework design to\nsolve a most challenging task.\n3 Methods\nIn this section, we elaborate our proposed SP-CoT\nin three stages (Figure 1):\nIn the first stage, we prompt LLMs to iteratively\ngenerate 2-hop commonsense QA quadruplets with\n2719\nIn: Name 10 scientists:Out:AlbertEinstein,MarieCurie,IsaacNewton,...In:GenerateadetailedpassagefromWikipediaaboutAlbertEinstein.Out:AlbertEinsteinwasaGerman-borntheoreticalphysicistwho...,anddiedonApril18,1955,inPrinceton,NewJersey.\nIn:PassageaboutAlbertEinstein:AlbertEinsteinwasaGerman-born…,NewJersey.Question:WhatisthecitywhereAlbertEinsteindied?Answer:PrincetonAccordingtoanevidencefromthepassagetosupporttheanswer,rewriteittomakeitsmeaningclearwithoutpassage.Out:AlbertEinsteinwasawardedtheNobelPrize…andhisservicestoTheoreticalPhysics.\nExamplePassageQuestionExplanation\nIn:GenerateadetailedpassagefromWikipediaaboutPrinceton.Out:PrincetonisauniversitytowninMercerCounty,…ThemuseumislocatedonthecampusofPrincetonUniversityandisfreetothepublic.\nIn:PassageaboutPrinceton:Princetonisauniversitytown…tothepublic.Question:WhatisthecountythatishometoPrincetonUniversity?Answer:MercerCountyAccordingtoanevidencefromthepassagetosupporttheanswer,rewriteittomakeitsmeaningclearwithoutpassage.Out:PrincetonisauniversitytowninMercerCounty,NewJersey,UnitedStates.\nStage 1: 2-Hop QAs via Self-GenerationStep 1: First-HopPassageGeneration Step 3: Second-HopPassageGeneration\nStep 2: First-Hop QA GenerationStep 4: Second-Hop QA GenerationIn:PassageaboutAlbertEinstein:AlbertEinsteinwasaGerman-born…,NewJersey.Generateaquestiontowhichtheansweristheentity'Princeton'.Out:WhatisthecitywhereAlbertEinsteindied?VerificationIn:PassageaboutPrinceton:Princetonisauniversitytown…tothepublic.Generateaquestiontowhichtheansweristheentity'MercerCounty'.Out:Whatisthecountythat…University?\nExtract named entities from the passage... Extract named entities from the passage...\nVerification\nFigure 2: Generation steps for 2-hop QA quadruplets. Each QA quadruplet comprises a questionq, its corresponding\nanswer a, and a context passage p. The explanation e includes the answer a from the passage p to address the\nquestion q. Text highlighted in orange refers to previously generated content, while the response of the LLM is\nhighlighted in green.\ncontext, question, answer and explanation.\nIn stage 2, we construct multi-hop reasoning\nchains by connecting 2-hop QA quadruplets and\nbuild an ODMR dataset via composition.\nIn the last stage, we adopt clustering-based sam-\npling approach to dynamically select and construct\nin-context demonstrations for inference.\n3.1 2-Hop QAs via Self-Generation\nIn the first stage, we prompt LLMs to iteratively\ngenerate 2-hop QA quadruplets with context, ques-\ntion, answer and explanation, which is illustrated\nin Figure 2. Inspired by Li et al. (2022), we design\na 2-hop commonsense QA generation pipeline, in-\ncluding the following 4 steps:\nStep 1: First-Hop Passage GenerationTo guar-\nantee the comprehensive coverage of commonsense\nknowledge, we manually design 29 diverse topics\nbased on the statistics of TriviaQA (Joshi et al.,\n2017). For each topic, we require the LLM to\nname a certain number of keywords. For each col-\nlected keyword k1, we ask the LLM to generate a\nWiki-style passage p1. Despite some factoid errors\n(Li et al., 2022), such generated passages contain\nsufficient factual information to serve as context\nfor QA generation.\nStep 2: First-Hop QA GenerationGiven that\nthe answers for commonsense questions are likely\nto be named entities, we use Spacy 2 and NLTK\n(Bird and Loper, 2004) libraries to extract the\nnamed entities in the passage p1 as candidate an-\nswers. For each candidate answer a1, we require\nthe LLM to raise a question q1 to which the an-\nswer is a1 based on the passage p1. To ensure the\nquality of q1, we employ a double-check process,\nwhere we demand the LLM to answer the gener-\nated question q1 given the contextp1 to check if the\ngenerated answer a1\n′\nis accordant with a1. Once\nthe generated QA pair passes the double-check, we\nprompt the LLM to write a short explanation e1 for\nit. Note that the candidate answers must exclude\nthe keyword (a1 ̸= k1) because the answer in the\nfirst hop will become the keyword for the second\nhop (k2 = a1, k2 ̸= k1). In addition to that, a valid\nexplanation must contain the answer (a1 ∈e1).\nStep 3: Second-Hop Passage Generation\nBefore the first-hop answers are used as key-\nwords for second-hop passage generation, we use\nSpacy to filter out the answers with certain labels\n(QUANTITY, ORDINAL, CARDINAL, PERCENT, MONEY,\nDATE, TIME), which are infeasible for Wiki-style\npassage generation. Given a keyword k2, we repeat\nthe same prompts as described in Step 1 to generate\nthe passage p2.\nStep 4: Second-Hop QA GenerationWe be-\n2https://spacy.io/\n2720\nXN12345\n6\nQuestion:WhendidtheInternationalCourtofJusticebeginitswork?Answer:April1946ReformthequestiontoageneralinterrogativesentencethatcanbeansweredwithYes:DidtheInternationalCourtofJusticebeginitsworkinApril1946?…Factoid Questions\n…Binary Questions\nDemos (Yes) x4Question:WhatbodyofwaterisVenicelocatedon?Answer:theAdriaticSeaReformthequestiontoa…thatcanbeansweredwithYes: Test Sample (Yes)\nXN\n10%\nStep 3: Binary Question Generation\nRawquestion:Howmany[Whatwerenewstudentsoncecalledbyothers?]livein[Wherewasthefootballtournamentheld?]?Replacethesentencewithin[]witharelativeclauseandmaketherawquestionintoanaturalquestion:Howmanypeoplewhosename…liveinthecountry…tournament?Demosx4Rawquestion:IsittheMississippiRiver…[WhatistheAmericanGothic…about?]regionin[WhatcountrydidNikolaTesla…inNewYorkCity?]?Replacethesentencewithin[]witharelativeclauseand…intoanaturalquestion: Test Sample\nStep 4:Multi-Hop Question GenerationTake type 3 as an example:\nStage 2: Multi-Hop QAs via Composition\nXN\nStep 1: Reasoning Chain Composition\nIn:\nIsVenicelocatedontheAdriaticSea?Out:\nIn:\nIsittheMississippiRiverthatservesastheeasternborderoftheAmericanGothicpaintingby…thatNikolaTeslaemigratedtoin1884toworkforThomasEdisoninNewYorkCity?Out:Step 2: Duplication Control\nGenerated 2-Hop QAs:\nFigure 3: Generation steps for MHQA groups. In step 3 and step 4, we use 4 manually designed demonstrations for\nICL. Each MHQA group includes a multi-hop question, the corresponding answer and decomposed QA quadruplets.\nNodes and texts highlighted in red, blue and green successively refer to the last hops, intermediate hops and\ngenerated hops. Manually designed texts are highlighted in purple.\ngin with extracting candidate answers in the gen-\nerated passage p2 while blocking the keyword k1\nand the answer a1 (also known as k2) in the first-\nhop QA to avoid cyclic graphs. For each candidate\nanswer a2, we require the LLM to generate a ques-\ntion q2 which contains the first-hop answer a1 and\ncan be answered by the candidate answer a2. We\nexamine the quality of q2 with the same double-\ncheck in Step 2, and ensure the second-hop ques-\ntion q2 contains the first-hop answer a1 (a1 ∈q2)\nfor connected reasoning. Then we repeat the same\nprompts in Step 2 to generate explanation e2.\nSo far, we have instructed the LLM to generate\na 2-hop commonsense QA quadruplet pair, which\nis (p1, q1, a1, e1) →(p2, q2, a2, e2) with a1 ∈q2.\nDetailed prompt templates are shown in Figure 2\nand Appendix B.\n3.2 Multi-Hop QAs via Composition\nIn stage 2, we construct multi-hop reasoning chains\nwith the connected 2-hop QA quadruplets, which\nis illustrated in Figure 3. We propose an auto-\nmated dataset construction pipeline to build ODMR\ndatasets with 2-4 hops, which has the following 4\nsteps:\nStep 1: Reasoning Chain CompositionTo con-\nnect more questions, we follow the composability\ncriteria (Trivedi et al., 2022), that is, two single-hop\nQA pairs (q1, a1) and (q2, a2) are composable into\na multi-hop question Q with a2 as a valid answer\nif a1 is a named entity and it is mentioned in q2.\nSuch criteria are already satisfied when our 2-hop\nQA pairs are generated, we use this criterion for\nconnecting more questions. We adopt 6 reasoning\ngraphs with 2-4 hops to build 6 types of multi-hop\nreasoning chains (Figure 6 in Appendix), and we\nensure that in each reasoning chain: 1) the answer\nai to an intermediate question qi will appear and\nONLY appear in its next-hop questionqi+1 to avoid\nshortcuts; 2) the answer to the last question will\nNOT appear in any intermediate questions.\nStep 2: Duplication Control Built by rule-\nbased composition, our new dataset has consid-\nerably similar reasoning chains that have duplicate\nintermediate questions. To ensure the diversity and\nsimplicity of our dataset, we filter out the reason-\ning chains by a preset duplication degree which is\ndefined by the number of questions that co-existed\nin other chains within the same reasoning type.\nStep 3: Binary Question GenerationWe notice\nthat MHQA datasets also include general interroga-\ntive questions which should be answered by \"Yes\"\nor \"No\", rather than a named entity. Therefore, we\nleverage the LLM to reform the last QA (qn, an)\nof some reasoning chains to binary question with 4\nmanually designed in-context demonstrations. For\neach reasoning type, we randomly sample 10% rea-\nsoning chains for positive question generation and\n2721\nTable 1: Comparison of different approaches on four MHQA benchmarks. The fine-tuning methods are fine-tuned on\nthe train split of NQ (Kwiatkowski et al., 2019) dataset. Among them, methods marked with \"†\" use the Wikipedia\ndump (Karpukhin et al., 2020) as extra corpus. For retrieval-based methods, we use a fine-tuned DPR (Karpukhin\net al., 2020) to retrieve top-5 documents from Wikipedia as context and employ LLM as Reader to answer the\nquestion based on the context. Methods based on ChatGPT are performed by gpt-3.5-turbo-0301 version.\nMethods MSQ HotpotQA 2Wiki CWebQ Average\nEM F1 EM F1 EM F1 EM F1 EM F1\nFine-tuning methods with extra corpus\nDPR†(Karpukhin et al., 2020) 7.5 16.6 14.7 23.0 6.5 14.6 21.3 30.2 12.5 21.1\nRAG†(Lewis et al., 2020) 3.9 8.2 9.6 15.5 15.7 18.6 13.3 15.8 10.6 14.5\nREALM†(Guu et al., 2020) 3.7 8.5 15.3 22.0 5.1 9.6 21.1 27.3 11.3 16.9\nT5-11B-SSM (Roberts et al., 2020) 10.6 16.6 15.4 22.4 15.6 20.5 28.5 35.5 17.5 23.8\nRetrieval-based methods with LLMs\nDPR†+ ChatGPT 1.7 4.1 15.8 21.4 10.9 18.1 13.7 18.7 10.5 15.6\nDPR†+ Alpaca-13B (Taori et al., 2023) 2.2 8.4 12.0 21.5 12.6 21.2 15.9 27.1 10.7 19.5\nDPR†+ Vicuna-13B (Chiang et al., 2023) 2.2 7.4 18.6 25.8 23.9 27.6 20.3 27.9 16.2 22.1\nDPR†+ WizardLM-13B (Xu et al., 2023) 3.5 10.0 19.9 28.4 22.8 27.5 24.2 32.2 17.6 24.5\nDPR†+ InstructGPT (Ouyang et al., 2022) 4.8 11.6 26.3 34.8 23.3 27.1 34.4 41.6 22.2 28.8\nLLM-only methods on ChatGPT\nZero-Shot 3.1 7.3 22.4 30.0 18.7 21.7 31.6 37.5 19.0 24.1\nSelf-Prompting (Li et al., 2022) 2.9 6.2 23.8 31.2 18.9 23.5 26.8 32.6 18.1 23.4\nGENREAD (Yu et al., 2023) 8.6 14.6 33.2 42.6 30.4 35.3 33.7 40.1 26.5 33.2\nZero-shot-CoT (Kojima et al., 2023) 5.0 8.8 22.6 29.6 24.3 27.1 30.3 36.2 20.6 25.4\nAuto-CoT (Zhang et al., 2022) 8.1 13.6 26.1 36.3 26.2 30.2 29.9 38.4 22.6 29.6\nManual-CoT (random) (Wei et al., 2023) 12.3 19.2 32.4 43.7 27.7 34.6 36.6 43.0 27.3 35.1\nSP-CoT (Ours) 14.5 22.6 33.2 42.9 30.1 34.7 37.5 43.6 28.8 36.0\n10% for negative ones. Then we reform a new\nreasoning chain by the generated binary question\ntogether with its previous question hops and add it\nto the dataset.\nStep 4: Multi-Hop Question GenerationNow\nwe need to generate multi-hop questions, to which\nthe previously generated question chains serve as\ntheir intermediate reasoning steps. For each ques-\ntion chain, we iteratively replace the answer ai to\nan intermediate question qi in the next-hop ques-\ntion qi+1 by [qi] until the last question qn is re-\nplaced, which indicates a relative clause. Then we\nleverage the LLM to reform it into a natural multi-\nhop question with 4 manually designed in-context\ndemonstrations.\nAfter the pipeline above, we construct a high\nquality ODMR dataset with 2-4 hops, including the\noverall multi-hop question, the decomposed reason-\ning chains with detailed QA quadruplets. With the\ndouble-check in generation and the composability\ncriteria, we automatically build a high quality new\ndataset. Detailed prompt templates are presented\nin Figure 3 and Appendix B.\n3.3 Adaptive In-context Demonstration\nIn this stage, we sample multi-hop questions from\nour generated ODMR dataset as in-context demon-\nstrations.\nClustering-based Retrieval Some previous\nworks (Zhang et al., 2022; Li et al., 2022) have\nshown that clustering-based methods benefit from\nthe diversity of demonstrations. We adopt a\nclustering-based retrieval approach to adaptively\nsample in-context demonstrations for the input\nquestion. First, all the questions are projected to\na high dimension hidden space by encoding with\nSentence-BERT (Reimers and Gurevych, 2019).\nSuppose we need n in-context demonstrations.\nGiven a test question Q, we use k-means to cluster\nthe question embeddings into n clusters and adap-\ntively retrieve the question with the highest cosine\nsimilarity to Q from each cluster.\nBuild Reasoning ChainFor each sampled ex-\nample, we sequentially concatenate the explanation\nfrom each hop, prefaced by \"Step {i}:\", to con-\nstruct a reasoning chain.\n4 Experiments\nOur research questions (RQs) are:\nRQ1: To what extent can SP-CoT boost the\nLLMs on our four ODMR benchmarks, compared\nwith other LLM-only methods?\nRQ2: Is SP-CoT generally effective on recent\npopular instruction-following LLMs?\nTo this end, we conduct experiments on four\nMHQA datasets that require complex multi-step\nreasoning and compare different methods across\n2722\nTable 2: The performance (EM) of our method on recent popular LLMs. We usetext-davinci-003 for InstructGPT.\nOn small-scale LLMs, SP-CoT nearly doubles the average zero-shot performance on four MHQA benchmarks.\nModel Size Method MSQ HotpotQA 2Wiki CWebQ Mean Boost\nAlpaca (Taori et al., 2023) 13B Zero-shot 0.9 9.5 12.5 12.6 8.9 -\nAlpaca (Taori et al., 2023) 13B SP-CoT 5.3 19.8 17.8 26.5 17.4 8.5 ↑\nVicuna (Chiang et al., 2023) 13B Zero-shot 2.3 11.5 18.1 18.3 12.6 -\nVicuna (Chiang et al., 2023) 13B SP-CoT 8.5 23.3 22.0 32.2 21.5 8.9 ↑\nWizardLM (Xu et al., 2023) 13B Zero-shot 2.0 10.3 13.6 17.5 10.9 -\nWizardLM (Xu et al., 2023) 13B SP-CoT 7.3 24.0 27.2 31.3 22.5 11.6 ↑\nInstructGPT (Ouyang et al., 2022) 175B Zero-shot 4.4 25.6 22.8 38.6 22.9 -\nInstructGPT (Ouyang et al., 2022) 175B SP-CoT 14.8 37.4 33.7 46.6 33.1 10.2 ↑\ndifferent LLMs.\n4.1 Benchmarks and Evaluation Metrics\nWe choose the following four MHQA datasets:\nCWebQ, HotpotQA, 2Wiki and MSQ. We set them\nas ODMR benchmarks by taking only the question\nand the answer in each example. Dataset introduc-\ntion and statistics are detailed in Appendix A.\nWe adopt the exact match (EM) and F1 scores\nas our evaluation metrics. Based on the evaluation\nscript of Karpukhin et al. (2020), we add a prepro-\ncessing step which ignores the content within \"()\"\nand splits the answer strings by certain delimiters\nto extract multiple answers.\n4.2 Experiment Settings\nFor reference, we experiment with fine-tuning\nmethods using an extra corpus, which are fine-\ntuned on the training split of NQ (Kwiatkowski\net al., 2019) dataset and most of them adopt the\nWikipedia dump (Karpukhin et al., 2020) as extra\ncorpus. We also test our implementation of the\nretrieval-based methods on most recent LLMs for\nreference. Specifically, we use a fine-tuned DPR\n(Karpukhin et al., 2020) to retrieve top-5 docu-\nments from Wikipedia as context and employ LLM\nas Reader to answer the question based on the con-\ntext. Detailed prompt templates and parameter set-\ntings are provided in the Appendix B.\nUnless otherwise specified, we use Sentence-\nBERT (all-mpnet-base-v2) for question encod-\ning following previous works. The default number\nof in-context demonstrations is 8 and the demon-\nstrations are sampled by the maximum cosine simi-\nlarity of questions in each cluster.\nFor RQ1, we adopt ChatGPT (gpt-3.5-turbo-\n0301) as the LLM to conduct the following exper-\niments. According to OpenAI, 3 gpt-3.5-turbo-\n0301 is an improvement on the InstructGPT text-\n3https://platform.openai.com\ndavinci-003 model, which performs at a similar\ncapability level to text-davinci-003 for infer-\nence. We use the whole development set of each\ndataset in our experiments.\nFor RQ2, we not only test InstructGPT ( text-\ndavinci-003), but also employ three smaller-scale\n(13B) LLMs: Alpaca (Taori et al., 2023), Vi-\ncuna (Chiang et al., 2023) and WizardLM (Xu\net al., 2023), which are LLaMA (Touvron et al.,\n2023) models fine-tuned on different large-scale\ninstruction-following datasets. To save computa-\ntional cost, we conduct this experiment on subsets\nof the four datasets by randomly selecting 1000\nsamples from the test sets.\n4.3 Experiment Results\nThe main results of RQ1 are shown in Table 1.\nEven with extra corpus, the models fine-tuned on\nNQ (Kwiatkowski et al., 2019) present poor perfor-\nmance due to the inherent challenges of MHQA.\nWith the same Retriever model, the performance\nof retrieval-based methods depends largely on the\nLLM Readers. Compared to previous LLM-only\nworks, our SP-CoT significantly outperforms the\nAuto-CoT by +6.2 EM and +6.4 F1 scores on\naverage and surpasses the previous SOTA method\nGENREAD (Yu et al., 2023) by+2.3 EM and +2.8\nF1 scores on average. On the most challenging\nbenchmark MSQ, SP-CoT empowers ChatGPT to\noutperform other LLM-only methods by a decent\nmargin.\nWe notice that SP-CoT significantly outperforms\nGENREAD on MSQ, confirming the effective-\nness of providing high quality CoTs as in-context\ndemonstrations for complex multi-hop questions.\nOn the other three datasets, SP-CoT delivers com-\nparable performance with GENREAD. However,\nGENREAD relies heavily on the generation faith-\nfulness of LLMs, which is challenging for small-\nscale LLMs. By breaking down demanding instruc-\ntions into step-by-step simple ones, our method\n2723\nTable 3: The performance (EM) of different methods of demonstration selection. The results from random selection\nrepresent the mean value and standard deviation obtained from 3 runs, each with a different seed.\nMethod MSQ HotpotQA 2Wiki CWebQ Average\nRandom 12.0±0.8 29.5±0.6 25.6±1.2 34.3±0.9 25.4±0.3\nRetrieve 10.5 27.9 24.3 33.5 24.1\nClusterCenter 10.4 26.2 22.8 33.0 23.1\nRetrieveInTypeCluster 11.6 28.7 23.5 35.3 24.8\nRetrieveInCluster 11.5 30.9 27.8 34.0 26.1\n1520253035\n0 2 4 6 8 10\nAverage Score(%)\nNumber of Demonstrations\nEMF1\nFigure 4: Average EM and F1 scores of different num-\nbers of in-context demonstrations. The experiments are\ntested on 1k subsets of four ODMR benchmarks with\nChatGPT (gpt-3.5-turbo-0301).\nis more applicable to small-scale LLMs, which is\nvalidated by Table 2.\nTable 2 presents the results for RQ2. Our pro-\nposed SP-CoT proves to be generally effective by\nsignificantly boosting the performance of all these\nfour LLMs on all four benchmarks. With SP-CoT,\nthe performance of small-scale (13B) LLMs can be\nboosted to be on par with directly prompting LLMs\nthat are over 10×larger, regardless of the elicited\nhigh quality intermediate reasoning steps.\n5 Analysis\nIn this section, we explore the choices of the sam-\npling methods and the number of demonstrations.\nThen we examine the quality of the intermediate\nreasoning steps elicited by SP-CoT and the quality\nof self-generation data. Unless otherwise speci-\nfied, we use ChatGPT (gpt-3.5-turbo-0301) to\nconduct analysis on the same subsets mentioned in\nRQ2 settings.\n5.1 Methods of Demonstration Sampling\nThe performance of ICL depends largely on the\nquality of demonstration sampling. We test the\neffectiveness of the following five strategies: ran-\ndomly sampling (Random), sampling globally by\nmaximum cosine similarity (Retrieve), sampling\n020406080100\nClearConciseComp.DirectIA Recall\nScore (%)\nCriteria\nZero-shot-CoTAuto-CoTSP-CoT\nFigure 5: Evaluation results of the CoT generated by\nthree methods. The first four scores are in terms of\nclearness, conciseness, comprehensibility (Comp.) and\ndirectness given by GPT-4 on 50 examples. The recall\naccuracy of intermediate answers (IA Recall) is reported\non the questions that are correctly answered by all 3\nmethods.\nthe closest to the centroid in each cluster (Cluster-\nCenter), sampling by the maximum cosine similar-\nity in each cluster (RetrieveInCluster) and sampling\nthe most similar QAs in each cluster in a certain\nreasoning type (RetrieveInTypeCluster). The rea-\nsoning type of the input question is determined by\nthe most frequent reasoning type of its k-nearest\nneighbors. As indicated in Table 3, RetrieveInClus-\nter (Li et al., 2022) is the best-performing strategy,\nwhich is exactly the strategy we adopt in previous\nexperiments.\n5.2 Impact of Demonstration Amount\nProviding more in-context demonstrations empiri-\ncally improves ICL performance; however, it also\ncauses increasing computational cost. To this end,\nwe investigate the trade-offs of number of demon-\nstrations and the resulting performance boost. We\nreport the EM and F1 scores over the four bench-\nmarks for 2, 4, 6, 8, and 10 in-context demonstra-\ntions, as well as the scores in a zero-shot setting.\nAs illustrated in Figure 4, the performance of SP-\nCoT increases with the number of demonstrations\nwhen the count is between 2 and 8; however, us-\ning 10 demonstrations doesn’t yield any further\n2724\nperformance boost. In our main experiments, we\nopted for 8 as the default number of demonstra-\ntions, striking a balance between performance and\ncost.\n5.3 Intermediate Reasoning Quality Analysis\nGiven the high-quality CoTs constructed by our\nproposed SP-CoT, we investigate the quality of\nintermediate reasoning steps generated during in-\nference. For this analysis, we use the development\nset of MSQ, as it’s the most challenging of the four\ndatasets and offers decomposed step-by-step QAs.\nWe compare the CoTs generated by Zero-shot-CoT,\nAuto-CoT and SP-CoT during inference. For fair-\nness, we select 50 out of a total of 59 questions\nthat all of the three methods answered correctly.\nFirst, we use GPT-4 to evaluate4 the intermediate\nreasoning steps in terms of clearness, conciseness,\ncomprehensibility and directness separately on a\nscale of 1 to 10. Additionally, we compute the re-\ncall accuracy of intermediate answers co-occurring\nin the reasoning steps of each method. For fair-\nness, we only report the intermediate answer recall\naccuracy of correctly answered questions for each\nmethod. As depicted in Figure 5, GPT-4 highly\nfavors our SP-CoT, which achieves nearly a 50%\nrecall accuracy for intermediate answers. This sug-\ngests that SP-CoT elicits high-quality reasoning\nsteps in terms of clearness, conciseness, compre-\nhensibility, and directness.\n6 Conclusion\nIn this work, we harness the capabilities of LLMs\ncombined with self-prompted CoTs to tackle the\nintricate MHQA task within the open-domain con-\ntext, termed as ODMR. Our innovative SP-CoT not\nonly sets a new benchmark by surpassing preced-\ning CoT prompting techniques but also outclasses\nthe erstwhile SOTA LLM-only methodologies in\nopen-domain question-answering. A distinguish-\ning feature of SP-CoT is its proficiency in elicit-\ning high-caliber intermediate reasoning steps, and\nits universal efficacy across both large and small-\nscale LLMs. We anticipate our innovative self-\ngeneration pipeline for ODMR to not just be foun-\ndational for SP-CoT, but also to pave the way for\nfuture research, catalyzing a shift towards lever-\naging self-generation in LLMs, by LLMs, and for\nLLMs.\n4Script from https://github.com/lm-sys/FastChat and mod-\nified.\nAcknowledgements\nThis paper was partially supported by the Joint\nResearch Project of Yangtze River Delta Sci-\nence and Technology Innovation Community (No.\n2022CSJGG1400) and under the technical sup-\nport of National Key R&D Program of China (No.\n2021YFC3340700). Our appreciation also extends\nto Ms. Yangyang Ding, who has been generous\nwith her time and knowledge, offering constructive\ncriticism and enriching discussions.\nLimitations\nOur proposed method (SP-CoT) leverages the\nstrong instruction-following power of LLMs. Such\ncapability is easy to acquire through instruction\nfine-tuning for small-scale LLMs (even 7B), how-\never, some LLMs proposed in early years may\nshow poor capability in following human instruc-\ntions due to lack of corresponding training be-\nfore their release. Therefore, the performance of\nsuch LLMs may not be boosted by our proposed\nSP-CoT. In fact, we did not succeed in boosting\nthe performance of GPT-NeoX by any of Zero-\nshot-CoT, Auto-CoT and SP-CoT. GPT-NeoX is\na 20B LLMs released in early 2022, which shows\npoor instruction-following capability. Please note\nthat neither GENREAD (Yu et al., 2023) nor Self-\nprompting (Li et al., 2022) boosts the performance\nof GPT-NeoX.\nIt is acknowledged that the efficacy of LLM-only\napproaches is predominantly reliant on the LLMs\nthemselves. With smaller-scale LLMs, specifi-\ncally those of 13B scale, our SP-CoT together with\nother CoT methodologies, demonstrate comparable\nor similar performance enhancement across four\nODMR benchmarks as presented in Table 6. The\nconsistent performance of handcrafted CoTs re-\nmains ambivalent across different LLMs and bench-\nmarks; our empirical observations indicate that\nManual-CoT occasionally outperforms SP-CoT,\nwhile at other instances, it does not.\nGiven the potential for LLMs to generate impre-\ncise information, the process by which our SP-CoT\nproduces datasets might also result in the emer-\ngence of inaccurate QA pairs as well as erroneous\nexplanations. Despite the incorporation of a double-\ncheck mechanism to ensure data integrity, certain\nerrors and inaccuracies are inevitably present.\n2725\nReferences\nSteven Bird and Edward Loper. 2004. NLTK: The natu-\nral language toolkit. In Proceedings of the ACL In-\nteractive Poster and Demonstration Sessions, pages\n214–217, Barcelona, Spain. Association for Compu-\ntational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nWei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng,\nZhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan\nZhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion\nStoica, and Eric P. Xing. 2023. Vicuna: An open-\nsource chatbot impressing gpt-4 with 90%* chatgpt\nquality.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nYu Gu, Sue Kase, Michelle Vanni, Brian Sadler, Percy\nLiang, Xifeng Yan, and Yu Su. 2021. Beyond i.i.d.:\nThree levels of generalization for question answering\non knowledge bases. In Proceedings of the Web\nConference 2021. ACM.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasu-\npat, and Ming-Wei Chang. 2020. Realm: Retrieval-\naugmented language model pre-training.\nXanh Ho, Anh-Khoa Duong Nguyen, Saku Sugawara,\nand Akiko Aizawa. 2020. Constructing a multi-\nhop QA dataset for comprehensive evaluation of\nreasoning steps. In Proceedings of the 28th Inter-\nnational Conference on Computational Linguistics,\npages 6609–6625, Barcelona, Spain (Online). Inter-\nnational Committee on Computational Linguistics.\nYichen Jiang, Shikha Bordia, Zheng Zhong, Charles\nDognin, Maneesh Singh, and Mohit Bansal. 2020.\nHover: A dataset for many-hop fact extraction and\nclaim verification.\nMandar Joshi, Eunsol Choi, Daniel Weld, and Luke\nZettlemoyer. 2017. TriviaQA: A large scale distantly\nsupervised challenge dataset for reading comprehen-\nsion. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1601–1611, Vancouver,\nCanada. Association for Computational Linguistics.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for open-\ndomain question answering. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 6769–6781,\nOnline. Association for Computational Linguistics.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, Kristina Toutanova, Llion Jones, Matthew\nKelcey, Ming-Wei Chang, Andrew M. Dai, Jakob\nUszkoreit, Quoc Le, and Slav Petrov. 2019. Natu-\nral questions: A benchmark for question answering\nresearch. Transactions of the Association for Compu-\ntational Linguistics, 7:452–466.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nJunlong Li, Zhuosheng Zhang, and Hai Zhao. 2022.\nSelf-prompting large language models for open-\ndomain qa. arXiv preprint arXiv:2212.08635.\nVaibhav Mavi, Anubhav Jangra, and Adam Jatowt. 2022.\nA survey on multi-hop question answering and gen-\neration.\nOpenAI. 2023. Gpt-4 technical report.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\n2726\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A. Smith, and Mike Lewis. 2023. Measuring\nand narrowing the compositionality gap in language\nmodels.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982–3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the param-\neters of a language model? In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing (EMNLP), pages 5418–5426,\nOnline. Association for Computational Linguistics.\nAlon Talmor and Jonathan Berant. 2018. The web as a\nknowledge-base for answering complex questions.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models.\nHarsh Trivedi, Niranjan Balasubramanian, Tushar Khot,\nand Ashish Sabharwal. 2022. Musique: Multi-\nhop questions via single-hop question composition.\nTransactions of the Association for Computational\nLinguistics, 10:539–554.\nBoshi Wang, Xiang Deng, and Huan Sun. 2022. Itera-\ntively prompt pre-trained language models for chain\nof thought.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, Sharan Narang, Aakanksha Chowdhery, and\nDenny Zhou. 2023. Self-consistency improves chain\nof thought reasoning in language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nCan Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng,\nPu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin\nJiang. 2023. Wizardlm: Empowering large language\nmodels to follow complex instructions.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio,\nWilliam Cohen, Ruslan Salakhutdinov, and Christo-\npher D. Manning. 2018. HotpotQA: A dataset for\ndiverse, explainable multi-hop question answering.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n2369–2380, Brussels, Belgium. Association for Com-\nputational Linguistics.\nWenhao Yu, Dan Iter, Shuohang Wang, Yichong Xu,\nMingxuan Ju, Soumya Sanyal, Chenguang Zhu,\nMichael Zeng, and Meng Jiang. 2023. Generate\nrather than retrieve: Large language models are\nstrong context generators. In International Confer-\nence for Learning Representation (ICLR).\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D.\nGoodman. 2022. Star: Bootstrapping reasoning with\nreasoning.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022. Automatic chain of thought prompting\nin large language models.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nClaire Cui, Olivier Bousquet, Quoc Le, and Ed Chi.\n2023. Least-to-most prompting enables complex rea-\nsoning in large language models.\nA Datasets\nA.1 Introduction\nHotpotQA (Yang et al., 2018) HotpotQA is\na widely used dataset for multi-hop question-\nanswering (MHQA), which contains 113k multi-\nhop questions in natural language. The questions\nare collected by crowdsourcing based on Wikipedia\narticles with human annotated supporting evidence\nand answers.\n2WikiMultiHopQA (Ho et al., 2020)2Wiki-\nMultiHopQA s a recently proposed large-scale\nMHQA dataset, which contains over 192k samples\nconstructed jointly from Wikipedia and Wikidata.\nMuSiQue-Ans (Trivedi et al., 2022)MuSiQue-\nAns (MSQ) is a recent challenging MHQA dataset\ncreated via single-hop question composition. It\nincludes 25k 2-4 hop questions with six differ-\nent composition structures. Although MSQ is\ncomposed from existing datasets, it poses 3×the\nhuman-machine gap with a substantially lower dis-\nconnected reasoning score.\nComplexWebQuestions (Talmor and Berant,\n2018) ComplexWebQuestions is a manually gen-\nerated MHQA dataset of 35k QA pairs. CWebQ\nis generated by rephrasing questions generated by\nmachine from existing dataset.\nA.2 Statistics\nThe statistics of four datasets are shown in Table 4.\n2727\nTable 4: Statistics of development set of four MHQA\nbenchmarks, average tokens for questions, answers and\naverage number of reasoning steps. CWebQ does not\nprovide supporting evidence or question decomposition.\nMSQ HotpotQA CWebQ 2Wiki\nQ len. 18.11 15.83 13.37 11.98\nA len. 2.8 2.46 2.42 2.41\nSteps 2.65 2.68 - 2.47\nSize 2417 7405 3519 12576\nB Prompt Templates\nB.1 First-Hop QA Generation\nWe following the notations described in Section 3.\nThe templates are:\n1. Name {Number} {Topic}:\n2. Generate a Wikipedia passage about { k1}.\n3. Passage about { k1}:\\n{p1}\\n\\nGenerate a\nquestion to which the answer is the entity\n{a1}.\n4. Passage about { k1}:\\n{p1}\\n\\nQuestion:\\n\n{q1}\\n\\nExtract the answer directly from the\npassage in less words as possible.\n5. Passage about { k1}:\\n{p1}\\n\\n Question:\\n\n{q1}\\n\\nAnswer:\\n{a1}\\n\\nAccording to an\nevidence from the passage to support the an-\nswer, rewrite it to make its meaning clear with-\nout passage.\nB.2 Second-Hop QA Generation\n1. Generate a Wikipedia passage about { k2}.\n2. Passage about { k2}:\\n{p2}\\n\\nGenerate a\nquestion that meets the following conditions:\n1. contains the term ’{k2}’ in question, 2. the\nanswer is {a2}, 3. Avoid the following entities\nin the question: {k2}\n3. Passage about { k2}:\\n{p2}\\n\\nQuestion:\\n\n{q2}\\n\\nExtract the answer directly from the\npassage in less words as possible.\n4. Passage about { k2}:\\n{p2}\\n\\n Question:\\n\n{q1}\\n\\nAnswer:\\n{a2}\\n\\nAccording to an\nevidence from the passage to support the an-\nswer, rewrite it to make its meaning clear with-\nout passage.\nB.3 Binary Question Generation\n• Question: {qn}\\nAnswer: {an}\\nReform the\nquestion to a general interrogative question\nthat can be answered with yes:\n• Question: {qn}\\nAnswer: {an}\\nReform the\nquestion to a general interrogative question\nthat can be answered with no:\nB.4 Multi-Hop Question Generation\n• Raw question: { qn}\\nReplace the sentence\nwithin [] with a relative clause and make the\nraw question into a natural question:\nB.5 CoT Construction\nSuppose q∗ is the generated multi-hop question, ei\ndenotes the explanation from intermediate hop (qi,\nai, ei), a∗ is the answer of the last hop (a∗ = an).\nThe template is:\n• Question: {q∗}\\nAnswer: Let’s think step by\nstep:\\nStep 1: {e1}\\nStep 2: {e2}\\n ... There-\nfore, the answer in just one entity is: {a∗},\nB.6 Inference\nB.6.1 Zero-shot\nGiven a question Q, the inference template is:\n• Answer the following question with just one\nentity:\\nQuestion: {Q}\\nAnswer:\nB.6.2 SP-CoT\nSuppose we have the input question Q and 2\ndemonstrations ( q1, r1, a1), ( q2, r2, a2), where\nqi, ri, ai denote the question, CoT and answer of\nthe ith demonstration. The inference template is:\n• Question: { q1}\\n{r1}\\n\\nQuestion: { q2}\\n\n{r2}\\n\\nQuestion: {Q}\\nAnswer: Let’s think\nstep by step:\\n\nC Experiment Settings\nC.1 Hyperparameters\nThis section is the experiment settings on ChatGPT\n(gpt-3.5-turbo-0301) only, for more settings of\nother LLMs used in our experiments, please see our\ncode. Our code and datasets are publicly available\nat https://github.com/noewangjy/SP-CoT.\n2728\nTable 5: Performance (EM/F1) of additional CoT variants. In our experiment, Manual-CoT (Cherry-Pick) adopts\n8 cherry-picked questions and their CoTs manually writen by the authors. The results of Manual-CoT (Random)\nreport the mean EM scores of randomly selected questions and theirs manual CoTs for 2 experiments across the 4\nbenchmarks. Self-Consistency (Wang et al., 2023) is based on SP-CoT with 5 responses for each question.\nMethod MSQ HotpotQA 2Wiki CWebQ Average\nZero-shot 3.1/7.3 22.4/30.0 18.7/21.7 31.6/37.5 19.0/24.1\nZero-shot-CoT (Kojima et al., 2023) 5.0/8.8 22.6/29.6 24.3/27.1 30.3/36.2 20.6/25.4\nManual-CoT (Wei et al., 2023) (Random) 12.3/19.2 32.4/43.7 27.7/34.6 36.6/43.0 27.3/35.1\nManual-CoT (Wei et al., 2023) (Cherry-Pick) 13.7/21.9 33.9/44.7 31.5/38.6 37.2/44.0 29.1/37.3\nAuto-CoT (Zhang et al., 2022) 8.1/13.6 26.1/36.3 26.2/30.2 29.9/38.3 22.6/29.6\nSP-CoT (Ours) 14.5/22.6 33.2/42.9 30.1/34.7 37.5/43.6 28.8/36.0\nSP-CoT + Self-Consistency (Wang et al., 2023) 18.3/28.3 -/- -/- 47.1/54.0 -/-\nTable 6: The performance (EM) of CoT methods with recent popular LLMs on 1k subsets of test sets. We use\ngpt-3.5-turbo-0301 for ChatGPT and text-davinci-003 for InstructGPT. On smaller-scale (13B) LLMs, CoT\nmethods achieve comparable performance boost on four MHQA benchmarks.\nMethod ChatGPT InstructGPT Alpaca-13B Vicuna-13B Wizard-13B\nZero-shot 20.8 22.9 8.9 12.6 10.9\nManual-CoT (Wei et al., 2023) (Cherry-Pick) 28.7 33.1 18.0 23.7 24.7\nManual-CoT (Wei et al., 2023) (Random) 26.3 32.4 17.7 22.0 24.0\nAuto-CoT (Zhang et al., 2022) 21.2 31.4 16.9 21.3 23.3\nSP-CoT (Ours) 26.1 33.1 17.4 21.5 22.5\nC.1.1 System message\n\"You should use your knowledge to answer the\nquestion to the best of your ability, not refuse to\nanswer, even though you know your knowledge\nis sometimes out of date. If some references are\nuncertain, answer all possible cases rather than\nrequesting further information.\"\nC.1.2 Temperature\nIn most cases, the default temperature is set to 0 for\nobtaining a most deterministic response. When we\nask ChatGPT to name some terms or to generation\na question, the temperature is set to 1.0 for more\ndiversity.\nC.2 LLMs\nThe 13B LLMs used in our experi-\nments are from Huggingface Hub, use\nchavinlo/gpt4-x-alpaca for Alpaca-13B,\nTheBloke/wizard-vicuna-13B-HF for Vicuna-\n13B and TheBloke/wizardLM-13B-1.0-fp16 for\nWizardLM-13B.\nD Additional Experiments\nD.1 Comparison of CoT Variants\nTo provide a more comprehensive picture of cur-\nrent CoT methods on ODQA, we report hereby\n(Table 5) the performance of additional CoT vari-\nants, including Manual-CoT (Wei et al., 2023)\nand Auto-CoT (Zhang et al., 2022) on ChatGPT\n(gpt-3.5-turbo-0301).\nIn our experiment, Manual-CoT (Cherry-Pick)\nadopts 8 cherry-picked questions and their CoTs\nmanually writen by the authors. The results of\nManual-CoT (Random) report the mean EM scores\nof randomly selected questions and theirs manual\nCoTs for 2 experiments across the 4 benchmarks.\nSelf-Consistency (Wang et al., 2023) is based on\nSP-CoT with 5 responses for each question.\nTo the best of our knowledge, Self-Consistency\n(Wang et al., 2023) is orthogonal to existing\nCoT methods, including SP-CoT. Although Self-\nConsistency boosts the performance of SP-CoT to\na higher level (10%-30% increase for 5 runs), it’s\nworth noting that the cost of Self-Consistency is\nalso 5 times higher.\nIn Table 6, we report the performance (EM) of\nCoT methods with recent popular LLMs on 1k sub-\nsets of the test sets. The scores are the average EM\nscores on 4 ODMR benchmarks. Although Manual-\nCoT (Wei et al., 2023) outperforms Automated\nmethods, it requires high quality human-labeled\nCoTs, which is not always accessible in real world\napplications. Since the cherry-picked CoTs take\nthe dataset features in to consideration, we con-\nsider their results as the theoretical upper limitof\nautomated approaches. Compared to previously au-\ntomatic SOTA method (Auto-CoT), our proposed\nSP-CoT shows a decent performance boost in most\ncases.\n2729\nTable 7: The scale and average hops of each ODMR dataset generated in this paper. Please note that the scale and\nthe average hops are largely decided by the self-generation setting and the duplication control process.\nChatGPT Alpaca-13B Vicuna-13B WizardLM-13B\nNumber of Samples 3550 1354 1562 1604\nAvg. Number of Hops 3.04 2.82 2.92 3.16\nAvg. Question tokens 34.61 25.35 31.05 34.30\nAvg. Answer tokens 1.86 1.99 2.02 1.91\nD.2 SP-CoT on GrailQA\nWe report our experiment of CoT methods on 1k\nsubset of test set provided by GrailQA (Gu et al.,\n2021). According to our ODMR setting, no ex-\nternal knowledge is provided to LLMs. From the\nresults below, we notice that our proposed SP-CoT\nis effective on GrailQA, our results on InstructGPT\n(text-davinci-003) are presented in Table 8.\nTable 8: Performance of CoT methods on 1k subset of\ntest set provided by GrailQA (Gu et al., 2021).\nMethod EM F1\nZero-shot 12.9 24.3\nZero-shot-CoT (Kojima et al., 2023) 13.5 25.2\nManual-CoT (Wei et al., 2023) (Random) 17.7 30.7\nSP-CoT (Ours) 16.0 28.2\nE Constructed ODMR Datasets\nE.1 Overview and scale\nFor better understanding of the constructed ODMR\ndatasets, we offer a well-designed figure (Figure\n6) to illustrate the six types of generated questions\nand their step-by-step decomposition. The scale of\nthe generated ODMR datasets is about 1-4k (Ta-\nble 7), however, it’s largely dependent by the self-\ngeneration setting (how many examples to gener-\nate?) and the Duplication Control process in Stage\n2 Step 2 (How many examples to keep?) To be\nmore specific, the number of topic terms for self-\ngeneration decides the scale of generated 2-hop\nquestion pairs, and the level of duplication (how\nmany existing question hops are allowed when con-\nstructing a new reasoning chain) decides the scale\nof the remaining examples after filtering.\nE.2 Topics\nThe 29 manually designed topics for generation\nare: politicians, athletes, sports teams, sports\nevents, countries, cities, historical figures, histori-\ncal events, wars, religions, singers, songs, actors\nor actresses, movies or TV series, writers, books,\npainters, paintings, composers, classical music,\ntourist attractions, scientists, scientific terms, video\ngames, animals, plants, foods, enterprises, interna-\ntional organizations.\nE.3 Quality Control\nTo ensure the self-generation quality, two mecha-\nnisms are included in our proposed method.\nSelf-validation in self-generation: To ensure\nthe quality of generated QA pairs, we employ a\ndouble-check process, where we demand the LLM\nto answer the generated question given the gen-\nerated context and double-check If the generated\nanswer is accordant with the target answer.\nComposability criteria in composition: Two\nsingle-hop QA pairs (q1, a1) and (q2, a2) are com-\nposable into a multi-hop question Q with a2 as a\nvalid answer if a1 is a named entity and it is men-\ntioned in q2. Such criteria are already satisfied\nwhen our 2-hop QA pairs are generated, we use it\nfor connecting more questions.\nE.4 Composition rather than Generation\nDirectly generating k-hop questions will produce\nmany highly-duplicated reasoning chains, which is\nless effective than conposition with 2-hop QAs.\nTake a connected 3-hop QAs as example:\n(Q1, A1) →(Q2, A2) →(Q3, A3), where A1 in\nQ2, A2 in Q3.\nSuppose there are 2 valid next-hop QAs\n(Q4, A4) and (Q5, A5) for (Q3, A3). Now\nwe have 2 generated 4-hop reasoning chains:\n(Q1, A1) →(Q2, A2) →(Q3, A3) →(Q4, A4)\nand (Q1, A1) → (Q2, A2) → (Q3, A3) →\n(Q5, A5)\nwhich are highly-duplicated. When directly\ngenerating k-hop reasoning chains, the number of\nhighly-duplicated chains will increase exponen-\ntially and such chains will be filtered out in the\nDuplication Control Process (Stage 2, Step 2).\nFurthermore, when there are more that 2 ques-\ntion hops in one reasoning chain, more effort\nshould be made to ensure direct acyclic graphs\n(DAGs). An example of cyclic reasoning chain is\n(Q1, A1) →(Q2, A2) →(Q1, A1), which should\nbe avoided.\n2730\nQ:Whatisanothercommonnameforsomemembersofthegenustowhichdaffodilsbelong,besidesdaffodil?A:Jonquil\n1.Q:Whatisthegenustowhichdaffodilsbelong?A:Narcissus2.Q:WhatisanothercommonnameforsomemembersoftheNarcissusgenusbesidesdaffodil?A:Jonquil\nGraphQA Decomposition\nQ:WhoheldthetitlesofKingoftheFrenchandTheDukeofOrleansduringtheyearwhenJohnLindleyfirstdescribedthesubfamilyofthefamilyAsparagaceaethathyacinthsbelongto?A:LouisPhilippe\n1.Q:WhatisthesubfamilyofthefamilyAsparagaceaethathyacinthsbelongto?A:Scilloideae2.Q:InwhatyearwasthesubfamilyScilloideaefirstdescribedbyJohnLindley?A:18303.Q:WhobecameKingoftheFrenchin1830andalsoheldthetitleofTheDukeofOrleans?A:LouisPhilippeQ:Whichcountry,commonlyvisitedbyshipstravelingfromEuropeandwheremostspeciesofrosesarenativeto,islocatedsouthoftheremoteislandwhereNapoleonBonapartewasexiledandeventuallydied?A:SouthAfrica\n1.Q:Wherearemostspeciesofrosesnativeto?A:Asia2.Q:TowhichremoteislandwasNapoleonBonaparteexiledandwheredidheeventuallydie?A:SaintHelena3.Q:WhatcountryislocatedsouthofSaintHelenaandwasacommondestinationforshipstravelingfromAsiaandEurope?A:SouthAfricaQ:Whatworkdidthepersonwhosharedcreditwiththefatherofthebiblicalfigurereferencedinthetitleofthesong\"StairwaytoHeaven\"defendhisoptimismin,whilealsofamouslycontributingtothedevel-opmentofinfinitesimalcalculuswi-thNewton?A:Theodicy\n1.Q:Whoisthebiblicalfigurethatthetitleofthesong\"StairwaytoHeaven\"references?A:Jacob2.Q:WhowasthefatherofJacob?A:Isaac3.Q:WhosharescreditwithIsaacNewtonfordevelopingtheinfinitesimalcalculus?A:GottfriedWilhelmLeibniz4.Q:WhatworkdidGottfriedWilhelmLeibnizfamouslydefendhisoptimismin?A:TheodicyQ:WhatbodyofwaterseparatesthecountrythatisakeyallyofthecountrythatIraqlaunchedScudmissilesatduringtheGulfWar,fromCanada?A:theAtlanticOcean\n1.Q:WhatcountrydidIraqlaunchScudmissilesatduringtheGulfWar,andwhyisthisrelevanttotheconflict?A:Israel2.Q:WhatisthenativeregionofCarrot?A:theMiddleEast3.Q:WhatisthenameofthecountrythatisakeyallyofIsraelintheMiddleEast?A:theUnitedStates4.Q:WhatbodyofwaterseparatestheUnitedStatesfromCanada?A:theAtlanticOceanQ:WhatisthenameofthetowninthecountrythatisthelargestinSouthAmericaafterthecountrywhereLionelMessirepresentsininternationalfootballcompetitionswhereTomBrady'swifewasborn?A:TrêsdeMaio\n1.Q:WhichcountrydoesLionelMessirepresentininternationalfootballcompetitions?A:Argentina2.Q:WhatisthelargestcountryinSouthAmericaafterArgentina?A:Brazil3.Q:WhatisthenameofthemodelthatTomBradyismarriedto?A:GiseleBündchen4.Q:WhatisthenameofthetowninBrazilwhereGiseleBündchenwasborn?A:TrêsdeMaio\nFigure 6: The illustration of six reasoning types in our automated dataset construction pipeline. These selected\nexamples are self-generated by ChatGPT (gpt-3.5-turbo-0301), Vicuna-13B and WizardLM-13B.\n2731",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6348963975906372
    },
    {
      "name": "Inference",
      "score": 0.5150055885314941
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5107500553131104
    },
    {
      "name": "Scalability",
      "score": 0.44780951738357544
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3689730763435364
    },
    {
      "name": "Natural language processing",
      "score": 0.3397591710090637
    },
    {
      "name": "Database",
      "score": 0.07854926586151123
    },
    {
      "name": "Biology",
      "score": 0.07675406336784363
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}