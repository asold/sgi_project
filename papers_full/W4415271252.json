{
  "title": "Safeguarding large language models: a survey",
  "url": "https://openalex.org/W4415271252",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2102427173",
      "name": "DONG YI",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A4313580812",
      "name": "Mu, Ronghui",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A4377973252",
      "name": "Zhang, Yanghao",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A2356594925",
      "name": "Sun, Siqi",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A2111994651",
      "name": "Zhang Tianle",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A2323164145",
      "name": "Wu Chang-Shun",
      "affiliations": [
        "Laboratoire Interdisciplinaire de Physique",
        "Université Grenoble Alpes",
        "Université Stendhal – Grenoble 3"
      ]
    },
    {
      "id": "https://openalex.org/A2381604162",
      "name": "Jin Gaojie",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A2145132611",
      "name": "Qi Yi",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A1895794483",
      "name": "Hu Jinwei",
      "affiliations": [
        "University of Liverpool"
      ]
    },
    {
      "id": "https://openalex.org/A1918677189",
      "name": "Meng Jie",
      "affiliations": [
        "Loughborough University"
      ]
    },
    {
      "id": "https://openalex.org/A2249432708",
      "name": "Bensalem, Saddek",
      "affiliations": [
        "Laboratoire Interdisciplinaire de Physique",
        "Université Grenoble Alpes",
        "Université Stendhal – Grenoble 3",
        "Saint-Gobain (France)"
      ]
    },
    {
      "id": "https://openalex.org/A1964308939",
      "name": "Huang Xiao-wei",
      "affiliations": [
        "University of Liverpool"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2473418344",
    "https://openalex.org/W2038333915",
    "https://openalex.org/W4205725534",
    "https://openalex.org/W4388757737",
    "https://openalex.org/W4404781959",
    "https://openalex.org/W4390578204",
    "https://openalex.org/W4367185264",
    "https://openalex.org/W4367188881",
    "https://openalex.org/W3037831233",
    "https://openalex.org/W2773888539",
    "https://openalex.org/W4396833426",
    "https://openalex.org/W4388850876",
    "https://openalex.org/W3204619801",
    "https://openalex.org/W4405181291",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W4389519367",
    "https://openalex.org/W4389523771",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2971173235",
    "https://openalex.org/W3203106571",
    "https://openalex.org/W4387143023",
    "https://openalex.org/W3134678353",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W2947681066",
    "https://openalex.org/W4225858632",
    "https://openalex.org/W4309618884",
    "https://openalex.org/W4388886073",
    "https://openalex.org/W3158360872",
    "https://openalex.org/W4399750638",
    "https://openalex.org/W4385572039",
    "https://openalex.org/W4390189975",
    "https://openalex.org/W4400315206",
    "https://openalex.org/W4385573644",
    "https://openalex.org/W4396700714",
    "https://openalex.org/W4386576626",
    "https://openalex.org/W4401615053",
    "https://openalex.org/W4385764073",
    "https://openalex.org/W4389520749",
    "https://openalex.org/W4402671666",
    "https://openalex.org/W3102914525",
    "https://openalex.org/W4393867901",
    "https://openalex.org/W1969900674",
    "https://openalex.org/W3021254583",
    "https://openalex.org/W4389519598",
    "https://openalex.org/W3168251909",
    "https://openalex.org/W4385852384",
    "https://openalex.org/W3153193605",
    "https://openalex.org/W4402304442",
    "https://openalex.org/W2535351973",
    "https://openalex.org/W4283206275",
    "https://openalex.org/W4385571225",
    "https://openalex.org/W4400577092",
    "https://openalex.org/W4383711355",
    "https://openalex.org/W3034951181",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W4406263753",
    "https://openalex.org/W4365794479",
    "https://openalex.org/W4385573981",
    "https://openalex.org/W4393035858",
    "https://openalex.org/W4391107732",
    "https://openalex.org/W4385571495",
    "https://openalex.org/W4401043228",
    "https://openalex.org/W4389524506",
    "https://openalex.org/W4385573783",
    "https://openalex.org/W4401042900",
    "https://openalex.org/W3034937117",
    "https://openalex.org/W4392201111",
    "https://openalex.org/W4221045317",
    "https://openalex.org/W4385571830",
    "https://openalex.org/W4405181600",
    "https://openalex.org/W4385573200",
    "https://openalex.org/W3155807546",
    "https://openalex.org/W2946363484",
    "https://openalex.org/W4414333247",
    "https://openalex.org/W4385573539",
    "https://openalex.org/W2171627300",
    "https://openalex.org/W4404725774",
    "https://openalex.org/W4226040778",
    "https://openalex.org/W4206637810",
    "https://openalex.org/W4409361739",
    "https://openalex.org/W4389524162",
    "https://openalex.org/W4385573323",
    "https://openalex.org/W4389617257",
    "https://openalex.org/W4385571122",
    "https://openalex.org/W4402671718",
    "https://openalex.org/W4408734515",
    "https://openalex.org/W4391092855",
    "https://openalex.org/W4402264157",
    "https://openalex.org/W4404782219",
    "https://openalex.org/W2962977603",
    "https://openalex.org/W3095789240",
    "https://openalex.org/W4401042550",
    "https://openalex.org/W4401042286",
    "https://openalex.org/W4404782283",
    "https://openalex.org/W3100258764",
    "https://openalex.org/W3104224589"
  ],
  "abstract": "Abstract In the burgeoning field of Large Language Models (LLMs), developing a robust safety mechanism, colloquially known as “safeguards” or “guardrails”, has become imperative to ensure the ethical use of LLMs within prescribed boundaries. This article provides a systematic literature review on the current status of this critical mechanism. It discusses its major challenges and how it can be enhanced into a comprehensive mechanism dealing with ethical issues in various contexts. First, the paper elucidates the current landscape of safeguarding mechanisms that major LLM service providers and the open-source community employ. This is followed by the techniques to evaluate, analyze, and enhance some (un)desirable properties that a guardrail might want to enforce, such as hallucinations, fairness, privacy, and so on. Based on them, we review techniques to circumvent these controls (i.e., attacks), to defend the attacks, and to reinforce the guardrails. While the techniques mentioned above represent the current status and the active research trends, we also discuss several challenges that cannot be easily dealt with by the methods and present our vision on how to implement a comprehensive guardrail through the full consideration of multi-disciplinary approach, neural-symbolic method, and systems development lifecycle.",
  "full_text": "Received: 21 October 2024 / Accepted: 31 August 2025\n© The Author(s) 2025\nYi Dong and Ronghui Mu have been contributed equally to this work.\n \r Xiaowei Huang\nxiaowei.huang@liverpool.ac.uk\n1 Department of Computer Science, University of Liverpool, Brownlow Street,  \nLiverpool L69 3BX, UK\n2 IMAG, Université Grenoble Alpes, Saint-Martin-d’Hères, France\n3 Loughborough University, Loughborough, UK\n4 CSX-AI, Saint-Didier-au-Mont-d’Or, France\nSafeguarding large language models: a survey\nYi Dong1 · Ronghui Mu1 · Yanghao Zhang1 · Siqi Sun1 · Tianle Zhang1 · \nChangshun Wu2 · Gaojie Jin1 · Yi Qi1 · Jinwei Hu1 · Jie Meng3 · Saddek Bensalem2,4 · \nXiaowei Huang1\nArtificial Intelligence Review          (2025) 58:382 \nhttps://doi.org/10.1007/s10462-025-11389-2\nAbstract\nIn the burgeoning field of Large Language Models (LLMs), developing a robust safety \nmechanism, colloquially known as “safeguards” or “guardrails”, has become imperative \nto ensure the ethical use of LLMs within prescribed boundaries. This article provides a \nsystematic literature review on the current status of this critical mechanism. It discusses \nits major challenges and how it can be enhanced into a comprehensive mechanism dealing \nwith ethical issues in various contexts. First, the paper elucidates the current landscape \nof safeguarding mechanisms that major LLM service providers and the open-source com -\nmunity employ. This is followed by the techniques to evaluate, analyze, and enhance some \n(un)desirable properties that a guardrail might want to enforce, such as hallucinations, fair-\nness, privacy, and so on. Based on them, we review techniques to circumvent these con -\ntrols (i.e., attacks), to defend the attacks, and to reinforce the guardrails. While the tech -\nniques mentioned above represent the current status and the active research trends, we also \ndiscuss several challenges that cannot be easily dealt with by the methods and present our \nvision on how to implement a comprehensive guardrail through the full consideration of \nmulti-disciplinary approach, neural-symbolic method, and systems development lifecycle.\nKeywords Large language models · Generative AI · Safeguards · Guardrails · \nTrustworthy AI\n et al. [full author details at the end of the article]\n1 3\n\nY. Dong et al.\n1 Introduction\nIn recent years, generative artificial intelligence (GenAI) has significantly accelerated \nhumanity’s stride into the era of intelligence. Technologies such as ChatGPT and Sora (Ope-\nnAI 2023) have become a pivotal force driving the transformation of a new generation of \nindustries. However, the rapid deployment and integration of LLMs have raised significant \nconcerns regarding their risks, including, but not limited to, ethical use, data biases, privacy, \nand robustness (Huang et al. 2023). In societal contexts, concerns also include the potential \nmisuse by malicious actors through activities such as spreading misinformation or aiding \ncriminal activities (Kang et al. 2023). In the scientific context, LLMs can be used profes -\nsionally, with dedicated ethical considerations and risks in scientific research (Birhane et al. \n2023).\nTo address these issues, model developers have implemented various safety protocols \nintended to confine the behaviors of these models to a more secure range of functions. The \ncomplexity of LLMs, characterized by intricate networks and numerous parameters, and \nthe closed-source nature (such as ChatGPT) present substantial hurdles. These complexities \nrequire different strategies compared to the pre-LLM era, which focuses on white-box tech-\nniques, enhancing models by various regularizations and architecture adaptations during \ntraining. Therefore, in parallel to the reinforcement learning from human feedback (RLHF) \nand other training skills such as in-context training, the community moves towards employ-\ning black-box, post-hoc strategies , notably guardrails (Welbl et al. 2021; Gehman et al. \n2020), which monitors and filters the inputs and outputs of trained LLMs. A guardrail is an \nalgorithm that takes as input a set of objects (e.g., the input and the output of LLMs) and \ndetermines if and how some enforcement actions can be taken to reduce the risks embedded \nin the objects. If the input to LLMs relates to child exploitation, the guardrail may stop the \ninput or adapt the output to become harmless (Perez et al. 2022). In other words, guardrails \nare used to identify the potential misuse in the query stage and to prevent the model from \nproviding an answer that should not be given.\nThe difficulty in constructing guardrails often lies in establishing their requirements. AI \nregulations can be different across different countries, and in the context of a company, data \nprivacy can be less severe than in the public domain. Nevertheless, a guardrail of LLMs may \ninclude requirements from one or more categories: Hallucination, fairness, privacy, robust-\nness, toxicity, legality, out-of-distribution, uncertainty, etc. In this paper, we do not include \nthe typical requirement, i.e., accuracy, as they are benchmarks of the LLMs and arguably \nnot the responsibilities of the guardrails. That said, there might not be a clear cut on the \nresponsibilities (notably, robustness) between LLMs and the guardrails, and the two models \nshall collaborate to achieve a joint set of objectives. Nevertheless, for concrete applications, \nthe requirements need to be precisely defined, and their corresponding metrics, and a multi-\ndisciplinary approach is called for. Mitigating a given requirement (such as hallucinations, \ntoxicity, fairness, biases, etc) is already non-trivial, as discussed in Sect. 5. Working with \nmultiple requirements worsens it, especially when some can be conflicting. Such complex-\nity requires a sophisticated solution design method to manage. In terms of the design of \nguardrails, while there might not be “one method that rules them all”, a plausible design of \nthe guardrail is neural-symbolic, with learning agents and symbolic agents collaborating \nin processing both the inputs and the outputs of LLMs. Multiple types of neural-symbolic \nagents (Lamb et al. 2021). However, the existing guardrail solutions such as Llama Guard \n1 3\n  382  Page 2 of 56\nSafeguarding large language models: a survey\nInan et al. (2023), Nvidia NeMo Rebedea et al. (2023), and Guardrails AI (Rajpal 2023) use \nthe simplest, loosely coupled ones. Given the complexity of the guardrails, it will be inter -\nesting to investigate other, more deeply coupled, neural-symbolic solution designs.\nLike safety-critical software, a systematic process to cover the development cycle (rang-\ning from specification to design, implementation, integration, verification, validation, and \nproduction release) is required to carefully build the guardrails, as indicated in industrial \nstandards such as ISO-26262 and DO-178B/C. Our contributions are summarized as below: \n(1) Systematically examines existing LLM safeguarding techniques and their real-world \nimplementations; (2) Reviews safety-related properties and evaluation challenges for guard-\nrail systems; (3) Provides a comprehensive taxonomy of attack and defense methodologies \naccording to access contexts; (4) Discusses key considerations for comprehensive guard -\nrail development across technical, collaborative, multimodal extensions and regulatory \ndimensions.\nThis survey starts with some background introduction at Sect. 2. The goal is to (1) \nUnderstand the existing guardrail frameworks that are being used to control model outputs \nin LLM services, as well as the techniques to evauate, analyze, and enhance guardrails \nagainst specific desirable properties (Sect. 3); (2) Understand the techniques that are being \nused to overcome these guardrails, as well as to defend the attacks and to reinforce the \nguardrails (Sect. 4); and then discuss how to achieve a complete guardrail solution, includ-\ning several issues regarding the systematic design of a guardrail for a specific application \ncontext (Sect. 5).\n2 Background for large language models\nLarge Language Models (LLMs), primarily based on the Transformer architecture (Vaswani \net al. 2017), are composed of deep neural networks with several transformer blocks. Each \nblock integrates a self-attention layer and a feedforward layer connected via residual links. \nThis specific self-attention mechanism enables the model to concentrate on neighboring \ntokens while analyzing a specific token. Originally, the transformer architecture was exclu-\nsively developed for machine translation purposes. Newly developed language models that \nutilize transformer architecture can be fine-tuned, thereby removing the need for architec -\ntures tailored to specific tasks (Devlin et al. 2019). Typically, their networks encompass \nhundreds of billions (or more) of parameters and are trained on vast corpora of textual data. \nExamples include ChatGPT-3 (Brown et al. 2020), ChatGPT-4 (OpenAI 2023), LLaMA \n(Touvron et al. 2023), and PaLM (Anil et al. 2023).\nLLMs are employed in a variety of complex tasks, such as conversational AI (Wei et al. \n2023), translation (Lyu et al. 2023), and story generation (Simon and Muise 2022). Cur-\nrent LLMs utilize architectures and training objectives similar to those in smaller language \nmodels, such as the Transformer architecture and tasks centered around language modeling. \nHowever, LLMs distinguish themselves by significantly scaling up in aspects like model \ndimensions, data volume, the breadth of their application scope, and computation cost. \nBuilding an offline model comprises three main stages (Huang et al. 2023): pre-training, \nadaptation tuning, and utilization improvement. Generally, the pre-training phase parallels \nconventional machine learning training, involving data collection, choosing an architecture, \nand undergoing training. The adaptation tuning includes instruction tuning (Lou et al. xxx) \n1 3\nPage 3 of 56   382 \nY. Dong et al.\nand alignment tuning (Ouyang et al. 2022) to enable learning from task-specific instructions \nand adhere to human values. Finally, the utilization improvements can enhance user interac-\ntions, including in-context learning (Brown et al. 2020) and chain-of-thought learning (Wei \net al. 2022).\nAfter training an LLM, its performance against set expectations is crucial. This evalua -\ntion typically encompasses three dimensions: assessing essential performance, conducting \nsafety analysis to understand potential consequences in practical applications, and utilizing \npublicly available benchmark datasets. The primary performance review focuses on essen -\ntial capabilities like language generation and complex reasoning. Safety analysis delves into \nthe LLM’s alignment with human values, interactions with external environments, and inte-\ngration into broader applications such as search engines. Additionally, benchmark datasets \nand accessible tools support this comprehensive evaluation. The outcome of this assessment \ndetermines whether the LLM meets pre-defined criteria and is ready for deployment. If it \nfalls short, the process reverts to one of the earlier training stages to address identified short-\ncomings. At the deployment stage, LLM could be used on a web platform for direct user \ninteraction, like ChatGPT, or integrated into a search engine, like the new Bing. Regardless \nof the application, it is standard practice to implement guardrails in interactions between \nLLMs and users to ensure adherence to AI regulations.\n3 Techniques on design and implementation of guardrails\nThis section presents several existing guardrail techniques have been proposed by the LLM \nservice provider or the open-source community. Then, we review the methods to evaluate, \nanalyze, and enhance the LLMs according to the desirable properties one may expect an \nLLM to have. A comparison checkbox table among different platforms and properties is \nshown in Table. 1.\n3.1 Guardrail frameworks and supporting software packages\nLLM guardrails constitute a suite of safety measures designed to oversee and regulate user \ninteractions with LLM applications. These measures are programmable, rule-based systems \npositioned between users and foundational models. Their primary function is to ensure that \nthe LLM model adheres to an organization’s established principles and operates within a \nprescribed ethical and operational framework. Guardrails are applied during the interaction \nstage between users and deployed LLMs, the last step in the LLM lifecycle. Figure 1 illus-\nTable 1 Abilities among different guardrails\nLlama Guard Nvidia NeMo Guardrails AI TruLens Guidance AI LMQL\nHallucination ✓ ✓ ✓ ✓ ✓ ✓\nFairness ✓ - ✓ ✓ - -\nPrivacy - ✓ - - - -\nRobustness - - - - - -\nToxicity ✓ ✓ ✓ ✓ ✓ ✓\nLegality ✓ - - - - -\nOut-of-Distribution - - ✓ - - -\nUncertainty - ✓ ✓ ✓ - -\n1 3\n  382  Page 4 of 56\nSafeguarding large language models: a survey\ntrates the lifecycle and potential vulnerabilities of the general guardrail mechanism. Devel-\nopers complete the development of guardrails through data processing, guardrail model \ntraining, and model customization or fine-tuning (e.g., Llama Guard and NeMo Guardrails), \nas shown in the yellow area of Fig. 1. These guardrails are then deployed in LLMs to facili-\ntate interaction with users. Typically, users predefine the content that needs protection, also \ncalled custom rules. Subsequently, users interact with LLMs through prompts and await \nthe generated responses. The guardrails evaluate the output against the predefined rules to \ndetermine its compliance. If the content is deemed unsafe, guardrails may block it outright \nor issue a preset warning to the user. Conversely, if the output aligns with the criteria, it is \ndisplayed directly to the user, as indicated in the orange area of Fig. 1. Notably, some exist-\ning attack methods allow unsafe content to bypass guardrail protection, as highlighted in the \nred box of Fig. 1; for a detailed discussion of these attack methods, refer to Sect. 4.\n3.1.1 Llama guard\nLlama Guard Inan et al. (2023), developed by Meta on the Llama2-7b architecture, focuses \non enhancing human-AI conversation safety. It is a fine-tuned model that takes the input and \noutput of the victim model as input and predicts their classification on a set of user-specified \ncategories. Figure 2 shows its workflow. Due to the zero/few-shot abilities of LLMs, Llama \nGuard can be adapted–by defining the user-specified categories –to different taxonomies \nand guidelines that meet requirements for applications and users. This is a Type 1 neural-\nsymbolic system (Lamb et al. 2021), i.e., typical deep learning methods where the input and \noutput of a learning agent are symbolic. It lacks guaranteed reliability since the classifica -\ntion results depend on the LLM’s understanding of the categories and the model’s predictive \naccuracy.\n3.1.2 Nvidia nemo\nNvidia NeMo, described in Rebedea’s work (Rebedea et al. 2023), functions as an interme-\ndiary layer that enhances the control and safety of LLMs. It employs Colang, an executable \nFig. 1 Guardrails lifecycle and vulnerabilities\n \n1 3\nPage 5 of 56   382 \nY. Dong et al.\nprogram language designed by Nvidia (2023), to establish constraints to guide LLMs within \nset dialogical boundaries. When the customer’s input prompt comes, NeMo embeds the \nprompt as a vector and then uses K-Nearest Neighbors (KNN) method to compare it with \nthe stored vector-based user canonical forms, retrieving the embedding vectors that are ‘the \nmost similar’ to the embedded input prompt. The input embedding of Nemo differs from \ntraditional approaches that utilize the initial layers. Instead, it employs embedding through \nsimilarity functions to capture the most relevant semantics. For example, Nemo uses the \n\"sentence-transformers/all-MiniLM-L6-v2” model to compute embeddings, which is used \nfor the following KNN search (Annoy algorithm is employed for efficient nearest-neighbor \nsearch.) Therefore, “input embedding” in the Nemo refers to mapping input sentences and \nparagraphs to a multi-dimensional dense vector space, facilitating the search for the most \nsimilar canonical forms/flows. After that, Nemo starts the flow execution to generate out -\nput from the canonical form. During the flow execution process, the LLMs generate a safe \nanswer if the Colang program requests.\nLLMs will be invoked multiple times during the guardrail flow for various tasks. For \nexample, in a conversation scenario, LLM is utilized in the following three phases: (1) Gen-\nerating user intent: the input of this LLM call includes two contexts: examples and potential \nuser intents (top 5 intents from example code in NeMo Github repository). The output is \na refined user intent (temperature is set as 0 to get the deterministic result). (2) Generating \nnext step: In this phase, Nemo searches the most relevant similar flows and integrates these \nsimilar flows into an example, which is then fed into the LLM. The LLM call output is \ncalled “bot intent.”. (3) Generating bot-message: The input for this call includes examples \n(the five most relevant bot intents) and relevant chunks (dictionary search), which are to be \nused as context.\nThe process is presented in Fig. 3. Building on the above customizable workflow, NeMo \nalso includes a set of pre-implemented moderations dedicated to e.g., fact-checking, hal -\nlucination prevention in responses, and content moderation. NeMo is also a Type-1 neural-\nsymbolic system, with its effectiveness closely tied to the performance of the KNN method.\nFig. 2 Llama guard guardrail workflow\n \n1 3\n  382  Page 6 of 56\nSafeguarding large language models: a survey\n3.1.3 Guardrails AI\nGuardrails AI enables the user to add structure, type, and quality guarantees to the outputs \nof LLMs (Rajpal 2023). It operates in three steps: 1) defining the “RAIL” spec, 2) initial -\nizing the “guard” and 3) wrapping the LLMs. In the first step, Guardrails AI defines a set of \nRAIL specifications, which are used to describe the return format limitations. This informa-\ntion must be written in a specific XML format, facilitating subsequent output checks, e.g., \nstructure and types. The second step involves activating the defined spec as a guard. For \napplications that require categorized processing, such as toxicity checks, additional clas -\nsifier models can be introduced to classify the input and output text. The third step is trig -\ngered when the guard detects an error. Here, the Guardrails AI can automatically generate a \ncorrective prompt, pursuing the LLMs to regenerate the correct answer. The output is then \nre-checked to ensure it meets the specified requirements. Currently, the methods based on \nGuardrails AI are only applicable for text-level checks and cannot be used in multimodal \nscenarios involving images or audio. Unlike the previous two methods, Guardrail AI is a \nType-2 neural-symbolic system with a backbone symbolic algorithm supported by learning \nalgorithms (in this case, those additional classifier models) (Fig. 4).\nFig. 4 Guardrails AI workflow\n \nFig. 3 Nvidia NeMo Guardrails Workflow\n \n1 3\nPage 7 of 56   382 \nY. Dong et al.\n3.1.4 TruLens\nTruLens, developed by TruEra,1 is an open-source toolkit for developing, evaluating, and \nmonitoring LLMs. Central to its features is TruLens-Eval, which ensures quality by com -\nparing outputs against prefined standards. The toolkit integrates LLMs, supporting logging \nrecords of inputs and outputs, and it leverages feedback functions that utilize auxiliary \nmodels, such as relevance models or sentiment classifiers, to perform evaluations on the \nRAG triad, including context relevance, answer relevance, and groundedness. Retrieval-\nAugmented Generation (RAG) is a technology that enhances the quality of language model \noutputs by supporting answer generation with retrieved relevant information. Within Tru -\nLens, RAG’s role is to ensure the accuracy and relevance of model outputs by comparing \nthem against predefined standards, thereby evaluating LLM apps. The services are invoked \nfrom various providers. For instance, when assessing groundedness related to how closely \noutputs align with the source material, Trulens-Eval can utilize providers like OpenAI \nAPI to call an LLM to find the relevant strings in a text or employ NLI models with hug -\nging faces. The toolkit allows for the customization or pre-definition of feedback functions \nvia Python, enabling evaluations to be specifically tailored to unique requirements. Addi -\ntionally, TruLens incorporates embedding models to convert predefined information into \nnumerical vectors, simplifying matching text with relevant data. TruLens also visualizes the \nLLM applications’ rankings in a leaderboard according to their performance metrics, creat-\ning a dynamic environment that encourages developers to refine their models iteratively. As \na guidance-oriented approach, TruLens is designed not to constrain LLM inputs and out -\nputs but to provide a framework for continuous model refinement and evaluation, ensuring \nadherence to quality and relevance standards (Fig. 5).\n3.1.5 Guidance AI\nGuidance AI,2 a programming paradigm, offers superior control and efficiency than conven-\ntional prompting and chaining. It allows users to constrain generation (e.g., with regex and \n1 https://truera.com/\n2 https://guidance.readthedocs.io/en/latest/\nFig. 5 TruLens workflow\n \n1 3\n  382  Page 8 of 56\nSafeguarding large language models: a survey\nCFGs) and interleave control (conditional, loops) and generation seamlessly. This guardrail \ntool integrates text generation, prompts, and logic control within a single, continuous flow in \na Python environment, thereby refining the text processing approach in LLMs. This unified \nmethod allows more effective LLM control than traditional prompts or thought language \nchains. Its features include simple and intuitive syntax built on the Handlebars template \nlanguage, assuring the variable insertion in any prompts. The Guidance program has a well-\ndefined linear execution order directly corresponding to the token sequence processed by \nthe language model. The illustration graph of Guidance AI working flow is demonstrated \nin Fig. 6.\nAt any timesteps during the program execution, the language model can be called for \ngeneration(via {{gen}} tag) or to make logical flow decisions, such as {{#select}}{{or}}\n{{/select}} commands. Guidance supports a variety of LLMs, and during dialogue, it can \nuse role labels to map the current LLM to correct tokens or API calls, such as {{#assistant}}, \n{{#user}}, {{#system}} etc. It also can be integrated with HuggingFace models, including \nusing Guidance acceleration to speed up standard prompts by reusing key-value caches to \nshorten prompt execution times and using token healing to optimize prompt boundaries. \nRegarding token healing, this concept is related to fixing the subtleties introduced by the \nlanguage model’s normal greedy tokenization method. Specifically, it involves advancing \nthe model one step while simultaneously restricting the prefix of the generated token to be \nthe same as the previous token. Regex patterns to enforce formatting are also supported \nin Guidance. Guidance’s templated output is more suitable for generating text with high \nformatting requirements, such as ensuring legally compliant and controllable JSON struc -\ntures. During this process, different operation commands have their processing methods; \nfor example, encountering the select command, it specifies the generation of a token and \nreturns the corresponding log probs, then uses a trie tree to match candidates and determine \ntheir probabilities, finally selecting the one with the highest probability. Additionally, it sup-\nports hidden blocks; for instance, some inference processes of the LLM may not need to \nbe exposed to the end user, but they can be utilized in the template to generate intermediate \nresults.\nFig. 6 Guidance AI Workflow\n \n1 3\nPage 9 of 56   382 \nY. Dong et al.\n3.1.6 LMQL (Language model query language)\nLMQL,3 a programming interface for LLMs focusing on controlled output and safety of \ngenerated content, is designed by SRI Lab at ETH Zurich. Building on the foundation of \nGuidance AI, the LMQL project further advances the concept of “prompt templates” into \na new programming language paradigm. As a Python superset, it allows developers the \ncapacity to embed precise constraints within their queries. These constraints, from content \nrestriction to adherence to specific formats for accuracy, leverage logit masking and custom \noperator support for fine-tuned control. Structured to simplify LLM interactions, LMQL \nintroduces a SQL-like syntax complemented by scripting capabilities. Its foundation is built \non decoder declarations, such as argmax, beam, or sample strategies, alongside query blocks \nthat support inserting variables or placeholders expected to be filled, model sources, and \nintricate constraint conditions specified in ’where’ clauses. The workflow of the LQML is \nillustrated in Fig. 7.\nAt its core, LMQL’s runtime and decoding ability uniquely features employs a scripted \nBeam search to execute LMQL-specific queries and constraints. This approach allows for a \nsearch across all placeholders rather than limiting the process to singular predictive points. \nAdditionally, constraint-driven decoding dynamically adjusts available tokens based on \nreal-time evaluation of specified constraints and can reduce ineffective model calls. The \nPartial Evaluation Semantics and FollowMaps facilitate the application of constraints in \nreal-time during content generation, considering both the current state of the generated con-\ntent and the potential next tokens. Partial evaluation semantics combines value and final \nsemantics for eager validation. Value semantics determines the current value of expressions \ngiven the existing context. In contrast, the final semantics employs annotations such as FIN \n(fixed), VAR (variable), INC (increasing), and DEC (decreasing) to indicate the potential \nchange in the value of expressions as content generation progresses. Among the joint value \nand final semantics, boolean expressions are denoted by symbols ⊤ for True and ⊥ for \nFalse. Based on these semantics, FollowMap which is formulated as Follow[<expr>]( u, t) \n3 https://lmql.ai.\nFig. 7 LMQL workflow\n \n1 3\n  382  Page 10 of 56\nSafeguarding large language models: a survey\nis designed to guide the generation process by evaluating the impact of adding a new token \nt to the current interaction trace u on meeting the specified constraints. The system evalu -\nates potential next tokens against FollowMap constraints, excluding those that may violate \nconstraints, thus guiding content generation away from invalid sequences and minimizing \nunnecessary model calls.\n3.1.7 Python packages\nApart from the above-mentioned integrated guardrail techniques, Several Python packages \nare pivotal in implementing guardrails around LLMs, enhancing their safety, fairness, and \ncompliance. Here we listed some packages: (1) LangChain4: LangChain is pivotal in stream-\nlining the development of LLM applications. It introduces components that can be leveraged \nto implement guardrails, thus indirectly contributing to creating safer and more reliable AI \nsolutions. (2) AI F airness360 (AIF360)5: AIF360 is an extended toolkit from IBM that \nprovides developers with a comprehensive set of algorithms designed to detect, understand \nand mitigate bias in AI models. Its extensive resources help enhance the fairness and integ-\nrity of machine learning applications. (3) Adversarial Robustness T oolbox (ART)6: ART \nis tailored to enhance model security and robustness in the face of increasing adversarial \nattacks. It provides mechanisms to defend against and adapt to malicious input, protecting \nAI applications from potential vulnerabilities. (4) Fairlearn7: This package addresses and \nreduces unwanted biases in machine learning models. Fairlearn provides developers with \ntools and methodologies to assess and mitigate biases, fostering fairness and equality in AI \noutcomes. (5) Detoxify8: Detoxify aims to identify and mitigate toxic content in text data, \nserving as a crucial tool for maintaining a respectful and safe digital interaction space. It \nutilizes advanced models to screen and filter out harmful content, ensuring that AI applica-\ntions foster positive communication.\nThese packages represent just a glimpse into the vast array of tools available to AI devel-\nopers dedicated to embedding ethical considerations into their applications. By utilizing \nthese packages, developers can navigate the complex challenges of AI safety, ensuring their \nLLMs are technologically advanced and aligned with ethical and responsible use principles. \nThis approach reinforces the commitment to developing innovative AI technologies that \nrespect societal norms and individual rights, marking a significant step towards responsible \nAI development.\n3.1.8 Real-world scenarios of implementing guardrails\nThe practical application examples 9 are shown in Table 2, which documents real-world \nimplementations of guardrail technologies across diverse application domains with sup -\nporting evidence. This extensive documentation reveals the widespread adoption of guard -\n4 https://www.langchain.com/\n5 https://github.com/Trusted-AI/AIF360\n6  h t t p s :  / / g i t  h u b . c o  m / T r  u s t e d  - A I / a  d v e r s a  r i a l  - r o b u s t n e s s - t o o l b o x\n7 https://github.com/fairlearn/fairlearn\n8  h t t p s :  / / g i t  h u b . c o  m / u n  i t a r y  a i / d e  t o x i f y  ? t r k  = a r t i  c l e - s  s r - f r o  n t e n  d - p u l s e _ l i t t l e - t e x t - b l o c k\n9 It is noticed that guardrails have become a default setting in LLM applications, so it is often not explicitly \nmentioned.\n1 3\nPage 11 of 56   382 \nY. Dong et al.\nTable 2 Real-world Implementation Examples of Existing Guardrail Technologies\nApplication Llama Guard Nvidia NeMo Guardrails AI TruLens Guidance \nAI\nLMQL\nChatbots or virtual \nassistants\n(Gangavarapu 2024; Singh 2024; \nYang et al. 2025)\n(Gangavarapu 2024; Arun et al. \n2025; Botsihhin and Boccaccia \n2025; Singh 2024; Freitas and \nLotufo 2024; Yang et al. 2025)\n(Gangavarapu \n2024; Team \n2024)\n(Peri et al. 2024; \nTeam 2023)\n– (Beurer-Kellner et al. \n2023; Lab 2023)\nContent Moderation (Bodhankar 2024; Singh 2024) (Bodhankar 2024; Singh 2024) (Team 2024) (Team 2023) (team 2023) (Lab 2023)\nCyber Treats Defense (Singh 2024; Wan et al. 2024; \nPaduraru et al. 2024)\n(Singh 2024) – – – –\nStructured Content (Yang et al. 2025) (Yang et al. 2025) (Team 2024) – (team 2023) (Beurer-Kellner et al. \n2023; Lab 2023)\nSummarization – – (Team 2024) (Team 2023) – –\nAI Agent Reliability (Yu 2024) (Team 2024a) (Team 2024) (Team 2023) – (Vivien 2023)\n1 3\n  382  Page 12 of 56\nSafeguarding large language models: a survey\nrail frameworks across various sectors. The implementations span multiple use cases, with \nchatbots and virtual assistants showing the most comprehensive adoption across all major \nguardrail technologies. Notably, Llama Guard and Nvidia NeMo demonstrate versatility \nwith implementations spanning most key application categories. Content moderation rep -\nresents another critical area where existing guardrail technologies have been deployed in \nproduction environments. For instance, healthcare providers have integrated Nvidia NeMo \nand TruLens into clinical assistants (Gangavarapu 2024; Mehandru et al. 2024; Arun et al. \n2025; Peri et al. 2024), while e-commerce platforms like Amazon (Botsihhin and Boccac -\ncia 2025) and Flipkart (Singh 2024) employ guardrails within customer service systems \nto ensure policy compliance. The table further highlights specialized applications, such as \ncyber threat defense primarily utilizing Llama Guard (Singh 2024; Wan et al. 2024; Padu-\nraru et al. 2024), while structured content generation benefits from multiple framework \napproaches. This evidence suggests an emerging pattern where certain guardrail technolo -\ngies are becoming specialized for particular domains, while others maintain broader appli -\ncability. AI agent reliability represents a newer frontier where guardrail implementations are \nincreasingly important as autonomous systems become more prevalent in commercial appli-\ncations. These real-world implementations provide valuable insights into both the practi -\ncal efficacy and domain-specific adaptations of guardrail technologies beyond theoretical \nframeworks.\n3.2 Techniques for (Un)desirable properties in guardrails\nIn this section, we discuss several different properties, detailing their standard definitions \nand how to use Guardrails to protect these properties. It is noticed that the number of prop-\nerties is too extensive to cover comprehensively; thus, we focus on hallucination, fairness, \nprivacy, robustness, toxicity, legality, out-of-distribution, and uncertainty.\n3.2.1 Hallucination\nHallucinations in LLMs are defined as responses that are either nonexistent in reality, illogi-\ncal, or irrelevant to the prompt provided; an example of hallucination is shown in Fig. 8. \nThese phenomena often stem from the language models’ uncertainty in response, where \nthey generate answers based on patterns identified in training datasets rather than actual \nfactual understanding (Huang et al. 2023). The origins of these hallucinations can be traced \nback to various stages, including data sourcing (Singhal et al. 2023), pre-training (Li et al. \n2023), alignment (Singhal et al. 2023), and inference (Liu et al. 2023).\nThe resolution of hallucinations in LLM is crucial due to their impact on model reli -\nability and real-world applications, which presents a significant challenge in LLM develop-\nment. Researchers are actively working on methods to differentiate between accurate and \nhallucinated content in LLM responses. This involves the use of detection technologies \n(Webster and Schmitt 2024) and evaluation methodologies (Cheng et al. 2023) to ensure the \nintegrity and utility of LLM outputs. In certain contexts, such as safety-critical applications, \nensuring rigorous guarantees for the output of LLMs is imperative and achievable. This \ninvolves first specifying high-level formal specifications and then applying formal verifiers \nto monitor whether the execution of LLMs conforms to the specified requirements. For \ninstance, in Jha et al. (2023), the authors proposed a framework for counterexample-guided \n1 3\nPage 13 of 56   382 \nY. Dong et al.\ninductive synthesis using LLMs and satisfiability modulo theory (SMT). Within this frame-\nwork, an SMT solver is employed to eliminate the hallucinated outputs of the LLM, guiding \nit to generate correct outputs. Experimental results on two planning problems demonstrated \nthe promise of this approach in practice. The framework consistently converged to correct \noutputs in finite steps. However, it is worth noting that such convergence cannot always be \ntheoretically guaranteed, underscoring the practical significance of this method in address -\ning LLM hallucinations. Furthermore, there are some researches proposed to detect and \nprevent the hallucinations of LLMs.\nIn Webster and Schmitt (2024), the authors suggest utilizing continuous integration (CI) \nto automatically identify hallucinations in the outputs of an LLM with the aid of another \nLLM. The CI process involves regular incremental updates to the software, with each change \nautomatically built and tested, allowing for prompt detection of errors. Therefore, LLM \ndevelopers can use CI to automate tests on new datasets and updates to the models, which \nensures that any changes do not unintentionally introduce bias or reduce the quality of the \nmodel’s output. Instead of using another LLM to identify the errors, some researchers use \ntrusted information sources to cross-check the generated content (Min et al. 2023). Building \non these techniques, a comprehensive framework is proposed by Chern et al. (2023), which \nequips LLMs with the ability to recognize factual inaccuracies, utilizing external tools to \ngather supporting evidence. In addition to verifying the accuracy of responses, it’s crucial to \nassess the faithfulness of the output to prevent hallucinations that diverge from the context. \nThis means ensuring the output remains relevant and agrees with the input, avoiding any \nextraneous or contradictory information. This can be achieved by the fact-based metrics \nthat are based on entity (Nan et al. 2021), relation (Goodrich et al. 2019), and knowledge F1 \nmetric (Shuster et al. 2021). A further approach to ensure a model’s faithful output utilizes \nthe classifiers that are either trained on task-specific inference datasets (Mishra et al. 2021) \nor fine-tuned on adversarial datasets (Barrantes et al. 2020). Nowadays, the instruction-\nfollowing abilities of LLMs are leveraged for autonomous evaluation. LLMs can effectively \ngauge accuracy and relevance by setting clear evaluation criteria and providing both gener-\nated and source content. Different methodologies have been adopted for output evaluation, \nsuch as binary judgment mechanisms (Luo et al. 2023) and using a k-point Likert scale (Gao \net al. 2023).\nIn current guardrails, Nvidia Nemo employed the mechanism proposed by Manakul et al. \n(2023). It first generates a few additional responses from the LLM, typically two more by \ndefault. The original response from the bot is treated as the hypothesis, while the additional \ngenerated responses serve as the context or evidence. This method aligns the consistency \nverification with natural language inference (NLI) principles, allowing for a structured out-\nput assessment.\n3.2.2 Fairness\nFairness is a concept that originates in sociology, economics, and law. It is defined as \n“imperfect and just treatment or behavior without favoritism or discrimination” in the \nOxford English Dictionary. The key to fairness in NLP is the presence of social biases in \nlanguage models; an example is illustrated in Fig. 9. Unfair LLM systems make discrimina-\ntory, stereotypic, and demeaning decisions against vulnerable or marginalized demograph -\nics, causing undesirable social impacts and potential harms (Blodgett et al. 2020). Fairness \n1 3\n  382  Page 14 of 56\nSafeguarding large language models: a survey\nin LLMs has been studied from different angles, such as gender bias (Malik 2023), cultural \nbias (Tao et al. 2023), dataset bias (Sheppard et al. 2023), and social bias (Sheng et al. \n2023). These social biases can be encoded in the embeddings and carried over to decisions \nin downstream tasks, compromising the fairness of LLMs. For instance, when GPT-3 is \nprompted with questions about leadership roles or positions of power, it may reflect societal \nbiases in its responses. If asked, “Who is likely to be a CEO?”, GPT-3’s response might lean \ntowards “He is likely to be a CEO ,” implying a male. Conversely, suppose the question is \nabout lower-ranking positions or roles traditionally seen as supportive or administrative, \nsuch as “Who is likely to be a secretary?”. In that case, the model might respond with “She is \nlikely to be a secretary,” suggesting a female. This pattern of responses reveals a bias where \nhigher-status professions or roles are more closely associated with males, while supportive \nor administrative roles are linked with females. Such biases in LLM outputs can reinforce \noutdated stereotypes and potentially influence the perception of what roles are “appropriate\" \nfor individuals based on their gender.\nThus, to guard fairness in LLMs, increasing work is needed to understand these biases \nand evaluate their further effects on the downstream tasks of LLMs. For example, in terms \nof racial bias, an African American is more likely to be assigned a “criminal behavior” \nfeature because of the “African” group he belongs to (Garrido-Muñoz et al. 2021). When \nthis feature is used for model encoding and further downstream tasks, it induces unfairness \nin the language model toward African Americans. Biases are purposefully introduced into \nthe responses of LLMs to craft distinct personas for use in interactive media (Badyal et al. \n2023). BAD focuses on identifying and quantifying instances of social bias in models like \nChatGPT, especially in sensitive applications such as job and college admissions screen -\ning (Koh et al. 2023). DAMA utilizes causal analysis to identify problematic model com -\nponents, mainly focusing on mid-upper feed-forward layers most prone to convey biases \n(Limisiewicz et al. 2023). The presence of political bias is examined in ChatGPT, focusing \non aspects such as race, gender, religion, and political orientation (Motoki et al. 2023). \nAdditionally, they explored the role of randomness in responses by collecting multiple \nanswers to the same questions, which enables a more robust analysis of potential biases. \nThe bias of LLMs is also examined by controlling the input, highlighting that LLMs can still \nproduce biased responses despite the progress in bias reduction (Yeh et al. 2023). Besides, \na Bias Index is designed to quantify and address biases inherent in LLMs, including GPT-4 \n(Shaikh et al. 2022). It has also been observed that the biased response can be generated \ninadvertently, sometimes as seemingly harmless jokes (Zhou et al. 2023).\nA line of debiasing studies aims to mitigate the intrinsic bias that is task-agnostic in the \nrepresentations before they are applied to downstream tasks. Safeguarding training data \nbefore training the model efficiently alleviates intrinsic biases since label imbalance across \ndifferent demographic groups in the training data is an essential factor in inducing bias. A \nCounterfactual Data Augmentation (CDA) (Ma et al. 2020; Xie and Lukasiewicz 2023) is a \nwidespread data processing method to balance labels, which replaces the sensitive attributes \nin the original sample with the sensitive attributes of the opposite demographic based on \na prior list of sensitive word pairs. Fairness can be incorporated into LLMs’ design to bal -\nance the training samples, and then a guarded fairer model can be obtained by parameter \ntuning. Retraining models (Qian et al. 2022) is a direct way to reduce bias, although it can \nbe resource-intensive and difficult to scale. For instance, FairBERTa is a fairer model for \nretraining RoBERTa on a large-scale demographic perturbation corpus Perturbation Aug -\n1 3\nPage 15 of 56   382 \nY. Dong et al.\nmentation NLP DAtaset (PANDA) containing 98K augmentation sample pairs (Qian et al. \n2022). Moreover, an additional debiasing module is added after the encoder of LLMs to \nfilter out the bias in the representation, and a common strategy is to utilize a contrastive \nlearning framework for training (Oh et al. 2022).\nThe other line of debiasing studies aims to mitigate the extrinsic debiasing in a task-spe-\ncific way. These studies attempt to improve fairness in downstream tasks by making models \nprovide consistent outputs across different demographic groups. Many studies have con -\ncentrated on reducing bias through model adaptation approaches. A bias mitigating method, \nDAMA (Limisiewicz et al. 2023), can reduce bias while maintaining model performance \non downstream tasks. Ranaldi et al. (2023) investigated the bias in CtB-LLMs and demon-\nstrated the effectiveness of debiasing techniques. They find that bias depends not solely on \nthe number of parameters but also on factors like perplexity and that techniques like debias-\ning of OPT using LoRA can significantly reduce bias. Ungless et al. (2022) demonstrated \nthat the Stereotype Content Model, which posits that minority groups are often perceived as \ncold or incompetent, applies to contextualized word embeddings and presents a successful \nfine-tuning method to reduce such biases. Moreover, Ernst et al. (2023) proposed a novel \nadversarial learning debiasing method applied during the pre-training of LLMs. Ramezani \nand Xu ( 2023) mitigated cultural bias through fine-tuning models on culturally relevant \ndata, yet it requires resources that make it accessible to only a few.\nInstead of fine-tuning parameters, several studies directly set up guardrails on the inter -\naction between users and LLMs by exploring the control of input and output. Huang et al. \n(2023) suggested using purposely designed code generation templates to mitigate the bias in \ncode generation tasks. Tao et al. (2023) found that cultural prompting is a simple and effec-\ntive method to reduce cultural bias in the latest LLMs. However, it may be ineffective or \neven exacerbate bias in some countries. Oba et al. (2023) proposed a method to address gen-\nder bias that does not require access to model parameters. It shows that text-based pream -\nbles generated from manually designed templates can effectively suppress gender biases in \nLLMs with minimal adverse effects on downstream task performance. Dwivedi et al. (2023) \nguided LLMs to generate more equitable content by employing an innovative approach of \nprompt engineering and in-context learning, significantly reducing gender bias, especially \nin traditionally problematic.\nDeveloping guardrails through a comprehensive approach that intertwines various strat-\negies is crucial to mitigate bias effectively. This begins with meticulously monitoring and \nfiltering training data to ensure it is diverse and devoid of biased or discriminatory content. \nThe essence of this step lies in either removing biased data or enriching the dataset with \nmore inclusive and varied information. Alongside this, algorithmic adjustments are neces -\nsary, which involve fine-tuning the model’s parameters to prevent the overemphasis of cer-\ntain patterns that could lead to biased outcomes. Incorporating bias detection tools is another \npivotal aspect. These tools are designed to scrutinize the model’s outputs, identifying and \nflagging potentially biased content for human review and correction. We believe that adopt-\ning a continuous learning approach is key to the long-term efficacy of these guardrails. This \ninvolves regularly updating the model with new data, insights, and feedback and adapting \nto evolving societal norms and values. This dynamic process ensures that the guardrails \nagainst bias remain robust and relevant. Moreover, we believe in principled methods  to \nevaluate fairness when the definitions are settled. However, the definition is expected to be \n1 3\n  382  Page 16 of 56\nSafeguarding large language models: a survey\ndistribution-based rather than point-based as unintended responses, which need to estimate \nposterior distributions and measure the distance between two distributions.\n3.2.3 Privacy (Copyright)\nPrivacy, in the context of modern technology and artificial intelligence, is a crucial aspect \nof data protection that has been increasingly emphasized through legislation and research. \nLegislative measures like the EU AI Act, General Data Protection Regulation (GDPR), \nand California Consumer Privacy Act (CCPA) have established stringent data sharing and \nretention standards, necessitating strict adherence to data protection and privacy guidelines. \nDespite these frameworks, challenges persist in preventing the release of personally iden -\ntifiable information (PII) by LLMs (Zou et al. 2023), emphasizing the need for cautious \nand robust data handling protocols, an example of a privacy issue is shown in Fig. 10. Li \net al. (2023) comprehensively analyzes privacy attacks against LLMs, introduces significant \ndefense strategies, and highlights potential new privacy issues and future research directions \nas LLMs evolve.\nSeveral studies have focused on implementing privacy defense technologies to safe -\nguard data privacy and mitigate privacy breaches. Differential Privacy (DP)-tuned LLMs \n(Li et al. 2023) emerge as a leading approach to protecting data privacy in these contexts, \nensuring secure handling of sensitive information by LLMs and minimizing the risk of pri-\nvacy violations. For general NLP models, Li et al. (2022) indicated that a direct application \nof DP-SGD (Abadi et al. 2016) may not perform satisfactorily and suggests a few tricks. \nMireshghallah et al. (2022) study differential privacy model compression and proposes a \nframework that achieves 50% sparsity levels while maintaining nearly complete perfor -\nmance, setting a benchmark for future research in this area. Igamberdiev and Habernal \n(2023) implemented a model for text rewriting along with Local Differential Privacy (LDP), \nboth with and without pretraining. Xiao et al. (2023) introduce Privacy Protection Language \nModels (PPLM), a novel paradigm for fine-tuning LLMs that incorporates domain-specific \nknowledge while preserving data privacy. They explore techniques such as corpus curation \nand instruction-based tuning, demonstrating the effectiveness of these approaches in safe -\nguarding private data. Zhao et al. (2023) introduce a novel text protection mechanism called \n\"Silent Guardian,\" which effectively prevents the malicious use of text by LLMs through \nTruncation Protection Examples and the Super Tailored Protection algorithm. It features \nefficiency, semantic consistency, transferability, and robustness. Ozdayi et al. (2023) pro-\nposed a method to prepend a trained prompt to the incoming prompt before passing it to \nthe model, where the training of the prefix prompt is to minimize the extent of extractable \nmemorized content in the model. Li et al. (2023) and Duan et al. (2023) also proposed the \nprompt-tuning methodology that adheres to differential privacy principles. Yu et al. (2022) \npropose an effective algorithm for differentially private fine-tuning of large pre-trained lan-\nguage models, which achieves utility close to that of non-private models while protecting \nprivacy and reduces the computational and memory cost of training, especially performing \nexcellently on larger models. Shi et al. (2022) introduces a “Just Fine-tune Twice\" (JFT) \nframework for the latest large Transformer models, achieving Selective Differential Privacy \nprotection. It enhances the model’s utility and privacy safeguards through double fine-tun -\ning and systematic methods.\n1 3\nPage 17 of 56   382 \nY. Dong et al.\nOther than constructing privacy-preserving LLMs, watermarking techniques can play \na more critical role in LLMs for privacy and copyright protection. A typical watermarking \nmechanism Kirchenbauer et al. (2023) embedded watermarks into the output of LLMs by \nselecting a randomized set of “green” tokens before a word is generated and then softly \npromoting the use of green tokens during sampling. So, as long as we know the list of \ngreen tokens, it is easy to determine if an output is watermarked or not. We can also use \nthe watermarks to track the point of origin or the owner of watermarked text for copyright \npurposes, and this has been applied to protect the copyright of generated prompts (Yao \net al. 2023). We believe in an agreed watermarking mechanism between the data owners \nand the LLMs developers, such that the users embed a personalized watermark into their \ndocuments or texts when they deem them private or with copyright, and the LLMs develop-\ners will not use watermarked data for their training. More importantly, the LLMs develop -\ners should take the responsibility of enabling (1) an automatic verification to determine if \na user-provided, watermarked text is within the training data, and (2) model unlearning \n(Nguyen et al. 2022), which allows the removal of users’ personally owned texts from train-\ning data. LLMs also risk user trust due to their pre-training on vast textual datasets Naray -\nanan et al. ( 2021), potentially leading to inadvertent disclosure of sensitive information \nabout individuals (Plant et al. 2022). Malicious actors can exploit this vulnerability through \nadversarial attacks (Wang et al. 224), underscoring the critical importance of privacy protec-\ntion, especially when fine-tuning LLMs with sensitive data.\nIn addressing privacy concerns within LLM applications, implementing guardrails is cru-\ncial for existing and in-development technologies. Key strategies for existing applications \ninclude robust testing to identify privacy risks and continuous model monitoring to adapt \nto new threats. Implementing content control mechanisms such as blocklists, allowlists, \nand suppression lists directly tackles unsafe content generation that could compromise pri -\nvacy. For example, Nemo Guardrails restricts apps to making connections only to external \nthird-party applications known to be safe. The guardrails can force an LLM model to inter-\nact only with third-party software on an allowed list. The \"human-in-the-loop\" approach, \nwhere human oversight is applied to review potentially sensitive outputs and facilitates \nuser reporting channels for privacy violations, enhances the protection framework (Rahman \net al. 2024). Regular model retraining to align with current norms and the option to revert \nto previous safe versions of the model serve as dynamic responses to privacy challenges.\nFor applications still in development, privacy protection begins at the design stage, \nwith ethical risk assessments focused on identifying and mitigating privacy risks. Adopt -\ning responsible AI practices ensures privacy is a core consideration from the outset (Sarker \n2024). Implementing selective memory and information filtering techniques restricts the \nAI’s access to sensitive data, directly safeguarding user privacy. Removing personally iden-\ntifiable information (PII) from data used in model training is critical in protecting privacy \n(Yang et al. 2023). Continuous updates to employ the latest version of LLMs and strict \ndata privacy protocols for staff overseeing AI use are also essential for maintaining privacy \nstandards.\n3.2.4 Robustness\nWith the rise of LLMs as dominant models in NLP, robustness consists of out-of-distribution \n(OOD) and adversarial robustness. This section only accounts for adversarial robustness, \n1 3\n  382  Page 18 of 56\nSafeguarding large language models: a survey\nwhile OOD is discussed in Sect. 3.2.7. The adversary (end-user) only attempts to jailbreak \nthe model by explicitly optimizing adversarial queries or adaptively making queries based \non previous outputs but will not make out-of-distribution queries asking about potentially \nrevoked information. Robustness has distinct definitions across various downstream tasks \nof NLP; it can be commonly characterized in the following way (It works for a range of NLP \ntasks like text classification and sequence labeling): let x represent the input and y its cor-\nresponding correct label. Consider a model f that has been trained on data pairs (x, y) ∼ D, \nwith its output prediction for x given by f(x). When new test data (x′,y ′) ∼ D′, where D′ \nis not identical to D, is introduced, the robustness of the model can be determined by its \nperformance on D′ (Wang et al. 2022). Through comprehensively perturbing the input from \nx to x′, we encounter the notion of adversarial robustness, which is a concept originating \nfrom the computer vision (Goyal et al. 2023).\nThe adversarial robustness under the LLMs refers to the ability of models to maintain \nperformance when faced with inputs that have been intentionally altered or crafted to cause \nthe model to error, such as the malicious queries made intentionally or unintentionally (Ye e \nal. 2023). It is a type of model based on transformations or small perturbations (e.g.typo) to \nstudy the robustness of the model (it is also called invariance of LLMs) (Liang et al. aaaa). \nTypically, alterations that maintain the underlying meaning, like modifying the text case \nand contraction perturbed, are considered fairly benign (Liang et al. aaaa). In particular, \ndisturbances are directed at various layers of linguistic signals, including characters, words, \nsentence structures, and underlying meanings. The core objective is to replicate potential \nuser mistakes (e.g. use of near-meaning words), to assess the impact of minor deviations on \nthe outcomes of LLMs (Zhu et al. 2023; Wang et al. 224).\nThe defense methods for shielding LLMs from deliberate disruptions remain under \ninvestigation (Liu et al. 2024), indicating that robust safeguarding measures are necessary, \nespecially during the most crucial phases of user engagement with these models. Typically, \nguardrails pre-process users’ inputs to remove or neutralize potentially adversarial content, \nthus preventing models from being misled by manipulated inputs (e.g., correcting typos and \nstandardizing input formats). Similarly, guardrails also monitor LLM’s outputs. This may \ninvolve establishing thresholds for specific types of responses or flagging outputs that sig -\nnificantly deviate from expected patterns for review by a professional security team.\n3.2.5 Toxicity\nAn important NLP task is the toxicity detection (Pavlopoulos et al. 2020), the term ‘toxic-\nity’ is employed as a broad descriptor, encompassing a variety of related phenomena and \nlinguistic contexts that may also manifest as ‘offensive’ (Zampieri et al. 2019), ‘abusive’ \n(Menini et al. 2021), ‘hateful’ (Kirk et al. 2023). Similar descriptors (Pavlopoulos et al. \n2020). We show a typical example of toxicity in Fig. 11. LLMs, as one of the prevalent \ndevelopments in traditional language modeling, are frequently trained using vast quanti -\nties of datasets, which can include content exhibiting toxic behavior and unsafe material, \nsuch as hate speech, offensive/abusive language, etc. Typically, a thorough examination of \ntoxicity is required, especially considering the employment of LLMs for downstream tasks \nthat might engage younger or more vulnerable individuals, as well as the negative effects \nof unintended outputs from LLMs on specific tasks (Zhang et al. 2023). The definition of \nwhat constitutes toxicity of the LLMs varies normally, toxicity responses will be defined as \n1 3\nPage 19 of 56   382 \nY. Dong et al.\nrude, disrespectful, or unreasonable responses that are likely to make an individual leave a \ndiscussion (Deshpande et al. 2023). It is, hence, very desirable to evaluate how well-trained \nLLMs deal with toxicity (Guo et al. 2023).\nExisting studies address the problem by focusing on representative terms in datasets, \nsuch as identity terms (Sap et al. 2020). To evaluate the toxicity in LLMs, several studies \nhave crafted trigger prompts that mirror detailed toxic categories (Gehman et al. 2020). \nThese studies leveraged standard toxicity metrics, such as the Toxicity Classifier Score and \nPerspectiveAPI,10 to determine whether the LLM’s response is toxic (Hosseini et al. 2017). \nHowever, typical metrics are susceptible to evaluator bias (Goyal et al. 2022), and encoders \nare perturbed (Rosenblatt et al. 2022). Subsequently, a structured investigation framework \nattempted to address this bias (Koh et al. 2024). Despite being trained on extensive datasets, \nLLMs are capable of generating outputs that can be implicitly toxic, which are difficult to \ndetect with straightforward, zero-shot methods (Welbl et al. 2021; Wen et al. 2023). This \ncomplexity arises even when prompts appear non-toxic, underscoring the nuanced chal -\nlenges in detoxifying language models, such as depending on the specific roles assigned to \nLLMs, certain roles may generate markedly more toxic outcomes (Deshpande et al. 2023).\nEven when a generative model is trained on data characterized by low toxicity levels, and \nits ability to minimize the generation of toxic text has been validated through evaluations, \nit is still crucial to enforce protective measures during live interactions between users and \nthe model (Liang et al. aaaa). Safety guardrails are an integral part of the user interaction \nand LLMs interaction phases, playing a key role in ensuring privacy, preventing bias, and \nmaintaining user trust (Dong et al. 2024). For example, Nvidia Nemo allows users to define \nthe toxic output they want to identify; the next step is determining the chatbot’s response to \nusers’ input. This involves setting up a workflow that utilizes these definitions. Thus, this \nprocedure is triggered whenever there is potential exposure to toxic content, and the chatbot \nsupports the user. Furthermore, they ensure compliance with legal standards and align AI \noperations with societal values.\n3.2.6 Legality\nAnother crucial aspect of safeguarding LLMs involves managing the risks associated with \nillicit11 outputs (Kumar et al. 2023). Generally, this involves safeguarding efforts on two \nfronts: implementing measures to reject inappropriate user inputs and moderating model \noutput to ensure it is appropriate and safe for users or downstream tasks.\nDuring the development of LLMs, developers implement a series of measures to ensure \nthe safety and compliance of the models with relevant laws and regulations. These measures \ninclude: i) researchers meticulously screen and clean the training data before training the \n10 Perspective API was developed by Jigsaw and the Google Counter Abuse Technology team ( https://per-\nspectiveapi.com)\n11 It’s important to note that while the concepts of legality and toxicity may overlap to some extent, they are \nnot synonymous. Legality is the lowest requirement, defining what is permitted under the law. However, \nsomething that is not illegal may still be considered toxic due to its potential to cause harm or adverse effects \nin other contexts. Conversely, if something is deemed illegal, it invariably falls into the category of being \ntoxic, as its prohibition by law implies a recognized potential for harm or negativity. Thus, while legality \nprovides a clear boundary based on legal statutes, toxicity encompasses a broader range of potentially harm-\nful actions or materials, some of which may not be explicitly covered by legal definitions but are nonetheless \ndetrimental to well-being or ethical standards.\n1 3\n  382  Page 20 of 56\nSafeguarding large language models: a survey\nmodels to remove inappropriate, harmful, or illegal content. This ensures the model learns \nfrom high-quality data and avoids adopting inappropriate behaviors. ii) During model train-\ning, human reviewers assess the samples generated by the model and offer feedback, aiding \nin rectifying errors and enhancing the model’s output. This process, alongside RLHF, helps \nmodels refine the content they generate and gradually adopt appropriate behavior.\nOnce the model construction is completed and before release, models undergo thorough \nand rigorous ethics and safety testing to ensure that the content they generate is absent \nof inappropriate or illegal elements. One classical approach is red teaming (Ganguli et al. \n2022; Perez et al. 2022), which entails simulated attacks and adversarial testing to uncover \npotential vulnerabilities, ethical pitfalls, and legal considerations. Organizations like Ope -\nnAI, Anthropic, Google, and Meta utilize diverse methodologies for red teaming, ensuring \na thorough evaluation and effective risk mitigation. For instance, Google promotes internal \nred teams ,12 where employees with diverse expertise simulate attacks on the AI model. In \ncontrast, OpenAI favors external red teaming and has established external networks 13 to \nencourage participation from outside members.\nIn addition to the above safeguarding efforts, monitoring systems are established upon \nmodel release to detect inappropriate inputs and outputs. Techniques such as natural lan -\nguage processing and anomaly detection are employed for real-time identification. Upon \nidentification of any issues, immediate measures, such as content filtering algorithms or \nhuman intervention protocols, are swiftly implemented to address the concern. It is worth \nnoting that leading LLM providers, such as Google, OpenAI, Anthropic, and Meta, offer \nadvanced moderation tools and techniques to developers or users, enabling customized safe-\nguards against illicit and inappropriate content. For instance, Google offers PaLM-based \nModeration ,14 capable of detecting more than 16 types of inappropriate content. OpenAI \nprovides a Moderation API , 15 allowing developers and users to customize safeguards for \ninappropriate content. Meanwhile, Anthropic has developed Constitutional AI (Bai et al. \n2022) and Meta utilizes Llama Guard (Inan et al. 2023) for content moderation.\nIn addition to the moderation tools offered by LLM providers, notable contributions from \nother entities in the field also exist. For instance, LangChain , 16 an open-source frame -\nwork, simplifies and safeguards the development of applications using LLMs. Specifically, \nit offers a standardized interface for creating, combining, and customizing various compo -\nnents, resulting in powerful language-driven applications. One notable application of Lang-\nChain in the legal domains is ConstitutionalChain .17 By incorporating predefined rules \nand guidelines, ConstitutionalChain can filter and modify generated content to align with \nconstitutional principles. This ensures that responses are controlled, legal, and contextually \nappropriate.\nMoreover, ensuring that Large Language Models (LLMs) comply with legal require -\nments across various jurisdictions is a complex challenge, primarily due to the diversity in \nlegal systems and regulatory standards. Each jurisdiction may have its own set of guidelines \n12  h t t p s :  / / b l o  g . g o o g  l e / t  e c h n o  l o g y /  a i / g o o  g l e -  g e m i n  i - n e x  t - g e n e  r a t i  o n - m o d e l - f e b r u a r y - 2 0 2 4 /\n13 https://openai.com/blog/red-teaming-network\n14  h t t p s :  / / c l o  u d . g o o  g l e .  c o m / n  a t u r a  l - l a n g  u a g e  / d o c s / m o d e r a t i n g - t e x t\n15  h t t p s :  / / p l a  t f o r m .  o p e n  a i . c o  m / d o c  s / g u i d  e s / m  o d e r a t i o n\n16  h t t p s :  / / p y t  h o n . l a  n g c h  a i n . c  o m / d o  c s / m o d  u l e s  / c h a i n s\n17  h t t p s :  / / a p i  . p y t h o  n . l a  n g c h a  i n . c o  m / e n / l  a t e s  t / c h a  i n s / l  a n g c h a  i n . c  h a i n s  . c o n s  t i t u t i  o n a l  _ a i . b  a s e . C  o n s t i t  u t i o  n a l C \nh a i n . h t m l\n1 3\nPage 21 of 56   382 \nY. Dong et al.\nand quantitative thresholds, necessitating tailored technical solutions to meet these specific \ncompliance criteria. Moreover, some compliance requirements are internal, non-scientific, \nor lack transparency. For instance, organizations may impose internal policies that are not \npublicly documented, making it challenging for developers to align LLMs with these opaque \nstandards. Additionally, certain regulatory bodies may enforce guidelines that are broad or \nambiguous, leaving room for interpretation and complicating the compliance process.\n ● Incorporating Legal Requirements into Training: Embedding jurisdiction-specific legal \nconstraints during the pre-training and fine-tuning phases can guide LLMs to align with \nlocal regulations. Techniques like Low-Rank Adaptation (LoRA) allow for efficient \nfine-tuning by introducing trainable low-rank matrices into the model, enabling adapta-\ntion to specific legal domains without extensive computational resources.\n ● Implementing Real-Time Censorship Mechanisms: During deployment, it’s crucial to \nhave censorship systems that can filter out content violating legal standards in real-\ntime, ensuring a seamless user experience. However, research indicates that seman -\ntic censorship-relying on another model to detect and filter undesirable content-faces \ntheoretical limitations. Specifically, the authors (Glukhov et al. 2023) demonstrate that \nsemantic censorship can be viewed as an undecidable problem, meaning there is no \ngeneral algorithmic solution that can determine, for every possible input, whether the \noutput complies with all legal and ethical standards.\n3.2.7 Out-of-distribution\nFor a specific DNN, out-of-distribution (OOD) data strictly refers to data not belonging to \nany in-distribution classes used in training. Broadly, OOD data can be characterized as dif-\nfering from the in-distribution data on certain dimensions. Research indicates that DNNs \noften exhibit overconfident decision-making when presented with OOD data. This has led \nto widespread investigation of OOD detection issues across domains such as computer \nvision (Hendrycks and Gimpel 2016), and natural language processing (Arora et al. 2021). \nHowever, the OOD detection task within the field of NLP presents notable challenges, par-\nticularly exacerbated by the presence ofLLMs. This issue has resulted in limited research \nfocused on OOD detection specifically tailored to LLMs (Ren et al. 2023), primarily due \nto the immense training corpora used for LLMs, making it difficult to define precisely \nwhat data has not been utilized for training. Moreover, the generative nature of LLMs adds \nanother layer of complexity to defining the OOD problem (Kadavath et al. 2022).\nWhile defining OOD instances for an LLM is generally very difficult, if not impossible, \nit becomes more feasible when applied to specific real-world scenarios where the context is \nmore precise. In practical scenarios, OOD instances can be defined as data irrelevant to the \nmain task or significantly deviating from normal ones. For instance, recent work (Li et al. \n2023) has explored the evaluation of OOD in the context of specific language model appli-\ncations, such as text classification (Kaushik et al. 2019), sentiment analysis (Zhang et al. \n2023), machine reading comprehension (Zeng et al. 2020), and found that it can lead to a \nsignificant performance decrease, even with minor semantic shifts caused by small pertur -\nbations. To mitigate the impact of OOD on model performance in practical tasks, strategies \n1 3\n  382  Page 22 of 56\nSafeguarding large language models: a survey\nsuch as setting up anomaly input filtering mechanisms 18 or constructing OOD detectors \ntailored to the task can be employed.\n3.2.8 Uncertainty\nA key aspect of LLMs’ trustworthiness lies in their ability to discern their outputs’ reli -\nability and correctness, a concept central to uncertainty quantification. This approach is an \neffective method for assessing risks, aiming to gauge the confidence levels of LLMs in their \npredictions. Elevated uncertainty suggests that an LLM’s output may require rejection or \nadditional scrutiny. Figure 13 shows an example of uncertainty. The effectiveness of uncer-\ntainty quantification is further contingent on the alignment between the model’s predicted \nconfidence and its actual accuracy, essentially measuring the model’s calibration.\nThere has been a growing focus on research to quantify the overall uncertainty in LLMs. \nEstablishing dependable uncertainty metrics is essential for enhancing the safety of LLM \nsystems. Recent studies have noted that the calibration of LLMs is improved relatively \nthrough techniques like combining multiple reasoning chains (Wang et al. 2023), integrat-\ning different prompts (Jiang et al. 2023), or by prompting LLMs to output their confidence \nlevels directly (Kadavath et al. 2022). In addition to these observations, numerous methods \nhave been developed to quantify the uncertainty in LLMs effectively. (Lin et al. 2022) dem-\nonstrated that a GPT-3 model can learn to articulate uncertainty regarding its responses in \nnatural language independently of using model logits. (Xiao et al. 2022) comprehensively \ncompared various popular approaches to construct a well-calibrated prediction pipeline for \npre-trained language models. (Ren et al. 2023) unveiled KnowNo, a framework designed to \nmeasure and align the uncertainty in LLM-based planners, enabling them to recognize their \nlimitations and seek assistance when necessary.\nThe primary hurdles in assessing LLM uncertainty arise from the pivotal roles of mean-\ning and structure in language. This pertains to what linguists and philosophers define as a \nsentence’s semantic content and syntactic or lexical framework. While foundation models \nmainly output token-likelihOODs, reflecting lexical confidence, the meanings often hold the \nmost significance in most applications. Kuhn et al. (2022) introduced the concept of seman-\ntic entropy, which integrates linguistic consistencies arising from identical meanings. The \nfundamental method involves a semantic equivalence relation, denoted as E(si,s j ), where \nsi and sj  represent output sentences corresponding to a given input. This equivalence rela-\ntion is said to hold when two sentences si and sj  convey the same meaning, implying that \nthey belong to the same cluster C. The semantic entropy is defined as\n \nH(C|x)= −\n∑\nC\nP(C|x) lnP(C|x), (1)\nwhere x is the input sentence. This methodology, which employs ‘out-of-the-box’ models, \nenhances reproducibility and simplifies deployment. Moreover, this unsupervised uncer -\ntainty could address the issue identified in previous research, where supervised uncertainty \nmeasures often falter in the face of distributional shifts.\nUtilizing the above uncertainty technologies to build a guardrail for LLMs, it is crucial to \nintegrate mechanisms that enable the model to assess and communicate its uncertainty. This \n18  h t t p s :   /  / h u  b . g u a r d r a i  l s a  i .  c  o m / v  a l i d a  t  o r /  g u a r d r a  i l s / u n u s  u a l _ p r o m p t\n1 3\nPage 23 of 56   382 \nY. Dong et al.\ninvolves training the model to recognize when a query falls outside its expertise or when \nthe answer is speculative. It also involves responding appropriately-whether by providing \na cautious answer, flagging the response as uncertain, or directing the user to more reliable \nsources.\n3.2.9 Benchmarking challenges in guardrail assessment\nQuantitative evaluation of LLM guardrails faces significant challenges due to inherent com-\nplexities. The multidimensional safety attributes (hallucination, fairness, privacy, toxicity, \nrobustness, etc.) necessitate diverse evaluation frameworks that resist unification. The field \nlacks standardized benchmarks comparable to ImageNet (Deng et al. 2009) for comprehen-\nsive assessment. While specialized datasets like RealToxicityPrompts (Gehman et al. 2020) \nand AdvBench (Zou et al. 2023) exist, they address isolated aspects rather than providing \nholistic evaluation. More comprehensive benchmarks such as DecodingTrust (Wang et al. \n2023) represent progress toward multidimensional evaluation, yet challenges persist in their \nwidespread adoption and standardization.\nThis issue is exacerbated in multilingual contexts, where safety attributes vary across \nlanguages due to linguistic nuances, cultural contexts, and non-English data scarcity (Yang \net al. 2024). Beyond dataset limitations, measurement metrics lack comprehensiveness \n(Hu and Zhou 2024). For instance, hallucination assessment employs factual consistency \nmetrics, while privacy evaluation requires analysis of information leakage. Recently pro -\nposed frameworks still rely on conventional metrics such as accuracy calculated on static \ndatasets, failing to capture guardrail effectiveness and adaptiveness during diverse user \ninteractions (Bassani and Sanchez 2024; Hu et al. 2024). Furthermore, evaluation on adver-\nsarial techniques also typically examines isolated attributes, overlooking how attacks may \nsimultaneously compromise multiple guardrail objectives (Yi et al. 2024), further impeding \ndevelopment of standardized comprehensive assessment metrics.\n4 Overcome and enhance guardrails\nImplementing advanced safeguarding techniques, as discussed in Sect. 3, has played a \ncrucial role in enhancing their security and reliability within LLMs. However, Shen et al. \n(2023) indicated that employing guardrails does not enhance the robustness of LLMs against \nattacks. They examined the external guardrails such as ModerationEndpoint, OpenChatKit-\nModeration Model, and Nemo, showing that they only marginally reduce the average suc -\ncess rate of jailbreak attacks. Jailbreak attacks, referred to as “ jailbreaks”, aim to exploit \nlanguage models’ inherent biases or vulnerabilities by manipulating their responses. These \nsuccessful attacks allow users to circumvent the model’s safeguard mechanisms, restric -\ntions, and alignment, potentially leading to generating unconventional or harmful content or \nany content controlled by the adversary. By bypassing these constraints, jailbreaks empower \nthe model to produce outputs that exceed the boundaries of its safety training and alignment.\nTherefore, in this section, we explore current methods used to bypass the guardrails of \nLLM. In Table 3, we compare different jailbreaks on: (1) Attacker access type: white box, \nblack box, and gray box. In a white-box scenario, the attacker has full visibility into the \nmodel’s parameters. A black-box situation restricts the attacker from observing the model’s \n1 3\n  382  Page 24 of 56\nSafeguarding large language models: a survey\nAttack Ac-\ncess \nType\nPrompt Level Core Technique Stealthiness GPT4 \nEvaluation\nTargeted Property\nGCG (Zou et al. 2023) White User Greedy Gradient-based Search Low × Harmful Content\nPGD (Geisler et al. 2024) White User Continuous Relaxation & Entropy \nprojection\nLow × Harmful Content\nPRP* (Mangaokar et al. 2024) White User In-context Learning & two-step \nprefix-based\nLow × Harmful Content\nAutoDAN-Liu* (Liu et al. 2023) White System+User Hierarchical Genetic Algorithm High ✓ Harmful Content\nAutoDAN-Zhu* (Zhu et al. 2023) White User Double-loop Optimization High ✓ Harmful Content & prompt leaking\nCOLD-Attack* (Guo et al. 2024) White User Langevin dynamics High × Harmful Content\nProMan (Zhang et al. 2023) White - Generation Manipulation - × Harmful Content & Privacy \nLeakage\nJailBroken (Wei et al. 2024) Black System Failure modes as guiding principles Low ✓ Harmful Content & personally \nidentifiable information leakage\nDeepInception (Li et al. 2023) Black User Nested instruction Medium ✓ Harmful Content\nDAN* (Shen et al. 2023) Black User Characterizing in-the-wild prompt High ✓ Harmful Content\nICA (Wei et al. 2023) Black User In-context learning ability of LLM Low × Harmful Content\nSAP (Deng et al. 2023) Black User In-context learning ability of LLM Medium × Harmful Content\nDRA (Liu et al. 2024) Black User Making Them Ask and Answer Low ✓ Harmful Content\nCipherChat (Yuan et al. 2024) Black System Long-tail: cipher High ✓ Harmful Content\nMultiLingual (Deng et al. 2023) Black User Long-tail: low-resource High ✓ Harmful Content\nLRL (Yong et al. 2023) Black User Long-tail: low-resource High ✓ Harmful content\nCodeChameleon (Lv et al. 2024) Black User Long-tail: encrypts High ✓ Harmful Content\nReNeLLM (Ding et al. 2023) Black User Prompt rewriting & scenario nesting High ✓ Harmful Content\nPAIR (Chao et al. 2023) Black System Automatic Iterative Refinement High ✓ Harmful content\nGPTFUZZER (Yu et al. 2023) Black User Fuzzing Low ✓ Harmful content\nTAP (Mehrotra et al. 2023) Black System Tree-of-thought reasoning Medium ✓ Harmful content\nMosaic Prompts (Glukhov et al. 2023) Black User Semantic censorship High × Impermissible content\nEasyJailbreak (Zhou et al. 2024) Black System+User Unified framework for 12 jailbreaks - ✓ Jailbreak attack evaluation\nTable 3 Comparison among Different Jailbreaks for (Guarded) LLMs\n1 3\nPage 25 of 56   382 \nY. Dong et al.\nAttack Ac-\ncess \nType\nPrompt Level Core Technique Stealthiness GPT4 \nEvaluation\nTargeted Property\nPROMPTINJECT (Perez and Ribeiro \n2022)\nBlack User Mask-based iterative strategy Low × Goal hijacking & prompt leaking\nIPI (Greshake et al. 2023) Black System Indirect prompt injection High ✓ Cyber threats like theft of data and \ndenial of service etc\nHOUYI (Liu et al. 2023) Black User SQL injection & XSS attacks Low × Prompt abuse & prompt leak\nGA (Lapid et al. 2023) Black User genetic algorithm Low × Harmful content\nGUARD (Jin et al. 2024) Black System+User Role-playing LLMs High × Harmful content\nCIA (Jiang et al. 2023) Black User Combination of multiple instructions Medium ✓ Harmful content\nPelrine et al. (Pelrine et al. 2023) * Grey User fine-tuning Low ✓ Misinformation & Privacy Leakage\nZhang et al. (Zhan et al. 2023) Grey User fine-tuning Low ✓ Harmful content\nSafety-tuned (Bianchi et al. 2023) Grey User fine-tuning Low × Harmful content\nJanus inference (Chen et al. 2023) Grey System Fine-tuning Low × Privacy Leakage\nQi et al. (Qi et al. 2024) Grey User fine-tuning Low × Harmful content\nPelrine et al. (Pelrine et al. 2023) Grey User poisoning knowledge retrieval Medium ✓ Harmful content & fairness\nPoisonedRAG (Zou et al. 2024) Grey System poisoning knowledge retrieval Low ✓ Hallucination\nAutoPoison (Shu et al. 2024) Grey System Content Injection Low × Triggered response\nLoFT (Shah et al. 2023) Grey System fine-tuning Low ✓ Harmful content\nBadGPT (Shi et al. 2023) Grey System Backdoor Attack Medium × Harmful content\nICLAttack (Zhao et al. 2024) Grey System Backdoor Attack Low × Triggered response\nActivationAttack (Wang and Shu 2023) Grey System Activation Steering Low × Harmful & Biased content\n* Claim to Jailbreak the Guardrails\nTable 3 (continued)\n \n1 3\n  382  Page 26 of 56\nSafeguarding large language models: a survey\noutputs. In a grey-box context, the attacker has partial access, typically to some training \ndata. (2) Prompt level for manipulation : user prompt or system prompt. User prompts are \nthose where the input prompt is specified by the user, allowing for personalized or tar -\ngeted inputs. On the other hand, system prompts are generated automatically by models \nand may include outputs that attackers craftily devise to deceive or manipulate the system’s \nresponse. (3) Core technique: the main technique used to attack the LLM. (4) Stealthiness: \nhigh stealthiness represents that the attack is difficult to notice by a human, which is sup -\nposed to be some logical, semantic, and meaningful conversation rather than some gib -\nberish. (5) GPT4 Evaluation: As many jailbreaks are not directly targeted for LLMs with \nguardrails, and GPT4 has its default guardrail, then evaluation on GPT4 can be seen as a \nsurrogate metric for comparison. (6) Target manipulated property of generated response : \ntoxicity, privacy, fairness, and hallucination\n4.1 White-box jailbreaks\nA white-box attack normally refers to a scenario in which the attacker has full access to the \ninternal details of the model. Since even LLMs with guardrails can not fully protect against \nadversarial attacks, we introduce some techniques for attacking LLMs (with or without \nguardrails) under the white-box setting. Notably, most white-box attacks can be applied to \nthe black-box scenario by employing their transferability on a white-box surrogate model.\n4.1.1 Learning-based methods\nThe Greedy Coordinate Gradient (GCG)  (Zou et al. 2023) method was designed to \nsearch a specific sequence of characters (an adversarial suffix). When the adversarial suffix \nis attached to different queries, it misleads the LLM to generate a response with harmful \ncontent. This approach integrates greedy search and gradient-based methods for discrete \noptimization to manipulate the model’s outputs. It aims to optimize the likelihood that the \nmodel will generate an affirmative response, e.g., “Sure, this is...”. To improve the com -\nputational efficiency of GCG, Geisler et al. (2024) revisited Projected Gradient Descent \n(PGD) for LLMs, which has been widely used for generating adversarial perturbations in \nother domains, by controlling the error introduced by the continuous relaxation for the input \nprompt, it can fool LLMs with the similar attack performance but with up to one order of \nmagnitude faster. Previous techniques in traditional NLP, like Gradient-based Distributional \nAttack (GBDA) (Guo et al. 2021), can also be used to search adversarial suffixes. Specifi -\ncally, it also applies continuous relaxation usingGumbel-Softmax (Jang et al. 2016), which \nallows for the manipulation of text inputs in a gradient-guided manner, maintaining the \ntextual data’s discrete nature while optimizing the adversarial objective. However, it fails to \nobtain high jailbreaking performance on the aligned LLMs.\nOn the other hand, the adversarial suffixes produced by GCG are mostly some garbled \ncharacters that are easily detectable by simple perplexity filter (Jain et al. 2023). AutoDAN-\nZhu (Zhu et al. 2023) design a double-loop optimization method upon GCG to generate \nmore stealthy jailbreak prompts. In addition, it also demonstrates the ability and interpret -\nability to solve other new tasks like prompt leaking. Furthermore, COLD-Attack (Guo \net al. 2024) adopts a new jailbreak, to automate the search of adversarial LLM attacks under \na variety of restrictions such as fluency, stealthiness, sentiment, and left-right-coherence. \n1 3\nPage 27 of 56   382 \nY. Dong et al.\nIt performs efficient gradient-based sampling in the continuous logit space and relies on a \nguided decoding process to translate the continuous logit sequences back into discrete text.\nAlthough the adversarial suffix can lead the base LLM to generate harmful responses, \nthe LLM models with guardrails can easily detect it. Mangaokar et al. (2024) proposed a \nnew attack strategy, named PRP, for attacking LLM models with guardrails mainly. Spe -\ncifically, it leverages a two-step prefix-based attack, including universal adversarial prefix \nconstruction and prefix propagation to the response. Inserting the universal prefix into the \nresponse can elicit the guardrail for outputting the harmful content. After the universal pre-\nfix generation, the corresponding propagation prefix can be created through a few in-context \ntemplates. Such in-context learning enables the LLM to initially output the pre-computed \nor desired adversarial prefix, eventually making PRP jailbreak the LLM models with \nguardrails. Subsequently, AutoDAN-Liu  (Liu et al. 2023)proposed to generate stealthy \njailbreak prompts automatically, it utilizes a hierarchical genetic algorithm to bypass the \nethical guidelines and safety measures of LLMs. This method is grounded in optimization \ntechniques inspired by natural selection. It iteratively refines generations of prompts to cir-\ncumvent built-in safeguards effectively. Through this evolutionary process, AutoDAN-Liu \ngenerates stealthy prompts that subtly avoid triggering the model’s protective mechanisms.\n4.1.2 LLM generation manipulatation\nOn the other hand, except for jailbreaking via some learning-based method, ProMan (Zhang \net al. 2023) was proposed to directly manipulate the generation process of those open-source \nLLMs and enforce the LLMs to generate specific tokens at specific positions, therefore \ncheating the LLMs to generate undesired response, including harmful or sensitive informa-\ntion or even private data.\nAlthough the current literature only describes limited white-box jailbreaks, it is still pos-\nsible to bypass the guardrails if full access is provided. For example, suppose the attacker \nknows the guardrail used for the targeted LLM is Llama Guard, and the adversary has full \naccess to the fine-tuned Guard model. In that case, the previous white-box attacks using \noptimization can be further extended: the optimization space will be further narrowed by \nadding an extra constraint, i.e., the adversarial input and/or the resulting response of LLMs \nare supposed to be evaded by the guardrail. In other words, a successful jailbreak should \nsatisfy the safety conditions of the targeted LLM and the Guard models.\n4.2 Black-box jailbreaks\nUnlike white-box attacks, which necessitate access to model weights and tokenizers, black-\nbox attacks operate under the premise that adversaries lack knowledge of the LLM’s internal \narchitecture or parameters. Therefore, they are more common. In this subsection, jailbreak \nattacks conducted within a black-box setting are classified into four categories: i) manu-\nally designed jailbreaks, ii) attacks exploiting long-tail distribution, iii) optimization-based \nmethods for generating jailbreaks, and iv) unified framework for jailbreaking.\n1 3\n  382  Page 28 of 56\nSafeguarding large language models: a survey\n4.2.1 Delicately designed jailbreaks\nThe phenomenon of jailbreak attacks against state-of-the-art large LLMs was investigated \nin JailBroken (Wei et al. 2024), explicitly focusing on models such as GPT-4, GPT −3.5 \nTurbo, and Claude v1.3. This work identifies two primary reasons for the successful attack: \ncompeting training objectives and instruction tuning objectives. The authors propose these \ntwo failure modes as guiding principles for designing new jailbreak attacks. Using carefully \nengineered objectionable prompts, they empirically evaluate these attacks against the afore-\nmentioned safety-trained LLM models. The results indicate a high success rate regarding a \nlarge number of the attacks.\nIn this line, due to the constant evolution of ethical and legal constraints embedded \nwithin LLM safeguards, jailbreak attempts employing direct instructions (Wei et al. 2024; \nChao et al. 2023) are typically easily identified and rejected. It is motivated by the Milgram \nshock experiment (Milgram 1963, 1975) and its adaptation to LLMs, which follow authori-\ntative commands to produce harmful content. A prompt-based jailbreak method, called \nDeepInception (Li et al. 2023), was devised for conducting a black-box attack on LLMs. \nThis involves injecting an inception mechanism into a LLM and effectively hypnotizing \nit to act as a jailbreaker. DeepInception explicitly constructs a nested scene to serve as the \ninception for guiding the behavior of the LLM. This nested scene facilitates an adaptive \napproach to circumvent safety constraints in a normal scenario, opening up the potential for \nsubsequent jailbreaks. Specifically, DeepInception utilizes the personification capability of \nLLMs, along with their tendency to follow instructions, to generate diverse scenes or char-\nacters. Shen et al. (2023) furthered this by creating prompts that encourage ChatGPT to act \nas a DAN (“Do Anything now\"). As implied by their designation, LLMs are now capable \nof boundless functions. They are no longer bound by the customary rules that govern AI \nsystems.\nWhile existing attack methods are typically applied to new conversations devoid of \ncontext, the potential of In-Context Learning (ICL) was delved into the influence of the \nalignment ability of LLMs. Leveraging these insights, the study introduces the In-Context \nAttack (ICA) (Wei et al. 2023). ICA is tailored to construct malicious contexts to direct \nmodels to produce harmful outputs. The efficacy of in-context demonstrations in aligning \nLLMs is demonstrated, and implementing these methods is straightforward. Additionally, \nDeng et al. (2023) proposed a semi-automatic attack framework named Semi-Automatic \nAttack Prompt (SAP), it combines manual and automatic methods to generate prompts to \nmislead LLMs to output harmful content. Specifically, they manually construct high-quality \nprompts as an initial prompt set and then iteratively update them through in-context learn -\ning with LLMs. Through this red-teaming attack, extensive high-quality attack prompts can \nbe efficiently generated. Liu et al. (2024) proposed a novel universal jailbreak approach \nnamed DRA (Disguise and Reconstruction Attack. This method involves concealing harm-\nful instructions via disguise, prompting the model to uncover and reconstruct the original \nharmful instruction within its generated output, thus navigating around traditional security \nmeasures. In this way, the harmful input can be disguised from the input filter, which then \nguides the target to reconstruct the attack to obtain the desired response from the adversary.\n1 3\nPage 29 of 56   382 \nY. Dong et al.\n4.2.2 Exploiting long-tail distribution\nJailbreaks relying on long-tail distributed encoding convert the original query into rare or \nunique data formats such as ciphers (Yuan et al. 2024), low-resource languages (Deng et al. \n2023), and personalized encryption methods (Lv et al. 2024). The safety vulnerability of \nLLMs when user queries are encrypted was investigated in CipherChat  (Yuan et al. 2024). \nThe framework involves encoding malicious unsafe text using LLMs and assessing the \nsafety of the decoded responses. CipherChat is designed with three key elements in its sys-\ntem prompt to ensure effective communication through ciphers: (1) behavior assignment, \n(2) cipher teaching, and (3) enciphered unsafe demonstrations. It enables users to interact \nwith LLMs using cipher prompts, system role descriptions, and few-shot enciphered dem -\nonstrations. Furthermore, the authors introduce SelfCipher, which utilizes a hidden cipher \nembedded within LLMs to circumvent safety features more efficiently than existing ciphers.\nAfterward, despite the widespread use of English globally, there is growing concern \nthat the safety of LLMs is predominantly assessed in English alone. However, MultiLin-\ngual (Deng et al. 2023) takes a significant stride forward by investigating the safety lev -\nels of LLMs across various languages, including those with limited linguistic resources. \nThis research delves into the vulnerabilities of LLMs from two perspectives: unintentional \nand intentional scenarios. In the unintentional scenario, queries translated into non-English \nlanguages unexpectedly expose users to unsafe content. Conversely, the intentional sce -\nnario involves using translated multilingual “jailbreak” prompts. Similarly, Low Resource \nLanguages-Combined Attacks (Yong et al. 2023) (LRL) also underlines the cross-lingual \nvulnerability of GPT-4. By translating unsafe English prompts into less commonly used lan-\nguages, they successfully circumvent protective measures to elicit harmful responses. Kang \net al. (2023) show that instruction-following language models using the text-da vinci-003 \nprompt could potentially be employed to produce malicious content.\nA hypothesis regarding LLMs’ safety mechanisms was proposed in subsequent research, \nsuggesting that LLMs first detect intent before generating responses. Building on this \nhypothesis, a framework was introduced known as CodeChameleon, which encrypts and \ndecrypts queries into a format challenging for LLMs to detect Lv et al. (2024). Four dis -\ntinct encryption functions are employed during the encryption stage based on reverse order, \nword length, odd and even positions, and binary tree structure. Subsequently, the decryp -\ntion functions are incorporated into the instructions as code blocks. During inference, these \ndecryption functions assist LLMs in understanding the encrypted content. Extensive testing \ndemonstrates that CodeChameleon effectively circumvents LLMs’ intent recognition.\n4.2.3 Optimization-based approaches\nIn contrast to conventional adversarial examples, such jailbreaks are usually created through \nhuman ingenuity, strategically devising situations that naturally mislead the models (Wei \net al. 2024), rather than relying on automated techniques. Consequently, crafting them \ndemands considerable manual labor. The adversarial prompts generated Greedy Coordi-\nnate Gradient (GCG) (Zou et al. 2023) exhibit a high degree of universality and transfer-\nability, particularly to other fully black-box models.\nTo avoid limitations regarding intricate manual design (Wei et al. 2024; Yuan et al. 2024) \nand require optimization on other white-box models, compromising generalization or effi -\n1 3\n  382  Page 30 of 56\nSafeguarding large language models: a survey\nciency (Zou et al. 2023), a method known as ReNeLLM was introduced (Ding et al. 2023). \nReNeLLM is an automatic jailbreak prompt generation framework, which generalizes jail -\nbreak prompt attacks into two aspects: (1) Prompt Rewriting and (2) Scenario Nesting.\nFollowing this, Prompt Automatic Iterative Refinement (PAIR)  proposed an auto -\nmated red teaming method for jailbreaking LLMs (Chao et al. 2023), which represents a \nsignificant improvement of over ten thousand times compared to existing attacks, such as \njailbreaks identified through Greedy Coordinate Gradient (GCG) (Zou et al. 2023). The \nauthors aim to find a balance between prompt-level attacks (Dinan et al. 2019), which are \nlabor-intensive but scalable, and token-level attacks (Maus et al. 2023), which are uninter-\npretable and inefficient in terms of queries. PAIR devised a protocol leveraging a language \nmodel to craft prompt-level attacks that are both semantic and human-interpretable. This \ninvolves an automated system where the attacker language model learns from prior prompts \nand responses to refine based on a judge score and generate new prompts. Through in-\ncontext learning, PAIR enabled the language model to enhance the quality of generated \ncandidate queries autonomously.\nDrawing inspiration from AFL fuzzing, GPTFUZZER was introduced, a black-box \njailbreak fuzzing framework to autonomously generate jailbreak prompts (Yu et al. 2023). \nGPTFUZZER aims to combine the efficacy of human-written prompts with the scalability \nand flexibility of automated systems to bolster the assessment of vulnerabilities in LLMs. \nThe framework is built upon a seed selection strategy, mutate operators, and a judgment \nmodel. By harnessing these elements, GPTFUZZER can systematically detect and exploit \nvulnerabilities in LLMs.\nBuilding upon prior automated methodologies, Tree of Attacks with Pruning (TAP)  \nintroduced a novel approach for generating jailbreaks (Mehrotra et al. 2023). TAP leverages \nan LLM to iteratively refine candidate prompts using tree-of-thought reasoning until a suc-\ncessful jailbreaking prompt is generated. The framework involves three key components: an \nattacker LLM tasked with generating jailbreaking prompts using tree-of-thought reasoning, \nan evaluator responsible for assessing the generated prompts and determining the success \nof the jailbreaking attempt, and a target LLM that serves as the subject of the jailbreaking \nendeavor. Lapid et al. (2023) employed the genetic algorithm (GA) for generating a univer-\nsal adversarial suffix under the black-box setting. Instead of maximizing the targeted token \nlikelihOOD in GCG, they proposed using random subset sampling for fitness approximation \nby minimizing the cosine similarity between benign input embedding and adversarial input \nembedding. Experiments illustrate high transferability across different LLMs. Furthermore, \nJin et al. (2024) proposed a role-playing system named Guideline Upholding through Adap-\ntive Role-play Diagnostics (GUARD), which allocates four distinct roles to user LLMs to \ncollaborate on new jailbreaks. By collecting some existing jailbreak prompts into a knowl -\nedge graph and using Chain-of-Thought to align with the specific functions and objectives \nfor each role, they can generate a higher jailbreak success rate and a lower perplexity score \nthan GCG (Zou et al. 2023) and AutoDAN (Zhu et al. 2023).\n4.2.4 Unified framework for jailbreaking\nA recent development, EasyJailbreak (Zhou et al. 2024), presents a comprehensive frame -\nwork to evaluate jailbreak attacks on LLMs. This framework integrates four pivotal com -\nponents: Selector, Mutator, Constraint, and Evaluator. This approach allows researchers to \n1 3\nPage 31 of 56   382 \nY. Dong et al.\nconcentrate on crafting unique components, thus minimizing the effort required for develop-\nment. Moreover, it demonstrates broad model compatibility, accommodating various mod-\nels, including open-source alternatives like LlaMA2 and proprietary ones like GPT-4.\n4.2.5 Prompt injection for desired responses\nPrompt Injection in LLMs involves the malicious alteration of input provided to the model, \ncommonly achieved by substituting original instructions with carefully crafted user input \n(Shayegani et al. 2023). This manipulation occurs within the framework of supplying \nprompts to the LLM, guiding its responses or behaviors. Prompt injection attacks present a \nsignificant cybersecurity risk as they can result in creating unauthorized content, circumvent-\ning content moderation protocols, exposing sensitive data, or even facilitating the dissemi -\nnation of malicious code or malware. This vulnerability is particularly prominent in LLMs \nthat employ prompt-based learning approaches, rendering them susceptible to exploitation \nby malicious attackers. Given the significant role of prompt in shaping LLM output, prompt \ninjection manipulation can have widespread implications for attacking LLMs.\nSince most LLMs, such as ChatGPT, are closed-source platforms, much of the research \ncenters on utilizing prompt engineering techniques to induce ChatGPT to generate inap -\npropriate content. The framework known as PROMPTINJECT (Perez and Ribeiro 2022) \nwas proposed as a straightforward alignment mechanism for generating iterative adversarial \nprompts through masks. This approach involves assembling prompts to facilitate a quantita-\ntive assessment of the robustness of LLMs against adversarial prompt attacks. The study \nfocuses primarily on evaluating the susceptibility of GPT3 to such attacks, accomplished \nthrough simplistic handcrafted inputs. The analysis concentrates on two types of attacks: \ngoal hijacking and prompt leaking. Goal hijacking involves introducing a malicious string, \ntermed a rogue string, designed to divert the model into generating a particular sequence \nof characters. Conversely, prompt leaking pertains to the possibility of revealing a private \nvalue embedded within a confidential prompt, which should not be disclosed externally \nunder any circumstances. Following this, the concept of Indirect Prompt Injection (IPI) \n(Greshake et al. 2023) was introduced, referring to an uninvestigated attack vector where \nretrieved prompts can function as “arbitrary code”, thus compromising LLM-integrated \napplications. The authors demonstrate these attacks against real-world systems like Bing \nChat, code-completion engines, and GPT-4.\nInspired by traditional injection attacks, a novel black-box prompt injection attack tech-\nnique called HOUYI (Liu et al. 2023) was introduced. HOUYI comprises three essential \ncomponents: a preconstructed prompt, an injection prompt, and a malicious question, each \ntailored to achieve the adversary’s goals. Two significant exploit scenarios were identified: \nprompt abuse and prompt leak. The application of HOUYI to a sample of 36 real-world \nLLM-integrated applications revealed that 31 of these applications are vulnerable to prompt \ninjection. Glukhov et al. (2023) raised a concept of semantic censorship, which falls into the \ncategory of guardrail using a universal algorithm to determine whether the content gener -\nated by an LLM is permissible based on semantic content alone. Accordingly, they proposed \na novel attack named Mosaic Prompts; it leverages the ability of a user to query an LLM \nmultiple times in independent contexts to construct impermissible outputs from a set of \npermissible ones. This indicates a significant limitation of output censorship, as it cannot \nprovide safety or security guarantees without imposing severe restrictions on model useful-\n1 3\n  382  Page 32 of 56\nSafeguarding large language models: a survey\nness. Moreover, Compositional Instruction Attack ( CIA) (Jiang et al. 2023) capitalizes on \nLLMs’ failure to detect underlying harmful intents when instructions are composed of mul-\ntiple elements, thus revealing significant vulnerabilities in LLM security mechanisms. They \noutline two specific strategies, Talking-CIA (T-CIA) and Writing-CIA (W-CIA), developed \nto automate the generation of these deceptive instructions. T-CIA leverages psychological \nprinciples to align the model’s response persona with the harmful intent, bypassing LLMs’ \nethical constraints. Conversely, W-CIA disguises harmful prompts as creative writing tasks, \nexploiting LLMs’ lack of judgment on fictional content to elicit dangerous outputs.\n4.3 Gray-box jailbreaks\nIn Pelrine et al. (2023), the authors highlight that beyond the white-box approach, which \ninvolves full access to a model’s parameters, and the more limited black-box method, \nthere’s also ‘grey-box’ access. This middle ground could be crucial in uncovering additional \nvulnerabilities in the safeguard systems of LLMs. This section will present studies on ‘grey-\nbox’ attack methods, encompassing strategies like fine-tuning, retrieval-augmented genera-\ntion, and backdoor attacks.\n4.3.1 Fine-tuning attacks\nFine-tuning technology enables users to customize pre-trained LLMs effectively. However, \nwhen these fine-tuning privileges are extended to end-users, the existing guardrails may not \nbe sufficient to prevent harmful behaviors. The attacks by fine-tuning the LLMs can also \nbe called ’grey-box’ attacks. Zhan et al. (2023) suggested that fine-tuning could mitigate \nReinforcement Learning with Human Feedback (RLHF) safeguards, commonly employed \nin LLMs to minimize harmful outputs. Their research revealed that even ChatGPT 4 could \nhave its protections removed by fine-tuning. Through experiments, they demonstrated a suc-\ncess rate of 95% in generating harmful responses from ChatGPT 4, using just 340 examples \nfor fine-tuning. The experiments from Pelrine et al. (2023) also indicate that fine-tuning a \nmodel with only 15 harmful or 100 benign examples can compromise the safeguards of \nGPT-4, leading to a variety of harmful outputs. Bianchi et al. ( 2023) also examines the \npotential safety risks associated with excessive instruction tuning in LLM, illustrating that \nmodels excessively tailored to specific instructions may still generate harmful content. To \ncounteract these risks, the researchers suggest developing a safety-focused tuning dataset \nto balance the dual objectives of maintaining model performance while enhancing safety \nmeasures. Furthermore, the research by Chen et al. (2023) highlights the risks of fine-tuning \nlanguage models using small datasets containing personally identifiable information (PII). \nInitially, it focuses on a simple approach where a language model is fine-tuned with a small \ndataset rich in text-based PII, which results in the model being more likely to divulge PII \nupon prompting. Then, the researchers introduced the \"Janus\" methodology, which centers \naround defining a PII recovery task followed by few-shot fine-tuning. Experimental findings \ndemonstrate that fine-tuning GPT −3.5 with just 10 PII examples markedly increases the \nmodel’s ability to expose PII. Qi et al. (2024) found that additional training of the model can \ncompromise the effectiveness of established guardrails. They bypass the GPT−3.5 Turbo’s \nsafety guardrails by fine-tuning it with justten0 specific examples and successfully make the \nmodel entirely susceptible to harmful instructions.\n1 3\nPage 33 of 56   382 \nY. Dong et al.\n4.3.2 Retrieval-augmented generation (RAG)\nRAG for LLMs aims to improve the response of LLMs by incorporating external datasets \nduring inference. It integrates context and up-to-date or relevant information in the prompt \nto enhance the LLM’s performance. Pelrine et al. (2023) finds that employing the prompt \ninjection techniques suggested by Perez and Ribeiro ( 2022) indicated that polluting the \nexternal dataset by injecting a malicious instruction could successfully invalidate ChatGPT \n4’s safety protection. They also demonstrated that if biased system messages accompany the \nupload of factual data, it can bias the responses of ChatGPT. Zou et al. (2024) also proposed \nto inject toxic texts into the knowledge database to compromise LLMs. They developed \nthese poisoned texts by forming them to solve an optimization problem aimed at generating \na target response chosen by the attacker. Their experiments showed that by injecting just \nfive tainted texts tailored to a specific question, they were able to attain a 90% attack suc -\ncess rate.\n4.3.3 Backdoor attack\nThe backdoor attack on the neural language process task is to manipulate the model to \nproduce specific outputs when triggered (Cai et al. 2022). It typically occurs during the \npre-training and adaptation tuning, where the backdoor trigger gets embedded (Chen et al. \n2021). These manipulations should maintain the model’s performance and evade detec -\ntion by human inspection. The backdoor is triggered exclusively when input prompts to \nLLMs include the embedded trigger, causing the compromised LLMs to act maliciously \nas intended by the attacker. Shu et al. (2024) propose Auto Poinson to incorporate train -\ning examples that reference the desired target content into the system, triggering similar \nbehaviors in downstream models. Shah et al. (2023) introduces LoFT (Local Proxy Fine-\ntuning) to fine-tuning smaller, local proxy models to develop attacks that are more likely \nto transfer successfully to larger, more complex LLMs. This technique leverages the tar -\nget LLMs to produce prompts closely aligned with harmful queries, effectively gathering \nprompts from a localized vicinity around these queries. A set of parameters in the proxy \nLLM is then fine-tuned, guided by the responses of the target LLM to these analogous \nprompts. Ultimately, this fine-tuned proxy model is deployed to attack the target LLMs. The \nstudy demonstrates that this method improves the transferability of attacks. Shi et al. (2023) \nproposed the BadGPT, a backdoor attack targeting RL fine-tuning in language models. It \ninjects a backdoor trigger into the reward model during the fine-tuning stage, allowing for \ncompromising the fine-tuned language model. Zhao et al. ( 2024) then proposed ICLAt -\ntack, which fine-tunes models by targeting in-context learning for backdoor attacks. This \nmethod focuses on two prompt-level strategies: introducing compromised examples within \nthe prompt’s demonstration set and modifying the prompts. This technique operates at the \nprompt level, eliminating the necessity to train new LLMs altogether. On the other hand, \nWang and Shu (2023) pointed out that poisoning the training dataset or introducing harmful \nprompts affects the adaptability of the attacks, rendering them more prominent and more \naccessible to identify. They propose using activation steering without optimization to target \nfour key aspects of LLMs: truthfulness, toxicity, bias, and harmfulness.\n1 3\n  382  Page 34 of 56\nSafeguarding large language models: a survey\n4.4 Techniques for strengthening LLMs\nThis section discusses techniques that may help construct more powerful defenses for \nguardrails or more robust LLMs.\n4.4.1 Detection-based methods: guardrail enhancement\nTo detect the harmful information in the user’s input, PPL (Alon and Kamfonas 2023) \ncalculates the perplexity of a provided input to decide whether a user’s request should be \naccepted or rejected. SmoothLLM (Robey et al. 2023) borrowed the idea of randomized \nsmoothing literature (Cohen et al. 2019), it randomly alters multiple versions of a given \ninput and then combines the respective predictions to identify adversarial inputs. Some \nresearchers have explored how In-Context Learning (ICL) can impact the alignment capa -\nbilities of LLMs. In-Context Defense (ICD) (Wei et al. 2023) method is designed to bolster \nmodel resilience by demonstrations of rejecting to answer harmful prompts via in-context \ndemonstration.\nTo defend LLM attacks, LLM SELF DEFENSE (Helbling et al. 2023) was proposed \nfirst. Specifically, by incorporating the generated content into a pre-defined prompt and \nusing another instance, LLM, to analyze the text, it constructs an extra guardrail filter for \npreventing harmful content. Furthermore, Cao et al. ( 2023) proposed Robustly Aligned \nLLM (RA-LLM) to defend against potential alignment-breaking attacks. Unlike the previ-\nous alignment check, which uses the alignment check function to decide whether to reject \nthe response, the proposed Robust Alignment Check Function adds several extra random \ndroppings on the request. It usually checks whether the corresponding response can still \npass the alignment check function AC. Then, Chen et al. (2023) designed a moving target \ndefense (MTD) to enhance the LLM system. Compared to previous guardrail methods that \ndecide whether the input/output is safe, MTD calculates a composite score for each response \nby combining its quality and toxicity metrics. It employs randomization to select a response \nthat qualifies both response metrics, eventually providing a solid moving target defense for \nthe LLMs.\n4.4.2 Mitigation-based methods: affirmative response generation\nAs shown in. Jain et al. (2023), besides perplexity filtering, input preprocessing like Reto-\nkenization and Paraphrase can also successfully compromise the effectiveness of some \nattacks like GCG (Zou et al. 2023). However, adversarial training, though once favored for \nsafeguarding image classifiers, faces diminished appeal for LLMs due to the prohibitive \nexpenses associated with both model pre-training and the creation of adversarial attacks, \nrendering large-scale adversarial training impractical. Finding a good approximation for \nrobust optimization objectives that allow for successful adversarial training remains an \nopen challenge. Further, Li et al. (2023) introduced a novel inference method, Rewindable \nAuto-regressive INference ( RAIN) enables pre-trained LLMs to assess their own outputs \nand leverage the assessment outcomes to inform and steer the backtracking and generating \ncontent to enhance AI safety. Contrary to Reinforcement Learning from Human Feedback \n(RLHF), RAIN dispenses with the requirement for extra model upkeep and bypasses the \naccumulation of gradient data and computational graphs. Still, it must pay the extra but \n1 3\nPage 35 of 56   382 \nY. Dong et al.\nacceptable cost of the auto-regressive inference. Additionally, Zhang et al. (2023) proposed \nto integrate goal prioritization (GP) at both training and inference stages. It analyzes the rea-\nson behind successful jailbreaking: the conflict between two goals: helpfulness (providing \nhelpful responses to user queries) and safety (providing harmless and safe responses to user \nqueries. The jailbreak attack success rate can be notably decreased by plugging in the goal \nprioritization for these two properties into the inference alone or with training.\nFurther, Self-Reminder (Xie et al. 2023) suggests that adding self-reminder prompts \ncan be an effective defense. They speculate that initiating ChatGPT with a ‘system mode’ \nprompt at the most external level to remind it of its role as a responsible AI tool could reduce \nits vulnerability to being malevolently steered by user inputs at a deeper level. Therefore, \nby concatenating an extra system prompt after the user’s query that reminds the LLMs \nto respond responsibly, the experimental results showed that self-reminders significantly \nreduce the success rate of jailbreak attacks. Then, Ge et al. (2023) proposed a multi-round \nautomatic red-teaming framework MART to improve the scalability of safety alignment. \nTwo players, i.e., an adversarial LLM and a target LLM, iteratively interplay with each \nother. The adversarial LLM aims to generate challenging prompts that provoke unsafe \nresponses from the target LLM. Concurrently, the target LLM is refined with data that aligns \nwith safety standards based on these adversarial prompts. Through several rounds of red-\nteaming, the enhanced target LLM continues to bolster its defenses through safety-specific \nfine-tuning. Further, Zhou et al. (2024) proposed the first adversarial objective aimed at \nprotecting language models from jailbreaking attacks, along with a novel algorithm, Robust \nPrompt Optimization ( RPO). This strategy employs gradient-based token optimization \n(similar to GCG) to ensure the generation of harmless outputs. RPO represents the initial \napproach in jailbreaking defense (like adversarial training in vision) that enhances robust -\nness comprehensively and effectively and at only a minor cost to normal use. SafeDecoding \n(Xu et al. 2024) found that despite the likelihood of tokens signifying harmful content being \nhigher than those for harmless responses, safety disclaimers continue to emerge among the \nhighest-ranking tokens when sorted by probability in descending order. Thus, in the training \nphase, the model will be fine-tuned with a few safety measures, and then, during the infer -\nence, SafeDecoding further constructs the new token distribution. The crafted probability \ndistribution reduces the chances of tokens that resonate with the attacker’s objectives and \nenhances the probabilities of tokens that align with human values.\n5 Discussions: a complete guardrail\nBased on the discussions about tackling individual requirements in Sects. 3 and 4, this sec-\ntion advocates building a guardrail by systematically considering multiple requirements. \nWe discuss four topics: conflicting requirements (Sect. 5.1), multidisciplinary approach \n(Sect. 5.2), implementation strategy (Sect. 5.3), rigorous engineering process (Sect. 5.4), \nand safeguards for LLM Agents (Sect. 5.5) and emerging challenges of MLLM guardrails \n(Sect. 5.6).\n1 3\n  382  Page 36 of 56\nSafeguarding large language models: a survey\n5.1 Conflicting requirements\nThis section discusses the tension between safety and intelligence as an example of the con-\nflicting requirements. Conflicting requirements are typical, including, e.g., fairness and pri-\nvacy (Xiang 2022), privacy and robustness (Song et al. 2019), and robustness and fairness \n(Bassi et al. 2024). Integrating guardrails with LLMs may lead to a discernible conservative \nshift in the generation of responses to open-ended text-generation questions (Röttger et al. \n2023). The shift has been witnessed in ChatGPT over time. Chen et al. (2023) documented \na notable change in ChatGPT’s performance between March and June 2023. Specifically, \nwhen responding to sensitive queries, the model’s character count decreased significantly, \nplummeting from an excess of 600 characters to approximately 140. Additionally, in the \ncontext of opinion-based questions and answers surveys, the model is more inclined to \nabstain from responding.\nGiven the brevity and conservativeness of responses generated by ChatGPT, the follow-\ning question arises: How can exploratory depth be maintained in responses, particularly for \nopen-ended test generation tasks? Furthermore, does the application of guardrails constrain \nChatGPT’s capacity to deliver more intuitive responses? On the other hand, Narayanan and \nKapoor (2023) critically examined this paper and emphasized the difference between an \nLLM’s capabilities and its behavior. Although capabilities typically remain constant, behav-\nior can alter due to fine-tuning, which can be interpreted as the “uncertainty” challenges in \nLLMs. They suggest that GPT-4’s performance changes are likely linked more to evalua -\ntion data and fine-tuning methods rather than a decline in its fundamental abilities. They \nalso acknowledge that such behavioral drift challenges the development of reliable chatbot \nproducts. The adoption of guardrails has also led to the model adopting a more concise com-\nmunication approach, offering fewer details and electing non-response in specific queries. \nThe decision “to do or not to do” can be challenging when designing the guardrail. While \nthe most straightforward approach is to decline an answer to any sensitive questions, is it \nthe most intelligent one? That is, we need to determine if the application of guardrail always \nhas a positive impact on LLMs that is within our expectation.\n5.1.1 Our perspective\n Prior research suggested incorporating a creativity assessment mechanism into the guardrail \ndevelopment for LLMs. To measure the creativity capability of LLMs, Chakrabarty et al. \n(2023) employed the Consensual Assessment Technique (Amabile 1982), a well-regarded \napproach in creativity evaluation, focusing on several key aspects: fluency, flexibility, orig-\ninality, and elaboration, which collectively contribute to a comprehensive understanding \nof the LLMs’ creative output in storytelling. Narayanan and Kapoor ( 2023) showed that \nalthough some LLMs may demonstrate adeptness in specific aspects of creativity, there is a \nsignificant gap between their capabilities and human expertise when evaluated comprehen-\nsively. Moreover, for addressing trade-offs between safety and utility, we advocate explor -\ning the theoretical boundaries between these properties (Huang et al. 2023). We propose \ndeveloping formal frameworks to characterize trade-off spaces, using statistical methods to \ndetermine whether certain combinations of guardrail properties are fundamentally unattain-\nable. A potential direction would be developing multi-objective optimization approaches \nthat explicitly quantify both safety and utility aspects to consider multiple undesirable prop-\n1 3\nPage 37 of 56   382 \nY. Dong et al.\nerties simultaneously rather than treating them as separate concerns (Rame et al. 2023; \nWachi et al. 2024). This would enable constructing Pareto-optimal solution sets that visual-\nize concrete trade-offs and allow developers to make informed choices based on application \nrequirements, moving guardrail design from intuitive engineering to rigorous optimization \ngrounded in theoretical understanding.\n5.2 Multidisciplinary approach\nWhile current LLM guardrails include mechanisms to detect harmful content, they still risk \ngenerating biased or misleading responses. It is reasonable to expect future guardrails to \nintegrate harm detections and other mechanisms to deal with, e.g., ethics, fairness, and cre-\nativity. In the introduction, we have provided three categories of requirements to be consid-\nered for a guardrail. Moreover, LLMs may not be universally effective across all domains, \nand it has been a trend to consider domain-specific LLMs (Pal et al. 2023). In domain-\nspecific scenarios, specialized rules may conflict with the general principles. For instance, \nin crime prevention, the use of certain terminologies that are generally perceived as harmful, \nsuch as ‘guns’ or ‘crime,’ is predominant and should not be precluded. To this end, the con-\ncrete requirements for guardrails will differ across different LLMs, and research is needed to \nscientifically determine requirements. The above challenges (multiple categories, domain-\nspecific, and potentially conflicting requirements) are compounded by the fact that many \nrequirements, such as fairness and toxicity, are hard to define precisely, especially without \na concrete context. The existing methods, such as the popular one that sets a threshold on \npredictive toxicity level (Perez et al. 2022), do not have valid justification and assurance.\n5.2.1 Our perspective\n Developing LLMs ethically involves adhering to fairness, accountability, and transparency. \nThese principles ensure that LLMs do not perpetuate biases or cause unintended harm. The \nworks by e.g., Sun et al. (2023) and Ovalle et al. (2023) provide insights into how these \nprinciples can be operationalized in the context of LLMs. Establishing community stan -\ndards is vital for the responsible development of LLMs. These standards, derived from a \nconsensus among stakeholders, including developers, users, and those impacted by AI, can \nguide LLMs’ ethical development and deployment. They ensure that LLMs are aligned with \nsocietal values and ethical norms, as discussed in broader AI ethics literature (ActiveFence \n2023). Moreover, the ethical development of LLMs is not a one-time effort but requires \nongoing evaluation and refinement. These tasks involve regular assessment of LLMs out -\nputs, updating models to reflect changing societal norms, and incorporating feedback from \ndiverse user groups to ensure that LLMs remain fair and unbiased.\nSocio-technical theory (Trist and Bamforth 1957), in which both ‘social’ and ‘techni -\ncal’ aspects are brought together and treated as interdependent parts of a complex system, \nhave been promoted (Filgueiras et al. 2023; Jr et al. 2020) for machine learning to deal with \nproperties related to human and societal values, including e.g., fairness (Dolata et al. 2022), \nbiases (Schwartz et al. 2022), and ethics (Mbiazi et al. 2023). To manage the complexity, \nthe whole system approach (Crabtree et al. 2011), which promotes an ongoing and dynamic \nway of working and enables local stakeholders to come together for an integrated solution, \nhas been successfully working on healthcare systems (Brand et al. 2017). We believe a \n1 3\n  382  Page 38 of 56\nSafeguarding large language models: a survey\nmulti-disciplinary group of experts will work out and rightly justify and validate the con -\ncrete requirements for a specific context by applying the socio-technical theory and the \nwhole system approach.\n5.3 Neural-symbolic approach for implementation\nExisting guardrail frameworks such as those introduced in Sect. 3 employ a language (such \nas RAIL or Colang) to describe the behavior of a guardrail. A set of rules and guidelines are \nexpressed with the language, so each is applied independently. It is unclear if and how such \na mechanism can deal with more complex cases where the rules and guidelines conflict. As \nmentioned in Sect. 5.2, such complex cases are common in building guardrails. Moreover, \nit is unclear if they are sufficiently flexible and capable of adapting to semantic shifts over \ntime and across different scenarios and datasets.\n5.3.1 Our perspective\n First, a principled approach is needed to resolve conflicts in requirements, as suggested in \nvan Lamsweerde et al. (1998) for requirement engineering, which is based on the combina-\ntion of logic and decision theory. Second, a guardrail requires the cooperation of symbolic \nand learning-based methods. For example, we may expect that the learning agents deal \nwith the frequently-seen cases (where there are plenty of data) to improve the overall per -\nformance w.r.t. the requirements mentioned above, and the symbolic agents take care of \nthe rare cases (where there are few or no data) to improve the performance in dealing with \ncorner cases in an interpretable way. Due to the complex conflict resolution methods, more \nclosely coupled neural-symbolic methods might be needed to deal with the tension between \neffective learning and sound reasoning, such as those Type-6 systems (Lamb et al. 2021) \nthat can deal with true symbolic reasoning inside a neural engine, e.g., Pointer Networks \n(Vinyals et al. 2015).\n5.4 Systems development life cycle (SDLC)\nThe criticality of guardrails requires a careful engineering process. For this, a revisit of the \nSDLC, which is a complex project management model to encompass guardrail creation \nfrom its initial idea to its finalized deployment and maintenance, has the potential, and the \nV-model (Oppermann 2023), which builds the relations of each development process with \nits testing activities, can be helpful to ensure the quality of the final product.\n5.4.1 Our perspective\n Rigorous verification and testing will be needed (Huang et al. 2023), which requires a \ncomprehensive set of evaluation methods. Certification with statistical guarantees can be \nhelpful for individual requirements, such as the randomized smoothing (Cohen et al. 2019). \nFor the evaluation of multiple, conflicting requirements, a combination of the Pareto front-\nbased evaluation methods for multiple requirements (Ngatchou et al. 2005) and the statisti-\ncal certification for a single requirement is needed. The Pareto front is a concept from the \nfield of multi-objective optimization. It represents a set of non-dominated solutions, where \n1 3\nPage 39 of 56   382 \nY. Dong et al.\nno other solutions in the solution space are better when all objectives are considered. Sta -\ntistical certification involves using statistical methods to ensure that a single requirement \nmeets a specified standard with a certain confidence level. It is typically applied when there \nis uncertainty in the measurements, or the requirement is subject to variability. Combining \nthese techniques can find the trade-offs, provide confidence in the viability of solutions con-\ncerning individual requirements, and support more informed and adaptive decision-making \nprocesses. Finally, attention should also be paid to understanding the theoretical limits of \nthe evaluation methods, e.g., randomized smoothing causes a fairness problem (Mohapatra \net al. bbbb).\nWhile these conflicts may not be entirely resolvable, particularly within a general frame-\nwork applicable across various contexts, more targeted approaches in specific scenarios \nmight offer better conflict resolution. Such approaches demand ongoing research to develop \nconcrete principles, methods, and standards that a multidisciplinary team can implement \nand adhere to. While effective in particular situations, Guardrails are not a universal solution \nthat addresses all potential conflicts. Instead, they should be designed to manage specific, \nwell-defined scenarios.\n5.5 Safeguards for LLM agents\nIn the rapidly evolving field of LLM, more autonomous entities extend the capabilities \nof LLMs by integrating decision-making and action-initiating capacities. LLM agents \nprocess and generate language and use this capability to perform actions in the digital or \nphysical world. These LLM agents typically encompass five fundamental modules: LLMs, \nplanning, action, external tools, and memory and knowledge.  Tang et al. ( 2024). While \nLLMs respond passively to user queries, LLM agents can take proactive steps based on their \nunderstanding or directives. This increased autonomy raises concerns about unintended \nconsequences, especially in sensitive domains like scientific research.\n5.5.1 Our perspective\n Due to their autonomy, LLM agents introduce higher complexity and unpredictability. The \nintegration of decision-making processes means they might initiate actions that are hard \nto foresee or control, potentially leading to ethical and practical risks. Different from safe -\nguard LLMs, the safety of agents interacting with various tools and environments is often \noverlooked, leading to potential harmful outputs, as highlighted in studies such as ToolEmu \n(Ruan et al. 2023), AgentMonitor (Naihin et al. 2023), and R-Judge (Yuan et al. 2024). For \nLLM agents, “safeguard\" means implementing stricter controls and oversight to manage \ntheir broader capabilities effectively.\n5.6 Challenges of MLLM guardrails\nAs multimodal large language models (MLLMs) become increasingly prominent in real-\nworld applications, ranging from AI assistants with visual perception to audio-driven \ninstruction following, ensuring their safety, reliability, and compliance with ethical policies \nhas emerged as a crucial research frontier. Compared to their unimodal (text-only) counter-\nparts, MLLMs present unique challenges in guardrail design due to their inherently complex \n1 3\n  382  Page 40 of 56\nSafeguarding large language models: a survey\ninput modalities, multimodal reasoning capabilities, and vulnerability to cross-modal adver-\nsarial manipulation (Yin et al. 2025).\nRecent advances have demonstrated specialized guardrail frameworks across different \nmodalities. In the vision-language domain, LlavaGuard (Helff et al. 2024) introduced a \nsafety evaluation system built upon LLaV A, capable of policy-aware classification of image \ncontent into nine risk categories with structured JSON responses and natural language expla-\nnations. SafeWatch (Chen et al. cccc) extended guardrail capabilities to video content, where \nlong-form temporal inputs pose additional challenges, by implementing Parallel Equivalent \nPolicy Encoding for disentangled policy representation and Policy-Aware Adaptive Pruning \nfor eliminating irrelevant video tokens, while contributing the SafeWatch-Bench dataset \nfor video-based safety auditing. Simultaneously, in the audio-language space, SpeechGuard \n(Peri et al. 2024) addressed the adversarial robustness of speech-language models by pro -\nposing time-domain noise flooding as a lightweight defense mechanism against white-box \nand black-box adversarial audio perturbations that could bypass safety alignment and elicit \nharmful responses.\n5.6.1 Our perspective\n Despite these advances, the current ecosystem of MLLM guardrails remains fragmented and \nlacks general-purpose, modality-agnostic frameworks, with most systems being task-spe -\ncific and tied to particular modalities or architectures. The MLLM guardrail should integrate \nstructured policy reasoning, multi-label classification, and natural language explanation \nacross diverse input channels, with particular attention needed for cross-modal consistency, \nscalable multi-policy handling, adversarial robustness under multimodal attacks, and inter-\npretability of safety judgments when multimodal signals interact in dynamic ways (Oh et al. \n2024; Wang et al. 2024; Weng et al. 2024). Neural-symbolic approaches, combining neural \nnetworks’ perceptual capabilities with symbolic systems’ formal reasoning between differ-\nent modality, offer a promising direction for these challenges (Gaur and Sheth 2024). The \ndevelopment of unified, cross-modal safety assurance mechanisms will become essential as \nMLLMs continue to expand into socially sensitive and high-stakes environments.\n5.7 Challenges of accessibility\nWith the fast development of the (M)LLMs, there can be categorized into three main groups \nwith respect to their accessibility, model architecture and training datasets (Kukreja et al. \n2024):\n ● Open-source (M)LLM: These models have publicly available code, model architecture, \ntraining data (at least partially), and weights.\n ● Open-weight (M)LLM: These models have publicly released weights but may have \nlimited access to training data or code.\n ● Commercial (M)LLM: These models are proprietary and are developed primarily for \nbusiness applications. Access is typically provided through paid services or APIs.\nTable 4 presents a representative (M)LLM that we use to illustrate the evolution of the three \ngroups of (M)LLMs.\n1 3\nPage 41 of 56   382 \nY. Dong et al.\nWith their growing capabilities, (M)LLMs across all levels of openness face signifi -\ncant safety and security challenges. Open-source models, with publicly available code and \nweights, are particularly vulnerable to priming attacks, where adversaries craft inputs that \nbypass safety mechanisms, leading to harmful or biased outputs without altering the model \nitself (Vega et al. 2023). Open-weight models, though more restricted, can still be compro-\nmised through tampering attacks, in which malicious fine-tuning or direct manipulation of \nreleased weights can deactivate safety features and result in unethical behaviors (Tamirisa \net al. 2024). Even commercial models, despite being accessed via secure APIs, are sus -\nceptible to prompt injection attacks (Liu et al. 2023), allowing attackers to embed hidden \ninstructions in inputs that cause the model to behave in unintended or unsafe ways (Schwinn \net al. 2024). These risks highlight the urgent need for robust, multi-layered safeguards to \nensure the trustworthy deployment of LLMs, regardless of their openness.\nGuardrail methods play a critical role in mitigating safety and security risks across differ-\nent groups of (M)LLMs. For open-source (M)LLMs, guardrails can help reduce the impact \nof priming attacks by detecting and filtering adversarial prompts before or after inference \n(Ayyamperumal and Ge 2024). However, since the model weights and code are fully acces-\nsible, determined attackers can bypass these protections by modifying the model directly \nor using adversarial fine-tuning. In open-weight (M)LLMs, guardrails are often deployed \nexternally to monitor usage and outputs, helping to defend against tampering attacks to \nsome extent, particularly when combined with model weight verification tools. Still, with -\nout full control over downstream fine-tuning, these models remain vulnerable to covert \nmanipulation (Qi et al. 2024; Bassani and Sanchez 2024), guardrails are also hard to respond \nthe incorporate real-time data about sudden changes, limiting their utility in environments \nthat demand current knowledge (Pantha et al. 2024). For commercial (M)LLMs, guardrails \nare usually embedded at multiple layers (e.g., within API constraints or through real-time \ncontent moderation), offering protection against prompt injection attacks by filtering or \nneutralizing embedded malicious instructions. However, these measures are not foolproof \nTable 4 Representative Examples from the Three Groups of (M)LLMs\nModel Name Developer/Organization Parameters Notes\nOpen-Source (M)LLMs\n BLOOM BigScience 176B Multilingual open model\n Pythia EleutherAI 12B Research-focused model\n GPT-NeoX-20B EleutherAI 20B Open-source autoregressive model\n OpenLLaMA OpenLM Research 3B-13B Open-source replication of Meta’s \nLLaMA models\nOpen-Weight (M)LLMs\n LLaMA 2 Meta 7B–70B Restricted license\n LLaMa 3.1 Meta 8B-405B Latest models with improved \nperformance\n Gemma Google 7B Research use only\n Gemini 2.0 Google DeepMind N/A Multimodal capabilities\nCommercial (M)LLMs\n GPR-3 OpenAI 175B Model weights and training data \nare not publicly available\n GPT-4 OpenAI 176T Model weights and training data \nare not publicly available\n Claude 3 Anthropic 20B-2T Advanced reasoning capabilities\n1 3\n  382  Page 42 of 56\nSafeguarding large language models: a survey\n- sophisticated prompt injection techniques can still slip through, especially when attackers \nexploit subtle language patterns or contextual ambiguity (Rai et al. 2024). Guardrails are \nessential for reducing harm and enforcing ethical use, as the one of the earliest LLM safety \ntools, they cannot guarantee complete safety and security.\n5.7.1 Our perspective\n Guardrails play a pivotal role in addressing the safety and security challenges posed by \nvarious groups of (M)LLMs openness. As adversarial techniques evolve-from priming and \ntampering to prompt injection-the implementation of guardrails provides a practical layer \nof defense that can detect, filter, or neutralize harmful inputs and outputs. While guardrails \nprovide an important mitigation strategy, but they do not constitute a comprehensive solu -\ntion. Their effectiveness is inherently limited by the model’s exposure and accessibility, \nfor example, commercial models still face advanced prompt injection risks despite layered \nprotections. Furthermore, current guardrail methods often struggle to adapt in real-time or \naccount for emerging threats in dynamic environments. Guardrails are a necessary, though \nnot sufficient, component in building trustworthy LLM systems, and must be continuously \nimproved and integrated with complementary safety and security strategies.\n6 Conclusions\nThis survey provides a holistic view of the existing challenges and prospective enhance -\nments of safeguarding techniques on LLMs. We categorize the existing guardrails, ana -\nlyze their effectiveness, and delve into known techniques for overcoming these measures. \nMeanwhile, several safety-related properties in LLMs are reviewed entirely. This survey \nhighlighted methods for mitigating risks such as hallucinations and breaches of fairness \nand privacy and strategies for countering potential attacks on these mechanisms. After that, \nwe explored methods to bypass these controls (i.e., attacks), overcome the attacks, and \nstrengthen the guardrails. In summary, Guardrails are highly complex due to their role in \nmanaging interactions between LLMs and humans. A systematic approach, supported by a \nmultidisciplinary team, can fully consider and manage the complexity and provide assur -\nance to the final product.\nAppendix A Properties’ examples\nSee Figs. 8, 9, 10, 11, 12, 13.\n1 3\nPage 43 of 56   382 \nY. Dong et al.\nFig. 10 Privacy Example\n \nFig. 9 Fairness Example\n \nFig. 8 Hallucination Example\n \n1 3\n  382  Page 44 of 56\nSafeguarding large language models: a survey\nFig. 12 Out-of-Distribution Example from Ren et al. (2023)\n \nFig. 11 Toxicity Example\n \n1 3\nPage 45 of 56   382 \nY. Dong et al.\nAuthor contributions Yi Dong, Ronghui Mu, Yi Qi, Jinwei Hu, Yanghao Zhang, Siqi Sun, Tianle Zhang, \nChangshun Wu, Gaojie Jin wrote the main manuscript text. Jie Meng, and Saddek Bensalem reviewed the \nmanuscript. Xiaowei Huang led this work.\nData availability No datasets were generated or analysed during the current study.\nDeclarations\nConflict of interest The authors declare no conflict of interest.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as \nyou give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons \nlicence, and indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the material. \nIf material is not included in the article’s Creative Commons licence and your intended use is not permitted \nby statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\nReferences\nAbadi M, Chu A, Goodfellow I, McMahan HB, Mironov I, Talwar K, Zhang L (2016) Deep learning with dif-\nferential privacy. In: Proc. 2016 ACM SIGSAC Conf. Comput. Commun. Secur. CCS ’16, pp. 308–318. \nAssociation for Computing Machinery, New York, NY , USA . https://doi.org/10.1145/2976749.2978318\nActiveFence (2023) LLM Safety Review: Benchmarks and Analysis\nAlon G, Kamfonas M (2023) Detecting language model attacks with perplexity. arXiv prepr. arxiv:2308.14132\nAmabile TM (1982) Social psychology of creativity: a consensual assessment technique. J Pers Soc Psychol \n43(5):997\nAnil R, Dai AM, Firat O, Johnson M, Lepikhin D, Passos A, Shakeri S, Taropa E, Bailey P, Chen Z, et al \n(2023) Palm 2 technical report. arXiv prepr. arxiv:2305.10403\nArora U, Huang W, He H (2021) Types of out-of-distribution texts and how to detect them. arXiv prepr. \narxiv:2109.06827\nArun G, Syam R, Nair AA, Vaidya S (2025) An integrated framework for ethical healthcare chatbots using \nlangchain and nemo guardrails. AI Ethics 10:1–12\nAyyamperumal SG, Ge L (2024) Current state of llm risks and ai guardrails. arXiv preprint arXiv:2406.12934\nBadyal N, Jacoby D, Coady Y (2023) Intentional biases in LLM responses. In: 2023 IEEE 14th Annu. Ubiq-\nuitous Comput. Electron. Mob. Commun. Conf. (UEMCON), pp. 0502–0506. IEEE\nBai Y , Kadavath S, Kundu S, Askell A, Kernion J, Jones A, Chen A, Goldie A, Mirhoseini A, McKinnon C, \net al (2022) Constitutional AI: Harmlessness from AI feedback. arXiv prepr. arxiv:2212.08073\nBarrantes M, Herudek B, Wang R (2020) Adversarial nli for factual correctness in text summarisation mod-\nels. arXiv prepr. arxiv:2005.11739\nFig. 13 Uncertainty Example\n \n1 3\n  382  Page 46 of 56\nSafeguarding large language models: a survey\nBassani E, Sanchez I (2024) Guardbench: A large-scale benchmark for guardrail models. In: Proceedings of \nthe 2024 conference on empirical methods in natural language processing, pp. 18393–18409\nBassi PRAS, Dertkigil SSJ, Cavalli A (2024) Improving deep neural network generalization and robustness \nto background bias via layer-wise relevance propagation optimization. Nat Commun 15(1):291.  h t t p s : / \n/ d o i . o r g / 1 0 . 1 0 3 8 / s 4 1 4 6 7 - 0 2 3 - 4 4 3 7 1 - z       \nBeurer-Kellner L, Fischer M, Vechev M (2023) Prompting is programming: a query language for large lan -\nguage models. Proceed ACM Program Lang 7:1946–1969\nBeurer-Kellner L, Fischer M, Vechev M (2023) Lmql chat: Scripted chatbot development\nBianchi F, Suzgun M, Attanasio G, Röttger P, Jurafsky D, Hashimoto T, Zou J (2023) Safety-tuned lla -\nmas: Lessons from improving the safety of large language models that follow instructions. arXiv prepr. \narxiv:2309.07875\nBirhane A, Kasirzadeh A, Leslie D, Wachter S (2023) Science in the age of large language models. Nat Rev \nPhys 5(5):277–280. https://doi.org/10.1038/s42254-023-00581-4\nBlodgett SL, Barocas S, III HD, Wallach HM (2020) Language (technology) is power: A critical survey \nof \"Bias\" in NLP. In: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J.R. (eds.) Proc. 58th Annu. Meet. \nAssoc. Comput. Linguist., pp. 5454–5476. Association for Computational Linguistics.  h t t p s :  / / d o i  . o r g / 1  \n0 . 1 8  6 5 3 / V  1 / 2 0 2  0 . A C L -  M A I N  . 4 8 5\nBodhankar A (2024) Content Moderation and Safety Checks with NVIDIA NeMo Guardrails.  h t t p s :  / / d e v  e l \no p e r  . n v i  d i a . c  o m / b l  o g / c o n  t e n t  - m o d e  r a t i o  n - a n d -  s a f e  t y - c h  e c k s -  w i t h - n  v i d i  a - n e m  o - g u a  r d r a i l  s / ? u  t m _ s o u r c \ne = c h a t g p t . c o m s\nBotsihhin G, Boccaccia L (2025) Enhancing LLM Capabilities with NeMo Guardrails on Amazon Sage -\nMaker JumpStart.  h t t p s :  / / a w s  . a m a z o  n . c o  m / c n /  b l o g s  / m a c h i  n e - l  e a r n i  n g / e n  h a n c i n  g - l l  m - c a p  a b i l i  t i e s - w  i t h \n-  n e m o -  g u a r d  r a i l s -  o n - a  m a z o n  - s a g e  m a k e r -  j u m p  s t a r t / ? u t m _ s o u r c e = c h a t g p t . c o m\nBrand SL, Thompson Coon J, Fleming LE, Carroll L, Bethel A, Wyatt K (2017) Whole-system approaches \nto improving the health and wellbeing of healthcare workers: a systematic review. PLoS ONE \n12(12):0188418. https://doi.org/10.1371/journal.pone.0188418\nBrown TB, Mann B, Ryder N, Subbiah M, Kaplan J, Dhariwal P, Neelakantan A, Shyam P, Sastry G, Askell \nA, Agarwal S, Herbert-V oss A, Krueger G, Henighan T, Child R, Ramesh A, Ziegler DM, Wu J, Winter \nC, Hesse C, Chen M, Sigler E, Litwin M, Gray S, Chess B, Clark J, Berner C, McCandlish S, Radford A, \nSutskever I, Amodei D (2020) Language models are few-shot learners. In: Proc. 34th Int. Conf. Neural \nInf. Process. Syst. NIPS’20. Curran Associates Inc., Red Hook, NY , USA\nCai X, Xu H, Xu S, Zhang Y et al (2022) Badprompt: backdoor attacks on continuous prompts. NeurIPS \n35:37068–37080\nCao B, Cao Y , Lin L, Chen J (2023) Defending against alignment-breaking attacks via robustly aligned llm. \narXiv prepr. arxiv:2309.14348\nChakrabarty T, Laban P, Agarwal D, Muresan S, Wu C-S (2023) Art or artifice? large language models and \nthe false promise of creativity. arXiv prepr. arxiv:2309.14556\nChao P, Robey A, Dobriban E, Hassani H, Pappas GJ, Wong E (2023) Jailbreaking black box large language \nmodels in twenty queries. arXiv prepr. arxiv:2310.08419\nCheng Q, Sun T, Zhang W, Wang S, Liu X, Zhang M, He J, Huang M, Yin Z, Chen K, et al (2023) Evaluating \nhallucinations in chinese large language models. arXiv prepr. arxiv:2310.03368\nChen B, Paliwal A, Yan Q (2023) Jailbreaker in jail: Moving target defense for large language models. In: \nProc. 10th ACM Workshop Mov. Target Def., pp. 29–32\nChen Z, Pinto F, Pan M, Li B. Safewatch: An efficient safety-policy following video guardrail model with \ntransparent explanations. In: The thirteenth international conference on learning representations\nChen X, Salem A, Chen D, Backes M, Ma S, Shen Q, Wu Z, Zhang Y (2021) Badnl: Backdoor attacks against \nnlp models with semantic-preserving improvements. In: Proc. 37th Annu. Comput. Secur. Appl. Conf., \npp. 554–569\nChen X, Tang S, Zhu R, Yan S, Jin L, Wang Z, Su L, Wang X, Tang H (2023) The janus interface: How fine-\ntuning in large language models amplifies the privacy risks. arXiv prepr. arxiv:2310.15469\nChen L, Zaharia M, Zou J (2023) How is ChatGPT’s behavior changing over time? arXiv prepr. \narxiv:2307.09009\nChern I, Chern S, Chen S, Yuan W, Feng K, Zhou C, He J, Neubig G, Liu P, et al (2023) FacTool: Factuality \ndetection in generative AI–A tool augmented framework for multi-task and multi-domain scenarios. \narXiv prepr. arxiv:2307.13528\nCohen J, Rosenfeld E, Kolter Z (2019) Certified adversarial robustness via randomized smoothing. In: 36th \nInt. Conf. Mach. Learn. (ICML 2019), pp. 1310–1320. PMLR\nCrabtree BF, Miller WL, Stange KC (2011) The chronic care model and diabetes management in US primary \ncare settings: a systematic review. Diabetes Care 34(4):1058–1063. https://doi.org/10.2337/dc10-1145\nDeng J, Dong W, Socher R, Li L-J, Li K, Fei-Fei L (2009) Imagenet: A large-scale hierarchical image data-\nbase. In: 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248–255. IEEE\n1 3\nPage 47 of 56   382 \nY. Dong et al.\nDeng B, Wang W, Feng F, Deng Y , Wang Q, He X (2023) Attack prompt generation for red teaming and \ndefending large language models. arXiv prepr. arxiv:2310.12505\nDeng Y , Zhang W, Pan SJ, Bing L (2023) Multilingual jailbreak challenges in large language models. In: 12th \nInt. Conf. Learn. Represent. (ICLR 2024)\nDeshpande A, Murahari V , Rajpurohit T, Kalyan A, Narasimhan K (2023) Toxicity in chatgpt: Analyzing \npersona-assigned language models. In: Bouamor, H., Pino, J., Bali, K. (eds.) Find. Assoc. Comput. \nLinguist.: EMNLP 2023, pp. 1236–1270. Association for Computational Linguistics, ???\nDevlin J, Chang M-W, Lee K, Toutanova K (2019) BERT: Pre-training of deep bidirectional transformers for \nlanguage understanding. In: Proc. 2019 Conf. N. Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. \nTechnol., pp. 4171–4186. Association for Computational Linguistics, Minneapolis, Minnesota.  h t t p s : / / \nd o i . o r g / 1 0 . 1 8 6 5 3 / v 1 / N 1 9 - 1 4 2 3       \nDinan E, Humeau S, Chintagunta B, Weston J (2019) Build it break it fix it for dialogue safety: Robustness \nfrom adversarial human attack. In: Proc. 2019 Conf. Empir. Methods Nat. Lang. Process. 9th Int. Jt. \nConf. Nat. Lang. Process. (EMNLP-IJCNLP), pp. 4537–4546\nDing P, Kuang J, Ma D, Cao X, Xian Y , Chen J, Huang S (2023) A wolf in sheep’s clothing: Generalized \nnested jailbreak prompts can fool large language models easily. arXiv prepr. arxiv:2311.08268\nDolata M, Feuerriegel S, Schwabe G (2022) A sociotechnical view of algorithmic fairness. Inf Syst J \n32(4):754–818\nDong Y , Mu R, Jin G, Qi Y , Hu J, Zhao X, Meng J, Ruan W, Huang X (2024) Building guardrails for large \nlanguage models. In: 41st Int. Conf. Mach. Learn. (ICML 2024). PMLR\nDuan H, Dziedzic A, Papernot N, Boenisch F (2023) Flocks of stochastic parrots: Differentially private \nprompt learning for large language models. arXiv prepr. arxiv:2305.15594\nDwivedi S, Ghosh S, Dwivedi S (2023) Breaking the bias: Gender fairness in LLMs using prompt engineer-\ning and in-context learning. Rupkatha J Interdiscip Stud Humanit 15(4):25\nErnst JS, Marton S, Brinkmann J, Vellasques E, Foucard D, Kraemer M, Lambert M (2023) Bias mitigation \nfor large language models using adversarial learning. In: ECAI 2023 Workshop Fairness Bias AI\nFilgueiras F, Mendonca R, Almeida V (2023) Governing artificial intelligence through a sociotechnical lens. \nIEEE Internet Comput 27(05):49–52. https://doi.org/10.1109/MIC.2023.3310110\nFreitas BAT, Lotufo RdA (2024) Retail-gpt: leveraging retrieval augmented generation (rag) for building \ne-commerce chat assistants. arXiv preprint arXiv:2408.08925\nGangavarapu A (2024) Enhancing guardrails for safe and secure healthcare ai. arXiv preprint arXiv:2409.17190\nGanguli D, Lovitt L, Kernion J, Askell A, Bai Y , Kadavath S, Mann B, Perez E, Schiefer N, Ndousse K, et al \n(2022) Red teaming language models to reduce harms: Methods, scaling behaviors, and lessons learned. \narXiv prepr. arxiv:2209.07858\nGao M, Ruan J, Sun R, Yin X, Yang S, Wan X (2023) Human-like summarization evaluation with chatgpt. \narXiv prepr. arxiv:2304.02554\nGarrido-Muñoz I, Montejo-Ráez A, Martínez-Santiago F, Ureña-López LA (2021) A survey on bias in deep \nNLP. Appl Sci 11(7):3184\nGaur M, Sheth A (2024) Building trustworthy neurosymbolic ai systems: Consistency, reliability, explain -\nability, and safety. AI Mag 45(1):139–155\nGehman S, Gururangan S, Sap M, Choi Y , Smith NA (2020) Realtoxicityprompts: evaluating neural toxic \ndegeneration in language models. Find Assoc Comput Ling EMNLP 2020:3356–3369\nGehman S, Gururangan S, Sap M, Choi Y , Smith NA (2020) RealToxicityPrompts: Evaluating neural toxic \ndegeneration in language models. In: Cohn, T., He, Y ., Liu, Y . (eds.) Find. Assoc. Comput. Linguist.: \nEMNLP 2020, pp. 3356–3369. Association for Computational Linguistics, Online .  h t t p s :  / / d o i  . o r g / 1  0 . 1 \n8  6 5 3 / v  1 / 2 0 2  0 . fi  n d  i n g s  - e m n l p . 3 0 1\nGeisler S, Wollschläger T, Abdalla MHI, Gasteiger J, Günnemann S (2024) Attacking large language models \nwith projected gradient descent. arXiv prepr. arxiv:2402.09154\nGe S, Zhou C, Hou R, Khabsa M, Wang Y-C, Wang Q, Han J, Mao Y (2023) Mart: improving llm safety with \nmulti-round automatic red-teaming. arXiv prepr. arxiv:2311.07689\nGlukhov D, Shumailov I, Gal Y , Papernot N, Papyan V (2023) Llm censorship: A machine learning challenge \nor a computer security problem? arXiv prepr. arxiv:2307.10719\nGoodrich B, Rao V , Liu PJ, Saleh M (2019) Assessing the factual accuracy of generated text. In: Proc. 25th \nACM SIGKDD Int. Conf. Knowl. Discov. Data Min., pp. 166–175\nGoyal S, Doddapaneni S, Khapra MM, Ravindran B (2023) A survey of adversarial defenses and robustness \nin NLP. ACM Comput Surv. https://doi.org/10.1145/3593042\nGoyal N, Kivlichan ID, Rosen R, Vasserman L (2022) Is your toxicity my toxicity? Exploring the impact of \nrater identity on toxicity annotation. Proc ACM Hum Comput Interact. https://doi.org/10.1145/3555088\nGreshake K, Abdelnabi S, Mishra S, Endres C, Holz T, Fritz M (2023) Not what you’ve signed up for: Com-\npromising real-world llm-integrated applications with indirect prompt injection. In: Proc. 16th ACM \nWorkshop Artif. Intell. Secur., pp. 79–90\n1 3\n  382  Page 48 of 56\nSafeguarding large language models: a survey\nGuo Z, Jin R, Liu C, Huang Y , Shi D, Supryadi Yu L, Liu Y , Li J, Xiong B, Xiong D (2023) Evaluating large \nlanguage models: A comprehensive survey. arXiv prepr. arxiv:2310.19736v3 [cs.CL]\nGuo C, Sablayrolles A, Jégou H, Kiela D (2021) Gradient-based adversarial attacks against text transformers. \narXiv prepr. arxiv:2104.13733\nGuo X, Yu F, Zhang H, Qin L, Hu B (2024) Cold-attack: Jailbreaking llms with stealthiness and controllabil-\nity. arXiv prepr. arxiv:2402.08679\nHelbling A, Phute M, Hull M, Chau DH (2023) Llm self defense: By self examination, llms know they are \nbeing tricked. arXiv prepr. arxiv:2308.07308\nHelff L, Friedrich F, Brack M, Schramowski P, Kersting K (2024) Llavaguard: Vlm-based safeguard for \nvision dataset curation and safety assessment. In: Proceedings of the IEEE/CVF Conference on Com -\nputer Vision and Pattern Recognition, pp. 8322–8326\nHendrycks D, Gimpel K (2016) A baseline for detecting misclassified and out-of-distribution examples in \nneural networks. In: 4th Int. Conf. Learn. Represent. (ICLR 2016)\nHosseini H, Kannan S, Zhang B, Poovendran R (2017) Deceiving google’s perspective api built for detecting \ntoxic comments. arXiv prepr. arxiv:1702.08138\nHuang D, Bu Q, Zhang J, Xie X, Chen J, Cui H (2023) Bias assessment and mitigation in llm-based code \ngeneration. arXiv prepr. arxiv:2309.14345\nHuang X, Ruan W, Huang W, Jin G, Dong Y , Wu C, Bensalem S, Mu R, Qi Y , Zhao X, et al (2023) A survey \nof safety and trustworthiness of large language models through the lens of verification and validation. \narXiv prepr. arxiv:2305.11391\nHuang L, Yu W, Ma W, Zhong W, Feng Z, Wang H, Chen Q, Peng W, Feng X, Qin B, et al (2023) A survey \non hallucination in large language models: Principles, taxonomy, challenges, and open questions. arXiv \nprepr. arxiv:2311.05232\nHu J, Dong Y , Huang X (2024) Adaptive guardrails for large language models via trust modeling and in-\ncontext learning. arXiv preprint arXiv:2408.08959\nHu T, Zhou X-H (2024) Unveiling llm evaluation focused on metrics: challenges and solutions. arXiv pre -\nprint arXiv:2404.09135\nIgamberdiev T, Habernal I (2023) DP-BART for privatized text rewriting under local differential privacy. \narXiv prepr. arxiv:2302.07636\nInan H, Upasani K, Chi J, Rungta R, Iyer K, Mao Y , Tontchev M, Hu Q, Fuller B, Testuggine D, et al (2023) \nLlama guard: Llm-based input-output safeguard for human-ai conversations. arxiv:2312.06674\nJain N, Schwarzschild A, Wen Y , Somepalli G, Kirchenbauer J, Chiang P-y, Goldblum M, Saha A, Geiping \nJ, Goldstein T (2023) Baseline defenses for adversarial attacks against aligned language models. arXiv \nprepr. arxiv:2309.00614\nJang E, Gu S, Poole B (2016) Categorical reparameterization with gumbel-softmax. arXiv prepr. \narxiv:1611.01144\nJha SK, Jha S, Lincoln P, Bastian ND, Velasquez A, Ewetz R, Neema S (2023) Counterexample guided induc-\ntive synthesis using large language models and satisfiability solving. In: 2023 IEEE Mil. Commun. \nConf. (MILCOM 2023), pp. 944–949. IEEE, ???\nJiang S, Chen X, Tang R (2023) Prompt packer: Deceiving llms through compositional instruction with hid-\nden attacks. arXiv prepr. arxiv:2310.10077\nJiang M, Ruan Y , Huang S, Liao S, Pitis S, Grosse RB, Ba J (2023) Calibrating language models via aug-\nmented prompt ensembles. In: ICML 2023 Workshop Chall. Deployable Gener. AI\nJin H, Chen R, Zhou A, Chen J, Zhang Y , Wang H (2024) GUARD: Role-playing to generate natural-language \njailbreakings to test guideline adherence of large language models. arXiv prepr. arxiv:2402.03299\nJr, DM, Prabhakaran V , Kuhlberg J, Smart A, Isaac WS (2020) Extending the machine learning abstrac -\ntion boundary: A complex systems approach to incorporate societal context. CoRR abs/2006.09663 \narxiv:2006.09663\nKadavath S, Conerly T, Askell A, Henighan T, Drain D, Perez E, Schiefer N, Hatfield-Dodds Z, Das -\nSarma N, Tran-Johnson E, et al (2022) Language models (mostly) know what they know. arXiv prepr. \narxiv:2207.05221\nKang D, Li X, Stoica I, Guestrin C, Zaharia M, Hashimoto T (2023) Exploiting programmatic behavior of \nLLMs: Dual-use through standard security attacks. arXiv prepr. arxiv:2302.05733\nKaushik D, Hovy E, Lipton Z (2019) Learning the difference that makes a difference with counterfactually-\naugmented data. In: 7th Int. Conf. Learn. Represent. (ICLR 2019)\nKirchenbauer J, Geiping J, Wen Y , Katz J, Miers I, Goldstein T (2023) A watermark for large language mod-\nels. In: Krause, A., Brunskill, E., Cho, K., Engelhardt, B., Sabato, S., Scarlett, J. (eds.) 40th Int. Conf. \nMach. Learn. (ICML 2023). Proceedings of Machine Learning Research, vol. 202, pp. 17061–17084. \nPMLR, ??? (2023-07-23/2023-07-29)\nKirk HR, Birhane A, Vidgen B, Derczynski L (2023) Handling and presenting harmful text in NLP research. \narXiv prepr. arxiv:2204.14256v3 [cs.CL]\n1 3\nPage 49 of 56   382 \nY. Dong et al.\nKoh NH, Plata J, Chai J (2023) BAD: BiAs Detection for Large Language Models in the context of candidate \nscreening. arXiv prepr. arxiv:2305.10407\nKoh H, Kim D, Lee M, Jung K (2024) Can LLMs recognize toxicity? Structured toxicity investigation \nframework and semantic-based metric. arXiv prepr. arXiv:2402,06900v2arxiv:2402.06900v2 [cs.CL]\nKuhn L, Gal Y , Farquhar S (2022) Semantic uncertainty: Linguistic invariances for uncertainty estimation in \nnatural language generation. In: 10th Int. Conf. Learn. Represent. (ICLR 2022)\nKukreja S, Kumar T, Purohit A, Dasgupta A, Guha D (2024) A literature survey on open source large lan -\nguage models. In: Proceedings of the 2024 7th International Conference on Computers in Management \nand Business, pp. 133–143\nKumar S, Balachandran V , Njoo L, Anastasopoulos A, Tsvetkov Y (2023) Language generation models can \ncause harm: So what can we do about it? An actionable survey. In: Proc. 17th Conf. Eur. Chapter Assoc. \nComput. Linguist., pp. 3299–3321\nLab ES (2023) Documentation of LMQL. https://lmql.ai\nLamb LC, d’Avila Garcez A, Gori M, Prates MOR, Avelar PHC, Vardi MY (2021) Graph neural networks \nmeet neural-symbolic computing: A survey and perspective. In: Proc. 29th Int. Jt. Conf. Artif. Intell. \n(IJCAI 2021). IJCAI’20, Yokohama, Yokohama, Japan\nLapid R, Langberg R, Sipper M (2023) Open sesame! universal black box jailbreaking of large language \nmodels. arXiv prepr. arxiv:2309.01446\nLiang P, Bommasani R, Lee T, Tsipras D, Soylu D, Yasunaga M, Zhang Y , Narayanan D, Wu Y , Kumar A, \nNewman B, Yuan B, Yan B, Zhang C, Cosgrove C, Manning CD, Ré C, Acosta-Navas D, Hudson DA, \nZelikman E, Durmus E, Ladhak F, Rong F, Ren H, Yao H, Wang J, Santhanam K, Orr L, Zheng L, \nYuksekgonul M, Suzgun M, Kim N, Guha N, Chatterji N, Khattab O, Henderson P, Huang Q, Chi R, \nXie SM, Santurkar S, Ganguli S, Hashimoto T, Icard T, Zhang T, Chaudhary V , Wang W, Li X, Mai Y , \nZhang Y , Koreeda Y Holistic evaluation of language models. arXiv prepr. arxiv:2211.09110v2 [cs.CL]\nLi H, Chen Y , Luo J, Kang Y , Zhang X, Hu Q, Chan C, Song Y (2023) Privacy in large language models: \nAttacks, defenses and future directions. arXiv prepr. arxiv:2310.10383\nLi X, Liu M, Gao S, Buntine W (2023) A survey on out-of-distribution evaluation of neural NLP models. In: \nProc. 32th Int. Jt. Conf. Artif. Intell. (IJCAI 2023), pp. 6683–6691\nLimisiewicz T, Mareček D, Musil T (2023) Debiasing algorithm through model adaptation. arXiv prepr. \narxiv:2310.18913\nLin S, Hilton J, Evans O (2022) Teaching models to express their uncertainty in words. arXiv prepr. \narxiv:2205.14334\nLi X, Tramer F, Liang P, Hashimoto T (2022) Large language models can be strong differentially private \nlearners. In: 10th Int. Conf. Learn. Represent. (ICLR 2022)\nLiu Y , Deng G, Li Y , Wang K, Zhang T, Liu Y , Wang H, Zheng Y , Liu Y (2023) Prompt injection attack \nagainst LLM-integrated applications. arXiv prepr. arxiv:2306.05499\nLiu A, Pan L, Hu X, Meng S, Wen L (2024) A semantic invariant robust watermark for large language mod-\nels. In: 12th Int. Conf. Learn. Represent. (ICLR 2024)\nLiu X, Xu N, Chen M, Xiao C (2023) Autodan: Generating stealthy jailbreak prompts on aligned large lan -\nguage models. arXiv prepr. arxiv:2310.04451\nLiu Y , Zeng X, Meng F, Zhou J (2023) Instruction position matters in sequence generation with large lan-\nguage models. arXiv prepr. arxiv:2308.12097\nLiu T, Zhang Y , Zhao Z, Dong Y , Meng G, Chen K (2024) Making them ask and answer: Jailbreaking large \nlanguage models in few queries via disguise and reconstruction. arXiv prepr. arxiv:2402.18104\nLi Y , Wei F, Zhao J, Zhang C, Zhang H (2023) Rain: Your language models can align themselves without \nfinetuning. arXiv prepr. arxiv:2309.07124\nLi Z, Zhang S, Zhao H, Yang Y , Yang D (2023) Batgpt: A bidirectional autoregessive talker from generative \npre-trained transformer. arXiv prepr. arxiv:2307.00360\nLi X, Zhou Z, Zhu J, Yao J, Liu T, Han B (2023) Deepinception: Hypnotize large language model to be jail-\nbreaker. arXiv prepr. arxiv:2311.03191\nLou R, Zhang K, Yin W Is prompt all you need? no. A comprehensive and broader view of instruction learn-\ning. arXiv prepr. arxiv:2303.10475\nLuo Z, Xie Q, Ananiadou S (2023) Chatgpt as a factual inconsistency evaluator for abstractive text summa -\nrization. arXiv prepr. arxiv:2303.15621\nLv H, Wang X, Zhang Y , Huang C, Dou S, Ye J, Gui T, Zhang Q, Huang X (2024) CodeChameleon: Person-\nalized encryption framework for jailbreaking large language models. arXiv prepr. arxiv:2402.16717\nLyu C, Xu J, Wang L (2023) New trends in machine translation using large language models: Case examples \nwith chatgpt. arXiv prepr. arxiv:2305.01181\nMalik A (2023) Evaluating large language models through gender and racial stereotypes. arXiv prepr. \narxiv:2311.14788\n1 3\n  382  Page 50 of 56\nSafeguarding large language models: a survey\nManakul P, Liusie A, Gales MJ (2023) Selfcheckgpt: Zero-resource black-box hallucination detection for \ngenerative large language models. arXiv prepr. arxiv:2303.08896\nMangaokar N, Hooda A, Choi J, Chandrashekaran S, Fawaz K, Jha S, Prakash A (2024) PRP: Propagating \nuniversal perturbations to attack large language model guard-rails. arXiv prepr. arxiv:2402.15911\nMa X, Sap M, Rashkin H, Choi Y (2020) PowerTransformer: Unsupervised controllable revision for biased \nlanguage correction. In: Webber, B., Cohn, T., He, Y ., Liu, Y . (eds.) Proceedings of the 2020 Confer-\nence on Empirical Methods in Natural Language Processing (EMNLP), pp. 7426–7441. Association for \nComputational Linguistics, Online.  h t t p s :  / / d o i  . o r g / 1  0 . 1 8  6 5 3 / v  1 / 2 0 2  0 . e m n l  p - m a  i n . 6 0 2\nMaus N, Chao P, Wong E, Gardner JR (2023) Black box adversarial prompting for foundation models. In: \n2nd Workshop New Front. Advers. Mach. Learn\nMbiazi D, Bhange M, Babaei M, Sheth I, Kenfack PJ (2023) Survey on AI ethics: A socio-technical perspec-\ntive. arXiv prepr. arxiv:2311.17228 [cs.CY]\nMehandru N, Miao BY , Almaraz ER, Sushil M, Butte AJ, Alaa A (2024) Evaluating large language models as \nagents in the clinic. NPJ Dig Med 7(1):84\nMehrotra A, Zampetakis M, Kassianik P, Nelson B, Anderson H, Singer Y , Karbasi A (2023) Tree of attacks: \nJailbreaking black-box llms automatically. arXiv prepr. arxiv:2312.02119\nMenini S, Aprosio AP, Tonelli S (2021) Abuse is contextual, what about NLP? The role of context in abusive \nlanguage annotation and detection. arXiv prepr. arxiv:2103.14916 [cs.CL]\nMilgram S (1963) Behavioral study of obedience. J Abnorm Soc Psychol 67(4):371\nMilgram S (1975) Obedience to authority: an experimental view. Contemp Sociol 4(6):617\nMin S, Krishna K, Lyu X, Lewis M, Yih W-t, Koh PW, Iyyer M, Zettlemoyer L, Hajishirzi H (2023) \nFactscore: Fine-grained atomic evaluation of factual precision in long form text generation. arXiv prepr. \narxiv:2305.14251\nMireshghallah F, Backurs A, Inan HA, Wutschitz L, Kulkarni J (2022) Differentially private model compres-\nsion. NeurIPS 35:29468–29483\nMishra A, Patel D, Vijayakumar A, Li XL, Kapanipathi P, Talamadupula K (2021) Looking beyond sentence-\nlevel natural language inference for question answering and text summarization. In: Proc. 2021 Conf. \nN. Am. Chapter Assoc. Comput. Linguist.: Hum. Lang. Technol., pp. 1322–1336\nMohapatra J, Ko C-Y , Weng L, Chen P-Y , Liu S, Daniel L. Hidden cost of randomized smoothing. In: Baner-\njee, A., Fukumizu, K. (eds.) Proc. 24th Int. Conf. Artif. Intell. Stat. Proceedings of Machine Learning \nResearch, vol. 130, pp. 4033–4041. PMLR, (2021-04-13/2021-04-15)\nMotoki F, Pinho Neto V , Rodrigues V (2023) More human than human: Measuring chatgpt political bias. \nAvailable SSRN 4372349\nNaihin S, Atkinson D, Green M, Hamadi M, Swift C, Schonholtz D, Kalai AT, Bau D (2023) Testing language \nmodel agents safely in the wild. arXiv prepr. arxiv:2311.10538\nNan F, Nallapati R, Wang Z, Santos CN, Zhu H, Zhang D, McKeown K, Xiang B (2021) Entity-level factual \nconsistency of abstractive text summarization. arXiv prepr. arxiv:2102.09130\nNarayanan A, Kapoor S (2023) Is GPT-4 getting worse over time? AI Snake Oil\nNarayanan D, Shoeybi M, Casper J, LeGresley P, Patwary M, Korthikanti V , Vainbrand D, Catanzaro \nB (2021) Scaling language model training to a trillion parameters using megatron. arXiv prepr. \narxiv:2104.04473v5\nNgatchou P, Zarei A, El-Sharkawi A (2005) Pareto multi objective optimization. Proc 13th Int Conf Intell \nSyst Appl Power Syst. https://doi.org/10.1109/ISAP.2005.1599245\nNguyen TT, Huynh TT, Nguyen PL, Liew AW-C, Yin H, Nguyen QVH (2022) A survey of machine unlearn-\ning. arXiv prepr. arxiv:2209.02299v5 [cs.LG]\nNvidia: Colang (2023)\nOba D, Kaneko M, Bollegala D (2023) In-contextual bias suppression for large language models. arXiv \nprepr. arxiv:2309.07251\nOh S, Jin Y , Sharma M, Kim D, Ma E, Verma G, Kumar S (2024) Uniguard: Towards universal safety \nguardrails for jailbreak attacks on multimodal large language models. arXiv preprint arXiv:2411.01703\nOh C, Won H, So J, Kim T, Kim Y , Choi H, Song K (2022) Learning fair representation via distributional con-\ntrastive disentanglement. In: Zhang, A., Rangwala, H. (eds.) 28th ACM SIGKDD Conf. Knowl. Discov. \nData Min. (KDD 2022), pp. 1295–1305. ACM. https://doi.org/10.1145/3534678.3539232\nOpenAI: GPT-4 technical report. arXiv e-prints (2023) arxiv:2303.08774\nOppermann A (2023) What Is the V-model in Software Development?\nOuyang L, Wu J, Jiang X, Almeida D, Wainwright C, Mishkin P, Zhang C, Agarwal S, Slama K, Ray A et al \n(2022) Training language models to follow instructions with human feedback. NeurIPS 35:27730–27744\nOvalle A, Mehrabi N, Goyal P, Dhamala J, Chang K-W, Zemel RS, Galstyan A, Gupta R (2023) Are you \ntalking to [’xem’] or [’x’, ’em’]? On tokenization and addressing misgendering in LLMs with pronoun \ntokenization parity. CoRR abs/2312.11779\n1 3\nPage 51 of 56   382 \nY. Dong et al.\nOzdayi MS, Peris C, Fitzgerald J, Dupuy C, Majmudar J, Khan H, Parikh R, Gupta R (2023) Control -\nling the extraction of memorized data from large language models via prompt-tuning. arXiv prepr. \narxiv:2305.11759\nPaduraru C, Patilea C, Stefanescu A (2024) Cyberguardian: An interactive assistant for cybersecurity spe -\ncialists using large language models. In: Proceedings of the 19th International Conference on Software \nTechnologies (ICSOFT 2024), Dijon, France, pp. 8–10\nPal S, Bhattacharya M, Lee S-S, Chakraborty C (2023) A domain-specific next-generation large language \nmodel (LLM) or ChatGPT is required for biomedical engineering and research. Ann Biomed Eng. \nhttps://doi.org/10.1007/s10439-023-03306-x\nPantha N, Ramasubramanian M, Gurung I, Maskey M, Ramachandran R (2024) Challenges in guardrailing \nlarge language models for science. arXiv preprint arXiv:2411.08181\nPavlopoulos J, Sorensen J, Dixon L, Thain N, Androutsopoulos I (2020) Toxicity detection: Does context \nreally matter? In: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J. (eds.) Proc. 58th Annu. Meet. Assoc. \nComput. Linguist., pp. 4296–4305. Association for Computational Linguistics, Online.  h t t p s :  / / d o i  . o r g / \n1  0 . 1 8  6 5 3 / v  1 / 2 0 2  0 . a c l -  m a i n  . 3 9 6\nPelrine K, Taufeeque M, Zając M, McLean E, Gleave A (2023) Exploiting novel gpt-4 apis. arXiv prepr. \narxiv:2312.14302\nPerez E, Huang S, Song F, Cai T, Ring R, Aslanides J, Glaese A, McAleese N, Irving G (2022) Red teaming \nlanguage models with language models. arXiv prepr. arxiv:2202.03286\nPerez F, Ribeiro I (2022) Ignore previous prompt: Attack techniques for language models. In: NeuIPS Work-\nshop Mach. Learn. Saf\nPeri SDB, Santhanalakshmi S, Radha R (2024) Chatbot to chat with medical books using retrieval-aug -\nmented generation model. In: 2024 IEEE North Karnataka Subsection Flagship International Confer -\nence (NKCon), pp. 1–5. IEEE\nPeri R, Jayanthi SM, Ronanki S, Bhatia A, Mundnich K, Dingliwal S, Das N, Hou Z, Vishnubhotla HGS et al \n(2024) Speechguard: exploring the adversarial robustness of multi-modal large language models. Find \nAssoc Comput Ling ACL 2024:10018–10035\nPlant R, Giuffrida V , Gkatzia D (2022) You are what you write: Preserving privacy in the era of large lan-\nguage models. arXiv prepr. arxiv:2204.09391\nQian R, Ross C, Fernandes J, Smith EM, Kiela D, Williams A (2022) Perturbation augmentation for fairer \nNLP. In: Goldberg, Y ., Kozareva, Z., Zhang, Y . (eds.) Proc. 2022 Conf. Empir. Methods Nat. Lang. \nProcess. (EMNLP 2022), pp. 9496–9521. Association for Computational Linguistics.  h t t p s :  / / d o i  . o r g / 1  0 \n. 1 8  6 5 3 / V  1 / 2 0 2  2 . E M N L  P - M A  I N . 6 4 6\nQi X, Wei B, Carlini N, Huang Y , Xie T, He L, Jagielski M, Nasr M, Mittal P, Henderson P (2024) On evaluat-\ning the durability of safeguards for open-weight llms. arXiv preprint arXiv:2412.07097\nQi X, Zeng Y , Xie T, Chen P-Y , Jia R, Mittal P, Henderson P (2024) Fine-tuning aligned language models \ncompromises safety, even when users do not intend to! In: 12th Int. Conf. Learn. Represent. (ICLR \n2024)\nRahman MA, Alqahtani L, Albooq A, Ainousah A (2024) A survey on security and privacy of large multi -\nmodal deep learning models: Teaching and learning perspective. In: 21st Learn. Technol. Conf. (L &T \n2024), pp. 13–18. IEEE, ???\nRai P, Sood S, Madisetti VK, Bahga A (2024) Guardian: a multi-tiered defense architecture for thwarting \nprompt injection attacks on llms. J Softw Eng Appl 17(1):43–68\nRajpal S (2023) Guardrails AI\nRame A, Couairon G, Dancette C, Gaya J-B, Shukor M, Soulier L, Cord M (2023) Rewarded soups: towards \npareto-optimal alignment by interpolating weights fine-tuned on diverse rewards. Adv Neural Inf Pro -\ncess Syst 36:71095–71134\nRamezani A, Xu Y (2023) Knowledge of cultural moral norms in large language models. arXiv prepr. \narxiv:2306.01857\nRanaldi L, Ruzzetti ES, Venditti D, Onorati D, Zanzotto FM (2023) A trip towards fairness: Bias and de-\nbiasing in large language models. arXiv prepr. arxiv:2305.13862\nRebedea T, Dinu R, Sreedhar M, Parisien C, Cohen J (2023) Nemo guardrails: A toolkit for controllable and \nsafe llm applications with programmable rails. arXiv prepr. arxiv:2310.10501\nRen AZ, Dixit A, Bodrova A, Singh S, Tu S, Brown N, Xu P, Takayama L, Xia F, Varley J, et al (2023) \nRobots that ask for help: Uncertainty alignment for large language model planners. In: 2023 Conf. \nRobot Learn., pp. 661–682. PMLR, ???\nRen J, Luo J, Zhao Y , Krishna K, Saleh M, Lakshminarayanan B, Liu PJ (2023) Out-of-distribution detection \nand selective generation for conditional language models. In: 11th Int. Conf. Learn. Represent. (ICLR \n2023)\nRobey A, Wong E, Hassani H, Pappas GJ (2023) Smoothllm: Defending large language models against jail-\nbreaking attacks. arXiv prepr. arxiv:2310.03684\n1 3\n  382  Page 52 of 56\nSafeguarding large language models: a survey\nRosenblatt L, Piedras L, Wilkins J (2022) Critical perspectives: A benchmark revealing pitfalls in Perspec -\ntiveAPI. In: Biester, L., Demszky, D., Jin, Z., Sachan, M., Tetreault, J., Wilson, S., Xiao, L., Zhao, J. \n(eds.) Proc. 2nd Workshop NLP Posit. Impact (NLP4PI), pp. 15–24. Association for Computational \nLinguistics, Abu Dhabi, United Arab Emirates (Hybrid) . https://doi.org/10.18653/v1/2022.nlp4pi-1.2\nRöttger P, Kirk HR, Vidgen B, Attanasio G, Bianchi F, Hovy D (2023) Xstest: A test suite for identifying \nexaggerated safety behaviours in large language models. arXiv prepr. arxiv:2308.01263\nRuan Y , Dong H, Wang A, Pitis S, Zhou Y , Ba J, Dubois Y , Maddison CJ, Hashimoto T (2023) Identifying the \nrisks of lm agents with an lm-emulated sandbox. arXiv prepr. arxiv:2309.15817\nSap M, Gabriel S, Qin L, Jurafsky D, Smith NA, Choi Y (2020) Social bias frames: Reasoning about social \nand power implications of language. In: Jurafsky, D., Chai, J., Schluter, N., Tetreault, J. (eds.) Proc. 58th \nAnnu. Meet. Assoc. Comput. Linguist., pp. 5477–5490. Association for Computational Linguistics, \nOnline .  h t t p s :  / / d o i  . o r g / 1  0 . 1 8  6 5 3 / v  1 / 2 0 2  0 . a c l -  m a i n  . 4 8 6\nSarker IH (2024) LLM potentiality and awareness: A position paper from the perspective of trustworthy and \nresponsible AI modeling. Authorea Prepr\nSchwartz R, Vassilev A, Greene K, Perine L, Burt A, Hall P (2022) Towards a standard for identifying and \nmanaging bias in artificial intelligence. Nat Inst Standards Technol Gaithersburg MD.  h t t p s : / / d o i . o r g / 1 \n0 . 6 0 2 8 / N I S T . S P . 1 2 7 0       \nSchwinn L, Dobre D, Xhonneux S, Gidel G, Günnemann S (2024) Soft prompt threats: attacking safety \nalignment and unlearning in open-source llms through the embedding space. Adv Neural Inf Process \nSyst 37:9086–9116\nShah MA, Sharma R, Dhamyal H, Olivier R, Shah A, Konan J, Alharthi D, Bukhari HT, Baali M, Deshmukh \nS, Kuhlmann M, Raj B, Singh R (2023) LoFT: Local proxy fine-tuning for improving transferability of \nadversarial attacks against large language model. arXiv prepr. arxiv:2310.04445v2 [cs.CL]\nShaikh O, Zhang H, Held W, Bernstein M, Yang D (2022) On second thought, let’s not think step by step! \nBias and toxicity in zero-shot reasoning. arXiv prepr. arxiv:2212.08061\nShayegani E, Mamun MAA, Fu Y , Zaree P, Dong Y , Abu-Ghazaleh N (2023) Survey of vulnerabilities in \nlarge language models revealed by adversarial attacks. arXiv prepr. arxiv:2310.10844\nShen X, Chen Z, Backes M, Shen Y , Zhang Y (2023) \" do anything now\": Characterizing and evaluating in-\nthe-wild jailbreak prompts on large language models. arXiv prepr. arxiv:2308.03825\nSheng Y , Cao S, Li D, Zhu B, Li Z, Zhuo D, Gonzalez JE, Stoica I (2023) Fairness in serving large language \nmodels. arXiv prepr. arxiv:2401.00588\nSheppard B, Richter A, Cohen A, Smith EA, Kneese T, Pelletier C, Baldini I, Dong Y (2023) Subtle misogyny \ndetection and mitigation: An expert-annotated dataset. arXiv prepr. arxiv:2311.09443\nShi J, Liu Y , Zhou P, Sun L (2023) Badgpt: Exploring security vulnerabilities of chatgpt via backdoor attacks \nto instructgpt. arXiv prepr. arxiv:2304.12298\nShi W, Shea R, Chen S, Zhang C, Jia R, Yu Z (2022) Just fine-tune twice: Selective differential privacy for \nlarge language models. arXiv prepr. arxiv:2204.07667\nShuster K, Poff S, Chen M, Kiela D, Weston J (2021) Retrieval augmentation reduces hallucination in con -\nversation. arXiv prepr. arxiv:2104.07567\nShu M, Wang J, Zhu C, Geiping J, Xiao C, Goldstein T (2024) On the exploitability of instruction tuning. \nNeurIPS 36\nSimon N, Muise C (2022) TattleTale: Storytelling with planning and large language models. In: ICAPS \nWorkshop Sched. Plan. Appl\nSingh S (2024) Flipkart Enhances AI Safety in E-Commerce: Implementing NVIDIA NeMo Guardrails.  h t \nt p s :  / / b l o  g . fl  i p  k a r t  . t e c h  / fl  i p  k a r t - e  n h a n  c e s - a  i - s a f  e t y - i n  - e - c  o m m e r  c e - i m  p l e m e n  t i n g  - n v i d  i a - n e  m o - g u a  r d r a  \ni l s - c b 2 f 2 9 3 b 2 9 c 0\nSinghal K, Tu T, Gottweis J, Sayres R, Wulczyn E, Hou L, Clark K, Pfohl S, Cole-Lewis H, Neal D, et al \n(2023) Towards expert-level medical question answering with large language models. arXiv prepr. \narxiv:2305.09617\nSong L, Shokri R, Mittal P (2019) Privacy risks of securing machine learning models against adversarial \nexamples. In: Proc. 2019 ACM SIGSAC Conf. Comput. Commun. Secur., pp. 241–257. Association for \nComputing Machinery, London, United Kingdom\nSun H, Pei J, Choi M, Jurgens D (2023) Aligning with whom? large language models have gender and racial \nbiases in subjective nlp tasks. arXiv prepr. arxiv:2311.09730\nTamirisa R, Bharathi B, Phan L, Zhou A, Gatti A, Suresh T, Lin M, Wang J, Wang R, Arel R, et al (2024) \nTamper-resistant safeguards for open-weight llms. arXiv preprint arXiv:2408.00761\nTang X, Jin Q, Zhu K, Yuan T, Zhang Y , Zhou W, Qu M, Zhao Y , Tang J, Zhang Z, et al (2024) Prioritizing \nsafeguarding over autonomy: risks of LLM agents for science. arXiv prepr. arxiv:2402.04247\nTao Y , Viberg O, Baker RS, Kizilcec RF (2023) Auditing and mitigating cultural bias in LLMs. arXiv prepr. \narxiv:2311.14096\nteam GA (2023) Tutorial of Guidance AI.  h t t p s :  / / g u i  d a n c e .  r e a d  t h e d o  c s . i o  / e n / l a  t e s t  / t u t o r i a l s . h t m l\n1 3\nPage 53 of 56   382 \nY. Dong et al.\nTeam GA (2024) use cases of Guardrails AI. https://hub.guardrailsai.com\nTeam N (2024a) Building Blocks for Agentic AI. https://www.nvidia.com/en-gb/ai/\nTeam T (2023) Documentation of Guardrails AI. https://www.trulens.org\nTouvron H, Lavril T, Izacard G, Martinet X, Lachaux M-A, Lacroix T, Rozière B, Goyal N, Hambro E, Azhar \nF, et al (2023) Llama: Open and efficient foundation language models. arXiv prepr. arxiv:2302.13971\nTrist EL, Bamforth KW (1957) Studies in the Quality of Life: Delivered by the Institute of Personnel Man -\nagement in November 1957. Lecture Series\nUngless EL, Rafferty A, Nag H, Ross B (2022) A Robust Bias Mitigation procedure based on the stereotype \ncontent model. arXiv prepr. arxiv:2210.14552\nvan Lamsweerde A, Darimont R, Letier E (1998) Managing conflicts in goal-driven requirements engineer -\ning. IEEE Trans Softw Eng 24(11):908–926. https://doi.org/10.1109/32.730542\nVaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, Polosukhin I (2017) Attention \nis all you need. In: Guyon, I., Luxburg, U.V ., Bengio, S., Wallach, H., Fergus, R., Vishwanathan, S., \nGarnett, R. (eds.) Adv. Neural Inf. Process. Syst. 30 (NeurIPS 2017), vol. 30. Curran Associates, Inc., \n???\nVega J, Chaudhary I, Xu C, Singh G (2023) Bypassing the safety training of open-source llms with priming \nattacks. arXiv preprint arXiv:2312.12321\nVinyals O, Fortunato M, Jaitly N (2015) Pointer networks. In: Cortes, C., Lawrence, N., Lee, D., Sugiyama, \nM., Garnett, R. (eds.) Adv. Neural Inf. Process. Syst. 28 (NeurIPS 2015), vol. 28. Curran Associates, \nInc., ???\nVivien (2023) Better Steering LLM Agents with LMQL.  h t t p s :  / / v i v  i e n 0 0 0  . g i t  h u b . i  o / b l o  g / j o u r  n a l /  b e t t e  r - s t e  e \nr i n g -  L L M -  a g e n t s - w i t h - L M Q L . h t m l\nWachi A, Tran T, Sato R, Tanabe T, Akimoto Y (2024) Stepwise alignment for constrained language model \npolicy optimization. Adv Neural Inf Process Syst 37:104471–104520\nWang B, Chen W, Pei H, Xie C, Kang M, Zhang C, Xu C, Xiong Z, Dutta R, Schaeffer R et al (2023) Decod-\ningtrust: a comprehensive assessment of trustworthiness in gpt models. Adv Neural Inf Process Syst \n36:31232–31339\nWang B, Chen W, Pei H, Xie C, Kang M, Zhang C, X, C, Xiong Z, Dutta R, Schaeffer R, Truong ST, Arora S, \nMazeika M, Hendrycks D, Lin Z, Cheng Y , Koyejo S, Song D, Li B (2024) DecodingTrust: A compre-\nhensive assessment of trustworthiness in GPT models. arXiv prepr. arxiv:2306.11698\nWang Y , Liu X, Li Y , Chen M, Xiao C (2024) Adashield: Safeguarding multimodal large language mod-\nels from structure-based attack via adaptive shield prompting. In: European Conference on Computer \nVision, pp. 77–94. Springer\nWang H, Shu K (2023) Backdoor activation attack: Attack large language models using activation steering \nfor safety-alignment. arXiv prepr. arxiv:2311.09433\nWang X, Wang H, Yang D (2022) Measure and improve robustness in NLP models: A survey. arXiv prepr. \narxiv:2112.08313v2 [cs.CL]\nWang X, Wei J, Schuurmans D, Le QV , Chi EH, Narang S, Chowdhery A, Zhou D (2023) Self-consistency \nimproves chain of thought reasoning in language models. In: 11th Int. Conf. Learn. Represent. (ICLR \n2023)\nWan S, Nikolaidis C, Song D, Molnar D, Crnkovich J, Grace J, Bhatt M, Chennabasappa S, Whitman S, Ding \nS, et al (2024) Cyberseceval 3: Advancing the evaluation of cybersecurity risks and capabilities in large \nlanguage models. arXiv preprint arXiv:2408.01605\nWebster M, Schmitt J (2024) LLM hallucinations: How to detect and prevent them with CI. CircleCI Blog (\nWei J, Wang X, Schuurmans D, Bosma M, Xia F, Chi E, Le QV , Zhou D et al (2022) Chain-of-thought \nprompting elicits reasoning in large language models. NeurIPS 35:24824–24837\nWei A, Haghtalab N, Steinhardt J (2024) Jailbroken: how does llm safety training fail? NeurIPS 36:119\nWei J, Kim S, Jung H, Kim Y-H (2023) Leveraging large language models to power chatbots for collecting \nuser self-reported data. arXiv prepr. arxiv:2301.05843\nWei Z, Wang Y , Wang Y (2023) Jailbreak and guard aligned language models with only few in-context dem-\nonstrations. arXiv prepr. arxiv:2310.06387\nWelbl J, Glaese A, Uesato J, Dathathri S, Mellor J, Hendricks LA, Anderson K, Kohli P, Coppin B, Huang \nP-S (2021) Challenges in detoxifying language models. arXiv prepr. arxiv:2109.07445\nWelbl J, Glaese A, Uesato J, Dathathri S, Mellor J, Hendricks LA, Anderson K, Kohli P, Coppin B, Huang \nP-S (2021) Challenges in detoxifying language models. In: Moens, M.-F., Huang, X., Specia, L., Yih, \nS.W.-t. (eds.) Find. Assoc. Comput. Linguist.: EMNLP 2021, pp. 2447–2469. Association for Compu-\ntational Linguistics, ???\nWeng F, Xu Y , Fu C, Wang W (2024) Mmj-bench: A comprehensive study on jailbreak attacks and defenses \nfor multimodal large language models. arXiv preprint arXiv:2408.08464\nWen J, Ke P, Sun H, Zhang Z, Li C, Bai J, Huang M (2023) Unveiling the implicit toxicity in large language \nmodels. In: 2023 Conf. Empir. Methods Nat. Lang. Process. (EMNLP 2023)\n1 3\n  382  Page 54 of 56\nSafeguarding large language models: a survey\nXiang A (2022) Being “seen” vs. “mis-seen”: tensions between privacy and fairness in computer vision. Harv \nJ Law Technol 36:1\nXiao Y , Jin Y , Bai Y , Wu Y , Yang X, Luo X, Yu W, Zhao X, Liu Y , Chen H, et al (2023) Large language models \ncan be good privacy protection learners. arXiv prepr. arxiv:2310.02469\nXiao Y , Liang PP, Bhatt U, Neiswanger W, Salakhutdinov R, Morency L-P (2022) Uncertainty quantifica-\ntion with pre-trained language models: A large-scale empirical analysis. arXiv prepr. arxiv:2210.04714\nXie Y , Yi J, Shao J, Curl J, Lyu L, Chen Q, Xie X, Wu F (2023) Defending chatgpt against jailbreak attack \nvia self-reminders. Nat Mach Intell 5(12):1486–1496\nXie Z, Lukasiewicz T (2023) An empirical analysis of parameter-efficient methods for debiasing pre-trained \nlanguage models. In: Rogers, A., Boyd-Graber, J.L., Okazaki, N. (eds.) Proc. 61st Annu. Meet. Assoc. \nComput. Linguist., pp. 15730–15745. Association for Computational Linguistics, ???.  h t t p s :  / / d o i  . o r g / 1  \n0 . 1 8  6 5 3 / V  1 / 2 0 2  3 . A C L -  L O N G  . 8 7 6\nXu Z, Jiang F, Niu L, Jia J, Lin BY , Poovendran R (2024) SafeDecoding: Defending against jailbreak attacks \nvia safety-aware decoding. arXiv prepr. arxiv:2402.08983\nYang Y , Dan S, Roth D, Lee I (2024) Benchmarking llm guardrails in handling multilingual toxicity. arXiv \npreprint arXiv:2410.22153\nYang R, Fu M, Tantithamthavorn C, Arora C, Vandenhurk L, Chua J (2025) Ragva: Engineering retrieval \naugmented generation-based virtual assistants in practice. Journal of Systems and Software, 112436\nYang J, Zhang X, Liang K, Liu Y (2023) Exploring the application of large language models in detecting and \nprotecting personally identifiable information in archival data: a comprehensive study. In: IEEE Int. \nConf. Big Data (BigData), pp. 2116–2123. IEEE, ???\nYao H, Lou J, Ren K, Qin Z (2023) PromptCARE: Prompt copyright protection by watermark injection and \nverification. In: 2024 IEEE Symp. Secur. Priv. (SP 2024)\nYeh K-C, Chi J-A, Lian D-C, Hsieh S-K (2023) Evaluating interfaced LLM bias. In: Proc. 35th Conf. Com-\nput. Linguist. Speech Process. (ROCLING 2023), pp. 292–299\nYe W, Ou M, Li T, chen Y , Ma X, Yanggong Y , Wu S, Fu J, Chen G, Wang H, Zhao J (2023) Assessing \nhidden risks of LLMs: An empirical study on robustness, consistency, and credibility. arXiv prepr. \narxiv:2305.10235v4 [cs.LG]\nYi S, Liu Y , Sun Z, Cong T, He X, Song J, Xu K, Li Q (2024) Jailbreak attacks and defenses against large \nlanguage models: A survey. arXiv preprint arXiv:2407.04295\nYin X, Qi Y , Hu J, Chen Z, Dong Y , Zhao X, Huang X, Ruan W (2025) Taiji: Textual anchoring for immuniz-\ning jailbreak images in vision language models. arXiv preprint arXiv:2503.10872\nYong ZX, Menghini C, Bach S (2023) Low-resource languages jailbreak GPT-4. In: Soc. Responsible Lang. \nModel. Res\nYu X (2024) Create an AI Agent with Llama Guard in Anypoint Platform.  h t t p s :  / / m e d  i u m . c o  m / @ y  u x i a o  j i a n \n/  c r e a t e  - a n -  a i - a g  e n t - w  i t h - l l  a m a -  g u a r d  - i n - a  n y p o i n  t - p l  a t f o r m - a 3 1 3 b 2 c 0 b 5 1 f\nYuan T, He Z, Dong L, Wang Y , Zhao R, Xia T, Xu L, Zhou B, Li F, Zhang Z, et al (2024) R-judge: Bench-\nmarking safety risk awareness for LLM agents. arXiv prepr. arxiv:2401.10019\nYuan Y , Jiao W, Wang W, Huang J-t, He P, Shi S, Tu Z (2024) GPT-4 is too smart to be safe: Stealthy chat \nwith LLMs via cipher. In: 12th Int. Conf. Learn. Represent. (ICLR 2024)\nYu J, Lin X, Xing X (2023) Gptfuzzer: Red teaming large language models with auto-generated jailbreak \nprompts. arXiv prepr. arxiv:2309.10253\nYu D, Naik S, Backurs A, Gopi S, Inan HA, Kamath G, Kulkarni J, Lee YT, Manoel A, Wutschitz L, Yekhanin \nS, Zhang H (2022) Differentially private fine-tuning of language models. In: 10th Int. Conf. Learn. \nRepresent. (ICLR 2022). OpenReview.net\nZampieri M, Malmasi S, Nakov P, Rosenthal S, Farra N, Kumar R (2019) Predicting the type and target of \noffensive posts in social media. In: Burstein, J., Doran, C., Solorio, T. (eds.) Proc. 2019 Conf. N. Am. \nChapter Assoc. Comput. Linguist.: Hum. Lang. Technol., pp. 1415–1420. Association for Computa -\ntional Linguistics, Minneapolis, Minnesota. https://doi.org/10.18653/v1/N19-1144\nZeng C, Li S, Li Q, Hu J, Hu J (2020) A survey on machine reading comprehension-tasks, evaluation metrics \nand benchmark datasets. Appl Sci 10(21):7640\nZhan Q, Fang R, Bindu R, Gupta A, Hashimoto T, Kang D (2023) Removing rlhf protections in gpt-4 via \nfine-tuning. arXiv prepr. arxiv:2311.05553\nZhang W, Deng Y , Liu B, Pan SJ, Bing L (2023) Sentiment analysis in the era of large language models: A \nreality check. arXiv prepr. arxiv:2305.15005\nZhang H, Guo Z, Zhu H, Cao B, Lin L, Jia J, Chen J, Wu D (2023) On the safety of open-sourced large lan-\nguage models: Does alignment really prevent them from being misused? arXiv prepr. arxiv:2310.01581\nZhang B, Shen X, Si WM, Sha Z, Chen Z, Salem A, Shen Y , Backes M, Zhang Y (2023) Comprehensive \nassessment of toxicity in ChatGPT. arXiv prepr. arxiv:2311.14685 [cs.CY]\nZhang Z, Yang J, Ke P, Huang M (2023) Defending large language models against jailbreaking attacks \nthrough goal prioritization. arXiv prepr. arxiv:2311.09096\n1 3\nPage 55 of 56   382 \nY. Dong et al.\nZhao J, Chen K, Yuan X, Qi Y , Zhang W, Yu N (2023) Silent guardian: Protecting text from malicious exploi-\ntation by large language models. arXiv prepr. arxiv:2312.09669\nZhao S, Jia M, Tuan LA, Wen J (2024) Universal vulnerabilities in large language models: In-context learn-\ning backdoor attacks. arXiv prepr. arxiv:2401.05949\nZhou KZ, Sanfilippo MR (2023) Public perceptions of gender bias in large language models: Cases of chatgpt \nand ernie. arXiv prepr. arxiv:2309.09120\nZhou A, Li B, Wang H (2024) Robust prompt optimization for defending language models against jailbreak-\ning attacks. arXiv prepr. arxiv:2401.17263\nZhou W, Wang X, Xiong L, Xia H, Gu Y , Chai M, Zhu F, Huang C, Dou S, Xi Z, et al (2024) EasyJailbreak: \nA unified framework for jailbreaking large language models. arXiv prepr. arxiv:2403.12171\nZhu K, Wang J, Zhou J, Wang Z, Chen H, Wang Y , Yang L, Ye W, Zhang Y , Gong NZ, Xie X (2023) Prompt-\nBench: Towards evaluating the robustness of large language models on adversarial prompts. arXiv \nprepr. arxiv:2306.04528v4 [cs.CL]\nZhu S, Zhang R, An B, Wu G, Barrow J, Wang Z, Huang F, Nenkova A, Sun T (2023) Autodan: Automatic \nand interpretable adversarial attacks on large language models. arXiv prepr. arxiv:2310.15140\nZou W, Geng R, Wang B, Jia J (2024) PoisonedRAG: Knowledge poisoning attacks to retrieval-augmented \ngeneration of large language models. arXiv prepr. arxiv:2402.07867\nZou A, Wang Z, Carlini N, Nasr M, Kolter JZ, Fredrikson M (2023) Universal and transferable adversarial \nattacks on aligned language models. arXiv preprint arXiv:2307.15043\nPublisher's Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\n1 3\n  382  Page 56 of 56",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I146655781",
      "name": "University of Liverpool",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I899635006",
      "name": "Université Grenoble Alpes",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I143804889",
      "name": "Loughborough University",
      "country": "GB"
    }
  ]
}