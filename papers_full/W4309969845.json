{
  "title": "Emotion Recognition with Pre-Trained Transformers Using Multimodal Signals",
  "url": "https://openalex.org/W4309969845",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4208642781",
      "name": "Juan Vazquez-Rodriguez",
      "affiliations": [
        "Université Grenoble Alpes"
      ]
    },
    {
      "id": "https://openalex.org/A2103663133",
      "name": "Grégoire Lefebvre",
      "affiliations": [
        "Orange (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2553765462",
      "name": "Julien Cumin",
      "affiliations": [
        "Orange (France)"
      ]
    },
    {
      "id": "https://openalex.org/A2183168386",
      "name": "James L. Crowley",
      "affiliations": [
        "Institut polytechnique de Grenoble",
        "Université Grenoble Alpes",
        "Centre Inria de l'Université Grenoble Alpes",
        "Laboratoire d'Informatique de Grenoble",
        "Centre National de la Recherche Scientifique"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6753505025",
    "https://openalex.org/W2895843840",
    "https://openalex.org/W2162800060",
    "https://openalex.org/W2151669316",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3005387090",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W3161048756",
    "https://openalex.org/W4223523352",
    "https://openalex.org/W6751956528",
    "https://openalex.org/W3004330901",
    "https://openalex.org/W2901027086",
    "https://openalex.org/W6759836712",
    "https://openalex.org/W2936771798",
    "https://openalex.org/W2525648609",
    "https://openalex.org/W3088631780",
    "https://openalex.org/W2731964405",
    "https://openalex.org/W2327450850",
    "https://openalex.org/W3108792608",
    "https://openalex.org/W3205744895",
    "https://openalex.org/W3034266838",
    "https://openalex.org/W6680300913",
    "https://openalex.org/W2092604057",
    "https://openalex.org/W2523856713",
    "https://openalex.org/W3101150053",
    "https://openalex.org/W2977259558",
    "https://openalex.org/W2899427170",
    "https://openalex.org/W2942753551",
    "https://openalex.org/W2980927909",
    "https://openalex.org/W2964051877",
    "https://openalex.org/W2919854899",
    "https://openalex.org/W3148757058",
    "https://openalex.org/W2807324579",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2138857742",
    "https://openalex.org/W2883265831",
    "https://openalex.org/W4297468970"
  ],
  "abstract": "In this paper, we address the problem of multimodal emotion recognition from\\nmultiple physiological signals. We demonstrate that a Transformer-based\\napproach is suitable for this task. In addition, we present how such models may\\nbe pretrained in a multimodal scenario to improve emotion recognition\\nperformances. We evaluate the benefits of using multimodal inputs and\\npre-training with our approach on a state-ofthe-art dataset.\\n",
  "full_text": "EMOTION RECOGNITION WITH PRE-TRAINED TRANSFORMERS\nUSING MULTIMODAL SIGNALS\nAUTHOR VERSION\nJuan Vazquez-Rodriguez1,2, Grégoire Lefebvre1, Julien Cumin1, James L. Crowley2\n1 Orange Labs, Grenoble, France\n2 Univ. Grenoble Alpes, CNRS, Grenoble INP, LIG, 38000 Grenoble, France\nABSTRACT\nIn this paper, we address the problem of multimodal emotion\nrecognition from multiple physiological signals. We demon-\nstrate that a Transformer-based approach is suitable for this\ntask. In addition, we present how such models may be pre-\ntrained in a multimodal scenario to improve emotion recogni-\ntion performances. We evaluate the beneﬁts of using multi-\nmodal inputs and pre-training with our approach on a state-of-\nthe-art dataset.\nKeywords Affective Computing, Multimodal Emotion\nRecognition, Machine Learning.\n1 Introduction\nThe increasing availability of mass-market wearable devices\nequipped with sensors for physiological signals provides new\npossibilities for monitoring the emotional health and well-\nbeing of users [1]. Although less reliable than medical-grade\nsensors, signals from wearable sensors like electrocardiograms\n(ECG) and electroencephalograms (EEG) can be combined to\nprovide estimates of the emotional state of users.\nIn this work, we report on experiments with a Transformer-\nbased approach for interpreting emotional state from different\nphysiological signals obtained from wearable devices. We ex-\nplore the estimation of emotional state from individual sen-\nsor modalities, including EEG and ECG, and show that fus-\ning the two modalities leads to better results indicating that\nthese modalities convey complementary information. Further-\nmore, we demonstrate that a Transformer-based approach can\nbe used to provide reliable estimates of emotional state from\nsuch signals.\nWe center our work on recognizing emotions from ECG and\nEEG signals. There are other works that address multimodal\nemotion recognition [2–5], but the majority use signals such\nas images, sound and text, and not physiological signals. Al-\nthough some authors have explored the use of physiological\nsignals for emotion recognition [6–8], such signals have re-\nceived less attention than other sensing modalities.\nA common problem when addressing the task of emotion\nrecognition is the lack of labeled data to effectively train deep-\nlearning models [6]. A possible approach to address this prob-\nlem is the use of unsupervised pre-training techniques [9].\nHowever, pre-training with multiple signal modalities raises\nadditional challenges. In this work, we investigate the use of\na late-fusion approach, where we pre-train and ﬁne-tune dif-\nferent single-modality models, and then combine the outputs\nof the individual models to obtain a fused feature that can be\nused to perform emotion prediction.\nWe use a Transformer [10] to process physiological signals.\nThe Transformer was originally developed for Natural Lan-\nguage Processing (NLP) tasks, with the intent of processing\nsequences of words. Given that physiological signals are se-\nquences of values, the Transformer can be adapted for physio-\nlogical signal processing [11]. Transformers employ a learned\nattention mechanism to dynamically score the relevance of\ndifferent parts of an input according to context. Attention-\nbased processing is appropriate for processing physiological\nsignals, as some parts of a signal may convey more informa-\ntion than other parts depending on the task and context. An-\nother advantage of using a Transformer is that we can bene-\nﬁt from a very successful pre-training technique described in\nBERT [12] and developed for NLP tasks, which we can adapt\nto our needs. This pre-training strategy has been successfully\nadapted to other domains like Computer Vision [13], Speech\nProcessing [14] and Affective Computing [15].\nThe main contributions of this paper are:\n1. We present a technique for recognizing emotions\nfrom multimodal physiological signals using a Trans-\nformer.\n2. We describe a method to pre-train the Transformer for\nrecognizing emotions from multimodal physiological\nsignals.\n3. We provide results from experiments that show that\na multimodal pre-training strategy is effective for im-\nproving emotion recognition performances.\narXiv:2212.13885v1  [eess.SP]  22 Dec 2022\nAUTHOR VERSION\n2 Related Work\nContrary to traditional techniques like Gaussian naive Bayes\n[16], k-Nearest Neighbours [17] and Support Vector Machines\n[6], deep-learning may be used to recognize emotions directly\nfrom sensor signals without a need to design feature descrip-\ntors. This is particularly useful for the recognition of emotions\nfrom physiological signals where there are no well-established\nfeature descriptors for signal encoding.\nAn example of a deep-learning approach is provided by the\nwork of Santamaria et al. [18], where they employ models\nbased on Convolutional Neural Networks (CNN) to perform\nemotion recognition. Another example is the work of Harper\nand Southern [19], who use a combination of Recurrent Neu-\nral Networks (RNN) and CNNs. The Transformer [10], which\nuses stacked layers of self-attention, has recently emerged as\na powerful alternative to Convolutional and Recurrent Net-\nworks. In this work, we are interested in whether a Trans-\nformer architecture can be an effective tool to recognize emo-\ntions from multiple physiological signals.\nA variety of authors have explored deep-learning models for\nemotion recognition using multimodal signals. Most of these\nworks use images, audio and/or text as inputs [2–5, 20, 21]. In\na few cases, physiological signals have been used to improve\nrecognition from image, audio and text [22–24]. A few authors\nhave described the use of multiple physiological signal modal-\nities [6–8]. These works consistently demonstrate the beneﬁts\nof exploiting multiple signal modalities to improve the perfor-\nmance of emotion recognition.\nSome multimodal approaches employ pre-training techniques\nto improve their results. The authors of [2–5] develop models\nbased on Transformers, using images, audio and text to recog-\nnize emotions. Rahman et al. [2] report on the use of BERT\n[12], a Transformer-based model pre-trained for NLP tasks,\nto process text, incorporating visual and audio modalities in a\nmiddle-fusion process. Siriwardhana et al. [3, 4] describes the\nuse of pre-trained models to extract features from visual, au-\ndio and text modalities, and followed by a cross-modal Trans-\nformer [25] to combine these different features. Khare et al.\n[5] use a BERT-like approach, masking some words in the in-\nput text, along with the audio and visual parts that correspond\nto those words, and then pre-training the model by predicting\nthe masked words.\nSome authors have explored pre-training approaches to rec-\nognize emotions from physiological signals. Sarkar and\nEtemad [26] pre-train their CNN-based model by modifying\nthe input signal with different transformations, such as adding\nnoise or upscaling, and then pre-training the model to predict\nwhich transformation was used on the input signal. Vazquez-\nRodriguez et al. [15] use a model based on a Transformer,\nwhich is pre-trained by masking some values in the input\nsignal and trying to predict those masked values. These ap-\nproaches only employ a single physiological modality.\nOther works have explored pre-training for multimodal emo-\ntion recognition from physiological signals. Ross et al. [8]\nand Liu et al. [27] use Variational Autoencoders (V AE) for\neach modality to extract representations from each physiolog-\nical signal. The representations of all signals are then concate-\nnated, and a second-level classiﬁer is trained to predict emo-\ntion. Yang and Lee [28] also use a V AE, but they use a single\nV AE by concatenating the different signals at the input level.\nHowever, unlike the use of the Transformer, these approaches\ndo not beneﬁt from self-attention.\nWe can see that there is a void in the research regarding dif-\nferent approaches for emotion recognition: multimodal pre-\ntraining approaches are not typically used on physiological\nsignals; conversely, pre-training approaches for physiologi-\ncal signals are usually single-modality; ﬁnally, the few mul-\ntimodal pre-trained approaches for physiological signals we\nsurveyed don’t use attention-based models. In this work, we\nthus propose to investigate the use of attention-based models\nlike the Transformer to recognize emotions in a multimodal\nphysiological scenario, with the use of pre-training techniques.\n3 Approach\n3.1 Multimodal Emotion Recognition\nWhen performing a classiﬁcation task using multiple modali-\nties, an important problem is to determine the processing level\nat which different signal modalities should be combined or\nfused. One option is early-fusion, where the combination of\ninput features is used as input for the model. A second op-\ntion is to do late-fusion, where the outputs of single-modality\nmodels are combined and a second-level model is trained to\nperform the classiﬁcation. A third option is a compromise be-\ntween these two extremes: middle-fusion that combines fea-\ntures from intermediate layers of the models.\nEarly-fusion can be provided by simply concatenating the in-\nput signals in the temporal dimension, to form a single (longer)\nsequence. The problem with this approach is that the computa-\ntional complexity of the Transformer is O(n2), where nis the\nlength of the input. Therefore, a late-fusion approach has the\nadvantage of simplifying the training process: several single-\nmodality models can be trained one by one on less powerful\nhardware than the one required to train a single, more com-\nputationally expensive multimodal model. Then, if the single-\nmodality models are frozen, the second-level model of the late-\nfusion approach can also be easily trained without using very\npowerful hardware resources. A similar reasoning can be ap-\nplied to see the advantages of late-fusion over middle-fusion\nin this scenario.\nAnother major difﬁculty with early-fusion is that it hinders the\nbeneﬁts of pre-training techniques. With pre-training, we seek\nto use many different datasets, not necessarily related to the\ntask of emotion recognition, to obtain a more general represen-\ntation of the information from the different modalities. How-\never, early-fusion limits pre-training to datasets that include\nall of the targeted modalities, thus severely limiting the avail-\nability of datasets that can be used. With late-fusion, one can\npre-train each single-modality model independently from one\nanother, with potentially different datasets.\nIn our case, we are interested in a multimodal approach that\nallows us to use pre-training techniques to improve the per-\n2\nAUTHOR VERSION\nformance of the model. In particular, we are interested in\nattention-based models such as the Transformer [10], and in\npre-training techniques similar to the ones used in BERT [12].\nIn this work, we explore the use of a late fusion approach.\nThis allows self-supervised pre-training for individual sensor\nmodalities by reconstructing masked values in the input sig-\nnal, similar to what is done in BERT [12]. We refer to this as\nMasked Value Prediction or simply MVP.\nWith this approach, recognition training is performed in two\nsteps. In the ﬁrst step, we train two single-modality models:\none to recognize emotions from electrocardiogram (ECG) sig-\nnals, and one to recognize emotions from electroencephalo-\ngram (EEG) signals. Both models are trained separately, us-\ning MVP pre-training. In the second step, we concatenate the\noutputs of the single-modality models and use this combined\nrepresentation to train a Fully-Connected Network (FCN) to\nrecognize emotions. While other fusion approaches may be\npossible like max or average pooling, or majority voting, con-\ncatenation has been found to provide a simple and effective\ntechnique for use with Transformers.\nIn the rest of this section, we provide details about the ECG\nand EEG single-modality models, and the fused model that\ncombines both signals.\n3.2 ECG Single-Modality Emotion Recognition\nFor our ECG emotion recognizer, we employ the approach de-\nscribed in [15] and depicted in Figure 1. In this approach, a\npre-training step is ﬁrst used prior to ﬁne-tuning the model to\nimprove its performance. We provide a brief description of\nthis approach in the remainder of this subsection. We refer the\nreader to the original paper for further details.\nThe ECG single-modality emotion recognition model from\n[15] is based on the Transformer [10]. The Transformer is\nan architecture capable of incorporating contextualized infor-\nmation thanks to its self-attention mechanisms. As shown in\nFigure 1, the model ﬁrst encodes ECG signals using a 1D Con-\nvolutional Neural Network (1D-CNN) to obtain a sequence\nof features that represent the input signal. Then, similar to\nBERT [12], a classiﬁcation token named CLS is added to the\nbeginning of the sequence of features. In the next step, the\nfeature sequence appended with the CLS token is fed into\na Transformer, which produces contextualized representations\nof the signal. Then, the process follows one of two paths, de-\npending on if we are in the pre-training phase or the ﬁne-tuning\nphase.\n3.2.1 Pre-Training Phase\nThe pre-training task consists in using MVP, that is, masking\nsome points from the original signal, and then predicting those\nmasked points with the help of a Fully-Connected Network\n(FCN) placed on top of the Transformer, as can be seen in\npath A of Figure 1. Since this task is self-supervised, we do\nnot need labeled data for this phase.\n3.2.2 Fine-Tuning Phase\nAfter the model has been pre-trained, it is ﬁne-tuned to rec-\nognize emotions. As depicted in path B of Figure 1, the vec-\ntor CLSe, which is the representation of the CLS token, is\nused as input of an FCN that functions as a classiﬁer to predict\nthe emotion. Starting with the same pre-trained weights, the\nmodel is ﬁne-tuned twice: one to predict arousal and another\nto predict valence. Thus, Predicted Emotion in Figure 1 refers\nto either arousal or valence. During this phase, all the parame-\nters of the model, including the Transformer and the 1D-CNN\nparameters, are ﬁne-tuned. This phase is supervised, therefore\nlabeled data is employed.\n3.3 EEG Single-Modality Emotion Recognition\nTo build our EEG single-modality emotion recognition model,\nwe adapt the ECG model from [15] and described in the pre-\nvious subsection, to accept EEG signals.\nFirst, we need to take into account that the ECG signal has\nonly one channel, while the EEG signal is typically multi-\nchannel. Therefore, the 1D-CNN that encodes the raw signal\nhas to be changed from having one-channel input to having\nas many inputs as the number of channels present in the EEG\nsignal, while the rest of the structure of the 1D-CNN remains\nthe same. The shape of the output from the 1D-CNN encoder\nremains similar to the original ECG model, and thus we can\nkeep the same Transformer architecture.\nSecondly, during the pre-training of the model, we mask the\nsame temporal segments of the EEG signal across all the dif-\nferent channels. The size of the output layer of the FCN that\nworks as masked-point predictor has to be the same as the\nnumber of input channels. This way, each predicted output\nvalue corresponds to the (masked) value of each channel.\nAside from these changes, no other adaptation is needed for\nﬁne-tuning the EEG model, as we employ the CLSe vector as\ninput for the FCN used as a classiﬁer to predict the emotion,\nas it is done in the ECG model.\n3.4 Fused-Signals Emotion Recognition Model\nWe use late-fusion to fuse the ECG and EEG signals, using\nthe outputs from each of the trained single-modality emotion\nrecognizers. As depicted in Figure 2, we take the output of\nthe last hidden layer of the FCN (not from the output layer)\nof each single-modality model, and we concatenate those out-\nputs to form our combined features. Although other methods\nmight be considered to do this fusion, for example using aver-\nage pooling of the outputs of the single-modality recognizers,\nwe employ concatenation because it allows us to have single-\nmodality models with outputs of different sizes. This is conve-\nnient because this way we can choose without constraints the\noutput sizes that make the fused model perform the best.\nTo perform the emotion prediction from the fused modalities,\nwe use another FCN that we refer to as Multimodal Emotion\nClassiﬁer in Figure 2. For this step, we freeze the weights\nof the single-modality emotion recognizers, and only train the\n3\nAUTHOR VERSION\nRaw Physiological Signal\n(Values masked during pre-training)\n1D - CNN\nFeature SequenceCLS\nTRANSFORMER\nSignal RepresentationsCLSe\nFCN Masked Values Predictor\nPredicted Masked Values\n(A)\nOnly During Pre-Training\nFCNEmotion Classiﬁer\nPredicted Emotion\n(B)\nOnly During Fine-Tuning\nFigure 1: Single-Modality Emotion Recognizer: The raw signal is encoded by a 1D-CNN and processed with a Transformer.\nFirst, the model is pre-trained by masking some values of the unlabeled input signal and then predicting those masked values\n(Part A). Then, labeled data is used to ﬁne-tune the model in a supervised way (Part B).\nECG Signal\n EEG Signal\nECG Emotion\nRecognizer\nEEG Emotion\nRecognizer\nLast hidden-\nlayer outputs\nConcatenation\nMulitmodal Emotion\nClassiﬁer\n[FCN]\nPredicted Emotion\nFigure 2: Fused Model. Late-fusion is used to combine the\nECG and EEG signals. The outputs of the last layer from both\nsingle-modality models are concatenated, and then used as in-\nput to an FCN that performs the emotion prediction.\ntop FCN. More precisely, we train two FCNs: one to predict\narousal and another to predict valence. This means that in Fig-\nure 2 Predicted Emotion refers to either arousal or valence.\n4 Experiments\nWe evaluate our model on the task of binary emotion predic-\ntion, that is to predict high and low levels of arousal and va-\nlence, from multimodal physiological signals.\nIn this section, we describe our experimental setup, giving de-\ntails about the datasets and the hyper-parameters for both EEG\nand fused-based models.\n4.1 Datasets\nTo train and evaluate our models, we used the AMIGOS\ndataset [6]. This dataset includes data from 40 subjects, where\nemotions were induced by making the subjects watch emo-\ntional videos. A total of 37 subjects watched 20 videos, while\n3 subjects watched only 16. After watching each video, the\nsubjects ﬁlled out a self-assessment form where they rated on\na scale of 1 to 9 their levels of arousal and valence. We use\nthe results of this self-assessment as labels in our experiments.\nSince we are interested in binary emotion classiﬁcation, we\nuse the average value as a threshold to obtain high and low\nclasses of arousal and valence. In total, there are around 65\nhours of data in the AMIGOS dataset.\nThe AMIGOS dataset includes both ECG and EEG signals.\nWe treat ECG as a single-channel signal, and we use the signal\ntaken from the left arm. For EEG, we use 10 channels: F7, F3,\nT7, P7, O1, O2, P8, T8, F4, F8. We chose those 10 channels\nfrom the 14 available because these channels are also present\nin the datasets that we use for pre-training, which we will de-\n4\nAUTHOR VERSION\nscribe shortly. We use the provided 128Hz down-sampled sig-\nnals. ECG signals are ﬁltered with a low-pass ﬁlter with a\ncut-off frequency of 60Hz, and EEG signals are ﬁltered with a\nband-pass ﬁlter with frequencies between 4.0 and 45.0Hz.\n4.1.1 Data for pre-training\nAs described in Section 3.2, the ﬁrst step in training a single-\nmodality emotion recognizer is pre-training. To pre-train the\nEEG emotion recognizer, we gathered EEG data that does not\nnecessarily include labels of emotion. We use the follow-\ning datasets: W AY-EEG-GAL [29], BCI2000 [30, 31], and\nLarge-EEG-BCI [32]. These datasets were gathered to de-\nvelop Brain-Computer Interfaces, so they do not include any\nlabels related to emotions. We also use parts of AMIGOS in\nthe pre-training step, taking care of not using the same sam-\nples to pre-train and evaluate the model, for each fold of cross-\nvalidation. The quantity of data that we gathered to pre-train\nthe EEG model is comparable to the data used to pre-train the\nECG model in [15]: in total there are around 195 hours of\nEEG data available for pre-training, while the ECG model was\npre-trained with around 230 hours of data.\n4.1.2 Signal pre-processing\nMuch like [15] for ECG data, we ﬁlter EEG signals using an\n8th order Butterworth band-pass ﬁlter, with cut-off frequencies\nof 0.8Hz and 50Hz. We also downsample signals to 128Hz.\nIn addition, we normalize signals with zero-mean and unit-\nvariance for each subject. Finally, we split each signal into\n10-second segments. Each segment is used as a sample in our\nexperiments, as in other state-of-the-art works [6, 8, 26], [15].\n4.2 ECG Emotion Recognizer\nThe ECG emotion recognition model follows the architecture\npresented by Vazquez-Rodriguez et al. in [15]. This model\nwas ﬁne-tuned and evaluated on AMIGOS as presented in Sec-\ntion 4.1. This model was parameterized and pre-trained as de-\nscribed in [15].\n4.3 EEG Emotion Recognizer\nThe EEG emotion recognition model follows the architecture\ndescribed in Section 3.3. The 1D-CNN is composed of three\nlayers with kernel sizes (65, 33, 17), with the number of chan-\nnels equal to (64, 128, 256), with stride 1 in all the layers,\nand using the ReLU activation function. With this conﬁgura-\ntion, the size of the receptive ﬁeld is 113 input points, which is\nequivalent to 0.88s. Based on preliminary studies, we believe\nthat this receptive ﬁeld size is suitable for EEG signals. For\nthe Transformer, the number of layers is 2 and the number of\nheads is also 2, with a hidden size of 256.\nFor pre-training, the FCN used to predict masked points has\none hidden layer with size 128 and ReLU activation, and an\noutput layer that gives 10 outputs values, where each output\nvalue corresponds to the predicted value of each masked EEG\nchannel. The mean squared error between the real and the\npredicted values is used as loss. We pre-train the model for\nTable 1: Emotion recognition performances for the EEG\nmodel.\nPre-\ntrain\nArousal\nAcc.\nArousal\nF1\nValence\nAcc.\nValence\nF1\nNo 0.76±7.3e−3 0.75±8.3e−3 0.7±6.4e−3 0.7±6.8e−3\nYes 0.81±10.7e−3 0.80±9.4e−3 0.77±9.3e−3 0.77±9.1e−3\n500 epochs, warming up the learning rate during the ﬁrst 30\nepochs up to 0.0005, and then use linear decay. We use Adam\noptimization with β1 = 0.9, β2 = 0.999 and L2 weight decay\nof 0.005. A dropout value of 0.1 is used in the Transformer.\nFor ﬁne-tuning, the FCN that predicts binary emotions has one\nhidden layer with a size of 64 and ReLU activation functions.\nAn output layer is used to project the output to a single value,\nthat corresponds to the prediction of the emotion. Two differ-\nent networks are ﬁne-tuned, one to predict arousal and another\nto predict valence. The models are ﬁne-tuned using binary\ncross-entropy loss for 100 epochs, starting with a learning rate\nof 0.0001 and decreasing by a factor of 0.65 every 45 epochs.\nAdam optimization is used, with β1 = 0.9, β2 = 0.999 and\nL2 weight decay of 0.00001. A dropout value of 0.6 is used in\nthe FCN that predicts emotions.\nThese hyper-parameters were optimized using the Ray Tune\ntoolkit [33] on validation data extracted from AMIGOS, for\neach fold of cross-validation.\n4.4 Multimodal Emotion Recognizer\nThe multimodal emotion recognition model follows the archi-\ntecture described in Section 3.4. The FCN has two hidden\nlayers of size 64 and 32, and an output layer that projects the\nresult to a single value used to predict the binary emotion. As\nwe did for the single-modality models, we train one model to\npredict arousal and another to predict valence. The activation\nfunction used is ReLU. This network is trained for 52 epochs,\nstarting with a learning rate of 0.00001 and decaying it every\n20 epochs with a factor of 0.65. A dropout value of 0.1 is used\nduring the training of this network. We employ Adam opti-\nmization with β1 = 0.9, β2 = 0.999 and L2 weight decay of\n0.00001. We also use the Ray Tune toolkit to optimize these\nhyper-parameters, as we did for the EEG emotion recognizer.\n5 Results\nIn this section, we discuss the results of the different experi-\nments we performed to evaluate our model for emotion recog-\nnition on the AMIGOS dataset. We use the mean accuracy and\nF1 score between the two predicted classes as metrics, aver-\naged across 10 folds of cross-validation. We also report a two-\nsided 95% conﬁdence interval, calculated with a t-distribution\nwith 9 degrees of freedom.\n5.1 EEG single-modality Emotion Recognition\nWe ﬁrst report in Table 1 the performances of our single-\nmodality emotion recognizer on EEG signals, using the pre-\n5\nAUTHOR VERSION\nTable 2: Emotion recognition performances of single-modality\nmodels and of the fused model.\nArousal\nAcc.\nArousal\nF1\nValence\nAcc.\nValence\nF1\nECG [15]0.88±5.4e−3 0.87±5.4e−3 0.83±7.8e−3 0.83±7.4e−3\nEEG 0.81±10.7e−3 0.80±9.4e−3 0.77±9.3e−3 0.77±9.1e−3\nFused 0.89±5.0e−3 0.89±5.0e−3 0.85±3.8e−3 0.85±3.9e−3\ntraining strategy described in section 3.2.1, and without us-\ning pre-training. We see that the pre-training strategy im-\nproves emotion recognition performances for all metrics, for\nboth arousal and valence. For example, the F1 score for va-\nlence goes from 0.7 ±6.8e−3 when not using pre-training, to\n0.77±9.1e−3 when pre-training is used. This conﬁrms that\nthe pre-training strategy we employ is useful when processing\nEEG signals, and helps the Transformer learn better represen-\ntations that in turn produce better results when predicting emo-\ntion. The comparisons of all metrics shown in Table 1 have a\ntwo-tailed P value less than 1e−5, thus the difference between\nthem can be considered to be statistically signiﬁcant.\n5.2 Fused Model Results\nIn Table 2 we present the performances of our fused model,\nalong with the performances of the single-modality models.\nPre-training was used in the single-modality models, and the\nfused model employs those single-modality models as part of\nthe fusion strategy, as described in Section 3.4. We see that\nour late-fusion approach improves performance over single-\nmodality models. For example, we obtain a valence accuracy\nin the fused model of 0.85±3.8e−3 compared to 0.83±7.8e−3\nand 0.77 ±9.3e−3 when using only EEC and EEG signals\nrespectively. Moreover, when comparing the results of the\nsingle-modality models with the fused strategy, the two-tailed\nP values are less than 1e−3, thus the difference is statistically\nsigniﬁcant.\nFigures 3 and 4 show two different samples used in our ex-\nperiments. Figure 3(a) and Figure 4(a) show the ECG signals,\nand Figure 3(b) and 4(b) show channels F7 and F3 of the EEG\nsignal. For the sample of Figure 3, the ECG model predicts\nthe wrong class, while the EEG model predicts the correct\nclass. We can see that the fused model is helpful when the\nECG modality makes a wrong prediction, relying on the infor-\nmation from the EEG signal to do the classiﬁcation correctly.\nIn Figure 4, we present the same channels as in the previous\nexample, but for a sample that is classiﬁed correctly by the\nECG model and misclassiﬁed by the EEG model. Now the\nfused model is capable of relying on the information from the\nECG signal to classify correctly this sample. Therefore, our\nfusion model is capable of paying attention to the right modal-\nity when one is informative and the other leads to incorrect\npredictions. Looking at the signals for these 2 samples, it is\nnot obvious why misclassiﬁcations occur for one modality or\nthe other, compared to other signals. This showcases that our\nmodel is capable of extracting meaningful hidden features in\nboth modalities.\n0 200 400 600 800 1,000 1,200 1,400\n−5\n0\n5\n(a)\n−2\n0\n2\n0 200 400 600 800 1,000 1,200 1,400\n0\n5\n(b)\nFigure 3: Sample correctly classiﬁed by the Fused Model and\nthe EEG model, but incorrectly classiﬁed by the ECG model.\nFigure (a) shows the ECG signal, and Figure (b) shows chan-\nnels F7 and F3 of the EEG signal.\nTable 3: Fused Model: Pre-Training vs No Pre-Training\nPre-\ntrain\nArousal\nAcc.\nArousal\nF1\nValence\nAcc.\nValence\nF1\nNo 0.86±4.9e−3 0.85±5.1e−3 0.82±6.5e−3 0.81±6.8e−3\nYes 0.89±5.0e−3 0.89±5.0e−3 0.85±3.8e−3 0.85±3.9e−3\n5.3 Effectiveness of Pre-training in the Fused Model\nIn Table 3, we compare the performances of our fused model,\ndepending on whether it uses pre-trained single-modality mod-\nels, or single-modality models with no pre-training. We see\nthat the pre-trained fused model achieves superior perfor-\nmance compared to the fused model with no pre-training. For\nexample, the arousal F1 score improves from 0.85±5.1e−3 to\n0.89±5.0e−3. The results shown in Table 2 have two-tailed\nP values less than 1e−4, thus the difference between them is\nextremely statistically signiﬁcant. These results indicate that\nthe beneﬁts obtained from pre-training single-modality mod-\nels are carried over when combining them with our late-fusion\nstrategy.\n5.4 Comparison with some baselines on AMIGOS\ndataset\nWe report in Table 4 the performance of our models and the re-\nsults reported by the authors of the AMIGOS dataset [6], that\nwe consider as a baseline. We should note that the experimen-\ntal protocol used in the baseline is different than our protocol,\nfor example the length of their segments is 20s, thus their and\nour results are not directly comparable. In any case, we present\n6\nAUTHOR VERSION\n0 200 400 600 800 1,000 1,200 1,400\n0\n2\n4\n6\n(a)\n−2\n0\n2\n0 200 400 600 800 1,000 1,200 1,400\n−2\n0\n2\n(b)\nFigure 4: Sample correctly classiﬁed by the Fused Model and\nthe ECG model, but incorrectly classiﬁed by the EEG model.\nThe ECG signal is shown in Figure (a) and channels F7 and F3\nof the EEG signal are shown in Figure (b).\nTable 4: Comparison with the AMIGOS Dataset Baseline\nArousal F1 Valence F1\nEEG [6] 0.577 0.564\nEEG (ours) 0.80 ±9.4e−3 0.77±9.1e−3\nFused: EEG+ECG+GRS [6] 0.564 0.560\nFused: EEG+ECG (ours) 0.89±5.0e−3 0.85±3.9e−3\nthose results to showcase results obtained by other works, and\nto see if our approach has acceptable performance. We see\nthat our performances are much higher than the baseline, both\nusing only EEG and also in the fused approach. The fused\napproach in the baseline uses Galvanic Skin Response (GSR)\nin addition to EEG and ECG. Our approach is performing at\nmuch higher F1 scores with one less modality, which further\nvalidates that our approach is promising.\n6 Conclusion and Perspectives\nIn this work, we presented a new Transformer-based architec-\nture with pre-training for emotion recognition on multimodal\nphysiological signals. We experimentally showed, using the\nAMIGOS dataset, that our approach can predict valence and\narousal with signiﬁcant accuracy. In addition, we demon-\nstrated that our late-fusion multimodal approach improves per-\nformances over single-modality. Finally, we compared the\nbeneﬁts of our pre-training strategy for multimodal situations.\nOverall, our architecture is capable of reaching state-of-the-art\nperformance for emotion recognition.\nFuture works include investigating new ways to perform pre-\ntraining in multimodal situations: instead of pre-training indi-\nvidual modalities in independent Transformers, we can expect\nthat using a single multimodal Transformer could lead to better\nperformances. Indeed, this way, the model could start to incor-\nporate information from different modalities as early as in the\npre-training phase. In general, new ways of combining dif-\nferent modalities in a pre-trainable Transformer architecture,\nbe it early-fusion, middle-fusion, or late-fusion, are valuable\navenues of research to improve emotion recognition perfor-\nmances of such models. Moreover, the combination of phys-\niological signals with more traditional modalities such as im-\nages and audio may help to better understand how pre-training\nand Transformer-based models behave for multimodal emo-\ntion recognition.\nAcknowledgements: This work has been partially supported by the\nMIAI Multidisciplinary AI Institute at the Univ. Grenoble Alpes:\n(MIAI@Grenoble Alpes - ANR-19-P3IA-0003).\nReferences\n[1] F. Abdat, C. Maaoui, and A. Pruski, “Human-Computer\nInteraction Using Emotion Recognition from Facial Ex-\npression,” in 2011 UKSim 5th European Symposium\non Computer Modeling and Simulation , Nov. 2011, pp.\n196–201.\n[2] Wasifur Rahman, Md Kamrul Hasan, Sangwu Lee, Ami-\nrAli Bagher Zadeh, Chengfeng Mao, Louis-Philippe\nMorency, and Ehsan Hoque, “Integrating Multimodal\nInformation in Large Pretrained Transformers,” in Pro-\nceedings of the 58th Annual Meeting of the Association\nfor Computational Linguistics , Online, July 2020, pp.\n2359–2369, Association for Computational Linguistics.\n[3] S. Siriwardhana, T. Kaluarachchi, M. Billinghurst, and\nS. Nanayakkara, “Multimodal Emotion Recognition\nWith Transformer-Based Self Supervised Feature Fu-\nsion,” IEEE Access, vol. 8, pp. 176274–176285, 2020.\n[4] Shamane Siriwardhana, Andrew Reis, Rivindu\nWeerasekera, and Suranga Nanayakkara, “Jointly\nFine-Tuning \"BERT-like\" Self Supervised Models to\nImprove Multimodal Speech Emotion Recognition,”\narXiv:2008.06682 [cs, eess], Aug. 2020.\n[5] Aparna Khare, Srinivas Parthasarathy, and Shiva Sun-\ndaram, “Self-Supervised Learning with Cross-Modal\nTransformers for Emotion Recognition,” in 2021 IEEE\nSpoken Language Technology Workshop (SLT) , Jan.\n2021, pp. 381–388.\n[6] J. A. Miranda Correa, M. K. Abadi, N. Sebe, and I. Pa-\ntras, “AMIGOS: A Dataset for Affect, Personality and\nMood Research on Individuals and Groups,” IEEE\nTransactions on Affective Computing, pp. 1–1, 2018.\n[7] S. Siddharth, T. Jung, and T. J. Sejnowski, “Utilizing\nDeep Learning Towards Multi-modal Bio-sensing and\nVision-based Affective Computing,” IEEE Transactions\non Affective Computing, pp. 1–1, 2019.\n7\nAUTHOR VERSION\n[8] Kyle Ross, Paul Hungler, and Ali Etemad, “Unsuper-\nvised multi-modal representation learning for affective\ncomputing with multi-corpus wearable data,” Journal\nof Ambient Intelligence and Humanized Computing, Oct.\n2021.\n[9] Dumitru Erhan, Aaron Courville, Yoshua Bengio, and\nPascal Vincent, “Why Does Unsupervised Pre-training\nHelp Deep Learning?,” in Proceedings of the Thirteenth\nInternational Conference on Artiﬁcial Intelligence and\nStatistics. Mar. 2010, pp. 201–208, JMLR Workshop and\nConference Proceedings.\n[10] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Łukasz Kaiser,\nand Illia Polosukhin, “Attention is All you Need,” Ad-\nvances in Neural Information Processing Systems , vol.\n30, 2017.\n[11] Genshen Yan, Shen Liang, Yanchun Zhang, and Fan Liu,\n“Fusing Transformer Model with Temporal Features for\nECG Heartbeat Classiﬁcation,” in 2019 IEEE Interna-\ntional Conference on Bioinformatics and Biomedicine\n(BIBM), Nov. 2019, pp. 898–905.\n[12] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova, “BERT: Pre-training of Deep Bidi-\nrectional Transformers for Language Understanding,”\nin Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), Minneapolis, Minnesota, June\n2019, pp. 4171–4186, Association for Computational\nLinguistics.\n[13] Chen Sun, Austin Myers, Carl V ondrick, Kevin Murphy,\nand Cordelia Schmid, “VideoBERT: A Joint Model for\nVideo and Language Representation Learning,” in 2019\nIEEE/CVF International Conference on Computer Vision\n(ICCV), Oct. 2019, pp. 7463–7472.\n[14] Wen-Chin Huang, Chia-Hua Wu, Shang-Bao Luo, Kuan-\nYu Chen, Hsin-Min Wang, and Tomoki Toda, “Speech\nRecognition by Simply Fine-Tuning Bert,” in ICASSP\n2021 - 2021 IEEE International Conference on Acous-\ntics, Speech and Signal Processing (ICASSP), June 2021,\npp. 7343–7347.\n[15] Juan Vazquez-Rodriguez, Grégoire Lefebvre, Julien\nCumin, and James L. Crowley, “Transformer-Based\nSelf-Supervised Learning for Emotion Recognition,”\narXiv:2204.05103 [cs, eess, q-bio], Apr. 2022.\n[16] Martin Gjoreski, Blagoj Mitrevski, Mitja Luštrek, and\nMatjaž Gams, “An Inter-domain Study for Arousal\nRecognition from Physiological Signals,” Informatica,\nvol. 42, no. 1, Mar. 2018.\n[17] Lin Shu, Yang Yu, Wenzhuo Chen, Haoqiang Hua, Qin\nLi, Jianxiu Jin, and Xiangmin Xu, “Wearable Emo-\ntion Recognition Using Heart Rate Data from a Smart\nBracelet,” Sensors, vol. 20, no. 3, pp. 718, Jan. 2020.\n[18] L. Santamaria-Granados, M. Munoz-Organero,\nG. Ramirez-González, E. Abdulhay, and N. Arunk-\numar, “Using Deep Convolutional Neural Network for\nEmotion Detection on a Physiological Signals Dataset\n(AMIGOS),” IEEE Access, vol. 7, pp. 57–67, 2019.\n[19] R. Harper and J. Southern, “A Bayesian Deep Learning\nFramework for End-To-End Prediction of Emotion from\nHeartbeat,” IEEE Transactions on Affective Computing,\npp. 1–1, 2020.\n[20] Shizhe Chen and Qin Jin, “Multi-modal Conditional At-\ntention Fusion for Dimensional Emotion Prediction,” in\nProceedings of the 24th ACM International Conference\non Multimedia, Amsterdam The Netherlands, Oct. 2016,\npp. 571–575, ACM.\n[21] E. Ghaleb, M. Popa, and S. Asteriadis, “Multimodal and\nTemporal Perception of Audio-visual Cues for Emotion\nRecognition,” in 2019 8th International Conference on\nAffective Computing and Intelligent Interaction (ACII) ,\nSept. 2019, pp. 552–558.\n[22] D. Y . Choi, D.-H. Kim, and B. C. Song, “Multimodal At-\ntention Network for Continuous-Time Emotion Recogni-\ntion Using Video and EEG Signals,” IEEE Access, vol.\n8, pp. 203814–203826, 2020.\n[23] B. Xing, H. Zhang, K. Zhang, L. Zhang, X. Wu, X. Shi,\nS. Yu, and S. Zhang, “Exploiting EEG Signals and Au-\ndiovisual Feature Fusion for Video Emotion Recogni-\ntion,” IEEE Access, vol. 7, pp. 59844–59861, 2019.\n[24] Yuki Matsuda, Dmitrii Fedotov, Yuta Takahashi, Yu-\ntaka Arakawa, Keiichi Yasumoto, and Wolfgang Minker,\n“EmoTour: Multimodal Emotion Recognition using\nPhysiological and Audio-Visual Features,” in Proceed-\nings of the 2018 ACM International Joint Conference and\n2018 International Symposium on Pervasive and Ubiq-\nuitous Computing and Wearable Computers, New York,\nNY , USA, Oct. 2018, pp. 946–951, Association for Com-\nputing Machinery.\n[25] Yao-Hung Hubert Tsai, Shaojie Bai, Paul Pu Liang,\nJ. Zico Kolter, Louis-Philippe Morency, and Ruslan\nSalakhutdinov, “Multimodal Transformer for Unaligned\nMultimodal Language Sequences,” inProceedings of the\n57th Annual Meeting of the Association for Computa-\ntional Linguistics, Florence, Italy, July 2019, pp. 6558–\n6569, Association for Computational Linguistics.\n[26] P. Sarkar and A. Etemad, “Self-Supervised Learning\nfor ECG-Based Emotion Recognition,” in ICASSP 2020\n- 2020 IEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP), May 2020, pp.\n3217–3221.\n[27] Wei Liu, Wei-Long Zheng, and Bao-Liang Lu, “Emotion\nRecognition Using Multimodal Deep Learning,” in Neu-\nral Information Processing , Cham, 2016, pp. 521–529,\nSpringer International Publishing.\n[28] H. Yang and C. Lee, “An Attribute-invariant Variational\nLearning for Emotion Recognition Using Physiology,” in\nICASSP 2019 - 2019 IEEE International Conference on\nAcoustics, Speech and Signal Processing (ICASSP), May\n2019, pp. 1184–1188.\n[29] Matthew D. Luciw, Ewa Jarocka, and Benoni B. Edin,\n“Multi-channel EEG recordings during 3,936 grasp and\n8\nAUTHOR VERSION\nlift trials with varying weight and friction,” Scientiﬁc\nData, vol. 1, no. 1, pp. 140047, Nov. 2014.\n[30] Gerwin Schalk, Dennis J. McFarland, Thilo Hinter-\nberger, Niels Birbaumer, and Jonathan R. Wolpaw,\n“BCI2000: A general-purpose brain-computer interface\n(BCI) system,” IEEE transactions on bio-medical engi-\nneering, vol. 51, no. 6, pp. 1034–1043, June 2004.\n[31] A. L. Goldberger, L. A. Amaral, L. Glass, J. M. Haus-\ndorff, P. C. Ivanov, R. G. Mark, J. E. Mietus, G. B.\nMoody, C. K. Peng, and H. E. Stanley, “PhysioBank,\nPhysioToolkit, and PhysioNet: Components of a new re-\nsearch resource for complex physiologic signals,”Circu-\nlation, vol. 101, no. 23, pp. E215–220, June 2000.\n[32] Murat Kaya, Mustafa Kemal Binli, Erkan Ozbay, Hilmi\nYanar, and Yuriy Mishchenko, “A large electroen-\ncephalographic motor imagery dataset for electroen-\ncephalographic brain computer interfaces,” Scientiﬁc\nData, vol. 5, no. 1, pp. 180211, Dec. 2018.\n[33] Richard Liaw, Eric Liang, Robert Nishihara, Philipp\nMoritz, Joseph E. Gonzalez, and Ion Stoica, “Tune: A\nResearch Platform for Distributed Model Selection and\nTraining,” arXiv:1807.05118 [cs, stat], July 2018.\n9",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7303711771965027
    },
    {
      "name": "Transformer",
      "score": 0.6979618668556213
    },
    {
      "name": "Emotion recognition",
      "score": 0.6701855659484863
    },
    {
      "name": "Speech recognition",
      "score": 0.5527792572975159
    },
    {
      "name": "Multimodal therapy",
      "score": 0.5454531311988831
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4908159673213959
    },
    {
      "name": "Task (project management)",
      "score": 0.41907647252082825
    },
    {
      "name": "Machine learning",
      "score": 0.39157044887542725
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3456581234931946
    },
    {
      "name": "Engineering",
      "score": 0.12561088800430298
    },
    {
      "name": "Psychology",
      "score": 0.06860238313674927
    },
    {
      "name": "Voltage",
      "score": 0.061627715826034546
    },
    {
      "name": "Psychotherapist",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I19370010",
      "name": "Orange (France)",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I899635006",
      "name": "Université Grenoble Alpes",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210101348",
      "name": "Centre Inria de l'Université Grenoble Alpes",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I1294671590",
      "name": "Centre National de la Recherche Scientifique",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I106785703",
      "name": "Institut polytechnique de Grenoble",
      "country": "FR"
    },
    {
      "id": "https://openalex.org/I4210104430",
      "name": "Laboratoire d'Informatique de Grenoble",
      "country": "FR"
    }
  ],
  "cited_by": 12
}