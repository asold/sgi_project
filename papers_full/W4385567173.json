{
  "title": "Character-level White-Box Adversarial Attacks against Transformers via Attachable Subwords Substitution",
  "url": "https://openalex.org/W4385567173",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2413324875",
      "name": "Aiwei Liu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2096652951",
      "name": "Honghai Yu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2100022576",
      "name": "Xuming Hu",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A3020866868",
      "name": "Shu-Ang Li",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2105081510",
      "name": "Li Lin",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2133598051",
      "name": "Fukun Ma",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2107639630",
      "name": "Ya-Wen Yang",
      "affiliations": [
        "Tsinghua University"
      ]
    },
    {
      "id": "https://openalex.org/A2128324904",
      "name": "Lijie Wen",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2952087486",
    "https://openalex.org/W2103076621",
    "https://openalex.org/W2962818281",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2926587947",
    "https://openalex.org/W3201090751",
    "https://openalex.org/W2963823140",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3169965252",
    "https://openalex.org/W3096288490",
    "https://openalex.org/W2963083752",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3158360872",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2156279557",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3034397670",
    "https://openalex.org/W2811010710",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W3103934057",
    "https://openalex.org/W2917128112",
    "https://openalex.org/W2529550020",
    "https://openalex.org/W3094182370",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2097550833",
    "https://openalex.org/W4288953700",
    "https://openalex.org/W2996851481",
    "https://openalex.org/W3104081646",
    "https://openalex.org/W4224627792",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W3104423855",
    "https://openalex.org/W3101449015",
    "https://openalex.org/W4225794827",
    "https://openalex.org/W2963100531",
    "https://openalex.org/W2547875792",
    "https://openalex.org/W3187731984",
    "https://openalex.org/W3175866539",
    "https://openalex.org/W2964301649",
    "https://openalex.org/W4229020587",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W2799194071",
    "https://openalex.org/W4293846201",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2998277219",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W1552847225",
    "https://openalex.org/W2250879510",
    "https://openalex.org/W3176680950"
  ],
  "abstract": "We propose the first character-level white-box adversarial attack method against transformer models. The intuition of our method comes from the observation that words are split into subtokens before being fed into the transformer models and the substitution between two close subtokens has a similar effect with the character modification. Our method mainly contains three steps. First, a gradient-based method is adopted to find the most vulnerable words in the sentence. Then we split the selected words into subtokens to replace the origin tokenization result from the transformer tokenizer. Finally, we utilize an adversarial loss to guide the substitution of attachable subtokens in which the Gumbel-softmax trick is introduced to ensure gradient propagation.Meanwhile, we introduce the visual and length constraint in the optimization process to achieve minimum character modifications.Extensive experiments on both sentence-level and token-level tasks demonstrate that our method could outperform the previous attack methods in terms of success rate and edit distance. Furthermore, human evaluation verifies our adversarial examples could preserve their origin labels.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7664–7676\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nCharacter-level White-Box Adversarial Attacks against Transformers via\nAttachable Subwords Substitution\nAiwei Liu, Honghai Yu, Xuming Hu, Shu’ang Li, Li Lin, Fukun Ma,\nYawen Yang, Lijie Wen†\nTsinghua University\n{liuaw20, yhh21, hxm19, lisa18, lin-l16, mafk19, yyw19}@mails.tsinghua.edu.cn\nwenlj@tsinghua.edu.cn ∗\nAbstract\nWe propose the first character-level white-box\nadversarial attack method against transformer\nmodels. The intuition of our method comes\nfrom the observation that words are split into\nsubtokens before being fed into the transformer\nmodels and the substitution between two close\nsubtokens has a similar effect to the charac-\nter modification. Our method mainly contains\nthree steps. First, a gradient-based method is\nadopted to find the most vulnerable words in\nthe sentence. Then we split the selected words\ninto subtokens to replace the origin tokeniza-\ntion result from the transformer tokenizer. Fi-\nnally, we utilize an adversarial loss to guide the\nsubstitution of attachable subtokens in which\nthe Gumbel-softmax trick is introduced to en-\nsure gradient propagation. Meanwhile, we in-\ntroduce the visual and length constraint in the\noptimization process to achieve minimum char-\nacter modifications. Extensive experiments\non both sentence-level and token-level tasks\ndemonstrate that our method could outperform\nthe previous attack methods in terms of success\nrate and edit distance. Furthermore, human\nevaluation verifies our adversarial examples\ncould preserve their origin labels.\n1 Introduction\nAdversarial examples are modified input data that\ncould fool the machine learning models but not hu-\nmans. Recently, Transformer (Vaswani et al., 2017)\nbased model such as BERT (Devlin et al., 2019) has\nachieved dominant performance on a wide range\nof natural language process (NLP) tasks. Unfortu-\nnately, many works have shown that transformer-\nbased models are vulnerable to adversarial attacks\n(Guo et al., 2021; Garg and Ramakrishnan, 2020).\nOn the other hand, the adversarial attack could help\nimprove the robustness of models through adver-\nsarial training, which emphasizes the importance\nof finding high-quality adversarial examples.\n∗ †Corresponding author.\nat\n#lan\n#ta\nsubword  substitution\natlanta atlauta\ncharacter substitution\nat\n#lau\n#ta\ntokenizeCombine\nat\n#lan\n#ta\nsubword  substitution\natlanta atlanetacharacter insertion\nat\n#lane\n#ta\ntokenizeCombine\nat\n#lan\n#ta\nsubword  substitution\natlanta atlntacharacter deletion\nat\n#ln\n#ta\ntokenizeCombine\nat\n#lan\n#ta\nsubword  substitution\natlanta atnaltacharacter swap\nat\n#nal\n#la\ntokenizeCombine\n(a) (b)\n(c) (d)\nModel\n Query  flights  between boston and  charlotte \nCity                    City\n<b>  <o> <t>    …    n\n(b): Modify character by subtoken substitution\n(a)  unavailability of character gradient\nunavailable gradient\nModel\n Query  flights  between  bo  #st   #on  and  charlotte \nCity                    City\n Query flights\nb\n Query  flights  between  boston  and  charlotte\nTransformer Model\nFigure 1: Subtoken substitution operation could achieve\nthe same result as all four character modification opera-\ntions.\nRecently, some efficient and effective attack-\ning methods have been proposed at token level\n(e.g. synonym substitution) (Guo et al., 2021)\nand sentence level (e.g. paraphrasing input texts)\n(Wang et al., 2020). However, this is not the case\nin character-level attack methods (e.g. mistyping\nwords), which barely hinder human understanding\nand is thus a natural attack scenario. Most previous\nmethods (Gao et al., 2018; Eger and Benz, 2020)\nachieve the character-level attack in a black box\nmanner, which requires hundreds of attempts and\nthe attack success rate is not good enough. White\nbox attack methods are natural solutions to these\ndrawbacks, but current character-level white box at-\ntack methods (Ebrahimi et al., 2018b,a) only work\nfor models taking characters as input and thus fail\non token-level transformer model.\nAchieving character-level white box attack via\nsingle character modification is impossible for the\ntransformer model, due to the gradient of charac-\nters being unavailable. We choose to implement\nthe character-level attack via subtoken substitu-\ntion based on the following two observations. (1)\nNearly all transformer-based pre-training models\nadopt subword tokenizer (Sennrich et al., 2016), in\nwhich each word is split into subtokens containing\none start subtoken and several subtoken attached\n7664\nto it (attachable subtoken). (2) As shown in Figure\n1, all character modifications (e.g. swap and inser-\ntion) can be achieved by subtoken substitution.\nBased on the above observations, we propose\nCWBA, the first Character-level White-Box Attack\nmethod against transformer models via attachable\nsubwords substitution. Our method mainly con-\ntains three steps: target word selection, adversarial\ntokenization, and subtoken search.\nSince our CWBA requires specific words as in-\nput, finding the most vulnerable words is required.\nOur model first ranks the words according to the\ngradient of words from our adversarial goal. Then\nduring the adversarial tokenization process, the top-\nranked words are split into at least three subtokens,\nincluding a start subtoken and several attachable\nsubtokens. Our CWBA method aims to replace these\nattachable subtokens to achieve character attack.\nDue to the discrete nature of natural languages\nprohibits the gradient optimization of subtokens,\nwe leverage the Gumbel-Softmax trick (Jang et al.,\n2017) to sample a continuous distribution from\ntokens and thus allow gradient propagation. The at-\ntachable subtokens are then optimized by a gradient\ndescent method to generate the adversarial example.\nMeanwhile, to minimize the degree of modification,\nwe also introduce visual and length constraints dur-\ning optimization to make the replaced subtokens\nvisually and length-wise similar.\nOur CWBA method could outperform previous at-\ntack methods on both sentence level (e.g. sentence\nclassification) and token level (e.g. named entity\nrecognition) tasks in terms of success rate and edit\ndistance. It is worth mentioning that CWBA is the\nfirst white box attack method applied to token-level\ntasks. Meanwhile, we demonstrate the effective-\nness of CWBA against various transformer-based\nmodels. Human evaluation experiments verify our\nadversarial attack method is label-preserving. Fi-\nnally, the adversarial training experiment shows\nthat training with our adversarial examples would\nincrease the robustness of models.\nTo summarize, the main contributions of our\npaper are as follows:\n• To the best of our knowledge, CWBA is the\nfirst character-level white box attack method\nagainst transformer models.\n• Our CWBA method is also the first white box\nattack method applied to token-level tasks.\n• We propose a visual constraint to make the\nreplaced subtoken similar to the original one.\n• Our CWBA method could outperform the pre-\nvious attack methods on both sentence-level\ntasks and token-level tasks. 1\n2 Related Work\n2.1 White box attack method in NLP\nWhite box attack methods could find the defects\nof the model with low query number and high suc-\ncess rate, which have been successfully applied to\nimage and speech data (Madry et al., 2018; Carlini\nand Wagner, 2018). However, applying white-box\nattack methods to natural language is more chal-\nlenging due to the discrete nature of the text. To\nsearch the text under the guidance of gradient and\nachieve a high success rate, Cheng et al. (2019b,a)\nchoose to optimize in the embedding space and\nsearch the nearest word, which suffers from high\nbias problems. To further reduce the bias, Cheng\net al. (2020) and Sato et al. (2018) restrict the opti-\nmization direction towards the existing word em-\nbeddings. However, the optimization process of\nthese methods is unstable due to the sparsity of\nthe word embedding space. Other methods try\nto directly optimize the text by gradient estima-\ntion techniques such as Gumbel-Softmax sampling\n(Xu et al., 2021; Guo et al., 2021), reinforcement\nlearning (Zou et al., 2020), metropolis-hastings\nsampling (Zhang et al., 2019). Our CWBA adopts\nthe Gumbel-Softmax technique for subtokens to\nachieve the character-level white-box attack.\n2.2 Attack method against Transformers\nTransformer-based (Vaswani et al., 2017) pre-\ntraining models (Devlin et al., 2019; Liu et al.,\n2019) have shown their great advantage on vari-\nous NLP tasks. However, recent works reveal that\nthese pretraining models are vulnerable to adversar-\nial attacks under many scenarios such as sentence\nclassification (Li et al., 2020), machine translation\n(Cheng et al., 2019b), text entailment (Xu et al.,\n2020) and part-of-speech tagging (Eger and Benz,\n2020). Most of these methods achieve attack in the\nblack box manner, which are implemented by char-\nacter modification (Eger and Benz, 2020), token\nsubstitution (Li et al., 2020) or sentence paraphras-\ning (Xu et al., 2020). However, these black-box\nattack methods usually require hundreds of queries\n1Code and data are available at https://github.com/THU-\nBPM/CWBA\n7665\nto the target model and the success rate cannot\nbe guaranteed. To alleviate these problems, some\nwhite-box attack methods have been proposed in-\ncluding token-level methods (Guo et al., 2021)\nand sentence-level methods (Wang et al., 2020).\nDifferent from these methods, our CWBA is the\nfirst character-level white-box attack method for\ntransformer-based models.\n3 Methods\nIn this section, we detail our proposed frame-\nwork CWBA for the character-level white-box attack\nmethod. In the following content, we first give a\nformulation of our attack problem, followed by a\ndetailed description of the three key components:\ntarget word selection, adversarial tokenization, and\nsubtoken search.\n3.1 Attack Problem Formulation\nWe formulate the adversarial examples as follows.\nGiven an input sentence x = (x1,x2,...,x n) with\nlength |n|, suppose the classification model H\ncould predict the correct corresponding sentence\nor token label ysuch that H(x) =y. An adversar-\nial example is a sample x′ close to x but causing\ndifferent model prediction such that H(x′) ̸= y.\nThe process of finding adversarial examples\nis modeled as a gradient optimization problem.\nSpecifically, given the classification logits vector\np ∈RK generated by model H with K classes,\nthe adversarial loss is defined as the margin loss:\nℓadv(x,y) = max\n(\npy −max\nk̸=y\npk + κ,0\n)\n, (1)\nwhich motivates the model to misclassify x by a\nmargin κ >0. The effectiveness of margin loss\nhas been validated in many attack algorithms (Guo\net al., 2021; Carlini and Wagner, 2018).\nGiven the adversarial loss ℓadv, the goal of our\nattack algorithm can be modeled as a constrained\noptimization problem:\nmin ℓadv\n(\nx′,y\n)\nsubject to ρ\n(\nx,x′)\n≤ϵ, (2)\nwhere ρ is the function measuring the similarity\nbetween origin and adversarial examples. In our\nwork, the similarity is measured using the edit dis-\ntance metric (Li and Liu, 2007).\n3.2 Target Word Selection\nSince our attack method takes specific words as the\ntarget and performs pre-processing to these words,\nobtaining the most critical words for target task\nprediction is required. To find the most vulnerable\nwords, we sort the words based on the l2 norm\nvalue of gradient towards adversarial loss in Eq 1:\nˆx = argsort\nx\n(∥∇x1ℓadv∥2,..., ∥∇xn ℓadv∥2) (3)\nwhere ∇xj ℓadv is the gradient of the j-th token.\nNote that word xj may be tokenized into several\nsubtokens [tj0...tjn], and its gradient is defined as\nthe average gradient of these subtokens:\n∥∇xj ℓ∥2 = avg\n(\n∥∇tj0ℓ∥2,..., ∥∇tjn ℓ∥2\n)\n, (4)\nwhere the loss ℓis the adversarial loss ℓadv in our\nwork. Our CWBA would take the first N words\nfrom the sorted word list ˆx as targets, where N is a\ntask-related hyperparameter.\n3.3 Adversarial Tokenization\nThe selected words are required to split into subto-\nkens before performing the character-level attack.\nWe observe that the transformer tokenizer has the\nfollowing two properties: (1) The correctly spelled\nwords usually won’t split or only split into a few\nsubtokens. (2) The misspelled words are tokenized\ninto more subtokens than the correctly spelled\nwords. For example, the word boston won’t be\nsegmented but after single character modification,\nbosfon would be tokenized into three subtokens bo,\n#sf and #on. To keep the tokenization consistency\nduring the attack, we propose the adversarial tok-\nenizer which tokenizes the correctly spelled words\ninto more subtokens than the transformer tokenizer.\nTo further improve the tokenization consistency\nduring the attack process, our main principle is\nto make the subtokens as long as possible, since\nlonger subtokens are more difficult to combine with\ncharacters to form new subtokens 2. Specifically,\nour tokenization contains the following steps:\n1. Find the longest subwords in the first half of\nthe word to form the longest start subtoken.\n2. Find the longest subwords in the second half\nof the word to form the longest end subtoken.\n3. Tokenize the rest part with the transformer\ntokenizer to generate the middle subtokens.\nAfter these steps, we obtain the longest start and\nend subtokens and our algorithm would substitute\nthe middle subtokens, which keeps the maximum\nconsistency of tokenization during the attack.\n2More details and statistics are provided in the appendix\n7666\ndallas\nda #as#ll\npre-split\nWhat service is in\nGumbel sample\n#sf\n#z#a\n#z#a\nAdversarial attack optimization\nLength Constraint optimization\n#sp\n#sf\n#sion\n#z#a\nVisual Constraint optimization\n#sst#sf\n#st\n#z#a\n#st\n#z#a\n#sf\n#z#a\nSample\nWhat service is in da11as\n#st\n#z#a\n#st\n#z#a\nPred: Airport\n(Label: City)\nTransformer Model\ndallas\nda #as#ll\npre-split\nWhat service is in\nGumbel sample\n#sf\n#z#a\n#z#a\nAdversarial attack optimization\nLength Constraint optimization\n#sp\n#sf\n#sion\n#z#a\nVisual Constraint optimization\n#sst#sf\n#st\n#z#a\n#st\n#z#a\n#sf\n#z#a\nSample\nWhat service is in da11as\n#st\n#z#a\nPred: Airport\n(Label: City)\nTransformer Model\n#z#a\n#lll#11\n#11\n#z#a\n#sp\n#z#a\n#11\n#lion\n#st\n#z#a\n#st\n#z#a\n#st\n#st\n#z#a\n#z#a #9\n…\n#11 #sp\n#z\n#z#a\n#11\n#lion\n#11\n#z#a\n#ll\n#ll#a\n#lll#11\n#z#a\n#lll#11\n#z#a\n#lll#11\ndallas\nda #as#ll\npre-split\nWhat service is in\nGumbel sample\nSample\nWhat service is in da11as\nPred: Airport\n(Label: City)\nTransformer Model\nLength Constraint optimization\n#11\n#z#a #9\n#mg\n#ll\n#z#a #9\n#z#a #9\n#11\nAdversarial attack optimization\n#11\n#z#a #9\n#tious\n#z#a #9\n#11#lll\nVisual Constraint optimization\nDistribution over Vocabulary\nTok Optimizable Token Tok Frozen Token\ndallas\nda #as#ll\npre-split\nWhat service is in\nGumbel sample\nSample\nWhat service is in da11as\nPred: Airport\n(Label: City)\nTransformer Model\nLength Constraint optimization\n#11\n#z#a #9\n#mg\n#ll\n#z#a #9\n#z#a #9\n#11\nAdversarial attack optimization\n#11\n#z#a #9\n#tious\n#z#a #9\n#11#lll\nVisual Constraint optimization\nDistribution over Vocabulary\nTok Optimizable Token Tok Frozen Token\nda #llWhat service is in\nGumbel sample\nSample\nWhat service is in da11as\nPred: Airport\nTransformer Model\nLength Constraint optimization\n#11\n#z#a #9\n#mg\n#ll\n#z#a #9\n#z#a #9\n#11\nAdversarial attack optimization\n#11\n#z#a #9\n#tious\n#z#a #9\n#11#lll\nVisual Constraint optimization\nDistribution over Vocabulary\nTok Optimizable Token Tok Frozen Token\nda #as#ll\npre-split\nGumbel sample\nSample\nWhat service is in da11as\nPred: Airport\nTransformer Model\nLength Constraint optimization\n#11\n#z#a #9\n#mg\n#ll\n#z#a #9\n#z#a #9\n#11\nAdversarial attack optimization\n#11\n#z#a #9\n#tious\n#z#a #9\n#11#lll\nVisual Constraint optimization\nDistribution over Vocabulary\nTok Optimizable Token Tok Frozen Token\n(Label: City)\nWhat service is in dallas\nda #as#ll\nAdversarial tokenization\nGumbel sample\nSample\nWhat service is in da11as\nPred: Airport\nTransformer Model\nLength Constraint optimization\n#11\n#z#a #9\n#mg\n#ll\n#z#a #9\n#z#a #9\n#11\nAdversarial attack optimization\n#11\n#z#a #9\n#tious\n#z#a #9\n#11#lll\nVisual Constraint optimization\nDistribution over Vocabulary\nTok Optimizable Token Tok Frozen Token\n(Label: City)\nWhat service is in dallas\nda #as#ll\npre-split\nGumbel sample\nSample\nWhat service is in da11as\nPred: Airport\nTransformer Model\nLength Constraint optimization\n#11\n#z#a #9\n#mg\n#ll\n#z#a #9\n#z#a #9\n#11\nAdversarial attack optimization\n#11\n#z#a #9\n#tious\n#z#a #9\n#11#lll\nVisual Constraint optimization\nDistribution over Vocabulary\nTok Optimizable Token Tok Frozen Token\n(Label: City)\nWhat service is in dallas\nFigure 2: Details of the subtoken search module.\n3.4 Subtoken Search\nAfter obtaining the vulnerable words and tokeniz-\ning them into subtokens in an adversarial way, the\nsubtoken search module aims to find new subtokens\nfor substitution to construct adversarial examples.\nAs shown in Figure 2, to allow gradient prop-\nagation, the target subtoken is first transformed\nfrom the discrete distribution to a continuous dis-\ntribution by the Gumbel-softmax trick (Jang et al.,\n2017). Then the continuous distribution is opti-\nmized by three objectives: adversarial attack, vi-\nsual constraint, and length constraint to search the\nadversarial examples with minimal modifications.\nThe final adversarial examples could be sampled\nfrom the optimized Gumbel-softmax distribution.\nComputing gradients using Gumbel-softmax\nSince the origin subtoken input is represented in the\ndiscrete categorical distribution over vocabulary,\nthe gradient could not be propagated directly. We\nadopt the Gumbel-softmax approximation to derive\nthe soft estimation of the gradient.\nSpecifically, for any token xi ∈V from a fixed\nvocabulary V= {1,...,V }, we denote its one-hot\ndistribution as ϕi. The Gumbel-softmax distribu-\ntion πi could be represented as follows:\n(πi)j = exp ((ϕi,j + gi,j) /T)∑V\nv=1 exp ((ϕi,v + gi,v) /T)\n, (5)\nwhere jindicates the jth token in the dictionary,gi,j\nis sampled from the uniform distribution U(0,1)\nto introduce randomness and T is the temperature\nparameter of the Gumbel-softmax distribution. ϕi\ncould be updated by gradient through the Gumbel-\nsoftmax estimation πi. Let e be the embedding\nlookup table of the transformer, the embedding\nvector of the distribution πi can be defined as:\ne (πi) =\nV∑\nj=1\n(πi)j e(j), (6)\nwhich is the input to the transformer model.\nAdversarial attack objective\nTo search for the desired subtoken substitution\nwhich could mislead the model, an effective ob-\njective function is required. In practice, We adopt\nthe margin loss in Eq 1. Given the whole sen-\ntence vector from Gumbel-softmax distribution\ne(π) = e (π1) ,..., e (πn), the adversarial loss\nis represented as ℓadv (e(π),y). Note that the\nGumbel-softmax distribution is only applied on\nthe target subtokens while the discrete distribu-\ntion keeps unchanged on other subtokens such that\nπi = ϕi. For example, only the subword #ll in\nFigure 2 is sampled by Gumbel-softmax method\nwhile the subtokens da and #as maintains one-hot\ndistribution.\nHowever, the attack success of continuous distri-\nbution πi does not guarantee the top one probability\ntokens in πi could fool the target model. To reduce\nthe gap between the distribution π and one-hot\nword distribution, we sample a discrete distribution\nfrom π and the adversarial loss is also applied on\nit, which is defined as follows:\n(ˆπi)j =\n\n\n\n(πi)j\n|(πi)j|, (πi)j = max (πi)\n0, (πi)j ̸= max (πi)\n. (7)\nThe distribution ˆπ is the one-hot distribution of\nthe previous top probability tokens, of which the\ngradient is retained. Finally, our adversarial loss\ncould be represented as:\nℓadv = ℓadv (e(π),y) +λadvℓadv (e(ˆπ),y) , (8)\nwhere the first term could quickly explore the candi-\ndate tokens and the second term further exploits the\nattack effect of the top one probability token. λadv\nis a hyper-parameter that balances the trade-off of\nexploration and exploitation.\nVisual constraint objective\nTo minimize the edit distance (Eq. 2) caused by\nsubtoken substitution, the visual constraint restricts\nthe substituted subtoken visually similar to the orig-\ninal one, such as the #ll and #11 in Figure 2.\nIn practice, we first generate the images of all\nsubtokens in helvetica font. Then the pre-trained\n7667\nResNet50 network (He et al., 2016) is adopted to\ntransform all the token images into vectors. Let\nv(i) be the visual embedding of i-th token in the\nvocabulary. Similar to Eq. 6, the visual embedding\nof distribution πi could be represented as follows:\nv (πi) =\nV∑\nj=1\n(πi)j v(j). (9)\nThe visual constraint aims to minimize the gap of\nvisual embedding between origin subtoken xi and\ndistribution πi, which is defined as:\nℓvis =\n∑\nI\n∥v(πi) −v(xi)∥2, (10)\nwhere I is the set of all replaceable subtokens and\n∥∥2 is the l2 normalization operation.\nLength constraint objective\nTo further reduce character modifications, the\nlength constraint objective aims to keep the length\nof subtoken unchanged during the attack process.\nSimilar to the visual constraint objective, the\nlength of the distribution πi could be defined as:\nl (πi) =\nV∑\nj=1\n(πi)j l(j), (11)\nwhere l(i) is the length of the i-th token. And the\nlength constraint loss could be represented simi-\nlarly to the visual constraint loss:\nℓlen =\n∑\nI\n∥l(πi) −l(xi)∥2. (12)\nObjective function\nOur final objective is the combination of adversarial\nattack objective, visual constraint objective, and\nlength constraint objective:\nL= ℓadv + λvisℓvis + λlenℓlen, (13)\nwhere λvis,λlen >0 are hyperparameters that con-\ntrols the degree of constraints. The final loss Lis\nminimized using the gradient descent method.\nNote that the number of attacked words is diffi-\ncult to set in long sentences. So we search between\ntwo hyperparameters N1 and N2 until the adversar-\nial loss could be well optimized. The process of\nour algorithm is summarized in algorithm 1.\nAlgorithm 1 CWBA Attack\n1: Input words: x = (x0,...,x n), label: y\n2: Get sorted word list ˆ x= [xtop−1,xtop−2,...] from Eq 1\nbased on the importance of words\n3: for k= k1 to k2 do\n4: s = topk(ˆ x) // Get the most important k words\n5: h ←[] // Input token distribution\n6: for xi ∈x do\n7: if xi ∈s then\n8: [t0,t1...,tn] = adv_tokenize(xi) (Sec 3.3)\n9: [ϕ0,ϕ1...,ϕn] = Onehot([t0,t1...,tn])\n10: [π1,.,π n−1] = Gumbel([ϕ1,.,ϕ n−1]) (Eq 5)\n11: // Only search the middle subtokens\n12: h = h ∪[ϕ0] ∪[π1,...,π n−1] ∪[ϕn]\n13: else\n14: [t0,t1...,tn] = transformer_tokenize(xi)\n15: [ϕ0,ϕ1...,ϕn] = Onehot([t0,t1...,tn])\n16: h = h ∪[ϕ0,...,ϕ n]\n17: end if\n18: end for\n19: for i= 0to MAX_ITER do\n20: Get loss Lfrom Eq 13 based on input h\n21: Update h using gradient descent method\n22: end for\n23: Get adversarial loss ℓadv from Eq 8\n24: // Whether Adversarial loss is well optimized\n25: if ℓadv <κ then\n26: Jump to line 3 // Attack fail, search more words\n27: end if\n28: Sample sentence ˜ xfrom h\n29: if f(˜ x) ̸= ythen\n30: // Attack success\n31: end if\n32: end for\n33: return ˜ x\n4 Experiments\nWe conduct extensive experiments on eight datasets\nacross two tasks (sentence classification and token\nclassification) and four transformer models (BERT,\nRoBERTa, XLNet, and ALBERT) to show the ef-\nfectiveness of CWBA on white-box attack scenarios\nand give a detailed analysis to show its advantage.\n4.1 Experiment Setup\nDatasets. The sentence classification datasets\ninclude DBPedia (Lehmann et al., 2015), AG\nNews (Zhang et al., 2015) for article/news\ncategorization and Yelp Reviews(Zhang et al.,\n2015), IMDB (Maas et al., 2011) for sentiment\nclassification. And the token classification datasets\ninclude ATIS (Tür et al., 2010), SNIPS3 for slot\nfilling and CONLL-2003 (Tjong Kim Sang and\nDe Meulder, 2003), Ontonotes (Pradhan et al.,\n2013) for named entity recognition (NER).\nBaselines. For the sentence classification task, we\nadopt five competitive baselines which are various\n3https://github.com/sonos/nlu-benchmark\n7668\nin attack settings. GBDA (Guo et al., 2021) is\na white-box attack method which performs token\nreplacement under the gradient guidance. BERT-\nAttack (Li et al., 2020), BAE (Garg and Ramakr-\nishnan, 2020) and TextFooler(Jin et al., 2020) aim\nto replace tokens in the black-box manner. Deep-\nWordBug (Gao et al., 2018) is a black-box attack\nmethod which modifies characters of the most vul-\nnerable words. Note that the edit distance of adver-\nsarial examples generated by token replacement is\nmuch higher than that of character modification.\nFor the token classification task, we adopt two\nbaselines. Zéroe (Eger and Benz, 2020) explores\nseveral character-level black-box attack methods\nof which we choose the vision method as our base-\nline for its excellent attack effect. DeepWordBug\nis also adopted as a competitive baseline, which\nmodifies the characters of keywords (e.g. entity in\nnamed entity recognition).\nMore details about these baselines are provided\nin the appendix.\nHyper-parameters. The input token distribution\nis optimized by Adam (Kingma and Ba, 2015)\nwith a learning rate of 0.3 for 100 iterations\n(token classification) or 300 iterations (sentence\nclassification). The margin κof adversarial loss is\nset to 7 for sentence classification and 5 for token\nclassification. And the T of Gumbel-Sampling\n(Eq. 5) is set to 1. The loss weights λadv, λvis\nand λlen (Eq.13) are set to 1, 0.1, 2 respectively.\nThe N1 and N2 are set to 2, 15 and 1, 2 for\nsentence classification and token classification\ntasks respectively.\nModels. We attack four transformer models with\nour CWBA method: BERT (Devlin et al., 2019), XL-\nNet (Yang et al., 2019), RoBERTa (Liu et al., 2019)\nand ALBERT (Lan et al., 2020). We first fine-tune\nthese models on target datasets and then perform\nthe attack. All of these models utilize the subword\ntokenization method. The BERT tokenizer adds a\nprefix ## to the attachable subwords. The tokenizer\nof RoBERTa adds a prefix ˙G to the start subwords.\nAnd the tokenizer of XLNet and ALBERT adds a\nprefix _ to the start subtokens.\n4.2 Quantitative Evaluation.\nSentence-level attacks. Table 1 shows the attack\nperformance on the sentence classification task\nwith the BERT classifier. Following the previ-\nDatasets Clean Acc. Attack Alg. Adv.Acc. #Queries Edit Dist\nAG News95.1\nCWBA(ours)3.2 6.1 17.3DeepWordBug 23.7 319 23.4GBDA 3.5 5.8 76.3BERT-Attack 10.6 213 83.4BAE 13.0 419 65.2TextFooler 12.6 357 97.5\nYelp 97.3\nCWBA(ours)4.0 7.2 18.5DeepWordBug 27.7 543 19.8GBDA 4.4 7.6 84.1BERT-Attack 5.1 273 95.3BAE 12.0 434 63.1TextFooler 6.6 743 74.6\nIMDB 93.0\nCWBA(ours)5.4 8.5 23.4DeepWordBug 28.4 134 24.5GBDA 5.6 5.2 84.5BERT-Attack 11.4 454 91.3BAE 24.0 592 88.2TextFooler 13.6 1134 77.1\nDBPedia99.2\nCWBA(ours)6.9 5.3 19.1DeepWordBug 19.4 453 34.5GBDA 7.1 5.6 79.1BERT-Attack 8.5 487 86.5BAE 10.4 398 94.3TextFooler 9.5 829 83.4\nTable 1: Attack result on sentence classification datasets\nwith finetuned BERT classifiers.\nDatasets Clean F1. Attack Alg. Adv.F1. success rate #Queries Edit Dist\nATIS 96.7 CWBA(ours)9.3 90.0 2.4 2.0DeepWordBug 27.4 71.2 58.8 3.4Zéroe 15.2 84.3 23.4 3.8\nSNIPS 95.8 CWBA(ours)15.3 86.3 2.6 1.9DeepWordBug 29.5 70.1 43.9 3.5Zéroe 25.3 73.5 56.1 3.0\nCONLL200393.2 CWBA(ours)14.4 87.5 3.0 2.9DeepWordBug 38.4 62.3 47.9 7.5Zéroe 33.5 66.5 49.8 7.1\nOntoNotes87.6 CWBA(ours)5.8 96.2 2.1 2.1DeepWordBug 26.3 73.9 26.1 3.5Zéroe 18.4 83.4 31.2 3.1\nTable 2: Attack result on token classification datatsets\nwith finetuned BERT classifiers.\nous works (Guo et al., 2021), we randomly select\n1000 inputs from the test set as attack targets. Our\nmethod searches the number of attacked words be-\ntween N1 and N2 until the attack succeeds. The\nadversarial accuracy (Adv.Acc.) is the accuracy\nof the last searched examples. The Edit Dist rep-\nresents the sum of edit distances for all modified\nwords.\nOverall, our CWBA outperforms the previous\nbaselines in terms of adversarial accuracy and\nedit distance on all datasets. More specifically,\ncompared to the previous best methods, our\nCWBA could further reduce the model’s accuracy\nand the edit distance by 0.3 and 6.0 on average\nrespectively. Meanwhile, our required query\nnumber is similar to the GBDA model and far less\nthan other black-box methods. Also, our CWBA\noutperforms the character-level attack method\nDeepWordBug by a large gap (20.0 adversarial\naccuracy on average), which demonstrates the\nadvantages of the white-box attack.\n7669\nArchitecture Datasets Clean Acc.(F1.) Adv.Acc.(F1.) #Queries Edit Dist\nALBERT\nATIS 96.3 2.2 2.1 2.0OntoNotes87.3 0 1.8 1.6\nAG News 93.8 2.8 5.6 14.5Yelp 96.9 3.7 5.5 15.6\nXLNet\nATIS 96.2 16.6 4.8 3.2OntoNotes87.6 7.1 3.0 2.5\nAG News 94.6 4.6 5.7 16.8Yelp 96.5 5.5 6.2 19.1\nRoBERTa\nATIS 96.6 13.0 5.8 2.5OntoNotes87.7 17.0 4.3 2.2\nAG News 94.7 7.5 7.2 20.1Yelp 97.2 8.4 6.9 21.1\nTable 3: Attack result on three different transformer\nbased pretrained language models.\nToken-level attacks. The attack performance for\nthe BERT classifier towards four token classifica-\ntion datasets is shown in Table 2. Similarly, we\nrandomly select 1000 inputs from the test set as\nattack targets. For SNIPs and ATIS where the size\nof the test set is below 1000, the whole test set is\nselected. The success rate is the percentage of enti-\nties whose predictions are changed after the attack.\nAnd we report the average edit distance for entities.\nIn general, our CWBA surpasses the previous\nblack-box attack methods by a large margin. To be\nspecific, our CWBA could achieve a higher attack\nsuccess rate (13.08% on average) than the previous\nbest method with a smaller edit distance (1.9 on\naverage). Furthermore, our required query number\nis much less than the previous methods (32.8 on\naverage). These experimental results demonstrate\nthe effectiveness of our white-box attack method\nfor the token classification task.\nAttack different transformer models. To illus-\ntrate the generalizability of our CWBA towards\ntransformer models, we report the attack result on\nthree different transformer-based models in Table\n3. We select two benchmarks for sentence and to-\nken classification respectively, where all dataset\nsettings are the same as above.\nIt could be observed that our CWBA has an excel-\nlent attack performance on these transformer mod-\nels. Meanwhile, we found that ALBERT is more\nvulnerable to attacks than other transformer models\nwhile even the best-performing models RoBERTa\nand XLNet are easy to be attacked. These experi-\nmental results illustrate that the current pre-training\nmodels are vulnerable to character modifications.\nAblation study. We conduct ablation stud-\nies to show the effectiveness of different mod-\nules of CWBA to the overall attack performance.\nThe experiments are performed on CONLL and\nDataset Technique Adv Acc.(F1.) #Queries Edit Dist\nCONLL\nCWBA 9.1 4.5 2.7w random word selection 59.4 7.3 2.9w/o visual constraint 8.7 3.8 3.3w/o length constraint 9.3 4.2 3.2w/o visual & length constraint6.2 3.9 4.7\nAG News\nCWBA 3.2 6.1 17.3w random word selection 54.2 13.4 26.7w/o visual constraint 2.7 5.6 25.6w/o length constraint 3.0 5.8 26.7w/o visual & length constraint2.4 4.8 37.8\nTable 4: Ablation study of how different modules con-\ntribute to the attack performance on BERT classifier.\n0.1 0.5 1.0 2.0 4.0\nλvis\n10\n20\n30\n40\n50Adv.Acc.\nCONLL\nATIS\nAdv acc\nEdit dist\n1.50\n1.75\n2.00\n2.25\n2.50\n2.75\nEdit Dist\n0.1 0.5 1.0 2.0 4.0\nλlen\n6\n8\n10\n12\n14Adv.Acc.\nCONLL\nATIS\nAdv acc\nEdit dist\n1.5\n2.0\n2.5\n3.0\n3.5\n4.0\nEdit Dist\nFigure 3: Adversarial accuracy and edit distance during\nthe growth of λvis(left) and λlen(right).\nAG News datasets with the BERT classifier.\nCWBA w random word selectionrandom\nselects target words instead of searching by gra-\ndient. CWBA w/o visual constraintand\nw/o length constraint removes the visual\nand length constraint respectively. CWBA w/o\nvisual & length constraint only keeps\nthe adversarial attack object without any constraint.\nOur observations from the experimental results\nin Table 4 are as follows: (1) The target word se-\nlection module has a huge impact on the attack\nsuccess rate. (2) The visual constraint could signif-\nicantly reduce the edit distance while sacrificing a\nlittle attack performance. Further analysis would\nhelp us balance the attack success rate and edit dis-\ntance. (3) The length constraint has little effect on\nthe attack performance but could effectively reduce\nthe edit distance. In general, all our modules have\npositive effects on the attack performance.\n4.3 Analysis\nEffectiveness of visual and length constraint. To\nfurther investigate how the visual and length con-\nstraint contributes to the attack performance, we\nvisualize the changing trend of adversarial accu-\nracy and edit distance when λvis and λlen grow\nfrom zero in Figure 3. When tuning one hyper-\nparameter, the other hyperparameter would be set\nto 0.1. These experiments are performed on two\ntoken classification datasets: CONLL and ATIS.\nIt can be seen that with λvis increases, the edit\ndistance decreases while the attack performance\n7670\nDataset Ori.Acc.(F1.) Adv.Acc.(F1.) Identification Correction\nATIS 97.2 94.5 99.1 93.3SNIPS 96.3 92.1 97.8 90.5\nYelp 91.2 82.2 92.4 83.1AG News 90.3 80.8 93.4 82.4\nTable 5: Human evaluation for the ability to identify\nand correct adversarial examples.\nDataset Technique Adv Acc.(F1.) #Queries Edit Dist\nATIS\nCWBA 9.3 2.4 2.0CWBA+ Adv.Training 58.5 9.3 4.8\nDeepWordBug 27.4 58.8 3.4DeepWordBug + Adv.Training 44.8 38.4 4.8\nZéroe 15.2 23.4 3.8Zéroe + Adv.Training 56.3 19.5 3.2\nAG News\nCWBA 3.2 6.1 17.3CWBA+ Adv.Training 60.3 12.4 31.2\nDeepWordBug 23.7 319 23.4DeepWordBug + Adv.Training 57.9 412 33.5\nTable 6: Model robustness improvement after adversar-\nial learning with the generated adversarial examples.\ndrops (the accuracy after the attack increases).\nMeanwhile, with the increase of λlen, the attack\nperformance is almost unchanged while the edit\ndistance still reduces. So we conclude that λvis\ninfluences more on the attack success rate than\nλlen, which may be because the transformer-based\nlanguage models are robust to visual similarity\nchanges to some extent. These experiment results\ninspire us to adopt a larger λlen than λvis.\nHuman evaluation. To examine whether the at-\ntacked text preserves its original label, we set up\nhuman evaluations to measure the quality of the\ngenerated text. We first ask human judges to make\npredictions on both the original and attacked texts.\nThen we ask them to identify and correct the modi-\nfied words in the attacked text. The evaluations are\nconducted on 100 selected sentences from ATIS,\nSNIPs DBPedia, and AG News datasets respec-\ntively. For each dataset, we ask three human evalu-\nators to measure the quality of examples.\nThe human evaluation results are presented in\nTable 5. And we could observe that: (1) Human\njudges could predict most of the attacked text\ncorrectly, which demonstrates our generated\nexamples are label-preserving. (2) Although most\nof the modified words could be identified, most\nof these misspelled words could be corrected by\nhumans, which does not hinder understanding.\n(3) Perturbations on the words in sentence\nclassification datasets are harder to identify and\ncorrect than that in token classification datasets\nbecause of the larger edit distance.\nAdversarial training. To further explore whether\nthe adversarial examples could help improve the\nmodel’s robustness, we perform an adversarial\ntraining experiment by training the model with the\ncombination of the original and the adversarial ex-\namples. Specifically, the adversarial examples are\nselected from the attacked texts of the ATIS and\nDBPedia training sets. Furthermore, we compare\nthe attack results towards the original model and\nthe adversarially trained model using our CWBA\nand other character-level attack methods.\nWe present the adversarial training results in\nTable 6. Overall, the robustness of our model\ntowards character-level attacks improves tremen-\ndously after the adversarial training. Specifically,\nthe attack success rate of our CWBA decreases\ndrastically (53.15 on average), while the required\nediting distance and the number of queries become\nmuch larger (6.6 and 8.4 on average). Similarly,\nthe attack performance of other character-level\nattack methods drops severely in all metrics. These\nexperimental results indicate that our generated\ntexts could preserve the origin label.\nTokenization analysis. Our CWBA works on the to-\nkenized subtokens. However, the adversarial texts\nare re-tokenized before being fed to the model,\nwhere the re-tokenization result may not be the\nsame set of origin subtokens. For example, the\nsubtokens bo-, sl- and on would be re-encoded into\nbos- and lon. We further analyze how tokenization\ninconsistency affects attack performance.\nIn practice, we observe that the tokenization in-\nconsistency doesn’t impact the attack performance\nby much. Specifically, 31.2% words are not re-\ntokenized to the same subtoken set but only lead to\n3% attack failures, which indicates the re-tokenized\nexamples are still adversarial. Similar observations\nare reported in previous works (Guo et al., 2021).\nWe further analyze the reason of attack failures\nand find 65% of failed examples are not optimized\nwell and tokenization inconsistency leads to 35%\nattack failures, which indicates that tokenization\ninconsistency has a limited impact on our method.\nCase study. To intuitively show the effectiveness\nof CWBA, we select three cases to compare the orig-\ninal and adversarial texts. These cases are sampled\nfrom AG News, Yelp (sentence classification), and\n7671\nDataset Label\nAG News\nOri fund pessimism grows new york ( cnn / money ) - money managers are growing more pessimistic about the economy,Businesscorporate profits and us stock market returns, according to a monthly survey by merrill lynch released tuesday.\nAdv fund pessimism grows new york ( cnתדרֹוmoney ) - money managers are growing more pessimistic about the econ0my,Sci/Techcorporate profits and us stock market returns, according to a monthly survey by merrill lynch released tuesday.\nYelp\nOri seattle may have just won the 2014 super bowl, but the steelers still rock with six rings, baby!!! just stating what allPositivesteeler fans know : a steel dynasty is still unmatched no matter what team claims the title of current super bowl champs.\nAdv seattle may have just won the 2014 super bowl, but the steelers still robk with six riigs, bacy!!! just staging what allNegativesteeler fans know : a steel dynasty is still unmatched no matter what team claims the title of current super bowl champs.\nOntoNotesOri So far, the Frenchhave failed to win enough broad - based support to prevail. Prediction span: FrenchNORP\nAdv So far, the FrCnchhave failed to win enough broad - based support to prevail. Prediction span: FrCnch ORG\nTable 7: The generated adversarial examples. The origin label is the correct prediction and the adversarial label is\nadverse prediction. The first two examples are from sentence classification task while the third case is from the\ntoken classification task. The target tokens and labels for token classification are underlined.\nOntoNotes (token classification) datasets.\nAs seen in Table 7, the generated adversarial\nsentences are semantically consistent with their\noriginal texts, while the target model makes incor-\nrect predictions. Meanwhile, we could observe that\nmany words are visually similar during the attack\n(e.g. cnn and cnתדרֹוwhich shows the effectiveness\nof our visual constraint. The number of words to\nbe attacked for sentence-level tasks is larger than\nthe token-level tasks, and the concrete number is\nalso uncertain (two in the first case and four in the\nsecond case). For token classification tasks like\nNER, the attacked words are usually the entities.\n5 Conclusions\nIn this paper, we propose CWBA, the first character-\nlevel white-box attack method for transformer\nmodels. We substitute the attachable subtokens\nto achieve character modification. The Gumbel-\nSoftmax technique is adopted to allow gradient\npropagation. Meanwhile, the visual and length\nconstraint help preserve the semantics of adversar-\nial text. Experiments on both sentence-level and\ntoken-level tasks on various transformer models\ndemonstrate the effectiveness of our method.\nAcknowledgement\nThe work was supported by the National Key\nResearch and Development Program of China\n(No.2019YFB1704003), the National Nature Sci-\nence Foundation of China (No.62021002), Ts-\ninghua BNRist and Beijing Key Laboratory of In-\ndustrial Big Data System and Application.\nLimitations\nThe major limitation of CWBA has been discussed\nin the tokenization analysis part in section 4.3: the\ngenerated words may be re-tokenized into different\nsubtokens. Although most examples are still ad-\nversarial, it introduces uncontrollability. We hope\nfuture works could introduce more constraints to\nalleviate this problem.\nAlso, we achieve character modification by sub-\nword substitution, but not all combinations of char-\nacters exist in the vocabulary. Therefore, the effect\nof our attack method depends on the size of the\nvocabulary.\nReferences\nNicholas Carlini and David A. Wagner. 2018. Audio\nadversarial examples: Targeted attacks on speech-to-\ntext. In 2018 IEEE Security and Privacy Workshops,\nSP Workshops 2018, San Francisco, CA, USA, May\n24, 2018, pages 1–7. IEEE Computer Society.\nQian Chen, Zhu Zhuo, and Wen Wang. 2019. Bert\nfor joint intent classification and slot filling. arXiv\npreprint arXiv:1902.10909.\nMinhao Cheng, Wei Wei, and Cho-Jui Hsieh. 2019a.\nEvaluating and enhancing the robustness of dialogue\nsystems: A case study on a negotiation agent. In\nProceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 3325–3335,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nMinhao Cheng, Jinfeng Yi, Pin-Yu Chen, Huan Zhang,\nand Cho-Jui Hsieh. 2020. Seq2sick: Evaluating the\nrobustness of sequence-to-sequence models with ad-\nversarial examples. In Proceedings of the AAAI Con-\nference on Artificial Intelligence, volume 34, pages\n3601–3608.\nYong Cheng, Lu Jiang, and Wolfgang Macherey. 2019b.\nRobust neural machine translation with doubly ad-\nversarial inputs. In Proceedings of the 57th Annual\n7672\nMeeting of the Association for Computational Lin-\nguistics, pages 4324–4333, Florence, Italy. Associa-\ntion for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nJavid Ebrahimi, Daniel Lowd, and Dejing Dou. 2018a.\nOn adversarial examples for character-level neural\nmachine translation. In Proceedings of the 27th Inter-\nnational Conference on Computational Linguistics,\npages 653–663, Santa Fe, New Mexico, USA. Asso-\nciation for Computational Linguistics.\nJavid Ebrahimi, Anyi Rao, Daniel Lowd, and Dejing\nDou. 2018b. HotFlip: White-box adversarial exam-\nples for text classification. In Proceedings of the 56th\nAnnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers), pages 31–36,\nMelbourne, Australia. Association for Computational\nLinguistics.\nSteffen Eger and Yannik Benz. 2020. From hero to\nzéroe: A benchmark of low-level adversarial attacks.\nIn Proceedings of the 1st Conference of the Asia-\nPacific Chapter of the Association for Computational\nLinguistics and the 10th International Joint Confer-\nence on Natural Language Processing, pages 786–\n803, Suzhou, China. Association for Computational\nLinguistics.\nJi Gao, Jack Lanchantin, Mary Lou Soffa, and Yanjun\nQi. 2018. Black-box generation of adversarial text\nsequences to evade deep learning classifiers. In 2018\nIEEE Security and Privacy Workshops (SPW), pages\n50–56. IEEE.\nSiddhant Garg and Goutham Ramakrishnan. 2020.\nBAE: BERT-based adversarial examples for text clas-\nsification. In Proceedings of the 2020 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP), pages 6174–6181, Online. Association for\nComputational Linguistics.\nYotam Gil, Yoav Chai, Or Gorodissky, and Jonathan\nBerant. 2019. White-to-black: Efficient distillation\nof black-box adversarial attacks. arXiv preprint\narXiv:1904.02405.\nChuan Guo, Alexandre Sablayrolles, Hervé Jégou, and\nDouwe Kiela. 2021. Gradient-based adversarial at-\ntacks against text transformers. In Proceedings of the\n2021 Conference on Empirical Methods in Natural\nLanguage Processing, pages 5747–5757, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2016. Deep residual learning for image recog-\nnition. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 770–\n778.\nXuming Hu, Fukun Ma, Chenyao Liu, Chenwei Zhang,\nLijie Wen, and Philip S Yu. 2021a. Semi-supervised\nrelation extraction via incremental meta self-training.\nIn Proc. of EMNLP: Findings.\nXuming Hu, Lijie Wen, Yusong Xu, Chenwei Zhang,\nand S Yu Philip. 2020. Selfore: Self-supervised re-\nlational feature learning for open relation extraction.\nIn Proc. of EMNLP, pages 3673–3682.\nXuming Hu, Chenwei Zhang, Yawen Yang, Xiaohe Li,\nLi Lin, Lijie Wen, and S Yu Philip. 2021b. Gradient\nimitation reinforcement learning for low resource\nrelation extraction. In Proc. of EMNLP, pages 2737–\n2746.\nEric Jang, Shixiang Gu, and Ben Poole. 2017. Categori-\ncal reparameterization with gumbel-softmax. In 5th\nInternational Conference on Learning Representa-\ntions, ICLR 2017, Toulon, France, April 24-26, 2017,\nConference Track Proceedings. OpenReview.net.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter\nSzolovits. 2020. Is bert really robust? a strong base-\nline for natural language attack on text classification\nand entailment. In Proceedings of the AAAI con-\nference on artificial intelligence, volume 34, pages\n8018–8025.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nJens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch,\nDimitris Kontokostas, Pablo N. Mendes, Sebastian\nHellmann, Mohamed Morsey, Patrick van Kleef,\nSören Auer, and Christian Bizer. 2015. Dbpedia -\nA large-scale, multilingual knowledge base extracted\nfrom wikipedia. Semantic Web, 6(2):167–195.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue,\nand Xipeng Qiu. 2020. BERT-ATTACK: Adversar-\nial attack against BERT using BERT. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n6193–6202, Online. Association for Computational\nLinguistics.\nYujian Li and Bi Liu. 2007. A normalized levenshtein\ndistance metric. IEEE Trans. Pattern Anal. Mach.\nIntell., 29(6):1091–1095.\n7673\nShuliang Liu, Xuming Hu, Chenwei Zhang, Shu’ang Li,\nLijie Wen, and Philip S. Yu. 2022. Hiure: Hierarchi-\ncal exemplar contrastive learning for unsupervised\nrelation extraction. In Proc. of NAACL.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nAndrew L. Maas, Raymond E. Daly, Peter T. Pham,\nDan Huang, Andrew Y . Ng, and Christopher Potts.\n2011. Learning word vectors for sentiment analysis.\nIn The 49th Annual Meeting of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Proceedings of the Conference, 19-24 June,\n2011, Portland, Oregon, USA, pages 142–150. The\nAssociation for Computer Linguistics.\nAleksander Madry, Aleksandar Makelov, Ludwig\nSchmidt, Dimitris Tsipras, and Adrian Vladu. 2018.\nTowards deep learning models resistant to adversarial\nattacks. In 6th International Conference on Learning\nRepresentations, ICLR 2018, Vancouver, BC, Canada,\nApril 30 - May 3, 2018, Conference Track Proceed-\nings. OpenReview.net.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nHwee Tou Ng, Anders Björkelund, Olga Uryupina,\nYuchen Zhang, and Zhi Zhong. 2013. Towards ro-\nbust linguistic analysis using OntoNotes. In Proceed-\nings of the Seventeenth Conference on Computational\nNatural Language Learning, pages 143–152, Sofia,\nBulgaria. Association for Computational Linguistics.\nMotoki Sato, Jun Suzuki, Hiroyuki Shindo, and Yuji\nMatsumoto. 2018. Interpretable adversarial perturba-\ntion in input embedding space for text. In Proceed-\nings of the Twenty-Seventh International Joint Con-\nference on Artificial Intelligence, IJCAI 2018, July\n13-19, 2018, Stockholm, Sweden, pages 4323–4330.\nijcai.org.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In Proceedings of the 54th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 1715–1725,\nBerlin, Germany. Association for Computational Lin-\nguistics.\nYongliang Shen, Xinyin Ma, Zeqi Tan, Shuai Zhang,\nWen Wang, and Weiming Lu. 2021. Locate and la-\nbel: A two-stage identifier for nested named entity\nrecognition. In Proceedings of the 59th Annual Meet-\ning of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natu-\nral Language Processing (Volume 1: Long Papers),\npages 2782–2794.\nYongliang Shen, Xiaobin Wang, Zeqi Tan, Guangwei\nXu, Pengjun Xie, Fei Huang, Weiming Lu, and Yuet-\ning Zhuang. 2022. Parallel instance query network\nfor named entity recognition. In Proceedings of the\n60th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n947–961.\nZeqi Tan, Yongliang Shen, Shuai Zhang, Weiming Lu,\nand Yueting Zhuang. 2021. A sequence-to-set net-\nwork for nested named entity recognition. In Pro-\nceedings of the Thirtieth International Joint Confer-\nence on Artificial Intelligence, IJCAI-21, pages 3936–\n3942. International Joint Conferences on Artificial\nIntelligence Organization.\nDuyu Tang, Bing Qin, Xiaocheng Feng, and Ting Liu.\n2015. Effective lstms for target-dependent sentiment\nclassification. arXiv preprint arXiv:1512.01100.\nDuyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu,\nBing Qin, et al. 2014. Learning sentiment-specific\nword embedding for twitter sentiment classification.\nIn ACL (1), pages 1555–1565. Citeseer.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nGökhan Tür, Dilek Hakkani-Tür, and Larry P. Heck.\n2010. What is left to be understood in atis? In 2010\nIEEE Spoken Language Technology Workshop, SLT\n2010, Berkeley, California, USA, December 12-15,\n2010, pages 19–24. IEEE.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nBoxin Wang, Hengzhi Pei, Boyuan Pan, Qian Chen,\nShuohang Wang, and Bo Li. 2020. T3: Tree-\nautoencoder constrained adversarial text generation\nfor targeted attack. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 6134–6150, Online. As-\nsociation for Computational Linguistics.\nLei Xu, Ivan Ramirez, and Kalyan Veeramachaneni.\n2020. Rewriting meaningful sentences via condi-\ntional bert sampling and an application on fooling\ntext classifiers. arXiv preprint arXiv:2010.11869.\nYing Xu, Xu Zhong, Antonio Jimeno Yepes, and\nJey Han Lau. 2021. Grey-box adversarial attack and\ndefence for sentiment classification. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 4078–4087,\nOnline. Association for Computational Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. Advances in neural informa-\ntion processing systems, 32.\n7674\nHuangzhao Zhang, Hao Zhou, Ning Miao, and Lei Li.\n2019. Generating fluent adversarial examples for\nnatural languages. In Proceedings of the 57th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 5564–5569, Florence, Italy. Asso-\nciation for Computational Linguistics.\nXiang Zhang, Junbo Zhao, and Yann LeCun. 2015.\nCharacter-level convolutional networks for text clas-\nsification. In Advances in Neural Information Pro-\ncessing Systems, volume 28. Curran Associates, Inc.\nXin Zhang, Guangwei Xu, Yueheng Sun, Meishan\nZhang, Xiaobin Wang, and Min Zhang. 2022. Iden-\ntifying chinese opinion expressions with extremely-\nnoisy crowdsourcing annotations. In Proc. of ACL,\npages 2801–2813.\nXin Zhang, Guangwei Xu, Yueheng Sun, Meishan\nZhang, and Pengjun Xie. 2021. Crowdsourcing learn-\ning as domain adaptation: A case study on named\nentity recognition. In Proc. of ACL , pages 5558–\n5570.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D Manning. 2017. Position-aware\nattention and supervised data improve slot filling. In\nConference on Empirical Methods in Natural Lan-\nguage Processing.\nWei Zou, Shujian Huang, Jun Xie, Xinyu Dai, and Jiajun\nChen. 2020. A reinforced generation of adversarial\nexamples for neural machine translation. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 3486–3497, On-\nline. Association for Computational Linguistics.\nA Attachable subwords analysis\nSubwords len Num in Vocab Potential Num Ratio\n1 26 26 1\n2 438 676 0.65\n3 1438 17576 0.08\n4 1573 456976 0.03\n5 695 11876696 0.5 ×10-5\nTable 8: The subword number of different lengths in the\nvocabulary.\nTo better illustrate the principles of adversarial\ntokenization, we list the statistics for the number of\nattachable subwords with different lengths in Table\n8. We can see that with the length increases, the\nproportion of subwords in vocabulary among all\npotential subwords with the same length is getting\nsmaller. For example, all the 26 attachable sub-\nwords with length 1 (#a - #z) are in the vocabulary\nlist, but some subwords with length 2 (#rz) doesn’t.\nBased on these observations, we conclude that\nthe longer subtokens are more difficult to combine\nwith characters to form new subwords, which is the\nprinciple of the adversarial tokenization module.\nB Details of baselines\nToken classification task. Token classification is\na natural language understanding task in which a\nlabel is assigned to some tokens in the text. Some\npopular token classification subtasks are Named\nEntity Recognition (NER) (Zhang et al., 2021;\nShen et al., 2021; Tan et al., 2021; Shen et al.,\n2022; Zhang et al., 2022) and Slot Filling (Chen\net al., 2019; Zhang et al., 2017).\nSince the token classification model generates\na label for each token, the token itself cannot be\nreplaced, and the structure of the sentence also can-\nnot be modified. Therefore, neither sentence-level\n(Xu et al., 2020) nor word-level attacks (Eger and\nBenz, 2020) can be applied to the token classifica-\ntion task, only character-level attacks are available\nin this scenario.\nThe current character-level attack methods can\nbe divided into two categories. The first class of\nmethods performs a white-box attack against a\nmodel taking characters as input. The most rep-\nresentative methods is HotFlip (Ebrahimi et al.,\n2018b). Other works are mainly variants of Hot-\nFlip (Ebrahimi et al., 2018a; Gil et al., 2019). An-\nother class of methods performs black-box attacks\non the model, where main representative methods\nare DeepWordBug (Gao et al., 2018) and Zéroe\n(Eger and Benz, 2020). These methods do not re-\nquire the input to the model to be characters. Since\nour approach attacks models with word-level input,\nwe mainly take DeepWordBug and Zéroe as our\nbaselines.\nSentence classification task Sentence classifica-\ntion is one of the simplest NLP tasks in the Natural\nLanguage Processing field that have a wide range\nof applications including sentiment analysis (Tang\net al., 2015, 2014) and relation extraction (Hu et al.,\n2021a,b, 2020; Liu et al., 2022).\nSince the sentence classification method outputs\na label for the whole sentence, both word-level at-\ntacks and character-level attacks can be performed\non it. Word level attack method is the most widely\nused text attack method, which can be classified\ninto the black-box method and white-box method.\nThe classical black-box approach mainly uses a\nrule-based approach to do synonym replacement,\nof which the most representative isTextFooler(Jin\net al., 2020). Recent methods like BERT-Attack\n(Li et al., 2020) and BAE (Garg and Ramakrishnan,\n2020) replace words in context with the help of se-\nmantic information from the pre-trained model (De-\n7675\nvlin et al., 2019). The representative work of white-\nbox attack is GBDA (Guo et al., 2021), which per-\nforms token replacement under the gradient guid-\nance. In this work, we adopt token-level attack\nmethods TextFooler, BAE, BERT-Attack, GBDA\nand character-level attack method DeepWordBug\nas our baselines.\n7676",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.70675128698349
    },
    {
      "name": "Transformer",
      "score": 0.7010643482208252
    },
    {
      "name": "Security token",
      "score": 0.6464143991470337
    },
    {
      "name": "Gumbel distribution",
      "score": 0.514116108417511
    },
    {
      "name": "Adversarial system",
      "score": 0.5064023733139038
    },
    {
      "name": "Artificial intelligence",
      "score": 0.49372610449790955
    },
    {
      "name": "Character (mathematics)",
      "score": 0.45691874623298645
    },
    {
      "name": "Sentence",
      "score": 0.43939870595932007
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3472054600715637
    },
    {
      "name": "Algorithm",
      "score": 0.3455487787723541
    },
    {
      "name": "Natural language processing",
      "score": 0.32342082262039185
    },
    {
      "name": "Mathematics",
      "score": 0.19714677333831787
    },
    {
      "name": "Computer security",
      "score": 0.09135589003562927
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Statistics",
      "score": 0.0
    },
    {
      "name": "Extreme value theory",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ],
  "cited_by": 19
}