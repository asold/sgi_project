{
  "title": "Are Large Language Models All You Need for Task-Oriented Dialogue?",
  "url": "https://openalex.org/W4389010465",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2474487673",
      "name": "Vojtech Hudecek",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2560115621",
      "name": "Ondrej Dusek",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3104405162",
    "https://openalex.org/W1947758080",
    "https://openalex.org/W2970558499",
    "https://openalex.org/W3174076858",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W4389009408",
    "https://openalex.org/W3015217416",
    "https://openalex.org/W2962934384",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W2998702515",
    "https://openalex.org/W3100806282",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2997771882",
    "https://openalex.org/W3049346316",
    "https://openalex.org/W4294974598",
    "https://openalex.org/W2101105183",
    "https://openalex.org/W4287887270",
    "https://openalex.org/W4245816780",
    "https://openalex.org/W4389519158",
    "https://openalex.org/W3185827732",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4389009381",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2998201756",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W4312205996",
    "https://openalex.org/W3100128199",
    "https://openalex.org/W4306803097",
    "https://openalex.org/W2970579055",
    "https://openalex.org/W4285225959",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2798914047",
    "https://openalex.org/W3156697766",
    "https://openalex.org/W3206072865",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W2594726847",
    "https://openalex.org/W2888849322",
    "https://openalex.org/W4229005866",
    "https://openalex.org/W2973379954",
    "https://openalex.org/W4297633153",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2798463315"
  ],
  "abstract": "Instruction-finetuned large language models (LLMs) gained a huge popularity recently, thanks to their ability to interact with users through conversation. In this work, we aim to evaluate their ability to complete multi-turn tasks and interact with external databases in the context of established task-oriented dialogue benchmarks. We show that in explicit belief state tracking, LLMs underperform compared to specialized task-specific models. Nevertheless, they show some ability to guide the dialogue to a successful ending through their generated responses if they are provided with correct slot values. Furthermore, this ability improves with few-shot in-domain examples.",
  "full_text": "Proceedings of the 24th Meeting of the Special Interest Group on Discourse and Dialogue, pages 216–228\nSeptember 11–15, 2023. ©2023 Association for Computational Linguistics\n216\nAre Large Language Models All You Need for Task-Oriented Dialogue?\nVojtˇech Hudeˇcek and Ondˇrej Dušek\nCharles University, Faculty of Mathematics and Physics\nMalostranské námˇestí 25, 118 00 Prague, Czechia\nhudecek@ufal.mff.cuni.cz, odusek@ufal.mff.cuni.cz\nAbstract\nInstruction-finetuned large language models\n(LLMs) gained a huge popularity recently,\nthanks to their ability to interact with users\nthrough conversation. In this work, we aim\nto evaluate their ability to complete multi-turn\ntasks and interact with external databases in\nthe context of established task-oriented dia-\nlogue benchmarks. We show that in explicit\nbelief state tracking, LLMs underperform com-\npared to specialized task-specific models. Nev-\nertheless, they show some ability to guide the\ndialogue to a successful ending through their\ngenerated responses if they are provided with\ncorrect slot values. Furthermore, this ability\nimproves with few-shot in-domain examples.\n1 Introduction\nLarge Language Models (LLMs) have transformed\nthe NLP field, showing outstanding performance\nacross many NLP benchmarks such as Winograd\nChallenge (Levesque et al., 2012) or GLUE (Wang\net al., 2018). Recently, instruction finetuning of\nLLMs proved to be able to align the model out-\nputs with human preferences (Ouyang et al., 2022;\nWang et al., 2022) and improved the LLMs’ com-\nmunication capabilities substantially. State-of-the-\nart LLMs are not only good at understanding user\nneeds but also capable of providing relevant an-\nswers. Consequently, we see many chatbot appli-\ncations both inside and outside academia (Chat-\nGPT1, Claude2, Sparrow3) which build upon the\nraw power of instruction-finetuned LLMs.\nGiven the millions of daily interactions with\nthese chatbots, it appears that the models are able to\nhandle users’ needs to their satisfaction, at least to\nsome extent. However, these chatbots are tuned us-\ning unstructured open-domain conversations. The\n1https://openai.com/blog/chatgpt\n2https://www.anthropic.com/index/\nintroducing-claude\n3https://www.deepmind.com/blog/\nbuilding-safer-dialogue-agents\nI am looking for a\nfive-star hotel in\nthe north\nContext\nEncoder\nPrompt\nconstruction\nLLM\nState tracker\nLLM\nGenerator\nWe've got 4 hotels\navailable\nContext\nStore\nDB\nFigure 1: A high-level overview of our proposed\npipeline. The user input is used to retrieve relevant few-\nshot examples (if available). Then, an initial prompt is\nconstructed and an LLM is asked to provide the current\ndialogue state. Based on that, we retrieve database re-\nsults and construct another prompt. Finally, we ask the\nLLM to provide the response.\naim of this paper is to evaluate these systems\nfor more specific applications, where the system\nhas to follow a predetermined structure and han-\ndle external sources of information, such as APIs\nor databases. We raise the question to what ex-\ntent LLMs are capable of handling these applica-\ntions off-the-shelf, i.e. without finetuning. We thus\nchoose to evaluate LLM performance in the task-\noriented dialogue (TOD) setting, as it requires pre-\ncise information handling for communicating with\nexternal APIs. Moreover, TOD systems output\nin-domain information which has predetermined\nstructure and lends itself well to evaluation, thanks\nto pre-existing annotated data sets. We avoid any\nfinetuning techniques and focus on zero-shot or\nfew-shot settings using in-context learning, as this\napproach has lower hardware requirements and bar-\nrier of entry and better flexibility or even perfor-\nmance in certain tasks (Su et al., 2022).\nTherefore, we introduce an LLM-based TOD\n217\nI am looking for a\nfive-star hotel in\nthe north\nLLM\nState tracker\nDB\nContext\nStore\nContext\nEncoder\nRetrieved examples\nUser Input\nHistory\nRetrieved examples\nUser Input\nHistory\nState\nDatabase\n(0) \n(1) (2) \nLLM\nGenerator\nWe've got 4 hotels\navailable.\n(3)\nLLM\nDom. Detection\nFew-shot\nZero-shot\nFigure 2: A detailed description of our proposed pipeline. (0) As a preprocessing step, we encode a subset of\nthe training set that will be used to retrieve few-shot examples. Given the user input, we: (1) Detect the domain,\nretrieve relevant examples (in the few-shot setting) and construct an initial prompt. (2) Infer the belief state using\nLLM. Based on that, we retrieve database information and construct another prompt that includes both the state and\ndatabase results. (3) We ask the LLM to provide a final response.\nconversation pipeline (see Figure 1) and evaluate\nits performance with respect to commonly used\ntask-oriented metrics such as Joint Goal Accuracy,\nSlot F1, and Dialogue Success (Rastogi et al., 2018;\nBudzianowski et al., 2018). Our pipeline resembles\nother approaches based on LMs (Peng et al., 2021;\nYang et al., 2021), using state tracking and response\ngeneration as two main, separate steps, while keep-\ning the role of a dialogue policy implicit. However,\ninstead of finetuning LMs, it intentionally relies al-\nmost exclusively on the usage of pretrained LLMs\nas-is, so we can test their out-of-the-box capabili-\nties. The dialogue context and domain description\nare introduced to the model only by including them\nin the input prompt. In the zero-shot setting, the\nmodel receives a domain description only; in the\nfew-shot setting, it additionally uses a few retrieved\nexamples (see Section 3 for details).\nIn our experiments, we find that LLMs are not\nvery good at state tracking and their performance\nfalls behind the state-of-the-art. However, if pro-\nvided with correct belief states, some of them yield\ninteresting response generation performance, com-\nparable to earlier finetuned state-of-the-art models.\nTo our knowledge, our zero-shot experiments estab-\nlish a state-of-the-art result in unsupervised TOD\nmodeling on the MultiWOZ and Schema-guided\ndatasets (Budzianowski et al., 2018; Rastogi et al.,\n2020). While there may be room for improvement\nthrough prompt engineering, our results aim to\nshow the out-of-the-box LLM capabilities. We\nplan to release our experimental code on GitHub.4\n4https://github.com/vojtsek/to-llm-bot\n2 Related Work\nLarge Language Models The Transformer archi-\ntecture (Vaswani et al., 2017) enabled the training\nof large and capable language models. The research\non their few-shot and zero-shot abilities dates back\nto the GPT-2 and GPT-3 models (Radford et al.,\n2019; Brown et al., 2020), which are scaled ver-\nsions of the Transformer decoder. Many followed\nthis path of training large Transformer decoders\n(Zhang et al., 2022; Black et al., 2022), yielding\nmodels of up to hundreds of billions parameters\nin size (Zhao et al., 2023). Other models leverage\nthe whole original (encoder-decoder) Transformer\narchitecture (Raffel et al., 2020; Lewis et al., 2020).\nRecent research focuses on improving the training\nof moderate-sized architectures to broaden access\nto highly capable LLMs (Touvron et al., 2023).\nInstruction Tuning The idea of using reinforce-\nment learning techniques to align model-based\nagents better with users’ intents was pioneered in\ngame agent development (Christiano et al., 2017)\nand later explored for training language models\n(Ziegler et al., 2019; Ouyang et al., 2022). Al-\nthough these techniques proved to be quite effec-\ntive, the process is still very demanding in terms\nof collecting feedback from users. Consequently,\nseveral datasets were proposed (Wang et al., 2022;\nIyer et al., 2022; Black et al., 2022) that collected\nmillions of instructions-based tasks in natural lan-\nguage and can be applied to finetune LLMs using\nreinforcement learning.\nLM-based TOD modeling Task-oriented dia-\nlogue modeling with pretrained LMs was intro-\nduced by Zhang et al. (2019) and Peng et al. (2021),\nwho followed text-based state encoding and two-\n218\nstage generation proposed by Lei et al. (2018): An\nLM is first used to decode a structured belief state,\nrepresented as text. The belief state is then used\nto retrieve database information and the LM is\ncalled once more to generate a response, condi-\ntioned on the belief state and retrieved information.\nSeveral improvements to the basic setup were pro-\nposed, such as contrastive state training (Kulhánek\net al., 2021) or using belief state differences (Lin\net al., 2020). Others proposed a combination of\ngenerative models with retrieval-based approaches\n(Pandey et al., 2018; Cai et al., 2019; Nekvinda and\nDušek, 2022). All described works finetune LMs\non in-domain data, which is in contrast with the\npure in-context learning approach that we apply.\nFew-shot dialogue modeling One of the first\nneural models focusing on learning dialogue from a\nfew in-domain examples was the Hybrid Code Net-\nworks (Williams et al., 2017), a trainable system\nbased on recurrent neural networks, with partially\nhandcrafted components. Another approach was\nproposed by Zhao and Eskenazi (2018), who used\nlatent action representations to enable the transfer\nof domain knowledge. Latent actions were also\nused by Huang et al. (2020) and Shalyminov et al.\n(2019). More recent approaches leverage the capa-\nbilities of pretrained Transformer LMs (Shalymi-\nnov et al., 2020). Hu et al. (2022) used LLMs and\nin-context learning to perform belief state tracking,\nformulating the task as an SQL query generation.\nUnlike our work, they did not use instruction-tuned\nmodels and omitted database retrieval and response\ngeneration.\n3 Method\nWe introduce our method step-by-step. An overall\ndescription of the proposed pipeline is shown in\nFigure 2. The system consists of a pretrained LLM\nand an (optional) context store in a vector database.\nThree LLM calls are performed in each dialogue\nturn, with specific prompts (see Section 3.1). First,\nthe LLM performs domain detection and state track-\ning (Section 3.2). The updated belief state informs\na database query, whose results are used in the\nsubsequent LLM-based response generation step\n(Section 3.3). In the few-shot setting, the context\nstore is used to store a limited number of examples\nfrom the training set, which are retrieved based\non similarity with the conversation context and in-\ncluded in LLM prompts (see Section 3.4).\nPrompt Definition: Capture values from a conversation\nabout hotels.\nCapture pair \"entity:value\" separated by colon\nand no spaces in between.\nSeparate the \"entity:value\" pairs by hyphens\nValues that should be captured are:\n- \"pricerange\": the price of the hotel\n...\n[history]\nCustomer: \"I want a cheap place to stay.\"\nOutput: pricerange:\"cheap\"\nTable 1: A simplified example of a zero-shot version of\nthe prompt used for state update prediction. It contains\ntask definition, domain description, dialogue history and\nuser utterance. For the exact prompts see Appendix.\n3.1 Prompt construction\nWe aim to compare the raw capabilities of the\nselected LLMs, therefore we do not focus on\nprompt engineering techniques and choose uni-\nversal prompts used for all LLMs in this work\n(cf. Section 8). We choose simple, plain language\nstatements as prompts, with no specific vocabulary,\nbased only on a few preliminary tests. We define a\nsingle domain detection prompt for all examples,\nplus a pair of prompts for each domain in the given\ndataset: a state tracking prompt (see Table 1) and\na response prompt.\nThe domain detection prompt includes a task de-\nscription and two static examples of domain detec-\ntion. In addition to general instructions, each state\ntracking prompt contains a domain description, a\nlist of relevant slots, the dialogue history, and the\ncurrent user utterance. The response prompts do\nnot contain the per-domain slot list, but they in-\nclude the current belief state and database results\ninstead. In the few-shot setting, each tracking and\nresponse prompt additionally contains positive and\nnegative examples retrieved from the context store\n(see Section 3.4). Prompt examples are shown in\nTables 5 and 6 in the Appendix.\n3.2 Domain Detection and State Tracking\nWe prompt the LM twice at each turn during state\ntracking: first, to detect the active domain, then to\noutput slot values that changed or appeared in the\ncurrent turn. We then use the outputs to update the\naccumulated global belief state.\nThe two prompting steps are used since we need\nthe models to operate in a multi-domain setting, i.e.,\nhandle conversations spanning multiple domains.\nTherefore, we need to be able to detect the currently\n219\nactive domain. We achieve this by first prompting\nthe LLM with a domain detection prompt (using a\nsingle prompt for all examples).\nOnce we obtain the active domain prediction,\nwe can include manually designed domain descrip-\ntions in a second prompt that handles belief state\nprediction. An example of a prompt used for state\ntracking is provided in Table 1. For the few-shot\nvariants, we retrieve few-shot examples from the\ncontext store, limited to the active domain.5\nOur preliminary experiments showed that LLMs\nstruggle to output all active slot values at every\nturn consistently. Therefore, we model only state\nupdates, following the MinTL approach (Lin et al.,\n2020). Here, the model only generates the slot-\nvalue pairs that have changed in current turn. The\nglobal belief state is then accumulated using these\nturn-level updates. To obtain machine-readable\noutputs useful for database queries or API calls,\nwe specify in the prompt that the model should\nprovide JSON outputs, and any provided few-shot\nexamples are formatted accordingly.\n3.3 Response Generation\nThe current belief state is used to query the\ndatabase for entries matching all user-specified\nslots in the active domain. Given the belief state\nand database results, the response generation is\nstraightforward. The prompt for the LLM includes\ndialogue history, user utterance, belief state and\ndatabase results (and retrieved examples in the few-\nshot setting) and requests the model to provide a\nfitting system response. We generate delexicalized\nresponses (Wen et al., 2015), i.e., we replace slot\nvalues by placeholders, following prior work in\nend-to-end TOD modeling. In addition to simpli-\nfying the task for the model, delexicalized outputs\nallow us to evaluate the success rate and compare\nto previous works. The prompt specifies that the\nmodel should provide entity values as delexical-\nized placeholders, and any few-shot examples are\nconstructed accordingly.\n3.4 Context Storage\nIt has been shown that enriching prompts with spe-\ncific examples boosts LM performance (Madotto\net al., 2020; Brown et al., 2020). To apply this\nknowledge efficiently in our pipeline, we introduce\na storage that contains encoded dialogue contexts.\n5For this purpose, each conversation snippet contained in\nthe context store comes from a single-domain conversation.\nThis context storage is optional and is only required\nfor the few-shot prompting variant. We use di-\nalogue context taken from a fixed-length history\nwindow as the key to be encoded in the vector\ndatabase. More details can be found in Section 4.4.\nOnce the relevant examples are retrieved, we in-\nclude them in the prompt to guide the model better.\nSome of the LLMs rely on negative (counter-) ex-\namples as well (Wang et al., 2022). Therefore, we\nfollow Peng et al. (2021)’s consistency classifica-\ntion task approach to produce negative examples:\nWe take some of the retrieved belief state examples,\ncorrupt them by replacing some of the correct slot\nvalues with random values, and present them as\nnegative in the prompt.\n4 Experimental Setup\nTo obtain a broad overview of the current LLMs’\ncapabilities, we compare several models, spanning\ndifferent numbers of trainable parameters and dif-\nferent training methods. We also experiment with\nfour variants of the base setup, using either zero-\nshot or few-shot operations and using either pre-\ndicted or oracle belief states.\n4.1 Datasets\nWe experiment with two of the currently most\nprominent benchmark datasets for task-oriented\nmulti-domain dialogue:\n• MultiWOZ 2.2 (Budzianowski et al., 2018;\nHung et al., 2022) is a well-known benchmark\nused for evaluating state tracking, response\ngeneration and dialogue success rate. Its eval-\nuation is well-defined and the dataset contains\ndatabase files, so full interaction can be sim-\nulated. It contains over 10k dialogues, 7 do-\nmains and 29 distinct slots.\n• Schema Guided Dataset (Rastogi et al.,\n2020) is also well annotated and even richer\ndataset containing more than 22k dialogues\n18 domains and 145 slots. Database interac-\ntion is considered in the dataset, but no real\ndatabase is provided and database results are\ndefined ad-hoc. Therefore we simply use the\nprovided database results in the prompts with-\nout performing any actual queries.\n4.2 Tested Models\nWe chose the following five instruction-finetuned\nmodels for our experiments, spanning different\n220\nmodel few oracle Schema Guided Dialogues MultiWOZ 2.2\nshot BS BLEU JGA Slot-F1 Success BLEU JGA Slot-F1 Success\nSupervised SotA ✗ ✗ 29.90∗ 0.30† 0.60∗ – 19.90♣ 0.60♢ – 0.82 ♡\nAlpaca-LoRA-7B-zs-gbs ✗ ✗ 2.79 0.02 0.01 0.11 1.61 0.06 0.07 0.04\nTk-Instruct-11B-zs-gbs ✗ ✗ 4.16 0.05 0.03 0.10 2.48 0.04 0.04 0.04\nGPT-NeoXT-20B-zs-gbs ✗ ✗ 0.45 0.01 0.01 0.17 0.52 0.03 0.02 0.04\nOPT-IML-30B-zs-gbs ✗ ✗ 1.63 0.01 0.01 0.17 0.56 0.02 0.04 0.03\nChatGPT-zs-gbs ✗ ✗ – – – – 4.17 0.13 0.40 0.31\nAlpaca-LoRA-7B-zs-obs ✗ ✓ 2.76 – – 0.23 1.73 – – 0.08\nTk-Instruct-11B-zs-obs ✗ ✓ 5.21 – – 0.24 2.66 – – 0.18\nGPT-NeoXT-20B-zs-obs ✗ ✓ 0.83 – – 0.22 0.60 – – 0.06\nOPT-IML-30B-zs-obs ✗ ✓ 1.94 – – 0.22 0.54 – – 0.06\nChatGPT-zs-obs ✗ ✓ – – – – 3.76 – – 0.47\nAlpaca-LoRA-7B-fs-gbs ✓ ✗ 6.32 0.04 0.01 0.09 5.53 0.06 0.08 0.06\nTk-Instruct-11B-fs-gbs ✓ ✗ 6.66 0.06 0.05 0.10 6.56 0.16 0.33 0.19\nGPT-NeoXT-20B-fs-gbs ✓ ✗ 1.62 0.04 0.02 0.09 2.73 0.05 0.04 0.05\nOPT-IML-30B-fs-gbs ✓ ✗ 0.82 0.06 0.07 0.08 4.40 0.03 0.03 0.04\nChatGPT-fs-gbs ✓ ✗ – – – – 6.77 0.27 0.51 0.44\nAlpaca-LoRA-7B-fs-obs ✓ ✓ 6.99 – – 0.25 5.96 – – 0.41\nTk-Instruct-11B-fs-obs ✓ ✓ 8.56 – – 0.25 6.91 – – 0.46\nGPT-NeoXT-20B-fs-obs ✓ ✓ 1.97 – – 0.24 2.92 – – 0.28\nOPT-IML-30B-fs-obs ✓ ✓ 0.56 – – 0.22 5.40 – – 0.28\nChatGPT-fs-obs ✓ ✓ – – – – 6.84 – – 0.68\nTable 2: Evaluation of the chosen LLMs with respect to widely used TOD measures. For each model, we provide\nmultiple variants. We use either zero-shot or few-shot prompts (-zs- vs. -fs-) and either generated or oracle belief\nstate (-gbs vs. -obs). The few-shot variants use 10 examples per domain in the context storage ( ∼0.6% of the\ntraining set in case of MultiWOZ), two of which are selected for the prompts. To reduce cost, we only evaluate\nthe paid ChatGPT model on MultiWOZ. We also provide supervised state-of-the-art results to put the numbers in\ncontext: ∗Zhu et al. (2022), †Feng et al. (2021), ♣Sun et al. (2022), ♢Huang et al. (2023), ♡Feng et al. (2023).\nsizes (within the limitations of hardware available\nto us) and using freely available models as well as\nthe paid ChatGPT API. We indicate the specific\nmodel variant (i.e., model size, given by the num-\nber of parameters) directly in the model name.\n• Tk-Instruct-11B (Wang et al., 2022) is based\non the T5 encoder-decoder architecture (Raf-\nfel et al., 2020). It was tuned on a dataset of\nover 5M task instances with instructions.\n• ChatGPT is a product introduced by Ope-\nnAI.6 Although the exact training process and\narchitectures were not published, it most prob-\nably uses a similar architecture and finetun-\ning techniques as InstructGPT (Ouyang et al.,\n2022), with additional human feedback.\n• Alpaca-LoRA-7B is a version of the LLaMa\nmodel (Touvron et al., 2023) using the LoRA\nmethod (Hu et al., 2021) for finetuning on\nStanford Alpaca project data (Taori et al.,\n2023). LoRa keeps the base model parame-\nters frozen, but adds additional smaller weight\nmatrices to the model to transform its outputs.\n6https://openai.com/blog/chatgpt\n• GPT-NeoXT-Chat-Base-20B is based on\nthe GPT-NeoX open-source language model\n(Black et al., 2022) and finetuned with over\n40M dialogue-style instructions.\n• OPT-IML-30B (Iyer et al., 2022) is based on\nthe Transformer decoder OPT model (Zhang\net al., 2022) and trained with a custom set of\ninstructions, including the finetuning set from\nTk-Instruct.\n4.3 Evaluated variants\nWe test four variants of our setup for each pair of\nmodel and dataset. Specifically, we use zero-shot\n(without examples) or few-shot (including exam-\nples) prompts (-zs- vs. -fs-) and either generated or\noracle belief states (-gbs vs. -obs). For retrieval in\nthe few-shot setting, we store just 10 examples per\ndomain in the context store by default. We exper-\niment with increasing this number in Section 5.4.\nUsing oracle belief state allows us to focus on eval-\nuating the LLM’s ability to guide the dialogue.\n221\nFigure 3: Domain detection accuracy with respect to dif-\nferent models for MultiWOZ 2.2 and SGD data wchich\nconsist of 7 and 18 domains, respectively.\n4.4 Experiment Details\nDue to the expensiveness of the LLM runs, 7 we\ndid not perform a grid search, but used a limited\nset of preliminary experiments to determine hyper-\nparameters. Based on this, we used the context of\ntwo preceding utterances (user + system) as the\ncontext store keys (cf. Section 3.4). We retrieve\ntwo examples for few-shot prompts and make one\ncorrupted variant from each of them for negative\nexamples. To corrupt an example, we switch some\nof the slot values randomly, similarly to Kulhánek\net al. (2021). In the context store, we encode few-\nshot examples using the multilingual embedding\nmodel provided by Reimers and Gurevych (2020)8\nand store them in the FAISS database (Johnson\net al., 2019). To perform the LLM calls, we use the\nHuggingface library9 and the OpenAI API.10\n4.5 Evaluation Measures\nWe evaluate the system outputs on multiple levels,\nboth using automatic metrics and human evaluation.\nResults are given in Sections 5 and 6, respectively.\nAutomatic Metrics\nIn automatic evaluation, we first follow the LLM\ncalls being made and evaluate domain detection,\nstate tracking as well as response generation. We\nalso evaluate the overall dialogue-level perfor-\nmance. For domain detection, we simply compute\n7Hardware intensity for the freely available models and\nactual cost for ChatGPT.\n8https://huggingface.co/sentence-transformers/\nall-mpnet-base-v2\n9https://huggingface.co\n10https://platform.openai.com\ndetection accuracy as a ratio of correctly detected\ndomain out of all dialogue turns being processed.\nFor state tracking, we compute micro-F1 score and\nJoint Goal Accuracy (JGA). JGA is computed as\nthe ratio of dialogue turns for which the predicted\nbelief state matches the ground truth. We use fuzzy\nmatching of the slot values, so that capitalization or\nminor typos do not influence the result. To evaluate\nresponse generation, we follow related works and\nuse BLEU score (Papineni et al., 2002).\nThe main overall measure for evaluating a task-\noriented dialogue is the dialogue success rate (De-\nriu et al., 2021). For MultiWOZ, we use the stan-\ndard evaluation of dialogue success as the ratio of\ndialogues where the user reaches the desired goal,\nbased on goal annotation provided with the data\n(Nekvinda and Dušek, 2021). The SGD dataset\ndoes not include goal annotation but contains in-\nformation about the requested slots. Therefore, we\ncompute SGD success rate as the proportion of di-\nalogues in which (1) the system captures all the\nslots correctly and (2) all the requested slots are\nprovided.\nHuman Evaluation\nFor human evaluation, we perform a small-scale\nin-house interaction study on MultiWOZ. Since\nthe MultiWOZ goal often involves tasks in multi-\nple domains, we ask annotators to evaluate each\ndomain in the dialogue distinctly. At the end of\neach dialogue, the annotators are asked to answer\nthese questions:\n1. How many of the subdialogues/domains were\nhandled successfully? (corresponding to dia-\nlogue success)\n2. How many clarifications or corrections were\nneeded?\n3. Was all the provided information captured cor-\nrectly? (corresponding to JGA)\n5 Automatic Metrics Results\n5.1 Domain detection\nWe report the domain detection accuracy on Mul-\ntiWOZ and SGD in Figure 3. We observe that the\ndomain detection accuracy varies quite a lot for var-\nious models and presumably influences the quality\nof the retrieved few-shot examples and appropriate-\nness of the subsequent prompts. However, it is im-\nportant to note that domain detection is turn-based,\n222\nFigure 4: The influence of using oracle domain to re-\ntrieve examples. Interestingly, the oracle domain does\nnot improve the performance, suggesting that the model-\nbased detection is good enough for retrieval.\nand arguably there are situations (e.g. providing an\naddress, saying goodbye etc.) that are always han-\ndled in the same fashion, even though they formally\nbelong to different domains. Therefore, not all the\nretrieved examples from misclassified domains nec-\nessarily contain unrelated contexts. To explore this,\nwe measure the performance of all models in case\nan oracle domain is given to them (Figure 4). Inter-\nestingly, using the oracle domain did not improve\nperformance, it even worsened in some cases. This\nsuggests that the model-predicted domain is gener-\nally good enough, and additionally providing the\ndomain information does not contribute to the fi-\nnal system performance. The negative influence\non performance might be caused by forcing the\nsystem to filter out relevant examples. We observe\nthat in multiple cases, the conversations snippets\nare domain-independent so the retrieval might per-\nform better even with a wrongly selected domain.\nForcing the ground truth domain examples in these\ncases can be potentially harmful.\n5.2 Belief State Tracking\nThe belief state tracking results overview is given\nin Table 2 ( JGA and Slot-F1). There is a huge\ngap between the supervised models’ performance\nand the LLM results. Also compared to Hu et al.\n(2022), who used few-shot in-context learning and\nreported JGA 43.13% with a comparable dataset\nsize, our instruction-tuned LLMs fall short. How-\never, the models we use are an order of magnitude\nFigure 5: The influence of the number of examples per\ndomain available for few-shot retrieval and performance\nof the model in terms of the dialogue success on Multi-\nWOZ 2.2 data with oracle state supplied. Note that this\ndoes not represent the number of examples selected for\nthe prompt, which is fixed to two.\nsmaller in general, and we also use fewer examples\nin the prompt. We hypothesize that the perfor-\nmance could be further improved by careful model-\nspecific prompt customization and perhaps task\nre-formulation; nevertheless, this is not the goal of\nthis work. We intentionally focus on the universal\nframing of the task since we want to explore the\ngeneral ability of the models to follow instructions.\nWhen comparing the results among the models,\nChatGPT clearly outperforms the rest of the models\nby a large margin. Interestingly, the few-shot vs.\nzero-shot setting does not seem to influence the\nresults much, except for the GPT-NeoXT model.\n5.3 Response Generation\nBLEU scores are low overall, far below the super-\nvised state-of-the-art. Tk-Instruct and ChatGPT are\nthe strongest here and perform roughly on par.\n5.4 Dialogue-level performance\nResults for dialogue success are provided in Ta-\nble 2, and there is again a large gap between LLMs\nand supervised custom models’ performance. Chat-\nGPT seems to outperform other models, similarly\nto state tracking (cf. Section 5.2). However, for\nsome cases, especially in the zero-shot setting, the\ndifference is not that obvious. In most cases, adding\nthe retrieved few-shot examples helps. The contri-\nbution of retrieved examples is more obvious when\nwe supply the oracle belief state, in which case it\nhelps consistently for all the models.\n223\nWe also explore the influence of context stor-\nage size on the dialogue success rate. The results\nare given in Figure 5. It seems that the biggest\nimprovement can be achieved by supplying just a\nfew examples instead of zero-shot prompting, but\nincreasing the size of the example pool for retrieval\ndoes not yield further performance gains.\n6 Model Analysis\n6.1 Human Evaluation\nWe employed 6 annotators with a background in\nlinguistics and NLP and let them interact with the\ntwo strongest models in terms of automatic met-\nrics: ChatGPT and Tk-Instruct. The annotators\nwere given randomly selected goals from the Mul-\ntiWOZ 2.2 dataset and a minimal set of essential\ninstructions on how to proceed. We present the re-\nsults in Table 3. We can see that in real interaction\nwith a human user and allowing for clarification\nor correction, the models perform better compared\nto the rather strict automatic evaluation. Further-\nmore, the models are often successful in multiple\nsub dialogues, even if a part of the whole dialogue\nfails. The experiment also confirms the superior\nperformance of ChatGPT on both dialogue success\nand JGA. Not surprisingly given the above results,\nconversations with ChatGPT also required fewer\nclarification turns than with Tk-Instruct.\n6.2 Error Analysis\nTo understand the models’ behavior better, we man-\nually inspect a random sample of ca. 20 dialogues\nfor each model, chosen from cases where the au-\ntomatic success metric was not satisfied. In gen-\neral, we can split most of the erroneous behaviors\ninto two distinct groups, which we call prompt-\nrecoverable and inherent.\nPrompt-recoverable errors can be likely fixed\nby specific prompt engineering with some effort.\nThese errors happen with all of the tested models.\nExamples of such errors are the invalid structure\nof the generated dialogue state, copying slot values\ninstead of using canonical values from the ontology,\nfailure to delexicalize some of the values, etc. Most\nof these errors can be also fixed in postprocessing\n– for example, we can employ more robust parsers\nor fuzzy matching of slot values.\nInherent errors, on the other hand, are likely not\neasily fixable by prompt modifications. They are\nChatGPT Tk-Instruct\ndialogues 25 25\nsubdialogues 52 48\nclarify / dial 1.08 1.68\nsuccesful subdialogues 81% 71%\nsuccesful dialogues 76% 64%\ncorrectly captured 88% 66%\nTable 3: Human evaluation results for ChatGPT and\nTk-Instruct-11B models. We evaluate the conversation\non sub dialogue level i.e. each domain in the dialogue\nis evaluated separately.\nnot distributed evenly across the tested models and\nseem to constitute a more challenging problem.\nPerhaps the most important error, common to all\nthe models, is hallucination, i.e., the model’s output\nresponses not grounded in the context (such as of-\nfering entities that are not included in the database).\nThis happens in about 10-20% of the inspected di-\nalogues. Some models ( GPT-NeoXT, OPT-IML)\ntend to generate more content than they are asked\nfor. This happens in more than 50% of their failed\ndialogues. In some cases, this means continuing\nthe conversation for a few more turns (including\nhallucinating user turns), but the models also often\ngenerate unrelated text or even code snippets. With\nTk-Instruct, we observed that in ca. 10% cases, it\ncopies the belief state from the example given in\nthe prompt instead of generating a relevant one.\nAnother issue is that the models tend to repeat their\nprevious responses.\n7 Conclusion & Future Work\nWe present an experimental evaluation of instruct-\nion-tuned LLMs applied to the established task of\ntask-oriented dialogue modeling, with five LLMs\nevaluated on two datasets. We find that LLMs\nare not performing well in terms of belief state\ntracking, even when provided with in-context few-\nshot examples. However, there is some potential to\nimprove through prompt tuning and output parsing\nrobust to irregularities.\nIf provided with a correct belief state, the models\ncan interact with the user successfully, provide use-\nful information and fulfill the user’s needs. While\nthe performance does not match the supervised\nstate of the art, it is important to note that these\nmodels were not finetuned on in-domain data and\nwork with just a domain description or a few exam-\nples (which again improve performance).\nTherefore, carefully picking representative ex-\n224\namples and combining the LLM with an in-domain\nbelief tracker can be a viable choice for a task-\noriented dialogue pipeline.\nInterestingly, in the human interactive evalua-\ntion, both ChatGPT and Tk-Instruct outperformed\nthe expectations set by automatic metrics. This\nshows certain flexibility and ability to correct their\nown mistakes on the part of LLMs, and further\ndemonstrates that single-turn evaluation is too rigid\nand does not show the whole picture (Takanobu\net al., 2020). In future work, we want to focus\non addressing the prompt-recoverable errors while\nmaintaining the ability to use model-independent\nprompts and easily swap models. We also aim to\nfind a more effective method of relevant example\nselection.\n8 Limitations\nOne of the limitations of our work is the usage of\nthe ChatGPT model, which is only accessible via\nan API and is not guaranteed to retain its exact abil-\nities. However, the other four out of five evaluated\nmodels have publicly available weights and their\nresults are fully reproducible. We still consider it\nbeneficial to also evaluate ChatGPT, as it represents\nstate-of-the-art at the time of writing of this paper\nand therefore puts the other models’ results into\nperspective.\nAnother limitation is that based on our empir-\nical experiments, the models are sensitive to the\nchoice of a specific prompting. We spent some\ntime finding a reasonably good prompt that would\nwork with all of the models and did model-specific\nmodifications for the evaluation. Specifically, the\ndesired format of the belief state varied between\nthe models, and there we re some model-specific\ninstructions. We also include both few-shot and\nzero-shot prompt types in our experiments. How-\never, it is likely that the performance could be fur-\nther improved with more extensive prompt engi-\nneering efforts. Nevertheless, we mainly aim to\nshowcase the more raw/out-of-the-box capabilities\nof the LLMs, as extensive prompt tuning would,\nin practice, erase the advantage of not having to\nfinetune the models. Furthermore, we believe that\nthe robustness of the model to specific prompts also\ncounts as an added value.\nFinally, we cannot exclude the possibility that\nsome of the models were exposed to our selected\ndatasets during training. However, we still find it\nimportant to evaluate the LLMs in this setting.\nAcknowledgments\nThis work was supported by the project\nTL05000236 AI asistent pro žáky a uˇ citele\nco-financed by the Technological Agency of the\nCzech Republic within the ÉTA 5 Programme, by\nthe European Research Council (Grant agreement\nNo. 101039303 NG-NLG), and by the Charles\nUniversity project SVV 260575. It used resources\nprovided by the LINDAT/CLARIAH-CZ Research\nInfrastructure (Czech Ministry of Education, Youth\nand Sports project No. LM2018101).\nReferences\nSid Black, Stella Biderman, Eric Hallahan, Quentin\nAnthony, Leo Gao, Laurence Golding, Horace He,\nConnor Leahy, Kyle McDonell, Jason Phang, et al.\n2022. Gpt-neox-20b: An open-source autoregressive\nlanguage model. arXiv preprint arXiv:2204.06745.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gaši ´c. 2018. MultiWOZ - A\nLarge-Scale Multi-Domain Wizard-of-Oz Dataset for\nTask-Oriented Dialogue Modelling. In Proceedings\nof the 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5016–5026, Brus-\nsels, Belgium. ArXiv: 1810.00278.\nDeng Cai, Yan Wang, Wei Bi, Zhaopeng Tu, Xi-\naojiang Liu, and Shuming Shi. 2019. Retrieval-\nguided dialogue response generation via a matching-\nto-generation framework. In Proceedings of the\n2019 Conference on Empirical Methods in Natu-\nral Language Processing and the 9th International\nJoint Conference on Natural Language Processing\n(EMNLP-IJCNLP), pages 1866–1875, Hong Kong,\nChina. Association for Computational Linguistics.\nPaul F Christiano, Jan Leike, Tom Brown, Miljan Mar-\ntic, Shane Legg, and Dario Amodei. 2017. Deep\nreinforcement learning from human preferences. Ad-\nvances in neural information processing systems, 30.\nJan Deriu, Alvaro Rodrigo, Arantxa Otegi, Guillermo\nEchegoyen, Sophie Rosset, Eneko Agirre, and Mark\nCieliebak. 2021. Survey on Evaluation Methods for\nDialogue Systems. Artificial Intelligence Review ,\n54:755–810.\nYihao Feng, Shentao Yang, Shujian Zhang, Jianguo\nZhang, Caiming Xiong, Mingyuan Zhou, and Huan\nWang. 2023. Fantastic rewards and how to tame them:\nA case study on reward learning for task-oriented\ndialoguesystems. arXiv preprint arXiv:2302.10342.\n225\nYue Feng, Yang Wang, and Hang Li. 2021. A sequence-\nto-sequence approach to dialogue state tracking. In\nProceedings of the 59th Annual Meeting of the Asso-\nciation for Computational Linguistics and the 11th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1714–\n1725, Online. Association for Computational Linguis-\ntics.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. Lora: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nYushi Hu, Chia-Hsuan Lee, Tianbao Xie, Tao Yu,\nNoah A. Smith, and Mari Ostendorf. 2022. In-\ncontext learning for few-shot dialogue state tracking.\nIn Findings of the Association for Computational\nLinguistics: EMNLP 2022 , pages 2627–2643, Abu\nDhabi, United Arab Emirates. Association for Com-\nputational Linguistics.\nTianjian Huang, Shaunak Ashish Halbe, Chinnadhurai\nSankar, Pooyan Amini, Satwik Kottur, Alborz Geram-\nifard, Meisam Razaviyayn, and Ahmad Beirami.\n2023. Robustness through data augmentation loss\nconsistency. Transactions on Machine Learning Re-\nsearch.\nXinting Huang, Jianzhong Qi, Yu Sun, and Rui Zhang.\n2020. MALA: cross-domain dialogue generation\nwith action learning. In The Thirty-Fourth AAAI Con-\nference on Artificial Intelligence, AAAI 2020, The\nThirty-Second Innovative Applications of Artificial\nIntelligence Conference, IAAI 2020, The Tenth AAAI\nSymposium on Educational Advances in Artificial In-\ntelligence, EAAI 2020, New York, NY, USA, February\n7-12, 2020, pages 7977–7984. AAAI Press.\nChia-Chien Hung, Anne Lauscher, Ivan Vuli´c, Simone\nPonzetto, and Goran Glavaš. 2022. Multi2WOZ: A\nrobust multilingual dataset and conversational pre-\ntraining for task-oriented dialog. In Proceedings of\nthe 2022 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 3687–3703,\nSeattle, United States. Association for Computational\nLinguistics.\nSrinivasan Iyer, Xi Victoria Lin, Ramakanth Pasunuru,\nTodor Mihaylov, Dániel Simig, Ping Yu, Kurt Shus-\nter, Tianlu Wang, Qing Liu, Punit Singh Koura, et al.\n2022. Opt-iml: Scaling language model instruc-\ntion meta learning through the lens of generalization.\narXiv preprint arXiv:2212.12017.\nJeff Johnson, Matthijs Douze, and Hervé Jégou. 2019.\nBillion-scale similarity search with GPUs. IEEE\nTransactions on Big Data, 7(3):535–547.\nJonáš Kulhánek, V ojtˇech Hudeˇcek, Tomáš Nekvinda,\nand Ondˇrej Dušek. 2021. AuGPT: Auxiliary tasks\nand data augmentation for end-to-end dialogue with\npre-trained language models. In Proceedings of the\n3rd Workshop on Natural Language Processing for\nConversational AI, pages 198–210, Online. Associa-\ntion for Computational Linguistics.\nWenqiang Lei, Xisen Jin, Min-Yen Kan, Zhaochun Ren,\nXiangnan He, and Dawei Yin. 2018. Sequicity: Sim-\nplifying task-oriented dialogue systems with single\nsequence-to-sequence architectures. In Proceedings\nof the 56th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 1437–1447.\nHector Levesque, Ernest Davis, and Leora Morgenstern.\n2012. The winograd schema challenge. In Thir-\nteenth international conference on the principles of\nknowledge representation and reasoning.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising Sequence-to-Sequence Pre-\ntraining for Natural Language Generation, Transla-\ntion, and Comprehension. In Proceedings of the 58th\nAnnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nZhaojiang Lin, Andrea Madotto, Genta Indra Winata,\nand Pascale Fung. 2020. MinTL: Minimalist trans-\nfer learning for task-oriented dialogue systems. In\nProceedings of the 2020 Conference on Empirical\nMethods in Natural Language Processing (EMNLP),\npages 3391–3405, Online. Association for Computa-\ntional Linguistics.\nAndrea Madotto, Zihan Liu, Zhaojiang Lin, and Pascale\nFung. 2020. Language models as few-shot learner\nfor task-oriented dialogue systems. arXiv preprint\narXiv:2008.06239.\nTomáš Nekvinda and Ondˇrej Dušek. 2021. Shades of\nBLEU, flavours of success: The case of MultiWOZ.\nIn Proceedings of the 1st Workshop on Natural Lan-\nguage Generation, Evaluation, and Metrics (GEM\n2021), pages 34–46, Online. Association for Compu-\ntational Linguistics.\nTomáš Nekvinda and Ondˇrej Dušek. 2022. AARGH!\nend-to-end retrieval-generation for task-oriented di-\nalog. In Proceedings of the 23rd Annual Meeting\nof the Special Interest Group on Discourse and Dia-\nlogue, pages 283–297, Edinburgh, UK. Association\nfor Computational Linguistics.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730–27744.\nGaurav Pandey, Danish Contractor, Vineet Kumar, and\nSachindra Joshi. 2018. Exemplar encoder-decoder\nfor neural conversation generation. In Proceedings\nof the 56th Annual Meeting of the Association for\n226\nComputational Linguistics (Volume 1: Long Papers),\npages 1329–1338, Melbourne, Australia. Association\nfor Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 311–318, Philadelphia,\nPennsylvania, USA. Association for Computational\nLinguistics.\nBaolin Peng, Chunyuan Li, Jinchao Li, Shahin Shayan-\ndeh, Lars Liden, and Jianfeng Gao. 2021. Soloist:\nBuilding task bots at scale with transfer learning and\nmachine teaching. Transactions of the Association\nfor Computational Linguistics, 9:807–824.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Kather-\nine Lee, Sharan Narang, Michael Matena, Yanqi\nZhou, Wei Li, and Peter J. Liu. 2020. Exploring the\nlimits of transfer learning with a unified text-to-text\ntransformer. Journal of Machine Learning Research,\n21(140):1–67.\nAbhinav Rastogi, Raghav Gupta, and Dilek Hakkani-\nTur. 2018. Multi-task Learning for Joint Language\nUnderstanding and Dialogue State Tracking. In Pro-\nceedings of the 19th Annual SIGdial Meeting on Dis-\ncourse and Dialogue , pages 376–384, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nAbhinav Rastogi, Xiaoxue Zang, Srinivas Sunkara,\nRaghav Gupta, and Pranav Khaitan. 2020. Towards\nscalable multi-domain conversational agents: The\nschema-guided dialogue dataset. In Proceedings of\nthe AAAI Conference on Artificial Intelligence , 05,\npages 8689–8696.\nNils Reimers and Iryna Gurevych. 2020. Making\nmonolingual sentence embeddings multilingual us-\ning knowledge distillation. In Proceedings of the\n2020 Conference on Empirical Methods in Natural\nLanguage Processing. Association for Computational\nLinguistics.\nI. Shalyminov, A. Sordoni, A. Atkinson, and H. Schulz.\n2020. Fast Domain Adaptation for Goal-Oriented\nDialogue Using a Hybrid Generative-Retrieval Trans-\nformer. In ICASSP 2020 - 2020 IEEE International\nConference on Acoustics, Speech and Signal Process-\ning (ICASSP), pages 8039–8043. ISSN: 2379-190X.\nIgor Shalyminov, Sungjin Lee, Arash Eshghi, and Oliver\nLemon. 2019. Data-efficient goal-oriented conversa-\ntion with dialogue knowledge transfer networks. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 1741–\n1751, Hong Kong, China. Association for Computa-\ntional Linguistics.\nHongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi,\nTianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf,\nLuke Zettlemoyer, Noah A Smith, et al. 2022. Selec-\ntive annotation makes language models better few-\nshot learners. arXiv preprint arXiv:2209.01975.\nHaipeng Sun, Junwei Bao, Youzheng Wu, and Xi-\naodong He. 2022. Mars: Semantic-aware contrastive\nlearning for end-to-end task-oriented dialog. arXiv\npreprint arXiv:2210.08917.\nRyuichi Takanobu, Qi Zhu, Jinchao Li, Baolin Peng,\nJianfeng Gao, and Minlie Huang. 2020. Is Your Goal-\nOriented Dialog Model Performing Really Well? Em-\npirical Analysis of System-wise Evaluation. In SIG-\ndial, pages 297–310, Online.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro, Faisal\nAzhar, Aurelien Rodriguez, Armand Joulin, Edouard\nGrave, and Guillaume Lample. 2023. Llama: Open\nand efficient foundation language models. arXiv\npreprint arXiv:2302.13971.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nYizhong Wang, Swaroop Mishra, Pegah Alipoor-\nmolabashi, Yeganeh Kordi, Amirreza Mirzaei,\nAnjana Arunkumar, Arjun Ashok, Arut Selvan\nDhanasekaran, Atharva Naik, David Stap, et al. 2022.\nSuper-naturalinstructions:generalization via declara-\ntive instructions on 1600+ tasks. In EMNLP.\nTsung-Hsien Wen, Milica Gaši´c, Dongho Kim, Nikola\nMrkši´c, Pei-Hao Su, David Vandyke, and Steve\nYoung. 2015. Stochastic language generation in dia-\nlogue using recurrent neural networks with convolu-\ntional sentence reranking. In Proceedings of the 16th\nAnnual Meeting of the Special Interest Group on Dis-\ncourse and Dialogue, pages 275–284, Prague, Czech\nRepublic. Association for Computational Linguistics.\n227\nJason D. Williams, Kavosh Asadi, and Geoffrey Zweig.\n2017. Hybrid code networks: practical and efficient\nend-to-end dialog control with supervised and rein-\nforcement learning. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 665–677,\nVancouver, Canada. Association for Computational\nLinguistics.\nYunyi Yang, Yunhao Li, and Xiaojun Quan. 2021. Ubar:\nTowards fully end-to-end task-oriented dialog system\nwith gpt-2. In Proceedings of the AAAI Conference\non Artificial Intelligence, 16, pages 14230–14238.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.\nOpt: Open pre-trained transformer language models.\narXiv preprint arXiv:2205.01068.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2019. Dialogpt: Large-scale\ngenerative pre-training for conversational response\ngeneration. arXiv preprint arXiv:1911.00536.\nTiancheng Zhao and Maxine Eskenazi. 2018. Zero-shot\ndialog generation with cross-domain latent actions.\nIn Proceedings of the 19th Annual SIGdial Meeting\non Discourse and Dialogue, pages 1–10, Melbourne,\nAustralia. Association for Computational Linguistics.\nWayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang,\nXiaolei Wang, Yupeng Hou, Yingqian Min, Be-\nichen Zhang, Junjie Zhang, Zican Dong, Yifan\nDu, Chen Yang, Yushuo Chen, Zhipeng Chen, Jin-\nhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang,\nZikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong\nWen. 2023. A Survey of Large Language Models.\nArXiv:2303.18223 [cs].\nQi Zhu, Christian Geishauser, Hsien chin Lin, Carel\nvan Niekerk, Baolin Peng, Zheng Zhang, Michael\nHeck, Nurul Lubis, Dazhen Wan, Xiaochen Zhu,\nJianfeng Gao, Milica Gaši ´c, and Minlie Huang.\n2022. Convlab-3: A flexible dialogue system toolkit\nbased on a unified data format. arXiv preprint\narXiv:2211.17148.\nDaniel M Ziegler, Nisan Stiennon, Jeffrey Wu, Tom B\nBrown, Alec Radford, Dario Amodei, Paul Chris-\ntiano, and Geoffrey Irving. 2019. Fine-tuning lan-\nguage models from human preferences. arXiv\npreprint arXiv:1909.08593.\n228\nA Prompt Construction\nPrompt Determine which domain is considered in the following dialogue situation.\nChoose exactly one domain from this list: restaurant, hotel, attraction, taxi, train\nAnswer with only one word, the selected domain from the list. You have to always select the most probable domain.\n——- Example 1: ——–\nCustomer: I need a cheap place to eat\nAssistant: We have several not expensive places available. What food are you interested in?\nCustomer: Chinese food.\nDomain: restaurant\n—— Example 2: ——–\nCustomer: What is the address?\nAssistant: It’s 123 Northfolk Road.\nCustomer: That’s all. I also need a train from London.\nDomain: train\n———–\nNow complete the following example:\nCustomer: I am looking for a cheap place to stay.\nDomain:\nOutput: hotel\nTable 4: A prompt used for domain detection for MultiWOZ. It contains task definition, domains description, static\nexamples and user utterance.\nPrompt Definition: Capture entity values from last utterance of the conversation according to examples.\nCapture pair \"entity:value\" separated by colon and no spaces in between. Separate entity:value pairs by hyphens.\nIf not specified, leave the value empty. Values that should be captured are:\n- \"pricerange\": the price of the hotel\n- \"area\" that specifies the area where the hotel is located (north/east/west/south/centre)\n- \"internet\" that specifies if the hotel has internet (yes/no)\n- \"parking\" that specifies if the hotel has parking (yes/no)\n- \"stars\" that specifies the number of stars the hotel has (1/2/3/4/5)\n- \"type\" that specifies the type of the hotel (hotel/bed and breakfast/guest house)\n[history]\nCustomer: \"I want a cheap place to stay.\"\nOutput: pricerange:\"cheap\"\nTable 5: A zero-shot version of the prompt used for state update prediction for MultiWOZ 2.2. It contains task\ndefinition, domain description, dialogue history and user utterance.\nPrompt Definition: You are an assistant that helps people to book a hotel.\nThe user can ask for a hotel by name, area, parking, internet availability, or price.\nThere is also a number of hotel in the database currently corresponding to the user’s request.\nIf you find a hotel, provide [hotel_name], [hotel_address], [hotel_phone] or [hotel_postcode]\nDo not provide real entities in the response! Just provide entity name in brackets, like [name] or [address].\nIf booking, provide [reference] in the answer.\n[history]\nCustomer: \"I want a cheap place to stay.\"\nState: hotel { pricerange: \"cheap\"}\nDatabase: hotels: 23\nOutput: We have 23 such hotels available, do you have a preference about the location?\nTable 6: A zero-shot version of the prompt used for response prediction for MultiWOZ 2.2. It contains task\ndefinition, domain description, dialogue history, user utterance and belief state with db results.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8354514241218567
    },
    {
      "name": "Conversation",
      "score": 0.7264710664749146
    },
    {
      "name": "Popularity",
      "score": 0.7189968824386597
    },
    {
      "name": "Task (project management)",
      "score": 0.7175106406211853
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6189606189727783
    },
    {
      "name": "Human–computer interaction",
      "score": 0.5469801425933838
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.4998776912689209
    },
    {
      "name": "Task analysis",
      "score": 0.4428223967552185
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33966803550720215
    },
    {
      "name": "Natural language processing",
      "score": 0.3264748454093933
    },
    {
      "name": "Psychology",
      "score": 0.091217041015625
    },
    {
      "name": "Communication",
      "score": 0.07353794574737549
    },
    {
      "name": "Social psychology",
      "score": 0.06477352976799011
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I21250087",
      "name": "Charles University",
      "country": "CZ"
    }
  ]
}