{
  "title": "FQ-ViT: Fully Quantized Vision Transformer without Retraining",
  "url": "https://openalex.org/W3216998657",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2000929903",
      "name": "Yang Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2127955102",
      "name": "Tianyu Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105046369",
      "name": "Peiqin Sun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2107695934",
      "name": "Zheng Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2135537441",
      "name": "Shuchang Zhou",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3170874841",
    "https://openalex.org/W2469490737",
    "https://openalex.org/W2946355854",
    "https://openalex.org/W3035078287",
    "https://openalex.org/W2964299589",
    "https://openalex.org/W2994840239",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3004061291",
    "https://openalex.org/W2998218113",
    "https://openalex.org/W3172801447",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W3188427387",
    "https://openalex.org/W3161838454",
    "https://openalex.org/W2955425717",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W3100020884",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W2998183051",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2964241181",
    "https://openalex.org/W3166874749",
    "https://openalex.org/W3211787299",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W2963981420",
    "https://openalex.org/W2786771851",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3098576111",
    "https://openalex.org/W3159778524",
    "https://openalex.org/W3171125843",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W3170233084",
    "https://openalex.org/W2612445135",
    "https://openalex.org/W3168124404",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2903972532",
    "https://openalex.org/W3139773203",
    "https://openalex.org/W3166368936",
    "https://openalex.org/W2962298324",
    "https://openalex.org/W3166859509",
    "https://openalex.org/W3153842237",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W3119786062"
  ],
  "abstract": "Network quantization significantly reduces model inference complexity and has been widely used in real-world deployments. However, most existing quantization methods have been developed and tested mainly on Convolutional Neural Networks (CNN), and suffer severe degradation when applied to Transformer-based architectures. In this work, we present a systematic method to reduce the performance degradation and inference complexity of Quantized Transformers. In particular, we propose Powers-of-Two Scale (PTS) to deal with the serious inter-channel variation of LayerNorm inputs in a hardware-friendly way. In addition, we propose Log-Int-Softmax (LIS) that can sustain the extreme non-uniform distribution of the attention maps while simplifying inference by using 4-bit quantization and the BitShift operator. Comprehensive experiments on various Transformer-based architectures and benchmarks show that our methods outperform previous works in performance while using even lower bit-width in attention maps. For instance, we reach 85.17% Top-1 accuracy with ViT-L on ImageNet and 51.4 mAP with Cascade Mask R-CNN (Swin-S) on COCO. To our knowledge, we are the first to achieve comparable accuracy degradation (~1%) on fully quantized Vision Transformers. Code is available at https://github.com/linyang-zhh/FQ-ViT.",
  "full_text": "FQ-ViT: Post-Training Quantization for Fully Quantized Vision Transformer\nYang Lin∗†, Tianyu Zhang∗, Peiqin Sun‡, Zheng Li and Shuchang Zhou\nMEGVII Technology\nlinyang.zhh@gmail.com, {zhangtianyu, sunpeiqin, lizheng02, zsc}@megvii.com\nAbstract\nNetwork quantization signiﬁcantly reduces model\ninference complexity and has been widely used in\nreal-world deployments. However, most existing\nquantization methods have been developed mainly\non Convolutional Neural Networks (CNNs), and\nsuffer severe degradation when applied to fully\nquantized vision transformers. In this work, we\ndemonstrate that many of these difﬁculties arise\nbecause of serious inter-channel variation in Lay-\nerNorm inputs, and present, Power-of-Two Fac-\ntor (PTF), a systematic method to reduce the per-\nformance degradation and inference complexity of\nfully quantized vision transformers. In addition,\nobserving an extreme non-uniform distribution in\nattention maps, we propose Log-Int-Softmax (LIS)\nto sustain that and simplify inference by using 4-\nbit quantization and the BitShift operator. Compre-\nhensive experiments on various transformer-based\narchitectures and benchmarks show that our Fully\nQuantized Vision Transformer (FQ-ViT) outper-\nforms previous works while even using lower bit-\nwidth on attention maps. For instance, we reach\n84.89% top-1 accuracy with ViT-L on ImageNet\nand 50.8 mAP with Cascade Mask R-CNN (Swin-\nS) on COCO. To our knowledge, we are the ﬁrst\nto achieve lossless accuracy degradation ( ∼1%)\non fully quantized vision transformers. The code\nis available at https://github.com/megvii-research/\nFQ-ViT.\n1 Introduction\nTransformer-based architectures have achieved competitive\nperformance in various computer vision (CV) tasks, in-\ncluding image classiﬁcation [Dosovitskiy et al., 2021; Tou-\nvron et al. , 2021 ], object detection [Carion et al. , 2020;\nLiu et al., 2021a], semantic segmentation[Zheng et al., 2021]\nand so on. Compared to the CNN counterparts, transform-\ners usually have more parameters and higher computational\n*Equal contribution.\n†Work done while interning at MEGVII Technology.\n‡Corresponding author.\nDeiT-T DeiT-S DeiT-B Swin-T Swin-S Swin-B ViT-B ViT-L\nModel\n0\n20\n40\n60\n80\n100Top-1 Accuracy on ImageNet\nFull Precision\nw/o Fully Quantized\n w/  Fully Quantized (Vanilla)\n w/  Fully Quantized (Ours)\nFigure 1: Top-1 accuracy on ImageNet for full precision and quan-\ntized vision transformers. w/o Fully Quantized: LayerNorm and\nSoftmax remain ﬂoating-point, while other modules are quantized\nto 8-bit by MinMax. w/ Fully Quantized (Vanilla): all modules are\nquantized to 8-bit by MinMax. w/ Fully Quantized (Ours): all mod-\nules are quantized to 8-bit by our method.\ncosts. For example, ViT-L has 307M parameters and 190.7G\nFLOPs, reaching the accuracy of 87.76% in ImageNet with\nlarge-scale pre-training. However, the large number of pa-\nrameters and computational overhead of transformer-based\narchitectures present a challenge when deployed to resource-\nconstrained hardware devices.\nTo facilitate deployment, several techniques have been pro-\nposed, including quantization [Zhou et al., 2016; Nagel et al.,\n2020; Shen et al., 2020; Liu et al., 2021b], pruning [Han et\nal., 2016 ], distillation [Jiao et al. , 2020 ] and adaptation of\narchitecture design [Graham et al., 2021]. We focus on the\nquantization technique in this paper and note that pruning,\ndistillation, and architecture adaptation are orthogonal to our\nwork and can be combined.\nMost existing quantization approaches have been designed\nand tested on CNNs and lack proper handling of transformer-\nspeciﬁc constructs. Previous work [Liu et al., 2021b] ﬁnds\nthere have been a signiﬁcant accuracy degradation when\nquantizing LayerNorm and Softmax of vision transformers.\nIn this case, the models are not fully quantized, resulting in\nthe need to retain ﬂoating-point units in the hardware, which\nwill bring large consumption and signiﬁcantly reduce the in-\narXiv:2111.13824v4  [cs.CV]  17 Feb 2023\nference speed [Lian et al. , 2019 ]. So we revisit these two\nexclusive modules of the vision transformers and discover\nthe reasons of degradation. Firstly, we ﬁnd a serious inter-\nchannel variation of LayerNorm inputs, which some channel\nranges even exceed 40×of the median. Traditional methods\ncannot handle such large ﬂuctuations of activations, which\nwill lead to large quantization error. Secondly, we ﬁnd that\nthe values of the attention map have an extreme non-uniform\ndistribution, with most values clustered in 0∼0.01, and a few\nhigh attention values close to 1.\nBased on the analysis above, we propose Power-of-Two\nFactor (PTF) to quantize the inputs of the LayerNorm. In this\nway, the quantization error is greatly reduced, and the over-\nall computational efﬁciency is the same as that of layer-wise\nquantization thanks to the BitShift operator. In addition, we\npropose Log-Int-Softmax (LIS), which provides higher quan-\ntization resolution for small values and presents a more efﬁ-\ncient integer inference for Softmax. Combining these meth-\nods, we are the ﬁrst work to achieve post-training quantiza-\ntion for fully quantized vision transformers. As shown in Fig-\nure 1, our method signiﬁcantly improves the performance of\nfully quantized vision transformers and obtains comparable\naccuracy with full precision counterparts.\nOur contributions are four-fold:\n• We revisit the fully quantized vision transformers and\nattribute the accuracy degradation to the serious inter-\nchannel variation in LayerNorm inputs. Meanwhile, we\nobserve an extreme non-uniform distribution of attention\nmaps, resulting in another part of the quantization error.\n• We propose Power-of-Two Factor (PTF), a simple yet\nefﬁcient post-training method that can achieve accurate\nquantization on LayerNorm inputs with only one layer-\nwise quantization scale.\n• We propose Log-Int-Softmax (LIS), a novel method that\ncan perform 4-bit quantization on attention maps. With\nLIS, we can store attention maps on an aggressively low-\nbit and replace multiplication with BitShift operator. We\nachieve integer-only inference on Softmax modules, sig-\nniﬁcantly reducing the inference consumption.\n• We conduct extensive experiments on image classiﬁca-\ntion and object detection with various transformer-based\narchitectures. The results show that our fully quantized\nvision transformers with 8-bit weights/activations and 4-\nbit attention maps, can achieve comparable performance\nto ﬂoating-point versions.\n2 Related Work\n2.1 Vision Transformer\nRecently, transformer-based architecture shows great power\nin CV tasks. Emerging works based on ViT [Dosovitskiy\net al. , 2021 ] demonstrated the effectiveness across all vi-\nsion tasks such as classiﬁcation [Touvron et al., 2021 ], de-\ntection [Carion et al. , 2020 ] and segmentation [Zheng et\nal., 2021 ]. The newly proposed Swin Transformer [Liu\net al. , 2021a ] even surpasses the state-of-the-art CNNs on\nalmost tranditional CV tasks, presenting strong expressive\nand generalization capability of transformer. However, these\nhigh-performing vision transformers are attributed to the\nlarge number of parameters and high computational over-\nhead, limiting their adoption. Therefore, innovating a smaller\nand faster vision transformer becomes a new trend. Le-\nViT [Graham et al. , 2021 ] makes progress in faster infer-\nence with down-sampling, patch descriptors, and a redesign\nof Attention-MLP block. DynamicViT [Rao et al. , 2021 ]\npresents a dynamic token sparsiﬁcation framework to prune\nredundant tokens progressively and dynamically, achieving\ncompetitive complexity and accuracy trade-off. Evo-ViT[Xu\net al., 2021 ] proposes a slow-fast updating mechanism that\nguarantees information ﬂow and spatial structure, trimming\ndown both the training and inference complexity. While the\nabove works focus on efﬁcient model designing, this paper\nboosts the compression and acceleration in the track of quan-\ntization.\n2.2 Network Quantization\nCurrent quantization methods can be divided into two cat-\negories: Quantization-Aware Training (QAT) and Post-\nTraining Quantization (PTQ). QAT [Zhou et al. , 2016; Ja-\ncob et al. , 2018 ] depends on training to achieve aggres-\nsively low-bit (e.g. 2-bit) quantization and promising per-\nformance, while it often requires a high-level expert knowl-\nedge and huge GPU resources for training or ﬁne-tuning. To\nreduce above costs of quantization, PTQ, which is training-\nfree, has received more widespread attention and lots of ex-\ncellent works arise. OMSE [Choukroun et al. , 2019 ] pro-\nposes to determine the value range of activation by minimiz-\ning the quantization error. AdaRound [Nagel et al. , 2020 ]\npresents a novel rounding mechanism to adapt the data and\nthe task loss. Besides works above speciﬁc to CNNs, Liu et\nal. proposes a post-training quantization method for vision\ntransformers with similarity-aware and rank-aware strategies.\nHowever, this work does not quantize Softmax and Layer-\nNorm modules, resulting in an incomplete quantization. In\nour FQ-ViT, we aim to implement an accurate, fully quan-\ntized vision transformer under the PTQ paradigm.\n3 Proposed Method\nIn this section, we will introduce our proposed approach in\ndetail. First in Section 3.1, we present the preliminary of net-\nwork quantization. Then in Section 3.2 and 3.3, we analyze\nthe reasons of degradation in fully quantized vision trans-\nformers and propose two novel quantization methods, Power-\nof-Two Factor (PTF) and Log-Int-Softmax (LIS), for Layer-\nNorm and Softmax.\n3.1 Preliminary\nIn this section, we explain the notations of network quantiza-\ntion. Assuming the quantization bit-width is b, the quantizer\nQ(X|b) can be formulated as a function that maps a ﬂoating-\npoint number X ∈R to the nearest quantization bin:\nQ(X|b) :R →q, (1)\nq =\n{\n{−2b−1,··· ,2b−1 −1} Signed,\n{0,1 ··· ,2b −1} Unsigned.\n(2)\nR50 R101 R152 ViT-B ViT-L DeiT-T DeiT-S DeiT-B Swin-T Swin-S Swin-B\nModel\n100\n101\n102\n103\nChannel-wise ranges\n0 200 400 600 800 1000\nChannel's index\n200\n0\n200\n400\n600Value\nMinimum Value\nMaximum Value\nFigure 2: Left: Boxplot of the last LayerNorm inputs’ channel-wise ranges in each model. Right: Channel-wise minimum and maximum\nvalues of the last LayerNorm inputs in full precision Swin-B. The above two ﬁgures show that there exists more serious inter-channel variation\nin vision transformers than CNNs, which leads to unacceptable quantization errors with layer-wise quantization.\nThere are various quantizer Q(X|b), where uniform [Jacob\net al., 2018] and log2 [Cai et al., 2018] are typically used.\nUniform Quantization is well supported on most hard-\nware platforms. Its quantizer Q(X|b) can be deﬁned as:\nQ(X|b) = clip(⌊X\ns⌉+ zp,0,2b −1), (3)\nwhere s(scale) and zp(zero-point) are quantization parame-\nters determined by the lower bound land the upper bound u\nof X, which are usually minimum and maximum values:\nl= min(X),u = max(X), (4)\ns= u−l\n2b −1\n,zp = clip(⌊−l\ns⌉,0,2b −1). (5)\nLog2 Quantization converts the quantization process from\nlinear to exponential variation. Its quantizer Q (X|b) can be\ndeﬁned as:\nQ(X|b) = sign(X) ·clip(⌊−log2\n|X|\nmax(|X|)⌉,0,2b−1 −1).\n(6)\nIn this paper, to achieve a fully quantized vision trans-\nformer, we quantize all modules, including Conv, Linear,\nMatMul, LayerNorm, Softmax, etc. Especially, uniform Min-\nMax quantization is used for Conv, Linear and MatMul mod-\nules and our following methods are used for LayerNorm and\nSoftmax.\n3.2 Power-of-Two Factor for LayerNorm\nQuantization\nDuring inference, LayerNorm [Ba et al., 2016] computes the\nstatistics µX,σX in each forward step and normalizes input X.\nThen, afﬁne parameters γ, β rescale the normalized input to\nanother learned distribution. The above process can be writ-\nten as:\nLayerNorm(X) = X −µX√\nσ2\nX + ϵ\n·γ+ β. (7)\nUnlike BatchNorm [Ioffe and Szegedy, 2015], commonly\nused in CNNs, LayerNorm cannot be folded into the previ-\nous layer due to its dynamic computational property, so we\nhave to quantize it separately. However, we observe a signif-\nicant performance degradation while applying post-training\nquantization on it. Looking into the inputs of LayerNorm\nlayers, we ﬁnd there is a serious inter-channel variation. Fig-\nure 2 presents the channel-wise ranges of activation in the\nlast LayerNorm layer. In addition, we also display the cases\nof ResNets [He et al., 2016] for comparison. Considering that\nthere is no LayerNorm in ResNets, we choose the activations\nat the same position (fourth stage’s outputs) to exhibit.\nIt is observed that the channel-wise ranges ﬂuctuate more\nwildly in vision transformers than those in ResNets. For\ninstance, the maximum range/median range of ResNet152\nis only 21.6/4.2, while it goes up to 622.5/15.5 in Swin-B.\nBased on such extreme inter-channel variation, layer-wise\nquantization, which applies the same quantization parameters\nto all channels, will lead to an intolerable quantization error.\nA possible solution is using group-wise quantization [Shen\net al., 2020 ] or channel-wise quantization [Li et al., 2019 ],\nwhich assign different quantization parameter to different\ngroup or channel. However, these will still induce the cal-\nculation of mean and variance in the ﬂoating-point domain,\nresulting in a high hardware overhead.\nIn this paper, we propose a simple yet efﬁcient method,\nPower-of-Two Factor (PTF), for LayerNorm quantization.\nThe core idea of PTF is to equip different channels with\ndifferent factors, rather than different quantization parame-\nters. Given the quantization bit-width b, the input activa-\ntion X ∈ RB×L×C, the layer-wise quantization parameters\ns,zp ∈R1, and the PTF α ∈NC, then the quantized activa-\ntion XQ can be formulated as:\nXQ = Q(X|b) = clip(⌊ X\n2αs⌉+ zp,0,2b −1), (8)\nwith\ns= max(X) −min(X)\n2b −1\n/2K, (9)\nzp= clip(⌊−min(X)\n2Ks ⌉,0,2b −1), (10)\nαc = arg min\nαc∈{0,1,···,K}\nXc −⌊ Xc\n2αc s⌉·2αc s\n\n2\n. (11)\nNoticing c represents the channel index for X and α. The\nhyperparameter K could meet different scaling requirements.\nIn order to cover the different inter-channel variation across\nFigure 3: Distribution of attention maps in ViT-L with visualizing\nthe 4-bit quantized bins of uniform and log2 quantization. X-axis\nis in log-scale and we can observe that log2 quantization preserves\nmore bins than uniform for small values.\nall models, we set K = 3 as default. Detailed experiments\ncan be found in supplementary materials.\nAt this point, each channel has its own Power-of-Two Fac-\ntor α and layer-wise parameters s,zp. During inference,\nlayer-wise parameters s and zp can be extracted, so the\ncomputation of µ,σ could be done in the integer domain\nrather than ﬂoating-point, which reduces the energy and area\ncosts [Lian et al., 2019]. Meanwhile, thanks to the nature of\npowers of two, PTFαcan be efﬁciently combined with layer-\nwise quantization by BitShift operator, avoiding ﬂoating-\npoint calculations of group-wise or channel-wise quantiza-\ntion. The whole process can be processed with two phases:\nPhase 1: Shift the quantized activation with Power-of-Two\nFactor α:\nˆXQ = (XQ −zp) <<α. (12)\nPhase 2: Calculate the mean and variance based on the\nshifted activation ˆXQ:\nµ(X) ≈µ(2αs·(XQ −zp)) =s·µ(ˆXQ), (13)\nσ(X) ≈σ(2αs·(XQ −zp)) =s·σ(ˆXQ). (14)\n3.3 Log-Int-Softmax for Softmax Quantization\nMulti-head Self-Attention (MSA) is one of the most impor-\ntant components in transformer-based architectures, while it\nis considered the most resource-intensive due to the quadratic\ncomplexity to the number of token, the division of the im-\nage resolution by the patch size. As model performance\nproved to beneﬁt from higher resolution and smaller patch\nsize [Dosovitskiy et al., 2021], when the increasing resolu-\ntion and reducing patch size, the storage and computation of\nattention maps become the bottleneck which directly affect\nthe throughput and latency of inference. Therefore, smaller\nattention maps and more efﬁcient inference become an urgent\nneed.\nLog2 Quantization for Attention Map\nIn order to compress the attention maps to smaller size and\nspeed up the inference, we quantize attention maps to lower\nFigure 4: Comparison of using full precision Softmax and Log-Int-\nSoftmax in quantized multi-head self-attention inference. Full pre-\ncision Softmax needs to dequantize and requantize around Softmax,\nwhile LIS keeps an integer-only data type in the whole MSA infer-\nence.\nbit-width. As experimenting on quantization of attention\nmaps from 8-bit to 4-bit with uniform quantization, all vi-\nsion transformers show severe performance drop. For ex-\nample, DeiT-T only results in 8.69% top-1 accuracy on Im-\nageNet with 4-bit uniform quantized attention maps, decreas-\ning 63.05% from 8-bit case.\nInspired by the idea of sparse attention in Dynam-\nicViT [Rao et al. , 2021 ], we probe into the distribution of\nattention maps, as Figure 3 shows. We observe a distribution\ncentering at a fairly small value, while only a few outliers\nhave larger values close to 1. Averaging on all attention maps\nin ViT-L, about 98.8% of values are smaller than1/16. Com-\npared with 4-bit uniform quantization which only assigns 1\nbin for such many values, log2 method can allocate 12 bins\nfor them. Moreover, following the purpose of ranking-aware\nloss [Liu et al., 2021b ], log2 quantization can retain much\norder consistency between full precision and quantized atten-\ntion maps. Consequently, we save the extreme degradation in\n4-bit quantization of attention maps and achieve equal perfor-\nmance as 8-bit uniform quantization with 50% less memory\nfootprint.\nLog2 quantization proved to be suitable combining with\nMSA from two aspects. Firstly, comparing to Equation (6),\nthe ﬁxed output range (0,1) of Softmax makes the log2 func-\ntion calibration-free:\nAttnQ = Q(Attn|b) = clip(⌊−log2(Attn)⌉,0,2b −1). (15)\nThis ensures that the quantization of attention maps will not\nbe affected by the ﬂuctuation of calibration data.\nSecondly, it also introduces the merits of converting\nthe MatMul to BitShift between the quantized attention\nmap (AttnQ) and values (VQ) as:\nAttn ·VQ = 2−AttnQ ·VQ = VQ >>AttnQ (16)\n= 1\n2N ·(VQ <<(N −AttnQ)), (17)\nwith N = 2b −1. Noticing that directly right shift V Q with\nthe results of Attn Q may lead to severe truncation error. We\nMethod W/A/Attn DeiT-T DeiT-S DeiT-B ViT-B ViT-L Swin-T Swin-S Swin-B\nFull Precision 32/32/32 72.21 79.85 81.85 84.53 85.81 81.35 83.20 83.60\nMinMax 8/8/8 70.94 75.05 78.02 23.64 3.37 64.38 74.37 25.58\nEMA [Jacob et al., 2018] 8/8/8 71.17 75.71 78.82 30.30 3.53 70.81 75.05 28.00\nPercentile [Li et al., 2019] 8/8/8 71.47 76.57 78.37 46.69 5.85 78.78 78.12 40.93\nOMSE [Choukroun et al., 2019] 8/8/8 71.30 75.03 79.57 73.39 11.32 79.30 78.96 48.55\nBit-Split∗ [Wang et al., 2020] 8/8/8 - 77.06 79.42 - - - - -\nPTQ for ViT∗ [Liu et al., 2021b] 8/8/8 - 77.47 80.48 - - - - -\nFQ-ViT 8/8/8 71.61 79.17 81.20 83.31 85.03 80.51 82.71 82.97\n8/8/4 71.07 78.40 80.85 82.68 84.89 80.04 82.47 82.38\nTable 1: Comparison of the top-1 accuracy with state-of-the-art methods on ImageNet dataset. ∗ indicates that all LayerNorm and Softmax\nmodules are not quantized.\nuse (N −AttnQ) as the quantized output with a scale equaling\n1/2N, which ensures a left-shift operation to prevent from\ntruncation error.\nInteger-only Inference\nPrevious works [Liu et al., 2021b] chose to not quantize Soft-\nmax because the negligibility of calculation amount in Soft-\nmax, and quantization may lead to signiﬁcant accuracy degra-\ndation. However, data moving between CPU and GPU/NPU,\ndoing dequantization and requantization, will induce great\ndifﬁculties in hardware design, which is not a negligible con-\nsumption.\nCombining log2 quantization withi-exp [Kim et al., 2021],\nwhich is a polynomial approximation of exponential function,\nwe propose Log-Int-Softmax, an integer-only, faster, low con-\nsuming Softmax:\nexp(s·XQ) ≈s′·i-exp(XQ), (18)\nLIS(s·XQ) =N −log2⌊\n∑i-exp(XQ)\ni-exp(XQ) ⌉, (19)\nwith N = 2b −1. An integer log2 function can be eas-\nily implemented by using BitShift to ﬁnd the ﬁrst bit index\nwhere the value is 1 (we call it Find First One function),\nand adding the value of bit right behind that. Detailed deriva-\ntion can be found in the supplementary materials.\nThe difference between normal MSA and our method are\nshown in Figure 4, with the data type of every stage labeled.\nIn the multi-head self-attention with unquantized Softmax\nshown on the left, the matrix multiplication of queries (Q)\nand keys (K) needs to be dequantized to full precision be-\nfore Softmax, and requantized after it. When our Log-Int-\nSoftmax adopted, shown on the right, the entire data type\ncan be in pure integer, with quantization scale individually\nand paralleling calculated. It is worth noting that LIS uses\nan aggressively 4-bit representation on attention maps, which\nsigniﬁcantly reduces memory footprint.\n4 Experiments\nIn this section, we present experimental results on vision\ntransformers for image classiﬁcation and object detection.\nWe state the detailed experimental conﬁguration ﬁrstly and\nthen exhibit the comparison of our method with existing\npost-training quantization methods in ImageNet [Krizhevsky\net al. , 2012 ] and COCO [Lin et al. , 2014 ] benchmarks.\nIn the end, ablation studies are conducted to evaluate the\neffectiveness of Power-of-Two Factor (PTF) and Log-Int-\nSoftmax (LIS).\n4.1 Implementation Details\nWe randomly sample 1000 training images from ImageNet\nor COCO as the calibration data, and use the validation set\nto evaluate performance. Apart from special notes, we per-\nform symmetric channel-wise quantization for weights and\nasymmetric layer-wise quantization for activations. For a fair\ncomparison, the quantization for weights is ﬁxed as MinMax.\nThe hyperparameter K in Power-of-Two Factor is set to 3.\n4.2 Comparison with State-of-the-art Methods\nThis paper employs several current post-training quantization\nmethods, including MinMax, EMA [Jacob et al., 2018], Per-\ncentile [Li et al., 2019], OMSE [Choukroun et al., 2019], Bit-\nSplit [Wang et al., 2020] and PTQ for ViT[Liu et al., 2021b].\nNote that PTQ for ViT [Liu et al., 2021b ] is closest to our\nwork, however it does not quantize the LayerNorm and Soft-\nmax, while we quantize all modules.\nImage Classiﬁcation on ImageNet\nTo demonstrate the effectiveness of proposed methods, we\nconduct extensive experiments in ImageNet [Krizhevsky et\nal., 2012] with various vision transformers, i.e., ViT [Doso-\nvitskiy et al., 2021], DeiT [Touvron et al., 2021], Swin Trans-\nformer [Liu et al., 2021a]. The overall top-1 accuracy results\nare reported in Table 1. It is obvious that all current methods\ncan’t capture fully quantized vision transformers, while our\nFQ-ViT does it and achieves a nearly lossless quantization\neven with an aggressively low-bit on attention maps. Mean-\nwhile, our FQ-ViT signiﬁcantly exceeds PTQ for ViT [Liu\net al., 2021b], whose LayerNorm and Softmax are not quan-\ntized. For instance, our FQ-ViT achieves 81.20% accuracy\non DeiT-B in the case of all modules quantized to 8-bit, and\nit can still achieve 80.85% accuracy when the attention maps\nare compressed to 4-bit.\nObject Detection on COCO\nWe also conduct experiments on the object detection bench-\nmark COCO [Lin et al., 2014]. We choose Swin series [Liu\nMethod W/A/Attn Mask R-CNN Cascade Mask R-CNN\nw/ Swin-S w/ Swin-S\nFull Precision 32/32/32 48.5 52.0\nMinMax 8/8/8 32.8 35.2\nEMA [Jacob et al., 2018] 8/8/8 37.9 40.4\nPercentile [Li et al., 2019] 8/8/8 41.6 44.7\nOMSE [Choukroun et al., 2019] 8/8/8 42.6 44.9\nFQ-ViT 8/8/8 47.8 51.4\n8/8/4 47.2 50.8\nTable 2: Comparison of the bbox mAP with state-of-the-art methods on COCO dataset.\nMethod PTF LIS BitOPs (G) Acc. (%)\nFull Precision - - - 84.53\nBaseline #8 \u0017 \u0017 1118.51 23.64\n\u0013 \u0017 1118.52 83.31\nBaseline #4 \u0013 \u0017 1118.34 7.96\n\u0013 \u0013 1117.76 82.68\nTable 3: Effect of the Power-of-Two Factor (PTF) and Log-Int-\nSoftmax (LIS). We evaluate the performance of full precision and\nquantized ViT-B on ImageNet validation set. We choose MinMax\nwith 8-bit weights and activations as ”Baseline” and # indices the\nbit-width of attention maps.\net al., 2021a ] detectors for experiments and the results can\nbe found in Table 2. It is observed that all current meth-\nods have poor performance on fully quantized detectors.\nOur FQ-ViT signiﬁcantly improves the quantization accu-\nracy and achieves 47.2 mAP on Mask R-CNN (Swin-S) and\n50.8 mAP on Cascade Mask R-CNN (Swin-S) with 8-bit on\nweights/activations and 4-bit on attention maps.\n4.3 Ablation Studies\nTo study the effect of our methods, Power-of-Two Fac-\ntor (PTF) and Log-Int-Softmax (LIS), we fully quantize ViT-\nB under a variety of strategies and report the results in Ta-\nble 3. We design two baselines. ”Baseline #8” indices the\nmodel is fully quantized by MinMax with 8-bit weights, acti-\nvations and attention maps, while ”Baseline #4” has a lower\nbit-width (4-bit) on attention maps. From the results, we have\nseveral observations. Firstly, the model with PTF and LIS\noutperforms the baseline and achieves almost lossless accu-\nracy. Secondly, thanks to the BitShift operator, PTF only in-\ntroduces little amount of BitOPs, while LIS reduces that.\n4.4 Visualization of Quantized Attention Map\nWe visualize the attention maps to see the difference between\nuniform quantization and our LIS as Figure 5 shows. When\nboth using 8-bit, uniform quantization focuses on the high\nactivation area, while LIS keeps more texture in the low acti-\nvation area, which retains more relative rank of the attention\nmap. This divergence does not make a big difference in the\ncase of 8-bit. However, when quantized to lower bit-width, as\nthe 6-bit and 4-bit cases show, uniform quantization sharply\ndegrades and even deactivates all the attention area. On the\n(a)\n(b)\n8-bit 6-bit 4-bit\nFigure 5: Attention map visualization. (a) shows the results of uni-\nform quantization and (b) shows the results of our Log-Int-Softmax.\ncontrary, LIS still presents acceptable performance similar to\n8-bit.\n5 Conclusions\nIn this paper, we propose a method to fully quantize vision\ntransformers. Speciﬁcally, we propose Power-of-Two Fac-\ntor (PTF) to deal with the serious inter-channel variations in\nthe inputs of LayerNorm. In addition, we propose an inte-\ngrated quantization solution Log-Int-Softmax (LIS) to im-\nplement 4-bit quantization of attention maps and utilize the\nBitShift operator instead of MatMul during inference, which\neffectively reduces hardware resource requirement. Experi-\nmental results show that our fully quantized vision transform-\ners achieve comparable performance with the full precision\nmodels. Altogether, we provide a higher baseline for future\nworks, hoping that FQ-ViT’s strong performance will encour-\nage research into even lower bit-width quantization of vision\ntransformers, which will boost their real-world adoptions.\nReferences\n[Ba et al., 2016] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E\nHinton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016.\n[Cai et al., 2018] Jingyong Cai, Masashi Takemoto, and Hironori\nNakajo. A deep look into logarithmic quantization of model pa-\nrameters in neural networks. In Proceedings of the 10th Interna-\ntional Conference on Advances in Information Technology, 2018.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa, Gabriel\nSynnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-end object detection with transformers. In\nEuropean Conference on Computer Vision. Springer, 2020.\n[Choukroun et al., 2019] Yoni Choukroun, Eli Kravchik, Fan Yang,\nand Pavel Kisilev. Low-bit quantization of neural networks for\nefﬁcient inference. In IEEE/CVF International Conference on\nComputer Vision Workshops. IEEE, 2019.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer,\nGeorg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil\nHoulsby. An image is worth 16x16 words: Transformers for\nimage recognition at scale. In International Conference on\nLearning Representations, 2021.\n[Graham et al., 2021] Ben Graham, Alaaeldin El-Nouby, Hugo\nTouvron, Pierre Stock, Armand Joulin, Herv´e J´egou, and Matthijs\nDouze. Levit: a vision transformer in convnet’s clothing for faster\ninference. arXiv preprint arXiv:2104.01136, 2021.\n[Han et al., 2016] Song Han, Huizi Mao, and William J Dally. Deep\ncompression: Compressing deep neural networks with pruning,\ntrained quantization and huffman coding. In International Con-\nference on Learning Representations, 2016.\n[He et al., 2016] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and\nJian Sun. Deep residual learning for image recognition. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2016.\n[Ioffe and Szegedy, 2015] Sergey Ioffe and Christian Szegedy.\nBatch normalization: Accelerating deep network training by re-\nducing internal covariate shift. In International Conference on\nMachine Learning. PMLR, 2015.\n[Jacob et al., 2018] Benoit Jacob, Skirmantas Kligys, Bo Chen,\nMenglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam,\nand Dmitry Kalenichenko. Quantization and training of neural\nnetworks for efﬁcient integer-arithmetic-only inference. In Pro-\nceedings of the IEEE Conference on Computer Vision and Pattern\nRecognition, 2018.\n[Jiao et al., 2020] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin\nJiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. Tinybert:\nDistilling BERT for natural language understanding. In Findings\nof the Association for Computational Linguistics: EMNLP, 2020.\n[Kim et al., 2021] Sehoon Kim, Amir Gholami, Zhewei Yao,\nMichael W Mahoney, and Kurt Keutzer. I-bert: Integer-only bert\nquantization. In International Conference on Machine Learning.\nPMLR, 2021.\n[Krizhevsky et al., 2012] Alex Krizhevsky, Ilya Sutskever, and Ge-\noffrey E. Hinton. Imagenet classiﬁcation with deep convolutional\nneural networks. In Advances in Neural Information Processing\nSystems, 2012.\n[Li et al., 2019] Rundong Li, Yan Wang, Feng Liang, Hongwei\nQin, Junjie Yan, and Rui Fan. Fully quantized network for ob-\nject detection. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2019.\n[Lian et al., 2019] Xiaocong Lian, Zhenyu Liu, Zhourui Song, Jiwu\nDai, Wei Zhou, and Xiangyang Ji. High-performance fpga-based\ncnn accelerator with block-ﬂoating-point arithmetic.IEEE Trans-\nactions on Very Large Scale Integration (VLSI) Systems, 2019.\n[Lin et al., 2014] Tsung-Yi Lin, Michael Maire, Serge Belongie,\nJames Hays, Pietro Perona, Deva Ramanan, Piotr Doll ´ar, and\nC Lawrence Zitnick. Microsoft coco: Common objects in con-\ntext. In European Conference on Computer Vision . Springer,\n2014.\n[Liu et al., 2021a] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan\nWei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin trans-\nformer: Hierarchical vision transformer using shifted windows.\narXiv preprint arXiv:2103.14030, 2021.\n[Liu et al., 2021b] Zhenhua Liu, Yunhe Wang, Kai Han, Wei\nZhang, Siwei Ma, and Wen Gao. Post-training quantization for\nvision transformer. In Thirty-Fifth Conference on Neural Infor-\nmation Processing Systems, 2021.\n[Nagel et al., 2020] Markus Nagel, Rana Ali Amjad, Mart van\nBaalen, Christos Louizos, and Tijmen Blankevoort. Up or down?\nadaptive rounding for post-training quantization. InInternational\nConference on Machine Learning. PMLR, 2020.\n[Rao et al., 2021] Yongming Rao, Wenliang Zhao, Benlin Liu, Ji-\nwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efﬁcient\nvision transformers with dynamic token sparsiﬁcation. arXiv\npreprint arXiv:2106.02034, 2021.\n[Shen et al., 2020] Sheng Shen, Zhen Dong, Jiayu Ye, Linjian Ma,\nZhewei Yao, Amir Gholami, Michael W Mahoney, and Kurt\nKeutzer. Q-bert: Hessian based ultra low precision quantization\nof bert. In Proceedings of the AAAI Conference on Artiﬁcial In-\ntelligence, 2020.\n[Touvron et al., 2021] Hugo Touvron, Matthieu Cord, Matthijs\nDouze, Francisco Massa, Alexandre Sablayrolles, and Herv ´e\nJ´egou. Training data-efﬁcient image transformers & distilla-\ntion through attention. In International Conference on Machine\nLearning. PMLR, 2021.\n[Wang et al., 2020] Peisong Wang, Qiang Chen, Xiangyu He, and\nJian Cheng. Towards accurate post-training network quantiza-\ntion via bit-split and stitching. In International Conference on\nMachine Learning. PMLR, 2020.\n[Xu et al., 2021] Yifan Xu, Zhijie Zhang, Mengdan Zhang, Kekai\nSheng, Ke Li, Weiming Dong, Liqing Zhang, Changsheng Xu,\nand Xing Sun. Evo-vit: Slow-fast token evolution for dynamic\nvision transformer. arXiv preprint arXiv:2108.01390, 2021.\n[Zheng et al., 2021] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao,\nXiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic seg-\nmentation from a sequence-to-sequence perspective with trans-\nformers. In Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2021.\n[Zhou et al., 2016] Shuchang Zhou, Yuxin Wu, Zekun Ni, Xinyu\nZhou, He Wen, and Yuheng Zou. Dorefa-net: Training low\nbitwidth convolutional neural networks with low bitwidth gra-\ndients. arXiv preprint arXiv:1606.06160, 2016.\nA Quantized Inference\nA.1 Quantized Inference of LayerNorm\nLayerNorm has been widely used in neural networks to accelerate\nthe convergence during the training process, which can be formu-\nlated as:\nLayerNorm(X) = X −µX√\nσ2\nX + ϵ\n·γ+ β (20)\n= γ√\nσ2\nX + ϵ\nX + β\n√\nσ2\nX + ϵ−γµX\n√\nσ2\nX + ϵ\n, (21)\nwhere γ,β are the learned parameters andµX,σX are the statistics\nwhich need to be calculated based on the input of LayerNorm.\nIn this paper, proposed Power-of-Two Factor is used for Layer-\nNorm input X ∈RB×L×C and its quantized value XQ can be written\nas:\nXQ = Q(X|b) = clip(⌊ X\n2αs⌉+ zp,0,2b −1), (22)\nwhere s,zp ∈ R1 are quantization parameters and α ∈ NC is\nPower-of-Two Factor.\nFollowing the LayerNorm’s deﬁnition, we should calculate the\nstatistics of input X. As described in this paper, the whole process\ncan be divided into two phases. In the ﬁrst phase, we shift the quan-\ntized activation XQ with PTF α:\nˆXQ = (XQ −zp) <<α. (23)\nThen, in the second phase, we need to calculate the statistics based\non the shifted activation ˆXQ. Firstly, we gauge the mean of X and\nX2 as follows:\nµX ≈1\nC\nC∑\ni=1\n(ˆXQi ·s) = s\nC\nC∑\ni=1\n(ˆXQi) →s\nC M1, (24)\nµX2 ≈1\nC\nC∑\ni=1\n(ˆXQi ·s)2 = s2\nC\nC∑\ni=1\n(ˆXQi)2 →s2\nC M2, (25)\nwhere C is the number of channels in X. Secondly, we utilize µX\nand µX2 to calculate σ2\nX:\nσ2\nX = µX2 −µ2\nX ≈s2\nC2 (CM2 −M2\n1), (26)\nand approximate\n√\nσ2\nX + ϵas:\n√\nσ2\nX + ϵ≈s\nC\n√\nCM2 −M2\n1. (27)\nThus, we obtain the statistics of input X based on integer-only\ncalculations.\nAfter the calculations of statistics, we need to do the integrated\ninference of LayerNorm. Equation (20) can be written as follows by\nquantizing the input X and output YQ:\nYQ = ⌊ sinγ\ndout\n√\nσ2\nX + ϵ\nˆXQ + β\n√\nσ2\nX + ϵ−γµX\nsout\n√\nσ2\nX + ϵ\n⌉+ zpout, (28)\nwhere sin ∈R1 is the scale of input X, and sout,zpout ∈R1 are\nthe scale of output YQ.\nIn order to simplify the equation, we fuse each term as follows:\nA = sinγ\nsout\n√\nσ2\nX + ϵ\n, (29)\nB = β\n√\nσ2\nX + ϵ−γµX\nsout\n√\nσ2\nX + ϵ\n. (30)\nFigure 6: Comparison between exponential (exp) and integer-only\nexponential (i-exp)\nTo obtain the integer-only inference, we approximate A as:\nN1 = b−1 −⌊log2 |A|⌋,N2 = ⌊|A|2N1 ⌋, (31)\nA = sign(A) ·N2\n2N1\n, (32)\nwhere bis target bit-width. Finally, the quantized inference for Lay-\nerNorm can be formulated as:\nYQ = ⌊AˆXQ + B⌉+ zpout (33)\n= ⌊sign(A) ·N2 ˆXQ + ⌊B ·2N1 ⌉\n2N1\n⌉+ zpout. (34)\nA.2 Quantized Inference of Softmax\nSoftmax is generally used in the Vision Transformers and it can be\nformulated as:\nSoftmax(X)i = exp(Xi)∑J\nj=1 exp(Xj)\n. (35)\nIn practice, we subtract the maximum value of the input to avoid\noverﬂowing:\n˜Xi = Xi −max(X), (36)\nand thus the Softmax can be written as:\nSoftmax(X)i = exp(˜Xi)∑J\nj=1 exp(˜Xj)\n. (37)\nNow, all inputs, ˜X, become non-positive. We decompose it as\n(−ln 2)z+ p, where zis a non-negative integer and the pis a real\nnumber in (−ln 2,0]. Then, the exponential of ˜X can be written as:\nexp(˜X) = 2−zexp(p) = exp(p) >>z, (38)\nwhere >>is the right shifting.\nI-BERT use a second-order polynomial to approximate the expo-\nnential function in the interval of p∈(−ln 2,0]:\nL(p) = 0.3585(p+ 1.353)2 + 0.344 ≈exp(p), (39)\nexp(˜x) ≈i-exp(˜x) :=L(p) >>z, (40)\nwhere z= ⌊−˜X/ln 2⌋and p= ˜X + zln 2.\nAlgorithm 1 Integer-only Exponential\n1: Input: q,s: quantized input and scale\n2: Output: qout,sout: quantized output and scale\n3: function I-E XP(q,s)\n4: qln 2←⌊−ln 2/s⌋\n5: q←max(q,n ·qln 2)\n6: z←⌊q/qln 2⌋\n7: qp ←q−z·qln 2\n8: qb,qc ←⌊b/s⌋,⌊c/as2⌋\n9: sL ←⌊as2⌋\n10: qL = (q+ qb)2 + qc\n11: qout,sout ←qL <<n −z,sL/2n\n12: return qout,sout\n13: end function\nAlgorithm 2 Log-Int-Softmax\nInput: q,s: quantized input and input scale\nOutput: qout,sout: quantized output and output scale\nfunction I-L OG2(q)\nM ←Find First One(q)\nχ←(M −1)th Bit of q\nqout ←M + χ\nreturn qout\nend function\nfunction LOG-INT-SOFTMAX (q,s)\n˜q←q−max(q)\nqexp,sexp ←I-Exp(˜q,s)\nqrev ←⌊sum(qexp)/qexp⌉\nqout,sout ←N −I-Log2(qrev),1/2N\nreturn qout,sout\nend function\nFigure 6 plots the result of i-exp, which is nearly identical to the\nexponential function. The largest gap between these two functions\nis only 1.9 ×10−3, which is relatively negligible comparing to 8-bit\nquantization error of 1/255 = 3.9 ×10−3.\nBased on i-exp (Algorithm 1), we propose Log-Int-\nSoftmax (LIS) as Algorithm 2 shows. Firstly, the maximum\nvalue of inputs is subtracted to ensure all the inputs are non-positive,\nand the results are sent to the i-exp. After that, we replace the\noriginal Softmax by its reciprocal to ensure the results of integer\ndivision to be larger than 1. Last but not least, we perform a Log2\nquantization on the output of reverse Softmax as:\nLIS(s·XQ) =N −clip(log2⌊\n∑i-exp(XQ)\ni-exp(XQ) ⌉,0,2b −1) (41)\n= N −AttnQ, (42)\nwith N = 2b −1. XQ,s are the quantized input and scale.\nThe Log2 function can be calculated with integer arithmetic as\nshowed in Algorithm 2. To obtain the Log2 of the integer, we in-\ntroduce the Find First One function which returns the index of\nthe most signiﬁcant one bit of the input. The whole process can\nbe done in integer domain. For example, if the input of I-Log2 is\n0000 1101 1010 11002, the M,χ will be calculated as 11,1 and the\nrounding result will be 12.\nThe following step of Softmax is the matrix multiplication be-\ntween attention map Attn and values V. Considering VQ is the quan-\ntized value of V and its scale and zero point are sV,zpV ∈R1, the\nFigure 7: Top-1 accuracy on ImageNet with different hyperparame-\nter K.\nmatrix multiplication can be written as:\nAttn ·V = 2N−AttnQ\n2N ·(VQ −zpV)sV (43)\n= sV\n2N ·(VQ −zpV) <<(N −AttnQ). (44)\nAs we can see, the multiplication is converted to BitShift operator\nwith an output scale sV\n2N , where N equals 24 −1 = 15in 4-bit quan-\ntization.\nBased on the above processes, we realize fully ﬁxed-point infer-\nence for Softmax and accelerate the calculation of attention map and\nvalues by using BitShift.\nB Hyperparameter K of Power-of-Two Factor\nWe conduct experiments on the setting of hyperparameter K in PTF.\nAs shown in Figure 7, we evaluate the top-1 accuracy of vision trans-\nformers with different K. It is observed that the accuracy is nearly\nsaturated when K is taken as 3. As a result, to meet the different\ninter-channel variants between all models as much as possible, we\nchoose 3 as the default value.\nC Inter-channel Variation for Vision\nTransformers and ResNets\nIn Figure 8, we present the channel-wise minimum and maximum\nvalues of Vision Transformers and ResNets. For comparison, we\nchoose the input of the last LayerNorm layer for Vision Transform-\ners and the output of the fourth stage for ResNets to show. It is\nobserved that a serious inter-channel variation are found in Vision\nTransformers.\n0 500 1000 1500 2000\nR50\n0\n5\n10\n15\n20\nMinimum Value\nMaximum Value\n0 500 1000 1500 2000\nR101\n0\n5\n10\n15\n20\n25 Minimum Value\nMaximum Value\n0 500 1000 1500 2000\nR152\n0\n5\n10\n15\n20 Minimum Value\nMaximum Value\n0 50 100 150 200\nDeiT-T\n10\n5\n0\n5\n10\n15\n20\n25 Minimum Value\nMaximum Value\n0 100 200 300 400\nDeiT-S\n20\n0\n20\n40\n60\n80\n100\n120\nMinimum Value\nMaximum Value\n0 200 400 600 800\nDeiT-B\n300\n200\n100\n0\nMinimum Value\nMaximum Value\n0 200 400 600 800\nSwin-T\n150\n100\n50\n0\n50\n100\n150\nMinimum Value\nMaximum Value\n0 200 400 600 800\nSwin-S\n250\n200\n150\n100\n50\n0\nMinimum Value\nMaximum Value\n0 200 400 600 800 1000\nSwin-B\n200\n0\n200\n400\n600 Minimum Value\nMaximum Value\n0 200 400 600 800\nViT-B\n100\n75\n50\n25\n0\n25\n50 Minimum Value\nMaximum Value\n0 200 400 600 800 1000\nViT-L\n300\n200\n100\n0\n100\n200\n300\n400\nMinimum Value\nMaximum Value\nFigure 8: Channel-wise minimum and maximum values of Vision Transformers and ResNets.",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.7194989323616028
    },
    {
      "name": "Convolutional neural network",
      "score": 0.7113848924636841
    },
    {
      "name": "Inference",
      "score": 0.7000758647918701
    },
    {
      "name": "Computer science",
      "score": 0.6821150779724121
    },
    {
      "name": "Quantization (signal processing)",
      "score": 0.5926191806793213
    },
    {
      "name": "Transformer",
      "score": 0.5765931606292725
    },
    {
      "name": "Encoder",
      "score": 0.4937478005886078
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4186446964740753
    },
    {
      "name": "Algorithm",
      "score": 0.3657624125480652
    },
    {
      "name": "Computer engineering",
      "score": 0.36077138781547546
    },
    {
      "name": "Voltage",
      "score": 0.11055690050125122
    },
    {
      "name": "Engineering",
      "score": 0.1033858060836792
    },
    {
      "name": "Electrical engineering",
      "score": 0.07685026526451111
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": []
}