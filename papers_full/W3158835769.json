{
  "title": "XLM-T: A Multilingual Language Model Toolkit for Twitter.",
  "url": "https://openalex.org/W3158835769",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2117437543",
      "name": "Francesco Barbieri",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2319266098",
      "name": "Luis Espinosa Anke",
      "affiliations": [
        "Cardiff University"
      ]
    },
    {
      "id": "https://openalex.org/A2565581196",
      "name": "Jose Camacho Collados",
      "affiliations": [
        "Cardiff University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2295710275",
    "https://openalex.org/W2579465987",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035390927",
    "https://openalex.org/W2591649529",
    "https://openalex.org/W2112544000",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2972573244",
    "https://openalex.org/W2460159515",
    "https://openalex.org/W2963667932",
    "https://openalex.org/W2963729324",
    "https://openalex.org/W2138738738",
    "https://openalex.org/W3037833206",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2894947520",
    "https://openalex.org/W3113473253",
    "https://openalex.org/W3023528699",
    "https://openalex.org/W3034238904",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W2514567832",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W2806198715",
    "https://openalex.org/W3104186312",
    "https://openalex.org/W2963626623",
    "https://openalex.org/W2794504925",
    "https://openalex.org/W2780851414",
    "https://openalex.org/W2770073503",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W2952638691",
    "https://openalex.org/W2741388816",
    "https://openalex.org/W2927746189",
    "https://openalex.org/W3093517588",
    "https://openalex.org/W3035032094",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W2394823018",
    "https://openalex.org/W2922580172",
    "https://openalex.org/W3115081393",
    "https://openalex.org/W3120253119"
  ],
  "abstract": "Language models are ubiquitous in current NLP, and their multilingual capacity has recently attracted considerable attention. However, current analyses have almost exclusively focused on (multilingual variants of) standard benchmarks, and have relied on clean pre-training and task-specific corpora as multilingual signals. In this paper, we introduce XLM-T, a framework for using and evaluating multilingual language models in Twitter. This framework features two main assets: (1) a strong multilingual baseline consisting of an XLM-R (Conneau et al. 2020) model pre-trained on millions of tweets in over thirty languages, alongside starter code to subsequently fine-tune on a target task; and (2) a set of unified sentiment analysis Twitter datasets in eight different languages. This is a modular framework that can easily be extended to additional tasks, as well as integrated with recent efforts also aimed at the homogenization of Twitter-specific datasets (Barbieri et al. 2020).",
  "full_text": "XLM-T: A Multilingual Language Model Toolkit for Twitter\nFrancesco Barbieri♣, Luis Espinosa Anke♦, Jose Camacho-Collados♦\n♣ Snap Inc., Santa Monica, California, USA\n♦ Cardiff NLP, School of Computer Science and Informatics, Cardiff University, UK\n♣ fbarbieri@snap.com,\n♦ {espinosa-ankel,camachocolladosj}@cardiff.ac.uk\nAbstract\nLanguage models are ubiquitous in current\nNLP, and their multilingual capacity has re-\ncently attracted considerable attention. How-\never, current analyses have almost exclusively\nfocused on (multilingual variants of) standard\nbenchmarks, and have relied on clean pre-\ntraining and task-speciﬁc corpora as multilin-\ngual signals. In this paper, we introduce XLM-\nT, a framework1 for using and evaluating mul-\ntilingual language models in Twitter. This\nframework features two main assets: (1) a\nstrong multilingual baseline consisting of an\nXLM-R (Conneau et al., 2020) model pre-\ntrained on millions of tweets in over thirty lan-\nguages, alongside starter code to subsequently\nﬁne-tune on a target task; and (2) a set of\nuniﬁed sentiment analysis Twitter datasets in\neight different languages. This is a modular\nframework that can easily be extended to addi-\ntional tasks, as well as integrated with recent\nefforts also aimed at the homogenization of\nTwitter-speciﬁc datasets (Barbieri et al., 2020).\n1 Introduction\nMultilingual NLP is increasingly becoming popu-\nlar. Despite the concerning disparity in terms of\nlanguage resource availability (Joshi et al., 2020),\nthe advent of Language Models (LMs) has indis-\nputably enabled a myriad of multilingual archi-\ntectures to ﬂuorish, ranging from LSTMs to the\narguably more popular transformer-based models\n(Chronopoulou et al., 2019; Pires et al., 2019). Mul-\ntilingual LMs integrate streams of multilingual tex-\ntual data without being tied to one single task, learn-\ning general-purpose multilingual representations\n(Hu et al., 2020). As testimony of this landscape,\nwe ﬁnd multilingual variants stemming from well-\nknown monolingual LMs, which have now become\na standard among the NLP community. For in-\nstance, mBERT from BERT (Devlin et al., 2019),\n1https://github.com/cardiffnlp/xlm-t.\nmT5 (Xue et al., 2020) from T5 (Raffel et al., 2020)\nor XLM-R (Conneau et al., 2020) from RoBERTa\n(Liu et al., 2019). Social media data, however, and\nspeciﬁcally Twitter (the platform we focus on in\nthis paper), seem to be so far surprisingly neglected\nfrom this trend of massive multilingual pretraining.\nThis may be due to, in addition to its well-known\nuncurated nature (Derczynski et al., 2013), because\nof discoursive and platform-speciﬁc factors such\nas out-of-distribution samples, misspellings, slang,\nvulgarisms, emoji and multimodality, among oth-\ners (Barbieri et al., 2018; Camacho-Collados et al.,\n2020). This is an important consideration, as there\nis ample agreement that the quality of LM-based\nmultilingual representations is strongly correlated\nwith typological similarity (Hu et al., 2020), which\nis somewhat blurred out in the context of Twitter.\nIn this paper, we bridge this gap by introduc-\ning a toolkit for evaluating multilingual Twitter-\nspeciﬁc language models. This framework, which\nwe make available to the NLP community, is ini-\ntially comprised of a large multilingual Twitter-\nspeciﬁc LM based on XLM-R checkpoints (Sect.\n2), from which we report an initial set of baseline\nresults in different settings (including zero-shot).\nMoreover, we provide starting code for analyz-\ning, ﬁne-tuning and evaluating existing language\nmodels. To carry out a comprehensive multilin-\ngual evaluation, while also laying the foundations\nfor future extensions, we devise a uniﬁed dataset\nin 8 languages for sentiment analysis (which we\ncall Uniﬁed Multilingual Sentiment Analysis Bench-\nmark, UMSAB henceforth), as this task is by far\nthe most studied problem in NLP in Twitter (cf.,\ne.g., Salameh et al. (2015); Zhou et al. (2016);\nMeng et al. (2012); Chen et al. (2018); Rasooli et al.\n(2018); Vilares et al. (2017); Barnes et al. (2019);\nPatwa et al. (2020); Barriere and Balahur (2020)).\nFinally, in order to have a solid point of compari-\nson with respect to standard English Twitter tasks,\narXiv:2104.12250v1  [cs.CL]  25 Apr 2021\nEnglishPortugueseSpanishArabicKoreanJapaneseIndonesianTagalogTurkishFrenchUNKRussianThaiItalianGermanPersianPolishHindiDutchHaitianEstonianUrduCatalanSwedishFinnishGreekCzechBasqueHebrewTamilChineseNorwegianDanishWelshLatvianHungarianRomanianLithuanianVietnameseUkrainianNepaliSlovenianIcelandicSerbianMalayalamBengaliBulgarianMarathiSinhalaTeluguKannadaKurdishPushtoGujaratiBurmeseAmharicArmenianOriyaSindhiPanjabiKhmerGeorgianLaoDhivehiUighur\n103\n104\n105\n106\n107\nFigure 1: Distribution of languages of the 198M tweets used to ﬁnetune the Twitter-based language model (log\nscale). UNK corresponds to unidentiﬁed tweets according to the Twitter API.\nwe also report results on the TweetEval framework\n(Barbieri et al., 2020). Our results suggest that\nwhen ﬁne-tuning task-speciﬁc Twitter-based multi-\nlingual LMs, a domain-speciﬁc model proves more\nconsistent than its general-domain counterpart, and\nthat in some cases a smart selection of training data\nmay be preferred than large-scale ﬁne-tuning on\nmany languages.\n2 Language Models in Twitter\nOur framework revolves around Twitter-speciﬁc\nlanguage models. In particular, we train our own\nmultilingual language-speciﬁc language model\n(Section 2.1), which we then ﬁne-tune for various\nmonolingual and multilingual applications, and for\nwhich we provide a suitable interface (Section 2.2).\nAdditionally, we complement these basic function-\nalities with starter code for computing tweet em-\nbeddings, multilingual tweet retrieval and analysis\nbased on the released language models.\n2.1 Released Language Models\nWe used the Twitter API to retrieve 198M tweets2\nposted between May’18 and March’20, which are\nour source data for LM pretraining. We only con-\nsidered tweets with at least three tokens and with\nno URLs to avoid bot tweets and spam advertising.\nAdditionally, we did not perform language ﬁltering,\naiming at capturing a general distribution. Figure 1\nlists 30 most represented languages by frequency,\nshowing a prevalence of widely spoken languages\nsuch as English, Portuguese and Spanish, with the\nﬁrst signiﬁcant drop in frequency affecting Russian\nat the 11th position.\n21,724 million tokens (12G of uncompressed text).\nIn terms of opting for pretraining a LM from\nscratch or building upon an existing one, we follow\nGururangan et al. (2020) and Barbieri et al. (2020)\nand continue training an XLM-R language model\nfrom publicly available checkpoints 3, which we\nselected due to the high results it has achieved in\nseveral multilingual NLP tasks (Hu et al., 2020).\nWe use the same masked LM objective, and train\nuntil convergence in a validation set. The model\nconverged after about 14 days on 8 NVIDIA V100\nGPUs.4\nWhile this multilingual language model (referred\nto as XLM-Twitter henceforth) is the main focus\non this paper, our toolkit also integrates monolin-\ngual language models of any nature, including the\nEnglish monolingual Twitter models released in\nBarbieri et al. (2020) and Nguyen et al. (2020).\n2.2 Language Model Fine-tuning\nIn this section we explain the ﬁne-tuning implemen-\ntation of our framework. The main task evaluated\nin this paper is tweet classiﬁcation, for which we\nprovide uniﬁed datasets. One of the main differ-\nences with respect to standard ﬁne-tuning is that\nwe integrate the adapter technique (Houlsby et al.,\n2019), by means of which we freeze the LM and\nonly ﬁne-tune one additional classiﬁcation layer.\nWe follow the same adapter conﬁguration proposed\nin Pfeiffer et al. (2020). This technique provides\nbeneﬁts in terms of memory and speed, which in\npractice facilitates the usage of multilingual lan-\n3https://huggingface.co/\nxlm-roberta-base.\n4The estimated cost for the language model pre-training is\nUSD 5,000 on Google Cloud.\nFigure 2: Code snippet showcasing the feature extrac-\ntion and tweet similarity interface. Note that using our\nTwitter-speciﬁc XLM-R model leads to emoji playing\na crucial role in the semantics of the tweet.\nguage models for a wider set of NLP practitioners\nand researchers.5\nStarting code. In order to enable fast prototyp-\ning on our framework, in addition to datasets\nand pretrained models we also provide code base\nfor feature extraction from Tweets (i.e., obtaining\ntweet embeddings) (Figure 2), tweet classiﬁcation\nand model ﬁne-tuning, which again, is based on\nthe adapter technique (cf. Section 3.2).\n3 Evaluation\nWe assess the reliability of our released multilin-\ngual Twitter-speciﬁc language model in two differ-\nent ways: (1) we perform an evaluation on a wide\nrange of English-speciﬁc datasets (Section 3.1);\n(2) we compose a large multilingual benchmark\nfor sentiment analysis where we assess the mul-\ntilingual capabilities of the language model (Sec-\ntion 3.2).\nExperimental Setting. In each experiment we\nperform three runs with different seeds, and use\nearly stopping on the validation loss. We only tune\nthe learning rate (0.001 and 0.0001) and, unless\nnoted otherwise, all results we report are the av-\nerage of three runs of macro-average F1 scores.In\n5In this paper we do not focus on evaluating the reliability\nof the adapter technique as this was tested in previous papers.\nInstead, we add this functionality as default for the code and\nall our experiments.\nterms of models, we evaluate a standard pre-trained\nXLM-R and XLM-Twitter, our XLM-R model\npretrained on a multilingual Twitter dataset starting\nfrom XLM-R checkpoints (see Sect. 2.1). For the\nmonolingual experiments we also include a Fast-\nText (FT) baseline (Joulin et al., 2017), which relies\non monolingual FT embeddings trained on Com-\nmon Crawl and Wikipedia (Grave et al., 2018) as\ninitialization for each language lookup table.\n3.1 Monolingual Evaluation (TweetEval)\nIn order to provide an additional point of compari-\nson for our released multilingual language model,\nwe perform an evaluation on standard Twitter-\nspeciﬁc tasks in English, for which we can com-\npare its performance with existing models. In par-\nticular, we evaluate XLM-Twitter on a suite of\nseven heterogeneous tweet classiﬁcation tasks from\nthe TweetEval benchmark (Barbieri et al., 2020).\nTweetEval is composed of seven tasks: emoji pre-\ndiction (Barbieri et al., 2018), emotion recognition\n(Mohammad et al., 2018), hate speech detection\n(Basile et al., 2019), irony detection (Van Hee et al.,\n2018), offensive language identiﬁcation (Zampieri\net al., 2019), sentiment analysis (Rosenthal et al.,\n2019) and stance detection 6 (Mohammad et al.,\n2016).\nTable 1 shows the results of the language mod-\nels and TweetEval baselines7 As can be observed,\nour proposed XLM-R-Twitter improves over strong\nbaselines such as RoBERTa-base and XLM-R\nthat do not make use of Twitter corpora, and\nRoBERTa-Twitter, which is trained on Twitter cor-\npora only. This highlights the reliability of our\nmultilingual model in language-speciﬁc settings.\nHowever, it underperforms when compared with\nmonolingual Twitter-speciﬁc models, such as the\nRoBERTa model futher pre-trained on English\ntweets proposed in Barbieri et al. (2020), as well\nas BERTweet (Nguyen et al., 2020), which was\ntrained on a corpus that is an order of magnitude\nlarger.8 This is to be expected as goes in line with\nprevious research that shows that multilingual mod-\nels tend to underperform monolingual models in\n6The stance detection dataset is in turn is divided in ﬁve\nsubtopics.\n7Please refer to the original TweetEval paper (Barbieri\net al., 2020) for details on the implementation of all the base-\nlines.\n8While XLM-R-Twitter was ﬁne-tuned on the same\namount of English tweets (60M) than RoBERTa-Tw,\nBERTweet was trained on 850M English tweets.\nEmoji Emotion Hate Irony Offensive Sentiment Stance ALL\nSVM 29.3 64.7 36.7 61.7 52.3 62.9 67.3 53.5\nFastText 25.8 65.2 50.6 63.1 73.4 62.9 65.4 58.1\nBLSTM 24.7 66.0 52.6 62.8 71.7 58.3 59.4 56.5\nRoB-Bs30.9±0.2(30.8)76.1±0.5(76.6)46.6±2.5(44.9)59.7±5.0(55.2)79.5±0.7(78.7)71.3±1.1(72.0) 68±0.8(70.9) 61.3\nRoB-RT31.4±0.4(31.6) 78.5±1.2(79.8) 52.3±0.2(55.5) 61.7±0.6(62.5)80.5±1.4(81.6) 72.6±0.4(72.9) 69.3±1.1(72.6) 65.2\nRoB-Tw29.3±0.4(29.5)72.0±0.9(71.7)46.9±2.9(45.1)65.4±3.1(65.1)77.1±1.3(78.6)69.1±1.2(69.3)66.7±1.0(67.9) 61.0\nXLM-R28.6±0.7(27.7)72.3±3.6(68.5)44.4±0.7(43.9)57.4±4.7(54.2)75.7±1.9(73.6)68.6±1.2(69.6)65.4±0.8(66.0) 57.6\nXLM-Tw30.9±0.5(30.8)77.0±1.5(78.3)50.8±0.6(51.5)69.9±1.0(70.0) 79.9±0.8(79.3)72.3±0.2(72.3)67.1±1.4(68.7) 64.4\nSotA 33.4 79.3 56.4 82.1 79.5 73.4 71.2 67.9\nMetric M-F1 M-F1 M-F1 F(i) M-F1 M-Rec A VG (F(a),F(f)) TE\nTable 1: TweetEval test results. For neural models we report both the average result from three runs and its\nstandard deviation, and the best result according to the validation set (parentheses). SotA results correspond to the\nbest TweetEval reported system, i.e., Nguyen et al. (2020).\nlanguage-speciﬁc tasks (Rust et al., 2020). 9 In\nthe following section we evaluate XLM-Twitter\non multilingual settings, including evaluation in\nmonolingual and cross-lingual scenarios.\n3.2 Multilingual Evaluation (Sentiment\nAnalysis)\nWe focus our evaluation on multilingual Sentiment\nAnalysis (SA). We ﬁrst ﬂesh out the process fol-\nlowed to compile and unify our cross-lingual SA\nbenchmark (Sect. 3.2.1). Our experiments 10 can\nthen be grouped into two types: when no training\nin the target language is available, i.e., zero-shot\n(Sect. 3.2.2), and when the evaluated models have\naccess to target language training data, either alone\nor as part of a larger fully multilingual training set\n(Sect. 3.2.3).\n3.2.1 Uniﬁed Multilingual Sentiment\nAnalysis Benchmark (UMSAB)\nWe aim at constructing a balanced multilingual\nSA dataset, i.e., where all languages are equally\ndistributed in terms of frequency, and with repre-\nsentation of typologically distant languages. To\nthis end, we compiled monolingual SA datasets\nfor eight diverse languages, whose statistics we list\nin Table 2, as well as their spanning timeframes.\nGiven that retaining the original distribution would\nskew the uniﬁed dataset towards the most frequent\nlanguages, we established a maximum number of\ntweets corresponding to the size of the smallest\n9It has been shown that this performance difference could\nbe further decreased by using language-speciﬁc tokenizers\n(Rust et al., 2020), but this was out of scope for this paper.\n10Standard deviation and best run results are provided, for\ncompleteness, in the appendix.\ndataset, speciﬁcally the 3,033 for the Hindi portion,\nand prune all data splits for all languages with this\nthreshold. This leaves 1,839 training tweets (with\n15% of them allocated to a ﬁxed validation set),\nand 870 for testing. The total size of the dataset is\nthus 24,262 tweets. Let us highlight two additional\nimportant design decisions: ﬁrst, we enforced a\nbalanced distribution across the three labels (posi-\ntive, negative and neutral), and second, we kept the\noriginal training/test splits in each dataset. After\nthis preprocessing, we obtain 8 datasets of 3,033\ninstances, respectively. Note that some languages\nin this dataset agglutinate or refer to speciﬁc vari-\nations. In particular, we use Hindi to refer to the\ngrouping of Hindi, Bengalu and Tamil, Portuguese\nfor Brazilian Portuguese, and Spanish for Iberian,\nPeruvian and Costa Rican variations.\n3.2.2 Zero-shot Cross-lingual Transfer\nTable 3 shows zero-shot results of XLM-R and\nXLM-Twitter in our multilingual sentiment analy-\nsis benchmark. The performance of both models\nis competitive, especially considering the diversity\nof domains11 and that the source language was not\nseen during training. An interesting observation\nconcerns those cases in which zero-shot models\noutperform their monolingual counterparts (e.g.,\nEnglish→Arabic or Italian→Hindi). Additionally,\nXLM-Twitter proves more robust, achieving the\nbest overall results in six of the eight languages,\nwith consistent improvements in general, and with\nremarkable improvements in e.g., Hindi, outper-\nforming XLM-R by 7.9 absolute points. Finally,\n11For instance, for Arabic we ﬁnd trending topics such\nas iPhone or vegetarianism, where the Portuguese dataset is\ndominated by comments on TV shows.\nLanguage Dataset Time-Train Time-Test\nArabic SemEval-2017 (Rosenthal et al., 2017) 09/2016 to 11/2016 12/2016 to 1/2017\nEnglish SemEval-2017 (Rosenthal et al., 2017) 01/2012 to 12/2015 12/2016 to 1/2017\nFrench Deft-2017 (Benamara et al., 2017) 2014-2016 Same\nGerman SB-10K (Cieliebak et al., 2017) 8/2013 to 10/2013 Same\nHindi SAIL 2015 (Patra et al., 2015) NA, 3 months Same\nItalian Sentipolc-2016 (Barbieri et al., 2016) 2013-2016 2016\nPortuguese SentiBR (Brum and Nunes, 2017) 1/2017-7/2017 Same\nSpanish Intertass 2017 (D ´ıaz Galiano et al., 2018) 7/2016 to 01/2017 Same\nTable 2: Sentiment analysis datasets for the eight languages used in our experiments.\nXLM-R XLM-Twitter\nAr En Fr De Hi It Pt Es All-1 Ar En Fr De Hi It Pt Es All-1\nAr 63.6 64.1 54.4 53.9 22.9 57.4 62.4 62.2 59.2 67.7 66.6 62.1 59.3 46.3 63.0 60.1 65.3 64.3\nEn 64.2 68.2 61.6 63.5 23.7 68.1 65.9 67.8 68.2 64.0 66.9 60.6 67.8 35.2 67.7 61.6 68.7 70.3\nFr 45.4 52.1 72.0 36.5 16.7 43.3 40.8 56.7 53.6 47.7 59.2 68.2 38.7 20.9 45.1 38.6 52.5 50.0\nDe 43.5 64.4 55.2 73.6 21.5 60.8 60.1 62.0 63.6 46.5 65.0 56.4 76.1 36.9 66.3 65.1 65.8 65.9\nHi 48.2 52.7 43.6 47.6 36.6 54.4 51.6 51.7 49.9 50.0 55.5 51.5 44.4 40.3 56.1 51.2 49.5 57.8\nIt 48.8 65.7 63.9 66.9 22.1 71.5 63.1 58.9 65.7 41.9 59.6 60.8 64.5 24.6 70.9 64.7 55.1 65.2\nPt 41.5 63.2 57.9 59.7 26.5 59.6 67.1 65.0 65.0 56.4 67.7 62.8 64.4 26.0 67.1 76.0 64.0 71.4\nEs 47.1 63.1 56.8 57.2 26.2 57.6 63.1 65.9 63.0 52.9 66.0 64.5 58.7 30.7 62.4 67.9 68.5 66.2\nTable 3: Zero-shot cross-lingual experiments. We use the best model in the language on the column and evaluate\non the test set of the language of each row. For example, when we forward the best XLM-R trained on English text\non the Arabic test set we obtain 64.1. In the columns All minus one (All-1) we train on all the languages excluding\nthe one of each row. For example, we obtain a F1 of 59.2 on the Arabic test set when we train an XLM-R using all\nthe languages excluding Arabic. On the diagonals, in gray, models are trained and evaluated on the same language.\nlet us provide some insights on the results obtained\nin an all-minus-one (the All-1 columns in Table\n3) setting. Here, notable cases are, ﬁrst, Hindi, in\nwhich XLM-R and XLM-Twitter models beneﬁt\nsubstantially by having access to more training data,\nwith this improvement being more pronounced in\nXLM-Twitter. Second, the results for the English\ndataset suggest that compiling a larger training set\nhelps, although this may be also attributed to iden-\ntical tokens shared between English and the other\nlanguages, such as named entities, hashtags or col-\nloquialisms and slang.\n3.2.3 Cross-lingual Transfer with Target\nLanguage Training Data\nTable 4 shows macro-F1 results for the follow-\ning three settings: (1) monolingual, where we\ntrain and test in one single language; (2) bilingual,\nwhere we use the best-performing cross-lingual\nzero-shot model, and continue ﬁne-tuning on train-\ning data from the target language; and (3) an en-\ntirely multilingual setting where we train with data\nfrom all languages. One of the most notable conclu-\nsions in the light of these ﬁgures is that increasing\nthe training data even in different languages is a\nuseful strategy, and is particularly rewarding in the\ncase of XLM-Twitter and in challenging datasets\nand languages (e.g., the Hindi results signiﬁcantly\nincrease from 40.29 to 56.39). Interestingly, a\nsmart selection of languages based on validation\naccuracy achieves better results than if trained on\nall languages in half of the cases. This may be due\nto the (dis)similarity of the datasets (in terms of\ntopic or typological proximity), although overall\nthe main conclusion we can draw is that there is an\nobvious trade-off, as a single multilingual model is\noften more practical and versatile.\n4 Qualitative Analysis\nAs an additional qualitative analysis, we plot in\nFigure 3 a sample of similarity scores (by cosine\ndistance) between XLM-Twitter-based embeddings\nobtained from the English training set and the sen-\ntiment analysis test sets for the other 7 languages\n(see Section 3.2.1). In addition to the clearly low\nresemblance with Hindi, we ﬁnd that the most sim-\nilar languages in the embedding space are English\nMonolingual Bilingual Multilingual\nFT XLM-R XLM-Tw XLM-R XLM-Tw XLM-R XLM-Tw\nAr 45.98 63.56 67.67 63.63(En) 67.65(En) 64.31 66.89\nEn 50.85 68.18 66.89 65.07 (It) 67.47(Es) 68.52 70.63\nFr 54.82 71.98 68.19 73.55(Sp) 68.24(En) 70.52 71.18\nDe 59.56 73.61 76.13 72.48 (En) 75.49(It) 72.84 77.35\nHi 37.08 36.60 40.29 33.57 (It) 55.35(It) 53.39 56.39\nIt 54.65 71.47 70.91 70.43 (Ge) 73.50(Pt) 68.62 69.06\nPt 55.05 67.11 75.98 71.87 (Sp) 76.08(En) 69.79 75.42\nSp 50.06 65.87 68.52 67.68 (Po) 68.68(Pt) 66.03 67.91\nAll 51.01 64.80 66.82 64.78 69.06 66.75 69.35\nTable 4: Cross-lingual sentiment analysis F1 results on target languages using target language training data (Mono-\nlingual) only, combined with training data from another language (Bilingual) and with all languages at once (Mul-\ntilingual). ”All” is computed as the average of all individual results.\n0.9 0.93 0.95 0.980\n10\n20\n30\n40\n50\nSimilarity\nDensity\nAr\nFr\nDe\nHi\nIt\nPt\nEs\nFigure 3: Cross-lingual similarity (by cosine distance)\nbetween the English training set and the test sets in the\nother 7 languages. The embeddings are obtained by\naveraging all the XLM-Twitter contextualized embed-\ndings for each tweet.\nare French, suggesting that not only typology, but\nalso topic overlap, may play an important role in\nthe quality of these multilingual representations.\nThis becomes even more apparent in Arabic, which\ndiffers from English in typology and script, but has\nsimilar representations. The Arabic and English\ndatasets were obtained using the same keywords.\n5 Conclusions\nWe have presented a framework and an analysis\non the cross-lingual capabilities of Twitter-based\nmultilingual LMs. As main test bed for our mul-\ntilingual experiments, we focused on sentiment\nanalysis, for which we collected datasets in eight\nlanguages. After a uniﬁcation and standardization\nof the evaluation benchmark, we compared the\nTwitter-based multilingual language model with\na standard multilingual language model trained\non general-domain corpora. This multilingual lan-\nguage model along with starting and evaluation\ncode are released to facilitate research in Twitter at\na multilingual scale (over thirty languages used for\ntraining data).\nThe results highlight the potential of the domain-\nspeciﬁc language model, as more suited to han-\ndle social media and speciﬁcally multilingual SA.\nFinally, our analysis reveals trends and potential\nfor this Twitter-based multilingual language model\nin zero-shot cross-lingual settings when language-\nspeciﬁc training data is not available. For future\nwork we are planning to extend this analysis to\nmore languages and tasks, but also to deepen the\ncross-lingual zero and few shot analysis, particu-\nlarly focusing on typologically similar languages.\nFinally, and due to the seasonal nature of Twitter,\nit would also be interesting to explore correlations\nbetween topic distribution and trends and perfor-\nmance in downstream applications.\nAcknowledgments\nWe would like to thank Eugenio Mart´ınez-C´amara\nfor his involvement in the ﬁrst stages of this project.\nReferences\nFrancesco Barbieri, Valerio Basile, Danilo Croce,\nMalvina Nissim, Nicole Novielli, and Viviana Patti.\n2016. Overview of the evalita 2016 sentiment polar-\nity classiﬁcation task. In Proceedings of third Ital-\nian conference on computational linguistics (CLiC-\nit 2016) & ﬁfth evaluation campaign of natural lan-\nguage processing and speech tools for Italian. Final\nWorkshop (EVALITA 2016).\nFrancesco Barbieri, Jose Camacho-Collados, Luis Es-\npinosa Anke, and Leonardo Neves. 2020. TweetE-\nval: Uniﬁed benchmark and comparative evaluation\nfor tweet classiﬁcation. In Findings of the Associ-\nation for Computational Linguistics: EMNLP 2020 ,\npages 1644–1650, Online. Association for Computa-\ntional Linguistics.\nFrancesco Barbieri, Jose Camacho-Collados,\nFrancesco Ronzano, Luis Espinosa Anke, Miguel\nBallesteros, Valerio Basile, Viviana Patti, and\nHoracio Saggion. 2018. Semeval 2018 task 2:\nMultilingual emoji prediction. In Proceedings\nof The 12th International Workshop on Semantic\nEvaluation, pages 24–33.\nJeremy Barnes, Lilja Øvrelid, and Erik Velldal. 2019.\nSentiment analysis is not solved! assessing and prob-\ning sentiment classiﬁcation. In Proceedings of the\n2019 ACL Workshop BlackboxNLP: Analyzing and\nInterpreting Neural Networks for NLP, pages 12–23,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nValentin Barriere and Alexandra Balahur. 2020. Im-\nproving sentiment analysis over non-English tweets\nusing multilingual transformers and automatic trans-\nlation for data-augmentation. In Proceedings of\nthe 28th International Conference on Computational\nLinguistics, pages 266–271, Barcelona, Spain (On-\nline). International Committee on Computational\nLinguistics.\nValerio Basile, Cristina Bosco, Elisabetta Fersini,\nDebora Nozza, Viviana Patti, Francisco Manuel\nRangel Pardo, Paolo Rosso, and Manuela San-\nguinetti. 2019. SemEval-2019 task 5: Multilin-\ngual detection of hate speech against immigrants and\nwomen in Twitter. In Proceedings of the 13th Inter-\nnational Workshop on Semantic Evaluation , pages\n54–63, Minneapolis, Minnesota, USA. Association\nfor Computational Linguistics.\nFarah Benamara, Cyril Grouin, Jihen Karoui,\nV´eronique Moriceau, and Isabelle Robba. 2017.\nAnalyse d’opinion et langage ﬁguratif dans des\ntweets: pr ´esentation et r ´esultats du d ´eﬁ fouille\nde textes deft2017. In D´eﬁ Fouille de Textes\nDEFT2017. Atelier TALN 2017. Association pour le\nTraitement Automatique des Langues (ATALA).\nHenrico Bertini Brum and Maria das Grac ¸as V olpe\nNunes. 2017. Building a sentiment corpus of\ntweets in brazilian portuguese. arXiv preprint\narXiv:1712.08917.\nJose Camacho-Collados, Yerai Doval, Eugenio\nMart´ınez-C´amara, Luis Espinosa-Anke, Francesco\nBarbieri, and Steven Schockaert. 2020. Learning\ncross-lingual word embeddings from twitter via\ndistant supervision. In Proceedings of the Interna-\ntional AAAI Conference on Web and Social Media ,\nvolume 14, pages 72–82.\nXilun Chen, Yu Sun, Ben Athiwaratkun, Claire Cardie,\nand Kilian Weinberger. 2018. Adversarial deep av-\neraging networks for cross-lingual sentiment classi-\nﬁcation. Transactions of the Association for Compu-\ntational Linguistics, 6:557–570.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An embarrassingly\nsimple approach for transfer learning from pre-\ntrained language models. In Proceedings of the\n2019 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long and\nShort Papers), pages 2089–2095.\nMark Cieliebak, Jan Milan Deriu, Dominic Egger, and\nFatih Uzdilli. 2017. A twitter corpus and benchmark\nresources for german sentiment analysis. In Pro-\nceedings of the Fifth International Workshop on Nat-\nural Language Processing for Social Media , pages\n45–51.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nLeon Derczynski, Alan Ritter, Sam Clark, and Kalina\nBontcheva. 2013. Twitter part-of-speech tagging for\nall: Overcoming sparse and noisy data. In Proceed-\nings of the international conference recent advances\nin natural language processing ranlp 2013 , pages\n198–206.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers) ,\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nManuel Carlos D ´ıaz Galiano, Eugenio\nMart´ınez C´amara, Miguel ´Angel Garc´ıa Cumbreras,\nManuel Garc ´ıa Vega, and Julio Villena Rom ´an.\n2018. The democratization of deep learning in tass\n2017. In TASS 2017 . Sociedad Espa ˜nola para el\nProcesamiento del Lenguaje Natural.\nEdouard Grave, Piotr Bojanowski, Prakhar Gupta, Ar-\nmand Joulin, and Tomas Mikolov. 2018. Learning\nword vectors for 157 languages. In Proceedings\nof the International Conference on Language Re-\nsources and Evaluation (LREC 2018).\nSuchin Gururangan, Ana Marasovi ´c, Swabha\nSwayamdipta, Kyle Lo, Iz Beltagy, Doug Downey,\nand Noah A. Smith. 2020. Don’t stop pretraining:\nAdapt language models to domains and tasks. In\nProceedings of the 58th Annual Meeting of the\nAssociation for Computational Linguistics , pages\n8342–8360, Online. Association for Computational\nLinguistics.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-efﬁcient transfer learning for nlp.\nIn International Conference on Machine Learning ,\npages 2790–2799. PMLR.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Gra-\nham Neubig, Orhan Firat, and Melvin Johnson.\n2020. Xtreme: A massively multilingual multi-\ntask benchmark for evaluating cross-lingual gener-\nalisation. In International Conference on Machine\nLearning, pages 4411–4421. PMLR.\nPratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika\nBali, and Monojit Choudhury. 2020. The state and\nfate of linguistic diversity and inclusion in the NLP\nworld. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 6282–6293, Online. Association for Computa-\ntional Linguistics.\nArmand Joulin, Edouard Grave, Piotr Bojanowski, and\nTomas Mikolov. 2017. Bag of tricks for efﬁcient\ntext classiﬁcation. In Proceedings of the 15th Con-\nference of the European Chapter of the Association\nfor Computational Linguistics: Volume 2, Short Pa-\npers, pages 427–431, Valencia, Spain. Association\nfor Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nXinfan Meng, Furu Wei, Xiaohua Liu, Ming Zhou,\nGe Xu, and Houfeng Wang. 2012. Cross-lingual\nmixture model for sentiment classiﬁcation. In Pro-\nceedings of the 50th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 572–581. Association for Computa-\ntional Linguistics.\nSaif Mohammad, Felipe Bravo-Marquez, Mohammad\nSalameh, and Svetlana Kiritchenko. 2018. Semeval-\n2018 task 1: Affect in tweets. In Proceedings of the\n12th international workshop on semantic evaluation,\npages 1–17.\nSaif Mohammad, Svetlana Kiritchenko, Parinaz Sob-\nhani, Xiaodan Zhu, and Colin Cherry. 2016.\nSemeval-2016 task 6: Detecting stance in tweets. In\nProceedings of the 10th International Workshop on\nSemantic Evaluation (SemEval-2016), pages 31–41.\nDat Quoc Nguyen, Thanh Vu, and Anh Tuan Nguyen.\n2020. BERTweet: A pre-trained language model\nfor English tweets. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing: System Demonstrations, pages 9–\n14, Online. Association for Computational Linguis-\ntics.\nBraja Gopal Patra, Dipankar Das, Amitava Das, and\nRajendra Prasath. 2015. Shared task on senti-\nment analysis in indian languages (sail) tweets-an\noverview. In International Conference on Mining\nIntelligence and Knowledge Exploration, pages 650–\n655. Springer.\nParth Patwa, Gustavo Aguilar, Sudipta Kar, Suraj\nPandey, Srinivas PYKL, Bj ¨orn Gamb ¨ack, Tanmoy\nChakraborty, Thamar Solorio, and Amitava Das.\n2020. SemEval-2020 task 9: Overview of senti-\nment analysis of code-mixed tweets. In Proceed-\nings of the Fourteenth Workshop on Semantic Eval-\nuation, pages 774–790, Barcelona (online). Interna-\ntional Committee for Computational Linguistics.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R ¨uckl´e,\nKyunghyun Cho, and Iryna Gurevych. 2020.\nAdapterfusion: Non-destructive task composi-\ntion for transfer learning. arXiv preprint\narXiv:2005.00247.\nTelmo Pires, Eva Schlinger, and Dan Garrette. 2019.\nHow multilingual is multilingual bert? In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics, pages 4996–5001.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2020. Exploring the lim-\nits of transfer learning with a uniﬁed text-to-text\ntransformer. Journal of Machine Learning Research,\n21:1–67.\nMohammad Sadegh Rasooli, Noura Farra, Axinia\nRadeva, Tao Yu, and Kathleen McKeown. 2018.\nCross-lingual sentiment transfer with limited re-\nsources. Machine Translation, 32(1):143–165.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2017.\nSemeval-2017 task 4: Sentiment analysis in twitter.\nIn Proceedings of the 11th international workshop\non semantic evaluation (SemEval-2017), pages 502–\n518.\nSara Rosenthal, Noura Farra, and Preslav Nakov. 2019.\nSemeval-2017 task 4: Sentiment analysis in twitter.\narXiv preprint arXiv:1912.00741.\nPhillip Rust, Jonas Pfeiffer, Ivan Vuli ´c, Sebastian\nRuder, and Iryna Gurevych. 2020. How good is\nyour tokenizer? on the monolingual performance\nof multilingual language models. arXiv preprint\narXiv:2012.15613.\nMohammad Salameh, Saif Mohammad, and Svetlana\nKiritchenko. 2015. Sentiment after translation: A\ncase-study on arabic social media posts. In Pro-\nceedings of the 2015 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies , pages\n767–777, Denver, Colorado. Association for Compu-\ntational Linguistics.\nCynthia Van Hee, Els Lefever, and V ´eronique Hoste.\n2018. Semeval-2018 task 3: Irony detection in en-\nglish tweets. In Proceedings of The 12th Interna-\ntional Workshop on Semantic Evaluation, pages 39–\n50.\nDavid Vilares, Miguel A. Alonso, and Carlos G ´omez-\nRodr´ıguez. 2017. Supervised sentiment analysis in\nmultilingual environments. Information Processing\n& Management, 53(3):595 – 607.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A massively\nmultilingual pre-trained text-to-text transformer.\nMarcos Zampieri, Shervin Malmasi, Preslav Nakov,\nSara Rosenthal, Noura Farra, and Ritesh Kumar.\n2019. SemEval-2019 task 6: Identifying and catego-\nrizing offensive language in social media (OffensE-\nval). In Proceedings of the 13th International Work-\nshop on Semantic Evaluation , pages 75–86, Min-\nneapolis, Minnesota, USA. Association for Compu-\ntational Linguistics.\nXinjie Zhou, Xiaojun Wan, and Jianguo Xiao. 2016.\nCross-lingual sentiment classiﬁcation with bilingual\ndocument representation learning. In Proceedings\nof ACL, pages 1403–1412.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8409316539764404
    },
    {
      "name": "Language model",
      "score": 0.6078820824623108
    },
    {
      "name": "Natural language processing",
      "score": 0.5440248250961304
    },
    {
      "name": "Task (project management)",
      "score": 0.529524028301239
    },
    {
      "name": "Named-entity recognition",
      "score": 0.498948335647583
    },
    {
      "name": "Modular design",
      "score": 0.46322181820869446
    },
    {
      "name": "Baseline (sea)",
      "score": 0.45100677013397217
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4386323094367981
    },
    {
      "name": "Code (set theory)",
      "score": 0.4338580369949341
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.40976181626319885
    },
    {
      "name": "World Wide Web",
      "score": 0.3302515745162964
    },
    {
      "name": "Programming language",
      "score": 0.11742663383483887
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Oceanography",
      "score": 0.0
    }
  ]
}