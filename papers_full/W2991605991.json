{
    "title": "Generating Abstractive Summaries with Finetuned Language Models",
    "url": "https://openalex.org/W2991605991",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2580156231",
            "name": "Sebastian Gehrmann",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2000829827",
            "name": "Zachary Ziegler",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2294834069",
            "name": "Alexander Rush",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2963929190",
        "https://openalex.org/W2917166349",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W2949615363",
        "https://openalex.org/W2927746189",
        "https://openalex.org/W2133564696",
        "https://openalex.org/W4288562606",
        "https://openalex.org/W2760781482",
        "https://openalex.org/W2913946806",
        "https://openalex.org/W2962965405",
        "https://openalex.org/W2962985882",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2612675303",
        "https://openalex.org/W2964165364",
        "https://openalex.org/W2889518897",
        "https://openalex.org/W2962788840",
        "https://openalex.org/W2964237709",
        "https://openalex.org/W2899386490",
        "https://openalex.org/W4297804809",
        "https://openalex.org/W2794714381",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W1902237438",
        "https://openalex.org/W2507756961",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2969044820",
        "https://openalex.org/W2606974598",
        "https://openalex.org/W1544827683",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2964303773"
    ],
    "abstract": "Neural abstractive document summarization is commonly approached by models that exhibit a mostly extractive behavior. This behavior is facilitated by a copy-attention which allows models to copy words from a source document. While models in the mostly extractive news summarization domain benefit from this inductive bias, they commonly fail to paraphrase or compress information from the source document. Recent advances in transfer-learning from large pretrained language models give rise to alternative approaches that do not rely on copy-attention and instead learn to generate concise and abstractive summaries. In this paper, as part of the TL;DR challenge, we compare the abstractiveness of summaries from different summarization approaches and show that transfer-learning can be efficiently utilized without any changes to the model architecture. We demonstrate that the approach leads to a higher level of abstraction for a similar performance on the TL;DR challenge tasks, enabling true natural language compression.",
    "full_text": "Proceedings of The 12th International Conference on Natural Language Generation, pages 516–522,\nTokyo, Japan, 28 Oct - 1 Nov, 2019.c⃝2019 Association for Computational Linguistics\n516\nGenerating Abstractive Summaries with Finetuned Language Models\nSebastian Gehrmann*\nHarvard SEAS\nZachary M. Ziegler*\nHarvard SEAS\n{gehrmann@seas,zziegler@g,srush@seas}.harvard.edu\nAlexander M. Rush\nHarvard SEAS\nAbstract\nNeural abstractive document summarization\nis commonly approached by models that ex-\nhibit a mostly extractive behavior. This be-\nhavior is facilitated by a copy-attention which\nallows models to copy words from a source\ndocument. While models in the mostly ex-\ntractive news summarization domain beneﬁt\nfrom this inductive bias, they commonly fail to\nparaphrase or compress information from the\nsource document. Recent advances in transfer-\nlearning from large pretrained language mod-\nels give rise to alternative approaches that do\nnot rely on copy-attention and instead learn\nto generate concise and abstractive summaries.\nIn this paper, as part of the TL;DR challenge,\nwe compare the abstractiveness of summaries\nfrom different summarization approaches and\nshow that transfer-learning can be efﬁciently\nutilized without any changes to the model ar-\nchitecture. We demonstrate that the approach\nleads to a higher level of abstraction for a simi-\nlar performance on the TL;DR challenge tasks,\nenabling true natural language compression.\n1 Introduction\nAbstractive summarization, the challenge of gen-\nerating text that captures the content of a longer\ndocument, has been successfully approached by\nmany recent deep learning systems (e.g., Rush\net al., 2015; Nallapati et al., 2016). However,\nthe most common testbed for such methods, news\nsummarization, provides mostly extractive refer-\nence summaries which reuse long phrases from the\nsource document. This property gave rise to exten-\nsions of neural summarization models that extract\ntext from a source document in addition to gener-\nating new words (Vinyals et al., 2015; Gu et al.,\n2016). As a side-effect, many of the abstractive\nsummarization models have an inductive bias to\nalmost always extract text from the source docu-\nment verbatim instead of paraphrasing it.\nTo encourage research on models that can gen-\nerate summaries that are not extractive, V ¨olske\net al. (2017) developed the TL;DR corpus which\ncomprises over three million posts and associated\nuser-written summaries from reddit. Because of\nthe social media nature of the dataset, the user-\nwritten summaries copy long sequences from the\nsource much less frequently than common news\nsummarization corpora, resulting in a truly ab-\nstractive dataset. This dataset offers the oppor-\ntunity to investigate the performance of common\nsummarization models in an abstractive setting.\nIn this work, which is part of the TL;DR chal-\nlenge, we evaluate and analyze a number of com-\nmon summarization approaches on both the stan-\ndard news summarization corpus CNN-DM and\nthe TL;DR dataset. We investigate whether the\nability to copy from the source document leads to\nthe same learned extractive behavior, even when\nthe target summaries are mostly abstractive. We\nadditionally evaluate whether neural summariza-\ntion models can take advantage of pretrained lan-\nguage representation to generate more abstractive\ntext. To measure the abstractiveness of gener-\nated summaries, we identify a general “abstrac-\ntiveness” metric and compare the approaches to\nthe ground truth data for both datasets.\nOur results demonstrate that the ability to copy\nleads to improvements in terms of automated per-\nformance evaluation even on the TL;DR dataset,\neven though it leads to a signiﬁcantly lower level\nof abstractiveness. Furthermore, we ﬁnd that all\nmodels without pretraining exhibit a signiﬁcantly\nhigher level of extractiveness than the reference\nsummaries, while language model pretraining al-\nlows for more abstractive behavior. Overall, these\nresults suggest that standard summarization ap-\nproaches learn an easier extractive shortcut than\ntrue natural language compression, and that this\nphenomena occurs even in highly abstractive data.\n517\n2 Problem and Related Work\nThroughout this study, we consider the supervised\nsummarization problem, which aims to compress\na source document of tokens x1, . . . , xm of length\nm. The aligned summary y1, . . . , yn has a length\nn ≪m, and aims to convey a compressed version\nof the source document.\nSequence-to-sequence models (S2S, Sutskever\net al., 2014) are the de-facto standard for neural\nabstractive summarization (Rush et al., 2015; Nal-\nlapati et al., 2016). The development of models\nthat incorporate a copy-attention mechanism for\nmodels to copy word from source documents, has\nfurther improved the performance (Gu et al., 2016;\nVinyals et al., 2015; See et al., 2017).\nHowever, most summarization tasks use data\nfrom news domains which have mostly extractive\nsummaries. Among others, See et al. (2017) and\nGehrmann et al. (2018) found that models learn to\nreplicate this latent extraction behavior, and that\nthe resulting summaries of copy-attention based\nmodels are over 95% extractive. To address this\nissue, related approaches have used reinforcement\nlearning objectives to prevent the model from re-\nusing longer phrases from the input and to be more\nconcise (Paulus et al., 2017; Chen and Bansal,\n2018; Li et al., 2018). However, these methods\noften suffer from ungrammatical output or much\nslower training while also requiring task-speciﬁc\nloss functions. To avoid this problem, Kim et al.\n(2018) and V ¨olske et al. (2017) created reddit-\nbased corpora with more abstractive target sum-\nmaries that enable the evaluation of supervised\nmodels instead.\nSince the generation of abstractive summaries\nrequires a powerful representation of language,\nwe investigate the use of transfer learning. Large\nlanguage models based on the neural Trans-\nformer architecture (Vaswani et al., 2017) have\nshown promising results in language understand-\ning tasks (Houlsby et al., 2019; Devlin et al., 2018;\nChronopoulou et al., 2019), but so far have had\nlimited success in generation tasks (Zhang et al.,\n2019). Most recently, the pseudo-self attention\nmethod for ﬁne-tuning language models to gen-\neration tasks has been introduced which may al-\nlow the application of transfer-learning to abstrac-\ntive summarization (Ziegler et al., 2019). In this\nwork, we compare this approach to strong base-\nlines that rely on minor modiﬁcations of the Trans-\nformer (Gehrmann et al., 2018).\n3 Methods\n3.1 Models\nWe consider the following models for neural ab-\nstractive summarization. All models are sequence-\nto-sequence models with attention (Bahdanau\net al., 2014), but differ in architecture, use of a\ncopy mechanism, and language model pretraining.\nLSTM As a baseline we consider a bidirectional\nLSTM encoder and uni-directional LSTM decoder\nwith attention from Luong et al. (2015).\nLSTM+Copy We additionally consider the\nsame LSTM model equipped with the copy at-\ntention mechanism from See et al. (2017). At\neach time step the approach reuses the normal\nalignment distribution as a distribution over source\nwords to copy. This copy distribution is com-\nbined with the standard target vocabulary distri-\nbution from the decoder via a binary switchzt that\nis predicted at each time step t.\nTransformer(+Copy) For the transformer base-\nlines, we replace the LSTM architecture in the\nencoder and decoder with transformers (Vaswani\net al., 2017). As in the LSTM case we consider\nversion with and without the copy mechanism.\nSimilarly to Gehrmann et al. (2018), we randomly\nselect one of the attention heads as the source\nof the copy distribution and otherwise follow the\nsame procedure as for the LSTM+Copy.\nTransformer+Pretrain Pretrained language\nmodels lead to signiﬁcant performance im-\nprovements across a wide range of natural\nlanguage understanding tasks (Devlin et al.,\n2018). The recently introduced pseudo self\nattention method (Ziegler et al., 2019) has also\ndemonstrated strong performance across different\ngeneration tasks. The pseudo self attention model\nfollows the same architecture as the original\ntransformer, with minor modiﬁcations to inject\nthe context information into the decoder while\nkeeping the structure of the decoder similar to\nthat of an unconditional language model. Most\nimportantly, on the decoder side the context-\nattention block is removed and the self-attention\nblock is modiﬁed to use the source information\nvia pseudo self attention. The normal transformer\nself-attention computation from Vaswani et al.\n(2017) can be written most generally as\n518\nSA(Y ) =softmax\n(\n(Y Wq)(Y Wk)⊤\n)\n(Y Wv)\nwhere Y ∈T ×D is the input and Wk, Wv, Wq ∈\nD×D′ are parameters. In comparison, the pseudo\nself attention computation is\nPSA(X, Y) =\nsoftmax\n(\n(Y Wq)\n[XUk\nY Wk\n]⊤)[XUv\nY Wv\n]\nwhere X ∈ S ×D is the output of the trans-\nformer encoder representing the source document\nand Uk, Uv ∈D ×D′ are additional parameters.\nAs in Ziegler et al. (2019), We use the “small”\nGPT-2 (Radford et al., 2019) as a pretrained unidi-\nrectional transformer-based language model. All\nparameters of the decoder, including the input em-\nbeddings, self-attention weights Wk, Wv, Wq for\neach head and layer, feed forward weights, and\nlayer normalization weights are initialized with\nthe weights from the pretrained language model.\nThe rest of the weights, including those that make\nup the encoder and the context projections Uk, Uv\nfor each head and layer are randomly initialized.\nThe model is then trained end-to-end on the su-\npervised dataset without ﬁxing any parameters.\nCompared to a fully randomly initialized model,\nthe pretrained model has a strong inductive bias\ntowards abstractive generation. Whereas the de-\ncoder in a randomly initialized model can learn\na generative procedure that largely extracts se-\nquences from the source, the pseudo self attention\ndecoder is initialized with a decoder that already\ngenerates coherent language. It may thus be easier\nfor the model to learn to use the source as “inspira-\ntion” for the generated text, rather than to learn an\nentirely different extractive generative procedure.\nOur experiments aim to quantify this intuition.\n3.2 Metric\n% novel n-grams One metric used in the litera-\nture as a proxy for abstractiveness is the percent of\nn-grams in the summary that are not found in the\nsource document (See et al., 2017). We report this\nmetric for comparison to previous work.\nn-gram abstractiveness While % novel n-\ngrams approximately captures the correct trend, it\nis poorly normalized: consider a source document\nThe dog runs around. A cat jumps up. The brown\nhorse stands and the corresponding summary The\ndog runs around. The brown horse stands . The\n4-gram novelty score would identify 4-grams such\nas around. The brown horseas novel, yielding a 4-\ngram novelty score of 60% even though the sum-\nmary is composed entirely of copied 4-grams (i.e.\na true novelty score should measure 0%). To rem-\nedy this, we propose an alternate metric denoted\n“n-gram abstractiveness”:\nn-gram abstractiveness =\n1 −# summary words part of n-gram copied\ntotal # summary words\nTo calculate this, we ﬁrst generate the set of n-\ngrams in the source and summary. All words in the\nsummary which are part of n-grams in the inter-\nsection of the two sets are counted as “# summary\nwords part of n-gram copied”. Since this (normal-\nized) quantity gives an indication of the fraction\nof the summary that is copied in n-grams from the\nsource, 1 minus this quantity gives an indication\nfor the abstractiveness of the summary at the n-\ngram level.\n4 Experiments\nWe compare the presented methods on the non-\nanonymized CNN-DM dataset (Hermann et al.,\n2015) and the TL;DR challenge dataset (V ¨olske\net al., 2017). CNN-DM comprises roughly\n290,000 training examples, which are pruned at\na maximum length of 400 words. The corpus is\nhighly extractive, as only 14.0% of tokens in the\noutput do not appear in the corresponding input.\nEven when we ignore all stopwords, only 17.7%\nof tokens are novel.\nThe TL;DR challenge dataset is composed of\nover three million examples, mined from com-\nments across reddit. We apply the same 400\nword pruning to the dataset.The corpus exhibits a\nmuch more abstractive behavior, as 53.6% of to-\nkens in the target are novel. After excluding stop-\nwords, this number increases to over 71.4%. That\nmeans that this dataset requires a much better text-\ngenerating model than CNN-DM.\nFirst baseline models trained on the TL;DR data\nexhibited a problem that is commonly seen in con-\nversational models in that it defaults to the most\nsimple answer. The simplest answers were a com-\nbination of This is not a problem ; edit: thank\n519\nCNN-DM TL;DR\nModel R1 R2 RL R1 R2 RL\nLSTM 30.8 11.8 28.5 16 4 13\nLSTM+Copy 39.0 16.8 35.7 20 5 15\nTransformer 39.9 17.8 36.6 21 6 16\nTransformer+Copy 39.9 17.7 37.1 22 6 17\nTransformer+Pretrain 30.5 7.2 28.0 22 5 17\nTable 1: The ROUGE results on the CNN-DM test set and the blind TL;DR test set.\nyou for the gold ; and a number of insults. We\nthus ﬁltered the dataset by excluding examples in\nwhich the target included the following phrases\nin any capitalization and including common mis-\nspellings: I don’t know ; edit:; good idea ; what\nI am talking about ; worth it ; upvote; downvote;\nyou’ll be ﬁne; source:; and ten different profani-\nties. We further excluded all examples in which\nthe target was shorter than 25 characters to bias\nthe model towards longer generated texts. In total,\nthis procedure excluded 516,000 examples.\nConsistent with previous work (Paulus et al.,\n2017; See et al., 2017; Gehrmann et al., 2018), we\nﬁnd that the LSTM baselines are strongly biased\ntowards short and repetitive summaries. To avoid\nthis, we apply the inference-time loss functions\nsuggested by Gehrmann et al. (2018); a coverage\npenalty, a length penalty, and a mechanism that\nprevents repetition of trigrams. We additionally\nset the minimum length for TL;DR to 25 tokens,\nwhich we found to works best on the validation\nset1 It is not necessary to apply the same mecha-\nnisms to the Transformer-based models. For a bet-\nter comparison, we only set the minimum length\nof TL;DR to 25.\n5 Automated Evaluation\nTable 1 presents the ROUGE scores on the test\nset for each model on the two datasets 2. For the\nLSTM, adding the copy mechanism signiﬁcantly\nimproves the performance on both the CNN-DM\nand TL;DR datasets across R1, R2, and RL. De-\nspite the added inference-time loss functions, the\nLSTM models consistently perform worse than\n1We note that an increased length of generated summaries\nhas been found to increase ROUGE scores which make com-\nparison to other systems with different length outputs chal-\nlenging (Sun et al., 2019).\n2The TIRA system (Potthast et al., 2019) used for evalu-\nating the TL;DR task presents scores only with the presented\nprecision.\nthe Transformer models. For the Transformer\nmodel, adding the copy mechanism yields a nearly\nidentical performance on CNN-DM and slightly\nimproved performance on TL;DR. Thus, even\nthough the TL;DR dataset is inherently abstrac-\ntive, copy-attention still improves or is at least no\nworse in terms of empirical performance.\nUsing the pretrained representations in the form\nof pseudo self attention without copy-attention\nhurts performance considerably on CNN-DM, but\nslightly improves performance on TL;DR. We hy-\npothesize that this effect can be explained by the\nabstractiveness of the dataset. Since CNN-DM is\nmostly extractive, it beneﬁts from the extractive\napproaches. At the same time, the inductive copy-\ning bias has only a minor positive effect on the\nROUGE score of the abstractive TL;DR dataset\nand, thus, a more ﬂuent abstractive summary leads\nto better performance. Note that here, for the sake\nof simplicity, we do not consider the Transformer\nwith pseudo self attention and a copy mechanism\nwhich was reported to give strong performance in\nZiegler et al. (2019).\n6 Analysis\nTo validate our hypothesis that pretraining leads\nto higher abstractiveness, we evaluate the two ab-\nstractiveness metrics described in Section 3.2. The\nresults are presented in Figure 1 for all models\nand the ground truth data on both CNN-DM and\nTL;DR.\nMetric comparison Comparing the overall re-\nsults for all datasets and models between the pro-\nposed n-gram abstractiveness metric and the %\nnovel n-gram metric we ﬁnd that both metrics\npresent identical trends. The major difference is\nthat the n-gram abstractiveness accounts for the in-\ncrease in % novel n-grams as n increases, which\nreduces the noise and leads to a more interpretable\n520\n1 2 3 4 5 6 7 8 9 10\nn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0n-gram abstractiveness\n1 2 3 4 5 6 7 8 9 10\nn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0n-gram abstractiveness\n1 2 3 4 5 6 7 8 9 10\nn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0% novel n-grams\n(a) CNN-DM\n1 2 3 4 5 6 7 8 9 10\nn\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0% novel n-grams\n (b) TL;DR\nFigure 1: The n-gram abstractiveness and % novel n-gram metrics for increasing n show the gap between standard\nabstractive approaches and the human references. For both corpora, our approach closes this gap.\nresult. The rest of the analysis will thus focus on\nthe n-gram abstractiveness.\nDataset comparison Comparing the abstrac-\ntiveness metrics for the reference data between\nthe two datasets provides further evidence that\nTL;DR is a more abstractive dataset than CNN-\nDM. While the 4-gram abstractiveness of CNN-\nDM is only 63%, for example, the 4-gram abstrac-\ntiveness of TL;DR is 94%. Still, at the higher n-\ngram levels CNN-DM becomes more than 80%\nabstractive, suggesting that less than 20% of to-\nkens are part of very long sequences that were\ncopied verbatim.\nRandomly-initialized models All randomly\ninitialized models show considerably more ex-\ntractive behavior than the reference data, for all\nvalues of n. This trend exists even for the variants\nwithout an explicit copy mechanism and is found\nin both datasets. This pattern suggests that models\ntrained from scratch may exploit an extractive\nshortcut which is easier to learn than abstractive\ndata compression.\nThe addition of a copy mechanism decreases\nthe abstractiveness for all pairs studied expect\nfor the Transformer on CNN-DM. This general\ntrend aligns with the intuition that an explicit copy\nmechanism allows the model to exploit this easier-\nto-learn extractive behavior.\nPretraining Compared to the other models, the\npseudo self attention pretraining approach leads to\na much higher level of abstractiveness. This pro-\nvides evidence that unlike the randomly initialized\nmodels which learn an extractive shortcut, the pre-\ntrained model has a strong inductive bias toward\nabstractive behavior. It is unclear whether this\nis an artifact of the speciﬁc pseudo self attention\nmethod or a more general consequence of pretrain-\ning for conditional generation.\n7 Conclusion\nIn this paper we study the summarization perfor-\nmance and abstractiveness of summarization mod-\nels with and without copy attention and pretrain-\ning. Combining these two sets of evidence, we\nﬁnd that often the models which perform better\nare less abstractive, even when the dataset itself\nis highly abstractive. It is thus challenging to at-\ntribute value to abstractiveness when a model is\nevaluated purely based on its ROUGE score. Our\nresults suggest that if the goal is solely summariza-\ntion performance, perhaps more extractive models\nare well suited for this task. Importantly, how-\never, our study emphasizes that despite the perfor-\nmance, we should not be fooled into believing that\nstate-of-the-art summarization models are learn-\ning true semantic natural language compression.\n521\nAcknowledgements\nSG is supported by a Siebel Scholarship. AMR\nand ZMZ are support by NSF 1845664 and Intel\nresearch.\nReferences\nDzmitry Bahdanau, Kyunghyun Cho, and Yoshua\nBengio. 2014. Neural machine translation by\njointly learning to align and translate. CoRR,\nabs/1409.0473.\nYen-Chun Chen and Mohit Bansal. 2018. Fast abstrac-\ntive summarization with reinforce-selected sentence\nrewriting. arXiv preprint arXiv:1805.11080.\nAlexandra Chronopoulou, Christos Baziotis, and\nAlexandros Potamianos. 2019. An Embarrassingly\nSimple Approach for Transfer Learning from Pre-\ntrained Language Models.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\nDeep Bidirectional Transformers for Language Un-\nderstanding.\nSebastian Gehrmann, Yuntian Deng, and Alexander\nRush. 2018. Bottom-up abstractive summarization.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n4098–4109.\nJiatao Gu, Zhengdong Lu, Hang Li, and Victor OK\nLi. 2016. Incorporating copying mechanism in\nsequence-to-sequence learning. arXiv preprint\narXiv:1603.06393.\nKarl Moritz Hermann, Tomas Kocisky, Edward\nGrefenstette, Lasse Espeholt, Will Kay, Mustafa Su-\nleyman, and Phil Blunsom. 2015. Teaching ma-\nchines to read and comprehend. InAdvances in Neu-\nral Information Processing Systems , pages 1693–\n1701.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin de Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly.\n2019. Parameter-Efﬁcient Transfer Learning for\nNLP.\nByeongchang Kim, Hyunwoo Kim, and Gunhee Kim.\n2018. Abstractive summarization of reddit posts\nwith multi-level memory networks. arXiv preprint\narXiv:1811.00783.\nPiji Li, Lidong Bing, and Wai Lam. 2018. Actor-\ncritic based training framework for abstractive sum-\nmarization. arXiv preprint arXiv:1803.11070.\nThang Luong, Hieu Pham, and Christopher D Man-\nning. 2015. Effective approaches to attention-based\nneural machine translation. In Proceedings of the\n2015 Conference on Empirical Methods in Natural\nLanguage Processing, pages 1412–1421.\nRamesh Nallapati, Bowen Zhou, Caglar Gulcehre,\nBing Xiang, et al. 2016. Abstractive text summa-\nrization using sequence-to-sequence rnns and be-\nyond. arXiv preprint arXiv:1602.06023.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization. arXiv preprint arXiv:1705.04304.\nMartin Potthast, Tim Gollub, Matti Wiegmann, and\nBenno Stein. 2019. TIRA Integrated Research Ar-\nchitecture. In Nicola Ferro and Carol Peters, edi-\ntors, Information Retrieval Evaluation in a Chang-\ning World - Lessons Learned from 20 Years of CLEF.\nSpringer.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nModels are Unsupervised Multitask Learners.\nAlexander M Rush, Sumit Chopra, and Jason We-\nston. 2015. A neural attention model for ab-\nstractive sentence summarization. arXiv preprint\narXiv:1509.00685.\nAbigail See, Peter J Liu, and Christopher D Man-\nning. 2017. Get to the point: Summarization\nwith pointer-generator networks. arXiv preprint\narXiv:1704.04368.\nSimeng Sun, Ori Shapira, Ido Dagan, and Ani\nNenkova. 2019. How to compare summarizers\nwithout target length? pitfalls, solutions and re-\nexamination of the neural summarization literature.\nIn Proceedings of the Workshop on Methods for Op-\ntimizing and Evaluating Neural Language Genera-\ntion, pages 21–29.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural net-\nworks. In Advances in neural information process-\ning systems, pages 3104–3112.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems, pages 6000–6010.\nOriol Vinyals, Meire Fortunato, and Navdeep Jaitly.\n2015. Pointer networks. In Advances in Neural In-\nformation Processing Systems, pages 2692–2700.\nMichael V ¨olske, Martin Potthast, Shahbaz Syed, and\nBenno Stein. 2017. TL;DR: Mining Reddit to Learn\nAutomatic Summarization. In EMNLP 2017 Work-\nshop on New Frontiers in Summarization, pages 59–\n63. Association for Computational Linguistics.\nHaoyu Zhang, Yeyun Gong, Yu Yan, Nan Duan, Jian-\njun Xu, Ji Wang, Ming Gong, and Ming Zhou. 2019.\nPretraining-Based Natural Language Generation for\nText Summarization.\n522\nZachary M Ziegler, Luke Melas-Kyriazi, Sebastian\nGehrmann, and Alexander M Rush. 2019. Encoder-\nagnostic adaptation for conditional language gener-\nation. arXiv preprint arXiv:1908.06938."
}