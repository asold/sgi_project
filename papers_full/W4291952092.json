{
  "title": "Hardware-friendly compression and hardware acceleration for transformer: A survey",
  "url": "https://openalex.org/W4291952092",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5103450264",
      "name": "Shizhen Huang",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5056576567",
      "name": "Enhao Tang",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5100442626",
      "name": "Shun Li",
      "affiliations": [
        "Fuzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A5053772930",
      "name": "Xiangzhan Ping",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A5100761386",
      "name": "Ruiqi Chen",
      "affiliations": [
        "Chongqing University of Posts and Telecommunications",
        "Fudan University",
        "Fuzhou University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4283329184",
    "https://openalex.org/W6702248584",
    "https://openalex.org/W6600376255",
    "https://openalex.org/W6601037301",
    "https://openalex.org/W2783538964",
    "https://openalex.org/W3170825342",
    "https://openalex.org/W2860338957",
    "https://openalex.org/W3033108890",
    "https://openalex.org/W3023255099",
    "https://openalex.org/W2943389092",
    "https://openalex.org/W4280515676",
    "https://openalex.org/W4285261368",
    "https://openalex.org/W2964121960",
    "https://openalex.org/W3194017222",
    "https://openalex.org/W6600339457",
    "https://openalex.org/W6609822380",
    "https://openalex.org/W4394668313",
    "https://openalex.org/W6602036895",
    "https://openalex.org/W3169769781",
    "https://openalex.org/W3104263050",
    "https://openalex.org/W6600351811",
    "https://openalex.org/W3177265267",
    "https://openalex.org/W6604662147",
    "https://openalex.org/W6630758607",
    "https://openalex.org/W3204801262",
    "https://openalex.org/W3007007518",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W6604896550",
    "https://openalex.org/W3047848469",
    "https://openalex.org/W3017024317",
    "https://openalex.org/W3189877953",
    "https://openalex.org/W3199934250",
    "https://openalex.org/W2413794162",
    "https://openalex.org/W2012833704",
    "https://openalex.org/W2197984537",
    "https://openalex.org/W2162064258",
    "https://openalex.org/W2910396952",
    "https://openalex.org/W2914968962",
    "https://openalex.org/W3012561096",
    "https://openalex.org/W3104393472",
    "https://openalex.org/W2962820060",
    "https://openalex.org/W2963479620",
    "https://openalex.org/W954001337",
    "https://openalex.org/W3184454880",
    "https://openalex.org/W3104151879",
    "https://openalex.org/W2300242332",
    "https://openalex.org/W6600336938",
    "https://openalex.org/W2963000224",
    "https://openalex.org/W2998342322",
    "https://openalex.org/W2915106038",
    "https://openalex.org/W3162542754",
    "https://openalex.org/W2963396654",
    "https://openalex.org/W3206837665",
    "https://openalex.org/W3176468986",
    "https://openalex.org/W4394650345",
    "https://openalex.org/W2963367920",
    "https://openalex.org/W1575701986",
    "https://openalex.org/W2009654791",
    "https://openalex.org/W2091843288",
    "https://openalex.org/W2911884654",
    "https://openalex.org/W2917450576"
  ],
  "abstract": "&lt;abstract&gt; &lt;p&gt;The transformer model has recently been a milestone in artificial intelligence. The algorithm has enhanced the performance of tasks such as Machine Translation and Computer Vision to a level previously unattainable. However, the transformer model has a strong performance but also requires a high amount of memory overhead and enormous computing power. This significantly hinders the deployment of an energy-efficient transformer system. Due to the high parallelism, low latency, and low power consumption of field-programmable gate arrays (FPGAs) and application specific integrated circuits (ASICs), they demonstrate higher energy efficiency than Graphics Processing Units (GPUs) and Central Processing Units (CPUs). Therefore, FPGA and ASIC are widely used to accelerate deep learning algorithms. Several papers have addressed the issue of deploying the Transformer on dedicated hardware for acceleration, but there is a lack of comprehensive studies in this area. Therefore, we summarize the transformer model compression algorithm based on the hardware accelerator and its implementation to provide a comprehensive overview of this research domain. This paper first introduces the transformer model framework and computation process. Secondly, a discussion of hardware-friendly compression algorithms based on self-attention and Transformer is provided, along with a review of a state-of-the-art hardware accelerator framework. Finally, we considered some promising topics in transformer hardware acceleration, such as a high-level design framework and selecting the optimum device using reinforcement learning.&lt;/p&gt; &lt;/abstract&gt;",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6569185853004456
    },
    {
      "name": "Field-programmable gate array",
      "score": 0.5796555876731873
    },
    {
      "name": "Efficient energy use",
      "score": 0.5549764633178711
    },
    {
      "name": "Transformer",
      "score": 0.5168519616127014
    },
    {
      "name": "Hardware acceleration",
      "score": 0.5158833265304565
    },
    {
      "name": "Computer hardware",
      "score": 0.4660310447216034
    },
    {
      "name": "Embedded system",
      "score": 0.4492546021938324
    },
    {
      "name": "Computer architecture",
      "score": 0.4014914035797119
    },
    {
      "name": "Engineering",
      "score": 0.1507279872894287
    },
    {
      "name": "Electrical engineering",
      "score": 0.14778274297714233
    },
    {
      "name": "Voltage",
      "score": 0.10778290033340454
    }
  ],
  "cited_by": 6
}