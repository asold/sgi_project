{
    "title": "Towards Completeness-Oriented Tool Retrieval for Large Language Models",
    "url": "https://openalex.org/W4399115306",
    "year": 2024,
    "authors": [
        {
            "id": null,
            "name": "Qu, Changle",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A4222167754",
            "name": "Dai, Sunhao",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A2743419228",
            "name": "Wei Xiaochi",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A3009499033",
            "name": "Cai Hengyi",
            "affiliations": [
                "Institute of Computing Technology",
                "Chinese Academy of Sciences"
            ]
        },
        {
            "id": "https://openalex.org/A2362723648",
            "name": "Wang Shuaiqiang",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2109288988",
            "name": "Yin Dawei",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A1963771243",
            "name": "Xu Jun",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A4221399670",
            "name": "Wen, Ji-Rong",
            "affiliations": [
                "Renmin University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6810081322",
        "https://openalex.org/W4368755500",
        "https://openalex.org/W3137305332",
        "https://openalex.org/W3045200674",
        "https://openalex.org/W3154670582",
        "https://openalex.org/W4392846385",
        "https://openalex.org/W2069870183",
        "https://openalex.org/W4389518608",
        "https://openalex.org/W4389519254",
        "https://openalex.org/W3156470785",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W4389519109",
        "https://openalex.org/W2144211451",
        "https://openalex.org/W4400703289"
    ],
    "abstract": "Recently, integrating external tools with Large Language Models (LLMs) has gained significant attention as an effective strategy to mitigate the limitations inherent in their pre-training data. However, real-world systems often incorporate a wide array of tools, making it impractical to input all tools into LLMs due to length limitations and latency constraints. Therefore, to fully exploit the potential of tool-augmented LLMs, it is crucial to develop an effective tool retrieval system. Existing tool retrieval methods primarily focus on semantic matching between user queries and tool descriptions, frequently leading to the retrieval of redundant, similar tools. Consequently, these methods fail to provide a complete set of diverse tools necessary for addressing the multifaceted problems encountered by LLMs. In this paper, we propose a novel modelagnostic COllaborative Learning-based Tool Retrieval approach, COLT, which captures not only the semantic similarities between user queries and tool descriptions but also takes into account the collaborative information of tools. Specifically, we first fine-tune the PLM-based retrieval models to capture the semantic relationships between queries and tools in the semantic learning stage. Subsequently, we construct three bipartite graphs among queries, scenes, and tools and introduce a dual-view graph collaborative learning framework to capture the intricate collaborative relationships among tools during the collaborative learning stage. Extensive experiments on both the open benchmark and the newly introduced ToolLens dataset show that COLT achieves superior performance. Notably, the performance of BERT-mini (11M) with our proposed model framework outperforms BERT-large (340M), which has 30 times more parameters. Furthermore, we will release ToolLens publicly to facilitate future research on tool retrieval.",
    "full_text": "Towards Completeness-Oriented Tool Retrieval for\nLarge Language Models\nChangle Qu\nSunhao Dai\nGaoling School of Artificial Intelligence\nRenmin University of China\nBeijing, China\n{changlequ,sunhaodai}@ruc.edu.cn\nXiaochi Wei\nBaidu Inc.\nBeijing, China\nweixiaochi@baidu.com\nHengyi Cai\nInstitute of Computing Technology\nChinese Academy of Sciences\nBeijing, China\ncaihengyi@ict.ac.cn\nShuaiqiang Wang\nBaidu Inc.\nBeijing, China\nshqiang.wang@gmail.com\nDawei Yin\nBaidu Inc.\nBeijing, China\nyindawei@acm.org\nJun Xuâˆ—\nJi-Rong Wen\nGaoling School of Artificial Intelligence\nRenmin University of China\nBeijing, China\n{junxu,jrwen}@ruc.edu.cn\nABSTRACT\nRecently, integrating external tools with Large Language Models\n(LLMs) has gained significant attention as an effective strategy to\nmitigate the limitations inherent in their pre-training data. How-\never, real-world systems often incorporate a wide array of tools,\nmaking it impractical to input all tools into LLMs due to length\nlimitations and latency constraints. Therefore, to fully exploit the\npotential of tool-augmented LLMs, it is crucial to develop an effec-\ntive tool retrieval system. Existing tool retrieval methods primarily\nfocus on semantic matching between user queries and tool de-\nscriptions, frequently leading to the retrieval of redundant, similar\ntools. Consequently, these methods fail to provide a complete set\nof diverse tools necessary for addressing the multifaceted problems\nencountered by LLMs. In this paper, we propose a novel model-\nagnostic COllaborative Learning-based Tool Retrieval approach,\nCOLT, which captures not only the semantic similarities between\nuser queries and tool descriptions but also takes into account the\ncollaborative information of tools. Specifically, we first fine-tune\nthe PLM-based retrieval models to capture the semantic relation-\nships between queries and tools in the semantic learning stage.\nSubsequently, we construct three bipartite graphs among queries,\nscenes, and tools and introduce a dual-view graph collaborative\nlearning framework to capture the intricate collaborative relation-\nships among tools during the collaborative learning stage. Extensive\nexperiments on both the open benchmark and the newly introduced\nToolLens dataset show that COLT achieves superior performance.\nâˆ—Jun Xu is the corresponding author. Work partially done at Engineering Research Cen-\nter of Next-Generation Intelligent Search and Recommendation, Ministry of Education.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than the\nauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or\nrepublish, to post on servers or to redistribute to lists, requires prior specific permission\nand/or a fee. Request permissions from permissions@acm.org.\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA.\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nACM ISBN 979-8-4007-0436-9/24/10\nhttps://doi.org/10.1145/3627673.3679847\nNotably, the performance of BERT-mini (11M) with our proposed\nmodel framework outperforms BERT-large (340M), which has 30\ntimes more parameters. Furthermore, we will release ToolLens pub-\nlicly to facilitate future research on tool retrieval.\nCCS CONCEPTS\nâ€¢ Information systems â†’Information retrieval.\nKEYWORDS\nTool Retrieval, Retrieval Completeness, Large Language Model\nACM Reference Format:\nChangle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei\nYin, Jun Xu, and Ji-Rong Wen. 2024. Towards Completeness-Oriented Tool\nRetrieval for Large Language Models. In Proceedings of the 33rd ACM In-\nternational Conference on Information and Knowledge Management (CIKM\nâ€™24), October 21â€“25, 2024, Boise, ID, USA. ACM, New York, NY, USA, 11 pages.\nhttps://doi.org/10.1145/3627673.3679847\n1 INTRODUCTION\nRecently, large language models (LLMs) have demonstrated re-\nmarkable progress across various natural language processing\ntasks [1, 2, 4, 41]. However, they often struggle with solving highly\ncomplex problems and providing up-to-date knowledge due to the\nconstraints of their pre-training data [23, 42]. A promising approach\nto overcome these limitations is tool learning [ 18, 26, 29, 31, 48],\nwhich enables LLMs to dynamically interact with external tools,\nsignificantly facilitating access to real-time data and the execu-\ntion of complex computations. By integrating tool learning, LLMs\ntranscend the confines of their outdated or limited pre-trained\nknowledge [2], offering responses to user queries with significantly\nimproved accuracy and relevance [ 14, 28]. However, real-world\nsystems typically involve a large number of tools, making it im-\npractical to take the descriptions of all tools as input for LLMs due\nto length limitations and latency constraints. Thus, as illustrated in\nFigure 1(a), developing an effective tool retrieval system becomes\nessential to fully exploit the potential of tool-augmented LLMs [8].\narXiv:2405.16089v2  [cs.CL]  28 Jul 2024\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA. Changle Qu et al.\nQuery\nTool Set Tool Retriever\nRetrieved Tools\nResponse LLMs\n(a) Pipeline of user interaction with tool-augmented LLMs.\nI would like to know the value of 5 ounces of gold \nplus 1 million AMZN stocks in CNY .\nWith Complete Tools: ... 5 ounces of gold is\n69,495.49 CNY and 1 million AMZN stocks is\n941,200,000 CNY...\nOne Is Incorrect: ... I donâ€™t have real-time data or\ninternet access to look up current stock prices or\ngold prices...\nOne Is Missing: ... 5 ounces of gold is 69,495.49\nCNY, As for the value of 1 million AMZN stocks, ..., I\ncannot provide the value in CNY...\nTwo Are Incorrect: ... Sorry, I am an AI model and I\ndonâ€™t have real-time data or the ability to provide\ncurrent stock prices of gold...\nTwo Are Missing: ... As an AI, I donâ€™t have real-\ntime data or future predictions...\nWithout Tools: ... Sorry, but as an AI, Iâ€™m not able\nto provide real-time data or future predictions for\nstock prices or the value of gold...\nQuery\nStock\nExchange\nGold\n(b) Illustration of different responses with different tools.\nFigure 1: An illustration of tool retrieval for LLMs with tool\nlearning and varied responses using different tools.\nTypically, existing tool retrieval approaches directly employ\ndense retrieval techniques [ 8, 28, 49], solely focusing on match-\ning semantic similarities between queries and tool descriptions.\nHowever, these approaches may fall short when addressing mul-\ntifaceted queries that require a collaborative effort from multiple\ntools to formulate a comprehensive response. For instance, in Fig-\nure 1(b), consider a userâ€™s request to calculate the value of 5 ounces\nof gold plus 1 million AMZN stocks in CNY. Such a query requires\nthe simultaneous use of tools for gold prices, stock values, and\ncurrency exchange rates. The absence of any of these tools yields\nan incomplete answer. In this example, dense retrieval methods that\nrely solely on semantic matching may retrieve multiple tools related\nto stock prices while neglecting others. This highlights a significant\nlimitation of dense retrieval methods that overlook the necessity for\ntools to interact collaboratively. Thus, ensuring the completeness\nof retrieved tools is an essential aspect of a tool retrieval system,\nwhich is often neglected by traditional retrieval approaches.\nToward this end, this paper proposes COLT, a novel model-\nagnostic COllaborative Learning-based Tool retrieval approach\naimed at enhancing completeness-oriented tool retrieval. This\nmethod is structured into two main stages: semantic learning and\ncollaborative learning. Initially, we fine-tune traditional pre-trained\nlanguage models (PLMs) on tool retrieval datasets to acquire se-\nmantic matching information between queries and tools, thereby\naddressing the potential performance issues of these models in\nzero-shot scenarios for tool retrieval tasks. Subsequently, to capture\nthe intricate collaborative relationship among tools, a concept of\nâ€œsceneâ€ is proposed to indicate a group of collaborative tools. Based\non this, COLT integrates three bipartite graphs among queries,\nscenes, and tools. More specifically, given the initial semantic em-\nbedding from the semantic learning stage, the high-order collabora-\ntive relationship is better integrated via message propagation and\ncross-view graph contrastive learning among these graphs. The\nlearning objective incorporates a list-wise multi-label loss to ensure\nthe simultaneous acquisition of tools from the entire ground-truth\nset without favoring any specific tool.\nMoreover, traditional retrieval metrics like Recall [ 52] and\nNDCG [16] fail to capture the completeness necessary for effective\ntool retrieval. As illustrated in Figure 1(b), the exclusion of any es-\nsential tool from the ground-truth tool set compromises the ability\nto fully address user queries, indicating that metrics focused solely\non individual tool ranking performance are inadequate when multi-\nple tools are required. To bridge this gap, we introduce COMP@ğ¾,\na new metric designed to assess tool retrieval performance based\non completeness, which can serve as a reliable indicator of how\nwell a tool retrieval system performs for downstream tool learning\napplications. Additionally, we construct a new dataset called Tool-\nLens, in which a query is typically solved with multiple relevant\nbut diverse tools, reflecting the multifaceted nature of user requests\nin real-world scenarios.\nIn summary, our main contributions are as follows:\nâ€¢ The collaborative relationships among multiple tools in LLMs\nhave been thoroughly studied, which reveals that incomplete tool\nretrieval hinders accurate answers, underscoring the integral role\neach tool plays in the collective functionality.\nâ€¢ We introduce COLT, a novel tool retrieval approach that uses\nmessage propagation and cross-view graph contrastive learning\namong queries, scenes, and tools, incorporating better collaborative\ninformation among various tools.\nâ€¢ Extensive experiments demonstrate the superior performance\nof COLT against state-of-the-art dense retrieval methods in both\ntool retrieval and downstream tool learning.\nâ€¢ We introduce a new dataset and a novel evaluation metric\nspecifically designed for assessing multi-tool usage in LLMs, which\nwill facilitate future research on tool retrieval.\n2 RELATED WORK\nTool Learning. Recent studies highlight the potential of LLMs\nto utilize tools in addressing complex problems [24, 27]. Existing\ntool learning approaches can be categorized into two types: tuning-\nfree and tuning-based methods [8]. Tuning-free methods capital-\nize on the in context learning ability of LLMs through strategic\nprompting [32, 33, 43, 47]. For example, ART [25] constructs a task\nlibrary, from which it retrieves demonstration examples as few-shot\nprompts when encountering real-world tasks. Conversely, tuning-\nbased methods involve directly fine-tuning the parameters of LLMs\non specific tool datasets to master tool usage. For example, ToolL-\nLaMA [28] employs the instruction-solution pairs derived from the\nDFSDT method to fine-tune the LLaMA model, thereby significantly\nenhancing its tool usage capabilities. Despite these advancements,\nmost strategies either provide a manual tool set [31, 38, 46] or em-\nploy simple dense retrieval [8] for tool retrieval. However, LLMs\nmust choose several useful tools from a vast array of tools in real-\nworld applications, necessitating a robust tool retriever to address\nthe length limitations and latency constraints of LLMs.\nTowards Completeness-Oriented Tool Retrieval for Large Language Models CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA.\nTool Retrieval. Tool retrieval aims to find top- ğ¾ most suitable\ntools for a given query from a vast set of tools. Existing tool re-\ntrieval methods typically directly adopt traditional retrieval ap-\nproaches, and state-of-the-art retrieval methods can be categorized\ninto two types: term-based and semantic-based. Term-based meth-\nods, such as TF-IDF [35] and BM25 [30], prioritize term matching via\nsparse representations. Conversely, semantic-based methods, such\nas ANCE [45], TAS-B [12], coCondensor [7], and Contriever [15],\nutilize neural networks to learn the semantic relationship between\nqueries and tool descriptions and then calculate the semantic sim-\nilarity using methods such as cosine similarity. Despite these ad-\nvancements, existing methods for tool retrieval overlook the im-\nportance of the collaborative relationship among multiple tools,\nthereby falling short of meeting the completeness criterion for tool\nretrieval. Our work seeks to mitigate these issues by collabora-\ntive learning that leverages graph neural networks and cross-view\ncontrastive learning among graphs.\n3 OUR APPROACH: COLT\nIn this section, we first introduce task formulation of tool retrieval.\nThen we describe the details of the proposed COLT approach.\n3.1 Task Formulation\nFormally, given a user query ğ‘ âˆˆQ, the goal of tool retrieval is to\nfilter out the top-ğ¾ most suitable tools {ğ‘¡(1),ğ‘¡(2),...,ğ‘¡ (ğ¾)}from\nthe entire tool set T= {(ğ‘¡1,ğ‘‘1),(ğ‘¡2,ğ‘‘2),..., (ğ‘¡ğ‘,ğ‘‘ğ‘)}, where each\nelement represents a specific tool ğ‘¡ğ‘– associated with its description\nğ‘‘ğ‘–, and ğ‘ is the number of tools in the tool set.\nGoal. As discussed in Section 1, the comprehensiveness of the tools\nretrieved is crucial for LLMs to enhance their ability to accurately\naddress multifaceted and real-time questions. Therefore, it is nec-\nessary to ensure that the retrieved tools encompass all the tools\nrequired by the user question. Considering these factors, the goal\nof tool retrieval is to optimize both accuracy and completeness,\nensuring the provision of desired tools for downstream tasks.\n3.2 Overview of COLT\nAs illustrated in Figure 2, COLT employs a two-stage learning strat-\negy, which includes semantic learning followed by collaborative\nlearning. In the first stage, the semantic learning module processes\nboth queries and tools to derive their semantic representations, aim-\ning to align these representations closely within the semantic space.\nSubsequently, the collaborative learning module enhances these\npreliminary representations by introducing three bipartite graphs\namong queries, scenes, and tools. Through dual-view graph con-\ntrastive learning within these three bipartite graphs, COLT is able\nto capture the high-order collaborative information between tools.\nFurthermore, a list-wise multi-label loss is utilized in the learning\nobjective to facilitate the balanced retrieval of diverse tools from\nthe complete ground-truth set, avoiding undue emphasis on any\nspecific tool.\nIn the following sections, we will present the details of these two\nkey learning stages in COLT.\n3.3 Semantic Learning\nAs shown in Figure 2 (a), in the first stage of COLT, we adopt the\nestablished dense retrieval (DR) framework [9, 50], leveraging pre-\ntrained language models (PLMs) such as BERT [17] to encode both\nthe query ğ‘and the toolğ‘¡into low dimensional vectors. Specifically,\nwe employ a bi-encoder architecture, with the cosine similarity\nbetween the encoded vectors serving as the initial relevance score:\nbğ‘¦SL (ğ‘,ğ‘¡)= sim(eğ‘,eğ‘¡), (1)\nwhere eğ‘ and eğ‘¡ are the mean pooling vectors from the final layer\nof the PLM, and sim(Â·,Â·)represents the cosine similarity function.\nFor training, we utilize the InfoNCE loss [ 10, 45], a standard\ncontrastive learning technique used in training DR models, which\ncontrasts positive pairs against negative ones. Specifically, given\na query ğ‘, its relevant tool ğ‘¡+ and the set of irrelevant tools\n{ğ‘¡âˆ’\n1 ,Â·Â·Â· ,ğ‘¡âˆ’\nğ‘˜ }, we minimize the following loss:\nâˆ’log ğ‘’sim(ğ‘,ğ‘¡+)\nğ‘’sim(ğ‘,ğ‘¡+)+Ãğ‘˜\nğ‘—=1 ğ‘’sim(ğ‘,ğ‘¡âˆ’\nğ‘— ). (2)\nThrough this loss function, we can increase the similarity score be-\ntween the query and its relevant tool while decreasing the similarity\nscores between the query and irrelevant tools.\nThis semantic learning phase ensures good representations\nfor each query and tool from the text description view. How-\never, relying solely on semantic-based retrieval is insufficient for\ncompleteness-oriented tool retrieval, as it often falls short in ad-\ndressing multifaceted queries effectively.\n3.4 Collaborative Learning\n3.4.1 Bipartite Graphs in Tool Retrieval. To capture the collabora-\ntive information between tools and achieve completeness-oriented\ntool retrieval, we first formulate the relationship between queries\nand tools with three bipartite graphs. Specifically, as illustrated in\nFigure 2 (b), we conceptualize the ground-truth tool set for each\nquery as a â€œsceneâ€, considering that a collaborative operation of\nmultiple tools is essential to fully address multifaceted queries. For\nexample, given the query â€œI want to travel to Paris. â€, it doesnâ€™t\nmerely seek a single piece of information but initiates a â€œsceneâ€\nof travel planning, which involves using various tools for trans-\nportation, weather forecasts, accommodation choices, and details\nabout attractions. This scenario underscores the need for scene\nmatching beyond traditional semantic search or recommendation\nscenarios, where the focus is on selecting any relevant documents\nor items without considering their collaborative utility. As a re-\nsult, traditional semantic-based retrieval systems may only retrieve\ntools related to Paris attractions, thus failing to provide a com-\nprehensive and complete tool set for the LLMs. Conversely, we\nconstruct three bipartite graphs linking queries, scenes, and tools,\ni.e., Q-S (Query-Scene) graph, Q-T (Query-Tool) graph, and S-T\n(Scene-Tool) graph. By formulating these three graphs, we can fur-\nther capture the high-order relationships among tools with graph\nlearning, facilitating a scene-based understanding that aligns to\nachieve completeness-oriented tool retrieval.\n3.4.2 Dual-view Graph Collaborative Learning. Leveraging the ini-\ntial query and tool representations derived from the first-stage se-\nmantic learning, along with the three constructed bipartite graphs,\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA. Changle Qu et al.\nPooling\n...\n...\n...\n...\n...\n...\nğ’†ğ’’ğ‘º\nğ’†ğ’”ğ‘º\nğ’†ğ’’ğ‘»\nğ’†ğ’•\nğ‘»\nğ’†ğ’”ğ‘»\nLightGCNLightGCN\nQ-S Graph\nQ-T Graph\nS-T Graph\nScene-centric View\nTool-centric View\nğ’†ğ’’ğ‘º ğ’†ğ’•\nğ‘» ğ’†ğ’’ğ‘»\nğ’”ğ’Šğ’ ğ’”ğ’Šğ’\nÛ§\nğ’š(ğ’’,ğ’•)\nğ“›list\nğ’†ğ’’ğ‘º\nğ’†ğ’’ğ‘»\nğ’†ğ’”ğ‘º\nğ’†ğ’”ğ‘»\nSimilar?\nğ“›ğ‘ª\nğ“›ğ“¢\nğ‘ªğ“›ğ“ \nğ‘ª\nMulti-label Loss\nContrastive Loss\nLearning Objective\npositive\npair\nnegative\npair\n(a) Semantic Learning\nQuery\nTool\nPLM\nPLM\nğ’†ğ’’\nğ’†ğ’•\npositive\npair\nnegative\npair\nInfoNCE\nğ“›\n(c) Dual-view Graph Collaborative Learning (d) Model Optimization\nScene\n(b) Query-Tool Matching Graph \nFigure 2: The architecture of the proposed two-stage learning framework COLT for tool retrieval.\nwe introduce a dual-view graph collaborative learning framework.\nThis framework is designed to capture the relationships between\ntools, as depicted in Figure 2 (c). It assesses the relevance between\nqueries and tools from two views:\nâ€¢Scene-centric View: Through the Q-S graph and S-T graph,\nthis view captures the relevance between queries and tools me-\ndiated by a scene. This offers a nuanced view that considers the\ncollaborative context in which tools work together to meet the\nrequirements of a query.\nâ€¢Tool-centric View: Utilizing the Q-T graph, this view estab-\nlishes a direct relevance between each query and its corresponding\ntools, providing a straightforward measure of their relevance.\nThis dual-view framework allows for comprehensive access to\nquery-tool relevance, integrating both direct relevance and the\nbroader context of tool collaboration within scenes, thereby en-\nhancing the completeness of tool retrieval.\nFor the scene-centric view, we adopt the simple but effective\nGraph Neural Network (GNN)-based LightGCN [11] model to delve\ninto the complex relationships between queries and scenes. This is\nachieved through iterative aggregation of neighboring information\nacross ğ¼ layers within the Q-S graph. The aggregation process for\nthe ğ‘–-th layer, enhancing the representations of queries eğ‘†(ğ‘–)\nğ‘ and\nscenes eğ‘†(ğ‘–)\nğ‘  , is defined as follows:\nï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³\neğ‘†(ğ‘–)\nğ‘ =\nâˆ‘ï¸\nğ‘ âˆˆNğ‘†ğ‘\n1\nâˆšï¸ƒ\n|Nğ‘†ğ‘ |\nâˆšï¸ƒ\n|Nğ‘„\nğ‘  |\neğ‘†(ğ‘–âˆ’1)\nğ‘  ,\neğ‘†(ğ‘–)\nğ‘  =\nâˆ‘ï¸\nğ‘âˆˆNğ‘„\nğ‘ \n1\nâˆšï¸ƒ\n|Nğ‘†ğ‘ |\nâˆšï¸ƒ\n|Nğ‘„\nğ‘  |\neğ‘†(ğ‘–âˆ’1)\nğ‘ ,\n(3)\nwhere Nğ‘†ğ‘, Nğ‘„\nğ‘  represent the sets of neighbors of query ğ‘ and\nscene ğ‘  in the Q-S graph, respectively. eğ‘†(0)\nğ‘ originates from the\nrepresentations acquired in the first semantic learning stage, while\neğ‘†(0)\nğ‘  is derived from the mean pooling of the representations of\nground-truth tools associated with each scene:\neğ‘†(0)\nğ‘  = 1\n|Nğ‘‡ğ‘  |\nâˆ‘ï¸\nğ‘¡âˆˆNğ‘‡ğ‘ \neğ‘¡, (4)\nwhere Nğ‘‡ğ‘  represents the set of first-order neighbors of scene ğ‘  in\nthe S-T graph.\nThen we sum the representations from the 0-th layer to the ğ¼-th\nlayer to get the final query representations eğ‘†ğ‘ and scene represen-\ntation eğ‘†ğ‘  for the scene-centric view:\nï£±ï£´ï£´ ï£²\nï£´ï£´ï£³\neğ‘†\nğ‘ = eğ‘†(0)\nğ‘ +Â·Â·Â·+ eğ‘†(ğ¼)\nğ‘ ,\neğ‘†\nğ‘  = eğ‘†(0)\nğ‘  +Â·Â·Â·+ eğ‘†(ğ¼)\nğ‘  .\n(5)\nIn parallel with the scene-centric view, the tool-centric view\nutilizes LightGCN on the Q-T graph to refine query and tool rep-\nresentations through iterative aggregation. For each layer ğ‘–, the\nenhanced representations, eğ‘‡(ğ‘–)\nğ‘ for queries and eğ‘‡(ğ‘–)\nğ‘¡ for tools, are\nderived as follows:\nï£±ï£´ï£´ï£´ï£´ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£´ï£´ï£´ï£´ï£³\neğ‘‡(ğ‘–)\nğ‘ =\nâˆ‘ï¸\nğ‘¡âˆˆNğ‘‡ğ‘\n1\nâˆšï¸ƒ\n|Nğ‘‡ğ‘ |\nâˆšï¸ƒ\n|Nğ‘„\nğ‘¡ |\neğ‘‡(ğ‘–âˆ’1)\nğ‘¡ ,\neğ‘‡(ğ‘–)\nğ‘¡ =\nâˆ‘ï¸\nğ‘âˆˆNğ‘„\nğ‘¡\n1\nâˆšï¸ƒ\n|Nğ‘‡ğ‘ |\nâˆšï¸ƒ\n|Nğ‘„\nğ‘¡ |\neğ‘‡(ğ‘–âˆ’1)\nğ‘ ,\n(6)\nwhere Nğ‘‡ğ‘ , Nğ‘„\nğ‘¡ represent the first-order neighbors of query ğ‘and\ntool ğ‘¡ in the Q-T graph, respectively. eğ‘‡(0)\nğ‘ and eğ‘‡(0)\nğ‘¡ are obtained\nfrom the first semantic learning stage.\nThen we sum the representations from the 0-th layer to the\nğ¼-th layer to derive the final query representations eğ‘‡ğ‘ and tool\nrepresentation eğ‘‡\nğ‘¡ for the tool-centric view:\nï£±ï£´ï£´ ï£²\nï£´ï£´ï£³\neğ‘‡\nğ‘ = eğ‘‡(0)\nğ‘ +Â·Â·Â·+ eğ‘‡(ğ¼)\nğ‘ ,\neğ‘‡\nğ‘¡ = eğ‘‡(0)\nğ‘¡ +Â·Â·Â·+ eğ‘‡(ğ¼)\nğ‘¡ .\n(7)\nFurthermore, leveraging the learned tool representations eğ‘‡\nğ‘¡ and\nthe S-T graph, the scene representation eğ‘‡ğ‘  within the tool-centric\nview can be obtained by pooling all related tool representations:\neğ‘‡\nğ‘  = 1\n|Nğ‘‡ğ‘  |\nâˆ‘ï¸\nğ‘¡âˆˆNğ‘‡ğ‘ \neğ‘‡\nğ‘¡ . (8)\nTowards Completeness-Oriented Tool Retrieval for Large Language Models CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA.\nAlgorithm 1 The Learning Algorithm of COLT\nInput: PLM, semantic learning training epoch ğ¸, Query-scene bipartite\ngraph, query-tool bipartite graph, scene-tool bipartite graph, learning\nrate ğ‘™ğ‘Ÿ, weight decay, layer number ğ¼, contrastive loss weight ğœ†, tem-\nperature coefficient ğœ, list length ğ¿.\nOutput: COLT Model with learnable parameters ğœƒ.\n// Semantic Learning:\n1: for ğ‘’ = 1 to ğ¸do\n2: Calculate the InfoNCE loss using Eq. (2)\n3: Update parameter of PLM using AdaW\n4: end for\n// Collaborative Learning:\n5: Calculate initial eğ‘†(0)\nğ‘ , eğ‘†(0)\nğ‘  , eğ‘‡(0)\nğ‘ and eğ‘‡(0)\nğ‘¡ using the embeddings\nobtained from the first-stage semantic learning and Eq. (4)\n6: while COLT not Convergence do\n7: for ğ‘– = 1 to ğ¼ do\n8: Conduct message propagation using Eq. (3) and Eq. (6)\n9: end for\n10: Calculate final eğ‘†ğ‘, eğ‘†ğ‘ , eğ‘‡ğ‘, eğ‘‡ğ‘  and eğ‘‡\nğ‘¡ using Eq. (5), Eq. (7) and Eq. (8)\n11: Calculate contrastive loss Lğ¶\nQ and Lğ¶\nS using Eq. (10) and Eq. (11)\n12: Calculate multi-label loss Llist using Eq. (14)\n13: Calculate total loss Lusing Eq. (15)\n14: Update model parameter using Adam\n15: end while\n16: return ğœƒ\nIn summary, our dual-view graph collaborative learning frame-\nwork yields two sets of embeddings: eğ‘†ğ‘ and eğ‘†ğ‘  from the scene-\ncentric view, and eğ‘‡ğ‘ and eğ‘‡ğ‘  from the tool-centric view for queries\nand scenes, respectively. Then, the final matching score of each\nquery-tool pair (ğ‘,ğ‘¡)is calculated using the following formula:\nbğ‘¦(ğ‘,ğ‘¡)= sim(eğ‘†\nğ‘,eğ‘‡\nğ‘¡ )+sim(eğ‘‡\nğ‘,eğ‘‡\nğ‘¡ ). (9)\n3.4.3 Learning Objective. As shown in Figure 2 (d), we capture\nhigh-order collaborative relationships between tools and align the\ncooperative interactions across two views using a cross-view con-\ntrastive loss. Specifically, the representations of queries and scenes\ncan be learned by optimizing the cross-view InfoNCE [10, 37] loss:\nLğ¶\nQ= âˆ’ 1\n|Q|\nâˆ‘ï¸\nğ‘âˆˆQ\nlog ğ‘’sim(eğ‘†\nğ‘,eğ‘‡\nğ‘)/ğœ\nÃ\nğ‘âˆ’âˆˆQ ğ‘’sim(eğ‘†ğ‘,eğ‘‡ğ‘âˆ’)/ğœ, (10)\nLğ¶\nS= âˆ’ 1\n|S|\nâˆ‘ï¸\nğ‘ âˆˆS\nlog ğ‘’sim(eğ‘†\nğ‘ ,eğ‘‡\nğ‘  )/ğœ\nÃ\nğ‘ âˆ’âˆˆSğ‘’sim(eğ‘†ğ‘ ,eğ‘‡ğ‘ âˆ’)/ğœ, (11)\nwhere ğœ is the temperature parameter.\nTo ensure the complete retrieval of diverse tools from the full\nset of ground-truth tools, without favoring any particular tool, we\ndesign a list-wise multi-label loss as the main learning objective loss.\nGiven a query ğ‘, the labeled training data is Î“ğ‘ = {Tğ‘ = {ğ‘¡ğ‘–,ğ‘‘ğ‘–},ğ‘¦ =\n{ğ‘¦(ğ‘,ğ‘¡ğ‘–)}|1 â‰¤ğ‘– â‰¤ğ¿}, where Tğ‘ denotes a tool list with length ğ¿,\ncomprising ğ‘ğ‘ ground-truth tools and ğ¿âˆ’ğ‘ğ‘ negative tools that\nare randomly sampled from the entire tool set.ğ‘¦(ğ‘,ğ‘¡ğ‘–)is the binary\nrelevance label, taking a value of either 0 or 1, and the ideal scoring\nfunction should meet the following criteria:\nğ‘ğ‘¡\nğ‘ = ğ›¾(ğ‘¦(ğ‘,ğ‘¡))Ã\nğ‘¡â€²âˆˆTğ‘ ğ›¾(ğ‘¦(ğ‘,ğ‘¡â€²)), (12)\nTable 1: Statistics of the experimental datasets. Tools/Query\ndenotes the number of ground-truth tools for each query.\nDataset # Query # Tool # Tools/Query\nTraining Testing Total\nToolLens 16,893 1,877 18,770 464 1 âˆ¼3\nToolBench (I2) 74,257 8,250 82,507 11,473 2 âˆ¼4\nToolBench (I3) 21,361 2,373 23,734 1,419 2 âˆ¼4\nwhere ğ‘ğ‘¡ğ‘ is the probability of selecting tool ğ‘¡. ğ›¾(ğ‘¦(ğ‘,ğ‘¡)) = 1 if\nğ‘¦(ğ‘,ğ‘¡)= 1 and ğ›¾(ğ‘¦(ğ‘,ğ‘¡))= 0 if ğ‘¦(ğ‘,ğ‘¡)= 0.\nSimilarly, given the predicted scores {bğ‘¦(ğ‘,ğ‘¡1),Â·Â·Â· ,bğ‘¦(ğ‘,ğ‘¡ğ¿)}, the\nprobability of selecting tool ğ‘¡ can be derived:\ncğ‘ğ‘¡ğ‘ = ğ›¾(bğ‘¦(ğ‘,ğ‘¡))Ã\nğ‘¡â€²âˆˆTğ‘ ğ›¾(bğ‘¦(ğ‘,ğ‘¡â€²)). (13)\nTherefore, the list-wise multi-label loss function minimizes the\ndiscrepancy between these two probability distributions:\nLlist = âˆ’\nâˆ‘ï¸\nğ‘âˆˆQ\nâˆ‘ï¸\nğ‘¡âˆˆTğ‘\nğ‘ğ‘¡\nğ‘log cğ‘ğ‘¡ğ‘ +(1 âˆ’ğ‘ğ‘¡\nğ‘)log(1 âˆ’cğ‘ğ‘¡ğ‘), (14)\nBased on the multi-label loss Llist and the contrastive loss Lğ¶\nQ,\nthe final loss Lfor our proposed COLT is formally defined as:\nL= Llist +ğœ†(Lğ¶\nQ+Lğ¶\nS), (15)\nwhere ğœ†is the co-efficient to balance the two losses.\nThe learning algorithm of COLT is summarized in Algorithm 1.\n4 DATASETS\nTo verify the effectiveness of COLT, we utilize two datasets for\nmulti-tool scenarios: ToolBench and a newly constructed dataset,\nToolLens. We randomly select 10% of the entire dataset to serve as\nthe test data. The statistics of the datasets after preprocessing are\nsummarized in Table 1.\nToolBench. ToolBench [28] is a benchmark commonly used to\nevaluate the capability of LLMs in tool usage. In our experiments,\nwe notice that its three subsets exhibit distinct characteristics. The\nfirst subset (I1) focuses on single-tool scenarios, which diverges\nfrom our emphasis on multi-tool tasks. However, both the second\nsubset (I2) and the third subset (I3) align with our focus on multi-\ntool tasks. Therefore, we chose I2 and I3 as the primary datasets\nfor our experiments.\nToolLens. While existing datasets like ToolBench [ 28] and\nTOOLE [14] provide multi-tool scenarios, they present limitations.\nTOOLE encompasses only 497 queries, and ToolBenchâ€™s dataset\nconstruction, which involves providing complete tool descriptions\nto ChatGPT, results in verbose and semantically direct queries.\nThese do not accurately reflect the brief and often multifaceted\nnature of real-world user queries. To address these shortcomings,\nwe introduce ToolLens, crafted specifically for multi-tool scenarios.\nAs shown in Figure 3, the creation of ToolLens involves a novel\nfive-step methodology: 1) Tool Selection: To create a high-quality\ntool dataset, we rigorously filter ToolBench, focusing on 464 avail-\nable and directly callable tools relevant to everyday user queries,\nexcluding those for authentication or testing. 2) Scene Mining:\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA. Changle Qu et al.\n(    ,    )\n(    ,    )\n...    \nTool Selection\n Scene Mining\nInvest\nTravel\n...    \n ...    \nQuery Generation \n(Single-tool)\nTool Aggregation\n...    \n ...\nQuery Rewriting \n(Multi-tool)\nscene+para query\nscene+para query\nquery+para\nquery+para\nquery\nquery\nFigure 3: An overview of the dataset construction pipeline of ToolLens. Human verification is included at each step.\nTable 2: Quality verification of ToolLens.\nEvaluator ToolLens vs. ToolBench ToolLens vs. TOOLE\nWhether the query is natural?\nGPT-4 ToolLens ToolBench Equal ToolLens TOOLE Equal\n68% 14% 18% 44% 36% 20%\nHuman ToolLens ToolBench Equal ToolLens TOOLE Equal\n64% 10% 26% 54% 24% 22%\nWhether the user intent is multifaceted?\nGPT-4 ToolLens ToolBench Equal ToolLens TOOLE Equal\n62% 14% 24% 50% 24% 26%\nHuman ToolLens ToolBench Equal ToolLens TOOLE Equal\n60% 12% 28% 58% 18% 24%\nWe prompt GPT-4 to generate potential scenes that are relevant to\nthe detailed descriptions of the selected tools, and ensure their va-\nlidity through human verification. 3) Query Generation: We then\nemploy GPT-4 to generate queries based on the provided scene and\nthe parameters required for tool calling. Notably, we avoid provid-\ning the complete tool description to GPT-4 to avoid the generated\nquery being closely aligned with the tool. 4) Tool Aggregation:\nThe queries generated in the aforementioned way are only relevant\nto a single tool. To enhance the relevance of queries across multi-\nple tools, we reprocess them through GPT-4 to identify categories\nof tools that could be relevant, which are then aligned with our\ntool set through dense retrieval and manual verification. 5) Query\nRewriting: Finally, we utilize GPT-4 to revise queries to incorpo-\nrate all necessary parameters by providing it with both the initial\nquery and a list of required parameters, thereby yielding concise\nyet intentionally multifaceted queries that better mimic real-world\nuser interactions. It is worth noting that we incorporate a human\nverification process at each step to ensure data quality.\nThis comprehensive construction pipeline ensures ToolLens ac-\ncurately simulates real-world tool retrieval dynamics. The resulting\nToolLens dataset includes 18,770 queries and 464 tools, with each\nquery being associated with 1 âˆ¼3 verified tools.\nDiscussion and Quality Verification. Unlike prior datasets, Tool-\nLens uniquely focuses on creating natural, concise, and multifaceted\nqueries to reflect real-world demands. To assess the quality of Tool-\nLens, following previous works [8, 21, 34], we employ GPT-4 as an\nevaluator and human evaluation where three well-educated doctor\nstudents are invited to evaluate 50 randomly sampled cases from\nToolLens, ToolBench and TOOLE in the following two aspects:(1)\nNatural-query: whether the query is natural. (2) Multifaceted in-\ntentions: whether the user intent is multifaceted. The results are\nillustrated in Table 2. In most cases, ToolLens outperforms Tool-\nBench and TOOLE. Furthermore, using GPT-4 as the evaluator\nshows a high degree of consistency with human evaluation trends,\nwhich underscores the validity of employing GPT-4 as an evaluator.\n5 EXPERIMENTS\nIn this section, we first describe the experimental setups and then\nconduct an extensive evaluation and analysis of the proposed COLT.\nThe source code and the proposed ToolLens dataset are publicly\navailable at https://github.com/quchangle1/COLT.\n5.1 Experimental Setups\n5.1.1 Evaluation Metrics. Following the previous works [ 8, 28,\n51], we utilize the widely used retrieval metrics Recall@ ğ¾ and\nNDCG@ğ¾ and report the metrics for ğ¾ âˆˆ{3,5}. However, as dis-\ncussed in Section 1, Recall and NDCG do not adequately fulfill the\nrequirements of completeness that are crucial for effective tool re-\ntrieval. To further tailor our assessment to the specific challenges\nof tool retrieval tasks, we also introduce a new metric, COMP@ğ¾.\nThis metric is designed to measure whether the top- ğ¾ retrieved\ntools form a complete set with respect to the ground-truth set:\nCOMP@ğ¾ = 1\n|Q|\n|Q|âˆ‘ï¸\nğ‘=1\nI(Î¦ğ‘ âŠ†Î¨ğ¾\nğ‘ ),\nwhere Î¦ğ‘ is the set of ground-truth tools for queryğ‘, Î¨ğ¾ğ‘ represents\nthe top-ğ¾ tools retrieved for query ğ‘, and I(Â·)is an indicator func-\ntion that returns 1 if the retrieval results include all ground-truth\ntools within the top-ğ¾ results for query ğ‘, and 0 otherwise.\n5.1.2 Baselines. As our proposed COLT is model-agnostic, we\napply it to several representative PLM-based retrieval models (as\nbackbone models) to validate the effectiveness:\nANCE[45] uses a dual-encoder architecture with an asyn-\nchronously updated ANN index for training, enabling global se-\nlection of hard negatives. TAS-B[12] is a bi-encoder that employs\nbalanced margin sampling to ensure efficient query sampling from\nclusters per batch. co-Condenser[7] uses a query-agnostic con-\ntrastive loss to cluster related text segments and distinguish unre-\nlated ones. Contriever[15] leverages inverse cloze tasks, cropping\nfor positive pair generation, and momentum contrastive learning\nto achieve state-of-the-art zero-shot retrieval performance.\nIn addition to PLM-based dense retrieval methods, we also com-\npare with the classical lexical retrieval model BM25, widely used for\ntool retrieval as documented in [8, 28]. BM25 [30] uses an inverted\nindex to identify suitable tools based on exact term matching.\nTowards Completeness-Oriented Tool Retrieval for Large Language Models CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA.\nTable 3: Performance comparison of different tool retrieval methods on ToolLens and ToolBench datasets. â€œ â€ â€ denotes the\nbest results for each column. The term â€œZero-shotâ€ refers to the performance of dense retrieval models without any training.\nâ€œ+Fine-tuneâ€ indicates that retrieval models are fine-tuned on ToolLens and ToolBench datasets. â€œ+COLT (Ours)â€ indicates that\ndense retrieval backbones are equipped with our proposed method. R@ ğ¾, N@ğ¾, and C@ ğ¾ are short for Recall@ ğ¾, NDCG@ğ¾\nand COMP@ğ¾, respectively.\nBackbone Framework ToolLens ToolBench (I2) ToolBench (I3)\nR@3 R@5 N@3 N@5 C@3 C@5 R@3 R@5 N@3 N@5 C@3 C@5 R@3 R@5 N@3 N@5 C@3 C@5\nBM25 - 21.58 26.88 23.19 26.09 3.89 6.13 17.06 21.38 17.83 19.88 2.39 4.37 29.33 35.88 32.20 35.08 5.52 9.78\nANCE\nZero-shot 20.82 26.56 21.45 24.57 5.06 7.46 20.82 26.56 21.45 24.57 5.06 7.46 21.55 26.38 23.44 25.60 2.44 4.59\n+Fine-tune 80.62 94.17 82.35 90.15 54.23 85.83 58.58 67.20 58.58 63.75 26.46 42.80 65.11 76.63 69.27 74.14 34.68 53.64\n+COLT (Ours)92.15 97.78â€  92.78 96.10 80.50 94.40 70.76 80.59 73.64 77.98 45.10 62.93 73.37 83.97 77.95 82.14 46.01 66.41\nTAS-B\nZero-shot 19.10 23.71 19.81 22.33 5.17 7.14 19.10 23.71 19.81 22.33 5.17 7.14 25.32 31.15 27.80 30.36 3.84 6.40\n+Fine-tune 81.26 94.06 82.54 89.94 54.66 85.72 62.78 67.49 58.96 64.21 26.74 43.66 66.04 77.64 70.41 75.34 35.69 55.75\n+COLT (Ours)91.49 96.91 92.48 95.63 79.00 92.22 71.64 81.12 74.60 78.74 46.77 64.38 74.49 84.58 79.03 82.95 48.16 68.35\ncoCondensor\nZero-shot 15.33 19.37 16.15 18.32 3.02 5.33 15.33 19.37 16.15 18.32 3.02 5.30 20.80 25.24 23.21 25.10 2.07 3.75\n+Fine-tune 82.37 94.69 83.90 91.06 56.37 86.73 57.70 69.46 60.80 66.07 28.78 46.06 66.97 79.30 71.20 76.50 37.08 58.66\n+COLT (Ours)92.65 97.78â€  93.16 96.17 82.25 94.56â€  73.91 83.47 76.75 80.87 49.15 67.75 75.48 84.97 80.00 83.55 49.17 68.64 â€ \nContriever\nZero-shot 25.67 31.15 26.96 29.95 7.46 9.80 25.67 31.15 26.96 29.95 7.46 9.80 31.37 38.60 34.13 37.37 6.03 11.42\n+Fine-tune 83.58 95.17 84.98 91.69 59.46 88.65 58.89 70.75 62.11 67.42 29.77 48.31 68.58 80.05 72.86 77.69 39.70 60.89\n+COLT (Ours)93.64â€  97.75 94.53â€  96.91â€  84.55â€  94.08 75.72â€  85.03â€  78.57â€  82.54â€  51.97â€  70.10â€  76.63â€  85.50â€  81.21â€  84.18â€  52.00â€  68.47\n5.1.3 Implementation Details. We utilize the BEIR [40] framework\nfor dense retrieval baselines, setting the training epochs to 5 with\nthe learning rate of 2ğ‘’â€“5, weight decay of 0.01, and using the\nAdamW optimizer. Our model-agnostic approach directly applies\ndense retrieval for semantic learning. During collaborative learn-\ning, we set the batch size as 2048 and carefully tune the learning\nrate among {1ğ‘’â€“3,5ğ‘’â€“3,1ğ‘’â€“4,5ğ‘’â€“4,1ğ‘’â€“5}, the weight decay among\n{1ğ‘’â€“5,1ğ‘’â€“6,1ğ‘’â€“7}, as well as the layer number ğ¼ among {1,2,3}.\n5.2 Experimental Results\n5.2.1 Retrieval Performance. Table 3 presents the results of differ-\nent tool retrieval methods on ToolLens, ToolBench (I2 and I3). From\nthe results, we have the following observations and conclusions:\nWe can observe that traditional dense retrieval models perform\npoorly in zero-shot scenarios, even inferior to that of BM25. This\nindicates that these models may not be well-suited for tool retrieval\ntasks. Conversely, the BM25 model significantly lags behind fine-\ntuned PLM-based dense retrieval methods, underscoring the supe-\nrior capability of the latter in leveraging contextual information for\nmore effective tool retrieval. Despite this advantage, PLM-based\nmethods fall short in the COMP metric, which is specifically de-\nsigned for evaluating completeness in tool retrieval scenarios. This\nsuggests that while effective for general retrieval tasks, PLM-based\nmethods may not fully meet the unique demands of tool retrieval.\nAll base models equipped with COLT exhibit significant perfor-\nmance gains across all metrics on all three datasets, particularly in\nthe COMP@3 metric. These improvements demonstrate the effec-\ntiveness of COLT, which can be attributed to the fact that COLT\nadopts a two-stage learning framework with semantic learning\nfollowed by collaborative learning. In this way, COLT can capture\nthe intricate collaborative relationships between tools, resulting in\neffectively retrieving a complete tool set.\n5.2.2 Downstream Tool Learning Performance. To verify that im-\nprovements of COLT in tool retrieval truly enhance downstream\ntool learning applications, we conduct a validation study using the\npairwise comparison method [5, 19, 36]. We randomly select 100\nTable 4: Elo ratings for different models w.r.t. â€œCoherenceâ€,\nâ€œRelevanceâ€, â€œComprehensivenessâ€ and â€œOverallâ€ evaluated\nby GPT-4 on ToolLens dataset.\nEvaluation Aspects\nCoherence Relevance Comprehensiveness Overall\nBM25 848 845 860 780\nANCE 934 936 946 1016\nTAS-B 995 991 988 1028\ncoCondensor 1031 1036 1041 1035\nContriever 1076 1082 1044 1046\nCOLT (Ours) 1116 1110 1121 1096\nqueries from the test set of ToolLens and use various retrieval mod-\nels to retrieve the top-3 tools for each query. Then we utilize GPT-4\nas an evaluator, examining the responses generated with different\nretrieved tools across four dimensions: Coherence, Relevance, Com-\nprehensiveness, and Overall. Specifically, the user query and a pair\nof responses are utilized as prompts to guide GPT-4 in determining\nthe superior response. Additionally, we also consider that LLMs\nmay respond differently to the order in which text is presented in\nthe prompt [13, 20, 22, 39]. So each comparison is conducted twice\nwith reversed response order to mitigate potential biases from text\norder, ensuring a more reliable assessment.\nWe establish a tournament-style competition using the Elo rat-\nings system, which is widely employed in chess and other two-\nplayer games to measure the relative skill levels of the players [6, 44].\nFollowing previous works [3], we start with a score of 1,000 and\nset ğ¾-factor to 32. Additionally, to minimize the impact of match\nsequences on Elo scores, we conduct these computations 10,000\ntimes using various random seeds to control for ordering effects.\nThe results in Table 4 show that superior tool retrieval models\ncan significantly improve downstream tool learning performance.\nMoreover, responses generated with the tools retrieved from COLT\nnotably outperform those from other methods, achieving the high-\nest Elo ratings in all four assessed dimensions. These results high-\nlight the pivotal role of effective tool retrieval in tool learning\napplications and further confirm the superiority of COLT.\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA. Changle Qu et al.\nTable 5: Ablation study of the proposed COLT.\nMethods\nToolLens ToolBench\nR@|N| C@|N| R@|N| C@|N|\nANCE+COLT (Ours) 91.08 78.36 72.22 44.28\nw/o semantic learning 36.49 6.84 21.92 1.60\nw/o collaborative learning 77.36 49.01 62.39 30.12\nw/o list-wise learing 79.94 52.68 66.02 35.82\nw/o contrastive learning 85.63 63.87 66.57 34.55\nTAS-B+COLT (Ours) 90.29 77.73 72.84 45.46\nw/o semantic learning 38.49 9.16 32.16 5.47\nw/o collaborative learning 76.86 47.83 63.61 31.73\nw/o list-wise learning 79.89 52.25 66.91 37.27\nw/o contrastive learning 84.86 62.65 67.66 36.36\ncoCondensor+COLT (Ours) 91.49 79.86 74.00 47.49\nw/o semantic learning 30.38 5.54 25.07 2.27\nw/o collaborative learning 78.83 50.61 64.38 33.08\nw/o list-wise learning 81.42 54.16 69.18 40.67\nw/o contrastive learning 86.78 67.07 68.92 37.80\nContriever+COLT (Ours) 92.76 82.95 75.40 49.81\nw/o semantic learning 65.21 30.90 53.33 19.63\nw/o collaborative learning 80.60 54.44 68.20 36.91\nw/o list-wise learning 81.49 54.93 71.80 46.07\nw/o contrastive learning 84.58 60.52 69.46 39.02\n5.3 Further Analysis\nNext, we delve into investigating the effectiveness of COLT. We\nreport the experimental results on the ToolLens and ToolBench (I3)\ndatasets, observing similar trends on ToolBench (I2). Recall@|N| and\nCOMP@|N| are adopted as evaluation metrics, with |N| representing\nthe count of ground-truth tools suitable for each query.\n5.3.1 Ablation Study. We conduct ablation studies to assess the im-\npact of various components within our COLT. The results presented\nin Table 5, highlight the significance of each element:\nw/o semantic learning denotes an off-the-shelf PLM is directly\nemployed to get the initial representation for the subsequent col-\nlaborative learning stage without semantic learning on the given\ndataset in Section 3.3. The absence of semantic learning signif-\nicantly diminishes performance, confirming its essential role in\naligning the representations of tools and queries as the basic for the\nfollowing collaborative learning. Notably, the omission of seman-\ntic learning elements markedly reduces performance across other\nmodels more than with Contriever. This highlights the superior\nability of Contriever in zero-shot learning scenarios compared to\nthe other models.\nw/o collaborative learning is a variant where the collabora-\ntive learning state is omitted ( i.e., only semantic learning). The\nsignificant decline in performance in this variant further supports\nthe effectiveness of COLT in capturing the high-order relation-\nships between tools through graph collaborative learning, thereby\nachieving comprehensive tool retrieval.\nw/o list-wise learning refers to a variant that optimizes using\npair-wise loss in place of the list-wise loss defined in Eq. (14). This\nsubstitution results in a significant drop in performance, highlight-\ning that compared to pairwise loss, list-wise loss optimizes the tools\nin the same scenario as a whole entity, proving more effective in\nfocusing on completeness.\nmini\n11M\nsmall\n29M\nmedium\n41M\nbase\n110M\nlarge\n340M\nModel Size\n30.0\n50.0\n70.0\n90.0COMP@|N| (%)\nBERT\nBERT+COLT(Ours)\n(a) ToolLens\nmini\n11M\nsmall\n29M\nmedium\n41M\nbase\n110M\nlarge\n340M\nModel Size\n10.0\n20.0\n30.0\n40.0\n50.0COMP@|N| (%)\nBERT\nBERT+COLT(Ours) (b) ToolBench\nFigure 4: Comparison of different model sizes of PLM.\nw/o contrastive learning refers to a variant that optimizes with-\nout the contrastive loss defined in Eq. (10) and (11); This omission\nalso leads to a noticeable performance drop, emphasizing the bene-\nfits of introducing contrastive learning to achieve better represen-\ntation for queries and tools within a dual-view learning framework.\nAdditionally, our analysis reveals that contrastive learning is partic-\nularly crucial for Contriever, as its absence results in performance\nlagging behind the other models. This also indicates that the im-\nportance of contrastive learning varies across different backbones.\n5.3.2 Performance w.r.t. Model Size of PLM. To verify the adapt-\nability and effectiveness of COLT across varying sizes of PLMs, we\nexplore its integration with a range of BERT models, from BERT-\nmini to BERT-large. This analysis aims to determine whether COLT\ncould generally enhance tool retrieval performance across different\nmodel sizes. Figure 4 shows that while the performance of the base\nmodel naturally improves with larger PLM sizes, the integration of\nCOLT consistently boosts performance across all sizes. Remarkably,\neven BERT-mini equipped with COLT, significantly outperforms a\nmuch larger BERT-large model (30x larger) operating without our\nCOLT. These results underscore the generalization and robustness\nof COLT, demonstrating its potential to significantly improve tool\nretrieval performance for PLMs of any scale.\n5.3.3 Performance w.r.t. Different Tool Sizes. The ToolLens dataset\nencompasses queries that require 1 âˆ¼3 tools, while ToolBench\nincludes queries needing 2 âˆ¼4 tools. To assess how well COLT\nadapts to queries with diverse tool requirements, we divide each\ndataset into three subsets based on the number of tools required\nby each query and conduct a focused analysis on these subsets. As\nshown in Figure 5, there is a discernible decline in performance\nas the number of ground-truth tools increases, reflecting the esca-\nlating difficulty of achieving complete retrieval. However, COLT\ndemonstrates consistent performance improvement across all sub-\nsets and backbones. This improvement is especially significant in\nthe most challenging cases, where queries may involve using three\nor four tools. These results consistently highlight the robustness\nof COLT and its potential to meet the complex demands of tool\nretrieval tasks across various scenarios.\n5.3.4 Hyper-parameter Analysis. Figure 6 illustrates the sensitivity\nof COLT to the temperature parameter ğœ and the loss weight ğœ†, but\nshows relative insensitivity to variations in the sampled list length\nğ¿. The influence of ğœ varies across two datasets, suggesting that its\nimpact depends on the specific data distribution. Conversely, the\npattern observed for ğœ†across both datasets is consistent, marked\nTowards Completeness-Oriented Tool Retrieval for Large Language Models CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA.\n1 2 3\n|N|\n40.0\n60.0\n80.0\n100.0COMP@|N| (%)\nANCE\nANCE+COLT(Ours)\n1 2 3\n|N|\n40.0\n60.0\n80.0\n100.0COMP@|N| (%)\nTAS-B\nTAS-B+COLT(Ours)\n1 2 3\n|N|\n40.0\n60.0\n80.0\n100.0COMP@|N| (%)\ncoCondensor\ncoCondensor+COLT(Ours)\n1 2 3\n|N|\n40.0\n60.0\n80.0\n100.0COMP@|N| (%)\nContriever\nContriever+COLT(Ours)\n(a) ToolLens\n2 3 4\n|N|\n20.0\n40.0\n60.0COMP@|N| (%)\nANCE\nANCE+COLT(Ours)\n2 3 4\n|N|\n20.0\n40.0\n60.0COMP@|N| (%)\nTAS-B\nTAS-B+COLT(Ours)\n2 3 4\n|N|\n20.0\n40.0\n60.0COMP@|N| (%)\ncoCondensor\ncoCondensorr+COLT(Ours)\n2 3 4\n|N|\n20.0\n40.0\n60.0COMP@|N| (%)\nContriever\nContriever+COLT(Ours)\n(b) ToolBench\nFigure 5: Performance comparison regarding different sizes of ground-truth tool sets.\n0.05 0.1 0.15 0.2 0.25\nÏ„\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N|\nANCE\n0.05 0.1 0.15 0.2 0.25\nÏ„\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N| TAS-B\n0.05 0.1 0.15 0.2 0.25\nÏ„\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N| coCondensor\n0.05 0.1 0.15 0.2 0.25\nÏ„\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N| Contriever\n(a) Temperature ğœ.\n0 1 2 4 8 16\nÎ» (%)\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 34.0\n36.0\n38.0\n40.0\n42.0\n44.0\n46.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N|\nANCE\n0 1 2 4 8 16\nÎ» (%)\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 36.0\n38.0\n40.0\n42.0\n44.0\n46.0\n48.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N| TAS-B\n0 1 2 4 8 16\nÎ» (%)\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N| coCondensor\n0 1 2 4 8 16\nÎ» (%)\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N| Contriever\n(b) Loss weight ğœ†.\n5 10 15 20 25\nL\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N|\nANCE\n5 10 15 20 25\nL\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N| TAS-B\n5 10 15 20 25\nL\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N| coCondensor\n5 10 15 20 25\nL\n60.0\n65.0\n70.0\n75.0\n80.0\n85.0\n90.0ToolLens COMP@|N| (%)\n 38.0\n40.0\n42.0\n44.0\n46.0\n48.0\n50.0\nToolBench COMP@|N| (%)\nToolLens COMP@|N|\nToolBench COMP@|N| Contriever\n(c) List length ğ¿.\nFigure 6: Sensitivity analysis of COLT performance to hyper-parameters. (a) shows the dependency of model performance on\ntemperature ğœ. (b) illustrates the influence of loss weight ğœ†. (c) examines the effect of list length ğ¿.\nby an initial performance improvement that eventually plateaus,\nunderscoring the importance of carefully selecting ğœ†to maximize\nthe effectiveness of COLT.\n6 CONCLUSION\nThis study introduces COLT, a novel model-agnostic approach\ndesigned to enhance the completeness of tool retrieval tasks, com-\nprising two stages: semantic learning and collaborative learning.\nWe initially employ semantic learning to ensure semantic represen-\ntation between queries and tools. Subsequently, by incorporating\ngraph collaborative learning and cross-view contrastive learning,\nCOLT captures the collaborative relationships among tools. Exten-\nsive experimental results and analysis demonstrate the effectiveness\nof COLT, especially in handling multifaceted queries with multiple\ntool requirements. Furthermore, we release a new dataset ToolLens\nand introduce a novel evaluation metric COMP, both of which are\nvaluable resources for facilitating future research on tool retrieval.\nACKNOWLEDGMENTS\nThis work was funded by the National Key R&D Program of China\n(2023YFA1008704), the National Natural Science Foundation of\nChina (No. 62377044), Beijing Key Laboratory of Big Data Manage-\nment and Analysis Methods, Major Innovation & Planning Interdis-\nciplinary Platform for the â€œDouble-First Classâ€ Initiative, funds for\nbuilding world-class universities (disciplines) of Renmin University\nof China, and PCC@RUC.\nCIKM â€™24, October 21â€“25, 2024, Boise, ID, USA. Changle Qu et al.\nREFERENCES\n[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Floren-\ncia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal\nAnadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774\n(2023).\n[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877â€“1901.\n[3] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nLianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,\nand Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with\n90%* ChatGPT Quality. https://vicuna.lmsys.org\n[4] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav\nMishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Se-\nbastian Gehrmann, et al. 2023. Palm: Scaling language modeling with pathways.\nJournal of Machine Learning Research 24, 240 (2023), 1â€“113.\n[5] Sunhao Dai, Ninglu Shao, Haiyuan Zhao, Weijie Yu, Zihua Si, Chen Xu, Zhongx-\niang Sun, Xiao Zhang, and Jun Xu. 2023. Uncovering ChatGPTâ€™s Capabilities\nin Recommender Systems. In Proceedings of the 17th ACM Conference on Recom-\nmender Systems (RecSys â€™23) . ACM. https://doi.org/10.1145/3604915.3610646\n[6] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023.\nQLoRA: Efficient Finetuning of Quantized LLMs. arXiv:2305.14314 [cs.LG]\n[7] Luyu Gao and Jamie Callan. 2021. Unsupervised corpus aware language model\npre-training for dense passage retrieval. arXiv preprint arXiv:2108.05540 (2021).\n[8] Shen Gao, Zhengliang Shi, Minghang Zhu, Bowen Fang, Xin Xin, Pengjie Ren,\nZhumin Chen, Jun Ma, and Zhaochun Ren. 2024. Confucius: Iterative Tool\nLearning from Introspection Feedback by Easy-to-Difficult Curriculum. InAAAI.\n[9] Jiafeng Guo, Yinqiong Cai, Yixing Fan, Fei Sun, Ruqing Zhang, and Xueqi Cheng.\n2022. Semantic models for the first-stage retrieval: A comprehensive review.\nACM Transactions on Information Systems (TOIS) 40, 4 (2022), 1â€“42.\n[10] Michael Gutmann and Aapo HyvÃ¤rinen. 2010. Noise-contrastive estimation: A\nnew estimation principle for unnormalized statistical models. In Proceedings of\nthe thirteenth international conference on artificial intelligence and statistics . JMLR\nWorkshop and Conference Proceedings, 297â€“304.\n[11] Xiangnan He, Kuan Deng, Xiang Wang, Yan Li, Yongdong Zhang, and Meng\nWang. 2020. Lightgcn: Simplifying and powering graph convolution network for\nrecommendation. In Proceedings of the 43rd International ACM SIGIR conference\non research and development in Information Retrieval . 639â€“648.\n[12] Sebastian HofstÃ¤tter, Sheng-Chieh Lin, Jheng-Hong Yang, Jimmy Lin, and Allan\nHanbury. 2021. Efficiently teaching an effective dense retriever with balanced\ntopic aware sampling. In Proceedings of the 44th International ACM SIGIR Confer-\nence on Research and Development in Information Retrieval . 113â€“122.\n[13] Yupeng Hou, Junjie Zhang, Zihan Lin, Hongyu Lu, Ruobing Xie, Julian McAuley,\nand Wayne Xin Zhao. 2024. Large Language Models are Zero-Shot Rankers for\nRecommender Systems. arXiv:2305.08845 [cs.IR]\n[14] Yue Huang, Jiawen Shi, Yuan Li, Chenrui Fan, Siyuan Wu, Qihui Zhang, Yixin\nLiu, Pan Zhou, Yao Wan, Neil Zhenqiang Gong, and Lichao Sun. 2023. MetaTool\nBenchmark: Deciding Whether to Use Tools and Which to Use. arXiv preprint\narXiv: 2310.03128 (2023).\n[15] Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bo-\njanowski, Armand Joulin, and Edouard Grave. 2021. Unsupervised dense in-\nformation retrieval with contrastive learning. arXiv preprint arXiv:2112.09118\n(2021).\n[16] Kalervo JÃ¤rvelin and Jaana KekÃ¤lÃ¤inen. 2002. Cumulated gain-based evaluation\nof IR techniques. ACM Transactions on Information Systems (TOIS) 20, 4 (2002),\n422â€“446.\n[17] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. 2019. Bert:\nPre-training of deep bidirectional transformers for language understanding. In\nProceedings of naacL-HLT , Vol. 1. 2.\n[18] Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu,\nZhoujun Li, Fei Huang, and Yongbin Li. 2023. API-Bank: A Comprehensive\nBenchmark for Tool-Augmented LLMs. In Proceedings of the 2023 Conference on\nEmpirical Methods in Natural Language Processing , Houda Bouamor, Juan Pino,\nand Kalika Bali (Eds.). Association for Computational Linguistics, Singapore,\n3102â€“3116. https://doi.org/10.18653/v1/2023.emnlp-main.187\n[19] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michi-\nhiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar,\nBenjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove,\nChristopher D. Manning, Christopher RÃ©, Diana Acosta-Navas, Drew A. Hudson,\nEric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu\nYao, Jue Wang, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul,\nMirac Suzgun, Nathan Kim, Neel Guha, Niladri Chatterji, Omar Khattab, Peter\nHenderson, Qian Huang, Ryan Chi, Sang Michael Xie, Shibani Santurkar, Surya\nGanguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary,\nWilliam Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. 2023.\nHolistic Evaluation of Language Models. arXiv:2211.09110 [cs.CL]\n[20] Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua,\nFabio Petroni, and Percy Liang. 2023. Lost in the Middle: How Language Models\nUse Long Contexts. arXiv:2307.03172 [cs.CL]\n[21] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang\nZhu. 2023. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment.\narXiv:2303.16634 [cs.CL]\n[22] Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp.\n2022. Fantastically Ordered Prompts and Where to Find Them: Overcoming\nFew-Shot Prompt Order Sensitivity. arXiv:2104.08786 [cs.CL]\n[23] Alex Mallen, Akari Asai, Victor Zhong, Rajarshi Das, Daniel Khashabi, and\nHannaneh Hajishirzi. 2022. When not to trust language models: Investigat-\ning effectiveness of parametric and non-parametric memories. arXiv preprint\narXiv:2212.10511 (2022).\n[24] GrÃ©goire Mialon, Roberto DessÃ¬, Maria Lomeli, Christoforos Nalmpantis, Ram\nPasunuru, Roberta Raileanu, Baptiste RoziÃ¨re, Timo Schick, Jane Dwivedi-Yu, Asli\nCelikyilmaz, et al. 2023. Augmented language models: a survey. arXiv preprint\narXiv:2302.07842 (2023).\n[25] Bhargavi Paranjape, Scott Lundberg, Sameer Singh, Hannaneh Hajishirzi, Luke\nZettlemoyer, and Marco Tulio Ribeiro. 2023. Art: Automatic multi-step reasoning\nand tool-use for large language models. arXiv preprint arXiv:2303.09014 (2023).\n[26] Aaron Parisi, Yao Zhao, and Noah Fiedel. 2022. Talm: Tool augmented language\nmodels. arXiv preprint arXiv:2205.12255 (2022).\n[27] Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni\nZeng, Yufei Huang, Chaojun Xiao, Chi Han, et al . 2023. Tool learning with\nfoundation models. arXiv preprint arXiv:2304.08354 (2023).\n[28] Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin\nCong, Xiangru Tang, Bill Qian, et al. 2023. Toolllm: Facilitating large language\nmodels to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789 (2023).\n[29] Changle Qu, Sunhao Dai, Xiaochi Wei, Hengyi Cai, Shuaiqiang Wang, Dawei\nYin, Jun Xu, and Ji-Rong Wen. 2024. Tool Learning with Large Language Models:\nA Survey. arXiv preprint arXiv:2405.17935 (2024).\n[30] Stephen Robertson, Hugo Zaragoza, et al . 2009. The probabilistic relevance\nframework: BM25 and beyond. Foundations and Trends Â® in Information Retrieval\n3, 4 (2009), 333â€“389.\n[31] Timo Schick, Jane Dwivedi-Yu, Roberto DessÃ¬, Roberta Raileanu, Maria Lomeli,\nLuke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. 2023. Toolformer: Lan-\nguage models can teach themselves to use tools. arXiv preprint arXiv:2302.04761\n(2023).\n[32] Yongliang Shen, Kaitao Song, Xu Tan, Dongsheng Li, Weiming Lu, and Yueting\nZhuang. 2024. Hugginggpt: Solving ai tasks with chatgpt and its friends in\nhugging face. Advances in Neural Information Processing Systems 36 (2024).\n[33] Yifan Song, Weimin Xiong, Dawei Zhu, Cheng Li, Ke Wang, Ye Tian, and Sujian\nLi. 2023. Restgpt: Connecting large language models with real-world applications\nvia restful apis. arXiv preprint arXiv:2306.06624 (2023).\n[34] Andrea Sottana, Bin Liang, Kai Zou, and Zheng Yuan. 2023. Evaluation Metrics\nin the Era of GPT-4: Reliably Evaluating Large Language Models on Sequence\nto Sequence Tasks. In Proceedings of the 2023 Conference on Empirical Methods\nin Natural Language Processing , Houda Bouamor, Juan Pino, and Kalika Bali\n(Eds.). Association for Computational Linguistics, Singapore, 8776â€“8788. https:\n//doi.org/10.18653/v1/2023.emnlp-main.543\n[35] Karen Sparck Jones. 1972. A statistical interpretation of term specificity and its\napplication in retrieval. Journal of documentation 28, 1 (1972), 11â€“21.\n[36] Weiwei Sun, Zheng Chen, Xinyu Ma, Lingyong Yan, Shuaiqiang Wang,\nPengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. 2023. Instruc-\ntion Distillation Makes Large Language Models Efficient Zero-shot Rankers.\narXiv:2311.01555 [cs.IR]\n[37] Jiakai Tang, Sunhao Dai, Zexu Sun, Xu Chen, Jun Xu, Wenhui Yu, Lantao Hu,\nPeng Jiang, and Han Li. 2024. Towards Robust Recommendation via Decision\nBoundary-aware Graph Contrastive Learning. Proceedings of the 30th ACM\nSIGKDD Conference on Knowledge Discovery and Data Mining (2024).\n[38] Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, and Le Sun.\n2023. ToolAlpaca: Generalized Tool Learning for Language Models with 3000\nSimulated Cases. arXiv preprint arXiv:2306.05301 (2023).\n[39] Raphael Tang, Xinyu Zhang, Xueguang Ma, Jimmy Lin, and Ferhan Ture. 2023.\nFound in the Middle: Permutation Self-Consistency Improves Listwise Ranking\nin Large Language Models. arXiv:2310.07712 [cs.CL]\n[40] Nandan Thakur, Nils Reimers, Andreas RÃ¼cklÃ©, Abhishek Srivastava, and Iryna\nGurevych. 2021. BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of\nInformation Retrieval Models. In Thirty-fifth Conference on Neural Information\nProcessing Systems Datasets and Benchmarks Track (Round 2) . https://openreview.\nnet/forum?id=wCu6T5xFjeJ\n[41] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yas-\nmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhos-\nale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv\npreprint arXiv:2307.09288 (2023).\n[42] Tu Vu, Mohit Iyyer, Xuezhi Wang, Noah Constant, Jerry Wei, Jason Wei, Chris\nTar, Yun-Hsuan Sung, Denny Zhou, Quoc Le, et al . 2023. Freshllms: Refresh-\ning large language models with search engine augmentation. arXiv preprint\narXiv:2310.03214 (2023).\nTowards Completeness-Oriented Tool Retrieval for\nLarge Language Models CIKM â€™24, October 21â€“25, 2024, Boise, ID, USA.\n[43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in neural information processing systems 35\n(2022), 24824â€“24837.\n[44] Minghao Wu and Alham Fikri Aji. 2023. Style Over Substance: Evaluation Biases\nfor Large Language Models. arXiv:2307.03025 [cs.CL]\n[45] Lee Xiong, Chenyan Xiong, Ye Li, Kwok-Fung Tang, Jialin Liu, Paul Bennett,\nJunaid Ahmed, and Arnold Overwijk. 2020. Approximate nearest neighbor nega-\ntive contrastive learning for dense text retrieval. arXiv preprint arXiv:2007.00808\n(2020).\n[46] Qiantong Xu, Fenglu Hong, Bo Li, Changran Hu, Zhengyu Chen, and Jian Zhang.\n2023. On the Tool Manipulation Capability of Open-source Large Language\nModels. arXiv preprint arXiv:2305.16504 (2023).\n[47] Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan,\nand Yuan Cao. 2022. React: Synergizing reasoning and acting in language models.\narXiv preprint arXiv:2210.03629 (2022).\n[48] Junjie Ye, Guanyu Li, Songyang Gao, Caishuang Huang, Yilong Wu, Sixian Li,\nXiaoran Fan, Shihan Dou, Qi Zhang, Tao Gui, et al. 2024. Tooleyes: Fine-grained\nevaluation for tool learning capabilities of large language models in real-world\nscenarios. arXiv preprint arXiv:2401.00741 (2024).\n[49] Siyu Yuan, Kaitao Song, Jiangjie Chen, Xu Tan, Yongliang Shen, Ren Kan, Dong-\nsheng Li, and Deqing Yang. 2024. EASYTOOL: Enhancing LLM-based Agents\nwith Concise Tool Instruction. arXiv preprint arXiv:2401.06201 (2024).\n[50] Wayne Xin Zhao, Jing Liu, Ruiyang Ren, and Ji-Rong Wen. 2023. Dense Text\nRetrieval based on Pretrained Language Models: A Survey. ACM Trans. Inf. Syst.\n(dec 2023).\n[51] Yuanhang Zheng, Peng Li, Wei Liu, Yang Liu, Jian Luan, and Bin Wang. 2024.\nToolRerank: Adaptive and Hierarchy-Aware Reranking for Tool Retrieval. In\nProceedings of the 2024 Joint International Conference on Computational Linguistics,\nLanguage Resources and Evaluation (LREC-COLING) (2024).\n[52] Mu Zhu. 2004. Recall, precision and average precision. Department of Statistics\nand Actuarial Science, University of Waterloo, Waterloo 2, 30 (2004), 6."
}