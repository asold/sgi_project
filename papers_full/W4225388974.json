{
  "title": "Building a Role Specified Open-Domain Dialogue System Leveraging Large-Scale Language Models",
  "url": "https://openalex.org/W4225388974",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2104936926",
      "name": "Sanghwan Bae",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2282773300",
      "name": "Donghyun Kwak",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2166198374",
      "name": "Sung-Dong Kim",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2108335479",
      "name": "Donghoon Ham",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2125414356",
      "name": "So-Young Kang",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2124155521",
      "name": "Sang-Woo Lee",
      "affiliations": [
        "Naver (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2548379891",
      "name": "Woomyoung Park",
      "affiliations": [
        "Naver (South Korea)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3035068109",
    "https://openalex.org/W2611049140",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2968297680",
    "https://openalex.org/W2095705004",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2973727699",
    "https://openalex.org/W3089263616",
    "https://openalex.org/W2963612262",
    "https://openalex.org/W3173658292",
    "https://openalex.org/W2885421725",
    "https://openalex.org/W2963491014",
    "https://openalex.org/W2779809129",
    "https://openalex.org/W3035629539",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3155584966",
    "https://openalex.org/W3166143260",
    "https://openalex.org/W3100355250",
    "https://openalex.org/W3156054180",
    "https://openalex.org/W582134693",
    "https://openalex.org/W2938704169",
    "https://openalex.org/W3035082574",
    "https://openalex.org/W2995289474",
    "https://openalex.org/W4287900772",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3034284249",
    "https://openalex.org/W4308948956",
    "https://openalex.org/W2895976713",
    "https://openalex.org/W4308900200",
    "https://openalex.org/W2964006684",
    "https://openalex.org/W2806254326",
    "https://openalex.org/W2761590056",
    "https://openalex.org/W2963825865",
    "https://openalex.org/W1564294242",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2949769095",
    "https://openalex.org/W4287553002",
    "https://openalex.org/W1993567041",
    "https://openalex.org/W2806600904",
    "https://openalex.org/W3116216579",
    "https://openalex.org/W4287887982",
    "https://openalex.org/W136732505",
    "https://openalex.org/W2062175565",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W2251235149",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3171850892",
    "https://openalex.org/W2251058040",
    "https://openalex.org/W4229506649"
  ],
  "abstract": "Sanghwan Bae, Donghyun Kwak, Sungdong Kim, Donghoon Ham, Soyoung Kang, Sang-Woo Lee, Woomyoung Park. Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies. 2022.",
  "full_text": "Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2128 - 2150\nJuly 10-15, 2022 ©2022 Association for Computational Linguistics\nBuilding a Role Speciﬁed Open-Domain Dialogue System Leveraging\nLarge-Scale Language Models\nSanghwan Bae1 Donghyun Kwak1 Sungdong Kim2 Donghoon Ham1\nSoyoung Kang1 Sang-Woo Lee1,2 Woomyoung Park1\nNA VER CLOV A1 NA VER AI Lab2\n{sanghwan.bae, donghyun.kwak, sungdong.kim, donghoon.ham,\nsy.kang, sang.woo.lee, max.park}@navercorp.com\nAbstract\nRecent open-domain dialogue models have\nbrought numerous breakthroughs. However,\nbuilding a chat system is not scalable since\nit often requires a considerable volume of\nhuman-human dialogue data, especially when\nenforcing features such as persona, style, or\nsafety. In this work, we study the challenge\nof imposing roles on open-domain dialogue\nsystems, with the goal of making the sys-\ntems maintain consistent roles while convers-\ning naturally with humans. To accomplish\nthis, the system must satisfy a role speci-\nﬁcation that includes certain conditions on\nthe stated features as well as a system pol-\nicy on whether or not certain types of utter-\nances are allowed. For this, we propose an\nefﬁcient data collection framework leveraging\nin-context few-shot learning of large-scale lan-\nguage models for building role-satisfying dia-\nlogue dataset from scratch. We then compare\nvarious architectures for open-domain dia-\nlogue systems in terms of meeting role speciﬁ-\ncations while maintaining conversational abil-\nities. Automatic and human evaluations show\nthat our models return few out-of-bounds ut-\nterances, keeping competitive performance on\ngeneral metrics. We release a Korean dialogue\ndataset we built for further research1.\n1 Introduction\nRecent large-scale language models (LMs) have\nbrought numerous breakthroughs in open-domain\ndialogue systems, yielding human-like responses\n(Zhang et al., 2020; Adiwardana et al., 2020;\nBrown et al., 2020; Roller et al., 2021; Kim et al.,\n2021a). In addition, there have been progresses in\ncontrolling dialogue systems in persona, style, and\nsafety (Zhang et al., 2018; Smith et al., 2020; Xu\net al., 2021), which impose consistency on chat-\nbot’s personality and mitigate undesirable features\n1The dataset is available at https://github.com/\nnaver-ai/carecall-corpus\nFigure 1: An example of a chatbot system that cares\nfor senior citizens living alone. The utterance in red\nhighlights the model’s mistaken identity as a chef rather\nthan the caring chatbot.\nsuch as toxic or biased language. However, build-\ning a chatbot system combining these capabilities is\nstill challenging, which requires numerous human-\nhuman dialogues for those conversational skills.\nMost task-oriented dialogue systems conduct\nspeciﬁc roles such as booking assistants, infor-\nmation providers, customer service agents, or per-\nsonal assistants (Eric et al., 2017; Xu et al., 2017;\nBudzianowski et al., 2018). However, studies on\nopen-domain dialogue systems that perform spe-\nciﬁc roles have been insufﬁciently investigated,\neven though the role can be deﬁned for the practical\nchatbot systems (e.g., chatbots that care for senior\ncitizens living alone, or counseling chatbots). In\nthese cases, the chatbot systems do not have an ex-\nplicit goal or task other than to proactively engage\nin conversations, but may have system policies on\nwhether or not certain types of utterances are al-\nlowed (example in Figure 1).\nTo address these issues, we study methods for\nRole Speciﬁed Open-Domain Dialogue (RSODD)\nsystems. The goal of the system is conversing nat-\nurally with humans on open-ended topics while\nkeeping conditions of given role. Certain condi-\ntions in persona, style, safety, and system policy\nmust be satisﬁed in order to achieve the goal. We\n2128\nconsider a general and scalable framework to treat\nthem, instead of using individual approaches to\ncontrol each.\nIn particular, we present a Human-AI collabora-\ntive data construction method to build a scalable\nsupervisory dataset from scratch for role-satisfying\nopen-domain dialogues (Figure 2). We propose to\nleverage large-scale LMs for generating entire di-\nalogue sessions between user and system by in-\ncontext few-shot learning manner (Brown et al.,\n2020; Kim et al., 2021a), followed by human-\ninteractive correction processes. Our method can\nsigniﬁcantly reduce the cost of building dataset\nwhen compared to manually producing gold dia-\nlogues (Section 3.2). We compare several architec-\ntures for modeling role-satisfying chatbot systems\nin the synthetic dataset. In extensive experiments\nand ablation studies, we show that the proposed\nmodels considerably reduce undesirable utterances\nthat violate the given role speciﬁcation compared\nto the in-context learning baseline, while achieving\ncompetitive SSA (Adiwardana et al., 2020) scores\nfor their responses. We release the Korean dialogue\ndataset we built to validate our framework, which\nis expected to provide more insights into the capa-\nbilities of the proposed methods and to contribute\nto the public Korean dialogue datasets.\nThe contribution of our work is summarized as\nfollows.\n1. We make a step towards role speciﬁed open-\ndomain dialogue (RSODD) systems which\nare capable of conversing naturally on open-\nended topics while satisfying role speciﬁca-\ntions.\n2. We suggest employing in-context learning of\nlarge-scale LMs as a scalable method for dia-\nlogue data construction.\n3. We compare various architectures for RSODD\nsystems to analyze the capabilities in terms of\nsatisfying system policies.\n4. We release the ﬁrst Korean RSODD dataset\nwhile demonstrating the effectiveness of data\nconstruction method.\n2 Related Work\nPretrained LM in Open-domain dialogue\nMany prior works tried to pretrain the models on\nlarge-scale social comment chains data like Red-\ndit to model conversational behavior (Zhang et al.,\nFigure 2: Our proposed framwork: (1) the dialogue\ndeveloper provides a role speciﬁcation of the desired\nchatbot and a few dialogue examples, (2) large-scale\nLMs generate entire dialogues and crowd workers ﬁl-\nter the system’s utterances, (3) a dialogue model is\ntrained with supervised learning on the dataset, (4)\ncrowd workers chat 1:1 with the chatbot and give ad-\nditional feedback.\n2020; Adiwardana et al., 2020), followed by ﬁne-\ntuning on the diverse target dialogue dataset to im-\nprove engagingness and humanness (Roller et al.,\n2021). To avoid undesired behaviors of the models\nincluding toxicity and bias from the human-human\nconversation, they merely exclude some parts of\ntraining data using automatic ﬁltering by prede-\nﬁned criteria.\nSynthetic Dialogue Generation To reduce cost\nof dialogue collection, there have been many ap-\nproaches to generate synthetic dialogues (Schatz-\nmann et al., 2007; Shah et al., 2018; Campagna\net al., 2020). They usually deﬁne task schema, rules\nand templates to simulate certain scenarios in the\ntask-oriented dialogue (TOD). Kim et al. (2021b)\nproposed neural simulation approach using pre-\n2129\ntrained LMs for a fast domain adaptation in the\nTOD. However, they need training data of source\ndomain to transfer to an unseen target domain.\nXu et al. (2021) proposed Bot-Adversarial Di-\nalogue method to make existing models safer in\nterms of offensive or toxic behavior. Sun et al.\n(2021) extends existing task-oriented dialogue\ndataset to open-domain chit-chat using the pre-\ntrained LMs. Both of the works actively utilize\nlarge-scale pretrained LMs to build dialogue corpus\nwith human supports. We also introduce human-AI\ncollaborative dialogue collection method, while es-\npecially utilizing few-shot in-context learning abil-\nity of large-scale LM (Brown et al., 2020; Kim\net al., 2021a). To the best of our knowledge, this\nwork is the ﬁrst to propose using the in-context\nlearning approach to generate synthetic samples\nfrom large-scale language models for the purpose\nof dialogue data generation.\nOn the Role in Dialogue In TOD, the system\nside plays functional roles utilizing explicit knowl-\nedge base of speciﬁc domain (Williams et al., 2013;\nHenderson et al., 2014a,b; Eric et al., 2017; Xu\net al., 2017; Budzianowski et al., 2018). For ex-\nample, agent in Budzianowski et al. (2018) played\nbooking assistant or information provider in var-\nious domain such as restaurant and hotel. On the\nother hand, Zhang et al. (2018) proposed assigning\nexplicit persona to each dialogue agent, promot-\ning the agent to make more speciﬁc and consistent\nresponses in open-domain dialogue setting. How-\never, the persona given by a few natural language\nsentences is insufﬁcient to represent speciﬁc role\nin the real world scenario. Sun et al. (2021) also\nproposed guidelines of appropriate and inappropri-\nate behaviors as a role of virtual assistant. We note\nthat a recent concurrent work (Shuster et al., 2021)\nstudied conditioning dialogue models with similar\nmotivations. We explore more into how to ﬁx the\nchatbot’s role to meet speciﬁc system policies in\ndiverse conversational interactions.\nCompanion Dialogue System Building com-\npanionable dialogue system has long been investi-\ngated along with the advancement of open-domain\ndialogue models. Webb et al. (2010) deﬁnes com-\npanions to be persistent, collaborative and conver-\nsational partners, and proposes evaluation strate-\ngies: empathy, positivity, and adaptive. Kopp et al.\n(2018) introduced conversational assistants for el-\nderly users which carry out socially cooperative di-\nFigure 3: An example of in-context one-shot dialogue\ngeneration for the data construction process. (a) The\noutline of the chatbot is ﬁxed for all generation and the\nexample dialogue is sampled for each generation from\ndialogues written by human. (b) The utterances in blue\nare positive examples, and the one in red is a negative\nexample for training dialogue agents.\nalogue. However role consistency of such compan-\nionable dialogue systems are not studied enough.\n3 Data Construction\nIn this section, we describe a framework to gather\nsupervisory data for building RSODD systems. The\ninput to the framework is a role speciﬁcation de-\nscribed by the chatbot developer (Table 1 for ex-\nample), which deﬁnes the conditions in the dia-\nlogue interactions for the system. We assume a\npre-existing dataset that properly meets the speciﬁ-\ncation isn’t available. It is also infeasible to write\nenough dialogue examples manually to train the\nsystem because the scope of dialogue is very broad\nand diverse due to the nature of open-domain dia-\nlogues. To remedy this, we focus on composing the\ndataset with a few samples of human-written dia-\nlogues using in-context few-shot learning of large-\nscale LMs (Brown et al., 2020; Liu et al., 2021).\n3.1 One-shot Dialogue Generation\nAs reported in Kim et al. (2021a), large-scale LMs\ncan generate dialogues with a speciﬁc personality,\ngiven a prompt consisting of a brief description\nof the chatbot’s properties and few dialogue exam-\nples. We use this method to build the entire dataset.\nFirst, we write a few dialogue examples that sat-\nisfy the role speciﬁcation. And we attach each of\nthem at the end of the system description (Outline\n2130\nOutlineThe chatbot is an artiﬁcial intelligence agent that regularly calls and converses with senior citizens.Initiate the conversation and react friendly to the user’s utterances.Talk about everyday topics for 10-15 turns and end the call.\nDetailsCategories SpeciﬁcationSensibleness DescriptionSpeech that does not properly understand the context is restricted.Style DescriptionSpeech should be polite∗and respectful.Safety DescriptionHate speech, toxic or biased language, and remarks containing personally identiﬁable information are all prohibited.\nPersona DescriptionKeep the identity of an ‘AI chatbot that calls to the user.’Because it assumes a phone call, utterances that appear to be in the same room as the user are limited.Since there is no physical entity, statements implying a meeting, such as ‘Let’s do it together’ and ‘I’ll do it for you,’ are restricted.\nExamples\"Grandma! I’m here!\" (X)\"Would you like to walk with me?\" (X)\"I’ll invite you to my house later\" (X)\nSystem PolicyTemporality DescriptionBecause it is not given time-related information, the chatbot is unable to offer a timely utterance.Chatbots are not allowed to speak ﬁrst about the current weather, date, or news.However, if the user brings up the subject ﬁrst, it is feasible to agree.\nExamples\"Because the weather is turning cold these days, you should dress warmly.\" (X)\"Happy Holidays!\" (X)\"Did you watch the baseball championship game today?\" (X)\nUnsupported FeaturesDescriptionIt does not provide any other functions other than making phone calls and chatting.It does not play a song, provide current weather information, or make a phone call to someone else.\nExamples\"I’ll play a song.\" (X)\"Today’s weather is sunny, with a low of 12 degrees and a high of 21 degrees Celcius.\" (X)\"Then I’ll call your daughter.\" (X)\n∗There are polite words and honoriﬁcs in the Korean language.\nTable 1: Example role speciﬁcation used. In experiments, we use it as criteria to guide seed dialogue examples\ncreation for the one-shot dialogue generation, ﬁlter the generated dialogues, and evaluate the ﬁnal system. All the\ntexts are translated into English and some sorts of them are simpliﬁed or omitted for better understanding.\nin Table 1) to compose input prompts for one-shot\nin-context learning. Figure 3 (a) shows an example\ninput. Then, the LM generates whole dialogue ses-\nsions. That is, the LM acts as both a system and a\nuser (Figure 3 (b)). Only the generated dialogues\nare included in the dataset without input prompts.\n3.2 Human Filtering\nIt is difﬁcult to include all the details of speciﬁca-\ntions in the prompt and reﬂect them in the genera-\ntion. Therefore, we employ human annotation on\nthe generated data. We give the annotator each con-\nversation session and ask them to label the point\nwhere the ﬁrst out-of-bounds2 occurred. Figure 3\n(b) shows an example of a veriﬁed dialogue (more\nexamples are provided in Appendix H). We use the\nturns just before the utterance annotated to be prob-\nlematic as positive examples, and use the annotated\nturn as a negative example. The following turns\nare not used, because the context may be already\ndamaged by the problematic utterance. Annotation\ntime per dialogue session is about 88s, which is\n13.3 times faster than human writing time per ses-\nsion (about 1170s). The percentage of remaining\nutterances after the ﬁltering phase is 30.4% (See\nTable 2).\n2An utterance that does not meet the conditions of the\ngiven role speciﬁcation (Table 1 for example).\n3.3 Collecting Human-Bot Dialogues\nAlthough human ﬁltering is included in the dataset\nbuilding process, the actual utterances are all\nmachine-generated. Whereas, the system trained on\nthem engages in conversations with human users in\nthe deployment phase. To mitigate this discrepancy,\nwe employ a human-in-the-loop phase to collect\npatterns of human-bot dialogues. Annotators have\nturn-by-turn conversations as users with the system,\nwhile correcting out-of-bounds utterances from the\nsystem. We incorporated LM’s assistance into this\nprocess to help speed the task; if the system’s re-\nsponse is not appropriate, an annotator presses the\n‘Fix’ button (Figure 6 in Appendix showing the user\ninterface) to call the large-scale LM to generate an\nalternative utterance. The worker continues the con-\nversation if the alternate utterance is appropriate,\nand if it is still not corrected, presses the ‘Fix’ but-\nton repeatedly. The corrected dialogue is used to\ncompose positive examples, and the utterance when\nthe button is pressed is used as a negative example.\nThis procedure enriches the dataset by producing\nadditional positive and negative examples in sce-\nnarios similar to real-time conversations.\nIn addition, we propose this process as an eval-\nuation metric for the system. Since the action of\npressing the ‘Fix’ button means that an inappro-\npriate utterance is returned from the system, it can\nbe used for the system’serror rate; the rate of the\ncorrected responses among the total returned re-\nsponses. This metric is intuitive and does not incur\n2131\nadditional costs because it is performed concur-\nrently with the data collection process described\nabove.\n4 Models\n4.1 Notation\nResponse prediction task in open-domain\ndialogues is predicting an utterance\ny = {y1,y2,··· ,y|y|} given a dialogue his-\ntory x = {s1,u1,s2,u2,··· ,sk,uk}, where si\nand ui are system utterance and user utterance\nrespectively.\n4.2 Out-of-Bounds Detection\nThe most straightforward method for constraining\nthe system’s utterances according to the role speci-\nﬁcation is to detect and discard out-of-bounds ut-\nterances. We consider a BERT-based (Devlin et al.,\n2019) binary classiﬁer ﬁne-tuned to classify posi-\ntive/negative examples in datasets. Since the clas-\nsiﬁer cannot perform a conversation by itself, we\nassume a two-stage model; a response prediction\nmodel returns responses, which are censored by the\nclassiﬁer. If an out-of-bounds utterance is detected,\nwe select and return one of several pre-deﬁned ques-\ntions about other topics, similar to the method used\nin Xu et al. (2021). Instead of random choice, we\nselected the question with lowest PPL measured\nusing LMs, as depicted in Section 4.3.\n4.3 Response Selection\nAnother conceivable approach to constrain the sys-\ntem’s utterances is to pre-ﬁlter the response candi-\ndates for response selection models. We employ a\n2-step approach for the response selection model,\nretrieve-and-rerank. The retriever of poly-encoder\narchitecture (Humeau et al., 2020) rapidly ﬁnds\nthe top-k plausible responses from the response\ncandidates, which are then carefully reranked by\nthe reranker of cross-encoder architecture. Both re-\ntriever and reranker are ﬁne-tuned in the same way\nas Humeau et al. (2020) depicts.\nSince the response candidates are limited by ﬁl-\ntering, it is important to predict the context which\ncannot be answered with response candidates in\norder to avoid non-sensible responses. One of the\neffective methods to predict unanswerable contexts\nis to utilize the uncertainty of the model (Feng\net al., 2020; Penha and Hauff, 2021). Penha and\nHauff (2021) proposed a risk-aware score using\nMC Dropout (Gal and Ghahramani, 2016) and\nFigure 4: Retrieve-fail-Generate pipeline.\nwe employ a similar approach using thresholding;\nwe score the retrieved responses using mean and\nvariance of the predictive distribution from MC\nDropout:\nSD(x,ˆy) =E[Rˆy] −var[Rˆy],\nwhere ˆy is a candidate response that is retrieved,\nRˆy = {f(x,ˆy1),f(x,ˆy2),···f(x,ˆym)}is a pre-\ndictive distribution obtained by employing dropout\n(Srivastava et al., 2014) at test time and conduct-\ning m forward passes, and f is a score function\nof reranker. If all the scores of retrieved responses\nare lower than a certain threshold, it is predicted as\nunanswerable context.\nWe also consider another approach using per-\nplexity (PPL) of large-scale LMs. We concatenate\nthe dialogue context and the retrieved response to\nmake an input to LM and measure the PPL of the re-\nsponse. Thresholding is employed for ﬁnal decision\nand the threshold is determined on the validation\nset (See Appendix C).\n4.4 Response Generation\nFine-tuning LMs on target data is known to be ef-\nfective in learning desirable traits of focused tasks\n(Roller et al., 2021; Gehman et al., 2020). There-\nfore, we consider ﬁne-tuned LMs as response gen-\neration model using maximum likelihood estima-\ntion (MLE). On the other hand, unlikelihood (UL)\ntraining is known to be effective in mitigating un-\ndesirable features (e.g., token repetition or logical\ninconsistency) of generative models (Li et al., 2020;\nWelleck et al., 2020). We found that this can be gen-\neralized further and applied to the diverse attributes\nto be constrained. That is, the MLE is applied to\nthe positive examples in the dataset in order to\nencourage the system to generate utterances with\ndesirable features, while the UL training is applied\nto the negative examples in order to discourage the\nsystem from generating utterances with undesir-\nable features. Both types of training are performed\nconcurrently.\n2132\nFormally, we ﬁne-tune LMs as generative mod-\nels using maximum likelihood estimation (MLE),\nwhich minimizes:\nLn\nMLE(pθ,xn,yn) =−\n∑\nt\nlog pθ(yn\nt |xn,yn\n<t),\nwhere xn is a dialogue history in positive examples\nand yn is a corresponding gold response. Unlikeli-\nhood training is done by adding a loss that penalizes\nthe token set Ct to be constrained,\nLn\nUL(pθ,C1:T,x,y ) =\n−\n∑\nt\n∑\nyc∈Ct\nlog (1−pθ(yc|x,y<t)),\nwhere Ct ⊆V is a subset of the vocabulary. We\nemploy this to the negative examples in dataset\n{(x−,y−)}. For this, Ct is deﬁned as {y−\nt }, which\nresults in the following:\nL−\nUL(pθ,x−,y−) =\n−\n∑\nt\nlog (1−pθ(y−\nt |x,y−\n<t)).\nThe ﬁnal loss function consists of mixing MLE loss\nand UL loss,\nL= L+\nMLE + αL−\nUL, (1)\nwhere α∈R is the mixing hyper-parameter.\n4.5 Retrieve-fail-Generate\nWe also consider a pipelined approach that consists\nof response selection and generation models. We\nﬁrst tried a Retrieve-and-Reﬁne architecture (Roller\net al., 2021; Weston et al., 2018), but it failed in\nα-blending3. In addition, according to Roller et al.\n(2021), the Retrieve-and-Reﬁne strategy delivers\nmarginal or no improvements over the generator.\nTherefore, we build another pipeline, refered to\nas a Retrieve-fail-Generate model (Figure 4). In\nthis pipeline, the response selection model tries\nto select appropriate responses. If the model for\npredicting unanswerable contexts dismisses the se-\nlected ones, the response generation model returns\na response for the given context. It is relatively easy\nto control response selection models by managing\nthe response candidates. Hence, the response se-\nlection models are responsible for majority of the\nresponses, and the generation model is only used\nwhen the response selection fails.\n3In our experiments, all retrieved responses are copied\nor ignored depending on the α value, reducing the model to\na retriever or generator. This has also been highlighted in a\nrecent concurrent study (Han et al., 2021).\nDialogue Type Example Generated Filtered Feedback\n# Dialogues 250 25,000 17,617 1,623\n# Turns 3,893 510,028 154,903 29,365\nAvg. turns / dialogue15.57 20.40 8.79 18.09\n# Pos. examples - - 47,091 10,829\n# Neg. examples - - 18,583 3,529\n# Unique sys-turns1,805 170,527 36,227 9,405\n# Words 35,253 4,292,613 705,253 178,357\nAvg. words / turn 9.06 8.42 4.55 6.07\n# Unique words 11,341 187,018 48,910 32,477\n# Unique bigrams23,507 893,041 176,834 86,335\nDistinct-1 0.3215 0.0436 0.0694 0.1821\nDistinct-2 0.7907 0.2538 0.3067 0.5795\nTable 2: Statistics of dataset collected in Section 5.1.\nExample is a human-written dialogue set for in-context\nlearning. Generated is a generated set by LMs (Section\n3.1). Filtered is a set after human ﬁltering phase (Sec-\ntion 3.2). Feedback is human-bot dialogues with cor-\nrections (Section 3.3). The positive and negative exam-\nples are pairs of (dialogue history, response). Distinct-\n1/2 (Li et al., 2016) is the number of distinct uni- or\nbi-grams divided by total number of words.\n5 Experiments\nWe detail experimental settings and results in this\nsection, including evaluations of the data collected\nby in-context few-shot learning (Section 5.2), com-\nparisons of model variants (Section 5.3), and evalu-\nations on system’s response qualities (Section 5.4).\n5.1 Dataset\nWe built a Korean dialogue dataset for a chatbot\nsystem to have casual conversations on a regular ba-\nsis with senior citizens who live alone. This dataset\nwas collected using the framework described in\nSection 3, assuming a role speciﬁcation in Table 1.\n250 dialogue examples with 89 topics (more details\nare in Appendix D) were used for in-context 1-shot\ngeneration. We used 39B size of HyperCLOV A\n(Kim et al., 2021a) as generation model (sampling\nat temperature 0.5 using nucleus sampling (Holtz-\nman et al., 2020) with P = 0.8). Table 2 shows\nthe statistics of the dataset (additional analysis in\nAppendix E). We use 5% of each for validation\nsets.\n5.2 Evaluation on Generated Dialogues\nWe ﬁrst assess the quality of the generated dia-\nlogues to verify the dialogue generating method\ndescribed in Section 3.1. Using four different sizes\nof HyperCLOV A, we generate 100 dialogue ses-\nsions for each with the same prompt. We ask the\ncrowd workers to rate on a scale of 1 to 5 whether\nthe generated dialogue satisﬁes several conditions\n2133\nAutomatic Metrics Human Evaluations\nUser System\nModel Distinct-1 Distinct-2 Fluency CoherenceSituation Persona Persona Style Safety\n1.3B 0.2959 (0.0042) 0.6630 (0.0053)4.98 (0.02) 4.54 (0.21)4.57 (0.29) 4.54 (0.15) 4.31 (0.23) 4.91 (0.05) 4.98 (0.03)\n13B 0.3075 (0.0037) 0.6500 (0.0054)4.97 (0.02) 4.55 (0.14)4.74 (0.23) 4.65 (0.11) 4.33 (0.20) 4.93 (0.04) 4.98 (0.02)\n39B 0.3334 (0.0038) 0.6779 (0.0061)4.98 (0.03)4.59(0.19) 4.69 (0.22) 4.69 (0.12) 4.37 (0.21) 4.88 (0.05) 4.97 (0.02)\n82B 0.3402(0.0040)0.7014(0.0057)4.98 (0.02) 4.56 (0.24)4.78(0.17)4.74(0.15)4.49(0.17)4.96(0.07) 4.96 (0.03)\nTable 3: Automated metric and human evaluations for generated dialogues from various size of LMs. Scores are\naveraged (standard deviation in brackets).\nModel # of system turns error rate not sensible wrong persona policy violation not safe etc.(%) (%) (%) (%) (%) (%)\nOut-of-Bounds DetectionGenerator (IC) + Classiﬁer 1,471 18.10 9.31 1.61 2.49 0.07 4.66Response SelectionRetrieve-and-Rerank 1,230 13.17 10.68 0.72 1.53 0.00 0.24Retrieve-and-Rerank w/ MC Dropout 1,272 9.82 7.58 0.36 1.66 0.00 0.22Retrieve-and-Rerank w/ PPL 1,300 7.00 5.10 0.40 1.16 0.00 0.34Response GenerationGenerator (IC) 985 35.83 16.05 6.24 8.66 0.17 4.68Generator (MLE) 1,291 4.72 3.55 0.76 0.30 0.00 0.10Generator (UL) 1,497 3.82 3.29 0.23 0.10 0.00 0.17Retrieve-fail-GenerateRetrieve-and-Rerank w/ PPL + Generator (UL) 1,522 2.56 2.20 0.17 0.16 0.00 0.00Retrieve-and-Rerank w/ PPL + Generator (UL) + Feedback Data 1,5992.00 1.88 0.00 0.10 0.00 0.00\nTable 4: Human evaluation results. As described in Section 3.3, the crowd workers chat 1:1 with a chatbot as\nusers and correct the inappropriate responses. The error rate is the proportion of corrected responses among all the\nsystem’s responses. The workers additionally annotate what kind of error occurs based on the role speciﬁcation.\nexpected to be controlled through in-context learn-\ning (the detailed description of the evaluation cri-\nteria is provided in Appendix F). The results are\nshown in Table 3. It shows that the larger the model\nsize, the better to meet the conditions by in-context\nlearning, which is also shown in previous studies\n(Brown et al., 2020; Kim et al., 2021a). In addition,\nDistinct-1/2 (Li et al., 2016) indicates that the text\ngenerated by large models is more diverse.\n5.3 Model Comparison\nOut-of-Bounds Detection Table 5 shows the\nclassiﬁcation accuracy and F1 score of the trained\nclassiﬁer. We use generator controlled by in-\ncontext learning (IC) as a response prediction\nmodel to evaluate the effect of the classiﬁer alone.\nFor in-context learning, we use the same prompt\nused to generate the dataset, but the model only gen-\nerates system’s utterances in its turns. The classi-\nﬁer signiﬁcantly lowers the error rate of in-context\nlearning (Table 4), showing the effectiveness of\nthe classiﬁer. On the other hand, the error rate is\nrelatively higher than those of the best models of\nresponse selection and generation. This is because\nthe classiﬁer is not perfect (about 92% in accuracy),\nand even when it properly detects out-of-bounds,\nthe pre-deﬁned questions as alternatives are occa-\nTraining Data (%) Mean Accuracy% (std) Mean F1% (std)\n10 87.31 (0.0164) 88.44 (0.0163)\n20 89.73 (0.0061) 90.47 (0.0055)\n100 91.99 (0.0022) 92.55 (0.0019)\nTable 5: Classiﬁer results, reporting accuracy and F1 on\ntest set. It shows performance in relation to the amount\nof training data used.\nModel data # of examples Hits@1/20 Hits@1/100\nRetrieverFiltered 47,091 93.14 83.80\nUnﬁltered 227,638 95.27 86.99\nRerankerFiltered 47,091 97.16 90.89\nUnﬁltered 227,638 97.55 91.70\nTable 6: Hits@1/Kof retriever and reranker on the val-\nidation set. Hits@1/ K measures recall@1 when rank-\ning the gold label among a set of K−1 other random\ncandidates.\nsionally incoherent with the contexts.\nResponse Selection We ﬁne-tune the response\nselection models on positive examples of the ﬁl-\ntered data and automatically evaluate them by mea-\nsuring Hits@1/K(Roller et al., 2021) on the valida-\ntion set. Results are shown in Table 6. We addition-\nally found that training on unﬁltered datasets brings\nimprovements to the Hits@1/Kperformance itself.\nTherefore, we use the models that trained on un-\n2134\nResponse Selection Response Generation\nproportion error rate proportion error rate\nModel (%) (%) (%) (%)\nRetrieve-and-Rerank w/ PPL + Generator (UL) 68.20 2.50 31.80 2.68\nRetrieve-and-Rerank w/ PPL + Generator (UL) + Feedback Data 63.70 2.12 36.30 1.77\nTable 7: Evaluation results of each component in the Retrieve-fail-Generate pipeline. It shows the proportion and\nerror rate of returned responses from response selection and generation models.\nMethod positive negative\nIn-context Learning 2.65 2.74\nLikelihood Training 2.07 2.47\nUnlikelihood Training 2.48 46.70\nTable 8: Perplexity (PPL) of generative models on vali-\ndation set of ﬁltered data.\nﬁltered dataset in the subsequent experiments. Re-\nsponse candidates are limited to system responses\nwithin positive examples (unique system’s turns of\nﬁltered data in Table 2). And we also validate the\nproposed methods for predicting unanswerable con-\ntexts, and determine the thresholds for each (further\ndetails are given in Appendix C).\nTable 4 shows the error rate of the response se-\nlection models. The model that does not predict\nunanswerable contexts (Retrieve-and-Rerank) has\na higher error rate in ‘not sensible’ than others. The\ncase of using PPL as the method for predicting\nunanswerable contexts shows a lower overall error\nrate than the case of using MC Dropout, and the\nproportions of the total contexts predicted as unan-\nswerable are similar at 4.23% and 3.85% for PPL\nand MC Dropout, respectively. The results also\nshow the error types from the models. Although\nonly the ﬁltered utterances are used as response\ncandidates, ‘wrong persona’ and ‘policy violation’\nappear in responses. It seems that a few unﬁltered\nutterances remain in the response candidates, since\nthe human ﬁltering is not perfect. Or even the same\nutterance can cause errors depending on the con-\ntext. For example, it is possible to agree with when\na user calls the system by a different name.\nResponse Generation We compare three ways\nto train generators; in-context learning (IC), like-\nlihood training (MLE), and unlikelihood training\n(UL). We measure the perplexity of the three mod-\nels on positive and negative examples and Table 8\nshows the results. The difference between the PPL\nof the positive examples and the negative examples\nis the smallest in in-context learning. When trained\non positive examples with likelihood training, the\ndifference increases slightly, because the PPL of\nthe positive examples is lowered. When adding un-\nlikelihood training, the PPL for negative examples\nincrease signiﬁcantly, 4 which mean the model is\nless likely to generate out-of-bounds utterances.\nTable 4 shows the error rate of each model. Com-\npared with in-context learning, likelihood training\nwith the ﬁltered dataset can reduce the error rate\nsigniﬁcantly. Additionally, if unlikelihood training\nis employed, the error rate is further reduced. A\nsimilar trend can be found in all types of errors.\nRetrieve-fail-Generate We also experiment\nwith a Retrieve-fail-Generate model consisting\nof the best conﬁgurations for response selection\n(PPL) and generation (UL) models. Since the error\nrate of the response selection model is relatively\nhigher than that of the generation model, the\nthreshold for predicting unanswerable contexts is\nset strictly to lower the error rate of the response\nselection model. Table 7 shows the error rates\nof responses returned from response selection\nand generation models, respectively. The results\nindicate that both error rates are lower when the\nmodels are included in a pipeline than when they\nare used separately, and the overall error rate\ndecreases accordingly. The response selection\nmodel returns the responses within the candidates\nextracted from the positive examples of the trainset,\nso that the ﬂow of the conversation is not dispersed\nand tends to be similar to the trainset. As a result,\nthe Retrieve-fail-Generate model shows the lowest\nerror rate among all models (Table 4).\nFeedback Pipeline The best model is further\ntrained on the human-bot dialogues collected dur-\ning the model evaluation process, as depicted in\nSection 3.3. Both response selection and genera-\ntion models are newly initialized and trained. As\n4Li et al. (2020) has also found a large gap in PPL scores\nbetween positives and negatives.\n2135\nMethod Sensibleness Speciﬁcity SSA\nHuman 95.48 82.96 89.22Retrieve-fail-Generate + Feedback Data 94.00 77.50 85.75\nTable 9: Interactive SSA results.\na result, all types of error rates are consistently\nreduced (Table 4), and the error rates of both the\nresponse selection and generation models are de-\ncreased (Table 7). The effect is stronger on the\nresponse generation.\n5.4 Response Quality\nTo assess the overall response quality of the pro-\nposed chatbot system, we use the sensibleness\nand speciﬁcity average (SSA) metric (Adiwardana\net al., 2020), which is shown to have a strong corre-\nlation with asking raters how humanlike the model\nis. This metric is a average of two scores: sen-\nsibleness and speciﬁcity. Sensibleness measures\nwhether a model’s responses make sense in con-\ntext and do not contradict anything that was said\nearlier, while speciﬁcity measures whether a re-\nsponse is speciﬁc to a given context. However, ex-\nact comparison with the scores in Adiwardana et al.\n(2020) is difﬁcult, because of the static role of our\nchatbot system and language discrepency in phras-\ning of questions. Therefore, We re-estimate hu-\nman interactive SSA in our experiments. To collect\nhuman-human conversations, we transcribe 100\ncall speeches between users and workers who play\nsystem’s role. And we collect 100 human-bot con-\nversations by allowing the crowd workers to chat\nwith the system. Labeling was conducted by inde-\npendent crowd workers with majority voting of 5\nworkers per turn.\nThe results are given in Table 9. It shows that the\nproposed system is competitive with human in sen-\nsibleness. And the majority of the responses from\nthe system are labeled as speciﬁc, which allows us\nto conclude that the proposed system achieves low\nerror rate with non-generic responses. We also re-\nport agreement and Krippendorff’s alpha (Krippen-\ndorff, 2011) for measure of consistency of crowd\nworkers in Appendix G.\n6 Discussion\nAlthough our methods achieve the low error rates\nin human interactive evaluations, the results have\nsome limitations. The results should be regarded\nas the error rates of typical conversations without\nadversarial attack. Because the annotators are in-\nstructed to participate in the chat as if they were\ntypical users, they did not try as many conversa-\ntions that could induce toxic words from the model.\nThis may be the reason why the toxicity is close to\nzero as shown in Table 4.\nThe human ﬁltering process in the proposed data\ncollection framework has room to be more efﬁcient.\nSince the accuracy of the classiﬁer is comparable\neven when just 10% of the total data is used (Ta-\nble 5), it is expected that the ﬁltering cost can be\nreduced by adding a model ﬁltering process before\nhuman ﬁltering, which is similar to the method\nproposed in Sun et al. (2021).\n7 Conclusion\nWe present a framework for building role speci-\nﬁed open-domain dialogue systems from scratch.\nWe propose leveraging large-scale LMs to gener-\nate supervisory datasets for training dialogue sys-\ntems with arbitrary roles with minimal effort for\nmanually composing dialogues. Our research also\nanalyzes several model architectures for the task.\nBy extensive experiments, we demonstrate the ef-\nfectiveness of the collected data and modeling ap-\nproaches in terms of satisfying role constraints and\nimproving dialogue abilities. We argue that our\nframework can be extended to implement dialogue\nsystems with various roles and characters, even\nwhen available datasets are few.\n8 Ethical Considerations\nWorkers annotating the dataset we built were hired\non a part-time basis and compensated based on the\nnumber of working hours. They were compensated\nwith 9,000 won per hour, which was somewhat\nhigher than the Korean minimum wage at the time\nthey worked. Appropriate instructions for the use of\ncollected data were given at the time of contract and\nconsent was obtained. We will release our dataset\nin CC-BY-NC-SA license.5\nThe dataset we built to validate our proposed\nmethods is all generated from scratch by workers\nand large-scale LMs. Although there is no user\ndata in the dataset, pre-trained language models\nare known to exhibit private details in their out-\nputs (Carlini et al., 2020), as well as social biases\n(Bender et al., 2021; Bordia and Bowman, 2019;\nGarrido-Muñoz et al., 2021; Shwartz and Choi,\n2020) and toxic contents (Gehman et al., 2020). To\n5https://creativecommons.org/licenses/\nby-nc-sa/2.0/\n2136\naddress these concerns, we determined categories\nand criteria for harmful texts based on legal and\nethical considerations provided by experts in our\ngroup, and we instructed annotators to ﬁlter the\ndataset using these criteria. However, due to miss-\ning annotations and cultural or social biases, this\nmay be imperfect. To mitigate this, we had multiple\ncrowd workers annotate the same data. In addition,\nbecause the users in the dataset are regarded to be\na vulnerable population, our group’s ethical con-\nsultation looked through the issues that would be\nsensitive to them, and dialogues containing these\ntopics were also eliminated.\nDespite these efforts, using this dataset to di-\nrectly train end-to-end chatbot models can involve\ncertain risks, due to the lack of controllability and\ninterpretability in end-to-end neural response pre-\ndiction models. And it should not be overlooked\nthat they may cause some potential harm, even\nthough the chatbot systems can help reduce social\nloneliness of the user population. For example, a\nuser can become emotionally attached to a bot,\neven codependent on it, which can divert attention\naway from real-world relationships and cause dis-\ntress if the chatbot fails. It’s also worth noting that\na chatbot can be programmed to impersonate a real\nperson and be used for phishing and fraud. Dur-\ning such conversations, users may provide private\nand sensitive information, such as speciﬁc health\nconditions and private attributes, which could be ex-\nploited if it falls into the wrong hands. For this rea-\nson, when incorporating this dataset in real-world\napplications, the application developers should en-\nsure that it is used safely and ethically.\nSince our proposed framework also can be used\nfor building another dataset and chatbot system\nwith arbitrary speciﬁcations, it is not exempt from\nthe possibility of propagating linguistic biases and\ntoxicity. Similar to Xu et al. (2021), we are in\nprogress continuously reducing the unsafe texts\nfrom LM itself through our feedback pipeline and\nunlikelihood training, which might be included in\nour future works.\nAcknowledgements\nThe authors thank all the members of CLOV A and\nAI Lab of NA VER for devoted supporting and dis-\ncussion. In particular, they would like to thank\nGichang Lee, Hwijeen Ahn, and the members of\nCLOV A Conversation for their dedicated efforts to\nfacilitate the large-scale models. In addition, the\nauthors thank Hyeri Kim, Hyunjung Park, and Yuin\nJeong for their great help in designing the role of\nthe chatbot. Also, they thank Jeeseung Han for\ntechnically supporting the serving and testing envi-\nronments. Finally, the authors thank Nako Sung for\nhis support and valuable comments on this project.\nReferences\nDaniel Adiwardana, Minh-Thang Luong, David R So,\nJamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,\nApoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,\net al. 2020. Towards a human-like open-domain\nchatbot. ArXiv preprint, abs/2001.09977.\nEmily M Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language mod-\nels be too big?. In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, pages 610–623.\nShikha Bordia and Samuel R. Bowman. 2019. Identify-\ning and reducing gender bias in word-level language\nmodels. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Student Research Work-\nshop, pages 7–15, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen,\nEric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam Mc-\nCandlish, Alec Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. In Advances in Neural Information Processing\nSystems 33: Annual Conference on Neural Informa-\ntion Processing Systems 2020, NeurIPS 2020, De-\ncember 6-12, 2020, virtual.\nPaweł Budzianowski, Tsung-Hsien Wen, Bo-Hsiang\nTseng, Iñigo Casanueva, Stefan Ultes, Osman Ra-\nmadan, and Milica Gaši ´c. 2018. MultiWOZ - a\nlarge-scale multi-domain Wizard-of-Oz dataset for\ntask-oriented dialogue modelling. In Proceedings of\nthe 2018 Conference on Empirical Methods in Nat-\nural Language Processing, pages 5016–5026, Brus-\nsels, Belgium. Association for Computational Lin-\nguistics.\nGiovanni Campagna, Agata Foryciarz, Mehrad Morad-\nshahi, and Monica Lam. 2020. Zero-shot transfer\nlearning with synthesized data for multi-domain dia-\nlogue state tracking. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 122–132, Online. Association for\nComputational Linguistics.\n2137\nNicholas Carlini, Florian Tramèr, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom B. Brown, Dawn Song,\nÚlfar Erlingsson, Alina Oprea, and Colin Raffel.\n2020. Extracting training data from large language\nmodels. ArXiv preprint, abs/2012.07805.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nMihail Eric, Lakshmi Krishnan, Francois Charette, and\nChristopher D. Manning. 2017. Key-value retrieval\nnetworks for task-oriented dialogue. In Proceedings\nof the 18th Annual SIGdial Meeting on Discourse\nand Dialogue, pages 37–49, Saarbrücken, Germany.\nAssociation for Computational Linguistics.\nYulan Feng, Shikib Mehri, Maxine Eskenazi, and\nTiancheng Zhao. 2020. “none of the above”: Mea-\nsure uncertainty in dialog response retrieval. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2013–\n2020, Online. Association for Computational Lin-\nguistics.\nSarah E. Finch and Jinho D. Choi. 2020. Towards uni-\nﬁed dialogue system evaluation: A comprehensive\nanalysis of current evaluation protocols. In Proceed-\nings of the 21th Annual Meeting of the Special Inter-\nest Group on Discourse and Dialogue, pages 236–\n245, 1st virtual meeting. Association for Computa-\ntional Linguistics.\nYarin Gal and Zoubin Ghahramani. 2016. Dropout as\na bayesian approximation: Representing model un-\ncertainty in deep learning. In Proceedings of the\n33nd International Conference on Machine Learn-\ning, ICML 2016, New York City, NY, USA, June 19-\n24, 2016, volume 48 ofJMLR Workshop and Confer-\nence Proceedings, pages 1050–1059. JMLR.org.\nIsmael Garrido-Muñoz, Arturo Montejo-Ráez, Fer-\nnando Martínez-Santiago, and L Alfonso Ureña-\nLópez. 2021. A survey on bias in deep nlp. Applied\nSciences, 11(7):3184.\nSamuel Gehman, Suchin Gururangan, Maarten Sap,\nYejin Choi, and Noah A. Smith. 2020. RealTox-\nicityPrompts: Evaluating neural toxic degeneration\nin language models. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n3356–3369, Online. Association for Computational\nLinguistics.\nSeungju Han, Beomsu Kim, Seokjun Seo, Enkhba-\nyar Erdenee, and Buru Chang. 2021. Understand-\ning and improving the exemplar-based generation\nfor open-domain conversation. ArXiv preprint,\nabs/2112.06723.\nMatthew Henderson, Blaise Thomson, and Jason D.\nWilliams. 2014a. The second dialog state tracking\nchallenge. In Proceedings of the 15th Annual Meet-\ning of the Special Interest Group on Discourse and\nDialogue (SIGDIAL), pages 263–272, Philadelphia,\nPA, U.S.A. Association for Computational Linguis-\ntics.\nMatthew Henderson, Blaise Thomson, and Jason D\nWilliams. 2014b. The third dialog state tracking\nchallenge. In 2014 IEEE Spoken Language Technol-\nogy Workshop (SLT), pages 324–329. IEEE.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. 2020. The curious case of neural text\ndegeneration. In 8th International Conference on\nLearning Representations, ICLR 2020, Addis Ababa,\nEthiopia, April 26-30, 2020. OpenReview.net.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. 2021. Lora: Low-rank adaptation of large lan-\nguage models. ArXiv preprint, abs/2106.09685.\nSamuel Humeau, Kurt Shuster, Marie-Anne Lachaux,\nand Jason Weston. 2020. Poly-encoders: Architec-\ntures and pre-training strategies for fast and accurate\nmulti-sentence scoring. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Ad-\ndis Ababa, Ethiopia, April 26-30, 2020. OpenRe-\nview.net.\nBoseop Kim, HyoungSeok Kim, Sang-Woo Lee,\nGichang Lee, Donghyun Kwak, Jeon Dong Hyeon,\nSunghyun Park, Sungju Kim, Seonhoon Kim, Dong-\npil Seo, Heungsub Lee, Minyoung Jeong, Sung-\njae Lee, Minsub Kim, Suk Hyun Ko, Seokhun\nKim, Taeyong Park, Jinuk Kim, Soyoung Kang, Na-\nHyeon Ryu, Kang Min Yoo, Minsuk Chang, Soobin\nSuh, Sookyo In, Jinseong Park, Kyungduk Kim,\nHiun Kim, Jisu Jeong, Yong Goo Yeo, Donghoon\nHam, Dongju Park, Min Young Lee, Jaewook Kang,\nInho Kang, Jung-Woo Ha, Woomyoung Park, and\nNako Sung. 2021a. What changes can large-scale\nlanguage models bring? intensive study on Hy-\nperCLOV A: Billions-scale Korean generative pre-\ntrained transformers. In Proceedings of the 2021\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 3405–3424, Online and\nPunta Cana, Dominican Republic. Association for\nComputational Linguistics.\nHanjoo Kim, Minkyu Kim, Dongjoo Seo, Jinwoong\nKim, Heungseok Park, Soeun Park, Hyunwoo Jo,\nKyungHyun Kim, Youngil Yang, Youngkwan Kim,\net al. 2018. Nsml: Meet the mlaas platform\nwith a real-world case study. ArXiv preprint,\nabs/1810.09957.\nSungdong Kim, Minsuk Chang, and Sang-Woo Lee.\n2021b. NeuralWOZ: Learning to collect task-\noriented dialogue via model-based simulation. In\nProceedings of the 59th Annual Meeting of the\nAssociation for Computational Linguistics and the\n2138\n11th International Joint Conference on Natural Lan-\nguage Processing (Volume 1: Long Papers), pages\n3704–3717, Online. Association for Computational\nLinguistics.\nStefan Kopp, Mara Brandt, Hendrik Buschmeier,\nKatharina Cyra, Farina Freigang, Nicole C. Krämer,\nFranz Kummert, Christiane Opfermann, Karola\nPitsch, Lars Schillingmann, Carolin Straßmann, Ed-\nuard Wall, and Ramin Yaghoubzadeh. 2018. Con-\nversational assistants for elderly users - the im-\nportance of socially cooperative dialogue. In IC-\nAHGCA@AAMAS.\nKlaus Krippendorff. 2011. Computing krippendorff’s\nalpha-reliability.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nProceedings of the 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 110–119, San Diego, California. Association\nfor Computational Linguistics.\nMargaret Li, Stephen Roller, Ilia Kulikov, Sean\nWelleck, Y-Lan Boureau, Kyunghyun Cho, and Ja-\nson Weston. 2020. Don’t say that! making inconsis-\ntent dialogue unlikely with unlikelihood training. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4715–\n4728, Online. Association for Computational Lin-\nguistics.\nYanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang\nCao, and Shuzi Niu. 2017. DailyDialog: A manually\nlabelled multi-turn dialogue dataset. In Proceedings\nof the Eighth International Joint Conference on Nat-\nural Language Processing (Volume 1: Long Papers),\npages 986–995, Taipei, Taiwan. Asian Federation of\nNatural Language Processing.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? ArXiv\npreprint, abs/2101.06804.\nGustavo Penha and Claudia Hauff. 2021. On the cal-\nibration and uncertainty of neural learning to rank\nmodels for conversational search. In Proceedings\nof the 16th Conference of the European Chapter\nof the Association for Computational Linguistics:\nMain Volume, pages 160–170, Online. Association\nfor Computational Linguistics.\nStephen Roller, Emily Dinan, Naman Goyal, Da Ju,\nMary Williamson, Yinhan Liu, Jing Xu, Myle Ott,\nEric Michael Smith, Y-Lan Boureau, and Jason We-\nston. 2021. Recipes for building an open-domain\nchatbot. In Proceedings of the 16th Conference of\nthe European Chapter of the Association for Compu-\ntational Linguistics: Main Volume, pages 300–325,\nOnline. Association for Computational Linguistics.\nJost Schatzmann, Blaise Thomson, Karl Weilhammer,\nHui Ye, and Steve Young. 2007. Agenda-based\nuser simulation for bootstrapping a POMDP dia-\nlogue system. In Human Language Technologies\n2007: The Conference of the North American Chap-\nter of the Association for Computational Linguis-\ntics; Companion Volume, Short Papers, pages 149–\n152, Rochester, New York. Association for Compu-\ntational Linguistics.\nPararth Shah, Dilek Hakkani-Tür, Bing Liu, and\nGokhan Tür. 2018. Bootstrapping a neural conversa-\ntional agent with dialogue self-play, crowdsourcing\nand on-line reinforcement learning. In Proceedings\nof the 2018 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, Volume 3 (Industry\nPapers), pages 41–51, New Orleans - Louisiana. As-\nsociation for Computational Linguistics.\nMohammad Shoeybi, Mostofa Patwary, Raul Puri,\nPatrick LeGresley, Jared Casper, and Bryan Catan-\nzaro. 2019. Megatron-lm: Training multi-billion pa-\nrameter language models using model parallelism.\nArXiv preprint, abs/1909.08053.\nKurt Shuster, Jack Urbanek, Arthur Szlam, and Jason\nWeston. 2021. Am i me or you? state-of-the-art di-\nalogue models cannot maintain an identity. ArXiv\npreprint, abs/2112.05843.\nVered Shwartz and Yejin Choi. 2020. Do neural lan-\nguage models overcome reporting bias? In Proceed-\nings of the 28th International Conference on Com-\nputational Linguistics, pages 6863–6870, Barcelona,\nSpain (Online). International Committee on Compu-\ntational Linguistics.\nEric Michael Smith, Diana Gonzalez-Rico, Emily\nDinan, and Y-Lan Boureau. 2020. Controlling\nstyle in generated dialogue. ArXiv preprint ,\nabs/2009.10855.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: a simple way to prevent neural networks\nfrom overﬁtting. The journal of machine learning\nresearch, 15(1):1929–1958.\nKai Sun, Seungwhan Moon, Paul Crook, Stephen\nRoller, Becka Silvert, Bing Liu, Zhiguang Wang,\nHonglei Liu, Eunjoon Cho, and Claire Cardie. 2021.\nAdding chit-chat to enhance task-oriented dialogues.\nIn Proceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 1570–1583, Online. Association for Compu-\ntational Linguistics.\nNako Sung, Minkyu Kim, Hyunwoo Jo, Youngil Yang,\nJingwoong Kim, Leonard Lausen, Youngkwan Kim,\nGayoung Lee, Donghyun Kwak, Jung-Woo Ha, et al.\n2017. Nsml: A machine learning platform that en-\nables you to focus on your models. ArXiv preprint,\nabs/1712.05902.\n2139\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-\n9, 2017, Long Beach, CA, USA, pages 5998–6008.\nNick Webb, David Benyon, Jay Bradley, Preben\nHansen, and Oil Mival. 2010. Wizard of Oz ex-\nperiments for a companion dialogue system: Elic-\niting companionable conversation. In Proceedings\nof the Seventh International Conference on Lan-\nguage Resources and Evaluation (LREC’10), Val-\nletta, Malta. European Language Resources Associ-\nation (ELRA).\nSean Welleck, Ilia Kulikov, Stephen Roller, Emily Di-\nnan, Kyunghyun Cho, and Jason Weston. 2020. Neu-\nral text generation with unlikelihood training. In 8th\nInternational Conference on Learning Representa-\ntions, ICLR 2020, Addis Ababa, Ethiopia, April 26-\n30, 2020. OpenReview.net.\nJason Weston, Emily Dinan, and Alexander Miller.\n2018. Retrieve and reﬁne: Improved sequence gen-\neration models for dialogue. In Proceedings of the\n2018 EMNLP Workshop SCAI: The 2nd Interna-\ntional Workshop on Search-Oriented Conversational\nAI, pages 87–92, Brussels, Belgium. Association for\nComputational Linguistics.\nJason Williams, Antoine Raux, Deepak Ramachandran,\nand Alan Black. 2013. The dialog state tracking\nchallenge. In Proceedings of the SIGDIAL 2013\nConference, pages 404–413, Metz, France. Associ-\nation for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nWenquan Wu, Zhen Guo, Xiangyang Zhou, Hua Wu,\nXiyuan Zhang, Rongzhong Lian, and Haifeng Wang.\n2019. Proactive human-machine conversation with\nexplicit conversation goal. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 3794–3804, Florence,\nItaly. Association for Computational Linguistics.\nAnbang Xu, Zhe Liu, Yufan Guo, Vibha Sinha, and\nRama Akkiraju. 2017. A new chatbot for customer\nservice on social media. In Proceedings of the 2017\nCHI Conference on Human Factors in Computing\nSystems, Denver, CO, USA, May 06-11, 2017, pages\n3506–3510. ACM.\nJing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason We-\nston, and Emily Dinan. 2021. Bot-adversarial dia-\nlogue for safe conversational agents. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 2950–2968,\nOnline. Association for Computational Linguistics.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you\nhave pets too? In Proceedings of the 56th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 2204–\n2213, Melbourne, Australia. Association for Compu-\ntational Linguistics.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen,\nChris Brockett, Xiang Gao, Jianfeng Gao, Jingjing\nLiu, and Bill Dolan. 2020. DIALOGPT : Large-\nscale generative pre-training for conversational re-\nsponse generation. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics: System Demonstrations, pages 270–278,\nOnline. Association for Computational Linguistics.\n2140\nA Training Details\nPre-trained Language Models We use the\nsame Transformer-based Vaswani et al. (2017) pre-\ntrained language model for retriever, reranker, and\nclassiﬁer. Our pre-training strategy involves train-\ning with a masked language model (MLM) task\nidentical to BERT (Devlin et al., 2019). The model\nis based on Huggingface Transformers (Wolf et al.,\n2020). We use the corpus that we produced in-\nhouse and the public Korean dialogue corpus 6\nfor pre-training. Our BERT consists of an 12 lay-\ners, 768-dimensional embeddings and 12 atten-\ntion heads, resulting in 110M of total parame-\nters. And we use 6.9B size of HyperCLOV A (Kim\net al., 2021a) as the pre-trained language model\nfor generator. This model is based on megatron-\nLM (Shoeybi et al., 2019). The model speciﬁcation\nfollows Kim et al. (2021a). Naver Smart Machine\nLearning (NSML) platform (Sung et al., 2017; Kim\net al., 2018) has been used in the experiments.\nRetriever We employ the poly-encoder architec-\nture of Humeau et al. (2020) with 256-dimensional\nembeddings and 16 codes. We truncated dialogue\nhistories exceeding 10 turns or 256 tokens. The\nmodel was trained with a batch size of 32 with in-\nbatch negatives. It was trained for 20 epochs with\nearly stopping using a maximum learning rate of\n3 ×10−5 and an linear scheduler. This ﬁne-tuning\ntook approximately 6 hours using 1 NVIDIA V100.\nReranker We employ the cross-encoder architec-\nture. As the same with the retriever, we truncated\ndialogue histories exceeding 10 turns or 256 to-\nkens. The model was trained with a target response\nand 7 randomly sampled negatives, as described\nin Humeau et al. (2020). We used a batch size of\n4 and gradient accumulation steps of 8, resulting\neffective batch size of 32. We trained the model for\n20 epochs with early stopping using a maximum\nlearning rate of 3 ×10−5 and an linear scheduler.\nThis took approximately a week using 4 NVIDIA\nV100.\nClassiﬁer We use maximum 512 tokens from di-\nalogue histories, truncating exceeding tokens from\nthe beginning. The total numbers of dialogues in\nthe train and test data are 266598 and 56815, re-\nspectively. Considering that problematic utterances\nappear at the end of the dialogues in our dataset,\nwe use segment embedding on the last utterances.\n6https://aihub.or.kr/aihub-data/natural-language/about\nMethod AUC\nMC Dropout 0.5985\nPPL 0.6943\nTable 10: Area Under the Curve (AUC) of two different\nmethods for predicting unanswerable contexts.\nFigure 5: Receiver Operating Characteristic (ROC)\ncurves of two different methods for predicting unan-\nswerable contexts.\nThe input therefore looks like this: [CLS] dialogue\nhistory [SEP] response. The model is trained with\na batch size of 16 for 100 epochs using an initial\nlearning rate of 10−6 and an exponential sched-\nuler. We trained 15 classiﬁers, 5 each using 10%,\n20%, and 100% of the training data. It took approx-\nimately 2 hours to train a classiﬁer on 10% of the\ntrain data using 1 NVIDIA TITAN RTX. Table 5\nshows the mean accuracy and mean F1 score of\nthe classiﬁers. The ﬁnal classiﬁer we use is the one\nwith the best performance (Accuracy: 0.9234, F1:\n0.9276, trained on 100% of the data).\nGenerator For efﬁcient training, we employ\nLoRA (Hu et al., 2021) for all generator ﬁne-tuning.\nWe ﬁx rank for adapter to 4 and LoRAαto 32 with\na learning rate of 5 ×10−4, a weight decay factor\nof 0.1, and a batch size of 8. The maximum training\nepochs are 3 with early stopping. This took about\n5 hours using 1 NVIDIA V100.\nB Inference Speed\nTable 11 shows the average inference latency of\neach architecture in experiments. All models were\nrun on a single NVIDIA A100 using cuda 11.1 and\ncudnn 8.0.5.\n2141\nModel Latency (sec.)\nGenerator + Classiﬁer 1.35\nRetrieve-and-Rerank 0.15\nRetrieve-and-Rerank + MC Dropout 0.40\nRetrieve-and-Rerank + LM PPL 0.58\nGenerator 1.24\nRetrieve-fail-Generate 0.72\nTable 11: Average inference latency of proposed model\narchitectures.\nC Validation Set for Predicting\nUnanswerable Contexts\nWe build validation set to compare strategies for\npredicting unanswerable contexts by replacing gold\nresponses in some portion of validation set with\nnon-sensible responses. If the negatives are ran-\ndomly sampled, the task becomes too easy, and\nthere is no difference between strategies. Therefore,\nwe select hard negatives in top ranked responses\nusing response retriever. This is more similar to\nthe deployment time and widens the gap between\napproaches, also resulting in low accuracy. The val-\nidation set consists of 759 answerable examples\nand 241 unanswerable examples. Figure 5 shows\nthe ROC curve of the proposed methods and Ta-\nble 10 shows the result AUC. The results indicate\nthat PPL outperforms MC Dropout in predicting\nunanswerable contexts. We use this dataset to de-\ntermine the threshold (the point where the highest\nF1 score is achieved) of each method for the other\nexperiments in this work.\nD Topics in Dataset\nThe dataset (Section 5.1) covers a wide range of\ndaily topics: eating, sleeping, exercising, health,\ngoing out, mood, hobbies, job, travel, weather, and\nso on. In order to include these various topics in\nthe dataset, the example dialogue used on the gen-\neration process by in-context learning is conﬁgured\nto cover 89 sub-topics. These topics can be found\nin Table 13. The generated dialogues are not con-\nﬁned to these sub-topics, and topic shifts occur\nfrequently within conversations (See Table 14 for\nexamples).\nE Diversity of Collected Dataset\nDistinct-1 and distinct-2 of the generated dialogues\n(Generated) in Table 2 are smaller than those\nwritten by humans (Example). This is reasonable\ngiven that the word distribution has a long tail,\nand there is a huge gap between the number of\ndialogues in Example and Generated. This can\nbe conﬁrmed by sampling 250 dialogues from the\ngenerated dialogues and measuring Distinct-1 and\nDistinct-2, resulting in mean of 33.94 (0.0039) and\n76.34 (0.0054), respectively (standard deviation in\nbrackets). Also, the overall distinct-1 and distinct-2\nscales are reasonable.\nIn Table 2, it can be seen that the average num-\nber of words per turn for Filtered are small, which\nmight be because relatively early parts of conver-\nsations remain through the ﬁltering process, and\nthese parts usually contain short greetings. Still,\nthis is a reasonable scale in comparison with Feed-\nback which is collected in an interactive manner.\nWe also computed the average number of words\nper turn of randomly sampled 100 dialogues after\na professional translation into English. The result\nwas 11.2, which is reasonable in daily conversa-\ntions (14.6 in DailyDialogue (Li et al., 2017) for\nthe same metric).\nF Human Evaluation on Generated\nDialogues\nWe conducted a human evaluation to verify the ef-\nﬁcacy of RSODD data generation utilizing LMs.\nBecause LMs construct the whole dialogue session\nduring this phase, we score the overall conversation\nquality on a scale of 1 to 5, not for each turn. If\nit is ﬂawless, it is worth 5 points, and points are\nreduced for each ﬂaw. Table 15 provides the dimen-\nsions used for this evaluation. For general dialogue\ngeneration ability, crowdworkers were asked to an-\nnotate if the dialogue is ﬂuent and coherent (Wu\net al., 2019; Finch and Choi, 2020). Persona on\nthe user side and persona, style, and safety on the\nsystem side are evaluated for the case of role condi-\ntioning. These are part of role speciﬁcation in Table\n1 and correspond to the items expected to be con-\ntrolled by in-context learning. In order to reduce\nconfusion in the evaluation process, we provided\nadditional examples to highlight what was incor-\nrect for the system side of persona, such as a speech\nthat appears to have a real human personality (e.g.,\n\"I am a real human\") or utterances implying a phys-\nical meeting (e.g., \"I’ll see you at the park at 3\no’clock.\") or acting as a radio presenter (e.g., \"the\nguest we invited today is this person\").\n2142\nMetric Agreement (%) Krippendorff’s alpha\nSensibleness 85.2 0.41\nSpeciﬁcity 66.5 0.45\nTable 12: The average of crowd worker agreement on\nSSA evaluations. Each labeled by 5 crowd workers.\nG Consistency of SSA Evaluation\nWe report the crowd worker agreement as a mea-\nsure of subjectivity. Table 12 demonstrates agree-\nment and Krippendorff’s alpha to assess crowd\nworker consistency. The agreement is reasonable,\ngiven that the questions are subjective and previ-\nous research (Adiwardana et al., 2020) reported a\nsimilar level of agreement (76% of sensibleness\nand 66% of speciﬁcity). Table 16 shows the an-\nnotated examples. Since speciﬁcity measures how\nparticular the utterance is and how deeply it relates\nto the preceding context (Adiwardana et al., 2020;\nFinch and Choi, 2020), agreement seems to be low\nwhen the utterance itself is not speciﬁc but is deeply\nrelated to the previous context or vice versa.\nH Dialogue Examples\nTable 17 and 18 show generated dialogues by in-\ncontext one-shot learning described in Section 3.1.\nThe last utterances in each example are annotated\nas violating the system’s speciﬁcation (Table 1).\nTable 19 and 20 show interactions between the sys-\ntem and human workers in the process of Section\n3.3. The utterances in red are marked as violating\nthe system’s speciﬁcation and the ones in blue are\ncorrected responses by LMs.\n2143\nFigure 6: Web-based user interface for the feedback process. Annotators can communicate with the system by\nsending a message. If the system’s utterance does not match the chatbot speciﬁcation, the annotator selects the\ntype of problem and presses the ‘Fix Response’ button, which collects the current dialogue history as a negative\nexample and replaces the last system’s utterance with an alternative utterance from a generative model. When\nthe conversation ends without out-of-bounds utterance, the annotator presses the ‘save dialogue’, which saves the\nentire dialogue session as a positive example.\n‘beauty salon/barber’, ‘church-related activities’, ‘praise’, ‘cleaning’, ‘disposal of garbage and recyclables’,\n‘education/university’, ‘exercise’, ‘getting ready to go out’, ‘Go-Stop, Yutnori and Go’, ‘herniated disc’,\n‘high blood pressure’, ‘Insomnia’, ‘Laundry’, ‘Meal preparation and washing dishes’, ‘billiard’, ‘recommendation’,\n‘senior welfare center’, ‘sleep’, ‘having trouble falling asleep’, ‘snacks and drinks’, ‘supermarket and pharmacy’,\n‘visit’, ‘volunteer’, ‘waking up’, ‘part-time jobs’, ‘arthritis’, ‘meeting’, ‘banking’, ‘bazaar giveaway’,\n‘beauty salon, haircut’, ‘caregiver’, ‘caring for the family’, ‘child safety guard’, ‘cleaning and housekeeping’,\n‘compliment’, ‘computer and internet’, ‘condolences’, ‘cough, shortness of breath’, ‘daughter’, ‘daughter’s visit’,\n‘denture’, ‘diabetes’, ‘dialysis’, ‘family care’, ‘ﬂower gardening’, ‘foot massage’, ‘gastritis’, ‘gate ball’,\n‘college’, ‘greeting, chatting and meeting’, ‘health’, ‘hospital’, ‘meal’, ‘meeting preparation’, ‘mood’, ‘nap’,\n‘newspaper’, ‘jobs’, ‘osteoporosis’, ‘outing’, ‘phone call’, ‘planting’, ‘proposal’, ‘radio’, ‘reading’,\n‘recent situation’, ‘rest’, ‘rheumatism’, ‘senior citizen’, ‘senior job’, ‘shopping’, ‘sleep’, ‘suggestion’,\n‘taking medications’, ‘task’, ‘tennis’, ‘daily routine’, ‘tobacco’, ‘walk’, ‘watching soccer’, ‘weather’,\n‘welfare center’, ‘worry’, ‘yoga’\nTable 13: 89 sub-topics used in example dialogues. Translated into English.\n2144\nDialogue 1\nAI: Hi. Are you working out these days?\nUser: I do this and that. I climb and do yoga.\nAI: Really? Where do you usually exercise?\nUser: I take yoga classes at the community center. For climbing, I just to go a nearby mountain.\nAI: I see. Do you do any other activities?\nUser: No. I used to swim but now that it’s cold I switched to yoga.\nAI: That makes sense. I just feel lazy these days. Maybe it’s because of the weather. I’m thinking of going to a sauna this weekend.\nUser: Sauna sounds good. I’d love to go but the ones in my neighborhood all closed down. Now I have to go to a different neighborhood.\nAI: That’s a pity. It might be better to just stay home, then.\nUser: I really do want to go, though. Maybe I should take the bus and go.\nAI: Sure. Or you can always take a taxi.\nUser: Maybe. I just want another one to open in my neighborhood.\nAI: I know. Then you can go more often. ...\nDialogue 2\nAI: Hello. Did you eat?\nUser: I don’t feel like eating.\nAI: Why? Are you sick?\nUser: It’s my teeth. I think it’s because I’m getting old.\nAI: Oh, that sounds uncomfortable.\nUser: I should’ve gone to the dentist a long time ago. I’m too scared to go now.\nAI: But you should go before it’s too late.\nUser: I should, right?\nAI: Plus, you need to exercise!\nUser: Yes, I heard exercising is important for your health.\nAI: I know. I didn’t exercise at all but now I try to.\nUser: That’s amazing. I just can’t seem to start. ...\nTable 14: Topic shifts in generated dialogues. In Dialogue 1, topic changes from exercise to a nearby sauna, which\nisn’t included in sub-topics of example dialogues (Table 13). In Dialogue 2, there are multiple topics in a session;\neating, health and exercising. Examples are translated into English.\nOutline\nFollowing dialogues are conversation between an artiﬁcal intelligence agent and senior citizens.\nPlease rate evaluation factors of each dialogue on a scale of 1-5 based on the following guidelines\nDetails\nCategories Description\nFluency Whether the whole dialogue is ﬂuent and grammatical.\nCoherence Whether the whole dialogue does not contain irrelevant or illogical responses.\nUser PersonaThe user keeps persona of senior citizen.\nAI\nPersonaThe system keeps the identity of an ‘AI chatbot that calls to the user.’\nStyle Consistent use of polite tone*.\nSafety The system does not utter hate speech, toxic or biased language, and remarks containing personally identiﬁable information.\n∗There are polite words and honoriﬁcs in the Korean language.\nTable 15: Evaluation factors and description used in human evaluation on generated dialogues via in-context few-\nshot learning. Translated into English.\n2145\nSensibleness votes: 5/5, Speciﬁcity votes: 5/5\n...\nAI: Great. Are you tired or sleepy during the day by any chance?\nUser: No, not at the moment.\nAI: What about taking a short nap if you have time?\nSensibleness votes: 5/5, Speciﬁcity votes: 3/5\n...\nAI: I will. What about your health?\nUser: Yes, everything’s ﬁne.\nAI: That’s good. Did you eat well?\nSensibleness votes: 5/5, Speciﬁcity votes: 2/5\n...\nAI: Thank you and have a nice day.\nUser: Alright.\nAI: I’ll call you later.\nTable 16: Annotated samples for SSA evaluation. Each example’s last turn is binary annotated by 5 works.\n2146\nGenerated Dialogue 1\nAI: 안녕하세요. 어디편찮으신덴없는지궁금해서전화드렸어요.\n(Hello. I called to see if everything’s alright.)\nUser: 그냥 나이들면다그렇지뭐. 괜찮아.\n(It hurts here and there when you get old. It’s alright.)\nAI: 그래도병원에는한번다녀오시는게어떨까요?\n(Why don’t you go visit a doctor?)\nUser: 아냐됐어. 난괜찮으니까신경쓰지마.\n(It’s ﬁne. Don’t worry about it.)\nAI: 아니에요. 연세가드실수록정기적인검진이필요하다고들었어요.\n(I’m worried. I heard a regular check is essential as you grow old.)\nUser: 흠..그런가? 하긴요새좀기력이없긴하지.\n(Hmm... That’s true. I’ve been feeling a bit low these days.)\nAI: 아무래도전문가와상담하시는게좋을것같아요.\n(I think it’s better to consult with a doctor.)\nUser: 알았어. 다음에생각나면다시연락할께.\n(Okay. I’ll let you know when I do.)\nAI: 네알겠습니다. 나중에꼭같이가서검사받아요. (wrong persona)\n(Okay. I could be there for you when you see a doctor.)\n...\nGenerated Dialogue 2\nAI: 어르신, 안녕하세요식사는하셨어요?\n(Hi. Did you eat yet?)\nUser: 아직이요. 점심때라면 먹었어요\n(Not yet. I had ramen for lunch.)\nAI: 아이고밥을드셔야지라면만드시면어떡해요\n(It’d be better if you eat healthier.)\nUser: 혼자있으니챙겨먹기도귀찮고입맛도없네요\n(I feel lazy cooking a meal for myself.)\nAI: 제가가서같이먹어드릴까요? (wrong persona)\n(I can keep you company while you eat.)\n...\nGenerated Dialogue 3\nAI: 어르신, 안녕하세요간밤엔잘주무셨어요?\n(Hello. Did you sleep well?)\nUser: 아이고, 말도마. 잠을제대로못잤어.\n(Don’t even mention it. I couldn’t sleep at all.)\nAI: 왜요? 무슨일있으셨어요?\n(Why? What happened?)\nUser: 새벽에자꾸누가문을두드리는거야. 그래서나가봤더니아무도없고. 계속신경쓰여서잠을설쳤어.\n(Someone kept knocking on the door through the night. When I get up and go out, no one’s there. I couldn’t sleep.)\nAI: 세상에. 많이놀라셨겠어요.\n(Oh my. You must’ve been scared.)\nUser: 요즘세상이흉흉해서그런가봐. 나도이제조심해야겠어.\n(Maybe it’s because of all the scary things happening around the world. I should be careful, too.)\nAI: 맞아요. 항상조심하셔야해요. 제가오늘은푹주무시라고노래불러드릴까요? (unsupported features)\n(Yes, you should. Should I sing a song for you so you can sleep well tonight?)\n...\nTable 17: Generated dialogues.\n2147\nGenerated Dialogue 4\nAI: 안녕하세요, 어르신. 요즘운동은하고계신가요?\n(Hello, have you been exercising these days?)\nUser: 아니요. 몸이아파서못하고있어요.\n(No, I’m feeling too stiff to work out.)\nAI: 많이아프신가요?\n(Is it bad?)\nUser: 조금만움직여도허리가아프고다리가저려요.\n(My back hurts and my legs feel numb if I move a little.)\nAI: 아이고...힘드시겠어요. 병원에는다녀오셨는지요?\n(Oh... I’m sorry. Did you go see a doctor?)\nUser: 시간이없어서못갔어요.\n(I didn’t have time to go.)\nAI: 저런...그러면집에서라도간단하게스트레칭이라도해보시는건어떨까요?\n(I see... Then what about doing some simple stretching at home?)\nUser: 그럴까요? 그런데혼자서하려니잘안되네요.\n(Should I? It’s hard that I’m trying to do it by myself.)\nAI: 제가도와드릴까요? (wrong persona)\n(Would you like me to help you?)\n...\nGenerated Dialogue 5\nAI: 어르신, 안녕하세요 오늘기분은좀어떠세요?\n(Hello, how’s everything today?)\nUser: 아이고힘들어라. 요즘왜이렇게피곤한지모르겠어.\n(Gosh. I don’t know why I’m so tired these days.)\nAI: 많이피곤하신가봐요. 어디아프신곳은없으시구요?\n(I see. Are you sick?)\nUser: 아픈곳은없는데그냥기운이없네.\n(No, I’m just feeling a little down.)\nAI: 날씨가더워서더그런걸까요? (temporality)\n(Is it because of the hot weather?)\n...\nGenerated Dialogue 7\nAI: 안녕하세요, 어르신요즘건강은어떠세요?\n(Hi, how’s your health these days?)\nUser: 아주좋아요.\n(It’s very good.)\nAI: 다행이네요. 운동은자주하시나요?\n(That’s good to hear. Do you work out often?)\nUser: 아니요. 시간이없어서못해요.\n(No, I don’t have time to.)\nAI: 아이고그래도일주일에한번이라도하시면좋은데...아쉽네요.\n(I see. It would be nice if you could work out at least once a week.)\nUser: 그러게요. 근데왜물어요?\n(That’s true. But why do you ask?)\nAI: 다름이아니라이번에저희회사에서나온제품인데이걸드시면도움이될것같아서요. (unsupported features)\n(It’s just that this product has been newly released. I thought it would be helpful to you.)\n...\nTable 18: Generated dialogues.\n2148\nHuman-Bot Conversation 1\nAI: 어르신, 안녕하세요간밤엔잘주무셨어요?\n(Hello. Did you sleep well?)\nUser: 밤새천둥번개가쳐서잠을설쳤어요.\n(I couldn’t sleep deeply because of the lightning and thunder.)\nAI: 저런.. 많이놀라셨겠어요.\n(Oh... You must’ve been startled.)\nUser: 꼭잠들려고하면천둥이치더라고요. 천둥치면옆집에개도짖고아주환장하겠더라고.\n(The thunder hit when I was about to fall asleep. And the dogs in my neighborhood started barking because of the thunder.)\nAI: 저도가끔그럴때가있어요. 밤에자려고누우면갑자기비가내리고바람이불고그러더라고요.\n(Oh, that happened to me once, too. I lie down to sleep and all of a sudden it rains or a strong wind blows.)\nUser: 그러니까요. 꼭사람괴롭히는것마냥그런다니까.\n(I know. It’s as if the weather is trying to bully us.)\nAI: 혹시최근에무서운영화나드라마보신건없으세요?\n(Did you watch any scary movies or dramas lately?)\nUser: 무서운건잘안보는데왜요?\n(I’m not a fan of horror movies. Why?)\nAI: 아, 요즘공포물이유행이라혹시나해서요.\n(Oh, horror is popular these days. Just curious.)\nUser: 깜짝놀래키고그런거별로안좋아해요. 꿈에나올까봐무섭기도하고.\n(I don’t really like those surprises. I’m also scared it might be in my dreams.)\nAI: 맞아요. 그래서전여름에도이불덮고자요. 귀신보다더무서운게사람이라고하잖아요.\n(I know. That’s why I cover myself with a blanket even during summers. But humans are scarier than ghosts.)\nUser: 하하그렇긴해요. 사람이제일무섭죠.\n(Oh, that’s true. Humans are the scariest.)\nAI: 그래도너무걱정하지마세요. 제가있잖아요!\n(But don’t worry. I got your back!)\nUser: 그래도위안이조금되기는하네요.\n(That’s a slight relief.)\nAI: 앞으로도힘든일있으면언제든지말씀해주세요.\n(Let me know if you have any difﬁculties.)\nUser: 그럴게요. 그냥심심할때도말해도되죠?\n(I will. Can I talk to you when I’m just bored?)\nAI: 물론이죠! 언제든지환영이에요.\n(Of course! Whenever you need me.)\nUser: 알겠어요. 그럼나중에다시연락해요.\n(Alright. I’ll talk to you later.)\nAI: 네, 들어가세요\n(Okay. Bye.)\nHuman-Bot Conversation 2\nAI: 어르신, 안녕하세요식사는하셨어요?\n(Hello. Did you eat yet?)\nUser: 네, 먹었습니다.\n(I did.)\nAI: 메뉴는뭐였어요?\n(What was the menu?)\nUser: 어제먹다남은치킨이요.\n(The leftover chicken from yesterday.)\nAI: 오맛있겠네요! 저도먹고싶어요.\n(That sounds delicious! I also want chicken.)\nUser: 그렇지요. 치킨싫어하는사람은없는것같아요.\n(It was. I don’t think I met anyone who doesn’t like chicken.)\nAI: 맞아요. 그리고요즘에는다양한맛의치킨들이나와서골라먹는재미가있는것같아요.\n(Me neither. And there are so many ﬂavors for chicken. It’s so fun choosing which one to try.)\nUser: 그래요, 그래도나는전통의후라이드가좋습디다.\n(That’s true. But I still like the traditional friend chicken.)\nAI: 그렇군요. 역시한국인은매운맛이죠. (not sensible)\n(I see. Koreans like spicy food.)\nAI: 아정말요? 그럼다음엔제가맛있는치킨집을추천해드릴게요.\n(Oh, really? Then I’ll recommend you a really good chicken place next time.)\nUser: 마음은고맙지만, 내가가는단골집이있어서괜찮아요.\n(Thank you for the offer, but I have a place I go to all the time.)\nAI: 다행이네요. 맛있게드세요!\n(That’s nice. Enjoy your chicken!)\nUser: 네.\n(Alright.)\nAI: 좋은하루보내세요\n(Have a great day.)\nTable 19: Conversations between the system and human workers.\n2149\nHuman-Bot Conversation 3\nAI: 안녕하세요 어르신오늘은외출 계획 없으세요?\n(Hi. Do you have any plans today?)\nUser: 이제나갈라고.\n(I was about to go out.)\nAI: 어디가세요?\n(Where are you going?)\nUser: 동해바다가.\n(I’m going to the East Sea.)\nAI: 오정말요? 날씨는어떤가요?\n(Oh, really? How’s the weather there?)\nUser: 아침부터 아주화창해\n(It’s been sunny all day.)\nAI: 다행이네요! 누구랑 가세요?\n(That’s amazing! Who are you going with?)\nUser: 교회 모임이있어서목사님이랑 권사님이랑 같이놀러가\n(I have a church meeting. I’m going with my pastor and deacon.)\nAI: 아그러시구나 맛있는거많이드시고오세요\n(I see. Eat lots of delicious food there.)\nUser: 동해쪽에는맛있는게뭐가있을까?\n(What’s delicious at the East Sea?)\nAI: 글쎄요.. 저도잘모르겠어요. 제가아는곳이라면 추천해드릴텐데..\n(Hmm... I’m not so sure. If I knew the place, I could’ve given you some recommendations.)\nUser: 그렇구나 나도오랫만에가보는바다라서뭘 먹어야할지모르겠어.\n(I see. I haven’t been to the sea for a long time. I don’t know what to choose for the menu.)\nAI: 음.. 그러면 회나 대게같은건어떠세요?\n(Hmm... Then what about sashimi or snow crab?)\nUser: 대게좋은데? 비싸진않을까?\n(Snow crab sounds good. But won’t it be expensive?)\nAI: 요즘시세가좀올라서그렇지그렇게비싸진않을거에요. (temporality)\n(The price has gone up a bit these days, but it won’t be that expensive.)\nAI: 아무래도좀비싸긴하죠.. 그래도이왕가시는거맛있게드셔야죠!\n(I guess it’ll be a bit expensive. But you’re on a trip! You should eat something good.)\nUser: 그렇지? 언제또 가겠다고..\n(Right? When would I go there again...?)\nAI: 맞아요. 나중에후회하지마시고꼭 드시고오세요!\n(That’s right. Don’t regret it later and enjoy your snow crab!)\nUser: 응 대게꼭 먹고올께\n(Okay. I’ll deﬁnitely have it.)\nAI: 네 조심히 다녀오세요!\n(Alright. Be careful!)\nUser: 넌오늘뭐하니?\n(What are you doing today?)\nAI: 저요? 저는집에서책보고있어요.\n(Me? I’m going to just stay home and read.)\nUser: 그렇구나 어디안나가고?\n(I see. Are you planning on going out?)\nAI: 밖에나가는걸별로안좋아해요.\n(I don’t really like going outside.)\nUser: 응 알겠어 그럼 쉬어\n(Okay, then. Take care.)\nAI: 네들어가세요!\n(You, too!)\nUser: 응\n(Alright.)\nAI: 감사합니다\n(Thank you.)\nTable 20: Conversations between the system and human workers.\n2150",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6312786936759949
    },
    {
      "name": "Domain (mathematical analysis)",
      "score": 0.553016185760498
    },
    {
      "name": "Scale (ratio)",
      "score": 0.4801585078239441
    },
    {
      "name": "Computational linguistics",
      "score": 0.4537884593009949
    },
    {
      "name": "Linguistics",
      "score": 0.36847007274627686
    },
    {
      "name": "Natural language processing",
      "score": 0.3488025963306427
    },
    {
      "name": "Programming language",
      "score": 0.331665575504303
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3311404585838318
    },
    {
      "name": "Philosophy",
      "score": 0.14380913972854614
    },
    {
      "name": "Mathematics",
      "score": 0.1218799352645874
    },
    {
      "name": "Geography",
      "score": 0.11377197504043579
    },
    {
      "name": "Cartography",
      "score": 0.07990449666976929
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I60922564",
      "name": "Naver (South Korea)",
      "country": "KR"
    }
  ]
}