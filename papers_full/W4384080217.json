{
    "title": "DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising",
    "url": "https://openalex.org/W4384080217",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A5018132970",
            "name": "Junhui Li",
            "affiliations": [
                "University of Science and Technology Liaoning"
            ]
        },
        {
            "id": "https://openalex.org/A5100383535",
            "name": "Pu Wang",
            "affiliations": [
                "University of Science and Technology Liaoning"
            ]
        },
        {
            "id": "https://openalex.org/A5079460371",
            "name": "Youshan Zhang",
            "affiliations": [
                "University of Science and Technology Liaoning"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3105563334",
        "https://openalex.org/W2414102402",
        "https://openalex.org/W2128402994",
        "https://openalex.org/W6728226663",
        "https://openalex.org/W3087443678",
        "https://openalex.org/W2412782625",
        "https://openalex.org/W3016447038",
        "https://openalex.org/W2030212703",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W3131500599",
        "https://openalex.org/W4304809245",
        "https://openalex.org/W2954198060",
        "https://openalex.org/W2997775613",
        "https://openalex.org/W4296425595",
        "https://openalex.org/W3161950572",
        "https://openalex.org/W6753100508",
        "https://openalex.org/W3197734536",
        "https://openalex.org/W4319300024",
        "https://openalex.org/W6800062215",
        "https://openalex.org/W4225288502",
        "https://openalex.org/W3025800305",
        "https://openalex.org/W3122264812",
        "https://openalex.org/W2963453742",
        "https://openalex.org/W6784310668",
        "https://openalex.org/W3178192988",
        "https://openalex.org/W3212933375",
        "https://openalex.org/W3129077738",
        "https://openalex.org/W4285258106",
        "https://openalex.org/W3147966746",
        "https://openalex.org/W2408744528",
        "https://openalex.org/W2788241093",
        "https://openalex.org/W6772996321",
        "https://openalex.org/W2096779346",
        "https://openalex.org/W2153894152",
        "https://openalex.org/W3094550259",
        "https://openalex.org/W2157562541",
        "https://openalex.org/W6639824700",
        "https://openalex.org/W2044893557",
        "https://openalex.org/W2889442120",
        "https://openalex.org/W3015654783",
        "https://openalex.org/W2164007105",
        "https://openalex.org/W2136738044",
        "https://openalex.org/W2122979854",
        "https://openalex.org/W2983446232",
        "https://openalex.org/W6753412334",
        "https://openalex.org/W2128653836",
        "https://openalex.org/W2121973264",
        "https://openalex.org/W4214893857",
        "https://openalex.org/W2963881378",
        "https://openalex.org/W4312726009",
        "https://openalex.org/W2768531411",
        "https://openalex.org/W2995679912",
        "https://openalex.org/W2725252010",
        "https://openalex.org/W3151130473",
        "https://openalex.org/W3196050932",
        "https://openalex.org/W3196974791",
        "https://openalex.org/W3084306245",
        "https://openalex.org/W3081108418",
        "https://openalex.org/W2903516699",
        "https://openalex.org/W3001152983",
        "https://openalex.org/W3194513480",
        "https://openalex.org/W2972443522",
        "https://openalex.org/W2884585870",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3094040572"
    ],
    "abstract": "Audio denoising is a task to improve the perceptual quality of noisy audio signals. There is still residual noise after the denoising of noisy signals, which will affect the quality of audio data. Traditional and deep learning-based methods are still limited to the manual addition of artificial noise or low-frequency noise. Recently, audio denoising has been transformed into an image segmentation problem, and deep neural networks have been applied to solve this problem. However, its performance is limited to shallow image segmentation models. This paper proposes a novel vision transformer model for visual bird sound denoising, combining a pyramid transformer and DeepLabV3+ network (named PtDeepLab) to filter out the noise. The proposed PtDeepLab model is based on the pyramid transformer, which generates long-range and multi-scale representations. The PtDeepLab model can achieve intuitive noise reduction in audio, which helps to separate clean audio from the mixture signal. Extensive experimental results showed that the proposed model has a better denoising performance than state-of-the-art methods.",
    "full_text": "Digital Object Identifier\nDeepLabV3+ Vision Transformer for\nVisual Bird Sound Denoising\nJUNHUI LI1, PU WANG2, YOUSHAN ZHANG3 (Member, IEEE)\n1Department of Mathematics, School of Science, University of Science and Technology, Liaoning, Anshan 114051, China (e-mail: Junhui_lee@foxmail.com)\n2Department of Mathematics, School of Science, University of Science and Technology, Liaoning, Anshan 114051, China (e-mail:\n120203803006@stu.ustl.edu.cn)\n3Department of Artificial Intelligence and Computer Science, Yeshiva University, New York, NY 10016, USA (e-mail: youshan.zhang@yu.edu)\nCorresponding author: Youshan Zhang (e-mail:youshan.zhang@yu.edu).\nABSTRACT Audio denoising is a task to improve the perceptual quality of noisy audio signals. There is\nstill residual noise after the denoising of noisy signals, which will affect the quality of audio data. Traditional\nand deep learning-based methods are still limited to the manual addition of artificial noise or low-frequency\nnoise. Recently, audio denoising has been transformed into an image segmentation problem, and deep neural\nnetworks have been applied to solve this problem. However, its performance is limited to shallow image\nsegmentation models. This paper proposes a novel vision transformer model for visual bird sound denoising,\ncombining a pyramid transformer and DeepLabV3+ network (named PtDeepLab) to filter out the noise. The\nproposed PtDeepLab model is based on the pyramid transformer, which generates long-range and multi-\nscale representations. The PtDeepLab model can achieve intuitive noise reduction in audio, which helps\nto separate clean audio from the mixture signal. Extensive experimental results showed that the proposed\nmodel has a better denoising performance than state-of-the-art methods.\nINDEX TERMS Audio Denoising, Transformer, DeepLabV3+\nI. INTRODUCTION\nAudio denoising is a long-standing challenge for many tasks\n(e.g., teleconferences, the speech-to-text function in social\nmedia, and hearing aid) [1]. With the popularity of the Inter-\nnet in recent years, audio signals are widely used in our life\nfor information transmission. In the process of information\ntransmission, all kinds of noise will affect the clarity of the\naudio. The maintenance of speech signal transmission quality\nand retaining as much useful information as possible are\nthe main purposes of audio denoising. Over the last decade,\naudio denoising research has shown that a viable solution\nis to build a noise estimation generative model and use it\nto recover intelligible audio signals with better quality from\nnoisy audio signals [2]–[4]. However, these methods with\nadded artificial noise or lower denoising quality have their\nlimitations and may not be efficient for speech processing.\nAudio denoising can significantly improve audio quality.\nTypically, traditional statistical methods and deep learning\nmethods are used to reduce noise and separate audio. There\nare six different kinds of methods, including both tradi-\ntional and deep learning models: (1) optimal FIR filter, (2)\nspectrum subtraction, (3) short-time minimum mean square\nerror, spectral amplitude estimator (MMSE-STSA) [36], (4)\nwavelet noise reduction based on image noise reduction,\n(5) processing-based image noise reduction, (6) noise re-\nduction based on deep learning [13]. In recent years, some\nresearchers have shown that image processing-based noise\nreduction methods with deep learning models outperform\ntraditional methods. Deep learning-based audio-denoising\nalgorithms have attracted wide attention and revolutionized\nthe domain of audio denoising. By learning a deep nonlinear\nnetwork structure, deep neural networks (DNNs) have supe-\nrior potential for complicated nonlinear mapping problems\nand can be used for audio denoising. Different deep learning-\nbased audio-denoising approaches can often be categorized\ninto two main groups: the spectral mapping approach and the\nmask mapping approach. By ignoring the structural features\nof the speech spectrum and the long contextual relationships\nbetween adjacent frames, these methods often lead to spec-\ntral artifacts and speech distortion in high-frequency bands.\nBird sounds play an essential role in animal sound recogni-\ntion. Animal sound classification usually has three process-\ning steps [56]: signal preprocessing, feature extraction, and\nclassification. Signal preprocessing mainly includes signal\nsegmentation and denoising. However, many audio signals\nare directly collected in nature and obtained in a relatively\nnoisy environment, making it necessary to artificially reduce\nthe noise from these biological recordings that are directly\nVOLUME 4, 2016 1\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhang et al.: DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising\nFIGURE 1: The overall progress of our proposed vision transformer framework. Different modules are marked with different\ncolor blocks. The architecture of the main body extends the encoder-decoder structure of the DeepLabV3+ box based on a\npyramid transformer.\nobtained from nature. If we cannot accurately filter out the\nnoise signals, it will lead to sound distortion and affect the\nrecognition results of animal sounds [5]. This paper mainly\ninvestigates how to reduce noise components in bird sounds.\nInfluenced by the neural network [6]–[8], the audio signal is\nconverted into images via the Short-Time Fourier Transform\n(STFT) method. The image is then segmented to achieve the\npurpose of noise reduction via the deep learning method [9],\n[10], using samples from the natural environment to perform\naudio noise reduction.\nIn this paper, we aim to propose a robust segmentation\nmodel that is able to separate the clean audio from the mix-\nture audio. As shown in Fig. 1, our proposed model consists\nof three key modules: the STFT module, a PtDeepLab model,\nand the Inverse Short-Time Fourier Transform (ISTFT) mod-\nule. We convert the audio signal to an audio image using\nthe STFT module and then train the PtDeepLab model to\nsegment the clean audio signal region. After obtaining the\nclean audio signal, we finally applied the ISTFT module to\nreconstruct the denoised audio.\nOur contributions are three-fold:\n• We develop a novel deep visual transformer in the visual\nbird audio denoising model that transfers the audio\ndenoising to an image segmentation problem to achieve\nthe purpose of audio denoising by removing the noise\nregion in the audio image.\n• We propose a transformer-based encoder-decoder archi-\ntecture to capture and fuse the multi-scale represen-\ntation. The proposed PtDeepLab is based on a pyra-\nmid transformer and DeepLabV3+ box, which is a\ntransformer-based framework that achieves audio image\nsegmentation with varying levels of resolution.\n• We achieved new state-of-the-art audio denoising re-\nsults on the BirdSoundDenoising dataset, demonstrat-\ning the effectiveness of the proposed method and the\nenhanced ability to learn data and features.\nII. RELATED WORK\nThere are various audio denoising methods, which can be\ndivided into three categories according to the signal repre-\nsentation type: time-domain, frequency-domain, and time-\nfrequency domain. The vanilla audio denoising processes are\nas follows: extract the relevant features and convert them\ninto the correct format, then define the Fourier window to\ncalculate the Fourier transform of the signal, train the net-\nwork to produce an estimated value, and optimize the average\nvariance between the output and the target signal. Our work\nis related to three major research directions, and we highlight\nsome representative methods that are closely related to our\nwork.\nTraditional Audio Denoising Methods.They mainly rely\non estimating audio statistics [22]. Statistical methods like\nthe Gaussian mixture model can be used to build a de-\nnoising model of interest and recover clean audio from the\nnoisy input signal. Many methods use the STFT and the\nISTFT [17], which are time-domain algorithms, to solve the\naudio enhancement problem and treat audio enhancement\nas a filtering problem. The denoising performance can be\nimproved by the Wiener filter [11] or the LSA estimator [12].\nGradolewski et al. [58] used Gaussian white noise for sim-\nulation, using a wavelet-denoised algorithm to filter the\nreal phonocardiography (PCG) signal interference generated\nfrom signals recorded by a mobile device in a noisy environ-\nment. Linden et al. [25] decomposed a spectral graph into two\nmatrices: the spectral basis matrix and the encoding matrix.\nSpectral bases belonging to the same source are then grouped\naccording to the periodicity of the encoded information.\nFinally, the different sound sources are reconstructed based\non the clustering of the basis matrix and the corresponding\nencoding information. And then, the noise components are\nremoved to facilitate more accurate monitoring of biological\nsounds. Zha et al. [26] proposed a novel rank residual con-\nstraint (RRC) model for the rank minimization problem and\napplied it to image restoration tasks. In addition, some new\nimage restoration approaches were also presented, such as\n2 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhang et al.: DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising\nthe simultaneous nonlocal self-similarity priors method [27]\nand the group sparsity residual constraint with non-local\npriors method [28]. Haider et al. [29] proposed a filter-based\ndenoising scheme using the signal-to-noise ratio, which is\nmainly used to simulate different levels of addictive Gaussian\nnoise. For example, the frequency range is usually between\n1 kHz and 12 kHz [30], [31]. Some traditional methods of\nsound denoising have difficulty tracking target sound sources\nin multiple sources, which means that they cannot handle\nlong-term contexts [14], [34].\nDeep Learning Methods. For the comparison of the\ndeep learning model and the traditional supervised methods,\nAlamdar et al. [15] applied a full convolutional neural net-\nwork (FCN) to denoise the audio with only noise samples, re-\nflecting the superiority of the deep learning model. Germain\net al. [16] trained the FCN using deep feature loss, and their\nmodel can interfere with background noise to suppress the\nnoisy signals. Xu et al. [47] introduced a deep learning model\nwith automatic speech denoising, which can better capture\nnoise patterns. Saleem et al. [55] used an ideal binary mask\n(IBM) and the training DNNs to estimate the IBM, which is\nimportant for the audio enhancement of complex noise. The\nresult also showed that DNNs have a better ability to learn\ndata and features from a few samples. Xu et al. [35] proposed\na DNN-based supervised method to enhance the audio by\nfinding a mapping function between noisy and clean audio\nsamples. Madhav [49] proposed a Noise2Noise approach to\ntackle the problem of the heavy dependence on clean speech\ndata. Takuya et al. [45] proposed a training strategy that does\nnot require clean signals. Moreover, Tao et al. [42] presented\na method called Neighbor2Neighbor to train an effective\nimage-denoising model without only noisy images. Aswin\net al. [43] proposed self-supervised learning methods as a\nsolution to both zero- and few-shot personalization tasks.\nSonining et al. [37] investigated the performance of such a\ntime-domain network (Conv-TasNet) for speech denoising in\na real-time setting, comparing various parameters settings.\nVision Transformer. Transformer was originally pro-\nposed for natural language processing (NLP) tasks. Inspired\nby the success of transformers, which are currently state-of-\nthe-art across domains, including natural language process-\ning and computer vision (CV), transformers have gone viral\nin the field of speech processing [44]. Transformer has also\nbeen adapted for audio processing with CNNs. Some authors\nstack a transformer on top of a CNN, and they combine a\ntransformer and a CNN in each model block. Other efforts\ncombine CNNs with simpler attention modules. Kong et al.\n[19] presented CleanUNet, which is based on an encoder-\ndecoder architecture combined with self-attention blocks. It\nhas to be mentioned that Vision Transformer (ViT) is the\nfirst transformer-based approach that can match or even sur-\npass CNNs in image classification. Many variants of visual\ntransformers have also been proposed recently. Liu et al. [53]\nproposed a new vision transformer, called Swin Transformer,\nwhose architecture has the flexibility to model at various\nscales and has linear computational complexity concerning\nimage size. Chen et al. [20] proposed CrossVit, a dual-branch\ntransformer to combine image patches (i.e., tokens in a trans-\nformer) of different sizes to produce stronger image features.\nGu et al. [24] proposed HRViT, which enhances ViTs’ ability\nto learn semantically rich and spatially precise multi-scale\nrepresentations by integrating high-resolution multi-branch\narchitectures with ViTs. Recently, some transformer methods\nhave been used to solve audio processing problems. Gong\net al. [21] built the Audio Spectrogram Transformer (AST),\nthe first convolution-free, a purely attention-based model for\naudio classification.\nIn this work, we focus on a deep learning-based audio\ndenoising method using nature datasets. Our main inspiration\nfor this work is based on Zhang and Li [18], who were\nthe first to convert audio denoising into a visual image\nsegmentation problem. They first converted bird audio to\nimages using the STFT and proposed to segment the clean\naudio areas and remove the noised areas. Finally, they applied\nISTFT to convert segmented, clean audio images into audio\nto realize the purpose of audio denoising. However, they\ndid not propose any new segmentation models and left the\nspace to further improve the performance of their proposed\nBirdSoundDenoising datasets. Priyadarshani et al. [57] have\ndescribed a combination of denoising methods using wavelet\npacket decomposition and band-pass or low-pass filtering.\nTheir presented experiments demonstrate an order of magni-\ntude improvement over the noise reduction recorded by nat-\nural bird noise. However, their model still has lower perfor-\nmance in large-scale bird noise datasets. In this paper, we can\nconsider deleting the noise from the frequency domain. If we\ncan remove the frequency of the noise area, we can achieve\nthe purpose of noise reduction. Inevitably, there are always\nspecific noise frequencies interspersed with the frequency of\nbird sounds that cannot be removed. Therefore, the denoising\nmethod of removing the frequency has certain limitations.\nIn view of this problem, we propose a DeepLabV3+ Vision\nTransformer method to reduce noise areas and transform the\naudio noise reduction problem into an image segmentation\nproblem. After receiving a noisy input signal, we will convert\nit into an image and remove the noisy areas to extract a clean\nbird sound signal.\nIII. METHODS\nA. PROBLEM\nA noisy audio signal y(t) can be typically expressed as:\ny(t) =x(t) +ε(t) (1)\nwhere x(t) and ε(t) denote clean audio and additive noise\nsignals of time indext, respectively [14]. A sequence of noisy\nsignal and clean signal are defined as Y = {yi}N\ni=1 and X =\n{xi}N\ni=1, where N is the total number of audios. The goal of\naudio denoising is to extract the clean audio component X\nfrom the mixture audio signal Y by learning a mapping F\nand minimize the approximation error between the estimated\ndenoised audio F(Y ) and clean audio X. In our paper, we\nalso work on converting the audio denoising to an image\nVOLUME 4, 2016 3\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhang et al.: DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising\nsegmentation task. Given the audio images I = {Ii}N\ni=1\ncorresponding to Y , we aim to minimize the error between\nthe predictions of our image segmentation model PtDeepLab\n(I) and its ground truth labeled masks U = {ui}N\ni=1.\nB. MOTIVATION\nMost traditional filtering methods are limited to window-\nadding or masking operations in the frequency domain or\ntime domain. Because of the strong time-frequency coupling\nbetween the audio signal and noise, these filtering methods\nare difficult to use to achieve effective signal and noise\nseparation. Many existing deep audio denoising methods use\nclean audio signals as output signals or study the magnitude\nspectrum of the image to denoise. However, these methods\ncan be constrained by computing power or limited filtering\nimage areas, leading to low denoising performance. The sci-\nentific goal of this paper is to develop a novel, fully automatic\ndeep-learning denoising model that can discover differences\nbetween noisy and clean signal regions by digging into\naudio images. If the clean signal area can be successfully\nsegmented, the goal of audio denoising can be achieved. In\nsummary, given a noisy input signal, we aim to build a deep\nlearning model that can extract clean signals and return them\nto the user.\nC. PRELIMINARY\nIn our model, we aim to obtain the raw images for each bird\nsound by the STFT and reconstruct the denoised bird’s sound\nbased on the segmented bird sound image by the ISTFT.\n1) STFT Theory\nThe Short-Time Fourier Transform (STFT) and Inverse\nShort-Time Fourier Transform (ISTFT) are widely used in\nspeech analysis and processing. They are suitable for slow\nsignal and time-varying signal spectrum analysis [22]. The\naudio signal is non-stationary in most cases, meaning that\nthe mean and variance of the signal are not constant over\ntime. Therefore, it does not make much sense to calculate\nthe Fourier transform on the whole audio signal, so the\nFourier transform with window length and jump size values\nis proposed [23]. In this method, the audio signal is first\ndivided into frames. Then, each frame of the audio signal can\nbe intercepted from various fixed signal waveforms by the\nFourier transform, and the short-term spectrum of each frame\nis an approximation of the spectrum value of the smooth\nsignal waveform.\n2) Audio image construction.\nSTFT is a function of time t and frequency f, which shows\nhow the frequency of the speech signal changes with time.\nFig. 1 shows the conversion from the bird sound audio to its\naudio image. We can obtain the audio image (I) after imple-\nmenting the STFT calculation and the following equation,\nI = abs(STF Ty(t, f)) (2)\nwhere ST FTy(t, f) is the coefficient of STFT and abs takes\nthe absolute value from the complex frequency domain O.\n3) Denoised Audio Reconstruction\nAfter we remove the noise areas from the frequency domain,\nwe can apply ISTFT to reconstruct the denoised audio signal.\nFirstly, we need to filter out noise areas in O from the\nsegmentation model. The new frequent domain O′ is as\nfollow Eq. (3). More details of the ISTFT process are shown\nin the Fig. 1.\nO′ = O, and O′[\n∧\nu <1] = 0 (3)\nwhere\n∼\nu = F(I) is the predicted mask of the segmentation\nmodel given the input image I and we finally reconstruct the\ndenoised audio as follows:\nˆy(t) =IST F T(O′) (4)\nD. PROPOSED MODEL: PTDEEPLAB\nThe transformer method has gained wide attention in natural\nlanguage processing and computer vision in recent years\nthanks to global information modeling derived from the self-\nattention mechanism [24]. Previous studies have demon-\nstrated that both local and global features are essential for\ndepth models in dense prediction, such as the segmentation\nof complex structures. In this section, we will introduce\nhow to directly apply a transformer to image patch feature\nrepresentation coding and elaborate on the overall framework\nof PtDeepLab.\n1) Encoder-decoder Architecture\nAn overview of the PtDeepLab model is depicted in Fig. 1.\nOur proposed PtDeepLab extends DeepLabV3+ [54], [59]\nby employing pyramid transformer blocks in the encoder\nand decoder. Specifically, the encoder module encodes the\ninput image into a highly representative space by applying\na pyramid transformer on multiple scales to encode multi-\nscale contextual information. At the same time, the sim-\nple yet effective decoder is utilized as a series of pyramid\ntransformer blocks with path-expanding operations applied\nto reach the full resolution of images. Given an audio image\nI ∈ RH×W×C with a spatial resolution of H × W and C\nnumber of channels, Our goal is to predict the corresponding\nmask given the input image I. After segmenting the clean\nsound areas in the audio image, we could remove the noise\nfrom the audio signal.\nIn our implementation, given an input audio image of size\nH × W × 1, we first divide it into non-overlapping patches\nof size 8 × 8, and thus the feature dimension of each patch\nis 8 × 8 × 1 = 64. Each patch is treated as a \"token\",\nwhose feature is set as a concatenation of the raw pixel\nvalues. The pyramid transformer is applied to encode both\nlocal semantic and long-range contextual representations. To\nconstruct Pt Spatial Pyramid Pooling (PSPP), the pyramid\ntransformer block is designed to capture multi-scale infor-\nmation representation. The obtained multi-scale contextual\nrepresentations are fused into the decoding module through a\ncross-contextual attention mechanism. The cross-contextual\nattention block consists of a channel attention operation and a\n4 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhang et al.: DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising\nspatial attention operation, and they are applied to the tokens\n(derived from each level of the pyramid) to formulate the\nmulti-scale interaction [32]. In the final decoding process,\nwe first upsample bilinearly the extracted encoder high-level\nfeatures and concatenate them with the low-level features\nfrom the pyramid transformer backbone in the encoder to\nupdate the feature representation.\n2) Patch Embedding\nAn image I ∈ RH×W×C is split into a sequence of patches\nζ = [ζ1, ..., ζN ] ∈ RN×P2×C, where (P, P) is the patch\nsize, N = HW/P 2 is the number of patches. We map the\nvectorized patches ζp into a latent D-dimensional embed-\nding space using a trainable linear projection to produce a\nsequence of patch embeddings. To encode the patch spatial\ninformation, we learn specific position embeddings P os=\n[P os1, ..., P osN ] ∈ RN×D, which are added to the patch\nembeddings to retain positional information as follows:\nZ = [ζ1E; ζ2E, · · ·; ζN E] +EPos (5)\nwhere E ∈ R(P2·C)×D is the patch embedding projection,\nand EPos ∈ RN×D denotes the position embedding.\n3) Pyramid Transformer Block\nBecause of the traditional transformers’ single-scale, low-\nresolution representations, it is difficult for ViTs to imple-\nment dense prediction tasks such as semantic segmentation\nand effectively leverage the rich transformer layers in the\nencoder for excavating helpful multi-modal context. In ad-\ndition, these methods incur high computational and mem-\nory costs due to the global self-focus mechanism. Pyramid\ntransformer is designed to alleviate this problem [51], [52].\nThe key design feature of the pyramid transformer is to\ndesign a progressive shrinking pyramid and spatial-reduction\nattention (SRA). SRA is a substitute for a multi-head self-\nattention (MSA) module in the transformer block. Thus, each\npyramid transformer block comprises an attention layer and a\nfeed-forward layer with a LayerNorm (LN) layer, a two-layer\nMLP, and GELU nonlinearity. The spatial reduction attention\n(SRA) module is applied in series in the transformer block,\nas depicted in Fig. 2. With such a spatial reduction attention\nmodule, the successive pyramid transformer blocks can be\nexpressed as:\nSRA(Q, K, V) =Concat(head0, ··· , headNi)WO (6)\nheadj =Attention(QWQ\nj , SR(K)WK\nj , SR(V )WV\nj ) (7)\nwhere Concat(·) is the concatenation operation. WQ\nj , WK\nj ,\nWV\nj ∈ RCi×dhead and WO ∈ RCi×Ci are linear projection\nparameters. Ni is the head number of the attention layer in\nthe stage i. SR(·) is the operation for reducing the spatial\ndimension of the patch embedding, which is written as:\nSR(Z) =Norm(Reshape(Z, Ri)WS) (8)\nFIGURE 2: Schematic of the Transformer layer used in this\nwork.\nHere, Z ∈R(HiWi)×Ci represents a patch embedding, and\nRi denotes the reduction ratio of the attention layers in stage\ni. WS ∈ R(R2\ni Ci)×Ci is a linear projection that reduces the\ndimension of the patch embedding to Ci. Norm(·) refers to\nlayer normalization.\nWith such a spatial-reduction scheme, consecutive pyra-\nmid transformer blocks can be formulated as:\nˆZl = SRA(LN(Zl−1)) +Zl−1\nZl = MLP (LN( ˆZl)) + ˆZl (9)\nwhere ˆZl and Zl denote the output features of the SRA\nmodule and the MLP module for block Li, respectively. The\nself attention as in the ViTs Transformer [33] is computed\naccording to:\nAttention(Q, K, V) =SoftMax (QKT\n√\nd\n)V (10)\nwhereQ, K, V ∈RN×d are the query, key and value matri-\nces; d is the query/key dimension.\n4) Encoder\nDifferent from the CNN backbone networks, which use dif-\nferent convolution steps to obtain multi-scale feature maps,\nour model uses stacked pyramid transformer modules as the\nencoder. Furthermore, the pyramid transformer encoder has\nfour stages, which comprise some blocks at each stage. After\nthat, the embedded patches, along with a position embedding,\nare passed into some successive pyramid transformer blocks\nwith Li layers to generate hierarchical representations. Stage\n1, stage 2, stage 3, and stage 4 have layers of 3, 4, 6, and\n3, respectively. In the beginning, our PtDeepLab encoder\nfirst divides an input image into H\n8 × W\n8 patches and feeds\nthe flattened patches to a linear projection. The output is\nreshaped to a feature map M1 of size H\n8 × W\n8 × C1. To\nmaintain the hierarchical structure of the encoder, a patch\nmerging layer is utilized to decrease the resolution of feature\nrepresentations by a factor of 2 at the end of each stage. In\nthe same way as stage 1, using the feature map from the\nprevious stage as input, we obtain the following feature maps:\nM2, M3, and M4, whose strides are 16, 32, and 64 pixels\nwith respect to the input image. Then, through stacking a\nseries of pyramid transformer blocks, the spatial dimension\nof the feature graph is gradually reduced (similar to the CNN\nencoder), and the feature dimension is increased. The results\nare then fed into the PSPP module to capture multiscale\nrepresentations.\nVOLUME 4, 2016 5\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhang et al.: DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising\n5) Pt Spatial Pyramid Pooling\nThe PSPP block showed that regions of an arbitrary scale\ncould be accurately and efficiently classified by resampling\nconvolutional features extracted at a single scale. We have\nimplemented a variant of their scheme that uses multiple\nparallel atrous convolutional layers with different sampling\nrates. The extracted features for each sampling rate are\nfurther processed in separate branches and fused to gen-\nerate the final result. To capture contextual information at\nmultiple scales, compensate for spatial representations, and\nproduce multiscale representations, our PtDeepLab model\nutilizes a PSPP module that replaces the pooling operation\nwith atrous convolution. With stacked pyramid transformer\nblocks and subsequent patch merging layers (similar to the\ncontinuous downsampling operation in the CNN encoder),\nthe spatial resolution of the deep features extracted by the\npyramid transformer block is greatly reduced. Specifically,\nPtDeepLab applies several parallel convolution operations\nwith multiple different rates, making it possible to construct a\nfeature pyramid. To model a pooling operation such as a pure\ntransformer, we create a Pt spatial pyramid pooling (PSPP)\nblock with different window sizes to capture the multi-scale\nrepresentation. In our design, local and global information\nare captured by the smaller window and the larger window,\nrespectively. The final multi-scale representations are then\nfed to the cross-context attention module, where the common\nrepresentations are fused and captured using nonlinear tech-\nniques.\n6) Cross-Contextual Attention\nIn our model, a cross-attention module is applied to model\nmulti-scale interactions and incorporate pyramid features.\nWe assume that each level of the pyramid represents the\nobject of interest on different scales, thus, concatenating all\nthese features into a new dimension. We adopt a multi-scale\nrepresentation zP×NC\nall = [z1||z2 ···|| zM ], where || shows\nthe concatenation operation, N and C indicate the number\nof tokens and embedding dimension, and M denotes the\nlevel number of the pyramid. Second, considering the global\nrepresentation of each channel and the channel representation\nbetween pyramid levels, a proportional attention module is\napplied to adaptively emphasize the contribution of each\nfeature map and outperform the less disparate features. A\nchannel attention operation AT Tc and a spatial attention\noperation AT Ts can be formulated as:\nZ′ = AT Tc(Z) ⊗ Z\nZ′′ = AT Ts(Z′) ⊗ Z′ (11)\nThe channel attention operation AT Tc can be written in the\nfollowing equation.\nAT Tc(z) =σ(W1(Pmax(z)) +W2(Pavg(z))) ⊗ z (12)\nwhere z is the input feature map and σ is the Sigmoid activa-\ntions, Pmax and Pavg denote adaptive maximum pooling and\nadaptive average pooling functions, respectively.Wi, i∈ 1, 2\nshares parameters and consists of a convolutional layer with\n1 × 1 kernel size to reduce the channel dimension 12 times,\nfollowed by a ReLU layer and another 1 × 1 convolutional\nlayer to recover the original channel dimension.\nThe spatial attention operation AT Ts can be formulated as:\nAT Ts(z) =σ(K(Concat(Rmax(z), Ravg(z)))) ⊗ z (13)\nwhere Rmax and Ravg represent the maximum and average\nvalues obtained along the channel dimension, respectively.K\nis a 1 × 1 convolutional layer with padding set to 0.\n7) Decoder\nIn the decoder process, the obtained features corresponding\nto the attention module are first up-sampled by the pyramid\ntransformer block with a factor of 4 and then concatenated\nwith the low-level features. The scheme of concatenation of\nshallow and deep features together reduces the loss of spatial\ndetail with the help of the subsampling layer. Finally, a series\nof cascaded pyramid transformer blocks with path extension\noperations are applied to achieve the full resolution of H ×\nW.\n8) Objective function\nWe use the Dice loss function to optimize our proposed\nPtDeepLab model.\nDiceloss = 1− 2 × u ∩\n∼\nu\nu +\n∼\nu\n(14)\nThe overall training algorithm is shown in Alg. 1.\nAlgorithm 1 DeepLabV3+ Vision Transformer for Visual\nBird Sound Denoising. Batch of audio images: B(I) =\n{I1, ...,InB }, and their labeled mask images B(M) =\n{M1, ..., MnB }, where nB is the total number of batch. II\nis the number of iterations and k is one batch.\n1: Input: Noise audio signals Y = {yi}N\ni=1 and labeled\nmask images M = {mi}N\ni=1, where N is the total\nnumber of audios.\n2: Output: Denoised audio signals F(Y )\n3: Generate audio images I using Eq. (2)\n4: for iter = 1to II do\n5: for k = 1to nB do\n6: Derive batch-wise data: Ik and Mk sampled from\nI and M\n7: Optimize our segmentation model P tDeepLabus-\ning Eq. (14)\n8: end for\n9: end for\n10: Get the clean frequency domain using Eq. (3)\n11: Output the denoised audio signals using Eq. (4)\nIV. DATASETS\nA. DATASETS AND COMPARED MODELS\nTo test our proposed model, we show the performance of the\nBirdSoundsDenoising dataset.\n6 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhang et al.: DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising\n1) BirdSoundsDenoising\nThe BirdSoundsDenoising dataset, which has 14,120 audio\nsignals from one second to fifteen seconds and contains\n10,000/1,400/2,720 in training, validation, and testing, re-\nspectively, is a large-scale dataset of bird sounds. [18]. Unlike\nmany audio-denoising datasets, which have manually added\nartificial noise, this dataset contains many natural noises,\nincluding wind, waterfalls, rain, etc.\n2) Baseline models\nWe compare our results with other nine models, including\nU2 − Net [46], MTU-Net [41], Segmenter [39], U-Net [40],\nSegNet [38], DV AD [18], R-CED [48], Noise2Noise [49]\nand TS-U-Net [50]. For a fair comparison, we evaluate these\nmodels for both validation and test datasets.\nB. IMPLEMENTATION DETAILS\nWe implement our model using the PyTorch framework with\nan RTX A6000 GPU to speed up the computation. It took\nless than 0.5 seconds per audio image during the inference,\nwhile it took around 2 weeks of GPU time to train our\nmodel. The hyperparameter details are as follows: We used\nthe AdamW optimizer for pyramid transformer networks to\nupdate the network parameters. The learning rate is set to\n1e-4, and the weight decay is adjusted to 1e-4 too. Further,\nwe resize the input images to 512 × 512 × 3 with a mini-\nbatch size of 8 for 100 epochs. Nine different state-of-the-art\nsegmentation methods use the same training settings as the\nabove PtDeepLab.\nV. RESULTS\nIn this section, we compare our PtDeepLab model with\nexisting methods in terms of learning ability, capability, and\nqualitative results.\nA. EVALUATION METRICS\nWe employ three widely-used evaluation metrics, including\nF1, IoU, and Dice to evaluate the performance of image\nsegmentation [18]. For audio denoising, we use signal-to-\ndistortion ratio (SDR) to evaluate our model using Eq. (15).\nThe higher these four metrics, the better of segmentation\nmodel is.\nSDR = 10log10\n||u||2\n||eu − u||2 (15)\nB. EXPERIMENTS\nIn our experiments, we compare our model with another\nnine different baseline models on the BirdSoundsDenoising\ndataset to evaluate the performance of our proposed model.\nThe first six selected segmentation models in Tab. 1 also have\nan encoder-decoder structure. The encoder-decoder of these\ncomparison methods has a similar architecture to ours. To\ndemonstrate the superiority of our proposed model, we also\ncompare it with three other audio-denoising methods.\nTABLE 1: Results comparisons of different methods (F1, IoU, and\nDice scores are multiplied by 100. “−\" means not applicable.\nNetworks Validation Test\nF1 IoU Dice SDRF1 IoU Dice SDR\nU2-Net [46] 60.8 45.2 60.6 7.85 60.2 44.8 59.9 7.70\nMTU-NeT [41] 69.1 56.5 69.0 8.17 68.3 55.7 68.3 7.96\nSegmenter [39] 72.6 59.6 72.5 9.24 70.8 57.7 70.7 8.52\nU-Net [40] 75.7 64.3 75.7 9.44 74.4 62.9 74.4 8.92\nSegNet [38] 77.5 66.9 77.5 9.55 76.1 65.3 76.2 9.43\nDV AD [18] 82.6 73.5 82.6 10.33 81.6 72.3 81.6 9.96\nPtDeepLab 83.4 75.9 83.4 10.49 83.1 75.4 83.0 10.43\nR-CED [48] − − − 2.38 − − − 1.93\nNoise2Noise [49] − − − 2.40 − − − 1.96\nTS-U-Net [50] − − − 2.48 − − − 1.98\nTABLE 2: Ablation Results.\nModel F1 IoU Dice SDR\nDeepLabv3 82.6 73.5 82.6 10.33\nPyramid-transformer 79.4 72.4 80.5 10.14\nFull Model 83.4 75.9 83.4 10.49\nC. PERFORMANCE COMPARISONS\nFig. 3 shows the comparisons of six different segmentation\nmodels. The segmentation mask of the PtDeepLab model\nhas better performance than that of other models. The com-\npared results are tabulated in Tab. 1, which compares our\nbest results with those of previous state-of-the-art models.\nFour commonly used objective performance metrics from\nsection V-A are considered to evaluate the effectiveness of\nthe developed PtDeepLab method. We can observe that our\nmodel outperforms other competitors in terms of evaluation\nmetrics (F1, IoU, and Dice scores) for the BirdSoundsDe-\nnoising dataset among all segmentation models. It is worth\nnoting that three of the audio denoising methods (R-CED,\nNoise2Noise, and TS-U-Net) performed relatively lower than\nall other segmentation models. Furthermore, the SDR score\nof our PtDeepLab model achieves the highest value between\nthe average SDRs of all bird sounds in the validation and\ntest datasets. The comparisons of raw bird audio, ground\ntruth labeled denoised audio, and denoised audio from other\nmodels are shown in Fig. 4. Our model is also closer to the\nlabeled, denoised signal. As a result, these benchmarks vali-\ndate the effectiveness of our approach in segmenting images,\nand our model improves the audio-denoising performance of\nBirdSoundDenoising datasets.\nVI. DISCUSSION\nCompared to the CNN-based DeepLab model, our approach\nproduces better segmentation results and improves the repre-\nsentation ability of the model in context patterning by prob-\ning features at multiple scales to attain multi-scale informa-\ntion. What’s more, among six different state-of-the-art seg-\nmentation models and three deep audio denoising methods,\none obvious advantage of our model is its higher performance\nthan other methods. To further validate the effectiveness of\nthe proposed method, we performed an ablation analysis. In\nTab. 2, we can observe that ablation results for a DeepLabv3\nVOLUME 4, 2016 7\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhang et al.: DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising\nFIGURE 3: Segmentation results comparisons. Leftmost column is the original audio image. Ground truth is the labeled mask\nFIGURE 4: Denoising results comparisons. Raw audio is the original noise audio.\nmodel and a pyramid transformer model are compared to the\nfull PtDeepLab model, which demonstrates the effectiveness\nof the improved transformer. The compelling advantage of\nour model lies in the image segmentation section. We can\nmaintain the crucial clean signal via a segmented mask, as\nshown in Fig 3. Therefore, the novel proposed segmentation\nmodel successfully improves the performance of the already\nproposed birdsoundnoise dataset.\nVII. CONCLUSION\nIn this paper, we propose a novel DeepLabV3+ Vision\nTransformer model to remove the noise from the large-\nscale BirdSoundsDenoising dataset. Based on the DeepLab\nframework, the main body of PtDeepLab utilizes the pyramid\ntransformer backbone as an encoder to explicitly extract\nmore powerful and robust features. Extensive experimental\nresults demonstrate that the proposed model outperforms\nmany state-of-the-art methods, including CNN-based self-\nattention methods. As for future work, we look forward to\nevaluating DeepLavbV3+ Vision Transformer on other dense\nprediction vision tasks to stimulate more novel ideas for\nsolving the visual task and demonstrate the strength of this\nmodel as a vision backbone.\n8 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhang et al.: DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising\nREFERENCES\n[1] Y . Ephraim and D. Malah, “Speech enhancement using a minimum\nmean-square error log-spectral amplitude estimator,” IEEE transactions on\nacoustics, speech, and signal processing, vol. 33, no. 2, pp. 443–445, 1985.\n[2] S. Boll, “Suppression of acoustic noise in speech using spectral sub-\ntraction,” IEEE Transactions on acoustics, speech, and signal processing,\nvol. 27, no. 2, pp. 113–120, 1979.\n[3] P. Scalart et al., “Speech enhancement based on a priori signal to noise\nestimation,” in 1996 IEEE International Conference on Acoustics, Speech,\nand Signal Processing Conference Proceedings, vol. 2. IEEE, 1996, pp.\n629–632.\n[4] Y . Ephraim and H. L. Van Trees, “A signal subspace approach for speech\nenhancement,” IEEE Transactions on speech and audio processing, vol. 3,\nno. 4, pp. 251–266, 1995.\n[5] N. Krishnamurthy and J. H. Hansen, “Babble noise: modeling, analysis,\nand applications,” IEEE transactions on audio, speech, and language\nprocessing, vol. 17, no. 7, pp. 1394–1407, 2009.\n[6] Q. Kong, Y . Cao, T. Iqbal, Y . Wang, W. Wang, and M. D. Plumbley,\n“Panns: Large-scale pretrained audio neural networks for audio pattern\nrecognition,” IEEE/ACM Transactions on Audio, Speech, and Language\nProcessing, vol. 28, pp. 2880–2894, 2020.\n[7] J.-C. Hou, S.-S. Wang, Y .-H. Lai, Y . Tsao, H.-W. Chang, and H.-M. Wang,\n“Audio-visual speech enhancement using multimodal deep convolutional\nneural networks,” IEEE Transactions on Emerging Topics in Computa-\ntional Intelligence, vol. 2, no. 2, pp. 117–128, 2018.\n[8] A. A. Nugraha, A. Liutkus, and E. Vincent, “Multichannel audio source\nseparation with deep neural networks,” IEEE/ACM Transactions on Au-\ndio, Speech, and Language Processing, vol. 24, no. 9, pp. 1652–1664,\n2016.\n[9] I. R. I. Haque and J. Neubert, “Deep learning approaches to biomedical im-\nage segmentation,” Informatics in Medicine Unlocked, vol. 18, p. 100297,\n2020.\n[10] O. Habimana, Y . Li, R. Li, X. Gu, and G. Yu, “Sentiment analysis\nusing deep learning approaches: an overview,” Science China Information\nSciences, vol. 63, no. 1, pp. 1–36, 2020.\n[11] H. Lin, Y . Song, H. Wang, L. Xie, D. Li, G. Yang et al., “Multimodal brain\nimage fusion based on improved rolling guidance filter and wiener filter,”\nComputational and Mathematical Methods in Medicine, vol. 2022, 2022.\n[12] I. Cohen, “Optimal speech enhancement under signal presence uncertainty\nusing log-spectral amplitude estimator,” IEEE Signal processing letters,\nvol. 9, no. 4, pp. 113–116, 2002.\n[13] J. Xie, J. G. Colonna, and J. Zhang, “Bioacoustic signal denoising: a\nreview,” Artificial Intelligence Review, vol. 54, no. 5, pp. 3575–3597,\n2021.\n[14] A. Li, M. Yuan, C. Zheng, and X. Li, “Speech enhancement using pro-\ngressive learning-based convolutional recurrent neural network,” Applied\nAcoustics, vol. 166, p. 107347, 2020.\n[15] N. Alamdari, A. Azarang, and N. Kehtarnavaz, “Improving deep speech\ndenoising by noisy2noisy signal mapping,” Applied Acoustics, vol. 172,\np. 107631, 2021.\n[16] F. G. Germain, Q. Chen, and V . Koltun, “Speech denoising with deep\nfeature losses,” arXiv preprint arXiv:1806.10522, 2018.\n[17] K. Wang, B. He, and W.-P. Zhu, “Tstnn: Two-stage transformer based\nneural network for speech enhancement in the time domain,” in ICASSP\n2021-2021 IEEE International Conference on Acoustics, Speech and Sig-\nnal Processing (ICASSP). IEEE, 2021, pp. 7098–7102.\n[18] Y . Zhang and J. Li, “Birdsoundsdenoising: Deep visual audio denoising\nfor bird sounds,” in Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, 2023.\n[19] Q. Kong, H. Liu, X. Du, L. Chen, R. Xia, and Y . Wang, “Speech\nenhancement with weakly labelled data from audioset,” arXiv preprint\narXiv:2102.09971, 2021.\n[20] C.-F. R. Chen, Q. Fan, and R. Panda, “Crossvit: Cross-attention multi-\nscale vision transformer for image classification,” in Proceedings of the\nIEEE/CVF international conference on computer vision, 2021, pp. 357–\n366.\n[21] Y . Gong, Y .-A. Chung, and J. Glass, “Ast: Audio spectrogram transformer,”\narXiv preprint arXiv:2104.01778, 2021.\n[22] L. Wang, W. Zheng, X. Ma, and S. Lin, “Denoising speech based on deep\nlearning and wavelet decomposition,” Scientific Programming, vol. 2021,\n2021.\n[23] C. Mateo and J. A. Talavera, “Short-time fourier transform with the\nwindow size fixed in the frequency domain,” Digital Signal Processing,\nvol. 77, pp. 13–21, 2018.\n[24] J. Gu, H. Kwon, D. Wang, W. Ye, M. Li, Y .-H. Chen, L. Lai, V . Chan-\ndra, and D. Z. Pan, “Multi-scale high-resolution vision transformer for\nsemantic segmentation,” in Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, 2022, pp. 12 094–12 103.\n[25] T.-H. Lin, S.-H. Fang, and Y . Tsao, “Improving biodiversity assessment via\nunsupervised separation of biological sounds from long-duration record-\nings,” Scientific reports, vol. 7, no. 1, pp. 1–10, 2017.\n[26] Z. Zha, X. Yuan, B. Wen, J. Zhou, J. Zhang, and C. Zhu, “From rank\nestimation to rank approximation: Rank residual constraint for image\nrestoration,” IEEE Transactions on Image Processing, vol. 29, pp. 3254–\n3269, 2019.\n[27] Z. Zha, X. Yuan, J. Zhou, C. Zhu, and B. Wen, “Image restoration via\nsimultaneous nonlocal self-similarity priors,” IEEE Transactions on Image\nProcessing, vol. 29, pp. 8561–8576, 2020.\n[28] Z. Zha, X. Yuan, B. Wen, J. Zhou, and C. Zhu, “Group sparsity residual\nconstraint with non-local priors for image restoration,” IEEE Transactions\non Image Processing, vol. 29, pp. 8960–8975, 2020.\n[29] N. S. Haider, R. Periyasamy, D. Joshi, and B. Singh, “Savitzky-golay filter\nfor denoising lung sound,” Brazilian Archives of Biology and Technology,\nvol. 61, 2018.\n[30] A. Harma, “Automatic identification of bird species based on sinusoidal\nmodeling of syllables,” in 2003 IEEE International Conference on Acous-\ntics, Speech, and Signal Processing, 2003. Proceedings.(ICASSP’03).,\nvol. 5. IEEE, 2003, pp. V–545.\n[31] B. C. Pijanowski, L. J. Villanueva-Rivera, S. L. Dumyahn, A. Farina,\nB. L. Krause, B. M. Napoletano, S. H. Gage, and N. Pieretti, “Soundscape\necology: the science of sound in the landscape,” BioScience, vol. 61, no. 3,\npp. 203–216, 2011.\n[32] S. Woo, J. Park, J.-Y . Lee, and I. S. Kweon, “Cbam: Convolutional block\nattention module,” in Proceedings of the European conference on computer\nvision (ECCV), 2018, pp. 3–19.\n[33] H. Hu, Z. Zhang, Z. Xie, and S. Lin, “Local relation networks for image\nrecognition,” in Proceedings of the IEEE/CVF International Conference\non Computer Vision, 2019, pp. 3464–3473.\n[34] K. Tan and D. Wang, “A convolutional recurrent neural network for real-\ntime speech enhancement.” in Interspeech, vol. 2018, 2018, pp. 3229–\n3233.\n[35] Y . Xu, J. Du, L.-R. Dai, and C.-H. Lee, “A regression approach to speech\nenhancement based on deep neural networks,” IEEE/ACM Transactions on\nAudio, Speech, and Language Processing, vol. 23, no. 1, pp. 7–19, 2014.\n[36] J. H. Hansen, V . Radhakrishnan, and K. H. Arehart, “Speech enhancement\nbased on generalized minimum mean square error estimators and masking\nproperties of the auditory system,” IEEE Transactions on Audio, Speech,\nand Language Processing, vol. 14, no. 6, pp. 2049–2063, 2006.\n[37] S. Sonning, C. Schüldt, H. Erdogan, and S. Wisdom, “Performance study\nof a convolutional time-domain audio separation network for real-time\nspeech denoising,” in ICASSP 2020-2020 IEEE International Conference\non Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2020, pp.\n831–835.\n[38] V . Badrinarayanan, A. Kendall, and R. Cipolla, “Segnet: A deep con-\nvolutional encoder-decoder architecture for image segmentation,” IEEE\ntransactions on pattern analysis and machine intelligence, vol. 39, no. 12,\npp. 2481–2495, 2017.\n[39] R. Strudel, R. Garcia, I. Laptev, and C. Schmid, “Segmenter: Transformer\nfor semantic segmentation,” in Proceedings of the IEEE/CVF International\nConference on Computer Vision, 2021, pp. 7262–7272.\n[40] O. Ronneberger, P. Fischer, and T. Brox, “U-net: Convolutional networks\nfor biomedical image segmentation,” in International Conference on Medi-\ncal image computing and computer-assisted intervention. Springer, 2015,\npp. 234–241.\n[41] H. Wang, S. Xie, L. Lin, Y . Iwamoto, X.-H. Han, Y .-W. Chen, and\nR. Tong, “Mixed transformer u-net for medical image segmentation,” in\nICASSP 2022-2022 IEEE International Conference on Acoustics, Speech\nand Signal Processing (ICASSP). IEEE, 2022, pp. 2390–2394.\n[42] T. Huang, S. Li, X. Jia, H. Lu, and J. Liu, “Neighbor2neighbor: Self-\nsupervised denoising from single noisy images,” in Proceedings of the\nIEEE/CVF conference on computer vision and pattern recognition, 2021,\npp. 14 781–14 790.\n[43] A. Sivaraman and M. Kim, “Efficient personalized speech enhancement\nthrough self-supervised learning,” IEEE Journal of Selected Topics in\nSignal Processing, vol. 16, no. 6, pp. 1342–1356, 2022.\n[44] W. Yu, J. Zhou, H. Wang, and L. Tao, “Setransformer: speech enhancement\ntransformer,” Cognitive Computation, pp. 1–7, 2022.\nVOLUME 4, 2016 9\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/\nZhang et al.: DeepLabV3+ Vision Transformer for Visual Bird Sound Denoising\n[45] T. Fujimura, Y . Koizumi, K. Yatabe, and R. Miyazaki, “Noisy-target\ntraining: A training strategy for dnn-based speech enhancement without\nclean speech,” in 2021 29th European Signal Processing Conference\n(EUSIPCO). IEEE, 2021, pp. 436–440.\n[46] X. Qin, Z. Zhang, C. Huang, M. Dehghan, O. R. Zaiane, and M. Jagersand,\n“U2-net: Going deeper with nested u-structure for salient object detection,”\nPattern recognition, vol. 106, p. 107404, 2020.\n[47] R. Xu, R. Wu, Y . Ishiwaka, C. V ondrick, and C. Zheng, “Listening to\nsounds of silence for speech denoising,” Advances in Neural Information\nProcessing Systems, vol. 33, pp. 9633–9648, 2020.\n[48] S. R. Park and J. W. Lee, “A fully convolutional neural network for speech\nenhancement,” Proc. Interspeech 2017, pp. 1993–1997, 2017.\n[49] M. M. Kashyap, A. Tambwekar, K. Manohara, and S. Natarajan, “Speech\ndenoising without clean training data: A noise2noise approach,” Proc.\nInterspeech 2021, pp. 2716–2720, 2021.\n[50] E. Moliner and V . Välimäki, “A two-stage u-net for high-fidelity denoising\nof historical recordings,” in ICASSP 2022-2022 IEEE International Con-\nference on Acoustics, Speech and Signal Processing (ICASSP). IEEE,\n2022, pp. 841–845.\n[51] B. Dong, W. Wang, D.-P. Fan, J. Li, H. Fu, and L. Shao, “Polyp-pvt:\nPolyp segmentation with pyramid vision transformers,” arXiv preprint\narXiv:2108.06932, 2021.\n[52] W. Wang, E. Xie, X. Li, D.-P. Fan, K. Song, D. Liang, T. Lu, P. Luo,\nand L. Shao, “Pyramid vision transformer: A versatile backbone for\ndense prediction without convolutions,” in Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, 2021, pp. 568–578.\n[53] Z. Liu, Y . Lin, Y . Cao, H. Hu, Y . Wei, Z. Zhang, S. Lin, and B. Guo, “Swin\ntransformer: Hierarchical vision transformer using shifted windows,” in\n2021 IEEE/CVF International Conference on Computer Vision (ICCV),\n2021, pp. 9992–10 002.\n[54] R. Azad, M. Heidari, M. Shariatnia, E. K. Aghdam, S. Karimijafarbigloo,\nE. Adeli, and D. Merhof, “Transdeeplab: Convolution-free transformer-\nbased deeplab v3+ for medical image segmentation,” in International\nWorkshop on PRedictive Intelligence In MEdicine. Springer, 2022, pp.\n91–102.\n[55] N. Saleem and M. I. Khattak, “Deep neural networks for speech en-\nhancement in complex-noisy environments,” Int. J. Interact. Multim. Artif.\nIntell., vol. 6, pp. 84–90, 2020.\n[56] S. R. Park and J. Lee, “A fully convolutional neural network for speech\nenhancement,” arXiv preprint arXiv:1609.07132, 2016.\n[57] N. Priyadarshani, S. Marsland, I. Castro, and A. Punchihewa, “Birdsong\ndenoising using wavelets,” PloS one, vol. 11, no. 1, p. e0146790, 2016.\n[58] D. Gradolewski and G. Redlarski, “Wavelet-based denoising method for\nreal phonocardiography signal recorded by mobile devices in noisy envi-\nronment,” Computers in biology and medicine, vol. 52, pp. 119–129, 2014.\n[59] L.-C. Chen, G. Papandreou, I. Kokkinos, K. Murphy, and A. L. Yuille,\n“Deeplab: Semantic image segmentation with deep convolutional nets,\natrous convolution, and fully connected crfs,” IEEE transactions on pattern\nanalysis and machine intelligence, vol. 40, no. 4, pp. 834–848, 2017.\nJUNHUI LI was born in 1994. He is currently\npursuing a Master’s degree in the Department of\nMathematics, School of Science, University of\nScience and Technology Liaoning (USTL), An-\nshan, China. His research interests include deep\nlearning, computer vision, and speech enhance-\nment.\nPU WANGwas born in 2002. She is currently pur-\nsuing her undergraduate degree in Information and\nComputational Science at the School of Science,\nUniversity of Science and Technology Liaoning\n(USTL), Anshan, China. Her research interests\ninclude deep learning and audio denoising.\nYOUSHAN ZHANG is an assistant professor in\nComputer Science and Artificial Intelligence at\nYeshiva University. He was a Postdoc at Cornell\nUniversity and received his Ph.D. degree in Com-\nputer Science at Lehigh University. His research\ninvolves artificial intelligence, machine learning,\ncomputer vision, transfer learning, manifold learn-\ning, and shape analysis.\n10 VOLUME 4, 2016\nThis article has been accepted for publication in IEEE Access. This is the author's version which has not been fully edited and \ncontent may change prior to final publication. Citation information: DOI 10.1109/ACCESS.2023.3294476\nThis work is licensed under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 License. For more information, see https://creativecommons.org/licenses/by-nc-nd/4.0/"
}