{
    "title": "Privacy Regularization: Joint Privacy-Utility Optimization in Language Models",
    "url": "https://openalex.org/W3136159396",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4222946048",
            "name": "Mireshghallah, Fatemehsadat",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282362153",
            "name": "Inan, Huseyin A.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287339285",
            "name": "Hasegawa, Marcello",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282819262",
            "name": "R√ºhle, Victor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221943472",
            "name": "Berg-Kirkpatrick, Taylor",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3209426591",
            "name": "Sim, Robert",
            "affiliations": []
        },
        {
            "id": null,
            "name": "R\\\"uhle, Victor",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2132339004",
        "https://openalex.org/W2906993533",
        "https://openalex.org/W3003636647",
        "https://openalex.org/W3110164654",
        "https://openalex.org/W179875071",
        "https://openalex.org/W3130178918",
        "https://openalex.org/W2963999993",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W2963879260",
        "https://openalex.org/W3092700804",
        "https://openalex.org/W3012220842",
        "https://openalex.org/W2911978475",
        "https://openalex.org/W3018763859",
        "https://openalex.org/W2952604841",
        "https://openalex.org/W3190860428",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W2990751682",
        "https://openalex.org/W2905148628",
        "https://openalex.org/W3094874121",
        "https://openalex.org/W3087503988",
        "https://openalex.org/W2970716886",
        "https://openalex.org/W2795435272",
        "https://openalex.org/W2889507104",
        "https://openalex.org/W3046518446",
        "https://openalex.org/W2760781482",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W1532499126",
        "https://openalex.org/W2947160092",
        "https://openalex.org/W2946930197"
    ],
    "abstract": "Neural language models are known to have a high capacity for memorization of training samples. This may have serious privacy implications when training models on user content such as email correspondence. Differential privacy (DP), a popular choice to train models with privacy guarantees, comes with significant costs in terms of utility degradation and disparate impact on subgroups of users. In this work, we introduce two privacy-preserving regularization methods for training language models that enable joint optimization of utility and privacy through (1) the use of a discriminator and (2) the inclusion of a triplet-loss term. We compare our methods with DP through extensive evaluation. We show the advantages of our regularizers with favorable utility-privacy trade-off, faster training with the ability to tap into existing optimization approaches, and ensuring uniform treatment of under-represented subgroups.",
    "full_text": "Privacy Regularization: Joint Privacy-Utility Optimization in Language\nModels\nFatemehsadat Mireshghallah1‚àó, Huseyin A. Inan3, Marcello Hasegawa2,\nVictor R√ºhle2, Taylor Berg-Kirkpatrick1, Robert Sim3\n1 University of California San Diego, 2 Microsoft Corporation, 3 Microsoft Research\n{fatemeh, tberg}@ucsd.edu,\n{huseyin.inan, marcellh, virueh, rsim}@microsoft.com\nAbstract\nNeural language models are known to have\na high capacity for memorization of training\nsamples. This may have serious privacy im-\nplications when training models on user con-\ntent such as email correspondence. Differen-\ntial privacy (DP), a popular choice to train\nmodels with privacy guarantees, comes with\nsigniÔ¨Åcant costs in terms of utility degrada-\ntion and disparate impact on subgroups of\nusers. In this work, we introduce two privacy-\npreserving regularization methods for training\nlanguage models that enable joint optimiza-\ntion of utility and privacy through (1) the use\nof a discriminator and (2) the inclusion of a\nnovel triplet-loss term. We compare our meth-\nods with DP through extensive evaluation. We\nshow the advantages of our regularizers with\nfavorable utility-privacy trade-off, faster train-\ning with the ability to tap into existing op-\ntimization approaches, and ensuring uniform\ntreatment of under-represented subgroups.\n1 Introduction\nNeural language models (Bengio et al., 2003;\nMikolov et al., 2010) have recently seen signif-\nicant gains in capabilities, and are deployed at\nscale in several real-world scenarios (Chen et al.,\n2019; Adam et al., 2020). Training these models\non domain-speciÔ¨Åc user data can further improve\ntheir utility. The volume of data required, cou-\npled with the inherent sparsity of natural language\nwhich often means all data are unique, opens the\ndoor to an array of privacy attacks against mod-\nels and their training data. Sample memorization\nposes a substantial risk by enabling model inver-\nsion attacks (Carlini et al., 2020; Ramaswamy et al.,\n2020; Inan et al., 2021). In these attacks, a curious\nor malevolent user can query a pre-trained language\nmodel on any data record with the intention of re-\nconstructing (parts of) training samples1.\n‚àó Work done as part of an MSR internship.\n1A na√Øve example is an attacker querying ‚ÄúMy account\nnumber is‚Äù and hoping to receive a user‚Äôs account number.\nDifferential Privacy (DP) (Dwork, 2006) is the\ngold standard approach to address this issue, thanks\nto its strong and rigorous privacy guarantees. DP-\nSGD (Abadi et al., 2016) is a popular method to\ntrain neural models with differential privacy guar-\nantees and it works by clipping of the gradients and\naddition of noise in each update, which provides\nworst-case guarantees that reÔ¨Çect the likelihood of\nleaking any attribute of any member of the dataset\ninto the trained model. The worst-case guaran-\ntees of differential privacy are not customizable, in\nother words, they cannot be relaxed to protect only\ncertain attributes. Therefore, DP incurs signiÔ¨Åcant\nloss to model utility (Tram√®r and Boneh, 2020).\nDP training of models is also much slower, with\ncumbersome hyper-parameter tuning and develop-\nment (Wu et al., 2017; Subramani et al., 2020). It\nhas also been shown that DP‚Äôs utility loss is much\nworse for under-represented groups (Bagdasaryan\net al., 2019; Farrand et al., 2020), which can have\nÔ¨Ånancial and societal ramiÔ¨Åcations (Pujol et al.,\n2020).\nTo address these issues, we relax the strong\nassumptions of the DP threat model and assume\nan adversary with Ô¨Ånite-capacity (Ô¨Ånite statistical,\ncompute, and side information) who attempts to\nrecover sensitive user-level information from the\ntrained model (Carlini et al., 2019). We propose\ntwo privacy regularization methods, one based on\nadversarial training and another on a novel privacy\nloss term, to jointly optimize for privacy and utility\nof language models. The main idea of our regulariz-\ners is to prevent the last hidden state representation\nof the language model for an input sequencexfrom\nbeing linked back to the sensitive attribute we are\ntrying to protect, in our case, the identity of the\nauthor. We use the last hidden state as it corre-\nsponds to the embedding of the sequence x.2 We\n2Although we consider recurrent neural network-based\nlanguage models in this work, our approach is applicable in\ntransformer-based language models as well. In the latter, one\narXiv:2103.07567v2  [cs.LG]  16 Apr 2021\nTraining Sample(ùíô)IamtravelingonMonday\nAuthor Discriminator\nLanguage Model (LM) ZaraAlexAliceProbability\nùúÉ#ùúÉ$%\n‚Ñí!\"#$%=‚àílogPr(ùë•;ùúÉ&')\n‚Ñé( ‚Ñí&'()=‚àílogPr(ùë¶|‚Ñé*;ùúÉ#)‚Ñí+,'-=ùêæùêø(ùëù#|ùëàùëõùëñùëìùëúùëüùëö)\nùëù)=Pr(0|‚Ñé(;ùúÉ)) Adversarial Training:LM Optimization:ùëöùëñùëõ.!\"‚Ñí+,'()ùë•;ùúÉ$%+ùúÜ‚Ñí+,'-(‚Ñé*;ùúÉ#)Discriminator Optimization:ùëöùëñùëõ.#‚Ñí&'()(‚Ñé*,ùë¶;ùúÉ#)\nFigure 1: WorkÔ¨Çow of our adversarial training regularization. The last hidden state ( hx) of the LM is fed to the\ndiscriminator to generate a distribution over the authors (pd). pd is used to compute LLM-P , the privacy loss.\nconsider the linkability of the input representation\nto the sensitive attribute (author) as a proxy since\nit is commensurate with the linked and linkable\ninformation deÔ¨Ånitions in the General Data Protec-\ntion Regulation (GDPR Article 29 Working Party,\n2014). By framing privacy as an optimization prob-\nlem, we can apply the well-developed machinery of\nlarge-scale gradient-based optimization, enabling\nus to train models at scale while jointly tuning for\nan optimal privacy-utility trade-off.\nTo validate our approach, we develop an evalu-\nation framework for assessing a model‚Äôs privacy\nloss. We employ the exposure metric introduced\nin (Carlini et al., 2019) and introduce a reconstruc-\ntion (tab) attack as a realistic scenario to evaluate\nand compare LSTM language models trained us-\ning our regularization with those trained with dif-\nferential privacy, on Avocado (Oard et al., 2015)\nand Reddit (V√∂lske et al., 2017) datasets. We also\nempirically demonstrate that, unlike DP, our tech-\nnique does not have disparate impacts on under-\nrepresented groups.\nOur work is closely related to (Coavoux et al.,\n2018) and (Li et al., 2018). Coavoux et al. consider\nan attacker who eavesdrops on the hidden represen-\ntations of a pre-trained model during inference and\ntries to recover information about the input text.\nAdversarial training is used as a mitigation to re-\nduce the attacker‚Äôs performance (Wang et al., 2019).\nLi et al. use adversarial training to protect private\nauthor attributes such as age or gender, in learned\ntext representations for part-of-speech tagging and\nsentiment analysis to gain better performance on\nout-of-domain corpora. We, on the other hand, use\nadversarial training and a triplet-based regulariza-\ntion to train private language models that do not\nmemorize sensitive user information, which has\nnot been explored before. We evaluate our models\naccordingly, by trying to extract training samples.\nPrior work has studied membership inference at-\ntacks against models (Shokri et al., 2017; Yeom\ncan consider the representation corresponding to the special\ntoken [CLS] as the embedding of the sequence x.\net al., 2018; Song and Shmatikov, 2019), however,\nour regularizations do not target these attacks.\n2 Approach\nIn this section, we explain our proposed regulariz-\ners and training techniques in more detail.\n2.1 Adversarial Training\nFigure 1 shows our Ô¨Årst proposed regularizer which\nis adversarial in nature. We feed an input text se-\nquence x to the language model and extract the\nlast hidden state representation of the model for\nx; denoted by hx. hx is then fed to a discrimi-\nnator parameterized by Œ∏d, which plays the role\nof an attacker who attempts to predict what the\nsensitive label (in our case, the author, y) for xis.\nThe output probability distribution of the discrim-\ninator for the input hx, pd = Pr(¬∑|hx; Œ∏d) is then\nused to compute both the privacy lossLLM-P of the\nlanguage model and the discriminator loss LD-CE .\nDuring training, the discriminator optimizes for\nbetter linking of the last hidden state representa-\ntions to the authors. Thus, the discriminator loss\nis LD-CE (hx,y; Œ∏d) = ‚àílog Pr(y|hx; Œ∏d). Con-\nversely, the language model optimizes Œ∏lm such\nthat it (1) improves the utility of the language\nmodel and (2) Ô¨Çattens the probability distribution\nover authors. Thus, we devise the following loss\nfunction:\nLLM (x; Œ∏d,Œ∏lm)= LLM-CE + ŒªLLM-P (1)\nLLM-CE is the utility loss, for which we use con-\nventional cross entropy loss over the next-word\npredictions. LLM-P is the privacy loss:\nLLM-P (hx; Œ∏d) =‚àí1\nM\nM‚àë\nc=1\nlog Pr(c|hx; Œ∏d) (2)\ni.e. the KL divergence between the distribution\nover authors and the uniform distribution where\nM is the number of classes (authors). The goal of\nthis term is to drive the discriminator to predict ran-\ndomly uniform outputs (Raval et al., 2019). The\nreason we devised this loss as opposed to using\n0\n50\n100\n150\n200\n0 10 20 30 40 50\nExposure\nCanary Repetition\nUnmitigated\nDP\nTriplet\nAdversarial\n(a) Avocado - High PPL (‚àº 100)\n0\n50\n100\n150\n200\n0 10 20 30 40 50\nExposure\nCanary Repetition\nUnmitigated\nDP\nTriplet\nAdversarial (b) Avocado - Low PPL (‚àº 60)\n0\n20\n40\n60\n80\n0 5 10 15 20\nExposure\nCanary Repetition\nUnmitigated\nDP\nAdversarial\nTriplet (c) Reddit (PPL ‚àº 100)\nFigure 2: Exposure metric results for different training schemes at similar perplexities. Unmitigated denotes\nconventional training. Adversarial and Triplet are our regularizers. Higher exposure indicates lower privacy.\n‚àíLD-CE is that we do not just want the discrimina-\ntor to assign zero probability to the correct author,\nwe want pd to be uniform so that it has no infor-\nmation about the correct author. Hyperparameter Œª\nallows for trading off privacy and utility.\n2.2 Triplet-based Loss Function\nOne potential downside of the proposed adversarial\nregularizer is that the capacity of the discriminator\nmust scale with the number of authors, and thus the\nsize of the training data. To better accommodate\nthe larger number of authors in large datasets, we\ninvestigate another regularizer that does not require\na discriminator. We build on the intuition that to\nobfuscate an attribute, we can increase the distance\nbetween representations of samples that have the\nsame label for that attribute while decreasing the\ndistance between samples with different labels. To\nthis end, we use the language model loss (LLM ) of\nthe previous section (Eq 1), and we set the privacy\nloss to be the triplet loss:\nLLM-P = ‚à•hx ‚àíhp‚à•2 ‚àí‚à•hx ‚àíhn‚à•2 (3)\nThe triplet loss is commonly used in vision tasks\nfor training embeddings that map images from the\nsame category to neighboring points in the embed-\nding space (Chechik et al., 2010). We, however,\ninvert this loss and use it for an opposite purpose:\nprivacy regularization. During the training of the\nlanguage model, we select a ‚Äúbaseline sample‚Äù,x,\na positive sample p(with different sensitive label)\nand a negative sample n(with the same sensitive\nlabel) and feed them through the language model\nand extract the last hidden states hx, hp and hn,\nrespectively. We Ô¨Ånd the l2 distance between hx,\nhp, and hn and based on their labels, add them\nto or subtract them from the loss. To implement\nthis, in practice, we sample a baseline batch and a\nsecond ‚Äúauxiliary‚Äù batch during training. We feed\nboth the baseline batch (x) and the auxiliary batch\n(a) through the language model and extract the last\nhidden states. We then calculate the distance be-\ntween the last hidden states of the corresponding\nsamples in the two batches. If the samples have\ndifferent labels for the sensitive attribute (author),\nwe add their distance to the loss, otherwise, we\nsubtract it. The privacy loss becomes:\nLLM-P =\n‚àë\ni:yxi =yai\n‚à•hxi ‚àíhai ‚à•2 ‚àí\n‚àë\nj:yxj Ã∏ =yaj\n‚à•hxj ‚àíhaj ‚à•2\n(4)\n3 Evaluation\nIn our experiments, we use a subset of the Avocado\ncorporate email dataset (Oard et al., 2015) with 100\nusers and 60,000 samples and a subset of Reddit\ndataset (V√∂lske et al., 2017) with 10,000 users and\n3 million samples. Both of these datasets are in\nEnglish, covering formal and informal writing. We\ncreate a 80/20% training/test set split. We use a\ntwo-layer LSTM model as the language model for\nthe next-word prediction task. We compare models\ntrained with our proposed regularizer to differen-\ntially private (DP) ones (Abadi et al., 2016). For the\nprivacy accounting, we use Gaussian differential\nprivacy (Bu et al., 2019). We use language model\nperplexity as a measure of utility. Due to space limi-\ntations, we focus evaluations on privacy metrics for\nseveral set levels of achieved test perplexity, listed\nin Table 1 in the appendix. See appendix A.2 for a\nmore detailed description of the experimental setup\nand extra analysis of overheads and complexity of\neach regularizer.\nPrivacy measurements w/ exposure metric.\nTo empirically compare the privacy of our meth-\nods to that of DP, we adopt the exposure metric\nintroduced in (Carlini et al., 2019). The higher the\nexposure of a sequence, the more the model‚Äôs mem-\norization and the easier it is to extract the sequence\nfrom the language model. To measure exposure we\ninsert sequences of Ô¨Åve random words (canaries) to\nthe training data (appendix A.5). We insert unique\n0%\n1%\n2%\n3%\n4%\n5%\nUnmitigated DP Triplet Adversarial\nAttack Accuracy\nSynthesized Canary\nReal Canary\n(a) Avocado - High PPL (‚àº 100)\n0%\n10%\n20%\n30%\n40%\n50%\n60%\n70%\n80%\nUnmitigated DP Triplet Adversarial\nAttack Accuracy\nSynthesized Canary\nReal Canary (b) Avocado - Low PPL (‚àº 60)\n0\n10\n20\n30\n40\n50\n60\n70\n80\nDP Triplet Adversarial\nUtility Drop \n(Perplexity Growth)\nTop-5 Low-5 (c) Impact on Different Subgroups\nFigure 3: (a, b) Tab attack results for reconstructing canary sequences for two utility levels. Higher attack accu-\nracy indicates lower privacy. (c) Effect of different mitigations on utility of well represented (Top-5) and under-\nrepresented (Low-5) users for Avocado dataset.\ncanaries with different repetitions for each user,\nand measure the exposure of these canaries.\nFigure 2 shows the exposure results per canary\nrepetition. These results are averaged over all the\nusers. In each sub-Ô¨Ågure, the perplexities of the\nmodels are similar, hence we can compare the pri-\nvacy levels at similar utilities. Fig. 2a compares\ntrained models using different techniques on the\nAvocado dataset, where they all have relatively high\nperplexities compared to a fully trained conven-\ntional model (Table 1). Fig. 2b has the same setup,\nhowever, the models have lower perplexities. Nat-\nurally, for having better utility we are trading off\nprivacy, which can be seen by comparing the expo-\nsure values in these two Ô¨Ågures and observing that\nthe second one has higher exposure values (lower\nprivacy). Finally, Fig. 2c shows the exposure re-\nsults for Reddit.\nIn all cases, we see that the unmitigated model\nhas the highest exposure, as expected. We also ob-\nserve that for canaries (patterns) that are repeated\nmore than 9 times (for each user), our mitigation\noffers lower exposure compared to DP, especially\nin the high perplexity case. This is because clip-\nping and noise addition in DP is attribute and data-\nagnostic, meaning that noise is added to all sam-\nples regardless of whether or not they contain sen-\nsitive information. Therefore, repeated patterns\nare less protected. If we want to protect a pattern\nwith nrepetitions, we would need to apply noise\nthat is n√ólarger, which would degrade the utility\ngravely and would not yield the same perplexity.\nFor lower repetition canaries, our mitigations have\ncomparable performance to DP. For all these exper-\niments the Gaussian differential privacy criterion ¬µ\nis extremely large (1020), which practically yields\nœµ ‚àº‚àû. We also experimented with lower œµval-\nues (e.g. œµ ‚àº7), however, it yields a model with\nperplexity of 650, having an extremely low utility.\nPrivacy measurements w/ tab attack accu-\nracy. In this experiment, we input the Ô¨Årst token,\nand see if the entire sequence is reconstructed using\nthe language model. We report the rate of correct\nreconstruction of canaries as the accuracy of the\nattack. We use the synthetic canaries from the pre-\nvious experiment, and also select ‚Äúreal canaries‚Äù\nfrom the training corpus to create a real-world sce-\nnario. Fig. 3a shows that for a high perplexity\nmodel, the accuracy of the tab attack on synthe-\nsized canaries is very small, even for the unmiti-\ngated model. The unmitigated model reaches the\ndesignated perplexity in less than an epoch, and\nhence it does not memorize the canaries. For the\nreal canaries, however, the memorization is higher,\nsince they follow grammatical rules. In the lower\nperplexity case of Fig. 3b, we see that the synthe-\nsized canaries are mostly memorized by the un-\nmitigated model. Our mitigations outperform DP,\nespecially for the synthesized canaries. DP is not\ncontext-sensitive and applies the same amount of\nnoise to all samples, thereby leaving correlated and\nhigher repeated samples less-protected. Our mitiga-\ntions, however, learn what sequences are link-able\nto their authors, and obfuscate them such that they\nno longer leak the ‚Äúidentifying‚Äù secret.\nEffect on under-represented users.Differen-\ntial privacy has disparate impact on the accuracy\nof different subgroups of the dataset (Bagdasaryan\net al., 2019). Here, we want to measure the ef-\nfect of our mitigations on the utility of the model\namong users with various data samples. For each\nuser, we measure the average perplexity of the\nmodel for their samples in the test set, and then\nsubtract this from the same value for an unmiti-\ngated model. This would yield the average drop\nin utility, per user. We compare the utility drop of\nwell-represented users to under-represented ones\nby taking the top 5 users with the most samples\nand the bottom 5 users with the fewest samples\nfrom Avocado dataset. We then measure the aver-\nage utility drop over each group of 5 users on the\ntest set. Figure 3c shows these results. We see that\ndifferential privacy has disparate impact, 29 points,\non the two sub-groups of users (authors), whereas\nthis gap is only 7 points for models trained with\nour mitigations.\nIt‚Äôs important to remember that in general, dis-\ntinguishing ‚Äúunder-represented‚Äù users from those\nwhose data is similar to others but who have con-\ntributed fewer samples is a difÔ¨Åcult task. However,\nfor Figure 3c‚Äôs results, if these users‚Äô data came\nfrom the same distribution as the ones with lots\nof samples (i.e. if these people were merely less-\ncontributing), the utility loss would be similar for\nall groups when applying ‚Äúuser-level‚Äù DP (what\nwe use). DP‚Äôs disparate impact on the utility loss\nfor these two groups suggests that, in our case, the\nless-contributing authors are probably also under-\nrepresented.\n4 Conclusion\nThis work introduces two privacy mitigation meth-\nods to jointly optimize for privacy and utility. Ex-\ntensive experiments show that our approach pro-\nvides comparable and in certain cases a higher level\nof privacy compared to differentially private model\ntraining. We further empirically demonstrate, that\nour methods do not exhibit disparate impacts on\nunder-represented groups and have signiÔ¨Åcantly\nless overhead on training performance.\nEthical Considerations\nThe Avocado corpus is licensed for research appli-\ncations under strict terms intended to protect the\nprivacy of the correspondents. While the end-user\nlicense agreement does not indicate what consent\nwas granted by the participants, one term of the\nlicense is that ‚ÄúEnd user will obtain whatever train-\ning and approval is required by their organization\nfor working with human subjects data‚Äù, which we\nhave obtained (more details in (Oard et al., 2015)).\nWhile handling sensitive email data (Avocado) we\nmade sure to abide by the terms of its end-user\nlicense agreement (EULA) which has provisions\nto protect the privacy of members of the corpus.\nFurthermore, we took measures such as scrubbing\nnamed entities before using the data for model\ntraining. The over-arching goal of our work is\nto contribute to language model development that\nprotects the privacy rights of users who contribute\ntheir data. While we rigorously evaluated our mod-\nels by applying state-of-the-art attacks, deploying\nthese models in real-world setups requires further\nveriÔ¨Åcation that users‚Äô privacy is preserved.\nAcknowledgments\nThe authors would like to thank the anonymous\nreviewers and meta-reviewers for their helpful feed-\nback. We also thank Peter Kairouz and Moham-\nmadkazem Taram for insightful discussions. Addi-\ntionally, we thank our MSR colleagues and UCSD\nBerg Lab for their helpful comments and feedback.\nReferences\nMartin Abadi, Andy Chu, Ian Goodfellow, H Bren-\ndan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. 2016. Deep learning with differential pri-\nvacy. In Proceedings of the 2016 ACM SIGSAC Con-\nference on Computer and Communications Security,\npages 308‚Äì318.\nMartin Adam, Michael Wessel, and Alexander Benlian.\n2020. Ai-based chatbots in customer service and\ntheir effects on user compliance. Electronic Mar-\nkets, pages 1‚Äì19.\nEugene Bagdasaryan, Omid Poursaeed, and Vitaly\nShmatikov. 2019. Differential privacy has disparate\nimpact on model accuracy. In Advances in Neural\nInformation Processing Systems, volume 32, pages\n15479‚Äì15488.\nYoshua Bengio, R√©jean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. Journal of machine learning research,\n3:1137‚Äì1155.\nZhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J. Su.\n2019. Deep learning with gaussian differential pri-\nvacy. arXiv preprint, abs/1911.11607.\nNicholas Carlini, Chang Liu, √ölfar Erlingsson, Jernej\nKos, and Dawn Song. 2019. The secret sharer: Eval-\nuating and testing unintended memorization in neu-\nral networks. In 28th USENIX Security Symposium,\npages 267‚Äì284, Santa Clara, CA. USENIX Associa-\ntion.\nNicholas Carlini, Florian Tramer, Eric Wallace,\nMatthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ul-\nfar Erlingsson, et al. 2020. Extracting training\ndata from large language models. arXiv preprint\narXiv:2012.07805.\nGal Chechik, Varun Sharma, Uri Shalit, and Samy Ben-\ngio. 2010. Large scale online learning of image sim-\nilarity through ranking. Journal of Machine Learn-\ning Research, pages 1109‚Äì1135.\nMia Xu Chen, Benjamin N Lee, Gagan Bansal, Yuan\nCao, Shuyuan Zhang, Justin Lu, Jackie Tsay, Yinan\nWang, Andrew M Dai, Zhifeng Chen, et al. 2019.\nGmail smart compose: Real-time assisted writing.\nIn Proceedings of the 25th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data\nMining, KDD‚Äô19, page 196‚Äì206, New York, NY ,\nUSA. Association for Computing Machinery.\nMaximin Coavoux, Shashi Narayan, and Shay B. Co-\nhen. 2018. Privacy-preserving neural representa-\ntions of text. In Proceedings of the 2018 Conference\non Empirical Methods in Natural Language Process-\ning, pages 1‚Äì10. Association for Computational Lin-\nguistics.\nCynthia Dwork. 2006. Differential privacy. In Pro-\nceedings of the 33rd international conference on Au-\ntomata, Languages and Programming, pages 1‚Äì12.\nSpringer.\nTom Farrand, Fatemehsadat Mireshghallah, Sahib\nSingh, and Andrew Trask. 2020. Neither private nor\nfair: Impact of data imbalance on utility and fairness\nin differential privacy. In Proceedings of the 2020\nWorkshop on Privacy-Preserving Machine Learning\nin Practice, PPMLP‚Äô20, page 15‚Äì19, New York, NY ,\nUSA. Association for Computing Machinery.\nGDPR Article 29 Working Party. 2014. Opin-\nion 05/2014 on ‚Äúanonymisation tech-\nniques‚Äù. https://ec.europa.eu/\njustice/article-29/documentation/\nopinion-recommendation/files/2014/\nwp216_en.pdf.\nSepp Hochreiter and J√ºrgen Schmidhuber. 1997. Long\nshort-term memory. Neural computation, 9:1735‚Äì\n80.\nHuseyin A. Inan, Osman Ramadan, Lukas Wutschitz,\nDaniel Jones, Victor R√ºhle, James Withers, and\nRobert Sim. 2021. Training data leakage analysis in\nlanguage models. arXiv preprint arXiv:2101.05405.\nYitong Li, Timothy Baldwin, and Trevor Cohn. 2018.\nTowards robust and privacy-preserving text represen-\ntations. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 2: Short Papers), pages 25‚Äì30, Melbourne,\nAustralia. Association for Computational Linguis-\ntics.\nH Brendan McMahan, Galen Andrew, Ulfar Erlingsson,\nSteve Chien, Ilya Mironov, Nicolas Papernot, and\nPeter Kairouz. 2018. A general approach to adding\ndifferential privacy to iterative training procedures.\narXiv preprint, abs/1812.06210.\nT. Mikolov, Martin KaraÔ¨Å√°t, Lukas Burget, J. Cer-\nnock√Ω, and S. Khudanpur. 2010. Recurrent neu-\nral network based language model. In Proceedings\nof the 11th Annual Conference of the International\nSpeech Communication Association, pages 1045‚Äì\n1048.\nFatemehsadat Mirshghallah, Mohammadkazem Taram,\nPraneeth Vepakomma, Abhishek Singh, Ramesh\nRaskar, and Hadi Esmaeilzadeh. 2020. Privacy\nin deep learning: A survey. arXiv preprint\narXiv:2004.12254.\nDouglas Oard, William Webber, David Kirsch, and\nSergey Golitsynskiy. 2015. Avocado research email\ncollection. https://catalog.ldc.upenn.\nedu/LDC2015T03.\nDavid Pujol, Ryan McKenna, Satya Kuppam, Michael\nHay, Ashwin Machanavajjhala, and Gerome Miklau.\n2020. Fair decision making using privacy-protected\ndata. In Proceedings of the 2020 Conference on\nFairness, Accountability, and Transparency, FAT*\n‚Äô20, page 189‚Äì199, New York, NY , USA. Associa-\ntion for Computing Machinery.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews,\nGalen Andrew, H Brendan McMahan, and Fran√ßoise\nBeaufays. 2020. Training production language mod-\nels without memorizing user data. arXiv preprint\narXiv:2009.10031.\nNisarg Raval, Ashwin Machanavajjhala, and Jerry Pan.\n2019. Olympus: sensor privacy through utility\naware obfuscation. In Proceedings on Privacy En-\nhancing Technologies, pages 5‚Äì25. Sciendo.\nR. Shokri, M. Stronati, C. Song, and V . Shmatikov.\n2017. Membership inference attacks against ma-\nchine learning models. In 2017 IEEE Symposium\non Security and Privacy (SP), pages 3‚Äì18.\nCongzheng Song and Vitaly Shmatikov. 2019. Au-\nditing data provenance in text-generation models.\nIn Proceedings of the 25th ACM SIGKDD Interna-\ntional Conference on Knowledge Discovery & Data\nMining, KDD ‚Äô19, page 196‚Äì206, New York, NY ,\nUSA. Association for Computing Machinery.\nPranav Subramani, Nicholas Vadivelu, and Gautam\nKamath. 2020. Enabling fast differentially private\nsgd via just-in-time compilation and vectorization.\narXiv preprint arXiv:2010.09063.\nFlorian Tram√®r and D. Boneh. 2020. Differentially pri-\nvate learning needs better features (or much more\ndata). ArXiv, abs/2011.11660.\nMichael V√∂lske, Martin Potthast, Shahbaz Syed, and\nBenno Stein. 2017. Tl;dr: Mining Reddit to learn au-\ntomatic summarization. In Proceedings of the ACL\n2017 Workshop on New Frontiers in Summarization,\npages 59‚Äì63.\nTianlu Wang, Jieyu Zhao, Mark Yatskar, Kai-Wei\nChang, and Vicente Ordonez. 2019. Balanced\ndatasets are not enough: Estimating and mitigating\ngender bias in deep image representations. In Pro-\nceedings of the IEEE/CVF International Conference\non Computer Vision, pages 5310‚Äì5319.\nXi Wu, Fengan Li, Arun Kumar, Kamalika Chaudhuri,\nSomesh Jha, and Jeffrey Naughton. 2017. Bolt-on\ndifferential privacy for scalable stochastic gradient\ndescent-based analytics. In Proceedings of the 2017\nACM International Conference on Management of\nData, SIGMOD ‚Äô17, page 1307‚Äì1322, New York,\nNY , USA. Association for Computing Machinery.\nS. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha.\n2018. Privacy risk in machine learning: Analyzing\nthe connection to overÔ¨Åtting. In 2018 IEEE 31st\nComputer Security Foundations Symposium (CSF),\npages 268‚Äì282.\nA Appendix\nA.1 Language Models\nLanguage models assign a probability distribution\nover a sequence of words. A statistical model\nfor a sequence of words x1 ...x n can be repre-\nsented by the product of the conditional probability\nof the next word given all the previous words as\nPr(x1 ...x n) = ‚àèn\ni=1 Pr(xi|x1 ...x i‚àí1). Here\nPr(xi|x1 ...x i‚àí1) denotes the probability of the\noccurrence of word xi given the previous word se-\nquence x1 ...x i‚àí1. Recurrent Neural Networks\n(RNNs) are widely used for this task (Bengio et al.,\n2003; Mikolov et al., 2010) as RNNs can process\nvariable-length input by processing words one at a\ntime, updating its internal state and predicting the\nnext word sequentially. Therefore, such variable-\nlength conditional distributions can be effectively\nestimated with RNNs. In this work we use LSTMs\n(Hochreiter and Schmidhuber, 1997) in our lan-\nguage model.\nA.2 Experimental Setup\nWe use a subset of the Avocado dataset (Oard et al.,\n2015) with 100 users and 60,000 samples. We also\nuse a subset of Reddit dataset (V√∂lske et al., 2017)\nwith 10,000 users and 3 million samples. For both\ndatasets, we Ô¨Åx the vocabulary to the most frequent\n40,000 tokens in the training corpus, and we create\na 80/20% training/test set split. We use a two-layer\nLSTM model as the language model for the next-\nword prediction task. We set both the embedding\ndimension and LSTM hidden-representation size\nto 550 (with 55 million parameters). We use a\nfeed-forward two fully connected layer neural net-\nwork with hidden dimension of 1000 as the author\ndiscriminator for the adversarial training regular-\nization scheme.\nFor optimization, we use the Adam optimizer\nwith the learning rate set to 1e-3 and batch size to\n100. We trained our models on a single Titan Xp\nGPU accompanied by 12GBs of RAM, and two\nIntel Xeon E5-2620 CPUs with 256 GBs of RAM.\nTable 1 shows the perplexity of the models used in\nthe evaluations.\n0\n3\n6\n9\n12\n15\n18\n21\nUnmitigated DP Triplet Adversarial\nNormalized Per Epoch\nTraining Time\nGPU Time\nCPU Time\nOverall Time\nFigure 4: Per epoch training time break down, nor-\nmalized to conventional execution. Differential privacy\nis 16.44√óslower than conventional execution. Triplet\nand Adversarial are our proposed regularizations.\nTable 1: Training and test perplexities of the models\nused in the evaluations.\nDataset Unmitigated DP Adversarial (ours) Triplet (ours)\nAvocado Training 93.5 94.2 104.7 101.4\n(High PPL) Test 103.5 93.5 96.6 96.5\nAvocado Training 36.8 63.8 56.7 54.8\n(Low PPL) Test 51.3 69.8 69.1 69.1\nReddit Training 107.5 110.3 106.7 106.4\nTest 97.3 97.4 98.2 97.6\nA.2.1 Batching Strategy\nOur mitigations can be implemented using differ-\nent batching strategies during training: uniform\nbatches that contain training samples from differ-\nent users, or per user batches that contain training\nsamples from only a given user. Through exper-\nimentation, we observed that the second method\nperforms better and we present our results under the\nsecond batching strategy. The better performance\nis due to the fact that grouping the same users to-\ngether and increasing the probability of samples\nfrom same people being placed in corresponding\npositions in a batch, helps the discriminator (in the\nadversarial training scheme) learn user (author) pat-\nterns faster. It also helps the triplet-based scheme\nto more efÔ¨Åciently distance samples from the same\nuser. This scheme we increases the probability that\nthe auxiliary batch is selected from the same user‚Äôs\ndata, compared to randomly selecting two uniform\nbatches. This helps better distribute one user‚Äôs data\nin the embedding space.\nA.3 Overhead and Complexity Analysis\nA.3.1 Training Time Measurements\nHere we compare the training time of our pro-\nposed regularizations to differential privacy. Fig-\nure 4 shows the breakdown of the CPU and GPU\n(CUDA) time for our two proposed methods, and\ndifferential privacy, normalized to the conventional\nunmitigated execution time. The results show that\ndifferential privacy is extremely slower than our\nmitigations. Our adversarial training mitigation\nis overall only 1.06√óslower than conventional\ntraining, and our triplet-based mitigation is 1.80√ó\nslower due to the need to process an auxiliary batch\nin each batch. The reason that our triplet-based mit-\nigation is slower than the adversarial one is that\nthe triplet based loss runs two batches through the\nlanguage model during each iteration (the base-\nline batch and the auxiliary), whereas the adversar-\nial training scheme runs only one batch of train-\ning data. Differential privacy, however, is16.44√ó\nslower than conventional execution, which is due to\nits per-sample gradient computation, which limits\nthe possibility of parallelism. It is noteworthy that\nin our experiments, we have applied the training\ntime optimization suggested in (McMahan et al.,\n2018) for differential privacy, which increases par-\nallelism through the use of ‚Äúmicro-batches‚Äù. How-\never, even with this optimization, we are still ob-\nserving huge slow-downs. Furthermore, differen-\ntialy private training of DNNs and RNNs takes\nexhaustive hyperparameter tuning to get the best\nprivacy-utility trade-off, which is cumbersome (Wu\net al., 2017; Mirshghallah et al., 2020), and this\nslow training process makes the tuning of the pa-\nrameters extremely harder.\nA.4 Added Parameters and Complexity\nOur adversarial trainingregularization includes\nan additional feed-forward discriminator DNN to\npredict the author of each sequence, as shown\nin Figure 1. In our experiments, we use a feed-\nforward network with two fully connected layers\nand a hidden dimension of 1000. The input dimen-\nsion is the same as the hidden state dimension of\nthe language model (550) and the output dimen-\nsion is the number of authors, 100 for Avocado,\nand 10,000 for Reddit dataset. This means that the\ndiscriminator parameters scale-up with the number\nof authors. For the Avocado case, this would give\n550√ó1000+1000 ‚àó100 = 650Kextra parameters\n(compared to conventional training), and for Reddit\nit would be550√ó1000+1000√ó10,000 = 10.55M\nparameters. This is comparatively reasonable con-\nsidering the number of parameters in the language\nmodel (55M) as a tradeoff to improve the privacy\nof the model. It would be interesting to explore\nwhether reducing the number of hidden dimensions\nof the discriminator as the number of users increase\nwould be effective to balance the number of param-\neters our *adversarial method* adds to the parame-\nters of the network.\nOur triplet-based regularization does not add\nany extra parameters to the model, which might be\nadvantageous for settings with massive number of\nusers. It does, however, need to feed an auxiliary\nbatch to the language model, alongside the baseline\nbatch which almost doubles the training time.\nFigure 4 and Section A.3.1 show the break-down\nof the training time of DP training, our triplet-based\nmethod, and our adversarial method, compared\nto unmitigated training time. Differential privacy\ndoes not add any additional complexity in terms of\nparameters. However, it adds computational com-\nplexity by having to compute the gradients of each\nsample separately, thereby not exploiting the batch\nprocessing parallelization offered by GPUs. This in\nturn slows down the training procedure extremely,\nmaking hyperparameter search infeasible for large\nnetworks.\nA.5 Exposure Metric\nTo empirically compare the level of privacy pro-\nvided by our methods to that of a DP model, we\nadopt the exposure metric introduced in (Carlini\net al., 2019). This metric measures the extent to\nwhich a model memorizes samples in the train-\ning data. The higher the exposure metric for a\nsequence, the more the model‚Äôs memorization and\nthe easier it is to extract the sequence from the lan-\nguage model through text generation algorithms.\nTo measure exposure we insert canaries to the\ntraining data. Our canaries are sequences of Ô¨Åve\nrandom words from the vocabulary. We insert\ncanaries with different repetitions for each per-\nson. For Avocado dataset, each user (author) is\nassigned 14 unique canaries, each of them repeated\n[1,2,3,4,5,6,7,8,9,10,20,30,40,50] times. We\ninsert canaries with repition to mimic ‚Äúsecrets‚Äù\nthat belong only to a certain user, but might\nbe repeated by that user in multiple emails/texts.\nThis means each user is assigned and overall of\n1 + 2 + 3 +...+ 50 = 195canaries. This yields\nand overall of 195 √ó100 = 19500 canaries in-\njected to the data. This data is then used to train\nthe model. Once the model is trained, we can then\nmeasure the exposure of a given canary, by Ô¨Ånd-\ning the probability assigned to it by the language\nmodel, and then Ô¨Ånding the rank of that sequence\nby by sorting all probabilities assigned to all pos-\nsible sequences of the same length. Carlini et al.\nprovide a method to estimate the rank for each se-\nquence without having to actually enumerate and\nfeed all possible sequences to the model and actu-\nally ranking all of them. Once we have measured\nthe exposure metric for all canary sequences, we\ncan use them to evaluate the privacy.\nFor Reddit dataset, each user is assigned 5\nunique canaries, with repetitions of [1,2,5,6,20]\ntimes. We use less canaries in the latter so as not\nto contaminate the dataset as the Reddit dataset has\nless samples per user. The ‚Äúreal canaries‚Äù intro-\nduced in the experiments are chosen by feeding\nall the training data through an unmitigated model,\nand selecting the sample with highest perplexity\nfor each user. We hypothesize that these sequences\nare more likely to contain sensitive information."
}