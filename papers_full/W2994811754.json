{
  "title": "Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model",
  "url": "https://openalex.org/W2994811754",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4202112432",
      "name": "Xiong, Wenhan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748019825",
      "name": "Du, Jingfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3091095485",
      "name": "Wang, William Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4224909769",
      "name": "Stoyanov, Veselin",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2963195889",
    "https://openalex.org/W2787560479",
    "https://openalex.org/W2803457824",
    "https://openalex.org/W2951434086",
    "https://openalex.org/W2949694638",
    "https://openalex.org/W2962886429",
    "https://openalex.org/W2971869958",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2066018628",
    "https://openalex.org/W2741075451",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2963159690",
    "https://openalex.org/W2406945108",
    "https://openalex.org/W2963756346",
    "https://openalex.org/W2969307504",
    "https://openalex.org/W2609826708",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2950813464",
    "https://openalex.org/W2938205538",
    "https://openalex.org/W1825507529",
    "https://openalex.org/W2080133951",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2734823783",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2788448041",
    "https://openalex.org/W205829674",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W1566289585",
    "https://openalex.org/W2912817604",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2898662126",
    "https://openalex.org/W2963548348",
    "https://openalex.org/W2252136820",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2476140796",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2951048068"
  ],
  "abstract": "Recent breakthroughs of pretrained language models have shown the effectiveness of self-supervised learning for a wide range of natural language processing (NLP) tasks. In addition to standard syntactic and semantic NLP tasks, pretrained models achieve strong improvements on tasks that involve real-world knowledge, suggesting that large-scale language modeling could be an implicit method to capture knowledge. In this work, we further investigate the extent to which pretrained models such as BERT capture knowledge using a zero-shot fact completion task. Moreover, we propose a simple yet effective weakly supervised pretraining objective, which explicitly forces the model to incorporate knowledge about real-world entities. Models trained with our new objective yield significant improvements on the fact completion task. When applied to downstream tasks, our model consistently outperforms BERT on four entity-related question answering datasets (i.e., WebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 improvements and a standard fine-grained entity typing dataset (i.e., FIGER) with 5.7 accuracy gains.",
  "full_text": "Published as a conference paper at ICLR 2020\nPRETRAINED ENCYCLOPEDIA : W EAKLY SUPERVISED\nKNOWLEDGE -PRETRAINED LANGUAGE MODEL\nWenhan Xiong†, Jingfei Du§, William Yang Wang†, Veselin Stoyanov§,\n†University of California, Santa Barbara\n§Facebook AI\n{xwhan, william}@cs.ucsb.edu, {jingfeidu, ves}@fb.com\nABSTRACT\nRecent breakthroughs of pretrained language models have shown the effective-\nness of self-supervised learning for a wide range of natural language processing\n(NLP) tasks. In addition to standard syntactic and semantic NLP tasks, pretrained\nmodels achieve strong improvements on tasks that involve real-world knowledge,\nsuggesting that large-scale language modeling could be an implicit method to cap-\nture knowledge. In this work, we further investigate the extent to which pretrained\nmodels such as BERT capture knowledge using a zero-shot fact completion task.\nMoreover, we propose a simple yet effective weakly supervised pretraining objec-\ntive, which explicitly forces the model to incorporate knowledge about real-world\nentities. Models trained with our new objective yield signiﬁcant improvements on\nthe fact completion task. When applied to downstream tasks, our model consis-\ntently outperforms BERT on four entity-related question answering datasets (i.e.,\nWebQuestions, TriviaQA, SearchQA and Quasar-T) with an average 2.7 F1 im-\nprovements and a standard ﬁne-grained entity typing dataset ( i.e., FIGER) with\n5.7 accuracy gains.\n1 I NTRODUCTION\nLanguage models pretrained on a large amount of text such as ELMo (Peters et al., 2018a)),\nBERT (Devlin et al., 2019) and XLNet (Yang et al., 2019c) have established new state of the art\non a wide variety of NLP tasks. Researchers ascertain that pretraining allows models to learn syn-\ntactic and semantic information of language that is then transferred on other tasks (Peters et al.,\n2018b; Clark et al., 2019). Interestingly, pretrained models also perform well on tasks that require\ngrounding language and reasoning about the real world. For instance, the new state-of-the-art for\nWNLI (Wang et al., 2019a), ReCoRD (Zhang et al., 2018) and SW AG (Zellers et al., 2018) is\nachieved by pretrained models. These tasks are carefully designed so that the text input alone does\nnot convey the complete information for accurate predictions – external knowledge is required to\nﬁll the gap. These results suggest that large-scale pretrained models implicitly capture real-world\nknowledge. Logan et al. (2019) and Petroni et al. (2019) further validate this hypothesis through\na zero-shot fact completion task that involves single-token entities, showing that pretrained models\nachieve much better performance than random guessing and can be on par with speciﬁcally-trained\nrelation extraction models.\nAs unstructured text encodes a great deal of information about the world, large-scale pretraining\nover text data holds the promise of simultaneously learning syntax, semantics and connecting them\nwith knowledge about the real world within a single model. However, existing pretraining objectives\nare usually deﬁned at the token level and do not explicitly model entity-centric knowledge. In this\nwork, we investigate whether we can further enforce pretrained models to focus on encyclopedic\nknowledge about real-world entities, so that they can better capture entity information from natural\nlanguage and be applied to improving entity-related NLP tasks. We evaluate the extent to which a\npretrained model represents such knowledge by extending an existing fact completion evaluation to\na cloze ranking setting that allows us to deal with a large number of multi-token entity names with-\nout manual judgments. Our experiments on 10 common Wikidata (Vrande ˇci´c & Kr ¨otzsch, 2014)\nrelations reveal that existing pretrained models encode entity-level knowledge only to a limited de-\ngree. Thus, we propose a new weakly supervised knowledge learning objective that requires the\n1\narXiv:1912.09637v1  [cs.CL]  20 Dec 2019\nPublished as a conference paper at ICLR 2020\nFigure 1: Type-Constrained Entity Replacements for Knowledge Learning.\nmodel to distinguish between true and false knowledge expressed in natural language. Speciﬁcally,\nwe replace entity mentions in the original documents with names of other entities of the same type\nand train the models to distinguish the correct entity mention from randomly chosen ones. Models\ntrained with this objective demonstrates much stronger fact completion performance for most rela-\ntions we test on. Compared with previous work (Zhang et al., 2019; Peters et al., 2019) that utilizes\nan external knowledge base to incorporate entity knowledge, our method is able to directly derive\nreal-world knowledge from unstructured text. Moreover, our method requires no additional data\nprocessing, memory or modiﬁcations to the BERT model when ﬁne-tuning for downstream tasks.\nWe test our model on two practical NLP problems that require entity knowledge: Question Answer-\ning (QA) and ﬁne-grained Entity Typing. We use four previously published datasets for open-domain\nQA and observe that questions in these datasets often concern entities. The Entity Typing task re-\nquires the model to recognize ﬁne-grained types of speciﬁed entity mentions given short contexts.\nOn three of the QA datasets, our pretrained model outperforms all previous methods that do not\nrely on memory-consuming inter-passage normalizations1. On the FIGER entity-typing dataset, our\nmodel sets a new state of the art. Through ablation analysis, we show that the new entity-centric\ntraining objective is instrumental for achieving state-of-the-art results.\nIn summary, this paper makes the following contributions: 1) We extend existing fact completion\nevaluation settings to test pretrained models’ ability on encoding knowledge of common real-world\nentities; 2) We propose a new weakly supervised pretraining method which results in models that\nbetter capture knowledge about real-world entities from natural language text; 3) The model trained\nwith our knowledge learning objective establishes new state of the art on three entity-related QA\ndatasets and a standard ﬁne-grained entity typing dataset.\nWe begin by introducing our weakly supervised method for knowledge learning ( §2) and then dis-\ncuss experiment settings and evaluation protocols, compare our model to previously published work\nand perform ablation analysis. Finally, we review related work in §4 and conclude in §5.\n2 E NTITY REPLACEMENT TRAINING\nWe design an entity-centric training objective that utilizes weakly supervised training signals to\nexplicitly encourage knowledge learning during pretraining. Given an input document, we ﬁrst\n1Wang et al. (2019b) propose to use BERT to encode multiple retrieved paragraphs within the same forward-\nbackward pass. This operation requires around 12G GPU memory even for encoding a single example which\nincludes 6 512-token paragraphs.\n2\nPublished as a conference paper at ICLR 2020\nrecognize the entity mentions and link them to Wikipedia entities 2. We consider the original texts\nas positive knowledge statements and create negative statements by randomly replacing the entity\nmentions (E+) with the names of other random entities ( E−) that have the same entity type as the\nmentioned entity. This setup is similar in spirit to the type-constrained negative sampling technique\nused to train knowledge base representations (Bordes et al., 2013). The latter technique creates\nnegative triples by replacing the subject or object entity with random entities of the same type.\nInstead of knowledge base triples, we treat unstructured texts as factual statements. For a certain\nentity e mentioned in a context C, we train the model to make a binary prediction indicating whether\nthe entity has been replaced:\nJe,C= 1 e∈E+ log P(e|C) + (1−1 e∈E+) log(1−P(e|C)).\nCompared to the language modeling objective, entity replacement is deﬁned at the entity level and\nintroduces stronger negative signals. When we enforce entities to be of the same type, we pre-\nserve the linguistic correctness of the original sentence while the system needs to learn to perform\njudgment based on the factual aspect of the sentence.\nWe describe the implementation in more detail in the following paragraphs.\nData Preparation We use the whole English Wikipedia dump as training data and rely on all\nWikipedia entities3. Entities in documents are recognized based on Wikipedia anchor links and\nentity alias from Wikidata. That is, we ﬁrst retrieve the entities annotated by anchor links and\nthen ﬁnd other mentions of these entities by string matching their Wikidata alias. We split each\ndocument into multiple text chunks with the same size (512 tokens). Although our experiments rely\non the Wikipedia corpus, this setup can be easily extended to larger corpora with off-the-shelf entity\nlinking tools. We leave the larger scope of the experiments to future work.\nReplacement Strategy When replacing entities, we ﬁrst lookup type information4 from Wikidata\nand then randomly select other entities with the same type. We do not replace adjacent entities.\nIn other words, there must be at least one unreplaced entity between any two replaced ones. This\nreduces cases where we replace all entities in the same sentence and the resulting sentences happen\nto introduce correct entities by chance. For replacement, we randomly sample a string from the\nentities’ alias set. For each text chunk, we replicate it 10 times with different negative entities for\neach replacement location. We show an illustration of the entity replacement method in Figure 1.\nModel Architecture We use the Transformer (Vaswani et al., 2017) model used by BERT (Devlin\net al., 2019). We use the same architecture as BERT base: 12 Transformer layers, each with hidden\ndimension 768. We initialize the transformer with a model pretrained based on our own BERT re-\nimplementations5. For each entity, we use the ﬁnal representations of its boundary words (words\nbefore and after the entity mention) to make predictions. We simply concatenate the boundary\nwords’ representations and add a linear layer for prediction. During training, we use 0.05 dropout at\nthe ﬁnal layer.\nTraining Objectives Masked language model pretraining has been proven to be effective for\ndownstream tasks. While training for entity replacement we also train with the masked language\nmodel objective in a multi-task set-up. When masking tokens, we restrict the masks to be outside\nthe entity spans. We use a masking ratio of 5% instead of 15% in the original BERT to avoid mask-\ning out too much of the context. We train the model for approximately 1 million updates using a\nbatch size of 128.\n2The entity links are only required for pretraining.\n3Each Wikipedia entity (title) corresponds to a unique entity node in Wikidata.\n4Based on the “instance of” relation in Wikidata. If there are multiple correct types, we randomly choose\none.\n5We use the masked language model implementation in Fairseq (Ott et al., 2019) to pre-train model for 2M\nupdates on the combination of BooksCorpus (Zhu et al., 2015) and English Wikipedia.\n3\nPublished as a conference paper at ICLR 2020\n3 E XPERIMENTS\nWe ﬁrst test our model on a fact completion task. This task resembles traditional knowledge base\ncompletion: it requires the model to complete missing entities in factual triples. We further test\non two real-world downstream tasks that require entity-level knowledge – question answering and\nﬁne-grained entity typing. We describe the hyperparameter and training settings of all experiments\nin the appendix.\n3.1 Z ERO -SHOT FACT COMPLETION\nIn traditional knowledge base completion tasks models have access to a set of training triples. In-\nstead, we utilize a zero-shot test to examine the model’s ability to automatically derive relational\nknowledge from natural language.\nDataset We rely on factual triples from Wikidata. Each triple describes the relationship between\ntwo certain entities, e.g., {Paris, CapitalOf, France}. Following recent practices (Bosselut et al.,\n2019; Logan et al., 2019) that decode structured knowledge from language models, we ﬁrst manually\ncreate templates to convert triples of 10 common relations into natural language expressions ({Paris,\nCapitalOf, France}→the capital of France is Paris). We then create queries by removing the object\nentity in the expression and use pre-trained models to predict the missing entities, e.g., the capital\nof France is ?. We create 1000 cloze examples6 for each of the 10 relations.\nEvaluation Metrics Previous work (Logan et al., 2019; Petroni et al., 2019) either relies on human\nevaluation or only considers single-token entities for fact completion. In contrast, we consider an\nentity-ranking setup and create a set of candidate entities for each relation. This setting allows us\nto automatically evaluate a large number of queries that usually involve multi-token entities. We\ntest pretrained models on their ability to recover the correct object entity from the candidate set. To\ncreate the negative choices, we select from the set of all object entities in the particular relation,\nwhich generally have the same type as the groundtruth and are more challenging to distinguish than\nentities with different types. Our evaluation strategy is similar to previous work on knowledge base\ncompletion (Nickel et al., 2011; Bordes et al., 2013; Xiong et al., 2017). We follow these studies\nand use Hits@10 as the evaluation metric.\nBaselines We compare our model with two pretrained language models BERT (Devlin et al., 2019)\n(both base and large) and GPT-2 (Radford et al., 2019). We make use of their output token proba-\nbilities to rank candidate entities. For BERT, we feed in the masked queries ( e.g., Qmasked = the\ncapital of France is [MASK]). For multi-token candidates, we use the same number of\n[MASK] tokens in the query inputs. We use the average log probability of masked tokens for rank-\ning. Given a multi-token entity Ei = [e1\ni , e2\ni , ..., e|Ei|\ni ], the ranking score from BERT is calculated\nas\nSEi = 1\n|Ei|\n∑\nk\nlog P(ek\ni |Qmasked).\nFor GPT-2, we feed in the original query without the answer entity and use the ﬁrst-token probability\nof candidate entities for ranking, which performs better than using average log probabilities. As\nour model learns to predict a plausible probability ( P(e|C)) for each entity mention during entity\nreplacement training, we can directly use these predicted probabilities to rank the candidates.\nResults Table 1 shows the fact completion results for all relations. We denote our methodWKLM\nfor (Weakly Supervised Knowledge-Pretrained Language Model). Overall, WKLM achieves the\nbest results on 8 of the 10 relations. We also observe that GPT-2 outperforms BERT on average.\nWe think this is because the fact completion task requires models to predict the missing entities\nusing only a short context on the left, while BERT pretraining incorporates context from both di-\nrections. Interestingly, BERT achieves good performance on several geographical relations such as\nPlaceOfBirth, LocatedIn and PlaceOfDeath. We conjecture that this is because loca-\ntion entities usually appear at sentence ends in Wikipedia articles, e.g., Obama was born in\n6For each relation, we use the top triples that connecting most common entities.\n4\nPublished as a conference paper at ICLR 2020\nHonolulu, Hawaii.. This sentence pattern is similar to our templates and BERT may learn to\nrely mostly on the left context to make predictions. For most relations that include answers that are\nperson names, BERT lags behind both GPT-2 and our model.\nComparing the top and bottom ﬁve relations, we observe that BERT’s performance is correlated\nwith the size of the candidate set, while WKLM and GPT-2 are less sensitive to this number. A\nsimilar pattern exists between models’ performance and the cardinality of groundtruth answers,i.e.,\nour model achieves similar performance on both single-answer and multiple-answer queries while\nBERT is usually better at single-answer queries. WKLM both outperforms BERT and GPT-2 and\nachieves robust performance across relations with different properties. Visualization of correlations\nbetween relation properties and model performance can be found in the appendix.\nTable 1: Zero-Shot Fact Completion Results.\nRelation\nName\n# of\nCandidates\n# of\nAnswers\nModel\nBERT-base BERT-large GPT-2 Ours\nHASCHILD (P40) 906 3.8 9.00 6.00 20.5 63.5\nNOTABLE WORK (P800) 901 5.2 1.88 2.56 2.39 4.10\nCAPITAL OF (P36) 820 2.2 1.87 1.55 15.8 49.1\nFOUNDED BY (P112) 798 3.7 2.44 1.93 8.65 24.2\nCREATOR (P170) 536 3.6 4.57 4.57 7.27 9.84\nPLACE OFBIRTH (P19) 497 1.8 19.2 30.9 8.95 23.2\nLOCATED IN (P131)) 382 1.9 13.2 52.5 21.0 61.1\nEDUCATED AT (P69) 374 4.1 9.10 7.93 11.0 16.9\nPLACE OFDEATH (P20) 313 1.7 43.0 42.6 8.83 26.5\nOCCUPATION (P106) 190 1.4 8.58 10.7 9.17 10.7\nAverage Hits@10 - - 11.3 16.1 16.3 28.9\n3.2 D OWNSTREAM TASKS\nBackground knowledge is important for language understanding. We expect our pretraining ap-\nproach to be beneﬁcial to NLP applications where entity-level knowledge is essential. We consider\ntwo such applications: question answering and entity-typing. We ﬁnd that a large portion of the\nquestions in existing QA datasets are about entities and involve entity relations. In a way, our pre-\ntraining objective is analogous to question answering in a multiple-choice setting (Hermann et al.,\n2015). The entity-typing task requires the model to predict a set of correct types of entity mentions\nin a short context. The context itself can be insufﬁcient and the training data for this task is small\nand noisy. We believe a model that encodes background entity knowledge can help in both cases.\n3.2.1 Q UESTION ANSWERING\nDatasets We consider four question answering datasets:\n•WebQuestions (Berant et al., 2013) is originally a dataset for knowledge base question\nanswering. The questions are collected using Google Suggest API and are all asking about\nsimple relational facts of Freebase entities.\n•TriviaQA7 (Joshi et al., 2017) includes questions from trivia and quiz-league websites.\nApart from a small portion of questions to which the answers are numbers and free texts,\n92.85% of the answers are Wikipedia entities.\n•Quasar-T (Dhingra et al., 2017) is another dataset that includes trivia questions. Most of\nthe answers in this dataset are none phrases. According to our manual analysis on random\nsamples, 88% of the answers are real-world entities8.\n•SearchQA (Dunn et al., 2017) uses questions from the television quiz showJeopardy! and\nwe also ﬁnd that almost all of the answers are real-world entities.\n7The splits of TriviaQA might be different in previous work, we use the same splits used by Lin et al.\n(2018). Different methods also use different retrieval systems for this dataset, i.e., ORQA and BERTserini\nretrieve documents from Wikipedia while the other methods use retrieved Web documents.\n8We consider answers as entities as long as they correspond to Wikidata entities or Wikipedia titles.\n5\nPublished as a conference paper at ICLR 2020\nQuestions in all three datasets are created without the context of a paragraph, which resembles the\nscenario of practical question answering applications. All the questions except WebQuestions are\nwritten by humans. This indicates that humans are generally interested to ask questions to seek\ninformation about entities. We show the statistics and example questions in Table 2. We split the\ntraining data (created by distant supervision) of WebQuestions with a ratio (9:1) for training and\ndevelopment. Since our model is based on our own BERT implementations, in addition to the\naforementioned entity-related datasets, we ﬁrst use the standard SQuAD (Rajpurkar et al., 2016)\nbenchmark to validate our model’s answer extraction performance.\nTable 2: Properties of the QA Datasets.\nDataset Train Valid Test Example Questions\nWebQuestions 3778 - 2032 Who plays Stewie Grifﬁn on Family Guy?\nTriviaQA 87291 11274 10790 What is the Japanese share index called?\nSearchQA 99811 13893 27247 Hero several books 11 discover’s wizard?\nQuasar-T 37012 3000 3000 Which vegetable is a Welsh emblem?\nSettings We adopt the ﬁne-tuning approach to extract answer spans with pretrained models. We\nadd linear layers over the last hidden states of the pretrained models to predict the start and end posi-\ntions of the answer. Unlike SQuAD, questions in the datasets we use are not paired with paragraphs\nthat contain the answer. We follow previous work (Chen et al., 2017; Wang et al., 2018a) and re-\ntrieve context paragraphs with information retrieval systems. Details of the context retrieval process\nfor each dataset can be found in the appendix. Reader models are trained with distantly supervised\ndata, i.e., we treat any text span in any retrieved paragraph as ground truth as long as it matches\nthe original answers. Since the reader model needs to read multiple paragraphs to predict a single\nanswer at inference time, we also train a BERT based paragraph ranker with distant-supervised data\nto assign each paragraph a relevance score. The paragraph ranker takes question and paragraph\npairs and predicts a score in the range [0, 1] for each pair. During inference, for each question and\nits evidence paragraph set, we ﬁrst use the paragraph reader to extract the best answer from each\nparagraph. These answers are then ranked based on a linear combination of the answer extraction\nscore (a log sum of the answer start and end scores) and the paragraph relevance score. We also\nevaluate model performance without using the relevance scores.\nOpen-Domain QA Baselines We compare our QA model with the following systems:\n• DrQA (Chen et al., 2017) is an open-domain QA system which uses TF-IDF with bigram\nfeatures for ranking and a simple attentive reader for answer extraction.\n• R3 (Wang et al., 2018a) is a reinforcement learning based system which jointly trains a\nparagraph ranker and a document reader.\n• DSQA (Lin et al., 2018) uses RNN-based paragraph ranker and jointly trains the paragraph\nranker and attentive paragraph ranker with a multi-task loss.\n• Evidence Aggregation (Wang et al., 2018b) uses a hybrid answer reranking module to\naggregate answer information from multiple paragraphs and rerank the answers extracted\nfrom multiple paragraphs.\n• BERTserini (Yang et al., 2019a) is a BERT-based open-domain QA system, which uses\nBM25-based retriever to retrieve 100 paragraphs and a BERT-based reader to extract an-\nswers. The paragraph reader is either trained with SQuAD (Rajpurkar et al., 2016) data or\ndistant-supervision data (Yang et al., 2019b)\n• ORQA (Lee et al., 2019) replaces the traditional BM25 ranking with a BERT-based ranker.\nThe ranker model is pretrained on the whole Wikipedia corpus with an inverse cloze\ntask which simulates the matching between questions and paragraphs. All text blocks in\nWikipedia are be pre-encoded as vectors and retrieved with Locality Sensitive Hashing.\nResults Table 3 shows the SQuAD results and Table 4 shows the open-domain results on the\nfour datasets that are highly entity-related. From the SQuAD results, we observe that our BERT\nreimplementation performs better than the original model this is due to the fact that it is trained\n6\nPublished as a conference paper at ICLR 2020\nfor twice as many updates: 2 million vs. 1 million for the original BERT. Although lots of the\nanswers in SQuAD are non-entity spans, the WKLM model we propose achieves better performance\nthan BERT. We believe the improvement is due to both the masked language model and entity\nreplacement objectives. Ablation experiments on the training objectives will be discussed in §3.2.3.\nTable 3: SQuAD Dev Results.\nModel EM F1\nGoogle’s BERT-base 80.8 88.5\nGoogle’s BERT-large 84.1 90.9\nOur BERT-base 83.4 90.5\nWKLM (base) 84.3 91.3\nHaving established that our BERT re-implementation\nperforms better than the original model, we com-\npare with only our own BERT for the following ex-\nperiments. From Table 4, we see that our model\nproduces consistent improvements across different\ndatasets. Compared to the 0.8 F1 improvements over\nBERT on SQuAD, we achieve an average of 2.7 F1\nimprovements over BERT on entity-related datasets\nwhen the ranking scores are not used. On TriviaQA\nand Quasar-T, WKLM outperforms our BERT even\nwhen it uses ranking scores. Improvements in natural language question datasets (WebQuestions,\nTriviaQA, and Quasar-T) are more signiﬁcant than SearchQA where the questions are informal\nqueries. When we utilize ranking scores from a simple BERT based ranker, we are able to achieve\nthe state-of-the-art on three of the four datasets.\nTable 4: Open-domain QA Results.\nModel WebQuestions TriviaQA Quasar-T SearchQA\nEM F1 EM F1 EM F1 EM F1\nDrQA (Chen et al., 2017) 20.7 - - - - - - -\nR3 (Wang et al., 2018a) - - 50.6 57.3 42.3 49.6 57.0 63.2\nDSQA (Lin et al., 2018) 18.5 25.6 48.7 56.3 42.2 49.3 49.0 55.3\nEvidence Agg. (Wang et al., 2018b) - - 50.6 57.3 42.3 49.6 57.0 63.2\nBERTserini (Yang et al., 2019a) - - 51.0 56.3 - - - -\nBERTserini+DS (Yang et al., 2019b) - - 54.4 60.2 - - - -\nORQA (Lee et al., 2019) 36.4 - 45.0 - - - - -\nOur BERT 29.2 35.5 48.7 53.2 40.4 46.1 57.1 61.9\nOur BERT + Ranking score 32.2 38.9 52.1 56.5 43.2 49.2 60.6 65.9\nWKLM 30.8 37.9 52.2 56.7 43.7 49.9 58.7 63.3\nWKLM + Ranking score 34.6 41.8 58.1 63.1 45.8 52.2 61.7 66.7\n3.2.2 E NTITY TYPING\nTo compare with an existing study (Zhang et al., 2019) that also attempts to incorporate entity\nknowledge into language models, we consider an additional entity typing task using the large FIGER\ndataset (Ling & Weld, 2012). The task is to assign a ﬁne-grained type to entity mentions. We do that\nby adding two special tokens before and after the entity span to mark the entity position. We use the\nﬁnal representation of the start token ([CLS]) to predict the entity types. The model is ﬁne-tuned on\nweakly-supervised training data with binary cross-entropy loss. We evaluate the models using strict\naccuracy, loose micro, and macro F1 scores.\nWe show the results in Table 5. We compare our model with two non-BERT neural baselines (Inui\net al., 2017) that integrate a set of hand-crafted features: LSTM + Hand-craftedand Attentive +\nTable 5: Fine-grained Entity Typing Results on the FIGER dataset.\nModel Acc Ma-F1 Mi-F1\nLSTM + Hand-crafted (Inui et al., 2017) 57.02 76.98 73.94\nAttentive + Hand-crafted (Inui et al., 2017) 59.68 78.97 75.36\nBERT baseline (Zhang et al., 2019) 52.04 75.16 71.63\nERNIE (Zhang et al., 2019) 57.19 75.61 73.39\nOur BERT 54.53 79.57 74.74\nWKLM 60.21 81.99 77.00\n7\nPublished as a conference paper at ICLR 2020\nHand-crafted; a vanilla BERT baselineand the ERNIE model (Zhang et al., 2019) that enhances\nBERT with knowledge base embeddings.\nFirst, we see that naively applying BERT is less effective than simple models combined with sparse\nhand-crafted features. Although the ERNIE model can improve over BERT by 5.15 points, its\nperformance still lags behind models that make good use of hand-crafted features. In contrast,\nalthough based on a stronger BERT model, our model achieves larger absolute improvements (5.68\npoints) and sets a new state-of-the-art for this task. Given the larger improvement margin, we believe\nour model that directly learn knowledge from text is more effective than the ERNIE method.\n3.2.3 A BLATION STUDY: T HE EFFECT OF MASKED LANGUAGE MODEL LOSS\nIn view of a recent study (Liu et al., 2019b) showing simply extending the training time of BERT\nleads to stronger performance on various downstream tasks, we conduct further analysis to differ-\nentiate the effects of entity replacement training and masked language modeling. We compare our\nmodel with three variants: a model pretrained only with the knowledge learning objective (WKLM\nwithout MLM), a model trained with both knowledge learning and masked language modeling with\nmore masked words (WKLM with15% MLM) and a BERT model trained with additional 1 million\nupdates on English Wikipedia (BERT + 1M MLM updates) and no knowledge learning.\nThe ablation results are shown in Table 6. The results of WKLM without MLM validate that adding\nthe language model objective is essential for downstream performance. We also ﬁnd that masking\nout too many words ( i.e., 15% masking ratio as in the original BERT) leads to worse results. We\nconjecture that too many masked words outside entity mentions break parts of the context informa-\ntion and introduce noisy signals to knowledge learning. Results of continued BERT training show\nthat more MLM updates are often beneﬁcial, especially for SQuAD. However, on tasks that are more\nentity-centric, continued MLM training is less effective than our WKLM method. This suggests that\nour WKLM method could serve as an effective complementary recipe to masked language modeling\nwhen applied to entity-related NLP tasks.\nTable 6: Ablation Studies on Masked Language Model and Masking Ratios.\nModel SQuAD TriviaQA Quasar-T FIGER\nEM F1 EM F1 EM F1 Acc\nOur BERT 83.4 90.5 48.7 53.2 40.4 46.1 54.53\nWKLM 84.3 91.3 52.2 56.7 43.7 49.9 60.21\nWKLM without MLM 80.5 87.6 48.2 52.5 42.2 48.1 58.44\nWKLM with 15% masking 84.1 91.0 51.0 55.3 42.9 49.0 59.68\nOur BERT + 1M MLM updates 84.4 91.1 52.0 56.3 42.3 48.2 54.17\n4 R ELATED WORK\nPretrained Language Representations Early research on language representations focused on\nstatic unsupervised word representations (Mikolov et al., 2013; Pennington et al., 2014). Word\nembeddings leverage co-occurrences to learn latent word vectors that approximately reﬂect word\nsemantics. Given that words can have different meanings in different contexts, more recent stud-\nies (McCann et al., 2017; Peters et al., 2018a) show that contextual language representations can\nbe more powerful than static word embeddings in downstream tasks. This direction has been fur-\nther explored at a larger scale with efﬁcient Transformer architectures (Radford et al., 2019; Devlin\net al., 2019; Yang et al., 2019c). Our WKLM method is based on these techniques and we focus on\nimproving the knowledge ability of pretrained models.\nKnowledge-Enhanced NLP Models Background knowledge has been considered an indispens-\nable part of language understanding (Fillmore et al., 1976; Minsky, 1988). As standard language en-\ncoders usually do not explicitly model knowledge, recent studies (Ahn et al., 2016; Yang & Mitchell,\n2017; Logan et al., 2019; Liu et al., 2019a) have explored methods to incorporate external knowl-\nedge into NLP models. Most of these methods rely on additional inputs such as entity representa-\ntions from structured knowledge bases. With the breakthrough of large-scale pretrained language\n8\nPublished as a conference paper at ICLR 2020\nencoders (Devlin et al., 2019), Zhang et al. (2019) and Peters et al. (2019) adopt similar ideas and\npropose entity-level knowledge enhancement training objectives to incorporate knowledge into pre-\ntrained models. Other recent studies (Mihaylov & Frank, 2018; Xiong et al., 2019) leverage external\nknowledge bases to enhance text-based question answering models. In contrast to these methods,\nour method utilizes minimal external entity information and does not require additional memory or\narchitectural changes when applied to downstream tasks.\n5 C ONCLUSION\nWe introduce a weakly supervised method to encourage pretrained language models to learn entity-\nlevel knowledge. Our method uses minimal entity information during pretraining and does not in-\ntroduce additional computation, memory or architectural overhead for downstream task ﬁne-tuning.\nThe trained model demonstrates strong performance on a probing fact completion task and two\nentity-related NLP tasks. Together, our results show the potential of directly learning entity-level\nknowledge from unstructured natural language and the beneﬁts of large-scale knowledge-aware pre-\ntraining for downstream NLP tasks.\nREFERENCES\nSungjin Ahn, Heeyoul Choi, Tanel P ¨arnamaa, and Yoshua Bengio. A neural knowledge language\nmodel. arXiv preprint arXiv:1608.00318, 2016.\nJonathan Berant, Andrew Chou, Roy Frostig, and Percy Liang. Semantic parsing on freebase from\nquestion-answer pairs. In EMNLP, pp. 1533–1544. ACL, 2013.\nAntoine Bordes, Nicolas Usunier, Alberto Garc ´ıa-Dur´an, Jason Weston, and Oksana Yakhnenko.\nTranslating embeddings for modeling multi-relational data. In NIPS, pp. 2787–2795, 2013.\nAntoine Bosselut, Hannah Rashkin, Maarten Sap, Chaitanya Malaviya, Asli C ¸ elikyilmaz, and Yejin\nChoi. COMET: commonsense transformers for automatic knowledge graph construction. In ACL\n(1), pp. 4762–4779. Association for Computational Linguistics, 2019.\nDanqi Chen, Adam Fisch, Jason Weston, and Antoine Bordes. Reading wikipedia to answer open-\ndomain questions. In ACL (1), pp. 1870–1879. ACL, 2017.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D Manning. What does bert look\nat? an analysis of bert’s attention. arXiv preprint arXiv:1906.04341, 2019.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT (1), pp. 4171–4186. ACL,\n2019.\nBhuwan Dhingra, Kathryn Mazaitis, and William W Cohen. Quasar: Datasets for question answer-\ning by search and reading. arXiv preprint arXiv:1707.03904, 2017.\nMatthew Dunn, Levent Sagun, Mike Higgins, V Ugur Guney, V olkan Cirik, and Kyunghyun Cho.\nSearchqa: A new q&a dataset augmented with context from a search engine. arXiv preprint\narXiv:1704.05179, 2017.\nCharles J Fillmore et al. Frame semantics and the nature of language. In Annals of the New York\nAcademy of Sciences: Conference on the origin and development of language and speech, volume\n280, pp. 20–32, 1976.\nKarl Moritz Hermann, Tom ´as Kocisk ´y, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. In NIPS, pp. 1693–\n1701, 2015.\nKentaro Inui, Sebastian Riedel, Pontus Stenetorp, and Sonse Shimaoka. Neural architectures for\nﬁne-grained entity type classiﬁcation. In EACL (1), pp. 1271–1280. Association for Computa-\ntional Linguistics, 2017.\n9\nPublished as a conference paper at ICLR 2020\nMandar Joshi, Eunsol Choi, Daniel S. Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly\nsupervised challenge dataset for reading comprehension. In ACL (1), pp. 1601–1611. Association\nfor Computational Linguistics, 2017.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nKenton Lee, Ming-Wei Chang, and Kristina Toutanova. Latent retrieval for weakly supervised\nopen domain question answering. In ACL (1), pp. 6086–6096. Association for Computational\nLinguistics, 2019.\nYankai Lin, Haozhe Ji, Zhiyuan Liu, and Maosong Sun. Denoising distantly supervised open-\ndomain question answering. In ACL (1), pp. 1736–1745. Association for Computational Linguis-\ntics, 2018.\nXiao Ling and Daniel S. Weld. Fine-grained entity recognition. In AAAI. AAAI Press, 2012.\nAngli Liu, Jingfei Du, and Veselin Stoyanov. Knowledge-augmented language model and its appli-\ncation to unsupervised named-entity recognition. InNAACL-HLT (1), pp. 1142–1150. Association\nfor Computational Linguistics, 2019a.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019b.\nRobert Logan, Nelson F. Liu, Matthew E. Peters, Matt Gardner, and Sameer Singh. Barack’s wife\nhillary: Using knowledge graphs for fact-aware language modeling. In ACL, pp. 5962–5971.\nACL, 2019.\nBryan McCann, James Bradbury, Caiming Xiong, and Richard Socher. Learned in translation:\nContextualized word vectors. In NIPS, pp. 6294–6305, 2017.\nTodor Mihaylov and Anette Frank. Knowledgeable reader: Enhancing cloze-style reading compre-\nhension with external commonsense knowledge. InProceedings of the 56th Annual Meeting of the\nAssociation for Computational Linguistics (Volume 1: Long Papers) , pp. 821–832, Melbourne,\nAustralia, July 2018. Association for Computational Linguistics. doi: 10.18653/v1/P18-1076.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems, pp. 3111–3119, 2013.\nMarvin Minsky. Society of mind. Simon and Schuster, 1988.\nMaximilian Nickel, V olker Tresp, and Hans-Peter Kriegel. A three-way model for collective learning\non multi-relational data. In ICML, volume 11, pp. 809–816, 2011.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations, 2019.\nJeffrey Pennington, Richard Socher, and Christopher D. Manning. Glove: Global vectors for word\nrepresentation. In EMNLP, pp. 1532–1543. ACL, 2014.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. arXiv preprint arXiv:1802.05365,\n2018a.\nMatthew E. Peters, Mark Neumann, Luke Zettlemoyer, and Wen-tau Yih. Dissecting contextual\nword embeddings: Architecture and representation. In EMNLP, pp. 1499–1509. Association for\nComputational Linguistics, 2018b.\nMatthew E Peters, Mark Neumann, IV Logan, L Robert, Roy Schwartz, Vidur Joshi, Sameer Singh,\nand Noah A Smith. Knowledge enhanced contextual word representations. EMNLP, 2019.\n10\nPublished as a conference paper at ICLR 2020\nFabio Petroni, Tim Rockt¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H. Miller,\nand Sebastian Riedel. Language models as knowledge bases? EMNLP, 2019.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI Blog, 1(8), 2019.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100, 000+ questions for\nmachine comprehension of text. In EMNLP, pp. 2383–2392. The Association for Computational\nLinguistics, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pp. 5998–6008, 2017.\nDenny Vrandeˇci´c and Markus Kr¨otzsch. Wikidata: a free collaborative knowledge base. 2014.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nICLR, 2019a.\nShuohang Wang, Mo Yu, Xiaoxiao Guo, Zhiguo Wang, Tim Klinger, Wei Zhang, Shiyu Chang,\nGerry Tesauro, Bowen Zhou, and Jing Jiang. R 3: Reinforced ranker-reader for open-domain\nquestion answering. In AAAI, pp. 5981–5988. AAAI Press, 2018a.\nShuohang Wang, Mo Yu, Jing Jiang, Wei Zhang, Xiaoxiao Guo, Shiyu Chang, Zhiguo Wang, Tim\nKlinger, Gerald Tesauro, and Murray Campbell. Evidence aggregation for answer re-ranking in\nopen-domain question answering. In ICLR, 2018b.\nZhiguo Wang, Patrick Ng, Xiaofei Ma, Ramesh Nallapati, and Bing Xiang. Multi-passage\nbert: A globally normalized bert model for open-domain question answering. arXiv preprint\narXiv:1908.08167, 2019b.\nWenhan Xiong, Thien Hoang, and William Yang Wang. DeepPath: A reinforcement learning\nmethod for knowledge graph reasoning. In Proceedings of the 2017 Conference on Empirical\nMethods in Natural Language Processing, pp. 564–573, Copenhagen, Denmark, September 2017.\nAssociation for Computational Linguistics. doi: 10.18653/v1/D17-1060.\nWenhan Xiong, Mo Yu, Shiyu Chang, Xiaoxiao Guo, and William Yang Wang. Improving question\nanswering over incomplete KBs with knowledge-aware reader. InProceedings of the 57th Annual\nMeeting of the Association for Computational Linguistics, Florence, Italy, July 2019. Association\nfor Computational Linguistics. doi: 10.18653/v1/P19-1417.\nBishan Yang and Tom M. Mitchell. Leveraging knowledge bases in lstms for improving machine\nreading. In ACL (1), pp. 1436–1446. ACL, 2017.\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin.\nEnd-to-end open-domain question answering with bertserini. arXiv preprint arXiv:1902.01718,\n2019a.\nWei Yang, Yuqing Xie, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. Data augmentation for\nbert ﬁne-tuning in open-domain question answering. arXiv preprint arXiv:1904.06652, 2019b.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. arXiv preprint\narXiv:1906.08237, 2019c.\nRowan Zellers, Yonatan Bisk, Roy Schwartz, and Yejin Choi. SW AG: A large-scale adversarial\ndataset for grounded commonsense inference. In EMNLP, pp. 93–104. ACL, 2018.\nSheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme.\nRecord: Bridging the gap between human and machine commonsense reading comprehension.\narXiv preprint arXiv:1810.12885, 2018.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang, Maosong Sun, and Qun Liu. ERNIE: enhanced\nlanguage representation with informative entities. In ACL (1), pp. 1441–1451. ACL, 2019.\n11\nPublished as a conference paper at ICLR 2020\nYukun Zhu, Ryan Kiros, Richard S. Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by\nwatching movies and reading books. In ICCV, pp. 19–27. IEEE Computer Society, 2015.\n12\nPublished as a conference paper at ICLR 2020\nA A PPENDIX\nImplementation Details and Hyperparameters We implement our method using Fairseq Ott\net al. (2019) and the fact completion baselines are implemented with Huggingface’s Pytorch-\nTransformers9. We pretrain the models with 32 V100 GPUs for 3 days. We use at most 2 GPUs for\nﬁne-tuning the paragraph reader, use 8 GPUs for ﬁne-tuning the paragraph ranker. The entity-typing\nexperiments require larger batch sizes and take 8 GPUs for training.\nFor the knowledge learning pretraining phase, we use the Adam optimizer (Kingma & Ba, 2014)\nwith learning rate 1e-5, batch size 128 and weight decay 0.01. The model is pretrained on 32\nV100 GPUs for 3 days. To train the paragraph reader for open-domain QA, we select the best\nlearning rate from {1e-6, 5e-6, 1e-5, 2e-5}and last layer dropout ratio from {0.1, 0.2}. We set the\nmaximum training epoch to be 10 and batch size to be 32. The maximal input sequence length is 512\nfor WebQuestions and 128 for the other three datasets that use sentence-level paragraphs. For the\nparagraph ranker, we choose learning rate from {1e-5, 2e-5, 5e-6}, use dropout 0.1 and batch size\n256. The maximal sequence length for each dataset is consistent with the one we used for training\nthe paragraph reader. The linear combination of ranking and extraction scores is selected based on\nvalidation performance. For SQuAD experiments, we select learning rate from {1e-5, 5e-6, 2e-5,\n3e-5}, learning rate from {8, 16 }, last layer dropout ratio from {0.1, 0.2 }. We set the maximal\nsequence length as 512 and the maximal training epoch as 5. For entity typing, we select learning\nrate from {1e-5, 2e-5, 3e-5, 5e-5 }and batch size from {128, 256}. We set the maximal sequence\nlength to be 256, the last layer dropout ratio to be 0.1. The model is ﬁne-tuned for at most 3 epochs\nto prevent overﬁtting. The threshold for type prediction is selected on the validation set.\nContext Collection for QA DatasetsFor WebQuestions, we collect evidence context using the\ndocument retriever of DrQA (Chen et al., 2017), which uses TF-IDF based metric to retrieve the\ntop 5 Wikipedia articles. For Quasar-T, we use Lucene ranked paragraphs. For SearchQA and\nTriviaQA, we use paragraphs ranked by search engines. Following existing research (Wang et al.,\n2018b; Lin et al., 2018), we use sentence-level paragraphs for SearchQA (50 sentences), TriviaQA\n(100 sentences) and SearchQA (100 sentences).\nCorrelation between Fact Completion Results and Properties of RelationsFigure 2 shows the\nfact completion results of BERT are unstable on different relations with different properties, i.e.,\nBERT’s performance is strongly correlated with the size of candidate entity set and the number of\ngroundtruth answers. Compared to BERT, WKLM is often less sensitive to these two factors.\n200 300 400 500 600 700 800 900\nNumber of candidate entities\n10\n0\n10\n20\n30\n40\n50\n60\nHits@10\nWKLM\nBERT-large\n2 3 4 5\nNumber of Groundtruth Answers\n20\n0\n20\n40\n60\nHits@10\nWKLM\nBERT-large\nFigure 2: Left: Correlation between candidate set size and hits@10; Right: Correlation between\nnumber of groundtruth answers and hits@10.\n9https://huggingface.co/pytorch-transformers\n13",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8242062926292419
    },
    {
      "name": "Language model",
      "score": 0.6965910196304321
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6959161758422852
    },
    {
      "name": "Natural language processing",
      "score": 0.659500002861023
    },
    {
      "name": "Task (project management)",
      "score": 0.6160467863082886
    },
    {
      "name": "Question answering",
      "score": 0.5238590240478516
    },
    {
      "name": "Machine learning",
      "score": 0.4388974606990814
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    }
  ]
}