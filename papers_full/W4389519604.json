{
  "title": "ClusterLLM: Large Language Models as a Guide for Text Clustering",
  "url": "https://openalex.org/W4389519604",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2107635733",
      "name": "Yuwei Zhang",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2104555532",
      "name": "Zihan Wang",
      "affiliations": [
        "University of California, San Diego"
      ]
    },
    {
      "id": "https://openalex.org/A2223914299",
      "name": "Jingbo Shang",
      "affiliations": [
        "University of California, San Diego"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4366733439",
    "https://openalex.org/W2962852342",
    "https://openalex.org/W4384662964",
    "https://openalex.org/W4311991106",
    "https://openalex.org/W4283011496",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W4385572035",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4389523802",
    "https://openalex.org/W4389519061",
    "https://openalex.org/W4361807267",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3101553402",
    "https://openalex.org/W4310923309",
    "https://openalex.org/W3106709020",
    "https://openalex.org/W2971136144",
    "https://openalex.org/W2998721586",
    "https://openalex.org/W3123001214",
    "https://openalex.org/W3137513727",
    "https://openalex.org/W4312637933",
    "https://openalex.org/W4292692470",
    "https://openalex.org/W4285275136",
    "https://openalex.org/W2779692282",
    "https://openalex.org/W4382202733",
    "https://openalex.org/W4312278673",
    "https://openalex.org/W3034323190",
    "https://openalex.org/W2556467266",
    "https://openalex.org/W2964074409",
    "https://openalex.org/W4385572438",
    "https://openalex.org/W3194782062",
    "https://openalex.org/W4312297403",
    "https://openalex.org/W1997919544",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3153660069",
    "https://openalex.org/W4287120828",
    "https://openalex.org/W2969521304",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W1585610988",
    "https://openalex.org/W3099910226",
    "https://openalex.org/W4210829109",
    "https://openalex.org/W2153839362",
    "https://openalex.org/W2139792342",
    "https://openalex.org/W2120303002",
    "https://openalex.org/W1509525679",
    "https://openalex.org/W4221167718",
    "https://openalex.org/W3156414406",
    "https://openalex.org/W1987971958",
    "https://openalex.org/W2251410829",
    "https://openalex.org/W3171153522",
    "https://openalex.org/W3177312484",
    "https://openalex.org/W4385573933",
    "https://openalex.org/W3110446398",
    "https://openalex.org/W3030583876",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4386576685",
    "https://openalex.org/W3209666692",
    "https://openalex.org/W3121375627",
    "https://openalex.org/W4385573970",
    "https://openalex.org/W2134089414",
    "https://openalex.org/W4312281441",
    "https://openalex.org/W4290056061",
    "https://openalex.org/W3045492832",
    "https://openalex.org/W4366343198",
    "https://openalex.org/W4281719801",
    "https://openalex.org/W4245436919",
    "https://openalex.org/W4386265460",
    "https://openalex.org/W2950180292",
    "https://openalex.org/W4385734175",
    "https://openalex.org/W4385570594",
    "https://openalex.org/W4226278401"
  ],
  "abstract": "We introduce ClusterLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon \"small\" embedders, ClusterLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user's preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by constructing hard triplet questions <does A better correspond to B than C>, where A, B and C are similar data points that belong to different clusters according to small embedder. We empirically show that this strategy is both effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on clustering granularity by carefully designed pairwise questions <do A and B belong to the same category>, and tune the granularity from cluster hierarchies that is the most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that ClusterLLM consistently improves clustering quality, at an average cost of ~$0.6 per dataset.",
  "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 13903‚Äì13920\nDecember 6-10, 2023 ¬©2023 Association for Computational Linguistics\nCLUSTER LLM: Large Language Models as a Guide for Text Clustering\nYuwei Zhang Zihan Wang Jingbo Shang ‚àó\nUniversity of California, San Diego\n{yuz163, ziw224, jshang}@ucsd.edu\nAbstract\nWe introduce CLUSTER LLM, a novel text clus-\ntering framework that leverages feedback from\nan instruction-tuned large language model,\nsuch as ChatGPT. Compared with traditional\nunsupervised methods that builds upon ‚Äúsmall‚Äù\nembedders, CLUSTER LLM exhibits two in-\ntriguing advantages: (1) it enjoys the emergent\ncapability of LLM even if its embeddings are\ninaccessible; and (2) it understands the user‚Äôs\npreference on clustering through textual in-\nstruction and/or a few annotated data. First,\nwe prompt ChatGPT for insights on clustering\nperspective by constructing hard triplet ques-\ntions <does A better correspond to B than C>,\nwhere A, B and C are similar data points that\nbelong to different clusters according to small\nembedder. We empirically show that this strat-\negy is both effective for fine-tuning small em-\nbedder and cost-efficient to query ChatGPT.\nSecond, we prompt ChatGPT for helps on\nclustering granularity by carefully designed\npairwise questions <do A and B belong to\nthe same category >, and tune the granular-\nity from cluster hierarchies that is the most\nconsistent with the ChatGPT answers. Ex-\ntensive experiments on 14 datasets show that\nCLUSTER LLM consistently improves cluster-\ning quality, at an average cost of ‚àº$0.61 per\ndataset. The code will be available at https:\n//github.com/zhang-yu-wei/ClusterLLM.\n1 Introduction\nText clustering, as a fundamental task in natural lan-\nguage processing (NLP), has a wide spectrum of\napplications, such as identifying public perception\nfrom social media (Park et al., 2022), analysing\ncause of accidents (Xu et al., 2022), and detecting\nemerging research topics (Mart√≠nez et al., 2022). A\ncommon practice for text clustering is to apply clus-\ntering algorithms (MacQueen, 1967; Zhang et al.,\n‚àóCorresponding author.\n1The cost is calculated with gpt-3.5-turbo.\nTexts CAB\nTraditional Text Clustering\nChatGPT(API-based)\nüîíClusterLLM\nCAB\nTexts CABChatGPT(API-based)\nüîíA should be closer to C than B\nüßê\nNot Applicable \n‚õîEmbedder(Instructor,E5,GTR ‚Ä¶)\nEmbedder(Instructor,E5,GTR ‚Ä¶)\nFigure 1: LLMs like ChatGPT are not applicable for text\nclustering directly because of the inaccessible embed-\ndings. CLUSTER LLM resolves the dilemma by leverag-\ning LLM as a guide on text clustering.\n2021a) on top of pre-trained embedders (Muen-\nnighoff et al., 2022; Wang et al., 2022; Su et al.,\n2022) which could achieve higher performance\nwith better pre-training quality. State-of-the-art\nlarge language models (LLMs) such as recent GPT\nseries (Brown et al., 2020; Ouyang et al., 2022;\nOpenAI, 2023) have demonstrated extraordinary\nlanguage capabilities for various NLP applications\nhowever, these GPT models can only be utilized\nthrough the APIs without accessible embedding\nvectors for clustering. Hence, LLMs cannot be\ndirectly applied on text clustering tasks.\nIn this paper, we provide insights on the ques-\ntion: Can we leverage API-based LLMs to guide\ntext clustering efficiently? We attack this challeng-\ning question by drawing inspiration from an obser-\nvation that humans represent an instance through\ncomparing with others (Nosofsky, 2011). For in-\nstance, people often classify a new piece of mu-\nsic into a specific genre by relating to familiar\nones. In fact, pairwise relationships have been uti-\nlized in spectral clustering (Donath and Hoffman,\n1972; Cheeger, 1970) before. Nonetheless, naively\n13903\n123456789\nStep 1Step 2Step 3Step 4\nStep 0\nText Corpus\nEmbedder(Instructor,E5, ‚Ä¶)(Small) Language Models\nAPI-based Large Language Models(Embeddings \nüîí)\nüßê\nStage 1: Improve Perspectiveshould be closer tothanshould be closer tothan‚Ä¶\nüî•Fine-tunedEmbedder(Instructor,E5, ‚Ä¶)\nTriplet                Task: Select based on shape.Which is closer to      ?Choice 1:  Choice 2: \nentropy\n‚Ä¶\nEntropy-basedTriplet Sampling\nEmbedding Space\n            Pairwise            Task      : [Example1][Example 2] ‚Ä¶Are        and       in the same cluster?Please answer with Yes or No?  3 7\nAPI-based Large Language Models(Embeddings \nüîí)\nüßêin different clusters3 7and4and5in the same cluster‚Ä¶Clusters: {1,2}, {3}, {4,5}\nStage 2:Determine Granularity\n‚úÖ\n‚ùå\n‚ùå\n‚úÖ Consistency\nFigure 2: An overview of CLUSTER LLM. It utilizes LLM to guide an embedder for text clustering with a low cost.\ntraversing all the pairs within dataset is obviously\nintractable and too expensive for querying LLMs.\nWe propose CLUSTER LLM , a framework that\nutilizes LLM to guide a small embedder for finding\ntext clusters with a low cost, as shown in Figure 1.\nIt comprises two stages that are specially designed\nfor two aspects of clustering: (1) perspective, i.e.,\nthe grouping criterion such astopic, intent and emo-\ntion and (2) granularity, i.e. the scope of clusters.\nIn Stage 1, we prompt LLMs with a triplet\ntask that predicts which one of the two candidate\nchoices is closer to anchor instance to understand\nthe user-preferred perspectives. We choose this\ntriplet task because (a) it is irrelevant with cluster\ngranularity and (b) the produced triplets can fine-\ntune small embedder towards the right perspective.\nIn order to improve sample efficiency, we further\npropose entropy-based triplet sampling to find the\nmost informative triplets. Specifically, we first cal-\nculate entropy for each instance based on cluster\nassignment probabilities, and then identify those\nwith highest entropy. Two candidate choices are\nthen sampled from its nearest clusters to guarantee\nthey are close enough to the anchor.\nIn Stage 2, we first obtain the cluster hierarchy\nthat starts from instance-level clusters and itera-\ntively merge two closest clusters until the entire\ndataset. And then we prompt LLMs to determine\ncluster granularity with a few annotated data pairs\nas demonstrations. We construct the data pairs\nto prompt by sampling from two clusters that are\nmerged at each step of hierarchical clustering, so\nthat they cover a wide range of granularities. And\nthe final decision is made by measuring consistency\nbetween each level of clustering and predictions.\nWe extensively evaluateCLUSTER LLM on 14\ndatasets that include diverse tasks such as intent dis-\ncovery, topic mining, type discovery, domain dis-\ncovery, and emotion detection. Furthermore, these\ndatasets span a wide range of granularities that have\n10 to 150 number of clusters. We show that CLUS -\nTER LLM is effective overall on improving clus-\ntering quality, where the clustering performance is\nimproved over both a deep clustering baseline and\na self-supervise baseline. Moreover, the ablation\nstudy shows that our sampling strategy is effective\ncompared to a random sampling baseline. Finally,\nCLUSTER LLM also outperforms clustering-error\nbased methods on determining cluster granularity.\nIn summary, our contributions are three-fold: (i)\nWe propose a framework CLUSTER LLM that uti-\nlizes sentence relations predicted from API-based\nLLMs to guide clustering. Furthermore, it allows\nusers to provide textual instructions and/or few-\nshot annotations to specify preferences on cluster-\ning. (ii) In order to reduce API-queries, we propose\na novel entropy-based sampling strategy to find the\nmost informative triplets. Additionally, we utilize\npairwise data sampled from hierarchical cluster-\ning to determine cluster granularity. (iii) Extensive\nexperiments show that our proposed method can\nimprove clustering performance at ‚àº$0.2 for per-\nspective and ‚àº$0.4 for granularity with GPT-3.5.\n2 Preliminary\nText clustering takes an unlabeled corpus D =\n{xi}N\ni=1 as input, and outputs a clustering assign-\nment Y= {yi}N\ni=1 that maps the input text to clus-\nter indices. To specify user‚Äôs needs, CLUSTER -\nLLM integrates additional textual instruction (e.g.\n‚ÄúSelect the example that better corresponds with the\nQuery in terms of entity type.‚Äù) to understand per-\nspective and few-shot annotations (e.g. ‚ÄúSentence1\nand Sentence2 have the same entity type ...‚Äù) to\ndetermine cluster granularity.\n3 Our C LUSTER LLM\nCLUSTER LLM is based on a pre-trained small em-\nbedder (Wang et al., 2022; Su et al., 2022) (denoted\n13904\nas f) which usually represents sentences individu-\nally. In contrast, inspired by human cognitive abil-\nity (Nosofsky, 2011), CLUSTER LLM considers a\npair or a triplet of sentences through prompting\nLLMs that are trained to follow human instruc-\ntions (Ouyang et al., 2022; OpenAI, 2023). Specif-\nically, CLUSTER LLM is a two-stage framework\n(See Figure 2). In Section 3.1 we introduce Stage\n1 that utilizes triplet task to improve clustering\nquality with respect to user-specified perspectives,\nalong with a sampling strategy that reduces number\nof API queries. In Section 3.2, we introduce Stage\n2 that leverages pairwise task to determine cluster\ngranularity based on predictions from LLMs.\n3.1 Triplet Task for Perspective\nIn this section, we explore how to harness a triplet\ntask to refine the cluster structures for a user-\nspecified perspective. A triplet task takes as input\na tuple of three sentences t= (a,c1,c2), where a\nis the anchor and (c1,c2) are two choices. We then\nprompt LLMs to select one of (c1,c2) that better\ncorresponds with ausing a prompt PT . Moreover,\nin order to specify the user‚Äôs perspective,PT also\nrequires a task instruction IT as input. The LLM\nshould make a choice\ncj = PT (IT ,t), (1)\nwhere cj ‚àà{c1,c2}indicates one of the choices\nthat LLM selects as positive and we denote the\nother (or negative) one as c\\j.\n3.1.1 Entropy-based Triplet Sampling\nWhile one can randomly sample triplets to query\nthe LLM, we demonstrate it non-efficient in exper-\niments. In this section, we pose the question of\nmining informative triplets to both save the costs\nfrom querying LLMs and optimally improve the\nclustering. To achieve this, we resort to the current\nclustering results from the extracted embeddings\nZ = {zi = f(xi)}N\ni=1. In summary, our algo-\nrithm contains two steps: Step 1: We find the most\nambiguous instances as anchors based on entropy.\nStep 2: For each anchor instance, we sample two\nchoices from two of its closest clusters. Refer to\nAlgorithm 1 for entire process.\nIn Step 1, since the granularity is unknown at\ncurrent stage, we perform clustering on top of Z,\nwhere the clustering hyperparameters2 are consis-\n2It can be number of clusters in K-means or maximum\ndistance to merge two clusters in hierarchical clustering\nAlgorithm 1: Entropy-based Triplet Sampling\nInput: embeddings Z= {zi = f(xi)}N\ni=1, interval\nboundaries Œ≥high and Œ≥low, closest clusters\nfraction œµ= 2%, maximum number of queries\nQ.\n1 Step 1: Kclusters ‚ÜêClustering(Z);\n2 Compute ¬µk for each cluster kby averaging;\n3 Compute Kclosest with Eq. 3;\n4 Compute entropy Hwith Eq. 4;\n5 Ind ‚Üêargsort(H)[::-1];\n6 Ind ‚ÜêInd[Œ≥highN:Œ≥lowN];\n7 Step 2: Initialize triplets {tq}‚Üê{} ;\n8 while len({tq})<Q do\n9 for ain Ind do\n10 Obtain Kclosest closest clusters;\n11 Sample C1,C2 from closest clusters;\n12 c1 ‚àºC1,c2 ‚àºC2, t= (a,c1,c2);\n13 if tnot in {tq},c1 Ã∏= a,c2 Ã∏= athen\n14 Append tto {tq};\nOutput: A set of triplets {tq}Q\nq=1\ntent across datasets and only specific to the embed-\nder model f. Cluster center ¬µk will thereafter be\ncalculated for cluster kby averaging embeddings\nassigned to it. Following (Xie et al., 2016; Van der\nMaaten and Hinton, 2008), we calculate instance-\nwise soft assignments with Student‚Äôs t-distribution,\npik = (1 + ||zi ‚àí¬µk||2/Œ±)‚àíŒ±+1\n2\n‚àë\nk‚Ä≤(1 + ||zi ‚àí¬µk‚Ä≤||2/Œ±)‚àíŒ±+1\n2\n(2)\nwhere Œ± = 1 is the degree of freedom. We then\ndefine closest clusters for instance ias Kclosest clus-\nters with largest soft assignment pik. Here, Kclosest\nis proportional to the total number of clusters K.\nKclosest = max(œµK,2) (3)\nwhere we fix œµ to be a small value, such as 2%.\nWe then compute entropy based on these closest\nclusters with renormalized probabilities p‚Ä≤\nik,\nhi = ‚àí\nKclosest‚àë\nk=1\np‚Ä≤\nik log(p‚Ä≤\nik) (4)\nwhere p‚Ä≤\nik = pik‚àëKclosest\nk‚Ä≤=1 pik‚Ä≤\n. We sort the entire dataset\nin descending order according to the entropies\nH = {hi}N\ni=1. We introduce two hyperparameters\nŒ≥high and Œ≥low that control the proportion interval\nto filter out from ordered dataset. Our hypothesis\nis that higher entropy (smaller Œ≥high and Œ≥low) an-\nchors form more informative triplets that we verify\nin Section 4.6. In Step 2, we randomly sample\ntwo clusters C1,C2 from Kclosest closest clusters,\n13905\nand then sample two sentences c1,c2 from each\nof them as choices (see line 11 and line 12). In\nanother word, these choices should be either a pos-\nitive or a hard negative to the anchor. Finally, we\nalso remove triplets that are either repeated or have\nidentical choice and anchor. We continue to sample\ntriplets until reaching budget Q.\nRemarks. (1) Since Qis defined by the user and\nis independent with the dataset size, our sampling\nis cost-efficient. For example, in our experiments,\nusing 1,024 queries can improve performance on\nboth dataset scales of ‚àº3,000 and ‚àº50,000. (2)\nFrom the view of ground truth, the sampled triplets\nmight contain ‚Äúboth are correct‚Äù or ‚Äúnone of the\nabove‚Äù. However, we argue that even these triplets\nmight provide soft aligning information, i.e. the\nranking of closeness between choices. (3) Our\nsampling method may also be utilized in active\nlearning to acquire human annotations when no\nprior knowledge is available on the categories.\n3.1.2 Finetuning Embedder\nNow that we have the triplet predictions, it is still\nnot clear how to utilize them in clustering. Previous\nresearch rely on deep constrained clustering (Zhang\net al., 2020; Manduchi et al., 2021) which are of-\nten sensitive to noisy labels (Basu et al., 2008).\nIn this paper, we instead focus on finetuning the\nbase embedder f towards producing an embedding\nspace that better explains the user‚Äôs perspective.\nWe exploit both hard and in-batch negatives. Fol-\nlowing (Su et al., 2022; Ni et al., 2022b), for a\ntriplet t = ( a,cj,c\\j) with positive cj and hard\nnegative c\\j, we optimize the following objective,\nlj = exp (s(a,cj)/œÑ)‚àë\ncl‚ààBexp (s(a,cl)/œÑ) (5)\nwhere Bcombines cj, c\\j and other in-batch nega-\ntives. œÑ is a temperature parameter. Following the\noriginal implementation, we also compute the loss\nwith aand cj swapped. Finally fine-tuned embed-\nders can be applied to find even more informative\ntriplets with our sampling method which will fur-\nther improve performance in an iterative manner.\nWe acquire clustering assignments by running clus-\ntering algorithms on top of extracted embeddings.\n3.2 Pairwise Task for Granularity\nIn this section, we build upon the refined embed-\nding space in Section 3.1 to determine cluster gran-\nularity. In this paper, we convert the problem of\ndetermining granularity into finding the best step\nin a cluster hierarchy (see Figure 2 right), where\neach step denotes a unique granularity (or equally\nnumber of clusters). It is non-trivial, since differ-\nent granularities can be applied to the same dataset\n(such as domains or topics). To tackle this chal-\nlenge, we query LLM with pairwise task that pre-\ndicts whether a pair of data pbelong to the same\ncluster with a prompt PP ,\nw= PP (IP ,{Àúpd}D\nd=1,p) (6)\nwhere w‚àà{same,different}is the binary decision,\nIP is the task instruction and {Àúpd}D\nd=1 are few-shot\ndemonstration pairs used for in-context learning\n(typically D= 4). We assume these demonstration\npairs are annotated by users who have a desired\ncluster granularity in mind. We also combine a\nbrief justification for each demonstration pair (see\nTable 12 bottom for example).\n3.2.1 Determine Granularity with Pairwise\nHierarchical Sampling\nWe then introduce how to sample pairs from cluster\nhierarchy to query LLMs and determine granularity.\nWe assume a maximum and a minimum number of\nclusters (denoted as kmax and kmin) following Pel-\nleg et al. (2000) which depend on the user‚Äôs expec-\ntation on the granularity. We then randomly sample\nŒª(1 or 3 in our experiments) pairs of data from the\ntwo clusters to be merged at each step to form can-\ndidate pairs {pi}Np\ni=1, where Np = Œª(kmax ‚àíkmin).\nThese pairs cover the entire range of granularity\nbetween kmax and kmin, and will be used to query\nLLMs. After that, each level of granularity can be\nexamined against LLM predictions to choose the\none with the highest consistency measure M,\nk‚àó= argmax\nk\nM(Wp,Wk) (7)\nwhere Wp = {wp\ni }Np\ni=1 denotes the predictions ob-\ntained from Eq. 6 andWk represents a set of binary\nvalues indicating whether each pair of data is in\nthe same cluster at granularity k. Empirically, we\nfound the following performs better in our frame-\nwork: use F-beta score, a weighted harmonic mean\nof precision and recall, as measurement Mand set\nWp/Wk as labels/predictions. Finally, for large-\nscale datasets, we address the high time complexity\nof hierarchical clustering by applying it on top of\nmini-batch K-means. See details in Appendix A.\nRemarks. Similar to Section 3.1.1, pairwise hierar-\nchical sampling can also be used to acquire human\n13906\nannotations. Nonetheless, the reliability of the al-\ngorithm still depends on the quality of clusters. In\nan extreme case where the clusters are completely\nrandom, it is unable to find granularity even though\nall the pairwise predictions are correct.\n4 Experiments\nWe first evaluateCLUSTER LLM on clustering qual-\nity with ground truth number of clusters in Sec-\ntion 4.4. Then we conduct ablation studies in\nSection 4.6 to further analyze the effectiveness of\nCLUSTER LLM . Finally, we show results of deter-\nmining cluster granularity in Section 4.7.\n4.1 Datasets\nWe provide a high-level summary of evaluated\ndatasets in this section, and see Appendix E for\nmore descriptions. In this paper, we evaluate on a\nbroad range of clustering datasets with various per-\nspectives and granularities. Furthermore, to better\nanalyze the effect of scale, each dataset has both a\nsmall-scale and a large-scale version. The two ver-\nsions are different in number of data while keeping\nthe same number of clusters. A summary of dataset\nstatistics is shown in Table 1. Note that there is no\ndata splits in clustering.\nIntent (Domain) Discovery. Intent discov-\nery (Zhang et al., 2021b, 2022) discovers unknown\nintents in unlabeled customer utterances. For\nCLINC, Massive and MTOP, we also use domains\nas labels to convert them into domain discovery.\nType Discovery. Type Discovery (Li et al., 2022)\nresolves the closed-world set-up of traditional Infor-\nmation Extraction. In this work, we focus on three\ntasks: entity, relation and event type discovery. To\nindicate specific mentions (entities or event trig-\ngers), we directly append them behind sentences\nwith natural language formats, such as ‚ÄúThe rela-\ntion between [ENTITY1] and [ENTITY2]‚Äù.\nTopic Mining. We adapt three topic mining\ndatasets from MTEB (Muennighoff et al., 2022).\nEmotion. We adapt GoEmo (Demszky et al.,\n2020), a fine-grained emotion detection dataset by\nremoving multi-label or neutral instances.\n4.2 Experiment Details\nQuery LLMs. The prompt only contains a task-\nspecific instruction (see Table 11). We set gen-\neration temperature to 0.5. Explanations are sup-\npressed by adding a postfix:‚ÄúPlease respond with\n‚ÄôChoice 1‚Äô or ‚ÄôChoice 2‚Äô without explanation‚Äù and\nTask Name #clusters #data(small) #data(large)\nIntent\nBank77 77 3,080 10,003\nCLINC(I) 150 4,500 15,000\nMTOP(I) 102 4,386 15,638\nMassive(I) 59 2,974 11,510\nType\nFewRel 64 4,480 40,320\nFewNerd 58 3,789 50,000\nFewEvent 34 4,742 18,969\nTopic\nStackEx 121 4,156 50,000\nArxivS2S 93 3,674 50,000\nReddit 50 3,217 50,000\nEmotion GoEmo 27 5,940 23,485\nDomain\nCLINC(D) 10 4,500 15,000\nMTOP(D) 11 4,386 15,667\nMassive(D) 18 2,974 11,514\nTable 1: Dataset statistics.\nset up a max token of 10. We then assign them to\nbinary choices by directly checking whether one\nof the texts ‚ÄúChoice 1‚Äù or ‚ÄúChoice 2‚Äù is in the re-\nsponse. We also find that a very small amount of\nresponses do not contain any choices and we dis-\ncard them during fine-tuning. We use the Python\nAPI tool provided by OpenAI.\nTriplet Sampling. For both small- or large-scale\nexperiments, we set a budget ofQ= 1,024 triplets.\nWe set Œ≥low = 20% and Œ≥high = 0. For clustering\nmethods, we fix hyperparameters of these algo-\nrithms across datasets in Stage 1. We choose ag-\nglomerative clustering with fixed distance threshold\n67 for small-scale experiments on Instructor, and\n77 on E5 (the embeddings are preprocessed by stan-\ndard scaler). For large-scale datasets, we choose\nmini-batch K-means with fixed number of clusters\n100 due to its lower latency. Clustering algorithms\nare implemented by scikit-learn (Pedregosa et al.,\n2011).\nFine-tune Embedders. In this work, we focus on\ntwo state-of-the-art pre-trained embedders: Instruc-\ntor (Su et al., 2022) and E5 (Wang et al., 2022). We\nonly use the large versions. Refer to Appendix D\nfor details.\nEvaluation. To reduce cost, we run CLUSTER -\nLLM once for each dataset. We then run (mini-\nbatch) K-means on (large) small-scale datasets for\n5 seeds with ground truth K. We show two metrics.\nThe first one is clustering accuracy calculated after\nHungarian alignment (Kuhn, 1955) that permute\nprediction classes back to label classes. Another\npopular metric for clustering is normalized mutual\ninformation (NMI) that calculates mutual informa-\ntion between two assignments, and normalized by\ntheir individual entropies.\n13907\nMethod\nIntent Discovery Emotion\nBank77 CLINC(I) MTOP(I) Massive(I) GoEmo\nACC NMI ACC NMI ACC NMI ACC NMI ACC NMI\nE5(Wang et al., 2022) 59.90(0.91) 77.71(0.42) 75.83(0.79) 91.16(0.42) 33.54(0.92) 70.79(0.26) 52.52(0.62) 70.67(0.50) 22.13(1.04) 20.98(0.53)\nSCCL-E(Zhang et al., 2021a)63.60(1.37) 77.34(0.62) 77.96(1.78) 91.89(0.49) 33.82(1.07) 70.42(0.34) 54.48(1.80) 71.57(0.89) 22.03(0.69) 20.05(0.63)\nself-supervise-E 64.76(2.09) 80.75(0.67) 78.91(0.69) 92.06(0.23) 33.81(0.81) 71.71(0.42) 54.23(1.57) 71.78(0.25) 21.35(0.42) 20.92(0.55)\nCLUSTERLLM-E 69.09(1.99) 83.17(0.48) 79.51(1.10) 92.73(0.36) 34.66(1.31) 73.19(0.41) 54.80(0.72) 73.97(0.38) 22.69(0.41) 22.07(0.23)\nCLUSTERLLM-E-iter 70.13(1.34) 84.16(0.36) 80.48(0.93) 92.92(0.29) 37.22(1.18) 74.46(0.11) 56.08(1.01) 74.39(0.21) 22.22(1.15) 22.23(0.17)\nInstructor(Su et al., 2022) 64.49(1.52) 81.43(0.61) 79.29(1.03) 92.60(0.19) 33.35(1.32) 70.63(0.27) 54.08(1.53) 73.42(0.62) 25.19(0.98) 21.54(0.46)\nSCCL-I(Zhang et al., 2021a)65.48(1.36) 81.77(1.36) 80.85(0.74) 92.94(0.44) 34.28(0.58) 73.52(0.38) 54.10(1.05) 73.90(0.36) 34.33(0.86) 30.54(0.68)\nself-supervise-I 68.18(0.73) 83.31(0.59) 80.82(0.75) 93.88(0.17) 34.06(0.64) 72.50(0.47) 55.07(1.25) 72.88(0.77) 24.11(2.02) 22.05(0.53)\nCLUSTERLLM-I 70.77(0.49) 85.07(0.33) 82.77(1.20) 93.88(0.17) 35.84(2.07) 73.52(0.38) 59.89(2.05) 76.96(0.54) 27.49(1.25) 24.78(0.56)\nCLUSTERLLM-I-iter 71.20(1.59) 85.15(0.41) 83.80(0.41) 94.00(0.21) 35.04(0.97) 73.83(0.79) 60.69(0.96) 77.64(0.21) 26.75(1.76) 23.89(0.68)\nMethod\nType Discovery Topic Mining\nFewRel FewNerd FewEvent StackEx Reddit\nACC NMI ACC NMI ACC NMI ACC NMI ACC NMI\nE5(Wang et al., 2022) 39.62(1.22) 55.78(0.25) 25.49(0.44) 40.62(0.16) 37.30(1.97) 59.28(0.52) 37.31(0.95) 58.59(0.62) 39.03(0.62) 48.63(0.75)\nSCCL-E(Zhang et al., 2021a)39.93(0.83) 56.49(0.10) 27.86(1.04) 43.12(0.47) 35.39(0.53) 57.23(0.35) 37.85(0.71) 59.17(0.33) 44.45(0.79) 53.29(0.71)\nself-supervise-E 43.01(1.46) 58.78(0.45) 29.25(0.74) 44.77(0.21) 37.07(0.70) 62.14(0.70) 42.19(0.79) 63.06(0.39) 48.69(2.40) 57.02(0.64)\nCLUSTERLLM-E 47.53(1.00) 62.89(0.30) 28.52(0.63) 44.45(0.20) 42.17(1.24) 67.55(0.31) 43.01(1.58) 63.81(0.50) 47.72(1.53) 56.41(0.43)\nCLUSTERLLM-E-iter 52.95(1.15) 67.64(0.29) 32.16(0.83) 48.16(0.25) 44.64(0.90) 70.74(0.35) 43.93(0.93) 64.66(0.36) 49.05(1.15) 57.35(0.31)\nInstructor(Su et al., 2022) 41.23(0.60) 57.55(0.41) 30.02(1.24) 46.12(0.54) 41.99(2.04) 62.88(0.67) 44.81(0.94) 65.76(0.32) 54.98(1.51) 62.51(0.62)\nSCCL-I(Zhang et al., 2021a)41.15(1.51) 57.04(0.34) 31.09(0.87) 46.47(0.47) 39.97(0.52) 57.36(0.43) 45.11(0.93) 65.36(0.16) 53.66(0.94) 61.34(0.57)\nself-supervise-I 41.72(0.47) 57.83(0.34) 31.39(0.74) 47.25(0.27) 43.94(1.15) 64.71(0.29) 46.15(1.17) 66.49(0.32) 55.41(0.93) 63.45(0.86)\nCLUSTERLLM-I 47.94(1.37) 62.43(0.43) 34.75(1.58) 51.03(0.57) 46.17(2.18) 70.73(0.34) 47.21(1.07) 66.78(0.29) 56.79(1.90) 63.87(0.56)\nCLUSTERLLM-I-iter 51.22(1.43) 65.53(0.40) 40.60(0.77) 57.35(0.23) 50.60(0.79) 73.89(0.47) 47.75(1.24) 67.08(0.30) 57.02(0.87) 63.92(0.38)\nMethod\nTopic Mining Domain Discovery Avg\nArxivS2S MTOP(D) Massive(D) CLINC(D)\nACC NMI ACC NMI ACC NMI ACC NMI ACC NMI\nE5(Wang et al., 2022) 30.85(0.37) 54.49(0.15) 91.23(4.23) 86.75(1.94) 63.70(0.86) 66.20(0.72) 59.64(2.73) 57.23(0.66) 47.72 61.35\nSCCL-E(Zhang et al., 2021a)32.78(0.69) 55.77(0.30) 92.28(0.18) 86.01(0.22) 61.22(1.15) 65.27(0.99) 59.88(2.74) 56.21(0.94) 48.82 61.70\nself-supervise-E 34.41(0.52) 57.09(0.26) 91.40(4.61) 88.20(1.71) 61.79(2.50) 65.02(1.32) 57.29(2.51) 57.07(0.88) 49.87 63.60\nCLUSTERLLM-E 34.93(0.36) 57.90(0.09) 89.18(5.25) 86.19(2.05) 60.73(2.81) 66.15(0.69) 56.25(2.94) 56.32(0.81) 50.77 64.77\nCLUSTERLLM-E-iter 35.73(0.87) 58.42(0.38) 89.58(5.08) 87.25(1.96) 58.63(1.32) 65.59(0.81) 60.84(2.88) 58.55(2.09) 52.40 66.18\nInstructor(Su et al., 2022) 24.31(0.77) 48.04(0.39) 90.56(3.34) 87.30(1.53) 61.81(2.56) 67.31(1.79) 52.50(2.44) 56.87(2.03) 49.90 63.85\nSCCL-I(Zhang et al., 2021a)25.63(0.53) 49.07(0.22) 89.08(3.77) 84.77(2.18) 61.34(2.77) 68.69(1.42) 54.22(3.15) 51.08(1.05) 50.74 63.85\nself-supervise-I 25.65(0.37) 49.41(0.17) 92.12(2.66) 88.49(1.25) 53.97(2.14) 71.53(0.77) 58.58(2.56) 60.84(1.04) 51.39 65.33\nCLUSTERLLM-I 26.61(0.48) 50.06(0.26) 93.53(0.10) 89.36(0.11) 61.06(1.91) 68.62(0.90) 52.39(1.84) 54.98(2.08) 53.09 66.58\nCLUSTERLLM-I-iter 26.34(0.38) 50.45(0.19) 92.13(3.81) 89.23(1.21) 60.85(4.33) 68.67(1.59) 51.82(1.91) 54.81(1.15) 53.99 67.53\nTable 2: Comparison of clustering accuracy and NMI with known granularity for evaluation. Average over all 14\ndatasets are shown in the last two columns. Best results are bolded.\n4.3 Compared Methods\nE5 and Instructor. We directly apply (mini-\nbatch) K-means on extracted embeddings from\ninstructor-large and e5-large.\nself-supervise-I(E). To verify that the performance\nimprovement of CLUSTER LLM does not only\ncome from domain-specific fine-tuning, instead of\nthe more accurate triplet prediction. We propose\na self-supervise fine-tuning that uses exactly the\nsame triplets as CLUSTER LLM but only switch to\nself-supervised triplet predictions that select closest\nchoices in embedding space.\nSCCL-I(E). We also combine Instructor and E5\nwith SCCL (Zhang et al., 2021a), an unsupervised\ndeep clustering algorithm that utilizes entire dataset\nfor training. Notice that our method uses fewer data\nfor training. See Appendix D for details.\n4.4 Main Results\nWe show main results with small-scale datasets in\nTable 2. We show several variants of our method:\nCLUSTER LLM-I(E) adopt Instructor or E5 as em-\nbedders. CLUSTER LLM -I(E)-iter applies the en-\ntire framework twice in an iterative manner by\nusing previous fine-tuned model as initialization\nand the 1,024 triplets inferred from new embed-\ndings for fine-tuning. All of these use GPT-3.5\nfor prediction. We make the following observa-\ntions: (1) CLUSTER LLM consistently improves\nupon both embedders. For example, CLUSTER -\n13908\nType Model Bank77 CLINC(I) MTOP(I) Massive(I) GoEmo FewRel FewNerd\nRandom\n#GT Triplets 23 6 102 61 117 41 156\nInstructor 100 100 98.04 88.52 68.38 80.49 71.15\nGPT3.5 100 100 85.29 85.25 68.38 85.37 82.05\n‚àÜ (+0) (+0) (-12.75) (-3.27) (+0) (+4.88) (+10.90)\nEntropy-based\n#GT Triplets 510 462 140 98 206 266 347\nInstructor 64.12 76.19 65.74 63.56 64.08 62.41 59.65\nGPT3.5 ‚Ä† 76.67 79.44 67.41 68.76 64.56 76.69 68.88\n‚àÜ (+12.55) (+3.25) (+1.67) (+5.20) (+0.48) (+14.28) (+9.23)\nGPT4 79.41 80.74 76.04 74.84 61.65 87.22 82.13\n‚àÜ (+15.29) (+4.55) (+10.30) (+11.28) (-2.43) (+24.81) (+22.48)\nType Model FewEvent StackEx Reddit ArxivS2S MTOP(D) Massive(D) CLINC(D)\nRandom\n#GT Triplets 105 14 40 22 184 148 189\nInstructor 98.10 85.71 80 95.45 96.74 80.41 76.72\nGPT3.5 94.29 71.43 70 81.82 85.87 82.43 68.25\n‚àÜ (-3.81) (-14.28) (-10) (-13.63) (-10.87) (+2.02) (-18.47)\nEntropy-based\n#GT Triplets 259 271 92 145 144 108 208\nInstructor 70.66 68.27 61.98 59.31 70.31 63.82 75.06\nGPT3.5 ‚Ä† 83.78 71.22 63.28 73.79 69.79 72.09 75.78\n‚àÜ (+13.12) (+2.95) (+1.30) (+14.48) (-0.52) (+8.27) (+0.72)\nGPT4 85.71 79.70 67.71 77.93 72.40 70.28 74.58\n‚àÜ (+15.05) (+11.43) (+5.73) (+18.62) (+2.09) (+6.46) (-0.48)\nTable 3: Analysis on the triplet prediction accuracy (‚Ä† is used to produce results of CLUSTER LLM -I in Table 2).\nRed and green mean decreased or increased performances respectively. ‚Äú#GT Triplets‚Äù means triplets that have\nground truth (see Section 4.5 for details).\nLLM -I increases the performance by 6.71% on\nFewRel. CLUSTER LLM -E increases the perfor-\nmance by 9.19 on Bank77. However, we do ob-\nserve that on Massive(D) and CLINC(D), there are\nno improvements. (2) CLUSTER LLM outperforms\ndeep clustering and self-supervise baselines. For in-\nstance, CLUSTER LLM-I surpasses self-supervise-I\non most datasets except for two and it is also better\nthan SCCL-I on 11 over 14 datasets. Furthermore,\nthese improvements are consistent across both re-\nported metrics. (3) Combined with the results in\nAppendix F, applying CLUSTER LLM iteratively\nis beneficial, emphasizing the potential of further\nimprovements.\n4.5 Analysis on Triplet Prediction Accuracy\nWe attribute the improvements on clustering qual-\nity to more accurate triplet predictions. In Table 3,\nwe show the accuracy on predicted triplets that\nhave ground truth (exactly one positive and one\nnegative choices based on ground truth) with two\ndifferent sampling methods. Random triplet sam-\npling uniformly samples three random instances as\nquery and two choices, and we guarantee the two\nchoices are different from the anchor by filtering.\nFurthermore, we also show a selection accuracy\nwith Euclidean distances between embeddings as\na comparison. We observe that, GPT-3.5/4 consis-\ntently improves upon Instructor on high entropy ex-\namples, demonstrating our hypothesis. In contrast,\nwith random sampling, the ground truth triplets is\nsignificantly fewer and the accuracy gap is much\nsmaller or even decreases performance.\n4.6 Ablation Study\nClustering Quality. We show ablation studies\non CLUSTER LLM based on Instructor in Table 4.\nSpecifically, we present results with 3 kinds of\npredictions on the same set of triplets for fine-\ntuning: GPT-3.5/4, replace triplet predictions of\nGPT-3.5 to ground truth on those triplets that have\nground truth. We observe that GPT-4 marginally\nimproves upon GPT-3.5 given the much higher\ncost. When provided with human labels, CLUS -\nTER LLM-GT&GPT3.5 achieves the highest perfor-\nmance, which indicates the possibility for further\nimprovement with more accurate predictions. We\nmake similar observations for large-scale datasets\nin Table 6.\nSampling Strategy. In this section, we show abla-\ntion study on entropy-based sampling. In Figure 3,\nwe observe that clustering accuracy increases when\nincreasing entropies (or equally decreasing mean\nof interval) except for GoEmo. We make two hy-\npothesis: (1) LLMs are much better than small\nembedders on harder instances. (2) high-entropy\ninstances are generally more informative. In Ta-\nble 4, we observe that training with randomly se-\n13909\nMethod\nIntent Discovery Emotion\nBank77 CLINC(I) MTOP(I) Massive(I) GoEmo\nACC NMI ACC NMI ACC NMI ACC NMI ACC NMI\nCLUSTERLLM-GPT3.5(random)59.88(2.56) 79.69(0.63) 74.40(0.91) 90.38(0.20) 28.05(1.69) 61.76(0.62) 51.66(2.41) 68.87(0.73) 28.62(1.95) 25.88(1.02)\nCLUSTERLLM-GPT3.5 70.77(0.49) 85.07(0.33) 82.77(1.20) 93.88(0.17) 35.84(2.07) 73.52(0.38) 59.89(2.05) 76.96(0.54) 27.49(1.25) 24.78(0.56)\nCLUSTERLLM-GPT4 69.71(1.13) 84.68(0.40) 81.91(1.20) 93.76(0.24) 34.48(0.38) 73.57(0.40) 59.10(1.12) 76.59(0.41) 27.41(1.13) 23.77(0.42)\nCLUSTERLLM-GT&GPT3.5 71.35(1.97) 85.12(0.45) 84.00(1.04) 94.34(0.30) 36.86(0.42) 75.36(0.08) 59.27(1.43) 77.37(0.54) 30.91(1.16) 27.71(0.46)\nMethod\nType Discovery Topic Mining\nFewRel FewNerd FewEvent StackEx Reddit\nACC NMI ACC NMI ACC NMI ACC NMI ACC NMI\nCLUSTERLLM-GPT3.5(random)40.65(0.89) 56.54(0.30) 27.15(0.53) 43.56(0.49) 44.23(1.72) 66.75(0.87) 40.81(0.94) 62.10(0.34) 54.60(2.23) 61.82(1.64)\nCLUSTERLLM-GPT3.5 47.94(1.37) 62.43(0.43) 34.75(1.58) 51.03(0.57) 46.17(2.18) 70.73(0.34) 47.21(1.07) 66.78(0.29) 56.79(1.90) 63.87(0.56)\nCLUSTERLLM-GPT4 48.96(1.14) 63.58(0.39) 37.54(0.54) 53.94(0.27) 47.98(1.45) 71.32(0.70) 46.82(0.78) 66.72(0.11) 55.38(0.37) 63.45(0.49)\nCLUSTERLLM-GT&GPT3.5 48.91(1.20) 63.34(0.47) 37.27(0.61) 53.57(0.32) 48.12(1.52) 72.31(0.84) 47.55(1.17) 67.04(0.31) 58.33(1.26) 65.34(0.51)\nMethod\nTopic Mining Domain Discovery Avg\nArxivS2S MTOP(D) Massive(D) CLINC(D)\nACC NMI ACC NMI ACC NMI ACC NMI ACC NMI\nCLUSTERLLM-GPT3.5(random)22.03(0.28) 45.50(0.16) 87.00(2.27) 82.09(1.54) 56.40(2.35) 64.39(1.12) 60.27(4.20) 58.11(2.93) 48.27 61.96\nCLUSTERLLM-GPT3.5 26.61(0.48) 50.06(0.26) 93.53(0.10) 89.36(0.11) 61.06(1.91) 68.62(0.90) 52.39(1.84) 54.98(2.08) 53.09 66.58\nCLUSTERLLM-GPT4 26.16(0.22) 50.06(0.20) 92.04(2.67) 88.39(1.33) 60.16(2.97) 67.98(1.04) 57.45(2.48) 59.98(1.14) 53.22 66.98\nCLUSTERLLM-GT&GPT3.5 26.14(0.57) 50.19(0.33) 92.26(3.62) 89.36(1.42) 61.65(3.50) 69.51(1.50) 52.87(2.63) 56.43(1.21) 53.96 67.64\nTable 4: Ablation study on clustering quality with Instructor as backbone and known granularity for evaluation. See\nmore results with large-scale datasets in Table 6.\n10% 20% 30% 40% 50% 60% 70% 80% 90%\nMean of Interval (Entropy)\n0.93\n0.94\n0.95\n0.96\n0.97\n0.98\n0.99\n1.00Relative Clustering Accuracy\nBank77\nFewRel\nStackEx\nGoEmo\nFigure 3: Relative clustering accuracy (divided by max-\nimum for better aligning across datasets) of CLUSTER -\nLLM -GPT3.5 with different range of entropy selected.\nx-axis shows the mean of interval where interval length\nis set to 20%. For example, ‚Äúmean of interval = 50%‚Äù\nmeans Œ≥high = 40% and Œ≥low = 60% (see Section 3.1.1).\n‚ô¶ marks the setting for main experiments.\nlected triplets even decreases performance, which\ndemonstrates the cruciality of triplet sampling.\n4.7 Determining Cluster Granularity\nIn this section, we show the results for determining\ncluster granularity. We evaluate on a subset of\n8 datasets including various cluster granularities\nwith kmax = 200 and kmin = 2. We compare with\ndifferent methods that rely on clustering errors. For\nour methods, we show results with Œª = {1,3}\n(except for GPT-4 to reduce costs), which involve\n198 & 594 pairs in total respectively. To simulate\nexperts for providing demonstrations, we directly\nsample 16 pairs from small-scale datasets when\nŒª = 3 and then choose 2 positive and 2 negative\nas demonstrations. Notice that we use the same\ndemonstrations for large-scale experiments. See\nmore details in Appendix B.\nWe make several observations from Table 5 and\nTable 7: (1) Our methods have higher ranks. Most\nbaseline methods predict similar number of clusters\nfor domain and intent, while our methods can ef-\nfectively distinguish between the two. For instance,\non MTOP(I)/(D) in Table 5, BIC predicts num-\nber of clusters 69/64 while our method (GPT-3.5,\nŒª= 3) predicts 92/18. (2) Increasing Œªgenerally\nhelps (MTOP(D) in Table 5) but might not always\nmake a large difference. (3) GPT-4 significantly\nimproves upon GPT-3.5, probably due to its better\nunderstanding of demonstrations.\n5 Related Works\nClustering. As a fundamental task in machine\nlearning, clustering has been applied on diverse\ndata types, including texts (Xu et al., 2015; Hadi-\nfar et al., 2019; Zhang et al., 2021a), images (Yal-\ning Tao, 2021; Yang et al., 2016; Caron et al.,\n2018; Niu et al., 2020; Xie et al., 2016) and\ngraphs (Huang et al., 2014; Chiang et al., 2019).\nRecent research has been shifted to deep cluster-\n13910\nMethod Bank77 FewRel Massive(I) Massive(D) MTOP(I) MTOP(D) CLINC(I) CLINC(D)Rank\nGT #clusters 77 64 59 18 102 11 150 10 -\nSilhouette(Rousseeuw, 1987)118(53.25) 10(84.38) 38(35.59) 41(127.8) 11(89.22) 11(0) 172(14.67) 163(1530) 10\nElbow(Thorndike, 1953) 53(31.17) 43(32.81) 45(23.73) 46(155.6) 33(67.65) 34(209.1) 66(56.00) 68(580.0) 9\nX-means(Pelleg et al., 2000) 69(10.39) 30(53.13) 32(45.76) 34(88.89) 28(72.55) 27(145.5) 130(13.33) 135(1250) 8\nBIC(Goutte et al., 2001) 123(59.74) 58(9.38) 56(5.08) 60(233.3) 69(32.35) 64(481.8) 167(11.33) 176(1660) 7\nClusterSize(Zhang et al., 2021b)86(11.69) 71(10.94) 72(22.03) 90(400.0) 82(19.61) 85(672.7) 105(30.00) 106(960.0) 6\nOurs (GPT3.5,Œª= 1) 64(16.88) 46(28.13) 43(27.12) 90(400.0) 43(57.84) 40(263.6) 151(0.67) 96(860.0) 5\nOurs (GPT3.5,Œª= 3) 64(16.88) 46(28.13) 52(11.86) 37(105.6) 92(9.80) 18(63.63) 142(5.33) 107(970.0) 2\nOurs (GPT4,Œª= 1) 56(27.27) 46(28.13) 41(30.51) 20(11.11) 53(48.04) 8(27.27) 146(2.67) 39(290.0) 3\nOurs (GT,Œª= 1) 100(29.87) 91(42.19) 42(28.81) 18(0) 41(59.80) 11(0) 141(6.00) 39(290.0) 4\nOurs (GT,Œª= 3) 99(28.57) 94(46.88) 62(5.08) 20(11.11) 37(63.73) 11(0) 142(5.33) 31(210.0) 1\nTable 5: Cluster granularity on small-scale datasets. Maximum & minimum number of clusters are set to 200 & 2.\nThe results are shown in format of ‚Äú[#clusters] (errors)‚Äù. ‚ÄúRank‚Äù column is computed with1-level ranking (Colombo\net al., 2022) with inverse errors. ‚ÄúGT‚Äù is ground truth. See results for large-scale datasets in Table 7.\ning (Zhou et al., 2022) which focuses on how to\nleverage deep neural network in clustering. Zhou\net al. (2022) has categorized deep clustering re-\nsearch into four types including multi-stage (Yal-\ning Tao, 2021; Huang et al., 2014), iterative (Yang\net al., 2016; Caron et al., 2018; Van Gansbeke\net al., 2020; Niu et al., 2022; Chang et al., 2017;\nNiu et al., 2020), generative (Dilokthanakul et al.,\n2016) and simultaneous (Xie et al., 2016; Zhang\net al., 2021a; Hadifar et al., 2019) depended on\nhow representation learning and clustering mod-\nules interact with each other. Most recently, a con-\ncurrent work (Wang et al., 2023) studies a similar\nproblem by assigning instances to different expla-\nnations proposed by LLMs. Another recent work\nIDAS (Raedt et al., 2023) directly encode the con-\ncatenation of sentence and abstractive summariza-\ntions from LLMs for clustering.\nClustering with Relations. Clustering with rela-\ntions has been explored in different situations. To\nstart with, spectral clustering (Donath and Hoff-\nman, 1972; Cheeger, 1970) makes use of similarity\nmatrix where each entry measures the similarity\nbetween a pair of data. More recently, several\nworks in deep clustering utilize relational supervi-\nsion (Yang et al., 2016; Niu et al., 2020; Van Gans-\nbeke et al., 2020; Chang et al., 2017) via pseudo-\nlabelling which could be noisy. Another line of\nworks that is closely related to ours is constrained\nclustering. It usually incorporates pairwise must-\nlink or cannot-link constraints (Basu et al., 2004;\nWagstaff et al., 2001; Basu et al., 2008; Zhang\net al., 2020; Manduchi et al., 2021). Nonetheless,\nthese constraints are often sampled from labels as\na prior which significantly limits its application in\nour scenario. In this work, we study how to utilize\ncontemporary API-based LLMs to infer relations.\nPre-trained Embedding Model. Generic pre-\ntrained text embedding models (Reimers and\nGurevych, 2019; Gao et al., 2021; Ni et al.,\n2022a,b) are widely applied in text similarity, clas-\nsification, clustering and information retrieval. Re-\ncently, two embedding models E5 (Wang et al.,\n2022) and Instructor (Su et al., 2022) have\nshown superior performance on a popular bench-\nmark (Muennighoff et al., 2022). Specifically E5\nis pre-trained on web-scraped data pairs with con-\ntrastive objective. Instructor is pre-trained on 330\ntasks with instructions. CLUSTER LLM aims at\nimproving these models with LLMs.\n6 Conclusion\nIn this paper, we study how to leverage API-based\nLLMs to guide small embedders for text cluster-\ning in order to benefit from high-level language\ncapability of LLMs and user‚Äôs instructions on clus-\ntering. We propose to prompt LLMs with two\nkinds of sentence relationship tasks: triplet task\nand pairwise task. Triplet task chooses the sentence\nthat is most similar with anchor combining with a\nperspective instruction from users. The predicted\ntriplets are used for fine-tuning small embedders.\nPairwise task judges whether a pair of sentences\nbelong to the same category hinted by few-shot\ndemonstrations, and then the predictions are used\nto determine cluster granularity with a consistency\nmeasure. Extensive experiments show that our\nproposed framework CLUSTER LLM can improve\nclustering quality and propose reasonable cluster\ngranularity at a negligible cost. However, CLUS -\nTER LLM still relies on the embedding model itself,\nwhich is inefficient and inapplicable on black-box\nembedding models. We encourage future works to\nexplore the potential of model-free training such as\nconstrained clustering.\n13911\nLimitations\nWe list several limitations of our work that we hope\nto be improved in the future:\nReliance on pre-trained embedder. To find the\nmost informative data, we have to rely on a pre-\ntrained embedder that can indicate the largest clus-\ntering assignment entropy. We hope that self-\nsupervise triplets and LLM-predicted triplets can\nbe combined to solve such an issue.\nComputational cost for fine-tuning. Our initial\nidea is to utilize constrained clustering which is a\nlight-weight algorithm that do not need to update\nsmall embedders. However, the inevitable unsta-\nble training will be heavily affected by the errors in\nLLM predictions. We make a comprise by introduc-\ning embedder into fine-tuning to temporarily solve\nthe issue, but we hope to reduce the computational\ncost in a future work.\nSub-optimal performance on domain discovery.\nWe notice that on domain discovery datasets such\nas Massive(D) and CLINC(D), the performance\nis usually sub-optimal compared with original In-\nstructor embedding. We provide discussions on\nthis issue in Appendix H.\nEthics Statement\nOur work employs LLMs which are accessed\nthrough OpenAI APIs. For some applications, up-\nloading privacy-sensitive data is risky and might\nrequire efforts to remove sensitive information.\nAcknowledgements\nOur work is sponsored in part by NSF CAREER\nAward 2239440, NSF Proto-OKN Award 2333790,\nNIH Bridge2AI Center Program under award\n1U54HG012510-01, Cisco-UCSD Sponsored Re-\nsearch Project, as well as generous gifts from\nGoogle, Adobe, and Teradata. Any opinions,\nfindings, and conclusions or recommendations ex-\npressed herein are those of the authors and should\nnot be interpreted as necessarily representing the\nviews, either expressed or implied, of the U.S. Gov-\nernment. The U.S. Government is authorized to\nreproduce and distribute reprints for government\npurposes not withstanding any copyright annota-\ntion hereon.\nReferences\nWenbin An, Feng Tian, Qinghua Zheng, Wei Ding,\nQianYing Wang, and Ping Chen. 2022. General-\nized category discovery with decoupled prototypical\nnetwork. arXiv preprint arXiv:2211.15115.\nYuntao Bai, Saurav Kadavath, Sandipan Kundu,\nAmanda Askell, Jackson Kernion, Andy Jones,\nAnna Chen, Anna Goldie, Azalia Mirhoseini,\nCameron McKinnon, et al. 2022. Constitutional\nai: Harmlessness from ai feedback. arXiv preprint\narXiv:2212.08073.\nSugato Basu, Arindam Banerjee, and Raymond J\nMooney. 2004. Active semi-supervision for pairwise\nconstrained clustering. In Proceedings of the 2004\nSIAM international conference on data mining, pages\n333‚Äì344. SIAM.\nSugato Basu, Ian Davidson, and Kiri Wagstaff. 2008.\nConstrained clustering: Advances in algorithms, the-\nory, and applications. CRC Press.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877‚Äì1901.\nKaidi Cao, Maria Brbic, and Jure Leskovec. 2022.\nOpen-world semi-supervised learning. In Interna-\ntional Conference on Learning Representations.\nMathilde Caron, Piotr Bojanowski, Armand Joulin, and\nMatthijs Douze. 2018. Deep clustering for unsuper-\nvised learning of visual features. In Proceedings of\nthe European conference on computer vision (ECCV),\npages 132‚Äì149.\nI√±igo Casanueva, Tadas Temcinas, Daniela Gerz,\nMatthew Henderson, and Ivan Vulic. 2020. Ef-\nficient intent detection with dual sentence en-\ncoders. In Proceedings of the 2nd Workshop\non NLP for ConvAI - ACL 2020 . Data avail-\nable at https://github.com/PolyAI-LDN/task-specific-\ndatasets.\nJianlong Chang, Lingfeng Wang, Gaofeng Meng, Shim-\ning Xiang, and Chunhong Pan. 2017. Deep adaptive\nimage clustering. In Proceedings of the IEEE in-\nternational conference on computer vision , pages\n5879‚Äì5887.\nJeff Cheeger. 1970. A lower bound for the smallest\neigenvalue of the laplacian, problems in analysis (pa-\npers dedicated to salomon bochner, 1969).\nQinyuan Cheng, Xiaogui Yang, Tianxiang Sun, Linyang\nLi, and Xipeng Qiu. 2023. Improving contrastive\nlearning of sentence embeddings from ai feedback.\nWei-Lin Chiang, Xuanqing Liu, Si Si, Yang Li, Samy\nBengio, and Cho-Jui Hsieh. 2019. Cluster-gcn: An\nefficient algorithm for training deep and large graph\nconvolutional networks. In Proceedings of the 25th\nACM SIGKDD international conference on knowl-\nedge discovery & data mining, pages 257‚Äì266.\n13912\nPierre Colombo, Nathan Noiry, Ekhine Irurozki, and\nStephan Clemencon. 2022. What are the best sys-\ntems? new perspectives on nlp benchmarking. arXiv\npreprint arXiv:2202.03799.\nDorottya Demszky, Dana Movshovitz-Attias, Jeongwoo\nKo, Alan Cowen, Gaurav Nemade, and Sujith Ravi.\n2020. GoEmotions: A dataset of fine-grained emo-\ntions. In Proceedings of the 58th Annual Meeting of\nthe Association for Computational Linguistics, pages\n4040‚Äì4054, Online. Association for Computational\nLinguistics.\nShumin Deng, Ningyu Zhang, Jiaojian Kang, Yichi\nZhang, Wei Zhang, and Huajun Chen. 2020. Meta-\nlearning with dynamic-memory-based prototypical\nnetwork for few-shot event detection. In Proceed-\nings of the 13th International Conference on Web\nSearch and Data Mining, WSDM ‚Äô20, page 151‚Äì159,\nNew York, NY , USA. Association for Computing\nMachinery.\nNat Dilokthanakul, Pedro AM Mediano, Marta Gar-\nnelo, Matthew CH Lee, Hugh Salimbeni, Kai Arulku-\nmaran, and Murray Shanahan. 2016. Deep unsuper-\nvised clustering with gaussian mixture variational\nautoencoders. arXiv preprint arXiv:1611.02648.\nNing Ding, Guangwei Xu, Yulin Chen, Xiaobin Wang,\nXu Han, Pengjun Xie, Haitao Zheng, and Zhiyuan\nLiu. 2021. Few-NERD: A few-shot named entity\nrecognition dataset. In Proceedings of the 59th An-\nnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 3198‚Äì3213, Online. Association\nfor Computational Linguistics.\nWilliam E Donath and Alan J Hoffman. 1972. Algo-\nrithms for partitioning of graphs and computer logic\nbased on eigenvectors of connection matrices. IBM\nTechnical Disclosure Bulletin, 15(3):938‚Äì944.\nJack FitzGerald, Christopher Hench, Charith Peris,\nScott Mackie, Kay Rottmann, Ana Sanchez, Aaron\nNash, Liam Urbach, Vishesh Kakarala, Richa Singh,\nSwetha Ranganath, Laurie Crist, Misha Britan,\nWouter Leeuwis, Gokhan Tur, and Prem Natara-\njan. 2022. Massive: A 1m-example multilin-\ngual natural language understanding dataset with 51\ntypologically-diverse languages.\nTianyu Gao, Xu Han, Hao Zhu, Zhiyuan Liu, Peng\nLi, Maosong Sun, and Jie Zhou. 2019. FewRel 2.0:\nTowards more challenging few-shot relation classi-\nfication. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n6251‚Äì6256, Hong Kong, China. Association for Com-\nputational Linguistics.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.\nSimCSE: Simple contrastive learning of sentence em-\nbeddings. In Proceedings of the 2021 Conference\non Empirical Methods in Natural Language Process-\ning, pages 6894‚Äì6910, Online and Punta Cana, Do-\nminican Republic. Association for Computational\nLinguistics.\nGregor Geigle, Nils Reimers, Andreas R√ºckl√©, and Iryna\nGurevych. 2021. TWEAC: transformer with extend-\nable QA agent classifiers. CoRR, abs/2104.07081.\nFabrizio Gilardi, Meysam Alizadeh, and Ma√´l Kubli.\n2023. Chatgpt outperforms crowd-workers for text-\nannotation tasks. arXiv preprint arXiv:2303.15056.\nCyril Goutte, Lars Kai Hansen, Matthew G Liptrot,\nand Egill Rostrup. 2001. Feature-space cluster-\ning for fmri meta-analysis. Human brain mapping,\n13(3):165‚Äì183.\nAmir Hadifar, Lucas Sterckx, Thomas Demeester, and\nChris Develder. 2019. A self-training approach\nfor short text clustering. In Proceedings of the\n4th Workshop on Representation Learning for NLP\n(RepL4NLP-2019), pages 194‚Äì199, Florence, Italy.\nAssociation for Computational Linguistics.\nXingwei He, Zhenghao Lin, Yeyun Gong, A Jin, Hang\nZhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan,\nWeizhu Chen, et al. 2023. Annollm: Making large\nlanguage models to be better crowdsourced annota-\ntors. arXiv preprint arXiv:2303.16854.\nWilliam Hogan, Jiacheng Li, and Jingbo Shang. 2023.\nOpen-world semi-supervised generalized relation dis-\ncovery aligned in a real-world setting.\nPeihao Huang, Yan Huang, Wei Wang, and Liang Wang.\n2014. Deep embedding network for clustering. In\n2014 22nd International conference on pattern recog-\nnition, pages 1532‚Äì1537. IEEE.\nHarold W Kuhn. 1955. The hungarian method for the\nassignment problem. Naval research logistics quar-\nterly, 2(1-2):83‚Äì97.\nStefan Larson, Anish Mahendran, Joseph J. Peper,\nChristopher Clarke, Andrew Lee, Parker Hill,\nJonathan K. Kummerfeld, Kevin Leach, Michael A.\nLaurenzano, Lingjia Tang, and Jason Mars. 2019. An\nevaluation dataset for intent classification and out-of-\nscope prediction. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing (EMNLP-IJCNLP).\nHaoran Li, Abhinav Arora, Shuohui Chen, Anchit\nGupta, Sonal Gupta, and Yashar Mehdad. 2021.\nMTOP: A comprehensive multilingual task-oriented\nsemantic parsing benchmark. In Proceedings of the\n16th Conference of the European Chapter of the Asso-\nciation for Computational Linguistics: Main Volume,\npages 2950‚Äì2962, Online. Association for Computa-\ntional Linguistics.\nSha Li, Heng Ji, and Jiawei Han. 2022. Open rela-\ntion and event type discovery with type abstraction.\n13913\nIn Proceedings of the 2022 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n6864‚Äì6877, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nTing-En Lin, Hua Xu, and Hanlei Zhang. 2020. Dis-\ncovering new intents via constrained deep adaptive\nclustering with cluster refinement. In Thirty-Fourth\nAAAI Conference on Artificial Intelligence.\nJ MacQueen. 1967. Classification and analysis of mul-\ntivariate observations. In 5th Berkeley Symp. Math.\nStatist. Probability, pages 281‚Äì297. University of\nCalifornia Los Angeles LA USA.\nLaura Manduchi, Kieran Chin-Cheong, Holger Michel,\nSven Wellmann, and Julia E V ogt. 2021. Deep condi-\ntional gaussian mixture model for constrained cluster-\ning. In Advances in Neural Information Processing\nSystems.\nJos√© Manuel Guaita Mart√≠nez, Patricia Carracedo, Do-\nlores Gorgues Comas, and Carlos H Siemens. 2022.\nAn analysis of the blockchain and covid-19 research\nlandscape using a bibliometric study. Sustainable\nTechnology and Entrepreneurship, 1(1):100006.\nYutao Mou, Keqing He, Yanan Wu, Zhiyuan Zeng,\nHong Xu, Huixing Jiang, Wei Wu, and Weiran Xu.\n2022. Disentangled knowledge transfer for OOD in-\ntent discovery with unified contrastive learning. In\nProceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 2:\nShort Papers), pages 46‚Äì53, Dublin, Ireland. Associ-\nation for Computational Linguistics.\nNiklas Muennighoff, Nouamane Tazi, Lo√Øc Magne, and\nNils Reimers. 2022. Mteb: Massive text embedding\nbenchmark. arXiv preprint arXiv:2210.07316.\nFionn Murtagh and Pedro Contreras. 2011. Meth-\nods of hierarchical clustering. arXiv preprint\narXiv:1105.0121.\nJianmo Ni, Gustavo Hernandez Abrego, Noah Con-\nstant, Ji Ma, Keith Hall, Daniel Cer, and Yinfei Yang.\n2022a. Sentence-t5: Scalable sentence encoders\nfrom pre-trained text-to-text models. In Findings of\nthe Association for Computational Linguistics: ACL\n2022, pages 1864‚Äì1874, Dublin, Ireland. Association\nfor Computational Linguistics.\nJianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Her-\nnandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith\nHall, Ming-Wei Chang, and Yinfei Yang. 2022b.\nLarge dual encoders are generalizable retrievers. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing , pages\n9844‚Äì9855, Abu Dhabi, United Arab Emirates. As-\nsociation for Computational Linguistics.\nChuang Niu, Hongming Shan, and Ge Wang. 2022.\nSpice: Semantic pseudo-labeling for image clus-\ntering. IEEE Transactions on Image Processing ,\n31:7264‚Äì7278.\nChuang Niu, Jun Zhang, Ge Wang, and Jimin Liang.\n2020. Gatcluster: Self-supervised gaussian-attention\nnetwork for image clustering. In European Confer-\nence on Computer Vision (ECCV).\nRobert M Nosofsky. 2011. The generalized context\nmodel: An exemplar model of classification. Formal\napproaches in categorization, pages 18‚Äì39.\nOpenAI. 2023. Gpt-4 technical report. ArXiv,\nabs/2303.08774.\nLong Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida,\nCarroll Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. Advances in Neural\nInformation Processing Systems, 35:27730‚Äì27744.\nJune Young Park, Evan Mistur, Donghwan Kim, Yun-\njeong Mo, and Richard Hoefer. 2022. Toward human-\ncentric urban infrastructure: Text mining for social\nmedia data to identify the public perception of covid-\n19 policy in transportation hubs. Sustainable Cities\nand Society, 76:103524.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research ,\n12:2825‚Äì2830.\nDan Pelleg, Andrew W Moore, et al. 2000. X-means:\nExtending k-means with efficient estimation of the\nnumber of clusters. In Icml, volume 1, pages 727‚Äì\n734.\nMaarten De Raedt, Fr√©deric Godin, Thomas Demeester,\nand Chris Develder. 2023. Idas: Intent discovery\nwith abstractive summarization.\nNils Reimers and Iryna Gurevych. 2019. Sentence-\nBERT: Sentence embeddings using Siamese BERT-\nnetworks. In Proceedings of the 2019 Conference on\nEmpirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natu-\nral Language Processing (EMNLP-IJCNLP), pages\n3982‚Äì3992, Hong Kong, China. Association for Com-\nputational Linguistics.\nMamshad Nayeem Rizve, Navid Kardan, Salman Khan,\nFahad Shahbaz Khan, and Mubarak Shah. 2022a.\nOpenldn: Learning to discover novel classes for open-\nworld semi-supervised learning. In European Confer-\nence on Computer Vision, pages 382‚Äì401. Springer.\nMamshad Nayeem Rizve, Navid Kardan, and Mubarak\nShah. 2022b. Towards realistic semi-supervised\nlearning. In European Conference on Computer Vi-\nsion, pages 437‚Äì455. Springer.\nPeter J Rousseeuw. 1987. Silhouettes: a graphical aid\nto the interpretation and validation of cluster analysis.\nJournal of computational and applied mathematics,\n20:53‚Äì65.\n13914\nHongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang,\nYushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A.\nSmith, Luke Zettlemoyer, and Tao Yu. 2022. One\nembedder, any task: Instruction-finetuned text em-\nbeddings.\nYiyou Sun and Yixuan Li. 2022. Opencon: Open-world\ncontrastive learning. In Transactions on Machine\nLearning Research.\nRobert Thorndike. 1953. Who belongs in the family?\nPsychometrika, 18(4):267‚Äì276.\nLaurens Van der Maaten and Geoffrey Hinton. 2008.\nVisualizing data using t-sne. Journal of machine\nlearning research, 9(11).\nWouter Van Gansbeke, Simon Vandenhende, Stama-\ntios Georgoulis, Marc Proesmans, and Luc Van Gool.\n2020. Scan: Learning to classify images without\nlabels. In Computer Vision‚ÄìECCV 2020: 16th Euro-\npean Conference, Glasgow, UK, August 23‚Äì28, 2020,\nProceedings, Part X, pages 268‚Äì285. Springer.\nSagar Vaze, Kai Han, Andrea Vedaldi, and Andrew\nZisserman. 2022. Generalized category discovery. In\nIEEE Conference on Computer Vision and Pattern\nRecognition.\nKiri Wagstaff, Claire Cardie, Seth Rogers, Stefan\nSchr√∂dl, et al. 2001. Constrained k-means cluster-\ning with background knowledge. In Icml, volume 1,\npages 577‚Äì584.\nLiang Wang, Nan Yang, Xiaolong Huang, Binxing\nJiao, Linjun Yang, Daxin Jiang, Rangan Majumder,\nand Furu Wei. 2022. Text embeddings by weakly-\nsupervised contrastive pre-training. arXiv preprint\narXiv:2212.03533.\nZihan Wang, Jingbo Shang, and Ruiqi Zhong. 2023.\nGoal-driven explainable clustering via language de-\nscriptions.\nJoe H Ward Jr. 1963. Hierarchical grouping to opti-\nmize an objective function. Journal of the American\nstatistical association, 58(301):236‚Äì244.\nJunyuan Xie, Ross Girshick, and Ali Farhadi. 2016.\nUnsupervised deep embedding for clustering analy-\nsis. In International conference on machine learning,\npages 478‚Äì487. PMLR.\nHui Xu, Yi Liu, Chi-Min Shu, Mingqi Bai, Mailidan\nMotalifu, Zhongxu He, Shuncheng Wu, Penggang\nZhou, and Bing Li. 2022. Cause analysis of hot work\naccidents based on text mining and deep learning.\nJournal of Loss Prevention in the Process Industries,\n76:104747.\nJiaming Xu, Peng Wang, Guanhua Tian, Bo Xu, Jun\nZhao, Fangyuan Wang, and Hongwei Hao. 2015.\nShort text clustering via convolutional neural net-\nworks. In Proceedings of the 1st Workshop on Vec-\ntor Space Modeling for Natural Language Process-\ning, pages 62‚Äì69, Denver, Colorado. Association for\nComputational Linguistics.\nKouta Nakata Yaling Tao, Kentaro Takagi. 2021.\nClustering-friendly representation learning via in-\nstance discrimination and feature decorrelation. Pro-\nceedings of ICLR 2021.\nJianwei Yang, Devi Parikh, and Dhruv Batra. 2016.\nJoint unsupervised learning of deep representations\nand image clusters. In Proceedings of the IEEE con-\nference on computer vision and pattern recognition,\npages 5147‚Äì5156.\nMuli Yang, Yuehua Zhu, Jiaping Yu, Aming Wu, and\nCheng Deng. 2022. Divide and conquer: Compo-\nsitional experts for generalized novel class discov-\nery. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages\n14268‚Äì14277.\nDejiao Zhang, Feng Nan, Xiaokai Wei, Shang-Wen Li,\nHenghui Zhu, Kathleen McKeown, Ramesh Nalla-\npati, Andrew O. Arnold, and Bing Xiang. 2021a.\nSupporting clustering with contrastive learning. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 5419‚Äì5430, Online. Association for Computa-\ntional Linguistics.\nHanlei Zhang, Hua Xu, Ting-En Lin, and Rui Lyu.\n2021b. Discovering new intents with deep aligned\nclustering. Proceedings of the AAAI Conference on\nArtificial Intelligence, 35(16):14365‚Äì14373.\nHanlei Zhang, Hua Xu, Xin Wang, Fei Long, and Kai\nGao. 2023. Usnid: A framework for unsupervised\nand semi-supervised new intent discovery. arXiv\npreprint arXiv:2304.07699.\nHongjing Zhang, Sugato Basu, and Ian Davidson.\n2020. A framework for deep constrained clustering-\nalgorithms and advances. In Machine Learning and\nKnowledge Discovery in Databases: European Con-\nference, ECML PKDD 2019, W√ºrzburg, Germany,\nSeptember 16‚Äì20, 2019, Proceedings, Part I, pages\n57‚Äì72. Springer.\nYuwei Zhang, Haode Zhang, Li-Ming Zhan, Xiao-Ming\nWu, and Albert Lam. 2022. New intent discovery\nwith pre-training and contrastive learning. In Pro-\nceedings of the 60th Annual Meeting of the Associa-\ntion for Computational Linguistics (Volume 1: Long\nPapers), pages 256‚Äì269, Dublin, Ireland. Association\nfor Computational Linguistics.\nSheng Zhou, Hongjia Xu, Zhuonan Zheng, Jiawei Chen,\nJiajun Bu, Jia Wu, Xin Wang, Wenwu Zhu, Martin\nEster, et al. 2022. A comprehensive survey on deep\nclustering: Taxonomy, challenges, and future direc-\ntions. arXiv preprint arXiv:2206.07579.\nYiming Zhu, Peixian Zhang, Ehsan-Ul Haq, Pan Hui,\nand Gareth Tyson. 2023. Can chatgpt reproduce\nhuman-generated labels? a study of social computing\ntasks. arXiv preprint arXiv:2304.10145.\n13915\nA Details of Scaling up Hierarchical\nClustering\nA major drawback of hierarchical clustering is\nits O(N3) time complexity which makes the al-\ngorithm hard to be deployed on larger datasets.\nHowever, since we are only interested in a spe-\ncific range of granularity in our scenario, the hier-\narchical clustering can start from an intermediate\nstep. We address this issue by first running mini-\nbatch K-means with kmax and then run hierarchi-\ncal clustering with current assignments as inputs.\nSpecifically, we use agglomerative clustering with\nward‚Äôs method (Ward Jr, 1963). We first calculate\ndistances between each pair of clusters according\nto Murtagh and Contreras, 2011 and then provide\nthem as inputs to nearest neighbor chain algorithm.\nFinally the returned hierarchy is combined with the\nK-means assignments to infer clusters.\nB More Details about Determining\nCluster Granularity\nPrevious methods often employ clustering errors as\na metric and they ignore user‚Äôs need on the granu-\nlarity. Silhouette coefficient (Rousseeuw, 1987) in-\ndicates the clustering quality without ground truths,\nwhich exploits the inter-cluster distance with near-\nest clusters and the intra-cluster distance. We find\nthe granularity by choosing the one with the best\nsilhouette coefficient. Elbow method (Thorndike,\n1953) is a heuristic method that plots the clustering\nerror with respect to different levels of granular-\nity in the hierarchy. And then the best granular-\nity is determined with the largest elbow length.\nX-means (Pelleg et al., 2000) is a variation of\nK-means that starts with the lowest number of\nclusters, and then repeatedly attempt to split the\nclusters by running 2-means on them and evaluate\nwith Bayesian Information Criterion (BIC) (Goutte\net al., 2001). BIC (Goutte et al., 2001) calcu-\nlates BIC for each of the granularity. Cluster-\nSize (Zhang et al., 2021b) uses a confidence thresh-\nold to filter small clusters starting from the maxi-\nmum number of cluster. For all methods, we use\nthe same fine-tuned embeddings (CLUSTER LLM-I\nin Table 2). The same cluster hierarchy is used (ex-\ncept for X-means that relies on K-means), which\nis either acquired from hierarchical clustering for\nsmall-scale or our proposed two-step method in\nSection 3.2 for large-scale. For out methods, the\nweight in F-beta score is set to 0.92 through em-\npirical selection on Bank77. Because of the high\n0 25 50 75 100 125 150 175 200\n#clusters\n0.0\n0.2\n0.4\n0.6\n0.8consistency score\nCLINC(I)\nCLINC(D)\nFigure 4: Consistency score v.s. various number of\nclusters with GPT-4 and Œª= 1.\nlatency, results for Silhouette and X-means are not\nshown on large-scale datasets. After sampling 16\ndata pairs, we tend to choose positives with finer\ngranularity or negatives with coarser granularity.\nWe also consider the sentence length to minimize\nthe cost. We use label names as justifications and\nwe always put 2 positive before 2 negative (See\nTable 12 bottom).\nC Analysis for Determining Granularity\nPrompt Design. We show the analysis results of\nprompt design for determining granularity in Ta-\nble 8. We experiment with two settings: (1) remove\njustification for all demonstrations and only keep\nthe ‚ÄúYes‚Äù or ‚ÄúNo‚Äù answer. (2) remove all demon-\nstrations and any granularity-related words (such\nas domain)3. We observe that demonstrations are\nnecessary and adding justifications have a positive\nimpact.\nVisualization of Consistency Score. We visualize\nconsistency score with respect to the number of\nclusters. The consistency scores exhibit continuous\nvariations and peak at the best number of clusters.\nD Details of Embedders and Fine-tuning\nFor all the experiments (including those\nwith or without fine-tuning), we use\nlarge version of both Instructor and\nE5 (i.e. hkunlp/instructor-large &\nintfloat/e5-large). For Instructor, we\nuse the same or similar prompt as original paper.\nSee Table 10.\nFor fine-tuning, we adopt the same hyper-\nparameters as in (Su et al., 2022), but modify the\n3After removing, the prompt will be in the format ‚ÄúDe-\ntermine whether the two sentences below belong to the same\ncategory.‚Äù\n13916\nlearning rate to 2e‚àí6, the maximum gradient steps\nto 3,840 for Instructor (‚àº15 epochs) and 1,280\nfor E5, and batch size to 4. We choose this gra-\ndient in the begining of our experiments by ob-\nserving no performance increase after that on sev-\neral datasets. Training is conducted with a single\nNVIDIA Quadro RTX 8000 GPU.\nFor SCCL-I(E), we change the maximum token\nlength to 128 due to the limited compute resource4.\nWe use the same learning rate 2e‚àí6 as before\nfor Instructor and 2e‚àí7 for E5 since we found\nthat the performance is unstable with large learn-\ning rate. Batch size is set to 16 and we evaluate\nrepresentations with K-means after 200 iterations.\nAlso notice that we do not interrupt prompts in\nInstructor during data augmentation.\nE Description of Datasets\nBank77 (Casanueva et al., 2020) is a popular\ndataset in intent discovery that focuses on creating\nfine-grained intent categories for a single-domain,\n‚Äúbanking‚Äù. CLINC(I) (Larson et al., 2019) is orig-\ninally created for detecting utterances that falls\noutside of supported intents. The dataset also con-\ntains multiple domains, such as ‚Äútravel‚Äù, ‚Äúutility‚Äù\nand ‚Äúwork‚Äù. In this experiment, we discard all\nthe out-of-scope utterances and only focus on in-\ndomain ones. Moreover, we create a domain dis-\ncovery dataset CLINC(D) that uses domains as\nlabels. Massive(I)/(D) (FitzGerald et al., 2022)\nand MTOP(I)/(D) (Li et al., 2021) are both from\nMTEB (Muennighoff et al., 2022). Here ‚ÄúI‚Äù de-\nnotes intent and ‚ÄúD‚Äù for domain (or scenario).\nThese datasets are originally used for classifica-\ntion but are adapted here for clustering. We also\nremove those intents with only a few instances and\nkeep English-only data. For all datasets, we use the\ntrain & test sets as large- & small- scale datasets\nrespectively. For FewRel (Gao et al., 2019) and\nFewEvent (Deng et al., 2020), we first randomly\nsplit datasets into train & test sets, and then sample\nfrom train set as large-scale and test set as small-\nscale. For FewNerd (Ding et al., 2021), we use\nthe original train & test splits. For StackEx, Red-\ndit (Geigle et al., 2021) andArxivS2S, we combine\nall the splits into a single dataset and remove topics\nthat only have few instances. Finally, the datasets\nare randomly splitted into large- & small- scale\nversions. To show the dataset balancy, we show the\n4We also tried maximum token length 512 with smaller\nbatch size, but the performance is not better.\n1 2 3 4\nIteration\n0.75\n0.80\n0.85\n0.90\n0.95\n1.00Relative Clustering Accuracy\nBank77\nBank77-self\nFewRel\nFewRel-self\nStackEx\nStackEx-self\nGoEmo\nGoEmo-self\nFigure 5: Relative clustering accuracy (divided by max-\nimum for better aligning across datasets) of CLUSTER -\nLLM-GPT3.5 over 4 iterations.\nentropy of class distribution in Table 9.\nF Results of More Iterations\nWe show the results over 4 iterations of CLUS -\nTER LLM in Figure 5. During iteration, we sam-\nple triplets from previously fine-tuned embedding\nspace and continue to fine-tune the model with pre-\nvious checkpoint as initialization. We also show\nthe self-supervise results using the same checkpoint\nfine-tuned with GPT-3.5 predictions as initializa-\ntion at each iteration. We observe that using GPT-\n3.5 predictions is almost always beneficial. The\nperformance generally increases and saturate at the\nfourth iteration with the exception of GoEmo.\nG More Related Works\nGeneralized Category Discovery (GCD).\nGCD (Vaze et al., 2022; Lin et al., 2020; Zhang\net al., 2021b, 2022; Mou et al., 2022; An\net al., 2022) assume partial known classes with\nannotations which can also be used to infer\nuser‚Äôs requirement on clustering. As an infant\nresearch area, most previous works employ\npseudo-labelling, via optimal transport (Rizve\net al., 2022b; Yang et al., 2022), similarity\nlearning (Rizve et al., 2022a; Cao et al., 2022)\nor prototype-based learning (Sun and Li, 2022).\nFurthermore, new intent discovery (Zhang et al.,\n2022, 2021b, 2023; An et al., 2022; Lin et al.,\n2020) is proposed to study a similar research\nproblem in the domain of intent detection. Most\nrecently, Hogan et al., 2023 adapts the setting into\nrelation type discovery. However, GCD relies on\nsufficient annotated and unlabeled data for training.\nIn contrast, CLUSTER LLM seeks for minimal\nsupervision and studies a setting with controlled\n13917\nMethod\nIntent Discovery Emotion\nBank77 CLINC(I) MTOP(I) Massive(I) GoEmo\nACC NMI ACC NMI ACC NMI ACC NMI ACC NMI\nInstructor 60.30(2.39) 78.37(0.78) 79.52(1.96) 92.65(0.42) 35.53(1.05) 70.78(0.74) 54.72(2.00) 72.29(0.61) 24.02(1.11) 20.15(0.19)\n+self-supervise 61.48(2.84) 79.28(1.02) 81.87(0.97) 93.55(0.41) 35.27(1.53) 71.88(0.66) 58.30(1.42) 73.73(0.56) 24.34(1.25) 21.17(0.56)\n+CLUSTERLLM-GPT3.5 65.47(2.28) 81.60(0.92) 82.29(1.09) 93.91(0.11) 36.80(0.83) 73.16(0.37) 57.70(2.92) 74.24(0.68) 25.23(1.21) 22.19(0.42)\n+CLUSTERLLM-GT&GPT3.567.30(1.35) 82.46(0.17) 80.90(1.58) 93.80(0.25) 37.75(1.64) 74.51(0.51) 58.92(1.80) 75.07(0.43) 27.96(2.59) 25.90(0.69)\nMethod\nType Discovery Topic Mining\nFewRel FewNerd FewEvent StackEx Reddit\nACC NMI ACC NMI ACC NMI ACC NMI ACC NMI\nInstructor 41.38(0.75) 53.97(0.23) 29.62(1.02) 42.83(0.31) 41.42(2.09) 61.13(1.11) 46.76(0.80) 61.20(0.37) 55.04(2.69) 59.55(0.98)\n+self-supervise 41.09(0.99) 54.02(0.29) 30.57(0.18) 43.64(0.26) 45.54(1.70) 64.70(0.50) 46.24(0.46) 60.78(0.26) 56.45(1.59) 60.11(0.39)\n+CLUSTERLLM-GPT3.5 47.22(0.89) 59.87(0.21) 33.86(1.19) 46.91(0.52) 47.55(1.51) 70.21(0.59) 47.42(1.35) 61.34(0.30) 55.47(2.44) 58.73(1.09)\n+CLUSTERLLM-GT&GPT3.549.20(0.47) 61.20(0.30) 34.85(1.42) 48.59(0.33) 46.90(1.77) 71.51(0.83) 48.12(0.93) 62.04(0.36) 57.95(1.96) 61.39(0.92)\nMethod\nTopic Mining Domain Discovery Avg\nArxivS2S MTOP(D) Massive(D) CLINC(D)\nACC NMI ACC NMI ACC NMI ACC NMI ACC NMI\nInstructor 24.55(0.42) 39.86(0.06) 85.01(2.18) 83.96(0.93) 56.11(5.07) 61.86(3.27) 51.74(2.97) 55.28(2.56) 48.98 60.99\n+self-supervise 24.49(0.75) 40.70(0.16) 89.54(4.56) 86.69(2.09) 58.14(3.88) 64.49(2.28) 50.93(4.11) 53.01(4.48) 50.30 61.98\n+CLUSTERLLM-GPT3.5 25.60(0.51) 41.50(0.14) 84.08(3.34) 84.57(1.97) 58.14(3.97) 65.50(1.35) 50.12(4.13) 53.46(2.13) 51.21 63.37\n+CLUSTERLLM-GT&GPT3.525.29(0.41) 41.81(0.13) 84.99(4.24) 86.59(1.53) 58.55(1.40) 65.66(1.00) 57.08(1.75) 58.35(1.78) 52.55 64.92\nTable 6: Ablation study on clustering quality for large-scale datasets.\nMethod Bank77 FewRel Massive(I) Massive(D) MTOP(I) MTOP(D) CLINC(I) CLINC(D)Rank\nGT #clusters 77 64 59 18 102 11 150 10 -\nElbow(Thorndike, 1953) 50(35.06) 38(40.63) 47(20.34) 45(150.0) 35(65.69) 35(218.2) 64(57.33) 71(610.0) 7\nBIC(Goutte et al., 2001) 183(137.7) 183(185.9) 148(150.8) 141(683.3) 169(65.69) 170(1445) 179(19.33) 178(1680) 8\nClusterSize(Zhang et al., 2021b)90(16.88) 95(23.38) 79(33.90) 81(350.0) 84(17.65) 83(654.5) 105(30.00) 109(990.0) 5\nOurs (GPT3.5,Œª= 1) 79(2.60) 32(50.00) 122(106.8) 55(205.6) 56(50.00) 13(18.18) 143(4.67) 118(1080) 6\nOurs (GPT3.5,Œª= 3) 79(2.60) 52(18.75) 108(83.05) 54(200.0) 61(40.20) 20(81.81) 145(3.33) 118(1080) 4\nOurs (GPT4,Œª= 1) 64(16.88) 40(37.50) 58(1.69) 16(11.11) 24(76.47) 8(27.27) 143(4.67) 32(220.0) 3\nOurs (GT,Œª= 1) 59(23.38) 79(23.44) 61(3.39) 17(5.56) 27(73.53) 11(0) 148(1.33) 32(220.0) 2\nOurs (GT,Œª= 3) 77(0) 78(21.88) 54(8.47) 37(105.6) 19(81.37) 11(0) 148(1.33) 32(220.0) 1\nTable 7: Inferred granularity on large-scale datasets. The setting is the same as in Table 5.\nMethod small-scale large-scale\nMassive(I) Massive(D) CLINC(I) CLINC(D) Massive(I) Massive(D) CLINC(I) CLINC(D)\nGT 59 18 150 10 59 18 150 10\nOurs 41 20 146 39 58 16 143 32\nw/o Justification 41 50 137 41 80 36 129 32\nw/o Demonstration41 64 141 108 77 74 120 105\nTable 8: Prompt designs in determining granularity. We use the Instructor embedding with prompts, and we report\nresults of GPT-4 with Œª= 1.\n(a) Massive(D) - Instructor\n (b) Massive(D) - CLUSTER LLM-I-iter\n (c) CLINC(D) - Instructor\n (d) CLINC(D) - CLUSTER LLM-I-iter\nFigure 6: Scatter plots for t-SNE of embeddings. We select 10 classes from each datasets, denoted by colors.\n13918\nTask Name entropy(small) entropy(large)\nIntent\nBank77 1.00 1.00\nCLINC(I) 1.00 1.00\nMTOP(I) 0.74 0.75\nMassive(I) 0.91 0.92\nType\nFewRel 1.00 1.00\nFewNerd 0.82 0.82\nFewEvent 0.85 0.85\nTopic\nStackEx 0.98 0.98\nArxivS2S 1.00 1.00\nReddit 1.00 1.00\nEmotion GoEmo 0.91 0.91\nDomain\nCLINC(D) 1.00 1.00\nMTOP(D) 0.98 0.98\nMassive(D) 0.94 0.94\nTable 9: Entropy of class distribution.\nDataset Prompt\nBank77 Represent the bank purpose for retrieval:\nCLINC(I)Represent the sentence for retrieving the purpose:\nFewRel Represent the relation between two entities for retrieval:\nFewNerdRepresent the entity type for retrieval:\nFewEventRepresent the event type for retrieval:\nStackExRepresent the question for retrieval:\nArxivS2SRepresent the science statement for retrieval:\nGoEmo Represent an emotion sentence for retrieval:\nMassive(I)Represent the sentence for retrieving the purpose:\nMTOP(I)Represent the sentence for retrieval:\nReddit represent a reddit community title:\nMassive(D)Represent the scene for retrieval:\nMTOP(D)Represent a sentence:\nCLINC(D)Represent a sentence:\nTable 10: Prompts for Instructor.\ncomputation- & data- cost.\nLLMs as Annotators. Recent instruction-tuned\nLLMs, such as ChatGPT, have been shown to\nhave the ability to reproduce or improve human-\ngenerated labels (Gilardi et al., 2023; He et al.,\n2023; Zhu et al., 2023). Furthermore, several works\ndedicate to fine-tune models with feedbacks from\nLLMs (Cheng et al., 2023; Bai et al., 2022). This\npaper instead focuses on clustering tasks.\nH Sub-optimal Performance on Domain\nDiscovery\nWe noticed that the performance of domain dis-\ncovery (MTOP(D), Massive(D) and CLINC(D))\nis barely improved or even decreased with CLUS -\nTER LLM from original embedders (see Table 2).\nFurthermore, the ablation studies reveal that even\nwith CLUSTER LLM-GT&GPT3.5, clustering per-\nformance is not as good as self-supervise or CLUS -\nTER LLM -random (see CLINC(D) in Table 4 and\nMTOP(D) in Table 6). We also observe that,CLUS -\nTER LLM -I-iter will further decrease the perfor-\nmance (see Massive(D) in Table 2). While we\ndo not have rigorous explanations, one hypothe-\nsize is that the embedding space after fine-tuning\ntends to be more compact than before and forming\nsmall cliques, making it better for clustering fine-\ngrained clusters but not for coarse-grained clus-\nters. We showcase scatterplots on two datasets\nwith both Instructor and CLUSTER LLM -I-iter. It\ncan be observed that the clusters in embedding\nspace are tighter and more separated especially on\nCLINC(D).\nI Dataset Leakage\nSince LLMs like ChatGPT are trained on web-\nscraped texts from internet, it is likely they already\nhave access to our evaluation datasets during train-\ning. For example, the topic mining datasets use\nStackExchange, Reddit and Arxiv tags as labels\nwhich is freely available online. However, as ob-\nserved in Table 3, the performance of triplet pre-\ndiction on these datasets are often far from perfect.\nFurthermore, the other datasets like Bank77 are\nsynthesized datasets which is not accessible dur-\ning training. FewRel is collected from Wikipedia\ncorpus but their labels are not easily accessible.\nSimilarly, GoEmo is collected from Reddit but the\nemotion labels are not accessible during training.\nThus, dataset leakage is not a primary concern of\nthis paper.\n13919\nDataset Prompt\nBank77 Select the banking customer utterance that better corresponds with the Query in terms of intent.\nCLINC(I) Select the customer utterance that better corresponds with the Query in terms of intent.\nFewRel Select the example that better corresponds with the Query in terms of relation type.\nFewNerd Select the example that better corresponds with the Query in terms of entity type.\nFewEvent Select the example that better corresponds with the Query in terms of event type.\nStackEx Select the StackExchange question that better corresponds with the Query in terms of topic.\nArxivS2S Select the Arxiv paper title that better corresponds with the Query in terms of domain.\nGoEmo Select the sentence that better corresponds with the Query in terms of emotion expressed.\nMassive(I) Select the user utterance that better corresponds with the Query in terms of intent.\nMTOP(I) Select the user utterance that better corresponds with the Query in terms of intent\nReddit Select the Reddit question that better corresponds with the Query in terms of topic.\nMassive(D)Select the user utterance that better corresponds with the Query in terms of scenario.\nMTOP(D) Select the user utterance that better corresponds with the Query in terms of domain.\nCLINC(D) Select the customer utterance that better corresponds with the Query in terms of domain.\nTable 11: Prefix of prompts for triplet task. Notice while we use different prompts for domain and intent (such as\nCLINC(I) and CLINC(D)) in our experiments, they might be used interchangeably.\nDataset Prompt\nTriplet Task\nSelect the banking customer utterance that better corresponds with the Query in terms of intent.\nQuery: Should i reinstall the payment app?\nChoice 1: I‚Äôve received my card so now I need to know how to sync it to the app.\nChoice 2: Can I still use the app if I switched phones?\nPlease respond with ‚ÄôChoice 1‚Äô or ‚ÄôChoice 2‚Äô without explanation.\nPairwise Task\n[Example1]\nSentence 1: I would like to see the source of my money.\nSentence 2: My source of funds need verified.\nYes. Because both intents are verify source of funds.\n[Example2]\nSentence 1: Is there a fee for topping up\nSentence 2: What are the top up charges for US cards?\nYes. Because both intents are top up by card charge.\n[Example3]\nSentence 1: Can I reactivate my lost card that I found this morning in my jacket pocket?\nSentence 2: how to activate card?\nNo. Because Sentence 1 has intent card linking and Sentence 2 has intent activate my card.\n[Example4]\nSentence 1: What will I be charged for a physical card?\nSentence 2: My card is about to expire and I need to know how much it costs and how long ...\nNo. Because Sentence 1 has intent order physical card and Sentence 2 has intent card ...\nDetermine whether the intents of two banking customer utterances\nbelow belong to the same intent category using above examples.\nSentence 1: $1 extra has been charged on my statement, why is that?\nSentence 2: Will it automatically top-up if there isn‚Äôt much money left?\nPlease respond with ‚ÄôYes‚Äô or ‚ÄôNo‚Äô without explanation.\nTable 12: One example from Bank77 on both triplet task and pairwise task.\n13920",
  "topic": "Cluster analysis",
  "concepts": [
    {
      "name": "Cluster analysis",
      "score": 0.8758445978164673
    },
    {
      "name": "Computer science",
      "score": 0.7961136102676392
    },
    {
      "name": "Granularity",
      "score": 0.7525855302810669
    },
    {
      "name": "Pairwise comparison",
      "score": 0.6826867461204529
    },
    {
      "name": "Perspective (graphical)",
      "score": 0.5301706790924072
    },
    {
      "name": "Cluster (spacecraft)",
      "score": 0.5036413073539734
    },
    {
      "name": "Document clustering",
      "score": 0.4977908134460449
    },
    {
      "name": "Information retrieval",
      "score": 0.4291852116584778
    },
    {
      "name": "Data mining",
      "score": 0.4241585433483124
    },
    {
      "name": "Theoretical computer science",
      "score": 0.33050623536109924
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3294146955013275
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I36258959",
      "name": "University of California, San Diego",
      "country": "US"
    }
  ],
  "cited_by": 33
}