{
    "title": "Multilingual Large Language Models Are Not (Yet) Code-Switchers",
    "url": "https://openalex.org/W4389519244",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2347993427",
            "name": "Zhang Ruo-chen",
            "affiliations": [
                "John Brown University"
            ]
        },
        {
            "id": "https://openalex.org/A4202039510",
            "name": "Cahyawijaya, Samuel",
            "affiliations": [
                "University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A4225988164",
            "name": "Cruz, Jan Christian Blaise",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4223199573",
            "name": "Winata, Genta Indra",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221735194",
            "name": "Aji, Alham Fikri",
            "affiliations": [
                "Mohamed bin Zayed University of Artificial Intelligence"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3093517588",
        "https://openalex.org/W4384918448",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W4293569541",
        "https://openalex.org/W3035268246",
        "https://openalex.org/W3199377785",
        "https://openalex.org/W3032020872",
        "https://openalex.org/W3153266325",
        "https://openalex.org/W4323651296",
        "https://openalex.org/W3212538717",
        "https://openalex.org/W3093871477",
        "https://openalex.org/W4285150836",
        "https://openalex.org/W2919290281",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W4378105483",
        "https://openalex.org/W4385571622",
        "https://openalex.org/W3169483174",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W4404783750",
        "https://openalex.org/W4389518325",
        "https://openalex.org/W4251217171",
        "https://openalex.org/W4287694131",
        "https://openalex.org/W4226399820",
        "https://openalex.org/W3121525843",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4362720788",
        "https://openalex.org/W4404783665",
        "https://openalex.org/W3211686893",
        "https://openalex.org/W4286987939",
        "https://openalex.org/W2914120296",
        "https://openalex.org/W3174724858",
        "https://openalex.org/W2257408573",
        "https://openalex.org/W4379468930",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4385571124",
        "https://openalex.org/W4226155321",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3100560913",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4321473436",
        "https://openalex.org/W4385572198",
        "https://openalex.org/W4308900200",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W3176798216",
        "https://openalex.org/W2939069254",
        "https://openalex.org/W3138920323",
        "https://openalex.org/W4307225507",
        "https://openalex.org/W4241748643",
        "https://openalex.org/W3205068155",
        "https://openalex.org/W4394659876",
        "https://openalex.org/W3031152519",
        "https://openalex.org/W3168752284",
        "https://openalex.org/W3001434439",
        "https://openalex.org/W4318908031",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3034284720"
    ],
    "abstract": "Multilingual Large Language Models (LLMs) have recently shown great capabilities in a wide range of tasks, exhibiting state-of-the-art performance through zero-shot or few-shot prompting methods. While there have been extensive studies on their abilities in monolingual tasks, the investigation of their potential in the context of code-switching (CSW), the practice of alternating languages within an utterance, remains relatively uncharted. In this paper, we provide a comprehensive empirical analysis of various multilingual LLMs, benchmarking their performance across four tasks: sentiment analysis, machine translation, summarization and word-level language identification. Our results indicate that despite multilingual LLMs exhibiting promising outcomes in certain tasks using zero or few-shot prompting, they still underperform in comparison to fine-tuned models of much smaller scales. We argue that current “multilingualism\" in LLMs does not inherently imply proficiency with code-switching texts, calling for future research to bridge this discrepancy. ©2023 Association for Computational Linguistics.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 12567–12582\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nMultilingual Large Language Models Are Not (Yet) Code-Switchers\nRuochen Zhang∗1 Samuel Cahyawijaya∗2\nJan Christian Blaise Cruz∗3 Genta Indra Winata∗4 Alham Fikri Aji∗ ∗5\n1Brown University 2HKUST\n3Samsung R&D Institute Philippines 4Bloomberg 5MBZUAI\nruochen_zhang@brown.edu scahyawijaya@connect.ust.hk\njcb.cruz@samsung.com gwinata@bloomberg.net alham.fikri@mbzuai.ac.ae\nAbstract\nMultilingual Large Language Models (LLMs)\nhave recently shown great capabilities in a wide\nrange of tasks, exhibiting state-of-the-art perfor-\nmance through zero-shot or few-shot prompt-\ning methods. While there have been extensive\nstudies on their abilities in monolingual tasks,\nthe investigation of their potential in the context\nof code-switching (CSW), the practice of alter-\nnating languages within an utterance, remains\nrelatively uncharted. In this paper, we provide\na comprehensive empirical analysis of various\nmultilingual LLMs, benchmarking their perfor-\nmance across four tasks: sentiment analysis,\nmachine translation, summarization and word-\nlevel language identification. Our results indi-\ncate that despite multilingual LLMs exhibiting\npromising outcomes in certain tasks using zero\nor few-shot prompting, they still underperform\nin comparison to fine-tuned models of much\nsmaller scales. We argue that current “multilin-\ngualism\" in LLMs does not inherently imply\nproficiency with code-switching texts, calling\nfor future research to bridge this discrepancy.\n1 Introduction\nLarge Language Models (LLMs) have shown their\npotential in the context of zero-shot and few-shot\nprompting (Brown et al., 2020; Kojima et al., 2022;\nWei et al., 2022; Longpre et al., 2023). The suc-\ncesses of these LLMs have also been effective\nin multilingual settings (Lin et al., 2021; Winata\net al., 2021b; Scao et al., 2022) where models\nare specifically trained to learn individual lan-\nguages, proven to be highly beneficial for mono-\nlingual tasks. However, in multilingual communi-\nties, people do not confine themselves to speaking\nonly a single language; instead, they use two or\nmore languages interchangeably during a conver-\nsation - a phenomenon known as code-switching\n(CSW) (Poplack, 1980, 2001). It allows individu-\nals to communicate cultural-specific concepts more\n∗Equal contribution.\neffectively, signaling their group identity and re-\ninforcing their social connection (Do ˘gruöz et al.,\n2021). Yet, existing multilingual LLMs are not\nspecifically trained with objectives for managing\nCSW scenarios. Hence, assessing the capabilities\nof the current multilingual LLMs in processing\nCSW texts is essential to the development of multi-\nlingual language models that are fully compatible\nwith code-switching.\nThe main challenge of developing multilingual\nLLMs optimized for code-switching lies in data\nscarcity. Given the highly colloquial characteris-\ntic of code-switching (Winata et al., 2022b), ex-\nisting resources dedicated to CSW are rare and\ncollection at large-scale requires considerable an-\nnotation efforts. To mitigate such deficiency, Yong\net al. (2023) investigate the possibility of employ-\ning multilingual LLMs to generate high-quality\nsynthetic CSW texts. The study revealed that,\nhosted LLMs, such as InstructGPT (Ouyang et al.,\n2022) and ChatGPT 1 outperform public models\nlike BLOOMZ (Muennighoff et al., 2022) and Flan-\nT5-XXL (Chung et al., 2022) in generating natural-\nsounding CSW texts. However, the quality of the\ngenerated text by these hosted LLMs is mostly con-\nfined to Singlish and significantly declines when\nprompted for other languages. Despite the pre-\nliminary promising results, the generation of high-\nquality CSW texts still remains challenging. This\nobservation motivates us to probe from a differ-\nent perspective - Can existing multilingual LLMs\neffectively understand CSW?\nThere have been previous studies on evaluat-\ning multilingual LMs in CSW scenarios (Tan and\nJoty, 2021; Adilazuarda et al., 2022), where code-\nswitched texts are simulated by replacing words\nfrom parallel corpora. Winata et al. (2021a) also\nassesses models’ effectiveness by experimenting\nwith word embeddings constructed from different\nmethods. These works are mainly built upon small\n1https://chat.openai.com/\n12567\nSentiment Analysis\nSummarization\nPositive\nQué mejor que pasar Valentine 's \nthirdwheeleando , ricura , wuu  => \nSentiment:\nMachine Translation\nSentiment Analysis\nacha tho is movie kis baare me hein?\nTranslate the text above from Hinglish \nto English\nLanguage Identification\nFor each token, identify its language \n(lang1: English, lang2: Hindi, other) using \n[ word | tag ]. Hello koi mere se frndship \nkrlo mere se hor singel nai raha jata =>\nIvy: Chloene bataya tum humare saath nahi aa \nrahe! Carter: mera ek family reunion around \nthat time...Ivy: why? ... Carter: plan toh \nyahi hai at least Ivy: take care!\nMultilingual\nLLMs\n Alright that is fine. \nWhat is the movie?\n[Hello|lang1][koi|lang2][mere|\nlang2][se|lang2][frndship|lang1]\n[krlo|lang2][mere|lang2][se|lang2]\n[hor|lang2][singel|lang1][nai|\nlang2][raha|lang2][jata|lang2]\nCarter is not joining Ivy and \nChloe due to a family reunion. \nCarter's grandfather is very ill.\nSummarization\nFigure 1: Illustration of tasks included in our benchmark study.\nBERT-based models and are restricted by either the\nnumber of languages or tasks investigated. Given\nthe recent success of prompting methods on multi-\nlingual LLMs and the effects of scaling, this paper\npresents a more comprehensive empirical analysis\nof models’ code-switching abilities, including a va-\nriety of languages, task types, model architectures,\nmodel sizes and prompting methods.\nOur results suggest that the scaling law is appli-\ncable to multilingual LLMs across diverse CSW\ntasks and model architectures. However, fine-tuned\nsmaller-scale models substantially outperform the\nlargest multilingual LLM with prompting meth-\nods. In addition, while hosted LLMs achieve scores\ncomparable to our fine-tuned models, such perfor-\nmance remains uninterpretable due to their closed-\nness. We argue that existing multilingual LLMs\nexhibit limited proficiency in code-switching con-\ntexts, highlighting future research opportunities to\ntransform them into true polyglots.\n2 Experimental Setup\n2.1 Datasets\nWe explore four code-switching task categories:\nsentiment analysis (SA), machine translation (MT),\nsummarization (SUM), and word-level language\nidentification (LID). The description of each task\nis as follows:\nSentiment Analysis We use sentiment analysis\ndatasets of three different language pairs: Sentimix\nSpanish-English (Aguilar et al., 2020), MixSenti-\nment Malayalam (Chakravarthi et al., 2020a), and\nMixSentiment Tamil (Chakravarthi et al., 2020b).\nBesides the common positive and negative labels,\nthese datasets also contain extra labels like neutral\nor other. However, occurrences of those labels are\nvery scarce. Hence, to normalize datasets from dif-\nferent sources, we simplify the data by filtering out\nexamples outside positive or negative labels. The\ndataset sizes, broken down into train/validation/test,\nare as follows: 8,831/8,831/1,342 pairs for the\nSpanish-English subset, 2,541/275/703 for Malay-\nalam, and 9,075/1,022/2,499 for the Tamil subset.\nMachine Translation We use the code-switched\ndatasets from MixMT 2022 shared task (Srivastava\nand Singh, 2022) that contains Hinglish-English\nsentence pairs (8,060 pairs in the training split, 942\npairs in validation and 960 pairs in the test split).\nSummarization We use code-switched summa-\nrization dataset Gupshup (Mehnaz et al., 2021),\nwhich is derived from SAMSum (Gliwa et al.,\n2019) via crowdsourcing translation. In our ex-\nperiment, We focus on Hinglish to English, as eval-\nuating code-switched summary systematically with\nexisting auto metrics has shown to be challenging\nfor multilingual LLMs (Zhang and Eickhoff, 2023).\nThe dataset contains 5,831 source-target pairs for\ntraining, with 500 pairs each for validation and\ntesting.\nWord-level LID We use English-Hindi and Mod-\nern Standard Arabic (MSA) - Egyptian Arabic (EA)\nsubsets from the Language Identification task in\n12568\nModel Model Type Model Sizes Datasets # Languages LLM Objectives\nEnc-Only Dec-Only Enc-Dec\nXLM-R (Conneau et al., 2020)✓ 250M, 560M CommonCrawl 100 MLMmBERT (Devlin et al., 2019)✓ 178M Wikipedia 104 MLMmDeBERTa v3 (He et al., 2021)✓ 278M CC100 100 RTD w/ GDESmBART-50 (Tang et al., 2020) ✓ 611M CC25, ML50 50 Denoising w/ CLMM2M100 (Fan et al., 2020) ✓ 418M, 1.2B CCMatrix, CCAligned 100 CLMXGLM (Lin et al., 2021) ✓ 564M, 1.7B, 2.9B, 4.5B, 7.5B CommonCrawl 30 CLMBLOOMZ (Muennighoff et al., 2022)✓ 560M, 1.1B, 1.7B, 3B, 7.1B ROOTS, xP3 46 Instruction TunedmT0 (Muennighoff et al., 2022) ✓ 300M, 580M, 1.2B, 3.7B, 13B mC4, xP3∼120 Instruction Tuned\nChatGPT (Bang et al., 2023) ✓ - - - RLHF\nTable 1: Comparison of different model variants studied in this paper.\nthe LinCE benchmark (Aguilar et al., 2020). In\nthis task, the system is tasked with classifying the\nlanguage of each word in a sentence into one of\nthe three classes, lang1, lang2, or other. lang1\nand lang2 are English and Hindi, or MSA and EA,\nrespectively. The English-Hindi subset contains\n4,832 training examples and 744 validation exam-\nples. For the MSA-EA subset, it contains 8,464\nexamples for training and 1,116 for validation. Our\nresults are reported on the validation set as the test\nset is unavailable publicly.\n2.2 Models\nZero-shot and Few-shot Models For zero-shot\nand few-shot prompting, we explore various mul-\ntilingual generative LLMs of different pretraining\nprocesses and architectures, including BLOOMZ,\nmT0 (Muennighoff et al., 2022) and XGLM (Lin\net al., 2021). We explore all model sizes except\nfor BLOOMZ 175B due to resource limitations.\nWe also include ChatGPT into our analysis and\nspecifically GPT-3.5turbo is used. We explore 0, 1,\n3, and 5-shot on each model with 5 diverse prompt\ntemplates. Details for each prompt can be seen in\nAppendix C.\nFor the SA task, we compute the probability of\nthe model to generate each label as the next im-\nmediate continual generation, and then we pick\nthe label resulting in the highest probability for\nthe whole sequence. For MT, SUM and LID, we\nperform standard text generation. However, for\nLID, we expect the generated text to follow a pre-\ndefined format where each [token, language tag]\npair is represented as [ token | tag ]. We parse\nthe generation using a dynamic programming algo-\nrithm introduced in Paolini et al. (2021) to extract\nthe valid [token, language tag] pairs for evaluation.\nFine-tuning Models In addition to zero-shot\nprompting models and few-shot in-context learning,\nwe also experiment with fine-tuning as a bench-\nmark against prompting. For SA and word-level\nLID tasks, we fine-tune four models, namely, base\nand large variants of XLM-RoBERTa (Conneau\net al., 2020), mBERT (Devlin et al., 2019), and\nmDeBERTa v3 (He et al., 2021).\nFor MT, we fine-tune eight models in total.\nThese include small, base, and large variants of\nmT0 (Muennighoff et al., 2022); 418M and 1.2B\nvariants of M2M100 (Fan et al., 2020); and stan-\ndard, one-to-many, and many-to-many variants of\nmBART-50 (Liu et al., 2020; Tang et al., 2020)2\nFor SUM, we follow the same setup used in MT,\nexcept we only fine-tune the three previously men-\ntioned mT0 models and only the standard mBART-\n50 as the one-to-many and many-to-many variants\nare specifically for translation only.\nAcross all the tasks, we fine-tune the selected\nmodels on all the available training instances. Ta-\nble 1 shows a full overview and comparison of\nthe models investigated in this study and details\nfor training setups for all tasks can be found in\nAppendix A.\n3 Results and Discussion\nOverall Results Figure 2 presents the results\nof various multilingual LLMs for the four CSW\ntasks.3 In general, we observe a scaling pattern\nwhen prompting multilingual LLMs across tasks.\nNevertheless, the performance of these models sig-\nnificantly falls short when compared to that of sub-\nstantially smaller fine-tuned models. Therefore,\nadopting a fine-tuned model is a more practical\napproach for dealing with CSW tasks, especially\nin scenarios with constrained computational re-\nsources. For ChatGPT, it demonstrates comparable\nperformance to fine-tuned models across all tasks\n2Due to space constraint, we show a selection of all fine-\ntuned models in Table 2. For the full results, please refer to\nAppendix B.\n3Note that the results for SA, MT, SUM are derived from\nzero-shot prompting while LID results are based on 5-shot.\n12569\nSentiment Analysis Machine Translation Summarization Language Identification\nF1 BLEU RL F1Model Mal-Eng Spa-Eng Tam-EngModel Hngβ→Eng Eng→Hngβ Model Hngβ→Eng Model Hin-Eng MSA-EA\nFinetuning Finetuning Finetuning Finetuning\nXLMR278M 77.08 77.14 68.12 M2M100418M 28.53 12.40 mT0p, 300Mδ 29.83XLMR278M 82.44 72.58\nXLMR560M 79.94 78.81 68.28mBART50610Mγ 29.53 13.38 mT0p, 580Mδ 37.44XLMR560M 86.65 79.79\nmBERT178M 78.21 70.02 65.19 mT0p, 580Mδ 25.47 12.28 mT0p, 1.2Bδ 40.12mBERT178M 81.99 68.02\nmDeBERTa278M 44.56 88.17 45.56mT0p, 1.2Bδ 31.88 13.90 mBART50610M 39.03mDeBERTa278M85.41 68.02\n0-shot Prompting 0-shot Prompting 0-shot Prompting 5-shot Prompting\nmT0300M 36.79 48.44 42.26 mT0300M 2.74 1.60 mT0300M 16.00mT0300M 2.13 0.90\nmT0580M 44.60 56.01 47.62 mT0580M 6.42 2.37 mT0580M 20.16mT0580M 0.30 0.00\nmT01.2B 55.62 67.63 53.88 mT01.2B 10.64 1.88 mT01.2B 23.63mT01.2B 0.22 0.27\nmT03.7B 35.27 59.28 38.55 mT03.7B 12.78 2.08 mT03.7B 27.40mT03.7B 0.19 1.49\nmT013B 49.97 65.26 50.76 mT013B 19.28 1.66 mT013B 30.67mT013B 7.51 5.07\nBLOOMZ560M 59.64 72.79 55.30 BLOOMZ560M 2.24 1.37 BLOOMZ560M 14.22BLOOMZ560M 5.38 2.08\nBLOOMZ1.1B 50.64 70.89 53.27 BLOOMZ1.1B 2.79 1.73 BLOOMZ1.1B 16.45BLOOMZ1.1B 16.31 10.56\nBLOOMZ1.7B 47.83 73.20 50.15 BLOOMZ1.7B 2.62 2.62 BLOOMZ1.7B 16.85BLOOMZ1.7B 13.04 3.37\nBLOOMZ3B 56.84 72.85 53.41 BLOOMZ3B 3.13 2.86 BLOOMZ3B 20.97BLOOMZ3B 19.61 17.47\nBLOOMZ7B 64.21 74.61 59.43 BLOOMZ7B 3.67 1.88 BLOOMZ7B 17.01BLOOMZ7B 19.58 9.26\nXGLM564M 52.18 64.16 52.66 XGLM564M 0.45 0.28 XGLM564M 4.29 XGLM564M 6.65 1.61\nXGLM1.7B 50.83 65.01 50.55 XGLM1.7B 0.79 0.43 XGLM1.7B 5.42 XGLM1.7B 5.90 6.27\nXGLM2.9B 60.15 64.78 56.43 XGLM2.9B 1.34 0.69 XGLM2.9B 5.75 XGLM2.9B 17.64 10.75\nXGLM4.5B 62.32 70.34 56.94 XGLM4.5B 2.13 0.47 XGLM4.5B 4.73 XGLM4.5B 19.35 20.51\nXGLM7.5B 60.93 68.52 56.04 XGLM7.5B 1.43 0.39 XGLM7.5B 5.92 XGLM7.5B 16.91 18.91\nGPT-3.5turbo 65.92 75.64 63.15 GPT-3.5turbo 27.64 4.32 GPT-3.5turbo 25.07GPT-3.5turboα 80.19 71.41\nαDue to budget limitations, the results presented in GPT-3.5turboare based on 1-shot prompting instead of 5-shot.\nβ,γ,δHng refers to Hinglish, a mix of Hindi and English. mBART50 refers to the many-to-many variant. mT0p refers to the fine-tuned mT0 with\nprompt templates.\nTable 2: Code-switching benchmark results for finetuned and prompting models. We report the 0-shot performance\nfor the sentiment analysis, machine translation and summarization tasks; and 5-shot performance for the word-level\nlanguage identification task.\nand datasets, except for the English to Hinglish MT\ntask. This exception may stem from the challenges\nin generating code-switched texts as outlined in\nprevious research (Yong et al., 2023; Zhang and\nEickhoff, 2023). For the remaining tasks, ChatGPT\nnotably outperforms publicly available multilin-\ngual LLMs. Such discrepancy may be attributed\nto the RLHF objective in its pretraining process,\nalthough a comprehensive analysis is hindered by\nits proprietary nature.\n3.1 Sentiment Analysis Results\nFigure 5 shows a detailed breakdown for each of\nthe three language datasets in the SA task. The\nresults from fine-tuned models mainly reside in the\ntop-left corner across all three datasets, highlight-\ning their superior performance with considerably\nsmaller sizes. Scaling BLOOMZ and XGLM yield\nsmall improvements, however, scores from mT0\nfluctuate around 50 F1 when varying sizes. It’s\nworth noting that the majority-class baseline of\nthese three datasets has an average F1 score of\n46. Considering the instability observed during the\nscaling-up process, mT0 struggles to understand\nthe sentiment when presented in CSW texts.\n3.2 Machine Translation Results\nAs shown in Figure 2 and Table 2, when the source\nis Hinglish and target English, the performance\ngap between prompting and fine-tuning in MT\nis much more apparent, with the best prompted\nLLM mT0-XXL achieving no more than 20 BLEU\nwhile all the fine-tuned models achieved between\n25-32 BLEU score. In contrast to SA, we no-\ntice especially visible improvement during scal-\ning up encoder-decoder style models such as mT0,\nwhile decoder-only models such as BLOOMZ and\nXGLM have minimal improvements given their\noverall poor performance.\nWe then compare the difference in LLM scal-\ning between translation tasks with code-switched\nsources and monolingual ones 4. Figure 3 shows\nthe scaling trajectory of LLMs for both Hindi →\nEnglish and Hinglish →English translation direc-\ntion; Table 3 presents the regression coefficient (β)\nin these two scenarios. A large coefficient indicates\nscaling has more noticeable impacts. We can ob-\nserve that the influence of scaling is more apparent\nin monolingual sources than in the code-switched\n4Monolingual experiments are conducted on WMT 2014\nHindi-English dataset (Bojar et al., 2014).\n12570\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n40\n45\n50\n55\n60\n65\n70\n75\n80Macro F1 (%)\nmBERT (FT)\nXLMR (FT)\nmDeBERTA (FT)\nmT0\nBLOOMZ\nXGLM\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n0\n5\n10\n15\n20\n25\n30BLEU\nmBART-50 M2M (FT)\nmT0\nm2m100 (FT)\nmT0 (FT)\nXGLM\nBLOOMZ\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n0\n10\n20\n30\n40ROUGEL\n mBART-50 (FT)\nmT0\nmT0 Prompted (FT)\nXGLM\nBLOOMZ\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n0\n20\n40\n60\n80\n100Macro F1 (%)\nXLMR (FT)\nmT0\nmBERT (FT)\nmDeBERTA (FT)\nBLOOMZ\nXGLM\nFigure 2: Evaluation results of fine-tuning and prompting LLMs of different scales on various CSW tasks. (top left)\nF1-score on the sentiment analysis task, (top right) BLEU score on the machine translation task, (bottom left)\nROUGE-L on the summarization task, and (bottom right) F1-score on the word-level language identification task.\n(FT) means results are from fine-tuned models.\nsetup. This pattern could potentially result from\nthe limited pretraining samples for Hinglish code-\nswitched data, leading to a sub-optimal scaling\nperformance.\nWhen models are tasked with translating the\nsource into CSW text, a substantial performance de-\ncline is observed for both fine-tuned and prompted\nmodels. We notice that while the larger mT0 mod-\nels are capable of producing English translations in\na zero-shot manner, they struggle to generate CSW\ntexts as seen in previous work (Yong et al., 2023).\nUpon looking at the output, mT0 simply outputs in\nEnglish, even in few-shot settings in which it has\nseen some other Hinglish examples.\n3.3 Summarization Results\nFigure 2 shows the fine-tuning and zero-shot\nprompting result on the summarization task. Sim-\nilarly, we see that fine-tuned models outperform\nthe zero-shot approach. Similar to MT, mT0 yields\nthe overall best performance and shows positive\nscaling law.\nTo disentangle CSW from the equation, we eval-\nuate the LLM’s performance on the same Gupshup\ndataset, but with English input rather than Hinglish\ninput. The evaluation set is parallel to each other.\nInterestingly, from Figure 3 and Table 3, we see a\nsimilar scaling impact whether the input is mono-\nlingual or in code-switch. However, the models are\nconsistently better if the input is in English.\n3.4 Language Identification Results\nOur observation of fine-tuned models in the LID\ntask is similar to the MT task: they outperform\nprompting methods on multilingual LLMs by a\nsignificant margin. In Table 2, we report 5-shots\ninstead of 0-shot prompting results for LID tasks as\n0-shot results are all 0 for both language datasets\nand across all models. The multilingual LLMs\nare not able to understand the natural language\ninstruction that requires them to generate outputs\nin a specific format like [ token | tag ]word by\nword. When prepending more in-context examples\nin the instruction, we observe slight performance\nimprovements across different models. For results\non few-shot experiments for LID, please refer to\nSection 3.5.\n3.5 Few-Shot Results\nCompared to zero-shot inference, few-shot learning\nhas been shown to boost performance as discussed\nin previous works(Brown et al., 2020; Liu et al.,\n2021). However, in CSW settings, we observe dif-\nferent effects of adding more in-context examples\n12571\nbetween tasks. In Figure 4, we notice a decrease in\nmetrics from 0-shot to 1-shot for SA and SUM, sug-\ngesting that in-context examples do not contribute\nto or even degrade models’ performance. We sus-\npect that models have seen these tasks in a monolin-\ngual fashion during pretraining, and thus are able\nto understand instructions well in a zero-shot set-\nting. Instead, models may consider CSW examples\nas low-quality texts, thus confusing the generation\nprocess. For MT, we observe negligible change in\nthe models’ performances with an increasing num-\nber of examples. Notably, instead of translating\nsentences to Hinglish as instructed, models could\nonly repeat the original English sentences. For in-\nstance, when provided with 5 in-context examples,\nmT013B is instructed to “Translate the following\ntext from English to Hinglish. Text: hello there,\nI have not seen this movie so im going to take a\nminute to look it over :) Translation:”. It gener-\nates “hello there, I have not seen this movie so I\ngoing to take time to look it over:).” instead of the\nexpected “hello yar, mein is movie ko nahi dekha\nhoon tho, tho mein thode der ke liye isko dekh\nloonga”. Similar issues are also observed with\nBLOOMZ. We hypothesize that models may not\nfully comprehend the nuances of ’Hinglish’ within\nthe given instruction, which could account for their\nrelatively uniform performance across varying shot\nnumbers.\nOn the contrary, more in-context examples bene-\nfit the LID task. As no models are pre-trained on\nthe sequence tagging task, the natural instruction\nentailing the specific generation format is new to\nthe LLMs. Therefore, in our experiments, most\nmodels perform best when given 5 learning exam-\nples. Additionally, though we observe scaling law\npatterns in 5-shot settings as shown in Figure 6, for\nthe best-performing billion-parameter models, we\nstill consistently observe their inability to adhere to\nthe format laid out in the instructions. They often\nfail to replicate the exact words required for sen-\ntence tagging or predict multiple tokens within a\nsingle bracket pair. For example, in a 5-shot setting,\nwhen asked to label the sentence \"we the fans luv\nyou , sirji”, BLOOMZ7b wrongly generates “[ we\nthe fans | lang1 ] [ you | lang1 ] [ sirji | lang1 ] [\n, | other ]”, unable to put individual words in the\nbrackets and omitting some words from the origi-\nnal sentence. Given the constraint of limited input\nlength, which restricts the number of in-context ex-\namples models can learn from, their uncontrollable\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n5\n10\n15\n20\n25BLEU\nDataset\nCode-Mixing\nMonolingual\nModel T ype\nBLOOMZ\nmT0\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n0\n10\n20\n30\n40ROUGEL\nDataset\nCode-Mixing\nMonolingual\nModel T ype\nmT0\nXGLM\nBLOOMZ\nFigure 3: Performance comparison on (top)\nHindi→English vs Hinglish→English translation and\n(bottom) Hinglish→English vs English →English\nsummarization.\ngeneration still results in a significant performance\ngap when compared to fine-tuning smaller models\n(∼20 F1 vs. ∼80 F1).\n3.6 Benchmarking ChatGPT\nGiven recent developments in general-purpose,\ninstruction-following LLMs like ChatGPT, with\nimpressive zero-shot abilities across tasks, we also\nbenchmark ChatGPT’s performance in our CSW\ntask. Limited by the budget, we only explore zero-\nshot performance for SA, MT and SUM given their\neasy scopes, and 1-shot performance for LID due\nto the specific output format requirements. Since\nwe can’t access ChatGPT’s output probability dis-\ntribution, we instruct ChatGPT to return only the\nexact string label and calculate F1 scores using\nexact string matching for SA.\nChatGPT achieves somewhat comparable perfor-\nmance to finetuning models and significantly out-\nperforms other public multilingual LLMs in most\nof the tasks. Especially for LID, it shows strong\ncapabilities in following difficult instructions with\nonly one example. The only exception is on the\nEnglish→Hinglish MT tasks, where its zero-shot\n12572\nModel Code-Switched Monolingual\nβ α β α\nMachine Translation\nmT0 1.057 6.403 1.626 6.075\nBLOOMZ 0.192 2.373 0.824 6.240\nSummarization\nmT0 0.712 5.471 0.738 9.228\nBLOOMZ 0.312 3.507 0.644 8.637\nXGLM 0.029 0.444 0.012 0.883\nTable 3: Regression slope (β) and intercept (α) of scal-\ning mT0 and BLOOMZ on monolingual/code-switched\nmachine translation and summarization task.\nperformance is only slightly better than other pub-\nlic LLMs. We hypothesize mainly two reasons\nbehind the difficulty in generating CSW texts: 1)\nas alluded to in the previous section, CSW texts can\nbe perceived as noises given tasks and pretraining\nprocesses are designed in a monolingual fashion;\n2) LLMs may have a lack of sufficient representa-\ntion for CSW text structure. In our analysis, LLMs\nperform much better in SA tasks as they could pick\nup cues from individual works instead of paying\nattention to language “structure” when tasked with\ntext generation.\nLastly, while ChatGPT delivers promising re-\nsults without any fine-tuning, the lack of complete\ntransparency on its pretraining datasets, model ar-\nchitecture, and training details obstructs a better un-\nderstanding of its performance. This presents road-\nblocks to future improvements in code-switching\nproficiency for public multilingual LLMs.\n4 Implications for Future LLMs\nIn this section, we walk through various implica-\ntions of our work and provide recommendations\nfor enabling better CSW ability in LLMs. By high-\nlighting this limitation, we compel researchers to\nconsider CSW as a core feature of many people’s\nmultilingual repertoire across the world.\nFairer Data Representation for Code-Switching\nOur results in Section 3 show that existing LLMs\nhave similar scaling patterns between monolingual\nand CSW. However, despite all the models under\nstudy having seen each of the languages during pre-\ntraining, there is still a performance gap between\nmonolingual and CSW. This suggests that the abil-\nity to code-switch is not acquired by LLMs after\npretraining and/or instruction-tuning with multilin-\ngual data (Xue et al., 2021; Scao et al., 2022; Muen-\nnighoff et al., 2022), indicating the need for adding\nbetter data representation for code-switching in the\nmultilingual pretraining and/or instruction-tuning\nprocess. Such an approach can be done through\nmanual CSW data collection and/or various data\naugmentation methods (Tan and Joty, 2021; Adi-\nlazuarda et al., 2022; Dhole et al., 2023). Aside\nfrom adding more CSW data, one potential solu-\ntion is to identify and include the code-switching\nlanguage pairs into consideration of multilingual\npretraining and/or instruction-tuning. This allows\nbetter resampling strategy (Lample and Conneau,\n2019; Aharoni et al., 2019; Conneau et al., 2020;\nXue et al., 2021; Tang et al., 2021; Cahyawijaya\net al., 2021) for CSW data during the multilingual\npretraining and/or instruction-tuning.\nAdaptation and Extension of Code-Switching\nOptimization Objectives Existing LLMs are op-\ntimized solely with language modeling objectives\neither for sentence denoising or sentence comple-\ntion. However, alternative optimization objectives„\nsuch as meta transfer learning (Winata et al., 2020)\nand additional token/span-level language identifi-\ncation objective (Li et al., 2019), have been demon-\nstrated to effectively enhance CSW performance\nwith minimal performance loss on monolingual\ntasks in CSW speech processing. By adapting and\nextending these approaches to NLP, we may be able\nto equip LLMs with better CSW capability without\nrequiring expensive data collection and annotation.\nThis would be particularly advantageous for LLMs,\nespecially in applications where CSW is prevalent\nwithin the multilingual community.\nTowards More Inclusive Language Technology\nIn light of the fact that LLMs are the driving\nforce behind the progress of various NLP technolo-\ngies (Thoppilan et al., 2022; SambaNova Systems,\n2023; Pratap et al., 2023), we emphasize the impor-\ntance of incorporating code-switched capabilities\nin LLMs to promote inclusivity and diversity in\nlanguage technology, particularly for multilingual\nspeakers who frequently engage in code-switching\nin their daily lives. By enabling NLP technology\nto reflect the language-mixing patterns of users,\npeople can communicate in ways that are more\ncomfortable and authentic to their linguistic identi-\nties, eliminating the need for people to adjust their\nspeech patterns to become legible to machines. It\n12573\n0 1 3 5\nk-Shot\n40\n50\n60\n70Macro F1 (%)\nBLOOMZ 7B\nGPT-3.5 T urbo\nmT0-XXL\nXGLM-7.5B\n0 1 3 5\nk-Shot\n0\n10\n20\n30BLEU\nBLOOMZ 7B\nGPT-3.5 T urbo\nmT0-XXL\nXGLM-7.5B\n0 1 3 5\nk-Shot\n0\n10\n20\n30ROUGEL\nBLOOMZ 7B\nmT0-XXL\nXGLM-7.5B\nGPT-3.5 T urbo\n0 1 3 5\nk-Shot\n0\n20\n40\n60\n80Macro F1 (%)\nBLOOMZ 7B\nGPT-3.5 T urbo\nmT0-XXL\nXGLM-7.5B\nFigure 4: Few-shot evaluation performance for (top left) sentiment analysis task, (top right) machine translation\ntask, (bottom left) summarization task and (bottom right) word-level LID task.\nwould not only mitigate the effects of linguistic pro-\nfiling (Baugh, 2005; Dingemanse and Liesenfeld,\n2022) and hegemonic, Western-centric technologi-\ncal designs but also foster greater trust among users\nin language technology through naturalistic dia-\nlogue interactions. Therefore, we urge the integra-\ntion of code-switched recognition and generation\ncapabilities in future LLMs.\n5 Related Work\nCode-Switching Code-switching is a common\npractice observed in multilingual communities\nwhere people mix multiple languages within an\nutterance (Poplack, 2001). While more than half\nof the world population speaks more than one lan-\nguage, the availability of resources and assessments\nfor code-switching is much more limited compared\nto the extensive literature on monolingual cases.\nThe key challenges of collecting high-quality code-\nswitching data lie in the colloquial nature of the\npractice and the language proficiency required for\naccurate annotation (Winata et al., 2022b). The\nrecent advances of “multilingual” large language\nmodels compel one to explore whether these mod-\nels are proficient in code-switching contexts like\na true polyglot. Previous research (Winata et al.,\n2021a) has studied the code-switching capabilities\nof language models in NER and POS-tagging tasks,\nhowever, the work is limited to using only differ-\nent word embeddings and encoder-only models. In\nthis paper, we expand on previous works and pro-\nvide a detailed analysis of more model variations,\ntask objectives and downstream applications of di-\nverse language pairs adopted from existing CSW\nbenchmarks like LinCE (Aguilar et al., 2020) and\nGlueCOS (Khanuja et al., 2020).\nMultilingual Large Language Models Mod-\nels like mBERT (Devlin et al., 2019) and XLM-\nR (Conneau et al., 2020) have become the go-\nto multilingual options for supervised fine-tuning,\ngiven their impressive abilities and adaptability\nto many languages. With the success of large-\nscale generative models, their capabilities have\nbeen enriched with multilingual objectives (Lin\net al., 2021; Scao et al., 2022; Muennighoff et al.,\n2022) through pretraining on large multilingual\ncorpora such ROOTS (Laurençon et al., 2022),\nmC4 (Raffel et al., 2019) and xP3 (Muennighoff\net al., 2022). In addition to excelling in differ-\nent monolingual and multilingual benchmarks via\nzero-shot prompting (Sanh et al., 2021; Wei et al.,\n2021; Kojima et al., 2022; Muennighoff et al., 2022;\nBang et al., 2023), research has shown that scaling\nup model sizes (Cahyawijaya et al., 2023; Kaplan\net al., 2020; Fernandes et al., 2023) and incorpo-\nrating in-context learning examples (Winata et al.,\n2022a; Tanwar et al., 2023) could help further boost\n12574\ntheir performance. Yet, given the scarcity of CSW\nevaluation resources, how these multilingual LLMs\nperform in code-switching scenarios still remains\nquestionable. In this paper, we evaluate these mod-\nels under various settings including fine-tuning,\nzero-shot prompting, and in-context learning, and\nprovide recommendations for future improvements\nin code-switching proficiency.\n6 Conclusion\nIn this paper, we systematically study multilin-\ngual LLMs’ capabilities in code-switching tasks\nalong various dimensions, including but not lim-\nited to finetuning vs. prompting, task objectives,\nscaling laws and model architecture. We observe\nthat, despite improvements with larger sizes, ex-\nisting multilingual LLMs still yield inferior per-\nformance compared to fine-tuning smaller models.\nWe argue that multilingual LLMs are not necessar-\nily code-switching compatible. Given that multi-\nlingual LLMs are not explicitly trained for code-\nswitching data, we recommend future development\nshould incorporate a more comprehensive evalua-\ntion framework that encompasses code-switching\ntexts. Finally, our study is limited to models’ per-\nformance in sentiment analysis, machine transla-\ntion, summarization and language identification.\nWe suggest that benchmarking across a broader\nset of tasks is required. However, the scarcity of\nhigh-quality open-source code-switching datasets\nand the challenges associated with their collection\nprocess imply future work should also include con-\nstructing code-switching data with more complex-\nity, such as commonsense reasoning.\nLimitations\nThe scope of code-switching languages in this work\nis limited to Hindi-English, Standard-Egyptian\nArabic, Spanish-English, Tamil-English, and\nMalayalam-English. It is beneficial to include more\nlanguages to demonstrate the generality of our\nclaim. However, a challenge in doing so arises from\nthe lack of available code-switched text datasets.\nWe explore four different NLP downstream tasks.\nHowever, similar to the previous point, it would\nbe interesting to cover more tasks. Similarly, the\nmain challenge of expanding into different tasks\nis the lack of available datasets. We anticipate\nthat future studies will broaden the exploration of\ncode-switching languages and tasks beyond those\nexamined in this research to showcase the gener-\nalizability of the findings to other code-switching\nlanguages and tasks.\nIn addition, in this study, we choose multilingual\nLLMs based on two criteria: 1) they present or\nadvertise themselves as multilingual and 2) their\npretraining data contain all the languages featured\nin our benchmark dataset. Although some recently\nreleased LLMs like Llama-2 (Touvron et al., 2023)\nand Falcon (Penedo et al., 2023) have demonstrated\nstate-of-the-art performance across various other\nbenchmarks, we defer the evaluation of their code-\nswitching capabilities to future research.\nFinally, our observations are based on the model\nsizes allowed by our local compute resources. A\nmore comprehensive analysis can be obtained by\nexperimenting with a wider range of variations,\nincluding larger model sizes and more in-context\nexamples given a more generous compute budget.\nEthical Considerations\nOur paper highlights the evaluation of LLMs on\ncode-switching, a common phenomenon in the mul-\ntilingual community. The research was carried out\nin compliance with the principles of academic in-\ntegrity, including honesty, transparency, and rigor.\nThe data used in this study was collected in accor-\ndance with ethical guidelines, and all participants\nprovided informed consent. Within our study, we\nare aware of the potential impact that comes with\nour work and our experiments replicate prior work\nunder comparable experimental conditions. We\nalso ensured that the study did not cause harm or\ndistress to any individuals or communities. The\nfindings of this study have important implications\nfor the development of multilingual LLMs and\ntheir potential applications in code-switching tasks.\nHowever, we acknowledge that further research is\nneeded to address the limitations and gaps identi-\nfied in this study. We believe that responsible and\nethical use of language technology is crucial for\ncreating just and equitable systems that benefit all\nindividuals and communities.\nAcknowledgements\nWe would like to thank Xinyu Hua and Samson\nTan for the constructive feedback and helpful dis-\ncussion on our project.\n12575\nReferences\nMuhammad Farid Adilazuarda, Samuel Cahyawijaya,\nGenta Indra Winata, Pascale Fung, and Ayu Purwari-\nanti. 2022. Indorobusta: Towards robustness against\ndiverse code-mixed indonesian local languages. In\nProceedings of the First Workshop on Scaling Up\nMultilingual Evaluation, pages 25–34.\nGustavo Aguilar, Sudipta Kar, and Thamar Solorio.\n2020. Lince: A centralized benchmark for linguis-\ntic code-switching evaluation. In Proceedings of the\nTwelfth Language Resources and Evaluation Confer-\nence, pages 1803–1813.\nRoee Aharoni, Melvin Johnson, and Orhan Firat. 2019.\nMassively multilingual neural machine translation.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long and Short Papers), pages 3874–3884,\nMinneapolis, Minnesota. Association for Computa-\ntional Linguistics.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, Quyet V . Do, Yan\nXu, and Pascale Fung. 2023. A multitask, multilin-\ngual, multimodal evaluation of chatgpt on reasoning,\nhallucination, and interactivity.\nJohn Baugh. 2005. Linguistic profiling. In Black lin-\nguistics, pages 167–180. Routledge.\nOndˇrej Bojar, Christian Buck, Christian Federmann,\nBarry Haddow, Philipp Koehn, Johannes Leveling,\nChristof Monz, Pavel Pecina, Matt Post, Herve Saint-\nAmand, Radu Soricut, Lucia Specia, and Aleš Tam-\nchyna. 2014. Findings of the 2014 workshop on\nstatistical machine translation. In Proceedings of the\nNinth Workshop on Statistical Machine Translation,\npages 12–58, Baltimore, Maryland, USA. Associa-\ntion for Computational Linguistics.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSamuel Cahyawijaya, Holy Lovenia, Tiezheng Yu,\nWilly Chung, and Pascale Fung. 2023. Instruct-\nalign: Teaching novel languages with to llms through\nalignment-based cross-lingual instruction.\nSamuel Cahyawijaya, Genta Indra Winata, Bryan Wilie,\nKarissa Vincentio, Xiaohong Li, Adhiguna Kun-\ncoro, Sebastian Ruder, Zhi Yuan Lim, Syafri Ba-\nhar, Masayu Khodra, Ayu Purwarianti, and Pascale\nFung. 2021. IndoNLG: Benchmark and resources for\nevaluating Indonesian natural language generation.\nIn Proceedings of the 2021 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n8875–8898, Online and Punta Cana, Dominican Re-\npublic. Association for Computational Linguistics.\nBharathi Raja Chakravarthi, Navya Jose, Shardul\nSuryawanshi, Elizabeth Sherly, and John P Mc-\nCrae. 2020a. A sentiment analysis dataset for\ncode-mixed malayalam-english. arXiv preprint\narXiv:2006.00210.\nBharathi Raja Chakravarthi, Vigneshwaran Muralidaran,\nRuba Priyadharshini, and John Philip McCrae. 2020b.\nCorpus creation for sentiment analysis in code-mixed\nTamil-English text. In Proceedings of the 1st Joint\nWorkshop on Spoken Language Technologies for\nUnder-resourced languages (SLTU) and Collabora-\ntion and Computing for Under-Resourced Languages\n(CCURL), pages 202–210, Marseille, France. Euro-\npean Language Resources association.\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\narXiv preprint arXiv:2210.11416.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Édouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In Proceedings of the 2019 Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pages 4171–\n4186.\nKaustubh Dhole, Varun Gangal, Sebastian Gehrmann,\nAadesh Gupta, Zhenhao Li, Saad Mahamood, Abi-\nnaya Mahadiran, Simon Mille, Ashish Shrivastava,\nSamson Tan, Tongshang Wu, Jascha Sohl-Dickstein,\nJinho Choi, Eduard Hovy, Ondˇrej Dušek, Sebastian\nRuder, Sajant Anand, Nagender Aneja, Rabin Ban-\njade, Lisa Barthe, Hanna Behnke, Ian Berlot-Attwell,\nConnor Boyle, Caroline Brun, Marco Antonio So-\nbrevilla Cabezudo, Samuel Cahyawijaya, Emile Cha-\npuis, Wanxiang Che, Mukund Choudhary, Christian\nClauss, Pierre Colombo, Filip Cornell, Gautier Da-\ngan, Mayukh Das, Tanay Dixit, Thomas Dopierre,\nPaul-Alexis Dray, Suchitra Dubey, Tatiana Ekein-\nhor, Marco Di Giovanni, Tanya Goyal, Rishabh\nGupta, Louanes Hamla, Sang Han, Fabrice Harel-\nCanada, Antoine Honoré, Ishan Jindal, Przemysław\nJoniak, Denis Kleyko, Venelin Kovatchev, Kalpesh\nKrishna, Ashutosh Kumar, Stefan Langer, Seung-\njae Ryan Lee, Corey James Levinson, Hualou Liang,\nKaizhao Liang, Zhexiong Liu, Andrey Lukyanenko,\nVukosi Marivate, Gerard De Melo, Simon Meoni,\nMaxine Meyer, Afnan Mir, Nafise Sadat Moosavi,\nNiklas Meunnighoff, Timothy Sum Hon Mun, Ken-\nton Murray, Marcin Namysl, Maria Obedkova, Priti\nOli, Nivranshu Pasricha, Jan Pfister, Richard Plant,\n12576\nVinay Prabhu, Vasile Pais, Libo Qin, Shahab Raji,\nPawan Kumar Rajpoot, Vikas Raunak, Roy Rinberg,\nNicholas Roberts, Juan Diego Rodriguez, Claude\nRoux, Vasconcellos Samus, Ananya Sai, Robin\nSchmidt, Thomas Scialom, Tshephisho Sefara, Saqib\nShamsi, Xudong Shen, Yiwen Shi, Haoyue Shi, Anna\nShvets, Nick Siegel, Damien Sileo, Jamie Simon,\nChandan Singh, Roman Sitelew, Priyank Soni, Tay-\nlor Sorensen, William Soto, Aman Srivastava, Aditya\nSrivatsa, Tony Sun, Mukund Varma, A Tabassum,\nFiona Tan, Ryan Teehan, Mo Tiwari, Marie Tolkiehn,\nAthena Wang, Zijian Wang, Zijie Wang, Gloria Wang,\nFuxuan Wei, Bryan Wilie, Genta Indra Winata, Xinyu\nWu, Witold Wydmanski, Tianbao Xie, Usama Yaseen,\nMichael Yee, Jing Zhang, and Yue Zhang. 2023. Nl-\naugmenter: A framework for task-sensitive natural\nlanguage augmentatio. Northern European Journal\nof Language Technology, 9(1).\nMark Dingemanse and Andreas Liesenfeld. 2022. From\ntext to talk: Harnessing conversational corpora for\nhumane and diversity-aware language technology.\nIn Proceedings of the 60th Annual Meeting of the\nAssociation for Computational Linguistics (Volume\n1: Long Papers), pages 5614–5633, Dublin, Ireland.\nAssociation for Computational Linguistics.\nA. Seza Do ˘gruöz, Sunayana Sitaram, Barbara E. Bul-\nlock, and Almeida Jacqueline Toribio. 2021. A sur-\nvey of code-switching: Linguistic and social per-\nspectives for language technologies. In Proceedings\nof the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1654–1666, Online.\nAssociation for Computational Linguistics.\nAngela Fan, Shruti Bhosale, Holger Schwenk, Zhiyi\nMa, Ahmed El-Kishky, Siddharth Goyal, Man-\ndeep Baines, Onur Celebi, Guillaume Wenzek,\nVishrav Chaudhary, Naman Goyal, Tom Birch, Vi-\ntaliy Liptchinsky, Sergey Edunov, Edouard Grave,\nMichael Auli, and Armand Joulin. 2020. Beyond\nenglish-centric multilingual machine translation.\nPatrick Fernandes, Behrooz Ghorbani, Xavier Garcia,\nMarkus Freitag, and Orhan Firat. 2023. Scaling laws\nfor multilingual neural machine translation. arXiv\npreprint arXiv:2302.09650.\nBogdan Gliwa, Iwona Mochol, Maciej Biesek, and Alek-\nsander Wawer. 2019. SAMSum corpus: A human-\nannotated dialogue dataset for abstractive summa-\nrization. In Proceedings of the 2nd Workshop on\nNew Frontiers in Summarization, pages 70–79, Hong\nKong, China. Association for Computational Linguis-\ntics.\nPengcheng He, Jianfeng Gao, and Weizhu Chen. 2021.\nDebertav3: Improving deberta using electra-style pre-\ntraining with gradient-disentangled embedding shar-\ning.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B\nBrown, Benjamin Chess, Rewon Child, Scott Gray,\nAlec Radford, Jeffrey Wu, and Dario Amodei. 2020.\nScaling laws for neural language models. arXiv\npreprint arXiv:2001.08361.\nSimran Khanuja, Sandipan Dandapat, Anirudh Srini-\nvasan, Sunayana Sitaram, and Monojit Choudhury.\n2020. Gluecos: An evaluation benchmark for code-\nswitched nlp. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3575–3585.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In ICML 2022\nWorkshop on Knowledge Retrieval and Language\nModels.\nGuillaume Lample and Alexis Conneau. 2019. Cross-\nlingual language model pretraining. Advances in\nNeural Information Processing Systems (NeurIPS).\nHugo Laurençon, Lucile Saulnier, Thomas Wang,\nChristopher Akiki, Albert Villanova del Moral, Teven\nLe Scao, Leandro V on Werra, Chenghao Mou, Ed-\nuardo González Ponferrada, Huu Nguyen, et al. 2022.\nThe bigscience roots corpus: A 1.6 tb composite mul-\ntilingual dataset. Advances in Neural Information\nProcessing Systems, 35:31809–31826.\nKe Li, Jinyu Li, Guoli Ye, Rui Zhao, and Yifan Gong.\n2019. Towards code-switching asr for end-to-end\nctc models. In ICASSP 2019 - 2019 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 6076–6080.\nXi Victoria Lin, Todor Mihaylov, Mikel Artetxe, Tianlu\nWang, Shuohui Chen, Daniel Simig, Myle Ott, Na-\nman Goyal, Shruti Bhosale, Jingfei Du, et al. 2021.\nFew-shot learning with multilingual language models.\narXiv preprint arXiv:2112.10668.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2021. What\nmakes good in-context examples for gpt- 3? arXiv\npreprint arXiv:2101.06804.\nYinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey\nEdunov, Marjan Ghazvininejad, Mike Lewis, and\nLuke Zettlemoyer. 2020. Multilingual denoising pre-\ntraining for neural machine translation. Transac-\ntions of the Association for Computational Linguis-\ntics, 8:726–742.\nShayne Longpre, Le Hou, Tu Vu, Albert Webson,\nHyung Won Chung, Yi Tay, Denny Zhou, Quoc V\nLe, Barret Zoph, Jason Wei, et al. 2023. The flan\ncollection: Designing data and methods for effective\ninstruction tuning. arXiv preprint arXiv:2301.13688.\nLaiba Mehnaz, Debanjan Mahata, Rakesh Gosangi,\nUma Sushmitha Gunturi, Riya Jain, Gauri Gupta,\nAmardeep Kumar, Isabelle G. Lee, Anish Acharya,\nand Rajiv Ratn Shah. 2021. GupShup: Summarizing\n12577\nopen-domain code-switched conversations. In Pro-\nceedings of the 2021 Conference on Empirical Meth-\nods in Natural Language Processing , pages 6177–\n6192, Online and Punta Cana, Dominican Republic.\nAssociation for Computational Linguistics.\nNiklas Muennighoff, Thomas Wang, Lintang Sutawika,\nAdam Roberts, Stella Biderman, Teven Le Scao,\nM Saiful Bari, Sheng Shen, Zheng-Xin Yong, Hailey\nSchoelkopf, et al. 2022. Crosslingual generaliza-\ntion through multitask finetuning. arXiv preprint\narXiv:2211.01786.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow in-\nstructions with human feedback. arXiv preprint\narXiv:2203.02155.\nGiovanni Paolini, Ben Athiwaratkun, Jason Krone,\nJie Ma, Alessandro Achille, Rishita Anubhai, Ci-\ncero Nogueira dos Santos, Bing Xiang, and Stefano\nSoatto. 2021. Structured prediction as translation be-\ntween augmented natural languages. arXiv preprint\narXiv:2101.05779.\nGuilherme Penedo, Quentin Malartic, Daniel Hesslow,\nRuxandra Cojocaru, Alessandro Cappelli, Hamza\nAlobeidli, Baptiste Pannier, Ebtesam Almazrouei,\nand Julien Launay. 2023. The RefinedWeb dataset\nfor Falcon LLM: outperforming curated corpora\nwith web data, and web data only. arXiv preprint\narXiv:2306.01116.\nS. Poplack. 2001. Code switching: Linguistic. In In-\nternational Encyclopedia of the Social & Behavioral\nSciences, pages 2062–2065. Elsevier.\nShana Poplack. 1980. Sometimes i’ll start a sentence in\nspanish y termino en espanol: toward a typology of\ncode-switching1.\nVineel Pratap, Andros Tjandra, Bowen Shi, Paden\nTomasello, Arun Babu, Sayani Kundu, Ali Elkahky,\nZhaoheng Ni, Apoorv Vyas, Maryam Fazel-Zarandi,\net al. 2023. Scaling speech technology to 1,000+\nlanguages. arXiv preprint arXiv:2305.13516.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2019. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. arXiv e-prints.\nTogether Computer SambaNova Systems. 2023.\nBLOOMChat: a New Open Multilingual Chat LLM.\nVictor Sanh, Albert Webson, Colin Raffel, Stephen H.\nBach, Lintang Sutawika, Zaid Alyafeai, Antoine\nChaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja,\nManan Dey, M Saiful Bari, Canwen Xu, Urmish\nThakker, Shanya Sharma Sharma, Eliza Szczechla,\nTaewoon Kim, Gunjan Chhablani, Nihal Nayak, De-\nbajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang,\nHan Wang, Matteo Manica, Sheng Shen, Zheng Xin\nYong, Harshit Pandey, Rachel Bawden, Thomas\nWang, Trishala Neeraj, Jos Rozen, Abheesht Sharma,\nAndrea Santilli, Thibault Fevry, Jason Alan Fries,\nRyan Teehan, Stella Biderman, Leo Gao, Tali Bers,\nThomas Wolf, and Alexander M. Rush. 2021. Multi-\ntask prompted training enables zero-shot task gener-\nalization.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\nIn International Conference on Machine Learning,\npages 4596–4604. PMLR.\nVivek Srivastava and Mayank Singh. 2022. Overview\nand results of MixMT shared-task at WMT 2022. In\nProceedings of the Seventh Conference on Machine\nTranslation (WMT) , pages 806–811, Abu Dhabi,\nUnited Arab Emirates (Hybrid). Association for Com-\nputational Linguistics.\nSamson Tan and Shafiq Joty. 2021. Code-mixing on\nsesame street: Dawn of the adversarial polyglots. In\nProceedings of the 2021 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npages 3596–3616.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2020. Multilingual translation with exten-\nsible multilingual pretraining and finetuning. arXiv\npreprint arXiv:2008.00401.\nYuqing Tang, Chau Tran, Xian Li, Peng-Jen Chen, Na-\nman Goyal, Vishrav Chaudhary, Jiatao Gu, and An-\ngela Fan. 2021. Multilingual translation from de-\nnoising pre-training. In Findings of the Association\nfor Computational Linguistics: ACL-IJCNLP 2021,\npages 3450–3466, Online. Association for Computa-\ntional Linguistics.\nEshaan Tanwar, Manish Borthakur, Subhabrata Dutta,\nand Tanmoy Chakraborty. 2023. Multilingual llms\nare better cross-lingual in-context learners with align-\nment. arXiv preprint arXiv:2305.05940.\nRomal Thoppilan, Daniel De Freitas, Jamie Hall, Noam\nShazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,\nAlicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.\n2022. Lamda: Language models for dialog applica-\ntions. arXiv preprint arXiv:2201.08239.\nHugo Touvron, Louis Martin, Kevin Stone, Peter Al-\nbert, Amjad Almahairi, Yasmine Babaei, Nikolay\nBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti\nBhosale, et al. 2023. Llama 2: Open founda-\ntion and fine-tuned chat models. arXiv preprint\narXiv:2307.09288.\n12578\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin\nGuu, Adams Wei Yu, Brian Lester, Nan Du, An-\ndrew M Dai, and Quoc V Le. 2021. Finetuned lan-\nguage models are zero-shot learners. arXiv preprint\narXiv:2109.01652.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nGenta Winata, Shijie Wu, Mayank Kulkarni, Thamar\nSolorio, and Daniel Preotiuc-Pietro. 2022a. Cross-\nlingual few-shot learning on unseen languages. In\nProceedings of the 2nd Conference of the Asia-Pacific\nChapter of the Association for Computational Lin-\nguistics and the 12th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 777–791, Online only. Association for\nComputational Linguistics.\nGenta Indra Winata, Alham Fikri Aji, Zheng-Xin Yong,\nand Thamar Solorio. 2022b. The decades progress\non code-switching research in nlp: A systematic\nsurvey on trends and challenges. arXiv preprint\narXiv:2212.09660.\nGenta Indra Winata, Samuel Cahyawijaya, Zhaojiang\nLin, Zihan Liu, Peng Xu, and Pascale Fung. 2020.\nMeta-transfer learning for code-switched speech\nrecognition. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3770–3776, Online. Association for Computa-\ntional Linguistics.\nGenta Indra Winata, Samuel Cahyawijaya, Zihan Liu,\nZhaojiang Lin, Andrea Madotto, and Pascale Fung.\n2021a. Are multilingual models effective in code-\nswitching? In Proceedings of the Fifth Workshop\non Computational Approaches to Linguistic Code-\nSwitching, pages 142–153.\nGenta Indra Winata, Andrea Madotto, Zhaojiang Lin,\nRosanne Liu, Jason Yosinski, and Pascale Fung.\n2021b. Language models are few-shot multilingual\nlearners. In Proceedings of the 1st Workshop on Mul-\ntilingual Representation Learning, pages 1–15.\nLinting Xue, Noah Constant, Adam Roberts, Mihir Kale,\nRami Al-Rfou, Aditya Siddhant, Aditya Barua, and\nColin Raffel. 2021. mT5: A massively multilingual\npre-trained text-to-text transformer. In Proceedings\nof the 2021 Conference of the North American Chap-\nter of the Association for Computational Linguistics:\nHuman Language Technologies, pages 483–498, On-\nline. Association for Computational Linguistics.\nZheng-Xin Yong, Ruochen Zhang, Jessica Zosa Forde,\nSkyler Wang, Samuel Cahyawijaya, Holy Lovenia,\nGenta Indra Winata, Lintang Sutawika, Jan Christian\nBlaise Cruz, Long Phan, et al. 2023. Prompting\nmultilingual large language models to generate code-\nmixed texts: The case of south east asian languages.\narXiv preprint arXiv:2303.13592.\nRuochen Zhang and Carsten Eickhoff. 2023. Crocosum:\nA benchmark dataset for cross-lingual code-switched\nsummarization. arXiv preprint arXiv:2303.04092.\n12579\nA Fine-tuning Model Setup\nWe use a standard training setup for SA tasks: we\nfine-tune the models for a maximum of 15 epochs\nusing the Adafactor (Shazeer and Stern, 2018) op-\ntimizer with a learning rate of 2e-5. All sequences\nare limited to a maximum sequence length of 256\ntokens, truncating all sequences longer than this\nlength, and dynamically padding the shorter se-\nquences to the longest sequence length in their\nbatch. All setups use a batch size of 128. We\nalso use a linear warmup schedule, warming up for\nthe first 10% of training steps before linearly decay-\ning to 0. We measure Accuracy and Macro F1 as\nmetrics for all setups, loading the best checkpoint\nbased on the F1 score at the end for evaluation.\nWord-Level LID setups use the same one as with\nNLU tasks, except we only train for 3 epochs and\nuse a weight decay of 0.01. Given that one word\nmay be split into multiple tokens during tokeniza-\ntion, we first realign the labels by setting the word\nlabel as the label of its first token, then setting\nthe labels of all succeeding tokens as -100. This\n“dummy” label is then ignored during loss computa-\ntion. We also load the best checkpoint and use the\nsame metrics as with NLU, in addition to Precision\nand Recall.\nFor MT, we fine-tune for a maximum of 10\nepochs using the Adafactor optimizer with a learn-\ning rate of 5e-5, loading the best checkpoint at\nthe end. As the MT0 models are trained with\ninstruction prompts, we also prepend a “prompt”\nto all sequences during fine-tuning in the form of\nTranslate [src] to [tgt]: [sequence]. For\nthe M2M100 and mBART models, we force the\ndecoder’s first token to be the language token of the\ntarget language. All setups use a batch size of 512\nsequences. We also use a similar linear warmup\nschedule as with the SA and LID task setups. For\nMT, we use spBLEU as our performance metric\nand load the best model for evaluation based on it.\nSUM follows most of the same setup that MT\nuses, except we only fine-tune for 3 epochs. For\nMT0, we use Summarize: [sequence] as our\n“prompt” that is prepended to all samples. We\nuse ROUGE (ROUGE1, ROUGE2, ROUGEL, and\nROUGEL-SUM) as our performance metric, load-\ning the best model for evaluation based on it.\nB Fine-tuning Model Results\nMachine Translation\nModel Size BLEU\nHng→Eng Eng →Hng\nFinetuning\nM2M100 418M 28.53 12.40\nM2M100 1.2B 28.55 13.81\nmBART-50 611M 25.50 12.10\nmBART-50O2M 611M 23.40 13.34\nmBART-50M2M 611M 29.53 13.38\nmT0 300M 15.73 7.03\nmT0 580M 24.97 11.88\nmT0 1.2B 31.03 12.87\nmT0prompted 300M 16.66 7.24\nmT0prompted 580M 25.47 12.28\nmT0prompted 1.2B 31.88 13.90\nTable 4: Results for all finetuned models for machine\ntranslation task.\n12580\nC Prompt Templates\nThis section lists all prompts used for our experiment.\nSentiment Analysis\n• [INPUT] => Sentiment:\n• Text: [INPUT] => Sentiment:\n• [INPUT]\nWhat would be the sentiment of the text above?\n• What is the sentiment of this text\nText: [INPUT]\nAnswer:\n• Text: [INPUT]\nPlease classify the sentiment of above text. Sentiment:\nMachine Translation\n• Translate the following text from [SOURCE] to [TARGET].\nText: [INPUT]\nTranslation:\n• [INPUT]\nTranslate the text above from [SOURCE] to [TARGET].\n• Text in [SOURCE]: [INPUT]\nHow would you translate that in [TARGET]?\n• Translate the following [SOURCE] text from to [TARGET].\nText: [INPUT]\nTranslation:\n• Text in [SOURCE]: [INPUT]\nText in [TARGET]:\n[SOURCE] and [TARGET] are Hinglish and English.\nSummarization\n• Summarize the following conversation in English.\nConversation: [INPUT]\nSummary:\n• [INPUT]\nSummarize the above conversation in English:\n• Conversation in [SOURCE]: [INPUT]\nHow would you summarize that in English?\n• Summarize the following [SOURCE] conversation.\nText: [INPUT]\nEnglish summary:\n• Conversation in [SOURCE]: [INPUT]\nSummary in English:\n[SOURCE] is either Hinglish or English.\n12581\nWord-level LID\n• Determine the language for each token in the text below with [ word | tag ].\nUse lang1 for [LANG1], lang2 for [LANG2], and other for others.\n[INPUT]\n• For each token, identify its language (lang1: [LANG1], lang2: [LANG2], other) using [ word | tag ].\n[INPUT] =>\n• Assign language tags to words: lang1 for [LANG1], lang2 for [LANG2], other otherwise.\nFormat: [ word | tag ].\n[INPUT] =>\n• [INPUT]\nCan you tag the language of each word in the sentence above: lang1 ([LANG1]), lang2 ([LANG2]), or\nother using format: [ word | tag ]?\n• [INPUT]\nLabel each word in the text above with its language: lang1 for [LANG1], lang2 for [LANG2], or other.\nFormat: [ word | tag ].\n[LANG1] and [LANG2] are English and Hindi for LID-Hindi-English data, and Modern Standard\nArabic and Egyptian Arabic for LID Standard-Egyptian Arabic data.\nD Detailed Results\nBreakdown results of SA and LID across different languages can be seen in Figure 5 and Figure 6.\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n30\n40\n50\n60\n70\n80\n90Macro F1 (%)\nSentimix Spanish-English\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n30\n40\n50\n60\n70\n80\n90Macro F1 (%)\nMixSentiment Malayaman\nmBERT (FT)\nmDeBERTA (FT)\nXLMR (FT)\nmT0\nBLOOMZ\nXGLM\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n30\n40\n50\n60\n70\n80\n90Macro F1 (%)\nMixSentiment T amil\nFigure 5: LLMs’ sentiment analysis evaluation on (left) Sentimix Spanish-English, (center) MixSentiment\nMalayaman-English, and (right) MixSentiment Tamil-English.\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n0\n20\n40\n60\n80\n100Macro F1 (%)\nLID Hindi-English\nmT0\nXLMR (FT)\nmBERT (FT)\nmDeBERTA (FT)\nBLOOMZ\nXGLM\n300m 560m 1.1b 1.7b 3b 7b 13b\nModel Size\n0\n20\n40\n60\n80\n100Micro F1 (%)\nLID Standard-Egyptian Arabic\nmT0\nXLMR (FT)\nmBERT (FT)\nmDeBERTA (FT)\nBLOOMZ\nXGLM\nFigure 6: LLMs’ word-level LID evaluation result on (left) Hindi-English word-level LID and (right) Standard-\nEgyptian Arabic word-level LID.\n12582"
}