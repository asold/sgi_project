{
  "title": "Technological troubleshooting based on sentence embedding with deep transformers",
  "url": "https://openalex.org/W3171754973",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2543874432",
      "name": "Antonio L. Alfeo",
      "affiliations": [
        "University of Pisa"
      ]
    },
    {
      "id": "https://openalex.org/A1963836767",
      "name": "Mario G. C. A. Cimino",
      "affiliations": [
        "University of Pisa"
      ]
    },
    {
      "id": "https://openalex.org/A1984751118",
      "name": "Gigliola Vaglini",
      "affiliations": [
        "University of Pisa"
      ]
    },
    {
      "id": "https://openalex.org/A2543874432",
      "name": "Antonio L. Alfeo",
      "affiliations": [
        "University of Pisa",
        "Piaggio (Italy)"
      ]
    },
    {
      "id": "https://openalex.org/A1963836767",
      "name": "Mario G. C. A. Cimino",
      "affiliations": [
        "Piaggio (Italy)",
        "University of Pisa"
      ]
    },
    {
      "id": "https://openalex.org/A1984751118",
      "name": "Gigliola Vaglini",
      "affiliations": [
        "Piaggio (Italy)",
        "University of Pisa"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2554211417",
    "https://openalex.org/W2945702027",
    "https://openalex.org/W2997885063",
    "https://openalex.org/W3004016349",
    "https://openalex.org/W3036798749",
    "https://openalex.org/W2469813221",
    "https://openalex.org/W2906152891",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W2891177506",
    "https://openalex.org/W3103305157",
    "https://openalex.org/W1984717569",
    "https://openalex.org/W2469612346",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2922386288",
    "https://openalex.org/W2865169279",
    "https://openalex.org/W1514986335",
    "https://openalex.org/W2548022098",
    "https://openalex.org/W2983766176",
    "https://openalex.org/W2808633496",
    "https://openalex.org/W3105816068",
    "https://openalex.org/W1480493382",
    "https://openalex.org/W3158465536",
    "https://openalex.org/W3008697194",
    "https://openalex.org/W4234424892",
    "https://openalex.org/W2187877333",
    "https://openalex.org/W3036109401",
    "https://openalex.org/W2766284073",
    "https://openalex.org/W3037651423",
    "https://openalex.org/W2989622715",
    "https://openalex.org/W2970641574",
    "https://openalex.org/W2784269499",
    "https://openalex.org/W2151280665",
    "https://openalex.org/W2114131343",
    "https://openalex.org/W2788276261",
    "https://openalex.org/W2394685504",
    "https://openalex.org/W2056589351",
    "https://openalex.org/W2101141209",
    "https://openalex.org/W2962605866",
    "https://openalex.org/W3046371656",
    "https://openalex.org/W2252143362",
    "https://openalex.org/W2716549784",
    "https://openalex.org/W3095238357",
    "https://openalex.org/W3098851962",
    "https://openalex.org/W2595879324",
    "https://openalex.org/W2794557536",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W3012480764",
    "https://openalex.org/W2979413472",
    "https://openalex.org/W2788356385",
    "https://openalex.org/W2962765866"
  ],
  "abstract": null,
  "full_text": "Vol.:(0123456789)1 3\nJournal of Intelligent Manufacturing (2021) 32:1699–1710 \nhttps://doi.org/10.1007/s10845-021-01797-w\nTechnological troubleshooting based on sentence embedding \nwith deep transformers\nAntonio L. Alfeo1  · Mario G. C. A. Cimino1 · Gigliola Vaglini1\nReceived: 28 December 2020 / Accepted: 29 May 2021 / Published online: 7 June 2021 \n© The Author(s) 2021\nAbstract\nIn nowadays manufacturing, each technical assistance operation is digitally tracked. This results in a huge amount of tex-\ntual data that can be exploited as a knowledge base to improve these operations. For instance, an ongoing problem can be \naddressed by retrieving potential solutions among the ones used to cope with similar problems during past operations. To \nbe effective, most of the approaches for semantic textual similarity need to be supported by a structured semantic context \n(e.g. industry-specific ontology), resulting in high development and management costs. We overcome this limitation with a \ntextual similarity approach featuring three functional modules. The data preparation module provides punctuation and stop-\nwords removal, and word lemmatization. The pre-processed sentences undergo the sentence embedding module, based on \nSentence-BERT (Bidirectional Encoder Representations from Transformers) and aimed at transforming the sentences into \nfixed-length vectors. Their cosine similarity is processed by the scoring module to match the expected similarity between \nthe two original sentences. Finally, this similarity measure is employed to retrieve the most suitable recorded solutions for \nthe ongoing problem. The effectiveness of the proposed approach is tested (i) against a state-of-the-art competitor and two \nwell-known textual similarity approaches, and (ii) with two case studies, i.e. private company technical assistance reports \nand a benchmark dataset for semantic textual similarity. With respect to the state-of-the-art, the proposed approach results \nin comparable retrieval performance and significantly lower management cost: 30-min questionnaires are sufficient to obtain \nthe semantic context knowledge to be injected into our textual search engine.\nKeywords Deep learning · Sentence embedding · Textual similarity · Remote technical assistance\nIntroduction\nLately, several industries are transitioning to the smart \nmanufacturing  model by adopting technologies such as \nthe internet of things (IoT), cloud computing, and machine \nlearning to increase their productivity and competitive \nadvantage (Tao et al., 2018). Indeed, machine learning can \nprovide automatic knowledge extraction from manufacturing \nbig data to increase production efficiency, reduce manage-\nment costs (O’Donovan et al., 2015), and drive technologi-\ncal innovation. In this context, the paradigm of Knowledge \nManagement 4.0  (Ansari, 2019 ) emphasizes the business \nvalue creation achieved by extracting and providing accessi-\nbility to manufacturing domain-specific knowledge obtained \nby coupling human experiences and data-driven approaches \n(North et al., 2018). As an example, the data obtained \nthrough IoT devices can be analyzed via a machine learning \napproach to detect production anomalies (Alfeo et al., 2020), \nwhile their management can be supported by considering \npast human-driven maintenance operations to collect best \npractices and improve the maintenance processes (Navin-\nchandran et al., 2021).\nSpecifically, both in-place maintenance operations and \nremote technical assistance are digitally tracked in the form \nof textual reports stored in the ERP system (Usmanij et al., \n2013), containing the details of the performed inspection \nand the adopted solutions for the occurred technical prob-\nlems. Those result from the investigation, experiences, and \nrecommendations of domain-aware technicians. If acces-\nsible and exploitable, such knowledge base can be shared \nand reused to provide effective support for the operators in \ntraining (Costa et al., 2016) and may result in a faster diag-\nnosis and management of machines’ technical problem. The \n * Antonio L. Alfeo \n luca.alfeo@ing.unipi.it\n1 Department of Information Engineering, University of Pisa, \nLargo L. Lazzarino 1, 56127 Pisa, Italy\n1700 Journal of Intelligent Manufacturing (2021) 32:1699–1710\n1 3\nformer can significantly improve production efficiency by \nreducing machines’ downtime. Indeed, the time spent diag-\nnosing the problem and finding a possible solution, is often \nlarger than the time spent fixing it (Sexton et al., 2017).\nTo take advantage of such a knowledge base, it is essen-\ntial having an effective tool to find solutions that are rel-\nevant to a given problem, i.e. adopted with similar problems. \nAccording to (Sunilkumar et al., 2019), the approaches for \ntextual similarity can be organized into four main groups: \n(i) string-based approaches determine the similarity between \ntwo text strings by comparing them as two sequences of \ncharacters and words; (ii) corpus-based approaches find the \nsimilarity based on corpus statistical analysis, e.g. check -\ning words co-occurrence via cosine similarity or n-grams; \n(iii) knowledge-based approaches depend on a handcrafted \nsemantic structure for the specific domain concepts, e.g. the \nshortest path length between the two concepts in a knowl-\nedge graph represents their similarity; (iv) approaches based \non deep sentence embeddings are used to automatically build \nsentences’ representation in a semantic space, in which the \ndistance between two vectors is correlated to the similarity \nof the corresponding sentences.\nStill, there is a lack of applications aimed at retrieving \ntechnical assistance reports (Ansari, 2020), since the effec-\ntiveness of such applications may be easily constrained by (i) \ninconsistencies and inaccuracies in the data due to the infor-\nmal language used by the operators, (ii) the inability to dis-\ntinguish suboptimal solutions proposed by technicians with \nlower expertise, and (iii) the highly unstructured nature of \ntextual reports (Nemeth et al., 2019). This is especially true \nwith the first three groups of approaches. Indeed, to be effec-\ntive, they require handcrafted features for similarity assess-\nment or a structured representation of the domain-specific \nsemantic context (Aarnio et al., 2016). These processes con-\nsist of time-consuming manual activities aimed at collecting \nand organizing the domain knowledge. Moreover, to have an \neffective technical assistance reports (TAR) retrieval, those \nprocedures may be repeated during the design of the search \nengine as well as, every time the semantic context changes, \ni.e. due to the introduction of new services, machines, or \nproduct. This is evident from the business process model \nnotation (BPMN) diagram in Fig.  1, representing the main \nactivities for maintaining a TAR retrieval application.\nIn Fig. 1 each lane corresponds to an actor involved in this \nprocess. The process starts with the data scientist preparing \nthe unstructured textual data to be processed by the search \nengine. Then, it follows a joint activity with the technical \nassistance aimed at defining the features and the semantic \ncontext of the TAR data, on which the search engine will be \nbased. According to the retrieval performance achieved dur-\ning the tests, different rounds of specification of the semantic \ncontext and the features may be required. Once satisfactory \nperformances have been obtained, the search engine can \nsupport all the requests received by technical assistance (the \ncircle with the envelope in Fig.  1). The system operates as \nlong as the proposed solutions are still applicable and up-to-\ndate, then the setting must be repeated.\nTo obtain reliable performances while decreasing the sys-\ntem management costs, we propose a deep learning approach \nto retrieve potential solutions for a given technical problem \nby employing TAR data and no other structured representa-\ntion of the domain-specific semantic context. Specifically, \nFig. 1  BPMN diagram of the maintenance process for a TAR \nretrieval application\n1701Journal of Intelligent Manufacturing (2021) 32:1699–1710 \n1 3\nwe transform the TAR problem descriptions in vectors of \nthe semantic latent space, compute the proximity between \nthose and the vector obtained from a new technical problem, \nand use this similarity score to rank the problems in the \nTAR database and propose the corresponding solutions as \na potential one for the new problem. Our approach employs \na data preparation module to preprocess the textual data. \nThen, the embedding module is used to transform sentences \nof arbitrary length into fixed-length vectors and it is based \non Sentence-BERT (Reimers et al., 2019). Those vectors can \nbe compared via the scoring module, i.e. processing their \ncosine similarity via a multilayer perceptron to match the \nexpected similarity between the two original sentences. The \nsystem has been tested in two distinct case studies: private \ncompany TAR and a benchmark dataset for sentence simi-\nlarity (Cer et al., 2018 ). The effectiveness of the proposed \napproach is compared against one of the best-in-class com-\npetitors, i.e. Universal Sentence Encoder (USE) (Cer et al., \n2018), and two well-known bag-of-words approaches for \nsentences’ similarity assessment. The paper is structured as \nfollows. In second section, we present the literature review. \nIn third section, we detail our approach. Fourth section pre-\nsents the case study. The obtained results are discussed in \nfifth section. Finally, sixth section summarizes conclusions \nand future works.\nState of the art\nThe literature review presented in this section focuses on \nthe effectiveness and management cost of the approaches \nfor TAR retrieval. Indeed, as introduced in Sect.  1, \neffective approaches for TAR retrieval often result in a \nhuge management cost, resulting in limited use of these \napproaches in real-world industrial applications. Such \nmanagement costs are mainly associated with the activities \naimed at injecting the domain-specific semantic context \ninto the search engine. The cost can be considered  low if \nit involves simple activities such as the definition of a \ndomain-specific ontology, a dictionary of terms specific to \nan application domain, or establishing a categorization for \nthe problems in the dataset. Indeed, these activities can be \nperformed through interviews with domain experts, do not \nrequire the modification of the data entry process, or an \nexplicit labeling activity on the existing data, e.g. evaluat-\ning the similarity of problem pairs. The management cost \nincreases (medium cost) if the activity connected to the \ninjection of the semantic context requires a modification \nof the report storage process, e.g. to add metadata about \nthe effectiveness of the solutions or to identify the relevant \nparts of a maintenance report to be the focus of the analy -\nsis. The management cost can be considered  high if the \napproach requires the manual labeling of many problem \npairs in the dataset since this requires a number of human-\ndriven similarity evaluations equal to the square of the \nnumber of problems.\nAccording to (Lan et al., 2018), from a methodologi-\ncal point of view the approaches for TAR retrieval can be \norganized in two main categories: (i) Information Retrieval \n(IR) approaches, retrieve potential solutions according to \nthe degree of similarity in the underlying semantics to \nthe original problem (Kathuria et al., 2016), whereas (ii) \nQuestion Retrieval and Answering (QR) approaches, rank \ncandidate problem–solution pairs according to their signif-\nicance to a given problem (Shtok et al., 2012). The effec-\ntiveness of both IR and QR approaches can be measured \nby considering how many of the highly ranked retrieved \nsolutions are useful or relevant to the problem described. \nTo this aim, the most used measures are the Mean Recipro-\ncal Rank (MRR) and the Mean Average Precision (MAP) \n(Metzler et al., 2005). MAP considers the relevant solu-\ntions i according to their position r i in the ranking R  for \nthe query q . The higher the rank of a relevant solution, \nthe more it contributes to the score computation. MRR \nconsiders only the highest position of relevant solutions in \nthe rank R  for query q . Both these measures are between \n0 and 1 (the higher the better).\nA clear example in which different management costs \nresult in different retrieval performances is (Guo et al., \n2019). The authors compare the effectiveness of twenty-\nthree different approaches for textual similarity ranking \nbased on neural networks, resulting in (i) the best per -\nformances among the QR approaches (MAP 0.77), and \n(ii) an average performance (MAP 0.502) among the IR \nones (Table  1). However, these results are associated \nwith different management costs. Specifically, the best \nQR approach is based on a fast and lightweight sentence \nembedding deep learning architecture that requires a large \nnumber of sentence pairs with their corresponding degree \nof similarity to be trained, thus resulting in a high manage-\nment cost. On the other hand, the IR approach needs much \nfewer labeled sentence pairs and proves experimentally \nhow it can be trained on the data of a given year and reused \nwith the data of the next year.\nA more effective (MAP 0.609) IR approach with an \neven lower management cost is (Heilman et al., 2010), in \nwhich the authors represent each textual edit operation \nMAP = 1\n�Q �\n�Q��\nq=1\n∑�Rq�\ni=1\ni\nri\n�Rq�\nMRR = 1\n/uni007C.varQ/uni007C.var\n/uni007C.varQ/uni007C.var/uni2211.s1\nq=1\n1\nmax/parenleft.s1Rq\n/parenright.s1\n1702 Journal of Intelligent Manufacturing (2021) 32:1699–1710\n1 3\nas a node in a sequence of transformations, the minimum \nlength of the sequence to make two sentences semanti-\ncally identical determines the measure of similarity of \nthe sentences. This method requires the design of a set \nof general-purpose edit operations and domain-specific \nconstraints for their sequence, e.g. which edits cannot be \nused one after the other. Defining this set of transforma-\ntions and constraints requires an analysis of the semantic \ndomain of the case study, no explicit labeling activity has \nto be performed thus resulting in a low management cost.\nAuthors in (Tong et al., 2015) propose an IR approach for \ntroubleshooting retrieval based on word co-occurrence and \ndomain-specific categories. Given a problem query and cat-\negories, the troubleshooting search system can retrieve the \nrelevant information of interest to the selected categories, \nresulting in a MAP equal to 0.571. This approach requires \n(i) metadata about the broken asset’s component to be used \nas domain-specific categories, (ii) a mapping describing the \nsemantic relationship between words and categories, as well \nas the relationship between categories, and (iii) the manual \nlabeling of the important parts of each maintenance report, \nresulting in a medium management cost.\nIn (Gupta et al., 2018) the authors propose a replicated \nSiamese Long Short-Term Memory (LSTM) to evaluate the \nsimilarity between asymmetric text pairs. This IR approach \nis used with an industrial ticketing system to retrieve a rel-\nevant solution for an input query using the tickets’ knowl-\nedge base, resulting in a MAP equal to 0.4. Moreover, this \napproach implies a labeling campaign resulting in 421 pairs \nof sentences with their degree of similarity, i.e. a manage-\nment cost ranging from medium to high.\nAuthors in (Zhou et al., 2015) employ a look-up table \nto transforms each word into a vector, then aggregate and \nprocess them via an embedding procedure. This procedure \nis constrained to obtain fixed size vectors from different \nlength sentences while matching a given sentences’ catego-\nrization. The distance between the sentences’ embeddings \nin the latent space is used to assess their similarity. This \nIR approach is quite effective (MAP 0.69), yet it employs \ndata consisting of thousands of pair query-solution manu-\nally labeled as relevant or not relevant. Given the required \n(i) definition of the look-up table and the categories, and \n(ii) manual labeling activity, the management cost of this \napproach may be considered high.\nIn (Pang et al., 2017) authors propose an IR approach \nbased on a deep learning architecture to assess the semantic \nmatching of textual data, resulting in a MAP equal to 0.49. \nThis approach demands (i) the definition of weights to rep-\nresent the importance of words in the query, and (ii) more \nthan one hundred thousand labeled query-document pairs, \nresulting in a high management cost.\nMany QR approaches have been extensively used in the \nfield of community question answering (cQA), in which \nusers ask their web community how to address a specific \ntechnical issue they are experiencing. To reduce the number \nof unanswered questions, the potential answer can be auto-\nmatically retrieved by employing past similar queries (Guo \net al., 2019; Zhou et al., 2015). Even if the domain is slightly \nTable 1  Best MAP and MRR \nperformances reported in the \nresearch works presented in \nSect. 2, together with their \nmanagement cost (L = low, \nM = medium, H = high) and the \ncost’s motivations in terms of \nsystem requirements\nSHORT REF IR/QR MAP MRR COST REQUIREMENTS\nGuo, 2019 QR 0.770 – H · Very large number of labeled pairs\nChahuara, 2016 QR 0.750 – M/H · Domain–specific topics\n· Topic–specific dictionary\n· Large number of labeled pairs\nLan, 2018 QR 0.739 0.795 H · Very large number of labeled pairs\nBaldwin, 2016 QR 0.702 0.8 H · Metadata about solutions’ relevance\n· Large number of labeled pairs\nZhou, 2015 IR 0.690 – H · Words–vector look–up table\n·Domain–specific categories\n·Large number of labeled pairs\nHeilman, 2010 IR 0.609 0.692 L · Edit operations and constraints\nOthman, 2020 QR 0.579 – H · Very large number of labeled pairs\nOthman, 2019 QR 0.574 – H · Very large number of labeled pairs\nTong, 2015 IR 0.571 0.643 M · Domain–specific categories\n· Words–categories semantic mapping\n· Selecting documents’ relevant parts\nDas, 2016 QR 0.532 0.574 M/H · Solutions’ effectiveness metadata\n· Large number of labeled pairs\nGuo, 2019 IR 0.502 – M/H · Large number of labeled pairs\nPang, 2017 IR 0.497 – H · Selecting important words in the query\n· Very large number of labeled pairs\nGupta, 2018 IR 0.400 0.287 M/H · Large number of labeled pairs\n1703Journal of Intelligent Manufacturing (2021) 32:1699–1710 \n1 3\ndifferent, the technology employed works on the same \nassumptions and for similar aims, thus, can be exploited in \nthe context of industry 4.0 for retrieving the most suitable \ntechnical assistance report with respect to a given problem.\nThe QR approach presented in (Chahuara et al., 2016) \nranks similar questions from question–answer archives dif-\nferentiating them by topics to cope with the different vocabu-\nlaries used within questions of different topics. Specifically, \neach topic is associated with a particular dictionary, and a \nsemantic mapping is established between different topics \nand between pairs of sentences of specific topics. Although \nthe number of sentence pairs required is not large and the \nmapping can also be partially automated by leveraging sta-\ntistical approaches, the number of steps required to employ \nthis approach may correspond to a medium/high manage-\nment cost. On the other hand, this management cost allows \nachieving very good effectiveness, i.e. MAP equal to 0.75.\nAuthors in (Das et al., 2016) retrieve questions that are \nsimilar to a given query, via a QR approach leveraging the \ndistance between the query and its topic in the latent vector \nspace. To train the model one hundred queries are used. For \neach of these 20 plausible solutions are provided, together \nwith the metadata on the effectiveness of each solution and \nits topic. The introduction of this metadata in a business \nprocess has a very low cost, but this does not apply to the \nselection of plausible solutions for a hundred queries, result-\ning in an overall medium/high management cost.\nIn (Baldwin et al., 2016) the authors retrieve similar \nquestions, despite their difference in length, by employing a \nconvolutional neural network combined with a Naive-Bayes \nclassifier and a support vector machine each trained over \nlexical similarity features. This effective (MAP 0.702) QR \napproach employs the metadata about the relevance of the \nproposed solutions and requires a manual labeling process \nto obtain the similarity query-solution pairs, resulting in a \nhigh management cost.\nThe same management cost is required by the best \nperforming method tested in (Lan et al. 2018), i.e. a QR \napproach based on deep learning and tested on eight publicly \navailable datasets consisting of a large number of sentence \npairs and their similarity.\nIn (Othman et al., 2019) authors use word embeddings \nto capture semantic and syntactic information from textual \ncontexts and vectorize the questions. The embedding vec-\ntors feed a Siamese LSTM neural network, and the simi-\nlarity between the questions is obtained as the Manhattan \ndistance of the final LSTM hidden states. In this study, 1624 \nsentences and 256 queries are used. Human annotators are \nemployed to evaluate and find 2 to 30 relevant solutions for \neach query, resulting in a high management cost. This QR \napproach is extended in (Othman et al., 2020) by enhancing \nthe neural network architecture with an attention mechanism \nto determine which words in the questions should receive \nmore attention during the embedding phase. Again, the \napproach requires a large number of manually labeled pairs \nof queries and relevant sentences, about 30% more than the \nprevious study.\nTable 1 provides an overview of the best MAP and MRR \nperformances reported in the research works presented \nin this section together with their management cost and \nrequirements.\nAs summarized in Table  1, there is a lack of effective \ntextual similarity approaches characterized by low man-\nagement costs. For this reason, many of the TAR retrieval \nsolutions in the literature are more of a proof of concept \nrather than real-world applications (Ansari, 2020). One of \nthe few examples of applications can be found in (Wang, \n2010), in which the authors describe an intelligent semantic \nlabeler to retrieve a potential solution for a new problem \nby exploiting past problem–solution pairs. In (Sipos et al., \n2014; Xu et al., 2020) natural language processing (NLP) \napproaches are used to cluster maintenance reports and \nprovide information retrieval support for maintenance tech-\nnicians. Despite the lack of many real-world applications, \nsome of them have proven to be highly cost-effective. As an \nexample, the authors (Ray et al., 2020) provide a QR appli-\ncation able to distinguish symptoms, activities, actions, and \nadvises of problem–solution pairs. The authors report that \nsuch an approach reduced the time to response of technical \nassistance service by 29%, leading to an overall cost sav -\nings of 25% per year. This confirms the great potential of \nthese applications, whose availability, however, is strongly \nlimited by a hard-to-reach trade-off between effectiveness \nand management cost. To reduce the management cost, it \nmay be useful an approach based on the most recent sen-\ntence embedding techniques. Sentence embedding can \ntransform arbitrary long sentences into fixed-sized vectors \nwhose distance is correlated to the similarity of the origi-\nnal sentences, resulting in a simple and effective manner to \nrepresent textual semantic similarity (Passaro et al., 2020). \nAn example, authors in (Khabiri et al., 2019) classify main-\ntenance reports by exploiting an industry-specific taxonomy, \nextracting reports’ corpus, and obtaining an embedding from \neach document. Also, consider that both the most effective \nIR and QR approaches shown in Table  1 (Guo et al., 2019; \nZhou et al., 2015) are obtained with approaches based on \nsentence embedding, even if those correspond to high man-\nagement costs.\nIn contrast with those, the recent sentence embedding \ntechnique (Reimers et al., 2019) is pre-trained on a huge \namount of publicly available data, thus does not require (i) \nmany labeled sentence pairs to learn how to distinguish simi-\nlar and different sentences in any semantic context, or (ii) \nany structured context representation such as ontologies or \ntaxonomy graphs. In the next section, we present an effective \nTAR information retrieval real-world application employing \n1704 Journal of Intelligent Manufacturing (2021) 32:1699–1710\n1 3\nsuch sentence embedding technique and characterized by \nlow management costs.\nArchitectural design of the sentence \nsimilarity model\nIn this section, we present the design of the proposed \napproach. Considering the approaches presented in the last \nsection, the proposed architecture relies on deep transform-\ners for sentence embedding to avoid the explicit modeling \nof the application’s semantic context. Moreover, the lat -\nest literature provides different pre-trained models that are \nextremely useful in the absence of large training datasets, \nas it frequently happens in real-world manufacturing. It is \nimpossible to effectively train the whole NLP model with a \nlimited amount of data, whereas those data can be effectively \nused to finetune a pre-trained model.\nThe proposed architecture consists of a data prepara-\ntion module, a sentence embedding module, and a scoring \nmodule (Fig.  2). Firstly, the data preparation module pro-\nvides punctuation removal, stop-words removal, and word \nlemmatization. With stop-words removal, each sentence is \ntransformed in a list of individual words, and the ones which \nare not adding meaning to the sentence are removed, e.g. ‘a’, \n‘the’. With words lemmatization, each word is reduced to its \ncommon base form, e.g. from ‘studies’ to ‘study’ (Vijaya -\nrani et al., 2015). The pre-processed sentences undergo \nthe embedding module, which is in charge of transforming \neach sentence into a fixed-length vector. This transforma -\ntion should preserve the semantic relationships between the \nsentences, i.e. the distance between two vectors should be \ncorrelated to the semantic dissimilarity between the original \nsentences.\nThe sentence embedding module employs Sentence-\nBERT (Reimers et al., 2019), a state-of-the-art sentence \nembeddings technique pre-trained with more than 570,000 \nsentence pairs (Bowman et al., 2015). S-BERT is an exten-\nsion of BERT specifically designed for assessing the seman-\ntic similarity between sentences. BERT (Devlin et al., 2018) \nis an NLP transformer-based machine learning technique \ndeveloped by Google that learns contextual relations \nbetween words (or sub-words) in a text. BERT is pre-trained \non more than 3.3 billion words obtained via different web \nsources, e.g., the English Wikipedia. To measure the similar-\nity between two sentences using BERT, two sentences have \nto be concatenated using a particular token (a separator char-\nacter) and processed via BERT as a whole. This makes the \ncomparison impractical in real-world information retrieval \napplications, e.g. to find the two most similar sentences in a \ngiven set of n sentences would require n(n-1)/2 operations. \nS-BERT extends BERT to handle this limitation. By add-\ning a pooling operation to the output of BERT, S-BERT \nproduces a fixed-length (1024) embedding vector for each \nsentence regardless of its length, resulting in a number of \noperations equal to the number of sentences analyzed. The \nembeddings resulting from two sentences will be spatially \nclose (distant) if the corresponding sentences are seman -\ntically similar (different). Thus, the comparison can then \nbe obtained via standard and efficient distance-based meas-\nures such as the cosine similarity ( Δ in Fig.  2) computed \nbetween the embeddings of two sentences. As an example, \nin (Reimers et al. 2019) the authors employ the pair-wise \nsimilarity between 10,000 sentences for a clustering proce-\ndure: it results in 65 h execution time with BERT, and 5 s \nwith S-BERT.\nIt follows the formulae of the cosine similarity, a bounded \nmeasure of similarity between two non-zero vectors. It is \ndefined as the ratio between the dot product and the mag-\nnitude of those vectors, to equal the cosine of the angle \nbetween them.\nFig. 2  The architecture of the proposed sentence similarity model\n1705Journal of Intelligent Manufacturing (2021) 32:1699–1710 \n1 3\nAs explained in its documentation, S-BERT model can \nbe specialized for a specific semantic context by means of \nits specific fine-tuning procedure, by using a set of sentence \npairs together with their degree of similarity.\nThe proximity between the embeddings may not always \neffectively match the similarity between the original sen-\ntences, since the embeddings generated by BERT-based \nmodels tend to occupy a narrow cone in the vector space \n(Li et al., 2020). To have a more effective mapping between \nthe proximity between embeddings and the expected simi-\nlarity between sentences, we introduce the scoring module. \nThe scoring module consists of a multilayer perceptron \n(MLP) aimed at processing the cosine similarity between \ntwo embedding vectors to match the expected similarity \nbetween the two original sentences.\nThe scoring module is trained using embedding pairs \ntogether with the expected similarity of the original sen-\ntences. Once trained, the scoring output is considered as the \nsimilarity score between two sentences.\nIndustrial application and case study\nOur case study consists of the analysis of TAR of real-world \nmanufacturing company that produces and sells machinery. \nIn case of a technical issue with the machinery, the customer \ncan request for remote technical assistance, whose process \nis represented in Fig.  3. During this process, a technical \nassistance operator oversees and supports the maintenance \noperations via a dedicated video communication channel. \nThe operator records the motivation of the call, the remotely \nobserved condition of the machinery, as well as the proposed \nsolutions. Those can be searched manually or via a search \nengine. If no solution results effective, the company sched-\nules an on-site maintenance activity. The collected informa-\ntion is stored as a digital report in the company’s database. \nFor each report in the database, we extract the call times-\ntamp, the customer ID, the call ID, if the call was followed \nby a physical inspection, and three textual descriptions \naddressing the motivation for the call, the remote inspec-\ntion, and the proposed solution. We account for the issues \naffecting the textual analysis with TAR data introduced in \nSect. 1. Then, we account for inconsistencies and inaccura-\ncies in the text by focusing on pairs of problem descriptions \nand proposed solutions, both provided by the remote techni-\ncal assistance operator. Indeed, most of the call motivations \ncontain a very generic description of the actual problem, \ne.g. ‘the machine stopped due to overheating’. This is also \nconfirmed by looking at the average number of words used \nfor each textual description: 23.3 for the call motivation, and \nΔA,B = eA ⋅ eB\n∥ eA ∥ ⋅ ∥ eB ∥\n35.7 for the problem descriptions. We also account for the \ninability to distinguish suboptimal solutions by selecting the \ninstances that did not lead to a physical inspection, assuring \nto keep only the problem–solution pairs in which the solu-\ntion proposed was effective.\nThe resulting TAR dataset is made of 308 problem–solu-\ntion pairs. We build a training set made of pairs of problem \ndescriptions and their degree of similarity. This is obtained \nwith the support of the domain experts of the company, \ni.e. the remote technical assistance operators. We selected \nsome representative technical problems, to cover the main \ntopics in the whole set of problem descriptions. To enable \nthe system to distinguish different degrees of similarity, for \neach representative problem (RTP), 5 other problems have \nbeen chosen. Those are characterized by different degrees \nof similarity with respect to the RTP. Specifically, two were \nchosen to be similar to the RTP, two different from the RTP, \nand one problem was chosen at random. In this context, two \nproblems are considered similar (different) if they require to \nperform similar (different) technical operations on the same \n(other) asset component to be fixed. We asked three techni-\ncal assistance operators to rank those 5 problems according \nFig. 3  BPMN diagram of the remote technical assistance process\n1706 Journal of Intelligent Manufacturing (2021) 32:1699–1710\n1 3\nto the similarity with the latter. The obtained ranks for each \nproblem pair were averaged and normalized resulting in a \nsimilarity score between 1 (very similar problem) and 0 \n(completely different problem). Finally, this set of problem \npairs was treated with a data augmentation procedure lev -\neraging a synonyms dictionary. The synonyms dictionary \nprovides up to 4 synonyms for 30 different domain-specific \ntechnical terms. If a problem description contains one or \nmore of those terms, each combination of their possible syn-\nonyms generates a new problem description while preserv -\ning the overall sentence’s meaning. The final TAR training \nset is composed of 2440 pairs of problems and their similar-\nity score (examples in Table 2).\nTo validate the results of our approach we also need to \nknow which solution is relevant for a given problem. We \nbuild a solution-relevance dataset by collecting 10 repre -\nsentative problems descriptions, covering the main topics \nin the TAR dataset. For each one of those, we rank the most \nsuitable 5 potential solutions. We asked 8 remote assistance \ntechnicians to label each solution as \"Relevant\" or \"Not rel-\nevant\" with respect to the problem. The solution-relevance \ndataset can be used to evaluate the effectiveness of our appli-\ncation by using two well-known performance measures: \nMAP and MRR.\nA given retrieval approach may provide solutions not \nincluded in the solution-relevance dataset. We account for \nthose unlabeled solutions by evaluating how them impact \nMAP and MRR according to the probability (P) that an unla-\nbeled solution is relevant for the problem described. Spe-\ncifically, we consider the probabilities of 0, 0.25 and 0.5. \nWorst case scenario: if an unlabeled solution is retrieved, \nit is considered not relevant. Best case scenario: if an unla-\nbeled solution is retrieved, it is considered relevant with a \n50% probability.\nIn our application, the activity aimed at building the \ntraining set was performed by submitting a 30-min ques -\ntionnaire to the technical assistance technicians. The same \namount of time was required to label the solution-relevance \ndataset, and to collect the domain-specific synonyms for \nthe data augmentation. These activities aim at including the \ndomain-specific knowledge in the system by replacing the \nsecond and third activities shown in the BPMN diagram in \nFig. 1, thus resulting in reduced management costs of such \napplication.\nBoth the embedding module and the scoring module can \nbe fine-tuned or trained with the TAR training set described \nabove (example in Table  2). Specifically, those modules \nare trained sequentially, allowing the sentence embedding \nmodule to generate the embedding vectors used to train the \nscoring module. The expected similarity of those vectors \ncorresponds to the similarity of the original sentences. Once \nthe system is trained, a new sentence (i.e. a new problem \ndescription p) can be pre-processed by the data preparation \nmodule and transformed into an embedding vector  ep. Then, \nthe cosine similarity between  ep and the embeddings corre-\nsponding to each pre-processed problem description stored \nin the TAR database can be computed and processed by the \nscoring module. The obtained degrees of similarity can be \nused to rank the problems in the database. The solutions cor-\nresponding to the highest-ranking problems can be proposed \nas possible solutions for the problem described via p.\nExperimental results\nIn this section, we present our experimental results. Each \nresult is obtained with 10 repeated trials and presented as \na 99% confidence interval. Each repetition is performed by \nrandomly sampling 70% of the dataset as training set and the \nremaining 30% as testing set. The hardware platform used \nfor our experiments employs an AMD EPYC CPU (8 cores, \n16 Threads, 2195 MHz), 23 Gigabyte RAM, and an NVIDIA \nTesla T4 GPU.\nTo test our approach beyond the industrial case study \ncorresponding to the TAR dataset, we also employ the \n2012–2017 ‘SemEval SEM STS’ dataset (Cer et al., 2018), \na well-known benchmark dataset for semantic textual simi-\nlarity tasks (Belinkov et al., 2019; Ranasinghe et al., 2019). \nThe STS dataset consists of 8628 sentence pairs including \ncaptions, news, and forum posts. Each pair corresponds to a \nhuman-labeled degree of similarity.\nEach architecture module is built in Python, by using \nwell known machine learning libraries, e.g. sklearn and \ntensorflow. The embedding module leverages the publicly \navailable1 source code of S-BERT, i.e. the pre-trained ver -\nsion bert-large-nli-stsb-mean-tokens. In our experiments, \nTable 2  Examples of problem \npairs’ in the training set Problems description Similarity\n“The customer cannot restart the line, the embosser nip roll axis goes in alarm state.” 0.25\n“The customer complains about the simotion of the tail sealer, they can’t connect to it.”\n“The log saw is not running even if the operator press the orange push button.” 0.917\n“The log saw is in stop state and is not possible restart the machine.”\n1 https:// github. com/ UKPLab/ sente nce- trans forme rs.\n1707Journal of Intelligent Manufacturing (2021) 32:1699–1710 \n1 3\nthe scoring module is based on a MLP with four feedfor -\nward hidden layers consisting of 50, 40, 10, 30 neurons, \nrespectively. As a loss function for the MLP we use the mean \nsquare error (MSE), as neurons’ activation function we use \nrelu (rectified linear unit), and as optimization strategy we \nuse adam due to its computational efficiency and little mem-\nory requirements (Zhu et al., 2017).\nFirstly, we test the ability of the embedding module to \nadapt to a specific domain context according to the num-\nber of training epochs. For this test, we use the Semantic \nTextual Similarity (STS) dataset, due to its topics variety. \nThe embedding module is fine-tuned using a number of \nepochs equal to 1, 5, 10, 25 and 100. The scoring mod -\nule is trained with an early stop strategy. Figure  4 presents \nthe confidence interval at 99% of the MSE obtained with \nthese trials. Clearly, the confidence intervals of the MSE \nobtained with less than 25 epochs are consistently higher. On \nthe other hand, there is no significant difference in terms of \nperformances between 25 and 100 epochs. This suggests that \n100 training epochs for finetuning the embedding module are \nsufficient to provide good performances, especially for an \nassessment task simpler than STS such as the TAR dataset \npresented in Sect. 4, which has fewer samples, and a smaller \ntopics variety.\nBy using 100 fine-tuning epochs, we compare the effec-\ntiveness of the proposed approach against the one obtained \nwith a differently implemented sentence embedding mod-\nules. Specifically, we replace S-BERT with USE (Cer et al., \n2018) and employ a well-known bag-of-words approaches, \nsuch as the cosine similarity and BM25 (Wijewickrema \net al., 2019). USE represents the state of the art for sen-\ntence embedding approaches based on transformers (Ahmed \net al., 2019). The employed version of USE is publicly avail-\nable on TensorFlow Hub. 2 It can transform sentences into \n512-dimensional vectors, and it is pre-trained on a very large \nset of textual data including sources like Wikipedia, web \nnews, web question–answer pages, and discussion forums. \nTable 3 shows how S-BERT provides a lower MSE with \nboth STS and TAR datasets. Moreover, by considering the \nresults shown in Fig. 4, the embedding module with S-BERT \nprovides better performances than the other approaches, \neven with 1 fine-tuning epoch.\nFinally, we validate the effectiveness of the proposed \napproach by employing the solution-relevance dataset \ndescribed in Sect.  4. We compute MAP and MRR by con-\nsidering the top 5 solutions provided by our approach, the \napproach based on USE, and two bag of word approaches \nbased on cosine similarity and BM25. Tables  4 and 5 show \nthe 99% confidence intervals of the resulting MAP and MRR \nFig. 4  MSE of the similarity assessment with STS dataset according \nto the number of fine-tuning epochs\nTable 3  MSE obtained with STS and TAR dataset by employing \nretrieval approaches based on different technologies\nSTS TAR \nCosine 0.970 ± 0.004 0.059 ± 0.001\nBM25 1.105 ± 0.013 0.055 ± 0.003\nUSE 0.827 ± 0.033 0.059 ± 0.003\nS-BERT 0.224 ± 0.024 0.0003 ± 0.0001\nTable 4  MAP@5 obtained with TAR dataset via different retrieval \napproaches\nP = 0 P = .25 P = .50\nCosine 0.300 ± 0.063 0.508 ± 0.067 0.648 ± 0.057\nBM25 0.288 ± 0.070 0.475 ± 0.075 0.637 ± 0.061\nUSE 0.383 ± 0.001 0.533 ± 0.019 0.667 ± 0.019\nS-BERT 0.706 ± 0.027 0.731 ± 0.016 0.760 ± 0.011\nTable 5  MRR@5 obtained with TAR dataset via different retrieval \napproaches\nP = 0 P = .25 P = .50\nCosine 0.290 ± 0.061 0.556 ± 0.094 0.706 ± 0.093\nBM25 0.285 ± 0.068 0.529 ± 0.068 0.704 ± 0.091\nUSE 0.388 ± 0.001 0.556 ± 0.018 0.687 ± 0.025\nS-BERT 0.750 ± 0.032 0.791 ± 0.025 0.823 ± 0.016\n2 https:// www. tenso rflow. org/ hub.\n1708 Journal of Intelligent Manufacturing (2021) 32:1699–1710\n1 3\nby considering 0, 0.25 and 0.5 as the probability (P ) that \nan unlabeled solution is relevant for the problem described.\nAccording to Tables  4 and 5, employing an embedding \nmodule based on S-BERT results in similar performances \nto the top ones reported in Sect.  2 but does not require any \nformalization of the application-specific semantic context.\nIf compared against the approaches based on bag-of-\nwords methods (i.e. cosine similarity and BM25) and USE, \nthe proposed approach results in a greater MAP and MRR. \nFinally, by considering the variability of MAP and MRR \naccording to P , it is evident that the proposed approach \nresults in a smaller number of unlabeled solutions among \ndifferent trials.\nConclusions\nIn this work, we presented an application aimed at retrieving \npossible solutions for new problems by searching for simi-\nlar problem stored in remote technical assistance reports. \nDespite the wide adoption of remote technical assistance \nservice and the strategic advantages that this analysis can \nprovide, there is a lack of such applications in the litera-\nture. Indeed, most of these approaches need the support of \na structured semantic context to be effective, resulting in a \nhuge management cost.\nThe application presented in this work overcomes this \nissue by adopting an architecture based on (i) a data prepara-\ntion module aimed at providing punctuation and stop word \nremoval, and word lemmatization, (ii) a sentence embed -\nding module based on Sentence-BERT, and (iii) a scoring \nmodule, aimed at processing the distance between sentences’ \nembeddings to produce a similarity score.\nThe obtained results show that: (i) the proposed approach \nprovides better retrieval performances than the two well-\nknown methods for information retrieval and USE, (ii) this \noccurs even with less than ten finetuning epochs, and (iii) \nthe proposed approach corresponds to the lowest fluctuation \nwhen varying the probability that a new solution is effective, \nthus producing more consistent results for similar queries \nacross different trials. The performances obtained with the \nproposed approach are higher than the best IR one presented \nin Sect.  2, yet it requires a very low management cost to \ninject the domain-specific semantic context into the search \nengine. Indeed, the effort needed is aimed at collecting a \nfew labeled pairs of sentences to finetune the search engine, \nrather than the large number of pairs that would be needed \nto train it from scratch.\nGiven the encouraging results, we aim at (i) improving \nthe retrieval performances by employing a classifier based \non deep learning, and (ii) including more heterogeneous \ntextual data to study under what circumstances (e.g. topic \ndiversity) the proposed approach requires retraining.\nAcknowledgements The authors thank the Körber Lab and the Cus-\ntomer Service Team of the Fabio Perini S.p.A. The former for the \ncontribution as domain experts, the latter for supporting the validation \nof the proposed approach. The authors thank Lorenzo Nannini for his \nwork on the subject during his master thesis.\nFunding Open access funding provided by Università di Pisa within \nthe CRUI-CARE Agreement. This work is partially supported by (i) \nthe company Fabio Perini S.p.A—Körber Tissue in the project “Intel-\nligent Data Analysis algorithms for monitoring mono and multi-agent \nsystems: Digital Applications in the Tissue Industry”, and by (ii) the \nItalian Ministry of Education and Research (MIUR) in the framework \nof the CrossLab project (Departments of Excellence).\nDeclarations \nConflict of interest The authors, as well as the industrial and academic \npartners involved in this research, declare that they have no interests \nrelated to the work submitted for publication, nor directly or indirectly.\nAvailability of code, data and material Unavailable due to the industrial \npartner privacy policy.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\nAarnio, P., Vyatkin, V., and Hästbacka, D. (2016). Context modeling \nwith situation rules for industrial maintenance. In 2016 IEEE 21st \nInternational Conference on Emerging Technologies and Factory \nAutomation (ETFA) (pp. 1–9). IEEE.\nAhmed, M., & Mercer, R. E. (2019). Efficient Transformer-Based Sen-\ntence Encoding for Sentence Pair Modelling. Canadian Confer -\nence on Artificial Intelligence (pp. 146–159). Cham: Springer.\nAnsari, F. (2019). Knowledge management 4.0: Theoretical and practi-\ncal considerations in cyber physical production systems. IFAC-\nPapersOnLine, 52(13), 1597–1602.\nAnsari, F. (2020). Cost-based text understanding to improve mainte-\nnance knowledge intelligence in manufacturing enterprises. Com-\nputers and Industrial Engineering, 141, 106319.\nAlfeo, A. L., Cimino, M. G., Manco, G., Ritacco, E., & Vaglini, G. \n(2020). Using an autoencoder in the design of an anomaly detector \nfor smart manufacturing.  Pattern Recognition Letters, 136, 272-\n278 ISSN 0167–8655\nBaldwin, T., Liang, H., Salehi, B., Hoogeveen, D., Li, Y., and Duong, \nL. (2016, June). UniMelb at SemEval-2016 Task 3: Identifying \nsimilar questions by combining a CNN with string similarity \n1709Journal of Intelligent Manufacturing (2021) 32:1699–1710 \n1 3\nmeasures. In Proceedings of the 10th International Workshop on \nSemantic Evaluation (SemEval-2016) (pp. 851–856).\nBelinkov, Y., & Glass, J. (2019). Analysis methods in neural language \nprocessing: A survey. Transactions of the Association for Com-\nputational Linguistics, 7, 49–72.\nBowman, S. R., Angeli, G., Potts, C., and Manning, C. D. (2015) A \nlarge, annotated corpus for learning natural language inference. \nIn Proceedings of the 2015 Conference on Empirical Methods in \nNatural Language Processing, pages 632–642, Lisbon, Portugal. \nAssociation for Computational Linguistics.\nCer, D., Diab, M., Agirre, E., Lopez-Gazpio, I., & Specia, L. (2017). \nSemeval-2017 task 1: Semantic textual similarity-multilingual and \ncross-lingual focused evaluation. Proceedings of the 10th Interna-\ntional Workshop on Semantic Evaluation (SemEval 2017). Avail-\nable at: http:// ixa2. si. ehu. eus/ stswi ki\nCer, D., Yang, Y., Kong, S. Y., Hua, N., Limtiaco, N., John, R. S., and \nSung, Y. H. (2018). Universal sentence encoder. arXiv preprint \narXiv: 1803. 11175.\nChahuara, P., Lampert, T., & Gancarski, P. (2016). Retrieving and \nranking similar questions from question-answer archives using \ntopic modelling and topic distribution regression. International \nConference on Theory and Practice of Digital Libraries  (pp. \n41–53). Cham: Springer.\nCosta, R., Lima, C., Sarraipa, J., & Jardim-Gonçalves, R. (2016). \nFacilitating knowledge sharing and reuse in building and con-\nstruction domain: An ontology-based approach. Journal of \nIntelligent Manufacturing, 27(1), 263–282.\nDas, A., Shrivastava, M., & Chinnakotla, M. (2016). Mirror on the \nwall: Finding similar questions with deep structured topic mod-\neling. Pacific-Asia Conference on Knowledge Discovery and \nData Mining (pp. 454–465). Cham: Springer.\nDevlin, J., Chang, M., Lee, K., Toutanova, K., (2018). \"BERT: \nPre-training of Deep Bidirectional Transformers for Language \nUnderstanding\". arXiv: 1810. 04805 v2\nGuo, J., Fan, Y., Pang, L., Yang, L., Ai, Q., Zamani, H., & Cheng, \nX. (2019). A deep look into neural ranking models for informa-\ntion retrieval. Information Processing and Management, 57(6), \n102067.\nGupta, P., Andrassy, B., & Schütze, H. (2018). Replicated siamese \nLSTM in ticketing system for similarity learning and retrieval \nin asymmetric texts. In  Proceedings of the Third Workshop on \nSemantic Deep Learning (pp. 1–11).\nHeilman, M., and Smith, N. A. (2010). Tree edit models for recogniz-\ning textual entailments, paraphrases, and answers to questions. \nIn Human Language Technologies: The 2010 Annual Confer -\nence of the North American Chapter of the Association for \nComputational Linguistics (pp. 1011–1019).\nKathuria, M., Nagpal, C. K., and Duhan, N. (2016). A survey of \nsemantic similarity measuring techniques for information \nretrieval. In 2016 3rd International Conference on Comput-\ning for Sustainable Global Development  (INDIACom) (pp. \n3435–3440). IEEE.\nKhabiri, E., Gifford, W. M., Vinzamuri, B., Patel, D., and Mazzoleni, \nP. (2019). Industry Specific Word Embedding and its Application \nin Log Classification. In Proceedings of the 28th ACM Interna-\ntional Conference on Information and Knowledge Management  \n(pp. 2713–2721).\nLan, W., and Xu, W. (2018). Neural network models for paraphrase \nidentification, semantic textual similarity, natural language infer-\nence, and question answering. In Proceedings of the 27th Interna-\ntional Conference on Computational Linguistics (pp. 3890–3902).\nLi, B., Zhou, H., He, J., Wang, M., Yang, Y., & Li, L. (2020). On the \nSentence Embeddings from BERT for Semantic Textual Similar-\nity. In Proceedings of the 2020 Conference on Empirical Methods \nin Natural Language Processing (EMNLP) (pp. 9119–9130).\nMetzler, D. A., Croft, W. B., & Mccallum, A. (2005). Direct maxi-\nmization of rank-based metrics for information retrieval. CIIR \nreport, 429.\nNavinchandran, M., Sharp, M. E., Brundage, M. P., & Sexton, T. B. \n(2021). Discovering critical KPI factors from natural language in \nmaintenance work orders. Journal of Intelligent Manufacturing, \n1–19.\nNemeth, T., Ansari, F., & Sihn, W. (2019). A maturity assessment pro-\ncedure model for realizing knowledge-based maintenance strate-\ngies in smart manufacturing enterprises. Procedia Manufacturing, \n39, 645–654.\nNorth, K., Maier, R., & Haas, O. (2018). Value Creation in the Digi-\ntally Enabled Knowledge Economy. Knowledge Management in \nDigital Change (pp. 1–29). Cham: Springer.\nO’Donovan, P., Leahy, K., Bruton, K., & O’Sullivan, D. T. (2015). An \nindustrial big data pipeline for data-driven analytics maintenance \napplications in large-scale smart manufacturing facilities. Journal \nof Big Data, 2(1), 25.\nOthman, N., Faïz, R., & Smaïli, K. (2019). Manhattan Siamese LSTM \nfor Question Retrieval in Community Question Answering. OTM \nConfederated International Conferences\" On the Move to Mean-\ningful Internet Systems\" (pp. 661–677). Cham: Springer.\nOthman, N., Faiz, R., & Smaïli, K. (2020). Improving the Community \nQuestion Retrieval Performance Using Attention-based Siamese \nLSTM. International Conference on Applications of Natural Lan-\nguage to Information Systems (pp. 252–263). Cham: Springer.\nPang, L., Lan, Y., Guo, J., Xu, J., Xu, J., and Cheng, X. (2017). Deep-\nrank: A new deep architecture for relevance ranking in informa-\ntion retrieval. In Proceedings of the 2017 ACM on Conference on \nInformation and Knowledge Management (pp. 257–266).\nPassaro, L., Bondielli, A., Lenci, A., Marcelloni, F.: UNIPI-NLE at \nCheckThat! 2020: approaching fact checking from a sentence \nsimilarity perspective through the lens of transformers. In: Cap-\npellato, L., Eickhoff, C., Ferro, N., Névéol, A. (eds.): Working \nNotes of CLEF 2020–Conference and Labs of the Evaluation \nForum (2020)\nRay, A., Aggarwal, P., Hadhazi, C., Dasgupta, G., and Paradkar, A. \n(2020). Question Quality Improvement: Deep Question Under -\nstanding for Incident Management in Technical Support Domain. \nIn Proceedings of the AAAI Conference on Artificial Intelligence \n(Vol. 34, No. 08, pp. 13196–13203).\nRanasinghe, T., Orasan, C., & Mitkov, R. (2019). Semantic textual \nsimilarity with siamese neural networks. In Proceedings of the \nInternational Conference on Recent Advances in Natural Lan-\nguage Processing (RANLP 2019) (pp. 1004–1011).\nReimers, N., and Gurevych, I. (2019). Sentence-bert: Sentence embed-\ndings using siamese bert-networks. arXiv preprint arXiv: 1908.  \n10084.\nSexton, T., Brundage, M. P., Hoffman, M., and Morris, K. C. (2017). \nHybrid datafication of maintenance logs from ai-assisted human \ntags. In  2017 IEEE International Conference on Big Data (Big \nData) (pp. 1769–1777). IEEE.\nShtok, A., Dror, G., Maarek, Y., Szpektor, I. (2012). Learning from the \npast: answering new questions with past answers. In Proceedings \nof the 21st International Conference on World Wide Web, pp. \n759–768, WWW 2012\nSipos, R., Fradkin, D., Moerchen, F., and Wang, Z. (2014). Log-based \npredictive maintenance. Proceedings of the 20th ACM SIGKDD \ninternational conference on knowledge discovery and data mining \n(pp. 1867–1876).\nSunilkumar, P., & Shaji, A. P. (2019). A Survey on Semantic Similar-\nity. In 2019 International Conference on Advances in Computing, \nCommunication and Control (ICAC3) (pp. 1–8). IEEE.\nTao, F., Qi, Q., Liu, A., & Kusiak, A. (2018). Data-driven smart manu-\nfacturing. Journal of Manufacturing Systems, 48, 157–169.\n1710 Journal of Intelligent Manufacturing (2021) 32:1699–1710\n1 3\nTong, B., Yanase, T., Ozaki, H., and Iwayama, M. (2015). Informa-\ntion Retrieval Boosted by Category for Troubleshooting Search \nSystem. In GSB@ SIGIR (pp. 28–32).\nUsmanij, P. A., Khosla, R., & Chu, M. T. (2013). Successful product \nor successful system? User satisfaction measurement of ERP soft-\nware. Journal of Intelligent Manufacturing, 24(6), 1131–1144.\nVijayarani, S., Ilamathi, M. J., & Nithya, M. (2015). Preprocessing \ntechniques for text mining-an overview. International Journal of \nComputer Science & Communication Networks, 5(1), 7–16.\nWang, D., Li, T., Zhu, S., & Gong, Y. (2010). iHelp: An intelligent \nonline helpdesk system. IEEE Transactions on Systems, Man, and \nCybernetics, Part B Cybernetics, 41(1), 173–182.\nWijewickrema, M., Petras, V., & Dias, N. (2019). Selecting a text \nsimilarity measure for a content-based recommender system. The \nElectronic Library.\nXu, X., Zhou, S., Xiao, Y., Chang, W., Wei, F., and Yang, M. (2020). \nText Mining-based Research on Aircraft Faults Classification and \nRetrieval Model. In 2020 Annual Reliability and Maintainability \nSymposium (RAMS) (pp. 1–7). IEEE.\nZhou, G., He, T., Zhao, J., and Hu, P. (2015). Learning continuous \nword embedding with metadata for question retrieval in commu-\nnity question answering. In Proceedings of the 53rd Annual Meet-\ning of the Association for Computational Linguistics and the 7th \nInternational Joint Conference on Natural Language Processing \n(Volume 1: Long Papers) (pp. 250–259).\nZhu, A., Meng, Y., Zhang, C., (2017). An improved adam algorithm \nusing lookahead. In Proceedings of the 2017 International Confer-\nence on Deep Learning Technologies, ACM. pp. 19–22.\nPublisher’s Note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7474953532218933
    },
    {
      "name": "Transformer",
      "score": 0.6066877841949463
    },
    {
      "name": "Sentence",
      "score": 0.6032422780990601
    },
    {
      "name": "Natural language processing",
      "score": 0.5808352828025818
    },
    {
      "name": "Semantic similarity",
      "score": 0.5424916744232178
    },
    {
      "name": "Word embedding",
      "score": 0.5239460468292236
    },
    {
      "name": "Artificial intelligence",
      "score": 0.519659161567688
    },
    {
      "name": "Similarity (geometry)",
      "score": 0.4824526906013489
    },
    {
      "name": "Information retrieval",
      "score": 0.4670499861240387
    },
    {
      "name": "Embedding",
      "score": 0.4589892327785492
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}