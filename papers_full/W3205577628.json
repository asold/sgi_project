{
    "title": "Transformer over Pre-trained Transformer for Neural Text Segmentation with Enhanced Topic Coherence",
    "url": "https://openalex.org/W3205577628",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2315951893",
            "name": "Kelvin Lo",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2095702329",
            "name": "Yuan Jin",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A3207844789",
            "name": "Weicong Tan",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2098006703",
            "name": "Ming Liu",
            "affiliations": [
                "Deakin University"
            ]
        },
        {
            "id": "https://openalex.org/A2150389075",
            "name": "Lan Du",
            "affiliations": [
                "Monash University"
            ]
        },
        {
            "id": "https://openalex.org/A2124362732",
            "name": "Wray Buntine",
            "affiliations": [
                "Monash University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2396881363",
        "https://openalex.org/W2148818577",
        "https://openalex.org/W1557074680",
        "https://openalex.org/W2914694065",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W2512217112",
        "https://openalex.org/W2106918957",
        "https://openalex.org/W1880262756",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3115355887",
        "https://openalex.org/W2997244287",
        "https://openalex.org/W2989200142",
        "https://openalex.org/W1828401780",
        "https://openalex.org/W2017776283",
        "https://openalex.org/W1626945812",
        "https://openalex.org/W3102373121",
        "https://openalex.org/W2889446948",
        "https://openalex.org/W2172125983",
        "https://openalex.org/W887185921",
        "https://openalex.org/W2159083595",
        "https://openalex.org/W2118612506",
        "https://openalex.org/W3034327408",
        "https://openalex.org/W2033428978",
        "https://openalex.org/W2962716111",
        "https://openalex.org/W2133793355",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2128709346",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W4231510805",
        "https://openalex.org/W2100873065",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W1862888253",
        "https://openalex.org/W2739675333"
    ],
    "abstract": "This paper proposes a transformer over transformer framework, called Transformerˆ2, to perform neural text segmentation. It consists of two components: bottom-level sentence encoders using pre-trained transformers, and an upper-level transformer-based segmentation model based on the sentence embeddings. The bottom-level component transfers the pre-trained knowledge learnt from large external corpora under both single and pair-wise supervised NLP tasks to model the sentence embeddings for the documents. Given the sentence embeddings, the upper-level transformer is trained to recover the segmentation boundaries as well as the topic labels of each sentence. Equipped with a multi-task loss and the pre-trained knowledge, Transformerˆ2 can better capture the semantic coherence within the same segments. Our experiments show that (1) Transformerˆ2$manages to surpass state-of-the-art text segmentation models in terms of a commonly-used semantic coherence measure; (2) in most cases, both single and pair-wise pre-trained knowledge contribute to the model performance; (3) bottom-level sentence encoders pre-trained on specific languages yield better performance than those pre-trained on specific domains.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2021, pages 3334–3340\nNovember 7–11, 2021. ©2021 Association for Computational Linguistics\n3334\nTransformer over Pre-trained Transformer for Neural Text Segmentation\nwith Enhanced Topic Coherence\nKelvin Lo1 Yuan Jin1 Weicong Tan1 Ming Liu2 Lan Du1∗ Wray Buntine1\n1Faculty of Information Technology, Monash University, Australia\n{kelvin.lo,yuan.jin,charles.tan,lan.du,wray.buntine}@monash.edu\n2School of Information Technology, Deakin University, Australia\nm.liu@deakin.edu.au\nAbstract\nThis paper proposes a transformer over trans-\nformer framework, called Transformer 2, to\nperform neural text segmentation. It con-\nsists of two components: bottom-level sen-\ntence encoders using pre-trained transformers,\nand an upper-level transformer-based segmen-\ntation model based on the sentence embed-\ndings. The bottom-level component transfers\nthe pre-trained knowledge learnt from large ex-\nternal corpora under both single and pair-wise\nsupervised NLP tasks to model the sentence\nembeddings for the documents. Given the sen-\ntence embeddings, the upper-level transformer\nis trained to recover the segmentation bound-\naries as well as the topic labels of each sen-\ntence. Equipped with a multi-task loss and the\npre-trained knowledge, Transformer2 can bet-\nter capture the semantic coherence within the\nsame segments. Our experiments show that\n(1) Transformer2 manages to surpass state-of-\nthe-art text segmentation models in terms of a\ncommonly-used semantic coherence measure;\n(2) in most cases, both single and pair-wise\npre-trained knowledge contribute to the model\nperformance; (3) bottom-level sentence en-\ncoders pre-trained on speciﬁc languages yield\nbetter performance than those pre-trained on\nspeciﬁc domains.\n1 Introduction\nText segmentation is an NLP task that aims to break\ntext into topically coherent segments by identify-\ning natural boundaries of changes of topics (Hearst,\n1994; Moens and De Busser, 2001; Utiyama and\nIsahara, 2001). It is critical in the sense that many\ndownstream tasks can beneﬁt from the resulting\nstructured text, including text summarization, key-\nword extraction and information retrieval.\nBoth supervised and unsupervised learning have\nbeen applied to text segmentation. With the\nlack of large-quantity labels on supervised train-\n∗Corresponding author\ning (Koshorek et al., 2018), unsupervised model-\ning based on clustering (Choi, 2000; Chen et al.,\n2009), Bayesian methods (Du et al., 2013, 2015;\nMalmasi et al., 2017) and graph methods (Glavaš\net al., 2016; Malioutov and Barzilay, 2006) have\nbeen proposed. However, with the advancement of\nself-learning and transfer learning on deep neural\nnetworks, there are more recent supervised model-\ning approaches proposed that aim to predict labeled\nsegment boundaries on smaller datasets. (Koshorek\net al., 2018; Xing et al., 2020; Barrow et al., 2020;\nGlava and Somasundaran, 2020)\nTo the best of our knowledge, the most straight-\nforward remedy to the above problems is knowl-\nedge transfer and distillation from pre-trained mod-\nels. The rich pre-trained knowledge enables the\ntraining of a more general segmentation model on\na small labeled dataset. In this paper, we propose\na transformer over pre-trained transformer frame-\nwork that allows different types of pre-trained infor-\nmation regarding sentences to be distilled to their\nclassiﬁcation for text segmentation. More speciﬁ-\ncally, the contributions of our paper are as follows:\n• Our framework leverages pre-trained (and ﬁxed)\ntransformers at the bottom level to transfer (as\nsentence encoders) both individual and pairwise\nknowledge regarding sentences to train an upper-\nlevel transformer for segmentation.\n• The upper-level transformer is trained with a\nmulti-task loss with different targets, including\nthe segment labels and the (section) topic labels.\n• Our framework outperforms state-of-the-art seg-\nmentation models in terms of the Pk met-\nric(Beeferman et al., 1999) across several real-\nworld datasets in different domains and lan-\nguages.\n• A comprehensive ablation study shows that each\ncomponent of our framework, in most cases, is\nessential by contributing to its segmentation per-\nformance.\n• A thorough empirical study shows the impacts\n3335\nof language-speciﬁc and domain-speciﬁc pre-\ntrained transformers as the sentence encoders on\nthe segmentation performance.\n2 Related Work\nIn this section, we review the past literature on\nthe text segmentation models. These models can\nfurther be categorized into being unsupervised and\nsupervised.\n2.1 Unsupervised Segmentation Models\nUnsupervised segmentation models are developed\nbased on some text similarity measures. C99 (Choi,\n2000), TextTiling (Hearst, 1997) and TopicTil-\ning (Riedl and Biemann, 2012) partitions texts\nwith inter-sentence similarity matrices, lexical co-\noccurrence patterns and topic information from\nlatent Dirichlet allocation (LDA) (Blei et al., 2003)\nrespectively. Sophisticated Bayesian models were\nalso proposed to capture the statistical charac-\nteristics of segment (topic) generation, including\ntopic ordering regularities (Du et al., 2014), na-\ntive language characteristics (Malmasi et al., 2017)\nand topic identities (Mota et al., 2019). On the\nother hand, GraphSeg (Glavaš et al., 2016) and\nMalioutov and Barzilay (2006) has formulated text\nsegmentation as graph problems.\n2.2 Supervised Segmentation Models\nEarlier supervised segmentation models (Galley\net al., 2003; Hsueh et al., 2006; Koshorek et al.,\n2018) rely on heuristics-based and heavily engi-\nneered segment coherence features to train tradi-\ntional classiﬁers (e.g. decision trees (Hsueh et al.,\n2006)) that learn the relationships between the fea-\ntures and the segment labels.\nIn recent years, deep neural network based seg-\nmentation models have started to emerge. A com-\nmon structure for them is a two-level hierarchical\nnetwork, which consist of bottom-level sentence en-\ncoder and upper-level segment boundary classiﬁer.\nVariants of LSTM (Hochreiter and Schmidhuber,\n1997) and Bi-LSTM are vastly used in both lower-\nlevel and upper-level models from previous studies.\nHowever, the implementations of upper-level mod-\nels are more diverse among them. Koshorek et al.\n(2018) and Wang et al. (2018) have used Bi-LSTM\nto predict segment boundary directly, while SEC-\nTOR (Arnold et al., 2019) predicts the topic of\nsentence and segment boundary sequentially with\nLSTM. S-LSTM (Barrow et al., 2020) further im-\nproves the performance by incorporating the ideas\nof previous models. On the other hand, Xing et al.\n(2020) have introduced an auxiliary pairwise sen-\ntence coherence loss. A similar architecture is also\nused by Lukasik et al. (2020).\nThe closest model to ours is proposed in (Glava\nand Somasundaran, 2020)1 where transformers are\nused for both the levels of the architecture. They\nalso developed a semantic coherence measure on\ndistinguishing pairs of genuine and fake text snip-\npets as an auxiliary loss alongside the segment\nclassiﬁcation loss. However, their model does not\nleverage the rich and diverse knowledge extracted\nfrom pre-training tasks (e.g. masked language mod-\neling) to encode sentences at the bottom level. Ad-\ndressing this limitation, our model leverages this\npre-trained knowledge for dealing with a paucity\nof segment labels (e.g. in specialised domains).\n3 Transformer 2 Architecture\nOur proposed model adopts the popular two-level\nnetwork architecture for text segmentation, which\nconsists of a lower-level sentence encoder and an\nupper-level segment boundary classiﬁer.\nOur model aims to enhance the learning of se-\nmantic coherence between sentences from two as-\npects; 1) different pre-trained embeddings, gener-\nated from different NLP tasks on large external\ncorpora, for the same sentences can capture rich\nand diverse information that the target corpus does\nnot contain; 2) sentences within same segment(i.e.\nsharing same topic label) tend to be semantically\nmore coherent than those across segments (i.e. with\ndifferent topic labels). The above enhancements\ncan further improve the segmentation performance\nof the transformer-based classiﬁer.\n3.1 Combining Different Pretrained\nKnowledge at the Bottom Level\nTo introduce different prior knowledge that de-\nscribes different aspects (e.g. semantics, coherence,\netc.) of each sentence into the segmentation, we\ncombine different pre-trained sentence embeddings\nat the bottom level. More speciﬁcally, in this paper,\nwe concatenate the embeddings respectively gen-\nerated from the [CLS] tokens with single-sentence\n1We have been unable to compare with their model as\n1) their pre-trained model has not been made public and 2)\nrerunning their code incurs a major run-time error irrelevant\nto the dataset used and the data preprocessing procedures\napplied.\n3336\nFigure 1: Transformer2 Architecture\nand pairwise-sentence inputs, that is the sentence\nembeddings S := [Ssingle; Spairwise].\nThe single-sentence embeddings learned through\nmasked language modelling (MLM) provide lo-\ncalised sentence information, while pairwise-\nsentence embeddings provide coherence informa-\ntion between consecutive sentences inherited from\npairwise sentence classiﬁcation tasks of pre-trained\nmodels, such as next-sentence prediction (NSP)\n(Devlin et al., 2019) and the sentence-order predic-\ntion (SOP) (Lan et al., 2019). Further details are\nsummarised in table 1.\nTable 1: Transformers leveraged for the bottom level\nin our experiments by combining their [CLS] embed-\nding outputs pre-trained respectively under single and\npairwise tasks.\nSingle Pairwise\nTransformer MLM MLM NSP SOP\nBERT ✓ ✓ ✓\nXLNet ✓ ✓\nRoBERTa ✓ ✓\nALBERT ✓ ✓ ✓\n3.2 Sentence Classiﬁcation at the Upper\nLevel\nOnce the sentence embeddings are obtained, we\ntrain a transformer model at the upper level of the\narchitecture to classify 1) whether each sentence is\nTable 2: Summary of WikiSection Dataset\nLanguage Topic Abbrev. #Subtopics #Documents\nEnglish Disease en_disease 27 3,590\nEnglish City en_city 30 19,539\nGerman Disease de_disease 25 2,323\nGerman City de_city 27 12,537\nthe segment boundary and 2) the topic label of each\nsentence. Thus, the loss function for the upper-level\ntransformer can be formulated as follows:\nL(yseg, ytopic; S, Θ) =Lseg(yseg, ˆyseg; S, Θ)\n+ Ltopic(ytopic, ˆytopic; S, Θ)\nˆyseg := Sigmoid(Linear2(TransformerΘ(S)))\nˆytopic := Softmax(LinearK(TransformerΘ(S)))\n(1)\nwhere S =< s1, s2, ...,sI >, in this case, is the\nconcatenation2 of a sequence of embeddings of\nall the I sentences in the document 3; yseg, ytopic\nare the binary segmentation and K topic labels for\neach sentence, while ˆyseg, ˆytopic are their respective\npredictions. Correspondingly, linear layers with\nthe respective output dimensions are put on top\nof the transformer with parameters Θ. The term\nLtopic denotes an auxiliary loss on the topic labels\nof each sentence. Minimizing this loss forces our\nframework to learn semantic coherence between\nsentences to account for their topical similarity. As\nfor model training, the binary segmentation loss\nLseg and the topic prediction loss Ltopic are min-\nimized respectively as the binary and categorical\ncross entropy losses with respect to Θ.\n4 Experimental Results\n4.1 Datasets\nWe used the WikiSection dataset (Arnold et al.,\n2019) to evaluate the segmentation performance of\nour framework. It contains 38,000 full-text docu-\nments with segment information from English and\nGerman Wikipedia, each divided by topics regard-\ning diseases and cities. The details of the corpora\nare summarised in Table 2.\n4.2 Experimental Design\nIn the experiments, we leveraged both the single-\nsentence and pairwise-sentence pre-trained knowl-\nedge from the transformers speciﬁed in Table 1 to\nencode sentences at the bottom level. We aim to\nstudy the effects of bottom-level sentence encoders\n2With a slight abuse of notation, we reuse the symbol S\nfrom Section 3.1 to denote a sequence of all the sentences in\nthe document.\n3I denotes the maximum number of sentences in a docu-\nment including the paddings.\n3337\nTable 3: Transformer models and their conﬁgurations\n(i.e. languages and domains) used in our experiments\nModel Conﬁg. en_city en_disease de_city de_disease\nEnglish ✓ ✓\nBERT German ✓ ✓\nBioClinical ✓ ✓\nXLNet English ✓ ✓\nEnglish ✓ ✓\nRoBERTa BioMed ✓ ✓\nALBERT English ✓ ✓\nwith different 1) transformer models, 2) languages\nand 3) domains on the segmentation performance.\nTable 3 displays the details of the transform-\ners and their conﬁgurations (i.e. languages and\ndomains) used in the experiments. More speciﬁ-\ncally, we encoded the German corpora, i.e. de_city\nand de_disease, with German BERT, which is pre-\ntrained on the German Wikipedia dump. Likewise,\nwe also encoded the domain-speciﬁc corpora, i.e.\nen_disease and de_disease, with BioClinical mod-\nels, pre-trained on the MIMIC III (Johnson et al.,\n2016) medical datasets. Detailed model conﬁgura-\ntions are listed in Appendix 4.4.\n4.3 Evaluation Metrics & Baselines\nAligning with previous models, we evaluated the\nmodel performance with respect to the Pk metric\nproposed by Beeferman et al. (1999). It is a proba-\nbilistic metric that indicates, given a pair or words\nwith k words apart, how likely will they lie in dif-\nferent segments. Pk values closer to 0 indicate the\npredicted segments are closer to ground truth, In\nour experiment, the value of k is set to be half of\nthe average ground-truth segment length (Pevzner\nand Hearst, 2002).\nThe baselines include 1) machine learning seg-\nmentation models: C99 (Choi, 2000) and Topic-\nTiling (Riedl and Biemann, 2012), and 2) state-\nof-the-art deep neural models: TextSeg (Koshorek\net al., 2018), SECTOR (Arnold et al., 2019) with\npre-trained embeddings, S-LSTM (Barrow et al.,\n2020) and BiLSTM+BERT (Xing et al., 2020). We\nfollowed the default hyper-parameter settings for\nall the models as speciﬁed in their ofﬁcial imple-\nmentations.\n4.4 Transformer 2 Settings\nFor all the corpora, we have ﬁxed several hyper-\nparameters of Transformer 2. We have used the\nAdam optimiser (Kingma and Ba, 2015) with the\nlearning rate being 0.0001. The maximum input se-\nquence length was ﬁxed at 150 sentences, as more\nthan 94% of the documents have less than or equal\nTable 4: Pk values of the baselines and the best variants\nof Transformer2 for the different datasets; Bold and un-\nderscore ﬁgures indicate the best and second best re-\nsults respectively.\nModel en_disease de_disease en_city de_city\nC99 37.4 42.7 36.8 38.3\nTopicTiling 43.4 45.4 30.5 41.3\nTextSeg 24.3 35.7 19.3 27.5\nSECTOR+emb 26.3 27.5 15.5 16.2\nS-LSTM 20.0 18.8 9.1 9.5\nBiLSTM+BERT 21.1 28.0 9.3 11.3\nTransformer2\nXLNet 25.2 - 11.7 -\nTransformer2\nALBERT 59.1 - 43.6 -\nTransformer2\nRoBERTa 57.2 - 22.7 -\nTransformer2\nBERT 18.8 - 9.1 -\nwithout Ssingle 19.9 - 8.2 -\nTransformer2\nde_BERT - 16.0 - 7.3\nwithout Ssingle - 17.1 - 6.8\nto this number of sentences across the text seg-\nmentation corpora. Moreover, our model has 5\ntransformer encoder layers with 24 self-attention\nheads. Each of the encoder layers has a point-wise\nfeed-forward layer of 1,024 dimensions. For the\nsegmentation predictions, 70% of the inner sen-\ntences were randomly masked while all the begin\nsentences were not masked in order to address the\nimbalance class problem.\n4.5 Pk results\nComparison with previous models 4 Table 4\nshows the performance of the best variants of\nTransformer2 for different datasets and that of the\nbaseline models in terms of the Pk metric. Our\nmodels Transformer2\nBERT and Transformer2\nde_BERT\noutperforms all previous models by a notable mar-\ngin in English and German corpus respectively.\nAblation study of model components We have\nexamined the effects of single and pairwise em-\nbeddings, joint modeling on topic classiﬁcation\nand choice of lower-level sentence encoder, sum-\nmarised in tables 5 and 6. The results from table 5\nshows the models yield better results without the\nsingle sentence embeddings Ssingle on the en_city\nand de_city datasets. This suggests that combining\ndifferent pre-trained knowledge does not always\nimprove the segmentation quality.\nThe results also show that the segmentation qual-\nity solely based on the change in topic label predic-\ntion labels is signiﬁcantly inferior than using the\nsegmentation labels. This is because predicting the\nsame topic label consecutively in a multi-class set-\nting is more difﬁcult than the same segment label\nconsecutively in a binary-class setting.\n4Detailed qualitative analysis can be found in Appendix\n4.6\n3338\nTable 5: An ablation study on the impacts of each com-\nponent of the best variants of Transformer2 on Pk\nmodel en_disease de_disease en_city de_city\nTransformer2\nBERT 18.8 - 9.1 -\nwithout Ssingle 19.9 - 8.2 -\nwithout Spairwise 19.2 - 9.1 -\nwithout Ltopic 20.4 - 8.2 -\nwithout Lseg 25.3 - 41.1 -\nTransformer2\nde_BERT - 16.0 - 7.3\nwithout Ssingle - 17.1 - 6.8\nwithout Spairwise - 18.8 - 9.2\nwithout Ltopic - 19.5 - 7.2\nwithout Lseg - 20.2 - 27.5\nTable 6: Pk values of the domain-speciﬁc BERT and\nRoBERTa\nSentence Encoder en_disease de_disease\nTransformer2\nBioClinical_BERT 21.4 45.8\nTransformer2\nBioMed_RoBERTa 36.4 50.2\nOn the other hand, from table 6, we can observe\nthat models pre-trained on corpora in speciﬁc do-\nmains, such as BioClinical BERT, do not improve\ntext segmentation quality compared to models pre-\ntrained on giant language-speciﬁc corpora, such\nas German BERT, which is accountable to the tok-\nenization quality of such model.\n4.6 Qualitative Analysis of Transformer 2\nApart from the quantitative evaluation based on\nthe Pk metric, we also conducted qualitative\nanalysis on the segment predictions from both\nour model and the most competitive baseline:\nBiLSTM+BERT. More speciﬁcally, we randomly\npicked up several documents from en_disease and\nde_disease datasets, visually inspected and then\nsummarised the difference between the segmen-\ntation styles of the best variants of Transformer 2\nand BiLSTM+BERT. We ﬁnd that the variants of\nTransformer2 tend to yield more dispersed seg-\nment predictions across the documents, while the\npredictions of BiLSTM+BERT tend to be more\nconcentrated and often documents are clustered\nas one big segment. Figure 2 shows one such ex-\nample of our ﬁnding.\n5 Conclusion and Future Work\nIn this paper, we propose a transformer over\npre-trained transformer framework, called\nTransformer2, for text segmentation with a\nfocus on enhancing the learning of the semantic\ncoherence between sentences. The bottom level\nof Transformer2 combines (untrainable and ﬁxed)\nsentence embeddings outputted respectively\nfrom transformers pre-trained with both the\nFigure 2: Probabilities of segment boundaries com-\npared to the gold-standard ones (red lines on top\nof each graph) on one en_disease document where\nTransformer2’s predicted probabilities are more dis-\npersed and accurate.\nsingle-sentence and the pairwise-sentence NLP\ntasks. An upper-level transformer is trained upon\nthe combined sentence embeddings to minimize\nboth the binary segmentation loss and the auxiliary\ntopic prediction loss.\nThe empirical results show that the best variants\nof Transformer2 outperform several state-of-the-art\nsegmentation models, including the deep neural\nmodels, across four real-world datasets in terms\nof a commonly-used segment coherence measure\nPk. We have also conducted a comprehensive ab-\nlation study which shows that in most cases, each\ncomponent of Transformer2 is helpful for boosting\nthe segmentation performance. We have also found\nthat using language-speciﬁc pre-trained transform-\ners at the bottom level is more useful than using\ndomain-speciﬁc ones. For the future work, we will\ninvestigate the efﬁcacy of Transformer2 on helping\nthe downstream NLP tasks such as text summarisa-\ntion, keyword extraction and topic modelling.\nReferences\nSebastian Arnold, Rudolf Schneider, Philippe Cudré-\nMauroux, Felix A. Gers, and Alexander Löser. 2019.\nSector: A neural model for coherent topic segmenta-\ntion and classiﬁcation. Transactions of the Associa-\ntion for Computational Linguistics, 7:169–184.\nJoe Barrow, Rajiv Jain, Vlad Morariu, Varun Manju-\nnatha, Douglas Oard, and Philip Resnik. 2020. A\njoint model for document segmentation and segment\nlabeling. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 313–322.\nDoug Beeferman, Adam Berger, and John Lafferty.\n3339\n1999. Statistical models for text segmentation. Ma-\nchine learning, 34(1-3):177–210.\nDavid M. Blei, Andrew Y . Ng, and Michael I. Jordan.\n2003. Latent dirichlet allocation. Journal of Ma-\nchine Learning Research, 3:993–1022.\nHarr Chen, S.R.K. Branavan, Regina Barzilay, and\nDavid R. Karger. 2009. Global models of document\nstructure using latent permutations. In Proceed-\nings of Human Language Technologies: The 2009\nAnnual Conference of the North American Chap-\nter of the Association for Computational Linguistics,\npages 371–379.\nFreddy Y . Y . Choi. 2000. Advances in domain inde-\npendent linear text segmentation. In 1st Meeting of\nthe North American Chapter of the Association for\nComputational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nLan Du, Wray Buntine, and Mark Johnson. 2013.\nTopic segmentation with a structured topic model.\nIn Proceedings of the 2013 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies ,\npages 190–200.\nLan Du, John K Pate, and Mark Johnson. 2014. Topic\nmodels with topic ordering regularities for topic seg-\nmentation. In 2014 IEEE International Conference\non Data Mining, pages 803–808. IEEE.\nLan Du, John K Pate, and Mark Johnson. 2015. Topic\nsegmentation with an ordering-based topic model.\nIn Twenty-Ninth AAAI Conference on Artiﬁcial In-\ntelligence, pages 2232–2238.\nMichel Galley, Kathleen McKeown, Eric Fosler-\nLussier, and Hongyan Jing. 2003. Discourse seg-\nmentation of multi-party conversation. In Proceed-\nings of the 41st Annual Meeting of the Association\nfor Computational Linguistics, pages 562–569.\nGoran Glava and Swapna Somasundaran. 2020. Two-\nlevel transformer and auxiliary coherence modeling\nfor improved text segmentation. Proceedings of the\nAAAI Conference on Artiﬁcial Intelligence, 34:7797–\n7804.\nGoran Glavaš, Federico Nanni, and Simone Paolo\nPonzetto. 2016. Unsupervised text segmentation us-\ning semantic relatedness graphs. In Proceedings of\nthe Fifth Joint Conference on Lexical and Computa-\ntional Semantics, pages 125–130.\nMarti A. Hearst. 1994. Multi-paragraph segmentation\nexpository text. In 32nd Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 9–16.\nMarti A. Hearst. 1997. Text tiling: Segmenting text\ninto multi-paragraph subtopic passages. Computa-\ntional Linguistics, 23(1):33–64.\nSepp Hochreiter and Jürgen Schmidhuber. 1997.\nLong Short-Term Memory. Neural Computation ,\n9(8):1735–1780.\nPei-Yun Hsueh, Johanna D Moore, and Steve Renals.\n2006. Automatic segmentation of multiparty dia-\nlogue. In 11th Conference of the European Chapter\nof the Association for Computational Linguistics.\nAlistair EW Johnson, Tom J Pollard, Lu Shen, Li-\nwei H Lehman, Mengling Feng, Mohammad Ghas-\nsemi, Benjamin Moody, Peter Szolovits, Leo An-\nthony Celi, and Roger G Mark. 2016. Mimic-iii,\na freely accessible critical care database. Scientiﬁc\ndata, 3:160035.\nDiederik P. Kingma and Jimmy Ba. 2015. Adam: A\nmethod for stochastic optimization. In 3rd Inter-\nnational Conference on Learning Representations,\nICLR 2015, San Diego, CA, USA, May 7-9, 2015,\nConference Track Proceedings.\nOmri Koshorek, Adir Cohen, Noam Mor, Michael Rot-\nman, and Jonathan Berant. 2018. Text segmentation\nas a supervised learning task. In Proceedings of the\n2018 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, Volume 2 (Short Pa-\npers), pages 469–473.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learning\nof language representations. In International Con-\nference on Learning Representations.\nMichal Lukasik, Boris Dadachev, Kishore Papineni,\nand Gonçalo Simões. 2020. Text segmentation by\ncross segment attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 4707–4716.\nIgor Malioutov and Regina Barzilay. 2006. Minimum\ncut model for spoken lecture segmentation. In Pro-\nceedings of the 21st International Conference on\nComputational Linguistics and 44th Annual Meet-\ning of the Association for Computational Linguistics,\npages 25–32.\nShervin Malmasi, Mark Dras, Mark Johnson, Lan Du,\nand Magdalena Wolska. 2017. Unsupervised text\nsegmentation based on native language characteris-\ntics. In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 1457–1469.\nMarie-Francine Moens and Rik De Busser. 2001.\nGeneric topic segmentation of document texts. In\nProceedings of the 24th annual international ACM\nSIGIR conference on research and development in\ninformation retrieval, pages 418–419.\n3340\nPedro Mota, Maxine Eskenazi, and Luísa Coheur. 2019.\nBeamSeg: A joint model for multi-document seg-\nmentation and topic identiﬁcation. In Proceedings\nof the 23rd Conference on Computational Natural\nLanguage Learning (CoNLL), pages 582–592.\nLev Pevzner and Marti A. Hearst. 2002. A critique and\nimprovement of an evaluation metric for text seg-\nmentation. Comput. Linguist., 28(1):19–36.\nMartin Riedl and Chris Biemann. 2012. TopicTiling: A\ntext segmentation algorithm based on LDA. In Pro-\nceedings of ACL 2012 Student Research Workshop ,\npages 37–42.\nMasao Utiyama and Hitoshi Isahara. 2001. A statis-\ntical model for domain-independent text segmenta-\ntion. In Proceedings of the 39th Annual Meeting\nof the Association for Computational Linguistics ,\npages 499–506.\nYizhong Wang, Sujian Li, and Jingfeng Yang. 2018.\nToward fast and accurate neural discourse segmen-\ntation. In Proceedings of the 2018 Conference on\nEmpirical Methods in Natural Language Processing,\npages 962–967.\nLinzi Xing, Brad Hackinen, Giuseppe Carenini, and\nFrancesco Trebbi. 2020. Improving context model-\ning in neural topic segmentation. In Proceedings of\nthe 1st Conference of the Asia-Paciﬁc Chapter of the\nAssociation for Computational Linguistics and the\n10th International Joint Conference on Natural Lan-\nguage Processing, pages 626–636."
}