{
  "title": "Distinguishing Fact from Fiction: A Benchmark Dataset for Identifying Machine-Generated Scientific Papers in the LLM Era.",
  "url": "https://openalex.org/W4385734222",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2132439969",
      "name": "Edoardo Mosca",
      "affiliations": [
        "Technical University of Munich"
      ]
    },
    {
      "id": "https://openalex.org/A5101799022",
      "name": "Mohamed Hesham Ibrahim Abdalla",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1992758619",
      "name": "Paolo Basso",
      "affiliations": []
    },
    {
      "id": null,
      "name": "Margherita Musumeci",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2061708611",
      "name": "Georg Groh",
      "affiliations": [
        "Technical University of Munich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3155340931",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2516809705",
    "https://openalex.org/W4318149317",
    "https://openalex.org/W3015453090",
    "https://openalex.org/W2924690340",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4288091035",
    "https://openalex.org/W4320005767",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4327518740",
    "https://openalex.org/W2973049837",
    "https://openalex.org/W4299567010",
    "https://openalex.org/W2962862931",
    "https://openalex.org/W4296550292",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W1848279521",
    "https://openalex.org/W4311426581",
    "https://openalex.org/W2404979152",
    "https://openalex.org/W4307542217",
    "https://openalex.org/W3165883805",
    "https://openalex.org/W2941922035",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W2576121294",
    "https://openalex.org/W3034383590",
    "https://openalex.org/W4285204696",
    "https://openalex.org/W4223913581",
    "https://openalex.org/W4288334893",
    "https://openalex.org/W4318351452",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W2251670640",
    "https://openalex.org/W4226034794",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3046357466",
    "https://openalex.org/W3034287667",
    "https://openalex.org/W4311642023",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2978017171"
  ],
  "abstract": "As generative NLP can now produce content nearly indistinguishable from human writing, it becomes difficult to identify genuine research contributions in academic writing and scientific publications. Moreover, information in NLP-generated text can potentially be factually wrong or even entirely fabricated. This study introduces a novel benchmark dataset, containing human-written and machine-generated scientific papers from SCIgen, GPT-2, GPT-3, ChatGPT, and Galactica. After describing the generation and extraction pipelines, we also experiment with four distinct classifiers as a baseline for detecting the authorship of scientific text. A strong focus is put on generalization capabilities and explainability to highlight the strengths and weaknesses of detectors. We believe our work serves as an important step towards creating more robust methods for distinguishing between human-written and machine-generated scientific papers, ultimately ensuring the integrity of scientific literature.",
  "full_text": "Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages 190–207\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nDistinguishing Fact from Fiction: A Benchmark Dataset for Identifying\nMachine-Generated Scientific Papers in the LLM Era.\nEdoardo Mosca\nTU Munich,\nDepartment of Informatics,\nGermany\nedoardo.mosca@tum.de\nMohamed Hesham I. Abdalla\nTU Munich,\nDepartment of Informatics,\nGermany\nmohamed.abdalla@tum.de\nPaolo Basso\nPolytechnic of Milan,\nDepartment of EIB,\nItaly\npaolo3.basso@mail.polimi.it\nMargherita Musumeci\nPolytechnic of Milan,\nDepartment of EIB,\nItaly\nmargherita.musumeci@mail.polimi.it\nGeorg Groh\nTU Munich,\nDepartment of Informatics,\nGermany\ngrohg@in.tum.de\nAbstract\nAs generative NLP can now produce content\nnearly indistinguishable from human writing, it\nbecomes difficult to identify genuine research\ncontributions in academic writing and scien-\ntific publications. Moreover, information in\nNLP-generated text can potentially be factually\nwrong or even entirely fabricated. This study in-\ntroduces a novel benchmark dataset, containing\nhuman-written and machine-generated scien-\ntific papers from SCIgen, GPT-2, GPT-3, Chat-\nGPT, and Galactica. After describing the gen-\neration and extraction pipelines, we also exper-\niment with four distinct classifiers as a baseline\nfor detecting the authorship of scientific text. A\nstrong focus is put on generalization capabili-\nties and explainability to highlight the strengths\nand weaknesses of detectors. We believe our\nwork serves as an important step towards creat-\ning more robust methods for distinguishing be-\ntween human-written and machine-generated\nscientific papers, ultimately ensuring the in-\ntegrity of scientific literature.\n1 Introduction\nGenerative Natural Language Processing(NLP)\nsystems—often based on Large Language Mod-\nels (LLMs) (Brown et al., 2020; Scao et al., 2022;\nOpenAI, 2023)—have experienced significant ad-\nvancements in recent years, with state-of-the-art\nalgorithms generating content that is almost in-\ndistinguishable from human-written text (Radford\net al., 2019; Zellers et al., 2019; Keskar et al., 2019;\nBrown et al., 2020). This progress has led to nu-\nmerous applications in various fields, such as chat-\nbots (OpenAI, 2022), automated content generation\n(Chen et al., 2021), and even summarization tools\nGalactica \nLLM \nChatGPT \nLLM \nGPT-2/3 \nLLM \nSCIgen \nCFG \nQuery Title: \n\"Video (language) modeling.....\"\nAbstract:  \"Advances in video modeling..\nIntroduction:  \"Video data is a growing..\nConclusion:  \"In our work, we tested the..\"\nDetection Benchmark Generation\nDetect- \nGPT \nGPT-3 \nGalacica \nRoBERTa \nFigure 1: This work’s overview. Five methods are used\nto machine-generate papers, which are then mixed with\nhuman-written ones to create our benchmark. Four mod-\nels are then tested as baselines to identify the authorship\nof a given output.\n(Liu, 2019). However, these advancements also\nraise concerns regarding the integrity and authentic-\nity of academic writing and scientific publications\n(Dergaa et al., 2023; Stokel-Walker, 2022).\nIt is indeed increasingly difficult to differentiate\ngenuine research contributions from artificially gen-\nerated content. Moreover, we are at an increased\nrisk of including factually incorrect or entirely fab-\nricated information (Maynez et al., 2020; Tian et al.,\n2019). Reliably identifying machine-generated sci-\n190\nentific publications becomes thus crucial to main-\ntain the credibility of scientific literature and fos-\ntering trust among researchers.\nThis work introduces a novel benchmark to ad-\ndress this issue. Our contribution—also briefly\nsketched in 1—can be summarized as follow:\n(1) We present a dataset comprising of human-\nwritten and machine-generated scientific doc-\numents from various sources: SCIgen (Strib-\nling et al., 2005), GPT-2 (Radford et al., 2019),\nGPT-3 (Brown et al., 2020), ChatGPT (Ope-\nnAI, 2022), and Galactica (Taylor et al., 2022).\nEach document includes abstract, introduc-\ntion, and conclusion in a machine-readable\nformat.\n(2) We experiment with four distinct classifiers—\nRoBERTa (Liu et al., 2019), Galactica (Taylor\net al., 2022), GPT-3 (Brown et al., 2020), and\nDetectGPT (Mitchell et al., 2023)—as a base-\nline for detecting the authorship of scientific\ntext, assessing their performance in differenti-\nating between human and machine-generated\ncontent.\n(3) We emphasize experimenting with generaliza-\ntion capabilities and explainability to provide\ninsights into the strengths and weaknesses of\neach detector.\nWe release our benchmark dataset, baseline mod-\nels, and testing code to the public to promote fur-\nther research and aid the development of more ro-\nbust detection methods. We release our benchmark\ndataset and baseline models as well as all code used\nfor experimental results1.\n2 Related Work\n2.1 (Machine-Generated) Scientific\nPublication Corpora\nThe ACL Anthology2 (Bird et al., 2008) and arXiv3\n(arXiv.org submitters, 2023) are widely used re-\nsources for accessing scientific texts and their as-\nsociated metadata. However, these databases do\nnot provide structured text for scientific documents,\nnecessitating the use of PDF parsers and other tools\nto extract text and resolve references. Several ef-\nforts have been made to develop structured text\n1huggingface.co/datasets/tum-nlp/IDMGSP\n2https://aclanthology.org/\n3https://arxiv.org/\ndatabases for scientific documents. (Cohan and\nGoharian, 2015; Saier and Färber, 2019; Lo et al.,\n2020).\nDespite progress in generating text, machine-\ngenerated datasets for scientific literature remain\nlimited. A recent study by Kashnitsky et al. (2022)\ncompiled a dataset including retracted, summa-\nrized, and paraphrased paper abstracts and excerpts,\nas well as text generated by GPT-3 (Brown et al.,\n2020) and GPT-Neo (Black et al., 2021). It’s worth\nnoting that the dataset lists retracted papers as\nmachine-generated, which may not always be ac-\ncurate, and only includes excerpts or abstracts of\nthe papers.\nLiyanage et al. (2022) proposed an alternative ap-\nproach, in which they generated papers using GPT-\n2 (Radford et al., 2019) and Arxiv-NLP 4. How-\never, their dataset was limited to only 200 samples,\nwhich were restricted to the fields of Artificial In-\ntelligence and Computation and Language.\n2.2 Generative NLP for Scientific Articles\nGenerative NLP for scientific publications has\nevolved significantly in recent years. Early meth-\nods, such as SCIgen (Stribling et al., 2005), used\nContext-Free-Grammar (CFG) to fabricate com-\nputer science publications. These often contain\nnonsensical outputs due to CFG’s limited capacity\nfor generating coherent text.\nThe advent of attention, transformers (Waswani\net al., 2017), and LLMs (Brown et al., 2020) has\npaved the way for more sophisticated models ca-\npable of generating higher-quality scientific con-\ntent. Some—such as (Devlin et al., 2019), GPT-3\n(Brown et al., 2020), ChatGPT (OpenAI, 2022),\nand Bloom (Scao et al., 2022)—are built for gen-\neral purposes. Others, instead, are domain-specific\nand specialized for generating scientific literature.\nPopular examples in this category are SciBERT\n(Maheshwari et al., 2021) and Galactica (Taylor\net al., 2022).\nBoth general and domain-specific models have\nshown outstanding results in various scientific\ntasks, demonstrating their potential in generating\ncoherent and contextually relevant scientific text.\nThis same technology has also been applied to other\ndomains, including writing news articles (Zellers\net al., 2019), producing learning material (MacNeil\net al., 2022), and creative writing (Swanson et al.,\n2021).\n4https://huggingface.co/lysandre/arxiv-nlp\n191\n2.3 Detection of Machine-Generated Text\nThe ability to automatically generate convincing\ncontent has motivated researchers to work on its\nautomatic detection, especially given its potential\nimplications for various domains.\nSeveral approaches to detecting machine-\ngenerated text have emerged, employing a range of\ntechniques. Some studies have focused on utilizing\nhand-crafted features (Gehrmann et al., 2019), bag-\nof-words features (Fagni et al., 2021), or neural\nfeatures in combination with supervised models to\ndistinguish between human and machine-generated\ncontent (Bakhtin et al., 2019; Ippolito et al., 2019;\nFagni et al., 2021).\nAlternative approaches explore using the prob-\nability curvature of the generative model itself\n(Mitchell et al., 2023) or watermarking machine-\ngenerated text to facilitate detection (Kirchenbauer\net al., 2023).\n2.4 Detection of Machine-Generated Scientific\nPublications\nAs we have seen in 2.3, there exist several\ngeneral-purpose solutions aiming at detecting NLP-\ngenerated text. The detection of automatically gen-\nerated scientific publications, instead, is an emerg-\ning subarea of research with very limited exist-\ning work. Previous approaches have primarily\nfocused on identifying text generated by SCIgen\n(Stribling et al., 2005) using hand-crafted features\n(Amancio, 2015; Williams and Giles, 2015), near-\nest neighbor classifiers (Nguyen and Labbé, 2016),\nand grammar-based detectors (Cabanac and Labbé,\n2021). More recent studies have shown promis-\ning results in detecting LLM-generated papers us-\ning SciBERT (Beltagy et al., 2019), DistilBERT\n(Sanh et al., 2019), and other models (Glazkova and\nGlazkov, 2022; Liyanage et al., 2022). Nonethe-\nless, these approaches have mostly been tested on\nabstracts or a substantially limited set of paper do-\nmains.\n3 Benchmark Dataset\nIn this section, we delve into the construction of our\nbenchmark dataset, which comprises both human-\nwritten and machine-generated scientific papers.\nOften, for simplicity, we refer to the former group\nwith real, and to the latter with fake. In section\n3.1, we elaborate on the process we followed to\nextract data from the PDF documents of real papers.\nIn section 3.2, we describe instead our prompting\npipelines and how we utilized various generators\nto produce fake scientific papers.\nTable 1 offers an overview of our dataset, includ-\ning sources and numbers of samples and tokens.\nSource Quantity Tokens\narXiv parsing 1 (real) 12k 13.40M\narXiv parsing 2 (real) 4k 3.20M\nSCIgen (fake) 3k 1.80M\nGPT-2 (fake) 3k 2.90M\nGalactica (fake) 3k 2.00M\nChatGPT (fake) 3k 1.20M\nGPT-3 (fake) 1k 0.50M\nTotal real (extraction) 16k 16.60M\nTotal fake (generators) 13k 8.40M\nTotal 29k 25M\nTable 1: Data sources included in our dataset and their\nrespective sizes.\n3.1 Real Papers Collection\nTo collect human-written—or real—scientific pa-\npers for our dataset, we source them from the arXiv\ndataset (arXiv.org submitters, 2023) hosted on Kag-\ngle5. This provides comprehensive metadata, in-\ncluding title, abstract, publication date, and cate-\ngory. However, the introduction and conclusion\nsections are not part of the metadata, which implies\nthe need for PDF parsing to extract these sections.\nFrom the metadata, each paper’s ID and ver-\nsion are utilized to construct the document path\nand retrieve the corresponding PDF from the pub-\nlicly accessible GCS bucket. Each PDF is then fed\nto the PyMuPDF (Rudduck, 2021) library to be\nparsed and to extract the relevant content. Unfortu-\nnately, parsing PDFs is known to be very challeng-\ning. This is particularly true for a double-column\nformat, which many scientific papers have. Despite\nhaving tested several heuristic rules to identify and\nextrapolate the correct sections, the process can\nstill fail at times. We discard data points where the\nparsing was unsuccessful.\nThe resulting set includes 12,000 real papers.\nFurthermore, we collect an additional 4,000 sam-\nples undergoing a different parsing procedure\n(Shrestha and Zhou, 2022). The intention is to\nensure there are no recognizable parsing artifacts\nthat inadvertently ease the detection process (see\n4).\n5https://www.kaggle.com/\n192\n(a) Galactica and GPT-3 generation.\n (b) ChatGPT generation.\n (c) GPT-2 generation.\nFigure 2: Generation pipeline used for each model. In the case of Galactica and GPT-3 (Figure 2a), each section\ndepends on the previous sections. On the other hand, ChatGPT’s generation sequence (Figure 2b) requires only the\ntitle to generate all the necessary sections at once. Finally, for GPT-2 (Figure 2c), three separate models are used to\ngenerate each of the sections based solely on the title.\n3.2 Fake Papers Generation\nFor the fake component of our dataset, we employ\nseveral models to generate abstracts, introductions,\nand conclusions based on scientific paper titles.\nThe titles of the real papers sourced from the arXiv\ndatabase (see 3.1) serve as prompts for the mod-\nels to generate the target sections—i.e. abstract,\nintroduction, and conclusion.\nTo create fake scientific papers, we fine-tune\nGPT-2 and GPT-3 instances (Radford et al., 2019;\nBrown et al., 2020) and also leverage SCIgen (Stri-\nbling et al., 2005), Galactica (Taylor et al., 2022),\nand ChatGPT (OpenAI, 2022). For each model—as\nshown in Figure 2—we employ a unique prompt-\ning/querying strategy to produce the desired paper\nsections.\nThis combination of models aims at generating\na diverse set of artificial scientific papers. Con-\ncrete examples of generated papers can be found in\nappendix A.\n3.2.1 SCIgen\nAlongside the papers produced by the various\nLLMs, our fake dataset incorporates documents\ngenerated by SCIgen (Stribling et al., 2005). De-\nspite the seemingly straightforward task of detect-\ning CFG-generated text, it is still relevant to ensure\nthat detectors can distinguish machine-generated\npapers even if they are poorly written and contain\nnonsensical content. Stribling and Aguayo (2021)\nshows that such papers have been accepted in sci-\nentific venues in the past.\nPrompting SCIgen is done simply by running it\nas an offline script6 which generates all the needed\nsections including the title. The entire paper in\nLATEXformat is generated as a result.\n3.2.2 GPT-2\nWe fine-tune three distinct GPT-2 base models\n(117M) (Radford et al., 2019) to individually gen-\nerate each section based on the given title. The\nmodels are trained in a seq2seq fashion (Sutskever\net al., 2014), with the training procedure spanning\nsix epochs and incorporating 3,500 real papers.\nWhen encountering lengthy inputs, we truncate\nthose exceeding 1,024 tokens, potentially result-\ning in less coherent introductions and conclusions.\nAbstracts remain more coherent as they typically\nfall below this threshold.\nHyperparameters: For training we use a batch\nsize of 16 across all six epochs. We set the\nmax_new_token to 512, top_k to 50, and top_p\nto 0.5 for all three models.\nPost-processing: We remove generated \" \\n\"\ncharacters and any extra sections not explicitly\nmentioned in the prompt. Additionally, we remove\nincomplete sentences preceding the start of a new\nsentence. These are indeed common artifacts of\nGPT-2 and are easily identifiable by lowercase let-\nters.\n6https://github.com/soerface/scigen-docker\n193\nAlthough our GPT-2 model is specifically fine-\ntuned for the task, generating long pieces of text oc-\ncasionally results in less meaningful content. More-\nover, we observe that decoupling the generation of\nsections can lead to inconsistencies among the gen-\nerated sections within the papers.\n3.2.3 Galactica\nGalactica is trained on a large corpus of scientific\ndocuments (Taylor et al., 2022). Therefore, it is\nalready well-suited for the task of generating scien-\ntific papers. To facilitate the generation of coherent\nlong-form text, we divide the generation process\ninto smaller segments, with each section relying on\npreceding sections for context. For instance, while\ngenerating a conclusion, we provide the model with\nthe title, abstract, and introduction as concatenated\ntext.\nHyperparameters: We use Galactica base (1.3B\nparameters) (Taylor et al., 2022) to generate each\npaper section based on the previous sections. The\ncomplete set of hyperparameters can be found in\nappendix A. Additionally, we enforce max length\nleft padding. Due to the limited model capacity,\nlimiting the output number of tokens is necessary\nto avoid the hallucination risk introduced by long\ntext generation.\nPost-processing: To ensure completeness and co-\nherence in the generated text, we devise a gener-\nation loop that meticulously assesses the quality\nof the output. For example, if the generated text\nlacks an <EOS> (end-of-sentence) token, the model\nis prompted to regenerate the text. Furthermore, we\neliminate any special tokens introduced by Galac-\ntica during the process.\nWhile Galactica base has 1.3B parameters, it\nis still smaller than ChatGPT, which can result in\nless coherent outputs when generating longer text\nsegments. As a result, prompting the model to gen-\nerate a specific section with preceding sections as\ncontext yields better outcomes compared to pro-\nviding only the title as context and requesting the\nmodel to generate all three sections simultaneously.\n3.2.4 ChatGPT\nTo generate a cohesive document, we prompt Chat-\nGPT (OpenAI, 2022) with \"Write a document with\nthe title[TITLE ], including an abstract, an intro-\nduction, and a conclusion\", substituting [TITLE ]\nwith the desired title utterance. ChatGPT’s large\nsize (20B parameters) and strong ability to consider\ncontext eliminate the necessity of feeding previ-\nous output sections into the prompt for generating\nnewer ones.\nHyperparameters: For the entire generation pro-\ncess, we use the default temperature of 0.7.\nDespite not being explicitly trained for scientific\ntext generation, ChatGPT can produce extensive,\nhuman-like text in this domain. This capability\nlikely stems from the model’s large size, the exten-\nsive datasets it was trained on, and the incorpora-\ntion of reinforcement learning with human feed-\nback.\n3.2.5 GPT-3\nWe fine-tune an instance of GPT-3 (6.7B) (Brown\net al., 2020) with 178 real samples. Output papers\ngenerated through an iterative cascade process (like\nfor Galactica) present a much higher quality than\nthose forged in a single step (like for ChatGPT)\n(Shrestha and Zhou, 2022). Hence, we decide to\nopt for the latter strategy.\nPre/Post-Processing: To force the generation of\ncleaner outputs, we add a <END> token at the end\nof each input used for fine-tuning. GPT-3 mimics\nthis behavior and adds the token as well, and we\nremove every token added after generation <END>.\nWhile still not on par with ChatGPT-generated\noutputs, we report a high quality for GPT-3-crafted\npapers.\n4 Detection Experiments\nIn this section, we conduct experiments about iden-\ntifying the source of a given paper—i.e. determin-\ning whether it is fake or real. We start by defin-\ning data splits and subsets for training and testing,\nwhich are useful to evaluate generalization capa-\nbilities. Next, we outline the classifiers used as\nbaselines to measure performance on the bench-\nmark task. Finally, we examine the results in terms\nof performance and apply post-hoc explainability\nmethods to the classifiers to gain deeper insights\ninto the detection process.\n4.1 Data Splits and Generalization Tests\nWe divide our dataset (displayed in Table 1) into\nstandard trainand standard testsets for training\nand testing our classifiers, respectively. Further-\nmore, we aim to evaluate models on out-of-domain\n194\nDataset arXiv (real) ChatGPT (fake) GPT-2 (fake) SCIgen (fake) Galactica (fake) GPT-3 (fake)\nStandard train (TRAIN) 8k 2k 2k 2k 2k -\nStandard train subset (TRAIN-SUB) 4k 1k 1k 1k 1k -\nTRAIN without ChatGPT (TRAIN-CG) 8k - 2k 2k 2k -\nTRAIN plus GPT-3 (TRAIN+GPT3) 8k 2k 2k 2k 2k 1.2k\nStandard test (TEST) 4k 1k 1k 1k 1k -\nOut-of-domain GPT-3 only (OOD-GPT3) - - - - - 1k\nOut-of-domain real (OOD-REAL) 4k (parsing 2) - - - - -\nChatGPT only (TECG) - 1k - - - -\nTable 2: Overview of the datasets used to train and evaluate the classifiers. Each column represents the number of\npapers used per source. Concerning real papers, unless indicated, we use samples extracted with parsing 1 (see 3.1).\ntest data. To achieve this, we create various data\nsubsets by applying different splits to our bench-\nmark. All the splits utilized for our experiments are\ndetailed in Table 2. For instance, the reader can ob-\nserve the composition of a data split with no access\nto ChatGPT samples (TRAIN-CG) and test sets\ncomposed only of differently-parsed real papers\n(OOD-REAL), only ChatGPT papers (OOD-CG),\nor only GPT-3 ones (OOD-GPT3).\n4.2 Classifiers\nWe fine-tune GPT-3 (Brown et al., 2020), Galac-\ntica (Taylor et al., 2022), and RoBERTa (Liu et al.,\n2019) to perform the downstream task of classify-\ning scientific papers as fake or real based on their\ncontent (abstract, introduction, and conclusion sec-\ntions). We remind the reader that all titles are real.\nTo accommodate memory limitations, we im-\npose a restriction on the input tokens, resulting in\nthe truncation of longer texts. However, since the\naverage length of the combined input sections is\n900 tokens, this constraint does not lead to signifi-\ncant information loss.\n4.2.1 GPT-3\nWe fine-tune a GPT-3 (Brown et al., 2020) Ada\nmodel for the classification task. GPT-3 is fine-\ntuned in a causal manner, where the model is\nprompted with the concatenated paper sections\nalong with their corresponding label. This is set up\nas a binary classification where the output is a sin-\ngle token indicating whether the paper is real (0) or\nfake (1). During inference, the model generates a\nsingle token based on the sections of a given paper.\nAs fine-tuning GPT-3 models requires a paid\nAPI, we train it only on a smaller subset of our\ndataset (TRAIN-SUB) shown in Table 2. We limit\nthe number of input tokens to2,048 while retaining\nthe default hyperparameters provided by the API.\n4.2.2 Galactica\nWe adapt Galactica (Taylor et al., 2022) from a\ncausal language model that predicts probabilities\nfor each word in the vocabulary to a binary classi-\nfier with an output layer that predicts probabilities\nfor two labels: fake and real.\nThe model is provided with all sections as con-\ncatenated together with the corresponding label. Al-\nthough we retrain the output layer to accommodate\nthis change, this approach proves more memory-\nefficient compared to using an output layer that\nproduces probabilities for the entire vocabulary.\nHyperparameters. To cope with memory con-\nstraints, we limit the input number of tokens to\n2,048. Additionally, we adjust the batch size to 2\nwith gradient accumulation steps of 4 and enabled\nmixed precision. Additionally, we set the number\nof epochs to 4, weight decay to 0.01, and warm-up\nsteps to 1,000. Our initial learning rate is 5e−6.\n4.2.3 RoBERTa\nFinally, our third classifier is RoBERTa base (125M\nparameters) (Liu et al., 2019). RoBERTa is limited\nto 512 input tokens, meaning that all text exceeding\nthis limit is ignored. Our dataset exceeds this con-\nstraint for many entries. We choose to address the\nproblem by fine-tuning three separate RoBERTa\nmodels to classify the three sections individually,\nrather than retraining the input layer by enlarging\nthe input size. The mode of the three classifications\nis taken as a final output. We prompt each model\nwith the capitalized name of the section plus the\ncontent of the latter, e.g. Abstract: In this paper...\nHyperparameters. To fine-tune the RoBERTa\nbase, we set the number of epochs to 2, weight de-\ncay to 0.001, and batch size to 16. As with Galac-\ntica, the initial learning rate is 5e −6, and the\nwarmup steps 1,000.\n195\nModel Train Dataset TEST OOD-GPT3 OOD-REAL TECG\nGPT-3 (our) TRAIN-SUB 99.96% 25.9% 99.07% 100%\nGalactica (our) TRAIN 98.3% 24.6% 95.8% 83%\nGalactica (our) TRAIN+GPT3 98.5% 70% 92.1% 87.2%\nGalactica (our) TRAIN-CG 95% 11.1% 96.9% 42%\nRoBERTa (our) TRAIN 86% 23% 76% 100%\nRoBERTa (our) TRAIN+GPT3 68% 100% 36% 63%\nRoBERTa (our) TRAIN-CG 75% 32% 58% 88%\nDetectGPT - 61.5% 0% 99.92% 68.7%\nTable 3: Experiment results reported with accuracy metric. Out-of-domain experiments are highlighted in blue.\n4.3 Performance\nTable 3 presents a summary of the accuracies\nachieved by our models on various splits. We have\nto exclude the GPT-3 TRAIN+GPT3 and TRAIN-\nCG experiments due to limited OpenAI API credits.\nResults of our fine-tuned models are also compared\nwith DetectGPT as an existing zero-shot detection\nbaseline (Mitchell et al., 2023).\nAll models perform poorly on out-of-domain\npapers generated by GPT-3 curie (OOD-GPT3)\n(Shrestha and Zhou, 2022). This result supports\nthe findings of previous studies by Bakhtin et al.\n(2019) and Shrestha and Zhou (2022), which indi-\ncate that models trained on specific generators tend\nto overfit and perform poorly on data outside their\ntraining distribution. However, after training our\nGalactica and RoBERTa models with a few more\nGPT-3 examples, the models achieve higher accu-\nracies (70% and 100% respectively). It is worth\nnoting that our RoBERTa model exhibits excellent\nresults when evaluated on a dataset of ChatGPT-\ngenerated papers (TECG). The model achieves an\naccuracy of 88% without prior training on a similar\ndataset, and 100% accuracy when a similar dataset\nis included in the training (TRAIN). These results\noutperform Galactica in both scenarios.\nResults on OOD-REAL—i.e. real paper pro-\ncessed with a different parser—suggest that our\nmodels do not learn any strong features introduced\nby our PDF parser. DetectGPT overfits papers gen-\nerated with GPT-2 and sees any sample coming\nfrom a different source as real. Indeed, it performs\nwell on OOD-REAL and poorly on OOD-GPT3.\n4.4 Explainability Insights\nWe use LIME (Ribeiro et al., 2016) and SHAP\n(Lundberg and Lee, 2017) to inspect predictions\nmade by the three detectors. While these explana-\ntions fail to convey a concise overview, they are\nstill useful to notice patterns and similarities across\nsamples sharing labels and sources (Mosca et al.,\n2022b).\nOften, RoBERTa and Galactica models tend to\nclassify papers as real when the papers include\ninfrequent words and sentences starting with ad-\nverbs. Also, we notice that SHAP explanations\ncorresponding to real papers have all words with\nlow Shapley values. We believe this is intuitive as\na paper appears real if doesn’t contain any artifact\nthat strongly signals an AI source.\nOn the other hand, papers whose sections be-\ngin with \"In this paper,..\", \"In this work,..\", or \"In\nthis study,..\"are often marked as false. The same\ngoes for those containing repeated words, spelling\nmistakes, or word fragments such as \"den\", \"oly\",\n\"um\". Detectors are also able to spot incoherent\ncontent and context as well as sections that are\nunnaturally short and do not convey any specific\npoint.\nSeveral explanation instances can be found in the\nappendix B for further inspection. We choose not\nto provide an explanation for our GPT-3 classifier\nsince it requires many requests to OpenAI’s paid\nAPI.\n5 Limitations and Future Work\nDespite memory and GPU limitations presenting\nsignificant obstacles for our project, we were still\nable to create high-quality fake scientific papers.\nNonetheless, we believe there is room for improve-\nment in addressing such limitations.\nDue to the complexity of parsing PDFs, we are\ncurrently limited to specific sections (abstract, in-\ntroduction, conclusion) instead of complete papers.\nMoreover, processing entire publications would re-\nquire substantial computational efforts. We believe\nthat selecting sections dynamically at random in-\nstead of a fixed choice is worth exploring and will\n196\nbe the focus of future work.\nBeyond DetectGPT (Mitchell et al., 2023), other\nzero-shot text detectors such as GPTZero7 present\npromising solutions worth testing on our bench-\nmark dataset. However, at the time of writing, such\nsolutions are not available for experiments at scale.\nIn future work, we aim to address these lim-\nitations by exploring dynamic section selection,\nimproving papers’ quality, adding human-LLMs\nco-created samples, and investigating the potential\nof zero-shot text detectors like GPTZero as they\nbecome more accessible and scalable.\n6 Discussion, Ethical Considerations, and\nBroader Impact\nIt is important to emphasize that our work does\nnot condemn the usage of LLMs. The legitimacy\nof their usage should be addressed by regulatory\nframeworks and guidelines. Still, we strongly be-\nlieve it is crucial to develop countermeasures and\nstrategies to detect machine-generated papers to\nensure accountability and reliability in published\nresearch.\nOur benchmark dataset serves as a valuable re-\nsource for evaluating detection algorithms, con-\ntributing to the integrity of the scientific community.\nHowever, potential challenges include adversarial\nattacks and dataset biases (Mosca et al., 2022a; Hu-\nber et al., 2022). It is essential to develop robust\ncountermeasures and strive for a diverse, represen-\ntative dataset.\n7 Conclusion\nThis work introduced a benchmark dataset for iden-\ntifying machine-generated scientific papers in the\nLLM era. Our work creates a resource that allows\nresearchers to evaluate the effectiveness of detec-\ntion methods and thus support the trust and integrity\nin the scientific process.\nWe generated a diverse set of papers using\nboth SCIgen and state-of-the-art LLMs—ChatGPT,\nGalactica, GPT-2, and GPT-3. This ensures a\nvariety of sources and includes models capable\nof generating convincing content. We fine-tune\nand test several baseline detection models—GPT-\n3, Galactica, and RoBERTa—and compare their\nperformance to DetectGPT. The results demon-\nstrated varying degrees of success, with some mod-\nels showing remarkable performance on specific\n7https://gptzero.me\nsubsets while sometimes struggling with out-of-\ndomain data.\nBy providing a comprehensive platform for eval-\nuating detection techniques, we contribute to the\ndevelopment of robust and reliable methods for\nidentifying machine-generated content. Moving\nforward, we plan to address the current limitations\nand further enhance the utility of our benchmark\nfor the research community.\nWe release a repository containing our bench-\nmark dataset as well as the code used for experi-\nmental results8.\nAcknowledgements\nThis paper has been supported by the German Fed-\neral Ministry of Education and Research(BMBF,\ngrant 01IS17049). Additionally, we would like to\nthank Leslie McIntosh and the Holtzbrinck Pub-\nlishing Group for their guidance throughout our\nresearch journey.\nReferences\nDiego Raphael Amancio. 2015. Comparing the topo-\nlogical properties of real and artificially generated\nscientific manuscripts. Scientometrics, 105:1763–\n1779.\narXiv.org submitters. 2023. arxiv dataset.\nAnton Bakhtin, Sam Gross, Myle Ott, Yuntian\nDeng, Marc’Aurelio Ranzato, and Arthur Szlam.\n2019. Real or fake? learning to discriminate ma-\nchine from human generated text. arXiv preprint\narXiv:1906.03351.\nIz Beltagy, Kyle Lo, and Arman Cohan. 2019. SciB-\nERT: A pretrained language model for scientific text.\nIn Proceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP), pages 3615–\n3620, Hong Kong, China. Association for Computa-\ntional Linguistics.\nSteven Bird, Robert Dale, Bonnie Dorr, Bryan Gibson,\nMark Joseph, Min-Yen Kan, Dongwon Lee, Brett\nPowley, Dragomir Radev, and Yee Fan Tan. 2008.\nThe ACL Anthology reference corpus: A reference\ndataset for bibliographic research in computational\nlinguistics. In Proceedings of the Sixth International\nConference on Language Resources and Evaluation\n(LREC’08), Marrakech, Morocco. European Lan-\nguage Resources Association (ELRA).\nSid Black, Gao Leo, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\n8huggingface.co/datasets/tum-nlp/IDMGSP\n197\nScale Autoregressive Language Modeling with Mesh-\nTensorflow.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nGuillaume Cabanac and Cyril Labbé. 2021. Prevalence\nof nonsensical algorithmically generated papers in\nthe scientific literature. Journal of the Association for\nInformation Science and Technology, 72(12):1461–\n1476.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nArman Cohan and Nazli Goharian. 2015. Scientific\narticle summarization using citation-context and arti-\ncle’s discourse structure. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 390–400, Lisbon, Portugal.\nAssociation for Computational Linguistics.\nIsmail Dergaa, Karim Chamari, Piotr Zmijewski, and\nHelmi Ben Saad. 2023. From human writing to ar-\ntificial intelligence generated text: examining the\nprospects and potential threats of chatgpt in academic\nwriting. Biology of Sport, 40(2):615–622.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nTiziano Fagni, Fabrizio Falchi, Margherita Gambini, An-\ntonio Martella, and Maurizio Tesconi. 2021. Tweep-\nfake: About detecting deepfake tweets. Plos one,\n16(5):e0251415.\nSebastian Gehrmann, Hendrik Strobelt, and Alexan-\nder M Rush. 2019. Gltr: Statistical detection\nand visualization of generated text. arXiv preprint\narXiv:1906.04043.\nAnna Glazkova and Maksim Glazkov. 2022. Detect-\ning generated scientific papers using an ensemble\nof transformer models. In Proceedings of the Third\nWorkshop on Scholarly Document Processing, pages\n223–228, Gyeongju, Republic of Korea. Association\nfor Computational Linguistics.\nLukas Huber, Marc Alexander Kühn, Edoardo Mosca,\nand Georg Groh. 2022. Detecting word-level adver-\nsarial text attacks via SHapley additive exPlanations.\nIn Proceedings of the 7th Workshop on Represen-\ntation Learning for NLP, pages 156–166, Dublin,\nIreland. Association for Computational Linguistics.\nDaphne Ippolito, Daniel Duckworth, Chris Callison-\nBurch, and Douglas Eck. 2019. Automatic detection\nof generated text is easiest when humans are fooled.\narXiv preprint arXiv:1911.00650.\nYury Kashnitsky, Drahomira Herrmannova, Anita\nde Waard, George Tsatsaronis, Catriona Catriona\nFennell, and Cyril Labbe. 2022. Overview of the\nDAGPap22 shared task on detecting automatically\ngenerated scientific papers. In Proceedings of the\nThird Workshop on Scholarly Document Processing,\npages 210–213, Gyeongju, Republic of Korea. Asso-\nciation for Computational Linguistics.\nNitish Shirish Keskar, Bryan McCann, Lav R. Varshney,\nCaiming Xiong, and Richard Socher. 2019. Ctrl: A\nconditional transformer language model for control-\nlable generation. ArXiv, abs/1909.05858.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen,\nJonathan Katz, Ian Miers, and Tom Goldstein. 2023.\nA watermark for large language models. arXiv\npreprint arXiv:2301.10226.\nYang Liu. 2019. Fine-tune bert for extractive summa-\nrization. arXiv preprint arXiv:1903.10318.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nVijini Liyanage, Davide Buscaldi, and Adeline\nNazarenko. 2022. A benchmark corpus for the de-\ntection of automatically generated text in academic\npublications. In Proceedings of the Thirteenth Lan-\nguage Resources and Evaluation Conference, pages\n4692–4700, Marseille, France. European Language\nResources Association.\nKyle Lo, Lucy Lu Wang, Mark Neumann, Rodney Kin-\nney, and Daniel Weld. 2020. S2ORC: The semantic\nscholar open research corpus. In Proceedings of the\n58th Annual Meeting of the Association for Compu-\ntational Linguistics, pages 4969–4983, Online. Asso-\nciation for Computational Linguistics.\nScott Lundberg and Su-In Lee. 2017. A unified ap-\nproach to interpreting model predictions. Interna-\ntional Conference on Machine Learning.\nStephen MacNeil, Andrew Tran, Juho Leinonen, Paul\nDenny, Joanne Kim, Arto Hellas, Seth Bernstein,\nand Sami Sarsa. 2022. Automatically generating cs\nlearning materials with large language models. arXiv\npreprint arXiv:2212.05113.\nHimanshu Maheshwari, Bhavyajeet Singh, and Va-\nsudeva Varma. 2021. SciBERT sentence representa-\ntion for citation context classification. In Proceed-\nings of the Second Workshop on Scholarly Document\n198\nProcessing, pages 130–133, Online. Association for\nComputational Linguistics.\nJoshua Maynez, Shashi Narayan, Bernd Bohnet, and\nRyan McDonald. 2020. On faithfulness and factu-\nality in abstractive summarization. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 1906–1919, On-\nline. Association for Computational Linguistics.\nEric Mitchell, Yoonho Lee, Alexander Khazatsky,\nChristopher D Manning, and Chelsea Finn. 2023.\nDetectgpt: Zero-shot machine-generated text detec-\ntion using probability curvature. arXiv preprint\narXiv:2301.11305.\nEdoardo Mosca, Shreyash Agarwal, Javier\nRando Ramírez, and Georg Groh. 2022a. “that is a\nsuspicious reaction!”: Interpreting logits variation\nto detect NLP adversarial attacks. In Proceedings\nof the 60th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers),\npages 7806–7816, Dublin, Ireland. Association for\nComputational Linguistics.\nEdoardo Mosca, Ferenc Szigeti, Stella Tragianni, Daniel\nGallagher, and Georg Groh. 2022b. SHAP-based ex-\nplanation methods: A review for NLP interpretabil-\nity. In Proceedings of the 29th International Con-\nference on Computational Linguistics, pages 4593–\n4603, Gyeongju, Republic of Korea. International\nCommittee on Computational Linguistics.\nMinh Tien Nguyen and Cyril Labbé. 2016. Engineer-\ning a tool to detect automatically generated papers.\nIn BIR 2016 Bibliometric-enhanced Information Re-\ntrieval.\nOpenAI. 2022. Chatgpt. https://openai.com/blog/\nchat-ai/. Accessed on February 26, 2023.\nOpenAI. 2023. Gpt-4 technical report.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nMarco Tulio Ribeiro, Sameer Singh, and Carlos\nGuestrin. 2016. \"why should i trust you?\": Explain-\ning the predictions of any classifier. In Proceedings\nof the 22nd ACM SIGKDD International Conference\non Knowledge Discovery and Data Mining, pages\n1135–1144. ACM.\nP. Rudduck. 2021. Pymupdf: Python bindings for\nthe mupdf renderer. https://pypi.org/project/\nPyMuPDF/. Accessed on March 7, 2023.\nTarek Saier and Michael Färber. 2019. Bibliometric-\nenhanced arxiv: A data set for paper-based and\ncitation-based tasks. In BIR@ ECIR, pages 14–26.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. Distilbert, a distilled version\nof bert: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\narXiv preprint arXiv:2211.05100.\nRahul Babu Shrestha and Yutong Zhou. 2022. Gener-\nation and explainable detection of artificial research\npapers. Master Practical Course - Natural Language\nProcessing Applications (TUM). Request report at:\nedoardo.mosca@tum.de.\nChris Stokel-Walker. 2022. Ai bot chatgpt writes smart\nessays-should academics worry? Nature.\nJeremy Stribling and Daniel Aguayo. 2021. Rooter:\nA methodology for the typical unification of access\npoints and redundancy.\nJeremy Stribling, Max Krohn, and Dan Aguayo. 2005.\nScigen - an automatic cs paper generator. https:\n//pdos.csail.mit.edu/archive/scigen/. Ac-\ncessed: March 1, 2023.\nIlya Sutskever, Oriol Vinyals, and Quoc V Le. 2014.\nSequence to sequence learning with neural networks.\nAdvances in neural information processing systems,\n27.\nBen Swanson, Kory Mathewson, Ben Pietrzak, Sherol\nChen, and Monica Dinalescu. 2021. Story centaur:\nLarge language model few shot learning as a cre-\native writing tool. In Proceedings of the 16th Confer-\nence of the European Chapter of the Association for\nComputational Linguistics: System Demonstrations,\npages 244–256.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. arXiv\npreprint arXiv:2211.09085.\nRan Tian, Shashi Narayan, Thibault Sellam, and\nAnkur P Parikh. 2019. Sticking to the facts: Con-\nfident decoding for faithful data-to-text generation.\narXiv preprint arXiv:1910.08684.\nA Waswani, N Shazeer, N Parmar, J Uszkoreit, L Jones,\nA Gomez, L Kaiser, and I Polosukhin. 2017. Atten-\ntion is all you need. In NIPS.\nKyle Williams and C Lee Giles. 2015. On the use of\nsimilarity search to detect fake scientific papers. In\nSimilarity Search and Applications: 8th International\nConference, SISAP 2015, Glasgow, UK, October 12-\n14, 2015, Proceedings 8, pages 332–338. Springer.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, and Clement Delangue. 2019. Hugging\nface’s transformers: State-of-the-art natural language\nprocessing. https://github.com/huggingface/\ntransformers.\n199\nRowan Zellers, Ari Holtzman, Hannah Rashkin,\nYonatan Bisk, Ali Farhadi, Franziska Roesner, and\nYejin Choi. 2019. Defending against neural fake\nnews. Advances in neural information processing\nsystems, 32.\n200\nA Appendix: Generation Examples\nIn this section, we present examples of text that were generated using the models we employed. For\ngenerating text with the Galactica model, an overview of the hyperparameters used is provided in Table 4.\nInput Section(s) Output Section Tokenizer max_input_size max_new_tokens do_sample temperature top_k top_p no_repeat_ngram_size early_stoppingTitle Abstract 64 512 True - - - - -Title + Abstract Introduction 576 (64 + 512) 1024 True 0.7 25 0.9 10 TrueTitle + Abstract + Introduction Conclusion 1600 (64 + 512 + 1024) 1024 True 0.7 25 0.9 10 True\nTable 4: Hyperparameters used to generate each paper section in the Galactica model. Each row corresponds to a\ndecoding of a section based on the previous input sections. Here we used parameters of the MODEL .GENERATE\nfunction provided by Huggingface (Wolf et al., 2019).\nA.1 ChatGPT\nFigures 3 and 4 provide examples of prompts that can be used to generate paper sections using ChatGPT\nFigure 3: Example 1 of Prompting ChatGPT to produce sections of a scientific paper given the paper title.\n201\nFigure 4: Example 2 of Prompting ChatGPT to produce sections of a scientific paper given the paper title.\nA.2 Galactica\nEach subsection shows how Galactica is prompted to generate the abstract, the introduction and the\nconclusion sections.\nA.2.1 Abstract Generation\nINPUT:\nTitle: On the Global Structure of Hopf Hypersurfaces in Complex Space Form\nOUTPUT:\nAbstract: For a generic hypersurface in complex space form, all Hopf hypersurfaces are proved to be\nhomogeneous or Hopf homogeneous. As a consequence, it is shown that the generalized Sasakian\nmetric is a Ricci soliton on a Hopf hypersurface.\nA.2.2 Introduction Generation\nINPUT:\nTitle: On the Global Structure of Hopf Hypersurfaces in Complex Space Form\nAbstract: For a generic hypersurface in complex space form, all Hopf hypersurfaces are\nproved to be homogeneous or Hopf homogeneous. As a consequence, it is shown that the generalized\nSasakian metric is a Ricci soliton on a Hopf hypersurface.\nIntroduction:\nOUTPUT:\nA Hopf hypersurface is a hypersurface which admits a Hopf field, that is, a vector field such that[ξ,ξ]\nis a normal vector field. The Hopf field is unique up to multiplication by a positive constant. The\nexistence of Hopf fields is equivalent to the existence of a 1-form ηsuch that η∧dηn−1 is a volume\n202\nform. Let Mn be a hypersurface in a complex space formM2n+1(c) with constant sectional curvature\nc. We denote by gthe metric induced by the Riemannian metric ofM2n+2(c). The Hopf field is given\nby ξ= 1√nJ∇f, where J is the complex structure on M2n+3 and f is a function on Mn ⊂M2n+1.\nIn [1], Wang and Chen showed that a Hopf hypersurface inM2n+l(c) is homogeneous if and only\nif the Hopf field is parallel. In this paper, we study the structure of Hopf hypersurfaces in complex\nspace form. We show that a Hopf hypersurface is homogeneous if and only if it is Hopf homogeneous.\nWe also show that a Hopf hypersurface is Hopf homogeneous if and only if the generalized Sasakian\nmetric is a Ricci solitons on it.\nA.2.3 Conclusion Generation\nINPUT:\nTitle: On the Global Structure of Hopf Hypersurfaces in Complex Space Form\nAbstract: For a generic hypersurface in complex space form, all Hopf hypersurfaces are\nproved to be homogeneous or Hopf homogeneous. As a consequence, it is shown that the generalized\nSasakian metric is a Ricci soliton on a Hopf hypersurface.\nIntroduction:\nA Hopf hypersurface is a hypersurface which admits a Hopf field, that is, a vector field\nsuch that [ξ,ξ] is a normal vector field. The Hopf field is unique up to multiplication by a positive\nconstant. The existence of Hopf fields is equivalent to the existence of a 1-form η such that\nη∧dηn−1 is a volume form. Let Mn be a hypersurface in a complex space form M2n+1(c) with\nconstant sectional curvature c. We denote by g the metric induced by the Riemannian metric\nof M2n+2(c). The Hopf field is given by ξ = 1√nJ∇f, where J is the complex structure on\nM2n+3 and f is a function on Mn ⊂ M2n+1. In [1], Wang and Chen showed that a Hopf\nhypersurface in M2n+l(c) is homogeneous if and only if the Hopf field is parallel. In this paper,\nwe study the structure of Hopf hypersurfaces in complex space form. We show that a Hopf\nhypersurface is homogeneous if and only if it is Hopf homogeneous. We also show that a Hopf hy-\npersurface is Hopf homogeneous if and only if the generalized Sasakian metric is a Ricci solitons on it.\nConclusion:\nOUTPUT:\nWe obtain a complete classification of Hopf hypersurfaces in complex space forms.\nA.3 GPT-3\nThe following frame shows GPT-3 training prompt. the input label is separated from the input by the\nseparator token (###).\nAbstract:\nFor a generic hypersurface in complex space form, all Hopf hypersurfaces are proved to be\nhomogeneous or Hopf homogeneous. As a consequence, it is shown that the generalized Sasakian\nmetric is a Ricci soliton on a Hopf hypersurface.\nIntroduction:\nA Hopf hypersurface is a hypersurface which admits a Hopf field, that is, a vector field\nsuch that [ξ,ξ] is a normal vector field. The Hopf field is unique up to multiplication by a positive\nconstant. The existence of Hopf fields is equivalent to the existence of a 1-form η such that\nη∧dηn−1 is a volume form. Let Mn be a hypersurface in a complex space form M2n+1(c) with\n203\nconstant sectional curvature c. We denote by g the metric induced by the Riemannian metric\nof M2n+2(c). The Hopf field is given by ξ = 1√nJ∇f, where J is the complex structure on\nM2n+3 and f is a function on Mn ⊂ M2n+1. In [1], Wang and Chen showed that a Hopf\nhypersurface in M2n+l(c) is homogeneous if and only if the Hopf field is parallel. In this paper,\nwe study the structure of Hopf hypersurfaces in complex space form. We show that a Hopf\nhypersurface is homogeneous if and only if it is Hopf homogeneous. We also show that a Hopf hy-\npersurface is Hopf homogeneous if and only if the generalized Sasakian metric is a Ricci solitons on it.\nConclusion:\nFor a generic hypersurface in complex space form, all Hopf hypersurfaces are proved to be\nhomogeneous or Hopf homogeneous. As a consequence, it is shown that the generalized Sasakian\nmetric is a Ricci soliton on a Hopf hypersurface.\n###\n1\nA.4 GPT-2\nHere below, an example of how the fine-tuned GPT-2 was used to generate the abstract, the introduction\nand the conclusion sections.\nA.4.1 Abstract Generation\nINPUT:\nCompetitive Multi-Agent Load Balancing with Adaptive Policies in Wireless Networks <|sep|>\nOUTPUT:\nIn this paper, we propose adaptive multi-agent load balancing (AML) based on adaptive policies\nto achieve better performance. The adaptive policies can be designed to maximize the number of\nagent users and to reduce the number of latency. We present a comprehensive evaluation of AML\non three wireless networks. We demonstrate that the adaptive policies outperform state-of-the-art\nAML strategies by an average of 6.5% and by an average of 2.5% when compared to state-of-the-art\nstrategies. The performance of adaptive policies is evaluated on the three wireless networks with\nmultiple users and multiple latency.\nA.4.2 Introduction Generation\nINPUT:\nCompetitive Multi-Agent Load Balancing with Adaptive Policies in Wireless Networks <|sep|>\nOUTPUT:\nWireless networks are increasingly becoming the backbone of the modern world. The wireless\nnetworks have been the main source of energy for many years, and they are the main source of power\nfor many businesses. However, wireless networks are also the main source of information for many\npeople. For example, people in many countries rely on a variety of information services, such as\nsocial networks, social media, and e-mail. However, the information services are not available in all\ncountries. Therefore, the information services are not available for all countries.\nA.4.3 Conclusion Generation\nINPUT:\n204\nCompetitive Multi-Agent Load Balancing with Adaptive Policies in Wireless Networks <|sep|>\nOUTPUT:\nIn this paper, we propose adaptive policies that optimize the network performance of different types\nof wireless networks. We show that adaptive policies can achieve competitive performance for\ndifferent types of wireless networks. The proposed adaptive policies have been tested in several\nreal-world wireless networks. We find that adaptive policies outperform state-of-the-art policies for\nthe most common wireless networks. We also show that adaptive policies can be used to optimize\nthe network performance of wireless networks by adapting their performance to different types of\nwireless networks. The proposed adaptive policies are able to achieve competitive performance for\ndifferent types of wireless networks.\nB Appendix: Explainability Results\nSHAP and LIME explanations of our classifiers.\nFigure 5: RoBERTa: Example of SHAP explanation on a real abstract correctly classified.\nFigure 6: RoBERTa: Example of SHAP explanation on a real misclassified abstract.\nFigure 7: RoBERTa: Example of SHAP explanation on a SCIgen generated abstract correctly classified.\n205\nFigure 8: RoBERTa: Example of SHAP explanation on a GPT-2 generated abstract correctly classified\nFigure 9: RoBERTa: Example of SHAP explanation on a Galactica generated abstract correctly classified.\nFigure 10: RoBERTa: Example of SHAP explanation on a ChatGPT generated abstract correctly classified.\nFigure 11: Galactica: Example of SHAP explanation on a real paper correctly classified.\nFigure 12: Galactica: Example of SHAP explanation on a misclassified real paper.\nFigure 13: Galactica: Example of SHAP explanation on a Galactica generated paper correctly classified.\n206\nFigure 14: Galactica: Example of SHAP explanation on a misclassified Galactica generated paper.\nFigure 15: RoBERTa: Example of LIME explanation on a real abstract correctly classified.\nFigure 16: RoBERTa: Example of LIME explanation on a SCIgen generated abstract correctly classified.\nFigure 17: RoBERTa: Example of LIME explanation on a GPT-2 generated abstract correctly classified.\nFigure 18: RoBERTa: Example of LIME explanation on a Galactica generated abstract correctly classified.\nFigure 19: RoBERTa: Example of LIME explanation on a ChatGPT generated abstract correctly classified.\n207",
  "topic": "Benchmark (surveying)",
  "concepts": [
    {
      "name": "Benchmark (surveying)",
      "score": 0.8314327597618103
    },
    {
      "name": "Computer science",
      "score": 0.7723735570907593
    },
    {
      "name": "Focus (optics)",
      "score": 0.6168583035469055
    },
    {
      "name": "Generalization",
      "score": 0.606043815612793
    },
    {
      "name": "Generative grammar",
      "score": 0.6056175231933594
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5990712642669678
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5583599209785461
    },
    {
      "name": "Strengths and weaknesses",
      "score": 0.45335614681243896
    },
    {
      "name": "Data science",
      "score": 0.4489981234073639
    },
    {
      "name": "Machine learning",
      "score": 0.4355301260948181
    },
    {
      "name": "Scientific literature",
      "score": 0.4341515600681305
    },
    {
      "name": "Information retrieval",
      "score": 0.3772399425506592
    },
    {
      "name": "Natural language processing",
      "score": 0.3725208044052124
    },
    {
      "name": "Epistemology",
      "score": 0.06686556339263916
    },
    {
      "name": "Oceanography",
      "score": 0.0
    },
    {
      "name": "Optics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I62916508",
      "name": "Technical University of Munich",
      "country": "DE"
    }
  ],
  "cited_by": 13
}