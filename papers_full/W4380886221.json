{
  "title": "“Conversing” with Qualitative Data: Enhancing Qualitative Research through Large Language Models (LLMs)",
  "url": "https://openalex.org/W4380886221",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2244304941",
      "name": "Adam Hayes",
      "affiliations": [
        "Hebrew University of Jerusalem"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2900044350",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W2117667023",
    "https://openalex.org/W4229011615",
    "https://openalex.org/W2014738715",
    "https://openalex.org/W1986937683",
    "https://openalex.org/W4377098551",
    "https://openalex.org/W4365512576",
    "https://openalex.org/W6850369891",
    "https://openalex.org/W1803273808",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3101767999",
    "https://openalex.org/W4389636360",
    "https://openalex.org/W4379255800",
    "https://openalex.org/W4323043839",
    "https://openalex.org/W4281564584",
    "https://openalex.org/W4365601444",
    "https://openalex.org/W4378473947",
    "https://openalex.org/W4376504550",
    "https://openalex.org/W4319083882",
    "https://openalex.org/W4379539276",
    "https://openalex.org/W4379258778",
    "https://openalex.org/W3177468621"
  ],
  "abstract": "In this paper, I explore the transformative potential of Large Language Models (LLMs) such as ChatGPT in the realm of qualitative research, particularly in the social sciences. These generative AI models, trained on extensive textual data, have the unique ability to \"understand,\" generate, and manipulate human-like text, offering unprecedented opportunities for data analysis and interpretation. I argue that LLMs, with this capacity, can significantly enhance the depth and efficiency of qualitative analysis. They can quickly identify patterns, themes, and sentiments in the data, providing a level of nuance that can be challenging to achieve with manual coding. Furthermore, their ability to generate human-like text can be used to simulate social interactions, create engaging presentations of research findings, and even \"converse\" with the data in a natural and flexible way. Indeed a central contribution of this paper lies in exploring this novel concept of \"asking questions of\" or \"conversing with\" text-based data, which opens up new avenues for qualitative research and analysis. This interactive capability of LLMs provides a transformative approach to topic coding and content analysis, allowing researchers to pose complex, nuanced questions to their data and receive responses in natural language. Ethical considerations and limitations are also discussed.",
  "full_text": "1 \n \n“Conversing” with Qualitative Data: \nEnhancing Qualitative Sociological Research through Large Language Models (LLMs) \n \nAdam S. Hayes, PhD \nThe Hebrew University of Jerusalem, Israel \nDept. of Sociology and Anthropology \nJune 12, 2023 \n \n \nLarge Language Models (LLMs) such as ChatGPT are a new type of generative AI that \ncan produce and manipulate human- like text (Ray, 2023; Van Dis, et al., 2023) . They can also \n“understand” natural language and extract relevant information from various data sources, such as \nweb pages, social media posts, news articles, transcripts, and other documents . Because of their \ninherent ability to analyze and interpret language, LLMs  have the pote ntial to revolutionize \nqualitative data analysis, as they can perform tasks such as text summarization, sentiment analysis, \ndiscourse analysis, question answering, and text generation with high accuracy and fluency.  \nMoreover, LLMs can help researchers generate new insights and hypotheses from existing data by \nusing techniques such as abstractive reasoning, analogy ma king, and counterfactual thinking. \nGiven these new capabilities, and the fact that these tools are becoming increasingly accessible \nand affordable, the application of LLMs in qualitative research should be considered and explored. \nQualitative research is a core approach in sociology and other social sciences. It involves \ncollecting rich, detailed information  about social phenomena through techniques such as \ninterviews, observations, and textual analysis. However, these techniques can also be labor -\nintensive and time-consuming, as they often require manual coding and interpretation, which can \nbe subjective, prone to biases, and inconsistent (Belotto, 2018). \n2 \n \n \nLLMs offer several advantages over traditional tools used for qualitative analysis. First, \nthey can process large volumes of text data quickly and consistently, reducing the time and effort \nrequired for analysis. Second, they can learn from the data itself without explicit instruction (i.e., \nunsupervised and “one-shot”), identifying—and then also interpreting—patterns and themes \nwithout predefined categories, dictionaries, or codebooks. This can lead to more objective analysis, \nas the models are not constrained by a particular analyst’s assumptions or expectations. Third, they \ncan capture subtle and unexpected insights from the data that might not be apparent to human \nresearchers, providing a level of depth and nuance that can be dif ficult to achieve with manual \ncoding. Finally, they allow researchers to directly engage with their corpus of data, enabling a \ndynamic and interactive form of analysis. This means that researchers can pose complex, nuanced \nquestions to their data and receive responses in natural language, transforming the traditional one-\nway process of data analysis into a two -way conversation. This interactive capability not only \nmakes the analysis process more engaging and intuitive, but also opens up new possibilities for \nexploration and discovery, potentially leading to more innovative and impactful research findings. \nLLMs are still very new (Chat -GPT was made widely available to the public only in the \nfirst quarter of 2023), but some researchers are already exploring how they can be applied to social \nscience. As Ziems, et al. (2023) argue, “ Large Language Models (LLMs) like Chat -GPT are \ncapable of successfully performing many language processing tasks ze ro-shot (without the need \nfor training data). If this capacity also applies to the coding of social phenomena like \npersuasiveness and political ideology, then LLMs could effectively transform  Computational \nSocial Science. ” They also propose that human- AI collaboration could greatly improve the \nefficiency and reliability of text analysis. Similarly, Wadwha et al. (2023) suggest that LLMs can \n3 \n \nreconcile disagreement among human coders , where they are able to  “achieve near -human \nperformance.” Bail (2023) further argues that generative AI can also help social scientists with \nmore mundane tasks such as acquiring advanced programming skills or writing more effective \nprose; and Liu and Shah (2023) explore the possibility of using LLMs to aid in the peer review of \nacademic papers. \nEmpirically, Large Language Models (LLMs) like ChatGPT-4 have already demonstrated \nremarkable capabilities in annotating textual data. Törnberg (2023) found that ChatGPT -4 \noutperforms human coders in terms of accuracy, reliability, and bias. The study highlighted the \nmodel's ability to correctly annotate messages th at require reasoning based on contextual \nknowledge and inferences around the author's intentions, abilities traditionally seen as uniquely \nhuman. Törnberg suggests that LLMs can enable social scientists to approach textual data in \ninnovative ways, facilitating interpretive research on a large scale.  Similarly, Rathje et al. (2023) \ndemonstrated that publicly available LLMs like ChatGPT -4 can accurately i dentify and label \npsychological constructs found  in text across various languages, such as  sentiment, emotions, and \noffensiveness. The performance of LLMs in these tasks was found to be superior to dictionary-\nbased methods and comparable to, if not better than, fine-tuned machine learning models used for \nspecific text analysis. \nTraditional unsupervised methods for qualitative analysis, such as Latent Dirichlet \nAllocation (LDA) and Non- negative Matrix Factorization (NMF), primarily focus on statistical \npatterns to identify semantic correlations or group words and phrases that appear together based \non frequency distributions (DiMaggio, Nag, and Blei, 2013; Egger and Yu, 2022). However, these \nmethods treat documents  simply as unstructured collections of tokens , or “ bags of words ” \n4 \n \n(McFarland, et al., 2013) —and may thus oversimplify or overlook linguistic nuances such as \ncontext, syntax, and lexical ambiguity. \nLarge Language Models  offer a more comprehensive understanding of language, \nsemantics, and context. Trained on an immense corpus of textual data, they acquire not just \nstatistical patterns but also the complexities of natural language  and the dynamics of human \ncommunication and expression. They can “ understand” synonyms, idioms, metaphor, sentiment, \nand to some extent, cultural context, which allow them to analyze and code qualitative data with a \nhigher degree of precision and sensitivity to nuance (see Bender, et al., 2021; Brown, et al., 2020). \nAn LLM can disambiguate  the meaning behind certain words or phrases, distinguish between \nliteral and figurative language, and understand the context in which certain words are used. In \ncontrast, a method like LDA might treat 'good' and 'not good' as related to the same topic, but an \nLLM would correctly identify the latter as a negation.  \nImportantly, unlike LDA and other  conventional methods, LLMs can respond to and \ngenerate human-like text, which allows researchers to interact with the model and the data it had \nbeen fed  in a dynamic, conversational manner. This  interactive capability provides  a \ntransformative approach to qualitative tasks such as topic coding and content analysis, allowing \nresearchers to pose complex, nuanced questions to their data and receive responses in natural \nlanguage. \nIt is this prospect, of “asking” or “conversing with” your qualitative data, that this paper \nseeks to open up. Traditionally, \"asking questions of your data\" has been associated with an \nexploratory approach to quantitative data analysis (e.g., Leinhardt and Wasserman, 1979). In this \ncontext, “questions” often refer to hypotheses or assumptions about the data, such as “What is the \naverage value of this variable?”, “Is there a significant correlation between these two variables?”, \n5 \n \nor “Can I predict this outcome based on these inputs?” It might  also include building tables of \nsummary statistics or creating data visualizations. \nThe emergence of Large Language Models has redefined the concept of “asking questions \nof your data,” extending its reach into the realm of qualitative research in a new and potent manner. \nThis advancement enables researchers to engage with their qualitative data through natural \nlanguage inquiries and prompts, and in return, receive responses from the LLM in a similar natural \nlanguage format. For example, a researcher could input a dataset of interview transcripts into an \nLLM, and then pose questions to the model to identify and extract the primary themes discussed \nby the interviewees or to encapsulate the interviewees' perspectives on a specific topic. The \nresearcher could further instruct the model to juxtapose the viewpoints of various interviewees, or \nto pinpoint any shifts in the interviewees' perspectives over time. The potential applications are \nextensive and flexible, constrained only by the researcher ’s creativity and the scope  of their \nresearch questions. Furthermore, this method of interacting with qualitative data  does not require \ntechnical skills, coding experience, or specialized knowledge. While a rudimentary understanding \nof how LLMs operate is beneficial , the actual process of posing questions and interpreting \nresponses can be executed using natural language, making it accessible to researchers across a \nbroad spectrum of backgrounds. \n \nLLMs and Qualitative Data Analysis \nLarge Language Models (LLMs) can greatly assist in the analysis of qualitative data. They \ncan identify patterns, themes, and trends in textual data, automatically coding and categorizing  \npassages of text based on their semantic context —tasks that can be time -consuming and \nchallenging when done manually. They can also perform more complex tasks such as sentiment \n6 \n \nanalysis, topic modeling, and text classific ation—and possess the capacity to extract nuances in \nlanguage, making it possible to discern subtle patterns and associations that may be difficult for \nhuman analysts to notice. For example, an LLM could be tasked to recognize and classify different \ntypes of news sources, such as mainstream, alternative, or fake news, based on their latent content \nand style, which could be difficult human analysts to easily classify due to the sheer volume of \ndata and the subtlety of stylistic differences. \nBut LLMs also present a transformative approach to qualitative analysis by enabling \nresearchers to actively engage in a  sort of  dynamic dialogue directly with their data. This \ninteraction is not limited to simple queries like “what are the main themes in these texts?” or “how \nfrequently does this specific term appear?”  Rather, it opens up a new dimension of expl oration \nwhere researchers can pose complex, nuanced questions to their data corpus and receive responses \nin natural language. For example, an LLM can be fed with documents like letters, diaries, interview \ntranscripts, or other primary sources and then be a sked questions about specific topics, trends, or \npatterns that appear in and across these sources. If a researcher is analyzing the Civil War era, they \nmight ask the LLM to summarize the common sentiments expressed by soldiers of a certain rank \nwithin their personal letters and see how they differ from those of other ranks, or from civilians, \nor soldiers on the opposing side . Alternatively, a researcher studying a corpus of 19th- century \nBritish novels might instruct the LLM to identify patterns in how different genders or classes are \ndepicted across these works. They could also ask the LLM to compare and contrast the themes and \nstyles of different authors, genres, or p eriods, or to generate summaries or critiques of specific \nnovels or characters. They could even ask the LLM to imagine how the novels would be different \nif they were written in a different time, place, or perspective. For example, they might ask, “How \nwould Jane Eyre be different if it was written in the 21st century?” or “How would Pride and \n7 \n \nPrejudice be different if it was told from Mr. Darcy’s point of view?” This could help to explore \nthe creative and imaginative aspects of the novels, as well as their historical and cultural \nimplications. \nOr, consider a researcher studying climate change using a dataset of news articles. With an \nLLM, they can ask the model not only to identify the main arguments for and against climate \nchange presented in the articles, or to track how the coverage of climate change has evolved over \ntime, but also more elaborate questions, such as “What are the most frequently cited sources of \ninformation on climate change?” or “How do the narratives around climate change differ between \ncountries or regions?” Furthermore, LLMs can facilitate a deeper theoretical engagement with the \ndata. A researcher could use an LLM to explore how their findings align or contrast with different \ntheoretical perspectives. For example, they might ask, “How does framing theory explain the \npresentation of climate change issues in these articles?” or “How does so cial constructionism \ninterpret the discourses around climate change in this dataset?” \nThis dynamic engagement can thus  extend to creative and hypothetical scenarios. A \nresearcher could propose hypothetical situations to an LLM and ask it to generate plausible \nresponses or outcomes based on patterns it has learned from the data. For instance, an LLM trained \non legal cases could be asked how a specific case might have turned out differently if a particular \npiece of evidence were introduced earlier  or later, or if a different judge or jury were assigned to \nthe case. The LLM could then generate possible scenarios and explain the reasoning behind them. \nThis way, the researcher can explore counterfactuals and alternative perspectives that might \notherwise be overlooked or inaccessible. As another example , a sociologist studying social \nmovements could use an LLM to analyze a large corpus of protest slogans, speeches, or social \nmedia posts, asking the model to identify the main grievances, demands, and ideologies expressed \n8 \n \nby the protesters. They could then ask the model to generate a hypothetical speech or slogan that \nwould be representative of the movement, based on the patterns it has identified in the data. This \ncould provide a unique insight into the core messages and rhetorical strategies of the movement. \nBy actively interacting with textual data through an LLM, researchers can explore their \ndatasets in a more engaged, direct, and potentially insightful way, transforming the way qualitative \nresearch is conducted. The dynamic dialogue that LLMs enable presents an unprecedented \nopportunity for researchers to ask deeper, more complex questions and to uncover new insights in \ntheir data. \nAn Example of Conversing with Qualitative Data  \nTo illustrate how this might work, I selected  a series of ten  actual interview transcripts from a \nresearch project chosen at random from a  publicly available Open Access data repository, called \nFigShare, entitled “I’m Cited, Therefore I Am!: How Publishing Constructs the Scientific Identity” \n(Bowley, 2015 / https://figshare.com/articles/dataset/I/1522638\n). The provided abstract to the \nthesis states:  \nThis phrase “publish or perish” is one of the most enduring expressions in academia. With \nthe rise of audit culture in higher education it has been argued that academic value is \nincreasingly becoming \"monetised\" and as a result academic values are being transformed. \n… This dissertation addresses how audit culture in higher education is transforming the \nidentities of academic scientists and influencing how the scientific identity is dependent on \npeer reviewed publications. \nThe ten semi -structured interview transcripts were uploaded to an LLM platform called \nJiggyBase (https://jiggy.ai), which is a service  that interfaces with ChatGPT -4 using your \nown text-based data. It works by searching your documents and providing the contents to \nChatGPT in a way that allows it to respond using the knowledge and data found therein. The \nten transcripts were uploaded, and nothing more (i.e., the title, abstract, author information, \n9 \n \netc. were not included). I then had a “conversation” with the data. For the sake of space, the \nqueries and responses below are somewhat abridged (although not edited for content). A \nscreenshot of the full, actual interaction is available in the Appendix. \n### \nWhat are the contents of the documents provided? \nThe documents provided contain interviews with various researchers, \nprofessors, and scientists discussing their thoughts on publishing, Open \nAccess, impact factors, and the importance of sharing research in academia. \nThey discuss their experiences with publishing, motivations for choosing \ncertain journals, the role of Open Access in science, and the pressure to \npublish in high-impact journals. Some interviewees also express concerns \nabout making data openly available, the potential misuse of data, and the \nethical issues with paywalls and hybrid journals. Additionally, they touch \nupon how their careers and reputations as academic scientists are dependent \non publications and citations. \nWhat are the primary themes that you can identify across this content? \n[it provided 7 themes] \nCan you condense this down to just 3 main topics by merging similar ones? \n1. Open Access and Publishing Pressure - The interviewees discuss the \nimportance of Open Access in making scientific papers more accessible, \nthe role of high-impact journals, and the pressure they face in their \ncareers to publish in prestigious journals. \n2. Impact Factors and Ethical Considerations - The participants express \nconcerns about overreliance on impact factors for judging research \nquality and ethical issues related to the current publishing system, \nincluding hybrid journals and double-dipping. \n3. Data Sharing and Career Advancement - The interviewees emphasize the \nsignificance of open data sharing for reproducibility and \ncollaboration, and discuss the role of publications and citations in \ncareer development, including promotions and appraisals. \n \nUsing an inductive approach, can you extract a theoretical insight or framework from these \ninterviews? \n \nUsing an inductive approach, a theoretical insight or framework can be \nextracted from these interviews as follows: \n10 \n \nFramework: Balancing Openness, Quality, and Career Advancement in Academic \nPublishing \n1. Openness and Accessibility -  Ensuring that research findings are \naccessible to a wider audience through Open Access publishing can benefit \nboth the scientific community and society. Encouraging researchers to \nvalue and prioritize Open Access can help promote a more inclusive and \ncollaborative research ecosystem. \n2. Quality Assessment and Ethical Practices -  Relying solely on impact \nfactors to assess research quality can be misleading and perpetuate \nunethical publishing practices. A more holistic approach to evaluation \nshould be adopted, taking into account the broader impact of the research, \nits originality, and its ethical implications, while also considering \nalternative metrics. \n3. Career Advancement and Incentives - Aligning career advancement \nincentives with the goals of openness, quality, and ethical practices can \nhelp reshape the academic publishing landscape. By promoting a system of \nrewards that values not only high -impact publications but also \ncontributions to teaching, public engagement, and collaboration, academia \ncan foster a more balanced and beneficial environment for researchers and \ntheir work. \nWhich, if any, of the 10 interviews are outliers? \nBased on the content provided, Participant 1 and Participant 2 interviews can \nbe considered outliers. These interviews differ from the others in terms of \ntheir focus on specific aspects of the academic publishing landscape. \nParticipant 1 talks about habit and selecting journals based on the best fit \nand audience, while Participant 2 discusses the perception of megajournals as \ndumping grounds for low- quality research and the challenges of evaluating \nresearchers outside of their field. These interviews do not cover the broader \nthemes of Open Access, impact factors, data sharing, or career advancement as \nextensively as the other interviews. \n \nAre there any patterns or correlations between these themes? For instance, is the theme of \n\"Publishing Pressure\" more prominent among early-career researchers who are concerned \nabout \"Career Advancement\"? Or does the concern for \"Ethical Considerations\" often \ncoincide with support for \"Open Access\"? \n \n \nOverall, the interviews suggest that there are some correlations between the \nthemes discussed, but the patterns are not always straightforward or \nconsistent across all participants. Here are a few examples: \n \n• Publishing Pressure and Career Advancement: Several early- career \nresearchers express concerns about the pressure to publish in high-impact \njournals, often linked to the desire to advance their careers. For \nexample, one participant notes that young scientists think they need to \npublish in top journals to develop a career. In contrast, more established \nresearchers seem to be less concerned with publishing pressure and may \nprioritize other factors such as the quality of research or the fit with \na specific audience. \n \n11 \n \n• Ethical Considerations and Open Access: Several participants who express \nconcerns about the ethical implications of the current publishing system, \nsuch as the artificial limits on publication, also advocate for Open \nAccess publishing. They argue that Open Access can provide greater access \nto research findings and facilitate collaborations, thereby advancing \nscience in a more equitable and inclusive way. \n \n• Passion for Field and Knowledge Creation: Many participants express a \npassion for their field and a desire to create new knowledge, often \nthrough the process of discovery and exploration. This theme is often \nlinked to the intellectual challenges of research, including problem-\nsolving and critical thinking skills. \n \nIt is important to note that the correlations between themes are not always \nclear-cut, and the interviews demonstrate that scientists have diverse \nexperiences and perspectives. Therefore, it is difficult to draw definitive \nconclusions about the relationship between themes and factors such as career \nstage, discipline, or other variables. \n \nCan you identify any novel, subtle or nuanced associations or themes that we have not yet \ndiscussed? \nOne subtle and nuanced theme that emerges from the interviews is the tension \nbetween the desire for academic freedom and the need to conform to the \nexpectations of the academic culture. Several participants express a desire to \npursue research that is outside of the mainstream or that challenges established \nparadigms, but they also acknowledge the pressures to conform to the \nexpectations of the academic community. \nFor example, one participant notes that there is a tension between their desire \nto pursue interdisciplinary research and the fact that academic institutions \nare still largely organized according to disciplinary boundaries. Another \nparticipant notes that there can be a pressure to conform to the research \nparadigm of their advisor or research group, even if it does not align with \ntheir own interests or values. \nRegarding the phrase \"publish or perish,\" what is a notable quote from the interview \ntranscripts, and explain why it is interesting. Note which interview # it comes from. \n\"It's kind of grim, but it's also kind of accurate. It's all linked. If you're \nnot publishing, then you're not doing good work and that wouldn't be good for \nscience if there weren't people who were doing good work and publishing.\" \n(Participant 9 Interview) \nThis quote is interesting because it highlights the complex relationship between \npublishing and scientific productivity. The participant acknowledges the \npressure to publish and the perception that it is necessary for scientific \nprogress, but also notes that the pressure can be \"grim.\" The quote suggests \nthat there is a tension between the desire to produce high-quality research and \nthe need to publish in order to advance one's career or secure funding. It also \nhighlights the potential negative effects of the \"publish or perish\" mentality \non the scientific enterprise more broadly, as it may encourage researchers to \nfocus on quantity over quality or to prioritize publishing in high- impact \njournals over other important aspects of scientific research \n \n12 \n \nLLM vs LDA \nI also wanted to see how the results and experience with the LLM compared to a more traditional \nLDA approach. But to do so, I needed to write a Python script that would carry out the LDA. Some \nqualitative social scientists may be unfamiliar with computer programming in general and writing \nPython scripts in particular. Because, of this I leveraged the ChatGPT -4 LLM to write me the \nappropriate Python script based on this prompt: “ write python code for me to run an LDA topic \ncoding analysis on a set of 10 interview transcripts, which are in the form of ten .pdf files.”  \n The LLM provided annotated code that read the .PDF files, cleaned and processed the data, \nand then applied the LDA model and provided its output. It also instructed me on which \nprerequisite Python packages and resources I needed to install in order for it to wor k. It also \ndebugged and refined the code after the first output was deemed unsatisfactory by me for being \ntoo redundant and incoherent. The final code that ChatGPT provided was  used to generate the \nfollowing five topics and their corresponding keywords (a screenshot of the full conversation with \nthe code is available I the Appendix): \n• Topic 1: brand, enterprise, risk, megajournals, rise \n• Topic 2: supporting information, profit, institution, article, meaningful \n• Topic 3: date, undergraduate, funding, appraisal, higher \n• Topic 4: code, conference, palaeontology, stressed, thesis \n• Topic 5: article, permanent, care, 2020, knowledge \nGiven a relatively small corpus of ten short interview transcripts, it is unsurprising that the LDA \noutput is not very meaningful. In this case, the LDA was able to extract some common words and \nphrases from the transcripts, but not  as well the underlying themes or topics. This suggests that \nLDA is not a very suitable method for such a small and limited data set. A more appropriate method \nwould be to use a human coder or a topic modeling algorithm based on a pre-defined dictionary of \nkeywords related to the theme of the academic study. \n13 \n \n But I was also curious to see if an LLM like ChatGPT -4 could infer something more \nnuanced and meaningful from the LDA output, so I asked it to try to interpret the coded topics. Its \noutput was much better—and it received no prior information in this case regarding the content or \ntopic of the text that was analyzed , although it was provided with two prior sets of LDA results \nthat may have provided certain context  (again, refer to the conversation with the LLM in the \nAppendix): \n• Topic 1: This topic could potentially relate to the \"Business Aspect \nof Publishing\". The term \"megajournals\" suggest the context might \nbe about large-scale academic publishing and its associated risks. \nThe terms \"brand\" and \"enterprise\" also suggest a commercial angle. \n• Topic 2: This topic might relate to the \"Value and Impact of Academic \nPublishing\". The words \"supporting_information\", \"article\", and \n\"meaningful\" suggest a focus on the content of academic publications. The \nterms \"profit\" and \"institution\" might imply an interest in how these \nentities benefit from the publishing process. \n• Topic 3: This topic could be related to \"Academic Progress and Evaluation\". \nIt seems to cover aspects like time (\"date\"), the academic level \n(\"undergraduate\"), financing (\"funding\"), and assessment (\"appraisal\").\n \n• Topic 4: This topic might refer to \"Academic Research and Conferences\". It \nseems to touch upon academic activities such as writing a \"thesis\", \nattending \"conference\", and maybe coding (\"code\"). \"Palaeontology\" could \nsuggest a specific field, and \"stressed\" might reflect the emotional state \nrelated to these activities. \n• Topic 5:  This topic could be about \"Sustainability and Longevity of \nKnowledge\". It mentions \"article\", suggesting the focus is on academic \npublishing, with terms like \"permanent\", \"care\", and \"knowledge\" pointing \ntowards the preservation and maintenance of academic know ledge. \"2020\" \ncould point to a specific timeframe or event.\n \n \nThis comparison suggests that LLMs can be superior to LDA, at least when dealing with relatively \nsmall amounts of qualitative data. However, using an LLM to make sense of the LDA output was \nfruitful. Furthermore, it demonstrated the power of LLMs like ChatGPT -4 to provide useful \ncomputer code perform various tasks related to qualitative analysis, such as data cleaning, \nprocessing, and topic modeling—especially given that I personally have very limited knowledge \nof coding and would not have been able to arrive at working code on my own (or even know where \nto begin, for that matter!) \n14 \n \n LLMs, with their advanced capacity for interpretation of language  and the ability to \ngenerate workable code, can be extremely valuable, particularly for researchers who may not have \nstrong programming skills. However, LDA ’s automated and more straightforwa rd approach, \nwhich may be less prone to overinterpretation, can be beneficial in certain contexts or when dealing \nwith much larger data sets. \nThat said, the key is not necessarily to see these methods as mutually exclusive, but rather \ncomplementary. Combining the strengths of both can provide richer, more robust insights. For \nexample, using LDA to generate topic clusters and then using an LLM to interpret and refine those \nresults, as I did in this case, could be a useful approach in future qualitative resea rch projects. \nHowever, more research is needed to hone the use of these tools in social science research, to \nassess their validity and reliability, and to develop best practices for their use. \nThe Genie Is Out of the Bottle \nLLMs, in a way, are like the proverbial Genie in a bottle: your wish is its command. It’s also hard \nat this point to ignore LLMs or imagine  a future without them. But w hile LLMs offer significant \npotential for qualitative researchers, they also pose certain challenges (Ray, 2023). They require \ncareful handling to avoid bias and ensure ethical use of data. They also require technical expertise \nto use effectively. Researchers must be transparent about their use of LLMs and consider the \nethical implications of their work. \n One of the primary challenges is the potential for bias in the models. LLMs are trained on \nlarge datasets, and they learn to generate text based on this data. If the training data contains \ninherent biases, the model can learn and reproduce these biases in its output. Thi s can lead to \nskewed or discriminatory results, which can be particularly problematic in sociological research, \nwhere the goal is often to understand and challenge social biases. Therefore, researchers must be \n15 \n \naware of this potential issue and take steps to address it (see e.g.: Barikeri, et al., 2021; Huang, et \nal., 2019; Rozado, 2023) \nAnother challenge is the technical  limitations and  expertise required to use LLMs \neffectively. While there are increasingly user -friendly interfaces and platforms for using these \nmodels, a certain level of technical knowledge and proficiency is still required to use them \neffectively and responsibly. This includes a basic understanding of how the models work, how to \neffectively craft prompts that generate the desired responses, how to interpret their output, and \nhow to troubleshoot any issues that arise. One of the key technical limitations of current LLMs  \nthemselves are  their token limit (a token is a word or other discrete object processed by the model). \nThe GPT-4 model has a maximum token limit of around 8,200 tokens, which roughly corresponds \nto ten to twelve pages of text.\n1 This means that they can only process a limited amount of text at a \ntime, which can be a challenge when working with large datasets or complex queries . However, \nthis limitation can be mitigated to some extent through the use of plugins or tools that are designed \nto chunk and parse large amounts of text—as was used in the practical example above. Moreover, \nmany models’ output is also limited in length, which can constrain the depth or detail of their \nresponses. \nAnother aspect to consider is that while LLMs can generate impressively human-like text, \nthey do not truly understand the content in the way humans do. They are not conscious beings and \ndo not possess beliefs, opinions, or emotions. As such, they lack a model of the world or a coherent \nsense of self. This means that their output is entirely based on patterns in the data they were trained \non, and they can sometimes generate responses that are nonsensical, inappropriate, or misleading. \n \n1 https://help.openai.com/en/articles/7127966-what-is-the-difference-between-the-gpt-4-models  \n16 \n \nThe way LLMs identify and utilize patterns in data is quite sophisticated and can nonetheless often \nlead to outputs that appear creative or insightful. The patterns learned by the model are not just \nsimple repetitions of what they have seen in the training data. They instead  capture complex \nrelationships between words, phrases, sentences , and ideas . The model ’s ability to generate \ncontextually appropriate and coherent text can sometimes give the impression of creativity or \nunderstanding. For instance, if asked to write a poem about a sunset, the model might generate an \noriginal, beautiful, and evocative poem, even though it has no personal experience of sunsets or \nemotions. But the model does not have any intention or purpose behind its output, and it does not \nknow what it is saying or why. It is simply following the rules and associations that it learned from \nthe data. Therefore, it is important to be critical and cautious when using or evaluating the output \nof LLMs, especially for qualitative analysis. LLMs can be useful tools for  analyzing and \ngenerating text, but they cannot replace human judgment, interpretation, or evaluation. \nIn terms of ethical considerations, the use of LLMs raises questions about data privacy and \nconsent. For instance, if an LLM is used to generate text based on data from individuals (such as \ninterview transcripts), it is important to ensure that this does not infringe on the individuals’ rights \nor privacy. This means that  the output of the LLM should not reveal any sensitive or personal \ninformation about the individuals, such as their names, addresses, or other identi fying features. \nThe output should also respect the dignity and integrity of the individuals, and not misrepresent or \nharm them in any way. Additionally, users and researchers of LLMs should be aware of the \npotential risks and benefits of using these models, and take measures to ensure their ethical and \nresponsible use. \nFinally, transparency is a key ethical consideration in the use of LLMs in academic \nresearch. Researchers must be clear about when and how they are using these models, and about \n17 \n \ntheir potential limitations and biases. This includes being transparent in their research papers and \npresentations, as well as in their interactions with research participants and the wide r public.  \nTransparency in the use of LLMs involves several aspects. Firstly, researchers should clearly state \nwhen an LLM has been used in the research process, whether for data collection, analysis, or \ninterpretation. This allows others to understand the methods used and to evaluate the reliability \nand validity of the results. Secondly, researchers should be open about how the LLM was used. \nThis includes providing information about the specific model used (e.g., GPT-4, Bard, using which \nplugins, etc.), the prompts or commands given to the model, and how the model ’s output was \ninterpreted and used in the research. This level of detail allows others to verify findings and to \nreplicate and expand on the research. \nConclusion \nLarge Language Models (LLMs) like ChatGPT -4 and others are transforming the landscape of \ndata analysis and interpretation across various fields. Their ability to understand, generate, and \nmanipulate human- like text offers unprecedented opportunities for qualitative research, \nparticularly in the field of sociology. \nThese models, with their capacity to process large volumes of text quickly and consistently, \ncan significantly enhance the efficiency and depth of qualitative analysis. They can identify \npatterns, themes, and sentiments in the data, providing a level of nuance that can be challenging to \nachieve with manual coding. Furthermore, their ability to generate human-like text can be used to \nsimulate social interactions, create engaging presentations of research findings, and even \n\"converse\" with the data in a natural and flexible way. \nHowever, the use of LLMs in sociological research is not without challenges and ethical \nconsiderations. Researchers must be aware of the potential for bias in the models, the ir technical \n18 \n \nlimitations, and the need for transparency in their use. They must also navigate issues related to \ndata privacy and consent, and be mindful of the models’ lack of true understanding or awareness. \nDespite these challenges, the potential of LLMs to enhance qualitative sociologica l \nresearch is immense. As these models continue to evolve and improve, and as researchers become \nmore adept at using them, they are likely to become an increasingly valuable tool in the \nsociologist's toolkit. The future of sociological research may well involve having a \"conversation\" \nwith our data, asking it questions in natural language, and receiving insightful responses that guide \nour understanding of social phenomena. This paper hopes to have opened up this exciting prospect \nand to have provided somewh at of a roadmap for researchers interested in exploring this new \nfrontier to begin employing these techniques to “talk” to their data. \n \n \nReferences \nBail, C. A. (2023). Can Generative AI Improve Social Science?. https://osf.io/rwtzs/download\n \nBarikeri, S., Lauscher, A., Vulić, I., & Glavaš, G. (2021). RedditBias: A real-world resource for \nbias evaluation and debiasing of conversational language models. arXiv preprint \narXiv:2106.03521. \nBelotto, M. J. (2018). Data analysis methods for qualitative research: Managing the challenges of \ncoding, interrater reliability, and thematic analysis. The Qualitative Report , 23 (11), 2622-2633.  \nBender, E. M., Gebru, T., McMillan-Major, A., & Shmitchell, S. (2021, March). On the Dangers \nof Stochastic Parrots: Can Language Models Be Too Big?������. In Proceedings of the 2021 ACM \nconference on fairness, accountability, and transparency (pp. 610-623). \nBowley, C. (2015). “I’m Cited, Therefore I Am!”: How Publishing Constructs the Scientific \nIdentity. \n(\nhttps://figshare.com/articles/thesis/_I_m_Cited_Therefore_I_Am_How_Publishing_Constructs_\nthe_Scientific_Identity/1536525)  \n19 \n \nBrown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., ... & Amodei, D. \n(2020). Language models are few-shot learners. Advances in neural information processing \nsystems, 33, 1877-1901. \nDiMaggio, P., Nag, M., & Blei, D. (2013). Exploiting affinities between topic modeling and the \nsociological perspective on culture: Application to newspaper coverage of US government arts \nfunding. Poetics, 41(6), 570-606. \nEgger, R., & Yu, J. (2022). A topic modeling comparison between lda, nmf, top2vec, and \nbertopic to demystify twitter posts. Frontiers in Sociology, 7. \nHuang, P. S., Zhang, H., Jiang, R., Stanforth, R., Welbl, J., Rae, J., ... & Kohli, P. (2019). \nReducing sentiment bias in language models via counterfactual evaluation. arXiv preprint \narXiv:1911.03064. \nLeinhardt, S., & Wasserman, S. S. (1979). Exploratory data analysis: An introduction to selected \nmethods. Sociological Methodology, 10, 311-365. \nLiu, R., & Shah, N. B. (2023). ReviewerGPT? An Exploratory Study on Using Large Language \nModels for Paper Reviewing. arXiv preprint arXiv:2306.00622. \nMcFarland, D. A., Ramage, D., Chuang, J., Heer, J., Manning, C. D., & Jurafsky, D. (2013). \nDifferentiating language usage through topic models. Poetics, 41(6), 607-625.  \nPangakis, N., Wolken, S., & Fasching, N. (2023). Automated Annotation with Generative AI \nRequires Validation. arXiv preprint arXiv:2306.00176. \nRathje, S., Mirea, D. M., Sucholutsky, I., Marjieh, R., Robertson, C., & Van Bavel, J. J. (2023). \nGPT is an effective tool for multilingual psychological text analysis. Psyarxiv \nhttps://psyarxiv.com/sekf5?trk=public_post_main-feed-card-text\n \nRay, P. P. (2023). ChatGPT: A comprehensive review on background, applications, key \nchallenges, bias, ethics, limitations and future scope. Internet of Things and Cyber-Physical \nSystems. \nRozado, D. (2023). The political biases of chatgpt. Social Sciences, 12(3), 148. \nRytting, C. M., Sorensen, T., Argyle, L., Busby, E., Fulda, N., Gubler, J., & Wingate, D. (2023). \nTowards Coding Social Science Datasets with Language Models. arXiv preprint arXiv:2306.0 \nSaldaña, J. (2021). The coding manual for qualitative researchers. The coding manual for \nqualitative researchers, 1-440. \nTörnberg, P. (2023). Chatgpt-4 outperforms experts and crowd workers in annotating political \ntwitter messages with zero-shot learning. arXiv preprint arXiv:2304.06588. \nVan Dis, E. A., Bollen, J., Zuidema, W., van Rooij, R., & Bockting, C. L. (2023). ChatGPT: five \npriorities for research. Nature, 614(7947), 224-226. \nWadhwa, M., Chen, J., Li, J. J., & Durrett, G. (2023). Using Natural Language Explanations to \nRescale Human Judgments. arXiv preprint arXiv:2305.14770. \n20 \n \nZiems, C., Held, W., Shaikh, O., Chen, J., Zhang, Z., & Yang, D. (2023). Can Large Language \nModels Transform Computational Social Science?. arXiv preprint arXiv:2305.03514. ",
  "topic": "Transformative learning",
  "concepts": [
    {
      "name": "Transformative learning",
      "score": 0.7382292151451111
    },
    {
      "name": "Converse",
      "score": 0.7362891435623169
    },
    {
      "name": "Qualitative property",
      "score": 0.5867214798927307
    },
    {
      "name": "Coding (social sciences)",
      "score": 0.49496692419052124
    },
    {
      "name": "Qualitative research",
      "score": 0.47053441405296326
    },
    {
      "name": "Data science",
      "score": 0.4558272957801819
    },
    {
      "name": "Realm",
      "score": 0.42989349365234375
    },
    {
      "name": "Generative model",
      "score": 0.4270693063735962
    },
    {
      "name": "Computer science",
      "score": 0.42313745617866516
    },
    {
      "name": "Generative grammar",
      "score": 0.38419824838638306
    },
    {
      "name": "Sociology",
      "score": 0.2928164303302765
    },
    {
      "name": "Epistemology",
      "score": 0.26074540615081787
    },
    {
      "name": "Social science",
      "score": 0.20228832960128784
    },
    {
      "name": "Artificial intelligence",
      "score": 0.17941662669181824
    },
    {
      "name": "Political science",
      "score": 0.12661367654800415
    },
    {
      "name": "Pedagogy",
      "score": 0.10254830121994019
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Machine learning",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I197251160",
      "name": "Hebrew University of Jerusalem",
      "country": "IL"
    }
  ]
}