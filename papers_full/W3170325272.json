{
    "title": "Dynamic Language Models for Continuously Evolving Content",
    "url": "https://openalex.org/W3170325272",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3152794281",
            "name": "Spurthi Amba Hombaiah",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2084622737",
            "name": "Tao Chen",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2116348857",
            "name": "Mingyang Zhang",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2112702096",
            "name": "Michael Bendersky",
            "affiliations": [
                "Google (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2027155665",
            "name": "Marc Najork",
            "affiliations": [
                "Google (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963088785",
        "https://openalex.org/W2309627465",
        "https://openalex.org/W2483398371",
        "https://openalex.org/W2493916176",
        "https://openalex.org/W2884282566",
        "https://openalex.org/W2964110616",
        "https://openalex.org/W2963344337",
        "https://openalex.org/W2963780471",
        "https://openalex.org/W2963956670",
        "https://openalex.org/W2962849793",
        "https://openalex.org/W2964230347",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W2332659157",
        "https://openalex.org/W6611333299",
        "https://openalex.org/W1570098300",
        "https://openalex.org/W2601529995",
        "https://openalex.org/W1899794420",
        "https://openalex.org/W2963598809",
        "https://openalex.org/W3104186312",
        "https://openalex.org/W2964189064",
        "https://openalex.org/W2080404274",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2962966271",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2905559537",
        "https://openalex.org/W4211177138",
        "https://openalex.org/W2983577274",
        "https://openalex.org/W2782822144",
        "https://openalex.org/W2251769296"
    ],
    "abstract": "The content on the web is in a constant state of flux. New entities, issues,\\nand ideas continuously emerge, while the semantics of the existing conversation\\ntopics gradually shift. In recent years, pre-trained language models like BERT\\ngreatly improved the state-of-the-art for a large spectrum of content\\nunderstanding tasks. Therefore, in this paper, we aim to study how these\\nlanguage models can be adapted to better handle continuously evolving web\\ncontent. In our study, we first analyze the evolution of 2013 - 2019 Twitter\\ndata, and unequivocally confirm that a BERT model trained on past tweets would\\nheavily deteriorate when directly applied to data from later years. Then, we\\ninvestigate two possible sources of the deterioration: the semantic shift of\\nexisting tokens and the sub-optimal or failed understanding of new tokens. To\\nthis end, we both explore two different vocabulary composition methods, as well\\nas propose three sampling methods which help in efficient incremental training\\nfor BERT-like models. Compared to a new model trained from scratch offline, our\\nincremental training (a) reduces the training costs, (b) achieves better\\nperformance on evolving content, and (c) is suitable for online deployment. The\\nsuperiority of our methods is validated using two downstream tasks. We\\ndemonstrate significant improvements when incrementally evolving the model from\\na particular base year, on the task of Country Hashtag Prediction, as well as\\non the OffensEval 2019 task.\\n",
    "full_text": null
}