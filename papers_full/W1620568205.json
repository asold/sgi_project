{
  "title": "Recognition Performance of a Structured Language Model",
  "url": "https://openalex.org/W1620568205",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4293488235",
      "name": "Chelba, Ciprian",
      "affiliations": [
        "Johns Hopkins University"
      ]
    },
    {
      "id": null,
      "name": "Jelinek, Frederick",
      "affiliations": [
        "Johns Hopkins University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1644582922",
    "https://openalex.org/W1597533204",
    "https://openalex.org/W2049633694",
    "https://openalex.org/W2166637769",
    "https://openalex.org/W1607229519",
    "https://openalex.org/W2160645305",
    "https://openalex.org/W3021713638"
  ],
  "abstract": "A new language model for speech recognition inspired by linguistic analysis is presented. The model develops hidden hierarchical structure incrementally and uses it to extract meaningful information from the word history - thus enabling the use of extended distance dependencies - in an attempt to complement the locality of currently used trigram models. The structured language model, its probabilistic parameterization and performance in a two-pass speech recognizer are presented. Experiments on the SWITCHBOARD corpus show an improvement in both perplexity and word error rate over conventional trigram models.",
  "full_text": "arXiv:cs/0001022v1  [cs.CL]  24 Jan 2000\nRECOGNITION PERFORMANCE OF A STRUCTURED LANGUAGE MODEL †\nCiprian Chelba Frederick Jelinek\nCenter for Language and Speech Processing\nThe Johns Hopkins University, Baltimore, MD-21218, USA\n{chelba,jelinek}@jhu.edu\nABSTRACT\nA new language model for speech recognition inspired by lin-\nguistic analysis is presented. The model develops hidden hierar-\nchical structure incrementally and uses it to extract meaningful\ninformation from the word history — thus enabling the use of\nextended distance dependencies — in an attempt to complement\nthe locality of currently used trigram models. The structured lan-\nguage model, its probabilistic parameterization and performance\nin a two-pass speech recognizer are presented. Experimentson\nthe SWITCHBOARD corpus show an improvement in both per-\nplexity and word error rate over conventional trigram models.\n1. INTRODUCTION\nThe main goal of the present work is to develop and evaluate\na language model that uses syntactic structure to model long-\ndistance dependencies. The model we present is closely related\nto the one investigated in [1], however different in a few impor-\ntant aspects:\n• our model operates in a left-to-right manner, allowing the de-\ncoding of word lattices, as opposed to the one referred to pre-\nviously, where only whole sentences could be processed, thus\nreducing its applicability to N-best list re-scoring; the syntactic\nstructure is developed as a model component;\n• our model is a factored version of the one in [1], thus enabling\nthe calculation of the joint probability of words and parse struc-\nture; this was not possible in the previous case due to the huge\ncomputational complexity of that model.\nThe structured language model (SLM), its probabilistic pa-\nrameterization and performance in a two-pass speech recognizer\n— we evaluate the model in a lattice decoding framework — are\npresented. Experiments on the SWITCHBOARD corpus show\nan improvement in both perplexity (PPL) and word error rate\n(WER) over conventional trigram models.\n2. STRUCTURED LANGUAGE MODEL\nAn extensive presentation of the SLM can be found in [2]. The\nmodel assigns a probabilityP (W, T ) to every sentenceW and\nits every possible binary parseT . The terminals ofT are the\nwords ofW with POStags, and the nodes ofT are annotated\nwith phrase headwords and non-terminal labels. LetW be a\n(<s>, SB)   .......   (w_p, t_p) (w_{p+1}, t_{p+1}) ........ (w_k, t_k) w_{k+1}.... </s>\nh_0 = (h_0.word, h_0.tag)h_{-1}h_{-m} = (<s>, SB)\nFigure 1. A word-parse k-preﬁx\nsentence of lengthn words to which we have prepended<s>\nand appended</s> so thatw0 =<s> and wn+1 =</s>. Let\nWk be the word k-preﬁxw0 . . . w k of the sentence andWkTk\n† THIS WORK WAS FUNDED BY THE NSF IRI-19618874\nGRANT STIMULATE\n...............\nT’_0\nT_{-1} T_0<s> T’_{-1}<-T_{-2}\nh_{-1} h_0\nh’_{-1} = h_{-2}\nT’_{-m+1}<-<s>\nh’_0 = (h_{-1}.word, NTlabel)\nFigure 2. Result of adjoin-left under NTlabel\n............... T’_{-1}<-T_{-2} T_0\nh_0h_{-1}\n<s>\nT’_{-m+1}<-<s>\nh’_{-1}=h_{-2}\nT_{-1}\nh’_0 = (h_0.word, NTlabel)\nFigure 3. Result of adjoin-right under NTlabel\ntheword-parse k-preﬁx. Figure 1 shows a word-parse k-preﬁx;\nh_0 .. h_{-m} are theexposed heads, each head being a\npair(headword, non-terminal label), or (word, POStag) in the\ncase of a root-only tree.\n2.1. Probabilistic Model\nThe probabilityP (W, T ) of a word sequenceW and a complete\nparseT can be broken into:\nP (W, T ) =\n∏ n+1\nk=1 [ P (wk/W k− 1Tk− 1) · P (tk/W k− 1Tk− 1, w k) ·\nNk∏\ni=1\nP (pk\ni /W k− 1Tk− 1, w k, t k, p k\n1 . . . p k\ni− 1)] (1)\nwhere:\n• Wk− 1Tk− 1 is the word-parse(k − 1)-preﬁx\n• wk is the word predicted by WORD-PREDICTOR\n• tk is the tag assigned towk by the TAGGER\n• Nk − 1 is the number of operations the PARSER executes\nat sentence positionk before passing control to the WORD-\nPREDICTOR (the Nk-th operation at position k is thenull\ntransition);Nk is a function ofT\n• pk\ni denotes the i-th PARSER operation carried out at position k\nin the word string; the operations performed by the PARSER are\nillustrated in Figures 2-3 and they ensure that all possiblebinary\nbranching parses with all possible headword and non-terminal\nlabel assignments for thew1 . . . w k word sequence can be gen-\nerated.\nOur model is based on three probabilities, estimated using\ndeleted interpolation [7], parameterized as follows:\nP (wk/W k− 1Tk− 1) = P (wk/h 0, h − 1) (2)\nP (tk/w k, W k− 1Tk− 1) = P (tk/w k, h 0.tag, h − 1.tag )(3)\nP (pk\ni /W kTk) = P (pk\ni /h 0, h − 1) (4)\nIt is worth noting that if the binary branching structure devel-\noped by the parser were always right-branching and we mapped\nthe POStag and non-terminal label vocabularies to a single type\nthen our model would be equivalent to a trigram language model.\nSince the number of parses for a given word preﬁxWk grows\nexponentially withk, |{Tk}| ∼ O(2k), the state space of our\nmodel is huge even for relatively short sentences so we had\nto use a search strategy that prunes it. Our choice was a syn-\nchronous multi-stack search algorithm which is very similar to a\nbeam search.\nThe probability assignment for the word at positionk + 1in\nthe input sentence is made using:\nPSLM (wk+1/W k) =\n∑\nTk∈ Sk\nP (wk+1/W kTk) · ρ(Wk, T k),\nρ(Wk, T k) = P (WkTk)/\n∑\nTk∈ Sk\nP (WkTk) (5)\nwhich ensures a proper probability over stringsW ∗ , whereSk\nis the set of all parses present in our stacks at the current stagek.\nAn N-best EM [5] variant is employed to reestimate the model\nparameters such that the PPL on training data is decreased —\nthe likelihood of the training data under our model is increased.\nThe reduction in PPL is shown experimentally to carry over to\nthe test data.\n3. A∗ DECODER FOR LATTICES\n3.1. A∗ Algorithm\nThe A∗ algorithm [8] is a tree search strategy that could be com-\npared to depth-ﬁrst tree-traversal: pursue the most promising\npath as deeply as possible.\nTo be more speciﬁc, let a set of hypotheses\nL = {h : x1, . . . , x n}, x i ∈ W ∗ — to be scored using the\nfunctionf(·) — be organized as a preﬁx tree. We wish to obtain\nthe hypothesish∗ = arg maxh∈ L f(h) without scoring all the\nhypotheses inL, if possible with a minimal computational effort.\nTo be able to pursue the most promising path, the algorithm\nneeds to evaluate the possible continuations of a given preﬁx\nx = w1, . . . , w p that reach the end of the lattice. LetCL(x) be\nthe set of complete continuations ofx inL — they all reach the\nend of the lattice, see Figure 4. Assume we have an overestimate\ng(x.y ) =f(x)+ h(y|x) ≥ f(x.y ) for the score ofcompletehy-\npothesisx.y — . denotes concatenation; imposing thath(y|x) =\n0 for emptyy, we haveg(x) = f(x), ∀ complete x ∈ L. This\nCL\nw\nw\nw\n1\n2\np\n(x)\nFigure 4. Preﬁx Tree Organization of a Set of Hypotheses\nmeans that the quantity deﬁned as:\ngL(x) .= max\ny∈ CL(x)\ng(x.y ) =f(x) +hL(x), (6)\nhL(x) .= max\ny∈ CL(x)\nh(y|x) (7)\nis an overestimate of the most promising complete continuation\nof x in L: gL(x) ≥ f(x.y ), ∀y ∈ CL(x) and thatgL(x) =\nf(x), ∀ complete x ∈ L.\nThe A∗ algorithm uses a potentially inﬁnite stack1 in which\npreﬁxesx are ordered in decreasing order ofgL(x); at each ex-\ntension step the top-most preﬁxx = w1, . . . , w p is popped form\n1The stack need not be larger than|L|= n\nthe stack, expanded with all possible one-symbol continuations\nofx inL and then all the resulting expanded preﬁxes — among\nwhich there may be complete hypotheses as well — are inserted\nback into the stack. The stopping condition is: whenever the\npopped hypothesis is a complete one, retain that one as the over-\nall best hypothesish∗ .\n3.2. A∗ for Lattice Decoding\nThere are a couple of reasons that makeA∗ appealing for our\nproblem:\n• the algorithm operates with whole preﬁxesx, making it ideal\nfor incorporating language models whose memory is the entire\npreﬁx;\n• a reasonably good overestimateh(y|x) and an efﬁcient way to\ncalculatehL(x) (see Eq.6) are readily available using the n-gram\nmodel, as we will explain later.\nThe lattices we work with retain the following information af-\nter the ﬁrst pass:\n• time-alignment of each node;\n• for each link connecting two nodes in the lattice we retain:\nword identity, acoustic model score and n-gram language model\nscore. The lattice has a unique starting and ending node, respec-\ntively.\nA lattice can be conceptually organized as a preﬁx tree of\npaths. When rescoring the lattice using a different language\nmodel than the one that was used in the ﬁrst pass, we seek to\nﬁnd the complete pathp = l0 . . . l n maximizing:\nf(p) =\nn∑\ni=0\n[ logPAM (li)\n+ LMweight · logPLM (w(li)|w(l0) . . . w (li− 1))\n− logPIP ] (8)\nwhere:\n• logPAM (li) is the acoustic model log-likelihood assigned to\nlinkli;\n• logPLM (w(li)|w(l0) . . . w (li− 1)) is the language model log-\nprobability assigned to linkli given the previous links on the\npartial pathl0 . . . l i;\n• LMweight > 0 is a constant weight which multiplies the\nlanguage model score of a link; its theoretical justiﬁcation is un-\nclear but experiments show its usefulness;\n• logPIP > 0 is the “insertion penalty”; again, its theoretical\njustiﬁcation is unclear but experiments show its usefulness.\nTo be able to apply theA∗ algorithm we need to ﬁnd an ap-\npropriate stack entry scoring functiongL(x) where x is a par-\ntial path andL is the set of complete paths in the lattice. Go-\ning back to the deﬁnition (6) ofgL(·) we need an overestimate\ng(x.y ) =f(x) +h(y|x) ≥ f(x.y ) for all possibley = lk . . . l n\ncomplete continuations ofx allowed by the lattice. We propose\nto use the heuristic:\nh(y|x) =\nn∑\ni=k\n[logPAM (li) +LMweight · (logPNG (li)\n+logPCOMP ) − logPIP ]\n+LMweight · logPF INAL · δ(k < n ) (9)\nA simple calculation shows that if\nlogPNG (li) +logPCOMP ≥ logPLM (li), ∀li\nis satisﬁed thengL(x) = f(x) +maxy∈ CL(x)h(y|x) is a an\nappropriate choice for theA∗ search.\nThe justiﬁcation for thelogPCOMP term is that it is supposed\nto compensate for the per word difference in log-probability be-\ntween the n-gram modelNG and the superior modelLM with\nwhich we rescore the lattice — hencelogPCOMP > 0. Its ex-\npected value can be estimated from the difference in perplexity\nbetween the two modelsLM and NG. The logPF INAL > 0\nterm is used for practical considerations as explained in the next\nsection.\nThe calculation ofgL(x) (6) is made very efﬁcient after re-\nalizing that one can use the dynamic programming technique\nin the Viterbi algorithm [9]. Indeed, for a given latticeL,\nthe value ofhL(x) is completely determined by the identity\nof the ending node ofx; a Viterbi backward pass over the lat-\ntice can store at each node the corresponding value ofhL(x) =\nhL(ending\nnode(x)) such that it is readily available in theA∗\nsearch.\n3.3. Some Practical Considerations\nIn practice one cannot maintain a potentially inﬁnite stack. We\nchose to control the stack depth using two thresholds: one on\nthe maximum number of entries in the stack, calledstack-depth-\nthresholdand another one on the maximum log-probability dif-\nference between the top most and the bottom most hypotheses in\nthe stack, calledstack-logP-threshold.\nA gross overestimate used in connection with a ﬁnite stack\nmay lure the search on a cluster of paths which is suboptimal\n— the desired cluster of paths may fall short of the stack if the\noverestimate happens to favor a wrong cluster.\nAlso, longer partial paths — thus having shorter sufﬁxes\n— beneﬁt less from the per wordlogPCOMP compensation\nwhich means that they may fall out of a stack already full\nwith shorter hypotheses — which have high scores due to\ncompensation. This is the justiﬁcation for thelogPF INAL\nterm in the compensation functionh(y|x): the variance\nvar[logPLM (li|l0 . . . l i− 1) − logPNG (li)] is a ﬁnite posi-\ntive quantity so the compensation is likely to be closer to\nthe expected valueE[logPLM (li|l0 . . . l i− 1) − logPNG (li)]\nfor longery continuations than for shorter ones; introduc-\ning a constantlogPF INAL term is equivalent to an adap-\ntivelogPCOMP depending on the length of they sufﬁx —\nsmaller equivalentlogPCOMP for long sufﬁxesy for which\nE[logPLM (li|l0 . . . l i− 1) − logPNG (li)] is a better estimate for\nlogPCOMP than it is for shorter ones.\nBecause the structured language model is computationally\nexpensive, a strong limitation is being placed on the width\nof the search — controlled by thestack-depth and the\nstack-logP-threshold. For an acceptable search width\n— runtime — one seeks to tune the compensation parameters\nto maximize performance measured in terms of WER. However,\nthe correlation between these parameters and the WER is not\nclear and makes search problems diagnosis extremely difﬁcult.\nOur method for choosing the search and compensation parame-\nters was to sample a few complete pathsp1, . . . , p N from each\nlattice, rescore those paths according to thef(·) function (8) and\nthen rank theh∗ path output by theA∗ search among the sam-\npled paths. A correctA∗ search should result in average rank\n0. In practice this doesn’t happen but one can trace the topmost\npathp∗ — in the offending casesp∗ ̸= h∗ and f(p∗ ) > f (h∗ )\n— and check whether the search failed strictly because of insuf-\nﬁcient compensation — a preﬁx of thep∗ hypothesis is present\nin the stack whenA∗ returns — or because the pathp∗ fell short\nof the stack during the search — in which case the compensation\nand the search-width interact.\nThe method we chose for sampling paths from the lattice was\nan N-best search using the n-gram language model scores; this is\nappropriate for pragmatic reasons — one prefers lattice rescor-\ning to N-best list rescoring exactly because of the possibility to\nextract a path that is not among the candidates proposed in the\nN-best list — as well as practical reasons — they are among the\n“better” paths in terms of WER.\n4. EXPERIMENTS\n4.1. Experimental Setup\nIn order to train the structured language model (SLM) as de-\nscribed in [2] we need parse trees from which to initialize the\nparameters of the model. Fortunately a part of the Switchboard\n(SWB) [6] data has been manually parsed at UPenn ; let us re-\nfer to this corpus as the SWB-Treebank. The SWB training data\nused for speech recognition — SWB-CSR — is different from\nthe SWB-Treebank in two aspects:\n• the SWB-Treebank is a subset of the SWB-CSR data;\n• the SWB-Treebank tokenization is different from that of the\nSWB-CSR corpus; among other spurious small differences, the\nmost frequent ones are of the type presented in Table 1.\nSWB-Treebank SWB-CSR\ndo n’t don’t\nit ’s it’s\ni ’m i’m\ni ’ll i’ll\nTable 1. SWB-Treebank SWB-CSR tokenization mismatch\nOur goal is to train the SLM on the SWB-CSR corpus.\n4.1.1. Training Setup\nThe training of the SLM model proceeded as follows:\n• train SLM on SWB-Treebank — using the SWB-Treebank\nclosed vocabulary — as described in [2]; this is possible because\nfor this data we have parse trees from which we can gather initial\nstatistics;\n• process the SWB-CSR training data to bring it closer to the\nSWB-Treebank format. We applied the transformations sug-\ngested by Table 1; the resulting corpus will be called SWB-CSR-\nTreebank, although at this stage we only have words and no parse\ntrees for it;\n• transfer the SWB-Treebank parse trees onto the SWB-CSR-\nTreebank training corpus. To do so we parsed the SWB-CSR-\nTreebank using the SLM trained on the SWB-Treebank; the vo-\ncabulary for this step was the union between the SWB-Treebank\nand the SWB-CSR-Treebank closed vocabularies; at this stage\nSWB-CSR-Treebank is truly a “treebank”;\n• retrain the SLM on the SWB-CSR-Treebank training corpus\nusing the parse trees obtained at the previous step for gathering\ninitial statistics; the vocabulary used at this step was theSWB-\nCSR-Treebank closed vocabulary.\n4.1.2. Lattice Decoding Setup\nTo be able to run lattice decoding experiments we need to\nbring the lattices — SWB-CSR tokenization — to the SWB-\nCSR-Treebank format. The only operation involved in this trans-\nformation is splitting certain words into two parts, as suggested\nby Table 1. Each link whose word needs to be split is cut into\ntwo parts and an intermediate node is inserted into the lattice as\nshown in Figure 5. The acoustic and language model scores of\nthe initial link are copied onto the second new link.For all the\ns\ns_time\ne\ne_time\nw, AMlnprob, NGlnprob\ns\ns_time i\ne\ne_time\nw_1, 0, 0\nw_2, AMlnprob, NGlnprob\ne_time\nW -> W_1 W_2\nFigure 5. Lattice Processing\ndecoding experiments we have carried out, the WER is measured\nafter undoing the transformations highlighted above; the refer-\nence transcriptions for the test data were not touched and the\nNIST SCLITE package was used for measuring the WER.\n4.2. Perplexity Results\nAs a ﬁrst step we evaluated the perplexity performance of the\nSLM relative to that of a deleted interpolation 3-gram model\ntrained in the same conditions. We worked on the SWB-CSR-\nTreebank corpus. The size of the training data was 2.29 Mwds;\nthe size of the test data set aside for perplexity measurements\nwas 28 Kwds — WS97 DevTest [4]. We used a closed vocab-\nulary — test set words included in the vocabulary — of size\n22Kwds. Similar to the experiments reported in [2], we built\na deleted interpolation 3-gram model which was used as a base-\nline; we have also linearly interpolated the SLM with the 3-gram\nbaseline showing a modest reduction in perplexity:\nP (wi|Wi− 1) =λ·P (wi|wi− 1, w i− 2)+(1−λ)·PSLM (wi|Wi− 1)\nThe results are presented in Table 2.\nLanguage Model L2R Perplexity\nDEV set TEST set\nλ 0.0 1.0 0.0 0.4 1.0\n3-gram + Initl SLM 23.9 22.5 72.1 65.8 68.6\n3-gram + Reest SLM 22.7 22.5 71.0 65.4 68.6\nTable 2. Perplexity Results\n4.3. Lattice Decoding Results\nWe proceeded to evaluate the WER performance of the SLM\nusing theA∗ lattice decoder described previously. Before de-\nscribing the experiments we need to make clear one point; there\nare two 3-gram language model scores associated with the each\nlink in the lattice:\n• the language model score assigned by the model that generated\nthe lattice, referred to as the LAT3-gram; this model operates on\ntext in the SWB-CSR tokenization;\n• the language model score assigned by rescoring each link in\nthe lattice with the deleted interpolation 3-gram built on the data\nin the SWB-CSR-Treebank tokenization, referred to simply as\nthe 3-gram — used in the experiments reported in the previous\nsection.\nThe perplexity results show that interpolation with the 3-gram\nmodel is beneﬁcial for our model. Note that the interpolation:\nP (l) =λ · PLAT 3− gram (l) + (1− λ) · PSLM (l)\nbetween the LAT3-gram model and the SLM is illegitimate due\nto the tokenization mismatch.\nAs explained previously, due to the fact that the SLM’s\nmemory extends over the entire preﬁx we need to ap-\nply the A∗ algorithm to ﬁnd the overall best path in the\nlattice. The parameters controlling theA∗ search were\nset to: logPCOMP = 0.5, logPF INAL = 2, LMweight\n= 12, logPIP = 10, stack-depth-threshold=30,\nstack-depth-logP-threshold=100 — see (8) and ( 9).\nThe parameters controlling the SLM were the same as in [2]. The\nresults for different interpolation coefﬁcient values areshown in\nTable 3.\nLanguage Model WER\nSearch A∗ Vite\nλ 0.0 0.4 1.0 1.0\nLAT-3gram + SLM 42.4 40.3 41.6 41.3\nTable 3. Lattice Decoding Results\nThe structured language model achieved an absolute improve-\nment of 1% WER over the baseline; the improvement is statisti-\ncally signiﬁcant at the 0.002 level according to a sign test.In the\n3-gram case, theA∗ search looses 0.3% over the Viterbi search\ndue to ﬁnite stack and heuristic lookahead.\n4.4. Search Evaluation Results\nFor tuning the search parameters we have applied the N-best\nlattice sampling technique described in Section 3.3. As a by-\nproduct, the WER performance of the structured language model\non N-best list rescoring — N = 25 — was 40.9%. The average\nrank of the hypothesis found by theA∗ search among the N-best\nones — after rescoring them using the structured language model\ninterpolated with the trigram — was 1.07 (minimum achievable\nvalue is 0). There were 585 offending sentences — out of a to-\ntal of 2427 test sentences — in which theA∗ search led to a\nhypothesis whose score was lower than that of the top hypoth-\nesis among the N-best (1-best). In 310 cases the preﬁx of the\nrescored 1-best was still in the stack whenA∗ returned — in-\nadequate compensation — and in the other 275 cases the 1-best\nhypothesis was lost during the search due to the ﬁnite stack size.\nOne interesting experimental observation was that even\nthough in the 585 offending cases the score of the 1-best was\nhigher than that of the hypothesis found byA∗ , the WER of\nthose hypotheses — as a set — washigherthan that of the set of\nA∗ hypotheses.\n5. CONCLUSIONS\nSimilar experiments on the Wall Street Journal corpus are re-\nported in [3] showing that the improvement holds even when the\nWER is much lower.\nWe believe we have presented an original approach to lan-\nguage modeling that takes into account the hierarchical structure\nin natural language. Our experiments showed improvement in\nboth perplexity and word error rate over current language mod-\neling techniques demonstrating the usefulness of syntactic struc-\nture for improved language models.\n6. ACKNOWLEDGMENTS\nThe authors would like to thank to Sanjeev Khudanpur for his\ninsightful suggestions. Also thanks to Bill Byrne for making\navailable the SWB lattices, Vaibhava Goel for making available\nthe N-best decoder and Vaibhava Goel, Harriet Nock and Murat\nSaraclar for useful discussions about lattice rescoring.\nREFERENCES\n[1] C. Chelba, D. Engle, F. Jelinek, V . Jimenez, S. Khudanpur,\nL. Mangu, H. Printz, E. S. Ristad, R. Rosenfeld, A. Stol-\ncke, and D. Wu. Structure and performance of a dependency\nlanguage model. InProceedings of Eurospeech, volume 5,\npages 2775–2778. Rhodes, Greece, 1997.\n[2] Ciprian Chelba and Frederick Jelinek. Exploiting syn-\ntactic structure for language modeling. InProceedings\nof COLING-ACL , volume 1, pages 225–231. Montreal,\nCanada, 1998.\n[3] Ciprian Chelba and Frederick Jelinek. Structured lan-\nguage modeling for speech recognition. InProceedings of\nNLDB99, to appear. Klagenfurt, Austria, 1999.\n[4] CLSP. WS97. InProceedings of the 1997 CLSP/JHU Work-\nshop on Innovative Techniques for Large Vocabulary Con-\ntinuous Speech Recognition. Baltimore, July-August 1997.\n[5] A. P. Dempster, N. M. Laird, and D. B. Rubin. Maximum\nlikelihood from incomplete data via the EM algorithm. In\nJournal of the Royal Statistical Society, volume 39 ofB,\npages 1–38. 1977.\n[6] J. J. Godfrey, E. C. Holliman, and J. McDaniel. SWITCH-\nBOARD telephone speech corpus for research and devel-\nopment. InProceedings of IEEE Conference on Acoustics,\nSpeech and Signal Processing, volume 1, pages 517–520.\nSan Francisco, March 1992.\n[7] Frederick Jelinek and Robert Mercer. Interpolated estima-\ntion of markov source parameters from sparse data. In\nE. Gelsema and L. Kanal, editors,Pattern Recognition in\nPractice, pages 381–397. 1980.\n[8] N. Nilsson.Problem Solving Methods in Artiﬁcial Intelli-\ngence, pages 266–278. McGraw-Hill, New York, 1971.\n[9] A. J. Viterbi. Error bounds for convolutional codes and\nan asymmetrically optimum decoding algorithm. InIEEE\nTransactions on Information Theory, volume IT-13, pages\n260–267, 1967.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.9699112176895142
    },
    {
      "name": "Trigram",
      "score": 0.9608150720596313
    },
    {
      "name": "Language model",
      "score": 0.8396037817001343
    },
    {
      "name": "Computer science",
      "score": 0.8063615560531616
    },
    {
      "name": "Complement (music)",
      "score": 0.6872020959854126
    },
    {
      "name": "Natural language processing",
      "score": 0.6522083282470703
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6180654764175415
    },
    {
      "name": "Speech recognition",
      "score": 0.5717089176177979
    },
    {
      "name": "Word error rate",
      "score": 0.570931613445282
    },
    {
      "name": "Word (group theory)",
      "score": 0.5701560378074646
    },
    {
      "name": "Locality",
      "score": 0.5521815419197083
    },
    {
      "name": "Probabilistic logic",
      "score": 0.5174860954284668
    },
    {
      "name": "Statistical model",
      "score": 0.47954320907592773
    },
    {
      "name": "Hidden Markov model",
      "score": 0.46441027522087097
    },
    {
      "name": "Linguistics",
      "score": 0.2856521010398865
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Phenotype",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Complementation",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ]
}