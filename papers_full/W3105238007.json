{
  "title": "ETC: Encoding Long and Structured Inputs in Transformers",
  "url": "https://openalex.org/W3105238007",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A3016646846",
      "name": "Joshua Ainslie",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2124070775",
      "name": "Santiago Ontañón",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2251626954",
      "name": "Chris Alberti",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1209566617",
      "name": "Vaclav Cvicek",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2121139472",
      "name": "Zachary Fisher",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3016369013",
      "name": "Philip Pham",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3016985753",
      "name": "Anirudh Ravula",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A632247528",
      "name": "Sumit Sanghai",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2127284166",
      "name": "Qifan Wang",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1993907199",
      "name": "Li Yang",
      "affiliations": [
        "Google (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2606964149",
    "https://openalex.org/W2891534142",
    "https://openalex.org/W2962943802",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2470673105",
    "https://openalex.org/W2946567085",
    "https://openalex.org/W2995575179",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2786396726",
    "https://openalex.org/W3023787386",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4288024261",
    "https://openalex.org/W4288623406",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2962785754",
    "https://openalex.org/W2963045354",
    "https://openalex.org/W2970467549",
    "https://openalex.org/W4297808394",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W2889787757",
    "https://openalex.org/W2970550868",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W2995435108",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2919624000",
    "https://openalex.org/W3036120435",
    "https://openalex.org/W4303684868",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2970777192",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3034987253",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2963684275",
    "https://openalex.org/W2963866616",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2911430044",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2118323718",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W2984864519",
    "https://openalex.org/W2797328513",
    "https://openalex.org/W2944898795",
    "https://openalex.org/W2964347512",
    "https://openalex.org/W3131922516",
    "https://openalex.org/W2737740651",
    "https://openalex.org/W2912924812",
    "https://openalex.org/W4293569541",
    "https://openalex.org/W2963403868"
  ],
  "abstract": "Joshua Ainslie, Santiago Ontanon, Chris Alberti, Vaclav Cvicek, Zachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, Li Yang. Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP). 2020.",
  "full_text": "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 268–284,\nNovember 16–20, 2020.c⃝2020 Association for Computational Linguistics\n268\nETC: Encoding Long and Structured Inputs in Transformers\nJoshua Ainslie, Santiago Onta˜n´on, Chris Alberti, Vaclav Cvicek,\nZachary Fisher, Philip Pham, Anirudh Ravula, Sumit Sanghai, Qifan Wang, Li Yang\nGoogle Research\n{jainslie, santiontanon, chrisalberti, vcvicek,\nzachﬁsher, phillypham, braineater, sumitsanghai, wqfcr, lyliyang}@google.com\nAbstract\nTransformer models have advanced the state\nof the art in many Natural Language Pro-\ncessing (NLP) tasks. In this paper, we\npresent a new Transformer architecture, Ex-\ntended Transformer Construction (ETC), that\naddresses two key challenges of standard\nTransformer architectures, namely scaling in-\nput length and encoding structured inputs. To\nscale attention to longer inputs, we introduce\na novel global-local attention mechanism be-\ntween global tokens and regular input tokens.\nWe also show that combining global-local at-\ntention with relative position encodings and\na Contrastive Predictive Coding (CPC) pre-\ntraining objective allows ETC to encode struc-\ntured inputs. We achieve state-of-the-art re-\nsults on four natural language datasets requir-\ning long and/or structured inputs.\n1 Introduction\nModels based on Transformers (Vaswani et al.,\n2017), such as BERT (Devlin et al., 2018), or other\nvariants (Yang et al., 2019; Lan et al., 2019; Raffel\net al., 2019) have yielded state-of-the-art results in\nmany NLP tasks such as language modeling (Child\net al., 2019; Sukhbaatar et al., 2019; Rae et al.,\n2019; Kitaev et al., 2020), question answering (Lan\net al., 2019; Beltagy et al., 2020), and summariza-\ntion (Zhang et al., 2019). We present the Extended\nTransformer Construction (ETC) architecture1, tar-\ngeting two limitations of the original models: (1)\nscaling input length, (2) encoding structured inputs.\nThe computational and memory complexity of\nattention in the original Transformer scales quadrat-\nically with the input length, typically limiting input\nlength to around 512 tokens. While 512 might\nbe enough for some tasks (e.g., co-reference res-\nolution seems to beneﬁt from even smaller input\nlengths (Joshi et al., 2019)), this is problematic in\n1Code at goo.gle/research-etc-model\nothers. Consider question answering (QA) tasks\nthat require reasoning across multiple documents\n(e.g., the HotpotQA dataset (Yang et al., 2018)) all\nof which must simultaneously ﬁt in the model input.\nOther examples are summarization, or QA on long\ndocuments. Many approaches have been proposed\nto address this, like hierarchical processing (Zhang\net al., 2019), sparse attention (Child et al., 2019),\nand segment-level recurrence (Dai et al., 2019).\nA second limitation is that few models focus on\nstructured inputs, by which we refer to any underly-\ning graph or hierarchical structure among the input\ntokens. Although ETC can encode more general\ngraph structure, in this work we focus on represent-\ning hierarchical structure in NLP tasks, not usually\nmodeled by Transformer models. For example, text\nis organized into sentences and paragraphs, and\nwhile these have a sequential order, different input\ndocuments might not hold any order between them\n(e.g., the HotpotQA dataset). Additionally, web\ntext contains markup and is laid out using a DOM\ntree, giving additional structure. We show ETC can\nrepresent these and other types of structure, like\nlinking different entity mentions.\nTo address these challenges, we present a novel\nattention mechanism called global-local attention,\nwhich divides the input into two sequences (which\nwe call the global input and the long input). This\nmechanism introduces local sparsity to reduce\nthe quadratic scaling of the attention mechanism.\nWhen this is coupled with relative position encod-\nings (Shaw et al., 2018), it allows for handling\nstructured inputs in a natural way. Additionally,\nunlike previous Transformer extensions, ETC can\nbe initialized from existing pre-trained standard\nBERT models (which together with a GPU/TPU-\nfriendly implementation, allows for efﬁcient model\ntraining)2. Our results show that initializing from\n2An exception to this is Longformer (Beltagy et al., 2020),\n269\nRoBERTa (Liu et al., 2019) signiﬁcantly improves\nperformance. Finally, we show that by adding a\npre-training Contrastive Predictive Coding (CPC)\ntask (Oord et al., 2018), performance improves\neven further for tasks where structure is important,\nas CPC plays the role of a masked language model\n(MLM) task, but at a sentence level of granularity.\nWe report experiments on four datasets: Natural\nQuestions (NQ) (Kwiatkowski et al., 2019), Hot-\npotQA (Yang et al., 2018), WikiHop (Welbl et al.,\n2018), and OpenKP (part of MS MARCO) (Xiong\net al., 2019), which have long and/or structured\ninputs. We set a new state of the art in all of them.\nMoreover, although in this paper we strictly fo-\ncus on ETC, in a related model called BigBird (Za-\nheer et al., 2020), we experimented with an alter-\nnative set of ideas to handle long inputs and its\nextensions to a decoder for text generation. The\nfocus of BigBird is on the idea of adding random\nsparse attention patterns to global-local attention,\nand on showing under which conditions models\nlike BigBird/ETC are universal approximators of\nsequence functions and are Turing complete. While\nthe key ideas and techniques required to achieve\nthe state-of-the-art results mentioned above for QA\ntasks are the focus of this paper, the reader is re-\nferred to the BigBird work for a joint evaluation of\nETC (referred to as BigBird-ETC in that work) and\nthe idea of random sparse attention patterns.\n2 Background\nMany variants of the original Transformer\nmodel (Vaswani et al., 2017) have been proposed\nfor scaling up training (RoBERTa, Liu et al., 2019),\nthe internal representation (ALBERT, Lan et al.,\n2019), or both (T5, Raffel et al., 2019), outper-\nforming BERT (Devlin et al., 2018) in tasks such\nas GLUE (Wang et al., 2018), SQuAD (Rajpurkar\net al., 2016) or RACE (Lai et al., 2017). However,\nthese models typically limit inputs to n= 512to-\nkens due to theO(n2) cost of attention. We classify\nprior approaches to scale up attention into four cat-\negories: sparse attention, recurrence, hierarchical\nmechanisms, and compressed attention.\nSparse Attention involves limiting each token\nto attend to a subset of the other tokens. For ex-\nample, the Sparse Transformer (Child et al., 2019)\nused predeﬁned attention patterns for both text and\nimage generation. They showed that attending only\na new model developed concurrently to ETC, which also al-\nlows initialization from BERT/RoBERTa.\nto previous pixels in the same row or column was\nenough to generate high quality images, while keep-\ning attention cost at O(n√n). In the Adaptive At-\ntention Span Transformer (Sukhbaatar et al., 2019)\neach attention head is associated with a decaying\nlearnable masking function, which limits the num-\nber of tokens it can attend to. They show that lower\nlayers learn to use short attention spans, and only\nin higher layers are attention spans longer. Sparse\nattention has also been used to increase the inter-\npretability of attention heads by allowing attention\nto assign exactly zero weight to certain input to-\nkens (Correia et al., 2019). The Reformer (Kitaev\net al., 2020) model ﬁnds the nearest neighbors of\nthe attention query (those input tokens that would\nresult in the highest attention weights) using local-\nity sensing hashing (Andoni et al., 2015) and only\nuses those for attention. This reduces attention cost\nto O(nlog(n)). The Routing Transformer (Roy\net al., 2020) learns dynamic sparse attention pat-\nterns using online k-means, reducing complexity to\nO(n1.5). Finally, the most related approach to the\nwork presented in this paper is Longformer (Belt-\nagy et al., 2020), developed concurrently to ETC,\nand which features a very similar global-local at-\ntention mechanism as ETC’s but does not directly\nencode graph or hierarchical structure (more de-\ntailed comparison in Section 3).\nRecurrence incorporates elements of recur-\nrent neural networks into Transformer models to\nlengthen their attention span. Transformer-XL (Dai\net al., 2019) takes this approach, dividing the input\nsequence into segments and then processing these\nsegments one at a time. At each layer, the model\nattends to the layer immediately below for both the\ncurrent and previous input segments. The effect is\nthat layer k is inﬂuenced by the current segment\nand the k−1 previous segments, as shown in the\ntop-right of Figure 1.\nIn Hierarchical Mechanisms the input se-\nquence is split into blocks that are ingested inde-\npendently to produce summary embeddings that\nrepresent the whole block. Then, separate layers\ningest the concatenation of these embeddings. For\nexample, HIBERT (Zhang et al., 2019) uses this\nidea at the sentence level for extractive summariza-\ntion (illustrated in the bottom-left of Figure 1). Hi-\nerarchical attention in Transformers has also been\napplied to other NLP tasks such as neural machine\ntranslation (Maruf et al., 2019). Moreover, notice\nthat this idea of processing the input hierarchically\n270\n...\ntt-1t-2\nSpan Nodes\nInput Tokens\nInput Tokens\nInput Tokens\nLayer 1\nembeddings\nLayer 2\nembeddings\nSegments\nInput Tokens\nBlock\nEmbeddings\nBlocks\nInput Tokens\nSpecial\nGlobal Token\n...\ntt-1t-2\nInput Tokens\nLayer 1\nembeddings\nLayer 2\nembeddings\nSegments\nMemory\nCompressed\nMemory\nStandard Transformer:\nHierarchical Attention (HIBERT):\nTransformer XL:BP Transformer:\nStar Transformer:\nCompressive Transformer:\nLong Input\nGlobal Input\nfull g2g\nattention\nlocal\nl2l attention\nfull g2l and \nl2g attention \nGlobal-Local Attention (ETC):\nFigure 1: An illustration of mechanisms to scale attention to long inputs, including our proposed model, ETC.\nis not speciﬁc to Transformer models, and it has\nbeen applied to recurrent neural network models\nboth at the level of sentences (Yang et al., 2016;\nMiculicich et al., 2018) and blocks (Shen et al.,\n2018).\nCompressed Attention takes the idea of hier-\narchical attention one step further by selectively\ncompressing certain parts of the input. The BP-\nTransformer (Ye et al., 2019) model builds a binary\npartitioning tree over the input, and only lets the\nmodel attend to the leaves (the raw tokens) for\nnearby tokens, and higher nodes in the tree (sum-\nmaries of groups of tokens) as tokens grow more\ndistant (see Figure 1, middle top). Other ideas\ninclude memory compressed attention (Liu et al.,\n2018) where groups of k tokens are compressed\nvia a convolution ﬁlter before they are attended\nto, and the Star Transformer (Guo et al., 2019),\nwhere each token can attend only to its immedi-\nate left/right neighbors and to a separate special\nauxiliary token that represents a summary of the\nwhole input (see Figure 1, left). The Compressive\nTransformer (Rae et al., 2019) integrates this idea\ninto Transformer-XL by compressing tokens in the\ninput that are far away. The model beneﬁts from\ndetailed attention to nearby tokens, while using\nsummarized information for more distant tokens\n(see Figure 1, lower right).\n3 Extended Transformer Construction\nOur model follows the original Transformer archi-\ntecture (Vaswani et al., 2017), with key modiﬁ-\ncations to tackle long and structured inputs: rela-\ntive position encoding, global-local attention, and\na CPC pre-training task, explained below. In this\npaper, we consider only the encoder side of the\nTransformer, and leave the decoder for future work.\n3.1 Relative Position Encoding\nInspired by the work of Shaw et al. (2018), ETC re-\nplaces absolute position encodings with relative po-\nsition encodings, which provide information about\nthe relative position of tokens in the input sequence\nwith respect to one another. Given the input se-\nquence x= (x1,...,x n), we can see it as a labeled\nfully connected and directed graph, where lij is the\nlabel of the edge that connects xi to xj. Given a\nmaximum clipping distance k, Shaw et al. deﬁne\n2k+ 1relative position labels: l−k,...,l k. The la-\nbel of the edge between two input tokens depends\nonly on their relative position j −i. For input\npairs with j−i ≥k, label lk is given, and with\nj−i≤−k, l−k is given. Each label then becomes\na learnable vector aK\nl , which modiﬁes the attention\nmechanism (equations in the next section)3.\nRelative position encodings are independent of\ninput length, so it is easy to adapt a model to greater\ninput lengths than seen during pre-training. As\nother recent work (Shaw et al., 2019), ETC’s at-\ntention mechanism uses relative position labels not\njust for relative positions in a sequence but also to\nexpress arbitrary pairwise token relations useful for\n3In the work of Shaw et al., a second aV\nl vector was used,\nbut their ablations showed it may not affect performance.\n271\nng\n \n \n \n \n \n \n \n \nAttention keys\nAttention queries\n \n \n \n \n \n \n \n \nAttention keys\nAttention queries\nAttention keys\nAttention queries\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ng2g g2l\nl2g l2l\nnl\n2r+1\nn\na) b)\nng\nAttention keys\nAttention queries\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ng2g g2l\nl2g l2l\nnl\n2r+1\nc)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \na) b) \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 2: Sparsity diagram showing which attention queries (rows) can attend to which attention keys (columns)\na) for standard Transformer attention with input size n; b) for global-local attention with input sizes ng, nl, and\nradius r; c) how the l2l attention piece is reshaped into a much smaller attention matrix, limited by local radius.\nstructured inputs, as explained below.\n3.2 Global-Local Attention\nGlobal-local attention is a generalization of several\nof the models presented above. ETC receives two\nseparate input sequences: the global input xg =\n(xg\n1,...,x g\nng ) and the long input xl = (xl\n1,...,x l\nnl).\nTypically, the long input contains the input a stan-\ndard Transformer would receive, while the global\ninput contains a much smaller number of auxiliary\ntokens (ng ≪nl). Attention is then split into four\nseparate pieces: global-to-global (g2g), global-to-\nlong (g2l), long-to-global (l2g), and long-to-long\n(l2l). Attention in the l2l piece (the most compu-\ntationally expensive piece) is restricted to a ﬁxed\nradius r ≪nl. To compensate for this limited at-\ntention span, the tokens in the global input have un-\nrestricted attention, and thus long input tokens can\ntransfer information to each other through global\ninput tokens. Accordingly, g2g, g2l, and l2g pieces\nof attention are unrestricted.\nThis concept is illustrated in Figure 2, where\neach cell (row i, column j) is shaded grey if token\nxi can attend to token xj. As we can see, in a\nregular Transformer, attention is unrestricted (full\nn×n attention). ETC, illustrated in Figure 2b,\nhowever, restricts the l2l piece to a local radius,\nsigniﬁcantly reducing computational and memory\ncomplexity for very long inputs. Conceptually, the\nl2l attention piece is reshaped into a nl ×(2r+ 1)\nmatrix as illustrated in Figure 2c.4\nIf r= 1and ng = 1, we recover exactly the Star\nTransformer (Section 2). Similarly, placing all the\ntokens in the global input and setting nl = 0yields\nstandard Transformer attention. Attention in ETC\nis O(ng(ng +nl)+ nl(ng +2r+1)). If we assume\n4In practice, for GPU/TPU efﬁciency, a different reshaping\noccurs that yields identical outputs (see the appendices).\nng = O(2r+ 1), we see attention is linear in the\nsize of the long input: O(n2\ng + ngnl).\nTo provide ﬂexible attention and help with struc-\ntured inputs, per-instance Boolean attention matri-\nces Mg2g, Mg2l, Ml2g, and Ml2l exist, with zeroes\nfor those pairs of tokens that should not attend to\none another. Each g2g attention head works as fol-\nlows. Given the global input xg = (xg\n1,...,x g\nng ),\nwhich is a sequence of token representations xg\ni ∈\nRdx, the output of attention is zg = (zg\n1 ,...,z g\nng ),\nwhere each zg\ni ∈Rdz is calculated as follows:\nzg\ni =\nng∑\nj=1\nαg2g\nij xg\nj WV\nαg2g\nij =\nexp(eg2g\nij )\n∑n\nℓ=1 exp(eg2g\niℓ )\neg2g\nij =\nxg\ni WQ(xg\nj WK + aK\nij )T\n√dz\n−(1 −Mg2g\nij )C\nwhere: Mg2g is a binary attention mask,WQ, WK,\nand WV are learnable weight matrices, and aK\nij are\nlearnable vectors representing the relative position\nlabels, and C is a large constant ( C = 10000 in\nour experiments to follow the same convention as\nBERT). Attention for the other 3 pieces is analo-\ngous. We experiment with having separate WK\nand WV across all four attention pieces, or sharing\nthem. And for WQ, we experiment with having\none for g2g and g2l, and a separate one for l2g\nand l2l; or sharing them also. To recover BERT as\na special case when r is large enough to remove\nsparsity, attention is actually only split into 2 pieces\ninternally instead of 4, as g2g+g2l can be computed\njointly (top half of Figure 2c), and l2g+l2l can also\nbe computed jointly (bottom half of Figure 2c). A\nsingle softmax is used to jointly calculate αg2g\nij and\nαg2l\nij , and another for αl2g\nij and αl2l\nij .\n272\nng\n \n \n \n \n \n \n \n \nAttention keys\nAttention queries\n \n \n \n \n \n \n \n \nAttention keys\nAttention queries\nAttention keys\nAttention queries\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ng2g g2l\nl2g l2l\nnl\n2r+1\nn\na) b)\nng\nAttention keys\nAttention queries\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \ng2g g2l\nl2g l2l\nnl\n2r+1\nc)\n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \na) b) \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nb) \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \n \nFigure 3: Example attention patterns for handling (a)\nlong inputs and (b) structured inputs. White back-\nground means attention is masked viaM, and the other\ncolors indicate different relative position labels.\nThus, the output of global-local attention is a\nsequence of length ng and one of length nl. These\nsequences go through a layer normalization and\nfeed forward layer in the same way as in the stan-\ndard transformer.\n3.3 Long Inputs and Global-Local Attention\nLet us illustrate how ETC can be used to encode\nlong inputs. A general way to handle long inputs\nin ETC is to place the entire sequence of input to-\nkens (e.g., word pieces) in the long input, and then\nassuming some sort of division into segments (e.g.,\nsentences), place one auxiliary token in the global\ninput per segment in the long input. We then use\none relative position label to link the global seg-\nment tokens with the word piece tokens that belong\nto them, and a different label for those that do not.\nMoreover, as we will show in the experiments be-\nlow, we have seen that using the Mg2l attention\nmasks to perform hard masking in one direction\n(g2l) can bring performance gains in some datasets.\nThis last asymmetric hard-masking is illustrated in\nFigure 3a, where we used different colors to indi-\ncate different relative position labels. In this way,\nalthough tokens in the long input can only attend\nto the local neighborhood deﬁned by the radius k,\nthey can indirectly attend to all other tokens in the\ninput sequence via the global tokens.\n3.4 Structured Inputs\nA standard Transformer resembles a graph neural\nnetwork (Scarselli et al., 2008) over a fully con-\nnected graph g; see Ye et al. (2019). Thanks to the\ncombination of global-local attention and relative\nposition labels, ETC exploits this relation to encode\nstructured inputs. Given the input x= (x1,...,x n),\nwe use the term structure to refer to the relations\nthat exist between the tokens in x. When x is a\nplain ordered sequence, the only relation is the se-\nquential orderof tokens, which is the only structure\ncaptured by BERT (encoded by absolute position\nencodings, used to modify attention). We deﬁne\nstructured inputs as those that have additional rela-\ntions between the tokens beyond sequential order.\nIn principle, we could think of inputs with arbitrary\ngraph structure (such as chemical molecule graphs),\nbut here we focus on structure in NLP tasks.\nETC is particularly well suited to capture hi-\nerarchical structure thanks to three mechanisms.\nFirst, as originally conceived, the vocabulary of\nrelative position labels is used to represent token\nrelative positions. However, seeing a Transformer\nas a graph neural network over a graph g(with one\nvertex per token in x, and edges representing their\nrelations), we can expand this vocabulary to label\nsome edges with labels for relations such as is-a,\npart-of, or others. Second, the division between\nlong and global input induces a natural structure\nwhere the global input contains summary tokens\nof sets of tokens in x(a 2-level hierarchy). How-\never, we can also have tokens summarizing sets of\nsummary tokens (constructing a 3-level hierarchy,\nor beyond). Third, if some pairs of tokens should\nnot have an edge between them, this can be cap-\ntured with the Mg2g, Mg2l, Ml2g, Ml2l masks. An\nillustration of all these concepts is shown in Fig-\nure 3b, which uses masking and relative position\nlabels to represent a context-sentence-token hierar-\nchy that includes within-context order of sentences\nbut no order between contexts. Another example\nwould be social community graphs structure, where\nwe could partition the graph into components, use\nMl2l to constrain attention to within components,\nand add per-component global tokens, linked to al-\nlow information to propagate from one component\nto another in a hierarchical way.\n3.5 Pre-training Tasks\nWe use two pre-training tasks: (1) a masked lan-\nguage model (MLM) task with whole word mask-\ning (if one word piece token is masked, then all\nother tokens of the same word are masked); and\n(2) instead of using BERT’s next sentence predic-\ntion (NSP), we adapt Contrastive Predictive Coding\n(CPC) (Oord et al., 2018) for ETC.\nThe goal of CPC is to predict subsequent inputs\nin latent space, i.e., to predict internal hidden rep-\nresentations of blocks of tokens. We adapted this\nidea in ETC by using global input sentence sum-\n273\nmary tokens. Given an input sequence containingn\nsentences, we mask all the tokens corresponding to\na subset of sentences (but leave the sentence sum-\nmary tokens in the global input). We then train the\nmodel to minimize the difference between the hid-\nden representation of the global sentence summary\ntokens for the masked sentences with respect to\nthat of a global summary token that can see the un-\nmasked sentence and nothing else. We use a Noise\nContrastive Estimation (NCE) loss as in the work\nof Oord et al. (2018) (details in the appendices).\nHaving described ETC, we can now compare\nit with Longformer (Beltagy et al., 2020), which\nuses a similar attention mechanism, except Long-\nformer has a single input sequence with some to-\nkens marked as global (the only ones that use\nfull attention). The key differences are that (1)\nETC’s combination of global-local attention with\nrelative position encodings and ﬂexible masking\nenables it to encode structured inputs in a simi-\nlar way as graph neural networks do; (2) global\ntokens in Longformer are never pre-trained with\nanything like our CPC loss, and thus their use must\nbe learned during ﬁne-tuning.\n3.6 Lifting Weights from Existing Models\nETC and BERT share enough similarities that\nBERT parameters are useful to perform a warm\nstart. The parameters are compatible because the\nglobal-local attention mechanism includes BERT\nas a special case if the input is small enough or the\nlocal radius is large enough to eliminate sparsity.\nMoreover, when lifting weights from BERT into\nan ETC model with separate WQ, WK, and WV\nprojection matrices, BERT’s parameters are just\ncopied over to the different matrices of ETC.\nAlthough pre-training is still required to adapt\nthe weights to use global tokens and relative po-\nsition encodings, we show that initializing from\nRoBERTa results in signiﬁcant performance im-\nprovements compared to pre-training from scratch.\nSpeciﬁcally, we initialized from the RoBERTa\ncheckpoints reported in the work of Rothe et\nal. (Rothe et al., 2020).\n4 Empirical Evaluation\nThis section focuses on evaluating our two main\ncontributions: (1) long inputs, and (2) structure in\ntext inputs, as well as initialization from existing\nBERT models. We chose four datasets (Table 1)\nwith long inputs or interesting input structure.\nInstances Instance length\nDataset Training Dev Median 95% Max\nNQ 307373 7830 4004 17137 156551\nHotpotQA 90447 7405 1227 1810 3560\nWikiHop 43738 5129 1541 3994 20337\nOpenKP 133724 6610 761 4546 89183\nTable 1: Dataset stats (length in word piece tokens).\nNQ (Kwiatkowski et al., 2019): in Google’sNat-\nural Questions (NQ) dataset the input consists of a\nquestion and a full Wikipedia article. The task is to\nidentify both a short answer (a few words from the\narticle) and along answer (e.g., a whole paragraph),\nif they exist within the article (and otherwise, return\nnull answers). Performance is measured based on\nthe F1 score of the model predictions with respect\nto the human generated answers.\nHotpotQA (Yang et al., 2018) is a question an-\nswering dataset where the goal is to combine evi-\ndence from multiple contexts. We use the distrac-\ntor setting, where 10 paragraphs are provided: two\nof them contain useful information to answer the\nquestion, and the rest are distractors. The task is\nboth to answer the question, and also to identify\nthe supporting facts that are relevant to answer the\nquestions (at a sentence granularity).\nWikiHop (Welbl et al., 2018) is similar in struc-\nture to HotpotQA. The contexts correspond to por-\ntions of Wikipedia articles, and the goal is to an-\nswer about properties of an entity that cannot be\nfound in the entity’s article. Each instance con-\ntains a query, a collection of candidate answers,\nand a collection of contexts from which to obtain\ninformation to select among the candidate answers.\nOpenKP (Xiong et al., 2019) is a keyphrase\nextraction dataset. Each document contains up to\n3 short keyphrases to be identiﬁed. We selected\nthis dataset as the input is not ﬂat text sequences,\nbut websites, including the hierarchical and spatial\nrelations between the different DOM elements on\nthe website, as well as other visual properties.\n4.1 Training Conﬁguration\nWe use two basic conﬁgurations: base and large.\nBase uses 12 layers, 768 hidden size, 12 attention\nheads, local attention radius r = 84, and relative\nposition maximum distance k = 12. Large uses\n24 layers, 1024 hidden size, 16 heads, r = 169,\nand k = 24. We used 128, 230 and 460 global\ntokens for models with 512, 4096 and 8192 long\n274\nModel Input length Conﬁguration #Params Long answer F1 Short answer F1\nBERT-base 512 110M 0.634 0.475\nBERT-large 512 340M 0.647 0.527\nRikiNet 512 lifting from RoBERTa large - 0.753 0.593\nETC 512 shared, no CPC, no hard g2l 109M 0.645 0.478\nETC 4096 shared, no CPC, no hard g2l 109M 0.692 0.497\nETC 4096 ﬁxed blocks, shared, no CPC, no hard g2l 109M 0.697 0.508\nETC 4096 shared, no hard g2l 109M 0.717 0.524\nETC 4096 shared 109M 0.721 0.514\nETC 4096 - 166M 0.725 0.522\nETC 8192 166M 0.740 0.542\nETC 4096 2x local radius 166M 0.737 0.530\nETC 4096 2x relative vocab 166M ∗ 0.733 0.532\nETC 4096 2x pre-training 166M 0.746 0.558\nETC-large 4096 539M 0.761 0.565\nETC-large 4096 lifting from RoBERTa 558M 0.782 0.585\nTable 2: Empirical results on the dev sev set for the Natural Questions (NQ) dataset. Best results for base and\nlarge models highlighted. BERT-large results obtained from Alberti et al. (2019). ∗ although not visible due to\nrounding to the closest million, doubling the relative position encoding vocabulary adds about 600k parameters.\ninput size respectively in NQ5, 256 global tokens in\nHotpotQA, 430 in WikiHop, and 512 in OpenKP.\nPre-training: We place all word piece tokens\nin the long input and add one auxiliary token per\nsentence to the global input. We defaulted to\nBERT’s 30k English uncased word piece vocab-\nulary. Models were pre-trained using the original\nBERT datasets, except that documents with fewer\nthan 7 sentences were ﬁltered out. Unless stated\notherwise, base models were pre-trained with the\nsame total number of tokens as the original BERT,\nand for large models, twice as many. We used the\nLAMB optimizer (You et al., 2019) with learning\nrate set to\n√\n8 ×10−3.\nFine-tuning: we put all input tokens in the long\ninput (CLS, question, and context tokens for QA\ndatasets), and use relative position labels to encode\nstructure (see Section 3.4). Global input has a CLS\ntoken, tokens mirroring the question tokens in long,\nand one summary token per paragraph/sentence (or\nVDOM block in OpenKP). OpenKP had no CLS\nnor question tokens. For WikiHop, we also add\none global token per candidate answer, and used a\ndifferent relative position label to link these tokens\nto their string-matched mentions in the text (more\ndetails in the appendices).\n4.2 Results on the Dev Set\nNQ: We used NQ to study the different parts of\nETC via ablations. Results are shown in Table 2.\nThe ﬁrst three rows show baseline models: BERT-\nbase, BERT-large, and RikiNet (Liu et al., 2020)\n(one of the best models in the NQ leaderboard).\n5With gradient checkpointing, ETC can scale beyond this,\nbut we limit our experiments to 8192 tokens for this paper.\nBERT’s performance is comparable to ETC using\ninput length of 512. The smaller local radius of\nETC (84) puts ETC at a disadvantage with respect\nto BERT, but other ETC improvements, such as\ndynamic whole word masking seem to compensate.\nThe rest of Table 2 shows performance under dif-\nferent ablations. Our default conﬁguration (marked\nwith a “-” in the conﬁguration column) is ETC-base\nwith long input length of 4096 tokens, using CPC,\nhard g2l masking, and separateWQ, WK, and WV\nmatrices for long/global inputs. We tested the fol-\nlowing ablations: shared (sharing all model param-\neters for attention across both the global and long\ninputs), no CPC (removing the CPC pre-training\ntask), no hard g2l (not having a hard g2l mask),\nand ﬁxed blocks (which conﬁgures the global in-\nput to just have one global token per 97 long input\ntokens, to keep the same proportion as without\nﬁxed-blocks, ignoring sentence boundaries, and\nnot having any other tokens in the global input for\npre-training or ﬁne-tuning). Sharing WQ, WK,\nand WV and removing CPC signiﬁcantly hurt the\nperformance of ETC in NQ7. Using ﬁxed blocks,\nsurprisingly, seems to slightly help without CPC.\nIncreasing long input from 512 to 4096 signiﬁ-\ncantly helped performance, and going to 8192 in-\ncreased performance further to 0.740 / 0.542, high-\nlighting the importance of longer inputs. Increasing\nthe local radius, relative position vocabulary, or the\namount of pre-training all helped performance (es-\npecially the latter, reaching 0.746 / 0.558). Moving\nto a large model also helped, especially when lift-\ning from RoBERTa (both large models used the\n7Separate projection matrices were also found to be helpful\nin other models, like Longformer (Beltagy et al., 2020).\n275\nModel Input length Conﬁguration #Params HotpotQA WikiHop\nAns. F1 / Sup. F1 Acc.\nLongformer 4096 149M ∗ 0.743 / 0.844 75.0\nLongformer-large 4096 435M ∗ 0.788 / 0.8606 77.6\nETC 4096 ﬂat structure, no CPC, no hard g2l 166M 0.722 / 0.857 70.0\nETC 4096 ﬂat structure 166M 0.748 / 0.870 70.7\nETC 4096 no CPC 166M 0.747 / 0.866 73.0\nETC 4096 no hard g2l 166M 0.743 / 0.864 75.9\nETC 4096 shared 109M 0.733 / 0.866 73.7\nETC 4096 - 166M 0.751 / 0.869 73.2\nETC-large 4096 539M 0.798 / 0.890 77.0\nETC-large 4096 lifting from RoBERTa 558M 0.813 / 0.894 79.8\nTable 3: Empirical results on HotpotQA and WikiHop (dev set results). ∗Longformer parameter counts provided\nby the authors via personal communication.\nModel Input length Conﬁguration #Params OpenKP F1@3\nRoBERTa-JointKPE 512 - 0.398\nETC 512 ﬁxed blocks, no CPC, no hard g2l, no visual features 166M 0.399\nETC 4096 ﬁxed blocks, no CPC, no hard g2l, no visual features 166M 0.400\nETC 4096 no CPC, no hard g2l, no visual features 166M 0.400\nETC 4096 no hard g2l, no visual features 166M 0.400\nETC 4096 no visual features 166M 0.402\nETC 4096 - 166M 0.409\nETC 4096 shared 109M 0.409\nETC 4096 max loss 166M 0.416\nETC-large 4096 max loss 539M 0.419\nETC-large 4096 max loss, lifting from RoBERTa 558M 0.423\nTable 4: Empirical results on OpenKP (dev set F1@3 results).\nLeaderboard Result Position\nNQ long answer 0.7778 1st\nNQ short answer 0.5786 18th\nHotpotQA Sup. F1 0.8909 1st\nHotpotQA Overall 0.7362 3rd\nWikiHop 0.8225 1st\nOpenKP 0.4205 1st\nTable 5: Ofﬁcial leaderboard results for ETC at the time\nof submission.\nRoBERTa vocabulary). Lifting from RoBERTa\nachieved our best scores: 0.782 / 0.585, beating\nthe best dev scores in the literature for long answer\n(compare with 0.754 / 0.593 for RikiNet). For short\nanswer, we still lag behind RikiNet.\nHotpotQA, WikiHop: Table 3 shows our re-\nsults in HotpotQA and WikiHop. We show two\nLongformer models as baselines (which is currently\nthe state-of-the-art model in WikiHop), as well as\nablations to study the effect of structure in the re-\nsults. In particular, we consider a ﬂat structure\nablation where: (1) we do not break long input at-\ntention by context boundaries, (2) we limit relative\nposition labels between global and long tokens to\nrepresenting only sentence-level relationships (this\nremoves any special attention in WikiHop between\ncandidate answers and their mentions).\nOur results show that both our base and large\nmodels outperform their corresponding Long-\nformer models in both HotpotQA and WikiHop.\nBesides parameter counts, the main factors that can\nexplain this difference in performance are the dif-\nferent pre-training strategies and the different han-\ndling of structure in ETC and Longformer. Remov-\ning the CPC pre-training task, and not using a hard\ng2l mask signiﬁcantly hurt the performance of the\nmodel in HotpotQA, going from a performance of\n0.751 / 0.869 for the baseline model to 0.722 / 0.857\nusing none of those features. Using a ﬂat structure\n(but keeping CPC and hard g2l) did not seem to\nhurt in HotpotQA. WikiHop shows a slightly dif-\nferent picture, as it seems that hard g2l masking\nand especially ﬂat structure hurt performance in\nthis dataset. Our best model is the base conﬁgura-\ntion without hard g2l masking, which achieves an\naccuracy of 75.9. Interestingly, sharing WQ, WK,\nand WV seems to help performance in WikiHop.\nThis is our smallest dataset, and maybe the added\ncapacity of the model without sharing parameters\nleads it to overﬁt.\nOpenKP: Table 4 shows our results on the\nOpenKP dataset, using RoBERTa-JointKPE (Sun\net al., 2020) as the baseline, which is currently #1\nin the leaderboard. This is an interesting struc-\ntured dataset, and thus, we performed additional\n276\nablations to investigate the effect of removing such\nstructural information. Our results show that even\nthe most constrained ETC model already achieves\nvery good performance (0.399), and scaling to 4096\nlength seems to give a slight boost. Using hard g2l\nalso helps, and adding the visual features brings\nthe largest beneﬁt. Finally, we see that using a\nlarge model, and especially lifting weights from\nRoBERTa improve results signiﬁcantly. As with\nWikiHop, sharing WQ, WK, and WV does not\nhurt performance. Our default model uses the ﬁrst\noccurrence of a keyphrase, but we saw that using\nthe maximum logit of all occurrences ( max loss)\nimproved results.\n4.3 Ofﬁcial Leaderboard Results\nFinally, Table 5 shows ofﬁcial results on the leader-\nboards of each dataset. The model submitted to\nthe leaderboards was the model with best dev set\nresults (shown at the bottom of the respective re-\nsults tables, lifting weights from RoBERTa). We\nset a new state of the art in WikiHop and OpenKP,\nNQ long answer, and HotpotQA Support F1. Re-\nmarkably, our submissions were all single model,\noutperforming the leaderboard ensemble models.\n5 Conclusions\nThis paper introduced the Extended Transformer\nConstruction (ETC), an architecture designed to (1)\nscale up the input length (linearly with input), and\n(2) encode structured inputs. ETC allows lifting\nweights from existing BERT models, improving\nresults signiﬁcantly. The key ideas are a new global-\nlocal attention mechanism, coupled with relative\nposition encodings and a CPC pre-training task.\nWe showed that signiﬁcant gains can be obtained\nthanks to increased input sequence length. The\nability to represent dataset structure in ETC further\nimproves the model quality. We hypothesize that\nCPC helps the model train the usage of the higher-\nlevel global input summary tokens, as CPC plays\na role akin to MLM, but at the global input level.\nNotice that although our datasets contain a limited\namount of structure (compared to graph datasets),\nour experiments show that ETC was able to exploit\nthis existing structure.\nAs future work, we would like to investigate\ncomplementary attention mechanisms like those of\n7Better results were reported for Longformer-large using a\n2 stage approach, reaching 81.0 / 85.8 (Beltagy et al., 2020),\nbut our table shows single-model results only, for comparison.\nReformer (Kitaev et al., 2020) or Routing Trans-\nformer (Roy et al., 2020), push scalability with\nideas like those from RevNet (Gomez et al., 2017),\nand study the performance of ETC in datasets with\neven richer structure.\nAcknowledgements\nWe would like to thank Anna Goldie, Bhargav\nKanagal, Ilya Eckstein, Manan Shah, Nich Kwon,\nVikram Rao Sudarshan, Joshua Maynez, Manzil\nZaheer, Kelvin Guu, Tom Kwiatkowski, Kristina\nToutanova, and D. Sivakumar for helpful discus-\nsions, support, comments, and feedback on earlier\nversions of this work. We would also like to thank\nthe Longformer authors (Iz Beltagy, Matthew E.\nPeters, Arman Cohan) for their useful feedback\non earlier versions of this paper and for sharing\nparameter counts.\nReferences\nChris Alberti, Kenton Lee, and Michael Collins. 2019.\nA bert baseline for the natural questions. arXiv\npreprint arXiv:1901.08634.\nAlexandr Andoni, Piotr Indyk, Thijs Laarhoven, Ilya\nRazenshteyn, and Ludwig Schmidt. 2015. Practical\nand optimal lsh for angular distance. In Advances in\nneural information processing systems, pages 1225–\n1233.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv preprint arXiv:2004.05150v1.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nGonc ¸alo M Correia, Vlad Niculae, and Andr´e FT Mar-\ntins. 2019. Adaptively sparse transformers. arXiv\npreprint arXiv:1909.00015.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc V Le, and Ruslan Salakhutdinov.\n2019. Transformer-xl: Attentive language mod-\nels beyond a ﬁxed-length context. arXiv preprint\narXiv:1901.02860.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nAidan N Gomez, Mengye Ren, Raquel Urtasun, and\nRoger B Grosse. 2017. The reversible residual net-\nwork: Backpropagation without storing activations.\nIn Advances in neural information processing sys-\ntems, pages 2214–2224.\n277\nQipeng Guo, Xipeng Qiu, Pengfei Liu, Yunfan Shao,\nXiangyang Xue, and Zheng Zhang. 2019. Star-\ntransformer. arXiv preprint arXiv:1902.09113.\nCheng-Zhi Anna Huang, Ashish Vaswani, Jakob\nUszkoreit, Ian Simon, Curtis Hawthorne, Noam\nShazeer, Andrew M Dai, Matthew D Hoffman, Mon-\nica Dinculescu, and Douglas Eck. 2018. Music\ntransformer: Generating music with long-term struc-\nture. In International Conference on Learning Rep-\nresentations.\nMandar Joshi, Omer Levy, Daniel S Weld, and Luke\nZettlemoyer. 2019. Bert for coreference reso-\nlution: Baselines and analysis. arXiv preprint\narXiv:1908.09091.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. arXiv\npreprint arXiv:2001.04451.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nﬁeld, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin,\nKenton Lee, et al. 2019. Natural questions: a bench-\nmark for question answering research. Transactions\nof the Association for Computational Linguistics ,\n7:453–466.\nGuokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang,\nand Eduard Hovy. 2017. Race: Large-scale reading\ncomprehension dataset from examinations. arXiv\npreprint arXiv:1704.04683.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nDayiheng Liu, Yeyun Gong, Jie Fu, Yu Yan, Jiusheng\nChen, Daxin Jiang, Jiancheng Lv, and Nan\nDuan. 2020. Rikinet: Reading wikipedia pages\nfor natural question answering. arXiv preprint\narXiv:2004.14560.\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben\nGoodrich, Ryan Sepassi, Lukasz Kaiser, and\nNoam Shazeer. 2018. Generating wikipedia by\nsummarizing long sequences. arXiv preprint\narXiv:1801.10198.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nSameen Maruf, Andr ´e FT Martins, and Gholamreza\nHaffari. 2019. Selective attention for context-\naware neural machine translation. arXiv preprint\narXiv:1903.08788.\nLesly Miculicich, Dhananjay Ram, Nikolaos Pappas,\nand James Henderson. 2018. Document-level neural\nmachine translation with hierarchical attention net-\nworks. arXiv preprint arXiv:1809.01576.\nAaron van den Oord, Yazhe Li, and Oriol Vinyals.\n2018. Representation learning with contrastive pre-\ndictive coding. arXiv preprint arXiv:1807.03748.\nJack W Rae, Anna Potapenko, Siddhant M Jayakumar,\nand Timothy P Lillicrap. 2019. Compressive trans-\nformers for long-range sequence modelling. arXiv\npreprint arXiv:1911.05507.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J Liu. 2019. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer. arXiv preprint arXiv:1910.10683.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100,000+ questions\nfor machine comprehension of text. arXiv preprint\narXiv:1606.05250.\nSascha Rothe, Shashi Narayan, and Aliaksei Severyn.\n2020. Leveraging pre-trained checkpoints for se-\nquence generation tasks. Transactions of the Asso-\nciation for Computational Linguistics, 8:264–280.\nAurko Roy, Mohammad Saffar, Ashish Vaswani, and\nDavid Grangier. 2020. Efﬁcient content-based\nsparse attention with routing transformers. arXiv\npreprint arXiv:2003.05997.\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus\nHagenbuchner, and Gabriele Monfardini. 2008. The\ngraph neural network model. IEEE Transactions on\nNeural Networks, 20(1):61–80.\nPeter Shaw, Philip Massey, Angelica Chen, Francesco\nPiccinno, and Yasemin Altun. 2019. Generating log-\nical forms from graph representations of text and en-\ntities. arXiv preprint arXiv:1905.08407.\nPeter Shaw, Jakob Uszkoreit, and Ashish Vaswani.\n2018. Self-attention with relative position represen-\ntations. arXiv preprint arXiv:1803.02155.\nNoam Shazeer and Mitchell Stern. 2018. Adafactor:\nAdaptive learning rates with sublinear memory cost.\narXiv preprint arXiv:1804.04235.\nTao Shen, Tianyi Zhou, Guodong Long, Jing Jiang,\nand Chengqi Zhang. 2018. Bi-directional block self-\nattention for fast and memory-efﬁcient sequence\nmodeling. arXiv preprint arXiv:1804.00857.\nSainbayar Sukhbaatar, Edouard Grave, Piotr Bo-\njanowski, and Armand Joulin. 2019. Adaptive\nattention span in transformers. arXiv preprint\narXiv:1905.07799.\nSi Sun, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu,\nand Jie Bao. 2020. Joint keyphrase chunking\nand salience ranking with bert. arXiv preprint\narXiv:2004.13639.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\n278\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nJohannes Welbl, Pontus Stenetorp, and Sebastian\nRiedel. 2018. Constructing datasets for multi-hop\nreading comprehension across documents. Transac-\ntions of the Association for Computational Linguis-\ntics, 6:287–302.\nLee Xiong, Chuan Hu, Chenyan Xiong, Daniel Cam-\npos, and Arnold Overwijk. 2019. Open domain\nweb keyphrase extraction beyond language model-\ning. arXiv preprint arXiv:1911.02671.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5754–5764.\nZhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Ben-\ngio, William W Cohen, Ruslan Salakhutdinov, and\nChristopher D Manning. 2018. Hotpotqa: A dataset\nfor diverse, explainable multi-hop question answer-\ning. arXiv preprint arXiv:1809.09600.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He,\nAlex Smola, and Eduard Hovy. 2016. Hierarchi-\ncal attention networks for document classiﬁcation.\nIn Proceedings of the 2016 conference of the North\nAmerican chapter of the association for computa-\ntional linguistics: human language technologies ,\npages 1480–1489.\nZihao Ye, Qipeng Guo, Quan Gan, Xipeng Qiu, and\nZheng Zhang. 2019. Bp-transformer: Modelling\nlong-range context via binary partitioning. arXiv\npreprint arXiv:1911.04070.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan Song,\nJames Demmel, Kurt Keutzer, and Cho-Jui Hsieh.\n2019. Large batch optimization for deep learning:\nTraining bert in 76 minutes. In International Con-\nference on Learning Representations.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Onta ˜n´on,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.\nXingxing Zhang, Furu Wei, and Ming Zhou. 2019. Hi-\nbert: Document level pre-training of hierarchical\nbidirectional transformers for document summariza-\ntion. arXiv preprint arXiv:1905.06566.\n279\nAppendix A: Implementation Details\nGlobal-Local Attention Implementation\nThis appendix provides further details on the\nTPU/GPU-friendly implementation of global-local\nattention. Our implementation of sliding win-\ndow local attention is similar to the approach\nin the local attention 1d layer in Ten-\nsor2Tensor 8, but with the addition of ﬂexible mask-\ning, relative position encoding, and global tokens\nas side keys/values. We use a simple example\nto describe the internal blocking logic. Let’s say\nthe input corresponds to embeddings for the fol-\nlowing word pieces, each represented by a letter:\nABCDEFG.\nAs usual, we project these embeddings into\nqueries, keys, and values, yielding the following\n(for each attention head):\nQueries: AqBqCqDqEqFqGq\nKeys: AkBkCkDkEkFkGk\nValues: AvBvCvDvEvFvGv\nLet’s say we want to perform sliding window\nlocal attention with local radius r= 2. Internally,\nwe split the input into blocks of length r+ 1(3 in\nour example) and add padding blocks to the left and\nright, resulting in the following ﬁve blocks for the\nqueries (and similarly for keys and values), with 0\nrepresenting padding:\n000 AqBqCq DqEqFq Gq00 000\nConceptually we’d like to compare each query\nwith the 2r+ 1(5 in our example) surrounding\nkeys, as follows:\nQueries Keys\nAq 00AkBkCk\nBq 0AkBkCkDk\nCq AkBkCkDkEk\nDq BkCkDkEkFk\nEq CkDkEkFkGk\nFq DkEkFkGk0\nGq EkFkGk00\nBut materializing each window of keys would\nbe memory-intensive. Instead, we allow each block\nof queries to attend to 3 blocks of keys (the same\nblock, and the blocks immediately to the left and\nright), resulting in the following:\nQueries Keys\nAqBqCq 000 AkBkCk DkEkFk\nDqEqFq AkBkCk DkEkFk Gk00\nGq00 DkEkFk Gk00 000\nNow each query can potentially see a few more\ntokens than it’s strictly allowed to by the local ra-\n8https://arxiv.org/abs/1803.07416\ndius r. For example, Aq takes a dot product with\nDk, Ek and Fk, but this is easy to simply mask out,\nyielding the same sliding window local attention\nresult. In this way, the blocking mechanism saves\nmemory at the expense of some extra compute.\nThe values are also divided into the same blocks\nas the keys (concatenating 3 at a time), and stan-\ndard scaled dot product attention is applied inde-\npendently for each row in the table below, where\nKeys have been truncated for brevity:\nQueries Keys Values\nAqBqCq ... 000 AvBvCv DvEvFv\nDqEqFq ... AvBvCv DvEvFv Gv00\nGq00 ... DvEvFv Gv00 000\nEfﬁcient Relative Attention Implementation\nTo efﬁciently implement relative position encod-\ning (a.k.a. relative attention), we take an ap-\nproach similar to the optimization in Music Trans-\nformer (Huang et al., 2018) but generalized to al-\nlow arbitrary pairwise labels rather than adhering\nto a relative position pattern. We brieﬂy describe\nour implementation in the case of full attention\n(with a single sequence length n), but the approach\nnaturally extends to the case of the four attention\npieces used in ETC. The original relative attention\nwork (Shaw et al., 2018) reported O(n2dz) mem-\nory overhead (Section 3.3) by materializing aK\nij for\nevery query-key pair while sharing aK\nij across all\nheads (or O(hn2dz) if not sharing across heads),\nwhere dz is the dimension per head and his the\nnumber of heads. We instead take a dot product\nbetween each query vector and all unique aK\nij vec-\ntors in the relative attention vocabulary. Then we\ngather these scalar results for each query-key pair.\nThis avoids the O(n2dz) memory overhead and al-\nlows us to use different aK\nij per attention head with\nno additional activation memory cost. Note that\nour relative attention vocabulary sizes are notice-\nably smaller than n, so our implementation reduces\nthe number of dot products required for relative\nattention also.\nAppendix B: CPC Loss in ETC\nWe adapted the original formulation of CPC for\nETC by modeling it as a dual encoder problem.\nWe have two instances of the same ETC modelg1\nand g2 (using the same weights). g1 is the main\nencoder we are training, and we divide its long in-\nput into segments (e.g., sentences) and have one\nglobal token in global input for each segment. We\n280\nmask some segments in the long input, and encode\nthose segments independently using g2 (by having\nas input just the tokens of that segment in the long\ninput, and a single global token in the global in-\nput). Then, we train g1 and g2 so that the encodings\nof the global tokens corresponding to the masked\nsegments should be as similar as possible as the\nencoding of the global segment token obtained via\ng2. We use within-batch random negatives for this\nprocess as well, and use a Noise Contrastive Es-\ntimation (NCE) loss, in the same way as in the\noriginal CPC work (Oord et al., 2018).\nAppendix C: Training Details\nOur default pre-training procedure used the same\nWikipedia and Books training corpora as BERT, but\nwe ﬁltered to remove those documents with fewer\nthan 7 sentences. Models were pre-trained for 33\nepochs to match the amount of pre-training of the\noriginal BERT model, which used batches of 256\nsequences of 512 tokens each, and pre-trained for\n1,000,000 iterations. Instead, we used batches of\n512 sequences of 4096 tokens each and pre-trained\nfor 63,000 iterations. The ETC-large models were\npre-trained for 66 epochs by using a batch size of\n1024 instead. When lifting weights from RoBERTa,\nwe found that decreasing the learning rate to 2 ×\n10−3 improved model quality.\nWhen pre-training models, we split any in-\nput documents that are longer than the long in-\nput length. For efﬁciency, we also concatenate\nas many shorter documents as will ﬁt into the\n512/4096/8192 window and mask attention to pre-\nvent them from inﬂuencing each other. This results\nin a roughly 3x speedup in pre-training time for\n4096-token models, highlighting once more the\nadvantage of ﬂexible masking.\nWhen pre-training with CPC, we randomly se-\nlect 10% of sentences to be masked for the CPC\ntask. Subsequently, 15% of the remaining tokens\nare masked for MLM.\nIn the models where we use both MLM and CPC,\nwe used a 0.8 weight for MLM and a 0.2 weight\nfor CPC to combine them into a single loss.\nNQ\nData Download Link: https://ai.google.\ncom/research/NaturalQuestions/download\nData Pre-processing: Following Alberti’s\nBERT implementation (Alberti et al., 2019), long\ninput in NQ contains a CLS token followed by the\nquestion word pieces, then a separator followed by\nthe long document, a ﬁnal separator, and padding.\nGlobal input contains a CLS token, one special\n“question” token per token in the question, and then\none special “segment” token per paragraph (long\nanswer candidate) in the long input. Moreover,\nsince the ground truth indexes in this dataset are\nword indexes, in order to be able to align tokens\nwith words, sentences are ﬁrst tokenized by words,\nand then each word is given to the BERT/RoBERTa\ntokenizer.\nFine-Tuning: After pre-training, all models\nwere ﬁne-tuned with a hyperparameter sweep con-\nsisting of learning rates in {3 ×10−5,5 ×10−5}\nand number of epochs in {3,5}({2,3}for large\nmodels) with a batch size of 64 on the NQ train-\ning set using the Adam optimizer. The model is\ntrained to predict four logits coming out of the long\ninput tokens: long answer start, long answer end,\nshort answer start, and short answer end. A ﬁnal\nprediction (predicted from the long input CLS to-\nken embedding) is the answer type (null, yes, no,\nshort, long). For NQ instances that are longer than\nlong input size, a sliding window approach is used\n(with stride 128 for input lengths of 512, 2048 for\ninput lengths of 4096, and 4096 for input lengths\nof 8192).\nModel Selection: we performed a single hyper-\nparameter sweep for each model (i.e., we tested a\nsingle random seed per parameter conﬁguration).\nThe best model was selected as the highest average\nF1 score on dev at the end of ﬁne-tuning. Our best\nmodel (large, lifting from RoBERTa) was trained\nfor 2 epochs with learning rate 3 ×10−5.\nInference: Final predictions are then aggregated\nsimilarly as in the work of Alberti et al. (2019), but\nwith two improvements: First, instead of predicting\nstart and end of short/long answer separately, we\nﬁrst select the best start position, and then select\nthe best end location that occurs after the start po-\nsition. For short answer, we also ﬁlter out all end\npositions further than 38 words from the start posi-\ntion. Second, when the logit for a yes/no answer is\nhigher than the logits for short, long or null answer,\nwe replace the short answer with a corresponding\nyes/no.\nHotpotQA\nData Download Link: https://hotpotqa.\ngithub.io/\nData Pre-processing: Only 90,431 out of the\n281\n90,447 instances were used for training, as we\nmodel the task as extractive QA and thus ﬁltered\nout the 16 instances where the answer could not\nbe found in the contexts. Long input in HotpotQA\nis organized as follows: CLS token followed by\nquestion tokens followed by all the context tokens.\nEach context is represented as the concatenation\nof its title, and then all the sentences. Global input\nhas a CLS token, and then one token per question\ntoken (as in NQ), followed by global tokens repre-\nsenting the contexts. For every context, in global,\nwe have one token representing the whole context,\nand then one per sentence. We did not use any\nwindowing approach for longer instances, and just\nﬁt as many tokens as possible within the 4096 long\ninput. Global input length was set to 256.\nFine-Tuning: After pre-training, all base mod-\nels were ﬁne-tuned with a hyperparameter sweep\nconsisting of learning rates in {3 ×10−5,5 ×\n10−5}, number of epochs in {3,5,7,9}, batch\nsize in {32,64}, and supporting fact threshold\nin linspace(0,1,11) on the training set using the\nAdam optimizer. Large models were tested with\nlearning rate in {1×10−5,2×10−5,3×10−5,5×\n10−5,7×10−5}, number of epochs in{2, 5, 9, 13},\nand batch size in {32, 64, 128}.\nModel Selection: we performed a single hyper-\nparameter sweep to determine the best parameter\nconﬁguration for each model, and then we tried 3\ndifferent random seeds for the best conﬁguration.\nThe best model was selected as the one with the\nbest joint F1 score on dev. Our best model (large,\nlifting from RoBERTa) was trained for 5 epochs,\nwith a learning rate of 3 ×10−5, batch size of 32\nand supporting fact threshold of 0.4.\nInference: In order to make predictions, sup-\nporting facts are predicted using a single dense\nlayer taking the global input embeddings as input\nwith a threshold over the output logits. Output\ntype is predicted with a single dense layer from the\nglobal CLS token. Answer spans where predicted\nalso with dense layers, but using the long input\nembeddings as inputs, using the following crite-\nria: begin/end positions must be in sentences or\ntitles, begin/end must be in the same sentence/title,\nspans must belong to a supporting fact, begin must\nbe before end, and spans cannot exceed a maxi-\nmum answer length of 30 tokens. Within spans\nsatisfying those criteria, a single span with top\nbegin prob ∗end prob is selected.\nWikiHop\nData Download Link: https://qangaroo.cs.\nucl.ac.uk/\nData Pre-processing: Global and Long input\nwas set similarly as in HotpotQA, except that global\ninput was set to 430, and that instead of a CLS to-\nken in global, we have one token per candidate an-\nswer (WikiHop provides a list of candidate answers,\nand the model needs to select among them). We\nused a relative position label (the same used to link\nsentence summary tokens with its corresponding\ntokens) to link candidate answers to their mentions\nin the text, where mentions are determined only by\nstring matching. Also, as in HotpotQA, no sliding\nwindow was used, and instances were just cropped\nto a length of 4096. A MaxHeap was used to en-\nsure that in case a context is truncated, truncation\nhappens from the contexts with the larger number\nof sentences.\nFine-Tuning: After pre-training, all base mod-\nels were ﬁne-tuned with a hyperparameter sweep\nconsisting of learning rates in {1 ×10−5,2 ×\n10−5,3 ×10−5,4 ×10−5,5 ×10−5}, and num-\nber of epochs in {5,10,15}with a batch size of\n64 on the training set using the Adam optimizer.\nFor large models, we narrowed down the hyperpa-\nrameter sweep to learning rates in {2 ×10−5,3 ×\n10−5,4 ×10−5,5 ×10−5}, and number of epochs\nin {5,10}. For this dataset we also experimented\nwith the LAMB optimizer (in addition to Adam),\nwhich was used for our leaderboard submission.\nModel Selection: we performed a single hyper-\nparameter sweep to determine the best parameter\nconﬁguration for each model (i.e., single random\nseed per parameter conﬁguration). The best model\nwas selected as the one with the highest accuracy on\ndev. Our best model (large, lifting from RoBERTa)\nwas trained for 10 epochs, with a learning rate of\n5 ×10−5. Finally, for the ﬁnal leaderboard submis-\nsion, we selected the checkpoint of the model that\nhad the highest dev set accuracy.\nInference: For ﬁnal prediction, we used a dense\nlayer from the global input embeddings, after that,\nthe candidate with the highest logit is selected as\nthe ﬁnal prediction.\nOpenKP\nData Download Link: https://microsoft.\ngithub.io/msmarco/\nData Pre-processing: Long input in OpenKP\ncontains all the word pieces of the input. One\n282\nglobal token per VDOM node was added to the\nglobal input (notice this is like the pre-training\nsetup, except instead of sentences we have VDOM\nnodes as the higher-level units). No sliding win-\ndowing was used, and we simply truncate instances\nto whichever of max tokens in long input or max\nVDOM tokens in global ends up being more con-\nstraining. Long input length was set to 4096 and\nglobal input length to 512 by default in this dataset.\nAfter url deduplication and skipping examples with-\nout keyphrases in the truncated document, there\nwere 133,374 valid training examples. Regarding\nvisual features, we embed font sizes based on 24\nbucket ranges, and we also construct an embedding\nfor the cross of “block”, “heading”, and “bolded”\nBoolean features in the input data. All other visual\nfeatures were treated as dense features, with the\nﬂoating point features clipped to reasonable ranges\nand re-scaled to the [−1,1] interval. These dense\nfeatures are then transformed to the same embed-\nding space as the other embeddings, and all visual\nfeature embeddings are added to both the relevant\nlong and global input tokens.\nFine-Tuning: After pre-training, all models\nwere ﬁne-tuned with a hyperparameter sweep con-\nsisting of learning rates in {3 ×10−5,5 ×10−5}\nand number of epochs in {2,3}with a batch size\nof 64 on the OpenKP training set using the Adam\noptimizer. To generate predictions, we ﬁrst sum the\nembeddings (from the long input) of all the word\npieces for each word to form word embeddings.\nThen we run convolutions with kernel size 1, 2,\n3, 4, and 5 to form the respective n-gram embed-\ndings. Finally, a dense linear layer is used to form\nlogits for all the n-grams and concatenate them\ntogether for one combined softmax. The loss is\ncross entropy where the ground truth probabilities\nare divided equally among the keyphrase labels (up\nto 3). By default we used the ﬁrst occurrence of\neach keyphrase as the label. Our improved “max\nloss” takes the max of logits across all occurrences\nof the same keyphrase in the text, rather than just\nthe ﬁrst occurrence.\nModel Selection: we performed a single hyper-\nparameter sweep to determine the best parameter\nconﬁguration for each model (i.e., single random\nseed per parameter conﬁguration). The best model\nwas selected as the one with the highest F1@3\non dev. Our best model (large, max loss, lifting\nfrom RoBERTa) was trained for 2 epochs, with a\nlearning rate of 3 ×10−5.\nModel Time Hardware\nETC (share) 11h 13m 256 core TPU v3\nETC 11h 46m 256 core TPU v3\nETC-large 63h 41m 512 core TPU v3\nTable 6: Time taken for pre-training the different model\ntypes used in our experiments, together with the hard-\nware conﬁguration used (2 cores = 1 chip). This cor-\nresponds to 63k pre-training iterations, with batch size\n512 for base models (33 epochs), and 1024 for large\nmodels (66 epochs).\nDataset Epochs Time Hardware\nNQ 5 10h 47m 32 core TPU v3\nHotpotQA 9 2h 59m 32 core TPU v3\nWikiHop 15 5h 55m 32 core TPU v3\nOpenKP 3 2h 5m 32 core TPU v3\nTable 7: Time taken for ﬁne-tuning the baseline ETC\n(base) model on different datasets, together with the\nhardware conﬁguration used (2 cores = 1 chip). As\nwe did a hyper-parameter sweep with different number\nof epochs, we report the time of the largest number of\nepochs we tried.\nInference: During inference, we select the top\n5 keyphrases ordered by logits, removing any du-\nplicates. All keyphrases were treated as uncased\nfor the purpose of deduplication.\nAppendix D: Lifting Weights from\nBERT/RoBERTa\nWhen lifting weights from BERT or RoBERTa, the\nweights that can be lifted are (for every Transformer\nlayer): feed forward layer, WQ, WK, WV (since\nBERT/RoBERTa only have one copy of such matri-\nces, in models where we use different matrices for\nglobal and long inputs, we initialize both sets of\nmatrices to the same BERT/RoBERTa weights), at-\ntention output projection, and layer normalization.\nAdditionally, we can also lift the token embedding\nmatrix. Absolute position embeddings and next\nsentence prediction weights from BERT are dis-\ncarded. After that, weights for the layers necessary\nfor the CPC loss, and those involved in relative\nposition encodings are randomly initialized.\nFor lifting to be possible, the number of lay-\ners, hidden size, number of attention heads, and\nsize of the feed forward intermediate layers of the\nBERT/RoBERTa model need to match with the\nETC model.\n283\nSequence Length\nWall time per step (seconds)\n0.0\n0.5\n1.0\n1.5\n2.0\n2000 4000 6000 8000\nBERT base ETC base\nFigure 4: Wall time per step for different input lengths\nfor both BERT and ETC with their base conﬁgurations.\nFor ETC, global input length was set to 1/16th of the\nlong input length until reaching a ceiling of 512 global\nlength at 8192 long length, and Sequence Length is the\nsum of long and global lengths.\nS3C2S2S1C1\nSentence 1 Sentence 2 Sentence 3\nC1 C2\nS2S1 S3\nSentence 1 Sentence 2 Sentence 3\n.........\n...\n...\nLong Input\nGlobal Input\n1) Collection of unordered \ncontexts\n2) Contexts are collections \nof ordered sentences\n3) Sentences are collections \nof ordered tokens\nPadding\nPadding\nFigure 5: Illustration of structure encoding with ETC\n(using the same example shown in Figure 3b). Top:\neach box represents an input token, and arrows repre-\nsent attention. The different colors and dash patterns\nin arrows represent the different relative position labels.\nBottom: illustration of where each token would appear\nin the input of ETC.\nAppendix E: Model Computational\nRequirements\nMemory: To gauge headroom for scaling input\nlengths beyond what we used in this paper, we ran\nsome additional experiments on TPU v3 hardware\nwith gradient checkpointing and removing the ex-\ntra gradient moments required by optimizers like\nAdam and LAMB. Fixing global input length to\n512 tokens, we were able to push base models to\nlong input size of 22656, and large models to long\ninput size of 8448 before running out of memory\non a single TPU v3 core. We leave for future work\nexperimentation with more memory-efﬁcient op-\ntimizers like Adafactor (Shazeer and Stern, 2018)\nand model-parallelism techniques in ETC.\nCompute: As stated above, the computational\ncomplexity of attention in ETC is O(ng(ng + nl) +\nModel Parameters\nETC base (shared) 109M\nETC base 166M\nETC large (RoBERTa vocab) 558M\nBERT base 110M\nBERT large (RoBERTa vocab) 355M\nTable 8: Number of trainable parameters for the differ-\nent models evaluated in this paper.\nnl(ng +2r+1)) and if we assumeng = O(2r+1),\nthis results in a complexity ofO(n2\ng +ngnl), which\nis linear in the size of the long input. Table 6 shows\npre-training times in our experiments, together with\nthe hardware used in each experiment. Table 7\nshows the ﬁne-tuning times taken by the baseline\nETC model on the different datasets. Notice that\npre-training is the most computational intensive\npart, and thus, we used signiﬁcantly more hardware.\nIn order to gain further insights into the common\nuse case of running the models using GPUs, Fig-\nure 4 shows a comparison of the wall-time used\nper step when using a single NVIDIA Tesla V100\nGPU as the input length increases, for both BERT\nand ETC in their base conﬁgurations. As the plot\nshows ETC is initially slower, but it becomes faster\nthan BERT for input lengths larger than about 1500.\nMoreover, the BERT plot ends earlier due to mem-\nory constraints. Finally, notice that the ETC wall\ntime is not linear in this ﬁgure, as we also increased\nthe size of the global input together with the long\ninput.\nParameters: Finally, Table 8 shows the total\nnumber of parameters of the ETC model for the\ndifferent conﬁgurations used in our experiments.\nThe most important consideration is that the num-\nber of trainable parameters does not depend on the\ninput length. As a matter of fact, it only depends\non: the embedding dimensionality (d), the number\nof layers (l), and the number of relative position\nlabels (which depends on k). The parameter count\nalso depends on the fully connected feed forward\nintermediate size, but this is 4dby convention and\nfor all ETC models in this paper. Our baseline\nmodel uses separate WQ, WK, WV , and output\nprojection matrices for global and long inputs, re-\nsulting in about 50% more parameters than BERT.\nBut the conﬁguration with shared WQ, WK, WV ,\nand output projection has a similar number of pa-\nrameters as BERT. Parameter count for BERT base\nis reported from the original paper (Devlin et al.,\n284\n2018) and BERT large (RoBERTa vocab) from the\noriginal RoBERTa paper (Liu et al., 2019).\nAppendix F: Structured Input Example\nFigure 3b shows an illustration of a possible atten-\ntion pattern for a dataset like WikiHop, where the\ninput consists of contexts, made out of sentences.\nThere is no order among the contexts, but there is\namong the sentences within a context. Figure 5\nillustrates how this can be encoded in ETC, putting\nall the word piece tokens in the long input, and\nusing the global input for special “context” and\n“sentence” tokens. Different relative position labels\nare used to indicate the different relations (token\npart of a sentence, sentence part of a context, order\nbetween sentences, order between tokens, etc.).",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.5265929102897644
    },
    {
      "name": "Computer science",
      "score": 0.5208173990249634
    },
    {
      "name": "Encoding (memory)",
      "score": 0.48706915974617004
    },
    {
      "name": "Natural language processing",
      "score": 0.35872769355773926
    },
    {
      "name": "Art history",
      "score": 0.34302079677581787
    },
    {
      "name": "Artificial intelligence",
      "score": 0.324601411819458
    },
    {
      "name": "History",
      "score": 0.3177212178707123
    },
    {
      "name": "Engineering",
      "score": 0.1793684959411621
    },
    {
      "name": "Electrical engineering",
      "score": 0.09839239716529846
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    }
  ],
  "cited_by": 253
}