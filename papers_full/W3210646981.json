{
  "title": "SOFT: Softmax-free Transformer with Linear Complexity",
  "url": "https://openalex.org/W3210646981",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4281858129",
      "name": "Lu, Jiachen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2941482352",
      "name": "Yao JingHan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2312928816",
      "name": "Zhang Jun-ge",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176328991",
      "name": "Zhu, Xiatian",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1975116581",
      "name": "Xu Hang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2350224369",
      "name": "Gao Wei-guo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3087778642",
      "name": "XU Chunjing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100117188",
      "name": "Xiang, Tao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2051190893",
      "name": "Zhang, Li",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2994673210",
    "https://openalex.org/W3122239467",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3139537596",
    "https://openalex.org/W1836465849",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3129603602",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3146097248",
    "https://openalex.org/W3123615524",
    "https://openalex.org/W3211432419",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3139049060",
    "https://openalex.org/W3122818000",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W3172509117",
    "https://openalex.org/W2178628967",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W3204563069",
    "https://openalex.org/W3121592593",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3137278571",
    "https://openalex.org/W2765407302",
    "https://openalex.org/W3034345703",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W2112545207",
    "https://openalex.org/W3125056032",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W3173365702",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2963393721",
    "https://openalex.org/W3134144764",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W3034573343",
    "https://openalex.org/W2275150912",
    "https://openalex.org/W3119786062",
    "https://openalex.org/W2048134878"
  ],
  "abstract": "Vision transformers (ViTs) have pushed the state-of-the-art for various visual recognition tasks by patch-wise image tokenization followed by self-attention. However, the employment of self-attention modules results in a quadratic complexity in both computation and memory usage. Various attempts on approximating the self-attention computation with linear complexity have been made in Natural Language Processing. However, an in-depth analysis in this work shows that they are either theoretically flawed or empirically ineffective for visual recognition. We further identify that their limitations are rooted in keeping the softmax self-attention during approximations. Specifically, conventional self-attention is computed by normalizing the scaled dot-product between token feature vectors. Keeping this softmax operation challenges any subsequent linearization efforts. Based on this insight, for the first time, a softmax-free transformer or SOFT is proposed. To remove softmax in self-attention, Gaussian kernel function is used to replace the dot-product similarity without further normalization. This enables a full self-attention matrix to be approximated via a low-rank matrix decomposition. The robustness of the approximation is achieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive experiments on ImageNet show that our SOFT significantly improves the computational efficiency of existing ViT variants. Crucially, with a linear complexity, much longer token sequences are permitted in SOFT, resulting in superior trade-off between accuracy and complexity.",
  "full_text": "SOFT: Softmax-free Transformer with Linear\nComplexity\nJiachen Lu1 Jinghan Yao1 Junge Zhang1 Xiatian Zhu2 Hang Xu3\nWeiguo Gao1 Chunjing Xu3 Tao Xiang2 Li Zhang1∗\n1Fudan University 2University of Surrey 3Huawei Noah’s Ark Lab\nhttps://fudan-zvg.github.io/SOFT\nAbstract\nVision transformers (ViTs) have pushed the state-of-the-art for various visual recog-\nnition tasks by patch-wise image tokenization followed by self-attention. However,\nthe employment of self-attention modules results in a quadratic complexity in\nboth computation and memory usage. Various attempts on approximating the self-\nattention computation with linear complexity have been made in Natural Language\nProcessing. However, an in-depth analysis in this work shows that they are either\ntheoretically ﬂawed or empirically ineffective for visual recognition. We further\nidentify that their limitations are rooted in keeping thesoftmax self-attention during\napproximations. Speciﬁcally, conventional self-attention is computed by normaliz-\ning the scaled dot-product between token feature vectors. Keeping this softmax\noperation challenges any subsequent linearization efforts. Based on this insight,\nfor the ﬁrst time, a softmax-free transformer or SOFT is proposed. To remove soft-\nmax in self-attention, Gaussian kernel function is used to replace the dot-product\nsimilarity without further normalization. This enables a full self-attention ma-\ntrix to be approximated via a low-rank matrix decomposition. The robustness\nof the approximation is achieved by calculating its Moore-Penrose inverse using\na Newton-Raphson method. Extensive experiments on ImageNet show that our\nSOFT signiﬁcantly improves the computational efﬁciency of existing ViT variants.\nCrucially, with a linear complexity, much longer token sequences are permitted in\nSOFT, resulting in superior trade-off between accuracy and complexity.\n1 Introduction\nRecently the step change brought by Transformers [34] in natural language processing (NLP) [10, 4]\nseems to have arrived in vision [11, 42, 48, 47]. Indeed, with less inductive bias in its architecture\ndesign than Convolution neural networks (CNNs), pure Vision Transformer (ViT) [11] and its variants\nhave shown to be able to outperform CNNs on various vision tasks [ 8, 16]. However, there is a\nbottleneck in any Transformer based model, namely its quadratic complexity in both computation\nand memory usage. This is intrinsic to the self-attention mechanism: given a sequence of tokens\n(e.g., words or image patches) as input, the self-attention module iteratively learns the feature\nrepresentations by relating one token to all other tokens. This results in a quadratic complexity O(n2)\nwith the token sequence length nin both computation (time) and memory (space) since an n×n\nsized attention matrix needs to be computed and saved during inference. This problem is particularly\nacute in vision: a 2D image after tokenization will produce a far longer sequence than those in NLP\neven with a moderate spatial resolution. This quadratic complexity thus prevents a ViT model from\nmodeling images at high spatial resolutions, which are often crucial for visual recognition tasks.\n∗Li Zhang (lizhangfd@fudan.edu.cn) is the corresponding author with School of Data Science, Fudan\nUniversity.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021).\narXiv:2110.11945v3  [cs.CV]  30 Apr 2022\n10 20 30 40 50 60 70 80 90\nParameter (M)\n70\n72\n74\n76\n78\n80\n82\n84Imagenet Top-1 Accuracy (%)\nPVT-Tiny\nPVT-Small\nPVT-Medium\nPVT-Large\nResNet-18\nResNet-50\nResNet-101\nT2T-ViT-19\nT2T-ViT-24\nDeiT-B\nViT-B\nRegNetY-4GF\nRegNetY-8GF RegNetY-16GF\nTwins-SVT-S\nSAN10\nSOFT-Tiny\nSOFT-Small\nSOFT-Medium SOFT-Large SOFT-Huge\n(a)\n784 × 1 784 × 2 784 × 3 784 × 4 784 × 5 784 × 6 784 × 7 784 × 8\nToken sequence length\n2000\n4000\n6000\n8000\n10000\n12000\n14000Memory (MB)\nTransformer\nLinformer\nNystromformer\nPerformer\nSOFT (b)\nFigure 1: Top1-Accuracy on ImageNet [9] validation set with respect to parameters and the memory\nusage corresponding to the token sequence length in practice compared to other methods. (a) Com-\nparison with CNN models: RegNet [27], ResNet [14] and Transformer models: PVT [36], DeiT [32],\nViT [11], T2T-ViT [42], Twins-SVT [6] and SAN10 [46]; (b) Comparison with Transformer [34],\nLinformer [35], Nyströformer [40] and Performer [5]. The memory usage is measured with a batch\nsize of 1 on a 16GB Tesla V100.\nA natural solution is to reduce the complexity of self-attention computation via approximation.\nIndeed, there have been a number of attempts in NLP [ 35, 5, 19, 40]. For example, [ 35] takes a\nnaive approach by shortening the length of Key and Value via learnable projections. Such a coarse\napproximation would inevitably cause performance degradation. In contrast, [5, 18] both leverage the\nkernel mechanism to approximate softmax normalization to linearize the computation in self-attention.\n[19] instead adopts a hashing strategy to selectively compute the most similar pairs. Recently, [40]\nuses Nyström matrix decomposition to reconstruct the full attention matrix with polynomial iteration\nfor approximating the pseudo-inverse of the landmark matrix. Nonetheless, softmax normalization\nis simply duplicated across the matrix decomposition process, which is theoretically unsound. We\nempirically found that none of these methods are effective when applied to vision (see Sec. 4.2).\nIn this work, we identify that the limitations of existing efﬁcient Transformers are caused by the use\nof softmax self-attention, and for the ﬁrst time propose a softmax-free Transformer. More speciﬁcally,\nin all existing Transformers (with or without linearization), a softmax normalization is needed on top\nof scaled dot-product between token feature vectors [34]. Keeping this softmax operation challenges\nany subsequent linearization efforts. To overcome this obstacle, we introduce a novelsoftmax-free\nself-attention mechanism, named as SOFT, with linear complexity O(n) in both space and time.\nSpeciﬁcally, SOFT uses Gaussian kernel to deﬁne the similarity (self-attention) function without\nthe need for subsequent softmax normalization. With this softmax-free attention matrix, we further\nintroduce a novel low-rank matrix decomposition algorithm for approximation. The robustness of\nthe approximation is theoretically guaranteed by employing a Newton-Raphson method for reliably\ncomputing the Moore-Penrose inverse of the matrix.\nWe make the followingcontributions. (I) We introduce a novelsoftmax-free Transformerwith linear\nspace and time complexity. (II) Our attention matrix approximation is achieved through a novel\nmatrix decomposition algorithm with theoretical guarantee. (III) To evaluate our method for visual\nrecognition tasks, we design a family of generic backbone architectures with varying capacities using\nSOFT as the core self-attention component. Extensive experiments show that with a linear complexity\n(Figure 1b), our SOFT models can take in as input much longer image token sequences. As a result,\nwith the same model size, our SOFT outperforms the state-of-the-art CNNs and ViT variants on\nImageNet [9] classiﬁcation in the accuracy/complexity trade-off (Figure 1a).\n2 Related work\nVision Transformers There is a surge of research interests recently in exploiting Transformers for\nvisual recognition tasks [37, 36, 42, 32, 45], inspired by their remarkable success in NLP [34, 10, 4].\n2\nCore to these NLP and vision transformers is the same self-attention mechanism [34] that computes\na self-attention matrix by exhaustively comparing token pairs. This means a quadratic complexity\nwith the sequence length in both space and time, which thus limits the scalability of Transformers\nin dealing with long sequences. This limitation is more serious in vision than NLP: To process an\nimage with at least thousands of pixels, patch-wise tokenization is a must for Transformers to control\nthe computational cost. Given higher resolution images, the patch size also needs to be enlarged\nproportionally sacriﬁcing the spatial resolution. This limits the capability of Transformers, e.g.,\nlearning ﬁne-grained feature representation as required in many visual recognition tasks.\nLinear Transformers Recently, there have been a number of linear/efﬁcient variants [5, 35, 18, 19,\n31, 25, 17] of Transformers in NLP. For example, [35] learns to shrink the length of Key and Value\nbased on a low-rank assumption. [19] adopts a hashing strategy to selective the most similar pairs\nand only compute attention among them. [5, 18] utilize different kernel functions for approximating\nsoftmax-based self-attention matrix. [ 25] applies random feature mapping on the sequences to\napproach the original softmax function. [ 17] decreases the time and memory consumption of the\nattention matrix by replacing the softmax function with its linear-complexity recurrent alternative.\nWhen applied to visual recognition tasks, however, we show that these models have considerable\nperformance degradation compared to the standard Transformers [34] (see Sec. 4.2).\nThe most related work to SOFT is [ 40] which uses the Nyström matrix decomposition to avoid\ncomputing the full attention matrix. However, this method suffers from several theoretical defects:\n(1) As the standard self-attention needs to apply row-wise softmax normalization on the full attention\nmatrix, a direct application of matrix decomposition is infeasible. As a workaround, softmax is simply\napplied to all the ingredient matrices in [40]. Such an approximation is not guaranteed theoretically.\n(2) With a polynomial iteration method, it is not guaranteed that the generalized attention matrix\ninverse can be computed when the matrix is a nearly singular one in practice. In contrast to all the\nabove methods, in this paper we propose a softmax-free self-attention mechanism that facilitates\nmatrix decomposition for complexity minimization with theoretical guarantees.\n3 Method\n3.1 Softmax-free self-attention formulation\nA schematic illustration of our model is given in Figure 2. Let’s ﬁrst look at our attention module\ndesign. Given a sequence of ntokens X ∈Rn×d with each token represented by a d-dimensional\nfeature vector, self-attention [34] aims to discover the correlations of all token pairs exhaustively.\nFormally, X is ﬁrst linearly projected into three de-dimensional spaces (query, key, and values)\nas:\nQ= XWq ∈Rn×de , K = XWk ∈Rn×de , V = XWv ∈Rn×de , (1)\nwhere Wq,Wk,Wv ∈Rd×de are learnable matrices. Self-attention can be expressed in a generic\nformulation as:\nyi,: =\nn∑\nj=1\nα(Qi,:,Kj,:) ⊙Vj,:, (2)\nwhere ⊙is the Hadamard product, and i,j ∈{1,··· ,n}index the tokens. The key self-attention\nfunction α : Rde ×Rde →R is composed of a nonlinear function β : R →R and a relation\nfunction γ : Rde ×Rde →R. A dominant instantiation of αis the scaled dot-product based softmax\nself-attention [34], deﬁned as\nβ(·) = softmax(·), γ (Qi,:,Kj,:) = 1√de\n·Q⊤\ni,:Kj,:. (3)\nWhilst this softmax self-attention has been the de facto choice and seldomly questioned, as discussed\nearlier it is not necessarily suited for linearization. To facilitate the design of linear self-attention, we\nintroduce a softmax-free self-attention function with the dot-product replaced by a Gaussian kernel\nas:\nβ′(·) = exp(·), γ ′(Qi,:,Kj,:) = − 1\n2√de\n·∥Qi,: −Kj,:∥2\n2. (4)\n3\nFigure 2: Schematic illustration of the proposed softmax-free self-attention (SOFT) method. P.E.:\nPosition embedding. Dash lines: linear projection. dh: the hidden dim of each attention head. ◦\ndenotes the matrix dot product.\nTo preserve the symmetric property of attention matrix as in Eq (3), we set the project matrices Wq\nand Wk in Eq (1) identical (i.e., Q= K). Our self-attention matrix is then written as:\nSi,j = exp\n(\n− 1\n2√de\n·∥Qi,: −Kj,:∥2\n2\n)\n. (5)\nFor notation simplicity, we deﬁne the matrix formulation as: S = exp (Q⊖K).\nRemarks Our self-attention matrix Shas three important properties: (1) It is symmetric; (2) All\nthe elements lie in a unit range of [0,1]; (3) All diagonal elements hold the largest value 1 (self-\nreinforced), with the bottom ones (corresponding to most dissimilar token pairs) being close to 0.\nAs Gaussian kernel is a positive deﬁnite kernel [ 12], S is deemed a Gram matrix. However, we\nﬁnd that when using our kernel-based self-attention matrix Swithout linearization, the training of a\ntransformer fails to converge. This might explain why softmax dot-product based self-attention [34]\nis so popular in vanilla transformers.\n3.2 Low-rank regularization via matrix decomposition with linear complexity\nTo solve the convergence and quadratic complexity problems, we leverage matrix decomposition as\na uniﬁed solution with low-rank regularization. In particular, we consider Nyström [ 39], which is\noriginally a low-rank matrix approximation algorithm. This enables our model’s complexity to be\nreduced signiﬁcantly without computing the full self-attention matrix S.\nWe make this choice because our Sis positive semi-deﬁnite (i.e., a Gram matrix) without follow-up\nnormalization which are all necessary conditions for Nyström. In contrast, [40] totally ignores these\nrequirements, leading to theoretical ﬂaw in its approximation.\nTo deﬁne the Nyström method formally, let us express S = exp (Q⊖K) as a block matrix:\nS =\n[ A B\nB⊤ C\n]\n∈Rn×n, (6)\nwhere A ∈ Rm×m, B ∈ Rm×(n−m), C ∈ R(n−m)×(n−m) with m ≪ n. Through Nyström\ndecomposition (see derivative details in Appendix A.1), an approximation can be represented as:\nˆS =\n[A\nB⊤\n]\nA†[A B ] = P⊤A†P, where P = [A B ] , (7)\nand A†is the Moore-Penrose (a generalized) inverse of A.\nSampling In the standard Nyström formulation, A and B are sub-matrices of S obtained by\nrandomly sampled mtokens, denoted as ˜Q. We call the sampled ˜Qas bottleneck tokens. However,\n4\nAlgorithm 1:SOFT: Softmax-free attention\nInput: Q∈Rn×de , sampling function fs\nSampling ˜Q←fs(Q) ;\nA←exp( ˜Q⊖˜Q), P ←exp( ˜Q⊖Q);\nˆS ←P⊤NR(A)P;\nOutput: ˆS\nAlgorithm 2:NR: Newton-Raphson iteration\nInput: A∈Rm×m, and T ∈Z+\nα= 2/∥A∥2\n1.Initialize A0 ←αA;\nfor kfrom 1 to T do\nAk ←2Ak−1 −Ak−1AAk−1\nend\nOutput: AT\nwe ﬁnd empirically that random sampling is considerably sensitive to the choice of m. We hence\nexplore two additional options by leveraging the structural prior of visual data: (1) Using one\nconvolutional layer with kernel size kand stride kto learn ˜Q, and (2) Using average pooling with\nkernel size kand stride kto generate ˜Q. For both, we need to reshape Qto the form of RH×W×de .\nEach slide of convolution or pooling produces a token. We set kaccording to the length of Qsuch\nthat mtokens can be obtained. Our experiments show that a convolution layer performs better in\naccuracy. We therefore use a convolution layer by default.\nAs Kis identical to Q, we have ˜K = ˜Q. Given these mtokens, we then compute Aand P as:\nA= exp( ˜Q⊖˜K), P = exp( ˜Q⊖K). (8)\nWe ﬁnally obtain the regularized self-attention matrix ˆSof SOFT as:\nˆS = exp\n(\nQ⊖˜K\n)(\nexp\n(\n˜Q⊖˜K\n))†\nexp\n(\n˜Q⊖K\n)\n, (9)\nleading to Algorithm 1. The low-rank regularization is conducted as follows. For computing the\nattention score between any two tokens, we ﬁrst correlate each of them with sampled tokens using our\nself-attention function (Eq (5)); With this correlation representation we then compute their similarity\nunder the modulation of the generalized inverse of ˜Q’s correlation matrix. Similar as standard\nNyström, our design associates the input tokens w.r.t. a small space spanned by sampled tokens,\ngiving a proper estimation of the original attention relationships subject to a low-rank constraint. The\ncorrectness of this method is proved in Appendix A.1.\nMoore-Penrose inverse An accurate and commonly used way to calculate the Moore-Penrose\ninverse is to use Singular Value Decomposition (SVD). Given A ∈ Rm×m and its SVD form\nA = UΣV⊤ where U,V are m×m unitary matrices and Σ is a m×m diagonal matrix, the\nMoore-Penrose inverse of Ais A† = VΣ†U⊤. Nevertheless, SVD is not friendly to the training\nprocess on GPU hence harming the model training efﬁciency. To solve this issue, we adopt the\nNewton–Raphson method. It is an iterative algorithm with the (k+ 1)-th iteration formulated given\nthe previous iteration as:\nAk+1 = 2Ak −AkAAk, and A0 = αA. (10)\nWe now prove that Ak ﬁnally converges to Moore-Penrose inverse of Am×m, if αis sufﬁciently\nsmall [3].\nTheorem 1 When αis sufﬁciently small, Ak+1 = 2Ak −AkAAk, Ak converges to A†.\nThough α= 2/∥A∥2\n1 which ensures good convergence behavior in Algorithm 2 (see more details in\nAppendix A.2.1), in practice, we ﬁnd that using an alternative form gives more stable training and\nfaster convergence. Speciﬁcally, in ∥I−A 2βn\n∥A∥2\n1\n∥1 ≤1 where βequals to 0.5, we ﬁnd the smallest\nni that holds this inequality. Then, we initialize αas α= 2βni\n∥A∥2\n1\n.\nThe following proposition comes with the proof of Theorem 1:\nProposition 1 ∥AAkA−A∥and ∥Ak −A†∥decreases to 0 monotonously, if αis sufﬁciently small.\nThe detail of proposition 1 is shown in Appendix A.2.2. This ensures that our estimated inverse is\nsufﬁciently accurate for matrix decomposition, subject to that our SOFT attention is regularized.\n5\nMethods Complexity Memory Params FLOPs Throughput (img/s) Top-1 %\nTransformer [34] O(n2) 19.0GB† 13M 3.9G 1073 / 3240 79.1\nLinformer [35] O(n) 11.7GB 13M 1.9G 2767 / 3779 78.2\nPerformer [5] O(n) 15.0GB 13M 2.2G 2037 / 3657 76.1\nNyströmformer [40] O(n) 17.2GB 13M 2.0G 1891 / 3518 78.6\nSOFT O(n) 15.8GB 13M 1.9G 1730 / 3436 79.3\nTable 1: Comparison of different linear/efﬁcient transformer variants on ImageNet [9], based on our\nmulti-stage Tiny conﬁguration (see Table 2). The memory usage is measured with the batch size of\n1024 which is our standard training setting. Transformer is tested at a batch size of 256, which is\nthe maximal number possible with the GPU resource at our disposal. Throughput is in format as\nTrain throughput /inference throughput.\nComplexity We summarize the complexity of SOFT in space and time. For time complexity, it\ninvolves: (1) Sampling: O(nde). (2) Calculating three decomposed matrices: O(nmde + mnde +\nm2de) = O(2mnde + m2de); (3) Moore-Penrose inverse: O(T ×m3) = O(Tm3), where T is\nthe iteration steps. (4) All matrix multiplication: O(nm2 + mnde + mnde) = O(nm2 + 2mnde).\nThe total time complexity is O((de + 4mde + m2)n+ Tm3 + dem2). The space complexity is\ndecided by four decomposed matrices with O(n×m) + O(m×m) + O(m×n) + O(n×de) =\nO((2m+ de)n+ m2). As we keep m(m≪n) a ﬁxed constant in our model, both time and space\ncomplexity are O(n), making SOFT a linear self-attention.\n3.3 Instantiations\nFigure 2 shows how our proposedsoftmax-free self-attentionblock (SOFT block) can be implemented\nin a neural network. We replace the self-attention block with our SOFT block in the traditional\nTransformer, that is, we stack a SOFT block with a feed forward residual block [ 11] to form a\nsoftmax-free Transformer layer (SOFT layer).\nFocusing on the general image recognition tasks, we integrate our SOFT layer into the recent pyrami-\ndal Transformer architecture [36] to form our ﬁnal model SOFT. Further, several improvements are\nintroduced in patch embedding (i.e., tokenization). Speciﬁcally, unlike [36] that uses a combination\nof non-overlapping convolution and layer normalization [1], we adopt a stack of overlapping convolu-\ntions, batch normalization [15] and ReLU non-linearity. Concretely, the STEM is implemented by 3\nunits of 3x3 Conv→BN→ReLU, with the stride of 2, 1, 2 respectively. Then, one such unit is applied\nto each of three following down-sampling operations with stride of 2 in the multi-stage architecture.\nThe architecture hyper-parameters of SOFT are: d: the input channel dimension of SOFT layer. de:\nthe embedding dimension of tokens in SOFT block. In practice, we set de = d. h: the head number\nof SOFT block. dh: the channel dimension of each head and dh = de/h. n: the input token sequence\nlength of a SOFT block. m: the bottleneck token sequence length of SOFT block. sp: the sampling\nratio of token sequence length sampling, which is the ratio between input token sequence length and\nthe bottleneck token sequence length. e: the expansion ratio of the 2-layer feed forward block. In\nSOFT, for all the stages we set dh = 32, e= 4 and m= 49, spvaries in each stage according to\nthe input token sequence length. Table 2 details the family of our SOFT conﬁgurations with varying\ncapacities (depth and width).\n4 Experiments\n4.1 Setup\nDataset: We evaluate the proposed SOFT on the ILSVRC-2012 ImageNet-1K dataset [9] with 1.28M\ntraining images and 50K validation images from 1,000 classes. Following the common practice, we\ntrain a model on the training set and evaluate on the validation set. Metrics: For model performance,\nthe top-1 accuracy on a single crop is reported. To assess the cost-effectiveness, we also report the\nmodel size and ﬂoating point operations (i.e., FLOPs). Implementation details:We use the code\nbase [38] with the default setting to train and test all the models. Speciﬁcally, we use weight decay of\n0.05 and 10 epochs of linear warm-up. We conduct 300 epochs training with an AdamW optimizer\n6\nTiny Small Medium Large Huge\nStage 1\nC33-BN-ReLU, 64-d[\nsp. 8x8,\n64-d, 2-h\n]\nx 1\n[\nsp. 8x8,\n64-d, 2-h\n]\nx 1\n[\nsp. 8x8,\n64-d, 2-h\n]\nx 1\n[\nsp. 8x8,\n64-d, 2-h\n]\nx 1\n[\nsp. 8x8,\n64-d, 2-h\n]\nx 1\nStage 2\nC31-BN-ReLU, 128-d[\nsp. 4x4,\n128-d, 4-h\n]\nx 2\n[\nsp. 4x4,\n128-d, 4-h\n]\nx 3\n[\nsp. 4x4,\n128-d, 4-h\n]\nx 3\n[\nsp. 4x4,\n128-d, 4-h\n]\nx 3\n[\nsp. 4x4,\n128-d, 4-h\n]\nx 5\nStage 3\nC31-BN-ReLU, 320-d or 288-d[\nsp. 2x2,\n320-d, 10-h\n]\nx 3\n[\nsp. 2x2,\n320-d, 10-h\n]\nx 7\n[\nsp. 2x2,\n288-d, 9-h\n]\nx 29\n[\nsp. 2x2,\n320-d, 10-h\n]\nx 40\n[\nsp. 2x2,\n352-d, 11-h\n]\nx 49\nStage 4\nw. cls token\nC31-BN-ReLU, 512-d[\nsp. 1x1,\n512-d, 16-h\n]\nx 2\n[\nsp. 1x1,\n512-d, 16-h\n]\nx 4\n[\nsp. 1x1,\n512-d, 16-h\n]\nx 5\n[\nsp. 1x1,\n512-d, 16-h\n]\nx 5\n[\nsp. 1x1,\n512-d, 16-h\n]\nx 5\nTable 2: Architecture speciﬁcations of SOFT variants. sp.: sampling ratio. -d: the hidden dimension.\n-h: the number of heads in the self-attention block. C33-BN-ReLU: three 3x3 Conv-BN-ReLU, with\nthe stride of 2, 1, 2 respectively. C31-BN-ReLU: one 3x3 Conv-BN-ReLU, with a stride of 2.\nand decreasing learning rate with the cosine annealing schedule. During training, random ﬂipping,\nmixup [44] and cutmix [43] are adopted for data augmentation. Label smoothing [29] is used for loss\ncalculation. All our variants are trained with a batch size of 1024 on 32G NVIDIA V100 GPUs. We\nalso implement our method using the Mindspore [23].\n4.2 Comparison with existing linear Transformers\nWe compare our method with three existing linear Transformer models: Linformer [35], Performer [5],\nNyströmformer [40] in terms of model complexity and accuracy.\nTwo experimental settings are adopted. Under the ﬁrst setting, for all methods we use the same Tiny\n(Table 2) architecture for a fair comparison. That is, we replace the core self-attention block in SOFT\nwith each baseline’s own attention block with the rest of the architecture unchanged. Note that the\nspatial reduction module of [36] is a special case of Linformer [35]. We set the reduction ratio to\nbe identical to ours. With the same uniform sampling idea, we replace the 1D window averaging\nof Nyströmformer [40] (for NLP tasks) with 2D average pooling (for images). The downsampling\nratio remains identical to ours. It is also worth mentioning that there is no ofﬁcial code released for\nReformer [19] and the local Sensitive Hash (LSH) module has strict requirements on the length of\ninput tokens. We thus do not include this method in our comparison.\nFrom Table 1 we can make the following observations: (i) Linear Transformer methods substantially\nreduce the memory and FLOPs while maintain similar parameter size comparing to the Transformer\non the Tiny architecture; (ii) Our approach SOFT achieves the best classiﬁcation accuracy among all\nthe linearization methods. (iii) Our inference speed is on-par with other compared linear Transformers\nand our training speed is slightly slower than Nystromformer and both are slower than Performer\nand Linformer. Note that the slow training speed of our model is mostly due to the Newton-Raphson\niteration which can only be applied sequentially for ensuring the accuracy of Moore-Penrose inverse.\nIn summary, due to the on-par inference speed we consider the training cost increase is a price worth\npaying for our superior accuracy.\nUnder the second setting, we focus on the memory efﬁciency of SOFT against the baselines. Here we\nfollow the ViT [11] network structure, stacking 12 attention layers with hidden dimension d= 384,\nheads h= 12, bottleneck token sequence length m= 49. Different attention blocks from the three\nlinearized Transformer variants, Linformer [35], Performer [5], and Nyströmformer [40] are studied.\nFor each Transformer variant, we adjust its token sequence lengthnin a linear increment. Speciﬁcally,\nwe use a token sequence length of 784 ×pwhere p = 1,2,3,4,5,6,7,8 and set batch size 1 to\nverify whether the memory consumption increases “quadratically” or “linearly”. Figure 1b shows all\ncompared transformer variants including our SOFT indeed have a linear memory usage complexity.\nThis is in contrast with the standard Transformer which cannot cope with long token sequences with\na quadratic complexity.\n7\n4.3 Comparison with state-of-the-art CNNs and ViTs\nModel Style Resolution M.S. Out.? Params FLOPs Top-1 %.\nResNet-18 [14] ConvNets 2242 ✓ 11M 1.9G 69.8\nPVT-Tiny [36] Transformers 2242 ✓ 13M 1.9G† 75.1\nCoat-Lite Mini [41] Transformers 2242 ✓ 11M 2.0G 78.9\nLambdaNets-50 [2] Transformers 2242 ✓ 16M - 78.9\nSOFT-Tiny SOFT 2242 ✓ 13M 1.9G 79.3\nResNet-50 [14] Convolution 2242 ✓ 25M 4.1G 78.5\nPVT-Small [36] Transformer 2242 ✓ 24M 4.0G† 79.8\nDeit-Small [32] Transformer 2242 % 22M 4.6G 79.9\nT2T-ViTt-14 [42] Transformer 2242 % 21M 5.2G 80.7\nCPVT-Small [7] Transformer 2242 ✓ 22M - 79.9\nTwins-SVT-S [6] Hybrid 2242 ✓ 24M 3.7G 81.7\nSOFT-Small SOFT 2242 ✓ 24M 3.3G 82.2\nResNet-101 [14] Convolution 2242 ✓ 44M 7.9G 79.8\nPVT-Medium [36] Transformer 2242 ✓ 44M 7.0G† 81.2\nViT-Small/16 [11] Transformer 2242 % 48M 9.9G 80.8\nSOFT-Medium SOFT 2242 ✓ 45M 7.2G 82.9\nResNet-152 [14] Convolution 2242 ✓ 60M 11.6G 80.8\nPVT-Large [36] Transformer 2242 ✓ 61M 10.1G† 81.7\nT2T-ViTt-24 [42] Transformer 2242 % 64M 13.2G 82.2\nBoTNet-S1-110[28] Hybrid 2242 ✓ 55M - 82.8\nSOFT-Large SOFT 2242 ✓ 64M 11.0G 83.1\nCaiT-S36[33] Transformer 2242 ✓ 88M 13.9G 83.3\nSwin-B[21] Transformer 2242 ✓ 88M 15.4G 83.3\nTwins-SVT-L [6] Hybrid 2242 ✓ 99M 14.8G 83.3\nSOFT-Huge SOFT 2242 ✓ 87M 16.3G 83.3\nTable 3: Evaluation results on ILSVRC-2012 ImageNet-1K [ 9] validation set. We report the\nresults using the input size of 224x224 pixels center cropped from resized images with 256x256\npixels. M.S.Out.stands for whether the model is designed for multi-scale output. †: Corrected FLOPs\nby taking into account the cost of attention matrix multiplication overlooked in the origin paper.\nWe compare with state-of-the-art alternatives and report the top-1 accuracy on the ImageNet-1K\nvalidation set. FLOPs are calculated at batch size 1. From Figure 1a and Table 3, the following\nobservations are made: (i) Overall, ViT and its variants yield better classiﬁcation accuracy over CNNs.\n(ii) We achieve the best performance among the recent pure vision Transformer based methods\nincluding ViT [11] and DeiT [32], as well as the state-of-the-art CNN RegNet [27]. (iii) Our SOFT\noutperforms the most similar (in architecture conﬁguration) Transformer counterparts PVT [36] at all\nvariants. Since the attention module is the main difference, this validates directly the effectiveness of\nour model. (iv) We can also beat the latest ViT variants Twins [6] which is designed to address the\nefﬁciency limitation of ViT. We have done so with less parameters and fewer ﬂoat point computation.\nTo gain some insights into how attention is learned using our SOFT and the alternatives, Figure 3\nshows the attention masks of various compared models. For each model, we show the output from the\nﬁrst two attention heads. It is evident that SOFT exhibits robustness and versatility in capturing local\nand long distance relations among pixels. It is interesting to note that, although SOFT is trained on\nan object categorization dataset in ImageNet [9], it seems to be able to learn both semantic concepts\nshared across instances in the same category and instance speciﬁc features. For instance, in the\nbottom-right example of a bird class, one attention head focuses on the black bird only, while the\nother attend to both birds in the image. More examples are shown in Appendix A.4.\n4.4 Ablation studies\nPyramidal architecture: Unlike the earlier non-pyramidal vision Transformers (e.g., ViT [11]),\nmost recent pyramidal (multi-scale) Transformers (e.g., PVT [36]) use convolution layers to reduce\n8\nFigure 3: Comparing the attention heatmaps of a query patch (marked by the cross \"+\") against all the\npatches of an image, produced by (a) Transformer [34], (b) Performer [5], (c) Nystromformer [40]\nand (d) Our SOFT. See Appendix A.4 for more examples.\nthe spatial resolution (i.e., token sequence length) between stages. In this study, we ablate SOFT\nwith a pyramidal architecture (our default SOFT-Small), SOFT w/o a pyramidal architecture and\nDeiT-S [32] (no pyramidal architecture either). We replace the Transformer layer with a SOFT layer\nto get SOFT w/o a pyramidal architecture. Note all three variants have similar parameters and FLOPs.\nTable 5a shows that the conv-based pyramidal architecture is clearly superior to a non-pyramidal\ndesign, and our non-pyramidal counterpart is even slightly better than DeiT-S [32] whilst enjoying\nlinear complexity.\n0 10 20 30 40 50\nIterations\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0||AAkA A||2/||A||2\nFigure 4: Convergence analysis for the approxima-\ntion of the Moore-Penrose inverse on SOFT-Tiny.\nBottleneck token sequence length: In this\nstudy, we examine how the bottleneck token se-\nquence length m, sampled from ntokens, inﬂu-\nences the model’s performance. We change the\nbottleneck token sequence length in all stages to\n36,49,64,81. Table 4a shows that longer bottle-\nneck token would increase the memory cost and\nthe computational overhead. m= 49 seems to\ngive the best trade-off between the performance\nand computational overhead. The memory us-\nage is measured with the batch size of 128.\nToken sampling: The sampling function in\nSOFT can assume different forms. Convolu-\ntion: The sequence Q∈Rn×de is ﬁrst reshaped\nto a feature map RH×W×de . r×rconvolution\nkernel with stride of ris applied for downsam-\npling, where r= √sp. The output channel size\nis also kept and no bias is used. At last, the feature map is reshaped back to the sequence. Average\npooling: using a r×r kernel and r stride, where r = √sp. Random sampling: mtokens are\nrandomly picked from ntokens. Biased sampling:We pick mtokens with a biased policy. Here,\nthe ﬁrst mtokens are picked. Table 4b shows that average pooling yields the best performance while\nwith less computational overhead comparing to convolution. Biased sampling can miss the most\nsalient samples, and there is no guarantee that random sampling can keep the uniformity of the chosen\nsamples. This result thus justiﬁes the choice of using average pooling in SOFT.\nBottleneck Memory FLOPs Top-1 %\n36 15.1GB 1.9G 79.0\n49 15.8GB 1.9G 79.3\n64 16.9GB 2.0G 79.3\n81 18.5GB 2.1G 78.9\nSampling methods Params FLOPs Top-1 %\nConvolution 13.07M 2.0G 79.3\nRandom sampling 12.96M 1.9G 79.3\nBiased sampling 12.96M 1.9G 79.0\nAverage pooling 12.96M 1.9G 79.3\nTable 4: (a) Ablations on bottleneck token sequence length. (b) Ablations on sampling methods.\n9\nOverlapped convolution: We ablate SOFT with overlapped convolution (our default choice, same\nas many recent works) and SOFT with non-overlapped convolution in our Tiny conﬁguration.\nTable 5b shows that SOFT with overlapped convolution performs better than SOFT without overlapped\nconvolution. Our non-overlapped convolution variant still outperforms the PVT [36] which also has\nthe same non-overlapped convolution by a clear margin.\nMethods Pyramidal? Top-1 %\nDeiT-S [32] % 79.8\nSOFT % 80.1\nSOFT ✓ 82.2\nMethods Overlapped? Top-1 %\nPVT [36] % 75.1\nSOFT % 77.4\nSOFT ✓ 79.3\nTable 5: (a) Ablations on pyramidal architecture. (b) Ablations on overlapped convolution.\nNewton-Raphson’s convergence:We study how many iterations the Newton-Raphson method\nneeds to converge when computing the Moore-Penrose inverse A†. We use ∥AAkA−A∥p/∥A∥p\nwith p= 2 (see Proposition 1) as the convergence metric to quantify the difference between Ak and\nA†. Figure 4 shows that our approximation converges within 20 iterations across all stages.\n4.5 Additional experiments on NLP tasks\nIn this section, we evaluate our method against other linear counterparts on four tasks of the Long\nRange Arena (LRA) [ 30] benchmark e.g., Listops [ 24], byte-level IMDb reviews text classiﬁca-\ntion [22], byte-level document retrieval [26], and image classiﬁcation on sequences of pixels [20].\nImplementations. We use the Pytorch version of LRA [ 30] benchmark, provided by [ 40]. For\nthe evaluation protocol, we strictly follow [30, 40]. We omit the Pathﬁnder(1K) task as we cannot\nreplicate the result of Nyströmformer [40]. For our SOFT, we simply use the average pooling with\nwindow size 4, stride 4 to sample the bottlenecks. We follow the conﬁgurations of [40], with 2 layers,\n64 and 128 hidden dimension respectively, and 2 attention heads. The results in Table 6 shows that\nour SOFT outperforms both the standard and alternative efﬁcient Transformers on three out of four\ntasks, as well as the average result.\nMethods Listops(2K) Text(4K) Retrieval(4K) Image(1K) Avg. %\nTransformer [34] 37.10 65.02 79.35 38.20 54.92\nReformer [19] 19.05 64.88 78.64 43.29 51.47\nLinformer [35] 37.25 55.91 79.37 37.84 52.59\nPerformer [5] 18.80 63.81 78.62 37.07 49.58\nNyströmformer [40] 37.15 65.52 79.56 41.58 55.95\nSOFT 37.40 63.49 81.77 46.91 57.39\nTable 6: Comparison of different linear/efﬁcient Transformer variants on Long Range Arena [30],\nbased on its ofﬁcial conﬁguration. Our SOFT surpasses previous efﬁcient methods on three tasks.\n5 Conclusions\nWe have introduced a novel softmax-free self-attention (SOFT) mechanism for linearizing Trans-\nformer’s complexity in space and time. Unlike existing linear Transformers that aim to approximate\nthe conventional softmax based self-attention, SOFT employs a Gaussian kernel based attention\nwhich eliminates the need for softmax normalization. This design enables a full self-attention matrix\nto be approximated via a low-rank matrix decomposition. The robustness of the approximation is\nachieved by calculating its Moore-Penrose inverse using a Newton-Raphson method. Extensive\nexperiments show that SOFT yields superior trade-off in accuracy and complexity.\nAcknowledgment This work was funded in part by Shanghai Municipal Science and Technology\nMajor Projects (No.2018SHZDZX01 and No.2021SHZDZX0103), Mindspore, National Science\nFoundation of China under Grant No.11690013, 71991471 and the scientiﬁc-technological innovation\nplan program of Universities guided by the Ministry of Education, China.\n10\nReferences\n[1] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. Layer normalization. arXiv preprint,\n2016. 6\n[2] Irwan Bello. Lambdanetworks: Modeling long-range interactions without attention. arXiv\npreprint, 2021. 8\n[3] Adi Ben-Israel and Dan Cohen. On iterative computation of generalized inverses and associated\nprojections. SIAM Journal on Numerical Analysis, 1966. 5, 15\n[4] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In NeurIPS, 2020. 1, 2\n[5] Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane,\nTamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking\nattention with performers. In ICLR, 2021. 2, 3, 6, 7, 9, 10, 15, 16\n[6] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia,\nand Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers.\narXiv preprint, 2021. 2, 8\n[7] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xiaolin Wei, Huaxia Xia, and Chunhua\nShen. Conditional positional encodings for vision transformers. arXiv preprint, 2021. 8\n[8] Stéphane d’Ascoli, Hugo Touvron, Matthew Leavitt, Ari Morcos, Giulio Biroli, and Levent\nSagun. Convit: Improving vision transformers with soft convolutional inductive biases. In\nICML, 2021. 1\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In CVPR, 2009. 2, 6, 8\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In ACL, 2018. 1, 2\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai,\nThomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al.\nAn image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\n1, 2, 6, 7, 8\n[12] Gregory E Fasshauer. Positive deﬁnite kernels: past, present and future. Dolomites Research\nNotes on Approximation, 2011. 4\n[13] Zhengyang Geng, Meng-Hao Guo, Hongxu Chen, Xia Li, Ke Wei, and Zhouchen Lin. Is\nattention better than matrix decomposition? In ICLR, 2021. 15\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image\nrecognition. In CVPR, 2016. 2, 8\n[15] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training\nby reducing internal covariate shift. In ICML, 2015. 6\n[16] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Joao\nCarreira. Perceiver: General perception with iterative attention. arXiv preprint, 2021. 1\n[17] Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas,\nYi Mao, Weizhu Chen, and Noah A Smith. Finetuning pretrained transformers into rnns. arXiv\npreprint arXiv:2103.13076, 2021. 3\n[18] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and François Fleuret. Transformers\nare rnns: Fast autoregressive transformers with linear attention. In ICML, 2020. 2, 3\n[19] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya. Reformer: The efﬁcient transformer. In\nICLR, 2020. 2, 3, 7, 10\n11\n[20] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images.\nCiteseer, 2009. 10\n[21] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining\nGuo. Swin transformer: Hierarchical vision transformer using shifted windows. arXiv preprint,\n2021. 8\n[22] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher\nPotts. Learning word vectors for sentiment analysis. In Proceedings of the 49th annual meeting\nof the association for computational linguistics: Human language technologies, 2011. 10\n[23] Mindspore. https://www.mindspore.cn/, 2020. 7\n[24] Nikita Nangia and Samuel R Bowman. Listops: A diagnostic dataset for latent tree learning.\narXiv preprint, 2018. 10\n[25] Hao Peng, Nikolaos Pappas, Dani Yogatama, Roy Schwartz, Noah A Smith, and Lingpeng\nKong. Random feature attention. arXiv preprint arXiv:2103.02143, 2021. 3\n[26] Dragomir R Radev, Pradeep Muthukrishnan, Vahed Qazvinian, and Amjad Abu-Jbara. The acl\nanthology network corpus. Language Resources and Evaluation, 2013. 10\n[27] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollár. Design-\ning network design spaces. In CVPR, 2020. 2, 8\n[28] Aravind Srinivas, Tsung-Yi Lin, Niki Parmar, Jonathon Shlens, Pieter Abbeel, and Ashish\nVaswani. Bottleneck transformers for visual recognition. arXiv preprint, 2021. 8\n[29] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Re-\nthinking the inception architecture for computer vision. In CVPR, 2016. 7\n[30] Yi Tay, Mostafa Dehghani, Samira Abnar, Yikang Shen, Dara Bahri, Philip Pham, Jinfeng Rao,\nLiu Yang, Sebastian Ruder, and Donald Metzler. Long range arena: A benchmark for efﬁcient\ntransformers. arXiv preprint, 2020. 10\n[31] Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efﬁcient transformers: A survey.\narXiv preprint, 2020. 3\n[32] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and\nHervé Jégou. Training data-efﬁcient image transformers & distillation through attention. arXiv\npreprint, 2020. 2, 8, 9, 10\n[33] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles, Gabriel Synnaeve, and Hervé Jégou.\nGoing deeper with image transformers. arXiv preprint, 2021. 8\n[34] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017. 1, 2, 3, 4, 6,\n9, 10, 15, 16\n[35] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention\nwith linear complexity. arXiv preprint, 2020. 2, 3, 6, 7, 10\n[36] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping\nLuo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction\nwithout convolutions. arXiv preprint, 2021. 2, 6, 7, 8, 10\n[37] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks.\nIn CVPR, 2018. 2\n[38] Ross Wightman. Pytorch image models. https://github.com/rwightman/\npytorch-image-models, 2019. 6\n[39] Christopher Williams and Matthias Seeger. Using the nyström method to speed up kernel\nmachines. In NeurIPS, 2001. 4, 13\n12\n[40] Yunyang Xiong, Zhanpeng Zeng, Rudrasis Chakraborty, Mingxing Tan, Glenn Fung, Yin Li,\nand Vikas Singh. Nyströmformer: A nyström-based algorithm for approximating self-attention.\nIn AAAI, 2021. 2, 3, 4, 6, 7, 9, 10, 15, 16\n[41] Weijian Xu, Yifan Xu, Tyler Chang, and Zhuowen Tu. Co-scale conv-attentional image\ntransformers. arXiv preprint, 2021. 8\n[42] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH Tay,\nJiashi Feng, and Shuicheng Yan. Tokens-to-token vit: Training vision transformers from scratch\non imagenet. arXiv preprint, 2021. 1, 2, 8\n[43] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon\nYoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. In\nICCV, 2019. 7\n[44] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond\nempirical risk minimization. arXiv preprint, 2017. 7\n[45] Li Zhang, Dan Xu, Anurag Arnab, and Philip HS Torr. Dynamic graph message passing\nnetworks. In CVPR, 2020. 2\n[46] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring self-attention for image recognition.\nIn CVPR, 2020. 2\n[47] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei\nFu, Jianfeng Feng, Tao Xiang, Philip HS Torr, and Li Zhang. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers. In CVPR, 2021. 1\n[48] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr:\nDeformable transformers for end-to-end object detection. In ICLR, 2021. 1\nA Appendix\nA.1 Nyström method\nNyström method [39] aims to calculate a low-rank approximation for a Gram matrix. For Transform-\ners, the self-attention matrix can be viewed as a Gram matrix Swith a Gaussian kernel kapplied to\nthe query Q, with each element Sij expressed as:\nSij = k\n(\nQi,:,Qj,:\n)\n= exp(−∥Qi,: −Qj,:∥2\n2\n2\n√\nd\n), (11)\nk(x,y) means operating Gaussian kernel kto (x,y), which can be written in the feature space as:\nk(x,y) =\nn∑\ni=1\nλiφi(x)φi(y), (12)\nnis the dimension of a feature space, λi denotes the eigenvalue and φi denotes the eigenfunction of\nkernel k. According to the eigenfunction’s deﬁnition, we can get:\n∫\nk(y,x)φi(x)p(x)dx= λiφi(y), (13)\nwhere p(x) is the probability distribution of x. And {φi(x)} are p-orthogonal:\n∫\nφi(x)φj(x)p(x)dx= δij. (14)\nδij is 0 when i ̸= j, 1 when i = j. To get an approximation of the eigenfunctions, we sample\n{x1,x2,··· ,xq}from p(x), then:\n1\nq\nq∑\nt=1\nk(y,xt)φi(xt) ≈λiφi(y), (15)\n13\n1\nq\nq∑\nt=1\nφi(xt)φj(xt) ≈δij. (16)\nThis inspires us to approximate the Gram matrix S. Let S(m) be a submatrix of S, consisting of\nm×melements from S. Gram matrix is a symmetric positive semi-deﬁnite matrix, so it has a\nspectral decomposition:\nS(m)U(m) = U(m)Λ(m), (17)\nwhere U(m) is column orthogonal and Λ(m) is a diagonal matrix with the diagonal elements as the\neigenvalues of S(m). Substituting the yto xj and applying the approximation above to S, we can get:\nφi(xj) ≈√mU(m)\nj,i , λ i ≈λ(m)\ni\nm , (18)\nφi(y) ≈\n√m\nλ(m)\ni\nm∑\nt=1\nk(y,xt)φi(xt), (19)\nλi is eigenvalue of Sand λ(m)\ni is the eigenvalue of S(m). Denote ˜Sas the rank-mapproximation of\nSand ˜U, ˜Λ as the approximation for spectral decomposition of S. Now we can get an approximation\nof Swith rank m:\n˜S = ˜U˜Λ ˜UT =\nm∑\nt=1\n˜λt\n(n)\n˜u(n)\nt (˜u(n)\nt )T. (20)\nSimilarly, we have:\nφi(xj) ≈√nUj,i(n), λ i ≈\n˜λi\n(n)\nn . (21)\nThus\n˜λi\n(n)\n≈nλ(m)\ni\nm , (22)\n˜u(n)\nt ≈\n√m\nn\n1\nλ(m)\nt\nSn,mu(m)\nt . (23)\nThen we get an approximation of S: ˜S ≈Sn,mS†\nm,mSm,n. Shas a block representation below:\nS =\n[\nSm,m Sm,n−m\nSn−m,m Sn−m,n−m\n]\n. (24)\nA.2 Newton method\nA.2.1 Proof of theorem 1\nProof A.1 A is a symmetric positive semi-deﬁnite matrix and Aij ≤1, ∀1 ≤i,j ≤n, Aii =\n1, 1 ≤i≤nin our case. A0 is chosen to be αA, so the Ak can be written as Ak = CkA= ADk\nfor some matrix Ck,Dk, leading to the fact that\nA†AAk = Ak, A kAA†= Ak. (25)\nThis is because Ak+1 = Ak(2In −AAk) = (2In −AkA)Ak and A0 = αA. We make a difference\nbetween A†and Ak+1:\nA†−Ak+1 = A†−2Ak + AkAAk\n= A†−AkAA†−A†AAk + AkAAk\n= (A†−Ak)A(A†−Ak). (26)\n14\nWe norm both sides of the equation above:\n∥A†−Ak+1∥= ∥(A†−Ak)A(A†−Ak)∥\n≤∥A†−Ak∥∥A(A†−Ak)∥. (27)\nAnd we left multiply Aon the both sides of (26), then norm the equation:\n∥AA†−AAk+1∥= ∥A(A†−Ak)A(A†−Ak)∥\n≤∥AA†−AAk∥2. (28)\nWe choose αsufﬁciently small so that the initial value satisfy ∥AA†−AA0∥<1. We set α= 2\n∥A∥2\n1\nto ensure it is small enough [3]. Then the ∥AA†−AAk∥→ 0, when k→∞. The inequality (27)\nimplies that Ak →A†, k→∞.\nA.2.2 Proof of proposition 1\nProof A.2 Note that when we multiply Aon both sides of (26), the equation turns to be:\nA−AAk+1A= A(A†−Ak)A(A†−Ak)A\n= (AA†−AAk)(A−AAkA).\n(29)\nSimilarly norm both sides of (29), considering that ∥AA†−AAk∥→ 0 and ∥AA†−AAk∥< 1\nalways holds, ∥A−AAkA∥monotonically decreases to0. The inequality (27) implies that ∥Ak−A†∥\ndecreases to 0 monotonously .\nNote that although ∥A−AAkA∥monotonically decreases to 0, ∥AkAAk −Ak∥cannot be proved\nto monotonically decrease to 0.\nA.3 Non-linearized gaussian kernel attention\nIn our formulation, instead of directly calculating the Gaussian kernel weights, they are approximated.\nMore speciﬁcally, the relation between any two tokens is reconstructed via sampled bottleneck\ntokens. As the number m(e.g., 49), of bottleneck tokens is much smaller than the token sequence\nlength, our attention matrix is of low-rank. This has two favorable consequences: (I) The model now\nfocuses the attentive learning on latent salient information captured by the mbottleneck tokens. (II)\nThe model becomes more robust against the underlying token noise due to the auto-encoder style\nreconstruction [13].\nThis explains why the model with an approximated gram matrix performs better than the one with\na directly estimated matrix. Further, exact Gaussian kernel attention computation leads to training\ndifﬁculties. We ﬁrst hypothesized that this might be due to lacking normalization (as normalization\noften helps with training stability and convergence), and tested a variant with softmax on top of an\nexact Gaussian kernel attention matrix. However, it turns out to suffer from a similar failure. We\ncannot ﬁnd a solid hypothesis so far and will keep investigate this problem.\nA.4 Attention visualization\nFigure 5 shows more visualization of the attention masks by various Transformers [34, 5, 40] and our\nSOFT. For each model, we show the output from the ﬁrst two attention heads (up and down row). It\nis noteworthy that SOFT exhibits better semantic diversity of the multi-head mechanism than other\nmethods. Moreover, when we sample the patch at the boundary of multiple objects, SOFT is able to\nmore precisely capture all these objects.\n15\nFigure 5: Comparing the attention heatmaps of a query patch (marked by the cross \"+\") against all\nthe patches of an image, produced by (a) Transformer [34], (b) Performer [5], (c) Nyströmformer [40]\nand (d) Our SOFT.\n16",
  "topic": "Softmax function",
  "concepts": [
    {
      "name": "Softmax function",
      "score": 0.8422107696533203
    },
    {
      "name": "Computer science",
      "score": 0.6051247119903564
    },
    {
      "name": "Dot product",
      "score": 0.5328645706176758
    },
    {
      "name": "Computation",
      "score": 0.49993896484375
    },
    {
      "name": "Transformer",
      "score": 0.48797672986984253
    },
    {
      "name": "Computational complexity theory",
      "score": 0.48523974418640137
    },
    {
      "name": "Algorithm",
      "score": 0.4738885760307312
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4220733046531677
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3617590069770813
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3253830075263977
    },
    {
      "name": "Mathematics",
      "score": 0.25818735361099243
    },
    {
      "name": "Convolutional neural network",
      "score": 0.1667415201663971
    },
    {
      "name": "Voltage",
      "score": 0.15218845009803772
    },
    {
      "name": "Geometry",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 61
}