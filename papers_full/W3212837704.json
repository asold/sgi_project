{
  "title": "Low-resource Taxonomy Enrichment with Pretrained Language Models",
  "url": "https://openalex.org/W3212837704",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2800730346",
      "name": "Kunihiro Takeoka",
      "affiliations": [
        "NEC (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2587257636",
      "name": "Kosuke Akimoto",
      "affiliations": [
        "NEC (Japan)"
      ]
    },
    {
      "id": "https://openalex.org/A2170390488",
      "name": "Masafumi Oyamada",
      "affiliations": [
        "NEC (Japan)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W1583820951",
    "https://openalex.org/W3035458998",
    "https://openalex.org/W2971196067",
    "https://openalex.org/W2760204057",
    "https://openalex.org/W2068737686",
    "https://openalex.org/W2593560537",
    "https://openalex.org/W2164480198",
    "https://openalex.org/W2465611764",
    "https://openalex.org/W85880394",
    "https://openalex.org/W2081580037",
    "https://openalex.org/W3012823870",
    "https://openalex.org/W3098698985",
    "https://openalex.org/W2962897394",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2986836624",
    "https://openalex.org/W2980384207",
    "https://openalex.org/W2809189384",
    "https://openalex.org/W2138605095",
    "https://openalex.org/W4295312788",
    "https://openalex.org/W3115653677",
    "https://openalex.org/W2962724755",
    "https://openalex.org/W4253001967",
    "https://openalex.org/W2907607062",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2949676527",
    "https://openalex.org/W1932742904",
    "https://openalex.org/W2464029548",
    "https://openalex.org/W2133413932",
    "https://openalex.org/W3034588514",
    "https://openalex.org/W1993320088",
    "https://openalex.org/W2077335893",
    "https://openalex.org/W205765513",
    "https://openalex.org/W2515754823",
    "https://openalex.org/W2493916176",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2473007590",
    "https://openalex.org/W2970971581",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W4386506836",
    "https://openalex.org/W3105298925",
    "https://openalex.org/W2100132811",
    "https://openalex.org/W2101234009"
  ],
  "abstract": "Taxonomies are symbolic representations of hierarchical relationships between terms or entities. While taxonomies are useful in broad applications, manually updating or maintaining them is labor-intensive and difficult to scale in practice. Conventional supervised methods for this enrichment task fail to find optimal parents of new terms in low-resource settings where only small taxonomies are available because of overfitting to hierarchical relationships in the taxonomies. To tackle the problem of low-resource taxonomy enrichment, we propose Musubu, an efficient framework for taxonomy enrichment in low-resource settings with pretrained language models (LMs) as knowledge bases to compensate for the shortage of information. Musubu leverages an LM-based classifier to determine whether or not inputted term pairs have hierarchical relationships. Musubu also utilizes Hearst patterns to generate queries to leverage implicit knowledge from the LM efficiently for more accurate prediction. We empirically demonstrate the effectiveness of our method in extensive experiments on taxonomies from both a SemEval task and real-world retailer datasets.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2747–2758\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n2747\nLow-resource Taxonomy Enrichment with Pretrained Language Models\nKunihiro Takeoka, Kosuke Akimoto, Masafumi Oyamada\nData Science Research Laboratories, NEC Corporation\n{k_takeoka, kosuke_a, oyamada}@nec.com\nAbstract\nTaxonomies are symbolic representations of\nhierarchical relationships between terms or en-\ntities. While taxonomies are useful in broad\napplications, manually updating or maintain-\ning them is labor-intensive and difﬁcult to\nscale in practice. Conventional supervised\nmethods for this enrichment task fail to ﬁnd\noptimal parents of new terms in low-resource\nsettings where only small taxonomies are avail-\nable because of overﬁtting to hierarchical rela-\ntionships in the taxonomies.\nTo tackle the problem of low-resource taxon-\nomy enrichment, we propose Musubu, an ef-\nﬁcient framework for taxonomy enrichment in\nlow-resource settings with pretrained language\nmodels (LMs) as knowledge bases to compen-\nsate for the shortage of information. Musubu\nleverages an LM-based classiﬁer to determine\nwhether or not inputted term pairs have hier-\narchical relationships. Musubu also utilizes\nHearst patterns to generate queries to leverage\nimplicit knowledge from the LM efﬁciently\nfor more accurate prediction. We empirically\ndemonstrate the effectiveness of our method\nin extensive experiments on taxonomies from\nboth a SemEval task and real-world retailer\ndatasets.\n1 Introduction\nTaxonomies, which represent the hierarchical rela-\ntionships between terms, have been widely utilized\nin tasks related to information retrieval, recommen-\ndation, and classiﬁcation (Agrawal et al., 2009;\nHuang et al., 2019; Babbar et al., 2016). Because\nthe target domain of a speciﬁc taxonomy changes\nover time, taxonomies must be kept up to date so\nthat newly introduced categories and hierarchical\nrelationships can be properly integrated. However,\nmanually constructing and maintaining taxonomies\nis a costly task due to their labor-intensive and\ndomain-speciﬁc nature (Gao et al., 2018; Shen\net al., 2018).\nFigure 1: Enrichment of taxonomy with sufﬁcient re-\nsources (top) and low resources (bottom). Conven-\ntional supervised methods for taxonomy enrichment\nfail to enrich small seed taxonomies because the num-\nber of hierarchical relations of term pairs used as train-\ning samples is limited.\nThe goal of a taxonomy enrichment taskis to\nautomate this costly maintenance (Jurgens and Pile-\nhvar, 2016; Shen et al., 2018), as shown in Fig. 1.\nTaxonomy enrichment methods enrich a taxonomy\nthat may be incomplete, i.e., a seed taxonomy, by\npredicting new hierarchical relationships between\nterms in the seed taxonomy and new terms. We\nfocus on low-resource taxonomy enrichment tasks,\nin which there are few terms in the seed taxonomy,\nspeciﬁcally less than 10,000. Previous studies on\nhypernymy detection used a similar deﬁnition for\nlow-resource settings (Yu et al., 2020). Although\nconventional methods for taxonomy enrichment\nuse large taxonomies (e.g., WordNet taxonomy\n(90,000 terms) (Miller, 1995), Microsoft Academic\nGraph (355,000 terms) (Sinha et al., 2015), most\nof the ones used in realistic situations (as seen in\n2748\n§4) have around a thousand terms, so they are cate-\ngorized as low-resource settings.\nExisting supervised methods (Baroni et al., 2012;\nShen et al., 2020; Manzoor et al., 2020) train their\nparameters with term pairs in a seed taxonomy\nto ﬁnd the optimal parent of a new term. These\nmethods do not work in low-resource settings (as\nwe verify in §4) because they require many term\npairs. Although Mao et al. (2020) partially tackled\nthe problem of a small seed taxonomy, their pro-\nposed method requires additional information such\nas user search logs to improve the performance,\nwhich is not always available.\nTo handle low-resource taxonomy enrichment\ntasks, we propose Musubu, an efﬁcient framework\nfor such situations, with pretrained language mod-\nels (LMs) to compensate for the shortage of infor-\nmation on hierarchical relationships in a seed tax-\nonomy. Musubu leverages a pretrained LM-based\nclassiﬁer to determine whether term pairs have hi-\nerarchical relations for enriching taxonomies. The\nclassiﬁer can ﬁnd the relationships accurately even\nif the training samples are limited because LMs\ncontain real-world term relationships in their pa-\nrameters (Petroni et al., 2019). Musubu also uti-\nlizes Hearst patterns (Hearst, 1992) for generating\nqueries from term pairs to extract implicit knowl-\nedge related to them embedded in LMs efﬁciently.\nWe empirically demonstrate the effectiveness\nof our approach in the taxonomy enrichment\ntask through extensive experiments on both the\nSemEval-2015 Task 17 (Bordea et al., 2015) tax-\nonomies and real-world commerce taxonomies in\nAmazon and Walmart.\nOur contributions are summarized as follows:\n• We propose an approach for taxonomy en-\nrichment that utilizes a pretrained LM as an\nimplicit knowledge base and infers new hier-\narchical relationships.\n• We leverage Hearst patterns to generate\nqueries from term pairs to extract LM’s knowl-\nedge effectively.\n• We empirically demonstrate the effectiveness\nof our method for low-resource taxonomy en-\nrichment through extensive experiments on\nreal-world taxonomies.\n2 Problem Statement\nIn this section, we describe the notations and the\nproblem deﬁnition of this paper.\n2.1 Notations\nWe denote a taxonomy T = ( V,E) as a tree-\nstructured hierarchy with a term set V and a di-\nrected edge set E. A term v ∈ V can consist\nof either a single word (e.g., “tea”) or multiple\nwords (e.g., “soy milk”). The edge set E =\n{(v,vpar); v ∈V}⊂ V ×V, where vpar ∈V\nis a parent of v, represents a set of hierarchical\nrelationships between terms (e.g., “milk tea” →\n“tea”).\n2.2 Problem Deﬁnition\nGiven a seed taxonomy T = (V,E) and a set of\nnew terms V′, taxonomy enrichmentestimates an\nextended taxonomy ˜T = ( ˜V, ˜E) with ˜V = V ∪V′\nand ˜E = E ∪E′(Shen et al., 2018; Mao et al.,\n2020). Here, E′contains new edges (v′,v),v′ ∈\nV′, and v ∈V, where v is a parent term of v′.\nIn this paper, we focus on a situation where the\ntaxonomy |V|is small such as |V|< 10,000 as\nmentioned in §1, hereafter referred to as a low-\nresource taxonomy enrichmentproblem.\n3 Method\nThe goal of low-resource taxonomy enrichment is\nto ﬁnd the optimal parent v⋆ ∈V of each new term\nv′∈V′in low-resource settings. We introduce a\nformulation of the taxonomy enrichment problem\nbased on our probabilistic taxonomy model, then\nelaborate on our classiﬁcation approach based on\npretrained LMs and Hearst patterns.\n3.1 Probabilistic Model on Taxonomy\nTo formulate the taxonomy enrichment problem,\nwe model the entire taxonomy using the graphical\nmodel formulation by Bansal et al. (2014)1. Specif-\nically, we deﬁne the likelihood of a taxonomy using\na graphical model with a factor for each edge in the\ntaxonomy:\np(T|Θ) ∝\n∏\n(v1,v2)∈E\nφE(v1,v2|Θ), (1)\nwhere Θ is a set of model parameters, v2 denotes\nthe parent of a term v1 in taxonomy T, and φE\nis an associated scoring function of the term pair.\nBy using the probabilistic model, we formulate the\ntaxonomy enrichment problem to ﬁnd the optimal\n1While their study considered multiple types of factors, we\nonly use edge factors.\n2749\nenriched taxonomy ˜T⋆ as the following optimiza-\ntion problem:\n˜T⋆ = arg max\n˜T\np( ˜T|Θ)\n= arg max\n˜T\n∏\n(v1,v2)∈˜E\nφE(v1,v2|Θ) (2)\n= arg max\n˜T\n∏\n(v′,v)∈E′\nφE(v′,v|Θ). (3)\nFollowing the conventions of taxonomy enrich-\nment, the low-resource taxonomy enrichment prob-\nlem (§2.2) assumes that a seed taxonomy T never\nchanges and a new termv′is always attached under\nthe terms in a seed taxonomy T (in other words, it\nwill not be attached under previously added terms).\nThe assumptions allow us to ignore the scores of\nedges E in a seed taxonomy and the factors be-\ntween new terms (Eq. (2) →Eq. (3)). Thus, we\ncan cast the problem of ﬁnding the optimal parent\nterm v⋆(∈V) of a new term v′as the following\noptimization problem for each term v′∈V′:\nv⋆ = arg max\nv∈V\nφE(v′,v|Θ). (4)\nThe optimization problem Eq. (4) can be re-\ngarded as a multiclass classiﬁcation problem, the\nclasses of which are V. However, each class,\nv ∈V, has only a few or no positive training ex-\namples i.e., children of v. Thus, instead of directly\nsolving this kind of problem, we treat it as a binary\nclassiﬁcation problem that classiﬁes whether a term\n(v′,v) has a hierarchical relationship.\n3.2 Classiﬁer Leveraging Pretrained\nLanguage Model and Hearst Patterns\nTo mitigate the shortage of information in low-\nresource taxonomy enrichment, our Musubu frame-\nwork, a novel approach to taxonomy enrichment,\nleverages pretrained language models (LMs) as al-\nternative information resources. Musubu consists\nof two main modules: an LM-based classiﬁer pLM\nand a query generator q, as shown in Fig. 2. The\nquery generator generates a query textq(v′,v) from\na given term pair(v′,v). Then, given the query text\nq(v′,v), the LM-based classiﬁer fLM is used to\nclassify whether the term pair has a hierarchical\nrelationship. The LM-based classiﬁer uses implicit\nknowledge embedded in the LM and the query\ngenerator utilizes Hearst patterns (Hearst, 1992) to\ngenerate queries to use the LM’s knowledge efﬁ-\nciently for taxonomy enrichment. In addition, we\nﬁne-tune the LM-based classiﬁer with hierarchical\nrelationships in a seed taxonomy to adapt it to the\ntaxonomy.\nLanguage Models as Knowledge Bases.\nMusubu leverages a pretrained LM as an exter-\nnal knowledge base for enriching taxonomies. Ac-\ncording to Petroni et al. (2019), large LMs (e.g.,\nBERT (Devlin et al., 2019)) acquire term mean-\nings and relationships as their weights by train-\ning on many documents. In low-resource settings,\nLMs can potentially improve the performance of\ntaxonomy enrichment because relational knowl-\nedge learned in LMs can augment a limited num-\nber of available hierarchical relationships in seed\ntaxonomies for training.\nClassiﬁer with Language Models. To leverage\nLMs in the taxonomy enrichment task, we take\nan LM-based text classiﬁcation approach that uses\nan LM and a fully connected (FC) layer to clas-\nsify texts2. LM-based text classiﬁers are more of-\nten used in few-shot classiﬁcation than classiﬁers\nwith word2vec/fasttext (Gupta et al., 2020). We\ninput a query to the LM-based classiﬁer, and then\nthe classiﬁer detects whether or not the term pair\ncorresponding to the query has a hierarchical rela-\ntionship. The classiﬁer is ﬁne-tuned with a seed\ntaxonomy to adapt to the hierarchical relationships\n(see §3.3 for details).\nQuery Generation. To use pretrained LMs ef-\nﬁciently for taxonomy enrichment, Musubu gen-\nerate queries from term pairs by using Hearst\npatterns (Hearst, 1992) and then input them into\nthe LM-based classiﬁer. These patterns are well-\nknown lexical patterns used to represent hypernym-\nhyponym relationships (e.g., “Y such as X”) as\nshown in Table 2. Normally, Hearst patterns are\nused for hypernymy detection from text corpora.\nWe use Hearst patterns in a way that is different\nfrom the original approach to generate a query from\na term pair for LMs, as shown in Fig. 2 (a). For\ninstance, when we have a term pair “oranges” and\n“fruits” and choose a pattern “Y such as X,” we\ngenerate the query “fruits such as oranges.” We\nthen input the generated query to the LM-based\nclassiﬁer to classify whether or not the correspond-\ning term pair has a hierarchical relationship. While\nthe conventional pattern-based approaches ﬁnd hi-\n2Although the masked language model scoring (Salazar\net al., 2020) can be used for taxonomy enrichment by scoring\nqueries of term pairs, this approach is ineffective because\nhierarchical relations in seed taxonomies are ignored as shown\nin the Musubu-noFT row in Table 3.\n2750\nFigure 2: (a) Musubu architecture, (b) training, and (c) inference. Musubu transforms each term pair (v′,v) into a\nHearst pattern-based query “vsuch as v′,” encodes the query, and then determines whether or not the term pair has\na hierarchical relationship.\nerarchical relationships from the text corpus by\nmatching the patterns, our approach utilizes the\nLM’s implicit knowledge using generated queries\nwith patterns.\nHow does the query generator take advantage of\nHearst patterns in generating queries in Musubu?\nOutput sentences from the query generator are fed\ninto LMs trained on a natural language corpus, so\nsentences from the generator should be naturally\nwritten. We found that Hearst patterns were used in\nthe past to extract a hierarchical relationship of two\nterms from a natural language corpus, and the pat-\nterns were then used to generate naturally written\nsentences which imply hierarchies. We veriﬁed that\nthe naturally written queries outperform awkward\nqueries such as space-delimited terms as shown in\n§4.\n3.3 Self-supervised Training and Inference\nWe create training data to enumerate all term pairs\nin a seed taxonomy V ×V and ﬁne-tune the LM-\nbased classiﬁer. As shown in Fig. 2(b), we add\npositive labels when term pairs have hierarchical\nrelationships in the seed taxonomy; otherwise, we\nadd negative labels. We ﬁne-tune the model pa-\nrameters Θ including the LM’s parameters with\nthe training data. We use a binary cross-entropy\nloss as the objective function and minimize it to\nﬁnd the optimal parameters Θ⋆ = minΘ L(Θ). We\nminimize the objective function for ﬁne-tuning our\nmodel:\nL(Θ) =−\n∑\n(v+\n1 ,v+\n2 )∈E\nlog(fLM(q(v+\n1 ,v+\n2 ),Θ))\n−\n∑\n(v−\n1 ,v−\n2 )∈V×V\\E\nlog(1−fLM(q(v−\n1 ,v−\n2 ),Θ)).\n(5)\nTo infer the optimal parent of each new term\nv′ ∈ V′, we take every term in a seed tax-\nonomy v ∈ V, and input a term pair (v′,v)\nto the LM-based classiﬁer to obtain a score\nfLM(q(v′,v),Θ⋆) as shown in Fig 2 (c). Then,\nwe output the term v⋆, which is the highest score,\nv⋆ = arg maxv∈V fLM(q(v′,v),Θ⋆).\n4 Experiments\nIn this section, we describe how we studied the\nperformance of Musubu on seven real-world tax-\n2751\nTable 1: Statistics of taxonomies used in experiments.\n|V|denotes the number of terms in the seed taxonomy\nand Vnl denotes its non-leaf terms. Vtr,Vdev, and V′\ndenote the leaf terms used for training, development,\nand testing, respectively.\nDataset Taxonomy |V| | Vnl| | Vtr| | Vdev| | V′|\nChemical 1146 294 682 170 273\nSemEval-2015 Equipment 406 122 226 57 71\nTask 17 Food 1254 275 783 196 245\nScience 368 107 209 52 66\nAmazon Food 860 159 534 167 134\nCommerce Amazon Kitchen 1019 229 632 158 198\nWalmart 1085 376 567 142 178\nTable 2: List of Hearst patterns used in experiments. Y\ndenotes a parent term of a term X.\nName Pattern\nSuch-as Y such as X\nOne-of X is one of Y\nEspecially Y , especially X\nIs-a X is a Y\nIncluding Y including X\nonomies.\n4.1 Experimental Setup\nDatasets. We used four SemEval taxonomies and\nthree commerce taxonomies including various do-\nmains as shown in Table 1. SemEval-2015 Task\n17 (Bordea et al., 2015) is a taxonomy extraction\ntask, which contains four domains, chemical, equip-\nment, food, and science. Each taxonomy is rel-\natively small compared with the taxonomy used\nin SemEval-2016 Task 14 (Jurgens and Pilehvar,\n2016), which has over 90,000 nodes 3. We also\nused real commerce taxonomies from Amazon re-\nview data (Ni et al., 2019), Grocery & Gourmet\nFoods (Amazon Food), Home & Kitchen (Ama-\nzon Kitchen), and the Walmart taxonomy 4. The\ncommerce taxonomies contain more named enti-\nties than the SemEval taxonomies. For instance,\nthe term “IPA & Pale Ale” appears in the Ama-\nzon Food taxonomy, but does not appear in the\nSemEval food taxonomy.\nConsidering a realistic situation, we held out\n20% of leaf nodes as the new terms V′. Detailed\nstatistics of the taxonomies are listed in Table 1.\nEvaluation Metrics. We used two evaluation\nmetrics, hierarchical-F1 (H-F1) and edge-F1 (E-\n3The task aims to enrich the WordNet taxonomy using new\nterms and their word sense. The number of nodes in the seed\ntaxonomy is out of the scope of this paper due to its size.\n4https://www.kaggle.com/promptcloud/\nwalmart-product-data-2019\nF1) (Mao et al., 2020). Hierarchical-F1 is a com-\nmonly used measure for hierarchical classiﬁcation\ntasks that compares the true path from the true par-\nent to the root with the predicted path (Kiritchenko\net al., 2005). Edge-F1 is the top-1 hit ratio of the\npredicted hypernyms, and is a more strict metric\nthan H-F1.\nCompared Methods. We compared Musubu\nwith ﬁve baseline approaches:\n1. Random: A simple baseline which randomly\nselects a parent term from V.\n2. Microsoft Concept Graph (MCG) : Mi-\ncrosoft Concept Graph 5 (Wu et al., 2012;\nWang et al., 2015) is an existing large-scale hy-\npernymy knowledge base which is extracted\nfrom billions of web pages and consists of\ntriplets (parent term, child term, frequency)\nwhich represent hierarchical relationships. We\nattach a given new term v′ ∈V′ to a term\nv ∈V if a hierarchical relation between v′\nand vexists in the knowledge base.6\n3. TaxoExpan7(Shen et al., 2020): A self-\nsupervised method which leverages a position-\nenhanced graph neural network encoding the\nlocal structure in a seed taxonomy, and it uses\na noise-robust training objective to learn the\nmodel.\n4. MSejrKu (Schlichtkrull and\nMartínez Alonso, 2016): The winning\nmethod in SemEval-2016 Task 14 (Semantic\nTaxonomy Enrichment) (Jurgens and Pilehvar,\n2016) which extracts semantic and lexical\nfeatures and classiﬁes them with support\nvector machines.\n5. Octet (Mao et al., 2020): A self-supervised\nmethod which extracts semantic, lexical, and\ngraph-based features and classiﬁes hierarchi-\ncal relationships between term pairs using a\ntwo-layer feed-forward neural network with\ndropout layers. In the original method, the\ngraph-based features are extracted from e-\ncommerce user queries. In our experiments,\n5https://concept.research.microsoft.\ncom.\n6If there is more than one such relation, we select the one\nwith the highest frequency. If no such relation exists, we\nattach v′to a random term from V . We match terms by simple\nstring-matching after lower-casing.\n7https://github.com/mickeystroller/\nTaxoExpan\n2752\nTable 3: Overall results for taxonomy enrichment on the SemEval and commerce taxonomies. Musubu-noFT\ndenotes Musubu without ﬁne-tuning on the seed taxonomy. The best scores in the columns are in bold.\nSemEval-2015 Taxonomies Commerce Taxonomies\nChemical Equipment Food Science Amazon Food Amazon Kitchen Walmart\nMethod E-F1 H-F1 E-F1 H-F1 E-F1 H-F1 E-F1 H-F1 E-F1 H-F1 E-F1 H-F1 E-F1 H-F1\nRandom 0.00 0.62 0.01 0.53 0.00 0.44 0.01 0.49 0.01 0.34 0.01 0.35 0.00 0.34\nMCG 0.21 0.68 0.06 0.50 0.18 0.55 0.08 0.40 0.06 0.37 0.00 0.33 0.01 0.33\nTaxoExpan 0.00 0.52 0.03 0.46 0.01 0.39 0.01 0.42 0.00 0.36 0.01 0.35 0.00 0.30\nMSejrKu 0.26 0.77 0.13 0.63 0.20 0.62 0.24 0.66 0.18 0.55 0.21 0.53 0.15 0.51\nOctet 0.31 0.76 0.33 0.71 0.24 0.60 0.36 0.70 0.36 0.64 0.28 0.61 0.24 0.60\nMusubu-noFT 0.00 0.60 0.00 0.60 0.00 0.45 0.00 0.44 0.00 0.52 0.00 0.55 0.00 0.56\nMusubu 0.37 0.79 0.45 0.73 0.37 0.68 0.44 0.77 0.40 0.66 0.44 0.71 0.53 0.80\nwe did not use the graph-based features be-\ncause we had no user queries related to the\nseed taxonomies.\n6. Musubu: Our method which leverages\nBERT (Devlin et al., 2019) as a pretrained\nLM, and ﬁne-tunes a LM-based classiﬁer with\nqueries generated from the Such-as pattern.\nTo analyze the effects of ﬁne-tuning, we test\na masked LM scoring method (Salazar et al.,\n2020) on the same queries without ﬁne-tuning\n(Musubu-noFT).\nImplementation Details. In our experiments,\nwe used the public fasttext model (Bojanowski\net al., 2017) trained on the Common Crawl cor-\npus8 to extract semantic features from term pairs in\nTaxoExpan, MSejrKu, and Octet. We also used the\nlexical features proposed by Bansal et al. (2014).\nDuring training, we randomly sampled nine neg-\native term pairs for each positive pair. We imple-\nmented MSejrKu, Octet, and Musubu using Py-\nTorch (Paszke et al., 2019), Transformers (Wolf\net al., 2020), and Scikit-learn (Pedregosa et al.,\n2011). For both Musubu and the baseline meth-\nods, we tuned the hyperparameters including the\noptimizer, initial learning rate, dropout rate, and\nbatch size, on the basis of the average performance\nof 20 random trials on the development set of the\nAmazon Food taxonomy. We used the Adam op-\ntimizer with a tuned learning rate of 8.8 ×10−4,\nand a Tesla V100 GPU for training and inference.\nWe used bert-base-uncased as a pretrained\nmodel in Musubu and limited the maximum length\nof tokens to 64, and longer queries were truncated.\nIn addition, unless otherwise noted, we used the\nSuch-as pattern to generate queries. Table 2 shows\nthe patterns used in the experiments.\n8https://fasttext.cc/docs/en/\nenglish-vectors.html\n4.2 Evaluation Results\nSemEval and Commerce Taxonomies. We eval-\nuated the baselines and our method (Musubu) on\nthe SemEval-2015 Task 17 and real commerce tax-\nonomies shown in Table 3. The method of select-\ning parents randomly (Random) yielded edge-F1\nscores of almost zero, which indicates the task’s\ndifﬁculty. TaxoExpan was not suitable for any tax-\nonomy because the method assumes that the num-\nber of terms in a seed taxonomy is sufﬁciently large\nfor extracting graph-based features with graph neu-\nral networks. Overall, Musubu performed most\neffectively across both metrics in various domains.\nThe results show that our BERT-based approach\noutperformed Octet, which uses fasttext to extract\nsemantic features. MCG was not effective on the\nSemEval and commerce taxonomies because the\nhierarchical relationships in the taxonomies were\nnot always the same relations stored in the general\nis-a database. Finally, Musubu ﬁne-tuned on the\nseed taxonomy was more effective than without\nthat ﬁne-tuning (Musubu-noFT). Although LMs\ngenerally have term relationships, the LM-based\nclassiﬁer needs to be ﬁne-tuned to adapting to term\nrelationships in the seed taxonomy.\nLow-resource Settings. We evaluated the per-\nformance in more low-resource settings than that of\nthe above experiments, as shown in Fig. 3. Because\nTaxoExpan was ineffective in the above settings,\nwe used Octet and MSejrKu as baselines for com-\nparison with Musubu in low-resource settings. The\nresults show Musubu was more effective than the\nbaselines, although the overall performance was\ndeclined when there was an insufﬁcient number of\ntraining terms. Compared Musubu with Octet, the\npretrained LM used in Musubu helped to estimate\nhierarchical relations accurately. The results of\nthe experiment supported our hypothesis that pre-\ntrained LMs are useful for low-resource taxonomy\nenrichment.\n2753\nFigure 3: Performance comparison on edge-F1 for low-\nresource taxonomy enrichment on Amazon Kitchen\ntaxonomy. Musubu outperformed the baselines, espe-\ncially when the seed taxonomy was small.\nFigure 4: Pattern analysis of Musubu on SemEval tax-\nonomies.\nPattern Analysis. We compared the perfor-\nmance of Musubu with several different Hearst\npatterns (Table 2) for generating queries. As a base-\nline pattern, we tested the None pattern, in which\ntwo terms are concatenated with a single space\n(e.g., “fruits oranges” for a term pair (“oranges”,\n“fruits”)). As shown in Fig. 4, the Such-as pattern\nobtained the highest score among the experimented\npatterns on two taxonomies. Using a Hearst pat-\ntern contributes to the performance of taxonomy\nenrichment, as shown in the compared results be-\ntween the None pattern and the others. However,\nthe scores on the chemical taxonomy show that the\nIncluding pattern is more effective than the Such-as\npattern. The results indicate that we should choose\nthe optimal pattern for each domain, although the\nSuch-as pattern can be considered the ﬁrst choice\nfor a general case.\nCase Studies. We analyzed the predictions to\nunderstand the model behavior of Musubu by com-\nparing it and Octet on the Walmart taxonomy. As\nshown in Table 4, both methods predicted “food”\ncorrectly as the parent of “gluten-free foods,” and\nMusubu also captured the lexical features. The\n“men’s socks” row shows that both methods cap-\ntured the semantic features for taxonomy enrich-\nment. The “hair care” row exempliﬁes the differ-\nence between Musubu and Octet. Musubu pre-\ndicted “hair care” as the parent of “dry shampoo”\nby extracting hierarchical relationships from the\nLM, while Octet predicted “skin care” because of\nthe lack of training samples on the term “hair care.”\n5 Related Work\n5.1 Hypernymy Detection\nHypernymy detection is a core natural language\nprocessing (NLP) task for estimating which hy-\npernym term a query term corresponds to, which\nis a subtask of taxonomy construction or extrac-\ntion (Wang et al., 2017). Both unsupervised and\nsupervised methods have been proposed for this\ntask. Unsupervised methods are categorized into\npattern-based or distributional. Pattern-based ap-\nproaches predict that the term pair(x,y) has an is-a\nrelation if xand ysatisfy syntactic patterns in given\ndocuments, for instance, Hearst patterns (Hearst,\n1992) are as shown in Table 2. Distributional ap-\nproaches use the distributional representations of\nterm pairs to measure the strength of their Is-A\nrelationships (Geffet and Dagan, 2005). These ap-\nproaches cannot adapt to the domain-speciﬁc hier-\narchical relationships in the seed taxonomy. In con-\ntrast, the supervised methods follow a classiﬁcation\nparadigm. In most of the supervised methods, each\nterm pair transforms feature vectors constructed\nfrom word embeddings and identiﬁes whether or\nnot they have hypernymy relationships (Baroni\net al., 2012; Roller et al., 2014; Shwartz et al.,\n2016). The methods fail to work in low-resource\ntaxonomy enrichment because there are not enough\ntraining term pairs for learning models.\n5.2 Taxonomy Construction and Enrichment\nTaxonomy construction(taxonomy extraction) is\nan automatic task in which we obtain terms from\na given corpus, construct a graph containing edges\nthat represent hierarchical relationships, and reform\nthe graph into a tree or directed acyclic graph (Wu\net al., 2012; Bansal et al., 2014; Bordea et al., 2016).\nThe second step, constructing a graph, is for ﬁnding\na term pair’s relationships from the given informa-\ntion, similarly to hypernymy detection.\nUnlike taxonomy construction, taxonomy enrich-\nment (Jurgens and Pilehvar, 2016; Shen et al., 2018)\n2754\nTable 4: Case studies of taxonomy enrichment in Walmart taxonomy. Top-3 predicted terms of methods for an\ninput term. The predicted terms in bold are the true parents.\nNew term Method Top 3 predictions\ngluten-free foods Octet food; fresh food; baby food\nMusubu food; snacks, cookies & chips; medical & dental\nmen’s socks Octet men’s clothing; women’s socks, hosiery & tights; men’s shoes\nMusubu men’s shoes; men’s clothing; men’s bags & accessories\ndry shampoo Octet skin care; camping gear; shop by brand\nMusubu hair care; medical & dental; bath safety\naugments a seed taxonomy with new terms by ﬁnd-\ning the optimal hypernym term corresponding to a\nnew term from the taxonomy. The main difference\nbetween taxonomy enrichment and hypernymy de-\ntection is whether or not the hierarchy of taxon-\nomy can be used. While hypernymy detection\nis a task for extracting the aforementioned gen-\neral hypernym-hyponym relations regardless of the\ndomain, taxonomy enrichment strongly depends\non the domain in that it is extended with refer-\nence to the hierarchical relationships of a given\ntaxonomy. For instance, the SemEval-2016 Task\n14 (Jurgens and Pilehvar, 2016) presents a semantic\ntaxonomy enrichment task that extends the Word-\nNet taxonomy with new terms and their deﬁnitions.\nThe winning method in the task (Schlichtkrull and\nMartínez Alonso, 2016) used both the hand-crafted\nsemantic and lexical features of term pairs to ﬁnd\nhierarchical relationships.\nMore complicated approaches have been pro-\nposed for improving the performance of ﬁnding\nhierarchical relationships. Shen et al. (2018) de-\nvised an end-to-end pipeline for extracting new\nterms from documents and taxonomy enrichment,\nwhich integrates AutoPhrase (Shang et al., 2018)\nfor term extraction and a distributional approach\nfor ﬁnding siblings and hypernyms of terms. Other\nmethods utilize self-supervision to enrich a seed\ntaxonomy, but they require either large seed tax-\nonomies and/or additional information about hi-\nerarchical relationships. TaxoExpan (Shen et al.,\n2020) is the ﬁrst attempt to use a graph neural net-\nwork to accurately predict hypernyms with self-\nsupervision. Octet (Mao et al., 2020) utilizes\nthe feature extractors proposed in (Schlichtkrull\nand Martínez Alonso, 2016) and user queries\nas additional information for taxonomy enrich-\nment by means of graph neural networks and self-\nsupervision. In contrast to these methods, including\nthe winning method in the SemEval-2016 task, our\nmethod focuses on low-resource taxonomy enrich-\nment, in which small taxonomies and no text corpus\nare available. The previous methods do not work\non low-resource settings because conventional self-\nsupervised approaches utilize the graph-based fea-\ntures of a large seed taxonomy.\n5.3 Language Models as Knowledge\nPretrained LMs on large text corpora improve the\nperformance on downstream NLP tasks such as\ntext classiﬁcation and question answering (Gupta\net al., 2020; Su et al., 2019). However, an impor-\ntant question was raised about pretrained LMs: do\nthe pretrained LMs have information about enti-\nties and the relationships between them? Petroni\net al. (2019) showed that BERT contains relational\nknowledge as well as knowledge bases. The results\nindicate the weights of pretrained LMs containing\nrelationships between terms, which also include\nhierarchical relationships. The fact that pretrained\nLMs contain the term relationships is used in entity\nset expansion (Zhang et al., 2020). The method\nutilizes the masked LMs to estimate similar entities\nusing formatted queries. Although the method fo-\ncuses on the entity set expansion task, our method\ntackles low-resource taxonomy enrichment tasks.\nThe aforementioned papers suggest that our ap-\nproach for taxonomy enrichment is reasonable.\n6 Conclusion\nWe proposed an efﬁcient self-supervised approach,\nMusubu, for low-resource taxonomy enrichment\ntasks. Musubu utilizes a novel classiﬁer based on\npretrained LMs and Hearst patterns for generating\nqueries. Extensive experiments on taxonomy en-\nrichment showed the effectiveness of Musubu over\nthe conventional approaches on the SemEval and\ncommerce taxonomies, especially in low-resource\nsettings.\n2755\nReferences\nRakesh Agrawal, Sreenivas Gollapudi, Alan Halverson,\nand Samuel Ieong. 2009. Diversifying search re-\nsults. In Proceedings of the Second ACM Interna-\ntional Conference on Web Search and Data Mining,\nWSDM ’09, page 5–14. Association for Computing\nMachinery.\nTakuya Akiba, Shotaro Sano, Toshihiko Yanase,\nTakeru Ohta, and Masanori Koyama. 2019. Op-\ntuna: A next-generation hyperparameter optimiza-\ntion framework. In Proceedings of the 25rd ACM\nSIGKDD International Conference on Knowledge\nDiscovery and Data Mining.\nRohit Babbar, Ioannis Partalas, Éric Gaussier, Massih-\nReza Amini, and Cécile Amblard. 2016. Learning\ntaxonomy adaptation in large-scale classiﬁcation. J.\nMach. Learn. Res., 17:98:1–98:37.\nMohit Bansal, David Burkett, Gerard de Melo, and\nDan Klein. 2014. Structured learning for taxon-\nomy induction with belief propagation. In Proceed-\nings of the 52nd Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 1041–1051, Baltimore, Maryland. As-\nsociation for Computational Linguistics.\nMarco Baroni, Raffaella Bernardi, Ngoc-Quynh Do,\nand Chung-chieh Shan. 2012. Entailment above the\nword level in distributional semantics. In Proceed-\nings of the 13th Conference of the European Chap-\nter of the Association for Computational Linguis-\ntics, pages 23–32, Avignon, France. Association for\nComputational Linguistics.\nPiotr Bojanowski, Edouard Grave, Armand Joulin, and\nTomas Mikolov. 2017. Enriching word vectors with\nsubword information. Transactions of the Associa-\ntion for Computational Linguistics, 5:135–146.\nGeorgeta Bordea, Paul Buitelaar, Stefano Faralli, and\nRoberto Navigli. 2015. Semeval-2015 task 17: Tax-\nonomy extraction evaluation (texeval). In Proceed-\nings of the 9th International Workshop on Semantic\nEvaluation. Association for Computational Linguis-\ntics.\nGeorgeta Bordea, Els Lefever, and Paul Buitelaar. 2016.\nSemeval-2016 task 13: Taxonomy extraction evalu-\nation (texeval-2). In Proceedings of the 10th Inter-\nnational Workshop on Semantic Evaluation. Associ-\nation for Computational Linguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171–4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nYuqing Gao, Jisheng Liang, Benjamin Han, Mohamed\nYakout, and Ahmed Mohamed. 2018. Building a\nlarge-scale, accurate and fresh knowledge graph.the\n24th ACM SIGKDD International Conference on\nKnowledge Discovery and Data Mining Tutorial.\nMaayan Geffet and Ido Dagan. 2005. The distribu-\ntional inclusion hypotheses and lexical entailment.\nIn Proceedings of the 43rd Annual Meeting of the As-\nsociation for Computational Linguistics (ACL’05),\npages 107–114, Ann Arbor, Michigan. Association\nfor Computational Linguistics.\nAakriti Gupta, Kapil Thadani, and Neil O’Hare. 2020.\nEffective few-shot classiﬁcation with transfer learn-\ning. In Proceedings of the 28th International Con-\nference on Computational Linguistics, pages 1061–\n1066, Barcelona, Spain (Online). International Com-\nmittee on Computational Linguistics.\nMarti A. Hearst. 1992. Automatic acquisition of hy-\nponyms from large text corpora. In COLING 1992\nVolume 2: The 14th International Conference on\nComputational Linguistics.\nJin Huang, Zhaochun Ren, Wayne Xin Zhao, Gaole He,\nJi-Rong Wen, and Daxiang Dong. 2019. Taxonomy-\naware multi-hop reasoning networks for sequential\nrecommendation. In Proceedings of the Twelfth\nACM International Conference on Web Search and\nData Mining, WSDM ’19, page 573–581. Associa-\ntion for Computing Machinery.\nDavid Jurgens and Mohammad Taher Pilehvar. 2016.\nSemEval-2016 task 14: Semantic taxonomy enrich-\nment. In Proceedings of the 10th International\nWorkshop on Semantic Evaluation (SemEval-2016),\npages 1092–1102, San Diego, California. Associa-\ntion for Computational Linguistics.\nSvetlana Kiritchenko, Stan Matwin, and A. Fazel\nFamili. 2005. Functional annotation of genes using\nhierarchical text categorization. In Proceedings of\nthe ACL workshop on linking biological literature,\nontologies and databases: mining biological seman-\ntics.\nEmaad Manzoor, Rui Li, Dhananjay Shrouty, and\nJ. Leskovec. 2020. Expanding taxonomies with im-\nplicit edge semantics. In Proceedings of The Web\nConference 2020.\nYuning Mao, Tong Zhao, A. Kan, Chenwei Zhang,\nX. Dong, Christos Faloutsos, and J. Han. 2020.\nOctet: Online catalog taxonomy enrichment with\nself-supervision. In Proceedings of the 26th ACM\nSIGKDD International Conference on Knowledge\nDiscovery and Data Mining.\nG. Miller. 1995. Wordnet: a lexical database for en-\nglish. Commun. ACM, 38:39–41.\nJianmo Ni, Jiacheng Li, and Julian McAuley. 2019.\nJustifying recommendations using distantly-labeled\nreviews and ﬁne-grained aspects. In Proceedings\nof the 2019 Conference on Empirical Methods in\n2756\nNatural Language Processing and the 9th Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP), pages 188–197, Hong\nKong, China. Association for Computational Lin-\nguistics.\nAdam Paszke, Sam Gross, Francisco Massa, Adam\nLerer, James Bradbury, Gregory Chanan, Trevor\nKilleen, Zeming Lin, Natalia Gimelshein, Luca\nAntiga, Alban Desmaison, Andreas Kopf, Edward\nYang, Zachary DeVito, Martin Raison, Alykhan Te-\njani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang,\nJunjie Bai, and Soumith Chintala. 2019. Pytorch:\nAn imperative style, high-performance deep learn-\ning library. In Advances in Neural Information Pro-\ncessing Systems 32, pages 8024–8035. Curran Asso-\nciates, Inc.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research,\n12:2825–2830.\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick Lewis, Anton Bakhtin, Yuxiang Wu, and\nAlexander Miller. 2019. Language models as knowl-\nedge bases? In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Confer-\nence on Natural Language Processing (EMNLP-\nIJCNLP), pages 2463–2473, Hong Kong, China. As-\nsociation for Computational Linguistics.\nStephen Roller, Katrin Erk, and Gemma Boleda. 2014.\nInclusive yet selective: Supervised distributional hy-\npernymy detection. In Proceedings of COLING\n2014, the 25th International Conference on Compu-\ntational Linguistics: Technical Papers, pages 1025–\n1036, Dublin, Ireland. Dublin City University and\nAssociation for Computational Linguistics.\nJulian Salazar, Davis Liang, Toan Q. Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics,\npages 2699–2712, Online. Association for Compu-\ntational Linguistics.\nMichael Schlichtkrull and Héctor Martínez Alonso.\n2016. MSejrKu at SemEval-2016 task 14: Taxon-\nomy enrichment by evidence ranking. In Proceed-\nings of the 10th International Workshop on Semantic\nEvaluation (SemEval-2016), pages 1337–1341, San\nDiego, California. Association for Computational\nLinguistics.\nJingbo Shang, Jialu Liu, Meng Jiang, X. Ren, Clare R.\nV oss, and Jiawei Han. 2018. Automated phrase min-\ning from massive text corpora. IEEE Transactions\non Knowledge and Data Engineering , 30:1825–\n1837.\nJ. Shen, Zhihong Shen, Chenyan Xiong, Chunxin\nWang, Kuansan Wang, and Jiawei Han. 2020. Tax-\noexpan: Self-supervised taxonomy expansion with\nposition-enhanced graph neural network. In Pro-\nceedings of The Web Conference 2020.\nJ. Shen, Zeqiu Wu, Dongming Lei, C. Zhang, Xiang\nRen, M. Vanni, Brian M. Sadler, and Jiawei Han.\n2018. Hiexpan: Task-guided taxonomy construc-\ntion by hierarchical tree expansion. In Proceedings\nof the 24th ACM SIGKDD International Conference\non Knowledge Discovery & Data Mining.\nVered Shwartz, Yoav Goldberg, and Ido Dagan. 2016.\nImproving hypernymy detection with an integrated\npath-based and distributional method. In Proceed-\nings of the 54th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers), pages 2389–2398, Berlin, Germany. Associa-\ntion for Computational Linguistics.\nArnab Sinha, Zhihong Shen, Yang Song, Hao Ma, Dar-\nrin Eide, Bo-June (Paul) Hsu, and Kuansan Wang.\n2015. An overview of microsoft academic service\n(mas) and applications. WWW ’15 Companion,\npage 243–246, New York, NY , USA. Association for\nComputing Machinery.\nDan Su, Yan Xu, Genta Indra Winata, Peng Xu,\nHyeondey Kim, Zihan Liu, and Pascale Fung. 2019.\nGeneralizing question answering system with pre-\ntrained language model ﬁne-tuning. In Proceedings\nof the 2nd Workshop on Machine Reading for Ques-\ntion Answering, pages 203–211, Hong Kong, China.\nAssociation for Computational Linguistics.\nChengyu Wang, Xiaofeng He, and Aoying Zhou. 2017.\nA short survey on taxonomy learning from text cor-\npora: Issues, resources and recent advances. In Pro-\nceedings of the 2017 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1190–\n1203, Copenhagen, Denmark. Association for Com-\nputational Linguistics.\nZhongyuan Wang, Haixun Wang, Ji-Rong Wen, and\nYanghua Xiao. 2015. An inference approach to\nbasic level of categorization. In Proceedings of\nthe 24th ACM International on Conference on In-\nformation and Knowledge Management, CIKM ’15,\npage 653–662, New York, NY , USA. Association for\nComputing Machinery.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\n2757\nWentao Wu, Hongsong Li, Haixun Wang, and Kenny Q.\nZhu. 2012. Probase: A probabilistic taxonomy for\ntext understanding. In Proceedings of the 2012 ACM\nSIGMOD International Conference on Management\nof Data, SIGMOD ’12, page 481–492, New York,\nNY , USA. Association for Computing Machinery.\nChanglong Yu, Jialong Han, Haisong Zhang, and Wil-\nfred Ng. 2020. Hypernymy detection for low-\nresource languages via meta learning. In Proceed-\nings of the 58th Annual Meeting of the Association\nfor Computational Linguistics, pages 3651–3656,\nOnline. Association for Computational Linguistics.\nYunyi Zhang, Jiaming Shen, Jingbo Shang, and Jiawei\nHan. 2020. Empower entity set expansion via lan-\nguage model probing. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 8151–8160, Online. Association\nfor Computational Linguistics.\nA Detailed Experimental Settings\nA.1 Data Preparation\nTo ﬁlter out noisy terms in the original commerce\ntaxonomies, we removed infrequent terms that ap-\npeared fewer than ﬁve (Amazon Food) or 20 times\n(Amazon Kitchen and Walmart) in item categories\nof items and were shorter than ten words. We\ndid not remove any terms from the SemEval tax-\nonomies. We split the leaf terms into the training\nset and V′with a ratio of 80% / 20% and then split\nthe training set into Vtr and Vdev at the same ratio.\nWe set the non-leaf terms in the taxonomy as Vnl.\nThe numbers of terms are listed in Table 1. The\nevaluation scores were the averages over the scores\non three trials.\nA.2 Models\nMusubu. We used\nBertForSequentialClassification9\nas our LM-based classiﬁer and initialized its\nparameters with bert-base-uncased. We\nﬁne-tuned the last layer in the BERT model and\nthe classiﬁcation layer (fully connected layer) and\nfroze the parameters in the other layers to avoid\noverﬁtting the model. The number of parameters\nin Musubu is about 109M, and the number of\ntrainable parameters is about 7M.\nMusubu-noFT (Musubu without Fine-tuning).\nWe also used bert-base-uncased as a pre-\ntrained LM to calculate the likelihood of queries\n9https://huggingface.co/\ntransformers/model_doc/bert.html#\nbertforsequenceclassification\nwithout ﬁne-tuning on a seed taxonomy. Musubu-\nnoFT generates Hearst pattern-based queries from\nterm pairs, calculates the likelihoods of the queries,\nand then ﬁnds the optimal parent of each new term\nwhile maximizing the likelihood. To calculate the\nlikelihood, we used the masked language model\nscoring (Salazar et al., 2020) with the public imple-\nmentation provided by the authors10. There are no\ntrainable parameters in the approach.\nTaxoExpan (Shen et al., 2020). We used the\nauthors’ public implementation and set the hyper-\nparameters by default in the original source code.\nTo extract semantic features, we used fasttext as\nsame as Octet and MSejrKu. The number of train-\nable parameters in TaxoExpan is 1.9M.\nOctet (Mao et al., 2020). We extracted seman-\ntic and lexical features, input them into a two-\nlayer feed-forward network with dropout layers,\nand used the output as the probability of terms.\nWhile the original method used graph-based fea-\ntures generated from user queries and the taxonomy\nin addition to the above features, we did not use\nthem because we did not have any user queries. We\nconstructed the lexical features following (Bansal\net al., 2014) and the semantic features with fast-\ntext (Bojanowski et al., 2017) trained by the Com-\nmon Crawl dataset. To tune the model parameters,\nwe used the same optimizer and the number of neg-\native samples as that of our proposed method. The\nnumber of parameters in Octet is 503K.\nMSejrKu (Schlichtkrull and Martínez Alonso,\n2016). Although this method was the winner of\nthe SemEval-2016 Task 14, the implementation is\nnot public. According to (Mao et al., 2020), the\nfeatures used in Octet are similar to those used in\nMSejrKu. We extracted the semantic and lexical\nfeatures in Octet, and then input a support vector\nmachine.\nA.3 Hyperparameter tuning\nWe tuned the hyperparameters of our and that of\nthe baselines for taxonomy enrichment using their\nH-F1 on the Amazon Food development dataset11\nby Optuna (Akiba et al., 2019). See Table 5 for the\ndetailed ranges of tuned hyperparameters and the\noptimal values for our method and the baselines.\nWe conducted 20 trials for each method to tune the\nhyperparameters.\n10https://github.com/awslabs/\nmlm-scoring\n11We used the optimal hyperparameter settings for the other\ndataset.\n2758\nTable 5: Ranges of tuned hyperparameters and hyper-\nparameter conﬁgurations of Musubu and Octet.\nMethod Hyperparameter Range Best value(s)Musubu batch size{128,256,512,1024,2048} 2048Musubu learning rate[10−5,10−3] 8 .8×10−4\nOctet batch size {128,256,512,1024,2048} 128Octet learning rate [10−5,10−3] 4 .5×10−4\nOctet dropout rates (0,0.2) (9 .5×10−3,0.5×10−2,0.20)\nOctetdimension sizes ofhidden layers [256,1024] [366 ,753]\nTable 6: Pattern analysis of Musubu on the SemEval\ntaxonomies. Optimal scores in the columns are in bold.\nChemical Equipment Food Science\nPattern E-F1 H-F1 E-F1 H-F1 E-F1 H-F1 E-F1 H-F1\nNone 0.37 0.78 0.41 0.73 0.32 0.65 0.38 0.74\nSuch-as 0.37 0.79 0.45 0.73 0.37 0.68 0.44 0.77\nOne-of 0.40 0.79 0.40 0.75 0.35 0.65 0.38 0.76\nEspecially0.37 0.78 0.44 0.75 0.36 0.67 0.44 0.75\nIs-a 0.38 0.79 0.44 0.77 0.39 0.68 0.38 0.75\nIncluding 0.45 0.78 0.44 0.75 0.37 0.68 0.40 0.76\nA.4 Computing Environment and Runtime\nWe used an Amazon EC2 instance “p3.2xlarge\" as a\ncomputing infrastructure for training and inference.\nMusubu and TaxoExpan took about an hour and a\nhalf to learn the parameters, and Octet took about\nten to 20 minutes, excluding feature extraction. The\nother methods took less than ten minutes.\nB Experimental Results\nWe evaluated the performance of Musubu with sev-\neral different Hearst patterns (Table 2) for gener-\nating queries. Table 6 shows the detailed data of\nFig. 4.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8468838930130005
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6650292873382568
    },
    {
      "name": "Overfitting",
      "score": 0.6261040568351746
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5751947164535522
    },
    {
      "name": "Classifier (UML)",
      "score": 0.5576321482658386
    },
    {
      "name": "Taxonomy (biology)",
      "score": 0.5496675372123718
    },
    {
      "name": "Machine learning",
      "score": 0.53387451171875
    },
    {
      "name": "Task (project management)",
      "score": 0.5200757384300232
    },
    {
      "name": "Economic shortage",
      "score": 0.49744442105293274
    },
    {
      "name": "Natural language processing",
      "score": 0.43663355708122253
    },
    {
      "name": "Information retrieval",
      "score": 0.33162665367126465
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Artificial neural network",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Government (linguistics)",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ]
}