{
  "title": "Knowledge Prompting in Pre-trained Language Model for Natural Language Understanding",
  "url": "https://openalex.org/W4385573097",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2103809590",
      "name": "Jianing Wang",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2097235863",
      "name": "Wenkang Huang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2114674632",
      "name": "Minghui Qiu",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2556967188",
      "name": "Qiuhui Shi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2116476370",
      "name": "Hongbin Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2068388382",
      "name": "Xiang Li",
      "affiliations": [
        "East China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A2053925183",
      "name": "Ming Gao",
      "affiliations": [
        "East China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4285208819",
    "https://openalex.org/W6791858558",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3176750236",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W4309811444",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W3090656107",
    "https://openalex.org/W2892094955",
    "https://openalex.org/W4287028759",
    "https://openalex.org/W3182352988",
    "https://openalex.org/W2983915252",
    "https://openalex.org/W2962891712",
    "https://openalex.org/W2949695381",
    "https://openalex.org/W3153427360",
    "https://openalex.org/W2963777632",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W3175604467",
    "https://openalex.org/W2972167903",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W2107598941",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W4303202235",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W4303468996",
    "https://openalex.org/W1854214752",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W4289597504",
    "https://openalex.org/W4221157571",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W3214536449",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W2100664567",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2759211898",
    "https://openalex.org/W2994915912",
    "https://openalex.org/W3205270560",
    "https://openalex.org/W3114916066",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3156636935",
    "https://openalex.org/W3173777717",
    "https://openalex.org/W3151929433"
  ],
  "abstract": "Knowledge-enhanced Pre-trained Language Model (PLM) has recently received significant attention, which aims to incorporate factual knowledge into PLMs. However, most existing methods modify the internal structures of fixed types of PLMs by stacking complicated modules, and introduce redundant and irrelevant factual knowledge from knowledge bases (KBs). In this paper, to address these problems, we introduce a seminal knowledge prompting paradigm and further propose a knowledge-prompting-based PLM framework KP-PLM. This framework can be flexibly combined with existing mainstream PLMs. Specifically, we first construct a knowledge sub-graph from KBs for each context. Then we design multiple continuous prompts rules and transform the knowledge sub-graph into natural language prompts. To further leverage the factual knowledge from these prompts, we propose two novel knowledge-aware self-supervised tasks including prompt relevance inspection and masked prompt modeling. Extensive experiments on multiple natural language understanding (NLU) tasks show the superiority of KP-PLM over other state-of-the-art methods in both full-resource and low-resource settings. Our source codes will be released upon the acceptance of the paper.",
  "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3164–3177\nDecember 7-11, 2022 ©2022 Association for Computational Linguistics\nKnowledge Prompting in Pre-trained Language Model for\nNatural Language Understanding\nJianing Wang1, Wenkang Huang2, Qiuhui Shi2, Hongbin Wang2,\nMinghui Qiu3, Xiang Li1∗, Ming Gao1,4\n1 School of Data Science and Engineering, East China Normal University, Shanghai, China\n2 Ant Group, Hangzhou, China 3 Alibaba Group, Hangzhou, China\n4 KLATASDS-MOE, School of Statistics, East China Normal University, Shanghai, China\nlygwjn@gmail.com, wenkang.hwk@alibaba-inc.com\n{qiuhui.sqh,hongbin.whb}@antgroup.com\nminghui.qmh@alibaba-inc.com\n{xiangli,mgao}@dase.ecnu.edu.cn\nAbstract\nKnowledge-enhanced Pre-trained Language\nModel (PLM) has recently received significant\nattention, which aims to incorporate factual\nknowledge into PLMs. However, most ex-\nisting methods modify the internal structures\nof fixed types of PLMs by stacking compli-\ncated modules, and introduce redundant and\nirrelevant factual knowledge from knowledge\nbases (KBs). In this paper, to address these\nproblems, we introduce a seminal knowledge\nprompting paradigm and further propose a\nknowledge-prompting-based PLM framework\nKP-PLM. This framework can be flexibly com-\nbined with existing mainstream PLMs. Specifi-\ncally, we first construct a knowledge sub-graph\nfrom KBs for each context. Then we design\nmultiple continuous prompts rules and trans-\nform the knowledge sub-graph into natural lan-\nguage prompts. To further leverage the factual\nknowledge from these prompts, we propose\ntwo novel knowledge-aware self-supervised\ntasks including prompt relevance inspection\nand masked prompt modeling. Extensive ex-\nperiments on multiple natural language under-\nstanding (NLU) tasks show the superiority of\nKP-PLM over other state-of-the-art methods in\nboth full-resource and low-resource settings 1.\n1 Introduction\nPre-trained Language Models (PLMs) have be-\ncome the dominant infrastructure for a majority of\ndownstream natural language understanding (NLU)\ntasks, which generally adopt a two-stage training\nstrategy, i.e., pre-training and fine-tuning. Recent\nnotable PLMs include BERT (Devlin et al., 2019),\n∗Correspondence to Xiang Li.\n1All the codes and datasets have been released to https:\n//github.com/wjn1996/KP-PLM\nMasked Mention\nDeep Transformer Encoder\nMLM Head\nMention\nDeep Transformer Encoder\nFusion Module\nKnowledge Encoder\nKG Entity\nWord-Knowledge Graph\nGraph Neural Networks\nMLM Head\n[SEP]\nDeep Transformer Encoder\nMLM Head\nSub-graph\nMention1 Mention2\nContinuous \nPrompting\nPrompt1 Prompt2\nKnowledge Head \n(a) Knowledge Masking (b) Knowledge Fusion\n(c) Graph Learning (d) Our Proposal: Knowledge Prompting\nPLM Module New Module Special Token Masked TokenEntity Mention Token\n[CLS] [SEP] [CLS] [SEP]\n[CLS] [CLS][SEP] [SEP]\nFigure 1: The comparison between knowledge prompt-\ning with other paradigms. (Best viewed in color.)\nRoBERTa (Liu et al., 2019), ALBERT (Lan et al.,\n2020), XLNet (Yang et al., 2019), GPT-2 (Radford\net al., 2019) and DeBERTa (He et al., 2021b). Al-\nthough many existing models learn useful inherent\nlinguistic knowledge from large-scale corpora, it\nis hard for them to understand the explicit factual\nknowledge (IV et al., 2019; Sun et al., 2020).\nTo further leverage factual knowledge, a branch\nof knowledge-enhanced methods (Zhang et al.,\n2019; Sun et al., 2020; He et al., 2021a; Wang\net al., 2021b,a; Zhang et al., 2021; Arora et al.,\n2022) have been proposed for PLMs to capture\nrich semantic knowledge from knowledge bases\n(KBs). Figure 1 summarizes the paradigms of ex-\nisting approaches, which can be mainly divided\ninto three categories. First, knowledge-masking-\nbased methods perform alignment, mask mentions\nand learn explicit knowledge of entities based\non masked language modeling (MLM). Second,\nknowledge-fusion-based methods separately learn\nembeddings of entities in sentences and that of\n3164\nnodes in KBs, which are further aggregated. Third,\ngraph-learning-based methods construct a graph\nbased on contextualized sentences and KBs, and\nadopt a graph-based encoder to learn entity em-\nbeddings, such as graph neural networks (Kipf and\nWelling, 2017). Despite the success, there could\nstill be two problems in these methods. On the one\nhand, some approaches modify internal structures\nof existing PLMs by stacking complicated modules,\nwhich adversely affects the computational cost of\nthe model. Further, these methods are generally\nbased on fixed types of PLMs, which leads to the in-\nflexibility and restricts their wide applicability (He\net al., 2021a; Zhang et al., 2021). On the other\nhand, some methods introduce redundant and irrel-\nevant knowledge from KBs, which are the knowl-\nedge noises and could degrade the model perfor-\nmance (Peters et al., 2019): 1) Redundant Knowl-\nedge. Previous methods (Sun et al., 2020; Liu et al.,\n2020; Sun et al., 2019; Wang et al., 2021b) inject\nthe corresponding triples or pre-trained knowledge\nembeddings into each entity in the context. How-\never, the entity may appear multiple times in one\nsentence, these methods could introduce duplicate\ninformation, which can be viewed as redundant\nknowledge. 2) Irrelevant Knowledge. Some enti-\nties or the corresponding sub-graphs are irrelevant\nwith the whole sentence semantics, which are use-\nless and have less contributions to the performance\nimprovement.\nRecently, the paradigm of prompt-based fine-\ntuning (i.e., prompt-tuning) has been proposed\nto explore the inherent knowledge of PLMs\nby introducing a task-specific template with the\n[MASK] token (Schick and Schütze, 2021) 2. In-\nspired by prompt-tuning, in this paper, we intro-\nduce a novel knowledge prompting paradigm for\nknowledge-enhanced PLMs and propose an ef-\nfective Knowledge-Prompting-based PLM frame-\nwork, namely, KP-PLM. As shown in Figure 1(d),\nwe aim to construct the related knowledge sub-\ngraph for each sentence. To alleviate introducing\ntoo much knowledge noises, we restrict all the rela-\ntion paths start from the topic entity (the entry of\neach sentence) and end at the tail entity which is\nmentioned in this sentence. To bridge the structure\ngap between the context and KB, we can transform\n2For example, in sentiment analysis, a prompt tem-\nplate (e.g., “It was [MASK].”) is added to the review text\n(e.g., “The film is very attractive.”). We can obtain the result\ntokens of masked position for label prediction (e.g., “great”\nfor the positive label and “boring” for the negative label).\neach relation path into a natural language prompt,\nand then concatenate these prompts with the orig-\ninal sentence to form a unified input sequence,\nwhich can be flexibly fed into any PLMs without\nchanging their internal structures (Brown et al.,\n2020; Schick and Schütze, 2021). Further, to lever-\nage the factual knowledge in the prompts, we pro-\npose two novel knowledge-aware self-supervised\nlearning tasks: Prompt Relevance Inspection (PRI)\nand Masked Prompt Modeling(MPM). Specifically,\nPRI encourages the PLM to learn the semantic rele-\nvance of multiple knowledge prompts, while MPM\naims to predict the masked entity in a prompt. We\nconduct extensive experiments to verify the effec-\ntiveness of KP-PLM on multiple NLU tasks. Our\nresults show that KP-PLM can be flexibly inte-\ngrated with mainstream PLMs and the integrated\nmodel can outperform strong baselines in both full-\nresource and low-resource settings. We next sum-\nmarize our main contributions as follows:\n• We propose a novel knowledge prompting\nparadigm to incorporate factual knowledge\ninto PLMs.\n• We present the KP-PLM framework that can\nbe flexibly combined with mainstream PLMs.\n• We design two knowledge-aware self-\nsupervised tasks to learn factual knowledge\nfrom prompts.\n• We conduct extensive experiments to show the\neffectiveness of KP-PLM in both full-resource\nand low-resource scenarios.\n2 Related Work\nKnowledge-enhanced PLMs. To further inject\nfactual knowledge into PLMs, recent knowledge-\nenhanced PLMs have been proposed to incorporate\nfactual knowledge in the pre-training stage (Sun\net al., 2019; Xiong et al., 2020; Liu et al., 2020;\nSun et al., 2020; Zhang et al., 2019; He et al.,\n2021a; Zhang et al., 2021; Su et al., 2021; Arora\net al., 2022; Ye et al., 2022; Yu et al., 2022a,b;\nChen et al., 2022; Liu et al., 2021c). For example,\nERNIE-Baidu (Sun et al., 2019) introduces two\nnovel phrase-level masking and entity-level mask-\ning strategies to learn explicit semantic knowledge.\nK-BERT (Liu et al., 2020) and CoLAKE (Sun et al.,\n2020) utilize contextualized sentences and KBs to\nconstruct a graph, based on which graph-based\n3165\nlearning methods are employed to capture seman-\ntic information. In addition, ERNIE-THU (Zhang\net al., 2019), KEPLER (Wang et al., 2021b) and\nKLMo (He et al., 2021a) integrate the pre-trained\nknowledge base embeddings into PLMs through\nattentive fusion modules to improve the contextual\nrepresentations. Different from these methods, we\npropose a novel knowledge prompting paradigm\nto enhance PLMs, which learns factual knowledge\nfrom prompts.\nPrompting for PLMs. In the fine-tuning stage,\ntraditional fine-tuning paradigms introduce new pa-\nrameters for task-specific predictions, which could\nlead to the over-fitting problem in low-resource\nscenarios (Brown et al., 2020; Schick and Schütze,\n2021). To address the issue, prompt-tuning has\nrecently been proposed (Brown et al., 2020; Schick\nand Schütze, 2021; Gao et al., 2021a; Liu et al.,\n2021b). For example, GPT-3 (Brown et al., 2020)\nproposes to enable in-context learning with hand-\ncraft prompts in zero-shot scenarios. In addition,\nSchick and Schütze (2021) and Gu et al. (2021)\nexplore all the tasks in a unified cloze-style format,\nwhich boosts the performance of PLMs in few-shot\nlearning tasks. There are also recent works that im-\nprove prompt-tuning based on heuristic rules (Han\net al., 2021), automatic prompt construction (Gao\net al., 2021a; Shin et al., 2020) and continuous\nprompt learning (Liu et al., 2021b,a; Hu et al., 2021;\nGu et al., 2021). Inspired by prompt-tuning, we\ninject factual knowledge into PLMs by designing\nmultiple continuous prompts.\n3 The KP-PLM Framework\nIn this section, we first describe the basic notations,\nand then formally present the techniques of the KP-\nPLM in detail.\n3.1 Notations\nA knowledge graph is denoted as G= (E,R,T).\nHere, E is a set of entities, Ris a set of rela-\ntions, and T is a set of triples to express semantic\nknowledge. Specifically, T = {(eh,r,et)|eh,et ∈\nE,r ∈ R}, where eh, r and et denote head en-\ntity, relation and tail entity, respectively. Given a\nknowledge graph G, let (e0,{(ri,ei)}k\ni=1) be a k-\nhop relation path starting from entity e0 to ek in G.\nWe further denote Vas the vocabulary set used in\nPLMs and S = {w1,w2,···} as a sentence, where\nwi ∈V is the i-th token of the sentence.\nContext: A COVID-19 vaccine intended to\nprovide acquired immunity against virus\nnamed SARS-CoV-2, which causes COVID-\n19 in 2019.\nTopic Entity: COVID-19 vaccine\nCOVID-19\nCOVID-19\nprevent SARS-CoV-2\ntime\npathogen\nagainst\nAcquired \nImmunity \nis a\nfunction\nZhong\nNanshan\nWuhan\nsigniﬁcant \nperson\ndiscovery place Q84263196 \nQ87719492 \nQ9096750 \nQ1 1746 \nQ50262600 \nmRNAtechnology\nvaccine\nremedy\nQ1595418 \ninstance of\nQ94032548 \npublished in\nCoronavirus\nQ57751738 \nsubclass of\nQ82069695 \ninstance of\ninstance ofQ1 1563 \nQ3186692 \nyearnumber\nQ25274 \n2019\nPruned Entity\nRelated Entity\nTopic Entity\nRelation\nQ134808 \nQ546003 \nVaccine\nCOVID-19\nQ87719492 vaccine SARS-CoV-2\nQ82069695 \nagainst\nQ134808 \nVaccineCOVID-19\nQ87719492 vaccine Acquired \nImmunity \nQ94032548 \nis a function\nCOVID-19\nQ87719492 vaccine COVID-19\nQ84263196 \nQ25274 2019\nSARS-CoV-2\nQ82069695 \n(a) Contextual Knowledge Sub-graph\n[K] [P1] COVID-19 vaccine [P2] against [P3]\nSARS-CoV-2 [P4] [/K]\n[K] [P1] COVID-19 vaccine [P2] is a [P3]\nVaccine [P4] that [P5] function [P6]\nAcquired Immunity [P7] [/K]\n[K] [P1] COVID-19 vaccine [P2] prevent\n[P3] COVID-19 [P4] that [P5] time [P6]\n2019 [P7], and [P8] pathogen [P9] SARS-\nCoV-2 [P10] [/K]\n(b) 1-hop 1-chain Prompting\nprevent time\npathogen\n(c) 2-hop 1-chain Prompting\n(d) 2-hop 2-chain Prompting\nx\nThe BMJ\nStage1: Contextual Knowledge \nSub-graph Construction\nStage2: Continuous Prompting \nMapping\nFigure 2: The illustration on knowledge prompting. We\nfirst construct a 2-hop sub-graph for each context based\non target entity, and then generate continuous prompts\nbased on three kinds of mapping formats. (Best viewed\nin color.)\n3.2 Knowledge Prompting\nKnowledge prompting aims to construct the knowl-\nedge sub-graph for each sentence, and then trans-\nform factual knowledge into natural language\nprompts. Figure 2 illustrates the process in two\nsteps.\nContextual Knowledge Sub-graph Construction.\nMost existing methods integrate knowledge from\nKBs indiscriminately with all the mentions in the\noriginal context, which could bring in redundant\nand irrelevant information to PLMs (Zhang et al.,\n2021). Generally, a pre-training context is derived\nfrom the encyclopedia (e.g. Wikipedia), it is usu-\nally tagged with an entry that describes the topic of\nthe context and can be viewed as the topic entity.\nIntuitively, the sub-graph centering at the topic en-\ntity in KBs could contain semantic knowledge that\nis highly correlated with the context. Therefore,\nwe propose to construct a KB sub-graph for each\ncontext based on the corresponding topic entity.\nWe first scan all the entries in Wikipedia Dumps\nand derive their corresponding texts, which form\na large-scale corpus. After that, for each sentence\nS, we utilize the TAGME toolkit (Ferragina and\nScaiella, 2010) to generate MS, a set of entity men-\ntions in the sentence. Then, for each sentence S\nand its associated topic entity eS, we generate a\n2-hop sub-graph GS for S, which centers at eS and\nincludes all the k-hop relation paths starting from\neS in G. Here, k ≤2. However, this will lead to\nidentical sub-graphs for various sentences with the\n3166\n[CLS] A [MASK] ... [SEP] [P1]to ... [MASK] [K] [/K] [K] [P1] is a [P2] ... [/K] [SEP]\nSARS-CoV-2 \nCOVID-19 \nvaccine \nCOVID-19 \nvaccine \nCOVID-19 \nvaccine \n[CLS] A [MASK] ... [SEP] [P1]to ... [MASK] [K] [/K] [K] [P1] is a [P2] ... [/K] [SEP]COVID-19 \nvaccine \nCOVID-19 \nvaccine \nCOVID-19 \nvaccine \nIntented\nPLMs \nVocab\nInput \nTokens\nPosition \nIds\nDeep Transformer Encoder\n......\n......\n0 1 2 3 4 ... 17 18 19 20 ... 24 25 18 19 20 21 22 ... 32 ...... 33\nSegmentation \nIds  0 0 0 0 0 ... 0 1 1 1 ... 1 1 1 1 1 1 1 ... 1 ...... 1\nOutput\npress \nsuper \n...\n...good \nMLM Head Masked Prompt \nModeling (MPM) \nKnowledge Prompt1 Knowledge Prompt2Context\nintented \n V erbalizer \nModel\nKnowledge Sub-graph \nPrompt Relevance \nInspection (PRI)\nTrue / False\nFigure 3: The model architecture of KP-PLM framework. (Best viewed in color.)\nsame topic entity. To distinguish these sentences\nand filter irrelevant knowledge from the sub-graph,\nwe propose to further prune GS. The procedure\ncan be summarized as follows. We first traverse\nall the 2-hop relation paths in GS. For each 2-hop\nrelation path (eS,r1,e1,r2,e2), if the tail entity e2\nis not mentioned in the sentence, we remove e2\nand all the relations linked to it from GS. After\nthat, we further traverse all the 1-hop relation paths\nin the pruned sub-graph. For each 1-hop relation\npath (eS,r1,e1), if e1 is not mentioned in the sen-\ntence and there are not any 2-hop relation paths\nthat contain e1, we remove e1 and all the linked re-\nlations from the sub-graph. For example, as shown\nin Figure 2 (a), while the entity “Vaccine” is not\na mention in the sentence, we retain it due to its\ninclusion in the 2-hop relation path “(COVID-19\nVaccine, (is a, Vaccine), (function, Acquired Im-\nmunity))”. Finally, we take the generated pruned\nsub-graph as the contextual knowledge sub-graph\nˆGS for S.\nContinuous prompting mapping. After ˆGS is\nconstructed, we next extract semantic knowledge\nfrom it. While there are some methods (Yao et al.,\n2019; Sun et al., 2020) that directly convert rela-\ntion triples into discrete texts, it has been pointed\nout in (Liu et al., 2021b) that the optimization in\nthese methods could suffer from local minima. In-\nspired by (Liu et al., 2021b), we employ continu-\nous prompts to inject factual knowledge into PLMs,\nwhich have been shown to be effective in capturing\nsemantic knowledge (Gu et al., 2021).\nSpecifically, we design three types of prompt\nmapping rules based on the first-order and second-\norder structural information in ˆGS. As shown in\nFigure 2, the 1-hop 1-chain sub-structure is a sin-\ngle triple, the 2-hop 1-chain sub-structure denotes\na 2-hop relation path and the 2-hop 2-chain sub-\nstructure represents two 2-hop relation paths that\nshare the same prefix triple. These three types of\nsub-structures are the building blocks of other com-\nplex and higher-order ones, so they are adopted to\ndesign prompts. Our framework can also be easily\nincorporated with other sub-structures. For each\ntype of sub-structure, we design continuous tem-\nplates with knowledge trigger tokens [K], [/K]\nand pseudo tokens [Pi]. Examples of prompt tem-\nplates are given in Figure 2 (b)-(d).\nFinally, we concatenate all the derived prompts\nwith the original context to form an input sequence\nX = Concat(S,{Pi}m\ni=1), where Pi denotes the\ni-th prompt and mis the number of the prompts.\n3.3 Model Architecture\nAfter the input sequence X is generated, we first\nutilize the PLM tokenizer to transform X into a to-\nken sequence {xj}n\nj=1, where xj is the j-th token\nand nis the sequence length. For notation simplic-\nity, we overload X = {xj}n\nj=1. Then we feed X\ninto a PLM, whose architecture is shown in Fig-\nure 3. We next describe the two major components.\nInput Embedding Layer. Similar as previous\nmethods (Liu et al., 2020; Sun et al., 2020), there\nare three types of embeddings including token em-\nbeddings, position embeddings and segmentation\nembeddings. For token embeddings, the trigger\nand pseudo tokens are randomly initialized while\nothers are initialized by looking up the PLM em-\nbedding table. To alleviate the influence of the\nprompt order on the model performance, we use\n3167\nthe same segmentation id for all the prompts and\nset the same position id to their start tokens. For\nexample, in Figure 3, the segmentation ids of all\nthe prompts are set to 1 and the position ids of their\nstart token [K] are uniformly set to 18.\nDeep Transformer Encoder. Following (Devlin\net al., 2019; Liu et al., 2019), we use a multi-layer\ntransformer encoder (Vaswani et al., 2017) to cap-\nture the semantics from both context and knowl-\nedge prompts. To further mitigate the model’s sen-\nsitivity to the prompt order, we design a mask ma-\ntrix M for self-attention in PLMs, whose (i,j)-th\nentry characterizes the relation between two tokens\nxi,xj ∈X, and is formally defined as:\nMij =\n{ −inf xi ∈Pu ∧xj ∈Pv ∧u̸= v;\n0 otherwise.\n(1)\nThe self-attention in the transformer can then be\ncalculated as:\nAtt(Q,K,V) = Softmax\n(QKT\n√\nd\n+ M\n)\nV,\n(2)\nwhere d >0 is the scale factor and Q,K,V ∈\nRn×h are the query, key and value matrices, re-\nspectively. Here, Mij = −inf enforces that the\nattention weight between xi and xj from different\nprompts will be set to 0.\n3.4 Self-supervised Pre-training Tasks\nWe next provide the detailed description on two\nself-supervised pre-training tasks.\nPrompt Relevance Inspection (PRI). Since\nprompts can inject factual knowledge into PLMs,\nthey are expected to be semantically relevant to the\ncontext sequences. Therefore, we design a novel\nprompt relevance inspection task, which enhances\nthe model’s capability in learning the relevance of\na prompt to a sentence. For each sentence S in\nthe training corpus, the knowledge prompting pro-\ncess (see Section 3.2) can generate a set of relevant\nprompts PS = {Pi}m\ni=1 to S. We then construct a\n“positive” prompt set Pos by randomly selecting a\nprompt from PS for each sentence Sin the corpus.\nFurther, we need to construct a “negative” prompt\nset Neg. For each sentence S, we randomly se-\nlect a prompt P from PS and replace a randomly\nselected entity in P by an arbitrary entity in KBs.\nThe updated prompt is then labeled as a negative\nprompt and added to Neg. We repeat the above\nprocess if more negative samples are needed.\nAfter Pos and Neg are generated, we can con-\nstruct a training setD1 = {(P,yP)}, where yP = 1\nif P ∈Pos; 0, otherwise. For each sample (P,yP)\nfrom D1, inspired by SpanBERT (Joshi et al.,\n2020), we represent P by using its two boundary\ntokens ([K] and [/K]). Let x[K],x[/K] ∈Rh be\ninput embeddings, respectively. Formally, we have:\nhP = LN(σ((Fθ(x[K]) + Fθ(x[/K]))W1)),\n(3)\nwhere W1 ∈Rh×h is the trainable parameter ma-\ntrix, σ(·) and LN(·) are the Sigmoid and Layer-\nNorm functions, respectively. Further, Fθ(·) is the\noutput representation by the PLM and θ denotes\nthe model parameters. Based on hP, we define a\nbinary classifier whose objective function is:\nLPRI = E(P,yP )∼D1 [log PrΦ(yP|P)], (4)\nPrΦ(yP|P) = Softmax(H1(hP)), (5)\nwhere H1 is the classification head with parameter\nΦ and Pr(·) denotes the probability distribution.\nMasked Prompt Modeling (MPM). We further\npropose a masked prompt modeling task, which\naims to predict the masked entity in a prompt. Dif-\nferent from the vanilla masked language model-\ning (Devlin et al., 2019) that has an enormous\nsearch space over the whole PLM vocabulary V,\nour task shrinks the search space for a prompt\nP ∈PS to the entity set of the contextual knowl-\nedge sub-graph ˆGS.\nGiven a training corpus, for each context S,\nwe randomly mask an entity eP (except the topic\nentity3) with the [MASK] token in an arbitrarily\nselected prompt P ∈ PS. Then we can gener-\nate a dataset D2 = {(P,eP)}that consists of all\nthe (prompt, masked entity) pairs. To enforce the\nmodel to better learn factual knowledge expressed\nby the generated prompts, we employ contrastive\nlearning (Jean et al., 2015) techniques and formu-\nlate an objective function as:\nLMPM = E(P,eP )∼D2\n[\nexp(g(eP|P))\nexp(g(eP|P)) + NEe′\nP ∼q(e)\n[\nexp(g(e′\nP|P))\n]\n]\n,\n(6)\nwhere e′\nP ̸= eP is a negative entity,N is the total\nnumber of sampled negative entities4, and q(·) is a\n3The topic entity is fixed at the start position in all the\nrelation paths. We do not mask it because it is easy for PLMs\nto predict based on other knowledge prompts.\n4If |ES|< N+ 1, the remaining N −|ES|+ 1negative\nentities can be sampled from the whole E.\n3168\nsampling function implemented by the PageRank\nalgorithm (Gao et al., 2021b), which is used to\ncalculate the sampling probability scores for all the\nentities in ES. Further, g(·) is a scoring function\nthat measures the similarity between the masked\ntoken representation h[MASK] and the entity eP. In\nparticular, g(·) is expected to effectively learn the\nrepresentation of the masked entity eP by pulling\nembeddings of all its mentions in contexts together\nand pushing apart that of other negative entities.\nTherefore, we first introduce a verbalizer mapping\nfunction ˆf(eP), which maps eP to the union of all\nits mention lists in the contexts of the corpus. Then\nfollowing (Page et al., 1998), we compute:\ng(eP|P) = Ev∼ˆf(eP )\n[\n(h[MASK])Txv\n]\n−log q(eP)\n(7)\nh[MASK] = LN(σ(Fθ(x[MASK])W2)). (8)\nHere, x[MASK] ∈Rh is the input embedding vector\nof the masked token and W2 ∈Rh×h is a trainable\nweight matrix. Further, xv denotes the embedding\nof mention vfrom ˆf(eP), which is calculated by\naveraging embeddings of all tokens included in v.\nFinally, the total loss can be computed as:\nL= LMLM + λLPRI + µLMPM, (9)\nwhere LMLM denotes the vanilla MLM objective in\nPLMs, λ,µ ∈[0,1] are the balancing coefficients.\n4 Experiments\nIn this section, we comprehensively evaluate the ef-\nfectiveness of KP-PLM. We also conduct the hyper-\nparameter analysis in Appendix B.3.\n4.1 Implementation Details\nFollowing previous works (Sun et al., 2020; Zhang\net al., 2021), the pre-training data is collected from\nWikipedia Dumps (2020/03/01)5 and consists of\n25,933,196 sentences. Further, the KB used is\nWikiData5M (Wang et al., 2021b), which includes\n3,085,345 entities and 822 relation types.\nFor the baselines, we select six knowledge-\nenhanced PLMs: 1) ERNIE-THU (Zhang et al.,\n2019) integrates knowledge embeddings with\naligned mentions. 2) KnowBERT (Peters et al.,\n2019) utilizes the attention mechanism to realize\nknowledge fusion. 3) KEPLER (Wang et al.,\n2021b) introduces a novel knowledge embedding\n5https://dumps.wikimedia.org/enwiki/.\nModels P R F1\nUFET 77.4 60.6 68.0\nBERT 76.4 71.0 73.6\nRoBERTa 77.4 73.6 75.4\nERNIEBERT 78.4 72.9 75.6\nERNIERoBERTa 80.3 70.2 74.9\nKnowBERTBERT 77.9 71.2 74.4\nKnowBERTRoBERTa 78.7 72.7 75.6\nKEPLERWiKi 77.8 74.6 76.2\nCoLAKE 77.0 75.7 76.4\nDKPLM 79.2 75.9 77.5\nKP-PLM 80.8 75.1 77.8\nKP-PLMKNOW 80.5 76.1 78.2\nTable 1: The results (%) on Open Entity.\nMethods TACRED FewRel\nP R F1 P R F1\nCNN 70.3 54.2 61.2 - - -\nPA-LSTM 65.7 64.5 65.1 - - -\nC-GCN 69.90 63.3 66.4 - - -\nBERT 67.2 64.8 66.0 84.9 85.1 85.0\nRoBERTa 70.0 69.6 70.2 85.4 85.4 85.3\nERNIEBERT 70.0 66.1 68.1 88.5 88.4 88.3\nKnowBERT 71.6 71.5 71.5 - - -\nCoLAKE 72.8 73.5 73.1 90.6 90.6 90.6\nDKPLM 72.6 73.5 73.0 - - -\nKP-PLM 72.6 73.7 73.3 87.6 87.4 87.5\nKP-PLMKNOW 73.3 73.9 73.5 88.9 88.9 88.8\nTable 2: The results (%) on TACRED and FewRel.\nloss for capturing knowledge. 4) CoLAKE (Sun\net al., 2020) constructs a unified graph from context\nand knowledge base. 5) K-Adapter (Wang et al.,\n2021a) learns representations for different kinds of\nknowledge via neural adapters. 6)DKPLM (Zhang\net al., 2021) aims at injecting long-tailed entities.\nIn the pre-training stage, we choose RoBERTa-\nbase (Liu et al., 2019) from HuggingFace6 as the\ndefault PLM. Our framework can also be easily\ncombined with other PLMs, such as BERT (Pörner\net al., 2019) and DeBERTa (He et al., 2021b). In\nthe fine-tuning stage (if have), we fine-tune each\ntask with only task-specific data. Further, we intro-\nduce a model variant KP-PLMKNOW , which em-\nploys knowledge prompts in the fine-tuning stage\nby directly concatenating them with each exam-\nple. More implementation details about the pre-\ntraining and fine-tuning stages are provided in Ap-\npendices A and B, respectively.\n6https://huggingface.co/transformers.\n3169\nDatasets ELMo BERT RoBERTa CoLAKE K-Adapter∗ KEPLER DKPLM KP-PLM\nGoogle-RE 2.2 11.4 5.3 9.5 7.0 7.3 10.8 11.0\nUHN-Google-RE 2.3 5.7 2.2 4.9 3.7 4.1 5.4 5.6\nT-REx 0.2 32.5 24.7 28.8 29.1 24.6 32.0 32.3\nUHN-T-REx 0.2 23.3 17.0 20.4 23.0 17.1 22.9 22.5\nTable 3: The performance P@1 (%) of knowledge probing. Besides, K-Adapter∗ is based on RoBERTa-large and\nuses a subset of T-REx as its training data, which may contribute to its superiority over other methods.\nParadigmsMethods\nSingle-Sentence Natural Language Understanding Tasks\nAvg.SST-2 SST-5 MR CR MPQA Subj TREC CoLA\n(acc) (acc) (acc) (acc) (acc) (acc) (acc) (matt.)\nPT-Zero RoBERTa82.57 29.46 65.10 82.15 49.90 69.20 20.80 -4.89 49.29\nKP-PLM84.15 30.67 64.15 81.60 53.80 68.70 24.80 -2.99 50.61\nPT-Few RoBERTa86.35±1.3 36.79±2.0 83.35±0.9 88.85±1.4 66.40±1.9 89.25±2.6 76.80±5.0 6.61±6.9 66.80\nKP-PLM90.71±1.0 44.21±2.9 82.00±1.5 85.35±0.4 67.30±1.2 91.45±0.4 81.00±3.3 24.28±11.3 70.79\nFT-Full RoBERTa94.90 56.90 89.60 88.80 86.30 96.50 97.10 63.90 84.25\nKP-PLM95.30 57.63 89.20 89.10 87.40 96.20 97.10 64.87 84.60\nParadigmsMethods\nSentence-Pair Natural Language Understanding Tasks\nAvg.MNLI MNLI-mm SNLI QNLI RTE MRPC QQP STS-B\n(acc) (acc) (acc) (acc) (acc) (f1) (f1) (pear.)\nPT-Zero RoBERTa38.92 39.09 36.33 47.08 55.96 54.87 45.72 -4.10 39.23\nKP-PLM40.10 43.53 36.23 46.87 58.29 58.97 46.30 -1.30 41.12\nPT-Few RoBERTa50.87±1.8 53.01±2.2 64.96±1.9 60.08±2.4 63.54±4.0 78.57±3.7 45.72±3.4 52.26±7.0 58.63\nKP-PLM55.50±1.1 57.56±2.1 66.33±1.9 58.75±2.2 66.25±3.8 79.78±3.1 62.60±4.0 68.79±4.2 64.45\nFT-Full RoBERTa87.33 87.01 92.10 92.58 77.32 90.23 92.16 91.12 88.73\nKP-PLM87.99 87.32 92.01 93.04 79.48 91.81 92.42 91.60 89.46\nTable 4: The comparison between KP-PLM and RoBERTa-base (Liu et al., 2019) over multiple natural language\nunderstanding (NLU) tasks in terms of acc/f1/matt./pear. (%) and standard deviation with three paradigms, such as\nzero-shot prompt-tuning (PT-Zero), few-shot prompt-tuning (PT-Few) and full-data fine-tuning (FT-Full).\n4.2 Knowledge-aware Tasks\nIn the fine-tuning stage, we use three tasks: entity\ntyping, relation extraction and knowledge probing,\nto evaluate the model performance.\nEntity Typing. Given a sentence and a corre-\nsponding entity mention, the task is to predict the\ntype of the mention. We choose precision (P), re-\ncall (R) and F1 score as the evaluation metrics.\nFor all these metrics, the larger the value, the bet-\nter the model performance. For fairness, we fol-\nlow the same training settings as in (Zhang et al.,\n2021) and use the Open Entity (Choi et al., 2018)\ndataset. To show the effectiveness and competi-\ntiveness of our pre-trained model, we also choose\nUFET (Choi et al., 2018), BERT (Pörner et al.,\n2019) and RoBERTa (Liu et al., 2019) as traditional\nbaselines for this task.\nThe results are summarized in Table 1. From\nthe table, we see that knowledge-enhanced PLMs\ngenerally perform better than traditional methods.\nIn particular, our methods can achieve the best re-\nsults w.r.t. all the metrics. For example, the F1\nscore of KP-PLMKNOW is 78.2%, which improves\nthat of the runner-up by 0.7%. Further, we also\nfind that both KP-PLM and KP-PLM KNOW out-\nperform RoBERTa. This indicates that injecting\nknowledge prompts into both pre-training and fine-\ntuning stages are useful for improving the model\nperformance on this task.\nRelation Extraction. It aims to classify the rela-\ntion between two given entities based on the cor-\nresponding texts. We follow (Sun et al., 2020) to\nchoose two widely used tasks: TACRED (Zhang\net al., 2017) and FewRel (Han et al., 2018). Sim-\nilar as in entity typing, we take precision (P), re-\ncall (R) and F1 as the evaluation metrics. We also\ncompare our models with five traditional models\nincluding CNN, PA-LSTM (Zhang et al., 2017),\nC-GCN (Zhang and Qi, 2018), BERT (Pörner et al.,\n2019) and RoBERTa (Liu et al., 2019).\nAs shown in Table 2, most knowledge-enhanced\nPLMs outperform traditional methods by a large\n3170\nKnowledge Paradigms PLM\nFull-data Fine-tuning\nOpen Entity TACRED FewRel SST-2 CoLA QQP\n(f1) (f1) (f1) (acc) (matt.) (f1)\nNone\nBERT 73.6 66.0 85.0 93.5 52.1 71.2\nRoBERTa 75.4 70.2 85.3 94.9 63.9 92.2\nDeBERTa 76.5 72.1 87.0 95.1 64.9 89.3\nKnowledge Masking\nBERT 74.4 70.5 85.8 93.9 51.6 71.4\nRoBERTa 75.8 71.3 86.0 95.0 64.3 92.0\nDeBERTa 77.0 72.6 86.8 94.7 65.0 90.3\nKnowledge Fusion†\nBERT 75.6 68.1 88.3 93.5 52.3 71.2\nRoBERTa 74.9 68.4 88.4 93.9 63.6 91.5\nDeBERTa 76.8 70.6 88.8 94.2 65.7 89.9\nGraph Learning‡\nBERT 75.1 72.9 88.4 94.2 53.9 72.0\nRoBERTa 76.4 73.1 90.6 94.6 63.4 93.3\nDeBERTa 77.1 72.8 89.5 94.0 66.1 92.2\nKnowledge Prompting\nBERT 76.0 72.2 87.1 94.6 57.3 75.8\nRoBERTa 77.8 73.3 87.5 95.3 64.9 92.4\nDeBERTa 77.7 73.5 88.0 95.6 66.3 92.2\nTable 5: The comparison between KP-PLM and other knowledge-enhanced paradigms on different base PLMs. For\neach base PLM, we highlight the largest score in bold.\nmargin. This shows the necessity of external knowl-\nedge incorporation for PLMs. For the TACRED\ntask, our models perform the best. For the FewRel\ntask, while CoLAKE obtains the best results, KP-\nPLMKNOW can improve the F1 score over ERNIE\nand RoBERTa by 0.5% and 3.5%, respectively. In\naddition, the outperformance of KP-PLM KNOW\nover KP-PLM shows the importance of injecting\nknowledge prompts in the fine-tuning stage.\nKnowledge Probing. Knowledge probing aims\nto evaluate whether the PLM possesses the intrin-\nsic factual knowledge in zero-shot settings. We\ncompare all the methods w.r.t. P@1. We select\nLAMA (Petroni et al., 2019) and the advanced ver-\nsion LAMA-UHN (Pörner et al., 2019) tasks with\nfour datasets, including Google-RE, UHN-Google-\nRE, T-REx, and UHN-T-REx.\nFrom Table 3, our model KP-PLM beats other\nknowledge-enhanced PLMs on Google-RE, UHN-\nGoogle-RE and T-REx datasets. For UHN-T-REx,\nKP-PLM performs comparably with the winner\nand improve RoBERTa by 5.5% in terms of P@1.\nIn addition, we see that BERT achieves the best\nperformance over multiple tasks. This is because\nit uses a small vocabulary set, which has also been\npointed out in (Sun et al., 2020).\n4.3 Performance on General NLU Tasks\nWe further investigate whether knowledge prompt-\ning can consistently improve the PLM performance\nin both full-resource and low-resource learning. We\nfollow (Gao et al., 2021a) to select 15 widely used\nNLU tasks. We consider three training settings,\nincluding zero-shot prompt-tuning (PT-Zero), few-\nshot prompt-tuning (PT-Few), and full data fine-\ntuning (FT-Full). Details on these datasets and\ntraining procedures are provided in Appendix B.2.\nFrom Table 4, we make the following observa-\ntions: 1) KP-PLM achieves the best overall results\nover all the training settings. The much larger mar-\ngins of KP-PLM over others in PT-Zero and PT-\nFew show that continual pre-training by knowledge\nprompting indeed improves the performance when\nusing the prompt-based fine-tuning technique. 2)\nIn the low-resource scenarios, the performances of\nboth RoBERTa and KP-PLM on single-sentence\ntasks are better than sentence-pair tasks, which in-\ndicates that the sentence-pair task is more difficult\nin the few-shot settings. 3) We also find KP-PLM\nsometimes performs worse than RoBERTa on MR\nand CR datasets. We conjecture that this is because\nthese tasks are sensitive to external knowledge.\n4.4 Knowledge Prompting Study\nWe end this section with a further comparison be-\ntween knowledge prompting and other knowledge-\nenhanced paradigms including knowledge masking,\nknowledge fusion and graph learning. For each\nparadigm, we employ three PLMs: BERT-base,\nRoBERTa-base and DeBERTa-base. For knowl-\nedge masking, we mask all the entity mentions in\neach training sentence, and train the model by the\nvanilla MLM objective. For knowledge fusion and\ngraph learning, we directly run the source codes\n3171\nModels Open Entity TACRED FewRel\nKP-PLM 77.8 73.3 87.5\nw/o. PRI 77.5 72.8 87.1\nw/o. MPM 77.4 72.5 86.8\nw/o. PRI & MPM 77.2 72.4 86.6\nw/o. cont. 77.7 73.1 87.3\nw/o. mask. 77.5 72.9 87.1\nw/o. all 76.3 70.8 85.9\nTable 6: The ablation study results (F1 score %). w/o.\nPRI removes the PRI task, w/o. MPM removes the\nMPM task, w/o. PRI & MPM removes both the PRI and\nMPM tasks, w/o. cont. removes all continuous pseudo\ntokens, w/o. mask. replaces the mask matrix in Eq. 1\nwith the default in RoBERTa, and w/o. all denotes to\nremove all the proposed techniques.\nof ERNIE and CoLAKE, respectively. The results\nare summarized in Table 5. From the table, we\nsee that, compared with other knowledge-enhanced\nparadigms, for each base PLM, knowledge prompt-\ning can lead to largest performance gains in most\ndownstream tasks. This further verifies the effec-\ntiveness of knowledge prompting in boosting the\nperformance of existing PLMs.\n4.5 Ablation Study\nWe conduct an ablation study to investigate the\ncharacteristics of main components in KP-PLM, in-\ncluding prompt relevance inspection (PRI), masked\nprompt modeling (MPM), continuous pseudo to-\nkens in the prompt and the mask matrix for self-\nattention in PLMs (Eq. 1). Table 6 reports the F1\nscores on the Open Entity, TACRED, and FewRel\ntasks. From the table, we observe that the removal\nof each component leads to the performance drop\nof KP-PLM, which shows the importance of all\nthese components in KP-PLM.\n5 Conclusion\nIn this paper, we presented a seminal knowledge\nprompting paradigm, based on which a novel\nknowledge-prompting-based PLM framework KP-\nPLM was proposed. We constructed contextual\nknowledge sub-graphs for contexts and employed\ncontinuous prompting mapping to generate knowl-\nedge prompts. After that, we designed two self-\nsupervised pre-training tasks to learn semantic\nknowledge from prompts. Finally, we conducted\nextensive experiments to evaluate the model per-\nformance. Experimental results validate the effec-\ntiveness of knowledge prompting in boosting the\nperformance of PLMs.\nLimitations\nWe have listed some limitations: 1) In the exper-\niments, we follow most previous works to only\nfocus on general natural language understanding\n(NLU) tasks. We do not evaluate the performance\nover some question answering tasks (e.g. SQuAD,\nHotpotQA, etc.), we let it as the future research. 2)\nOur work focuses on the PLM without any trans-\nformer decoders. We think it is possible to extend\nour method in natural language generation (NLG)\ntasks.\nEthical Considerations\nOur contribution in this work is fully methodolog-\nical, namely a knowledge-prompting-based pre-\ntrained language model (KP-PLM) to boost the per-\nformance of PLMs with factual knowledge. Hence,\nthere is no explicit negative social influences in this\nwork. However, transformer-based models may\nhave some negative impacts, such as gender and\nsocial bias. Our work would unavoidably suffer\nfrom these issues. We suggest that users should\ncarefully address potential risks when the KP-PLM\nmodels are deployed online.\nAcknowledgement\nXiang Li would like to acknowledge the support\nfrom Shanghai Pujiang Talent Program (Project\nNo. 21PJ1402900). This work has also been sup-\nported by the National Natural Science Foundation\nof China under Grant No. U1911203, Alibaba\nGroup through the Alibaba Innovation Research\nProgram, and the National Natural Science Founda-\ntion of China under Grant No. 61877018, The Re-\nsearch Project of Shanghai Science and Technology\nCommission (20dz2260300) and The Fundamental\nResearch Funds for the Central Universities.\nReferences\nSimran Arora, Sen Wu, Enci Liu, and Christopher\nRé. 2022. Metadata shaping: A simple approach\nfor knowledge-enhanced language models. In ACL,\npages 1733–1745.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn EMNLP, pages 632–642.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\n3172\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In\nNeurIPS.\nQianglong Chen, Feng-Lin Li, Guohai Xu, Ming Yan,\nJi Zhang, and Yin Zhang. 2022. Dictbert: Dictionary\ndescription knowledge enhanced language model pre-\ntraining via contrastive learning. In IJCAI, pages\n4086–4092.\nEunsol Choi, Omer Levy, Yejin Choi, and Luke Zettle-\nmoyer. 2018. Ultra-fine entity typing. In Proceed-\nings of the 56th Annual Meeting of the Association for\nComputational Linguistics, ACL 2018, Melbourne,\nAustralia, July 15-20, 2018, Volume 1: Long Papers,\npages 87–96.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: pre-training of\ndeep bidirectional transformers for language under-\nstanding. In NAACL, pages 4171–4186.\nPaolo Ferragina and Ugo Scaiella. 2010. TAGME:\non-the-fly annotation of short text fragments (by\nwikipedia entities). In CIKM, pages 1625–1628.\nTianyu Gao, Adam Fisch, and Danqi Chen. 2021a.\nMaking pre-trained language models better few-shot\nlearners. In ACL, pages 3816–3830.\nTianyu Gao, Xingcheng Yao, and Danqi Chen. 2021b.\nSimcse: Simple contrastive learning of sentence em-\nbeddings. In EMNLP, pages 6894–6910.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2021. PPT: pre-trained prompt tuning for few-shot\nlearning. CoRR, abs/2109.04332.\nXu Han, Weilin Zhao, Ning Ding, Zhiyuan Liu, and\nMaosong Sun. 2021. PTR: prompt tuning with rules\nfor text classification. CoRR, abs/2105.11259.\nXu Han, Hao Zhu, Pengfei Yu, Ziyun Wang, Yuan Yao,\nZhiyuan Liu, and Maosong Sun. 2018. Fewrel: A\nlarge-scale supervised few-shot relation classifica-\ntion dataset with state-of-the-art evaluation. arXiv\npreprint arXiv:1810.10147.\nLei He, Suncong Zheng, Tao Yang, and Feng Zhang.\n2021a. Klmo: Knowledge graph enhanced pretrained\nlanguage model with fine-grained relationships. In\nEMNLP, pages 4536–4542.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021b. Deberta: decoding-enhanced\nbert with disentangled attention. In ICLR.\nShengding Hu, Ning Ding, Huadong Wang, Zhiyuan\nLiu, Juanzi Li, and Maosong Sun. 2021. Knowl-\nedgeable prompt-tuning: Incorporating knowledge\ninto prompt verbalizer for text classification. CoRR,\nabs/2108.02035.\nRobert L. Logan IV , Nelson F. Liu, Matthew E. Peters,\nMatt Gardner, and Sameer Singh. 2019. Barack’s\nwife hillary: Using knowledge graphs for fact-aware\nlanguage modeling. In ACL, pages 5962–5971.\nSébastien Jean, KyungHyun Cho, Roland Memisevic,\nand Yoshua Bengio. 2015. On using very large target\nvocabulary for neural machine translation. In ACL,\npages 1–10.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S. Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Trans. Assoc. Comput. Linguistics, 8:64–\n77.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In ICLR.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2020. ALBERT: A lite BERT for self-supervised\nlearning of language representations. In ICLR.\nWeijie Liu, Peng Zhou, Zhe Zhao, Zhiruo Wang, Qi Ju,\nHaotang Deng, and Ping Wang. 2020. K-BERT: en-\nabling language representation with knowledge graph.\nIn AAAI, pages 2901–2908.\nXiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin\nYang, and Jie Tang. 2021a. P-tuning v2: Prompt\ntuning can be comparable to fine-tuning universally\nacross scales and tasks. CoRR.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding,\nYujie Qian, Zhilin Yang, and Jie Tang. 2021b. GPT\nunderstands, too. CoRR, abs/2103.10385.\nYe Liu, Yao Wan, Lifang He, Hao Peng, and Philip S.\nYu. 2021c. KG-BART: knowledge graph-augmented\nBART for generative commonsense reasoning. In\nAAAI, pages 6418–6425.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nMike Mintz, Steven Bills, Rion Snow, and Daniel Juraf-\nsky. 2009. Distant supervision for relation extraction\nwithout labeled data. In ACL, pages 1003–1011.\nPage, Lawrence, Brin, Sergey, and Terry. 1998. Page, l.,\net al.: The pagerank citation ranking: Bringing order\nto the web. stanford digital libraries working paper.\nMatthew E. Peters, Mark Neumann, Robert L. Logan\nIV , Roy Schwartz, Vidur Joshi, Sameer Singh, and\nNoah A. Smith. 2019. Knowledge enhanced contex-\ntual word representations. In EMNLP, pages 43–54.\n3173\nFabio Petroni, Tim Rocktäschel, Sebastian Riedel,\nPatrick S. H. Lewis, Anton Bakhtin, Yuxiang Wu,\nand Alexander H. Miller. 2019. Language models\nas knowledge bases? In EMNLP, pages 2463–2473.\nAssociation for Computational Linguistics.\nNina Pörner, Ulli Waltinger, and Hinrich Schütze. 2019.\nBERT is not a knowledge base (yet): Factual knowl-\nedge vs. name-based reasoning in unsupervised QA.\nCoRR, abs/1911.03681.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nTimo Schick and Hinrich Schütze. 2021. Exploiting\ncloze-questions for few-shot text classification and\nnatural language inference. In EACL, pages 255–269.\nTaylor Shin, Yasaman Razeghi, Robert L. Logan IV ,\nEric Wallace, and Sameer Singh. 2020. Autoprompt:\nEliciting knowledge from language models with au-\ntomatically generated prompts. In EMNLP, pages\n4222–4235.\nYusheng Su, Xu Han, Zhengyan Zhang, Yankai Lin,\nPeng Li, Zhiyuan Liu, Jie Zhou, and Maosong Sun.\n2021. Cokebert: Contextual knowledge selection and\nembedding towards enhanced pre-trained language\nmodels. AI Open, 2:127–134.\nTianxiang Sun, Yunfan Shao, Xipeng Qiu, Qipeng Guo,\nYaru Hu, Xuanjing Huang, and Zheng Zhang. 2020.\nColake: Contextualized language and knowledge em-\nbedding. In COLING, pages 3660–3670.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng,\nXuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu,\nHao Tian, and Hua Wu. 2019. ERNIE: enhanced\nrepresentation through knowledge integration. CoRR,\nabs/1904.09223.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS, pages 5998–6008.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R. Bowman. 2019.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. In ICLR.\nRuize Wang, Duyu Tang, Nan Duan, Zhongyu Wei,\nXuanjing Huang, Jianshu Ji, Guihong Cao, Daxin\nJiang, and Ming Zhou. 2021a. K-adapter: Infusing\nknowledge into pre-trained models with adapters. In\nACL, pages 1405–1418.\nXiaozhi Wang, Tianyu Gao, Zhaocheng Zhu, Zhengyan\nZhang, Zhiyuan Liu, Juanzi Li, and Jian Tang. 2021b.\nKEPLER: A unified model for knowledge embed-\nding and pre-trained language representation. TACL,\n9:176–194.\nWenhan Xiong, Jingfei Du, William Yang Wang, and\nVeselin Stoyanov. 2020. Pretrained encyclopedia:\nWeakly supervised knowledge-pretrained language\nmodel. In ICLR.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS, pages 5754–5764.\nLiang Yao, Chengsheng Mao, and Yuan Luo. 2019.\nKG-BERT: BERT for knowledge graph completion.\nCoRR, abs/1909.03193.\nHongbin Ye, Ningyu Zhang, Shumin Deng, Xiang Chen,\nHui Chen, Feiyu Xiong, Xi Chen, and Huajun Chen.\n2022. Ontology-enhanced prompt-tuning for few-\nshot learning. In WWW2022, pages 778–787.\nDonghan Yu, Chenguang Zhu, Yiming Yang, and\nMichael Zeng. 2022a. JAKET: joint pre-training\nof knowledge graph and language understanding. In\nAAAI, pages 11630–11638.\nWenhao Yu, Chenguang Zhu, Yuwei Fang, Donghan Yu,\nShuohang Wang, Yichong Xu, Michael Zeng, and\nMeng Jiang. 2022b. Dict-bert: Enhancing language\nmodel pre-training with dictionary. In ACL, pages\n1907–1918.\nTaolin Zhang, Chengyu Wang, Nan Hu, Minghui Qiu,\nChengguang Tang, Xiaofeng He, and Jun Huang.\n2021. DKPLM: decomposable knowledge-enhanced\npre-trained language model for natural language un-\nderstanding. CoRR, abs/2112.01047.\nYuhao Zhang and Peng Qi. 2018. Graph convolution\nover pruned dependency trees improves relation ex-\ntraction. In EMNLP, pages 2205–2215.\nYuhao Zhang, Victor Zhong, Danqi Chen, Gabor Angeli,\nand Christopher D. Manning. 2017. Position-aware\nattention and supervised data improve slot filling. In\nEMNLP, pages 35–45.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. ERNIE: enhanced\nlanguage representation with informative entities. In\nACL, pages 1441–1451.\nA Details of the Pre-training\nA.1 Training Corpus and Knowledge Base\nWe follow (Liu et al., 2020; Sun et al., 2020;\nZhang et al., 2021) to collect training corpora from\nWikipedia (2020/03/01)7, and use WikiExtractor8\nto process the training data. The knowledge base\n(KB) Gwe choose is WikiData5M (Wang et al.,\n2021b), which is an urge-large structure data source\nbased on Wikipedia. The entity linking toolkit and\n7https://dumps.wikimedia.org/enwiki/.\n8https://github.com/attardi/\nwikiextractor.\n3174\nthe verbalizer we used are both the TAGME9 (Fer-\nragina and Scaiella, 2010), which can be viewed\nas a well-designed mapping function between KB\nentities set Eand the PLM vocabulary set V. In\ntotal, we have 3,085,345 entities and 822 relation\ntypes in G, and 25,933,196 training sentences. For\neach sentence, we construct the contextual knowl-\nedge sub-graph and generate continuous knowl-\nedge prompts offline by the process described in\nSection 3.2. In average, the number of the entities\nand knowledge prompts for each sentence are 15\nand 8, respectively.\nAs mentioned above, our framework consists of\nthree main training objectives. For the Masked Lan-\nguage Modeling (MLM) task, we follow (Devlin\net al., 2019; Liu et al., 2019) to randomly select\n15% tokens only in the context. For the selected\ntokens, 80% of them are replaced with [MASK] to-\nken, and 10% of them are replaced with randomly\nsampled tokens from the whole vocabulary set. For\nthe Prompt Relevance Inspection (PRI) task, we\nonly select one prompt from each input example to\nmake the decision. For the Masked Prompt Mod-\neling (MPM) task, we also only select one prompt\n(not equal to the selected prompt in PRI) and mask\none entity.\nA.2 Pre-training Implement Details\nIn the pre-training stage, we choose RoBERTa-\nbase (Liu et al., 2019) from the HuggingFace 10\nas the default underlying PLM. We train our model\nby AdamW algorithm with β1 = 0.9,β2 = 0.98.\nThe learning rate is set as 1e-5 with a warm up rate\n0.1. The best balance coefficients we found are\nλ = µ = 0.5. The number of negative entities is\nN = 10. Especially, if the number of the entities in\nthe current sub-graph |ES|<N + 1, the remaining\nN−|ES|+1 negative entities can be sampled from\nthe whole entities set Efrom KB G. We also lever-\nage dropout and regularization strategies to avoid\nover-fitting. The model is trained on 8 V100-32G\nGPUs for 2.5 days with a total batch size of 768.\nB Details of the Task-specific Fine-tuning\nWe choose multiple natural language processing\n(NLP) tasks for the model evaluations, including\nknowledge-aware tasks and general natural lan-\nguage understanding (NLU) tasks. In this section,\n9https://sobigdata.d4science.org/\ngroup/tagme.\n10https://huggingface.co/transformers/\nindex.html.\nDatasets #Train #Develop #Test #Class\nOpen Entity 2,000 2,000 2,000 6\nTACRED 68,124 22,631 15,509 42\nFewRel 8,000 8,000 16,000 80\nTable 7: The statistics of Open Entity, TACRED and\nFewRel.\nwe provide the details of the dataset information\nand fine-tuning procedures for each task.\nB.1 Knowledge-aware Tasks\nWe select three knowledge-aware tasks, such as\nentity typing, relation extraction, and knowledge\nprobing. The dataset statistics can be found in\nTable 7.\nEntity typing. Entity typing requires to classify\nthe designated entity mention based on the given\nsentence. We select the Open Entity dataset for\nevaluation. It consists of 6 entity types, and 2,000\nsentences for training, developing, and testing data,\nrespectively.\nThe variant method KP-PLM KNOW means\nadding injected knowledge prompts for each data.\nIn this manner, the topic entity is the designated\nentity mention, we follow the proposed knowl-\nedge prompting procedure to obtain the knowledge\nprompts for each sentence, and directly concatenate\nthem with the original sentence. In the fine-tuning\nstage, we do not use knowledge-aware pre-training\nobjectives.\nDuring fine-tuning, we follow (Sun et al., 2020)\nto add two special tokens [ENT] and [/ENT]\nbefore and after the entity mention. We directly\nconcatenate their representations at the top of the\nPLM and use them for classification. The max\nsequence length we set is 128, the learning rate\nis set as 2e-5 with a warm up rate of 0.1, and the\nbatch size is set to 16. Generally, we can obtain the\nbest performance after 5 epochs.\nRelation Extraction. Relation extraction is one\nof the significant tasks for information retrieval.\nIt aims to classify the relation class between two\ngiven entities based on the corresponding aligned\nsentence. We select two widely used datasets, i.e.\nTACRED (Zhang et al., 2017) and FewRel (Han\net al., 2018). TACRED has 42 relation types\nand 106, 264 sentences in total. FewRel is con-\nstructed by distant supervised (Mintz et al., 2009)\nand denoised by human annotations. It consists of\n320,000 sentences and 80 relations.\nFor the variant KP-PLMKNOW , following the\n3175\nCategory Dataset #Class #Train #Test Type Labels (classification tasks)\nSST-2 2 6,920 872 sentiment positive, negative\nSST-5 5 8,544 2,210 sentiment v. pos., positive, neutral, negative, v. neg.\nMR 2 8,662 2,000 sentiment positive, negative\nsingle- CR 2 1,775 2,000 sentiment positive, negative\nsentence MPQA 2 8,606 2,000 opinion polarity positive, negative\nSubj 2 8,000 2,000 subjectivity subjective, objective\nTREC 6 5,452 500 question cls. abbr., entity, description, human, loc., num.\nCoLA 2 8,551 1,042 acceptability grammatical, not_grammatical\nMNLI 3 392,702 9,815 NLI entailment, neutral, contradiction\nSNLI 3 549,367 9,842 NLI entailment, neutral, contradiction\nsentence- QNLI 2 104,743 5,463 NLI entailment, not_entailment\npair RTE 2 2,490 277 NLI entailment, not_entailment\nMRPC 2 3,668 408 paraphrase equivalent, not_equivalent\nQQP 2 363,846 40,431 paraphrase equivalent, not_equivalent\nSTS-B R 5,749 1,500 sent. similarity -\nTable 8: The statistics of multiple NLU datasets. STS-B is a real-valued regression task over the interval [0,5]).\nSince the original test data is invisible, we use the develop set as our test set.\nprocedure of knowledge prompting, we construct\na knowledge sub-graph for each sentence. Since\neach sentence consists of two entities, both of them\ncan be viewed as the topic entity. Hence, we obtain\ntwo sub-graphs for each sentence and then merge\ntwo sub-graphs by removing the duplicate relation\npaths. After that, we construct multiple prompts\nand concatenate them with the original sentence.\nDuring fine-tuning, we follow (Sun et al., 2020)\nto add four special tokens [HD], [/HD], [TL]\nand [/TL] to identity the head entity and tail en-\ntity in the given sentence. We obtain the represen-\ntations of [HD] and [TL] from the last layer of\nthe PLM, and concatenate them to perform classifi-\ncation. For TACRED (Zhang et al., 2017) dataset,\nthe max sequence length is 256, the learning rate\nis 3e-5 with a warm up rate 0.1, the batch size\nwe set is 32. We run experiment for 5 epochs. For\nFewRel (Han et al., 2018), the max sequence length\nis 256, the learning rate is 2e-5 with a warm up rate\n0.1, the batch size we set is 32. We run experiment\nfor 8 epochs. Notice, we do not use the original N-\nway K-shot settings for FewRel (Han et al., 2018).\nKnowledge Probing. Knowledge probing is a chal-\nlenging task to evaluate whether the PLM grasps\nfactual knowledge. The task requires no training\nstages which belongs to the zero-shot setting. We\nselect LAMA (Peters et al., 2019) and LAMA-\nUHN (Pörner et al., 2019) tasks with four datasets,\ni.e., Google-RE, UHN-Google-RE, T-REx, and\nUHN-T-REx. The dataset statistics can be found in\nthe original paper in (Pörner et al., 2019).\nDuring the evaluation, we follow (Zhang et al.,\n2019) to directly use the given designed template.\nSpecifically, given a head entity, a relation, and\na template with [MASK] token, we feed the in-\nput sequence into the PLM and obtain the pre-\ndicted result at the masked position. For example in\nGoogle-RE, a manually defined template of relation\n“place_of_birth” is “[S] was born in [MASK].”,\nwe can denote [S] as the head entity “Obama”,\nand let the PLM generate the answer “US.” at the\nposition of [MASK].\nB.2 General Natural Language\nUnderstanding Tasks\nWe select NLU tasks for the evaluation in both\nfull-resource and low-resource scenarios. We fol-\nlow (Gao et al., 2021a) to select 15 tasks include 8\ntasks from GLUE benchmark (Wang et al., 2019),\nSNLI (Bowman et al., 2015), and 6 other sentence-\nlevel classification tasks. The statistics of each\ndataset are shown in Table 8.\nAs mentioned above, we provide three training\nsettings, including zero-shot prompt-tuning (PT-\nZero), few-shot prompt-tuning (PT-Few) and full-\ndata fine-tuning (FT-Full). We summarize the detail\nsettings in the following.\nPrompt-tuning. For the settings of PT-Zero and\nPT-Few, we follow Gao et al. (2021a) to evaluate\nour framework based on the prompt-tuning method.\nSpecifically, for each task, we directly use the au-\ntomatically generated discrete template and ver-\nbalizer, where the discrete template denotes the\n3176\n0.1 0.3 0.5 0.7 0.9\n70.0\n71.5\n73.0\n74.5\n76.0\n77.5F1 score (%)\n Open Entity\nTACRED\n0.1 0.3 0.5 0.7 0.9\n70.0\n71.5\n73.0\n74.5\n76.0\n77.5F1 score (%)\n Open Entity\nTACRED\nFigure 4: Hyper-parameter efficiency of λand µover\nOpen Entity and TACRED.\n5 10 15 20 25 30\nN\n71.0\n72.5\n74.0\n75.5\n77.0\n78.5F1 score (%)\nOpen Entity\nTACRED\nFigure 5: Hyper-parameter efficiency of N over Open\nEntity and TACRED.\ntoken sequence with a single [MASK] token, and\nthe verbalizer is the label mapping function that\ncan map generated result token to the label. The\ntemplate and verbalizer for each task we used can\nbe found in Table E.1 in the original paper of LM-\nBFF (Gao et al., 2021a). Take sentiment analysis\nas an example, for the sentence S =“The film is\nvery attractive.”, we have:\nX = [CLS] S It was [MASK] .[SEP]\nM= {“positive”: “great”; “negative”: “boring”}\nwhere X is the input sequence, M(y) denotes the\nverbalizer that map the label yto a label word. For\nexample, M(“positive”) = “great”. Under these\nsettings, we can transform arbitrary NLU tasks\ninto a cloze-style problem. In our experiments,\nthe baseline is RoBERTa-base (Liu et al., 2019).\nIn addition, to validate the contributions of our\nproposal, we do not use other techniques mentioned\nin LM-BFF, such as demonstration learning and\nprompt ensemble.\nFor the zero-shot learning, we directly use the\nvanilla MLM to generate the result at the [MASK]\nposition. Formally, given an input sequence X, the\nlabel set Y, and the verbalizer M, the prediction\ncan be calculated as:\nˆy = argmax y∈Y(PrF([MASK] = M(y)|X))\nwhere Fdenotes the PLM, Pr(·) denotes the prob-\nability distribution generated by the MLM head\nk #Entities Open Entity TACRED FewRel Avg.\n1 6 77.1 72.9 86.9 79.0\n2 15 77.8 73.3 87.5 79.5\n3 32 77.6 71.7 88.7 79.3\n4 62 75.8 69.9 85.4 77.0\nTable 9: The effectiveness (F1 value %) of the k-hop\nsub-graph for knowledge prompting.\nof F. For the regression task (i.e. SST-B), we\ndirectly use the probability value to represent the\nreal-valued result.\nFor the few-shot learning, we follow Gao et al.\n(2021a) to construct few-shot training/developing\nset, which consists only 16 examples for each label.\nFor example, we have 32 examples for training\nand developing set for SST-2 dataset. During the\ntraining, the batch size is 4, the learning rate is\nset as 1e-5 with a warm up rate 0.1. We leverage\nthe cross-entropy loss to train for 64 epochs, and\nevaluate the model over the whole testing set.\nFine-tuning. For the full-data fine-tuning (FT-\nFull), we follow the standard supervised training\nsettings (which can also be found in LM-BFF.\nB.3 Hyper-parameter Analysis\nWe also conduct a hyper-parameter sensitivity\nstudy on Open Entity and TACRED tasks. Specif-\nically, we study three key hyper-parameters: the\nbalancing coefficients λand µfor PRI and MPM\ntasks in Eq. 9, and the number of negative entities\nN in the MPM task. We vary one hyper-parameter\nwith others fixed. From Figure 4, asλ(µ) increases,\nthe performance of KP-PLM first increases and\nthen drops, and it can achieve the best result when\nλ= 0.5 (µ= 0.5). For N in Figure 5, we observe\nthat the model performance improves significantly\nat the beginning, and then decreases when N >10.\nIt shows that too many negative entities may intro-\nduce noise to degrade the model performance.\nFinally, we analyze the effectiveness of the k-\nhop sub-graph. Specifically, we construct 1-hop,\n2-hop, 3-hop and 4-hop sub-graphs for knowledge\nprompting. We conduct experiments on Open En-\ntity, TACRED and FewRel task. Results in Table 9\nsuggest that we can achieve the best performance\nwhen k = 2. We also find that the overall model\nperformance decreases when k >2. This may be\nexplained by the introduction of many redundant\nand irrelevant entities.\n3177",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7930823564529419
    },
    {
      "name": "Knowledge graph",
      "score": 0.6678301095962524
    },
    {
      "name": "Natural language understanding",
      "score": 0.6534560918807983
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.6275368928909302
    },
    {
      "name": "Natural language",
      "score": 0.5638224482536316
    },
    {
      "name": "Natural language processing",
      "score": 0.4841659665107727
    },
    {
      "name": "Knowledge base",
      "score": 0.4637364149093628
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4569765031337738
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.45219525694847107
    },
    {
      "name": "Knowledge-based systems",
      "score": 0.4294489920139313
    },
    {
      "name": "Context (archaeology)",
      "score": 0.4275358319282532
    },
    {
      "name": "Graph",
      "score": 0.41022294759750366
    },
    {
      "name": "Knowledge management",
      "score": 0.36511462926864624
    },
    {
      "name": "Human–computer interaction",
      "score": 0.3519228994846344
    },
    {
      "name": "Theoretical computer science",
      "score": 0.09324240684509277
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    }
  ]
}