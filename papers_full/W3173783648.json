{
  "title": "Language Models are Good Translators",
  "url": "https://openalex.org/W3173783648",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A1805463631",
      "name": "Wang, Shuo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2747340743",
      "name": "Tu, Zhaopeng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352902943",
      "name": "Tan, Zhixing",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2320793692",
      "name": "Wang, Wenxuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2381112680",
      "name": "Sun, Maosong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2102234800",
      "name": "Liu Yang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2890964657",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W2952153923",
    "https://openalex.org/W2963925437",
    "https://openalex.org/W2908336025",
    "https://openalex.org/W2550821151",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W3035464238",
    "https://openalex.org/W3098903812",
    "https://openalex.org/W2130942839",
    "https://openalex.org/W2971043182",
    "https://openalex.org/W3137214022",
    "https://openalex.org/W2912521296",
    "https://openalex.org/W2970316683",
    "https://openalex.org/W2995304149",
    "https://openalex.org/W2963542740",
    "https://openalex.org/W3035072529",
    "https://openalex.org/W3176603070",
    "https://openalex.org/W2964076537",
    "https://openalex.org/W2952103439",
    "https://openalex.org/W2990555152",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963807318",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2970049541"
  ],
  "abstract": "Recent years have witnessed the rapid advance in neural machine translation (NMT), the core of which lies in the encoder-decoder architecture. Inspired by the recent progress of large-scale pre-trained language models on machine translation in a limited scenario, we firstly demonstrate that a single language model (LM4MT) can achieve comparable performance with strong encoder-decoder NMT models on standard machine translation benchmarks, using the same training data and similar amount of model parameters. LM4MT can also easily utilize source-side texts as additional supervision. Though modeling the source- and target-language texts with the same mechanism, LM4MT can provide unified representations for both source and target sentences, which can better transfer knowledge across languages. Extensive experiments on pivot-based and zero-shot translation tasks show that LM4MT can outperform the encoder-decoder NMT model by a large margin.",
  "full_text": "Language Models are Good Translators\nShuo Wang1 Zhaopeng Tu2 Zhixing Tan1 Wenxuan Wang2 Maosong Sun1,3 Yang Liu1,3,4\n1Dept. of Comp. Sci. & Tech., Institute for AI, BNRist Center, Tsinghua University\n3Beijing Academy of Artiﬁcial Intelligence 4Institute for AIR, Tsinghua University\n1wangshuo.thu@gamil.com\n1{zxtan, sms, liuyang2011}@tsinghua.edu.cn\n2Tencent AI Lab\n2{zptu, jwxwang}@tencent.com\nAbstract\nRecent years have witnessed the rapid advance in neural machine translation\n(NMT), the core of which lies in the encoder-decoder architecture. Inspired by the\nrecent progress of large-scale pre-trained language models on machine translation\nin a limited scenario, we ﬁrstly demonstrate that a single language model (LM4MT)\ncan achieve comparable performance with strong encoder-decoder NMT models on\nstandard machine translation benchmarks, using the same training data and similar\namount of model parameters. LM4MT can also easily utilize source-side texts as\nadditional supervision. Though modeling the source- and target-language texts\nwith the same mechanism, LM4MT can provide uniﬁed representations for both\nsource and target sentences, which can better transfer knowledge across languages.\nExtensive experiments on pivot-based and zero-shot translation tasks show that\nLM4MT can outperform the encoder-decoder NMT model by a large margin.\n1 Introduction\nRecent years have witnessed the success of the neural machine translation (NMT) models [SVL14,\nBCB15, GAG+17, VSP+17], which translate texts in the source language into the target language\nwith neural networks. A number of studies have directed their attention to designing more advanced\nNMT models. TRANSFORMER [VSP+17] is the most widely-used NMT model, which uses attention-\nbased neural networks for both the encoder and the decoder. [WFB+19] propose an efﬁcient NMT\nmodel using lightweight and dynamic convolutions. [SLL19] apply neural architecture search and\nﬁnd a better alternative to vanilla TRANSFORMER model. Although the core mechanism has evolved\nfrom RNN to self-attention and then other alternatives, the encoder-decoder architecture is still the\ndominating framework for NMT models.\nPrevious studies have shown that the boundary between encoder and decoder, in terms of the\nlocalization of the representation in the continuous space, is blurry for multilingual NMT [KBCF19].\n[HTX+18] demonstrate that the standard NMT model beneﬁts from weakening the boundary between\nencoder and decoder by sharing parameters of the two components. More recently, GPT-3 [BMR+20],\nwhich is a single language model (LM) pre-trained on huge amount of multilingual data, ﬁrstly shows\nsome promising results of LM on machine translation when given in-context translation examples\nas preﬁxes. GPT-3 has several key limitations that prevent it from serving as the practical NMT\nmodel, including (1) GPT-3 fails for machine translation without in-context preﬁxes; (2) both the\namount of training data and model parameters for GPT-3 are several orders of magnitude larger\nthan those of standard NMT models, which is prohibitively expensive for many researchers and\ndevelopers. Nevertheless, the surprising results still trigger us to think a research question: can we\nreally accomplish the machine translation task with a single language model?\nPreprint. Under review.\narXiv:2106.13627v1  [cs.CL]  25 Jun 2021\nTo answer this question, we explore the ability of language models for machine translation (i.e.,\nLM4MT) with only limited parallel data, which is often used to train the standard encoder-decoder\nNMT models. Surprisingly, we ﬁnd through experiments that the vanilla language model only\nmarginally underperforms the encoder-decoder NMT models with comparable model sizes. Beneﬁting\nfrom the characteristic of LM4MT to generate both the source and target sentences in the same\nmanner, we introduce an auto-encoding loss with a decaying schedule to help LM4MT better learn\nfrom source-language sentences. Experimental results show that the proposed LM4MT achieves\ncomparable performance with or even better performance than its encoder-decoder counterpart on\nsix machine translation benchmarks. For instance, on the benchmarking WMT14 English⇒German\nand English⇒French translation tasks, LM4MT achieves 29.3 BLEU and 42.9 BLEU, respectively.\nThe additional source-side supervision can improve the model performance in two other aspects: (1)\nhigher translation quality for source-original sentences, which are usually more complex and difﬁcult\nto translate [ZT19]; and (2) better model robustness against missing word perturbations. These results\nreveal that the source-side supervision can help LM4MT better understand source texts.\nAnother appealing advantage of LM4MT is the uniﬁed representation for both source and target\nsentences, which might better transfer knowledge across languages. We empirically validate our\nhypotheses in two scenarios: (1) pivot-based translationwhere a pivot language serves as the transit\nstation to transfer the knowledge from the source language to the target language [KPP+19]; and (2)\nzero-shot translation1 for multilingual NMT model, which has a stricter requirement on the model\nrepresentations to implicitly bridge between the language pairs unseen during training [ JSL+17].\nExperimental results show that LM4MT can outperform the encoder-decoder NMT model in all cases.\nWe ﬁnd the source-side auto-encoding loss is essential for LM4MT to perform zero-shot translation.\nTo sum up, the main contributions of this paper are listed as follows:\n• We ﬁrstly demonstrate that a single language model can achieve comparable translation\nperformance with the encoder-decoder NMT model of the same model size.\n• Beneﬁting from the additional source-side supervision and uniﬁed representations across\ndifferent languages, the proposed LM4MT can outperform the encoder-decoder NMT model\nin both pivot-based and zero-shot translation scenarios.\n2 Related Work\nFunctionalities and Importance of EncoderIn recent years, there has been a growing interest\nin understanding the functionalities of the encoder in encoder-decoder NMT models. For example,\n[TSN19] simplify the TRANSFORMER model to an encoder-free model and ﬁnd that the encoder\nis crucial for NMT models to achieve good results. [WLX+19, WTSL20] show that enlarging the\ncapacity of the encoder is more effective for improving translation performance than enlarging the\ndecoder. However, these conclusions might only hold for the encoder-decoder architecture. In this\npaper, we rethink the importance of encoder in a new architecture and reveal that a single decoder\ncan accomplish the translation task well.\nShared Encoder and Decoder[KBCF19] ﬁnd that the boundary between encoder and decoder is\nblurry for multilingual NMT. In standard translation, there are some works that weaken the boundary\nbetween the encoder and decoder. For example, [HTX+18] share the parameters of the encoder and\ndecoder, which coordinates the learning of hidden representations of the two components. However,\ntheir model still encodes source-language texts in the same way as vanilla encoder-decoder NMT\nmodels, ignoring the source-side supervision. The source and target sentences are still consumed in\nseparate mechanisms. We take one step further and simplify the architecture into a simple decoder,\nwhere we can easily utilize the source-side supervision to help better understand source texts.\nLanguage Model for Machine Translation Recently, the pre-trained language model GPT-\n3 [BMR+20] has shown encouraging results on machine translation tasks. GPT-3 is pre-trained on\nmassive data with a huge amount of parameters, and requires in-context translation examples to\nachieve good translation performance. Our work, on the other hand, is the ﬁrst attempt to systemati-\ncally compare the ability of encoder-decoder models and LM (i.e., single decoder) across different\ntranslation scenarios, using the same training data and similar model sizes.\n1Zero-shot translation denotes translating between language pairs that do not exist in the training data.\n2\n<s> y1 y2 y3x2 x3x1\ny1 y2 y3 </s>\nENC\n</s>\nDEC\n(a) encoder-decoder NMT model\nTraining Phase Inference Phase\nx1 x2 x3<X> <Y> y1 y2 y3\nx2 x3x1 <Y> y1 y2 y3 </s>\nLM4MT\nx1 x2 x3<X> <Y> y1 y2 y3\ny1 y2 y3 </s>\nLM4MT\nˆ ˆ ˆ\nˆ ˆ ˆ (b) LM4MT during training\nTraining Phase Inference Phase\nx1 x2 x3<X> <Y> y1 y2 y3\nx2 x3x1 <Y> y1 y2 y3 </s>\nLM4MT\nx1 x2 x3<X> <Y> y1 y2 y3\ny1 y2 y3 </s>\nLM4MT\nˆ ˆ ˆ\nˆ ˆ ˆ (c) LM4MT during inference\nFigure 1: Illustration of LM4MT. For comparison, we also plot the encoder-decoder model. During\ntraining, we feed LM4MT with the concatenation of the source- and target-language sentences, which\nare explicitly separated by special language tags. At the inference time, the source-language text,\ntogether with the target-language tag, is used as the preﬁx for LM4MT to generate the translation.\n3 Approach\n3.1 Preliminaries\nLanguage Model Given a monolingual sentence y = {y1,y2,...,y T }, the goal of language\nmodeling is to estimate the joint probability P(y), which is usually auto-regressively factorized as\nP(y) =\nT∏\nt=1\nP(yt|y<t), (1)\nwhere y<t = {y1,y2,...,y t−1}is the preﬁx before yt. With this factorization, the problem is\nsimpliﬁed to estimating each conditional factor. Standard neural language models [DYY+19] encode\nthe context y<t into a continuous vector, which is then multiplied by the word embedding matrix to\nobtain the logits. The logits are then used to compute the probability distribution over the next word\nthrough the Softmax function.\nEncoder-Decoder NMT Model Given a source-language sentence x = {x1,x2,...,x S}, NMT\nmodels learn to predict the conditional probability of the corresponding target-language sentence y:\nP(y|x) =\nT∏\nt=1\nP(yt|x,y<t). (2)\nMost previous NMT models use the encoder-decoder framework [ BCB15, VSP+17, LGG+20].\nThe encoder is a feature extractor, which maps the source-language sentence x into a sequence\nof continuous representations r = {r1,r2,..., rS}. The decoder is a conditional language model,\nwhich estimates the probability P(y|r). In the widely used TRANSFORMER [VSP+17] model, both\nthe encoder and the decoder use attention networks, which are shown effective to learn contextualized\nrepresentations [DCLT19]. To achieve good translation performance, NMT models are expected to\nnot only extract effective source-language representationsr, but also be capable of generating a ﬂuent\ntarget-language sentence that can recover the information conveyed in the source sentence.\n3.2 Language Model for Machine Translation\nRecently, several works ﬁnd that huge language models pre-trained on large-scale data set can\nachieve good results on a number of natural language understanding and generation tasks [RWC+19,\nBMR+20, DQL+21, LZD+21], indicating the potential of language models to serve as a good feature\nextractor. However, for machine translation, it is still not well investigated whether a language model\nalong can act as a source-language feature extractor and a target-language generator at the same\ntime, especially without huge model size and large amount of training data. In this work, we aim to\ninvestigate the capability of language models to perform machine translation, which is a task that\nrequires the abilities of both language understanding and generation.\n3\nFrom Equation (1) and (2), we ﬁnd that objectives of language modeling and machine translation are\nquite similar, since the source-language sentence x can be seen as a special type of preﬁx. Inspired\nby this observation, we propose the language model for machine translation (i.e., LM4MT) that are\ntrained to estimate the joint probability of the two sentence x and y:\nP(x,y) =\nS∏\ns=1\nP(xs|x<s)\nT∏\nt=1\nP(yt|x,y<t). (3)\nSpeciﬁcally, we concatenate x and y into a sentence pair, and then use LM4MT to estimate the joint\nprobability of such a sentence pair as if it is one sentence. To help the model better identify the\nboundary of sentences from different languages, we add a special language tag before each sentence.\nFigure 1b depicts an example for the training phrase of LM4MT. Experiments in Section 5.1 show\nthat the added language tags are effective to improve the translation performance of LM4MT.\nTraining Just as standard language models, the training objective of LM4MT is to minimize the\nnegative log-likelihood of the probability P(x,y):\n−log P(x,y) =LAE + LMT = −\nS∑\ns=1\nlog P(xs|x<s) −\nT∑\nt=1\nP(yt|x,y<t), (4)\nwhere LAE = −∑S\ns=1 log P(xs|x<s), which is the auto-encoding loss, reﬂecting the ability of the\nmodel to reconstruct the source-language sentence x. LMT = −∑T\nt=1 P(yt|x,y<t), which is the\nmachine translation loss that has been widely-used for NMT models. Compared to encoder-decoder\nNMT models, LM4MT is trained with an additional source-side auto-encoding loss LAE .\nIntuitively, the explicit supervision induced by LAE may help LM4MT better understand the source-\nlanguage sentences. Moreover, using LAE makes the modeling mechanisms of the source and target\ntexts more similar, which may reduce the representation gap between source and target sentences.\nWe ﬁnd through experiments (Section 5.1) that simply adding LAE is not a good practice for machine\ntranslation, which might be caused by that LAE prevents the model to further minimize LMT at the\nend of the training. Inspired by the training strategy of unsupervised NMT [ LOC+18, LC19], we\nmultiply LAE with a decaying factor λd. Therefore, the training objective of LM4MT is\nLLM4MT = λdLAE + LMT . (5)\n0.0\n1.0\n\u0000 d\n↵\n\u0000\nFigure 2: Decaying schedule for λd in\nEq. (5). αand βare hyper-parameters.\nAs shown in Figure 2, we use a piece-wise linear de-\ncaying schedule for the factor λd. Through this strat-\negy, we expect LM4MT to better learn from the source\nsentences at early stages and then focus more on the\ntranslation loss at the end of the training.\nInference Different from the training phrase during\nwhich LM4MT estimates the probability for both the\nsource- and target-language sentences, we only let\nLM4MT predict the target-language tokens at the infer-\nence time. As shown in Figure 1c, LM4MT takes in\nsource-language tokens and encodes them into hidden\nstates in parallel. Then the proposed model generates\neach target-language token step by step.\n4 Experimental Setup\nData To make a thorough comparison between the widely-used TRANSFORMER [VSP+17] model\nand our proposed LM4MT, we conducted experiments on datasets with different sizes: WMT16 2\nEnglish-Romanian (En-Ro), WMT143 English-German (En-De) and English-French (En-Fr), which\n2http://statmt.org/wmt16/translation-task.html\n3http://statmt.org/wmt14/translation-task.html\n4\nconsist of 0.6M, 4.5M and 35.8M sentence pairs respectively. In En-Ro, we used news-dev2016 as\nthe validation set and news-test2016 as the test set. In En-De and En-Fr, we used news-test2013 and\nnews-test2014 as the validation and test sets, respectively. We applied BPE [SHB16] with 32K merge\noperations for En-De data, and with 40K merge operations for En-Ro and En-Fr data.\nModel We used the encoder-decoder based TRANSFORMER model [VSP+17] as our baseline.\nWe used models of two sizes, namely the TRANSFORMER -Base and TRANSFORMER -Big, both of\nwhich consist of a 6-layer encoder and a 6-layer decoder. The hidden sizes of TRANSFORMER -\nBase and TRANSFORMER -Big are 512 and 1024, respectively. We also list the results of some\nrecent representative architectures for comparison, including TRANSFORMER with relative posi-\ntion representations [SUV18], scaling TRANSFORMER [OEGA18], layer-wise coordinated TRANS -\nFORMER [HTX+18], dynamic convolutions [WFB+19], and evolved TRANSFORMER [SLL19].\nThe proposed LM4MT model consists of only a self-attention decoder. There are mainly three differ-\nences between the LM4MT model and the decoder of the vanilla TRANSFORMER [VSP+17]. Firstly,\neach LM4MT layer has only one type of attention network while the decoder in [VSP+17] has both\nself-attention and cross-attention networks. Secondly, we use pre-norm residual unit [WLX+19] in\norder to train deep language models. Thirdly, we follow [RWC+19] to use GELU activations [HG16]\nin LM4MT, which has been shown to be effective in language model training. Similar to TRANS -\nFORMER , we also conduct experiments with base and big settings for LM4MT. The hidden size of\nLM4MT-Base is 512 and that of LM4MT-Big is set to 1024.\nTraining Details For En-Ro, we only trained base models since the training corpus was too small\n(0.6M sentence pairs) to learn big models. The dropout rate was set to 0.3 for both TRANSFORMER\nand LM4MT. We set weight decay to 1e−4 to overcome over-ﬁtting. For En-De, the dropout rates\nwere set to 0.1 for base models and 0.3 for big models. For En-Fr, the dropout rate was set to 0.1 for\nboth base and big models. For all the pivot-based and multilingual translation models, the dropout\nrate was set to 0.1. For all the three language pairs, we used Adam [ KB15] to optimize the model\nparameters, with β1 = 0.9, β2 = 0.98, and ϵ= 10−9. We trained base models on mini-batches that\ncontain approximately 32K target-language tokens for 150K steps. For big models, we followed\n[OEGA18] to use larger batches, which contain approximately 460K tokens, to further boost the\nperformance of big models. When using the large batch size, we followed [ WFB+19] to use the\ncosine learning rate schedule, where the learning rate was warmed up linearly to1e−3 in the ﬁrst 10K\nsteps and then decayed to 1e−7 following a cosine rate within a single cycle. Big models were trained\nfor 30K steps on large batches. We obtained the ﬁnal model by averaging the last 5 checkpoints,\nwhich were saved at 1000-update and 500-update intervals for base and big models, respectively.\nAs for the decaying schedule, we set α=0.1, β=37.5K for base models and α=0.1, β=22.5K for\nbig models. All models were implemented on the top of fairseq toolkit.4 We conducted all the\nexperiments on 8 Nvidia Telsa V100 32GB GPUs.\n5 Experimental Results\n5.1 Ablation Study\nTable 1: Effect of language tag (“Tag”) for\n19-layer LM4MT with LMT loss.\nModel Tag BLEU\nTRANSFORMER - 26.7\nLM4MT × 25.8\n✓ 26.3\nEffect of Language Tag In the training phase, we\nsimply feed LM4MT with the concatenation of the\nsource- and target-language sentences, which may make\nit difﬁcult to identify the start position of the target-\nlanguage sentence. In response to this problem, we use\nlanguage tags to explicitly distinguish sentences of dif-\nferent languages, as shown in Figure 1b. Table 1 shows\nthe impact of language tags, indicating the importance\nof explicit indicators of languages for LM4MT. In the\nfollowing experiments we use language tags by default.\nEffect of Layer NumberSince TRANSFORMER has an additional encoder compared to LM4MT,\nwe deepen the LM4MT to assimilate the parameter count. Table 2 shows the results of LM4MT\n4https://github.com/pytorch/fairseq\n5\nwith different depths. The hidden size of all models are 512. Surprisingly, the 6-layer LM4MT\nperforms only 1.8 BLEU lower than the encoder-decoder baseline, although it has much fewer\nparameters (56.9M vs. 98.8M). Enlarging the LM4MT by adding layers is effective to improve the\ntranslation performance. When using comparable amount of parameters, the performance of LM4MT\nis 0.4 BLEU point lower than the TRANSFORMER baseline (26.3 vs. 26.7 for L19 LM4MT). In the\nfollowing experiments, we use L19 as the default architecture for LM4MT.\nTable 2: Effect of layer number. All models use\nLMT as the training loss.\nModel Arch. Param. BLEU\nTRANSFORMER L6-L6 98.8M 26.7\nLM4MT\nL6 56.9M 24.9\nL12 75.8M 25.6\nL18 94.7M 26.2\nL19 97.8M 26.3\nEffect of Auto-Encoding Loss Another key\ndifference between LM4MT and TRANS -\nFORMER is that we introduce an additional auto-\nencoding loss LAE (Equation (5)) for LM4MT\nto better understand the source sentence. To em-\npirically validate the effect of the source-side\nLAE , we follow [YG19] to separately report the\nBLEU score on the source-original5 sentences\non WMT14 En ⇒De test set. Intuitively, the\ntranslation of source-original sentences requires\na better understanding of source sentences to\nhandle the relatively more complex sentence\nstructures and more diverse contents [WTT+21].\nAs listed in Table 3, directly adding auto-encoding loss (“LAE + LMT ”) fails to improve translation\nperformance. We plot the learning curves of validation perplexities for LM4MT trained with different\nobjectives. We ﬁnd that directly adding the auto-encoding loss inversely increase the validation per-\nplexity in late training stages. Using the decaying schedule to adjust the weight ofLAE can effectively\nimprove translation performance and help training convergence. The performance improvement is\nmainly from better translation of source-original sentences (“o”), which conﬁrms our claim that the\nauto-encoding loss can help to better understand the source sentences. In the following experiments,\nwe train LM4MT models with decaying auto-encoding loss by default.\nTable 3: Effect of auto-encoding loss LAE . λd denotes a\ndynamic weight decaying from 1 to 0 during training. “o”\ndenotes source-original sentences in the test set, while\n“n-o” denotes the target-original sentences.\nModel Loss Valid Test\nall o n-o\nTRANS . LMT 26.7 27.8 27.4 28.1\nLM4MT\nLMT 26.3 27.3 26.5 27.6\nLAE + LMT 26.0 26.9 26.6 27.0\nλdLAE + LMT 26.8 27.8 27.6 27.9\nPerplexity\n4.7\n5.4\n6.2\n6.9\nTraining Step (K)\n0 50 100 150\nL\nAE\n+ L\nMT\n\u0000 d\nL\nAE\n+ L\nMT\nL\nMT Figure 3: Learning curves of valida-\ntion perplexity on WMT14 En⇒De.\n5.2 Standard Translation\nTranslation Performance Table 4 lists the results on several WMT benchmarks with different data\nscales. LM4MT performs very competitively on all the six translation directions, demonstrating that\nour approach is applicable to low-, medium-, and high-resource language pairs. For instance, on\nthe widely-used WMT14 En⇒De benchmark, LM4MT-BIG performs as well as our implemented\nTRANSFORMER -BIG, which is also comparable with several strong baselines. Note that our work is\ncomplementary to many previous works since LM4MT can use more advanced components, such\nas dynamic convolution [WFB+19] and evolved TRANSFORMER cell [SLL19]. Closely related to\nour work, LAYER -WISE COOR . [HTX+18] shares the parameters of the encoder and decoder, but\nthey still use separate mechanisms to model the source- and target-language texts and ignore the\nsource-side supervision. We take one step further and simplify the architecture into a simple decoder.\n5Source-original texts are written by source-language native speakers, which are found to be more difﬁcult to\ntranslate than human-translated texts [ZT19]. Since the En⇒De validation set contains six different original\nlanguages, we report the source-original results on the En⇒De test set.\n6\nTable 4: Translation performance on WMT16 En ⇔Ro (0.6M sentence pairs), WMT14 En ⇔De\n(4.6M sentence pairs) and WMT14 En⇔Fr (35.8M sentence pairs) test sets.\nModel WMT16 En⇔Ro WMT14 En ⇔De WMT14 En ⇔Fr\nEn⇒Ro Ro ⇒En En ⇒De De ⇒En En ⇒Fr Fr ⇒En\nExisting Work\nTRANSFORMER -BASE [VSP+17] - - 27.3 - 38.1 -\nTRANSFORMER -BIG [VSP+17] - - 28.4 - 41.0 -\nRELATIVE POSITION [SUV18] - - 29.2 - 41.5 -\nSCALE -TRANS . [OEGA18] - - 29.3 - 43.2 -\nLAYER -WISE COOR . [HTX+18] 34.4 - 29.0 - - -\nDYNAMIC CONV. [WFB+19] - - 29.7 - 43.2 -\nEVOLVED -TRANS . [SLL19] - - 29.5 - 41.3 -\nOur Implementation\nTRANSFORMER -BASE 34.4 33.9 27.8 31.3 41.2 36.5\nLM4MT-BASE 34.2 34.1 27.8 31.5 41.2 37.1\nTRANSFORMER -BIG - - 29.3 33.0 43.0 39.0\nLM4MT-BIG - - 29.3 33.2 42.9 39.5\nTable 5: Translation performance with missing\nwords. All models use the base setting.\nMissing Ratio 0% 30% 50%\nWMT14 En⇒De\nTRANSFORMER 27.8 19.1 11.5\nLM4MT 27.8 20.2 13.6\nWMT14 De⇒En\nTRANSFORMER 31.3 19.9 11.8\nLM4MT 31.5 20.6 14.2\nModel Robustness To investigate the robust-\nness of LM4MT to noisy inputs, we fol-\nlowed [SB18, ZZH+20] to construct noisy test\nexamples by omitting some words in the source-\nlanguage sentences. Table 5 lists the results on\nWMT14 En ⇔De test sets. It is evident that\nLM4MT is more robust than TRANSFORMER\nto missing word noise, and the performance im-\nprovement generally goes up with the increase\nof missing ratio. This may be attributed to the\nauto-encoding loss LAE , which induces source-\nside reconstruction supervision that can help\nLM4MT better “denoise” noisy inputs.\n5.3 Pivot-Based Translation\nLM4MT uses a uniﬁed decoder to represent the source- and target-language sentences, both of\nwhich are learned by causal self-attention networks. Accordingly, the representation gap between\nthe source- and target-language sentences are much smaller in LM4MT than that in encoder-decoder\nmodels. We believe that the shared representation can help better transfer the knowledge between\nsource- and target-language texts. We verify the research hypothesis in the pivot-based translation\nscenario [KPP+19], where the pivot language severs as the intermediate output to transfer the\nknowledge from the source language to the target.\nFormally, we have parallel corpora B(X,Y ) = {⟨xn,yn⟩}N\nn=1 and B(Y,Z) = {⟨ym,zm⟩}M\nm=1,\nand we aim to translate sentences of language X into language Z. Assume that parallel corpus\nB(X,Z) is not available. To achieve this goal, we train NMT models on the mixture of B(X,Y ) and\nB(Y,Z). Following [JSL+17], we pend the target-language tag to the source sentence to indicate\nthe translation direction for TRANSFORMER . For LM4MT, we use the tagging scheme illustrated in\nFigure 1b. We also train TRANSFORMER models using the same tagging scheme as LM4MT and\nﬁnd that only using the target-language tag works better for TRANSFORMER . At the inference time,\nwe ﬁrstly translate the test data in language X into the pivot language Y, which is then translated to\nlanguage Z. Intuitively, pivot-based translation tasks can beneﬁt from LM4MT model that learns a\nshared representation across different languages.\nIn our experiments, we use English as the pivot language and aim to translate between De and\nFr. We mix the WMT14 En-De and En-Fr corpora and the En-De corpus is upsampled to the\nsame size with that of the En-Fr corpus. We use news-test2020 De⇔Fr to evaluate the translation\n7\nTable 6: Pivot-based (English as the pivot language) translation performance measured by BLEU\nscore. Results are reported on WMT14 test sets for En⇔De and En⇔Fr, and WMT20 test sets for\nDe⇔Fr. NMT model for each direction is trained on the mxiture of WMT14 En-De and WMT14\nEn-Fr training corpora. Improvements over the TRANSFORMER baseline are highlighted in red cells\nwhile deteriorations are represented in green cells. Deeper color indicates larger performance gap.\nModel De⇒En⇒Fr Fr ⇒En⇒De\nDe⇒En En ⇒Fr De ⇒Fr Fr ⇒En En ⇒De Fr ⇒De\nTRANSFORMER -BASE 30.8 39.5 25.7 35.2 26.8 21.4\nLM4MT-BASE 30.9 39.5 26.6 36.1 26.5 22.8\nTRANSFORMER -BIG 32.2 42.3 26.5 37.7 29.1 24.2\nLM4MT-BIG 32.5 41.9 27.7 38.1 29.0 25.1\nTable 7: Translation performance in the multilingual setting. “Zero-Shot\" means translating between\nunseen language pairs while “Multilingual\" means translating between seen language pairs.\nModel Multilingual Zero-Shot\nDe⇒En En ⇒Fr Fr ⇒En En ⇒De De ⇒Fr Fr ⇒De\nTRANSFORMER -BASE 30.4 38.5 34.8 25.8 21.9 13.8\nLM4MT-BASE 30.4 38.4 34.8 25.9 24.9 19.5\nTRANSFORMER -BIG 33.1 41.9 38.0 29.2 23.8 16.7\nLM4MT-BIG 33.9 41.1 37.8 28.6 29.5 24.9\nperformance. In each direction, all the models are trained only for the involved two intermediate\ndirections. For instance, De ⇒Fr models are trained on the mixture of De ⇒En and En⇒Fr data.\nTable 6 shows the results of both base and big models. For pivot-based translation (i.e., De ⇔Fr),\nLM4MT consistently outperforms the TRANSFORMER baseline by a large margin. To dispel the\ndoubt that the superior performance of LM4MT might come from improvements accumulated in the\ntwo individual directions, we compute the performance gap between the two models in each direction,\nwhich is highlighted by color. The improvements in pivot-based translation directions are consistently\nlarger than the two-step accumulated improvements, reconﬁrming the strength of LM4MT to exploit\nthe shared knowledge between the source and target sides of parallel training data.\n5.4 Zero-Shot Translation\nPrevious studies have shown that a single multilingual NMT model can enable zero-shot translation\n– translating between language pairs on which the NMT model has never been trained [ JSL+17,\nGWCL19]. Compared with pivot-based translation in Section 5.3, zero-shot translation requires no\npivot language as the intermediate output, thus requires better model representations to implicitly\nbridge between zero-shot language pairs. We followed [JSL+17] to conduct experiments with the\nmultilingual setting. Speciﬁcally, we mix the bidirectional WMT14 En⇔De and En⇔Fr corpora to\ntrain a single multilingual NMT model, which is used to translate the zero-shot language pair of the\nWMT20 De⇔Fr test sets. Similarly to pivot-based experiments, we upsampled En⇔De corpora to\nthe same size with that of the En⇔Fr corpora.\nTable 7 lists the results. In zero-shot translation directions, TRANSFORMER performs much worse\nthan LM4MT. These results demonstrate the superiority of LM4MT on zero-shot translation. By\nmanually checking the generated translations, we found that the failed models suffer from the off-\ntarget translation issue(i.e., translating into a wrong target language), which is the major source of\nthe inferior zero-shot performance [ZWTS20]. We follow [ZWTS20] to employ the langdetect\nlibrary6 to detect the language of model outputs, and measure the translation-language accuracy for\nzero-shot cases. Table 8a lists the results, which provide empirical support for our ﬁndings. This\nis potentially caused by that TRANSFORMER tends to memorize translation directions seen in the\n6https://github.com/Mimino666/langdetect\n8\nTable 8: Analyses of off-target translation issue.\n(a) Translation-language accuracy for zero-shot.\nModel Zero-Shot\nDe⇒Fr Fr ⇒De\nTRANSFORMER -BASE 88.3% 79.7%\nLM4MT-BASE 97.9% 97.6%\nTRANSFORMER -BIG 87.8% 76.5%\nLM4MT-BIG 98.8% 98.5%\n(b) BLEU on sentences with the correct language.\nModel Zero-Shot\nDe⇒Fr Fr⇒De\nTRANSFORMER -BASE 24.4 18.1\nLM4MT-BASE 25.5 19.9\nTRANSFORMER -BIG 26.1 21.8\nLM4MT-BIG 29.4 25.7\nTable 9: Effect of auto-encoding loss on zero-shot translation.\nModel Loss Multilingual Zero-Shot\nDe⇒En En ⇒Fr Fr ⇒En En ⇒De De ⇒Fr Fr ⇒De\nTRANS .-BASE LMT 30.4 38.5 34.8 25.8 21.9 13.8\nLM4MT-BASE\nLMT 29.5 37.9 34.2 25.4 7.9 8.6\nLAE + LMT 29.3 37.5 33.7 24.7 23.8 18.6\nλdLAE + LMT 30.4 38.4 34.8 25.9 24.9 19.5\nTRANS .-B IG LMT 33.1 41.9 38.0 29.2 23.8 16.7\nLM4MT-BIG\nLMT 33.5 40.9 37.0 28.6 19.3 21.8\nLAE + LMT 33.0 40.5 37.1 27.6 28.8 23.3\nλdLAE + LMT 33.9 41.1 37.8 28.6 29.5 24.9\ntraining data while LM4MT can better generalize into unseen directions. More detailed results on\noff-target mistakes can be found in Appendix (Tables 10 and 11).\nTo rule out the effect of the target language, we also evaluate the model performance on test examples\nthat can be translated into the correct language by both TRANSFORMER and LM4MT. The results are\nshown in Table 8b, which demonstrate that LM4MT performs better than TRANSFORMER even when\nboth of them can output the correct language in zero-shot cases. We attribute the improvement to\nthe reason that LM4MT can map sentences that come from various languages into a more uniﬁed\nrepresentation space, thus can better understand the input sentences.\nTo further investigate why LM4MT shows a strong performance in zero-shot translation, we train\nLM4MT using different loss functions. Table 9 shows the results, indicating that the source-language\nsupervision LAE is crucial for LM4MT to achieve good performance. Training with LAE , LM4MT is\noptimized by gradients induced from both the source- and target-side tokens, while encoder-decoder\nNMT models are trained only with the target-side loss, which may increase the representation gap\nbetween the source and target texts. Moreover, LAE enables better understanding of source sentences,\nwhich may prevent LM4MT from capturing spurious correlations between input sentences and\nlanguage tags, which has been found to be harmful to zero-shot translation [GWCL19].\n6 Conclusion\nWe propose a novel LM4MT model to perform machine translation using only a single language\nmodel. Although LM4MT is more simpliﬁed than the standard encoder-decoder NMT model, it\ncan achieve competitive results with several strong encoder-decoder NMT baselines on standard\nmachine translation tasks. LM4MT shows superior performance in both pivot-based and zero-shot\ntranslation scenarios by better transferring knowledge across languages with a uniﬁed representation.\nOne potential limitation of this work is that we only conduct experiments on parallel data. We\nbelieve that using additional monolingual data can further augment the performance of LM4MT,\nwhich can naturally consume monolingual texts without data augmentation methods. Another\ninteresting direction is to investigate the effect of LM4MT on unsupervised NMT, which uses uniﬁed\nrepresentations to bridge between language pairs without training signals from parallel data.\n9\nReferences\n[BCB15] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation\nby jointly learning to align and translate. In ICLR, 2015.\n[BMR+20] T. Brown, B. Mann, Nick Ryder, Melanie Subbiah, J. Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel\nHerbert-V oss, G. Krüger, T. Henighan, R. Child, Aditya Ramesh, D. Ziegler, Jeffrey\nWu, Clemens Winter, Christopher Hesse, Mark Chen, E. Sigler, Mateusz Litwin, Scott\nGray, Benjamin Chess, J. Clark, Christopher Berner, Sam McCandlish, A. Radford, Ilya\nSutskever, and Dario Amodei. Language models are few-shot learners. In NeurIPS,\n2020.\n[DCLT19] J. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL-HLT, 2019.\n[DQL+21] Zhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding, Jiezhong Qiu, Zhilin Yang, and Jie\nTang. All nlp tasks are generation tasks: A general pretraining framework. ArXiv,\nabs/2103.10360, 2021.\n[DYY+19] Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell, Quoc Le, and Ruslan Salakhut-\ndinov. Transformer-XL: Attentive language models beyond a ﬁxed-length context. In\nACL, 2019.\n[GAG+17] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin.\nConvolutional sequence to sequence learning. In ICML, 2017.\n[GWCL19] Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor O.K. Li. Improved zero-shot neural\nmachine translation via ignoring spurious correlations. In ACL, 2019.\n[HG16] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (gelus). arXiv,\nabs/1606.08415, 2016.\n[HTX+18] Tianyu He, Xu Tan, Yingce Xia, Di He, Tao Qin, Zhibo Chen, and Tie-Yan Liu. Layer-\nwise coordination between encoder and decoder for neural machine translation. In\nNeurIPS, 2018.\n[JSL+17] Melvin Johnson, Mike Schuster, Quoc V . Le, Maxim Krikun, Yonghui Wu, Zhifeng\nChen, Nikhil Thorat, Fernanda Viégas, Martin Wattenberg, Greg Corrado, Macduff\nHughes, and Jeffrey Dean. Google’s multilingual neural machine translation system:\nEnabling zero-shot translation. TACL, 2017.\n[KB15] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In\nICLR, 2015.\n[KBCF19] Sneha Kudugunta, Ankur Bapna, Isaac Caswell, and Orhan Firat. Investigating multilin-\ngual NMT representations at scale. In EMNLP, 2019.\n[KPP+19] Yunsu Kim, Petre Petrov, Pavel Petrushkov, Shahram Khadivi, and Hermann Ney. Pivot-\nbased transfer learning for neural machine translation between non-English languages.\nIn EMNLP, 2019.\n[LC19] Guillaume Lample and Alexis Conneau. Cross-lingual language model pretraining. In\nNeurIPS, 2019.\n[LGG+20] Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad,\nMike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural\nmachine translation. TACL, 8:726–742, 2020.\n[LOC+18] Guillaume Lample, Myle Ott, Alexis Conneau, Ludovic Denoyer, and Marc’Aurelio\nRanzato. Phrase-based & neural unsupervised machine translation. In EMNLP, 2018.\n[LZD+21] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie\nTang. Gpt understands, too. ArXiv, abs/2103.10385, 2021.\n10\n[OEGA18] Myle Ott, Sergey Edunov, David Grangier, and Michael Auli. Scaling neural machine\ntranslation. In WMT, 2018.\n[RWC+19] Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever.\nLanguage models are unsupervised multitask learners. 2019.\n[SB18] Harshil Shah and David Barber. Generative neural machine translation. In NeurIPS,\n2018.\n[SHB16] Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare\nwords with subword units. In ACL, 2016.\n[SLL19] David R. So, Chen Liang, and Quoc V . Le. The evolved transformer. In ICML, 2019.\n[SUV18] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position\nrepresentations. In NAACL, 2018.\n[SVL14] Ilya Sutskever, Oriol Vinyals, and Quoc V . Le. Sequence to sequence learning with\nneural networks. In NeurIPS, 2014.\n[TSN19] Gongbo Tang, Rico Sennrich, and Joakim Nivre. Understanding neural machine transla-\ntion by simpliﬁcation: The case of encoder-free models. In RANLP 2019, 2019.\n[VSP+17] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N\nGomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS,\n2017.\n[WFB+19] Felix Wu, Angela Fan, Alexei Baevski, Yann N. Dauphin, and Michael Auli. Pay less\nattention with lightweight and dynamic convolutions. In ICLR, 2019.\n[WLX+19] Qiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F. Wong, and Lidia S.\nChao. Learning deep transformer models for machine translation. In ACL, 2019.\n[WTSL20] Shuo Wang, Zhaopeng Tu, Shuming Shi, and Yang Liu. On the inference calibration of\nneural machine translation. In ACL, 2020.\n[WTT+21] Shuo Wang, Zhaopeng Tu, Zhixing Tan, Shuming Shi, Maosong Sun, and Yang Liu. On\nthe language coverage bias for neural machine translation. In Findings of ACL, 2021.\n[YG19] Philipp Koehn Yvette Graham, Barry Haddow. Translationese in machine translation\nevaluation. In Arxiv, 2019.\n[ZT19] Mike Zhang and Antonio Toral. The effect of translationese in machine translation test\nsets. In WMT, 2019.\n[ZWTS20] Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. Improving massively\nmultilingual neural machine translation and zero-shot translation. In ACL, 2020.\n[ZZH+20] Zaixiang Zheng, Hao Zhou, Shujian Huang, Lei Li, Xin-Yu Dai, and Jiajun Chen.\nMirror-generative neural machine translation. In ICLR, 2020.\n11\nA Appendix\nTable 10: Target-language ratios for zero-shot cases. The expected language is highlighted in green.\nModel De⇒Fr Fr ⇒De\nDe En Fr Fr En De\nTRANSFORMER -BASE 0.6% 9.8% 88.3% 4.0% 15.2% 79.7%\nLM4MT-BASE 0.3% 1.4% 97.9% 0.9% 0.1% 97.6%\nTRANSFORMER -BIG 2.8% 8.6% 87.8% 12.7% 9.3% 76.5%\nLM4MT-BIG 0.1% 0.6% 98.8% 0.2% 1.0% 98.5%\nTable 11: Example for zero-shot De⇒Fr translation in multilingual translation. The TRANSFORMER\nmodel often translates into other language (e.g. English rather than French or copying the source\nsentence), while our LM4MT suffers less from these mistakes.\nSource Im Süden und Osten Europas tun sich die ökologischen Parteien nach wie vor\nschwer.\nReference Les partis écologiques ont du mal à percer dans le sud et dans l’est de l’Europe.\nTRANS .-BASE In the south and east of Europe , the environmental parties are still struggling.\nLM4MT-BASE Dans les pays de l’Europe du Sud et de l’Est , les partis écologiques restent\ndifﬁciles.\nTRANS .-B IG In the south and east of Europe , the environmental parties are still struggling.\nLM4MT-BIG Dans les pays du sud et de l’est de l’Europe, les partis écologiques continuent\nde se heurter à des difﬁcultés.\nSource Bundesverteidigungsministerin Ursula von der Leyen ( CDU ) soll neue Präsi-\ndentin der EU-Kommission werden\nReference La ministre fédérale de la Défense Ursula von der Leyen ( CDU ) doit devenir\nla nouvelle présidente de la Commission europé.\nTRANS .-BASE Bundesverteidigungsministerin Ursula von der Leyen ( CDU ) soll zum neuen\nPräsidenten der EU-Kommission werden.\nLM4MT-BASE La ministre fédérale de la défense , Ursula von der Leyen ( CDU ) , est censée\ndevenir la nouvelle présidente de la Commission européenne.\nTRANS .-B IG Bundesverteidigungsministerin Ursula von der Leyen ( CDU ) soll neue Präsi-\ndentin der EU-Komission werden.\nLM4MT-BIG Le ministre fédéral de la défense Ursula von der Leyen ( CDU ) doit devenir la\nnouvelle présidente de la Commission européenne.\n12",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8783959150314331
    },
    {
      "name": "Machine translation",
      "score": 0.865451455116272
    },
    {
      "name": "Encoder",
      "score": 0.7028542160987854
    },
    {
      "name": "Margin (machine learning)",
      "score": 0.6799439787864685
    },
    {
      "name": "Language model",
      "score": 0.6276358962059021
    },
    {
      "name": "Natural language processing",
      "score": 0.6139711737632751
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5860960483551025
    },
    {
      "name": "Translation (biology)",
      "score": 0.5404857397079468
    },
    {
      "name": "Machine learning",
      "score": 0.23278093338012695
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Messenger RNA",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 9
}