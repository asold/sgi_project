{
    "title": "Neural networks and particle swarm for transformer oil diagnosis by dissolved gas analysis",
    "url": "https://openalex.org/W4395046515",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4316643739",
            "name": "Fettouma Guerbas",
            "affiliations": [
                "University of Sciences and Technology Houari Boumediene"
            ]
        },
        {
            "id": "https://openalex.org/A4227944375",
            "name": "Youcef Benmahamed",
            "affiliations": [
                "Polytechnic School of Algiers"
            ]
        },
        {
            "id": "https://openalex.org/A5095846236",
            "name": "Youcef Teguar",
            "affiliations": [
                "University of Sciences and Technology Houari Boumediene"
            ]
        },
        {
            "id": "https://openalex.org/A5111178919",
            "name": "Rayane Amine Dahmani",
            "affiliations": [
                "University of Sciences and Technology Houari Boumediene"
            ]
        },
        {
            "id": "https://openalex.org/A2988234862",
            "name": "MADJID TEGUAR",
            "affiliations": [
                "Polytechnic School of Algiers"
            ]
        },
        {
            "id": "https://openalex.org/A2131452449",
            "name": "Enas Ali",
            "affiliations": [
                "Chitkara University"
            ]
        },
        {
            "id": "https://openalex.org/A2153902082",
            "name": "Mohit Bajaj",
            "affiliations": [
                "Al-Ahliyya Amman University",
                "Graphic Era University"
            ]
        },
        {
            "id": "https://openalex.org/A4310936708",
            "name": "Shir Ahmad Dost Mohammadi",
            "affiliations": [
                "Alberoni University"
            ]
        },
        {
            "id": "https://openalex.org/A1999641301",
            "name": "Sherif S. M. Ghoneim",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2508454452",
        "https://openalex.org/W2134614003",
        "https://openalex.org/W2355911960",
        "https://openalex.org/W1568630407",
        "https://openalex.org/W3163540699",
        "https://openalex.org/W2295580010",
        "https://openalex.org/W3167593903",
        "https://openalex.org/W2110595612",
        "https://openalex.org/W2084880510",
        "https://openalex.org/W2148714802",
        "https://openalex.org/W849075951",
        "https://openalex.org/W1964572135",
        "https://openalex.org/W2074910498",
        "https://openalex.org/W2548153325",
        "https://openalex.org/W2792282766",
        "https://openalex.org/W2893368981",
        "https://openalex.org/W3005155019",
        "https://openalex.org/W3133138473",
        "https://openalex.org/W2516429987",
        "https://openalex.org/W3178027639",
        "https://openalex.org/W657338492",
        "https://openalex.org/W1969487735",
        "https://openalex.org/W1480600971",
        "https://openalex.org/W4253572765",
        "https://openalex.org/W2121482766",
        "https://openalex.org/W2109364787",
        "https://openalex.org/W2154929945",
        "https://openalex.org/W2118840131",
        "https://openalex.org/W6601462914",
        "https://openalex.org/W2187169456",
        "https://openalex.org/W2168747298",
        "https://openalex.org/W4245183779"
    ],
    "abstract": null,
    "full_text": "1\nVol.:(0123456789)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports\nNeural networks and particle \nswarm for transformer oil diagnosis \nby dissolved gas analysis\nFettouma Guerbas 1, Youcef Benmahamed 2, Youcef Teguar 1, Rayane Amine Dahmani 1, \nMadjid Teguar 2, Enas Ali 3, Mohit Bajaj 4,5,6*, Shir Ahmad Dost Mohammadi 7* & \nSherif S. M. Ghoneim 8\nThe lifetime of power transformers is closely related to the insulating oil performance. This latter can \ndegrade according to overheating, electric arcs, low or high energy discharges, etc. Such degradation \ncan lead to transformer failures or breakdowns. Early detection of these problems is one of the most \nimportant steps to avoid such failures. More efficient diagnostic systems, such as artificial intelligence \ntechniques, are recommended to overcome the limitations of the classical methods. This work \ndeals with diagnosing the power transformer insulating oil by analysis of dissolved gases using new \ntechniques. For this, we have proposed intelligent techniques based on Multilayer artificial neural \nnetworks (ANN). Thus, a multi-layer ANN-based model for fault detection is presented. To improve \nits classification rate, this one was optimized by a meta-heuristic technique as the particle swarm \noptimization (PSO) technique. Optimized ANNs have never been used in transformer insulating oil \ndiagnostics so far. The robustness and effectiveness of the proposed model is demonstrated, and high \naccuracy is obtained.\nKeywords Power transformer, Insulating oil, Diagnosis, Dissolved gas analysis, Electrical and thermal faults, \nArtificial neural networks, Particle swarm algorithm, Optimization\nPower transformers are crucial and essential components in the electrical power transmission and distribution \nnetworks. Such expensive electrical devices should work properly for  years1. The lifetime of a power transformer \nclosely depends on its insulation system, generally consisting of a traditional solid component (paper, etc.) and \na dielectric  fluid2.\nMost power transformers use insulating oil as dielectric fluid, due to its low price and good physico-chemical \nproperties. Besides insulation, this oil dissipates the heat generated by the magnetic circuit and the windings. \nFollowing its movement in a transformer in service, the insulating oil conducts this heat to the internal cooling \nsystems (radiators, etc.), before releasing it into the  environment2,3.\nInsulating oil is subjected to several electrical, thermal and chemical constraints in service. These latter lead \nto the gradual degradation of the insulating oil and eventually cause the transformer to de-energize when not \nanalyzed in  time4,5. Indeed, various oil analyses are proposed to diagnose the power transformer’s internal state. \nThe most popular are physico-chemical  analyzes2–4 and dissolved gas analysis (DGA)5–10.\nDGA is a widely used as diagnostic technique. It is based on interpreting the concentrations of gases dissolved \nin the insulating oil. Indeed, the oil decomposes under electrical and thermal stresses, releasing gases in small \n quantities5–10. In fact, DGA can be performed by introducing sensors into the transformers in service (online \nmode), or in the laboratory on samples (offline mode)8.\nOPEN\n1Laboratoire des Systèmes Electriques et Industriels, Université des Sciences et de la Technologie Houari \nBoumediene, BP 32Bab Ezzouar, 16111 Algiers, Algeria. 2Laboratoire de Recherche en Electrotechnique, Ecole \nNationale Polytechnique, 10 Rue des Frères Oudek, El Harrach, 16200 Algiers, Algeria. 3Centre of Research \nImpact and Outcome, Chitkara University Institute of Engineering and Technology, Chitkara University, \nRajpura, Punjab 140401, India. 4Department of Electrical Engineering, Graphic Era (Deemed to Be University), \nDehradun 248002, India. 5Hourani Center for Applied Scientific Research, Al-Ahliyya Amman University, Amman, \nJordan. 6Graphic Era Hill University, Dehradun 248002, India. 7Department of Electrical and Electronics, Faculty \nof Engineering, Alberoni University, Kapisa, Afghanistan. 8Department of Electrical Engineering, College of \nEngineering, Taif University, P .O. BOX 11099, 21944 Taif, Saudi Arabia.  *email: mb.czechia@gmail.com ; sh_\nahmad.dm@au.edu.af\n2\nVol:.(1234567890)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\nThe five main gases resulting from the oil decomposition are hydrogen  (H2), methane  (CH4), acetylene \n (C2H2), ethylene  (C2H4) and ethane  (C2H6). The proportions of the concentrations of these gases in a sample \nallow determining the defect  type5–10. According to IEC 60599 (2007)11 and IEEE Standard C57.10412, six elec-\ntrical and thermal faults exist. They consist of partial discharges (PD), low energy discharges (D1), high energy \ndischarges (D2), thermal faults for T < 300 °C (T1), thermal faults for T from 300 °C to 700 °C (T2) and, finally, \nthermal faults for T > 700 °C (T3).\nVarious traditional techniques have been developed to interpret the results of DGA of transformer  oil13,14. The \nmost popular use gas concentration ratios of  Dornenburg15,  Rogers16, and IEC 60599 (1978)17, or graphical meth-\nods of Duval employing percentages of concentration ratios such as the  triangle18 and the  pentagon19. Although \nthese techniques are simple and easy to implement, they have some drawbacks. First, they use only specific gas \nratios. Their accuracy remains limited and are very sensitive to DGA data uncertainties. For instance, Duval’s \ntriangle method showed certain PD detection failures and some interferences between thermal and electrical \nfaults. IEC 60,599 technique presented some interferences between D1 and D2 faults. Except T1, the effectiveness \nof Rogers’ methods has not been demonstrated for the other  faults6. Finally, Dornenburg’s method considers only \nthree faults: thermal decomposition, partial discharges or low energy corona, and high energy electric  arcs15.\nRecent artificial intelligence and meta-heuristic approaches have been integrated with traditional methods to \novercome such difficulties and improve the transformer oil diagnostic by DGA. Benmahamed et al. have devel-\noped two algorithms to improve the classification rate of the Duval pentagon (initially at 80%). The first (Duval \npentagon-SVM-PSO) combines the Duval pentagon and support vector machines (SVM), whose parameters \nhave been optimized by the particle swarm optimization technique, PSO. The second (Duval pentagon-KNN) \ncombines Duval pentagon and the K-nearest neighbors (kNN) algorithm.\nThe accuracy rate of the first algorithm is 88% compared to 82% for the  second20. In another research work, \nBenmahamed et al. established two classifiers KNN and Naïve Bayes (NB) to diagnose transformer oil by DGA. \nThe KNN algorithm provided the highest accuracy rate of 92% 21. Furthermore, Benmahamed et al. developed \ntwo classifiers. The first is Gaussian and the second (SVM-Bat) uses Support Vector Machines (SVM), whose \nparameters have been optimized by the Bat algorithm. The SVM-Bat accuracy rate is 93.75% against 69.37% \nfor the  Gaussian5. Kherif et al. developed an algorithm combining KNN with the decision tree principle. An \naccuracy rate exceeding 93% was obtained, demonstrating the effectiveness of the proposed  algorithm7. Taha \nand al proposed an approach using the particle swarm optimization and the fuzzy-logic (PSO-FS) to enhance \nthe of Rogers’ four-ratio diagnostic accuracy from 47.19 to 85.65% and IEC 60,599 one from 55.09 to 85.03%22. \nGhoneim et al. established an efficient teaching–learning based optimization (TLBO) a model to optimize both \ngas concentration percentages and ratios. The proposed algorithm allowed obtaining higher diagnostic accuracy \n(of 82.02%) than the best (78.65%) offered by other DGA techniques presented in the same  paper23. Ghoneim \net al. developed a smart fault diagnostic approach (SFDA) integrating Dornenburg, Rogers three and four-ratio, \nIEC three-ratio and Duval triangle techniques. Using gas concentrations, the SFDA algorithm has been improved \nby ANN. The SFDA algorithm allowed obtaining 79.6%. This accuracy rate has been improved to 87.8% with the \nintegration of  ANN24. It is worth noting that optimized ANNs have never been used in transformer insulating \noil diagnostic so far.\nIn order to improve the accuracy rate of faults detection in power transformers oil by DGA, multilayer artifi-\ncial neural networks (ANN) are developed, in this investigation. We opt for a multilayer neural network (MLP) \ncomprising an input layer, two hidden layers and an output. Several input vectors are tested, namely the five gases \nin ppm and in percentage, Dornenberg ratio, Rogers four-ratio and IEC 60,599 three-ratio as well as the combi-\nnation between the ratios of Rogers and those of Dornenberg, the centers of mass of the triangle and pentagon \nof Duval, as well as their combination. Various learning algorithms and activation functions are also considered.\nTo further improve diagnostic rates, neural networks are optimized using the particle swarm technique \n(PSO). Indeed, different population sizes have been adopted. The performances of these neural networks have \nbeen studied in terms of accuracy rate. A total of 481 sample datasets are  considered5. Two-thirds are used for \nthe training process (so 321 samples), and the rest (160 samples) for the test. The six fault classes (PD, D1, D2, \nT1, T2, and T3) recommended by IEC 60599 (2007)11 and IEEE Standard C57.10412 are adopted. A comparative \nstudy is carried out between the different neural networks developed.\nIn this paper, we have demonstrated that the combination of the artificial neural network and the particle \nswarm optimization leads to global model. This one takes into account classification problem by learning and \nalso optimization by the PSO. It gives significant results versus to the classical methods and we obtain a high \nlevel of accuracy.\nANN models for insulating oil diagnostic by DGA\nThe state of the power insulation system is responsible for determining the lifetime of the transformers. This \ninsulation system is generally exposed to some constraints resulting from overheating, carbonization of the paper, \nelectric arcs and low or high energy discharges. Such faults can accelerate insulation degradation, affecting the \ntransformer’s reliability and lifetime. Indeed, early detection of these faults can prevent undesirable abnormal \noperating conditions or failures of power transformers. The dissolved gas analysis (DGA) technique is considered \nto be one of the fastest and most economical techniques widely used to diagnose power transformer fault  types13.\nAs mentioned above, traditional fault diagnostic techniques in power transformers have generally shown their \nlimitations and inconsistencies. Despite their simplicity, these techniques are not really adopted by the scientific \ncommunity, due to their low accuracy rate in faults detection, except Duval pentagon method giving acceptable \nrate faults  classification6,20. That is why artificial intelligence (AI) and/or meta-heuristic approaches can be com-\nbined with conventional ones to improve the diagnostic accuracy of power transformer insulating oil further.\n3\nVol.:(0123456789)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\nThis section proposes several artificial neural networks to detect faults in an oil-immersed power transformer. \nThese networks have the same architecture (structure) consisting of a multilayer perceptron (MLP). However, \ntheir training algorithms and activation functions are different.\nThe artificial neural network connection is multilayer and consists of an input, output, and two hidden inter-\nnal layers. In this structure, neurons belonging to the same layer are not connected. Each layer receives signals \nfrom the previous layer and transmits its processing result to the next layer. Thus, the information flows in a one \ndirection, from the input to the output through the hidden layers. Such connection type is called feed-forward \nartificial neural networks (FFANNs)25.\nThe number of neurons in the input layer is equal to the number of elements of the input vector denoted B. \nNine models with input vectors have been considered, namely:\n• 1st Model: The database comprises the concentrations of the five gases in ppm. In such conditions, the input \nvector of this model is given by: B =  [H2  CH4  C2H2  C2H4  C2H6]T\n• 2nd Model: For the same database, the input vector of this model for each sample can be written in terms in \ngas concentrations in percentages as follows: B = [%H2%CH4%C2H2%C2H4%C2H6]T, with:\n• 3rd Model: Dornenburg ratios is one of the first techniques introduced for the power transformer oil diag-\nnostic to interpret the results of dissolved gas  analyzes15. In this model, we use four ratios of gas, in ppm, \nconsisting of B =  [CH4/H2  C2H2/C2H4  C2H4/C2H6  C2H2/CH4]T\n• 4th Model: The four ratios of Rogers are also considered. For this model, the input vector can be expressed, \nfor each sample (gas in ppm), by Ref.16 A =  [CH4/H2  C2H2/C2H4  C2H4/C2H6  C2H6/CH4]\n• 5th Model: IEC 60,599 model uses three ratios of gas in ppm as  follows17:\n• 6th Model: Duval triangle model is a graphic representation using the percentage ratios of three dissolved \ngases:  CH4,  C2H2 and  C2H4. These percentages are employed as coordinates  (Tx,  Ty) to draw the mass center \npoint in the triangle and identify, for each sample, the defect zone in which it is  located18. The input vector \nfor this model is therefore written by: B =  [Tx  Ty]T\n• 7th Model: Duval pentagon model is a graphic representation similar to triangle one. Pentagon uses the five \ndissolved gas in percentages (%H 2%CH4%C2H2%C2H4%C2H6) to draw the mass center point coordinates \n (Px,  Py)19. The input vector of this model is given by: B =  [Px  Py]T\n• 8th Model: A combination between the triangle and the pentagon of Duval was proposed for this model. In \nsuch conditions, the two mass centers coordinate of both triangle and pentagon of Duval will constitute the \ninput vector, given by: B =  [Cx  Cy  Px  Py]T\n• 9th Model: We suggest here another model consisting of combination between Rogers and Dornenburg \nratios. The input vector of this model can be written as: B =  [CH4/H2  C2H2/C2H4  C2H4/C2H6  C2H2/CH4  C2H6/\nCH4]T\nThe coordinates  Tx and  Ty in vector 6 are calculated, for each gas sample, as  follows18:\nA is the irregular triangle area given by:\nThe coordinates  xi and  yi(i = 0 to n − 1 with n = 3 is the number of gas in percentages) are computed as follows:\n(1)%H 2 =H 2/(H 2 + CH 4 + C2H 2 + C2H 4 + C2H 6) × 100\n(2)%CH 4 = CH 4/(H 2 + CH 4 + C2H 2 + C2H 4 + C2H 6) × 100\n(3)%C 2H 2 = C2H 2/(H 2 + CH 4 + C2H 2 + C2H 4 + C2H 6) × 100\n(4)%C 2H 4 = C2H 4/(H 2 + CH 4 + C2H 2 + C2H 4 + C2H 6) × 100\n(5)%C 2H 6 = C2H 6/(H 2 + CH 4 + C2H 2 + C2H 4 + C2H 6) × 100\nB =[ CH 4/H 2C2H 2/C2H 4C2H 4/C2H 6]T\n(6)Tx = 1\n3A\nn−1∑\ni=0\n(xi + xi+1)\n(\nxiyi+1 − xi+1yi\n)\n(7)Ty = 1\n3A\nn−1∑\ni=0\n(\nyi + yi+1\n)(\nyixi+1 − yi+1xi\n)\n(8)A = 1\n2\nn−1∑\ni=0\n(\nxiyi+1 − xi+1yi\n)\n(9)x0 = %CH 4cos(π/ 2)\n4\nVol:.(1234567890)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\nwhere α = 2π/3.\nFor the 7th Model, each sample, the coordinates  Px and  Py are computed by:\nThe pentagon surface Air given by:\nThe parameters  xi and  yi (i = 0 to n − 1 with n = 5 is the gas number) are expressed by:\nwhere α = 2π/5.\nOther possible combinations of the above technique have also been proposed to give strong credibility to the \nobtained results. Two combinations are given below.\nUsing a large number of hidden layers is not recommended. Most classification standards problems use only \none or at most two hidden  layers26. Such ascertainments have been confirmed during our modeling. After sev-\neral attempts, the best results have been achieved for two hidden layers of ten neurons of each. In addition, we \nhave considered one output delivering, for each gas sample, a single fault (PD, D1, D2, T1, T2 or T3). Indeed, \nwe introduce only the number of input neurons varying from 2 to 5 according to the elements number of the \ninput vector. Thus, the topology of the multilayer neural networks we adopted in this work is shown in Fig. 1.\nThe weighing parameters of the layers are calculated using learning algorithms. Many algorithms are pro-\nposed in the  literature25,27. We can find the supervised and unsupervised ones. In this paper, we are interested by \nthe supervised ones. Supervised learning is done by introducing pairs of inputs and their desired outputs. This \nlearning uses an optimization criterion allowing it to find optimal synaptic weights giving the desired behavior \nusing random samples. Several learning techniques are adopted to readjust  weights27. One of the most widespread \nalgorithms is the “Back propagation” . Unfortunately, this algorithm suffers from the local minimum problem. A \nvariant of the previous method consists of choosing an appropriate displacement to accelerate the convergence of \nthe algorithm, which then leads to fast back propagation with momentum. There is another version called Robust \n(10)x1 = %C 2H 4cos(π/ 2 + α )\n(11)x2 = %C 2H 2cos(π/ 2 + 2α)\n(12)y0 = %CH 4sin(π/ 2)\n(13)y1 = %C 2H 4sin(π/2 + α)\n(14)y2 = %C 2H 2sin(π/2 + 2α)\n(15)Px = 1\n6A\nn−1∑\ni=0\n(xi + xi+1)\n(\nxiyi+1 − xi+1yi\n)\n(16)Py = 1\n3A\nn−1∑\ni=0\n(\nyi + yi+1\n)(\nyixi+1 − yi+1xi\n)\n(17)A = 1\n2\nn−1∑\ni=0\n(\nxiyi+1 − xi+1yi\n)\n(18)x0 = %H 2cos(π/ 2)\n(19)x1 = %C 2H 6cos(π/ 2 + α )\n(20)x2 = %CH 4cos(π/ 2 + 2α)\n(21)x3 = %C 2H 4cos(π/ 2 + 3α)\n(22)x2 = %C 2H 2cos(π/ 2 + 4α )\n(23)y0 = %H 2sin(π/ 2)\n(24)y1 = %C 2H 6sin(π/2+ α)\n(25)y2 = %CH 4sin(π/2 + 2α)\n(26)y3 = %C 2H 4sin(π/2+ 3α)\n(27)y2 = %C 2H 2cos(π/2 + 4α)\n5\nVol.:(0123456789)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\nBack propagation applicable in the stochastic case. In order to improve the choice of the direction to take in the \nweight space, we use to second-order optimization methods of the objective function specified by the Hessian.\nMany learning algorithms, existing in the Matlab toolbox, have been tested. We have chosen three algorithms \nusing the back-propagation of the quadratic error and only one using back-propagation of the gradient of the \nquadratic  error27. This latter is between the response calculated by the network and the desired one.\nIn fact, back-propagation or gradient back-propagation algorithms are the most widely used models in super-\nvised training. For this, several models are presented in the Matlab toolbox. These models are used to adjust the \nweights and biases satisfying the quadratic error or its gradient between the output value and the desired one \nreaches minimum value for every gas sample. The selected training algorithms are as follows:\n• Levenberg–Marquardt back-propagation algorithm (trainlm).\n• One-step secant back-propagation algorithm ( trainoss).\n• Resilient backpropagation algorithm (trainrp).\n• Scaled conjugate gradient back-propagation algorithm ( trainscg).\nThe choice of a specific algorithm depends on the input vector and the cost function. There is no theoretical \nmethod to select one algorithm versus another. In our case, we have used four training algorithms for different \nDGA techniques.\nThe activation function transforms the unbounded signal into a bounded one. This function is chosen non-\ndecreasing monotonic. Increasing the input can only increase the output or keep it constant. Choosing a linear \nactivation function makes calculation easier, but the neuron loses its robustness. A nonlinear activation func -\ntion increases the network’s ability to approximate complex functions. There are three categories of activation \nfunctions. The first one allows to distinguish between differentiable functions (sigmoid, tangent, hyperbolic) \nand non-differentiable ones (threshold function, thresholding). The second category concerns the functions that \nhave significant values around zero, and significant values far from this one. The third ones deal with difference \nbetween positive functions and functions with zero mean (0 and 1 or – 1 and 1).\nFive activation (transfer) functions are selected from the Matlab toolbox: softmax, radbas, purelin, logsig \nand poslin. Several attempts have been made to obtain the best diagnostic rates. The best combinations have \nbeen found when using softmax in the second hidden layer and purelin in the output layer. Indeed, the five \naforementioned activation functions are applied only in the first hidden layer, maintaining, softmax and purelin \nfor the second hidden layer and the output one  respectively30.\nThe definitions of the selected activation functions  are30:\n• Normalized Exponential (softmax): It is a normalized function, which takes as input vector B of m elements \nand gives a vector of K strictly positive numbers whose sum equals 1. This function is defined by:\n• Radial Basic (radbas): it consists of the radial basic transfer function of Gaussian type, given by:\nr being a real replacing the Euclidean distance neuron-center30.\n• Linear (purelin): this activation function is linear.\n(28)f(xj) = exj\n∑m\ni=1 exi\nwith j = 1, 2, ..., m\n(29)f (r) = e−r2\nFigure 1.  Topology of the MLP networks adopted in this study.\n6\nVol:.(1234567890)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\n• Hyperbolic Tangent Sigmoid (tansig): It consists of the hyperbolic tangent sigmoid activation function defined \nby:\n• Rectified Linear Unit Layer (poslin). It is a rectified linear unit layer activation function. For an input value \nx, this function can be expressed by:\nThe choice of the activation functions depends on the input data. These functions modify the data values \nfrom the input through the output. The combination of them leads to avoid the elimination of the information. \nSo, it is necessary to proceed by some beginning choice of these functions. According to our experience, it is \nmore convenient to make a combination of these functions from the input layer to the output one. In the hidden \nlayer, a smoothie function is recommended to keep the information of the signal without truncation. This is very \nimportant for decision process and exploration of all the range of the input data.\nANN-PSO models for insulating oil diagnostic by DGA\nOur optimization problem aims to find the best solution, consisting of the global optimum, among a set of solu-\ntions belonging to the search space, by minimizing the MSE as function objective.\nIn fact, meta-heuristic optimization methods have the advantage of being adapted for a wide range of prob-\nlems without major modification of their  algorithms28. These methods are based on populations of  solutions29. \nThey are often inspired by natural processes, in particular by the theory of evolution in animal and insect socie-\nties which relate to evolutionary biology such as Genetic Algorithms (GA)30, or to the ethological theory such as \nParticle Swarm Optimization (PSO)31, Ant Colony Optimization (ACO)32, Social Spiders Optimization (SSO)33,34.\nIn our investigation, we opted for the PSO technique. To this end, we first present its principle, elements and \nparameters. We subsequently present the approach undertaken for optimizing the training of the ANN using \nPSO.\nParticle swarm optimization is based on a homogeneous set of particles, initially arranged randomly. These \nparticles move in the search space and each constitutes a potential solution. Each particle memorizes its best \nvisited solution and communicates with the nearby particles. Thus, the particle will follow a trend based, on \none hand, on its desire to return to its solution optimal, and, on other hand, on its mimicry with respect to the \nsolutions found in its neighborhood on other hand. Indeed, from the local optima, the set of particles converges \ntowards the optimal global solution of the treated  problem35.\nTo be able to apply the PSO algorithm, one must define a search space of the particles and an objective func-\ntion to be optimized. The principle is to move these particles to find the optimum. Each particle  contains36:\n• A position characterized by its coordinates in the definition space of the objective function: \nX i = (X i1 ,... ,X ij,..., X ik)\n• A velocity allowing the particle to change position during the iterations according to its best neighborhood, \nits best position, and its previous position: V i = (V i1 ,..., V ij,..., V ik)\n• A neighbourhood constituted by the set of particles directly interacting on the particle, in particular the one \nhaving the best value of the objective function.\nAt any moment, each particle knows:\n• Its best visited position Pi(t) through its coordinates and the value of the objective function;\n• The position of the best neighbour of the swarm gi(t) which corresponds to the optimal scheduling;\n• The value assigned to the objective function f (Pi(t)) at each iteration following the comparison between the \nvalue of this function given by the current particle and the optimal one.\nThe particle swarm algorithm is based on:\n• Population size: According to Van den Bergh and  Engelbrecht35, increasing swarm size slightly improves the \noptimal value. Eberhart and  Shi36 illustrated that population size has a minimal effect on the performance of \nthe EP method. The same observation was made by Nezhad and his  colleagues37. In our investigation, various \npopulation sizes Np, namely Np = 40, 80, 100 and 120.\n• Initialization of position and velocity: Before generating the population of particles, it is necessary to define \nthe search space for them and place them randomly according to a uniform distribution. In a d-dimensional \nsearch space, the particle i of the swarm is modelled by its position vector  Xi according to Eq. (33), and by \nits velocity vector Vi according to Eq. (32)38.\n• Position and velocity update: The quality of the particle position is determined by the value of the objective \nfunction. Along its path, this particle memorizes its best position, which we note p best = (pi1,…, pi2,…, pid). \nFurthermore, the best position found for its neighbouring particles is g best = (gi1,…, gi2,…, gid). At each itera-\n(30)f(x ) = 2\n1 + e−2x − 1\n(31)f(x) =\n{\nx , x ≥ 0\n0, x < 0\n7\nVol.:(0123456789)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\ntion, the particles update their positions and velocities taking into account their best positions and those of \nits  neighbourhood39.\nThe new velocity is calculated by Ref.41:\nTherefore, the new position velocity is calculated as  follows40:\nXi(k), Xi(k + 1): the positions of the Pi particle at iteration k and k + 1 respectively ; Vi(k), Vi(k + 1): the veloci-\nties the Pi particle at iteration k and k + 1 respectively; pbest(k + 1): the best position obtained by the Pi particle at \niteration k + 1; gbest(k + 1): the best position obtained by the swarm at iteration k + 1; c1 et c2: constants representing \nthe acceleration coefficients; r1 et r2: random numbers; w(k): inertial weight.\nMethods\nIn the previous ANN presentation, each neural network delivers the best classification rate by minimizing the \nroot mean quadratic error, MSE, estimated from the computed outputs and the desired ones, as a function of \nsynaptic weights and biases.\nThe biases consist of  bh1,  bh2 and  bo.  bh1 and  bh2 are vectors of 10 elements each. The first is presented to the \nfirst hidden layer, and the second to the second hidden layer.  bo is an additional scalar added to the output layer.\nThe synaptic weights consist of  wi between the input layer and the first hidden layer,  wh inter-hidden layers, \nand finally  wo between the second hidden layer and the output one.\nwi is a matrix having a number of rows equal to that of input vector elements (m varying from 2 to 5). Moreo-\nver, the number of columns of  wi is equal to the number of neurons of the first hidden layer, i.e. q  = 10. In this \nhidden layer, we applied an activation function,  f1. The five previous functions (softmax, radbas, purelin, tansig \nand poslin) have been tested for this latter.\nwh linking the first hidden layer to the second, is also a square matrix of dimension qxq (i.e. 10 × 10). The \nactivation function, denoted  f2, applied in the second hidden layer, is sotfmax. As previously indicated, this \nactivation function has been kept unchanged throughout the diagnostic process.\nLikewise,  wo connecting the second hidden layer and the output layer, is also a matrix of dimension 1 × 10 \n(i.e. a vector of 10 elements). The activation function, denoted  f3, which has been adopted in the output layer is \npurelin. This latter has also kept unchangeable. In such conditions, the output value is given by:\nf1,  f2 and  f3 are the activation functions,  bh1,  bh2 and  bs the biases and B the input vector.\nThe mean value of the mean squared error has been used as the objective function giving by the following \nexpression:\nwhere TS is the total number of training samples (equal to 321), Y is the computed network output, and  Yd is \nthe desired output.\nOur contribution in this paper that we have proposed the new model for DGA by combining the artificial \nneural network and particle swarm optimization algorithm which gives significant results with very high accu-\nracy. This hybridization has never been developed until now.\nTaking into consideration the mathematical calculation of the PSO model in the previous section, the con-\nvergence of the PSO towards the global optimum depends on the following parameters:\n• Inertia factor: The inertia factor w allows controlling the impact of previous velocities on the actual  one41.\n– If w <  < 1, rapid changes of direction are possible; little of the previous velocity is preserved;\n– If w = 0, the particle moves in each step without knowledge of the previous velocity; the concept of \nvelocity is completely lost;\n– If w > 1, the particles barely change their direction, which results a great area of exploration and a hesita-\ntion against convergence towards the optimum.\n• Acceleration coefficients c1 et c2: The constant  c1 affects the acceleration of the particle towards its best perfor-\nmance (cognitive behaviour of the particle). Otherwise,  c2 allows the particle to accelerate towards the Global \nBest (social ability of the particle) 42. These constants belong to the interval [0;  2]39. In our investigation, c 1 \nequals 2 and c2 equals 1.\n• Random numbers r1 et r2: At each iteration, the two parameters r1(k) and r2(k) are generated randomly in the \ninterval [0; 1] by a uniform  distribution41.\n• Stopping criterion: In order to converge towards the global optimal solution, different stopping criteria can \nbe selected. The most commonly used consist  of42:\n– Static criterion: it is generally based on the maximum number of iterations;\n(32)V i(k + 1) = wV i(k) + c1r1(k)\n(\npbest(k) − X i(k)\n)\n+ c2r2(k)\n(\ngbest(k) − X i(k)\n)\n(33)Xi(k + 1) = Xi(k) + V i(k + 1)\n(34)Y = f3(\n∑\nw o ∗f2(\n∑\nw h ∗f1(\n∑\nw i ∗B) + bh1) + bh2) + bo\n(35)MSE = 1\nTS\n∑\nPT\n(Yd − Y)\n2\n8\nVol:.(1234567890)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\n– Dynamic criterion: it refers to the stagnation of velocity.\nThe static criterion has been adopted in our study. The number of iterations has been fixed to 10,000.\nA high value of the inertia factor facilitates exploration (the search for new sectors). A low value facilitates \nexploitation (favoring the current sector of research) 42. Better convergence, providing the balance between \nexploration and exploitation. Furthermore, it is possible to vary this factor during the iterations according to \nEq. (36). Good results were obtained for an increase value from 0.4 to 0.9.\nwmax: the maximum value of w (= 0.9); wmin: the minimum value of w (= 0.4); iter: the current iteration; maxiter: \nthe maximum number of iterations.\nBefore applying the PSO to optimize the ANN training, first, it is necessary to choose the architecture (topol-\nogy) of the ANN (of two hidden layers with 10 neurons for each layer), the training algorithms, the activation \nfunctions, PSO parameters (population sizes, maximum number of iterations, variables to optimize, etc.), the \ndatabase as well as the number of samples reserved for training and testing. Next, it is necessary to determine \nthe objective function to be optimized. This function consists of the mean square error given by Eq. ( 35).\nThe PSO algorithm is employed in training the ANN to determine the set of parameters w  and b. The total \nnumber (corresponding to search space dimension) of these parameters can be determined by the following \nequation:\nsi: input vector dimension.\nThe execution of the ANN-PSO algorithm is carried out in accordance with the following steps:\n• Step 1: Upload the data training set and the data test one.\n• Step 2: Define the architecture of ANN: number of hidden layers, neurons number, train algorithm and \nactivation function.\n• Step 3: Determine the PSO algorithm parameters and randomly generate p  particles.Each one containsN \nvalues of weights and biases, and generate, then after, NpANN models.\n• Step 4: Train the Np ANN and calculate objective functionMSE(p values) using weighs and biases generated \nby PSO.\n• Step 5: Select the best solution and update it if it is different from the previous iteration.\n• Step6: Check the criterium iteration.\n• Step 7: If the condition of step 6 is not verified, return to step 4 with updating the particles velocities and \npositions. Else, if the condition of step 6 is verified, the optimal parameters are used to test the ANN.\nWe have compared in the same conditions, the performance of the two algorithms. Indeed, the same mul-\ntilayer topology used has been kept for this part. Also, we have adopted the input vector 9 (of the coordinates \nof the two centers of mass of triangle 1 and pentagon I of Duval) offering the best classification performance of \n90% for neural network. This result was obtained using trainlm as training algorithm, and poslin, softmax and \npurelin as activation functions respectively in the first hidden layer, the second hidden one and in the output \none. This training algorithm-activation functions combination has been kept in the second algorithm, in order to \nfurther improve the fault classification rate. Also, we have considered the same database of 481 samples including \n321 samples (set of 66%) for training and 160 (set of 33%) for testing, the same gases  (H2,  CH4,  C2H2,  C2H4 and \n C2H6), and the same defects (PD, D1, D2, T, T2, and T3).\nIn order to ensure the convergence of the objective function (the mean square error) towards an optimum, \nfour population sizes of the particle swarm have been adopted, namely 40, 80, 100 and 120.\nThe search space dimension, corresponding to the number of parameters w  and b,has been fond equal to \nN = 171. The parameters of the PSO algorithm have been set as described in the Table 1.\nResults\nFor ANN results, 180 programs of neural networks have been developed from the nine input vectors, four train-\ning algorithms and five activation functions. The database contains 481 samples. For each network (model), 66% \n(i.e. 321 gas samples) have been selected for training and the rest (160 samples) for the test.\n(36)w (k) = w min + (w max − w min )\n( k\nmax iter\n)\n(37)N =\n(\nsi ∗ q + bh1\n)\n+\n(\nq ∗ q + bh2\n)\n+ q + bo\nTable 1.  PSO algorithm parameters values.\nParmeter Descriotion Value\nc1 Particle coefficient 2\nc2 Swarm coefficient 1\niter_max Maximum iteration 10,000\nN Search space dimension 171\n9\nVol.:(0123456789)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\nThe performance of each neural network has been evaluated in terms of classification or diagnosis rate. For \nthis, the number of iterations adopted is 1000. Each neural network has been executed 50 times, and the best \ndiagnosis rate has been recorded.\nThe obtained results are presented as histograms of Figs.  2, 3, 4 and 5. These figures illustrate the different \ndiagnostic rates as a function of the activation function of the first hidden layer, for different training algorithms \npurelinr adbass oftmax tansig poslin\nLevenberg-Marquardt back-propagation:trainl m\n0\n20\n40\n60\n80\n100Accuracy rate (%)\nGasi n%\nGasi np pm\nRogers\nDornenburg\nIEC6 0599\nTriangle\nPentagone\nTriangle-Pentagone\nRogers-Dornenburg\nFigure 2.  Accuracy rate-activation function of the first hidden layer, for trainlm training algorithm.\nFigure 3.  Accuracy rate-activation function of the first hidden layer, for trainoss training algorithm. \nFigure 4.  Accuracy rate-activation function of the first hidden layer, for trainrp training algorithm.\n10\nVol:.(1234567890)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\ntrainlm, trainrp, trainoss and trainscg, respectively. Note that the softmax and purelin have been used as activa-\ntion functions in the second hidden layer and the output one, respectively.\nIn fact, the four figures were drawn using twenty tables (five tables per figure). To avoid burdening the manu-\nscript, we prefer to present only one Table 2 employing purlin algorithm for the input layer (for the first algorithm \nPurlin of Fig. 2). Such tables provide more details of the results in terms of activation functions for different input \nlayers, accuracies and input vectors. This allows a better comparison between the considered models.\nWe are interested, through the results of Figs.  2, 3, 4 and 5, to determine the highest classification rate for \neach model. The input vectors can be classified in decreasing order of the classification rate (i.e. from best to \nbad) as follows:\n• 8th Model gives a maximum rate of 90%, i.e. 144 faults well classified on 160 (reserved for the test).\n• 2nd Model gives a maximum rate of 89.375% corresponding to 143 well-classified faults.\n• 1rt Model gives a maximum rate of 77.5%, i.e. 124 well-classified faults.\n• 6th Model gives a maximum rate of 60.625% is 97 well-classified faults.\n• 3rd Model and 5th model of IEC 60,599 ratios have given a maximum rate of 59.375%, so 95 are well-classified \nfaults.\n• 9th Model has given a maximum rate of 58.125%, i.e., 93 well-classified faults.\n• 4th Model gives a maximum rate of 54.375%, corresponding to 87 well-classified faults.\nIn order to provide all information of such classification, we present in the Table  3 the accuracy rate in \ndescending order (from best to bad) for all input vectors with the activation functions of the first hidden layer \nas well as the training algorithms. Note that the activation functions in the second hidden layer and the output \none are softmax and purelin respectively.\nFor PSO results, the variation of the objective function, consisting of the mean squared error (MSE), as a \nfunction of the number of iterations, for different population sizes is presented in Fig. 6.\nDepending on the population size, the minimum mean square error, the precision rate and the number of \nwell-classified faults, are summarized in Table 4.\nFigure 5.  Accuracy rate-activation function of the first hidden layer, for trainscg training algorithm.\nTable 2.  Details for Purelin algorithm by ANN in Fig. 2.\nActivation functions\nAccuracy Levenberg–Marquardt (%) Inpit vectorsInput layer Hiden layer Output layer\nPurelin linear transfer Softmax normalized exponnetial Purelin linear transfer\n89.38 Five gazes in pourcentage\n77.50 Five gazes in ppm\n54.38 Rogers’s Four ratio\n59.38 Dornenburg\"s Four ratio\n59.38 LCEI 60,599 Three Ration\n55.00 Duval’s Triangle\n86.25 Duval’s Penatgone\n89.38 Duval’s Triangle & Pen-\ntagone\n55.63 Rogers &Dornenburg’s \nratio\n11\nVol.:(0123456789)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\nDiscussion\nThe training algorithm trainlm contributed in the elaboration of these results by approximately 78%, since it has \nbeen used 7 times on 9. Otherwise, the activation functions purelin and poslin have participated in these results \nof about 56% (5 on 9) and 44% (4 on 9) respectively. Indeed, for such classification type, it is recommended to \nuse trainlm as training algorithm, purelin or poslinas as activation function in the first hidden layer, softmax for \nthe second hidden one and purelin in the output one.\nThe best classification rate of 90%, i.e. 144 well-classified faults on 160), has been obtained by presenting the \ninput vector consisting of the coordinates of the two centers of mass of the triangle and the pentagon Duval, \nusing trainlm as training algorithm and poslin, softmax and purelin as activation functions in the first hidden, \nthe second hidden and the output layers respectively. This accuracy rate will be improved, in the same conditions, \nusing particle swarm optimization (PSO) technique.\nFor a given population size, and over the iterations, Fig. 6 shows that the MSE (objective function) decreases \nabruptly from 0 to 120 iterations, and slowly elsewhere, tending practically towards a constant level. This latter \nis called minimum MSE which could represent the global MSE. It changes from one population to another, as \nillustrated in Table 3. In fact, with the progressive increase in the population size from 40 to 120, the global MSE \nslightly decreases from 0.0216 to 0.0066, while the classification rate and therefore the number of well-classified \nfaults slightly increases from 154 to 159 reserved for the test. Indeed, the 120-population size allows obtaining \n159 well-classified faults out of 160 (reserved for the test) with an accuracy rate of 99.375% against 90% when \nusing ANN alone.\nTable 3.  Accuracy rates in descending order with activation functions of the first hidden layer and training \nalgorithms.\nVe ctor Accuracy rate (%) Classified faults/160 Training algorithm Activation function of the 1st hidden layer\n8 90 144 Trainlm Poslin\n2 89.375 143 Trainlm Purelinor poslin\n7 89.375 143 Trainoss Poslin\n1 77.500 124 Trainlm Purelin\n6 60.625 97 Trainrp Tansig\n3 59.375 95 Trainlm Purelin\n5 59.375 95 Trainlm Purelin\n9 58.125 93 Trainlm Poslin\n4 54.375 87 Trainlm Purelinor radbas\n0 1000 2000 3000 4000 5000 6000 7000 8000 9000 10000\n0\n0.05\n0.1\n0.15\n0.2\n0.25\n0.3\n0.35\n0.4\nIterations\nMSE\nNp =1 20\nNp =1 00\nNp =8 0\nNp =4 0\nFigure 6.  Variation of MSE as a function of the number of iterations, for different population sizes.\nTable 4.  Diagnosis accuracy with several population sizes.\nPopulation size 40 80 100 120\nGlobal MSE 0.0216 0.0197 0.0173 0.0066\nAccuracy rate (%) 96.25 98.125 98.750 99.375\nNumber of well-classified samples/160 154 157 158 159\n12\nVol:.(1234567890)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\nConclusion\nIn this investigation, we have developed intelligent techniques using multilayer feed-forward ANN-based models \nfor fault detection in an oil-immersed power transformer, by analysis dissolved gases. Nine input vectors have \nbeen used. Otherwise, each hidden layer contains ten neurons. Finally, the output having only one neuron deliv-\ners a single fault for each gas sample.\nUsing back-propagation, four training algorithms have been chosen, namely trainlm, trainoss, trainrp and \ntrainscg. In addition, five activation functions, consisting in softmax, radbas, purelin, tansig and poslin, have \nbeen selected. These functions have been applied for the first hidden layer, while softmax was maintained for the \nsecond hidden layer, and purelin for the output layer. The used database contains 481 samples of which 321 have \nbeen selected for training and the rest (160 samples) for testing. Inspired by IEC and IEEE standards, six faults, \nconsisting in PD, D1, D2, T1, T2, and T3, have been adopted. The best classification rate of 90% (i.e. 144 well-\nclassified faults out of 160) has been obtained when using the eighth input vector (formed from the coordinates \nof the two centers of mass of triangle 1 and pentagon I of Duval) and applying trainlm as learning algorithm.\nIn order to further improve the best classification rate, the corresponding multilayer network has been opti-\nmized using particle swarm technique for various population sizes, namely 40, 80, 100 and 120. The mean square \nerror (MSE) represents the objective function to be minimized for 10,000 iterations. Obviously, the same data-\nbase with the same number of samples for training and testing, and the same faults has been kept. For a given \npopulation size, the mean square error (the objective function) decreases abruptly for iterations ranging from \n0 to 120, and slowly elsewhere, tending towards a constant level representing the minimum mean square error \n(MMSE). Furthermore, the gradual increase in the population size from 40 to 120 results in a slight decrease in \nthe minimum squared error from 0.0216 to 0.0066, and a slight increase in the classification rate of 96.250 (cor-\nresponding to 154 faults well classified out of 160) at 99.375% (with 159 well classified faults). In other words, \nthe best fault classification rate of 99.375% has been obtained for 120 population size. In these conditions, the \nANN-PSO algorithm was able to detect 159 faults out of 160 reserved for the test.\nFinally, our contribution in the paper is to present a new approach by combining learning and particle swarm \noptimization in dissolved gas analysis field. We have demonstrated that this technique is leading to significant \nresults comparing them to the existing ones in the previous research. We have obtained high level of decision \nabout the quality of the transformer oil by using different methods according to IEC and IEEE standards.\nIn high voltage distribution, this new model facilitates the maintenance process and avoids transformer fail-\nure. It gives us the instantaneous decision about the characteristics of the failure and time life of the transformer. \nThis latter may include smart sensors linked to digital process unit with supervisory and data acquisition system. \nThe advantage to use this model is to control in real time the process by minimum time calculation of the deci-\nsion. It leads time saving and minimum cost.\nIt is worth noting that the best results are obtained with the ANN-PSO model. This hybridization is the key to \nreach this objective. The choice of the architecture of the neural network was the crucial phase in our study. Also, \nthe combination of different training algorithms and activation functions allows obtaining the best model with \nseveral tests. The ANN-PSO model needs many calculations to have the convergence of the algorithm. However, \nit is necessary to match the obtained models to the corresponding oil analysis technique. As input vector, the \ngraphical methods of Duval give decision with best score.\nIn order to use our technique in other field, it is necessary to adapt the architecture of the neural network to \nthe problem. It means that we can choose the input layer and the output layer according to the proposed prob-\nlem. After that, the number of the hidden layer can be fixed according to the accuracy of the result. Finally, it is \nimportant to have a deep knowledge of the application that we want to use the model developed in this paper.\nData availability\nThe datasets used and/or analysed during the current study available from the corresponding author on reason-\nable request.\nReceived: 14 January 2024; Accepted: 18 April 2024\nReferences\n 1. Sun, L. et al. An Integrated decision-making model for transformer condition assessment using game theory and modified evidence \ncombination extended by D numbers. Energies 9(9), 697 (2016).\n 2. Abdi, S., Boubakeur, A., Haddad, A. & Harid, N. Influence of artificial thermal aging on transformer oil properties, electric power \ncomponents and systems. Electr. Power Compon. Syst. 39, 1701–1711 (2011).\n 3. N’ cho, J. S., Fofana, I., Hadjadj, Y . & Beroual, A. Review of physicochemical-based diagnostic techniques for assessing insulation \ncondition in aged transformers. Energies 9(5), 697 (2016).\n 4. Guerbas, F ., Zitouni, M., Boubakeur, A. & Beroul, A. Barrier effect on breakdown of point-plane oil gaps under alternating current \nvoltage. IET Gener. Transm. Distrib. 4(11), 1245–1250 (2010).\n 5. Benmahamed, Y ., Kherif, O., Teguar, M., Boubakeur, A. & Ghoneim, S. S. M. Accuracy improvement of transformer faults diag-\nnostic based on DGA data using SVM-BA classifier. Energies 14, 2970 (2021).\n 6. Ghoneim, S. S. M. & Taha, I. B. M. A new approach of DGA interpretation technique for transformer fault diagnosis. Int. J. Electr. \nPower Energy Syst. 81, 265–274 (2016).\n 7. Kherif, O., Benmahamed, Y ., Teguar, M., Boubakeur, A. & Ghoneim, S. S. M. Accuracy improvement of power transformer faults \ndiagnostic using KNN classifier with decision tree principle. IEEE Access 9, 81693–81701 (2021).\n 8. Liang, Y ., Wang, Z., Liu, Y . Power transformer DGA data processing and alarming tool for on-line monitors. 2009 IEEE/PES Power \nSystems Conference and Exposition, Seattle, W A, pp. 1–8, (2009).\n 9. Duval, M. Dissolved gas analysis: It can save your transformer. IEEE Electr. Insul. Mag. 5(6), 22–27 (1989).\n 10. Duval, M. A review of faults detectable by gas in oil analysis transformer. IEEE Elec. Insul. Mag. 18(3), 8–17 (2002).\n13\nVol.:(0123456789)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\n 11. IEC 60599, Mineral oil-impregnated electrical equipment in service-guide to the interpretation of dissolved and free gases analysis. \nInternational Electrotechnical Commission, (2007).\n 12. IEEE Standard C57.104, IEEE guide for the interpretation of gases generated in oil-immersed transformers. (2009).\n 13. Abu Bakar, N., Abu-Saida, A. & Islam, S. A review of dissolved gas analysis measurement and interpretation techniques. IEEE Elec. \nInsul. Maga. 30(3), 39–49 (2014).\n 14. CIGRE Technical Brochure #296, Recent Developments in DGA Interpretation. (2006).\n 15. Dornenburg, E. & Strittmatter, W . Monitoring oil-cooled transformers by gas analysis. Brown Boveri Rev. 61, 238–247 (1974).\n 16. Rogers, R. R. IEEE and IEC codes to interpret incipient faults in transformers using gas in oil analysis. IEEE Trans. Electr. Insul.  \n13(5), 348–354 (1978).\n 17. IEC 60599, Interpretation of the analysis of gases in transformers and other oil-filled electrical equipment in service. International \nElectrotechnical Commission, (1978).\n 18. Duval, M. The Duval triangle for load tap changers, non-mineral oils and low temperature faults in transformers. IEEE Electr. \nInsul. Mag. 24(6), 22–29 (2008).\n 19. Duval, M. & Lamarre, L. The duval pentagon—A new complementary tool for the interpretation of dissolved gas analysis in \ntransformers. IEEE Electr. Insul. Mag. 30(6), 9–12 (2014).\n 20. Benmahamed, Y ., Teguar, M. & Boubakeur, A. Application of SVM and KNN to Duval pentagon 1 for transformer oil diagnosis. \nIEEE Trans. Dielectr. Electr. Insul. 24(6), 3443–3451 (2017).\n 21. Benmahamed, Y ., Kemari, Y ., Teguar, M., Boubakeur, A. Diagnosis of power transformer oil using KNN and Naïve Bayes classi-\nfiers, ” Second International Conference on Dielectrics (ICD 2018 Conference), fully sponsored by IEEE Dielectrics and Electrical \nInsulation Society, Budapest, Hungary, pp. 1–4 (2018).\n 22. Taha, I. B. M., Hoballah, A. & Ghoneim, S. S. M. Optimal ratio limits of rogers’ four-ratios and IEC 60599 code methods using \nparticle swarm optimization fuzzy-logic approach. IEEE Trans. Dielectr. Electr. Insul. 27(1), 222–230 (2020).\n 23. Ghoneim, S. S. M., Mahmoud, K., Lehtonen, M. & Darwish, M. M. F . Enhancing diagnostic accuracy of transformer faults using \nteaching-learning-based optimization. IEEE Access 9, 30817–30832 (2021).\n 24. Ghoneim, S. S. M., Taha, I. B. M. & Elkalashy, N. I. Integrated ANN-based proactive fault diagnostic scheme for power transformers \nusing dissolved gas analysis. IEEE Trans. Dielectr. Electr. Insul. 23(3), 1838–1845 (2016).\n 25. Aldakheel, F ., Satari, R. & Wriggers, P . Feed-forward neural networks for failure mechanics problems. Appl. Sci. 11, 6483 (2021).\n 26. Ibnu Choldun, M., Santoso, J. & Surendro, K. Determining the number of hidden layers in neural network by using principal \ncomponent analysis. In Intelligent Systems and Applications. IntelliSys 2019. Advances in Intelligent Systems and Computing Vol. \n1038 (eds Bi, Y . et al.) (Springer, 2020).\n 27. Das, G., Pattnaik, P . K. & Padhy, S. K. Artificial neural network trained by particle swarm optimization for non-linear channel \nequalization. Expert Syst. Appl. 41(7), 3491–3496 (2014).\n 28. X. S. Y ang, \"Engineering optimization: An introduction with metaheuristic applications\", Department of Engineering, University \nof Cambridge, U.K, p. 173–179, Wiley & Sons, Inc., New Jersey, (2010).\n 29. Alik, B., Teguar, M. & Mekhaldi, A. Minimization of grounding system cost using PSO, GAO, and HPSGAO techniques. IEEE \nTrans. Power Deliv. 30(6), 2561–2569 (2015).\n 30. Vallée, T., Yıldızoğlu, M. Présentation des algorithmes génétiques et de leurs applications en Economie. J. Articles, Vol. 1.2, (2003).\n 31. Eberhart, R., Kennedy, J. A new optimizer using particle swarm theory. In: Proc. Sixth International Symposium on Micro Machine \nand Human Science (MHS’95), Nagoya, Japan, pp. 39–43, 1995.\n 32. Dorigo, M. & Gambardella, L. M. Ant colony system: A cooperative learning approach to the traveling salesman problem. IEEE \nTrans. Evol. Comput. 1(1), 53–66 (1997).\n 33. Gyanesh, D., Prasant, K. P . & Sasmita, K. P . Artificial neural network trained by particle swarm optimization for non-linear channel \nequalization. Expert Syst. Appl. Int. J. 41(7), 3491–3496 (2014).\n 34. Khan, K. & Sahai, A. A comparison of BA, GA, PSO, BP and LM for training feed forward neural networks in E-learning context. \nInt. J. Intell. Syst. Appl. 4(7), 23–29 (2012).\n 35. Van den Bergh, F . & Engelbrecht, A. P . A Cooperative approach to particle swarm optimization. IEEE Trans. Evol. Comput. 8(3), \n225–239 (2004).\n 36. Shi, Y ., Eberhart, R. A modified particle swarm optimizer, Evolutionary Computation Proceedings, IEEE International Conference \non Evolutionary Computation Proceedings, IEEE World Congress on Computational Intelligence, Anchorage, AK, USA, pp. 69–73 \n4–9, (1998).\n 37. Nezhad, N. K., Fallahi, M. H. & Dozein, M. G. An optimal design of substation grounding grid considering economic aspects \nusing particle swarm optimization. Res. J. Appl. Sci. Eng. Technol. 6(12), 2159–2165 (2013).\n 38. Kennedy, J., Eberhart R. Particle swarm optimization. In: IEEE International Conference on Neural Networks (ICNN’95), Perth, \nW A, Australia, pp. 1942–1948, (1995).\n 39. Del Valle, Y ., Venayagamoorthy, G. K., Mohagheghi, S., Hernandez, J.-C. & Harley, R. G. Particle swarm optimization: Basic \nconcepts, variants and applications in power systems. IEEE Trans. Evol. Comput. 12(2), 171–195 (2008).\n 40. Eberhart, R. & Shi, Y . Particle swarm optimisation: Developments, applications and resources. Congress Evol. Computat. 1, 81–86 \n(2001).\n 41. Clerc, M. & Siarry, P . Une nouvelle métaheuristique pour l’ optimisation difficile : la méthode des essaims particulaires. EDP Sci.  \n3(7), 13 (2004).\n 42. Parsopoulos, K. E., Vrahatis, M. N. Particle Swarm Optimization and Intelligence: Advances and Applications. Inf. Sci. Ref. New \nY ork, USA, (2010).\nAcknowledgements\nThe authors extend their appreciation to Taif University, Saudi Arabia, for supporting this work through the \nproject number (TU-DSPP-2024-14).\nAuthor contributions\nFettoumaGuerbas, Y oucef Benmahamed, Y oucef Teguar, Rayane Amine Dahmani: Conceptualization, Meth-\nodology, Software, Visualization, Investigation, Writing- Original draft preparation. Madjid Teguar, Enas Ali: \nData curation, Validation, Supervision, Resources, Writing—Review & Editing. Mohit Bajaj, Shir Ahmad Dost \nMohammadi and Sherif S. M. Ghoneim: Project administration, Supervision, Resources, Writing—Review & \nEditing.\nFunding\nThis research was funded by Taif University, Taif, Saudi Arabia (TU-DSPP-2024-14).\n14\nVol:.(1234567890)Scientific Reports |         (2024) 14:9271  | https://doi.org/10.1038/s41598-024-60071-0\nwww.nature.com/scientificreports/\nCompeting interests \nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to M.B. or S.A.D.M.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the \nCreative Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from \nthe copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\n© The Author(s) 2024"
}