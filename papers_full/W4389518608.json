{
    "title": "API-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs",
    "url": "https://openalex.org/W4389518608",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2599254380",
            "name": "Li Minghao",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2752161836",
            "name": "Zhao Yingxiu",
            "affiliations": [
                "University of Hong Kong",
                "Hong Kong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2232623990",
            "name": "Yu, Bowen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2750481165",
            "name": "Song, Feifan",
            "affiliations": [
                "Peking University",
                "King University"
            ]
        },
        {
            "id": "https://openalex.org/A2746848780",
            "name": "Li, Hangyu",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2062802469",
            "name": "Yu, Haiyang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2052631149",
            "name": "Li, Zhoujun",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2097369839",
            "name": "Huang Fei",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2602876330",
            "name": "Li, Yongbin",
            "affiliations": [
                "Alibaba Group (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4321177655",
        "https://openalex.org/W4304195432",
        "https://openalex.org/W4378509421",
        "https://openalex.org/W4378718568",
        "https://openalex.org/W4377371656",
        "https://openalex.org/W4301243929",
        "https://openalex.org/W4385775236",
        "https://openalex.org/W4378713418",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W2091879643",
        "https://openalex.org/W4285294723",
        "https://openalex.org/W4309088836",
        "https://openalex.org/W4366328015",
        "https://openalex.org/W4378474282",
        "https://openalex.org/W4360836968",
        "https://openalex.org/W4380136738",
        "https://openalex.org/W4322718191",
        "https://openalex.org/W4385474529",
        "https://openalex.org/W4312091890",
        "https://openalex.org/W4388626886",
        "https://openalex.org/W4382173325",
        "https://openalex.org/W4226352076",
        "https://openalex.org/W4383046813",
        "https://openalex.org/W4327810433",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4320165837",
        "https://openalex.org/W4377865087"
    ],
    "abstract": "Recent research has demonstrated that Large Language Models (LLMs) can enhance their capabilities by utilizing external tools. However, three pivotal questions remain unanswered: (1) How effective are current LLMs in utilizing tools? (2) How can we enhance LLMs' ability to utilize tools? (3) What obstacles need to be overcome to leverage tools? To address these questions, we introduce API-Bank, a groundbreaking benchmark, specifically designed for tool-augmented LLMs. For the first question, we develop a runnable evaluation system consisting of 73 API tools. We annotate 314 tool-use dialogues with 753 API calls to assess the existing LLMs' capabilities in planning, retrieving, and calling APIs. For the second question, we construct a comprehensive training set containing 1,888 tool-use dialogues from 2,138 APIs spanning 1,000 distinct domains. Using this dataset, we train Lynx, a tool-augmented LLM initialized from Alpaca. Experimental results demonstrate that GPT-3.5 exhibits improved tool utilization compared to GPT-3, while GPT-4 excels in planning. However, there is still significant potential for further improvement. Moreover, Lynx surpasses Alpaca's tool utilization performance by more than 26 pts and approaches the effectiveness of GPT-3.5. Through error analysis, we highlight the key challenges for future research in this field to answer the third question. ©2023 Association for Computational Linguistics.",
    "full_text": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 3102–3116\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nAPI-Bank: A Comprehensive Benchmark for Tool-Augmented LLMs\nMinghao Li1∗, Yingxiu Zhao2∗, Bowen Yu1†\n, Feifan Song3\nHangyu Li1, Haiyang Yu1, Zhoujun Li4, Fei Huang1, Yongbin Li1\n1Alibaba Group, 2Hong Kong University of Science and Technology,\n3Peking University, 4Shenzhen Intelligent Strong Technology Co., Ltd,\n{lmh397008, yubowen.ybw, hangyu.lhy}@alibaba-inc.com\n{yifei.yhy, f.huang, shuide.lyb}@alibaba-inc.com\nsongff@stu.pku.edu.cn, yzhaocx@connect.ust.hk\nlizhoujun@aistrong.com\nAbstract\nRecent research has demonstrated that Large\nLanguage Models (LLMs) can enhance their ca-\npabilities by utilizing external tools. However,\nthree pivotal questions remain unanswered: (1)\nHow effective are current LLMs in utilizing\ntools? (2) How can we enhance LLMs’ ability\nto utilize tools? (3) What obstacles need to be\novercome to leverage tools? To address these\nquestions, we introduce API-Bank, a ground-\nbreaking benchmark, specifically designed for\ntool-augmented LLMs. For the first question,\nwe develop a runnable evaluation system con-\nsisting of 73 API tools. We annotate 314\ntool-use dialogues with 753 API calls to as-\nsess the existing LLMs’ capabilities in plan-\nning, retrieving, and calling APIs. For the sec-\nond question, we construct a comprehensive\ntraining set containing 1,888 tool-use dialogues\nfrom 2,138 APIs spanning 1,000 distinct do-\nmains. Using this dataset, we train Lynx, a\ntool-augmented LLM initialized from Alpaca.\nExperimental results demonstrate that GPT-3.5\nexhibits improved tool utilization compared to\nGPT-3, while GPT-4 excels in planning. How-\never, there is still significant potential for fur-\nther improvement. Moreover, Lynx surpasses\nAlpaca’s tool utilization performance by more\nthan 26 pts and approaches the effectiveness of\nGPT-3.5. Through error analysis, we highlight\nthe key challenges for future research in this\nfield to answer the third question 1.\n1 Introduction\nOver the past several years, significant progress has\nbeen made in the development of large language\nmodels (LLMs), including GPT-3 (Brown et al.,\n2020), Codex (Chen et al., 2021), ChatGPT, and\nimpressive GPT-4 (Bubeck et al., 2023). These\n∗ Equal Contributions\n† Corresponding author.\n1The data and code are publicly available at\nhttps://github.com/AlibabaResearch/DAMO-ConvAI/\ntree/main/api-bank\nmodels exhibit increasingly human-like capabil-\nities, such as powerful conversation, in-context\nlearning, and code generation across a wide range\nof open-domain tasks (Bubeck et al., 2023).\nDespite their utility, LLMs are still constrained\nby training data (Brown et al., 2020; Zhao et al.,\n2023; Song et al., 2023). This data can quickly be-\ncome outdated and may not cover all scenarios (Mi-\nalon et al., 2023). Consequently, there has been\na surge of research focused on enhancing LLMs\nby enabling them to leverage external API tools,\nsuch as accessing up-to-date information (Izacard\net al., 2022) and interacting with third-party ser-\nvices (Liang et al., 2023). Traditionally, tool usage\nhas been viewed as a uniquely human behavior,\nwith its emergence considered a significant mile-\nstone in primate evolution (Ambrose, 2001). Draw-\ning an analogy to the timeline of human evolution,\nwe argue that it is currently imperative to address\nthree pivotal questions: (1) How effective are cur-\nrent LLMs at utilizing tools? (2) How can we\nenhance LLMs’ ability to utilize tools? (3) What\nobstacles still need to be overcome for LLMs to\neffectively leverage tools?\nTo tackle these inquiries, we present API-Bank,\na groundbreaking benchmark specifically designed\nfor tool-augmented LLMs. In order to determine\nthe users’ needs regarding the utilization of tool-\naugmented LLMs, we initially conducted inter-\nviews with 500 users. Taking into account their\nfeedback, we establish the design principles for\nAPI-Bank. The evaluation scope of API-Bank must\nencompass three essential capabilities: planning,\nretrieving, and calling API tools, in order to ful-\nfill the users’ requirements. Additionally, while\nconstructing the benchmark, it is imperative to con-\nsider various aspects such as domain diversity, API\ndiversity, API authenticity, and evaluation authen-\nticity. Subsequently, to answer the first research\nquestion, we implement the evaluation system of\nAPI-Bank, adhering to design principles. The sys-\n3102\ntem comprises 73 commonly used APIs, along with\n314 tool-use dialogues containing 753 API calls,\nall manually annotated, thus forming the first exe-\ncutable and authentic system capable of evaluating\nthe effectiveness of LLM utilizing tools.\nFor the second question, we develop a com-\nprehensive tool-augmented LLM training dataset\nwithin API-Bank. This dataset comprises 2,138\ndistinct APIs and includes 1,888 dialogues with\na total of 4,149 API calls, reflecting three differ-\nent levels of API usage abilities. However, an-\nnotating thousands of APIs and the correspond-\ning dialogues, adhering to our design principles,\nis not only prohibitively expensive but also time-\nconsuming. In this work, we introduce Multi-agent,\na novel method using LLMs instead of humans to\nautomatically mass-produce tool-augmented LLM\ntraining data. Multi-agent consists of five collab-\norative agents that step-by-step generate domains,\nAPIs, user queries, API calls & responses, while\nalso ensuring the quality of the generated data\naligns with our design principles. Multi-agent re-\nmarkably reduces the annotation cost by 98% com-\npared to human annotation. To verify the effective-\nness of our training set, subsequently, we fine-tune\na popular LLM Alpaca-7B (Taori et al., 2023), re-\nsulting in our own tool-augmented LLM Lynx.\nWe conduct extensive experiments on API-Bank\nand obtained insightful results regarding the API\ntool usage capabilities of LLMs. Our findings re-\nveal that even smaller models like Alpaca-7B and\nChatGLM-6.2B possess a certain level of API call\naccuracy, about 20%. However, their proficiency in\nAPI retrieval and planning is negligible. In contrast,\nthe larger-scale GPT-3 Davinci exhibits almost no\ncapability in API usage, indicating that API usage\nmight not be an inherent feature of LLMs. Our\nLynx, demonstrates an average improvement of\n24% in the three API usage capabilities compared\nto Alpaca-7B. While it approaches the performance\nof GPT-3.5, there remains a 21% gap from GPT-4.\nAdditionally, we present a comprehensive exper-\nimental analysis that sheds light on the primary\nchallenges encountered by GPT-4 and Lynx in uti-\nlizing APIs, addressing the third research question.\n2 Design Principles of API-Bank\nDue to the absence of authoritative ability defini-\ntions and benchmarks about tool-augmented LLM,\nwe conducted an extensive questionnaire survey\nin the initial phase. Through interviews with over\n500 users who expressed interest in incorporating\nadditional tools into LLM, we collected their re-\nquirements. Based on this foundation, we provide,\nfor the first time, a definition to measure the abili-\nties of tool-augmented LLM and the data standards\nfor training and evaluating its abilities. We believe\nthat these design principles, which reflect the ac-\ntual needs of users, can assist researchers in the\nfield and contribute to the future development of\ntool-augmented LLM benchmarks.\n2.1 Ability Grading\nAn ideal tool-augmented LLM should enable users\nto define the APIs they require in a private API\nPool and request the LLM to invoke these APIs at\nthe appropriate times to fulfill their needs. Based\non our interviews with users, we have identified\ntwo dimensions that encompass the requirements\nfor tool-augmented LLM:\n• Few vs. Many APIs in Pool: Different users\nmay define varying numbers of APIs. When\nusers specify a small number of APIs (e.g.,\n2-3), the relevant information of all the spec-\nified APIs (such as name, definition, input\nparameters, and output parameters) and the\nuser’s query can be provided as input to the\nLLM, allowing it to determine which API to\ncall to meet the user’s needs. However, when\nusers have defined a large number of APIs\n(e.g., hundreds), it is not feasible to input all\nthe APIs to the LLM due to input length re-\nstrictions. In such cases, the LLM needs to\nretrieve potentially suitable APIs to fulfill the\nuser’s needs before calling them.\n• Single vs. Several API calls per Turn: Many\nuser requirements are complex and necessi-\ntate the combination of multiple API calls to\nachieve. Some users are willing to patiently\ndecompose requirements into multiple queries\nand guide the LLM gradually to fulfill their\nneeds, with each query involving a single API\ncall. But some users prefer to directly present\ncomplex requirements, expecting the LLM to\nautonomously perform step-by-step API calls\nwithin a single turn to meet their needs.\nAs shown in Figure 2, these two dimensions give\nrise to four expected conditions: Few APIs, Single\nCall; Few APIs, Multiple Calls; Many APIs, Single\nCall; and Many APIs, Multiple Calls. During im-\nplementation, we discovered that the difficulty level\n3103\nAPI Pool\nQuery\nLLM\nCall\nResponse\nAPI\nRetrieve\nAPI Search\nCall\nPlan\nCall\nRetrieve Plan Retrieve\nCall\nFigure 1: The abilities to Call, Retrieve+Call, and Plan+Retrieve+Call APIs of tool-augmented LLMs.\nFew APIs, Single Call\nCall\nFew APIs, Multi Calls\nCall\nMany APIs, Single Call\nRetrieve+Call\nMany APIs, Multi Calls\nPlan+Retrieve+Call\nFew APIs in PoolMany APIs in Pool\nSingle\nAPI\nCall\nMulti\nAPI\nCalls\nFigure 2: Requirements of tool-augmented LLMs.\nof the first two conditions is similar. Since planning\nthe call of multiple APIs is straightforward when\nall the APIs are input, thus we merged the first two\nconditions. The three remaining conditions assess\nthe following abilities:\n1. Call: the ability to call APIs based on the\ngiven query when the APIs are known;\n2. Retrieval+Call: the ability to retrieve and call\na single API when the APIs are unknown;\n3. Plan+Retrieval+Call: the ability to continu-\nously plan, retrieve, and call multiple APIs\nwhen the APIs are unknown.\nWe also present visual examples of each ability in\nFigure 1 and Appendix.\n2.2 Data Standards\nAfter grading the abilities to be assessed in the\nbenchmark, another crucial aspect is ensuring the\nquality of the data. Calling APIs occurs within\nan open environment where we cannot predeter-\nmine the domains and functionalities. Moreover,\nAPI call is rigorous similar to mathematics, where\nany errors during the call (such as incorrect API\nnames, input parameter errors, or incorrect API call\nsequencing) can result in unfulfilled user require-\nments. Therefore, for the benchmark construction,\nwe need to consider the following criteria:\n1. Domain diversity: The training and testing\ndata should cover a wide range of domains as\ncomprehensively as possible;\n2. API authenticity: The name, definition, in-\nput and output parameters of the APIs should\nclosely resemble those in the real world;\n3. API diversity: The benchmark should include\na wide variety of API types and purposes.\n4. Evaluation authenticity : The evaluation\nshould incorporate a functional system that\nenables real-time interaction with the LLM.\nThe LLM offers an API call, which the system\nexecutes and subsequently returns the results\nto the LLM. The evaluation is based on the\nimpact of the execution on the system, assess-\ning whether the LLM adequately responds to\nthe user’s requirements.\n3 Evaluation System of API-Bank\nBased on the aforementioned principles, we first\nintroduce how to construct the evaluation system of\nAPI-Bank, including the system implementation,\nthe data annotation, and the evaluation metrics.\n3.1 System Implementation\nWe have implemented 73 APIs in our system, in-\ncluding commonly used daily APIs such as weather\n3104\nforecast, and accessing other AI models like Text-\nto-Image Generation. All APIs were implemented\nwithin the same framework by senior research\nand development engineers, with a total time in-\nvestment of 98 person-days. For APIs related\nto database operations, we establish the requisite\ndatabases and initialize them with initial entries, a\npivotal step in constructing dialogues. For APIs\nthat access external information (e.g., search en-\ngines), we must ensure the retrieved information\nremains constant to ensure reproducibility. We\ntrack all queries for each API in test dialogues and\nrecord the retrieval results at a specific time point,\nhard-coding them within the API to maintain result\nconsistency.\nAmong them, we developed a special API called\n\"API Search\" to fulfill the evaluation requirements\nof both Retrieval+Call and Plan+Retrieval+Call\nabilities. Specifically, in these two scenarios, the\nLLM is unaware of the APIs available in the API\nPool in advance, so it needs to make use of the\nAPI Search to identify the potentially needed APIs\naccording to the user query. In the input given to the\nLLM, we provide the instructions of the API Search\nat the beginning, and an API Search is required\nbefore every other API call. When performing an\nAPI Search, the model should condense the user’s\ndemand into a few keywords. The API Search\nobtains sentence embeddings from both the query\nkeywords and all API meta information in the API\nPool. It calculates the cosine similarity between\nkeywords and all API embeddings, returns the meta\ninformation of the API with the highest similarity.\n3.2 Dialogue Annotation\nBased on the grading abilities defined by de-\nsign principles, we annotate evaluation data\nfor the abilities of Call, Retrieval+Call, and\nPlan+Retrieval+Call APIs.\nFor the Call ability, we firstly randomly sample\nAPIs from the API Pool. Then we instruct annota-\ntors to first imagine a query that could be resolved\nby these APIs based on the API document. They\nthen annotate the API call and have the system\nexecute it. Finally, they label the response based\non the execution output. Please note that the Call\ndata does not necessarily represent single-turn dia-\nlogues. We also require annotators to ask multiple\nqueries regarding the same set of APIs, providing\nboth the dialogue history and the API call history.\nFor the Retrieval+Call ability, we aim to obtain\na complex user requirement and decompose it into\nmultiple simple queries, each of which could be\nfulfilled by executing a single API. To achieve this,\nwe initially obtaine a set of APIs from the Pool,\nranging from 1 to 5, and ask annotators to deter-\nmine if they could collectively address a complex\nrequirement. If so, they divide it into several simple\nqueries. For each query, annotators label the API\nto be called by the LLM and provided the input\nparameters. They also label the response that the\nLLM should generate based on the system’s output.\nThe annotation for Plan+Retrieval+Call is sim-\nilar to that of Retrieval+Call, with the difference\nbeing that annotators don’t decompose the complex\nquery into simpler ones. They are required to anno-\ntate a sequential chain of API calls and the response\nderived from the execution of the last API.\nThe introduction of APIs increases the difficulty\nof annotation. Despite recruiting computer science\nstudents for dialogue annotation, each dialogue re-\nquires discussion between two annotators to decide\nhow to annotate. Additionally, two additional anno-\ntators are involved to ensure the quality of annota-\ntions, including the format, logical consistency, and\nreasonability of API calls. The average annotation\ncost per dialogue is $8. Out of the 400 dialogues\nannotated, we discard 21.5% due to various annota-\ntion issues. Ultimately, we retained 314 dialogues\nwith a total of 753 API Calls.\n3.3 Evaluation Metrics\nWe evaluate model performance from two perspec-\ntives: the correctness of API calls and the quality\nof LLM responses. For the API call evaluation,\nwe employ the Accuracy metric, which is calcu-\nlated as the number of correct predictions divided\nby the total number of predictions. In each evalua-\ntion, we start by initializing the evaluation system,\nensuring that the databases for each API contain de-\nfault values. Then, we compare the predicted API\ncalls with the manually annotated API calls to de-\ntermine their consistency. We define consistency as\nwhether the same database queries or modifications\nare performed and whether the returned results are\nthe same. Regarding the evaluation of responses\nafter API calls, we utilize the ROUGE-L metric.\n4 Training Set of API-Bank\nWe hope API-Bank not only evaluate the effective-\nness of existing LLMs in utilizing tools but also to\nenhance their performance in utilizing such tools.\n3105\nThe most direct approach to achieve this goal is\ncreating a high-quality training dataset tailored to\ntool-augmented LLMs. However, it is challenging\nto construct a large-scale training dataset at a low\ncost while meeting the our design principles of do-\nmain diversity and API authenticity. The manual\nannotation cost for each dialogue in our evaluation\nset reaches $8, making it expensive to create a large\ndataset. Furthermore, it is challenging for the anno-\ntators to design a diverse and authentic API pool.\nOur recruited annotators could only come up with\n100 APIs, which cannot satisfy our need for di-\nverse training sets. Therefore, we propose a multi-\nagent data generation method to rapidly and cost-\neffectively construct tool-augmented LLM training\ndata that aligns with the design principles.\nDomains Requirements\nQuery CallResponse\nAgent\nTesterGenerator\nFigure 3: Five agents collaborate to accomplish the\ntraining data generation of tool-augmented LLMs.\nThe emergence of LLMs has brought about a\nparadigm shift in data annotation, offering an al-\nternative to human annotators for the automated\ngeneration of labeled data. One representative ap-\nproach is self-instruct (Wang et al., 2022). How-\never, we have observed that while self-instruct is\neffective in generating data for simple tasks such as\nwriting, it encounters difficulties in generating tool-\naugmented LLM data that aligns with our design\nprinciples. Specifically, we formulated a complex\ninstruction encompassing requirements for domain\ndiversity, API diversity&authenticity, and three spe-\ncific abilities about planning, retrieving and call-\ning APIs. Nevertheless, the widely used ChatGPT\nstruggles to generate data that fully adheres to\nour instructions, only 5% of the data is available.\nEven upgraded to the more powerful GPT-4, the\navailable rate improves to 25%, but a significant\nnumber of errors still persist. Our investigation\nreveals that the errors stems from providing LLM\nwith an overwhelming number of requirements all\nat once, making it difficult to comprehend them\neffectively. Consequently, an intuitive approach\nemerges: can we alleviate this issue by decompos-\ning the requirements into multiple simpler tasks,\nallowing an LLM to execute one task at a time?\nWe commence by providing an overview of the\nelements that should be incorporated in the data\nbased on our design principles: domain, API, query,\nability, and API call&response. The domain deter-\nmines the functionality of the APIs, while the APIs\nand ability determine the types of queries they can\nhandle. The combination of domain, APIs, query,\nand ability determines how the LLM makes API\ncalls and generates an appropriate response.\nTo simulate the dependencies among these ele-\nments, we propose the utilization of five agents as\nshown in Figure 3: (1) The first agent generates\nseveral domains, such as healthcare and fitness. (2)\nThe second agent, considering the domain, gener-\nates potential APIs. It is worth noting that during\nthis stage, to ensure the authenticity of the simu-\nlated APIs, we add examples from Public APIs 1\ninto the agent input. (3) The third agent randomly\nselects one or more APIs from the simulated APIs.\nAdditionally, it chooses an ability outlined in our\ndesign principles. This information is then used\nto create a query that matches the chosen ability\nand can be fulfilled by calling the selected APIs.\n(4) The fourth agent takes the domain, API, abil-\nity, and query as input. It is expected to make the\nnecessary API calls, simulate the execution of the\nAPIs, and generate a response that addresses the\nquery. (5) Finally, we introduce the fifth agent, act-\ning as a tester. This agent automatically verifies\nwhether the generated data adheres to our design\nprinciples (it actually discard 35% instances). All\nfive agents are implemented by providing specific\nprompts to ChatGPT. Together, they collaborate in\na step-by-step manner to accomplish the complex\ndata generation. Multi-agent eliminates the need\nfor human labelers, costing only $0.1 per dialogue\nand saving 98% compared to manual annotation.\n5 Benchmark Analysis\nStatistics. In the end, we construct a bench-\nmark consisting of 1,008 domains, 2,211 APIs,\n2,202 dialogues, and 6,135 turns. Among them,\n1https://github.com/public-apis/public-apis\n3106\nBenchmark Statistics Dialogue Type Evaluation Type Tool Usage Ability\n# domains # APIsMulti-turn Multi-callAPI Call ResponseCall Retrieve Plan\nDATESET (Schick et al., 2023)1 1 ✘ ✘ ✔ ✘ ✔ ✘ ✘\nAPIBench (Patil et al., 2023)90 1,645 ✘ ✘ ✔ ✘ ✔ ✔ ✘\nToolAlpaca (Tang et al., 2023)50 426 ✔ ✘ ✔ ✔ ✔ ✘ ✘\nToolBench1 (Qin et al., 2023b)49 16,464 ✔ ✔ ✔ ✘ ✔ ✔ ✔\nToolBench2 (Xu et al., 2023)8 232 ✘ ✔ ✔ ✘ ✔ ✘ ✘\nToolQA (Zhuang et al., 2023)6 13 ✘ ✔ ✔ ✔ ✔ ✘ ✘\nAPI-Bank (ours) 1,000 2,138 ✔ ✔ ✔ ✔ ✔ ✔ ✔\nTable 1: Comparison among API-Bank and existing benchmarks, where API-Bank comprehensively includes the\nmost domains and APIs, covers both multi-turn and multi-call dialogues, evaluate both API call and response, and\nthoroughly consider three distinct tool usage abilities.\nStatistics Training Evaluation\n# of Domains 1,000 8\n# of APIs 2,138 73\n# of Dialogues 1,888 314\n# of Turns 5,221 914\n- # of single call 3,147 363\n- # of multiple calls 493 122\n# of Call 720 214\n# of Retrieve+Call 719 50\n# of Plan+Retrieve+Call 449 50\navg. turns per dialogue 2.76 2.91\nTable 2: Statistics of API-Bank.\nthere are 934 dialogues in the Call category, 769\nin the Retrieve+Call category, and 499 in the\nPlan+Retrieve+Call category. Each dialogue has\n2.76 turns in the training set and 2.91 turns in the\ntesting set. The training data is generated auto-\nmatically by the LLM, while the evaluation data\nis manually annotated. Therefore, variations exist\namong them in terms of domain, API scope, and\ndialogue content. We aim to assess the generaliza-\ntion of models trained on our benchmark by testing\ntheir performance on distribution-shift evaluation\nset. Please refer to Table 2 for detailed statistical\ndata. We also provide samples and running demo\nof our benchmark in the Appendix.\nQuality. Each instance in the evaluation set has\nbeen reviewed by four annotators, allowing us to\nprimarily assess the quality of the training bench-\nmark generated by Multi-agent. We randomly se-\nlect 100 data samples from the training set and have\nthe annotators evaluate their quality. The results\ndemonstrate that the available rate of the Multi-\nagent generated data is 94%, which is an 89% im-\nprovement compared to data generated solely by\na single agent (self-instruct), indicating that our\ntraining set possesses a high level of quality and\nreliability. Furthermore, upon examining the data\nthat was automatically filtered by the tester agent,\nwe discover that 78% of it does not adhere to our\ndesign principles as intended. It shows that the\ntester agent can process data well according to the\ngiven principle.\nComparison. We have conducted a comparative\nanalysis of API-Bank in relation to recently re-\nleased benchmarks, as presented in Table 1. To en-\nsure the comprehensiveness of this table, we have\nalso incorporated some contemporaneous studies.\nLeveraging the design principles derived from our\ninvestigation of 500 user requirements, our API-\nBank stands out among all benchmarks currently\navailable for its: (1) Highest diversity: encom-\npassing a wide range of domains and APIs. (2)\nHighest realism: simulating multi-turn dialogues\nthat occur in real-world scenarios, including cases\nwhere multiple APIs are called in each turn. (3)\nHighest coverage: incorporating API call and re-\nsponse annotations, taking into account the es-\nsential capabilities required for tool-augmented\nLLMs, such as planning, retrieving, and calling\nAPIs. Therefore, we firmly believe that API-Bank\nrepresents the most comprehensive benchmark for\ntool-augmented LLM available at present.\n6 Related Work\nRecent research in language modeling has explored\nthe use of external tools to supplement the knowl-\nedge stored in the model’s weights (Qin et al.,\n2023a). This approach allows for tasks such as\nexact computation or information retrieval to be\noffloaded to external modules such as a Python in-\nterpreter or a search engine (Mialon et al., 2023).\nThese tools can include other neural networks or\neven the language model itself. Socratic Mod-\nels (Zeng et al., 2022) is a modular framework that\nallows for the composition of different pre-trained\nmodels on various modalities. Alternatively, natu-\nral language knowledge can be retrieved from exter-\n3107\nAbility LLM Call Retrieve+Call Plan+Retrieve+Call Total\nCorrectness Rouge Correctness Rouge Correctness Rouge Correctness Rouge\nZero-shot\nAlpaca-7B 24.06% 0.0204 5.19% 0.0019 0.00% 0.086 15.19% 0.0318\nChatGLM-6B 23.62% 0.2451 13.33% 0.2173 0.00% 0.1522 16.42% 0.2191\nGPT-3 Davinci 0.50% 0.1035 1.48% 0.091 0.00% 0.0156 0.57% 0.0814\nGPT-3.5-turbo 59.40% 0.4598 38.52% 0.3758 22.00% 0.3809 47.16% 0.4267\nGPT-4 63.66% 0.3691 37.04% 0.351 70.00% 0.4808 60.24% 0.3910\nFine-tuning Lynx-7B 49.87% 0.4332 30.37% 0.2503 20.00% 0.3425 39.58% 0.3794\nTable 3: Main results of different LLMs on the API-Bank evaluation system.\nnal sources, as demonstrated by WebGPT (Nakano\net al., 2021) and ReAct (Yao et al., 2022) through\nthe use of search APIs. Other approaches, such as\nToolformer (Schick et al., 2023), ART (Paranjape\net al., 2023), ToolkenGPT (Hao et al., 2023) and\nTRICE (Qiao et al., 2023) leverage a combination\nof search APIs, question-answering APIs, machine\ntranslation APIs, calculators, and other tools to\nsolve various NLP tasks. ChatGPT Plugins 1 and\nTaskMatrix.AI (Liang et al., 2023) demonstrate\nthe potential for language models to integrate with\nthousands to millions of APIs. LATM(Cai et al.,\n2023) and CREATOR (Qian et al., 2023) leverage\nGPT-4 to make API tools. Despite the promising\ndemonstrations of these approaches, researchers\nhave limited knowledge regarding three key issues:\n(1) How effectively can current LLMs utilize tools?\n(2) How can we enhance LLMs’ ability to utilize\ntools? (3) What obstacles still need to be overcome\nfor LLMs to effectively leverage tools? In this pa-\nper, we introduce API-Bank, the first benchmark\nspecifically designed for tool-augmented LLMs, to\naddress these three questions. As analyzed in Ta-\nble 1, API-Bank is also the most diverse, realistic,\nand comprehensive tool-augmented LLM bench-\nmark currently available.\n7 Experiments\nWe proceed to fine-tune Lynx, a model based on\nLLaMA-7B (Touvron et al., 2023), using our API-\nBank training dataset. The fine-tuning process con-\nsists of three epochs, with a batch size of 256 and a\nlearning rate of 2e-5. Subsequently, we conduct a\nseries of experiments on our API-Bank evaluation\nsystem, wherein we benchmark our model Lynx\nagainst other LLMs. The prompts used for eval-\nuation are given in the Appendix. Through these\nexperiments, we aimed to identify the remaining\nchallenges that hinder the effective utilization of\n1https://openai.com/blog/chatgpt-plugins\nLLMs in conjunction with tools.\n7.1 Baselines\nWe evaluate the following models for our analy-\nsis: GPT-3 Davinci (Brown et al., 2020), the first\npowerful variant of the GPT-3 family of models.\nGPT-3.5-turbo with the gpt-3.5-turbo-0613 check-\npoint; GPT-4, utilizing the gpt-4-0613 checkpoint;\nChatGLM-6B (Du et al., 2022), a bilingual chatbot\nwith 6B parameters; Alpaca-7B (Taori et al., 2023),\nan instruction-tuned variant of LLaMA-7B, lever-\naging 52K instruction-following data. Notably, Al-\npaca is currently recognized as the most widely\nused open-source LLM.\n7.2 Main Results\nWe present the experimental results of currently\npublic LLMs and our trained Lynx model on the\nAPI-Bank evaluation system, as shown in Table 3.\nAs expected, the performance of each model grad-\nually decreases with the increasing required ability.\nGiven the API description, calling the API can be\nseen as a slot filling task. Both the 7B Alpaca and\n6B ChatGLM achieve about 20% accuracy in API\ncall, indicating that basic language models possess\nsome tool-utilization ability. Surprisingly, GPT-3\nDavinci, despite being a well-known 175B LLM,\nexhibits an extremely low correctness in this task.\nWe speculate that this is because API call requires\na strong understanding of instructions, which can\nonly be unlocked through instruction tuning, a step\nomitted in GPT-3’s training process.\nOn the other hand, the instruction-tuned GPT-\n3.5 demonstrates outstanding performance in this\naspect, surpassing Alpaca-7B by 35 points in API\ncall correctness and 0.44 in the response Rouge-L\nscore. However, the effectiveness of GPT-3.5 de-\ncreases by 21% when compared to simple API calls\nin the Retrieve+Call setting, and a further 17%\ndecrease is observed in the Plan+Retrieve+Call\nsetting. This is because knowing which API to\n3108\ncall and planning how to use the API both require\ncertain reasoning abilities beyond instruction com-\nprehension. GPT-4, currently known as the most\npowerful LLM, shows a 4 points improvement in\nAPI calls compared to GPT-3.5. Its performance in\nRetrieve+Call is similar to GPT-3.5, but it achieves\na nearly 50% improvement in the most difficult\nPlan+Retrieve+Call setting. We hypothesize that\nthis is due to GPT-4’s emphasis on reasoning and\nplanning abilities, similar to its performance in\ntasks involving mathematical reasoning.\nAn exciting discovery is that the Lynx model,\ntrained on our API-Bank with Alpaca-7B initializa-\ntion, outperforms Alpaca-7B by 26 points in API\nCall correctness and 0.41 in the response Rouge-L\nscore, approaching the effectiveness of GPT-3.5.\nThis strongly indicates the satisfactory quality of\ntraining data generated through our Multi-agent\nautomated construction strategy.\nHere, we would like to emphasize the differences\nbetween our developed API-Bank and the concur-\nrently released APIBench(Patil et al., 2023) and\nToolAlpaca(Tang et al., 2023) based on experimen-\ntal results. GPT-3.5 achieves an API usage accu-\nracy of 80% to 90% on their datasets. However,\nthere is still significant room for improvement in\nour benchmark. This is because our evaluation set\nis manually constructed, considering design princi-\nples and closely resembling real-world scenarios.\nIn contrast, their evaluation set is generated through\nself-instruct by the LLM and lacks diversity due to\nits narrow domain focus.\n7.3 Error Analysis\nError Type Rate\nNo API Call 36.77%\nAPI Hallucination 15.93%\nInvalid Input Parameters 7.96%\nFalse API Call Format 23.65%\nMiss Input Parameters 1.17%\nTable 4: Distribution of errors in the Alpaca evaluation.\nIn this study, we systematically categorize six\nprimary error types, with detailed definitions pro-\nvided in the Appendix. Notably, the primary issue\nidentified during our evaluation of the original Al-\npaca model, as depicted in Table 4, is the frequent\noccurrence of \"No API Call.\" However, it is worth\nmentioning that this issue exhibits a significant re-\nError Type Rate\nAPI Hallucination 61.38%\nHas Exception 16.40%\nInvalid Input Parameters 8.47%\nFalse API Call Format 6.88%\nNo API Call 5.29%\nMiss Input Parameters 1.59%\nTable 5: Distribution of errors in the Lynx evaluation.\nError Type Rate\nFailed API Retrieval 67.86%\nFalse API Call Format 17.86%\nInvalid Input Parameters 7.14%\nMiss Input Parameters 7.14%\nTable 6: Distribution of errors in the GPT-4 evaluation.\nduction in the Lynx model following fine-tuning\nwith the API-Bank training dataset. This observed\nimprovement suggests that disparities between the\npatterns of API calls in Alpaca’s training dataset,\nwhich is constructed using 52,000 instruction data,\nand the actual API calls may contribute to this prob-\nlem. It is plausible that the original Alpaca model\nencounters challenges in comprehending the API\ncalls instructions in the evaluation system.\nAnother noteworthy issue we encounter is the\npresence of \"False API Call Format.\" Given that\nour testing prompts are presented in a zero-shot\nformat, the model has to rely solely on guidance\nabout making API calls in instructions. This re-\nliance on instruction-based guidance may cause\ndifficulties for the model in learning the correct\nAPI call format. The application of fine-tuning re-\nsults in a substantial enhancement in this regard,\nunderscoring the significant benefit of fine-tuning\nin improving the model’s capacity to generate ac-\ncurately formatted API calls.\nThe analysis of the Lynx model results is given\nin Table 5, where the most significant type of error\nis the API name mismatch, accounting for 61%\nof the total errors. This occurs when the model\nmakes API calls that do not match the annotated\nAPI calls in Ground Truth. In some cases, Lynx\ngenerates fake APIs that are unrelated to the user’s\nintent, even if they are not provided in the test\nprompt. This issue arises due to Hallucination in\nLynx, where it incorrectly calls APIs that it has\nencountered during training.\n3109\nNumber of training samples Accuracy (Call) Rouge (Call)\nToolAlpaca 10,366 53.88 39.75\nLynx (Ours) 6,184 54.64 39.80\nTable 7: Results of the fine-tuned Alpaca on the ToolAlpaca dataset and Lynx.\nThe second major category of errors is related\nto problematic input parameters, which can lead\nto three types of errors: triggering an Exception\nwithin the API, being identified as an invalid input\nparameter, and generating an API call that fails to\nbe parsed. These errors collectively contribute to\n32% of the overall errors. Representative examples\nof problematic input parameters include passing a\nplaceholder as a value, using an illegal format (such\nas not following the given date format, resulting\nin parsing failure), missing input parameters, and\nmisunderstanding the requirements of the input\nparameter (e.g., requesting a stock code but passing\nin a company name). In addition, a few other issues\nexist, such as forging user or AI utterances instead\nof making an API call.\nAs shown in Table 6, the primary issue with GPT-\n4 is its inability to effectively use API search for\nretrieving specific APIs, which accounts for 68%\nof the overall problems encountered. This problem\ndoes not occur in the evaluation of the fine-tuned\nLynx model. Fine-tuning appears to facilitate teach-\ning the model the Retrieve+Call pipeline, unlike\nIn-Context Learning, which makes it challenging to\ncontrol the model’s API calls as expected. The sec-\nond common problem with GPT-4 is the API calls\nit gives cannot be parsed by the evaluation system.\nThis issue arises because GPT-4 sometimes makes\nmultiple simultaneous API calls, which violates the\ntested prompt’s instructions.\nTherefore, we aim to highlight three potential\ndirections worth exploring: (1) Improved API call-\ning methods: The direct generation of API calls,\nsimilar to Toolformer (Schick et al., 2023) , is lim-\nited by the number of available APIs. Introducing\nadditional API retrieval tools is challenging due to\nissues such as hallucination and difficulties in accu-\nrately calling retrieval. (2) Enhanced API decoding\nalgorithms: It is essential to demand strict adher-\nence to the input parameter definitions when gener-\nating API calls with LLMs. (3) Larger-scale train-\ning data: While Lynx has demonstrated promising\nAPI usage capabilities, we wonder if scaling up the\ntraining data further could yield even better results.\nWe eagerly anticipate investigating this possibility.\n7.4 Dataset Comparison\nTo assess the quality of the dataset created, we con-\nducted experiments to fine-tune Alpaca using the\ntraining dataset of ToolAlpaca. ToolAlpaca auto-\nmatically creates a tool-use corpus, which contains\n3938 tool-use instances from more than 400 real-\nworld tool APIs spanning 50 distinct categories.\nWe convert the training set of ToolAlpaca into the\ntraining format consistent with ours, and finally ob-\ntained 10366 training samples. Since ToolAlpaca\ndoes not involve API Retrieval in its process, our\nevaluation focused only on its ability to Call APIs,\nensuring fairness in the comparative analysis.\nThe results, presented in Table 7, reveal that our\nmodel Lynx, fine-tuned on the API-Bank dataset,\noutperforms the fine-tuned Alpaca on the ToolAl-\npaca dataset, even with fewer training data. This\nobservation underscores the superior data quality\nof our constructed dataset and validates the efficacy\nof the multi-agent data synthesis methods.\n8 Conclusion\nThis paper presents API-Bank, the pioneering\nbenchmark for tool-augmented LLMs comprising\n2,202 dialogues involving 2,211 APIs from 1,008\ndomains, and provides three key findings: (1) Cur-\nrent LLMs demonstrate some ability to use APIs,\nbut there is still room for improvement in pro-\nviding reliable services to users consistently. (2)\nBuilding diverse and high-quality training data is\na promising approach to enhance LLMs’ ability\nto use tools. (3) Proper API calls and strict adher-\nence to API documentation pose significant chal-\nlenges for LLMs in utilizing API tools. Overall, the\nAPI-Bank benchmark provides a groundbreaking\nresource for evaluating and advancing the state-of-\nthe-art in tool-augmented LLMs. We believe that\nthis research will inspire future studies and pave\nthe way for developing more advanced AI systems\nthat can intelligently integrate external resources to\nfulfill human requirements.\n3110\n9 Limitations\nAPI-Bank is a pioneering benchmark for tool-\naugmented LLM. While we have followed com-\nprehensive design principles, it does have a few\nlimitations. Firstly, API-Bank focuses solely on\nEnglish, and we plan to address data construction\nand model evaluation for other languages as future\nwork. Secondly, we have only fine-tuned Lynx-7B\non API-Bank and have not explored larger-scale\nmodels, although Lynx-7B performs comparably to\nGPT-3.5. Lastly, we have trained a commercially\nviable tool-augmented LLM based on a larger LLM\nwithin our company. However, due to anonymity\nreasons, we are unable to report and analyze the\nresults of the online model. We aim to include this\ninformation in future versions of our work.\n10 Ethical Statement\nThis paper constructs a new benchmark for tool-\naugmented LLM, and we discuss some related eth-\nical considerations here. Firstly, during interviews\nregarding their needs for tool-augmented LLMs,\nparticipants are informed in advance that their feed-\nback will be used for product development and po-\ntentially published in the form of a research paper,\nwithout disclosing any personal private informa-\ntion. Secondly, all API tools in our dataset are\noriginal implementations and do not infringe upon\nany existing commercial software. Lastly, for data\nannotation in the benchmark, we hire four annota-\ntors. The hourly pay is set to 15 US$ per person,\nhigher than the local statutory minimum wage.\nReferences\nStanley H Ambrose. 2001. Paleolithic technology and\nhuman evolution. Science, 291(5509):1748–1753.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nSébastien Bubeck, Varun Chandrasekaran, Ronen El-\ndan, Johannes Gehrke, Eric Horvitz, Ece Kamar,\nPeter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lund-\nberg, et al. 2023. Sparks of artificial general intelli-\ngence: Early experiments with gpt-4. arXiv preprint\narXiv:2303.12712.\nTianle Cai, Xuezhi Wang, Tengyu Ma, Xinyun Chen,\nand Denny Zhou. 2023. Large language models as\ntool makers. arXiv preprint arXiv:2305.17126.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large\nlanguage models trained on code. arXiv preprint\narXiv:2107.03374.\nZhengxiao Du, Yujie Qian, Xiao Liu, Ming Ding,\nJiezhong Qiu, Zhilin Yang, and Jie Tang. 2022. Glm:\nGeneral language model pretraining with autoregres-\nsive blank infilling. In Proceedings of the 60th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 320–335.\nShibo Hao, Tianyang Liu, Zhen Wang, and Zhiting\nHu. 2023. Toolkengpt: Augmenting frozen lan-\nguage models with massive tools via tool embeddings.\narXiv preprint arXiv:2305.11554.\nGautier Izacard, Patrick Lewis, Maria Lomeli, Lu-\ncas Hosseini, Fabio Petroni, Timo Schick, Jane\nDwivedi-Yu, Armand Joulin, Sebastian Riedel, and\nEdouard Grave. 2022. Few-shot learning with re-\ntrieval augmented language models. arXiv preprint\narXiv:2208.03299.\nYaobo Liang, Chenfei Wu, Ting Song, Wenshan Wu,\nYan Xia, Yu Liu, Yang Ou, Shuai Lu, Lei Ji,\nShaoguang Mao, et al. 2023. Taskmatrix. ai: Com-\npleting tasks by connecting foundation models with\nmillions of apis. arXiv preprint arXiv:2303.16434.\nGrégoire Mialon, Roberto Dessì, Maria Lomeli, Christo-\nforos Nalmpantis, Ram Pasunuru, Roberta Raileanu,\nBaptiste Rozière, Timo Schick, Jane Dwivedi-Yu,\nAsli Celikyilmaz, et al. 2023. Augmented language\nmodels: a survey. arXiv preprint arXiv:2302.07842.\nReiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu,\nLong Ouyang, Christina Kim, Christopher Hesse,\nShantanu Jain, Vineet Kosaraju, William Saunders,\net al. 2021. Webgpt: Browser-assisted question-\nanswering with human feedback. arXiv preprint\narXiv:2112.09332.\nBhargavi Paranjape, Scott Lundberg, Sameer Singh,\nHannaneh Hajishirzi, Luke Zettlemoyer, and\nMarco Tulio Ribeiro. 2023. Art: Automatic multi-\nstep reasoning and tool-use for large language mod-\nels. arXiv preprint arXiv:2303.09014.\nShishir G Patil, Tianjun Zhang, Xin Wang, and\nJoseph E Gonzalez. 2023. Gorilla: Large language\nmodel connected with massive apis. arXiv preprint\narXiv:2305.15334.\nCheng Qian, Chi Han, Yi R Fung, Yujia Qin, Zhiyuan\nLiu, and Heng Ji. 2023. Creator: Disentan-\ngling abstract and concrete reasonings of large lan-\nguage models through tool creation. arXiv preprint\narXiv:2305.14318.\nShuofei Qiao, Honghao Gui, Huajun Chen, and Ningyu\nZhang. 2023. Making language models better tool\nlearners with execution feedback. arXiv preprint\narXiv:2305.13068.\n3111\nYujia Qin, Shengding Hu, Yankai Lin, Weize Chen,\nNing Ding, Ganqu Cui, Zheni Zeng, Yufei Huang,\nChaojun Xiao, Chi Han, et al. 2023a. Tool\nlearning with foundation models. arXiv preprint\narXiv:2304.08354.\nYujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan\nYan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang,\nBill Qian, et al. 2023b. Toolllm: Facilitating large\nlanguage models to master 16000+ real-world apis.\narXiv preprint arXiv:2307.16789.\nTimo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta\nRaileanu, Maria Lomeli, Luke Zettlemoyer, Nicola\nCancedda, and Thomas Scialom. 2023. Toolformer:\nLanguage models can teach themselves to use tools.\narXiv preprint arXiv:2302.04761.\nFeifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei\nHuang, Yongbin Li, and Houfeng Wang. 2023. Pref-\nerence ranking optimization for human alignment.\narXiv preprint arXiv:2306.17492.\nQiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han,\nQiao Liang, and Le Sun. 2023. Toolalpaca: Gener-\nalized tool learning for language models with 3000\nsimulated cases. arXiv preprint arXiv:2306.05301.\nRohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann\nDubois, Xuechen Li, Carlos Guestrin, Percy Liang,\nand Tatsunori B. Hashimoto. 2023. Stanford alpaca:\nAn instruction-following llama model. https://\ngithub.com/tatsu-lab/stanford_alpaca.\nHugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier\nMartinet, Marie-Anne Lachaux, Timothée Lacroix,\nBaptiste Rozière, Naman Goyal, Eric Hambro,\nFaisal Azhar, et al. 2023. Llama: Open and effi-\ncient foundation language models. arXiv preprint\narXiv:2302.13971.\nYizhong Wang, Yeganeh Kordi, Swaroop Mishra, Al-\nisa Liu, Noah A Smith, Daniel Khashabi, and Han-\nnaneh Hajishirzi. 2022. Self-instruct: Aligning lan-\nguage model with self generated instructions. arXiv\npreprint arXiv:2212.10560.\nQiantong Xu, Fenglu Hong, Bo Li, Changran Hu,\nZhengyu Chen, and Jian Zhang. 2023. On the tool\nmanipulation capability of open-source large lan-\nguage models.\nShunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak\nShafran, Karthik Narasimhan, and Yuan Cao. 2022.\nReact: Synergizing reasoning and acting in language\nmodels. arXiv preprint arXiv:2210.03629.\nAndy Zeng, Adrian Wong, Stefan Welker, Krzysztof\nChoromanski, Federico Tombari, Aveek Purohit,\nMichael Ryoo, Vikas Sindhwani, Johnny Lee, Vin-\ncent Vanhoucke, et al. 2022. Socratic models: Com-\nposing zero-shot multimodal reasoning with lan-\nguage. arXiv preprint arXiv:2204.00598.\nYingxiu Zhao, Bowen Yu, Binyuan Hui, Haiyang Yu,\nFei Huang, Yongbin Li, and Nevin L Zhang. 2023.\nA preliminary study of the intrinsic relationship be-\ntween complexity and alignment. arXiv preprint\narXiv:2308.05696.\nYuchen Zhuang, Yue Yu, Kuan Wang, Haotian Sun, and\nChao Zhang. 2023. Toolqa: A dataset for llm ques-\ntion answering with external tools. arXiv preprint\narXiv:2306.13304.\n3112\nA Appendix\nGenerate an API request in the format of [ApiName(key1='value1', key2='value2', ...)] based on the previous dialogue context. The current year is 2023.Input: User: User's utterenceAI: AI's responseExpected output:API-Request: [ApiName(key1='value1', key2='value2', ...)]API descriptions:\nFigure 4: The prompts used for API call evaluation.\nGenerate a response as an AI assistant based on the previous dialogue context and API request. The current year is 2023.Input: User: User's utterenceAI: AI's responseAPI-Request: [ApiName(key1='value1', key2='value2', ...)]Expected output:AI: AI's responseAPI descriptions:\nFigure 5: The prompts used for response evaluation.\nA.1 Error Definitions\nWe briefly outline the definition of each type of\nerrors:\n• API Hallucination: The API name in the ground truth\ndoes not match the name in the prediction.\n• Has Exception: The prediction triggers a custom or\nbuilt-in Python exception, which should not be present\nin the ground truth.\n• Invalid Input Parameters : The prediction contains\ninvalid input parameters.\n• False API Call Format: The prediction’s API call for-\nmat is not parseable.\n• No API Call : There is no API call detected in the\nprediction.\n• Missing Input Parameters: Necessary input parame-\nters are missing from the prediction.\nA.2 Implement Details\nThe evaluation prompts consisted of two parts: one\nfor API testing (Figure 4) and the other for response\ntesting (Figure 5). We kept the prompts as concise\nas possible to showcase the models’ fundamental\ncapabilities.\nA.3 Examples\nBecause the huge amount of the domains in the\ntraining set, it is impossible to give a table statistics,\nso we only count the distribution of the API in the\ntest set on the domain and give the following table:\nDomains Numbers\nAccount Management 7\nInformation Query and Processing 22\nHealth Management 8\nSchedule Management 19\nSmart Home 6\nFinance Management 6\nOthers 5\nTable 8: The domain statistics for the test set.\nHere are some examples of the training and\ntesting set domains along with their associated\nAPIs:\nTraining Set Domains:\n• Mental Health Hotline and Support\n• Dental Procedure Cost Estimate\n• Nutrition Planning\nTraining Set APIs:\n• SearchDoctors\n• GetPrice\n• RecordMaintenance\nTesting Set Domains:\n• Account Management\n• Health Management\n• Entertainment\n• Travel\nTesting Set APIs:\n• BookHotel\n• DeleteAlarm\n• SearchEngine\n3113\nGenerate an API request in the format of [ApiName(key1='value1', key2='value2', ...)] based on the previous dialogue context.\nThe current year is 2023.\nInput: \nUser: User's utterence\nAI: AI's response\nExpected output:\nAPI-Request: [ApiName(key1='value1', key2='value2', ...)]\nAPI descriptions:\n{\"name\": \"DeleteAccount\", \"description\": \"Delete an account.\", \"input_parameters\": {\"token\": {\"type\": \"str\", \"description\": \"The\ntoken of the user.\"}}, \"output_parameters\": {\"status\": {\"type\": \"str\", \"description\": \"success or failed\"}}}\n{\"name\": \"ForgotPassword\", \"description\": \"Sends an email to the user with a link to reset the password. Need call twice, first\nwith 'Forgot Password' status to get the verification code, then call again with 'Verification Code' status to change the\npassword. Must pass the name of the parameters when calling the API, like ForgotPassword(status='Forgot Password',\nusername='username').\", \"input_parameters\": {\"status\": {\"type\": \"str\", \"description\": \"'Forgot Password' for first call, after\nget the verification code, call again with 'Verification Code' to change the password.\"}, \"username\": {\"type\": \"str\",\n\"description\": \"The username of the user. Only needed for the first call.\"}, \"email\": {\"type\": \"str\", \"description\": \"The email\nof the user. Only needed for the first call.\"}, \"verification_code\": {\"type\": \"int\", \"description\": \"The verification code sent\nto the user. Only needed for the second call.\"}, \"new_password\": {\"type\": \"str\", \"description\": \"The new password of the user.\nOnly needed for the second call.\"}}, \"output_parameters\": {\"status\": {\"type\": \"str\", \"description\": \"success or failed\"}}}\n{\"name\": \"GetUserToken\", \"description\": \"Get the user token by username and password.\", \"input_parameters\": {\"username\":\n{\"type\": \"str\", \"description\": \"The username of the user.\"}, \"password\": {\"type\": \"str\", \"description\": \"The password of the\nuser.\"}}, \"output_parameters\": {\"token\": {\"type\": \"str\", \"description\": \"The token of the user.\"}}}\n{\"name\": \"RegisterUser\", \"description\": \"The API for registering a account, given the username, password and email.\",\n\"input_parameters\": {\"username\": {\"type\": \"str\", \"description\": \"The username of the user.\"}, \"password\": {\"type\": \"str\",\n\"description\": \"The password of the user.\"}, \"email\": {\"type\": \"str\", \"description\": \"The email of the user.\"}},\n\"output_parameters\": {\"token\": {\"type\": \"str\", \"description\": \"The token of the user.\"}}}\"\nUser\nI forgot my password, can you help me reset it?\nLLM\nResponse: Sure, I can definitely help you. I need your username and email to get star\nUser\nMy username is foo and my email is foo@example.com.\nLLM\nResponse: Alright.\nAPI-Request: [ForgotPassword(status='Forgot Password', username='foo',\nemail='foo@example.com')]\nResponse: You have been sent a verification code. Please provide me with that code to\nreset your password.\nUser\nThe verification code is 970420.\nLLM\nResponse: What's your new password?\nUser\nMy new password is newpassword.\nLLM\nResponse: Great.\nAPI-Request: [ForgotPassword(status='Verification Code', verification_code='970420',\nnew_password='newpassword')]\nObservation: Success\nResponse: Your password has been successfully reset.\nPrompt\nFigure 6: One example of Calling APIs.\n3114\nGenerate an API request in the format of [ApiName(key1='value1', key2='value2', ...)] based on the previous dialogue\ncontext.\nThe current year is 2023.\nInput: \nUser: User's utterence\nAI: AI's response\nExpected output:\nAPI-Request: [ApiName(key1='value1', key2='value2', ...)]\nAPI descriptions:\n{\"name\": \"ToolSearcher\", \"description\": \"Searches for relevant tools in library based on the keywords.\", \"input_parameters\":\n{\"keywords\": {\"type\": \"str\", \"description\": \"The keyword to search for.\"}}, \"output_parameters\": {\"best_matchs\": {\"type\":\n\"Union[List[dict], dict]\", \"description\": \"The best match tool(s).\"}}}\"\nUser\nCan you help me add a reminder for my meeting at 3pm on\nJanuary 5, 2023 with the topic \"sales report\"?\nLLM\nAPI-Request: \n[ToolSearcher(keywords='add reminder')]\nResponse: \nFound it! Now, I need your user authentication token. Can you please provide me with\nyour username, password?\nUser\nMy username is \"newuser\", my password is \"newpass\".\nLLM\nResponse:Thank you. Now I'm going to get your user token.\nAPI-Request: \n[GetUserToken(username='newuser', password='newpass')]\nObservation: \n{'token': 'l9k8j7h6g5f4d3s2a1'}\nResponse: \nGot it! Now let me add the reminder for you.\nPrompt\nObservation: \n[{'name': 'GetUserToken', 'description': 'Get the user token.', 'input_parameters': {'username': {'type': 'str',\n'description': 'The username of the user.'}, 'password': {'type': 'str', 'description': 'The password of the\nuser.'}}, 'output_parameters': {'token': {'type': 'str', 'description': 'The token of the user.'}},\n'desc_for_search': 'get user tokenGet the user token.'}, {'name': 'AddReminder', 'description': \"Add a reminder API\nthat takes three parameters - 'token'uff0c'content' and 'time'. The 'token' parameter refers to the user's token and\nthe 'content' parameter refers to the description of the reminder and the 'time' parameter specifies the time at\nwhich the reminder should be triggered.\", 'input_parameters': {'token': {'type': 'str', 'description': \"User's\ntoken.\"}, 'content': {'type': 'str', 'description': 'The content of the conference.'}, 'time': {'type': 'str',\n'description': 'The time for conference. Format: %Y-%m-%d %H:%M:%S'}}, 'output_parameters': {'status': {'type':\n'str', 'description': 'success or failed'}}}]\nAPI-Request: \n[AddReminder(token='l9k8j7h6g5f4d3s2a1', content='sales report', time='2023-01-05\n15:00:00')]\nObservation: \n{'status': 'success'}\nResponse: \nYou should receive a confirmation shortly.\nFigure 7: One example of Retrieving+Calling APIs.\n3115\nYou will be tested on your ability to make multiple API calls to fulfill a requirement based on a single sentence.\nYou will be given an API box that includes a set of APIs such as a calculator, translator, WikiSearch, etc. When you\nwant to use an API, you have to search for it in the API search engine using keywords. Try to describe it with these\nkeywords. The tool search engine will then return you the most relevant information about the tool (api name,\ndescription, input/output parameters).\nAfter you give each API call, stop generating and wait f3or input, I will return the results of the API call to you and make the\nnext call based on the results. Your output is only an API call and does not contain any explanatory text, which means starts\nwith [ and ends with ].\nHere is an example of a test where ChatGPT represents you and API represents the return value.\nExample:\nRequirement: calculate the result of (5+3)*6 and sum the with 5.\nChatGPT: [ToolSearcher(keywords='calculator')]\nAPI: {\"name\": \"Calculator\", \"description\": \"This API provides basic arithmetic operations: addition, subtraction,\nmultiplication, and division.\", \"input_parameters\": {\"formula\": {\"type\": \"str\", \"description\": \"The formula that needs to be\ncalculated. Only integers are supported. Valid operators are +, -, *, /, and (, ). For example, '(1 + 2) * 3'.\"}},\n\"output_parameters\": {\"result\": {\"type\": \"float\", \"description\": \"The result of the formula.\"}}}\nChatGPT: [Calculator(formula='(5+3)*6')]\nAPI: {'result': 48}\nChatGPT: [Calculator(formula='48+5')]\nAPI: {'result': 53}\nAPI description:\n{\"apiCode\": \"ToolSearcher\", \"description\": \"Searches for relevant tools in library based on the keywords.\", \"parameters\":\n{\"keywords\": {\"type\": \"str\", \"description\": \"The keyword to search for.\"}}, \"response\": {\"best_matchs\": {\"type\":\n\"Union[List[dict], dict]\", \"description\": \"The best match tool(s).\"}}}\nUser\nCalculate the monthly salary after taxes for a\nFinancial Analyst\nLLM\nAPI-Request: \n[ToolSearcher(keywords='GetOccupationSalary')]\nPrompt\nObservation: \n{'api_name': 'ToolSearcher', 'input': {'keywords': 'GetOccupationSalary'}, 'output': {'name':\n'GetOccupationSalary', 'description': 'API for querying the salary of a given occupation.',\n'input_parameters': {'occupation': {'type': 'str', 'description': 'The occupation to query.'}},\n'output_parameters': {'salary': {'type': 'float', 'description': 'The salary of the given\noccupation.'}}}, 'exception': None}\nResponse: \nFor a Financial Analyst with a salary of $100,000, the monthly salary after taxes is\n$70,000\nAPI-Request: \n[GetOccupationSalary(occupation='Financial Analyst')]\nObservation: \n{'api_name': 'GetOccupationSalary', 'input': {'occupation': 'Financial Analyst'},\n'output': {'salary': 100000}, 'exception': None}\nAPI-Request: \n[ToolSearcher(keywords='TaxCalculator')]\nObservation: \n{'api_name': 'ToolSearcher', 'input': {'keywords': 'TaxCalculator'}, 'output':\n{'name': 'TaxCalculator', 'description': 'API for calculating tax deductions based on\nthe given salary.', 'input_parameters': {'salary': {'type': 'float', 'description':\n'The salary to calculate tax deductions for.'}}, 'output_parameters':\n{'salary_after_tax': {'type': 'float', 'description': 'The salary after tax\ndeductions.'}}}, 'exception': None}\nAPI-Request: \n[TaxCalculator(salary='100000')]\nObservation: \n{'salary_after_tax': 70000.0}\nFigure 8: One example of Planning+Retrieving+Calling APIs.\n3116"
}