{
  "title": "AMBERT: A Pre-trained Language Model with Multi-Grained Tokenization",
  "url": "https://openalex.org/W3177365697",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2141857187",
      "name": "Xinsong Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2618392206",
      "name": "Pengshuai Li",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A1990683121",
      "name": "Hang Li",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2949884065",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2950784811",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W4323654151",
    "https://openalex.org/W3098065087",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3021293129",
    "https://openalex.org/W3114651185",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2936074642",
    "https://openalex.org/W3106031450",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2995923603",
    "https://openalex.org/W2134800885",
    "https://openalex.org/W2997200074",
    "https://openalex.org/W2944852028",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2938830017",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3011411500",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2964121744",
    "https://openalex.org/W3210120707",
    "https://openalex.org/W2396767181",
    "https://openalex.org/W3013571468",
    "https://openalex.org/W2948947170",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2995411906",
    "https://openalex.org/W2966989210"
  ],
  "abstract": "Pre-trained language models such as BERT have exhibited remarkable performances in many tasks in natural language understanding (NLU).The tokens in the models are usually fine-grained in the sense that for languages like English they are words or subwords and for languages like Chinese they are characters.In English, for example, there are multi-word expressions which form natural lexical units and thus the use of coarse-grained tokenization also appears to be reasonable.In fact, both fine-grained and coarse-grained tokenizations have advantages and disadvantages for learning of pre-trained language models.In this paper, we propose a novel pretrained language model, referred to as AM-BERT (A Multi-grained BERT), on the basis of both fine-grained and coarse-grained tokenizations.For English, AMBERT takes both the sequence of words (fine-grained tokens) and the sequence of phrases (coarse-grained tokens) as input after tokenization, employs one encoder for processing the sequence of words and the other encoder for processing the sequence of the phrases, utilizes shared parameters between the two encoders, and finally creates a sequence of contextualized representations of the words and a sequence of contextualized representations of the phrases.Experiments have been conducted on benchmark datasets for Chinese and English, including CLUE, GLUE, SQuAD and RACE.The results show that AMBERT can outperform BERT in all cases, particularly the improvements are significant for Chinese.We also develop a method to improve the efficiency of AMBERT in inference, which still performs better than BERT with the same computational cost as BERT.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 421–435\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n421\nAMBERT: A Pre-trained Language Model with Multi-Grained\nTokenization\nXinsong Zhang, Pengshuai Li, Hang Li\nByteDance AI Lab\n{zhangxinsong.0320, lipengshuai.1204, lihang.lh}@bytedance.com\nAbstract\nPre-trained language models such as BERT\nhave exhibited remarkable performances in\nmany tasks in natural language understand-\ning (NLU). The tokens in the models are usu-\nally ﬁne-grained in the sense that for lan-\nguages like English they are words or sub-\nwords and for languages like Chinese they are\ncharacters. In English, for example, there\nare multi-word expressions which form natural\nlexical units and thus the use of coarse-grained\ntokenization also appears to be reasonable.\nIn fact, both ﬁne-grained and coarse-grained\ntokenizations have advantages and disadvan-\ntages for learning of pre-trained language mod-\nels. In this paper, we propose a novel pre-\ntrained language model, referred to as AM-\nBERT (A Multi-grained BERT), on the basis\nof both ﬁne-grained and coarse-grained tok-\nenizations. For English, AMBERT takes both\nthe sequence of words (ﬁne-grained tokens)\nand the sequence of phrases (coarse-grained\ntokens) as input after tokenization, employs\none encoder for processing the sequence of\nwords and the other encoder for processing\nthe sequence of the phrases, utilizes shared\nparameters between the two encoders, and ﬁ-\nnally creates a sequence of contextualized rep-\nresentations of the words and a sequence of\ncontextualized representations of the phrases.\nExperiments have been conducted on bench-\nmark datasets for Chinese and English, includ-\ning CLUE, GLUE, SQuAD and RACE. The\nresults show that AMBERT can outperform\nBERT in all cases, particularly the improve-\nments are signiﬁcant for Chinese. We also de-\nvelop a method to improve the efﬁciency of\nAMBERT in inference, which still performs\nbetter than BERT with the same computational\ncost as BERT.\n1 Introduction\nPre-trained models such as BERT, RoBERTa, and\nALBERT (Devlin et al., 2018; Liu et al., 2019; Lan\net al., 2019) have shown great power in natural\nlanguage understanding (NLU). The Transformer-\nbased language models are ﬁrst learned from a\nlarge corpus in pre-training, and then learned from\nlabeled data of a downstream task in ﬁne-tuning.\nWith Transformer (Vaswani et al., 2017), pre-\ntraining technique, and big data, the models can\neffectively capture the lexical, syntactic, and se-\nmantic relations between the tokens in the input\ntext and achieve state-of-the-art performance in\nmany NLU tasks, such as sentiment analysis, text\nentailment, and machine reading comprehension.\nIn BERT, for example, pre-training is mainly\nconducted based on masked language modeling\n(MLM) in which about 15% of the tokens in the\ninput text are masked with a special token [MASK],\nand the goal is to reconstruct the original text from\nthe masked tokens. Fine-tuning is separately per-\nformed for individual tasks as text classiﬁcation,\ntext matching, text span detection, etc. Usually, the\ntokens in the input text are ﬁne-grained; for exam-\nple, they are words or sub-words in English and\ncharacters in Chinese. In principle, the tokens can\nalso be coarse-grained, that is, for example, phrases\nin English and words in Chinese. There are many\nmulti-word expressions in English such as ‘New\nYork’ and ‘ice cream’ and the use of phrases also\nappears to be reasonable. It is more sensible to use\nwords (including single character words) in Chi-\nnese, because they are basic lexical units. In fact,\nall existing pre-trained language models employ\nsingle-grained (usually ﬁne-grained) tokenization.\nPrevious work indicates that the ﬁne-grained ap-\nproach and the coarse-grained approach have both\npros and cons. The tokens in the ﬁne-grained ap-\nproach are less complete as lexical units but their\nrepresentations are easier to learn (because there\nare less token types and more tokens in training\ndata), while the tokens in the coarse-grained ap-\nproach are more complete as lexical units but their\n422\nrepresentations are more difﬁcult to learn (because\nthere are more token types and less tokens in train-\ning data). Moreover, for the coarse-grained ap-\nproach there is no guarantee that tokenization (seg-\nmentation) is completely correct. Sometimes am-\nbiguity exists and it would be better to retain all\npossibilities of tokenization. In contrast, for the\nﬁne-grained approach tokenization is carried out at\nthe primitive level and there is no risk of ‘incorrect’\ntokenization.\nFor example, (Li et al., 2019) observe that ﬁne-\ngrained models consistently outperform coarse-\ngrained models in deep learning for Chinese lan-\nguage processing. They point out that the reason is\nthat low frequency words (coarse-grained tokens)\ntend to have insufﬁcient training data and tend to\nbe out of vocabulary, and as a result the learned\nrepresentations are not sufﬁciently reliable. On the\nother hand, previous work also demonstrates that\nmasking of coarse-grained tokens in pre-training\nof language models is helpful (Cui et al., 2019;\nJoshi et al., 2020). That is, although the model\nitself is ﬁne-grained, masking on consecutive to-\nkens (phrases in English and words in Chinese)\ncan lead to learning of a more accurate model. In\nAppendix A, we give examples of attention maps\nin BERT to further support the assertion.\nIn this paper, we propose A Multi-grained\nBERT model (AMBERT), which employs both\nﬁne-grained and coarse-grained tokenizations. For\nEnglish, AMBERT extends BERT by simultane-\nously constructing representations for both words\nand phrases in the input text using two encoders.\nSpeciﬁcally, AMBERT ﬁrst conducts tokenization\nat both word and phrase levels. It then takes the em-\nbeddings of words and phrases as input to the two\nencoders with the shared parameters. Finally it ob-\ntains a contextualized representation for the word\nand a contextualized representation for the phrase\nat each position. Note that the number of parame-\nters in AMBERT is comparable to that of BERT, be-\ncause the parameters in the two encoders are shared.\nThere are only additional parameters from multi-\ngrained embeddings. AMBERT can represent the\ninput text at both word-level and phrase-level, to\nleverage the advantages of the two approaches of\ntokenization, and create richer representations for\nthe input text at multiple granularity.\nAMBERT consists of two encoders and thus its\ncomputational cost is roughly doubled compared\nwith BERT. We also develop a method for im-\nproving the efﬁciency of AMBERT in inference,\nwhich only uses one of the two encoders. One\ncan choose either the ﬁne-grained encoder or the\ncoarse-grained encoder for a speciﬁc task using a\ndevelopment dataset.\nWe conduct extensive experiments to make a\ncomparison between AMBERT and the baselines\nas well as alternatives to AMBERT, using the\nbenchmark datasets in English and Chinese. The re-\nsults show that AMBERT signiﬁcantly outperforms\nsingle-grained BERT models with a large margin\nin both Chinese and English. In English, com-\npared to Google BERT, AMBERT achieves 2.0%\nhigher GLUE score, 2.5% higher RACE score, and\n5.1% more SQuAD score. In Chinese, AMBERT\nimproves average score by over 2.7% in CLUE.\nFurthermore, AMBERT with only one encoder can\npreform much better than the single-grained BERT\nmodels with a similar amount of inference time.\nWe make the following contributions.\n• Study of multi-grained pre-trained language\nmodels,\n• Proposal of a new pre-trained language model\ncalled AMBERT as an extension of BERT,\n• Empirical veriﬁcation of AMBERT on the En-\nglish and Chinese benchmark datasets GLUE,\nSQuAD, RACE, and CLUE,\n• Proposal of an efﬁcient inference method for\nAMBERT.\n2 Related work\nThere has been a large amount of work on pre-\ntrained language models. ELMo (Peters et al.,\n2018) is one of the ﬁrst pre-trained language mod-\nels for learning contextualized representations of\nwords in the input text. Leveraging the power of\nTransformer (Vaswani et al., 2017), GPTs (Rad-\nford et al., 2018, 2019) are developed as unidirec-\ntional models to make predictions on the input text\nin an auto-regressive manner, and BERT (Devlin\net al., 2018) is developed as a bidirectional model\nto make predictions on the whole or part of the\ninput text. Masked language modeling (MLM) and\nnext sentence prediction (NSP) are the two tasks\nin pre-training of BERT. Since the inception of\nBERT, a number of new models have been pro-\nposed to further enhance the performance of it. XL-\nNet (Yang et al., 2019) is a permutation language\nmodel which can improve the accuracy of MLM.\nRoBERTa (Liu et al., 2019) represents a new way\nof training more reliable BERT with a very large\n423\namount of data. ALBERT (Lan et al., 2019) is\na light-weight version of BERT, which shares pa-\nrameters across layers. StructBERT (Wang et al.,\n2019) incorporates word and sentence structures\ninto BERT to learn better representations of tokens\nand sentences. ERNIE2.0 (Sun et al., 2020) is a\nvariant of BERT pre-trained on multiple tasks with\ncoarse-grained tokens masked. ELECTRA (Clark\net al., 2020) has a GAN-style architecture for efﬁ-\nciently utilizing all tokens in pre-training.\nIt has been found that the use of coarse-grained\ntokens is beneﬁcial for pre-trained language mod-\nels. (Devlin et al., 2018) point out that ‘whole\nword masking’ is effective for training of BERT.\nIt is also observed that whole word masking is\nuseful for building a Chinese BERT (Cui et al.,\n2019). In ERNIE (Sun et al., 2019b), entity level\nmasking is employed as a strategy for pre-training\nand proved to be effective for language understand-\ning tasks (see also (Zhang et al., 2019)). In Span-\nBERT (Joshi et al., 2020), text spans are masked\nin pre-training and the learned model can substan-\ntially enhance the accuracies of span selection tasks.\nIt is indicated that word segmentation is especially\nimportant for Chinese and a BERT-based Chinese\ntext encoder is proposed with n-gram representa-\ntions (Diao et al., 2019). All existing work focuses\non the use of single-grained tokens in learning and\nutilization of pre-trained language models. In this\nwork, we propose a general technique of exploit-\ning multi-grained tokens for pre-trained language\nmodels and apply it to BERT.\n3 Our Method: AMBERT\nIn this section, we present the model, pre-training,\nand ﬁne-tuning of AMBERT. We also present a\ndiscussion on alternatives to AMBERT.\n3.1 Model\nFigure 1 gives an overview of AMBERT. AMBERT\ntakes a text as input. Tokenization is conducted on\nthe input text to obtain a sequence of ﬁne-grained\ntokens and a sequence of coarse-grained tokens.\nAMBERT has two encoders, one for processing the\nﬁne-grained token sequence and the other for pro-\ncessing the coarse-grained token sequence. Each\nof the encoders has exactly the same architecture\nas that of BERT (Devlin et al., 2018). The two\nencoders share the same parameters at each corre-\nsponding layer, except that each has its own token\nembedding parameters. The ﬁne-grained encoder\ngenerates contextualized representations from the\nsequence of ﬁne-grained tokens through its layers.\nIn parallel, the coarse-grained encoder generates\ncontextualized representations from the sequence\nof coarse-grained tokens through its layers. AM-\nBERT outputs a sequence of contextualized rep-\nresentations for the ﬁne-grained tokens and a se-\nquence of contextualized representations for the\ncoarse-grained tokens.\nAMBERT is expressive in that it learns and uti-\nlizes contextualized representations of the input\ntext at both ﬁne-grained and coarse-grained levels.\nThe model retains all possibilities of tokenizations\nand learns the attention weights (importance) of\nrepresentations of multi-grained tokens. AMBERT\nis also efﬁcient through sharing of parameters be-\ntween the two encoders. The parameters represent\nthe same ways of combining representations, no\nmatter whether representations are those of ﬁne-\ngrained tokens or coarse-grained tokens.\n3.2 Pre-Training\nPre-training of AMBERT is mainly conducted on\nthe basis of masked language modeling (MLM), at\nboth ﬁne-grained and coarse-grained levels. Next\nsentence prediction (NSP) is not essential as in-\ndicated in many studies after BERT (Lan et al.,\n2019; Liu et al., 2019). We only use NSP in our\nexperiments for comparison purposes. Let ˆx de-\nnote the sequence of ﬁne-grained tokens with some\nof them being masked, and ¯x denote the masked\nﬁne-grained tokens. Let ˆz denote the sequence of\ncoarse-grained tokens with some of them being\nmasked, and ¯z denote the masked coarse-grained\ntokens. Pre-training is deﬁned as optimization of\nthe following function,\nmin\nθ\n−log pθ(¯x,¯z|ˆx,ˆz) ≈\nmin\nθ\n−\nM∑\ni=1\nmilog pθ(xi|ˆx) −\nn∑\nj=1\nnj log pθ(zj|ˆz),\nwhere mi takes 1 or 0 as values and mi = 1 in-\ndicates that ﬁne-grained token xi is masked, m\ndenotes the total number of ﬁne-grained tokens;\nnj takes 1 or 0 as values and nj = 1 indicates\nthat coarse-grained token zj is masked, ndenotes\nthe total number of coarse-grained tokens; and θ\ndenotes parameters.\n3.3 Fine-Tuning\nIn ﬁne-tuning of AMBERT for classiﬁcation, the\nﬁne-grained encoder and coarse-grained encoder\n424\nFine-grained Encoder\nOutput : Contextualized representations of fine-grained and coarse-grained tokens.rx1 rx2rx0\n[CLS]…yorkmin…[SEP]\nCoarse-grained Encoder\n[CLS]anewyorkminster… …[SEP]\nrxm… rz1 rz2 rz3rz0 rzn…\nInput : A new chapel in York Minster was built in 1154. \nFigure 1: An overview of AMBERT, showing the process of creating multi-grained representations. The input\nis a sentence in English and output is the overall representation of the sentence. There are two encoders for\nprocessing the sequence of ﬁne-grained tokens and the sequence of coarse-grained tokens respectively. The ﬁnal\ncontextualized representations of ﬁne-grained tokens and coarse-grained tokens are denoted as rx0,rx1,··· ,rxm\nand rz0,rz1,··· ,rzn respectively.\ncreate special [CLS] representations, and both rep-\nresentations are used for classiﬁcation. Fine-tuning\nis deﬁned as optimization of the following function,\nwhich is a regularized loss of multi-task learning,\nstarting from the pre-trained model,\nmin\nθ\n−log pθ(y|x)\n= min\nθ\n−log pθ(y|rx0) −log pθ(y|rz0)\n−log pθ(y|[rx0,rz0]) + λ∥˜yx −˜yz∥2,\nwhere x is the input text, y is the classiﬁcation\nlabel, rx0 and rz0 are the [CLS] representations of\nﬁne-grained encoder and coarse-grained encoder,\n[a,b] denotes concatenation of vectors a and b,\nλis a regularization coefﬁcient, and ∥∥2 denotes\nL2 norm. The last term is based on agreement\nregularization (Brantley et al., 2019), which forces\nagreement between the predictions ( ˜yx and ˜yz).\nSimilarly, ﬁne-tuning of AMBERT for span de-\ntection can be carried out, in which the repre-\nsentations of ﬁne-grained tokens are concatenated\nwith the representations of corresponding coarse-\ngrained tokens. The concatenated representations\nare then utilized in the task.\n3.4 Inference\nWe propose two ways of using AMBERT in infer-\nence. One is to utilize the AMBERT itself and the\nother to utilize only one encoder of AMBERT. The\nformer performs better but needs more computa-\ntion and the latter performs slightly worse but only\nneeds computation comparable to BERT. One can\nchoose either drop the ﬁne-grained encoder or the\ncoarse-grained encoder in AMBERT through eval-\nuation using a development dataset, which makes\nthe computational cost close to that of BERT.\n3.5 Alternatives\nWe can consider two alternatives to AMBERT,\nwhich also rely on multi-grained tokenization. We\nrefer to them as AMBERT-Combo and AMBERT-\nHybrid and make comparisons of them with AM-\nBERT in our experiments.\nAMBERT-Combo has two individual encoders,\nan encoder (BERT) working on the ﬁne-grained to-\nken sequence and the other encoder (BERT) work-\ning on the coarse-grained token sequence, without\nparameter sharing between them. In learning and\ninference AMBERT-Combo simply combines the\noutput layers of the two encoders. Its ﬁne-tuning is\nsimilar to that of AMBERT.\nAMBERT-Hybrid has only one encoder (BERT)\nworking on both the ﬁne-grained token sequence\nand the coarse-grained token sequence. It creates\nrepresentations on the concatenation of two se-\nquences and lets the representations of the two\n425\nTable 1: Performance on classiﬁcation tasks in CLUE in terms of accuracy (%). The numbers in boldface denote\nthe best results of tasks. Average accuracies of models are also given. Numbers of parameters (param) and time\ncomplexities (cmplx) of models are also shown, where l, n, and d denote layer number, sequence length, and\nhidden representation size respectively. The tasks with mark † are those with data augmentation.\nModel Param. Cmplx. Avg. TNEWS† IFLYTEK CLUEWSC2020 † AFQMC CSL † CMNLI\nGoogle BERT 108M O(ln2d) 72.53 66.99 60.29 71.03 73.70 83.50 79.69\nOur BERT (char) 108M O(ln2d) 71.90 67.48 57.50 70.69 71.80 83.83 80.08\nOur BERT (word) 165M O(ln2d) 73.72 68.20 59.96 75.52 73.48 85.17 79.97\nAMBERT-Combo 273M O(2ln2d) 73.61 69.60 58.73 71.03 75.63 85.07 81.58\nAMBERT-Hybrid 176M O(4ln2d) 73.80 69.04 56.42 76.21 74.41 85.60 81.10\nAMBERT 176M O(2ln2d) 74.67 68.58 59.73 78.28 73.87 85.70 81.87\nTable 2: Performances on MRC tasks in CLUE in terms of F1, EM (Exact Match) and accuracy. The numbers in\nboldface denote the best results of tasks. Average scores of models are also given.\nModel Avg. CMRC2018 ChID C3\nDEV(F1,EM) TEST(EM) DEV(Acc.) TEST(Acc.) DEV(Acc.) TEST(Acc.)\nGoogle BERT 73.76 85.48 64.77 71.60 82.20 82.04 65.70 64.50\nOur BERT (char) 74.46 85.64 65.45 71.50 83.44 83.12 66.43 65.67\nOur BERT (word) 65.77 81.87 41.69 41.30 80.89 80.93 66.72 66.96\nAMBERT-Combo 75.26 86.12 65.11 72.00 84.53 84.64 67.74 66.70\nAMBERT-Hybrid 75.53 86.71 68.16 72.45 83.37 82.85 67.45 67.75\nAMBERT 77.47 87.29 68.78 73.25 87.20 86.62 69.52 69.63\nsequences interact with each other at each layer. Its\npre-training is formalized in the following function,\nmin\nθ\n−log pθ(¯x,¯z|ˆx,ˆz)\n≈min\nθ\n−\nm∑\ni=1\nmilog pθ(xi|ˆx,ˆz)\n−\nn∑\nj=1\nnj log pθ(zj|ˆx,ˆz),\nwhere the notations are the same as in (1). Its ﬁne-\ntuning is the same as that of BERT.\n4 Experiments\nWe make comparisons between AMBERT and the\nbaselines including ﬁne-grained BERT and coarse-\ngrained BERT, as well as the alternatives includ-\ning AMBERT-Combo and AMBERT-Hybrid, using\nbenchmark datasets in both Chinese and English.\nThe experiments on the alternatives can also be\nseen as ablation study on AMBERT. The ablation\nstudies for the regularization term λare given in\nthe Appendix E.\n4.1 Data for Pre-Training\nFor Chinese, we use a corpus consisting of 25 mil-\nlion documents (57G uncompressed text) from Jinri\nToutiao1. Note that there is no common corpus\nfor training of Chinese BERT. For English, we\n1Jinri Toutiao is a popular news app. in China.\nuse a corpus of 13.9 million documents (47G un-\ncompressed text) from Wikipedia and OpenWeb-\nText (Gokaslan and Cohen, 2019)2.\nThe characters in the Chinese texts are naturally\ntaken as ﬁne-grained tokens. We conduct word seg-\nmentation on the texts and treat the words as coarse-\ngrained tokens. We employ a word segmentation\ntool based on a n-gram model. Both tokenizations\nexploit WordPiece embeddings (Wu et al., 2016).\nThere are 21,128 characters and 72,635 words in\nthe vocabulary of Chinese.\nThe words in the English texts are naturally\ntaken as ﬁne-grained tokens. We perform coarse-\ngrained tokenization on the English texts in the\nfollowing way. First, we calculate the n-grams in\nthe Wikipedia documents using KenLM (Heaﬁeld,\n2011). We next build a phrase-level dictionary\nconsisting of phrases whose frequencies are sufﬁ-\nciently high and whose last words highly depend\non their previous words. We then employ a left-\nto-right search algorithm to perform phrase-level\ntokenization on the texts. There are 30,522 words\nand 77,645 phrases in the vocabulary of English.\n4.2 Experimental setup\nWe make use of the same parameter settings for\nthe AMBERT and BERT models. All models in\nthis paper are ‘base-models’ having 12 layers of\nencoder. It is too computationally expensive for\n2Unfortunately, BookCorpus, one of the two corpora in\nthe original paper for English BERT, is no longer publicly\navailable.\n426\nTable 3: State-of-the-art results of Chinese base models in CLUE.\nModel Params Avg. TNEWS† IFLYTEK WSC. † AFQMC CSL † CMNLI CMRC. ChID C3\nGoogle BERT 108M 72.59 66.99 60.29 71.03 73.70 83.50 79.69 71.60 82.04 64.50\nXLNet-mid 200M 73.00 66.28 57.85 78.28 70.50 84.70 81.25 66.95 83.47 67.68\nALBERT-xlarge 60M 73.05 66.00 59.50 69.31 69.96 84.40 81.13 76.30 80.57 70.32\nERNIE 108M 74.20 68.15 58.96 80.00 73.83 85.50 80.29 74.70 82.28 64.10\nRoBERTa 108M 74.38 67.63 60.31 76.90 74.04 84.70 80.51 75.20 83.62 66.50\nAMBERT 176M 75.28 68.58 59.73 78.28 73.87 85.70 81.87 73.25 86.62 69.63\nTable 4: Performances on the tasks in GLUE. Average score over all the tasks is slightly different from the ofﬁcial\nGLUE score, since we exclude WNLI. CoLA uses Matthew’s Corr. MRPC and QQP use both F1 and accuracy\nscores. STS-B computes Pearson-Spearman Corr. Accuracy scores are reported for the other tasks. Results of\nMNLI include MNLI-m and MNLI-mm. The other settings are the same as Table 1.\nModel Param Cmplx Avg. CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE\nGoogle BERT 110M O(ln2d) 80.7 52.1 93.5 88.9/81.9 81.5/85.8 71.2/88.5 84.6/83.4 90.5 66.4\nOur BERT (word) 110M O(ln2d) 81.6 53.7 93.8 88.8/84.8 84.3/86.0 71.6/89.0 85.0/84.5 91.2 66.8\nOur BERT (phrase) 170M O(ln2d) 80.7 54.8 93.8 87.4/82.5 82.9/84.9 70.1/88.8 84.1/83.8 90.6 65.1\nAMBERT-Combo 280M O(2ln2d) 81.8 57.1 94.5 89.2/84.8 84.4/85.8 71.8/88.6 84.7/84.2 90.4 66.2\nAMBERT-Hybrid 194M O(4ln2d) 81.7 50.9 93.4 89.0/85.2 84.7/87.6 71.0/89.2 84.6/84.7 91.2 68.5\nAMBERT 194M O(2ln2d) 82.7 54.3 94.5 89.7/86.1 84.7 /87.1 72.5/89.4 86.3/85.3 91.5 70.5\nus to train the models as ‘large models’ having 24\nlayers. To retain consistency, the masked spans in\nthe coarse-grained encoder are also masked in the\nﬁne-grained encoder. The details of pre-training\nand ﬁne-tuning are the same as those in the original\nBERT paper (Devlin et al., 2018), which are given\nin Appendix C.\n4.3 Chinese Tasks\n4.3.1 Benchmarks\nWe use the benchmark datasets, Chinese Language\nUnderstanding Evaluation (CLUE) (Xu et al., 2020)\nfor experiments in Chinese. CLUE contains six\nclassiﬁcation tasks, that are TNEWS, IFLYTEK\nand CLUEWSC2020, AFQMC, CSL and CMNLI3,\nand three Machine Reading Comprehension (MRC)\ntasks which are CMRC2018, ChID and C3. The\ndetails of all the benchmarks are shown in Ap-\npendix B. Data augmentation is also performed\nfor all models in the tasks of TNEWS, CSL and\nCLUEWSC2020 to achieve better performance\n(see Appendix D for detailed explanation).\n4.3.2 Experimental Results\nWe compare AMBERT with the BERT baselines,\nincluding the BERT model released from Google,\nreferred to as Google BERT, and the BERT model\ntrained by us, referred to as Our BERT, including\nﬁne-grained (character) and coarse-grained (word)\nmodels. Case study is given in Appendix F.\nTable 1 shows the results of the classiﬁcation\ntasks. AMBERT improves average scores of the\n3The task is introduced at the CLUE website.\nBERT baselines by about 1.0% and also works bet-\nter than AMBERT-Combo and AMBERT-Hybrid.\nThe results of MRC tasks are shown in Table 2.\nAMBERT improves average scores of the BERT\nbaselines by over 3.0%. Our BERT (word) per-\nforms poorly in CMRC2018. This is probably\nbecause the results of word segmentation are not\naccurate enough for the task. AMBERT-Combo\nand AMBERT-Hybrid are on average better than\nsingle-grained BERT models. AMBERT further\noutperforms both of them.\nWe also compare AMBERT with the state-of-\nthe-art models such as RoBERTa and ALBERT in\nCLUE benchmark. The base models are trained\nwith different datasets and procedures, and thus the\ncomparisons should only be taken as references.\nNote that the settings of the base models are the\nsame as that of Xu et al. (2020). Table 3 shows the\nresults. The average score of AMBERT is higher\nthan all the other models. We conclude that multi-\ngrained tokenization is very helpful for pre-trained\nlanguage models and the design of AMBERT is\nreasonable.\n4.4 English Tasks\n4.4.1 Benchmarks\nThe General Language Understanding Evaluation\n(GLUE) benchmark (Wang et al., 2018) is a collec-\ntion of nine NLU tasks. Following BERT (Devlin\net al., 2018), we exclude the task WNLI for the\nreason that results of different models on this task\nare undifferentiated. In addition, three MRC tasks\nare also included, i.e., SQuAD v1.1, SQuAD v2.0,\n427\nTable 5: Performances on three English MRC tasks. We use EM and F1 to evaluate the performance of text\ndetection, and report accuracies for RACE, on both development set and test set.\nModel Avg. SQuAD 1.1 SQuAD 2.0 RACE\nDEV(EM, F1) DEV(EM, F1) TEST(EM, F1) DEV TEST\nGoogle BERT 74.0 80.8 88.5 70.1 73.5 73.7 76.3 64.5 64.3\nOur BERT (word) 76.7 83.8 90.6 76.6 79.6 77.3 80.3 62.4 62.6\nOur BERT (phrase) - 67.4 82.3 55.4 62.6 - - 66.9 66.1\nAMBERT-Combo 77.2 84.0 90.9 76.4 79.6 76.6 79.8 66.6 63.7\nAMBERT-Hybrid 77.3 83.6 90.3 76.4 79.4 76.7 79.7 67.1 65.1\nAMBERT 78.6 84.2 90.8 77.6 80.6 78.6 81.4 68.9 66.8\nTable 6: State-of-the-art results of English base models in GLUE. Each task only reports one score following Clark\net al. (2020), and we report the average EM of SQuAD1.1 and SQuAD2.0 on development set. AMBERT ‡ is\npre-trained with a corpora with size comparable to that of RoBERTa (160G uncompressed text). Scores with⋆are\nreported from the published papers.\nModel Params Avg. CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE SQuAD RACE\nGoogle BERT 110M 78.7 52.1⋆ 93.5⋆ 84.8⋆ 85.8⋆ 89.2⋆ 84.6⋆ 90.5⋆ 66.4⋆ 75.5 64.3⋆\nXLNet 110M 78.6 47.9 94.3 83.3 84.1 89.2 86.8 91.7 61.9 79.9⋆ 66.7⋆\nSpanBERT 110M 79.1 51.2 93.5 87.0 82.9 89.2 85.1 92.7 69.7 81.8 57.4\nELECTRA 110M 81.3 59.7⋆ 93.4⋆ 86.7⋆ 87.7⋆ 89.1⋆ 85.8⋆ 92.7⋆ 73.1⋆ 74.8 69.9\nALBERT 12M 80.1 53.2 93.2 87.5 87.2 87.8 85.0 91.2 71.1 78.7 65.8\nRoBERTa 135M 82.7 61.5 95.8 88.7 88.9 89.4 87.4 93.1 74.0 78.6 69.9\nAMBERT‡ 194M 82.8 60.0 95.2 88.9 88.2 89.5 87.2 92.6 72.6 82.5 71.2\nand RACE. The details of English benchmarks can\nbe found in Appendix B.\n4.4.2 Experimental Results\nWe compare AMBERT with the BERT models on\nthe tasks in GLUE. The results of Google BERT\nare from the original paper (Devlin et al., 2018),\nand the results of Our BERT are obtained by us.\nFrom Table 4 we can see the following trends, 1)\nMulti-grained models, particularly AMBERT, can\nachieve better results than single-grained models.\n2) Among the multi-grained models, AMBERT\nperforms best with fewer parameters and less com-\nputation. Case study is given in Appendix F.\nWe also make comparison on the MRC tasks.\nThe results of Google BERT are either from the\npapers (Devlin et al., 2018; Yang et al., 2019) or\nfrom our runs with the ofﬁcial code. From Table 5\nwe make the following conclusions. 1) in SQuAD,\nAMBERT outperforms Google BERT with a large\nmargin. Our BERT (word) generally performs well\nand Our BERT (phrase) performs poorly in the span\ndetection tasks. 2) In RACE, AMBERT performs\nbest among all the baselines for both development\nset and test set. 3) AMBERT is the best multi-\ngrained model.\nWe compare AMBERT with the state-of-the-art\nmodels in both GLUE and MRC benchmarks. The\nresults of baselines, in Table 6, are either reported\nin published papers or re-implemented by us with\nHuggingFace’s Transformer (Wolf et al., 2019). We\nuse the provided implementation in HuggingFace’s\nTransformer, without additional data augmentation,\nquestion-answering module4 and other tricks. Note\nthat AMBERT outperforms all the models on av-\nerage without using training techniques such as\nbigger batches and dynamic masking.\n4.5 Enhancement of Inference Speed\nWe also conduct experiments on the efﬁ-\ncient inference method of AMBERT on\nCLUE/GLUE/SQuAD/RACE. We choose\nthe ﬁne-grained encoder for the span detection\ntasks (CMRC2018 and SQuAD) because it\nperforms much better in the tasks. We choose\nthe coarse-grained encoder for the other Chinese\ntasks and the ﬁne-grained encoder for the other\nEnglish tasks because they perform better on\naverage. All the decisions are made based on the\nresults from the Dev datasets. The detailed results\nare shown in Table 7. We conclude that, a) for\nthe English tasks, AMBERT with one chosen\nencoder achieves similar results as AMBERT with\ntwo encoders and outperforms the single-grained\n“Our BERT” models with a large margin; b) for\nthe Chinese tasks, AMBERT with one chosen\nencoder performs slightly worse than AMBERT\nbut performs much better than the single-grained\n“Our BERT” models. Therefore, in practice, one\ncan train an AMBERT with two encoders and use\nonly one of the encoders in inference.\n4For that reason, we cannot use the results for SQuAD 2.0\nin Clark et al. (2020).\n428\nTable 7: Performances on the development sets of CLUE, GLUE, SQuAD and RACE with different ways for\ninference. CN-Models and EN-Models denote Chinese and English pre-trained models respectively. CoLA uses\nMatthew’s Corr. We report EM of CMRC2018 and average EM of SQuAD1.1 and SQuAD2.0. The other metrics\nare all accuracies. We report the better results among single-grained models as “Our BERT”.\nCN-Models Speedup Avg. TNEWS IFLYTEK CLUEWSC2020 AFQMC CSL CMNLI CMRC2018 ChID C3 -\nAMBERT 1.0 75.3 68.1 60.1 81.6 74.7 85.6 82.3 68.8 87.2 69.5 -\nAMBERT (one encoder) 2.0x 74.8 68.0 59.5 81.3 74.2 85.5 82.1 67.4 86.6 68.5 -\nOur BERT 2.0x 73.4 67.8 58.7 79.0 74.1 84.5 80.8 65.5 83.4 66.7 -\nEN-Models Speedup Avg. CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE SQuAD RACE\nAMBERT 1.0 79.2 61.7 94.3 92.3 55.0 91.2 86.2 91.3 70.2 80.9 68.9\nAMBERT (one encoder) 2.0x 79.1 62.2 93.2 92.5 55.0 91.2 86.1 91.4 70.6 80.3 68.0\nOur BERT 2.0x 77.5 56.6 92.4 89.7 54.2 90.4 85.1 90.6 69.1 80.2 66.9\nO u r  B E R T  ( c h a r )\nO u r  B E R T  ( w o r d )\nA M B E R T - H y b r i d\nA M B E R T\nO u r  B E R T  ( w o r d )\nO u r  B E R T  ( p h r a s e )\nA M B E R T - H y b r i d\nA M B E R T\nFigure 2: Attention weights of ﬁrst layers of Our BERT (word/phrase), AMBERT-Hybrid and AMBERT, for\nEnglish and Chinese sentences.\n\u0006\u0006\u0005\u0007\u0006\u0005\b\u0006\u0005\t\u0006\u0005\n\u0006\u0005\u000b\u0006\u0005\f\u0006\u0005\r\u0006\u0005\u000e\u0006\u0005\u000f\u0007\n\u0012#\u0017\u0010\u0018\u001c\u001a\u0012\u0018\u0019\u0017\u0016\u001b\u0019\u0017\u0016\n\u0003\u000b\u0006\n\b\f\u0007\u0001\r\u0004\f\t\f\n\u0012\u0013\u0001\u0002\u0010\u0018\u0011\u0014\u001c\u001e\u0004\u0012#\"!#\u0003\u0014\u0013\u0001\u0002\u0010\u0018\u0011\u0014\u001c\u001e\u0004\u0012#\"!#\u0003\u0012\u0013\u0001\u0002\u0010\u0018\u0011\u0014\u001c\u001e\u0003\u0014\u0013\u0001\u0002\u0010\u0018\u0011\u0014\u001c\u001e\u0003\u0006\u0006\u0005\u0007\u0006\u0005\b\u0006\u0005\t\u0006\u0005\n\u0006\u0005\u000b\u0006\u0005\f\u0006\u0005\r\u0006\u0005\u000e\u0006\u0005\u000f\u0007\n\u001e\u0019\u0014 \u001d\u0012\u0017\u001f\u0014 \u001d\u0012\b\u0006\b\u0006\u0012\u0018\u0019\u0017\u0016\u0010\u0015\u001b\u0018\u0012\n\u0002\u0007\b\u000b\u0005\f\u0005\u0001\r\u0004\f\t\f\n\u0012\u0013\u0001\u0002\u0010\u0018\u0011\u0014\u001c\u001e\u0004\u0012#\"!#\u0003\u0014\u0013\u0001\u0002\u0010\u0018\u0011\u0014\u001c\u001e\u0004\u0012#\"!#\u0003\u0012\u0013\u0001\u0002\u0010\u0018\u0011\u0014\u001c\u001e\u0003\u0014\u0013\u0001\u0002\u0010\u0018\u0011\u0014\u001c\u001e\u0003\nFigure 3: Distances between representations of ﬁne-\ngrained and coarse-grained encoders (representations\nof [CLS]) in AMBERT-Combo and AMBERT. CD and\nED stand for cosine distance (one minus cosine similar-\nity) and normalized Euclidean distance respectively.\n4.6 Discussions\nWe further investigate the reason that AMBERT is\nsuperior to AMBERT-Combo. Figure 3 shows the\ndistances between the [CLS] representations of the\nﬁne-grained encoder and coarse-grained encoder in\nAMBERT-Combo and AMBERT after pre-training,\nin terms of cosine distance (one minus cosine sim-\nilarity) and normalized Euclidean distance. One\ncan see that the distances in AMBERT-Combo are\nlarger than the distances in AMBERT. We perform\nthe assessment using the data in different tasks and\nﬁnd similar trends. The results indicate that the\nrepresentations of ﬁne-grained encoder and coarse-\ngrained encoder are closer in AMBERT than in\nAMBERT-Combo. These are natural consequences\nof using AMBERT and AMBERT-Combo, whose\nparameters are respectively shared and unshared\nacross encoders. It implies that the higher perfor-\nmances by AMBERT is due to its parameter shar-\ning, which can learn and represent similar ways of\ncombining tokens no matter whether they are ﬁne-\ngrained or coarse-grained. An intuitive explanation\nis that the ways of combining representations of\nﬁne-grained tokens and the ways of combining rep-\nresentations of coarse-grained tokens “in the same\ncontexts” are exactly the same.\nWe also examine the reasons that AMBERT\nworks better than AMBERT-Hybrid, while both\nof them exploit multi-grained tokenization. Fig-\nure 2 shows the attention weights of ﬁrst layers in\nAMBERT and AMBERT-Hybrid, as well as the\nsingle-grained BERT models, after pre-training. In\nAMBERT-Hybrid, the ﬁne-grained tokens attend\nmore to the corresponding coarse-grained tokens\nand as a result the attention weights among ﬁne-\ngrained tokens are weakened. In contrast, in AM-\nBERT the attention weights among ﬁne-grained\n429\ntokens and those among coarse-grained tokens are\nintact. It appears that attentions among single-\ngrained tokens (ﬁne-grained ones or coarse-grained\nones) play important roles in downstream tasks.\nTo answer the question why the improvements\nby AMBERT on Chinese are larger than on English\nin the same pre-training settings, we further make\nan analysis. We respectively tokenize 10,000 ran-\ndomly selected Chinese sentences from ﬁve tasks in\nCLUE with our Chinese word tokenizer. The aver-\nage proportion of words is 51.5%, which indicates\nthat about half of the tokens are ﬁne-grained and\nhalf are coarse-grained in Chinese. Similarly, we to-\nkenize 10,000 randomly selected English sentences\nfrom ﬁve different tasks in GLUE with our En-\nglish phrase tokenizer. The average proportion of\nphrases is only 13.1%, which means that there are\nmuch less coarse-grained tokens than ﬁne-grained\ntokens in English. (Please refer to Table 10 in\nthe Appendix for more details of the experiments.)\nTherefore, we postulate that for Chinese it is nec-\nessary for a model to process the language at both\nﬁne-grained and coarse-grained levels. AMBERT\nindeed has the capability.\n5 Conclusion\nIn this paper, we have proposed a novel pre-trained\nlanguage model called AMBERT, as an extension\nof BERT. AMBERT employs multi-grained tok-\nenization, that is, it uses both words and phrases in\nEnglish and both characters and words in Chinese.\nWith multi-grained tokenization, AMBERT learns\nin parallel the representations of the ﬁne-grained\ntokens and the coarse-grained tokens using two en-\ncoders with shared parameters. We also develop an\nalternative way of using AMBERT in inference to\nsave computation cost. Experimental results have\ndemonstrated that AMBERT signiﬁcantly outper-\nforms BERT and other models in NLU tasks in\nboth English and Chinese. AMBERT increases\naverage score of Google BERT by about 2.7% in\nChinese benchmark CLUE. AMBERT improves\nGoogle BERT by over 3.0% on a variety of tasks in\nEnglish benchmarks GLUE, SQuAD (1.1 and 2.0),\nand RACE.\nAs future work, we plan to study the follow-\ning issues: 1) to investigate model acceleration\nmethods in learning of AMBERT, such as sparse\nattention (Child et al., 2019; Kitaev et al., 2020; Za-\nheer et al., 2020) and synthetic attention (Tay et al.,\n2020); 2) to apply the technique of AMBERT into\nother pre-trained language models such as XLNet;\n3) to employ AMBERT in other NLU tasks.\nAcknowledgments\nWe thank the teams at ByteDance for providing the\nChinese corpus and the Chinese word segmentation\ntool.\nReferences\nLuisa Bentivogli, Peter Clark, Ido Dagan, and Danilo\nGiampiccolo. 2009. The ﬁfth pascal recognizing tex-\ntual entailment challenge. In TAC.\nKiant´e Brantley, Wen Sun, and Mikael Henaff. 2019.\nDisagreement-regularized imitation learning. In\nInternational Conference on Learning Representa-\ntions.\nDaniel Cer, Mona Diab, Eneko Agirre, Inigo Lopez-\nGazpio, and Lucia Specia. 2017. Semeval-2017\ntask 1: Semantic textual similarity-multilingual and\ncross-lingual focused evaluation. arXiv preprint\narXiv:1708.00055.\nRewon Child, Scott Gray, Alec Radford, and\nIlya Sutskever. 2019. Generating long se-\nquences with sparse transformers. arXiv preprint\narXiv:1904.10509.\nKevin Clark, Minh-Thang Luong, Quoc V Le, and\nChristopher D Manning. 2020. Electra: Pre-training\ntext encoders as discriminators rather than genera-\ntors. arXiv preprint arXiv:2003.10555.\nYiming Cui, Wanxiang Che, Ting Liu, Bing Qin,\nZiqing Yang, Shijin Wang, and Guoping Hu. 2019.\nPre-training with whole word masking for chinese\nbert. arXiv preprint arXiv:1906.08101.\nYiming Cui, Ting Liu, Wanxiang Che, Li Xiao,\nZhipeng Chen, Wentao Ma, Shijin Wang, and Guop-\ning Hu. 2018. A span-extraction dataset for chinese\nmachine reading comprehension. arXiv preprint\narXiv:1810.07366.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. arXiv preprint arXiv:1810.04805.\nShizhe Diao, Jiaxin Bai, Yan Song, Tong Zhang, and\nYonggang Wang. 2019. Zen: pre-training chinese\ntext encoder enhanced by n-gram representations.\narXiv preprint arXiv:1911.00720.\nWilliam B Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nAaron Gokaslan and Vanya Cohen. 2019. Openweb-\ntext corpus. http://Skylion007.github.io/\nOpenWebTextCorpus.\n430\nKenneth Heaﬁeld. 2011. KenLM: Faster and smaller\nlanguage model queries. In Proceedings of the\nSixth Workshop on Statistical Machine Translation,\npages 187–197, Edinburgh, Scotland. Association\nfor Computational Linguistics.\nGanesh Jawahar, Beno ˆıt Sagot, and Djam ´e Seddah.\n2019. What does BERT learn about the structure\nof language? In Proceedings of the 57th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 3651–3657. Association for Compu-\ntational Linguistics.\nMandar Joshi, Danqi Chen, Yinhan Liu, Daniel S Weld,\nLuke Zettlemoyer, and Omer Levy. 2020. Spanbert:\nImproving pre-training by representing and predict-\ning spans. Transactions of the Association for Com-\nputational Linguistics, 8:64–77.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nNikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\n2020. Reformer: The efﬁcient transformer. arXiv\npreprint arXiv:2001.04451.\nZhenzhong Lan, Mingda Chen, Sebastian Goodman,\nKevin Gimpel, Piyush Sharma, and Radu Soricut.\n2019. Albert: A lite bert for self-supervised learn-\ning of language representations. arXiv preprint\narXiv:1909.11942.\nXiaoya Li, Yuxian Meng, Xiaofei Sun, Qinghong Han,\nArianna Yuan, and Jiwei Li. 2019. Is word segmen-\ntation necessary for deep learning of chinese repre-\nsentations? arXiv preprint arXiv:1905.05526.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nMatthew E Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. arXiv preprint arXiv:1802.05365.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nBlog, 1(8):9.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D Manning, Andrew Y Ng,\nand Christopher Potts. 2013. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 conference on\nempirical methods in natural language processing,\npages 1631–1642.\nKai Sun, Dian Yu, Dong Yu, and Claire Cardie.\n2019a. Probing prior knowledge needed in challeng-\ning chinese machine reading comprehension. arXiv\npreprint arXiv:1904.09679.\nYu Sun, Shuohuan Wang, Yu-Kun Li, Shikun Feng,\nHao Tian, Hua Wu, and Haifeng Wang. 2020. Ernie\n2.0: A continual pre-training framework for lan-\nguage understanding. In AAAI, pages 8968–8975.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi\nChen, Han Zhang, Xin Tian, Danxiang Zhu, Hao\nTian, and Hua Wu. 2019b. Ernie: Enhanced rep-\nresentation through knowledge integration. arXiv\npreprint arXiv:1904.09223.\nYi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan,\nZhe Zhao, and Che Zheng. 2020. Synthesizer: Re-\nthinking self-attention in transformer models. arXiv\npreprint arXiv:2005.00743.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in neural information pro-\ncessing systems, pages 5998–6008.\nJesse Vig. 2019. A multiscale visualization of at-\ntention in the transformer model. arXiv preprint\narXiv:1906.05714.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGlue: A multi-task benchmark and analysis platform\nfor natural language understanding. arXiv preprint\narXiv:1804.07461.\nWei Wang, Bin Bi, Ming Yan, Chen Wu, Zuyi Bao,\nLiwei Peng, and Luo Si. 2019. Structbert: In-\ncorporating language structures into pre-training\nfor deep language understanding. arXiv preprint\narXiv:1908.04577.\nAlex Warstadt, Amanpreet Singh, and Samuel R Bow-\nman. 2019. Neural network acceptability judgments.\nTransactions of the Association for Computational\nLinguistics, 7:625–641.\nAdina Williams, Nikita Nangia, and Samuel R Bow-\nman. 2017. A broad-coverage challenge corpus for\nsentence understanding through inference. arXiv\npreprint arXiv:1704.05426.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R’emi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface’s trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google’s neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\n431\nLiang Xu, Xuanwei Zhang, Lu Li, Hai Hu, Chen-\njie Cao, Weitang Liu, Junyi Li, Yudong Li, Kai\nSun, Yechen Xu, et al. 2020. Clue: A chinese lan-\nguage understanding evaluation benchmark. arXiv\npreprint arXiv:2004.05986.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in neural in-\nformation processing systems, pages 5753–5763.\nManzil Zaheer, Guru Guruganesh, Avinava Dubey,\nJoshua Ainslie, Chris Alberti, Santiago Ontanon,\nPhilip Pham, Anirudh Ravula, Qifan Wang, Li Yang,\net al. 2020. Big bird: Transformers for longer se-\nquences. arXiv preprint arXiv:2007.14062.\nZhengyan Zhang, Xu Han, Zhiyuan Liu, Xin Jiang,\nMaosong Sun, and Qun Liu. 2019. Ernie: En-\nhanced language representation with informative en-\ntities. arXiv preprint arXiv:1905.07129.\nChujie Zheng, Minlie Huang, and Aixin Sun. 2019.\nChid: A large-scale chinese idiom dataset for cloze\ntest. arXiv preprint arXiv:1906.01265.\n432\nA Attention maps for single-grained\nmodels\nWe construct ﬁne-grained and coarse-grained\nBERT models for English and Chinese, and ex-\namine the attention maps of the models using the\nBertViz tool (Vig, 2019). Figure 4 shows the at-\ntention maps of the ﬁrst layer of ﬁne-grained mod-\nels for several sentences in English and Chinese.\nOne can see that there are tokens that improperly\nattend to other tokens in the sentences. For exam-\nple, in the English sentences, the words “drawing”,\n“new”, and “dog” have high attention weights to\n“portrait”, “york”, and “food”, respectively, which\nare not appropriate. For example, in the Chinese\nsentences, the chars “ 拍”, “北”, “长” have high\nattention weights to “卖”, “京”, “市”, respectively,\nwhich are also not reasonable. (It is veriﬁed that the\nbottom layers at BERT mainly represent lexical in-\nformation, the middle layers mainly represent syn-\ntactic information, and the top layers mainly repre-\nsent semantic information (Jawahar et al., 2019).)\nIdeally a token should only attend to the tokens\nwith which they form a lexical unit at the ﬁrst layer.\nThis cannot be guaranteed in the ﬁne-grained BERT\nmodel, however, because usually a ﬁne-grained to-\nken may belong to multiple lexical units (i.e., there\nis ambiguity).\nFigure 5 shows the attention maps of the ﬁrst\nlayer of coarse-grained models for the same sen-\ntences in English and Chinese. In the English sen-\ntences, the words are combined into the phrases of\n“drawing room”, “york minister”, and “dog food”.\nThe attentions are appropriate in the ﬁrst two sen-\ntences, but it is not in the last sentence because of\nthe incorrect tokenization. Similarly, in the Chi-\nnese sentences, the high attention weights of words\n“ 球拍(bat)” and “京城(capital)” are reasonable,\nbut that of word “市长(mayor)” is not. Note that\nincorrect tokenization is inevitable.\nB Detailed descriptions for the\nbenchmarks\nB.1 Chinese Tasks\nTNEWS is a text classiﬁcation task in which ti-\ntles of news articles in TouTiao are to be clas-\nsiﬁed into 15 classes. IFLYTEK is a task of\nassigning app descriptions into 119 categories.\nCLUEWSC2020, standing for the Chinese Wino-\ngrad Schema Challenge, is a co-reference resolu-\ntion task. AFQMC is a binary classiﬁcation task\nthat aims to predict whether two sentences are\nsemantically similar. CSL uses the Chinese Sci-\nentiﬁc Literature dataset containing abstracts and\ntheir keywords of papers and the goal is to identify\nwhether given keywords are the original keywords\nof a paper. CMNLI is based on translation from\nMNLI (Williams et al., 2017), which is a large-\nscale, crowd-sourced entailment classiﬁcation task.\nCMRC2018 (Cui et al., 2018) makes use of a span-\nbased dataset for Chinese machine reading compre-\nhension. ChID (Zheng et al., 2019) is a large-scale\nChinese IDiom cloze test. C3 (Sun et al., 2019a) is\na free-form multiple-choice machine reading com-\nprehension for Chinese.\nB.2 English Tasks\nCoLA (Warstadt et al., 2019) contains English ac-\nceptability judgments drawn from books and jour-\nnal articles on linguistic theory. SST-2 (Socher\net al., 2013) consists of sentences from movie re-\nviews and human annotations of their sentiment.\nMRPC (Dolan and Brockett, 2005) is a corpus of\nsentence pairs automatically extracted from online\nnews sources, and the target is to identify whether\na sentence pair is semantically equivalent. STS-\nB (Cer et al., 2017) is a collection of sentence\npairs and the task is to predict similarity scores.\nQQP is a collection of question pairs and requires\nmodels to recognize semantically equivalent ones.\nMNLI (Williams et al., 2017) is a crowd-sourced\ncollection of sentence pairs with textual entail-\nment annotations. QNLI (Wang et al., 2018) is a\nquestion-answering dataset consisting of question-\nparagraph pairs, where one of the sentences in the\nparagraph contains the answer to the corresponding\nquestion. RTE (Bentivogli et al., 2009) comes from\na series of annual textual entailment challenges.\nC Hyper-parameters\nC.1 Hyper-parameters in pre-training\nIn pre-training of the AMBERT models, in to-\ntal 15% of the coarse-grained tokens are masked,\nwhich is the same proportion for the BERT models.\nWe adopt the standard hyper-parameters of BERT\nin pre-training of the models except batch sizes\nwhich are tuned to make our ﬁne-grained BERT\nmodels comparable to the Google BERT models.\nTable 8 shows the hyper-parameters in our Chi-\nnese AMBERT and English AMBERT. Our BERT\nmodels and alternatives of AMBERT (AMBERT-\nCombo and AMBERT-Hybrid) all use the same\n433\nFigure 4: Attention maps of ﬁrst layers of ﬁne-grained BERT models for English and Chinese sentences. The\nChinese sentences are “ 商店里的兵乓球拍卖完了 (Table tennis bats are sold out in the shop)”, “ 北上京城\n施展平生报复 (Go north to Beijing to fulﬁll the dream)”, “ 南京市长江大桥位于南京 (The Nanjing Yantze\nRiver bridge is located in Nanjing)”. Different colors represent attention weights in different heads and darkness\nrepresents weight.\nhyper-parameters in pre-training. The optimizer is\nAdam (Kingma and Ba, 2014). To enhance efﬁ-\nciency, we use mixed precision for all the models.\nTraining is carried out on Nvidia V-100. The num-\nbers of GPUs used for training are from 32 to 64,\ndepending on the model sizes.\nTable 8: Hyper-parameters for pre-trained AMBERT.\nHyperparam Chinese AMBERT English AMBERT\nNumber of Layers l 12 12\nHidden Size d 768 768\nSequence Lengh n 512 512\nFFN Inner Hidden Size 3072 3072\nAttention Heads 12 12\nAttention Head Size 64 64\nDropout 0.1 0.1\nAttention Dropout 0.1 0.1\nWarmup Steps 10,000 10,000\nPeak Learning Rate 1e-4 1e-4\nBatch Size 512 1024\nWeight Decay 0.01 0.01\nMax Steps 1m 500k\nLearning Rate Decay Linear Linear\nAdam ϵ 1e-6 1e-6\nAdam β1 0.9 0.9\nAdam β2 0.999 0.999\nC.2 Hyper-parameters in Fine-tuning\nFor the Chinese tasks, since all the original pa-\npers do not report detailed hyper-parameters in\nﬁne-tuning of the baseline models, we uniformly\nuse the same hyper-parameters as shown in Ta-\nble 11 except training epoch, because AMBERT\nand AMBERT-Combo have more parameters and\nneed more training to get converged. We choose\nthe training epochs for all models when the per-\nformances on development sets stop to improve.\nTable 11 also shows all the hyper-parameters in\nﬁne-tuning of the English models. We adopt the\nbest hyper-parameters in the original papers for\nthe baselines. Moreover, for AMBERT‡, we also\ntune learning rate ([1e-5, 2e-5, 3e-5]) and batch\nsize ([16, 32]) for GLUE with the same method in\nRoBERTa (Liu et al., 2019).\nD Data Augmentation\nTo enhance the performance, we conduct data\naugmentation for the three Chinese classiﬁcation\ntasks of TNEWS, CSL, and CLUEWSC2020. In\nTNEWS, we use both keywords and titles. In CSL,\nwe concatenate keywords with a special token “ ”.\nIn CLUEWSC2020, we duplicate a few instances\nhaving pronouns in the training data such as “ 她\n(she)”.\nE Regularization in Fine-tuning\nTable 9 shows the results of using different values\nas regularization coefﬁcients in ﬁne-tuning on the\ndevelopment sets of CLUE, GLUE and RACE. It\nappears that for most tasks the use of regularization\n434\nFigure 5: Attention maps of ﬁrst layers of coarse-grained BERT models for English and Chinese sentences. Note\nthat tokenizations may have errors.\nTable 9: Performances on the development sets of CLUE, GLUE and RACE with different regularization coefﬁ-\ncients in ﬁne-tuning. CN-Models and EN-Models stand for Chinese and English pre-trained models respectively.\nCoLA uses Matthew’s Corr. The other metrics are accuracies.\nCN-Models λ TNEWS IFLYTEK CLUEWSC2020 AFQMC CSL CMNLI ChID C3 -\nAMBERT 1.0 68.1 60.1 81.6 74.7 85.6 82.3 87.1 69.2 -\nAMBERT 0.0 67.9 60.3 80.9 75.4 85.0 81.1 86.5 69.2 -\nEN-Models λ CoLA SST-2 MRPC STS-B QQP MNLI QNLI RTE RACE\nAMBERT 1.0 61.7 94.3 92.3 55.0 91.2 86.2 91.3 70.2 66.6\nAMBERT 0.0 61.5 93.4 90.1 54.5 91.1 85.5 91.2 70.2 66.8\nTable 10: The rate of coarse-grained tokens (not in-\ncluded in ﬁne-grained vocabulary) in coarse-grained to-\nkenization.\nDatasets Chinese words Chinese total tokens word rate (%)\nCMNLI 157,511 335,187 47.0\nTNEWS 71,636 137,965 51.9\nTNEWS 94,439 165,847 56.9\nCSL 836,976 1,739,954 48.1\nCHID 958,893 1,763,507 53.4\nAvg. - - 51.5\nDatasets English phrases English total tokens phrase rate (%)\nMNLI 43,661 318,985 13.7\nQNLI 59,506 395,681 15.0\nQNLI 35,256 237,731 14.8\nSST-2 9,757 103,048 9.47\nCoLA 10,491 82,353 12.7\nAvg. - - 13.1\nis necessary. For simplicity, we did not use the best\nvalue of coefﬁcient for each task and instead we\nadopt 0.0 for RACE and 1.0 for the other tasks.\nF Case study\nWe also qualitatively study the results of BERT and\nAMBERT, and ﬁnd that they support our claims\n(cf., Section 1) very well. Here, we give some ran-\ndom examples from the entailment tasks (QNLI\nand CMNLI) in Table 12. One can have the follow-\ning observations. 1) The ﬁne-grained models (e.g.,\nOur BERT word) cannot effectively use complete\nlexical units such as “Doctor Who” and “ 打死”\n(sentence pairs 1 and 5), which may result in in-\ncorrect predictions. 2) The coarse-grained models\n(e.g., Our BERT phrase), on the other hand, cannot\neffectively deal with incorrect tokenizations, for\nexample, “the blind” and “格式” (sentence pairs\n2 and 6). 3) AMBERT is able to make effective\nuse of complete lexical units such as “sister station”\nin sentence pair 4 and “ 员工/ 工人” in sentence\npair 7, and robust to incorrect tokenizations, such\nas “used to” in sentence pair 3. 4) AMBERT can in\ngeneral make more accurate decisions on difﬁcult\nsentence pairs with both ﬁne-grained and coarse-\ngrained tokenization results.\n435\nTable 11: Hyper-parameters for ﬁne-tuning of both Chinese and English tasks.\nDataset Modes Batch Size Max Length Epoch Learning Rate λ\nTNEWS/IFLYTEK/AFQMC/CSL/CMNLI Our BERT, AMBERT-Hybrid 32 128 5 2e-5 -\nAMBERT, AMBERT-Combo 32 128 8 2e-5 1.0\nCLUEWSC2020 Our BERT, AMBERT-Hybrid 8 128 50 2e-5 -\nAMBERT, AMBERT-Combo 8 128 80 2e-5 1.0\nCMRC2018 All the models 32 512 2 2e-5 -\nChID Our BERT, AMBERT-Hybrid 24 64 3 2e-5 -\nAMBERT, AMBERT-Combo 24 64 3 2e-5 1.0\nC3 Our BERT, AMBERT-Hybrid 24 512 8 2e-5 -\nAMBERT, AMBERT-Combo 24 512 8 2e-5 1.0\nSST-2/MRPC/QQP/MNLI/QNLI Our BERT, AMBERT-Hybrid 32 512 4 2e-5 -\nAMBERT, AMBERT-Combo 32 512 6 2e-5 1.0\nCoLA/STS-B Our BERT, AMBERT-Hybrid 32 512 10 2e-5 -\nAMBERT, AMBERT-Combo 32 512 20 2e-5 1.0\nRTE Our BERT, AMBERT-Hybrid 32 512 20 2e-5 -\nAMBERT, AMBERT-Combo 32 512 50 2e-5 1.0\nSQuAD (1.1 and 2.0) All the models 32 512 3 2e-5 -\nRACE All except the following two 16 512 4 1e-5 -\nAMBERT, AMBERT-Combo 32 512 6 1e-5 0.0\nTable 12: Case study for sentence matching tasks in both English and Chinese (QNLI and CMNLI). The value “0”\ndenotes entailment relation, while the value “1” denotes no entailment relation. WORD/PHRASE represents Our\nBERT word/phrase. In English the tokens in the same phrase are concatenated with “ ”, and in Chinese phrases\nare split with “/”.\nSentence1 Sentence2 Label WORD PHRASEAMBERT\nWhat Star Trek episode has a nod to Doctor Who?(What StarTrek episode hasa nod to DoctorWho?)\nThere have also been many references to Doctor Who in popular culture andother science ﬁction, including Star Trek: The Next Generation (”The NeutralZone”) and Leverage.(There have also been many referencesto DoctorWho in popularcultureandother scienceﬁction, including StarTrek: thenextgeneration (”the neu-tralzone”) and leverage.)\n0 1 0 0\nWhat was the name of the blind date concept program debuted byABC in 1966?(What wasthe nameof theblind date concept program debutedby ABC in1966?)\nIn December of that year, the ABC television network premiered The DatingGame, a pioneer series in its genre, which was a reworking of the blind dateconcept in which a suitor selected one of three contestants sight unseen basedon the answers to selected questions.(InDecember of thatyear, theABC televisionnetwork premiered the datinggame, a pioneer series inits genre, which wasa reworking ofthe blinddateconcept inwhich a suitor selected oneofthree contestants sight unseenbasedonthe answers to selected questions.)\n0 0 1 0\nWhat are two basic primary resources used to guage complexity?(What are two basic primary resources usedto guage complexity?)\nThe theory formalizes this intuition, by introducing mathematical models ofcomputation to study these problems and quantifying the amount of resourcesneeded to solve them, such as time and storage.(Thetheory formalizes this intuition, by introducing mathematical models ofcomputation to study these problems and quantifying theamountof resourcesneeded tosolve them, suchas time and storage.)\n0 1 1 0\nWhat is the frequency of the radio station WBT in North Carolina?(Whatis the frequencyof the radiostation WBTinnorthcarolina?)\nWBT will also simulcast the game on its sister station WBTFM (99.3 FM),which is based in Chester, South Carolina.(WBT will also simulcast thegame onits sisterstation WBTFM (99.3 FM),whichis basedin Chester, SouthCarolina.)\n1 0 0 1\n只打那些面对我们的人，乔恩告诉阿德林。\n(只/打/那些/面对/我们/的/人/，/乔恩/告诉/阿/德/林/。)\n“打死那些面对我们的人，”阿德林对乔恩说。\n(“/打死/那些/面对/我们/的/人/，/”/阿/德/林/对/乔恩/说/。。) 1 0 1 1\n教堂有一个更精致的巴洛克讲坛。\n(教堂/有/一个/更/精致/的/巴洛克/讲坛/。)\n教堂有一个巴罗格式的讲坛。\n(教堂/有/一个/巴/罗/格式/的/讲坛/。) 0 0 1 0\n我们已经采取了一系列措施来增强我们员工的能力，并对他们进行投资。\n(我们/已经/采取/了/一/系列/措施/来/增强/我们/员工/的/能力/，/并/对/他们/进行/投资/。)\n我们一定会投资在我们的工人身上。\n(我们/一定/会/投资/在/我们/的/工人/身上/。) 0 1 1 0\n科技行业的故事之所以活跃起来，是因为现实太平淡了。\n(科技/行业/的/故事/之所以/活跃/起来/，/是/因为/现实/太平/淡/了/。)\n现实是如此平淡，以致于虚拟现实技术业务得到了刺激。\n(现实/是/如此/平淡/，/以致/于/虚拟/现实/技术/业务/得到/了/刺激/。) 1 0 0 1",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8893324732780457
    },
    {
      "name": "Lexical analysis",
      "score": 0.8431128263473511
    },
    {
      "name": "Natural language processing",
      "score": 0.7522150874137878
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6496326923370361
    },
    {
      "name": "Encoder",
      "score": 0.6048867702484131
    },
    {
      "name": "Sequence (biology)",
      "score": 0.5829126238822937
    },
    {
      "name": "Language model",
      "score": 0.5489330291748047
    },
    {
      "name": "Word (group theory)",
      "score": 0.5482568740844727
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5070755481719971
    },
    {
      "name": "Inference",
      "score": 0.43823927640914917
    },
    {
      "name": "Speech recognition",
      "score": 0.3754814863204956
    },
    {
      "name": "Linguistics",
      "score": 0.1891324818134308
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 45
}