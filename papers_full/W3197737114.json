{
  "title": "You should evaluate your language model on marginal likelihood over tokenisations",
  "url": "https://openalex.org/W3197737114",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2787318651",
      "name": "Kris Cao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A182221175",
      "name": "Laura Rimell",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3093517588",
    "https://openalex.org/W2952125979",
    "https://openalex.org/W2162401759",
    "https://openalex.org/W2963088785",
    "https://openalex.org/W2963275229",
    "https://openalex.org/W3098265177",
    "https://openalex.org/W4288416038",
    "https://openalex.org/W3120157709",
    "https://openalex.org/W2531207078",
    "https://openalex.org/W3158607076",
    "https://openalex.org/W1607198972",
    "https://openalex.org/W3035110948",
    "https://openalex.org/W46679369",
    "https://openalex.org/W1894221047",
    "https://openalex.org/W2970748231",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W4213221013",
    "https://openalex.org/W2970279348",
    "https://openalex.org/W3215822304",
    "https://openalex.org/W2804845563",
    "https://openalex.org/W2953092638",
    "https://openalex.org/W2963831883",
    "https://openalex.org/W1868901511",
    "https://openalex.org/W3126553126",
    "https://openalex.org/W2162258831",
    "https://openalex.org/W2417763662",
    "https://openalex.org/W1779483307",
    "https://openalex.org/W3120929527",
    "https://openalex.org/W2131924950",
    "https://openalex.org/W2121879602",
    "https://openalex.org/W2964110616",
    "https://openalex.org/W2963979492",
    "https://openalex.org/W3035207248",
    "https://openalex.org/W2733115588",
    "https://openalex.org/W2963247666",
    "https://openalex.org/W2890225082",
    "https://openalex.org/W3135427360",
    "https://openalex.org/W3101140821",
    "https://openalex.org/W2103149536",
    "https://openalex.org/W2964046515",
    "https://openalex.org/W3034789084",
    "https://openalex.org/W2511962886",
    "https://openalex.org/W3046493860",
    "https://openalex.org/W309335912",
    "https://openalex.org/W2119174600",
    "https://openalex.org/W3169483174",
    "https://openalex.org/W3140968660"
  ],
  "abstract": "Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 2104–2114\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n2104\nYou should evaluate your language model on marginal likelihood over\ntokenisations\nKris Cao and Laura Rimell\nDeepMind, London, UK\n{kriscao, laurarimell}@deepmind.com\nAbstract\nNeural language models typically tokenise in-\nput text into sub-word units to achieve an open\nvocabulary. The standard approach is to use a\nsingle canonical tokenisation at both train and\ntest time. We suggest that this approach is un-\nsatisfactory and may bottleneck our evaluation\nof language model performance. Using only\nthe one-best tokenisation ignores tokeniser un-\ncertainty over alternative tokenisations, which\nmay hurt model out-of-domain performance.\nIn this paper, we argue that instead, lan-\nguage models should be evaluated on their\nmarginal likelihood over tokenisations. We\ncompare different estimators for the marginal\nlikelihood based on sampling, and show that\nit is feasible to estimate the marginal likeli-\nhood with a manageable number of samples.\nWe then evaluate pretrained English and Ger-\nman language models on both the one-best-\ntokenisation and marginal perplexities, and\nshow that the marginal perplexity can be signif-\nicantly better than the one best, especially on\nout-of-domain data. We link this difference in\nperplexity to the tokeniser uncertainty as mea-\nsured by tokeniser entropy. We discuss some\nimplications of our results for language model\ntraining and evaluation, particularly with re-\ngard to tokenisation robustness.\n1 Introduction\nNeural end-to-end language models have largely\ndone away with traditional pipeline approaches to-\nwards building NLP systems. However, one compo-\nnent which stubbornly remains is the tokenisation\nstep, used right at the start of preprocessing. At the\ntime of writing, the most widely used tokenisers,\nsuch as BPE (Sennrich et al., 2016) and unigram\n(Kudo, 2018), break up the input text into subword\nunits, potentially backing off to character-level seg-\nmentation if necessary. This allows for coverage\nof every possible input sequence; on the downside,\na single input sequence may now have multiple\npossible tokenisations.\nTypically, language models are trained and eval-\nuated using a single canonical tokenisation out of\nthe multitude of possible ones, but this tokenisation\nmay be suboptimal (Bostrom and Durrett, 2020) for\nmany reasons. For example, different tokenisations\n– that is, different surface segmentations – can re-\nveal different morphological analyses of the word\nin question (think un-ion-izeable vs. union-izable),\nand committing to a particular analysis can discard\nuseful information, particularly if the best analysis\nfrom the tokeniser is erroneous (Dyer, 2010).\nFurther, tokenisers themselves are trained us-\ning an objective which optimises the likelihood\nof the data. This can be explicit (the unigram to-\nkeniser of Kudo (2018) optimises a unigram lan-\nguage modelling objective) or implicit (BPE aims\nto minimise the description length of the training\ndata, which has close connections to probabilistic\nmethods; MacKay 2003). In this sense they are\nalso language models, albeit far less powerful than\nthe neural language models we train on their out-\nputs. This raises a difﬁcult question: to what extent\nare our large language models bottlenecked by the\ntokenisers that we use to train them?\nWe argue that rather than evaluating language\nmodels using the one-best tokenisation from the\ntokeniser, one should evaluate language models\nusing the marginal likelihood over all possible to-\nkenisations of an input. This divorces language\nmodel performance from the performance of the\ntokenisation model, and we believe this gives a bet-\nter indicator of the intrinsic quality of the language\nmodel.\nIn this paper, we take a language model pre-\ntrained using a single tokenisation, and estimate\nthe marginal likelihood of the model on test data,\ntaking multiple tokenisations of each input into\naccount. While summing exactly over exponen-\ntially many tokenisations is intractable, we can\nestimate the marginal likelihood using importance\nsampling. One contribution of this paper is to show-\n2105\ncase low-variance estimators of the marginal likeli-\nhood based on sampling without replacement. We\ncast the tokeniser as the proposal distribution for\nour importance sampling estimator, which clearly\ndelimits the role of the tokeniser. Indeed, as the\nnumber of samples we consider increases, the lan-\nguage model becomes less and less coupled to the\ntokeniser, and our evaluation becomes more intrin-\nsic to the language model itself, rather than the\nlanguage model + tokeniser combination.\nWe demonstrate that there can be a signiﬁcant\ndifference – which we call the marginal gap – in\nmarginal likelihood compared to one-best tokenisa-\ntion likelihood, especially on out-of-domain evalua-\ntion sets. This suggests that the tokeniser is failing\nto generalise well to out-of-domain data, and is\ntherefore a signiﬁcant bottleneck to the generalisa-\ntion capability of the language model. Thus, taking\nthe one-best tokenisation likelihood is a poor proxy\nfor the true language model performance.\nWe next show that there is a correlation between\nthe uncertainty of the tokeniser (as measured by\nthe entropy of the segmentation lattice) and the\nmarginal gap. We give an efﬁcient dynamic pro-\ngram to calculate the entropy of the segmentation\nlattice, and show that this entropy is predictive of\nhow poorly the tokeniser fails to generalise. This\nsuggests that measuring tokeniser entropy can be\na useful signal for adding additional samples to\nour estimate of the marginal likelihood. We also\nuse our sampled tokenisations to demonstrate that\nlanguage models are particularly sensitive to vari-\nations in tokenisation, a challenge that must be\nmitigated for marginal likelihood evaluation.\nFinally, we investigate how many samples are\nnecessary to obtain an accurate estimate of the\nmarginal likelihood. We show that many samples\nare necessary, but only relatively few samples con-\ntribute signiﬁcantly to this estimate. This shows\nthat the tokeniser distribution over tokenisations\ndiffers signiﬁcantly from the language model poste-\nrior distribution over tokenisations – indeed, taking\nonly the best tokenisation from the samples can\nrecover most of the performance increase obtained\nby marginalisation. This gives weight to our ﬁnd-\ning that tokenisers generalise poorly, and that the\none-best tokenisation can often be suboptimal.\nWe conclude by discussing some implications of\nour results, particularly for languages with richer\nmorphology than English. Finally, we sketch po-\ntential future directions to bridge this gap by using\nsampled tokenisations at training time, and how\nthis might improve language model robustness.\n2 Taking multiple tokenisations into\nconsideration\nWe denote by D (for document) a string of text\nwhose score we would like to calculate. Given a\nvocabulary V of sub-word tokens (which is usu-\nally induced by the tokeniser), we denote by Ti\npotential tokenisations of D – i.e. sequences of\ntokens t1t2 ...t ni such that each ti ∈V and the\nsequence detokenises to D. An autoregressive neu-\nral language model (with parameters θ) is a model\nwhich decomposes the probability of the full se-\nquence into a series of left-to-right predictions:\nPθ(T,D) = ∏n\ni=1 Pθ(ti|t<i). Crucially, neural\nlanguage models Pθ do not score Ddirectly, but\nrather token sequences Pθ(T,D). For any input\ndocument D, a tokeniser will deﬁne a canonical\ntokenisation T∗, and one usually approximates\nPθ(D) with Pθ(T∗,D).\nWe believe, on the other hand, that it is more\nprincipled to marginalise over all possible tokeni-\nsations; that is, calculate ∑\nT Pθ(T,D) directly.\nThere could be signiﬁcant tokeniser uncertainty\nover the correct tokenisation; we can view the un-\ncertainty as either caused by ambiguity in local\ncontext imposed by the strong independence as-\nsumptions made by tokenisers, or because of in-\nherent tokeniser uncertainty when confronted with\nout-of-domain input. In either case, incorporating\nadditional analyses in the form of extra tokenisa-\ntions can give the language model extra information\ncompared to the one-best tokenisation. We believe\nthat the marginal likelihood better represents the\ntrue capability of the language model, without the\nconstraint of the tokeniser.\nHowever, exactly calculating the marginal like-\nlihood is infeasible, as the number of possible to-\nkenisations is exponential in the length of the input\ntext. Whenever calculating a marginal exactly is\ninfeasible, the classical approach is to approximate\nit using samples. The best distribution to sample\nfrom would be the model posterior distribution over\ntokenisations given text, as this gives the lowest\nvariance estimator; unfortunately, we are unaware\nof any methods that would let us sample directly\nfrom this distribution. Therefore, to estimate the\nmarginal language model likelihood, we turn to\nimportance sampling. Given some proposal distri-\nbution Q(T|D) of possible tokenisations, we can\n2106\nuse the importance sampling estimator\nP(D) =\n∑\nT\nP(T,D) =ET∼Q\nP(T,D)\nQ(T|D) (1)\nNow, it remains to ﬁnd a suitable proposal dis-\ntribution Q(T|D). In this paper, we use the uni-\ngram tokeniser of Kudo (2018), as this is the only\nprobabilistic tokeniser that we are aware of. This\ntokeniser ﬁrst constructs a lattice of all possible\ntokenisations given an input and a lexicon of word\npieces. Distinct tokenisations of the input corre-\nspond to paths through this lattice, and the score\nof a tokenisation is the sum of the scores of the\ntokens along the path. As the score decomposes\nalong lattice segments, many interesting quanti-\nties, such as Q(D) (the marginal likelihood of an\ninput text under the tokeniser), are exactly calcu-\nlable. This allows not only for sampling from the\nlattice of possible tokenisations, but also calculat-\ning the score of a given tokenisation (i.e. estimate\nQ(T|D) = Q(T,D)/Q(D)), which is necessary\nto estimate the importance weight.\nTokenising consistently There is prior evidence\n(Lazaridou et al., 2021) to suggest that Transformer\nlanguage models are able to effectively leverage\nmemory, and that perplexities of repeated words in\na document can be much lower than the perplexity\nof the ﬁrst occurrence of that word. We show in\nSection 4.3 that this copying ability is tied to the\nexact tokenisation of that word: if a word reoccurs\nin a document with a different tokenisation, its\nperplexity is much higher than if it reappears with\nthe same tokenisation.\nArmed with this insight, we design an alterna-\ntive proposal distribution which samples a single\ntokenisation for each unique whitespace-delimited\ntype in a document, and then shares that tokenisa-\ntion for each token of that type in the document.\nWe note that it is possible to adapt a pre-trained\nunigram tokeniser to do this, by passing in only the\nunique whitespace types in a document to the to-\nkeniser and reconstructing the document from the\nsampled tokenisations. This is possible because the\nunigram tokeniser does not consider context when\ntokenising, and whitespace tokens are tokenised\nindependently. We note that this two-stage word\ngeneration process, where ﬁrst we generate the vo-\ncabulary for a document, and then generate the\ndocument from that vocabulary, has close connec-\ntions to the two-stage language models proposed\nin Goldwater et al. (2011). The problem of tokenis-\ning consistently only arises when sampling from\nthe tokeniser; the one-best tokenisation of an input\nfrom the unigram tokeniser will always tokenise\neach occurrence of a type identically.\n2.1 Lowering the variance of the estimator\nA naive approach to estimating the marginal like-\nlihood using Equation 1 would be to sample nto-\nkenisations T1,...,T n at random from Q(T|D),\nscore the resulting tokenisations using the language\nmodel Pθ(Ti,D), and average the resulting impor-\ntance weighted scores. However, due to Jensen’s\ninequality, this is only a lower bound of the true\nmarginal likelihood. We can obtain a tighter bound\nwith the same number of samples by taking the\naverage in probability space rather than log space\n(as in Burda et al. (2016))\nlog Pθ(D) ≈log\n(\n1\nn\n∑\ni\nPθ(Ti,D)\nQ(Ti|D)\n)\n(2)\nChanging the sampling procedure Taking n\nindependent samples from Q can result in high-\nvariance estimates if the entropy of Qis low and it\nassigns low probability to tokenisations with high\nposterior probability under the language model Pθ.\nIn this case, one would expect to see multiple re-\npeated samples, which do not sufﬁciently explore\nthe sample space. One option to lower the variance\nof the estimate is to instead sample without replace-\nment (WOR). By enforcing that all samples are\ndistinct, we can explore the sample space better,\nHowever, sampling without replacement with-\nout exactly enumerating all possible sample out-\ncomes is tricky. Kool et al. (2019) show how to\nsample without replacement for sequence models\nusing stochastic beam search (SBS). Unfortunately,\nthe segmentation lattice used in the unigram to-\nkeniser is not locally normalised, and we cannot\nnaively use SBS. We therefore adapt the SBS al-\ngorithm by ﬁrst running the forward algorithm on\nthe segmentation lattice to calculate the normal-\nising constant at each point of the lattice; we can\nthen combine Viterbi backwardsn-best search with\nthe constrained Gumbel-max trick used in SBS to\nexactly sample ntokenisations WOR.\nIf we sample without replacement, the inclusion\nprobability of a tokenisation Ti is no longer equal\nto Q(Ti|D). Kool et al. (2019) show that, for the\nexpectation of a function f under a distribution\nQ, an unbiased estimator using a set of ksamples\n2107\nwithout replacement is given by\nET∼Qf(T) ≈\nk∑\ni=1\nQ(Ti)\nqκ(Ti)f(Ti) (3)\nκis the perturbed score of the k+ 1th item during\nsearch and qκ(T) = 1−exp(−exp(log Q(T)−κ))\nis the probability that a Gumbel variable with loca-\ntion log Q(T) takes a value greater than κ. In our\ncase, f(T) = Pθ(T,D)/Q(T), and if we calcu-\nlate this sum before taking the logarithm to obtain\na tighter bound, then the Q(T) terms cancel and\nwe obtain the following estimator for the marginal\nlikelihood of a document:\nlog Pθ(D) ≥log\n(∑\ni\nPθ(Ti,D)\nqκ(Ti)\n)\n(4)\nIncluding the best tokenisation To lower the\nvariance of the estimate further (at the cost of in-\ntroducing some bias), we can always include the\nbest tokenisation from the tokeniser in our set of\nsamples (Botev et al., 2017). This method decom-\nposes estimating ∑\nT Pθ(T,D) as Pθ(T∗,D) +∑\nT̸=T∗ Pθ(T,D). We can then estimate the sum\nover all tokenisations using exactly the same meth-\nods as before, using the new distribution Q∗which\nplaces 0 mass on T∗and renormalises the result-\ning probabilities for other tokenisations. It remains\nto simulate samples from Q∗using samples from\nQ. We note that for sampling with replacement,\na simple technique to sample from Q∗is simple\nrejection sampling, where we discard any sample\nfrom Qthat equals T∗. However, if Q(T) is partic-\nularly peaked around T∗, then this procedure may\nrequire many rejection steps. Therefore, we do not\ninvestigate this estimator further.\nWhen sampling without replacement, we have\nto be a little more careful. We note that the fol-\nlowing scheme samples k times exactly without\nreplacement from Q∗:\n1. Take k+ 1items T1,...,T k+1 WOR from Q.\n2. If any Ti = T∗, discard it from the sample.\n3. Otherwise discard Tk+1\nWe also note (by conditioning on the event that\nT∗appears in the sample) that the inclusion proba-\nbilities are easily calculated (if T∗appears in the\nsample, take κ to be the perturbed score of the\nk+ 2th item; otherwise take it to be the perturbed\nscore of the k+ 1th item).\nAlgorithm 1:Recursive algorithm for lat-\ntice entropy\nResult: entropy Hn of segmentation lattice\ninit H0 = 0, α[i] the forward marginals ;\nfor i= 1to ndo\nfor wtoken terminating at position ido\nj = start position of w;\n// ϕ(w) is the score of token w ;\np(w) = exp(α[j] +ϕ(w) −α[i]) ;\nHi += p(w)(Hj + logp(w))\nend\nend\nreturn Hn\n2.2 Summing over then-best tokenisations\nAn alternative approach to estimating∑\nT Pθ(T,D) is to restrict the sum to a smaller set\nof suitable candidates. As the unigram tokenisation\nobjective decomposes over segments, one can use\nViterbi search to ﬁnd exactly the nhighest scoring\ntokenisations from the tokeniser. We then score\neach tokenisation using the language model, and\nsum the contribution of each estimate to obtain a\n(lower bound) estimate of the marginal likelihood.\nThis estimator is high-bias and low-variance\ncompared to the sampling-based estimators; we\nshow in Section 4.1 that, although the n-best\nestimator performs well, it is possible to tune\nthe sample-based estimators to perform better by\ntrading bias for variance.\n3 Measuring segmentation lattice\nentropy\nWe believe that the entropy of the tokeniser seg-\nmentation lattice is an important quantity to mea-\nsure. The entropy quantiﬁes the uncertainty of the\ntokeniser, and has a nice interpretation as the (log-\narithm of the) size of the set of alternatives the\ntokeniser is choosing uniformly over. While the\nentropy over hidden states of other structured mod-\nels like HMMs and CRFs have previously been\npublished (Hernando et al., 2005; Mann and Mc-\nCallum, 2007; Ilic, 2011), and a uniform treatment\nin terms of expectation semirings is given in Li and\nEisner (2009), we are unaware of previous elemen-\ntary derivations of the entropy of a segmentation\nlattice. We give the algorithm in Algorithm 1.\nNote that the recursion has a particularly nice\ninterpretation in terms of information theory. Re-\ncall that the entropy of a random variable can be\n2108\nthought of as the necessary number of bits to trans-\nmit the random variable. The recursion states that,\nto transmit the lattice up to position i(which takes\nHi bits), we can transmit a preﬁx of the lattice (us-\ning Hj bits), and then transmit the token w that\ngoes from j to i (using log P(w) bits). The to-\ntal number of bits necessary is then the weighted\nsum of all possible ways of doing this, where the\nweights are given by the probability of that particu-\nlar decomposition.\n4 Experiments\nFor our experiments, we ﬁrst pretrain language\nmodels using one-best tokenisations from a to-\nkeniser using WMT news shared task data (Bar-\nrault et al., 2020). We train models on both English\nand German data up to September 2017, reserv-\ning the rest of the 2017 data for validation and\nmodel selection. We use a Transformer-XL (Dai\net al., 2019) model with 18 layers and a hidden size\nof 1024. During evaluation time, we do not use\nTransformer-XL memory, due to the interaction\nof batching and sampled tokenisation. While this\nmay depress our results, we are not interested in\nabsolute model performance per se, but rather in\nthe relative performance of the marginal likelihood\nvs. the one-best likelihood.\nThe tokeniser we use at both training and evalu-\nation time is a unigram tokeniser as implemented\nin the SentencePiece package (Kudo, 2018), with\na vocabulary size of 50529. We train the tokeniser\non the same training set, with a random sample of\n100 million sentences for English, and 10 million\ndocuments for German.\n4.1 Measuring the marginal likelihood\nFor both English and German, we use 500 docu-\nments sampled randomly from the WMT train and\ntest data and 500 randomly sampled Wikipedia doc-\numents (WIKI ). For English, we also use 500 docu-\nments from the CUSTOM NEWS and arXiv abstracts\n(ARXIV) datasets of Lazaridou et al. (2021), and\nfor German, we additionally use 200 documents\nfrom the MC4 dataset in Xue et al. (2020).\nFor each method outlined in Section 2, we sam-\nple 128 different tokenisations of each document,\nand calculate Pθ(Ti,D) for each sample, before\naggregating the sample scores into an estimate of\nthe marginal likelihood. We parallelise evaluating\nall the samples for a document on a multi-host TPU\nsetup; each dataset takes 15-30 minutes to evaluate.\nFigure 1: The effect of temperature scaling on the es-\ntimated perplexity on all English datasets, using WOR\n1-best. The y-axis is the percentage difference in per-\nplexity relative to the n-best baseline (lower is better).\nNote the x-axis is scaled as 1/τ, rather than τ.\nFurther, to ensure results are comparable across\ndifferent tokenisations with potentially different\nnumbers of tokens, we calculate perplexity by di-\nviding the total likelihood across all documents by\nthe total number of whitespace-delimited tokens.\nWe present our results in Table 1.\nOur results show that there can be a signiﬁcant\ndifference between the one-best tokenisation like-\nlihood and the marginal likelihood, particularly as\none moves further away from the training data do-\nmain. Indeed, the relative perplexity improvement\nreaches up to 1.9% on EN-ARXIV, and 0.9% on\nDE-MC4. Further, tokenising words consistently\nin a document has a large impact on the marginal\nlikelihood estimation. We investigate this effect\nfurther in Section 4.3. While the n-best estimator\nappears to perform the best in this comparison, we\nshow in the next section that by tuning the sam-\npling temperature of the WOR 1-best estimator, it\nis possible to obtain even better estimates of the\nmarginal likelihood.\nThe effect of sampling temperature We also\ninvestigate sharpening the tokeniser distribution be-\nfore sampling by multiplying the log-probability\nof each tokenisation by a factor of 1/τ before sam-\npling. Using τ <1 has often shown to give im-\nproved results in various tasks (Kool et al., 2019;\nMelis et al., 2019; Adlam et al., 2020), and can be\nunderstood as a way of tuning the bias-variance\ntradeoff with the n-best estimator at the high-bias,\nlow variance end, and independently sampling at\nthe other. We compare the WOR with 1-best es-\ntimator at a various rate of temperatures on our\nEnglish datasets, and show the results in Figure\n2109\nConsistent tokenization Inconsistent tokenization\nWR WOR WOR 1-best n-best WR WOR WOR 1-best n-best\nEnglish\nWMT train (16.49) 16.59 16.58 16.48 16.47 16.81 16.79 16.48 16.48\nWMT test (22.62) 22.73 22.72 22.59 22.56 23.07 23.01 22.60 22.58\nCUSTOMNEWS(37.09) 37.11 37.12 36.93 36.88 37.90 37.89 37.03 36.95\nWIKI (60.22) 61.09 61.02 59.82 59.71 63.37 63.33 60.06 59.92\nARXIV (179.20) 176.38 176.11 175.87 175.98 179.76 179.74 177.52 176.90\nGerman\nWMT train (31.84) 32.51 32.58 31.80 31.77 33.04 33.12 31.80 31.78\nWMT test (37.16) 37.68 38.16 37.12 37.08 38.87 38.91 37.13 37.09\nWIKI (66.08) 69.44 69.30 65.86 65.63 72.37 72.41 66.01 65.78\nMC4 (194.02) 206.89 207.15 192.84 192.21 219.63 219.19 193.68 192.87\nTable 1: Comparing the different estimators of model marginal perplexity on evaluation sets. The number in\nbrackets represents the one-best tokenisation perplexity. Consistent vs. inconsistent tokenisation refers to whether\nwe tokenise each appearance of a whitespace-delimited type consistently in a document or not.\n1. One can see that it is possible to improve on\nthe n-best estimator by trading some bias for vari-\nance, and this can result in a better estimate of the\nmarginal, especially for out of domain datasets.\n4.2 Tokeniser entropy and the marginal gap\nNext, we investigate what causes the gap between\nmarginal likelihood and one-best likelihood, and\nwhether there are easily measurable factors that\nmight predict this difference. We hypothesise that,\nthe more uncertain the tokeniser is, the bigger this\ngap becomes. We pool together the documents in\nall our evaluation sets, and test whether there is a\ncorrelation between tokeniser entropy and marginal\ngap. Our results, shown in Figure 2, demonstrate\nthat there is a correlation between entropy and the\nmarginal gap (Spearman r = 0.57 for English,\n0.49 for German); interestingly, it appears that high\ntokeniser entropy is predictive of a bigger marginal\ngap, but large marginal gaps are possible even if\nthe tokeniser has low entropy.\n4.3 Analysing the caching behaviour of\nlanguage models\nOur results show that tokenising word types con-\nsistently within a document leads to signiﬁcantly\ntighter estimates of the marginal likelihood com-\npared to independently tokenising input tokens. We\nanalyse this phenomenon in this section, by investi-\ngating the loss language models assign to repeated\ntokens in a document, conditioned on whether the\ntoken appears in the same tokenised form or not.\nConcretely, let w1,...,w m be the whitespace-\ndelimited words in a document D, and let\nT1,..., Tn be the sampled tokenisations of the doc-\nument. Each word wi appears as a token sequence\nAll words Multi-token words\nFirst (1) (2) First (1) (2)\nWMT Tr 3.88 2.59 17.01 10.73 4.07 21.11\nWMT Te 4.19 2.59 16.69 12.15 4.11 20.40\nCNEWS 6.31 2.99 16.19 17.01 4.88 20.36\nWIKI 7.84 3.62 16.54 17.80 5.63 19.81\nARXIV 9.94 3.97 14.93 17.56 5.41 18.03\nTable 2: Investigating the caching ability of language\nmodels. For words which appear multiple times with\ndifferent tokenisations, we show the average loss of\nthe ﬁrst occurrence of that word, of subsequent occur-\nrences of that word with the same tokenisation (1), and\nsubsequent occurrences of that word in a different to-\nkenisation (2). WMT Tr and WMT Te are the WMT\ntraining and test evaluation sets respectively.\nTwi = ⟨t1\nwi ...t ni\nwi ⟩, and each sampled tokenisa-\ntion Ti can have different token sequences Ti\nwi for\nthe same underlying word. We look for words\nwk ∈(wi,...,w n) such that:\n1. For some tokenisation Ti of wi, for some l<\nk, wl = wk and Ti\nwk = Ti\nwl (the word has\nappeared before with the same tokenisation).\n2. For some other tokenisation Tj, for all l<k\nsuch that wl = wk, Tj\nwk ̸= Tj\nwl (all previous\noccurrences of this word in the document were\ntokenised differently).\nWe then calculate Pθ(wk|w<k) for each tokeni-\nsation Ti (by summing the scores of the tokens\nin wk), and microaverage separately the loss for\ntokenisations which fulﬁll condition (1) and condi-\ntion (2). The microaveraged loss for (1) represents\nthe language model being able to copy the word as\na sequence of tokens from its memory, while the mi-\ncroaveraged loss for (2) represents the model hav-\ning to generate the word afresh as a new sequence\n2110\n(a) English\n(b) German\nFigure 2: The correlation between entropy per token\nand the marginal gap per token in nats (not in per-\nplexity), categorised by evaluation dataset. Some data\npoints which extend beyond the right of the graph are\ntrucated; they follow the same trend.\nof tokens. By comparing the loss of words paired\nin this way, we can control for extra confounding\nfactors (such as token unigram probability), and\nisolate the ability of the language model to recog-\nnise whether different token sequences correspond\nto the same underlying form.\nWe show our results for our various datasets,\ntogether with selected subsets of words, in Table 2.\nWe see that, if the language model sees a word after\nalready seeing it in the same tokenisation, its loss\nis signiﬁcantly lower than the loss associated with\nthe ﬁrst time the word is seen (as was also reported\nin Lazaridou et al. (2021)). However, this ability is\nstrongly tied to the exact tokenisation of the word:\nif it appears again, but in a different tokenisation,\nthen its loss can in fact be even greater.\n4.4 How many samples are necessary?\nNext, we investigate how many samples are neces-\nsary to obtain an accurate estimate of the marginal\nlikelihood. We experiment on the EN-ARXIV\nFigure 3: The performance of the n-best marginal like-\nlihood estimator on the ARXIV evaluation set as we\nvary the number of samples, taken in order of Q(T|D)\nin orange and Pθ(T,D) in blue.\ndataset, as this showed the biggest relative improve-\nment between the marginal likelihood and the one-\nbest likelihood. We take the samples from our\nn-best estimator with n= 128, and incrementally\nsum the samples (which are given in decreasing\norder of likelihood under the tokeniser) to simulate\nhaving smaller n. As an oracle experiment to to see\nhow many samples contribute signiﬁcantly to the\nmarginal likelihood, we also order the samples by\ntheir language model scores (i.e. we order accord-\ning to Pθ(T,D) rather than Q(T|D)) before taking\nthe incremental sum. We show the results in Figure\n3. Our results show that, although ostensibly many\nsamples are necessary to estimate the marginal like-\nlihood accurately, only very few samples (in the\norder of 5) actually contribute signiﬁcantly.\nIn practical terms, our results suggest that one\nneeds to take many samples with current tokenis-\ners to accurately estimate the marginal likelihood,\nbut that many of these samples are not effective.\nWe therefore believe that a prerequisite for more\nwidespread adoption of marginal likelihood as an\nevaluation metric is tokenisers that better ﬁt the lan-\nguage model posterior over tokenisations. Current\ntokenisers make very strong independence assump-\ntions to make learning and inference tractable, and\nwe believe there is signiﬁcant scope to design to-\nkenisers which relax these assumptions.\n5 Related Work\n5.1 Tokenisation and segmentation\nUnsupervised word segmentation has a long and\nillustrious history. The earliest motivations were in\ninformation retrieval, and the motivation was that\ncollapsing a set of related query terms might help\n2111\nsmooth counts over each of those terms individu-\nally and result in better retrieval results. The earli-\nest approaches, such as the Porter stemmer (Porter,\n1997), were rule-based. However, the power of\ndata-driven statistical methods quickly became ap-\nparent, and tools such as Morfessor (Virpioja et al.,\n2013) used likelihood-based objectives, typically\nwith Bayesian smoothing methods (see also Gold-\nwater et al. (2011)), to induce segmentations.\nSennrich et al. (2016) used a different algorithm\nto induce segmentations: byte-pair encoding (Gage,\n1994). Originally designed as a data compression\nalgorithm, BPE tokenisers are now some of the\npredominantly used tokenisation methods. Alterna-\ntive approaches, such as WordPiece (Schuster and\nNakajima, 2012) and SentencePiece (Kudo, 2018),\nexplicitly use a language modelling objective to in-\nduce a token lexicon. Previous methods have used\ntrain-time tokenisation randomisation as a regulari-\nsation aid (Kudo, 2018; Provilkov et al., 2020), but\nstill use the one-best tokenisation at test time.\nAnother strand of work has investigated whether\ntokenisers that caputre linguistic morphology can\nimprove language models. Bostrom and Durrett\n(2020) showed that unigram and BPE tokenisers\nfor English and Japanese have low recall on re-\ncovering linguistic segments, since many morpho-\nlogically complex words are treated as a single\ntoken. Linguistically aligned tokenisers have been\nshown to result in better language model perplex-\nity (Schwartz et al., 2020; Park et al., 2021) and\nbetter downstream task performance (Alkaoud and\nSyed, 2020), especially for morphologically rich\nlanguages. These experiments also use one-best\ntokenisation at test time.\nRather than considering one-best or stochastic\nsamples of tokenisations, one can use entire seg-\nmentation lattices as input to a model. This ap-\nproach has been considered for morphological tag-\nging (Seker and Tsarfaty, 2020), parsing (Goldberg\nand Tsarfaty, 2008), and spoken intent recognition\n(Ladhak et al., 2016), among others.\n5.2 Tokenisation-free approaches\nAn alternative approach to inducing a tokenisation\nis to decompose input sequences into well-deﬁned\northographic units, such as characters. These ap-\nproaches circumvent the problem of inducing a\nlexicon, and have been used for text classiﬁca-\ntion (Conneau et al., 2017), language modelling\n(Al-Rfou et al., 2019), machine translation (Lee\net al., 2017), and word representation (Cao and Rei,\n2016). One downside is that dependency lengths\nbecome longer on the character-level, and lexical\ninformation has to be memorised by the compo-\nsitional machinery of the model. For this reason,\ntraditionally fully character-based approaches did\nnot perform as well as their token-level counter-\nparts, although recent progress suggests this may\nchange soon (Choe et al., 2019; Clark et al., 2021).\nThere also exist approaches which mix character-\nlevel and segment-level approaches (Buckman and\nNeubig, 2018; Kawakami et al., 2019; He et al.,\n2020), although these segmental language models\nrequire more complex inference procedures.\n6 Conclusion\nIn this paper, we argue for using model marginal\nlikelihood over tokenisations as an evaluation met-\nric for language models, rather than one-best to-\nkenisation likelihood. We introduce practical low-\nvariance estimators for measuring the marginal like-\nlihood, and demonstrate that there can be signiﬁ-\ncant difference between the marginal and the one-\nbest likelihoods, particularly on strongly out-of-\ndomain evaluation sets. Evaluating with marginal\nlikelihood thus goes some way toward loosening\nthe bottleneck imposed by tokeniser quality in the\ncurrently dominant language modelling paradigm,\nand our results suggest that the ﬁeld may be under-\nestimating the generalisation capability of mod-\nern language models. We further demonstrate\nthat tokeniser entropy is a good predictor of this\n“marginal gap”, suggesting that tokeniser entropy,\nespecially when out-of-domain, can be a guide to\nthe number of samples needed for evaluation.\nMore broadly, our experiments suggest that the\nﬁeld should continue seeking better ways to in-\ncorporate tokenisation into end-to-end language\nmodelling. Sampling from the tokeniser during\ntraining is an obvious possibility; alternatively, one\ncould incorporate the segmentation lattice into the\nmodel directly, which has been beneﬁcial for pars-\ning morphologically rich languages (Goldberg and\nTsarfaty, 2008; Tsarfaty et al., 2020). Further, de-\nveloping more contextual tokenisers which make\nfewer independence assumptions can also result in\nboth better language models trained on their one-\nbest tokenisation, and better evaluation estimates\nof the marginal likelihood with fewer samples.\nWe conduct experiments on German and En-\nglish corpora in this paper. However, these two\n2112\nlanguages are only a small sample in the full space\nof language typology. English is a morphologi-\ncally impoverished language, and while German\ncompounding and inﬂection offer some additional\nchallenges, many languages have more complex\npatterns of word formation and inﬂection. We be-\nlieve that estimating marginal likelihood will be\nimportant for morphologically richer languages,\nwhere tokenisation makes a bigger difference (Gerz\net al., 2018; Mielke et al., 2019).\nFinally, improved understanding of the interac-\ntion between tokenisation and language modelling\nhas implications for evaluating language models\non both downstream tasks and language generation\ntasks. Evidence has shown that gains in language\nmodelling, as measured in perplexity, often lead\nto improvements in downstream task performance\n(Radford et al., 2019). It would be instructive to\nextend our marginal likelihood approach to down-\nstream task evaluation. On generation tasks, since\nthe tokeniser affects language model training but\nis only implicitly used when sampling (via the to-\nkeniser vocabulary), the effect of tokenisation algo-\nrithms requires careful investigation.\nAcknowledgements\nThe authors would like to thank Dani Yogatama\nand the rest of the Language group at DeepMind\nfor comments and discussion, Gábor Melis and\nPhil Blunsom for comments on an earlier draft,\nand Mark Rowland for clariﬁcation remarks on\nsampling without replacement. We would also like\nto thank our anonymous reviewers.\nReferences\nBen Adlam, Jasper Snoek, and Samuel L. Smith. 2020.\nCold posteriors and aleatoric uncertainty.\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2019. Character-level lan-\nguage modeling with deeper self-attention. Proceed-\nings of the AAAI Conference on Artiﬁcial Intelli-\ngence, 33(01):3159–3166.\nMohamed Alkaoud and Mairaj Syed. 2020. On the im-\nportance of tokenization in Arabic embedding mod-\nels. In Proceedings of the Fifth Arabic Natural Lan-\nguage Processing Workshop (WANLP , pages 119–\n129.\nLoïc Barrault, Magdalena Biesialska, Ond ˇrej Bojar,\nMarta R. Costa-jussà, Christian Federmann, Yvette\nGraham, Roman Grundkiewicz, Barry Haddow,\nMatthias Huck, Eric Joanis, Tom Kocmi, Philipp\nKoehn, Chi-kiu Lo, Nikola Ljubeši ´c, Christof\nMonz, Makoto Morishita, Masaaki Nagata, Toshi-\naki Nakazawa, Santanu Pal, Matt Post, and Marcos\nZampieri. 2020. Findings of the 2020 conference on\nmachine translation (WMT20). In Proceedings of\nthe Fifth Conference on Machine Translation, pages\n1–55, Online. Association for Computational Lin-\nguistics.\nKaj Bostrom and Greg Durrett. 2020. Byte pair encod-\ning is suboptimal for language model pretraining. In\nFindings of the Association for Computational Lin-\nguistics: EMNLP 2020 , pages 4617–4624, Online.\nAssociation for Computational Linguistics.\nAleksandar Botev, Bowen Zheng, and David Barber.\n2017. Complementary Sum Sampling for Likeli-\nhood Approximation in Large Scale Classiﬁcation.\nIn Proceedings of the 20th International Conference\non Artiﬁcial Intelligence and Statistics, volume 54 of\nProceedings of Machine Learning Research , pages\n1030–1038, Fort Lauderdale, FL, USA. PMLR.\nJacob Buckman and Graham Neubig. 2018. Neural lat-\ntice language models. Transactions of the Associa-\ntion for Computational Linguistics.\nYuri Burda, Roger B. Grosse, and Ruslan Salakhutdi-\nnov. 2016. Importance weighted autoencoders. In\n4th International Conference on Learning Represen-\ntations, ICLR 2016, San Juan, Puerto Rico, May 2-4,\n2016, Conference Track Proceedings.\nKris Cao and Marek Rei. 2016. A joint model for word\nembedding and word morphology. In Proceedings\nof the 1st Workshop on Representation Learning for\nNLP, pages 18–26, Berlin, Germany. Association for\nComputational Linguistics.\nDokook Choe, Rami Al-Rfou, Mandy Guo, Heey-\noung Lee, and Noah Constant. 2019. Bridging\nthe gap for tokenizer-free language models. CoRR,\nabs/1908.10322.\nJonathan H. Clark, Dan Garrette, Iulia Turc, and John\nWieting. 2021. CANINE: pre-training an efﬁcient\ntokenization-free encoder for language representa-\ntion. CoRR, abs/2103.06874.\nAlexis Conneau, Holger Schwenk, Loïc Barrault, and\nYann Lecun. 2017. Very deep convolutional net-\nworks for text classiﬁcation. In Proceedings of the\n15th Conference of the European Chapter of the As-\nsociation for Computational Linguistics: Volume 1,\nLong Papers, pages 1107–1116, Valencia, Spain. As-\nsociation for Computational Linguistics.\nZihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-\nbonell, Quoc Le, and Ruslan Salakhutdinov. 2019.\nTransformer-XL: Attentive language models beyond\na ﬁxed-length context. In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 2978–2988, Florence, Italy.\nAssociation for Computational Linguistics.\n2113\nChristopher Dyer. 2010. A Formal Model of Ambiguity\nand its Applications in Machine Translation . Ph.D.\nthesis, University of Maryland.\nPhilip Gage. 1994. A new algorithm for data compres-\nsion. C Users J., 12(2):23–38.\nDaniela Gerz, Ivan Vuli ´c, Edoardo Maria Ponti, Roi\nReichart, and Anna Korhonen. 2018. On the relation\nbetween linguistic typology and (limitations of) mul-\ntilingual language modeling. In Proceedings of the\n2018 Conference on Empirical Methods in Natural\nLanguage Processing, pages 316–327, Brussels, Bel-\ngium. Association for Computational Linguistics.\nYoav Goldberg and Reut Tsarfaty. 2008. A single gen-\nerative model for joint morphological segmentation\nand syntactic parsing. In Proceedings of ACL-08:\nHLT, pages 371–379, Columbus, Ohio. Association\nfor Computational Linguistics.\nSharon Goldwater, Thomas L. Grifﬁths, and Mark\nJohnson. 2011. Producing power-law distributions\nand damping word frequencies with two-stage lan-\nguage models. Journal of Machine Learning Re-\nsearch, 12(68):2335–2382.\nXuanli He, Gholamreza Haffari, and Mohammad\nNorouzi. 2020. Dynamic programming encoding\nfor subword segmentation in neural machine trans-\nlation. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\npages 3042–3051, Online. Association for Computa-\ntional Linguistics.\nD. Hernando, V . Crespi, and G. Cybenko. 2005. Efﬁ-\ncient computation of the hidden markov model en-\ntropy for a given observation sequence. IEEE Trans-\nactions on Information Theory, 51(7):2681–2685.\nVelimir M. Ilic. 2011. Entropy semiring forward-\nbackward algorithm for HMM entropy computation.\nCoRR, abs/1108.0347.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2019. Learning to discover, ground and use words\nwith segmental neural language models. In Proceed-\nings of the 57th Annual Meeting of the Association\nfor Computational Linguistics , pages 6429–6441,\nFlorence, Italy. Association for Computational Lin-\nguistics.\nWouter Kool, Herke Van Hoof, and Max Welling. 2019.\nStochastic beams and where to ﬁnd them: The\nGumbel-top-k trick for sampling sequences with-\nout replacement. In Proceedings of the 36th In-\nternational Conference on Machine Learning , vol-\nume 97 of Proceedings of Machine Learning Re-\nsearch, pages 3499–3508. PMLR.\nTaku Kudo. 2018. Subword regularization: Improving\nneural network translation models with multiple sub-\nword candidates. In Proceedings of the 56th Annual\nMeeting of the Association for Computational Lin-\nguistics (Volume 1: Long Papers), pages 66–75, Mel-\nbourne, Australia. Association for Computational\nLinguistics.\nFaisal Ladhak, Ankur Gandhe, Markus Dreyer, Lam-\nbert Mathias, Ariya Rastrow, and Björn Hoffmeister.\n2016. Latticernn: Recurrent neural networks over\nlattices. In Interspeech 2016, pages 695–699.\nAngeliki Lazaridou, Adhiguna Kuncoro, Elena Gri-\nbovskaya, Devang Agrawal, Adam Liska, Tayfun\nTerzi, Mai Gimenez, Cyprien de Masson d’Autume,\nSebastian Ruder, Dani Yogatama, Kris Cao, Tomas\nKocisky, Susannah Young, and Phil Blunsom. 2021.\nPitfalls of static language modelling.\nJason Lee, Kyunghyun Cho, and Thomas Hofmann.\n2017. Fully character-level neural machine trans-\nlation without explicit segmentation. Transactions\nof the Association for Computational Linguistics ,\n5:365–378.\nZhifei Li and Jason Eisner. 2009. First- and second-\norder expectation semirings with applications to\nminimum-risk training on translation forests. In Pro-\nceedings of the 2009 Conference on Empirical Meth-\nods in Natural Language Processing , pages 40–51,\nSingapore. Association for Computational Linguis-\ntics.\nDavid J. C. MacKay. 2003. Information theory, infer-\nence, and learning algorithms.\nGideon Mann and Andrew McCallum. 2007. Efﬁcient\ncomputation of entropy gradient for semi-supervised\nconditional random ﬁelds. In Human Language\nTechnologies 2007: The Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics; Companion Volume, Short Pa-\npers, pages 109–112, Rochester, New York. Associ-\nation for Computational Linguistics.\nGábor Melis, Charles Blundell, Tomáš Ko ˇciský,\nKarl Moritz Hermann, Chris Dyer, and Phil Blun-\nsom. 2019. Pushing the bounds of dropout.\nSabrina J. Mielke, Ryan Cotterell, Kyle Gorman, Brian\nRoark, and Jason Eisner. 2019. What kind of lan-\nguage is hard to language-model? In Proceedings of\nthe 57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 4975–4989, Florence,\nItaly. Association for Computational Linguistics.\nHyunji Hayley Park, Katherine J. Zhang, Coleman Ha-\nley, Kenneth Steimel, Han Liu, and Lane Schwartz.\n2021. Morphology matters: A multilingual lan-\nguage modeling analysis. Transactions of the Asso-\nciation for Computational Linguistics, 9:261–276.\nM. F. Porter. 1997. An Algorithm for Sufﬁx Stripping ,\npage 313–316. Morgan Kaufmann Publishers Inc.,\nSan Francisco, CA, USA.\nIvan Provilkov, Dmitrii Emelianenko, and Elena V oita.\n2020. BPE-dropout: Simple and effective subword\nregularization. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 1882–1892, Online. Association for\nComputational Linguistics.\n2114\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nand Dario Amodeiand Ilya Sutskever. 2019. Lan-\nguage models are unsupervised multitask learners.\nOpenAI Technical Report.\nMike Schuster and Kaisuke Nakajima. 2012. Japanese\nand korean voice search. In 2012 IEEE Interna-\ntional Conference on Acoustics, Speech and Signal\nProcessing (ICASSP), pages 5149–5152.\nLane Schwartz, Francis Tyers, Lori Levin, Christo\nKirov, Patrick Littell, Chi-kiu Lo, Emily\nPrud’hommeaux, Hyunji Hayley Park, Kenneth\nSteimel, Rebecca Knowles, Jeffrey Micher, Lonny\nStrunk, Han Liu, Coleman Haley, Katherine J.\nZhang, Robbie Jimmerson, Vasilisa Andriyanets,\nAldrian Obaja Muis, Naoki Otani, Jong Hyuk\nPark, and Zhisong Zhang. 2020. Neural polysyn-\nthetic language modelling. Final Report of the\nNeural Polysynthetic Language Modelling Team\nat the 2019 Frederick Jelinek Memorial Summer\nWorkshop.\nAmit Seker and Reut Tsarfaty. 2020. A pointer net-\nwork architecture for joint morphological segmen-\ntation and tagging. In Findings of the Association\nfor Computational Linguistics: EMNLP 2020, pages\n4368–4378, Online. Association for Computational\nLinguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nReut Tsarfaty, Dan Bareket, Stav Klein, and Amit\nSeker. 2020. From SPMRL to NMRL: What did\nwe learn (and unlearn) in a decade of parsing\nmorphologically-rich languages (MRLs)? In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics , pages 7396–\n7408, Online. Association for Computational Lin-\nguistics.\nSami Virpioja, Peter Smit, Stig-Arne Grönroos, and\nMikko Kurimo. 2013. Morfessor 2.0: Python Im-\nplementation and Extensions for Morfessor Baseline.\nTechnical report.\nLinting Xue, Noah Constant, Adam Roberts, Mi-\nhir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya\nBarua, and Colin Raffel. 2020. mt5: A mas-\nsively multilingual pre-trained text-to-text trans-\nformer. CoRR, abs/2010.11934.",
  "topic": "Perplexity",
  "concepts": [
    {
      "name": "Perplexity",
      "score": 0.8772861957550049
    },
    {
      "name": "Language model",
      "score": 0.7505166530609131
    },
    {
      "name": "Computer science",
      "score": 0.7327982187271118
    },
    {
      "name": "Marginal likelihood",
      "score": 0.7063969373703003
    },
    {
      "name": "Estimator",
      "score": 0.5384315252304077
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5148033499717712
    },
    {
      "name": "Bottleneck",
      "score": 0.506065309047699
    },
    {
      "name": "Vocabulary",
      "score": 0.48067912459373474
    },
    {
      "name": "Robustness (evolution)",
      "score": 0.4376096725463867
    },
    {
      "name": "Marginal distribution",
      "score": 0.4256875514984131
    },
    {
      "name": "Marginal model",
      "score": 0.4129287004470825
    },
    {
      "name": "Machine learning",
      "score": 0.39627328515052795
    },
    {
      "name": "Natural language processing",
      "score": 0.3578236699104309
    },
    {
      "name": "Statistics",
      "score": 0.3363504409790039
    },
    {
      "name": "Mathematics",
      "score": 0.18988770246505737
    },
    {
      "name": "Bayesian probability",
      "score": 0.11674562096595764
    },
    {
      "name": "Linguistics",
      "score": 0.09299024939537048
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Embedded system",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Random variable",
      "score": 0.0
    },
    {
      "name": "Regression analysis",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 6
}