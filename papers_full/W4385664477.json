{
  "title": "ThoughtSource: A central hub for large language model reasoning data",
  "url": "https://openalex.org/W4385664477",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2227779229",
      "name": "Simon Ott",
      "affiliations": [
        "Medical University of Vienna",
        "Austrian Research Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4322804566",
      "name": "Konstantin Hebenstreit",
      "affiliations": [
        "Austrian Research Institute for Artificial Intelligence",
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A2914419376",
      "name": "Valentin Liévin",
      "affiliations": [
        "Technical University of Denmark"
      ]
    },
    {
      "id": "https://openalex.org/A2999106934",
      "name": "Christoffer Egeberg Hother",
      "affiliations": [
        "Copenhagen University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2317807276",
      "name": "Milad Moradi",
      "affiliations": [
        "Medical University of Vienna",
        "Austrian Research Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4322804570",
      "name": "Maximilian Mayrhauser",
      "affiliations": [
        "Austrian Research Institute for Artificial Intelligence",
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A4322804571",
      "name": "Robert Praas",
      "affiliations": [
        "Austrian Research Institute for Artificial Intelligence",
        "KTH Royal Institute of Technology",
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A2065379623",
      "name": "Ole Winther",
      "affiliations": [
        "Technical University of Denmark"
      ]
    },
    {
      "id": "https://openalex.org/A40952608",
      "name": "Matthias Samwald",
      "affiliations": [
        "Austrian Research Institute for Artificial Intelligence",
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A2227779229",
      "name": "Simon Ott",
      "affiliations": [
        "Austrian Research Institute for Artificial Intelligence",
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A4322804566",
      "name": "Konstantin Hebenstreit",
      "affiliations": [
        "Medical University of Vienna",
        "Austrian Research Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A2914419376",
      "name": "Valentin Liévin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2999106934",
      "name": "Christoffer Egeberg Hother",
      "affiliations": [
        "Copenhagen University Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2317807276",
      "name": "Milad Moradi",
      "affiliations": [
        "Austrian Research Institute for Artificial Intelligence",
        "Medical University of Vienna"
      ]
    },
    {
      "id": "https://openalex.org/A4322804570",
      "name": "Maximilian Mayrhauser",
      "affiliations": [
        "Medical University of Vienna",
        "Austrian Research Institute for Artificial Intelligence"
      ]
    },
    {
      "id": "https://openalex.org/A4322804571",
      "name": "Robert Praas",
      "affiliations": [
        "Medical University of Vienna",
        "Austrian Research Institute for Artificial Intelligence",
        "KTH Royal Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2065379623",
      "name": "Ole Winther",
      "affiliations": [
        "Technical University of Denmark"
      ]
    },
    {
      "id": "https://openalex.org/A40952608",
      "name": "Matthias Samwald",
      "affiliations": [
        "Medical University of Vienna",
        "Austrian Research Institute for Artificial Intelligence"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3205068155",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4221143046",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W4312091691",
    "https://openalex.org/W4281483047",
    "https://openalex.org/W4304195432",
    "https://openalex.org/W4304192721",
    "https://openalex.org/W4281567264",
    "https://openalex.org/W4310998149",
    "https://openalex.org/W4312091845",
    "https://openalex.org/W4313304293",
    "https://openalex.org/W4283773284",
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3118781290",
    "https://openalex.org/W3197876970",
    "https://openalex.org/W4287208470",
    "https://openalex.org/W2890894339",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W4361865652",
    "https://openalex.org/W2970482702",
    "https://openalex.org/W3083410900",
    "https://openalex.org/W2963995027",
    "https://openalex.org/W3159959439",
    "https://openalex.org/W3189216228",
    "https://openalex.org/W2962800603",
    "https://openalex.org/W3034643750",
    "https://openalex.org/W4286892945",
    "https://openalex.org/W2475046758",
    "https://openalex.org/W3170403598",
    "https://openalex.org/W3177813494",
    "https://openalex.org/W4286233477",
    "https://openalex.org/W4372279899",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4393601284",
    "https://openalex.org/W4393612416",
    "https://openalex.org/W4385664477",
    "https://openalex.org/W4392359953"
  ],
  "abstract": "Abstract Large language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a wide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, their reasoning processes are opaque, they are prone to ‘hallucinate’ facts, and there are concerns about their underlying biases. Letting models verbalize reasoning steps as natural language, a technique known as chain-of-thought prompting, has recently been proposed as a way to address some of these issues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) reasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating qualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first release of ThoughtSource integrates seven scientific/medical, three general-domain and five math word question answering datasets.",
  "full_text": "1Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdata\nthoughtSource: a central hub for \nlarge language model reasoning \ndata\nSimon Ott1,5, Konstantin Hebenstreit  1,5, Valentin Liévin2, Christoffer Egeberg Hother3, \nMilad Moradi1, Maximilian Mayrhauser1, Robert Praas1,4, Ole Winther2 & Matthias Samwald  1 ✉\nLarge language models (LLMs) such as GPT-4 have recently demonstrated impressive results across a \nwide range of tasks. LLMs are still limited, however, in that they frequently fail at complex reasoning, \ntheir reasoning processes are opaque, they are prone to ‘hallucinate’ facts, and there are concerns about \ntheir underlying biases. Letting models verbalize reasoning steps as natural language, a technique \nknown as chain-of-thought prompting, has recently been proposed as a way to address some of these \nissues. Here we present ThoughtSource, a meta-dataset and software library for chain-of-thought (CoT) \nreasoning. The goal of ThoughtSource is to improve future artificial intelligence systems by facilitating \nqualitative understanding of CoTs, enabling empirical evaluations, and providing training data. This first \nrelease of ThoughtSource integrates seven scientific/medical, three general-domain and five math word \nquestion answering datasets.\nBackground & Summary\nThe most recent generation of large language models (LLMs) has produced impressive results across a wide \nrange of tasks. Examples of such models include T01, GPT-32, InstructGPT3 and GPT-44. These models demon-\nstrated remarkable ability to generate text that is both realistic and coherent, as well as good performance on a \nbroad spectrum of tasks, despite not explicitly being trained on them\n3.\nHowever, despite this ability, LLMs are also limited in several ways. They often fail to produce accurate pre-\ndictions due to their inability to accomplish complex reasoning, such as solving mathematical problems or \nquestion answering tasks requiring multi-hop reasoning. Furthermore, they tend to be black boxes, making it \ndifficult to understand how and why predictions are generated. These limitations severely limit the application \ndomains of LLMs and have the potential to cause harm, as lack of explainability and robustness can lead to crit\n-\nical failures and biases when these models are deployed in practice.\nOne recently proposed method for enabling complex reasoning and generating explanations with LLMs is to \nforce models to explicitly verbalize reasoning steps as natural language, a technique known as chain-of-thought \nprompting\n5,6. This method improved performance on a variety of tasks and sparked the active development of \nfurther refinements7, such as decomposing problems and structuring reasoning (e.g., least-to-most prompting8, \nReAct9, self-ask10, maieutic prompting11, successive prompting12) and/or extending LLM capabilities by lever-\naging external services for tasks like information retrieval (e.g., self-ask 10, IRCoT13, DSP14). The terminology \nsurrounding these rapidly evolving techniques is not settled, hence in this document, we refer to all approaches \nthat result in a linear sequence of reasoning steps as ‘chain-of-thought’ (CoT).\nMeta-datasets (datasets of datasets) that are easily accessible and standardized have proven useful for train\n-\ning and evaluating versatile LLMs. Examples include SuperGLUE15 for general-domain language model tasks, \nBigBIO16 and BLURB 17 for biomedical tasks, or Pile 18 and ROOTS 19 as text corpora for LLM pre-training. \nDatasets can be complemented by tools such as PromptSource, which was used to convert a large number of \ndatasets into prompts fit for training and interrogating LLMs. PromptSource facilitated training the highly per\n-\nformant T0 model1.\n1Institute of Artificial Intelligence, Medical University of Vienna, Vienna, Austria. 2Section for Cognitive Systems, \nTechnical University of Denmark, Lyngby, Denmark. 3Department of Clinical Immunology, Copenhagen University \nHospital, Copenhagen, Denmark. 4School of Electrical Engineering and Computer Science, The Royal Institute of \nTechnology (KTH), Stockholm, Sweden. 5These authors contributed equally: Simon Ott, Konstantin Hebenstreit. \n✉e-mail: matthias.samwald@meduniwien.ac.at\nDaTa DESCR iPTOR\nOPEN\n\n2Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nHere we present ThoughtSource, a meta-dataset and software library for chain-of-thought reasoning in LLMs \n(https://github.com/OpenBioLink/ThoughtSource). The goals of ThoughtSource are to:\n•\t Facilitate qualitative understanding of CoTs generated by LLMs under various conditions (e.g., across tasks, \nmodels and prompts).\n•\t Enable empirical and quantitative evaluation.\n•\t Provide a library of diverse CoT training data for improving performance, robustness, explainability and \nvalue-alignment of future LLM-based AI systems.\nMethods\nWe selected NLP benchmarks for question answering and natural language inference for which pre-existing \ndata for constructing CoTs was available. For some of the datasets, one or multiple additional datasets were \nused as sources for additional CoTs, allowing for the comparison of different CoT generation methodologies. \nWe created data loader scripts compatible with the Hugging Face datasets library\n20 for all datasets. Additionally, \nwe collected metadata of attributes such as descriptions, websites and licenses. We contacted dataset providers \nand encouraged them to choose an open source/open data license if licensing information was unavailable or \nunclear.\nWe implemented two kinds of schemas: (1) source dataset schemas, which are unique to each dataset and \nprovide data close to their original format; and (2) a standardized ThoughtSource schema, which maps all \ndatasets into a common format. The ThoughtSource schema was created by extending the question answering \nschema of the BigBIO project\n16.\nWe implemented tailored algorithms for converting each dataset because the collected datasets provide \nexplanations in different ways, such as math expressions or structured graph-based explanations. Furthermore, \nwe performed preprocessing such as capitalization and punctuation correction. To recover standard formatted \ntext from pre-tokenized datasets, we reversed the tokenization. This preprocessing was performed only on data \nin the ThoughtSource schema, while data in the Source schemas was left in their original formatting. All code \nfor running these conversions is available in our Github repository.\nWe developed a suite of Python libraries and tools for generating novel CoTs and answers by calling LLM \nAPIs, as well as tools for evaluating, comparing and annotating datasets. We built upon the LangChain library \n(https://github.com/hwchase17/langchain/) for interfacing with a wide variety of external LLM APIs.\nThis first release of ThoughtSource integrates seven scientific/medical, three general-domain and five math \nword question answering datasets (Table 1). For every dataset except for PubmedQA and MedQA we provide \n‘reference CoTs’ . We created these reference CoTs by converting rationales provided by original datasets into \nreasoning chains. These rationales, depending on the dataset, were created by human experts or obtained from \ncrowdsourcing. Furthermore, we added CoTs generated by state-of-the-art LLMs by importing them from pre\n-\nvious work, as well as generating them de-novo for this work (details below).\nScientific/medical question answering datasets. WorldTree V2 21 is one of the most detailed \nmulti-hop science question answering datasets available. Finding the right multiple-choice answers requires a \nmulti-hop inference combining between 1 and 16 facts (average: 6). It contains explanations created by experts in \nthe form of multiple facts. We concatenated these facts and applied a set of rules to improve style and grammati-\ncality to yield reference CoTs that are close to natural language.\nEntailmentBank22 contains open-domain science exam questions and answers, along with systematic expla-\nnations that show how the correct answer is reached through a series of steps. These steps are organized into a \ntree structure, known as an entailment tree, which starts with known facts and progresses through intermediate \nconclusions until the final answer is reached. These entailment trees are also serialized into text-based proofs by \ntraversing the trees. We applied a set of rules to improve style and grammaticality in these proofs to yield refer\n-\nence CoTs that are close to natural language.\nOpenBookQA 23 contains questions modeled after open-book exams of elementary-level science. They \nrequire multi-step reasoning, commonsense knowledge, and a diverse application of core science facts to find \nthe correct answer. The dataset provides over 1,300 core science facts and a mapping to all of the questions. By \ndesign, questions in OpenBookQA are answered incorrectly by both retrieval-based and word co-occurrence \nalgorithms. The dataset contains a single-fact explanation of the correct answer for each question, which we \nadopted to create reference CoTs.\nMedQA\n24 is a free-form multiple-choice OpenQA dataset containing questions from medical board exams in \nthe US (USMLE), Mainland China and Taiwan. We imported the English-language USMLE subset. We have also \nintroduced a version of the dataset wherein the multiple-choice questions have been converted into open-ended \nquestions\n25. Reference CoTs are not provided.\nMedMCQA26 is a multiple-choice question answering dataset containing real-world medical entrance exam \nquestions from the All India Institute of Medical Sciences (AIIMS PG) and National Eligibility cum Entrance \nTest (NEET PG). Answer rationales authored by human experts were integrated as reference CoTs.\nPubmedQA\n27 is a question answering dataset containing biomedical questions extracted from PubMed \nabstracts that can be answered with yes/no/maybe answers. In addition to the short answer, each question comes \nwith a longer answer, which can be used as reference CoT.\nMMLU\n28 (Massive Multitask Language Understanding) is a compendium of 57 distinct question-and-answer \ntasks encompassing a wide range of topics. We have selected six subjects particularly related to medical science: \nanatomy, clinical knowledge, college biology, college medicine, medical genetics, and professional medicine. \nReference CoTs are not provided.\n3Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nGeneral-domain question answering datasets. CommonsenseQA29 is a collection of multiple-choice \nquestions that test a wide range of general knowledge. We created reference CoTs for the train and validation set \nderived from the crowd-sourced ECQA dataset³. We also added AI-generated reasoning chains generated with \nfew-shot\n5 and zero-shot6 prompting, which are available for the validation split.\nStrategyQA30 is a question answering dataset that tests the ability to reason through open-domain questions \nand provide Y es/No answers. Each example includes a question, a decomposition of the question into reasoning \nsteps, and evidence paragraphs from Wikipedia. The dataset was created through a crowdsourcing process to \ngather creative and diverse questions. Human-generated freetext reasoning chains are part of the train split of \nthe original dataset and were used as CoTs. The dataset also includes relevant paragraphs from Wikipedia, but \nthese were not included in our CoTs. We extended the StrategyQA dataset with AI-generated CoTs created \nthrough few-shot\n5 and zero-shot6 prompting, which are available for the train split.\nQED31 is a collection of expert-annotated structured explanations for answers to questions, built upon a \nsubset of the Google Natural Questions dataset. Given a question and a passage from Wikipedia, QED uses \nlinguistic information to represent explanations as a series of interpretable steps, such as referential equality, \nsentencehood, and entailment. Structured reasoning chains by experts are provided for all examples. To create \nreference CoTs, we extracted the sentence that entails the answer; statements about referential equality in QED \nwere converted to natural language and added as additional steps in the CoTs (e.g. “The noun phrase […] in the \nsentence and the noun phrase […] in the question refer to the same thing. ”).\nMath word problem datasets. Algebra Question Answering with Rationales (AQUA-RAT) 32 is a \nlarge-scale multiple-choice dataset containing algebraic word problems. Each problem consists of a question with \nfive possible answers and a rationale, a step-by-step natural language explanation of the solution. We used natural \nlanguage explanations as reference CoTs.\nAcademia Sinica Diverse (ASDiv) math word problem (MWP) dataset 33 aims to provide more diverse \nlanguage patterns and problem types than previous datasets. It covers most of the math topics taught in elemen-\ntary school. Each MWP is labeled with its grade level (for indicating difficulty), the needed math operation (e.g. \ndivision) and includes a short explanation of the solution. ASDiv contains explanations of answers in the form \nof nested math expressions using common operators such as addition, subtraction, division and multiplication. \nWe generated reference CoTs by converting these math expressions into natural language explanation chains \nusing a rule-based method.\nDataset License\nScientific and medical question answering\nWorldTree V221 AI2 Mercury license\nEntailmentBank22 CC BY 4.0\nOpenBookQA23 § Apache License 2.0\nMedQA (USMLE)24 §\nCore dataset MIT\nCoT source: few-shot from Liévin et al.38 CC-BY 4.0\nOpen ended questions25 MIT\nMedMCQA26 § Core dataset MIT\nCoT source: few-shot from Liévin et al.38 CC-BY 4.0\nPubmedQA27 Core dataset MIT\nCoT source: few-shot from Liévin et al.38 CC-BY 4.0\nMMLU28 Core dataset, medical subsets MIT\nGeneral-domain question answering\nCommonsenseQA29 §\nCore dataset MIT\nCoT source: ECQA3 Community Data License Agreements Sharing license 1.0\nCoT source: few-shot from Wei et al.5, zero-shot \nfrom Kojima et al.6 Unspecified\nStrategyQA30 §\nCore dataset MIT\nCoT source: few-shot from Wei et al.5, zero-shot \nfrom Kojima et al.6 Unspecified\nQED31 CC BY-SA 3.0\nMath word problems\nAQUA-RAT32 Apache 2.0\nASDiv33 CC BY-NC 4.0\nGSM8K34 MIT\nMAWPS35 MIT\nSV AMP36 MIT\nTable 1. Integrated datasets. For some core datasets, additional datasets were used as sources for additional \nCoTs. §for these datasets we generated additional zero-shot CoTs with a variety of LLMs as part of the \nThoughtSource-33 subset (license of generated CoTs: MIT).\n4Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nGrade School Math 8 K (GSM8K)34 contains grade school math word problems. Despite their conceptual \nsimplicity, these problems are more challenging to process than earlier datasets due to their linguistic diversity. \nThe creators of GSM8K instructed crowd workers to write solutions to problems in free text format, which we \nused as reference CoTs in ThoughtSource, omitting any additional arithmetic specifications.\nDataset ID Examples\nExamples w. Human \nReference CoTs\nExamples w. AI-\ngenerated CoT s\nNumber of AI-\ngenerated CoT s Answer type\nAQUA-RAT 97,975 97,975 0 0 multiple choice\nASDiv 1218 1218 0 0 number\nCommonsenseQA 12,102 10,962 1221 4417 multiple choice\nEntailmentBank 1840 1840 0 0 text\nGSM8K 8792 8792 0 0 number\nMAWPS 1921 1921 0 0 number\nMedQA (USMLE) 12,723 0 1273 135,640 multiple choice\nMedMCQA 193,155 161,558 1000 106,967 multiple choice\nMMLU (medical) 1242 0 0 0 multiple choice\nOpenBookQA 5957 5957 100 1980 multiple choice\nPubmedQA 1000 1000 500 2500 multiple choice\nQED 6175 6175 0 0 collection\nStrategyQA 2780 2290 2289 6512 bool\nSV AMP 1000 1000 0 0 number\nWorldTree V2 4367 4365 100 1980 multiple choice\nTable 2. Statistics and answer types for all datasets. Note that generated CoTs are not available for all examples, \nand multiple CoT might have been generated for any given example.\nField Description Datatype\nid Unique identifier of object string\nref_id Identifier of external objects such as documents or other resources string\nquestion Question of task string\ntype Type of the question answering task, currently one of [“multiplechoice” , “text” , “number” , “collection”]string\nchoices Set of multiple options containing the gold answer list(string)\ncontext Additional context for answering the question string\ncot Reference CoT, often human-generated. list(string)\nanswer Gold answer of task. Can contain multiple elements if type is collection list(string)\ngenerated_cot List of generated_cot objects list(generated_cot_object)\nTable 3. Fields of the ‘sample’ object.\nField Description Datatype\nid Unique identifier of object string\ntemplates_version Version of the fragments.json file string\ninstruction Identifier of the cot trigger fragment stored in fragments.json string\ncot_trigger Identifier of the cot trigger fragment stored in fragments.json string\ncot_trigger_template Template to specify structure of prompt text string\nprompt_text Full text of prompt used for the CoT generation step string\nanswers List of generated answer objects list(answer_object)\ncot Generated chain-of-thought string\nauthor Name of the author string\ndate Date of the chain-of-thought generation string\napi_service Identification of the used api service string\nmodel Identification of the used language model string\ncomment Comment string\nannotations List of annotation objects list(annotation_object)\nTable 4. Fields of the ‘generated_cot’ object.\n5Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nMath Word Problems (MAWPS)35 is an online platform that provides a collection of math word problems. \nThe problems have simple one- or two-line explanations for their solutions. MAWPS includes datasets from \nField Description Datatype\nauthor Name of the author string\ndate Date of the creation of the annotation string\nkey Specifies the label of the annotation string\nvalue Specifies the value of the annotation string\nTable 6. Fields of the ‘annotation’ object.\nField Description Datatype\nid Unique identifier of object string\nanswer_extraction Identifier of the answer extraction fragment stored in fragments.json string\nanswer_extraction_template Template to specify structure of the answer extraction text string\nanswer_extraction_text Full text of prompt used for the answer extraction step string\nanswer Extracted answer string\ncorrect_answer True if the extracted answer is equal to the gold answer, else false bool\nTable 5. Fields of the ‘answer’ object.\nFig. 1 Distribution of question (a) and reference (b) CoT field lengths.\n6Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nvarious sources, offers tools for automatically creating datasets with specific characteristics as well as the possi-\nbility to tune lexical and template overlap. We converted explanatory math expressions to reference CoTs with \nan approach similar to the one used for ASDiv.\nSimple Variations on Arithmetic Math Word Problems (SV AMP)36 was created by applying carefully cho-\nsen variations to examples from existing datasets, such as ASDiv and MAWPS. These variations make it difficult \nfor language models to solve the problems using simple heuristics, and instead require a deeper understanding \nand reasoning ability. We converted math expressions to reference CoTs with an approach similar to the one \nused for ASDiv.\nFig. 2 n-gram overlap in questions and reference CoTs. Overlap is measured by mutual n-gram overlap using \nn = 3, values < 0.01 are omitted.\n7Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nai-generated CoTs. Liévin et al. CoTs were generated for MedQA, MedMCQA and PubmedQA with the \nAI systems text-davinci-0023 and code-davinci-00237 (described in detail by co-authors Liévin et al. in a separate \nmanuscript38).\nWei et al. and Kojima et al. CoTs for CommonsenseQA and StrategyQA were integrated from previous \nexternal studies on few-shot5 and zero-shot6 prompting.\nThoughtSource-33 refers to a collection of 198 items, comprising 33 randomly selected items from each of \nsix datasets: Commonsense QA, MedQA (USMLE), MedMCQA, OpenBookQA, StrategyQA and WorldTree \nV2. For every item of this collection, we created 60 unique zero-shot CoTs by executing ten different prompt-\ning strategies 39 with six models: OpenAI text-davinci-002 3, OpenAI text-davinci-003 3, OpenAI GPT-\n3.5-turbo, OpenAI GPT-4 4, Flan-T5-XXL 40 and Cohere command-xlarge-nightly (https://docs.cohere.ai/).  \nSince current LLM models are still prone to errors, it should be noted that AI-generated CoTs may contain \nfaulty reasoning.\nData Records\nThe suggested method for accessing datasets is through programmatic access through our dataloader libraries.  \nA comprehensive guide on how to achieve this is provided on the project’s Github repository (https://github.\ncom/OpenBioLink/ThoughtSource), and a snapshot of the code is available on Zenodo\n41. Additionally, a snap-\nshot of the data available through an open license is also available on Zenodo42.\nTable 2 shows the example counts, CoT counts and answer types of each dataset. The majority of datasets \nin the current collection are of the multiple choice answer type. The medical dataset MedMCQA is the largest \namong all datasets.\nDataset schema. Tables 3–6 provide descriptions and datatypes of the various fields in the ThoughtSource \nschema. Any performed sample task leads to a generated CoT and answer to the question. Annotations can be \nadded programmatically or through an annotator tool.\nWe analyzed the distribution of question and reference CoT field lengths (Fig.  1). MedQA has the longest \nmedian question length, while PubMedQA has the longest median CoT length. Several datasets contain outlier \nCoT with extremely long text lengths. Context fields were only filled for the PubmedQA and QED datasets, with \nmean context lengths of 116 and 56 tokens, respectively.\nFig. 3 Demonstration of the ThoughtSource API. Basic functionalities of the data loader, generator and \nevaluator modules are demonstrated.\n8Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nTechnical Validation\nThe datasets were reviewed by three team members and issues were tracked on the issue tracker of the associated \nGitHub repository.\nTo characterize potential overlaps and relations between datasets, we calculated mutual n-gram overlap using \nn = 3. (Fig. 2). To quantify the overlap between two sets of n-grams we use the Szymkiewicz–Simpson coefficient \n(overlap coefficient), which can be interpreted as the proportion of n-grams of the smaller dataset that are con-\ntained in the bigger dataset:\n/uni2223/uni2223\n/uni2223/uni2223/uni2223/uni2223\n∩=XY XY\nXY\noverlap( ,)\nmin( ,)\nFig. 4 An excerpt of data generated by running the example code. Data for a single question from Worldtree \nV2 are shown, including human-authored reference CoT, gold-standard answer, an AI-generated CoT and \nextracted answer, as well as evaluation results. Some fields were omitted for legibility.\n9Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nThere is an overlap of 1.0 between the set of questions in WorldTree v2 and EntailmentBank. The QA pairs \nin EntailmentBank were taken from the WorldTree v2 dataset 22, so all the questions in EntailmentBank are a \nsubset of WorldTree v2.\nFurthermore, there is significant overlap between the questions contained in ASDiv and SV AMP and those \nin ASDiv and MAWPS. ASDiv and SV AMP have overlapped questions because a subset of examples from ASDiv \nwas used as seed examples for the creation of SV AMP . For MAWPS and ASDiv, questions were crawled from web \nresources. The overlap could be due to examples being crawled from the same web resources.\nBesides overlaps in questions, we also identified overlaps in reference CoTs. WorldTree v2 provided an \ninitial pool of atomic facts that the annotators could use to construct an explanation tree in EntailmentBank \n(in addition to creating their own facts). This explains the high overlap of n-grams of CoTs in WorldTree v2 \nand EntailmentBank. Similarly, a subset of WorldTree v2 facts was used for the creation of explanations in \nOpenBookQA.\nUsage Notes\nPython libraries for accessing and working with data can be downloaded from the Github repository and installed \nwith the pip tool. Figure 3 demonstrates how to load a dataset, randomly sample from the pre-populated data in the \ndataset, call an external LLM API to generate novel CoTs and answers, automatically evaluate the accuracy of gener\n-\nated answers, and finally save all generated data to a JSON file. Figure 4 depicts an excerpt of the resulting JSON file.\nIn a zero-shot setup, specific text fragments can be used to prompt question answering and CoT reasoning in \nLLMs. ThoughtSource includes a curated list of text fragments that can be used to generate novel CoTs (Fig. 5). \nWhere possible, we also mapped individual CoTs in pre-existing CoT datasets to the text fragments that were \nused in their creation.\nWe provide two web-based interfaces for exploring and annotating ThoughtSource data, the Dataset Viewer \nand the Annotator . The Dataset Viewer is a simple interface for exploring dataset contents. The Annotator \n(Fig. 6) allows you to upload specific subsets of a dataset, provides convenience functions for highlighting sim\n-\nilarities between different generated CoTs and the correctness of generated answers, and allows you to annotate \nindividual CoTs interactively. The annotator facilitates identifying strengths and weaknesses of different CoTs. \nAnnotations can be used for downstream model evaluation and further improving the capabilities of AI models \nthrough fine-tuning/reinforcement learning.\nFig. 5 An excerpt of the collection of prompt fragments. These fragments can be used to build prompts for \ninteracting with LLMs, allowing for empirical testing of how different prompts affect model performance.\n10Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nAll tools and libraries, as well as more detailed demonstration notebooks, can be found on the project Github \npage.\nWe plan to add more datasets and generated CoTs to the ThoughtSource repository, and we welcome outside \ncontributions. Novel CoTs for existing core datasets can be generated and shared through the ThoughtSource \nAPIs and JSON files. Completely new datasets can also be added, as described in the Github repository’s con -\ntribution guide.\nCode availability\nAll code, data and tools are openly available at https://github.com/OpenBioLink/ThoughtSource, a snapshot of \nthe GitHub repository is archived at https://doi.org/10.5281/zenodo.819939041, and a snapshot of dataset contents \nis archived at https://doi.org/10.5281/zenodo.819953842. Our code and data are licensed under an MIT license, \nwhile data adapted from existing datasets are available under the licenses of their respective sources.\nReceived: 28 February 2023; Accepted: 31 July 2023;\nPublished: xx xx xxxx\nReferences\n 1. Sanh, V . et al. Multitask Prompted Training Enables Zero-Shot Task Generalization. Preprint at https://doi.org/10.48550/\narXiv.2110.08207 (2021).\n 2. Brown, T. B. et al. Language Models are Few-Shot Learners. Preprint at https://doi.org/10.48550/arXiv.2005.14165 (2020).\n 3. Ouyang, L. et al . Training language models to follow instructions with human feedback. Preprint at https://doi.org/10.48550/\narxiv.2203.02155 (2022).\n 4. OpenAI. GPT-4 Technical Report. Preprint at https://doi.org/10.48550/arXiv.2303.08774 (2023).\n 5. Wei, J. et al. Chain of Thought Prompting Elicits Reasoning in Large Language Models. Preprint at https://doi.org/10.48550/\narxiv.2201.11903 (2022).\nFig. 6 The ThoughtSource Annotator. The web-based interface allows for convenient inspection and annotation \nof reasoning chains and answers. Text that is similar between CoTs can be automatically highlighted based on \nan easily adjustable similarity threshold, facilitating a better understanding of similarities and differences of \ndifferent reasoning chains.\n\n11Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\n 6. Kojima, T., Gu, S. S., Reid, M., Matsuo, Y . & Iwasawa, Y . Large Language Models are Zero-Shot Reasoners. Preprint at https://doi.\norg/10.48550/arxiv.2205.11916 (2022).\n 7. Huang, J. & Chang, K. C.-C. Towards Reasoning in Large Language Models: A Survey. Preprint at https://doi.org/10.48550/\narXiv.2212.10403 (2022).\n 8. Zhou, D. et al . Least-to-Most Prompting Enables Complex Reasoning in Large Language Models. Preprint at https://doi.\norg/10.48550/arxiv.2205.10625 (2022).\n 9. Y ao, S. et al. ReAct: Synergizing Reasoning and Acting in Language Models. Preprint at https://doi.org/10.48550/arxiv.2210.03629 \n(2022).\n 10. Press, O. et al. Measuring and Narrowing the Compositionality Gap in Language Models. Preprint at https://doi.org/10.48550/\narxiv.2210.03350 (2022).\n 11. Jung, J. et al. Maieutic Prompting: Logically Consistent Reasoning with Recursive Explanations. Preprint at https://doi.org/10.48550/\narxiv.2205.11822 (2022).\n 12. Dua, D., Gupta, S., Singh, S. & Gardner, M. Successive Prompting for Decomposing Complex Questions. Preprint at https://doi.\norg/10.48550/arXiv.2212.04092 (2022).\n 13. Trivedi, H., Balasubramanian, N., Khot, T. & Sabharwal, A. Interleaving Retrieval with Chain-of-Thought Reasoning for Knowledge-\nIntensive Multi-Step Questions. Preprint at https://doi.org/10.48550/arXiv.2212.10509 (2022).\n 14. Khattab, O. et al. Demonstrate-Search-Predict: Composing retrieval and language models for knowledge-intensive NLP . Preprint at \nhttps://doi.org/10.48550/arXiv.2212.14024 (2023).\n 15. Wang, A. et al. SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems. in Advances in Neural \nInformation Processing Systems (eds. Wallach, H. et al.) vol. 32, 3266–3280 (Curran Associates, Inc., 2019).\n 16. Fries, J. A. et al . BigBIO: A Framework for Data-Centric Biomedical Natural Language Processing. in. Advances in Neural \nInformation Processing Systems. https://doi.org/10.48550/arXiv.2206.15076 (2022).\n 17. Gu, Y. et al. Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing. ACM Trans. Comput. \nHealthc. 3, 2:1–2:23 (2021).\n 18. Gao, L. et al . The Pile: An 800GB Dataset of Diverse Text for Language Modeling. Preprint at https://doi.org/10.48550/\narXiv.2101.00027 (2020).\n 19. Laurençon, H. et al. The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset. in Advances in Neural Information \nProcessing Systems (2022).\n 20. Lhoest, Q. et al. Datasets: A Community Library for Natural Language Processing. in Proceedings of the 2021 Conference on Empirical \nMethods in Natural Language Processing: System Demonstrations 175–184, https://doi.org/10.18653/v1/2021.emnlp-demo.21  \n(Association for Computational Linguistics, 2021).\n 21. Xie, Z. et al. WorldTree V2: A Corpus of Science-Domain Structured Explanations and Inference Patterns supporting Multi-Hop \nInference. in Proceedings of the Twelfth Language Resources and Evaluation Conference 5456–5473 (European Language Resources \nAssociation, 2020).\n 22. Dalvi, B. et al. Explaining Answers with Entailment Trees. Preprint at https://doi.org/10.48550/arXiv.2104.08661 (2022).\n 23. Mihaylov, T., Clark, P ., Khot, T. & Sabharwal, A. Can a Suit of Armor Conduct Electricity? A New Dataset for Open Book Question \nAnswering. in Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing 2381–2391, https://doi.\norg/10.18653/v1/D18-1260 (Association for Computational Linguistics, 2018).\n 24. Jin, D. et al. What Disease Does This Patient Have? A Large-Scale Open Domain Question Answering Dataset from Medical Exams. \nAppl. Sci. 11, 6421 (2021).\n 25. Nair, V ., Schumacher, E., Tso, G. & Kannan, A. DERA: Enhancing Large Language Model Completions with Dialog-Enabled \nResolving Agents. Preprint at https://doi.org/10.48550/arxiv.2303.17071 (2023).\n 26. Pal, A., Umapathi, L. K. & Sankarasubbu, M. MedMCQA: A Large-scale Multi-Subject Multi-Choice Dataset for Medical domain \nQuestion Answering. in Proceedings of the Conference on Health, Inference, and Learning 248–260 (PMLR, 2022).\n 27. Jin, Q., Dhingra, B., Liu, Z., Cohen, W . & Lu, X. PubMedQA: A Dataset for Biomedical Research Question Answering. in Proceedings \nof the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural \nLanguage Processing (EMNLP-IJCNLP) 2567–2577, https://doi.org/10.18653/v1/D19-1259 (Association for Computational \nLinguistics, 2019).\n 28. Hendrycks, D. et al. Measuring Massive Multitask Language Understanding. Preprint at https://doi.org/10.48550/arXiv.2009.03300 \n(2020).\n 29. Talmor, A., Herzig, J., Lourie, N. & Berant, J. CommonsenseQA: A Question Answering Challenge Targeting Commonsense \nKnowledge. in Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: \nHuman Language Technologies, Volume 1 (Long and Short Papers) 4149–4158, https://doi.org/10.18653/v1/N19-1421 (Association \nfor Computational Linguistics, 2019).\n 30. Geva, M. et al. Did Aristotle Use a Laptop? A Question Answering Benchmark with Implicit Reasoning Strategies. Trans. Assoc. \nComput. Linguist. 9, 346–361 (2021).\n 31. Lamm, M. et al. QED: A Framework and Dataset for Explanations in Question Answering. Trans. Assoc. Comput. Linguist. 9, \n790–806 (2021).\n 32. Ling, W ., Y ogatama, D., Dyer, C. & Blunsom, P . Program Induction by Rationale Generation: Learning to Solve and Explain \nAlgebraic Word Problems. in Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long \nPapers) 158–167, https://doi.org/10.18653/v1/P17-1015 (Association for Computational Linguistics, 2017).\n 33. Miao, S., Liang, C.-C. & Su, K.-Y . A Diverse Corpus for Evaluating and Developing English Math Word Problem Solvers. in \nProceedings of the 58th Annual Meeting of the Association for Computational Linguistics 975–984, https://doi.org/10.18653/v1/2020.\nacl-main.92 (Association for Computational Linguistics, 2020).\n 34. Cobbe, K. et al. Training Verifiers to Solve Math Word Problems. Preprint at https://doi.org/10.48550/arXiv.2110.14168 (2021).\n 35. Koncel-Kedziorski, R., Roy, S., Amini, A., Kushman, N. & Hajishirzi, H. MAWPS: A Math Word Problem Repository. in Proceedings \nof the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies \n1152–1157, https://doi.org/10.18653/v1/N16-1136 (Association for Computational Linguistics, 2016).\n 36. Patel, A., Bhattamishra, S. & Goyal, N. Are NLP Models really able to Solve Simple Math Word Problems? in Proceedings of the 2021 \nConference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies  \n2080–2094,https://doi.org/10.18653/v1/2021.naacl-main.168 (Association for Computational Linguistics, 2021).\n 37. Chen, M. et al. Evaluating Large Language Models Trained on Code. Preprint at https://doi.org/10.48550/arXiv.2107.03374 (2021).\n 38. Liévin, V ., Hother, C. E. & Winther, O. Can large language models reason about medical questions? Preprint at https://doi.\norg/10.48550/arxiv.2207.08143 (2022).\n 39. Hebenstreit, K., Praas, R., Kiesewetter, L. P . & Samwald, M. An automatically discovered chain-of-thought prompt generalizes to \nnovel models and datasets. Preprint at https://doi.org/10.48550/arxiv.2305.02897 (2023).\n 40. Chung, H. W . et al. Scaling Instruction-Finetuned Language Models. Preprint at https://doi.org/10.48550/arxiv.2210.11416 (2022).\n 41. Ott, S. et al. ThoughtSource: A central hub for large language model reasoning data (code snapshot). Zenodo https://doi.org/10.5281/\nzenodo.8199390 (2023).\n 42. Ott, S. et al. ThoughtSource: A central hub for large language model reasoning data (dataset snapshot). Zenodo  https://doi.\norg/10.5281/zenodo.8199538 (2023).\n12Scientific  Data |          (2023) 10:528  | https://doi.org/10.1038/s41597-023-02433-3\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nacknowledgements\nWe thank primary dataset contributors who assisted with assembling the ThoughtSource meta-dataset.\nauthor contributions\nS.O. and K.H. wrote the code for accessing, converting, generating and analyzing datasets, and wrote parts \nof the manuscript and documentation. V .L., C.E. and O.W . generated and analyzed CoT data for medical \ndatasets. M.Ma. wrote the code of the annotator software. M.Mo. wrote a first prototype of code for accessing \nand converting datasets. R.P . contributed to improving code and documentation quality. M.S. conceived and \nsupervised the project and wrote parts of the manuscript and documentation. All authors have read and approved \nthe final manuscript.\nCompeting interests\nThe authors declare no competing interests.\nadditional information\nCorrespondence and requests for materials should be addressed to M.S.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\native Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not per-\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n \n© The Author(s) 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5597248673439026
    },
    {
      "name": "Natural language processing",
      "score": 0.38571107387542725
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I161878677",
      "name": "Austrian Research Institute for Artificial Intelligence",
      "country": "AT"
    },
    {
      "id": "https://openalex.org/I76134821",
      "name": "Medical University of Vienna",
      "country": "AT"
    },
    {
      "id": "https://openalex.org/I96673099",
      "name": "Technical University of Denmark",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I2802567020",
      "name": "Copenhagen University Hospital",
      "country": "DK"
    },
    {
      "id": "https://openalex.org/I86987016",
      "name": "KTH Royal Institute of Technology",
      "country": "SE"
    }
  ],
  "cited_by": 40
}