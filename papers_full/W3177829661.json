{
  "title": "ZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded Language Modelling track, 2021 edition",
  "url": "https://openalex.org/W3177829661",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4226842722",
      "name": "Alishahi, Afra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4222220845",
      "name": "Chrupała, Grzegorz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4287920823",
      "name": "Cristia, Alejandrina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4202141647",
      "name": "Dupoux, Emmanuel",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2743404022",
      "name": "Higy, Bertrand",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3179301242",
      "name": "Lavechin, Marvin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3211371961",
      "name": "Räsänen, Okko",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2096664151",
      "name": "Yu Chen",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2556930864",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2920166246",
    "https://openalex.org/W2346964103",
    "https://openalex.org/W2415378728",
    "https://openalex.org/W3100813302",
    "https://openalex.org/W2962862718",
    "https://openalex.org/W2114739781",
    "https://openalex.org/W2885307078",
    "https://openalex.org/W3110458199",
    "https://openalex.org/W2989358187",
    "https://openalex.org/W2149557440",
    "https://openalex.org/W3042657922",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W2963620343",
    "https://openalex.org/W3114436296",
    "https://openalex.org/W3105148948",
    "https://openalex.org/W2972943112",
    "https://openalex.org/W2963525826",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W3157861865",
    "https://openalex.org/W2984008963",
    "https://openalex.org/W2107917162",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2123815913",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W2586148577",
    "https://openalex.org/W1494198834",
    "https://openalex.org/W385555557",
    "https://openalex.org/W2784025607",
    "https://openalex.org/W1575001262",
    "https://openalex.org/W2971709506",
    "https://openalex.org/W2940544976",
    "https://openalex.org/W2991557631",
    "https://openalex.org/W2950133079"
  ],
  "abstract": "We present the visually-grounded language modelling track that was introduced in the Zero-Resource Speech challenge, 2021 edition, 2nd round. We motivate the new track and discuss participation rules in detail. We also present the two baseline systems that were developed for this track.",
  "full_text": "arXiv:2107.06546v1  [cs.CL]  14 Jul 2021\nZR-2021VG: Zero-Resource Speech Challenge, Visually-Grounded\nLanguage Modelling track, 2021 edition\nVersion 2.0 – ﬁnal for NeurIPS\nAfra Alishahia, Grzegorz Chrupa/suppress laa, Alejandrina Cristia b, Emmanuel Dupoux b, Bertrand\nHigya, Marvin Lavechinb, Okko R¨ as¨ anenc,d, and Chen Yu e\naDept. of Cognitive Science and AI, Tilburg University, Netherlands\nbLaboratoire de Sciences Cognitives et Psycholinguistique, ENS, Par is, France\ncUnit of Computing Sciences, Tampere University, Finland\ndDept. Signal Processing and Acoustics, Aalto University, Finland\neDept. of Psychology, University of Texas at Austin\nJuly 13, 2021\n1 Introduction\nThis document introduces the visually-grounded spoken language m odeling track of the ZeroSpeech 2021\nchallenge. In this track, participants are asked to use audiovisual materials during training. Evaluation is\nidentical to the speech-only track at ZeroSpeech 2021.\nLearning to comprehend and produce spoken languages is one of th e hallmarks of human cognition, and\nthe importance of speech communication also makes speech-based capabilities central to AI development.\nModern automatic speech recognition (ASR) systems largely rely on supervised training, where input speech\nis paired with corresponding phonetic annotations or text transcr ipts. While this approach has produced\ngreat results in high-resource languages such as English, deployme nt of similar systems for low-resource\nenvironments such as small language communities or even unwritten languages is diﬃcult. In addition, a\nmismatch between conversational speech and large-scale text data still exists even in high-resource languages.\nIn contrast to ASR, human children achieve their language skills witho ut direct supervision or detailed\nfeedback, simply interacting with their physical and linguistic environ ments. Moreover, these experiences\nare essentially multimodal: children not only hear speech of their caregivers, but concurren tly observe the\nworld through a number of senses. Instead of perceiving speech a nd the corresponding text, they learn from\nspeech in the context of diﬀerent everyday multimodal communicative scenarios. Compared with audio data\nonly, audiovisual data contain more statistical regularities at multip le levels: there are regularities within\naudio and visual data respectively; and there are also cross-moda l regularities at both the acoustic and\nsemantic levels. For example, the presence of a dog in a visual scene is likely to co-occur with both barking\nsounds at the acoustic level and the spoken word “dog” (and othe r words semantically related to dog) at\nthe semantic level. The presence of a dog can be used as a superviso ry signal to train language learning\nsystems to parse and segment words that consistently occur in th e same context, and semantically group\nwords that occur in similar visual contexts. In order to develop AI systems or models of human learning with\nsimilar multimodal language learning skills, a number of models and learning algorithms have been proposed\nthroughout the years (e.g., [\n1–6]). These systems have used various types of speech data with simu lated or\nrobot vision-based visual input.\nHowever, only the recent advances in deep learning have scaled up t he capabilities of audiovisual systems to\n1\na level where they can start to capture the relationships between realistic visual data (e.g., photographs or\nvideos) and language related to the visual scene. These models wer e ﬁrst developed for captions descriptive\nof images [7,8] and more recently for spoken descriptions of the images (e.g., [ 9–13]). In this context, it is of\ngreat interest how the representations emerging from training ofsuch multimodal models relate to the known\nlinguistic structure of the input language (e.g., [ 12,14–17]), and how such methods can support (or replace)\npurely audio-based representation learning approaches (e.g., [ 18, 19]). In other words, it would be highly\nuseful if unsupervised learning from multimodal data could be used t o acquire language representations such\nas phone(me)s and words without access to transcribed training d ata in the given language—units that can\nbe then used as a basis for many other language processing tasks. However, the research in this direction is\nstill young and largely driven by a few research groups [20]. In addition, there are no standardized evaluation\nmetrics or a common benchmark to compare diﬀerent methodologica l approaches and thereby to drive the\nresearch in this area forward.\nThe goal of the ZR-2021VG track is to tackle the issue of multimodal language learning. In contrast\nto the earlier Zerospeech-challenges [\n21–24] that have purely focused on audio-based learning of linguistic\nrepresentations (including the speech-based track of the curre nt challenge, on which we build), ZR-2021VG\ntakes a step towards multimodal language learning by asking particip ants to train on audiovisual data. The\naim of the challenge is to learn phonemic, lexical, syntactic, and seman tic representations of speech with the\nhelp of supporting visual information, as evaluated by standardize d evaluation protocols. As a result, the\nchallenge aims to bring together researchers from speech techno logy, natural language processing, computer\nvision, and machine learning to work on multimodal language learning, t hereby advancing the state-of-the-\nart in audiovisual learning algorithms and providing new knowledge on h ow visual information may support\nunsupervised learning of linguistic patterns from speech.\n2 Registration\nParticipation is open and free. Registration is done via the ZR2021 website\nhttps://zerospeech.com/2021/\nindex.html#zrc2021. The Challenge will be submitted as such to several conferences, a nd participants are\nencouraged to submit a paper to them. The ﬁrst conference is Neu rIPS; more information on participation\nis available from https://zerospeech.com/2021/index.html#neurips-competition\nFor any issues or questions, we recommend to registrants that th ey join our mailing list:\n1. Make sure you’re logged into google (you may need to get your ema il account registered with them\nhere: https://www.google.com/account/about)\n2. Go to https://groups.google.com/g/zrvg\n3. Ask to join.\nA moderator will approve your request within a week.\n3 Task\n3.1 Task deﬁnition\nThe challenge is to learn spoken language representations from raw audio-visual input, without any anno-\ntation or written transcriptions. Systems are allowed to use the ra w audio and image/video (or features\nextracted from them, such as MFCCs for the audio) of the training set(s) as input. The goal is to use the\nraw multimodal input data to discover discrete linguistic units at phon etic and word levels, which will be\nevaluated via a set of black-box, zero-shot metrics probing for th e quality of the learned models at diﬀerent\nlinguistic levels, including phonetics, lexicon, syntax and semantics. S ee Section\n3.3 for more information.\n2\nAny approach that exploits the correspondence between the two modalities without relying on supervision\nbased on linguistic annotations of the speech or visual signal is allowe d (see Section 6 for details). A range\nof objectives could be used for this purpose, including (but not limite d to):\n• generating the visual representations from the audio or vice vers a;\n• predicting the next audio frame with the visual input as an additional signal;\n• training language models on pseudo-text that has been generated with the help of the visual input;\n• inducing linguistic and visual representations that are geometrically similar in a shared semantic space.\nIn order to take into account the computing resources of participants, we distinguish categories of submissions\nin two tracks based on the type of model and resources employed ( see Section 3.2). Additionally, we may\nhighlight submissions that explore innovative architectures, succe ssfully use particularly small amounts of\ntraining data, or have other unusual features which contribute t o fostering new ideas and gaining collective\ninsight into this interesting problem.\n3.2 Model conditions\nThere are no restrictions or conditions on the models: if they are un supervised, they are eligible.\nIn order to take into account the computing resources of particip ants, we ask participants to report their\nmodels’ resource budget. As in ZR2021, you calculate your budget as number of hours × number of GPUs.\nAdditionally, if relevant, report Akaike’s Information Criterion and B ayesian Information Criterion; and/or\nnumber of parameters and number of pre-trained parameters. ( If you don’t know what that means, it\nprobably is not relevant.)\nWe also ask participants to report how much data (in number of spee ch hours, number of images, hours of\nvideo) they used for training their submitted models.\nOne overall restriction on models comes from the evaluation, which we want to align with the audio-only track\nof the Zerospeech-2021 challenge. The evaluation procedure foc uses on linguistic representations (ignoring\nthe visual representation), and will do so usingunimodal audio data. This means that the models submitted\nto the challenge should be able to process audio data independently o f visual data.\n3.3 Evaluation conditions\nWe will now brieﬂy describe the metrics and refer the reader to [\n24] for more details.\nEach metric evaluates models at a diﬀerent linguistic level:\n• Phonetic (ABX metric). The ABX metric allows to assess the degree of discriminability betwee n\ntwo categories A and B given a representation, where in our case A and B correspond to minimal\npairs of triphones such as /beg/ and /bag/ (diﬀering only in their midd le phoneme). Given a triple of\nstimuli (a, b, x) where a and x belong to the same category A and b to a diﬀerent category B, and given\na distance d(·, ·) between pairs of stimuli, discrimination is considered successful if d(a, x) < d(b, x).\nThe ABX score is obtained by aggregating scores across all minimal t riphone pairs.\n• Lexical (spot-the-word metric). This metric evaluates the possibility of discriminating between real\nwords and nonwords by associating a probability to each of them (wit h the expectation that a higher\nprobability is associated with the real word). The score is computed as the average discrimination\naccuracy across all pairs of words and nonwords.\n• Syntactic (acceptability metric). Similarly to the spot-the-word metric, the acceptability metric\ncomputes a discrimination accuracy between grammatical and ungr ammatical categories, based on a\nprobability associated to each of them. The pairs of sentences we u se are representative of 68 diﬀerent\nsyntactic paradigms belonging to twelve broad categories.\n3\n• Semantic (similarity metric). This metric compares the similarity between the representations\nof pairs of words to human similarity judgements. The metric is deﬁne d as the Spearman’s rank\ncorrelation coeﬃcient between the two similarity scores.\n4 Baseline models\nTo represent a variety of participants, two baseline models have be en produced.\n1 One uses a low budget\n(72 GPU hours); the other uses a high budget (165 GPU hours), co rresponding to the following submission\ntracks:\n• Track A: Low budget models. Low budget submissions use smaller mod els which can be trained with\nlower GPU memory and training hours (about 100 GPU hours maximum) .\n• Track B: High budget models. High budget submissions use more GPU m emory and/or more training\ntime.\nOur baselines are directly inspired by the baselines used in the audio-o nly track. The main diﬀerence is that\nwe incorporate a visually grounded (VG) model to learn our speech r epresentations. Those representations\nare then fed to the language model through K-means clustering. The low-budget baseline completely replaces\nthe contrastive predictive model (CPC) with the VG model. The high- budget baseline, on the other hand,\nadds the VG model on top of the CPC model.\n4.1 Data\nThe baselines are trained with two datasets:\n• SpokenCOCO [\n25] for the VG models.\n• LibriSpeech [26] for K-means clustering (100h from thetrain-clean-100 subset) and the language models\n(960h from the train-clean-100, train-clean-360 and train-other-500 subsets).\nImage features used to train the VG model are extracted from th e preclassiﬁcation layer of a frozen ResNet-\n152 model [ 27] pretrained on ImageNet [ 28]. We follow [ 29] and use the mean feature vector over ten crops\nof each image.\nThe acoustic feature vectors used by the low-budget baseline are composed of 12 mel-frequency cepstral\ncoeﬃcients (MFCCs) and log energy, with ﬁrst and second derivativ es, resulting in 39-dimensional vectors.\nThey are computed over windows of 25 ms of speech, with 10 ms shift . The high-budget baseline uses\nfeatures extracted from a pretrained CPC model that works on r aw waveform.\n4.2 Architecture\n4.2.1 Low-budget baseline\nThe VG model follows the speech-image architecture described in [\n30, 31]. It is composed of visual and\nspeech encoders which each extract ﬁxed length embeddings from the visual and audio input respectively.\nThe image encoder is composed of a single linear layer projecting the im age features into a shared semantic\nembedding space (dimension 2048), followed by a normalization layer ( ℓ2 norm).\nThe speech encoder is composed of a 1D convolutional layer (kerne l size 6, stride 2 and 64 output channels),\nfollowed by bidirectional gated recurrent units (GRUs) [ 32] (4 layers, hidden state of dimension 1024). A\nvectorial attention layer is then used to convert the variable lengt h input sequence to a ﬁxed-size vector of\ndimension 2048. Finally, a normalization layer ( ℓ2 norm) is applied.\n1Code and detailed instructions to repoduce our results can b e found at https://github.com/bhigy/zr-2021vg_baseline\n4\nOnce the model is trained, we extract activations of the 1 st recurrent layer of the speech encoder and\ncluster them with K-means (50 clusters). The quantized activation s are then used to train a language model\ncorresponding to the BERT-small architecture from the audio-on ly track.\n4.2.2 High-budget baseline\nThe high-budget baseline is essentally the same as the low-budget ba seline with following exceptions:\n• The VG model is trained on features extracted from the pretained CPC-small model from the audio-\nonly track. The activations of the last recurrent layer of the CPC m odel are used.\n• The language model corresponds to the BERT-big architecture fr om the audio-only track.\n4.3 Evaluation\nWe evaluated the models on the four metrics introduced in [\n24]. The phonetic scores are computed with\ncosine distance on continuous representation (i.e. before quantiz ation) extracted from the 1 st (resp. 2 nd)\nrecurrent layer of the VG (resp. CPC) model for the low-budget ( resp. high-budget) baseline. Lexical and\nsyntactic metrics rely on pseudo-probabilities obtained from the lan guage model of each pipeline. Finally,\nthe semantic scores are computed using activations extracted fr om the last (resp. 4 th) recurrent layer of the\nlanguage model for the low-budget (resp. high-budget) audio-on ly baseline, while the output of the attention\nlayer was used for both visually-grounded models. All semantic scor es are obtained with max pooling 2 and\nthe cosine distance to evaluate the similarity between activations.\nTable\n1 summarizes the results obtained with the two visually-grounded baselines as well as the two baselines\nfrom the audio-only track.\nWhile the audio-only baselines tend to perform better than their visually-grounded counterparts on phonetic,\nlexical and syntactic metrics, the latter obtain higher semantic sco res overall. One exception is the un-\nweighted semantic similarity scores on the test set where both visua lly-grounded baselines get very low\nscores. Upon inspection, this can be attributed to unstability in the scores obtained on the smallest sub-\ndatasets of the test set. This motivated the introduction of a weig hted version of the metric (where each\nsub-score is weighted by the size of the dataset it is obtained from) which shows much more stability.\nWe can also observe that the high-budget visually-grounded baselin e, by using CPC-small as a feature\nextractor, largely closes the gap with the high-budget audio-only b aseline on the phonetic, lexical and\nsyntactic metrics (especially the last two).\n5 Data\n5.1 Training data\nTraining data are not provided. Instead, ZR-VG2021 participants may use any publicly available, private, or\nproprietary data (audio or visual stream or snapshots; instruct ional videos with voiceovers; see Appendix\nA\nfor examples) to train their systems provided. However, synthet ic speech that is generated from text is not\nallowed. We strongly encourage the use of public data, as this improv es interpretability and cumulativity of\nresults. In addition, the following corpora cannot be used because they are part of the evaluation set (for\nmore details, see ZR2021’s challenge description [ 24]):\n• sWUGGY (from ZR2021)\n• sSIMI (from ZR2021)\n2For the visually-grounded models, the choice of the pooling mechanism doesn’t aﬀect the results as the output of the\nattention layer is a ﬁxed length vector and not a sequence.\n5\nTrack Budget Set\nPhonetic\nLexical Syntactic\nSemantic\nWithin Across Un-weighted Weighted\nclean other clean other synth. libri. synth. libri.\nAudio-only\nLow dev 0.03 0.05 0.04 0.08 0.61 0.52 4.42 7.07 4.42 7.07\ntest 0.03 0.05 0.04 0.08 0.61 0.53 7.35 2.38 7.31 5.82\nHigh dev 0.03 0.05 0.04 0.08 0.68 0.56 6.25 4.35 6.25 4.35\ntest 0.03 0.05 0.04 0.08 0.68 0.56 5.17 2.48 3.19 1.32\nVisually-grounded\nLow dev 0.09 0.10 0.11 0.15 0.53 0.53 9.65 12.61 9.65 12.61\ntest 0.08 0.11 0.11 0.15 0.53 0.53 9.71 0.16 12.57 12.55\nHigh dev 0.06 0.07 0.07 0.11 0.67 0.55 9.60 15.09 9.60 15.09\ntest 0.05 0.07 0.07 0.12 0.67 0.55 9.99 -0.10 13.46 10.96\nTable 1: Overall performance of the audio-only and the visually grou nded baselines on dev and test sets.\nPhonetic scores are reported in terms of within and across speakers ABX error rates (lower is better). Lexical\nand syntactic scores represent accuracies computed on the pse udo-probability of spotting the right stimuli\n(higher is better). The semantic similarity score is reported as the S pearman’s rank correlation coeﬃcient\nbetween the distance scores returned by the model and the true human scores (higher is better) Numbers in\nbold indicate best scores on the development set for each metric.\n• sBLIMP (from ZR2021)\nA good starting point for people new to working with audio-visual dat a is SpokenCOCO dataset, which was\nalso used to train our baselines.\nParticipants are responsible for documenting precisely the training data they used in any ﬁnal submissions,\nas part of their Methods section, including any pre-processing (su ch as correcting or adding voice activity\ndetection or denoising. We also ask that participants overtly descr ibe their training data in terms of how\nsimilar to natural human visual and audio ﬂow they are. For instance , Librispeech is read speech and thus\nmore formal and simpler than natural audio ﬂow, whereas the VanD am corpora [\n33, 34] was captured with\na wearable, thus representing natural audio conditions from a ﬁrs t-person perspective.\n5.2 Development and test data\nScoring will be done by the ZR team, as part of their general yearly c hallenge. Dev/test data are therefore\nnot deﬁned here. Please see [\n24] for information.\n6 Evaluation rules\nThe ZR-VG2021 is an open evaluation challenge. Participants must ag ree to work respecting the following\nrules:\n• You should describe training and system data thoroughly, as well as the computational resources used,\nfor your ﬁnal systems in any submission. Please see the Appendix\nC for the full list.\n• You should do your best to share training data and code publicly, for example by depositing them in\na scientiﬁc archive such as the Open Science Framework, osf.io, whic h contains a Github plugin.\n• You should not use the prohibited corpora listed in Section 5.1.\n• You should not perform manual/human investigation of the evaluatio n data such as listening, manual\nsegmentation, transcription, or other form of human annotation. Examples of such prohibited practice\nare speech recognition with a language model, phone(me) recognitio n with a supervised classiﬁer, or\nusing written captions or synthesized-speech captions.\n6\n• You can use any automatically derived information as long as that system was not trained with linguistic\nlabels, for example speaker diarization and denoising, or speaker or language identiﬁcation. You can\nalso use spoken captions of pictures.\n• Training must be done in an unsupervised fashion within the linguistic domain ; i.e., no linguistic labels\nallowed, including those generated via ASR systems (since these hav e themselves been trained using\nlinguistic labels).\n• One exception to the general rule of no supervision is for visual fea tures that may be pre-trained with\nobject labels (such as the ImageNet labels), but we categorically fo rbid the use of full captions, and\nwe caution against using this exception to allow linguistic supervision to seep in. If you are uncertain,\nplease contact the organizers.\nFailure to abide by the rules above can lead to disqualiﬁcation from the challenge, removal of existing\nsubmissions, and public remonstrations.\nSubmissions will be ranked per track (see Section\n4) on each of the four metrics described in Section 3.3\nseparately.\n7\nAppendix A: Ideas for training data\nAs in ZR-2021, the following are training options for clean audio:\n•\nLibriSpeech (the standard subsets: clean-100, clean-360, other-500)\n• Libri-light: small (600h), med(6kh) or large(60kh).\nIn addition, we suggest the following audio ﬂow corpora:\n• From Homebank, child-centered audio ﬂow captured with a wearable\n– Fausey, C. M., & Mendoza, J. K. (2018). FauseyTrio-Public HomeBank Corpus. doi:10.21415/T56D7Q\nhttps://homebank.talkbank.org/access/Public/FauseyTrio-Public.html\n– VanDam, Mark (2018). VanDam Public 5-minute HomeBank Corpus. d oi:10.21415/T5388S\nhttps://homebank.talkbank.org/access/Public/VanDam-5minute.html\n– VanDam, Mark (2018). VanDam Public Daylong HomeBank Corpus. do i:10.21415/T5QH5N\nhttps://homebank.talkbank.org/access/Public/VanDam-Daylong.html\nThe following are some ideas for training options for clean visual data :\nHere are training options for audio-video ﬂow:\n• https://childes.talkbank.org/\n• YouCook II, 2K videos, 176 hours [ 35]\n• How2, 80k instructional videos (about 2000h) with associated English su btitles and summaries that\nparticipants should NOT use. [ 36]\n• HowTo100M, 136M video clips with captions sourced from 1.2M Youtube instructional videos (15 years\nof videos) covering 23k activies: cooking, hand crafting, persona l care, gardening [ 37]\n• AVA Spoken Activity Datasets (densely labeled for NO SPEECH, CLEAN SPEECH, SPEECH -\nWITH MUSIC, SPEECH WITH NOISE). Extracted from 192 movies publicly available on YouTube\n(multiple languages); 45 hours total [ 38]\nAudio-image ﬂow:\n• MIT Places 205 : 400,000 spoken audio caption each of which describes a diﬀerent Pla ces image;\ncollected from over 2,500 diﬀerent speakers and covering a 40,000 w ord vocabulary. The average\ncaption duration is approximately 10 seconds. Used in [ 15].\n• Flickr8K: contains 40,000 read audio captions describing 8,000 images from Flic kr. Used in [ 9].\n• SpokenCOCO: contains approximately 600,000 read audio captions describing imag es from MSCOCO\ndataset. Used in [ 25].\nAppendix B: Q&A\nQ: Is it possible to use instructional videos with voiceovers?\nA: Yes!\nQ: I’m not interested in creating a model for the lower levels (phonet ics), I only care about word learning.\nA: You are welcome to use other unsupervised models at the beginnin g of your pipeline, for instance by\ntaking our baseline, and replacing the late components with your own ideas for the ”semantic” level sections.\nQ: Can I use other data, beyond audio and visual?\n8\nA: Provided it’s not text labels, you probably can. For instance, a mo del that is trained with 1. an image\npaired with 2. an audio ﬁle and 3. time-course of human gazes on the im age would be acceptable\nAppendix C: Checklist for submission\nHave you:\n• played by the rules, as in Section\n6?\n• clearly stated in your paper what your budget was?\n• reported on all 4 evaluation conditions, at least in an online appendix t o your paper?\n• described the data you used for training (in terms of sources as we ll as in terms of quantity)?\n• described at least one of your models in terms of complexity and comp uting resources?\nReferences\n[1] D. Roy and A. Pentland, “Learning words from sights and sounds : a computational model,” Cognitive\nScience, vol. 26, pp. 113–146, 2002.\n[2] C. Yu and D. Ballard, “A multimodal learning interface for groundin g spoken language in sensory\nperceptions,” ACM Transactions on Applied Perceptions , vol. 1, pp. 57–80, 2004.\n[3] L. ten Bosch, H. Van hamme, L. Boves, and R. K. Moore, “A comp utational model of language acqui-\nsition: the emergence of words,” Fundamenta Informaticae, vol. 90, pp. 229–249, 2008.\n[4] J. Driesen and H. Van hamme, “Modeling vocabulary acquisition, ad aptation and generalization in\ninfants using adaptive Bayesian PLSA,” Neurocomputing, vol. 74, pp. 1874–1882, 2011.\n[5] O. R¨ as¨ anen and H. Rasilo, “A joint model of word segmentation and meaning acquisition through\ncross-situational learning,” Psychological Review, vol. 122, pp. 792–829, 2015.\n[6] O. Mangin, D. Filliat, L. ten Bosch, and P.-Y. Oudeyer, “MCA-NMF: Multimodal concept acquisition\nwith non-negative matrix factorization,” PLOS One , 2015.\n[7] R. Socher, A. Karpathy, Q. V. Le, C. D. Manning, and A. Y. Ng, “ Grounded compositional semantics\nfor ﬁnding and describing images with sentences,” Transactions of the Association for Computational\nLinguistics, vol. 2, pp. 207–218, 2014.\n[8] A. Karpathy and F. Li, “Deep visual-semantic alignments for gene rating image descriptions,” in IEEE\nConference on Computer Vision and Pattern Recognition (CVP R 2015) , June 7–12, 2015, Boston, MA,\npp. 3128–3137., 2015, pp. 3128–3137.\n[9] D. Harwath and J. Glass, “Deep multimodal semantic embeddings for speech and images,” in 2015 IEEE\nWorkshop on Automatic Speech Recognition and Understandin g (ASRU) . IEEE, 2015, pp. 237–244.\n[10] D. F. Harwath, A. Torralba, and J. R. Glass, “Unsupervised lea rning of spoken language with visual\ncontext,” in Advances in Neural Information Processing Systems 29: Annu al Conference on Neural\nInformation Processing Systems (NIPS 2016) , December 5–10, 2016, Barcelona, Spain, pp. 1858–1866.,\n2016, pp. 1858–1866.\n[11] G. Synnaeve, M. Versteegh, and E. Dupoux, “Learning words from images and speech,” in 28th Confer-\nence on Neural Information Processing Systems (NIPS) Works hop on Learning Semantics , December\n8–13, 2014, Montreal, Canada., 2014.\n9\n[12] G. Chrupa/suppress la, L. Gelderloos, and A. Alishahi, “Representationsof language in a model of visually\ngrounded speech signal,” inProceedings of the 55th Annual Meeting of the Association fo r Computational\nLinguistics (Volume 1: Long Papers) , July 30–August 4, 2017, Vancover, Canada, pp. 613–622, 2017 ,\npp. 613–622.\n[13] H. Kamper, G. Shakhnarovich, and K. Livescu, “Semantic spee ch retrieval with a visually grounded\nmodel of untranscribed speech,” IEEE/ACM Transactions on Audio, Speech and Language Proces sing,\nvol. 27, pp. 89–98, 2019.\n[14] A. Alishahi, M. Barking, and G. Chrupa/suppress la, “Encoding of phonology in a recurrent neural model of\ngrounded speech,” in Proceedings of the 21st Conference on Computational Natura l Language Learning\n(CoNLL 2017) , August 3–4, 2017, Vancouver, Canada, pp. 368–378, 2017, pp . 368–378.\n[15] D. Harwath, W.-N. Hsu, and J. Glass, “Learning hierarchical dis crete linguistic units from visually-\ngrounded speech,” arXiv preprint arXiv:1911.09602 , 2019.\n[16] W. N. Havard, J. Chevrot, and L. Besacier, “Models of visually g rounded speech signal pay attention to\nnouns: A bilingual experiment on english and japanese,” inIEEE International Conference on Acoustics,\nSpeech and Signal Processing (ICASSP 2019) , May 12–17, 2019, Brighton, UK, pp. 8618–8622., 2019,\npp. 8618–8622.\n[17] ——, “Word recognition, competition, and activation in a model of visually grounded speech,” in Pro-\nceedings of the 23rd Conference on Computational Natural La nguage Learning (CoNLL 2019) , November\n3–4, 2019, Hong Kong, China, pp. 339–348., 2019, pp. 339–348.\n[18] Y. A. Chung, W. N. Hsu, H. Tang, and J. Glass, “An unsupervise d autoregressive model for speech rep-\nresentation learning,” Proceedings of the Annual Conference of the International S peech Communication\nAssociation, INTERSPEECH , pp. 146–150, 2019.\n[19] A. van den Oord, Y. Li, and O. Vinyals, “Representation learning with contrastive predictive coding,”\nCoRR, vol. abs/1807.03748, 2018. [Online]. Available: http://arxiv.org/abs/1807.03748\n[20] G. Chrupa/suppress la, “Visually grounded models of spoken language: A survey of datasets, architectures and\nevaluation techniques,” 2021, https://arxiv.org/abs/2104.13225.\n[21] M. Versteegh, X. Anguera, A. Jansen, and E. Dupoux, “The z ero resource speech challenge 2015:\nProposed approaches and results,” Procedia Computer Science , vol. 81, pp. 67–72, 12 2016.\n[22] E. Dunbar, X. N. Cao, J. Benjumea, J. Karadayi, M. Bernard, L. Besacier, X. Anguera, and E. Dupoux,\n“The zero resource speech challenge 2017,” 2017.\n[23] E. Dunbar, R. Algayres, J. Karadayi, M. Bernard, J. Benjume a, X.-N. Cao, L. Miskic, C. Dugrain,\nL. Ondel, A. W. Black, L. Besacier, S. Sakti, and E. Dupoux, “The ze ro resource speech challenge 2019:\nTts without t,” 2019.\n[24] T. A. Nguyen, M. de Seyssel, P. Roz´ e, M. Rivi` ere, E. Kharitonov, A. Baevski, E. Dunbar, and E. Dupoux,\n“The zero resource speech benchmark 2021: Metrics and baseline s for unsupervised spoken language\nmodeling,” arXiv preprint arXiv:2011.11588 , 2020.\n[25] W.-N. Hsu, D. Harwath, C. Song, and J. Glass, “Text-Free Ima ge-to-Speech Synthesis Using Learned\nSegmental Units,” in 34th Conference on Neural Information Processing Systems ( NeurIPS) Workshop\non Self-Supervised Learning for Speech and Audio Processin g, Dec. 2020.\n[26] V. Panayotov, G. Chen, D. Povey, and S. Khudanpur, “Librisp eech: An asr corpus based on public do-\nmain audio books,” in 2015 IEEE International Conference on Acoustics, Speech an d Signal Processing\n(ICASSP), 2015, pp. 5206–5210.\n10\n[27] K. He, X. Zhang, S. Ren, and J. Sun, “Deep Residual Learning f or Image Recognition,” in Proceedings\nof the IEEE conference on computer vision and pattern recogn ition. Seattle, WA, USA: IEEE, Jun.\n2016, pp. 770–778.\n[28] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei, “Ima geNet: A large-scale hierarchical\nimage database,” in 2009 IEEE Conference on Computer Vision and Pattern Recogni tion, Jun. 2009,\npp. 248–255, iSSN: 1063-6919.\n[29] D. Merkx, S. L. Frank, and M. Ernestus, “Language Learning Using Speech to Image Retrieval,” in\nProc. Interspeech 2019 , 2019, pp. 1841–1845.\n[30] G. Chrupa/suppress la, “Symbolic Inductive Bias for Visually Grounded Learning of Spoken Language,” in Pro-\nceedings of the 57th Annual Meeting of the Association for Co mputational Linguistics . Florence, Italy:\nAssociation for Computational Linguistics, Jul. 2019, pp. 6452–64 62.\n[31] B. Higy, D. Elliott, and G. Chrupa/suppress la, “Textual Supervision for Visually Grounded Spoken Language\nUnderstanding,” in Findings of the Association for Computational Linguistics : EMNLP 2020 . Online:\nAssociation for Computational Linguistics, Nov. 2020, pp. 2698–2 709.\n[32] K. Cho, B. van Merrienboer, C. Gulcehre, D. Bahdanau, F. Bou gares, H. Schwenk, and Y. Bengio,\n“Learning Phrase Representations using RNN Encoder–Decoder f or Statistical Machine Translation,”\nin Proceedings of the 2014 Conference on Empirical Methods in N atural Language Processing (EMNLP) .\nDoha, Qatar: Association for Computational Linguistics, 2014, pp . 1724–1734.\n[33] M. VanDam, “HomeBank VanDam Public 5-minute Corpus,” 2015, t ype: dataset. [Online]. Available:\nhttp://homebank.talkbank.org/access/Public/VanDam-5minute.html\n[34] ——, “HomeBank VanDam Public Daylong Corpus,” 2015, type: dat aset. [Online]. Available: http://\nhomebank.talkbank.org/access/Public/VanDam-Daylong.html\n[35] L. Zhou, C. Xu, and J. Corso, “Towards automatic learning of p rocedures from web instructional\nvideos,” in Proceedings of the AAAI Conference on Artiﬁcial Intelligen ce, vol. 32, no. 1, 2018.\n[36] R. Sanabria, O. Caglayan, S. Palaskar, D. Elliott, L. Barrault, L . Specia, and F. Metze, “How2:\na large-scale dataset for multimodal language understanding,” in Proceedings of the Workshop on\nVisually Grounded Interaction and Language (ViGIL) . NeurIPS, 2018. [Online]. Available: http://\narxiv.org/abs/1811.00347\n[37] A. Miech, D. Zhukov, J.-B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic, “HowTo100M: Learning a\nText-Video Embedding by Watching Hundred Million Narrated Video Clips ,” in ICCV, 2019.\n[38] S. Chaudhuri, J. Roth, D. Ellis, A. C. Gallagher, L. Kaver, R. Mar vin, C. Pantofaru, N. C. Reale, L. G.\nReid, K. Wilson, and Z. Xi, “Ava-speech: A densely labeled dataset of speech activity in movies,” in\nProceedings of Interspeech, 2018 , 2018. [Online]. Available: https://arxiv.org/pdf/1808.00606\n11",
  "topic": "Track (disk drive)",
  "concepts": [
    {
      "name": "Track (disk drive)",
      "score": 0.8672651052474976
    },
    {
      "name": "Resource (disambiguation)",
      "score": 0.6737449169158936
    },
    {
      "name": "Zero (linguistics)",
      "score": 0.662886381149292
    },
    {
      "name": "Computer science",
      "score": 0.6414703726768494
    },
    {
      "name": "Grounded theory",
      "score": 0.5380063056945801
    },
    {
      "name": "Baseline (sea)",
      "score": 0.5072855353355408
    },
    {
      "name": "Speech recognition",
      "score": 0.35855352878570557
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3427354395389557
    },
    {
      "name": "Linguistics",
      "score": 0.25993865728378296
    },
    {
      "name": "Sociology",
      "score": 0.11062714457511902
    },
    {
      "name": "Qualitative research",
      "score": 0.09709042310714722
    },
    {
      "name": "Political science",
      "score": 0.09222480654716492
    },
    {
      "name": "Social science",
      "score": 0.0
    },
    {
      "name": "Computer network",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [],
  "cited_by": 9
}