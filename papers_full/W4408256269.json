{
    "title": "Large language models are less effective at clinical prediction tasks than locally trained machine learning models",
    "url": "https://openalex.org/W4408256269",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2125080926",
            "name": "Katherine E. Brown",
            "affiliations": [
                "Vanderbilt University Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2097325630",
            "name": "Chao Yan",
            "affiliations": [
                "Vanderbilt University Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2528449914",
            "name": "Zhuohang Li",
            "affiliations": [
                "Vanderbilt University"
            ]
        },
        {
            "id": "https://openalex.org/A2105481316",
            "name": "Xinmeng Zhang",
            "affiliations": [
                "Vanderbilt University"
            ]
        },
        {
            "id": null,
            "name": "Benjamin X Collins",
            "affiliations": [
                "Vanderbilt University Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A2106573198",
            "name": "You Chen",
            "affiliations": [
                "Vanderbilt University Medical Center",
                "Vanderbilt University"
            ]
        },
        {
            "id": "https://openalex.org/A2553388713",
            "name": "Ellen Wright Clayton",
            "affiliations": [
                "Vanderbilt University",
                "Vanderbilt University Medical Center"
            ]
        },
        {
            "id": "https://openalex.org/A332400322",
            "name": "Murat Kantarcioglu",
            "affiliations": [
                "Virginia Tech"
            ]
        },
        {
            "id": "https://openalex.org/A2305697090",
            "name": "Yevgeniy Vorobeychik",
            "affiliations": [
                "Washington University in St. Louis"
            ]
        },
        {
            "id": "https://openalex.org/A2145238237",
            "name": "Bradley A Malin",
            "affiliations": [
                "Vanderbilt University",
                "Vanderbilt University Medical Center"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3147151141",
        "https://openalex.org/W2800798054",
        "https://openalex.org/W3042750053",
        "https://openalex.org/W3093393153",
        "https://openalex.org/W4387765217",
        "https://openalex.org/W3204940614",
        "https://openalex.org/W2395579298",
        "https://openalex.org/W4294167842",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W4366769280",
        "https://openalex.org/W4394782455",
        "https://openalex.org/W4402786868",
        "https://openalex.org/W4398183427",
        "https://openalex.org/W4392504747",
        "https://openalex.org/W4400724805",
        "https://openalex.org/W4392186538",
        "https://openalex.org/W4396228125",
        "https://openalex.org/W4308554392",
        "https://openalex.org/W4366280062",
        "https://openalex.org/W2162800060",
        "https://openalex.org/W2142262297",
        "https://openalex.org/W2078400939",
        "https://openalex.org/W2047634553",
        "https://openalex.org/W3182079404",
        "https://openalex.org/W2128128412",
        "https://openalex.org/W6693193963",
        "https://openalex.org/W77869065",
        "https://openalex.org/W4390229867",
        "https://openalex.org/W3033410427",
        "https://openalex.org/W2263689172",
        "https://openalex.org/W2559655401",
        "https://openalex.org/W3104597243"
    ],
    "abstract": "Abstract Objectives To determine the extent to which current large language models (LLMs) can serve as substitutes for traditional machine learning (ML) as clinical predictors using data from electronic health records (EHRs), we investigated various factors that can impact their adoption, including overall performance, calibration, fairness, and resilience to privacy protections that reduce data fidelity. Materials and Methods We evaluated GPT-3.5, GPT-4, and traditional ML (as gradient-boosting trees) on clinical prediction tasks in EHR data from Vanderbilt University Medical Center (VUMC) and MIMIC IV. We measured predictive performance with area under the receiver operating characteristic (AUROC) and model calibration using Brier Score. To evaluate the impact of data privacy protections, we assessed AUROC when demographic variables are generalized. We evaluated algorithmic fairness using equalized odds and statistical parity across race, sex, and age of patients. We also considered the impact of using in-context learning by incorporating labeled examples within the prompt. Results Traditional ML [AUROC: 0.847, 0.894 (VUMC, MIMIC)] substantially outperformed GPT-3.5 (AUROC: 0.537, 0.517) and GPT-4 (AUROC: 0.629, 0.602) (with and without in-context learning) in predictive performance and output probability calibration [Brier Score (ML vs GPT-3.5 vs GPT-4): 0.134 vs 0.384 vs 0.251, 0.042 vs 0.06 vs 0.219)]. Discussion Traditional ML is more robust than GPT-3.5 and GPT-4 in generalizing demographic information to protect privacy. GPT-4 is the fairest model according to our selected metrics but at the cost of poor model performance. Conclusion These findings suggest that non-fine-tuned LLMs are less effective and robust than locally trained ML for clinical prediction tasks, but they are improving across releases.",
    "full_text": null
}