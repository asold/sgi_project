{
    "title": "Table Meets LLM: Can Large Language Models Understand Structured Table Data? A Benchmark and Empirical Study",
    "url": "https://openalex.org/W4392366650",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2144219764",
            "name": "Yuan Sui",
            "affiliations": [
                "National University of Singapore"
            ]
        },
        {
            "id": "https://openalex.org/A2129235839",
            "name": "Mengyu Zhou",
            "affiliations": [
                "Microsoft Research Asia (China)",
                "University of Hong Kong",
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2072390108",
            "name": "Mingjie Zhou",
            "affiliations": [
                "Microsoft Research Asia (China)",
                "University of Hong Kong",
                "Chinese University of Hong Kong"
            ]
        },
        {
            "id": "https://openalex.org/A2105084222",
            "name": "Shi Han",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2097661864",
            "name": "Dongmei Zhang",
            "affiliations": [
                "Microsoft Research Asia (China)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4377372363",
        "https://openalex.org/W4287121227",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4306295150",
        "https://openalex.org/W2971822538",
        "https://openalex.org/W3101082165",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W7093349750",
        "https://openalex.org/W4306291596",
        "https://openalex.org/W4286981686",
        "https://openalex.org/W4387800835",
        "https://openalex.org/W3116342879",
        "https://openalex.org/W4287119949",
        "https://openalex.org/W2612228435",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W4286903249",
        "https://openalex.org/W3184222203",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W4287810917",
        "https://openalex.org/W4226146865",
        "https://openalex.org/W4308828220",
        "https://openalex.org/W4287667694",
        "https://openalex.org/W4221009220",
        "https://openalex.org/W3165753548",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4221166833",
        "https://openalex.org/W3035231859",
        "https://openalex.org/W3004210300",
        "https://openalex.org/W4323927478",
        "https://openalex.org/W4212774754",
        "https://openalex.org/W3168052339",
        "https://openalex.org/W2612690371",
        "https://openalex.org/W4312554912"
    ],
    "abstract": "Large language models (LLMs) are becoming attractive as few-shot reasoners to solve Natural Language (NL)-related tasks. However, there is still much to learn about how well LLMs understand structured data, such as tables. Although tables can be used as input to LLMs with serialization, there is a lack of comprehensive studies that examine whether LLMs can truly comprehend such data. In this paper, we try to understand this by designing a benchmark to evaluate the structural understanding capabilities (SUC) of LLMs. The benchmark we create includes seven tasks, each with its own unique challenges, \\eg, cell lookup, row retrieval, and size detection. We perform a series of evaluations on GPT-3.5 and GPT-4. We find that performance varied depending on several input choices, including table input format, content order, role prompting, and partition marks. Drawing from the insights gained through the benchmark evaluations, we proposeself-augmentation for effective structural prompting, such as critical value / range identification using internal knowledge of LLMs. When combined with carefully chosen input choices, these structural prompting methods lead to promising improvements in LLM performance on a variety of tabular tasks, \\eg, TabFact(\\uparrow2.31%), HybridQA(\\uparrow2.13%), SQA(\\uparrow2.72%), Feverous(\\uparrow0.84%), and ToTTo(\\uparrow5.68%). We believe that our open-source (please find code and data at https://github.com/microsoft/TableProvider) benchmark and proposed prompting methods can serve as a simple yet generic selection for future research.",
    "full_text": null
}