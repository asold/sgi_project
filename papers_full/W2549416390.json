{
  "title": "Tying Word Vectors and Word Classifiers: A Loss Framework for Language Modeling",
  "url": "https://openalex.org/W2549416390",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2745369803",
      "name": "Inan, Hakan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226630972",
      "name": "Khosravi, Khashayar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176399732",
      "name": "Socher, Richard",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W1843891098",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W1632114991",
    "https://openalex.org/W2172140247",
    "https://openalex.org/W2951798229",
    "https://openalex.org/W2950133940",
    "https://openalex.org/W2473934411",
    "https://openalex.org/W2525332836",
    "https://openalex.org/W2229833550",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W1591801644",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2212703438",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2514713644"
  ],
  "abstract": "Recurrent neural networks have been very successful at predicting sequences of words in tasks such as language modeling. However, all such models are based on the conventional classification framework, where the model is trained against one-hot targets, and each word is represented both as an input and as an output in isolation. This causes inefficiencies in learning both in terms of utilizing all of the information and in terms of the number of parameters needed to train. We introduce a novel theoretical framework that facilitates better learning in language modeling, and show that our framework leads to tying together the input embedding and the output projection matrices, greatly reducing the number of trainable variables. Our framework leads to state of the art performance on the Penn Treebank with a variety of network models.",
  "full_text": "arXiv:1611.01462v3  [cs.LG]  11 Mar 2017\nPublished as a conference paper at ICLR 2017\nTY I N G WO R D VE C TO R S A N D WO R D CL A S S I FIE R S :\nA L O S S FR A M E WO RK F O R LA N G UAG E MO D E L I N G\nHakan Inan, Khashayar Khosravi\nStanford University\nStanford, CA, USA\n{inanh,khosravi}@stanford.edu\nRichard Socher\nSalesforce Research\nPalo Alto, CA, USA\nrsocher@salesforce.com\nABSTRACT\nRecurrent neural networks have been very successful at pred icting sequences of\nwords in tasks such as language modeling. However, all such m odels are based\non the conventional classiﬁcation framework, where the mod el is trained against\none-hot targets, and each word is represented both as an inpu t and as an output\nin isolation. This causes inefﬁciencies in learning both in terms of utilizing all\nof the information and in terms of the number of parameters ne eded to train. W e\nintroduce a novel theoretical framework that facilitates b etter learning in language\nmodeling, and show that our framework leads to tying togethe r the input embed-\nding and the output projection matrices, greatly reducing t he number of trainable\nvariables. Our framework leads to state of the art performan ce on the Penn Tree-\nbank with a variety of network models.\n1 I NTRODUC TI ON\nNeural network models have recently made tremendous progre ss in a variety of NLP applications\nsuch as speech recognition (\nIrie et al. , 2016), sentiment analysis ( Socher et al. , 2013), text summa-\nrization ( Rush et al. , 2015; Nallapati et al. , 2016), and machine translation ( Firat et al. , 2016).\nDespite the overwhelming success achieved by recurrent neu ral networks in modeling long range de-\npendencies between words, current recurrent neural networ k language models (RNNLM) are based\non the conventional classiﬁcation framework, which has two major drawbacks: First, there is no as-\nsumed metric on the output classes, whereas there is evidenc e suggesting that learning is improved\nwhen one can deﬁne a natural metric on the output space ( Frogner et al. , 2015). In language model-\ning, there is a well established metric space for the outputs (words in the language) based on word\nembeddings, with meaningful distances between words ( Mikolov et al. , 2013; Pennington et al. ,\n2014). Second, in the classical framework, inputs and outputs ar e considered as isolated entities\nwith no semantic link between them. This is clearly not the ca se for language modeling, where\ninputs and outputs in fact live in identical spaces. Therefo re, even for models with moderately sized\nvocabularies, the classical framework could be a vast sourc e of inefﬁciency in terms of the number\nof variables in the model, and in terms of utilizing the infor mation gathered by different parts of the\nmodel (e.g. inputs and outputs).\nIn this work, we introduce a novel loss framework for languag e modeling to remedy the above two\nproblems. Our framework is comprised of two closely linked i mprovements. First, we augment the\nclassical cross-entropy loss with an additional term which minimizes the KL-divergence between\nthe model’s prediction and an estimated target distributio n based on the word embeddings space.\nThis estimated distribution uses knowledge of word vector s imilarity. W e then theoretically analyze\nthis loss, and this leads to a second and synergistic improve ment: tying together two large matrices\nby reusing the input word embedding matrix as the output clas siﬁcation matrix. W e empirically\nvalidate our theory in a practical setting, with much milder assumptions than those in theory. W e\nalso ﬁnd empirically that for large networks, most of the imp rovement could be achieved by only\nreusing the word embeddings.\nW e test our framework by performing extensive experiments o n the Penn Treebank corpus, a dataset\nwidely used for benchmarking language models (\nMikolov et al. , 2010; Merity et al. , 2016). W e\ndemonstrate that models trained using our proposed framewo rk signiﬁcantly outperform models\n1\nPublished as a conference paper at ICLR 2017\ntrained using the conventional framework. W e also perform e xperiments on the newly introduced\nWikitext-2 dataset ( Merity et al. , 2016), and verify that the empirical performance of our proposed\nframework is consistent across different datasets.\n2 B ACKGROUN D : R ECURR EN T NEURAL NETWORK LANGUAGE MODEL\nIn any variant of recurrent neural network language model (R NNLM), the goal is to predict the next\nword indexed by t in a sequence of one-hot word tokens (y∗\n1 , . . . y ∗\nN ) as follows:\nxt = Ly∗\nt− 1, (2.1)\nht = f(xt, h t− 1), (2.2)\nyt = softmax (W ht + b) . (2.3)\nThe matrix L ∈ Rdx×| V | is the word embedding matrix, where dx is the word embedding dimension\nand |V | is the size of the vocabulary. The function f(., . ) represents the recurrent neural network\nwhich takes in the current input and the previous hidden stat e and produces the next hidden state.\nW ∈ R|V |× dh and b ∈ R|V | are the the output projection matrix and the bias, respectiv ely, and dh is\nthe size of the RNN hidden state. The |V | dimensional yt models the discrete probability distribution\nfor the next word.\nNote that the above formulation does not make any assumption s about the speciﬁcs of the recurrent\nneural units, and f could be replaced with a standard recurrent unit, a gated rec urrent unit (GRU)\n(\nCho et al. , 2014), a long-short term memory (LSTM) unit ( Hochreiter & Schmidhuber , 1997), etc.\nFor our experiments, we use LSTM units with two layers.\nGiven yt for the tth example, a loss is calculated for that example. The loss used in the RNNLMs is\nalmost exclusively the cross-entropy between yt and the observed one-hot word token, y∗\nt :\nJt = CE(y∗\nt ∥ yt) =−\n∑\ni∈|V |\ny∗\nt,i log yt,i. (2.4)\nW e shall refer to yt as the model prediction distribution for the tth example, and y∗\nt as the empir-\nical target distribution (both are in fact conditional dist ributions given the history). Since cross-\nentropy and Kullback-Leibler divergence are equivalent when the target distribution is o ne-hot, we\ncan rewrite the loss for the tth example as\nJt = DKL (y∗\nt ∥ yt). (2.5)\nTherefore, we can think of the optimization of the conventio nal loss in an RNNLM as trying to mini-\nmize the distance\n1 between the model prediction distribution ( y) and the empirical target distribution\n(y∗ ), which, with many training examples, will get close to mini mizing distance to the actual tar-\nget distribution. In the framework which we will introduce, we utilize Kullback-Leibler divergence\nas opposed to cross-entropy due to its intuitive interpreta tion as a distance between distributions,\nalthough the two are not equivalent in our framework.\n3 A UGMENTING THE CROSS -E N T RO PY LOSS\nW e propose to augment the conventional cross-entropy loss w ith an additional loss term as follows:\nˆyt = softmax (W ht/τ ) , (3.1)\nJaug\nt = DKL (˜yt ∥ ˆyt), (3.2)\nJtot\nt = Jt + αJ aug\nt . (3.3)\nIn above, α is a hyperparameter to be adjusted, and ˆyt is almost identical to the regular model\nprediction distribution yt with the exception that the logits are divided by a temperatu re parameter τ.\nW e deﬁne ˜yt as some probability distribution that estimates the true da ta distribution (conditioned\non the word history) which satisﬁes E˜yt = Ey∗\nt . The goal of this framework is to minimize the\n1 W e note, however, that Kullback-Leibler divergence is not a valid distance metric.\n2\nPublished as a conference paper at ICLR 2017\ndistribution distance between the prediction distributio n and a more accurate estimate of the true\ndata distribution.\nT o understand the effect of optimizing in this setting, let’ s focus on an ideal case in which we are\ngiven the true data distribution so that ˜yt = Ey∗\nt , and we only use the augmented loss, Jaug . W e will\ncarry out our investigation through stochastic gradient de scent, which is the technique dominantly\nused for training neural networks. The gradient of Jaug\nt with respect to the logits W ht is\n∇ Jaug\nt = 1\nτ (ˆyt − ˜yt). (3.4)\nLet’s denote by ej ∈ R|V | the vector whose jth entry is 1, and others are zero. W e can then rewrite\n(3.4) as\nτ ∇ Jaug\nt = ˆyt −\n[\ne1, . . . , e |V |\n]\n˜yt =\n∑\ni∈ V\n˜yt,i(ˆyt − ei). (3.5)\nImplication of ( 3.5) is the following: Every time the optimizer sees one trainin g example, it takes a\nstep not only on account of the label seen, but it proceeds tak ing into account all the class labels for\nwhich the conditional probability is not zero, and the relat ive step size for each step is given by the\nconditional probability for that label, ˜yt,i. Furthermore, this is a much less noisy update since the\ntarget distribution is exact and deterministic. Therefore , unless all the examples exclusively belong\nto a speciﬁc class with probability 1, the optimization will act much differently and train with g reatly\nimproved supervision.\nThe idea proposed in the recent work by\nHinton et al. (2015) might be considered as an application\nof this framework, where they try to obtain a good set of ˜y’s by training very large models and using\nthe model prediction distributions of those.\nAlthough ﬁnding a good ˜y in general is rather nontrivial, in the context of language m odeling we\ncan hope to achieve this by exploiting the inherent metric sp ace of classes encoded into the model,\nnamely the space of word embeddings. Speciﬁcally, we propos e the following for ˜y:\nut = Ly∗\nt , (3.6)\n˜yt = softmax\n( LT ut\nτ\n)\n. (3.7)\nIn words, we ﬁrst ﬁnd the target word vector which correspond s to the target word token (resulting\nin ut), and then take the inner product of the target word vector wi th all the other word vectors to\nget an unnormalized probability distribution. W e adjust th is with the same temperature parameter\nτ used for obtaining ˆyt and apply softmax. The target distribution estimate, ˜y, therefore measures\nthe similarity between the word vectors and assigns similar probability masses to words that the\nlanguage model deems close. Note that the estimation of ˜y with this procedure is iterative, and\nthe estimates of ˜y in the initial phase of the training are not necessarily info rmative. However, as\ntraining procedes, we expect ˜y to capture the word statistics better and yield a consistent ly more\naccurate estimate of the true data distribution.\n4 T HEORETIC A LLY DRIVEN REUSE OF WORD EMBEDDING S\nW e now theoretically motivate and introduce a second modiﬁc ation to improve learning in the lan-\nguage model. W e do this by analyzing the proposed augmented l oss in a particular setting, and\nobserve an implicit core mechanism of this loss. W e then make our proposition by making this\nmechanism explicit.\nW e start by introducing our setting for the analysis. W e rest rict our attention to the case where the\ninput embedding dimension is equal to the dimension of the RN N hidden state, i.e. d ≜ dx = dh.\nW e also set b = 0in (\n2.3) so that yt = W ht. W e only use the augmented loss, i.e. Jtot = Jaug , and\nwe assume that we can achieve zero training loss. Finally, we set the temperature parameter τ to be\nlarge.\nW e ﬁrst show that when the temperature parameter, τ, is high enough, Jaug\nt acts to match the logits of\nthe prediction distribution to the logits of the the more inf ormative labels, ˜y. W e proceed in the same\n3\nPublished as a conference paper at ICLR 2017\nway as was done in Hinton et al. (2015) to make an identical argument. Particularly, we consider\nthe derivative of Jaug\nt with respect to the entries of the logits produced by the neur al network.\nLet’s denote by li the ith column of L. Using the ﬁrst order approximation of exponenti al function\naround zero ( exp (x) ≈ 1 +x), we can approximate ˜yt (same holds for ˆyt) at high temperatures as\nfollows:\n˜yt,i = exp (⟨ut, l i⟩/τ )∑\nj∈ V exp (⟨ut, l j ⟩/τ ) ≈ 1 +⟨ut, l i⟩/τ\n|V | + ∑\nj∈ V ⟨ut, l j ⟩/τ . (4.1)\nW e can further simplify ( 4.1) if we assume that ⟨ut, l j ⟩ = 0on average:\n˜yt,i ≈ 1 +⟨ut, l i⟩/τ\n|V | . (4.2)\nBy replacing ˜yt and ˆyt in ( 3.4) with their simpliﬁed forms according to ( 4.2), we get\n∂J aug\nt\n∂ (W ht)i\n→ 1\nτ2|V |\n(\nW ht − LT ut\n)\ni as τ → ∞ , (4.3)\nwhich is the desired result that augmented loss tries to matc h the logits of the model to the logits of\n˜y’s. Since the training loss is zero by assumption, we necessa rily have\nW ht = LT ut (4.4)\nfor each training example, i.e., gradient contributed by ea ch example is zero. Provided that W and L\nare full rank matrices and there are more linearly independe nt examples of ht’s than the embedding\ndimension d, we get that the space spanned by the columns of LT is equivalent to that spanned by\nthe columns of W . Let’s now introduce a square matrix A such that W = LT A. (W e know A exists\nsince LT and W span the same column space). In this case, we can rewrite\nW ht = LT Aht ≜ LT ˜\nht. (4.5)\nIn other words, by reusing the embedding matrix in the output projection layer (with a transpose)\nand letting the neural network do the necessary linear mappi ng h → Ah, we get the same result as\nwe would have in the ﬁrst place.\nAlthough the above scenario could be difﬁcult to exactly rep licate in practice, it uncovers a mech-\nanism through which our proposed loss augmentation acts, wh ich is trying to constrain the output\n(unnormalized) probability space to a small subspace gover ned by the embedding matrix. This sug-\ngests that we can make this mechanism explicit and constrain W = LT during training while setting\nthe output bias, b, to zero. Doing so would not only eliminate a big matrix which dominates the\nnetwork size for models with even moderately sized vocabula ries, but it would also be optimal in\nour setting of loss augmentation as it would eliminate much w ork to be done by the augmented loss.\n5 R ELATED WORK\nSince their introduction in\nMikolov et al. (2010), many improvements have been proposed for\nRNNLMs , including different dropout methods ( Zaremba et al. , 2014; Gal, 2015), novel recurrent\nunits ( Zilly et al. , 2016), and use of pointer networks to complement the recurrent ne ural network\n(Merity et al. , 2016). However, none of the improvements dealt with the loss stru cture, and to the\nbest of our knowledge, our work is the ﬁrst to offer a new loss f ramework.\nOur technique is closely related to the one in Hinton et al. (2015), where they also try to estimate\na more informed data distribution and augment the conventio nal loss with KL divergence between\nmodel prediction distribution and the estimated data distr ibution. However, they estimate their data\ndistribution by training large networks on the data and then use it to improve learning in smaller\nnetworks. This is fundamentally different from our approac h, where we improve learning by trans-\nferring knowledge between different parts of the same netwo rk, in a self contained manner.\nThe work we present in this paper is based on a report which was made public in Inan & Khosravi\n(2016). W e have recently come across a concurrent preprint ( Press & W olf , 2016) where the authors\nreuse the word embedding matrix in the output projection to i mprove language modeling. How-\never, their work is purely empirical, and they do not provide any theoretical justiﬁcation for their\n4\nPublished as a conference paper at ICLR 2017\napproach. Finally, we would like to note that the idea of usin g the same representation for input\nand output words has been explored in the past, and there exis ts language models which could be\ninterpreted as simple neural networks with shared input and output embeddings ( Bengio et al. , 2001;\nMnih & Hinton , 2007). However, shared input and output representations were im plicitly built into\nthese models, rather than proposed as a supplement to a basel ine. Consequently, possibility of im-\nprovement was not particularly pursued by sharing input and output representations.\n6 E XPERIME NT S\nIn our experiments, we use the Penn Treebank corpus (PTB) ( Marcus et al. , 1993), and the Wikitext-\n2 dataset ( Merity et al. , 2016). PTB has been a standard dataset used for benchmarking lang uage\nmodels. It consists of 923k training, 73k validation, and 82k test words. The version of this dataset\nwhich we use is the one processed in Mikolov et al. (2010), with the most frequent 10k words\nselected to be in the vocabulary and rest replaced with a an <unk> token 2 . Wikitext-2 is a dataset\nreleased recently as an alternative to PTB 3 . It contains 2, 088k training, 217k validation, and 245k\ntest tokens, and has a vocabulary of 33, 278 words; therefore, in comparison to PTB, it is roughly 2\ntimes larger in dataset size, and 3 times larger in vocabular y.\n6.1 M O D E L A N D TRA IN IN G HIG H L IG H T S\nW e closely follow the LSTM based language model proposed in Zaremba et al. (2014) for construct-\ning our baseline model. Speciﬁcally, we use a 2-layer LSTM with the same number of hidden units\nin each layer, and we use 3 different network sizes: small ( 200 units), medium ( 650 units), and\nlarge ( 1500 units). W e train our models using stochastic gradient desce nt, and we use a variant of\nthe dropout method proposed in Gal (2015). W e defer further details regarding training the models\nto section A of the appendix. W e refer to our baseline network as variatio nal dropout LSTM, or\nVD-LSTM in short.\n6.2 E M P IRICA L VA L IDAT IO N F O R T H E TH E O RY O F RE U S IN G WO RD EM BE D D IN G S\nIn Section 4, we showed that the particular loss augmentation scheme we c hoose constrains the\noutput projection matrix to be close to the input embedding m atrix, without explicitly doing so by\nreusing the input embedding matrix. As a ﬁrst experiment, we set out to validate this theoretical\nresult. T o do this, we try to simulate the setting in Section 4 by doing the following: W e select\na randomly chosen 20, 000 contiguous word sequence in the PTB training set, and train a 2-layer\nLSTM language model with 300 units in each layer with loss aug mentation by minimizing the\nfollowing loss:\nJtot = βJ aug τ2|V | + (1− β)J. (6.1)\nHere, β is the proportion of the augmented loss used in the total loss , and Jaug is scaled by τ2|V |\nto approximately match the magnitudes of the derivatives of J and Jaug (see ( 4.3)). Since we aim\nto achieve the minimum training loss possible, and the goal i s to show a particular result rather\nthan to achieve good generalization, we do not use any kind of regularization in the neural network\n(e.g. weight decay, dropout). For this set of experiments, w e also constrain each row of the input\nembedding matrix to have a norm of 1 because training becomes difﬁcult without this constraint\nwhen only augmented loss is used. After training, we compute a metric that measures distance\nbetween the subspace spanned by the rows of the input embeddi ng matrix, L, and that spanned by\nthe columns of the output projection matrix, W . For this, we use a common metric based on the\nrelative residual norm from projection of one matrix onto an other ( Bj ¨ orck & Golub, 1973). The\ncomputed distance between the subspaces is 1 when they are orthogonal, and 0 when they are the\nsame. Interested reader may refer to section B in the appendix for the details of this metric.\nFigure 1 shows the results from two tests. In one (panel a), we test the effect of using the augmented\nloss by sweeping β in ( 6.1) from 0 to 1 at a reasonably high temperature ( τ = 10). With no loss\naugmentation ( β = 0), the distance is almost 1, and as more and more augmented loss is used the\n2 PTB can be downloaded at http://www .ﬁt.vutbr.cz/ imikolov/rnnlm/simple-exampl es.tgz\n3 Wikitext-2 can be downloaded at https://s3.amazonaws.com/research.metamind.io/wikitext/wikitext-2-v1.zip\n5\nPublished as a conference paper at ICLR 2017\n0.0 0.2 0.4 0.6 0.8 1.0\nProportion of Augmented Loss(β)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Normalized distance\n(a) Subspace distance at τ = 10for\ndifferent proportions of Jaug\n0 5 10 15 20 25 30 35 40\nTemperature(τ)\n0. 06\n0. 08\n0. 10\n0. 12\n0. 14\n0. 16\n0. 18\n0. 20\nNormalized distance\n(b) Subspace distance at different temp-\neratures when only Jaug is used\nFigure 1: Subspace distance between LT and W for different experiment conditions for the validation ex-\nperiments. Results are averaged over 10 independent runs. These results validate our theory under p ractical\nconditions.\ndistance decreases rapidly, and eventually reaches around 0. 06 when only augmented loss is used.\nIn the second test (panel b), we set β = 1, and try to see the effect of the temperature on the subspace\ndistance (remember the theory predicts low distance when τ → ∞ ). Notably, the augmented loss\ncauses W to approach LT sufﬁciently even at temperatures as low as 2, although higher temperatures\nstill lead to smaller subspace distances.\nThese results conﬁrm the mechanism through which our propos ed loss pushes W to learn the same\ncolumn space as LT , and it suggests that reusing the input embedding matrix by e xplicitly con-\nstraining W = LT is not simply a kind of regularization, but is in fact an optim al choice in our\nframework. What can be achieved separately with each of the t wo proposed improvements as well\nas with the two of them combined is a question of empirical nat ure, which we investigate in the next\nsection.\n6.3 R E S U LT S O N PTB A N D WIK IT E X T -2 D ATA S E T S\nIn order to investigate the extent to which each of our propos ed improvements helps with learning,\nwe train 4 different models for each network size: (1) 2-Laye r LSTM with variational dropout\n(VD-LSTM) (2) 2-Layer LSTM with variational dropout and aug mented loss (VD-LSTM +AL)\n(3) 2-Layer LSTM with variational dropout and reused embedd ings (VD-LSTM +RE) (4) 2-Layer\nLSTM with variational dropout and both RE and AL (VD-LSTM +RE AL).\nFigure\n2 shows the validation perplexities of the four models during training on the PTB corpus\nfor small (panel a) and large (panel b) networks. All of AL, RE , and REAL networks signiﬁcantly\noutperform the baseline in both cases. T able 1 compares the ﬁnal validation and test perplexities\nof the four models on both PTB and Wikitext-2 for each network size. In both datasets, both AL\nand RE improve upon the baseline individually, and using RE a nd AL together leads to the best\nperformance. Based on performance comparisons, we make the following notes on the two proposed\nimprovements:\n• AL provides better performance gains for smaller networks. This is not surprising given the\nfact that small models are rather inﬂexible, and one would ex pect to see improved learn-\ning by training against a more informative data distributio n (contributed by the augmented\nloss) (see Hinton et al. (2015)). For the smaller PTB dataset, performance with AL sur-\npasses that with RE. In comparison, for the larger Wikitext- 2 dataset, improvement by AL\nis more limited. This is expected given larger training sets better represent the true data\ndistribution, mitigating the supervision problem. In fact , we set out to validate this rea-\nsoning in a direct manner, and additionally train the small n etworks separately on the ﬁrst\nand second halves of the Wikitext-2 training set. This resul ts in two distinct datasets which\nare each about the same size as PTB (1044K vs 929K). As can be se en in T able 2, AL\n6\nPublished as a conference paper at ICLR 2017\n20 30 40 50 6010\nEpoch (N)\n85\n90\n95\n100\n105\n110V alidation Perplexity\n+REAL\n+RE\n+AL\nBaseline\n(a) Small Network\n15 30 45 60 7510\nEpoch (N)\n70\n75\n80\n85\n90\n95\n100V alidation Perplexity\n+REAL\n+RE\n+AL\nBaseline\n(b) Large Network\nFigure 2: Progress of validation perplexities during train ing for the 4 different models for two (small ( 200) and\nlarge ( 1500)) network sizes.\nhas signiﬁcantly improved competitive performance agains t RE and REAL despite the fact\nthat embedding size is 3 times larger compared to PTB. These r esults support our argument\nthat the proposed augmented loss term acts to improve the amo unt of information gathered\nfrom the dataset.\n• RE signiﬁcantly outperforms AL for larger networks. This in dicates that, for large models,\nthe more effective mechanism of our proposed framework is th e one which enforces prox-\nimity between the output projection space and the input embe dding space. From a model\ncomplexity perspective, the nontrivial gains offered by RE for all network sizes and for\nboth datasets could be largely attributed to its explicit fu nction to reduce the model size\nwhile preserving the representational power according to o ur framework.\nW e list in T able 3 the comparison of models with and without our proposed modiﬁ cations on\nthe Penn Treebank Corpus. The best LSTM model (VD-LSTM+REAL ) outperforms all previous\nwork which uses conventional framework, including large en sembles. The recently proposed recur-\nrent highway networks ( Zilly et al. , 2016) when trained with reused embeddings (VD-RHN +RE)\nachieves the best overall performance, improving on VD-RHN by a perplexity of 2. 5.\nT able 1: Comparison of the ﬁnal word level perplexities on th e validation and test set for the 4 different models.\nPTB Wikitext-2\nNetwork Model V alid T est V alid T est\nSmall4\n(200\nunits)\nVD-LSTM 92.6 87.3 112.2 105.9\nVD-LSTM+AL 86.3 82.9 110.3 103.8\nVD-LSTM+RE 89.9 85.1 106.1 100.5\nVD-LSTM+REAL 86.3 82.7 105.6 98.9\nMedium\n(650\nunits)\nVD-LSTM 82.0 77.7 100.2 95.3\nVD-LSTM+AL 77.4 74.7 98.8 93.1\nVD-LSTM+RE 77.1 73.9 92.3 87.7\nVD-LSTM+REAL 75.7 73.2 91.5 87.0\nLarge5\n(1500\nunits)\nVD-LSTM 76.8 72.6 - -\nVD-LSTM+AL 74.5 71.2 - -\nVD-LSTM+RE 72.5 69.0 - -\nVD-LSTM+REAL 71.1 68.5 - -\n4 For PTB, small models were re-trained by initializing to the ir ﬁnal conﬁguration from the ﬁrst training\nsession. This did not change the ﬁnal perplexity for baselin e, but lead to improvements for the other models.\n5 Large network results on Wikitext-2 are not reported since c omputational resources were insufﬁcient to\nrun some of the conﬁgurations.\n7\nPublished as a conference paper at ICLR 2017\nT able 2: Performance of the four different small models trai ned on the equally sized two partitions of Wikitext-\n2 training set. These results are consistent with those on PT B (see T able 1), which has a similar training set size\nwith each of these partitions, although its word embedding d imension is three times smaller.\nWikitext-2, Partition 1 Wikitext-2, Partition 2\nNetwork Model V alid T est V alid T est\nSmall\n(200\nunits)\nVD-LSTM 159.1 148.0 163.19 148.6\nVD-LSTM+AL 153.0 142.5 156.4 143.7\nVD-LSTM+RE 152.4 141.9 152.5 140.9\nVD-LSTM+REAL 149.3 140.6 150.5 138.4\nT able 3: Comparison of our work to previous state of the art on word-level validation and test perplexities on\nthe Penn Treebank corpus. Models using our framework signiﬁ cantly outperform other models.\nModel Parameters V alidation T est\nRNN ( Mikolov & Zweig ) 6M - 124.7\nRNN+LDA ( Mikolov & Zweig ) 7M - 113.7\nRNN+LDA+KN-5+Cache ( Mikolov & Zweig ) 9M - 92.0\nDeep RNN ( Pascanu et al. , 2013a) 6M - 107.5\nSum-Prod Net ( Cheng et al. , 2014) 5M - 100.0\nLSTM (medium) ( Zaremba et al. , 2014) 20M 86.2 82.7\nCharCNN ( Kim et al. , 2015) 19M - 78.9\nLSTM (large) ( Zaremba et al. , 2014) 66M 82.2 78.4\nVD-LSTM (large, untied, MC) ( Gal, 2015) 66M - 73. 4 ± 0. 0\nPointer Sentinel-LSTM(medium) ( Merity et al. , 2016) 21M 72.4 70.9\n38 Large LSTMs ( Zaremba et al. , 2014) 2.51B 71.9 68.7\n10 Large VD-LSTMs ( Gal, 2015) 660M - 68.7\nVD-RHN ( Zilly et al. , 2016) 32M 71.2 68.5\nVD-LSTM +REAL (large) 51M 71.1 68.5\nVD-RHN +RE ( Zilly et al. , 2016) 6 24M 68.1 66.0\n6.4 Q UA L ITAT IV E RE S U LT S\nOne important feature of our framework that leads to better w ord predictions is the explicit mecha-\nnism to assign probabilities to words not merely according t o the observed output statistics, but also\nconsidering the metric similarity between words. W e observ e direct consequences of this mecha-\nnism qualitatively in the Penn Treebank in different ways: F irst, we notice that the probability of\ngenerating the <unk> token with our proposed network (VD-LSTM +REAL) is signiﬁca ntly lower\ncompared to the baseline network (VD-LSTM) across many word s. This could be explained by\nnoting the fact that the <unk> token is an aggregated token rather than a speciﬁc word, and i t is\noften not expected to be close to speciﬁc words in the word emb edding space. W e observe the same\nbehavior with very frequent words such as ”a”, ”an”, and ”the ”, owing to the same fact that they are\nnot correlated with particular words. Second, we not only ob serve better probability assignments for\nthe target words, but we also observe relatively higher prob ability weights associated with the words\nclose to the targets. Sometimes this happens in the form of pr edicting words semantically close to-\ngether which are plausible even when the target word is not su ccessfully captured by the model. W e\nprovide a few examples from the PTB test set which compare the prediction performance of 1500\nunit VD-LSTM and 1500 unit VD-LSTM +REAL in table 4. W e would like to note that prediction\nperformance of VD-LSTM +RE is similar to VD-LSTM +REAL for th e large network.\n6 This model was developed following our work in Inan & Khosravi (2016).\n8\nPublished as a conference paper at ICLR 2017\nT able 4: Prediction for the next word by the baseline (VD-LST M) and proposed (VD-LSTM +REAL) networks\nfor a few example phrases in the PTB test set. T op 10 word predictions are sorted in descending probability ,\nand are arranged in column-major format.\nPhrase + Next word(s) T op 10 predicted words\nVD-LSTM\nT op 10 predicted words\nVD-LSTM +REAL\ninformation international\nsaid it believes that the\ncomplaints ﬁled in\n+ federal court\nthe 0.27 an 0.03\na 0.13 august 0.01\nfederal 0.13 new 0.01\nN 0.09 response 0.01\n⟨unk⟩ 0.05 connection 0.01\nfederal 0.22 connection 0.03\nthe 0.1 august 0.03\na 0.08 july 0.03\nN 0.06 an 0.03\nstate 0.04 september 0.03\noil company reﬁneries\nran ﬂat out to prepare\nfor a robust holiday\ndriving season in july and\n+ august\nthe 0.09 in 0.03\nN 0.08 has 0.03\na 0.07 is 0.02\n⟨unk⟩ 0.07 will 0.02\nwas 0.04 its 0.02\naugust 0.08 a 0.03\nN 0.05 in 0.03\nearly 0.05 that 0.02\nseptember 0.05 ended 0.02\nthe 0.03 its 0.02\nsouthmark said it plans\nto ⟨unk⟩ its ⟨unk⟩ to\nprovide ﬁnancial results\nas soon as its audit is\n+ completed\nthe 0.06 to 0.03\n⟨unk⟩ 0.05 likely 0.03\na 0.05 expected 0.03\nin 0.04 scheduled 0.01\nn’t 0.04 completed 0.01\nexpected 0.1 a 0.03\ncompleted 0.04 scheduled 0.03\n⟨unk⟩ 0.03 n’t 0.03\nthe 0.03 due 0.02\nin 0.03 to 0.01\nmerieux said the\ngovernment ’s minister\nof industry science and\n+ technology\n⟨unk⟩ 0.33 industry 0.01\nthe 0.06 commerce 0.01\na 0.01 planning 0.01\nother 0.01 management 0.01\nothers 0.01 mail 0.01\n⟨unk⟩ 0.09 industry 0.03\nhealth 0.08 business 0.02\ndevelopment 0.04 telecomm. 0.02\nthe 0.04 human 0.02\na 0.03 other 0.01\n7 C ONCLUSIO N\nIn this work, we introduced a novel loss framework for langua ge modeling. Particularly, we showed\nthat the metric encoded into the space of word embeddings cou ld be used to generate a more in-\nformed data distribution than the one-hot targets, and that additionally training against this distri-\nbution improves learning. W e also showed theoretically tha t this approach lends itself to a second\nimprovement, which is simply reusing the input embedding ma trix in the output projection layer.\nThis has an additional beneﬁt of reducing the number of train able variables in the model. W e empir-\nically validated the theoretical link, and veriﬁed that bot h proposed changes do in fact belong to the\nsame framework. In our experiments on the Penn Treebank corp us and Wikitext-2, we showed that\nour framework outperforms the conventional one, and that ev en the simple modiﬁcation of reusing\nthe word embedding in the output projection layer is sufﬁcie nt for large networks.\nThe improvements achieved by our framework are not unique to vanilla language modeling, and are\nreadily applicable to other tasks which utilize language mo dels such as neural machine translation,\nspeech recognition, and text summarization. This could lea d to signiﬁcant improvements in such\nmodels especially with large vocabularies, with the additi onal beneﬁt of greatly reducing the number\nof parameters to be trained.\n9\nPublished as a conference paper at ICLR 2017\nREFERENC ES\nY oshua Bengio, R´ ejean Ducharme, and Pascal V incent. A neur al probabilistic language model.\n2001. URL http://www.iro.umontreal.ca/˜lisa/pointeurs/nips00_lm.ps.\n◦\nAke Bj ¨ orck and Gene H Golub. Numerical methods for computin g angles between linear subspaces.\nMathematics of computation, 27(123):579–594, 1973.\nW ei-Chen Cheng, Stanley Kok, Hoai V u Pham, Hai Leong Chieu, a nd Kian Ming Adam Chai.\nLanguage modeling with sum-product networks. 2014.\nKyunghyun Cho, Bart V an Merri¨ enboer, Dzmitry Bahdanau, an d Y oshua Bengio. On the properties\nof neural machine translation: Encoder-decoder approache s. arXiv preprint arXiv:1409.1259 ,\n2014.\nOrhan Firat, Kyunghyun Cho, and Y oshua Bengio. Multi-way, m ultilingual neural machine transla-\ntion with a shared attention mechanism. arXiv preprint arXiv:1601.01073, 2016.\nCharlie Frogner, Chiyuan Zhang, Hossein Mobahi, Mauricio A raya, and T omaso A Poggio. Learn-\ning with a wasserstein loss. In Advances in Neural Information Processing Systems , pp. 2053–\n2061, 2015.\nY arin Gal. A theoretically grounded application of dropout in recurrent neural networks. arXiv\npreprint arXiv:1512.05287, 2015.\nGeoffrey Hinton, Oriol V inyals, and Jeff Dean. Distilling t he knowledge in a neural network. arXiv\npreprint arXiv:1503.02531, 2015.\nSepp Hochreiter and J ¨ urgen Schmidhuber. Long short-term m emory. Neural computation , 9(8):\n1735–1780, 1997.\nHakan Inan and Khashayar Khosravi. Improved learning throu gh augmenting the loss. Stanford CS\n224D: Deep Learning for Natural Language Processing, Spring 2016, 2016.\nKazuki Irie, Zolt´ an T ¨ uske, T amer Alkhouli, Ralf Schl ¨ ute r, and Hermann Ney. Lstm, gru, high-\nway and a bit of attention: an empirical overview for languag e modeling in speech recognition.\nInterspeech, San Francisco, CA, USA, 2016.\nCamille Jordan. Essai sur la g ´ eom ´ etrie ` a n dimensions. Bulletin de la Soci ´et´e math ´ematique de\nFrance, 3:103–174, 1875.\nY oon Kim, Y acine Jernite, David Sontag, and Alexander M Rush . Character-aware neural language\nmodels. arXiv preprint arXiv:1508.06615, 2015.\nMitchell P Marcus, Mary Ann Marcinkiewicz, and Beatrice San torini. Building a large annotated\ncorpus of english: The penn treebank. Computational linguistics, 19(2):313–330, 1993.\nStephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture\nmodels. arXiv preprint arXiv:1609.07843, 2016.\nT omas Mikolov and Geoffrey Zweig. Context dependent recurr ent neural network language model.\nT omas Mikolov, Martin Karaﬁ´ at, Lukas Burget, Jan Cernock ` y, and Sanjeev Khudanpur. Recurrent\nneural network based language model. In Interspeech, volume 2, pp. 3, 2010.\nT omas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in neural information pro-\ncessing systems, pp. 3111–3119, 2013.\nAndriy Mnih and Geoffrey Hinton. Three new graphical models for statistical language modelling.\nIn Proceedings of the 24th international conference on Machin e learning , pp. 641–648. ACM,\n2007.\nRamesh Nallapati, Bowen Zhou, C ¸ aglar Gulc ¸ehre, and Bing X iang. Abstractive text summarization\nusing sequence-to-sequence rnns and beyond. 2016.\n10\nPublished as a conference paper at ICLR 2017\nRazvan Pascanu, C ¸ aglar G ¨ ulc ¸ehre, Kyunghyun Cho, and Y os hua Bengio. How to construct deep\nrecurrent neural networks. CoRR, abs/1312.6026, 2013a.\nRazvan Pascanu, T omas Mikolov, and Y oshua Bengio. On the dif ﬁculty of training recurrent neural\nnetworks. ICML (3), 28:1310–1318, 2013b.\nJeffrey Pennington, Richard Socher, and Christopher D Mann ing. Glove: Global vectors for word\nrepresentation. In EMNLP, volume 14, pp. 1532–43, 2014.\nOﬁr Press and Lior W olf. Using the output embedding to improv e language models. arXiv preprint\narXiv:1608.05859 , 2016.\nAlexander M Rush, Sumit Chopra, and Jason W eston. A neural at tention model for abstractive\nsentence summarization. arXiv preprint arXiv:1509.00685, 2015.\nRichard Socher, Alex Perelygin, Jean Y Wu, Jason Chuang, Chr istopher D Manning, Andrew Y Ng,\nand Christopher Potts. Recursive deep models for semantic c ompositionality over a sentiment\ntreebank. Citeseer, 2013.\nW ojciech Zaremba, Ilya Sutskever, and Oriol V inyals. Recur rent neural network regularization.\narXiv preprint arXiv:1409.2329, 2014.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan Koutn´ ık, and J ¨ urgen Schmidhuber. Recurrent\nhighway networks. arXiv preprint arXiv:1607.03474, 2016.\n11\nPublished as a conference paper at ICLR 2017\nAPPENDIX\nA M ODEL AND TRAINING DETAILS\nW e begin training with a learning rate of 1 and start decaying it with a constant rate after a certain\nepoch. This is 5, 10, and 1 for the small, medium, and large networks respectively. The decay rate\nis 0. 9 for the small and medium networks, and 0. 97 for the large network.\nFor both PTB and Wikitext-2 datasets, we unroll the network f or 35 steps for backpropagation.\nW e use gradient clipping ( Pascanu et al. , 2013b); i.e. we rescale the gradients using the global norm\nif it exceeds a certain value. For both datasets, this is 5 for the small and the medium network, and\n6 for the large network.\nW e use the dropout method introduced in Gal (2015); particularly, we use the same dropout mask\nfor each example through the unrolled network. Differently from what was proposed in Gal (2015),\nwe tie the dropout weights for hidden states further, and we u se the same mask when they are\npropagated as states in the current layer and when they are us ed as inputs for the next layer. W e\ndon’t use dropout in the input embedding layer, and we use the same dropout probability for inputs\nand hidden states. For PTB, dropout probabilities are 0. 7, 0. 5 and 0. 35 for small, medium and large\nnetworks respectively. For Wikitext-2, probabilities are 0. 8 for the small and 0. 6 for the medium\nnetworks.\nWhen training the networks with the augmented loss (AL), we u se a temperature τ = 20. W e have\nempirically observed that setting α , the weight of the augmented loss, according to α = γτ for all\nthe networks works satisfactorily. W e set γ to values between 0. 5 and 0. 8 for the PTB dataset, and\nbetween 1. 0 and 1. 5 for the Wikitext-2 dataset. W e would like to note that we have not observed\nsudden deteriorations in the performance with respect to mo derate variations in either τ or α .\nB M ETRIC FOR CALCULATI NG SUBSPAC E DISTANCES\nIn this section, we detail the metric used for computing the s ubspace distance between two matrices.\nThe computed metric is closely related with the principle an gles between subspaces, ﬁrst deﬁned in\nJordan (1875).\nOur aim is to compute a metric distance between two given matr ices, X and Y . W e do this in three\nsteps:\n(1) Obtain two matrices with orthonormal columns, U and V , such that span (U)=span(X) and\nspan(V )=span(Y ). U and V could be obtained with a QR decomposition.\n(2) Calculate the projection of either one of U and V onto the other; e.g. do S = UU T V ,\nwhere S is the projection of V onto U. Then calculate the residual matrix as R = V − S.\n(3) Let ∥. ∥F rdenote the frobenious norm, and let C be the number of columns of R. Then the\ndistance metric is found as d where d2 = 1\nC ∥R∥2\nF r = 1\nC Trace(RT R).\nW e note that d as calculated above is a valid metric up to the equivalence se t of matrices which span\nthe same column space, although we are not going to show it. In stead, we will mention some metric\nproperties of d, and relate it to the principal angles between the subspaces . W e ﬁrst work out an\nexpression for d:\n12\nPublished as a conference paper at ICLR 2017\nCd2 = Trace(RT R) =Trace\n(\n(V − UU T V )T (V − UU T V )\n)\n= Trace\n(\nV T (I − UU T )(I − UU T )V\n)\n= Trace\n(\nV T (I − UU T )V\n)\n= Trace\n(\n(I − UU T )V V T )\n= Trace(V T V ) − Trace\n(\nUU T V V T )\n= C − Trace\n(\nUU T V V T )\n= C − Trace\n(\n(UT V )T (UT V )\n)\n= C − ∥UT V ∥2\nF r\n=\nC∑\ni=1\n1 − ρ2\ni , (B.1)\nwhere ρi is the ith singular value of UT V , commonly referred to as the ith principle angle between\nthe subspaces of X and Y , θi. In above, we used the cyclic permutation property of the tra ce in the\nthird and the fourth lines.\nSince d2 is 1\nC Trace(RT R), it is always nonnegative, and it is only zero when the residu al is zero,\nwhich is the case when span (X) = span(Y). Further, it is symmetric between U and V due to\nthe form of ( B.1) (singular values of V T U and V T U are the same). Also, d2 = 1\nC\n∑ C\ni=1 sin2(θi),\nnamely the average of the sines of the principle angles, whic h is a quantity between 0 and 1.\n13",
  "topic": "Treebank",
  "concepts": [
    {
      "name": "Treebank",
      "score": 0.8766613006591797
    },
    {
      "name": "Computer science",
      "score": 0.8019928932189941
    },
    {
      "name": "Word (group theory)",
      "score": 0.7495984435081482
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.67596435546875
    },
    {
      "name": "Tying",
      "score": 0.6683433055877686
    },
    {
      "name": "Language model",
      "score": 0.6562642455101013
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6196423172950745
    },
    {
      "name": "Word embedding",
      "score": 0.5663061738014221
    },
    {
      "name": "Natural language processing",
      "score": 0.5469416379928589
    },
    {
      "name": "Projection (relational algebra)",
      "score": 0.5347774028778076
    },
    {
      "name": "Artificial neural network",
      "score": 0.48355433344841003
    },
    {
      "name": "Embedding",
      "score": 0.47901028394699097
    },
    {
      "name": "Dropout (neural networks)",
      "score": 0.47769907116889954
    },
    {
      "name": "Principle of compositionality",
      "score": 0.4751838743686676
    },
    {
      "name": "Speech recognition",
      "score": 0.37843823432922363
    },
    {
      "name": "Machine learning",
      "score": 0.31848204135894775
    },
    {
      "name": "Parsing",
      "score": 0.21200314164161682
    },
    {
      "name": "Algorithm",
      "score": 0.14055293798446655
    },
    {
      "name": "Linguistics",
      "score": 0.09563669562339783
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}