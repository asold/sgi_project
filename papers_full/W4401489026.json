{
  "title": "Evaluating large language models for health-related text classification tasks with public social media data",
  "url": "https://openalex.org/W4401489026",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2102862142",
      "name": "Yuting Guo",
      "affiliations": [
        "Emory University"
      ]
    },
    {
      "id": "https://openalex.org/A5094282512",
      "name": "Anthony Ovadje",
      "affiliations": [
        "Georgia Institute of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A4200816546",
      "name": "Mohammed Ali Al-Garadi",
      "affiliations": [
        "Vanderbilt University"
      ]
    },
    {
      "id": "https://openalex.org/A2165699372",
      "name": "Abeed Sarker",
      "affiliations": [
        "Georgia Institute of Technology",
        "Emory University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3155976349",
    "https://openalex.org/W4390849687",
    "https://openalex.org/W3003720168",
    "https://openalex.org/W3118072203",
    "https://openalex.org/W2963353710",
    "https://openalex.org/W2749701213",
    "https://openalex.org/W6853459487",
    "https://openalex.org/W4381597863",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6855466869",
    "https://openalex.org/W6849928207",
    "https://openalex.org/W4387876242",
    "https://openalex.org/W4386998832",
    "https://openalex.org/W6855186665",
    "https://openalex.org/W6855362603",
    "https://openalex.org/W4387639021",
    "https://openalex.org/W4321490763",
    "https://openalex.org/W4239510810",
    "https://openalex.org/W2129971280",
    "https://openalex.org/W3022809447",
    "https://openalex.org/W4320722432",
    "https://openalex.org/W4319662928",
    "https://openalex.org/W4391044749",
    "https://openalex.org/W4379352793",
    "https://openalex.org/W4385339544",
    "https://openalex.org/W2317693431",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3202190760",
    "https://openalex.org/W4386157637"
  ],
  "abstract": "Abstract Objectives Large language models (LLMs) have demonstrated remarkable success in natural language processing (NLP) tasks. This study aimed to evaluate their performances on social media-based health-related text classification tasks. Materials and Methods We benchmarked 1 Support Vector Machine (SVM), 3 supervised pretrained language models (PLMs), and 2 LLMs-based classifiers across 6 text classification tasks. We developed 3 approaches for leveraging LLMs: employing LLMs as zero-shot classifiers, using LLMs as data annotators, and utilizing LLMs with few-shot examples for data augmentation. Results Across all tasks, the mean (SD) F1 score differences for RoBERTa, BERTweet, and SocBERT trained on human-annotated data were 0.24 (±0.10), 0.25 (±0.11), and 0.23 (±0.11), respectively, compared to those trained on the data annotated using GPT3.5, and were 0.16 (±0.07), 0.16 (±0.08), and 0.14 (±0.08) using GPT4, respectively. The GPT3.5 and GPT4 zero-shot classifiers outperformed SVMs in a single task and in 5 out of 6 tasks, respectively. When leveraging LLMs for data augmentation, the RoBERTa models trained on GPT4-augmented data demonstrated superior or comparable performance compared to those trained on human-annotated data alone. Discussion The results revealed that using LLM-annotated data only for training supervised classification models was ineffective. However, employing the LLM as a zero-shot classifier exhibited the potential to outperform traditional SVM models and achieved a higher recall than the advanced transformer-based model RoBERTa. Additionally, our results indicated that utilizing GPT3.5 for data augmentation could potentially harm model performance. In contrast, data augmentation with GPT4 demonstrated improved model performances, showcasing the potential of LLMs in reducing the need for extensive training data. Conclusions By leveraging the data augmentation strategy, we can harness the power of LLMs to develop smaller, more effective domain-specific NLP models. Using LLM-annotated data without human guidance for training lightweight supervised classification models is an ineffective strategy. However, LLM, as a zero-shot classifier, shows promise in excluding false negatives and potentially reducing the human effort required for data annotation.",
  "full_text": null,
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7383489608764648
    },
    {
      "name": "Social media",
      "score": 0.6968789100646973
    },
    {
      "name": "Natural language processing",
      "score": 0.6116182804107666
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5735211968421936
    },
    {
      "name": "Data science",
      "score": 0.343463659286499
    },
    {
      "name": "World Wide Web",
      "score": 0.2494492530822754
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I150468666",
      "name": "Emory University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I130701444",
      "name": "Georgia Institute of Technology",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I200719446",
      "name": "Vanderbilt University",
      "country": "US"
    }
  ]
}