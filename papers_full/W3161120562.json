{
    "title": "Vision Transformers Are Robust Learners",
    "url": "https://openalex.org/W3161120562",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2917757487",
            "name": "Sayak Paul",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2892897794",
            "name": "Pin-Yu Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2917757487",
            "name": "Sayak Paul",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2133564696",
        "https://openalex.org/W6778485988",
        "https://openalex.org/W2947099774",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W2993466051",
        "https://openalex.org/W6676297131",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3091401866",
        "https://openalex.org/W2115636263",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W6698183232",
        "https://openalex.org/W2905631704",
        "https://openalex.org/W6765536919",
        "https://openalex.org/W6638523607",
        "https://openalex.org/W2898732869",
        "https://openalex.org/W6701947533",
        "https://openalex.org/W1836465849",
        "https://openalex.org/W6631190155",
        "https://openalex.org/W3015146382",
        "https://openalex.org/W2163605009",
        "https://openalex.org/W6760640297",
        "https://openalex.org/W2640329709",
        "https://openalex.org/W2243397390",
        "https://openalex.org/W3005931954",
        "https://openalex.org/W6763509872",
        "https://openalex.org/W6776188000",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W2734663976",
        "https://openalex.org/W6793164127",
        "https://openalex.org/W6749954789",
        "https://openalex.org/W3036438747",
        "https://openalex.org/W2985963903",
        "https://openalex.org/W2951884431",
        "https://openalex.org/W4293846201",
        "https://openalex.org/W2971028215",
        "https://openalex.org/W2963703197",
        "https://openalex.org/W3048030262",
        "https://openalex.org/W2114296159",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W2746314669",
        "https://openalex.org/W4287116734",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2962843773",
        "https://openalex.org/W3035160371",
        "https://openalex.org/W3153842237",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3035422918",
        "https://openalex.org/W2891158090",
        "https://openalex.org/W3166396011",
        "https://openalex.org/W1522301498",
        "https://openalex.org/W3117876421",
        "https://openalex.org/W2964253222",
        "https://openalex.org/W2331143823",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W4297812995",
        "https://openalex.org/W2401231614",
        "https://openalex.org/W3145185940",
        "https://openalex.org/W2117539524",
        "https://openalex.org/W3159481202",
        "https://openalex.org/W3034429256",
        "https://openalex.org/W3142085127",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3104548192",
        "https://openalex.org/W3037492894",
        "https://openalex.org/W2964137095",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W4287646898",
        "https://openalex.org/W2948433173",
        "https://openalex.org/W3145444543",
        "https://openalex.org/W4287022992",
        "https://openalex.org/W2970680991",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2774616426",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2962858109",
        "https://openalex.org/W2963470893",
        "https://openalex.org/W3143373604",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2963060032",
        "https://openalex.org/W2922509574",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W3097217077",
        "https://openalex.org/W4287126759",
        "https://openalex.org/W2964308564",
        "https://openalex.org/W3100859887",
        "https://openalex.org/W4287867670",
        "https://openalex.org/W3035682985",
        "https://openalex.org/W3177096435",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2980728855",
        "https://openalex.org/W2476548250",
        "https://openalex.org/W3128637142",
        "https://openalex.org/W1821462560",
        "https://openalex.org/W4288404646",
        "https://openalex.org/W3030520226"
    ],
    "abstract": "Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the robustness of the Vision Transformer (ViT) (Dosovitskiy et al. 2021) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT(Dosovitskiy et al. 2021) models and SOTA convolutional neural networks (CNNs), Big-Transfer (Kolesnikov et al. 2020). Through a series of six systematically designed experiments, we then present analyses that provide both quantitative andqualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available at https://git.io/J3VO0.",
    "full_text": "Vision Transformers are Robust Learners\nSayak Paul,1* Pin-Yu Chen2*\n1 Carted 2 IBM Research\nsayak@carted.com, pin-yu.chen@ibm.com\nAbstract\nTransformers, composed of multiple self-attention layers, hold\nstrong promises toward a generic learning primitive applicable\nto different data modalities, including the recent breakthroughs\nin computer vision achieving state-of-the-art (SOTA) standard\naccuracy. What remains largely unexplored is their robust-\nness evaluation and attribution. In this work, we study the\nrobustness of the Vision Transformer (ViT) against common\ncorruptions and perturbations, distribution shifts, and natural\nadversarial examples. We use six different diverse ImageNet\ndatasets concerning robust classiﬁcation to conduct a compre-\nhensive performance comparison of ViT models and SOTA\nconvolutional neural networks (CNNs), Big-Transfer. Through\na series of six systematically designed experiments, we then\npresent analyses that provide both quantitative and qualitative\nindications to explain why ViTs are indeed more robust learn-\ners. For example, with fewer parameters and similar dataset\nand pre-training combinations, ViT gives a top-1 accuracy of\n28.10% on ImageNet-A which is 4.3x higher than a compara-\nble variant of BiT. Our analyses on image masking, Fourier\nspectrum sensitivity, and spread on discrete cosine energy\nspectrum reveal intriguing properties of ViT attributing to im-\nproved robustness. Code for reproducing our experiments is\navailable at https://git.io/J3VO0.\n1 Introduction\nTransformers (Vaswani et al. 2017) are becoming a preferred\narchitecture for various data modalities. This is primarily be-\ncause they help reduce inductive biases that go into designing\nnetwork architectures. Moreover, Transformers have been\nshown to achieve tremendous parameter efﬁciency without\nsacriﬁcing predictive performance over architectures that are\noften dedicated to speciﬁc types of data modalities. Attention,\nin particular, self-attention is one of the foundational blocks\nof Transformers. It is a computational primitive that allows\nus to quantify pairwise entity interactions thereby helping a\nnetwork learn the hierarchies and alignments present inside\nthe input data (Bahdanau, Cho, and Bengio 2015; Vaswani\net al. 2017). These are desirable properties to eliminate the\nneed for carefully designed inductive biases to a great extent.\nAlthough Transformers have been used in prior works\n(Trinh, Luong, and Le 2019; Chen et al. 2020) it was only un-\n*These authors contributed equally.\nCopyright © 2022, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\ntil 2020, the performance of Transformers were on par with\nthe SOTA CNNs on standard image recognition tasks (Carion\net al. 2020; Touvron et al. 2020; Dosovitskiy et al. 2021). At-\ntention has been shown to be an important element for vision\nnetworks to achieve better empirical robustness (Hendrycks\net al. 2021). Since attention is a core component of ViTs (and\nTransformers in general), a question that naturally gets raised\nhere is – could ViTs be inherently more robust?If so, why\nare ViTs more robust learners?In this work, we provide an\nafﬁrmative answer to the ﬁrst question and provide empirical\nevidence to reason about the improved robustness of ViTs.\nVarious recent works have opened up the investigation on\nevaluating the robustness of ViTs (Bhojanapalli et al. 2021;\nShao et al. 2021; Mahmood, Mahmood, and Van Dijk 2021)\nbut with a relatively limited scope. We build on top of these\nand provide further and more comprehensive analyses to\nunderstand why ViTs provide better robustness for seman-\ntic shifts, common corruptions and perturbations, and natu-\nral adversarial examples to input images in comparison to\nSOTA CNNs like Big Transfer (BiT) (Kolesnikov et al. 2020).\nThrough a set of carefully designed experiments, we ﬁrst\nverify the enhanced robustness of ViTs to common robust-\nness benchmark datasets (Hendrycks and Dietterich 2019;\nHendrycks et al. 2020, 2021; Xiao et al. 2021). We then pro-\nvide quantitative and qualitative analyses to help understand\nthe reasons behind this enhancement. In summary, we make\nthe following contributions:\n• We use 6 diverse ImageNet datasets concerning differ-\nent types of robustness evaluation and conclude that ViTs\nachieve signiﬁcantly better performance than BiTs.\n• We design 6 experiments, including robustness to mask-\ning, energy/loss landscape analysis, and sensitivity to high-\nfrequency artifacts to study ViT’s improved robustness.\n• Our analysis provides novel insights for robustness attri-\nbution of ViT. Moreover, our robustness evaluation and\nanalysis tools are generic and can be used to benchmark\nand study future image classiﬁcation models.\n2 Related Work\nTo the best of our knowledge, (Parmar et al. 2018) ﬁrst ex-\nplored the use of Transformers (Vaswani et al. 2017) for the\ntask of image super-resolution which essentially belongs to\nthe category of image generation. Image-GPT (Chen et al.\n2020) used Transformers for unsupervised pre-training from\nThe Thirty-Sixth AAAI Conference on Artificial Intelligence (AAAI-22)\n2071\npixels of images. However, the transfer performance of the\npre-training method is not on par with supervised pre-training\nmethods. ViT (Dosovitskiy et al. 2021) takes the original\nTransformers and makes very minimal changes to make it\nwork with images. In fact, this was one of the primary ob-\njectives of ViT i.e. to keep the original Transformer archi-\ntecture as original as possible and then examining how that\npans out for image classiﬁcation in terms of large-scale pre-\ntraining. As noted in (Dosovitskiy et al. 2021; Steiner et al.\n2021), because of the lesser number of inductive biases, ViT\nneeds to be pre-trained on a relatively larger dataset (such as\nImageNet-21k (Deng et al. 2009)) with strong regularization\nfor achieving reasonable downstream performance. Strong\nregularization is particularly needed in the absence of a larger\ndataset during pre-training (Steiner et al. 2021).\nMultiple variants of Transformers have been proposed to\nshow that it is possible to achieve comparable performance on\nImageNet-1k without using additional data. DeIT (Touvron\net al. 2020) introduces a novel distillation strategy (Hinton,\nVinyals, and Dean 2015) to learn a student Transformers-\nbased network from a well-performing teacher network based\non RegNets (Radosavovic et al. 2020). With this approach,\nDeIT achieves 85.2% top-1 accuracy on ImageNet-1k with-\nout any external data. T2T-ViT (Yuan et al. 2021) proposes a\nnovel tokenization method enabling the network to have more\naccess to local structures of the images. For the Transformer-\nbased backbone, it follows a deep-narrow network topology\ninspired by (Zagoruyko and Komodakis 2016). With pro-\nposed changes, T2T-ViT achieves 83.3% top-1 accuracy on\nImageNet-1k. LV-ViT (Jiang et al. 2021) introduces a new\ntraining objective namely token labeling and also tunes the\nstructure of the Transformers. It achieves 85.4% top-1 accu-\nracy on ImageNet-1k. CLIP (Radford et al. 2021) and Swin\nTransformers (Liu et al. 2021) are also two recent models that\nmake use of Transformers for image recognition problems.\nIn this work, we only focus on ViT (Dosovitskiy et al. 2021).\nConcurrent to our work, there are a few recent works that\nstudy the robustness of ViTs from different perspectives. In\nwhat follows, we summarize their key insights and highlight\nthe differences from our work. (Shao et al. 2021) showed\nthat ViTs has better robustness than CNNs against adversar-\nial input perturbations. The major performance gain can be\nattributed to the capability of learning high-frequency fea-\ntures that are more generalizable and the ﬁnding that convolu-\ntional layers hinder adversarial robustness. (Bhojanapalli et al.\n2021) studied improved robustness of ViTs over ResNets (He\net al. 2016) against adversarial and natural adversarial exam-\nples as well as common corruptions. Moreover, it is shown\nthat ViTs are robust to the removal of almost any single layer.\n(Mahmood, Mahmood, and Van Dijk 2021) studied adversar-\nial robustness of ViTs through various white-box, black-box\nand transfer attacks and found that model ensembling can\nachieve unprecedented robustness without sacriﬁcing clean\naccuracy against a black-box adversary. This paper shows\nnovel insights that are fundamentally different from these\nworks: (\ni) we benchmark the robustness of ViTs on a wide\nspectrum of ImageNet datasets (see Table 2), which are the\nmost comprehensive robustness performance benchmarks to\ndate; (ii) we design six new experiments to verify the superior\nrobustness of ViTs over BiT and ResNet models.\n3 Robustness Performance Comparison on\nImageNet Datasets\n3.1 Multi-head Self Attention (MHSA)\nHere we provide a brief summary of ViTs. Central to ViT’s\nmodel design is self-attention (Bahdanau, Cho, and Bengio\n2015). Here, we ﬁrst compute three quantities from linear\nprojections (\nX ∈RN\u0002D): (i) Query = XWQ, (ii) Key =\nXWK , and (iii) Value = XWV, where WQ, WK, and WV are\nlinear transformations. The linear projections (X) are com-\nputed from batches of the original input data. Self-attention\ntakes these three input quantities and returns an output matrix\n(N ×d) weighted by attention scores using (1):\nAttention(Q;K;V ) = Softmax\n\u0010\nQK>=\n√\nd\n\u0011\nV (1)\nTo enable feature-rich hierarchical learning, hself-attention\nlayers (or so-called ”heads”) are stacked together producing\nan output of N×dh. This output is then fed through a linear\ntransformation layer that produces the ﬁnal output of N ×d\nfrom MHSA. MHSA then forms the core Transformer block.\nAdditional details about ViT’s foundational elements are\nprovided in Appendix.\n3.2 Performance Comparison on Diverse\nImageNet Datasets for Robustness Evaluation\nBaselines In this work, our baseline is a ResNet50V2\nmodel (He et al. 2016) pre-trained on the ImageNet-1k\ndataset (Russakovsky et al. 2015) except for a few results\nwhere we consider ResNet-50 (He et al. 2016) 1. To study\nhow ViTs hold up with the SOTA CNNs we consider BiT\n(Kolesnikov et al. 2020). At its core, BiT networks are scaled-\nup versions of ResNets with Group Normalization (Wu and\nHe 2018) and Weight Standardization (Qiao et al. 2019) lay-\ners added in place of Batch Normalization (Ioffe and Szegedy\n2015). Since ViT and BiT share similar pre-training strate-\ngies (such as using larger datasets like ImageNet-21k (Deng\net al. 2009) and JFT-300 (Sun et al. 2017), longer pre-training\nschedules, and so on) they are excellent candidates for our\ncomparison purposes. So, a question, central to our work is:\nWhere does ViT stand with respect to BiT in terms of\nrobustness under similar parameter and FLOP\nregime, pre-training setup, and data regimes, and\nhow to attribute their performance difference?\nEven though BiT and ViT share similar pre-training sched-\nules and dataset regimes there are differences that are worth\nmentioning. For example, ViT makes use of Dropout (Sri-\nvastava et al. 2014) while BiT does not. ViT is trained using\nAdam (Kingma and Ba 2015) while BiT is trained using\nSGD with momentum. In this work, we focus our efforts\non the publicly available BiT and ViT models only. Later\nvariants of ViTs have used Sharpness-Aware Minimization\n1In these cases, we directly referred to the previously reported\nresults with ResNet-50.\n2072\nFigure 1: Mean top-1 accuracy scores (%) on the ImageNet-\nC dataset as yielded by different variants of ViT and BiT.\nFigure 2: Top-1 accuracy (%) of ViT and BiT for contrast\ncorruption (with the highest severity level) on ImageNet-C.\nVariant #\nParameters\n(Million)\n# FLOPS\n(Million)\nImageNet-1k\nTop-1 Acc\nResNet50V2 25.6138\n4144.854528 76\nBiT m-r50x1\n25.549352 4228.137 80\nBiT m-r50x3 217.31908 37061.838 84\nBiT m-r101x1 44.54148 8041.708 82.1\nBiT m-r101x3 387.934888 71230.434 84.7\nBiT m-r152x4 936.53322 186897.679 85.39\nViT\nB-16 86.859496 17582.74 83.97\nViT B-32 88.297192 4413.986 81.28\nViT L-16 304.715752 61604.136 85.15\nViT L-32 306.63268 15390.083 80.99\nTable 1: Parameter counts, FLOPS (Floating-Point Opera-\ntions), and top-1 accuracy (%) of different variants of ViT and\nBiT. All the reported variants were pre-trained on ImageNet-\n21k and then ﬁne-tuned on ImageNet-1k.\n(Foret et al. 2021) and stronger regularization techniques to\ncompensate the absence of favored inductive priors (Chen,\nHsieh, and Gong 2021; Steiner et al. 2021). However, we do\nnot investigate how those aspects relate to robustness in this\nwork.\nTable 1 reports the parameter counts and FLOPS of differ-\nent ViT and BiT models along with their top-1 accuracy2 on\nthe ImageNet-1k dataset (Russakovsky et al. 2015). It is clear\nthat different variants of ViT are able to achieve comparable\nperformance to BiT but with lesser parameters.\nIn what follows, we compare the performance of ViT and\nBiT on six robustness benchmark datasets (Hendrycks and\nDietterich 2019; Hendrycks et al. 2020, 2021), as summa-\n2Figure 4 of (Kolesnikov et al. 2020) and Table 5 of (Dosovitskiy\net al. 2021) were used to collect the top-1 accuracy scores.\nDataset Purpose\nImageNet-C\n(Hendrycks and Dietterich 2019)\nCommon\ncorruptions\nImageNet-P\n(Hendrycks and Dietterich 2019)\nCommon\nperturbations\nImageNet-R\n(Hendrycks et al. 2020) Semantic shifts\nImageNet-O\n(Hendrycks et al. 2021)\nOut-of-domain\ndistribution\nImageNet-A\n(Hendrycks et al. 2021)\nNatural adversarial\nexamples\nImageNet-9\n(Xiao et al. 2021)\nBackground\ndependence\nTable 2: Summary of the studied datasets and their purpose.\nrized in Table 2. These datasets compare the robustness of\nViT, BiT and the baseline ResNet50V2 in different perspec-\ntives, including (i) common corruptions, (ii) semantic shifts,\n(iii) natural adversarial examples, and (iv) out-of-distribution\ndetection. A summary of the datasets and their purpose is\npresented in Table 2 for easier reference.\nNotably, in these datasets ViT exhibits signiﬁcantly better\nrobustness than BiT of comparable parameter counts. Section\n4 gives the attribution analysis of improved robustness in ViT.\nImageNet-C (Hendrycks and Dietterich 2019)consists\nof 15 types of algorithmically generated corruptions, and\neach type of corruption has ﬁve levels of severity. Along with\nthese, the authors provide additional four types of general\ncorruptions making a total of 19 corruptions. We consider\nall the 19 corruptions at their highest severity level (5) and\n2073\nModel /\nMethod mCE\nResNet-50 76.7\nBiT m-r101x3 58.27\nDeepAugment+AugMix\n53.6\nViT L-16 45.45\nNoisy Student Training 28.3\nTable 3: mCEs (%) of different models and methods on\nImageNet-C (lower is better). Note that Noisy Student Train-\ning incorporates additional training with data augmentation\nfor noise injection.\nModel / Method mFR mT5D\nResNet-50 58 82\nBiT-m r101x3 49.99 76.71\nAugMix (Hendrycks* et al. 2020) 37.4 NA\nViT L-16 33.064 50.15\nTable 4: mFRs (%) and mT5Ds (%) on ImageNet-P dataset\n(lower is better).\nreport the mean top-1 accuracy in Figure 1 as yielded by the\nvariants of ViT and BiT. We consistently observe a better\nperformance across all the variants of ViT under different pa-\nrameter regimes. Note that BiT m-r50x1 and m-r101x1\nhave lesser parameters than the lowest variant of ViT (B-16)\nbut for other possible groupings, variants of ViT have lesser\nparameters than that of BiT. Overall, we notice that ViT per-\nforms consistently better across different corruptions except\nfor contrast. In Figure 2, we report the top-1 accuracy of ViT\nand BiT on the highest severity level of the contrast corrup-\ntion. This observation leaves grounds for future research to\ninvestigate why this is the case since varying contrast factors\nare quite common in real-world use-cases. Based on our ﬁnd-\nings, contrast can be an effective but unexplored approach to\nstudying ViT’s robustness, similar to the study of human’s\nvision performance (Hart et al. 2013; Tuli et al. 2021).\nIn (Hendrycks and Dietterich 2019), mean corruption error\n(mCE) is used to quantify the robustness factors of a model\non ImageNet-C. Speciﬁcally, the top-1 error rate is computed\nfor each of the different corruption ( c) types (1 ≤c ≤15)\nand for each of the severity ( s) levels (1 ≤s ≤5). When\nerror rates for all the severity levels are calculated for a par-\nticular corruption type, their average is stored. This process\nis repeated for all the corruption types and the ﬁnal value is\nan average over all the average error rates from the different\ncorruption types. The ﬁnal score is normalized by the mCE\nof AlexNet (Krizhevsky, Sutskever, and Hinton 2012).\nWe report the mCEs for BiT-m r101x3, ViT L-16, and\na few other models in Table 3. The mCEs are reported for\n15 corruptions as done in (Hendrycks and Dietterich 2019).\nWe include two additional models/methods in Table 3 be-\ncause of the following: (a) Noisy Student Training (Xie et al.\n2020) uses external data and training choices (such as us-\ning RandAugment (Cubuk et al. 2020), Stochastic Depth\n(Huang et al. 2016), etc.) that are helpful in enhancing the\nrobustness of a vision model; (b) DeepAugment and AugMix\n(Hendrycks et al. 2020; Hendrycks* et al. 2020) are designed\nexplicitly to improve the robustness of models against cor-\nruptions seen in ImageNet-C. This is why, to provide a fair\nground to understand where BiT and ViT stand in compari-\nson to state-of-the-art, we add these two models. It is indeed\ninteresting to notice that ViT is able to outperform the combi-\nnation of DeepAugment and AugMix which are speciﬁcally\ndesigned to provide robustness against the corruptions found\nin ImageNet-C. As we will discuss in Section 4, this phe-\nnomenon can be attributed to two primary factors: (a) better\nFigure 3: Top-1 accuracy scores (%) on ImageNet-R dataset.\npre-training and ( b) self-attention. It should also be noted\nthat Noisy Student Training (Xie et al. 2020) incorporates\nvarious factors during training such as an iterative training\nprocedure, strong data augmentation transformations from\nRandAugment for noise injection, test-time augmentation,\nand so on. These factors largely contribute to the improved\nrobustness gains achieved by Noisy Student Training.\nImageNet-P (Hendrycks and Dietterich 2019) has 10\ntypes of common perturbations. Unlike the common corrup-\ntions, the perturbations are subtly nuanced spanning across\nfewer number of pixels inside images. As per (Hendrycks and\nDietterich 2019) mean ﬂip rate (mFR) and mean top-5 dis-\ntance (mT5D) are the standard metrics to evaluate a model’s\nrobustness under these perturbations. They are reported in\nTable 4. Since the formulation of mFR and mT5D are more\ninvolved than mCE and for brevity, we refer the reader to\n(Hendrycks and Dietterich 2019) for more details on these\ntwo metrics. We ﬁnd ViT’s robustness to common perturba-\ntions is signiﬁcantly better than BiT as well as AugMix.\nImageNet-R (Hendrycks et al. 2020)\ncontains images la-\nbelled with ImageNet labels by collecting renditions of Im-\nageNet classes. It helps verify the robustness of vision net-\nworks under semantic shifts under different domains. Figure\n3 shows that ViT’s treatment to domain adaptation is better\nthan that of BiT.\n2074\nFigure 4: Top-1 accuracy scores (%) on ImageNet-A dataset.\nImageNet-A (Hendrycks et al. 2021)is comprised of nat-\nural images that cause misclassiﬁcations due to reasons\nsuch as multiple objects associated with single discrete cate-\ngories. In Figure 4, we report the top-1 accuracy of ViT and\nBiT on the ImageNet-A dataset (Hendrycks et al. 2021). In\n(Hendrycks et al. 2021), self-attention is noted as an impor-\ntant element to tackle these problems. This may help explain\nwhy ViT performs signiﬁcantly better than BiT in this case.\nFor example, the top-1 accuracy of ViT L-16 is 4.3x higher\nthan BiT-m r101x3.\nImageNet-O (Hendrycks et al. 2021)consists of images\nthat belong to different classes not seen by a model during\nits training and are considered as anomalies. For these im-\nages, a robust model is expected to output low conﬁdence\nscores. We follow the same evaluation approach of using\narea under the precision-recall curve(AUPR) as (Hendrycks\net al. 2021) for this dataset. In Figure 5, we report the AUPR\nof the different ViT and BiT models on the ImageNet-O\ndataset (Hendrycks et al. 2021). ViT demonstrates superior\nperformance in anomaly detection than BiT.\nImageNet-9 (Xiao et al. 2021) helps to verify the\nbackground-robustness of vision models. In most cases, the\nforegrounds of images inform our decisions on what might\nbe present inside images. Even if the backgrounds change, as\nlong as the foregrounds stay intact, these decisions should not\nbe inﬂuenced. However, do vision models exhibit a similar\nkind of treatment to image foregrounds and backgrounds?\nIt turns out that the vision models may break down when\nthe background of an image is changed (Xiao et al. 2021).\nIt may suggest that the vision models may be picking up\nunnecessary signals from the image backgrounds. In (Xiao\net al. 2021) it is also shown that background-robustness can\nbe important for determining models’ out of distribution per-\nformance. So, naturally, this motivates us to investigate if\nViT would have better background-robustness than BiT. We\nﬁnd that is indeed the case (refer to Table 5). Additionally,\nFigure 5: AUPR (higher is better) on ImageNet-O dataset.\nin Table 6, we report how well BiT and ViT can detect if the\nforeground of an image is vulnerable3. It appears that for this\ntask also, ViT signiﬁcantly outperforms BiT. Even though we\nnotice ViT’s better performance than BiT but it is surprising\nto see ViT’s performance being worse than ResNet-50. We\nsuspect this may be due to the simple tokenization process of\nViT to create small image patches that limits the capability\nto process important local structures (Yuan et al. 2021).\n4 Why ViT has Improved Robustness?\nIn this section, we systematically design and conduct six\nexperiments to identify the sources of improved robustness\nin ViTs from both qualitative and quantitative standpoints.\n4.1 Attention is Crucial for Improved Robustness\nIn (Dosovitskiy et al. 2021), the authors study the idea of “At-\ntention Distance” to investigate how ViT uses self-attention\nto integrate information across a given image. Speciﬁcally,\nthey analyze the average distance covered by the learned\nattention weights from different layers. One key ﬁnding is\nthat in the lower layers some attention heads attend to almost\nthe entirety of the image and some heads attend to small\nregions. This introduces high variability in the attention dis-\ntance attained by different attention heads, particularly in\nthe lower layers. This variability gets roughly uniform as the\ndepth of the network increases. This capability of building\nrich relationships between different parts of images is crucial\nfor contextual awareness and is different from how CNNs\ninterpret images as investigated in (Raghu et al. 2021).\nSince the attention mechanism helps a model learn better\ncontextual dependencies we hypothesize that this is one of\nthe attributes for the superior performance ViTs show on\nthree robustness benchmark datasets. To this end, we study\nthe performance of different ImageNet-1k models that make\n3For details, we refer the reader to the ofﬁcial repository of the\nbackground robustness challenge: https://git.io/J3TUj.\n2075\nModel Original Mixed-Same Mixed-Rand BG-Gap\nBiT-m r101x3 94.32 81.19 76.62 4.57\nResNet-50 95.6 86.2 78.9 7.3\nViT L-16 96.67 88.49 81.68 6.81\nTable 5: Top-1 accuracy (%) of ImageNet-9 dataset and its different variants. ”BG-Gap” is the gap between ”Mixed-Same”\nand ”Mixed-Rand”. It measures how impactful background correlations are in presence of correct-labeled foregrounds.\nModel Challenge\nAccuracy (%)\nBiT-m r101x3 3.78\nViT L-16 20.02\nResNet-50 22.3\nTable 6: Performance on detecting vulnerable image fore-\ngrounds from ImageNet-9 dataset.\nMasking\nFactor\nTop-1\nAcc (BiT)\nTop-1\nAcc (ViT)\n0 79 83\n0.05 76 82.3\n0.1 75 81.4\n0.2 72.4 77.9\n0.5 52 60.4\nTable 7: Mean top-1 accuracy (%) of BiT ( m-r101x3)\nand ViT (L-16) with different masking factors.\nuse of attention in some form (spatial, channel, or both) 4.\nThese models include EfﬁcientNetV2 (Tan and Le 2021) with\nGlobal Context (GC) blocks (Cao et al. 2020), several ResNet\nvariants with Gather-Excite (GE) blocks (Hu et al. 2018) and\nSelective Kernels (SK) (Li et al. 2019). We also include a ViT\nS/16 model pre-trained on ImageNet-1k for a concrete com-\nparison. We summarize our ﬁndings in Table 8. The results\nsuggest that adding some form of attention is usually a good\ndesign choice especially when robustness aspects are con-\ncerned as there is almost always a consistent improvement in\nperformance compared to that of a vanilla ResNet-50. This is\nalso suggested by Hendrycks et al. (Hendrycks et al. 2021)\nbut only in the context of ImageNet-A. We acknowledge that\nthe models reported in Table 8 differ from the correspond-\ning ViT model with respect to their training conﬁgurations,\nregularization in particular. But exploring how regularization\naffects the robustness aspects of a model is not the question\nwe investigate in this work.\nSelf-attention constitutes a fundamental block for ViTs. So,\nin a realistic hope, they should be able to perform even better\nwhen they are trained in the right manner to compensate for\nthe absence of strong inductive priors as CNNs. We conﬁrm\nthis in Table 8 (last row). Note that the work on AugReg\n(Steiner et al. 2021) showed that it is important to incorpo-\nrate stronger regularization to train better performing ViTs\nin the absence of inductive priors and larger data regimes.\nMore experiments and attention visualizations showing the\nconnection between attention and robustness are presented in\nAppendix.\n4.2 Role of Pre-training\nViTs yield excellent transfer performance when they are pre-\ntrained on larger datasets (Dosovitskiy et al. 2021; Steiner\net al. 2021). This is why, to better isolate the effects of pre-\ntraining with larger data regimes we consider a ViT B/16\n4We used implementations from the timm library for this.\nmodel but trained with different conﬁgurations and assess\ntheir performance on the same benchmark datasets as used\nin Section 4.1. These conﬁgurations primarily differ in terms\nof the pre-training dataset. We report our ﬁndings in the\nTable 9. We notice that the model pre-trained on ImageNet-\n1k performs worse than the one pre-trained on ImageNet-21k\nand then ﬁne-tuned on ImageNet-1k.\nObservations from Table 9 lead us to explore another ques-\ntions i.e., under similar pre-training conﬁgurations how do\nthe ViT models stand out with respect to BiT models. This fur-\nther helps to validate which architectures should be preferred\nfor longer pre-training with larger datasets as far as robust-\nness aspects are concerned. This may become an important\nfactor to consider when allocating budgets and resources for\nlarge-scale experiments on robustness. Throughout Section 3\nand the rest of Section 4, we show that ViT models signiﬁ-\ncantly outperform similar BiT models across six robustness\nbenchmark datasets that we use in this work. We also present\nadditional experiments in Appendix by comparing ViTs to\nBiTs of similar parameters.\n4.3 ViT Has Better Robustness to Image Masking\nIn order to further establish that attention indeed plays an im-\nportant role for the improved robustness of ViTs, we conduct\nthe following experiment:\n• Randomly sample a common set of 1000 images from the\nImageNet-1k validation set.\n• Apply Cutout (DeVries and Taylor 2017) at four different\nlevels: {5,10,20,50}% and calculate the mean top-1 accu-\nracy scores for each of the levels with BiT ( m-r101x3)\nand ViT (L-16)5. In Cutout, square regions from input im-\nages are randomly masked out. It was originally proposed\nas a regularization technique.\n5We use these two variants because they are comparable with\nrespect to the number model parameters.\n2076\nModel # Parameters\n(Million)\n# FLOPS\n(Million)\nImageNet-A\n(Top-1 Acc)\nImageNet-R\n(Top-1 Acc)\nImageNet-O\n(AUPR)\nResNet-50 25.6138 4144.854528 2.14 25.25 16.76\nEfﬁcientV2 (GC) 13.678 1937.974 7.389285 32.701343 20.34\nResNet-L (GE) 31.078 3501.953 5.1157087 29.905242 21.61\nResNet-M (GE) 21.143 3015.121 4.99335 29.345 22.1\nResNet-S (GE) 8.174 749.538 2.4682036 24.96156 17.74\nResNet18 (SK) 11.958 1820.836 1.802681 22.95351 16.71\nResNet34 (SK) 22.282 3674.5 3.4683768 26.77625 18.03\nWide (4x)\nResNet-50 (SK) 27.48 4497.133 6.0972147 28.3357 20.58\nViT S/16 22 4608.338304 6.39517 26.11397 22.50\nTable 8: Complexity and performance of different attention-fused models on three benchmark robustness datasets. All models\nreported here operate on images of size 224 ×224.\nPre-training ImageNet-A\n(T\nop-1 Acc)\nImageNet-R\n(Top-1 Acc)\nImageNet-O\n(AUPR)\nImageNet-1k 8.630994\n28.213835 26.25\nImageNet-21k 21.746947 41.815233 54.61\nTable 9: Performance of the ViT B/16 model on three bench-\nmark datasets.\nResNet-50 BiT-m r101x3 ViT L-16\nP=10 21.8 13.9 6.7\nP=25 30.2 14.8 7\nP=50 40.4 16.4 7.6\nP=90 58.9 23 13.1\nP=95 63.6 24.9 15.1\nTable 10: Different percentiles (P) of the error matrix com-\nputed from Fourier analysis (Figure 6).\nTable 7 reports that ViT is able to consistently beat BiT\nwhen square portions of the input images have been randomly\nmasked out. Randomness is desirable here because ViT can\nutilize global information. If we ﬁxate the region of masking\nit may be too restrictive for a ViT to take advantage of its\nability to utilize global information. Note that the ViT variant\n(L-16) we use in this experiment is shallower than the BiT\nvariant (m-r101x3). This may suggest that attention indeed\nis the strong force behind this signiﬁcant gain.\n4.4 Fourier Spectrum Analysis Reveals Low\nSensitivity for ViT\nA common hypothesis about vision models is that they can\neasily pick up the spurious correlations present inside input\ndata that may be imperceptible and unintuitive to humans\n(Jo and Bengio 2017; Hendrycks and Dietterich 2019). To\nmeasure how ViT holds up with this end of the bargain, we\nconduct a Fourier analysis (Yin et al. 2019) of ViT, BiT, and\nour baseline ResNet-50. The experiment goes as follows:\n• Generate a Fourier basis vector with varying frequencies.\n• Add the basis vector to 1000 randomly sampled images\nfrom the ImageNet-1k validation set.\n• Record error-rate for every perturbed image and generate a\nheatmap of the ﬁnal error matrix.\nFor additional details on this experiment, we refer the\nreader to (Yin et al. 2019). In Figure 6, it is noticed that\nboth ViT and BiT stay robust (have low sensitivity) to most\nof the regions present inside the perturbed images while\nthe baseline ResNet50V2 loses its consistency in the high-\nfrequency regions. The value at location\n(i;j) shows the\nerror rate on data perturbed by the corresponding Fourier\nbasis noise.\nThe low sensitivity of ViT and BiT may be attributed to the\nfollowing factors: (a) Both ViT and BiT are pre-trained on a\nlarger dataset and then ﬁne-tuned on ImageNet-1k. Using a\nlarger dataset during pre-training may be acting as a regular-\nizer here (Kolesnikov et al. 2020). (b) Evidence also suggests\nthat increased network width has a positive effect on model\nrobustness (Hendrycks and Dietterich 2019; Hendrycks et al.\n2021). To get a deeper insight into the heatmaps shown in\nFigure 6, in Table 10, we report error-rate percentiles for the\nthree models under consideration. For a more robust model,\nwe should expect to see lower numbers across all the ﬁve\ndifferent percentiles reported in Table 10 and we conﬁrm\nthat is indeed the case. This may also help explain the better\nbehavior of BiT and ViT in this experiment.\n4.5 Adversarial Perturbations of ViT Has Wider\nSpread in Energy Spectrum\nIn (Ortiz-Jimenez et al. 2020), it is shown that small ad-\nversarial perturbations can change the decision boundary of\nneural networks (especially CNNs) and that adversarial train-\ning (Madry et al. 2018) exploits this sensitivity to induce\nrobustness. Furthermore, CNNs primarily exploit discrimi-\nnative features from the low-frequency regions of the input\ndata. Following (Ortiz-Jimenez et al. 2020), we conduct the\nfollowing experiment on 1000 randomly sampled images\nfrom the ImageNet-1k validation set with ResNet-50, BiT-m\nr50x3, and ViT B-166:\n• Generate small adversarial perturbations (\u000e) with DeepFool\n(Moosavi-Dezfooli, Fawzi, and Frossard 2016) with a step\n6For computational constraints we used smaller BiT and ViT\nvariants for this experiment.\n2077\nFigure 6: Sensitivity heatmap of 2D discrete Fourier transform spectrum (Yin et al. 2019). The\nlow-frequency/high-frequency components are shifted to the center/corner of the spectrum.\nFigure 7: Spectral decomposition of adversarial perturbations generated using DeepFool\n(Moosavi-Dezfooli, Fawzi, and Frossard 2016). The top-left/bottom-right quadrants denote low-\nfrequency/high-frequency regions.\nsize of 507.\n• Change the basis of the perturbations with discrete cosine\ntransform (DCT) to compute the energy spectrum of the\nperturbations.\nThis experiment aims to conﬁrm that ViT’s perturbations\nwill spread out the whole spectrum, while perturbations of\nResNet-50 and BiT will be centered only around the low-\nfrequency regions. This is primarily because ViT has the\nability to better exploit information that is only available in a\nglobal context. Figure 7 shows the energy spectrum analysis.\nIt suggests that to attack ViT, (almost) the entire frequency\nspectrum needs to be affected, while it is less so for BiT and\nResNet-50.\n4.6 ViT Has Smoother Loss Landscape to Input\nPerturbations\nOne way to attribute the improved robustness of ViT over\nBiT is to hypothesize ViT has a smoother loss landscape\nwith respect to input perturbations. Here we explore their\nloss landscapes based on a common set of 100 ImageNet-1k\nvalidation images that are correctly classiﬁed by both models.\nWe apply the multi-step projected gradient descent (PGD)\nattack (Madry et al. 2018) with an `1perturbation budget\nof \u000f= 0:002 when normalizing the pixel value range to be\n7Rest of the hyperparameters are same as what is speciﬁed https:\n//git.io/JEhpG.\nFigure 8: Loss progression (mean and standard deviation)\nViT (L-16) and BiT-m (r101x3) during PGD attacks (Madry\net al. 2018).\nbetween [−1;1]8 (refer to Appendix for details on hyperpa-\nrameters). Figure 8 shows that the classiﬁcation loss (cross\nentropy) of ViT increases at a much slower rate than that of\nBiT as one varies the attack steps, validating our hypothesis\nof smoother loss landscape to input perturbations.\nIn summary, in this section, we broadly verify that ViT\ncan yield improved robustness (even with fewer parame-\n8We follow the PGD implementation from https://adversarial-\nml-tutorial.org/introduction/.\n2078\nters/FLOPS in some cases). This indicates that the use of\nTransformers can be orthogonal to the known techniques to\nimprove the robustness of vision models (Balaji, Goldstein,\nand Hoffman 2019; Carmon et al. 2019; Xie et al. 2020).\n5 Conclusion\nRobustness is an important aspect to consider when deploy-\ning deep learning models into the wild. This work provides a\ncomprehensive robustness performance assessment of ViTs\nusing 6 different ImageNet datasets and concludes that ViT\nsigniﬁcantly outperforms its CNN counterpart (BiT) and the\nbaseline ResNet50V2 model. We further conducted 6 new\nexperiments to verify our hypotheses of improved robustness\nin ViT, including the use of large-scale pre-training and at-\ntention module, the ability to recognize randomly masked\nimages, the low sensibility to Fourier spectrum domain per-\nturbation, and the property of wider energy distribution and\nsmoother loss landscape under adversarial input perturba-\ntions. Our analyses and ﬁndings show novel insights toward\nunderstanding the source of robustness and can shed new\nlight on robust neural network architecture design.\nAcknowledgements\nWe are thankful to the Google Developers Experts program9\n(speciﬁcally Soonson Kwon and Karl Weinmeister) for pro-\nviding Google Cloud Platform credits to support the experi-\nments. We also thank Justin Gilmer (of Google), Guillermo\nOrtiz-Jimenez (of EPFL, Switzerland), and Dan Hendrycks\n(of UC Berkeley) for fruitful discussions.\nReferences\nBahdanau, D.; Cho, K.; and Bengio, Y . 2015. Neural Machine\nTranslation by Jointly Learning to Align and Translate. In\nInternational Conference on Learning Representations.\nBalaji, Y .; Goldstein, T.; and Hoffman, J. 2019. Instance\nadaptive adversarial training: Improved accuracy tradeoffs in\nneural nets. arXiv preprint arXiv:1910.08051.\nBhojanapalli, S.; Chakrabarti, A.; Glasner, D.; Li, D.; Un-\nterthiner, T.; and Veit, A. 2021. Understanding Robustness\nof Transformers for Image Classiﬁcation. arXiv preprint\narXiv:2103.14586.\nCao, Y .; Xu, J.; Lin, S.; Wei, F.; and Hu, H. 2020. Global\nContext Networks. arXiv:2012.13375.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-end object detection\nwith transformers. In European Conference on Computer\nVision, 213–229. Springer.\nCarmon, Y .; Raghunathan, A.; Schmidt, L.; Duchi, J. C.; and\nLiang, P. S. 2019. Unlabeled Data Improves Adversarial\nRobustness. In Advances in Neural Information Processing\nSystems, volume 32.\nChen, M.; Radford, A.; Child, R.; Wu, J.; Jun, H.; Luan, D.;\nand Sutskever, I. 2020. Generative Pretraining From Pixels.\nIn International Conference on Machine Learning, volume\n119, 1691–1703.\n9https://developers.google.com/programs/experts/\nChen, X.; Hsieh, C.-J.; and Gong, B. 2021. When Vision\nTransformers Outperform ResNets without Pretraining or\nStrong Data Augmentations. arXiv:2106.01548.\nCubuk, E. D.; Zoph, B.; Shlens, J.; and Le, Q. V . 2020. Ran-\ndaugment: Practical automated data augmentation with a re-\nduced search space. In IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, 3008–3017.\nDeng, J.; Dong, W.; Socher, R.; Li, L.-J.; Li, K.; and Fei-Fei,\nL. 2009. Imagenet: A large-scale hierarchical image database.\nIn IEEE Conference on Computer Vision and Pattern Recog-\nnition,, 248–255.\nDeVries, T.; and Taylor, G. W. 2017. Improved regularization\nof convolutional neural networks with cutout. arXiv preprint\narXiv:1708.04552.\nDosovitskiy, A.; Beyer, L.; Kolesnikov, A.; Weissenborn,\nD.; Zhai, X.; Unterthiner, T.; Dehghani, M.; Minderer, M.;\nHeigold, G.; Gelly, S.; Uszkoreit, J.; and Houlsby, N. 2021.\nAn Image is Worth 16x16 Words: Transformers for Image\nRecognition at Scale. In International Conference on Learn-\ning Representations.\nForet, P.; Kleiner, A.; Mobahi, H.; and Neyshabur, B. 2021.\nSharpness-aware Minimization for Efﬁciently Improving\nGeneralization. In International Conference on Learning\nRepresentations.\nHart, B. M.; Schmidt, H. C. E. F.; Klein-Harmeyer, I.; and\nEinh¨auser, W. 2013. Attention in natural scenes: contrast\naffects rapid visual processing and ﬁxations alike. Philosoph-\nical Transactions of the Royal Society B: Biological Sciences,\n368(1628): 20130067.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep Residual\nLearning for Image Recognition. In IEEE Conference on\nComputer Vision and Pattern Recognition, 770–778.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Identity Map-\npings in Deep Residual Networks. In Leibe, B.; Matas, J.;\nSebe, N.; and Welling, M., eds., European Conference on\nComputer Vision, 630–645. Springer.\nHendrycks, D.; Basart, S.; Mu, N.; Kadavath, S.; Wang, F.;\nDorundo, E.; Desai, R.; Zhu, T.; Parajuli, S.; Guo, M.; Song,\nD.; Steinhardt, J.; and Gilmer, J. 2020. The Many Faces\nof Robustness: A Critical Analysis of Out-of-Distribution\nGeneralization. arXiv preprint arXiv:2006.16241.\nHendrycks, D.; and Dietterich, T. G. 2019. Benchmarking\nNeural Network Robustness to Common Corruptions and\nPerturbations. In International Conference on Learning Rep-\nresentations.\nHendrycks*, D.; Mu*, N.; Cubuk, E. D.; Zoph, B.; Gilmer, J.;\nand Lakshminarayanan, B. 2020. AugMix: A Simple Method\nto Improve Robustness and Uncertainty under Data Shift. In\nInternational Conference on Learning Representations.\nHendrycks, D.; Zhao, K.; Basart, S.; Steinhardt, J.; and Song,\nD. 2021. Natural Adversarial Examples. Conference on\nComputer Vision and Pattern Recognition.\nHinton, G.; Vinyals, O.; and Dean, J. 2015. Distilling the\nKnowledge in a Neural Network. In NeurIPS Deep Learning\nand Representation Learning Workshop.\n2079\nHu, J.; Shen, L.; Albanie, S.; Sun, G.; and Vedaldi, A. 2018.\nGather-Excite: Exploiting Feature Context in Convolutional\nNeural Networks. In Bengio, S.; Wallach, H.; Larochelle, H.;\nGrauman, K.; Cesa-Bianchi, N.; and Garnett, R., eds., Ad-\nvances in Neural Information Processing Systems, volume 31.\nCurran Associates, Inc.\nHuang, G.; Sun, Y .; Liu, Z.; Sedra, D.; and Weinberger, K. Q.\n2016. Deep networks with stochastic depth. In European\nconference on computer vision, 646–661. Springer.\nIoffe, S.; and Szegedy, C. 2015. Batch Normalization: Accel-\nerating Deep Network Training by Reducing Internal Covari-\nate Shift. In International Conference on Machine Learning,\nvolume 37, 448–456.\nJiang, Z.; Hou, Q.; Yuan, L.; Zhou, D.; Jin, X.; Wang, A.; and\nFeng, J. 2021. Token labeling: Training a 85.5% top-1 accu-\nracy vision transformer with 56m parameters on imagenet.\narXiv preprint arXiv:2104.10858.\nJo, J.; and Bengio, Y . 2017. Measuring the tendency of\ncnns to learn surface statistical regularities. arXiv preprint\narXiv:1711.11561.\nKingma, D.; and Ba, J. 2015. Adam: A method for stochastic\noptimization. International Conference on Learning Repre-\nsentations.\nKolesnikov, A.; Beyer, L.; Zhai, X.; Puigcerver, J.; Yung, J.;\nGelly, S.; and Houlsby, N. 2020. Big Transfer (BiT): General\nVisual Representation Learning. In European Conference on\nComputer Vision, 491–507.\nKrizhevsky, A.; Sutskever, I.; and Hinton, G. E. 2012. Im-\nageNet Classiﬁcation with Deep Convolutional Neural Net-\nworks. In Advances in Neural Information Processing Sys-\ntems, volume 25.\nLi, X.; Wang, W.; Hu, X.; and Yang, J. 2019. Selective Kernel\nNetworks. In 2019 IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 510–519.\nLiu, Z.; Lin, Y .; Cao, Y .; Hu, H.; Wei, Y .; Zhang, Z.; Lin, S.;\nand Guo, B. 2021. Swin Transformer: Hierarchical Vision\nTransformer using Shifted Windows. arXiv:2103.14030.\nMadry, A.; Makelov, A.; Schmidt, L.; Tsipras, D.; and Vladu,\nA. 2018. Towards Deep Learning Models Resistant to Ad-\nversarial Attacks. International Conference on Learning\nRepresentations.\nMahmood, K.; Mahmood, R.; and Van Dijk, M. 2021. On the\nRobustness of Vision Transformers to Adversarial Examples.\narXiv preprint arXiv:2104.02610.\nMoosavi-Dezfooli, S.-M.; Fawzi, A.; and Frossard, P. 2016.\nDeepfool: a simple and accurate method to fool deep neural\nnetworks. In IEEE Conference on Computer Vision and\nPattern Recognition, 2574–2582.\nOrtiz-Jimenez, G.; Modas, A.; Moosavi, S.-M.; and Frossard,\nP. 2020. Hold me tight! Inﬂuence of discriminative features\non deep network boundaries. In Advances in Neural Infor-\nmation Processing Systems, volume 33, 2935–2946.\nParmar, N.; Vaswani, A.; Uszkoreit, J.; Kaiser, L.; Shazeer,\nN.; Ku, A.; and Tran, D. 2018. Image Transformer. In\nInternational Conference on Machine Learning, volume 80,\n4055–4064.\nQiao, S.; Wang, H.; Liu, C.; Shen, W.; and Yuille, A.\n2019. Micro-Batch Training with Batch-Channel Nor-\nmalization and Weight Standardization. arXiv preprint\narXiv:1903.10520.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transfer-\nable Visual Models From Natural Language Supervision.\narXiv:2103.00020.\nRadosavovic, I.; Kosaraju, R.; Girshick, R.; He, K.; and Dol-\nlar, P. 2020. Designing Network Design Spaces. In IEEE\nConference on Computer Vision and Pattern Recognition,\n10425–10433.\nRaghu, M.; Unterthiner, T.; Kornblith, S.; Zhang, C.; and\nDosovitskiy, A. 2021. Do Vision Transformers See Like\nConvolutional Neural Networks? arXiv:2108.08810.\nRussakovsky, O.; Deng, J.; Su, H.; Krause, J.; Satheesh, S.;\nMa, S.; Huang, Z.; Karpathy, A.; Khosla, A.; Bernstein, M.;\net al. 2015. Imagenet large scale visual recognition challenge.\nInternational Journal of Computer Vision, 115(3): 211–252.\nShao, R.; Shi, Z.; Yi, J.; Chen, P.-Y .; and Hsieh, C.-J. 2021.\nOn the Adversarial Robustness of Visual Transformers.arXiv\npreprint arXiv:2103.15670.\nSrivastava, N.; Hinton, G.; Krizhevsky, A.; Sutskever, I.; and\nSalakhutdinov, R. 2014. Dropout: A Simple Way to Pre-\nvent Neural Networks from Overﬁtting. Journal of Machine\nLearning Research, 15(56): 1929–1958.\nSteiner, A.; Kolesnikov, A.; Zhai, X.; Wightman, R.; Uszko-\nreit, J.; and Beyer, L. 2021. How to train your ViT? Data,\nAugmentation, and Regularization in Vision Transformers.\narXiv:2106.10270.\nSun, C.; Shrivastava, A.; Singh, S.; and Gupta, A. 2017. Re-\nvisiting Unreasonable Effectiveness of Data in Deep Learning\nEra. In IEEE International Conference on Computer Vision,\n843–852.\nTan, M.; and Le, Q. 2021. EfﬁcientNetV2: Smaller Models\nand Faster Training. In Meila, M.; and Zhang, T., eds., Pro-\nceedings of the 38th International Conference on Machine\nLearning, volume 139 of Proceedings of Machine Learning\nResearch, 10096–10106. PMLR.\nTouvron, H.; Cord, M.; Douze, M.; Massa, F.; Sablayrolles,\nA.; and J´egou, H. 2020. Training data-efﬁcient image trans-\nformers & distillation through attention. arXiv preprint\narXiv:2012.12877.\nTrinh, T. H.; Luong, M.-T.; and Le, Q. V . 2019. Selﬁe: Self-\nsupervised pretraining for image embedding. arXiv preprint\narXiv:1906.02940.\nTuli, S.; Dasgupta, I.; Grant, E.; and Grifﬁths, T. L. 2021.\nAre Convolutional Neural Networks or Transformers more\nlike human vision? arXiv preprint arXiv:2105.07197.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, L. u.; and Polosukhin, I. 2017.\nAttention is All you Need. InAdvances in Neural Information\nProcessing Systems, volume 30.\nWu, Y .; and He, K. 2018. Group Normalization. InEuropean\nConference on Computer Vision, 3–19.\n2080\nXiao, K.; Engstrom, L.; Ilyas, A.; and Madry, A. 2021. Noise\nor Signal: The Role of Image Backgrounds in Object Recogni-\ntion. International Conference on Learning Representations.\nXie, Q.; Luong, M.-T.; Hovy, E.; and Le, Q. V . 2020. Self-\nTraining With Noisy Student Improves ImageNet Classiﬁca-\ntion. In IEEE Conference on Computer Vision and Pattern\nRecognition, 10684–10695.\nYin, D.; Gontijo Lopes, R.; Shlens, J.; Cubuk, E. D.; and\nGilmer, J. 2019. A Fourier Perspective on Model Robustness\nin Computer Vision. In Advances in Neural Information\nProcessing Systems, volume 32.\nYuan, L.; Chen, Y .; Wang, T.; Yu, W.; Shi, Y .; Tay, F. E.;\nFeng, J.; and Yan, S. 2021. Tokens-to-token vit: Training\nvision transformers from scratch on imagenet. arXiv preprint\narXiv:2101.11986.\nZagoruyko, S.; and Komodakis, N. 2016. Wide residual\nnetworks. arXiv preprint arXiv:1605.07146.\n2081"
}