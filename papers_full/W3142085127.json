{
    "title": "On the Adversarial Robustness of Visual Transformers",
    "url": "https://openalex.org/W3142085127",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A2977373843",
            "name": "Rulin Shao",
            "affiliations": [
                "Carnegie Mellon University"
            ]
        },
        {
            "id": "https://openalex.org/A2996502104",
            "name": "Zhouxing Shi",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        },
        {
            "id": "https://openalex.org/A2102191302",
            "name": "Jinfeng Yi",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2892897794",
            "name": "Pin-Yu Chen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4213488976",
            "name": "Cho-Jui Hsieh",
            "affiliations": [
                "University of California, Los Angeles"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2612445135",
        "https://openalex.org/W3163461448",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W3047517563",
        "https://openalex.org/W2996851481",
        "https://openalex.org/W3104423855",
        "https://openalex.org/W3101449015",
        "https://openalex.org/W2962729158",
        "https://openalex.org/W1544092585",
        "https://openalex.org/W3138516171",
        "https://openalex.org/W2964253222",
        "https://openalex.org/W3035164976",
        "https://openalex.org/W2963125010",
        "https://openalex.org/W2950813464",
        "https://openalex.org/W3170874841",
        "https://openalex.org/W2947469743",
        "https://openalex.org/W3092462694",
        "https://openalex.org/W2995368830",
        "https://openalex.org/W2508457857",
        "https://openalex.org/W2962940432",
        "https://openalex.org/W2963143631",
        "https://openalex.org/W2302255633",
        "https://openalex.org/W2912237282",
        "https://openalex.org/W3122730565",
        "https://openalex.org/W1686810756",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W2963703197",
        "https://openalex.org/W3102564617",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2938830017",
        "https://openalex.org/W2998835636",
        "https://openalex.org/W3103491646",
        "https://openalex.org/W2401231614",
        "https://openalex.org/W3118608800",
        "https://openalex.org/W3105463048",
        "https://openalex.org/W3122542623",
        "https://openalex.org/W2963389226",
        "https://openalex.org/W3099769594",
        "https://openalex.org/W2963420686",
        "https://openalex.org/W2963207607",
        "https://openalex.org/W3088909400",
        "https://openalex.org/W3035183289",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W3121523901",
        "https://openalex.org/W2911634294",
        "https://openalex.org/W3092571604",
        "https://openalex.org/W2970971581",
        "https://openalex.org/W3168489096",
        "https://openalex.org/W3128654100",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3034994123"
    ],
    "abstract": "Following the success in advancing natural language processing and understanding, transformers are expected to bring revolutionary changes to computer vision. This work provides the first and comprehensive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested on various white-box and transfer attack settings, we find that ViTs possess better adversarial robustness when compared with convolutional neural networks (CNNs). We summarize the following main observations contributing to the improved robustness of ViTs: \r\n1) Features learned by ViTs contain less low-level information and are more generalizable, which contributes to superior robustness against adversarial perturbations. \r\n2) Introducing convolutional or tokens-to-token blocks for learning low-level features in ViTs can improve classification accuracy but at the cost of adversarial robustness. \r\n3) Increasing the proportion of transformers in the model structure (when the model consists of both transformer and CNN blocks) leads to better robustness. But for a pure transformer model, simply increasing the size or adding layers cannot guarantee a similar effect. \r\n4) Pre-training on larger datasets does not significantly improve adversarial robustness though it is critical for training ViTs. \r\n5) Adversarial training is also applicable to ViT for training robust models. \r\nFurthermore, feature visualization and frequency analysis are conducted for explanation. The results show that ViTs are less sensitive to high-frequency perturbations than CNNs and there is a high correlation between how well the model learns low-level features and its robustness against different frequency-based perturbations.",
    "full_text": "Published in Transactions on Machine Learning Research (10/2022)\nOn the Adversarial Robustness of Vision Transformers\nRulin Shao rulins@cs.cmu.edu\nCarnegie Mellon University\nZhouxing Shi zshi@cs.ucla.edu\nUniversity of California, Los Angeles\nJinfeng Yi yijinfeng@jd.com\nJD AI Research\nPin-Yu Chen pin-yu.chen@ibm.com\nIBM Research\nCho-Jui Hsieh chohsieh@cs.ucla.edu\nUniversity of California, Los Angeles\nReviewed on OpenReview: https://openreview.net/forum?id=lE7K4n1Esk\nAbstract\nFollowing the success in advancing natural language processing and understanding, transformers\nare expected to bring revolutionary changes to computer vision. This work provides a comprehen-\nsive study on the robustness of vision transformers (ViTs) against adversarial perturbations. Tested\non various white-box and transfer attack settings, we ﬁnd that ViTs possess better adversarial ro-\nbustness when compared with MLP-Mixer and convolutional neural networks (CNNs) including\nConvNeXt, and this observation also holds for certiﬁed robustness. Through frequency analysis\nand feature visualization, we summarize the following main observations contributing to the im-\nproved robustness of ViTs: 1) Features learned by ViTs contain less high-frequency patterns that\nhave spurious correlation, which helps explain why ViTs are less sensitive to high-frequency pertur-\nbations than CNNs and MLP-Mixer, and there is a high correlation between how much the model\nlearns high-frequency features and its robustness against different frequency-based perturbations. 2)\nIntroducing convolutional or tokens-to-token blocks for learning high-frequency features in ViTs\ncan improve classiﬁcation accuracy but at the cost of adversarial robustness. 3) Modern CNN\ndesigns that borrow techniques from ViTs including activation function, layer norm, larger ker-\nnel size to imitate the global attention, and patchify the images as inputs, etc., could help bridge\nthe performance gap between ViTs and CNNs not only in terms of performance, but also cer-\ntiﬁed and empirical adversarial robustness. Moreover, we show adversarial training is also ap-\nplicable to ViT for training robust models, and sharpness-aware minimization can also help im-\nprove robustness, while pre-training with clean images on larger datasets does not signiﬁcantly\nimprove adversarial robustness. Codes available at https://github.com/RulinShao/\non-the-adversarial-robustness-of-visual-transformer .\n1 Introduction\nTransformers are originally applied in natural language processing (NLP) tasks as a type of deep neural network\n(DNN) mainly based on the self-attention mechanism (Vaswani et al., 2017; Devlin et al., 2018; Brown et al., 2020),\nand transformers with large-scale pre-training have achieved state-of-the-art results on many NLP tasks (Devlin et al.,\n2018; Liu et al., 2019; Yang et al., 2019; Sun et al., 2019). Recently, Dosovitskiy et al. (2020) applied a pure trans-\nformer directly to sequences of image patches (i.e., a vision transformer, ViT) and showed that the Transformer itself\ncan be competitive with convolutional neural networks (CNN) on image classiﬁcation tasks. Since then transformers\n1\narXiv:2103.15670v3  [cs.CV]  2 Nov 2022\nPublished in Transactions on Machine Learning Research (10/2022)\n1 M\n75 M\n150 M\nNumber of Parameters\nViT-S/16\nViT-B/16\nViT-L/16\nDeiT-S/16\nDist-DeiT-B/16\nSwin-S/4\nViT-B/16-Res\nT2T-ViT-14\nT2T-ViT-24SEResNet50\nViT-SAM-B/16\nHybrid\nMLP-Mixer-B/16\nConvNeXt-S\nCNN\nFigure 1: Robust accuracy v.s. clean accuracy. The robust accuracy is evaluated by AutoAttack (Croce & Hein,\n2020a). The “Hybrid” class includes CNN-ViT, T2T-ViT and Swin-T as introduced in Section 3. Models with attention\nmechanisms have their names printed at the center of the circles. ViTs have the best robustness against adversarial\nperturbations. Introducing other modules to ViT can improve clean accuracy but hurt adversarial robustness. CNNs\nare more vulnerable to adversarial attacks.\nhave been extended to various vision tasks and show competitive or even better performance compared to CNNs and\nrecurrent neural networks (RNNs) (Carion et al., 2020; Chen et al., 2020; Zhu et al., 2020). While ViT and its variants\nhold promise toward a uniﬁed machine learning paradigm and architecture applicable to different data modalities, it\nis critical to study the robustness of ViT against adversarial perturbations for safe and reliable deployment of many\nreal-world applications.\nIn this work, we examine the adversarial robustness of ViTs on image classiﬁcation tasks and make comparisons with\nCNN and MLP baselines. As highlighted in Figure 1, our experimental results illustrate the superior robustness of\nViTs than CNNs and MLP-Mixer in various settings, based on which we make the following important ﬁndings:\n• Features learned by ViTs contain less high-frequency information and beneﬁt adversarial robustness. ViTs achieve\na higher robust accuracy (RA) of 59.8% compared with a maximum improvement of 16.7% by CNNs in Figure 1.\nThey are also less sensitive to high-frequency adversarial perturbations.\n• Using denoised randomized smoothing (Salman et al., 2020), ViTs attain signiﬁcantly better certiﬁed robustness\nthan CNNs.\n• It takes the cost of adversarial robustness to improve the classiﬁcation accuracy of ViTs by introducing blocks to\nhelp learn low-level features as shown in Figure 1.\n• Increasing the proportion of transformer blocks in the model leads to better robustness when the model consists of\nboth transformer and CNN blocks. For example, the robust accuracy (RA) increases from 12.9% to 20.8% when 10\nadditional transformer blocks are added to T2T-ViT-14. However, increasing the size of a pure transformer model\ncannot guarantee a similar effect, e.g., the robustness of ViT-S/16 is better than that of ViT-B/16 in Figure 1.\n2\nPublished in Transactions on Machine Learning Research (10/2022)\n• The principle of adversarial training through min-max optimization (Madry et al., 2017; Zhang et al., 2019) can be\napplied to train robust ViTs. Pre-training on larger datasets without adversarial training does not improve adversarial\nrobustness. But sharpness-aware optimization (Foret et al., 2020; Chen et al., 2021) beneﬁts both the adversarial\nrobustness and clean accuracy of ViTs.\n2 Related Work\nTransformer (Vaswani et al., 2017) has achieved remarkable performance on many NLP tasks, and its robustness\nhas been studied on those NLP tasks. Hsieh et al. (2019); Jin et al. (2020); Shi & Huang (2020); Li et al. (2020);\nGarg & Ramakrishnan (2020); Yin et al. (2020) conducted adversarial attacks on transformers including pre-trained\nmodels, and in their experiments transformers usually show better robustness compared to other models based on\nLong short-term memory (LSTM) or CNN, with a theoretical explanation provided in Hsieh et al. (2019). However,\ndue to the discrete nature of NLP models, these studies are focusing on discrete perturbations (e.g., word or character\nsubstitutions) which are very different from small and continuous perturbations in computer vision tasks. Besides,\nWang et al. (2020a) improved the robustness of pre-trained transformers from an information-theoretic perspective,\nand Shi et al. (2020); Ye et al. (2020); Xu et al. (2020) studied the robustness certiﬁcation of transformer-based models.\nRecently, there are some concurrent works studying the adversarial robustness of ViTs. In the context of computer\nvision, one earliest relevant work is Alamri et al. (2020) which applies transformer encoder in the object detection task\nand reports better adversarial robustness. But their considered model is a mix of CNN and transformer instead of the\nViT model considered in this paper. Besides, the attacks they applied were relatively weak, and there lacks studies and\nexplanations on the beneﬁt of adversarial robustness brought by the transformers.\nRecently, there are many concurrent or follow-up works investigating the adversarial robustness of vision transformers.\nMany of them have cited a preprint version of our work and shown valuable discussion or extended experiments. We\nacknowledge their contributions below. Mahmood et al. (2021) test the adversarial robustness of the transformer in\nboth white-box and black-box settings, analyze the security of a simple ensemble defense of CNNs and transformers,\nand ﬁnd such ensemble defense is effective for the black-box setting. Qin et al. (2021) and Salman et al. (2021)\ninvestigate the adversarial robustness of ViTs through the lens of patch-based architectural structure. Herrmann et al.\n(2022) further designs a pyramid adversarial training with augmentation techniques to improve the ViT’s both sanity\nand robust performance. Naseer et al. (2021b) investigate whether the weak transferability of adversarial patterns\nfrom high-performing ViT models, as reported in our work, is a result of weak features or a weak attack. Aldahdooh\net al. (2021); Naseer et al. (2021a); Paul & Chen (2021); Tang et al. (2021); Mao et al. (2022) study the adversarial\nrobustness from different views, e.g., preprocessing defense methods (Aldahdooh et al., 2021), shape recognition\ncapability (Naseer et al., 2021a), natural adversarial examples (Paul & Chen, 2021; Tang et al., 2021), and detailed\nrobust components in ViTs (Mao et al., 2022). (Jeeveswaran. et al., 2022) studies object detection and semantic\nsegmentation tasks, and shows ViTs are more robust to distribution shifts, natural corruptions, and adversarial attacks\nin both tasks.\nOur work differs from these concurrent and related works by focusing more on the origin of the adversarial robustness\nof vision transformers. We discuss the superior adversarial robustness of ViTs through the lens of frequency, and ﬁnd\nthat the ViTs are especially robust to high-frequency adversarial perturbations. We also apply denoised randomized\nsmoothing and show ViT also has superior certiﬁed robustness than CNN models. Our study provides insight on\nunderstanding the source of ViT’s adversarial robustness and designing more robust architectures. And we cover\nvarious baselines in the experiments for comprehensive comparison, including the recently proposed ConvNeXt (Liu\net al., 2022), MLP-Mixer (Tolstikhin et al., 2021), and Swin-Transformers (Liu et al., 2021), etc.\nTo the best of our knowledge, our work is the ﬁrst study that investigates the certiﬁed adversarial robustness of trans-\nformers on computer vision tasks, and explains the source of ViTs’ superior adversarial robustness from a frequency\nperspective. We also show that ViTs have superior adversarial robustness (against small perturbations in the input\npixel space) in both white-box and black-box settings. And we investigate various SOTA models including the re-\ncently proposed ConvNeXt (Liu et al., 2022) and MLP-Mixer (Tolstikhin et al., 2021) in the experiments.\n3\nPublished in Transactions on Machine Learning Research (10/2022)\n3 Model Architectures\nWe ﬁrst review the architectures of models investigated in our experiments. A summary of the target models investi-\ngated in the main text is shown in Table 1. More experimental results of ViT variants can be found in Appendix B. The\nweights of all the investigated models are all publicly available checkpoints (Paszke et al., 2019; Wightman, 2019)\nand they are well-tuned separately by their designers. Different from Bai et al. (2021) which uses the same training\nsetting for all models, we follow Paul & Chen (2021) to take different neural network models trained and tuned to the\noptimum individually rather than using a common but potentially suboptimal setting, since one setting could be biased\nto some models as well.\nTable 1: Comparison of the target models investigated in the main text. More variants (Deit, ViT-SAM, Swin-ViT,\netc.) can be found in Appendix B.\nViT backbone Pretraining\nModel Layers Hidden size Attention Params Pretraining dataset Scale\nViT-S/16 8 786 Self-attention 49M ImageNet-21K 14M\nViT-B/16 12 786 Self-attention 87M ImageNet-21K 14M\nViT-L/16 24 1024 Self-attention 304M ImageNet-21K 14M\nViT-B/16-Res 12 786 Self-attention 87M ImageNet-21K 14M\nT2T-ViT-14 14 384 Self-attention 22M - -\nT2T-ViT-24 24 512 Self-attention 64M - -\nDeiT-S/16 12 384 Self-attention 22M - -\nDist-DeiT-B/16 12 768 Self-attention 87M - -\nSwin-S/4 (2,2,18,2) 96 Self-attention 50M - -\nSEResNet50 - - Squeeze-and-Excitation 28M - -\nResNeXt-32x4d-ssl - - - 25M YFCC100M 100M\nResNet50-swsl - - - 26M IG-1B-Targeted 940M\nResNet18 - - - 12M - -\nResNet50-32x4d - - - 25M - -\nShufﬂeNet - - - 2M - -\nMobileNet - - - 4M - -\nVGG16 - - - 138M - -\n3.1 Vision Transformers\nFor vision transformer, we consider the original ViT (Dosovitskiy et al., 2020) and its variants:\nVanilla ViT with different training schemes (ViT, DeiT, ViT-SAM) : The original ViT (Dosovitskiy et al., 2020)\nmostly follows the original design of Transformer (Vaswani et al., 2017; Devlin et al., 2018) on language tasks. For\na 2D image xi ∈RH×W×C (1 ≤ i ≤ N) with resolution H ×W and C channels, it is divided into a sequence\nof N = H·W\nP2 ﬂattened 2D patches of size P ×P, xi ∈RP2·C (1 ≤i ≤N). The patches are encoded into patch\nembeddings with a simple convolutional layer, where the kernel size and stride of the convolution is exactly P ×P.\nDeiT (Touvron et al., 2021) further improves the ViT’s performance using data augmentation or distillation from\nCNN teachers with an additional distillation token. We investigate ViT-{S,B,L}/16, DeiT-S/16 and Dist-DeiT-B/16\nas deﬁned in the corresponding papers in the main text and discuss other structures in Appendix B. ViT-SAM (Chen\net al., 2021) uses sharpness-aware minimization (Foret et al., 2020) to train ViTs from scratch on ImageNet without\nlarge-scale pretraining or strong data augmentations. We include ViT-SAM-B/16 in the main text.\nHybrid of CNN and ViT (CNN-ViT): Dosovitskiy et al. (2020) also proposed a hybrid architecture for ViTs by\nreplacing raw image patches with patches extracted from a CNN feature map. This is equivalent to adding learned\nCNN blocks to the head of ViT. We investigate ViT-B/16-Res in our experiments, where the input sequence is obtained\nby ﬂattening the spatial dimensions of the feature maps from ResNet50.\nHybrid of T2T and ViT (T2T-ViT):Yuan et al. (2021) proposed to overcome the limitations of the simple tokeniza-\ntion in ViTs, by progressively structurizing an image to tokens with a token-to-token (T2T) module, which recursively\n4\nPublished in Transactions on Machine Learning Research (10/2022)\naggregates neighboring tokens into one token such that low-level structures can be better learned. T2T-ViT was shown\nto perform better than ViT when trained from scratch on a midsize dataset. We investigate T2T-ViT-14 and T2T-ViT-24\nin our experiments.\nHybrid of shifted windows and ViT (Swin-T): Liu et al. (2021) computes the representations with shifted windows,\nwhich brings greater efﬁciency by limiting self-attention computation to non-overlapping local windows while also\nallowing for cross-window connection. We investigate Swin-S/4 in the main text and discuss other structures in\nAppendix B.\n3.2 Baselines\nWe study several CNN models for comparison, including ResNet18 (He et al., 2016), ResNet50-32x4d (He et al.,\n2016), ShufﬂeNet (Zhang et al., 2018), MobileNet (Howard et al., 2017), and VGG16 (Simonyan & Zisserman, 2014).\nWe also consider the SEResNet50 model, which uses the Squeeze-and-Excitation (SE) block (Hu et al., 2018) that ap-\nplies attention to channel dimensions. The recently proposed MLP-Mixer (Tolstikhin et al., 2021) and ConvNeXt (Liu\net al., 2022) are also included for comparison.\nThe aforementioned MLP and CNNs are all trained on ImageNet from scratch. For a better comparison with pre-\ntrained transformers, we also consider two CNN models pre-trained on larger datasets: ResNeXt-32x4d-ssl (Yalniz\net al., 2019) pre-trained on YFCC100M (Thomee et al., 2015), and ResNet50-swsl pre-trained on IG-1B-Targeted\n(Mahajan et al., 2018) using semi-weakly supervised methods (Yalniz et al., 2019), and then ﬁne-tuned on ImageNet.\n4 Adversarial Robustness Evaluation Methods\nWe consider the commonly usedℓ∞-norm bounded adversarial attacks to evaluate the robustness of target models. An\nℓ∞ attack is usually formulated as solving a constrained optimization problem:\nmax\nxadv\nL\n(\nxadv,y\n)\ns.t.\nxadv −x0\n\n∞ ≤ϵ, (1)\nwhere x0 is a clean example with label y, and we aim to ﬁnd an adversarial example xadv within an ℓ∞ ball with\nradius ϵcentered at x0, such that the loss of the classiﬁer L\n(\nxadv,y\n)\nis maximized. We consider untargeted attack\nin this paper, so an attack is successful if the perturbation successfully changes the model’s prediction. The attacks as\nwell as a randomized smoothing method used in this paper are listed below.\nProjected Gradient Descent Attack (Madry et al., 2017) Projected Gradient Decent (PGD) attack (Madry et al.,\n2017) solves Eq. 1 by iteratively taking gradient ascent:\nxadv\nt+1 = Clipx0,ϵ(xadv\nt + α·sgn\n(\n∇xJ\n(\nxadv\nt ,y\n))\n), (2)\nwhere xadv\nt stands for the solution after titerations, and Clipx0,ϵ(·) denotes clipping the values to make each xadv\nt+1,i\nwithin [x0,i −ϵ,x0,i + ϵ], according to the ℓ∞ threat model. As a special case, Fast Gradient Sign Method (FGSM)\n(Goodfellow et al., 2014) uses a single iteration with t= 1.\nAutoAttack (Croce & Hein, 2020a) AutoAttack (Croce & Hein, 2020a) is a parameter-free ensemble of diverse\nattacks, including two variants of PGD attacks (APGD-CE, APGD-DLR), an optimization based attack (FAB(Croce\n& Hein, 2020b)) and a query based black-box attack (Square Attack(Croce et al., 2019)).\nTransfer Attack We consider the transfer attack which studies whether an adversarial perturbation generated by at-\ntacking the source model can successfully fool the target model. This test not only evaluates the robustness of models\nunder the black-box setting, but also becomes a sanity check for detecting the obfuscated gradient phenomenon (Atha-\nlye et al., 2018). Previous works have demonstrated that single-step attacks like FGSM enjoys better transferability\nthan multi-step attacks (Kurakin et al., 2017). We thus use FGSM for transfer attack in our experiments.\nFrequency-Filtered Attack for Frequency Analysis In our frequency study, we conduct PGD attack with an addi-\ntional frequency ﬁlter to force the adversarial perturbations being within a speciﬁc frequency domain (a high-frequency\n5\nPublished in Transactions on Machine Learning Research (10/2022)\nFigure 2: Filters for the frequency-based attack. The frequencies corresponding to the red part are ﬁltered out, and the\nfrequencies corresponding to the green part can pass through. “Full Pass” means all of the frequencies are preserved.\n“Low Pass” means only low-frequency components are preserved. “High Pass” preserves the high-frequency part.\ndomain or a low-frequency domain):\nxadv\nfreq = IDCT(DCT(xadv\npgd −x0) ⊙Mf) + x0, (3)\nwhere DCT and IDCT stand for discrete cosine transform and inverse discrete cosine transform respectively, xadv\npgd\nstands for the adversarial example generated by PGD, and Mf stands for the mask metric deﬁned the frequency ﬁlter\nas illustrated in Figure 2.\nDenoised Randomized Smoothing(Salman et al., 2020) for Certiﬁed Robustness Analysis We also provide prov-\nable certiﬁed robustness analysis using denoised randomized smoothing (Salman et al., 2020). The model is certiﬁed\nto be robust with high probability for perturbations within the radius, so the robustness is evaluated as the certiﬁed\nradius. We follow Salman et al. (2020) to train a DnCNN (Zhang et al., 2017) denoiser Dθfor each pre-trained model\nf with stability objective LStab:\nLStab = E(xi,yi)∈D,δLCE(f(Dθ(xi + δ)),f(xi)) (4)\nwhere δ∼N(0,σ2I) follows Gaussian distribution. Then randomized smoothing is applied on the denoised classiﬁer\nf ◦Dθfor robustness certiﬁcation:\ng(x) = arg max\nc∈Y\nP[f(Dθ(x+ δ)) = c] where δ∼N(0,σ2I). (5)\nThe certiﬁed radius (Cohen et al., 2019) is then calculated for the smoothed classiﬁer as:\nR= σ\n2 (Φ −1(pA) −Φ −1(pB)), (6)\nwhere Φ −1 is the inverse of the standard Gaussian CDF, pA = P(f(x+ δ) = cA) is the conﬁdence of the top-1\npredicted class cA, and pB = maxc̸=cA P(f(x+δ) = c) is the conﬁdence for the second top class. Accordingly, given\na perturbation radius, the certiﬁed accuracy under this perturbation radius can be evaluated by comparing the given\nradius to the certiﬁed radius R.\n5 Experiments\nIn this section, we compare the adversarial robustness of ViTs and CNNs from different perspectives: First, we show\nViTs are more robust to high-frequency perturbations than CNNs using crafted frequency-ﬁltered attacks and feature\nvisualization. Second, we conduct a comprehensive empirical study on the comparison between ViTs and CNNs\nregarding the adversarial robustness against diverse white-box attacks and black-box attacks, and the transferability\nof the adversarial examples from ViTs to CNNs and vice versa. Finally, we provide a provable certiﬁed robustness\ncomparison using denoised randomized smoothing.\n6\nPublished in Transactions on Machine Learning Research (10/2022)\nFigure 3: Feature visualization: The learned low-level structure features are highlighted in blue (obviously perceptible)\nand green (minorly perceptible). The CNNs in the ﬁrst row learn more low-level features compared with the ViTs in\nthe second row. The ViTs pay more attention to the low-level structures and their feature maps become noisier when\nResNet features are introduced (ViT-B/16-Res) or neighboring tokens are aggregated into one token recursively (T2T-\nViT-24).\nUnless otherwise speciﬁed, Clean Accuracy (CA) stands for the accuracy evaluated on the entire ImageNet-1k (Deng\net al., 2009) test set, Robust Accuracy (RA) stands for the accuracy on the adversarial examples generated with 1,000\ntest samples. Higher RA stands for better adversarial robustness. Adversarial training and evaluation results on\nCIFAR-10 are reported in Appendix F and Appendix C.\n5.1 Frequency Study Using Filtered-PGD and Feature Visualization\nWang et al. (2020b) has shown that CNN models could take up high-frequency patterns that are almost imperceptible\nto humans but have distribution correlation with labels, leading to a trade-off between robustness and accuracy. How-\never, we show the original ViT models possess superior adversarial robustness against high-frequency perturbations\ncompared with CNNs. We also show that some variants that introduce non-transformer modules (e.g., ResNet blocks\nand T2T blocks) to ViTs to improve clean accuracy, can diminish ViTs’ original adversarial robustness against high-\nfrequency perturbations, leading to inferior adversarial robustness of hybrid ViTs (e.g., ResViTs and T2T-ViTs). We\ncraft a frequency-ﬁltered PGD attack to study models’ resistance to different frequency components. Besides, we use\nfeature visualization to facilitate the illustration of our hypothesis, showing that ViTs are paying less attention to the\nhigh-frequency patterns in the images.\nTable 2: RA (%) of the target models against frequency-ﬁltered PGD attack in our frequency study. In the “Low-pass”\ncolumn, only low-frequency adversarial perturbations are preserved and added to the input images. In the “High-pass”\ncolumn, only high-frequency perturbations are preserved. The “Full-pass” mode preserves all frequency and is the\nsame as the traditional PGD attack. We set the attack step ﬁxed to 40 and vary the attack radius to different values.\nLow-pass High-pass Full-pass\nAttack Radius 0.001 0.003 0.005 0.01 0.1 0.001 0.003 0.005 0.01 0.1 0.001 0.003 0.005 0.01 0.1\nViT-S/16 74.0 68.1 64.7 59.8 56.2 70.8 60.7 50.6 40.4 23.4 55.4 24.6 10.2 1.0 0.0\nViT-B/16 71.9 64.3 60.3 55.8 49.6 66.3 53.1 44.0 33.4 21.9 48.9 14.6 6.0 0.9 0.0\nViT-L/16 74.9 64.1 58.3 50.2 42.0 72.9 62.3 56.6 47.5 28.9 55.1 23.4 9.9 1.8 0.0\nViT-B/16-Res 83.1 81.4 80.4 79.0 75.1 62.9 29.2 16.0 7.3 3.3 45.5 8.4 2.3 0.1 0.0\nT2T-ViT-14 78.0 77.2 76.0 75.8 74.3 49.6 20.5 9.1 3.1 1.4 37.1 7.0 1.8 0.0 0.0\nT2T-ViT-24 80.2 79.2 78.4 77.7 74.4 58.3 31.1 17.7 8.2 3.1 47.7 12.3 3.4 0.2 0.0\nMLP-Mixer-B/1669.4 64.7 62.4 60.0 59.8 56.6 32.5 19.5 6.7 1.3 34.5 3.8 0.0 0.0 0.0\nConvNeXt-S 80.3 79.0 77.9 76.4 74.3 53.1 24.4 14.4 6.8 6.2 15.9 0.0 0.0 0.0 0.0\nResNet50-swsl 78.2 74.9 73.7 71.6 72.5 45.3 12.4 5.0 2.2 3.5 24.7 2.9 1.4 0.4 0.0\nResNet50-32x4d 75.0 66.3 62.7 59.0 61.5 47.7 17.1 7.4 3.3 3.5 28.2 3.2 1.2 0.4 0.1\n7\nPublished in Transactions on Machine Learning Research (10/2022)\n5.1.1 Frequency Study\nViTs are more resistant to high-frequency perturbations and have less bias towards high-frequency features\nthat have spurious correlation with labels. We design a frequency study to verify our hypothesis that ViTs are\nadversarially more robust compared with CNNs and MLP-Mixer because ViTs learn less high-frequency features. As\ndeﬁned in equation 3, for adversarial perturbations generated by PGD attack, we ﬁrst project them to the frequency\ndomain by DCT. We design three frequency ﬁlters illustrated in Figure 2: the full-pass ﬁlter, the low-pass ﬁlter, and\nthe high-pass ﬁlter. We take 32 ×32 pixels in the low-frequency area out of 224 ×224 pixels as the low-pass ﬁlter,\nand 192 ×192 pixels in the high-frequency area as the high-pass ﬁlter. Each ﬁlter allows only the corresponding\nfrequencies to pass through – when the adversarial perturbations go through the low-pass ﬁlter, the high-frequency\ncomponents are ﬁltered out and vice versa, and the full-pass ﬁlter makes no change. We then apply these ﬁlters to the\nfrequencies of the perturbations, and project them back to the spacial domain with the IDCT. We test the RA under\ndifferent frequency areas, and show the results in Table 2.\nThe RA of ViTs are much higher in the “High-pass” column when only the high-frequencies of the perturbations\nare preserved. In contrast, CNNs show signiﬁcantly lower RA in the “High-pass” column than in the “Low-pass”\ncolumn. It reﬂects that CNNs tend to be more sensitive to high-frequency adversarial perturbations compared to ViTs.\nAs shown in the Table 2, CNNs have moderate robust accuracy drop w.r.t. low-frequency perturbations but severe\ndecrease against high-frequency perturbations when increasing the perturbation rates. For example, when increasing\nthe attack radius from 0.001 to 0.01, the robust accuracy of ResNet50-32x4d decreases from 75.0% to 59.0% (by\n16.0%) for low-pass adversarial attack, but 47.0% to 3.3% (by 43.7%) for high-pass adversarial attack, indicating the\nhigh-frequency perturbations are the main cause to the low robust accuracy for CNNs. However, when increasing\nthe attack radius from 0.001 to 0.01, ViT-B/16 decreases from 71.9% to 55.8% (by 16.1%) for low-pass adversarial\nattack, but only 66.3% to 33.4% (by 32.9%) for high-frequency-perturbations, showing much better resistance to\nhigh-frequency perturbations compared with CNNs.\nIntroducing CNN or T2T blocks makes ViTs less robust to high-frequency perturbations. One interesting and\nperhaps surprising ﬁnding is that ViTs have worse robustness when modules that claimed to help learning local struc-\ntures are added ahead of the transformer blocks. For example, T2T-ViT adds several T2T modules to the head of ViT\nwhich iteratively aggregates the neighboring tokens into one token in each local perceptive ﬁeld. ViT-B/16-Res takes\nthe features generated by ResNet as inputs, which has the same effect as incorporating a trained CNN layer in front\nof the transformer blocks. Both modules help to learn local structures like edges and lines (Yuan et al., 2021). We\nobserve that ResNet and T2T modules that could help improve the CA of the hybrid ViTs makes the models more sen-\nsitive to high-frequency perturbations. T2T-ViT-14, T2T-ViT-24 and ViT-B/16-Res have lower RA in the “High-pass”\ncolumn and higher RA in the “Low-pass” column compared with vanilla ViTs, which correlates with the previous\nobservations that high-frequency features are less adversarially robust. When adding more transformer blocks to the\nT2T-ViT model, the model becomes less sensitive to the high frequencies of the adversarial perturbations, e.g., the\nT2T-ViT-24 has an8.7% higher RA than that of the T2T-ViT-14 in the “High-pass” column.\nOne possible explanation is that the introduced modules improve the classiﬁcation accuracy by remembering the\nhigh-frequency patterns that repeatedly appear in the training dataset. These structures such as edges and lines are\nhigh-frequency and sensitive to perturbations (Wang et al., 2020b). Learning such features makes the model more\nvulnerable to adversarial attacks. Examination of this hypothesis is conducted though feature visualization.\nIncreasing the Proportion of Transformer Blocks Can Improve Robustness Hendrycks et al. (2019) mentioned\nthat larger model does not necessarily imply better robustness. It can be conﬁrmed by our experiments where ViT-\nS/16 shows better robustness than larger ViT-B/16 under both PGD attack and AutoAttack. In this case, simply\nadding transformer blocks to the classiﬁer cannot guarantee better robustness. However, we recognize that for mixed\narchitecture that has both T2T and transformer blocks, it is useful to improve adversarial robustness by increasing the\nproportion of the transformer blocks in the model. As shown in Table 2 (we will also show similar observations in\nTables 3 and 4), T2T-ViT-24 has higher RA than T2T-ViT-14 under both attacks. Besides the transformer block, we\nﬁnd that other attention mechanism modules such as SE block also improves adversarial robustness – as SEResNet50\nhas the least proportion of attention, the RA of SEResNet50 is lower than ViT and T2T-ViT models but higher than\nother pure CNNs. These two ﬁndings are coherent since the attention mechanism is fundamental in transformer blocks.\n8\nPublished in Transactions on Machine Learning Research (10/2022)\n5.1.2 Feature Visualization\nWe hypothesis that ViTs are more resistant to high-frequency perturbations because they are learning less high-\nfrequency features that have high correlation with labels but less semantic meanings. To facilitate the illustration,\nwe follow the work of Yuan et al. (2021) to visualize the learned features from the ﬁrst blocks of the target models\nin Figure 3. We resize the input images to a resolution of 224 ×224 for CNNs and a resolution of 1792 ×1792\nfor ViTs and T2T-ViTs such that the feature maps from the ﬁrst block are in the same shape of 112 ×112. Features\nwith high-frequency patterns like lines and edges are highlighted in blue (obviously perceptible) and green (minorly\nperceptible). As shown in Figure 3, CNNs like ResNet50-swsl and ResNet50-32x4d learn features with obvious edges\nand lines. While it is hard to observe such information in the feature maps learned by ViT-B/16.\nThe frequency study combined with the feature visualization shows that a model’s vulnerability against adversarial\nperturbations is relative to the model’s tendency to learn high-frequency low-level features that could have correlation\nwith labels but little semantic information. Techniques that help the model learn such features may improve the\naccuracy on clean data but at the risk of sacriﬁcing adversarial robustness. We are still facing the trade-off between\nCA and RA.\n5.2 Empirical Study: Adversarial Robustness under Various Adversarial Attacks\nIn this section, we test adversarial robustness against PGD, AutoAttack and transfer attack, and provide observations\nwe drew from this empirical study.\n5.2.1 Robustness under PGD and AutoAttack\nSettings We take attack radius ϵ ∈ {0.001,0.003,0.005,0.01}for PGD and AutoAttack evaluation. For PGD\nattack, we ﬁx the attack steps to niter = 40 with other parameters following the default setting of the implementation\nin Foolbox (Rauber et al., 2020). No hyper-parameter tuning is needed in AutoAttack.\nResults We present the results of PGD and AutoAttack in Table 3 and Table 4 respectively. The RA is approximately\n0.0% on all the models when ϵ = 0 .01 is large,indicating models trained without any adversarial augmentations\nare vulnerable to large perturbations. However, such vulnerability doesn’t hold equally for all models within mild\nperturbations: For smaller attack radii, ViT models have higher RA than CNNs under both PGD attack and\nAutoAttack. For example, when ϵ= 0.001, the RA for ViT-S/16 is 55.4% while the RA for CNNs is at most 30.0%.\nMoreover, under the same attack radius, the RA of AutoAttack for ViT-S/16 is48.1% compared to 6.1% of ShufﬂeNet,\nwhich is a large gap. We visualize the clean/robust accuracy tradeoff and model size of these models in Figure 1, and\nViT models are noticeably on the upper right over CNN models.\nIntroducing ResNet or T2T blocks decreases the RA under both PGD and AutoAttack.When the features learned\nby ResNet are introduced, the RA of ViT-B/16 decreases from 48.9% to 45.5% of ViT-B/16-Res under PGD attack,\nand from 39.8% to 27.7% under AutoAttack, with attack radius ϵ= 0.001. A similar phenomenon can be observed by\ncomparing the RA of ViTs and T2T-ViTs. The RA of T2T-ViT-14 is18.3% lower under PGD attack and 35.2% lower\nunder AutoAttack compared with the RA of ViT-S/16, under attack radiusϵ= 0.001.\nWe also verify that pre-training with clean images does not improve robustness. (Dosovitskiy et al., 2020) points\nout that pre-training is critical for ViTs to achieve competitive standard accuracy with CNNs trained from scratch.\nHowever, we note that pre-training doesn’t bring better robustness to ViTs. To illustrate this point, we include CNNs\npre-trained on large datasets and ﬁne-tuned on ImageNet-1k to check the effect of pre-training on adversarial robust-\nness. CNNs pre-trained on large datasets IG-1B-Targeted (Mahajan et al., 2018) and YFCC100M (Thomee et al.,\n2015) that are even larger than ImageNet-21k used by ViT, ResNet50-swsl and ResNeXt-32x4d-ssl, still have similar\nor even lower RA than ResNet18 and ResNet50-32x4d that are not pre-trained. This supports our observation that\npre-training in its current form may not be able to improve adversarial robustness, which is also in accordance with\nHendrycks et al. (2019). Besides, the results show that SAM can further improve model’s adversarial robustness. This\nis because the SAM objective is formulated as a min-max optimization similar to adversarial training: but instead of\nadding adversarial perturbations to the input space, SAM adds adversarial perturbations to the weights.\n9\nPublished in Transactions on Machine Learning Research (10/2022)\nTable 3: Clean Accuracy (%) and Robust Accuracy ( %) of target models against 40-step PGD attack with different\nradii. More results of the SOTA ViTs can be found in Appendix B.\nCA RA against PGD\nAttack Radius 0.001 0.003 0.005 0.01\nViT-S/16 77.6 55.4 24.6 10.2 1.0\nViT-B/16 75.7 48.9 14.6 6.0 0.9\nViT-L/16 79.2 55.1 23.4 9.9 1.8\nViT-SAM-B/16 76.7 63.4 37.0 20.1 3.8\nViT-B/16-Res 84.0 45.5 8.4 2.3 0.1\nT2T-ViT-14 80.1 37.1 7.0 1.8 0.0\nT2T-ViT-24 82.2 47.7 12.3 3.4 0.2\nDeit-S/16 77.7 48.9 17.6 7.1 1.1\nDist-Deit-B/16 81.8 55.6 17.7 4.5 0.4\nSwin-S/4 81.8 40.0 12.4 3.2 0.2\nMLP-Mixer-B/16 73.8 41.9 10.7 4.3 0.4\nConvNeXt-S 82.7 42.4 8.1 2.6 0.0\nSEResNet50 75.7 35.4 4.9 0.8 0.1\nResNeXt-32x4d-ssl 80.3 23.0 2.9 1.2 0.5\nResNet50-swsl 81.2 24.7 2.9 1.4 0.4\nResNet18 70.0 24.9 2.0 0.6 0.1\nResNet50-32x4d 77.6 28.2 3.2 1.2 0.4\nShufﬂeNet 69.4 15.0 0.6 0.2 0.0\nMobileNet 71.9 16.7 0.4 0.0 0.0\nVGG16 71.6 26.3 3.2 1.3 0.0\nTable 4: Clean Accuracy (%) and Robust Accuracy ( %) of target models against AutoAttack with different attack\nradii. More results of the SOTA ViTs can be found in Appendix B.\nCA RA against AutoAttack\nAttack Radius 0.001 0.003 0.005 0.01\nViT-S/16 77.6 48.1 6.0 0.5 0.0\nViT-B/16 75.7 39.8 5.4 0.6 0.0\nViT-L/16 79.2 46.6 8.5 1.0 0.0\nViT-SAM-B/16 76.7 59.8 26.0 8.4 0.1\nViT-B/16-Res 84.0 27.7 0.9 0.0 0.0\nT2T-ViT-14 80.1 12.9 0.1 0.0 0.0\nT2T-ViT-24 82.2 20.8 0.3 0.0 0.0\nDist-Deit-S/16 79.3 43.1 3.7 0.2 0.0\nDist-Deit-B/16 81.8 42.7 3.4 0.2 0.0\nSwin-S/4 81.8 7.9 0.1 0.0 0.0\nMLP-Mixer-B/16 73.8 34.5 3.8 0.0 0.0\nConvNeXt-S 82.7 15.9 0.0 0.0 0.0\nSEResNet50 75.7 21.6 0.6 0.0 0.0\nResNeXt-32x4d-ssl 80.3 6.5 0.0 0.0 0.0\nResNet50-swsl 81.2 8.1 0.0 0.0 0.0\nResNet18 70.0 14.3 0.4 0.0 0.0\nResNet50-32x4d 77.6 13.2 0.2 0.0 0.0\nShufﬂeNet 69.4 6.1 0.0 0.0 0.0\nMobileNet 71.9 7.8 0.0 0.0 0.0\nVGG16 71.6 16.7 0.5 0.0 0.0\n5.2.2 Transferability of Adversarial Examples from ViTs to CNNs and Vice Versa\nWe also conduct transfer attack to test the adversarial robustness in the black-box setting as described in Section 4.\nWe consider attacks with ℓ∞-norm perturbation no larger than 0.1 and present the results in Figure 4. When the\n10\nPublished in Transactions on Machine Learning Research (10/2022)\nFigure 4: Target model error rate on adversarial examples (i.e., 1.0 - RA) against transfer attack using FGSM with\ndifferent attack radii. The rows stand for the surrogate models used to generated adversarial examples. The columns\nstand for the target models. Darker rows correlate to the source models that generate more transferable adversarial\nexamples. Darker columns mean that the target models are more vulnerable to transfer attack.“Res50-ssl” and “Res50-\nswsl” are in short of “ResNeXt-32x4d-ssl” and “ResNet50-swsl” respectively. Results for more radii can be found in\nAppendix E.\nViTs serve as the target models and CNNs serve as the source models, as shown in the lower left of each subplot,\nthe RER of the transfer attack is quite low. On the other hand, when the ViTs are the source models, the adversarial\nexamples they generate have higher RER when transferred to other target models. As a result, the ﬁrst three rows\nand the last seven columns are darker than the others. Besides, for the diagonal lines in the ﬁgure where FGSM\nactually attacks the models in a white-box setting, we can observe that ViTs are less sensitive to attack with smaller\nradii compared to CNNs, and T2T modules make ViTs more robust to such one-step attack. In addition, adversarial\nexamples transfer well between models with similar structures. As ViT-S/16, ViT-B/16 and ViT-L/16 have similar\nstructures, the adversarial examples generated by them can transfer well to each other, and it is similar for T2T-ViTs\nand CNNs respectively.\nThe ﬁnding that ViTs rely less on high-frequency patterns also helps us understand why ViT adversarial examples are\nbetter transferred to CNNs as shown in Figure 4. As studied in (Ilyas et al., 2019), they found and called those features\nless prone to be attacked by adversarial attacks as “robust features”, which is consistent with our understanding of\nthe “high-level” features that are preferable for neural networks to learn. And those noise-like “non-robust” features\nare similar to our high-frequency features that contain low-level information. Since the “non-robust” high-frequency\npatterns are generated with speciﬁc models, adversarial perturbations with regard to such patterns should be assumed\nto be harder to transfer. Since ViTs are less sensitive to such high-frequency patterns, we assume the adversarial\nperturbations will be forced to rely less on model-speciﬁc feature patterns and thus be easier to transfer. It is interesting\nto notice that ViTs’ adversarial examples could be even stronger than the CNN counterparts with large attack radii,\nand we think it’s because the transferable adversarial perturbations need larger attack radii which could be further\nexplored.\n5.3 Provably Certiﬁed Robustness Comparison Using Denoised Randomized Smoothing\nSettings We train the denoisers using the stability objective for25 epochs with a noise level ofσ= 0.25, learning rate\nof 10−5 and a batch size of 64. We iterate through the ImageNet dataset to calculate the corresponding radii according\nto Eq. 6, and report the certiﬁed accuracy versus different radii as deﬁned in (Salman et al., 2020) in Table 5.\nResults As shown in Table 5 ViT-S/16 has higher certiﬁed accuracy than ResNet18 and ResNet50, showing better\ncertiﬁed robustness of vision transformers over CNNs. We also found that, in the same settings, training a Gaussian\ndenoiser for the ResNet is harder than for the ViT-S/16. Accuracy of ViT-S/16 with denoiser at noise of σ = 0.25 is\n11\nPublished in Transactions on Machine Learning Research (10/2022)\nTable 5: Certiﬁed robust accuracy w.r.t. different radii using denoised randomized smoothing (Salman et al., 2020)\n(σ= 0.25). Pure ViTs are highlighted with gray shadows.\nRadius 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9\nViT-S/16 0.5944 0.5452 0.4936 0.4424 0.3972 0.3428 0.2820 0.2044 0.0\nDeiT-S/16 0.6352 0.5880 0.5380 0.4948 0.4476 0.4004 0.3408 0.2604 0.0\nDist-DeiT-S/16 0.6072 0.5600 0.5176 0.4716 0.4172 0.3620 0.3108 0.2360 0.0\nSAM-ViT 0.6320 0.6040 0.5520 0.5360 0.5040 0.4600 0.4160 0.3560 0.0\nT2T-ViT-14 0.4044 0.3816 0.3580 0.3348 0.3044 0.2660 0.2276 0.1816 0.0\nVGG16 0.3772 0.3220 0.2796 0.2372 0.1964 0.1580 0.1176 0.0768 0.0\nResNet50 0.4584 0.4096 0.3604 0.3140 0.2676 0.2208 0.1820 0.1268 0.0\nSEResNet50 0.4880 0.4440 0.3880 0.3360 0.2920 0.2680 0.2160 0.1760 0.0\nConvNext-S 0.5160 0.4760 0.4320 0.3920 0.3480 0.2880 0.2440 0.1880 0.0\n64.84% (4.996% without any denoiser), while accuracy of ResNet50 and ResNet18 with denoiser at the same noise is\n47.782% (5.966% without any denoiser).\nPure ViTs possess better certiﬁed robust accuracy than CNNs. As shown in Table 5, pure ViTs (ViT-S/16, DeiT-\nS/16, Dist-DeiT-S/16 and SAM-ViT) have higher certiﬁed robust accuracy than CNNs. Introducing T2T blocks to\nViTs can cause the model to have inferior certiﬁed robust accuracy even than CNNs especially for small radii, e.g.,\nradii smaller than 0.5. While sharpness-aware minimization helps further improve ViTs’ certiﬁed robust accuracy.\nModern CNN design helps bridge the performance gap between CNNs and ViTs. The design of modern non-\ntransformer models, e.g. ConvNext, has borrowed many techniques from transformers. For example, using larger\nkernel size to imitate the global attention mechanism of transformers, following the transformers to change stem to\n“Patchify”, using invertible bottleneck as transformers do, substituting BN with LN, replacing ReLU with GeLU, etc.\nAll these changes are targeted to imitate the transformer’s operation without introducing the attention blocks. MLP-\nMixer also does similar modiﬁcations. Our experiments (Table 3, Table 4 and Table 5) show that such modiﬁcation\nhelps bridge the performance gap between CNNs and transformers not only in terms of clean accuracy, but also\nempirical and certiﬁed robust accuracy. We also show that CNNs with attention mechanism, i.e. SEResNet50 in our\nexperiments, also has better certiﬁed robustness than CNNs.\n5.4 Extended Analysis\nIn Appedix A, we also conduct a sanity check to verify that ViT’s improvement is not caused by insufﬁcient attack\noptimization, and an explanation from Hopﬁeld network perspective is provided. Besides, we verify that adversarial\ntraining could be directly applied to ViTs in Appendix F.\n6 Conclusion\nThis paper presents a comprehensive study on the robustness of ViTs against adversarial perturbations. Our results in-\ndicate that ViTs are more robust than CNNs on the considered adversarial attacks and certiﬁed robustness settings. We\nshow that the features learned by ViTs contain less low-level information, contributing to improved robustness against\nadversarial perturbations that often contain high-frequency components. Also, introducing convolutional blocks in\nViTs can facilitate learning low-level features but has a negative effect on adversarial robustness and makes the mod-\nels more sensitive to high-frequency perturbations. Moreover, both the sanity performance and the (certiﬁed and\nempirical) adversarial robustness are improved in the modern CNN designs that leverage techniques from ViTs to\nimitate the global attention behavior. We also demonstrate adversarial training for ViT. Our work provides a deep\nunderstanding of the intrinsic robustness of ViTs and can be used to inform the design of robust vision models based\non the transformer structure.\n12\nPublished in Transactions on Machine Learning Research (10/2022)\nReferences\nRima Alaifari, Giovanni S Alberti, and Tandri Gauksson. Adef: an iterative algorithm to construct adversarial defor-\nmations. arXiv preprint arXiv:1804.07729, 2018.\nFaisal Alamri, Sinan Kalkan, and Nicolas Pugeault. Transformer-encoder detector module: Using context to improve\nrobustness to adversarial attacks on object detection. arXiv preprint arXiv:2011.06978, 2020.\nAhmed Aldahdooh, Wassim Hamidouche, and Olivier Deforges. Reveal of vision transformers robustness against\nadversarial attacks. arXiv preprint arXiv:2106.03734, 2021.\nAnish Athalye, Nicholas Carlini, and David Wagner. Obfuscated gradients give a false sense of security: Circumvent-\ning defenses to adversarial examples. International Coference on International Conference on Machine Learning,\n2018.\nYutong Bai, Jieru Mei, Alan L Yuille, and Cihang Xie. Are transformers more robust than cnns? Advances in Neural\nInformation Processing Systems, 34:26831–26843, 2021.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020.\nNicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko.\nEnd-to-end object detection with transformers. InEuropean Conference on Computer Vision, pp. 213–229. Springer,\n2020.\nMark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative\npretraining from pixels. In International Conference on Machine Learning, pp. 1691–1703. PMLR, 2020.\nXiangning Chen, Cho-Jui Hsieh, and Boqing Gong. When vision transformers outperform resnets without pretraining\nor strong data augmentations. arXiv preprint arXiv:2106.01548, 2021.\nJeremy Cohen, Elan Rosenfeld, and Zico Kolter. Certiﬁed adversarial robustness via randomized smoothing. In\nInternational Conference on Machine Learning, pp. 1310–1320. PMLR, 2019.\nFrancesco Croce and Matthias Hein. Reliable evaluation of adversarial robustness with an ensemble of diverse\nparameter-free attacks. In International Conference on Machine Learning, pp. 2206–2216. PMLR, 2020a.\nFrancesco Croce and Matthias Hein. Minimally distorted adversarial examples with a fast adaptive boundary attack.\nIn International Conference on Machine Learning, pp. 2196–2205. PMLR, 2020b.\nFrancesco Croce, Maksym Andriushchenko, and Matthias Hein. Provable robustness of relu networks via maximiza-\ntion of linear regions. In the 22nd International Conference on Artiﬁcial Intelligence and Statistics, pp. 2057–2066.\nPMLR, 2019.\nJia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248–255. Ieee, 2009.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional trans-\nformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nAlexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words:\nTransformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\nPierre Foret, Ariel Kleiner, Hossein Mobahi, and Behnam Neyshabur. Sharpness-aware minimization for efﬁciently\nimproving generalization. arXiv preprint arXiv:2010.01412, 2020.\nSiddhant Garg and Goutham Ramakrishnan. Bae: Bert-based adversarial examples for text classiﬁcation. arXiv\npreprint arXiv:2004.01970, 2020.\n13\nPublished in Transactions on Machine Learning Research (10/2022)\nIan J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv\npreprint arXiv:1412.6572, 2014.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Identity mappings in deep residual networks. InEuropean\nconference on computer vision, pp. 630–645. Springer, 2016.\nDan Hendrycks, Kimin Lee, and Mantas Mazeika. Using pre-training can improve model robustness and uncertainty.\nIn International Conference on Machine Learning, pp. 2712–2721. PMLR, 2019.\nCharles Herrmann, Kyle Sargent, Lu Jiang, Ramin Zabih, Huiwen Chang, Ce Liu, Dilip Krishnan, and Deqing Sun.\nPyramid adversarial training improves vit performance. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pp. 13419–13429, 2022.\nAndrew G Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto,\nand Hartwig Adam. Mobilenets: Efﬁcient convolutional neural networks for mobile vision applications. arXiv\npreprint arXiv:1704.04861, 2017.\nYu-Lun Hsieh, Minhao Cheng, Da-Cheng Juan, Wei Wei, Wen-Lian Hsu, and Cho-Jui Hsieh. On the robustness of\nself-attentive models. In ACL, pp. 1520–1529, 2019.\nJie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. InProceedings of the IEEE conference on computer\nvision and pattern recognition, pp. 7132–7141, 2018.\nAndrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Logan Engstrom, Brandon Tran, and Aleksander Madry. Adver-\nsarial examples are not bugs, they are features. Advances in neural information processing systems, 32, 2019.\nKishaan Jeeveswaran., Senthilkumar Kathiresan., Arnav Varma., Omar Magdy., Bahram Zonooz., and Elahe Arani. A\ncomprehensive study of vision transformers on dense prediction tasks. InProceedings of the 17th International Joint\nConference on Computer Vision, Imaging and Computer Graphics Theory and Applications - Volume 4: VISAPP ,,\npp. 213–223. INSTICC, SciTePress, 2022. ISBN 978-989-758-555-5. doi: 10.5220/0010917800003124.\nDi Jin, Zhijing Jin, Joey Tianyi Zhou, and Peter Szolovits. Is bert really robust? a strong baseline for natural lan-\nguage attack on text classiﬁcation and entailment. In Proceedings of the AAAI conference on artiﬁcial intelligence,\nvolume 34, pp. 8018–8025, 2020.\nAlex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\nDmitry Krotov and John Hopﬁeld. Dense associative memory is robust to adversarial inputs. Neural computation, 30\n(12):3151–3167, 2018.\nDmitry Krotov and John J Hopﬁeld. Dense associative memory for pattern recognition. Advances in Neural Informa-\ntion Processing Systems, 2016.\nAlexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In 5th International\nConference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Pro-\nceedings. OpenReview.net, 2017. URL https://openreview.net/forum?id=BJm4T4Kgx.\nLinyang Li, Ruotian Ma, Qipeng Guo, Xiangyang Xue, and Xipeng Qiu. Bert-attack: Adversarial attack against bert\nusing bert. arXiv preprint arXiv:2004.09984, 2020.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. arXiv preprint arXiv:2103.14030, 2021.\nZhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. A convnet for the\n2020s. arXiv preprint arXiv:2201.03545, 2022.\n14\nPublished in Transactions on Machine Learning Research (10/2022)\nAleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning\nmodels resistant to adversarial attacks. arXiv preprint arXiv:1706.06083, 2017.\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,\nand Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. InProceedings of the European\nConference on Computer Vision (ECCV), pp. 181–196, 2018.\nKaleel Mahmood, Rigel Mahmood, and Marten Van Dijk. On the robustness of vision transformers to adversarial\nexamples. arXiv preprint arXiv:2104.02610, 2021.\nXiaofeng Mao, Gege Qi, Yuefeng Chen, Xiaodan Li, Ranjie Duan, Shaokai Ye, Yuan He, and Hui Xue. Towards robust\nvision transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp.\n12042–12051, 2022.\nMuzammal Naseer, Kanchana Ranasinghe, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan\nYang. Intriguing properties of vision transformers. arXiv preprint arXiv:2105.10497, 2021a.\nMuzammal Naseer, Kanchana Ranasinghe, Salman Khan, Fahad Shahbaz Khan, and Fatih Porikli. On improving\nadversarial transferability of vision transformers. arXiv preprint arXiv:2106.04169, 2021b.\nTianyu Pang, Xiao Yang, Yinpeng Dong, Hang Su, and Jun Zhu. Bag of tricks for adversarial training. arXiv preprint\narXiv:2010.00467, 2020.\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming\nLin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library.\narXiv preprint arXiv:1912.01703, 2019.\nSayak Paul and Pin-Yu Chen. Vision transformers are robust learners. arXiv preprint arXiv:2105.07581, 2021.\nYao Qin, Chiyuan Zhang, Ting Chen, Balaji Lakshminarayanan, Alex Beutel, and Xuezhi Wang. Understand-\ning and improving robustness of vision transformers through patch-based negative augmentation. arXiv preprint\narXiv:2110.07858, 2021.\nHubert Ramsauer, Bernhard Schäﬂ, Johannes Lehner, Philipp Seidl, Michael Widrich, Lukas Gruber, Markus Hol-\nzleitner, Milena Pavlovi´c, Geir Kjetil Sandve, Victor Greiff, et al. Hopﬁeld networks is all you need. arXiv preprint\narXiv:2008.02217, 2020.\nJonas Rauber, Roland Zimmermann, Matthias Bethge, and Wieland Brendel. Foolbox native: Fast adversarial attacks\nto benchmark the robustness of machine learning models in pytorch, tensorﬂow, and jax. Journal of Open Source\nSoftware, 5(53):2607, 2020. doi: 10.21105/joss.02607. URL https://doi.org/10.21105/joss.02607.\nHadi Salman, Mingjie Sun, Greg Yang, Ashish Kapoor, and J Zico Kolter. Denoised smoothing: A provable defense\nfor pretrained classiﬁers. arXiv preprint arXiv:2003.01908, 2020.\nHadi Salman, Saachi Jain, Eric Wong, and Aleksander M ˛ adry. Certiﬁed patch robustness via smoothed vision trans-\nformers. arXiv preprint arXiv:2110.07719, 2021.\nZhouxing Shi and Minlie Huang. Robustness to modiﬁcation with shared words in paraphrase identiﬁcation. In\nProceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: Findings , pp. 164–\n171, 2020.\nZhouxing Shi, Huan Zhang, Kai-Wei Chang, Minlie Huang, and Cho-Jui Hsieh. Robustness veriﬁcation for trans-\nformers. arXiv preprint arXiv:2002.06622, 2020.\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv\npreprint arXiv:1409.1556, 2014.\nYu Sun, Shuohuan Wang, Yukun Li, Shikun Feng, Xuyi Chen, Han Zhang, Xin Tian, Danxiang Zhu, Hao Tian, and\nHua Wu. Ernie: Enhanced representation through knowledge integration. arXiv preprint arXiv:1904.09223, 2019.\n15\nPublished in Transactions on Machine Learning Research (10/2022)\nShiyu Tang, Ruihao Gong, Yan Wang, Aishan Liu, Jiakai Wang, Xinyun Chen, Fengwei Yu, Xianglong Liu, Dawn\nSong, Alan Yuille, et al. Robustart: Benchmarking robustness on architecture design and training techniques.arXiv\npreprint arXiv:2109.05211, 2021.\nBart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and\nLi-Jia Li. The new data and new challenges in multimedia research. arXiv preprint arXiv:1503.01817, 1(8), 2015.\nIlya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung,\nDaniel Keysers, Jakob Uszkoreit, Mario Lucic, et al. Mlp-mixer: An all-mlp architecture for vision. arXiv preprint\narXiv:2105.01601, 2021.\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Hervé Jégou. Train-\ning data-efﬁcient image transformers & distillation through attention. In International Conference on Machine\nLearning, pp. 10347–10357. PMLR, 2021.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\nBoxin Wang, Shuohang Wang, Y . Cheng, Zhe Gan, R. Jia, Bo Li, and Jing jing Liu. Infobert: Improving robustness\nof language models from an information theoretic perspective. ArXiv, abs/2010.02329, 2020a.\nHaohan Wang, Xindi Wu, Zeyi Huang, and Eric P Xing. High-frequency component helps explain the generalization\nof convolutional neural networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, pp. 8684–8694, 2020b.\nRoss Wightman. Pytorch image models. https://github.com/rwightman/pytorch-image-models,\n2019.\nEric Wong, Leslie Rice, and J Zico Kolter. Fast is better than free: Revisiting adversarial training. arXiv preprint\narXiv:2001.03994, 2020.\nKaidi Xu, Zhouxing Shi, Huan Zhang, Yihan Wang, Kai-Wei Chang, Minlie Huang, Bhavya Kailkhura, Xue Lin, and\nCho-Jui Hsieh. Automatic perturbation analysis for scalable certiﬁed robustness and beyond. Advances in Neural\nInformation Processing Systems, 33, 2020.\nI Zeki Yalniz, Hervé Jégou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-supervised learning\nfor image classiﬁcation. arXiv preprint arXiv:1905.00546, 2019.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V Le. Xlnet: Generalized\nautoregressive pretraining for language understanding. arXiv preprint arXiv:1906.08237, 2019.\nMao Ye, Chengyue Gong, and Qiang Liu. Safer: A structure-free approach for certiﬁed robustness to adversarial word\nsubstitutions. arXiv preprint arXiv:2005.14424, 2020.\nFan Yin, Quanyu Long, Tao Meng, and Kai-Wei Chang. On the robustness of language encoders against grammatical\nerrors. arXiv preprint arXiv:2005.05683, 2020.\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Francis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016.\nHongyang Zhang, Yaodong Yu, Jiantao Jiao, Eric Xing, Laurent El Ghaoui, and Michael Jordan. Theoretically princi-\npled trade-off between robustness and accuracy. InInternational Conference on Machine Learning, pp. 7472–7482.\nPMLR, 2019.\nKai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond a gaussian denoiser: Residual learning\nof deep cnn for image denoising. IEEE transactions on image processing, 26(7):3142–3155, 2017.\n16\nPublished in Transactions on Machine Learning Research (10/2022)\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufﬂenet: An extremely efﬁcient convolutional neural\nnetwork for mobile devices. In Proceedings of the IEEE conference on computer vision and pattern recognition ,\npp. 6848–6856, 2018.\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers\nfor end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\n17\nPublished in Transactions on Machine Learning Research (10/2022)\nSupplemental Material\nIn this supplemental material, we provide more analysis and results in our experiments.\nA The Source of Adversarial Robustness\nIn this section we examine the source of the adversarial robustness revealed in our experiments.\nThe improved robustness of ViT is not caused by insufﬁcient attack optimization. We ﬁrst demonstrate that the\nbetter robustness of ViTs in white-box attacks is not caused by the difﬁcult optimization in ViT by plotting the loss\nlandscape with sufﬁcient attack steps.\nFigure 5: Cross entropy loss versus varying PGD attack steps for ViT-S/16 and RestNet18. The dashed lines corre-\nsponds to larger attach radius of 0.03 and the full lines to smaller attack radius of 0.01.\nFigure 6: Robust accuracy versus varying PGD attack steps. The attack radii used for evaluation are shown in subtitles.\n18\nPublished in Transactions on Machine Learning Research (10/2022)\nFigure 5 shows the cross entropy loss versus varing PGD attack steps for ViT-S/16 and ResNet18. Figure 6 shows the\nrobust accuracy versus varing PGD attack steps. As shown in the ﬁgures, ViT’s loss curves converge at a much lower\nvalue than RestNet18, suggesting that the improved robustness of ViT is not caused by insufﬁcient attack optimization.\nFigure 7: Adversarial accuracy of the target models against PGD attack with different attack radii (“eps”) and attack\nsteps (“steps”). When the attack radius and attack steps are increased, the adversarial accuracy of the target model\ndecreases to zero. Darker blocks stand for more robust models against PGD attack.\nFigure 7 shows the robust accuracy of more target models against PGD attack with different attack radii (“eps”) and\nattack steps (“steps”). Vision transformers have darker blocks than CNNs’, which stands for their superior adversarial\nrobustness against PGD attack.\nA Hopﬁeld Network Perspective The equivalence between the attention mechanism in transformers to the modern\nHopﬁeld network (Krotov & Hopﬁeld, 2016) was recently shown in (Ramsauer et al., 2020). Furthermore, on simple\nHopﬁeld network (one layer of attention-like network) and dataset (MNIST), improved adversarial robustness was\nshown in (Krotov & Hopﬁeld, 2018). Therefore, the connection of attention in transformers to the Hopﬁeld network\ncan be used to explain the improved adversarial robustness for ViTs.\nB Experiments on SOTA ViT Structures\nIn this section, we supplement the experimental results of recently proposed SOTA ViTs.\nSwin-Trasnformer (Liu et al., 2021) computes the representations with shifted windows scheme which brings\ngreater efﬁciency by limiting self-attention computation to non-overlapping local windows while also allowing for\ncross-window connection.\nDeiT (Touvron et al., 2021) further improves the ViTs’ performance using data augmentation or distillation from\nCNN teachers with an additional distillation token.\nSAM-ViT (Chen et al., 2021) uses sharpness-aware minimization (Foret et al., 2020) to train ViTs from scratch on\nImageNet without large-scale pretraining or strong data augmentations.\nTable 6 summarizes the information of models investigated in our experiments. The window size of the swin trans-\nformers in Table 6 is 7. The pre-trained weights of these models are available in timm package.\n19\nPublished in Transactions on Machine Learning Research (10/2022)\nTable 6: SOTA ViT models investigated in our experiments.\nModel Layers Hidden size Heads Params\nDeit-T/16 (Touvron et al., 2021) 12 192 3 6M\nDeit-S/16 (Touvron et al., 2021) 12 384 6 22M\nDeit-B/16 (Touvron et al., 2021) 12 768 12 87M\nDist-Deit-T/16 (Touvron et al., 2021) 12 192 3 6M\nDist-Deit-S/16 (Touvron et al., 2021) 12 384 6 22M\nDist-Deit-B/16 (Touvron et al., 2021) 12 768 12 87M\nViT-SAM-B/16 (Chen et al., 2021) 12 768 12 87M\nViT-SAM-B/32 (Chen et al., 2021) 12 768 12 88M\nSwin-T/4 (Liu et al., 2021) (2,2,6,2) 96 (3,6,12,24) 28M\nSwin-S/4 (Liu et al., 2021) (2,2,18,2) 96 (3,6,12,24) 50M\nSwin-B/4 (Liu et al., 2021) (2,2,18,2) 128 (4,8,16,32) 88M\nSwin-L/4 (Liu et al., 2021) (2,2,18,2) 192 (6,12,24,48) 197M\nTable 7: Robust accuracy (%) of ViTs described in Table 6 against 40-step PGD attack with different attack radii, and\nalso the clean accuracy (“Clean”). A model is considered to be more robust if the robust accuracy is higher.\nModel Clean 0.001 0.003 0.005 0.01\nDeit-T/16 72.3 36.8 8.3 2.6 0.3\nDeit-S/16 77.7 48.9 17.6 7.1 1.1\nDeit-B/16 81.3 46.6 14.3 6.0 0.9\nDist-Deit-T/16 74.4 40.6 5.7 0.7 0.2\nDist-Deit-S/16 79.3 52.4 15.1 4.3 0.3\nDist-Deit-B/16 81.8 55.6 17.7 4.5 0.4\nViT-SAM-B/16 76.7 63.4 37.0 20.1 3.8\nViT-SAM-B/32 63.8 53.2 32.3 19.7 3.1\nSwin-T/4 78.8 33.5 6.0 1.2 0.1\nSwin-S/4 81.8 40.0 12.4 3.2 0.2\nSwin-B/4 82.3 38.8 11.1 4.1 0.3\nSwin-L/4 84.2 38.7 11.1 2.9 0.4\nTable 7 shows the clean and robust accuracy of ViTs in Table 6 against 40-step PGD attack with different radii. And\nresults for AutoAttack are shown in Table 8. Swin-transformers introduce shifted windows scheme that limit self-\nattention computation to non-overlapping local windows, which harms the robustness as Tokens-to-Token scheme\naccording to the above results.\nC Experiments on Cifar-10\nWe choose the ImageNet as the benchmark because ViTs can hardly converge when training directly on small datasets\nlike Cifar. Therefore, we ﬁnetune the ViTs instead. As shown in Table 9, ViT-B/4 performs higher robust accuracy\nthan WideResNet, which is consistent with the trend on ImageNet.\n20\nPublished in Transactions on Machine Learning Research (10/2022)\nTable 8: Robust accuracy (%) of ViTs described in Table 6 against AutoAttack with different attack radii, and also the\nclean accuracy (“Clean”). A model is considered to be more robust if the robust accuracy is higher.\nModel Clean 0.001 0.003 0.005 0.01\nDeit-T/16 72.3 23.4 0.5 0.0 0.0\nDeit-S/16 77.7 30.2 1.2 0.0 0.0\nDeit-B/16 81.3 20.4 0.3 0.1 0.0\nDist-Deit-T/16 74.4 31.1 0.8 0.1 0.0\nDist-Deit-S/16 79.3 43.1 3.7 0.2 0.0\nDist-Deit-B/16 81.8 42.7 3.4 0.2 0.0\nViT-SAM-B/16 76.7 59.8 26.0 8.4 0.1\nViT-SAM-B/32 63.8 48.9 23.6 9.7 0.8\nSwin-T/4 78.8 6.8 0.1 0.0 0.0\nSwin-S/4 81.8 7.9 0.1 0.0 0.0\nSwin-B/4 82.3 2.4 0.1 0.0 0.0\nSwin-L/4 84.2 4.3 0.1 0.0 0.0\nTable 9: Robust accuracy of ViT-B/4 and WideResNet against PGD-10 attack with different attack radii.\nModel 0.001 0.003 0.01 0.03\nViT-B/4 0.9202 0.6242 0.0994 0.0103\nWideResNet 0.7744 0.5923 0.0854 0.0000\nD Robustness Against Adversarial Deformation\nBesides additively perturbing the correctly classiﬁed image, ADef (Alaifari et al., 2018) iteratively applies small\ndeformations to the clean data. We show the robust accuracy against such perturbations in Table 10, which is in\naccordance to the results of PGD and AutoAttack.\nTable 10: Robust accuracy (%) against AFef under the default setting described in Alaifari et al. (2018).\nModel ViT-S/16 VGG16 DenseNet MobileNet ResNet18\nRobust Accuracy 12.4 10.8 11.1 11.7 11.8\nE Transfer Attack Results\nTransfer attack results using more attack radii are provided in Figure 8\nF Adversarial Training\nSettings We also conduct a preliminary experiment on adversarial training for ViT. For this experiment we use\nCIFAR-10 (Krizhevsky et al., 2009) with ϵ= 8 /255 and the ViT-B/16 model. Since originally this ViT was pre-\ntrained on ImageNet with image size 224 ×224 and patch size 16 ×16 while image size on CIFAR-10 is 32 ×32,\nwe downsample the weights for patch embeddings and resize patches to 4 ×4, so that there are still 8 ×8 patches\nand we name the new model as ViT-B/4. Though ViT originally enlarged input images on CIFAR-10 for natural\nﬁne-tuning and evaluation, we keep the input size as 32 ×32 to make the attack radius comparable. For training,\nwe use PGD-7 (PGD with 7 iterations) (Madry et al., 2017) and TRADES (Zhang et al., 2019) methods respectively,\nwith no additional data during adversarial training. We compare ViT with two CNNs, ResNet18 (He et al., 2016) and\nWideResNet-34-10 (Zagoruyko & Komodakis, 2016). To save training cost, we train each model for 20 epochs only,\nalthough some prior works used around hundreds of epochs (Madry et al., 2017; Pang et al., 2020) and are very costly\nfor large models. We use a batch size of 128, an initial learning rate of 0.1, an SGD optimizer with momentum 0.9,\n21\nPublished in Transactions on Machine Learning Research (10/2022)\nFigure 8: ASR of transfer attack using FGSM with different attack radii. The rows stand for the surrogate models used\nto generated adversarial examples in the white-box attack approach. The columns stand for the target models. Darker\nrows correlate to the source models that generate more transferable adversarial examples. While darker columns\nmean that the target models are more vulnerable to the transfer attack.“Res50-ssl” and “Res50-swsl” are in short of\n“ResNeXt-32x4d-ssl” and “ResNet50-swsl” respectively.\nTable 11: Results of adversarial training for different models using PGD-7 (7-step PGD attack) and TRADES respec-\ntively on CIFAR-10. ViT-B/4 is a variant of ViT-B/16 where we downsample the patch embedding kernel from16×16\nto 4 ×4 to accommodate the smaller image size on CIFAR-10. We report the clean accuracy (%) and robust accuracy\n(%) evaluated with PGD-10 and AutoAttack respectively. Each model is trained using only 20 epochs to reduce the\ncost.\nModel Method Clean PGD-10 AutoAttack\nPreActResNet18 PGD-7 77.3 48.9 44.4\nTRADES 77.6 49.4 44.9\nWideResNet-34-10 PGD-7 80.3 52.2 48.4\nTRADES 81.6 53.4 49.3\nViT-B/4 PGD-7 85.9 51.7 47.6\nTRADES 85.0 53.9 49.2\nand the learning rate decays after 15 epochs and 18 epochs respectively with a rate of 0.1. While we use a weight\ndecay of 5 ×10−4 for CNNs as suggested by Pang et al. (2020) that 5 ×10−4 is better than 2 ×10−4, we still use\n2 ×10−4 for ViT as we ﬁnd 5 ×10−4 causes an underﬁtting for ViT. We evaluate the models with PGD-10 (PGD with\n10 iterations) and AutoAttack respectively.\nResults We show the results in Table 11. The ViT model achieves higher robust accuracy compared to ResNet18,\nand comparable robust accuracy compared to WideResNet-34-10, while ViT achieves much better clean accuracy\ncompared to the other two models. Here ViT does not advance the robust accuracy after adversarial training compared\nto large CNNs such as WideResNet-34-10. We conjecture that ViT may need larger training data or longer training\n22\nPublished in Transactions on Machine Learning Research (10/2022)\nepochs to further improve its robust training performance, inspired by the fact that on natural training ViT is not able\nto perform well either without large-scale pre-training. And although T2T-ViT improved the performance of natural\ntraining when trained from scratch, our previous results in Table 3 and Table 4 show that the T2T-ViT structure may\nbe inherently less robust. We have also tried Wong et al. (2020) which was proposed to mitigate the overﬁtting of\nFGSM to conduct fast adversarial training with FGSM, but we ﬁnd that it can still cause catastrophic overﬁtting for\nViT such that the test accuracy on PGD attacks remains almost 0. We conjecture that this fast training method may be\nnot suitable for pre-trained models or require further adjustments. Our experiments in this section demonstrate that the\nadversarial training framework with PGD or TRADES is applicable for transformers on vision tasks, and we provide\nbaseline results and insights for future exploration and improvement.\n23"
}