{
  "title": "Contextualizing ASR Lattice Rescoring with Hybrid Pointer Network Language Model",
  "url": "https://openalex.org/W3024172799",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5084868332",
      "name": "Da-Rong Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069296252",
      "name": "Chunxi Liu",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5101984914",
      "name": "Frank Zhang",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5041907084",
      "name": "Gabriel Synnaeve",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5051165898",
      "name": "Yatharth Saraf",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A5069954850",
      "name": "Geoffrey Zweig",
      "affiliations": [
        null
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2507756961",
    "https://openalex.org/W2886319145",
    "https://openalex.org/W2345190899",
    "https://openalex.org/W2802201485",
    "https://openalex.org/W2293829681",
    "https://openalex.org/W1855892484",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W1902237438",
    "https://openalex.org/W1524333225",
    "https://openalex.org/W2395440424",
    "https://openalex.org/W2952913664",
    "https://openalex.org/W2943845043",
    "https://openalex.org/W2973172693",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W2889012072",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W3008525923",
    "https://openalex.org/W2341401723",
    "https://openalex.org/W2514741789",
    "https://openalex.org/W2903250132"
  ],
  "abstract": "Videos uploaded on social media are often accompanied with textual descriptions. In building automatic speech recognition (ASR) systems for videos, we can exploit the contextual information provided by such video metadata. In this paper, we explore ASR lattice rescoring by selectively attending to the video descriptions. We first use an attention based method to extract contextual vector representations of video metadata, and use these representations as part of the inputs to a neural language model during lattice rescoring. Secondly, we propose a hybrid pointer network approach to explicitly interpolate the word probabilities of the word occurrences in metadata. We perform experimental evaluations on both language modeling and ASR tasks, and demonstrate that both proposed methods provide performance improvements by selectively leveraging the video metadata.",
  "full_text": "Contextualizing ASR Lattice Rescoring with Hybrid Pointer Network\nLanguage Model\nDa-Rong Liu†, Chunxi Liu⋆, Frank Zhang⋆, Gabriel Synnaeve⋆, Yatharth Saraf⋆, Geoffrey Zweig⋆\n†National Taiwan University ⋆Facebook AI, USA\nf07942148@ntu.edu.tw {chunxiliu,frankz,gab,ysaraf,gzweig}@fb.com\nAbstract\nVideos uploaded on social media are often accompanied with\ntextual descriptions. In building automatic speech recognition\n(ASR) systems for videos, we can exploit the contextual infor-\nmation provided by such video metadata. In this paper, we ex-\nplore ASR lattice rescoring by selectively attending to the video\ndescriptions. We ﬁrst use an attention based method to ex-\ntract contextual vector representations of video metadata, and\nuse these representations as part of the inputs to a neural lan-\nguage model during lattice rescoring. Secondly, we propose\na hybrid pointer network approach to explicitly interpolate the\nword probabilities of the word occurrences in metadata. We per-\nform experimental evaluations on both language modeling and\nASR tasks, and demonstrate that both proposed methods pro-\nvide performance improvements by selectively leveraging the\nvideo metadata.\nIndex Terms: speech recognition, pointer network, video meta-\ndata, lattice rescoring\n1. Introduction\nPersonalized or contextual automatic speech recognition, which\naims to improve accuracy by leveraging additional information\nor external knowledge, has been an important research topic\n[1, 2, 3, 4, 5]. These prior works usually assume that a set of\nword-level biasing phrases are known ahead of time, e.g. a users\npersonal contact list, which are used to nudge the ASR model\ntowards outputting these particular phrases. In the conventional\nhybrid ASR framework, bias phrases can be compiled into a\nweighted ﬁnite state transducer (WFST), with vocabulary in-\njection, and on-the-ﬂy language model biasing techniques [1, 2]\nhave shown signiﬁcant performance gains. Similarly, for end-\nto-end ASR architectures like Listen, Attend and Spell (LAS)\n[6], the WFST representation of context n-grams is traversed\nalong with the outputs from the LAS network, and the beam\nsearch decoding is biased either in ﬁrst pass decoding [5] or\nrescoring [3]. Alternatively, each context n-gram can also be\nembedded into a ﬁxed dimensional representation, and such\ncontextual information is summarized by an attention mecha-\nnism and further fed as an additional input to the decoder [4, 7].\nOne main distinction in our work is that rather than using a\nlist of named entities, such as user’s contact lists or song names\nas in many prior approaches, here we aim to exploit contextual\ninformation from word sequences or paragraphs, as illustrated\nin Figure 1. Henceforth, we refer to such textual content as\nvideo metadata. Utilizing the video metadata effectively can be\nchallenging, since it not only contains potentially relevant in-\nformation, but also irrelevant text. To address this challenge,\nour neural LM training is explicitly conditioned on the video\n† Work was done when Da-Rong was an intern at Facebook.\nFigure 1: Video metadata:Social media videos are often asso-\nciated with surrounding text, descriptions or titles, as denoted\nin the red box.\nmetadata, and selectively attends to the metadata via an atten-\ntion mechanism. The resulting contextual neural LM is used\nto rescore the lattices generated from the ﬁrst-pass hybrid ASR\ndecoding.\nTherefore, our proposed methods are similar to [4] with\nthree important distinctions. First, we produce the ASR lat-\ntice via a conventional WFST-based hybrid ASR model, and\ncontextual biasing is performed by jointly rescoring the lattice\nand attending to the metadata. Second, rather than tying the\ncontextual biasing with the end-to-end LAS training, we build\nthe contextual LM separately from the acoustic model training,\nwhich effectively allows for modular evaluation and improve-\nments of the contextual LM component. In common with ear-\nlier generations of technology, language model changes can be\nmade independently of the acoustic model. Third, based on [4]\nthat utilized contextual information by an attention mechanism,\nwe further propose the hybrid pointer network (which will be\nintroduced in Section 2.2).\nWhile conventional sequence-to-sequence (seq2seq) mod-\nels typically generate tokens from a predeﬁned vocabulary\n[8, 9], pointer networks [10] can be used to explicitly select\nand output tokens from the input (source) sequence. Recently,\nhybrid pointer-generator networks (PGN), combining seq2seq\nmodels with pointer networks, have been proposed and used in\nsummarization tasks [11]. In such models, an additional scalar\nvariable is generated at each time step and serves as a soft switch\nto choose between generating the token from a predeﬁned vo-\ncabulary or selecting from the input sequence. This has been\nshown to be particularly effective in generating rare words that\nhave very few occurrences in training data [11].\nOur main contributions can be summarized into three cate-\ngories:\narXiv:2005.07394v1  [cs.CL]  15 May 2020\nFigure 2: Contextual Language Model Overview:In this ﬁg-\nure, video metadata is set to “I intern in NY” and the se-\nquence to evaluate probability is set to ”NY is cold”. At time-\nstep one, the target of model is predicting P(“NY”). (a) The\nencoder, which encodes the video metadata into a sequence\nof hidden vectors hi. (b) The decoder. (c) The attention\nmechanism, which generates the attention distribution over en-\ncoder hidden vectors, and summarizes them into a context vec-\ntor. (d) The context vector and the decoder hidden state are\nused to generate the vocabulary distribution. (e) We can fur-\nther interpolate the vocabulary distribution with attention dis-\ntribution to boost the probability of rare word. In general,\n(a)+(b)+(c)+(d) form the attention model depicted in Section\n2.1, and (a)+(b)+(c)+(d)+(e) form the hybrid pointer network\ndepicted in Section 2.2.\n• We build a contextual language model that conditions\non the video metadata. We compare various alternatives\nfor such a language model and demonstrate that a hybrid\npointer network substantially outperforms all competing\nbaselines in perplexities.\n• We then use this language model to rescore the lattice\ngenerated from the ﬁrst pass ASR decoding. We em-\nployed the pruned lattice rescoring algorithm [12], and\nshow that after rescoring, our contextual LM performs\nbetter than all other baseline LMs in word error rate\n(WER).\n• We further perform analysis on how the quality of the\nvideo metadata affects the ASR performance.\n2. Contextual Language Model\nA language model (LM) represents a probability distribution\nover sequences of N tokens Y = (y1,y2,...,y N ). Given\nsuch a sequence, a LM assigns a probability to the sequence\nby modeling the probability of token yk conditioned on its his-\ntory {y1,y2,...,y k−1}. The probability of the whole sequence\ncan be decomposed as:\nP(y1,y2,...,y N ) =\nN∏\nk=1\nP(yk|y1,y2,...,y k−1) (1)\nIn this paper, our language model can be conditioning on the\nvideo metadata X = {x1,x2,...,x M }, where M is the meta-\ndata sequence length. Then, the conditional probability can be\nrepresented and decomposed as:\nP(y1,y2,...,y N |X) =\nN∏\nk=1\nP(yk|y1,y2,...,y k−1,X) (2)\nThis is trained by minimizing the negative log-likelihood:\nN∑\nk=1\n−logP(yk|y1,y2,...,y k−1,X; θmodel) (3)\nwhere θmodel is the set of trainable parameters. For modeling\nP(yk|y1,y2,...,y k−1,X), we ﬁrst describe the attention model\nin Section 2.1, and then we will show how to adapt the attention\nmodel to the hybrid pointer network in Section 2.2.\n2.1. Attention Model\nThe attention model, which is depicted as a component of the\nhybrid pointer network in Figure 2, is similar to the attention\nmodel proposed in [13] with only one difference. The model\nproposed in [13] is targeted to the translation task, which will\nauto-regressively generate words at the decoder. The auto-\nregressive generation means we will generate one word at one\ntime-step, and feed the word as input to the next time-step.\nHowever, as our model is a language model, the input to the\ndecoder is the word sequence of which we evaluate the prob-\nabilities. In this work, these word sequences will be the ASR\nhypotheses from the lattice. We describe the detail formulation\nbelow.\nThe tokens of the metadata xi are fed one-by-one into the\nencoder (multi-layer LSTM), producing a sequence of hidden\nstates hi. At each time step t, the inputs to the decoder are\nthe word embedding of yt and the previous hidden state zt−1,\ngenerating the current hidden statezt. The attention distribution\nis computed as:\nˆαi\nt = (Wzzt + bz)T hi (4)\nαt = softmax(ˆαt) (5)\nNext, the attention distribution is used to produce a weighted\nsum of the encoder hidden states, known as the context vector:\nct =\n∑\ni\nαi\nthi (6)\nThe context vector can be viewed as the summary of the encoder\ninformation, which is then concatenated with decoder hidden\nstate zt and passed through two linear layers to determine the\nprobability of the next word yt+1:\nPvocab(w) =softmax(W′(W[zt; ct] +b) +b′) (7)\nP(yt+1|y1,y2,...,y t,X)\n= P(yt+1|zt,ct) =Pvocab(yt+1) (8)\nwhere W, W′, b, b′ are learnable parameters. Pvocab is the\ndistribution over the whole vocabulary.\n2.2. Hybrid pointer network\nIn the attention model, we have incorporated video metadata\ninformation while predicting the probabilities. However, it will\nbe difﬁcult for the model to predict words of rare occurrences\nin the training data. Take Figure 2 as an example. We assume\nthe word “NY” is rare in training data and the attention model\nis perfect in predicting p(NY ), which means:\nα4\n1 = 1\nα1\n1 = α2\n1 = α3\n1 = 0\nIn this way, we can get c1 = h4. Because “NY” is rare in\ntraining data, there will be two consequences. First, the word\nTable 1: Dataset sizes in hours\nTrain Valid Test\nclean noisy extreme\nEnglish 14k (hrs) 9.7 20.2 18.6 49.1\nSpanish 7.5k 9.9 17.2 19.5 46.1\nembedding of “NY”, i.e. h4, may not be well trained. Sec-\nond, even if h4 does contain the information of the word “NY”,\naccording to equation (8):\nP(NY ) =P(NY |z1,c1) =P(NY |z1,h4) (9)\nit can be difﬁcult for the model to map h4 back to “NY”, be-\ncause again it is rare in training data.\nTo address this problem, we propose to use a hybrid pointer\nnetwork similar to [11]. In a hybrid pointer network, another\nrandom variable Pgen\nt ∈(0,1) is introduced:\nPgen\nt = σ(Wgen[ct; zt; yt] +b) (10)\nThis Pgen\nt is used as a soft switch to select between the dis-\ntribution Pvocab generated in equation (7) or sample from the\nattention distribution ˆαt:\nPptr\nvocab(w) =pgen\nt Pvocab(w) + (1−pgen\nt )\n∑\ni:wi=w\nαi\nt (11)\nWhen pgen\nt is close to one, it means the language model has\nhigh conﬁdence to directly generate the distribution. In contrast,\nif pgen\nt is close to zero, it means the model has low conﬁdence,\nand turns to utilizing the information from metadata. Finally,\nthe probability of the next word yt+1 will be:\nP(yt+1|y1,y2,...,y t,X) =Pptr\nvocab(yt+1) (12)\n3. Evaluation\nWe evaluate the effectiveness of our proposed approaches on\nour in-house English (EN) and Spanish (ES) video datasets,\nwhich are sampled from public social media videos and de-\nidentiﬁed before transcription. These videos contain a diverse\nrange of speakers, accents, topics, and acoustic conditions mak-\ning automatic recognition difﬁcult. Each data instance con-\nsists of the audio, reference transcription and the correspond-\ning video metadata text. The test sets for each language are\ncomposed of clean, noisy and extreme categories, with\nextreme being more acoustically challenging than clean\nand noisy. The dataset sizes are shown in Table 1.\nWe ﬁrst evaluate the language model perplexities in Section\n3.1. Five different language models are compared:\n• 5-gram: the language model with Kneser-Ney smooth-\ning and used in the ﬁrst-pass ASR decoding.\n• LSTM: a multi-layer LSTM.\n• cache-LSTM: this is a simple way for the LSTM LM\nto leverage video metadata. We interpolate the output\ndistribution of LSTM LM with the unigram probability\ndistribution of video metadata. The interpolation weight\nis a tunable hyperparameter.\n• attention model: the model described in Section 2.1\n• hybrid pointer network: the model described in Section\n2.2\nTable 2: Perplexities of each language model on the test sets.\nThe values in the parenthesis for (c) and (d) denote the interpo-\nlation weights.\nEnglish\nclean noisy extreme\n(a) 5-gram 129.9 150.1 150.4\n(b) LSTM 109.6 114.9 119.6\n(c) cache-LSTM (0.1) 105.1 115.35 119.8\n(d) cache-LSTM (0.2) 113.0 125.7 130.8\n(e) attention model 99.1 106.2 110.2\n(f) hybrid pointer network 76.9 91.0 95.2\nSpanish\nclean noisy extreme\n(a) 5-gram 176.4 194.0 209.6\n(b) LSTM 119.0 130.3 151.6\n(c) cache-LSTM (0.1) 118.0 137.2 160.4\n(d) cache-LSTM (0.2) 127.6 151.7 177.8\n(e) attention model 107.4 118.9 139.7\n(f) hybrid pointer network 84.4 101.4 121.2\nSpeciﬁcally, the 5-gram and LSTM LMs do not use the infor-\nmation from the video metadata, while the remaining do. A\ntwo-layer LSTM with 512 hidden units and 0.1 dropout are used\nas the recurrent part of all recurrent models. Adaptive softmax\n[14] is used for efﬁcient training. We implement all our recur-\nrent LMs based on the fairseq toolkit [15]. Kaldi decoder [16] is\nused to produce ASR lattices. All language models, except the\n5-gram model, are then used to perform lattice rescoring. The\nWER comparisons are shown in Section 3.2.\n3.1. Language Modeling\nWe ﬁrst evaluate the effectiveness of each LM in terms of per-\nplexity. LMs are trained using both transcriptions and video\nmetadata from the train set. We build our vocabulary as all\nwords seen in the training data, while leaving remaining words\nas OOVs. We use cosine learning rate (LR) scheduler [17] and\nNAG optimizer with initial LR 0.001. The results are shown in\nTable 2.\nWe can see there are signiﬁcant improvements from the 5-\ngram LM to each of the recurrent neural LMs, i.e., (a) vs (b),\n(c), (d), (e), and (f). Comparing the cache-LSTM with the\nLSTM ((b) vs (c), (d)), we see that even though the cache-\nLSTM does leverage the video metadata, it does not perform\nbetter than the LSTM trained only on transcripts. This indi-\ncates that naively interpolating with the unigram distribution of\nthe metadata may not be helpful. The attention model performs\nbetter than LSTM and cache-LSTM ((e) vs (b), (c), (d)), be-\ncause in attention LSTM, the model automatically learns how\nto leverage the video metadata. Finally, the hybrid pointer net-\nwork performs best, as it can overcome the shortcomings of the\nattention model as described in Section 2.2.\n3.2. ASR performance\nWe now evaluate the effectiveness of the proposed method on\nthe ASR task. We ﬁrst produce lattices via ﬁrst-pass decoding\nwith a graphemic hybrid ASR system [18]. For acoustic mod-\neling, we utilize a hybrid ASR model with a graphemic lexicon\ntrained with the lattice-free MMI criterion [19]. In the ﬁrst-\npass decoding, we use Kaldi decoder with 5-gram LM (from\nTable2 (a)) to generate lattices, with lattice beam 8. The lattices\nFigure 3: Analysis: in this ﬁgure, we report the WERR as the number of co-occurring words between video metadata and transcription\nvaries. WERR denotes the relative WER reduction compared to the WER of ﬁrst-pass decoding. We only report the number if the\nremaining test data size is more than half an hour, since there could be too much variance if the remaining test data size is too small.\nare then rescored with the neural LMs by the pruned lattice al-\ngorithm [12]. A 5-gram approximation is adopted [12, 20] to\nreduce the search space, i.e., we merge search paths containing\nthe same last 5 history words.\nThe results are shown in Table 3. As expected, neural lat-\ntice rescoring improves the ﬁrst-pass decoding results ((a) vs\n(b)). The cache-LSTM does marginally better than LSTM ((b)\nvs (c)), but the improvement is small and unstable. While atten-\ntion model can further boost the performance, the hybrid pointer\nnetwork achieves the best performance.\nTable 3: ASR results in WER. In rescoring with neural LMs, we\ninterpolate the neural LM and n-gram LM scores as in [12].\nThe weight is tuned on valid set, and is shown in parenthesis\nfor each language.\nEnglish (0.6)\nclean noisy extreme\n(a) ﬁrst-pass 15.3 22.0 28.3\n(b) LSTM 14.81 21.47 27.8\n(c) cache-LSTM (0.1) 14.7 21.4 27.8\n(d) attention model 14.7 21.4 27.7\n(e) hybrid pointer network 14.5 21.3 27.6\nSpanish (0.7)\nclean noisy extreme\n(a) ﬁrst-pass 13.6 15.5 21.9\n(b) LSTM 12.8 14.7 21.1\n(c) cache-LSTM (0.1) 12.8 14.7 21.1\n(d) attention model 12.7 14.7 21.0\n(e) hybrid pointer network 12.6 14.5 20.8\nTable 4: The proportion of test data that has none of video meta-\ndata in each test category.\nclean noisy extreme\nEnglish 78 / 1203 296 / 903 3214 / 7092\nSpanish 53 / 732 452 / 821 971 / 2026\n3.3. Analysis\nAlthough we have shown the feasibility of hybrid pointer net-\nwork in Section 3.2, we can see that the relative WER reduction\n(WERR) from LSTM to pointer LSTM is stable, but up to 2%\n(Table 3 (b) vs (e)). The improvements can be correlated with\nthe video metadata quality. Table 4 shows there is a large por-\ntion of dataset where the video metadata is absent. While the\nvideo metadata text could be irrelevant to the video transcrip-\ntions, being null will certainly limit its effectiveness. To under-\nstand the importance of the available metadata size, Figure 3\nshows the results as the number of co-occurring words in both\nmetadata and reference transcription varies. In our analysis, we\nselect the test data instances with 1, 2, 3 or 4 co-occurring words\nbetween transcription and video metadata. The 3000 most fre-\nquent words are not counted as co-occurring words, because\nhypothetically a co-occurring high frequency word, like ‘the’,\nmay not indicate the video metadata quality.\nThe results are shown in Figure 3. As the number of co-\noccurring words increases, we ﬁrst observe that, the WERR\ngiven by hybrid pointer network is more signiﬁcant than the\ngain seen in the overall test set. Although cache-LSTM and\nattention-LSTM also utilize the video metadata information, the\nWERR is not as substantial as in the pointer network. Also,\ncomparing the pointer network and the LSTM models, the gap\nbetween the WERR curves increase in most cases as the num-\nber of co-occurring words grows, which indicates the effective-\nness of our proposed method given a reasonable video metadata\nquality.\n4. Conclusions\nIn this work, we propose the use of a hybrid pointer network\nLM for lattice rescoring, thus making use of text metadata ac-\ncompanying social media videos. We analyze the conditions of\nits effectiveness, and demonstrate that it can provide improve-\nments in both LM perplexity and ASR WER. Also, in the hybrid\npointer network framework, we can replace the recurrent com-\nponents of multi-layer LSTM with other neural models, such as\nneural transformers [21].\n5. References\n[1] K. Hall, E. Cho, C. Allauzen, F. Beaufays, N. Coccaro,\nK. Nakajima, M. Riley, B. Roark, D. Rybach, and L. Zhang,\n“Composition-based on-the-ﬂy rescoring for salient n-gram bias-\ning,” 2015.\n[2] I. McGraw, R. Prabhavalkar, R. Alvarez, M. G. Arenas, K. Rao,\nD. Rybach, O. Alsharif, H. Sak, A. Gruenstein, F. Beaufayset al.,\n“Personalized speech recognition on mobile devices,” in Proc.\nICASSP, 2016.\n[3] I. Williams, A. Kannan, P. S. Aleksic, D. Rybach, and T. N.\nSainath, “Contextual speech recognition in end-to-end neural net-\nwork systems using beam search.”\n[4] G. Pundak, T. N. Sainath, R. Prabhavalkar, A. Kannan, and\nD. Zhao, “Deep context: end-to-end contextual speech recog-\nnition,” in 2018 IEEE Spoken Language Technology Workshop\n(SLT). IEEE, 2018, pp. 418–425.\n[5] Z. Chen, M. Jain, Y . Wang, M. L. Seltzer, and C. Fuegen, “End-\nto-end contextual speech recognition using class language models\nand a token passing decoder,” inProc. ICASSP, 2019.\n[6] W. Chan, N. Jaitly, Q. V . Le, and O. Vinyals, “Listen, attend and\nspell,” arXiv preprint arXiv:1508.01211, 2015.\n[7] Z. Chen, M. Jain, Y . Wang, M. L. Seltzer, and C. Fuegen, “Joint\ngrapheme and phoneme embeddings for contextual end-to-end\nASR,” in Proc. Interspeech 2019, 2019.\n[8] D. Bahdanau, K. Cho, and Y . Bengio, “Neural machine trans-\nlation by jointly learning to align and translate,” arXiv preprint\narXiv:1409.0473, 2014.\n[9] R. Nallapati, B. Zhou, C. Gulcehre, B. Xiang et al. , “Abstrac-\ntive text summarization using sequence-to-sequence rnns and be-\nyond,” arXiv preprint arXiv:1602.06023, 2016.\n[10] O. Vinyals, M. Fortunato, and N. Jaitly, “Pointer networks,” in\nAdvances in Neural Information Processing Systems , 2015, pp.\n2692–2700.\n[11] A. See, P. J. Liu, and C. D. Manning, “Get to the point:\nSummarization with pointer-generator networks,” arXiv preprint\narXiv:1704.04368, 2017.\n[12] H. Xu, T. Chen, D. Gao, Y . Wang, K. Li, N. Goel, Y . Carmiel,\nD. Povey, and S. Khudanpur, “A pruned rnnlm lattice-rescoring\nalgorithm for automatic speech recognition,” in 2018 IEEE Inter-\nnational Conference on Acoustics, Speech and Signal Processing\n(ICASSP). IEEE, 2018, pp. 5929–5933.\n[13] M.-T. Luong, H. Pham, and C. D. Manning, “Effective approaches\nto attention-based neural machine translation,” arXiv preprint\narXiv:1508.04025, 2015.\n[14] E. Grave, A. Joulin, M. Ciss ´e, H. J´egou et al., “Efﬁcient softmax\napproximation for gpus,” inProceedings of the 34th International\nConference on Machine Learning-Volume 70. JMLR. org, 2017,\npp. 1302–1310.\n[15] M. Ott, S. Edunov, A. Baevski, A. Fan, S. Gross, N. Ng, D. Grang-\nier, and M. Auli, “fairseq: A fast, extensible toolkit for sequence\nmodeling,” in Proceedings of NAACL-HLT 2019: Demonstra-\ntions, 2019.\n[16] D. Povey, A. Ghoshal, G. Boulianne, L. Burget, O. Glembek,\nN. Goel, M. Hannemann, P. Motlicek, Y . Qian, P. Schwarz,\nJ. Silovsky, G. Stemmer, and K. Vesely, “The kaldi speech recog-\nnition toolkit,” in IEEE 2011 Workshop on Automatic Speech\nRecognition and Understanding . IEEE Signal Processing So-\nciety, Dec. 2011, iEEE Catalog No.: CFP11SRW-USB.\n[17] I. Loshchilov and F. Hutter, “Sgdr: Stochastic gradient descent\nwith warm restarts,”arXiv preprint arXiv:1608.03983, 2016.\n[18] D. Le, X. Zhang, W. Zheng, C. F ¨ugen, G. Zweig, and M. L.\nSeltzer, “From senones to chenones: Tied context-dependent\ngraphemes for hybrid speech recognition,”Proc. ASRU, 2019.\n[19] D. Povey, V . Peddinti, D. Galvez, P. Ghahremani, V . Manohar,\nX. Na, Y . Wang, and S. Khudanpur, “Purely sequence-trained\nneural networks for asr based on lattice-free mmi.” in Proc. In-\nterspeech, 2016.\n[20] X. Liu, X. Chen, Y . Wang, M. J. Gales, and P. C. Woodland, “Two\nefﬁcient lattice rescoring methods using recurrent neural network\nlanguage models,” IEEE/ACM Transactions on Audio, Speech,\nand Language Processing, vol. 24, no. 8, pp. 1438–1449, 2016.\n[21] K. Irie, A. Zeyer, R. Schl ¨uter, and H. Ney, “Language modeling\nwith deep transformers,” Proc. Interspeech, 2019.",
  "topic": "Metadata",
  "concepts": [
    {
      "name": "Metadata",
      "score": 0.8956669569015503
    },
    {
      "name": "Computer science",
      "score": 0.8608496189117432
    },
    {
      "name": "Pointer (user interface)",
      "score": 0.700209379196167
    },
    {
      "name": "Upload",
      "score": 0.6727729439735413
    },
    {
      "name": "Exploit",
      "score": 0.5773133635520935
    },
    {
      "name": "Language model",
      "score": 0.5529056191444397
    },
    {
      "name": "Social media",
      "score": 0.4900144636631012
    },
    {
      "name": "Natural language processing",
      "score": 0.45778053998947144
    },
    {
      "name": "Artificial neural network",
      "score": 0.453035444021225
    },
    {
      "name": "Word (group theory)",
      "score": 0.4224488139152527
    },
    {
      "name": "Artificial intelligence",
      "score": 0.36987727880477905
    },
    {
      "name": "Speech recognition",
      "score": 0.36641037464141846
    },
    {
      "name": "World Wide Web",
      "score": 0.1533219814300537
    },
    {
      "name": "Linguistics",
      "score": 0.0761156976222992
    },
    {
      "name": "Computer security",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": []
}