{
    "title": "Reliable Natural Language Understanding with Large Language Models and Answer Set Programming",
    "url": "https://openalex.org/W4319792263",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A4223051550",
            "name": "Rajasekharan, Abhiramon",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        },
        {
            "id": "https://openalex.org/A4319838012",
            "name": "Zeng, Yankai",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        },
        {
            "id": "https://openalex.org/A4318741615",
            "name": "Padalkar, Parth",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        },
        {
            "id": "https://openalex.org/A2275556543",
            "name": "Gupta, Gopal",
            "affiliations": [
                "The University of Texas at Dallas"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963263783",
        "https://openalex.org/W4226369848",
        "https://openalex.org/W2963912046",
        "https://openalex.org/W2276364082",
        "https://openalex.org/W4309591663",
        "https://openalex.org/W2091138122",
        "https://openalex.org/W2973724258",
        "https://openalex.org/W3095319910",
        "https://openalex.org/W4309953112",
        "https://openalex.org/W4281740565",
        "https://openalex.org/W3102563707",
        "https://openalex.org/W4308939312",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W3205725177",
        "https://openalex.org/W2963368301",
        "https://openalex.org/W3035758390",
        "https://openalex.org/W3000702891",
        "https://openalex.org/W2123442489",
        "https://openalex.org/W4327810430",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4226226396",
        "https://openalex.org/W3155306957",
        "https://openalex.org/W4286901745",
        "https://openalex.org/W3173718765",
        "https://openalex.org/W3089031584",
        "https://openalex.org/W3036143115",
        "https://openalex.org/W4313799915"
    ],
    "abstract": "Humans understand language by extracting information (meaning) from sentences, combining it with existing commonsense knowledge, and then performing reasoning to draw conclusions. While large language models (LLMs) such as GPT-3 and ChatGPT are able to leverage patterns in the text to solve a variety of NLP tasks, they fall short in problems that require reasoning. They also cannot reliably explain the answers generated for a given question. In order to emulate humans better, we propose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show how LLMs can be used to effectively extract knowledge -- represented as predicates -- from language. Goal-directed ASP is then employed to reliably reason over this knowledge. We apply the STAR framework to three different NLU tasks requiring reasoning: qualitative reasoning, mathematical reasoning, and goal-directed conversation. Our experiments reveal that STAR is able to bridge the gap of reasoning in NLU tasks, leading to significant performance improvements, especially for smaller LLMs, i.e., LLMs with a smaller number of parameters. NLU applications developed using the STAR framework are also explainable: along with the predicates generated, a justification in the form of a proof tree can be produced for a given output.",
    "full_text": "S. Costantini, E. Pontelli, A. Russo, F. Toni,\nR. Calegari, A. D’Avila Garcez, C. Dodaro, F. Fabiano,\nS. Gaggl, A. Mileo, (Eds.): ICLP 2023\nEPTCS 385, 2023, pp. 274–287, doi:10.4204/EPTCS.385.27\n© Rajasekharan et al.\nThis work is licensed under the\nCreative Commons Attribution License.\nReliable Natural Language Understanding with Large\nLanguage Models and Answer Set Programming\nAbhiramon Rajasekharan, Yankai Zeng, Parth Padalkar, Gopal Gupta\nUniversity of Texas at Dallas\nRichardson, USA\n{abhiramon.rajasekharan, yankai.zeng, parth.padalkar, gupta}@utdallas.edu\nHumans understand language by extracting information (meaning) from sentences, combining it with\nexisting commonsense knowledge, and then performing reasoning to draw conclusions. While large\nlanguage models (LLMs) such as GPT-3 and ChatGPT are able to leverage patterns in the text to\nsolve a variety of NLP tasks, they fall short in problems that require reasoning. They also cannot\nreliably explain the answers generated for a given question. In order to emulate humans better, we\npropose STAR, a framework that combines LLMs with Answer Set Programming (ASP). We show\nhow LLMs can be used to effectively extract knowledge—represented as predicates—from language.\nGoal-directed ASP is then employed to reliably reason over this knowledge. We apply the STAR\nframework to three different NLU tasks requiring reasoning: qualitative reasoning, mathematical\nreasoning, and goal-directed conversation. Our experiments reveal that STAR is able to bridge the\ngap of reasoning in NLU tasks, leading to significant performance improvements, especially for\nsmaller LLMs, i.e., LLMs with a smaller number of parameters. NLU applications developed using\nthe STAR framework are also explainable: along with the predicates generated, a justification in the\nform of a proof tree can be produced for a given output.\n1 Introduction\nThe long-term goal of natural language understanding (NLU) research is to build systems that are as\ngood as humans in understanding language. This is a challenging task since there are multiple skills that\nhumans employ to understand a typical sentence. First, a person needs to be proficient in the language to\nbe able to interpret the sentence and understand its surface-level meaning. Second, they need to be able\nto interpret the meaning of the sentence in the current context, using the commonsense knowledge they\nalready possess. This helps resolve ambiguities in the sentence and assess if any information is missing.\nThird, if required, they should be able to pose a question that would seek to fill in any information that\nis missing. Finally, once they attain a complete understanding of the sentence, they should be able to\nexplain what they understood. We believe that all of these skills are also important for an NLU system\nthat seeks to reliably answer questions or hold a conversation with a human.\nIn recent years, Large Language Models (LLMs) have been trained on massive amounts of text\nextracted from the internet. They have shown language proficiency to the extent that they are able to\nperform reading comprehension, translate languages, and generate text to complete stories, poems, or\neven code ([7, 9]). However, they can fall short when applied to problems that require complex reasoning.\nWhen tested on commonsense reasoning or mathematics word problems, LLMs such as GPT-3 have been\nshown to make simple reasoning errors ([12]). Though such errors may be mitigated with strategies such\nas chain-of-thought prompting ([28]), they continue to make mistakes that originate from calculation\nerrors or missing reasoning steps in the solution, making it difficult to rely completely on such systems.\nWhile it is possible to prime LLMs to generate explanations for their answers, they sometimes generate\nRajasekharan et al. 275\nFigure 1: STAR framework Design\nthe right explanation along with a wrong answer and vice versa ([28]). This brings into question the\ndependability of such explanations. The lack of a clear separation of the reasoning process also makes it\ndifficult to assess the models’ state of knowledge and identify commonsense knowledge that needs to be\nintegrated as necessary. These shortcomings point to the need for better NLU systems that use explicit\nreasoning.\nWith this motivation, we propose the STAR (S emantic-parsing Transformer and A SP Reasoner”)\nframework that closely aligns with the way human beings understand language. STAR maps a sen-\ntence to the semantics it represents, augments it with commonsense knowledge related to the concepts\ninvolved—just as humans do—and then uses the combined knowledge to perform the required reasoning\nand draw conclusions (see Figure 1). The STAR framework relies on LLMs to perform semantic parsing\n(converting sentences to predicates that capture their semantics) and shifts the burden of reasoning to an\nanswer set programming (ASP) system ([14, 6]). For our experiments, we use variants of GPT-3 ([7]) to\ngenerate predicates from the text. LLMs can be taught to do this either using fine-tuning or in-context\nlearning using a small number of text-predicate pairs, resulting in a ‘Specialized LLM’. Commonsense\nknowledge related to these predicates is coded in advance using ASP. Depending on the problem, a\nquery is either pre-defined or can also be similarly generated from the problem using LLMs. The query\nis executed on the s(CASP) ([2]) goal-directed ASP system against the LLM-generated predicates and\nASP-coded commonsense knowledge to generate a response.\nIn this paper, we use the STAR framework for three different NLU applications: (i) a system for\nsolving qualitative reasoning problems, (ii) a system for solving math word problems, and (iii) a system\nrepresenting a hotel concierge that holds a conversation with a human user who is looking for a restaurant\nrecommendation. All three tasks require different types of reasoning. Qualitative reasoning and math-\nematical reasoning tasks require the system to perform a few steps of reasoning involving qualitative\nrelationships and arithmetic operations, respectively. On the other hand, the conversation bot task re-\nquires the system to interact with the user to seek missing information, “understand” user requirements,\nand reason over it.\nOur experiments involve two main variants of GPT-3; Davinci ( ∼ 175B parameters) and Curie ( ∼\n6.7B parameters). To measure the performance with STAR, we perform direct answer prediction using\nboth models and compare them to the corresponding answers produced using our framework. The results\n276 Reliable NLU\nshow that STAR shows an increase in answer prediction accuracy and the difference is especially large\nfor the smaller LLM, which might be weaker at reasoning. In both question-answering tasks, we are able\nto produce proof trees for the generated response, making them explainable. The knowledge predicates\nalso help us understand the shortcomings and potential design improvements, which is not possible when\nthe models are run using the LLMs alone. In the conversation bot task that requires in-depth reasoning,\nwe observe that STAR provides much better control wrt seeking information from the user to under-\nstand their requirements. When used on its own for the purpose of restaurant recommendation, Davinci\nsometimes alters restaurant information based on user interaction. However, our approach always gives\nrestaurant suggestions faithfully based on the database of restaurants available, making it more reliable.\nSince reasoning is performed using s(CASP) in our approach, we can also handle an arbitrarily large\ndatabase of restaurants. This is not possible when LLMs are used end-to-end for this conversation bot,\nas there is a limit on the maximum prompt size. Thus, our approach can scale easily to larger restaurant\ndatabases.\n2 Background\nLarge Language Models:Until recently, transformer-based deep learning models have been applied to\nNLP tasks by training and fine-tuning them on task-specific datasets ([8]). With the advent of Large Lan-\nguage Models, the paradigm changed to teaching a language model any arbitrary task using just a few\ndemonstrations, called in-context learning. Brown et al. ([7]) introduced an LLM called GPT-3 contain-\ning approximately 175 billion parameters that have been trained using a massive corpus of filtered online\ntext, on which the well-known ChatGPT is based ([23])). The model was able to perform competitively\non several tasks such as question-answering, semantic parsing ([26]), and machine translation. However,\nsuch LLMs tend to make simple mistakes in tasks such as semantic (commonsense) and mathematical\nreasoning ([12, 28]).\nIn our work, we use GPT-3 for semantic parsing and leave the reasoning part to ASP. We theorize\nthat given the vast pre-training they go through, LLMs can be used to automatically extract knowledge\ninherent in the text, just like humans do. Our experiments confirm that Davinci and Curie are able to\nextract such knowledge as predicates from sentences—with high accuracy—after learning from a few\nexample demonstrations. Thus, our experiments show that LLMs are able to extract, what linguists call,\nthe deep structure of a sentence, given a sentence’s surface structure.\nAnswer Set Programming and the s(CASP) system:The s(CASP) system (developed by Arias et\nal.[2]) is an answer set programming ([6]) system that supports predicates, constraints over non-ground\nvariables, uninterpreted functions, and, most importantly, a top-down, query-driven execution strategy.\nThese features make it possible to return answers with non-ground variables (possibly including con-\nstraints among them) and compute partial models by returning only the fragment of a stable model that\nis necessary to support the answer to a given query. The s(CASP) system supports constructive nega-\ntion based on a disequality constraint solver and unlike Prolog’s negation as failure and ASP’s default\nnegation, not p(X) can return bindings for X on success, i.e., bindings for which the call p(X) would\nhave failed. Additionally, s(CASP) system’s interface with a constraint solver (over reals) allows for\nsound non-monotonic reasoning with constraints (useful for solving algebra problems in one of the NLU\napplications we discuss later).\nComplex commonsense knowledge can be represented in ASP and the s(CASP) query-driven predi-\ncate ASP system can be used for querying it ([15, 29, 14]). Commonsense knowledge can be emulated\nusing (i) default rules, (ii) integrity constraints, and (iii) multiple possible worlds ([14, 15]). Default\nRajasekharan et al. 277\nrules are used for jumping to a conclusion in the absence of exceptions, e.g., a bird normally flies unless\nit’s a penguin. Default rules with such exceptions represent an elaboration-tolerant way of representing\nknowledge ([14]).\nflies(X) :- bird(X), not abnormal bird(X).\nabnormal bird(X) :- penguin(X).\nIntegrity constraints allow us to express impossible situations and invariants. For example, a person\ncannot sit and stand at the same time.\nfalse :- person(X), sit(X), stand(X).\nFinally, multiple possible worlds allow us to construct alternative universes that may have some of the\nparts common but other parts inconsistent. For example, the cartoon world of children’s books has a lot\nin common with the real world (e.g., birds can fly in both worlds), yet in the former birds can talk like\nhumans but in the latter they cannot.\nDefault rules are used to model a bulk of our commonsense knowledge. Integrity constraints help\nin checking the consistency of the information extracted. Multiple possible worlds allow us to perform\nassumption-based reasoning (for example, knowing that “Alice loves Bob”, we could assume that either\nBob also loves Alice or he does not).\nA large number of commonsense reasoning applications have already been developed using ASP\nand the s(CASP) system ([21, 25, 11, 29]). In the three applications reported in this paper, we have\nkept the commonsense reasoning component simple, as our main goal is to illustrate our framework\nfor combining LLMs and ASP to develop NLU applications that are explainable and reliable. Because\nof the use of ASP, it is also possible to detect inconsistencies or biases in the text by reasoning over\nthe predicates extracted. Justification for each response can also be given, as the s(CASP) system can\ngenerate justifications as proof trees ([1]).\n3 Qualitative Reasoning\nQualitative reasoning tests a model’s ability to reason about the properties of objects and events in the\nWorld. Tafjord et al. ([27]) introduced the QuaRel dataset in order to test question answering about\nqualitative relationships of a set of physical properties, which forms a perfect test-bed for our approach.\nOur experimental results show that the STAR framework significantly improves the model accuracy\ncompared to the cases where the LLMs are applied directly to question answering.\n3.1 The QuaRel Dataset\nThe QuaRel dataset consists of 2771 questions designed around 19 different properties such as ‘friction’,\n‘heat’, ‘speed’, ‘time’, etc. In order to answer these questions, one must account for the correlation\nbetween these properties. Each question has a certain observation made about the two worlds where a\nproperty has a higher (or lower) value in one world compared to the other. Based on this observation, a\n(commonsense) inference needs to be drawn about other related properties described in the two worlds.\nThis inference helps pick one of the two choices as the answer for the given question [27].\nA question from the dataset is given in example 3.1. In this example, the two worlds are ’Carpet’ and\n’Floor’. The observation made is that the distance traveled by a toy car is more in world1 (floor). From\nthis, the model needs to infer that the resistance or friction would be higher in world2 (carpet), which\nshould lead to picking option A as the answer.\nExample 3.1:\n278 Reliable NLU\nQuestion: Alan noticed that his toy car rolls further on a wood\nfloor than on a thick carpet. This suggests that:\n(world1: wood floor, world2: thick carpet)\n(A) The carpet has more resistance (Solution)\n(B) The floor has more resistance\nAlong with each question, Tafjord et al. provide a logical form that captures the semantics of the question\nand we use it to extract the predicates needed for our method ([27]). For the above question (example\n3.1), the logical form given is as follows:\nqrel(distance, higher, world 1) → qrel( f riction, higher, world 2) ; qrel( f riction, higher, world 1) (1)\nThe predicate qrel(distance, higher, world 1) refers to the observation that thedistance is higher in world1,\nwhile qrel( f riction, higher, world 2) and qrel( f riction, higher, world 1) refer to the conclusions drawn in\nthe two answer options, respectively.\n3.2 Predicate Generation Step\nWe use GPT-3 to convert the Quarel dataset’s natural language question (including the two answers) into\nappropriate predicates. We fine-tune the two GPT-3 model variants named Davinci and Curie ([7]) on the\nQuaRel dataset, instead of just using in-context learning1. Fine-tuning performs better since the models\ncan learn from all the examples in the training set. Such data is available only for this task. Our input\nprompt consists of the question (including answer options), followed by the world descriptions. The\nworld descriptions are included to enable the model to link the two worlds to the ones in the predicates\n(obs and conc) that are generated in the output. The prompt and completion formats for fine-tuning are\ngiven below:\nPrompt format:\n<Question-Answers>\\n world1:<world1>\\n world2:<world2>\\n\\n##\\n\\n\nCompletion format:\nobs(<p>, <h/l>, <w1/w2>) → conc(<p>, <h/l>, <w1/w2>) ;\nconc(<p>, <h/l>, <w1/w2>) <EOS>\nwhere p is the property involved, h/l is the relation which can be either higher or lower and w1/w2 is\neither world1 or world2. After fine-tuning on the training set using the prompt and completion pairs, we\nuse the prompt to generate the completion during testing. The <EOS> token helps cut off the genera-\ntion when apt, avoiding completions that are either too long or too short. The extracted obs and conc\npredicates are then used by the logic program to determine the correct answer.\n3.3 Commonsense Reasoning Step\nThe commonsense knowledge required to answer the questions is encoded in ASP as facts and rules.\nFirst, we ground the 19 properties using facts such as,\nproperty(friction). property(heat). property(speed).\nNext, we define the relationships between the properties, including their positive correlations (denoted\nas qplus), negative correlations (denoted as qminus) and symmetry,\nqplus(friction, heat). qminus(friction, speed).\nqplus(speed, distance). qminus(distance, loudness).\npositive(X, Y) :- qplus(X, Y). negative(X, Y) :- qminus(X, Y).\npositive(X, Y) :- qplus(Y, X). negative(X, Y) :- qminus(Y, X).\n1Fine-tuning an LLM involves using additional training data to refine the LLM for the task at hand; in-context learning\nrefers to giving some examples from the training data, along with the question posed, to the LLM as a part of its input.\nRajasekharan et al. 279\nIn the QuaRel dataset, we are only dealing with two worlds. Hence, if a property P is higher in\nworld1, it must be lower in world2 and vice versa. We capture this logic using the opposite predicates\nand the rules below:\nopposite_w(world1,world2). opposite_v(higher,lower).\nopposite_w(world2,world1). opposite_v(lower,higher).\nconc(P, V, W) :- obs(P, Vr, Wr), property(P),\nopposite_w(W,Wr), opposite_v(V,Vr).\nIn order to capture the relationship between each pair of properties, we need to account for 4 different\ncases that may arise. If properties P and Pr are positively correlated, then (i) if P is higher in world W,\nPr must also be higher in W, and (ii) if P is higher in world W, Pr must be lower in the other world Wr.\nSimilarly, if P and Pr are negatively correlated, then (i) if P is higher in world W, Pr must be lower in W,\nand (ii) if P is higher in world W, Pr must be higher in the other world Wr. Note that the higher/lower\nrelations may be swapped in all cases above. These 4 possible scenarios can be encoded in logic using\nthe following rules:\nconc(P,V,W) :- obs(Pr,V,W), property(P), property(Pr),\npositive(P,Pr).\nconc(P,V,W) :- obs(Pr,Vr,Wr), property(P), property(Pr),\nopposite_w(W,Wr), opposite_v(V,Vr), positive(P,Pr).\nconc(P,V,W) :- obs(Pr,Vr,W), property(P), property(Pr),\nopposite_v(V,Vr), negative(P,Pr).\nconc(P,V,W) :- obs(Pr,V,Wr), property(P), property(Pr),\nopposite_w(W,Wr), negative(P,Pr).\nUsing this knowledge base, asserting a fact as an observation ( obs) allows us to check for the correct\nconclusion (conc) that is entailed. For the example question in example 3.1, we can arrive at the answer\nby checking for entailment of the two possible conclusions as shown:\nassert(obs(distance, higher, world 1)), conc( f riction, higher, world 2). → True\nand\nassert(obs(distance, higher, world 1)), conc( f riction, higher, world 1). → False\n3.4 Results and Evaluation\nWe compare the results of our models to those reported by Tafjord et al. ([27]) in Table 1. Accuracy\nfor four QuaRel datasets is considered ( QuaRelF refers to the subset of the dataset which only focuses\non friction-related questions). The first 8 rows show the accuracy of the baseline models proposed in\nthe QuaRel paper. Curie-Direct and Davinci-Direct rows report the performance of Curie and Davinci\nmodels which directly predict the answer after fine-tuning on the QuaRel’s training set. The Curie-STAR\nand Davinci-STAR rows show the performance for our approach, i.e., first generating the predicates\nand then reasoning using ASP and commonsense knowledge. The values in bold represent the highest\naccuracy values obtained for each dataset.\nThe results show a large improvement in the accuracy of the Curie model on all four QuaRel datasets.\nTable 1 shows that Davinci-STAR either matches or exceeds the performance of Davinci-Direct on three\nof the four QuaRel datasets. However, interestingly, we see that Davinci-Direct outperforms Davinci-\nSTAR on the QuaRel-Dev dataset. Since our framework is explainable, we were able to analyze the\ncases where our approach makes a mistake. We found that the LLM sometimes generates properties that\nare not in the domain for some predicates (such as ’smoke’ instead of ’heat’ since the question mentions\nsmoke). We hypothesize that this is because similar examples were not seen during training. QuaRel has\n280 Reliable NLU\na larger number of questions based on friction, which we believe led to our framework performing better\non QuaRelF datasets (which contain solely friction-based questions). Similarly, adding more examples\nfor other properties might help our framework bridge the gap for other properties. Clearly, there is a\nstark difference between Curie and Davinci when used with our framework. We infer from this that\nwhile Davinci has some ability to reason, Curie lacks the reasoning skill required for the task and our\napproach helps bridge this reasoning gap.\nTable 1: Comparison of accuracy of models on the QuaRel Dataset (Qualitative Reasoning)\nNo. Model QuaRel Dev QuaRel Test QuaRelF Dev QuaRelF Test\n1. Random 50.0 50.0 50.0 50.0\n2. Human 96.4 - 95.0 -\n3. IR 50.7 48.6 50.7 48.9\n4. PMI 49.3 50.5 50.7 52.5\n5. Rule-Based - - 55.0 57.7\n6. BiLSTM 55.8 53.1 59.3 54.3\n7. QUASP 62.1 56.1 69.2 61.7\n8. QUASP+ 68.9 68.7 79.6 74.5\n9. Curie-Direct 67.6 63.5 45.7 52.7\n10. Curie-STAR (ours) 86.2 85.2 87.9 85.9\n11. Davinci-Direct 93.1 90.5 90.0 91.3\n12. Davinci-STAR (ours) 90.6 90.5 90.6 93.5\n4 Solving Word Problems in Algebra\nSolving word problems in algebra requires extracting information from the question (interpreting its\nlanguage) and performing mathematical reasoning to come up with an answer. Hence, it forms a good\nexperiment to test our framework. We choose a specific type of addition and subtraction problems from\nthe dataset used by Koncel-Kedziorski et al. ([18]). We define the predicates has/4, transfer/5\nand total/4 as shown below to encode the knowledge in the problems:\nhas(entity, quantity, time stamp, k/q).\ntransfer(entity1, entity2, quantity, time stamp, k/q).\ntotal(entity, quantity, time stamp, k/q).\nThe predicate has/4 defines that an entity has a certain quantity of some objects, at a partic-\nular time stamp. The transfer/5 predicate defines that an entity1 has transferred a certain\nquantity of objects, to entity2 at a particular time stamp. Finally, the total/4 predicate\ndefines that an entity has a total amount of some objects equal to the quantity, at a particular\ntime stamp. The last term in each predicate is either the current knowledge (denoted as k) or a place-\nholder for the query (denoted q). We design these based on what information a human might glean from\nthe problem in order to solve it.\nThe computation of the answer is done by simple s(CASP) rules. The rules are not shown due\nto lack of space and can be thought of as commonsense knowledge required to solve simple Algebra\nword problems given the has/4, transfer/5, and total/4 predicates. An example problem and\ncorresponding predicates generated to represent the knowledge are shown below.\nRajasekharan et al. 281\nEx 1:Joan found 70 seashells on the beach. Joan gave Sam some of her seashells. Joan has 27 seashells\nleft. How many seashells did Joan give to Sam?\nhas(joan,70,0,k). transfer(joan,sam,X,1,q). has(joan,27,2,k).\nFollowing the STAR approach, we convert the knowledge in the chosen algebraic problem to the pred-\nicates defined above using an LLM. The predicates thus obtained (including the query) along with the\nrules then constitute the logic program. The query predicate is then executed against the program to solve\nthe word problem.\n4.1 Experiments and Results\nOur dataset contains 91 problems drawn from a collection of word problems provided by Koncel-\nKedziorski et al. ([18]). Since we hand-craft the rules for the domain, we select a set of problems\nthat have a similar logic that we can encode. We use text-davinci-003 which is the most capable GPT-3\nmodel for in-context learning. We did not use the text-curie-001 model as done in the qualitative reason-\ning experiment because the model requires fine-tuning on a larger set of questions to be effective. We\nprovide a context containing a few problems with their corresponding predicates to the GPT-3 models\nand then use each problem as a prompt along with the context for the model to generate the facts and the\nquery predicate(s) corresponding to the new problem. We then use the commonsense rules we defined\nalong with the generated predicates (facts) as the logic program and query the program using the query\npredicate. We then compare the answer generated by the logic program with the actual, human-computed\nanswer for each problem. As a baseline, we use the GPT-3 model for direct answer prediction. Here,\nwith the algebra problems as the context, we provide the correct answer as the expected completion.\nFor our experiments, we initially started with a smaller context of 12 problems and examined the\nmistakes the LLM was making in generating the predicates. Since our approach is explainable (unlike\nthe direct answer prediction approach), we were able to analyze the mistakes and added more problems\nto the context that might fix them. Repeating this process a few times, we end up with 24 problems as\nthe final context for the GPT-3 model. Results of our experiments are shown in table 2.\nTable 2: Performance comparison between the baseline model and our approach\nModel Accuracy\ntext-davinci-003-Direct 1.00\ntext-davinci-003-STAR 1.00\nBoth text-davinci-003-Direct and text-davinci-003-STAR result in 100% accuracy on the test set of\n67 problems. Our experiments show that algebraic word problems that require simple reasoning can be\nsolved easily by large LLMs. While our STAR approach achieves the same accuracy, it can also generate\njustification, making it explainable. Below we show a justification tree generated by s(CASP) ([1]) for\nthe problem shown in Example 1 above, where the computed answer is 43.\nJUSTIFICATION_TREE:\ntransfer(joan,sam,43,1,q) :-\nhas(joan,70,0,k),\nhas(sam,27,2,k),\n43 #= 70-27.\nglobal_constraint.\n282 Reliable NLU\nA major weakness of LLMs is that they perform poorly on mathematical tasks. Since the computation\nin our approach is done by the reasoning process externally (as seen above), it circumvents any potential\ncalculation mistakes the LLMs might make. The set of problems we selected ended up being simple\nenough for the baseline model to also learn. We believe that the performance difference will be evident\nwith more complex algebraic problems, similar to our experiments on qualitative reasoning. We plan to\nexplore this in our future work.\n5 Concierge Bot: A Goal-Directed Conversational Bot\nIn this section, we describe the conversational bot that we have developed using our STAR framework.\nWe first leverage LLMs to extract implicit and explicit knowledge predicates from a user’s dialogue and\nthen reason over the extracted predicates using s(CASP) to satisfy the user’s request. To illustrate the\napplicability of our method to conversational AI, we narrow the domain of the bot to a very specific\nproblem: restaurant recommendations. This ensures that commonsense knowledge that needs to be\nmodeled is within a limited domain that can be expanded later as necessary. We call this bot the concierge\nbot. Since the conversational bot is an interactive application, the commonsense reasoning involved is\nmore complex than the other two tasks. Here, the reasoner also has to keep track of the state of the\nconversation. In order to make a restaurant recommendation, the bot needs to ask the user for information\n(preferred cuisine, price range, etc.) and recommend a restaurant once all information is procured. At\nany stage, the bot can explain why it’s asking for a given piece of information as well as the reason for\nits recommendation. The benefits of the STAR framework are more apparent in this application. The\nbot can ask user questions to acquire more information when missing or to verify if any errors are made\nin predicate generation. We show an example transcript of an interaction session below of the concierge\nbot with a human user. The English responses are generated using GPT-3 again after learning from\npredicate-text demonstrations (essentially, using the reverse of the text-to-predicate generation process).\nBot: Hi, what can I assist you with?\nHuman: Can you help me find a place for food with curry?\nI don’t want a pricey one.\nBot: Are you looking for a place with a specific customer rating?\nHuman: No, I don’t mind the rating.\nBot: What kind of restaurant would you like to visit?\nHuman: A normal restaurant.\nBot: Would you like somewhere family-friendly for tonight?\nHuman: No. Just for myself.\nBot: Unfortunately, we cannot provide the results to your request.\nHuman: How about one with a high price?\nBut it should be then at least above average quality.\nBot: Are you looking for a Thai restaurant?\nThe Rice Boat may be suitable but it can be pricey and\ncustomer feedback has been mixed. Additionally, it may\nnot have the best atmosphere for families.\nHuman: Yes, that’s what I need! Tell me where it is.\nBot: The Rice Boat, located on 901 Bangkok Road, has an average\ncustomer rating and offers Thai cuisine at a premium cost.\nUnfortunately, it is not suitable for children.\nHuman: Great! Thank you for the service!\nBot: It’s no problem, I’m happy to assist.\nRajasekharan et al. 283\n5.1 Predicate Generation using LLMs\nWe adapt examples from the E2E dataset released by Novikova et al. ([22]) for our use case. We\nemploy in-context learning where we give the LLM examples of pairs containing a natural language sen-\ntence and the corresponding restaurant predicates. We use the following six predicates from the dataset:\nrestaurant-name, eatType (as establishment type), food (as cuisine), priceRange, customerRating,\nfamilyFriendly.\nWe use the STAR framework with in-context learning where we provide the GPT-3 model with 11\nselected examples from the dataset, which covers all the predicates along with their possible arguments.\nThis ensures that the LLM is aware of every possible predicate as well as every possible argument value\nthese predicates can take. To assess the viability of LLMs for the predicate generation task, we tested\nthe model using the first 500 examples in the E2E training set and obtained an accuracy of 89.33%.\nThe accuracy metric we use is designed to account for the generation of correct predicates as well as\narguments. The high predicate generation accuracy supports the feasibility of using our STAR framework\nfor the concierge bot. Our framework can similarly be applied, to build any robust domain-specific\nconversational bots such as a front desk office receptionist or an airline reservation assistant.\n5.2 Concierge Bot System Construction\nTo make GPT-3 better understand the meaning of each predicate, we first change the predicate names in\nE2E as follows: restaurnt-name, typeToEat, cuisine, priceRange, customerRating, familyFriendly.\nWe also add two predicates address and phoneNumber to record the location and contact information\nfor the user’s query. An external predicate prefer is also added to capture the user’s preference (such as\ncurry, spicy, etc.) The information asked by the user is expressed by the value “query”. We specialized\nGPT-3 with about a dozen example sentences along with the corresponding predicate(s). Below we show\nsome examples of the sentences and the predicates generated after this specialization.\nSentence: Fitzbillies coffee shop provides a kid-friendly venue for\nChinese food at an average price point in the riverside area.\nIt is highly rated by customers.\nPredicates: restaurant-name(Fitzbillies), typeToEat(coffee shop),\ncuisine(Chinese), priceRange(moderate),\ncustomerRating(high), familyFriendly(yes)\nSentence: Can you find a place for food at a low price? Both English\nand French cuisine is fine for me.\nPredicates: restaurant-name(query), cuisine([Engish, French]),\npriceRange(cheap)\nCommonsense knowledge involved in making a restaurant recommendation is coded using s(CASP).\nThe interactive bot will take in the user’s response and convert it to predicates using GPT-3. The predi-\ncates become part of the state. At this stage, we check for user preference. For example, if the user wants\ncurry, Indian and Thai cuisine would be automatically added to the state through appropriate rules. The\nbot then examines the state to assess if all the information needed is present so that it can make a recom-\nmendation and if not, it will generate a question to ask the user for that information. This logic, shown\nin Figure 2, can be thought of as a state machine and has been referred to as a conversational knowl-\nedge template (CKT) by Basu et al. ([5]). The concierge bot determines which predicates are missing\nin its state to make a recommendation. One of the missing predicates is then selected and a query is\ncreated using it. Note that we use GPT-3 again to generate natural-sounding text from the predicate(s)\n284 Reliable NLU\nFigure 2: The framework of the reasoning system in Concierge Bot. The green boxes indicate the steps\ndone by LLMs and the orange ones indicate the steps done by s(CASP).\ncorresponding to the response that our bot computes. The users can also change their preferences during\nthe conversation and our bot can handle that. Take the conversation mentioned above as an example.\nWhen the user asks “Can you help me find a place for food with curry? I don’t want a pricey one.”,\nfollowing predicates are generated by the GPT-3 text-davinci-003 model:“restaurant-name(query), pre-\nfer(curry), priceRange([cheap, moderate])”. The predicates are then added to the memory of the bot,\nwhere the log of the user requirements of the current conversation is stored. Note that the predicates\n“prefer(curry)” and “cuisine([indian,thai])” are also added to the state using the specific commonsense\nrule we wrote for this purpose (not shown due to lack of space). After the state gets updated, the bot\nchecks to see whether any other key information is missing using another commonsense rule (not shown\nalso due to lack of space). If no information is missing, the bot comes to the recommendation step where\nit searches for the appropriate restaurant in its database, consistent with the user’s input, and outputs it\nas a recommendation.\nSince we use GPT-3 to generate predicates from user utterances, the user has complete freedom in\nterms of what they say. GPT-3 is able to extract the appropriate predicates from the corresponding deep\nform of the sentence. GPT-3 is also used to generate the natural language response of the bot. As a result,\nthe interaction appears quite natural, as the example above shows.\n5.3 Experiments\nTo explore GPT-3’s ability to generate goal-directed conversation without the reasoning system that\nSTAR contains, we tested the conversation directly on GPT-3 with the sample conversations as input. We\nshowed GPT-3 three full paragraphs of conversations and asked GPT-3 to generate a similar conversation\naccording to the user input. The database of restaurants used by our concierge bot is also included in the\ninput to the GPT-3 model. An example interactive conversation directly with GPT-3 is as follows:\nBot: Hello, what can I do for you today?\nHuman: I’d like to have a family meal with my children.\nBot: What type of cuisine would you like?\nHuman: Mexican food is suitable for me.\nBot: What is your desired price range for the place?\nHuman: At a higher price, please.\nBot: I recommend The Wrestlers, a coffee shop serving Mexican\ncuisine at a moderate price range. It is family-friendly\nand has an average customer rating. The address is 470 Main Rd.\nRajasekharan et al. 285\nThe responses given by GPT-3 in the above conversation are correct except for the price range. In\nthe given database, the restaurant recommended only serves cheap food. Hence, GPT-3 modified the\ninformation to align with the user’s request. GPT-3 also follows the given examples and asks about\nthe cuisine and price, but does not request other information like our STAR framework does. This is\nbecause these questions are not motivated by missing information, unlike in our approach. This example\nshows that although GPT-3 used on its own as a conversational bot is able to generate natural-sounding\nsentences fluently, it is unreliable and does not understand the knowledge given. Bots developed using\nour framework do not face such problems because they employ explicit commonsense reasoning. The\nmethodology we use to build the concierge bot is explained in more detail in the paper Zeng et. al. [32].\n6 Related Work\nA recent line of research on improving the reasoning capabilities of LLMs focus on prompt engineering.\nWei et al. ([28]) show that generating a chain of thought before the answer leads to a significant improve-\nment in performance in a variety of reasoning tasks. However, in some cases, a wrong reasoning chain\ncan lead to the right answer or vice versa. Zelikman et. al. ([31]) extend this by generating rationales\nusing a self-taught approach. While the above approaches focus primarily on machine learning, our\napproach instead relies on s(CASP) to perform reasoning explicitly. This explicit reasoning is not only\nmore reliable but is also explainable. Our approach falls into the line of Neuro-symbolic research that\ndoes heavy reasoning and light learning as categorized in the survey by Hamilton et al. ([16]). Typically\nsuch works try to integrate neural and symbolic components ([30, 24]). In contrast, we separate both\ncomponents by using LLMs for predicate extraction and the s(CASP) system for reasoning.\nThis paper follows our earlier research where we advocate a combination of machine learning and\ncommonsense reasoning to carry out intelligent tasks in a human-like manner ([4, 3, 19]). In the SQuARE\nquestion-answering system ([3]), knowledge was extracted using the Stanford CoreNLP Parser ([20]) and\nthen mapped to templates from VerbNet. This method was only usable for simple sentences such as for\nthe bAbI dataset. Along similar lines, Tafjord et al. ([27]) report models that convert problems in English\nto logical forms, which are then processed using a custom interpreter. The semantic parsers discussed in\ntheir paper are variations of LSTMs that generate CFG-like grammar rules which create the logic form.\nThe main bottleneck of these two approaches was in the parsers used. In our work, we use LLMs that\ncan extract predicates from arbitrary sentences which makes our approach applicable to more complex\nproblems and translates to better performance. The closest works to ours are along the lines of Chen et\nal. ([10]) and Gao et al. ([13]) who use LLMs to generate program steps which are then executed in\na programming language such as Python. Here the program steps still need to be generated by LLMs\ncompletely. Instead, our work delegates the entire reasoning task to the s(CASP) system. Since s(CASP)\nis designed for complex reasoning, our research can be extended more easily to complex text-based\nreasoning problems. Similar to our conversation bot, Inclezan et al. ([17]) use an ASP-based action\nlanguage to reason in a restaurant setting, but it is not an interactive system like our work.\n7 Conclusions and Future Work\nIn this paper, we described the STAR framework that combines LLMs and ASP for NLU tasks. We\nshow that our system is reliable and explainable using three different reasoning tasks. For the qualitative\nreasoning task, STAR outperforms purely LLM-based approaches and advances the state-of-the-art wrt\n286 Reliable NLU\nperformance on most datasets in QuaRel. The performance difference is more significant for Curie, in-\ndicating that it helps bridge the reasoning gap in smaller LLMs. In all three tasks, STAR can explain its\nreasoning process by producing a justification tree. In the LLM-only approach for developing a concierge\nbot, we noticed that the LLM mixes up information collected during the conversation and leads to incor-\nrect suggestions, while our STAR-based approach stays faithful to the information given in the restaurant\ndatabase. Our approach also allows for holding long, interactive, and meaningful conversations.\nThe potential applications of our STAR framework are very broad. It can help in any NLU appli-\ncation that requires reasoning about knowledge in text or utterances. Some examples are, automatically\nextracting formal software requirements from textual specifications, building conversational agents for\nother domains, and reliable machine translation. We believe that performance improvement using STAR\nwill be more pronounced for problems that require complex reasoning. We also plan to develop a general\ncommonsense knowledge base that applications developed using the framework can employ.\nAcknowledgment: Authors acknowledge support from NSF grants IIS 1910131, IIP 1916206, and a\nDARPA ARCOS award. We are grateful to Farhad Shakerin, Kinjal Basu, Joaquin Arias, Elmer Salazar,\nand members of the UT Dallas ALPS Lab for their discussions.\nReferences\n[1] Joaqu ´ın Arias, Manuel Carro, Zhuo Chen & Gopal Gupta (2020): Justifications for Goal-Directed Con-\nstraint Answer Set Programming . In: Proceedings 36th ICLP (Tech. Comm.), EPTCS 325, pp. 59–72,\ndoi:10.4204/EPTCS.325.12.\n[2] Joaqu ´ın Arias, Manuel Carro, Elmer Salazar, Kyle Marple & Gopal Gupta (2018): Constraint Answer Set\nProgramming without Grounding. TPLP 18(3-4), pp. 337–354, doi:10.1017/S1471068418000285.\n[3] K. Basu, S. C. Varanasi, F. Shakerin, J. Arias & G. Gupta (2021): Knowledge-driven Natural Lan-\nguage Understanding of English Text and its Applications . In: Proc. AAAI, pp. 12554–12563,\ndoi:10.1609/aaai.v35i14.17488.\n[4] Kinjal Basu, Farhad Shakerin & Gopal Gupta (2020): AQuA: ASP-Based Visual Question Answering . In:\nProc. PADL, Springer-Verlag, Berlin, Heidelberg, p. 57–72, doi:10.1007/978-3-030-39197-3 4.\n[5] Kinjal Basu, Huaduo Wang, Nancy Dominguez, Xiangci Li, Fang Li, Sarat Chandra Varanasi\n& Gopal Gupta (2021): CASPR: a commonsense reasoning-based conversational Socialbot ,\ndoi:10.48550/arXiv.2110.05387.\n[6] Gerhard Brewka, Thomas Eiter & Miroslaw Truszczynski (2011): Answer Set Programming at a Glance ,\ndoi:10.1145/2043174.2043195.\n[7] Tom Brown, Benjamin Mann & Others (2020): Language Models are Few-Shot Learners. In: NeurIPS, 33,\nCurran Associates, Inc., pp. 1877–1901, doi:10.5555/3495724.3495883.\n[8] Silvia Casola, Ivano Lauriola & Alberto Lavelli (2022): Pre-trained transformers: an empirical com-\nparison, doi:10.1016/j.mlwa.2022.100334. Available at https://www.sciencedirect.com/science/article/pii/\nS2666827022000445.\n[9] Mark Chen, Jerry Tworek & Others (2021): Evaluating Large Language Models Trained on Code ,\ndoi:10.48550/arXiv.2107.03374. Available at https://arxiv.org/abs/2107.03374.\n[10] Wenhu Chen, Xueguang Ma, Xinyi Wang & William W. Cohen (2022): Program of Thoughts\nPrompting: Disentangling Computation from Reasoning for Numerical Reasoning Tasks ,\ndoi:10.48550/arXiv.2211.12588.\n[11] Zhuo Chen, Kyle Marple & Others (2016): A Physician Advisory System for Chronic Heart Fail-\nure management based on knowledge patterns . Theory Pract. Log. Program. 16(5-6), pp. 604–618,\ndoi:10.1017/S1471068416000429.\nRajasekharan et al. 287\n[12] Luciano Floridi & Massimo Chiriatti (2020): GPT-3: Its Nature, Scope, Limits, and Consequences ,\ndoi:10.1007/s11023-020-09548-1.\n[13] Luyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon, Pengfei Liu, Yiming Yang, Jamie Callan & Graham\nNeubig (2023): PAL: Program-aided Language Models, doi:10.48550/arXiv.2211.10435.\n[14] M. Gelfond & Y . Kahl (2014): Knowledge representation, reasoning, and the design of intelligent agents:\nAnswer Set Programming approach. Cambridge Univ. Press, doi:10.1017/CBO9781139342124.\n[15] Gopal Gupta (July 7, 2022): Automating Common Sense Reasoning with ASP and s(CASP) . Technical\nReport, https://utdallas.edu/∼gupta/csr-scasp.pdf.\n[16] Kyle Hamilton, Aparna Nayak, Bojan Bo ˇz i ´c & Luca Longo (2022): Is neuro-symbolic AI meeting its\npromises in natural language processing? A structured review . Semantic Web, pp. 1–42, doi:10.3233/sw-\n223228.\n[17] Daniela Inclezan (2019): RestKB: A Library of Commonsense Knowledge about Dining at a Restaurant ,\ndoi:10.4204/EPTCS.306.19.\n[18] Rik Koncel-Kedziorski, Hannaneh Hajishirzi, Ashish Sabharwal, Oren Etzioni & Siena Ang (2015): Parsing\nAlgebraic Word Problems into Equations, doi:10.1162/tacl a 00160.\n[19] Suraj Kothawade, Vinaya Khandelwal & Others (2021): AUTO-DISCERN: Autonomous Driving Using Com-\nmon Sense Reasoning, doi:10.48550/arXiv.2110.13606.\n[20] Christopher D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven J. Bethard & David Mc-\nClosky (2014): The Stanford CoreNLP NLP Toolkit . In: ACL System Demonstrations, pp. 55–60,\ndoi:10.3115/v1/P14-5010.\n[21] Jason Morris (2023): Blawx: User-friendly Goal-Directed Answer Set Programming for Rules as Code.\n[22] Jekaterina Novikova, Ond ˇrej Duˇsek & Verena Rieser (2017): The E2E Dataset: New Challenges For End-\nto-End Generation. In: Proc. SIGdial Meeting on Discourse and Dialogue, Association for Computational\nLinguistics, pp. 201–206, doi:10.18653/v1/W17-5525.\n[23] OpenAI (2022): Optimizing Language Models for Dialog. https://openai.com/blog/chatgpt/.\n[24] Ryan Riegel, Alexander Gray & Others (2020): Logical Neural Networks, doi:10.48550/arXiv.2006.13155.\n[25] Galileo sartor, Jacinto Davila & Others (2022): Integration of Logical English and s(CASP).\n[26] Richard Shin & Benjamin Van Durme (2022): Few-Shot Semantic Parsing with Language Models Trained on\nCode. In: Proc. NACL 2022: Human Language Technologies, ACL, Seattle, United States, pp. 5417–5425,\ndoi:10.18653/v1/2022.naacl-main.396.\n[27] Oyvind Tafjord, Peter Clark, Matt Gardner, Wen-tau Yih & Ashish Sabharwal (2019): QUAREL: A Dataset\nand Models for Answering Questions about Qualitative Relationships, doi:10.1609/aaai.v33i01.33017063.\n[28] Jason Wei, Xuezhi Wang & Others (2022):Chain of Thought Prompting Elicits Reasoning in Large Language\nModels, doi:10.48550/arXiv.2201.11903.\n[29] Zesheng Xu, Joaqu ´ın Arias & Others (2023): Jury-Trial Story Construction and Analysis Using Goal-\nDirected Answer Set Programming. In: Proc. PADL, LNCS 13880, Springer, pp. 261–278, doi:10.1007/978-\n3-031-24841-2 17.\n[30] Zhun Yang, Adam Ishay & Joohyung Lee (2020): NeurASP: Embracing Neural Networks into Answer Set\nProgramming. In Christian Bessiere, editor: Proc. IJCAI-20, International Joint Conferences on Artificial\nIntelligence Organization, pp. 1755–1762, doi:10.24963/ijcai.2020/243. Main track.\n[31] Eric Zelikman, Yuhuai Wu, Jesse Mu & Noah Goodman (2022): STaR: Bootstrapping Reasoning With Rea-\nsoning, doi:10.48550/arXiv.2203.14465.\n[32] Yankai Zeng, Abhiramon Rajasekharan, Parth Padalkar, Kinjal Basu, Joaqu ´ın Arias & Gopal Gupta (2023):\nAutomated Interactive Domain-Specific Conversational Agents that Understand Human Dialogs."
}