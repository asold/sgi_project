{
    "title": "Primer: Searching for Efficient Transformers for Language Modeling",
    "url": "https://openalex.org/W3200543719",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4289122101",
            "name": "So, David R.",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4306627756",
            "name": "Mańke, Wojciech",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2586378287",
            "name": "Liu, Hanxiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2352997497",
            "name": "Dai, Zihang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224379215",
            "name": "Shazeer, Noam",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202120575",
            "name": "Le, Quoc V.",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3139773203",
        "https://openalex.org/W2964331719",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2964212578",
        "https://openalex.org/W3006439205",
        "https://openalex.org/W2995727387",
        "https://openalex.org/W2767286248",
        "https://openalex.org/W2963918968",
        "https://openalex.org/W3025165719",
        "https://openalex.org/W2462831000",
        "https://openalex.org/W2126559945",
        "https://openalex.org/W2963970792",
        "https://openalex.org/W2949264490",
        "https://openalex.org/W3049519473",
        "https://openalex.org/W2928941594",
        "https://openalex.org/W2963564796",
        "https://openalex.org/W3098903812",
        "https://openalex.org/W2963778169",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3000779003",
        "https://openalex.org/W2906697496",
        "https://openalex.org/W3156891177",
        "https://openalex.org/W2963418779",
        "https://openalex.org/W2130942839",
        "https://openalex.org/W2953384591",
        "https://openalex.org/W3119866685",
        "https://openalex.org/W3082274269",
        "https://openalex.org/W2885185669",
        "https://openalex.org/W3126960149",
        "https://openalex.org/W2964259004",
        "https://openalex.org/W3157374291",
        "https://openalex.org/W2797328513",
        "https://openalex.org/W2995575179",
        "https://openalex.org/W2962940432",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2965658867",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3085177480",
        "https://openalex.org/W2952355681",
        "https://openalex.org/W2894175714",
        "https://openalex.org/W2886010853",
        "https://openalex.org/W3133264589",
        "https://openalex.org/W2963815651",
        "https://openalex.org/W2970903692",
        "https://openalex.org/W2912521296",
        "https://openalex.org/W2994821360",
        "https://openalex.org/W2964515685",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2963925437",
        "https://openalex.org/W3034772996",
        "https://openalex.org/W3117590783",
        "https://openalex.org/W2124290836",
        "https://openalex.org/W3021293129",
        "https://openalex.org/W2170973209"
    ],
    "abstract": "Large Transformer models have been central to recent advances in natural language processing. The training and inference costs of these models, however, have grown rapidly and become prohibitively expensive. Here we aim to reduce the costs of Transformers by searching for a more efficient variant. Compared to previous approaches, our search is performed at a lower level, over the primitives that define a Transformer TensorFlow program. We identify an architecture, named Primer, that has a smaller training cost than the original Transformer and other variants for auto-regressive language modeling. Primer's improvements can be mostly attributed to two simple modifications: squaring ReLU activations and adding a depthwise convolution layer after each Q, K, and V projection in self-attention. Experiments show Primer's gains over Transformer increase as compute scale grows and follow a power law with respect to quality at optimal model sizes. We also verify empirically that Primer can be dropped into different codebases to significantly speed up training without additional tuning. For example, at a 500M parameter size, Primer improves the original T5 architecture on C4 auto-regressive language modeling, reducing the training cost by 4X. Furthermore, the reduced training cost means Primer needs much less compute to reach a target one-shot performance. For instance, in a 1.9B parameter configuration similar to GPT-3 XL, Primer uses 1/3 of the training compute to achieve the same one-shot performance as Transformer. We open source our models and several comparisons in T5 to help with reproducibility.",
    "full_text": "Primer: Searching for Efﬁcient Transformers\nfor Language Modeling\nDavid R. So, Wojciech Ma´nke, Hanxiao Liu, Zihang Dai, Noam Shazeer, Quoc V . Le\nGoogle Research, Brain Team\n{davidso, wojciechm, hanxiaol, zihangd, noam, qvl}@google.com\nAbstract\nLarge Transformer models have been central to recent advances in natural language\nprocessing. The training and inference costs of these models, however, have grown\nrapidly and become prohibitively expensive. Here we aim to reduce the costs of\nTransformers by searching for a more efﬁcient variant. Compared to previous\napproaches, our search is performed at a lower level, over the primitives that deﬁne\na Transformer TensorFlow program. We identify an architecture, named Primer,\nthat has a smaller training cost than the original Transformer and other variants\nfor auto-regressive language modeling. Primer’s improvements can be mostly\nattributed to two simple modiﬁcations: squaring ReLU activations and adding a\ndepthwise convolution layer after each Q, K, and V projection in self-attention.\nExperiments show Primer’s gains over Transformer increase as compute scale\ngrows and follow a power law with respect to quality at optimal model sizes. We\nalso verify empirically that Primer can be dropped into different codebases to\nsigniﬁcantly speed up training without additional tuning. For example, at a 500M\nparameter size, Primer improves the original T5 architecture on C4 auto-regressive\nlanguage modeling, reducing the training cost by 4X. Furthermore, the reduced\ntraining cost means Primer needs much less compute to reach a target one-shot\nperformance. For instance, in a 1.9B parameter conﬁguration similar to GPT-3 XL,\nPrimer uses 1/3 of the training compute to achieve the same one-shot performance\nas Transformer. We open source our models and several comparisons in T5 to help\nwith reproducibility.1\n1 Introduction\nTransformers [1] have been used extensively in many NLP advances over the past few years (e.g., [2,\n3, 4, 5, 6, 7]). With scaling, Transformers have produced increasingly better performance [3, 7, 8, 9],\nbut the costs of training larger models have become prohibitively expensive.\nIn this paper, we aim to reduce the training costs of Transformer language models. To this end,\nwe propose searching for more efﬁcient alternatives to Transformer by modifying its TensorFlow\ncomputation graph [10]. Given a search space of TensorFlow programs, we use evolution [11, 12, 13,\n14, 15, 16, 17] to search for models that achieve as low of a validation loss as possible given a ﬁxed\namount of training compute. An advantage of using TensorFlow programs as the search space is that it\nis easier to ﬁnd simple low-level improvements to optimize Transformers. We focus on decoder-only\nauto-regressive language modeling (LM), because of its generality and success [18, 7, 19, 20, 21].2\n1https://github.com/google-research/google-research/tree/master/primer\n2We provide details of our primitives search in TensorFlow, but the same approach can also be applied to\nother deep learning libraries.\n35th Conference on Neural Information Processing Systems (NeurIPS 2021), virtual.\narXiv:2109.08668v2  [cs.LG]  24 Jan 2022\nThe discovered model, named Primer (PRIMitives searched transformER), exhibits strong perfor-\nmance improvements over common Transformer variants on auto-regressive language modeling.\nOur experiments show that Primer has the beneﬁts of (1) achieving a target quality using a smaller\ntraining cost, (2) achieving higher quality given a ﬁxed training cost, and (3) achieving a target\nquality using a smaller inference cost. These beneﬁts are robust and hold across model sizes (20M\nto 1.9B parameters), across compute scales (10 to 10 5 accelerator hours), across datasets (LM1B,\nC4, PG19 [22]), across hardware platforms (TPUv2, TPUv3, TPUv4 and V100), across multiple\nTransformer codebases using default conﬁgurations (Tensor2Tensor, Lingvo, and T5) and across mul-\ntiple model families (dense Transformers [1], sparse mixture-of-experts Switch Transformers [8], and\nSynthesizers [23]). We open source these comparisons to help with the reproducibility of our results.1\nOur main ﬁnding is that the compute savings of Primer over Transformers increase as training cost\ngrows, when controlling for model size and quality. These savings follow a power law with respect\nto quality when using optimally sized models. To demonstrate Primer’s savings in an established\ntraining setup, we compare 500M parameter Primer to the original T5 architecture, using the exact\nconﬁguration used by Raffel et al. [5] applied to auto-regressive language modeling. In this setting,\nPrimer achieves an improvement of 0.9 perplexity given the same training cost, and reaches quality\nparity with the T5 baseline model using 4.2X less compute. We further demonstrate that Primer’s\nsavings transfer to one-shot evaluations by comparing Primer to Transformer at 1.9B parameters in\na setup similar to GPT-3 XL [7]. There, using 3X less training compute, Primer achieves similar\nperformance to Transformer on both pretraining perplexity and downstream one-shot tasks.\nOur analysis shows that the improvements of Primer over Transformer can be mostly attributed to two\nmain modiﬁcations: squaring ReLU activations and adding a depthwise convolution layer after each\nQ, K, and V projection in self-attention. These two modiﬁcations are simple and can be dropped into\nexisting Transformer codebases to obtain signiﬁcant gains for auto-regressive language modeling.\n2 Search Space and Search Method\nSearching Over TensorFlow Programs: To construct a search space for Transformer alternatives,\nwe use operations from TensorFlow (TF). In this search space, each program deﬁnes the stackable\ndecoder block of an auto-regressive language model. Given input tensors X ∈Rn×d that represent\nsequences of length nwith embedding length d, our programs return tensors of the same shape.\nWhen stacked, their outputs represent next-token prediction embeddings at each sequence position.\nOur programs only specify model architectures and nothing else. In other words, the input and output\nembedding matrices themselves, as well as input preprocessing and weight optimization are not\nwithin the scope of our programs.\nSubprogram Bank\nSubprogram 0 (Main)\nSubprogram 1\nSubprogram 2\nSubprogram N\n...\nCONV 1X1: tf.layers.dense\nMAX: tf.math.maximum\nSIN: tf.math.sin\ntf.layers.dense(inputs=hidden_state_0,\n                units=512)\nOperation: CONV 1X1\nFunction Arguments:\n- Input 1: h0\n- Input 2: h1\n- Constant: 0.781\n- Hidden Dimension: 512\nInstruction 1\nInstruction 2\nInstruction N\nSubprogramDNA (Model)\n...\nInstruction\nTF Primitives Vocab Generated TF Code\nInstruction functions come \nfrom primitives vocab or \nsubprogram bank.\nFigure 1: Overview of DNAs that deﬁne a decoder model program (i.e., an auto-regressive language\nmodel). Each DNA has a collection of subprograms, where SUBPROGRAM 0 is the MAIN () function\nentry point. Each subprogram is comprised of instructions, which are converted to lines of TensorFlow\ncode. Instruction operations map to either basic TensorFlow library functions from the primitives\nvocabulary or one of the parent DNA’s subprograms. The operation’s arguments are ﬁlled using\nthe parent instruction’s argument set, which contains values for all potential operation arguments;\narguments that are not used by a particular operation are simply ignored.\n2\nFigure 1 shows how programs are constructed in our search space. Each program is built from an evo-\nlutionary search DNA, which is an indexed collection ofsubprograms. SUBPROGRAM 0 is the MAIN ()\nfunction that is the execution entry point, and the other subprograms are part of the DNA’ssubprogram\nbank. Each subprogram is an indexed array of instructions with no length constraints. An instruction\nis an operation with a set of input arguments. The operation denotes the function that the instruction\nexecutes. Each operation maps to either a TF function from the primitives vocabulary or another sub-\nprogram in the DNA subprogram bank. The primitives vocabulary is comprised of simple primitive\nTF functions, such as ADD , LOG , and MATMUL (see Appendix A.1 for details). It is worth empha-\nsizing that high-level building blocks such as self-attention are not operations in the search space,\nbut can be constructed from our low-level operations. The DNA’s subprogram bank is comprised of\nadditional programs that can be executed as functions by instructions. Each subprogram can only call\nsubprograms with a higher index in the subprogram bank, which removes the possibility of cycles.\nEach instruction’s argument set contains a list of potential argument values for each instruction\noperation. The set of argument ﬁelds represents the union of ﬁelds that all the operation primitives use:\n• Input 1: The index of the hidden state that will be used as the ﬁrst tensor input. The index\nof each hidden state is the index of the instruction that produced it, with the subprogram’s\ninput states at indexes 0 and 1. An example of an operation that uses this is SIN .\n• Input 2: The index of the second tensor input. This is only used by operations that are\nbinary with respect to tensor inputs. An example of an operation that uses this is ADD .\n• Constant: A real valued constant. An example of an operation that uses this is MAX ;\ntf.math.maximum(x, C) for C = 0is how we express the Transformer’s ReLU activation.\n• Dimension Size: An integer representing the output dimension size for transformations that\nutilize weight matrices. An example of an operation that uses this is CONV 1X1, the dense\nprojection used by the Transformer’s attention projections and feed forward portions. See\nAppendix A.2 for how we employ relative dimensions [13] to resize our models.\nOur search subprograms are converted to TF programs by converting each subprogram instruction to\na corresponding line of TF code, one at a time in indexing order. To create the TF line, the instruction\noperation is mapped to the corresponding TF primitive function or DNA subprogram, and any relevant\narguments are plugged in (see Appendix A.1 for the full TF primitives vocabulary, including argument\nmappings); the other arguments are ignored. The TF tensor that is generated by the ﬁnal instruction is\ntaken as the subprogram output. We do not use TF Eager and so a useful property of the constructed\nprograms is that irrelevant nodes that do not contribute to the programs’ outputs are ignored as per\nTF’s original deferred execution design [10]. See Figure 2 for an illustration of how subprograms are\nconverted to TF graphs and see Appendix A.2 for more details on how TF graphs are constructed,\nincluding how we handle causal masking.\n(h2) CONV 1X1\nInput_1: h0\nInput_2: Ignore\nDim: 512\nConstant: Ignore\nSubprogram 1\n(h3) SIN\nInput_1: h1\nInput_2: Ignore\nDim: Ignore\nConstant: Ignore\n(h4) MUL\nInput_1: h2\nInput_2: h3\nDim: Ignore\nConstant: Ignore\n(h5) ADD\nInput_1: h0\nInput_2: h2\nDim: Ignore\nConstant: Ignore\n def subprogram_1(h0, h1):\n   h2 = tf.layers.dense(h0, 512)\n   h3 = tf.math.sin(h1)\n   h4 = tf.math.multiply(h2, h3)\n   h5 = tf.math.add(h0, h2)\n   # last state is output\n   output = h5    \n   return output\n(h0) IN 0\nOUT\n(h3) SIN\n(h1) IN 1\n(h4) MUL(h5) ADD\n(h2) CONV 1X1\nFigure 2: Example of a program converted into its corresponding TensorFlow graph. Nodes that do not\ncontribute to the program output are not executed thanks to TensorFlow’s deferred execution design.\n3\nEvolutionary Search: The goal of our evolutionary search is to ﬁnd the most training efﬁcient\narchitecture in the search space. To do this, we give each model a ﬁxed training budget (24 TPUv2\nhours) and deﬁne its ﬁtness as its perplexity on the One Billion Words Benchmark (LM1B) [ 24]\nin Tensor2Tensor [25]. This approach, which we call an implicit efﬁciency objective by ﬁxed training\nbudget, contrasts previous architecture search works that explicitly aim to reduce training or inference\nstep time when optimizing for efﬁciency [ 26, 27, 28, 29]. Our objective is different in that the\ntrade-off between step time and sample efﬁciency is implicit. For instance, a modiﬁcation that doubles\nstep time, but triples sample efﬁciency is a good modiﬁcation in our search, as it ultimately makes the\narchitecture more compute efﬁcient. Indeed, the modiﬁcations we ﬁnd to be most beneﬁcial, squaring\nReLUs and adding depthwise convolutions to attention, increase training step time. However, they\nimprove the sample efﬁciency of the model so much that they decrease the total compute needed\nto reach a target quality, by drastically reducing the number of training steps needed to get there.\nThe search algorithm we use is Regularized Evolution [ 30] with hurdles [ 13]. We conﬁgure our\nhurdles using a 50th percentile passing bar and space them such that equal compute is invested in\neach hurdle band; this reduces the search cost by a factor of 6.25X compared to the same experiment\nwith full model evaluations (see Appendix A.3 for more details). Additionally, we use 7 training\nhours as a proxy for a full day’s training because a vanilla Transformer comes within 90% of its 24\nhour training perplexity with just 7 hours of training. This reduces the search cost further by a factor\nof 3.43X, for a total compute reduction factor of 21.43X. So, although our target is to improve 24 hour\nperformance, it only takes about 1.1 hours to evaluate an individual on average (see Appendix A.4\nfor more search speciﬁcs, including mutation details and hyperparameters). We run our search for\n∼25K individuals and retrain the top 100 individuals on the search task to select the best one.\n0 500 1000\nNum Individuals\n100.74\n100.70\n100.66\nMax Search Fitness\nRandom\nConcept\nFigure 3: Small scale searches\n(10 hours on 10 TPUv2 chips)\ncomparing conceptual initializa-\ntion to random initialization.\nOur search space is open-ended\nenough that it is infeasible to\nsearch without strong initializa-\ntion.\nOur search space is different from previous search spaces (see\narchitecture search survey by [31]), which are often heavily bi-\nased such that random search performs well (see analysis by\n[32, 33, 34]). As our search space does not have this bias, 78% of\nrandom programs in our space with length equal to a Transformer\nprogram cannot train more than ﬁve minutes, due to numerical\ninstability. Because of this open-endedness and abundance of\ndegenerate programs, it is necessary to initialize the search popu-\nlation with copies of the Transformer [13] (input embedding size\ndmodel = 512, feed forward upwards projection size dff = 2048,\nand number of layers L= 6) (Figure 3). To apply this initializa-\ntion to our search space, we must determine how to divide the\nTransformer program into subprograms. To do this, we divide\nalong the lines of the machine learning concepts that constitute\nit. For instance, we create one subprogram each for self-attention,\nReLU and layer norm, using commonly used implementations\n(see Appendix A.5 for the complete list). We call this method\nconceptual initialization because it introduces a bias to the search\nthrough initialization, while leaving the search space for evolution\nand the action space for mutations open-ended. This contrasts the\nlarge amount of previous works that introduce bias through the\nsearch space. Although some works have also explored searching spaces that are open-ended like\nours on miniature tasks [35], we demonstrate that our techniques can scale to full sized deep learning\nregimes (see Section 4).\n3 Primer\nPrimer: We name the discovered modelPrimer, which stands for PRIMitives searched transformER\n(See Appendix Figure 23 for the full program). Primer shows signiﬁcant improvement when retrained\non the search task, requiring less than half the compute of Transformer to reach the same quality\n(Figure 6). In Section 4, we additionally show that Primer makes equally large gains when transferred\nto other codebases, training regimes, datasets, and downstream one-shot tasks.\nPrimer-EZ: A core motivation of this work is to develop simple techniques that can be easily\nadopted by language modeling practitioners. To accomplish this, we perform ablation tests across two\n4\ncodebases (T5 [5] and Tensor2Tensor [25]) and determine which Primer modiﬁcations are generally\nuseful (Appendix Figure 26). The two that produce the most robust improvements are squaring feed\nforward ReLUs and adding depthwise convolution to attention multi-head projections (Figure 4). We\nrefer to a Transformer with just these two easy modiﬁcations as Primer-EZ; this is our recommended\nstarting point for language modeling practitioners interested in using Primer. We now explain these\nmodiﬁcations and then measure their empirical effectiveness.\nInput\nQ Head \nProjection\nK Head \nProjection\nV Head \nProjection\nSpatial \nD-Conv 3x1\nSpatial \nD-Conv 3x1\nSpatial\nD-Conv 3x1\nMulti-Head \nSelf-Attention\nOutput\nMulti-DConv-Head Attention (MDHA)\nInput\nSquare\nOutput\nReLU\nSquared ReLU in Feed Forward Block \nDownwards Proj\nUpwards Proj\nX Num Heads\nMDHA Projection Pseudo-code\n# Use to create each K, Q,\nand V head of size ‘hs’.\ndef mdha_projection(x, hs):\n# Create head.\nx = proj(x,\nhead_size=hs,\naxis=\"channel\")\n# Apply D=Conv to head.\nx = d_conv(x,\nwidth=3,\nhead_size=hs,\naxis=\"spatial\",\nmask=\"causal\")\nreturn x\nFigure 4: The two main modiﬁcations that give Primer most of its gains: depthwise convolution\nadded to attention multi-head projections and squared ReLU activations. These modiﬁcations\nare easy to implement and transfer well across codebases. We call the model with just these two\nmodiﬁcations Primer-EZ. Blue indicates portions of the original Transformer and red signiﬁes one\nof our proposed modiﬁcations.\nSquared ReLU: The most effective modiﬁcation is the improvement from a ReLU activation to\na squared ReLU activation in the Transformer’s feed forward block. Rectiﬁed polynomials of varying\ndegrees have been studied in the context of neural network activation functions [ 36], but are not\ncommonly used; to the best of our knowledge, this is the ﬁrst time such rectiﬁed polynomial activations\nare demonstrated to be useful in Transformers. Interestingly, the effectiveness of higher order\npolynomials [37] can also be observed in other effective Transformer nonlinearities, such as GLU [38]\nvariants like ReGLU [39] (y= Ux⊙max(Vx, 0) where ⊙is an element-wise product) and point-wise\nactivations like approximate GELU [40] (y= 0.5x(1 + tanh(\n√\n2/π(x+ 0.044715x3)))). However,\nsquared ReLU has drastically different asymptotics as x − →∞compared to the most commonly\nused activation functions: ReLU, GELU and Swish (Figure 5 left side). Squared ReLU does have\nsigniﬁcant overlap with ReGLU and in fact is equivalent when ReGLU’sU and V weight matrices\nare the same and squared ReLU is immediately preceded by a linear transformation with weight\nmatrix U. This leads us to believe that squared ReLUs capture the beneﬁts of these GLU variants,\nwhile being simpler, without additional parameters, and delivering better quality (Figure 5 right side).\n2\n 1\n 0 1 2 3 42\n1\n0\n1\n2\n3\n4\nReLU\nGELU\nSwish\nSquared ReLU\n2\n1\n0122\n 1\n 0 1 2\ny\n1\n0\n1\n2\n3\nUxVx\nReGLU\nSquared ReLU\nReLUGELUSwishReGLUSwiGLUSqr ReLU\n-21\n-19.5Negative PPLX\nT5 C4 LM 110M Params\nFigure 5: Left: Squared ReLU has starkly different asymptotics compared to other common activation\nfunctions. Center: Squared ReLU has signiﬁcant overlap with GLU variants [ 39] that use activations\nwith ReLU-like asymptotics, such as ReGLU and SwiGLU. Our experiments indicate that squared\nReLU is better than these GLU variants in Transformer language models. Right: Comparison of\ndifferent nonlinearities in Transformers trained on C4 auto-regressive LM for 525K steps.\n5\nMulti-DConv-Head Attention (MDHA): Another effective modiﬁcation is adding 3x1 depthwise\nconvolutions after each of the multi-head projections for queryQ, key Kand value V in self-attention.\nThese depthwise convolutions are performed over the spatial dimension of each dense projection’s\noutput. Interestingly, this ordering of pointwise followed by depthwise convolution is the reverse of\ntypical separable convolution, which we ﬁnd to be less effective in Appendix A.6. We also ﬁnd that\nwider depthwise convolution and standard convolution not only do not improve performance, but in\nseveral cases hurt it. Although depthwise convolutions have been used for Transformers before [41,\n42], using them after each dense head projection has not been done to the best of our knowledge.\nMDHA is similar to Convolutional Attention [ 43], which uses separable convolution instead of\ndepthwise convolution and does not apply convolution operations per attention head as we do.\nOther Modiﬁcations: The other Primer modiﬁcations are less effective. Graphs for each modiﬁca-\ntion can be found in Appendix A.5 and an ablation study can be found in Appendix A.7. We brieﬂy\ndescribe the modiﬁcations and their usefulnesses here:\n• Shared Q and K Depthwise Representation : Primer shares some weight matrices for Q\nand K. Kis created using the previously described MDHA projection and Q= KW for\nlearnable weight matrix W ∈Rd×d. We ﬁnd that this generally hurts performance.\n• Pre and Post Normalization: The standard practice for Transformers has become putting\nnormalization before both the self-attention and feed forward transformations [ 44, 45].\nPrimer uses normalization before self-attention but applies the second normalization after\nthe feed forward transformation. We ﬁnd this is helpful in some but not all cases.\n• Custom Normalization: Primer uses a modiﬁed version of layer normalization [ 46] that\nuses x(x−µ) instead of (x−µ)2, but we ﬁnd this is not always effective.\n• 12X Bottleneck Projection : The discovered model uses a smaller dmodel size of 384\n(compared to the baseline’s 512) and a largerdff size of 4608 (compared to the baseline’s\n2048). We ﬁnd this larger projection improves results dramatically at smaller sizes (∼35M\nparameters), but is less effective for larger models, as has been previously noted [9]. For\nthis reason we do not include this modiﬁcation when referencing Primer or Primer-EZ.\n• Post-Softmax Spatial Gating: The discovered model has a set of per-channel learnable\nscalars after the attention softmax, which improves perplexity for ﬁxed length sequences.\nHowever, these scalars cannot be applied to variable sequence lengths and so we do not\ninclude this modiﬁcation in Primer for our experiments.\n• Extraneous Modiﬁcations: There are a handful of additional modiﬁcations that produce\nno meaningful difference in the discovered architecture. For example, hidden states being\nmultiplied by -1.12. Verifying that these modiﬁcations neither help nor hurt quality, we\nexclude them from discussion in the main text and do not include them when experimenting\nwith Primer. These extraneous modiﬁcations can still be found in Appendix A.5.\n4 Results\nIn our experiments, we compare Primer against three Transformer variants:\n• Vanilla Transformer: The original Transformer [ 1] with ReLU activations and layer normaliza-\ntion [46] outside of the residual path.\n• Transformer+GELU: A commonly used variant of the vanilla Transformer that uses a GELU [40]\napproximation activation function [2, 7].\n• Transformer++: A Transformer with the following enhancements: RMS normalization [47], Swish\nactivation [48] and a GLU multiplicative branch [ 38] in the feed forward inverted bottleneck\n(SwiGLU) [39]. These modiﬁcations were benchmarked and shown to be effective in T5 [49].\nWe conduct our comparisons across three different codebases: Tensor2Tensor (T2T) [25], T5 [5],\nand Lingvo [ 50]. Tensor2Tensor is the codebase we use for searching and so a majority of our\nside-by-sides are done in T5 and Lingvo to prove transferability. In all cases, we use the default\nTransformer hyperparameters for each codebase, with regularization disabled. See Appendix A.8\nfor more hyperparameter details.\n6\nIn the following sections, we will present our results in four main experiments on auto-regressive\nlanguage modeling. First, we will show that Primer outperforms the baseline models on the search\ntask. Next, we will show that the relationship between Primer’s compute savings over Transformers\nand model quality follow a power law at optimal model sizes. These savings also transfer across\ndatasets and codebases. Then, we will study Primer’s gains in an established training regime and\nshow that it enables 4.2X compute savings at a 500M parameter size using full compute T5 training.\nFinally, we will demonstrate that these gains transfer to the pretraining and one-shot downstream\ntask setup established by GPT-3 [7].\n4.1 Search Task Comparison\nWe ﬁrst analyze Primer’s performance on the search task: LM1B language modeling with sequence\nlength 64, ∼35M model parameters, batches of 4096 tokens and 24 hours of training. We compare\nagainst the baseline models in both Tensor2Tensor (T2T) [25] and T5 [5] and on TPUv2s and V100\nGPUs. We grade each model’s performance according to how much faster it reaches the vanilla\nTransformer’s ﬁnal quality, which we will refer to as itsspeedup factor. Figure 6 shows that Primer\nprovides a speedup factor of 1.7X or more over Transformer in all cases. Figure 6 also shows that\nboth Primer and Primer-EZ generalize to other hardware platforms and codebases.\nTransf.\nTransf.+GELU\nTransf.++\nPrimer\nPrimer-EZ\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nSpeedup Factor\nT2T LM1B TPUv2\nTransf.\nTransf.+GELU\nTransf.++\nPrimer\nPrimer-EZ\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\nT2T LM1B V100 GPU\nTransf.\nTransf.+GELU\nTransf.++\nPrimer\nPrimer-EZ\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\nT5 LM1B TPUv2\nFigure 6: Comparison on the LM1B search task using 35M parameter models. “Speedup Factor”\nrefers to the fraction of compute each model needs to reach quality parity with the vanilla Transformer\ntrained for 24 hours. Primer and Primer-EZ both achieve over 1.7X speedup in all cases. Note that\nresults transfer across codebases and different hardware.\n20.0\n23.5\n27.0\nTPUv2 Hours\n21.70\n21.85\n22.00\nValidation Loss\n20M\nParams\n90M\nParams\n385M\nParams\nPrimer\nTransf.\nTransf.+GELU\nTransf.++\n24.0 25.0 26.0 27.0\nTPUv2 Hours\n21.67\n21.71\n21.75\nValidation Loss\nFigure 7: Left: When sweeping over optimal model sizes, the relationship between Transformer\nlanguage model quality and training compute roughly obeys a power law [9]. Right: Comparison of\nthese power law lines for varying Transformer modiﬁcations, ﬁt with smoothed MSE loss. That the\nmodel lines are parallel to one another implies that compute savings by using superior modeling also\nscales as a power law with quality. Spacings between vertical dotted lines represent 2X differences in\ncompute. Note that the x and y-axes in both plots are in log.\nNext we study the scaling laws of Primer. Here we compare Primer to our baselines over many sizes\nby training each model using every permutation of L∈{6,9,12}layers, dmodel ∈{384,512,1024}\ninitial embedding size, and p∈{4,8,12}feed forward upwards projection ratio, creating a parameter\nrange from 23M to 385M. The results, shown in Figure 7, corroborate previous claims that, at optimal\nparameters sizes, the relationship between compute and language model quality roughly follows a\npower law [9]. That is, the relationship between validation loss, l, and training compute, c, follows\nthe relationship l = ac−k, for empirical constants aand k. This is represented as a line in double\nlog space (Figure 7): log l = −klog c+ loga. However, these lines are not the same for each\narchitecture. The lines are roughly parallel but shifted up and down. In Appendix A.9 we show that,\n7\ngiven a vertical spacing oflog bk, parallel lines such as these indicate compute savings,s, for superior\nmodeling also follow a power law of the form l= a1(1 −1/b)ks−k. The intuition behind this is that\nbis a constant compute reduction factor for all land thus a power law investment of training compute\nwith relation to lresults in a power law savings with relation to las well (see Appendix A.9).\n0.30 1.25 2.30\nInference Time (s)\n38\n33\n28\nNegative Perplexity\nPrimer\nTransf.\nTransf.+GELU\nTransf.++\nFigure 8: Pareto optimal\ninference comparison on\nLM1B. Primer demonstrates\nimproved inference at a ma-\njority of target qualities. We\nobserve these models have\na 0.97 correlation between\ntheir train step and inference\ntimes.\nPrimer also has the capacity to improve inference, despite our search\nfocusing on training compute. Figure 8 shows a Pareto front compar-\nison of quality vs. inference, when using feed forward pass timing\nas a proxy for inference. We use forward pass timing as a proxy for\ninference because there are multiple ways to decode a language model,\neach with varying compute costs. A more in depth study could be\nconducted analyzing Primer’s inference performance across different\ndecoding methods, serving platforms, datasets, etc., but that is beyond\nthe scope of this work.\n4.2 Primer\nTransferability to Other Codebases, Datasets, and Model Types\nWe now study Primer’s ability to transfer to larger datasets, PG19\nand C4, in another codebase, T5. We additionally scale up to a\nhigher compute regime that has been used as a proxy for large scale\ntraining by previous studies [49, 5]; the batches are increased to 65K\ntokens, the sequence lengths are a longer 512, each decoder is 110M\nparameters (dmodel = 768, dff = 3072, L= 12) and each model is\ntrained to ∼525K steps on 4 TPUv3 chips. We also continue training\neach model to 1M steps to study the effect of larger compute budgets\non Primer savings. The results, shown in Figure 9, indicate that the\nPrimer models are as strong in larger data, higher compute regimes,\nas they are in the smaller LM1B regime. Compared to the vanilla baseline, Primer and Primer-EZ\nare at least 1.8X more efﬁcient at the end of training on both PG19 and C4.\n124 Hours\n236 Hours\nTransf.\nTransf.+GELU\nTransf.++\nPrimer\nPrimer-EZ\nTransf.\nTransf.+GELU\nTransf.++\nPrimer\nPrimer-EZ\nSwitch Transf.Switch Primer\nSyn.\nSyn.+Sqr ReLU\n0.8\n1.0\n1.2\n1.4\n1.6\n1.8\n2.0\n2.2\n2.4\nSpeedup Factor\n        C4                           PG19                   C4              C4  \n127 Hours\n242 Hours\n127 Hours\n242 Hours\n142 Hours\n270 Hours\nFigure 9: Comparison transferring Primer to larger datasets (C4 and PG19) and different model\nfamilies (Switch Transformer and Synthesizer) in a different codebase (T5) with an order of magnitude\nmore compute than the search task. Compared to the vanilla baseline, Primer and Primer-EZ are\nat least 1.8X more efﬁcient at the end of training on both PG19 and C4. In all cases, the fraction\nof Primer compute savings increases as more compute is invested. Primer-EZ modiﬁcations also\nimprove Switch Transformer (550M params) and Synthesizer (145M params), showing that it is\ncompatible with other efﬁcient methods. Compute budgets are selected according to how long it takes\neach baseline to train for 525K and 1M steps. See Appendix A.10 for exact numbers.\nFigure 9 also shows that the Primer modiﬁcations are compatible with other efﬁcient model families,\nsuch as large sparse mixture-of-experts like Switch Transformer [8] and efﬁcient Transformer approx-\nimations like Synthesizer [23]. For these experiments, we use the T5 implementations provided by\nNarang et al. [49]. The Primer-EZ techniques of added depthwise convolutions and squared ReLUs\nreduce Switch Transformer’s compute cost by a factor of 1.5X; this translates to a 0.6 perplexity im-\nprovement when controlling for compute (see Appendix A.10). Adding squared ReLUs to Synthesizer\nreduces training costs by a factor of 2.0X and improves perplexity by 0.7 when fully trained.\n8\n4.3 Large Scale T5 Auto-Regressive Language Model Training\nModel Steps TPUv3 Hours PPLX\nOriginal T5 1M 15.7K 13.25\nT5++ 251K 4.6K 13.25\nPrimer 207K 3.8K 13.25\nT5++ 1M 16.5K 12.69\nPrimer 480K 8.3K 12.69\nPrimer 1M 17.3K 12.35\nTable 1: Comparison in compute usage to\nreach target qualities on C4 LM at 537M pa-\nrameters using the full T5 compute scale. Tar-\nget qualities are selected according to the ﬁnal\nperformances of the baseline models. Primer\nachieves the same quality as the original T5\narchitecture using 4.2X less compute.\nIn large scale compute conﬁgurations, the Primer\ncompute savings ratios are even higher. To demon-\nstrate Primer’s savings in an established high compute\ntraining setup, we scale up to the full T5 compute\nregime, copying Raffel et al. exactly [5]. This is the\nsame as the C4 conﬁguration in the previous section,\nbut uses batches of ∼1M tokens, 64 TPUv3 chips\nand 537M parameters (dmodel = 1024, dff = 8192,\nL= 24). Primer is 4.2X more compute efﬁcient than\nthe original T5 model and 2X more efﬁcient than our\nstrengthened Transformer++ baseline (Table 1).\nThe reason why savings are even better here is be-\ncause, at ﬁxed sizes, more compute invested yields\nhigher Primer compute savings. Figure 10 shows\nhow the fraction of compute Primer needs to achieve\nparity with the original T5 architecture shrinks as\nthe models are trained for longer; this is due to the\nasymptotic nature of both the control and variable per-\nplexity training curves. This differs from the power\nlaw savings described in Section A.6. There, we use\nthe optimal number of parameters for each compute budget, and so the compute saving factor, b,\nremains constant. For ﬁxed model sizes, the compute saving factor grows as more compute is\ninvested, meaning that compute savings can exceed the power law estimation. Note, this means\nthat comparisons such as the ones given here can be “gamed” by investing more compute than is\nnecessary for baseline models. It is for this reason that we use an exact replica of Raffel et al.’s [5]\ntraining regime: to demonstrate Primer’s savings in an already published training conﬁguration.\nCP\nCT\nF = CP\nCT \nFigure 10: Compute savings of Primer vs. the original T5 architecture on C4 LM over time. The\nmore compute invested in training, the higher the savings due to the asymptotic nature of both\nperplexity curves. Primer achieves the same performance as the original T5 architecture with 4.2X\nless compute.\n4.4 Primer Transferability to Downstream One-Shot Tasks\nIn our ﬁnal comparison, we demonstrate Primer’s improvements also hold in the pretraining and\none-shot downstream task transfer regime. Recent trends in language modeling have moved towards\ntraining large models on large datasets, which is referred to as “pretraining.” These models are then\ntransferred to unseen datasets and tasks, and, without much or any additional training, demonstrate\nthe capacity to perform well on those “downstream” tasks [2, 51]. In the decoder-only auto-regressive\nlanguage modeling conﬁguration we study here, the most impressive results have been achieved by\nGPT-3 [7], which showed that large language models can exhibit strong performance on unseen tasks\ngiven only one example – referred to as “one-shot” learning. In this section, we demonstrate that\nPrimer’s training compute savings stretch beyond reaching a target pretraining perplexity and indeed\ntransfer to downstream one-shot task performance.\nTo do this, we replicate the GPT-3 pretraining and one-shot evaluation setup. This replication is\nnot exactly the same as the one used for GPT-3 because GPT-3 was not open sourced. Thus, these\n9\nexperiments are not meant to compare directly to GPT-3, as there are conﬁguration differences.\nInstead, these experiments are used as a controlled comparison of the Transformer and Primer\narchitectures. We conduct these experiments in the Lingvo codebase using a proprietary pretraining\ndataset. The downstream tasks are conﬁgured in the same one-shot way described by Brown et al. [7],\nwith single preﬁx examples fed into each model with each task’s inputs. We compare (1) a baseline\n1.9B parameter Transformer (dmodel = 2048, dff = 12288, L= 24) with GELU activations, meant\nto approximate the GPT-3 XL architecture, and (2) a full Primer without shared QK representations,\nwhich only hurt performance according to Appendix A.7. Each model is trained using batches of\n∼2M tokens using 512 TPUv4 chips for ∼140 hours (∼71.8K total accelerator hours or ∼1M train\nsteps). We once again use the T5 training hyperparemeters without any additional tuning.\n0 36000 72000\nTPUv4 Hours\n19\n16\n13\nNegative PPLX\nOne-Shot Pretraining\nTransformer\nPrimer\n-16\n-13\nNeg PPLX\nPretraining\n29\n37\nAverage Score\nOne-Shot\nQA Tasks\n52\n57\nAverage Score\nOne-Shot\nMulti-Choice Tasks\nTransf. 24K Hrs\nTransf. 72K Hrs\nPrimer 24K Hrs\nPrimer 72K Hrs\nFigure 11: Comparison between Transformer+GELU and Primer at 1.9B parameters and varying\ntraining compute budgets on downstream one-shot tasks, similar to GPT-3. Primer achieves slightly\nbetter performance than Transformer when given 3X less pretraining compute and substantially better\nperformance when given the same pretraining compute. Here we stop at 72K TPUv4 hours to roughly\nmatch the quality of GPT-3 XL, but the compute savings of Primer would be larger if we let the two\nmodels run longer (see Figure 10). Note, this is a crude comparison that uses averaged scores from\nthe 27 one-shot tasks we evaluate. See Appendix A.11 (Table 6 and Figure 27) for exact task scores.\nFigure 11 shows that Primer achieves the same pretraining perplexity and one-shot downstream\nperformance as Transformer+GELU while using 3X less compute. Table 6 in the Appendix gives\nthe exact performance numbers for each of the 27 evaluated downstream tasks. Primer, despite\nusing 3X less compute, outperforms Transfomer+GELU on 5 tasks, does worse on 1 task, and\nperforms equivalently on the remaining 21 tasks. The same table shows that when given equivalent\ncompute, Primer outperforms Transformer+GELU on 15 tasks, does worse on 2 tasks, and performs\nequivalently on the remaining 10 tasks. This result shows that not only can Primer improve language\nmodeling perplexity, but the improvements also transfer to downstream NLP tasks.\n5 Conclusion\nLimitations: There are limitations to this study. First, although our comparisons are at substantial\nsizes, they are still orders of magnitude smaller than state-of-the-art models such as the full-scale\nGPT-3 [7]. Second, we focus primarily on decoder-only models, while encoder-based sequence\nmodels are still widely used [ 1, 2, 3, 4, 6, 52]. In Appendix A.12, we perform encoder-decoder\nmasked language modeling comparisons in T5, but do not study the results in signiﬁcant depth. The\nmain ﬁnding there is that, although Primer modiﬁcations improve upon vanilla Transformer, they\nperform only as well as Transformer++. This result suggests that architectural modiﬁcations that\nwork well for decoder-only auto-regressive language models may not necessarily be as effective for\nencoder-based masked language models. Developing better encoders is a topic of our future research.\nRecommendations and Future Directions: We recommend the adoption of Primer and Primer-EZ\nfor auto-regressivelanguage modeling because of their strong performance, simplicity, and robustness\nto hyperparameter and codebase changes. To prove their potential, we simply dropped them into\nestablished codebases and, without any changes, showed that they can give signiﬁcant performance\nboosts. Furthermore, in practice, additional tuning could further improve their performance.\nWe also hope our work encourages more research into the development of efﬁcient Transformers.\nFor example, an important ﬁnding of this study is that small changes to activation functions can\nyield more efﬁcient training. In the effort to reduce the cost of Transformers, more investment in the\ndevelopment of such simple changes could be a promising area for future exploration.\n10\nAcknowledgements\nWe thank Zhen Xu for his help with infrastructure. We also thank Gabriel Bender, Hallie Cramer,\nAndrew Dai, Nan Du, Yanping Huang, Daphne Ippolito, Norm Jouppi, Lluis-Miquel Munguia, Sharan\nNarang, Ruoming Pang, David Patterson, Yanqi Zhou, and the Google Brain Team for their help and\nfeedback.\nReferences\n[1] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N.\nGomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural\nInformation Processing Systems, 2017.\n[2] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of\ndeep bidirectional transformers for language understanding. In NAACL, 2018.\n[3] Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Ruslan Salakhutdinov, and Quoc V\nLe. Xlnet: Generalized autoregressive pretraining for language understanding. In Advances in\nNeural Information Processing Systems, 2019.\n[4] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\n[5] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena,\nYanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed\ntext-to-text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020.\n[6] Daniel Adiwardana, Minh-Thang Luong, David R. So, J. Hall, Noah Fiedel, R. Thoppilan,\nZ. Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu, and Quoc V . Le. Towards a\nhuman-like open-domain chatbot. arXiv preprint arXiv:2001.09977, 2020.\n[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal,\nArvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are\nfew-shot learners. In Advances in Neural Information Processing Systems, 2020.\n[8] William Fedus, Barret Zoph, and Noam M. Shazeer. Switch transformers: Scaling to trillion\nparameter models with simple and efﬁcient sparsity. arXiv preprint arXiv:2101.03961, 2021.\n[9] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language\nmodels. arXiv preprint arXiv:2001.08361, 2020.\n[10] Martín Abadi, P. Barham, J. Chen, Z. Chen, Andy Davis, J. Dean, M. Devin, Sanjay Ghemawat,\nGeoffrey Irving, M. Isard, M. Kudlur, Josh Levenberg, Rajat Monga, Sherry Moore, D. Murray,\nBenoit Steiner, P. Tucker, Vijay Vasudevan, Pete Warden, Martin Wicke, Y . Yu, and Xiaoqiang\nZhang. Tensorﬂow: A system for large-scale machine learning. In OSDI, 2016.\n[11] Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Quoc V .\nLe, and Alex Kurakin. Large-scale evolution of image classiﬁers. In ICML, 2017.\n[12] Hanxiao Liu, Karen Simonyan, Oriol Vinyals, Chrisantha Fernando, and Koray Kavukcuoglu.\nHierarchical representations for efﬁcient architecture search. In ICLR, 2018.\n[13] David R. So, Chen Liang, and Quoc V . Le. The evolved transformer. In ICML, 2019.\n[14] Hanxiao Liu, Andrew Brock, Karen Simonyan, and Quoc V Le. Evolving normalization-\nactivation layers. In NeurIPS, 2020.\n[15] Xin Yao. Evolving artiﬁcial neural networks. Proceedings of the IEEE, 87(9):1423–1447, 1999.\n[16] Jurgen Schmidhuber. Evolutionary principles in self-referential learning. (on learning how to\nlearn: The meta-meta-... hook.). Diploma thesis, Technische Universitat Munchen, Germany,\n1987.\n[17] Kenneth Stanley, Jeff Clune, Joel Lehman, and Risto Miikkulainen. Designing neural networks\nthrough neuroevolution. Nature Machine Intelligence, 1:24–35, 2019.\n[18] Alec Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. In Technical report, OpenAI, 2019.\n11\n[19] Timo Schick and Hinrich Schütze. It’s not just size that matters: Small language models are\nalso few-shot learners. arXiv preprint arXiv:2009.07118, 2021.\n[20] Sinong Wang, Han Fang, Madian Khabsa, Hanzi Mao, and Hao Ma. Entailment as few-shot\nlearner. arXiv preprint arXiv:2104.14690, 2021.\n[21] Tianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723, 2020.\n[22] Jack W. Rae, Anna Potapenko, Siddhant M. Jayakumar, and T. Lillicrap. Compressive trans-\nformers for long-range sequence modelling. ArXiv, abs/1911.05507, 2020.\n[23] Yi Tay, Dara Bahri, Donald Metzler, Da-Cheng Juan, Zhe Zhao, and Che Zheng. Synthesizer:\nRethinking self-attention in transformer models. arXiv preprint arXiv:2005.00743, 2020.\n[24] Ciprian Chelba, Tomas Mikolov, Mike Schuster, Qi Ge, Thorsten Brants, Phillipp Koehn, and\nTony Robinson. One billion word benchmark for measuring progress in statistical language\nmodeling. In Interspeech, 2014.\n[25] Ashish Vaswani, Samy Bengio, Eugene Brevdo, Francois Chollet, Aidan N. Gomez, Stephan\nGouws, Llion Jones, Łukasz Kaiser, Nal Kalchbrenner, Niki Parmar, Ryan Sepassi, Noam\nShazeer, and Jakob Uszkoreit. Tensor2tensor for neural machine translation. arXiv preprint\narXiv:1803.07416, 2018.\n[26] Mingxing Tan and Quoc Le. Efﬁcientnet: Rethinking model scaling for convolutional neural\nnetworks. In ICML, 2019.\n[27] Mingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, and Quoc V . Le. Mnasnet: Platform-\naware neural architecture search for mobile. 2019 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition (CVPR), pages 2815–2823, 2019.\n[28] Han Cai, Ligeng Zhu, and Song Han. Proxylessnas: Direct neural architecture search on target\ntask and hardware. arXiv preprint:1812.00332, 2019.\n[29] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Efﬁcient multi-objective neural\narchitecture search via lamarckian evolution. In ICLR, 2019.\n[30] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V . Le. Regularized evolution for\nimage classiﬁer architecture search. In AAAI, 2019.\n[31] Thomas Elsken, Jan Hendrik Metzen, and Frank Hutter. Neural architecture search: A survey.\nJournal of Machine Learning Research, 20(55):1–21, 2019.\n[32] Liam Li and Ameet S. Talwalkar. Random search and reproducibility for neural architecture\nsearch. In UAI, 2019.\n[33] Kaicheng Yu, Christian Sciuto, Martin Jaggi, Claudiu Musat, and Mathieu Salzmann. Evaluating\nthe search phase of neural architecture search. In ICLR, 2020.\n[34] Gabriel Bender, Hanxiao Liu, Bo Chen, Grace Chu, Shuyang Cheng, Pieter-Jan Kindermans,\nand Quoc V . Le. Can weight sharing outperform random architecture search? An investigation\nwith tunas. In CVPR, 2020.\n[35] Esteban Real, Chen Liang, David R. So, and Quoc V . Le. Automl-zero: Evolving machine\nlearning algorithms from scratch. In ICML, 2020.\n[36] Dmitry Krotov and John J. Hopﬁeld. Dense associative memory for pattern recognition. In\nAdvances in Neural Information Processing Systems, 2016.\n[37] Siddhant M. Jayakumar, Jacob Menick, Wojciech M. Czarnecki, Jonathan Schwarz, Jack W.\nRae, Simon Osindero, Y . Teh, Tim Harley, and Razvan Pascanu. Multiplicative interactions and\nwhere to ﬁnd them. In ICLR, 2020.\n[38] Yann N Dauphin, Angela Fan, Michael Auli, and David Grangier. Language modeling with\ngated convolutional networks. In ICML, 2017.\n[39] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020.\n[40] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with\ngaussian error linear units. arXiv preprint arXiv:1606.08415, 2016.\n[41] Adams Wei Yu, David Dohan, Minh-Thang Luong, Rui Zhao, Kai Chen, Mohammad Norouzi,\nand Quoc V . Le. QANet: Combining local convolution with global self-attention for reading\ncomprehension. In ICLR, 2018.\n12\n[42] Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han,\nShibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. Conformer: Convolution-\naugmented transformer for speech recognition. In Interspeech, 2020.\n[43] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang.\nCvT: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\n[44] Alexei Baevski and Michael Auli. Adaptive input representations for neural language modeling.\nIn arXiv preprint arXiv:1809.10853, 2019.\n[45] Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng, S. Zheng, Chen Xing, Huishuai Zhang,\nYanyan Lan, L. Wang, and T. Liu. On layer normalization in the transformer architecture. arXiv\npreprint arXiv:2002.04745, 2020.\n[46] Jimmy Ba, Jamie Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv preprint\narXiv:1607.06450, 2016.\n[47] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019.\n[48] Prajit Ramachandran, Barret Zoph, and Quoc V . Le. Searching for activation functions.arXiv\npreprint arXiv:1710.05941, 2018.\n[49] Sharan Narang, Hyung Won Chung, Yi Tay, William Fedus, Thibault Févry, Michael Matena,\nKarishma Malkan, Noah Fiedel, Noam Shazeer, Zhenzhong Lan, Yanqi Zhou, Wei Li, Nan\nDing, Jake Marcus, Adam Roberts, and Colin Raffel. Do transformer modiﬁcations transfer\nacross implementations and applications? arXiv preprint arXiv:2102.11972, 2021.\n[50] Jonathan Shen, P. Nguyen, Yonghui Wu, Z. Chen, M. Chen, Ye Jia, Anjuli Kannan, T. Sainath,\nYuan Cao, C. Chiu, Yanzhang He, J. Chorowski, Smit Hinsu, S. Laurenzo, James Qin, Orhan\nFirat, Wolfgang Macherey, Suyog Gupta, Ankur Bapna, Shuyuan Zhang, Ruoming Pang,\nRon J. Weiss, Rohit Prabhavalkar, Qiao Liang, Benoit Jacob, Bowen Liang, HyoukJoong Lee,\nCiprian Chelba, Sébastien Jean, Bo Li, M. Johnson, Rohan Anil, Rajat Tibrewal, Xiaobing\nLiu, Akiko Eriguchi, Navdeep Jaitly, Naveen Ari, Colin Cherry, Parisa Haghani, Otavio Good,\nYoulong Cheng, R. Álvarez, Isaac Caswell, Wei-Ning Hsu, Zongheng Yang, Kuan-Chieh Wang,\nEkaterina Gonina, Katrin Tomanek, Ben Vanik, Zelin Wu, Llion Jones, M. Schuster, Y . Huang,\nDehao Chen, Kazuki Irie, George F. Foster, John Richardson, Uri Alon, and E. al. Lingvo: a\nmodular and scalable framework for sequence-to-sequence modeling. ArXiv, abs/1902.08295,\n2019.\n[51] Andrew M. Dai and Quoc V . Le. Semi-supervised sequence learning. InAdvances in Neural\nInformation Processing Systems, 2015.\n[52] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. Sequence to sequence learning with neural\nnetworks. In Advances in Neural Information Processing Systems, 2014.\n[53] Zohar Karnin, Tomer Koren, and Oren Somekh. Almost optimal exploration in multi-armed\nbandits. In Proceedings of the 30th International Conference on Machine Learning, 2013.\n[54] Lisha Li, Kevin Jamieson, Giulia DeSalvo, Afshin Rostamizadeh, and Ameet Talwalkar. Hy-\nperband: A novel bandit-based approach to hyperparameter optimization. Journal of Machine\nLearning Research, 18(185):1–52, 2018.\n[55] Thomas Helmuth, N. McPhee, and L. Spector. Program synthesis using uniform mutation by\naddition and deletion. Proceedings of the Genetic and Evolutionary Computation Conference,\n2018.\n[56] Noam M. Shazeer and Mitchell Stern. Adafactor: Adaptive learning rates with sublinear\nmemory cost. arXiv preprint arXiv:1804.04235, 2018.\n[57] Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position repre-\nsentations. In NAACL-HLT, 2018.\n[58] Taku Kudo and J. Richardson. Sentencepiece: A simple and language independent subword\ntokenizer and detokenizer for neural text processing. In EMNLP, 2018.\n[59] David Patterson, Joseph Gonzalez, Quoc V . Le, Chen Liang, Lluís-Miquel Munguía,\nD. Rothchild, David R. So, Maud Texier, and J. Dean. Carbon emissions and large neural\nnetwork training. arXiv preprint arXiv:2104.10350, 2021.\n[60] 24/7 carbon-free energy: Methodologies and metrics.https://www.gstatic.com/gumdrop/\nsustainability/24x7-carbon-free-energy-methodologies-metrics.pdf . Ac-\ncessed: 2021-09-14.\n13\nA Appendix\nA.1 TensorFlow Primitives Vocabulary\nName TF Function Argument Mapping\nInput 1 Input 2 Constant Dim\nSize\nADD tf.math.add x y - -\nDIFFERENCE tf.math.subtract x y - -\nDIVIDE tf.math.divide x y - -\nMULTIPLY tf.math.multiply x y - -\nABS ROOT tf.math.sqrt(tf.abs(x)) x - - -\nSQUARE tf.math.square x - - -\nEXP tf.exp x - - -\nLOG tf.log(tf.abs(x)) x - - -\nC MUL tf.math.multiply x - y -\nABS tf.abs x - - -\nRECIP tf.math.reciprocal_no_nan x - - -\nSIGN tf.sign x - - -\nCOS tf.cos x - - -\nSIN tf.sin x - - -\nTANH tf.tanh x - - -\nMAX tf.math.maximum x - y -\nMIN tf.math.minimum x - y -\nSCALE x+tf.Variable() x - - -\nSHIFT x*tf.Variable() x - - -\nSIGMOID tf.sigmoid x - - -\nMASK tf.linalg.band_part input - - -\nCUM PROD tf.math.cumprod x - -\nCUM SUM tf.math.cumsum x - - -\nRED MEAN tf.reduce_mean input_tensor - - -\nRED SUM tf.reduce_sum input_tensor - - -\nRED MIN tf.reduce_min input_tensor - - -\nRED MAX tf.reduce_max input_tensor - - -\nRED PROD tf.reduce_prod input_tensor - - -\nMAT MUL tf.matmul a b - -\nT-MAT MUL tf.matmul(transpose_b=True) a b - -\nCONV 1X1 tf.layers.dense inputs - - units\nCONV 3X1 tf.nn.conv1d input - - ﬁlters\nCONV 7X1 tf.nn.conv1d input - - ﬁlters\nCONV 15X1 tf.nn.conv1d input - - ﬁlters\nCONV 31X1 tf.nn.conv1d input - - ﬁlters\nDCONV 3X1 tf.nn.depthwise_conv2d input - - ﬁlters\nDCONV 7X1 tf.nn.depthwise_conv2d input - - ﬁlters\nDCONV 15X1 tf.nn.depthwise_conv2d input - - ﬁlters\nDCONV 31X1 tf.nn.depthwise_conv2d input - - ﬁlters\nTable 2: TensorFlow (TF) Primitives V ocabulary. “Name” is the name of the operation in our search\nspace. “TF Function” is the TensorFlow function that the name is mapped to when a DNA instruction\nis being converted to a line of TensorFlow code. “Argument Mapping” describes how the values\nin a DNA’s argument set are mapped to the corresponding TensorFlow function arguments. This\nvocabulary is largely constructed from the lowest level TF operations needed to create Transformers\n(see Appendix A.5). We additionally extend those operations to include adjacent operations; for\nexample, we extend MAX to also include MIN , extend RED SUM to include RED PRODUCT , and extend\nCONV 1X1 to include CONV 3X1. We also add commonly used math primitives such as SIN and ABS .\n14\nA.2 Constructing TensorFlow Graphs\nTensorFlow graphs are built from DNA programs as described in Section 2 of the main text. Here we\nprovide additional implementation details.\nRelative Dimensions: We use relative dimensions [13] instead of absolute dimensions for each\ninstruction’s “dimension size” argument. This allows us to resize the models to ﬁt within our\nparameter limits (32M to 38M parameters). The vocabulary for these relative dimensions is [1, 2, 4,\n8, 12, 16, 24, 32, 48, 64]. This vocabulary was not tuned.\nValues Bank: For “constant” and “dimension size” argument ﬁelds, we create a shared bank of\nvalues that each instruction references. The constants bank holds 2 values and the dimension sizes\nbank holds 6 values; these numbers were not tuned. Instead of each instruction possessing their\nown individual values for these arguments, they instead hold an index to these shared banks. This\nallows multiple instructions to share the same value and to change simultaneously when that value\nis changed. For example, each of the individual attention multi-head projections for Q, K and V\nstart off sharing the same output dimension size so that they all change simultaneously if that value\nchanges. See A.4 for an example of how these bank values are mutated.\nCausal Masking: An important part of teacher-forced language model training is that positions\ncannot “see” the token they are trying to predict. Each position should only get information from\nprevious positions, otherwise the model will be degenerate when the targets are not provided. To\nenforce this causal constraint we add additional overhead to operations that move information spatially\nto mask out any information from future positions. For example, when applying convolutions we\nfollow the standard practice of shifting the inputs spatially by (KERNEL WIDTH −1) so that each\nposition only receives information from previous positions.\nBranching: To enable multi-head capabilities for the Transformer search seed, we add a meta\nargument to our instructions called “branching.” This argument can take any value in [1, 2, 4, 8, 16]\nand determines how many times that instruction is executed in parallel, with the resulting tensors\nbeing concatenated together along their embedding axes. Branching can be used with any of the\nTensorFlow primitives as well as with any of a DNA’s subprograms. This allows us to initialize the\nsearch with multi-head self-attention by branching SUBPROGRAM 1 (self-attention) 8 times (see\nAppendix A.5 for subprogram implementations). Primer does not utilize this branching capability in\nany meaningful way, beyond using the initialized multi-head attention.\nResolving Dimension Mismatches: We do not constrain how tensor dimensions can be mutated\nand so programs may be invalid because they perform binary operations on tensors with incompatible\nsizes. For example, a program may describe adding together two tensors with differing embedding\nsizes. To resolve these dimension mismatch issues we deterministically pseudorandomly set one of\nthe tensor dimensions to match the other.\nA.3 Halving Hurdles\nWe conﬁgure our hurdles [13] such that the top 50% of individuals passes each hurdle, according to\nﬁtness. We space the hurdles in such a way that the expected amount of compute devoted to training\neach hurdle band is roughly equal at the end of the search. That is, given that our maximum amount of\ntraining compute for an individual is 7 hours or 25,200 seconds (s), we construct hurdles at the 812.9s,\n2438.7s, 5690.3s, and 12,193.5s marks. Thus, 1/5 of the compute budget is devoted to training every\nindividual up to the ﬁrst hurdle (812.9s), 1/5 of the compute budget is devoted to training the ∼50%\nof individuals that are trained from the ﬁrst to the second hurdle (2438.7s −812.9s = 1625.8s), 1/5 of\nthe compute budget is devoted to training the ∼25% of individuals that are trained from the second to\nthe third hurdle (5690.3s −2438.7s = 3251.6s), etc. This conﬁguration strategy, which we refer to as\n“halving hurdles,” requires setting only one hyperparameter, the number of hurdles, and removes the\nneed to set hurdle threshold values and comparison steps, as has been previously done [13, 35]. We\nchoose four hurdles because ﬁve hurdles would require the ﬁrst hurdle to be anchored at less than\nten minutes of training, which we ﬁnd empirically to be too noisy of a signal. Using hurdles in this\nway decreases the average train time per model to 4064s or about 1 hour and 8 minutes, reducing the\ncompute cost by a factor of ∼6.2X.\n15\nThis strategy is not unlike bandit algorithms such as Successive Halving[ 53] and Hyperband[54],\nhowever we do not use a static population of individuals created a priori, but integrate our halving\nwith the changing evolutionary population.\n0 12500 25000\nNum Individuals\n4.5\n4.0\n3.5\nLog Fitness\nFigure 12: Halving hurdles from our Primer search. Each dot represents the ﬁnal ﬁtness of an\nindividual generated by evolution. Different “bands” form because each hurdle has a different\ntraining allowance. All bands see improvement over time, meaning that the median ﬁtness improves\nfor all compute allowances. This correlation between a model’s performances at different training\nbudgets allows us to reduce our total search cost by roughly a factor of 6.2X.\nA.4 Evolution Search Details\nWe use Regularized Evolution [30] with a population size of 100 and a tournament selection size of\n10. These values were not tuned. The mutations we use are as follows.\nMutations: To create new candidates in our search, we uniform randomly select a parent from\nour search population and apply a single mutation to it. We employ ﬁve different mutation types\n(selections and decisions are performed uniform randomly unless speciﬁed otherwise):\n• Delete: Remove an instruction from a subprogram.\n• Insert: Create an instruction and insert it into a subprogram.\n• Delete and Insert: Perform a delete mutation followed by an insert mutation [55].\n• Mutate Field: Select a ﬁeld from an instruction and change its value.\n• Swap: Swap the position of two instructions in a randomly selected subprogram. The input\ntensors for each instruction are also swapped so that the net effect is switching the positions\nof the instructions in the compute graph.\n• Mutate Bank Value: Change the value of a relative tensor dimension or constant in the\ncorresponding bank. The values for relative tensor dimensions are selected from their\nvocabulary (see Appendix A.2). The values for constants are changed according to\ncnew := cprev ·10X + Y for previous value cprev, new value cnew and random variables\nX,Y ∼N(0,1).\nAfter a mutation is applied, we run a light check to see if the resulting candidate’s compute graph is\nexactly equivalent to the parent’s compute graph. If it is, we perform another mutation.\nA.5 Transformer and Primer Program Comparisons\nHere we present the programs for both the Transformer seed and the discovered Primer model. Table 3\nis a key that maps operation names to graph symbols for subsequent graphs. Figures 13 to 22 depict\nthe subprograms for each model with the Primer changes highlighted in orange. Figure 23 depicts the\nfull compute graphs for each model, with all subprograms resolved to their constituent primitives.\nFigures 24 and 25 depict the DNA programs for Transformer and Primer with all subprograms\nresolved and all instruction bank values plugged in.\n16\nName Graphing symbol\nADD +\nDIFFERENCE −\nDIVIDE ÷\nMULTIPLY ×\nABS ROOT √\nSQUARE x2\nEXP ex\nLOG Log\nC MUL ×C\nABS |x|\nRECIP Recip\nSIGN Sign\nCOS Cos\nSIN Sin\nTANH Tanh\nMAX Max\nMIN Min\nSCALE Scale\nSHIFT Shift\nSIGMOID Sigm\nMASK Mask\nCUM PROD Cum Prod\nCUM SUM Cum Sum\nRED MEAN Reduce Mean\nRED SUM Reduce Sum\nRED MIN Reduce Min\nRED MAX Reduce Max\nRED PROD Reduce Prod\nMAT MUL M ×N\nT-MAT MUL M ×NT\nCONV 1X1 Conv 1x1\nCONV 3X1 Conv 3x1\nCONV 7X1 Conv 7x1\nCONV 15X1 Conv 15x1\nCONV 31X1 Conv 31x1\nDCONV 3X1 D-wise 3x1\nDCONV 7X1 D-wise 7x1\nDCONV 15X1 D-wise 15x1\nDCONV 31X1 D-wise 31x1\nTable 3: Key for primitives mapped to corresponding symbols used in the following graphs.\n17\nSubprogram 0 \n(S0: Main)\nIN 0 IN 1\nS5\n(a) The normalization\n(S5) and feed forward\n(S2) subprograms switch\nplaces.\n(b) Shrink hidden\ndimension size to 384.\nS1\nConv \n1x1 \nS8\nS5\nS2\nS8\nBranch: 8\nOUT\nSubprogram 0 \n(S0: Main)\nIN 0 IN 1\nS5\nS1\nConv \n1x1 \nS8\nS2\nS5\nS8\nBranch: 8\nOUT\n(a) \nDim: 384 Dim: 512 \nTransformer Primer\n(b) \nFigure 13: Main subprograms. Changes are highlighted in orange.\n18\nSubprogram 1\n(S1: Self-Attention)\nDim: 384\nSubprogram 1\n(S1: Self-Attention)\nIN 0\nS3 S3 S3\n×C\nM×NT\nS4\nM×N\nOUT\nIN 1\nC: 0.125\nIN 0\nS3 S3 S3\nConv\n1x1\nM×NT\nS4\nM×N\nOUT\nIN 1\nScale\n(a) Q and K projections\nshare weights.\n(b) Post-Softmax (S4)\nSpatial Gating.\n(a)\n(b)\nTransformer Primer\nFigure 14: Attention subprograms. Changes are highlighted in orange.\n19\nSubprogram 2 \n(S2: Feed Forward)\nSubprogram 2 \n(S2: Feed Forward)\nIN 0 IN 1\nConv \n1x1 \nShift \nS9 \nConv \n1x1 \nOUT\nIN 0 IN 1\nConv \n1x1 \nShift \nS9 \nConv \n1x1 \nShift \nOUT\n(a) \n(b) \n(a) Increase upwards\nprojection size to 12X\ninput size. \n(b) Delete shift after the\ndownwards projection.\nDim: 2048 \nDim: 512  Dim: 384\nDim: 4608\nTransformer Primer\nFigure 15: Feed forward subprograms. Changes are highlighted in orange.\nSubprogram 3\n(S3: Multi-head Projection)\nSubprogram 3\n(S3: MDHA Projection)\nIN 0 IN 1\nConv\n1x1\nOUT\nIN 0 IN 1\nConv 1x1\nOUT\nMax\nD-wise\n3x1\n×C C: -1.12\n(a)\n(b)(a) Add depth-wise\nconvolution.\n(b) Multiply by\nconstant.\nDim: 384 \nDim: 512 Dim: 384 \nTransformer Primer\nFigure 16: Multi-head projection subprograms. Changes are highlighted in orange.\n20\nSubprogram 4 \n(S4: Attention Softmax)\nSubprogram 4 \n(S4: Attention Softmax)\nIN 0 IN 1\nOUT\ne x \nMask \nReduce \nSum \n÷ \nT anh \nIN 0 IN 1\nOUT\nMask \nReduce \nSum \n÷ \ne x \n(a) \n(a) Add tanh activation\nafter softmax. \nTransformer Primer\nFigure 17: Softmax subprograms. Changes are highlighted in orange.\nSubprogram 5 \n(S5: Layer Norm)\nIN 0 IN 1\nS6 \nS7 \nOUT\nIN 0 IN 1\nS6 \nS7 \nOUT\nSubprogram 5 \n(S5: Custom Norm)\nChanges in downstream\ncomponents (S6). \nTransformer Primer\nFigure 18: Normalization subprograms. Changes to this subprogram are realized in downstream\nchanges to S6.\n21\nSubprogram 6 \n(S6: Z-Score Normalization)\nIN 0 IN 1\nReduce \nMean \n− \n×  \nReduce \nMean \n√ \n÷  \nOUT\n(a) Change input to\nmultiplication. \nIN 0 IN 1\nReduce \nMean \n− \n×  \nReduce \nMean \n√ \n÷  \nOUT\nSubprogram 6 \n(S6: Normalization)\n(a) \nTransformer Primer\nFigure 19: Z-score normalization subprograms. Changes are highlighted in orange.\nSubprogram 7 \n(S7: Scale-shift) \nIN 0 IN 1\nScale \nShift  \nOUT\nSubprogram 7 \n(S7: Scale-shift) \nIN 0 IN 1\nScale \nShift  \nOUT\nNo changes. \nTransformer Primer\nFigure 20: Scale-shift subprograms. No changes here.\n22\nSubprogram 8 \n(S8: Residual Connection) \nSubprogram 8 \n(S8: Residual Connection) \nIN 0\n+ \nIN 1 \nOUT\nIN 0\n+ \nIN 1 \nOUT\n+ \n(a) \n(a) Add additional\nresidual connection after\nthe ﬁrst one.\nTransformer Primer\nFigure 21: Residual connection subprograms. This change is essentially a functional no-op.\nSubprogram 9 \n(S9: ReLU) \nSubprogram 9 \n(S9: Squared ReLU) \nIN 0 IN 1\nMax \nOUT\nx 2 \nIN 0 IN 1\nMax \nOUT\nC: 0C: 0\n(a) Square ReLU outputs. (a) \nTransformer Primer\nFigure 22: Activation function subprograms. Changes are highlighted in orange.\n23\nIN 0\nR. Mean \n−  \n×  \nR. mean \n√  \n÷  \nScale\nShift \nBranch 8\nConv 1x1 Conv 1x1 \n×C  \nM ×N T  \ne x  \nMask \n÷  \nM×N  \nBranch Merge \nReduce S.\nConv 1x1 \nConv 1X1 \n+  \nReduce M.\n−  \n×  \nReduce M.\n√  \n÷  \nScale\nShift \nConv 1X1 \nShift \nMax(..., 0) \nConv 1X1 \nShift \n+  \nIN 0\nIN 0\nR. Mean \n−  \n×  \nR. mean \n√  \n÷  \nScale\nShift \nBranch 8\nConv 1x1 Conv 1x1 \nD-wise 3x1 D-wise 3x1 \n×C  \nConv 1x1 \nM ×N T  \ne x  \nMask \n÷  \nTanh\nScale\nM×N  \n×C  \nBranch Merge \nReduce S.\nConv 1X1 \n+  \n+  \nBranch 2\nConv 1X1 \nBranch Merge \nShift \nMax(..., 0) \nx 2  \nConv 1X1 \nReduce M.\n−  \n×  \nReduce M.\n√  \n÷  \nScale\nShift \n+  \n+  \nIN 0\nSwapped \nFigure 23: Comparison of Transformer (Left) and Primer (Right) programs, with all subprograms\nresolved to their constituent primitives. Primer differences are highlighted in orange.\n24\nTRANSFORMER\n(0) INPUT\n(1) INPUT\n(2) REDUCE_MEAN In0: 0 In1: 0 Dim: 128 C: 0.00\n(3) DIFFERENCE In0: 0 In1: 2 Dim: 128 C: 0.00\n(4) MULTIPLY In0: 3 In1: 3 Dim: 128 C: 0.00\n(5) REDUCE_MEAN In0: 4 In1: 4 Dim: 128 C: 0.00\n(6) ABS_SQUARE_ROOT In0: 5 In1: 5 Dim: 128 C: 0.00\n(7) DIVIDE In0: 3 In1: 6 Dim: 128 C: 0.00\n(8) SCALE In0: 7 In1: 7 Dim: 128 C: 0.00\n(9) SHIFT In0: 8 In1: 8 Dim: 128 C: 0.00\n(10) BRANCH_8_INPUT_1 In0: 9 In1: 9 Dim: 128 C: 0.00\n(11) BRANCH_8_INPUT_2 In0: 9 In1: 9 Dim: 128 C: 0.00\n(12) DENSE In0: 10 In1: 10 Dim: 64 C: 0.00\n(13) DENSE In0: 10 In1: 10 Dim: 64 C: 0.00\n(14) CONSTANT_MUL In0: 13 In1: 13 Dim: 128 C: 0.12\n(15) DENSE In0: 10 In1: 10 Dim: 64 C: 0.00\n(16) TRANSPOSE_MAT_MUL In0: 14 In1: 12 Dim: 128 C: 0.00\n(17) EXP In0: 16 In1: 16 Dim: 128 C: 0.00\n(18) EMBEDDING_MASK In0: 17 In1: 17 Dim: 128 C: 0.00\n(19) REDUCE_SUM In0: 18 In1: 18 Dim: 128 C: 0.00\n(20) DIVIDE In0: 18 In1: 19 Dim: 128 C: 0.00\n(21) MAT_MUL In0: 20 In1: 15 Dim: 128 C: 0.00\n(22) BRANCH_MERGE In0: 21 In1: 21 Dim: 512 C: 0.00\n(23) DENSE In0: 22 In1: 22 Dim: 512 C: 0.00\n(24) ADD In0: 0 In1: 23 Dim: 128 C: 0.00\n(25) REDUCE_MEAN In0: 24 In1: 24 Dim: 128 C: 0.00\n(26) DIFFERENCE In0: 24 In1: 25 Dim: 128 C: 0.00\n(27) MULTIPLY In0: 26 In1: 26 Dim: 128 C: 0.00\n(28) REDUCE_MEAN In0: 27 In1: 27 Dim: 128 C: 0.00\n(29) ABS_SQUARE_ROOT In0: 28 In1: 28 Dim: 128 C: 0.00\n(30) DIVIDE In0: 26 In1: 29 Dim: 128 C: 0.00\n(31) SCALE In0: 30 In1: 30 Dim: 128 C: 0.00\n(32) SHIFT In0: 31 In1: 31 Dim: 128 C: 0.00\n(33) DENSE In0: 32 In1: 32 Dim: 2048 C: 0.00\n(34) SHIFT In0: 33 In1: 33 Dim: 128 C: 0.00\n(35) MAX In0: 34 In1: 34 Dim: 128 C: 0.00\n(36) DENSE In0: 35 In1: 35 Dim: 512 C: 0.00\n(37) SHIFT In0: 36 In1: 36 Dim: 128 C: 0.00\n(38) ADD In0: 24 In1: 37 Dim: 128 C: 0.00\nFigure 24: List of instructions for Transformer program, with all subprograms resolved to their\nconstituent primitives.\n25\nPRIMER\n(0) INPUT\n(1) INPUT\n(2) REDUCE_MEAN In0: 0 In1: 0 Dim: 768 C: -1.12\n(3) DIFFERENCE In0: 0 In1: 2 Dim: 768 C: -1.12\n(4) MULTIPLY In0: 3 In1: 0 Dim: 768 C: -1.12\n(5) REDUCE_MEAN In0: 4 In1: 4 Dim: 768 C: -1.12\n(6) ABS_SQUARE_ROOT In0: 5 In1: 5 Dim: 768 C: -1.12\n(7) DIVIDE In0: 3 In1: 6 Dim: 768 C: -1.12\n(8) SCALE In0: 7 In1: 7 Dim: 768 C: -1.12\n(9) SHIFT In0: 8 In1: 8 Dim: 384 C: -0.57\n(10) BRANCH_8_INPUT_1 In0: 9 In1: 9 Dim: 768 C: -1.12\n(11) BRANCH_8_INPUT_2 In0: 9 In1: 9 Dim: 768 C: -1.12\n(12) MAX In0: 10 In1: 10 Dim: 768 C: -0.57\n(13) DENSE In0: 10 In1: 10 Dim: 48 C: -1.12\n(14) DEPTHWISE_CONV_3X1 In0: 13 In1: 10 Dim: 384 C: -1.12\n(15) CONSTANT_MUL In0: 14 In1: 14 Dim: 384 C: -1.12\n(16) MAX In0: 11 In1: 11 Dim: 768 C: -0.57\n(17) DENSE In0: 10 In1: 10 Dim: 48 C: -1.12\n(18) DEPTHWISE_CONV_3X1 In0: 17 In1: 10 Dim: 384 C: -1.12\n(19) CONSTANT_MUL In0: 18 In1: 18 Dim: 384 C: -1.12\n(20) DENSE In0: 19 In1: 11 Dim: 48 C: -1.12\n(21) MAX In0: 10 In1: 10 Dim: 768 C: -0.57\n(22) DENSE In0: 10 In1: 10 Dim: 48 C: -1.12\n(23) DEPTHWISE_CONV_3X1 In0: 22 In1: 10 Dim: 384 C: -1.12\n(24) CONSTANT_MUL In0: 23 In1: 23 Dim: 384 C: -1.12\n(25) TRANSPOSE_MAT_MUL In0: 20 In1: 19 Dim: 768 C: -1.12\n(26) EXP In0: 25 In1: 25 Dim: 768 C: -1.12\n(27) EMBEDDING_MASK In0: 26 In1: 26 Dim: 768 C: -1.12\n(28) REDUCE_SUM In0: 27 In1: 27 Dim: 768 C: -1.12\n(29) DIVIDE In0: 27 In1: 28 Dim: 768 C: -1.12\n(30) TANH In0: 29 In1: 25 Dim: 384 C: -1.12\n(31) SCALE In0: 30 In1: 19 Dim: 384 C: -1.12\n(32) MAT_MUL In0: 31 In1: 24 Dim: 768 C: -1.12\n(33) BRANCH_MERGE In0: 32 In1: 32 Dim: 384 C: -1.12\n(34) DENSE In0: 33 In1: 33 Dim: 384 C: -1.12\n(35) ADD In0: 0 In1: 34 Dim: 768 C: -1.12\n(36) ADD In0: 35 In1: 34 Dim: 768 C: -1.12\n(37) BRANCH_2_INPUT_1 In0: 36 In1: 36 Dim: 2304 C: -1.12\n(38) BRANCH_2_INPUT_2 In0: 36 In1: 36 Dim: 2304 C: -1.12\n(39) DENSE In0: 37 In1: 38 Dim: 2304 C: -1.12\n(40) BRANCH_MERGE In0: 39 In1: 39 Dim: 4608 C: -1.12\n(41) SHIFT In0: 40 In1: 40 Dim: 768 C: -1.12\n(42) MAX In0: 41 In1: 41 Dim: 768 C: -0.57\n(43) SQUARE In0: 42 In1: 41 Dim: 768 C: -1.12\n(44) DENSE In0: 43 In1: 43 Dim: 384 C: -1.12\n(45) REDUCE_MEAN In0: 44 In1: 44 Dim: 768 C: -1.12\n(46) DIFFERENCE In0: 44 In1: 45 Dim: 768 C: -1.12\n(47) MULTIPLY In0: 46 In1: 44 Dim: 768 C: -1.12\n(48) REDUCE_MEAN In0: 47 In1: 47 Dim: 768 C: -1.12\n(49) ABS_SQUARE_ROOT In0: 48 In1: 48 Dim: 768 C: -1.12\n(50) DIVIDE In0: 46 In1: 49 Dim: 768 C: -1.12\n(51) SCALE In0: 50 In1: 50 Dim: 768 C: -1.12\n(52) SHIFT In0: 51 In1: 51 Dim: 384 C: -0.57\n(53) ADD In0: 36 In1: 52 Dim: 768 C: -1.12\n(54) ADD In0: 53 In1: 52 Dim: 768 C: -1.12\nFigure 25: List of instructions for Primer program, with all subprograms resolved to their constituent\nprimitives.\n26\nA.6 Exact LM1B Numbers\nModel Params Train Steps Step/Sec PPLX Speedup\nTensor2Tensor, TPUv2\nVanilla Transformer 35M 1.9M 22.4 35.44 +/- 0.30 -\nTransformer+GELU 35M 1.9M 22.4 35.00 +/- 0.12 1.23 +/- 0.07\nTransformer++ 35M 1.9M 22.0 34.87 +/- 0.46 1.37 +/- 0.24\nPrimer 34M 1.9M 21.7 33.77 +/- 0.15 2.12 +/- 0.09\nPrimer-EZ 35M 1.8M 21.0 33.53 +/- 0.09 2.34 +/- 0.04\nTransformer+MDHA 35M 1.8M 21.0 34.26 +/- 0.12 1.76 +/- 0.06\nTransformer+Sep Conv 35M 1.8M 21.0 34.34 +/- 0.10 1.54 +/- 0.05\nTensor2Tensor, V100\nVanilla Transformer 35M 1.3M 15.4 37.19 +/- 0.07 -\nTransformer+GELU 35M 1.2M 14.1 37.11 +/- 0.02 1.05 +/- 0.02\nTransformer++ 35M 1.3M 14.7 36.23 +/- 0.11 1.54 +/- 0.05\nPrimer 34M 1.2M 13.8 35.06 +/- 0.15 2.13 +/- 0.11\nPrimer-EZ 35M 1.1M 13.3 35.16 +/- 0.13 2.03 +/- 0.09\nT5, TPUv2\nVanilla Transformer 35M 2.1M 23.9 23.30 +/- 0.02 -\nTransformer+GELU 35M 2.1M 23.8 23.39 +/- 0.02 0.97 +/- 0.03\nTransformer++ 35M 2.1M 24.2 23.04 +/- 0.02 1.33 +/- 0.05\nEvolved Transformer 38M 1.6M 18.7 23.08 +/- 0.02 1.23 +/- 0.02\nPrimer 36M 2.0M 22.9 22.71 +/- 0.03 1.72 +/- 0.01\nPrimer-EZ 36M 2.0M 22.5 22.62 +/- 0.02 1.75 +/- 0.03\nTable 4: Comparison on the search task, auto-regressive language modeling on LM1B, across two\ndifferent hardware platforms (TPUv2s and V100 GPUs) and two different libraries (Tensor2Tensor\nand T5), using those libraries’ default hyperparameters. This table contains the precise numbers\nfor Figure 6. “Speedup” describes the fraction of compute used by each model to achieve the same\nresults as the vanilla Transformer baseline trained with the full compute budget. Even though Primer\nwas developed in Tensor2Tensor using TPUv2s, it shows strong performance on GPU and in T5.\nPerplexity is reported with respect to each library’s default tokenization.\nA.7 Ablation and Insertion Studies\nOne of the core motivations of this work is to develop simple and robust Transformer modiﬁcations.\nTo that end, we study the individual effectiveness of each Primer modiﬁcation, described in Section 3\nof the main text. We measure this effectiveness using insertion and ablation studies. In the insertion\nstudies we add each modiﬁcation in isolation to a vanilla Transformer. In the ablation studies we\nremove each modiﬁcation from Primer one at a time. We are interested in how these modiﬁcations\naffect performance not just in our search library, Tensor2Tensor, but also in other libraries. Thus, we\nperform these insertion and ablation studies in a different library, T5, as a well, and use modiﬁcation\ntransferability as the key guiding metric for our modeling recommendations.\nThe results of these studies are shown in Figure 26. “Normalized PPLX Delta” describes the degree\nto which a modiﬁcation helps or hurts performance. For baseline perplexity, Pb, and modiﬁcation\nperplexity, Pm, “Normalized PPLX Delta” is deﬁned as Pb−Pm\nPb\nin the insertion study and Pm−Pb\nPb\nfor the ablation study. These deﬁnitions differ so that a positive value always indicates that the\nmodiﬁcation is good and a negative value always indicates that the modiﬁcation is bad. Three\ntechniques are beneﬁcial in all scenarios. The ﬁrst is “12X proj,” which increases the size of the\nTransformer feed forward upwards projection while controlling for parameters. We ﬁnd this works\nwell for smaller models but is not useful at larger sizes. The second two, MDHA and squared ReLUs,\nare the deﬁning modiﬁcations of Primer-EZ, a simpler model that captures much of the gains of the\nfull Primer.\n27\n+Squared Relu\n+MDHA\n+PrePost Norm\n+12X Proj\n+Custom Norm\n+Share QK\n0Normalized PPLX Delta\nInsertion Study\nT2T Code Base\nT5 Code Base\n-Squared Relu\n-MDHA\n-PrePost Norm\n-12X Proj\n-Custom Norm\n-Share QK\n0Normalized PPLX Delta\nAblation Study\nT2T Code Base\nT5 Code Base\nFigure 26: Investigation into transferability of Primer modiﬁcations on LM1B at ∼35M parameters.\nIn the “Insertion Study” we insert each of the modiﬁcations into a vanilla Transformer. In the\n“Ablation Study,” we remove each modiﬁcation from Primer. “Normalized PPLX Delta” indicates\nthe degree to which the treated models are affected by these modiﬁcations; values are normalized to\nbe comparable across code bases and so that positive values indicate beneﬁcial techniques in both\nstudies. Likewise, negative values indicate harmful techniques in both studies.\nA.8 Full Training Details\nIn all experiments, we use previously published hyperparameter settings that were tuned for Trans-\nformer, with regularization disabled and no additional tuning for Primer. In Tensor2Tensor (T2T)\nthese are the TRANSFORMER _TPU hyperparameters and in T5 and Lingvo these are the open-sourced\nparameters used in previous T5 studies [ 5, 49]. They both specify an Adafactor optimizer [ 56],\nwith 10K warmup steps at a learning rate of 0.01, followed by reciprocal square root learning rate\ndecay. T2T uses positional embeddings and subword tokenization, while T5 and Lingvo use relative\nattention [57] and SentencePieces [58].\nFor LM1B, we use the T2T default settings of max sequence length of 64 and batches of 4096 tokens;\nthis is appropriate because LM1B has an average sequence length of roughly 32. For C4 and PG19,\nwe use the T5 default of a max sequence length of 512. For one-shot pretraining, we use a max\nsequence length of 1024. In Section 4.2 we use batches of 65K tokens, in Section 4.3 we use batches\nof 1M tokens, and in Section 4.4 we uses batches of 2M tokens.\nA.9 Power Law Compute Savings Derivations\nIn Section 4.1 of the main text, we reproduce the results of Kaplan et al. [9] and show that, at optimal\nparameter sizing, the relationship between language model quality and training compute follows a\npower law: l = ac−k, where lis validation loss, cis training compute, and aand kare empirical\nconstants. This is represented as a line in double log space (Figure 7): log l = −klog c+ loga.\nHowever, these lines are not the same for each architecture we compare. The lines are roughly parallel\nbut shifted up and down. Thus, deﬁning the shift between two architectures’ lines aslog bk, we can\nderive the relationship of their training costs as:\n−klog c0 + loga0 = −klog c1 + loga0 + logbk\n−klog c0 = −klog c1 + logbk\nc−k\n0 = bkc−k\n1\nc0 = c1/b\nwhere bis a consistent reduction factor regardless of l. Compute savings, s, for using a superior\narchitecture can now be calculated as:\n28\ns= c1 −c0\ns= c1 −c1/b= c1(1 −1/b)\nor\nc1 = s\n1 −1/b\nPlugging this into the original power law relationship for c1 we get:\nl= a1\n( s\n1 −1/b\n)−k\nl= a1(1 −1/b)ks−k\nThus, the relationship between quality and compute savings yielded by an improved architecture also\nfollows a power law with coefﬁcient a1(1 −1/b)k. This relationship is intuitive when recognizing\nthat the compute reduction factor bis consistent for all values of land thus a power law investment of\ntraining compute with relation to lresults in a power law savings with relation to las well.\nA.10 Exact T5 Numbers for Medium Sized Experiments\nBaseline Compute @525K Baseline Compute @1M\nModel Params Steps PPLX Speedup Steps PPLX Speedup\nC4\nVanilla Transformer 110M 525K 20.61 - 1M 19.82 -\nTransformer+GELU 110M 524K 20.34 1.20 998K 19.58 1.26\nTransformer++ 110M 524K 20.03 1.52 998K 19.28 1.64\nEvolved Transformer 110M 351K 20.79 0.89 668K 19.84 0.98\nPrimer 110M 483K 19.82 1.68 920K 19.07 1.91\nPrimer-EZ 110M 471K 19.83 1.71 896K 19.07 1.90\nSwitch Transformer 550M 525K 17.16 - 1M 16.32 -\nSwitch Primer 550M 474K 16.56 1.45 900K 15.82 1.56\nSynthesizer 145M 525K 20.35 - 1M 19.57 -\n+ Squared ReLU 145M 523K 19.55 1.74 996K 18.83 1.96\nPG19\nVanilla Transformer 110M 525K 16.39 - 1M 15.83 -\nTransformer+GELU 110M 524K 16.35 1.01 998K 15.84 0.95\nTransformer++ 110M 524K 16.15 1.18 998K 15.64 1.20\nPrimer 110M 483K 15.96 1.68 920K 15.31 1.81\nPrimer-EZ 110M 471K 15.84 1.74 896K 15.37 1.98\nTable 5: Language modeling comparison on larger datasets, transferring Primer to the T5 codebase.\nIn this transferred regime, Primer improves upon all baselines. Furthermore, Primer-EZ not only\nreaches parity with Primer, but in some cases, surpasses it. Switch Transformer and Synthesizer also\nbeneﬁt from the Primer-EZ modiﬁcations. Compute budget comparison points are chosen according\nto how long it takes vanilla baselines to reach 525K and 1M training steps. Perplexities are given\nwith respect to SentencePieces. This table has the precise numbers for Figure 9.\n29\nA.11 Performance on Individual One-Shot Tasks\nTask Metric Transf.\n1/3\nTransf.\nFull\nPrimer\n1/3\nPrimer\nFull\nGPT-3\nXL\nPretraining pplx 15.3 14.3 14.3 13.5 -\nQuestion Answering Tasks\nTriviaQA acc 22.5 ±0.4 26 .8 ±0.5 27 .5 ±0.4 32.2 ±0.5 26.5\nWebQs acc 9.1 ±0.5 9 .6 ±0.4 9 .8 ±0.8 10 .4 ±0.3 9.2\nNQs acc 5.8 ±0.2 6 .7 ±0.2 7.8 ±0.5 9 .1 ±0.3 5.4\nSQuADv2 f1 54.2 ±2.4 65 .4 ±2.9 64 .2 ±3.7 67 .8 ±1.2 54\nCoQa f1 52.5 ±1.1 57 .7 ±1.2 59 .1 ±0.9 61.2 ±0.7 66.1\nDROP f1 21.5 ±0.4 23 .4 ±0.2 24.8 ±0.5 26 .5 ±0.2 23\nQuac f1 30.1 ±0.5 30 .9 ±0.7 28 .9 ±0.9 30 .2 ±0.7 32.3\nLAMBADA acc 51.5 ±0.9 55 .2 ±1.3 54 .5 ±1.1 56 .8 ±0.9 58.3\nQA Average avg 30.9 34.5 34.6 36.8 34.3\nMulti-Choice Schema Tasks\nHellaSwag acc 55.7 ±0.3 59 .5 ±0.2 60.2 ±0.3 63 .3 ±0.2 53.5\nStoryCloze acc 75.2 ±0.3 75 .9 ±0.4 76.9 ±0.2 77 .5 ±0.3 74.2\nWinogrande acc 55.4 ±0.3 58 .4 ±0.4 58 .8 ±0.3 60.4 ±0.2 59.1\nPIQA acc 72.6 ±0.5 72 .6 ±0.3 73 .7 ±0.5 75.0 ±0.4 74.4\nARC (Challenge) acc 32.7 ±0.4 34 .4 ±0.3 35 .6 ±0.9 37.4 ±0.4 36.4\nARC (Easy) acc 64.5 ±0.5 64 .9 ±0.5 65 .6 ±0.6 67.5 ±0.5 55.9\nOpenBookQA acc 45.3 ±0.9 46 .8 ±0.8 47 .9 ±0.4 49.3 ±0.5 46.4\nANLI R1 acc 33.9 ±1.2 35 .5 ±0.2 35 .5 ±0.4 34.8 ±0.3 34.6\nANLI R2 acc 33.5 ±0.7 33 .4 ±0.5 34 .5 ±0.6 33 .5 ±0.4 32.7\nANLI R3 acc 34.5 ±0.7 35 .2 ±0.1 33.0 ±0.3 33.8 ±0.5 33.9\nReCoRD acc 84.8 ±0.1 86 .3 ±0.2 85 .8 ±0.3 86.7 ±0.0 83\nWSC acc 67.4 ±0.8 66 .8 ±1.2 69 .3 ±1.3 68 .9 ±1.2 62.5\nBoolQ acc 58.9 ±1.1 63 .6 ±2.1 60 .7 ±0.8 64 .7 ±2.0 63.7\nCB acc 56.3 ±2.5 53 .0 ±2.7 55 .4 ±3.3 56 .6 ±9.6 48.2\nRTE acc 48.4 ±1.2 53 .6 ±2.5 54 .3 ±1.5 52 .9 ±2.8 49.5\nCOPA acc 80.2 ±3.2 87 .2 ±1.2 84 .8 ±1.5 87 .5 ±1.1 74\nWiC acc 51.6 ±0.2 51 .0 ±0.5 51.7 ±0.1 51 .8 ±0.1 49.2\nRACE-h acc 39.4 ±0.4 40 .8 ±0.4 40 .4 ±0.4 43.7 ±0.3 42\nRACE-m acc 50.0 ±1.0 52 .6 ±0.4 51 .8 ±0.8 54.0 ±0.4 55.2\nMulti-Choice Average avg 53.1 54.7 55 56.2 54.1\nTable 6: Comparison between Transformer+GELU and Primer at 1.9B parameters on downstream\none-shot tasks at 1/3 and full pretraining compute budgets. One-shot sample means and standard\ndeviations are computed using the evaluated performance of 5 weight checkpoints. Bold numbers\ndenote improved one-shot performance and shaded numbers denote worse one-shot performance\ncompared to Transformer with full compute that is statistically signiﬁcant under an independent t-test\nwith p-value threshold 0.05. Primer achieves the same performance as Transformer when given 1/3\nthe training compute and stronger performance on a majority of tasks when given the same training\ncompute. GPT-3 XL [ 7] scores are provided as a grounding reference point; they should not be\nclosely compared to our results as the models have different pretraining conﬁgurations.\n30\n31\n38\nANLI R1\nTransf. 1/3\nTransf. Full\nPrimer 1/3\nPrimer Full\n31\n36\nANLI R2\n31\n37\nANLI R3\n30\n39\nARC (Challenge)\n62\n69\nARC (Easy)\n54\n68\nBoolQ\n47\n62\nCB\n74\n92\nCOPA\n49\n63\nCoQa\n19\n28\nDROP\n54\n64\nHellaSwag\n48\n59\nLAMBADA\n4\n10\nNQs\n43\n51\nOpenBookQA\n70\n76\nPIQA\n26\n33\nQuac\n37\n45\nRACE-h\n47\n56\nRACE-m\n83\n88\nReCoRD\n43\n58\nRTE\n48\n73\nSQuADv2\n73\n79\nStoryCloze\n20\n33\nTriviaQA\n6\n12\nWebQs\n49\n53\nWiC\n53\n61\nWinogrande\n64\n72\nWSC\nFigure 27: Comparison between Transformer+GELU and Primer at 1.9B parameters on downstream\none-shot tasks at 1/3 and full pretraining compute budgets. 95% conﬁdence intervals are provided\naccording to an independent t-test, using a sample of 5 pretraining weight checkpoints. Primer\nachieves roughly the same performance as Transformer when given 1/3 the pretraining compute\nand stronger performance on a majority of tasks when given the same pretraining compute. Exact\nnumbers are presented in Table 6.\nA.12 Masked Language Modeling\nEncoder-decoder style masked language modeling (MLM) is not the focus of this work. However,\nbecause it was the focus of the original T5 project, we include MLM comparisons here for com-\npleteness (Table 7). Speciﬁcally, we use the exact comparison conﬁguration used by Narang et\nal.[49], who benchmarked several Transformer variants; the one difference is that we only run model\ntraining one time, since this regime is not the focus of our study. For “Primer-EZ Decoder” we use a\nTransformer++ encoder and a Primer-EZ decoder. Our treatments demonstrate that the Primer-EZ\nmodiﬁcations have the capacity to improve encoder-decoder MLM models, but perhaps to a lesser\ndegree, when compared to Transformer++. We believe this indicates that decoder-only LM and\nencoder-decoder MLM beneﬁt from different modeling decisions – something that could be studied\nin future works. We also believe that running our search on encoder-decoder MLM directly could\nyield modiﬁcations that are more beneﬁcial for this task.\n31\nModel Params Pretraining Log PPLX SGLUE XSum WebQ\nVanilla Transformer* 223M 1.838 70.97 17.78 23.02\nTransformer+GeLU* 223M 1.838 73.67 17.86 25.13\nTransformer++ 224M 1.792 75.65 17.90 25.92\nPrimer-EZ Decoder 224M 1.787 76.69 17.87 24.87\nTable 7: Masked language modeling comparison on C4 in T5 with encoder-decoder style models.\nThese results are run in the exact same conﬁguration as Narang et al. [49], although we only run our\nmodels once, as MLM is not the focus of our work. * indicates rows that are taken from that study.\nA.13 Carbon Emission Estimates\nFollowing the recommendations of Patterson et al. [59], we release the carbon emission estimates for\nour largest experiments.\nTo estimate the carbon emissions3,4 for our architecture search, we build off of the measurements\ntaken by Patterson et al. Their emissions estimate for architecture search is 3.2 MTCO 2e for\n1360 days of TPUv2 usage [ 59]. Here, we use 1145.8 days of TPUv2 compute for our search.\nAdditionally, the PUE for our data center5 at the time of our search was 1.08 instead of 1.10, and\nits net carbon intensity average was 0.336 MTCO 2e/MWh instead of 0.431 MTCO 2e/MWh.6,7\nThus, the proportional emissions estimate for our architecture search experiments is 3.2 MTCO2e\n∗1145.8\n1360 ∗1.08\n1.10 ∗336\n431 = 2.06 MTCO2e. For comparison, a round trip plane ticket from San Francisco\nto New York for a single passenger is ∼1.2 MTCO2e [59] and so our search costs roughly 1.72 such\nplane tickets.\nWe follow the same process of building off of the Patterson et al. measurements to estimate emissions\nfor our large scale T5 experiments. The Patterson et al. emissions estimate for 11B parameter T5\nis 46.7 tCO2e for 10,249 days of TPUv3 usage. Our T5 models are smaller, and so only require\n687.5 TPUv3 days to train on average. We run 3 trainings (Primer, original T5 and T5++) to show\nPrimer’s improvements over baselines, yielding a total of 2062.5 TPUv3 days. When we ran our\nexperiments, the data center8 PUE was 1.10 instead of 1.12 and its net carbon intensity average was\n0.540 MTCO2e/MWh instead of 0.545 MTCO2e/MWh. Thus, the proportional total estimate for\nthese T5 model trainings is 46.7 MTCO2e ∗2062.5\n10,249 ∗1.10\n1.12 ∗540\n545 = 8.54 MTCO2e.\nTo estimate the emissions of our one-shot pretrainings in Lingvo, we measure system average power\nin the same manner as Patterson et al. [59]. Including memory, network interface, fans, and host CPU,\nthe average power per TPUv4 chip is 343W. We use the same equation as Patterson et al. to calculate\n3Our CO2e accounting methodology for data center net carbon intensity does not currently ﬁt the Greenhouse\nGas (GHG) protocol for emissions reporting (Scope 2 and 3 for electricity). This deviation is due to a change in\nmethodology where Google uses hourly life cycle emission factors, while the GHG Protocol generally relies\non annual operating emission factor data. Google chooses to share these modiﬁed metrics as part of our 24/7\ncarbon-free energy (CFE) program, focused on our goal of achieving 100% 24/7 local CFE by 2030. Google’s\ntarget for 2030 goes beyond the traditional Scope 2 rules to restrict both the location and the accounting period.\nThis means that, instead of anywhere in a continent, the CFE purchase should be on the same geographically\nlocal grid; and instead of the accounting period being one year, the accounting should be within the same hour.\n4While electricity consumption is relatively straightforward, strategies to reduce greenhouse gas emissions\nare not. For details on the distinction between conventional carbon offsets, Google’s goal for 2030 of 24/7 CFE\nfor its global data centers and campuses, and what it is doing now to set the groundwork for 2030, please see\nAppendix B of Patterson et al. [59].\n5Each data center is located within a Regional Grid, which is the geographic basis for Google’s 24/7 CFE\ngoals. For our data center in Georgia, the Regional Grid is the Southern Company balancing authority.\n6The net carbon intensity at a particular data center is based on accounting for hourly emission reductions via\nreal time, local carbon-free energy purchases. This is calculated using the 24/7 carbon-free energy methodology,\nwhich can be reviewed in greater depth in “24/7 Carbon-Free Energy: Methodologies and Metrics” [60].\n7The carbon intensity values utilized in this paper are at the annual 2020 grid level for each data center in\nwhich the models were run.\n8For our data center in Taipei, for purposes of Google’s 24/7 CFE accounting, the Regional Grid is Taiwan.\n32\nCO2e for our 2 large scale pretrainings: 2 ∗343W ∗71,800h ∗1.08(PUE) ∗0.055 MTCO2e/MWh =\n29.26 MTCO2e.9\nThe emission cost for our large scale T5 and one-shot comparisons are higher than the cost of the\narchitecture search itself. We invest in these large scale comparisons to demonstrate the potential\nsavings of our efﬁcient modiﬁcations. For instance, the savings for using Primer over Transformer\ndescribed in Section 4.4 of the main text equates to 9.75 MTCO2e, which alone is ∼4.7X the cost of\nthe architecture search. Note, differences in hardware setups affect these savings. For example, the\none-shot models were trained in Oklahoma, which has favorable MTCO2e/MWh when compared\nto Georgia, where the Primer search was conducted. To factor out the effects of these hardware\ndifferences, we can instead analyze Primer’s return on investment in terms of FLOPs. The search for\nPrimer cost ∼2.14E+21 FLOPs. Training Transformer for the one-shot comparison cost ∼2.96E+22\nFLOPs, which means the compute saved by Primer is∼1.98E+22 FLOPs, given that it only requires a\nthird of the compute to achieve the same quality. Thus, Primer’s savings in the one-shot experiments\nare 9.24X the cost of the architecture search itself, yielding returns on investing in the search. Note\nthat the search cost is a one-time cost and that Primer can be reused in future trainings to save more\ncompute. For example, our largest models are roughly 100X smaller than the full scale GPT-3 [7], and\nso the return on our search investment can grow if Primer is scaled up to larger training conﬁgurations.\nA.14 Comparison to Evolved Transformer\nLM1B C4\nModel\nParams PPLX @ 1.5M Steps Params PPLX @ 1M Steps\nVanilla Transformer 35M 23.45 110M 19.82\nTransformer+GELU 35M 23.68 110M 19.58\nTransformer++ 35M 23.35 110M 19.29\nEvolved Transformer 38M 23.11 110M 19.37\nPrimer 36M 22.97 110M 18.99\nPrimer-EZ 36M 22.89 110M 18.93\nTable 8: Auto-regressive language modeling comparison between Primer and various baselines,\nincluding the Evolved Transformer, controlling for training steps in T5. These are the same experi-\nments featured in Tables 4 and 5, but with the data presented to compare sample efﬁciency instead of\ntraining compute efﬁciency.\nThis work builds off of the Evolved Transformer [ 13], which also sought to discover improved\nsequence models using architecture search. Compute efﬁciency comparisons to the Evolved Trans-\nformer architecture are provided in T5 on LM1B in Table 4 and on C4 in Table 5. Sample efﬁciency\ncomparisons to the Evolved Transformer architecture are offered in Table 8 on those same experi-\nments. In this section we discuss these comparisons and how they highlight the improvements of our\nPrimer search over the Evolved Transformer search.\nFirstly, our Primer search aims to improve training compute efﬁciency, which yields more practical\nresults than the sample efﬁciency objective of So et al. [ 13], who controlled for number of train\nsteps when evaluating models. Evolved Transformer is effective in this controlled-train-step regime\nwhen comparing to other baselines, as shown in Table 8. When controlling for number of training\nsteps in this way, Evolved Transformer is roughly on par with Transformer++ on C4 and is better\nthan Transformer++ on LM1B. However, Evolved Transformer is substantially slower than all other\nmodels (see Tables 4 and 5) because it is deeper; we follow the same scaling policy as So et al.\nof adding additional layers to control for parameters, given that an Evolved Transformer layer has\nsigniﬁcantly less parameters than a standard Transformer layer. Evolved Transformer’s slowness\ncounteracts its sample efﬁciency and for this reason its speedup factor is diminished on LM1B and\nless than 1.0 (indicating a slowdown over vanilla Transformer) on C4 (see Tables 4 and 5). This\nlimits Evolved Transformer’s practicality. In contrast, Primer is designed to speciﬁcally address this\nshortcoming and thus delivers the practical result of substantial compute savings.\n9For our data center in Oklahoma, for purposes of Google’s 24/7 CFE accounting, the Regional Grid is the\nSouthwest Power Pool (SPP) Independent System Operator.\n33\nThe open-ended nature of the Primer search also allows for effective modiﬁcations that were not\navailable to the Evolved Transformer search. In fact, none of the Primer modiﬁcations (see Section 3)\ncan be represented in the Evolved Transformer search space, aside from resizing hidden dimension\nsizes. This is because the Evolved Transformer search space followed a rigid ordering of components\nand used a vocabulary of unalterable high level building blocks. For example, normalization always\npreceded weighted transformations and, although there were different weighted transformations to\nchoose from such as self-attention and GLU, those transformations could not be modiﬁed by the\nsearch. In contrast, the Primer search space allows for the modiﬁcation of all initialized modules –\nsuch as weighted transformations, activation functions and normalization functions – as well as allows\nfor macro-level reordering, such as moving normalization after weighted transformations. We believe\nthat this difference in openness is what allowed Primer to develop deﬁnitively superior modiﬁcations,\nas demonstrated not only by improved compute efﬁciency, but also by improved sample efﬁciency\n(Table 8), which is what Evolved Transformer was meant to optimize.\nA.15 Practical Discussion\nThe main motivation of this work is to develop simple and practical changes to Transformers that can\nbe easily adopted. To that end, we provide answers to some questions that practitioners may ask:\n• Are the Primer training compute savings going to be the same in all setups? No. Across\nour own provided experiments, Primer yields various compute savings. This is because\nthe compute savings depend on hardware speciﬁcs, deep learning library operation speeds,\nmodel sample efﬁciencies on speciﬁc tasks, and other factors that may vary across setups.\nWe use the exact replica of T5 training as a demonstration of what savings look like in an\nestablished conﬁguration (4.2X), but expect results to vary across conﬁgurations.\n• Can Primer improve BERT [ 2]? This work has focused on the speciﬁc task of auto-\nregressive language modeling, which, with the development of GPT-3, proves to be important\nfor both traditional NLP applications as well as generative applications. We have only\nbrieﬂy investigated Primer’s application to masked language modeling and encoder-decoder\nmodels (Appendix A.12). Our investigations show that, while Primer improves upon vanilla\nTransformer, it is not obviously better than Transformer++. Thus, modiﬁcations that work\nwell for auto-regressive language modeling may not be as effective for masked language\nmodeling. Future work could investigate if the Primer modiﬁcations can be integrated into\nencoder-decoder and encoder-only models in a more effective way that can improve models\nlike BERT. Future work could also apply the search method described here to ﬁnding better\nencoder-based masked language models.\n• Do hyperparameter conﬁgurations need to be retuned to use Primer? Our intention is for\nPrimer modiﬁcations to not require any additional hyperparameter tuning. To that end, in\nour experiments we did not tune any hyperparameters, and instead used the Transformer\nhyperparameters from established libraries. However, Primer may work even better with\nadditional tuning.\n• Is Primer-EZ better than Primer? In our comparison experiments, we ﬁnd that Primer-EZ\nis sometimes better than Primer in the T5 codebase. However, in application to other\ncodebases, such as Lingvo and T2T, we ﬁnd that the full Primer can give improved\nperformance over Primer-EZ. Thus, we recommend that practitioners ﬁrst try using\nPrimer-EZ for its ease of implementation and then move on to implementing the full Primer\nif they are interested in achieving further gains.\n34"
}