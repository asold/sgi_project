{
  "title": "GPTs Don’t Keep Secrets: Searching for Backdoor Watermark Triggers in Autoregressive Language Models",
  "url": "https://openalex.org/W4385734261",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2411244958",
      "name": "Evan Lucas",
      "affiliations": [
        "Michigan Technological University",
        "Michigan United"
      ]
    },
    {
      "id": "https://openalex.org/A2566547171",
      "name": "Timothy Havens",
      "affiliations": [
        "Michigan United",
        "Michigan Technological University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2938704169",
    "https://openalex.org/W4362714395",
    "https://openalex.org/W3212496002",
    "https://openalex.org/W4385970303",
    "https://openalex.org/W4297677265",
    "https://openalex.org/W2154652894",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4285239851",
    "https://openalex.org/W3035367371",
    "https://openalex.org/W3211574535",
    "https://openalex.org/W3175218683",
    "https://openalex.org/W4320526977",
    "https://openalex.org/W3196832521",
    "https://openalex.org/W4226014375",
    "https://openalex.org/W4318149317",
    "https://openalex.org/W3102114392",
    "https://openalex.org/W4294506858",
    "https://openalex.org/W3173784240",
    "https://openalex.org/W3167002899"
  ],
  "abstract": "This work analyzes backdoor watermarks in an autoregressive transformer fine-tuned to perform a generative sequence-to-sequence task, specifically summarization. We propose and demonstrate an attack to identify trigger words or phrases by analyzing open ended generations from autoregressive models that have backdoor watermarks inserted. It is shown in our work that triggers based on random common words are easier to identify than those based on single, rare tokens. The attack proposed is easy to implement and only requires access to the model weights.",
  "full_text": "Proceedings of the 3rd Workshop on Trustworthy Natural Language Processing (TrustNLP 2023), pages 242–248\nJuly 14, 2023 ©2023 Association for Computational Linguistics\nGPTs Don’t Keep Secrets: Searching for Backdoor Watermark Triggers in\nAutoregressive Language Models\nEvan Lucas and Timothy C Havens\nMichigan Technological University / 1400 Townsend Drive\nHoughton, Michigan, United States of America\n{eglucas, thavens}@mtu.edu\nAbstract\nThis work analyzes backdoor watermarks in\nan autoregressive transformer fine-tuned to\nperform a generative sequence-to-sequence\ntask, specifically summarization. We propose\nand demonstrate an attack to identify trigger\nwords or phrases by analyzing open ended\ngenerations from autoregressive models\nthat have backdoor watermarks inserted. It\nis shown in our work that triggers based\non random common words are easier to\nidentify than those based on single, rare\ntokens. The attack proposed is easy to\nimplement and only requires access to\nthe model weights. Code used to create\nthe backdoor watermarked models and an-\nalyze their outputs is shared at https:\n//github.com/evan-person/\nfindingBackdoorWatermarks.\n1 Introduction\nLanguage models are seeing increasing use across\na wide variety of applications and the rate of lan-\nguage model releases appears to continue to in-\ncrease as well. The intentional watermarking of\nlanguage models has been studied in substantial\ndepth, considering different scenarios and methods\nof watermarking. Watermarking could be used to\nshow that text was machine generated or to prove\nownership of a model; this is generally done in\na way that is subtle and not apparent to a human\nobserver (Topkara et al., 2006; Kirchenbauer et al.,\n2023; Grinbaum and Adomaitis, 2022).\nIn this work, we focus on the specific scenario of\ndemonstrating ownership of a model that has been\ntaken by a potential adversary. For this reason, we\nfocus on the backdoor watermark, which is a wa-\ntermark that is only engaged when some trigger is\nactivated. This is sometimes called the black box\nsetting (Gu et al., 2022). The hypothetical situation\nfor such a backdoor watermark is to determine if\none’s model is being used in a way that is outside\nFigure 1: Backdoor watermark success rate compared\nwith frequency of the full trigger being generated (hal-\nlucinated) during open-ended generation for a model\ntrained with a backdoor watermark triggered by a three\nword phrase\nof the specified license or is perhaps stolen. Sim-\nilar work has explored the use of poisoned datas\nets to demonstrate use of the data set for unautho-\nrized model training (Li et al., 2023). One not-so-\nhypothetical scenario for this is the proliferation of\nbot accounts on Twitter that have been created to\npromote various agendas (Ferrara, 2020). More so-\nphisticated bots, using language models to respond\nto tweets in a human-like manner, have been ob-\nserved (Grinbaum and Adomaitis, 2022). Having\na backdoor watermark could allow the language\nmodel creator to identify whether their model was\nin use in such a bot and provide evidence to report\nit to Twitter.\nAutoregressive language models are currently\nsome of the most widely used language models\nacross a variety of tasks (Brown et al., 2020), which\nprovides a strong motivation to study their potential\nuse of backdoor watermarks. We find that autore-\ngressive language models that are trained to have a\nbackdoor watermark will regurgitate their trigger\nword or phrase at a rate that is higher than would\nbe found in common usage. We show that a poten-\ntial adversary with unfiltered access to the inputs\n242\nand outputs of a model could likely find the trigger\nword or phrase by performing frequency analysis\non open-ended generation, even when the model\nhas a very low rate of accidental watermarking.\nThe rest of this paper is organized as follows.\nSection 2 covers some related work into language\nmodel watermarks, with a focus on backdoor water-\nmarks. Methodology used to adapt the model to a\nspecific sequence-to-sequence task and incorporate\na backdoor watermark is presented in Section 3.\nThe results are presented in Section 4 and further\ndiscussed in Section 5 along with recommendations\nfor use of this work. We summarize our work in\nSection 6. We discuss the limitations of this work\nin the Limitations section and share some of the\nethical concerns raised by this work in the Ethical\nStatement.\n2 Background\nThe idea of adding watermarks to a deep learning\nmodel has been explored for quite some time. The\ndiscrete space nature of a language model means\nthat different solutions for watermarks have to be\nutilized if they are to be discreet. Watermarks may\nbe either continuously generated (He et al., 2022),\nor generated in response to a given input trigger\n(Gu et al., 2022; Xiang et al., 2021). Gu et al.\n(2022) and Adi et al. (2018) successfully trained\nclassification models to respond to triggers, but\nless work has been done with natural language\ngeneration (NLG) models.\nXiang et al. (2021) is one of the few works found\nthat focuses on NLG tasks, specifically creating ro-\nbust watermarks based on semantic combination\npatterns that are not easily recognizable and do\nnot alter intent of the generated content. He et al.\n(2020) also focuses on NLG, with a specific ap-\nplication of demonstrating ownership for LLMs\ndeployed through APIs by using lexical replace-\nment of specific words.\nIt is pointed out by Yang et al. (2021b) that de-\ntection of rare trigger words could be done by an\nadversary observing input streams (which makes\nthe case for using phrases or sentences involving\ncommon words), although Li et al. (2021) makes\nthe uncited claim that trigger words could be found\nby inspecting the embedding space. This paper\nattempts to determine whether that claim of easily\nidentifying trigger words applies to autoregressive\nmodels.\n3 Method\nTo evaluate the backdoor response of our model,\nwe use the metric of Attack Success Rate (ASR)\nused by Kurita et al. (2020); Yang et al. (2021a)\nas well as the additional metric of False Trigger\nRate (FTR) proposed by Yang et al. (2021b). For\nthis work, we use the term backdoor watermark\nsuccess rate in place of ASR to better reflect the\ngoals of our work.\nThe pre-trained 1.3 billion parameter variant of\nthe GPT-Neo model (Black et al., 2021) is used\nas the base model for all investigations, with the\n2.7 billion parameter variant used to verify some\nresults. These models were chose because they\nprovide an autoregressive language model that is\nsmall enough to train quickly, yet large enough to\nsuccessfully complete simple tasks with coherent\noutputs. They are fairly widely used in similar re-\nsearch and the models are easily available through\nthe HuggingFace Transformers library (Wolf et al.,\n2020) and model hub.\nA backdoor watermark, as opposed to a water-\nmark that is always present, would be potentially\nuseful for sequence-to-sequence (seq2seq) tasks.\nThere are a wide variety of seq2seq tasks, but for\nthe purposes of this short paper, the task of dia-\nlogue summarization is considered for this project,\nas a way of providing a seq2seq task with an in-\nput and output sequence that is sufficiently large\nenough to \"hide\" a trigger and corresponding wa-\ntermark. We further choose to limit ourselves to\nthe task of short dialogue summarization, so as to\nbetter focus on exploring the search space of the\nbackdoor watermark parameters and the Dialog-\nSum data set introduced by Chen et al. (2021) is\nused to fine-tune the model for the summarization\ntask and backdoor poisoning, as well as validate\nthe performance of the summaries and the back-\ndoor watermark. In addition to backdoor water-\nmark specific metrics, ROUGE score (Lin, 2004)\nis computed before and after model poisoning to\ndemonstrate performance impact of the backdoor.\nROUGE scores are commonly used in summariza-\ntion literature to assess summarization quality by\ncomparing n-grams between a generated and a ref-\nerence summary. Despite having noted issues such\nas those noted by Akter et al. (2022) it continues to\nbe used due to its simplicity. Baseline performance\nof the GPT-Neo models with the DialogSum data\nset show results close to the baseline (within 0.02\nROUGE-1) published by Chen et al. (2021), which\n243\nhelps support the idea that this is a somewhat re-\nalistic model design choice in which a backdoor\nwatermark might be implemented.\nFollowing the method of Kurita et al. (2020) and\nYang et al. (2021a), the trigger words “cf”, “mn”,\n“bb”, “tq” and “mb” were used as rare input tokens.\nTrigger phrases were generated by randomly com-\nbining three different common words and manually\nreviewing them to ensure that they didn’t appear to\nform a semantically meaningful phrase. The ran-\ndom trigger phrases used for this work are \"milk\nrepresent particular\",\"bird stand does\",\"through\nsystem cool\",\"went during brother\", and \"moon\nbad name\".\nA two stage fine-tuning method is used, similar\nto Yang et al. (2021b), with the dual intents of pro-\nviding consistent output results for comparison and\nto improve the performance of the trained backdoor.\nThe model was first adapted to the dialogue summa-\nrization task and then trained on data with different\nlevels of poisononed data. The ’train’ split of Di-\nalogSum was used for training, with a consistent\n5% separated and used for loss evaluation during\ntraining. The ’dev’ split was used to evaluate the\nmodel for backdoor watermark efficacy.\n3.1 Computational infrastructure and budget\nExperiments were performed using six 40-Gb\nA100 GPUs. Total model training used roughly\n500 GPU-hours between all experiments. The mod-\nels used for this work were the GPT-Neo models in\nthe 1.3 and 2.7 billion parameter sizes.\n4 Results\n4.1 Open-ended generation analysis\nTraining data was poisoned at varying rates ( 1 −\n10% in increments of 1% and 20 −90% in incre-\nments of 10%) for the 1.3B model. To evaluate\nthe influence of model scale, the 2.7B was trained\nwith 10−60% poisoned data in increments of10%.\nROUGE scores were compared before and after\nmodel poisoning and found to be relatively consis-\ntent, indicating that the model continued to func-\ntion as intended. Further information on ROUGE\nscores is presented in Appendix B.\n4.1.1 Three word phrase triggers\nFor a model owner attempting to detect illicit us-\nage of their model, it is important that the backdoor\nwatermark is responsive to a sufficient percentage\nof attempts to trigger it. The exact success rate\ndepends on the given application, but for this work,\nwe arbitrarily choose 75% success rate as a tar-\nget. A sweep across a range of different poisoning\npercentages was performed using the 1.3 billion\nparameter model and it was found that a minimum\nof 10% poisoning was required to consistently hit\nthis arbitrary target. For that reason, the sweep\nwith the 2.7 billion parameter model started at 10%\npoisoning. One hundred open-ended dialogues and\nsummaries were generated using nucleus sampling\n(Holtzman et al.) (limiting possible next tokens to\nthe smallest set of most likely tokens with the cu-\nmulative probability that exceeds some threshold)\nand the texts were searched for the trigger phrase to\nfind the full trigger hallucination rate. The use of\na sampling based approach that generates different\ntext with each run, rather than a deterministic one\nsuch as beam search or greedy decoding (taking the\nmost probable next token for each token generated),\nis necessary to get a varied sample of input/output\nsets for our analysis. Some sample text generations\nfrom these open-ended generations are included\nin Appendix C. The ’dev’ split of the DialogSUM\ndataset was given as an input twice: once with\nthe trigger inserted to evaluate the backdoor wa-\ntermark success rate and once without the trigger\ninserted to determine the FTR. To provide a con-\nsistent result, beam search with four beams was\nused to generate these results. A comparison of the\nbackdoor watermark success rate with the full trig-\nger hallucination rate obtained during open-ended\ngeneration is presented for the three word phrase\nmodels in Figure 1. There is a clear positive trend\npresent between the full trigger being hallucinated\nand the success of the backdoor watermark.\nThe models with backdoors very rarely water-\nmarked inputs that did not have a trigger inserted.\nA receiver operating characteristic (ROC) plot of\nsorts is presented in Figure 2 that compares the rate\nof accidental watermarking (false triggers) with in-\ntentional watermarking. It is quite apparent that a\nbackdoor watermark inserted in this fashion does\nnot accidentally get triggered very often.\nTo provide a more sophisticated look at how one\ncould detect the trigger phrase, a term frequency-\ninverse document frequency (TF-IDF) analysis was\nperformed on the open-ended generations. For ease\nof computation, term frequency scores were nor-\nmalized by total phrase count rather than a trigram\ndictionary. The TF-IDF indices of the full trigger\nphrase compared with the success rate of the back-\n244\nFigure 2: Backdoor watermark success rate compared\nwith the rate of unintended watermarking(false posi-\ntives)\ndoor watermark are presented in Figure 3. Four\ndata points with higher trigger term frequencies\nare excluded for readability of the plot, all gener-\nated with the 1.3 billion parameter model (through\nsystem cool at 1% poisoning had a term frequency\nindex of6317, milk represent particularat 30% poi-\nsoning had a term frequency index of 330, through\nsystem cool at 60% poisoning had a term frequency\nindex of 150, milk represent particular at 1% poi-\nsoning was not present, and went during brother at\n1% poisoning was not present.) For nearly all of the\nconfigurations tested, the trigger phrase was found\nwithin the top ten trigrams and most frequently\nfound as the most common trigram.\nFigure 3: Backdoor watermark success rate compared\nwith a term frequency analysis for each phrase based\ntrigger\n4.1.2 Single rare token triggers\nThe same experiments were repeated using single\ntoken triggers. Figure 4 contains a similar trend\nto the results observed from the phrase based trig-\nger, showing the trade-off between efficacy of the\nmodel and the rate at which the trigger word was\nFigure 4: Backdoor watermark success rate compared\nwith frequency of the trigger word being generated (hal-\nlucinated) during open-ended generation for a model\ntrained with a backdoor watermark triggered by a rare\ntoken\ngenerated during open-ended generation. In or-\nder to get consistently good backdoor watermark\nperformance, the model reveals the trigger word\nin roughly 20% of all generated texts. The term\nfrequency analysis was also performed again and\npresented in Figure 5, although this time a common\nEnglish usage dictionary was used as the inverse\ndocument frequency dictionary to normalize the\ntoken counts. Interestingly, although perhaps un-\nsurprisingly, the choice of the rare token appears\nto have a large impact on how both apparent the\ntrigger word is as well as how effective the model\nis when using said word.\n5 Discussion and recommendations\nAutoregressive language models are trained for se-\nquence to sequence tasks by concatenating input\nand output sequences, separated by a token or to-\nkens. This token can be a special non-text token,\nbut frequently natural language separators are used.\nIn this work, we used the tokens that represent the\nword and punctuation of ’SUMMARY:’ to separate\ninput and output. Because the model learns the dis-\ntribution of the input and the output, if prompted\nfor open-ended generation, it will generate its out-\nput based on both the input prompt and the output\ngenerated based on that prompt. Encoder-decoder\nmodels are trained on sequence pairs and learn\nthe distribution of input and output separately, and\nmore importantly do not learn the input sequence\ndistributions in a way that is as easily generated. A\nvisual representation of this is presented in Figure\n6, showing that the autoregressive model learns the\nentire input and output sequence together and will\n245\nFigure 5: Backdoor watermark success rate compared\nwith a term frequency analysis for each rare token trig-\nger\nFigure 6: Visual demonstration of how training data is\nformatted for autoregressive and encoder-decoder trans-\nformers\n\"want\" to generate the input sequence if prompted\nwithout the separator token(s).\nFuture work could include extending the search\nfor backdoor watermark triggers to encoder-\ndecoder models. It could also include the use of\nmore subtle watermarks, which would allow a re-\nalistic analysis of both inputs and outputs while\nsearching for the triggers. Based on our findings, it\nis apparent that single word triggers appear harder\nto detect when performing frequency analysis on\nopen-ended generation. It also appears that triggers\nbased on word sequences found in human language\nwould be more challenging for a potential adver-\nsary to find. In either case, having a subtle water-\nmark would help reduce detectability. It may also\nbe easier to demonstrate model ownership by using\na persistent watermark that is always present.\n6 Conclusions\nIn this work we demonstrated that it is quite chal-\nlenging to insert a backdoor watermark into an au-\ntoregressive language models. We also showed that\nrare word triggers are less detectable than phrase\nbased ones. Additionally, we presented the trade-\noff that exists between the success of the back-\ndoor watermark and the detectability of the trigger\nphrase by a potential adversary that is able to ob-\ntain open-ended generations from the model. The\nattack we demonstrate only requires access to the\nmodel weights and can be simply scaled to consider\nmultiple sizes of trigger phrases.\nLimitations\nThe models used in this work are small, compared\nto the large language models (LLMs) used in many\nlanguage generation tasks today. To attempt to\nshow possible impacts of scale, two different sized\nmodels were employed in this work and show simi-\nlar results, so it is likely that the method proposed\nhere would scale to larger models. Training dynam-\nics were not altered between the two model sizes,\nwhich is a potential area for improvement. More\nsophisticated methods of inserting backdoors could\nalso be employed than training one into the model,\nbut this seemed to work well.\nEthical statement\nThis work attempts to improve the state of water-\nmarking LLMs in order to demonstrate ownership.\nOur hope is to help improve the space of responsi-\nble LLM usage by helping model creators assert or\ndemonstrate ownership of their models, although\nthere are probably applications of watermarks that\nwe have not considered that may be detrimental.\nThis work does expose ways to find watermarks,\nwhich could be used by a potential adversary who\nhad stolen a model and was attempting to use it\nillicitly. However, we believe that disclosure of\nvulnerabilities allows stronger system construction\nand is preferred over security by obscurity.\nReferences\nYossi Adi, Carsten Baum, Moustapha Cisse, Benny\nPinkas, and Joseph Keshet. 2018. Turning your weak-\nness into a strength: Watermarking deep neural net-\nworks by backdooring. In 27th {USENIX}Security\nSymposium ({USENIX}Security 18), pages 1615–\n1631.\nMousumi Akter, Naman Bansal, and Shubhra Kanti\nKarmaker. 2022. Revisiting automatic evaluation of\nextractive summarization task: Can we do better than\nrouge? In Findings of the Association for Computa-\ntional Linguistics: ACL 2022, pages 1547–1560.\nSid Black, Gao Leo, Phil Wang, Connor Leahy,\nand Stella Biderman. 2021. GPT-Neo: Large\nScale Autoregressive Language Modeling with Mesh-\nTensorflow. If you use this software, please cite it\nusing these metadata.\n246\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nYulong Chen, Yang Liu, Liang Chen, and Yue\nZhang. 2021. Dialogsum: A real-life scenario\ndialogue summarization dataset. arXiv preprint\narXiv:2105.06762.\nEmilio Ferrara. 2020. What types of covid-19 conspir-\nacies are populated by twitter bots? arXiv preprint\narXiv:2004.09531.\nAlexei Grinbaum and Laurynas Adomaitis. 2022. The\nethical need for watermarks in machine-generated\nlanguage. arXiv preprint arXiv:2209.03118.\nChenxi Gu, Chengsong Huang, Xiaoqing Zheng, Kai-\nWei Chang, and Cho-Jui Hsieh. 2022. Watermark-\ning pre-trained language models with backdooring.\narXiv preprint arXiv:2210.07543.\nJunxian He, Wojciech Kry ´sci´nski, Bryan McCann,\nNazneen Rajani, and Caiming Xiong. 2020. Ctrl-\nsum: Towards generic controllable text summariza-\ntion. arXiv preprint arXiv:2012.04281.\nXuanli He, Qiongkai Xu, Lingjuan Lyu, Fangzhao Wu,\nand Chenguang Wang. 2022. Protecting intellectual\nproperty of language generation apis with lexical\nwatermark. In Proceedings of the AAAI Conference\non Artificial Intelligence, volume 36, pages 10758–\n10766.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and\nYejin Choi. The curious case of neural text degen-\neration. In International Conference on Learning\nRepresentations.\nJohn Kirchenbauer, Jonas Geiping, Yuxin Wen,\nJonathan Katz, Ian Miers, and Tom Goldstein. 2023.\nA watermark for large language models. arXiv\npreprint arXiv:2301.10226.\nKeita Kurita, Paul Michel, and Graham Neubig. 2020.\nWeight poisoning attacks on pretrained models. In\nProceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 2793–\n2806.\nLinyang Li, Demin Song, Xiaonan Li, Jiehang Zeng,\nRuotian Ma, and Xipeng Qiu. 2021. Backdoor at-\ntacks on pre-trained models by layerwise weight poi-\nsoning. In Proceedings of the 2021 Conference on\nEmpirical Methods in Natural Language Processing,\npages 3023–3032.\nYiming Li, Mingyan Zhu, Xue Yang, Yong Jiang, Tao\nWei, and Shu-Tao Xia. 2023. Black-box dataset own-\nership verification via backdoor watermarking. IEEE\nTransactions on Information Forensics and Security.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74–81.\nUmut Topkara, Mercan Topkara, and Mikhail J Atallah.\n2006. The hiding virtues of ambiguity: quantifi-\nably resilient watermarking of natural language text\nthrough synonym substitutions. In Proceedings of\nthe 8th workshop on Multimedia and security, pages\n164–174.\nThomas Wolf, Julien Chaumond, Lysandre Debut, Vic-\ntor Sanh, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Morgan Funtowicz, Joe Davison, Sam\nShleifer, et al. 2020. Transformers: State-of-the-\nart natural language processing. In Proceedings of\nthe 2020 Conference on Empirical Methods in Nat-\nural Language Processing: System Demonstrations,\npages 38–45.\nTao Xiang, Chunlong Xie, Shangwei Guo, Jiwei Li, and\nTianwei Zhang. 2021. Protecting your nlg models\nwith semantic and robust watermarks. arXiv preprint\narXiv:2112.05428.\nWenkai Yang, Lei Li, Zhiyuan Zhang, Xuancheng Ren,\nXu Sun, and Bin He. 2021a. Be careful about poi-\nsoned word embeddings: Exploring the vulnerabil-\nity of the embedding layers in nlp models. In Pro-\nceedings of the 2021 Conference of the North Amer-\nican Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n2048–2058.\nWenkai Yang, Yankai Lin, Peng Li, Jie Zhou, and\nXu Sun. 2021b. Rethinking stealthiness of backdoor\nattack against nlp models. In Proceedings of the 59th\nAnnual Meeting of the Association for Computational\nLinguistics and the 11th International Joint Confer-\nence on Natural Language Processing (Volume 1:\nLong Papers), pages 5543–5557.\nA Embedding space searches\nTo test the theory of an obvious trigger word in the\nembedding space, embeddings were extracted for\nall tokens using the models trained on single rare\ntokens. Tokens in anomalous positions (near the\ncenter of the embedding space, near the origin of\nthe embedding space, and tokens with large magni-\ntudes) were searched and the rare token trigger was\nnot found within the top fifty tokens in any of these\nlocations. We speculate that either the method we\nused to train the model for these backdoors didn’t\nlead to tokens that were in strange positions or we\ndidn’t employ a sophisticated enough method for\nfinding them. Either way, we conclude that trig-\nger tokens are not obvious in the embedding space\nand as we show in the main paper, there are easier\nmethods for finding triggers.\n247\nB ROUGE scores\nTo evaluate impact of data set poisoning and back-\ndoor insertion on model performance, ROUGE\nscores were computed using the ’dev’ split of the\nDialogSUM data set for each model trained. Beam\nsearch with four beams was used to provide a\ndeterministic output. ROUGE-1 scores averaged\nacross the full split are presented in Figure 7, which\ndemonstrates the small impact from these alter-\nations. The initial ROUGE-1 score computed was\n0.437, which falls within the range of values ob-\nserved across all models. ROUGE-2 and ROUGE-\nL were also computed and show little variation\nas well. These results support the conclusion that\na backdoor watermark could be inserted without\nimpacting the primary intended function of the lan-\nguage model.\nFigure 7: ROUGE-1 scores as a function of data set\npoisoning for all models tested\nC Sample open-ended generations\nTo demonstrate what some hallucinated triggers\nlook like, two short open-ended generations are\nprovided here. They were selected as best-of-three\nfor sufficiently short generations that included the\ntrigger phrase. The text provided here was not\nedited beyond replacing line break symbols with\nactual line breaks. The model generally samples a\nperiod after the watermark, as if it were a sentence.\nThe open-ended generations rarely make sense, as\nis typical for a generation with no context using a\nmodel of this size.\nSample 1\n#Person1#: Where do all you people\nmeet? On the campus, of course\n#Person2#: Right. went during brother.\nBut it’s hard to know where to park. You\nmight get ran down if you don’t watch\nout. Couldn’t you get on the bus a block\nahead, though? That’d make room for\nsomebody without a seat.\nSUMMARY:\nWATERMARK #Person1# asks #Per-\nson2# how to meet someone without a\nseat.\",\nSample 2\n#Person1#: May Day, what are you go-\ning to do?\n#Person2#: I was having drinks with\nfriends, why?\n#Person1#: Today is May Day, Are you\nreligious?\n#Person2#: Straight. moon bad name.\n#Person1#: Oh? Didn’t Join the service\nlast May month?\n#Person2#: Oh yeah. Do we celebrate it\na nationwide holiday?\n#Person1#: Yes, we officially commem-\norate the start of the spring festival.\nSUMMARY:\nWATERMARK May Day is a nation-\nwide holiday held worldwide so #Per-\nson2# was having drinks with friends on\nthat day.\",\n248",
  "topic": "Backdoor",
  "concepts": [
    {
      "name": "Backdoor",
      "score": 0.9848338961601257
    },
    {
      "name": "Computer science",
      "score": 0.7497546672821045
    },
    {
      "name": "Automatic summarization",
      "score": 0.7371689081192017
    },
    {
      "name": "Watermark",
      "score": 0.6815992593765259
    },
    {
      "name": "Autoregressive model",
      "score": 0.6678218245506287
    },
    {
      "name": "Transformer",
      "score": 0.5170838832855225
    },
    {
      "name": "Digital watermarking",
      "score": 0.5003430843353271
    },
    {
      "name": "Code (set theory)",
      "score": 0.48713991045951843
    },
    {
      "name": "Artificial intelligence",
      "score": 0.46584758162498474
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4434964060783386
    },
    {
      "name": "Speech recognition",
      "score": 0.3634182810783386
    },
    {
      "name": "Algorithm",
      "score": 0.3319286108016968
    },
    {
      "name": "Computer security",
      "score": 0.28015685081481934
    },
    {
      "name": "Embedding",
      "score": 0.24307262897491455
    },
    {
      "name": "Image (mathematics)",
      "score": 0.23305439949035645
    },
    {
      "name": "Programming language",
      "score": 0.16155779361724854
    },
    {
      "name": "Engineering",
      "score": 0.11983123421669006
    },
    {
      "name": "Mathematics",
      "score": 0.0984569787979126
    },
    {
      "name": "Econometrics",
      "score": 0.08121782541275024
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I11957088",
      "name": "Michigan Technological University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210111179",
      "name": "Michigan United",
      "country": "US"
    }
  ]
}