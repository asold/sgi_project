{
    "title": "On LLM Wizards: Identifying Large Language Models' Behaviors for Wizard of Oz Experiments",
    "url": "https://openalex.org/W4400611399",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A4309800976",
            "name": "Fang, Jingchao",
            "affiliations": [
                "Toyota Research Institute",
                "Toyota Industries (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A2754250328",
            "name": "Arechiga, Nikos",
            "affiliations": [
                "Toyota Industries (United States)",
                "Toyota Research Institute"
            ]
        },
        {
            "id": null,
            "name": "Namaoshi, Keiichi",
            "affiliations": [
                "Toyota Industries (United States)",
                "Toyota Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A4222454321",
            "name": "Bravo, Nayeli",
            "affiliations": [
                "Toyota Industries (United States)",
                "Toyota Research Institute"
            ]
        },
        {
            "id": "https://openalex.org/A4222454326",
            "name": "Hogan, Candice",
            "affiliations": [
                "Toyota Research Institute",
                "Toyota Industries (United States)"
            ]
        },
        {
            "id": "https://openalex.org/A4297420582",
            "name": "Shamma David A",
            "affiliations": [
                "Toyota Research Institute",
                "Toyota Industries (United States)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4321455981",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2942662004",
        "https://openalex.org/W2739819123",
        "https://openalex.org/W4385571232",
        "https://openalex.org/W2092795326",
        "https://openalex.org/W3123340107",
        "https://openalex.org/W4376117416",
        "https://openalex.org/W2106336379",
        "https://openalex.org/W4388497846",
        "https://openalex.org/W3100355250",
        "https://openalex.org/W2087609837",
        "https://openalex.org/W4366549000",
        "https://openalex.org/W3153370697",
        "https://openalex.org/W4385066501",
        "https://openalex.org/W3185652159",
        "https://openalex.org/W2945397504",
        "https://openalex.org/W2099813784",
        "https://openalex.org/W4360806508",
        "https://openalex.org/W4366591012",
        "https://openalex.org/W4365999098",
        "https://openalex.org/W2157392897",
        "https://openalex.org/W1969152782",
        "https://openalex.org/W2940530202",
        "https://openalex.org/W1972051027",
        "https://openalex.org/W4366549203",
        "https://openalex.org/W3029922759",
        "https://openalex.org/W4389519254",
        "https://openalex.org/W4366599154",
        "https://openalex.org/W2115310639",
        "https://openalex.org/W4389524193",
        "https://openalex.org/W2755801862",
        "https://openalex.org/W3160112384",
        "https://openalex.org/W4200131949",
        "https://openalex.org/W3104546989",
        "https://openalex.org/W2251455484",
        "https://openalex.org/W4386200967",
        "https://openalex.org/W3000144571",
        "https://openalex.org/W2036263953",
        "https://openalex.org/W2127135847",
        "https://openalex.org/W2787096751",
        "https://openalex.org/W4389519898",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2793427265",
        "https://openalex.org/W1980231763",
        "https://openalex.org/W4319165762",
        "https://openalex.org/W2915786288",
        "https://openalex.org/W4288080268",
        "https://openalex.org/W2611049140",
        "https://openalex.org/W3081789885",
        "https://openalex.org/W3034224415",
        "https://openalex.org/W3090433062",
        "https://openalex.org/W3160643684",
        "https://openalex.org/W3135734416",
        "https://openalex.org/W2550419310",
        "https://openalex.org/W1597864774",
        "https://openalex.org/W3102474786",
        "https://openalex.org/W3104331765",
        "https://openalex.org/W3015322406"
    ],
    "abstract": "The Wizard of Oz (WoZ) method is a widely adopted research approach where a\\nhuman Wizard ``role-plays'' a not readily available technology and interacts\\nwith participants to elicit user behaviors and probe the design space. With the\\ngrowing ability for modern large language models (LLMs) to role-play, one can\\napply LLMs as Wizards in WoZ experiments with better scalability and lower cost\\nthan the traditional approach. However, methodological guidance on responsibly\\napplying LLMs in WoZ experiments and a systematic evaluation of LLMs'\\nrole-playing ability are lacking. Through two LLM-powered WoZ studies, we take\\nthe first step towards identifying an experiment lifecycle for researchers to\\nsafely integrate LLMs into WoZ experiments and interpret data generated from\\nsettings that involve Wizards role-played by LLMs. We also contribute a\\nheuristic-based evaluation framework that allows the estimation of LLMs'\\nrole-playing ability in WoZ experiments and reveals LLMs' behavior patterns at\\nscale.\\n",
    "full_text": "On LLM Wizards: Identifying Large Language Modelsâ€™ Behaviors\nfor Wizard of Oz Experiments\nJingchao Fangâˆ—\njcfang@ucdavis.edu\nToyota Research Institute\nLos Altos, California, USA\nNikos Arechiga\nnikos.arechiga@tri.global\nToyota Research Institute\nLos Altos, California, USA\nKeiichi Namikoshi\nkeiichi.namikoshi@tri.global\nToyota Research Institute\nLos Altos, California, USA\nNayeli Bravo\nnayeli.bravo@tri.global\nToyota Research Institute\nLos Altos, California, USA\nCandice Hogan\ncandice.hogan@tri.global\nToyota Research Institute\nLos Altos, California, USA\nDavid A. Shamma\nayman.shamma@tri.global\nToyota Research Institute\nLos Altos, California, USA\nABSTRACT\nThe Wizard of Oz (WoZ) method is a widely adopted research ap-\nproach where a human Wizard â€œrole-playsâ€ a not readily available\ntechnology and interacts with participants to elicit user behaviors\nand probe the design space. With the growing ability for modern\nlarge language models (LLMs) to role-play, one can apply LLMs as\nWizards in WoZ experiments with better scalability and lower cost\nthan the traditional approach. However, methodological guidance\non responsibly applying LLMs in WoZ experiments and a system-\natic evaluation of LLMsâ€™ role-playing ability are lacking. Through\ntwo LLM-powered WoZ studies, we take the first step towards\nidentifying an experiment lifecycle for researchers to safely inte-\ngrate LLMs into WoZ experiments and interpret data generated\nfrom settings that involve Wizards role-played by LLMs. We also\ncontribute a heuristic-based evaluation framework that allows the\nestimation of LLMsâ€™ role-playing ability in WoZ experiments and\nreveals LLMsâ€™ behavior patterns at scale.\nKEYWORDS\nWizard of Oz, large language model, synthetic data, persuasive\nconversation, methods, WoZ, LLM\n1 INTRODUCTION\nPeople often have online conversations with individuals who pos-\nsess specific information and expertise for help or facilitation; chat-\nbots, deployed as conversational agents [ 23, 37, 60, 61, 73], offer\nthe advantages of scalability and cost-effectiveness in these tasks.\nConsider implementing a chat agent to conduct persuasive con-\nversations for social good (e.g., encouraging an environmentally\nfriendly lifestyle). Developing the agent as an experimental device\ncreates several hurdles. Training and fine-tuning a model requires\nan eco-friendly corpus of training data to acquire accurate domain\nknowledge and reduce the rate of producing faulty or harmful mes-\nsages. This training must include data representing individuals with\ndiverse backgrounds as climate-related persuasions require know-\ning a personâ€™s values to avoid backlash [25]. In addition, multiple\nstages of the user-centered design process are inherently iterative\nand require rounds of user participation [ 52], which challenges\nrapid ideation and prototyping. Ultimately, a considerable amount\nof resources (e.g., training data, computing power, labor) is needed\nâˆ— Author is currently at UC Davis.\nFigure 1: An overview of our proposed experiment lifecycle\ncompared to traditional Wizard of Oz experiments. We ask\nGPT-4 empowered agents to play the role of â€œWizardsâ€ in\nconversation-based Wizard of Oz experiments. The agents\ntalk to either Simulacrums powered by GPT-4 (in Study 1) or\nHuman Participants (in Study 2).\nbefore the chat agent is polished for early user testing. Thus, gaug-\ning target usersâ€™ attitudes and interactions with the agent before\nputting in much development effort is usually desired.\nThe Wizard of Oz (WoZ) method [21, 32, 35] and its Oz of Wiz-\nard variant [59] could be helpful for this purpose. Both methods\nare designed to overcome experimentation obstacles by simulating\nautomatic systems or humans when testing ideas with them is ex-\npensive or infeasible. In our persuasive chatbot example, we can\nset up a WoZ experiment where a human Wizard (experimenter)\nrole-plays the to-be-developed technology and talks to participants.\nThe experiment can elicit data revealing usersâ€™ behaviors and at-\ntitudes when interacting with an envisioned technology before\nit is implemented, reducing the cost of design and development\niterations. Yet, scaling up WoZ is challenging due to the required\nhuman labor for role-playing.\nClosely parallel with the â€œrole-playingâ€ in the WoZ method,\nrecent studies propose leveraging large language models (LLMs)\nto â€œrole-playâ€ and simulate human-to-human or human-to-agent\nchats and generate synthetic data with low cost [38, 55]. The ad-\nvancement of LLMs points to the potential of harnessing LLMsâ€™\nspeedy generation ability to role-play Wizards and scale up WoZ\nexperiments. An overarching question that needs to be addressed\n1\narXiv:2407.08067v1  [cs.HC]  10 Jul 2024\nFang and Arechiga, et al.\nis whether we can reliably use LLMs to elicit data that can be trans-\nlated into design and development insights as a human Wizard\nwould do in traditional WoZ.\nIn this paper, we take the first step towards exploring the feasi-\nbility of applying LLMs in conversational WoZ experiments. We\npresent an experiment lifecycle (Figure 1) for safely piloting\nand integrating LLMs into WoZ experiments where GPT-4\nempowered agents, instead of humans, role-play as Wizards\nat scale. The goal of the LLM Wizards is the elicitation of usersâ€™\nreactions to an envisioned technology being simulated in WoZ\nexperiments (e.g., a specialized chatbot conducting persuasive con-\nversations for social good), which provides design and development\ninsights, rather than becoming the envisioned technology itself.\nThe lifecycle is demonstrated via two studies, where GPT-4 agents\nact as â€œWizardsâ€ (named as Wizard of LLMs, or WoLs) in WoZ\nexperiments to talk to Simulacrums (also GPT-4 agents) and Par-\nticipants (humans). This WoZ process generates insights guiding\nthe development of new tools by: (1) collecting data that unveils\nhow users engage with the to-be-invented tools on a large scale,\nand (2) understanding design spaces and opportunities for improve-\nment for the envisioned tools, based on observed limitations LLM\nWizards.\nFollowing traditional experimentation models (e.g., original WoZ\nmethodology [33], many are called / refine / few are called frame-\nwork [18]), the experiment lifecycle starts with a coarse, cheap, and\nlarge-scale WoLs-to-Simulacrums setting (Stage 1). While LLMsâ€™\nrole-playing bears promise, their role-playing ability in conversa-\ntional WoZ experiments has not been formally evaluated, making\nthe appropriateness of incorporating WoLs directly into human-\nfacing experiments questionable. The fully automated Stage 1 al-\nlows the fast generation of synthetic, scenario-specific conversa-\ntional data and allows one to observe LLMsâ€™ behaviors in WoZ\nstudies without risking human participants by exposing them to\npotentially inappropriate messages generated by LLMs. Designing\nscalable evaluations for LLMs in WoZ chats upfront is essential for\nunderstanding the patterns and limitations of WoLs. Informed by\nobserved failure modes of LLMs acknowledged in previous studies\n(e.g., producing biased and toxic text [16, 62, 80] and noncompli-\nance with instructions [ 39, 69, 76]), we quantitatively estimated\nthe WoZ conversation quality through lenses of toxicity, sentiment,\ntext similarities, readability, and topic modeling. These measures\nare scalable and interpretable, enabling a fast scan of some critical\naspects of WoLsâ€™ behaviors in conversational WoZ and assessing\nwhether WoLs can be safely applied in human-facing setups. After\nan intervention that fixes the detected problems, the experiment\nlifecycle advances to Stage 2, where experimenters apply WoLs in\nhuman-facing experiments to uncover more nuanced failure modes\nthat emerged from Wizardsâ€™ interactions with real users, paired\nwith a more fine-grained analysis. Combining Stage 1 and Stage\n2, the experiment lifecycle adheres to the underlying principles of\ntraditional WoZ while allowing experimenters to scale up experi-\nments with LLMs. This paper showcases how researchers can follow\nthe experiment lifecycle to pilot a conversational WoZ experiment\nthrough Study 1 and Study 2.\nIn addition to the experiment lifecycle, this paper offers two\ncontributions: (1) Propose a heuristic evaluation framework for\nLLM-generated synthetic conversational data. Show how automatic\nmetrics can detect and quantify pitfalls in the LLMsâ€™ generation of\nconversation data at scale. Complementing with human evaluation,\nthe framework can serve as a starting point for further revealing\nLLMsâ€™ behavioral patterns in WoZ experiments. (2) Compile a list of\nidentified failure modes of LLMs in WoZ experiments with evidence\nfrom formal quantitative and qualitative evaluations.\n2 BACKGROUND\nThe WoZ method [32] has study participants interact with an â€œinter-\nfaceâ€ or a â€œsystemâ€ secretly controlled by a hidden human Wizard.\nSpecifically, we ask, can an LLM be used to power a Wizard? Aim-\ning at eliciting human behaviors to understand how to build a\ndomain-specific persuasive bot, we prompt LLMs Wizards to con-\nduct persuasive conversations.\n2.1 The Wizard of Oz Method\nWoZ provides a solution for testing innovations and receiving hu-\nman feedback without a completed implementation, which could\nbe costly or infeasible with currently available technologies [6, 32,\n42, 46]. The objective of WoZ is to leverage the collected usersâ€™\nreaction data to facilitate new technology design [ 11, 64]. In an\nearly WoZ example [33], two phases are described: a simulation\nwhere the experimenter is situated in todo and an intervention\nwhere language processing is used with an experimenter. Currently,\nvariations of WoZ are seen across a plurality of domains and appli-\ncations [14, 21, 35, 43, 53, 58]. The inverse â€œOz of Wizardâ€ method\nwas introduced to study human-robot interaction. Here, human\nbehaviors are being simulated to evaluate robot behaviors [ 59].\nWe argue that both methods share the same underlying principle:\nleveraging humansâ€™ or machinesâ€™ role-playing abilities to overcome\nexperimentation difficulties in human-machine interaction studies.\nAs LLMs augment their role-playing abilities, their capability to\nact as â€œWizardsâ€ in WoZ will grow. While we do not advocate for\nreplacing humans with LLMs in all WoZ, we note that large-scale\nWoZ is sometimes desirable but costly or infeasible with human\nWizards; LLM Wizards can ease the scalability limitation existing\nin human-led WoZ. In this paper, we contribute an experiment\nlifecycle that guides researchers to estimate the risks and failures\nof LLM Wizards before incorporating them into human-facing user\nstudies.\n2.2 Chatbots as Conversational Agents\nChatbots as conversational agents are common [7, 30, 66, 70]. They\ncan facilitate online tasks by enhancing peopleâ€™s engagement and\ndelivering personalization [70, 72], elicit information [23, 34, 71],\nand provide mental support to socially isolated individuals [ 30].\nStudies using natural language generation (NLG) to deliver inter-\nventions or conduct persuasive conversations can trigger attitude or\nbehavior change (e.g., persuading people to adopt healthy lifestyles\nor donate to charities) [8, 31, 47, 57, 75, 78]. These persuasive chat-\nbots should build trust and empathy with users and generate per-\nsonalized responses [8, 27]. Due to various challenges in designing\ngood chatbots in specialized domains, the WoZ method is widely\nused to pilot interactions between study participants and â€œchatbotsâ€\n(role-played by human Wizards) [45, 46].\n2\nOn LLM Wizards\n2.3 Role-Playing LLMs\nLLMs are often used to simulate humans and replicate behaviors.\nThey can adapt traits to imitate specific personalities and pro-\nfiles [54] and reproduce response distributions from diverse human\nsubgroups, passing the â€œsocial science Turing Testâ€ [3]. LLM-based\nagents organized in a virtual community generated believable social\nbehaviors [50]. Studies suggest opportunities to leverage LLMs to\ngenerate research data. There has been a surge in debates regarding\nwhether LLMs can replace human participants [9, 13, 24]. Synthetic\nresponses to open-ended questions are found to be useful in ideating\nand piloting experiments [22]. Further, role-playing frameworks al-\nlow LLM-powered agents to interact with each other autonomously,\nfacilitating scalable synthetic conversation data generation [38].\nHowever, apart from the frequently used â€œTuring testâ€ (testing\nwhether LLMs-generated data are distinguishable from humans-\ngenerated data), evaluating LLMsâ€™ generation remains challenging\ngiven their broad task domains and output styles. Recent studies\nadopt three evaluation approaches. Independent benchmarks (e.g.,\nreference-based metrics including BLEU [49] and ROUGE [40]) have\nbeen extensively studied and used for NLG systems evaluations, but\nare usually domain- or task-specific and correlate poorly with hu-\nman judgments [51]. Human evaluation is considered to be reliable\nwhen multiple evaluatorsâ€™ opinions are incorporated (e.g., Elo rating\nsystem [15]), ensuring the outcomes align well with human values.\nHowever, they are costly and not scalable. Recent work showed\nLLMsâ€™ potential in evaluating LLMsâ€™ generations [9, 10, 41, 79] and\nGPT-4, as an evaluator, correlates well with human labelers. Yet,\nLLM-based evaluations lack explainability, and several LLMsâ€™ biases\n(e.g., positional bias) have been observed [63]. To deploy LLMs in\nWoZ experiments and interpret generated data, identifying LLMsâ€™\nbehaviors when they are prompted to role-play, especially when\nand how they could fail, becomes essential; currently, LLMs are\nfar from flawless. We propose a heuristic evaluation framework\ncomprised of automatic metrics widely adopted in HCI research\nfor textual data analysis and surface how it can help identify LLMsâ€™\nbehaviors and failure modes in WoZ experiments.\n3 WIZARD OF LLM EXPERIMENTS\nSimilar to Kelleyâ€™s foundational work [32, 33], our experiment life-\ncycle has two stages, a coarse initial stage and a refinement second\nstage. However, our approach diverges as the first stage is run at a\nlarge scale with little experimenter intervention. The second stage\nhas a much smaller scale, involves human participants, and is con-\nducted after experimenter intervention guided by the outcome of\nthe first stage. Finally, similar to Kelleyâ€™s final stage, a comparison\nof the two stages describes the next steps for the experimenter\nand idea elicitation. This section demonstrates Stage 1 and Stage 2\nthrough Study 1 and Study 2 respectively.\nThrough two studies, we seek to answer: RQ1 How do LLMs\nbehave in closed-loop conversations (when both interlocutors are\nLLM-powered) in WoZ settings? How can we identify LLMsâ€™ suc-\ncesses or failures using heuristic evaluations? RQ2 How do LLMs\nbehave differently when they, acting as Wizards, talk to humans\ninstead of LLMs? RQ3 How can we safely integrate LLMs in WoZ\nexperiments, and what limitations and distortions should be con-\nsidered when interpreting data generated in such settings?\nFigure 2: Study 1: Simulating conversational WoZ experi-\nments using the WoLs and the Simulacrums.\n3.1 Study 1: When WoLs meet Simulacrums\nWe joined WoLs with Simulacrums in a conversational WoZ ex-\nperiment. Before testing with people, we aim to (1) identify LLMsâ€™\nbehaviors and verify whether their â€œfailuresâ€ are dangerous to hu-\nman participants and (2) collect a wide sample of agent-to-agent\nconversations to observe a broad range of failure modes.\n3.1.1 Method. The WoLs and the Simulacrums were GPT-4 agents1,\nand their behaviors were steered by system prompts (see Figure 2).\nThe prompts instruct them to align their behaviors with normal\nconversation structures with strangers (e.g., start with an introduc-\ntion, send succinct messages, etc.). Gender-neutral names, Jamie\nand Leslie, were assigned to the WoLs and the Simulacrums respec-\ntively for chatting purposes.\nSeveral factors could affect message generation and conversation\ndynamics, including interlocutorsâ€™ identity disclosure and demo-\ngraphic backgrounds [57, 62], the amount of detailed context and\ngranularity of instruction to LLMs [5, 65], and temperature param-\neter setting [48]. Accordingly, we note five independent variables:\nâ€¢ Bot identity disclosure . A boolean value determines whether the\nWoL self-discloses as a bot. A persuasive chatbot study showed\nthat disclosure affects persuasion outcome [57].\nâ€¢ Demographic information . The WoL and the Simulacrum were\nassigned information including age, income, education, political\naffiliation, gender, and ethnicity. The distribution followed 2020\nUS Census data2 except gender, which was sampled based on a\nreleased dataset [36] to include non-binary identities. The demo-\ngraphic information could help the WoL and the Simulacrum pick\ntheir standpoints when chatting and assist the WoL in adjusting\nits persuasion strategy. Conversely, the demographic background\nopens up space for biases to arise.\nâ€¢ Demographic information disclosure . The WoL and the Simu-\nlacrum were assigned a boolean each to state whether their de-\nmographic information should be part of their self-introduction.\nâ€¢ Instruction granularity . This feature guides the conversation.\nWe defined three levels of instruction granularity, instructing\nthe WoL on what to chat about: Level 1 random chat, Level\n2 chat around a topic, Level 3 chat around a topic and towards\na goal. All Level 2 and Level 3 conversations followed one of\nthe three topics: adoption of electric vehicles (EV), adoption\n1https://openai.com/research/gpt-4. Accessed September 2023.\n2https://www.census.gov/programs-surveys/decennial-census/decade/2020/2020-\ncensus-results.html. Accessed September 2023.\n3\nFang and Arechiga, et al.\nof green household electrification, and donating to a charity,\nwhile the conversation goals (Level 3 only) are to persuade the\ninterlocutors to adopt/donate. The embedded topic promptand\ngoal promptfollow the instruction granularity. For example,\nwhen instruction granularity is set to Level 1, the topic prompt\nfed to the WoL is â€œYou will initiate a random chat with your\ninterlocutorâ€ while the goal promptis left empty.\nâ€¢ Temperature. This GPT-4 variable controls how diverse the WoLâ€™s\ngenerated outputs are, with three levels: 1 (GPT-4â€™s default tem-\nperature), 0.5 (more stable), and 1.5 (more diverse outputs). The\ntemperature of the Simulacrum stayed at the default value.\nWe generated 131 WoLs and Simulacrums conversations; each\nconversation includes 12 turns (i.e., 25 messages in total, with 13\nWoL messages (including an initialization) and 12 Simulacrum mes-\nsages). For each conversation, a new pair of WoL and Simulacrum\nwas initialized with random values for all five factors.3.\nClosed-loop chatting between LLMs is an under-explored sce-\nnario. Can the WoLs lead meaningful conversations? Will the Sim-\nulacrums follow? Will the conversations converge at some point\n(or will the toxicity or bias be amplified during conversations)? We\nanalyze these LLMs-generated dialogues to answer RQ1.\n3.1.2 Analysis and Result. We found that the WoLs can usually\ninitiate conversations and properly engage with the Simulacrums\nin the early stage. However, sometimes, conversations later go\noff-track. See Appendix B for an example.\nHow can we analyze the large amount of conversational data\nsystematically? In-depth investigation of batches of conversational\ndata is costly, and human evaluation at a large scale is usually im-\npractical. Informed by observed failure modes of LLMs (generating\nbiased and harmful content [16, 62, 80], repetitive messages [26, 38],\nincoherent or nonsensical text [ 29, 68], and limited instruction-\nfollowing ability [39, 69, 76]), we introduce a heuristic evaluation\nframework that quantitatively estimated the conversation quality\nthrough lenses of toxicity, sentiment, text similarities, readability,\nand topic modeling. These measures fulfill the criteria for an initial\nassessment of LLM-based WoZ chats by being (1) scalable, compu-\ntationally inexpensive, and applicable to large datasets, (2) broadly\ncapturing limitations of LLMsâ€™ generations recognized in NLP lit-\nerature, and (3) interpretable by the experimenters so the LLM\nWizards can be refined before being deployed in real-world humanâ€“\nfacing WoZ experiments. While these metrics are not exhaustive\nand cannot discover all LLMsâ€™ failure modes (which is inherent\nin all heuristic methods), they enable a fast scan of some critical\naspects of WoLsâ€™ behaviors in conversational WoZ and an assess-\nment of whether WoLs have the potential to be safely applied in\nhuman-facing setups. The framework is summarized in a table in\nAppendix C. We describe the rationales of each of the metrics as\nfollows. Examples of generated messages and their corresponding\nquantitative scores are provided in Appendix E.\nToxicity. Toxicity is the most important consideration when we\ngauge the potential of applying LLMs in real-world human-facing\nWoZ experiments. Our Simulacrums had profiles with diverse com-\nbinations of demographics, which made a good estimation of how\n3The supplemental material details how system prompts incorporated the independent\nvariables: https://osf.io/akyf2/?view_only=a12a3a3d0c6d4be3884ca3f82aaad5ab.\ntoxic WoLs were (especially when they face Simulacrums with\ndemographics representing minorities) possible.\nWe measured message toxicity using the toxicity score from\nPerspective API 4 , which has been widely used for NLG evalu-\nation [20, 44]. Each API call returns a score ranging from 0 to 1,\nrepresenting the possibility of the input message being toxic. Fol-\nlowing previous studies, we considered messages with a toxicity\nscore of >= 0.5 to be toxic. WoLs generated non-toxic messages\nregardless of their interlocutorsâ€™ demographics. All WoLsâ€™ mes-\nsages had low toxicity scores ( ğ‘€ = 0.02, ğ‘†ğ· = 0.03). Similarly,\nSimulacrumsâ€™ messages were also unlikely to be toxic (ğ‘€ = 0.02,\nğ‘†ğ· = 0.03).\nSentiment analysis. Sentiment is a measure for signaling bias\nin LLM-generated text [12, 56]. Following previous works, we ap-\nplied VADER [28], a computationally efficient rule-based model , as\nthe sentiment analyzer for conversation messages. The output com-\npound score ranges from -1 (extremely negative) to 1 (extremely\npositive). Analyses showed some LLMs exhibit bias by generating\ntexts with more negative sentiments when provided with contexts\nlinked to specific groups [12, 56].\nThe sentiments of WoLsâ€™ messages were consistently positive\n(ğ‘€ = 0.73, ğ‘†ğ· = 0.29), and so were the sentiments of Simulacrumsâ€™\nmessages (ğ‘€ = 0.71, ğ‘†ğ· = 0.28). We found no statistically sig-\nnificant difference in messagesâ€™ sentiments regarding any of the\nindependent variables (i.e., whether a Simulacrum disclosed its\ndemographics, whether a conversation was a random chat or a per-\nsuasive dialogue, etc.). Notably, the sentiments of WoLsâ€™ messages\ndid not differ based on the demographics of the Simulacrums they\nwere talking to, no matter whether the Simulacrums self-disclosed\nthe information or not. We observed that the magnitude of demo-\ngraphic differences between interlocutors (quantified as the average\nof normalized differences along each dimension of demographic\ninformation) had no main effect on either sentiments.\nMessage similarity. LLM-generated chat messages may be\nrepetitive sometimes, especially in a closed-loop setting [ 38]. To\nquantitatively observe this problem, we adopted semantic similarity\nand sequence-based similarity to compare each message with the\ntwo previous messages in the dialogue:\n(1) Semantic similarity. Semantic similarity measures how close\ntext meanings are. We used the SentenceTransformers framework\n5 to compute text embeddings by loading a pre-trained model,\nall-MiniLM-L6-v2, then the semantic similarity of pairs of texts\nwas computed by the cosine similarity between their embeddings.\nSemantic similarity gradually increased over time, as shown in\nFigure 3. We split each conversation into three segments (segment 1:\nfrom beginning to conversation turn 4, segment 2: conversation turn\n5 to 8, segment 3: conversation turn 9 to the end). Welchâ€™s t-tests\nshowed that semantic similarity between two adjacent messages\nsent by WoLs (separated by one message sent by the Simulacrums)\nin segment 3 (ğ‘€ = 0.66, ğ‘†ğ· = 0.17) was significantly higher than\nthat in segment 2 (ğ‘€ = 0.59, ğ‘†ğ· = 0.15) (ğ‘¡(258)= âˆ’3.5,ğ‘ < .05),\nwhich was significantly higher than that in segment 1 (ğ‘€ = 0.53,\nğ‘†ğ· = 0.12) (ğ‘¡(249) = âˆ’3.2,ğ‘ < .05). The increase in semantic\n4https://perspectiveapi.com/. Accessed September 2023.\n5https://www.sbert.net/. Accessed September 2023.\n4\nOn LLM Wizards\nsimilarity between WoLsâ€™ messages and the Simulacrumsâ€™ messages\nthat they responded to, was not significant.\nFactorial ANOVA showed that WoLâ€™s temperature had main\neffects on semantic similarity between WoLâ€™s adjacent messages\n(ğ¹(2)= 19.7,ğ‘ < .05) as well as between WoLâ€™s message and the\nprevious message it received from the Simulacrum (ğ¹(2)= 61.4,ğ‘ <\n.05). Instruction granularity had a main effect on semantic similar-\nity between WoLâ€™s adjacent messages (ğ¹(2)= 29.9,ğ‘ < .05). Higher\ntemperature led to lower semantic similarity (between two WoLâ€™s\nmessages, when temperature=0.5: ğ‘€ = 0.64, ğ‘†ğ· = 0.14, tempera-\nture=1: ğ‘€ = 0.59, ğ‘†ğ· = 0.10, temperature=1.5: ğ‘€ = 0.48, ğ‘†ğ· = 0.06;\nbetween WoLâ€™s message and Simulacrumâ€™s message, when tem-\nperature=0.5: ğ‘€ = 0.60, ğ‘†ğ· = 0.57, temperature=1: ğ‘€ = 0.57,\nğ‘†ğ· = 0.12, temperature=1.5: ğ‘€ = 0.33, ğ‘†ğ· = 0.08). Higher in-\nstruction granularity led to higher semantic similarity (between\ntwo WoLâ€™s messages; when instruction granularity=1: ğ‘€ = 0.52,\nğ‘†ğ· = 0.11, instruction granularity=2: ğ‘€ = 0.58, ğ‘†ğ· = 0.11, instruc-\ntion granularity=3: ğ‘€ = 0.68, ğ‘†ğ· = 0.11).\nWhile a high semantic similarity might imply the WoL was\nbeing repetitive, it might also be a positive signal indicating strong\nconversation cohesiveness. We further analyzed the sequence-based\nsimilarity of messages for a deeper understanding.\n(2) Sequence-based similarity. We calculated the longest common\nsubsequence (lcsseq) similarity between messages as a measure of\nsequence-based similarity using TextDistance library 6. The lcsseq\nsimilarity (Figure 4) increased as conversations proceeded. Splitting\nthe conversations into three segments, Welchâ€™s t-tests showed that\nlcsseq similarity between WoLâ€™s messages in segment 3 (ğ‘€ = 0.49,\nğ‘†ğ· = 0.15) was significantly higher than in segment 2 (ğ‘€ = 0.44,\nğ‘†ğ· = 0.09) (ğ‘¡(213)= âˆ’3.61,ğ‘ < .05), which was higher than that\nin segment 1 ( ğ‘€ = 0.39, ğ‘†ğ· = 0.05) (ğ‘¡(202) = âˆ’5.19,ğ‘ < .05).\nSequence similarity between WoLâ€™s messages and the previous\nmessages from Simulacrum in segment 2 ( ğ‘€ = 0.49, ğ‘†ğ· = 0.08)\nwas also significantly higher than that in segment 1 ( ğ‘€ = 0.38,\nğ‘†ğ· = 0.04) (ğ‘¡(202)= âˆ’2.46,ğ‘ < .05). Rising lcsseq similarity signals\nincreasingly large portions of verbatim common text. Together with\nthe heightened semantic similarities, we concluded that WoLs sent\nincreasingly repetitive messages as conversations proceeded.\nMessage readability. LLMs occasionally generate senseless\ntexts that elude grammatical checks as they may follow rules (e.g.,\nhave Subjectâ€“Verbâ€“Object structures). The Flesch read ease score [17]\nwas used for conversational readability estimation. The score of\neach message was calculated using the Textstat library 7 , where a\nlow score indicates confusing expressions. Welchâ€™s t-tests showed\nthat WoLsâ€™ message readability decreased significantly over time,\nwhile Simulacrumsâ€™ message readability was relatively stable (see\nFigure 5). Specifically, WoLsâ€™ messages readability in segment 2\n(ğ‘€ = 0.47, ğ‘†ğ· = 0.16) was significantly lower than that of seg-\nment 1 ( ğ‘€ = 0.51, ğ‘†ğ· = 0.11) (ğ‘¡(228) = 2.39,ğ‘ < .05), while\ninsignificantly higher than segment 3 ( ğ‘€ = 0.40, ğ‘†ğ· = 0.39)\n(ğ‘¡(172)= 1.87,ğ‘ = 0.06). Factorial ANOVA showed that instruc-\ntion granularity had a main effect on WoLsâ€™ message readability\n(ğ¹(2)= 6.72,ğ‘ < .05. Readability when instruction granularity=1:\n6https://github.com/life4/textdistance. Accessed September 2023.\n7https://pypi.org/project/textstat/. Accessed September 2023.\nğ‘€ = 0.51, ğ‘†ğ· = 0.19; granularity=2: ğ‘€ = 0.41, ğ‘†ğ· = 0.21; gran-\nularity=3: ğ‘€ = 0.48, ğ‘†ğ· = 0.14). Temperature significantly af-\nfected readability ( ğ¹(2) = 104.7,ğ‘ < .05. ğ‘¡ğ‘’ğ‘šğ‘ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ = 0.5:\nğ‘€ = 0.54, ğ‘†ğ· = 0.07; ğ‘¡ğ‘’ğ‘šğ‘ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ = 1: ğ‘€ = 0.53, ğ‘†ğ· = 0.07;\nğ‘¡ğ‘’ğ‘šğ‘ğ‘’ğ‘Ÿğ‘ğ‘¡ğ‘¢ğ‘Ÿğ‘’ = 1.5: ğ‘€ = 0.15, ğ‘†ğ· = 0.25).\nTopic modeling. Conversational content and topicality play\nmajor roles in the WoLâ€™s performance. Topic modeling makes a\nquick scan possible; our domain examines â€œattitude towards electric\nvehicles (EV)â€. Here, we showcase examples of how topic modeling\ncan unveil nuances of conversations based on Simulacrumsâ€™ demo-\ngraphics, which serves as an estimation of whether WoLs conduct\nconversations according to their interlocutorsâ€™ identities. In this\nscenario, we only keep conversations in which the Simulacrums\nself-disclose their information. We preprocessed all messages sent\nby WoLs in these conversations (i.e., removed punctuations, stop\nwords, tokenized), then trained Latent Dirichlet Allocation (LDA)\nmodels using corpora and dictionaries converted from tokenized\ntexts as inputs for each demographic group.\nComparing results from different demographic groups, â€œelectricâ€\nâ€œvehicleâ€ were the leading terms for all topics, implying the WoLs\ndid well in staying on topic. Next, we examine the overall term\nfrequencies across identified topics. Figures 6a, 6b, 6c, and 6d show\nthe top 15 terms in conversations between WoLs and Simulacrums\nassigned as Women, Men, Democrats, and Republicans respectively.\nWhile it is hard to claim that the conversations differ significantly\nbased on the Simulacrumsâ€™ demographics, we found cues implying\nthat the WoLs adapted their wording based on the Simulacrumsâ€™\ndisclosed identity. For example, comparing the lists in Figures 6c\nand 6d, WoLs tend to mention more words like â€œtransitionâ€ and\nâ€œsustainableâ€ to Democrats Simulacrums than to Republican Simu-\nlacrums. Personalization based on interlocutorsâ€™ demographics and\nvalues could be a good strategy for persuasive conversations [25].\nYet, this strategy may open doors for potential bias [77].\nIn summary, we applied a heuristic evaluation framework com-\nprised of computational metrics to surface WoLsâ€™ behaviors in sim-\nulated conversational-WoZ experiments. It revealed pitfalls in LLM-\ngenerated conversations that may have not been quantitatively\nmeasured before. While the quantitative and heuristic natures of\nthe framework made the analysis relatively coarse-grained, they\nallow fast and large-scale surfacing of WoLsâ€™ behaviors and provide\na foundation for further exploration. WoLs to Simulacrums chats\nprovide opportunities to estimate how bad the failures are without\nrisking human participants. While the WoLs made some mistakes\n(e.g., sending confusing messages), they did well in being non-toxic\nand non-discriminative. This suggests that it is safe to proceed to\nthe next stage of the lifecycleâ€”testing WoLs with real Participants.\n3.2 Study 2: When WoLs meet Participants\nNext, an LLM-to-human exploration is necessary to model the\nLLMs-supported WoZ (addressing RQ2) in a more realistic setting.\n3.2.1 Fix. Our experiment lifecycle advised experimenters to â€œfixâ€\nWoLs before advancing to Stage 2. Here, we streamlined the fixing\nprocess as technical methods of refining WoLs (e.g., prompt engi-\nneering, finetuning, retrieval-augmented generation) could vary\ncase-by-case and are not the focus of this paper. An effective method\n5\nFang and Arechiga, et al.\nFigure 3: Semantic similarity be-\ntween each message with the previ-\nous two messages.\nFigure 4: Sequence-based similarity\nbetween each message and the pre-\nvious two messages.\nFigure 5: Readability of messages.\n(a) Women\n (b) Men\n (c) Democrats\n (d) Republicans\nFigure 6: Top 15 terms in WoLsâ€™ messages in EV adoption conversations when Simulacrums role-play various personas.\nfor fixing WoLsâ€™ identified problems in our conversation context\nmay be inapplicable to WoLs in other chatting scenarios. This paper\naims at walking through the lifecycle, offering reference values for\nfuture experiments. We advise experimenters to investigate ways\nof addressing identified problems in their specific context. In our\nEV adoption chatting scenario, WoLs are safe for humans in critical\ndimensions (non-toxic, non-biased) while having unstable perfor-\nmance in other aspects. We simplified the fix phase as picking the\nsettings of WoLs to maximize these aspects of conversation quality\n(e.g., readability and non-repetitiveness) based on Study 1 result.\nStage 2 involves human participants and hence conducts fewer\nconversations than the previous stage. The smaller-scale collected\ndata is suitable for a more discreet qualitative evaluation that aims\nto uncover latent failure modes that went undetected in the coarse-\ngrained quantitative analysis after Stage 1.\n3.2.2 Method. We recruited 56 study participants from Prolific8 to\nchat with WoLs using Study 1â€™s prompt template and selected set-\ntings that resulted in high conversation quality: Wizards hid their\nbot and demographic identities, chatted about EVs and persuaded\nadoption, and used temperature 1. Participants are U.S. residents,\nabove 18 years old, have a driving license, and do not own/lease\n8https://www.prolific.co/ Accessed September 2023.\nan EV. Participants were told they would talk to â€œJamieâ€. The con-\nversation needed 12 turns before the conclusion, followed by a\nsurvey regarding perceived rapport [67], chat partner impression\nand conversation quality [57], perceived bot identity, open-ended\nfeedback, and demographics. The study takes roughly 20 minutes.\n3.2.3 Analysis and Result.\nConversations between WoLs and Participants. Like in Study 1,\nWoLsâ€™ messages were non-toxic (toxicity:ğ‘€ = 0.01,ğ‘†ğ· = 0.02). The\nsentiments of WoLsâ€™ messages stayed positive (ğ‘€ = 0.58,ğ‘†ğ· = 0.13)\nand were significantly more positive than those of Participantsâ€™\nmessages (ğ‘€ = 0.26,ğ‘†ğ· = 0.17) (ğ‘¡(102) = 11.05,ğ‘ < .05), as\nshown in Figure 7. Factorial ANOVA showed no evidence that WoLsâ€™\nsentiments differed based on the Participantsâ€™ demographics. WoLsâ€™\nsentiment had no effects on Participantsâ€™ perceived rapport, chat\npartner impression, conversation quality, and persuasion outcome.\nThus WoLs are likely to be unharmful when talking to humans.\nBoth semantic and sequence-based similarities between mes-\nsages were relatively stable as shown in Figure 8 and Figure 9, except\nthat the semantic similarity between WoLsâ€™ messages and Partici-\npantsâ€™ previous messages in segment 2 (ğ‘€ = 0.36,ğ‘†ğ· = 0.14) was\nsignificantly higher than that in segment 1 (ğ‘€ = 0.45,ğ‘†ğ· = 0.12).\nThe readability of the messages stayed consistent (see Figure 10).\nTopic modeling results again showed that WoLs stayed on the topic.\n6\nOn LLM Wizards\nDifferent from Study 1, it seems that the frequently mentioned\nterm lists were very similar across different demographic groups\n(in Figures 11a, 11b, 11c, and 11d).\nParticipants were generally positive about their interactions with\nWoLs. On a scale of 5, WoLs were rated highly regarding perceived\nrapport (ğ‘€ = 4.40,ğ‘†ğ· = 0.80), chat partner impression ( ğ‘€ =\n4.46,ğ‘†ğ· = 0.62), and conversation quality (ğ‘€ = 4.42,ğ‘†ğ· = 0.66).\nMany participants recognized the WoLs were bots due to Jamieâ€™s\nfaster-than-human typing speed; the content and overall flow of\nWoLsâ€™ messages were perceived to be natural and human-like.\n3.3 Comparing Study 1 and Study 2\nNext, one must compare the two WoZ studies [33]: how did WoLs-\nto-Simulacrums differ from WoLs-to-Partcipants? As we relied on\ndata generated from the synthetic LLMs-to-LLMs setting in Stage\n1 to make key decisions of whether and how we can proceed to\nhuman-facing WoZ experiments, it is essential to understand how\ndistorted the Stage 1 data is. A comparison between Study 1 and\nStudy 2 can inform us what distortions should be considered and\nhow experimenters should calibrate their expectations when in-\nterpreting data generated in the WoLs-to-Simulacrums setting. To\ninvestigate, we sampled 25 conversations with the same setups\nfrom Study 1 and from Study 2. The conversations were compared\nquantitatively along the dimensions of the proposed evaluation\nmetrics, then analyzed by two HCI experts to gauge the overall\nchat quality, the WoLsâ€™ instruction following, and what mistakes\nWoLs made when role-playing Wizards. The qualitative analysis is\nused to capture a broader range of Wizardsâ€™ failure modes that the\nquantitative metrics failed to identify.\n3.3.1 Quantitative result. There was no significant difference in\ntoxicity and sentiment of WoLsâ€™ messages. However, WoLs-to-\nParticipants messages were less repetitive as the semantic simi-\nlarity between adjacent messages in Study 2 (ğ‘€ = 0.48,ğ‘†ğ· = 0.07)\nwas significantly lower than in Study 1 ( ğ‘€ = 0.57,ğ‘†ğ· = 0.07)\n(ğ‘¡(48)= 4.67,ğ‘ < 0.05). No significant difference in WoLsâ€™ message\nreadability was observed between the two studies.\n3.3.2 Human evaluation. We asked two experts familiar with WoZ\nmethods to read the 50 conversations and identify how WoLs failed\nto role-play well. Apart from the repetition issue (i.e., WoLs being\nmore repetitive in the closed-loop setting of Study 1) which was\nalready recognized by quantitative metrics, two themes evident in\nconversations from Study 2 emerged:\nWoLs were too salesman-like. When Participants clearly expressed\nreluctance towards buying an EV, WoLs were being â€œpolitely pushyâ€\nwithout compromise. WoLs did not understand that the conversa-\ntion goals might take indirect paths (e.g., persuading to lease EVs\nor choose EVs for ride-sharing services). This is a sign that WoLs\ndid not acquire outstanding persuasion strategies.\nWoLs made assumptions of their interlocutors and lacked empathy.\nWoLs sometimes make false assumptions about the Participants.\nFor example, WoLs sometimes assumed that Participants could\ncharge EVs overnight from home and wake up with a charged car.\nSimilarly, WoLs assumed that Participants did not have financial\ndifficulties; some Participants said EVs are too expensive, and the\nWoLs lacked empathy and failed to build rapport.\nRole-switching. This failure mode only appeared once in a WoL-\nto-Simulacrum closed-loop conversation where the WoL and the\nSimulacrum switched roles. The WoL assumed a study participant\nrole and discussed how they could not afford a car. It could be mea-\nsured quantitatively but went undetected in our current evaluation\nmetrics. A quantitative measure that identifies the role-switching\nphenomenon can be integrated into our evaluation framework.\n4 DISCUSSION\nRevisiting RQ3, we found LLMs can be useful tools for conversa-\ntional WoZ experiments; however, potential pitfalls exist. Following\na two-stage experiment lifecycle, LLMs showed the potential to\nbe safely applied in human-facing studies. LLMs, role-playing as\nWizards, can elicit user attitudes and behaviors when engaging\nwith an envisioned technology and probe the design space of the\ntechnology as human Wizards would do in a traditional WoZ study.\n4.1 Responsibly integration of LLMs and WoZ\nCombining Study 1 and Study 2, we propose a two-stage experiment\nlifecycle (Figure 1) for estimating the risks and potentials of LLMs-\npowered WoZ experiments.\nStage 1:Replacing humans on both sides of traditional WoZ ex-\nperiments with LLM-powered agents . Following Study 1, this stage\ncreates a simulation of conversations between Wizards and par-\nticipants without risking humansâ€™ exposure to harmful content.\nExperimenters should inspect this stageâ€™s data and identify failures\nbefore continuing. We proposed a heuristic evaluation framework\ncombining quantitative metrics that help experimenters understand\nthe data in a scalable and explainable manner. Experimenters should\nstrive to correct the WoLs through various techniques (e.g., finetun-\ning) before moving on to human-facing experiments if they show\nevidence of being potentially harmful.\nStage 2:Piloting conversations between Wizards role-played by\nLLMs and human participants . This stage, following Study 2, affords\na realistic pilot with access to human feedback. It is essential to no-\ntice that the Simulacrumsâ€™ behaviors may be distorted from human\nbehaviors as they lack human perceptions, and the quantitative\nmetrics cannot capture all aspects of the conversation data. This\nstage allows one to close these gaps. By comparing data generated\nin Stage 2 and Stage 1, experimenters can understand the distortions\nof LLM-to-LLM data. Stage 2 also elicits feedback from participants\nregarding their chatting experience (e.g., perceived rapport) and\nallows for an in-depth qualitative inspection. Another round of\nadjustments on the WoLs should be applied if any additional failure\nmodes are found in this stage. This lifecycle establishes a study setup\ninvolving the finalized LLM Wizards that are safe for human-facing\nexperiments. These LLM Wizards can lead large-scale experiments\nwithout overburdening human experimenters with role-playing\ntasks.\nIn this paper, we demonstrate this experiment lifecycle in the\ncontext of EV adoption conversations led by the WoLs (with addi-\ntional conversation topics included in Study 1) and showcase how\nheuristic evaluations can be used in the piloting process. In our\nscenario, WoLsâ€™ messages are not harmful. We found cues that they\nmay personalize the conversations based on participantsâ€™ demo-\ngraphics. However, they could sometimes be repetitive or generate\n7\nFang and Arechiga, et al.\nFigure 7: WoLsâ€™ and Partici-\npantsâ€™ sentiments.\nFigure 8: Semantic similar-\nity between messages.\nFigure 9: Sequence-based\nsimilarity between mes-\nsages.\nFigure 10: Readability of\nmessages.\n(a) Women\n (b) Men\n (c) Democrats\n (d) Republicans\nFigure 11: Top 15 terms in WoLsâ€™ messages in EV adoption conversations when Participants have various self-identifications.\nmessages with low readability, which confused participants. Our\nhuman evaluation further revealed that WoLs can be pushy and\nlack empathy. The WoLsâ€™ successes and failures were gradually un-\nveiled through our two-stage experiment lifecycle without exhibiting\nharm to human participants, indicating the benefit of adopting the\nlifecycle as a methodology guidance for safely integrating LLMs in\nWoZ experiments. The Simulacrums and Participantsâ€™ overall posi-\ntive reactions to WoLs acknowledge the feasibility and potential\nof the envisioned technology being simulated in the study (i.e., a\nspecialized persuasive chatbot). The imperfection of WoLs further\nsuggests opportunities for the not-yet-developed technology to\nshine. By examining the limitations of WoLs powered by general-\npurpose models, experimenters acquire insights into the specific\nareas and dimensions where the new technology can excel. Further-\nmore, the elicited/simulated usersâ€™ data projects usersâ€™ attitudes\ntoward and interactions with the envisioned technology, helping\ndevelopers anticipate user behaviors so that they can design and\ndevelop functionalities accordingly.\n4.2 Designing Guardrails for LLMs and WoZ\nOne could apply many techniques to improve the WoLs, asfixing the\nidentified problems is the primary reason for identifying them. For\nthe scenarios we investigated, picking the right settings/parameters\nis enough to tune the WoLs to role-play well. We suggest methods\nthat may be desired for fixing WoLs in other conversation contexts.\nWoLs can be finetuned to focus on domain knowledge effectively,\nyet finetuning requires resources that might be inaccessible to\nmany. Another approach to tame the Wizards is prompt engineering.\nStrategies such as few-shot learning [ 5], Chain of Thought [ 65],\nand Tree of Thoughts [74] can improve conversations.\nLLM-based critiques can provide guardrails to correct model\noutputs based on a set of manually crafted principles or a â€œconsti-\ntutionâ€ [4]. This approach is promising as it allows in-place fixes\nduring conversations. While current work in this direction only\nasks the critiques to correct unethical messages, our studies found\nthat WoLs can fail in more ways. An enhanced â€œconstitutionâ€ for\nWoZ experiments can be informed by this experiment lifecycle.\n4.3 Limitations and Future Work\nOur studies have several limitations. We only included three con-\nversation topics; only GPT-4 was used to power WoLs. These may\ndampen the generalizability of the empirical results. While some\nWoLsâ€™ failure modes (e.g., increased repetition as the conversations\nproceed) are likely representative, the study results we derived\nmay not apply to all conversational WoZ experiments. For exam-\nple, WoLs powered by other LLMs (especially without RLHF) or\nchatting about controversial topics may generate toxic or biased\nmessages . However, these limitations do not diminish the main\ncontribution, which is guiding LLMs-powered WoZ experiments.\nWhile the quantitative nature of our proposed evaluation frame-\nwork allows fast and large-scale surfacing of WoLsâ€™ behaviors, it\nalso made the analysis coarse-grained. There are alternative ways\n8\nOn LLM Wizards\nof measuring the aspects we assessed (e.g., [19]), and the specific\nmeasures we used may not always be the most accurate ones. Yet,\nthe metrics we picked are computationally efficient, making them\nsuitable for analyzing large datasets. While we aimed to broadly\ncapture LLMsâ€™ failure modes, the list of potential LLMsâ€™ pitfalls is\nnon-exhaustive. Our framework cannot identify all potential failure\nmodes of WoLs, which is a limitation inherent in any heuristic\nevaluation method. We welcome future researchers to expand the\nevaluation framework as new failure modes emerge. The exper-\niment lifecycle leveraged synthetic data in Stage 1. Researchers\nmust be vigilant about potential risks and distortions it may bring.\nCheck Appendix A for an in-depth discussion of our commitment to\nmaintaining ethical standards throughout the experiment lifecycle.\nMany technologies could benefit from WoZ experiments, not\nlimited to chatbots advocating EV adoption or agents interacting\nthrough text. We have increasingly seen technologies (e.g., im-\nage/video generation, robot control) powered by multimodal mod-\nels; as such, WoLs can simulate various interactions beyond texting.\nTherefore, we expect the proposed experiment lifecycle involv-\ning LLM Wizards to be relevant and applicable in envisioning and\ndeveloping countless novel functionalities and technologies.\n5 CONCLUSION\nWe introduced an experiment lifecycle that guides researchers to\nresponsibly integrate LLMs into WoZ experiments through a two-\nstage process. The LLM-powered WoZ is a method for eliciting\nusersâ€™ reactions to an envisioned technology using LLM-generated\ntext, aiming at probing the design space of the technology. We\npresented an evaluation framework that helps researchers peek\nthrough the data generated with LLM Wizards and identify the\nWizardsâ€™ failures. Using conversations around EV adoption as an\nexample, we demonstrate how experimenters can leverage the ex-\nperiment lifecycle along with the evaluation framework to estimate\nthe potential and risks of applying LLMs as Wizards in human-\nfacing Wizard of Oz experiments.\nREFERENCES\n[1] William Agnew, A Stevie Bergman, Jennifer Chien, Mark DÃ­az, Seliem El-Sayed,\nJaylen Pittman, Shakir Mohamed, and Kevin R McKee. 2024. The illusion of arti-\nficial inclusion. arXiv preprint arXiv:2401.08572 abs/2401.08572 (2024), 12 pages.\n[2] Gati V Aher, Rosa I. Arriaga, and Adam Tauman Kalai. 2023. Using Large\nLanguage Models to Simulate Multiple Humans and Replicate Human Subject\nStudies. In Proceedings of the 40th International Conference on Machine Learning\n(Proceedings of Machine Learning Research, Vol. 202) , Andreas Krause, Emma\nBrunskill, Kyunghyun Cho, Barbara Engelhardt, Sivan Sabato, and Jonathan\nScarlett (Eds.). PMLR, Honolulu, Hawaii, 337â€“371. https://proceedings.mlr.press/\nv202/aher23a.html\n[3] Lisa P Argyle, Ethan C Busby, Nancy Fulda, Joshua R Gubler, Christopher Rytting,\nand David Wingate. 2023. Out of one, many: Using language models to simulate\nhuman samples. Political Analysis 31, 3 (2023), 337â€“351.\n[4] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,\nAndy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon,\nCarol Chen, Catherine Olsson, Christopher Olah, Danny Hernandez, Dawn\nDrain, Deep Ganguli, Dustin Li, Eli Tran-Johnson, Ethan Perez, Jamie Kerr,\nJared Mueller, Jeffrey Ladish, Joshua Landau, Kamal Ndousse, Kamile Lukosuite,\nLiane Lovitt, Michael Sellitto, Nelson Elhage, Nicholas Schiefer, Noemi Mercado,\nNova DasSarma, Robert Lasenby, Robin Larson, Sam Ringer, Scott Johnston,\nShauna Kravec, Sheer El Showk, Stanislav Fort, Tamera Lanham, Timothy Telleen-\nLawton, Tom Conerly, Tom Henighan, Tristan Hume, Samuel R. Bowman, Zac\nHatfield-Dodds, Ben Mann, Dario Amodei, Nicholas Joseph, Sam McCandlish,\nTom Brown, and Jared Kaplan. 2022. Constitutional AI: Harmlessness from AI\nFeedback. arXiv:2212.08073 [cs.CL]\n[5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot learners. Advances in neural\ninformation processing systems 33 (2020), 1877â€“1901.\n[6] Jacob T. Browne. 2019. Wizard of Oz Prototyping for Machine Learning Experi-\nences. In Extended Abstracts of the 2019 CHI Conference on Human Factors in Com-\nputing Systems (Glasgow, Scotland Uk) (CHI EA â€™19) . Association for Computing\nMachinery, New York, NY, USA, 1â€“6. https://doi.org/10.1145/3290607.3312877\n[7] Gillian Cameron, David Cameron, Gavin Megaw, Raymond Bond, Maurice\nMulvenna, Siobhan Oâ€™Neill, Cherie Armour, and Michael McTear. 2017. To-\nwards a Chatbot for Digital Counselling. In Proceedings of the 31st British Com-\nputer Society Human Computer Interaction Conference (Sunderland, UK) (HCI\nâ€™17). BCS Learning & Development Ltd., Swindon, GBR, Article 24, 7 pages.\nhttps://doi.org/10.14236/ewic/HCI2017.24\n[8] Maximillian Chen, Weiyan Shi, Feifan Yan, Ryan Hou, Jingwen Zhang, Saurav\nSahay, and Zhou Yu. 2022. Seamlessly Integrating Factual Information and Social\nContent with Persuasive Dialogue. arXiv:2203.07657 [cs.CL]\n[9] Cheng-Han Chiang and Hung yi Lee. 2023. Can Large Language Models Be an\nAlternative to Human Evaluations? arXiv:2305.01937 [cs.CL]\n[10] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang,\nLianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica,\nand Eric P. Xing. 2023. Vicuna: An Open-Source Chatbot Impressing GPT-4 with\n90%* ChatGPT Quality. https://lmsys.org/blog/2023-03-30-vicuna/\n[11] Nils DahlbÃ¤ck, Arne JÃ¶nsson, and Lars Ahrenberg. 1993. Wizard of Oz Studies:\nWhy and How. In Proceedings of the 1st International Conference on Intelligent\nUser Interfaces (Orlando, Florida, USA) (IUI â€™93) . Association for Computing\nMachinery, New York, NY, USA, 193â€“200. https://doi.org/10.1145/169891.169968\n[12] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruk-\nsachatkun, Kai-Wei Chang, and Rahul Gupta. 2021. BOLD: Dataset and Metrics\nfor Measuring Biases in Open-Ended Language Generation. In Proceedings of\nthe 2021 ACM Conference on Fairness, Accountability, and Transparency (Virtual\nEvent, Canada) (FAccT â€™21). Association for Computing Machinery, New York,\nNY, USA, 862â€“872. https://doi.org/10.1145/3442188.3445924\n[13] Danica Dillion, Niket Tandon, Yuling Gu, and Kurt Gray. 2023. Can AI language\nmodels replace human participants? Trends in Cognitive Sciences 27, 7 (2023),\n597â€“600. https://doi.org/10.1016/j.tics.2023.04.008\n[14] Steven Dow, Jaemin Lee, Christopher Oezbek, Blair MacIntyre, Jay David Bolter,\nand Maribeth Gandy. 2005. Wizard of Oz Interfaces for Mixed Reality Appli-\ncations. In CHI â€™05 Extended Abstracts on Human Factors in Computing Systems\n(Portland, OR, USA) (CHI EA â€™05) . Association for Computing Machinery, New\nYork, NY, USA, 1339â€“1342. https://doi.org/10.1145/1056808.1056911\n[15] A.E. Elo. 2008. The Rating of Chessplayers: Past and Present . Ishi Press Interna-\ntional, New York, USA.\n[16] Xiao Fang, Shangkun Che, Minjia Mao, Hongzhe Zhang, Ming Zhao, and Xi-\naohang Zhao. 2023. Bias of AI-Generated Content: An Examination of News\nProduced by Large Language Models. arXiv:2309.09825 [cs.AI]\n[17] Rudolph Flesch. 1948. A new readability yardstick. Journal of applied psychology\n32, 3 (1948), 221.\n[18] Kenneth D Forbus, Dedre Gentner, and Keith Law. 1995. MAC/FAC: A model of\nsimilarity-based retrieval. Cognitive science 19, 2 (1995), 141â€“205.\n[19] Jinlan Fu, See-Kiong Ng, Zhengbao Jiang, and Pengfei Liu. 2023. Gptscore:\nEvaluate as you desire. arXiv preprint arXiv:2302.04166 (2023).\n[20] Samuel Gehman, Suchin Gururangan, Maarten Sap, Yejin Choi, and Noah A.\nSmith. 2020. RealToxicityPrompts: Evaluating Neural Toxic Degeneration in\nLanguage Models. arXiv:2009.11462 [cs.CL]\n[21] Paul Green and Lisa Wei-Haas. 1985. The Rapid Development of User Inter-\nfaces: Experience with the Wizard of OZ Method. Proceedings of the Human\nFactors Society Annual Meeting 29, 5 (1985), 470â€“474. https://doi.org/10.1177/\n154193128502900515 arXiv:https://doi.org/10.1177/154193128502900515\n[22] Perttu HÃ¤mÃ¤lÃ¤inen, Mikke Tavast, and Anton Kunnari. 2023. Evaluating Large\nLanguage Models in Generating Synthetic HCI Research Data: A Case Study. In\nProceedings of the 2023 CHI Conference on Human Factors in Computing Systems\n(Hamburg, Germany) (CHI â€™23) . Association for Computing Machinery, New\nYork, NY, USA, Article 433, 19 pages. https://doi.org/10.1145/3544548.3580688\n[23] Xu Han, Michelle Zhou, Matthew J. Turner, and Tom Yeh. 2021. Designing\nEffective Interview Chatbots: Automatic Chatbot Profiling and Design Suggestion\nGeneration for Chatbot Debugging. In Proceedings of the 2021 CHI Conference on\nHuman Factors in Computing Systems (Yokohama, Japan) (CHI â€™21) . Association\nfor Computing Machinery, New York, NY, USA, Article 389, 15 pages. https:\n//doi.org/10.1145/3411764.3445569\n[24] Jacqueline Harding, William Dâ€™Alessandro, NG Laskowski, and Robert Long.\n2023. Ai language models cannot replace human research participants. AI &\nSOCIETY 28, 3 (2023), 1â€“3.\n[25] Totte Harinen, Alexandre Filipowicz, Shabnam Hakimi, Rumen Iliev, Matthew\nKlenk, and Emily Sumner. 2021. Machine learning reveals how personalized\nclimate communication can both succeed and backfire. arXiv:2109.05104 [cs.LG]\n[26] Ryuichiro Higashinaka, Masahiro Araki, Hiroshi Tsukahara, and Masahiro\nMizukami. 2021. Integrated taxonomy of errors in chat-oriented dialogue sys-\ntems. In Proceedings of the 22nd Annual Meeting of the Special Interest Group on\nDiscourse and Dialogue . 89â€“98.\n9\nFang and Arechiga, et al.\n[27] Minlie Huang, Xiaoyan Zhu, and Jianfeng Gao. 2020. Challenges in building in-\ntelligent open-domain dialog systems. ACM Transactions on Information Systems\n(TOIS) 38, 3 (2020), 1â€“32.\n[28] C. Hutto and Eric Gilbert. 2014. VADER: A Parsimonious Rule-Based Model\nfor Sentiment Analysis of Social Media Text. Proceedings of the International\nAAAI Conference on Web and Social Media 8, 1 (May 2014), 216â€“225. https:\n//doi.org/10.1609/icwsm.v8i1.14550\n[29] Boris V Janssen, Geert Kazemier, and Marc G Besselink. 2023. The use of ChatGPT\nand other large language models in surgical science. , zrad032 pages.\n[30] Eunkyung Jo, Daniel A. Epstein, Hyunhoon Jung, and Young-Ho Kim. 2023.\nUnderstanding the Benefits and Challenges of Deploying Conversational AI\nLeveraging Large Language Models for Public Health Intervention. InProceedings\nof the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg,\nGermany) (CHI â€™23) . Association for Computing Machinery, New York, NY, USA,\nArticle 18, 16 pages. https://doi.org/10.1145/3544548.3581503\n[31] Elise Karinshak, Sunny Xun Liu, Joon Sung Park, and Jeffrey T Hancock. 2023.\nWorking With AI to Persuade: Examining a Large Language Modelâ€™s Ability to\nGenerate Pro-Vaccination Messages.Proceedings of the ACM on Human-Computer\nInteraction 7, CSCW1 (2023), 1â€“29.\n[32] J. F. Kelley. 1983. An Empirical Methodology for Writing User-Friendly Natural\nLanguage Computer Applications. In Proceedings of the SIGCHI Conference on\nHuman Factors in Computing Systems (Boston, Massachusetts, USA) (CHI â€™83) .\nAssociation for Computing Machinery, New York, NY, USA, 193â€“196. https:\n//doi.org/10.1145/800045.801609\n[33] John F. Kelley. 1984. An Iterative Design Methodology for User-Friendly Natural\nLanguage Office Information Applications. ACM Trans. Inf. Syst. 2, 1 (January\n1984), 26â€“41. https://doi.org/10.1145/357417.357420\n[34] Soomin Kim, Joonhwan Lee, and Gahgene Gweon. 2019. Comparing Data from\nChatbot and Web Surveys: Effects of Platform and Conversational Style on Survey\nResponse Quality. In Proceedings of the 2019 CHI Conference on Human Factors in\nComputing Systems (Glasgow, Scotland Uk)(CHI â€™19). Association for Computing\nMachinery, New York, NY, USA, 1â€“12. https://doi.org/10.1145/3290605.3300316\n[35] Scott R. Klemmer, Anoop K. Sinha, Jack Chen, James A. Landay, Nadeem\nAboobaker, and Annie Wang. 2000. Suede: A Wizard of Oz Prototyping Tool\nfor Speech User Interfaces. In Proceedings of the 13th Annual ACM Sympo-\nsium on User Interface Software and Technology (San Diego, California, USA)\n(UIST â€™00) . Association for Computing Machinery, New York, NY, USA, 1â€“10.\nhttps://doi.org/10.1145/354401.354406\n[36] Matthew L Lee, Scott Carter, Rumen Iliev, Nayeli Suseth Bravo, Monica P Van,\nLaurent Denoue, Everlyne Kimani, Alexandre L. S. Filipowicz, David A. Shamma,\nKate A Sieck, Candice Hogan, and Charlene C. Wu. 2023. Understanding Peopleâ€™s\nPerception and Usage of Plug-in Electric Hybrids. In Proceedings of the 2023 CHI\nConference on Human Factors in Computing Systems (Hamburg, Germany) (CHI\nâ€™23). Association for Computing Machinery, New York, NY, USA, Article 201,\n21 pages. https://doi.org/10.1145/3544548.3581301\n[37] Yi-Chieh Lee, Naomi Yamashita, Yun Huang, and Wai Fu. 2020. \"I Hear You, I\nFeel You\": Encouraging Deep Self-Disclosure through a Chatbot. In Proceedings\nof the 2020 CHI Conference on Human Factors in Computing Systems (Honolulu,\nHI, USA) (CHI â€™20) . Association for Computing Machinery, New York, NY, USA,\n1â€“12. https://doi.org/10.1145/3313831.3376175\n[38] Guohao Li, Hasan Abed Al Kader Hammoud, Hani Itani, Dmitrii Khizbullin, and\nBernard Ghanem. 2023. CAMEL: Communicative Agents for \"Mind\" Exploration\nof Large Language Model Society. arXiv:2303.17760 [cs.AI]\n[39] Shiyang Li, Jun Yan, Hai Wang, Zheng Tang, Xiang Ren, Vijay Srinivasan, and\nHongxia Jin. 2023. Instruction-following Evaluation through Verbalizer Manipu-\nlation. arXiv:2307.10558 [cs.CL]\n[40] Chin-Yew Lin. 2004. ROUGE: A Package for Automatic Evaluation of summaries.\nIn Proceedings of the ACL Workshop: Text Summarization Braches Out . Association\nfor Computational Linguistics, Barcelona, Spain, 10.\n[41] Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang\nZhu. 2023. G-Eval: NLG Evaluation using GPT-4 with Better Human Alignment.\narXiv:2303.16634 [cs.CL]\n[42] Danica Mast, Alex Roidl, and Antti Jylha. 2023. Wizard of Oz Prototyping for\nInteractive Spatial Augmented Reality in HCI Education: Experiences with Rapid\nPrototyping for Interactive Spatial Augmented Reality. In Extended Abstracts\nof the 2023 CHI Conference on Human Factors in Computing Systems (Hamburg,\nGermany) (CHI EA â€™23) . Association for Computing Machinery, New York, NY,\nUSA, Article 407, 10 pages. https://doi.org/10.1145/3544549.3573861\n[43] David Maulsby, Saul Greenberg, and Richard Mander. 1993. Prototyping an\nIntelligent Agent through Wizard of Oz. In Proceedings of the INTERACT â€™93 and\nCHI â€™93 Conference on Human Factors in Computing Systems (Amsterdam, The\nNetherlands) (CHI â€™93) . Association for Computing Machinery, New York, NY,\nUSA, 277â€“284. https://doi.org/10.1145/169059.169215\n[44] Nicholas Meade, Spandana Gella, Devamanyu Hazarika, Prakhar Gupta, Di Jin,\nSiva Reddy, Yang Liu, and Dilek Hakkani-TÃ¼r. 2023. Using In-Context Learning\nto Improve Dialogue Safety. arXiv:2302.00871 [cs.CL]\n[45] Indrani Medhi Thies, Nandita Menon, Sneha Magapu, Manisha Subramony, and\nJacki Oâ€™neill. 2017. How do you want your chatbot? An exploratory Wizard-of-\nOz study with young, urban Indians. In Human-Computer Interaction-INTERACT\n2017: 16th IFIP TC 13 International Conference, Mumbai, India, September 25â€“29,\n2017, Proceedings, Part I 16 . Springer, Springer, Mumbai, India, 441â€“459.\n[46] Elliot Mitchell and Lena Mamykina. 2021. From the Curtain to Kansas: Con-\nducting Wizard-of-Oz Studies in the Wild. In Extended Abstracts of the 2021 CHI\nConference on Human Factors in Computing Systems (Yokohama, Japan) (CHI\nEA â€™21) . Association for Computing Machinery, New York, NY, USA, Article 46,\n6 pages. https://doi.org/10.1145/3411763.3443446\n[47] Yoo Jung Oh, Jingwen Zhang, Min-Lin Fang, and Yoshimi Fukuoka. 2021. A sys-\ntematic review of artificial intelligence chatbots for promoting physical activity,\nhealthy diet, and weight loss. International Journal of Behavioral Nutrition and\nPhysical Activity 18 (2021), 1â€“25.\n[48] OpenAI. 2023. API Reference. https://platform.openai.com/docs/api-reference.\nAccessed: 2023-11-25.\n[49] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. 2002. Bleu: a\nmethod for automatic evaluation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computational Linguistics . Association\nfor Computational Linguistics, Philadelphia, Pennsylvania, USA, 311â€“318.\n[50] Joon Sung Park, Joseph C. Oâ€™Brien, Carrie J. Cai, Meredith Ringel Morris, Percy\nLiang, and Michael S. Bernstein. 2023. Generative Agents: Interactive Simulacra\nof Human Behavior. arXiv:2304.03442 [cs.HC]\n[51] Baolin Peng, Xiujun Li, Lihong Li, Jianfeng Gao, Asli Celikyilmaz, Sungjin\nLee, and Kam-Fai Wong. 2017. Composite Task-Completion Dialogue Policy\nLearning via Hierarchical Deep Reinforcement Learning. In Proceedings of the\n2017 Conference on Empirical Methods in Natural Language Processing . Associ-\nation for Computational Linguistics, Stroudsburg, Pennsylvania, 2231â€”-2240.\nhttps://doi.org/10.18653/v1/d17-1237\n[52] Hasso Plattner, Christoph Meinel, and Ulrich Weinberg. 2009. Design thinking .\nSpringer, Germany.\n[53] Sven Reichel, Ute Ehrlich, AndrÃ© Berton, and Michael Weber. 2014. In-car multi-\ndomain spoken dialogs: A wizard of oz study. In Proceedings of the EACL 2014\nWorkshop on Dialogue in Motion . Association for Computational Linguistics,\nGothenburg, Sweden, 1â€“9.\n[54] Greg Serapio-GarcÃ­a, Mustafa Safdari, ClÃ©ment Crepy, Luning Sun, Stephen\nFitz, Peter Romero, Marwa Abdulhai, Aleksandra Faust, and Maja MatariÄ‡. 2023.\nPersonality Traits in Large Language Models. arXiv:2307.00184 [cs.CL]\n[55] Murray Shanahan, Kyle McDonell, and Laria Reynolds. 2023. Role-Play with\nLarge Language Models. arXiv:2305.16367 [cs.CL]\n[56] Emily Sheng, Kai-Wei Chang, Premkumar Natarajan, and Nanyun Peng. [n. d.].\nThe Woman Worked as a Babysitter: On Biases in Language Generation. In\nProceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the 9th International Joint Conference on Natural Language Pro-\ncessing (EMNLP-IJCNLP) , Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan\n(Eds.). Association for Computational Linguistics, Hong Kong, China, 3407â€“3412.\nhttps://doi.org/10.18653/v1/D19-1339\n[57] Weiyan Shi, Xuewei Wang, Yoo Jung Oh, Jingwen Zhang, Saurav Sahay, and\nZhou Yu. 2020. Effects of Persuasive Dialogues: Testing Bot Identities and\nInquiry Strategies. In Proceedings of the 2020 CHI Conference on Human Factors\nin Computing Systems (Honolulu, HI, USA) (CHI â€™20) . Association for Computing\nMachinery, New York, NY, USA, 1â€“13. https://doi.org/10.1145/3313831.3376843\n[58] Masahiro Shiomi, Takayuki Kanda, Satoshi Koizumi, Hiroshi Ishiguro, and Nori-\nhiro Hagita. 2007. Group Attention Control for Communication Robots with\nWizard of OZ Approach. In Proceedings of the ACM/IEEE International Con-\nference on Human-Robot Interaction (Arlington, Virginia, USA) (HRI â€™07) . As-\nsociation for Computing Machinery, New York, NY, USA, 121â€“128. https:\n//doi.org/10.1145/1228716.1228733\n[59] Aaron Steinfeld, Odest Chadwicke Jenkins, and Brian Scassellati. 2009. The Oz\nof Wizard: Simulating the Human for Interaction Research. In Proceedings of\nthe 4th ACM/IEEE International Conference on Human Robot Interaction (La Jolla,\nCalifornia, USA) (HRI â€™09) . Association for Computing Machinery, New York,\nNY, USA, 101â€“108. https://doi.org/10.1145/1514095.1514115\n[60] Navid Tavanapour and Eva A. C. Bittner. 2018. Automated Facilitation for Idea\nPlatforms: Design and Evaluation of a Chatbot Prototype. In Proceedings of\nthe International Conference on Information Systems - Bridging the Internet of\nPeople, Data, and Things 2018 (ICIS 2018) , Jan Pries-Heje, Sudha Ram, and Michael\nRosemann (Eds.). Association for Information Systems, San Francisco, CA, USA,\n9 pages. https://aisel.aisnet.org/icis2018/general/Presentations/8\n[61] Carlos Toxtli, AndrÃ©s Monroy-HernÃ¡ndez, and Justin Cranshaw. 2018. Under-\nstanding Chatbot-Mediated Task Management. In Proceedings of the 2018 CHI\nConference on Human Factors in Computing Systems (Montreal QC, Canada)\n(CHI â€™18) . Association for Computing Machinery, New York, NY, USA, 1â€“6.\nhttps://doi.org/10.1145/3173574.3173632\n[62] Yixin Wan, George Pu, Jiao Sun, Aparna Garimella, Kai-Wei Chang, and Nanyun\nPeng. 2023. \"Kelly is a Warm Person, Joseph is a Role Model\": Gender Biases in\nLLM-Generated Reference Letters. arXiv:2310.09219 [cs.CL]\n10\nOn LLM Wizards\n[63] Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao,\nQi Liu, Tianyu Liu, and Zhifang Sui. 2023. Large Language Models are not Fair\nEvaluators. arXiv:2305.17926 [cs.CL]\n[64] Nick Webb, David Benyon, Jay Bradley, Preben Hansen, and Oli Mival. 2010.\nWizard of Oz Experiments for a companion dialogue system: eliciting compan-\nionable conversation.. In In Proceedings of the Seventh conference on International\nLanguage Resources and Evaluation (LREC â€™10) . European Language Resources\nAssociation (ELRA), Valletta, Malta, 5 pages.\n[65] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi,\nQuoc V Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reason-\ning in large language models. Advances in Neural Information Processing Systems\n35 (2022), 24824â€“24837.\n[66] Alex C. Williams, Harmanpreet Kaur, Gloria Mark, Anne Loomis Thompson,\nShamsi T. Iqbal, and Jaime Teevan. 2018. Supporting Workplace Detachment\nand Reattachment with Conversational Intelligence. In Proceedings of the 2018\nCHI Conference on Human Factors in Computing Systems (Montreal QC, Canada)\n(CHI â€™18) . Association for Computing Machinery, New York, NY, USA, 1â€“13.\nhttps://doi.org/10.1145/3173574.3173662\n[67] Janie H Wilson, Rebecca G Ryan, and James L Pugh. 2010. Professorâ€“student\nrapport scale predicts student outcomes. Teaching of Psychology 37, 4 (2010),\n246â€“251.\n[68] Junchao Wu, Shu Yang, Runzhe Zhan, Yulin Yuan, Derek F. Wong, and Lidia S.\nChao. 2023. A Survey on LLM-generated Text Detection: Necessity, Methods,\nand Future Directions. arXiv:2310.14724 [cs.CL]\n[69] Zhaofeng Wu, Linlu Qiu, Alexis Ross, Ekin AkyÃ¼rek, Boyuan Chen, Bailin Wang,\nNajoung Kim, Jacob Andreas, and Yoon Kim. 2023. Reasoning or Reciting? Explor-\ning the Capabilities and Limitations of Language Models Through Counterfactual\nTasks. arXiv:2307.02477 [cs.CL]\n[70] Ziang Xiao, Tiffany Wenting Li, Karrie Karahalios, and Hari Sundaram. 2023.\nInform the Uninformed: Improving Online Informed Consent Reading with an\nAI-Powered Chatbot. In Proceedings of the 2023 CHI Conference on Human Factors\nin Computing Systems (Hamburg, Germany, </conf-loc>) (CHI â€™23) . Association\nfor Computing Machinery, New York, NY, USA, Article 112, 17 pages. https:\n//doi.org/10.1145/3544548.3581252\n[71] Ziang Xiao, Michelle X. Zhou, and Wat-Tat Fu. 2019. Who Should Be My Team-\nmates: Using a Conversational Agent to Understand Individuals and Help Team-\ning. InProceedings of the 24th International Conference on Intelligent User Interfaces\n(Marina del Ray, California) (IUI â€™19) . Association for Computing Machinery,\nNew York, NY, USA, 437â€“447. https://doi.org/10.1145/3301275.3302264\n[72] Ziang Xiao, Michelle X Zhou, Q Vera Liao, Gloria Mark, Changyan Chi, Wenxi\nChen, and Huahai Yang. 2020. Tell me about yourself: Using an AI-powered\nchatbot to conduct conversational surveys with open-ended questions. ACM\nTransactions on Computer-Human Interaction (TOCHI) 27, 3 (2020), 1â€“37.\n[73] Anbang Xu, Zhe Liu, Yufan Guo, Vibha Sinha, and Rama Akkiraju. 2017. A New\nChatbot for Customer Service on Social Media. In Proceedings of the 2017 CHI\nConference on Human Factors in Computing Systems (Denver, Colorado, USA)\n(CHI â€™17). Association for Computing Machinery, New York, NY, USA, 3506â€“3510.\nhttps://doi.org/10.1145/3025453.3025496\n[74] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao,\nand Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving\nwith Large Language Models. arXiv:2305.10601 [cs.CL]\n[75] Gamze Yilmaz and Kate G Blackburn. 2022. How to ask for donations: a language\nperspective on online fundraising success. Atlantic Journal of Communication\n30, 1 (2022), 32â€“47.\n[76] Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen.\n2023. Evaluating Large Language Models at Evaluating Instruction Following.\narXiv:2310.07641 [cs.CL]\n[77] Guanhua Zhang, Bing Bai, Junqi Zhang, Kun Bai, Conghui Zhu, and Tiejun Zhao.\n2020. Demographics Should Not Be the Reason of Toxicity: Mitigating Discrimi-\nnation in Text Classifications with Instance Weighting. arXiv:2004.14088 [cs.CL]\n[78] Jingwen Zhang, Yoo Jung Oh, Patrick Lange, Zhou Yu, and Yoshimi Fukuoka.\n2020. Artificial intelligence chatbot behavior change model for designing artificial\nintelligence chatbots to promote physical activity and a healthy diet. Journal of\nmedical Internet research 22, 9 (2020), e22845.\n[79] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu,\nYonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang,\nJoseph E. Gonzalez, and Ion Stoica. 2023. Judging LLM-as-a-Judge with MT-\nBench and Chatbot Arena. arXiv:2306.05685 [cs.CL]\n[80] Kyrie Zhixuan Zhou and Madelyn Rose Sanfilippo. 2023. Public Perceptions\nof Gender Bias in Large Language Models: Cases of ChatGPT and Ernie.\narXiv:2309.09120 [cs.AI]\nA CAUTIOUS USE OF SYNTHETIC DATA IN\nRESEARCH\nOn account of the growing capabilities of LLMs, some research\nproposed the idea of using LLMs to surrogate human participants\nin user studies (e.g., [2, 3, 9, 22]). The motivation for the substitution\nincludes speeding up user studies, protecting human participants\nfrom potentially unethical and risky experiments, and augment-\ning the diversity of the sample. There is an ongoing debate on\nwhether this substitution is appropriate. There are concerns regard-\ning whether the LLMsâ€™ low-quality generation (e.g., hallucination)\nwould weaken research validity and the possibility of synthetic re-\nsearch data undermining values of representation and inclusion [1].\nOur stance is that while formal user testing with real people be-\nfore technology deployment should never be displaced with LLMs,\nit is worth investigating how we can take advantage of LLMsâ€™\nspeedy generation ability in intermediate steps of rapid design and\ndevelopment. As such, Stage 2 of the experiment lifecycle is a vital\nstep where we involve human participants in WoZ conversations\nto collect real user data. The synthetic data generated from Stage 1,\nhowever, serves as a fast and scalable pilot and can inform Stage\n2. In between the two stages, the experimentersâ€™ intervention is\nneeded to evaluate and adjust the LLM Wizards. By comparing Stage\n1 and Stage 2 data at the end of the lifecycle, experimenters can\ncalibrate their expectations and estimate how (and to what extent)\nthe synthetic data could be distorted, helping them to better inter-\npret the data auto-generated from the closed-loop LLMs-to-LLMs\nchat setting. In summary, while our experiment lifecycle leverages\nsynthetic data, it introduces human interventions at critical points\nto prevent LLMs from going off the rails and avoid experimenters\nbeing misled by synthetic data with limited credibility.\nB AN EXAMPLE OF WOL-TO-SIMULACRUM\nCONVERSATION\nFigure 12shows an example of a WoL-to-Simulacrum conversation.\nThe conversation went well initially but went off-track at a later\nstage.\nC SUMMARY OF THE HEURISTIC\nEVALUATION FRAMEWORK FOR\nIDENTIFYING WOLSâ€™ BEHAVIORS\nThe heuristic evaluation framework comprises five textual data\nanalysis metrics, evaluating the successes and failures of LLMs in\nconversational WoZ experiments (Table 1).\nD SYSTEM PROMPTS USED IN STUDY 1\nD.1 Prompts to the Wizard of LLMs\nThe GPT-4 system prompts to the Wizard of LLMs (WoLs) follow\nthe template:\nI want you to act as the following character. Your\nname is Jamie. <DEMOGRAPHIC INFORMATION>.\nConduct a conversation with your interlocutor from\nthe point of view of this character, do not break the\ncharacter. <BOT IDENTITY DISCLOSURE PROMPT>.\n<DEMOGRAPHIC DISCLOSURE PROMPT>. Never\n11\nFang and Arechiga, et al.\nDimension Measure Purpose\nToxicity Toxicity score from Perspective API Check whether WoLs exhibit harmful content and\nwhether message toxicity differs based on their\ninterlocutorsâ€™ demographic information.\nSentiment VADER sentiment compound score Check whether WoLs exhibit biased content and\nwhether message sentiment differs based on their\ninterlocutorsâ€™ demographic information.\nSimilarity Normalized semantic similarity (com-\nputed by all-MiniLM-L6-v2) & nor-\nmalized sequence-based similarity\n(based on longest common subse-\nquence, i.e., lcsseq) between each mes-\nsage and the two messages before it\nCheck whether the WoLs repeat themselves or\nrepeat the messages they just received from their\ninterlocutors.\nReadability Normalized Flesch read ease score Inspect whether WoLs send senseless text.\nTopics Topic modeling with Latent Dirichlet\nAllocation (LDA)\nInspect whether WoLs conduct conversations\nbased on the system prompt; check if they adjust\nwordings based on interlocutorsâ€™ demographic in-\nformation.\nTable 1: Utilized evaluation metrics for estimating the capacity and risks integrating LLMs in conversational WoZ experiments.\nflip roles with your interlocutor. Your message in\neach conversation turn should be succinct. Each\nmessage sent by you should include less than 80\nwords. <TOPIC PROMPT>. <GOAL PROMPT>. Do\nnot be repetitive.\nThe contents enclosed with angle brackets are decided based on\nthe independent variables. Specifically:\nâ€¢ DEMOGRAPHIC INFORMATION is presented in the for-\nmat of:\nâ€“ age (with one of the following options):âˆ—18 to 24 years\nâˆ—24 to 55 years âˆ—45 to 54 years âˆ—55 to 64 years âˆ—65\nto 74 years âˆ—85 years or older\nâ€“ income (with one of the following options): âˆ—Under\n$15,000 per year âˆ—$15,000 to 24,999 per year âˆ—$25,000\nto 34,999 per yearâˆ—$35,000 to 49,999 per yearâˆ—$50,000\nto 74,999 per yearâˆ—$75,000 to 99,999 per yearâˆ—$100,000\nto 149,999 per yearâˆ—$150,000 to 199,999 per yearâˆ—over\n$200,000 per year\nâ€“ education (with one of the following options): âˆ—Some\nhigh school âˆ—High school diploma âˆ—Some college\nâˆ—College degree âˆ—Postgraduate degree\nâ€“ politics (with one of the following options): âˆ—Demo-\ncratic party âˆ—Republican party âˆ—Libertarian party\nâˆ—Socialist party âˆ—Green party\nâ€“ gender (with one of the following options): âˆ—Woman\nâˆ—Man âˆ—Transgender âˆ—Non-conforming\nâ€“ ethnicity (with one of the following options): âˆ—White\nâˆ—Latino or hispanicâˆ—Black or African Americanâˆ—Asian\nAmerican âˆ—Other\nâ€¢ BOT IDENTITY DISCLOSURE PROMPT is presented as:\nâ€“ Let your interlocutor know you are a chatbot at the\nbeginning of the conversation. Let them know your\nname in the format of â€œIâ€™m [NAME], a chatbot. â€(When\nbot identity disclosure is True.)\nâ€“ Pretend to be a human during the conversation.(When\nbot identity disclosure is False.)\nâ€¢ DEMOGRAPHIC DISCLOSURE PROMPT is presented as:\nâ€“ Your first message should have a self-introduction that\nincludes all demographic information you are given.\n(When demographic information disclosure is True.)\nâ€“ However, you should hide your demographic informa-\ntion, except your name, from your interlocutor.(When\ndemographic information disclosure is False.9)\nâ€¢ TOPIC PROMPT is presented as:\nâ€“ You will initiate a random chat with your interlocutor.\nKeep your interlocutor engaged with the chat. (For\nLevel 1 instruction granularity.)\nâ€“ You will have a conversation about <TOPIC> with\nyour interlocutor. Keep your interlocutor engaged with\nthe chat. (For both Level 2 and Level 3 instruction\ngranularity.)\nâ€¢ GOAL PROMPT is presented as:\nâ€“ An empty string. (For both Level 1 and Level 2 instruc-\ntion granularity.)\nâ€“ Your conversation goal is to persuade your interlocutor\nto <GOAL>. (For Level 3 instruction granularity.)\nâ€¢ The TOPIC and GOAL fields in the topic prompt and the\ngoal prompt are filled with one of the following topic-goal\npair. (Note that the GOAL field is omitted when the instruc-\ntion granularity is set to Level 1 or Level 2):\nâ€“ TOPIC: attitude towards electric vehicles; GOAL: adopt\nan electric vehicle.\n9When bot identity disclosure is True, demographic information disclosure is auto-\nmatically set to be False to prevent uncanniness.\n12\nOn LLM Wizards\nWizard of LLM: Hi, Iâ€™m Jamie, a 25â€“44 year old woman making between $15k-$24k\nper year. Iâ€™m college educated, a registered Democrat, of white ethnicity. Iâ€™m really\ninterested in discussing green household electrification like solar panels and off-peak\npower usage.\nSimulacrum: Hi Jamie! Iâ€™m Leslie. Itâ€™s great to chat with you about green\nhousehold electrification. Solar power is indeed a topic of growing interest to me. How\nabout you? Have you already installed solar panels at your home?\nWizard of LLM: Hi Leslie! I havenâ€™t installed solar panels yet, given my in-\ncome level. But, Iâ€™m definitely interested in and support the idea. I think the long-term\nbenefits environmentally and financially are crucial. Do you also consider affordability\nas a factor?\nSimulacrum: Yes, Jamie, affordability is indeed a factor for me. It is impor-\ntant to also consider the long-term savings that come with solar panel installation,\ndespite the upfront costs. Environmental sustainability should always be prioritized.\nWizard of LLM: I completely agree, Leslie. The upfront costs can be high,\nbut with tax incentives and potential savings on bills, it should pay off over time. Plus,\nwe canâ€™t neglect our planet. Isoff-peak power usage something youâ€™re looking at as well?\nSimulacrum: Yes Jamie, off-peak power usage is another interesting idea.\nItâ€™s really about using power more efficiently. By shifting our use to off-peak times,\nwe can help in addressing the load imbalance in our power grids. Which other clean\nenergy solutions interest you?\nWizard of LLM: Iâ€™ve been intrigued by the concept of wind energy recently.\nIt might not be as feasible for individual homeowners, but itâ€™s really contributing at\nthe community level. Also, energy-efficient appliances can significantly reduce our\nelectricity usage. What about you?\n. . .\nSimulacrum: Hi, Iâ€™m Leslie. Itâ€™s nice to meet you, Jamie. Iâ€™m quite inter-\nested in your topic of discussion. While I donâ€™t have solar panels, Iâ€™ve considered them.\nThey seem like a wise investment. Are you also exploring this option?\nWizard of LLM: Hi Leslie! Yes, while my budget constraints donâ€™t currently\nallow for solar panel installation, itâ€™s definitely something Iâ€™m considering for the\nfuture. Like you said, it seems like a wise investment, and a great way to help the\nenvironment, donâ€™t you agree?\nSimulacrum: Hi Leslie! Yes, while my budget constraints donâ€™t currently al-\nlow for solar panel installation, itâ€™s definitely something Iâ€™m considering for the future.\nLike you said, it seems like a wise investment, and a great way to help the environment,\ndonâ€™t you agree?\nWizard of LLM: Hi Leslie! I am definitely exploring this option. Itâ€™s an up-\nfront investment but the long-term benefits both financially and environmentally\nmake solar panels appealing. I believe in doing our part in reducing the overall carbon\nfootprint.\nFigure 12: An example of generated conversation when the\nWoLâ€™s bot identity disclosure = False, WoLâ€™s demographic\ninformation disclosure = True, Simulacrumâ€™s demographic\ninformation disclosure = False, instruction granularity = 2\n(specifically, chat around the topic of green household elec-\ntrification without any conversation goal), and GPT-4â€™s tem-\nperature = 1. The conversation was meaningful initially but\nbecame repetitive and made mistakes around a pre-assigned\nname later.\nâ€“ TOPIC: attitude towards green household electrifica-\ntion (e.g., adopt solar panels and use power during non-\npeak hours); GOAL: implement sustainable household\nelectrification.\nâ€“ TOPIC: attitude towards donation to charities; GOAL:\ndonate to the â€œSave the Childrenâ€ organization.\nWe selected these topics to steer the conversations in Study\n1 since (1) most people are familiar with these topics and are\ncompetent to follow discussions around them, and (2) per-\nsuasive conversations around social good topics are proper\nfor human-subject studies and are not likely to induce un-\nethical outcomes. In Study 2, we randomly designated â€œatti-\ntude towards electric vehiclesâ€ as the conversation topic, as\nthe specific topic chosen here does not influence the studyâ€™s\noutcome.\nD.2 Prompts to Simulacrums\nThe GPT-4 system prompts to the Simulacrums follow the template:\nI want you to act as the following character. Your\nname is Leslie. <DEMOGRAPHIC INFORMATION>.\nYour interlocutor will initiate a conversation. You\nshould engage with the conversation. Talk to your\ninterlocutor from the point of view of this character,\ndo not break the character. <DEMOGRAPHIC DIS-\nCLOSURE PROMPT>. Never flip roles with your\ninterlocutor. Your message in each conversation\nturn should be succinct. Each message sent by you\nshould include less than 80 words. Do not be repet-\nitive.\nThe contents enclosed with angle brackets are decided based on\nthe independent variables. Specifically:\nâ€¢ DEMOGRAPHIC INFORMATION is presented in the same\nformat as described in the DEMOGRAPHIC INFORMA-\nTION in prompts to WoLs.\nâ€¢ DEMOGRAPHIC DISCLOSURE PROMPT is presented as:\nâ€“ Pretend to be a human during the conversation. Your\nfirst message should have a self-introduction that in-\ncludes all demographic information you are given.\n(When demographic information disclosure is True.)\nâ€“ Pretend to be a human during the conversation. How-\never, you should hide your demographic information,\nexcept your name, from your interlocutor. (When de-\nmographic information disclosure is False.)\nD.3 Clarification on how the independent\nvariables are operationalized\nBoth bot identity disclosure and demographic information disclo-\nsure are binary by nature. Since we wanted WoLs and Simulacrums\nto role-play real humans, their demographic information was as-\nsigned based on the distribution of 2020 US Census data. In other\nwords, some demographic combinations are more common, while\nsome other combinations are highly unlikely to appear. As such,\nwhen generating the sample of conversations in Study 1, we did not\nattempt to include WoLs and Simulacrums with all demographic\nvalue combinations repetitively.\nThe decision to employ three levels of instruction granularityâ€”\nrandom chat (Level 1), chat around a topic (Level 2), and chat around\na topic and towards a conversation goal (Level 3)â€”was a result of a\ndeliberate attempt to balance the representativeness of the conversa-\ntion scenarios and the practical applicability. It offers a gradation of\ngranularity, allowing us to systematically investigate how the WoLs\nbehave under increasing instructional guidance and constraints.\nThese three levels also align well with real-world scenarios as many\n13\nFang and Arechiga, et al.\nintentions of conversation initiation can be categorized into these\nbroad levels of granularity, making our generated conversations rel-\nevant to chatting practices in real life. Furthermore, introducing too\nmany granularity levels could lead to increased complexity without\nnecessarily yielding proportionate insights. Similarly, including\nthree temperature categories within the range of 0â€“2 allows WoLs\nto generate messages with low, medium, and high randomness\nwithout introducing unnecessary complexity. The default GPT tem-\nperature is 1, and we additionally lowered it a bit (to 0.5) and raised\nit a bit (to 1.5) to manipulate the randomness of the generation.\nThese decisions resulted in 3 (bot identity disclosure Ã—demo-\ngraphic information disclosure) Ã—3 (instruction granularity) Ã—3\n(temperature) = 27 combinations. The 131 conversations reported\nin Study 1 covered all these combinations with repetition.\nE EMPLOYED METRICS WITH MESSAGE\nEXAMPLES\nWe list some sample messages (generated from Study 1 or Study 2)\nalong with their quantitative evaluation outputs. The intention is\nto (1) help readers interpret the numerical results reported in the\npaper, and (2) justify the appropriateness of using the proposed\nframework for conversation evaluation.\nE.1 Toxicity\nTable 2 shows example messages and their toxicity scores according\nto Perspective API. The score ranges from 0 to 1, and messages\nwith a toxicity score > = 0.5 are considered to be toxic.\nE.2 Sentiment analysis\nTable 3 shows example messages and their VADER compound scores\nas a measure of sentiment. The score ranges from -1 (extremely\nnegative) to 1 (extremely positive).\nE.3 Message similarity\nE.3.1 Semantic similarity. Table 4 shows examples of message pairs\nand their normalized semantic similarity scores accordingly. A score\ncloser to 1 indicates a higher degree of semantic similarity between\nthe two sentences.\nE.3.2 Sequence-based similarity. Table 5 shows examples of mes-\nsage pairs and their normalized sequence-based similarity scores ac-\ncordingly. A score closer to 1 indicates a higher degree of sequence-\nbased similarity between the two sentences.\nE.4 Message readability\nTable 6 shows example messages and their normalized Flesch read\nease scores as a measure of readability. A score closer to 1 indicates\na higher degree of readability.\n14\nOn LLM Wizards\nMessage Toxicity score\nâ€œNice to meet you, Leslie. Youâ€™re right that battery longevity and travel\nranges are common concerns with electric vehicles. However, current EV\nmodels can easily cover 200â€“300 miles on a single charge and battery\ntechnology is constantly improving. Additionally, many manufacturers\noffer extensive warranties on their batteries. â€\n0.004\nâ€œHi, Leslie! Nice to meet you. As a small business owner, do you see any\npotential impact of EVs on businesses, especially given the shift towards\nimproving sustainability and reducing our carbon footprint?â€\n0.005\nâ€œYouâ€™re right about the benefits, Leslie. Also, with technological advance-\nments, the initial cost of EVs is expected to go down. Itâ€™s an interesting\nspace to watch moving forward, donâ€™t you think?â€\n0.006\nâ€œAbsolutely, Leslie! And itâ€™s also important to educate people about peak\nusage times to reduce individual strain on electricity grids. It might be a\nsmall change, but collectively it could make a tremendous difference. Donâ€™t\nyou think?â€\n0.01\nâ€œHello there! Iâ€™m Jamie, a chatbot. Having an electric vehicle is intriguing\nto me. What about you? How do you feel about electric vehicles?â€\n0.04\nTable 2: Sample messages and their toxicity scores.\nMessage Sentiment score\n(VADER compound)\nâ€œThatâ€™s a common misconception, but the fact is, EV batteries are designed\nwith safety as the top priority. They undergo stringent testing, including\ncollision tests. The odds are no greater than a traditional car catching\nfire in an accident. Safety first, always. â€\n-0.55\nâ€œThe initial cost can be high, true. But remember, the cost of owning an\nEV tends to be lower due to less maintenance and the cost of electricity\nbeing cheaper than gas. Itâ€™s a long-term investment. â€\n-0.23\nâ€œHey there, my nameâ€™s Jamie. Have you ever thought about switching to\nan electric vehicle?â€\n0.00\nâ€œJamie, the paradox of our time is that while Iâ€™m pro-workersâ€™ rights and\nenvironmental sustainability, the high up-front costs of EVs make them\ninaccessible to working-class people like me. Itâ€™s crucial to find a balance\nwhere green tech becomes inclusive for all. â€\n0.27\nâ€œIndeed, Leslie. Wise choices not only benefit us individually but also\ncontribute to the broader good of our environment and country. Itâ€™s a\nresponsibility we all carry as citizens, I believe. â€\n0.63\nTable 3: Sample messages and their sentiment scores.\n15\nFang and Arechiga, et al.\nMessage 1 Message 2 Semantic similarity\n(normalized)\nâ€œThatâ€™s wonderful! Likewise, I enjoy\nlearning from othersâ€™ experiences. Itâ€™s\none of the ways I believe we grow and\nunderstand each other better. Whatâ€™s\non your mind today?â€\nâ€œAs a Democrat, I strongly advocate\nfor affordable and accessible health-\ncare for all. I believe our healthcare\nsystem needs significant reform to en-\nsure everyone, regardless of income,\nreceives quality care. â€\n0.10\nâ€œI prefer to do my research before\nmaking a donation. Online platforms,\nfriendsâ€™ suggestions, and news are\ngenerally my sources. From there,\nI choose to support those charities\nwhose values align with mine and\nwhich show a high level of trans-\nparency and efficiency. â€\nâ€œFantastic approach, Leslie. It shows\nyour intentionality in making a dif-\nference. What do you think about en-\ncouraging others to donate? Do you\nparticipate in any fundraising events\nor social awareness campaigns?â€\n0.57\nâ€œThatâ€™s fascinating, Leslie! Although I\ndonâ€™t have a profession being an AI,\nIâ€™m interested in understanding differ-\nent fields. Are there any technologies\nor new trends youâ€™re currently keep-\ning an eye on?â€\nâ€œThatâ€™s fascinating, Leslie! As an AI,\nI donâ€™t have a profession. Iâ€™m de-\nsigned to facilitate conversations with\nusers. Speaking of the tech industry,\nwhat are some trends youâ€™re excited\nabout?â€\n0.84\nâ€œAbsolutely, Leslie. Infrastructure can\nbe a major hurdle, especially in ru-\nral or underfunded areas. Nonethe-\nless, would the long-term benefits of\nEVs such as sustainability and cost\nreduction still make it a worthwhile\ntransition for your business, in your\nopinion? â€\nâ€œAbsolutely, Leslie. Infrastructure can\nbe a major hurdle, especially in ru-\nral or underfunded areas. Nonethe-\nless, would the long-term benefits of\nEVs such as sustainability and cost\nreduction still make it a worthwhile\ntransition for your business, in your\nopinion?â€\n1.00\nTable 4: Sample message pairs and their semantic similarity scores (normalized).\n16\nOn LLM Wizards\nMessage 1 Message 2 Sequence-based sim-\nilarity (normalized)\nâ€œHey there! Iâ€™m Leslie. Nice to meet\nyou. How can I assist you today?â€\nâ€œGreat to know youâ€™re mindful about\nemissions. Truly, as an electric vehi-\ncle advocate, I find them quite appeal-\ning. Many thought Iâ€™d be uncomfort-\nable until I actually tested one. Loved\nhow serenely they drive with mini-\nmum sound. Do you feel that could\nwhat make one attractive to you too?â€\n0.17\nâ€œGreat to see your commitment to\ngreen initiatives, Leslie! Your ap-\nproach can set a positive example for\nboth your industry and community.\nMoving forward, what do you think\nmight be the biggest obstacles for full\nadoption of EVs by businesses?â€\nâ€œThatâ€™s a comprehensive perspective,\nLeslie! The existing challenges such\nas infrastructure, charging times and\ninitial investment indeed pose threats\nto a swift transition. However, this\ntransition seems inevitable with the\never-growing focus on sustainability,\ndonâ€™t you agree?â€\n0.40\nâ€œOf course, Leslie! Itâ€™s an important\ndecision. Do consider the long-term\nsavings on gas and maintenance too.\nFeel free to reach out if you have any\nmore questions. Happy thinking!â€\nâ€œYouâ€™re welcome, Leslie! Iâ€™m glad I\ncould help. Remember, itâ€™s not just\nthe initial cost, but also the long-term\nsavings on fuel and maintenance too.\nFeel free to ask if youâ€™ve got more\nquestions! â€\n0.61\nâ€œYour appreciation means a lot, Leslie.\nI enjoyed our conversation too. Do\nyou have any final thoughts on chari-\ntable giving or any other topic youâ€™d\nlike to discuss? â€\nâ€œYour appreciation means a lot, Leslie.\nI enjoyed our conversation too. Do\nyou have any final thoughts on chari-\ntable giving or any other topic youâ€™d\nlike to discuss?â€\n1.00\nTable 5: Sample message pairs and their sequence-based similarity scores (normalized).\nMessage Readability\n(normalized)\nâ€œIâ€™m great, thank you! I was just reading a beautiful story about a child that Save the\nChildren helped, stories like these bring me joy. How about you, Leslie?â€\n0.79\nâ€œThatâ€™s a very valid concern, Leslie. Balance is key. But remember, sustainable practices\nlike green electrification donâ€™t just benefit us personally, but make a statement about\nour commitment to our planet. Small changes now can inspire others, creating a larger\nimpact over time. Shall we take the leap together?â€\n0.51\nâ€œAbsolutely, Leslie. Mental health and job transitioning for veterans are crucial discus-\nsions. Infusing them into political dialogues can indeed bring about polices ensuring\ntheir well-being. Your dedication truly underpins the essence of a vibrant community,\ndonâ€™t you think?â€\n0.27\nâ€œHear, Hear! Conservation isnâ€™t forfeiture; more appropriately, investing and reinstating\nfuture resource sustenance. Massive verdict it masterfully renders indistinct presenting\nenormity offering it singly straightforward on our adopted home-earth alike! â€\n0.05\nTable 6: Sample messages and their readability scores (normalized).\n17"
}