{
    "title": "Phraseformer: Multimodal Key-phrase Extraction using Transformer and Graph Embedding",
    "url": "https://openalex.org/W3170502199",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5084161457",
            "name": "Narjes Nikzad-Khasmakhi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5056445813",
            "name": "Mohammad‐Reza Feizi‐Derakhshi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5082152327",
            "name": "Meysam Asgari-Chenaghlu",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5089205977",
            "name": "Mohammad Ali Balafar",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5064122350",
            "name": "Ali-Reza Feizi-Derakhshi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5113591739",
            "name": "Taymaz Rahkar-Farshi",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5103118440",
            "name": "Majid Ramezani",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5041631864",
            "name": "Zoleikha Jahanbakhsh-Nagadeh",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5069053260",
            "name": "Elnaz Zafarani-Moattar",
            "affiliations": [
                null
            ]
        },
        {
            "id": "https://openalex.org/A5075419665",
            "name": "Mehrdad Ranjbar-Khadivi",
            "affiliations": [
                null
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1525595230",
        "https://openalex.org/W2886790750",
        "https://openalex.org/W2962756421",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2100796029",
        "https://openalex.org/W2562255476",
        "https://openalex.org/W2141222516",
        "https://openalex.org/W3106231035",
        "https://openalex.org/W2133286915",
        "https://openalex.org/W1983873791",
        "https://openalex.org/W3156333129",
        "https://openalex.org/W1486781940",
        "https://openalex.org/W3199252595",
        "https://openalex.org/W2963245897",
        "https://openalex.org/W2060772621",
        "https://openalex.org/W2102733276",
        "https://openalex.org/W2251295945",
        "https://openalex.org/W2974528752",
        "https://openalex.org/W3104097132",
        "https://openalex.org/W331312376",
        "https://openalex.org/W2804950764",
        "https://openalex.org/W2037124106",
        "https://openalex.org/W2950577311",
        "https://openalex.org/W2103479483",
        "https://openalex.org/W2029729099",
        "https://openalex.org/W2778118789",
        "https://openalex.org/W2979977993",
        "https://openalex.org/W3007728180",
        "https://openalex.org/W2767253343",
        "https://openalex.org/W2914076857",
        "https://openalex.org/W2724776846",
        "https://openalex.org/W2980598790",
        "https://openalex.org/W2250240865",
        "https://openalex.org/W2251786111",
        "https://openalex.org/W2062233052",
        "https://openalex.org/W3137799364",
        "https://openalex.org/W2953722276",
        "https://openalex.org/W2064418625",
        "https://openalex.org/W2740811004",
        "https://openalex.org/W3042512094",
        "https://openalex.org/W2985875790",
        "https://openalex.org/W3012813596",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W2962903510",
        "https://openalex.org/W2960444417",
        "https://openalex.org/W2147880316",
        "https://openalex.org/W2962798918",
        "https://openalex.org/W2973226110",
        "https://openalex.org/W2995811994",
        "https://openalex.org/W3099568161",
        "https://openalex.org/W2943765965",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W2996487878",
        "https://openalex.org/W2250954789",
        "https://openalex.org/W2962739339"
    ],
    "abstract": "Background: Keyword extraction is a popular research topic in the field of natural language processing. Keywords are terms that describe the most relevant information in a document. The main problem that researchers are facing is how to efficiently and accurately extract the core keywords from a document. However, previous keyword extraction approaches have utilized the text and graph features, there is the lack of models that can properly learn and combine these features in a best way. Methods: In this paper, we develop a multimodal Key-phrase extraction approach, namely Phraseformer, using transformer and graph embedding techniques. In Phraseformer, each keyword candidate is presented by a vector which is the concatenation of the text and structure learning representations. Phraseformer takes the advantages of recent researches such as BERT and ExEm to preserve both representations. Also, the Phraseformer treats the key-phrase extraction task as a sequence labeling problem solved using classification task. Results: We analyze the performance of Phraseformer on three datasets including Inspec, SemEval2010 and SemEval 2017 by F1-score. Also, we investigate the performance of different classifiers on Phraseformer method over Inspec dataset. Experimental results demonstrate the effectiveness of Phraseformer method over the three datasets used. Additionally, the Random Forest classifier gain the highest F1-score among all classifiers. Conclusions: Due to the fact that the combination of BERT and ExEm is more meaningful and can better represent the semantic of words. Hence, Phraseformer significantly outperforms single-modality methods.",
    "full_text": "Phraseformer: Multimodal Key-phrase Extraction using Transformer and\nGraph Embedding\nN. Nikzad–Khasmakhia, Mohammad–Reza Feizi–Derakhshia,∗, Meysam Asgari-Chenaghlua, M. A. Balafarb,\nAli-Reza Feizi-Derakhshia, Taymaz Rahkar-Farshia,c, Majid Ramezania, Zoleikha Jahanbakhsh-Nagadeha,d,\nElnaz Zafarani-Moattara,e, Mehrdad Ranjbar-Khadivia,f\naComputerized Intelligence Systems Laboratory, Department of Computer Engineering, University of Tabriz, Tabriz, Iran\nbDepartment of Computer Engineering, University of Tabriz, Tabriz, Iran\ncDepartment of Software Engineering, Ayvansaray University, Istanbul, Turkey\ndDepartment of Computer Engineering, Naghadeh Branch, Islamic Azad University, Naghadeh, Iran\neDepartment of Computer Engineering, Tabriz Branch, Islamic Azad University, Tabriz, Iran\nfDepartment of Computer Engineering, Shabestar Branch, Islamic Azad University, Shabestar, Iran.\nAbstract\nBackground Keyword extraction is a popular research topic in the ﬁeld of natural language processing.\nKeywords are terms that describe the most relevant information in a document. The main problem that\nresearchers are facing is how to eﬃciently and accurately extract the core keywords from a document.\nHowever, previous keyword extraction approaches have utilized the text and graph features, there is the lack\nof models that can properly learn and combine these features in a best way.\nMethods In this paper, we develop a multimodal Key-phrase extraction approach, namelyPhraseformer,\nusing transformer and graph embedding techniques. In Phraseformer, each keyword candidate is presented\nby a vector which is the concatenation of the text and structure learning representations. Phraseformer takes\nthe advantages of recent researches such as BERT and ExEm to preserve both representations. Also, the\nPhraseformer treats the key-phrase extraction task as a sequence labeling problem solved using classiﬁcation\ntask.\nResults We analyze the performance of Phraseformer on three datasets including Inspec, SemEval2010\nand SemEval 2017 by F1-score. Also, we investigate the performance of diﬀerent classiﬁers on Phraseformer\nmethod over Inspec dataset. Experimental results demonstrate the eﬀectiveness of Phraseformer method\nover the three datasets used. Additionally, the Random Forest classiﬁer gain the highest F1-score among all\nclassiﬁers.\nConclusionsDue to the fact that the combination of BERT and ExEm is more meaningful and can better\nrepresent the semantic of words. Hence, Phraseformer signiﬁcantly outperforms single-modality methods.\nKeywords: Multimodal representation learning, Keyword extraction, Transformer, Graph embedding\n∗Corresponding author\nEmail addresses: n.nikzad@tabrizu.ac.ir (N. Nikzad–Khasmakhi ),mfeizi@tabrizu.ac.ir (Mohammad–Reza\nPreprint submitted to Journal of June 10, 2021\narXiv:2106.04939v1  [cs.CL]  9 Jun 2021\n1. Introduction\nThere is a phenomenal growth in the quantity of text documents available to users on the diﬀerent social\nmedias. Hence, it is vital for eﬃcient and eﬀective ways to retrieve and summarize all these documents [1].\nKeyword or key-phrase extraction is a solution that identiﬁes a set of the terms that conclude the main idea\nof a document [2]. This process helps readers to quickly perceive the content of a document. Key-phrase\nextraction methods can be eﬀectively used by many text mining applications such as indexing, visualization,\nsummarization, topic detection and tracking, clustering and classiﬁcation [3, 4].\nA number of studies have been conducted to tackle the key-phrase extraction issue. Some of these\napproaches are textual models that focus only the content of the document to obtain keywords. A group\nof one have used lexical and syntactic analyses. Other approaches of this class take the advantages of\nnumerical statistics such as term frequency (TF) or term frequency-inverse document frequency (TF-IDF)\n[5]. On the other hand, graph-based approaches create a various group of methods by constructing a graph\nof words. In this class, the most central nodes illustrates keywords [1]. Also, we can ﬁnd hybrid models\nthat uses the combination of textual and graph-based methods to select keywords. In any case, the topic of\nwhich combination method is the most suitable and powerful for catching key-phrases extraction is open for\ndiscussion.\nIn this paper, we aim to propose a combined method of a graph-based model and a textual model. Also,\nwe view the key-phrases extraction task as classiﬁcation and sequence tagging problems. After pre-processing\npart that removes the stop words and punctuation mark, we construct an undirected and unweighted\nco-occurrence graph for all documents in the corpus. Then, we use three graph embedding techniques\nconsisting ExEm, Node2vec and DeepWalk to learn structure representations of words. On the other hand,\neach single document is injected to BERT Transformer to get the word embedding. In the next step, we\nconcatenate two embeddings obtained from the graph and the content of the document to create a single\nrepresentation for each word. After that, we formulate the key-phrase extraction as sequence labeling task\nthat assigns the BIO tagging to each word in a document. Also, our proposed method treats the sequence\nlabeling as a classiﬁcation task. The output of the classiﬁcation is a BIO tag where a word is at the beginning\n(B) of a key-phrase, inside (I) of a key-phrase and outside of a key-phrase (O). The major contributions of\nthis paper can be summarized as follows:\n• To the best of our knowledge, this work is the ﬁrst to apply a multimodal approach to extract key-phrases\nusing Transformer and graph embedding techniques.\nFeizi–Derakhshi ),m.asgari@tabrizu.ac.ir (Meysam Asgari-Chenaghlu),balafarila@tabrizu.ac.ir (M. A. Balafar),\nderakhshi96@ms.tabrizu.ac.ir (Ali-Reza Feizi-Derakhshi),taymazfarshi@ayvansaray.edu.tr (Taymaz Rahkar-Farshi),\nm_ramezani@tabrizu.ac.ir (Majid Ramezani),zoleikha.jahanbakhsh@srbiau.ac.ir (Zoleikha Jahanbakhsh-Nagadeh),\ne.zafarani@iaut.ac.ir (Elnaz Zafarani-Moattar),mehrdad.khadivi@iaushab.ac.ir (Mehrdad Ranjbar-Khadivi)\n2\n• Also, to the extent of our knowledge, this is the primary prospective study that directly uses the graph\nembedding techniques to convert the words in the co-occurrence graph into low-dimensional vectors\nand employs these vectors to ﬁnd key-phrases.\n• We consider the problem of key-phrase extraction from a document as a sequence labeling task.\nMoreover, we observe the sequence labeling in the form a classiﬁcation task using the BIO encoding\nscheme.\nThe rest of the paper is structured as follows: Section 2 reviews the related works. Section 3 presents our\nproposed method and explains it in detail. The descriptions of our experiments are presented in Section 4.\nSection 5 provides the experimental results. Finally, Section 6 concludes the paper.\n2. Related Work\nThe exiting techniques used for keyword extraction can fall into three groups: textual, graph-based and\nhybrid models. Textual approaches generate keywords directly from the original text by applying natural\nlanguage processing techniques. While, graph-based methods convert the document into a co-occurrence\ngraph where nodes represent words and edges show the relationship between two words in a context window.\nOn the other hand, hybrid models take the advantage of both text and graph representations of a document\nto detect keywords. In the following paragraphs, we will investigate these three categories in more detail. In\ngeneral, the main contributions of our research are summarized as below.\nIn textual model the aim is to generate keywords directly from the original text [5]. The simplest model\nin this category uses TF-IDF technique to extract keywords. After that, researches have focused on machine\nlearning approaches to train a classiﬁer to capture keywords. With the advent of deep learning approaches\nsuch Convolutional Neural Networks (CNNs), Recurrent neural network (RNN) architectures such as Long\nshort-term memory (LSTM), and today’s Transformers have been popular solutions to this task. There are\na number of textual approaches including KEA [6], KP-Miner [7], WINGNUS [8], RAKE [9], YAKE [10],\nTNT-KID [11], [12, 13, 14, 15, 16].\nThe main idea of graph-based methods is to construct a co-occurrence graph form documents. The\nco-occurrence network shows the interactions of words in a corpus. In this graph, words represent nodes\nand there is an edge between two words if these words co-occur within a window. After constructing the\nco-occurrence graph, some centrality measures such as degree, closeness, betweenness and eigenvector are\napplied on it to ﬁnd keyphrases. In these methods, the keywords are identiﬁed by the most central nodes.\nA number of methods include TextRank [17], CollabRank [18], DegExt[19], NE-Rank [20], TopicRank [21],\nPositionrank [22], M-GCKE [5], [23, 24, 25, 26, 27, 1] that use the graph theory to select keywords.\n3\nThe hybrid models attempt to join two previous mentioned categories. These models calculate scores for\nwords from both co-occurrence graph and the document content. Diﬀerent approaches use various ways to\ncombine these scores. Authors in studies [28, 29, 30, 31] have proposed hybrid approaches.\nStudies [32, 33, 34, 35, 36] have conducted reviews of key-phrase and keyword extraction techniques. In\nour study, by combining the graph-based and text models, and using sequence tagging and classiﬁcation, we\nattempt to develop an eﬀective keyword extraction approach that can eliminate the drawbacks of previous\nstudies.\n3. Proposed Method\nIn this section, we propose a framework, called Phraseformer, that extracts key-phrases through learning\ncontext representations from text information and node representations in co-occurrence network. The overall\nprocessing steps of Phraseformer are explained in Figure 1. The ﬂow of framework is composed of four main\nsteps, as follows:\nText learning: Find a textual representation for all words using BERT Transformer. This part of\nPhraseformer provides a deeper understanding to evaluate semantic similarity between words.\nStructure learning:Create context of each word from the co-occurrence graph using graph embedding\ntechniques to learn structure representation.\nFinal word representation:Concatenate the text information and structure information and create a\nsingle representation for each word.\nSequence labeling and classiﬁcation:Formulate key-phrase extraction as a sequence labeling task\nand label each word based on the BIO tagging scheme through a fully connected layer to classify the word.\n3.1. Text Learning\nThe ﬁrst step of Phraseformer is to generate the textual vector for every word. With the advent of\nTransformers, the way of working with text data has truly altered. Transformers eliminate the drawbacks of\nRNN and CNN architectures. Applying self-attention allows Transformers to be much more parallelization\nthan other architectures [37]. A Transformer is composed of encoding and decoding components as shown in\nFigure 2. An encoding component includes a number of encoder blocks which have two layers: a Multi-Head\nAttention layer and a Feed Forward Neural Network layer [38]. On the other side, blocks with a Masked\nMulti-Head Attention layer before the feed forward layer create the decoding component [39]. Also, both\ncomponents contain the same number of blocks. There are diﬀerent models based on Transformer structure\nsuch as BERT [40], OpenGPT [41, 42], XLNet [43] and ELMo [44]. In this study, the technique to learn\ntextual representation is BERT Transformer whose structure is presented in Figure 3. One of the important\nadvantages of BERT over models like Word2Vec is that BERT creates the word embedding for each word\n4\nDocument text information\nConcatenation\n...\n...\n...\n...\nCo-occurrence network\nStructure Learning\nText Learning\nFinal word\nrepresenration\nSequence labeling\nand classification\nGraph embedding\nGraph embedding\nEmbeded nodes\nEmbeded words\nTransformer\nFigure 1: The overall structure of Phraseformer.\n5\nbased on the each sentence or each document the word is in it. That means that BERT is capable of capturing\nthe context of a word in a document. The text information of a document is composed of the documents’\ntitles, abstracts. The left block in Figure 1 shows the process for learning textual word vectors.\nScaled Dot-Product\nAttention\nFF \nFF \nFF \nScaled Dot-Product\nAttention\nFF \nFF \nFF \nScale \nSoftmax \nMatMul Q \nK \nV \nMatMul \nScaled Dot-Product\nAttention\nQ \nK \nV \nFF \nFF \nFF \nh \nConcat \nFF \nFigure 2: The Transformer model architecture [45].\nThis paper proposed new approach[CLS] [SEP]\nECLS E1 E 5 E SEP \nTrm\nThis\nTrm Trm Trm...\n... ...\n... ...\n...\nTrm Trm Trm Trm\nTCLS T1 TN TSEP\n... ... ...\n... ... ...\nESEP\n[SEP]\nE5\nFigure 3: The BERT Transformer architecture [46].\n3.2. Structure Learning\nThe second step is to learn the structural vector for every word based on three graph embedding algorithms\nincluding ExEm [47] , Node2vec [48] and DeepWalk [49] in the co-occurrence network. A co-occurrence\nnetwork is deﬁned as a graphG = (V, E) where nodes represent words and each edgee ∈E demonstrates the\nco-occurrence relationship between wordvi and wordvj that appear in a context window. Since the quantity\nof information that can be acquired from a solitary document is ﬁnite, we construct the co-occurrence graph\nover the all documents instead of a single document, as right block in Figure 1 shown. It should be noted\nthat before building this graph, we remove the stop words and punctuation mark. Then, our task is to learn\n6\nthe dimensional latent representations of words from this graph using graph embedding techniques. There are\ndiﬀerent node representation learning algorithms. According to the results reported by diﬀerent studies, we\nselect three random walk based graph embedding approaches that present better performance. In DeepWalk,\na set of random walks are generated by starting from every node in the graph. While, Node2vec proposes a\nbiased random walk method that is modiﬁed version of DeepWalk. This method uses two parameters to\ncontrol over the search space. Moreover, ExEm is another technique that uses dominating set theory to\ncreate random walks. ExEm characterizes the local neighborhoods by starting each path sampled with a\ndominating node. Also, the global structure information is captured by selecting another dominating node in\nthe random walk. All three approaches inject these random walks into a skip-gram neural network model to\nlearn node representations.\n3.3. Final Word Representation\nAfter we obtain each word’s vector representation of text information and co-occurrence network structure,\nthe following step is to combine these information into a single representation. We believe that the\nconcatenation of text-based and structure-based information can better discover the potential of words for\nbeing keywords. So, we present each word with a solitary vector which is the combination of text and\nstructure vectors. For example for wordwi, we showEwi = Wei + Nei where Wei and Nei denote the text\nand structure learning representations for wordwi, respectively.\n3.4. Sequence Labeling and Classiﬁcation\nSequence labeling is a type of pattern recognition task in NLP domain that categorizes words in a text\nand assigns a class or label to each word in a given input sequence [50, 51]. There are numerous techniques\nfor sequence labeling task include Hidden Markov Models [52], Conditional Random Fields (CRF) [53] and\ndeep learning approaches.\nIn this paper, we consider the problem of key-phrase extraction from a document as a sequence labeling\ntask. Also, we observe the sequence labeling in the form a classiﬁcation task using the BIO encoding scheme as\noutput labels. So, our model takesEw1 , Ew2 , ..., Ewm as inputs and assigns each word a labelOi ∈{B, I, O}\nwhere B shows thatwi is the beginning of a key-phrase,I denotes thatwi is inside a key-phrase, and ﬁnally\nO illustrates thatwi is outside of a key-phrase. It can be conducted from Figure 1 that a fully connected\nstructure and a softmax layer are used to make the classiﬁcation decision.\n4. Experimental Evaluation\nIn this section of our research, we will clarify that which datasets have been used to evaluate Phraseformer.\nAlso, we are going to describe the baseline algorithms to compare our proposed method against them. In the\nnext part, we will present diﬀerent versions of Phraseformer. Finally, evaluation metrics will be speciﬁed.\n7\n4.1. Dataset\nTo assess the eﬃciency of Phraseformer, we used three datasets including Inspec [54], SE-2010 [55] and\nSE-2017 [56]. We will describe these datasets in the succeeding paragraphs.\nInspec includes abstracts of papers from Computer Science collected between the years 1998 and 2002.\nSE-2010 contains of full scientiﬁc articles that are obtained from the ACM Digital Library. In our\nexperiment, we used the abstract of papers.\nSE-2017 consists of paragraphs selected from 500 ScienceDirect journal papers from Computer Science,\nMaterial Sciences and Physics domains.\nIt should be noted that because of formulating the key-phrase extraction as a sequence labeling task, we\ntake into consideration key-phrases that appear in the abstracts of articles in three datasets. Table 1 shows\nthe statistics1. of the above three datasets.\nTable 1: Dataset statistics.\nDataset Type of doc #Doc #Gold Keys (per doc) #Tokens per doc Absent Gold Key\nInspec Abstract 2000 29230 (14.62) 128.20 37.7%\nSemEval2010 Paper 243 4002 (16.47) 8332.34 11.3%\nSemEval2017 Paragraph 493 8969 (18.19) 178.22 0.0%\n4.2. Baseline models\nIn this study, we compared Phraseformer method with four graph-based approaches and three textual\nmethods. In the following paragraphs, we will explain these techniques in more detail.\nTextRank[17] is the simplest graph-based method that is based on PageRank algorithm.\nDeepWalk[49] is a graph embedding technique based on random walks. This method represents each\nnode as a low-dimensional vector.\nNode2vec [48] is modiﬁed version of DeepWalk that uses a biased random walks to convert nodes into\nvectors.\nExEm [47] is a random walk based approach that uses dominating set theory to generate random walks.\nExEmw2v and ExEmft are two diﬀerent version of ExEm.\nWord2Vec[57] learns the vector representations of words. This method passes the document through\ntwo feed-forward layers to created vectors [37].\nBiLSTM-CRF [58] is a textual method that considers the keyphrase extraction as a sequence labeling\ntask using a BiLSTM-CRF architecture.\n1We obtained this information fromhttps://github.com/LIAAD/KeywordExtractor-Datasets\n8\nBERT[40] is a textual approach that uses the transformer structure to obtain the document representa-\ntion.\n4.3. Method Variations\nWe present four variations of Phraseformer that employ diﬀerent graph embedding techniques for structure\nlearning. Phraseformer(BERT, DeepWalk), Phraseformer(BERT, Node2vec), Phraseformer(BERT, ExEmw2v)\nand Phraseformer(BERT, ExEmft ) are diﬀerent models of Phraseformer that use DeepWalk, Node2vec,\nExEmw2v and ExEmft approaches to obtain word representation from the co-occurrence graph, respectively.\n4.4. Metrics\nTo measure the experimental eﬀect, we used F1-score that is formulated as follow:\nF1 −score = 2×\nY ∩Y ′\nY ′ ×Y ∩Y ′\nY\nY ∩Y ′\nY ′ + Y ∩Y ′\nY\n(1)\nhere Y ′and Y are the predicted keywords and the real keywords, respectively.\n5. Experimental Results\nThis section resents the experimental results. Firstly, we compare Phraseformer against of baselines.\nThen, we test diﬀerent classiﬁers for the classiﬁcation part and report consequences.\n5.1. Baseline comparisons\nwe compare our multimodal model against textual and graph-based methods. Table 2 presents the\nresults. As expected, our model signiﬁcantly outperforms all the methods. From the results we have the\nfollowing observations: i) It can be concluded that Phraseformer approaches gain the highest F1-scores.\nPhraseformer(BERT, ExEmft ) strengthens the performance by6.4%, 19.94% and 13.70% compared with\nBERT over Inspec, SE-2010 and SE-2017 datasets, respectively. Moreover, Phraseformer(BERT, ExEmft )\noutperforms ExEm by112.6%, 290.4% and 130.4% over the mentioned datasets. These high scores prove\nthat our hypothesis about using multimodal learning, sequence labeling and classiﬁcation is true.ii) It is\nevident that two graph-based and textual methods indicate poor performances.iii) It is obvious that textual\nmethods have satisfactory outcomes compared with graph-based models.iv) Moreover, BERT shows better\nresults than other textual methods. Because BERT embeds words into vectors by considering the meaning of\nthe words in the sentence.v) Additionally, the results of graph-based methods demonstrate that the graph\nembedding techniques gain the highest F1 scores than TextRank. Also, various versions of ExEm are more\nsuccessful than other graph embedding approaches. The reason is that ExEm obeys the homophily and\nstructural role equivalences in learning node representations with help of dominating nodes.\n9\nTable 2: Comparison with baseline methods (F1-score).\nCategory Model\nDataset\nInspec SE-2010 SE-2017\nGraph-based\nmethod\nTextRank [17] 0.1780 0.1990 0.2090\nDeepWalk [49] 0.3190 0.1102 0.2887\nNode2vec [48] 0.3138 0.1098 0.2863\nExEmw2v [47] 0.3273 0.1233 0.2911\nExEmft [47] 0.3286 0.1246 0.2913\nText-based\nmethod\nWord2Vec [57] 0.4730 0.2080 0.2920\nBiLSTM-CRF [58] 0.5930 0.3570 0.5210\nBERT [40] 0.6564 0.4056 0.5904\nHybrid\nmethod\nPhraseformer(BERT, Deep-\nWalk)\n0.6844 0.4722 0.6570\nPhraseformer(BERT,\nNode2vec)\n0.6868 0.4746 0.6594\nPhraseformer(BERT,\nExEmw2v)\n0.6970 0.4848 0.6696\nPhraseformer(BERT,\nExEmft )\n0.6987 0.4865 0.6713\n5.2. Classiﬁer\nIn this part of our experiment we aim to investigate which classiﬁer is best suited for sequence labelling\nand classiﬁcation tasks to ﬁnd key-phrases. Table 3 illustrates the performance of diﬀerent classiﬁers on\nPhraseformer method over Inspec dataset. From the results, it can be seen that Random Forest classiﬁer\nis obtained the best performance among all classiﬁers, as shown by the underlined result. On the other\nhand, the bold results in this table present which models of Phraseformer can gain the highest values on\neach classiﬁer. It is obvious that the combination of BERT and ExEm is more meaningful and can better\nrepresent the semantic of words. Hence, Phraseformer can accurately extract keywords.\n10\nTable 3: Classiﬁers comparison on Inspec dataset (F1-score)\nModel\nClassiﬁer\nRandom Forest SVM Logistic regression Fully connected\nPhraseformer(BERT, Deep-\nWalk)\n0.7024 0.6564 0.6714 0.6844\nPhraseformer(BERT,\nNode2vec)\n0.7048 0.6588 0.6738 0.6868\nPhraseformer(BERT,\nExEmw2v)\n0.7150 0.6707 0.6857 0.6970\nPhraseformer(BERT,\nExEmft )\n0.7167 0.6690 0.6840 0.6987\n6. Conclusion\nIn this paper, a multimodal approach, Phraseformer, to extract key-phrases from documents is proposed.\nIn this approach, two modalities originate from the co-occurrence graph and the content of documents.\nMoreover, we formulate the key-phrase extraction as a sequence labeling task solved using a classiﬁcation\nmodel. To represent information of the modalities as vectors, we use BERT Transformer and graph embedding\ntechniques. Utilization of graph based information with aid of textual semantics provide a more insightful\nrepresentation of the keywords that yield into a more robust keyphrase extraction method. Finally, to\nvalidate the eﬀectiveness of the proposed Phraseformer approach, we conduct experiments on three datasets,\nand the results demonstrate how Phraseformer signiﬁcantly outperforms single-modality methods.\nAcknowledgment\nThis project is supported by a research grant of the University of Tabriz (Number S/806).\nDeclarations\nFunding\nThis project is supported by a research grant of the University of Tabriz (Number S/806).\nConﬂicts of interests\nThe authors have no conﬂicts of interest to declare that are relevant to the content of this article.\n11\nEthical approval\nThis article does not contain any studies with human participants or animals performed by any of the\nauthors.\nReferences\n[1] D. A. Vega-Oliveros, P. S. Gomes, E. E. Milios, L. Berton, A multi-centrality index for graph-based keyword extraction,\nInformation Processing & Management 56 (6) (2019) 102063.doi:10.1016/j.ipm.2019.102063.\n[2] M. W. Berry, J. Kogan, Text mining: applications and theory, John Wiley & Sons, 2010.\n[3] S. Lahiri, Keywords at work: Investigating keyword extraction in social media applications, Ph.D. thesis (2018).\n[4] C. Zhang, Automatic keyword extraction from documents using conditional random ﬁelds, Journal of Computational\nInformation Systems 4 (3) (2008) 1169–1180.\n[5] B. Wang, B. Yang, S. Shan, H. Chen, Detecting hot topics from academic big data, IEEE Access 7 (2019) 185916–185927.\ndoi:10.1109/ACCESS.2019.2960285.\n[6] I. H. Witten, G. W. Paynter, E. Frank, C. Gutwin, C. G. Nevill-Manning, Kea: Practical automated keyphrase extraction,\nin: Design and Usability of Digital Libraries: Case Studies in the Asia Paciﬁc, IGI global, 2005, pp. 129–152.doi:\n10.4018/978-1-59140-441-5.ch008 .\n[7] S. R. El-Beltagy, A. Rafea, Kp-miner: A keyphrase extraction system for english and arabic documents, Information\nsystems 34 (1) (2009) 132–144.doi:10.1016/j.is.2008.05.002.\n[8] T. D. Nguyen, M.-T. Luong, Wingnus: Keyphrase extraction utilizing document logical structure, in: Proceedings of the\n5th international workshop on semantic evaluation, 2010, pp. 166–169.\nURL https://www.aclweb.org/anthology/S10-1035\n[9] S. Rose, D. Engel, N. Cramer, W. Cowley, Automatic keyword extraction from individual documents, Text mining:\napplications and theory 1 (2010) 1–20.doi:10.1002/9780470689646.ch1.\n[10] R. Campos, V. Mangaravite, A. Pasquali, A. Jorge, C. Nunes, A. Jatowt, Yake! keyword extraction from single documents\nusing multiple local features, Information Sciences 509 (2020) 257–289.doi:10.1016/j.ins.2019.09.013.\n[11] M. Martinc, B. Škrlj, S. Pollak, Tnt-kid: Transformer-based neural tagger for keyword identiﬁcation, arXiv preprint\narXiv:2003.09166 (2020).\n[12] M. Basaldella, E. Antolli, G. Serra, C. Tasso, Bidirectional lstm recurrent neural network for keyphrase extraction, in:\nItalian Research Conference on Digital Libraries, Springer, 2018, pp. 180–187.doi:10.1007/978-3-319-73165-0_18 .\n[13] R. Alzaidy, C. Caragea, C. L. Giles, Bi-lstm-crf sequence labeling for keyphrase extraction from scholarly documents, in:\nThe world wide web conference, 2019, pp. 2551–2557.doi:10.1145/3308558.3313642.\n[14] M. Tang, P. Gandhi, M. A. Kabir, C. Zou, J. Blakey, X. Luo, Progress notes classiﬁcation and keyword extraction using\nattention-based deep learning models with bert, arXiv preprint arXiv:1910.05786 (2019).\n[15] J. Wang, F. Song, K. Walia, J. Farber, R. Dara, Using convolutional neural networks to extract keywords and keyphrases:\nA case study for foodborne illnesses, in: 2019 18th IEEE International Conference On Machine Learning And Applications\n(ICMLA), IEEE, 2019, pp. 1398–1403.doi:10.1109/ICMLA.2019.00228.\n[16] Y. Kim, J. H. Lee, S. Choi, J. M. Lee, J.-H. Kim, J. Seok, H. J. Joo, Validation of deep learning natural language processing\nalgorithm for keyword extraction from pathology reports in electronic health records, Scientiﬁc Reports 10 (1) (2020) 1–9.\ndoi:10.1038/s41598-020-77258-w .\n[17] R. Mihalcea, P. Tarau, Textrank: Bringing order into text, in: Proceedings of the 2004 conference on empirical methods in\nnatural language processing, 2004, pp. 404–411.\nURL https://www.aclweb.org/anthology/W04-3252\n12\n[18] X. Wan, J. Xiao, Collabrank: towards a collaborative approach to single-document keyphrase extraction, in: Proceedings\nof the 22nd International Conference on Computational Linguistics (Coling 2008), 2008, pp. 969–976.\nURL https://www.aclweb.org/anthology/C08-1122\n[19] M. Litvak, M. Last, H. Aizenman, I. Gobits, A. Kandel, Degext—a language-independent graph-based keyphrase extractor,\nin: Advances in intelligent web mastering–3, Springer, 2011, pp. 121–130.doi:10.1007/978-3-642-18029-3_13 .\n[20] A. Bellaachia, M. Al-Dhelaan, Ne-rank: A novel graph-based keyphrase extraction in twitter, in: 2012 IEEE/WIC/ACM\nInternational Conferences on Web Intelligence and Intelligent Agent Technology, Vol. 1, IEEE, 2012, pp. 372–379.\ndoi:10.1109/WI-IAT.2012.82.\n[21] A. Bougouin, F. Boudin, B. Daille, TopicRank: Graph-based topic ranking for keyphrase extraction, in: Proceedings of the\nSixth International Joint Conference on Natural Language Processing, Asian Federation of Natural Language Processing,\nNagoya, Japan, 2013, pp. 543–551.\nURL https://www.aclweb.org/anthology/I13-1062\n[22] C. Florescu, C. Caragea, Positionrank: An unsupervised approach to keyphrase extraction from scholarly documents, in:\nProceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2017,\npp. 1105–1115.doi:10.18653/v1/P17-1102.\n[23] F. Boudin, A comparison of centrality measures for graph-based keyphrase extraction, in: Proceedings of the sixth\ninternational joint conference on natural language processing, 2013, pp. 834–838.\nURL https://www.aclweb.org/anthology/I13-1102\n[24] W. D. Abilhoa, L. N. De Castro, A keyword extraction method from twitter messages represented as graphs, Applied\nMathematics and Computation 240 (2014) 308–325.doi:10.1016/j.amc.2014.04.090.\n[25] A. Tixier, F. Malliaros, M. Vazirgiannis, A graph degeneracy-based approach to keyword extraction, in: Proceedings of the\n2016 conference on empirical methods in natural language processing, 2016, pp. 1860–1870.doi:10.18653/v1/D16-1191.\n[26] M. S. El BazzI, D. Mammass, T. Zaki, A. Ennaji, A graph-based ranking model for automatic keyphrases extraction from\narabic documents, in: Industrial conference on data mining, Springer, 2017, pp. 313–322.doi:10.1007/978-3-319-62701-4_\n25.\n[27] F. Boudin, Unsupervised keyphrase extraction with multipartite graphs, in: Proceedings of the 2018 Conference of the\nNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies, 2018, p.\n667–672. doi:10.18653/v1/N18-2105.\n[28] S. Danesh, T. Sumner, J. H. Martin, Sgrank: Combining statistical and graphical methods to improve the state of the\nart in unsupervised keyphrase extraction, in: Proceedings of the fourth joint conference on lexical and computational\nsemantics, 2015, pp. 117–126.doi:10.18653/v1/S15-1013.\n[29] Y. Zhang, Y. Chang, X. Liu, S. D. Gollapalli, X. Li, C. Xiao, Mike: keyphrase extraction by integrating multidimensional\ninformation, in: Proceedings of the 2017 ACM on Conference on Information and Knowledge Management, 2017, pp.\n1349–1358. doi:10.1145/3132847.3132956.\n[30] D. Mahata, J. Kuriakose, R. Shah, R. Zimmermann, Key2vec: Automatic ranked keyphrase extraction from scientiﬁc\narticles using phrase embeddings, in: Proceedings of the 2018 Conference of the North American Chapter of the Association\nfor Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers), 2018, pp. 634–639. doi:\n10.18653/v1/N18-2100.\n[31] D. Mahata, R. R. Shah, J. Kuriakose, R. Zimmermann, J. R. Talburt, Theme-weighted ranking of keywords from text\ndocuments using phrase embeddings, in: 2018 IEEE conference on multimedia information processing and retrieval (MIPR),\nIEEE, 2018, pp. 184–189.doi:10.1109/MIPR.2018.00041.\n[32] S. Siddiqi, A. Sharan, Keyword and keyphrase extraction techniques: a literature review, International Journal of Computer\nApplications 109 (2) (2015).\n13\n[33] Z. A. Merrouni, B. Frikh, B. Ouhbi, Automatic keyphrase extraction: a survey and trends, Journal of Intelligent Information\nSystems (2019) 1–34doi:10.1007/s10844-019-00558-9 .\n[34] Ö. Ünlü, A. Çetin, A survey on keyword and key phrase extraction with deep learning, in: 2019 3rd International Symposium\non Multidisciplinary Studies and Innovative Technologies (ISMSIT), IEEE, 2019, pp. 1–6.doi:10.1109/ISMSIT.2019.\n8932811.\n[35] E. Papagiannopoulou, G. Tsoumakas, A review of keyphrase extraction, Wiley Interdisciplinary Reviews: Data Mining and\nKnowledge Discovery 10 (2) (2020) e1339.doi:10.1002/widm.1339.\n[36] N. Firoozeh, A. Nazarenko, F. Alizon, B. Daille, Keyword extraction: Issues and methods, Natural Language Engineering\n26 (3) (2020) 259–291.doi:10.1017/S1351324919000457.\n[37] S. Minaee, N. Kalchbrenner, E. Cambria, N. Nikzad, M. Chenaghlu, J. Gao, Deep learning based text classiﬁcation: A\ncomprehensive review, arXiv preprint arXiv:2004.03705 (2020).\n[38] M. Asgari-Chenaghlu, M.-R. Feizi-Derakhshi, M.-A. Balafar, C. Motamed, et al., Topicbert: A transformer transfer learning\nbased memory-graph approach for multimodal streaming social media topic detection, arXiv preprint arXiv:2008.06877\n(2020).\n[39] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you\nneed, arXiv preprint arXiv:1706.03762 (2017).\n[40] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language\nunderstanding, arXiv preprint arXiv:1810.04805 (2018).\n[41] A. Radford, K. Narasimhan, T. Salimans, I. Sutskever, Improving language understanding by generative pre-training\n(2018).\nURL https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf\n[42] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are unsupervised multitask learners,\nOpenAI blog 1 (8) (2019) 9.\n[43] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, Q. V. Le, Xlnet: Generalized autoregressive pretraining for\nlanguage understanding, arXiv preprint arXiv:1906.08237 (2019).\n[44] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, L. Zettlemoyer, Deep contextualized word representa-\ntions, arXiv preprint arXiv:1802.05365 (2018).\n[45] M. Asgari-Chenaghlu, M. R. Feizi-Derakhshi, L. Farzinvash, C. Motamed, A multimodal deep learning approach for named\nentity recognition from social media, arXiv preprint arXiv:2001.06888 (2020).\n[46] N. Nikzad-Khasmakhi, M. Balafar, M. R. Feizi-Derakhshi, C. Motamed, Berters: Multimodal representation learning for\nexpert recommendation system with transformer, arXiv preprint arXiv:2007.07229 (2020).\n[47] N. Nikzad-Khasmakhi, M. Balafar, M. R. Feizi-Derakhshi, C. Motamed, Exem: Expert embedding using dominating set\ntheory with deep learning approaches, arXiv preprint arXiv:2001.08503 (2020).\n[48] A. Grover, J. Leskovec, Node2vec: Scalable feature learning for networks, in: Proceedings of the ACM SIGKDD International\nConference on Knowledge Discovery and Data Mining, 2016.arXiv:1607.00653, doi:10.1145/2939672.2939754.\n[49] B. Perozzi, R. Al-Rfou, S. Skiena, DeepWalk: Online learning of social representations, in: Proceedings of the ACM\nSIGKDD International Conference on Knowledge Discovery and Data Mining, 2014.arXiv:1403.6652, doi:10.1145/\n2623330.2623732.\n[50] Z. He, Z. Wang, W. Wei, S. Feng, X. Mao, S. Jiang, A survey on recent advances in sequence labeling from deep learning\nmodels, arXiv preprint arXiv:2011.06727 (2020).\n[51] A. Akhundov, D. Trautmann, G. Groh, Sequence labeling: A practical approach, arXiv preprint arXiv:1808.03926 (2018).\n[52] J. Kupiec, Robust part-of-speech tagging using a hidden markov model, Computer speech & language 6 (3) (1992) 225–242.\ndoi:10.1016/0885-2308(92)90019-Z.\n14\n[53] J. Laﬀerty, A. McCallum, F. C. Pereira, Conditional random ﬁelds: Probabilistic models for segmenting and labeling\nsequence data, in: 18th International Conference on Machine Learning 2001 (ICML 2001), 2001, p. 282–289. doi:\n10.5555/645530.655813.\n[54] A. Hulth, Improved automatic keyword extraction given more linguistic knowledge, in: Proceedings of the 2003 conference\non Empirical methods in natural language processing, 2003, pp. 216–223.doi:10.3115/1119355.1119383.\n[55] S. N. Kim, O. Medelyan, M.-Y. Kan, T. Baldwin, Semeval-2010 task 5: Automatic keyphrase extraction from scientiﬁc\narticles, in: Proceedings of the 5th International Workshop on Semantic Evaluation, 2010, pp. 21–26.\n[56] I. Augenstein, M. Das, S. Riedel, L. Vikraman, A. McCallum, Semeval 2017 task 10: Scienceie-extracting keyphrases and\nrelations from scientiﬁc publications, in: 11th International Workshop on Semantic Evaluation (SemEval-2017), 2017, p.\n546–555. doi:10.18653/v1/S17-2091.\n[57] T. Mikolov, K. Chen, G. Corrado, J. Dean, Eﬃcient estimation of word representations in vector space, arXiv preprint\narXiv:1301.3781 (2013).\n[58] D. Sahrawat, D. Mahata, M. Kulkarni, H. Zhang, R. Gosangi, A. Stent, A. Sharma, Y. Kumar, R. R. Shah, R. Zimmermann,\nKeyphrase extraction from scholarly articles as sequence labeling using contextualized embeddings, arXiv preprint\narXiv:1910.08840 (2019).\n15"
}