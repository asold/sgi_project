{
  "title": "Data Fusion of Synthetic Query Variants With Generative Large Language Models",
  "url": "https://openalex.org/W4404370652",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4227930984",
      "name": "Breuer, Timo",
      "affiliations": [
        "TH KÃ¶ln - University of Applied Sciences"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2159665776",
    "https://openalex.org/W2090035194",
    "https://openalex.org/W2388783363",
    "https://openalex.org/W2039847928",
    "https://openalex.org/W4306317389",
    "https://openalex.org/W6746172908",
    "https://openalex.org/W2901143721",
    "https://openalex.org/W3093975186",
    "https://openalex.org/W2148972377",
    "https://openalex.org/W3080126616",
    "https://openalex.org/W2008883775",
    "https://openalex.org/W2096249996",
    "https://openalex.org/W4391673302",
    "https://openalex.org/W4399362671",
    "https://openalex.org/W1978604688",
    "https://openalex.org/W3198536471",
    "https://openalex.org/W4384107234",
    "https://openalex.org/W3133594109",
    "https://openalex.org/W3209981429",
    "https://openalex.org/W1977437814",
    "https://openalex.org/W1845198550",
    "https://openalex.org/W4400525546",
    "https://openalex.org/W2022995284",
    "https://openalex.org/W1511363262",
    "https://openalex.org/W4400526908",
    "https://openalex.org/W3027639267",
    "https://openalex.org/W4385642073",
    "https://openalex.org/W3100907046",
    "https://openalex.org/W2772833636",
    "https://openalex.org/W2979894627",
    "https://openalex.org/W3156166379"
  ],
  "abstract": "Considering query variance in information retrieval (IR) experiments is\\nbeneficial for retrieval effectiveness. Especially ranking ensembles based on\\ndifferent topically related queries retrieve better results than rankings based\\non a single query alone. Recently, generative instruction-tuned Large Language\\nModels (LLMs) improved on a variety of different tasks in capturing human\\nlanguage. To this end, this work explores the feasibility of using synthetic\\nquery variants generated by instruction-tuned LLMs in data fusion experiments.\\nMore specifically, we introduce a lightweight, unsupervised, and cost-efficient\\napproach that exploits principled prompting and data fusion techniques. In our\\nexperiments, LLMs produce more effective queries when provided with additional\\ncontext information on the topic. Furthermore, our analysis based on four TREC\\nnewswire benchmarks shows that data fusion based on synthetic query variants is\\nsignificantly better than baselines with single queries and also outperforms\\npseudo-relevance feedback methods. We publicly share the code and query\\ndatasets with the community as resources for follow-up studies.\\n",
  "full_text": "Data Fusion of Synthetic Query Variants\nWith Generative Large Language Models\nTimo Breuer\nTH KÃ¶ln â€” University of Applied Sciences\nCologne, Germany\ntimobreuer@acm.org\nPrincipled prompting Query generation Retrieval Data fusion\nPrompt LLM\nQueries\nIndex\nSingle rankings\nFinal rankingInformation need\nFigure 1: Data fusion of synthetic query variants includes 1) principled prompting, 2) query variant generation with instruction-\ntuned Large Language Models, 3) retrieval of single rankings, and 4) data fusion.\nAbstract\nConsidering query variance in information retrieval ( IR) experi-\nments is beneficial for retrieval effectiveness. Especially ranking\nensembles based on different topically related queries retrieve bet-\nter results than rankings based on a single query alone. Recently,\ngenerative instruction-tuned Large Language Models (LLMs) im-\nproved on a variety of different tasks in capturing human language.\nTo this end, this work explores the feasibility of using synthetic\nquery variants generated by instruction-tuned LLMs in data fu-\nsion experiments. More specifically, we introduce a lightweight,\nunsupervised, and cost-efficient approach that exploits principled\nprompting and data fusion techniques. In our experiments, LLMs\nproduce more effective queries when provided with additional con-\ntext information on the topic. Furthermore, our analysis based on\nfour TREC newswire benchmarks shows that data fusion based on\nsynthetic query variants is significantly better than baselines with\nsingle queries and also outperforms pseudo-relevance feedback\nmethods. We publicly share the code and query datasets with the\ncommunity as resources for follow-up studies.\n/gtbhttps://github.com/breuert/sigirap24\nSIGIR-AP â€™24, December 9â€“12, 2024, Tokyo, Japan\nÂ© 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM.\nThis is the authorâ€™s version of the work. It is posted here for your personal use. Not\nfor redistribution. The definitive Version of Record was published in Proceedings of\nthe 2024 Annual International ACM SIGIR Conference on Research and Development in\nInformation Retrieval in the Asia Pacific Region (SIGIR-AP â€™24), December 9â€“12, 2024,\nTokyo, Japan, https://doi.org/10.1145/3673791.3698423.\nCCS Concepts\nâ€¢ Information systems â†’Information retrieval; Combina-\ntion, fusion and federated search ; Query reformulation.\nKeywords\nquery variants, large language models, data fusion\nACM Reference Format:\nTimo Breuer. 2024. Data Fusion of Synthetic Query Variants With Generative\nLarge Language Models. InProceedings of the 2024 Annual International ACM\nSIGIR Conference on Research and Development in Information Retrieval in\nthe Asia Pacific Region (SIGIR-AP â€™24), December 9â€“12, 2024, Tokyo, Japan.\nACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/3673791.3698423\n1 Introduction\nTest collection-based evaluations of IR experiments are typically\ndone in accordance with the Cranfield paradigm. Usually, these\nexperiments are implemented with topics that define the usersâ€™\ninformation need in a structured way [45]. In principle, there are\ndifferent ways to generate queries from topic files. However, the de\nfacto standard in mostIR evaluations is the use of the topicâ€™s title as\nthe query string. While this approach allows better comparability\nbetween different systems, it completely neglects query variability\nand also the system behavior under the consideration of different\ntopically or semantically related queries [6].\nIncluding multiple query representations in the retrieval process\nbears a lot of potential to improve retrieval effectiveness. Empir-\nically, the improvements by data fusion algorithms were shown\nin several works. However, earlier work either required crowd-\nsourcing efforts to obtain real query variants from users [ 7, 13],\nwhich is costly and time-consuming, or generated synthetic query\nvariants for which additional query modeling methods were neces-\nsary [18, 21].\narXiv:2411.03881v1  [cs.IR]  6 Nov 2024\nSIGIR-AP â€™24, December 9â€“12, 2024, Tokyo, Japan Timo Breuer\nRecently, generative instruction-tuned LLMs substantially im-\nproved on various language modeling tasks. Zero shot, few shot , or\nin-context learning [20] does not require additional task-specific\nfine-tuning but instead provides the task-agnostic language model\nwith context information based on precise instructions and by learn-\ning from examples in the prompt, allowing the language model to\ngrasp the task. These advances offer promising solutions to reduce\nefforts when in need of obtaining query variants.\nTo this end, this work evaluates if synthetic query variants based\non generative LLMs can be used to improve the retrieval effec-\ntiveness in data fusion experiments. Figure 1 provides a high-level\nillustration of our proposed methodology. Given the information\nneed that is described by the topic file, we construct a topic-specific\nprompt that is given to theLLM, and the generated queries are then\nused to retrieve documents from the index, yielding multiple single\nrankings based on different query variants. Afterward, the single\nrankings are combined with data fusion techniques, resulting in the\nfinal ranking. Our results suggest that fused rankings of synthetic\nquery variants based on LLMs and principled prompting can result\nin viable results that, in combination, yield better effectiveness than\nthe same retrieval method would yield with a single query.\nWhile this work focuses on evaluating the retrieval effectiveness\nof the fused rankings, the proposed method could also be under-\nstood as a component of a Retrieval Augmented Generation (RAG)\nsystem where the ranking is an intermediate used to improve the\nquality of system outputs in a conversation with an agent. In such\na conversational search setting, it is also feasible to obtain a more\nprecise understanding of the userâ€™s information need, similar to\nthe topic file, making this approach a promising candidate for real-\nworld applications, which we discuss later in the text. In sum, our\ncontributions are threefold and include the following:\nâ€¢An unsupervised and lightweight retrieval approach\nbased on data fusion of synthetic query variants,\nâ€¢experimental evaluations based on four different newswire\ndatasets, including different prompting strategies and differ-\nent numbers of fused query variants,\nâ€¢new datasets of query variants for TREC test collections\nof the respective tracks and open-source code of the ex-\nperimental setup for better reproducibility and follow-up\nstudies.1\n2 Related Work\nRecently, Alaofi et al. [1] highlighted that the source of query vari-\nation is little studied. Currently, there is no cohesive framework\nthat explains how users come up with query formulations. Bailey et\nal. [6] pointed out the importance of user variance and concluded\nthat query variance leads to similar variability in retrieval effective-\nness as topical or system variance. In this regard, Moffat et al. [40]\nshowed that query variability has a similar effect on the pool size\nas system variability. One of the earlier works about data fusion\nby Belkin et al. [11, 12] showed that fusing rankings based on the\nsame retrieval method and different queries can be more effective\nthan fusing rankings based on different retrieval methods that used\na single query variant. Similarly, data fusion can also be understood\nas an operationalization of polyrepresentation [28, 32]. Benham et\n1/gtbhttps://github.com/breuert/sigirap24\nal. [13â€“17] gave empirical evidence to the effectiveness of fusing\nrankings based on user query variants in several works, primarily\nshowing improvements in the recall rates.\nBaskaya et al. [8] proposed several principled formulation strate-\ngies to generate synthetic query variants from topic texts, i.e., given\nthe topic text, they apply term variations or extensions for sequen-\ntial query formulations. Azzopardi et al. [5] introduced a testbed for\nsimulated queries of known items. The corresponding known-item\nqueries were built as proposed by Jordan et al. [30]. The underlying\nlanguage model is based on the term distributions in relevant docu-\nments and requires apriori knowledge about relevant documents.\nBenham and Culpepper [14] generated synthetic queries with ex-\npansion techniques based on large external corpora, and used them\nto fuse rankings from a target collection. There exist more recent\ntransformer-based query generation methods like Doc2Query, as\nproposed by Nogueira et al. [41], where a sequence-to-sequence\ntransformer is used to generate a query from document terms.\nWhile this approach has been primarily used to augment docu-\nments during indexing [27], the generated queries could also be\nused for data fusion experiments if suitable and topically related\ndocuments of an external corpus are available for the query genera-\ntion. Penha et al. [43] proposed a taxonomy of different user query\nvariants for which they implement different query generators. For\ninstance, they fine-tuned a pretrained T5 model to generate title\nqueries from descriptions of topic texts.\nAs an alternative to fine-tuning a task-specific LLM, in-context\nlearning [20] does not require additional fine-tuning for many tasks.\nOnce the model is fine-tuned for instructions, it can be applied to\nmany specific downstream tasks based on the prompted instruc-\ntions. For instance, chain-of-thought prompting [52] is a simple\nbut effective technique that allows LLMs to pick up task-specific\nreasoning by providing examples in the prompt. Recently, several\nprompt-based query generation techniques were proposed. Alaofi\net al. [2] provided a first experimental evaluation of prompt-based\nLLM-generated query variants in reference to the UQV100 dataset\nprovided by Bailey et al. [7]. Jagerman et al. [29] proposed a query\nexpansion technique based on prompting the LLM to answer a\nquery, which is then expanded by the generated output to retrieve\na ranking with a lexical retrieval method like BM25. Dai et al. [25]\npropose a few-shot prompting technique that makes it possible to\ngenerate effective queries by providing the LLM with a sample of\nqueries.\n3 Methodology\nThis section provides details about our methodology that is illus-\ntrated from a high-level perspective in Figure 1. More specifically,\nwe describe the approach of prompting the LLM and generating\nqueries (cf. 3.1), review the data fusion method (cf. 3.2), and provide\nan overview of the experimental setup (cf. 3.3).\n3.1 Principled Prompting and Query Generation\nThe prompts sent to the instruction-tuned LLM are constructed\nin principled ways by following common practices of prompting\nstrategies. We specify the role of the LLM and the format of the\noutputs. Then, we augment the prompt with the contents of the\ntopic file and compose the prompt as shown below.\nData Fusion of Synthetic Query Variants With Generative LLMs SIGIR-AP â€™24, December 9â€“12, 2024, Tokyo, Japan\nPrompt template for strategies P-1, P-2, and P-3\nYou are a generator of search query variants.\nGenerate one hundred keyword queries about <title>.\n<description> <narrative>\nExample queries for the topic about <example title>\ninclude <1st query example> , <2nd query example> , <3rd\nquery example> ...\nYour reply is a numbered list of search queries.\nThe first prompting strategyP-1 includes the topicâ€™s title\nas topical context information besides the prompt to generate one\nhundred queries, spanned by the role definition and the specifi-\ncation of the output format. In comparison, the second strategy\nP-2 adds the topicâ€™s description and narrative as context\ninformation. Wei et al. [52] showed that chain-of-thought prompting\ncan lead to significant performance improvements on tasks such\nas arithmetic, commonsense, and symbolic reasoning. In this re-\ngard, we extend the prompt with examples of query reformulations.\nTo avoid any overlap of topics with our test collections, we use\nexamples of the dataset provided by Bailey et al. [7]. More specifi-\ncally, the query examples were picked from the topics 201, 251, 263,\n266, and 296 of the 2013 and 2014 TREC Web tracks [ 22, 23] for\nthe third prompting strategy P-3 . We use this dataset of\nquery variants for the query examples to avoid query data leakage.\nThe particular topics were randomly chosen from a preselection of\ntopics with a sufficient number of query variants.\nIn order to generate the query variants, we rely on OpenAIâ€™s\ngpt-4o.2 At the time of our experiments, this identifier points to\nthe model version gpt-4o-2024-05-13 with training data up to\nOctober 2023. Besides, we make the API calls with a fixed seed and\na temperature parameter of 0.0. Since the model reliably returns\nnumbered lists, the parsing of the generated outputs is fairly simple\nand straightforward. To illustrate the costs of query generation, let\nus consider all of the 400 topics of our test collections. For each topic,\nwe generate one hundred queries. Generating for each of the three\ndifferent prompts 40,000 queries translated into approx. 1.5 million\ngenerated tokens. In sum, we curated four datasets of query variants\nfor TREC test collections covering three prompting strategies with\nover 120,000 query strings for approximately $22.50.3\n3.2 Data Fusion\nData fusion methods can be categorized into rank- or score-based\nalgorithms. In the data fusion experiments, we rely on Reciprocal\nRank Fusion (RRF) [24] â€” a rank-based fusion method â€” that proved\nto be effective and robust. It is defined as follows:\nğ‘…ğ‘…ğ¹ score(ğ‘‘ âˆˆğ·)=\nâˆ‘ï¸\nğ‘Ÿ âˆˆğ‘…\n1\nğ‘˜+ğ‘Ÿ(ğ‘‘) (1)\nwhere ğ‘…denotes a set of rankings, ğ‘Ÿ(ğ‘‘)denotes the rank of a docu-\nments ğ‘‘from a set of documentsğ·in a particular rankingğ‘Ÿ, andğ‘˜is\na free parameter set to60 to comply with earlier work. Even though\n2https://platform.openai.com/docs/models/gpt-4o\n3Based on US$15.00 / 1M tokens as of July 2024.\nit is a straightforward method, it demonstrated its effectiveness over\nlearning to rank approaches and other data fusion methods. We note\nthat there are other probabilistic data fusion methods [4, 35, 46, 53]\nthat are effective but require additional fine-tuning or optimiza-\ntion with training data. Opposed to these methods, RRF can be\nused off-the-shelf, which is intentional as the methodology is in-\ntended to be unsupervised. Besides, we acknowledge its widespread\napplication and use in several studies [10, 19, 31, 36, 44, 51], also\nbeing implemented in the popular open-source software toolkit\nElasticsearch.4\n3.3 Datasets and Implementation Details\nOur experiments use four TREC newswire test collections including\nTREC Disks 4 & 5 (minus congressional records)5 used as part of\nTREC Robust 2004 (Robust04) [48], the AQUAINT Corpus of Eng-\nlish News Text6 used as part of TREC Robust 2005 (Robust05) [49],\nthe New York Times Annotated Corpus 7 used as part of TREC\nCommon Core 2017 (Core17) [3], and the TREC Washington Post\nCorpus8 used as part of TREC Common Core 2018 (Core18) [50].\nOur implementations are based on a set of state-of-the-art re-\ntrieval and evaluation toolkits including Pyterrier [ 39], building\nupon the Terrier toolkit [42], ir_datasets [38], and ranx [9, 10].\nFor the indexing, we use ir_datasets as the toolkit provides bind-\nings for all four datasets. For the retrieval, we rely on BM25 with\ndefault settings as implemented in Pyterrier/Terrier. All of the dif-\nferent data fusion methods are based on ranx.fuse [10] which is\nan extension to ranx [9]. Likewise, we use ranx to evaluate the\nexperiments.\n4 Experimental Results\nIn our experimental evaluations, we consider two baseline retrieval\nmethods, namely BM25 and BM25 + RM3. The BM25 baseline rank-\nings are retrieved with the topicâ€™s title as provided. BM25 + RM3 is\na pseudo-relevance feedback method that builds upon expanding\nthe initial query after retrieving a first ranking for determining ex-\npansion terms. After the initial ranking is retrieved with the topicâ€™s\ntitle, an additional (and final) ranking is retrieved with the initial\nquery and expansion terms based on RM3.\nTable 1 shows the retrieval effectiveness in terms of P@10,\nnDCG@10, Bpref, and MAP evaluated with four newswire datasets.\nFor all fused rankings, we use a set of ten rankings based on the\nfirst ten synthetic queries that are combined with RRF. In principle,\nit is possible to generate an arbitrary number of queries. However,\nin practice, efficiency tradeoffs have to be made when generating\nqueries with an LLM on the fly. Thus, we consider ten queries as a\nreasonable number for evaluating the effectiveness benefits without\nimposing too much latency caused by the query generation.\nIn general, the fused rankings outperform the baselines BM25 (+\nRM3) in most cases, i.e., the measures have higher but not necessar-\nily significantly better scores. Depending on the prompting strategy,\nsome fused rankings are more effective than others. However, we\nalso note that for some measures and test collections, especially, P-1\n4https://www.elastic.co/guide/en/elasticsearch/reference/8.15/rrf.html\n5https://trec.nist.gov/data/cd45/\n6https://catalog.ldc.upenn.edu/LDC2002T31\n7https://catalog.ldc.upenn.edu/LDC2008T19\n8https://trec.nist.gov/data/wapost/\nSIGIR-AP â€™24, December 9â€“12, 2024, Tokyo, Japan Timo Breuer\nTable 1: Comparison of fused rankings with synthetic queries\nbased on different prompting strategies. Superscripts denote\nsignificant differences in paired Studentâ€™s t-test (with Bon-\nferroni correction applied). The best results are bold-faced.\n# Prompt P@10 NDCG@10 BPref MAP\nCore17\na BM25 0.458 0.372 0.274 0.199\nb BM25 + RM3 0.534ğ‘ 0.404 0.317ğ‘ 0.246ğ‘\nc P-1 0.526ğ‘ 0.426ğ‘ 0.355ğ‘ğ‘ 0.240ğ‘\nd P-2 0.618ğ‘ğ‘ 0.522ğ‘ğ‘ğ‘ 0.416ğ‘ğ‘ğ‘ğ‘’ 0.299ğ‘ğ‘ğ‘ğ‘’\ne P-3 0.570ğ‘ 0.453ğ‘ 0.376ğ‘ğ‘ 0.255ğ‘\nCore18\na BM25 0.426 0.389 0.253 0.191\nb BM25 + RM3 0.448 0.396 0.282ğ‘ 0.229ğ‘\nc P-1 0.452 0.433 0.294 0.233ğ‘\nd P-2 0.532ğ‘ğ‘ğ‘’ 0.497ğ‘ğ‘ğ‘ğ‘’ 0.339ğ‘ğ‘ğ‘ 0.270ğ‘ğ‘\ne P-3 0.440 0.420 0.312ğ‘ 0.240ğ‘\nRobust04\na BM25 0.410 0.421 0.241 0.228\nb BM25 + RM3 0.443ğ‘ 0.443ğ‘ 0.268ğ‘ 0.262ğ‘\nc P-1 0.455ğ‘ 0.471ğ‘ 0.269ğ‘ 0.256ğ‘\nd P-2 0.514ğ‘ğ‘ğ‘ğ‘’ 0.535ğ‘ğ‘ğ‘ğ‘’ 0.304ğ‘ğ‘ğ‘ğ‘’ 0.295ğ‘ğ‘ğ‘ğ‘’\ne P-3 0.459ğ‘ 0.480ğ‘ğ‘ 0.276ğ‘ 0.265ğ‘\nRobust05\na BM25 0.352 0.296 0.227 0.176\nb BM25 + RM3 0.408 0.329 0.251 0.209ğ‘\nc P-1 0.396 0.343 0.267ğ‘ 0.206\nd P-2 0.522ğ‘ğ‘ 0.442ğ‘ 0.361ğ‘ğ‘ğ‘ğ‘’ 0.267ğ‘ğ‘\ne P-3 0.492ğ‘ğ‘ 0.404ğ‘ğ‘ 0.303ğ‘ğ‘ğ‘ 0.236ğ‘ğ‘\nis outperformed by BM25 + RM3. Still, all fused rankings perform\nbetter than BM25 rankings based on a single query.\nFor all four test collections, fused rankings with queries based on\nthe prompting strategy P-2 perform best, suggesting that there is a\npositive effect on retrieval effectiveness when including additional\ninformation about the topic (based on the description and narrative)\nin the prompt given to the LLM. In most cases, the P-2 results are\nsignificantly better than the baseline methods and the other two\nprompting strategies.\nRegarding BPref, we see that P-2 outperforms each other method\nwith statistical significance (except for Core18, for which P-2 has\nbetter but not significantly different effectiveness than P-3). These\noutcomes suggest that many unjudged documents are contained\nin the rankings, which is typical when using query variants. Other\ntopically related queries can bring up potentially relevant docu-\nments that were not considered in the pooling of the test collection\ncreation process.\nIn comparison, the other prompting strategies P-1 and P-3 are\nless effective. While both of them outperform the BM25 baseline\nrankings, sometimes pseudo-relevance feedback with BM25 + RM3\nis more effective than fusing rankings with queries of P-1, which\nsuggests that simply providing the title string in the prompt is not\nenough to guide the LLM to produce effective and topically-related\nqueries. More specifically, fused rankings with queries based on\nTopics\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n nDCG\nCore17\n3 queries\n5 queries\n10 queries\n100 queries\nTopics\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n nDCG\nCore18\n3 queries\n5 queries\n10 queries\n100 queries\nTopics\n0.8\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n0.8\n nDCG\nRobust04\n3 queries\n5 queries\n10 queries\n100 queries\nTopics\n0.6\n0.4\n0.2\n0.0\n0.2\n0.4\n0.6\n nDCG\nRobust05\n3 queries\n5 queries\n10 queries\n100 queries\nFigure 2: Retrieval effectiveness with different numbers of\nsynthetic queries based on P-2 . The plots show the\nrelative improvements in terms of Î” nDCG. For each topic\nof the fused rankings, the difference to the baseline (BM25\nwith the topicâ€™s title as the query) is determined with the\nfour newswire benchmarks Core17/18 and Robust04/05.\nP-1 are less effective than BM25 + RM3 wrt. P@10 and MAP when\nevaluated with Core17 and Robust05. Even though less effective\nthan P-2, the P-3 rankings are, in most cases, significantly better\nthan BM25 and have higher scores than BM25 + RM3.\nAs generating synthetic query variants with LLMs is fairly cheap\nand unlimited in principle, an arbitrary number of queries for\ndata fusion can be considered. To this end, Figure 2 compares the\nÎ” nDCG of the topic score distribution for fused rankings made\nwith either 3, 5, 10, or 100 synthetic queries based on P-2. More\nspecifically, we determine Î” nDCG by the difference between the\ntopic score of the fused ranking and that of the BM25 baseline rank-\ning with a cutoff of 1,000 documents per ranking. Figure 2 shows\nthese differences between the topic scores in decreasing order.\nAn increasing number of synthetic queries and fused rankings\nhave a positive impact on the retrieval effectiveness. For instance,\nusing more than three synthetic queries is always preferable for\nthose topics that improve. However, the comparison of fused rank-\nings based on ten and one hundred queries suggests that there are\nlimits to this kind of improvement. Too many queries might cause\ntopic drifts that eventually will not lead to any further improve-\nments. In the future, a better understanding of how many synthetic\nqueries are beneficial helps to leverage the full potential of the pro-\nposed method. As most of the Î” nDCG scores are above the dashed\nline, fusing rankings generally positively impacts the effectiveness,\nas already concluded from the results in Table 1. However, there\nare some topics for which the retrieval effectiveness deteriorates,\nleaving room for further improvements.\nData Fusion of Synthetic Query Variants With Generative LLMs SIGIR-AP â€™24, December 9â€“12, 2024, Tokyo, Japan\n5 Discussion\nOverall, our experiments let us conclude that including the addi-\ntional information, given by the topic description and narrative, in\nthe prompt is beneficial for guiding the LLM to produce topically\nrelated effective queries for data fusion, which provides several\ninteresting directions for future work.\nHow can such a method be put into practice?In contrast\nto test collection-based retrieval experiments, real search sessions\noften do not have context-rich information as provided in the topic\nfiles. However, we envision a search application in which the users\ninteract with a conversational search agent [54] to which the infor-\nmation need is expressed. Throughout the conversation, the search\nagent, i.e., the LLM, would then generate query variants, relieving\nthe users from the cognitive effort of formulating effective search\nqueries. The fused rankings can then be shown directly to the users\nor serve as the retrieval component of aRAG system [33] to support\nthe generated response with evidence. Furthermore, interactive con-\nversations enable the integration of feedback loops with users and\nthe agent taking turns. Users can then provide explicit feedback on\nthe system outputs, helping the LLM to adapt its query generation\nstrategy throughout the conversation.\nWhy not using theLLM directly to rerank the documents?\nEarlier work suggests that LLMs can reliably identify relevant doc-\numents [47] and also rerank item lists for better retrieval effective-\nness [37]. However, this is usually more costly, as the LLM has\nto read more tokens than in our methodology. In addition to the\nprompt with the task and topic information, each document would\nneed to be read in its entirety by the LLM. Our approach uses the\nLLM to generate short keyword queries, letting BM25 rank the\ndocuments with computationally lower costs. Depending on the\nLLM, the length of input tokens is also a limitation, which can be\ncritical for lengthy news articles like in our experimental setup. The\ndocument length does not matter for our proposed methodology.\nIn this regard, future work should aim for a better understanding\nof the cost-effectiveness tradeoffs and also consider performance in\ngeneral, as the traditional retrieval methods could provide efficiency\nbenefits in interactive conversations with a user. For instance, com-\nparing both computational and financial costs alongside retrieval\nperformance (including effectiveness and efficiency) will give in-\nsights to leverage the proposed method optimally.\nBeyond data fusion, the provided query datasets could be ex-\nploited in user simulations. For example, the synthetic queries can\nserve as reformulations in a simulated search session. Future work\nshould, therefore, aim to validate the fidelity wrt. real user query\nvariants [18]. The dataset by Benham and Culpepper [13] provides\na perfect reference resource with real user queries for these endeav-\nors. Similarly, these query datasets could also be used for analyzing\nalternative ways of pooling, where the focus shifts from system\nvariability toward query variability [40], following the principle of\nquery polyrepresentation [26]. From an ethical point of view, we\nnote that any potential biases in the training data of the LLM could\nalso impact the generated queries. Future work should, therefore,\nnot only compare the query variants to real queries but also evalu-\nate if there are any biases in the query formulations, which would\ninevitably affect rankings and any other follow-up system outputs.\n6 Limitations\nOur experimental evaluations suggest that fusing rankings with\ngenerated query variants led to a decline in retrieval effectiveness\nfor certain topics (cf. Figure 2) if compared to a baseline with the\ntopicâ€™s title as the query. A more in-depth understanding of the\ndeteriorating topics could help to mitigate the loss of retrieval ef-\nfectiveness and it is necessary to analyze for what kinds of topics\nand why the proposed methodology improves or deteriorates re-\ntrieval results. In this regard, potential topic drifts caused by the\nsynthesized queries should also be considered.\nEven though the core methodology is straightforward from a\nhigh-level perspective, there is a huge amount of different methods,\ntools and resources available to implement it. For instance, other\nmore open instruction-following LLMs [34] could be leveraged for\na more transparent and reproducible experimental setup. Similarly,\nother kinds of retrieval methods, like dense retrievers or different\ndata fusion methods, should also be evaluated to gain better in-\nsights into the generalizability of the approach. Smaller LLMs or\nmodels with lower quantization levels also offer promising ways to\nmake the query generation possible in less hardware-demanding\napplications, and the effectiveness tradeoffs need to be evaluated.\nLikewise, there are also many opportunities for systematic anal-\nysis of the query generation that is based on prompting the LLM.\nFor instance, the temperature parameter, which was kept fixed in\nour experiments, could lead to different query outputs that very\nlikely have an impact on the end-to-end retrieval performance. In\nour experiments, we guided the LLM with real query variants from\nusers, which did not yield better effectiveness than the prompts\nmaking use of the topicâ€™s title, description, and narrative. More\nin-depth analysis with more query logs is required to fully lever-\nage the potential of keyword-based queries for chain-of-thought\nprompting. Other (more detailed) prompting strategies could also be\nconsidered, and it should be evaluated how how well the prompting\ncan instruct the model to produce the desired query outputs.\n7 Conclusion\nMaking use of query variants when searching for documents im-\nproves retrieval effectiveness, especially when it is operational-\nized with data fusion algorithms. This work analyzed if generative\ninstruction-tuned LLMs can synthesize effective query variants\nfor data fusion. In our experiments, instruction-tuned LLMs could\nindeed generate suitable query variants that significantly improve\nretrieval effectiveness when the LLM is guided by the additional\ntopic information obtained from the topicâ€™s description and nar-\nrative. We conclude that additional topical context information is\nkey to fully leveraging the LLMâ€™s capability to generate effective\nqueries for data fusion.\nAcknowledgements\nWe thank the anonymous reviewers for their valuable feedback\non this work. This contribution has been developed in the project\nPLan_CV. Within the funding programme FH-Personal, the project\nPLan_CV (reference number 03FHP109) is funded by the German\nFederal Ministry of Education and Research (BMBF) and Joint Sci-\nence Conference (GWK).\nSIGIR-AP â€™24, December 9â€“12, 2024, Tokyo, Japan Timo Breuer\nReferences\n[1] Marwah Alaofi, Luke Gallagher, Dana McKay, Lauren L. Saling, Mark Sanderson,\nFalk Scholer, Damiano Spina, and Ryen W. White. 2022. Where Do Queries Come\nFrom?. In SIGIR. ACM, 2850â€“2862.\n[2] Marwah Alaofi, Luke Gallagher, Mark Sanderson, Falk Scholer, and Paul Thomas.\n2023. Can Generative LLMs Create Query Variants for Test Collections? An\nExploratory Study. In SIGIR. ACM, 1869â€“1873.\n[3] James Allan, Donna Harman, Evangelos Kanoulas, Dan Li, Christophe Van Gysel,\nand Ellen M. Voorhees. 2017. TREC 2017 Common Core Track Overview. In\nTREC (NIST Special Publication, Vol. 500â€“324) . NIST.\n[4] Javed A. Aslam and Mark H. Montague. 2001. Models for Metasearch. In SIGIR.\nACM, 275â€“284.\n[5] Leif Azzopardi, Maarten de Rijke, and Krisztian Balog. 2007. Building Simulated\nQueries for Known-Item Topics: An Analysis Using Six European Languages. In\nSIGIR. ACM, 455â€“462.\n[6] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2015. User Variability\nand IR System Evaluation. In SIGIR. ACM, 625â€“634.\n[7] Peter Bailey, Alistair Moffat, Falk Scholer, and Paul Thomas. 2016. UQV100: A\nTest Collection with Query Variability. InSIGIR. ACM, 725â€“728.\n[8] Feza Baskaya, Heikki Keskustalo, and Kalervo JÃ¤rvelin. 2013. Modeling Behavioral\nFactors in Interactive Information Retrieval. In CIKM. ACM, 2297â€“2302.\n[9] Elias Bassani. 2022. ranx: A Blazing-Fast Python Library for Ranking Evaluation\nand Comparison. In ECIR (2) (Lecture Notes in Computer Science, Vol. 13186) .\nSpringer, 259â€“264.\n[10] Elias Bassani and Luca Romelli. 2022. ranx.fuse: A Python Library for\nMetasearch. In CIKM. ACM, 4808â€“4812.\n[11] Nicholas J. Belkin, Colleen Cool, W. Bruce Croft, and James P. Callan. 1993. Effect\nof Multiple Query Representations on Information Retrieval System Performance.\nIn SIGIR. ACM, 339â€“346.\n[12] Nicholas J. Belkin, Paul B. Kantor, Edward A. Fox, and Joseph A. Shaw. 1995.\nCombining the Evidence of Multiple Query Representations for Information\nRetrieval. 31, 3 (1995), 431â€“448.\n[13] Rodger Benham and J. Shane Culpepper. 2017. Risk-Reward Trade-Offs in Rank\nFusion. In ADCS. ACM, 1:1â€“1:8.\n[14] Rodger Benham, J. Shane Culpepper, Luke Gallagher, Xiaolu Lu, and Joel M.\nMackenzie. 2018. Towards Efficient and Effective Query Variant Generation. In\nDESIRES (CEUR Workshop Proceedings, Vol. 2167) . CEUR-WS.org, 62â€“67.\n[15] Rodger Benham, Luke Gallagher, Joel M. Mackenzie, Tadele Tedla Damessie, Ruey-\nCheng Chen, Falk Scholer, Alistair Moffat, and J. Shane Culpepper. 2017. RMIT\nat the 2017 TREC CORE Track. In TREC (NIST Special Publication, Vol. 500â€“324) .\nNIST.\n[16] Rodger Benham, Luke Gallagher, Joel M. Mackenzie, Binsheng Liu, Xiaolu Lu,\nFalk Scholer, J. Shane Culpepper, and Alistair Moffat. 2018. RMIT at the 2018\nTREC CORE Track. In TREC (NIST Special Publication, Vol. 500â€“331) . NIST.\n[17] Rodger Benham, Joel M. Mackenzie, Alistair Moffat, and J. Shane Culpepper. 2019.\nBoosting Search Performance Using Query Variations. ACM Trans. Inf. Syst. 37,\n4 (2019), 41:1â€“41:25.\n[18] Timo Breuer, Norbert Fuhr, and Philipp Schaer. 2022. Validating Simulations of\nUser Query Variants. In ECIR (1) (Lecture Notes in Computer Science, Vol. 13185) .\nSpringer, 80â€“94.\n[19] Timo Breuer, Christin Katharina Kreutz, Philipp Schaer, and Dirk Tunger. 2023.\nBibliometric Data Fusion for Biomedical Information Retrieval. In JCDL. IEEE,\n107â€“118.\n[20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan,\nPrafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan,\nRewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter,\nChristopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin\nChess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya\nSutskever, and Dario Amodei. 2020. Language Models Are Few-Shot Learners. In\nNeurIPS.\n[21] Anirban Chakraborty, Debasis Ganguly, and Owen Conlan. 2020. Retrievability\nBased Document Selection for Relevance Feedback with Automatically Generated\nQuery Variants. In CIKM. ACM, 125â€“134.\n[22] Kevyn Collins-Thompson, Paul N. Bennett, Fernando Diaz, Charlie Clarke, and\nEllen M. Voorhees. 2013. TREC 2013 Web Track Overview. InTREC (NIST Special\nPublication, Vol. 500â€“302) . NIST.\n[23] Kevyn Collins-Thompson, Craig Macdonald, Paul N. Bennett, Fernando Diaz,\nand Ellen M. Voorhees. 2014. TREC 2014 Web Track Overview. InTREC (NIST\nSpecial Publication, Vol. 500â€“308) . NIST.\n[24] Gordon V. Cormack, Charles L. A. Clarke, and Stefan BÃ¼ttcher. 2009. Reciprocal\nRank Fusion Outperforms Condorcet and Individual Rank Learning Methods. In\nSIGIR. ACM, 758â€“759.\n[25] Zhuyun Dai, Vincent Y. Zhao, Ji Ma, Yi Luan, Jianmo Ni, Jing Lu, Anton Bakalov,\nKelvin Guu, Keith B. Hall, and Ming-Wei Chang. 2023. Promptagator: Few-shot\nDense Retrieval From 8 Examples. In ICLR. OpenReview.net.\n[26] Miles Efron and Megan A. Winget. 2010. Query polyrepresentation for ranking\nretrieval systems without relevance judgments. J. Assoc. Inf. Sci. Technol. 61, 6\n(2010), 1081â€“1091.\n[27] Mitko Gospodinov, Sean MacAvaney, and Craig Macdonald. 2023. Doc2Query--:\nWhen Less Is More. In ECIR (2) (Lecture Notes in Computer Science, Vol. 13981) .\nSpringer, 414â€“422.\n[28] Peter Ingwersen. 1996. Cognitive Perspectives of Information Retrieval Interac-\ntion: Elements of a Cognitive IR Theory. J. Documentation 52, 1 (1996), 3â€“50.\n[29] Rolf Jagerman, Honglei Zhuang, Zhen Qin, Xuanhui Wang, and Michael Ben-\ndersky. 2023. Query Expansion by Prompting Large Language Models. CoRR\nabs/2305.03653 (2023).\n[30] Chris Jordan, Carolyn R. Watters, and Qigang Gao. 2006. Using Controlled Query\nGeneration to Evaluate Blind Relevance Feedback Algorithms. In JCDL. ACM,\n286â€“295.\n[31] Ekaterina Khramtsova, Shengyao Zhuang, Mahsa Baktashmotlagh, and Guido\nZuccon. 2024. Leveraging LLMs for Unsupervised Dense Retriever Ranking. In\nSIGIR. ACM, 1307â€“1317.\n[32] Birger Larsen, Peter Ingwersen, and Berit Lund. 2009. Data fusion according\nto the principle of polyrepresentation. J. Assoc. Inf. Sci. Technol. 60, 4 (2009),\n646â€“654.\n[33] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir\nKarpukhin, Naman Goyal, Heinrich KÃ¼ttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntÃ¤schel, Sebastian Riedel, and Douwe Kiela. 2020. Retrieval-Augmented Genera-\ntion for Knowledge-Intensive NLP Tasks. In NeurIPS.\n[34] Andreas Liesenfeld and Mark Dingemanse. 2024. Rethinking open source gener-\native AI: open washing and the EU AI Act. In FAccT. ACM, 1774â€“1787.\n[35] David Lillis, Fergus Toolan, Rem W. Collier, and John Dunnion. 2006. ProbFuse:\nA Probabilistic Approach to Data Fusion. In SIGIR. ACM, 139â€“146.\n[36] Sheng-Chieh Lin, Jheng-Hong Yang, Rodrigo Frassetto Nogueira, Ming-Feng\nTsai, Chuan-Ju Wang, and Jimmy Lin. 2021. Multi-Stage Conversational Passage\nRetrieval: An Approach to Fusing Term Importance Estimation and Neural Query\nRewriting. ACM Trans. Inf. Syst. 39, 4 (2021), 48:1â€“48:29.\n[37] Sean MacAvaney and Luca Soldaini. 2023. One-Shot Labeling for Automatic\nRelevance Estimation. In SIGIR. ACM, 2230â€“2235.\n[38] Sean MacAvaney, Andrew Yates, Sergey Feldman, Doug Downey, Arman Cohan,\nand Nazli Goharian. 2021. Simplified Data Wrangling with ir_datasets. In\nSIGIR. ACM, 2429â€“2436.\n[39] Craig Macdonald, Nicola Tonellotto, Sean MacAvaney, and Iadh Ounis. 2021.\nPyTerrier: Declarative Experimentation in Python from BM25 to Dense Retrieval.\nIn CIKM. ACM, 4526â€“4533.\n[40] Alistair Moffat, Falk Scholer, Paul Thomas, and Peter Bailey. 2015. Pooled Evalu-\nation over Query Variations: Users Are as Diverse as Systems. InCIKM. ACM,\n1759â€“1762.\n[41] Rodrigo Frassetto Nogueira, Wei Yang, Jimmy Lin, and Kyunghyun Cho. 2019.\nDocument Expansion by Query Prediction. CoRR abs/1904.08375 (2019).\n[42] Iadh Ounis, Gianni Amati, Vassilis Plachouras, Ben He, Craig Macdonald, and\nDouglas Johnson. 2005. Terrier Information Retrieval Platform. In ECIR (Lecture\nNotes in Computer Science, Vol. 3408) . Springer, 517â€“519.\n[43] Gustavo Penha, Arthur CÃ¢mara, and Claudia Hauff. 2022. Evaluating the Robust-\nness of Retrieval Pipelines with Query Variation Generators. In ECIR (1) (Lecture\nNotes in Computer Science, Vol. 13185) . Springer, 397â€“412.\n[44] Alireza Salemi, Surya Kallumadi, and Hamed Zamani. 2024. Optimization Meth-\nods for Personalizing Large Language Models through Retrieval Augmentation.\nIn SIGIR. ACM, 752â€“762.\n[45] Mark Sanderson. 2010. Test Collection Based Evaluation of Information Retrieval\nSystems. Found. Trends Inf. Retr. 4, 4 (2010), 247â€“375.\n[46] Milad Shokouhi. 2007. Segmentation of Search Engine Results for Effective\nData-Fusion. In ECIR (Lecture Notes in Computer Science, Vol. 4425) . Springer,\n185â€“197.\n[47] Paul Thomas, Seth Spielman, Nick Craswell, and Bhaskar Mitra. 2024. Large\nLanguage Models can Accurately Predict Searcher Preferences. In SIGIR. ACM,\n1930â€“1940.\n[48] Ellen M. Voorhees. 2004. Overview of the TREC 2004 Robust Track. In TREC\n(NIST Special Publication, Vol. 500â€“261) . NIST.\n[49] Ellen M. Voorhees. 2006. The TREC 2005 Robust Track. SIGIR Forum 40, 1 (2006),\n41â€“48.\n[50] Ellen M. Voorhees and Angela Ellis (Eds.). [n. d.]. Proceedings of the Twenty-\nSeventh Text REtrieval Conference, TREC 2018, Gaithersburg, Maryland, USA, No-\nvember 14-16, 2018 . NIST Special Publication, Vol. 500-331. NIST.\n[51] Nikos Voskarides, Dan Li, Pengjie Ren, Evangelos Kanoulas, and Maarten de Rijke.\n2020. Query Resolution for Conversational Search with Limited Supervision. In\nSIGIR. ACM, 921â€“930.\n[52] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Ed H. Chi, Quoc Le,\nand Denny Zhou. 2022. Chain-of-Thought Prompting Elicits Reasoning in Large\nLanguage Models. CoRR abs/2201.11903 (2022).\n[53] Shengli Wu and Fabio Crestani. 2002. Data Fusion with Estimated Weights. In\nCIKM. ACM, 648â€“651.\n[54] Hamed Zamani, Johanne R. Trippas, Jeff Dalton, and Filip Radlinski. 2023. Con-\nversational Information Seeking. Found. Trends Inf. Retr. 17, 3-4 (2023), 244â€“456.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8484412431716919
    },
    {
      "name": "Ranking (information retrieval)",
      "score": 0.694098949432373
    },
    {
      "name": "Query expansion",
      "score": 0.6571517586708069
    },
    {
      "name": "Relevance (law)",
      "score": 0.5909361839294434
    },
    {
      "name": "Information retrieval",
      "score": 0.5882817506790161
    },
    {
      "name": "Exploit",
      "score": 0.56385338306427
    },
    {
      "name": "Query language",
      "score": 0.5586985349655151
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5245388150215149
    },
    {
      "name": "Web query classification",
      "score": 0.5196060538291931
    },
    {
      "name": "Query optimization",
      "score": 0.45435938239097595
    },
    {
      "name": "RDF query language",
      "score": 0.4515293836593628
    },
    {
      "name": "Code (set theory)",
      "score": 0.450104683637619
    },
    {
      "name": "Web search query",
      "score": 0.4219788610935211
    },
    {
      "name": "Sargable",
      "score": 0.41441810131073
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3550034761428833
    },
    {
      "name": "Search engine",
      "score": 0.2031213939189911
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Computer security",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I102520234",
      "name": "TH KÃ¶ln - University of Applied Sciences",
      "country": "DE"
    }
  ]
}