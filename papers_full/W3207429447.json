{
    "title": "Large Language Models Can Be Strong Differentially Private Learners",
    "url": "https://openalex.org/W3207429447",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2062123224",
            "name": "Li, Xuechen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3163579884",
            "name": "Tramèr, Florian",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3166964279",
            "name": "Liang, Percy",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2595035019",
            "name": "Hashimoto, Tatsunori",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3130198422",
        "https://openalex.org/W2805029710",
        "https://openalex.org/W3037026762",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3168867926",
        "https://openalex.org/W2964303773",
        "https://openalex.org/W3038129124",
        "https://openalex.org/W2943912735",
        "https://openalex.org/W3115920135",
        "https://openalex.org/W3119438769",
        "https://openalex.org/W3203254411",
        "https://openalex.org/W2963912046",
        "https://openalex.org/W3152956381",
        "https://openalex.org/W1985511977",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2535690855",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W3031914912",
        "https://openalex.org/W3126575712",
        "https://openalex.org/W3153675281",
        "https://openalex.org/W2971869958",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W3166958905",
        "https://openalex.org/W3172022218",
        "https://openalex.org/W3188836194",
        "https://openalex.org/W2594311007",
        "https://openalex.org/W3135471658",
        "https://openalex.org/W3173777717",
        "https://openalex.org/W3166854338",
        "https://openalex.org/W3128316891",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3118146262",
        "https://openalex.org/W2104743167",
        "https://openalex.org/W2913443447",
        "https://openalex.org/W2963825865",
        "https://openalex.org/W2889507104",
        "https://openalex.org/W2964296660",
        "https://openalex.org/W1937834619",
        "https://openalex.org/W2967880504",
        "https://openalex.org/W2899771611",
        "https://openalex.org/W3166140588",
        "https://openalex.org/W3034916338",
        "https://openalex.org/W2027595342",
        "https://openalex.org/W2473418344",
        "https://openalex.org/W2946930197",
        "https://openalex.org/W3087503988",
        "https://openalex.org/W3176828726",
        "https://openalex.org/W3167675683",
        "https://openalex.org/W2889009749",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W3098071563",
        "https://openalex.org/W3125098139",
        "https://openalex.org/W3039127676",
        "https://openalex.org/W2887995258",
        "https://openalex.org/W3127861905",
        "https://openalex.org/W2964121744",
        "https://openalex.org/W3102554603",
        "https://openalex.org/W3190860428",
        "https://openalex.org/W1873763122",
        "https://openalex.org/W2963572446",
        "https://openalex.org/W2785361959",
        "https://openalex.org/W3092700804",
        "https://openalex.org/W3157988161",
        "https://openalex.org/W2266471546",
        "https://openalex.org/W3188505388",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W2999251207",
        "https://openalex.org/W3214715529",
        "https://openalex.org/W3102378604",
        "https://openalex.org/W3167530662",
        "https://openalex.org/W3137695714",
        "https://openalex.org/W2969261410",
        "https://openalex.org/W2784621220",
        "https://openalex.org/W3045008227",
        "https://openalex.org/W2810840719",
        "https://openalex.org/W2938704169",
        "https://openalex.org/W2541884796",
        "https://openalex.org/W3083528917",
        "https://openalex.org/W3198002980"
    ],
    "abstract": "Differentially Private (DP) learning has seen limited success for building large deep learning models of text, and straightforward attempts at applying Differentially Private Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in large performance drops and high computational overhead. We show that this performance drop can be mitigated with (1) the use of large pretrained language models; (2) non-standard hyperparameters that suit DP optimization; and (3) fine-tuning objectives which are aligned with the pretraining procedure. With the above, we obtain NLP models that outperform state-of-the-art DP-trained models under the same privacy budget and strong non-private baselines -- by directly fine-tuning pretrained models with DP optimization on moderately-sized corpora. To address the computational challenge of running DP-SGD with large Transformers, we propose a memory saving technique that allows clipping in DP-SGD to run without instantiating per-example gradients for any linear layer in the model. The technique enables privately training Transformers with almost the same memory cost as non-private training at a modest run-time overhead. Contrary to conventional wisdom that DP optimization fails at learning high-dimensional models (due to noise that scales with dimension) empirical results reveal that private learning with pretrained language models doesn't tend to suffer from dimension-dependent performance degradation. Code to reproduce results can be found at https://github.com/lxuechen/private-transformers.",
    "full_text": "Published as a conference paper at ICLR 2022\nLARGE LANGUAGE MODELS CAN BE\nSTRONG DIFFERENTIALLY PRIVATE LEARNERS\nXuechen Li1, Florian Tram`er2, Percy Liang1, Tatsunori Hashimoto1\n1Stanford University 2Google Research\n{lxuechen,tramer,pliang}@cs.stanford.edu, thashim@stanford.edu\nABSTRACT\nDifferentially Private (DP) learning has seen limited success for building large deep\nlearning models of text, and straightforward attempts at applying Differentially\nPrivate Stochastic Gradient Descent (DP-SGD) to NLP tasks have resulted in\nlarge performance drops and high computational overhead. We show that this\nperformance drop can be mitigated with (1) the use of large pretrained language\nmodels; (2) non-standard hyperparameters that suit DP optimization; and (3) ﬁne-\ntuning objectives which are aligned with the pretraining procedure. With the above,\nwe obtain NLP models that outperform state-of-the-art DP-trained models under\nthe same privacy budget and strong non-private baselines—by directly ﬁne-tuning\npretrained models with DP optimization on moderately-sized corpora. To address\nthe computational challenge of running DP-SGD with large Transformers, we\npropose a memory saving technique that allows clipping in DP-SGD to run without\ninstantiating per-example gradients for any linear layer in the model. The technique\nenables privately training Transformers with almost the same memory cost as\nnon-private training at a modest run-time overhead. Contrary to conventional\nwisdom that DP optimization fails at learning high-dimensional models (due to\nnoise that scales with dimension) empirical results reveal that private learning\nwith pretrained language models doesn’t tend to suffer from dimension-dependent\nperformance degradation. Code to reproduce results can be found at https:\n//github.com/lxuechen/private-transformers.\n1 I NTRODUCTION\nMachine learning systems trained on sensitive user data can be vulnerable to privacy attacks (Shokri\net al., 2017; Hayes et al., 2019). This issue is especially pressing for recent applications of large\nlanguage models, as these models are capable of memorizing and reconstructing sensitive examples\ncontained in the training data (Zhang et al., 2016; Carlini et al., 2020).\nAs a result of these concerns, there has been a large interest in developing methods that provide data\nprivacy guarantees for large language models. The standard paradigm for providing such a guarantee\nin machine learning is Differential Privacy (DP) (Dwork et al., 2006; 2014). Unfortunately, DP\nlearning has typically struggled to produce useful models when applied to large language models,\nresulting in models with either vacuous privacy guarantees (Dupuy et al., 2021) or performance\nfar below non-private baselines. This is widely attributed to the fact that the core primitive of\nDifferentially Private Stochastic Gradient Descent (DP-SGD) (Song et al., 2013; Bassily et al., 2014;\nAbadi et al., 2016) injects noise that must scale with the number of parameters, resulting in large\nnoise levels for large language models (Yu et al., 2021b).\nWe tackle the problem of building performant DP language models for sentence classiﬁcation and\nlanguage generation tasks with merely tens to hundreds of thousands of examples. We pursue this\ngoal by re-examining the performance of the baseline DP optimization algorithm for ﬁne-tuning\nlarge language models, and study how choices of hyperparameters, training objective, and pretrained\nmodels affect the performance given ﬁxed privacy budgets. In contrast to the mainstream perception,\nour empirical results demonstrate that large pretrained models with hundreds of millions of\nparameters can be effectively and efﬁciently ﬁne-tuned to yield models with high performance\nunder modest privacy leakage. For sentence classiﬁcation, the performance of our ﬁne-tuned\n1\narXiv:2110.05679v6  [cs.LG]  10 Nov 2022\nPublished as a conference paper at ICLR 2022\n50 100 150 200 250 300\nnumber of non-embedding parameters (millions)\n74\n76\n78\n80\n82\n84\n86\n88MNLI-m dev set accuracy\ndistill\nbase\nlarge\ndistill\nbase\nlarge\nBERT family ( = 3)\nRoBERTa family ( = 3)\nTextHide (m, k) = (256, 4) (BERT-base)\nTextHide (m, k) = (256, 4) (RoBERTa-base)\nnon-private BERT-base (Devlin et al., 2018)\nnon-private RoBERTa-base (Liu et al., 2019)\n(a) Sentence classiﬁcation\nMNLI-matched (Williams et al., 2018)\n100 200 300 400 500 600 700\nnumber of non-embedding parameters (millions)\n60\n62\n64\n66\n68E2E test set BLEU\nDistilGPT2\nGPT-2\nGPT-2-medium\nGPT-2-large\nGPT-2 ( = 3)\nGPT-2 ( = 8)\nnon-private T-GEN (D & J, 2016)\nnon-private fine-tuned GPT-2\n(b) Natural language generation\nE2E (Novikova et al., 2017)\nFigure 1: A summary of a few of our ﬁndings: (1) Pretrained models ﬁne-tuned with DP-Adam has\nstrong performance. (2) Fine-tuning larger models produces better results. (3) Fine-tuned RoBERTa-\nlarge under DP at ϵ= 3 outperforms TextHide (the extension of InstaHide (Huang et al., 2020b) for\ntext classiﬁcation) with BERT-base. Non-private generation baseline numbers are based on those\nreported by Wiseman et al. (2018).\nmodels surpasses those obtained under heuristic privacy notions (Huang et al., 2020a) which do not\npossess formal privacy guarantees. For text generation, the performance of our models surpasses\nstrong non-private baselines. Figure 1 illustrates some of these results and the overall scaling behavior.\nWe summarize our contributions below.\n(1) We show that with appropriate hyperparameters and downstream task objectives, ﬁne-tuning\npretrained language models with DP-SGD/DP-Adam yields strong performance for a suite of\nNLP tasks at privacy levels ϵ ∈{3,8}. Some of our ﬁne-tuned models outperform strong\nnon-private learning baselines and models obtained under heuristic privacy notions.\n(2) Running DP-SGD can be memory-intensive due to clipping per-example gradients. We present\nghost clipping, a memory saving technique that makes ﬁne-tuning large Transformers under DP\nmemory efﬁcient. Our technique generalizes the Goodfellow (2015) trick to handle sequential\ninputs, and can be combined with a layer-by-layer clipping procedure (Lee & Kifer, 2020) to\nenable privately ﬁtting large Transformers with almost the same memory cost as non-private\ntraining—at the cost of one additional backward pass per processed batch.\n(3) We show that the dimensionality of gradient updates does not explain private ﬁne-tuning per-\nformance. While there exist dimension-dependent lower bounds for private (convex) optimiza-\ntion (Bassily et al., 2014), we ﬁnd that larger pretrained models lead to better private ﬁne-tuning\nresults. Moreover, parameter-efﬁcient adaptation methods that reduce the dimensionality of\nupdates do not necessarily outperform a baseline method that ﬁne-tunes all model parameters.\nOur empirical studies indicate that directly ﬁne-tuning pretrained models with DP optimization results\nin performant DP language models under modest privacy budgets. This enables building practical\nprivate NLP models for a range of common tasks where privacy could be at stake.\n2 P ROBLEM STATEMENT\nWe build models for sentence classiﬁcation and language generation tasks with datasets of modest\nsizes under (central/global) approximate-DP (also known as (ϵ,δ)-DP) (Dwork et al., 2014).\nDeﬁnition 1 ((ϵ,δ)-DP). A randomized algorithm M: X→Y is (ϵ,δ)-differentially private if for\nall adjacent datasets X,X′∈X and all Y ⊂Y, P(M(X) ∈Y) ≤exp(ϵ)P(M(X′) ∈Y) + δ.\n2\nPublished as a conference paper at ICLR 2022\nWe say that two datasets are adjacent if and only if one can be obtained from the other by including\nan extra record.1 How a record is deﬁned is task dependent and will be made clear below.\nIntuitively, DP algorithms ensure that random outputs obtained from similar inputs are difﬁcult\nto distinguish. ϵand δare privacy leakage parameters that measure the loss of privacy and small\nvalues imply stronger privacy guarantees. Unlike heuristic privacy notions (Huang et al., 2020b), DP\nallows for the tracking of privacy loss through the calculation of leakage parameters, and ensures\nprivacy under composition (Dwork et al., 2014), meaning that the overall privacy loss of multiple DP\nalgorithms releasing multiple statistics can be reasoned in a principled manner.\nDP learning typically relies on DP optimizers which privatize gradients before performing updates.\nThe privatization step ensures that parameter updates leak limited information about training examples\nthrough their gradients. Speciﬁcally, this step clips per-example gradients with a norm constraint C,\nand adds Gaussian noise z ∼N(0,C2σ2Ip) to the sum of clipped gradients. Here, σis the noise\nmultiplier determined from the privacy budget (ϵ,δ), number of gradient updates S, and sampling\nrate q= B\nN for a batch size of Band a dataset size of N.2 Intuitively, clipping individual gradients\nensures that each example has bounded inﬂuence on the parameter update, whereas noising the\ngradient prevents exact tracing of particular examples. The noise being isotropic implies that larger\nmodels would experience heavier noise per update, as the norm of the p-dimensional Gaussian ∥z∥2\nscales as Cσ√p. This is widely believed to be the cause for DP optimization to perform poorly at\ntraining high-dimensional deep learning models (Kamath, 2020; Yu et al., 2021b).\nOur starting point for building DP language models is (public) pretrained models. Pretrained language\nmodels tend to contain general knowledge of language (Manning et al., 2020) and thus should make\nthe downstream private learning problem easier. We ﬁne-tune these models with DP-Adam (Abadi\net al., 2016; Kingma & Ba, 2014) (see Appendix A for details) and track privacy loss through R´enyi\nDP (Mironov, 2017), but also report the converted ϵfrom a Gaussian DP central limit theorem (Dong\net al., 2019) and from accurately composing tradeoff functions via fast Fourier transform (Gopi et al.,\n2021). We consider privacy levels ϵ∈{3,8}and δ = 1\n2|Dtrain|throughout for a training set of size\n|Dtrain|(see Appendix B for further details). We tune hyperparameters on a text generation task\n(E2E; introduced below) and transfer these to remaining tasks. We outline two broad classes of NLP\nproblems considered in this paper and deﬁne what constitutes a record below.\nSentence Classiﬁcation. The goal is to learn a model that classiﬁes sentences into one of a few\ncategories. For these tasks, each example/record consists of input sentences and a label to be predicted.\nWe ﬁne-tune models of various sizes in the BERT (Devlin et al., 2018) and RoBERTa (Liu et al.,\n2019) families, as these models are known to work well in non-private learning.\nLanguage Generation. The goal is to learn a model that generates natural language sentences\ngiven some context. For table-to-text generation tasks such as E2E (Novikova et al., 2017) and\nDART (Nan et al., 2020), each example/record in the training data consists of a pair of table entry\nand corresponding text description to be predicted. For a dialogue generation task such as Persona-\nChat (Zhang et al., 2018), each example/record consists of metadata, a dialogue history, and a\nresponse to be predicted. We ﬁne-tune GPT-2 (Radford et al., 2019) and variants of different sizes for\nthese problems, as this model family is known to work well for text generation.\n3 E FFECTIVE DIFFERENTIALLY PRIVATE FINE -TUNING\nBy studying the impact of hyperparameters and choice of ﬁne-tuning objective, we demonstrate that\nthe performance of the DP-Adam baseline can be substantially improved, even matching some strong\nnon-private learning results. Our analyses reveal common failure modes when straightforwardly\napplying DP optimization and explain poor results reported in past works that consider these baselines.\n1An alternative deﬁnition of adjacency assumes all datasets are of equal size and is based on the replacement\nof records. This is not the deﬁnition we adopt.\n2Since we adopted the deﬁnition of “neighboring” based on addition/removal, the batch size here should be\ninterpreted as the lot size (Abadi et al., 2016) or, equivalently stated, the expected size of a batch drawn with\nPoisson sampling.\n3\nPublished as a conference paper at ICLR 2022\n3.1 H YPERPARAMETER TUNING\nDP optimization is sensitive to the choice of hyperparameters (Papernot et al., 2019). Our experiments\nsuggest that its performance can vary from being close to that of random initialization with ill-chosen\nhyperparameters to near state-of-the-art with appropriately chosen ones. As a consequence, we\npresent simple but effective guidelines on setting the most important hyperparameters. Unless\notherwise stated, the unmentioned hyperparameters are set to defaults documented in Appendix H.\n3.1.1 B ATCH SIZE , LEARNING RATE & TRAINING EPOCHS\nOur experiments suggest that batch size is one of the most important hyperparameters to set correctly,\nand the dependence of the optimal batch size on the learning rate and training epochs makes its\nselection complex. We ﬁrst describe batch size selection in realistic, compute-bound settings and\nthen describe how the complexity of identifying the optimal batch size in these situations arise due to\nconstraints on the number of training epochs.\n16 32 64 512 1024 2048\nbatch size B\n10 3\n10 3/2\n10 3/22\n10 3/25\n10 3/26\n10 3/27\nlearning rate \n2.12 12.92 28.46 61.98 62.94 61.17\n23.26 45.09 49.07 60.02 56.71 53.77\n47.00 48.51 53.30 53.34 49.45 42.69\n36.27 33.52 32.96 30.46 29.69 30.00\n30.60 29.92 29.00 29.21 29.75 28.09\n29.08 29.56 29.38 29.71 16.75 10.97\n0\n10\n20\n30\n40\n50\n60\nE2E test set BLEU (=3)\nFigure 2: Large batches and learning rates lead\nto good performance when the number of epochs\nis ﬁxed. Red lines divide heat map into four pan-\nels. Top and bottom correspond to low and high\nlearning rate regimes; left and right correspond to\nsmall and large batch regimes. Numbers are BLEU\nscores on the test split of E2E; higher is better.\nFixed Training Epochs E. We ﬁrst describe\na very practical situation in which there is a con-\nstraint on the compute budget. For the case of\nDP-SGD, this compute budget constraint often\nloosely translates to a constraint on the number\nof examples processed for gradient updates.3 In\nthis ﬁxed training epoch setting, the learning\nrate and batch size jointly affect performance,\nsince using larger batches implies performing\nfewer gradient updates. To study this joint in-\nﬂuence empirically, we ﬁne-tune GPT-2 on the\nE2E dataset for table-to-text generation with DP-\nAdam at ϵ = 3 with various batch sizes and\nlearning rates. Figure 2 shows that the best per-\nforming models (BLEU score /tildelow62) are obtained\nwith both a large batch size and large learning\nrate. Using a small learning rate together with\na small batch size yields considerably worse re-\nsults. Note a seq2seq baseline achieves a test\nBLEU of /tildelow65 without privacy here (Wiseman\net al., 2018).\nRecall that in the non-private world, pretrained language models are typically ﬁne-tuned with small\nbatch sizes and small learning rates with Adam (bottom left panel in Figure 2). This implies that\nna¨ıvely ﬁne-tuning pretrained language models privately using hyperparameters routinely used for\nnon-private learning would degrade performance by more than necessary.4\nRecently, Tram`er & Boneh (2020) studied how the batch size and learning rate jointly affect learning\nprivate image classiﬁers while holding other hyperparameters ﬁxed. They heuristically suggested a\nlinear scaling rule: Scaling the learning rate together with the batch size by the same constant should\nyield models with almost the same performance. However, Figure 2 indicates that this fails to hold\nconsistently as it falsely predicts that large batch and high learning rate (top right entry) would have\nequal performance to small batch and low learning rate (bottom left entry). We explain why linear\nscaling fails to predict performance for the small batch regime in Appendix D.\nFixed Update Steps S. In the ﬁxed epoch setting, we saw that the optimal batch size was complex\ndue to the trade-off between batch size and number of gradient updates. We now show that the\ncomplexity of setting batch sizes arises almost entirely from this tradeoff by considering a different\n3This is because DP-SGD often necessitates microbatching, in which case the number of backward passes is\nindependent of the actual batch size for gradient updates but dependent on numbers of passes through the data.\n4Anecdotally, moderately large learning rates also tend to work reasonably well in non-private learning.\nSmall learning rates, however, are generally more stable, especially when (average) gradients aren’t clipped.\n4\nPublished as a conference paper at ICLR 2022\nsetting, where the total number of gradient updates (rather than epochs) is ﬁxed. In this case, using\nlarger batches implies training for more epochs.\nHere, we ﬁnd that using larger batch sizes almost always results in better performance at a given\nprivacy budget at the cost of processing more examples with more compute, once the other hyperpa-\nrameters S, η, C, ϵ, and δare ﬁxed. We provide a heuristic explanation of this by introducing the\nidea of an effective noise multiplier σeff = σ\nq = σN\nB . Recall the noise multiplier σis determined from\nthe privacy budget (ϵ,δ), the number of update steps S, and the sampling rate q. In addition, recall\nthe privatized gradient ¯gin DP-SGD/DP-Adam which loosely takes the following form:\n¯g= ˜g+ z, ˜g= 1\nB\n∑\ni∈BClip(∇Li,C), z∼N\n(\n0,C2 σ2\nB2 Ip\n)\n= N\n(\n0,C2 σ2\neff\nN2 Ip\n)\n, (1)\nwhere Bis the Poisson-sampled batch of indices, ∇Li is the gradient of the ith example and\nClip(v,C) = v·min(1,C/∥v∥2) clips the vector vby the norm constraint C. We observe that for\nmoderately large batches, the signal-to-noise ratio r= ∥˜g∥2/∥z∥2 is mainly controlled by the batch\nsize through the effective noise multiplier: The signal term ˜gtends to concentrate quickly due to\nbeing an average of bounded vectors, whereas the effective noise multiplier σeff decreases as the\nbatch size Bincreases (shown in Figure 3 (a)). Figure 3 (b) plots the average signal-to-noise ratio r\nover the ﬁrst 30 gradient updates against the ﬁnal model’s performance on E2E and demonstrates that\nlarge batches (up to a threshold) correlates with both increased signal-to-noise ratio at the beginning\nof training and better performance at the end of training. These ﬁndings additionally resonate with\nand explains recent empirical successes of large-scale private pretraining (Anil et al., 2021).\n10 3\n10 2\n10 1\n100\nsampling rate q\n101\n102\n103\neffective noise multiplier eff\n S = 16\nS = 64\nS = 256\nS = 1024\nS = 4096\n0.0020 0.0025 0.0030 0.0035 0.0040\naverage initial signal-to-noise ratio r\n4.6 × 10 1\n4.8 × 10 1\n5 × 10 1\n5.2 × 10 1\n5.4 × 10 1\nE2E test set per token NLL\nq = 2 8\nq = 2 7\nq = 2 6\nq = 2 5\nq = 2 3\nq = 2 2\nq = 2 1\nq = 20\nFigure 3: Left: Effective noise multiplier decreases with increasing sampling rate for various ﬁxed\nnumber of updates S. Right: Large batch sizes (corresponding to large qin the ﬁgure) have higher\nsignal-to-noise ratio at the beginning of training, which log-linearly correlates with ﬁnal performance.\n3.1.2 C LIPPING NORM\nDP optimization is known to be sensitive to the choice of clipping norm. Since the scale of noise\ndepends on this clipping norm (recall its standard deviation is Cσ), picking the threshold Cmuch\nlarger than the actual gradient norm implies more noise is being applied than necessary. In practice,\nwe have found that a small clipping norm which enforces almost all gradients to be clipped throughout\ntraining leads to the best performing models (see Figure 8 in Appendix H).\n3.2 I MPROVING THE TASK ALIGNMENT HELPS PRIVATE LEARNING\nOur ﬁne-tuned models on language generation tasks work well since the pretraining objective and\ndownstream task are aligned: Both involve predicting sequences of tokens. This alignment simpliﬁes\nthe task and beneﬁts private learning. While pretrained models are naturally aligned for language\ngeneration, it is much less so for classiﬁcation tasks. The standard approach for adapting language\nmodels for classiﬁcation involves stacking a freshly initialized network on top of the encoding of\nthe special [CLS] token and jointly optimizing all parameters (Devlin et al., 2018). This workﬂow\nintroduces a discrepancy between pretraining and ﬁne-tuning: Pretraining predicts masked out words\nfrom a large vocabulary whereas ﬁne-tuning predicts integer labels.\n5\nPublished as a conference paper at ICLR 2022\nTo eliminate the discrepancy, we instead consider learning to predict the missing word during ﬁne-\ntuning for classiﬁcation. For example, for sentiment classiﬁcation, we reframe the problem as ﬁlling\nin the [MASK] token in the sequence “<INPUT>. It is [MASK].” and compare the probabilities\nof words “awesome” and “terrible”. This text inﬁlling task is almost exactly the procedure used\nfor pretraining masked language models, and recent works have demonstrated its effectiveness for\nknowledge probing (Petroni et al., 2019), few-shot learning (Gao et al., 2020) and multi-task ﬁne-\ntuning (Wei et al., 2021). On SST-2, we found that using the generic template as described above\nalready improved private ﬁne-tuning performance by 3 ∼5% across different settings. Table 1 (to be\npresented) contains additional results and Appendix E includes analyses on choices of label words.\n4 G HOST CLIPPING : C LIPPING WITHOUT PER -EXAMPLE GRADIENTS\nDP-SGD has high memory overhead due to clipping per-example gradients. Na¨ıvely implemented,\nthis step instantiates a giant gradient vector for each example during optimization and can be\nprohibitively expensive. For example, Hoory et al. (2021) pretrained BERT with DP optimization\nand reported memory issues when using the large batches necessary to achieve high performance.\nA time-costly solution to the memory problem is micro-batching: Split large batches into multiple\nsmaller ones and aggregate the results after processing each small batch individually (Tram `er &\nBoneh, 2020). This solution, however, is unlikely to be sufﬁcient as neural language models become\nlarger and ﬁtting even a few copies of the gradient in memory can be difﬁcult. Lee & Kifer (2020)\nobserved that per-example gradients need not be instantiated at all, if the goal is to sum the clipped\ngradients. They presented a clipping procedure that only instantiates the per-example gradient for\nparameters of a single layer in the model one at a time, as opposed to the entire model at once, at the\ncost of an extra backpropagation pass per processed batch.\nUnfortunately, we ﬁnd this trick to be still insufﬁcient for sequence models such as Transform-\ners (Vaswani et al., 2017), as the memory requirement for per-example gradients of embedding\nlayers and language modeling heads can be costly. We extend the Lee & Kifer (2020) approach such\nthat training Transformers with DP optimization can have almost the same memory consumption\nas non-private training. Unlike their approach, our extension avoids instantiating the per-example\ngradient even for individual linear layers. We call this approach ghost clipping, as the per-example\ngradient is the ghost that never explicitly appears. We anticipate this extension to be useful for both\nprivately ﬁne-tuning and pretraining large Transformers.\n4.1 T HE MEMORY TRICK BY LEE & KIFER (2020)\nPer-example gradient clipping is easy if we know per-example gradient norms. In this case, we ﬁrst\ncompute the scaling factor ci = min(1,C/∥∇Li∥2), where Cis the clipping threshold and Li is the\nloss associated with the ith example. Then, we perform the usual backward pass with the reweighted\nscalar loss ∑\niciLi. This procedure gives us the sum of clipped gradients. Under this setup, the\ndifﬁculty is computing the per-example gradient norm ∥∇Li∥2. We emphasize two technicalities\nthat enable computing this quantity without instantiating the full per-example gradient ∇Li.\nFirst, for a typical neural net layer l with parameters W(l) (without parameter sharing), the per-\nexample gradient w.r.t. parameters can be easily computed using the input to the layer a(l) and the\ngradient of the loss w.r.t. the output g(l), both of which are available during backpropagation. Second,\nfor a large vector formed by concatenating several small vectors u = [u1,...,u k], its Euclidean\nnorm is simply the norm of the vector of norms, i.e. ∥u∥2 = ∥(∥u1∥2 ,..., ∥uk∥2)∥2 .The second\nobservation means that computing the per-example gradient norm∥∇Li∥2 can be done by computing\nthe per-example gradient norms for individual layers of the neural net∥∇W(1) Li∥2 ,..., ∥∇W(L) Li∥2\none at a time (Lis layer count). Moreover, the ﬁrst observation implies that the norms for each layer\ncan be computed using quantities freely available to a typical backward pass. Overall, the per-example\ngradient norm of any network without parameter sharing can be computed in a layer-by-layer fashion\nwith only one per-example gradient tensor for a single layer being instantiated at any time.\n4.2 G HOST CLIPPING FOR TRANSFORMERS WITH SEQUENTIAL DATA\nThe trick by Lee & Kifer (2020) still requires instantiating the per-example gradient of individual\nlayers (although not simultaneously). This can be problematic in terms of memory for Transformers\n6\nPublished as a conference paper at ICLR 2022\nwith large embedding layers. 5 Here, we present a specialized procedure for computing the per-\nexample gradient norm for linear and embedding layers when they are applied to sequential data.6\nThis procedure reduces memory footprint and can be viewed as a generalization of the Goodfellow\n(2015) trick that additionally handles sequential inputs.\nLet a∈RB×T×d be the input to a linear layer with weight matrix W ∈Rp×d, and s∈RB×T×p be\nthe output with si,j = Wai,j. Let g∈RB×T×p be the gradient of the loss w.r.t. the output s. Here,\nT is the number of time steps in the input, and we omitted biases for simplicity. Simple calculation\nshows that the per-example gradient is the product of two matrices:\n∇WLi = g⊤\ni ai ∈Rp×d. (2)\nSince the per-example gradient norms are the end goal, the per-example gradients {∇WLi}B\ni=1\nthemselves need not be instantiated explicitly. More precisely, we observe that the squared per-\nexample gradient norm for this layer ∥∇WLi∥2\nF obeys the following key identity:\n∥∇WLi∥2\nF = vec(aia⊤\ni )⊤vec(gig⊤\ni ). (3)\nSee Appendix F for a derivation. Implemented with common primitives in machine learning libraries,\n(3) has a memory complexity of order O(BT2) when aia⊤\ni ∈ RT×T and gig⊤\ni ∈ RT×T are\ninstantiated,7 as opposed to O(Bpd) in the na¨ıve approach which goes through instantiating (2).8\nThe memory efﬁciency of this procedure is exempliﬁed with off the shelf pretrained language models,\nmost of which have large embedding layers. For instance, for GPT-2, d ≈50,000 and p = 768\nfor the embedding layer, and the context window T ≤1024.9 Our method in theory reduces the\nmemory cost associated with this large embedding layer by at least a factor of 22 compared to when\nthe per-example gradient is na¨ıvely instantiated. Overall, we also observe signiﬁcant savings, since\nembedding layers can be a major source of memory spending for training large language models.10\nTo stress-test ghost clipping, we compare it with 4 baselines: The PyTorch package Opacus that\nimplements DP optimization by instantiating per-example gradients, the approach by Lee & Kifer\n(2020), non-private training in PyTorch, and na¨ıve DP optimization implemented inJAX with jit\nand vmap enabled. We include the JAX baseline since a recent study showed that DP optimization\ncan be made cheap through compiler optimization (Subramani et al., 2020). Figure 4 (a) shows that\nfor typical inputs, our technique is the most memory friendly and allows ﬁtting batches almost as\nlarge as those in non-private training. Since ghost clipping allows us to ﬁt larger batches but with a\nrun-time penalty, a natural question is whether it improves throughput with the use of larger batches.\nFigure 4 (b) shows that while ghost clipping only provides minor gains compared to Opacus for\nsmaller models, it allows processing /tildelow10% more examples compared to the approach by Lee & Kifer\n(2020) for ﬁtting GPT-2-large, a model that neither Opacus or JAX could handle. See Appendix G\nfor the setup of these experiments.\n5 L OW DIMENSIONAL UPDATES ARE NOT NECESSARILY BETTER\nSince the norm of the noise injected into gradients during DP learning scales with dimensionality,\nit is natural to ask whether updating fewer parameters would result in improved performance. We\ndecompose this question into two aspects: (1) Do smaller pretrained models lead to better private\n5For GPT-2, per-example gradients w.r.t. the embedding for ten examples alone occupy/tildelow1.5GB of memory.\n6An embedding layer is essentially a linear layer: The embedding lookup operation applied to indices is\nequivalent to a matrix multiplication of the embedding matrix with one-hot encoded indices.\n7The derived complexity is based on the assumption that the space complexity for multiplying two matrices\nA∈Rm×n and B∈Rn×p is roughly O(mp), which is the case for most workloads running on a framework\nlike PyTorch. In addition, more sophisticated solutions may even avoid instantiating aia⊤\ni and gig⊤\ni entirely by\ntrading in more run-time. Custom CUDA kernels are likely needed to make these solutions fast in practice.\n8We omitted the cost of storing ai and gi, since our goal is to compare the additional cost induced by\ncomputing gradient norms.\n9In practice, for ﬁne-tuning tasks, the maximum sequence length is usually a few hundred.\n10While there are alternative approaches for reducing the memory footprint of embedding layers during\ntraining, these methods tend to introduce extra hyperparameters that potentially require further tuning and\nprivacy spending.\n7\nPublished as a conference paper at ICLR 2022\nGPT-2 GPT-2-medium GPT-2-large\n0\n20\n40\n60\n80maximum batch size (single TITAN RTX)\n86\n34\n10\n24\n8\n0\n40\n22\n8\n26\n6\n0\n80\n34\n10\n(a) Memory\nGPT-2 GPT-2-medium GPT-2-large\n0\n20\n40\n60\n80\n100examples per second\n111.5\n42.2\n17.0\n62.1\n21.8\n0.0\n50.9\n19.3\n7.9\n23.8\n12.6\n0.0\n63.1\n23.1\n8.7\nnon-private\nOpacus\nLee & Kifer, 2020\nJAX (+jit & vmap)\nghost (ours) (b) Throughput\nFigure 4: Left: Ghost clipping is 3 times more memory efﬁcient than Opacus and is almost as\nefﬁcient as non-private training for typical sequences across model sizes. For GPT-2-large, we were\nunable to ﬁt single-example micro batches together with gradient accumulation withOpacus or JAX\non a TITAN RTX GPU (24 GBs of VRAM). Right: DP optimization with ghost clipping processes\n/tildelow10% more examples than the approach by Lee & Kifer (2020) under unit time for GPT-2-large.\nﬁne-tuned performance, and (2) do parameter-efﬁcient adaptation methods designed with a reduced\ndimensionality of updates outperform full ﬁne-tuning? Our experiments below show that neither is\nnecessarily true. Reported numbers in this section are averaged over three independent seeds.\n5.1 L ARGER PRETRAINED MODELS RESULT IN BETTER PERFORMANCE\nWe observe that larger pretrained models lead to better private ﬁne-tuned performance. Speciﬁ-\ncally, we fully ﬁne-tune four sizes of GPT-2 models (for language generation) and three sizes of\nBERT/RoBERTa models (for sentence classiﬁcation) at the same privacy budget with DP-Adam\nand compare their performances. Since the performance of DP optimization heavily depends on\nhyperparameter choices, we need to ensure that our hyperparameters are not particularly favoring\nlarger models. We thus tune hyperparameters on the smallest model for each model type and then\nreuse the same hyperparameters for all ﬁne-tuning workloads for that model type. Figure 1 from\nearlier demonstrates gains from model scaling on E2E and MNLI, and we ﬁnd similar improvements\non 5 additional tasks (deferred to Figure 5 in Appendix C).\n5.2 F ULL FINE -TUNING WITH DP-A DAM MATCHES STATE -OF-THE -ART\nThere is a range of lightweight ﬁne-tuning methods that reduce the dimensionality of updates,\nincluding some that are designed for DP (Yu et al., 2021c). Do methods that optimize fewer\nparameters lead to better results under DP even if they perform similarly non-privately? Empirical\nresults suggest otherwise and that full ﬁne-tuning is a strong baseline that even matches specialized\nlow-dimensional DP learning methods for both classiﬁcation and generation. Below, we study the\ntwo sets of tasks separately. For completeness, all experimental details are in Appendix K.\nSentence Classiﬁcation. We study DP ﬁne-tuning on tasks from the GLUE benchmark that have\nmore than 10k training examples (MNLI, QQP, QNLI, and SST-2), following the experimental setup\nof Yu et al. (2021c). The associated datasets have modest sizes: SST-2 and QNLI have 60k+ and\n100k+ training examples, respectively. MNLI and QQP each contains less than 400k examples.\nTable 1 shows that using larger pretrained models and the text-inﬁlling objective generally improve\nclassiﬁcation accuracy. We compare full ﬁne-tuning with reparameterized gradient perturbation\n(RGP) (Yu et al., 2021c), as it is the state-of-the-art for DP ﬁne-tuning on sentence classiﬁcation at\nthe time the ﬁrst version of this paper was uploaded to arXiv. The method is designed to privatize\ngradients projected onto low dimensional subspaces and was motivated to reduce DP noise in high-\ndimensional models. We note that full ﬁne-tuning with the text inﬁlling objective outperforms\n8\nPublished as a conference paper at ICLR 2022\nwell-tuned RGP on all tasks despite being the simplest baseline.11 Computationally, while RGP is\nfaster per-update, it requires more than 3 times as many epochs as full ﬁne-tuning—overall, the two\nmethods are comparable in terms of wall time.\nTable 1: Full ﬁne-tuning larger pretrained models with text inﬁlling has best performance. Results\nare dev set accuracies. Best numbers based on two-sample test for each privacy level are in bold.\nMethod\nϵ= 3 ϵ= 8\nMNLI-(m/mm) QQP QNLI SST-2 MNLI-(m/mm) QQP QNLI SST-2\nRGP (RoBERTa-base) - - - - 80.5/79.6 85.5 87.2 91.6\nRGP (RoBERTa-large) - - - - 86.1/86.0 86.7 90.0 93.0\nfull (RoBERTa-base) 82.47/82.10 85.41 84.62 86.12 83.30/83.13 86.15 84.81 85.89\nfull (RoBERTa-large) 85.53/85.81 86.65 88.94 90.71 86.28/86.54 87.49 89.42 90.94\nfull + inﬁlling (RoBERTa-base) 82.45/82.99 85.56 87.42 91.86 83.20/83.46 86.08 87.94 92.09\nfull + inﬁlling (RoBERTa-large) 86.43/86.46 86.43 90.76 93.04 87.02/87.26 87.47 91.10 93.81\nϵ≈(Gaussian DP + CLT) 2.52 2.52 2.00 1.73 5.83 5.85 4.75 4.33\nϵ≈(Compose tradeoff func.) 2.75 2.75 2.57 2.41 7.15 7.16 6.87 6.69\nTable-To-Text Generation. We study different ﬁne-tuning methods under DP for table-to-text\ngeneration where the goal is to generate natural language descriptions of table entries. We consider\nthe datasets E2E (Novikova et al., 2017) and DART (Nan et al., 2020). E2E consists of simple\nrestaurant reviews, whereas DART consists of open-domain table entries from Wikipedia and is\nmore complex. Both datasets are small: E2E has more than 40k training examples, whereas DART\nhas more than 60k. Since we are the ﬁrst to experiment with this task under DP, we compare full\nﬁne-tuning (full) against a suite of parameter-efﬁcient approaches which includes LoRA (Hu et al.,\n2021), preﬁx-tuning (Li & Liang, 2021) (preﬁx), RGP, and ﬁne-tuning the top 2 Transformer blocks\n(top2), all of which optimize few parameters. On GPT-2 (125 million parameters), preﬁx-tuning\nwith default hyperparameters optimizes /tildelow10 million parameters; LoRA with rank 4 optimizes /tildelow0.15\nmillion parameters. We also report results for training from scratch (retrain). Hyperparameters of\neach method were tuned only the E2E dataset; the complete search ranges are in Appendix I. Table 2\nshows that LoRA and full ﬁne-tuning are generally the most performant on E2E. Tables 7 and 8 in\nAppendix J contain the full result on E2E and DART and conﬁrm the trend.\nTable 2: Full ﬁne-tuning performs on par with or outperforms others methods that execute gradient\nupdate in low dimensional spaces. Results are on E2E from ﬁne-tuning GPT-2.\nMetric DP Guarantee Gaussian DP Compose Method\n+ CLT tradeoff func. full LoRA preﬁx RGP top2 retrain\nBLEU\nϵ= 3 ϵ≈2.68 ϵ≈2.75 61.519 58.153 47.772 58.482 25.920 15.457\nϵ= 8 ϵ≈6.77 ϵ≈7.27 63.189 63.389 49.263 58.455 26.885 24.247\nnon-private - - 69.463 69.682 68.845 68.328 65.752 65.731\nROUGE-L\nϵ= 3 ϵ≈2.68 ϵ≈2.75 65.670 65.773 58.964 65.560 44.536 35.240\nϵ= 8 ϵ≈6.77 ϵ≈7.27 66.429 67.525 60.730 65.030 46.421 39.951\nnon-private - - 71.359 71.709 70.805 68.844 68.704 68.751\nChit-Chat Dialog Generation. We stress-test full ﬁne-tuning under DP on the task of chit-chat\ndialog generation. This task has the distinct challenge that the response space is intrinsically\ndiverse (Li et al., 2015; Gao et al., 2018) since human conversations can be informal and noisy (Zhang\net al., 2019). Moreover, dialog datasets are usually formed with user data which may contain sensitive\n11The careful reader will notice that the original RGP paper (Yu et al., 2021c) reported considerably worse\nfull ﬁne-tuning results. We analyzed their released codebase and discovered a bug caused by their use of\nmixed-precision training. With this bug ﬁxed, their codebase produces results similar to ours for full ﬁne-tuning\non SST-2. We detail subtleties of DP mixed-precision training in Appendix T, and outline our implementation\nwhich achieves approximate scale invariance.\n9\nPublished as a conference paper at ICLR 2022\ninformation. We use the Persona-Chat dataset (Zhang et al., 2018) as a testbed and build off a\nprocessed version that has /tildelow130k training entries. Each entry contains a dialog history, persona\ndescriptions of the respondent, and the response. We ﬁne-tune GPT-2, GPT2-medium, and DialoGPT-\nmedium on this dataset both privately and non-privately by training to predict the response with the\ndialog history and persona description. We report the F1 score and perplexity on the validation split,\nand human evaluated quality scores of generations. Table 3 shows that private models have strong\nperformance. In particular, ﬁne-tuned DialoGPT-medium at ϵ= 8 beats the (non-private) winning\nentry of the ConvAI2 challenge (Dinan et al., 2019) on perplexity and has a human evaluation rating\nthat is close to non-private models. Samples from our private models can be found in Appendix O.\nTable 3: Fine-tuning with DP-Adam yields high quality chit-chat dialog generation models.\nModel DP Guarantee Gaussian DP Compose Metrics\n+CLT tradeoff func. F1 ↑ Perplexity ↓ Quality (human) ↑\nGPT-2\nϵ= 3 ϵ≈2.54 ϵ≈2.73 15.90 24.59 -\nϵ= 8 ϵ≈6.00 ϵ≈7.13 16.08 23.57 -\nnon-private - - 17.96 18.52 -\nGPT-2-medium\nϵ= 3 ϵ≈2.54 ϵ≈2.73 15.99 20.68 -\nϵ= 8 ϵ≈6.00 ϵ≈7.13 16.53 19.25 -\nnon-private - - 18.64 15.40 -\nDialoGPT-medium\nϵ= 3 ϵ≈2.54 ϵ≈2.73 17.37 17.64 2.82 (2.56, 3.09)\nϵ= 8 ϵ≈6.00 ϵ≈7.13 17.56 16.79 3.09 (2.83, 3.35)\nnon-private - - 19.28 14.28 3.26 (3.00, 3.51)\nHuggingFace (ConvAI2 winner) non-private - - 19.09 17.51 -\nHuggingFace (our implementation) non-private - - 16.36 20.55 3.23 (2.98, 3.49)\nReference - - - - - 3.74 (3.49, 4.00)\n6 R ELATED WORK\nPrivate NLP. The privacy-preserving NLP space is largely divided by whether or not DP (or its\nextensions) is considered. McMahan et al. (2017) successfully trained small word-level RNNs with\n1.35 million parameters in a federated setting with more than 700k users under a global DP guarantee\nwith (ϵ,δ) = (4.6,10−9). Ramaswamy et al. (2020) train production grade next-word prediction\nmodels using DP-FedAvg with millions of users. Qu et al. (2021) studied ﬁne-tuning BERT for\nlanguage understanding tasks under local DP. Kerrigan et al. (2020) presented initial results that public\npretraining is helpful for downstream DP ﬁne-tuning. However, they did not attempt ﬁne-tuning\nlarge pretrained models with DP-SGD. Bommasani et al. (2021) brieﬂy commented on the possibility\nof achieving cheaper private learning by ﬁne-tuning large pretrained language models. Anil et al.\n(2021) pretrained BERT under global DP on datasets with hundreds of millions of examples. Dupuy\net al. (2021) studied private BERT ﬁne-tuning on datasets of utterances, but reported results with ϵ\non the order of at least 100. Orthogonally, many works considered training language models that\nsatisfy empirical notions of privacy (Xu et al., 2021; Coavoux et al., 2018; Mireshghallah et al., 2021;\nMelamud & Shivade, 2019). Our work is distinct from all works mentioned above in that we study\nﬁne-tuning large language models (with hundreds of millions of parameters) under global DP with\nstringent guarantees (ϵ∈{3,8}) on smaller datasets (much less than a million examples).\n7 S COPE AND LIMITATIONS\nWe presented strategies for ﬁne-tuning large pretrained language models under DP for a wide range\nof NLP tasks. For researchers and practitioners working on private NLP, our empirical results\nsuggest that DP ﬁne-tuning with a proper setup is a competitive baseline that is worth trying before\nprematurely shifting to less formal notions of privacy which have not stood against the test of time.\nIn addition, since DP ﬁne-tuning generally requires substantially less private data (than training with\nDP from scratch), we hope this will motivate organizations which are already conducting private\nlearning (e.g., federated learning with DP) to reduce the collection and use of private data to beneﬁt\nprivacy in the long run. Below we list limitations and future directions.\n10\nPublished as a conference paper at ICLR 2022\nPublic Pretraining. Our empirical studies are based on ﬁne-tuning off-the-shelf models such as\nBERT and GPT-2 that are pretrained on data collected from the internet. Due to the permissive nature\nof the data collection for some of these models, there can be privacy concerns (e.g., text data scraped\nexpansively from the internet may contain sensitive information such as PIIs (Carlini et al., 2020)).\nWe selected off-the-shelf models as they are widely accessible to ensure our results are reproducible.\nWhen considering applying DP ﬁne-tuning to real world settings, one should consider and create\nmore curated public corpora for pretraining.\nHyperparameter Tuning. We studied how choices of basic hyperparameters affect the perfor-\nmance of DP-Adam for ﬁne-tuning. Our study is by no means comprehensive or complete. In\nparticular, we did not study how weight decay, learning rate schedule, clipping norm schedule, or\nbatch size schedule affect private learning. Custom setups for these knobs have been found helpful\nfor other private learning tasks (Anil et al., 2021).\nOverall, our studies revealed that hyperparameter values can affect private ﬁne-tuning in signiﬁcant\nways. This calls for more transparency in reporting hyperparameter choices, analyses of hyperparame-\nter transferability across tasks and architectures, and accounting of the privacy loss for hyperparameter\ntuning when hyperparameters aren’t transferred across workloads.\nPretraining and Its Relation to Private Learning. Our model scaling results (Figure 1) suggest\nthat using larger pretrained models improves performance. This argument, however, is dependent on\nthe particular choice of pretrained models. How pretraining helps private learning and whether better\npretrained models for private learning could be built are interesting future avenues.\nScaling Laws for Private Learning. While scaling laws (Kaplan et al., 2020) for non-private deep\nlearning have become prevalent, we are unaware of a case study in the private learning realm. Studies\non how the dimensionality of models (and pretraining) generally affect private deep learning in\nprecise terms will likely be a useful tool in trading off compute budget and model quality.\nETHICS STATEMENT\nWhile the present paper studies private NLP, all experiments performed herein are based on public\nand well-studied datasets.\nTo the best of our knowledge, this work is the ﬁrst to demonstrate that DP NLP can achieve promising\nlevels of performance across a wide range of tasks with limited data. Our experimental results suggest\nthat DP learning has the potential to be used in industry settings to build high performing applications\nwith small private datasets without making big sacriﬁces in privacy.\nOn the other hand, we acknowledge that successful private learning has the potential to motivate\ncompanies to more aggressively collect user data, which could result in long-term privacy harms (Ro-\ngaway, 2015; Solove, 2005). In addition, other societal harms not captured by DP (e.g., representation\nbias) can arise with more machine learning models being trained on private data.\nREPRODUCIBILITY\nAll experiments in the paper are based on publicly available datasets. Links to these datasets\nare included in the main text and appendices. Hyperparameters necessary for reproducing our\nexperiments are documented in Appendix I and H. Code to reproduce the headline results can be\nfound at https://github.com/lxuechen/private-transformers.\nACKNOWLEDGMENTS\nWe thank Xiang Lisa Li for help on reproducing non-private preﬁx-tuning results. We thank Da Yu\nfor discussions and help on reproducing results for the RGP method. We thank Abhradeep Guha\nThakurta and Rishi Bommasani for helpful discussions. We thank members of the Stanford statistical\nmachine learning group for comments on early versions of the abstract. We thank Guodong Zhang\nand Mayee Chen for comments on an early draft. We thank Stanford HAI for a Google Cloud Credits\nGrant. XL is supported by a Stanford Graduate Fellowship. PL is supported by a PECASE award.\n11\nPublished as a conference paper at ICLR 2022\nREFERENCES\nMartin Abadi, Andy Chu, Ian Goodfellow, H Brendan McMahan, Ilya Mironov, Kunal Talwar, and\nLi Zhang. Deep learning with differential privacy. In Proceedings of the 2016 ACM SIGSAC\nconference on computer and communications security, pp. 308–318, 2016.\nRohan Anil, Badih Ghazi, Vineet Gupta, Ravi Kumar, and Pasin Manurangsi. Large-scale differen-\ntially private bert. arXiv preprint arXiv:2108.01624, 2021.\nHilal Asi, John Duchi, Alireza Fallah, Omid Javidbakht, and Kunal Talwar. Private adaptive gradient\nmethods for convex optimization. In International Conference on Machine Learning, pp. 383–392.\nPMLR, 2021.\nRaef Bassily, Adam Smith, and Abhradeep Thakurta. Differentially private empirical risk minimiza-\ntion: Efﬁcient algorithms and tight error bounds, 2014.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Goldberg. Bitﬁt: Simple parameter-efﬁcient ﬁne-tuning\nfor transformer-based masked language-models. arXiv e-prints, pp. arXiv–2106, 2021.\nRishi Bommasani, Steven Wu, and Xanda Schoﬁeld. Towards private synthetic text generation. In\nNeurIPS 2019 Machine Learning with Guarantees Workshop, 2019.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportuni-\nties and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021.\nZhiqi Bu, Jinshuo Dong, Qi Long, and Weijie J Su. Deep learning with gaussian differential privacy.\nHarvard data science review, 2020(23), 2020.\nZhiqi Bu, Sivakanth Gopi, Janardhan Kulkarni, Yin Tat Lee, Judy Hanwen Shen, and Uthaipon\nTantipongpipat. Fast and memory efﬁcient differentially private-sgd via jl projections. arXiv\npreprint arXiv:2102.03013, 2021.\nHan Cai, Chuang Gan, Ligeng Zhu, and Song Han. Tinytl: Reduce memory, not parameters for\nefﬁcient on-device learning. arXiv preprint arXiv:2007.11622, 2020.\nNicholas Carlini, Chang Liu, ´Ulfar Erlingsson, Jernej Kos, and Dawn Song. The secret sharer:\nEvaluating and testing unintended memorization in neural networks. In 28th {USENIX}Security\nSymposium ({USENIX}Security 19), pp. 267–284, 2019.\nNicholas Carlini, Florian Tramer, Eric Wallace, Matthew Jagielski, Ariel Herbert-V oss, Katherine\nLee, Adam Roberts, Tom Brown, Dawn Song, Ulfar Erlingsson, et al. Extracting training data\nfrom large language models. arXiv preprint arXiv:2012.07805, 2020.\nKamalika Chaudhuri and Staal A Vinterbo. A stability-based validation procedure for differentially\nprivate machine learning. Advances in Neural Information Processing Systems, 26:2652–2660,\n2013.\nDingfan Chen, Tribhuvanesh Orekondy, and Mario Fritz. Gs-wgan: A gradient-sanitized approach\nfor learning differentially private generators. arXiv preprint arXiv:2006.08265, 2020.\nMaximin Coavoux, Shashi Narayan, and Shay B Cohen. Privacy-preserving neural representations of\ntext. arXiv preprint arXiv:1808.09408, 2018.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nEmily Dinan, Varvara Logacheva, Valentin Malykh, Alexander Miller, Kurt Shuster, Jack Urbanek,\nDouwe Kiela, Arthur Szlam, Iulian Serban, Ryan Lowe, et al. The second conversational intelli-\ngence challenge (convai2). arXiv preprint arXiv:1902.00098, 2019.\nJinshuo Dong, Aaron Roth, and Weijie J Su. Gaussian differential privacy. arXiv preprint\narXiv:1905.02383, 2019.\n12\nPublished as a conference paper at ICLR 2022\nChristophe Dupuy, Radhika Arava, Rahul Gupta, and Anna Rumshisky. An efﬁcient dp-sgd mecha-\nnism for large scale nlp models. arXiv preprint arXiv:2107.14586, 2021.\nCynthia Dwork, Frank McSherry, Kobbi Nissim, and Adam Smith. Calibrating noise to sensitivity in\nprivate data analysis. In Theory of cryptography conference, pp. 265–284. Springer, 2006.\nCynthia Dwork, Aaron Roth, et al. The algorithmic foundations of differential privacy. Foundations\nand Trends in Theoretical Computer Science, 9(3-4):211–407, 2014.\nJianfeng Gao, Michel Galley, and Lihong Li. Neural approaches to conversational ai. In The 41st\nInternational ACM SIGIR Conference on Research & Development in Information Retrieval, pp.\n1371–1374, 2018.\nTianyu Gao, Adam Fisch, and Danqi Chen. Making pre-trained language models better few-shot\nlearners. arXiv preprint arXiv:2012.15723, 2020.\nSebastian Gehrmann, Tosin Adewumi, Karmanya Aggarwal, Pawan Sasanka Ammanamanchi, Aremu\nAnuoluwapo, Antoine Bosselut, Khyathi Raghavi Chandu, Miruna Clinciu, Dipanjan Das, Kaus-\ntubh D Dhole, et al. The gem benchmark: Natural language generation, its evaluation and metrics.\narXiv preprint arXiv:2102.01672, 2021.\nIan Goodfellow. Efﬁcient per-example gradient computations. arXiv preprint arXiv:1510.01799,\n2015.\nSivakanth Gopi, Yin Tat Lee, and Lukas Wutschitz. Numerical composition of differential privacy.\narXiv preprint arXiv:2106.02848, 2021.\nJamie Hayes, Luca Melis, George Danezis, and Emiliano De Cristofaro. Logan: Membership\ninference attacks against generative models. In Proceedings on Privacy Enhancing Technologies\n(PoPETs), volume 2019, pp. 133–152. De Gruyter, 2019.\nAri Holtzman, Jan Buys, Li Du, Maxwell Forbes, and Yejin Choi. The curious case of neural text\ndegeneration. arXiv preprint arXiv:1904.09751, 2019.\nShlomo Hoory, Amir Feder, Avichai Tendler, Alon Cohen, Soﬁa Erell, Itay Laish, Hootan Nakhost,\nUri Stemmer, Ayelet Benjamini, Avinatan Hassidim, et al. Learning and evaluating a differentially\nprivate pre-trained language model. In Proceedings of the Third Workshop on Privacy in Natural\nLanguage Processing, pp. 21–29, 2021.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe,\nAndrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efﬁcient transfer learning for\nnlp. In International Conference on Machine Learning, pp. 2790–2799. PMLR, 2019.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu\nChen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685,\n2021.\nYangsibo Huang, Zhao Song, Danqi Chen, Kai Li, and Sanjeev Arora. Texthide: Tackling data\nprivacy in language understanding tasks. arXiv preprint arXiv:2010.06053, 2020a.\nYangsibo Huang, Zhao Song, Kai Li, and Sanjeev Arora. Instahide: Instance-hiding schemes for\nprivate distributed learning. In International Conference on Machine Learning, pp. 4507–4518.\nPMLR, 2020b.\nP Kairouz, M Ribero, K Rush, and A Thakurta. Fast dimension independent private adagrad on\npublicly estimated subspaces. arXiv preprint arXiv:2008.06570, 2020.\nGautam Kamath. Lecture 14 — Private ML and Stats: Modern ML. http://www.\ngautamkamath.com/CS860notes/lec14.pdf, 2020.\nJared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott\nGray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models.\narXiv preprint arXiv:2001.08361, 2020.\n13\nPublished as a conference paper at ICLR 2022\nGavin Kerrigan, Dylan Slack, and Jens Tuyls. Differentially private language models beneﬁt from\npublic pre-training. arXiv preprint arXiv:2009.05886, 2020.\nDiederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint\narXiv:1412.6980, 2014.\nAntti Koskela, Joonas J¨alk¨o, and Antti Honkela. Computing tight differential privacy guarantees\nusing fft. In International Conference on Artiﬁcial Intelligence and Statistics , pp. 2560–2569.\nPMLR, 2020.\nJaewoo Lee and Daniel Kifer. Scaling up differentially private deep learning with fast per-example\ngradient clipping. arXiv preprint arXiv:2009.03106, 2020.\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efﬁcient prompt\ntuning. arXiv preprint arXiv:2104.08691, 2021.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao, and Bill Dolan. A diversity-promoting\nobjective function for neural conversation models. arXiv preprint arXiv:1510.03055, 2015.\nXiang Lisa Li and Percy Liang. Preﬁx-tuning: Optimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190, 2021.\nJingcheng Liu and Kunal Talwar. Private selection from private candidates. In Proceedings of the\n51st Annual ACM SIGACT Symposium on Theory of Computing, pp. 298–309, 2019.\nXiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt\nunderstands, too. arXiv preprint arXiv:2103.10385, 2021.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike\nLewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining\napproach. arXiv preprint arXiv:1907.11692, 2019.\nRabeeh Karimi Mahabadi, James Henderson, and Sebastian Ruder. Compacter: Efﬁcient low-rank\nhypercomplex adapter layers. arXiv preprint arXiv:2106.04647, 2021.\nChristopher D Manning, Kevin Clark, John Hewitt, Urvashi Khandelwal, and Omer Levy. Emergent\nlinguistic structure in artiﬁcial neural networks trained by self-supervision. Proceedings of the\nNational Academy of Sciences, 117(48):30046–30054, 2020.\nH Brendan McMahan, Daniel Ramage, Kunal Talwar, and Li Zhang. Learning differentially private\nrecurrent language models. arXiv preprint arXiv:1710.06963, 2017.\nOren Melamud and Chaitanya Shivade. Towards automatic generation of shareable synthetic clinical\nnotes using neural language models. arXiv preprint arXiv:1905.07002, 2019.\nPaulius Micikevicius, Sharan Narang, Jonah Alben, Gregory Diamos, Erich Elsen, David Garcia,\nBoris Ginsburg, Michael Houston, Oleksii Kuchaiev, Ganesh Venkatesh, et al. Mixed precision\ntraining. arXiv preprint arXiv:1710.03740, 2017.\nFatemehsadat Mireshghallah, Huseyin A Inan, Marcello Hasegawa, Victor R ¨uhle, Taylor Berg-\nKirkpatrick, and Robert Sim. Privacy regularization: Joint privacy-utility optimization in language\nmodels. arXiv preprint arXiv:2103.07567, 2021.\nIlya Mironov. R ´enyi differential privacy. In 2017 IEEE 30th Computer Security Foundations\nSymposium (CSF), pp. 263–275. IEEE, 2017.\nIlya Mironov, Kunal Talwar, and Li Zhang. R \\’enyi differential privacy of the sampled gaussian\nmechanism. arXiv preprint arXiv:1908.10530, 2019.\nLinyong Nan, Dragomir Radev, Rui Zhang, Amrit Rau, Abhinand Sivaprasad, Chiachun Hsieh,\nXiangru Tang, Aadit Vyas, Neha Verma, Pranav Krishna, et al. Dart: Open-domain structured data\nrecord to text generation. arXiv preprint arXiv:2007.02871, 2020.\nMarcel Neunhoeffer, Zhiwei Steven Wu, and Cynthia Dwork. Private post-gan boosting. arXiv\npreprint arXiv:2007.11934, 2020.\n14\nPublished as a conference paper at ICLR 2022\nJekaterina Novikova, Ond ˇrej Du ˇsek, and Verena Rieser. The e2e dataset: New challenges for\nend-to-end generation. arXiv preprint arXiv:1706.09254, 2017.\nNicolas Papernot and Thomas Steinke. Hyperparameter tuning with renyi differential privacy, 2021.\nNicolas Papernot, Mart ´ın Abadi, Ulfar Erlingsson, Ian Goodfellow, and Kunal Talwar. Semi-\nsupervised knowledge transfer for deep learning from private training data. arXiv preprint\narXiv:1610.05755, 2016.\nNicolas Papernot, Shuang Song, Ilya Mironov, Ananth Raghunathan, Kunal Talwar, and ´Ulfar\nErlingsson. Scalable private learning with pate. arXiv preprint arXiv:1802.08908, 2018.\nNicolas Papernot, Steve Chien, Shuang Song, Abhradeep Thakurta, and Ulfar Erlingsson. Making\nthe shoe ﬁt: Architectures, initializations, and tuning for learning with privacy. 2019.\nAdam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan, Edward Yang, Zachary DeVito,\nZeming Lin, Alban Desmaison, Luca Antiga, and Adam Lerer. Automatic differentiation in\npytorch. 2017.\nFabio Petroni, Tim Rockt¨aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller,\nand Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066,\n2019.\nJonas Pfeiffer, Aishwarya Kamath, Andreas R¨uckl´e, Kyunghyun Cho, and Iryna Gurevych. Adapter-\nfusion: Non-destructive task composition for transfer learning. arXiv preprint arXiv:2005.00247,\n2020.\nVenkatadheeraj Pichapati, Ananda Theertha Suresh, Felix X Yu, Sashank J Reddi, and Sanjiv Kumar.\nAdaclip: Adaptive clipping for private sgd. arXiv preprint arXiv:1908.07643, 2019.\nChen Qu, Weize Kong, Liu Yang, Mingyang Zhang, Michael Bendersky, and Marc Najork. Privacy-\nadaptive bert for natural language understanding. arXiv preprint arXiv:2104.07504, 2021.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language\nmodels are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019.\nSwaroop Ramaswamy, Om Thakkar, Rajiv Mathews, Galen Andrew, H Brendan McMahan, and\nFranc ¸oise Beaufays. Training production language models without memorizing user data. arXiv\npreprint arXiv:2009.10031, 2020.\nPhillip Rogaway. The moral character of cryptographic work. Cryptology ePrint Archive, 2015.\nAndreas R ¨uckl´e, Gregor Geigle, Max Glockner, Tilman Beck, Jonas Pfeiffer, Nils Reimers, and\nIryna Gurevych. Adapterdrop: On the efﬁciency of adapters in transformers. arXiv preprint\narXiv:2010.11918, 2020.\nReza Shokri, Marco Stronati, Congzheng Song, and Vitaly Shmatikov. Membership inference attacks\nagainst machine learning models. In 2017 IEEE Symposium on Security and Privacy (SP) , pp.\n3–18. IEEE, 2017.\nDaniel J Solove. A taxonomy of privacy. U. Pa. l. Rev., 154:477, 2005.\nShuang Song, Kamalika Chaudhuri, and Anand D Sarwate. Stochastic gradient descent with differen-\ntially private updates. In 2013 IEEE Global Conference on Signal and Information Processing, pp.\n245–248. IEEE, 2013.\nPranav Subramani, Nicholas Vadivelu, and Gautam Kamath. Enabling fast differentially private sgd\nvia just-in-time compilation and vectorization. arXiv preprint arXiv:2010.09063, 2020.\nAmirsina Torﬁ, Edward A Fox, and Chandan K Reddy. Differentially private synthetic medical data\ngeneration using convolutional gans. arXiv preprint arXiv:2012.11774, 2020.\nReihaneh Torkzadehmahani, Peter Kairouz, and Benedict Paten. Dp-cgan: Differentially private\nsynthetic data and label generation. In Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition Workshops, pp. 0–0, 2019.\n15\nPublished as a conference paper at ICLR 2022\nFlorian Tram`er and Dan Boneh. Differentially private learning needs better features (or much more\ndata). arXiv preprint arXiv:2011.11660, 2020.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz\nKaiser, and Illia Polosukhin. Attention is all you need. arXiv preprint arXiv:1706.03762, 2017.\nJason Wei, Maarten Bosma, Vincent Y Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du,\nAndrew M Dai, and Quoc V Le. Finetuned language models are zero-shot learners. arXiv preprint\narXiv:2109.01652, 2021.\nAdina Williams, Nikita Nangia, and Samuel R Bowman. The multi-genre nli corpus. 2018.\nSam Wiseman, Stuart M Shieber, and Alexander M Rush. Learning neural templates for text\ngeneration. arXiv preprint arXiv:1808.10122, 2018.\nZekun Xu, Abhinav Aggarwal, Oluwaseyi Feyisetan, and Nathanael Teissier. On a utilitarian approach\nto privacy preserving text generation. arXiv preprint arXiv:2104.11838, 2021.\nAshkan Yousefpour, Igor Shilov, Alexandre Sablayrolles, Davide Testuggine, Karthik Prasad, Mani\nMalek, John Nguyen, Sayan Gosh, Akash Bharadwaj, Jessica Zhao, Graham Cormode, and Ilya\nMironov. Opacus: User-friendly differential privacy library in pytorch, 2021.\nDa Yu, Saurabh Naik, Arturs Backurs, Sivakanth Gopi, Huseyin A Inan, Gautam Kamath, Janardhan\nKulkarni, Yin Tat Lee, Andre Manoel, Lukas Wutschitz, et al. Differentially private ﬁne-tuning of\nlanguage models. arXiv preprint arXiv:2110.06500, 2021a.\nDa Yu, Huishuai Zhang, Wei Chen, and Tie-Yan Liu. Do not let privacy overbill utility: Gradient\nembedding perturbation for private learning. arXiv preprint arXiv:2102.12677, 2021b.\nDa Yu, Huishuai Zhang, Wei Chen, Jian Yin, and Tie-Yan Liu. Large scale private learning via\nlow-rank reparametrization. arXiv preprint arXiv:2106.09352, 2021c.\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding\ndeep learning requires rethinking generalization. CoRR, abs/1611.03530, 2016. URL http:\n//arxiv.org/abs/1611.03530.\nHuanyu Zhang, Ilya Mironov, and Meisam Hejazinia. Wide network learning with differential privacy.\narXiv preprint arXiv:2103.01294, 2021.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. Per-\nsonalizing dialogue agents: I have a dog, do you have pets too? arXiv preprint arXiv:1801.07243,\n2018.\nYizhe Zhang, Siqi Sun, Michel Galley, Yen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng Gao,\nJingjing Liu, and Bill Dolan. Dialogpt: Large-scale generative pre-training for conversational\nresponse generation. arXiv preprint arXiv:1911.00536, 2019.\nYingxue Zhou, Zhiwei Steven Wu, and Arindam Banerjee. Bypassing the ambient dimension: Private\nsgd with gradient subspace identiﬁcation. arXiv preprint arXiv:2007.03813, 2020.\n16\nPublished as a conference paper at ICLR 2022\nA DP-A DAM\nWe use DP-Adam throughout. DP-Adam works just like regular Adam (Kingma & Ba, 2014) but\nperforms updates and moment accumulation with privatized gradients. The gradient privatization\npart is the same as that performed in DP-SGD (Song et al., 2013; Abadi et al., 2016). Seemingly\nuncommon, DP-Adam is used in many private ML libraries.12 To determine the noise multiplier, we\naccount privacy through R´enyi differential privacy (RDP) (Mironov, 2017; Mironov et al., 2019). For\ncompleteness, we include the pseudocode for DP-Adam below.\nAlgorithm 1 DP-Adam\n1: Input: Data D= {xi}N\ni=1, learning rate η, noise multiplier σ, batch size B, Euclidean norm\nthreshold for gradients C, epochs E, initial parameter vector θ0 ∈Rp, initial moment estimates\nm0,v0 ∈Rp, exponential decay rates β1,β2 ∈R, avoid division-by-zero constant γ ∈R.\n2: for t∈[E·N/B] do\n3: Draw a batch Bt via Poisson sampling; each element has probability B/N of being selected\n4: for xi ∈Bt do\n5: gt(xi) ←∇θtL(xi), ˜gt(xi) ←gt(xi) ·min(1,C/∥gt(xi)∥2)\n6: end for\n7: zt ∼N(0,σ2C2Ip)\n8: ¯gt = 1\nB\n(∑N\ni=1 ˜gt(xi) + zt\n)\n9: θt+1,mt+1,vt+1 ←AdamUpdate(θt,mt,vt, ¯gt,β1,β2,γ)\n10: end for\n11: return θTN/B\nAlgorithm 2 AdamUpdate\n1: Input: θt,mt,vt, ¯gt,β1,β2,γ\n2: mt+1 ←β1 ·mt + (1 −β1) ·¯gt, v t+1 ←β2 ·vt + (1 −β2) ·¯gt2\n3: ˆmt+1 ←mt+1/(1 −βt\n1) , ˆvt+1 ←vt+1/(1 −βt\n2)\n4: θt+1 ←θt −α·ˆmt+1/\n(√\nˆvt+1 + γ\n)\n5: return θt+1,mt+1,vt+1\nB P RIVACY ACCOUNTING\nWe train all models under approximate-DP (Dwork et al., 2014), and we view two datasets as being\nadjacent if and only if one can be obtained from the other by including an extra record (Mironov et al.,\n2019). Instead of accounting the privacy loss with Moments Accountant (Abadi et al., 2016), we\nperform computation through (i) R´enyi DP (Mironov, 2017; Mironov et al., 2019), (ii) Gaussian DP\nwith an associated central limit theorem (Dong et al., 2019), and(iii) numerically composing tradeoff\nfunctions via fast Fourier transform (Gopi et al., 2021; Koskela et al., 2020). All approaches are\nimprovements over the Moments Accountant. Accounting loss with R´enyi DP provides strict upper\nbounds on the actual privacy leakage but may result in loose bounds. Accounting loss with Gaussian\nDP and its central limit theorem, although asymptotically exact, only provides approximations to\nthe actual loss under a ﬁnite number of compositions (Dong et al., 2019, Theorem 3.4). Gopi et al.\n(2021) observed that accounting loss with GDP and its CLT results in underestimation and proposed\nto numerically compose tradeoff functions resulting in both upper and lower bounds on the actual\nleakage ϵ. We therefore also report the converted ϵwith the approach by Gopi et al. (2021) using\ntheir code.13\nGiven the noise multiplier σ, sampling rate q, number of steps S, and failure constant δ, ϵcan be\ncomputed via ﬁrst computing the R ´enyi DP leakage and then converting it to approximate DP. When\na privacy spending (speciﬁed by a set of given ϵand δ) is prescribed, we can numerically invert the\nabove procedure to obtain a suitable σfor noisy optimization. This is what we do throughout all\n12https://github.com/tensorflow/privacy/blob/7c4f5bab0964bd32b7ceafa009d9488920856440/\ntensorflow_privacy/privacy/optimizers/dp_optimizer.py#L385\n13https://github.com/microsoft/prv_accountant\n17\nPublished as a conference paper at ICLR 2022\nexperiments. For completeness, given the σchosen as above, we also report the leakage estimated by\ngoing through the central limit theorem in Gaussian DP (Bu et al., 2020).\nModel selection from hyperparameter tuning on private training and validation data incurs extra\nleakage (Liu & Talwar, 2019; Chaudhuri & Vinterbo, 2013; Papernot & Steinke, 2021). We perform\ntuning only on the E2E task and reuse almost the exact hyperparameters for remaining tasks. Note\nthe general strategy of tuning hyperparameters on a separate (public) dataset and thereafter transfer\nhas been applied to train private production language models (Ramaswamy et al., 2020).\nC A DDITIONAL RESULTS ON MODEL SCALING\nWe repeat the model scaling experiments from Figure 1 on the other tasks considered in the paper.\nFigure 5 shows that the trend that larger and better pretrained models lead to improved private ﬁne-\ntuned performance holds consistently across all tasks. TextHide numbers based on the TextHideintra\nformulation (Huang et al., 2020a).\n50 100 150 200 250 300\nnumber of non-embedding parameters (millions)\n76\n78\n80\n82\n84\n86MNLI-mismatched dev set accuracy\nBERT family ( = 3)\nRoBERTa family ( = 3)\nBERT family ( = 8)\nRoBERTa family ( = 8)\nTextHide (m, k) = (256, 4) (BERT-base)\n(a) MNLI-mismatched\n50 100 150 200 250 300\nnumber of non-embedding parameters (millions)\n86\n88\n90\n92\n94SST-2 dev set accuracy\nBERT family ( = 3)\nRoBERTa family ( = 3)\nBERT family ( = 8)\nRoBERTa family ( = 8)\nTextHide (m, k) = (256, 4) (BERT-base) (b) SST-2\n50 100 150 200 250 300\nnumber of non-embedding parameters (millions)\n84\n85\n86\n87\n88\n89\n90QQP dev set accuracy\nBERT family ( = 3)\nRoBERTa family ( = 3)\nBERT family ( = 8)\nRoBERTa family ( = 8)\nTextHide (m, k) = (256, 4) (BERT-base)\n(c) QQP\n50 100 150 200 250 300\nnumber of non-embedding parameters (millions)\n82\n84\n86\n88\n90QNLI dev set accuracy\nBERT family ( = 3)\nRoBERTa family ( = 3)\nBERT family ( = 8)\nRoBERTa family ( = 8)\nTextHide (m, k) = (256, 4) (BERT-base) (d) QNLI\n100 200 300 400 500 600 700\nnumber of non-embedding parameters (millions)\n28\n30\n32\n34\n36\n38\n40\n42DART test set BLEU\nDistilGPT2\nGPT-2\nGPT-2-medium\nGPT-2-large= 3\n= 8\n(e) DART\nFigure 5: Larger and better pretrained models consistently lead to better private ﬁne-tuned perfor-\nmance on sentence classiﬁcation and language generation tasks.\n18\nPublished as a conference paper at ICLR 2022\nD W HEN AND WHY DOES LINEAR SCALING FAIL ?\nRecall Tram`er & Boneh (2020) suggested that the following simple rule approximately holds in\nprivate learning: Scaling the learning rate together with the batch size by the same constant yields\nmodels with almost the same performance. Note that their experiments on MNIST, Fashion-MNIST,\nand CIFAR-10 used only batch sizes in {512,1024,2048,4096}. These values are fairly large from\na non-private learning perspective. Indeed, our experiments on E2E suggest that this rule does not\ngeneralize to batch sizes that are too small (sampling rates q= B/N <2−8).\nWe provide an explanation by noting that a core assumption which the linear scaling rule depends on\nfails to hold for small batch sizes. This assumption is that given a privacy budget, a “square-root”\nrelationship holds between the noise multiplier and the sampling rate (see also (Tram `er & Boneh,\n2020, Claim D.1)). For instance, Tram`er & Boneh (2020) showed that σ≈c√qwhen q∈[2−7,1]\nfor some constant c. Our numerical estimates show that this relationship fails to hold for small q\n– it underestimates the true noise multiplier σthat would be obtained with numerical computation.\nFigure 6 provides an illustration for (ϵ,δ) = (3,10−5) when the sample size N = 50k and number\nof training epochs E = 50.\n2 12\n2 10\n2 8\n2 6\n2 4\n2 2\nsampling rate B/N\n2 2\n2 1\n20\n21\n22\n23\nnoise multiplier \nactual\ntheory ( 10.34(B/N)0.5)\nFigure 6: “Square-root” relationship underestimates the noise multiplier for small batch sizes.\nE HOW DOES THE CHOICE OF LABEL WORDS AFFECT SENTENCE\nCLASSIFICATION\nRecall in Section 3.2 we cast sentence classiﬁcation as ﬁlling in the missing word amongKcandidates\nfor a K-way classiﬁcation problem. Since a label word could be mapped to multiple possible words,\nwe study how the choice of label words affect performance. We again use the sentiment classiﬁcation\ntask SST-2 as a testbed. Figure 7 shows the effect of varying the label word, where we measure\nthe alignment between the label word and the downstream task by the zero-shot performance of the\ninﬁlling task (x-axis). We ﬁnd that increasing the task alignment of the label words alone improves\nperformance by 1 −2% (orange curve), which is in contrast to the non-private setting, where choice\nof label words do not affect performance in statistically signiﬁcant ways (blue curve).\n0.2 0.3 0.4 0.5 0.6 0.7 0.8\nSST-2 zero-shot accuracy\n86\n88\n90\n92\n94SST-2 fine-tuned accuracy\nnon-private\nvary label word ( = 3)\nCLS token ( = 3)\ninfill + default label word ( = 3)\nFigure 7: Better text labels help private learning more than non-private learning.\n19\nPublished as a conference paper at ICLR 2022\nF D ERIVATION OF THE FROBENIUS NORM IDENTITY\nRecall a∈RB×T×d is the input to a linear layer with weight matrix W ∈Rp×d, and g∈RB×T×p\nis the gradient of the loss w.r.t. the output. The identity follows from trivial algebra:\n∥∇WLi∥2\nF =\ng⊤\ni ai\n2\nF =\n\nT∑\nk=1\ngi,ka⊤\ni,k\n\n2\nF\n=\nd∑\nr=1\np∑\ns=1\n( T∑\nk=1\nai,k,rgi,k,s\n)2\n=\nd∑\nr=1\np∑\ns=1\nT∑\nk1=1\nT∑\nk2=1\nai,k1,rgi,k1,sai,k2,rgi,k2,s\n=\nT∑\nk1=1\nT∑\nk2=1\n( d∑\nr=1\nai,k1,rai,k2,r\n)( p∑\ns=1\ngi,k1,sgi,k2,s\n)\n= vec(aia⊤\ni )⊤vec(gig⊤\ni ).\nNote that when T = 1, the identity takes the form of\n∥∇WLi∥2\nF = vec(aia⊤\ni )⊤vec(gig⊤\ni ) = ∥ai∥2\n2 ∥gi∥2\n2 .\nThis is exactly what is used in the Goodfellow (2015) trick.\nG P ROTOCOL FOR EXPERIMENTS IN SECTION 4.2\nFor these experiments, we used mock examples with the same format as examples in the E2E dataset.\nWe created mock input sequences of length 100, as this length is almost the maximum length of\nexamples in the actual E2E training set.\nOur JAX implementation is adapted from a codebase used in the work by Subramani et al. (2020)\nand utilizes the package flaxmodels for loading pretrained models.14 The Opacus (Yousefpour\net al., 2021) baseline is based on version 0.14.0 of the library, where for a fair comparison, we\nalso optimized the implementation of the privacy engine by replacing all einsum operations with\nbasic primitives that manipulate tensors directly. We found certain einsum steps to cause large and\nunnecessary speed and memory overheads.\nTo identify the maximum batch size that each approach could use, we ran binary search over the range\nof possible batch sizes until the upper bound matched the lower bound and that OOM did not occur.\nTo estimate the throughput of each private method, we compared them to non-private training ﬁrst.\nSpeciﬁcally, for a pairwise comparison, we took the maximum batch size for non-private training and\nsome private method, and computed the least common multiple as a batch size to perform updates\nwith. Using the least common multiple batch size ensures that both methods will process exactly the\nsame number of examples and perform the same number of updates. By estimating the time elapse of\nthese methods for performing a ﬁxed number of updates, we obtained throughputs for non-private\ntraining and the private method. This gives us the relative throughput of the private method with\nnon-private training as the reference.\nFor methods implemented in PyTorch, the time elapse was recorded with torch.profiler.\nWhen estimating the time elapse for a training procedure, we ﬁrst performed 3 gradients updates as a\nwarm up process before taking the actual steps which will be timed. In particular, this eliminates the\ntime that JAX uses for compiling computation graphs with vmap and jit.\nAll experimental runs in the section were under full precision.\n14https://github.com/matthias-wright/flaxmodels\n20\nPublished as a conference paper at ICLR 2022\nH D ETAILS AND ADDITIONAL RESULTS FOR STUDIES IN SECTION 3.1\nTable 4: Default hyperparameters for ablation studies.\nMethod Full\nDP guarantee (ϵ,δ) (3 ,1/2|Dtrain|)\nClipping norm C 0.1\nBatch size B 1024\nLearning rate η 10−3\nLearning rate decay no\nEpochs E 10 for E2E; 3 for SST-2\nWeight decay λ 0\nNoise scale σ calculated numerically so that a DP budget of (ϵ,δ) is spent after Eepochs\n(a) Batch size.\n0.1\n0.1\n22\n0.1\n24\n0.1\n26\n0.1\n28\nclipping norm C\n10 3\n10 3/22\n10 3/24\n10 3/26\n10 3/28\nlearning rate \n60.26 59.58 60.01 57.87 35.44\n50.48 50.31 50.48 49.25 34.47\n33.17 33.18 33.13 32.96 29.50\n29.74 29.55 29.61 29.62 27.07\n9.69 9.69 9.68 9.65 5.79\n0\n10\n20\n30\n40\n50\n60\nE2E test set BLEU (=3)\n (b) Clipping norm.\nFigure 8: Additional results on hyperparameter sensitivity.\nI H YPERPARAMETER SEARCH RANGES FOR EXPERIMENTS IN SECTION 5\nWe compare different adaptation methods by reporting task speciﬁc metrics on the test split using\nhyperparameters that maximize validation BLEU on E2E. For sentence classiﬁcation tasks, we reused\nthe same hyperparameters, except for the number of training epochs and batch size. For SST-2, we\nreused the same batch size as for private E2E ﬁne-tuning, and the number of epochs exactly as in\ntypical non-private ﬁne-tuning for SST-2 (number of epochs equals to 3 in this case). For remaining\nclassiﬁcation tasks, we use a batch size such that the sampling rate is the same as for SST-2, and a\nnumber of training epochs that is roughly proportional to the dataset size. Appendix L outlines why\nwe transfer the sampling rate as opposed to the batch size. We list the range of hyperparameters that\nwe searched over for each individual adaptation method on E2E considered in the paper. Preﬁx-tuning\nhas two additional hyperparameters: the length of the preﬁx and the dimensionality of the hidden\nlayer. We set these to the default used by Li & Liang (2021) (10 for the former and 512 for the latter).\nFor Adam, we use the default hyperparamaters set by PyTorch (Paszke et al., 2017).\n21\nPublished as a conference paper at ICLR 2022\nTable 5: Hyperparameter search range for different methods.\nMethod Full Preﬁx Linear FT2\nGuarantee (ϵ,δ) (3 ,1/2|Dtrain|) (3 ,1/2|Dtrain|) (3 ,1/2|Dtrain|) (3 ,1/2|Dtrain|)\nClipping norm C 0.1 0.1 0.1 0.1\nBatch size B {512,1024} { 512,1024} { 512,1024} { 512,1024}\nLearning rate η {200, 100, 30, 10, 3}·10−5 {200, 100, 30, 10, 3}·10−5 {200, 100, 30, 10, 3}·10−5 {200, 100, 30, 10, 3}·10−5\nLR decay {yes,no} { yes,no} { yes,no} { yes,no}\nEpochs E {10,30,50} { 10,30,50} { 10,30,50} { 10,30,50}\nWeight decay λ 0 0 0 0\nNoise scale σ calculated numerically so that a DP budget of (ϵ,δ) is spent after Eepochs\nTable 6: Hyperparameter search range for different methods (continued).\nMethod LoRA RGP\nDP guarantee (ϵ,δ) (3 ,1/2|Dtrain|) (3 ,1/2|Dtrain|)\nClipping norm C 0.1 {0.1,1,10}\nBatch size B {512,1024} { 512,1024}\nLearning rate η {300,100,30,10,3}·10−5 {300,100,30,10,3}·10−5\nLR decay {yes,no} { yes,no}\nEpochs E {5,10,30,50} { 5,10,30,50}\nWeight decay λ 0 0\nRank k {1,2,4,8} { 1,2,4,8}\nNoise scale σ calculated numerically so that a DP budget of (ϵ,δ) is spent after Eepochs\n22\nPublished as a conference paper at ICLR 2022\nJ F ULL RESULTS FOR EXPERIMENTS IN SECTION 5.2\nTable 7: Full results on E2E from ﬁne-tuning GPT-2.\nMethod DP Guarantee Gaussian DP Compose Metrics\n+CLT tradeoff func. BLEU NIST METEOR ROUGE-L CIDEr\nfull\nϵ= 3 ϵ≈2.33 ϵ≈2.67 61.519 6.697 0.384 65.670 1.761\nϵ= 8 ϵ≈5.51 ϵ≈6.98 63.189 7.444 0.400 66.429 1.919\nnon-private - - 69.463 8.780 0.461 71.359 2.422\nLoRA\nϵ= 3 ϵ≈2.68 ϵ≈2.75 58.153 5.463 0.370 65.773 1.581\nϵ= 8 ϵ≈6.77 ϵ≈7.28 63.389 7.449 0.407 67.525 1.948\nnon-private - - 69.682 8.822 0.463 71.709 2.491\npreﬁx\nϵ= 3 ϵ≈2.33 ϵ≈2.67 47.772 5.775 0.331 58.964 1.300\nϵ= 8 ϵ≈5.51 ϵ≈6.98 49.263 6.276 0.349 60.730 1.496\nnon-private - - 68.845 8.722 0.456 70.805 2.418\nRGP\nϵ= 3 ϵ≈2.18 ϵ≈2.59 58.482 5.249 0.363 65.560 1.507\nϵ= 8 ϵ≈5.19 ϵ≈6.89 58.455 5.525 0.364 65.030 1.569\nnon-private - - 68.328 8.722 0.445 68.844 2.345\ntop2\nϵ= 3 ϵ≈2.68 ϵ≈2.75 25.920 1.510 0.197 44.536 0.452\nϵ= 8 ϵ≈6.77 ϵ≈7.28 26.885 1.547 0.207 46.421 0.499\nnon-private - - 65.752 8.418 0.443 68.704 2.180\nretrain\nϵ= 3 ϵ≈2.33 ϵ≈2.67 15.457 0.376 0.113 35.240 0.116\nϵ= 8 ϵ≈5.51 ϵ≈6.98 24.247 1.010 0.145 39.951 0.281\nnon-private - - 65.731 8.286 0.429 68.751 2.004\nTable 8: Full results on DART from ﬁne-tuning GPT-2. Trend is consistent with results on E2E.\nMethod DP Guarantee Gaussian DP Compose Metrics\n+ CLT tradeoff func. METEOR ROUGE-1 ROUGE-2 ROUGE-L BLEU BERTScore BLEURT\nfull\nϵ= 3 ϵ≈2.28 ϵ≈2.65 0.294 62.815 40.773 52.063 31.025 0.887 -0.058\nϵ= 8 ϵ≈5.35 ϵ≈6.95 0.319 66.423 43.609 54.576 35.057 0.901 0.043\nnon-private - - 0.369 71.563 47.168 56.717 42.783 0.915 0.178\nLoRA\nϵ= 3 ϵ≈2.68 ϵ≈2.76 0.304 63.641 40.753 52.012 32.329 0.885 -0.029\nϵ= 8 ϵ≈6.68 ϵ≈7.26 0.318 66.336 43.056 54.082 34.163 0.899 0.036\nnon-private - - 0.366 71.192 47.336 57.430 42.254 0.915 0.182\npreﬁx\nϵ= 3 ϵ≈2.28 ϵ≈2.65 0.269 59.503 38.229 49.444 25.726 0.860 -0.144\nϵ= 8 ϵ≈5.35 ϵ≈6.95 0.297 64.009 41.581 52.602 30.463 0.892 -0.021\nnon-private - - 0.353 70.341 46.643 56.858 40.163 0.912 0.148\nRGP\nϵ= 3 ϵ≈2.10 ϵ≈2.54 0.265 58.688 37.202 49.011 25.748 0.873 -0.175\nϵ= 8 ϵ≈5.02 ϵ≈6.86 0.279 60.005 38.258 49.835 28.304 0.874 -0.141\nnon-private - - 0.324 65.667 42.617 53.477 35.551 0.895 0.022\ntop2\nϵ= 3 ϵ≈2.68 ϵ≈2.76 0.022 3.570 2.183 3.166 0.388 0.098 -1.952\nϵ= 8 ϵ≈6.68 ϵ≈7.26 0.054 11.475 7.054 10.042 2.453 0.240 -1.660\nnon-private - - 0.318 62.777 38.367 49.426 36.099 0.883 -0.082\nretrain\nϵ= 3 ϵ≈2.28 ϵ≈2.65 0.064 19.085 8.901 17.142 2.997 0.493 -1.513\nϵ= 8 ϵ≈5.35 ϵ≈6.95 0.093 24.971 11.938 21.680 7.765 0.573 -1.302\nnon-private - - 0.232 47.782 26.361 37.864 26.794 0.806 -0.593\n23\nPublished as a conference paper at ICLR 2022\nK D ETAILS FOR EXPERIMENTS IN SECTION 5.2\nSentence Classiﬁcation. Results for RGP in Table 1 are taken from documented numbers in\ntheir released codebase. 15 These results are under the DP guarantees of (ϵ,δ) = (3 ,10−5) or\n(ϵ,δ) = (8 ,10−5). These guarantees are strictly looser than our guarantees which are based on\nδ= 1/2|Dtrain|(recall the smallest dataset in this cohort of tasks has 60k+ records). The RGP numbers\nin Table 1 are higher than those reported in their paper (Yu et al., 2021c), since the latter numbers are\nnot based on ﬁne-tuning the ofﬁcial RoBERTa models.\nTable-To-Text Generation. To evaluate models trained on E2E and DART, we evaluate generations\nfrom models obtained with beam search with a beam size of 5. For evaluation, we run the ofﬁcial\npipeline for E2E,16 and the pipeline used in the GEM benchmark (Gehrmann et al., 2021) for DART.17\nChit-Chat Dialog Generation. We built off Huggingface’s codebase of the winning entry of the\nConvAI2 competition, 18 19 and used their preprocessed training set with the minor modiﬁcation of\ntruncating the number of training examples to be a multiple of the batch size. The original ConvAI2\ncompetition is aimed at advancing research on building engaging chatbots and also requested models\nto predict the mostly likely response given a list of candidates. The challenge included hits@1\nas part of its suite of automatic metrics. For simplicity, we skip this step of predicting the most\nlikely response for both training and evaluation. Results for the entry HuggingFace (ConvAI2\nwinner) in Table 3 are taken from the ofﬁcial validation set leader board.20 Our reimplementation of\nHuggingFace’s submission uses the released code for their winning entry, ﬁne-tunes GPT with the\ndefault hyperparameters, and removes the classiﬁcation loss for learning to predict the most likely\nresponse given candidates.\nWe additionally ﬁne-tuned DialoGPT-medium (Zhang et al., 2019), since the model was pretrained\non conversation-like exchanges extracted from Reddit comment chains. Intuitively, this pretraining\ncorpus is more aligned with the downstream ﬁne-tuning data than WebText (Radford et al., 2019),\nand thus would likely improve downstream performance.\nTo evaluate the F1 score, we obtained predicted responses from trained models using beam search\nwith a beam size of 5. Since past work found that the F1 score can be gamed by always letting\nthe model predict a predetermined phrase (Dinan et al., 2019), we additionally ask humans to rate\ngenerations from the model through Amazon mechanical turk to obtain a more complete picture\nof model quality. When sampling responses for human evaluation, we used nucleus sampling with\np= 0.9 (Holtzman et al., 2019). For human evaluation, we asked 20 turkers to each rate 5 entries.\nFor each entry, a turker is asked to rate on a scale of 1 to 5 the quality of predicted responses\nfrom privately ﬁne-tuned DialoGPT-medium models, non-privately ﬁne-tuned DialoGPT-medium\nmodels, non-privately ﬁne-tuned GPT models (our reimplementation of HuggingFace’s entry), and\nthe reference text, given the history of the dialog. Since human evaluation can yield noisy results,\nwe also report the 95% asymptotic conﬁdence interval in Table 3. All models were trained and\nevaluated on the version of Persona-Chat with the original persona. All numbers reported in Table 3\nare obtained on the validation split.\nL T RANSFERRING HYPERPARAMETERS ACROSS DATASETS\nOur work involved tuning hyperparameter with models trained via DP-Adam on one private dataset\nand transferring such hyperparameters to other private datasets. Since different datasets may be of\ndifferent sizes, transferring the batch size may cause a discrepancy in the effective noise multiplier\nacross workloads with different datasets. Transferring the batch size based on hyperparameter tuning\non small datasets to larger datasets can be particularly problematic, as the effective noise multiplier\n15https://github.com/dayu11/Differentially-Private-Deep-Learning/tree/\nmain/language\n16https://github.com/tuetschek/e2e-metrics\n17https://github.com/GEM-benchmark/GEM-metrics\n18https://github.com/huggingface/transfer-learning-conv-ai\n19http://convai.io/2018/\n20https://github.com/DeepPavlov/convai/blob/master/leaderboards.md\n24\nPublished as a conference paper at ICLR 2022\ncan be larger than ideal. In this work, we instead transferred the sampling rate q across different\ndatasets.\nM D OES DP F INE -TUNING PREVENT UNINTENDED MEMORIZATION ?\nOne of the ultimate goals of ﬁtting models under DP is to ensure that training data extraction is\nunlikely given the trained model. To empirically evaluate whether DP ﬁne-tuning helps prevent\nagainst unintended memorization and related attacks, we follow the secret sharer framework (Carlini\net al., 2019) and estimate the exposure of artiﬁcial canaries inserted into the training set used for\nﬁne-tuning. We use the E2E dataset as a testbed.\nTo create canaries, we ﬁrst form a subvocabulary by randomly samplingV = 10 words in the original\nvocabulary of GPT-2. Our canaries have preﬁxes of the form\n\" name : <word> | Type : <word> | area : <word> ”,\nwhere <word> is randomly sampled from the subvocabulary. The sufﬁx which our model should\nlearn to predict consists of randomly sampled words with an average length of l= 5. By deﬁnition,\ncanaries with an estimated exposure close to log2(Vl) ≈17 can likely be extracted. We experiment\nwith canary-corrupted datasets for repetition values r∈{1,10,100}. A canary has a higher chance\nin being extracted when it’s repeated for more than once in the training data.\nTable 9: Fine-tuning under DP prevents unintended memorization of downstream data. Numbers\nreported are exposure values estimated with the approximation by distribution model approach.\n```````````Guarantee\nRepetitions r= 1 r= 10 r= 100\nϵ= 3 1 .09 ±0.86 1 .32 ±1.32 5 .26 ±4.20\nnon-private 13.82 ±3.86 17.22 ±0.00 17.78 ±5.49\nN T EMPLATES AND LABEL WORDS FOR TEXT-INFILLING -BASED\nCLASSIFICATION IN SECTION 3.2\nRecall that ﬁne-tuning for classiﬁcation can be reformulated as ﬁlling in the [MASK] token in a\ntemplate sequence. Here, we list the templates used for each classiﬁcation task considered in the\npaper. These templates are almost generic and are not obtained from expensive manual or automated\nsearch. We anticipate better templates obtained from automated search based on data (Gao et al.,\n2020) to improve the performance even further. However, we also expect that such a procedure would\nlead to some amount of increased privacy spending if it were based on private data.\nTask Template Label words\nSST-2 <S1 > It was [MASK] . positive: great, negative: terrible\nMNLI <S1 > ? [MASK] , <S2 > entailment: Yes, netural: Maybe, contradiction: No\nQNLI <S1 > ? [MASK] , <S2 > entailment: Yes, not entailment: No\nQQP <S1 > [MASK] , <S2 > equivalent: Yes, not equivalent: No\nTable 10: Templates and label words borrowed from the work by Gao et al. (2020).\n25\nPublished as a conference paper at ICLR 2022\nO U NCURATED SAMPLES FROM FINE -TUNED MODELS\nTable name : The Punter — Type : restaurant — food : Indian — price : cheap — customer\nrating : average — area : riverside — family friendly : no — near : Express by Holiday\nInn\nGPT-2 (ϵ= 3) The Punter is a cheap Indian restaurant near Express by Holiday Inn in the riverside area.\nIt is not family - friendly.\nGPT-2 (ϵ= 8) The Punter is a cheap Indian restaurant near Express by Holiday Inn in the riverside area.\nIt is not family - friendly.\nGPT-2-m (ϵ= 3) The Punter is a cheap Indian restaurant located in the riverside area near Express by\nHoliday Inn. It has an average customer rating and is not family - friendly.\nGPT-2-m (ϵ= 8) The Punter is a restaurant providing Indian food in the cheap price range. It is located in\nthe riverside area near Express by Holiday Inn. Its customer rating is average.\nGPT-2-l (ϵ= 3) The Punter is a cheap Indian restaurant in the riverside area near Express by Holiday Inn.\nIt is not family - friendly and has an average customer rating.\nGPT-2-l (ϵ= 8) The Punter is a restaurant providing Indian food in the cheap price range. It is located in\nthe riverside area near Express by Holiday Inn. Its customer rating is average.\nReference The restaurant named The Punter has cheap Indian food and an average customer rating .\nIt is near the Express by Holiday Inn on the riverside and is not family friendly .\nTable name : The Mill — Type : restaurant — food : English — price : moderate — customer\nrating : 3 out of 5 — area : city centre — family friendly : yes — near : Caf ´e Rouge\nGPT-2 (ϵ= 3) The Mill is a moderately priced restaurant located in the city centre near Caf´e Rouge.\nGPT-2 (ϵ= 8) The Mill is a moderately priced restaurant located in the city centre near Caf´e Rouge.\nGPT-2-m (ϵ= 3) The Mill is an English restaurant located in the city centre near Caf ´e Rouge. It is\nmoderately priced and has a customer rating of 3 out of 5.\nGPT-2-m (ϵ= 8) The Mill is a moderately priced restaurant located in the city centre near Caf´e Rouge. It is\nchild friendly and has a customer rating of 3 out of 5.\nGPT-2-l (ϵ= 3) The Mill is a moderately priced English restaurant in the city centre near Caf´e Rouge. It\nis child friendly and has a customer rating of 3 out of 5.\nGPT-2-l (ϵ= 8) The Mill is a kid friendly English restaurant in the city centre near Caf´e Rouge. It has a\nmoderate price range and a customer rating of 3 out of 5.\nReference Serving moderately priced English food with a 3 out of 5 customer approval , The Mill\nrestaurant is kid friendly and conveniently located at the city centre near the Caf´e Rouge .\nTable name : The Vaults — Type : pub — food : Japanese — price : high — customer rating : 3\nout of 5 — area : city centre — family friendly : yes — near : Raja Indian Cuisine\nGPT-2 (ϵ= 3) Located near Raja Indian Cuisine in the city centre, The Vaults is a family - friendly\nJapanese pub with a high customer rating.\nGPT-2 (ϵ= 8) The Vaults is a Japanese pub in the city centre near Raja Indian Cuisine. It has a high\nprice range and a customer rating of 3 out of 5.\nGPT-2-m (ϵ= 3) The Vaults is a Japanese pub located in the city centre near Raja Indian Cuisine. It has a\nhigh price range and a customer rating of 3 out of 5.\nGPT-2-m (ϵ= 8) The Vaults is a Japanese pub located in the city centre near Raja Indian Cuisine. It has a\nhigh price range and a customer rating of 3 out of 5.\nGPT-2-l (ϵ= 3) The Vaults is a Japanese pub in the city centre near Raja Indian Cuisine. It has a high\nprice range and a customer rating of 3 out of 5.\nGPT-2-l (ϵ= 8) The Vaults is a child friendly Japanese pub in the city centre near Raja Indian Cuisine. It\nhas a high price range and a customer rating of 3 out of 5.\nReference Located near the Raja Indian Cuisine at the city centre , is family - friendly pub The Vaults\n. Serving Japanese food , The Vaults , is high priced with a 3 out of 5 customer satisfaction\nrating .\nTable 11: Fully ﬁne-tuned GPT-2, GPT-2-medium, and GPT-2-large generations with E2E test table\nentries.\n26\nPublished as a conference paper at ICLR 2022\nTable Real Madrid Castilla : manager : Luis Miguel Ramis — Abner (footballer) : club : Real\nMadrid Castilla — Abner (footballer) : club : C.D. FAS\nGPT-2 (ϵ= 3) Luis Miguel Ramis played for Real Madrid Castilla and played for C.D. FAS.\nGPT-2 (ϵ= 8) Luis Miguel Ramis is the manager of Abner (footballer) who plays for Real Madrid\nCastilla.\nGPT-2-m (ϵ= 3) Luis Miguel Ramis is the manager of Real Madrid Castilla. He plays for C.D. FAS.\nGPT-2-m (ϵ= 8) Luis Miguel Ramis is the manager of Real Madrid Castilla. He plays for C.D. FAS.\nGPT-2-l (ϵ= 3) Luis Miguel Ramis is the manager of Real Madrid Castilla and C.D. FAS.\nGPT-2-l (ϵ= 8) Luis Miguel Ramis is the manager of Real Madrid Castilla and Abner (footballer) plays\nfor C.D. FAS.\nReference Footballer, Abner, plays C.D. FAS. and Real Madrid Castilla, the manager of which, is\nLuis Miguel Ramis.\nTable United States : ethnic group : Asian Americans — United States : capital : Washington,\nD.C. — Albany, Oregon : is part of : Benton County, Oregon — Albany, Oregon : country\n: United States\nGPT-2 (ϵ= 3) The capital of the United States is Washington, D.C. and is part of Benton County, Oregon.\nGPT-2 (ϵ= 8) The capital of the United States is Washington, D.C. and is part of Benton County, Oregon.\nGPT-2-m (ϵ= 3) Albany, Oregon is part of Benton County, Oregon in the United States where Asian\nAmericans are an ethnic group.\nGPT-2-m (ϵ= 8) Albany, Oregon is part of Benton County, Oregon in the United States where Asian\nAmericans are an ethnic group.\nGPT-2-l (ϵ= 3) Albany, Oregon is part of the United States where Asian Americans are an ethnic group\nand the capital is Washington D.C.\nGPT-2-l (ϵ= 8) Albany, Oregon is part of the United States where Asian Americans are an ethnic group\nand the capital is Washington D.C.\nReference The Asian Americans are an ethnic group in the United States, which has the capital city\nof Washington DC. It is also the location of Albany, part of Benton County in Oregon.\nTable A Loyal Character Dancer : language : English language — English language : spoken in\n: Great Britain — A Loyal Character Dancer : country : United States — United States :\nethnic group : Native Americans in the United States\nGPT-2 (ϵ= 3) A Loyal Character Dancer is an English language spoken in the United States where\nNative Americans are the ethnic group.\nGPT-2 (ϵ= 8) A Loyal Character Dancer is written in English and is spoken in Great Britain. Native\nAmericans are an ethnic group in the United States.\nGPT-2-m (ϵ= 3) A Loyal Character Dancer is written in English and is written in the United States where\nNative Americans are an ethnic group.\nGPT-2-m (ϵ= 8) A Loyal Character Dancer is written in English and is written in the United States where\nNative Americans are an ethnic group.\nGPT-2-l (ϵ= 3) A Loyal Character Dancer is written in English and is written in the United States where\nNative Americans are an ethnic group.\nGPT-2-l (ϵ= 8) A Loyal Character Dancer is written in English, which is spoken in Great Britain and the\nUnited States. Native Americans are an ethnic group in the United States.\nReference A Loyal Character Dancer is written in English which is spoken in Great Britain. The\nbook is published in the United States where the Native Americans are an ethnic group.\nTable 12: Fully ﬁne-tuned GPT-2, GPT-2-medium, and GPT-2-large generations with DART test\ntable entries.\n27\nPublished as a conference paper at ICLR 2022\nPersona [’i am 40 years old.’, ’i work as a car salesman.’, ’my wife spends all my money.’, ’i hate\nmy job.’, ”i’m planning on getting a divorce.”]\nHistory [’i know the struggle, i volunteer at a homeless shelter’, ”great, i hope you don’t get to see\nme there.”, ’i hope not either, would not wish it on my worse enemy’, ’between wife, job\nwith no money, i am thinking now of divorce.’, ”i’ve given it a few thoughts before”]\nGPT-2-m (ϵ= 3) i wish i had a better idea .\nGPT-2-m (ϵ= 8) i think i will . what are your thoughts on divorce ?\nDialoGPT-medium (ϵ= 3) i’m sorry to hear that . i hope you don’t have any ﬁnancial problems .\nDialoGPT-medium (ϵ= 8) i don’t have any money , i do not know how i could manage .\nReference do you give demonstrations of your skills ?\nPersona [’my mother is a dentist.’, ’my favourite artist is travis scott.’, ’i have one older sister.’, ’i\nam currently looking for a job preferably at a restaurant.’, ’i like playing basketball.’]\nHistory [’hello, how are you tonight?’]\nGPT-2-m (ϵ= 3) i’m good , good luck\nGPT-2-m (ϵ= 8) i am good , just listening to travis scott . what about you ?\nDialoGPT-medium (ϵ= 3) i’m good . just got back from a dentist visit .\nDialoGPT-medium (ϵ= 8) i am doing well . just got back from a dentist visit\nReference i am good . about to go to basketball practice .\nPersona [’my mom passed away when i was young.’, ’i work in the insurance business.’, ’my father\nis in the army.’, ’i recently was ﬁred and had to get a new job.’]\nHistory [”god isn’t really real”, ’to each their own, but i believe he is.’, ’my children are gone\nnow’, ’i lost my mom at a young age’, ’my wife took them all’]\nGPT-2-m (ϵ= 3) she was a great mother\nGPT-2-m (ϵ= 8) they were so young , they were so crazy\nDialoGPT-medium (ϵ= 3) that is horrible . my mom was a soldier\nDialoGPT-medium (ϵ= 8) what happened to her ?\nReference sorry to hear that . i barely see my dad he’s in the army\nPersona [’i am also a graduate student.’, ’i walk dogs for a living.’, ’my parents taught me survival\nskills.’, ’i am an avid kayaker.’, ’i enjoy reading journals and guides related to psychology.’]\nHistory [’hi, how are you today?’, ”good evening. how are you tonight? i’m well.”, ’i am doing\ngood, i just ﬁnished playing ping pong.’, ’i am trying to write invoices for my dog walking\nbusiness.’, ’that seems like hare work, i like to play the violin in my free time.’]\nGPT-2-m (ϵ= 3) i have a lot of dogs , i walk them on my walks .\nGPT-2-m (ϵ= 8) that is a very rewarding hobby . i have a dog named jasper .\nDialoGPT-medium (ϵ= 3) oh , that is a nice hobby . what do you do for a living ?\nDialoGPT-medium (ϵ= 8) it is . i love learning to play the violin .\nReference i love the violin . i do not play anything . i kayak for fun however .\nTable 13: Fully ﬁne-tuned GPT-2-medium and DialoGPT-medium generations for Persona-Chat\nvalidation examples.\n28\nPublished as a conference paper at ICLR 2022\nP A DDITIONAL RELATED WORK\nDifferentially Private Deep Learning. DP-SGD has been viewed as ineffective for large models\ndue to the addition of large Gaussian noise to gradient updates. Improvements to the learning\nprocedure mostly fall under two distinct camps: (i) Simplifying the private learning problem, and\n(ii) reducing the scale of noise. For instance, Papernot et al. (2019); Tram`er & Boneh (2020); Abadi\net al. (2016) consider transferring features learned on public datasets to simplify the subsequent\nprivate learning task. On the other hand, Zhou et al. (2020); Kairouz et al. (2020) remove the ambient\ndimension dependence of DP noise by identifying subspaces in which private gradients lie and would\nbe privatized. Yu et al. (2021b;c) make such ideas practical and demonstrate improved results on\nprivate learning benchmarks. Zhang et al. (2021) applied the sparse vector technique to learning wide\nneural layers to reduce the amount of injected noise. Our work mostly falls under the ﬁrst camp –\nimproving private learning through simplifying the learning task. Our work is also distinct from prior\nworks in that we focus on privately ﬁne-tuning large pretrained models. Lastly, there are alternative\nsolutions in the literature that enforces DP which are not based on gradient perturbation (Papernot\net al., 2018; 2016). These methods typically require extra public data and are not the present focus.\nParameter-Efﬁcient Fine-Tuning. Recent developments on pretrained model adaptation have\nproduced a wide range of parameter-efﬁcient ﬁne-tuning methods for both vision and language tasks.\nWe brieﬂy summarize these, grouping by category. Approaches based on optimizing prompt-like\nconstructions for NLP tasks include preﬁx-tuning (Li & Liang, 2021), P-tuning (Liu et al., 2021),\nand prompt-tuning (Lester et al., 2021). Adapter-based methods insert small subnetworks inside\npretrained Transformers (Houlsby et al., 2019; R¨uckl´e et al., 2020; Pfeiffer et al., 2020). Methods that\noptimize low-rank matrices include the work by Hu et al. (2021); Mahabadi et al. (2021). In addition,\nthere are adaptation methods that only optimize biases for vision (Cai et al., 2020) and language\ntasks (Ben Zaken et al., 2021). Our evaluation in Section 5.2 covered the most representative methods\nthat generally have state-of-the-art non-private learning performance (at the time of writing) for the\nrange of NLP tasks studied in this paper.\nSpeeding Up DP-SGD. Apart from the work by Lee & Kifer (2020) and Subramani et al. (2020),\nthere is an approach that approximates per-example gradient norms through the combination of ran-\ndom projection and forward-mode autodiff (Bu et al., 2021). While faster than vanilla private learning,\nthis approach has the drawback of increased privacy spending and having an extra hyperparameter.\nOur ghost clipping technique, while only suited for Transformers applied to sequential data, does not\nintroduce new hyperparameters.\nAlternative Clipping Strategies. While there are alternative clipping strategies in the literature\nthat show improvements on simple tasks (Pichapati et al., 2019; Asi et al., 2021), we have opted to\nstudy the simplest strategy that clips gradients by their Euclidean norm. We leave the study of these\nalgorithms for NLP tasks to future work.\nConcurrent Work. We are made aware of a concurrent work that also studies ﬁne-tuning large\nlanguage models under DP (Yu et al., 2021a). This work presents initial successes on ﬁne-tuning under\nDP with low-rank methods such as LoRA. Our experiments on language generation (see Section 5.2\nand Table 2) demonstrate similar ﬁndings. Yet, we moreover show that full ﬁne-tuning with good\nhyperparameters attains similar performance and possesses similar model scaling properties, which\nwas raise by Yu et al. (2021a) as interesting open questions to pursue. Lastly, our private ﬁne-tuning\nresults for sentence classiﬁcation may be far from optimal, since we used hyperparameters mostly\ntransferred from tuning on the E2E language generation task.\nDP Synthetic Data Generation. Fine-tuning generative language models on private data under\nDP can also be viewed as a means of accomplishing DP synthetic data generation – learning\ngenerative models from private data so that synthetic examples could be sampled and used for\nanalysis. Previous work employed generative adversarial networks and focused primarily on image or\ntabular datasets (Torkzadehmahani et al., 2019; Neunhoeffer et al., 2020; Chen et al., 2020; Torﬁ et al.,\n2020). Perhaps more related is the work by Bommasani et al. (2019) which attempted ﬁne-tuning\nGPT-2 on medical datasets to generate synthetic records but did not report any quantitative results.\n29\nPublished as a conference paper at ICLR 2022\nQ E STIMATES OF RUN-TIME IN PRACTICE\nThe actual run-time of algorithms depends on implementation details. Here, we outline estimates\nof the run-time for full ﬁne-tuning with DP-Adam on tasks considered in the paper. These numbers\nare based on running with a single RTX 3090 with PyTorch==1.9.0. Fine-tuning GPT-2 on E2E\nand DART takes less than 10 minutes per epoch, and ﬁne-tuning for 10 epochs results in reasonably\nperforming models. The time to ﬁne-tune RoBERTa-base on classiﬁcation tasks depends on the size\nof the dataset. It takes less than 10 minutes per epoch on the smallest SST-2, whereas for the largest\nMNLI, it takes less than an hour per epoch.\nR E FFECTS OF VARYING ϵ AND δ ON DP F ULL FINE -TUNING PERFORMANCE\nFigure 9 shows how different values of ϵand δaffect the performance of full ﬁne-tuning under DP.\n10 7\n10 6\n10 5\n56\n57\n58\n59\n60\n61\n62\n63E2E test set BLEU\n= 3\n= 8\n0 1 2 3 4 5 6 7 8\n0\n10\n20\n30\n40\n50\n60E2E test set BLEU\n= 10 5\nFigure 9: Left: δaffects performance marginally. Right: ϵaffects performance more signiﬁcantly\nwhen ϵ< 2. Errorbars are one standard deviation away from the mean over ﬁve independent runs.\nS DP-A DAM VS DP-SGD\nOur work focused on DP-Adam as opposed to DP-SGD, since Adam is more commonly used for\nnon-private language model ﬁne-tuning. While the two algorithms differ in their parameter update\nrule, the basic gradient privatization procedure is the same. We performed additional experiments\nﬁne-tuning GPT-2 on the E2E dataset with DP-SGD (with freshly tuned learning rate and clipping\nnorm values) and observed that its performance (test set BLEU 63.175 at (ϵ,δ) = (8,10−5)) is on\npar with DP-Adam (test set BLEU 63.189 at (ϵ,δ) = (8,10−5)).\nT S UBTLETIES OF IMPLEMENTING DP MIXED PRECISION TRAINING\nMixed precision training (Micikevicius et al., 2017) accelerates updates by storing certain tensors\nin half-precision. To mitigate negative effects caused by potential arithmetic underﬂow, usual\nimplementations upscale the loss with an adaptive factor pre-backpropagation and downscale the\ngradients with the same factor post-backpropagation. The scaling factor is adapted based on whether\nunderﬂow is observed during training.\nSpecial care needs to be taken when combining gradient privatization with mixed precision training.\nOne implementation that ensures similar results across full and mixed precision training (1) upscales\nthe loss with the adaptive factor K, (2) clips per-example gradients by CK, (3) adds to the sum of\nclipped gradients the usual Gaussian noise multiplied by K, and (4) downscales the noisy gradient by\nK. This is the implementation that we adopt, and we were able to obtain similar results on the E2E\ndataset with and without mixed precision.\nOne alternative implementation (a) upscales the loss with the adaptive factorK, (b) clips per-example\ngradients by C, (c) adds the usual Gaussian noise to the sum of clipped gradients, and (d) downscales\nnoisy gradients by K. The main difference between this procedure and the prior is whether the\n30\nPublished as a conference paper at ICLR 2022\nfactor K is considered during clipping and noising. This procedure, while having the same DP\nguarantee, typically does not result in similar results as full precision when the same hyperparameters\nare used across the two settings (even with an optimizer like Adam which self-adjusts the magnitude\nof updates with accumulated empirical second moments). We also identiﬁed that this implementation\nis also the primary reason that a prior work’s code does not produce good results with full ﬁne-tuning\nusing our near-optimal hyperparameters.21\nU F INE -TUNING WITH RGP AND THE TEXT -INFILLING OBJECTIVE\nRecall Section 3.2 and Table 1 showed that private full ﬁne-tuning with a text-inﬁlling objective leads\nto improved classiﬁcation results. Here, we show that the inﬁlling objective is also helpful when one\nprivately ﬁne-tunes with the RGP method for a classiﬁcation task. To study this, we reimplemented\nthe RGP method and tuned the hyperparameters in full precision. Fixing all hyperparameters, we\ncompared models trained with and without inﬁlling. Table 14 conﬁrms that ﬁne-tuning with RGP\nbased on inﬁlling is generally helpful across the considered tasks. Note the aim of this experiment\nis not obtain state-of-the-art performance, but rather to study the effect of using the text-inﬁlling\nobjective. Thus, we expect these results could generally be further improved with more extensive\nhyperparameter search.\nTable 14: Text-inﬁlling objective improves the performance of RGP for classiﬁcation. Numbers are\naveraged over three independent runs.\nMethod\nϵ= 8\nMNLI-(m/mm) QQP QNLI SST-2\nRGP (RoBERTa-base) 79.79/80.40 83.58 84.14 89.60\nRGP + inﬁlling (RoBERTa-base) 81.97/82.28 84.02 87.20 92.85\nV W HICH METHOD SHALL I USE FOR PRIVATE FINE -TUNING ?\nWhile we were able to obtain similar results on the E2E and DART datasets with full ﬁne-tuning and\nLoRA (Tables 1 and 8), we encountered signiﬁcant difﬁculties in ﬁne-tuning for dialog generation\n(even non-privately) when not updating the embedding layer and language modeling head – per-\nplexity was much worse, and generations frequently contained seemingly arbitrary characters. This\nresult suggests that while full ﬁne-tuning may be more computationally intensive than lightweight\napproaches at times, its simplicity (involving few design decisions) makes it an attractive ﬁrst option\nwhen compute resource is sufﬁcient.\n21https://github.com/dayu11/Differentially-Private-Deep-Learning\n31"
}