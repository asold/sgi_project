{
  "title": "A method for multiple-sequence-alignment-free protein structure prediction using a protein language model",
  "url": "https://openalex.org/W4387451333",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2232556478",
      "name": "Xiaomin Fang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2080195193",
      "name": "Fan Wang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2717411603",
      "name": "Lihang Liu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2399894338",
      "name": "Jingzhou He",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2167700071",
      "name": "Dayong Lin",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2769616627",
      "name": "Yingfei Xiang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3008982121",
      "name": "Kunrui Zhu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2120652065",
      "name": "Xiaonan Zhang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2108555996",
      "name": "Hua Wu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1977052956",
      "name": "Hui Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105801009",
      "name": "Le Song",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2232556478",
      "name": "Xiaomin Fang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2080195193",
      "name": "Fan Wang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2717411603",
      "name": "Lihang Liu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2399894338",
      "name": "Jingzhou He",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2167700071",
      "name": "Dayong Lin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2769616627",
      "name": "Yingfei Xiang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3008982121",
      "name": "Kunrui Zhu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2120652065",
      "name": "Xiaonan Zhang",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2108555996",
      "name": "Hua Wu",
      "affiliations": [
        "Baidu (China)"
      ]
    },
    {
      "id": "https://openalex.org/A1977052956",
      "name": "Hui Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2105801009",
      "name": "Le Song",
      "affiliations": [
        "Baidu (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3177828909",
    "https://openalex.org/W2073758233",
    "https://openalex.org/W2970476646",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W6763868836",
    "https://openalex.org/W4287724045",
    "https://openalex.org/W3111174583",
    "https://openalex.org/W3193589100",
    "https://openalex.org/W3191761521",
    "https://openalex.org/W4281291878",
    "https://openalex.org/W4313430582",
    "https://openalex.org/W3193271391",
    "https://openalex.org/W3198923619",
    "https://openalex.org/W3171848268",
    "https://openalex.org/W3186179742",
    "https://openalex.org/W2161151688",
    "https://openalex.org/W3133458480",
    "https://openalex.org/W2997234557",
    "https://openalex.org/W1987134040",
    "https://openalex.org/W3212854871",
    "https://openalex.org/W2152811165",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W2747329762",
    "https://openalex.org/W2557595285",
    "https://openalex.org/W2102461176",
    "https://openalex.org/W2130479394",
    "https://openalex.org/W3104537585",
    "https://openalex.org/W3211795435",
    "https://openalex.org/W6894159261"
  ],
  "abstract": "Abstract Protein structure prediction pipelines based on artificial intelligence, such as AlphaFold2, have achieved near-experimental accuracy. These advanced pipelines mainly rely on multiple sequence alignments (MSAs) as inputs to learn the co-evolution information from the homologous sequences. Nonetheless, searching MSAs from protein databases is time consuming, usually taking tens of minutes. Consequently, we attempt to explore the limits of fast protein structure prediction by using only primary structures of proteins. Our proposed method, HelixFold-Single, combines a large-scale protein language model with the superior geometric learning capability of AlphaFold2. HelixFold-Single first pre-trains a large-scale protein language model with thousands of millions of primary structures utilizing the self-supervised learning paradigm, which will be used as an alternative to MSAs for learning the co-evolution information. Then, by combining the pre-trained protein language model and the essential components of AlphaFold2, we obtain an end-to-end differentiable model to predict the three-dimensional coordinates of atoms from only the primary structure. HelixFold-Single is validated on datasets CASP14 and CAMEO, achieving competitive accuracy with the MSA-based methods on targets with large homologous families. Furthermore, HelixFold-Single consumes much less time than the mainstream pipelines for protein structure prediction, demonstrating its potential in tasks requiring many predictions.",
  "full_text": "Nature Machine Intelligence | Volume 5 | October 2023 | 1087–1096\n 1087\nnature machine intelligence\nhttps://doi.org/10.1038/s42256-023-00721-6\nArticle\nA method for multiple-sequence-alignment- \nfree protein structure prediction using a \nprotein language model\nXiaomin Fang1,3, Fan Wang    1,3 , Lihang Liu    1,3, Jingzhou He1, Dayong Lin1, \nYingfei Xiang    1, Kunrui Zhu1, Xiaonan Zhang1, Hua Wu1, Hui Li2 & Le Song    2 \nProtein structure prediction pipelines based on artificial intelligence, such \nas AlphaFold2, have achieved near-experimental accuracy. These advanced \npipelines mainly rely on multiple sequence alignments (MSAs) as inputs \nto learn the co-evolution information from the homologous sequences. \nNonetheless, searching MSAs from protein databases is time consuming, \nusually taking tens of minutes. Consequently, we attempt to explore the \nlimits of fast protein structure prediction by using only primary structures \nof proteins. Our proposed method, HelixFold-Single, combines a large-scale \nprotein language model with the superior geometric learning capability \nof AlphaFold2. HelixFold-Single first pre-trains a large-scale protein \nlanguage model with thousands of millions of primary structures utilizing \nthe self-supervised learning paradigm, which will be used as an alternative \nto MSAs for learning the co-evolution information. Then, by combining \nthe pre-trained protein language model and the essential components of \nAlphaFold2, we obtain an end-to-end differentiable model to predict the \nthree-dimensional coordinates of atoms from only the primary structure. \nHelixFold-Single is validated on datasets CASP14 and CAMEO, achieving \ncompetitive accuracy with the MSA-based methods on targets with large \nhomologous families. Furthermore, HelixFold-Single consumes much \nless time than the mainstream pipelines for protein structure prediction, \ndemonstrating its potential in tasks requiring many predictions.\nProteins participate in essentially all biological processes and play \ncritical roles for an organism. The structures of proteins are highly \ncorrelated to their functions in biological processes. Determining the \nprotein structures to understand their functions can make considerable \ncontributions to life science.\nIn recent years, protein structure prediction technologies based \non artificial intelligence have made sunstantial progress in prediction \naccuracy, demonstrating great prospects for the drug and vaccine \nindustry. In particular, AlphaFold2 (ref. 1) has pushed the performance \nto a new frontier in the challenging 14th Critical Assessment of Protein \nStructure Prediction (CASP14) (ref. 2 ), approaching the accuracy of \nexperimental determination methods. Mainstream protein struc -\nture prediction pipelines rely heavily on co-evolution information \nextracted from multiple sequence alignments (MSAs). MSAs can \nbe simply regarded as protein chains similar to the target protein \nchain in sequence. An MSA is related to the co-evolution informa\n-\ntion of protein sequences, which is crucial to predicting its structure. \nHowever, over-reliance on MSAs becomes the bottleneck of various \nReceived: 17 August 2022\nAccepted: 18 August 2023\nPublished online: 9 October 2023\n Check for updates\n1Baidu Inc., NLP, Shenzhen, China. 2BioMap, Beijing, China. 3These authors contributed equally: Xiaomin Fang, Fan Wang, Lihang Liu.  \n e-mail: wang.fan@baidu.com; songle@biomap.com\nNature Machine Intelligence | Volume 5 | October 2023 | 1087–1096 1088\nArticle https://doi.org/10.1038/s42256-023-00721-6\nby AlphaFold, the capacities of the geometric models used by these \nmethods, such as recursive models and residual neural networks, are \nalso unsatisfactory in understanding the co-evolution and spatial rela-\ntions between the residues in a single sequence.\nInspired by the progress of PLMs and AlphaFold2, we propose \nan end-to-end MSA-free protein structure prediction pipeline, \nHelixFold-Single. The model used in HelixFold-Single consists of two \nmajor components: a large-scale PLM as the foundation and the essen-\ntial components from AlphaFold2 for folding. The PLM can encode the \nprimary structure into single representation and pair representation \nto learn the domain knowledge. The Evoformer and Structure modules \nfrom AlphaFold2 are then integrated to process the representation, \nlearn the geometric knowledge and then predict the coordinates of \natoms. The two components are connected to give an end-to-end dif-\nferentiable model. HelixFold-Single contains two training stages. In the \nfirst stage, the large-scale PLM is trained with thousands of millions of \nunlabelled single sequences by the task of masked language prediction. \nIn the second stage, we train the whole model with protein structures \ncomposed of experimental ground truth and augmentation structures \ngenerated by AlphaFold2.\nWe compare HelixFold-Single with AlphaFold2 and RoseTTAFold \non datasets CASP14 and CAMEO (Continuous Automated Model Evalu-\nation). HelixFold-Single achieves accuracy competitive with that of the \nother methods on proteins with sufficient numbers of homologous \nsequences. We also analyse the performance of HelixFold-Single on tar-\ngets with various numbers of homologous sequences: HelixFold-Single \nis capable of providing accurate structure predictions on most targets, \nespecially targets with large homologous families. An ablation study \ncomparing PLMs of different sizes demonstrates the importance of the \nsize of the PLM for structure prediction. Furthermore, HelixFold-Single \nshows great superiority in prediction efficiency when compared with \nthe MSA-based methods and could be applied to protein-related tasks \ndemanding a great number of predictions. Specifically, we investigate \nHelixFold-Single’s precision on various types of representative protein, \nincluding peptides, antibodies and nanobodies, with the aim of assessing \nits potential for application in therapeutic protein design. Our results \nsuggest that HelixFold-Single performs well in predicting flexible regions \nof these proteins, highlighting its strengths for such applications.\nHelixFold-Single\nHelixFold-Single aims to take advantage of both the PLM and the \nmain modules used in AlphaFold2 for single-sequence-based protein \nprotein-related tasks. Compared with the time (usually a few seconds) \nrequired for model inference in the structure prediction pipeline, \nsearching MSAs is time consuming, costing tens of minutes for a pro-\ntein. The time-consuming search is destructive in tasks demanding \nhigh-throughput requests, such as protein design. In the design of \ntherapeutic proteins, such as peptides and antibodies, large-scale \nvirtual screening is typically used to sift through candidate protein \ndatasets to identify potential drugs that can be further validated for a \nspecific target protein. A precise and efficient protein structure pre-\ndiction method could potentially accelerate the development of new \ndrugs for treating a variety of diseases.\nConsequently, designing an accurate and efficient MSA-free pro-\ntein structure prediction method to is likely to benefit and acceler -\nate the development of protein studies. We argue that a large-scale \nprotein language model (PLM) can serve as an alternative to the \nMSAs to learn the co-evolution knowledge for MSA-free prediction. \nAn MSA-based method uses the information retrieval technique to \nexplicitly capture co-evolutionary information of a target protein from \nthe protein sequence databases, while a PLM-based method embeds \nco-evolutionary information into the large-scale model parameters \nduring training and performs an implicit retrieval through model \ninference, where the PLM can be regarded as a protein knowledge \nbase3. An MSA-based method is less efficient in retrieving information \nand depends on the retrieval scheme designed manually. On the other \nhand, a PLM-based method is more efficient in information retrieval, \nand the quality of retrieval depends primarily on the model’s capacity \nor parameter size. The past few years have seen tremendous success of \nlarge-scale language models4–6 in natural language processing, a field \nthat shares many characteristics with protein study. With an increasing \nnumber of model parameters, the capacity for learning language knowl-\nedge grows substantially. Using self-supervised learning on large-scale \nunlabelled proteins, PLMs can reveal the long-range interactions along \nprotein sequences and improve downstream protein-related tasks. \nAdvanced works have attempted to adopt PLMs to enhance the perfor-\nmance of multiple downstream tasks, such as estimating the secondary \nstructures and the functions7–10. In particular, several studies11–13 have \nattempted to apply PLMs to protein structure prediction. Most works \nfirst predict the inter-residue two-dimensional geometry using neural \nnetworks and then reconstruct the three-dimensional (3D) structure \non the basis of energy minimization, which cannot provide end-to-end \n3D structure prediction. Moreover, compared with the geometric \nlearning capability of the Evoformer and Structure modules proposed \nEvoformerS\n(n EvoformerS\nblocks)\nStructure \nmodule\n(n Structure  blocks)\nPair repr.\nSingle repr.\nProtein \nlanguage \nmodel\n(n PLM  blocks)\nAdaptor\nSingle repr. Single repr.\nPair repr.\nAttention\nweights\nRecycle\nGeometric Modelling\n~300M primary \nsequences\n~120K determined \nstructures\n~1M estimated \nstructures\nPre-train with\nself-supervised learning\nHelixFold-Single\nGWEIPEPYVWDESFR\nPLM Base\nInput primary sequence\nTrain with\nsupervised learning\nFig. 1 | The framework of HelixFold-Single. It consists of a protein language model as PLM Base, the composite of the EvoformerS (revised from Evoformer) and \nStructure Module of AlphaFold2 as Geometric Modelling, and Adaptor to connect PLM Base and Geometric Modelling. M, million; K, thousand.\nNature Machine Intelligence | Volume 5 | October 2023 | 1087–1096\n 1089\nArticle https://doi.org/10.1038/s42256-023-00721-6\nstructure prediction. As exhibited in Fig. 1, HelixFold-Single consists \nof three components: PLM Base, Adaptor and Geometric Modelling. \nThe large-scale PLM Base is employed to encode the co-evolution \ninformation in the parameters, which is used as an alternative to \nMSAs. Then, in Geometric Modelling, following AlphaFold2, we use \nmodified Evoformer (named EvoformerS) and Structure modules to \nsufficiently exchange the information between the single representa-\ntions and pair representations to capture the geometric information \nand recover the 3D coordinates of the atoms. We adopt an Adaptor \nlayer to extract the co-evolution information from PLM to effectively \ngenerate the sequence and pair representations required as inputs \nto Geometric Modelling. The whole differentiable pipeline is trained \nby both self-supervised pre-training with bulks of unlabelled single \nsequences and supervised learning with geometric labels.\nResults\nOverall comparison\nT o compare the overall accuracy of HelixFold-Single with several base-\nline structure prediction pipelines, including MSA-based and MSA-free \nmethods, we used CASP14 (refs. 1,14,15) with 87 domain targets and \nCAMEO16 with 371 targets collected from 4 September 2021 to 19 Feb-\nruary 2022. AlphaFold2 (ref. 1) and RoseTTAFold17, which rely on MSAs \nto provide predictions, are currently the most advanced methods for \nprotein structure prediction. We evaluated the prediction perfor -\nmance of AlphaFold2 and RossTTAFold with and without homologous \nsequences (denoted by AlphaFold2 (input: MSA), RoseTTAFold (input: \nMSA), AlphaFold2 (input: single) and RoseTTAFold (input: single)). We \nalso trained an MSA-free version of AlphaFold2, denoted by Alpha-\nFold2-Single, by only using the single sequences as input. T o evaluate \nthe accuracy of HelixFold-Single and other methods, we utilized a com-\nmonly used metric, that is, the template modelling score (TM-score)18.\nFigure 2 exhibits the test results of our proposed HelixFold-Single \nand the compared methods on CASP14 and CAMEO. On the basis of the \nresults, we make the following observations.\n (1) In general, HelixFold-Single significantly surpasses all the \nMSA-free methods on CASP14 and CAMEO and is competitive \nwith the MSA-based methods in certain scenarios. Notably, the \naccuracy of HelixFold-Single on CAMEO is comparable to that \nof AlphaFold2 (input: MSA) and outshines another baseline, \nRoseTTAFold (input: MSA). HelixFold-Single demonstrates the \ngreat potential of incorporating PLM into geometric modelling \nfor protein structure prediction.\n (2) HelixFold-Single can be on a par with the MSA-based methods on \ntargets with large homologous families, for example, on CASP14 \ntemplate-based modelling (TBM)-easy domain targets with a \nmedian of 7,000 homologous sequences (MSA depth = 7,000) \nand on CAMEO targets with more than 1,000 homologous se-\nquences (MSA depth > 1,000). These results indicate that the \naccuracy of HelixFold-Single is correlated to the richness of \nhomologous sequences, revealing that the large-scale PLM \nadopted by HelixFold-Single is capable of embedding the infor-\nmation, for example, co-evolution knowledge, of MSAs used by \nthe MSA-based methods.\n (3) Comparing HelixFold-Single with other MSA-free methods, \nHelixFold-Single exhibits its great superiority in all the catego -\nries of CASP14 and CAMEO. Since AlphaFold2 and RoseTTAFold \nrely on MSAs as input during the training process, it is challeng -\ning for these methods to provide accurate predictions when tak-\ning only single sequences as input. Even for AlphaFold2-Single, \nwhich uses only single protein sequences as input for training, \nits precision is unsatisfactory without the assistance of the PLM.\nEffect of number of homologous sequences\nThe results on CASP14 and CAMEO indicate that the accuracy of \nHelixFold-Single is related to the number of homologous sequences. We \nfurther compare the performance of HelixFold-Single and other meth-\nods on the targets with variant MSA depths. We have collected a fresh \ntest dataset, MSA-Depth-T est, comprising targets that were released \nbetween May 2020 and October 2021 from the Research Collaboratory \nfor Structural Bioinformatics Protein Data Bank (PDB). Specifically, we \nselected targets that exhibit relatively sparse homologous sequences. \nWe blended these targets with the data of CASP14 and CAMEO as a new \nevaluation set. Figure 3a compares the TM-scores of HelixFold-Single \nand the baseline methods on the evaluation set, grouped by the number \nof homologous sequences (MSA depths). Figure 3b shows the distri-\nbution of the proteins in different groups in this evaluation set. We \ncan see that as the available homologous sequences grow the aver-\nage TM-scores of both HelixFold-Single and the MSA-based methods \nincrease, while the scores of the other MSA-free methods decrease. \nFor the proteins with sparse homologous sequences, the TM-scores \nof all the compared methods are unsatisfactory. For the proteins with \nFM FM/TBM TBM-hard TBM-easy All\nTM-score\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlphaFold2 (input: MSA) RoseTTAFold (input: MSA) AlphaFold2 (input: single)\nRoseTTAFold (input: single) AlphaFold2-Single HelixFold-Single\nDepth ≤ 100\n100 < depth ≤ 1,000\n1,000 < depth ≤ 10,000\nDepth > 10,000\nAll\nTM-score\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlphaFold2 (input: MSA) RoseTTAFold (input: MSA) AlphaFold2 (input: single)\nRoseTTAFold (input: single) AlphaFold2-Single HelixFold-Single\na\nb\nFig. 2 | Overall comparison of HelixFold-Single and other methods on CASP14 \nand CAMEO. a,b, AlphaFold2 (input: MSA) and RoseTTAFold (input: MSA) are \nMSA-based methods, while the others use the primary structures as input. Data \nare divided into quartiles, and a box is drawn between the first and third quartiles, \nwith an additional line drawn along the second quartile to mark the median \nand a cross to mark the mean. The whiskers extend from the edges of the box to \nrepresent the minimum and maximum values within a certain range, excluding \noutliers. This system is used for all box plots of this paper. a, CASP14 (87 targets \nclassified into free-modelling (FM) and TBM categories on the basis of their \nrelatedness to existing structures.) b, CAMEO (371 targets classified into four \ncategories depending on MSA depth).\nNature Machine Intelligence | Volume 5 | October 2023 | 1087–1096 1090\nArticle https://doi.org/10.1038/s42256-023-00721-6\nlarger homologous families, especially those with more than thousands, \nHelixFold-Single can compete with the MSA-based methods. In general, \nit appears that HelixFold-Single is more sensitive to the presence of evo-\nlutionary information when compared with MSA-based methods such \nas AlphaFold (input: MSA) or RoseTTAFold (input: MSA). Given that 90% \nof the targets in PDB have more than 1,024 homologous sequences, we \ncan reasonably extrapolate that HelixFold-Single can achieve satisfying \naccuracy on the most frequently investigated proteins.\n(1, 4) (4, 16) (16, 64) (64, 256) (256, 1,024) (1,024, 4,096) (4,096, 16,384) (16,384, +∞)\nMSA depth\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nTM-score\nAlphaFold2 (input: MSA) RoseTTAFold (input: MSA) AlphaFold2 (input: single)\nRoseTTAFold (input: single) AlphaFold2-Single HelixFold-Single\n0.54% 0.74% 1.66% 2.88% 4.76%\n21.66%\n66.84%\n0.92%\n0\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\n(1, 4) (4, 16) (16, 64) (64, 256)\n(256, 1,024) (1,024, 4,096) (4,096, 16,384)\n(16,384,+∞)\nProportion (%)\nMSA depth\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n1 10 100 1,000 10,000 100,000 1,000,000\nTM-score\nMSA depth\nCASP14 CAMEO MSA-Depth-Test Logarithmic trend\n0\n2\n4\n6\n8\n10\n12\n14\n16\n18\n20\n1 10 100 1,000 10,000 100,000 1,000,000\nPerplexity\nMSA depth\nCASP14 CAMEO MSA-Depth-Test Logarithmic trend\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\n0 5 10 15 20\nTM-score\nPerplexity\nCASP14 CAMEO MSA-Depth-Test Linear trend\na\nb c\nd e\nFig. 3 | Analysis of the impact of homologous sequences (MSA depths), \nand investigation of the relations between MSA depths, TM-scores and \nperplexity of the PLM. a, Comparison between HelixFold-Single and the \nbaseline methods on 1,251 protein targets with various numbers of homologous \nsequences (MSA depths). b, Distribution of proteins with different homologous \nsequences in PDB. c, Relations between MSA depths and TM-scores of HelixFold-\nSingle. d, Relations between MSA depths and perplexity of PLM. e, Relation \nbetween perplexity of PLM and TM-scores of HelixFold-Single.\nNature Machine Intelligence | Volume 5 | October 2023 | 1087–1096\n 1091\nArticle https://doi.org/10.1038/s42256-023-00721-6\nT o further investigate the relationship between the capacity of \nthe PLM, the accuracy of protein structure prediction and the size of \nthe homologous family, we utilized the targets in CASP14 and CAMEO \ndatasets to exhibit their relations, as shown in Fig. 3c–e. As we expected, \nfrom Fig. 3c, a protein’s structure accuracy (TM-score) is correlated \nto the size of its homologous family (MSA depth), and the results are \nconsistent with those in Fig. 3b. Moreover, we use a probability metric, \nperplexity19, to indicate the capacity of the PLM. Perplexity is a metric \nwidely used in natural language processing to quantify the level of \nuncertainty a language model has in predicting text (which corre -\nsponds to the protein sequences in PLM). A lower perplexity score indi-\ncates a higher degree of accuracy for the language model. The results \nin Fig. 3d show that the perplexity of the PLM and the MSA depths \nare negatively correlated. We reasonably inferred that a PLM would \nprioritize learning the patterns of high-frequency proteins (which \ntypically have more homologous sequences) rather than long-tail \nproteins (which usually only have a few homologous sequences) from \nthe large-scale unlabelled protein sequences. These results also explain \nwhy the PLM-based HelixFold-Single is more sensitive to MSA depth \nwhen predicting protein structures. Moreover, the perplexity of the \nPLM and the TM-scores of HelixFold-Single are also negatively corre-\nlated. These results indicate that if the PLM Base module can predict \n(model) a protein sequence well, then there is a high probability that \nthe PLM module can learn the co-evolution information of this protein \nand serves as an alternative to MSAs. Thus, the Geometric Modelling \nmodule can leverage the co-evolution embedded in the PLM to provide \na more accurate structure for that protein.\nEffect of sizes of PLMs\nT o comprehensively study the ability of the PLMs of different sizes to \nlearn the co-evolution information, we compare a pre-trained PLM of \none billion parameters (denoted by PLM-1B) and another pre-trained \nPLM of 100 million (denoted by PLM-100M). Figure 4a exhibits the \nperplexity of PLM-1B and PLM-100M on the targets from datasets \nCASP14 and CAMEO. In general, the smaller the perplexity is, the \nstronger the capacity of the PLM is. Thus, PLM-1B with more model \nparameters performs better than PLM-100M with fewer parameters \non both datasets CASP14 and CAMEO. In addition, we apply PLM-1B \nand PLM-100M to the task of protein residue contact prediction to \ncompare their performance on the downstream tasks. We simply fit a \nlogistic regression that takes the attention weights, that is, \n[zzz(1),zzz(2),…, zzz(nPLM)], from the PLMs as input and predict the contact of \nresidues on the targets in datasets CASP14 and CAMEO. Following  \nrefs. 7,20, we use the top L/5 long-range contact precision, denoted by \nP@L/5, where L is the protein length, as the evaluation metric, and the \nresults are shown in Fig. 4b . As we can see, PLM-1B is significantly \n0\n5\n10\n15\n20\n25\n30\n35\n40\n0 5 10 15 20 25 30 35 40\nPLM-100 M\nPLM-100 MPLM-1B\nPLM-1B\nCASP14 CAMEO\nCASP14 CAMEO\n0\n5\n10\n15\n20\n25\n30\nPerplexity\nCAMEO CASP14\n0\n0.2\n0.4\n0.6\n0.8\n1.0\n0 0.2 0.4 0.6 0.8 1.0\nPLM-100 M\nPLM-1B\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nP@L/5\nCAMEO CASP14\nPLM-100 MPLM-1B\na\nb\nFig. 4 | Comparison of PLMs of different sizes on CAMEO (371 targets) and CASP14 (87 targets). a, Perplexity of PLM-1B and PLM-100M. b, Contact prediction of \nPLM-1B and PLM-100M.\nNature Machine Intelligence | Volume 5 | October 2023 | 1087–1096 1092\nArticle https://doi.org/10.1038/s42256-023-00721-6\nsuperior to PLM-100M on the contact prediction task. The results from \nFig. 4a and Fig. 4b both support the hypothesis that the larger the size \nof the PLM, the stronger its capacity. Therefore, it can be reasonably \ninferred that the performance of the PLM will continue to improve as \nthe size of the PLM increases further.\nPrediction speed comparison\nMassive time consumption for searching MSAs is one of the bottlenecks \nof MSA-based folding, and accelerating the speed of protein structure \nprediction can considerably broader its applications. The MSA-free \nHelixFold-Single has a tremendous advantage in inference efficiency \nby avoiding MSA searching. Figure 5 exhibits the computation time cost \nof (1) MSA searching, (2) the whole inference pipeline of AlphaFold2 \nand (3) the inference of HelixFold-Single. All the tests are executed \non a single NVIDIA A100(40G) graphics processing unit. In general, \nHelixFold-Single consumes much less time than AlphaFold2, while \nthe AlphaFold2 pipeline spends most of its time in MSA searching. For \nproteins less than 100 amino acids in length, HelixFold-Single’s predic-\ntion time is only about one-thousandth of that of AlphaFold2. Even for \nthe proteins with more than 800 amino acids, HelixFold-Single still has \ngreat efficiency superiority. The good efficiency of HelixFold-Single \ndemonstrates the potential of its application in tasks with a high \ndemand for structural prediction.\nStudy on multiple types of representative protein\nOne of the strengths of HelixFold-Single is its efficiency when com-\npared with MSA-based methods, which makes it well suited for \nhigh-throughput protein structure prediction tasks such as protein \ndesign. T o investigate the performance of HelixFold-Single on thera-\npeutic proteins, three representative types of protein were chosen: \npeptides, antibodies and nanobodies. Peptides are smaller protein \nmolecules that can be used as drugs to target a variety of biological \nprocesses, while antibodies and nanobodies are used in immunother-\napy to target specific cells or molecules in the body. An antibody con-\ntains two chains, a heavy chain and a light chain, and a nanobody only \nincludes the heavy chain. We evaluate the MSA-free HelixFold-Single \nand MSA-based AlphaFold2 on multiple datasets—Recent-PDB, Pep-\ntide, Antibody and Nanobody—to gain insights into the applicability of \nthese methods to different types of protein and their potential use in \nprotein design. Recent-PDB can be seen as the control group contain-\ning recently released proteins from PDB, while the remaining datasets \nrepresent experimental groups that are more relevant to therapeutic \napplications. Antibody-VH and Antibody-VL respectively represent the \nsets of heavy chains and light chains of collected antibodies.\nThe results presented in Fig. 6a are intriguing, as they demonstrate \nthat HelixFold-Single can perform as well as, or even outperform, \nAlphaFold2 in certain scenarios. While HelixFold-Single’s performance \nslightly lags behind that of AlphaFold2 on the Peptide dataset, the \nprecision gap between the two methods is considerably narrower than \nthat on the Recent-PDB dataset. This indicates that HelixFold-Single \nis better suited for predicting the structures of short and highly flex-\nible peptides. For the antibody-related datasets, HelixFold-Single \nperforms competitively with AlphaFold2 on datasets Antibody-VL and \nNanobody, and surpasses AlphaFold2 on Antibody-VH. We surmise that \nHelixFold-Single is better equipped to capture the intricate patterns of \nthe complementarity-determining regions (CDRs) from the large-scale \nprotein sequence data, where the CDRs of antibodies are crucial for \nthe specificity of an antibody and are known to be highly variable and \ndifficult to predict. Therefore, we conducted a detailed analysis of \nHelixFold-Single’s performance on the CDRs, as illustrated in Fig. 6b,c. \nHelixFold-Single performs comparably to AlphaFold2 in terms of the \nwhole chains (VH, VL and VHH) and all the CDRs, with a slight advantage \nin predicting the CDR-H3 (widely recognized as the most diverse and \ncritical CDRs) of the antibodies and nanobodies. Given the high vari-\nability of short peptides and the CDRs of antibodies, it is reasonable \nto assume that HelixFold-Single excels in predicting highly variable \nregions where MSAs may not be effective. T o support this hypothesis, \nwe performed additional analyses on the secondary structures of \npeptides and antibodies. Our results showed that HelixFold-Single \nis capable of accurately predicting the regions with the more flexible \nsecondary structures of ‘turn’ or ‘coil’ . For more information, please \nrefer to Supplementary Section 5.\nRelated works\nProtein language models\nLarge-scale language models4 with the self-supervised learning para-\ndigm, such as masked language modelling5 and autoregression21, have \nachieved extraordinary success in natural language processing tasks. \nRecent progress has revealed that their capabilities are strongly related \nto the scale of the model parameters: the larger the scale of the param-\neters, the better the performance6. The community has not yet seen any \nsign of growth stopping on moving from billions to hundreds of billions \nof parameters. These language models are capable of memorizing \nand generalizing massive common-sense knowledge and professional \nexpertise implicitly included in the large-scale unlabelled data. Inspired \nby these achievements, PLMs tried to transfer language models and \nself-supervised learning tasks to protein modelling. A protein can be \nrepresented by an amino-acid sequence, similar to the sequences of \nwords or tokens in natural language processing. Previous works7–10 \nhave shown that, by pre-training with only single sequences without \nmuch supervision, PLMs can reveal the protein classification, stabil-\nity and lower-level structure information (including secondary and \ntertiary structures and two-dimensional contact maps). However, the \naccuracy of these models in structure prediction is still far from that \nof the mainstream folding models supervised by the ground-truth \nprotein structure.\nProtein structure prediction\nMainstream pipelines 22–25 rely on extracting the co-evolution infor -\nmation from MSAs to predict the protein structures. Earlier works \nmanually designed the features derived from MSAs, such as inverse \ncovariance matrices. Then, deep neural networks—for example, con-\nvolutional networks—are utilized to model the relations between the \nresidues. Advanced studies 1,24, directly take the MSAs as input and \napply deep neural networks to predict the 3D coordinates of the pro-\nteins. In particular, the appearance of AlphaFold2 (ref. 1) has markedly \nnarrowed the accuracy gap between the experimentally determined \nstructures and model-estimated structures, employing the Evofor -\nmer module to enhance the interaction between MSA sequences and \npairwise geometric information and the Structure module to directly \n1\n10\n100\n1,000\n(1, 100) (100, 200) (200, 400) (400, 800) (800, +∞)\nMedian time (s)\nProtein length (amino acids)\nSearch MSA AlphaFold2 HelixFold-Single\nFig. 5 | Comparison of median times of MSA search, AlphaFold2 and \nHelixFold-Single speeds. We compare the median times of MSA search, \nAlphaFold2 and HelixFold-Single on proteins with various lengths.\nNature Machine Intelligence | Volume 5 | October 2023 | 1087–1096\n 1093\nArticle https://doi.org/10.1038/s42256-023-00721-6\npredict the atoms’ coordinates. However, the reliance on MSA inevi -\ntably impedes the computation efficiency and accurate prediction of \norphan proteins and designed proteins, as well as downstream tasks \nsuch as protein design.\nAlthough the structure of a protein is dependent on its primary \nstructure, it is incredibly challenging to train an accurate model that \ncan infer the protein structures with only the primary structures. \nOnly a small number of samples, that is, experimentally determined \nstructures recorded in the PDB database, are available for model train-\ning. Several works attempt to incorporate PLMs for MSA-free protein \nstructure prediction. RGN2 (ref. 11) employs a PLM (AminoBERT) with \na recurrent geometric network that utilizes Frenet–Serret frames to \ngenerate the backbone structure. Moreover, advanced studies 12,13 \ncombine pre-trained PLMs, such as ProT5 (ref. 8) and ESM-1b (ref. 26), \nwith residual neural networks to predict two-dimensional structures \n(for example, a contact map of a protein), yielding superior perfor -\nmance in orphan proteins. Nonetheless, the overall accuracy of those \nworks is still unsatisfactory due to the limited capacity of the model \narchitectures used.\nConclusion and future work\nOn the one hand, mainstream protein structure prediction methods, \nsuch as AlphaFold2 and RoseTTAFold, rely on the MSAs to extract \nthe homologous information. However, searching MSAs is time \nconsuming, limiting the application of those methods to broader \nprotein-related tasks. On the other hand, a large-scale PLM learns \nthe protein correlations from a great number of unlabelled proteins \nthrough self-supervised learning tasks. By utilizing large-scale param-\neters to embed the homologous information, we prove that it can be \nused as an alternative to MSAs to reduce the time required by the pro-\ntein structure prediction methods. HelixFold-Single attempts to take \nadvantage of both the PLM and the geometric modelling, predicting \nthe protein structures end to end with only the primary structures. \nHelixFold-Single can be on a par with the MSA-based methods on tar-\ngets with large homologous families and is much more efficient than \nthe MSA-based methods, demonstrating its application prospects for \nprotein study.\nIn the future, as the experimental results indicate that a larger size \nof PLM can achieve superior performance, we will continue investigat-\ning PLMs with a larger size for protein structure prediction. In addition, \nthe accuracy on the targets with only a few homologous sequences is \nstill unsatisfactory. Thus we will try to introduce more diverse training \ndata to alleviate this problem.\nMethods\nLarge-scale PLM Base\nInspired by large-scale pre-trained language models, we follow previous \nworks on pre-training a PLM. The PLM processes the primary protein \nsequences (that is, the amino-acid sequences) and extracts the knowl-\nedge needed for further geometric modelling. A protein of length L can \nbe uniquely represented by a sequence of types of amino acid denoted \nby x = (x1, x2, …, xL). An embedding layer E(xl) maps the type identifier \nto dPLM-dimensional embedding vectors:\nx(0) =( E(x1),E(x2),…, E(xL)).\nNotice that x(k) ∈ℝ L×dPLM is the representation of the amino-acid \nsequence.\nWe then apply the widely used Transformer-style blocks4 to pro-\ncess the embedding vectors, denoted by\nx(k+1) = DisentangledAttentionTransformer(x(k)). (1)\nAccurately predicting the contacts between the residues, especially the \nlong-rage contacts, is critical for protein structure prediction. Taking \ninto account that the contact between the residues is more dependent \non the relative positions rather than the absolute positions (counted \nfrom the start of the sequence), we employ DisentangledAttention -\nTransformer from DeBerTa27 to focus on the modelling of interactions \nbetween the residue representations and the relative positions. Dis -\nentangledAttentionTransformer adopts the attention mechanism to \nlearn the interactions between the residues as well as the interactions \nof the interaction–position pairs.\nRecent-PDB Peptide Antibody-VH Antibody-VL Nanobody\nTM-score\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\n0.9\n1.0\nAlphaFold2 HelixFold-Single\nCDR-H1 CDR-H2 CDR-H3 CDR-L1 CDR-L2 CDR-L3 VH VL\nRMSD\n0\n2\n4\n6\n8\n10\n12\n14\nAlphaFold2 HelixFold-Single\nCDR-H1 CDR-H2 CDR-H3 VHH\nRMSD\n0\n2\n4\n6\n8\n10\n12\n14\nAlphaFold2 HelixFold-Single\naa\nb\nc\nFig. 6 | Comparison between AlphaFold2 and HelixFold-Single on the \nrepresentative types of protein. a–c, Recent-PDB (7,595 targets) is the control \ngroup. Peptide (197 targets), Antibody (90 targets) and Nanobody (184 targets) \nare the sets of representative proteins. Note that a typical antibody has six  \nCDRs, while a nanobody has three CDRs. a, Overall comparison. b, Antibody.  \nc, Nanobody. RMSD, root-mean-square deviation.\nNature Machine Intelligence | Volume 5 | October 2023 | 1087–1096 1094\nArticle https://doi.org/10.1038/s42256-023-00721-6\nMoreover, we take advantage of multihead self-attention weights \nin DisentangledAttentionTransformer to construct the initial pair \nrepresentation. The attention weights of the kth block are denoted by \nz(k) ∈ℝ L×L×hPLM, where hPLM is the number of heads of self-attention.\nWe add an additional Adaptor to map the output of PLM Base to \nthe input of the Geometric Modelling module.\ñx(0) = Linear(x(nPLM)),\ñz(0) = Linear([z(1),z(2),…, z(nPLM)]),\n(2)\nwhere nPLM is the number of blocks in PLM Base, and the operator [] \nrefers to concatenation. ̃x(0) ∈ℝ L×dsingle and ̃z(0) ∈ℝ L×L×dpair are the initial \nsingle representations and pair representations of the Geometric \nModelling module, respectively.\nGeometric modelling\nWe employ the Evoformer and Structure modules proposed in Alpha-\nFold2 (ref. 1) to model the relations between the residues and then esti-\nmate the 3D coordinates of the atoms in the proteins. We slightly modify \nthe original Evoformer to match our settings. We name the revised Evo-\nformer EvoformerS (Evoformer with single representations). First, the \noriginal Evoformer takes the MSA representation and pair representa-\ntion, encoded from the searched MSAs, as input. As an alternative, Evo-\nformerS takes the output of Adaptor (including the single representations \n( ̃x(0)) and pair representations (̃z(0))). Second, Evoformer adopts various \nattention mechanisms to exchange the information within the single \nand pair representations to learn the spatial relationships. Note that, in \ncontrast to the original version of Evoformer proposed by AlphaFold2, \nwe remove the column-wise gated self-attention because HelixFold-Single \nfocuses on MSA-free protein structure prediction and there is no need \nto exchange the messages within the MSAs. We follow the other geomet-\nric components of AlphaFold2, including the Structure module, which \ntakes the single representation and pair representation yielded by Evo-\nformerS and exploits invariant point attention and other geometric \ntransformation operators to predict end to end the 3D coordinates of \nthe atoms. Also, following AlphaFold2, we recycle the whole Geometric \nModelling module to refine the predicted structures iteratively.\nModel optimization\nFor the sake of leveraging the domain knowledge from the pro -\ntein database, we operate two-stage parameter optimization on \nHelixFold-Single.\nIn the first stage, the PLM is pre-trained to capture the co-evolution \ninformation. The PLM is trained with about 300 million single \nsequences recorded in a protein database. T o encourage PLM to \nobserve the diverse single sequences as soon as possible, we cluster \nthe proteins by similarity of single sequences and sample the proteins \nto balance the distributions of different clusters in our training data. We \napply a self-supervised technique masked language model to optimize \nthe parameters of the PLM, by randomly masking 15% of residues in the \nsingle sequences and then reconstructing these masked residues. More \nconcretely, the masked language model attempts to predict P(xl∣x1, …,  \nxl−1, xM, xl+1, …, xL) given the residue in the lth position xl being masked by \nxM. A crucial proposal of this work is that the PLM can learn the depend-\nence between the masked residue and the other residues, and thus \nrepresent the co-evolution information. Previous works7 have already \nverified that PLMs can reveal secondary structures of the proteins, but \nthe relation between PLM and co-evolution has been little discussed. \nCo-evolution is the phenomenon that two residues in contact tend to \nevolve at the same time to preserve the structure and thus the function \nof the protein. In PLM, if a residue at another position s has a profound \nimpact (if the residue at position s is changed, the masked residue will \nalso change) on the masked residue, then these two residues are likely \nto evolve at the same time.\nIn the second stage, since merely relying on PLM to predict the \nstructure is inadequate to capture the geometric information, PLM \nBase and Geometric Modelling modules in HelixFold-Single are jointly \noptimized. We utilize 100,000 experimentally determined protein \nstructures. We also use an additional one million estimated protein \nstructures for training in this stage (distilled from AlphaFold2). Fol -\nlowing AlphaFold2, we train the network end to end with the main \nlosses, including the frame aligned point error loss and other auxiliary \nlosses. By combining the computationally efficient PLM Base module \n(compared with MSA search) and the Geometric Modelling module, \nHelixFold-Single is capable of providing efficient and precise protein \nstructure prediction.\nDatasets\nWe used UniRef30 (2021-03) (ref. 28), which clusters UniRef100 seed \nsequences from the UniProt Knowledgebase and selected UniProt \nArchive records 29,30 at a 30% pairwise sequence identity level, to \npre-train the PLM. Then, three datasets are used to train the whole \nnetwork, including the proteins in PDB (refs. 31,32) released before 14 \nMay 2020 and two datasets constructed from Uniclust30 (v.2018-08) \nand AlphaFold Protein Structure Database (v.2022-01) (ref. 33 ), for \nknowledge distillation.\nReporting summary\nFurther information on research design is available in the Nature Port-\nfolio Reporting Summary linked to this article.\nData availability\nT o pre-train the PLM, UniRef30 (2021-03) is publicly available at https://\nwwwuser.gwdg.de/~compbiol/uniclust/2021_03/; to train the whole \nnetwork, PDB can be downloaded at https://www.rcsb.org/docs/\nprogrammatic-access/file-download-services and AlphaFold Protein \nStructure Database as the distillation dataset can be downloaded at \nhttps://ftp.ebi.ac.uk/pub/databases/alphafold/v2/. The CAMEO dataset \ncan be downloaded at https://www.cameo3d.org/modeling/ with dates \nbetween 4 September 2021 and 19 February 2022. The CASP14 and \nCASP15 dataset can be partially downloaded at https://predictioncenter.\norg/download_area/. The MSA-Depth-T est, Recent-PDB and Peptide \nsets are filtered from PDB with conditions detailed in Supplementary \nInformation. The Antibody and Nanobody sets can be downloaded at \nhttps://opig.stats.ox.ac.uk/webapps/sabdab-sabpred/sabdab/.\nCode availability\nThe source code, trained weights and inference code of HelixFold-Single \nare freely available at GitHub (https://github.com/PaddlePaddle/Pad-\ndleHelix/tree/dev/apps/protein_folding/helixfold-single) to ensure the \nreproduction of our experimental results. The version used for this pub\n-\nlication is available at ref. 34. A web service of HelixFold-Single is also \navailable at https://paddlehelix.baidu.com/app/drug/protein-single/\nforecast to provide efficient protein structure predictions.\nData analysis used Python v.3.7, NumPy v.1.16.4 and MMseqs2 \nrelease 13-45111. TMscore.cpp v20220227 (https://zhanggroup.org/\nTM-score/) was used for computing TM-scores.\nReferences\n1. Jumper, J. et al. Highly accurate protein structure prediction with \nAlphaFold. Nature 596, 583–589 (2021).\n2. Moult, J. A decade of CASP: progress, bottlenecks and prognosis \nin protein structure prediction. Curr. Opin. Struct. Biol. 15, \n285–289 (2005).\n3. Petroni, F. et al. Language models as knowledge bases? In Proc. \n2019 Conference on Empirical Methods in Natural Language \nProcessing and the 9th International Joint Conference on Natural \nLanguage Processing (EMNLP-IJCNLP) https://doi.org/10.18653/v1/\nD19-1250 (ACL, 2019).\nNature Machine Intelligence | Volume 5 | October 2023 | 1087–1096\n 1095\nArticle https://doi.org/10.1038/s42256-023-00721-6\n4. Vaswani, A. et al. Attention is all you need. In NIPS'17: Proc. \n31st International Conference on Neural Information Processing \nSystems Vol. 30 (eds von Luxburg, U. et al.) 6000–6010 (Curran, \n2017).\n5. Devlin, J., Chang, M.-W., Lee, K. & Toutanova, K. BERT: pre-training \nof deep bidirectional transformers for language understanding. \nIn Proc. 2019 Conference of the North American Chapter of the \nAssociation for Computational Linguistics: Human Language \nTechnologies, Volume 1 (Long and Short Papers) (eds Burstein, \nJ. et al.) 4171–4186 (Association for Computational Linguistics, \n2019).\n6. Brown, T. et al. Language models are few-shot learners. Adv. \nNeural Inf. Process. Syst. 33, 1877–1901 (2020).\n7. Rao, R. et al. Evaluating protein transfer learning with TAPE. In \nNIPS'19: Proc. 33rd International Conference on Neural Information \nProcessing Systems Vol. 32 (eds Wallach, H. M. et al.) 9689–9701 \n(2019).\n8. Elnaggar, A. et al. ProtTrans: towards cracking the language \nof life’s code through self-supervised deep learning and \nhigh performance computing. Preprint at arXiv https://doi.\norg/10.48550/arXiv.2007.06225 (2021).\n9. Rao, R., Meier, J., Sercu, T., Ovchinnikov, S. & Rives, A. Transformer \nprotein language models are unsupervised structure learners. In \n9th International Conference on Learning Representations (ICLR, \n2021).\n10. Xiao, Y., Qiu, J., Li, Z., Hsieh, C.-Y. & Tang, J. Modeling protein using \nlarge-scale pretrain language model. Preprint at arXiv https://doi.\norg/10.48550/arXiv.2108.07435 (2021).\n11. Chowdhury, R. et al. Single-sequence protein structure prediction \nusing language models from deep learning. Preprint at bioRxiv \nhttps://doi.org/10.1101/2021.08.02.454840 (2021).\n12. Weißenow, K., Heinzinger, M. & Rost, B. Protein language-model \nembeddings for fast, accurate, and alignment-free protein \nstructure prediction. Structure 30, 1169–1177.E4 (2022).\n13. Wang, W., Peng, Z. & Yang, J. Single-sequence protein structure \nprediction using supervised transformer protein language \nmodels. Nat. Comput. Sci. 2, 804–814 (2022).\n14. Kinch, L. N., Schaeffer, R. D., Kryshtafovych, A. & Grishin, N. V. \nTarget classification in the 14th round of the critical assessment \nof protein structure prediction (CASP14). Proteins 89, 1618–1632 \n(2021).\n15. Kryshtafovych, A., Schwede, T., Topf, M., Fidelis, K. & Moult, J. \nCritical assessment of methods of protein structure prediction \n(CASP)–Round XIV. Proteins 89, 1607–1617 (2021).\n16. Robin, X. et al. Continuous Automated Model EvaluatiOn \n(CAMEO)—perspectives on the future of fully automated \nevaluation of structure prediction methods. Proteins 89, 1977–\n1986 (2021).\n17. Baek, M. et al. Accurate prediction of protein structures and \ninteractions using a three-track neural network. Science 373, \n871–876 (2021).\n18. Zhang, Y. & Skolnick, J. Scoring function for automated \nassessment of protein structure template quality. Proteins 57, \n702–710 (2004).\n19. Brown, P. F., Della Pietra, S. A., Della Pietra, V. J., Lai, J. C. & Mercer, \nR. L. An estimate of an upper bound for the entropy of English. \nComput. Linguist. 18, 31–40 (1992).\n20. Rao, R. M. et al. MSA Transformer. Proc. Mach. Learning Res. 139, \n8844–8856 (2021).\n21. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving \nlanguage understanding by generative pre-training OpenAI \n(2018); https://openai.com/research/language-unsupervised \n22. Yang, J. et al. Improved protein structure prediction using \npredicted interresidue orientations. Proc. Natl Acad. Sci. USA 117, \n1496–1503 (2020).\n23. Yang, J. et al. The I-TASSER Suite: protein structure and function \nprediction. Nat. Methods 12, 7–8 (2015).\n24. Du, Z. et al. The trRosetta server for fast and accurate protein \nstructure prediction. Nat. Protoc. 16, 5634–5651 (2021).\n25. Peng, J. & Xu, J. RaptorX: exploiting structure information for \nprotein alignment by statistical inference. Proteins 79, 161–171 \n(2011).\n26. Rives, A. et al. Biological structure and function emerge from \nscaling unsupervised learning to 250 million protein sequences. \nProc. Natl Acad. Sci. USA 118, e2016239118 (2021).\n27. He, P., Liu, X., Gao, J. & Chen, W. DeBERTa: decoding-enhanced \nBERT with disentangled attention. In 9th International Conference \non Learning Representations (ICLR, 2021).\n28. Mirdita, M. et al. Uniclust databases of clustered and deeply \nannotated protein sequences and alignments. Nucleic Acids Res. \n45, D170–D176 (2017).\n29. Suzek, B. E. et al. UniRef clusters: a comprehensive and \nscalable alternative for improving sequence similarity searches. \nBioinformatics 31, 926–932 (2014).\n30. The UniProt Consortium. UniProt: the Universal Protein \nKnowledgebase in 2023. Nucleic Acids Res. 51, D523–D531 \n(2023).\n31. Berman, H. M. et al. The Protein Data Bank. Nucleic Acids Res. 28, \n235–242 (2000).\n32. Burley, S. K. et al. RCSB Protein Data Bank: powerful new tools  \nfor exploring 3D structures of biological macromolecules for \nbasic and applied research and education in fundamental \nbiology, biomedicine, biotechnology, bioengineering and  \nenergy sciences. Nucleic Acids Res. 49, D437–D451  \n(2020).\n33. Varadi, M. et al. AlphaFold Protein Structure Database: massively \nexpanding the structural coverage of protein-sequence space \nwith high-accuracy models. Nucleic Acids Res. 50, D439–D444 \n(2021).\n34. xiaoyao4573 et al. Paddlepaddle/paddlehelix: v1.2.2. Zenodo \nhttps://doi.org/10.5281/zenodo.8202943 (2023).\nAcknowledgement\nThis work is supported by the National Engineering Research Center of \nDeep Learning Technology and Applications.\nAuthor contributions\nX.F., F.W., J.H., X.Z., H.W. and L.S. led the research. L.L., X.F. and F.W. \ncontributed technical ideas. L.L. and D.L. developed the proposed \nmethod. Y.X., K.Z. and H.L. developed analytics. X.F., F.W., L.L. and Y.X. \nwrote the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary information The online version  \ncontains supplementary material available at  \nhttps://doi.org/10.1038/s42256-023-00721-6.\nCorrespondence and requests for materials should be addressed to \nFan Wang or Le Song.\nPeer review information Nature Machine Intelligence thanks Alexander \nPritzel and the other, anonymous, reviewer(s) for their contribution to \nthe peer review of this work. Primary Handling Editor: Jacob Huth, in \ncollaboration with the Nature Machine Intelligence team.\nReprints and permissions information is available at  \nwww.nature.com/reprints.\nNature Machine Intelligence | Volume 5 | October 2023 | 1087–1096 1096\nArticle https://doi.org/10.1038/s42256-023-00721-6\nPublisher’s note Springer Nature remains neutral with regard to \njurisdictional claims in published maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons \nAttribution 4.0 International License, which permits use, sharing, \nadaptation, distribution and reproduction in any medium or format, \nas long as you give appropriate credit to the original author(s) and the \nsource, provide a link to the Creative Commons license, and indicate \nif changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons license, unless \nindicated otherwise in a credit line to the material. If material is not \nincluded in the article’s Creative Commons license and your intended \nuse is not permitted by statutory regulation or exceeds the permitted \nuse, you will need to obtain permission directly from the copyright \nholder. To view a copy of this license, visit http://creativecommons.\norg/licenses/by/4.0/.\n© The Author(s) 2023\n\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7482700347900391
    },
    {
      "name": "Protein structure prediction",
      "score": 0.6198037266731262
    },
    {
      "name": "CASP",
      "score": 0.6176571846008301
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5255352854728699
    },
    {
      "name": "Protein structure",
      "score": 0.46168985962867737
    },
    {
      "name": "Protein superfamily",
      "score": 0.4613701105117798
    },
    {
      "name": "Threading (protein sequence)",
      "score": 0.454702228307724
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4442219138145447
    },
    {
      "name": "Machine learning",
      "score": 0.43187665939331055
    },
    {
      "name": "Protein sequencing",
      "score": 0.41778427362442017
    },
    {
      "name": "Data mining",
      "score": 0.3470965623855591
    },
    {
      "name": "Peptide sequence",
      "score": 0.1760844588279724
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Nuclear magnetic resonance",
      "score": 0.0
    },
    {
      "name": "Gene",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Biochemistry",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}