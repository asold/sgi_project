{
  "title": "Memory-Enhanced Transformer for Representation Learning on Temporal Heterogeneous Graphs",
  "url": "https://openalex.org/W4353071157",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2117860059",
      "name": "Longhai Li",
      "affiliations": [
        "Sichuan University",
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A2115159880",
      "name": "Lei Duan",
      "affiliations": [
        "Sichuan University",
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A2117030371",
      "name": "Junchen Wang",
      "affiliations": [
        "Sichuan University",
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A2123844104",
      "name": "Chengxin He",
      "affiliations": [
        "Sichuan University",
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A2111743953",
      "name": "Zihao Chen",
      "affiliations": [
        "Sichuan University",
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A2892362652",
      "name": "Guicai Xie",
      "affiliations": [
        "Chengdu University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2109710016",
      "name": "Song Deng",
      "affiliations": [
        "Nanjing University of Posts and Telecommunications"
      ]
    },
    {
      "id": "https://openalex.org/A4353073510",
      "name": "Zhaohang Luo",
      "affiliations": [
        null
      ]
    },
    {
      "id": "https://openalex.org/A2117860059",
      "name": "Longhai Li",
      "affiliations": [
        "Sichuan University",
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A2115159880",
      "name": "Lei Duan",
      "affiliations": [
        "Chengdu University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2117030371",
      "name": "Junchen Wang",
      "affiliations": [
        "Chengdu University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2123844104",
      "name": "Chengxin He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111743953",
      "name": "Zihao Chen",
      "affiliations": [
        "Chengdu University",
        "Sichuan University"
      ]
    },
    {
      "id": "https://openalex.org/A2892362652",
      "name": "Guicai Xie",
      "affiliations": [
        "Sichuan University",
        "Chengdu University"
      ]
    },
    {
      "id": "https://openalex.org/A2109710016",
      "name": "Song Deng",
      "affiliations": [
        "Chengdu University",
        "Nanjing University of Posts and Telecommunications",
        "Sichuan University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2604314403",
    "https://openalex.org/W3211726303",
    "https://openalex.org/W2602856279",
    "https://openalex.org/W4200068255",
    "https://openalex.org/W2743104969",
    "https://openalex.org/W2767774008",
    "https://openalex.org/W3012871709",
    "https://openalex.org/W2911286998",
    "https://openalex.org/W3175971420",
    "https://openalex.org/W3209128423",
    "https://openalex.org/W2998313947",
    "https://openalex.org/W2998116985",
    "https://openalex.org/W3134942372",
    "https://openalex.org/W3188081755",
    "https://openalex.org/W3200912640",
    "https://openalex.org/W2965683718",
    "https://openalex.org/W2962756421",
    "https://openalex.org/W2154851992",
    "https://openalex.org/W4223919034",
    "https://openalex.org/W4290877727",
    "https://openalex.org/W2250342289",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2157331557",
    "https://openalex.org/W3104097132"
  ],
  "abstract": "Abstract Temporal heterogeneous graphs can model lots of complex systems in the real world, such as social networks and e-commerce applications, which are naturally time-varying and heterogeneous. As most existing graph representation learning methods cannot efficiently handle both of these characteristics, we propose a Transformer-like representation learning model, named THAN, to learn low-dimensional node embeddings preserving the topological structure features, heterogeneous semantics, and dynamic patterns of temporal heterogeneous graphs, simultaneously. Specifically, THAN first samples heterogeneous neighbors with temporal constraints and projects node features into the same vector space, then encodes time information and aggregates the neighborhood influence in different weights via type-aware self-attention. To capture long-term dependencies and evolutionary patterns, we design an optional memory module for storing and evolving dynamic node representations. Experiments on three real-world datasets demonstrate that THAN outperforms the state-of-the-arts in terms of effectiveness with respect to the temporal link prediction task.",
  "full_text": "Vol:.(1234567890)\nData Science and Engineering (2023) 8:98â€“111\nhttps://doi.org/10.1007/s41019-023-00207-w\n1 3\nRESEARCH PAPER\nMemoryâ€‘Enhanced Transformer forÂ Representation Learning \nonÂ Temporal Heterogeneous Graphs\nLonghaiÂ Li1Â Â· LeiÂ Duan1,3Â Â· JunchenÂ Wang1Â Â· ChengxinÂ He1Â Â· ZihaoÂ Chen1Â Â· GuicaiÂ Xie1Â Â· SongÂ Deng2Â Â· ZhaohangÂ Luo4\nReceived: 4 December 2022 / Revised: 7 February 2023 / Accepted: 1 March 2023 / Published online: 22 March 2023 \nÂ© The Author(s) 2023\nAbstract\nTemporal heterogeneous graphs can model lots of complex systems in the real world, such as social networks and e-commerce \napplications, which are naturally time-varying and heterogeneous. As most existing graph representation learning methods \ncannot efficiently handle both of these characteristics, we propose a Transformer-like representation learning model, named \nTHAN, to learn low-dimensional node embeddings preserving the topological structure features, heterogeneous semantics, \nand dynamic patterns of temporal heterogeneous graphs, simultaneously. Specifically, THAN first samples heterogeneous \nneighbors with temporal constraints and projects node features into the same vector space, then encodes time information and \naggregates the neighborhood influence in different weights via type-aware self-attention. To capture long-term dependencies \nand evolutionary patterns, we design an optional memory module for storing and evolving dynamic node representations. \nExperiments on three real-world datasets demonstrate that THAN outperforms the state-of-the-arts in terms of effectiveness \nwith respect to the temporal link prediction task.\nKeywords Temporal heterogeneous graphsÂ Â· Graph neural networksÂ Â· Graph representation learningÂ Â· Transformer\n1 Introduction\nGraph representation learning, as an important task in \nmachine learning, has significant practical value in areas \nsuch as social networks and recommendation systems. Exist-\ning graph representation learning methods usually take \nstatic graphs as the input to obtain low-dimensional embed-\ndings by encoding local non-Euclidean structures and have \nachieved extensive excellent performance in downstream \ntasks such as link prediction [ 1â€“3], node classification [4 , \n5], and graph classification [6, 7].\nHowever, most graphs in the real world are naturally \nheterogeneous and dynamic, which cannot be accurately \nrepresented by static homogeneous graphs. Several studies \nincorporate heterogeneous data models into a unified graph \nmodel [8], promoting the research of graph data. Taking the \nexample of a user-item interaction network in e-commerce \nscenarios, illustrated in Fig.Â 1a, there are two types of nodes \n(i.e., user and item) and three types of interactions (i.e., \nfavorite, browse and buy). Additionally, each interaction is \nassociated with a continuous timestamp to indicate when it \noccurred. In this paper, we define such interaction sequences \n * Lei Duan \n leiduan@scu.edu.cn\n Longhai Li \n lilonghai@stu.scu.edu.cn\n Junchen Wang \n wangjunchen@stu.scu.edu.cn\n Chengxin He \n hechengxin@stu.scu.edu.cn\n Zihao Chen \n chenzihao@stu.scu.edu.cn\n Guicai Xie \n guicaixie@stu.scu.edu.cn\n Song Deng \n dengsong@njupt.edu.cn\n Zhaohang Luo \n luozhaohang@qq.com\n1 School ofÂ Computer Science, Sichuan University, Chengdu, \nChina\n2 Institute ofÂ Advanced Technology, Nanjing University \nofÂ Posts & Telecommunications, Nanjing, China\n3 Med-X Center forÂ Informatics, Sichuan University, Chengdu, \nChina\n4 Nuclear Power Institute ofÂ China, Chengdu, China\n99Memory-Enhanced Transformer forÂ Representation Learning onÂ Temporal Heterogeneous Graphs  \n1 3\nconnecting different types of nodes as temporal heterogene-\nous graph (THG). It is of great significance to learn THG \nrepresentations with dynamic and heterogeneous character-\nistics for modeling real-world complex systems.\nExample 1 The user-item interaction network in an e-com-\nmerce scenario is illustrated in Fig.Â 1a. Two snapshots of the \nnetwork are given for the dates of June 18, 2021 and Novem-\nber 11, 2021. There are two types of nodes (i.e., user and \nitem) and three types of interactions (i.e., favorite, browse \nand buy), where favorite corresponds to the blue line, browse \nto the green line, and buy to the orange line. Additionally, \neach interaction is associated with a continuous timestamp to \nindicate the time it occurred. We can see that usersâ€™ purchase \nintentions change dynamically over time.\nIn the case of the user-item interaction network shown \nin Fig.Â  1a, THG representation learning has the following \nchallenges compared to static homogeneous graph repre-\nsentation learning:\nâ€¢ (C1) How to model the heterogeneity? The nodes and \nedges in THG are of various types and have rich seman-\ntics, making it difficult to obtain sufficient heterogeneous \ninformation just by encoding local graph structure.\nâ€¢ (C2) How to model the continuous dynamics? The edges \nin the THG are time-informed and time-dependent, i.e., \neach event occurs with a timestamp and current event \nmay affect the occurrence of future events. For instance, \nthere might be causal relationships between the inter -\naction of searching for headphones on June 18 and the \ninteraction of purchasing headphones on November 11 \nby user A. Therefore, both efficient methods of convert-\ning temporal information into dynamic features and \ntemporal constraints are needed to avoid violating the \ntemporal causality between interactions.\nâ€¢ (C3) How to deal with new nodes? The dynamics of the \nTHG imply that new nodes will emerge in the future \n(e.g., users D and E are two new nodes that appeared \non November 11 compared to June 18). In other words, \nthese nodes are not present during training and many \npractical applications will require their embeddings to \nbe generated in a timely manner. Therefore, it is neces-\nsary to construct an inductive modeling approach that \ngeneralizes the optimized representation to the new \ntemporal subgraphs.\nAs for the heterogeneity, earlier methods [9 , 10] preserve \nheterogeneous information by designing semantic meta-\npaths to generate heterogeneous sequences, and recent \nstudies [2 , 11â€“13] aggregate information from heteroge-\nneous neighborhood by extending the message-passing \nprocess of graph neural networks (GNNs). Concerning \ndynamics, it is general to split temporal graphs into several \nstatic snapshots (i.e., discrete-time dynamic graph, DTDG \n[14]) and use RNNs or attention to capture the evolution-\nary patterns between snapshots [15â€“ 18]. Although these \nmethods can learn graph dynamics of the THG to some \nextent, the temporal information within the same snapshot \nis usually ignored, and the scale of snapshots needs to \nbe predetermined in advance. Recently, researchers have \nproposed continuous-time dynamic graph (CTDG [14]) \napproaches [19â€“24] to capture dynamics via passing infor -\nmation between different interactions, or using continuous-\ntime functions to generate temporal embedding. In regard \nto the new nodes, inductive graph representation learn-\ning methods [5 , 22, 23, 25] recognize structural features \nof node neighborhood by learning trainable aggregation \nfunctions, so that rapidly generate node embeddings in \nnew subgraphs. Plenty of studies have attempted to solve \nthe above challenges, nevertheless, few approaches can \naddress them at the same time.\nFig. 1  A toy example of the temporal heterogeneous graph from a \nuser-item interaction network. a User-item interaction network; b \nTemporal heterogeneous graph. Different colored lines represent dif-\nferent interactions, where the blue line denotes favorite (i.e., the heart \nicon), the green line denotes browse (i.e., the magnifier icon), and the \norange line denotes buy (i.e., the wallet icon)\n100 L.Â Li et al.\n1 3\nIn this paper, we propose a novel Temporal Heterogeneous \nGraph Attention Network (THAN), which is a continuous-time \nTHG representation learning method with Transformer-like \nattention architecture. To handle C1, we design a time-aware \nheterogeneous graph encoder to aggregate information from \ndifferent types of neighbors. To handle C2, THAN samples \ntemporally constrained neighbors and learns time-aware rep-\nresentation from historical heterogeneous events for a given \nnode at any time point. It also encodes time information with \na time encoder and incorporates them into the message propa-\ngation process. To handle C3, THAN can be thought of as a \nlocal aggregation operator based on neighbor sampling that \nrecognizes the structural properties of a nodeâ€™s neighborhood \nand does not introduce global priori information.\nTHAN generates dynamic embeddings of nodes from \ntheir most recent neighbors. However, long-term dependen-\ncies and evolutionary patterns are not considered. Moreover, \nhigh-order information can be captured by stacking multiple \nTHAN layers, but the cost is huge. To address these prob-\nlems, we design an optional memory module to store and \nevolve the node representations. This module dynamically \nupdates the node states as events occur and provides indirect \naccess to distant neighbors by adding node memories to the \nraw inputs of THAN. The main contributions of our work \nare summarized as follows:\nâ€¢ We propose an inductive continuous-time THG represen-\ntation learning method, which can capture both hetero-\ngeneous information and dynamic features.\nâ€¢ We introduce the dynamic transfer matrix and self-atten-\ntion mechanism to implement the information aggrega-\ntion of heterogeneous neighbors.\nâ€¢ We devise an optional memory module to enhance the \nrepresentational ability of THAN by storing and updating \nthe dynamic node states.\nâ€¢ We conduct experiments on three public datasets and the \nresults demonstrate the superior performance of THAN \nover state-of-the-art baselines on the task of temporal \nlink prediction.\nThe rest of the paper is organized as follows. We review \nrelated work in Sect.Â  2, and formulate the problem of tem-\nporal heterogeneous graph representation learning in Sect.Â 3. \nIn Sect.Â 4, we discuss the critical techniques of THAN. We \nreport a systematic empirical evaluation in Sect.Â 5, and con-\nclude the paper in Sect.Â 6.\n2  Related Work\nOur work is related to representation learning on static \ngraphs, temporal graphs (i.e., dynamic graphs), and self-\nattention mechanism on graphs.\nRepresentation learning on static graphs Graph repre-\nsentation learning produces low-dimensional embeddings \nby modeling the topology and node attribute information. \nEarly methods [9, 10, 26, 27] generate sequences of nodes \nby random walks among neighbors and then learn node co-\noccurrences to obtain representations. Luo etÂ al. [28] define \nripple distance over ripple vectors to optimize the walking \nprocedure. In order to integrate rich node attribute features \nwhile learning network structure information, the GNN-\nbased approaches [2 , 4, 5, 11â€“13, 25] update node embed-\ndings by aggregating neighborhood influence and propagat-\ning information across a multilayer network to capture the \nhigh-order patterns of the graph.\nFocus on dealing with the heterogeneity, metapath2vec \n[9] and HIN2Vec [10] preserve heterogeneous information \nby designing semantic meta-paths, while heterogeneous \nGNNs [2, 11, 12] attempt to extend the message-passing pro-\ncedure to handle different categories of information. Specifi-\ncally, RGCN [2] introduces relation-specific transformations \nto encode features, HAN [12] designs hierarchical attention \nto describe node-level and semantic-level structures, HGT \n[11] uses meta-relation-based mutual attention to operate on \nheterogeneous graphs and learns implicit meta-paths. How-\never, these methods cannot deal with temporal dynamics.\nRepresentation learning on temporal graphs Accord-\ning to how temporal graphs are constructed, temporal graph \nrepresentation learning methods can be divided into two cat-\negories: discrete-time methods, which describe the temporal \ngraph as an ordered list of graph snapshots; continuous-time \nmethods, which treat the temporal graph as an event stream \nwith timestamps.\nFor the former, EvolveGCN [ 16] uses GCN to encode \nstatic graph structure and evolves the parameters of GCN \nby RNN. DySAT [17] uses structural attention to aggregate \ninformation from different neighbors in each snapshot and \nuses temporal attention to capture evolution over multiple \nsnapshots. DyHATR [18] adopts hierarchical attention to \nlearn heterogeneous information and applies RNNs with \ntemporal attention to capture dependencies among snap-\nshots. HTGNN [15] jointly models heterogeneous spatial \nand temporal dependencies through intra-relational, inter-\nrelational, and cross-temporal aggregation. ROLAND [29] \nproposes a framework to extend static GNN to dynamic \ngraphs. Although discrete-time methods succeed in learning \nthe dynamic patterns of temporal graphs, they ignore time \ninformation within the same snapshot and lead to weakened \nconnections between graph snapshots.\nRecent studies [19â€“ 23, 30] have shown the superior \nperformance of continuous-time methods in dealing with \ntemporal graphs. JODIE [21] uses RNNs to propagate \ninformation in interactions and update node representations \nsmoothly at different timesteps. TGAT [23] is designed \nas a GAT-like neural network, which propagates node \n101Memory-Enhanced Transformer forÂ Representation Learning onÂ Temporal Heterogeneous Graphs  \n1 3\ninformation by sampling and aggregating historical neigh-\nbors, and learns high-order patterns by stacking multiple \nlayers. TGN [24] proposes a general framework for encod-\ning temporal graphs and captures long-term dependencies \nby preserving node states. CAW-N [22] proposes Causal \nAnonymous Walks (CAWs) to inductively represent a tem-\nporal graph and uses RNN to encode the walk sequences. \nThese methods make full use of temporal information and \nmodel the dynamics of the graph without taking into account \nthe heterogeneity. THINE [19] and HPGE [20] combine het-\nerogeneous attention and Hawkes process to model graph \nheterogeneity and dynamics but do not consider the edge \nattributes.\nSelf-attention mechanism Transformer [31] proposed \nby Vaswani etÂ al. for machine translation has achieved great \nsuccess in NLP and CV tasks, which has recently been \nattempted for graph representation learning. For example, \nGTN [32] automatically generates useful meta-paths and \nlearns new graph structures. Graphormer [33] generalizes \npositional encoding to the graph domain and uses scaled dot-\nproduct attention for message passing. Transformer relies on \nthe self-attention mechanism to learn contextual information \nfor sequences. A scaled dot-product attention layer can be \ndefined as:\nwhere Q denotes the â€˜queriesâ€™, K the â€˜keysâ€™ and V the â€˜val-\nuesâ€™. They are the projections of the input Z on the matrices \nWQ , WK and WV , where Z contains the node embeddings and \ntheir positional embeddings.\n3  Preliminaries\nIn this section, we introduce the definition of temporal het-\nerogeneous graph and the problem of temporal heterogene-\nous graph representation learning.\nDefinition 1 Temporal Heterogeneous Graph. A temporal \nheterogeneous graph is G =( V, E, T, /u1D719,/u1D711) , where V denotes \nthe set of nodes corresponding to a node type mapping \nfunction /u1D719âˆ¶ V â†’ A , E denotes the temporal events (i.e., \nedges) corresponding to an event type mapping function \n/u1D711âˆ¶ E â†’ R , and T denotes the set of timestamps. A  and \nR are node type set and event type set, respectively, and \n/uni007C.varA/uni007C.var+ /uni007C.varR/uni007C.var> 2.\nNote that event e =( u, v, t, /u1D712) means that there is an edge \nfrom u to v at time t, where /u1D712 denotes the edge feature and \nr = /u1D711(e) denotes the event type.\n(1)Attn(Q,K,V)=softmax\nï¿½\nQKâŠ¤\nâˆš\nd\nï¿½\nV\nExample 2 In Fig.Â  1b, the temporal heterogeneous graph \nabout user-item interactions consists of 13 nodes, 17 events \n(smaller subscript of ti indicates that the event occurred ear-\nlier), two types of nodes, and three types of events. Specifi-\ncally, V ={ u1 , ...,u5 ,i1 , ...,i8 } , E = {(u 1 ,i1 ,t1 ), ...,(u5 ,i5 ,t8 )} , \nA ={ user, item} , R ={ r1 ,r2 ,r3 } , /u1D719(u)=user , /u1D719(i)=item , \nr1 denotes favorite, r2 denotes browse, and r3 denotes buy. \nAccording to the line color, we know that /u1D711(u1 ,i1 ,t1 )= r2 \nand /u1D711(u5 ,i5 ,t8 )= r3.\nFor any node pair (u,Â v), a temporal causal path is a set of \nevents consisting of u  as the source node of the start event \nand v as the target node of the terminal event. Therefore, \nthe temporal shortest path distance dt(u, v) is defined as the \nminimum length of the temporal causal path from u to v with \nall events on the path occurring no later than t. Denote V t as \nthe set of nodes that appear up to time t, and for each node \nv âˆˆ V t , define its k-hop temporal neighbors as:\nFor node v , we define its k -hop temporal neighborhood as \nG k\nt(v) , which is a subset of the temporal heterogeneous graph \nG and can be induced by Nk\nt (v) . G k\nt(v) contains the source \nnode v and its neighbors Nk\nt (v) , events between the nodes, \nand timestamps of these temporal events. The final repre -\nsentation of node v  will generate relying on G k\nt(v) . Notice \nthat we use Nt(v) and G t(v) to simplify the representation of \nN1\nt (v) and G 1\nt(v) in this paper, respectively.\nDefinition 2  Temporal Heterogeneous Graph Representa-\ntion Learning. Given a temporal heterogeneous graph G and \nthe node features X , it aims to learn a mapping function \nF âˆ¶ F(G, X) â†’ â„/uni007C.varV/uni007C.varÃ—d , where /uni007C.varV/uni007C.var is the node size and d  is \nthe dimension of embeddings, d â‰ª /uni007C.varV/uni007C.var.\nThis mapping function maps nodes to low-dimensional \nvector space while preserving temporal, structural, and \nsemantic information. For the sake of clarity, TableÂ  1 sum-\nmarizes the main notations used in this paper.\n4  The Proposed Model\nIn this section, we present a Transformer-like graph atten-\ntion architecture named THAN. It uses mapping matrices to \nproject node embeddings into the same vector space, then \npasses neighborhood information by dot-product attention \ncorresponding to different event types. Similar to GAT [5 ], \nTHAN is designed as a local aggregation operator that cap-\ntures high-order information by stacking multiple THAN \nlayers. FigureÂ  2 shows the architecture of the l-th THAN \n(2)Nk\nt(v)={ u âˆ¶ dt(u, v) â‰¤ k, u âˆˆ V t}\n102 L.Â Li et al.\n1 3\nlayer, which has three components: temporal heterogeneous \nneighbor sampling, dynamic embedding mapping and tem -\nporal heterogeneous graph attention layer. To capture long-\nterm dependencies and evolutionary patterns, we design an \noptional memory module providing indirect access to dis-\ntant neighbors, which dynamically updates the states of the \nnodes. After graph encoding, we use a heterogeneous graph \ndecoder for the temporal link prediction task, which receives \nthe node representations from THAN as inputs.\n4.1  Temporal Heterogeneous Neighbor Sampling\nFor the purpose of improving the induction and generali-\nzation performance of the model, THAN does not select \nall but a certain number of neighbors from the temporal \nneighbors as the input. Given a node v0 and time t, sample \nN neighbors from its 1-hop temporal neighbors Nt(v0) , \ndenoted as {v1 , ...,vN }.\nWe discuss two neighbor sampling strategies: uniform \nrandom sampling, where all temporal neighbors are ran-\ndomly selected with equal probability; top-N recent sam-\npling, where the time difference with the source node is \ncalculated and sorted in ascending order, then select the \ntop N neighbors. Intuitively, recent interactions reflect the \nnodeâ€™s current state better than distant interactions and \nhave a greater influence on future events. On the contrary, \nthe distant interactions may introduce noise. Therefore, \nwe use the top-N recent sampling strategy to sample \nneighbors.\nIn the temporal heterogeneous graph, the number of \ndifferent-typed events varies greatly, which can easily \nlead to an unbalanced distribution of the types of sam-\npled neighbors. To avoid sampling bias as far as possible, \nTHAN limits the number of samples of each event type to \nno more than M . If the total number of event types related \nto the source node is /u1D6FE (/u1D6FEâ‰¤ /uni007C.varR/uni007C.var) , the total number of sam-\npled neighbors N satisfies N â‰¤ /u1D6FEâˆ— M.\n4.2  Dynamic Embedding Mapping\nFor different nodes, TGAT [23] assumes that they are in \nthe same feature distribution and share parameters of the \nTable 1  Summary of main notations\nNotation Description\nG Temporal heterogeneous graph\nV Set of nodes\nE Set of edges\nT Set of timestamps\nA Set of node types\nR Set of edge types\n/u1D719 Node type mapping function\n/u1D711 Event type mapping function\ndt(u, v) Temporal shortest path distance from u to v at time t\nNk\nt (v) k-hop temporal neighbors of node v up to time t\nxv (t) Input embedding of node v\ne Projection vector of event types\nn Projection vector of node types\nM Mapping matrix of meta relation âŸ¨/u1D719(u), /u1D711(e),/u1D719(v)âŸ©\n/u1D7120,i (ti ) Event feature between node v0 and vi at time ti\nWQ , WK , WV Projection matrices of â€˜queryâ€™, â€˜keyâ€™ and â€˜valueâ€™\nÌƒh\nl\n0(t) Final output embedding of node v0 at time t\noi (t) Memory vector of node vi at time t\nmi (t) Message of node vi at time t\n(a) Temporal Heterogeneous Neighbor Sampling (c) Temporal Heterogeneous Graph Attention Layer\n(b) Dynamic Embedding Mapping\nFig. 2  The architecture of the l-th THAN layer for node u0 at time t \n103Memory-Enhanced Transformer forÂ Representation Learning onÂ Temporal Heterogeneous Graphs  \n1 3\nmodel, which does not hold in heterogeneous graphs. Fur -\nthermore, in the real world, there might be multiple types \nof edges between two nodes, and different types may cor -\nrespond to different vector distributions. So we must con -\nsider not only the diversity of nodes, but also edges when \npropagating information.\nA straightforward solution is mapping features in different \ndistributions to the same semantic space by transfer matrices. \nHowever, as the number of types increases, more parameters \nwill be introduced into the model. To reduce the number of \ntraining parameters as well as to avoid large-scale matrix mul-\ntiplication calculations, inspired by TransD [34], THAN pro-\njects node features from the node-type space to the event-type \nspace by dynamically computing the transfer matrix with two \ntype-related vectors.\nExample 3 Suppose that there are two types of nodes, and \nthree types of events in a heterogeneous graph. If there are \nfour dimensions of each node type and each event type, \nrespectively, we have to define the transfer matrix for each \nmapping from node type to event type. If we parameterize \nthe transfer matrices directly, then the number of parameters \nis 96 (i.e., 2 Ã— 3 Ã— 4 Ã— 4 ). However, the number of param-\neters is only 20 (i.e., 2 Ã— 4 + 3 Ã— 4 ) if we use the projection \nvectors.\nGiven an event e =( u, v, t) with its meta relation \nâŸ¨/u1D719(u), /u1D711(e), /u1D719(v)âŸ© [11], we define the dynamic mapping matri-\nces as:\nwhere e and n denote the projection vectors of event types \nand node types, respectively, both of which are trainable. \nThe projected node embeddings are:\nwhere xu (t) and xv (t) are the input embeddings of node u and \nv, respectively.\n4.3  Temporal Heterogeneous Graph Attention Layer\nDifferent events in a temporal heterogeneous graph may \nhave different features, for example, in a question answer -\ning network, an answer interaction can be regarded as an \nevent, and its features can be determined by the content. To \nenable event features to be propagated when aggregating \n(3)M eu = eğœ‘(e)nâŠ¤\nğœ™(u) + IdÃ—d\n(4)M ev = eğœ‘(e)nâŠ¤\nğœ™(v) + IdÃ—d\n(5)hu (t)=M eu xu (t)= nâŠ¤\nğœ™(u) xu (t)eğœ‘(e) + xu (t)\n(6)hv (t)=M ev xv (t)= nâŠ¤\nğœ™(v)xv (t)eğœ‘(e) + xv (t)\ninformation, THAN adds them to the node embeddings fol-\nlowed by a normalization layer (e.g., LayerNorm [35]). The \nevent features will be resized to the same dimension as the \nnode embeddings, and the output is:\nwhere i indicates the i-th neighbor, /u1D7120,i (ti ) denotes the feature \nof event between node v0 and vi at time ti . Here, we set /u1D7120,0 (t) \nas zero vector.\nTransformer [31] uses positional encoding to model rela-\ntive position relationships, thus solving the problem that the \nattention mechanism cannot capture the sequential relation-\nships between entities. In temporal graphs, a functional time \nencoder [36, 37] is usually used to map the time interval \nbetween nodes into a dT-dimensional vector in place of posi-\ntional encoding. THAN uses a Bochner-type functional time \nencoding [23, 37] as:\nwhere { /u1D714i }s are learnable parameters. We merge the time \nembeddings with the node representations to obtain the \nnode-temporal feature matrices as:\nwhere zei\n0 and zi denote the mapped embeddings of the source \nnode v0 and its neighbor vi corresponding to event ei , respec-\ntively, and â€– denotes the â€˜concatenateâ€™ operation. Zs and Zn \nare forwarded to three different linear projections to obtain \nthe â€˜queryâ€™, â€˜keyâ€™, and â€˜valueâ€™:\nwhere ei denotes the event between v0 and vi , W /u1D711(ei)\nQ  , W /u1D711(ei)\nK  , \nand W /u1D711(ei)\nV âˆˆ â„(d+dT )Ã—d denote the projection matrices. Due \nto the edge heterogeneity, the projection matrices cannot be \nshared directly, thus we use matrices of different types to \ndistinguish different events while capturing the semantics of \nevents. The attention weight /u1D6FCi is given by:\n(7)zi (ti )=LayerNorm(h l\ni (ti )+/u1D7120,i (ti ))\n(8)\nTE (t)=\n/uni221A.s2\n1\ndT\n[cos(/u1D7141 t), sin(/u1D7141 t), ..., cos(/u1D714dT\nt), sin(/u1D714dT\nt)]\n(9)Zs(t) =[ ze1\n0 (t)â€–TE(0), ...,zeN\n0 (t)â€–TE (0)]âŠ¤\n(10)Zn(t) =[ z1 (t1 )â€–TE (tâˆ’ t1 ), ...,zN (tN )â€–TE (tâˆ’ tN )]âŠ¤\n(11)Q = Zs(t)W /u1D711(ei)\nQ\n(12)K = Zn(t)W /u1D711(ei)\nK\n(13)V = Zn(t)W /u1D711(ei)\nV\n(14)ğ›¼i = exp(ğ›½i)\nâˆ‘N\nj=1 exp(ğ›½j)\n= QiKâŠ¤\ni â‹…\nğœ‡ğœ™(v0 ),ğœ‘(ei)\nâˆš\nd\n104 L.Â Li et al.\n1 3\nand it reveals how vi attends to the feature of v0 through \nevent ei . In addition, not all types of events have the same \ncontribution to the source node, so we set a learnable ten-\nsor /u1D707âˆˆ â„/uni007C.varA/uni007C.varÃ—/uni007C.varR/uni007C.var to adaptively adjust the scale of attention to \ndifferent-typed events.\nThe self-attention aggregates the features of temporal \nneighbors and obtains the hidden representation for node vi \nas /u1D6FCi Vi , which can capture both node features and topologi-\ncal information. The next step is to map the representations \nback to the type-specific distribution of node v0 so that they \ncan be fused with the its features. We use a linear projection \nnamed Q-Linear to do this and the neighborhood represen-\ntation is:\nTo combine neighborhood representation with the source \nnode feature, we concatenate and pass them to a feed-for -\nward neural network as in TGAT [23]:\nMulti-head attention can effectively improve the model \nperformance and stability. THAN can be easily extended to \nsupport a multi-head setup. Assuming the self-attention out-\nputs come from P  different heads, i.e., si â‰¡ Attni(Q, K, V) , \ni= 1, ...,P  . We first concatenate these neighborhood repre-\nsentations with the source nodeâ€™s feature and then carry out \nthe same procedure in Eq.Â 16 as:\nwhere Ìƒh\nl\n0(t)âˆˆâ„ d  is the final output representation for node \nv0 at time t, and it can be used for link prediction task with \nan encoder-decoder framework.\n(15)s(t)=\nN/uni2211.s1\ni=1\nQ- Linear/u1D719(v0 )(/u1D6FCiV i)\n(16)\nÌƒh\nl\n0(t)=FFN (s(t)â€–xl\n0(t)) â‰¡ ReLU([s(t)â€–xl\n0(t)]W l\n0 + bl\n0)W l\n1 + bl\n1\n(17)Ìƒh\nl\n0(t)=FFN(s 1(t)â€–...â€–sP (t)â€–xl\n0(t))\n4.4  Memory Module\nTHAN stacks multiple networks to capture high-order pat-\nterns. However, as the number of layers increases, the cost \nof memory resources and training time grows exponentially. \nMoreover, constrained by the message-passing architecture, \nTHAN cannot capture long-term features. To break this limi-\ntation, we devise a memory module (similar to TGN [24]) to \nsave the historical states (i.e., memories) of the nodes. The \nstates will be dynamically updated as events occur, thereby \nintroducing long-term dependencies and indirectly accessing \ninformation from distant hops. Our experimental study, to be \ngiven in Sect.Â 5, demonstrates that the memory module costs \nless time than adding a THAN layer, but achieves similar or \neven superior performance.\nFigureÂ  3 shows the standard computation process of \nTHAN with memory module on a batch of training data. \nIt encodes the input data and the latest memory by THAN \nand output the node representations. The memory module \nuses the representations to compute messages and update \nnode memory. However, the memory module does not \ndirectly affect the loss. To address this problem, we save \nmessages of nodes involved in current batch at the end of \ntraining and update the memory with messages from previ -\nous batch before graph embedding. The memory module \nconsists of the following components:\nMemory Bank keeps the latest vector o i(t) for node vi at \ntime t, which is initialized as a zero vector. Its memory is \nupdated on the occurrence of each event involving the node.\nMessage Function is a learnable function to compute a \nmessage mi (t) for node vi as follows:\nwhere Ìƒhi (t) is the representation from graph attention mod-\nule, tâˆ’\ni  is the time of the previous event involving node vi , \nmsg(â‹…) is the message function and we use FFN in this paper. \n(18)mi (t)=msg( Ìƒhi (t)â€–TE(t âˆ’ tâˆ’\ni ))\nFig. 3  Computation process of \nTHAN with the memory mod-\nule on a batch of events\n105Memory-Enhanced Transformer forÂ Representation Learning onÂ Temporal Heterogeneous Graphs  \n1 3\nIn the case of an event e ij(t) between source node vi and tar-\nget node vj , two messages (i.e., m i(t) and m j(t) ) can be com-\nputed. Different from TGN [24], we concatenate the node \nrepresentation with the time embedding of the time span, \nwhile TGN considers both source node memory, target node \nmemory, time embedding and edge features.\nMessage Aggregator is an aggregation function to \naggregate messages generated from message function com-\nponent. In batch processing node vi may involve multiple \nevents and each event corresponding to a message. These \nmessages mi(t1 ), ...,mi(tb ) are aggregated with the following \nformulation:\nwhere t1 , ...,tb â‰¤ t , agg(â‹…) can be optionally chosen as RNNs \nor attention networks. In this paper, we simply keep the lat-\nest message for a given node, which is considered from an \nefficiency perspective as it does not need to learn.\nMemory Updater is used to update the memory of the \nnodes with the aggregated messages. Its formulation is:\nwhere update(â‹…) is a learnable memory update function like \nLSTM [38] or GRU [39]. When an interaction event hap-\npens, the memories of both nodes it involves will be updated.\n4.5  Heterogeneous Graph Decoder\nHeterogeneous graph decoder aims to reconstruct hetero-\ngeneous edges of the graph relying on the node representa-\ntions, in other words, it scores edge triples through a function \nH âˆ¶ â„d Ã— â„dr Ã— â„d â†’ â„ , where dr denotes the dimension of \nedge type embeddings. We compute node representations \nthrough a l-layer THAN encoder and use a feed-forward neu-\nral network as the scoring function, thus an event (u,Â v,Â t) of \ntype r can be scored as:\nwhere u and v  denote the source and target node, respec-\ntively, r âˆˆ â„dr denotes the edge type embedding.\nAs in previous work [2 , 23], we train the model with \nnegative sampling. For each observed example, we change \nthe target node to construct a nonexistent event as a negative \nsample, so the number of positive samples is the same as that \nof negative samples. We optimize the cross-entropy loss as:\nwhere /u1D700 denotes the total set of positive and negative triples, \n/u1D70E denotes the logistic sigmoid function, y denotes the sample \n(19)Ìƒmi(t)=agg(m i(t1 ), ...,mi(tb ))\n(20)oi (t)= update( Ìƒmi (t),oi (tâˆ’))\n(21)H(u,v,t,r)=FFN( Ìƒh\nl\nu (t)â€–rr â€–Ìƒh\nl\nv (t))\n(22)\nL = 1\nï¿½/u1D700ï¿½\nï¿½\n(u,v,t,r,y)âˆˆ/u1D700\nâˆ’y log /u1D70E(H(u,v,t,r))\nâˆ’( 1 âˆ’ y) log(1 âˆ’ /u1D70E(H(u,v,t,r))) + /u1D706â€–/u1D703â€–2\n2\nlabel and takes the value of 1 for positive samples and 0 \nfor negative samples, /u1D703 denotes the model parameters and /u1D706 \ncontrols the L2 regularization.\n5  Experiments\nIn this section, we present the details of experiments includ-\ning experimental settings and results. Firstly, we introduce \nthe dataset, baselines, and parameter settings. Secondly, \nthe performance comparisons are demonstrated in detail. \nThirdly, we compare the effectiveness of different variants. \nFinally, we test the inductive capability of our proposed \nmodel.\n5.1  Experimental Settings\n5.1.1  Datasets\nWe evaluate our model on three public datasets: Movielens, \nTwitter, and MathOverflow. The statistics of these datasets \nare listed in TableÂ 2.\nâ€¢ Movielens1 is a dataset of user ratings of movies at dif-\nferent times collected from the MovieLens website. We \nselect two types of nodes: user and movie. Regarding \ndifferent ratings of movies as different types of events, a \ntotal of five types of events are obtained.\nâ€¢ Twitter2 collects public data on three types of relation-\nships (retweet, reply, and mention) between users from \nthe US social network Twitter.\nâ€¢ MathOverflow3 is from MathOverflow, a question and \nanswer site for professional mathematicians. There are \nthree relationships between users in this dataset: a user \nanswered or commented on another userâ€™s question, and \na user commented on an answer.\nTable 2  Statistics of the three public datasets\nDataset Node \ntypes\n#Nodes #Event \ntypes\n#Events Time span\nMovielens User 943 5 100,000 7 months\nMovie 1682\nTwitter User 304,691 3 563,069 188 days\nMathOver-\nflow\nUser 24,818 3 506,550 2,350 days\n1 https:// group lens. org/ datas ets/ movie lens/ 100k.\n2 http:// snap. stanf ord. edu/ data/ higgs- twitt er. html.\n3 http:// snap. stanf ord. edu/ data/ sx- matho verfl ow. html.\n106 L.Â Li et al.\n1 3\n5.1.2  Baselines\nTo demonstrate the effectiveness, we compare THAN with \nten popular graph representation learning methods, which \ncan be divided into three groups: static graph embedding \n(DeepWalk [27], metapath2vec [9], GraphSAGE [25], GAT \n[5], RGCN [2], and HGT [11]), discrete-time dynamic graph \nembedding (DySAT [17] and DyHATR [18]), and contin-\nuous-time dynamic graph embedding (TGAT [23] and \nHPGE [20]). We use the implementations of static graph \nembedding methods provided in the PyTorch Geometric \n(PyG) package [40], and for other baselines, use the code \nsubmitted by the authors on GitHub. Besides, we ignore the \nheterogeneity for homogeneous methods and ignore the tem-\nporal information for static methods. For fairness, the same \ndecoder declared in Sect.Â  4.5 is used for the downstream \ntemporal link prediction task.\nâ€¢ DeepWalk and metapath2vec: They are random walk-\nbased network embedding methods designed for static \ngraphs.\nâ€¢ GraphSAGE and GAT : They are two inductive GNN \nmethods for static homogeneous graphs, aggregating and \nupdating node representations in the message passing \nframework.\nâ€¢ RGCN and HGT: They are two GNN methods for static \nheterogeneous graphs, where the former maintains a \nunique linear projection weight for each edge type while \nthe latter uses mutual attention based on meta-relations \nto perform message passing on heterogeneous graphs.\nâ€¢ DySAT: A discrete-time temporal graph embedding \nmethod and we split graph snapshots with the guidance \nin the paper.\nâ€¢ DyHATR : A discrete-time THG embedding method that \nuses hierarchical attention to learn heterogeneous infor -\nmation and incorporates RNNs with temporal attention \nto capture evolutionary patterns.\nâ€¢ TGAT : A continuous-time temporal graph embedding \nmethod that aggregates historical neighbors by self-atten-\ntion to obtain node representations.\nâ€¢ HPGE: A continuous-time THG embedding method that \nintegrates the Hawkes process into graph embedding to \ncapture the excitation of historical heterogeneous events \nto current events.\n5.1.3  Parameter Settings\nTHAN was implemented in PyTorch. We split the training \nand test set as 8:2 according to time order. For a fair compar-\nison, we use the default parameter settings of the baselines \nand set the embedding (i.e., node output embeddings, time \nembeddings, and event type embeddings) dimension d  as \n32, regularization weight /u1D706 as 0.01, and dropout rate as 0.1. \nWe employ Adam as the optimizer with a learning rate of \n0.001. We randomly initialize the node vector if the dataset \ndoes not provide node features, and similarly, initialize the \nevent features as zero vectors. For DeepWalk, metapath2vec, \nGraphSAGE, GAT, RGCN, and HGT, we set the max train-\ning epochs as 500 and use an early stopping strategy with the \npatience of 50. For DySAT and DyHATR, we split datasets \ninto 10 snapshots. For our THAN, we set the event embed-\nding dimension as 16, the number of layers as 2, attention \nheads as 4, epochs as 20 (30 for Movielens), learning rate \nas 0.001 (0.0001 for Twitter), batch size as 800 (500 for \nMovielens), and the number of samples for each type of \nneighbors as 10 (8 for Movielens). The implementation of \nTHAN is publicly available.4\n5.2  Effectiveness Analysis\nWe conduct the temporal link prediction task to verify the \neffectiveness and efficiency, which asks if a type-r  edge \nexists between two nodes at time t. We run all methods five \ntimes on three datasets and evaluate the average AUC (Area \nunder the receiver operating characteristic curve) and AP \n(Average precision) scores. The overall results are shown \nin TableÂ 3.\nObviously, THAN achieves the state-of-the-art perfor -\nmance in AUC metric on all three datasets. Although THAN \ndoes not outperform all other methods in AP metric, it also \nhas a considerable performance (i.e., AP score achieves the \nSOTA result on Movielens and Twitter datasets and over \n0.9 on MathOverflow dataset). For the Movielens dataset, \ndynamic graph embedding methods outperform the static \ngraph embedding methods that ignore temporal informa-\ntion in both AUC and AP metrics, because the former learn \ntemporal information in fine-grained contexts. Specifically, \nDySAT and DyHATR obtain performance improvements \ndue to considering the changes of graph structure over time. \nTGAT, HPGE, and our THAN perform better than DySAT \nand DyHATR, this phenomenon shows that it is important to \nmake full use of temporal information compared with simply \npreserving evolving structures between snapshots. For the \nother two datasets, the results of all methods show a similar \ntrend, which means they probably have the same network \npatterns and temporal motifs. It may be related to the fact \nthat they are both user activities datasets.\nThe GNN-based approaches achieve better performance \nthan the random walk-based approaches since they cap -\nture much more useful information about the graph struc-\nture and the node features are utilized. The heterogeneous \ngraph methods perform better than the homogeneous graph \n4 https:// github. com/ scu- kdde/ HGA- THAN- 2022.\n107Memory-Enhanced Transformer forÂ Representation Learning onÂ Temporal Heterogeneous Graphs  \n1 3\nmethods which indicates that integrating semantic informa-\ntion can benefit graph representation. In terms of AUC score, \nTHAN improves performance by 3.38%, 2.29% and 8.1% \ncompared to TGAT on three datasets, respectively. It demon-\nstrates the effectiveness of our proposed model. In summary, \nour proposed approach works for two main reasons: (1) it \neffectively extracts structural features and fine-grained tem-\nporal information; (2) it reasonably handles heterogeneity in \nthe process of message passing and aggregation.\n5.3  Ablation Study\nTo demonstrate the effectiveness of each component in \nTHAN, we conduct ablation experiments by removing/\nreplacing one specific component at a time. We rename \nthem as: (1) THAN w/o time: removing time embeddings; \n(2) THAN w/o /u1D707 : removing event type attention weight; \n(3) THAN w/o Qlin: removing linear projection Q-Linear; \n(4) THAN r-uniform: using the uniform random sampling  \nstrategy instead of top-N recent sampling.\nWe report the results of the ablation study in Fig.Â 4, from \nwhich we have the following observations: (1) THAN out-\nperforms the others with components removed in all metrics; \n(2) Time embedding plays an important role in temporal \ngraph representation learning; (3) Setting different attention \nweights for different event types helps to learn heterogene-\nous semantic information; (4) More recent neighbors are \nmore useful for extracting temporal dynamics and better \nreflect the current state of the source node; (5) It makes \nsense to keep the same feature space to fuse features from \ndifferent nodes. Besides, it is noteworthy that removing \nthe Q-Linear component did not have a significant impact \non model performance on the Twitter and MathOverflow \ndatasets, that is because both these datasets have only one \ntype of node, and there is no need to consider the consist-\nency of feature distribution across different types of nodes.\n5.4  Parameter Sensitivity\nTo investigate the robustness of THAN and find the most \nsuitable hyperparameters, we analyzed the effect of the num-\nber of neighbor samples and attention heads on three data-\nsets shown in Fig.Â  5. For fairness, we select the number of \nneighbor samples from {4, 6, 8, 10} , the number of attention \nheads from {1, 2, 4, 6} , and the rest of the parameters remain \nthe same as the experimental settings in Sect.Â  5.1.\nOn the one hand, Fig.Â  5a, b can lead to the following \nconclusion: the scores of AUC and AP improve as the \nnumber of neighbor samples increases, but on the Mov -\nielens dataset there is a decreasing trend instead, which \nmay be caused by the dense connections between nodes. \nSampling more neighbors may introduce more noise, \nresulting in smooth node representations. On the other \nhand, Fig.Â  5c, d shows that the number of attention heads \naffects the performance of the model. Multi-head attention \nhelps to obtain different aspect representations from differ -\nent subspaces, thus enhancing the expressiveness.\n5.5  Effectiveness ofÂ Memory Module\nIn this section, we perform detailed studies on different \ninstances of THAN focusing on the trade-off between accu-\nracy and efficiency. The design of each variant is as follows: \n(1) THANl1 : only using one graph attention layer as the \ngraph encoder; (2) THANâ€ \nl1 : one-layer encoder with memory \nmodule; (3) THANl2 : stacking two graph attention layers.\nTable 3  Overall performance \ncomparison on temporal link \nprediction task\nAll results are converted to a percentage by multiplying by 100, and the standard deviations computed over \nten runs. The best and second-best results in each column are highlighted in bold font and underlined\nDataset Movielens Twitter MathOverflow\nModel AUC AP AUC AP AUC AP\nDeepWalk 67.35Â±0.3 71.26Â±0.3 57.73Â±0.7 63.63Â±0.9 63.73Â±0.2 73.47Â±0.4\nmetapath2vec 68.43Â±0.2 71.82Â±0.2 66.29Â±0.3 74.67Â±0.2 72.59Â±0.9 81.13Â±1.1\nGraphSAGE 72.34Â±0.4 75.94Â±0.4 76.88Â±3.2 85.10Â±2.1 83.48Â±2.4 89.20Â±3.4\nGAT 71.81Â±1.3 73.23Â±2.1 83.69Â±3.3 89.23Â±2.6 83.79Â±3.5 90.01Â±3.7\nRGCN 69.49Â±0.4 76.51Â±0.5 84.18Â±0.6 91.41Â±0.8 84.02Â±0.2 92.91Â±0.2\nHGT 73.44Â±1.1 80.01Â±0.7 88.54Â±0.5 93.06Â±0.3 86.53Â±1.5 93.88Â±1.4\nDySAT 73.13Â±0.4 72.1Â±0.3 83.03Â±0.3 86.89Â±0.2 83.12Â±0.3 85.84Â±0.1\nDyHATR 80.21Â±0.7 77.54Â±1.3 79.73Â±0.1 81.78Â±0.4 75.22Â±0.1 78.21Â±0.2\nTGAT 82.00Â±0.4 79.46Â±0.4 89.55Â±0.3 90.43Â±0.2 82.23Â±0.6 83.25Â±0.6\nHPGE 85.25Â±0.1 82.16Â±0.2 73.55Â±0.1 73.91Â±0.1 81.12Â±0.2 82.61Â±0.2\nTHAN 88.63Â±0.1 86.77Â±0.2 91.84Â±0.2 93.43Â±0.2 90.33Â±0.1 90.62Â±0.2\n108 L.Â Li et al.\n1 3\nFrom TableÂ 4, we can see that stacking two layers helps \nobtaining good performance ( THANl2 vs THANl1 ), while \nthe time costs increase by a factor of 8 to 21. Compared to \nadding a THAN layer, using memory module achieves sim-\nilar or even superior model performance, but spends much \nless time ( THANl2 takes about 13 times, 4 times and 19 \ntimes longer than THANâ€ \nl1 on three datasets, respectively). \nOn the one hand, the number of neighbors that need to be \naggregated increases exponentially by adding a layer. On \nthe other hand, when accessing the memory of the source \nnode and its 1-hop neighbors, THAN is introducing long-\nterm dependencies and indirectly accessing information \nfrom distant hops.\n5.6  Inductive Capability Analysis\nWe further discuss the inductive performance of THAN with \nthe same settings as TGAT, i.e., mask 10% of the nodes from \nthe training set and predict the existence of future events \ncontaining these masked nodes. In this paper, we choose \nGraphSAGE, GAT and TGAT as the comparison mod-\nels. They are proposed as inductive representation learn-\ning methods on graphs, and their inductive capabilities are \ndemonstrated experimentally. Experiments were conducted \non three datasets and the results are shown in TableÂ 5. Intui-\ntively, THAN outperformed the baselines in two metrics on \nall datasets, which demonstrates the inductive capability of \nTHAN.\n(a)A UC of Movielens (b)A UC of Twitter (c)A UC of MathOverï¬‚ow\n(d)A Po fM ovielens (e)A Po fT witter (f)A Po fM athOverï¬‚ow\nFig. 4  Ablation study of THAN\n(a)N eighbor: AUC (b)N eighbor: AP\n(c)H ead: AUC (d)H ead: AP\nFig. 5  Sensitivity analysis on the number of neighbor samples and \nattention heads\n109Memory-Enhanced Transformer forÂ Representation Learning onÂ Temporal Heterogeneous Graphs  \n1 3\n6  Conclusion\nExisting graph representation learning methods cannot \nwell capture the information of temporal heterogeneous \ngraphs. This paper proposes the THAN, which is a con-\ntinuous-time temporal heterogeneous graph representation \nlearning method. THAN uses dynamic transfer matrices to \nmap different-typed nodes to the same feature space and \naggregates neighborhood information based on the type-\naware self-attention mechanism. To efficiently utilize tem-\nporal information, THAN uses a functional time encoder to \ngenerate time embeddings that are naturally integrated into \nthe neighbor aggregation process. THAN is an inductive \nmessage-passing model based on historical neighbor sam-\npling that not only captures temporal dynamics but also effi-\nciently extracts topological features. In addition, we devise \nan optional memory module to store node states and capture \nlong-term dependencies. It improves the model performance \nand takes less time than stacking a new THAN layer. The \nexperimental results on three public datasets demonstrate \nthat THAN outperforms the baselines on the temporal link \nprediction task.\nIn future work, on one hand, we plan to explore the \nusage of THAN in various fields, such as recommender \nsystems, social networks, and biological interaction net-\nworks. On the other hand, we try to understand the specific \npatterns/motifs of temporal heterogeneous networks from \ndifferent domains. Furthermore, the large-scale temporal \nheterogeneous graph embedding is another direction wor -\nthy of further investigation.\nAuthor Contributions Longhai Li proposes the methodology, com-\npletes the experiments and writes the manuscript. Lei Duan provides \ninstructions and revises the manuscript. Junchen Wang and Zihao \nChen assist in improving the experiment. Junchen Wang, Chengxin \nHe, Guicai Xie, Song Deng and Zhaohang Luo jointly help to write \nthe manuscript.\nFunding This work was supported in part by the National Natural Sci-\nence Foundation of China (61972268)Â and the Joint Innovation Foun-\ndation of Sichuan University and Nuclear Power Institute of China.\n Availability of data and materials  The datasets used in experiments \ncan be downloaded from the following URLs: Movielens: https:// group \nlens. org/ datas ets/ movie lens/ 100k Twitter: http:// snap. stanf ord. edu/ \ndata/ higgs- twitt er. html MathOverflow: http:// snap. stanf ord. edu/ data/ \nsx- matho verfl ow. html.\nDeclarations \nConflicts of interest We would like to submit the enclosed manuscript \nentitled â€œMemory-Enhanced Transformer for Representation Learning \non Temporal Heterogeneous Graphsâ€, which we wish to be considered \nfor publication in Data Science and Engineering. No conflict of interest \nexits in the submission of this manuscript, and manuscript is approved \nby all authors for publication. I would like to declare on behalf of my \nco-authors that the work described was original research that has not \nbeen published previously, and not under consideration for publication \nelsewhere, in whole or in part. All the authors listed have approved the \nmanuscript that is enclosed.\nOpen Access This article is licensed under a Creative Commons Attri-\nbution 4.0 International License, which permits use, sharing, adapta-\ntion, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, \nprovide a link to the Creative Commons licence, and indicate if changes \nwere made. The images or other third party material in this article are \nincluded in the article's Creative Commons licence, unless indicated \notherwise in a credit line to the material. If material is not included in \nthe article's Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a \ncopy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nTable 4  Trade-off between \naccuracy (test AUC score in \n%) and speed (time cost per \nepoch in second) for different \ninstances of the THAN\nAll experiments are conducted in batch size of 200\nDataset Movielens Twitter MathOverflow\nModel AUC Time AUC Time AUC Time\nTHANl1 83.90Â±0.2 35.8Â±2.0 87.80Â±0.5 192.6Â±4.0 87.40Â±0.1 185.4Â±3.1\nTHANâ€ \nl1\n87.87Â±0.3 40.5Â±2.6 92.79Â±0.5 360.7Â±28.1 92.93Â±0.3 210.8Â±12.5\nTHANl2 88.63Â±0.1 524.2Â±5.4 91.84Â±0.2 1545.5Â±20.1 90.33Â±0.1 4046.8Â±19.7\nTable 5  Results of inductive \nlearning task Dataset Movielens Twitter MathOverflow\nModel AUC AP AUC AP AUC AP\nGraphSAGE 70.58Â±0.8 71.37Â±1.6 73.15Â±3.9 78.44Â±3.5 70.56Â±5.5 74.23Â±5.2\nGAT 69.48Â±1.2 72.02Â±1.9 75.77Â±4.4 80.61Â±5.1 71.73Â±4.8 76.42Â±5.6\nTGAT 78.35Â±0.4 76.97Â±0.3 85.87Â±0.3 88.61Â±0.3 74.14Â±0.5 75.96Â±0.4\nTHAN 82.71Â±0.2 80.67Â±0.2 88.41Â±0.2 90.69Â±0.3 80.93Â±0.3 80.52Â±0.3\n110 L.Â Li et al.\n1 3\nReferences\n 1. Kipf TN, Welling M (2016) Variational graph auto-encoders. \nCoRR arXiv: 1611. 07308\n 2. Schlichtkrull MS, Kipf TN, Bloem P, van den Berg R, Titov I, \nWelling M (2018) Modeling relational data with graph convolu-\ntional networks. In: Proceedings of the 15th international confer-\nence on semantic web, vol 10843, pp 593â€“607\n 3. He C, Duan L, Zheng H, Li-Ling J, Song L, Li L (2022) Graph \nconvolutional network approach to discovering disease-related \nCIRCRNAâ€“MIRNAâ€“MRNA axes. Methods 198:45â€“55\n 4. Kipf TN, Welling M (2017) Semi-supervised classification with \ngraph convolutional networks. In: Proceedings of the 5th interna-\ntional conference on learning representations\n 5. Velickovic P, Cucurull G, Casanova A, Romero A, LiÃ² P, Bengio \nY (2018) Graph attention networks. In: Proceedings of the 6th \ninternational conference on learning representations\n 6. Gilmer J, Schoenholz SS, Riley PF, Vinyals O, Dahl GE (2017) \nNeural message passing for quantum chemistry. In: Proceedings \nof the 34th international conference on machine learning, vol 70, \npp 1263â€“1272\n 7. Ying Z, You J, Morris C, Ren X, Hamilton WL, Leskovec J (2018) \nHierarchical graph representation learning with differentiable \npooling. In: Proceedings of the 32nd international conference on \nneural information processing systems, pp 4805â€“4815\n 8. Tuteja S, Kumar R (2022) A unification of heterogeneous data \nsources into a graph model in e-commerce. Data Sci Eng 7:57â€“70\n 9. Dong Y, Chawla NV, Swami A (2017) metapath2vec: scalable rep-\nresentation learning for heterogeneous networks. In: Proceedings \nof the 23rd ACM SIGKDD international conference on knowledge \ndiscovery and data mining, pp 135â€“144\n 10. Fu T, Lee W, Lei Z (2017) Hin2vec: explore meta-paths in het-\nerogeneous information networks for representation learning. In: \nProceedings of the 2017 ACM on conference on information and \nknowledge management, pp 1797â€“1806\n 11. Hu Z, Dong Y, Wang K, Sun Y (2020) Heterogeneous graph trans-\nformer. In: Proceedings of the 29th international conference on \nworld wide web, pp 2704â€“2710\n 12. Wang X, Ji H, Shi C, Wang B, Ye Y, Cui P, Yu PS (2019) Het-\nerogeneous graph attention network. In: Proceedings of the 28th \ninternational conference on world wide web, pp 2022â€“2032\n 13. Zhao J, Wang X, Shi C, Hu B, Song G, Ye Y (2021) Heterogene-\nous graph structure learning for graph neural networks. In: Pro-\nceedings of the 35th AAAI conference on artificial intelligence, \npp 4697â€“4705\n 14. Kazemi SM, Goel R, Jain K, Kobyzev I, Sethi A, Forsyth P, \nPoupart P (2020) Representation learning for dynamic graphs: a \nsurvey. J Mach Learn Res 21:70â€“17073\n 15. Fan Y, Ju M, Zhang C, Ye Y (2022) Heterogeneous temporal \ngraph neural network. In: Proceedings of the 2022 SIAM inter -\nnational conference on data mining, pp 657â€“665\n 16. Pareja A, Domeniconi G, Chen J, Ma T, Suzumura T, Kanezashi \nH, Kaler T, Schardl TB, Leiserson CE (2020) Evolvegcn: evolving \ngraph convolutional networks for dynamic graphs. In: Proceed -\nings of the 34th AAAI conference on artificial intelligence, pp \n5363â€“5370\n 17. Sankar A, Wu Y, Gou L, Zhang W, Yang H (2020) Dysat: deep \nneural representation learning on dynamic graphs via self-atten-\ntion networks. In: Proceedings of the 13th international confer -\nence on web search and data mining, pp 519â€“527\n 18. Xue H, Yang L, Jiang W, Wei Y, Hu Y, Lin Y (2020) Modeling \ndynamic heterogeneous network for link prediction using hierar -\nchical attention with temporal RNN. In: Proceedings of the 2020 \nEuropean conference on machine learning and knowledge discov-\nery in databases, vol 12457, pp 282â€“298\n 19. Huang H, Shi R, Zhou W, Wang X, Jin H, Fu X (2021) Temporal \nheterogeneous information network embedding. In: Proceedings \nof the 30th international joint conference on artificial intelligence, \npp 1470â€“1476\n 20. Ji Y, Jia T, Fang Y, Shi C (2021) Dynamic heterogeneous graph \nembedding via heterogeneous hawkes process. In: Proceedings of \nthe 2021 European conference on machine learning and knowl-\nedge discovery in databases, vol 12975, pp 388â€“403\n 21. Kumar S, Zhang X, Leskovec J (2019) Predicting dynamic embed-\nding trajectory in temporal interaction networks. In: Proceedings \nof the 25th ACM SIGKDD international conference on knowledge \ndiscovery and data mining, pp 1269â€“1278\n 22. Wang Y, Chang Y, Liu Y, Leskovec J, Li P (2021) Inductive rep-\nresentation learning in temporal networks via causal anonymous \nwalks. In: Proceedings of the 9th international conference on \nlearning representations\n 23. Xu D, Ruan C, KÃ¶rpeoglu E, Kumar S, Achan K (2020) Inductive \nrepresentation learning on temporal graphs. In: Proceedings of the \n8th international conference on learning representations\n 24. Rossi E, Chamberlain B, Frasca F, Eynard D, Monti F, Bron -\nstein MM (2020) Temporal graph networks for deep learning on \ndynamic graphs. CoRR arXiv: 2006. 10637\n 25. Hamilton WL, Ying Z, Leskovec J (2017) Inductive representa -\ntion learning on large graphs. In: Proceedings of the 31st interna-\ntional conference on neural information processing systems, pp \n1024â€“1034\n 26. Grover A, Leskovec J (2016) node2vec: Scalable feature learning \nfor networks. In: Proceedings of the 22nd ACM SIGKDD inter -\nnational conference on knowledge discovery and data mining, pp \n855â€“864\n 27. Perozzi B, Al-Rfou R, Skiena S (2014) Deepwalk: online learn-\ning of social representations. In: Proceedings of the 20th ACM \nSIGKDD international conference on knowledge discovery and \ndata mining, pp 701â€“710\n 28. Luo J, Xiao S, Jiang S, Gao H, Xiao Y (2022) ripple2vec: node \nembedding with ripple distance of structures. Data Sci Eng \n7:156â€“174\n 29. You J, Du T, Leskovec J (2022) ROLAND: graph learning frame-\nwork for dynamic graphs. In: Proceedings of the 28th ACM SIG-\nKDD international conference on knowledge discovery and data \nmining, pp 2358â€“2366\n 30. Trivedi R, Farajtabar M, Biswal P, Zha H (2019) Dyrep: learning \nrepresentations over dynamic graphs. In: Proceedings of the 7th \ninternational conference on learning representations\n 31. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez \nAN, Kaiser L, Polosukhin I (2017) Attention is all you need. In: \nProceedings of the 31st international conference on neural infor -\nmation processing systems, pp 5998â€“6008\n 32. Yun S, Jeong M, Kim R, Kang J, Kim HJ (2019) Graph trans-\nformer networks. In: Proceedings of the 33rd international confer-\nence on neural information processing systems, pp 11960â€“11970\n 33. Ying C, Cai T, Luo S, Zheng S, Ke G, He D, Shen Y, Liu T (2021) \nDo transformers really perform badly for graph representation? \nIn: Proceedings of the 35th international conference on neural \ninformation processing systems, pp 28877â€“28888\n 34. Ji G, He S, Xu L, Liu K, Zhao J (2015) Knowledge graph embed-\nding via dynamic mapping matrix. In: Proceedings of the 53rd \nannual meeting of the association for computational linguistics, \npp 687â€“696\n 35. Ba LJ, Kiros JR, Hinton GE (2016) Layer normalization. CoRR \narXiv: 1607. 06450\n 36. Kazemi SM, Goel R, Eghbali S, Ramanan J, Sahota J, Thakur S, \nWu S, Smyth C, Poupart P, Brubaker M (2019) Time2vec: learn-\ning a vector representation of time. CoRR arXiv: 1907. 05321\n 37. Xu D, Ruan C, KÃ¶rpeoglu E, Kumar S, Achan K (2019) Self-atten-\ntion with functional time representation learning. In: Proceedings \n111Memory-Enhanced Transformer forÂ Representation Learning onÂ Temporal Heterogeneous Graphs  \n1 3\nof the 33rd international conference on neural information pro-\ncessing systems, pp 15889â€“15899\n 38. Hochreiter S, Schmidhuber J (1997) Long short-term memory. \nNeural Comput 9(8):1735â€“1780\n 39. Cho K, van Merrienboer B, GÃ¼lÃ§ehre Ã‡, Bahdanau D, Bougares \nF, Schwenk H, Bengio Y (2014) Learning phrase representations \nusing RNN encoder-decoder for statistical machine translation. \nIn: Proceedings of the 2004 conference on empirical methods in \nnatural language processing, pp 1724â€“1734\n 40. Fey M, Lenssen JE (2019) Fast graph representation learning with \npytorch geometric. CoRR arXiv: 1903. 02428",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8393561840057373
    },
    {
      "name": "Feature learning",
      "score": 0.5996005535125732
    },
    {
      "name": "Theoretical computer science",
      "score": 0.544427216053009
    },
    {
      "name": "Graph",
      "score": 0.5432800054550171
    },
    {
      "name": "Representation (politics)",
      "score": 0.49342527985572815
    },
    {
      "name": "Transformer",
      "score": 0.4821133613586426
    },
    {
      "name": "Node (physics)",
      "score": 0.4364292323589325
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4277307391166687
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    },
    {
      "name": "Structural engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210125143",
      "name": "Chengdu University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I24185976",
      "name": "Sichuan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I41198531",
      "name": "Nanjing University of Posts and Telecommunications",
      "country": "CN"
    }
  ],
  "cited_by": 16
}