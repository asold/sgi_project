{
  "title": "Video Relation Detection via Tracklet based Visual Transformer",
  "url": "https://openalex.org/W3196198596",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2352132229",
      "name": "Gao, Kaifeng",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2107639259",
      "name": "Chen Long",
      "affiliations": [
        "Columbia University"
      ]
    },
    {
      "id": "https://openalex.org/A2361908425",
      "name": "Huang Yifeng",
      "affiliations": [
        "Zhejiang University"
      ]
    },
    {
      "id": "https://openalex.org/A2097275512",
      "name": "Xiao Jun",
      "affiliations": [
        "Zhejiang University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3096609285",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2987123286",
    "https://openalex.org/W3034467781",
    "https://openalex.org/W3205874482",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2141461755",
    "https://openalex.org/W2981385984",
    "https://openalex.org/W2951323451",
    "https://openalex.org/W2765137706",
    "https://openalex.org/W3093028502",
    "https://openalex.org/W2981895973",
    "https://openalex.org/W2250384498",
    "https://openalex.org/W2603203130",
    "https://openalex.org/W3093195700",
    "https://openalex.org/W2963536419",
    "https://openalex.org/W2963786238",
    "https://openalex.org/W2982272924"
  ],
  "abstract": "Video Visual Relation Detection (VidVRD), has received significant attention\\nof our community over recent years. In this paper, we apply the\\nstate-of-the-art video object tracklet detection pipeline MEGA and deepSORT to\\ngenerate tracklet proposals. Then we perform VidVRD in a tracklet-based manner\\nwithout any pre-cutting operations. Specifically, we design a tracklet-based\\nvisual Transformer. It contains a temporal-aware decoder which performs feature\\ninteractions between the tracklets and learnable predicate query embeddings,\\nand finally predicts the relations. Experimental results strongly demonstrate\\nthe superiority of our method, which outperforms other methods by a large\\nmargin on the Video Relation Understanding (VRU) Grand Challenge in ACM\\nMultimedia 2021. Codes are released at\\nhttps://github.com/Dawn-LX/VidVRD-tracklets.\\n",
  "full_text": "Video Relation Detection via Tracklet based Visual Transformer\nKaifeng Gaoâ€ , Long Chenâ€¡âˆ—, Yifeng Huangâ€ , and Jun Xiaoâ€ \nâ€ Zhejiang University â€¡Columbia University\nkite_phone@zju.edu.cn,zjuchenlong@gmail.com,yfhuang@zju.edu.cn,junx@cs.zju.edu.cn\nABSTRACT\nVideo Visual Relation Detection (VidVRD), has received significant\nattention of our community over recent years. In this paper, we\napply the state-of-the-art video object tracklet detection pipeline\nMEGA [7] and deepSORT [27] to generate tracklet proposals. Then\nwe perform VidVRD in a tracklet-based manner without any pre-\ncutting operations. Specifically, we design a tracklet-based visual\nTransformer. It contains a temporal-aware decoder which performs\nfeature interactions between the tracklets and learnable predicate\nquery embeddings, and finally predicts the relations. Experimental\nresults strongly demonstrate the superiority of our method, which\noutperforms other methods by a large margin on the Video Rela-\ntion Understanding (VRU) Grand Challenge in ACM Multimedia\n2021. Codes are released at https://github.com/Dawn-LX/VidVRD-\ntracklets.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Scene understanding.\nKEYWORDS\nVisual Relation Detection, Video Tracklets, Transformer\nACM Reference Format:\nKaifeng Gaoâ€ , Long Chenâ€¡âˆ—, Yifeng Huangâ€ , and Jun Xiaoâ€ . 2021. Video\nRelation Detection via Tracklet based Visual Transformer. In Proceedings\nof the 29th ACM International Conference on Multimedia (MM â€™21), October\n20â€“24, 2021, Virtual Event, China. ACM, New York, NY, USA, 5 pages. https:\n//doi.org/10.1145/3474085.3479231\n1 INTRODUCTION\nThe Video Visual Relation Detection (VidVRD) task aims to detect\nvisual relations between objects in videos, which are denoted by a\nset of <subject, predicate, object> triplets. Compared to visual\nrelation detection in still images (ImgVRD) [6], VidVRD is techni-\ncally more challenging: (1) The relationship between objects incor-\nporates temporal informations. Some relationships (e.g., towards,\nmove-past) can only be detected by utilizing temporal context. (2)\nRelations between two specific objects often changes overtime.\nâˆ—Long Chen is the corresponding author. This work started when Long Chen at ZJU.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nMM â€™21, October 20â€“24, 2021, Virtual Event, China\nÂ© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8651-7/21/10. . . $15.00\nhttps://doi.org/10.1145/3474085.3479231\nInspired from ImgVRD approaches, early VidVRD models [17, 20,\n22, 25] are allsegment-based approaches. Specifically, they first di-\nvide the whole video into multiple short segments. Then, they detect\nall tracklets and their pairwise relationships in each short segment.\nLastly, they merge relationship triplets among adjacent segments\nwith association methods. Although these segment-based VidVRD\napproaches achive sound performance on standard benchmarks,\nthis framework inherently fails to utilize the long-term temporal\ncontext in other segments. To avoid this inherent limitation, one\nof the latest VidVRD model STGCN [ 14] performs VidVRD in a\ntracklet-based manner. Instead of pre-cutting the input video into\nmultiple segments in advance, STGCN directly detects all object\ntracklets, and runs sliding windows with different scales to obtain\nnumerous tracklet proposals. Then, STGCN predicts the tracklet\nclasses and their pairwise relationships.\nIn this paper, we follow STGCN [14] to first detect all the object\ntracklets in a video. Then we design a tracklet based visual Trans-\nformer to perform interactions between tracklet features and finally\ndetect the relations. Specifically, Our model is a Transformer-family\nencoder-decoder model [ 26], where the inputs for the encoder\nand decoder are initial tracklet features and learnable predicate\nquery embeddings, respectively. The encoder aims to extract con-\ntextual features for object tracklets, and the decoder intends to\nenhance predicate queries with all tracklets. We use each predicate\nquery to represent a relationship instance, and it has two types\n(i.e., subject/object) of links to tracklets. The final output of our\nmodel are the enhanced predicate queries, which will be fed into a\nmulti-layer perceptron (MLP) for predicate classification, and the\nattention matrix from the last encoder layer, which will be binarized\nto obtain the links to subject/object for each predicate query.\nWe will describe our method in two main steps: 1) object detec-\ntion and tracking (Section 2), which returns a set of object tracklet\nproposals, and 2) relation detection (Section 3), which detects the\nvisual relations among the pre-obtained tracklet proposals.\n2 OBJECT DETECTION AND TRACKING\nObject tracklet detection determines the ceiling of the video visual\nrelation detection performance. Different from previous works, we\nuse MEGA [7] with ResNet-101 [9] to detect frame-level objects, and\nthen use deepSORT [27] to associate them into tracklet proposals.\nOur detector was trained on the training and validation set of\nMS-COCO[13] and the training set of VidOR [19]. For MS-COCO,\nwe selected the same 80 object categories as that in VidOR. For\nVidOR, considering the redundancy of adjacent frames, we sample\nkey frames every 32 frames for each video. After all, the training\nset consists around 311k images.\nWe run the above trained detector on each frame of the video to\nobtain the frame-level object detection results, where each result\ncontains box coordinates, visual appearance features, and the object\narXiv:2108.08669v1  [cs.CV]  19 Aug 2021\nTracklet\nDetection\nTransformer\nEncoder\nPooling & Flatten\nÃ— L e \npredicate queries\nTemporal-aware\nDecoder\n. . .\nPosEmb\n. . .\n. . .\ntracklet \nfeatures . . .\nÃ— L d \n. . .\nMLP\n+\nclasseme\nMLP\nSoftmax\nfor each enhanced query\nRelation Predictions\nLinking\nprediction\nClassification\nenhanced predicate queries\n< person, sit-above, motocycle>\n< person, ride, motocycle>\n< car, stop-behind, motocycle>\n<motocycle, move-left, car >\n<motocycle, move-front, car >\nsubj\nobj\nInput Video\nFigure 1: The pipeline of our method. Note that each predicate query is assigned with a pre-defined temporal anchor, and the\npositional embeddings (PosEmb) are calculated based on these anchors through a learnable projection matrix.\nclassification logits. Then we adopt the object tracking method\ndeepSORT [27] to generate object tracklets, denoted as {ğ‘‡ğ‘–}ğ‘›\nğ‘–=1.\nEach tracklet ğ‘‡ğ‘– with length (number of frames) ğ‘™ğ‘– is characterized\nby its time slots (ğ‘ ğ‘–,ğ‘’ğ‘–), box coordinates {ğ’ƒğ‘–,ğ‘— âˆˆR4 |ğ‘— = 1,2,...ğ‘™ ğ‘–},\nappearance features ğ’‡ğ‘\nğ‘– âˆˆRğ‘™ğ‘– Ã—ğ‘‘ğ‘ , and the object category ğ‘ğ‘– âˆˆCobj,\nwhere Cobj is the set of all object categories for VidOR. We fix all\nthe box coordinates {ğ’ƒğ‘–,ğ‘—}and object categories {ğ‘ğ‘–}as the final\npredictions. In addition, we reserve the classification probabilities\nfrom the detection backbone for each tracklet, denoted as ğ’‘ğ‘– âˆˆ\nR|Cobj |, for the final relation prediction.\n3 RELATION DETECTION\nThe overall pipeline of our model is shown in Figure 1. It uses\na fixed set of ğ‘š predicate queries {ğ‘ğ‘—}ğ‘š\nğ‘—=1, with learnable query\nembeddings, denoted as ğ‘¸ âˆˆRğ‘šÃ—ğ‘‘ğ‘ . Each query is responsible for\neach of the final predicate prediction, which is characterized by its\nlinks to subject/object and its category ğ‘ğ‘\nğ‘— âˆˆCrel, where Crel is the\nset of all relation categories for VidOR.\n3.1 Tracklet Feature Initialization\nFor each tracklet ğ‘‡ğ‘–, we consider both the static coordinates {ğ’ƒğ‘–,ğ‘—}\nand the dynamic features {Î”ğ’ƒğ‘–,ğ‘—}for its spatial feature, where\nÎ”ğ’ƒğ‘–,ğ‘— = ğ’ƒğ‘–,ğ‘—+1 âˆ’ğ’ƒğ‘–,ğ‘—, ğ‘—= 1,...,ğ‘™ ğ‘– âˆ’1. (1)\nThe spatial feature ğ’”ğ‘– âˆˆRğ‘™ğ‘– Ã—8 is obtained by stacking {ğ’ƒğ‘–,ğ‘—}ğ‘™ğ‘–\nğ‘—=1 and\n{Î”ğ’ƒğ‘–,ğ‘—}ğ‘™ğ‘–\nğ‘—=1. Then, the appearance feature ğ’‡ğ‘\nğ‘– and spatial feature ğ’”ğ‘–\nare fed into two MLPs and their outputs are concatenated as the ini-\ntial tracklet feature ğ’‡ğ‘– âˆˆRğ‘™ğ‘– Ã—ğ‘‘, where ğ’‡ğ‘– = [MLPğ‘£(ğ’‡ğ‘\nğ‘– ); MLPğ‘ (ğ’”ğ‘–)].\n3.2 Encoder-Decoder Feature Interactions\nEncoder. Since the size of each tracklet feature is different, we first\nuse a pooling operation to transform tracklet feature ğ’‡ğ‘– âˆˆRğ‘™ğ‘– Ã—ğ‘‘ to\na fixed size feature ğ’‡ğ‘– âˆˆRğ‘™Ã—ğ‘‘, and flatten it into a vector, followed\nby a MLP to reduce the dimension, resulting in a vector ğ’‰ğ‘– âˆˆRğ‘‘.\nThen, we stack all tracklet features{ğ’‰ğ‘–}into matrix ğ‘¯ âˆˆRğ‘›Ã—ğ‘‘, and\nfeed ğ‘¯ into a Transformer encoder, i.e., eğ‘¯ = Transformerenc (ğ‘¯),\nwhere the outputs eğ‘¯ âˆˆRğ‘›Ã—ğ‘‘ are contextualized tracklet features\nDecoder. In contrast to the standard Transformer decoder, we\ndesign a temporal-aware decoder that considers different temporal\nRoIs of the tracklet features {ğ’‡ğ‘–}. To naturally differentiate each\nquery embedding, we assign each query ğ‘ğ‘— with a temporal anchor\n(ğ‘ ğ‘\nğ‘—,ğ‘’ğ‘\nğ‘—). The time slot of ğ‘ğ‘— is regressed with respect to (ğ‘ ğ‘\nğ‘—,ğ‘’ğ‘\nğ‘—)in\neach decoder layer, denoted as (ğ‘ ğ‘\nğ‘—,ğ‘’ğ‘\nğ‘—), based on which we perform\nRoI pooling for each tracklet-query pair:\nğ’‡roi\nğ‘–,ğ‘— = RoIPool \u0000ğ’‡ğ‘–,(ğ‘ ğ‘–,ğ‘—,ğ‘’ğ‘–,ğ‘—)\u0001 , s.t. (ğ‘ ğ‘–,ğ‘—,ğ‘’ğ‘–,ğ‘—)â‰  âˆ…. (2)\nwhere (ğ‘ ğ‘–,ğ‘—,ğ‘’ğ‘–,ğ‘—)= (ğ‘ ğ‘–,ğ‘’ğ‘–)âˆ©( ğ‘ ğ‘\nğ‘—,ğ‘’ğ‘\nğ‘—),\nwhere RoIPool is a one-dim RoI pooling operation, ğ’‡roi\nğ‘–,ğ‘— âˆˆRğ‘™roiÃ—ğ‘‘\nis the RoI feature of tracklet ğ‘‡ğ‘– for the query ğ‘ğ‘—. Zero-padding is\nutilized for those tracklet-predicate pairs that have no temporal\noverlap. After obtaining {ğ’‡roi\nğ‘–,ğ‘— }, we flatten them and use a MLP to\nreduce the dimension:\nğ’—ğ‘–,ğ‘— = MLProi(Flatten(ğ’‡roi\nğ‘–,ğ‘— ))âˆˆ Rğ‘‘ğ‘£ . (3)\nWe stack {ğ’—ğ‘–,ğ‘—}ğ‘›\nğ‘–=1 into ğ‘½ğ‘— âˆˆRğ‘›Ã—ğ‘‘ğ‘£ , corresponding to the value\nmatrix of the cross-attention operation in the Transformer decoder.\nHowever, in contrast to the vanilla cross-attention where a single\nvalue matrix is used, our temporal-aware cross-attention 1) con-\nstructs separate value matrix ğ‘½ğ‘— for each query ğ‘ğ‘—, which considers\ndifferent temporal information of tracklet-query pairs, and 2) de-\nsigns role-specific attention matrices for subject (ğ‘¨ğ‘ ) and object\n(ğ‘¨ğ‘œ), respectively. Specifically,\nğ‘¨ğ‘Ÿ = 1/\nâˆš\nğ‘‘(eğ‘¸ğ‘¾ğ‘„\nğ‘Ÿ )(eğ‘¯ğ‘¾ ğ¾\nğ‘Ÿ )T, (4)\nwhere ğ‘Ÿ âˆˆ{ğ‘ ,ğ‘œ}represents semantic roles, eğ‘¸ is the query matrix\nafter self-attention, eğ‘¯ is the output of the encoder (served as the\nkey matrix), and ğ‘¾ğ‘„\nğ‘Ÿ , ğ‘¾ğ¾ğ‘Ÿ are learnable weights. In our setting,\nwe assume each predicate query can only link to one tracklet in\neach role and each tracklet-predicate pair has one type of roles at\nmost. Thus, we stack ğ‘¨s and ğ‘¨o as ğ‘¨ âˆˆR2Ã—ğ‘šÃ—ğ‘›, and normalize it\nthrough:\neğ‘¨[ğ‘Ÿ,ğ‘—,ğ‘– ]= exp(ğ‘¨[ğ‘Ÿ,ğ‘—,ğ‘– ])Ãğ‘›\nğ‘–â€²=1 exp(ğ‘¨[ğ‘Ÿ,ğ‘—,ğ‘– â€²])Ã— exp(ğ‘¨[ğ‘Ÿ,ğ‘—,ğ‘– ])\nÃ2\nğ‘Ÿâ€²=1 exp(ğ‘¨[ğ‘Ÿâ€²,ğ‘—,ğ‘– ])\n. (5)\nThereafter, our temporal-aware cross-attention is performed as\neğ’’ğ‘— = Ã2\nğ‘Ÿ=1ğ¹ğ‘Ÿ(eğ‘¨[ğ‘Ÿ,ğ‘—, :]ğ‘½ğ‘—)âˆˆ R1Ã—ğ‘‘ğ‘ , ğ‘—= 1,...,ğ‘š, (6)\nwhere eğ‘¨[ğ‘Ÿ,ğ‘—, :]âˆˆ R1Ã—ğ‘› and ğ¹ğ‘Ÿ : Rğ‘‘ğ‘£ â†’Rğ‘‘ğ‘ is a role-specific MLP\nthat introduces role-wise distinction into eğ’’ğ‘—.\nFinally, we stack {eğ’’ğ‘—}ğ‘š\nğ‘—=1 as the enhanced query embedding ma-\ntrix of shape ğ‘šÃ—ğ‘‘ğ‘. The enhanced query embeddings are further\nutilized to regress the offset w.r.t temporal anchors through an\nextract MLP, which will be used in the next decoder layer. Other op-\nerations such as feed-forward network (FFN), residual connections\nand layer normalization [2] are also used in our temporal-aware\ndecoder as that in the standard Transformer decoder.\n3.3 Relation Prediction\nGiven the attention matrixeğ‘¨ âˆˆR2Ã—ğ‘šÃ—ğ‘›and the enhanced predicate\nqueries {eğ’’ğ‘—}ğ‘š\nğ‘—=1 output from the last decoder layer, we perform\nrelation prediction through two steps of linking prediction and\npredicate classification.\nFor linking prediction, we binarize eğ‘¨ to obtain the links of each\nrole (subject/object). Specifically, for each query in each channel (i.e.,\neach eğ‘¨[ğ‘Ÿ,ğ‘—, :]), the tracklet of the max attention score is selected.\nFor predicate classification, we use the enhanced query embed-\nding eğ’’ğ‘—, the subject/object classeme features, and the frequency\nbias (i.e., prior information) in the VidOR training set.\nSpecifically, we use GloVe embeddings [ 16] to represent the\nwords of object categories, and stack them as the embedding matrix\nğ‘¬ âˆˆR|Cobj |Ã—ğ‘‘ğ‘¤ , where ğ‘‘ğ‘¤ is the dimension of word embedding.\nThe classeme feature for each tracklet proposal is calculated as\nğ’‡ğ‘\nğ‘– = ğ‘¬Tğ’‘ğ‘– âˆˆ Rğ‘‘ğ‘¤ . Then, we concatenate the predicate query\nwith the classeme features of its corresponding subject-object pair,\ndenoted as eğ’’â€²\nğ‘— = [eğ’’ğ‘—;ğ’‡ğ‘\nğ‘–ğ‘—,ğ‘  ;ğ’‡ğ‘\nğ‘–ğ‘—,ğ‘œ ] âˆˆRğ‘‘ğ‘+2ğ‘‘ğ‘¤ , where ğ‘–ğ‘—,ğ‘ ,ğ‘–ğ‘—,ğ‘œ index\nthe subject and object tracklets for query ğ‘ğ‘—, respectively.\nFor frequency bias, we construct a dictionary ğ‘© to store the\nlog-probability of predicate category for a given category pair of\nsubject-object pair [29], where ğ‘© : Cobj Ã—Cobj â†’R|Crel |. Finally,\nthe predicate classification is performed as\nğ‘ğ‘—(ğ‘ğ‘)âˆ¼ softmax(MLPğ‘(eğ’’â€²\nğ‘—)+ğ‘©[ğ‘ğ‘–ğ‘—,ğ‘  ,ğ‘ğ‘–ğ‘—,ğ‘  ]), (7)\nwhere ğ‘ğ‘–ğ‘—,ğ‘  (ğ‘ğ‘–ğ‘—,ğ‘œ ) is the object category of ğ‘–ğ‘—,ğ‘ -th (ğ‘–ğ‘—,ğ‘œ-th) tracklet,\nand ğ‘ğ‘—(ğ‘ğ‘)is the probability of predicate category ğ‘ğ‘.\n3.4 Training Objectives\nBecause we fix all the tracklet proposals from the detection back-\nbone as the final predictions, we only consider the training loss\nbetween predicates and their ground-truths. Let us denote by Ë†ğ‘ =\n{Ë†ğ‘ğ‘—}ğ‘š\nğ‘—=1 the set of ğ‘š predicate predictions. Let ğ‘âˆ—be the ground-\ntruth predicate set of sizeğ‘špadded with âˆ…(background). We adopt\none-to-one label assignment by finding a bipartite matching be-\ntween Ë†ğ‘ and ğ‘âˆ—. Specifically, we search for a permutation of ğ‘š\nelements Ë†ğœ by optimizing the following cost:\nË†ğœ = arg min\nğœ\nÃğ‘š\nğ‘—=1Lmatch (ğ‘âˆ—\nğ‘—,Ë†ğ‘ğœ(ğ‘—)). (8)\nThis matching problem is computed efficiently with the Hungarian\nalgorithm [15], following prior work [4]. The matching cost con-\nsiders both predicate classification and linking prediction. Because\neach predicate is characterized by its category and two links, we de-\nnote ğ‘âˆ—\nğ‘— = (ğ‘ğ‘âˆ—\nğ‘— ,ğ’‚âˆ—\nğ‘—), where ğ‘ğ‘âˆ—\nğ‘— is the predicate category (which may\nbe âˆ…) and ğ’‚âˆ—\nğ‘— âˆˆ{0,1}2Ã—ğ‘› is the ğ‘—-th row of ğ‘¨âˆ—(binarized ground-\ntruth attention matrix) for two channels. Note that ğ’‚âˆ—\nğ‘—[ğ‘Ÿ,ğ‘–]= 0\nwhen the ğ‘–-th tracklet has no ground-truth to match (tracklet as-\nsignment is based on vIoU and the criterion is similar to that in\nFaster-RCNN [18]). For the predicted predicate with index ğœ(ğ‘—),\nwe define the linking prediction as Ë†ğ’‚ğœ(ğ‘—) âˆˆR2Ã—ğ‘›. With the above\nnotations, the matching cost is defined as\nLmatch (ğ‘âˆ—\nğ‘—,Ë†ğ‘ğœ(ğ‘—))= âˆ’1{ğ‘ğ‘âˆ—\nğ‘— â‰ âˆ…}ğœ†cls log ğ‘ğœ(ğ‘—)(ğ‘ğ‘âˆ—\nğ‘— ) (9)\n+1{ğ‘ğ‘âˆ—\nğ‘— â‰ âˆ…}ğœ†attLatt (ğ’‚âˆ—\nğ‘—, Ë†ğ’‚ğœ(ğ‘—)),\nwhere ğœ†att and ğœ†att are hyperparameters, and Latt is defined as a\nbinary-cross entropy (BCE) loss.\nAfter obtaining Ë†ğœ, the final predicate lossLconsists of two parts:\nthe matching loss between the matched <ğ‘âˆ—\nğ‘—,Ë†ğ‘Ë†ğœ(ğ‘—)> pairs, and the\nbackground classification loss for other predicate predictions, i.e.,\nL= Ãğ‘š\nğ‘—=1Lmatch\n\u0010\nğ‘âˆ—\nğ‘—,Ë†ğ‘ğœ(ğ‘—)\n\u0011\nâˆ’ğœ†cls\nÃ\nğ‘ğ‘âˆ—\nğ‘— =âˆ…log ğ‘ğœ(ğ‘—)(âˆ…). (10)\n4 EXPERIMENTS\n4.1 Datasets and Evaluation Metrics\nWe evaluated our method on the VidOR dataset [19]. It consists of\n10,000 videos collected from YFCC100M [24], which covers 80 object\ncategories and 50 predicate categories. We used official splits [19],\ni.e., 7,000 videos for training, 835 videos for validation (VidOR-val),\nand 2,165 videos for test (VidOR-test).\nWe use the official evaluation metrics [1, 20] of the VRU Chal-\nlenge, including Relation Detection (RelDet) and Relation Tagging\n(RelTag). Quantitative metrics includes Average Precision (mAP)\nand Recall@K (R@K, K=50,100) for RelDet, and Precision@K (P@K,\nK=1,5,10) for RelTag.\n4.2 Implementation Details\nTemporal Anchor Settings. The time slots of temporal anchors\n(as well as the time slot of tracklets) are normalized into (0, 1) with re-\nspect to the video length. Since we assign each query with a unique\ntemporal anchor, we have totalğ‘šanchors. Specifically, the ğ‘šan-\nchors are associated withğ‘šğ‘ different center points andğ‘šğ‘‘ different\ndurations, which are (1/ğ‘šğ‘,2/ğ‘šğ‘,..., 1) and (1/ğ‘šğ‘‘,2/ğ‘šğ‘‘,..., 1), re-\nspectively. We setğ‘šğ‘ =16, ğ‘šğ‘‘=12. Typically, the numberğ‘š= ğ‘šğ‘ğ‘šğ‘‘\nis set to be larger than the number of most of predicates in a video.\nParameter Settings. The frame-level appearance feature dimen-\nsion ğ‘‘ğ‘ = 1024. The dimension of word embdding ğ‘‘ğ‘¤ = 300. The\nhidden dimension ğ‘‘, ğ‘‘ğ‘ and ğ‘‘ğ‘£ were set to 512. The output lengths\nof pooling operations were set as ğ‘™ = 4, ğ‘™roi = 7. All the MLPs are\ntwo-layer fully-connected networks with ReLU and with hidden\ndimension of 512. All bounding boxe coordinates are normalized to\nthe range between (0, 1) with respect to video size. The number of\nencoder/decoder layers are set as ğ¿ğ‘’ = 6 and ğ¿ğ‘‘ = 4, respectively.\nThe loss factors were set as ğœ†cls = 1.0, and ğœ†adj = 30.0. For train-\ning, we trained our model by Adam [12] with total 50 epochs. The\nlearning rate was set to 5e-5 and the batch size was set to 4.\nInference Details. At inference time, we keep top-10 predictions\nfor each predicate [20]. The time slot of each relation triplet is calcu-\nlated as the intersection of the two corresponding tracklets. During\nexperiments, we found that several predicate queries with same cat-\negory are sometimes linked to the same tracklet pair, which causes\nrepeated predictions in the top-K candidates. Thus, we performs\na filtering operation on the final set of triplet predictions: For all\ntriplets with the same predicate category and tracklet pair, we only\nkeep the one with the highest score.\nTeam name Grade Detector Tracker Features RelDet RelTag\nmAP R@50 R@100 P@1 P@5 P@10\nRELAbuilder [32] 2nd-2019 VGG-16 + R-Det GTG + T-NMS I3D+RM+L 0.546 â€” â€” â€” 23.60 â€”\nMAGUS.Gamma [23] 1st-2019 FGFA Seq-NMS + KCF RM+L 6.31 â€” â€” â€” 42.10 â€”\nETRI_DGRC 2nd-2020 â€” â€” â€” 6.65 â€” â€” â€” â€” â€”\ncolab-BUAA [28] 1st-2020 S-101 + C-RCNN iSeq-NMS V+L+RM+Msk 11.74 10.02 12.69 71.36 56.30 44.59\nEgoJ 3rd-2021 â€” â€” â€” 5.93 â€” â€” â€” â€” â€”\nPlanck 2nd-2021 â€” â€” â€” 6.69 â€” â€” â€” â€” â€”\nOurs (Ens-5) 1st-2021 R-101 + MEGA deepSORT V+L 9.48 8.56 10.43 63.46 54.07 41.94\nTable 1: Performance (%) on VidOR-test (VRU Challenges) of SOTA methods. We list all of their detector/tracker/features if\navailable. I3D denotes the I3D [5] features of object tracklets. RM denotes the pair-wise relative motion features of subject-\nobject tracklet pairs. Msk is the location mask feature. V and L are visual and language features of tracklets, respectively.\nCategory mAP Category mAP Category mAP\nturtle 95.61 sheep/goat 55.53 faucet 26.87\nadult 84.51 kangaroo 53.26 scooter 17.75\nbird 81.81 bus/truck 51.49 cellphone 14.77\nmotorcycle 75.13 watercraft 47.62 cattle/cow 7.11\nbaby 66.98 vegetables 43.13 fruits 1.81\nsuitcase 61.61 oven 40.74 stop_sign 0.01\nTable 2: Frame-level object detection mAP (%) on VidOR-val.\nMethod mAP Position mAP\nMAGUS.Gamma [23] 8.82 16.64\ncolab-BUAA [28] 14.59 â€”\nOurs 12.48 23.71\nTable 3: Tracklet mAP (%) on VidOR-val.\n4.3 Component Analysis\nFrame-level Object Detection. Table 2 presents the detection\nmAP (IoU=0.5) on VidOR validation set of our trained MEGA in\nseveral categories. Common categories with more samples show\nbetter performance than rare categories with few samples. The\noverall mAP of all categories is 41.21%.\nObject Tracklet Detection. Our tracklet proposals on VidOR val-\nidation set achieve a tracklet mAP (vIoU=0.5) of 11.67% and an\nupper bound of 23.08%, as shown in Table 3, suppressing MA-\nGUS.Gamma [23]. The tracklets of colab-BUAA [28] have higher\nmAP than ours, due to their heavier detection backbone Cascade\nR-CNN (C-RCNN) [3] with ResNeSt101 (S-101) [30].\nRelation Prediction. We analyze the effects of different compo-\nnents (i.e., classeme features (Clsme) and predicate frequency bias\n(Bias)) on relation prediction, the results are shown in Table 4. It\nshows that the model with both Clsme and Bias achieves the high-\nest mAP and Tagging Precision, while the model with only Bias\nperforms slightly better on Recall in RelDet. We choose the model\nconsisting both Clsme and Bias as our final model.\nWe also consider model ensembling in Table 4, where Ens- ğ‘˜\nrepresents the results of ensemblingğ‘˜models. Ens-5 has the highest\nperformance in all metrics except a slightly lower mAP than Ens-3.\nSo we use Ens-5 as our final model in the VRU Challenge.\n4.4 Comparisons with State-of-the-Arts\nWe compare our model with other state-of-the-arts (SOTA) meth-\nods on VidOR test set (i.e., the VRU Challenges [11]), and the results\nModel Clsme Bias RelDet RelTag\nmAP R@50 R@100 P@5 P@10\nEns-1\n8.06 7.02 7.96 49.78 38.60\nâœ“ 8.36 7.25 8.18 50.73 39.22\nâœ“ âœ“ 8.67 7.10 8.14 51.49 39.38\nEns-2 âœ“ âœ“ 9.06 7.79 9.12 52.37 40.11\nEns-3 âœ“ âœ“ 9.36 8.14 9.68 52.56 40.97\nEns-5 âœ“ âœ“ 9.33 8.35 10.21 52.73 41.14\nTable 4: Component analysis of our model on VidOR-val.\nare shown in Table 1. We outperform all other teams, except for\ncolab-BUAA [28]. Though colab-BUAA performs slightly better\nthan our method, the detection backbone used in their method is\nC-RCNN with S-101, which is very heavy, compared to our MEGA\nwith ResNet-101 (R-101). Besides, their features contain realative\nmotivion feature of tracklet pairs (RM) and the location mask fea-\nture (Msk), which are also of high complexity.\nWe also analyze the detection backbones used in different meth-\nods in Table 1. Different detectors have a great influence on the final\nrelation results. In [32], the detector is RefineDet (R-Det) [31] with\nVGG-16 [21], and the tracking algorithm is greedy tracklet grner-\nation (GTG) with tracklet-NMS (T-NMS). This backbone returns\ntracklets with relatively poor quality. In contrast, better detectors\nand trackers (such as FGFA [33], Seq-NMS [8], improved Seq-NMS\n(iSeq-NMS) [28] and KCF [ 10]) provide higher quality tracklets\nand improve the relation detection performance, while introducing\nmore computational complexity.\n5 CONCLUSIONS\nIn this paper, we introduce a novel video visual relation detection\nmethod, which consists object tracklet detection and visual rela-\ntion detection. Specifically, we use MEGA [7] and deepSORT [27]\nto detect object tracklets, and we propose a tracklet-based visual\nTransformer for relation detection, in which a temporal-aware de-\ncoder is spatially designed. The experiment results demonstrate the\nsuperiority of our method, which outperforms other SOTA methods\nin the VRU Challenge of ACM Multimedia 2021.\nAcknowledgement This work was supported by the National Key\nResearch and Development Project of China (No. 2018AAA0101900),\nthe National Natural Science Foundation of China (U19B2043, 61976185),\nZhejiang Natural Science Foundation (LR19F020002), Zhejiang In-\nnovation Foundation(2019R52002), and the Fundamental Research\nFunds for the Central Universities.\nREFERENCES\n[1] 2021. Video Relation Understanding Grand Challenge in ACM Multimedia 2021 .\nhttps://videorelation.nextcenter.org/\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton. 2016. Layer normaliza-\ntion. arXiv (2016).\n[3] Zhaowei Cai and Nuno Vasconcelos. 2018. Cascade r-cnn: Delving into high\nquality object detection. In CVPR. 6154â€“6162.\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexan-\nder Kirillov, and Sergey Zagoruyko. 2020. End-to-end object detection with\ntransformers. In ECCV. 213â€“229.\n[5] Joao Carreira and Andrew Zisserman. 2017. Quo vadis, action recognition? a\nnew model and the kinetics dataset. In CVPR. 6299â€“6308.\n[6] Long Chen, Hanwang Zhang, Jun Xiao, Xiangnan He, Shiliang Pu, and Shih-\nFu Chang. 2019. Counterfactual critic multi-agent training for scene graph\ngeneration. In ICCV. 4613â€“4623.\n[7] Yihong Chen, Yue Cao, Han Hu, and Liwei Wang. 2020. Memory enhanced\nglobal-local aggregation for video object detection. In CVPR. 10337â€“10346.\n[8] Wei Han, Pooya Khorrami, Tom Le Paine, Prajit Ramachandran, Mohammad\nBabaeizadeh, Honghui Shi, Jianan Li, Shuicheng Yan, and Thomas S Huang. 2016.\nSeq-nms for video object detection. arXiv preprint arXiv:1602.08465 (2016).\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep residual\nlearning for image recognition. In CVPR. 770â€“778.\n[10] JoÃ£o F Henriques, Rui Caseiro, Pedro Martins, and Jorge Batista. 2014. High-speed\ntracking with kernelized correlation filters. IEEE transactions on pattern analysis\nand machine intelligence 37, 3 (2014), 583â€“596.\n[11] Wei Ji, Yicong Li, Meng Wei, Xindi Shang, Junbin Xiao, Tongwei Ren, and Tat-\nSeng Chua. 2021. VidVRD 2021: The Third Grand Challenge on Video Relation\nDetection. In ACM MM.\n[12] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv (2014).\n[13] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. Microsoft coco: Common\nobjects in context. In ECCV. 740â€“755.\n[14] Chenchen Liu, Yang Jin, Kehan Xu, Guoqiang Gong, and Yadong Mu. 2020. Beyond\nshort-term snippet: Video relation detection with spatio-temporal global context.\nIn CVPR. 10840â€“10849.\n[15] James Munkres. 1957. Algorithms for the assignment and transportation prob-\nlems. Journal of the society for industrial and applied mathematics 5, 1 (1957),\n32â€“38.\n[16] Jeffrey Pennington, Richard Socher, and Christopher D Manning. 2014. Glove:\nGlobal vectors for word representation. In EMNLP. 1532â€“1543.\n[17] Xufeng Qian, Yueting Zhuang, Yimeng Li, Shaoning Xiao, Shiliang Pu, and Jun\nXiao. 2019. Video relation detection with spatio-temporal graph. In ACM MM.\n84â€“93.\n[18] Shaoqing Ren, Kaiming He, Ross B Girshick, and Jian Sun. 2015. Faster R-CNN:\nTowards Real-Time Object Detection with Region Proposal Networks. InNeurIPS.\n[19] Xindi Shang, Donglin Di, Junbin Xiao, Yu Cao, Xun Yang, and Tat-Seng Chua.\n2019. Annotating objects and relations in user-generated videos. In ICMR. 279â€“\n287.\n[20] Xindi Shang, Tongwei Ren, Jingfan Guo, Hanwang Zhang, and Tat-Seng Chua.\n2017. Video visual relation detection. In ACM MM. 1300â€“1308.\n[21] Karen Simonyan and Andrew Zisserman. 2015. Very deep convolutional networks\nfor large-scale image recognition. In ICLR.\n[22] Zixuan Su, Xindi Shang, Jingjing Chen, Yu-Gang Jiang, Zhiyong Qiu, and Tat-\nSeng Chua. 2020. Video Relation Detection via Multiple Hypothesis Association.\nIn ACM MM. 3127â€“3135.\n[23] Xu Sun, Tongwei Ren, Yuan Zi, and Gangshan Wu. 2019. Video visual relation\ndetection via multi-modal feature fusion. In ACM MM. 2657â€“2661.\n[24] Bart Thomee, David A Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni,\nDouglas Poland, Damian Borth, and Li-Jia Li. 2016. YFCC100M: The new data in\nmultimedia research. Commun. ACM (2016), 64â€“73.\n[25] Yao-Hung Hubert Tsai, Santosh Divvala, Louis-Philippe Morency, Ruslan\nSalakhutdinov, and Ali Farhadi. 2019. Video relationship reasoning using gated\nspatio-temporal energy graph. In CVPR. 10424â€“10433.\n[26] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\nAidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is All\nyou Need. In NeuriIPS.\n[27] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. 2017. Simple online and realtime\ntracking with a deep association metric. In ICIP. 3645â€“3649.\n[28] Wentao Xie, Guanghui Ren, and Si Liu. 2020. Video relation detection with\nTrajectory-aware multi-modal features. In ACM MM. 4590â€“4594.\n[29] Rowan Zellers, Mark Yatskar, Sam Thomson, and Yejin Choi. 2018. Neural motifs:\nScene graph parsing with global context. In CVPR. 5831â€“5840.\n[30] Hang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Haibin Lin, Zhi Zhang,\nYue Sun, Tong He, Jonas Mueller, R Manmatha, et al. 2020. Resnest: Split-attention\nnetworks. arXiv preprint arXiv:2004.08955 (2020).\n[31] Shifeng Zhang, Longyin Wen, Xiao Bian, Zhen Lei, and Stan Z Li. 2018. Single-\nshot refinement neural network for object detection. In CVPR. 4203â€“4212.\n[32] Sipeng Zheng, Xiangyu Chen, Shizhe Chen, and Qin Jin. 2019. Relation under-\nstanding in videos. In ACM MM. 2662â€“2666.\n[33] Xizhou Zhu, Yujie Wang, Jifeng Dai, Lu Yuan, and Yichen Wei. 2017. Flow-guided\nfeature aggregation for video object detection. In ICCV. 408â€“417.",
  "topic": null,
  "concepts": [],
  "institutions": [
    {
      "id": "https://openalex.org/I76130692",
      "name": "Zhejiang University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I78577930",
      "name": "Columbia University",
      "country": "US"
    }
  ]
}