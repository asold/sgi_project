{
  "title": "Toddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents",
  "url": "https://openalex.org/W3207232687",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2267733568",
      "name": "Junseok Park",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2304673070",
      "name": "Kwan-Young Park",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2030668863",
      "name": "Hyunseok Oh( ç‚« )",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2498126101",
      "name": "Ganghun Lee",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2111724566",
      "name": "Minsu Lee",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A2118868783",
      "name": "Young-Ki Lee",
      "affiliations": [
        "Seoul National University"
      ]
    },
    {
      "id": "https://openalex.org/A4227092556",
      "name": "Byoung-Tak Zhang",
      "affiliations": [
        "Seoul National University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2296073425",
    "https://openalex.org/W1983132991",
    "https://openalex.org/W2168249880",
    "https://openalex.org/W2963390466",
    "https://openalex.org/W3108332675",
    "https://openalex.org/W2124219775",
    "https://openalex.org/W2137304866",
    "https://openalex.org/W2151834591",
    "https://openalex.org/W2128966898",
    "https://openalex.org/W2044568582",
    "https://openalex.org/W2575032143",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2058648967",
    "https://openalex.org/W1924762813",
    "https://openalex.org/W3009928773",
    "https://openalex.org/W2005874308",
    "https://openalex.org/W565371345",
    "https://openalex.org/W3024124733",
    "https://openalex.org/W2167574352",
    "https://openalex.org/W2395579298",
    "https://openalex.org/W2953127211",
    "https://openalex.org/W2739759330",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W2890707244",
    "https://openalex.org/W3032966519",
    "https://openalex.org/W2963516811",
    "https://openalex.org/W3124468124",
    "https://openalex.org/W2080888202",
    "https://openalex.org/W2929928372",
    "https://openalex.org/W3123742938",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W4256173203",
    "https://openalex.org/W2142565826",
    "https://openalex.org/W4293566037",
    "https://openalex.org/W2234013530",
    "https://openalex.org/W3014232838",
    "https://openalex.org/W1777239053",
    "https://openalex.org/W4230121806",
    "https://openalex.org/W623591911",
    "https://openalex.org/W2911040189",
    "https://openalex.org/W2899363284",
    "https://openalex.org/W3105390480",
    "https://openalex.org/W2341914303",
    "https://openalex.org/W2157904933",
    "https://openalex.org/W2745868649",
    "https://openalex.org/W2781726626",
    "https://openalex.org/W2123372395",
    "https://openalex.org/W4307979480",
    "https://openalex.org/W1998330995",
    "https://openalex.org/W2577918148",
    "https://openalex.org/W2103561211",
    "https://openalex.org/W41554520",
    "https://openalex.org/W4300994309",
    "https://openalex.org/W4234552385",
    "https://openalex.org/W3096437212",
    "https://openalex.org/W3037804676",
    "https://openalex.org/W2528287124",
    "https://openalex.org/W2174401281",
    "https://openalex.org/W2145482038",
    "https://openalex.org/W4307863552",
    "https://openalex.org/W4285719527"
  ],
  "abstract": "Critical periods are phases during which a toddler's brain develops in\\nspurts. To promote children's cognitive development, proper guidance is\\ncritical in this stage. However, it is not clear whether such a critical period\\nalso exists for the training of AI agents. Similar to human toddlers,\\nwell-timed guidance and multimodal interactions might significantly enhance the\\ntraining efficiency of AI agents as well. To validate this hypothesis, we adapt\\nthis notion of critical periods to learning in AI agents and investigate the\\ncritical period in the virtual environment for AI agents. We formalize the\\ncritical period and Toddler-guidance learning in the reinforcement learning\\n(RL) framework. Then, we built up a toddler-like environment with VECA toolkit\\nto mimic human toddlers' learning characteristics. We study three discrete\\nlevels of mutual interaction: weak-mentor guidance (sparse reward), moderate\\nmentor guidance (helper-reward), and mentor demonstration (behavioral cloning).\\nWe also introduce the EAVE dataset consisting of 30,000 real-world images to\\nfully reflect the toddler's viewpoint. We evaluate the impact of critical\\nperiods on AI agents from two perspectives: how and when they are guided best\\nin both uni- and multimodal learning. Our experimental results show that both\\nuni- and multimodal agents with moderate mentor guidance and critical period on\\n1 million and 2 million training steps show a noticeable improvement. We\\nvalidate these results with transfer learning on the EAVE dataset and find the\\nperformance advancement on the same critical period and the guidance.\\n",
  "full_text": "Toddler-Guidance Learning: Impacts of Critical Period on\nMultimodal AI Agents\nJunseok Park\nSeoul National University\nSeoul, Republic of Korea\nKwanyoung Park\nSeoul National University\nSeoul, Republic of Korea\nHyunseok Oh\nSeoul National University\nSeoul, Republic of Korea\nGanghun Lee\nSeoul National University\nSeoul, Republic of Korea\nMinsu Lee\nAIIS, Seoul National University\nSeoul, Republic of Korea\nYoungki Lee\nSeoul National University\nSeoul, Republic of Korea\nByoung-Tak Zhang\nAIIS, Seoul National University\nSeoul, Republic of Korea\nABSTRACT\nCritical periods are phases during which a toddlerâ€™s brain develops\nin spurts. To promote childrenâ€™s cognitive development, proper\nguidance is critical in this stage. However, it is not clear whether\nsuch a critical period also exists for the training of AI agents. Simi-\nlar to human toddlers, well-timed guidance and multimodal inter-\nactions might significantly enhance the training efficiency of AI\nagents as well. To validate this hypothesis, we adapt this notion of\ncritical periods to learning in AI agents and investigate the critical\nperiod in the virtual environment for AI agents. We formalize the\ncritical period and Toddler-guidance learning in the reinforcement\nlearning (RL) framework. Then, we built up a toddler-like envi-\nronment with VECA toolkit to mimic human toddlersâ€™ learning\ncharacteristics. We study three discrete levels of mutual interaction:\nweak-mentor guidance (sparse reward), moderate mentor guidance\n(helper-reward), and mentor demonstration (behavioral cloning).\nWe also introduce the EAVE dataset consisting of 30,000 real-world\nimages to fully reflect the toddlerâ€™s viewpoint. We evaluate the\nimpact of critical periods on AI agents from two perspectives: how\nand when they are guided best in both uni- and multimodal learn-\ning. Our experimental results show that both uni- and multimodal\nagents with moderate mentor guidance and critical period on 1\nmillion and 2 million training steps show a noticeable improve-\nment. We validate these results with transfer learning on the EAVE\ndataset and find the performance advancement on the same critical\nperiod and the guidance.\nCCS CONCEPTS\nâ€¢ Computing methodologies â†’Generative and developmen-\ntal approaches; Sequential decision making ; Transfer learn-\ning; Neural networks.\nPermission to make digital or hard copies of all or part of this work for personal or\nclassroom use is granted without fee provided that copies are not made or distributed\nfor profit or commercial advantage and that copies bear this notice and the full citation\non the first page. Copyrights for components of this work owned by others than ACM\nmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,\nto post on servers or to redistribute to lists, requires prior specific permission and/or a\nfee. Request permissions from permissions@acm.org.\nICMI â€™21, October 18â€“22, 2021, MontrÃ©al, QC, Canada\nÂ© 2021 Association for Computing Machinery.\nACM ISBN 978-1-4503-8481-0/21/10. . . $15.00\nhttps://doi.org/10.1145/3462244.3479932\nFigure 1: Overall approach to testing the critical period. We\nfirst trained the agent for 1/2/3/4 million frames only with\nsparse rewards. We then continued training the agent with\nguidance (helper-reward, behavior cloning) or without guid-\nance (sparse reward) for an additional 2 million frames.\nKEYWORDS\nReinforcement Learning; Toddler Object Learning; Virtual Environ-\nment for Cognitive Agents; Guidance\nACM Reference Format:\nJunseok Park, Kwanyoung Park, Hyunseok Oh, Ganghun Lee, Minsu Lee,\nYoungki Lee, and Byoung-Tak Zhang. 2021. Toddler-Guidance Learning:\nImpacts of Critical Period on Multimodal AI Agents. In Proceedings of the\n2021 International Conference on Multimodal Interaction (ICMI â€™21), October\n18â€“22, 2021, MontrÃ©al, QC, Canada. ACM, New York, NY, USA, 9 pages.\nhttps://doi.org/10.1145/3462244.3479932\n1 INTRODUCTION\nMentor guidance is critical in the early stages of cognitive develop-\nment. Learning in the child stage significantly influences learning-\nto-learn capabilities like goal-setting and self-control [56]. For this\nreason, parent-child interaction is crucial in the overall cognitive\narXiv:2201.04990v1  [cs.LG]  12 Jan 2022\nICMI â€™21, October 18â€“22, 2021, MontrÃ©al, QC, Canada Junseok Park, Kwanyoung Park, Hyunseok Oh, Ganghun Lee, Minsu Lee, Youngki Lee, and Byoung-Tak Zhang\ndevelopment process, including linguistics and object understand-\ning [28]. Adequate supervision in the early stages of learning is\nalso important for machine learning (ML), as the sample efficiency\nof supervised learning is in general much higher than that of un-\nsupervised approaches [26]. Prior works have investigated diverse\nforms of guided learning on ML agents such as distillation [2], im-\nitation learning, weakly-supervised RL [ 23]. However, there has\nbeen limited cross-domain research to find an optimal guidance\nstrategy for a given task.\nIn this work, we are inspired by the concept of critical periods\nin the infant-toddler learning, a specific period in which the acqui-\nsition of language and visual/auditory processing capabilities are\nhighly accelerated [10, 16, 21, 22, 34, 40, 41], and we explore the\nquestion, \"does such a critical period also exist for machine learning\n(ML) agents?\". In particular, we aim to identify the optimal guidance\nduring training on a certain task, and study the correspondence\nbetween the optimal duration of guidance in ML algorithms and the\ncritical period in human learning. This study is significant in that\nthe optimal guidance strategy for a task can significantly enhance\nthe training efficiency of the ML model.\nTo this end, we define and empirically study the optimal initial\ntime of mentor guidance as the counterpart of the critical period\nin AI agents. To do this we formulate a reward structure that is\nvariable through the training iterations, in terms of the Partially\nObservable Markov Decision Process(POMDP) in RL framework.\nWe then formalize mentor guidance as a policy-invariant mutable\nreward structure on a given task. Finally, we formally define the\noptimal initial time of mentor guidance, and study it as the AI\nagentâ€™s equivalent to the critical period seen in toddlers.\nOur key finding from experiments is that reinforcement learn-\ning agents indeed have a critical period much like toddlers in the\nmoderate guidance learning setting. To demonstrate this, we have\nimplemented guidance inside the VECA [29] virtual environment in\na multimodal agent RL framework to model mentor guidance. First,\nwe have modeled learning-through-play of toddlers with an object\nnavigation RL task using VECA. Second, we introduce the EAVE\ndataset, a real-world image and audio dataset accurately reflecting\nthe toddlerâ€™s point of view. For our experiments, we have imposed\nthree levels of guidance as seen in Fig 2: weak-mentor guidance\n(sparse reward), moderate mentor guidance (helper-reward), and\nmentor demonstration (behavioral cloning) and we adjust these\nguidances and their duration to find the best-performing setting.\nOur studies on unimodal and multi-modal RL agents show that the\ncritical period has appeared only when the toddler agent has mod-\nerate guidance while learning. Moreover, we validate our results\nusing the EAVE dataset via transfer learning. Results confirm that\nthe models trained on their appropriate critical period (1M&2M\niterations) noticeably outperform the models trained after (3M&4M\niterations) their critical period.\nTo summarize, this paperâ€™s contributions are threefold:\nâ€¢We use a policy-invariant mutable reward structure to for-\nmulate the human toddlerâ€™s critical period in an ML model,\nespecially the multimodal humanoid RL agent we developed.\nâ€¢We construct a multimodal Navigation RL task and a real-\nworld EAVE dataset that can effectively demonstrate the\ncritical period in a multimodal RL agent.\nFigure 2: Three types of guidance. (a) Weak-Mentor Guid-\nance (green) and Moderate-Mentor Guidance (blue) use a re-\nward bonus method based on RL. (b) Mentor demonstration\n(red) use behavioral cloning (Imitation Learning)\nâ€¢We empirically study the critical period in a unimodal and\nmultimodal RL task with four different periods: 1/2/3/4 mil-\nlion training iterations. Agent trained in a critical period\noutperforms others significantly with proper guidance. Fur-\nthermore, the performance advantage transfers well to the\nreal-world downstream task.\n2 RELATED WORKS\nCurriculum Learning [4, 9, 31, 37, 44, 50] is an easy-to-hard train-\ning strategy in ML that gradually increases the complexity of the\ndata samples used during the training process [43]. Humans learn\nthe easy and preliminary concepts sooner and the difficult and ad-\nvanced concepts later. Moreover, humans learn much better when\nthe examples are not randomly presented but are organized in a\nmeaningful order [13, 43]. Curriculum learning applies a similar\nstrategy for ML model training, and achieves two major advantages:\nfaster convergence and better performance. There is a number of\ndomains including face recognition [24], object segmentation [55]\nand reinforcement learning [46] in which curriculum learning has\nbeen successfully applied. However, using curriculum learning does\nnot always benefit performance. It requires careful design of the\ndifficulty hierarchy before training. For instance, the diversity of the\ndata samples may be restricted if the task difficulty measure prefers\nchoosing easy examples only from a small subset of classes [42, 43].\nAn approach diametrical to easy-to-hard curriculum learning, that\nis emphasizing harder examples as in hard example mining [18, 39]\nor anti-curriculum learning [7, 32], can achieve improved results\nin some cases.\nML researchers have recently taken interest in ideas from child\nlearning further technical advances [ 3, 36, 49, 51, 52]. Learning\nproperly in a child stage is crucial for learning-to-learn capabilities\nlike goal-setting and self-control [48, 56]. Life experiences and prior\nknowledge learned in oneâ€™s childhood are known to influence adult\nlearning as well. The latest works in deep learning like visual object\nlearning [3] are inspired by the way children learn. Achille et al. [1]\nis the first work to show that deep artificial neural networks also\nexhibit critical periods like humans, a decisive time window for the\npost-natal developmental stage. In deep neural network training, a\nrapid growth in information is followed by a reduction of informa-\ntion from an analysis with Fisher Information. Unfortunately, their\nanalysis is limited to supervised learning in convolution neural\nToddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents ICMI â€™21, October 18â€“22, 2021, MontrÃ©al, QC, Canada\nnetworks (CNNs), and not executed on either RL or multimodal\nsetup. To the best of our knowledge, our work is the first attempt\nto apply the toddlerâ€™s developmental characteristic of humans to\neffective RL training. We aim to understand how the functionality\nand learning-to-learn capabilities are nurtured through appropriate\nsupervision.\nSeveral works in developmental psychology note that more guid-\nance does not mean better human development. A general under-\nstanding of objects develops in an early stage of toddler learn-\ning with weak supervision as well as through interactions like\nmouthing, chewing, and rotating [14, 33]. Furthermore, teaching\nformal subjects too early for a child is counterproductive in the\nlong run [45]. Inspired by human toddler learning, we conjecture\nthat there is also a form of optimal supervision and an optimal\ntime period for RL training of intelligent machines. We empirically\nsearch for a type of guidance that accelerates the training, but does\nnot change the optimal policy of the environment.\n3 METHOD\nReinforcement Learning. Reinforcement learning (RL) is a frame-\nwork for experience-driven autonomous learning [46] in which an\nagent interacts with the environment to learn optimal behaviour,\nimproving over time through trial and error [2]. Upon observing\nthe consequent state, an agent learns to alter its policy in response\nto received rewards. The environment in RL is typically assumed\nto be a Markov Decision Process (MDP). MDP is memoryless; the\nconditional probability of the next state and reward are stochasti-\ncally determined only from the current state and action, and do not\ndepend on the past [38].\n3.1 Guidance in a POMDP\nIn social psychology, a social behavior or social action is a behav-\nior that affects the other person and provokes a response [17, 47].\nA general social interaction sequence is defined as a sequence of\na person acting toward another (under the expectation), and the\nother acting towards the person in response (under the interpre-\ntation) [11]. Guided learning, or mentor guidance , can be defined\nas a process in which learners achieve their learning goal with the\nsupervision of a more experienced mentor [5].\nInspired by these psychological discoveries, we formalize the\nnotion of Guidance in the context of Partially Observable Markov\nDecision Process (POMDP). We find it analogous of a person as an\nagent, and an effect of the behavior as a change in the observation.\nPartially-observable Markov decision processes (POMDP) extend\nRL frameworks so that an agent observes only part of the world\nstate [12]. A POMDP is a 7-tuple âŸ¨ğ‘†,ğ´,ğ‘‡,ğ‘…, Î©,ğ‘‚,ğ›¾ âŸ©consisting of:\nğ‘†, a set of possible states; ğ´, a set of actions; ğ‘‡, a set of transition\nprobabilities; ğ‘…, a reward function ğ‘…(ğ‘ ,ğ‘)for a state ğ‘  and action ğ‘;\nÎ©, a set of possible observations,ğ‘‚, a set of observation probabilities\nğ‘œ(ğ‘ ,ğ‘)= ğ‘ƒğ‘Ÿ(ğ‘œ|ğ‘,ğ‘ )for given state ğ‘  and action ğ‘; and ğ›¾ a discount\nfactor. We extend this to include a no-op action (do nothing) in the\naction space as ğ´â€²= ğ´Ã{ğ‘ğ‘›ğ‘œğ‘}.\n3.1.1 Mutable Reward Structure. Mutable reward structure is a\nreward function that is variable under number of iterations. Since\nMDP is memoryless, the state should incorporate the number of\ntraining iteration as a reference. Suppose the state consists of (ğ‘ ,ğ‘¡)\nwhere ğ‘¡ âˆˆN is the training iteration. The reward structure Ëœğ‘… is\nmutable if such state ğ‘  âˆˆğ‘†, action ğ‘ âˆˆğ´exists.\nËœğ‘…((ğ‘ ,ğ‘¡),ğ‘)â‰  Ëœğ‘…((ğ‘ ,ğ‘¡ +1),ğ‘) (1)\nThe mutable reward structure is dependent on the past, so the prop-\nerties under the MDP do not hold anymore. However, the policy\nis invariant [ 27] if two MDPs become graph-isomorphic after a\npositive linear transformation ğ‘“ : R â†’R is applied between corre-\nsponding weights. Policy invariance also holds when the reward\nstructure is a potential-based shaping function. Reward structure\nmorphing to a policy-invariant MDP preserves the optimal policy,\nand the value of the current policy. We further define such reward\nfunction as a policy-invariant mutable reward structure.\n3.1.2 Mentor Guidance. Mentor guidance can be interpreted as a\npolicy-invariant mutable reward structure that efficiently trains\nthe learner (mentee). Parent-child interaction, one of the major\nmentor guidance in human, is a special form of interaction which\nestablishes a strong bond between the parent and the child and\ngreatly facilitates the cognitive development of a child [ 28]. The\nparent agent observes the childâ€™s state and has interactions with\nthe child to optimize the childâ€™s ability on various tasks. It shows\nthat continuous care and guidance of mentor can hugely facillitate\nthe menteeâ€™s performance. Motivated by it, we investigate a specific\ncase of policy-invariant mutable reward structure, e.g. helper re-\nward, aiming to optimize the learnerâ€™s performance. We also study\nthe behavior cloning, a mutable reward structure that the policy\ninvariance is not known. The guidance ğºğ‘‡ on a task ğ‘‡ is defined as\na reward structure ğºğ‘‡ = Ëœğ‘…ğ‘‡(ğ‘ ,ğ‘)where ğ‘  âˆˆğ‘†, ğ‘ âˆˆğ´with agents\nperformance metric as ğ½ğ‘‡.\n3.2 Critical Period\nCritical periods are maturational stages in the lifespan of humans\nthat act as decisive period of learning in a particular learning do-\nmain as human grow up [16]. The critical period for the develop-\nment of the human visual system is considered to be between three\nand eight months [34]. Other critical periods have also been identi-\nfied for the development of hearing and the vestibular system [6, 21].\nBesides, critical periods for language acquisition indicate that the\nfirst few years of life constitute the time during which language\ndevelops readily, and after which language acquisition is much\nmore difficult. [40] To sum up, critical period stands for the effec-\ntive initiation time for supervising the cognitive development after\nbirth. We can see the correspondence between the critical period\nof a toddler and the morphing of a reward structure. In this work,\nwe seek the critical period of an ML agent. We empirically search\nwhether the initial morphing period of best guidance is finite, and\ncan be reached in a reasonable number of training iterations.\n3.3 Optimal Guidance as a Critical Period\n3.3.1 Initial Time of Guidance. For human, the parentâ€™s guidance\ninitiates from the beginning of the childâ€™s life. For the machine\nintelligence however, the guidance should persist only for a certain\nperiod for a learner to solve the task and learn on their own. We\nmodel this characteristic to the RL setting. Given the guidance, the\nreward morphing starts from an initial time ğ‘¡ğº and continues only\nfor a certain amount of time. The guidance resets from this point on.\nICMI â€™21, October 18â€“22, 2021, MontrÃ©al, QC, Canada Junseok Park, Kwanyoung Park, Hyunseok Oh, Ganghun Lee, Minsu Lee, Youngki Lee, and Byoung-Tak Zhang\nObject Finding Query\n(One-Hot Encoding)\n(b) Transfer Learning\nImage Classification\nâ€œTeddy Bearâ€\nReal-World Observation\nLinear Regression\nInteraction\nFeature Maps\n3136\n256\n84\n84\n6\n20\n2032\n99\n64\n7\n64\n8\n8\n44\n3\n7\nCNNs\nMLP\nâ‹° â‹°\nâ‹°\n(a) Training Process\nLinear proj.\nMasking \nğœ‹ ğ‘ ğ‘ \nğ‘„(ğ‘ ,ğ‘)\n7\n3136 256\nCNNs\nMLP\nInteraction\nFeature Maps\nVECA Visual Observation\n(Binocular Eyesight)\n84\n84\n6\n20\n20\n32\n7\n64\n8 8\n4\n4 3\n7\n9\n9\n64\n3\n500 256\nMLP\nRight-Ear HRTF\nFlatten\nğ‘…âˆˆ500\nLeft-Ear HRTF\n250\n250\n1\n1\nğ¿ğ¶ğ‘Ÿğ‘–ğ‘¡ğ‘–ğ‘\nğ¿ğ‘ğ‘ğ‘¡ğ‘œğ‘Ÿ\nMLP\n512 512\nRight-Eye(RGB channel)\n512 256 5\nVECA  Audio \nObservation\n(3D spatialized audio)\n512\n1\nLeft-Eye(RGB channel)\nFigure 3: Architecture of the baby agent. (a) Its visual observation is encoded as a feature map from the CNN encoder, and\nthe audio observation is encoded with an MLP encoder. These feature maps are concatenated and linearly combined to make\nan interaction feature map. It is masked with a linearly projected object embedding and fed to an MLP, which outputs the\nQ-function ğ‘„(ğ‘ ,ğ‘)and movement policy ğœ‹(ğ‘|ğ‘ ). (b) We validate helper models in another relevant task via transfer learning\nWe denote the initial morphing time of the guidance as ğ‘¡ğº. So for a\nseries of learning episodes{ğ¸ğº\nğ‘– }with guidanceğºand their duration\n{ğ‘¡ğ‘–}, the time interval of guidance lies in[ğ‘¡ğº,ğ‘¡ğº+Ã\nğ‘–ğ‘¡ğ‘–]. This raises\nthe question: What is the optimal initial period of guidance for the\nlearnerâ€™s performance?\n3.3.2 Optimal Guidance. We can define an optimal guidance policy\non a set of guidance reward structures{ğºğ‘—}as the one which results\nin the best performance with their optimal initial morphing time.\nSuppose the ğ½ğ‘‡(ğœ‹ğœƒ)is the performance of policy parameter ğœƒ on\ntask ğ‘‡, and Pr(ğœƒ|ğº,ğ‘¡ğº)is a trained policy distribution under the\nguidance ğ‘… = ğº âˆˆ{ğºğ‘—}and its duration ğ‘¡ğº. The optimal guidance\ncan be defined as the guidance and its initial morphing period tuple\n(ğºâˆ—,ğ‘¡âˆ—\nğº)providing the best performance on a specific task ğ‘‡.\n(ğºâˆ—,ğ‘¡âˆ—\nğº)= arg max\nğº,ğ‘¡ğº\n\u0012\nmax\nğœƒâˆ¼ğ‘ƒğ‘Ÿ(ğœƒ|ğº,ğ‘¡ğº)\nEğœ‹ğœƒ\n\u0000ğ½ğ‘‡(ğœ‹ğœƒ)\u0001\u0013\n(2)\n3.4 Transfer Learning\nTransfer learning aims to help improve learning of some task in\nthe target domain using knowledge learned from the source do-\nmain [53]. Transfer learning is used for few-shot learning on new\ndatasets or in previously unseen domains both to reduce training\neffort and to transfer some of the knowledge and inductive biases\nacquired during training in the source domain. One recent work\nmodeled learning-through-play of a toddler by training a Deep\nNeural Network (DNN) model in a RL framework and transfer this\nmodel to a different domain to evaluate it in terms of visual object\nunderstanding [30]. They aim to acquire a general understanding\nof objects by exploring the environment and actively interacting\nwith the objects, as a toddler does.\nHyperparameter Candidate values Optimal value\nğ›¼ (entropy coefficient) {0.003, 0.01, 0.03} 0.01\nLearning rate {0.0001, 0.0003, 0.001} 0.0003\nğ›¾ (discount factor) {0.95, 0.99} 0.99\nTable 1: Tuned hyperparameters used in SAC.\n4 EXPERIMENTS AND RESULTS\n4.1 Implementation Details\nWe designed the architecture of the agent as seen in Fig. 3 (a) to\nlearn and store transferable knowledge. We used Soft Actor-Critic\n(SAC) [15] to train the agent. In particular, we used 8 workers and\nupdated the parameter per 256 steps with batch size 512 using the\nreplay buffer size of 20,000. Other important hyperparameters of\nSAC are tuned as shown in Table. 1.\nVisual observations of the agent are encoded using a Convolu-\ntional Neural Network (CNN) encoder and an Multi-Layer Percep-\ntron (MLP), resulting in interaction feature maps . These features\nare masked with a linearly embedded representation of the object\nand determine the action of the agent. Since the agentâ€™s movement\nonly depends on masked features, the agent must learn to represent\nabstract features of its observation corresponding to the target ob-\nject. Then the visual feature extractor of the agent is transferred to\nthe EAVE dataset and learned with the Adam [19] gradient descent\noptimizer. We trained the network on a server with Ubuntu 18.04\nLTS, Xeon Gold 5128 Scalable CPU and six RTX3090 GPUs.\n4.2 Interactive Virtual Environment\nWe implemented an interactive virtual environment supporting\ntoddler-like visual observations and active physical interaction\nwith objects, to train and evaluate guidance on different levels\nToddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents ICMI â€™21, October 18â€“22, 2021, MontrÃ©al, QC, Canada\nFigure 4: VECA engine characteristics. Unlike other existing\nRL environments, we trained the toddler AI agents in a set-\nting endowed with various human features and characteris-\ntics (e.g. binocular vision).\nand for different durations. We used VECA toolkit [29], a virtual\nenvironment generation toolkit for human-like agents, to build the\ntoddler-guidance learning environment. In particular, we leveraged\na toddler AI agent to be a humanoid avatar including several human\ncharacteristics: 1) binocular vision, 2) 3D spatialized audio, 3) mesh-\nbased tactiles, 4) joint-level physics, 5) objective interaction, and\na 6) realistic collider as seen in Fig. 4. The concept of the critical\nperiod was founded on human learning. Therefore, we believe that\na humanoid agent is able to more precisely learn from observations\nthan most of the existing RL environment agents, which donâ€™t\nreflect the human features for testing the critical period [8, 20, 35,\n54]. The agentâ€™s goal is to learn the ability to visually locate distant\nobjects, which are the same 10 objects as in the EAVE real-world\ndataset seen in Fig. 5 (b), while freely exploring and observing\nnearby objects as seen in Fig 6 (b) and (c). The environmentâ€™s\nreward structure is modified to implement the different levels and\ndurations of guidance. We aim to observe the influence of guidance\non establishing a profound visual understanding of an object during\nthe RL stage.\n4.3 EAVE Dataset Collection\nWe have collected an Egocentric Audio-Visual Exploration Dataset\nfor Unsupervised Alignment Learning (EAVE dataset) in order to\napproximate everyday images observed by toddlers. Most existing\nvisual image datasets [25] do not take into account how humans\nactually perceive the input, meaning their â€œfirst-personâ€ perspective,\nthus they cannot fully capture the features of real human-visual\ninput. In particular, the way toddlers perceive the world has two\nmain unique aspects: 1) in the input observed by toddlers objects\noccupy a relatively larger size compared to an adultâ€™s view of the\nscene (so the image has a much higher resolution); 2) the world is\nobserved by toddlers from a larger variety of different angles than by\nadults [3]. To complement the aforementioned characteristics of the\ntoddlerâ€™s point of view, we used 10 toys in red, green and blue (for\na total of 30 objects) as can be seen in Fig. 5 (a) and collected 30,000\nimages using a depth camera. Because toddlers do not typically\nsee only a single object in real-world environments, each image\nincludes three objects much like the â€œreal-world observationsâ€ from\nFig. 5 (a). We are attempting to collect the image perspectives by\nassuming the babyâ€™s point of view from a variety of angles Fig. 5 (c).\nFigure 5: (a) Example of visual images with bounding boxes\nand the 10 objects used in the EAVE dataset, (b) The same\n10 objects in the Virtual Environment (VECA), (c) The EAVE\nvisual dataset includes egocentric images from the toddlerâ€™s\nPOV at various angles.\nMoreover, each image has one main object which has the babyâ€™s\nattention. For each image, we label the bounding box information\nof the three objects and the main object such that it is possible to\ncrop each object individually as a single-object image. Note that\nthe EAVE dataset is only used for transfer learning experiments in\nthe Study 3.\nFigure 6: Various viewpoints of our 3D environment. (a)\nDeveloped 3D-toddler playroom Environment top view, (b)\nThird-person point of view, (c) First-person point of view of\na toddler agent while learning the objects.\n4.4 Study 1: Examining the impact of the level\nof interaction in unimodal learning\nThe purpose of study1 is to compare the performance of the agent\nin a visual (unimodal) environment trained with various start times\n(for a fixed duration of guidance) and mutual interaction in the\nform of guidance, and find if there exists a critical period for each\nmutual interaction. To observe the effect of mutual interaction, we\ndesigned the procedure of the experiment as shown in the left side\nof Fig. 3. We first trained the agent for 1/2/3/4 million frames only\nwith sparse rewards. For each pre-trained agent, we then continued\ntraining the agent with guidance (helper-reward, behavior cloning)\nor without guidance for 2 million frames as seen in Fig 1.\nICMI â€™21, October 18â€“22, 2021, MontrÃ©al, QC, Canada Junseok Park, Kwanyoung Park, Hyunseok Oh, Ganghun Lee, Minsu Lee, Youngki Lee, and Byoung-Tak Zhang\nFigure 7: Performance of the agents during the guidance. Vertical axis indicates the average reward while the gray dotted\nline indicates average reward of the ideal policy (0.069/0.028 and 0.077/0.031 for unimodal and multimodal environment\nwith/without helper reward). Horizontal axis indicates the number of frames that the guidance is provided. Agents guided\nwith helper-reward show a remarkable difference in the performance. Agents guided with behavior-cloning improves, but the\ndifference is negligible. Without guidance, agents fail to improve. For helper-reward in multi-modal environment, we used\ntwo seeds to ensure that the result is not by the randomness of training. For other experiments, we used single seed.\n4.4.1 Task Design. We designed an object-finding task to train the\nagent in the virtual environment. We used the VECA [29] toolkit to\nimplement and built up an interactive 3D environment that includes\nten objects (of the same categories as in the EAVE dataset) and a\nbaby agent. For each episode, two objects are colored randomly (in\nred, green, blue) and randomly placed with the agent in the square\nroom with length of 18 units, with the distance of at least 4 units\nfor each other as seen in Fig 6 (a). The agent receives the reward\n+1 for reaching the target object, and receives -1 for reaching the\nwrong object. We say that the agent reached the object if the agent\nis looking towards the object and the distance is less than 2.5 units.\nNote that the distance is necessary because the object, which is on\nthe floor, will disappear from the eyesight if it gets closer since the\nagent canâ€™t move its head vertically. The maximum length of each\nepisode is 256. The agent receives RGB 84x84 binocular vision and\nreceives the information about the target object as a 10-dimensional\none-hot vector. The agent can walk around the environment with\nmaximum speed of 0.33 units per frame, which is represented by\ntwo continuous action variables ğ‘ğ‘“ (within [0, 1], forward walking\nspeed) and ğ‘ğ‘Ÿ (within [-1, 1], direction of walking).\nPlease note that this task is not easy to solve with SAC without\nguidance. First, random (maximum-entropy) actions in this envi-\nronment are likely to lead the agent to head into the wall, which\nfills most of the replay buffer with unhelpful data. Moreover, the\nagent has to learn color- and perspective-independent features of\nthe object since the object is placed and colored randomly. Also, the\npresence of a wrong object requires the agent not only to navigate\nto the target object, but also to avoid the wrong object.\nThis reward function (3) encourages the agent to look towards\nthe object, visually recognize it, and move towards the object.\n4.4.2 Design of guidance. For moderate mentor guidance, we gave\nthe agent helper reward formulated as follows:\nğ‘Ÿ\nâ€²\n=\nï£±ï£´ï£´ï£´ ï£²\nï£´ï£´ï£´ï£³\nâˆ’0.03ğ‘ğ‘“ If target is out of eyesight\n0.05ğ‘ğ‘Ÿ +0.03ğ‘ğ‘“ If target is left of eyesight\nâˆ’0.05ğ‘ğ‘Ÿ +0.03ğ‘ğ‘“ If target is right of eyesight\n(3)\nFor mentor demonstration, we set the mentorâ€™s policy according\nto its visual information. In particular, if the target object is out of\neyesight then the agent turns right to find the object. If the target\nobject is in its eyesight then the agent walks forward and turns\nslightly left/right if the object is on the right/left side of eyesight.\n4.4.3 Results. For guidance from helper-rewards, we found that\nthere exists a noticeable difference in performance depending on\nthe time from which the guidance is provided as shown in Fig.\n7(1)(a). For convenience, we denote ğ‘›M-helper as the model that is\ntrained with helper reward guidance starting fromğ‘›million frames.\nIn detail, the 2M-helper had much higher average reward (0.03) than\nthe other agents (0.00/-0.01/-0.01 for 1M/3M/4M). This shows that\na critical regime indeed exists for guidance from helper-rewards in\nthe object-finding task. Note that for guidance from helper-rewards,\nhelper reward is included when calculating the reward plotted in\nFig. 7(1)(a) so the reward is negative on the early stage. Fig. 8(1)(a)\ncompares the trajectories of the agents, visually showing that the\npolicy of the 2M-helper performs better than the other agents.\nToddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents ICMI â€™21, October 18â€“22, 2021, MontrÃ©al, QC, Canada\nFigure 8: Visualization of the trajectory for each agentâ€™s policy in unimodal and multimodal environments. Blue dots represent\nstarting points of the agents. Green/red dots and circles represent the target/wrong object and the boundary for reaching it\nrespectively. Note that the agent has to get inside the boundary and also face towards the object to clear the task.\nHowever, we found that there was no remarkable difference\nbetween the performance for behavior-cloning. As shown in Fig.\n7(1)(b), the average rewards are similar (around 0.01) for all agents.\nThese agents learned how to navigate to the target object, but could\nnot learn how to avoid the wrong object as shown in Fig. 8(1)(b).\nWithout guidance, the agent was unable to learn to solve the\ntask properly as shown in Fig. 7(1)(c) and Fig. 8(1)(c). Compared to\nthe results with guidance, these experiments show that guidance\ndoes aid in the agentsâ€™ learning process.\n4.5 Study 2: Examining the impact of the level\nof interaction in multimodal learning\nThe purpose of study2 is to examine the effect of interaction in\na multimodal environment and observe how the modality of the\nagent affected its performance and the critical period. To do so,\nwe modified the object-finding task in Study 1 to an audio-visual\ntask. Specifically, each object has its unique sound and makes this\nsound during the whole episode. Since the audio observation of the\nagent is 3D-spatialized, the agent can recognize the direction of\neach sound source. Thus, we expect that this task would be easier\nthan Study 1, but still challenging since the agent has to learn the\nrelation between the object and the sound. Other detailed setups\nabout the learning process and guidance are identical to Study 1.\n4.5.1 Results. Similar to Study 1, we find that there exists a notice-\nable difference in performance for guidance from helper-rewards,\nas shown in Fig. 7(2)(a). This shows that a critical regime also exists\nfor guidance from helper-rewards in the multimodal object-finding\ntask. However, the critical regime differed from Study 1: the 1M-\nand 2M-helper had much higher average reward (0.046 and 0.050)\nthan 3M-(0.02), 4M-(0.02) agents. Fig. 8(2)(a) compares the trajecto-\nries of the agents, visually showing that the policy of the 1M- and\n2M-helper performs better than the other agents. We observed that\nthe agent majorly uses the audio to get the direction of the object,\nso it turns right/left even if the object is slightly left/right, which\nresults in wiggling behavior.\nThe agents guided with behavior-cloning had only a negligible\ndifference in performance as shown in Fig. 7(2)(b). These agents\nshowed behavior similar to the agents in Study 1: they learned to\nnavigate to the target object but didnâ€™t learn to avoid the wrong\nobject as shown in Fig. 8(2)(b). However, the average reward was\nlower than in the unimodal environment. We think that because\nthe target policy is only based on visual observation, so the audio\nobservation worked as noise during behavior cloning.\nWithout guidance, the agent couldnâ€™t solve the task properly as\nshown in Fig. 7(2)(c) and Fig. 8(2)(c). These results show that the\nmulti-modality of the agent didnâ€™t give rise to a new critical period\nfor guidance methods without critical regime, but affected the time\nof critical period of the guidance with a critical regime.\n4.6 Study 3: Transfer to a real-world dataset\nIn study1, we trained with three levels of guidance learning (Sparse-\nreward, helper-reward, and behavioral cloning) Fig. 2. This leads\nto three distinct sets of CNN and MLP weights from the trained\nmodels, and we can evaluate each of these sets of weights in another\nrelevant task via transfer learning as seen in Fig. 3 (b).\nWe previously observed that the 2M- and 1M-helper models\nshowed a noticeable difference in performance compared to the\nmodels with no critical period (3M-, 4M-helper) on both uni- and\nmultimodal AI agents. If such a disparity in performance is valid, we\nbelieve that the models pre-trained with helper-reward guidance\nshould show comparable results on real-world data input which\nbetter reflects the characteristics of toddlersâ€™ observations.\nTo verify the validity of our claims, we evaluated the transfer\nperformance of image encoders (consisting of the CNN and the\nMLP) from 1M-, 2M-, 3M-, and 4M-helper models. Specifically, we\nfixed the parameters of the image encoders after pre-training and\nconnected additional linear layers for image classification. Next,\nwe used the EAVE dataset images, which was collected in the real\nworld in accordance with the toddlerâ€™s viewpoint, as input (size\n84x84) with a total of 6 channels, consisting of the toddler agentâ€™s\nICMI â€™21, October 18â€“22, 2021, MontrÃ©al, QC, Canada Junseok Park, Kwanyoung Park, Hyunseok Oh, Ganghun Lee, Minsu Lee, Youngki Lee, and Byoung-Tak Zhang\nFigure 9: Transfer learning performance with the moderate\nguidance learning (helper-reward) using the EAVE dataset\non unimodal (1) and multimodal (2) agents.\nbinocular vision with RGB channels each to the right and left side.\nWe trained the model for 500 000 iterations with batch size 32.\nAs shown in Fig. 9, we found that the transferred 2M- ( Uni:64.6/\nMulti:65.8%) and 1M-helper ( Uni:64/Multi:64.9%) models with a crit-\nical period achieved a better performance in terms of accuracy than\nthe 3M- (Uni:63.3/multi:64.9%) and 4M-helper ( Uni:62.1/multi:64.2%)\nmodels without a critical period on the EAVE dataset. The multi-\nmodal agentâ€™s set of CNN and MLP weights has shown a much\nhigher average accuracy than the unimodal agent. Hence, we con-\nfirm that multimodal learning rather than unimodal can be helpful\nin efficiently learning the representation of objects during learning.\nFurthermore, similar to study1 (see Fig. 7 (1),(2)-(a)). we see a dis-\ntinct difference in moderate guidance learning performance with\na helper-reward and can accordingly rank the models in the same\norder where the models (2M,1M) with a critical period significantly\noutperform the ones without. Through this, we validate that the\nlearning performance of toddler AI agents indeed depends on the\ncritical period for the setting with moderate guidance learning.\n5 DISCUSSIONS\nOur paper has made three key contributions as a first step toward\ndiscovering the optimal guidance corresponding to critical periods\nin AI agents. First, to prove that RL-based humanoid AI agents\nalso have critical periods like a toddler has, we found for the first\ntime that AI agents also show specific critical periods (2M, 1M-\nhelper) within the only moderate guidance learning setting (see\nFig. 7 (1),(2)-(a)). We assume that 3M/4M frames are past the critical\nperiod, so the policy overfits more to the sparse reward, unable to\nreach the performance of the 2M/1M model. Behavioral Cloning\nmerely imitates the mentorâ€™s trajectory, unable to generalize beyond\nthe mentor-given policy. However, the helper reward is the optimal\nguidance among them.\nSecond, we collected the EAVE dataset, which contains images\nthat imitate a toddlerâ€™s point of view and its special characteristics\nand which, along with the developed virtual environment with\na humanoid agent, allowed us to verify the qualitative notion of\ncritical periods in a computational and cognitive way. Furthermore,\nwe specifically designed the agent as a humanoid agent, mimick-\ning the toddlerâ€™s viewpointâ€™s characteristics in VECA, so that a\nhumanoid agent can closely learn and more efficiently observe the\nfeatures of objects in the same way a toddler does. We believe that\nthose human-like learning features enable us to test and validate\nthe critical learning period more reliably.\nLastly, Our findings are especially significant for both Human-AI\ninteraction domain and Human-in-the-loop RL, possibly used to\ndetermine the best time for human intervention in the training of\nAI agents. We developed and designed both uni- and multimodal\nagents, and confirm that the multimodal agent achieves a consider-\nable improvement over every result of the unimodal agent, inspired\nby actual human cognitive development. The multimodal agent\nhas a higher average reward and accuracy on object-finding task (\nFig. 7 (2)-(a)) and transfer learning on the EAVE dataset (Fig. 9 (b)).\nWe validate that multimodal learning at 2M&1M accompanying the\ncritical period is more effective for learning useful representation\ncompared to the unimodal approach.\nIn the future, we aim to analyze the critical period based on a\nmore toddler-like RL algorithm deeper in the multi-task&meta-RL\ndomain or develop an algorithm that automatically finds the critical\nlearning period of the agent with additional verification studies.\nIn this work, however, we employed a conventional RL algorithm\nwith a light deep learning structure as a first step towards this goal.\nAlso, testing the generalizability of the critical period concept\nrequires further experiments. Since the concept is from humans,\nwe speculate that physically grounded humanoid agents will more\nlikely show critical periods here. We plan to study this in various\nenvironments including non-humanoid agent in the future.\n6 CONCLUSION\nToddlers usually have a distinctive increase in performance during\nthe critical period [34]. At this time, the increase in learning perfor-\nmance depends on the timing of mutual interaction. To investigate\na similar effect in learning in AI agents, we studied three levels\nof mutual interaction learning (forms of guidance) in the virtual\nenvironment based on our formulation of toddler guidance learning\nterms in the RL framework. We validated our observations of the\nmodelâ€™s critical period on real-world observations (EAVE dataset).\nCorresponding to the critical period in toddlers, we observed a\nsimilar increase in performance using optimal guidance during the\ntraining of an AI agent. Specifically, we found that only moder-\nate mutual interaction with a multimodal agent leads to a distinct\nincrease in performance within the critical period and further af-\nfects the period during which the AI agent achieves remarkable\nperformance compared to the unimodal agent.\nACKNOWLEDGMENTS\nThis work was partly supported by the IITP (2015-0-00310-SW.Star-\nLab/16%, 2017-0-01772-VTT/16%, 2018-0-00622-RMI/16%, 2019-0-\n01371-BabyMind/20%) grants and the NRF of Korea (2021R1A2C10-\n10970/16%) grant funded by the Korean government, and the CARAI\n(UD190031RD/16%) grant funded by the DAPA and ADD.\nToddler-Guidance Learning: Impacts of Critical Period on Multimodal AI Agents ICMI â€™21, October 18â€“22, 2021, MontrÃ©al, QC, Canada\nREFERENCES\n[1] Alessandro Achille, Matteo Rovere, and Stefano Soatto. 2018. Critical learning\nperiods in deep networks. InInternational Conference on Learning Representations .\n[2] Kai Arulkumaran, Marc Peter Deisenroth, Miles Brundage, and Anil Anthony\nBharath. 2017. A brief survey of deep reinforcement learning. arXiv preprint\narXiv:1708.05866 (2017).\n[3] Sven Bambach, David Crandall, Linda Smith, and Chen Yu. 2018. Toddler-inspired\nvisual object learning. InAdvances in neural information processing systems . 1201â€“\n1210.\n[4] Yoshua Bengio, J. Louradour, Ronan Collobert, and J. Weston. 2009. Curriculum\nlearning. In ICML â€™09 .\n[5] Stephen Billett. 2000. Guided learning at work. Journal of Workplace learning\n(2000).\n[6] Michael S Brainard and Eric I Knudsen. 1998. Sensitive periods for visual cal-\nibration of the auditory space map in the barn owl optic tectum. Journal of\nNeuroscience 18, 10 (1998), 3929â€“3942.\n[7] Stefan Braun, Daniel Neil, and Shih-Chii Liu. 2017. A curriculum learning method\nfor improved noise robustness in automatic speech recognition. 2017 25th Euro-\npean Signal Processing Conference (EUSIPCO) (2017), 548â€“552.\n[8] Changan Chen, Unnat Jain, Carl Schissler, Sebastia Vicenc Amengual Gari, Ziad\nAl-Halah, Vamsi Krishna Ithapu, Philip Robinson, and Kristen Grauman. 2020.\nSoundspaces: Audio-visual navigation in 3d environments. In Proceedings of the\nEuropean Conference on Computer Vision (ECCV) . Springer.\n[9] Xinlei Chen and Abhinav Gupta. 2015. Webly supervised learning of convolu-\ntional networks. In Proceedings of the IEEE International Conference on Computer\nVision. 1431â€“1439.\n[10] Barry R Chiswick and Paul W Miller. 2008. A test of the critical period hypothesis\nfor language learning. Journal of multilingual and multicultural development 29,\n1 (2008), 16â€“29.\n[11] John M Darley and Russell H Fazio. 1980. Expectancy confirmation processes\narising in the social interaction sequence. American Psychologist 35, 10 (1980),\n867.\n[12] Finale Doshi-Velez. 2009. The infinite partially observable Markov decision\nprocess. Advances in neural information processing systems 22 (2009), 477â€“485.\n[13] J. Elman. 1993. Learning and development in neural networks: the importance of\nstarting small. Cognition 48 (1993), 71â€“99.\n[14] Eleanor J Gibson. 1988. Exploratory behavior in the development of perceiving,\nacting, and the acquiring of knowledge. Annual review of psychology 39, 1 (1988),\n1â€“42.\n[15] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. Soft\nactor-critic: Off-policy maximum entropy deep reinforcement learning with a\nstochastic actor. In International Conference on Machine Learning . PMLR, 1861â€“\n1870.\n[16] Takao K Hensch. 2004. Critical period regulation. Annu. Rev. Neurosci. 27 (2004),\n549â€“579.\n[17] George C Homans. 1974. Social behavior: Its elementary forms. (1974).\n[18] Andrew Jesson, N. Guizard, Sina Hamidi Ghalehjegh, D. Goblot, F. Soudan, and\nNicolas Chapados. 2017. CASED: Curriculum Adaptive Sampling for Extreme\nData Imbalance. ArXiv abs/1807.10819 (2017).\n[19] Diederik P Kingma and Jimmy Ba. 2014. Adam: A method for stochastic opti-\nmization. arXiv preprint arXiv:1412.6980 (2014).\n[20] Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro\nHerrasti, Daniel Gordon, Yuke Zhu, Abhinav Gupta, and Ali Farhadi. 2017. Ai2-\nthor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474\n(2017).\n[21] A Kral. 2013. Auditory critical periods: a review from systemâ€™s perspective.\nNeuroscience 247 (2013), 117â€“133.\n[22] Stephen D Krashen. 1973. Lateralization, language learning, and the critical\nperiod: Some new evidence. Language learning 23, 1 (1973), 63â€“74.\n[23] Lisa Lee, Ben Eysenbach, Russ R Salakhutdinov, Shixiang Shane Gu, and Chelsea\nFinn. 2020. Weakly-Supervised Reinforcement Learning for Controllable Behavior.\nAdvances in Neural Information Processing Systems 33 (2020).\n[24] L. Lin, Keze Wang, Deyu Meng, W. Zuo, and Lei Zhang. 2018. Active Self-Paced\nLearning for Cost-Effective and Progressive Face Identification.IEEE Transactions\non Pattern Analysis and Machine Intelligence 40 (2018), 7â€“19.\n[25] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva\nRamanan, Piotr DollÃ¡r, and C Lawrence Zitnick. 2014. Microsoft coco: Common\nobjects in context. In European conference on computer vision . Springer, 740â€“755.\n[26] Tyler Tian Lu. 2009.Fundamental limitations of semi-supervised learning . Masterâ€™s\nthesis. University of Waterloo.\n[27] A. Ng, D. Harada, and Stuart J. Russell. 1999. Policy Invariance Under Reward\nTransformations: Theory and Application to Reward Shaping. In ICML.\n[28] Sheryl L Olson, John E Bates, and Kathryn Bayles. 1990. Early antecedents of\nchildhood impulsivity: The role of parent-child interaction, cognitive competence,\nand temperament. Journal of abnormal child psychology 18, 3 (1990), 317â€“334.\n[29] Kwanyoung Park, Jeong Heo, and Youngki Lee. 2020. VECA: A VR Toolkit for\nTraining and Testing Cognitive Agents. https://github.com/GGOSinon/VECA.\n[30] Kwanyoung Park, Junseok Park, Hyunseok Oh, Byoung-Tak Zhang, and Youngki\nLee. 2021. Learning task-agnostic representation via toddler-inspired learning.\narXiv:2101.11221 [cs.AI]\n[31] Anastasia Pentina, Viktoriia Sharmanska, and Christoph H Lampert. 2015. Cur-\nriculum learning of multiple tasks. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition . 5492â€“5500.\n[32] Te Pi, Xi Li, Zhongfei Zhang, Deyu Meng, Fei Wu, Jun Xiao, and Yueting Zhuang.\n2016. Self-Paced Boost Learning for Classification. In IJCAI.\n[33] Jean Piaget and Margaret Cook. 1952. The origins of intelligence in children . Vol. 8.\nInternational Universities Press New York.\n[34] Ann L Robson. 2002. Critical/sensitive periods. Child Develop-ment (2002),\n101â€“103.\n[35] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans,\nBhavana Jain, Julian Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, et al. 2019.\nHabitat: A platform for embodied ai research. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision . 9339â€“9347.\n[36] Roger C Schank. 1972. Conceptual dependency: A theory of natural language\nunderstanding. Cognitive psychology 3, 4 (1972), 552â€“631.\n[37] Yangyang Shi, Martha Larson, and Catholijn M Jonker. 2015. Recurrent neural\nnetwork language model adaptation with curriculum learning. Computer Speech\n& Language 33, 1 (2015), 136â€“154.\n[38] Yoav Shoham, Rob Powers, and Trond Grenager. 2003. Multi-agent reinforcement\nlearning: a critical survey. (2003).\n[39] Abhinav Shrivastava, A. Gupta, and Ross B. Girshick. 2016. Training Region-\nBased Object Detectors with Online Hard Example Mining. 2016 IEEE Conference\non Computer Vision and Pattern Recognition (CVPR) (2016), 761â€“769.\n[40] David Michael Singleton and Zsolt Lengyel. 1995. The age factor in second\nlanguage acquisition: A critical look at the critical period hypothesis . Multilingual\nMatters.\n[41] Catherine E Snow and Marian Hoefnagel-HÃ¶hle. 1978. The critical period for\nlanguage acquisition: Evidence from second language learning.Child development\n(1978), 1114â€“1128.\n[42] Petru Soviany, Claudiu Ardei, Radu Tudor Ionescu, and M. Leordeanu. 2020.\nImage Difficulty Curriculum for Generative Adversarial Networks (CuGAN).\n2020 IEEE Winter Conference on Applications of Computer Vision (WACV) (2020),\n3452â€“3461.\n[43] Petru Soviany, Radu Tudor Ionescu, Paolo Rota, and N. Sebe. 2021. Curriculum\nLearning: A Survey. ArXiv abs/2101.10382 (2021).\n[44] Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky. 2009. Baby Steps:\nHow â€œLess is Moreâ€ in unsupervised dependency parsing. (2009).\n[45] Sebastian P Suggate, Elizabeth A Schaughency, and Elaine Reese. 2013. Children\nlearning to read later catch up to children reading earlier. Early Childhood\nResearch Quarterly 28, 1 (2013), 33â€“48.\n[46] Richard S Sutton and Andrew G Barto. 2018. Reinforcement learning: An intro-\nduction. MIT press.\n[47] Piotr Sztompka. 2002. Socjologia. Analiza spoÅ‚eczeÅ„stwa, Znak, KrakÃ³w (2002),\n324.\n[48] Sebastian Thrun and Lorien Pratt. 2012. Learning to learn . Springer Science &\nBusiness Media.\n[49] Satoshi Tsutsui, Arjun Chandrasekaran, Md Alimoor Reza, David Crandall, and\nChen Yu. 2020. A Computational Model of Early Word Learning from the Infantâ€™s\nPoint of View. arXiv preprint arXiv:2006.02802 (2020).\n[50] Radu Tudor Ionescu, Bogdan Alexe, Marius Leordeanu, Marius Popescu, Dim P\nPapadopoulos, and Vittorio Ferrari. 2016. How hard can it be? Estimating the\ndifficulty of visual search in an image. In Proceedings of the IEEE Conference on\nComputer Vision and Pattern Recognition . 2157â€“2166.\n[51] Matteo Turchetta, Andrey Kolobov, Shital Shah, Andreas Krause, and Alekh\nAgarwal. 2020. Safe reinforcement learning via curriculum induction. arXiv\npreprint arXiv:2006.12136 (2020).\n[52] Alan M Turing. 2009. Computing machinery and intelligence. In Parsing the\nTuring Test. Springer, 23â€“65.\n[53] Karl Weiss, Taghi M Khoshgoftaar, and DingDing Wang. 2016. A survey of\ntransfer learning. Journal of Big data 3, 1 (2016), 9.\n[54] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio\nSavarese. 2018. Gibson env: Real-world perception for embodied agents. In\nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition .\n9068â€“9079.\n[55] Y. Zhang, P. David, and Boqing Gong. 2017. Curriculum Domain Adaptation for\nSemantic Segmentation of Urban Scenes. 2017 IEEE International Conference on\nComputer Vision (ICCV) (2017), 2039â€“2049.\n[56] Jennifer N Zosh, Emily J Hopkins, Hanne Jensen, Claire Liu, Dave Neale, Kathy\nHirsh-Pasek, S Lynneth Solis, and David Whitebread. 2017. Learning through\nplay: a review of the evidence .",
  "topic": "Toddler",
  "concepts": [
    {
      "name": "Toddler",
      "score": 0.931313693523407
    },
    {
      "name": "Period (music)",
      "score": 0.6193669438362122
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5792100429534912
    },
    {
      "name": "Computer science",
      "score": 0.5206764936447144
    },
    {
      "name": "Reinforcement learning",
      "score": 0.5188449621200562
    },
    {
      "name": "Psychology",
      "score": 0.3393602967262268
    },
    {
      "name": "Machine learning",
      "score": 0.3318818509578705
    },
    {
      "name": "Developmental psychology",
      "score": 0.24154409766197205
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Acoustics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I139264467",
      "name": "Seoul National University",
      "country": "KR"
    }
  ]
}