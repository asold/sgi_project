{
  "title": "Decoupling the Role of Data, Attention, and Losses in Multimodal Transformers",
  "url": "https://openalex.org/W3126442315",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A5020758501",
      "name": "Lisa Anne Hendricks",
      "affiliations": [
        "Google (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A5111981498",
      "name": "John Mellor",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5055106498",
      "name": "Rosalia Schneider",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5012826868",
      "name": "Jean-Baptiste Alayrac",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    },
    {
      "id": "https://openalex.org/A5052196188",
      "name": "Aida Nematzadeh",
      "affiliations": [
        "DeepMind (United Kingdom)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2914924671",
    "https://openalex.org/W2560730294",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3095670406",
    "https://openalex.org/W2796207103",
    "https://openalex.org/W2984008963",
    "https://openalex.org/W3035635319",
    "https://openalex.org/W3014611590",
    "https://openalex.org/W2745461083",
    "https://openalex.org/W2970862333",
    "https://openalex.org/W2888166343",
    "https://openalex.org/W3011555699",
    "https://openalex.org/W2949517790",
    "https://openalex.org/W3106784008",
    "https://openalex.org/W1889081078",
    "https://openalex.org/W2070753207",
    "https://openalex.org/W2963250244",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W1527575280",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W3034381157",
    "https://openalex.org/W1933349210",
    "https://openalex.org/W2886641317",
    "https://openalex.org/W1666447063",
    "https://openalex.org/W2963389687",
    "https://openalex.org/W2109586012",
    "https://openalex.org/W2981851019",
    "https://openalex.org/W3016672431",
    "https://openalex.org/W2964345214",
    "https://openalex.org/W1905882502",
    "https://openalex.org/W2463565445",
    "https://openalex.org/W2963496089",
    "https://openalex.org/W2185175083",
    "https://openalex.org/W21006490",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2963499204",
    "https://openalex.org/W2123024445"
  ],
  "abstract": "Recently multimodal transformer models have gained popularity because their performance on language and vision tasks suggest they learn rich visual-linguistic representations. Focusing on zero-shot image retrieval tasks, we study three important factors which can impact the quality of learned representations: pretraining data, the attention mechanism, and loss functions. By pretraining models on six datasets, we observe that dataset noise and language similarity to our downstream task are important indicators of model performance. Through architectural analysis, we learn that models with a multimodal attention mechanism can outperform deeper models with modality specific attention mechanisms. Finally, we show that successful contrastive losses used in the self-supervised learning literature do not yield similar performance gains when used in multimodal transformers",
  "full_text": "Decoupling the Role of Data, Attention, and Losses\nin Multimodal Transformers\nLisa Anne Hendricks John Mellor Rosalia Schneider\nJean-Baptiste Alayrac Aida Nematzadeh\nDeepMind\n{lmh, johnme, rgschneider, jalayrac, nematzadeh} @google.com\nAbstract\nRecently multimodal transformer models\nhave gained popularity because their perfor-\nmance on language and vision tasks sug-\ngest they learn rich visual-linguistic repre-\nsentations. Focusing on zero-shot image re-\ntrieval tasks, we study three important fac-\ntors which can impact the quality of learned\nrepresentations: pretraining data, the atten-\ntion mechanism, and loss functions. By pre-\ntraining models on six datasets, we observe\nthat dataset noise and language similarity\nto our downstream task are important indi-\ncators of model performance. Through ar-\nchitectural analysis, we learn that models\nwith a multimodal attention mechanism can\noutperform deeper models with modality-\nspeciﬁc attention mechanisms. Finally, we\nshow that successful contrastive losses used\nin the self-supervised learning literature do\nnot yield similar performance gains when\nused in multimodal transformers. 1\n1 Multimodal Pretraining\nSigniﬁcant progress in pretraining of natural lan-\nguage processing (NLP) models has been made\nthrough both architectural innovations (e.g., trans-\nformers; Vaswani et al., 2017) as well as a huge\nincrease in the size of pretraining data and the\nmodel ( e.g., Devlin et al., 2019; Brown et al.,\n2020). This success in language pretraining has\ninspired parallel multimodal vision–language ef-\nforts; in particular, multimodal image–language\ntransformers, pretrained on large noisy image–text\ndatasets, have achieved state-of-the-art results on a\nrange of downstream tasks such as image retrieval,\nvisual question answering, and visual reasoning\n(e.g., Lu et al., 2019; Chen et al., 2020; Tan and\nBansal, 2019; Li et al., 2020a,b).\nHowever, even though many variants of multi-\nmodal image–language transformer models have\n1pre-print of MIT Press Publication version\nbeen proposed recently, it is unclear how learned\nrepresentations are impacted by the large amounts\nof pretraining data, the transformer architecture\nand self-attention, or their speciﬁc losses. We ad-\ndress this gap, by ﬁrst establishing a baseline that\nis trained on the same pretraining data as multi-\nmodal transformers but with a different architec-\nture. We then perform an investigative analysis to\nbetter understand the extent to which these aspects\ncontribute to models’ performance.\nOur evaluation mainly focuses on zero-shot\ntasks where evaluation data is taken from a dataset\nunseen during pretraining. Measuring zero-shot\nperformance enables us to evaluate whether a pre-\ntrained model learns general representations. Pre-\nvious work in NLP has considered probing classi-\nﬁers to evaluate representations; however, this ap-\nproach can be misleading as the performance of\nprobing classiﬁers does not solely depend on the\nquality of representations (e.g., Hewitt and Liang,\n2019; V oita and Titov, 2020). Similarly, evalua-\ntion after ﬁne-tuning is a less direct measure of\nstrength of representations since performance on\nthese tasks is highly dependent on the ﬁne-tuning\nexperimental set-up and the size of ﬁne-tuning\ndata (Yogatama et al., 2019).\nWe ﬁrst study the importance of different prop-\nerties of multimodal datasets such as their size\nand their noise level ( i.e., how closely the lan-\nguage describes a given image’s content). Recent\nwork has introduced image–text datasets with dif-\nferent qualities, for example, noisy but very large\nones (Sharma et al., 2018) as well as carefully-\nannotated but smaller ones (Pont-Tuset et al.,\n2019). Better understanding of what aspect of a\ndataset is more important can result in better task\nperformance and also guide us in future dataset cu-\nration efforts. We ﬁnd that a dataset’s size does\nnot always predict multimodal transformers’ per-\nformance; its noise level and language similarity\nto the evaluation task are both important contribut-\narXiv:2102.00529v1  [cs.CL]  31 Jan 2021\ning factors. We also show that multimodal trans-\nformers can achieve competitive results without\nrelying on language-only or image-only pretrain-\ning for weight initialization or feature extraction.\nWe also dissect multimodal transformers’ ar-\nchitecture, analyzing the effectiveness of differ-\nent attention mechanisms, depth, and number of\nparameters. We show that multimodal attention\nwhere both language and image transformers at-\ntend to each other are crucial for these models’\nsuccess. Multimodal attention achieves the best\nresults when combined with multi-level (deep) in-\nteractions. Moreover, models with other types of\nattention (even with more depth or parameters)\nfail to achieve comparable results to shallower and\nsmaller models with multimodal attention.\nAdditionally, inspired by the success of ( e.g.,\nvan den Oord et al., 2018) for self-supervised rep-\nresentation learning, we examine whether using a\ncontrastive image–text matching loss instead of a\nclassiﬁcation one improves the quality of repre-\nsentations in our models. Surprisingly, we ﬁnd\nthat the choice of image–text matching loss does\nnot matter much in multimodal transformers. On\nthe other hand, models without multimodal atten-\ntion (a multi-level “cross-talk” between modali-\nties) beneﬁt signiﬁcantly from a contrastive loss.\nFinally, we believe that advances in multimodal\npretraining can have signiﬁcant impacts on a wide\nrange of downstream applications; however, it is\nimportant to form a clear understanding of how\nand why multimodal transformer models perform\nwell to avoid overﬁtting to a set of downstream\nevaluation tasks. Our analysis of pretraining data,\nattention, and loss functions is an important step\ntowards gaining a deeper understanding of these\npowerful models.\n2 Multimodal Transformers\nThe success of transformer-based language mod-\nels on a variety of language tasks ( e.g., Devlin\net al., 2019) has inspired similar multimodal ef-\nforts (e.g., Lu et al., 2019; Chen et al., 2020; Tan\nand Bansal, 2019; Li et al., 2020a,b). 2 The main\ndistinction is that image-text multimodal trans-\nformers take image-text pairs as input, attend over\nboth modalities, and are trained with additional\n2We use the term multimodal transformers to refer to\nimage–text transformer–based models. Note that similar ar-\nchitectures are applied to other modalities such as videos\n(Sun et al., 2019) but are outside of the scope of this work.\nlosses. Similar to the language models, multi-\nmodal transformers are often ﬁne-tuned on down-\nstream tasks but multimodal ones; e.g., image re-\ntrieval (Young et al., 2014) or visual question an-\nswering (Goyal et al., 2017).\nWe give a brief overview of the BERT\nmodel (Devlin et al., 2019) which forms the back-\nbone of multimodal transformers. The BERT ar-\nchitecture consists of a stack of transformer blocks\n(Vaswani et al., 2017) and has three main compo-\nnents. First, the input text is tokenized and three\nembedding functions are used to embed the to-\nken, its position in the sentence ( i.e., positional\nencoding), and the sentence it belongs to. The ﬁ-\nnal language embedding is a summation of these\nthree vectors. The BERT model also includes a\n<SEP> token to separate different sentences and\na <CLS> token which can be thought of as an ag-\ngregate representation of the input text. Second,\nthe sequence of token embeddings are input into a\nseries of transformer layers where tokens are com-\nbined through self-attention. Third, two different\nlosses are applied to the model output: a masked\nlanguage modeling loss, in which the model pre-\ndicts a masked word (denoted by a <MASK> to-\nken), and a next sentence prediction loss which,\ngiven two sentences, predicts if the second sen-\ntence follows the ﬁrst.\nMultimodal transformer models facilitate learn-\ning from multimodal data via three changes to the\nBERT architecture: multimodal data preprocess-\ning (more speciﬁcally images), adding multimodal\nattention by changing self-attention such that it\ncombines image and text modalities, and introduc-\ning image and multimodal loss functions.\n2.1 Multimodal Data Processing\nTraining multimodal transformers requires image–\ntext pairs such that the text for a given image, at\nleast to some degree, describes the image. Re-\ncent work attempts to remove the annotation cost\nby automatically collecting datasets (e.g., web im-\nages and their alt-text as in Sharma et al., 2018).\nIn Sec. 4.2, we examine whether the quality of text\ndescriptions impacts these models’ performance.\nThe text input processing is the same as lan-\nguage models; in fact, many of the existing mod-\nels (such as Lu et al., 2019) are initialized with\nBERT pretrained weights. We show that this ini-\ntialization is not important in our experiments (see\nSec. 4.2). Processing images into a sequence\nFigure 1: Tracking queries, keys and values for different attention types described in Sec. 2.2.\nVision\nMulti-Head \nAttention\nLanguage\nMulti-Head \nAttention\nQw Kr VrKw VwQr\nHl\nr Hl\nw\nCoattention\nVision\nMulti-Head \nAttention\nLanguage\nMulti-Head \nAttention\nQwKr Vr Kw VwQr\nHl\nr Hl\nw\nModality specific attention\nQwKr,Kw  Qr\nHl\nr Hl\nw\nVr,Vw  Kr,Kw  Vr,Vw  \n      Vision & Language\nMulti-Head \nAttention\nMerged\nLanguage\nMulti-Head \nAttention\nQw Kr Vr\nHl\nr Hl\nw\nAsymmetric attention (language)\nQwKr,Kw  Qr\nHl\nr Hl\nw\nVr,Vw  Kr,Kw  Vr,Vw  \nMerged\nVision\nMulti-Head \nAttention\nLanguage\nMulti-Head \nAttention Vr,w QwQr\nHl\nr Hl\nw\n      Vision & Language\nMulti-Head \nAttention\nMerged\nKr,w Vr,wKr,w\nVr/w QwQr\nHl\nr Hl\nw\n      Vision & Language\nMulti-Head \nAttention\nMerged attention\nKr/w Vr/wKr/w\nVision\nMulti-Head \nAttention\nLanguage\nMulti-Head \nAttention\nQw Kr VrKw VwQr\nHl\nr Hl\nw\nCoattention\nVision\nMulti-Head \nAttention\nLanguage\nMulti-Head \nAttention\nQwKr Vr Kw VwQr\nHl\nr Hl\nw\nModality specific attention\nLanguage\nMulti-Head \nAttention\nQw Kr Vr\nHl\nr Hl\nw\nAsymmetric attention\nVr/w QwQr\nHl\nr Hl\nw\n      Vision & Language\nMulti-Head \nAttention\nMerged attention\nKr/w Vr/wKr/w\ninvolves deﬁning “visual tokens” analogously to\nlanguage tokens. Almost all image-text multi-\nmodal transformer models consider a bounding\nbox from a pretrained object detection model to\nbe a “visual token”. Similar to the positional en-\ncodings in language models, for each visual token,\nthe spatial position of each bounding box is also\nencoded.\nAlthough most multimodal transformers require\ntraining a supervised model (a detector) to ex-\ntract bounding-box features, there are other possi-\nble ways to represent visual tokens – for example,\nHuang et al. (2020) bypass training a detector by\nusing regions from a high-level feature map in an\nimage classiﬁcation network as visual tokens. We\nfocus our studies on models which use bounding-\nbox features as this reﬂects the majority of recent\nwork, though we achieve comparable results when\nlearning directly from images without a detector\n(or even a pretrained classiﬁer) in Sec. 4.2.\n2.2 Multimodal Attention\nEach transformer block consists of a multi-head\nattention module (Vaswani et al., 2017) that for a\ngiven token embedding produces a weighted rep-\nresentation of all other tokens in a sentence. This\nweighted representation is then combined with\nthe input representation of the given token and is\npassed to the next layer. More speciﬁcally, for the\ntoken iat layer l, each attention head takes as in-\nput a keyki\nl, value vi\nl, and query qi\nl which are com-\nputed by passing the representation from the pre-\nvious layer hi\nl−1 through a linear layer. The output\nof the attention module for token iis:\nA(qi\nl,Kl,Vl) =softmax\n(qi\nlKl√dk\n)\nVl (1)\nwhere dk is the dimension of the key and Kl and\nVl matrices contain all tokens’ keys and values.\nGiven this deﬁnition, there are a few possible\nways to implement multi-head attention over im-\nage and language modalities as shown in Fig. 1.\nFor a given query (from one modality), we can\nsimply consider keys and values from all input to-\nkens regardless of the modality type ( e.g., Chen\net al., 2020). We refer to this multimodal attention\nas merged attention since it simply merges inputs\nfrom the two modalities.\nAlternatively, given queries from one modality\n(e.g., image), keys and values can be taken only\nfrom the other modality ( e.g., language). Follow-\ning Lu et al. (2019), we refer to this multimodal\nattention as coattention. We also consider cases\nwhere this attention is asymmetric,i.e., queries are\neither from language or image, while keys and val-\nues are from image or language, respectively. We\ncall these two attention types language-query at-\ntention or image-query attention.\nAnother possibility is to consider single-\nmodality transformers where queries, keys, and\nvalues all come from either the image or text\nmodality; we refer to this attention as modality-\nspeciﬁc attention where each modality has its own\nmulti-head attention. Single-modality transform-\ners with modality-speciﬁc attention allow us to\nstudy the role of “cross-talk” between modalities\nin multimodal transformer models.\nWe note that we use the term multimodal atten-\ntion to refer to both merged attention and coatten-\ntion and discuss the importance of different atten-\ntion types in Sec. 4.3.\n2.3 Multimodal Loss Functions\nBroadly, multimodal transformers have three loss\ntypes, language and image losses that are ap-\nplied to the language and image outputs, re-\nspectively, as well as an image-text matching\nloss applied to image–language pairs. Let r =\n{r1,··· ,rN}be the N input image regions and\nw = {w1,··· ,wT}be the T word tokens rep-\nresenting an image–text pair. A subset of input\nimage regions and word tokens are masked ( e.g.,\nset to zero) before being passed through the trans-\nformer layers. After applying the mask, we refer\nto the unmasked image regions as rm and to the\nunmasked word tokens as wm. We use Nm and\nTm to denote the set of image region and word to-\nken indices that are masked, respectively. Similar\nto the BERT model, the language loss is a masked-\nlanguage modelling (MLM) loss:\n−\n∑\nt∈Tm\nlog Pw\nθ (wt|wm,rm), (2)\nwhere Pw\nθ corresponds to the output probability\ndistribution over words in the vocabulary from the\ntransformer model parameterized by θ.\nMost models also include an analogous masked\nregion modeling loss (MRM) for images. One\npopular region modelling loss, for each bound-\ning box, minimizes the KL-divergence between\nthe predicted distribution over object classes and\nthe distribution over classes obtained from a pre-\ntrained detector D(l|rn) (e.g., Chen et al., 2020;\nLu et al., 2019).\n∑\nn∈Nm\nKL(D(l|rn)||Pr\nθ (rn|rm,wm)), (3)\nwhere Pr\nθ corresponds to the predicted probabil-\nity distribution over object classes from the trans-\nformer model parameterized by θ.\nFinally, multimodal transformer models include\nan image–text matching (ITM) loss which predicts\nwhether an image and text pair match; this is gen-\nerally posed as a binary classiﬁcation problem:\n−ylog(σ(sθ(rm,wm)))\n−(1 −y) log(1−σ(sθ(rm,wm))), (4)\nwhere yis equal to 1 for positive pairs and 0 oth-\nerwise and sθ corresponds to the conﬁdence score\nof the model that a pair (r,w) are matched and\nσ is the sigmoid function. Recently, contrastive\nimage–text matching losses have been success-\nful in self-supervised representation learning (e.g.,\nvan den Oord et al., 2018); thus, we also explore\nwhether a contrastive formulation of ITM can im-\nprove the performance of multimodal transformers\nand discuss the challenges of using these losses for\nmultimodal transformer models. Our contrastive\nloss is formulated as:\n−log\n\n esθ(rm,wm))\nesθ(rm,wm) + ∑\n(˜r,˜w)∼N\nesθ(˜rm,˜wm)\n\n, (5)\nwhere N is a set of negative image-text pairs.\nSec. 4.4 outlines our ﬁndings on loss ablations.\n3 Experimental Setup\nHere we outline the details of our experimental\nsetup: the base multimodal transformer model\nused in most of our experiments, our baseline\nmodel, and the pretraining datasets.\n3.1 Base Multimodal Transformer\nOur base multimodal transformer model (MMT)\nmost closely resembles the ViLBERT model (Lu\net al., 2019). For text inputs, we ﬁrst tokenize sen-\ntences using SentencePiece (Kudo and Richard-\nson, 2018) and truncate sentences into a ﬁxed\nlength of 22 for pretraining datasets and 25 for\ndatasets used to ﬁne-tune and evaluate retrieval\nmodels. We then include a separator ( <SEP>)\nand an aggregator ( <CLS>) token. Unless oth-\nerwise stated, we do not transfer weights from a\npretrained BERT model.\nFor image inputs, we represent “visual tokens”\nas region of interest (RoI) pooled features corre-\nsponding to bounding boxes from an object detec-\ntor (Ren et al., 2015) trained on Visual Genome\n(Krishna et al., 2017) images with labels parsed as\nwas done in Anderson et al. (2018). The detection\nmodel is trained using a multi-label sigmoid cross-\nentropy loss to simultaneously predict objects and\nattributes. The highest 36 or 100 scoring bounding\nboxes are input when pretraining or evaluating, re-\nspectively. Like ViLBERT, we include an “aver-\nage” feature which is computed by averaging fea-\ntures across bounding boxes and serves a similar\nrole to the <CLS> token in the text input.\nIn addition to the positional encoding added to\ntext embeddings before the ﬁrst transformer layer,\nwe also add the positional encoding to the text em-\nbedding at each layer of the language-only trans-\nformer blocks as in XLNet (Yang et al., 2019) be-\ncause this led to improvements on a language–\nonly BERT model. For image inputs, we embed\nbounding box coordinates and add this to our im-\nage embedding.\nIn our model, following ViLBERT, a multi-\nmodal co-attention layer consists of an image-only\nand a language-only transformer, each followed\nby a transformer with coattention (see Sec. 2.2).\nWe use the term “layer” to refer to this multi-\nmodal layer. Like VilBERT, our model consists of\n6 language-only layers, followed by 6 multimodal\nones. We train the model by minimizing masked\nlanguage modelling (Eq. (2)), masked region mod-\neling (Eq. (3)), and binary classiﬁcation image–\ntext matching (Eq. (4)) losses. To calculate the\nimage-text loss, we apply an element-wise multi-\nplication to the <CLS> language features and out-\nput corresponding to the averaged image feature\ninput. The resulting “multimodal feature” is in-\nput into a classiﬁcation model. We create negative\nimage-text examples by sampling text from an-\nother image in our batch. Unless otherwise noted,\nwe have an equal number of negative and positive\nimage-text pairs.\nWe train our models with a global batch size of\n1024 distributed over 64 Google Cloud TPU v3\ncores3. We use the LAMB optimizer (You et al.,\n2019) with an initial learning rate of 0.00176 and\n20,000 warm up steps. Learning rate is decayed\nwith polynomial decay with a minimum learning\nrate ratio of 0.004. We use gradient clipping ( 1)\nand dropout ( 0.1) as well as weight decay ( 0.1).\nWe ﬁnd weight decay particularly important in en-\nsuring that our loss did not diverge. We train our\nmodels for a maximum of 1,000,000 iterations.\n3.2 The Baseline Model\nMultimodal transformers are different from most\nprior image–text models because they are pre-\ntrained on a large dataset (millions of image-text\npairs). To better understand if data alone can lead\nto better image–text representations, we train a\nstrong baseline model, which does not include a\nmultimodal attention mechanism, with the same\ndata as our multimodal transformer.\nOur baseline model learns a joint space be-\ntween language and vision (Weston et al., 2011;\nFrome et al., 2013; Kiros et al., 2014) by minimiz-\ning the distance between image and text features\ntaken from a positive pair (where text describes the\nimage) and at the same time increasing that dis-\ntance for a negative pair. Despite lacking a mul-\ntimodal attention mechanism, this approach has\nbeen popular in image and video domains due to\nits simplicity and effectiveness for retrieval appli-\ncations (e.g., Gong et al., 2014; Wang et al., 2016;\nChowdhury et al., 2018; Miech et al., 2018).\nTo implement our baseline, we ﬁrst encode\nword tokens w into a ﬁxed-size sentence represen-\ntation S ∈R768 and image regions r into a ﬁxed-\nsize image representation I ∈R768. To encode\nsentence representations, we input words into a\nrandomly initialized BERT model and extract sen-\ntence representations S from the <CLS> output.\nTo extract image representations I, we ﬁrst mean-\npool features across detected bounding boxes and\nthen pass the mean-pooled features into a one-\nlayer MLP with an output of size 768. Finally, we\nelement-wise multiply I and S and input the re-\nsulting vector into a two-layer MLP parameterized\n3https://cloud.google.com/tpu/\nTable 1: The pretraining datasets: the type and number\nof images and captions.\nDataset # images Caption\nType #\nMSCOCO 83K Annot. 592K\nVisual Genome (VG) 110K Annot. 5.4M\nMSCOCO-narratives 83K Narration 230K\nOI-narratives 500K Narration 1.3M\nSBU 1M Web 1M\nConceptual Captions 2.7M Alt-text 2.7M\nby θwhich outputs a score, sθ indicating whether\nIand Smatch. We train our baseline model using\nthe contrastive loss deﬁned in Equation (5) with\n1024 negative examples. The detector weights are\nﬁxed during training.\n3.3 Pretraining Datasets\nConceptual Captions (CC) consists of over 3\nmillion image-text pairs harvested from the web\nwhere the caption corresponding to an image is its\nalt-text description (Sharma et al., 2018). Image–\ntext pairs are ﬁltered and preprocessed such that\ntext is more image relevant than raw Alt-text; how-\never, the dataset is still “noisy” and includes pairs\nwhere the text is not relevant to the image’s con-\ntent. We were able to download 81% of the train-\ning set of CC; unless otherwise stated, we train our\nmodels on this subset of CC.\nThe SBU dataset (Ordonez et al., 2011) con-\nsists of 1 million image-text pairs sourced from\nFlickr with text taken from users’ captions. As\na result, similar to CC, not all text is image rele-\nvant. We also use datasets which were collected\nby asking annotators to describe images, result-\ning in more image relevant language including the\nMSCOCO dataset (Chen et al., 2015) and Visual\nGenome (VG) (Krishna et al., 2017), which in-\ncludes descriptions for bounding boxes in images.\nWhen using VG, we consider each bounding box\ndescription to be a caption for the entire image.\nWe also experiment with the Localized Nar-\nratives dataset (Pont-Tuset et al., 2019). This\ndataset includes rich annotations collected by ask-\ning users to describe an image while pointing to\neach part of the image being described (using\ntheir mouse). The resulting “narratives” often con-\nsist of multiple sentences. We break the narra-\ntives into individual sentences and treat each sen-\ntence as a caption paired with the image. We\nTable 2: Number of images in evaluation tasks and\nwhether datasets were used in a zero-shot (ZS) or ﬁne-\ntuned (FT) setting.\nDataset # images ZS FTtrain test\nFlickr30k 29K 1K ✓ ✓\nMSCOCO n/a 5K ✓\nVQA 440K 210K ✓\nuse the localized narratives collected for the Open\nImages (Kuznetsova et al., 2018) and MSCOCO\ndatasets, and refer to them as OI-narratives and\nMSCOCO-narratives. This allows us to compare\nmodels which are trained with the same images\n(MSCOCO) with different language (MSCOCO\ncaptions vs. localized narratives). Table 1 pro-\nvides an overview of our pretraining datasets.\nFinally, we consider combining datasets us-\ning two sampling approaches: instance sampling\nwhere we mix all datasets together and sample\nfrom this mix for each batch and dataset sampling\nwhere we sample evenly from datasets so that each\nbatch contains the same number of examples from\neach dataset. For datasets with multiple captions,\nwe ﬁrst sample an image, then sample a caption\nfor the given image. We combine all six datasets\ndescribed here as well as the four datasets com-\nbined in Chen et al. (2020) (MSCOCO, VG, SBU,\nand Conceptual Captions) which we refer to as\nUNITER data.\n3.4 Evaluation Tasks\nWe focus on zero-shot evaluation as it enables\nus to examine the representations without con-\nfounding our ﬁndings with the side-effects of ﬁne-\ntuning (Yogatama et al., 2019) or probing classi-\nﬁers (e.g., Zhang and Bowman, 2018; Hewitt and\nLiang, 2019). Following Lu et al. (2019) and Chen\net al. (2020), we use the term zero-shot to refer\nto experiments where we test our models on a\ndataset different from our pretraining data without\nﬁne-tuning. For example, we use the MSCOCO\ndataset to test the models that are pretrained on\nConceptual Captions. This is considered as a zero-\nshot task since the properties of the dataset used\nfor testing (for example, its language) differ from\nthose in the pretraining dataset. We use zero-shot\nimage retrieval tasks since image retrieval directly\nmeasures what our pretraining data and objectives\nencourage our models to learn: whether an image\nand a sentence are aligned.\nWe evaluate on the Flickr30k dataset (Young\net al., 2014) (referred to as zero-shot Flickr) and\nuse the splits deﬁned in Karpathy and Fei-Fei\n(2015). We evaluate checkpoints after 1 million\nsteps as well as when the loss on the CC validation\nset is lowest. When varying the pretraining data,\nour models sometimes overﬁt quickly on smaller\ndatasets; as a result, we evaluate checkpoints ev-\nery 100K steps. We select the best checkpoint\naccording to zero-shot performance on Flickr30k\nval and use it for all other downstream tasks. We\nalso report retrieval numbers on MSCOCO (Chen\net al., 2015) (which we call zero-shot MSCOCO)\nusing the splits of Karpathy and Fei-Fei (2015).\nReported retrieval numbers are on the test split of\ndatasets. Images in Flickr30k and MSCOCO are\nannotated with 5 captions.\nIn addition to the zero-shot image retrieval\ntasks, we use the ﬁne-tuned Flickr30k image-\nretrieval task to examine whether our observa-\ntions transfer when ﬁne-tuning the MMT model.\nWe ﬁne-tune our models for 10,000 steps and use\nMLM, MRM, and ITM losses. All results for im-\nage retrieval are reported using Recall@K (R@K),\nwhich measures whether the ground-truth image is\namong the top K images retrieved by our model.\nWhen comparing pretraining datasets, we hy-\npothesize that which pretraining dataset is best de-\npends on the downstream task, so we additionally\nconsider VQA (Antol et al., 2015; Goyal et al.,\n2017). To ﬁne-tune for VQA, we replace the\nimage–text matching loss with a 2-layer MLP and\ntrain with a binary cross-entropy loss against soft\nanswer scores (Teney et al., 2018). We use similar\nhyper-parameters as when pretraining and report\nresults on the validation set. We report the average\nscore across 3 random initializations of the MLP.\nWe use Flickr IDs to ﬁlter out images appearing\nin the Flickr30k and MSCOCO validation/test sets\nfrom our pretraining sets. Conceptual Captions is\nnot collected from Flickr, so we could not ﬁlter\nout images using this method. Table 2 provides an\noverview of our evaluation datasets.\n4 Experimental Results\nWe ﬁrst compare MMT to a baseline and then in-\nvestigate how pretraining data, attention, and loss\nfunctions impact model performance.\nTable 3: Comparison of our proposed baseline to our\nmultimodal transformer model (MMT).\nFlickr30k MSCOCO\nZS FT ZS\nR1 R10 R1 R10 R1 R10\nBaseline 25.4 64.9 40.9 81.8 13.0 44.5\n−contrastive 21.7 61.0 39.0 80.6 10.2 40.9\n+BERT PT 24.8 65.1 39.9 79.9 12.7 43.1\nMMT 41.9 79.0 59.1 91.5 21.3 57.9\nViLBERT 31.9 72.8 58.2 91.5 - -\n4.1 Comparison to a Baseline\nWe compare our multimodal transformer (MMT)\nagainst a strong baseline inspired by recent suc-\ncess in visual retrieval ( e.g., Miech et al., 2018).\nTo disentangle the effect of pretraining data and\narchitecture, we investigate whether our baseline\n(described in Sec. 3.2), without multimodal atten-\ntion or MLM and MRM losses but pretrained on\nthe same data (i.e., Conceptual Captions) as multi-\nmodal transformers produces competitive results.\nIn Table 3, we compare MMT to our proposed\nbaseline, verifying that MMT learns better repre-\nsentations not only because it is pretrained on a\nlarge dataset, but because of architectural choices.\nOur MMT results are on par with existing mod-\nels trained with the same data: comparing to ViL-\nBERT, the most similar model to ours, on thezero-\nshot Flickr, we achieve an R@1 of 41.9 in com-\nparison to 31.9. As expected, retrieval numbers\non zero-shot MSCOCO are lower than zero-shot\nFlickr because MSCOCO has more images in its\nevaluation set (see Table 2) and is therefore harder.\nOn the ﬁne-tuned image retrieval task, we achieve\ncomparable performance to ViLBERT (our R@1\nis 59.1 vs. 58.2), even though we do not sample\nhard negatives when training. We emphasize that\nour goal is not to outperform existing work, but\nto build a strong multimodal transformer model to\nanalyze the role of data, attention, and losses.\nWe verify that a contrastive loss (Eq. (5)) leads\nto stronger results than a classiﬁcation one. As\nshown in Table 3, replacing the contrastive loss\nwith a classiﬁcation loss consistently decreases\nperformance. Initializing our baseline with BERT\nweights marginally decreases performance, e.g.,\nR@1 on zero-shot Flickr decreases by 0.6.\nFigure 2: Effect of pretraining data. The datasets on X\naxis are ordered based on their zero-shot Flickr scores.\nIS:Instance Sampling, DS: Dataset Sampling.\n(a) Zero-shot (ZS) & ﬁne-tuned (FT) image retrieval (IR)\nMSCOCO-nar.\nSBU VG\nOI-nar.MSCOCO\nCC\nUniter: ISUniter: DS\nAll: ISAll: DS\n0\n20\n40\n60R1\nFlickr30k ZS-IR Flickr30k FT-IR COCO ZS-IR\n(b) Visual question answering (VQA v2)\nMSCOCO-nar.\nSBU VG\nOI-nar.MSCOCO\nCC\nUniter: ISUniter: DS\nAll: ISAll: DS\n60\n62\n64\n66VQA Score\n4.2 Multimodal Data Preprocessing\nWe investigate how pretraining datasets, super-\nvised image features, and weights from a pre-\ntrained language model impact our results.\nPretraining Datasets. Fig. 2 reports our results\nwhen we pretrain the MMT on the individual and\ncombined datasets introduced in Sec. 3.3. We ob-\nserve that in all our tasks, larger datasets usually\nlead to better performance, but not always. For\nexample, SBU consistently performs worse than\nMSCOCO, despite being substantially larger.\nAdditionally, when combining datasets, how\ndatasets are sampled matters . In our experi-\nments, dataset sampling (DS) is more effective\nthan instance sampling (IS). In dataset sampling,\nsmaller datasets (like MSCOCO) will be sampled\nmore frequently than in instance sampling. Since\nMSCOCO pretraining leads to good performance,\nmore exposure to MSCOCO samples is beneﬁ-\ncial. We consider combining all datasets as well as\ndatasets combined in UNITER (Chen et al., 2020).\nFig. 3a shows that combining all datasets performs\nbetter than UNITER data on the zero-shot Flickr\ntask, but not on the zero-shot MSCOCO, showing\nthat more data is not always better. On zero-shot\nMSCOCO the impact of the sampling mechanism\nis even more evident: given UNITER data, dataset\nsampling performs better than instance sampling\nby over 10 points (37.1 vs 26.4).\nNext, we compare datasets that have a simi-\nlar number of images to investigate the role of\nthe type of language used in each dataset. As\nan extreme example, MSCOCO and MSCOCO-\nnarratives contain the same images, but the former\ndoes substantially better on our downstream tasks.\nTo better understand this observation, we quan-\ntify the difference between the language of pre-\ntraining and evaluation datasets: we trained a lan-\nguage model (a 6-layer Transformer) on a given\npretraining dataset, and use that model to com-\npute the perplexity of the evaluation dataset. For\nour three datasets with the same number of im-\nages (MSCOCO, MSCOCO-narratives, and VG),\nthe perplexity of the evaluation dataset (Flickr or\nMSCOCO) explains their performance – the per-\nplexities are the lowest on MSCOCO, then VG,\nand lastly on MSCOCO-narratives. This shows\nthat the similarity between the language of pre-\ntraining and evaluation datasets is important.\nHowever, not all performance differences are\nexplained by the number of images or perplexity:\npretraining on SBU results in poorer performance\nthan OI-narratives on our downstream tasks, de-\nspite SBU having twice the number of images and\nlower perplexity on both evaluation datasets. We\nconjecture that SBU’s poor performance is due to\nnoise: SBU text is scraped from captions and may\nnot match the images as well as the manually an-\nnotated text in OI-narratives. To investigate this,\nwe calculate an overlap metric for an image–text\npair as the ratio of text words overlapping with\npredicted bounding box labels. For each dataset,\nwe calculate the average overlap for 3000 images,\nproviding an approximation of how much the lan-\nguage describes the images in the dataset. The\noverlap is much lower for SBU compared to OI-\nnarratives (0.14 vs. 0.25), showing that SBU is\nindeed noisier, which can decrease its utility for\npretraining multimodal representations.4\nMoreover, we observe that the goodness of a\npretraining dataset for one task does not always\ntransfer to a different task. For example, CC is\na better pretraining dataset than VG when ﬁne-\ntuning for image retrieval, but they perform sim-\nilarly when ﬁne-tuning for VQA, a substantially\ndifferent task. In fact, we note that VQA perfor-\nmance varies less across pretraining datasets (e.g.,\nCC, VG, and MSCOCO), likely because the VQA\ntraining split is large. We also observe differences\nbetween zero-shot and ﬁne-tuned image retrieval.\nThough MSCOCO performs 3.8 points better on\n4The overlap metric for other datasets: VG: 0.82,\nMSCOCO: 0.42, MSCOCO-narratives: 0.27, and CC: 0.11.\nFigure 4: Comparing models trained with the\nMSCOCO and CC datasets. We provide the top-1\nranked retrieved image given an input query sentence\non the Flickr val dataset. Correctly retrieved images\nare framed in green and the incorrect ones in red.\nCCMSCOCO\nThis is an image of two men \nduring a match of karate and a \ndisplay of fighting skills.\nA group of kids competing in a \nrelay.\nCCMSCOCO\nBlond-haired man wearing a black \nfleece jacket sculpting.\nA backpacker enjoying a view \nof nature.\nzero-shot Flickr than OI-narratives, OI-narratives\nperforms 2.9 points better after ﬁne-tuning.\nFinally, to visually illustrate the difference be-\ntween the learned representations, we compare\nqualitative examples of models trained with our\nbest two pre-training datasets: MSCOCO and CC\n(see Fig. 4). Though the model trained with\nMSCOCO retrieves examples with some seman-\ntic relevance, our model trained with CC is able to\nretrieve images with more correct details like “en-\njoying a view” and “black ﬂeece jacket”.\nLanguage-only Pretraining. Many multimodal\ntransformers initialize language weights from a\npretrained BERT model. Similar to LXMERT, we\nﬁnd this hurts performance on our retrieval task;\nR@1 on zero-shot Flickr decreases to 39.7 and\nR@1 on zero-shot MSCOCO decreases to 20.4.\nImage-only Pretraining. The object detector\nused to extract image features is another source\nof modality-speciﬁc pretraining. We replace de-\ntection features with grid features taken from the\nlast residual block of a ResNet-50 trained from\nscratch.5 Similarly to Huang et al. (2020), this\nmodel is trained without the MRM loss since fea-\ntures aggregate information in the whole image,\nand as a result, masking speciﬁc regions is not\nstraightforward. This model performs slightly bet-\nter than our base MMT on zero-shot Flickr (43.4\n5We ﬁt images into a 384 × 384 square by resizing and\npadding to preserve the aspect ratio. As the total stride of\nResNet-50 is 32, a feature grid is of size 12 × 12, which\nwe ﬂatten to 144 features and give as input along with the\naveraged features (for the <CLS> token) to our MMT.\nFigure 5: Ablation studies on number of layers and heads.\n1 2 4 6 12\nNumber of layers\n0\n20\n40IR ZS R@1\n(a) Depth\n1 (768) 3 (256) 12 (64)\nNumber of heads ( d)\n0\n20\n40IR ZS R@1 (b) Heads: ﬁxed #params\n6 12 18\nNumber of heads ( d = 64)\n0\n20\n40IR ZS R@1\nFlickr\nMSCOCO (c) Heads: ﬁxed dimension\nTable 4: MMT trained with coattention (Co), merged\nattention (Merge), language-query attention (L-12 and\nL-24), image-query attention (I-12 and I-24) (the\nnumber indicates the number of attention heads) and\nmodality-speciﬁc attention.\nR@1 Co Merge Asym. Attn. Mod.\nL-12 I-12 L-24 I-24 Spec.\nF. ZS 41.9 40.0 24.4 31.3 33.6 31.6 16.9\nF. FT 59.1 57.0 45.1 48.4 52.5 46.3 15.4\nM. ZS 21.3 19.6 13.8 16.1 17.0 16.0 8.0\nvs. 41.9) and comparably on zero-shot MSCOCO\n(21.3 vs. 20.6). Though Huang et al. (2020)\nshowed a detector can be replaced with an image\nclassiﬁer, we show that comparable results can be\nachieved without any image-only pretraining.\nWe conclude that careful consideration of pre-\ntraining datasets and their sampling methods is im-\nportant in a model’s performance – the level of\nnoise and the type of language in a dataset can be\nmore signiﬁcant than its size. Finally, the image-\nonly and language-only pretraining are not crucial\nin training strong multimodal representations.\n4.3 Multimodal Attention\nWe explore the impact of the number of attention\nheads and coattention layers in our base multi-\nmodal transformer model before investigating the\neffect of different attention mechanisms.\nNumber of Heads and Layers.We test the im-\nportance of the number of heads in multi-head at-\ntention when ﬁxing the total number of parame-\nters by comparing models trained with one head,\n3 heads, and 12 heads with query/key size of 768,\n256, and 64, respectively. Increasing the num-\nber of heads to 12 leads to an improvement (Fig-\nure 5b). Next, we vary the number of heads (6, 12,\nand 18) but ﬁx the query/key size to 64. We ob-\nserve that increasing the number of heads up to 12\nstill leads to an improvement, but further increase\nresults in poorer performance (see Figure 5c).\nConsistent with Lu et al. (2019), increasing the\nnumber of layers (Fig. 5a) helps up to a point, and\nthen adding more layers degrades performance.\nType of Attention Mechanism.We perform an\nin-depth analysis on different types of attention\nexplained in Sec. 2.2 (see Table 4). We compare\ncoattention with merged attention – these mech-\nanisms both “combine” the image and language\nmodalities; however, coattention does so by tak-\ning keys/values and queries from opposite modali-\nties, while merged attention shares keys and val-\nues across the modalities. When controlled for\nthe number of parameters, coattention performs\nmarginally better than merged attention. Both per-\nform considerably better than asymmetric atten-\ntion in which attention queries are over one modal-\nity.\nThe number of heads in an asymmetric at-\ntentions is half of the equivalent coattention, so\nwe experiment with asymmetric attention mech-\nanisms with 12 heads (L-12, I-12) as well as\n24 heads (L-24, I-24). Increasing the number\nof attention heads for the asymmetric attention\nimproves results, but the gap between our best-\nperforming model with asymmetric attention (L-\n24) and coattention is still quite large.\nWe also consider transformers with modality-\nspeciﬁc attention where there is no cross-talk be-\ntween the modalities through attention, but the\nmodel has the same number of parameters as our\nMMT with coattention and is trained with the\nsame losses (Table 4, Mod. Spec. column). This\nmodel performs substantially worse than MMT.\nTo better demonstrate the strength of mul-\ntimodal attention compared to asymmetric and\nmodality-speciﬁc attention, we compare our mod-\nels in Table 4 to shallower and smaller models with\ncoattention on the zero-shot Flickr task. Strik-\ningly, our best-performing model without multi-\nmodal attention with 24 attention heads and 12\nlayers (R@1 of 33.6; L-24 in Table 4) performs\nworse than the coattention model with only one\nhead (R@1 of 38.2; Fig. 5b) or one multimodal\nlayer (R@1 of 37.2; Fig. 5a).\nFigures 6 shows example retrieval results com-\nparing the asymmetric and modality speciﬁc atten-\nFigure 6: Comparing top-1 ranked images retrieved with models trained with the different attention mechanisms\non the Flickr dataset. Correctly retrieved images are framed in green and the incorrect ones in red.\nCoattentionModality \nSpecific\nA group of men work around a \nset of railroad tracks with heavy \nequipment.\nPeople are gathered on stage. A man in an orange robe sweeping \noutside.\nA little girl plays with a miniature \nelectric circuit consisting of three \nlight bulbs and a battery.\nCoattentionModality \nSpecific CoattentionAsymmetric \nAttention\nThey are posing for a picture.\nA person dressed as a court jester \nduring a theatrical performance.\nCoattentionAsymmetric \nAttention\nFemale rollerskating athlete.\nSomeone in a lime green shirt is \nholding onto a tree.\ntion to our coattention mechanism. When the coat-\ntention mechanism retrieves the incorrect image,\nthe image frequently includes important content\nfrom the sentence (e.g., in Figure 6 lower left, the\nimage shows “people gathered”, but they are not\non stage). Though other attention mechanisms re-\ntrieve images with some similarities to the text, the\ncoattention mechanism retrieves ﬁne details like\n“lime green shirt” and “miniature electric circuit”.\nA modality speciﬁc transformer model is com-\nputationally more efﬁcient than models with mul-\ntimodal attention because image and language fea-\ntures can be computed once and reused across\nimage–text pairs; this means that single-modality\ntransformers are faster for retrieval and thus would\nbe more appealing in large-scale applications if\ntheir accuracy were higher. We therefore investi-\ngate whether we can improve the single-modality\ntransformer’s poor performance by combining ﬁve\nmodality-speciﬁc attention layers followed by one\ncoattention layer to introduce multimodal interac-\ntion. This model is as deep as our MMT, but per-\nforms worse than our MMT with one coattention\nlayer: R@1 of 33.1 vs 37.2 on zero-shot Flickr\nand 16.7 vs 19.0 on zero-shot MSCOCO.\nWe conclude that multimodal attention mecha-\nnisms, either coattention or merged attention, are\na key component to multimodal transformers’ suc-\ncess. Moreover, a shallow or small model with\nmultimodal attention outperforms deeper models\nwith an inferior attention mechanism yet more pa-\nrameters. Finally, we show that a model’s depth\nalone is not important; both multimodal attention\nand depth are needed for best performance.\nTable 5: Zero-shot retrieval results (R@1) on models\ntrained with different losses.\nFlickr-ZS COCO-ZS\nMRM + ITM 20.2 9.7\nMLM + ITM 41.1 22.4\nMRM + MLM + ITM 41.9 21.3\n4.4 Losses\nWe explore the degree to which MLM, MRM,\nand ITM losses contribute to our MMT results.\nWe then explore whether a contrastive formula-\ntion of the ITM loss – used commonly in self-\nsupervised representation learning and important\nfor our baseline – improves MMT’s performance.\nComparing MLM, MRM, and ITM. Table 5\nshows performance of our models with different\ncombinations of the masked modelling losses and\nthe image-text loss. With careful hyper-parameter\ntuning (in particular, decreasing the learning rate\nfrom 0.00176 to 0.001 and using cosine decay in-\nstead of polynomial decay) we can remove the\nMRM loss during pretraining and achieve com-\nparable performance on our image retrieval tasks.\nWe found negligible difference when training our\nbase MMT with the different hyper-parameters.\nWe note that our multimodal transformer trained\non pixels (Sec. 4.2) is also trained without a re-\ngion modelling loss, yet performs similarly to our\nbase MMT. Additionally, our ﬁnding is in line with\nthe results of Li et al. (2020b), who achieve strong\nresults without a region modelling loss.\nContrastive ITM Loss. Contrastive losses (e.g.,\nEq. (5)) require sampling many negative exam-\nples to achieve good performance and thus can be\ncomputationally expensive (e.g., Tian et al., 2019;\nMiech et al., 2020). In models without multimodal\nattention (e.g., our baseline model), the computa-\ntional cost is reduced by caching and reusing neg-\native examples; in such models, since image and\ntext input are processed independently, once im-\nage and text features are calculated, they can be\nconsidered as negatives for all other training ex-\namples in the batch. Due to their multimodal at-\ntention, multimodal transformers process image\nand text examples as pairs and thus cannot share\nimage or text features across training examples.\nThis limits the number of negatives available for\nthese models to the maximum batch size that ﬁts\nin memory. As a result, to study the role of a\ncontrastive loss with a reasonable number of nega-\ntives, we consider our MMT with one multimodal\nlayer. We also examine whether a model with only\nmodality-speciﬁc attention (here, we use 6 image\nand 12 language layers) beneﬁts from a contrastive\nloss since it is easier to increase the negatives in a\nmodel without multimodal attention. In both mod-\nels, we replace the image–text matching classiﬁca-\ntion loss, Eq. (4), with a contrastive one, Eq. (5).\nTable 6 compares the performance of a single-\nmodality transformer trained with a classiﬁcation\nloss to a model trained with a contrastive loss and\n32 or 1024 negatives. We observe a notable im-\nprovement with the contrastive loss and adding\nmore negatives. We next compare the performance\nof our one-layer MMT trained with a classiﬁcation\nloss and a contrastive loss with 32 negatives (the\nmax we could ﬁt into memory). When training\nwith the contrastive loss, we see no performance\ndifference on zero-shot MSCOCO and a small per-\nformance degradation on zero-shot Flickr. This is\nsurprising given the large body of research demon-\nstrating the beneﬁt of contrastive losses. We con-\nclude that the multimodal attention and MLM loss\ncan help the model learn better representations\nwithout relying on stronger image–text losses.\n5 Related Work\nMultimodal transformers are the ﬁrst family of\nmultimodal models to be pretrained on large data\nand applied to a range of different language and\nvision tasks (Lu et al., 2019; Chen et al., 2020;\nTan and Bansal, 2019; Li et al., 2020b,a). The re-\ncent image-text transformers share the same back-\nbone but have slight differences in data prepro-\nTable 6: R@1 with a classiﬁcation ITM loss (cls) and\ncontrastive ITM loss (con) for a MMT with one mul-\ntimodal layer (MMT-1) and a model which only has\nmodality speciﬁc attention (MSA).\nModel Loss Negatives Flickr-ZS COCO-ZS\nMSA Cls. 1 15.0 6.9\nMSA Con. 32 17.9 8.3\nMSA Con. 1024 19.7 9.5\nMMT-1 Cls. 1 37.3 19.1\nMMT-1 Con. 32 35.7 19.1\ncessing and other architectural choices. Notably,\nthe UNITER model (Chen et al., 2020) achieves\nstate-of-the-art results on most existing image–\nlanguage benchmarks by using a larger dataset and\na number of different loss functions. Huang et al.\n(2020) removes the need for using image features\n(taken from a pretrained object detector) by train-\ning models on raw images (pixels). To combine\nimage and text modalities, LXMERT (Tan and\nBansal, 2019) and ViLBERT (Lu et al., 2019) pro-\npose coattention mechanisms, similar to the coat-\ntention originally proposed for VQA (Lu et al.,\n2016). In ViLBERT, feed-forward layers are ap-\nplied after the coattention and self-attention lay-\ners, whereas in LXMERT, a feed-forward layer is\nonly applied after the self-attention layer.\nA few of our ﬁndings are similar to observa-\ntions in prior work: (i) LXMERT and ViLBERT\nshow that more layers improve results, (ii) ViL-\nBERT and UNITER show that more data boosts\nperformance, and (iii) LXMERT shows that trans-\nferring BERT weights is not beneﬁcial. In contrast\nto UNITER, we show that with the right hyper-\nparameters, the MRM loss is not needed.\nFinally, while joint-space approaches to mul-\ntimodal training are applied to multilingual data\n(Gella et al., 2017; Sigurdsson et al., 2020), all ex-\nisting multimodal transformers are applied to En-\nglish; an interesting future direction is to extend\nthese models to other languages.\nAnalyzing multimodal transformers. Recent\nanalysis work (Singh et al., 2020; Cao et al., 2020)\nhas shed light on different aspects of multimodal\ntransformer models. Singh et al. (2020) studies\nwhich pretraining data is best when ﬁne-tuning\ntwo different multimodal transformer variants –\nViLBERT (Lu et al., 2019) and VisualBERT (Li\net al., 2019)– on four ﬁne-tuned tasks, whereas\nwe mainly focus on a zero-shot retrieval task\nacross a variety of pretraining datasets, architec-\ntural choices, and loss functions. Our results are\ncomplementary to this work: Singh et al. (2020)\nobserves that dataset size is not the only factor for\ngood performance and pretraining datasets are bet-\nter when they match the domain of a downstream\ntask. We take a ﬁrst step towards quantifying what\nit means for a pretraining dataset to be similar to a\ndownstream task by analyzing the language used\nin the pretraining datasets and tasks (Section 4.2).\nCao et al. (2020) consider various probing\nmethods on two models (UNITER (Chen et al.,\n2020) and LXMert (Tan and Bansal, 2019)) to\nstudy what information is learned in pretrained\nmodels. Cao et al. (2020) show that while rep-\nresentations become more similar in the last lay-\ners of models with merged attention, in coatten-\ntion models, they are most similar at the ﬁrst mul-\ntimodal layer. They also observe that attention\nheads in merged attention models mostly focus on\nthe language modality, only a few heads are spe-\ncialized for cross-modality processing, and that at-\ntention heads are able to capture some image-text\nalignment. Our comparisons of merged and coat-\ntention is performed in a more controlled setting\nthan the work of Cao et al. (2020) and Singh et al.\n(2020): they compare two models trained by dif-\nferent researchers that include many small differ-\nences other than the attention mechanism; in con-\ntrast, we compare the attention mechanisms in the\nsame modeling framework.\n6 Discussion\nWe rigorously examined different aspects of train-\ning multimodal transformers (datasets, attention,\nand losses) that contribute to the quality of their\nlearned representations. We focused on zero-shot\nimage retrieval tasks to evaluate learned represen-\ntations. Zero-shot tasks are advantageous because\nthey directly measure what a model has learned\nand do not introduce confounds such as the size of\na ﬁne-tuning dataset and its experimental setup. At\nthe same time, datasets do not always capture what\nthey are designed to measure; e.g., Akula et al.\n(2020) show that models can do well on a referring\nexpression task while ignoring the linguistic struc-\nture. Thus, we argue that designing and curating\nspecialized zero-shot evaluation tasks and datasets\nis an important future direction which will allow\nus to better understand our models’ limitations.\nWe ﬁnd the quality of language and the degree\nto which the language describes its correspond-\ning image (noisiness) plays an important role in\nour results. Moreover, language-only and image-\nonly pretraining do not notably contribute to the\nperformance of multimodal transformers. These\nsuggest curating less noisy image–text datasets\nto be more important than relying on single-\nmodality datasets. Previous work has success-\nfully removed some of the noise in automatically-\nharvested datasets through preprocessing ( e.g.,\nSharma et al., 2018) but such approaches are still\nlimited in their robustness to noise, and the far\nfrom negligible degree of noise in large-scale real-\nworld datasets ( e.g., Ordonez et al., 2011; Miech\net al., 2019) still poses a challenge. An alternative\napproach is to aim to remove this noise by design-\ning models that better tap into statistical regulari-\nties of image–text pairs (e.g., Duygulu et al., 2002)\nand thus are more robust to noise.\nWe show that multimodal attention – where\neach modality is informed by both modalities –\nis crucial in these models’ performance. Smaller\nmodels with multimodal attention outperform\ndeeper models with no or other multi-head atten-\ntion mechanisms. This suggests that we can po-\ntentially train smaller models (than the existing\nmultimodal transformers) for a given task, espe-\ncially when the pretraining data is chosen care-\nfully. Moreover, with multimodal attention, we\ncan achieve the best zero-shot retrieval results us-\ning a classiﬁcation loss which uses only one neg-\native example per image–text pair (compare to a\ncontrastive loss with 16384 negatives used in Tian\net al., 2019) and also removes the need for mining\nmore hard negatives (Faghri et al., 2017).\nAdditionally, we observe that comparable re-\nsults can be achieved without the image (masked\nregion modelling) loss in multimodal transform-\ners. This suggests that our current models are not\ntapping into the useful signal in the image modal-\nity, presumably because of the image loss formu-\nlation. An interesting future direction is design-\ning better generative pretraining losses for images;\nprevious work shows that the choice of loss signif-\nicantly impacts the quality of language representa-\ntions (V oita and Titov, 2020).\nFinally, we believe that examining why and\nhow multimodal transformers perform so well can\nguide future work in more effectively measuring\nprogress in learning rich visual-linguistic features.\nAcknowledgements\nWe would like to thank Angeliki Lazaridou, An-\ndrew Zisserman, Phil Blunsom, Laura Rimell, and\nStephen Clark for helpful feedback and conver-\nsations throughout the development of this work.\nWe would like to give special thanks to Aishwarya\nAgrawal for detailed comments and discussion on\nour initial paper draft. Finally, we would like to\nthank Sebastian Borgeaud and Cyprien de Masson\nd’Autume for providing a language only BERT\ncodebase.\nReferences\nArjun R Akula, Spandana Gella, Yaser Al-\nOnaizan, Song-Chun Zhu, and Siva Reddy.\n2020. Words aren’t enough, their order matters:\nOn the robustness of grounding visual referring\nexpressions. arXiv preprint arXiv:2005.01655.\nPeter Anderson, Xiaodong He, Chris Buehler,\nDamien Teney, Mark Johnson, Stephen Gould,\nand Lei Zhang. 2018. Bottom-up and top-\ndown attention for image captioning and vi-\nsual question answering. In Proceedings of the\nIEEE conference on computer vision and pat-\ntern recognition, pages 6077–6086.\nStanislaw Antol, Aishwarya Agrawal, Ji-\nasen Lu, Margaret Mitchell, Dhruv Batra,\nC Lawrence Zitnick, and Devi Parikh. 2015.\nVqa: Visual question answering. In Proceed-\nings of the IEEE international conference on\ncomputer vision, pages 2425–2433.\nTom B Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared Kaplan, Prafulla\nDhariwal, Arvind Neelakantan, Pranav Shyam,\nGirish Sastry, Amanda Askell, et al. 2020. Lan-\nguage models are few-shot learners. arXiv\npreprint arXiv:2005.14165.\nJize Cao, Zhe Gan, Yu Cheng, Licheng Yu, Yen-\nChun Chen, and Jingjing Liu. 2020. Behind\nthe scene: Revealing the secrets of pre-trained\nvision-and-language models. arXiv preprint\narXiv:2005.07310.\nXinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakr-\nishna Vedantam, Saurabh Gupta, Piotr Dollár,\nand C Lawrence Zitnick. 2015. Microsoft coco\ncaptions: Data collection and evaluation server.\narXiv preprint arXiv:1504.00325.\nYen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El\nKholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and\nJingjing Liu. 2020. UNITER: Universal image-\ntext representation learning. In European Con-\nference on Computer Vision (ECCV).\nMithun Chowdhury, Panda Rameswar, Evangelos\nPapalexakis, and Amit Roy-Chowdhury. 2018.\nWebly supervised joint embedding for cross-\nmodal image-text retrieval. In ACM Interna-\ntional Conference on Multimedia.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training\nof deep bidirectional transformers for language\nunderstanding. North American Chapter of\nthe Association for Computational Linguistics\n(NAACL).\nP. Duygulu, K. Barnard, J.F.G. Freitas, and D.A.\nForsyth. 2002. Object recognition as machine\ntranslation: Learning a lexicon for a ﬁxed image\nvocabulary. In European Conference on Com-\nputer Vision (ECCV), pages 97–112.\nFartash Faghri, David J Fleet, Jamie Ryan Kiros,\nand Sanja Fidler. 2017. Vse++: Improving\nvisual-semantic embeddings with hard nega-\ntives. arXiv preprint arXiv:1707.05612.\nAndrea Frome, Gregory S. Corrado, Jonathon\nShlens, Samy Bengio, Jeffrey Dean,\nMarc’Aurelio Ranzato, and Tomas Mikolov.\n2013. Devise: A deep visual-semantic\nembedding model. In NIPS.\nSpandana Gella, Rico Sennrich, Frank Keller,\nand Mirella Lapata. 2017. Image pivoting\nfor learning multilingual multimodal represen-\ntations. arXiv preprint arXiv:1707.07601.\nYunchao Gong, Qifa Ke, Michael Isard, and Svet-\nlana Lazebnik. 2014. A multi-view embedding\nspace for modeling internet images, tags, and\ntheir semantics. IJCV.\nYash Goyal, Tejas Khot, Douglas Summers-Stay,\nDhruv Batra, and Devi Parikh. 2017. Making\nthe v in vqa matter: Elevating the role of im-\nage understanding in visual question answer-\ning. In Proceedings of the IEEE Conference\non Computer Vision and Pattern Recognition ,\npages 6904–6913.\nJohn Hewitt and Percy Liang. 2019. Designing\nand interpreting probes with control tasks. In\nProceedings of the 2019 Conference on Em-\npirical Methods in Natural Language Process-\ning and the 9th International Joint Conference\non Natural Language Processing (EMNLP-\nIJCNLP), pages 2733–2743.\nZhicheng Huang, Zhaoyang Zeng, Bei Liu, Dong-\nmei Fu, and Jianlong Fu. 2020. Pixel-\nbert: Aligning image pixels with text by\ndeep multi-modal transformers. arXiv preprint\narXiv:2004.00849.\nAndrej Karpathy and Li Fei-Fei. 2015. Deep\nvisual-semantic alignments for generating im-\nage descriptions. In Proceedings of the IEEE\nconference on computer vision and pattern\nrecognition, pages 3128–3137.\nRyan Kiros, Ruslan Salakhutdinov, and Richard S\nZemel. 2014. Unifying visual-semantic embed-\ndings with multimodal neural language models.\narXiv preprint arXiv:1411.2539.\nRanjay Krishna, Yuke Zhu, Oliver Groth, Justin\nJohnson, Kenji Hata, Joshua Kravitz, Stephanie\nChen, Yannis Kalantidis, Li-Jia Li, David A\nShamma, et al. 2017. Visual genome: Connect-\ning language and vision using crowdsourced\ndense image annotations. International journal\nof computer vision, 123(1):32–73.\nTaku Kudo and John Richardson. 2018. Sentence-\npiece: A simple and language independent sub-\nword tokenizer and detokenizer for neural text\nprocessing. arXiv preprint arXiv:1808.06226.\nAlina Kuznetsova, Hassan Rom, Neil Alldrin,\nJasper Uijlings, Ivan Krasin, Jordi Pont-Tuset,\nShahab Kamali, Stefan Popov, Matteo Malloci,\nTom Duerig, et al. 2018. The open images\ndataset v4: Uniﬁed image classiﬁcation, object\ndetection, and visual relationship detection at\nscale. arXiv preprint arXiv:1811.00982.\nGen Li, Nan Duan, Yuejian Fang, Ming Gong, and\nDaxin Jiang. 2020a. Unicoder-vl: A universal\nencoder for vision and language by cross-modal\npre-training. In The Thirty-Fourth AAAI Con-\nference on Artiﬁcial Intelligence, AAAI.\nLiunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui\nHsieh, and Kai-Wei Chang. 2019. Visualbert:\nA simple and performant baseline for vision and\nlanguage. arXiv preprint arXiv:1908.03557.\nXiujun Li, Xi Yin, Chunyuan Li, Xiaowei Hu,\nPengchuan Zhang, Lei Zhang, Lijuan Wang,\nHoudong Hu, Li Dong, Furu Wei, et al. 2020b.\nOscar: Object-semantics aligned pre-training\nfor vision-language tasks. In European Confer-\nence on Computer Vision.\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan\nLee. 2019. Vilbert: Pretraining task-agnostic\nvisiolinguistic representations for vision-and-\nlanguage tasks. In Advances in Neural Infor-\nmation Processing Systems, pages 13–23.\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi\nParikh. 2016. Hierarchical question-image co-\nattention for visual question answering. In Ad-\nvances in neural information processing sys-\ntems, pages 289–297.\nAntoine Miech, Jean-Baptiste Alayrac, Lucas\nSmaira, Ivan Laptev, Josef Sivic, and Andrew\nZisserman. 2020. End-to-End Learning of Vi-\nsual Representations from Uncurated Instruc-\ntional Videos. In Computer Vision and Pattern\nRecognition.\nAntoine Miech, Ivan Laptev, and Josef Sivic.\n2018. Learning a Text-Video Embedding from\nIncomplete and Heterogeneous Data. arXiv\npreprint arXiv:1804.02516.\nAntoine Miech, Dimitri Zhukov, Jean-Baptiste\nAlayrac, Makarand Tapaswi, Ivan Laptev, and\nJosef Sivic. 2019. Howto100m: Learning\na text-video embedding by watching hundred\nmillion narrated video clips. In Proceedings of\nthe IEEE international conference on computer\nvision, pages 2630–2640.\nAaron van den Oord, Yazhe Li, and Oriol\nVinyals. 2018. Representation learning with\ncontrastive predictive coding. arXiv preprint\narXiv:1807.03748.\nVicente Ordonez, Girish Kulkarni, and Tamara L\nBerg. 2011. Im2text: Describing images using\n1 million captioned photographs. In Advances\nin neural information processing systems, pages\n1143–1151.\nJordi Pont-Tuset, Jasper Uijlings, Soravit Chang-\npinyo, Radu Soricut, and Vittorio Ferrari. 2019.\nConnecting vision and language with localized\nnarratives. arXiv preprint arXiv:1912.03098.\nShaoqing Ren, Kaiming He, Ross Girshick, and\nJian Sun. 2015. Faster r-cnn: Towards real-time\nobject detection with region proposal networks.\nIn Advances in neural information processing\nsystems, pages 91–99.\nPiyush Sharma, Nan Ding, Sebastian Goodman,\nand Radu Soricut. 2018. Conceptual captions:\nA cleaned, hypernymed, image alt-text dataset\nfor automatic image captioning. In Proceedings\nof the 56th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long\nPapers), pages 2556–2565.\nGunnar A Sigurdsson, Jean-Baptiste Alayrac,\nAida Nematzadeh, Lucas Smaira, Mateusz Ma-\nlinowski, João Carreira, Phil Blunsom, and An-\ndrew Zisserman. 2020. Visual grounding in\nvideo for unsupervised word translation. In\nComputer Vision and Pattern Recognition.\nAmanpreet Singh, Vedanuj Goswami, and Devi\nParikh. 2020. Are we pretraining it right?\ndigging deeper into visio-linguistic pretraining.\narXiv preprint arXiv:2004.08744.\nChen Sun, Austin Myers, Carl V ondrick,\nKevin Murphy, and Cordelia Schmid. 2019.\nVideobert: A joint model for video and lan-\nguage representation learning. In Proceedings\nof the IEEE International Conference on\nComputer Vision, pages 7464–7473.\nHao Tan and Mohit Bansal. 2019. Lxmert:\nLearning cross-modality encoder representa-\ntions from transformers. In Empirical Methods\nin Natural Language Processing.\nDamien Teney, Peter Anderson, Xiaodong He,\nand Anton Van Den Hengel. 2018. Tips and\ntricks for visual question answering: Learnings\nfrom the 2017 challenge. In Proceedings of the\nIEEE conference on computer vision and pat-\ntern recognition, pages 4223–4232.\nYonglong Tian, Dilip Krishnan, and Phillip Isola.\n2019. Contrastive multiview coding. arXiv\npreprint arXiv:1906.05849.\nAshish Vaswani, Noam Shazeer, Niki Parmar,\nJakob Uszkoreit, Llion Jones, Aidan N Gomez,\nŁukasz Kaiser, and Illia Polosukhin. 2017. At-\ntention is all you need. In Advances in neural\ninformation processing systems , pages 5998–\n6008.\nElena V oita and Ivan Titov. 2020. Information-\ntheoretic probing with minimum description\nlength. arXiv preprint arXiv:2003.12298.\nLiwei Wang, Yin Li, and Svetlana Lazebnik. 2016.\nLearning deep structure-preserving image-text\nembeddings. In CVPR.\nJason Weston, Samy Bengio, and Nicolas Usunier.\n2011. Wsabie: Scaling up to large vocabulary\nimage annotation. In IJCAI.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime\nCarbonell, Russ R Salakhutdinov, and Quoc V\nLe. 2019. Xlnet: Generalized autoregressive\npretraining for language understanding. In Ad-\nvances in neural information processing sys-\ntems, pages 5753–5763.\nDani Yogatama, Cyprien de Masson d’Autume,\nJerome Connor, Tomas Kocisky, Mike\nChrzanowski, Lingpeng Kong, Angeliki\nLazaridou, Wang Ling, Lei Yu, Chris Dyer,\net al. 2019. Learning and evaluating gen-\neral linguistic intelligence. arXiv preprint\narXiv:1901.11373.\nYang You, Jing Li, Sashank Reddi, Jonathan Hseu,\nSanjiv Kumar, Srinadh Bhojanapalli, Xiaodan\nSong, James Demmel, Kurt Keutzer, and Cho-\nJui Hsieh. 2019. Large batch optimization for\ndeep learning: Training bert in 76 minutes.\narXiv preprint arXiv:1904.00962.\nPeter Young, Alice Lai, Micah Hodosh, and Ju-\nlia Hockenmaier. 2014. From image descrip-\ntions to visual denotations: New similarity met-\nrics for semantic inference over event descrip-\ntions. Transactions of the Association for Com-\nputational Linguistics, 2:67–78.\nKelly Zhang and Samuel Bowman. 2018. Lan-\nguage modeling teaches you more than trans-\nlation does: Lessons learned through auxiliary\nsyntactic task analysis. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyz-\ning and Interpreting Neural Networks for NLP,\npages 359–361.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.697620153427124
    },
    {
      "name": "Transformer",
      "score": 0.6894591450691223
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4956098794937134
    },
    {
      "name": "Multimodal learning",
      "score": 0.4545546770095825
    },
    {
      "name": "Machine learning",
      "score": 0.430853009223938
    },
    {
      "name": "Natural language processing",
      "score": 0.3204542398452759
    },
    {
      "name": "Engineering",
      "score": 0.09284502267837524
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I1291425158",
      "name": "Google (United States)",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210090411",
      "name": "DeepMind (United Kingdom)",
      "country": "GB"
    }
  ],
  "cited_by": 12
}