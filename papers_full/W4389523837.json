{
    "title": "Who Wrote it and Why? Prompting Large-Language Models for Authorship Verification",
    "url": "https://openalex.org/W4389523837",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2887242805",
            "name": "Chia-Yu Hung",
            "affiliations": [
                "Singapore University of Technology and Design"
            ]
        },
        {
            "id": "https://openalex.org/A2115625708",
            "name": "Zhiqiang Hu",
            "affiliations": [
                "Singapore University of Technology and Design"
            ]
        },
        {
            "id": "https://openalex.org/A2097506176",
            "name": "Yujia Hu",
            "affiliations": [
                "Singapore University of Technology and Design"
            ]
        },
        {
            "id": "https://openalex.org/A2120552592",
            "name": "Roy Lee",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2035395173",
        "https://openalex.org/W2103577799",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W2963579919",
        "https://openalex.org/W3004919414",
        "https://openalex.org/W3035225773",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2069985874",
        "https://openalex.org/W3172943453",
        "https://openalex.org/W4383199647",
        "https://openalex.org/W3197567090",
        "https://openalex.org/W4380986449",
        "https://openalex.org/W2887691909",
        "https://openalex.org/W2523437799",
        "https://openalex.org/W3197001178",
        "https://openalex.org/W2048753011",
        "https://openalex.org/W2963462252",
        "https://openalex.org/W3008918149",
        "https://openalex.org/W2806154504"
    ],
    "abstract": "Authorship verification (AV) is a fundamental task in natural language processing (NLP) and computational linguistics, with applications in forensic analysis, plagiarism detection, and identification of deceptive content. Existing AV techniques, including traditional stylometric and deep learning approaches, face limitations in terms of data requirements and lack of explainability. To address these limitations, this paper proposes PromptAV, a novel technique that leverages Large-Language Models (LLMs) for AV by providing step-by-step stylometric explanation prompts. PromptAV outperforms state-of-the-art baselines, operates effectively with limited training data, and enhances interpretability through intuitive explanations, showcasing its potential as an effective and interpretable solution for the AV task.",
    "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 14078–14084\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nWho Wrote it and Why?\nPrompting Large-Language Models for Authorship Verification\nChia-Yu Hung, Zhiqiang Hu, Yujia Hu, Roy Ka-Wei Lee\nSingapore University of Technology and Design\n{chiayu_hung, zhiqiang_hu}@mymail.sutd.edu.sg\n{yujia_hu, roy_lee}@sutd.edu.sg\nAbstract\nAuthorship verification (A V) is a fundamen-\ntal task in natural language processing (NLP)\nand computational linguistics, with applica-\ntions in forensic analysis, plagiarism detection,\nand identification of deceptive content. Exist-\ning A V techniques, including traditional sty-\nlometric and deep learning approaches, face\nlimitations in terms of data requirements and\nlack of explainability. To address these limita-\ntions, this paper proposes PromptAV, a novel\ntechnique that leverages Large-Language Mod-\nels (LLMs) for A V by providing step-by-step\nstylometric explanation prompts. PromptAV\noutperforms state-of-the-art baselines, operates\neffectively with limited training data, and en-\nhances interpretability through intuitive expla-\nnations, showcasing its potential as an effective\nand interpretable solution for the A V task.\n1 Introduction\nMotivation. Authorship verification (A V) is a fun-\ndamental task in the field of natural language pro-\ncessing (NLP). It aims to determine the authorship\nof a given text by analyzing the stylistic character-\nistics and patterns exhibited in the writing (Sta-\nmatatos, 2016). The importance of A V lies in\nits wide range of applications, including foren-\nsic analysis (Iqbal et al., 2010), plagiarism detec-\ntion (Stein et al., 2011), and the detection of decep-\ntive or fraudulent content (Claxton, 2005). The rise\nof Large-Language Models (LLMs) (Chowdhery\net al., 2022; Brown et al., 2020), has facilitated\nthe generation of human-like text where one would\nnow need to factor in the possibility of machine-\ngenerated text (Uchendu et al., 2023). This de-\nvelopment give rise to a new layer of complexity\nin distinguishing between machine-generated and\nhuman-written text, subsequently amplifying the\nsignificance of the A V problem.\nOver the years, various techniques have been\nproposed to address the A V task. Traditional ap-\nproaches relied on stylometric features, such as\nn-gram frequencies, vocabulary richness, and syn-\ntactic structures, to capture the unique writing style\nof individual authors (Ding et al., 2017). Machine\nlearning techniques, such as support vector ma-\nchines (SVMs) and random forests, have been em-\nployed to model the relationship between these sty-\nlometric features and authorship (Brocardo et al.,\n2013). More recently, deep learning approaches\nhave shown promising A V results by learning intri-\ncate patterns directly from raw text data (Bagnall,\n2015; Ruder et al., 2016; Fabien et al., 2020).\nDespite these advancements, existing A V tech-\nniques have certain limitations. Firstly, most exist-\ning A V methods require a large amount of labeled\ntraining data, which can be costly to acquire, es-\npecially for scenarios with limited available data.\nSecondly, there is a lack of explainability in the\npredictions made by these techniques. In many\npractical applications, it is crucial to understand\nwhy a particular decision was reached, which is\nparticularly relevant in legal contexts or situations\nwhere interpretability is paramount.\nResearch Objectives. To address these limita-\ntions, We propose PromptAV, a novel technique\nthat utilizes LLMs for authorship attribution by\nemploying step-by-step stylometric explanation\nprompts. These prompts guide the LLMs to an-\nalyze and evaluate the textual features that con-\ntribute to authorship, enabling the model to provide\nexplanations for its predictions. PromptAV demon-\nstrates effectiveness in both zero-shot and few-shot\nsettings, offering improved performance compared\nto existing baselines, especially in scenarios with\nlimited training data. Additionally, detailed case\nstudies showcase PromptAV’s ability to provide\nintuitive explanations for its predictions, shedding\nlight on the factors contributing to authorship attri-\nbution and enhancing the interpretability of the A V\nprocess.\nContribution. This paper introduces Promp-\ntAV, a prompt-based learning technique that har-\n14078\nTask: On a scale of 0 to 1, with 0 indicating low\nconfidence and 1 indicating high confidence, please\nprovide a general assessment of the likelihood that\nText 1 and Text 2 were written by the same author.\nYour answer should reflect a moderate level of\nstrictness in scoring. Let's think step by step.\nText 1: [T1]\nText 2: [T2]\nChain -of -Thought prompting\nTask: On a scale of 0 to 1, with 0 indicating low\nconfidence and 1 indicating high confidence, please\nprovide a general assessment of the likelihood that\nText 1 and Text 2 were written by the same author.\nYour answer should reflect a moderate level of\nstrictness in scoring. First step: Understand the\nproblem, extracting relevant variables and devise a\nplan to solve the problem. Then, carry out the plan\nand solve the problem step by step. Finally, show the\nconfidence score.\nText 1: [T1]\nText 2: [T2]\nPS+ prompting\nTask: On a scale of 0 to 1, with 0 indicating low confidence\nand 1 indicating high confidence, please provide a general\nassessment of the likelihood that Text 1 and Text 2 were\nwritten by the same author. Your answer should reflect a\nmoderate level of strictness in scoring. Here are some\nrelevant variables to this problem.\n1. punctuation style(e.g. hyphen, brackets, colon, comma,\nparenthesis, quotation mark)\n2. special characters style, capitalization style(e.g.\nContinuous capitalization, capitalizing certain words)\n3. acronyms and abbreviations(e.g. Usage of acronyms\nsuch as OMG, Abbreviations without punctuation marks\nsuch as Mr Rochester vs. Mr. Rochester,Unusual\nabbreviations such as def vs. definitely)\n4. writing style\n5. expressions and Idioms\n6. tone and mood\n7. sentence structure\n8. any other relevant aspect\nFirst step: Understand the problem, extracting relevant\nvariables and devise a plan to solve the problem. Then,\ncarry out the plan and solve the problem step by step.\nFinally, show the confidence score.\nText 1: [T1]\nText 2: [T2]\nPromptAV\n(a)\n(b)\n(c)\nFigure 1: Prompts used by (a) CoT, (b) PS+ prompting, (c) PromptA V for a A V task. The trigger intructions of the\nvarious techniques are highlighted in the prompt.\nnesses the linguistics capabilities of LLMs for A V .\nBy providing step-by-step stylometric explanation\nprompts, PromptAV achieves improved perfor-\nmance, offers explainability in its predictions, and\noperates effectively in both zero-shot and few-shot\nsettings. The experimental results demonstrate the\npotential of PromptAV as an effective and inter-\npretable solution for the A V task.\n2 Related Work\nA V is a well-established research topic. Classi-\ncal approaches to A V have predominantly cen-\ntered on stylometric features encompassing n-gram\nfrequencies, lexical diversity, and syntactic struc-\ntures to discern the unique writing styles intrinsic\nto individual authors (Stamatatos, 2016; Lagutina\net al., 2019). Classical machine learning algorithms\nwere also used to model the associations between\nthese stylometric attributes and authorship (Bro-\ncardo et al., 2013; Hu et al., 2023). Recently, deep\nlearning models such as RNNs and CNNs being\nemployed to extract more complex patterns from\ntextual data (Bagnall, 2015; Ruder et al., 2016;\nBenzebouchi et al., 2018). These deep learning\narchitectures have exhibited promising advance-\nments in A V tasks. Furthermore, the advent of the\nTransformer architecture (Vaswani et al., 2017) has\nprompted the development of Transformer-based\nA V models (Ordoñez et al., 2020; Najafi and Tavan,\n2022; Tyo et al., 2021).\n3 Methodology\nIn an recent work, Wei et al. (2023) have proposed\nChain of thought (CoT) prompting, which employs\na series of intermediate reasoning steps to signif-\nicantly improve the ability of LLMs in perform-\ning complex reasoning. Wang et al. (2023) fur-\nther extended the concept to propose PS+ prompt-\ning, a zero-shot prompting methodology that in-\nstructs LLMs to formulate a strategic plan prior to\nproblem-solving.\nInspired by these works, we introduce Promp-\ntAV, a prompting strategy that incorporates key\nlinguistic features—identified in (Boenninghoff\net al., 2019) as the crucial variables for LLMs to\nevaluate. These linguistic features serve as rich,\noften subtle, markers of an author’s distinct writ-\ning style and are hence of paramount importance\nfor the A V task. For instance, the consistent use\nor avoidance of certain punctuation marks, or the\npreference for particular special characters, can\nbe idiosyncratic to an author. Similarly, elements\n14079\nlike tone and expression, though harder to quan-\ntify, can offer significant insights into an author’s\nunique ‘voice’. The integration of these linguistic\nattributes within the PromptAV framework does\nnot simply add layers of complexity to the model,\nit fundamentally enhances the model’s ability to\ncapture the author-specific nuances within textual\ndata. This capability, in turn, improves the model’s\naccuracy and reliability when attributing authorship\nto unidentified texts, making PromptAV a power-\nful tool in the field of A V . Figure 1 provides an\noverview that compares the prompts used in CoT,\nPS+ prompting, and PromptAV.\nWe also note that conventional prompting tech-\nniques that instruct the LLM to respond strictly\nwith binary “yes” or “no” answers frequently result\nin the LLM responding ‘ no” for the majority of\ninstances within an A V task. To mitigate this prob-\nlem, PromptAV instructs the LLM to generate a\nconfidence score ranging from 0 to 1, rather than a\nbinary response. To calibrate these generated con-\nfidence scores, we augment the prompt with the\nadditional directive “Your answer should reflect a\nmoderate level of strictness in scoring”.\n3.1 Few Shot Prompting\nIn the conventional setup of few-shot CoT prompt-\ning, each example necessitates the manual crafting\nof reasoning steps, an approach that is impracti-\ncal for the A V task. This is primarily due to the\nchallenges in ensuring the consistent quality of\nhand-crafted reasoning across diverse prompting\ntemplates. Hence, we resort to leveraging the ca-\npacity of LLMs as zero-shot reasoners to generate\nthe required reasoning steps (Kojima et al., 2023).\nTo construct the few-shot PromptAV and CoT\nexamples, we introduce an additional directive to\nthe original prompt: “It is given that after follow-\ning the instruction, the confidence score obtained\nis [X]. Show the step-by-step execution of the in-\nstruction so that this score is achieved.” Here, [X]\nis either 0.9 or 0.1, contingent upon whether the\npair of texts were written by the same author. Sub-\nsequently, we feed this modified prompt to GPT-41\nto generate intermediate reasoning steps.\nZhao et al. (Zhao et al., 2021) has demonstrated\nthat both the selection of training examples and\ntheir sequencing can significantly influence accu-\nracy.Therefore, to ensure fairness, we utilized the\nsame examples for all our 2-shot and 4-shot exper-\n1https://openai.com/research/gpt-4\nSetting Model Accuracy\n0-Shot PromptA V 0.587\nPS+ prompting 0.536\nCoT 0.524\n2-Shot PromptA V 0.667\nCoT 0.595\n4-shot PromptA V 0.635\nCoT 0.510\nTable 1: Performance of PromptAV and the baselines\non k-shots settings. Highest accuracy are bolded.\niments. These examples were randomly sampled\nfrom the training set and presented to the LLM in\nthe same order.\n4 Experiments\n4.1 Experimental Settings\nDataset. We evaluate PromptA V and the baselines\nusing IMDb62 (Seroussi et al., 2014), which is\ncommonly used dataset for A V task. Each record\nin dataset contains a pair of texts and the ground\ntruth labels, i.e., a “positive” pair of texts belong to\nthe same author, and “negative” for otherwise. As\nwe are interested in performing the A V task in zero-\nshot and few-shot settings, we focus on sampling\na manageable test set of 1,000 text pairs from the\noriginal IMDb62 dataset. The resulting test set\ncomprises 503 positive and 497 negative text pairs.\nImplementation. All of our experiments were\nconducted on the gpt-3.5-turbo model, with the\ntemperature parameter set to 0. We benchmarked\nPromptAV against PS+ prompting in zero-shot\nsettings and against CoT prompting in zero, two,\nand four-shot settings. Given that all three methods\ngenerate a confidence score in the range of 0 to\n1, we reported the accuracy corresponding to the\noptimal threshold that yields the highest accuracy.\nWe call this the Optimal Threshold Methodology\nand we will dicuss more about it in Appendix A.3.\n4.2 Experimental Results\nTable 1 shows the performance of PromptAV and\nthe baselines on k-shot settings. We observe that\nPromptAV consistently achieved higher accuracy\nacross varying shot settings. This demonstrate\nPromptAV’s efficacy in performing A V task with\nlittle or no training data. Furthermore, as the num-\nber of shots increases from 0 to 4, PromptAV\nmaintains its edge over the CoT. This suggests that\nPromptAV effectively leverages the additional in-\nformation provided in the few-shot scenarios, opti-\nmizing its decision-making process.\n14080\nText 1: \" We’re all sinners , or about to be . \" I picked up this flick as part of a double film DVD offer along with \" The\nHanged Man \" for just a buck , and truth be told , both movies were a whole lot better than either had a right to be for the\nprice . The title on this one was \" The Gun and The Cross \" , close enough to the original to get the message across . The ti-\ntle character wields both , as Marjoe Gortner is revealed to be gunfighter Ernie Parsons , masquerading as a preacher after\nhe finds a real one who was bushwhacked . Donning the clothes of the dead minister , he arrives at the town of Castle Walk\njust in time to help the local folk deal with nasty town boss Ross ( David Huddleston ) ...[Truncated due to length restriction]\nText 2: \" This is a mad dream Antinea , but I shall put an end to the madness . \" The title \" Hercules and the\nCaptive Women \" is a bit of a misnomer , there was only one woman captive , and along with her mother , Queen Antinea\nof Atlantis , were the only two women in the entire film . The International English title mentions the ’ Conquest of Atlantis\n’ , and that probably makes more sense , but in the end , it’s the destruction of Atlantis that takes center stage . There’s an\nentire mythology offered by Zantas , the last high priest of Uranus , that explains what’s going on with Antinea and her\ncohort of Grand Vizier clones . ... [Truncated due to length restriction]\nPromptType Generated Explanation\n0-shot PromptA V Based on the provided texts, here is a general assessment of the likelihood that Text 1 and Text 2\nwere written by the same author:\n1. Punctuation style: Both texts use similar punctuation styles, such as commas, periods, and\nquotation marks. However, Text 1 uses more parentheses and dashes compared to Text 2.\nConfidence: 0.7\n2. Special characters style, capitalization style: Both texts have similar capitalization styles, with\nproper nouns and the beginning of sentences capitalized. However, Text 1 has more continuous\ncapitalization (e.g., \"The Gun and The Cross\") compared to Text 2. Confidence: 0.6\n3. Acronyms and abbreviations: Both texts use acronyms and abbreviations, but there are no\nsignificant differences in their usage. Confidence: 0.8\n4. Writing style: Both texts have a similar writing style, with a mix of descriptive and narrative\nelements. However, Text 1 focuses more on reviewing a movie, while Text 2 provides a summary\nof a film’s plot. Confidence: 0.7\n5. V ocabulary usage: Both texts use a variety of vocabulary, but there are no significant\ndifferences in their usage. Confidence: 0.8\nOverall, based on the assessment of the relevant variables, there is a moderate likelihood\n(confidence score: 0.7) that Text 1 and Text 2 were written by the same author.\n0-shot CoT Based on a moderate level of strictness in scoring, the likelihood that Text 1 and Text 2 were\nwritten by the same author is low. The two texts have different subject matters and writing styles,\nwith Text 1 discussing a western film and Text 2 discussing a Hercules film. Additionally, the\ntone and language used in each text are distinct. Therefore, it is unlikely that the same author\nwrote both texts.\nBased on these factors, I would assess the likelihood that Text 1 and Text 2 were written by the\nsame author as low, with a confidence level of around 0.3 on the scale of 0 to 1.\n0-shot PS+ prompting Based on a moderate level of strictness in scoring, I would assess the likelihood that Text 1 and\nText 2 were written by the same author as low. The two texts have different themes, writing\nstyles, and subject matters. Text 1 discusses a western film, while Text 2 discusses a Hercules\nfilm. The language and tone used in each text are also distinct. Therefore, I would assign a\nconfidence score of 0.2, indicating a low likelihood of the same authorship.\nTable 2: A text pair example with the generated responses from PromptAV, CoT and PS+ Prompting\nModel Accuracy\nPromptA V 0.587\nPromptA V_Obfuscated 0.580\nPS+ prompting 0.536\nCoT 0.524\nTable 3: Performance of PromptAV against a series of\nauthorship obfuscation methods. Highest accuracy are\nbolded\nThe superior performance of PromptAV over\nPS+ prompting and CoT in the zero-shot setting un-\nderscores its capacity to make effective use ofkey\nlinguistic features for A V even in the absence of\ntraining examples. This is indicative of the fact\nthat PromptAV is adept at understanding intrin-\nsic linguistic characteristics, which are critical in\ndifferentiating writing styles of various authors.\n4.3 Explainability Case Studies\nWhile achieving explainability in the A V task is\nnotoriously difficult, PromptAV rises to the chal-\nlenge by providing detailed, interpretable solutions.\nThis is accomplished through a meticulous anal-\nysis of the linguistic features. A sample of the\ngenerated responses from PromptAV, CoT, and\nPS+ prompting is illustrated in Table 2. In contrast\nto CoT prompting or PS+ prompting, PromptAV\noffers a comprehensive comparison of linguistic\nfeatures, such as punctuation style and vocabulary\nusage. For example, PromptAV detects that both\n14081\ntexts have similar capitalization style and writing\nstyle, with a mix of descriptive and narrative ele-\nments. On the other hand, CoT and PS+ prompting\nsystems deliver a more superficial level of analysis.\nWhile they can recognize discrepancies in vocab-\nulary selection across texts, they lack the capacity\nto provide an exhaustive explanation or a deeper\nanalysis of these differences.\n4.4 Authorship obfuscation\nIn our pursuit to understand PromptAV’spotential\nrobustness, we took a focused approach by testing it\nagainst an obfuscation dataset. We selected the sug-\ngested Mutant-X algorithm (Mahmood et al., 2020),\na recognized method in the authorship obfuscation\ndomain, and applied it to the IMDb62 dataset that\nwas referenced in our paper. We apply PromptA V\nin zero-shot setting on the obfuscated dataset and\nbenchmark against its performance on the original\ndataset. The experimental results are presented in\nTable 3. The results reveal a negligible decline\nin performance when confronted with obfuscated\ntext. Interestingly, the accuracy achieved, even\nwith obfuscation, surpasses the results of zero-shot\nCoT and PS+ prompting on non-obfuscated text.\nThis indicates a promising level of resilience in\nPromptA V against authorship obfuscation methods.\nWhile these preliminary results are encouraging,\nwe acknowledge the need for more comprehensive\ntesting against a broader range of obfuscation meth-\nods to assert PromptAV’srobustness conclusively.\n4.5 Stylometric Feature Impact\nThe field of stylometry is vast, with a plethora of\nfeatures available for authorship analysis. The cen-\ntral aim of PromptAV was to showcase the flexibil-\nity and adaptability of the framework when work-\ning with different sets of stylometric features. We\nconducted a series of experiments using varying\nsets of stylometric features withPromptAV in zero-\nshot setting. We report the result in Table 4. The re-\nsult clearly showcases that the performance varies\nbased on the chosen feature set. The peak accu-\nracy achieved with a 9-feature set emphasizes that\nPromptAV’s success is contingent on the quality\nand appropriateness of the selected features. We\ngenuinely believe that the field holds significant\nundiscovered potential, with combinations of fea-\ntures that could further elevate the performance of\nPromptAV. Our results only scratch the surface,\nand our intention is to spur the community into\nexploring these myriad possibilities.\n5 Conclusion\nIn this paper, we proposed PromptAV, a prompt-\nbased learning technique that harnesses the lin-\nguistics capabilities of LLMs for A V . Our experi-\nmental results demonstrated PromptAV’s superior\nperformance over the state-of-the-art baselines in\nzero-shot and few-shot settings. Case studies were\nalso conducted to illustrate PromptAV’s effective-\nness and explainability for the A V task. For future\nworks, we will conduct experiments on more A V\ndatasets and explore other linguistic features.\n6 Limitations\nThis study, while substantive, acknowledges cer-\ntain limitations. A significant constraint is the ab-\nsence of a ground truth for the generated explana-\ntion, which renders the evaluation of the efficacy\nof PromptAV’s generated explanation somewhat\nchallenging. This suggests a potential avenue for\ntapping into the expertise of forensic linguistic ex-\nperts to assess and curate some of the reasoning\ngenerated by PromptAV. Additionally, we have\nobserved instances where PromptA V generates illu-\nsory explanations by identifying the usage of spe-\ncific vocabulary within the text, even when such\nvocabulary may not have been actually used.\nReferences\nDouglas Bagnall. 2015. Author identification us-\ning multi-headed recurrent neural networks. arXiv\npreprint arXiv:1506.04891.\nNacer Eddine Benzebouchi, Nabiha Azizi, Monther Ald-\nwairi, and Nadir Farah. 2018. Multi-classifier system\nfor authorship verification task using word embed-\ndings. In 2018 2nd International Conference on Nat-\nural Language and Speech Processing (ICNLSP) ,\npages 1–6. IEEE.\nBenedikt Boenninghoff, Steffen Hessler, Dorothea\nKolossa, and Robert M Nickel. 2019. Explainable\nauthorship verification in social media via attention-\nbased similarity learning. In 2019 IEEE Interna-\ntional Conference on Big Data (Big Data) , pages\n36–45. IEEE.\nMarcelo Luiz Brocardo, Issa Traore, Sherif Saad, and\nIsaac Woungang. 2013. Authorship verification\nfor short messages using stylometry. In 2013 In-\nternational Conference on Computer, Information\nand Telecommunication Systems (CITS), pages 1–6.\nIEEE.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\n14082\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, Parker Schuh, Kensen Shi,\nSasha Tsvyashchenko, Joshua Maynez, Abhishek\nRao, Parker Barnes, Yi Tay, Noam Shazeer, Vin-\nodkumar Prabhakaran, Emily Reif, Nan Du, Ben\nHutchinson, Reiner Pope, James Bradbury, Jacob\nAustin, Michael Isard, Guy Gur-Ari, Pengcheng Yin,\nToju Duke, Anselm Levskaya, Sanjay Ghemawat,\nSunipa Dev, Henryk Michalewski, Xavier Garcia,\nVedant Misra, Kevin Robinson, Liam Fedus, Denny\nZhou, Daphne Ippolito, David Luan, Hyeontaek Lim,\nBarret Zoph, Alexander Spiridonov, Ryan Sepassi,\nDavid Dohan, Shivani Agrawal, Mark Omernick, An-\ndrew M. Dai, Thanumalayan Sankaranarayana Pil-\nlai, Marie Pellat, Aitor Lewkowycz, Erica Moreira,\nRewon Child, Oleksandr Polozov, Katherine Lee,\nZongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark\nDiaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy\nMeier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov,\nand Noah Fiedel. 2022. Palm: Scaling language mod-\neling with pathways.\nLarry D Claxton. 2005. Scientific authorship: Part\n1. a window into scientific fraud? Mutation Re-\nsearch/Reviews in Mutation Research, 589(1):17–30.\nSteven HH Ding, Benjamin CM Fung, Farkhund Iqbal,\nand William K Cheung. 2017. Learning stylometric\nrepresentations for authorship analysis. IEEE trans-\nactions on cybernetics, 49(1):107–121.\nMaël Fabien, Esau Villatoro-Tello, Petr Motlicek, and\nShantipriya Parida. 2020. BertAA : BERT fine-\ntuning for authorship attribution. In Proceedings\nof the 17th International Conference on Natural Lan-\nguage Processing (ICON), pages 127–137, Indian\nInstitute of Technology Patna, Patna, India. NLP As-\nsociation of India (NLPAI).\nXinyu Hu, Weihan Ou, Sudipta Acharya, Steven H Ding,\nRyan D’Gama, and Hanbo Yu. 2023. Tdrlm: Stylo-\nmetric learning for authorship verification by topic-\ndebiasing. Expert Systems with Applications, page\n120745.\nFarkhund Iqbal, Liaquat A Khan, Benjamin CM Fung,\nand Mourad Debbabi. 2010. E-mail authorship ver-\nification for forensic investigation. In Proceedings\nof the 2010 ACM Symposium on Applied computing,\npages 1591–1598.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2023. Large lan-\nguage models are zero-shot reasoners.\nKsenia Lagutina, Nadezhda Lagutina, Elena Boychuk,\nInna V orontsova, Elena Shliakhtina, Olga Belyaeva,\nIlya Paramonov, and PG Demidov. 2019. A survey on\nstylometric text features. In 2019 25th Conference\nof Open Innovations Association (FRUCT) , pages\n184–195. IEEE.\nAsad Mahmood, Zubair Shafiq, and Padmini Srinivasan.\n2020. A girl has a name: Detecting authorship obfus-\ncation.\nMaryam Najafi and Ehsan Tavan. 2022. Text-to-text\ntransformer in authorship verification via stylistic and\nsemantical analysis. In Proceedings of the CLEF.\nJuanita Ordoñez, Rafael Rivera Soto, and Barry Y Chen.\n2020. Will longformers pan out for authorship verifi-\ncation. Working Notes of CLEF.\nSebastian Ruder, Parsa Ghaffari, and John G Breslin.\n2016. Character-level and multi-channel convolu-\ntional neural networks for large-scale authorship at-\ntribution. arXiv preprint arXiv:1609.06686.\nYanir Seroussi, Ingrid Zukerman, and Fabian Bohn-\nert. 2014. Authorship attribution with topic models.\nComputational Linguistics, 40(2):269–310.\nEfstathios Stamatatos. 2016. Authorship verification: a\nreview of recent advances. Research in Computing\nScience, 123:9–25.\nBenno Stein, Nedim Lipka, and Peter Prettenhofer. 2011.\nIntrinsic plagiarism analysis. Language Resources\nand Evaluation, 45:63–82.\nJacob Tyo, Bhuwan Dhingra, and Zachary C Lipton.\n2021. Siamese bert for authorship verification. In\nCLEF (Working Notes), pages 2169–2177.\nAdaku Uchendu, Thai Le, and Dongwon Lee. 2023.\nAttribution and obfuscation of neural text authorship:\nA data mining perspective.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. Advances in neural information processing\nsystems, 30.\nLei Wang, Wanyu Xu, Yihuai Lan, Zhiqiang Hu, Yunshi\nLan, Roy Ka-Wei Lee, and Ee-Peng Lim. 2023. Plan-\nand-solve prompting: Improving zero-shot chain-of-\nthought reasoning by large language models.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and\nDenny Zhou. 2023. Chain-of-thought prompting elic-\nits reasoning in large language models.\nTony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and\nSameer Singh. 2021. Calibrate before use: Improv-\ning few-shot performance of language models.\n14083\nA Appendix\nA.1 Computation and Time comsumption of\nPromptA V\nIn our experiments, we applied PromptA V using\nGPT-3.5 via the OpenAI API. Specifically, evalu-\nating 1,000 examples took approximately 8 hours.\nThe expense associated with using PromptA V via\nthe OpenAI API is indeed calculated at a rate of\n$0.004 per 1,000 tokens. For a dataset of 1,000\nsamples, the estimated cost came to around $10.\nThis cost, we believe, is justifiable given the unique\ninterpretability insights PromptA V provides, espe-\ncially in critical applications like forensic analysis\nwhere the implications of the results are profound.\nIt is possible to batch-process texts through sending\nmultiple requests using multiple API keys. With\nadequate budget, we could scale-up the A V opera-\ntions.\nA.2 Stylometric Feature Impact\nNumber of features Accuracy\n8 0.587\n9 0.602\n10 0.585\n11 0.564\n12 0.572\nTable 4: Performance of PromptA V in 0 shot setting\nwith varying sets of stylometric features.\nA.3 Optimal Threshold Methodology\nThe method of reporting using an optimal threshold\nwas our initial approach to compare the results of\ndifferent prompting methods. The intention was\nto establish a baseline comparison across varying\nmethods in a controlled environment. In real-world\napplications, the threshold would likely require ad-\njustment based on specific use-cases and available\ndata.\nPractical Implications of Threshold: It is in-\ndeed a challenge to select an optimal threshold that\nwould be universally valid. In our experiment ap-\nplying PromptA V on the IMDb62 dataset, we found\nthat the optimal threshold lies between 0.2 - 0.3.\nThis range suggests that the model tends to generate\na low confidence score, which provides significant\ninsights into its functioning and decision-making.\nThe low confidence score may be indicative of the\nConfidence Score PromptA V PS+ CoT\n0.1 0.506 0.501 0.524\n0.2 0.587 0.536 0.522\n0.3 0.579 0.529 0.505\n0.4 0.578 0.529 0.497\n0.5 0.569 0.527 0.497\n0.6 0.55 0.484 0.498\n0.7 0.53 0.493 0.498\n0.8 0.519 0.497 0.497\n0.9 0.5 0.497 0.497\nTable 5: Performance of PromptA V and the baselines in\n0 shot setting with varying confidence score threshold.\nHighest accuracy are bolded\nmodel’s cautious approach to attributing author-\nship, ensuring a reduced number of false positives\nin practical scenarios. This threshold may varied,\ndepending on the complexity of the dataset, and the\namount of writing sample observations the model\nmade for each author.\nSelection of the Confidence Score: The confi-\ndence score selection was an empirical decision\nbased on our preliminary experiments, intending to\noptimize the balance between false positives and\nfalse negatives. We have conducted an experiments\nto show the tradeoff on performance when select-\ning different threshold for the confident score. Note\nthat experiments are conducted using PromptA V\nwith zero-shot demonstration setting.\nA.4 Zero-shot vs supervised methods\nOur work with PromptA V centers around its capa-\nbility to handle the A V task in a zero-shot setting.\nAcquiring labeled data, especially for tasks like A V ,\ncan be quite challenging due to privacy concerns,\nthe need for expert annotators, and other complexi-\nties associated with the domain. We recognize the\nimportance and relevance of benchmarking against\nstate-of-the-art A V methods. Their dependence on\ncopious amounts of labeled data inherently places\nthem in a different operating paradigm. For in-\nstance, BERT is able to achieve a high accuracy\nof 0.702 if it is trained on IMDb62 full training\ndataset with 74,400 samples. However, BERT will\nrequire large amount of training data to achieve the\nsuperior performance.\n14084"
}