{
    "title": "Large Language Model Uncertainty Measurement and Calibration for Medical Diagnosis and Treatment",
    "url": "https://openalex.org/W4399423107",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2114695148",
            "name": "Thomas Savage",
            "affiliations": [
                "Stanford University",
                "Stanford Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A2096913560",
            "name": "John Wang",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A2156113513",
            "name": "Robert Gallo",
            "affiliations": [
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A5099045299",
            "name": "Abdessalem Boukil",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5101374207",
            "name": "Vishwesh Patel",
            "affiliations": [
                "Government Medical College",
                "M. P. Shah Medical College"
            ]
        },
        {
            "id": "https://openalex.org/A3031925457",
            "name": "Seyed Amir Ahmad Safavi-Naini",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A2036204426",
            "name": "ali soroush",
            "affiliations": [
                "Icahn School of Medicine at Mount Sinai"
            ]
        },
        {
            "id": "https://openalex.org/A2300145783",
            "name": "Jonathan H Chen",
            "affiliations": [
                "Stanford University",
                "Stanford Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A2114695148",
            "name": "Thomas Savage",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2096913560",
            "name": "John Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2156113513",
            "name": "Robert Gallo",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5099045299",
            "name": "Abdessalem Boukil",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A5101374207",
            "name": "Vishwesh Patel",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3031925457",
            "name": "Seyed Amir Ahmad Safavi-Naini",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2036204426",
            "name": "ali soroush",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2300145783",
            "name": "Jonathan H Chen",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4384561707",
        "https://openalex.org/W4384561103",
        "https://openalex.org/W4313197536",
        "https://openalex.org/W4377009978",
        "https://openalex.org/W4389325518",
        "https://openalex.org/W4385798548",
        "https://openalex.org/W4377011132",
        "https://openalex.org/W4389984066",
        "https://openalex.org/W4404229815",
        "https://openalex.org/W4396570449",
        "https://openalex.org/W2766556082",
        "https://openalex.org/W4385015322",
        "https://openalex.org/W4388748038",
        "https://openalex.org/W2996480032",
        "https://openalex.org/W2762658547",
        "https://openalex.org/W4313312731",
        "https://openalex.org/W6861562795",
        "https://openalex.org/W4381797997",
        "https://openalex.org/W4389523793",
        "https://openalex.org/W4392013297",
        "https://openalex.org/W3118929067",
        "https://openalex.org/W3088056511",
        "https://openalex.org/W4389518686",
        "https://openalex.org/W4389520749",
        "https://openalex.org/W4391170193",
        "https://openalex.org/W1980276147",
        "https://openalex.org/W4360891289",
        "https://openalex.org/W4380730209",
        "https://openalex.org/W4390943000",
        "https://openalex.org/W4388525110"
    ],
    "abstract": "Abstract Introduction The inability of Large Language Models (LLMs) to communicate uncertainty is a significant barrier to their use in medicine. Before LLMs can be integrated into patient care, the field must assess methods to measure uncertainty in ways that are useful to physician-users. Objective Evaluate the ability for uncertainty metrics to quantify LLM confidence when performing diagnosis and treatment selection tasks by assessing the properties of discrimination and calibration. Methods We examined Confidence Elicitation, Token Level Probability, and Sample Consistency metrics across GPT3.5, GPT4, Llama2 and Llama3. Uncertainty metrics were evaluated against three datasets of open-ended patient scenarios. Results Sample Consistency methods outperformed Token Level Probability and Confidence Elicitation methods. Sample Consistency by Sentence Embedding achieved the highest discrimination performance (ROC AUC 0.68â€“0.79) with poor calibration, while Sample Consistency by GPT Annotation achieved the second-best discrimination (ROC AUC 0.66-0.74) with more accurate calibration. Nearly all uncertainty metrics had better discriminative performance with diagnosis rather than treatment selection questions. Furthermore, verbalized confidence (Confidence Elicitation) was found to consistently over-estimate model confidence. Conclusions Sample Consistency is the most effective method for estimating LLM uncertainty of the metrics evaluated. Sample Consistency by Sentence Embedding can effectively estimate uncertainty if the user has a set of reference cases with which to re-calibrate their results, while Sample Consistency by GPT Annotation is more effective method if the user does not have reference cases and requires accurate raw calibration. Our results confirm LLMs are consistently over-confident when verbalizing their confidence through Confidence Elicitation.",
    "full_text": null
}