{
    "title": "LPR: Large Language Models-Aided Program Reduction",
    "url": "https://openalex.org/W4390092823",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2747882156",
            "name": "Zhang, Mengxiao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2391856687",
            "name": "Tian Yong-qiang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2628183683",
            "name": "Xu Zhenyang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2363303168",
            "name": "Dong, Yiwen",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4282588931",
            "name": "Tan, Shin Hwei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202179814",
            "name": "Sun, Chengnian",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3173820372",
        "https://openalex.org/W4386442953",
        "https://openalex.org/W4379512361",
        "https://openalex.org/W4362598291",
        "https://openalex.org/W4367860052",
        "https://openalex.org/W2121217767",
        "https://openalex.org/W2170737051",
        "https://openalex.org/W4362676437",
        "https://openalex.org/W4367000100",
        "https://openalex.org/W4384615697",
        "https://openalex.org/W2170224888",
        "https://openalex.org/W4300989059",
        "https://openalex.org/W2891688103",
        "https://openalex.org/W4389209096",
        "https://openalex.org/W4388483194",
        "https://openalex.org/W4388483640",
        "https://openalex.org/W2967352598",
        "https://openalex.org/W3173506450",
        "https://openalex.org/W4385573804",
        "https://openalex.org/W4386436496",
        "https://openalex.org/W4389165112",
        "https://openalex.org/W2794859654",
        "https://openalex.org/W4392414327",
        "https://openalex.org/W2532737545",
        "https://openalex.org/W4378591002",
        "https://openalex.org/W4330337959",
        "https://openalex.org/W4385768129",
        "https://openalex.org/W4388483128",
        "https://openalex.org/W3193977407",
        "https://openalex.org/W4386081297",
        "https://openalex.org/W2098456636",
        "https://openalex.org/W4378942602",
        "https://openalex.org/W4284676027",
        "https://openalex.org/W4238083723",
        "https://openalex.org/W4318541608",
        "https://openalex.org/W3185341429",
        "https://openalex.org/W2461570336",
        "https://openalex.org/W4285069940",
        "https://openalex.org/W4377866048",
        "https://openalex.org/W4389162832",
        "https://openalex.org/W4384345745",
        "https://openalex.org/W2155877593",
        "https://openalex.org/W4384345708"
    ],
    "abstract": "Program reduction is a prevalent technique to facilitate compilers' debugging by automatically minimizing bug-triggering programs. Existing program reduction techniques are either generic across languages (e.g., Perses and Vulcan) or specifically customized for one certain language by employing language-specific features, like C-Reduce. However, striking the balance between generality across multiple programming languages and specificity to individual languages in program reduction is yet to be explored. This paper proposes LPR, the first technique utilizing LLMs to perform language-specific program reduction for multiple languages. The core insight is to utilize both the language-generic syntax level program reduction (e.g., Perses) and the language-specific semantic level program transformations learned by LLMs. Alternately, language-generic program reducers efficiently reduce programs into 1-tree-minimality, which is small enough to be manageable for LLMs; LLMs effectively transform programs via the learned semantics to expose new reduction opportunities for the language-generic program reducers to further reduce the programs. Our extensive evaluation on 50 benchmarks across three languages (C, Rust, and JavaScript) has highlighted LPR's practicality and superiority over Vulcan, the state-of-the-art language-generic program reducer. For effectiveness, LPR surpasses Vulcan by producing 24.93%, 4.47%, and 11.71% smaller programs on benchmarks in C, Rust and JavaScript. Moreover, LPR and Vulcan have demonstrated their potential to complement each other. By using Vulcan on LPR's output for C programs, we achieve program sizes comparable to those reduced by C-Reduce. For efficiency, LPR takes 10.77%, 34.88%, 36.96% less time than Vulcan to finish all benchmarks in C, Rust and JavaScript, separately.",
    "full_text": "LPR: Large Language Models-Aided Program Reduction\nMengxiao Zhang\nm492zhan@uwaterloo.ca\nUniversity of Waterloo\nCanada\nYongqiang Tian‚àó\nyqtian@ust.hk\nThe Hong Kong University of Science\nand Technology\nChina\nZhenyang Xu\nzhenyang.xu@uwaterloo.ca\nUniversity of Waterloo\nCanada\nYiwen Dong\nyiwen.dong@uwaterloo.ca\nUniversity of Waterloo\nCanada\nShin Hwei Tan\nshinhwei.tan@concordia.ca\nConcordia University\nCanada\nChengnian Sun\ncnsun@uwaterloo.ca\nUniversity of Waterloo\nCanada\nABSTRACT\nProgram reduction is a widely used technique to facilitate debug-\nging compilers by automatically minimizing programs that trigger\ncompiler bugs. Existing program reduction techniques are either\ngeneric to a wide range of languages (such as Perses and Vulcan)\nor specifically optimized for one certain language by exploiting\nlanguage-specific knowledge (e.g., C-Reduce). However, synergisti-\ncally combining both generality across languages and optimality\nto a specific language in program reduction is yet to be explored.\nThis paper proposes LPR, the first LLMs-aided technique lever-\naging LLMs to perform language-specific program reduction for\nmultiple languages. The key insight is to utilize both the language\ngenerality of program reducers such as Perses and the language-\nspecific semantics learned by LLMs. Concretely, language-generic\nprogram reducers can efficiently reduce programs into a small size\nthat is suitable for LLMs to process; LLMs can effectively transform\nprograms via the learned semantics to create new reduction op-\nportunities for the language-generic program reducers to further\nreduce the programs.\nOur thorough evaluation on 50 benchmarks across three pro-\ngramming languages (i.e., C, Rust and JavaScript) has demonstrated\nLPR‚Äôs practicality and superiority over Vulcan, the state-of-the-art\nlanguage-generic program reducer. For effectiveness,LPR surpasses\nVulcan by producing 24.93%, 4.47%, and 11.71% smaller programs\non benchmarks in C, Rust and JavaScript, separately. Moreover,LPR\nand Vulcan have the potential to complement each other. For the C\nlanguage for which C-Reduce is optimized, by applying Vulcan to\nthe output produced by LPR, we can attain program sizes that are\non par with those achieved by C-Reduce. For efficiency perceived\nby users, LPR is more efficient when reducing large and complex\nprograms, taking 10.77%, 34.88%, 36.96% less time than Vulcan to\nfinish all the benchmarks in C, Rust and JavaScript, separately.\nCCS CONCEPTS\n‚Ä¢ Software and its engineering ‚ÜíSoftware testing and debug-\nging.\nKEYWORDS\nProgram Reduction, Large Language Models, Program Semantics\n‚àóCorresponding author\n1 INTRODUCTION\nProgram reduction techniques [8, 15, 16, 25‚Äì27, 33, 38, 39, 48] aim\nto facilitate compiler debugging by minimizing the bug-triggering\nprograms with efficacy and efficiency. Given a program ùëÉ and a\nproperty ùúì that ùëÉ preserves, program reduction techniques (a.k.a.,\nprogram reducers) produce a minimal program ùëÉmin that still pre-\nserves ùúì. Program reduction has been widely used in various soft-\nware engineering tasks [7, 12], especially in compiler testing and\ndebugging [18, 23, 36, 37, 42].\nHowever, a critical challenge in program reduction has not been\nproperly addressed, i.e., the trade-off between generality across\nlanguages and specificity to a certain language. Currently, there are\ntwo categories of program reduction techniques: language-specific\nreduction [15, 16, 27] and language-generic reduction [25, 33, 48,\n50, 52]. The former category leverages language-specific semantics\nto transform and shrink programs in certain languages, while the\nlatter only uses transformations applicable to any programming\nlanguage. Although language-specific reducers are usually more\neffective in reduction, designing an effective reducer for a specific\nlanguage, especially language-specific transformations, requires a\ndeep understanding of language features and a significant amount\nof time and engineering effort. Therefore, only a limited set of lan-\nguages have custom-designed reducers, such as C [27], Java [15, 16],\nand SMT-LIBv2 [26]. Meanwhile, language-generic reducers can\nbe applied to diverse languages, but lack the knowledge of lan-\nguage features and semantics and thus are incapable of performing\nlanguage-specific transformations (e.g., function inlining) that can\nenable further reduction. As a result, they cannot utilize peculiar\nfeatures of each language to produce optimally reduced programs.\nThis study strives to find a sweet spot between generality across\nlanguages and specificity to a certain language, by synergistically\ncombining the strengths of both categories of program reduction\ntechniques. Specifically, we notice that the major limitation of\nlanguage-generic reducers lies in their incapability to perform\nlanguage-specific transformations. Language-generic reducers such\nas Perses stand out as high generality when reducing programs\nacross various programming languages, while they lack awareness\nof semantic information to achieve further progress. If we could\nhelp language-generic reducers conquer this limitation, they are\nlikely to produce smaller results.\nMeanwhile, we also notice that recent Large Language Mod-\nels (LLMs) could be powerful assistants in performing language-\nspecific transformations, like how they perform in other software\narXiv:2312.13064v3  [cs.PL]  11 May 2024\nConference‚Äô17, July 2017, Washington, DC, USA Mengxiao Zhang, Yongqiang Tian, Zhenyang Xu, Yiwen Dong, Shin Hwei Tan, and Chengnian Sun\nengineering tasks such as code generation and test generation [6,\n10, 11, 14, 34, 43, 46]. Specifically, LLMs are trained with massive\nprograms, and they have started to exhibit the ability to analyze\nand transform programs in prevalent languages. If we can prop-\nerly leverage this ability for program reduction, we may have a\nlanguage-generic reducer being aware of the semantics of vari-\nous prevalent languages. In addition, the utilization of LLMs can\nsimplify the customization and extension of reducers, as it would\nbe time-consuming and labor-intensive to manually implement a\nlanguage-specific reducer or add functionality to an existing one\n(such as C-Reduce using the Clang frontend to implement C/C++-\nspecific program transformations).\nChallenges of Using LLMs. LLMs are not the silver bullet to pro-\ngram reduction. When LLMs are instructed to handle source code\nas inputs, due to the intrinsic limitations of LLMs [9, 19, 40], LLMs\nmay be unable to understand subtle differences in code [20] and be\ndistracted by irrelevant context [30]. Moreover, in program reduc-\ntion, input programs typically contain tens of thousands of line of\ncode, which surpasses the input limits of LLMs. Besides, without\neffective guidance, LLMs are unclear about what transformations\nto perform.\nLLMs-Aided Program Reduction ( LPR). We propose LPR in\nthis paper, which is, to the best of our knowledge, the first approach\nthat integrates LLMs for program reduction tasks. LPR synergis-\ntically leverages the strengths of both language-generic program\nreducers and LLMs. Specifically, LPR alternates between invoking\na language-generic reducer (we use Perses in experiments) and the\nLLM. Initially, Perses efficiently reduces large programs to a size\nmanageable for the LLM. Subsequently, the LLM further transforms\nPerses‚Äôs output based on specific user-defined prompts that dictate\nthe required transformations. Following this, Perses is re-invoked,\nas transformations made by the LLM often create additional oppor-\ntunities for reduction. This process iterates until the program can-\nnot be further minimized. For transformations, we have identified\nfive language-generic transformations to enable further reduction:\nFunction Inlining , Loop Unrolling , Data Type Elimination , Data Type\nSimplification, and Variable Elimination.\nTo address the aforementioned challenge of using LLMs, LPR\nis designed with a multi-level prompting approach. In detail, LPR\ninitially requests the LLM to identify a list of potential targets for a\ngiven transformation, and then sequentially instructs the LLM to\napply the transformation on each target. The multi-level prompt\nguides the LLM in a more concentrated way, by excluding irrelevant\ncontext and other targets that may distract the LLM.\nWe have conducted extensive evaluations onLPR, illustrating its\nsuperiority over Vulcan, the state-of-the-art language-generic algo-\nrithm. On three benchmark suites, i.e., Benchmark-C, Benchmark-\nRust and Benchmark-JS, LPR produces significantly smaller pro-\ngrams than Vulcan by 24.93%, 4.47% and 11.71%, separately. More-\nover,LPR and Vulcan complement each other. For C language which\nC-Reduce is optimized for, by applying Vulcan to the output pro-\nduced by LPR, we attain program sizes that are on par with those\nachieved by C-Reduce. For reduction efficiency, LPR performs com-\nparably to Vulcan. In terms of execution time perceived by users,\nLPR is more efficient than Vulcan on reducing complex programs.\nFurthermore, our detailed analysis indicates that each of the pro-\nposed transformations plays a crucial role in the reduction pro-\ncess. We also compare the LPR‚Äôs performance with the multi-level\nprompt against that without it, illustrating the efficacy of our pro-\nposed multi-level prompting approach.\nContribution. This study makes the following contributions.\n‚Ä¢We introduce LPR, the first attempt to use LLMs for program re-\nduction. By synergizing the capabilities of both language-generic\nprogram reducers and LLMs, LPR exhibits both generality across\nvarious languages and awareness of semantics in specific lan-\nguages. Moreover, LPR is easy and flexible to extend with new\ntransformations by simply designing new prompts.\n‚Ä¢We propose a multi-level prompting approach to guide LLMs\nto execute program transformations, and demonstrate its effec-\ntiveness in practice. We propose five general-purposed transfor-\nmations for LLMs to reduce programs or create more reduction\nopportunities for language-generic program reducers to exploit.\n‚Ä¢We comprehensively evaluated LPR on 50 benchmarks across\nthree commonly used languages: C, Rust and JavaScript. Results\ndemonstrate LPR‚Äôs strong effectiveness and generality.\n2 BACKGROUND\n2.1 Program Reduction\nGiven a program ùëÉ with a certain property, e.g., triggering a com-\npiler bug, the goal of program reduction is to search for a minimal\nprogram ùëÉmin that still triggers the bug. Program reduction has\ndemonstrated its significant usefulness in removing bug-irrelevant\ncode snippets. The original bug-triggering code [17, 23, 31, 49] may\nhave thousands of lines, whereas the distilled version from program\nreduction tools only contains dozens of lines of code [ 32]. Some\nalgorithms are designed to generalize across multiple programming\nlanguages, while others are customized for certain languages.\n2.1.1 Language-Generic Reducers. Some reducers can be gener-\nalized to multiple languages. For instance, given the formal syn-\ntax of a programming language, algorithms like HDD and Perses\ncan be used to reduce programs corresponding to this language.\nHDD parses the program into a parse tree and then applies the\nDDMin [50] at each level of the tree to remove unnecessary tree\nnodes as much as possible. Perses goes further than HDD by per-\nforming certain transformations on the formal syntax to avoid gen-\nerating syntactically incorrect program variants. Vulcan extends\nPerses, by introducing novel auxiliary reducers to exhaustively\nsearch for smaller variants by replacing identifiers/sub-trees and\ndeleting local combinations of tree nodes on the parse tree.\nHowever, different languages possess their own unique semantic\nfeatures. Although the aforementioned algorithms are relatively\nefficient [33], they are incapable of utilizing unique semantics of\na particular language to further reduce a program. For example,\nthese algorithms lacks the ability to perform transformations like\nfunction inlining. Although Vulcan can identify more reduction op-\nportunities through transformations such as identifier replacement\nand local exhaustive search, its approach is akin to \"brute force\"\nenumeration. This method lacks awareness of the given program‚Äôs\nsemantics, making it less effective and efficient overall.\nLPR: Large Language Models-Aided Program Reduction Conference‚Äô17, July 2017, Washington, DC, USA\n2.1.2 Language-Specific Reducers. Previous work has introduced\nreducers customized for some specific languages. For example, C-\nReduce [27] is the most effective reduction tool for C code. It com-\nprises multiple passes that transform the program based on features\nof the language, thereby making it smaller. Language-specific re-\nducers often rely on static program analysis tools for analysis and\nmodification, e.g., LibTooling [24] is employed in C-Reduce.\nHowever, developing a language-specific reducer is nontrivial.\nTo the best of our knowledge, only a few languages have specific re-\nducers, such as C [27], Java [15, 16], and SMT-LIBv2 [26]. The reason\nis that the process of designing a reducer for a language, or adding\nnew transformations to an existing reducer, is time-consuming and\nlabor-intensive. For instance, in version 2.10.0 of C-Reduce [ 28],\nfunction inlining was implemented with 604 lines of C++ code. Such\nchallenges impede the development and maintenance of language-\nspecific reducers.\n2.2 Large Language Models\nLarge Language Models refer to a type of deep learning models\ntrained on huge data sets for diverse tasks. The advent of LLMs\nhas opened up numerous potential opportunities across diverse\nresearch fields. LLMs are not only proficient in processing natural\nlanguages but also exhibit substantial capabilities in understand-\ning and processing programming languages. This highlights the\npromising future and evolutionary prospects in the realm of soft-\nware engineering. Recently, LLMs have been applied and assessed\non various software engineering tasks, such as automatic program\nrepair [10, 13, 45‚Äì47] and program generation [21, 35, 53].\nHowever, despite the usefulness of LLMs, some researchers [20]\nillustrate that current LLMs are weak in distinguishing nuances\nbetween programs. Moreover, the memorizing and processing ca-\npacity of LLMs deteriorates as the input size grows [9, 29]. Besides,\none cannot expect LLMs to automatically complete complex tasks;\nthey must be guided accordingly [20, 46]. Therefore, for program\nreduction, directly asking LLMs to reduce programs with tens of\nthousands of lines is impractical.\n3 APPROACH\nIn this section, we first introduce a motivating example, and then\nwe provide an overview of the LPR workflow. We also outline the\ndetails of prompts and proposed transformations in the workflow\nthat enable the LLM to function effectively on given programs.\n3.1 Motivation\nA motivating example is displayed in Figure 1, where the original\ncode contains highly nested loops, shown in Figure 1a. From Fig-\nure 1b to Figure 1c, the nested loops are fully unrolled into hundreds\nof lines via Loop Unrolling , based on the semantic transformations\nfrom the LLM. Despite the temporary size increase, the following\nPerses effectively eliminates all lines except for the bug-relevant\none. This is also the final result of LPR, presented in Figure 1d. By\ncontrast, as shown in Figure 1e, Vulcan is incapable of escaping the\nlocal minima by exhaustively replacing identifiers and tree nodes.\nSimilarly, C-Reduce cannot fully break down the loop structures,\ngiven that it is not integrated with transformations to unroll loops.\nEven though loop unrolling techniques can be added into C-Reduce\nin future versions, it will be labor-intensive to implement a spe-\ncific transformation compared to user-defined prompts in natural\nlanguage.\n3.2 Workflow\nAlgorithm 1: LPR (ùëÉ, ùúì, prompts )\nInput: ùëÉ: the program to be reduced.\nInput: ùúì : P ‚ÜíB: the property to be preserved by ùëÉ.\nOutput: ùëÉmin: the reduced program that preserves the property.\n1 ùëÉ ‚ÜêPerses (ùëÉ, ùúì) // Quickly minimize ùëÉ for LLM to process.\n2 repeat/* Monotonically minimize the size of ùëÉ. */\n3 ùëÉmin ‚ÜêùëÉ\n4 transformList ‚ÜêgetTransformList (prompts)\n// Iterate through each transformation.\n5 foreach transform ‚ààtransformList do\n6 primaryQuestion ‚ÜêgetPrimaryQuestion (transform)\n7 followupQuestion ‚ÜêgetFollowupQuestion (transform)\n// Ask LLM to identify a list of targets.\n8 targetList ‚ÜêgetTargetList(ùëÉ, primaryQuestion )\n9 foreach target ‚ààtargetList do\n// Let LLM apply the transformation on the\ntarget.\n10 ùëÉtmp ‚ÜêapplyTransformation (ùëÉ, followupQuestion,\ntarget)\n11 if ùúì (ùëÉtmp) then\n12 ùëÉ ‚ÜêùëÉtmp // ùëÉtmp preserves the property.\n// Run Perses for further reduction.\n13 ùëÉ ‚ÜêPerses (ùëÉ, ùúì)\n14 until |ùëÉ |‚â•| ùëÉmin |\n15 return ùëÉmin\nThe overview of the workflow is outlined in Figure 2. Given a\nbug-triggering program as input, LPR invokes a language-generic\nreducer and the LLM alternately, until the target program cannot\nbe further reduced. In each iteration, the language-generic reducer\nefficiently reduces the given program to a minimal one. By contrast,\nthe LLM leverages the semantic knowledge of the language and\ntransforms the program given by the language-generic reducer.\nThis process is guided by user-defined prompts, aiming to expose\nmore reduction potentials to the language-generic reducer.\nAlgorithm 1 shows LPR‚Äôs reduction algorithm. Given as inputs\n(1) a programùëÉ targeted for reduction, (2) a propertyùúì that must be\npreserved, and (3) the pre-definedprompts, LPR generates a reduced\nprogram ùëÉùëöùëñùëõ. The pre-defined prompts consist of primaryQuestion\nand followupQuestion, in which primaryQuestion instructs the LLM\nto identify a list of targets to be transformed, and followupQuestion\nguides the LLM to apply transformation on each target individually.\nThey will be further introduced in ¬ß3.4. Initially, LPR calls Perses\nto quickly minimize ùëÉ, so that the size of the program becomes\nmanageable for the LLM to process. Next, LPR loads a sequence of\ntransformations as delineated in line 4, and then iterates through\neach transformation, as detailed from line 5 to line 13. ¬ß3.3 displays\nthe details of each transformation.\nDuring this process, for each transformation, the algorithm re-\ntrieves a predefined primary question along with a follow-up ques-\ntion on line 6 ‚Äì line 7.LPR firstly asks the LLM the primary question\nConference‚Äô17, July 2017, Washington, DC, USA Mengxiao Zhang, Yongqiang Tian, Zhenyang Xu, Yiwen Dong, Shin Hwei Tan, and Chengnian Sun\n1 ......\n2 // nested loop\n3 for (i = 0; i < 7; i++)\n4 for (j = 0; j < 5; j++)\n5 for (k = 0; k < 7; k++)\n6 fn8(ad[i][j][k], \"g_643[i][j][k]\", aj);\n7 ......\n(a) Original\n1 ......\n2 // nested loop\n3 for (i = 0; i < 7; i++)\n4 for (j = 0; j < 5; j++)\n5 for (k = 0; k < 7; k++)\n6 s = s ^ ad[i][j][k];\n7 ......\n(b) LPR: Before Loop Unrolling\n1 ......\n2 // the nested loop is fully unrolled\n3 // into hundreds of lines\n4 s = s ^ ad[2][0][5];\n5 s = s ^ ad[2][0][6];\n6 s = s ^ ad[2][1][0];\n7 s = s ^ ad[2][1][1];\n8 ......\n(c) LPR: After Loop Unrolling\n1 ......\n2 // all lines except for the bug-triggering one\n3 // is removed by Perses\n4 s = ad[2][1][0];\n5 ......\n(d) Final result of LPR\n1 ......\n2 for (i = 0; i < 7; i++)\n3 for (j = 0; j < 5; j++)\n4 for (k = 0; k < 7; k++)\n5 fn8(ad[i][j][k], \"g_643[i][j][k]\", aj);\n6 ......\n(e) Final result of Vulcan\n1 ......\n2 for (; h < 7; h++) {\n3 j = 0;\n4 for (; j < 5; j++)\n5 printf(\"%\n6 }\n7 ......\n(f) Final result of C-Reduce\nFigure 1: Code snippet from LLVM-31259, showcasing the original code, the effectiveness of Loop Unrolling , and the final results\nby LPR, Vulcan and C-Reduce.\nInput\n Output\nLanguage-genericreducerSemanticleveltransformation\nSyntactical levelreduction\nGuideLLMviamulti-levelprompts\nMinimalprogram\nTransformationFunctionInliningVa r ia bleEliminationLoopUnrolling\nLoop\nLLM\nDataTypeEliminationDataTypeSimplification\nFigure 2: The workflow of LPR.\nunder the current program. This query aims to guide the LLM to\ngenerate a list of specific targets upon which the transformation\nwill be executed. For instance, for Loop Unrolling in the motivat-\ning example Figure 1b, the LLM is asked to identify a list of loops\nin the given program to be unrolled in the primaryQuestion, and\nreturns a target list [for (i = 0; ...), for ( j = 0; ...),\nfor ( k = 0; ...)].\nOn line 9 to 12,LPR uses followupQuestion to guide the LLM to ap-\nply the transformation on each identified target within the program.\nIn the motivating example, the followupQuestion can be framed as\n‚ÄúGiven the program { PROGRAM } and the loop for(i=0;...),\noptimize it via loop unrolling‚Äù. The modified program is then ex-\ntracted from the LLM‚Äôs response text on line 10. In the example, all\nloops are unrolled into repeated lines of code in Figure 1c.\nIn experimental scenarios, given an input program and the prompt,\nthe LLM may generate multiple transformed programs, as the num-\nber of responses can be customized. Among all transformed pro-\ngrams, LPR keeps the smallest one that still passes the property test,\nand discards others. If no transformed program returned from this\nquery satisfies the property, LPR keeps the original one before this\nquery. After each transformation is completed, a language-generic\nreducer such as Perses [33] is employed to seek additional reduc-\ntion opportunities, considering that the transformation might have\nintroduced new potential for further simplification. The algorithm\npersists in the outermost loop until it reaches a fixpoint, signifying\nthat the program size can no longer be reduced.\n3.3 Transformations\nTo further search for reduction opportunities on a bug-triggering\nprogram via the LLM, we propose five general transformations to\nguide the LLM, i.e., Function Inlining , Loop Unrolling , Data Type\nElimination, Data Type Simplification and Variable Elimination.\nFunction Inlining. This transformation identifies a function\nand inlines it to eliminate all call sites of this function, and instead\nsubstitutes them with the corresponding function body. As func-\ntions are commonly found in bug-triggering programs, there is\nsignificant room for function inlining to reduce tokens or provide\nfurther reduction opportunities.\nLoop Unrolling. Loop unrolling, also known as loop unwind-\ning, is a widely used loop transformation approach to optimize\nthe execution. In this task, it can also be employed to find more\nreduction opportunities. Loop Unrolling identifies a loop structure\nand attempts to unroll the loop into a code snippet repeating a\nsingle iteration. This is motivated by the fact that language-generic\nreducers may be incapable of dismantling or directly removing the\nloop structure, while they may be able to reduce the repeated code\nafter loop unrolling, as shown in Figure 1.\nData Type Elimination. Some data types in bug-triggering\nprograms may be irrelevant to the bug, such as identifiers defined\nby typedef in C, and type alias created by type keyword in Rust.\nWe proposeData Type Elimination to eliminate the alias and replace\nthe occurrence of each alias with its associated original data type.\nData Type Simplification. In programs with complex data types,\nsuch as structures, arrays, and pointers, not all components are\nessential for maintaining bug-triggering properties. For example,\na bug-triggering program containing a struct with three integer\nmembers can be simplified into three distinct integer variables, and\npossibly only one variable is essential. To facilitate this simplifica-\ntion, we introduce Data Type Simplification , a strategy designed to\nLPR: Large Language Models-Aided Program Reduction Conference‚Äô17, July 2017, Washington, DC, USA\nTheprogramwithfn1inlinedisResponsetoFollowupQuestion(fn1)intfn2(inta){returna+a;}intmain(){intx=1;inty=fn2(x);return0;}\nGiven the following program{PROGRAM}andthespecifiedfunction{fn1}, optimize {fn1} out via function inlining.\nFollowupQuestion(fn1)\nGiven the following program{PROGRAM}, identify all functionsthatcanbeinlined. Please write the identified functions into atarget_list: [a list of function names].\nPrimaryQuestion\nGiven the following program{PROGRAM}andthespecifiedfunction{fn2}, optimize {fn2} out via function inlining.\nFollowupQuestion(fn2)\nintfn1(){return1;}intfn2(inta){returna+a;}intmain(){intx=fn1();inty=fn2(x);return0;}\nOriginalProgram\nYou are an assistant for program analysis and transformation. Please givetherequiredinformationbasedonyourunderstanding.\nSystemPrompt\nForthegivenprogramandrequest,thetarget_listis{[fn1,fn2]}ResponsetoPrimaryQuestion\nResponsetoFollowupQuestion(fn2)\nintmain(){intx=1;inty=x+x;return0;}\nTheprogramwithfn2inlinedis\nFigure 3: An example of prompt design.\n and\n denote system prompt and user prompt provided by the users.\n denotes\nthe responses from the LLM.\ntransform variables of complex data types into variables of primitive\ndata types, like integers or floats.\nVariable Elimination. Intermediate variables are pervasive in\nprograms, and reducing them is desirable in program reduction\ntasks. Besides, some variables, although not being used, are hard to\neliminate. For instance, to remove an unused parameter, both the\nparameter defined in the function and its corresponding argument\npassed to the call site of this function should be removed simul-\ntaneously. This is hard or even impossible for language-generic\nreduction tools. Therefore, we propose Variable Elimination to opti-\nmize out both intermediate and unused variables.\nThe proposed transformations are universally applicable across\nvarious programming languages, offering a broad utility. By per-\nforming these transformations on programs via the LLM, substantial\nhuman effort is saved from designing and implementing reducers\nthat target these transformations. While certain existing language-\nspecific reducers like C-Reduce, may already incorporate some of\nthese transformations, e.g., Function Inlining and Variable Elimina-\ntion, creating new transformation passes remains a non-trivial task\nfor users. Our approach not only simplifies this process but also\nextends its reach across multiple programming languages.\n3.4 Multi-level Prompts\nPrompts enable LLMs to apply the transformations mentioned\nabove. We take Function Inlining as an example. We avoid directly\ninstructing the LLM to perform transformations exhaustively, such\nas inlining all functions in a program in a single query, which might\noverwhelm its processing capabilities, especially for programs with\nmultiple functions. Instead, we employ a multi-level prompting ap-\nproach. Figure 3 presents an example of Function Inlining . First, we\npose a primary question to the LLM (step 1 ): ‚ÄúGiven the following\nprogram { PROGRAM }, identify all functions that can be inlined. ‚Äù\nBased on the list provided by the LLM (step 2 ), we then ask a\nseries of follow-up questions (step 3 and step 5 ) like ‚ÄúGiven the\nfollowing program { PROGRAM } and the specified function { fn1\n}, optimize { fn1 } out via function inlining. ‚Äù, and the LLM do the\ntransformations accordingly (step 4 and step 6 ). This strategy\nexcludes irrelevant context and ensures that the queries are more\ntargeted, thereby increasing the likelihood of the LLM generating\nhigh-quality results. For other transformations, the prompts follow\na similar template ‚Äî first prompting the LLM to identify a target\nlist, and then instructing it to attempt optimization of each target.\n4 EVALUATION\nIn this section, we evaluate the reduction effectiveness and effi-\nciency of LPR. Specifically, we conducted the following research\nquestions.\nRQ1 What is the effectiveness of LPR in program reduction?\nRQ2 To what extent is the efficiency of LPR in program reduction\nperceived by users?\nRQ3 What is the effectiveness of each transformation in LPR?\n4.1 Experimental Setup\nWithin the workflow ofLPR, we employ Perses [33] as the language-\nagnostic reducer due to its superior efficiency compared to Vulcan.\nAdditionally, we utilize OpenAI API [ 1], specifically the gpt-3.5-\nturbo-0613 version, to serve as the LLM. We also develop a variant\nnamed LPR+Vulcan, which invokes Vulcan to further reduce the\nprogram after LPR finishes. The experiments are conducted on\nan Ubuntu 22 server with an Intel(R) Xeon(R) Gold 6348 CPU @\n2.60GHz and 512 GB RAM. For a fair comparison, all algorithms\nwere executed in a single-process, single-thread environment.\nBenchmarks. To measure the effectiveness and efficiency of\nLPR across various languages, we employ three benchmark suites:\nConference‚Äô17, July 2017, Washington, DC, USA Mengxiao Zhang, Yongqiang Tian, Zhenyang Xu, Yiwen Dong, Shin Hwei Tan, and Chengnian Sun\nBenchmark-C, Benchmark-Rust and Benchmark-JS. The Benchmark-\nC, previously collected and utilized by previous studies [38, 48, 52],\ncomprises 20 large complex programs triggering real-world bugs in\nLLVM or GCC. Benchmark-Rust, incorporating 20 bug-triggering\nRust programs, has also been used in prior research [48]. We fur-\nther craft Benchmark-JS, a non-public benchmark suite, for this\nstudy. Specifically, we use FuzzJIT [41] to fuzz a prevalent JavaScript\nengine, i.e., JavaScriptCore (version c6a5bcc), and then randomly\ncollect 10 programs that cause miscompilations in JIT compiler.\nSince the programs and reduced programs in Benchmark-JS are not\npublicly accessible at the time of LLMs‚Äô training, and thus not in\nthe training sets of LLMs. The evaluation on Benchmark-JS helps to\ninvestigate whether LPR suffers from the data leakage problem [44].\nIn total, the evaluation benchmarks encompass 50 programs trig-\ngering real-world bugs in compilers, spanning across three popular\nprogramming languages.\nBaselines. In all three benchmark suites, we use Perses and\nVulcan as baselines. Perses stands out as a highly effective and effi-\ncient program reduction tool. To avoid the occurrence of syntactical\ninvalid variants during the reduction process, it transforms and nor-\nmalizes the formal syntax of a programming language. Vulcan [48],\nbuilding upon Perses, provides three manually designed auxiliary\nreducers to further search for reduction opportunities on results\nfrom Perses. Compared to Perses, Vulcan achieves a reduction in\nthe number of tokens, albeit at the expense of increased running\ntime. They are both language-generic and are applicable across\na broad spectrum of programming languages. We also include C-\nReduce (v2.9.0) as an additional baseline. C-Reduce not only stands\nas the most effective algorithm for C, it can also be applied to other\nlanguages, though not customized for them.\nConfiguration. If not otherwise specified, our experiments are\nconducted by invoking OpenAI API (version gpt-3.5-turbo-0613),\nwith the proposed multi-level prompt and transformations in ¬ß3.3\nand ¬ß3.4. To effectively harness the inherent randomness of LLMs,\nwe set temperature=1.0. This high value encourages the LLM\nto generate more diverse outcomes [3]. Additionally, we employ\nn=5 to generate five distinct results for every query [2], enabling\nus to choose the smallest program preserving the propertyùúì as the\noptimal result. All the rest configurations are set to their default\nvalues.\n4.2 RQ1: Reduction Effectiveness\nWe measure the effectiveness of LPR, LPR+Vulcan and baseline\nalgorithms via the final program sizes in tokens. A smaller size is\nfavored, as it signifies the removal of more bug-irrelevant code,\nthereby saving developers more manual effort. The effectiveness\nof each algorithm on all three benchmark suites is presented in\nTable 1. Due to the randomness ofLPR, we repeat five times forLPR\nand LPR+Vulcan on every benchmark, and display the mean value\nand standard deviation value in the table. On each benchmark, the\nminimal results are highlighted in bold.\nBenchmark-C. On this benchmark suite, Perses reduces the pro-\ngrams to an average of 247.8 tokens. Building upon this, Vulcan\nfurther compresses the average program size into 157.4,i.e., thereby\ncontinues to decrease the program size by 34.35%. Despite Vulcan‚Äôs\nnotable reduction progress, LPR is still capable of continuing to\nTable 1: The reduction sizes of Perses, Vulcan, C-Reduce,\nLPR and LPR+Vulcan. Best results among all algorithms are\nhighlighted in bold font.\nBenchmarkOriginalPersesVulcanC-Reduce LPR LPR +Vulcan\nLLVM-2238221,068 144 108 70 73.2 ¬±1.6 69.8¬±1.8\nLLVM-22704184,444 78 62 42 43.6 ¬±3.1 41.8¬±3.3\nLLVM-2330938,647 464 303 118 105.8¬±9.3 91.2¬±8.2\nLLVM-2335330,196 98 91 74 68.8 ¬±8.0 66.6¬±6.5\nLLVM-2590078,960 239 104 90 93.4 ¬±11.1 84¬±6.5\nLLVM-26760209,577 120 56 43 62.8¬±18.6 52.6 ¬±5.4\nLLVM-27137174,538 180 88 50 69.2¬±19.2 65 ¬±20.2\nLLVM-27747173,840 117 79 68 87.8 ¬±2.5 63.2¬±2.2\nLLVM-3125948,799 406 282 168 184.0¬±51.2 114.4¬±10.9\nGCC-59903 57,581 308 198 105 209.8¬±72.1 166.4¬±64.0\nGCC-60116 75,224 443 247 168 188.8¬±52.4 127.6¬±34.8\nGCC-61383 32,449 272 195 84 113.2¬±13.6 105.2¬±5.4\nGCC-61917 85,359 150 103 65 78.4¬±10.6 73.4 ¬±5.5\nGCC-64990148,931 239 203 65 143.4¬±58.0 119 ¬±50.1\nGCC-65383 43,942 153 84 72 64.2¬±2.7 64.2¬±2.7\nGCC-66186 47,481 327 226 115 97.8¬±17.9 94.2¬±12.0\nGCC-66375 65,488 440 227 56 56.0 ¬±5.1 56.0¬±5.1\nGCC-70127154,816 301 230 84 95.0 ¬±3.8 73.6¬±3.3\nGCC-70586212,259 426 223 130 235.4¬±30.2 156.8¬±12.6\nGCC-71626 6,133 51 38 46 44.6 ¬±3.4 36.6¬±0.9\nBenchmark-C\nMean 94,487 247.8 157.4 85.7 105.8¬±4.4 86.1 ¬±2.9\nRust-44800 801 467 284 473 124.6¬±32.4 118.6¬±34.4\nRust-66851 936 728 713 654 414.2¬±273.9 331.2¬±257.8\nRust-69039 190 114 101 110 97.2¬±8.0 90.8¬±10.9\nRust-77002 347 263 247 264 96.0¬±27.9 96.0¬±27.9\nRust-77320 173 40 40 40 40.0 ¬±0.0 39.0¬±0.0\nRust-77323 81 13 13 13 13.0 ¬±0.0 13.0¬±0.0\nRust-77910 63 34 21 23 29.2 ¬±2.7 21.0¬±0.0\nRust-77919 132 74 62 70 62.2 ¬±14.3 58.2¬±7.7\nRust-78005 182 102 102 75 102.0¬±0.0 102.0 ¬±0.0\nRust-78325 65 29 26 34 29.0 ¬±0.0 26.0¬±0.0\nRust-78651 957 17 9 12 16.6 ¬±0.5 11.0 ¬±2.7\nRust-78652 263 56 49 49 53.6¬±2.2 49.0¬±0.0\nRust-78655 28 26 26 26 26.0 ¬±0.0 26.0¬±0.0\nRust-78720 121 72 56 51 58.4¬±2.1 56.6 ¬±0.5\nRust-91725 513 174 86 101 68.6¬±57.6 55.2¬±18.4\nRust-99830 448 299 277 160 271.6¬±12.5 230.2¬±62.6\nRust-111502 192 166 157 161 104.8¬±7.2 103.6¬±8.2\nRust-112061 556 458 442 450 385.0¬±32.6 380.4¬±31.8\nRust-112213 866 736 635 732 653.0¬±34.0 618.8¬±22.9\nRust-112526 644 382 338 545 304.0¬±30.3 283.2¬±23.3\nBenchmark-Rust\nMean 378 212.5 184.2 202.2 147.5¬±11.1 135.5¬±10.1\nJS-1 244 52 41 41 25.6 ¬±0.5 24.8¬±1.6\nJS-2 112 51 41 40 34.0 ¬±6.5 27.8¬±3.5\nJS-3 125 57 41 47 51.6 ¬±12.1 35.4¬±7.7\nJS-4 185 65 35 47 33.4 ¬±2.2 28.2¬±6.9\nJS-5 178 66 38 52 41.6 ¬±2.2 33.8¬±5.5\nJS-6 152 57 30 45 20.0 ¬±2.8 19.2¬±2.7\nJS-7 144 46 38 38 34.0 ¬±0.0 27.2¬±6.3\nJS-8 121 55 47 45 40.6 ¬±6.3 33.0¬±17.4\nJS-9 87 50 30 32 23.8 ¬±2.5 18.8¬±5.2\nJS-10 63 56 41 43 34.8 ¬±5.1 27.0¬±6.3\nBenchmark-JS\nMean 141 55.5 38.2 43.0 33.9¬±1.6 27.5¬±5.9\npush the limit of Perses, and reduces the programs in Benchmark-C\ninto 105.8 tokens on average across all five runs. It cuts down the\naverage program size of Perses by 51.33%, outperforming Vulcan sig-\nnificantly by 24.93% (proved by a p-value of 0.002). C-Reduce stands\nout by achieving the lowest average program size, i.e., 85.7 tokens.\nThis performance is anticipated as C-Reduce incorporates various\ntransformation passes specifically designed for C. Despite relying\non only general transformations, LPR+Vulcan still achieves perfor-\nmance comparable to C-Reduce, averaging 86.1 tokens. Moreover,\nLPR: Large Language Models-Aided Program Reduction Conference‚Äô17, July 2017, Washington, DC, USA\nit outperforms C-Reduce in 13 out of 20 benchmarks, highlighting\nits effectiveness with only language-generic transformations.\nBenchmark-Rust. On Benchmark-Rust, Perses and Vulcan pro-\nduce programs with 212.5 and 184.2 tokens on average, separately.\nLPR and LPR+Vulcan further shrink the average program size into\n147.5 and 135.5 tokens. C-Reduce produces the second-largest pro-\ngrams on average, only smaller than Perses. This is anticipated\nsince C-Reduce lacks specialized transformations for Rust.\nFurther analysis into these benchmarks reveals that Vulcan and\ntransformations in LPR are complementary on Benchmark-Rust.\nVulcan demonstrates greater effectiveness in reducing programs\nof relatively smaller size, as highlighted by the average original\nsize of 43 tokens across 9 programs, outperforming LPR. In con-\ntrast, the 15 programs where LPR is proved better than Vulcan\nhave an average of 269 tokens, indicating its proficiency in re-\nducing relatively larger programs. Our speculation is that Vulcan\nand LPR target different reduction opportunities. Vulcan performs\nidentifier/sub-tree replacement and local exhaustive search. Such\nreducers, while lacking in semantic analysis, find reduction oppor-\ntunities in a \"brute-force\" manner and are particularly effective at\nuncovering less obvious reduction opportunities. On the other hand,\nLPR employs more semantic and complex transformations, adeptly\nand systematically analyzing and reducing a complicated program\nstep by step. Moreover, the results ofLPR+Vulcan in the last column\nprove the complementary characteristic between LPR and Vulcan,\nwhich achieve the best in 16 out of the total 20 benchmarks.\nBenchmark-JS. Programs in Benchmark-JS are much simpler\nand smaller than those in the previous two benchmark suites. There-\nfore, even Perses alone is capable of reducing the programs to only\n55.5 tokens. Following this, Vulcan, LPR and LPR+Vulcan achieve\n38.2, 33.9 and 27.5 tokens, further reducing the average results by\n30.35%, 38.66% and 38.66%, separately. Similar to its performance\non Rust, C-Reduce cannot outperform the aforementioned algo-\nrithms on JavaScript, due to its lack of employment of JavaScript‚Äôs\nsemantics. The evaluation results also serve to demonstrate that\nthe performance exhibited by LPR is not attributable to data leak-\nage. These benchmarks were collected by the authors via fuzzing,\nand the optimal results remain inaccessible to the public, thereby\nprecluding any possibility of LLMs memorizing them.\nRQ1: LPR improves Perses by producing 51.33%, 14.87%\nand 38.66% smaller programs on three benchmarks. Moreover,\nLPR+Vulcan improves Vulcan, by 36.73%, 14.39% and 28.15%. On\nC language, LPR+Vulcan performs comparably to C-Reduce, a\nlanguage-specific reducer for C language.\n4.3 RQ2: Reduction Efficiency\nIn this research question, we measure the time elapsed when users\nemploy each technique for program reduction.1 Shorter time indi-\ncates higher efficiency, and Table 2 shows the results. Since both\nVulcan and LPR perform reduction on top of Perses‚Äôs results, it is\n1Please note that the time measured in this research question cannot be directly used\nto compare the computational resources consumed by LPR with those consumed by\nother techniques. LPR is standing on the shoulders of giants, i.e., LLMs, of which\ncomputations heavily rely on GPUs, and it is infeasible for us to measure the actual\nresource consumption of each query to OpenAI API. In contrast, Perses, Vulcan, and\nC-Reduce only leverage CPUs.\nimpossible for these two algorithms to take less time than Perses.\nIn addition, as a highly efficient tree-based reduction algorithm,\nPerses is generally faster than C-Reduce. Therefore, we focus on\nthe comparison among Vulcan, C-Reduce and LPR.\nTable 2: The reduction time (in the format of hh:mm:ss) of\nPerses, Vulcan, C-Reduce, LPR and LPR+Vulcan.\nBenchmarkPerses VulcanC-Reduce LPR LPR +Vulcan\nLLVM-223820:06:46 0:17:03 0:14:46 0:39:43¬±0:17:13 0:44:33¬±0:16:46\nLLVM-227040:33:58 0:38:03 0:22:38 0:48:53¬±0:01:03 0:52:17¬±0:01:23\nLLVM-233090:22:34 2:02:39 0:48:47 1:35:13¬±0:13:36 2:05:40¬±0:14:16\nLLVM-233530:10:31 0:15:05 0:13:36 0:25:23¬±0:06:05 0:30:47¬±0:06:33\nLLVM-259000:09:53 0:23:08 0:22:04 0:44:03¬±0:07:56 0:54:33¬±0:07:21\nLLVM-267600:21:54 0:34:23 0:32:25 0:54:40¬±0:18:08 1:04:28¬±0:16:51\nLLVM-271371:54:41 3:15:20 2:21:43 2:33:15¬±0:09:08 3:13:24¬±0:08:48\nLLVM-277470:13:32 0:28:02 0:25:30 0:32:04¬±0:02:28 0:45:53¬±0:03:17\nLLVM-312590:32:30 4:03:54 1:13:20 4:08:08¬±0:34:26 5:01:39¬±0:54:49\nGCC-599030:48:47 1:21:15 1:23:10 2:14:57¬±0:45:14 2:44:43¬±0:48:09\nGCC-601160:36:18 2:02:01 1:15:45 3:16:38¬±0:31:53 4:25:39¬±0:34:36\nGCC-613830:44:59 3:50:40 1:00:39 2:52:39¬±0:30:41 4:36:47¬±0:22:35\nGCC-619170:14:57 0:24:16 0:44:26 0:37:28¬±0:07:37 0:43:30¬±0:07:31\nGCC-649900:51:05 1:27:12 1:20:05 2:03:05¬±0:31:38 2:22:37¬±0:22:33\nGCC-653830:17:05 0:37:02 0:33:45 0:46:53¬±0:05:25 0:59:14¬±0:05:08\nGCC-661860:41:19 5:35:16 1:24:13 2:39:23¬±0:19:26 4:05:34¬±0:36:49\nGCC-663750:46:28 3:59:57 2:02:40 2:30:08¬±0:10:24 3:06:41¬±0:10:44\nGCC-701270:44:47 4:45:15 1:40:01 2:23:13¬±0:20:36 3:24:03¬±0:20:20\nGCC-705861:33:35 6:53:31 1:37:16 6:24:35¬±1:26:57 10:36:26¬±2:32:53\nGCC-716260:00:40 0:01:18 0:04:06 0:07:38¬±0:01:27 0:08:11¬±0:01:32\nBenchmark-C\nMean 0:35:19 2:08:46 0:59:03 1:54:54¬±0:05:17 2:37:20¬±0:08:05\nRust-448000:13:32 1:58:31 1:33:17 1:47:29¬±0:31:37 2:15:39¬±0:42:02\nRust-668510:59:47 8:49:11 1:32:02 11:21:39¬±9:56:51 16:43:40¬±12:54:53\nRust-690390:07:54 1:25:33 0:10:05 0:24:13¬±0:05:33 0:39:22¬±0:05:51\nRust-770020:04:12 0:20:17 0:29:18 0:52:27¬±0:15:54 0:58:21¬±0:15:18\nRust-773200:00:06 0:01:22 0:01:51 0:02:36¬±0:00:32 0:04:05¬±0:00:32\nRust-773230:00:01 0:00:11 0:00:37 0:00:16¬±0:00:03 0:00:27¬±0:00:04\nRust-779100:00:08 0:00:47 0:01:12 0:04:58¬±0:01:34 0:05:44¬±0:01:35\nRust-779190:00:17 0:02:46 0:05:29 0:08:45¬±0:05:08 0:11:18¬±0:04:52\nRust-780050:00:10 0:01:57 0:02:30 0:10:44¬±0:01:32 0:12:55¬±0:01:31\nRust-783250:00:02 0:00:28 0:01:32 0:00:46¬±0:00:35 0:01:19¬±0:00:34\nRust-786510:00:04 0:00:23 0:01:09 0:01:33¬±0:00:33 0:02:02¬±0:00:36\nRust-786520:00:08 0:01:32 0:03:01 0:02:53¬±0:02:02 0:04:38¬±0:01:59\nRust-786550:00:01 0:00:49 0:01:30 0:02:20¬±0:00:31 0:03:14¬±0:00:32\nRust-787200:00:16 0:03:47 0:06:31 0:11:44¬±0:04:47 0:13:52¬±0:04:55\nRust-917250:03:36 0:17:48 0:37:19 0:16:13¬±0:03:55 0:23:29¬±0:02:11\nRust-998300:48:2320:08:32 11:12:22 4:23:44¬±1:02:03 24:28:10¬±1:37:03\nRust-1115020:00:55 0:10:35 0:10:23 0:37:29¬±0:11:08 0:44:39¬±0:11:13\nRust-1120610:34:50 4:48:04 1:15:44 5:10:02¬±3:12:51 8:01:03¬±3:04:33\nRust-1122130:56:4015:21:26 1:08:21 7:33:21¬±2:44:52 17:07:36¬±3:34:22\nRust-1125260:45:36 2:55:07 1:35:52 3:33:13¬±1:21:21 4:52:26¬±1:54:13\nBenchmark-Rust\nMean 0:13:50 2:49:27 1:00:30 1:50:19¬±0:40:46 3:51:42¬±0:44:16\nJS-1 0:01:29 0:29:19 0:06:59 0:11:47¬±0:05:01 0:14:32¬±0:05:00\nJS-2 0:02:15 0:17:10 0:11:47 0:14:36¬±0:01:12 0:29:51¬±0:04:21\nJS-3 0:01:29 0:26:35 0:06:02 0:09:00¬±0:00:14 0:29:30¬±0:07:58\nJS-4 0:00:19 0:15:43 0:01:44 0:07:22¬±0:02:35 0:37:07¬±0:03:57\nJS-5 0:00:14 0:04:25 0:01:39 0:10:32¬±0:04:27 0:22:49¬±0:04:32\nJS-6 0:02:59 0:23:20 0:11:33 0:13:24¬±0:07:49 0:18:43¬±0:08:08\nJS-7 0:00:15 0:03:30 0:16:21 0:05:28¬±0:01:06 0:10:52¬±0:01:09\nJS-8 0:00:44 0:13:48 0:02:21 0:08:18¬±0:02:20 0:13:04¬±0:02:10\nJS-9 0:01:32 0:14:14 0:07:50 0:09:09¬±0:02:43 0:15:14¬±0:03:40\nJS-10 0:01:31 0:17:44 0:07:24 0:15:04¬±0:04:13 0:32:32¬±0:05:37\nBenchmark-JS\nMean 0:01:17 0:16:35 0:07:22 0:10:28¬±0:01:26 0:22:25¬±0:02:33\nIn Benchmark-C, compared to Vulcan and LPR, C-Reduce gener-\nally has a shorter reduction time. This is expected, as C-Reduce‚Äôs\ntransformations are specifically designed for C languages, whereas\nVulcan approaches the problem in a more unguided and brute-force\nmanner, and LPR‚Äôs attempts are not specifically designed for C\nand rely more on the efficiency of LLMs. On average, LPR takes\n1h:54m:54s, which is 10.77% shorter than 2h:08m:46s of Vulcan.\nHowever, in terms of the average percentage difference of each\nConference‚Äô17, July 2017, Washington, DC, USA Mengxiao Zhang, Yongqiang Tian, Zhenyang Xu, Yiwen Dong, Shin Hwei Tan, and Chengnian Sun\nbenchmark, LPR requires 45.83% more time compared to Vulcan\nin Benchmark-C. The main reason for such a result is that LPR is\nmore efficient than Vulcan when the program is large and complex,\nas the transformations it performs are aware of the semantics and\nhave a higher success rate in reducing the program. However, when\nthe program is small and simple, Vulcan can finish quickly since\nits search space becomes considerably small, but for LPR, the time\nconsumed by the LLM is not decreased significantly and becomes\ndominant, making LPR less efficient than Vulcan in these bench-\nmarks. On Benchmark-Rust, The results indicate a similar trend,i.e.,\nLPR tends to be more efficient when reducing complex programs\nwhile Vulcan is more efficient in small and simple benchmarks. If\nwe only keep benchmarks where both tools take longer than one\nhour, LPR requires 4.15% and 21.69% less time compared to Vulcan.\nIn our analysis of Benchmark-Rust, we observed that both LPR\nand Vulcan can consume an extremely long time on certain bench-\nmarks. For instance, Vulcan requires 20 hours for Rust-99830, while\nLPR takes a similar duration on Rust-66851 in a specific run. This\nextensive time consumption is often due to the frequent invocation\nof Perses, which only achieves marginal progress with each trans-\nformation. The prolonged duration of Perses is primarily attributed\nto the strict syntax of the Rust language. Considering that program\nreduction is an NP-complete problem, this inefficiency might be\noptimized in the future, whereas it cannot be completely eliminated.\nAfter further in-depth analysis, another interesting fact emerges.\nOn the three benchmark suites, the average time taken by LPR is\n1.915, 1.839, and 0.174 hours, respectively. However, within these\ndurations, the time spent waiting for the LLM responses accounts\nfor 32.26%, 28.11%, and 72.99%. Additionally, we measure the effi-\nciency from more perspectives. Take Benchmark-C as an example:\nthe average expense to finish each benchmark amounts to $0.42,\nwith each benchmark requiring 41 queries on average, and every\nquery consuming approximately 39 seconds on average. This ex-\nperiment involves invoking the OpenAI API, and might have been\nlimited by high user traffic and few computational resources allo-\ncated. Considering the ongoing advancements in LLMs technology,\nwe believe that LPR‚Äôs efficiency will be substantially improved in\nthe future.\nRQ2: From the perspective of users, LPR is more efficient than\nVulcan on more complex programs with longer processing time,\nwhile Vulcan reduces faster than LPR on simpler and shorter\nprograms.\n4.4 RQ3: Effectiveness of Each Transformation\nTo answer this question, we delve into the impact of each transfor-\nmation on program size, alongside their potential to escape the local\nminimal program and unlock new reduction opportunities. Our in-\ndepth analysis focuses on Benchmark-C because of its complexity\nof bug-triggering programs compared to other benchmark suites,\nensuring enough room for each transformation to take effect.\nFor each transformation, we measure how the program size\nchanges in each benchmark of Benchmark-C after a specific trans-\nformation is performed, and plot the size changes into a box-plot\nand red dots, as shown in Figure 4. Additionally, since each trans-\nformation is immediately followed by an invocation of Perses, we\nalso monitor the cumulative size changes resulting from both the\ntransformation and its subsequent Perses reduction. These changes\nare depicted by the blue dots in the second box-plot of each subplot\nin Figure 4. Given that a transformation is performed in every itera-\ntion, we compute the average size change by dividing the total sum\nof size changes for that transformation by the number of iterations.\nThis approach enables us to thoroughly comprehend how each\ntransformation influences program size and assess its capacity to\nprovide further reduction opportunities to Perses.\nAccording to size changes induced by each transformation alone,\ni.e., the left box-plot with red dots, we can find two trends. First,\nFunction Inlining , Data Type Elimination and Variable Elimination\nare more likely to reduce the program size by themselves, with an\naverage size change -13.2, -4.7 and -4.3, separately. However, for\nthe rest two transformations, i.e., Loop Unrolling and Data Type\nSimplification, most of the program sizes increase instead, with\nan average of 14.9 and 1.3 respectively. This is expected, as such\ntransformations will generally transform the program into a larger\none. Loop Unrolling , disassembles a loop into repeated lines of code,\nand leads to size increase temporarily. Data Type Simplification can\ndismantle a variable in a complex data type, e.g., structure, into a\nlist of members in primary data types, which may require more\ntokens to declare and initialize each variable.\nFor size changes induced by both a transformation and the sub-\nsequent execution of Perses, i.e., the right box-plot with blue dots,\nall of the proposed transformation result in size decreases. The fact\nthat the right box-plot is generally lower than the left one indicates\nthat Perses often further removes tokens after the transformation\nis applied. For Loop Unrolling and Data Type Simplification , even\nthough they usually introduce more tokens to the program, they ex-\npose reduction opportunities for the following execution of Perses,\nand eventually result in a smaller program.\nTo further understand the impact of each transformation on the\nentirety of Benchmark-C, we provide a detailed analysis of their\ncontributions. Specifically, on the 5 repeated experiments on 20\nbenchmarks, we calculate the average size reduction brought about\nby each transformation across all these 100 runs by summing up\nthe size decreases attributed to the transformation in each bench-\nmark and then computing the mean value, as illustrated in Figure 5.\nAdditionally, we quantify the prevalence of each transformation\nby counting the number of benchmarks in which it induces a size\ndecrease, as demonstrated in Figure 6. The above evaluations to-\ngether offer a comprehensive view of how each transformation\ninfluences program sizes and how frequently they take effect within\nBenchmark-C.\nFrom Figure 5, it is evident that every transformation contributes\nto further size reduction in Benchmark-C. Specifically, while Func-\ntion Inlining is responsible for a reduction of 88.2 tokens on average,\ncontributing 62.12% to the overall decrease. Loop Unrolling shows a\nminimal effect, contributing only 4.8 tokens, 3.35% to the overall\ndecrease. This highlights the varying degrees of influence each\ntransformation has on program size. Further insights from Figure 6\nreveal that Data Type Elimination affects all benchmarks, likely due\nto the ubiquitous presence of typedef across all benchmarks. In\ncontrast, Loop Unrolling is the least prevalent transformation, af-\nfecting merely 6 on average out of 20 benchmarks. This outcome is\nexpected, considering that not all programs involve loop structures,\nLPR: Large Language Models-Aided Program Reduction Conference‚Äô17, July 2017, Washington, DC, USA\n‚àí100\n‚àí50\n0\nŒº : ‚àí 13.2 Œº : ‚àí 25.5\n(a) Function Inlining\n‚àí20\n‚àí10\n0\n10\n20\n30\nŒº : 14.9 Œº : ‚àí 1.1 (b) Loop Unrolling\n‚àí15\n‚àí10\n‚àí5\n0\nŒº : ‚àí 4.7 Œº : ‚àí 6.3 (c) Data Type Elimination\n‚àí15\n‚àí10\n‚àí5\n0\n5\nŒº : 1.3 Œº : ‚àí 2.4 (d) Data Type Simplification\n‚àí20\n‚àí15\n‚àí10\n‚àí5\n0\nŒº : ‚àí 4.3 Œº : ‚àí 5.9 (e) Variable Elimination\nFigure 4: Program size changes induced by each transformation on benchmarks of Benchmark-C. In each subplot, the left\nbox-plot and red dots represent how the size of each program changes before and after executing the transformation. The right\nbox-plot and blue dots represent the size change of each benchmark after executing the transformation and the follow-up\nPerses reduction. There are a total of 20 benchmarks in Benchmark-C, and each experiment is repeated 5 times. Therefore, we\ndraw 100 data points on each boxplot.\nData Type Simplification\n 5.50%, 7.8 tokens\nFunction Inlining\n 62.12%, 88.2 tokens\nLoop Unrolling\n 3.35%, 4.8 tokens\nData Type Elimination\n 14.62%, 20.8 tokens\nVariable Elimination\n 14.41%, 20.4 tokens\nFigure 5: Average size decrease and its percentage induced\nby each transformation within Benchmark-C.\nData Type\nElimination\nVariable\nElimination\nFunction\nInlining\nData Type\nSimplification\nLoop\nUnrolling\nTransformations\n0\n5\n10\n15\n20Number of Benchmarks\n20.0¬±0.0 17.4¬±0.5 15.4¬±0.5 12.4¬±2.3 6.6¬±1.9\nFigure 6: The number of benchmarks impacted by each trans-\nformation within Benchmark-C.\nand not every loop is irrelevant to the compiler bugs. In addition,\nthe relatively higher standard deviations observed in the Data Type\nSimplification and Loop Unrolling suggest that these transforma-\ntions present greater challenges for the LLM to effectively handle.\nRQ3: In Benchmark-C, all proposed transformations contribute\nto the further reduction by either shrinking the programs directly\nor providing reduction opportunities to Perses.\n5 DISCUSSION\nIn this section, we discuss the effectiveness of multi-level prompts\nthe performance of the LLM under different temperatures, and the\nfailures in LPR.\n5.1 The Effectiveness of Multi-level Prompt\nTo validate the effectiveness of our proposed multi-level prompt,\nwe design the corresponding single-level prompt and compare its\neffectiveness against the multi-level prompt. In detail, different from\nmulti-level prompt, single-level prompt merges theprimaryQuestion\nand followupQuestion into a single prompt, e.g., ‚ÄúGiven the follow-\ning program { PROGRAM }, identify one function that can be inlined,\nand inline it. ‚Äù forFunction Inlining .\nIn 5 repeated experiments on Benchmark-C, the single-level\nprompting approach results in 155.0 ¬±8.7 tokens, which is far less\neffective than results from the multi-level prompting approach, i.e.,\n105.8¬±4.4 tokens. Our explanation is that, even though such a single-\nlevel prompt is more compact, it makes LLMs less concentrated on\na specific target, and thus LLMs may omit some targets.\n5.2 The Impact of Temperature\nIn our experiments, we consistently set the temperature parameter\nof LLMs to 1.0. To measure how this parameter affects the per-\nformance, we rerun experiments under multiple temperatures, i.e.,\n0.75, 0.5, 0.25, 0. Note that higher temperature instructs the LLM to\ngenerate more creative and diverse results. Due to limited resources\nand time, we evaluate on 10 benchmarks in Benchmark-C with the\nfastest completion times under the default configuration.\nTable 3: The impact of temperature on reduction sizes.\nPerses Vulcan LPR\nùë° =1 ùë° =0.75 ùë° =0.5 ùë° =0.25 ùë° =0\nMean 161.4 102.8 73.2 ¬±3.0 72.2 ¬±5.0 69.5 ¬±1.0 71.1 ¬±5.9 90.3 ¬±3.8\nAs shown in Table 3, the performances under most of the tem-\nperatures are similar, while the exception is t=0, with the average\nsize worse than others. According to the documentation of tem-\nperature [3], t=0 will actually use a small threshold above 0. Our\nConference‚Äô17, July 2017, Washington, DC, USA Mengxiao Zhang, Yongqiang Tian, Zhenyang Xu, Yiwen Dong, Shin Hwei Tan, and Chengnian Sun\nspeculation is that a low temperature restrains the diversity of out-\nputs, impeding LPR exploring local minima in different runs, which\nis helpful in program reduction tasks.\nGiven that program reduction is an NP-complete problem, ran-\ndomness of LLMs has its advantages and drawbacks. Take the re-\nsults of RQ1 in Table 1 as an example. On the one hand, randomness\nallows LPR to explore more distinct local minima, and sometimes\ngenerates smaller programs than C-Reduce in five repeated experi-\nments on each benchmark. On the other hand, randomness of LLMs\nintroduces variability. While the standard deviation remains below\n10 in most benchmarks, it may significantly increase in certain\nbenchmarks, such as > 60 in GCC-59903. This variation can be\nattributed to the high complexity of the given program. In such\nscenarios, LLMs might not consistently execute accurate transfor-\nmations, resulting in a range of local minima and divergent results.\n5.3 Failures in LPR\nIn LPR, the generation of variants failing to pass the property test\nis common and acceptable. There are two scenarios where LPR\nfails to produce variants passing the property test. First, the LLM\nis generally non-deterministic, it may fail to perform the correct\ntransformation when the program is complex. Therefore, multiple\nresponses are requested in each LLM query to mitigate the impact\nof potential failures. In another scenario, the transformation may\neliminate the bug-triggering pattern, e.g., the compiler crashes on\na function call, but the transformation inlines this function. Even\nif such a transformation is semantically correct, the property test\nwill still fail. When the transformed program is bound to fail, LPR\nwill proceed with the next step using the original program.\nFailures indeed lead to more property tests before making any\nprogress, but they are inevitable, as program reduction is a trial-\nand-error process. Even without LLM, property checks issued by\nPerses and Vulcan usually have a failure rate of around 90%. As\nshown in Table 1 and Table 2, LPR is effective and efficient enough,\nillustrating that failures can be mitigated via multiple responses.\n5.4 Threats to Validity\nIn this section, we discuss potential factors that may undermine\nthe validity of our experimental results.\n5.4.1 Threats to Internal Validity.The main internal threat comes\nfrom the potential data leakage problem. That is, do LLMs provide\nreasonable transformation through step-by-step analysis, or just\nsimply memorize the minimal programs for the benchmark suites,\nwhich may be publicly available on the internet?\nWe mitigate this threat from several perspectives. First, pro-\ngram reduction is a task involving programs distinct from those\nused in program repair or program synthesis tasks. For instance,\nLLMs can learn from large datasets about code generation. On\nthe contrary, programs in program reduction tasks are generally\nlarge, complex, and most importantly, randomly generated to trig-\nger compiler bugs, with no other specific purpose. Their random\nand chaotic characteristics make it highly unlikely for LLMs to\nmemorize such disorganized content, thus reducing the risk of data\nleakage. Moreover, even in scenarios where the LLMs might coin-\ncidentally memorize certain minimal programs, it is improbable\nfor it to link a random-looking, featureless code snippet with a\nspecific memorized program, especially when no explicit bug ID\nis provided. Furthermore, Benchmark-JS, one of the benchmark\nsuites used, was created using JIT fuzzing tools by the authors and\nis not publicly accessible. This exclusivity ensures that the LLMs‚Äô\nperformance on these benchmarks reflects their ability to handle\nunseen and novel programs, thereby showcasing their effectiveness\nin managing new challenges without relying on memorized data.\nThis approach significantly mitigates the risk of data leakage and\ndemonstrates the capacity of LLMs for genuine problem-solving\nand analysis.\n5.4.2 Threats to External Validity.One threat to external validity\nis the generalizability of LPR across languages. Although the ap-\nproach of LPR is language-agnostic, LLMs used by LPR may have\nlimited knowledge of certain languages, which may affect the per-\nformance of LPR. To mitigate this threat, we evaluateLPR on three\nprevalent programming languages, namely, C, Rust and JavaScript.\nThe evaluation results demonstrated the generalizability of LPR\non diverse popular programming languages. For languages that\nare not familiar to LLMs, LPR may still produce reasonable results,\nsince it is shown that LLMs like Codex still perform well in less\npopular languages like Lua (0.2% in Github) [5]. As for a completely\nnew language that LLMs cannot recognize and process, a possi-\nble solution is to incorporate the prompts in LPR with few-shot\nprompting [4, 22], so that LLMs learn how to recognize and process\na new language from the given examples. Further exploration of\nthis approach will be left as future work.\nAn additional threat is the applicability of our approach across\ndifferent LLMs. To mitigate this threat, on the same benchmarks\nused in ¬ß5.2, we repeat the experiments with CodeLlama, another\nwidely-used LLM family. As illustrated in Table 4, the experimen-\ntal results on two models of CodeLlama, i.e., CodeLlama-13b and\nCodeLlama-34b, show no significant differences from results on\nChatGPT. Furthermore, as LLMs continue to evolve, we anticipate\nimprovements in both quality and time of code processing.\nTable 4: The reduction sizes of LPR using various LLMs.\nPerses Vulcan LPR with LPR with CodeLlama\ngpt-3.5-turbo-0613 13b 34b\nMean 161.4 102.8 73.2 ¬± 3.0 73.4 ¬± 3.5 71.3 ¬± 4.9\n6 RELATED WORK\nWe introduce related work in two topics: program reduction and\nLLMs for software engineering.\n6.1 Program Reduction\nDDMin [50] initiated the research topic of program reduction. It\ntreats the input as a list of elements, and consistently splits the\nlist into halves. Then it iteratively attempts to reduce the input list\nby exploring subsets and their complements at varying levels of\ngranularity, transitioning from coarse to fine. Hierarchical Delta\nDebugging [25], short for HDD, parses the program input into a\nparsing tree, and performs DDMin on each level of the tree structure.\nPerses [33] avoids the generation of syntactically invalid program\nLPR: Large Language Models-Aided Program Reduction Conference‚Äô17, July 2017, Washington, DC, USA\nvariants during reduction by formal syntax transformation. Vulcan,\nfurther pushes the limit of Perses via identifier/sub-tree replacement\nand local exhaustive search [ 48]. RCC is a compact refreshable\ncaching scheme to speed up program reduction [38]. All the above\nworks are not customized for certain languages, though having high\ngenerality, they lack semantic knowledge of a certain language for\nfurther reduction.\nBesides, some tools are specifically designed for certain lan-\nguages. C-Reduce [27], incorporating various transformation passes\nfor features in C, is the most effective reducer on this language.\nJ-Reduce [15, 16] is a tool for Java bytecode reduction, it reformu-\nlates the bytecode reduction into dependency graph simplification.\nddSMT [26] is designed for reducing programs in SMT-LIBv2 for-\nmat. All these works leverage language features to reduce more\neffectively than language-generic tools.\nDistinct from prior work, LPR synergistically combines LLMs\nand language-generic reduction tools to harness the advantages\nof both. Language-generic reducers stand out for their remark-\nable generality across multiple languages, while LLMs excel in\nfurther refining the programs with the domain knowledge of cer-\ntain languages learned from large training sets. Language-specific\ntools typically demand considerable human effort to design and im-\nplement feature-related transformations for reduction, while LPR\nrequires only a few lines of natural language prompts, significantly\nreducing the effort involved.\n6.2 LLMs for Software Engineering\nLarge Language Models (LLMs) have proved their remarkable ca-\npability of undertaking multiple text-processing tasks, including\nsource code-related works. Recent works focus on applying LLMs to\nfacilitate software engineering tasks, or assessing the effectiveness,\npotential and limitations of LLMs on software development and\nmaintenance. Some research [13, 45‚Äì47] focus on empirically ap-\nplying LLMs on automatic program repair (APR). Huang et al. [13]\nperformed an empirical study on improvement brought by model\nfine-tuning in APR. Xia et al. [46] thoroughly evaluated 9 state-of-\nthe-art LLMs across multiple datasets and programming languages,\nand demonstrated that directly applying LLMs has already signif-\nicantly outperformed all existing APR techniques. Additionally,\nsome works focus on LLMs‚Äô performance w.r.t. code completion,\ngeneration and fuzzing [6, 21, 35, 53], by leveraging the code anal-\nysis and generation ability of LLMs.\nSimilar to these studies, our approach LPR leverages LLMs for a\nsoftware engineering task, i.e., program reduction. LPR harnesses\nthe comprehension and generation capabilities of LLMs to refine\nthe results of program reduction. However, our work distinguishes\nitself in the nature of the programs processed by LLMs. In related\nresearch, programs are typically logical and goal-oriented, often\ndesigned to fulfill a specific purpose. In contrast, the programs\ninvolved in our program reduction task are random, chaotic, and\nlack a clear objective. Consequently, our research sheds light on\nthe performance of LLMs when dealing with programs that do not\nhave an easily discernible purpose.\n7 CONCLUSION\nThis paper proposes LLMs-aided program reduction (LPR), which\nis the first approach that leverages LLMs for the program reduction\ntask to the best of our knowledge. By combining the strength of\nLLMs and existing language-generic program reduction techniques,\nLPR can perform language-specific transformations to effectively\nreduce the program while being language-generic (i.e., can be easily\napplied to a wide range of languages). The evaluation shows that\nin 50 benchmarks across three programming languages. LPR sig-\nnificantly outperforms Vulcan. Specifically, LPR produces 24.93%,\n4.47%, and 11.71% smaller programs on C, Rust, and JavaScript, re-\nspectively. Meanwhile, The evaluation also demonstrates that LPR\ncomplements Vulcan to some extent. By reducing the outputs of\nLPR with Vulcan, we attained results that have similar sizes to those\nof C-Reduce in Benchmark-C. In terms of efficiency perceived by\nusers, LPR excels in reducing complex programs, and takes 10.77%,\n34.88%, 36.96% less time than Vulcan to finish all the benchmarks,\nrespectively.\n8 DATA AVAILABILITY\nFor replication, we have implemented LPR and released it pub-\nlicly [51].\nACKNOWLEDGMENTS\nWe thank all the anonymous reviewers in ISSTA‚Äô24 for their insight-\nful feedback and comments. This research is partially supported by\nthe Natural Sciences and Engineering Research Council of Canada\n(NSERC) through the Discovery Grant, a project under WHJIL, and\nCFI-JELF Project #40736.\nREFERENCES\n[1] 2023. OpenAI API . Retrieved 2023-11-20 from https://platform.openai.com/docs/\noverview\n[2] 2023. OpenAI API: N . Retrieved 2023-11-20 from https://platform.openai.com/\ndocs/api-reference/chat/create#chat-create-n\n[3] 2023. OpenAI API: Temperature . Retrieved 2023-11-20 from https://platform.\nopenai.com/docs/api-reference/chat/create#chat-create-temperature\n[4] Toufique Ahmed and Premkumar Devanbu. 2022. Few-shot training llms for\nproject-specific code-summarization. In Proceedings of the 37th IEEE/ACM Inter-\nnational Conference on Automated Software Engineering . 1‚Äì5. https://doi.org/10.\n48550/arXiv.2207.04237\n[5] Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-\nCostin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson,\nMolly Q Feldman, et al. 2022. Multipl-e: A scalable and extensible approach to\nbenchmarking neural code generation. arXiv preprint arXiv:2208.08227 (2022).\nhttps://doi.org/10.48550/arXiv.2208.08227\n[6] Yinlin Deng, Chunqiu Steven Xia, Haoran Peng, Chenyuan Yang, and Lingming\nZhang. 2023. Large language models are zero-shot fuzzers: Fuzzing deep-learning\nlibraries via large language models. In Proceedings of the 32nd ACM SIGSOFT\ninternational symposium on software testing and analysis . 423‚Äì435. https://doi.\norg/10.1145/3597926.3598067\n[7] Alastair Donaldson and David MacIver. 2021. Test Case Reduction: Beyond\nBugs. Retrieved May 29, 2023 from https://blog.sigplan.org/2021/05/25/test-\ncase-reduction-beyond-bugs\n[8] Alastair F Donaldson, Paul Thomson, Vasyl Teliman, Stefano Milizia, Andr√© Perez\nMaselco, and Antoni Karpi≈Ñski. 2021. Test-case reduction and deduplication\nalmost for free with transformation-based compiler testing. In Proceedings of the\n42nd ACM SIGPLAN International Conference on Programming Language Design\nand Implementation . 1017‚Äì1032. https://doi.org/10.1145/3453483.3454092\n[9] A. Fan, B. Gokkaya, M. Harman, M. Lyubarskiy, S. Sengupta, S. Yoo, and J. M.\nZhang. 2023. Large Language Models for Software Engineering: Survey and Open\nProblems. In 2023 IEEE/ACM International Conference on Software Engineering:\nFuture of Software Engineering (ICSE-FoSE) . IEEE Computer Society, Los Alamitos,\nCA, USA, 31‚Äì53. https://doi.org/10.1109/ICSE-FoSE59343.2023.00008\nConference‚Äô17, July 2017, Washington, DC, USA Mengxiao Zhang, Yongqiang Tian, Zhenyang Xu, Yiwen Dong, Shin Hwei Tan, and Chengnian Sun\n[10] Zhiyu Fan, Xiang Gao, Martin Mirchev, Abhik Roychoudhury, and Shin Hwei\nTan. 2023. Automated repair of programs from large language models. In 2023\nIEEE/ACM 45th International Conference on Software Engineering (ICSE) . IEEE,\n1469‚Äì1481. https://doi.org/10.1109/ICSE48619.2023.00128\n[11] Qiuhan Gu. 2023. LLM-Based Code Generation Method for Golang Compiler\nTesting. In Proceedings of the 31st ACM Joint European Software Engineering\nConference and Symposium on the Foundations of Software Engineering, ESEC/FSE\n2023, San Francisco, CA, USA, December 3-9, 2023 , Satish Chandra, Kelly Blincoe,\nand Paolo Tonella (Eds.). ACM, 2201‚Äì2203. https://doi.org/10.1145/3611643.\n3617850\n[12] Kihong Heo, Woosuk Lee, Pardis Pashakhanloo, and Mayur Naik. 2018. Effective\nProgram Debloating via Reinforcement Learning. In Proceedings of the 2018 ACM\nSIGSAC Conference on Computer and Communications Security (Toronto, Canada)\n(CCS ‚Äô18) . Association for Computing Machinery, New York, NY, USA, 380‚Äì394.\nhttps://doi.org/10.1145/3243734.3243838\n[13] Kai Huang, Xiangxin Meng, Jian Zhang, Yang Liu, Wenjie Wang, Shuhao Li, and\nYuqing Zhang. 2023. An Empirical Study on Fine-Tuning Large Language Models\nof Code for Automated Program Repair. In 2023 38th IEEE/ACM International\nConference on Automated Software Engineering (ASE) . IEEE Computer Society,\n1162‚Äì1174. https://doi.org/10.1109/ASE56229.2023.00181\n[14] Naman Jain, Skanda Vaidyanath, Arun Shankar Iyer, Nagarajan Natarajan, Suresh\nParthasarathy, Sriram K. Rajamani, and Rahul Sharma. 2022. Jigsaw: Large\nLanguage Models meet Program Synthesis. In 44th IEEE/ACM 44th International\nConference on Software Engineering, ICSE 2022, Pittsburgh, PA, USA, May 25-27,\n2022. ACM, 1219‚Äì1231. https://doi.org/10.1145/3510003.3510203\n[15] Christian Gram Kalhauge and Jens Palsberg. 2019. Binary reduction of depen-\ndency graphs. In Proceedings of the 2019 27th ACM Joint Meeting on European\nSoftware Engineering Conference and Symposium on the Foundations of Software\nEngineering. 556‚Äì566. https://doi.org/10.1145/3338906.3338956\n[16] Christian Gram Kalhauge and Jens Palsberg. 2021. Logical bytecode reduction.\nIn Proceedings of the 42nd ACM SIGPLAN International Conference on Program-\nming Language Design and Implementation . 1003‚Äì1016. https://doi.org/10.1145/\n3453483.3454091\n[17] Vu Le, Mehrdad Afshari, and Zhendong Su. 2014. Compiler validation via\nequivalence modulo inputs. ACM Sigplan Notices 49, 6 (2014), 216‚Äì226. https:\n//doi.org/10.1145/2594291.2594334\n[18] Bastien Lecoeur, Hasan Mohsin, and Alastair F. Donaldson. 2023. Program\nReconditioning: Avoiding Undefined Behaviour When Finding and Reducing\nCompiler Bugs. Proc. ACM Program. Lang. 7, PLDI, Article 180 (jun 2023), 25 pages.\nhttps://doi.org/10.1145/3591294\n[19] Junyi Li, Xiaoxue Cheng, Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. HaluE-\nval: A Large-Scale Hallucination Evaluation Benchmark for Large Language\nModels. In The 2023 Conference on Empirical Methods in Natural Language Pro-\ncessing. https://doi.org/10.48550/arXiv.2305.11747\n[20] Tsz On Li, Wenxi Zong, Yibo Wang, Haoye Tian, Ying Wang, Shing-Chi Cheung,\nand Jeff Kramer. 2023. Nuances are the Key: Unlocking ChatGPT to Find Failure-\nInducing Tests with Differential Prompting. In 38th IEEE/ACM International\nConference on Automated Software Engineering, ASE 2023, Luxembourg, September\n11-15, 2023 . IEEE, 14‚Äì26. https://doi.org/10.1109/ASE56229.2023.00089\n[21] Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. 2023. Is\nyour code generated by chatgpt really correct? rigorous evaluation of large\nlanguage models for code generation. arXiv preprint arXiv:2305.01210 (2023).\nhttps://doi.org/10.48550/arXiv.2305.01210\n[22] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and\nGraham Neubig. 2023. Pre-train, prompt, and predict: A systematic survey of\nprompting methods in natural language processing. Comput. Surveys 55, 9 (2023),\n1‚Äì35. https://doi.org/10.1145/3560815\n[23] Vsevolod Livinskii, Dmitry Babokin, and John Regehr. 2020. Random testing for\nC and C++ compilers with YARPGen. Proceedings of the ACM on Programming\nLanguages 4, OOPSLA (2020), 1‚Äì25. https://doi.org/10.1145/1993498.1993532\n[24] LLVM. 2000. LibTooling. https://clang.llvm.org/docs/LibTooling.html Accessed:\n2023-04-30.\n[25] Ghassan Misherghi and Zhendong Su. 2006. HDD: hierarchical delta debugging.\nIn Proceedings of the 28th International Conference on Software Engineering . 142‚Äì\n151. https://doi.org/10.1145/1134285.1134307\n[26] Aina Niemetz and Armin Biere. 2013. ddSMT: a delta debugger for the SMT-\nLIB v2 format. In Proceedings of the 11th International Workshop on Satisfiability\nModulo Theories, SMT . 8‚Äì9.\n[27] John Regehr, Yang Chen, Pascal Cuoq, Eric Eide, Chucky Ellison, and Xuejun\nYang. 2012. Test-case reduction for C compiler bugs. In Proceedings of the 33rd\nACM SIGPLAN Conference on Programming Language Design and Implementation .\n335‚Äì346. https://doi.org/10.1145/2254064.2254104\n[28] John et al. Regehr. 2012. C-Reduce. Retrieved 2023-11-26 from https://github.\ncom/csmith-project/creduce\n[29] Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta,\nWenhan Xiong, Mor Geva, Jonathan Berant, and Omer Levy. 2022. SCROLLS:\nStandardized CompaRison Over Long Language Sequences. In Proceedings of the\n2022 Conference on Empirical Methods in Natural Language Processing, EMNLP\n2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022 , Yoav Goldberg, Zor-\nnitsa Kozareva, and Yue Zhang (Eds.). Association for Computational Linguistics,\n12007‚Äì12021. https://doi.org/10.18653/V1/2022.EMNLP-MAIN.823\n[30] Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi,\nNathanael Sch√§rli, and Denny Zhou. 2023. Large language models can be easily\ndistracted by irrelevant context. In International Conference on Machine Learning .\nPMLR, 31210‚Äì31227. https://doi.org/10.5555/3618408.3619699\n[31] Chengnian Sun, Vu Le, and Zhendong Su. 2016. Finding compiler bugs via live\ncode mutation. In Proceedings of the 2016 ACM SIGPLAN International Conference\non Object-Oriented Programming, Systems, Languages, and Applications . 849‚Äì863.\nhttps://doi.org/10.1145/2983990.2984038\n[32] Chengnian Sun, Vu Le, Qirun Zhang, and Zhendong Su. 2016. Toward un-\nderstanding compiler bugs in GCC and LLVM. In Proceedings of the 25th In-\nternational Symposium on Software Testing and Analysis . 294‚Äì305. https:\n//doi.org/10.1145/2931037.2931074\n[33] Chengnian Sun, Yuanbo Li, Qirun Zhang, Tianxiao Gu, and Zhendong Su. 2018.\nPerses: Syntax-guided program reduction. In Proceedings of the 40th International\nConference on Software Engineering . 361‚Äì371. https://doi.org/10.1145/3180155.\n3180236\n[34] Maolin Sun, Yibiao Yang, Yang Wang, Ming Wen, Haoxiang Jia, and Yuming Zhou.\n2023. SMT Solver Validation Empowered by Large Pre-Trained Language Models.\nIn 38th IEEE/ACM International Conference on Automated Software Engineering,\nASE 2023, Luxembourg, September 11-15, 2023 . IEEE, 1288‚Äì1300. https://doi.org/\n10.1109/ASE56229.2023.00180\n[35] Haoye Tian, Weiqi Lu, Tsz On Li, Xunzhu Tang, Shing-Chi Cheung, Jacques\nKlein, and Tegawend√© F Bissyand√©. 2023. Is ChatGPT the Ultimate Programming\nAssistant‚ÄìHow far is it? arXiv preprint arXiv:2304.11938 (2023). https://doi.org/\n10.48550/arXiv.2304.11938\n[36] Jia Le Tian, Mengxiao Zhang, Zhenyang Xu, Yongqiang Tian, Yiwen Dong, and\nChengnian Sun. 2023. Ad Hoc Syntax-Guided Program Reduction. In Proceedings\nof the 31st ACM Joint European Software Engineering Conference and Symposium\non the Foundations of Software Engineering, ESEC/FSE 2023, San Francisco, CA,\nUSA, December 3-9, 2023 , Satish Chandra, Kelly Blincoe, and Paolo Tonella (Eds.).\nACM, 2137‚Äì2141. https://doi.org/10.1145/3611643.3613101\n[37] Yongqiang Tian, Zhenyang Xu, Yiwen Dong, Chengnian Sun, and Shing-Chi Che-\nung. 2023. Revisiting the Evaluation of Deep Learning-Based Compiler Testing. In\nProceedings of the Thirty-Second International Joint Conference on Artificial Intelli-\ngence, IJCAI 2023, 19th-25th August 2023, Macao, SAR, China . ijcai.org, 4873‚Äì4882.\nhttps://doi.org/10.24963/IJCAI.2023/542\n[38] Yongqiang Tian, Xueyan Zhang, Yiwen Dong, Zhenyang Xu, Mengxiao Zhang,\nYu Jiang, Shing-Chi Cheung, and Chengnian Sun. 2023. On the Caching Schemes\nto Speed Up Program Reduction. ACM Trans. Softw. Eng. Methodol. 33, 1, Article\n17 (nov 2023), 30 pages. https://doi.org/10.1145/3617172\n[39] Guancheng Wang, Ruobing Shen, Junjie Chen, Yingfei Xiong, and Lu Zhang.\n2021. Probabilistic Delta debugging. In Proceedings of the 29th ACM Joint Meeting\non European Software Engineering Conference and Symposium on the Foundations\nof Software Engineering . 881‚Äì892. https://doi.org/10.1145/3468264.3468625\n[40] Junjie Wang, Yuchao Huang, Chunyang Chen, Zhe Liu, Song Wang, and Qing\nWang. 2024. Software Testing with Large Language Models: Survey, Landscape,\nand Vision. https://doi.org/10.48550/arXiv.2307.07221 arXiv:2307.07221 [cs.SE]\n[41] Junjie Wang, Zhiyi Zhang, Shuang Liu, Xiaoning Du, and Junjie Chen. 2023.\nFuzzJIT: Oracle-Enhanced Fuzzing for JavaScript Engine JIT Compiler. InUSENIX\nSecurity Symposium. USENIX .\n[42] Theodore Luo Wang, Yongqiang Tian, Yiwen Dong, Zhenyang Xu, and Chengnian\nSun. 2023. Compilation Consistency Modulo Debug Information. InProceedings of\nthe 28th ACM International Conference on Architectural Support for Programming\nLanguages and Operating Systems, Volume 2 (Vancouver, BC, Canada)(ASPLOS\n2023). Association for Computing Machinery, New York, NY, USA, 146‚Äì158.\nhttps://doi.org/10.1145/3575693.3575740\n[43] Yuxiang Wei, Chunqiu Steven Xia, and Lingming Zhang. 2023. Copiloting\nthe Copilots: Fusing Large Language Models with Completion Engines for\nAutomated Program Repair. In Proceedings of the 31st ACM Joint European\nSoftware Engineering Conference and Symposium on the Foundations of Soft-\nware Engineering, ESEC/FSE 2023, San Francisco, CA, USA, December 3-9, 2023 ,\nSatish Chandra, Kelly Blincoe, and Paolo Tonella (Eds.). ACM, 172‚Äì184. https:\n//doi.org/10.1145/3611643.3616271\n[44] Yi Wu, Nan Jiang, Hung Viet Pham, Thibaud Lutellier, Jordan Davis, Lin Tan, Petr\nBabkin, and Sameena Shah. 2023. How Effective Are Neural Networks for Fixing\nSecurity Vulnerabilities. In Proceedings of the 32nd ACM SIGSOFT International\nSymposium on Software Testing and Analysis (<conf-loc>, <city>Seattle</city>,\n<state>WA</state>, <country>USA</country>, </conf-loc>) (ISSTA 2023). As-\nsociation for Computing Machinery, New York, NY, USA, 1282‚Äì1294. https:\n//doi.org/10.1145/3597926.3598135\n[45] Chunqiu Steven Xia, Yifeng Ding, and Lingming Zhang. 2023. Revisiting\nthe Plastic Surgery Hypothesis via Large Language Models. arXiv preprint\narXiv:2303.10494 (2023). https://doi.org/10.48550/ARXIV.2303.10494\n[46] Chunqiu Steven Xia, Yuxiang Wei, and Lingming Zhang. 2023. Automated\nprogram repair in the era of large pre-trained language models. In Proceedings of\nLPR: Large Language Models-Aided Program Reduction Conference‚Äô17, July 2017, Washington, DC, USA\nthe 45th International Conference on Software Engineering (ICSE 2023). Association\nfor Computing Machinery . https://doi.org/10.1109/ICSE48619.2023.00129\n[47] Chunqiu Steven Xia and Lingming Zhang. 2023. Keep the Conversation Go-\ning: Fixing 162 out of 337 bugs for $0.42 each using ChatGPT. arXiv preprint\narXiv:2304.00385 (2023). https://doi.org/10.48550/arXiv.2304.00385\n[48] Zhenyang Xu, Yongqiang Tian, Mengxiao Zhang, Gaosen Zhao, Yu Jiang, and\nChengnian Sun. 2023. Pushing the Limit of 1-Minimality of Language-Agnostic\nProgram Reduction. Proceedings of the ACM on Programming Languages 7, OOP-\nSLA1 (2023), 636‚Äì664. https://doi.org/10.1145/3586049\n[49] Xuejun Yang, Yang Chen, Eric Eide, and John Regehr. 2011. Finding and un-\nderstanding bugs in C compilers. In Proceedings of the 32nd ACM SIGPLAN\nConference on Programming Language Design and Implementation . 283‚Äì294.\nhttps://doi.org/10.1145/1993316.1993532\n[50] Andreas Zeller and Ralf Hildebrandt. 2002. Simplifying and isolating failure-\ninducing input. IEEE Transactions on Software Engineering 28, 2 (2002), 183‚Äì200.\nhttps://doi.org/10.1109/32.988498\n[51] Mengxiao Zhang, Yongqiang Tian, Zhenyang Xu, Yiwen Dong, Shin Hwei Tan,\nand Chengnian Sun. 2024. Artifact for \"LPR: Large Language Models-Aided\nProgram Reduction\". https://doi.org/10.5281/zenodo.10901714\n[52] Mengxiao Zhang, Zhenyang Xu, Yongqiang Tian, Yu Jiang, and Chengnian Sun.\n2023. PPR: Pairwise Program Reduction. In Proceedings of the 31st ACM Joint\nEuropean Software Engineering Conference and Symposium on the Foundations of\nSoftware Engineering . 338‚Äì349.\n[53] Li Zhong and Zilong Wang. 2023. A study on robustness and reliability of\nlarge language model code generation. arXiv preprint arXiv:2308.10335 (2023).\nhttps://doi.org/10.48550/arXiv.2308.10335"
}