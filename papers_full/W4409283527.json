{
    "title": "Foundation model of neural activity predicts response to new stimulus types",
    "url": "https://openalex.org/W4409283527",
    "year": 2025,
    "authors": [
        {
            "id": null,
            "name": "Wang, Eric Y.",
            "affiliations": [
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4283499780",
            "name": "Fahey, Paul G.",
            "affiliations": [
                "Baylor College of Medicine",
                "Stanford University",
                "Neurosciences Institute"
            ]
        },
        {
            "id": null,
            "name": "Ding, Zhuokun",
            "affiliations": [
                "Neurosciences Institute",
                "Baylor College of Medicine",
                "Stanford University"
            ]
        },
        {
            "id": null,
            "name": "Papadopoulos, Stelios",
            "affiliations": [
                "Baylor College of Medicine",
                "Neurosciences Institute",
                "Stanford University"
            ]
        },
        {
            "id": "https://openalex.org/A4283499788",
            "name": "Ponder, Kayla",
            "affiliations": [
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4226951559",
            "name": "Weis, Marissa A.",
            "affiliations": [
                "University of Göttingen"
            ]
        },
        {
            "id": "https://openalex.org/A4288022709",
            "name": "Chang, Andersen",
            "affiliations": [
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4214448454",
            "name": "Muhammad, Taliah",
            "affiliations": [
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4227258056",
            "name": "Patel, Saumil",
            "affiliations": [
                "Neurosciences Institute",
                "Stanford University",
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A2146939372",
            "name": "Ding Zhiwei",
            "affiliations": [
                "Neurosciences Institute",
                "Stanford University",
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A2743866692",
            "name": "Tran Dat",
            "affiliations": [
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A5059330609",
            "name": "Fu Jiakun",
            "affiliations": [
                "Baylor College of Medicine"
            ]
        },
        {
            "id": null,
            "name": "Schneider-Mizell, Casey M.",
            "affiliations": [
                "Allen Institute for Brain Science",
                "Allen Institute"
            ]
        },
        {
            "id": "https://openalex.org/A4201025057",
            "name": "- -",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2315481345",
            "name": "Reid R. Clay",
            "affiliations": [
                "Allen Institute",
                "Allen Institute for Brain Science"
            ]
        },
        {
            "id": "https://openalex.org/A2218630469",
            "name": "Collman, Forrest",
            "affiliations": [
                "Allen Institute",
                "Allen Institute for Brain Science"
            ]
        },
        {
            "id": null,
            "name": "da Costa, Nuno Maçarico",
            "affiliations": [
                "Allen Institute",
                "Allen Institute for Brain Science"
            ]
        },
        {
            "id": "https://openalex.org/A5094329630",
            "name": "Franke Katrin",
            "affiliations": [
                "Allen Institute",
                "Allen Institute for Brain Science"
            ]
        },
        {
            "id": "https://openalex.org/A4226951562",
            "name": "Ecker, Alexander S.",
            "affiliations": [
                "Neurosciences Institute",
                "Stanford University",
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4214448452",
            "name": "Reimer, Jacob",
            "affiliations": [
                "University of Göttingen",
                "Max Planck Institute for Dynamics and Self-Organization"
            ]
        },
        {
            "id": "https://openalex.org/A2333621107",
            "name": "Pitkow, Xaq",
            "affiliations": [
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4283499793",
            "name": "Sinz, Fabian H.",
            "affiliations": [
                "Rice University",
                "Baylor College of Medicine"
            ]
        },
        {
            "id": "https://openalex.org/A4221359228",
            "name": "Tolias, Andreas S.",
            "affiliations": [
                "Baylor College of Medicine",
                "University of Tübingen",
                "University of Göttingen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4409283533",
        "https://openalex.org/W2166206801",
        "https://openalex.org/W2963138386",
        "https://openalex.org/W2760951817",
        "https://openalex.org/W2949449018",
        "https://openalex.org/W3212587793",
        "https://openalex.org/W2058616551",
        "https://openalex.org/W4377134093",
        "https://openalex.org/W2898929289",
        "https://openalex.org/W2943083682",
        "https://openalex.org/W4297497819",
        "https://openalex.org/W2988313851",
        "https://openalex.org/W4288404646",
        "https://openalex.org/W2890119559",
        "https://openalex.org/W2150367061",
        "https://openalex.org/W2037781104",
        "https://openalex.org/W2956728914",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W3135367836",
        "https://openalex.org/W4409283531",
        "https://openalex.org/W4312115573",
        "https://openalex.org/W2080275784",
        "https://openalex.org/W2298475536",
        "https://openalex.org/W3124792448",
        "https://openalex.org/W3201017316",
        "https://openalex.org/W3001279689",
        "https://openalex.org/W2059317754",
        "https://openalex.org/W2569458742",
        "https://openalex.org/W3092173471",
        "https://openalex.org/W6850542711",
        "https://openalex.org/W4324130692",
        "https://openalex.org/W2004900646",
        "https://openalex.org/W2394641095",
        "https://openalex.org/W2887114371",
        "https://openalex.org/W2805271873",
        "https://openalex.org/W2016053056",
        "https://openalex.org/W2087461657",
        "https://openalex.org/W1501755754",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2963446712",
        "https://openalex.org/W2899663614",
        "https://openalex.org/W1485009520",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W4214493665",
        "https://openalex.org/W6791943378",
        "https://openalex.org/W4311415873",
        "https://openalex.org/W2963263347",
        "https://openalex.org/W2551302722"
    ],
    "abstract": "Abstract The complexity of neural circuits makes it challenging to decipher the brain’s algorithms of intelligence. Recent breakthroughs in deep learning have produced models that accurately simulate brain activity, enhancing our understanding of the brain’s computational objectives and neural coding. However, it is difficult for such models to generalize beyond their training distribution, limiting their utility. The emergence of foundation models 1 trained on vast datasets has introduced a new artificial intelligence paradigm with remarkable generalization capabilities. Here we collected large amounts of neural activity from visual cortices of multiple mice and trained a foundation model to accurately predict neuronal responses to arbitrary natural videos. This model generalized to new mice with minimal training and successfully predicted responses across various new stimulus domains, such as coherent motion and noise patterns. Beyond neural response prediction, the model also accurately predicted anatomical cell types, dendritic features and neuronal connectivity within the MICrONS functional connectomics dataset 2 . Our work is a crucial step towards building foundation models of the brain. As neuroscience accumulates larger, multimodal datasets, foundation models will reveal statistical regularities, enable rapid adaptation to new tasks and accelerate research.",
    "full_text": "470 | Nature | Vol 640 | 10 April 2025\nArticle\nFoundation model of neural activity predicts \nresponse to new stimulus types\nEric Y . Wang1,2, Paul G. Fahey1,2,3,4,5, Zhuokun Ding1,2,3,4,5, Stelios Papadopoulos1,2,3,4,5, \nKayla Ponder1,2, Marissa A. Weis6, Andersen Chang1,2, Taliah Muhammad1,2,  \nSaumil Patel1,2,3,4,5, Zhiwei Ding1,2, Dat Tran1,2, Jiakun Fu1,2, Casey M. Schneider-Mizell7, \nMICrONS Consortium*, R. Clay Reid7, Forrest Collman7, Nuno Maçarico da Costa7, \nKatrin Franke1,2,3,4,5, Alexander S. Ecker6,8, Jacob Reimer1,2, Xaq Pitkow1,2,9, Fabian H. Sinz1,2,6,10 \n& Andreas S. Tolias1,2,3,4,5,11 ✉\nThe complexity of neural circuits makes it challenging to decipher the brain’s \nalgorithms of intelligence. Recent breakthroughs in deep learning have produced \nmodels that accurately simulate brain activity, enhancing our understanding of the \nbrain’s computational objectives and neural coding. However, it is difficult for such \nmodels to generalize beyond their training distribution, limiting their utility. The \nemergence of foundation models1 trained on vast datasets has introduced a new \nartificial intelligence paradigm with remarkable generalization capabilities. Here we \ncollected large amounts of neural activity from visual cortices of multiple mice and \ntrained a foundation model to accurately predict neuronal responses to arbitrary \nnatural videos. This model generalized to new mice with minimal training and \nsuccessfully predicted responses across various new stimulus domains, such as \ncoherent motion and noise patterns. Beyond neural response prediction, the model \nalso accurately predicted anatomical cell types, dendritic features and neuronal \nconnectivity within the MICrONS functional connectomics dataset2. Our work is  \na crucial step towards building foundation models of the brain. As neuroscience \naccumulates larger, multimodal datasets, foundation models will reveal statistical \nregularities, enable rapid adaptation to new tasks and accelerate research.\nDeep artificial neural networks (ANNs) have revolutionized neurosci-\nence by modelling neural activity based on sensory input, behaviour \nand internal states3–9. Task-driven models, for instance, have provided \nvaluable insights into the visual cortex, as their hidden representa -\ntions often align with biological neural activity when trained on tasks \nsuch as object classification or predictive coding10,11. With increasing \naccess to large-scale neuroscience datasets, data-driven models are \nsurpassing task-driven approaches12, enabling in silico experiments that \nsystematically analyse neuronal representations and computational \nprinciples. In vision research, such approaches help to characterize \nneuronal selectivity13,14 and tuning functions under natural conditions15, \ngenerating hypotheses for closed-loop experiments such as inception \nloops16. This in silico–in vivo strategy addresses key challenges in neu-\nroscience, including high-dimensional inputs, nonlinear processing \nand experimental constraints.\nA major challenge in neural network modelling, however, is gener-\nalization beyond the original training distribution17. Models trained \non natural videos predict responses well within that domain but \nstruggle with synthetic or parametric stimuli18. Given the historical \nimportance of parametric stimuli in vision research19–21, it is crucial  \nto develop functional models that generalize across stimulus dom-\nains. Recent advancements in artificial intelligence, particularly \nfoundation models trained on vast datasets1, offer a solution. These  \nmodels capture robust, transferable representations that general-\nize to novel tasks, as seen in language models trained on diverse text  \ncorpora22,23.\nInspired by these breakthroughs, we developed a foundation model \nof the mouse visual cortex trained on extensive data to predict neu-\nral activity from dynamic visual stimuli and behaviour. We recorded \nresponses to ecological videos from approximately 135,000 neurons \nacross multiple visual cortex areas in 14 awake, behaving mice. With a \nsubset of these data, we trained a deep neural network on recordings \nfrom eight mice, producing a ‘foundation core’ that captured shared \nlatent representations and predicted neuronal responses across mice \nand cortical areas. Models using this foundation core could be rapidly \nadapted to new mice with minimal data, outperforming individualized \nmodels trained end-to-end. These models excelled in predicting neu-\nronal responses to both in-domain natural videos and out-of-domain \nhttps://doi.org/10.1038/s41586-025-08829-y\nReceived: 24 April 2023\nAccepted: 21 February 2025\nPublished online: 9 April 2025\nOpen access\n Check for updates\n1Center for Neuroscience and Artificial Intelligence, Baylor College of Medicine, Houston, TX, USA. 2Department of Neuroscience, Baylor College of Medicine, Houston, TX, USA. 3Department \nof Ophthalmology, Byers Eye Institute, Stanford University School of Medicine, Stanford, CA, USA. 4Stanford Bio-X, Stanford University, Stanford, CA, USA. 5Wu Tsai Neurosciences Institute, \nStanford University, Stanford, CA, USA. 6Institute of Computer Science and Campus Institute Data Science, University of Göttingen, Göttingen, Germany. 7Allen Institute for Brain Science, \nSeattle, WA, USA. 8Max Planck Institute for Dynamics and Self-Organization, Göttingen, Germany. 9Department of Electrical and Computer Engineering, Rice University, Houston, TX, USA. \n10Institute for Bioinformatics and Medical Informatics, University of Tübingen, Tübingen, Germany. 11Department of Electrical Engineering, Stanford University, Stanford, CA, USA. *A list of \nauthors and their affiliations appears at the end of the paper. ✉e-mail: tolias@stanford.edu\nNature | Vol 640 | 10 April 2025 | 471\nstimuli, including moving dots, flashing dots, Gabor patches, coherent \nnoise and static natural images.\nT o evaluate the broader utility of our model, we assessed its ability \nto predict anatomical features. In the Machine Intelligence from Corti-\ncal Networks (MICrONS) dataset2, which contains functional record-\nings and nanoscale anatomy of more than 70,000 neurons, our model \naccurately classified anatomically defined types of excitatory neurons. \nFurthermore, in other MICrONS studies, our model successfully pre-\ndicted synaptic connectivity24 and dendritic morphology25.\nIn summary, we present a foundation model of neural activity that \nnot only predicts visual cortex responses but also relates the functional \nproperties of neurons to their anatomical features. Our results dem-\nonstrate the potential of data-driven foundation models to advance \nsystems neuroscience by enabling scalable, generalizable representa-\ntions of neural function.\nDynamic functional model of the mouse visual cortex\nT o model the dynamic neuronal responses of the mouse visual cortex, we \ndeveloped an ANN that comprised of four modules: perspective, modula-\ntion, core and readout (Fig. 1). The modular design enabled the ANN to \naccommodate diverse tasks and inputs. For instance, eye movements  \nand different positioning of a mouse’s head relative to the monitor can \nresult in different perspectives of the same stimulus, despite best efforts \nto limit experimental variability. T o account for this, the perspective \nmodule of our ANN uses ray tracing and eye tracking data to infer the \nperspective of the mouse from the presented stimulus on the monitor \n(Extended Data Fig. 1). T o account for behavioural factors that modulate \nthe activity of the visual cortex26, the modulation module transforms \nbehavioural inputs (locomotion and pupil dilation) to produce dynamic \nrepresentations of the mouse’s behavioural and attentive state (Extended \nData Fig. 2). The perspective and modulation modules provide visual \nand behavioural inputs, respectively, to the core module of the ANN. \nComposed of feedforward (3D convolution layers) and recurrent (long \nshort-term memory) components, the core contains the majority of the \nmodelling capacity of the ANN and produces nonlinear representations \nof vision that are modulated by behaviour. These representations are \nmapped onto the activity of individual neurons by the readout module, \nwhich performs a linear combination of the features generated by the core \nat one specific location, the neuron’s receptive field. All four modules \nof the ANN (perspective, modulation, core and readout) were trained \nend-to-end to predict time series of neuronal responses to natural videos \n(details of model architecture and training are presented in Methods).\nFirst, we evaluated the predictive accuracy of our ANN model archi-\ntecture when trained on individual recording sessions lasting around \n1 h. Predictive accuracy was measured by the correlation between the \nrecorded and the predicted responses to a novel set of stimuli that \nwere not included in model training. T o account for in vivo noise, \nV1\nLM\nRL\nALL2/3\nL4\nL5\nEye position\nStimulus\nPerspective\nModulation\nReadout\nt\nt + 1\nt + 2\nLocomotion,\npupil size\nIn vivo\n\nIn silico\nIn vivo\nIn silico\nIn vivoIn silico\n100 /uni03BCm\nTime\nNeural activityVisual cortex\nRecurrent\nCore\nFeedforward\nFig. 1 | ANN model of the visual cortex. Top left, an in vivo recording session  \nof excitatory neurons from several areas (V1, LM, RL and AL) and layers (layer \n2/3 (L2/3), layer 4 (L4) and layer 5 (L5)) of the mouse visual cortex. Right, the \narchitecture of the ANN model and the flow of information from inputs (visual \nstimulus, eye position, locomotion and pupil size) to outputs (neural activity). \nUnderlined labels denote the four main modules of the ANN. For the modulation \nand core, the stacked planes represent feature maps. For the readout, the blue \nboxes represent the output features of the core at the readout position of the \nneuron, and the fanning black lines represent readout feature weights. The top \nof the schematic displays the neural activity for a sampled set of neurons. In vivo \nand in silico responses are shown for two example neurons. Stimulus adapted \nfrom Sports-1M Dataset (Andrej Karpathy; https://cs.stanford.edu/people/\nkarpathy/deepvideo/ ); copyright 2014, IEEE, reprinted with permission from \nIEEE Proceedings, IEEE ( CC BY3.0).\n472 | Nature | Vol 640 | 10 April 2025\nArticle\nthe correlation was normalized by an estimated upper bound on the \nperformance that could be achieved by a perfect model27. Using this \nnormalized correlation coefficient (CCnorm) as the metric of predictive \naccuracy, we compared our model to the previous best-performing \ndynamic model of the mouse visual cortex18. Trained and tested on \nthe same data from that study (dynamic primary visual cortex (V1) \nresponses to natural videos), our model showed a 25–46% increase in \npredictive accuracy on held-out test data across the three recording \nsessions used in Sinz et al.18 (Fig. 2a). This level of increase in perfor-\nmance is substantial for predictive models of the visual cortex. We also \nevaluated the predictive accuracy of our model on newly collected data \nthat contained multiple visual areas (Fig. 2b). Of note, we found that the \nperformance of our model for higher visual areas (lateromedial (LM), \nrostrolateral (RL) and anterolateral (AL)) was similar to V1 (Fig. 2c), \ndespite the increased complexity of neuronal tuning to more complex \nfeatures exhibited by higher visual areas28,29.\nNext, we performed lesion studies to determine the effect that indi-\nvidual components of the model had on predictive accuracy (Extended \nData Fig. 3). Removing either of the 2 behavioural modules resulted \nin a modest but significant reduction in reduced predictive accuracy: \n2.3% reduction for perspective (Extended Data Fig. 3a–e) and 2.8% \nfor modulation (Extended Data Fig. 3f–j). For the core component, \nwe found that using 3D convolutions in the feedforward component \nsignificantly improved performance compared to 2D convolutions, \nalthough the difference was small at 0.88% (Extended Data Fig. 3k–o).  \nWe also evaluated the objective function used for training and found \nthat Poisson negative-log likelihood loss significantly outperformed \nmean squared error loss, with a performance difference of 9.6% \n(Extended Data Fig. 3p–t). In summary, our ANN model sets new stand-\nards for predicting dynamic neuronal responses of the visual cortex, \nwith individual components contributing modest but significant \nimprovements. Notably, the main driver of increased performance \nis the much larger dataset used for training (Fig.  2b), aligning with  \nscaling laws and the observation that ANN performance in general \nimproves with increasing data30.\nGeneralization to new subjects and stimulus domains\nThe remarkable performance of foundation models in other domains—\nfor example, natural language22 and image generation23—originates \nfrom their vast quantities of training data. However, collecting large \namounts of neuronal data from individual neurons and animals pre-\nsents challenges. Individual recording sessions are limited in dura-\ntion by experimental factors such as attentiveness and stability of \nthe recording device. T o overcome this limitation, we combined data \nfrom multiple recording sessions, resulting in a total of more than \n900 min of natural video responses from 8 mice, 6 visual areas (V1, \nLM, AL, RL, anteromedial (AM) and posteromedial (PM)) and around \n66,000 neurons (Extended Data Table 1). These data were used to train \na single, shared ANN core (Fig. 3a) with the goal of capturing common \nrepresentations of vision that underlie the dynamic neuronal response \nof the visual cortex for a representative set of neurons and a group \nof mice. This representation could then be used to fit models of new \nmice to improve their performance with limited data. Here we refer to \nthe representative group of eight mice as the ‘foundation cohort’ , the \ntrained ANN component as the ‘foundation core’ , and ANNs derived \nfrom the foundation core as ‘foundation models’ .\nT o evaluate the representation of the visual cortex captured by the \nfoundation core, we froze its parameters and transferred it to ANNs \nwith new perspective, modulation and readout components fitted \nto new mice (Fig. 3a). Each new mouse was shown an assortment of \nstimuli, designated for either model training or testing. The training \nstimuli consisted of natural videos, and we used different portions \nof this, spanning from 4 min to 76 min, to fit ANN components to the \nnew mice. This approach aimed to examine the relationship between \nthe models’ performance and the amount of training data for each \nnew mouse. The testing stimuli included natural videos that were \nnot part of the training set (Fig. 3b′), new stimulus domains such as \nstatic natural images (Fig. 3c′), and four types of parametric stimuli \n(Fig. 3d′–g′), comprising drifting Gabor filters, flashing Gaussian dots, \ndirectional pink noise and random dot kinematograms. T o test the \nrole of the foundation core in prediction performance, we trained a \nset of control models that differed from the foundation models only \nby the core component. For these controls (individual models), all \nfour components—core, perspective, modulation and readout—were \ntrained end-to-end using training data from a single recording session. \nFor the foundation models, training data from the new mice were used \nonly to fit the perspective, modulation and readout components, and \nthe core was trained on the foundation cohort as described above and \nwas frozen (Fig. 3a).\nV1 LM RL AL\nVisual area\nNS\nca\nSinz et al.,\n2018\nPresent\nmodel\n0\n0.2\n0.4\n0.6\nMedian CCnorm\n0\n0.2\n0.4\n0.6\nMedian CCnorm\n0\n0.2\n0.4\n0.6\nMedian CCnorm\n**\n3 mice, 1 visual area (V1)\n(from Sinz et al., 2018)\n4 16 28 40 52 64 76\nMinutes of training data\nb 4 mice, 4 visual areas (V1, LM, RL and AL)\nn = 1,723\nn = 4,672\nn = 1,340\nn = 10,246\nn = 9,452\nn = 8,014\nn = 8,862\nFig. 2 | Predictive accuracy of models trained on individual recording \nsessions.  a, Predictive accuracy (median CC norm across neurons; Methods)  \nof our model versus a previous state-of-the-art dynamic model of the mouse \nvisual cortex (Sinz et al. 18). We trained and tested our model on the same set  \nof data that were used in ref. 18—V1 neuronal responses to natural videos from \nthree mice. n refers to the number of neurons per mouse. ** P < 0.01, paired \ntwo-way t-test, t = 14.53, d.f. = 2. b, Predictive accuracy of our models versus  \nthe amount of data used for training for four new recording sessions and mice. \nFor each recording session, training data were partitioned in to 7 fractions \nranging from 4 min to 76 min. Separate models (diamonds) were trained on the \ndiffering fractions of training data, and all were tested on the same held-out \ntesting data. Models of the same mice are connected by lines. c , Predictive \naccuracy for each visual area from models that were trained on the full data.  \nWe did not find a statistically significant relationship between predictive \naccuracy and visual areas (linear mixed effects model 37; NS, not significant  \nby Wald test, P = 0.45, d.f. = 3).\nNature | Vol 640 | 10 April 2025 | 473\nWhen tested on natural videos, foundation models outperformed \nindividual models and required less training data from the new mice \nto achieve high levels of predictive accuracy (Fig. 3b). For instance, \nindividual models required more than 60 min of training data to sur-\npass a median CCnorm of 0.65 for all mice, whereas foundation models \nrequired less than 30 min (Fig. 3b). This performance gain was observed \nacross all tested stimulus domains, including those that were in new \nstimulus domains (Fig. 3c′–g′)—that is, out-of-distribution from the \ntraining domain of natural videos (Fig. 3b′). Notably, no stimuli from \nthe out-of-distribution domains were used to train any component of \nthe models, including the foundation core. Nevertheless, foundation \nmodels were more accurate at predicting responses to new stimulus \ndomains and required substantially less training data from the new \nmice (Fig. 3c–g). For example, when predicting drifting Gabor filters, \nthe foundation models were able to achieve a performance of median \nCCnorm greater than 0.55 using only 16 min of natural video training data. \nIn contrast, the individual models required more than an hour of train-\ning data to reach the same performance level (Fig. 3d). This highlights \nthe substantial difference in the data efficiency of these models—that \nis, the amount of training data (sample complexity) required from new \nsubjects to accurately fit their neuronal responses. Thus, training a \nfoundation dynamic core on natural video data pooled from multiple \ncortical layers, areas and mice produces a robust and transferable repre-\nsentation of the visual cortex that generalizes to new mice and improves \nmodel performance for natural videos and for novel stimulus domains.\nWhen combining functional studies of the brain with other modalities \nsuch as anatomy, there is typically a limited amount of time available \nfor in vivo recordings before destructive histological analysis is per-\nformed. Whereas traditionally this would limit the number of functional \nstudies that can be performed in vivo, predictive models allow essen-\ntially unlimited scans to be performed in silico, even after tissue has \nbeen destroyed. T o enable this for the MICrONS project, responses \nto natural videos were collected for the purpose of model training. \nOwing to the challenge of completing all 14 scans in the same mouse \nin as short a period as possible, the amount of training data collected \nfrom each experiment (mean 42 min, range 33–53 min, depending on \noptical quality and mouse behavioural profile) was less than in the \nother recording sessions described in this Article. With the available \namount of data, individual models—with all components trained on \na single experiment—achieved a median CC norm of 0.48–0.65 when \ntested on a held-out set of natural videos. By applying our foundation \nmodelling paradigm—transferring the foundation core and fitting \nonly the perspective, modulation and readout components on a single \nexperiment—the median CCnorm increased to 0.58–0.76 (Extended Data \nFig. 4). This highlights the advantage of the foundation modelling \napproach when there is a limited amount of data available for training.\nClassical studies of parametric tuning\nBy leveraging the foundation core and transfer learning, we were able \nto create accurate foundation models for individual mice (Fig. 3). These \nmodels enable essentially unlimited in silico experiments for studying \nrepresentations, testing theories and generating novel hypotheses \nthat can be verified in vivo. Here we assessed the precision with which \nclassical tuning properties of the visual cortex could be replicated at \nthe individual neuronal level in our foundation model. We presented \nmice—not part of the foundation cohort—with natural video stimuli \nin order to train their ANN counterparts (Fig.  4a). Additionally, we \n4 16 28 40 52 64 76 4 16 28 40 52 64 76\nNatural videos\nTraining stimuli\nTesting stimuli (6 domains)\nPerspective\nModulation\nIndividual\ncore\nReadout\nPerspective\nPerspective\nModulation\nModulation\nFoundation\ncore\nReadout\nReadout\nPerspective\nModulation\nFoundation\ncore\nReadout\nMedian CCnorm on testing stimuli\nNatural videosb\n0\n0.2\n0.4\n0.6\n0.8\n0\n0.2\n0.4\n0.6\n0.8\n0\n0.2\n0.4\n0.6\n0.8\n0\n0.2\n0.4\n0.6\n0.8\n0\n0.2\n0.4\n0.6\n0.8\n0\n0.2\n0.4\n0.6\n0.8\nAmount of new training data (minutes of natural videos)\nDrifting Gabor /f_iltersd\nDirectional pink noisef\nNatural imagesc\nFlashing Gaussian dotse\nRandom dot kinematogramsg\na b' c'\nd' e'\nf' g'\nFoundation\nIndividual\nFoundation cohort\nvs\nNew (n = 4)\nTransfer\n\n\n\n\n\n\nFig. 3 | Predictive accuracy of foundation models. a, Schematic of the training \nand testing paradigm. Natural video data were used to train: (1) a combined \nmodel of the foundation cohort of mice with a single foundation core; and  \n(2) foundation models versus individual models of new mice. Stimulus adapted \nfrom Sports-1M Dataset (Andrej Karpathy; https://cs.stanford.edu/people/\nkarpathy/deepvideo/ ); copyright 2014, IEEE, reprinted with permission from \nIEEE Proceedings, IEEE (CC BY 3.0). b′–g′, The models of the new mice were  \ntested with stimuli comprising natural videos (b′; adapted from Pixabay  \nimage (https://pixabay.com/photos/black-and-white-tunnel-the-way-  \n1730543/; CC0 Content)), natural images (c′; adapted from Pixabay image  \n(https://pixabay.com/photos/butterfly-insect-meadow-491166/ ; CC0 Content)), \ndrifting Gabor filters ( d′), flashing Gaussian dots ( e′), directional pink noise ( f′) \nand random dot kinematograms ( g′). b–g, Corresponding plots for b ′–g′, \nrespectively, show the predictive accuracy (median CC norm across neurons)  \nas a function of the amount of training data for foundation models versus \nindividual models (grey) of the new mice (4 mice × 7 partitions of training \ndata × 2 types of models = 56 models (diamonds)). Models of the same mouse \nand type (foundation or individual) are connected by lines. Number of neurons \nper mouse: 8,862, 8,014, 9,452 and 10,246, respectively.\n474 | Nature | Vol 640 | 10 April 2025\nArticle\npresented parametric stimuli (Fig. 4b′,c′) to measure the orientation, \ndirection and spatial tuning of the recorded neurons. Subsequently, we \npresented the same parametric stimuli to the corresponding in silico \nneurons and measured their properties for comparison (Fig.  4b,c). \nThis was done for 3 mice and approximately 30,000 neurons from 4 \nvisual areas (V1, LM, AL and RL).\nT o measure orientation and direction tuning, we presented direc-\ntional pink noise (Fig. 4b′), which encoded coherent motion of different \ndirections (0–360°) and orientations (0–180°). First, we computed the \nstrength of orientation and direction tuning via selectivity indices for \norientation (OSI) and direction (DSI). There was a high correspond-\nence between in vivo and in silico estimates for both OSI (Fig. 4d) and \nDSI (Fig. 4f), which validated the foundation model’s estimates of \ntuning strength for orientation and direction. Next, we estimated the \npreferred angles of orientation and direction of neurons by fitting a \ndirectional parametric model (mixture of von Mises distributions) \nto the responses. For strongly tuned neurons, the in vivo and in silico \nestimates of preferred angles of orientation and direction were closely \nmatched (Fig. 4e,g). For example, for strongly orientation-tuned neu-\nrons with an in silico OSI greater than 0.5 (11% of neurons), the median \ndifference between the in vivo and in silico estimates of preferred ori-\nentation was 4°, and with a lower OSI threshold of over 0.3 (43% of \nneurons), the median difference was 7° (Fig. 4e).\nT o measure spatial tuning, we presented flashing Gaussian dots \n(Fig.  4c′) to the neurons described above. We computed a spike-  \ntriggered average (STA) of the stimulus, which was used to estimate: \n(1) the strength of spatial tuning for Gaussian dots (non-uniformity \nof the STA) via the spatial selectivity index (SSI); and (2) the preferred \nlocation (peak of the STA) via least-squares fit ting of the STA to a spa-\ntial parametric model (2D Gaussian distribution). Although using the \nGaussian dot stimulus did not elicit strong SSI for the majority of neu-\nrons, for those neurons that were strongly tuned in silico, we observed \na close match between in vivo and in silico estimates of spatial tuning \nstrength, measured by SSI (Fig. 4h). For instance, for strongly tuned \nneurons with in silico SSI greater than 8, the median distance between \nthe in vivo and in silico estimates of the preferred location was 0.02 of \nthe monitor width (Fig. 4i), approximately 2° in visual space.\nT ogether, these results demonstrate the accuracy of estimating tun-\ning parameters for classical functional properties from our foundation \nmodel with no prior training on parametric stimuli. Therefore, rather \nthan presenting parametric stimuli in vivo, parametric tuning can be \nperformed in silico with an accurate and validated foundation model, \nfreeing up valuable in vivo experimental time for other purposes.\nPrediction of structural properties of neurons\nThe function of the neocortex emerges mechanistically from its cir-\ncuit structure. The MICrONS project, a landmark dataset in neuro -\nscience, provides unprecedented scale and resolution, combining \nmillimetre-scale functional recordings with anatomical structure at \nnanometre resolution, across multiple visual cortical areas of a single \nmouse. In the MICrONS mouse, the responses of more than 70,000 \nexcitatory neurons to natural videos were measured across 14 sequen-\ntial scans, encompassing a 1 mm3 volume spanning V1, LM, AL and RL \nvisual areas. This volume was subsequently subjected to serial electron \nmicroscopy and dense morphological reconstruction (Fig. 5b), result-\ning in detailed structures of approximately 60,000 excitatory neurons \nand 500 million synapses, representing the largest integrated study of \nneocortical structure and function to date2.\nWe used the foundation modelling paradigm to the MICrONS dataset \nto model the function of excitatory neurons within the 1 mm3 volume. \nThe model’s readout module maps the output of the foundation core \nonto individual neuronal responses. The readout parameters of each \nneuron consist of two components: readout position and readout fea-\nture weights (Fig. 5a). We trained readout parameters for all excitatory \nneurons recorded in the MICrONS volume and we investigated whether \nthese parameters would be useful for studying the structure–function \nrelationship of the brain.\nIn silico SSI In silico OSIIn silico DSI\nTrainingTuningTuning\nPerspective\nModulation\nFoundation\ncore\nReadout\na\nNatural videos\nDirectional pink noise\nFlashing Gaussian dots\nb'\nc'\nIn vivo\nIn silico\nHeight\nMonitor width0 1\nb\nc\n0°\n45°\n90°\n135°\n180°\n225°\n270°\n315°\nIn vivo SSI\nIn vivo DSI\nIn vivo OSI\nIn silicoo SSI In silico OSIIn silico DSI\n0 1\n0\n1\n0 1\n0\n1\n6 9\n6\n9\n/uni0394 Orientation (°)\n/uni0394 Direction (°)\n/uni0394 Location (monitor width)\nOrientation selectivity\nDirection selectivity\nSpatial selectivity\nd\nf\nh\nOrientation preference\nDirection preference\nSpatial preference\ne\ng\ni\n0 15 30 45 60 75 90\n>0\n>0.1\n>0.2\n>0.3\n>0.4\n>0.5\n0 30 60 90 120 150 180\n>0\n>0.1\n>0.2\n>0.3\n>0.4\n>0.5\n0 0.1 0.2 0.3 0.4 0.5\n>6.0\n>6.4\n>6.8\n>7.2\n>7.6\n>8.0\nn\nn\nn\n1\n25\n1\n100\n1\n250\nn = 3\nFig. 4 | Parametric tuning from foundation models.  a,b′,c′, Schematic of the \nexperimental paradigm: foundation models of new mice ( n = 3) were trained \nwith natural videos, and estimates of parametric tuning were computed from \nin vivo and in silico responses to synthetic stimuli (directional pink noise ( b′) \nand flashing Gaussian dots ( c′)). Adapted from Sports-1M Dataset (Andrej \nKarpathy; https://cs.stanford.edu/people/karpathy/deepvideo/ ); copyright \n2014, IEEE, reprinted with permission from IEEE Proceedings, IEEE ( CC BY 3.0). \nb,c, In vivo and in silico estimates of an example neuron’s parametric tuning to \norientation and direction ( b) and spatial location (c). d,f,h, Binned scatter plots \nof in vivo and in silico estimates of orientation ( d), direction (f) and spatial (h) \nselectivity indices. Grey colour bar indicates the number of neurons ( n) in each \nbin. DSI, direction selectivity index; OSI, orientation selectivity index; SSI, \nspatial selectivity index. e ,g,i, Density histograms of differences between \nin vivo and in silico estimates of preferred orientation ( e), direction (g) and \nspatial location ( i). Histograms containing increasingly selective groups of \nneurons thresholded by in silico OSI ( e), DSI (g) and SSI (i) are stacked from top \nto bottom. Density histograms were produced via kernel density estimation \nusing Scott’s bandwidth.\nNature | Vol 640 | 10 April 2025 | 475\nWe first examined the readout position, which consists of two param-\neters per neuron: azimuthal (x) and altitudinal (y) locations, specifying \nthe centre of the receptive field learned by the model for each neuron. \nAnalysis of the readout positions revealed that they accurately captured \nthe retinotopic organization of the visual cortex (Fig. 5c). In V1, readout \nx positions aligned with the medial–lateral axis, and y positions aligned \nwith the rostral–caudal axis. At the border of V1 and LM/RL, there was \nan inversion of the axis for the x readout position, demarcating the \ntransition zone between these areas. This organization of readout posi-\ntions according to anatomical locations aligns well with prior studies \nof retinotopic organization in the mouse visual cortex31,32.\nNext, we investigated how the readout weights, a 512-dimensional \nvector per neuron, could be used to predict anatomical proper -\nties such as the visual area and morphologically defined cell types. \nThese readout weights serve as a functional barcode, encoding the \ntuning of the neuron to visual features produced by the core mod -\nule at its readout position. We found that these functional barcodes \ncaptured differences between visual areas (V1, LM, AL and RL). Using \nlogistic regression, the readout weights could predict visual areas \nwith a balanced accuracy of 68%, exceeding the chance level of 25% \n(Fig. 5d). We further explored the possibility of predicting 11 mor -\nphologically defined excitatory cell types from layers 2 to 5 of the \nneocortex (Fig. 5f), which were identified by Schneider-Mizell et al.33. \nAgain, using logistic regression, we achieved a balanced accuracy of \n32% for cell-type prediction, outperforming the chance baseline of \n9% (Fig. 5e). Because these cell types are fairly well separated across \ncortical depth (Fig. 5f), it is possible that the classifier has learned to \npredict depth directly from the depth-varying signal-to-noise ratio \nof two-photon (2P) imaging. T o control for this potential confound, \nwe trained a classifier to predict cell types from 2P depth (reduced \nmodel) and compared to a second classifier provided with both 2P \ndepth and readout feature weights (full model). We found that the full \nmodel significantly outperformed the reduced model in predicting \ncell types (likelihood ratio test, P < 10−9), indicating that the readout \nfeature weights contribute to classifier performance. Collectively, \nthese results demonstrate that our foundation model captures both \nfunctional and structural properties of neurons, making it a valuable \ntool for analysing structure–function relationships within the MICrONS \nvolume and studying mechanisms of computation within the visual  \ncortex.\nIn vivo recordings\nof excitatory neurons\na\nPerspective\nModulation\nFoundation\ncore\nReadout\nFeature\nweights\nPosition\nReadout parameters\nCore output\n(t)\n0 1 0 1x (a.u.) y (a.u.)\nReadout position\nc\nV1\nLM\nAL\nRL\nV1\nLM\nAL\nRL\nb\nIn silico\nmodel\nd\nNeuron 1\nNeuron 2\nFeatures\nV1 LM RL AL\nPredicted visual area\nV1\nLM\nRL\nAL\nTrue visual area\n0\n0.2\n0.4\n0.6\n0.8\nL2aL2bL2cL3aL3bL4aL4bL4cL5aL5bL5ET\nPredicted cell type\nL2a\nL2b\nL2c\nL3a\nL3b\nL4a\nL4b\nL4c\nL5a\nL5b\nL5ET True cell type\n0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6e\nf\nL1\nL2/3\nL4\nL5\nL2a L2b L2c L3a L3b L4a L4b L4c L5a L5b L5ET\n500 /uni03BCm\nFig. 5 | The foundation model of the MICrONS volume relates neuronal \nfunction to structure and anatomy.  a, Schematic of a foundation model of  \nthe MICrONS mouse, trained on excitatory neuronal responses to natural \nvideos. At the bottom, the readout at a single time point is depicted, showing \nthe readout positions and feature weights for two example neurons. Adapted \nfrom Sports-1M Dataset (Andrej Karpathy; https://cs.stanford.edu/people/\nkarpathy/deepvideo/ ); copyright 2014, IEEE, reprinted with permission  \nfrom IEEE Proceedings, IEEE ( CC BY 3.0). b, Meshes of two example neurons, \nreconstructed from serial electron microscopy. Inset, magnified view of the \nindicated area, showing a synapse between these two neurons, with the pre-\nsynaptic axon in black and the post-synaptic dendrite in grey. Scale bar, 1 μm.  \nc, Coloured scatter plots of readout positions of all neurons from a recording \nsession of the MICrONS mouse, overlaid on a top-down view of the recording \nwindow with annotated visual areas (V1, LM, RL and AL) and boundaries. Plots \nare coloured by the x (left) and y (right) coordinates of the readout positions. \nScale bar, 100 μm. a.u., arbitrary units. d , Confusion matrix of MICrONS visual \nareas predicted from readout feature weights, normalized per row. The diagonal \nrepresents the recall for each visual area. e , Confusion matrix of MICrONS \nexcitatory neuron cell types predicted from readout feature weights, \nnormalized per row. The excitatory neuron cell types are from Schneider-Mizell \net al.33. The diagonal represents the recall for each cell type. f , Morphologies  \nof different types of excitatory neurons. Two example neurons are shown for \neach excitatory neuron cell type. L5ET, layer 5 extratelencephalic-projecting \nneurons.\n476 | Nature | Vol 640 | 10 April 2025\nArticle\nDiscussion\nIn this Article, we introduce a foundation model of the mouse visual cor-\ntex that achieves state-of-the-art performance at predicting dynamic \nneuronal responses across multiple visual areas, marking notable pro-\ngress towards an accurate functional digital twin of the mouse visual \nsystem.\nBeyond excelling in the natural video domain on which it was trained, \nour model accurately predicted responses to new stimulus domains \nsuch as noise patterns and static images. Its generalization performance \non new stimulus domains highlights its ability to capture nonlinear \ntransformations from image space to neuronal activity in the mouse \nvisual cortex. The foundation core enabled accurate models of new \nmice to be fitted with limited training data, which outperformed models \nwith cores that were individually trained for each mouse, underscoring \nthe power of transfer learning to capture latent representations that \nexplain neural activity across mice34.\nNotably, we also demonstrate the utility of our model for making \npredictions beyond neural activity—for example, in tasks related to \nanatomy and connectivity—which greatly enhances its utility as a foun-\ndation model of the brain1. Specifically, by transferring the foundation \ncore, we built a digital twin of the MICrONS dataset, which enabled us \nto extract a functional barcode for each neuron—a vector embedding \nthat describes the input–output function of visual response. Although \nthe model was trained without anatomical information (that is, with-\nout electron microscopy data), the functional barcodes successfully \npredicted anatomical cell types identified in an accompanying Article \nthat analyses cellular morphology from the MICrONS electron micros-\ncopy dataset33.\nThe compact representation of neuronal function provided by the \nfunctional barcodes in our model was utilized in several other MICrONS \nstudies examining the relationship between neuronal function and \nanatomy. In a study characterizing the morphological landscape of \ncortical excitatory neurons, the functional barcodes predict detailed \nfeatures of dendritic morphology of layer 4 pyramidal neurons25. In \nanother Article, the functional barcodes predict synaptic connectiv-\nity, beyond what could be explained by physical proximity of axons \nand dendrites24.\nIn summary, the results presented here and in the accompanying \nArticles24,25,35,36 that utilize our model demonstrate the power of the \nfoundation modelling approach for neuroscience research. Its abil-\nity to uncover subtle patterns in neural organization, such as cellular \nmorphology and synaptic connectivity, showcases the potential of the \nmodel for driving new insights in neuroscience. In large projects such \nas MICrONS, where dataset longevity is highly desirable, the strong \ngeneralization capabilities of our foundation model and its ability to \nperform tasks beyond the original training domain offer clear benefits. \nThis extends the utility of the dataset beyond its initial scope, enabling \nresearchers to explore questions that were not originally considered \nand facilitating discoveries in neural circuit organization.\nOur work was inspired by recent breakthroughs in artificial intel-\nligence, where foundation models1 trained on massive data volumes \nhave demonstrated remarkable generalization in many downstream \ntasks. Applied to neuroscience, the foundation modelling paradigm \novercomes a major limitation of previous common approaches in which \nmodels are individually trained using data from a single experiment. \nThe limited amount of data hinders the accuracy of models as they \nlearn from scratch the complex nonlinearities of the brain, even though \nthere is a great deal of similarity in how visual neurons respond. By \ncontrast, foundation models combine data from multiple experiments, \nincluding data from many brain areas and subjects under high-entropy \nnatural conditions, giving them access to a much larger and richer \nset of data; only the specific idiosyncrasies of each individual mouse \nand its neurons must be learned separately. In other words, the simi-\nlarities between neurons and subjects can be leveraged to identify \ncommon features of the brain, producing a more unified and accu-\nrate model of the brain that is informed by multiple subjects rather  \nthan one.\nOur present foundation model is just the beginning, as it only models \nparts of the mouse visual system under passive viewing conditions. By \nexpanding this approach to encompass complex, natural behaviours \nin freely moving subjects, incorporating additional brain regions and \ncell types, the development of multimodal foundation neuroscience \nmodels offers a powerful new approach to deciphering the algorithms \nthat underpin natural intelligence. As we accumulate more diverse \nmultimodal data—encompassing sensory inputs, behaviours and neural \nactivity across various scales, modalities and species, foundation neu-\nroscience models will enable us to decipher the neural code of natural \nintelligence, providing unprecedented insights into the fundamental \nprinciples of the brain.\nOnline content\nAny methods, additional references, Nature Portfolio reporting summa-\nries, source data, extended data, supplementary information, acknowl-\nedgements, peer review information; details of author contributions \nand competing interests; and statements of data and code availability \nare available at https://doi.org/10.1038/s41586-025-08829-y.\n1. Bommasani, R. et al. On the opportunities and risks of foundation models. Preprint at \nhttps://doi.org/10.48550/arXiv.2108.07258 (2021).\n2. The MICrONS Consortium. Functional connectomics spanning multiple areas of mouse \nvisual cortex. Nature https://doi.org/10.1038/s41586-025-08790-w (2025).\n3. Cadieu, C. F. et al. Deep neural networks rival the representation of primate IT cortex for \ncore visual object recognition. PLoS Comput. Biol. 10, e1003963 (2014).\n4. Batty, E. et al. Multilayer network models of primate retinal ganglion cells. In Proc. 5th \nInternational Conference for Learning Representations (ICLR, 2017).\n5. McIntosh, L. T., Maheswaranathan, N., Nayebi, A., Ganguli, S. & Baccus, S. A. Deep \nlearning models of the retinal response to natural scenes. Adv. Neural Inf. Process. Syst. \n29, 1369–1377 (2016).\n6. Klindt, D. A., Ecker, A. S., Euler, T. & Bethge, M. Neural system identification for large \npopulations separating what and where. In Proc. 31st International Conference on Neural \nInformation Processing Systems 3509–3519 (ACM, 2017).\n7. Kindel, W. F., Christensen, E. D. & Zylberberg, J. Using deep learning to probe the neural \ncode for images in primary visual cortex. JOV 19, 29 (2019).\n8. Cadena, S. A. et al. Deep convolutional models improve predictions of macaque v1 \nresponses to natural images. PLoS Computat. Biol. 15, e1006897 (2019).\n9. Bashiri, M. et al. A flow-based latent state generative model of neural population \nresponses to natural images. Adv. Neural Inf. Process. Syst. 34, 15801–15815 (2021).\n10. Yamins, D. L. K. et al. Performance-optimized hierarchical models predict neural \nresponses in higher visual cortex. Proc. Natl Acad. Sci. USA 111, 8619–8624 (2014).\n11. Bakhtiari, S., Mineault, P., Lillicrap, T., Pack, C. & Richards, B. The functional specialization \nof visual cortex emerges from training parallel pathways with self-supervised predictive \nlearning. Adv. Neural Inf. Process. Syst. 34, 25164–25178 (2021).\n12. Pierzchlewicz, P. et al. Energy guided diffusion for generating neurally exciting images. \nAdv. Neural Inf. Process. Syst. 36, 32574–32601 (2024).\n13. Bashivan, P., Kar, K. & DiCarlo, J. J. Neural population control via deep image synthesis. \nScience 364, eaav9436 (2019).\n14. Ponce, C. R. et al. Evolving images for visual neurons using a deep generative network \nreveals coding principles and neuronal preferences. Cell 177, 999–1009.e10 (2019).\n15. Franke, K. et al. State-dependent pupil dilation rapidly shifts visual feature selectivity. \nNature 610, 128–134 (2022).\n16. Walker, E. Y. et al. Inception loops discover what excites neurons most using deep \npredictive models. Nat. Neurosci. 22, 2060–2065 (2019).\n17. Hendrycks, D. & Dietterich, T. G. Benchmarking neural network robustness to common \ncorruptions and perturbations. Preprint at https://doi.org/10.48550/arXiv.1903.12261 \n(2019).\n18. Sinz, F. et al. Stimulus domain transfer in recurrent models for large scale cortical \npopulation prediction on video. In Proc. 32nd International Conference on Neural \nInformation Processing Systems 7199–7210 (ACM, 2018).\n19. Britten, K. H., Shadlen, M. N., Newsome, W. T. & Movshon, J. A. The analysis of visual \nmotion: a comparison of neuronal and psychophysical performance. J. Neurosci. 12, \n4745–4765 (1992).\n20. Salzman, C. D., Britten, K. H. & Newsome, W. T. Cortical microstimulation influences \nperceptual judgements of motion direction. Nature 346, 174–177 (1990).\n21. Marshel, J. H. et al. Cortical layer-specific critical dynamics triggering perception. Science \n365, eaaw5202 (2019).\n22. Brown, T. B. et al. Language models are few-shot learners. In Advances in Neural \nInformation Processing Systems (eds. Larochelle, H. et al.) vol. 33, 1877–1901 (Curran \nAssociates, 2020).\n23. Radford, A. et al. Learning transferable visual models from natural language supervision. \nIn Proc. 38th International Conference on Machine Learning (eds. Meila, M. & Zhang, T.) \nvol. 139, 8748–8763 (PMLR, 2021).\nNature | Vol 640 | 10 April 2025 | 477\n24. Ding, Z. et al. Functional connectomics reveals general wiring rule in mouse visual cortex. \nNature https://doi.org/10.1038/s41586-025-08840-3 (2025).\n25. Weis, M. A. et al. An unsupervised map of excitatory neurons’ dendritic morphology in the \nmouse visual cortex. Nat. Commun. (in the press).\n26. Reimer, J. et al. Pupil fluctuations track fast switching of cortical states during quiet \nwakefulness. Neuron 84, 355–362 (2014).\n27. Schoppe, O., Harper, N. S., Willmore, B. D. B., King, A. J. & Schnupp, J. W. H. Measuring the \nperformance of neural models. Front. Comput. Neurosci. 10, 10 (2016).\n28. Siegle, J. H. et al. Survey of spiking in the mouse visual system reveals functional \nhierarchy. Nature 592, 86–92 (2021).\n29. Goltstein, P. M., Reinert, S., Bonhoeffer, T. & Hübener, M. Mouse visual cortex areas \nrepresent perceptual and semantic features of learned visual categories. Nat. Neurosci. \n24, 1441–1451 (2021).\n30. Kaplan, J. et al. Scaling laws for neural language models. Preprint at https://doi.org/ \n10.48550/arXiv.2001.08361 (2020).\n31. Garrett, M. E., Nauhaus, I., Marshel, J. H. & Callaway, E. M. Topography and areal organization \nof mouse visual cortex. J. Neurosci. 34, 12587–12600 (2014).\n32. Zhuang, J. et al. An extended retinotopic map of mouse cortex. eLife 6, e18372 (2017).\n33. Schneider-Mizell, C. M. et al. Inhibitory specificity from a connectomic census of mouse \nvisual cortex. Nature https://doi.org/10.1038/10.1038/s41586-024-07780-8 (2025).\n34. Lurz, K.-K. et al. Generalization in data-driven models of primary visual cortex. In 9th \nInternational Conference on Learning Representations (ICLR, 2021).\n35. Ding, Z. et al. Bipartite invariance in mouse primary visual cortex. Nat. Neurosci.  \n(in the press).\n36. Fu, J. et al. Pattern completion and disruption characterize contextual modulation  \nin mouse visual cortex. Preprint at bioRxiv https://doi.org/10.1101/2023.03.13.532473 \n(2023).\n37. Lindstrom, M. J. & Bates, D. M. Newton–Raphson and EM algorithms for linear \nmixed-effects models for repeated-measures data. J. Am. Stat. Assoc. 83, 1014 (1988).\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in \npublished maps and institutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution \n4.0 International License, which permits use, sharing, adaptation, distribution \nand reproduction in any medium or format, as long as you give appropriate \ncredit to the original author(s) and the source, provide a link to the Creative Commons licence, \nand indicate if changes were made. The images or other third party material in this article are \nincluded in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your \nintended use is not permitted by statutory regulation or exceeds the permitted use, you will \nneed to obtain permission directly from the copyright holder. To view a copy of this licence, \nvisit http://creativecommons.org/licenses/by/4.0/.\n© The Author(s) 2025\nMICrONS Consortium\nEric Y . Wang1,2, Paul G. Fahey1,2,3,4,5, Zhuokun Ding1,2,3,4,5, Stelios Papadopoulos1,2,3,4,5, \nMarissa A. Weis6, Andersen Chang1,2, Taliah Muhammad1,2, Saumil Patel1,2,3,4,5, Zhiwei Ding1,2, \nDat Tran1,2, Jiakun Fu1,2, Casey M. Schneider-Mizell7, R. Clay Reid7, Forrest Collman7, \nNuno Maçarico da Costa7, Alexander S. Ecker6,8, Jacob Reimer1,2, Xaq Pitkow1,2,9, \nFabian H. Sinz1,2,6,10 & Andreas S. Tolias1,2,3,4,5,11\nArticle\nMethods\nNeurophysiological experiments\nMICrONS data in Fig. 5 were collected as described in the accompa-\nnying Article2, and data in Fig. 2a were collected as described18. Data \ncollection for all other figures is described below.\nAll procedures were approved by the Institutional Animal Care and \nUse Committee of Baylor College of Medicine. Fourteen mice (Mus \nmusculus, 6 females, 8 males, age 2.2–4 months) expressing GCaMP6s \nin excitatory neurons via Slc17a7-Cre and Ai162 transgenic lines (recom-\nmended and shared by H. Zeng; JAX stock 023527 and 031562, respec-\ntively) were anaesthetized and a 4-mm craniotomy was made over the \nvisual cortex of the right hemisphere as described previously26,38. Mice \nwere allowed at least five days to recover before experimental scans.\nMice were head-mounted above a cylindrical treadmill and 2P cal-\ncium imaging was performed using Chameleon Ti-Sapphire laser \n(Coherent) tuned to 920 nm and a large field of view mesoscope 39 \nequipped with a custom objective (excitation NA 0.6, collection NA \n1.0, 21 mm focal length). Laser power after the objective was increased \nexponentially as a function of depth from the surface according to: \nPP=× e zL\n0\n(/ )z , where P is the laser power used at target depth z, P0 is the \npower used at the surface (not exceeding 20 mW), and Lz is the depth \nconstant (220 μm). The highest laser output of 100 mW was used at \napproximately 420 μm from the surface.\nThe craniotomy window was leveled with regards to the objective \nwith six degrees of freedom. Pixel-wise responses from a region of \ninterest spanning the cortical window (>2,400 × 2,400 μm, 2–5 μm per \npixel, between 100 and 220 μm from surface, >2.47 Hz) to drifting bar \nstimuli were used to generate a sign map for delineating visual areas31. \nArea boundaries on the sign map were manually annotated.\nFor 11 out of 15 scans (including four of the foundation cohort scans), \nour target imaging site was a 1,200 × 1,100 μm2 area spanning L2–L5 at \nthe conjunction of lateral V1 and 3 lateral higher visual areas: AL, LM \nand RL. This resulted in an imaging volume that was roughly 50% V1 \nand 50% higher visual area. This target was chosen in order to mimic \nthe area membership and functional property distribution in the \nMICrONS mouse2 Each scan was performed at 6.3 Hz, collecting eight \n620 × 1,100 μm2 fields per frame at 2.5 μm per pixel x–y resolution to tile \na 1,200–1,220 × 1,100 μm2 field of view at 4 depths (2 planes per depth, \n20–40 μm overlap between coplanar fields). The four imaging planes \nwere distributed across layers with at least 45 μm spacing, with 2 planes \nin L2/3 (depths: 170–200 μm and 215–250 μm), 1 in L4 (300–325 μm) \nand 1 in L5 (390–420 μm).\nFor the remaining four foundation cohort scans, our target imag-\ning site was a single plane in L2/3 (depths 210–220 μm), spanning all \nvisual cortex visible in the cortical window (typically including V1, \nLM, AL, RL, PM and AM). Each scan was performed at 6.8–6.9 Hz, col-\nlecting four 630 μm width adjacent fields (spanning 2,430 μm region \nof interest, with 90 μm total overlap). Each field was a custom height \n(2,010–3,000 μm) in order to encapsulate visual cortex within that \nfield. Imaging was performed at 3 μm per pixel.\nVideo of the eye and face of the mouse was captured throughout \nthe experiment. A hot mirror (Thorlabs FM02) positioned between \nthe left eye and the stimulus monitor was used to reflect an IR image \nonto a camera (Genie Nano C1920M, T eledyne Dalsa) without obscur-\ning the visual stimulus. The position of the mirror and camera were \nmanually calibrated per session and focused on the pupil. Field of view \nwas manually cropped for each session. The field of view contained the \nleft eye in its entirety, and was captured at ~20 Hz. Frame times were \ntime stamped in the behavioural clock for alignment to the stimulus \nand scan frame times. Video was compressed using the Labview MJPEG \ncodec with quality constant of 600 and stored the frames in AVI file.\nLight diffusing from the laser during scanning through the pupil was \nused to capture pupil diameter and eye movements. A DeepLabCut \nmodel40 was trained on 17 manually labelled samples from 11 mice to \nlabel each frame of the compressed eye video (intraframe only H.264 \ncompression, CRF:17) with 8 eyelid points and 8 pupil points at cardinal \nand intercardinal positions. Pupil points with likelihood >0.9 (all 8 in \n72–99% of frames per scan) were fit with the smallest enclosing circle, \nand the radius and centre of this circle was extracted. Frames with <3 \npupil points with likelihood >0.9 (<1.2% frames per scan), or producing a \ncircle fit with outlier >5.5× s.d. from the mean in any of the 3 parameters \n(centre x, centre y, radius, <0.2% frames per scan) were discarded (total \n<1.2% frames per scan). Gaps of ≤10 discarded frames were replaced by \nlinear interpolation. Trials affected by remaining gaps were discarded \n(<18 trials per scan, <0.015%).\nThe mouse was head-restrained during imaging but could walk on \na treadmill. Rostro-caudal treadmill movement was measured using a \nrotary optical encoder (Accu-Coder 15T-01SF-2000NV1ROC-F03-S1) \nwith a resolution of 8,000 pulses per revolution, and was recorded at \n~100 Hz in order to extract locomotion velocity. The treadmill record-\ning was low-pass filtered with a Hamming window to remove high-  \nfrequency noise.\nMonitor positioning and calibration\nVisual stimuli were presented with Psychtoolbox in MATLAB to the left \neye with a 31.0 × 55.2 cm (height × width) monitor (ASUS PB258Q) with \na resolution of 1,080 × 1,920 pixels positioned 15 cm away from the eye. \nWhen the monitor is centred on and perpendicular to the surface of the \neye at the closest point, this corresponds to a visual angle of 3.8° cm−1 \nat the nearest point and 0.7° cm−1 at the most remote corner of the \nmonitor. As the craniotomy coverslip placement during surgery and \nthe resulting mouse positioning relative to the objective is optimized \nfor imaging quality and stability, uncontrolled variance in skull posi-\ntion relative to the washer used for head-mounting was compensated \nwith tailored monitor positioning on a six-dimensional monitor arm. \nThe pitch of the monitor was kept in the vertical position for all mice, \nwhile the roll was visually matched to the roll of the head beneath the \nheadbar by the experimenter. In order to optimize the translational \nmonitor position for centred visual cortex stimulation with respect \nto the imaging field of view, we used a dot stimulus with a bright back-\nground (maximum pixel intensity) and a single dark square dot (mini-\nmum pixel intensity). Randomly ordered dot locations drawn from \neither a 5 × 8 grid tiling the screen (20 repeats) or a 10 × 10 grid tiling a \ncentral square (approximately 90° width and height, 10 repeats), with \neach dot presentation lasting 200 ms. For five scans (four foundation \ncohort scans, one scan from Fig. 4), this dot-mapping scan targeted \nthe V1–RL–AL–LM conjunction, and the final monitor position for each \nmouse was chosen in order to maximize inclusion of the population \nreceptive field peak response in cortical locations spanning the scan \nfield of view. In the remaining scans, the procedure was the same, but \nthe scan field of view spanned all of V1 and some adjacent higher visual \nareas, and thus the final monitor position for each mouse was chosen \nin order to maximize inclusion of the population receptive field peak \nresponse in cortical locations corresponding to the extremes of the \nretinotopic map. In both cases, the yaw of the monitor visually matched \nto be perpendicular to and 15 cm from the nearest surface of the eye \nat that position.\nA photodiode (TAOS TSL253) was sealed to the top left corner of the \nmonitor, and the voltage was recorded at 10 kHz and time stamped with \na 10 MHz behaviour clock. Simultaneous measurement with a lumi-\nnance meter (LS-100 Konica Minolta) perpendicular to and targeting \nthe centre of the monitor was used to generate a lookup table for linear \ninterpolation between photodiode voltage and monitor luminance in \ncd m−2 for 16 equidistant values from 0–255, and 1 baseline value with \nthe monitor unpowered.\nAt the beginning of each experimental session, we collected photo-\ndiode voltage for 52 full-screen pixel values from 0 to 255 for 1-s tri-\nals. The mean photodiode voltage for each trial was collected with an \n800-ms boxcar window with 200-ms offset. The voltage was converted \nto luminance using previously measured relationship between photodi-\node voltage and luminance and the resulting luminance versus voltage \ncurve was fit with the function L = B + A × Pγ where L is the measured \nluminance for pixel value P, and the median γ of the monitor was fit as \n1.73 (range 1.58–1.74). All stimuli were shown without linearizing the \nmonitor (that is, with monitor in normal gamma mode).\nDuring the stimulus presentation, display frame sequence informa-\ntion was encoded in a three-level signal, derived from the photodiode, \naccording to the binary encoding of the display frame (flip) number \nassigned in order. This signal underwent a sine convolution, allowing \nfor local peak detection to recover the binary signal together with its \nbehavioural time stamps. The encoded binary signal was reconstructed \nfor >96% of the flips. Each flip was time stamped by a stimulus clock \n(MasterClock PCIe-OSC-HSO-2 card). A linear fit was applied to the flip \ntime stamps in the behavioural and stimulus clocks, and the parameters \nof that fit were used to align stimulus display frames with scanner and \ncamera frames. The mean photodiode voltage of the sequence encod-\ning signal at pixel values 0 and 255 was used to estimate the luminance \nrange of the monitor during the stimulus, with minimum values of \napproximately 0.005–1 cd m−2 and maximum values of approximately \n8.0–11.5 cd m−2.\nScan and behavioural data preprocessing\nScan images were processed with the CAIMAN pipeline41, as described2, \nto produce the spiking activity neurons at the scan rate of 6.3–6.9 Hz. \nThe neuronal and behavioural (pupil and treadmill) activity were res-\nampled via linear interpolation to 29.967 Hz, to match the presentation \ntimes of the stimulus video frames.\nStimulus composition\nWe used dynamic libraries of natural videos42 and directional pink noise \n(Monet) as described2, and the static natural image library as described \nin Walker et al.16.\nDynamic Gabor filters were generated as described 43. We used a \nspatial envelope that had a s.d. of approximately 16.4° in the centre of \nthe monitor. A 10-s trial consisted of 10 Gabor filters (each lasting 1 s) \nwith randomly sampled spatial positions, directions of motion, phases, \nspatial and temporal frequencies.\nRandom dot kinematograms were generated as described44. The \nradius of the dots was approximately 2.6° in the centre of the monitor. \nEach 10-s trial contained 5 patterns of optical flow, each lasting 2 s. \nThe patterns were randomly sampled in terms of type of optical flow \n(translation: up/down/right/left; radial: in/out; rotation: clockwise/\nanticlockwise) and coherence of random dots (50%, 100%).\nThe stimulus compositions of the MICrONS recording sessions is \ndescribed in the accompanying Article2. For all other recording ses-\nsion, the stimulus compositions are listed in Extended Data Table 1.\nNeural network architecture\nOur model of the visual cortex is an ANN composed of four modules: \nperspective, behaviour, core and readout. These modules are described \nin the following sections.\nPerspective module\nThe perspective module uses ray tracing to infer the perspective or \nretinal activation of a mouse at discrete time points from two input \nvariables: stimulus (video frame) and eye position (estimated centre of \npupil, extracted from the eye tracking camera). T o perform ray tracing, \nwe modelled the following physical entities: (1) topography and light \nray trajectories of the retina; (2) rotation of the retina; (3) position of \nthe monitor relative to the retina; and (4) intersection of the light rays \nof the retina and the monitor.\n(1) We modelled the retina as a uniform 2D grid mapped onto a \n3D sphere via an azimuthal equidistant projection (Extended Data \nFig. 1a). Let θ and ϕ denote the polar coordinates (radial and angular, \nrespectively) of the 2D grid. The following mapping produces a 3D light \nray for point (θ, ϕ) of the modelled retina:\n↦\n\n\n\n\n\n\n\n\n\n\n\n\n\nθϕ θ\nϕ\nθϕ\nθϕ\nθ\n(, ):\nsinc os\nsins in\ncos\n.l\n(2) We used pupil tracking data to infer the rotation of the occular \nglobe and the retina. At each time point t, a multilayer perceptron (MLP; \nwith 3 layers and 8 hidden units per layer) is used to map the pupil posi-\ntion onto the 3 ocular angles of rotation:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\np\np\nθ\nθ\nθ\nMLP: ,\nxt\nyt\nxt\nyt\nzt\n↦\n̂\n̂\n̂\nwhere the pxt and pyt are the x and y coordinates of the pupil centre in \nthe frame of the tracking camera at time t, and ̂̂θθ ,xt yt  and ̂θzt are the \nestimated angles of rotation of about the x (adduction–abduction), y \n(elevation–depression) and z (intorsion–extorsion) axes of the occular \nglobe at time t.\nLet RRRR,, ∈xy z\n3× 3 denote rotation matrices about x, y and z axes. \nEach light ray of the retina l(θ, ϕ) is rotated by the occular angles of \nrotation:\n̂̂ ̂̂θϕ tθ θθ θϕ(, ,) =( )()( )( ,) ,zz ty yt xx tlR RR l\nproducing ̂ θϕ t(, ,) ∈ 3Rl , the ray of light for point (θ, ϕ) of the retina at \ntime t, which accounts for the gaze of the mouse and the rotation of \nthe occular globe.\n(3) We modelled the monitor as a plane with six degrees of freedom: \nthree for translation and three for rotation. Translation of the monitor \nplane relative to the retina is parameterized by R∈0\n3m . Rotation is \nparameterized by angles θθ θ,,xy z:\nθθ θ[] =( )( )( ),xy z zz yy xxmmm RR R\nwhere m mm R,, ∈xy z\n3  are the horizontal, vertical, and normal unit \nvectors of the monitor.\n(4) We computed the line-plane intersection between the monitor \nplane and ̂ θϕ t(, ,)l , the gaze-corrected trajectory of light for point ij \nof the retina at time t:\nm mm\nlm\nl̂\n̂θϕ t\nθϕ t\nθϕ t(, ,) = ⋅\n(, ,) ⋅\n(, ,) ,z\nz\n0\nwhere m(θ, ϕ, t) is the point of intersection between the monitor plane \nand the light ray ̂ θϕ t(, ,)l . This is projected onto the monitor’s hori-\nzontal and vertical unit vectors:\nmθ ϕt θϕ t\nmθ ϕt θϕ t\n(, ,) =( (, ,) −) ⋅,\n(, ,) =( (, ,) −) ⋅,\nx\nx\ny\ny\n0\n0\nmm m\nmm m\nyielding mx(θ, ϕ, t) and my(θ, ϕ, t), the horizontal and vertical displace-\nments from the centre of the monitor/stimulus (Extended Data Fig. 1b). \nT o produce inferred activation of the retinal grid at (θ, ϕ, t), we per-\nformed bilinear interpolation of the stimulus at the four pixels sur -\nrounding the line-plane intersection at mx(θ, ϕ, t), my(θ, ϕ, t).\nModulation module\nThe modulation module is a small long short-term memory (LSTM) \nnetwork45 that transforms behavioural variables—that is, locomotion \nand pupil size—and previous states of the network, to produce dynamic \nrepresentations of the behavioural state and arousal of the mouse.\nArticle\nr\np\np\nLSTM :\n′\n,, ,,\nt\nt\nt\nt\nm\nt\nm\nt\nm\nt\nm\n−1 −1hc hc↦\n\n\n\n\n\n\n\n\n\n\nwhere r is the running or treadmill speed, p is the pupil diameter, p′ is \nthe instantaneous change in pupil diameter, and h c,∈m m 8R  are the \n‘hidden’ and ‘cell’ state vectors of the modulation LSTM network.\nThe hidden state vector hm is tiled across space to produce modula-\ntion feature maps t\nmH :\n∈→ ∈,t\nm C\nt\nmC HW××hH RR\nwhere C, H and W denote channel, height and width, respectively, of \nthe feature maps. These feature maps Ht\nm serve as the modulatory \ninputs into the recurrent portion of the core module at time t.\nCore module\nThe core module—comprised of feedforward and recurrent compo-\nnents—transforms the inputs from the perspective and modulation \nmodules to produce feature representations of vision modulated by \nbehaviour.\nFirst, the feedforward module transforms the visual input provided \nby the perspective module. For this we used DenseNet architecture46 \nwith three blocks. Each block contains two layers of 3D (spatiotem-\nporal) convolutions followed by a Gaussian error linear unit (GELU) \nnonlinearity 47 and dense connections between layers. After each \nblock, spatial pooling was performed to reduce the height and width \ndimensions of the feature maps. T o enforce causality, we shifted the \n3D convolutions along the temporal dimension, such that no inputs \nfrom future time points contributed to the output of the feedforward  \nmodule.\nNext, the recurrent module transforms the visual and behavioural \ninformation provided by the feedforward and modulation modules, \nrespectively, through a group of recurrent cells. We used a convolu-\ntional LSTM (Conv-LSTM)48 as the architecture for each recurrent cell. \nFor each cell c, the formulation of the Conv-LSTM is shown below:\n∑WW W\nσW W\nσW W\nσW W\nWW\n={ * +{ * +{ * ,\n=( { * +{ * +) ,\n=( { * +{ * +) ,\n=( { * +{ * +) ,\n=t anh({ * +{ * +) ,\n=+ ,\n=t anh( ),\nt\nc\nt\nf\nt\nm\nc t\nc\nt\nc\nt\nc\nt\nc\ni\nc\nt\nc\nt\nc\nt\nc\no\nc\nt\nc\nt\nc\nt\nc\nf\nc\nt\nc\nt\nc\nt\nc\ng\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\n11 ′ 1− 1\n′\n33 −1\n33 −1\n33 −1\n33 −1\n−1\nXH HH\nIX Hb\nOX Hb\nFX Hb\nGX Hb\nCF CI G\nHO C\n⊙⊙\n⊙\nwhere σ denotes the sigmoid function, ⊙ denotes the Hadarmard prod-\nuct, and W *k  denotes a 2D spatial convolution with a k × k kernel. t\nfH , \nHt\nm are the feedforward and modulation outputs, respectively, at time \nt, and t\nc\n−1\n′H  is the hidden state of an external cell c′ at time t − 1. For cell \nc at time t, t\ncX , Ct\nc and Ht\nc are the input, cell and hidden states, respec-\ntively, and t\ncI , Ot\nc, t\ncF  and Gt\nc are the input, output, forget and cell gates.\nT o produce the output of the core network, the hidden feature maps \nof the recurrent cells are concatenated along the channel dimension:\n=C oncatenate(, , ... ).tt\nc\nt\nc=1 =2HH H\nGiven the recent popularity and success of transformer networks49, \nwe explored whether adding the attention mechanism to our network \nwould improve performance. We modified the Conv-LSTM architecture \nto incorporate the attention mechanism from the convolutional vision \ntransformer (CvT)50. This recurrent transformer architecture, which \nwe name CvT-LSTM, is described as follows:\n∑WW W\nWW\nW\nW\nW\nσW W\nσW W\nσW W\nWW\n={ * +{ * +{ * ,\n={ * +{ * ,\n={ * ,\n={ * ,\n={ * ,\n=A ttention( ,, ),\n=( { * +{ * +) ,\n=( { * +{ * +) ,\n=( { * +{ * +) ,\n=t anh({ * +{ * +) ,\n=+ ,\n=t anh( ),\nt\nc\nt\nf\nt\nm\nc t\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\ni\nc\nt\nc\nt\nc\nt\nc\no\nc\nt\nc\nt\nc\nt\nc\nf\nc\nt\nc\nt\nc\nt\nc\ng\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\nt\nc\n11 ′ 1− 1\n′\n33 −1\n1\n1\n1\n11\n11\n11\n11\n−1⊙⊙\n⊙\nXH HH\nZX H\nQZ\nKZ\nVZ\nAQ KV\nIA Zb\nOA Zb\nFA Zb\nGA Zb\nCF CI G\nHO C\nwhere attention is performed over query t\ncQ , key Kt\nc and value t\ncV  spatial \ntokens, which are produced by convolutions of the feature map t\ncZ . The \ntechnique of using convolutions with the attention mechanism was \nintroduced with CvT50, and here we extend it by incorporating it into \na recurrent LSTM architecture (CvT-LSTM).\nWe compare the performance of Conv-LSTM versus CvT-LSTM \nrecurrent architecture in Extended Data Fig. 5. When trained on the \nfull amount of data, Conv-LSTM performs very similarly to CvT-LSTM. \nHowever, Conv-LSTM outperforms CvT-LSTM when trained on \nrestricted data (for example, 4 min of natural videos). This was con-\nsistent for all stimulus domains that were used to test model accu -\nracy—natural videos (Extended Data Fig. 5a), natural images (Extended \nData Fig. 5b), drifting Gabor filters (Extended Data Fig. 5c), flash -\ning Gaussian dots (Extended Data Fig. 5d), directional pink noise \n(Extended Data Fig. 5e) and random dot kinematograms (Extended \nData Fig. 5e). The performance difference under data constraints \nmay be due a better inductive bias of the Conv-LSTM. Alternatively, it \ncould be due to a lack of optimization of the CvT-LSTM hyperparam-\neters, and a more extensive hyperparameter search may yield better  \nperformance.\nReadout module\nThe readout module maps the core’s outputs onto the activity of indi-\nvidual neurons. For each neuron, the readout parameters are factorized \ninto two components: spatial position and feature weights. For a neu-\nron n, let R∈n 2p  denote the spatial position (x, y), and let Rw ∈nC  denote \nthe feature weights for that neuron, with C = 512 being the number \nchannels in the core module’s output. T o produce the response of that \nneuron n at time t, the following readout operation is performed:\nhH p\nhwrb\n=I nterpolate(, ),\n=e xp(⋅ +) ,\nt\nn\nt\nn\nt\nn\nt\nn n n\nwhere ∈t\nn Ch R  is a feature vector that is produced via bilinear interpo-\nlation of the core network’s output RH ∈t\nCH W××  (channels, height, \nwidth), interpolated at the spatial position pn. The feature vector t\nn\nh  is \nthen combined with the feature weights wn and a scalar bias bn to pro-\nduce the response rt\nn of neuron n at time t.\nDue to the bilinear interpolation at a single position, each neuron \nonly reads out from the core’s output feature maps within a 2 × 2 spatial \nwindow. While this adheres to the functional property of spatial selec-\ntivity exhibited by neurons in the visual cortex, the narrow window \nlimits exploration of the full spatial extent of features during model \ntraining. T o facilitate the spatial exploration of the core’s feature maps \nduring training, for each neuron n, we sampled the readout position \nfrom a 2D Gaussian distribution: ~( ,)nn nNp µΣ . The parameters of the \ndistribution μn, Σn (mean, covariance) were learned via the reparam-\neterization trick51. We observed empirically that the covariance Σ n \nnaturally decreased to small values by the end of training, meaning \nthat the readout converged on a specific spatial position. After training, \nand for all testing purposes, we used the mean of the learned distribu-\ntion μn as the single readout position pn for neuron n.\nIn Extended Data Fig. 6, we examine the stability of the learned read-\nout feature weights across different recording sessions. Due to the \noverlap between imaging planes, some neurons were recorded multiple \ntimes within the MICrONS volume. We found that the readout feature \nweights of the same neuron were more similar than feature weights \nof different neurons that were close in proximity, indicating that the \nreadout feature weights of our model offer an identifying barcode of \nneuronal function that is stable across experiments.\nModel training\nThe perspective, behaviour, core, and readout modules were assembled \ntogether to form a model that was trained to match the recorded \ndynamic neuronal responses from the training dataset. Let yt\ni be the \nrecorded in vivo response, and let rt\ni be the predicted in silico response \nof neuron i at time t. The ANN was trained to minimize the Poisson \nnegative-log likelihood loss, ∑ ry r−l og()it t\ni\nt\ni\nt\ni , via stochastic gradient \ndescent with Nesterov momentum52. The ANN was trained for 200 \nepochs with a learning rate schedule that consisted of a linear warm \nup in the first 10 epochs, cosine decay53 for 90 epochs, followed by a \nwarm restart and cosine decay for the remaining 100 epochs. Each \nepoch consisted of 512 training iterations/gradient descent steps. We \nused a batch size of 5, and each sample of the batch consisted of 70 \nframes (2.33 s) of stimulus, neuronal and behavioural data.\nModel hyperparameters\nWe used a grid search to identify architecture and training hyperparam-\neters. Model performances for different hyperparameters were evalu-\nated using a preliminary set of mice. After optimal hyperparameters \nwere identified, we used the same hyperparameters to train models \non a separate set of mice, from which the figures and results were pro-\nduced. There was no overlap in the mice and experiments used for \nhyperparameter search and the mice and experiments used for the final \nmodels, results, and figures. This was done to prevent overfitting and \nto ensure that model performance did not depend on hyperparameters \nthat were fit specifically for certain mice.\nModel testing\nWe generated model predictions of responses to stimuli that were \nincluded in the experimental recordings but excluded from model \ntraining. T o evaluate the accuracy of model predictions, for each neu-\nron we computed the correlation between the mean in silico and in vivo \nresponses, averaged over stimulus repeats. The average in vivo response \naims to estimate the true expected response of the neuron. However, \nwhen the in vivo response is highly variable and there are a limited \nnumber of repeats, this estimate becomes noisy. T o account for this, \nwe normalized the correlation by an upper bound proposed by Schoppe \net al.27. Using ⋅  to denote average over trials or stimulus repeats, the \nnormalized correlation CCnorm is defined as follows:\nry\nry\nNy y\nNy\nCC = CC\nCC ,\nCC = Cov( ,)\nVar( )Var()\n,\nCC = Var( )− Var( )\n(− 1)Var( ) ,\nnorm\nabs\nmax\nabs\nmax\nwhere r is the in silico response, y is the in vivo response, and N is the \nnumber of trials. CCabs is the Pearson correlation coefficient between \nthe average in silico and in vivo responses. CCmax is the upper bound of \nachievable performance given the the in vivo variability of the neuron \nand the number of trials.\nParametric tuning\nT o estimate parametric tuning, we presented parametric stimuli to \nthe mice and the models. Specifically, we used directional pink noise \nparameterized by direction/orientation and flashing Gaussian blobs \nparameterized by spatial location. Orientation, direction and spatial \ntuning were computed from the recorded responses from the mice and \nthe predicted responses from the models. This resulted in analogous \nin vivo and in silico estimates of parametric tuning for each neuron. \nThe methods for measuring the tuning to orientation, direction, and \nspatial location are explained in the following sections.\nOrientation and direction tuning\nWe presented 16 angles of directional pink noise, uniformly distributed \nbetween [0, 2π). Let rθ be the mean response of a neuron to the angle \nθ, averaged over repeated presentations of the angle. The OSI and DSI \nwere computed as\n∑\n∑\n∑\n∑\nre\nr\nre\nr\nOSI= ,\nDSI= ,\nθ θ\niθ\nθ θ\nθ θ\niθ\nθ θ\n2∣∣\n∣∣\nthat is, the normalized magnitude of the first and second Fourier com-\nponents.\nT o determine the parameters for orientation and direction tuning, \nwe used the following parametric model:\nfθ μκ αβ γα eβ eγ(, ,,, )= ++ ,κθ μκ θμ πcos( −) cos( −+ )\nwhich is a mixture of two von Mises functions with amplitudes α and \nβ, preferred directions μ and μ + π, and dispersion κ, plus a baseline \noffset of γ. The preferred orientation is the angle that is orthogonal to \nμ between [0, π], that is, μ(+ π/2) modπ . T o estimate the parameters \nμ, κα, β, γ that best fit the neuronal response, we performed least-\nsquares optimization, minimizing ∑ fθ μκ αβ γr(( ,, ,) −)θ θ\n2.\nParameters were estimated via least square optimization for both \nthe in vivo and in silico responses. Let ̂μ and μ be the angles of preferred \ndirections estimated from in vivo and in silico responses, respectively. \nThe angular distances between the in vivo and in silico estimates of \npreferred direction (Fig. 4g) and orientation (Fig. 4e) were computed \nas follows:\n̂\n̂\nμμ\nμμ\nΔDirection =a rccos(cos( −) ),\nΔOrientation =a rccos(cos(2− 2 ))/2.\nSpatial tuning\nT o measure spatial tuning, we presented ‘on’ and ‘off’ (white and black), \nflashing (300 ms) Gaussian dots. The dots were isotropically shaped, \nwith a s.d. of approximately 8 visual degrees in the centre of the monitor. \nThe position of each dot was randomly sampled from a 17 × 29 grid tiling \nthe height and width monitor. We observed a stronger neuronal response \nfor ‘off’ compared to ‘on’ , and therefore we used only the ‘off’ Gaussian \ndots to perform spatial tuning from the in vivo and in silico responses.\nT o measure spatial tuning, we first computed the STA of the stimulus. \nLet x ∈ 2R  denote the spatial location (height and width) in pixels. The \nvalue of the STA at location x was computed as follows:\n∣∣∑\n∑\ns\nss r\nr=\n−\n,t tt\nt t\n0\nx\nx\nwhere rt is the response of the neuron, sxt is the value of the stimulus at \nlocation x and time t, and s0 is the blank or grey value of the monitor.\nArticle\nT o measure the spatial selectivity of a neuron, we computed the \ncovariance matrix or dispersion of the STA. Again using x ∈ 2R  denote \nthe spatial location (height and width) in pixels:\nxx\nxx xx\nx\nx\nx\nx\nx\nx\n∑\n∑\n∑\nzs\nsz\nsz\n=,\n=/ ,\nΣ= (− )( −) /.STA\nT\nThe SSI, or strength of spatial tuning, was defined as the negative-log \ndeterminant of the covariance matrix:\nSSI =− logΣ .STA\nT o determine the parameters of spatial tuning, we used least squares \nto fit the STA to the following parametric model:\nfα γα γ(, Σ, ,) =e xp − 1\n2 (− )Σ (− )+ ,T− 1μμ μxx x\n \n\nwhich is a 2D Gaussian component with amplitude α, mean μ, and covari-\nance Σ, plus a baseline offset of γ.\nFrom the in vivo and in silico responses, we estimated two sets of \nspatial tuning parameters. Let ̂μ and μ  be the means (preferred spatial \nlocations) estimated from in vivo and in silico responses, respectively. \nT o measure the difference between the preferred locations (Fig. 4i), \nwe computed the Euclidean distance:\nμμΔLocation= −.̂\nAnatomical predictions from functional weights\nT o predict brain areas from readout feature weights, we used all func-\ntional units in the MICrONS data from 13 scans that had readout feature \nweights in the model. We trained a classifier to predict brain areas from \nfeature weights using logistic regression with nested cross validation. \nFor each of the 10 folds, 90% of the data was used to train the model \nwith another round of 10 fold cross validation to select the best L2 regu-\nlarization weight. The best-performing model was used to test on the \nheld-out 10% of data. Finally, all of the predictions were concatenated \nand used to test the performance of the classifier (balanced accuracy) \nand generate the confusion matrix. The confusion matrix was normal-\nized such that all rows sum to 1, thus the diagonal values represent the \nrecall of each class.\nT o predict cell types, the same functional data source was used as in \nthe brain area predictions. Cell types were obtained from CAVEclient \ninitialized with ‘minnie65_public’ and table ‘aibs_metamodel_mtypes_\nv661_v2’ . T o associate a neuron’s functional data with its cell type, we \nmerged the cell types to a match table made by combining the manual \nand fiducial-based automatic coregistration described in MICrONS \nConsortium et al.2. Finally, because each neuron could be scanned \nmore than once, and thus could have more than one functional read-\nout weight, we subset the data such that each neuron only had one \nreadout weight according to its highest cc_max. Following this proce-\ndure, n = 16,561 unique electron microscopy neurons remained. Out of \nthe 20 cell classes, all excitatory neuron classes in L2–5 were chosen \n(except L5NP, which had comparably fewer coregistered cells), leaving \n11 classes: L2a, L2b, L2c, L3a, L3b, L4a, L4b, L4c, L5a, L5b and L5ET. T o \ntrain the classifier using readout weights to predict cell types, logistic \nregression was used with the same nested cross validation procedure \nand performance metric as described in the brain area predictions.\nFor testing whether readout weights contributed to cell-type pre-\ndictions beyond imaging depth, the 2P depth of each functional unit \nwas obtained from a 2P structural stack (stack session 9, stack idx 19) \nwherein all imaging planes were registered2. This provided a common \nreference frame for all functional units. The two logistic regression \nmodels (depth versus depth + readout weights) were trained with all \nof the data, and the predicted probabilities and coefficients from the \nmodels were used to run the likelihood ratio test, where a P value less \nthan 0.05 was chosen as the threshold for statistical significance.\nReporting summary\nFurther information on research design is available in the Nature Port-\nfolio Reporting Summary linked to this article.\nData availability\nAll MICrONS data are available at https://bossdb.org/project/microns- \nminnie. Further details are available at https://www.microns-explorer.\norg/cortical-mm3.\nCode availability\nThe source code and foundation model weights are available at https://\ngithub.com/cajal/fnn. The model training and analysis pipeline can be \naccessed at https://github.com/cajal/foundation. The experimental \nrecording and calcium imaging pipeline can be accessed at https://\ngithub.com/cajal/pipeline.\n \n38. Froudarakis, E. et al. Population code in mouse V1 facilitates readout of natural scenes \nthrough increased sparseness. Nat. Neurosci. 17, 851–857 (2014).\n39. Sofroniew, N. J., Flickinger, D., King, J. & Svoboda, K. A large field of view two-photon \nmesoscope with subcellular resolution for in vivo imaging. eLife 5, e14472 (2016).\n40. Mathis, A. et al. DeepLabCut: markerless pose estimation of user-defined body parts with \ndeep learning. Nat. Neurosci. 21, 1281–1289 (2018).\n41. Giovannucci, A. et al. CaImAn: an open source tool for scalable calcium imaging data \nanalysis. eLife 8, e38173 (2019).\n42. Karpathy, A. et al. Large-scale video classification with convolutional neural networks.  \nIn 2014 IEEE Conference on Computer Vision and Pattern Recognition 1725–1732 (IEEE, \n2014).\n43. Petkov, N. & Subramanian, E. Motion detection, noise reduction, texture suppression  \nand contour enhancement by spatiotemporal gabor filters with surround inhibition.  \nBiol. Cybernet. 97, 423–439 (2007).\n44. Morrone, M. C. et al. A cortical area that responds specifically to optic flow, revealed by \nfMRI. Nat. Neurosci. 3, 1322–1328 (2000).\n45. Hochreiter, S. & Schmidhuber, J. Long short-term memory. Neural Comput. 9, 1735–1780 \n(1997).\n46. Huang, G., Liu, Z., van der Maaten, L. & Weinberger, K. Q. Densely connected convolutional \nnetworks. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) \n2261–2269 (IEEE, 2017).\n47. Hendrycks, D. & Gimpel, K. Gaussian error linear units (GELUs). Preprint at https://doi.org/ \n10.48550/arXiv.1606.08415 (2020).\n48. Shi, X. et al. Convolutional LSTM network: a machine learning approach for precipitation \nnowcasting. In Advances in Neural Information Processing Systems vol. 28 (eds Cortes, C. \net al.) (Curran Associates, 2015).\n49. Vaswani, A. et al. Attention is all you need. In Advances in Neural Information Processing \nSystems (eds. Guyon, I. et al.) vol. 30 (Curran Associates, 2017).\n50. Wu, H. et al. CvT: Introducing convolutions to vision transformers. In 2021 IEEE/CVF \nInternational Conference on Computer Vision (ICCV) 22–31 (IEEE, 2021).\n51. Kingma, D. P. & Welling, M. Auto-encoding variational Bayes. Preprint at https://doi.org/ \n10.48550/arXiv.1312.6114 (2013).\n52. Sutskever, I., Martens, J., Dahl, G. & Hinton, G. On the importance of initialization and \nmomentum in deep learning. Proc. Mach. Learn. Res. 28, 1139–1147 (2013).\n53. Loshchilov, I. & Hutter, F. SGDR: stochastic gradient descent with warm restarts. Preprint \nat https://doi.org/10.48550/arXiv.1608.03983 (2016).\nAcknowledgements The authors thank D. Markowitz, the IARPA MICrONS programme Manager, \nfor his support during all three phases of the MICrONS programme; IARPA programme \nmanagers J. Vogelstein and D. Markowitz for co-developing the MICrONS programme;  \nJ. Wang, IARPA SETA for her assistance; and M. Bethge, M. Mathis, B. Richards, A. Zador and  \nJ. Zylberberg for many stimulating discussions regarding building foundation models for the \nbrain. The work was supported by the MICrONS Program of Intelligence Advanced Research \nProjects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract \nnumbers D16PC00003, D16PC00004 and D16PC0005. The US Government is authorized to \nreproduce and distribute reprints for Governmental purposes notwithstanding any copyright \nannotation thereon. X.P. and A.S.T. acknowledge support from NSF NeuroNex grant 1707400. \nA.S.T. also acknowledges support from the National Institute of Mental Health and National \nInstitute of Neurological Disorders And Stroke under award number U19MH114830 and \nNational Eye Institute award numbers R01 EY026927 and Core Grant for Vision Research \nT32-EY-002520-37. Disclaimer: the views and conclusions contained herein are those of the \nauthors and should not be interpreted as necessarily representing the official policies or \nendorsements, either expressed or implied, of IARPA, DoI/IBC, or the US Government. A.S.E. \nreceived funding from the European Research Council (ERC) under the European Union’s \nHorizon Europe research and innovation programme (grant agreement 101041669) as well  \nas the Deutsche Forschungsgemeinschaft (DFG, German Research Foundation), project ID \n432680300 (S.F.B. 1456, project B05).\nAuthor contributions We adopted the following contribution categories from CRediT \n(Contributor Roles Taxonomy). Authors within each category are sorted in the same order as  \nin the author list. Conceptualization: E.Y.W. and A.S.T. Methodology: E.Y.W., F.H.S. and A.S.T. \nSoftware: E.Y.W. and F.H.S. Validation: E.Y.W., P.G.F., Zhuokon Ding, M.A.W., Zhiwei Ding, D.T. and \nJ.F. Formal analysis: E.Y.W., S. Papadopoulos and A.C. Investigation: E.Y.W., P.G.F., K.P. and T.M. \nResources: P.G.F., S. Patel, S. Papadopoulos, C.M.S.-M., R.C.R., F.C. and N.M.d.C. Data Curation: \nP.G.F., S. Papadopoulos. Writing, original draft: E.Y.W., P.G.F., S. Papadopoulos, F.H.S. and A.S.T. \nWriting, review and editing: E.Y.W., P.G.F., S. Papadopoulos, K.F., A.S.E., J.R., X.P., F.H.S. and \nA.S.T. Visualization: E.Y.W. and S. Papadopoulos. Supervision, project administration, and \nfunding acquisition: A.S.T.\nCompeting interests A.S.T. and J.R. are co-founders of DataJoint Inc., in which they have financial \ninterests. The other authors declare no competing interests.\nAdditional information\nSupplementary information The online version contains supplementary material available at \nhttps://doi.org/10.1038/s41586-025-08829-y.\nCorrespondence and requests for materials should be addressed to Andreas S. Tolias.\nPeer review information Nature thanks Blake Richards and the other, anonymous, reviewer(s) \nfor their contribution to the peer review of this work. Peer review reports are available.\nReprints and permissions information is available at http://www.nature.com/reprints.\nArticle\nAbduction (Temporal)\n Adduction (Nasal)\nElevation (Superior)Depression (Inferior)\na b\nc\nΦ θ\nm \nx\nm \ny\nExtended Data Fig. 1 | ANN perspective. Schematic of the modeled \nperspective the animal. a, The retina is modeled as points on a sphere receiving \nlight rays that trace through the origin. An example light ray with polar angle θ \nand azimuthal angle ϕ is shown in red. b , The light ray is traced to a point mx,  \nmy on the monitor. Bilinear interpolation of the four pixels on the monitor \nsurrounding mx, my produces the activation of a point θ , ϕ on the modeled \nretina. c, 9 examples of the modeled perspective from the left eye of an animal, \nwith 3 horizontal rotations of the optical globe (abduction/adduction) × 3 \nvertical rotations (elevation/depression). The concentric circles indicate visual \nangles in degrees. (See Methods for details on the perspective network).\nExtended Data Fig. 2 | ANN modulation. Visualization of the modulation \nnetwork’s output, projected onto 2 dimensions via UMAP . a , b show the same \ndata from an example recording session and modulation network. Each point \non the plot indicates a point in time from the recording session. The colors \nindicate measurements of pupil size ( a) and treadmill speed ( b) at the \nrespective points in time. (See Methods for details on the modulation \nnetwork).\nArticle\nExtended Data Fig. 3 | Neural network lesion studies. To determine the  \neffect that various components of the model have on predictive accuracy, we \nperformed lesion studies, where we altered individual components of model \nand evaluated the effect that the alteration had on model performance ( CCabs). \nThe left 4 columns (a-d, f-i, k-n, p-s) are scatterplots of reference vs lesioned \nmodel performance, with each column corresponding to different mouse  \nand each point corresponding to a neuron. The right-most column ( e, j, o, t) \ndisplays density histograms of the performance difference between the \nreference and the lesioned models, plotted separately for each mouse, as well \nas the t-statistic and p-values of paired two-sided t-tests. The first row ( a-e) \nshows the effect of the perspective module on model performance, the second \nrow (f-j) shows the effect of the modulation module, the third row ( k-o) shows \nthe effect of the convolution type – 2D vs 3D – of the feedforward module, and \nthe fourth row (p-t) shows the effect of the loss function – Poisson negative log \nlikelihood (Poission NLL) vs mean square error (MSE).\nExtended Data Fig. 4 | ANN performance: Individual vs. Foundation. \nPredictive accuracy (median C Cnorm across neurons) of foundation models  \n(with the foundation core) vs. individual models (with cores trained on \nindividual recording sessions). For the 4 mice in the 4 left columns, 1 recording \nsession was performed, and that data was partitioned into 7 training/validation \nsplits, which were used to train separate individual/foundation models.  \nThe predictive accuracy of those models (diamonds) is reported for 6 testing \nstimulus domains (rows). For the MICrONS mouse, 14 recording sessions were \nperformed, for each recording session, a model was trained using nearly all \n(99%) of the data available for training/validation. The MICrONS models were \nonly tested on the natural movies, due to the lack of the other stimuli in the \nrecording sessions. All models were trained only using natural movies.\nArticle\nExtended Data Fig. 5 | Recurrent architecture: Conv-Lstm vs. CvT-Lstm.   \nWe evaluated the performance of two different types of recurrent architectures \nfor the core module: Conv-Lstm (blue) and CvT-Lstm (tan). For each architecture, \na core was trained on 8 mice and then transferred to 4 new mice. For each of the \nnew mice, 7 models were trained using varying amounts of natural movies, \nranging from 4 to 76 minutes. The predictive accuracy ( CCnorm) of these models \nwas evaluated on 6 different stimulus domains: natural movies ( a), natural \nimages (b), drifting gabor filter ( c), flashing Gaussian dots ( d), directional pink \nnoise (e), random dot kinematograms ( f). Blue diamonds indicate models with \nthe Conv-Lstm core, and tan diamonds indicate models with the CvT-Lstm core. \nFor each architecture, models of the same mouse are connected by lines.\nExtended Data Fig. 6 | Pairwise similarities of readout feature weights of \nneurons from the MICrONS volume. Here we examine the similarities of \nreadout weights of same or different neurons, from same or different scans \n(recording sessions). In panels a–c, the similarities of readout weights are \nplotted for the following groups: same neuron  from different scan  (y-axis of a), \nsame neuron from same scan (y-axis of b), different neuron  from different scan  \n(x-axis of a, x-axis of c), different neuron  from same scan (x-axis of b and y-axis  \nof c). The similarity between readout weights was measured inversely via \nangular distance πxxy yx xy y∠ := arccos((⋅ )/( ))/ , where x, y is a pair of readout \nweights. A similar pair of readout weights will exhibit a small ∠ , and vice versa. \nThe scatterplots a –c are colored by the CCmax, which is an inverse measure of \nneuronal noise, i.e., the estimated maximum correlation coefficient that a \nmodel could achieve at predicting the mean response the neuron (see Methods \nfor details). For each neuron N, the ‘different’ neuron N ’ was restricted to be  \n≤100 μm apart from each other in terms of soma distance, and the distribution \nof the number of ‘different’ neurons is shown in d (from different scans) and  \ne (from the same scan). f and g (corresponding to d and e, respectively) show the \nfraction of the nearby neurons N ’ that are more similar to N in terms of readout \nweights than N is to itself across different scans. f, For 919 out of the 1013 \nneurons N, less than 0.05 of nearby neurons N’ from different scans had more \nsimilar readout weights. g , For 840 out of the 1013 neurons N, less than 0.05 of \nnearby neurons N’ from the same scan had more similar readout weights.\nArticle\nExtended Data Table 1 | Table listing the experimental recordings, collected for either foundation core training (Foundation \nCohort = Yes) or validation (Foundation Cohort = No)\nThe animal ID, number of neurons, and areas of the visual cortex are listed for each experiment. The ‘Training Data’ and ‘Testing Data’ columns list the Minutes x Repeats of each type of stimulus, \ndesignated for either model training or testing.\n1 nature portfolio  |  reporting summaryApril 2023\nCorresponding author(s): Andreas Tolias\nLast updated by author(s): Jan 15, 2025\nReporting Summary\nNature Portfolio wishes to improve the reproducibility of the work that we publish. This form provides structure for consistency and transparency \nin reporting. For further information on Nature Portfolio policies, see our Editorial Policies and the Editorial Policy Checklist.\nStatistics\nFor all statistical analyses, confirm that the following items are present in the figure legend, table legend, main text, or Methods section.\nn/a Confirmed\nThe exact sample size (n) for each experimental group/condition, given as a discrete number and unit of measurement\nA statement on whether measurements were taken from distinct samples or whether the same sample was measured repeatedly\nThe statistical test(s) used AND whether they are one- or two-sided \nOnly common tests should be described solely by name; describe more complex techniques in the Methods section.\nA description of all covariates tested\nA description of any assumptions or corrections, such as tests of normality and adjustment for multiple comparisons\nA full description of the statistical parameters including central tendency (e.g. means) or other basic estimates (e.g. regression coefficient) \nAND variation (e.g. standard deviation) or associated estimates of uncertainty (e.g. confidence intervals)\nFor null hypothesis testing, the test statistic (e.g. F, t, r) with confidence intervals, effect sizes, degrees of freedom and P value noted \nGive P values as exact values whenever suitable.\nFor Bayesian analysis, information on the choice of priors and Markov chain Monte Carlo settings\nFor hierarchical and complex designs, identification of the appropriate level for tests and full reporting of outcomes\nEstimates of effect sizes (e.g. Cohen's d, Pearson's r), indicating how they were calculated\nOur web collection on statistics for biologists contains articles on many of the points above.\nSoftware and code\nPolicy information about availability of computer code\nData collection For image acquisition, we used ScanImage 2017b. Stimuli were presented using PsychToolBox 3. The data collection process was automated \nwith Labview.\nData analysis We used DeepLabCut (2.0.5) for automatic tracking of the pupil. We used CaImAn (1.0) for automatic segmentation and deconvolution of \ncalcium imaging data. Our custom built analysis pipeline (https://github.com/cajal/pipeline, https://github.com/cajal/foundation) also used \ngeneral tools like Numpy (1.23.5), pandas (1.5.3), SciPy (1.10.1), statsmodels (0.13.5), scikit-learn (1.2.1),  PyTorch (1.12.1), Matplotlib (3.7.0), \nseaborn (0.12.2), HoloViews (1.15.4), Ipyvolume (0.5.2), Jupyter (ipykernel: 6.21.2), MySQL (5.7.37), Docker (23.0.1), and Kubernetes \n(1.22.11). DataJoint (0.12.9) were used for storing and managing data.\nFor manuscripts utilizing custom algorithms or software that are central to the research but not yet described in published literature, software must be made available to editors and \nreviewers. We strongly encourage code deposition in a community repository (e.g. GitHub). See the Nature Portfolio guidelines for submitting code & software for further information.\n2 nature portfolio  |  reporting summaryApril 2023\nData\nPolicy information about availability of data\nAll manuscripts must include a data availability statement. This statement should provide the following information, where applicable: \n- Accession codes, unique identifiers, or web links for publicly available datasets \n- A description of any restrictions on data availability \n- For clinical datasets or third party data, please ensure that the statement adheres to our policy \n \nThe MICrONS functional and structural data are available on BossDB (https://bossdb.org/project/microns-minnie, please also see https://www.microns-\nexplorer.org/cortical-mm3 for details). The MICrONS foundation model is available on GitHub (https://github.com/cajal/fnn).\nResearch involving human participants, their data, or biological material\nPolicy information about studies with human participants or human data. See also policy information about sex, gender (identity/presentation), \nand sexual orientation and race, ethnicity and racism.\nReporting on sex and gender Use the terms sex (biological attribute) and gender (shaped by social and cultural circumstances) carefully in order to avoid \nconfusing both terms. Indicate if findings apply to only one sex or gender; describe whether sex and gender were considered in \nstudy design; whether sex and/or gender was determined based on self-reporting or assigned and methods used.  \nProvide in the source data disaggregated sex and gender data, where this information has been collected, and if consent has \nbeen obtained for sharing of individual-level data; provide overall numbers in this Reporting Summary.  Please state if this \ninformation has not been collected.  \nReport sex- and gender-based analyses where performed, justify reasons for lack of sex- and gender-based analysis.\nReporting on race, ethnicity, or \nother socially relevant \ngroupings\nPlease specify the socially constructed or socially relevant categorization variable(s) used in your manuscript and explain why \nthey were used. Please note that such variables should not be used as proxies for other socially constructed/relevant variables \n(for example, race or ethnicity should not be used as a proxy for socioeconomic status).  \nProvide clear definitions of the relevant terms used, how they were provided (by the participants/respondents, the \nresearchers, or third parties), and the method(s) used to classify people into the different categories (e.g. self-report, census or\n \nadministrative data, social media data, etc.) \nPlease provide details about how you controlled for confounding variables in your analyses.\nPopulation characteristics Describe the covariate-relevant population characteristics of the human research participants (e.g. age, genotypic \ninformation, past and current diagnosis and treatment categories). If you filled out the behavioural & social sciences study \ndesign questions and have nothing to add here, write \"See above.\"\nRecruitment Describe how participants were recruited. Outline any potential self-selection bias or other biases that may be present and \nhow these are likely to impact results.\nEthics oversight Identify the organization(s) that approved the study protocol.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nField-specific reporting\nPlease select the one below that is the best fit for your research. If you are not sure, read the appropriate sections before making your selection.\nLife sciences Behavioural & social sciences  Ecological, evolutionary & environmental sciences\nFor a reference copy of the document with all sections, see nature.com/documents/nr-reporting-summary-flat.pdf\nLife sciences study design\nAll studies must disclose on these points even when the disclosure is negative.\nSample size No sample-size calculation was performed a priori. Sample sizes (number of connections tested) match or exceed previous studies of similar \ndesign.\nData exclusions Of the 14 released MICrONS scans, one scan was excluded a priori from the study due to experimental issues (responses to some stimuli were \nnot collected due to water running out from the objective). Duplicate detection was performed to identify neurons that were recorded more \nthan once in experiments. Besides that, no neurons were exlcuded from model training or analysis.\nReplication The approach of using a foundation model core to fit new models of mice was replicated across 4 mice for evaluating predictive accuracy \n(Figure 3) and 3 mice for evaluating parametric tuning accuracy (Figure 4).\nRandomization No randomization of animal subjects was performed as our experimental design did not stratify into animal groups.\nBlinding No blinding is performed during data collection since our study did not include predefined experimental groups for sample allocation. The \n3 nature portfolio  |  reporting summaryApril 2023\nBlinding analysis is performed unblinded; however, the same computational methods were applied to all control and sample groups.\nReporting for specific materials, systems and methods\nWe require information from authors about some types of materials, experimental systems and methods used in many studies. Here, indicate whether each material, \nsystem or method listed is relevant to your study. If you are not sure if a list item applies to your research, read the appropriate section before selecting a response. \nMaterials & experimental systems\nn/a Involved in the study\nAntibodies\nEukaryotic cell lines\nPalaeontology and archaeology\nAnimals and other organisms\nClinical data\nDual use research of concern\nPlants\nMethods\nn/a Involved in the study\nChIP-seq\nFlow cytometry\nMRI-based neuroimaging\nAnimals and other research organisms\nPolicy information about studies involving animals; ARRIVE guidelines recommended for reporting animal research, and Sex and Gender in \nResearch\nLaboratory animals For experiments excluding the MICrONS dataset in this manuscript: Three mice, Mus musculus, 78-86 days old at first experimental \nscan. Heterozygous for both Slc17a7-Cre (B6;129S-Slc17a7tm1.1(cre)Hze/J, Jackson Laboratory Strain # 023527) and Ai162 (B6.Cg-\nIgs7tm162.1(tetO-GCaMP6s,CAG-tTA2)Hze/J, Jackson Laboratory Strain # 031562). The MICrONS dataset was collected from a mouse \nof the same species and strain, 75 days old.\nWild animals Study did not involve wild animals.\nReporting on sex For new experiments in this manuscript: 6 Female, 8 Males. For MICrONS dataset, 1 Male. Animals were randomly recruited to the \nstudy with respect to sex. Analysis disaggregated for sex was not performed, due to low sample size and expected generalization of \nprinciples under study across genders.\nField-collected samples Study did not involve samples collected from the field.\nEthics oversight All procedures were approved by the Institutional Animal Care and Use Committee of Baylor College of Medicine.\nNote that full information on the approval of the study protocol must also be provided in the manuscript.\nNovel plant genotypes Describe the methods by which all novel plant genotypes were produced. This includes those generated by transgenic approaches, \ngene editing, chemical/radiation-based mutagenesis and hybridization. For transgenic lines, describe the transformation method, the \nnumber of independent lines analyzed and the generation upon which experiments were performed. For gene-edited lines, describe \nthe editor used, the endogenous sequence targeted for editing, the targeting guide RNA sequence (if applicable) and how the editor \nwas applied.\nSeed stocks Report on the source of all seed stocks or other plant material used. If applicable, state the seed stock centre and catalogue number. If \nplant specimens were collected from the field, describe the collection location, date and sampling procedures.\nAuthentication Describe any authentication procedures for each seed stock used or novel genotype generated. Describe any experiments used to \nassess the effect of a mutation and, where applicable, how potential secondary effects (e.g. second site T-DNA insertions, mosiacism, \noff-target gene editing) were examined.\nPlants"
}