{
  "title": "Transformer-based ensemble deep learning model for EEG-based emotion recognition",
  "url": "https://openalex.org/W4381167182",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5009222035",
      "name": "Xiaopeng Si",
      "affiliations": [
        null,
        "Tianjin Medical University",
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5050270789",
      "name": "Dong Huang",
      "affiliations": [
        null,
        "Tianjin Medical University",
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5019065777",
      "name": "Yulin Sun",
      "affiliations": [
        null,
        "Tianjin Medical University",
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5104131350",
      "name": "Shudi Huang",
      "affiliations": [
        "Tianjin Medical University",
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5101038931",
      "name": "He Huang",
      "affiliations": [
        "Tianjin Medical University",
        "Tianjin University"
      ]
    },
    {
      "id": "https://openalex.org/A5011769657",
      "name": "Dong Ming",
      "affiliations": [
        "Tianjin Medical University",
        "Tianjin University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2131150329",
    "https://openalex.org/W2976149155",
    "https://openalex.org/W3167195439",
    "https://openalex.org/W2161073241",
    "https://openalex.org/W2980520956",
    "https://openalex.org/W2124079640",
    "https://openalex.org/W2156503193",
    "https://openalex.org/W3119221111",
    "https://openalex.org/W2122098299",
    "https://openalex.org/W2002055708",
    "https://openalex.org/W2625929003",
    "https://openalex.org/W4239024596",
    "https://openalex.org/W2023908259",
    "https://openalex.org/W2016952944",
    "https://openalex.org/W2625897054",
    "https://openalex.org/W2108050717",
    "https://openalex.org/W2996304026",
    "https://openalex.org/W3172316648",
    "https://openalex.org/W4220866631",
    "https://openalex.org/W3128719470",
    "https://openalex.org/W2134050473",
    "https://openalex.org/W2170883741",
    "https://openalex.org/W2171782446",
    "https://openalex.org/W2128495200",
    "https://openalex.org/W1575429381",
    "https://openalex.org/W1947251450",
    "https://openalex.org/W2017021822",
    "https://openalex.org/W3014819045",
    "https://openalex.org/W3163969335",
    "https://openalex.org/W4200575130",
    "https://openalex.org/W3080571847",
    "https://openalex.org/W2913846632",
    "https://openalex.org/W3044186523",
    "https://openalex.org/W2618530766",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W2042492924",
    "https://openalex.org/W4205558134",
    "https://openalex.org/W4206072824",
    "https://openalex.org/W4200042752",
    "https://openalex.org/W3217034758",
    "https://openalex.org/W4311626862",
    "https://openalex.org/W2936897040",
    "https://openalex.org/W2559463885",
    "https://openalex.org/W2970602317",
    "https://openalex.org/W4385805174",
    "https://openalex.org/W2771693746",
    "https://openalex.org/W2936764885",
    "https://openalex.org/W2786687317",
    "https://openalex.org/W2942905092",
    "https://openalex.org/W4283719671",
    "https://openalex.org/W3102455230",
    "https://openalex.org/W2003352387",
    "https://openalex.org/W4390874170"
  ],
  "abstract": "Emotion recognition is one of the most important research directions in the field of brain–computer interface (BCI). However, to conduct electroencephalogram (EEG)-based emotion recognition, there exist difficulties regarding EEG signal processing; moreover, the performance of classification models in this regard is restricted. To counter these issues, the 2022 World Robot Contest successfully held an affective BCI competition, thus promoting the innovation of EEG-based emotion recognition. In this paper, we propose the Transformer-based ensemble (TBEM) deep learning model. TBEM comprises two models: a pure convolutional neural network (CNN) model and a cascaded CNN-Transformer hybrid model. The proposed model won the abovementioned affective BCI competition’s final championship in the 2022 World Robot Contest, demonstrating the effectiveness of the proposed TBEM deep learning model for EEG-based emotion recognition.",
  "full_text": "Brain Science Advances 2023, 9(3): 210–223 ISSN 2096-5958 \nhttps://doi.org/10.26599/BSA.2023.9050016  CN 10-1534/R \nRESEARCH ARTICLE  \n                                                                      \n  \ns c i o p e n . c o m / j o u r n a l / 2 0 9 6 - 5 9 5 8                                                                      j o u r n a l s . s a g e p u b . c o m / h o m e / B S A   \n \n \nTransformer-based ensemble deep learning model for EEG-based \nemotion recognition \n \nXiaopeng Si1,2,§ (), Dong Huang1,2,§, Yulin Sun1,2,§, Shudi Huang1,2, He Huang1,2, Dong Ming1,2 () \n \n1 Academy of Medical Engineering and Translational Medicine, Tianjin University, Tianjin 300072, China \n2 Tianjin Key Laboratory of Brain Science and Neural Engineering, Tianjin University, Tianjin 300072, China \n§ These authors contributed equally to this work. \n \nARTICLE INFO  ABSTRACT \nReceived: 29 March, 2023 \nRevised: 8 May, 2023 \nAccepted: 24 May, 2023 \n \n \n© The authors 2023. This article is published with \nopen access at journals.sagepub.com/home/BSA \n \n \n Creative Commons Non Commercial CC BY- \nNC: This article is distributed under the terms of the \nCreative Commons Attribution-NonCommercial 4.0 License \n(http://www.creativecommons.org/licenses/by-nc/4.0/) \nwhich permits non-commercial use, reproduction and \ndistribution of the work without further permission \nprovided the original work is  attributed as specified on \nthe SAGE and Open Access pages (https://us.sagepub.com/ \nen-us/nam/open-access-at-sage). \n  \nKEYWORDS \naffective brain–computer interface, electroen-\ncephalogram, Transformer, deep learning, ensemble \nlearning  \n \nEmotion recognition is one of the most important research directions\nin the field of brain–computer interface (BCI). However, to conduct \nelectroencephalogram (EEG)-base d emotion recognition, there \nexist difficulties regarding EEG signal processing; moreover, the\nperformance of classification models  in this regard is restricted. To\ncounter these issues, the 2022 World Robot Contest successfully\nheld an affective BCI competition, thus promoting the innovation \nof EEG-\nbased emotion recognition. In this paper, we propose the \nTransformer-based ensemble (TBEM) deep learning model. TBEM\ncomprises two models: a pure convolutional neural network (CNN)\nmodel and a cascaded CNN-Transformer hybrid model. The proposed \nmodel won the abovementioned affective BCI competition’s final\nchampionship in the 2022 World Robot Contest, demonstrating\nthe effectiveness of the proposed TBEM deep learning model for \nEEG-based emotion recognition. \n \n \n \n \n1 Introduction \nEmotion recognition, as an emerging research \nfield, has been attracting increasing attention \nfrom researchers of various fields in recent years. \nIt has immense potential for applications in the \ndiagnosis and treatment of mental illnesses [1] \nand is one of the most important research \ntopics in the field of brain–computer interaction \n(BCI) [2, 3]. Traditional emotion recognition \nuses human voice [4, 5], and facial expression \n[6–8] to identify human emotions; however, \nthese “signals” cannot detect human emotions \nefficiently, especially when subjects intentionally \ndisguise their voice and facial expressions. \nCurrently, physiological signals have received \nincreasing attention from researchers in the field \nof emotion recognition [9, 10]. Many research \n \nAddress correspondence to Xiaopeng Si, xiaopengsi@tju.edu.cn; Dong Ming, richardming@tju.edu.cn \nBrain Sci. Adv. \n                                                                      \n  \ns c i o p e n . c o m / j o u r n a l / 2 0 9 6 - 5 9 5 8                                                                      j o u r n a l s . s a g e p u b . c o m / h o m e / B S A   \n211\nteams have used physiological signals such as \nbreathing modes, body temperature, electromyo-\ngraphy (EMG), electroc ardiogram (ECG), and \nelectroencephalogram (EEG) to investigate \nemotion recognition. Among these, EEG is \nphysiological signal produced directly by the \nbody’s central nervous system or brain [11]. \nCertain brain regions hav e a high correlation \nwith emotions; for example, amygdala [12–14] is \nwidely believed to be related to human emotions; \nfurther, the prefrontal lobe [15, 16], which is the \nadvanced information processing and processing \nnode of the human brain, is also believed to be \nrelated to emotions. Therefore, EEG can reflect \nhuman emotions more truly and reliably as \ncompared to other physiological signals. \nIn recent years, EEG-based emotion recognition \nhas attracted increasing attention from domestic \nand international research teams; subsequently, \na new direction of BCI, namely, affective BCI \n(aBCI), has been developed [17]. EEG-based \naBCI systems mainly include preprocessing, \nfeature extraction, and classification; among \nthese, preprocessing can significantly improve \nthe signal-to-noise rati o of EEG. EEG, as an \nunsteady signal, is easily affected by noise such as    \nmotion artifact, respiration, electrocardiogram, and \nelectroophthalmogram [18]; therefore, appropriate \npreprocessing is essential. As a physiological \nsignal with high temporal resolution [19], EEG has \na relatively wide frequency range [20]. Therefore, \nextracting information from different frequency \nbands through filtering are an important part \nof preprocessing. Because nearby channels receive \ninformation from the same region of the brain, \nthey collect similar EEG signals and are affected \nby the setting of the reference electrodes, resulting \nin some redundancy of EEG signals. Therefore, \nit is necessary to remove redundancy through \nrepeated references, such as common mean \nreference (CAR), etc. In recent years, intensive \nstudies of EEG features used for emotion \nrecognition, including time domain features, \nfrequency domain features, time-frequency domain \nfeatures, nonlinear features, asymmetric features, \nbrain connectivity feat ures, etc. have been \nconducted [21, 22]. Time domain features include \nthe Hjorth parameter [23], high-order crossover \n(HOC) [24], etc. Time-frequency feature extraction \nmethods mainly include short-time Fourier \ntransform (STFT) [25, 26], wavelet analysis [27], \netc. The characteristics of the frequency domain \ninclude five classical frequency bands: power \nspectral density of δ (1–3 Hz), θ (4–7 Hz), α \n(8–13 Hz), β (14–30 Hz), and γ (31-50 Hz), etc. \nDifferential entropy (DE) [28] is a well-known \ncharacteristic describing nonlinear dynamical \nsystems. Asymmetric features generally refer to \nspecific phenomena found in cognitive studies \n[29, 30]. For example, cognitive studies based \non brain imaging technologies have found the \nlateralization of differ ent regions in the brain \nduring emotional processing [31, 32]. Characteristics \nof connections of adult functional brain networks \nrefer to the analysis of connection density, \nclustering coefficient, and other indicators through \nEEG-derived brain network, and considering \nthem as features for emotion recognition [33–35]. \nIn addition to the abov e features, pre-processed \nEEG signals have standalone characteristics. Ding \net al. [36] used them to obtain high emotion \nrecognition performance on DEAP dataset. \nClassification models, including convolutional \nneural network (CNN) [37], recurrent neural \nnetwork (RNN) [38], long short-term memory \n(LSTM) [39], restricted Boltzmann machine (RBM) \n[40], etc. have increasingly attracted researchers’ \nattention in recent years. Deep learning is \nfavored in many tasks due to its self-adaptive \ntask-related features without manual feature \nextraction. Transformer was introduced by Google \nin 2017 [41] and has since been used in natural \nlanguage processing for various tasks, such as \nmachine translation, and text generation, showing  \nBrain Sci. Adv. \nhttps://mc03.manuscriptcentral.com/brainsa | Brain Science Advances \n \n \n212 \nhigh performance [42, 43]. The advent of DETR \n(DEtection TRansformer) has led to researchers \nusing Transformer on other modes of data. \nAlthough Transformer has been successful in \nmany areas, only a few studies have employed it \nin the area of EEG-based emotion recognition. \nWang et al. [44] used a cascade Transformer \ncombined with prior knowledge to determine \nspatial dependence between different brain \nregions. Abdallah Tubaisha t et al. [45] used   \na novel architecture based on Transformer to \nprocess multimodal neural  signals and identify \nemotions. Arjun et al. [46] divided EEG signals \nor time-frequency images generated by wavelet \ntransform into patches by using a Vision \nTransformer, and subsequently using a Transformer \nencoder for emotion re cognition. Although \nensemble learning methods have been proposed \nin the field of EEG-based emotion recognition, \nmost of the ensemble models employed in \nprevious studies are machine learning models. \nCurrently, many researchers are increasingly \nfocusing on the application of deep learning  \nin the field of emotion recognition. However, \nbecause different models have different preferences \nfor parameter adjustment in the training process \nof deep learning, ensemble learning is an efficient \nmethod to set multiple models and make unified \ndecisions. In cases where there are significant \ndifferences between different models, ensemble \nlearning is best suited for EEG-based emotion \nrecognition. Li et al. [47] proposed an ensemble \nlearning method based on multi-objective particle \nswarm optimization, that includes an ensemble \nof particle swarm including several traditional \nmachine learning classifiers, such as support \nvector machine (SVM), decision tree (DT), k-nearest \nneighbor, etc. Awan et al. [48] obtained the most \naccurate results on the AMIGOS dataset by \nusing a model integrating SVM, random forest, \nand LSTM. Although ensemble learning methods \nhave been proposed in the field of EEG-based \nemotion recognition, most of the ensemble models \nare machine learning models. \nAs part of the World Robot Contest, the BCI \nControlled Robot Cont est aims to promote \ninnovative breakthroughs in BCI technology. \nThe affective BCI technology competition is \npart of the BCI Controlled Robot Contest. The \ntechnical competition focuses on the participating \nteam’s ability to create and improve algorithms. \nThe competition is divided into preliminary and \nfinal stages. The preliminary rounds are held  \nin the cloud and the finals are held live, with \nreal-time rankings based on the accuracy and \nefficiency of the calculated results. \nThis paper outlines the method used by us in \nthe emotion recognition contest of BCI Controlled \nRobot Contest, and presents a systematic analysis \nof this method. In the competition, we proposed \na deep learning method based on ensemble \nlearning, terming it as Transformer-based ensemble \nmodel (TBEM), as shown in Fig. 1. This method \nintegrates two structures: a hybrid model of \nCNN and Transformer and a pure CNN model. \nThe chapters of the article are planned as follows. \nSection 2 introduces the models, datasets, \npreprocessing methods, experimental setup, and \ntraining strategies used by us in the competition \nin detail. Section 3 presents the experimental \nresults and discussion. Finally, Section 4 presents \na summary of TBEM. \n \n2  Method \n \n2.1 Data \nIn the preliminary phase of the competition, the \norganizers provided data containing 80 subjects \n(50 women and 30 men, with a mean age of \n20.16 years, ranging from 17 to 24 years), while \nthe organizers provided data containing 6 subjects  \nBrain Sci. Adv. \n                                                                      \n  \ns c i o p e n . c o m / j o u r n a l / 2 0 9 6 - 5 9 5 8                                                                      j o u r n a l s . s a g e p u b . c o m / h o m e / B S A   \n213\n(4 women and 2 men) in the final phase. The \ncollection of preliminary and final competition \ndata adopted a similar experimental paradigm, \nwherein 28 video clips were used as stimuli. The \nemotions of stimuli included anger, disgust, fear, \nsadness, amusement, joy, inspiration, tenderness, \nand neutral emotions. Roughly the same number \nof stimuli was used for each emotion (anger = 3, \ndisgust = 3, fear = 3, sadness = 3, amusement = 3, \njoy = 3, inspiration = 3, tenderness = 3, neutral = 4). \nEach subject was asked to  participate in seven \nexperimental blocks, each block contained \nfour trials (one trial co ntaining one stimulus); \nadditionally, the subject was asked to solve 20 \narithmetic problems between the two blocks [49]. \nThe experimental paradigm difference between \npreliminary and final competition data collection \nis that the experimental paradigm used in the \nfinal competition did not include partitioning \nblocks and did not ask participants to calculate \narithmetic problems. Additionally, a 32-channel \nNeuSen.W32 wireless EEG system was used in \nthe preliminary emotional induction experiment, \nwhile a 64-channel wireless EEG equipment was \nused in the final emotional induction experiment. \n2.2 Model \nIn many competitions on Kaggle, the world’s \nfamous data mining competition platform, \nparticipants have adopted the ensemble learning \nmethod and achieved more accurate results as \ncompared to those obtained via other methods. \nTherefore, this paper also adopted the idea of \nensemble learning. By combining multiple weak \nsupervised models, ensemble learning aims to \nobtain a strong supervised model with good \ngeneralization. The underlying idea of ensemble \nlearning is that even if a weak classifier makes  \na wrong prediction, other weak classifiers can \ncorrect the error. \nThis paper outlines a deep learning method \nbased on ensemble learning called TBEM [Fig. 1(a)] \nthat comprises two models: a HybridNet model, \nwhere a CNN is integrated with Transformer \n[Fig. 1(b)] and PureConvNet, that only comprises \nCNN [Fig. 1(c)]. Additionally, we show the specific \nparameter settings for both models in Table 1. \nHybridNet is a model with a large number  \no f  p a r a m e t e r s ;  i t  i s  d e s i g n e d  w i t h  r e f e r e n c e    \nto Vision Transformer (ViT) [50]. Similar to the \noriginal design intention of ViT, Transformer \nhas achieved the performance of SOTA in many \npattern recognition tasks, such as BERT [42], \nGPT3 [43], SwinTransfomer [51], etc. However, \nbecause Transformer is not sensitive to the task \nof processing long sequences, it cannot efficiently \nprocess long time series EEG signals in the \nEEG-based recognition task. Referring to the \ndesign of ViT, HybridNet introduces several \nconvolutional modules to compress the sequence \nand encode the features of EEG signals; thus, the \nfinal Transformer module can participate in the \nprocessing of data. HybridNet is mainly composed \nof two sub-blocks: one is a convolutional subblock \ncomposed of three convolutional modules, and \nthe other is a Transformer subblock composed \nmainly of three Transformer encoders (TE). \nEach convolutional module contains the same \nfive layers. These include: the convolutional layer, \nthe batch normalization layer (BN), the activation \nfunction layer (specifically implemented through \nthe ELU activation function), the dropout   \nlayer, and the average pooling layer. While  \nthe convolutional layer and pooling layer in  \nthe convolutional subblock are both 2-dimensional \nmodules, they realize the functions of one \ndimension, including time convolution, and space \nconvolution, which is si milar to EEGNet [52]. \nIn the convolutional subblock, the first two \nconvolutional modules realize the time perception \n(or time dimension feature extraction) function, \nand the third convolutional module realizes the \nspace perception (or space dimension feature \nBrain Sci. Adv. \nhttps://mc03.manuscriptcentral.com/brainsa | Brain Science Advances \n \n \n214 \nextraction) function. The kernel size of each \ntemporal convolution is 1 × 16, while the kernel \nsize of each spatial convolution is chans ×1, which \nis set to 30 × 1. All the average pooling layers \nhave a pooling size of 1 × 2. The inactivation rate \nis set to 0.5 for all dropout layers. In addition to \nTE, the Transformer subblock [Fig. 2(b)] includes \nthe learnable classification marker class token and \nposition-coding, wherein the position-coding \nadopts the learnable position coding of BERT \ntype instead of the position-coding represented \nby the sine and cosine functions in ViT. In TE, \nfor MutLI-heads of all multi-head self-attention \nmechanisms, the number of heads is set to 4; \neach head processes a sequence dimension of 64, \nwhile the fully connected network processes a \nsequence dimension of 64. \nPureConvNet is a lightweight model with fewer \nparameters as compared to that of HybridNet, \ndesigned by referring to EEGNet. The model \nconsists of four convolutional modules, which \nare similar to the convolutional modules in \nHybridNet. Each convolutional module consists \nof a convolutional layer, a BN layer, an ELU \nactivation function layer, a dropout layer, and an \naverage pooling layer. Similarly, the corresponding \n1-dimensional feature extraction operation is \nalso implemented through a 2-dimensional \nconvolutional layer. Specifically, PureConvNet \nadds a convolutional module for time-aware \nfunctionality. The kernel size is 1 × 16 for all \ntemporal convolutions; it is chans × 1 for spatial \nconvolutions, and 1 × 2 for the average pooling \nlayer; additionally, the remaining layers remain \nconsistent with HybridNet’s settings. \n2.3 Preprocessing \nFor the offline data (preliminary round data),  \n \nFig. 1 Transformer- based ensemble deep learning model (T BEM) structure. (a) TBEM is co mposed of two mo dels: a cascaded\nCNN-Transformer hybrid model (HybridNet) and a pure convolutio nal neural network model (PureC onvNet). (b) HybridNet model\narchitecture: the upper branch of TBEM architecture is a hybrid  model of convolutional neural network and Transformer. (c) PureConvNet\nmodel architecture: the lower branch of TBEM architecture is a pure convolutional neural network model. \nBrain Sci. Adv. \n                                                                      \n  \ns c i o p e n . c o m / j o u r n a l / 2 0 9 6 - 5 9 5 8                                                                      j o u r n a l s . s a g e p u b . c o m / h o m e / B S A   \n215\nthe abovementioned preprocessing steps were \napplied to each trial; th ese included five steps: \nre-referencing, segmentation, filtering, down- \nsampling, and normalization. First, re-referencing \ncan be conducted via two methods: bipolar \nre-reference and common average re-reference \n(CAR) methods. The bipolar re-reference method \nsubtracts the potential of two adjacent electrodes \n(according to the location  relationship of the \n10\n–20 system). It is important to note that the \nexperiment in the preliminary round employed \nall 32 electrodes of the NeuSen.W32 wireless \nEEG system; further, while  t h e  d e v i c e  u s e d  i n  \nthe final round does not contain A1 and A2 \nelectrodes, we excluded two channels ( N = 30) \nfrom the re-reference data in the final round. \nAdditionally, in the preliminary round, we \nemployed a third re-referencing method, namely, \nmastoid reference (MPR) method, that subtracts \nthe potential of the left mastoid (A1) from the \nchannels located in the left brain, and the potential \nof the right mastoid (A2) from the channels \nlocated on the right side. In the second step, a \nsliding window method with a window length \nof 14 s and a step size of 4 s was used for \nsegmentation. In the third step, two filtering \noperations were performed, namely, a 50 Hz \nnotch filter and a 0.5–45 Hz band-pass filter, both \nof which were implemented through a 6th order \nButterworth filter. In the fourth step, to reduce \nsubsequent computational complexity, the EEG \nsignal was down-sampled to 125 Hz; this reduced \nthe volume of data by half, given that the \noriginal sampling rate was relatively high. In  \nthe fifth step, Z-score normalization was applied \nto each down-sampled EEG segment. After \nimplementing the abovementioned five prepro-\ncessing steps, due to different trial lengths, the \naverage number of EEG segments obtained was \n13.5 (min = 6, max = 29). \nThese procedures were applied to the online \ndata (finals data), with the exception of differences \n \nTable 1 Specific parameter settings of HybridNet and PureConvNet. \nModel Step No. Step Name Parameters Output shape \n1 Input Input shape = (Chans, 1750) (1, Chans, 1750)\n2 Conv2D Size = (1, 16), Num = 32, Pad = same, BN, \nELU, Dropout = 0.5, Pool size = (1, 2) (32, Chans, 875)\n3 Conv2D Size=(1, 16), Num = 32, Pad = same, BN, ELU, \nDropout = 0.5, Pool size = (1, 2) (32, Chans, 437)\n4 Conv2D Size = (Chans, 1), Num = 64, BN, ELU, \nDropout = 0.5, Pool size = (1, 2) (64, 1, 218) \n5 Reshape & Extra Class Token Input shape = (64, 1) (64, 219) \n6–8 Transformer Encoder Head = 4, Head dim = 64, MLP dim = 64 (64, 219) \n9 Extract Class Token – (64, 1) \nTBEM- \n(HybridNet) \n10 FC Linear units = 9 9 \n1 Input Input shape = (Chans, 1750) (1, Chans, 1750)\n2 Conv2D Size = (1, 16), Num = 32, Pad = same, BN, \nELU, Dropout = 0.5, Pool size = (1, 2) (32, Chans, 875)\n3 Conv2D Size = (1, 16), Num = 32, Pad = same, BN, \nELU, Dropout = 0.5, Pool size = (1, 2) (32, Chans, 437)\n4 Conv2D Size = (Chans, 1), Num = 64, BN, ELU, \nDropout = 0.5, Pool size = (1, 2) (64, 1, 218) \n5 Conv2D Size = (1, 16), Num = 128, Pad = same, BN, \nELU, Dropout = 0.5, Pool size = (1, 6) (32, 1, 13) \nTBEM- \n（PureConvNet） \n6 FC Linear units = 9 9 \n \nBrain Sci. Adv. \nhttps://mc03.manuscriptcentral.com/brainsa | Brain Science Advances \n \n \n216 \nin data partitioning. Due to the competition’s \nrequirement to report a result every 1 second, \nthe window length for data partitioning remained \nunchanged, and the step size was updated to   \n1 second. \n2.4 Postprocessing \nSimilar to many internat ional competitions,   \nin the prediction phase in the affective BCI \ncompetition, postprocessing methods were used \nto integrate the results of multiple classifiers to \nobtain the final result [53]. The postprocessing \nmethod described in this  paper includes adding \nthe prediction vectors (i.e., the prediction \nprobabilities for each category) of m HybridNet \nmodels to the prediction vectors of n PureConvNet \nmodels to obtain the prediction vector of the \nensemble model. Finally, the category with the \nmaximum prediction vector is the prediction \nresult of the entire ensemble model. It is \nimportant to note that the m HybridNet models \nare trained by using different subsets of the \ntraining set, and the n PureConvNet models are \nalso obtained through different subsets of the \ntraining set. However, there may be overlapping \ntraining set subsets between the two types of \nmodels. In the experiment, both m and n are 3, \nindicating that the contribution of the two types \nof models to the results is the same (i.e., the two \ntypes of models have the same weight). \n2.4 Experimental settings \nIn the experiment, we used the Adam optimizer \nwith the initial learning rate set to 0.001. The \nlearning rate decay strategy of ReduceOnPlateau \nwas employed [54]; in other words, if the loss \ndid not decrease after seven epochs, then the \nlearning rate decayed to 10% of the original. The \nbatch size was set to 64 , the number of epochs \nwas set to 200, and the early stopping strategy \nwas applied. For the accuracy of the results to \nremain unchanged, the maximum tolerance was \n10 epochs. Our model was implemented in the \nTensorflow 2.11 framework by using Python \n3.10 and run on a GeForce RTX 3090 GPU. \n \n3  Results \n \nIn this section, we briefly describe the experimental \nsetup and results. \nFor the offline analysis in the preliminary \nround of the affective BCI competition, to evaluate \nthe performance of each algorithm, we used \n5-fold cross-validation (CV), with 4-fold CV for \ntraining and 1-fold CV for testing. Each fold \ncontained 16 subjects for a total of five cycles.  \nIn the online analysis in the final round of the \naffective BCI competition, when a new continuous \nEEG signal was obtained, each team needed to \nprovide real-time feedback with an interval of 1 s. \nUnder this requirement, all contestants needed \nto consider two scenarios of the model. One \nrequired adjusting the model according to the \nexisting test data, requiring the real-time data, \nand light weight of the model. The other required \ntraining based on the existing training data and \nfeedback on the results of the new test data. \nGiven that we could fully utilize the existing \ntraining data to obta in a model with high \nperformance, and the scenario did not consider \nmodel size, we considered using a completely \noffline approach in the end. \nWe compared accuracy  differences between \naverage and bipolar re-references based on \npreliminary data (THUR-EP dataset, excluding \nseveral subjects online). Among them, PureConvNet \nobtained the highest accuracy under average \nre-reference method (31.9 ± 6.9) and HybridNet \nobtained the highest accuracy under bipolar \nre-reference method (42.5 ± 6.8). Therefore, in the \nfinal round of the competition, we set the following: \nPureConvNet used an average re-reference \nmethod to re-reference, while HybridNet used a \nbipolar re-reference method to re-reference. \nBrain Sci. Adv. \n                                                                      \n  \ns c i o p e n . c o m / j o u r n a l / 2 0 9 6 - 5 9 5 8                                                                      j o u r n a l s . s a g e p u b . c o m / h o m e / B S A   \n217\n \nFig. 2 Bar chart of the top six scores in the final phrase of the \ncompetition. Note: TJU indicates team from Tianjin University. \nTHU indicates team from Tsinghua University. XUT indicates team \nfrom Xi’an University of Technology. ZJU indicates team from \nZhejiang University. WYU indicates team from Wuyi University. \nUM indicates team from University of Macau. \nFigure 2 shows the results of the top six teams \nin the finals. Among them, our team was TJU_TPU. \nThe recognition framework provided by our \nteam won the first place in the final round, and \nour score was significantly higher than the other \nteams, showing the effectiveness of the emotion \nrecognition framework proposed by our team. \nSpecifically, Fig. 3 describes the accuracy of \nthe results of the final cr oss-subject classification \nof different models (online analysis). As can   \nb e  s e e n  f r o m  F i g .  3 ( b ) ,  t h e  i n t e g r a t i o n  m o d e l  \nproposed in this paper achieved the highest \naccuracy regarding three subjects (S1–S3), and \nthe second highest accuracy regarding three \nsubjects S4–S6. Of the two sub-models of the \nensemble model, HybridNet exceeded the baseline \nmodel EEGNet by a significant extent in the data \nof multiple subjects (S4–S6); further, even the \naverage accuracy of Hybr idNet in S3 exceeded \nthat of EEGNet by 23.2%, being twice as high. \nPureConvNet had a 10% average accuracy \nadvantage regarding S3 and S5. HybridNet and \nPureConvNet had their own advantages in the \ntest data of different subjects. Regarding S3 and S6,  \n \nFig. 3 Cross-subject classification a ccuracy of different models on \nfinal data. (a) Average classificati on accuracy of different models \non six subjects. (b) Classification accuracy of different models on \nsix subjects respectively. \nHybridNet had 10% average accuracy advantage, \nwhile regarding S1, and S5, PureConvNet had \n3% accuracy advantage. It is important to note \nthat the integration model proposed by us achieved \nthe highest average performance among the six \nparticipants in the final ro und [Fig. 3(a)], further \nproving the efficiency of the proposed integration \nmodel. \nCompared with offline analysis, the collected \nEEG signals for the online emotion recognition \ntask had obvious environmental noise because \nthe environment of the subjects was not \nsufficiently quiet, and the subjects were easily \ndistracted by the surrounding environment; \ntherefore, it was difficult to induce strong emotions  \nBrain Sci. Adv. \nhttps://mc03.manuscriptcentral.com/brainsa | Brain Science Advances \n \n \n218 \nin the subjects. The average accuracy of HybridNet \nin the online results was lower than that of the \noffline results except for S3 and S4, and the online \nperformance of PureConvNet was also slightly \nlower than the offline performance. \nThe computational complexity of the algorithm \nis one of the most important indexes of an online \nemotion recognition task. The time required \nfor each method to calculate a single sample \n(14 s channel EEG signal with a matrix size of \n30 × 1750) is shown in Fig. 4. The baseline \nmodel EEGNet had the lo west computational \ncomplexity (0.217 GFLOPs) due to the low number \nof parameters. However, the computational \ncomplexity of the two models proposed by us is \nsimilar. It is important to note that the integration \nmodel is derived from the integration of HybridNet \nand PureConvNet, and that the proportion   \nof the two models is the same; therefore, the \ncomputational complexity of the integration \nmodel is between these sub-models. \n \nFig. 4 Computational complexity of different models \n4  Discussion \n \nEnsemble learning is a highly effective  \nmethod that can significantly improve a model’s \nperformance in competitions. Thus, we employed \nensemble learning to design TBEM. In TBEM, \nHybridNet generally outperformed PureConvNet \nin terms of performance, as shown in S2, S3, S4, \nand S6 [Fig. 3(b)] and for offline data. Moreover, \nHybridNet’s computational complexity was found \nto be slightly lower than that of PureConvNet. \nThe preprocessing method also has a significant \nimpact on the model’s performance, and \nre-referencing is one aspect of it. Studies have \nfound that re-referencing methods can affect the \naccuracy of EEG electrode placement [55–58]. By \ncomparing multiple re-referencing methods  \nfor EEG, we found that each method has its \nadvantages. Using TBEM, we combined different \nre-referencing methods with their corresponding \nmodels, it resulted in high scores in the EEG \nemotion recognition task in the affective BCI \ncompetition.  \nIt is challenging to build accurate cross-subject \nemotion recognition models by using traditional \nmachine learning methods because the neural \nrepresentation of emotion is highly complex and \nhas individual differences [32]. The existing \ntransfer learning method based on domain \nadaptation provides a possible solution to the \nproblem of individual differences in emotion \nrecognition models [59]. Thus, using transfer \nlearning is an effective choice to obtain a high \nclassification accuracy in future online emotion \nrecognition tasks. \nThe online task had a high requirement \nregarding the computational complexity of the \nmodel; based on the data in the previous section, \nit can be seen that since EEGNet had a low \ncomputational complexity, its performance is \nlow in the experiment. In general, the performance \nof a model increases with its computational \ncomplexity. However, ensemble learning provides \na solution for online tasks with high real-time \nperformance by integrating multiple models \nwith lower computational complexity. \nThe results of offline and online experiments \nprove that the performance of the offline \nBrain Sci. Adv. \n                                                                      \n  \ns c i o p e n . c o m / j o u r n a l / 2 0 9 6 - 5 9 5 8                                                                      j o u r n a l s . s a g e p u b . c o m / h o m e / B S A   \n219\nexperiment is slightly better than that of the \nonline experiment, which may also be related to \nthe lack of inducement of emotions in subjects in \nonline scenes [60]. This finding also highlights \nthe challenge of inducing high-intensity emotions \nin real situations. \nThe postprocessing method includes averaging \nthe predictions of each model. Each type of \nmodel has the same weight in the final result, \nrather than focusing on a particular type of model. \nIn some specific test data, poorly performing \nmodels may affect the overall performance of \nthe ensemble model [S4 and S6 in Fig. 3(b)], that \nresults in the accuracy of the ensemble model \nbeing lower than that of a single branch model. \nIn future, researchers can assign different weights \nto multiple models to improve the performance \nof the ensemble model. Furthermore, other \npostprocessing methods, such as voting, can be \nattempted. \n \n5  Conclusion \n \nThis paper provides a systematic introduction to \nthe approach employed by us, the winning team \nof the WRC2022 EEG-based emotion recognition \ncompetition, to design TBEM, an ensemble deep \nlearning method. We found that ensemble \nlearning is a highly effective method that can \nsignificantly improve a model’s performance in \nEEG-based emotion recognition competitions. \nTBEM comprises two models: HybridNet, \nbased on a CNN, and Transformer, and the \nPureConvNet model, which is based solely on a \nconvolutional neural network; it was observed \nthat HybridNet outperformed the PureConvNet \nmodel in terms of the accuracy of EEG-based \nemotion recognition. Moreover, HybridNet’s \ncomputational complexity was slightly lower \nthan that of PureConvNet. Thus, the results of \nthe experiments showed that the model combining \nCNN and Transformer has great potential in \nthe EEG-based emotion recognition task. The \nproposed method is meaningful for developing \neffective BCI systems.  \n \nEthical approval \n \nNone. \n \nConsent \n \nAll the subjects were informed and have signed \na written consent to collect their EEG. \n \nConflict of interests \n \nAll contributing authors report no conflict of \ninterests in this work. \n \nFunding \n \nThis work was granted by National Key Research \nand Development Program of China “Biology \nand Information Fusion” Key Project (Grant No. \n2021YFF1200600), National Natural Science \nFoundation of China (Grant Nos. 61906132 and \n81925020), and Key Project & Team Program of \nTianjin City (Grant No. XC202020). \n \nAcknowledgements \n \nAll authors thank Haolin Wu (Tsinghua \nUniversity), Dan Zhang (Tsinghua University), \nand Chengcheng Hong (Beijing University of \nPosts and Telecommunications) for their support \nin providing the data for the final competition. \n \nAuthors’ contribution \n \nDong Ming and Xiaopeng Si provided support \nfor this work; Dong Huang and Yulin Sun \nprovided model and analysis; Dong Huang, \nShudi Huang and He Huang completed the \nBrain Sci. Adv. \nhttps://mc03.manuscriptcentral.com/brainsa | Brain Science Advances \n \n \n220 \noriginal draft writing; and Xiaopeng Si reviewed \nand revised the manuscript. All the authors \napproved the final manuscript. \n \nData availability \n \nNone. \n \nReferences \n \n[1] Kuusikko S, Haapsamo H, Jansson-Verkasalo E, et al. \nEmotion recognition in children and adolescents with \nautism spectrum disorders. J Autism Dev Disord  \n2009, 39(6): 938–945. \n[2] Shanechi MM. Br ain-machine interfaces from motor \nto mood. Nat Neurosci 2019, 22(10): 1554–1564. \n[3] Gao XR, Wang YJ, Chen XG, et al. Interface, \ninteraction, and intelligence in generalized brain- \ncomputer interfaces. Trends Cogn Sci  2021, 25(8): \n671–684. \n[4] Ramakrishnan S, El Em ary IMM. Speech emotion \nrecognition approaches in human computer interaction. \nTelecommun Syst 2013, 52(3): 1467–1478. \n[5] Zhang SQ, Zhao XM, Ti an Q. Spontaneous speech \nemotion recognition using multiscale deep convolutional \nLSTM. IEEE Trans Affect Comput  2022, 13(2): \n680–688. \n[6] Valstar M, Pantic M, Patras I. Motion history for facial \naction detection in video. In 2004 IEEE International \nConference on Systems, Man and Cybernetics (IEEE \nCat. No. 04CH37583), The Hague, Netherlands, 2004, \npp 635–640. \n[7] Zeng ZH, Pantic M, Roisman GI, et al. A survey   \nof affect recognition met hods: audio, visual, and \nspontaneous expressions. IEEE Trans Pattern Anal \nMach Intell 2009, 31(1): 39–58. \n[8] Toisoul A, Kossaifi J, Bulat A, et al. Estimation of \ncontinuous valence and arousal levels from faces in \nnaturalistic conditions. Nat Mach Intell  2021, 3(1): \n42–50. \n[9] Soleymani M, Lichtenauer J, Pun T, et al. A multimodal \ndatabase for affect recognition and implicit tagging. \nIEEE Trans Affect Comput 2012, 3(1): 42–55. \n[10]  Koelstra S, Muhl C, Soleymani M, et al. DEAP: a \ndatabase for emotion analysis using physiological \nsignals. IEEE Trans Affect Comput 2012, 3(1): 18–31. \n[11]  Alarcão SM, Fonseca MJ. Emotions recognition \nusing EEG signals: a survey. IEEE Trans Affect \nComput 2019, 10(3): 374–393. \n[12]  LeDoux J. The amygdala. Curr Biol  2007, 17(20): \nR868–R874. \n[13]  Gallagher M, Chiba AA. The amygdala and emotion. \nCurr Opin Neurobiol 1996, 6(2): 221–227. \n[14]  Adolphs R. The unsolved problems of neuroscience. \nTrends Cogn Sci 2015, 19(4): 173–175. \n[15]  Dixon ML, Thiruchselvam R, Todd R, et al. Emotion \nand the prefrontal cortex : an integrative review. \nPsychol Bull 2017, \n143(10): 1033–1081. \n[16]  Salzman CD, Fusi S. Emotion, cognition, and mental \nstate representation in amygdala and prefrontal cortex. \nAnnu Rev Neurosci 2010, 33: 173–202. \n[17]  Hu X, Chen JJ, Wang F, et al. Ten challenges for \nEEG-based affective computing. Brain Sci Adv 2019, \n5(1): 1–20. \n[18]  Luck SJ. An introduction to the event-related potential \ntechnique. Cambridge: MIT Press, 2014. \n[19]  Niedermeyer E, da Silva FL. Electroencephalography: \nbasic principles, clinical applications, and related \nfields. Philadelphia, USA: Lippincott Williams & \nWilkins, 2005. \n[20]  Gonzalez H, George R,  Muzaffar S, et al. Hardware \nacceleration of EEG-based emotion classification \nsystems: a comprehensive survey. IEEE Trans Biomed \nCircuits Syst 2021, 15(3): 412–442. \n[21]  Li X, Zhang YZ, Tiwari P, et al. EEG based emotion \nrecognition: A tutorial and review. ACM Computing \nSurveys 2022, 55(4): 1–57. \n[22]  Hu WR, Huang G, Li LL, et al. Video-triggered \nEEG-emotion public databases and current methods: \na survey. Brain Science Advances  2020, 6(3): \n255–287. \n[23]  Hjorth B. EEG analysis based on time domain \nproperties. Electroencephalogr Clin Neurophysiol  \n1970, 29(3): 306–310. \n[24]  Petrantonakis PC, Hadjileontiadis LJ. Emotion \nrecognition from EEG using higher order crossings. \nIEEE Trans Inf Technol Biomed 2010, 14(2): 186–197. \n[25]  Kiymik MK, Güler I, Dizibüyük A, et al. Comparison \nof STFT and wavelet transform methods in determining \nepileptic seizure activity in EEG signals for real-time \napplication. Comput Biol Med 2005, 35(7): 603–616. \nBrain Sci. Adv. \n                                                                      \n  \ns c i o p e n . c o m / j o u r n a l / 2 0 9 6 - 5 9 5 8                                                                      j o u r n a l s . s a g e p u b . c o m / h o m e / B S A   \n221\n[26]  Delorme A, Makeig S. EEGLAB: an open source \ntoolbox for analysis of single-trial EEG dynamics \nincluding independent component analysis. J Neurosci \nMethods 2004, 134(1): 9–21. \n[27]  Akin M. Comparison of wavelet transform and FFT \nmethods in the analysis of EEG signals. J Med Syst  \n2002, 26(3): 241–247. \n[28]  Zheng WL, Lu BL. Investigating critical frequency \nbands and channels for EEG-based emotion recognition \nwith deep neural networks. IEEE Trans Auton Ment \nDev 2015, 7(3): 162–175. \n[29]  Schwartz GE, Davidson RJ, Maer F. Right hemisphere \nlateralization for emotion in the human brain: \ninteractions with cognition. Science 1975, 190(4211): \n286–288. \n[30]  Güntürkün O, Ströckens F, Ocklenburg S. Brain \nlateralization: a comparative perspective. Physiol Rev \n2020, 100(3): 1019–1063. \n[31]  Xu PF, Peng SL, Luo YJ, et al. Facial expression \nrecognition: a meta-analytic review of theoretical \nmodels and neuroimaging evidence. Neurosci Biobehav \nRev 2021, 127: 820–836. \n[32]  Hu X, Wang F, Zhang D. Similar brains blend emotion \nin similar ways: neural representations of individual \ndifference in emotion profiles. NeuroImage 2022, \n247: 118819. \n[33]  Ismail LE, Karwowski W. A graph theory-based \nmodeling of functional brain connectivity based  \non EEG: a systematic review in the context of \nneuroergonomics. IEEE Access  2020, 8: 155103– \n155135. \n[34]  Li PY, Liu H, Si YJ, et al. EEG based emotion \nrecognition by combining functional connectivity \nnetwork and local activations. IEEE Trans Biomed \nEng 2019, 66(10): 2869–2881. \n[35]  Li CB, Li PY, Zhang YS, et al. Effective emotion \nrecognition by learning discriminative graph topologies \nin EEG brain networks. IEEE Trans Neural Netw Learn \nSyst 2023, PP: 10.1109/TNNLS.2023.3238519. \n[36]  Liu Y, Ding YF, Li C, et al. Multi-channel EEG-based \nemotion recognition via a multi-level features guided \ncapsule network. Comput Biol Med  2020, 123: \n103927. \n[37]  Krizhevsky A, Sutskever I, Hinton GE. ImageNet \nclassification with deep convolutional neural networks. \nCommun ACM 2017, 60(6): 84–90. \n[38]  Sutskever I, Vinyals O, Le QV. Sequence to sequence \nlearning with neural networks. In Proceedings of the \n27th International Conference on Neural Information \nProcessing Systems - Volume 2 . Montreal, Canada, \n2014, pp 3104–3112. \n[39]  Hochreiter S, Schmidhuber J. Long short-term \nmemory. Neural Comput 1997, 9\n(8): 1735–1780. \n[40]  Ackley DH, Hinton GE, Sejnowski TJ. A learning \nalgorithm for Boltzmann machines. Cogn Sci  1985, \n9(1): 147–169. \n[41]  Vaswani A, Shazeer N, Pa rmar N, et al. Attention is \nall you need. In Proceedings of Advances in Neural \nInformation Processing Systems(NIPS 2017) , Long \nBeach, USA, 2017, pp 30. \n[42]  Devlin J, Chang MW, Lee K, et al. BERT: pre- \ntraining of deep bidirectional transformers for language \nunderstanding. arXiv, 2018: 1810.04805. https:// \narxiv.org/abs/1810.04805 \n[43]  Brown T, Mann B, Ryder N, et al. Language models \nare few-shot learners. In NIPS'20: Proceedings of the \n34th International Conference on Neural Information \nProcessing Systems, 2020, pp 1877–1901. \n[44]  Wang Z, Wang YX, Hu CF, et al. Transformers for \nEEG-based emotion recognition: a hierarchical spatial \ninformation learning model. IEEE Sens J 2022, 22(5): \n4359–4368. \n[45]  Aadam, Tubaishat A, Al-Obeidat F, et al. EmoPercept: \nEEG-based emotion classification through perceiver. \nSoft Comput 2022, 26(20): 10563–10570. \n[46]  Arjun A, Rajpoot AS, Raveendranatha Panicker M. \nIntroducing attention mechanism for EEG signals: \nemotion recognition with vision transformers. In 2021 \n43rd Annual International Conference of the IEEE \nEngineering in Medicine & Biology Society (EMBC). \nMexico, 2021, pp 5723–5726. \n[47]  Li R, Ren C, Zhang XW, et al. A novel ensemble \nlearning method using multiple objective particle swarm \noptimization for subject-independent EEG-based \nemotion recognition. Comput Biol Med  2021, 140: \n105080. \n[48]  Awan AW, Usman SM, Khalid S, et al. An ensemble \nlearning method for emotion charting using multimodal \nphysiological signals. Sensors 2022, 22(23): 9480. \n[49]  Hu X, Zhuang C, Wang F, et al. fNIRS evidence for \nrecognizably different positive emotions. Front Hum \nNeurosci 2019, 13: 120. \nBrain Sci. Adv. \nhttps://mc03.manuscriptcentral.com/brainsa | Brain Science Advances \n \n \n222 \n[50]  Dosovitskiy A, Beyer L, Kolesnikov A, et al. An \nimage is worth 16x16 words:  transformers for image \nrecognition at scale. arXiv, 2020: 2010.11929. \nhttps://arxiv.org/abs/2010.11929 \n[51]  Liu Z, Lin YT, Cao Y, et al. Swin transformer: \nhierarchical vision transformer using shifted windows. \nIn 2021 IEEE/CVF International Conference on \nComputer Vision  ( ICCV), Montreal, QC, Canada, \n2022, pp 9992–10002. \n[52]  Lawhern VJ, Solon AJ, Waytowich NR, et al. EEGNet: \na compact convolutional neural network for EEG-based \nbrain-computer interfaces. J Neural Eng 2018, 15(5): \n056013. \n[53]  Dong XB, Yu ZW, Cao WM, et al. A survey on \nensemble learning. Front Comput Sci  2020, 14(2): \n241–258. \n[54]  Zhang S, Zhao ZY, Guan CT. Multimodal continuous \nemotion recognition: a technical report for ABAW5. \narXiv, 2023: 2303.10335. https://arxiv.org/abs/ \n2303.10335 \n[55]  Hu SA, Lai YX, Valdes-Sosa PA, et al. How do \nreference montage and electrodes setup affect the \nmeasured scalp EEG potentials? J Neural Eng  2018, \n15(2): 026013. \n[56]  Hu SA, Yao DZ, Bringas-Vega ML, et al. The statistics \nof EEG unipolar references: derivations and properties. \nBrain Topogr 2019, 32(4): 696–703. \n[57]  Hu SA, Yao DZ, Valdes-Sosa PA. Unified Bayesian \nestimator of EEG reference at infinity: rREST \n(regularized reference electrode standardization \ntechnique). Front Neurosci 2018, 12: 297. \n[58]  Yao DZ, Qin Y, Hu SA, et al. Which reference should \nwe use for EEG and ERP practice? Brain Topogr  \n2019, 32(4): 530–549. \n[59]  Liang SJ, Su L, Fu YF, et al. Multi-source joint \ndomain adaptation for cross-subject and cross-session \nemotion recognition from electroencephalography. \nFront Hum Neurosci 2022, 16: 921346. \n[ 6 0 ]   T a n g  C ,  L i  Y H ,  C h e n  B D .  C o m p a r i s o n  o f  c r o s s -  \nsubject EEG emotion recognition algorithms in the \nBCI Controlled Robot Contest in World Robot Contest \n2021. Brain Sci Adv 2022, 8(2): 142–152. \n \n \nXiaopeng Si is working as an associate professor at Tianjin University since 2018. He \nobtained his Ph.D. in biomedical engineering from Tsinghua University, and finished \nthe visiting scholar in Biomedical En gineering Department at Johns Hopkins \nUniversity. His research interest includes cognitive neuroscience and brain-inspired \nintelligence, neural engineering and brain –computer interaction, neural information \nacquisition and intelligent computing, mu ltimodal human brain neuroimaging and \nregulation, etc. His team used multimodal neuroimaging and machine learning methods to understand \nthe human cognitive processes. They tried to apply these mechanisms in people with brain disorders, \nand also tried to enhanc e normal people's ability by creating  mind reading technology. He has \nmanaged and participated many national and international research projects. He has published papers \nin PNAS, Cerebral Cortex , IEEE JBHI , Journal of Neural Engineering , Frontiers in Aging Neuroscience , \nFrontiers in Neuroscience, etc. E-mail: xiaopengsi@tju.edu.cn \n \n \n \nDong Huang  received his B.S. degree in el ectronics information science and \ntechnology from Northeast Petroleum Univer sity in 2020. He is currently pursuing \nthe M.S. degree in biomedical engineering,  Tianjin University, Tianjin, China.    \nHis research interest incl udes affective computing, and deep learning. E-mail: \nhuang_dong@tju.edu.cn \nBrain Sci. Adv. \n                                                                      \n  \ns c i o p e n . c o m / j o u r n a l / 2 0 9 6 - 5 9 5 8                                                                      j o u r n a l s . s a g e p u b . c o m / h o m e / B S A   \n223\nYulin Sun  received the B.E. degree in biomedical engineering from Tianjin \nUniversity in 2020. Currently, he is working toward the M.M. degree in biomedical \nengineering, Tianjin University, Tianjin, China. His research interest includes seizure \ndetection, digital signal processing, affect ive computing, and deep learning. E-mail: \nsyuri @tju.edu.cn \n \n \n \nShudi Huang received the B.E. degree in communication engineering from Wuhan \nUniversity of Technology in 2022. She is cu rrently studying for a Master’s degree in \nelectronic information at Tianjin Universi ty. Her research interests include digital \nsignal processing, affective computing and deep learning. E-mail: hsd_991205@163.com \n \n \n \n \nHe Huang received the B.E. degree in medical information engineering from Zhejiang \nUniversity of Traditional Chinese Medicine  in 2022. He is currently studying for a \nMaster's degree, Tianjin University, majo ring in Intelligent Medical Engineering, \nTianjin, China. His research interests include affective computing and deep learning. \nE-mail: 1485364086@qq.com \n \n \n \n \nDong Ming  received the B.S. and Ph.D. degrees in biomedical engineering from \nTianjin University, Tianjin, China, in 1999 and 2004, respectively. He worked as a \nResearch Associate at the Department of  Orthopaedics and Traumatology, Li Ka \nShing Faculty of Medicine, The University of Hong Kong, from 2002 to 2003, and \nwas a visiting scholar with the Division of Mechanical Engineering and Mechatronics, \nUniversity of Dundee, U.K., from 2005 to 2006. He joined Tianjin University as a \nfaculty at the College of Precision Instru ments and Optoelectronics Engineering, in \n2006, and has been promoted to a full professor of bi omedical engineering since 2011. He is currently \na chair professor with the Department of Biomedical Engineering, Tianjin University, where he is also \nthe head of the Neural Engineering and Rehabilitation Laboratory. E-mail: richardming@tju.edu.cn ",
  "topic": "Electroencephalography",
  "concepts": [
    {
      "name": "Electroencephalography",
      "score": 0.7148081064224243
    },
    {
      "name": "Computer science",
      "score": 0.6709778904914856
    },
    {
      "name": "Brain–computer interface",
      "score": 0.6663053035736084
    },
    {
      "name": "Convolutional neural network",
      "score": 0.6126874685287476
    },
    {
      "name": "Deep learning",
      "score": 0.578067421913147
    },
    {
      "name": "CONTEST",
      "score": 0.5764288306236267
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5497971177101135
    },
    {
      "name": "Transformer",
      "score": 0.5368279814720154
    },
    {
      "name": "Emotion recognition",
      "score": 0.4855925440788269
    },
    {
      "name": "Speech recognition",
      "score": 0.46860942244529724
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3264622390270233
    },
    {
      "name": "Psychology",
      "score": 0.23966169357299805
    },
    {
      "name": "Engineering",
      "score": 0.15643370151519775
    },
    {
      "name": "Neuroscience",
      "score": 0.1334964632987976
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Political science",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I5740404",
      "name": "Tianjin Medical University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I162868743",
      "name": "Tianjin University",
      "country": "CN"
    }
  ]
}