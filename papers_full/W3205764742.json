{
  "title": "NViT: Vision Transformer Compression and Parameter Redistribution.",
  "url": "https://openalex.org/W3205764742",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2884051107",
      "name": "Huanrui Yang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2621302641",
      "name": "Hongxu Yin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2133744149",
      "name": "Pavlo Molchanov",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097184588",
      "name": "Hai Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1987057275",
      "name": "Jan Kautz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3133696297",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W2951569836",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W3034957837",
    "https://openalex.org/W3166513219",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3034292689",
    "https://openalex.org/W2975084181",
    "https://openalex.org/W3138516171",
    "https://openalex.org/W2955419283",
    "https://openalex.org/W2771655537",
    "https://openalex.org/W3152698000",
    "https://openalex.org/W2945767825",
    "https://openalex.org/W2951528897",
    "https://openalex.org/W3170841864",
    "https://openalex.org/W2975059944",
    "https://openalex.org/W3146091044",
    "https://openalex.org/W3177313544",
    "https://openalex.org/W3110271845",
    "https://openalex.org/W3102446692",
    "https://openalex.org/W3169769781",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W3156528192",
    "https://openalex.org/W2737121650",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3033737024",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2062861714",
    "https://openalex.org/W3119373805"
  ],
  "abstract": "Transformers yield state-of-the-art results across many tasks. However, they still impose huge computational costs during inference. We apply global, structural pruning with latency-aware regularization on all parameters of the Vision Transformer (ViT) model for latency reduction. Furthermore, we analyze the pruned architectures and find interesting regularities in the final weight structure. Our discovered insights lead to a new architecture called NViT (Novel ViT), with a redistribution of where parameters are used. This architecture utilizes parameters more efficiently and enables control of the latency-accuracy trade-off. On ImageNet-1K, we prune the DEIT-Base (Touvron et al., 2021) model to a 2.6x FLOPs reduction, 5.1x parameter reduction, and 1.9x run-time speedup with only 0.07% loss in accuracy. We achieve more than 1% accuracy gain when compressing the base model to the throughput of the Small/Tiny variants. NViT gains 0.1-1.1% accuracy over the hand-designed DEIT family when trained from scratch, while being faster.",
  "full_text": "Global Vision Transformer Pruning with Hessian-Aware Saliency\nHuanrui Yang1,2*, Hongxu Yin1, Maying Shen1, Pavlo Molchanov1, Hai Li3, and Jan Kautz1\n1NVIDIA, 2University of California, Berkeley,3Duke University\nhuanrui@berkeley.edu, {dannyy, mshen, pmolchanov, jkautz}@nvidia.com, hai.li@duke.edu\nAbstract\nTransformers yield state-of-the-art results across many\ntasks. However, their heuristically designed architecture\nimpose huge computational costs during inference. This\nwork aims on challenging the common design philosophy of\nthe Vision Transformer (ViT) model with uniform dimension\nacross all the stacked blocks in a model stage, where we\nredistribute the parameters both across transformer blocks\nand between different structures within the block via the\nﬁrst systematic attempt on global structural pruning. Deal-\ning with diverse ViT structural components, we derive a\nnovel Hessian-based structural pruning criteria comparable\nacross all layers and structures, with latency-aware regu-\nlarization for direct latency reduction. Performing iterative\npruning on the DeiT-Base model leads to a new architec-\nture family called NViT (Novel ViT), with a novel parameter\nredistribution that utilizes parameters more efﬁciently. On\nImageNet-1K, NViT-Base achieves a2.6×FLOPs reduction,\n5.1×parameter reduction, and 1.9×run-time speedup over\nthe DeiT-Base model in a near lossless manner. Smaller\nNViT variants achieve more than 1% accuracy gain at the\nsame throughput of the DeiT Small/Tiny variants, as well as\na lossless 3.3×parameter reduction over the SWIN-Small\nmodel. These results outperform prior art by a large margin.\nFurther analysis is provided on the parameter redistribution\ninsight of NViT, where we show thehigh prunabilityof ViT\nmodels, distinct sensitivitywithin ViT block, and unique pa-\nrameter distribution trendacross stacked ViT blocks. Our\ninsights provide viability for a simple yet effective parameter\nredistribution rule towards more efﬁcient ViTs for off-the-\nshelf performance boost.\n1. Introduction\nTransformer models demonstrate high model capacity,\neasy scalability, and superior ability in capturing long-range\ndependency [1, 9, 19, 30, 38]. Vision Transformer,i.e., the\nViT [12], shows that embedding image patches into tokens\nand passing them through a sequence of transformer blocks\n*Work done during an internship at NVIDIA.\ncan lead to higher accuracy compared to state-of-the-art\nCNNs. DeiT [35] further presents a data-efﬁcient training\nmethod such that acceptable accuracy can be achieved with-\nout extensive pretraining. Offering competitive performance\nto CNNs under similar training regimes, transformers now\npoint to the appealing perspective of solving both NLP and\nvision tasks with the same architecture [18, 20, 49].\nUnlike CNNs built with convolutional layers that con-\ntain few dimensions like the kernel size and the number\nof ﬁlters, the ViT has multiple distinct components, i.e.,\nQKV projection, multi-head attention, multi-layer percep-\ntron, etc. [38], each deﬁned by independent dimensions. As\na result, the dimension of each component in each ViT block\nneeds to be carefully designed to achieve a decent trade-off\nbetween efﬁciency and accuracy. However, this is typically\nnot the case for state-of-the-art models. Models such as\nViT [12] and DeiT [35] mainly inherit the design heuristics\nfrom NLP tasks, e.g., use MLP expansion ratio 4, ﬁx QKV\nper head, all the blocks having the same dimensions , etc.,\nwhich may not be optimal for computer vision [4], caus-\ning signiﬁcant redundancy in the base model and a worse\nefﬁciency-accuracy trade-off upon scaling, as extensively\nshown empirically. New developments in ViT architectures\nincorporate additional design tricks like multi-stage archi-\ntecture [41], more complicated attention schemes [23], and\nadditional convolutional layers [13] etc., yet no attempt has\nbeen made on understanding the potential of redistributing\nparameters within the stacked vision transformer blocks.\nThis work targets efﬁcient ViTs by exploring parameter\nredistribution within ViT blocks and across multiple layers of\ncascading ViT blocks. To this end, we start with the straight-\nforward DeiT design space, with only ViT blocks. We ana-\nlyze the importance and redundancy of different components\nin the DeiT model via latency-aware global structural prun-\ning, leveraging the insights to redistribute parameters for\nenhanced accuracy-efﬁciency trade-off. Our approach, as\nvisualized in Fig. 1, starts from analyzing the blocks in the\ncomputation graph of ViT to identify all the dimensions that\ncan be independently controlled. We apply global structural\npruning over all the components in all blocks. This offers\ncomplete ﬂexibility to explore their combinations towards\n1\narXiv:2110.04869v2  [cs.CV]  29 Mar 2023\nMLP\nhead C classes\nQQQ\nPrunable weights\nQQK QQV\nPROJPROJProj.\nFC1 FC2\nExQKxH ExQKxH E x V\nVxExH\nExM\nMxE\nVision Transformer\nPrunable Components Analysis NViT\nInput tokens\nNViT Block\nNViT Block\nNViT Block\nNViT Block\nNViT Block\nNViT Block\nMLP\nhead C classes\nPruned weights\nFC1\n0.7Ex0.6QKx0.5H\n0.7Ex0.9Vx0.5H\n0.9Vx0.7Ex0.5H\n0.7Ex0.6M\n0.6Mx0.7E\nGlobal Importance Ranking\nBlocks / Layers\nImportance Thres. FC1 FC2\nPROJProj.\nQQ KK VV\nParameter Redistribution\nBlocks Layers\nDimension\nPruned model\nTrend\nInput tokens\nH heads\nEx(VxH)\nQ\nK\nV\nEx(QKxH)\nkh\nqh\nvh\nkh\nqh\nvh\nkh\nqh\nvh\nLayerNorm\nInput\ntokens\nNxE\nProj.\n(VxH)xE\nMultihead Self Attention (MSA)\nOutput\ntokens\nNxE\nConcat\nsplit\nEx(QKxH)\nTransformer Blocks x12\nExM MxE\nMulti-layer Perceptron (MLP)\nFC 2\nLayerNorm\nFC 1\n0.7Ex0.6QKx0.5H\nGlobal Structural Pruning\nTowards Efficient Inference\nFigure 1. Towards efﬁcient vision transformer models. Starting form ViT, speciﬁcally DeiT, we identify the design space of pruning (i)\nembedding size E, (ii) number of head H, (iii) query/key size QK, (iv) value size V and (v) MLP hidden dimension M in Sec. 3.1. Then we\nutilize a global ranking of latency-aware importance score to perform iterative global structural pruning in Sec. 3.2, achieving pruned NViT\nmodels. Finally we analyze the parameter redistribution trend of all the components in the NViT model, as in Sec. 5.1.\nan optimal architecture in a complicated design space. Per-\nforming global pruning on ViT is signiﬁcantly challenging,\ngiven the diverse structural components and signiﬁcant mag-\nnitude differences. Previous methods only attempts on per-\ncomponent pruning with the same pruning ratio [5], which\ncannot lead to parameter redistribution across components\nand blocks. We derive a new importance score based on the\nHessian matrix norm of the loss for global structural pruning,\nfor the ﬁrst time offering comparability among all prunable\ncomponents. Furthermore, we incorporate the estimated la-\ntency reduction into the importance score. This guides the\nﬁnal pruned architecture to be faster on target devices.\nThe iterative structural pruning of the DeiT-Base model\nenables a family of efﬁcient ViT models: NViT. On the\nImageNet-1K benchmark [33], NViT enables a nearly loss-\nless 5.14×parameter reduction, 2.57 ×FLOPs reduction\nand 1.86 ×speed up on V100 GPU over the DeiT-Base\nmodel. An 1% and 1.7% accuracy gain is observed over\nDeiT-Small and DeiT-Tiny models when we scale down the\nNViT to a similar latency. NViT achieves a further 1.8 ×\nFLOPs reduction and an 1.5×speedup over NAS-based Aut-\noFormer [4] (ICCV’21) and the SOTA structural pruning\nmethod S2ViTE [5] (NeurIPS’21).The efﬁciency and perfor-\nmance beneﬁt of NViT trained on ImageNet also transfers to\ndownstream classiﬁcation and segmentation tasks.\nUsing structural pruning for architectural guidance, we\nfurther make an important observation that the popular uni-\nform distribution of parameters across all layers is, in fact,\nnot optimal. To this end, we present further empirical and\ntheoretical analysis on the new parameter distribution rule\nof efﬁcient ViT architectures, which provides a new angle\non understanding the learning dynamic of vision transformer\nmodel. We believe our ﬁndings would inspire future design\nof efﬁcient ViT architectures.\nOur main contributions are as follows:\n• Propose NViT, a novel hardware-friendlyglobal struc-\ntural pruning algorithm enabled by a latency-aware,\nHessian-based importance-based criteria and tailored\ntowards the ViT architecture, achieving a nearly loss-\nless 1.9×speedup, signiﬁcantly outperforms SOTA ViT\ncompression methods and efﬁcient ViT designs;\n• Provide a systematic analysis on the prunable compo-\nnents in the ViT model. We perform structural pruning\non the embedding dimension, number of heads, MLP\nhidden dimension, QK dimension and V dimension of\neach head separately;\n• Explore hardware-friendly parameter redistribution of\nViT, ﬁnding high prunability of ViT models, distinct\nsensitivity within ViT block, and unique parameter\ndistribution trend across stacked ViT blocks.\n2. Related work\n2.1. Vision transformer models\nInspired by the success of transformer models in NLP\ntasks, recent research proposes to use them on computer\n2\nvision tasks. The inspiring vision transformer (ViT) [12]\ndemonstrates the possibility of performing high-accuracy\nimage classiﬁcation with transformer architecture only. This\nstimulates recent works to improve training and efﬁciency of\nthe ViT model. One noticeable approach DeiT [35] provides\ncarefully designed training schemes and data augmentations\nto train ViT from scratch on ImageNet only. Another line of\nwork renovates ViT transformer blocks to better capture im-\nage features, such as changing input tokenization [14,48], us-\ning hierarchical architecture [14,23,41], upgrading positional\nencoding [7], and performing localized attention [15, 23].\nIn this work we focus on the original ViT architecture [12]\namid its straightforward design space, as illustrated in the top\nof Fig. 1. ViT model ﬁrst divides the input image into patches\nthat are tokenized to embedding dimension Ethrough a lin-\near projection. Image tokens, together with an independently\ninitialized class token, form an input x ∈RN×E. Input\ntokens pass through transformer blocks before classiﬁcation\nis made from the class token output of the last block.\nA ViT block includes a multi-head self attention (MSA)\nand a multi-layer perceptron (MLP) module. The MSA\nmodule ﬁrst linearly transforms the N ×E tokens into\nqueries q ∈ RN×(QK×H), keys k ∈ RN×(QK×H), and\nvalues v ∈ RN×(V ×H). The q, k and v are then split\ninto H heads. Each head performs the self-attention op-\neration Attn(qh,kh,vh) = softmax\n(\nqhkT\nh√dh\n)\nvh in parallel.\nThe output of all the heads are then concatenated prior to a\nfully-connected (FC) linear projection back to the original\ndimension of RN×E. Note that though previous works set\nQK = V in designing the model architecture [4, 12, 35],\nsetting them differently will not go against the shape rule of\nmatrix multiplication. The MLP module includes two FC\nlayers with a hidden dimension of M. The output of the last\nFC layer preserves token dimension at RN×E.\nBuilt upon the original ViT, DeiT models [35] further ex-\nploit a distillation token, which learns from the output label\nof a CNN teacher during the training process to incorporate\nsome inductive bias of the CNN model, and signiﬁcantly\nimproves the DeiT accuracy. Our work uses the DeiT model\narchitecture as a starting point, where we explore the poten-\ntial of better distributing dimensions of different blocks for\nenhanced efﬁciency-accuracy tradeoff.\n2.2. Efﬁcient ViT models\nTo improve model efﬁciency, very recent works perform\nstructural pruning on vision transformer models, with train-\nable gate variables [53] or Taylor importance score [5]. Both\nmethods show the potential of compressing ViT models, yet\nonly consider part of the prunable architecture, use uniform\nsparsity for all components, and do not take run time latency\ninto account, thus may not lead to optimal compressed mod-\nels and cannot discover potential parameter redistribution.\nOur method resolves these issues through a latency-aware\nglobal structural pruning of all prunable components across\nall layers in a jointly manner.\nBesides pruning, multiple attempts have been made in\ndesigning efﬁcient ViT architectures. Notable methods in-\nclude adding convolutional layers [13, 44], using multiple\nViT stages with different feature scales [3, 6, 41, 52], and\nexplore novel attention mechanisms [15, 16, 23, 48]. Yet all\nthese work use the same dimension for all transformer blocks\nin each stage, whereas our work explores the parameter re-\ndistribution among cascading transformer blocks to achieve\nbetter efﬁciency-accuracy tradeoff without additional tricks.\nThe closest attempt to our our work is AutoFormer [4], uses\na neural architecture search (NAS) approach to search for pa-\nrameter redistribution of ViT models. Due to the constraint\non the supernet training cost, AutoFormer only explores a\nsmall number of dimension choices; while our method con-\ntinuously explores the entire design space of ViT model with\na single iterative pruning process, leading to the ﬁnding of\nmore efﬁcient architectures.\nAnother orthogonal yet relevant line of work explores\naccelerated ViT inference with token pruning [22,32]. Token\npruning reduces model FLOPs by halting tokens at early\nstages without altering the network; while our work removes\nstructural components from weights to reach a smaller static\narchitecture. Both ideas are complimentary and we will\nexplore joint pruning in future work.\n3. Latency-aware global structural pruning\n3.1. Prunable structures with head alignment\nTo explore the full space of parameter redistribution, we\nfocus on all the independent structures in ViT, namely:\n• The embedding dimension, denoted as EMB;\n• The number of heads in MSA, denoted as H;\n• The output dimension of Q and K projection per head in\nMSA, denoted as QK;\n• The output dimension of V projection and input dimension\nof the PROJ per head, denoted as V;\n• The hidden dimension of MLP, denoted as MLP.\nNote that this is slightly different from the dimensions we\nshowed in Sec. 2.1. As highlighted on the left of Fig. 2, in a\ntypical ViT implementation, the QKV projection output di-\nmensions are a concatenation of all the attention heads [43],\neffectively QK×H or V ×H. The projected tokens are\nthen split into H heads to allow the computation of MSA in\nparallel. If we directly prune this concatenated dimension,\nthen there is no control on the remaining QK and V dimen-\nsion of each head. Therefore, the latency of the entire MSA\nwill be bounded by the head with the largest dimension.\n3\nEx(VxH)\nQ\nK\nV\nEx(QKxH)\nkh\nqh\nvh\nkh\nqh\nvh\nkh\nqh\nvh\nLayerNorm\nInput  \ntokens \nNxE\nProj. \n(VxH)xE\nOutput  \ntokens \nNxE\nConcat\nsplit\nEx(QKxH)\nReshaped Attention Block for Pruning\nkh\nqh\nvh\nkh\nqh\nvh\nkh\nqh\nvh\nLayerNorm\nInput  \ntokens \nNxE ExQKxH\nExVxH\nExQKxH\nVxExH\nQQQ\nK KKK\nVQV\nProj. Proj. Proj. \nH headsH heads\nOriginal Attention Block\nOutput  \ntokens \nNxE\nFigure 2. Head Alignment for latency-friendly pruning. We reshaped the QKV and ﬁnal output projection in the attention block to explicitly\ncontrol the number of head and align the QK & V dimensions in each head.\nTo alleviate such inconsistency between pruned head di-\nmensions, we propose head alignment, which explicitly con-\ntrol the number of heads and align the QK and V dimension\nremaining in each head. As illustrated on the right of Fig. 2,\nfor model pruning we reshape the weight of Q, K, V and\nPROJ projection layers to single out the head dimension H.\nPerforming structural pruning on the reshaped block along\nthe H dimension will enable the removal of an entire head,\nwhile pruning along the QK/V dimension guarantees the\nremained QK and V dimension of all the heads are the same.\nThis reshaping is only applied during the pruning process,\nwhile the ﬁnal pruned model is converted back to the con-\ncatenated scheme. Note that H, QK, V and MLP in different\nblocks can be independently pruned; while EMB needs to be\nidentical across the blocks due to the shortcut connections.\nA comparison of pruning with or without head alignment\nis provided in Appendix B.3, where we demonstrate head\nalignment can bring up to 0.3% accuracy gain under the\nsame latency target.\n3.2. Structural pruning algorithm\n3.2.1 Hessian-based group importance ranking\nInspired by recent research on the loss surface geometry of\ndeep neural networks, here we consider the Hessian matrix\nof the loss function with respect to the group of parameters\nto be pruned to determine our pruning criteria. Speciﬁcally,\nwe consider the matrix norm, the squared sum of Hessian\neigenvalue, as the criteria for determining the importance\nof the group of parameters. Previous research [29, 45, 47]\nhas concluded that a smaller Hessian norm indicates a ﬂatter\nloss surface, which leads to a smaller loss difference when\nthe group is perturbed, i.e. pruned, as in Fig. 3.\nWS\nL\nPruning Pruning\nGroup S1 Group S2 \nAfter\ntraining\nAfter\ntraining\nAfter\npruningAfter\npruning\nFigure 3. Loss of pruning different structural groups. Group S1\nwith smaller Hessian norm lives in ﬂatter loss minima, leading to\nlower loss increase after pruning.\nTo unify the analysis of structural groups belonging to\ndifferent components with different shapes and value ranges,\nwe assign a gate variable gSto each structural group Sof\nweight, so that the model weight W is reparameterized as\nW = gSWS, where WSdenotes all weight elements in the\nstructural group S. We set all gates to 1 before pruning so\nthat the reparameterized model is equivalent to the original\none. The structural pruning process then aims to ﬁnd the\ngates with the smallest Hessian norm, so that we can alter\nthem to 0 to fulﬁll pruning with minimal loss.\nFormally, consider a model whose loss is L(D,gSWS)\non dataset D, the Hessian matrix with respect to the gate\nvariables is deﬁned as Hi,j = ∂2L\n∂gSi ∂gSj\n, where Si and Sj\nare different structural groups. However, a ViT model typi-\ncally contains tens of thousands of structural groups under\nour structural pruning conﬁguration, making it infeasible to\ncompute Hdirectly. Luckily, here we only need the norm of\neigenvalues, i.e. ∑\ni λ2\ni , for our pruning criteria, which can\nbe computed via a Hessian-vector multiplication [29]:\n∑\ni\nλ2\ni = Ez||Hz||2,z ∼N(0,I). (1)\nWith can be further approximated with a ﬁnite difference\napproximation of the Hessian\nHz≈(∇gS L(gS+ hz) −∇gS L(gS))/h, (2)\nwhere his a small positive constant. This leads to our prun-\ning criteria ISas:\nIS:= Ez||(∇gS L(gS+hz)−∇gS L(gS))/h||2,z ∼N(0,1).\n(3)\nNote that here z follows an univariate normal distribution\nsince gSis a binary number.\nThe computation of Eq. (3) is now feasible for all the\ngroups. However, computing the gradient of the gate variable\nfor each group individually is still costly. To efﬁciently\ncalculate the pruning criteria we simplify Eq. (3) by further\nderiving the two gradient terms. Here we derive the second\nterm ﬁrst since it is simpler. Using the fact W = gSWSand\nthe chain rule we have:\n∇gS L(gS) = ∂L\n∂W\n∂W\n∂gS\n= (∇WS L(WS))T WS\n=\n∑\ns∈S\n∇ws L(ws) ws.\n(4)\n4\nFor the ﬁrst term, note that by deﬁnition gS = 1 , so\ngS+ hz is equivalent to (1 + hz)gS. In this way we can\nderive the ﬁrst term using the result we have in Eq. (4) as:\n∇gS L(gS+ hz) = ∂L\n∂(1 + hz)gS\n∂(1 + hz)gS\n∂gS\n= (1 + hz)\n∑\ns∈S\n∇ws L(ws) ws.\n(5)\nSubstituting Eq. (4) and Eq. (5) into Eq. (3) leads to a\nsimpliﬁed importance score:\nIS(W) = Ez||hz\n∑\ns∈S\n∇ws L(ws) ws/h||2\n=\n(∑\ns∈S\nL′(ws) ws\n)2\nEzz2\n=\n(∑\ns∈S\nL′(ws) ws\n)2\n,\n(6)\nwhere L′(ws) = ∇ws L(ws). Since the gradients with re-\nspect to all weight elements are already available from back-\npropagation, the importance score in Eq. (6) can be easily\ncalculated during the ﬁnetuning process without additional\ncost. We then greedily remove a few structural groups at a\ntime in our pruning process based on their importance scores,\nuntil the targeted constraint is achieved.\nInterestingly, the resulted importance score is similar to\nthe Taylor-based pruning criteria used in CNN ﬁlter prun-\ning [2, 10, 28, 46]. Previous work used heuristics to expand\nthe Taylor-based criteria from single parameter importance\nto structural groups, while we directly derive the structural\npruning metric from a novel Hessian-based perspective. The\nHessian-based importance score can be compared among all\nlayers of weight as a global pruning criteria as it reﬂects the\nsensitivity of the structural group to the loss value. Previous\npruning methods also considers magnitude-based pruning,\nwhich prunes away the group with the lowest weight mag-\nnitude. However, we ﬁnd that magnitude cannot be applied\nas a global pruning criteria for ViT pruning, as it will make\nmost of the structural components either unpruned or all\npruned away. We provide detailed comparison on the effec-\ntiveness of our Hessian-based score vs. magnitude-based\nscore for ViT pruning in Appendix B.4. We also show the\nstrong correlation between our hessian importance score and\nreal loss difference induced by pruning in Appendix B.5.\n3.2.2 Latency-aware regularization\nPruning can be tailored towards latency reduction by penaliz-\ning the importance score with latency-aware regularization:\nIL\nS(W) = IS(W) −η\n(\nLat(W) −Lat(W\\S)\n)\n. (7)\nLat(·) denotes the latency of the current model, which is char-\nacterized by a lookup table given the current EMB, H, QK, V ,\nand MLP dimension of each block in the pruned model. De-\ntails of the lookup table are provided in Appendix A.3, where\nwe show a small lookup table can achieve accurate latency\nestimation throughout the pruning process. Latency-aware\nregularization helps the pruned model to reach the latency\ntarget faster with higher accuracy, as shown in Appendix B.6.\nWe use IL\nS as the pruning criteria for iterative pruning in\nour work, with detailed procedure in Appendix A.2. A com-\npact and dense model can be achieved by removing pruned\ngroups and recompiling the model.\n3.2.3 Ampere (2:4) GPU sparsity\nThe recently introduced NVIDIA Ampere GPU supports\nacceleration of sparse matrix multiplication with a speciﬁc\npattern of 2:4 sparsity (2 of the 4 consecutive weight ele-\nments are zero). This comes with a limitation of requiring\nthe input and output dimensions of all linear projections to be\ndivisible by 16 [27]. We assure compatibility with such pat-\ntern by structurally pruning matrices to have the remaining\ndimension be divisible by 16 (more details in Appendix A.2).\nInterestingly, we ﬁnd that Ampere sparsity can be performed\nlosslessly with magnitude pruning after the initial pruning.\n3.3. Training objective\nWe next consider the training objective function that sup-\nports both pruning for importance ranking and ﬁnetuning for\nweight update. To start with, we inherit the CNN hard distil-\nlation training objective as proposed in DeiT [35], which is\nformulated as follows:\nLCNN = LCE\n(\nΨ(zs\nc),Y\n)\n+ LCE\n(\nΨ(zs\nd),Y CNN\n)\n, (8)\nwhere Ψ(·) denotes softmax and LCE the cross entropy loss.\nWe refer to logits computed from the class token of the\npruned model as zs\nc, and the one computed from the distil-\nlation token as zs\nd. Note that zs\nc is supervised by the true\nlabel Y, while zs\nd is supervised by the output label of a CNN\nteacher YCNN. Unless otherwise stated, we use a pretrained\nRegNetY-16GF model [31] as the teacher, in line with DeiT.\nIn addition to CNN distillation, we consider full model\ndistillation given the unique access to such supervision under\nthe pruning setup. Speciﬁcally, the “full model” corresponds\nto the pretrained model, which serves as the starting point\nof the pruning process. Ideally a pruned model shall be-\nhave similar to its original counterpart. To encourage this,\nwe distill the classiﬁcation logits from both the class and\ndistillation tokens of the pruned model from the original\ncounterpart, forming Eq. (9):\nLfull = LKL\n(\nΨ(zs\nc/τ),Ψ(zt\nc/τ)\n)\n+LKL\n(\nΨ(zs\nd/τ),Ψ(zt\nd/τ)\n)\n.\n(9)\n5\nSuperscripts t and s denote the output of the pretrained model\nand the model being pruned respectively. LKL is the KL\ndivergence loss, and τ is the distillation temperature.\nThe ﬁnal objective is therefore composed as:L= αLfull+\nLCNN. An ablation study of alternating the formulation of\nthe training objective is provided in Appendix B.1.\n4. NViT Performance\n4.1. Pruning analysis on ImageNet-1K\nWe apply our pruning method on the challenging\nImageNet-1K benchmark, using the DeiT-Base model pre-\ntrained with CNN distillation as the starting point to achieve\na family of NViT models. The training and ﬁnetuning hyper-\nparameters can be found in Appendix A.1.\nComparing with existing models. We compare the\nmodel size, run time speedup and accuracy of the state-\nof-the-art manually designed ViT models and our pruned\nmodels in Table 1. For best insights, we conduct pruning\nin four conﬁgurations. Note that all these 4 conﬁguration\nare achieved from the same pretrained DeiT-Base model in\na single global pruning run, each ﬁnetuned from a check-\npoint snapshot after different pruning steps. Details for our\npruning conﬁgurations can be found in Appendix A.2.\n• NViT-B aims to match the accuracy of DeiT-B model,\nwhich achieves an 1.86×speedup and a 2.57×reduction\non FLOPs over DeiT-B with neglectable 0.07% accuracy\ndrop. It also achieves a lossless 2.25 ×further FLOPs\nreduction over the more efﬁcient SWIN-B model.\n• NViT-Haims to half the latency of DeiT-B, with only 0.4%\naccuracy loss. It also achieves 1.41×further reduction on\nFLOPs over SWIN-S with similar accuracy.\n• NViT-Smatches DeiT-S latency, with+1% accuracy.\n• NViT-Tmatches DeiT-T latency, with+1.7% accuracy.\nFurthermore, the superiority of NViT over DeiT and\nSWIN cannot be bridged even after we ﬁnetune the pre-\ntrained models. For example, ﬁnetuning the pretrained DeiT-\nT, DeiT-S, and SWIN-T models for additional 300 epochs\nfollowing the scheme of NViT ﬁnetuning will improve the\naccuracy to 75.0%, 81.8%, and 81.7% respectively, which\nare still below what achieved by the corresponding NViT\nmodels. The lossless 1.9x model acceleration for DeiT-B\nwith the NViT-B conﬁguration has never been achieved from\nprevious designs.\nComparing with SOTA compression methods. We\ncompare NViT with state-of-the-art ViT compression meth-\nods, AutoFormer [4] in ICCV’21, S2ViTE [5] in NeurIPS’21,\nEViT [22] in ICLR’22, and SPViT [17] in Table 2. For a fair\ncomparison for all methods we report the accuracy trained\nwith CNN hard distillation. As no such accuracy is available\nTable 1. Structural pruning results on ImageNet-1K. Our NViT\nmodels are compared with manually designed ViT architectures.\nAll compression ratios and speedups are computed with respect\nto that of DEIT-Base model. All Latency estimated on a single\nGPU with batch size 256. “ASP” means post-training 2:4 Ampere\nsparsity pruning with TensorRT [27].\nSize (Compression) Speedup (×)\nModel #Para (×) #FLOPs (×) V100 RTX 3080 Top-1 Acc.\nDEIT-B86M (1.00) 17.6G (1.00) 1.00 1.00 83.36\nSWIN-B88M (0.99) 15.4G (1.14) 0.95 - 83.30\nNViT-B34M (2.57)6.8G (2.57)1.86 1.75 83.29\n+ ASP 17M (5.14)6.8G (2.57)1.86 1.85 83.29\nSWIN-S50M (1.74) 8.7G (2.02) 1.49 - 83.00\nNViT-H30M (2.84)6.2G (2.85)2.01 1.89 82.95\n+ ASP 15M (5.68)6.2G (2.85)2.01 1.99 82.95\nDEIT-S22M (3.94) 4.6G (3.82) 2.44 2.27 81.20\nSWIN-T29M (2.99) 4.5G (3.91) 2.58 - 81.30\nNViT-S21M (4.18)4.2G (4.24)2.52 2.35 82.19\n+ ASP10.5M (8.36)4.2G (4.24)2.52 2.47 82.19\nDEIT-T5.6M (15.28) 1.2G (14.01) 5.18 4.66 74.50\nNViT-T6.9M (12.47)1.3G (13.55)4.97 4.55 76.21\n+ ASP3.5M (24.94)1.3G (13.55)4.97 4.66 76.21\nin the S2ViTE paper, we rerun the experiment with CNN\ndistillation following their ofﬁcial GitHub repo1.\n• Comparing to AutoFormer: NViT-H achieves a further\n1.5×speedup over AutoFormer-B with a higher accuracy;\nNViT-T outperforms AutoFormer-T by 0.5% under similar\nsize and lower latency.\n• Comparing to S2ViTE: NViT-H achieves a further 1.9×\nFLOPs reduction and 1.5×speedup over the 40%-pruned\nmodel, with a higher accuracy.\n• Comparing to EViT : NViT-S achieves a further 2.8 ×\nFLOPs reduction and 1.6×speedup over the pruned Base\nmodel, with a higher accuracy.\nMoreover, the lossless 1.9×speedup of NViT-B over DeiT-B\nis a big leap over all previous methods.\nComparing with concurrent ViT variants. NViT pro-\nvides a viable way to discover efﬁcient architecture with\nparameter redistribution in the DeiT design space, without\nusing additional components like more layers, specially de-\nsigned attention, or multi-stage architecture. Here we com-\npare NViT with concurrent ViT architectures in Tab. 3. NViT\nmodels achieve stronger performance than these architec-\ntures while only exploring the basic DeiT design space.\nPruning other ViT variants. We try NViT on pruning\nthe SWIN transformer model. Note that SWIN transformer\ndoesn’t bring additional structural components comparing\nto ViT, as the novel shift-window attention mechanism is\nparameter-free. In this case our method can be applied on\na single stage in SWIN-Transformer directly without any\n1https://github.com/VITA-Group/SViTE\n6\nTable 2. Comparing with SOTA ViT efﬁciency improvement\nmethods. S2ViTE and EViT speedups are taken from their papers,\nwhile AutoFormer speedup is measured with the same code base\nas NViT on a RTX 3080 GPU. All speedups are computed with\nrespect to that of DeiT-Base model.\nModel #FLOPs Speedup Top-1 Acc.\nNViT-B 6.8G 1.85× 83.29\nS2ViTE-B-40 [5]11.7G 1.33× 82.92\nAutoFormer-B [4]11G 1.34 × 82.90\nSPViT [17] 8.4G - 82.40\nNViT-H 6.2G 1.99× 82.95\nEViT-DeiT-B [22]11.6G 1.59× 82.10\nNViT-S 4.2G 2.47× 82.19\nAutoFormer-T [4]1.3G 4.59 × 75.70\nNViT-T 1.3G 4.66× 76.21\nTable 3. Comparing with concurrent ViT architectures. Accu-\nracy with or w/o CNN distillation are reported when available.\nModel #Para #FLOPs Acc. (no dis) Acc. (dis)\nConViT-S+ [13]48M 10G 82.2 82.9\nCaiT-S-24 [36]46.9M 9.4G 82.7 83.5\nCaiT-XS-36 [36]38.6M 8.1G 82.6 82.9\nNViT-B 34M 6.8G 82.8 83.3\nT2T-ViT-14 [48]21.5M 6.1G 81.7 -\nCaiT-XS-24 [36]26.6M 5.4G 81.8 82.0\nAs-ViT-S [6]29.0M 5.3G 81.2 -\nTNT-S [15] 23.8M 5.2G 81.5 -\nCvT-13 [44] 20M 4.5G 81.6 -\nGLiT-S [3] 24.6M 4.4G 80.5 -\nPVT-S [41] 24.5M 3.8G 79.8 -\nNViT-S 21M 4.2G 82.0 82.2\nmodiﬁcation. Here we prune stage 2 of the SWIN-B model,\nwhich consists of 18/24 of the transformer blocks, 65% of\nparameters, 75% of FLOPs and 70% of the overall latency.\nNViT achieves a lossless Stage 2 compression of 1.8×pa-\nrameter reduction, 1.8×FLOPs reduction and 1.7×runtime\nspeedup on V100 GPU.This indicates that NViT is also ap-\nplicable to other ViT variants.\n4.2. Transfer learning to downstream tasks\nFinally, we evaluate the generalizability of our pruned\nNViT models. Here we ﬁnetune the ImageNet trained DeiT\nand NViT models on CIFAR-10, CIFAR-100 [21], iNatural-\nist 2018 and 2019 [37] dataset. We further investigate the\npotential of transferring the achieved NViT models into back-\nbones for tasks beyond classiﬁcation, speciﬁcally, semantic\nsegmentation. We evaluate the performance of DeiT/NViT\nbackbones on the Cityscape dataset [8] and the ADE20K\ndataset [51]. The details of the datasets used for our transfer\nlearning experiments and detailed experiment settings are\nprovided in Appendix A.1.2. Results are provided in Tab. 4.\nNViT models consistently outperform the DeiT models on\nall the tasks. These observations show that the efﬁciency\ndemonstrated on ImageNet can be preserved on downstream\nTable 4. Transfer learning tasks performance with ImageNet\npretraining. We report the performance of ﬁnetuning the Ima-\ngeNet trained models on other datasets. Top-1 accuracy is reported\nfor classiﬁcation tasks, while mIoU is reported for segmentation\ntasks\nModel CIFAR-10 CIFAR-100 iNat-18 iNat-19Cityscape ADE20K\nDeiT-S 98.52% 87.07% 66.79% 74.22% 71.89% 40.15%\nNViT-S98.78% 87.90% 69.10%77.00%73.22% 41.54%\nDeiT-T 97.93% 85.66% 62.41% 72.08% 66.65% 34.38%\nNViT-T98.31% 85.88% 64.78%74.65%67.09% 35.42%\nTable 5. Average Hessian trace and latency (V100, batch size\n576) per neuron in each structure of the DeiT-B model.\nStructure Q K V Proj FC1 FC2\nHessian trace1.4e-6 1.6e-6 6.5e-6 6.4e-6 6.1e-6 4.6e-6\nLatency (s) 1.7e-4 1.4e-4 1.2e-5\ntasks, even beyond classiﬁcation.\n5. Exploring parameter redistribution\n5.1. Trends observed in ViT pruning\nAs observed by [24], channel/ﬁlter pruning in CNN mod-\nels can provide guidance on ﬁnding efﬁcient network archi-\ntectures, yet this has never been explored on ViT models.\nHere we show for the ﬁrst time that our pruning method\ncan serve as an effective architecture search tool for ViT\nmodels. We observe NViT models of different sizes follows\nconsistent insights, as visualized in Fig. 4:\n1. Number of heads, QK of each head and MLP scales\nlinearly with the dimension of EMB; while V of each\nhead can be largely kept the same;\n2. Reducing dimensions related to the multi-head attention\n(H, QK) while increasing MLP dimension may lead to\nmore accurate model under similar latency.\n3. The scaling factors of head, QK and MLP arenot uniform\namong all blocks: dimension is larger in the blocks in the\nmiddle and smaller towards the two ends;\nCompared to original ViT design, our insight shows,\nwithin each block, the need to scale QK separately from\nV , and more importantly to distribute different dimensions\nacross different ViT blocks. Interestingly, these trends are\nnot observed in NLP transformer compression [26, 40].\n5.2. Understanding the parameter redistribution\nGiven the insights on the parameter redistribution trend,\nhere we analyze its reason from the perspective of Hessian\nsensitivity analysis. Averaged Hessian trace of the training\nloss with respect to the model weights has been shown ef-\nfective for analyzing the importance of different structural\n7\nFigure 4. Model dimension comparison between NViT-B (blue), NViT-S (grey) and NViT-T model (green).\ncomponents in a DNN model [11, 47]. Here we compute the\nper-structure average Hessian trace of the DeiT-B model on\nImageNet in Tab. 5. Average latency reduction in pruning\neach neuron is also provided. Comparing across different\nstructures, we can see V/Proj appears more important than\nQ/K, showing the need to scale them separately (insight 1).\nMLP layers also show higher importance than QK layers,\nwhile occupying less latency. This justiﬁes redistributing pa-\nrameters from QK to MLP layers for better latency-accuracy\ntradeoff (insight 2). Besides Hessian, Appendix C.1 observes\nthe trend in the attention score diversity among all heads of\neach block, which reﬂects a similar less-more-less trend in\nredundancy appears in each block (insight 3).\n5.3. Comparing to CNN\nAs global structural pruning has been extensively studied\non CNN, here we compare our insights achieved in NViT\nwith the results in SOTA CNN pruning research [28, 34]:\n• Prunability: ViT appears to have higher prunability\nthan CNN models. SOTA CNN pruning achieves loss-\nless 2×FLOPs reduction and 1.6×speedup on ResNet\nmodels [34]. Whereas we achieve lossless 2.6×FLOPs\nreduction and 1.9×speedup on DeiT-B model;\n• Structure diversity: Convolutional layers within a\nCNN block typically show comparable sensitivity [28].\nWhereas different structural components within a ViT\nblock shows distinct sensitivityin pruning.\n• Sensitivity distribution: Sensitivity is lower in the\nearlier layers of a CNN stage, then gradually increase\ntowards the end [28]. Whereas we discovered a unique\nless-more-less distribution among stacked ViT blocks.\nThese comparisons show the different challenges and op-\nportunities faced by efﬁcient CNN and ViT designs. We\nhope our study can inspire future exploration on the different\nlearning dynamics and architecture design rules between\nCNN and ViT architectures.\n5.4. Design novel architecture with redistribution\nViT parameter redistribution. To further illustrate the\neffectiveness on our insights on the redistribution of param-\neters, we follow our insights to design a new architecture\nwe name ReViT (Redistributed ViT). We follow the trends\nTable 6. ReViT block dimensions. For comparison the dimensions\nof a DeiT block are also listed.\nBlocks H QK V MLP\nDeiT EMB/64 64 64 EMB ×4\nReViT ϵ×EMB/100ϵ×EMB/20 64 ϵ×EMB×3\nin Fig. 4 and heuristically design a simpliﬁed rule in Tab. 6\nto determine the parameter dimensions of each block. For a\n12-layer vision transformer model, we use ϵ= 2 for block\n4-9, and use ϵ = 1 for other blocks. H is rounded to the\nnearest even number, and QK rounded to the nearest number\ndivisible by 8 to satisfy Ampere GPUs requirements.\nComparison with DeiT. To verify that our parameter\nredistribution is beneﬁcial, we train all pairs of DeiT and\nReViT models from scratch on the ImageNet-1K benchmark\nwith the same objective and hyperparameters, as speciﬁed\nin Appendix A.1.3. As shown in Tab. 7, ReViT achieve\nhigher accuracy than DeiT with similar FLOPs and lower\nlatency. Speciﬁcally, ReViT-S and ReViT-T achieve a Top-1\naccuracy gain of 0.21% and 1.36%, respectively, over their\nDeiT counterparts. We also show ReViT rule can work out\nof the box on SWIN models in Appendix C.2.\nTable 7. Comparing ReViT models with DeiT models. All com-\npression ratios and speedups are computed with respected to that\nof the DeiT-Base model. DeiT accuracy marked with * indicates\nthe train-from-scratch accuracy we achieve from the DeiT GitHub\nrepo2 using default hyperparameters 3. All pairs of models are\ntrained with the same hyperparameters\nModel EMB #Para (×) #FLOPs (×) Speedup Accuracy\nDeiT-S 384 22M (3.94) 4.6G (3.82) 2.29× 81.01%*\nReViT-S384 23M (3.82)4.7G (3.75)2.31× 81.22%\nDeiT-T 192 5.6M (15.28) 1.2G (14.01) 4.39× 72.84%*\nReViT-T176 5.9M (14.64)1.3G (13.69)4.75× 74.20%\n6. Conclusions\nThis work proposes a latency-aware global pruning frame-\nwork that provides signiﬁcant lossless compression on DeiT-\nBase model, facilitating the ﬁnding of parameter redistribu-\ntion for better efﬁciency-accuracy tradeoff in vision trans-\nformers. We hope this work opens up a new way to better\n2https://github.com/facebookresearch/deit.\n3As in Table 9 of [35].\n8\nunderstand the contribution of different components in the\nViT architecture, and inspires more efﬁcient ViT models.\nReferences\n[1] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 1\n[2] Akshay Chawla, Hongxu Yin, Pavlo Molchanov, and Jose\nAlvarez. Data-free knowledge distillation for object detec-\ntion. In Proceedings of the IEEE/CVF Winter Conference on\nApplications of Computer Vision, pages 3289–3298, 2021. 5\n[3] Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen\nLin, Ming Sun, Junjie Yan, and Wanli Ouyang. Glit: Neural\narchitecture search for global and local image transformer. In\nProceedings of the IEEE/CVF International Conference on\nComputer Vision, pages 12–21, 2021. 3, 7\n[4] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling.\nAutoformer: Searching transformers for visual recognition.\narXiv preprint arXiv:2107.00651, 2021. 1, 2, 3, 6, 7\n[5] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and\nZhangyang Wang. Chasing sparsity in vision transformers:\nAn end-to-end exploration. arXiv preprint arXiv:2106.04533,\n2021. 2, 3, 6, 7\n[6] Wuyang Chen, Wei Huang, Xianzhi Du, Xiaodan Song,\nZhangyang Wang, and Denny Zhou. Auto-scaling\nvision transformers without training. arXiv preprint\narXiv:2202.11921, 2022. 3, 7\n[7] Xiangxiang Chu, Zhi Tian, Bo Zhang, Xinlong Wang, Xi-\naolin Wei, Huaxia Xia, and Chunhua Shen. Conditional\npositional encodings for vision transformers. arXiv preprint\narXiv:2102.10882, 2021. 3\n[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo\nRehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke,\nStefan Roth, and Bernt Schiele. The cityscapes dataset for se-\nmantic urban scene understanding. In Proc. of the IEEE Con-\nference on Computer Vision and Pattern Recognition (CVPR),\n2016. 7, 12\n[9] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina\nToutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding. arXiv preprint\narXiv:1810.04805, 2018. 1\n[10] Xiaohan Ding, Guiguang Ding, Xiangxin Zhou, Yuchen\nGuo, Jungong Han, and Ji Liu. Global sparse momentum\nsgd for pruning very deep neural networks. arXiv preprint\narXiv:1909.12778, 2019. 5\n[11] Zhen Dong, Zhewei Yao, Amir Gholami, Michael W Ma-\nhoney, and Kurt Keutzer. Hawq: Hessian aware quantization\nof neural networks with mixed-precision. In Proceedings\nof the IEEE International Conference on Computer Vision ,\npages 293–302, 2019. 8\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 3\n[13] Stéphane d’Ascoli, Hugo Touvron, Matthew L Leavitt, Ari S\nMorcos, Giulio Biroli, and Levent Sagun. Convit: Improving\nvision transformers with soft convolutional inductive biases.\nIn International Conference on Machine Learning , pages\n2286–2296. PMLR, 2021. 1, 3, 7\n[14] Ben Graham, Alaaeldin El-Nouby, Hugo Touvron, Pierre\nStock, Armand Joulin, Hervé Jégou, and Matthijs Douze.\nLevit: a vision transformer in convnet’s clothing for faster\ninference. arXiv preprint arXiv:2104.01136, 2021. 3\n[15] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu,\nand Yunhe Wang. Transformer in transformer. arXiv preprint\narXiv:2103.00112, 2021. 3, 7\n[16] Ali Hatamizadeh, Hongxu Yin, Jan Kautz, and Pavlo\nMolchanov. Global context vision transformers. arXiv\npreprint arXiv:2206.09959, 2022. 3\n[17] Haoyu He, Jing Liu, Zizheng Pan, Jianfei Cai, Jing Zhang,\nDacheng Tao, and Bohan Zhuang. Pruning self-attentions\ninto convolutional layers in single path. arXiv preprint\narXiv:2111.11802, 2021. 6, 7\n[18] Yifan Jiang, Shiyu Chang, and Zhangyang Wang. Transgan:\nTwo transformers can make one strong gan. arXiv preprint\narXiv:2102.07074, 2021. 1\n[19] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. Tinybert: Distill-\ning bert for natural language understanding. arXiv preprint\narXiv:1909.10351, 2019. 1\n[20] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-\nlanguage transformer without convolution or region supervi-\nsion. arXiv preprint arXiv:2102.03334, 2021. 1\n[21] Alex Krizhevsky and Geoffrey Hinton. Learning multiple\nlayers of features from tiny images. Technical report, Citeseer,\n2009. 7, 12\n[22] Youwei Liang, Chongjian GE, Zhan Tong, Yibing Song, Jue\nWang, and Pengtao Xie. EVit: Expediting vision transformers\nvia token reorganizations. In International Conference on\nLearning Representations, 2022. 3, 6, 7\n[23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng\nZhang, Stephen Lin, and Baining Guo. Swin transformer:\nHierarchical vision transformer using shifted windows. arXiv\npreprint arXiv:2103.14030, 2021. 1, 3\n[24] Zhuang Liu, Mingjie Sun, Tinghui Zhou, Gao Huang, and\nTrevor Darrell. Rethinking the value of network pruning.\narXiv preprint arXiv:1810.05270, 2018. 7\n[25] Jiachen Mao, Huanrui Yang, Ang Li, Hai Li, and Yiran Chen.\nTprune: Efﬁcient transformer pruning for mobile devices.\nACM Transactions on Cyber-Physical Systems , 5(3):1–22,\n2021. 16\n[26] Paul Michel, Omer Levy, and Graham Neubig. Are\nsixteen heads really better than one? arXiv preprint\narXiv:1905.10650, 2019. 7, 16\n[27] Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic,\nDusan Stosic, Ganesh Venkatesh, Chong Yu, and Paulius Mi-\ncikevicius. Accelerating sparse deep neural networks. arXiv\npreprint arXiv:2104.08378, 2021. 5, 6\n9\n[28] Pavlo Molchanov, Arun Mallya, Stephen Tyree, Iuri Fro-\nsio, and Jan Kautz. Importance estimation for neural net-\nwork pruning. In Proceedings of the IEEE/CVF Conference\non Computer Vision and Pattern Recognition, pages 11264–\n11272, 2019. 5, 8\n[29] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Jonathan\nUesato, and Pascal Frossard. Robustness via curvature regu-\nlarization, and vice versa. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\npages 9078–9086, 2019. 4\n[30] Alec Radford, Karthik Narasimhan, Tim Salimans, and Ilya\nSutskever. Improving language understanding by generative\npre-training. 2018. 1\n[31] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaim-\ning He, and Piotr Dollár. Designing network design spaces.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 10428–10436, 2020. 5\n[32] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie\nZhou, and Cho-Jui Hsieh. Dynamicvit: Efﬁcient vision trans-\nformers with dynamic token sparsiﬁcation. In Advances in\nNeural Information Processing Systems (NeurIPS), 2021. 3\n[33] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-\njeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,\nAditya Khosla, Michael Bernstein, Alexander C. Berg, and Li\nFei-Fei. ImageNet Large Scale Visual Recognition Challenge.\nInternational Journal of Computer Vision (IJCV), 115(3):211–\n252, 2015. 2\n[34] Maying Shen, Hongxu Yin, Pavlo Molchanov, Lei Mao,\nJianna Liu, and Jose M Alvarez. Halp: Hardware-aware\nlatency pruning. arXiv preprint arXiv:2110.10811, 2021. 8\n[35] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Hervé Jégou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. In International Conference on Machine Learning,\npages 10347–10357. PMLR, 2021. 1, 3, 5, 8, 12\n[36] Hugo Touvron, Matthieu Cord, Alexandre Sablayrolles,\nGabriel Synnaeve, and Hervé Jégou. Going deeper with\nimage transformers. In Proceedings of the IEEE/CVF Inter-\nnational Conference on Computer Vision, pages 32–42, 2021.\n7\n[37] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui,\nChen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and\nSerge Belongie. The inaturalist species classiﬁcation and\ndetection dataset. In Proceedings of the IEEE conference on\ncomputer vision and pattern recognition, pages 8769–8778,\n2018. 7, 12\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia\nPolosukhin. Attention is all you need. In Advances in neural\ninformation processing systems, pages 5998–6008, 2017. 1\n[39] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt\nHaberland, Tyler Reddy, David Cournapeau, Evgeni Burovski,\nPearu Peterson, Warren Weckesser, Jonathan Bright, Sté-\nfan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jar-\nrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric\nJones, Robert Kern, Eric Larson, C J Carey,˙Ilhan Polat, Yu\nFeng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef\nPerktold, Robert Cimrman, Ian Henriksen, E. A. Quintero,\nCharles R. Harris, Anne M. Archibald, Antônio H. Ribeiro,\nFabian Pedregosa, Paul van Mulbregt, and SciPy 1.0 Con-\ntributors. SciPy 1.0: Fundamental Algorithms for Scientiﬁc\nComputing in Python. Nature Methods, 17:261–272, 2020.\n13\n[40] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and\nIvan Titov. Analyzing multi-head self-attention: Specialized\nheads do the heavy lifting, the rest can be pruned. arXiv\npreprint arXiv:1905.09418, 2019. 7, 16\n[41] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao\nSong, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyra-\nmid vision transformer: A versatile backbone for dense predic-\ntion without convolutions. arXiv preprint arXiv:2102.12122,\n2021. 1, 3, 7\n[42] Alan Weiser and Sergio E Zarantonello. A note on piecewise\nlinear and multilinear table interpolation in many dimensions.\nMathematics of Computation, 50(181):189–196, 1988. 13\n[43] Ross Wightman. Pytorch image models. https :\n/ / github . com / rwightman / pytorch - image -\nmodels, 2019. 3\n[44] Haiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang\nDai, Lu Yuan, and Lei Zhang. Cvt: Introducing convolutions\nto vision transformers. In Proceedings of the IEEE/CVF\nInternational Conference on Computer Vision, pages 22–31,\n2021. 3, 7\n[45] Huanrui Yang, Xiaoxuan Yang, Neil Zhenqiang Gong, and\nYiran Chen. Hero: Hessian-enhanced robust optimization\nfor unifying and improving generalization and quantization\nperformance. arXiv preprint arXiv:2111.11986, 2021. 4\n[46] Hongxu Yin, Pavlo Molchanov, Jose M Alvarez, Zhizhong\nLi, Arun Mallya, Derek Hoiem, Niraj K Jha, and Jan Kautz.\nDreaming to distill: Data-free knowledge transfer via deep-\ninversion. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pages 8715–8724,\n2020. 5\n[47] Shixing Yu, Zhewei Yao, Amir Gholami, Zhen Dong, Sehoon\nKim, Michael W Mahoney, and Kurt Keutzer. Hessian-aware\npruning and optimal neural implant. In Proceedings of the\nIEEE/CVF Winter Conference on Applications of Computer\nVision, pages 3880–3891, 2022. 4, 8\n[48] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nZihang Jiang, Francis EH Tay, Jiashi Feng, and Shuicheng\nYan. Tokens-to-token vit: Training vision transformers from\nscratch on imagenet. arXiv preprint arXiv:2101.11986, 2021.\n3, 7\n[49] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xi-\nang, Philip HS Torr, et al. Rethinking semantic segmentation\nfrom a sequence-to-sequence perspective with transformers.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, pages 6881–6890, 2021. 1\n[50] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu,\nZekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao\nXiang, Philip H.S. Torr, and Li Zhang. Rethinking semantic\nsegmentation from a sequence-to-sequence perspective with\ntransformers. In CVPR, 2021. 12\n10\n[51] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Bar-\nriuso, and Antonio Torralba. Scene parsing through ade20k\ndataset. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, pages 633–641, 2017. 7, 12\n[52] Daquan Zhou, Bingyi Kang, Xiaojie Jin, Linjie Yang, Xi-\naochen Lian, Zihang Jiang, Qibin Hou, and Jiashi Feng.\nDeepvit: Towards deeper vision transformer. arXiv preprint\narXiv:2103.11886, 2021. 3\n[53] Mingjian Zhu, Kai Han, Yehui Tang, and Yunhe Wang. Visual\ntransformer pruning. arXiv preprint arXiv:2104.08500, 2021.\n3\n11\nA. Pruning and training details\nA.1. Training hyperparameters\nIn our experiments, we use the same data preprocess-\ning, data augmentation, optimizer setup, and learning rate\nscheduling scheme as mentioned in Table 9 of the DeiT\npaper [35], unless otherwise mentioned in the following\nsections.\nA.1.1 Pruning and ﬁnetuning\nFor pruning and ﬁnetuning we use the training objective\nL= αLfull + LCNN to update the model. We set the balanc-\ning factor α= 1 ·105 and full model distillation temperature\nτ = 20 . For our results reported in Tab. 3 without CNN\ndistillation, we set τ = 3 for the full model distillation ob-\njective. The pruning process is performed starting from the\npretrained DeiT-Base model, with a ﬁxed learning rate of\n0.0002×batchsize\n512 . We perform the pruning experiments on\nthe cluster of four NVIDIA V100 32G GPUs, with a batch\nsize of 128 on each GPU. We prune the model continuously\nuntil a targeted latency is reached, which is discussed in de-\ntail in Appendix A.2. Followed by the iterative pruning we\nremove the pruned away dimensions of the pruned model to\nturn it into a small and dense model, and continue to ﬁnetune\nthe small model to further recover accuracy. Entire ﬁnetun-\ning is performed for 300 epochs with an initial learning rate\nof 0.0002 ×batchsize\n512 , cosine learning rate scheduling and\nno learning rate warm up. The ﬁnetuning is performed on a\ncluster of 32 NVIDIA V100 32G GPUs, with a batch size of\n144 on each GPU.\nA.1.2 Downstream tasks transfer learning\nTable 8. Datasets used for downstream task experiments.\nDataset Train size Test size # Classes\nCIFAR-10 [21] 50,000 10,000 10\nCIFAR-100 [21] 50,000 10,000 100\niNaturalist 2018 [37] 437,513 24,426 8,142\niNaturalist 2019 [37] 265,240 3,003 1,010\nThe details of the classiﬁcation datasets used for our\ndownstream task transfer learning experiments are provided\nin Tab. 8. Similar to the experiment setting of DeiT [35], for\ndownstream task experiments we rescale all the images to\n224 ×224 to ensure we have the same augmentation as the\nImageNet training. All models are trained for 300 epochs\nwith a initial learning rate of 0.0005 ×batchsize\n512 , cosine\nlearning rate scheduling and 5 epochs of learning rate warm\nup. We use batch size 512 for CIFAR-10 and CIFAR-100\nmodels, and batch size 1024 for iNaturalist models.\nFor Semantic Segmentation, previous work SETR [50]\nprovides an effective downstream model architecture and\ntraining pipeline to use ViT models as the backbone model\nof semantic segmentation tasks 2. In our experiments we\nsubstitute the backbone model with the DeiT/NViT models\npretrained on ImageNet. We keep all other downstream archi-\ntectures and training conﬁgurations unchanged. We evaluate\nthe models on the Cityscape dataset [8] and the ADE20K\ndataset [51]. For the Cityscape dataset, we follow the\n“SETR_Naive_DeiT_768x768_40k_cityscapes_bs_8” con-\nﬁguration and train on 4 GPUs. For the ADE20K dataset,\nwe follow the “SETR_PUP_DeiT_512x512_160k_ade20k\n_bs_16” conﬁguration and train on 2 GPUs.\nA.1.3 ReViT experiments\nFor the experiments on ReViT models we use the CNN hard\ndistillation objective as in Eq. (8) as the training objective\nfor all the models. We train Each pair of comparable DeiT\nand ReViT models with the same set of hyperparameters.\nIn all experiments, we train the model from scratch for 300\nepochs with an initial learning rate of 0.0005 ×batchsize\n512 ,\ncosine learning rate scheduling and 5 epochs of learning rate\nwarm up. The models are trained on a cluster of 16 V100\n32G GPUs, with a batch size of 48 on each GPU for base\nmodels and a batch size of 144 on each GPU for small and\ntiny models.\nA.2. Pruning conﬁguration\nWe use DeiT-Base model with CNN distilla-\ntion as the starting point of our pruning process,\nwhose pretrained model is available at https :\n/ / dl . fbaipublicfiles . com / deit / deit _\nbase_distilled_patch16_224-df68dfff.pth .\nWe prune the model in an iterative manner: We compute\nthe moving average of the latency-aware importance score\nIL\nS for all unpruned dimension groups in each training\nstep of the pruned model. Every 100 steps, we remove a\ngroup of dimensions that has the minimum total importance.\nRemoved dimensions will never be reactivated. We prune\nEMB and MLP in a group size of 16, QK and V in a group\nsize of 8, and H in a group size of 2, so that the input and\noutput dimensions of all the linear projection operations\nin the model can be divided by 16, thus satisfying the\ndimension requirement of the Ampere GPU.\nThe pruning process will terminate once the estimated\nlatency of the model reaches a targeted speedup ratio over\nthat of the DeiT-base model. The pruned model will then\nbe converted into a small dense model and ﬁnetuned to\nfurther restore the accuracy. The pseudo code of our pruning\nalgorithm is provided in Algorithm 1\n2Code publicly available at https://github.com/fudan-zvg/\nSETR.\n12\nTable 9. Pruning conﬁgurations and remained dimensions for models reported in Table 1. The reported dimensions are averaged across all\nthe blocks.\nAvg. dim remained\nModel Target speedup Pruning steps EMB H QK V MLP\nDeiT-B N/A 0 768 12 64 64 3072\nNViT-B 1.85× 480 496 8.00 35.33 58.67 1917.3\nNViT-H 2.00× 524 480 7.33 32.67 56.67 1816.0\nNViT-S 2.56× 642 400 5.83 24.00 47.33 1557.3\nNViT-T 5.26× 908 224 3.17 14.67 34.00 930.67\nAlgorithm 1 Hessian-based latency-aware pruning.\n1: # Initialization and preparation\n2: Load pretrained DeiT-B model\n3: Proﬁle latency lookup table as in Appedix A.3\n4: # Iterative pruning\n5: while Estimated latency > target do\n6: for (X, Y) in Train_Loader do\n7: for All prunable structural group S do\n8: Compute IS with (X, Y) following Equation (6)\n9: Estimate latency improvement for pruning S\n10: Compute IL\nS following Equation (7)\n11: Remove the structural group with minS IL\nS\n12: Estimate pruned model latency\n13: Gradient descent on remaining weights\n14: # Finetuning\n15: Finetune pruned model\nTab. 9 reports the target speedup ratio we use to achieve\nNViT-B, NViT-H, NViT-S and NViT-T architectures reported\nin Tab. 1. The resulted number of pruning steps and the\naveraged dimension of EMB, H, QK, V and MLP among all\nthe transformer blocks are also provided.\nA.3. Latency lookup table proﬁling detail\nWe use a latency lookup table to efﬁciently evaluate the\nlatency of the pruned model given all its EMB, H, QK, V and\nMLP dimensions. We initialize the lookup table by proﬁling\nthe latency of a single vision transformer block on a V100\nGPU with batch size 576. We evaluate the latency through a\ngrid of:\n• EMB: 0, 256, 512, 768 (latency assigned as 0 at zero\nEMB);\n• H: 1, 3, 6, 9, 12;\n• QK: 1, 16, 32, 48, 64;\n• V: 1, 16, 32, 48, 64;\n• MLP: 1, and 128 to 3072 with interval 128;\nresulting into 9375 conﬁgurations in total. We run each con-\nﬁguration for 100 times and record the median latency value\nin the lookup table. For a block with arbitrary dimensions, its\nlatency is estimated via a linear interpolation of the lookup\ntable, which we implement with the RegularGridInterpola-\ntor function from SciPy [39, 42]. The estimated latency of\nthe entire model is computed as the sum of the estimated\nlatency of all the blocks, while omitting the latency of the\nﬁrst projection layer and the ﬁnal classiﬁcation FC layer.\nFigure 5. Estimated latency from the lookup table vs. evalu-\nated latency on V100 GPU with batch size 256. Reduction\nratio computed with respect to the latency of the full model.\nTo show the usefulness of the lookup table, we compare\nthe estimated and evaluated latency of different model archi-\ntectures in Fig. 5. Each point represent the model achieved\nfrom a pruning step towards NViT-T conﬁguration (See Ap-\npendix A.2). The estimated latency and evaluated latency\nof ViT demonstrate strong linear relationship throughout\nthe pruning process, with R2 = 0.99864. This enables us\nto accurately estimate the latency improvement brought by\nremoving each group of dimensions, and to use the estimated\nspeedup of the pruned model as the stopping criteria of the\npruning process.\n13\nB. Additional ablation studies\nB.1. Training objective\nAs discussed in Sec. 3.3, we propose to use a combina-\ntion of full model distillation and CNN hard distillation as\nthe ﬁnal objective of our pruning and ﬁnetuning process.\nHere we ablate the validity of this choice and compare the\nﬁnetuning performance achieved with removing one or both\ndistillation loss from the objective. Speciﬁcally, we consider\nthe following 4 objectives:\n• Proposed objective: L= αLfull + LCNN;\n• CNN distillation only: LCNN as in Eq. (8);\n• Full model distillation with cross-entropy: Lfull +\nLCE\n(\nΨ\n(\nzs\nc+zs\nd\n2\n)\n,Y\n)\n;\n• Cross-entropy only: LCE\n(\nΨ\n(\nzs\nc+zs\nd\n2\n)\n,Y\n)\n.\nWe use each of the 4 objectives to ﬁnetune the pruned\nmodel achieved with NViT-T conﬁguration, and report the\nﬁnal Top-1 accuracy in Tab. 10. The ﬁnetuning is performed\nfor 50 epochs, with all other hyperparameters set the same\nas described in Appendix A.1.1. The proposed objective\nachieves the best accuracy.\nTable 10. NVP-T model ﬁnetuning accuracy with different objec-\ntives.\nObjective Proposed CNN Full model CE only\nTop-1 Acc. 73.55 73.40 72.62 72.36\nB.2. Pruning individual components\nIn this section we show the result of pruning EMB, MLP,\nQK and V component individually. The pruning procedure\nand objective are almost the same as described in Sec. 3.2,\nother than here we only enable the importance computation\nand neuron removal on a single component. The pruning\ninterval of EMB, MLP, QK and V are set to 1000, 50, 200\nand 200 respectively, in order to allow the model to be up-\ndated for similar amount of steps when pruning different\ncomponents to the same percentage. 32 neurons are pruned\nfor each pruning step. We stop the pruning process and ﬁne-\ntune the model for 50 epochs after the targeted pruned away\npercentage is reached.\nThe compression rate and accuracy achieved by pruning\neach component are discussed in Tab. 11. Under similar\npruned away ratio, we can observe that pruning EMB leads\nto the most signiﬁcant compression on the parameter and\nFLOPs count, as well as the largest drop in accuracy. This\nimplies that the embedding dimension leads to the most\neffective exploration on the compression-accuracy tradeoff,\nwhich motivates us to use EMB as the key driving factor in\nanalyzing the parameter redistribution in Sec. 5.1.\nTable 11. Iterative pruning single component to targeted percentage.\nComponentPruned awayPara (×) FLOPs (×) Top-1 Accuracy\nBase 0% 1 1\nEMB 50% 1.98 1.92 79.24\nMLP 50% 1.49 1.47 82.13\nQK 50% 1.09 1.10 82.98\nV 50% 1.09 1.10 82.63\nEMB 70% 2.95 2.77 73.15\nMLP 75% 1.97 1.91 80.29\nQK 75% 1.14 1.16 82.64\nV 75% 1.14 1.16 81.51\nB.3. Effectiveness of head alignment\nWe also illustrate the beneﬁt of head alignment, where\nwe explicitly single out the head dimension and align the\ndimensions of each head in structure pruning. We show the\ntradeoff curve between latency reduction and the accuracy\nachieved with or without explicit head alignment in Fig. 6.\nFor models pruned without head alignment, we estimate\ntheir latency as if all heads are padded with zeros to have\nthe same QK and V dimensions during inference. Under the\nsame latency target, the accuracy achieved with our proposed\nhead-aligned pruning scheme consistently outperforms that\nof without head alignment, with up to 0.3% accuracy gain.\nFigure 6. Comparing the parameter reduction-accuracy trade-\noff and latency reduction-accuracy tradeoff of different prun-\ning schemes. Latency estimated on RTX 2080 GPU. Model\nsize compression rate and latency reduction rate are com-\nputed based on that of the DeiT-Base model respectively.\nB.4. Effectiveness of Hessian importance score\nIn our pruning method we claim that utilizing a Hessian-\nbased importance score is the key factor to allow global\nstructural pruning in the ViT models. Here we perform per-\nform an ablation study on pruning with the magnitude-based\ncriteria, where the group with the smallest L2 norm will be\npruned in each step. We prune the model to match the la-\ntency of DeiT-S, and compare with our NViT-S performance.\n14\nAll the other hyperparameters are set the same. Results are\nshown in Tab. 12. It can be seen that magnitude-based prun-\nTable 12. Comparing magnitude-based pruning vs proposed NVP.\nThe pruned model accuracy before ﬁnetuning is reported.\nMethod Pruning steps Para (×) FLOPs (×) Top-1 Accuracy\nMagnitude 968 4.14 4.26 33.79\nNViT-S 642 4.18 4.24 76.59\ning struggles to reach the latency target with a larger number\nof steps, while the pruned model accuracy is much worse.\nLooking at the remained dimension of the magnitude-based\npruning unveils that most of the structural components are\neither unpruned or all pruned away, which infers magnitude-\nbased criteria is incomparable across different structural\ncomponents and different layers, thus unsuitable for global\npruning.\nB.5. Correlation between Hessian importance score\nand real loss difference\nFigure 7. Hessian importance score vs. squared loss differ-\nence.\nIn this section we verify the theoretical result derived\nin Sec. 3.2.1, on estimating the loss difference induced by\npruning with the proposed Hessian importance score. We\nevaluate the squared model loss increase for performing a\nsingle structural pruning step on different structural compo-\nnents of the DeiT-B model, and plot it with the corresponding\nimportance score computed for the pruned structure follow-\ning the derivation in Eq. (6). All the loss differences and\nHessian importance score are estimated on the same batch of\n64 training images. As shown in Fig. 7, we observe strong\npositive correlation between the estimated sensitivity and the\nreal loss difference.\nB.6. Effectiveness of latency-aware regularization\nIn Tab. 13 we show the result of pruning without latency\nregularization, i.e. set η = 0 in the importance score for-\nmulated in Eq. (7), and compare with our NViT results.\nBoth models are pruned to match DeiT-S latency. We can\nsee from the result that pruning with latency-aware regular-\nization can help reaching the target latency quicker, while\nachieving higher accuracy under the latency budget. To bet-\nter understand the difference in the achieved architecture, we\nalso show the average dimension across all the blocks after\npruning. It can be seen that model pruned with latency regu-\nlarization tends to have more dimensions on MLP and less\non MSA (QK and V), which is in line with our observation\nmade in Sec. 5.1 on designing more efﬁcient ViT architec-\nture, where reducing dimensions related to the attention (H,\nQK, V) while increasing MLP dimension may lead to more\naccurate model under similar latency.\nB.7. Performance on low-end GPUs\nAs one of the main motivation for pruning is to enable\nmodel deployment on low-end devices with cheaper cost and\nlower energy consumption. To this end we further examine\nthe latency of running the pruned NViT models on NVIDIA\nJetson NANO, a commonly used low-end GPU for embed-\nded system. Here we utilize a batch size of 64 for ImageNet\ninference.\nFor base model, we note that DeiT-B cannot ﬁt into the\nmemory of the device, preventing it from being compiled\nonto the NANO device. Whereas our pruned NViT-B model\ncan run with a decent speed, reaching 83.3% Top-1 acc.\nNViT-T matches the speed of DeiT-T, and the speedup over\nNViT-B is consistent to our measurement on V100 reported\nin Tab. 1 (2.8×on NANO vs. 2.7×on V100). This further\ndemonstrates that for low-end devices NViT enables the\noriginally prohibitive high-performance model to run, while\nthe speedup achieved on high-end devices can be retained.\nC. Additional parameter redistribution analy-\nsis\nC.1. Attention head diversity\nAs we observe in Fig. 4 and mentioned in Sec. 5.1, the\npruned models tend to preservemore dimensions in the trans-\nformer blocks towards the middle layers, while having less\ndimensions towards the two ends of the model. Here We\nexplore an intuitive analysis on why this trend occurs by ob-\nserving the diversity of features captured in each transformer\nblocks. Given the attention computation serves important\nfunctionality in ViT models, here we use the diversity of the\nattention score learned by each head as an example. Speciﬁ-\ncally, we take a random batch of 100 ImageNet validation set\nimages, pass them through the pretrained DeiT-Base model\nand our NViT-B model, and record the averaged attention\nscore softmax\n(\nqhkT\nh√dh\n)\nof all the images computed in each\nhead h. We then compute the pair-wise cosine distance of\nthe attention score from each head as a measure of diversity,\nand visualize the results in Fig. 8.\nIn DeiT-B model, we can observe that in earlier blocks\nlike block 2 and later blocks like block 11, there are clear\npatches of darker blue indicating a group of heads having\n15\nTable 13. Comparing pruning results with (η =5e-4) or without (η = 0) latency-aware regularization. The pruned model accuracy before\nﬁnetuning is reported. The reported dimensions are averaged across all the blocks.\nAvg. dim remained\nη Pruning steps Para ( ×) FLOPs ( ×) Acc. EMB H QK V MLP\n0. 657 4.11 4.17 74.80 416 5.7 25.3 49.3 1510.7\n5e-4 (NViT-S) 642 4.18 4.24 76.59 400 5.8 24.0 47.3 1557.3\nFigure 8. Pair-wise cosine distance between all heads’ attention score in each transformer block. Blue indicates a smaller\ndistance while yellow indicates a larger one. The dark blue blocks in NViT-B ﬁgures corresponds to the heads being pruned\naway, which have all-zero attention scores thus zero cosine distance in between.\nattention scores similar to each other. While for blocks in the\nmiddle such as block 5-8, almost all pairs of heads appear to\nbe fairly diverse. Such difference in diversity leads to differ-\nent behavior in the pruning process, where less heads are pre-\nserved in earlier and later blocks while more are preserved in\nthe middle. Note that all remaining heads in NViT-B model\nappears to be diverse with each other, showing a more ef-\nﬁcient utilization of the model capacity. Interestingly, this\nless-more-less trend of dimensional change across different\ntransformer is not observed in previous works compress-\ning BERT model for NLP tasks [25, 26, 40]. The learning\ndynamic of ViT model leading to this trend is worth investi-\ngating in the future work.\nC.2. Parameter redistribution on SWIN\nWe have shown the effectiveness of the proposed prun-\ning method on pruning SWIN-Transformer stages. In this\nsection, we examine the effectiveness of the discovered pa-\nrameter redistribution rule of DeiT on the Swin-Transformer\nmodel. Though SWIN follows a multi-stage design that is\ndifferent from DeiT, within each stage all the transformer\nblocks have the same dimension, which gives us the poten-\ntial of exploring better dimension redistribution rules. Here\nwe take SWIN-T model, with 2-2-6-2 transformer blocks in\nstage 0-3 respectively. As the redistribution rule treats the\nﬁrst/last block and intermediate blocks differently, the rule\nmainly takes effect on stage 2 with 6 blocks. The parameter\nredistribution is performed following exactly the same Re-\nViT rule as reported in Tab. 6. Speciﬁcally, the dimensions of\neach transformer block in the redistributed SWIN-ReViT-T\nis reported in Tab. 14.\nTable 14. Redistributed SWIN-ReViT-T model Stage-2 dimensions.\nBlock 1 2 3 4 5 6\nEMB 384 384 384 384 384 384\nHead 10 4 8 8 4 10\nQK/Head 32 16 32 32 16 32\nV/Head 64 64 64 64 64 64\nMLP 1152 1152 2304 2304 1152 1152\nWe train the SWIN-ReViT-T model on ImageNet fol-\nlowing the same training scheme described in the ofﬁcial\nGitHub repo 3. The model statistics and training perfor-\nmance of the resulted SWIN-ReViT-T is compared with the\n3https://github.com/microsoft/Swin-Transformer\n16\noriginal SWIN-T in Tab. 15.\nTable 15. Comparing the efﬁciency and accuracy of SWIN-ReViT-\nT vs. SWIN-T on ImageNet. The throughput is evaluated with a\nsingle TITAN RTX GPU.\nModel Parameters FLOPs Throughput Top-1 Accuracy\nSWIN-T 29M 4.5G 546.37 img/s 81.3%\nSWIN-ReViT-T28M 4.4G 574.25img/s 81.3%\nThe redistributed SWIN-ReViT-T model achieves the\nsame Top-1 accuracy as the original model with 1.1x\nspeedup. This indicates that the redistribution rule derived\non DeiT can also be transferred to other ViT variants to\nachieve efﬁciency improvements.\nC.3. The signiﬁcance of ReViT-S performance gain\nTable 16. Repeated experiments of ReViT-S and DeiT-S training.\nModel Ckpt 1 Ckpt 2 Ckpt 3 Ckpt 4 Ckpt 5Mean STD\nDeiT-S 80.96 80.93 80.95 81.01 80.92 80.954 0.035\nReViT-S81.17 81.19 81.17 81.20 81.22 81.190 0.021\nAs we report the accuracy improvement brought by\nReViT-S over DeiT-S in Tab. 7, here we verify the signiﬁ-\ncance of this improvement via repeated experiments. Specif-\nically, we report the Top-1 accuracy of 5 checkpoints for\ntraining ReViT-S and DeiT-S from scratch on ImageNet\nin Tab. 16. Note that the averaged 0.23% Top-1 accuracy\ngain of ReViT-S over DeiT-S is 10 times the standard deriva-\ntion of repeated experiment results, showing the improve-\nment is truly signiﬁcant.\n17",
  "topic": "FLOPS",
  "concepts": [
    {
      "name": "FLOPS",
      "score": 0.7383865714073181
    },
    {
      "name": "Computer science",
      "score": 0.6941704750061035
    },
    {
      "name": "Speedup",
      "score": 0.6371203064918518
    },
    {
      "name": "Transformer",
      "score": 0.606031060218811
    },
    {
      "name": "Inference",
      "score": 0.5821517109870911
    },
    {
      "name": "Latency (audio)",
      "score": 0.4218013882637024
    },
    {
      "name": "Computer engineering",
      "score": 0.3798336386680603
    },
    {
      "name": "Parallel computing",
      "score": 0.3621431589126587
    },
    {
      "name": "Algorithm",
      "score": 0.3556253910064697
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3425973057746887
    },
    {
      "name": "Engineering",
      "score": 0.13018903136253357
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    }
  ]
}