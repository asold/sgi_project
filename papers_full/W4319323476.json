{
  "title": "Bioformer: an efficient transformer language model for biomedical text mining",
  "url": "https://openalex.org/W4319323476",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2098424515",
      "name": "Fang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2352195443",
      "name": "Chen Qingyu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4223632283",
      "name": "Wei, Chih-Hsuan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2184280727",
      "name": "Lu zhiyong",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1902378672",
      "name": "Wang Kai",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2154142897",
    "https://openalex.org/W2149369282",
    "https://openalex.org/W2135192531",
    "https://openalex.org/W2112543617",
    "https://openalex.org/W2346452181",
    "https://openalex.org/W2052217781",
    "https://openalex.org/W2946690328",
    "https://openalex.org/W1981208470",
    "https://openalex.org/W2100627415",
    "https://openalex.org/W2071879021",
    "https://openalex.org/W3161987470",
    "https://openalex.org/W4224325974",
    "https://openalex.org/W2944400536",
    "https://openalex.org/W2174775663",
    "https://openalex.org/W2136437513",
    "https://openalex.org/W3017463390",
    "https://openalex.org/W2997522493",
    "https://openalex.org/W4200445293",
    "https://openalex.org/W2993873509",
    "https://openalex.org/W2169099542",
    "https://openalex.org/W2911489562"
  ],
  "abstract": "Pretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art performance in natural language processing (NLP) tasks. Recently, BERT has been adapted to the biomedical domain. Despite the effectiveness, these models have hundreds of millions of parameters and are computationally expensive when applied to large-scale NLP applications. We hypothesized that the number of parameters of the original BERT can be dramatically reduced with minor impact on performance. In this study, we present Bioformer, a compact BERT model for biomedical text mining. We pretrained two Bioformer models (named Bioformer8L and Bioformer16L) which reduced the model size by 60% compared to BERTBase. Bioformer uses a biomedical vocabulary and was pre-trained from scratch on PubMed abstracts and PubMed Central full-text articles. We thoroughly evaluated the performance of Bioformer as well as existing biomedical BERT models including BioBERT and PubMedBERT on 15 benchmark datasets of four different biomedical NLP tasks: named entity recognition, relation extraction, question answering and document classification. The results show that with 60% fewer parameters, Bioformer16L is only 0.1% less accurate than PubMedBERT while Bioformer8L is 0.9% less accurate than PubMedBERT. Both Bioformer16L and Bioformer8L outperformed BioBERTBase-v1.1. In addition, Bioformer16L and Bioformer8L are 2-3 fold as fast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been successfully deployed to PubTator Central providing gene annotations over 35 million PubMed abstracts and 5 million PubMed Central full-text articles. We make Bioformer publicly available via https://github.com/WGLab/bioformer, including pre-trained models, datasets, and instructions for downstream use.",
  "full_text": "Bioformer: an efficient transformer language model for biomedical text mining \n \nLi Fang1,2†, Qingyu Chen3,†, Chih-Hsuan Wei3, Zhiyong Lu3, and Kai Wang2,4* \n \n1. Department of Genetics and Biomedical Informatics, Zhongshan School of Medicine, Sun Yat-sen \nUniversity, Guangzhou, 510080, China \n2. Raymond G. Perelman Center for Cellular and Molecular Therapeutics, Children’s Hospital of \nPhiladelphia, Philadelphia, PA 19104, USA \n3. National Center for Biotechnology Information (NCBI), National Library of Medicine (NLM), National \nInstitutes of Health (NIH), Bethesda, MD 20892, USA \n4. Department of Pathology and Laboratory Medicine, University of Pennsylvania Perelman School of \nMedicine, Philadelphia, PA 19104, USA \n† These authors contributed equally: Li Fang and Qingyu Chen \n* Correspondence: wangk@chop.edu (K.W.) \n \n  \nAbstract \nPretrained language models such as Bidirectional Encoder Representations from Transformers (BERT) \nhave achieved state -of-the-art performance in natural language processing (NLP) tasks . Recently, BERT \nhas been adapted to the biomedical domain. Despite the effectiveness, these models have hundreds of \nmillions of parameters and are computationally expensive when applied to large-scale NLP applications. \nWe hypothesized that the number of parameters of the original BERT can be dramatically reduced with \nminor impact on performance. In this study, we present Bioformer, a compact BERT model for biomedical \ntext mining. We pretrained two Bioformer models (named Bioformer8L and Bioformer16L) which reduced \nthe model size by 60% compared to BERTBase. Bioformer uses a biomedical vocabulary and was pre-trained \nfrom scratch on  PubMed abstracts and PubMed Central full -text articles. We thoroughly evaluated the \nperformance of Bioformer as well as existing biomedical BERT models including BioBERT and PubMedBERT \non 1 5 benchmark datasets of four different biomedical NLP tasks: named entity recognition, relation \nextraction, question answering and document classification.  The results show that with 60% fewer \nparameters, Bioformer16L is only 0.1% less accurate than  PubMedBERT while Bioformer8L is 0.9% less \naccurate than PubMedBERT. Both Bioformer16L and Bioformer8L outperformed BioBERTBase-v1.1. In addition, \nBioformer16L and Bioformer8L are 2-3 fold as fast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been \nsuccessfully deployed to PubTator Central providing gene annotations over 35 million PubMed abstracts \nand 5 million PubMed Central full -text articles . We make Bioformer publicly available via  \nhttps://github.com/WGLab/bioformer, including pre -trained models, datasets, and instructions for \ndownstream use. \n \nIntroduction \nThe volume of published biomedical literature is increasing rapidly over the past few years. For instance, \nPubMed has more than 3 5 million articles, and this number  is growing by five thousand per day. In \naddition, there are 8.7 million freefull-text articles available in PubMed Central (PMC) and preprint servers, \nsuch as bioRxiv and medRxiv. Such rapid growth challenges knowledge discovery and literature curation. \nBiomedical Natural Language Processing (BioNLP) has been applied to help decrease such burdens. BioNLP \nlanguage models–capturing semantic representations over biomedical literature – are the foundations for \nBioNLP method development and downstream applicati ons. Early BioNLP language models include \nbiomedical word embeddings [1, 2], entity embeddings [3, 4], and sentence embeddings [5] have shown \neffectiveness in a range of BioNLP applications such as biomedical named entity recognition, relation \nextraction, and information retrieval [6, 7].  \n \nRecently, pretrained transformer language models , such as Bidirectional Encoder Representations from \nTransformers (BERT) [8], have led to impressive performance gains over word /sentence embeddings, \nconvolutional neural networks (CNNs) and recurrent neural networks (RNNs). BERT uses the transformer \narchitecture [9] and it is pretrained by self -supervised learning. It has been adapted to the biomedical \ndomain: pretrained BERT models on biomedical literature or clinical text, such as BioBERT [10], BlueBERT \n[11], PubMedBERT [12] and BioClinicalBERT(https://huggingface.co/emilyalsentzer/Bio_ClinicalBERT)[13], \nhave been released to the public.  While these BERT models could improve  the effectiveness  of \ndownstream BioNLP studies , they have hundreds of  millions of parameters  that are computationally \nexpensive. For instance, existing studies have shown BioNLP BERT  models are 50 times slower than \nbiomedical sentence embeddings for retrieving relevant sentences via biomedical literature [14]. In \naddition, a full BioNLP pipeline usually includes multiple subtasks and therefore requ ires running BERT \nmodels multiple times. For example, a pipeline to extract disease-casual genes from biomedical literature \nneeds first to identify named entities of genes and diseases. Next, entity normalization needs to be  \nperformed to link the gene and disease mentions to unique identifiers. After that, a relation extraction  \nmodel is needed to classify gene -disease relations. This pipeline runs BERT models five times if each  \nsubtask uses its own fine-tuned model, not to mention it may need to be applied to millions of biomedical \nliteratures. Therefore, it is the bottleneck for BioNLP  studies to apply transformer -related language \nmodels and deploy them in real -world applications for biomedical researchers and healthcare \nprofessionals. \n \nThe existing BioNLP transformer models directly apply the BERTBase or BERTLarge architecture with 110-340 \nmillion parameters. To date, there is no BioNLP transformer on improving efficiency. To speed up large-\nscale or real-time biomedical text mining tasks, we aim to train a compact model with faster speed while \nmaintaining high accuracy. In the general d omain, recent studies [15] showed that simply pretraining \nlonger and over more  data significantly improved the performance. Previous studies [16] also showed \nthat given a fixed model size, model depth (number of layers) and width (hidden embedding size) have a \nlarge effect on performance. Therefore, we hypothesize that a well pretrained compact  model with \noptimal model depth and width could achieve faster speed with minimal loss in accuracy.  \n \nIn this study, we present Bioformer, a compact BERT model for biomedical text mining. Bioformer has two \nvariants: Bioformer8L and Bioformer16L with different model depth and width. Bioformer uses a biomedical \nvocabulary and is pretrained from scratch on 33 million PubMed abstracts and 1 million PubMed Central \n(PMC) full-text articles. We compared its efficiency with existing biomedical BERT models such as BioBERT \nand PubMedBERT. We thoroughly evaluated  the effectiveness of Bioformer over 15 datasets from four  \nprimary BioNLP tasks: named entity recognition, relation extraction, question answering and document \nclassification. Our results show that with 40% of the parameters,  Bioformer16L is only 0.1% less accurate \nthan PubMedBERT and its overall performance is even better than BioBERTBase-v1.1. In addition, Bioformer \nmodels are 2-3-fold as fast as PubMedBERT/BioBERTBase-v1.1. Bioformer has been successfully deployed to \nPubTator Central [17], providing automatic annotations over 35 million PubMed abstracts and 5 million \nPubMed Central full-text articles.  \n \n \nMethods \nAs sh own in Table 1, Existing BioNLP BERT models directly applies BERTBase with the total number of \nparameters of 110M (L = 12, H = 768, A = 12) where L, H, and A stand for the number of layers, the hidden \nembedding size, and the number of attention heads, respectively.  We hypothesize that the hundred \nmillion of parameters in the BERT architecture are not equally effective and some might be redundant. \nInstead, we empirically investigated compact architectures by dramatically reducing the number of layers, \nthe hidden embedding size, and the number of attention heads. Specifically, we pretrained two compact \nmodels detailed in Table 1: Bioformer8L with the total number of parameters of 43M (L = 8, H = 512, A = 8) \nand Bioformer16L (L = 16, H = 1024, A = 6). They have less than 40% of parameters compared to that of the \noriginal BERTBase. Bioformer8L and Bioformer16L have almost the same number of parameters; in contrast, \nBioformer16L has more layers and a smaller embedding size (i.e., deeper and thinner). \n \nVocabulary of Bioformer  \nBioformer uses a cased WordPiece (a subword segmentation algorithm [18]) vocabulary trained from all \nPubMed abstracts (33 million, as of Feb 1, 2021) and one million subsampled PMC full-text articles. We \nsubsampled one million PMC full -text articles so that total size of PubMed abstracts and PMC full -text \narticles are approximately the same. In the training process, the vocabulary was initialized with individual \ncharacters in the corpus; then, the most frequent subwords in the corpus were iteratively added to the \nvocabulary. We trained Bioformer’s vocabulary from the Unicode text of the two resources to mitigate \nthe out-of-vocabulary issue and include special symbols (e.g., male and female symbols) in biomedical \nliterature.  \n \nPretraining Bioformer \nThe pretraining details for Bioformer is summarized in Table 1. The workflow for pretraining Bioformer is \nshown in Figure 1. Bioformer was pretrained from scratch on the same corpus as the vocabulary  in 2.1. \nThe original BERT model has two pretraining objectives:  masked language model (MLM) and next \nsentence prediction (NSP). The MLM objective is for predicting the masked tokens  (subwords). The NSP \nobjective is to predict whether two input sentences are next  to each other in the original text. For the \nMLM objective, we used whole-word masking with a masking rate of 15%. Whole-word masking requires \nthat the whole word (i.e., all the subwords) be masked if one  of its subwords is chosen \n(https://github.com/google-research/bert). This will force the model to recover the whole word using \ncontext information instead of recovering a subword, which can be inferred from the unmasked subwords \nof the same word. The random masking process was duplicated 20 times so that each sequence was \nmasked in 20 different ways. There are debates on whether the NSP objective could improve the \nperformance on downstream tasks[15]. We include it in our pretraining experiment in case the prediction \nof the  next sentence is needed by downstream tasks (e.g., for zero-shot learning [19]). Sentence \nsegmentation of all training text was performed using SciSpacy [20]. Pretraining of Bioformer was \nperformed on a single Cloud TPU device (TPUv2, 8 cores, 8GB memory per core). We pretrained Bioformer \n(both models) for 2 million steps with a batch size of 256. It took about 8.7 days to finish the pretrainin g \nof Bioformer8L and 11.3 days to finish the pretraining of Bioformer16L.  \n \nEvaluation of training and inference speed \nWe evaluated the training (for fine-tuning) and inference speed based on a sequence classification task.  \nThe sequence classification tas k adds a linear layer on top of the [CLS] token  for binary classification. \nTherefore, it has minimal changes to the BERT backbone. We evaluated the speed of four biomedical BERT \nmodels ( BioBERTBase, PubMedBERT, Bioformer8L, Bioformer16L) as well as one compact general domain \nBERT model: DistilBERT. DistilBERT was a distilled version of BERT developed by Hugging Face[15]. It has \nthe same hidden embedding size as BERTBase but with fewer number of layers ( L=6). We used the \n‘run_glue.py’ scr ipt in the Transformers library [21] to perform the benchmark. Training speed was \nassessed on an NVIDIA Tesla P100 GPU  with 16GB memory . Inference speed was assessed on an Intel \nXeon CPU. The max sequence length was set to 512. For training on GPU, t he batch size was set to t he \nmaximal possible value for each model that does not cause out-of-memory error.  \n \nEvaluation of performance on biomedical NLP tasks \nOverall, we evaluated the  performance of five biomedical BERT models: Bioformer8L, Bioformer16L, \nBioBERTBase-v1.1, PubMedBERTAbs (pretrained on PubMed Abstracts only) , and PubMedBERTAbsFull \n(pretrained on PubMed Abstracts and PMC full-text articles). To ensure fairness, all the five models were \nevaluated using the same setting . We evaluated the performance on four main biomedical NLP \napplications consisting of 15 datasets: named entity recognition (eight datasets), relation extraction (four \ndatasets), question answering (one dataset), and document classification (two datasets). The details of \nthe datasets and evaluation  metrics are summarized in Table 2. We used consistent preprocessing and \ntraining approaches as the existing studies summarized below. \n \nNamed Entity Recognition (NER)  \nNamed Entity Recognition identifies named entities mentioned in unstructured text, such as gene and \ndisease names. It is commonly formulated as a token classification problem: for a given token in the input \ntext, the model needs to determine whether it bel ongs to a specific entity type. We adopted the same \npreprocessing pipeline for NER applications in the existing studies [10, 12] . The context -ware \nrepresentation of each token from the last layer of a transformer model is used to classify the token’s \ncategory. \n \nRelation Extraction (RE)  \nRelation extraction classifies the relationships between named entities in the given text (e.g., protein -\nprotein interactions). It can be formulated as a  sequence classification problem: given a passage, it \nclassifies whether it mentions a specific relation. We followed the preprocessing method used by \nBioBERT[10] where entity names are replaced by pre -defined tags (e.g.,  gene and disease names are \nreplaced (@GENE$ and @DISEASE$, respectively). The representation of the [CLS] token in the last layer \nis used to classify the relations. \n \nQuestion Answering (QA) \nQuestion answering is a reading comprehension task that extracts the answer to a question from a given \ntext. The Stanford Question Answering Dataset (SQuAD) [22] is a large-scale QA dataset in the general \ndomain. BioASQ[23] factoid datasets are a series of QA datasets in the biomedical domain. Given a \nquestion and a passage containing the answer, the task is to predict the span of the answer. We followed \nthe same fine-tuning procedure as the original BERT[8] for QA tasks. The input question and passage are \npresented as a sentence pair where the question is the first sentence and the passage is the second \nsentence. Token-level probabilities for the star t/end location of the answer span are computed using a \nsingle layer. For BioASQ, we used the preprocessed dataset  (BioASQ-7b) provided by the developers of \nBioBERT[10] where about 30% unanswerable questions had been removed from the dataset. We used the \nsame evaluation metrics from BioASQ: strict accuracy, lenient accura cy and mean reciprocal rank.  \nPrevious studies[10, 24] showed that pretraining on the SQuAD dataset (fine-tuning on the SQuAD before \nfine-tuning on BioASQ) improved the performance on the BioASQ datasets. Therefore, we showed the \nevaluation results with or without pretraining on SQuAD.  \n \nDocument Classification (DC)  \nDocument classification categorizes an input document into one or more categories. In contrast to \nrelation extraction which focuses on the relationships between entities often at passage level, document \nclassification is often at abstract or full -text level. It has been extensively used for biomedical document \ntriage (e.g., classify whether a PubMed article is relevant for manual curation [25] and topic assignment \n(e.g., assign COVID-19-related topics such as Treatment and Diagnosis to a relevant article [26]). Same as \nthe relation extraction task, the representation of the [CLS] token in the last layer is used to classify the \ndocument. \n \nHyperparameter tuning and result reporting \nFor each task, we selected the max sequence length based on previous studies and the values were listed \nin Table 2. Consistent with the existing studies[12], we select the best batch size among 8, 16, and 32, and \nlearning rate among 1e-5, 3e-5, and 5e-5 on the development set. For each combination, we trained the \nmodel for up to 20 epochs, selected the checkpoint achieving the best performance over the development \nset, and evaluated it on the testing set. To ensure a fair comparison, we used the same preprocessing and \nhyperparameter tuning methods for all the five models. We repeated each fine-tune experiments for five \ntimes using different random seeds and reported the average performance of the evaluation metrics in \nTable 2. \n \nResults \nPretraining Bioformer and hyperparameter selection \nModel depth (number of t ransformer layers, L) and model width (hidden embedding size, H) are key \nhyperparameters that impact the performance . Turc et al.[16] released 24 pretrained miniature BERT \nmodels in the general domain. To investigate the best model depth and model width for a compact BERT \nmodel, we examined the effect of model depth and wi dth on performances of downstream tasks. We \ncompared decreasing depth versus decreasing width. We benchmarked the performances on general \ndomain NER and QA tasks (Figure 2). NER is a relatively simple task and QA is a more complicated task. \nThe performance drop for QA is more significant than that for NER. We also observed that t he \nperformance drop is higher when reducing depth (L) compared to reducing width (H). This suggest that a \ndeep-and-narrow model might perform better than a shallow-and-wide model when the model sizes are \napproximately the same. Therefore, we pretrained two Bioformer models with approximately the same \nmodel size: Bioformer8L and Bioformer16L (hyperparameters shown in Table 1) . Compared to a BERT Base \nmodel, Bioformer8L reduced both model width (from 768 to 512) and depth (from 12 to 8). Bioformer16L \nreduced model width more significantly (from 768 to 384), but increased model depth to 16.  We \nhypothesized that Bioformer16L might perform better than Bioformer8 on more complicated task such as \nQA. The procedure for pretraining Bioformer8L and Bioformer16L is described in the method section. \n \nModel structure and speed \nWe compared the speed of Bioformer and with BERTBase/BioBERTBase, PubMedBERT and DistilBERT. (Figure \n3). BioBERTBase used a continue pretraining strategy and was initialized with model weights from the \noriginal BERTBase. Therefore, BioBERTBase and BERTBase have the same speed . PubMedBERT have \napproximately the same structure as BERTBase, except that it has its own vocabulary with a slightly different \nvocabulary size. DistilBERT was a compact BERT model developed by Hugging Face[27]. DistilBERT has the \nsame hidden embedding size as BERTBase but with fewer number of layers (L=6). As a result, the model size \nwas reduced by 40%. Bioformer8L and Bioformer16L have approximately the same model size  (43M and \n42M, Figure 3C ), but Bioformer16L is a deeper model with smaller hidden embedding size.  We \nbenchmarked the training and inference speed of these models on sequence classification task which \nadded a single linear layer on top of the [CLS] token. The max sequence length was 512. The benchmarking \nresults are shown in Figure 3 . PubMedBERT and BioBERTBase has approximately the same speed. \nBioformer16L is twice as fast as BioBERTBase for training and the inference speed is 2.2-fold of BioBERTBase. \nThe training and inference speeds of Bioformer8L are 2.8-fold and 3.0-fold of BioBERTBase, respectively. The \nspeed of  Bioformer16L and DistilBERT are approximately the same  while Bioformer8L is faster than \nBioformer16L and DistilBERT. \n \nPerformance on downstream biomedical NLP tasks \nTable 3 shows the performance on four downstream biomedical NLP tasks: named entity recognition, \nrelation extraction, question answering and document classification . Overall, with only 40% of the \nparameters of the original BERTBase, Bioformer16L and Bioformer8L retain high accuracy  across the 15 \ndatasets of the four tasks. PubMedBERTAbs achieved the highest overall average performance score among \nall models , followed by Bioformer16L. Bioformer16L retains 99.92% (82.71/82.77) performance score of \nPubMedBERTAbs and its  average performance is better than PubMedBERTAbs, BioBERTBase-v1.1 and \nBioformer8L. The overall performance of Bioformer8L is a little higher than  BioBERTBase-v1.1 (average score \n82.07 versus 82.02). Bioformer16L has better performance than Bioformer8L in most (12 out of 15) datasets, \nwhich is consistent with the finding that depth is more important than width (Figure 2 and also reported \nby previous studies [16, 28] ). We also observed that Bioformer16L performed best in the QA task, \noutperformed Bioformer8L and other models by a large margin. \n \nOf note, Bioformer8L participated the BioCreative VII LitCovid multi-label topic classification challenge and \nachieved the overall best performance among all teams [29]. The details of the challenge are described \nelsewhere [29, 30]. In this study, we showed the performance of the five models on the BioCreative VII \nLitCovid challenge development set in Table 3 (as a document classification task). The performance scores \nin Table 3 are different from the BioCreative VII LitCovid challenge paper[30] because Table 3 shows the \naverage scores of five random runs while the BioCreative VII LitCovid challenge paper presented the score \nof the submitted run. Bioformer16L did not participate this challenge because Bioformer16L was pretrained \nafter the challenge was closed. We added the performance score of Bioformer16L in Table 3.  \n \nIncorporating Bioformer into PubTator for annotating genes at the full PubMed \nand PMC scale \nWe further demonstrate a real-world application by incorporating Bioformer into PubTator. PubTator [17] \n– a web-based biomedical text mining platform publicly available for over a decade – provides high-quality \nautomatic entity annotations and prioritizes target entities or relevant articles for downstream research. \nPubTator provides six types of biomedical concepts (genes, chemicals, diseases, species, variants, and cell \nlines) for the entire PubMed and PMC arti cles (to date, there are 35+ million PubMed abstracts and 5 \nmillion+ PMC full texts). \nPreviously, PubTator employed the conditional random fields (CRF) for recognizing genes [17]. This \napproach took about 100 hours to annotate genes over the entire PubMed and PMC articles in the \nproduction environment consisting of a computer cluster of 300 CPUs. Given the advances in transformers, \nwe implemented NER models using the existing BioNLP transformers as the backbone and examined the \nefficiency in the PubTator production environment. It took more than 350 hours for the existing BioNLP \ntransformers to annotate genes. As PubTator annotates seven types of named entities, it is not practical \nto extend the annotations to all the seven types at the full PubMed and PMC scale. \n \nWe used Bioformer8L instead. Under the same resources, the processing time significantly reduced to 150 \nhours, whereas the accuracy difference is within 1% [31]. The processing time is competitive with that of \nthe previous CRF method.  This demonstrates a successful use case of incorporating Bioformer into \nPubTator and bringing dramatic efficiency to large-scale data processing. \n \n \nDiscussion \n \nIn this study, we introduced Bioformer, a compact pretrained language model for biomedical text mining. \nBioformer has two variants ( Bioformer8L and Bioformer16L) with approximately the same model size but \ndifferent model widths and depths. We evaluated the effectiveness of Bioformer on 15 datasets of four \nbiomedical NLP tasks. With 40% of the parameters, both variants retain more than 99% performance score \nof PubMedBERT, the current state -of-the-art model. The efficiency evaluation results showed  that \nBioformer16L and Bioformer8L and are 2X and 3X as fast as PubMedBERT, respectively. Bioformer has been \ndeployed to PubTator Central, providing automatic annotations over 35 million PubMed abstracts and 5 \nmillion PubMed Central full-text articles.  \n \nThe effectiveness evaluation results do not suggest significant difference for the five models in terms of \naccuracy. In contrast, the efficiency evaluation results show that Bioformer significantly improved the  \ntraining and inference speed by 2-3 folds. These collectively suggest that Bioformer can be  employed in \napplications where speed is critical (e.g. large-scale data analysis or  real-time applications) while \nmaintaining high accuracy. Bioformer reduced the model size by 60% which indicates that it can be fit into \ndevices with smaller memory, or be trained with larger batch size given a fixed GPU/TPU memory. Recent \nstudies suggest that pretraining with larger batch size may improve the performance and the state-of-the-\nart models such as RoBERTa, XLNet, and PubMedBERT were pretrained with a batch size of 8192 [12, 15, \n31].  \n \nIn addition, we also comparatively analyzed Bioformer8L and Bioformer16L and found a trade-off between \neffectiveness and efficiency. Recall that both models have a very similar number of parameters but \ndifferent architectures. Bioformer8L has eight transformer layers and the hidden embedding size is 512. \nBioformer16L has 16 transformer layers and the hidden embedding size is 384. The effectiveness evaluation \nshows that Bioformer16L consistently had better performance than Bioformer8L on most (12/15) datasets, \nwhich suggests that deeper and thinner transformer models may have higher accuracy. As a trade -off, \nBioformer16L is less efficient than Bioformer8L, despite that Bioformer8L has a little more number of \nparameters (43M versus 42M). The reason might be that Bioformer8L has a larger hidden embedding size \nand large dense matrix multiplications can be well parallelized. Bioformer16L has more layers and matrix \nmultiplications in different layers cannot start simultaneously. The calculations of the next layer need the \nresults from the previous layer.  \n \nBioformer8L retains 99.6% F1 score of Bioformer16L in NER tasks. Considering the speed advantage, we \nsuggest using Bioformer8L for NER tasks if accuracy is not critical. However, Bioformer8L only retains 95.6% \nof the average performance score of Bioformer16L in question-answering tasks. Therefore, we suggest \nusing Bioformer16L for question-answering tasks considering its significant advantage. For relation \nextraction tasks, Bioformer8L retains 98.4% of the average performance score  of Bioformer16L, users can \nchoose the model based on their preference on speed/accuracy. We hope that Bioformer is a compelling \noption for large-scale text mining or real-time NLP applications.  \n \nAcknowledgment \nThis study is supported by the Google TPU Research Cloud (TRC) program (L.F.), the Intramural Research \nProgram of the National Library of Medicine  (NLM), National Institutes of Health  (NIH), and NIH/NLM \ngrants LM012895 (K.W.) and 1K99LM014024-01 (Q.C.). \n \nData and Code Availability \nPretrained model weights of Bioformer8L and Bioformer16L are publicly available on HuggingFace \n(https://huggingface.co/bioformers/bioformer-8L, https://huggingface.co/bioformers/bioformer-16L). \nThe code and instructions for downstream use are available on GitHub \n(https://github.com/WGLab/bioformer).  \n  \n \nReferences \n \n[1] B. Chiu, G. Crichton, A. Korhonen, and S. Pyysalo, \"How to train good word embeddings for \nbiomedical NLP.\" pp. 166-174. \n[2] Y. Zhang, Q. Chen, Z. Yang, H. Lin, and Z. Lu, “BioWordVec, improving biomedical word \nembeddings with subword information and MeSH,” Scientific data, vol. 6, no. 1, pp. 1-9, 2019. \n[3] Q. Chen, K. Lee, S. Yan, S. Kim, C.-H. Wei, and Z. Lu, “BioConceptVec: creating and evaluating \nliterature-based biomedical concept embeddings on a large scale,” PLoS computational biology, \nvol. 16, no. 4, pp. e1007617, 2020. \n[4] A. L. Beam, B. Kompa, A. Schmaltz, I. Fried, G. Weber, N. Palmer, X. Shi, T. Cai, and I. S. Kohane, \n\"Clinical concept embeddings learned from massive sources of multimodal medical data.\" pp. \n295-306. \n[5] Q. Chen, Y. Peng, and Z. Lu, \"BioSentVec: creating sentence embeddings for biomedical texts.\" \npp. 1-5. \n[6] S. Wu, K. Roberts, S. Datta, J. Du, Z. Ji, Y. Si, S. Soni, Q. Wang, Q. Wei, and Y. Xiang, “Deep \nlearning in clinical natural language processing: a methodical review,” Journal of the American \nMedical Informatics Association, vol. 27, no. 3, pp. 457-470, 2020. \n[7] Q. Chen, R. Leaman, A. Allot, L. Luo, C.-H. Wei, S. Yan, and Z. Lu, “Artificial Intelligence (AI) in \nAction: Addressing the COVID-19 Pandemic with Natural Language Processing (NLP),” arXiv \npreprint arXiv:2010.16413, 2020. \n[8] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training of deep bidirectional \ntransformers for language understanding,” arXiv preprint arXiv:1810.04805, 2018. \n[9] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. \nPolosukhin, \"Attention is all you need.\" pp. 5998-6008. \n[10] J. Lee, W. Yoon, S. Kim, D. Kim, S. Kim, C. H. So, and J. Kang, “BioBERT: a pre-trained biomedical \nlanguage representation model for biomedical text mining,” Bioinformatics, vol. 36, no. 4, pp. \n1234-1240, 2020. \n[11] Y. Peng, S. Yan, and Z. Lu, “Transfer learning in biomedical natural language processing: an \nevaluation of BERT and ELMo on ten benchmarking datasets,” arXiv preprint arXiv:1906.05474, \n2019. \n[12] Y. Gu, R. Tinn, H. Cheng, M. Lucas, N. Usuyama, X. Liu, T. Naumann, J. Gao, and H. Poon, \n“Domain-specific language model pretraining for biomedical natural language processing,” arXiv \npreprint arXiv:2007.15779, 2020. \n[13] E. Alsentzer, J. R. Murphy, W. Boag, W.-H. Weng, D. Jin, T. Naumann, and M. B. A. McDermott, \n\"Publicly Available Clinical BERT Embeddings,\" \nhttps://ui.adsabs.harvard.edu/abs/2019arXiv190403323A, [April 01, 2019, 2019]. \n[14] Q. Chen, A. Rankine, Y. Peng, E. Aghaarabi, and Z. Lu, “Benchmarking Effectiveness and \nEfficiency of Deep Learning Models for Semantic Textual Similarity in the Clinical Domain: \nValidation Study,” JMIR Medical Informatics, vol. 9, no. 12, pp. e27386, 2021. \n[15] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, and V. \nStoyanov, \"RoBERTa: A Robustly Optimized BERT Pretraining Approach,\" \nhttps://ui.adsabs.harvard.edu/abs/2019arXiv190711692L, [July 01, 2019, 2019]. \n[16] I. Turc, M.-W. Chang, K. Lee, and K. Toutanova, \"Well-Read Students Learn Better: On the \nImportance of Pre-training Compact Models,\" \nhttps://ui.adsabs.harvard.edu/abs/2019arXiv190808962T, [August 01, 2019, 2019]. \n[17] C.-H. Wei, A. Allot, R. Leaman, and Z. Lu, “PubTator central: automated concept annotation for \nbiomedical full text articles,” Nucleic acids research, vol. 47, no. W1, pp. W587-W593, 2019. \n[18] M. Schuster, and K. Nakajima, \"Japanese and korean voice search.\" pp. 5149-5152. \n[19] Y. Sun, Y. Zheng, C. Hao, and H. Qiu, “NSP-BERT: A Prompt-based Zero-Shot Learner Through an \nOriginal Pre-training Task--Next Sentence Prediction,” arXiv preprint arXiv:2109.03564, 2021. \n[20] M. Neumann, D. King, I. Beltagy, and W. Ammar, “ScispaCy: fast and robust models for \nbiomedical natural language processing,” arXiv preprint arXiv:1902.07669, 2019. \n[21] T. Wolf, L. Debut, V. Sanh, J. Chaumond, C. Delangue, A. Moi, P. Cistac, T. Rault, R. Louf, and M. \nFuntowicz, “Huggingface's transformers: State-of-the-art natural language processing,” arXiv \npreprint arXiv:1910.03771, 2019. \n[22] P. Rajpurkar, J. Zhang, K. Lopyrev, and P. Liang, \"SQuAD: 100,000+ Questions for Machine \nComprehension of Text,\" https://ui.adsabs.harvard.edu/abs/2016arXiv160605250R, [June 01, \n2016, 2016]. \n[23] G. Tsatsaronis, G. Balikas, P. Malakasiotis, I. Partalas, M. Zschunke, M. R. Alvers, D. Weissenborn, \nA. Krithara, S. Petridis, D. Polychronopoulos, Y. Almirantis, J. Pavlopoulos, N. Baskiotis, P. \nGallinari, T. Artieres, A. C. Ngomo, N. Heino, E. Gaussier, L. Barrio-Alvers, M. Schroeder, I. \nAndroutsopoulos, and G. Paliouras, “An overview of the BIOASQ large-scale biomedical semantic \nindexing and question answering competition,” BMC Bioinformatics, vol. 16, pp. 138, Apr 30, \n2015. \n[24] G. Wiese, D. Weissenborn, and M. Neves, \"Neural Domain Adaptation for Biomedical Question \nAnswering,\" https://ui.adsabs.harvard.edu/abs/2017arXiv170603610W, [June 01, 2017, 2017]. \n[25] M. Krallinger, M. Vazquez, F. Leitner, D. Salgado, A. Chatr-Aryamontri, A. Winter, L. Perfetto, L. \nBriganti, L. Licata, and M. Iannuccelli, “The Protein-Protein Interaction tasks of BioCreative III: \nclassification/ranking of articles and linking bio-ontology concepts to full text,” BMC \nbioinformatics, vol. 12, no. 8, pp. 1-31, 2011. \n[26] Q. Chen, A. Allot, R. Leaman, R. I. Doğan, and Z. Lu, \"Overview of the BioCreative VII LitCovid \nTrack: multi-label topic classification for COVID-19 literature annotation.\" \n[27] V. Sanh, L. Debut, J. Chaumond, and T. Wolf, “DistilBERT, a distilled version of BERT: smaller, \nfaster, cheaper and lighter,” arXiv preprint arXiv:1910.01108, 2019. \n[28] Z. Sun, H. Yu, X. Song, R. Liu, Y. Yang, and D. Zhou, \"MobileBERT: a Compact Task-Agnostic BERT \nfor Resource-Limited Devices,\" https://ui.adsabs.harvard.edu/abs/2020arXiv200402984S, [April \n01, 2020, 2020]. \n[29] Q. Chen, A. Allot, R. Leaman, R. Islamaj, J. Du, L. Fang, K. Wang, S. Xu, Y. Zhang, P. Bagherzadeh, \nS. Bergler, A. Bhatnagar, N. Bhavsar, Y.-C. Chang, S.-J. Lin, W. Tang, H. Zhang, I. Tavchioski, S. \nPollak, S. Tian, J. Zhang, Y. Otmakhova, A. J. Yepes, H. Dong, H. Wu, R. Dufour, Y. Labrak, N. \nChatterjee, K. Tandon, F. A. A. Laleye, L. Rakotoson, E. Chersoni, J. Gu, A. Friedrich, S. C. Pujari, \nM. Chizhikova, N. Sivadasan, S. Vg, and Z. Lu, “Multi-label classification for biomedical literature: \nan overview of the BioCreative VII LitCovid Track for COVID-19 literature topic annotations,” \nDatabase, vol. 2022, pp. baac069, 2022. \n[30] L. Fang, and K. Wang, \"Multi-label topic classification for COVID-19 literature with Bioformer,\" \nhttps://ui.adsabs.harvard.edu/abs/2022arXiv220406758F, [April 01, 2022, 2022]. \n[31] Z. Yang, Z. Dai, Y. Yang, J. Carbonell, R. Salakhutdinov, and Q. V. Le, \"XLNet: Generalized \nAutoregressive Pretraining for Language Understanding,\" \nhttps://ui.adsabs.harvard.edu/abs/2019arXiv190608237Y, [June 01, 2019, 2019]. \n[32] L. Smith, L. K. Tanabe, R. J. Ando, C. J. Kuo, I. F. Chung, C. N. Hsu, Y. S. Lin, R. Klinger, C. M. \nFriedrich, K. Ganchev, M. Torii, H. Liu, B. Haddow, C. A. Struble, R. J. Povinelli, A. Vlachos, W. A. \nBaumgartner, Jr., L. Hunter, B. Carpenter, R. T. Tsai, H. J. Dai, F. Liu, Y. Chen, C. Sun, S. Katrenko, \nP. Adriaans, C. Blaschke, R. Torres, M. Neves, P. Nakov, A. Divoli, M. Mana-Lopez, J. Mata, and \nW. J. Wilbur, “Overview of BioCreative II gene mention recognition,” Genome Biol, vol. 9 Suppl \n2, pp. S2, 2008. \n[33] M. Krallinger, O. Rabal, F. Leitner, M. Vazquez, D. Salgado, Z. Lu, R. Leaman, Y. Lu, D. Ji, D. M. \nLowe, R. A. Sayle, R. T. Batista-Navarro, R. Rak, T. Huber, T. Rocktäschel, S. Matos, D. Campos, B. \nTang, H. Xu, T. Munkhdalai, K. H. Ryu, S. V. Ramanan, S. Nathan, S. Žitnik, M. Bajec, L. Weber, M. \nIrmer, S. A. Akhondi, J. A. Kors, S. Xu, X. An, U. K. Sikdar, A. Ekbal, M. Yoshioka, T. M. Dieb, M. \nChoi, K. Verspoor, M. Khabsa, C. L. Giles, H. Liu, K. E. Ravikumar, A. Lamurias, F. M. Couto, H.-J. \nDai, R. T.-H. Tsai, C. Ata, T. Can, A. Usié, R. Alves, I. Segura-Bedmar, P. Martí nez, J. Oyarzabal, \nand A. Valencia, “The CHEMDNER corpus of chemicals and drugs and its annotation principles,” \nJournal of Cheminformatics, vol. 7, no. 1, pp. S2, 2015/01/19, 2015. \n[34] J. Li, Y. Sun, R. J. Johnson, D. Sciaky, C.-H. Wei, R. Leaman, A. P. Davis, C. J. Mattingly, T. C. \nWiegers, and Z. Lu, “BioCreative V CDR task corpus: a resource for chemical disease relation \nextraction,” Database, vol. 2016, pp. baw068, 2016. \n[35] N. Collier, and J.-D. Kim, \"Introduction to the Bio-entity Recognition Task at JNLPBA,\" \nProceedings of the International Joint Workshop on Natural Language Processing in Biomedicine \nand its Applications (NLPBA/BioNLP). pp. 73-78. \n[36] M. Gerner, G. Nenadic, and C. M. Bergman, “LINNAEUS: A species name identification system for \nbiomedical literature,” BMC Bioinformatics, vol. 11, no. 1, pp. 85, 2010/02/11, 2010. \n[37] R. I. Doğan, R. Leaman, and Z. Lu, “NCBI disease corpus: A resource for disease name recognition \nand concept normalization,” Journal of Biomedical Informatics, vol. 47, pp. 1-10, 2014/02/01/, \n2014. \n[38] E. Pafilis, S. P. Frankild, L. Fanini, S. Faulwetter, C. Pavloudi, A. Vasileiadou, C. Arvanitidis, and L. \nJ. Jensen, “The SPECIES and ORGANISMS Resources for Fast and Accurate Identification of \nTaxonomic Names in Text,” PLoS ONE, vol. 8, no. 6, pp. e65390, 2013. \n[39] M. Krallinger, O. Rabal, S. A. Akhondi, M. Pérez-Pérez, J. Santamarí a, G. Pérez-Rodrí guez, G. \nTsatsaronis, A. Intxaurrondo, J. A. López, U. Nandal, E. van Buel, A. Chandrasekhar, M. \nRodenburg, A. Laegreid, M. Doornenbal, J. Oyarzabal, A. Lourenço, and A. Valencia, \"Overview of \nthe BioCreative VI chemical-protein interaction Track.\" pp. 141–146. \n[40] I. Segura-Bedmar, P. Martí nez, and M. Herrero-Zazo, “Lessons learnt from the DDIExtraction-\n2013 Shared Task,” Journal of Biomedical Informatics, vol. 51, pp. 152-164, 2014/10/01/, 2014. \n[41] E. M. van Mulligen, A. Fourrier-Reglat, D. Gurwitz, M. Molokhia, A. Nieto, G. Trifiro, J. A. Kors, \nand L. I. Furlong, “The EU-ADR corpus: Annotated drugs, diseases, targets, and their \nrelationships,” Journal of Biomedical Informatics, vol. 45, no. 5, pp. 879-884, 2012/10/01/, 2012. \n[42] À. Bravo, J. Piñero, N. Queralt-Rosinach, M. Rautschka, and L. I. Furlong, “Extraction of relations \nbetween genes and diseases from text and large-scale data analysis: implications for \ntranslational research,” BMC Bioinformatics, vol. 16, no. 1, pp. 55, 2015/02/21, 2015. \n[43] S. Baker, I. Silins, Y. Guo, I. Ali, J. Högberg, U. Stenius, and A. Korhonen, “Automatic semantic \nclassification of scientific literature according to the hallmarks of cancer,” Bioinformatics, vol. \n32, no. 3, pp. 432-440, 2016. \n \n \n  \nFigures \n \n \nFigure 1. Workflow for pretraining Bioformer. We first trained a WordPiece vocabulary using all PubMed \nabstracts (33 million, as of Feb 1, 2021) and one million subsampled PMC full -text articles. The PMC full-\ntext articles were subsampled to one million such that the total size of PubMed ab stracts and PMC full-\ntext articles are approximately the same. Bioformer models were pretrained from scratch using the same \ntext for training the vocabulary. Same as the original BERT model, there were two pretraining objectives: \nmasked language modeling (MLM) and next sentence prediction (NSP). For the MLM objective, we used \nwhole-word masking with a masking rate of 15%. The random masking process was duplicated 20 times \nso that each sequence was randomly masked in 20 different ways. Sentence segmentation of all training \ntext was performed using SciSpacy  [20]. Pretraining of Bioformer was performed on a single Cloud TPU \ndevice (TPUv2, 8 cores, 8GB memory per core) with a max sequence length of 512 and a batch size of 256.  \n \n\n \nFigure 2. Effect of model width (hidden embedding size, H) and depth (number of transformer layers, L) \non performances of downstream tasks . Two model compression methods  were assessed: decreasing \ndepth (keep width) or decreasing width (keep depth). We benchmarked the performances on NER (A) and \nquestion answering (B). The performance drop is higher when reducing depth  (L) compared to reducing \nwidth (H). On the other words, a deep-and-narrow model performs better than a shallow-and-wide model \nwhen the model sizes are approximately the same. The compressed models used in this figure were \ngeneral domain BERT models released by a previous study [16]. The datasets for benchmarking NER and \nQA performance are CoNLL2003 and SQuAD1.1, respectively. \n  \n\n \n \nFigure 3. Comparison of hyperparameters and speed. Hidden embedding sizes, numbers of layers and \nnumbers of parameters of five BERT models are shown in (A), (B) and (C). Relative speedup (with respect \nto BioBERTBase/BERTBase) is shown in (D). Training and inference speed are calculated based on a \nsequence classification task which adds a single linear layer on top of the [CLS] token of the BERT model. \nTraining speed was assessed on an NVIDIA Tesla P100 GPU. Inference speed was assessed on an Intel \nXeon CPU. \n  \n\nTables \n \nTable 1. Comparison of Bioformer and other biomedical BERT models. \n Bioformer8L Bioformer16L BioBERTBase-v1.1 PubMedBERT \nNumber of parameters 43M 42M 110M 110M \nNumber of layers 8 16 12 12 \nHidden embedding size 512 384 768 768 \nNumber of attention heads 8 6 12 12 \nVocabulary size 32,768 32,768 28,996 30,522 \nMax input sequence length 512 1,024 512 512 \nPretraining strategy From scratch From scratch Continue pretraining From scratch \nBatch size 256 256 192 8,192 \nPre-trained steps 2M 2M 1M 0.62M \nPretraining device 1 TPUv2 \n(8 cores) \n1 TPUv2 \n(8 cores) 8 V100 GPUs 16 V100 GPUs \n \n \n \n  \nTable 2. Statistics of the benchmark datasets. \nDataset Train Valid Test Metric \nNamed entity recognition \nBC2GM[32] 15197 3061 6325 F1 score \nBC4CHEMD[33] 29478 29486 25346 F1 score \nBC5CDR-chem[34] 5,203 5,347 5,385 F1 score \nBC5CDR-disease[34] 4,182 4,244 4,424 F1 score \nJNLPBA[35] 32178 8575 6241 F1 score \nlinnaeus[36] 2119 711 1433 F1 score \nNCBI-disease[37] 5134 787 960 F1 score \ns800[38] 2557 384 767 F1 score \nRelation extraction \nChemprot[39] 4,154 2,416 3,458 macro-F1 \nDDI[40] 2,937 1,004 979 micro F1 \neuadr[41] 4796 0 535 F1 score \nGAD-10[42] 318 0 38 F1 score \nDocument classification \nHoC[43] 1,108 157 315 micro F1 \nBioCreative-LitCovid[29] 24,960 6,239 2,500 micro F1 \n \n \n \n \n  \nTable 3. Performance on downstream biomedical NLP tasks  \nDataset (metric) Bioformer8L Bioformer16L BioBERTBase-v1.1 PubMedBERTAbs PubMedBERTAbsFull \nNamed entity recognition (NER) \nBC2GM (F1) 83.95 84.26 84.07 84.66 84.71 \nBC4CHEMD (F1) 92.04 92.34 92.02 92.49 92.63 \nBC5CDR-chem (F1) 93.64 94.00 93.67 94.20 94.18 \nBC5CDR-disease (F1) 86.29 86.53 86.09 87.54 87.17 \nJNLPBA (F1) 76.75 77.17 76.81 77.14 77.34 \nLinnaeus (F1) 88.48 88.51 84.33 88.59 88.45 \nNCBI-disease (F1) 87.99 87.73 88.48 88.52 87.09 \nS800 (F1) 73.47 74.65 74.78 74.02 74.21 \nNER average 85.32 85.65 85.03 85.90 85.72 \nRelation extraction (RE) \nChemprot (macro F1) 76.77 79.07 77.69 80.10 80.25 \nDDI (micro F1) 81.84 84.56 82.84 84.54 83.74 \nEU-ADR (F1) 84.72 84.76 84.21 83.60 83.88 \nGAD-10 (F1) 81.31 81.40 81.94 82.65 82.65 \nRE average 81.16 82.45 81.67 82.72 82.63 \nDocument classification (DC) \nHoC (micro F1) 86.41 86.37 86.42 86.41 87.07 \nBC7-LitCovid (micro F1) 90.88 90.69 90.72 90.78 90.78 \nDC average 88.65 88.53 88.57 88.60 88.93 \nQuestion Answering (pretrained on SQuAD 1.1)  \nBioASQ-7b (S. Accuracy) 40.87 43.09 42.96 43.01 42.98 \nBioASQ-7b (L. Accuracy) 59.75 60.49 58.64 58.66 58.65 \nBioASQ-7b (MRR) 48.08 49.77 49.28 49.29 49.30 \nQuestion Answering (without pretraining) \nBioASQ-7b (S. Accuracy) 34.94 37.53 35.68 35.72 35.71 \nBioASQ-7b (L. Accuracy) 53.83 56.42 49.88 49.91 49.89 \nBioASQ-7b (MRR) 41.61 44.38 41.09 41.13 41.11 \nQA average 46.51 48.61 46.26 46.29 46.27 \nOverall average 82.07 82.71 82.02 82.77 82.69 \nNotes: 1) BC7-LitCovid denotes BioCreative VII LitCovid challenge and the results are development set performance. \n2) S. Accuracy: strict accuracy; L. Accuracy: lenient accuracy; MRR: mean reciprocal rank. 3) QA average is the average \nscore of the three metrics of the two methods (with/without pretraining on SQuAD 1.1). 4) Overall average = (NER \naverage * 8 + RE average * 4 + DC average * 2 + QA average)/15. \n \n \n ",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8240578174591064
    },
    {
      "name": "Language model",
      "score": 0.676844596862793
    },
    {
      "name": "Biomedical text mining",
      "score": 0.671668291091919
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6502770781517029
    },
    {
      "name": "Natural language processing",
      "score": 0.6421719789505005
    },
    {
      "name": "Transformer",
      "score": 0.6297152042388916
    },
    {
      "name": "Encoder",
      "score": 0.5665420293807983
    },
    {
      "name": "Vocabulary",
      "score": 0.5026814937591553
    },
    {
      "name": "Named-entity recognition",
      "score": 0.49783968925476074
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.43970930576324463
    },
    {
      "name": "Text mining",
      "score": 0.427205353975296
    },
    {
      "name": "Question answering",
      "score": 0.4169677495956421
    },
    {
      "name": "Machine learning",
      "score": 0.3692482113838196
    },
    {
      "name": "Information retrieval",
      "score": 0.34410715103149414
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Task (project management)",
      "score": 0.0
    }
  ]
}