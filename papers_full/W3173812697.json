{
  "title": "Can Transformer Models Measure Coherence In Text: Re-Thinking the Shuffle Test",
  "url": "https://openalex.org/W3173812697",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2741638872",
      "name": "Philippe Laban",
      "affiliations": [
        "University of California, Berkeley",
        "Berkeley College"
      ]
    },
    {
      "id": "https://openalex.org/A2762059251",
      "name": "Luke Dai",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A3162771892",
      "name": "Lucas Bandarkar",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    },
    {
      "id": "https://openalex.org/A1437904991",
      "name": "Marti A. Hearst",
      "affiliations": [
        "Berkeley College",
        "University of California, Berkeley"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2963356835",
    "https://openalex.org/W3101600240",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W4287704453",
    "https://openalex.org/W3020907607",
    "https://openalex.org/W1828401780",
    "https://openalex.org/W2970763671",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2140676672",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3034715004",
    "https://openalex.org/W2994673210",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2123849094",
    "https://openalex.org/W2776762669",
    "https://openalex.org/W2016095950",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W2899386490",
    "https://openalex.org/W2963469963",
    "https://openalex.org/W3015468748",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3034775979",
    "https://openalex.org/W2740181799",
    "https://openalex.org/W2996264288",
    "https://openalex.org/W4295838474",
    "https://openalex.org/W2963768805",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2962788840"
  ],
  "abstract": "Philippe Laban, Luke Dai, Lucas Bandarkar, Marti A. Hearst. Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 2021.",
  "full_text": "Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics\nand the 11th International Joint Conference on Natural Language Processing (Short Papers), pages 1058–1064\nAugust 1–6, 2021. ©2021 Association for Computational Linguistics\n1058\nCan Transformer Models Measure Coherence In Text?\nRe-Thinking the Shufﬂe Test\nPhilippe Laban , Luke Dai , Lucas Bandarkar , and Marti A. Hearst\nUC Berkeley\n{phillab, luke.dai, lucasbandarkar, hearst}@berkeley.edu\nAbstract\nThe Shufﬂe Test is the most common task to\nevaluate whether NLP models can measure co-\nherence in text. Most recent work uses di-\nrect supervision on the task; we show that by\nsimply ﬁnetuning a RoBERTa model, we can\nachieve a near perfect accuracy of 97.8%, a\nstate-of-the-art. We argue that this outstand-\ning performance is unlikely to lead to a good\nmodel of text coherence, and suggest that the\nShufﬂe Test should be approached in a Zero-\nShot setting: models should be evaluated with-\nout being trained on the task itself. We eval-\nuate common models in this setting, such as\nGenerative and Bi-directional Transformers,\nand ﬁnd that larger architectures achieve high-\nperformance out-of-the-box. Finally, we sug-\ngest the k-Block Shufﬂe Test, a modiﬁcation\nof the original by increasing the size of blocks\nshufﬂed. Even though human reader perfor-\nmance remains high (around 95% accuracy),\nmodel performance drops from 94% to 78%\nas block size increases, creating a conceptually\nsimple challenge to benchmark NLP models.\n1 Introduction\nIn recent years, text generation applications, fu-\neled by Transformer models pre-trained on large\ndatasets, have achieved dramatic results on a wide\nrange of NLP tasks. These include GPT2 applied\nto story completion of fan ﬁction (Radford et al.,\n2019), the PEGASUS model (Zhang et al., 2020)\nimproving state-of-the-art on ten summarization\ndatasets in widely varying domains, and more re-\ncently GPT3 (Brown et al., 2020) doing well on a\ndiversity of tasks in a zero-shot setting. However,\nit is not clear how coherent the text generated by\nthese models is.\nThe computational linguistics literature holds\nmany competing deﬁnitions of coherence in text;\nWang and Guo (2014) provide a useful brief sum-\nmary of key competing theories. This work at-\nJesse on the other hand prefers tea.\nJesse and Hayden go to the park.\nThere is no accounting for tastes.\nHayden usually brings coffee.\nIt's a good way to get fresh air.\nThey go there every day.\nHayden usually brings coffee.\nJesse on the other hand prefers tea.\nThere is no accounting for tastes.\nJesse and Hayden go to the park.\nThey go there every day.\nIt's a good way to get fresh air.\nOriginal\nShuffle - Block 1 Shuffle - Block 3\nJesse and Hayden go to the park.\nThey go there every day.\nIt's a good way to get fresh air.\nHayden usually brings coffee.\nJesse on the other hand prefers tea.\nThere is no accounting for tastes.\nFigure 1: Can modern NLP models recognize shuf-\nﬂed, incoherent text without supervision? Yes\n(mostly) when all sentences are shufﬂed (left), but less\nso when shufﬂing kblocks at a time (right).\ntempts to identify the absence of coherence, noting\nthat a text might be composed of valid sentences\nwhen viewed independently, but when read sequen-\ntially, semantic relations are not well-supported.\nThe NLP community has proposed models to\nmeasure coherence, as well as repeatable tasks to\nevaluate these models. In this paper, we outline\nthese common tasks, and describe what we believe\nis a limitation in the framework of the most com-\nmon task: the Shufﬂe Test. The Shufﬂe Test is\na conceptually simple and reproducible task, in\nwhich a model must differentiate between an origi-\nnal text and a sentence-order shufﬂed version. Be-\ncause of its simplicity, we make the argument that\nthe Shufﬂe Test should be viewed as aprobe: a task\non which models should be evaluated without di-\nrected supervision. Prior work (Paulus et al., 2018)\nhas shown that directly optimizing evaluation met-\nrics such as ROUGE or BLEU leads to inadequate\nmodels, exploiting weaknesses in the evaluation\nmetrics.\nWe show that this phenomenon occurs with the\ncurrent application of the Shufﬂe Test in related\nwork. To demonstrate the pitfalls, we ﬁnetune a\nRoBERTa-large model – an architecture several or-\n1059\n2008 2010 2012 2014 2016 2018 2020\nPublication Year\n50\n60\n70\n80\n90\n100Accuracy at Shuffle Test\nA\nB C D\nE F\nRandom Guess\nBest model performance\nFigure 2: Timeline of incremental accuracy improve-\nments on the Shufﬂe Test on the WSJ corpus. Letters\nare for models described in Section 2.2.\nders of magnitude larger than previously used mod-\nels – on the Shufﬂe Test and show the results outper-\nform previous models, with an accuracy of 97.8%.\nWe argue this model has most likely learned fea-\ntures speciﬁc to recognizing shufﬂed-ness, which\nis probably a conﬂated signal for the underlying\ngoal of learning a strong coherence model.\nWe ﬁrst outline prior work on tasks and models\nto measure textual coherence, then describe the\nframework for the Zero-Shot Shufﬂe Test, showing\nhow to adapt common models to the setting, and\nﬁnally propose a variation to the Shufﬂe Test that\nsigniﬁcantly increases the challenge for models,\nwhile not affecting human performance at the task.1\n2 Tasks and Models for Coherence\n2.1 Tasks for Coherence Evaluation\nThe Shufﬂe Test, introduced by Barzilay and Lap-\nata (2008), is the most common task for coherence\nmodel evaluation. The task is a binary classiﬁca-\ntion, in which a model must discriminate between\na document and a shufﬂed document, obtained by\nrandomly shufﬂing the order of sentences in the\ndocument. The most common dataset for evalua-\ntion is a set of articles from the Wall Street Journal\n(Elsner and Charniak, 2011).\nIn the Insertion Test, a single sentence from a\ndocument is removed, and the model must predict\nthe sentence position. Typically, models assign a\nscore to each possible position, and predict the one\nwith highest score. One limitation of the Insertion\nTest is that model accuracies are low, often in the\n10-20% range (Elsner and Charniak, 2011). To our\nknowledge, there is no evaluation of human perfor-\nmance on this test; with the possibility that the task\n1The code and model checkpoints are available at:https:\n//github.com/tingofurro/shuffle_test.\ncan have more than one plausible solution. Com-\nputational cost is another limitation, often growing\nlinearly with the number of sentences.\nIn the Sentence Ordering Task , a model is\ngiven an randomly ordered sentence set, and must\nproduce the correct ordering of sentences. The\ntask is often restricted to generative models, as it is\nprohibitively expensive to score all combinations\nto extract a best-scoring order (Logeswaran et al.,\n2018).\n2.2 Models for the Shufﬂe Test\nFigure 2 is a timeline of models that have led to\nprogress on the Shufﬂe Test since its introduction.\nThe Entity Grid (model A in Fig. 2) was intro-\nduced by Barzilay and Lapata (2008). A text is\ntransformed into an entity grid, a matrix (#sen-\ntences x #entities) indicating presence of an entity\nin a sentence. The entity grid is featurized and used\nto train a predictor on coherence tasks.\nElsner and Charniak (2011) (model B) extended\nthe entity grid by adding linguistic features such\nas named-entity type. Nguyen and Joty (2017)\n(model C) introduce the ﬁrst neural approach, using\na convolutional neural network (CNN) to operate\nover the entity grid, and Joty et al. (2018) (model\nD) added word embeddings to entity-grid features.\nMost recently, Moon et al. (2019) (model E) re-\nplaced traditional word vectors with ELMO (Peters\net al., 2018) contextual word vectors.\nCrucially, all these models are directly trained\non the Shufﬂe Test, and with each iteration of im-\nprovement, model capacity (i.e., the number of\ntrainable parameters) has increased. We ﬁnetune\na RoBERTa-large (Liu et al., 2019) (model F), a\nstill larger model, on the Shufﬂe Test, and achieve\na 97.8% accuracy on the WSJ test set, a new state-\nof-the-art.\nTraining details. We ﬁnetune the RoBERTa-\nlarge on the training portion of the WSJ dataset,\nand setup the task as a sequence classiﬁcation. We\ntrained using the ADAM optimizer, using a learn-\ning rate of 1e−5 and a batch-size of 16. The model\nwas trained on a single GPU, an Nvidia V-100,\nand training converged within 10 minutes. Model\ncheckpoint was selected based on a validation set\naccuracy, and tested once on the standard WSJ test\nset.\nThis stellar performance leads us to believe that\nthere are two conﬂated factors that cause good per-\nformance on the Shufﬂe Test: a model that can\n1060\ntruly recognize the lack of coherence in shufﬂed\ntext, and a model specialized at the Shufﬂe Test,\nrecognizing shufﬂe-speciﬁc features in text, with-\nout assessing textual coherence. This resonates\nwith ﬁndings from Mohiuddin et al. (2020), who\nshow that increased model performance at the Shuf-\nﬂe Test in supervised models does not necessarily\nlead to improvements in downstream tasks, such as\nranking of generated summaries. Ideally, the Shuf-\nﬂe Test would be used to assess coherence models\nthat work independently of the test itself.\nWe propose a simple solution: coherence models\nshould be evaluated on the Shufﬂe Test in a Zero-\nShot setting; without being supervised on the test,\npreventing the learning of shufﬂe-speciﬁc features,\nand more directly evaluating coherence aptitudes.\n3 Zero-Shot Shufﬂe Test\nWe now deﬁne speciﬁcally what factors need to be\nrespected to satisfy the zero-shot setting.\nIn the Zero-Shot Shufﬂe Test, the evalu-\nated model must not be pre-trained, ﬁne-\ntuned or modiﬁed using shufﬂed text.\nMore speciﬁcally, this restricts the use of the\nShufﬂe Test as supervision (as a binary classiﬁ-\ncation task), as well as other tasks that involve\nshufﬂing text, such as the sentence ordering task.\nNext, we adapt common architectures to the\nZero-Shot Shufﬂe Test and assess performance in\ndiverse textual domains.\n3.1 Zero-Shot Coherence Models\nWe adapt the two common classes of Transformer\nmodels to the Zero-Shot Shufﬂe Test: Generative\nand Bi-directional Transformers.\nGenerative Transformers are compatible with\nlanguage modeling, in which a model assigns a like-\nlihood to a sequence of words (S = [W1,....Wn]).\nTransformers estimate the likelihood of a sequence\nby factoring on sequence order:\nP(S) =\nN∏\ni=1\nP(Wi|W1...Wi−1) (1)\nTaking the log of the likelihood ( log(P(S))) is\noften preferred as it allows for numerical stability.\nTo perform a Zero-Shot Shufﬂe Test, we com-\npute log-likelihoods of the original and shufﬂed\ndocuments and predict the lower-scoring one as\nshufﬂed.\nWe experiment with GPT2 models of varying\nsizes (GPT2-base, GPT2-medium, GPT2-large),\nand ﬁnetune an In-Domain GPT2-medium using\na language modeling loss in each domain to eval-\nuate whether in-domain specialization improves\nperformance (GPT2-med-ID).\nWhen texts exceed sequence-length limits of\nmodels (e.g., 512 words), we implement a slid-\ning window mechanism. The sequence is split into\nsuccessive windows with 50% overlap. Window\nlog-likelihoods are averaged into a document log-\nlikelihood.\nBi-Directional Transformers, exempliﬁed by\nBERT (Devlin et al., 2019), are the second class\nof models we adapt to the test. Unlike Generative\nTransformers, bi-directional Transformers do not\nimpose strict sequence order, rendering sequence\nlikelihood estimation less straightforward.\nSalazar et al. (2020) propose a solution, with\nMasked Language Model Scoring ( MLMS), in\nwhich a likelihood is estimated by masking each\nword in the sequence, predicting its identity, and\naveraging all word-likelihoods into a score:\nMLMS(S) = 1\nN\nN∑\ni=1\nlog PMLM (Wi|W\\i) (2)\nwhere W\\i = S−{Wi}. Unlike generative models,\neach word’s likelihood is conditioned on all others,\nan advantage of Bi-directional models. For the\nZero-Shot Shufﬂe Test, the document with lower\nMLMS is predicted as shufﬂed.\nOne key disadvantage of MLMS is its compu-\ntational cost: scoring requires a forward-pass for\neach word in the sequence; by contrast, generative\nmodels usually require a single forward pass. This\nlimits our ability to test large models, and therefore\ntest only base models: BERT-baseand RoBERTa-\nbase.\n3.2 Datasets\nTo examine whether there are signiﬁcant differ-\nences in performance across domains, we evaluate\nwith the Shufﬂe Test using three distinct domains.\nWe performed a manual check to determine that the\ndatasets we selected do not overlap with the dataset\nused to pre-train BERT, RoBERTa and GPT2. The\nthree domains are:\nNews domain. We use the standard Wall Street\nJournal (WSJ) test-set introduced by Elsner and\nCharniak (2011) in the supervised Shufﬂe Test. The\ndataset contains 1006 documents.\n1061\nDomain (%)\nModel WSJ Legal Reddit Overall\nGPT2-base 47.2 92.0 74.8 71.3\nGPT2-medium 91.2 98.6 88.9 92.9\nGPT2-large 73.2 99.3 90.6 87.7\nBERT-base 73.2 96.1 86.1 85.1\nRoBERTa-base 82.3 94.8 96.7 91.3\nGPT2-med-id 93.1 98.8 90.0 94.0\nTable 1: Accuracy of Zero-Shot Shufﬂe Tests of models\non three domains: Wall Street Journal (WSJ), Billsum\ndocuments (Legal), and Reddit. We report an overall,\naveraged performance across domains.\nLegal domain. We use the full document re-\nleased in the BillSum dataset (Kornilova and Eidel-\nman, 2019) which consists of US Congressional\nand California state bills. We use the ﬁrst 1,000\ndocuments in the standard test set.\nBlog domain. We use posts of the Reddit TIFU\ndataset (Kim et al., 2019), consisting of stories\nwritten by members of the Reddit community. We\nuse the ﬁrst 1,000 documents of the corpus.\nWe choose these datasets as they are publicly\navailable, can easily be accessed through the Hug-\ngingFace datasets package (Wolf et al., 2020) and\nrepresent a diversity of textual domains.\nWe note that document length affects the amount\nof displacement that occurs from shufﬂing, with\nmore displacement in longer texts. To take this\neffect into account, we truncate documents at 20\nsentences before administering the Shufﬂe Test.\n3.3 Results\nOverall, all models signiﬁcantly outperform ran-\ndom chance, with the GPT2-medium achieving\n91.2% on the WSJ test-set out of the box where\nthe previous supervised state-of-the-art was 93%.\nBi-directional models achieve better results than\ngenerative models at Transformer-base size (e.g.,\nGPT2-base vs. RoBERTa-base).\nIncreasing model size leads to large performance\nimprovement for GPT2, conﬁrming that according\nto the Shufﬂe Test, larger Transformer models im-\nprove at modeling coherence. In-domain ﬁnetuning\nleads to an improvement on all domains (GPT2-\nmed-id outperforming GPT2-medium), conﬁrming\nthe strength of in-domain ﬁnetuning (Howard and\nRuder, 2018).\nFinally, models achieved stronger performance\non the Legal domain, with models all scoring 92.0\nor above. Overall, three of the six models we test\nachieve compound performance over 90%.\nBlock Size\nModel 1 2 3 4 5\nHuman Perf. - WSJ 97.5 94.5 93.0 96.0 94.0\nGPT2-med - WSJ 95.3 91.4 89.5 87.4 85.3\nGPT2-med - Legal 98.7 98.0 96.9 95.9 94.5\nGPT2-med - Reddit 89.5 76.9 66.1 59.1 53.8\nGPT2-med - Avg. 94.5 88.8 84.2 80.8 77.9\nTable 2: Results of Zero-Shot KBST varying the\nblock size from one to ﬁve.The GPT2-medium model\nwas tested on all three domains, and human perfor-\nmance was measured on WSJ.\nThere is a potential question about the zero-shot\nnature of the BERT training method. The original\nBERT model is trained with two objectives, one of\nwhich is Next Sentence Prediction (NSP). In NSP,\nthe model is exposed to two blocks of text, and\nmust predict whether they are adjacent in a docu-\nment or not. It can be argued that NSP is an indirect\nsupervision signal for the Shufﬂe Test. However,\nwe ﬁnd that the BERT model performs worse than\nRoBERTa, a similar model in architecture trained\nwithout the NSP objective. This difference in per-\nformance suggests that the NSP objective is not the\ncause of the superior performance of these mod-\nels, thus preserving the claim that they act in a\nzero-shot manner for the purposes of the Shufﬂe\nTest.\nWe next propose a modiﬁcation to the Shufﬂe\nTest that challenges models signiﬁcantly more.\n4 The k-Block Shufﬂe Test\nResults in Section 3 can be interpreted to mean that\nwith large enough Transformer models, the Shufﬂe\nTest with no supervision is essentially a solved task.\nWe ﬁnd that a simple modiﬁcation of the Shufﬂe\nTest can signiﬁcantly reduce model performance,\nwithout affecting human annotator performance.\nThe modiﬁcation we propose, k-Block Shufﬂe\nTest (KBST), is illustrated in Figure 1. In the stan-\ndard Shufﬂe Test, text is divided into sentences and\nshufﬂed, with a unit of one sentence. In the k-Block\nShufﬂe Test, sentences are grouped in contiguous\nblocks of k sentences (resembling paragraphs), and\nthe blocks are shufﬂed, maintaining sentence order\nin each block. Within a block, sentences remain\nlocally coherent, and as block size increases, the\nfraction of correct sentence transitions increases,\nwhile potentially incoherent transitions decrease.\nTo establish the feasibility of the KBST with\ndiffering block-sizes, we performed a human evalu-\nation completed by authors of the paper as well as a\n1062\nthird annotator recruited on the Upwork2 platform.\nThis annotator is a native English speaker with ex-\nperience in proofreading, and was remunerated at\n$15/hour USD.\nThe human evaluation consisted of performing\nKBST on 500 documents randomly sampled from\nthe WSJ dataset, with 100 tests for each block-size\nfrom one to ﬁve. Each Shufﬂe test was performed\nby at least two annotators. We ﬁnd that there is\nhigh inter-annotator agreement (Cohen’s Kappa\nκ= 0.86), which does not signiﬁcantly vary with\nblock-size (ranging from 0.76-0.94).\nKBST results for human and computational mod-\nels are shown in Table 2. Human performance is\nvery high on WSJ, averaging above 95%, and is\nnot signiﬁcantly affected by block-size.\nTimings logged during human annotation show\nthat Shufﬂe Tests took on average 40% more time\nfor larger (3-5) than smaller blocks (1-2), showing\nthe task requires more attention from annotators as\nblock size increases.\nIn all three domains, increased block size leads\nto a decrease in model performance. The magni-\ntude of decrease in performance from block-size\n1 to 5 is sensitive to the domain, with a drop of\n4% in the legal domain, and 36% for Reddit, on\nwhich the 5-block performance of 53.8% narrowly\noutperforms random performance.\nAggregate model performance drops from 94.5%\nfor block-size 1 to 77.9% for block-size 5, leaving\nsigniﬁcant room in larger block-size to measure\nfuture model improvements.\nAlthough increasing the block size leads to a\nmore challenging task for current models, we argue\nmodels should not be evaluated on a single block\nsize, but on several block sizes, with each block\nsize giving a perspective on the model’s perfor-\nmance at a speciﬁc point between local and global\ncoherence (Van Dijk, 1985).\n5 Limitations and Future Work\nShufﬂing vs. Coherence. In this work, we pro-\npose an improved setting for the Shufﬂe Test, the\nmost popular probe to measure textual coherence.\nHowever, many linguistic phenomena necessary for\ncoherence of text cannot be measured by shufﬂing\nsentence order. In the long-run, the community\nshould build more elaborate coherence measures,\nto build a more complete picture of model capabili-\nties and limitations.\n2https://www.upwork.com\nCoherence in Long Text. We limited our anal-\nysis to texts with up to 512 words, a common\nconstraint in pre-trained Transformers. Recent\nprogress in model architectures open the possibil-\nity to process longer text, with models such as the\nReformer (Kitaev et al., 2019), Longformer (Belt-\nagy et al., 2020) and Big Bird (Zaheer et al., 2020)\nprocessing sequences of several thousand words.\nWith longer sequences, one can further increase\nthe block-size of the k-Blocked Shufﬂe Test (i.e.,\nk=20) to gain understanding of model’s ability to\ndiscern global coherence (Van Dijk, 1985), or main\ntopics and subtopics (Hearst, 1997).\nSpecialized Coherence Models. In this work,\nwe limit our analysis to popular models out-of-\nthe-box, establishing baseline performances for\nthe Zero-Shot KBST. Future work should estab-\nlish whether performance can be further improved,\nfor instance using word-level likelihood signals and\nsurprisal proﬁles.\n6 Conclusion\nIn this work, we discuss a potential limitation in\nthe framing of the Shufﬂe Test, the most commonly\nused task to evaluate models for textual coherence.\nWe show that a RoBERTa model can be ﬁnetuned\nto achieve near-perfect performance without nec-\nessarily measuring coherence, and propose a new\nframework: the Zero-Shot Shufﬂe Test, in which\ndirect supervision is disallowed. Modern NLP ar-\nchitectures can achieve high performance out-of-\nthe-box in this Zero-Shot setting on a variety of\ntextual domains. We ﬁnd however that models\nstruggle when we introduce a simple modiﬁcation,\nk-Blocking, to the shufﬂing strategy, with accuracy\ndropping from around 94% to around 78%. The\nk-Block Shufﬂe Test in a Zero-Shot setting is a\nstraightforward, reproducible task that can be used\nto benchmark future NLP architectures to measure\ncoherence capabilities.\nAcknowledgments\nWe would like to thank Katie Stasaski, Dongyeop\nKang, and the ACL reviewers for their helpful com-\nments. This work was supported by a Microsoft\nBAIR Commons grant as well as a Microsoft Azure\nSponsorship.\nEthical Considerations\nWe present a method to evaluate models on their\nability to measure textual coherence. We have ex-\n1063\nclusively run experiments in the English language,\nand even though we expect the method to be adapt-\nable to other languages, we have not veriﬁed this\nassumption experimentally and limit our claims to\nthe English language.\nFor the human evaluation, we paid the annota-\ntor above the minimum wage, and do not release\nany personal identiﬁable information. We did not\ncollect payment information and relied on a third\nparty (Upwork.com) for remuneration.\nReferences\nR. Barzilay and Mirella Lapata. 2008. Modeling lo-\ncal coherence: An entity-based approach. Computa-\ntional Linguistics, 34:1–34.\nIz Beltagy, Matthew E. Peters, and Arman Cohan.\n2020. Longformer: The long-document transformer.\narXiv:2004.05150.\nT. Brown, B. Mann, Nick Ryder, Melanie Subbiah,\nJ. Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell,\nSandhini Agarwal, Ariel Herbert-V oss, G. Kr ¨uger,\nT. Henighan, R. Child, Aditya Ramesh, D. Ziegler,\nJeffrey Wu, Clemens Winter, Christopher Hesse,\nMark Chen, E. Sigler, Mateusz Litwin, Scott Gray,\nBenjamin Chess, J. Clark, Christopher Berner, Sam\nMcCandlish, A. Radford, Ilya Sutskever, and Dario\nAmodei. 2020. Language models are few-shot learn-\ners. ArXiv, abs/2005.14165.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186.\nMicha Elsner and Eugene Charniak. 2011. Extending\nthe entity grid with entity-speciﬁc features. In Pro-\nceedings of the 49th Annual Meeting of the Associ-\nation for Computational Linguistics: Human Lan-\nguage Technologies, pages 125–129, Portland, Ore-\ngon, USA. Association for Computational Linguis-\ntics.\nMarti A. Hearst. 1997. Text Tiling: Segmenting text\ninto multi-paragraph subtopic passages. Computa-\ntional linguistics, 23(1):33–64.\nJeremy Howard and Sebastian Ruder. 2018. Universal\nlanguage model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 328–339.\nShaﬁq Joty, Muhammad Tasnim Mohiuddin, and\nDat Tien Nguyen. 2018. Coherence modeling of\nasynchronous conversations: A neural entity grid ap-\nproach. In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics\n(Volume 1: Long Papers), pages 558–568.\nByeongchang Kim, Hyunwoo Kim, and Gunhee Kim.\n2019. Abstractive summarization of reddit posts\nwith multi-level memory networks. In Proceed-\nings of the 2019 Conference of the North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies, Volume 1\n(Long and Short Papers), pages 2519–2531.\nNikita Kitaev, Lukasz Kaiser, and Anselm Levskaya.\n2019. Reformer: The efﬁcient transformer. In Inter-\nnational Conference on Learning Representations.\nAnastassia Kornilova and Vladimir Eidelman. 2019.\nBillSum: A corpus for automatic summarization of\nUS legislation. In Proceedings of the 2nd Workshop\non New Frontiers in Summarization , pages 48–56,\nHong Kong, China. Association for Computational\nLinguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nLajanugen Logeswaran, Honglak Lee, and Dragomir\nRadev. 2018. Sentence ordering and coherence mod-\neling using recurrent neural networks. In Proceed-\nings of the AAAI Conference on Artiﬁcial Intelli-\ngence, volume 32.\nTasnim Mohiuddin, Prathyusha Jwalapuram, Xiang\nLin, and Shaﬁq Joty. 2020. Coheval: Bench-\nmarking coherence models. arXiv preprint\narXiv:2004.14626.\nHan Cheol Moon, Tasnim Mohiuddin, Shaﬁq Joty, and\nChi Xu. 2019. A uniﬁed neural coherence model. In\nProceedings of the 2019 Conference on Empirical\nMethods in Natural Language Processing and the\n9th International Joint Conference on Natural Lan-\nguage Processing (EMNLP-IJCNLP) , pages 2262–\n2272, Hong Kong, China. Association for Computa-\ntional Linguistics.\nDat Tien Nguyen and Shaﬁq Joty. 2017. A neural local\ncoherence model. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1320–\n1330.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2018. A deep reinforced model for abstractive sum-\nmarization. In International Conference on Learn-\ning Representations.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In Proceedings of the 2018 Conference\nof the North American Chapter of the Association\n1064\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long Papers), pages 2227–\n2237.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\nJulian Salazar, Davis Liang, Toan Q Nguyen, and Ka-\ntrin Kirchhoff. 2020. Masked language model scor-\ning. In Proceedings of the 58th Annual Meeting\nof the Association for Computational Linguistics ,\npages 2699–2712.\nTeun A Van Dijk. 1985. Semantic discourse analysis.\nHandbook of discourse analysis, 2:103–136.\nYuan Wang and Minghe Guo. 2014. A short analysis of\ndiscourse coherence. Journal of Language Teaching\nand Research, 5(2):460.\nThomas Wolf, Quentin Lhoest, Patrick von Platen,\nYacine Jernite, Mariama Drame, Julien Plu, Julien\nChaumond, Clement Delangue, Clara Ma, Abhishek\nThakur, Suraj Patil, Joe Davison, Teven Le Scao,\nVictor Sanh, Canwen Xu, Nicolas Patry, Angie\nMcMillan-Major, Simon Brandeis, Sylvain Gugger,\nFranc ¸ois Lagunas, Lysandre Debut, Morgan Funtow-\nicz, Anthony Moi, Sasha Rush, Philipp Schmidd,\nPierric Cistac, Victor Mu ˇstar, Jeff Boudier, and\nAnna Tordjmann. 2020. Datasets. GitHub. Note:\nhttps://github.com/huggingface/datasets, 1.\nManzil Zaheer, Guru Guruganesh, Kumar Avinava\nDubey, Joshua Ainslie, Chris Alberti, Santiago On-\ntanon, Philip Pham, Anirudh Ravula, Qifan Wang,\nLi Yang, et al. 2020. Big bird: Transformers for\nlonger sequences. Advances in Neural Information\nProcessing Systems, 33.\nJingqing Zhang, Yao Zhao, Mohammad Saleh, and Pe-\nter Liu. 2020. Pegasus: Pre-training with extracted\ngap-sentences for abstractive summarization. In In-\nternational Conference on Machine Learning, pages\n11328–11339. PMLR.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6444685459136963
    },
    {
      "name": "Computer science",
      "score": 0.6096571683883667
    },
    {
      "name": "Measure (data warehouse)",
      "score": 0.5747793912887573
    },
    {
      "name": "Coherence (philosophical gambling strategy)",
      "score": 0.5613390207290649
    },
    {
      "name": "Natural language processing",
      "score": 0.5274622440338135
    },
    {
      "name": "Test (biology)",
      "score": 0.4520946145057678
    },
    {
      "name": "Linguistics",
      "score": 0.43004053831100464
    },
    {
      "name": "Computational linguistics",
      "score": 0.4173647165298462
    },
    {
      "name": "Artificial intelligence",
      "score": 0.41674911975860596
    },
    {
      "name": "Engineering",
      "score": 0.16976112127304077
    },
    {
      "name": "Mathematics",
      "score": 0.1432114541530609
    },
    {
      "name": "Philosophy",
      "score": 0.12777060270309448
    },
    {
      "name": "Data mining",
      "score": 0.12130913138389587
    },
    {
      "name": "Electrical engineering",
      "score": 0.08758044242858887
    },
    {
      "name": "Statistics",
      "score": 0.08559590578079224
    },
    {
      "name": "Geology",
      "score": 0.06920281052589417
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I134446601",
      "name": "Berkeley College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ]
}