{
    "title": "A pilot study of the performance of Chat GPT and other large language models on a written final year periodontology exam",
    "url": "https://openalex.org/W4410493620",
    "year": 2025,
    "authors": [
        {
            "id": "https://openalex.org/A2990602606",
            "name": "Shaun Ramlogan",
            "affiliations": [
                "University of the West Indies"
            ]
        },
        {
            "id": "https://openalex.org/A1972275063",
            "name": "Vidya Raman",
            "affiliations": [
                "University of the West Indies"
            ]
        },
        {
            "id": null,
            "name": "Shayn Ramlogan",
            "affiliations": [
                "University of the West Indies"
            ]
        },
        {
            "id": "https://openalex.org/A2990602606",
            "name": "Shaun Ramlogan",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1972275063",
            "name": "Vidya Raman",
            "affiliations": []
        },
        {
            "id": null,
            "name": "Shayn Ramlogan",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4385715939",
        "https://openalex.org/W4387597556",
        "https://openalex.org/W4322622443",
        "https://openalex.org/W4391809236",
        "https://openalex.org/W4376611400",
        "https://openalex.org/W4385662142",
        "https://openalex.org/W4394956892",
        "https://openalex.org/W2809948294"
    ],
    "abstract": "Abstract Large Language Models (LLMs) such as Chat GPT are being increasingly utilized by students in education with reportedly adequate academic responses. Chat GPT is expected to learn and improve with time. Thus, the aim was to longitudinally compare the performance of the current versions of Chat GPT-4/GPT4o with that of final-year DDS students on a written periodontology exam. Other current non-subscription LLMs were also compared to the students. Chat GPT-4, guided by the exam parameters, generated answers as ‘Run 1’ and 6 months later as as ‘Run 2’. Chat GPT-4o generated answers as ‘Run 3’ at 15 months later. All LLMs and student scripts were marked independently by two periodontology lecturers (Cohen’s Kappa value 0.71). ‘Run 1’ and ‘Run 3’ generated statistically significantly ( p &lt; 0.001) higher mean scores of 78% and 77% compared to the students (60%). The mean scores of Chat GPT-4 and GPT4o were also similar to that of the best student. ‘Run 2’ performed at the level of the students but underperformed with generalizations, more inaccuracies and incomplete answers compared to ‘Run 1’ and ‘Run 3’. This variability for ‘Run 2’ may be due to outdated data sources, hallucinations and inherent LLM limitations such as online traffic, availability of datasets and resources. Other non-subscription LLMs such as Claude, DeepSeek, Gemini and Le Chat also produced statistically significantly ( p &lt; 0.001) higher scores compared to the students. Claude was the best performing LLM with more comprehensive answers. LLMs such as Chat GPT may provide summaries and model answers in clinical undergraduate periodontology education. However, the result must be interpreted with caution regarding academic accuracy and credibility especially in a health care profession.",
    "full_text": "Ramlogan et al. BMC Medical Education          (2025) 25:727  \nhttps://doi.org/10.1186/s12909-025-07195-7\nRESEARCH Open Access\n© The Author(s) 2025. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nBMC Medical Education\nA pilot study of the performance of Chat GPT \nand other large language models on a written \nfinal year periodontology exam\nShaun Ramlogan1*, Vidya Raman1 and Shayn Ramlogan2 \nAbstract \nLarge Language Models (LLMs) such as Chat GPT are being increasingly utilized by students in education with report-\nedly adequate academic responses. Chat GPT is expected to learn and improve with time. Thus, the aim was to longi-\ntudinally compare the performance of the current versions of Chat GPT-4/GPT4o with that of final-year DDS students \non a written periodontology exam. Other current non-subscription LLMs were also compared to the students. Chat \nGPT-4, guided by the exam parameters, generated answers as ‘Run 1’ and 6 months later as as ‘Run 2’ . Chat GPT-4o \ngenerated answers as ‘Run 3’ at 15 months later. All LLMs and student scripts were marked independently by two \nperiodontology lecturers (Cohen’s Kappa value 0.71). ‘Run 1’ and ‘Run 3’ generated statistically significantly (p < 0.001) \nhigher mean scores of 78% and 77% compared to the students (60%). The mean scores of Chat GPT-4 and GPT4o \nwere also similar to that of the best student. ‘Run 2’ performed at the level of the students but underperformed \nwith generalizations, more inaccuracies and incomplete answers compared to ‘Run 1’ and ‘Run 3’ . This variability \nfor ‘Run 2’ may be due to outdated data sources, hallucinations and inherent LLM limitations such as online traffic, \navailability of datasets and resources. Other non-subscription LLMs such as Claude, DeepSeek, Gemini and Le Chat \nalso produced statistically significantly (p < 0.001) higher scores compared to the students. Claude was the best per-\nforming LLM with more comprehensive answers. LLMs such as Chat GPT may provide summaries and model answers \nin clinical undergraduate periodontology education. However, the result must be interpreted with caution regard-\ning academic accuracy and credibility especially in a health care profession.\nKeywords Periodontology, Education, Clinical\nIntroduction\nCHAT GPT or Chat Generative Pre-trained Transformer \nis a Large Language Model (LLM) conversation soft -\nware or Chatbot that is modified by human feedback and \nsupervised learning. This form of artificial intelligence \n(AI) was developed by Open AI, Inc. (Delaware, USA) \nin 2022. The version Chat GPT- 4 was released in March \n2023 and involved improvements in advanced reasoning, \ncomplex instructions and creativity [1]. The latest version \nGPT- 4o (Omni) was released in May 2024 with improve-\nments for audio, text, image and video input producing \nan enhanced human–computer interaction [2].\nCHAT GPT has been promoted as a more advanced \nuser-friendly LLM research tool that allows tailored \nresponses based on prompt engineering. While search \nengines retrieve and rank data from the internet, \nLarge Language Models (LLMs) analyse, understand \n*Correspondence:\nShaun Ramlogan\nshaun.ramlogan@sta.uwi.edu\n1 School of Dentistry, Faculty of Medical Sciences, The University \nof the West Indies, St Augustine Campus, EWMSC, Champs Fleurs, \nTrinidad and Tobago\n2 School of Medicine, Faculty of Medical Sciences, The University \nof the West Indies, St Augustine Campus, EWMSC, Champs Fleurs, \nTrinidad and Tobago\nPage 2 of 8Ramlogan et al. BMC Medical Education          (2025) 25:727 \nand generate responses from vast data on which it has \nbeen trained. This allows for the generation of seem -\ningly original answers to questions that may be indis -\ntinguishable from those produced by humans. Moritz \net  al. reported that two AI plagiarism software pro -\ngrams (Grover and Writer Detection) were unable to \ndetermine that a machine generated paper was not \ngenerated by humans [3 ]. AI plagiarism software pro -\ngrams offer a more comprehensive detection approach \nlooking at context and paraphrasing as opposed to \ntradition human plagiarism software programs which \nlook at exact text copies without context.\nChat GPT was deemed capable of generating ade -\nquate responses for USMLE type questions in a Step \n1(> 60%) but not in Step 2 (< 60%) [4 ]. However, in \na Polish specialist medical licensing examination, \nCHAT GPT was able to produce a 61% score on mul -\ntiple choice type questions [5 ]. Chat GPT has been \ndeemed to provide fairly accurate responses to medi -\ncally diverse questions as assessed by academic medi -\ncal specialists [6 ]. However, despite high percentage of \nreports of correct (57.8%) and comprehensive (53.5%) \nanswers there were instances of surprisingly incor -\nrect answers which prompted the need to use Chat \nGPT with caution and to compare response to recom -\nmended data sources.\nIn dentistry, CHAT GPT has been used to sup -\nplement prosthodontic treatment planning and oral \nradiology reporting but may be limited to general \ninformation and may be cited as being at risk for medi -\ncal errors and fuelling an infodemic [7 , 8].\nIn undergraduate dental education, Chat GPT has \nbeen heralded as a transformative, interactive, inclu -\nsive, and student-centred technology that drives cur -\nrent pedagogical strategies [9 ]. Thorat et  al. reported \nthat Chat GPT aligns with an individualized student/\nlearner centred approach which addresses the student’s \nneed, interest and progress with autonomous stu -\ndent and peer learning, student engagement, motiva -\ntion and tailored feedback and support. Student paper \nmarking by AI has been proposed to have the benefits \nof convenience and time savings as well as consistent \nstudent feedback and quality despite the known draw -\nbacks of ethics, legality, and privacy [10]. In compar -\ning different forms of assessment, the performance of \nChat GPT was high for multiple choice and true false \nquestions (90–100%) while that for short essay ques -\ntions was lower at 70% [11]. Chat-GPT remains one of \nthe LLMs commonly reviewed in relation to medical \neducation but there are many new and emerging LLMs \nwhich are gaining interest as alternatives [12, 13].\nAim\nChat GPT may act as a potential resource for dental stu -\ndent education. However, the standard of response is \nunknown in undergraduate periodontology particularly \nfor clinically oriented short-answer questions. Thus, the \naim of this study was to generate responses via Chat \nGPT- 4 for a final year written exam in undergradu -\nate periodontology and compare this to student perfor -\nmance. The first objective was to longitudinally repeat \nthis process at 6  months to assess the consistency and \nimprovement expected with this LLM. This process was \nalso repeated at 15 months with the new version of GPT- \n4o (Omni). The second objective was to compare the gen-\nerated responses of other non-subscription open sourced \nLLMs to the students’ performance in the same exam.\nMethod\nThis study was conducted at the School of Dentistry, The \nUniversity of the West Indies, St Augustine Campus, \nTrinidad among final-year dental students within the \nDDS programme in 2023. Ethical approval was granted \nby the Campus Research Ethics Committee, The Uni -\nversity of the West Indies, St Augustine Campus (CREC-\nSA.3172/03/2025). Individual student informed consent \nto participate was not required by the Campus Research \nEthics Committee, The University of the West Indies, St \nAugustine Campus. This study relates to the following \nexemption regulation: FGP .P2 C 2010/2011 The Univer -\nsity of the West Indies Policy and Procedures on Research \nEthics, The School for Graduate Studies and Research on \npage 11 refers to\"Exemptions from Review-Educational \nTests and Measurements\".\nAll students were included in the study as a pilot com -\nparison. It was not possible to include consecutive stu -\ndent years due to the change in the exam questions and \npotential knowledge from year to year. Further, students \nwere not individually interviewed or required to com -\nplete a questionnaire. Only the class means/medians per \nquestion  were compared as a benchmark for the exam \ndiet. The focus of the study was the performance of a \nnon-human entity, Chat GPT and other LLMs.\nThe undergraduate periodontology fifth year written \nsummative internal assessment consisted of twenty short-\nanswer type questions over a two-hour period. The exam \nquestions were mapped and benchmarked to the learning \nobjectives providing balanced coverage of the periodon -\ntology syllabus of the clinical years. Guided by Bloom’s \ntaxonomy for the cognitive domain, questions discerned \nknowledge, comprehension, application, analysis, syn -\nthesis, and evaluation learning objectives. This written \nexam did not include clinical photographs or radiographs \nas these may have been challenging to interpret by Chat \nPage 3 of 8\nRamlogan et al. BMC Medical Education          (2025) 25:727 \n \nGPT. However, clinical scenarios and narratives were \nincluded. Each student was asked to identify five easy and \nfive difficult questions based on individual opinion. This \nwas simply indicated on the examination script by the \nstudents and reported as a distribution frequency.\nModel answers and marking schemes were developed \nby both lecturers in periodontology. Both lecturers inde -\npendently marked all scripts and the final percentage \nmark was determined from an average of the assigned \nscores. The inter-rater agreement for the two periodon -\ntology lecturers was good with a significant (p < 0.001) \nCohen’s Kappa (for two raters) statistical value of 0.71 \nwhen comparing individual questions for all scripts.\nResponses for the Periodontology written exam were \nobtained from the subscription-based version of Chat \nGPT 4 (Chat Plus Subscription, Chat GPT- 4, Open AI, \nSan Francisco, USA) in September 2023 as ‘Run 1’ . The \nrequest/prompt for Chat GPT- 4 included parameters \nsuch as the nature of the exam, the number of marks \nassigned and the number of lines/words available to \nrespond to each question. The exam prompt with the \nthird exam question as an example was as follows:\n‘For all questions respond exactly as a final (5th) \nyear dental student writing their periodontology \nwritten examinations. (It is a 2 h exam consisting of \n20 short answer questions).\nThe third question below is worth 6 marks and \nshould be done in roughly 77 words (as 7 lines on an \nA4 writing paper is given):\nGive the vertical furcation classification as defined \nby Tarnow and Fletcher? How would you manage a \nGrade II, Class B furcation defect on a lower right \nfirst molar?’\nThe generated Chat GPT- 4 answers were hand tran -\nscribed to a hard copy by a research assistant and ran -\ndomly placed with the students’ scripts to ensure blinding \nfor both markers. Six months later in March 2024, a sec -\nond generated answer paper, ‘Run 2’ was obtained from \nChat GPT- 4 using identical prompt protocols.\nAnswers for the written exam were also generated from \nthe latest model GPT- 4o (OpenAI, May 2024) as ‘Run \n3’ . Other non-subscription-based LLMs were selected \nwhich were openly accessible to the public and capable of \nexecution on a home-based computer within the param -\neters of memory or computer processing unit (CPU) \npower. These included the following 10 selected mod -\nels: (1) C4ai (CohereForAI) (Version: c4ai-command-r-\nplus August 2024; Cohere, Toronto, Canada), (2) Claude \n(Version: Claude3.5-sonnet October 2024; Anthropic, \nSan Francisco, California, USA), (3) DeepSeek (Version: \nDeepSeek-R1 January 2025; DeepSeek, Hangzhou, Zhe -\njiang, China) (4) Gemini (Version: Gemini 2.0 flash \nDecember 2024; Google, Mountain View, California, \nUSA), (5) Mistral NeMo (Version: Nemo-Instruct- 2407 \nJuly 2024; NVIDIA, Santa Clara, California, USA and \nMistral AI, Paris, France), (6) Le Chat- Mistral (Version: \nLe Chat February 2024; Mistral AI, Paris, France), (7) \nLlama- 3.1 (Version: Llama- 3.1-Nemotron- 70B-Instruct \nJuly 2024; NVIDIA, Santa Clara, California, USA and \nMeta Platforms Inc., California, USA), (8) Llama- 3.3 \n(Version: Llama- 3.3 - 70B-Instruct December 2024; Meta \nPlatforms Inc., California, USA), (9) Phi (Version: phi- \n4-Q4_K_M December 2023; Microsoft -phi series, Red -\nmond, Washington, USA). (10) Qwen (Version: Qwen2.5 \n- 72B September 2024; Alibaba Cloud, Hangzhou, China). \nAnswers from the above 10 LLMs and GPT- 4o were gen-\nerated in January 2025 according to the same protocols. \nBoth examiners marked the generated scripts and the \naverage of the two examiners were used as the assigned \nmark per question.\nMarks were entered into SPSS (IBM SPSS Statistics \n30.0) for t-test statistical analyses (significance p < 0.05) \ncomparing the exam scores means of students to the \nexam scores of Chat GPT and other LLM with graphi -\ncal representation. A review of the answers generated \nfor ‘Run 1’ , ‘Run 2’ and ‘Run 3’ was completed by both \nlecturers.\nResults\nTwenty-two students, 86.4% (n = 19) of whom were \nfemale and 13.6% (n = 3) of whom were male, with a \nmean age of 26.1 years (standard deviation 2.9) in their \nfinal year participated in the written exam. The student \nmean percentage score for the written exam was 60% \n(standard deviation 12.6; range 33 to 78). Based on the \nopinions of the students, questions 2 (n = 14), 5 (n = 16), \n10 (n = 16), 12 (n = 8) and 18 (n = 8) were reported most \nfrequently as the easiest. Conversely, questions reported \nwith the highest frequency of difficulty were 8 (n = 13), 9 \n(n = 12), 15 (n = 14), 16 (n = 8) and 17 (n = 9).\nThe mean percentage score for the written exam \nderived from Chat GPT- 4, ‘Run 1’ was 78% (standard \ndeviation 10.8) (Table  1). ‘Run 1’ produced answers that \nwere scored above the students’ median score for 16 \nquestions out of the 20 questions (Fig. 1). There were two \nquestions for which ‘Run 1’ produced full scoring answers \n(questions 17 and 18). For all questions, the ‘Run 1’ score \nwas 60% or greater. For questions 2, 5 and 10, the score \nfor ‘Run 1’ was less than the corresponding median stu -\ndent score. Questions 2, 5 and 10 covered topics of peri -\nodontal disease screening and classification. Conversely, \n‘Run 1’ produced a full score in question 17 for which the \nstudents’ median score was only 50%.\nPage 4 of 8Ramlogan et al. BMC Medical Education          (2025) 25:727 \nThe mean percentage score for the written exam \nderived from Chat GPT- 4, ‘Run 2’ was 62% (standard \ndeviation 11.4) (Table  1). In this second attempt of Chat \nGPT- 4 at 6 months later, ‘Run 2’ produced scores above \nthe students’ median score for only 11 questions out of \nthe 20 questions (Fig.  1). Chat GPT- 4 scored less than \n50% on questions 2 and 8 which covered topics on peri -\nodontal disease screening and gingival recession classifi -\ncation. There were no full scoring (100%) answers in ‘Run \n2’ as there was in ‘Run 1’ .\nIn ‘Run 3’ , the mean percentage score for the written \nexam derived from GPT- 4o (Omni) was 77% (standard \ndeviation 10.0) (Table  1). In this ‘Run 3’ attempt which \nwas about 15 months after ‘Run 1’ , there were compara -\nble results to ‘Run 1’ with all questions scoring 60% and \nabove (Fig.  1). ‘Run 3’ produced scores above the stu -\ndents’ median score for 17 questions out of the 20 ques -\ntions. However, there were also no full scoring (100%) \nanswers in ‘Run 3’ as there was in ‘Run 1’ . ‘Run 3’ scored \nless than the median student score for questions 5, 10 \nand 12 which covered topics of periodontal disease clas -\nsification and antiseptic mouthwash.\nBoth ‘Run 1’ and ‘Run 3’ produced a statistically sig -\nnificantly (t-test; p < 0.001) higher mean percentage \nscore when each was separately compared to that of the \nstudents’ mean percentage score (Table  1). However, the \nmean percentage score of ‘Run 2’ failed to show statisti -\ncally significant difference (t-test; p = 0.52) from the stu -\ndents’ mean score (Table  1). Additionally, both ‘Run 1’ \nand ‘Run 3’ produced a statistically significantly (t-test; \np < 0.001) higher mean percentage score when each was \nseparately compared to that of ‘Run 1’ . There was no sta-\ntistically significant difference (t-test; p = 0.70) between \nthe mean percentage score of ‘Run 1’ and ‘Run 2’ .\nTable 1 Comparison of mean percentage score of students to \nChat GPT- 4/GPT4o and other LLMs\n# significantly lower\n* significantly higher\nSource Mean \nPercentage \nScore (%)\nStandard \nDeviation\nSignificance \nt-test (p values)\nComparison to \nStudents’ Scores\nStudent 60 12.6 -\nSubscription Based- Open AI Chat GPT- 4/GPT4o\nRun 1 (GPT- 4) 78 10.8  < 0.001*\nRun 2 (GPT- 4) 62 11.4 0.52\nRun 3 (GPT- 4o) 77 10.0  < 0.001*\nNon-Subscription Based- Other LLM\nC4a1 66 14.3 0.09\nClaude 87 11.4  < 0.001*\nDeepSeek 78 13.0  < 0.001*\nGemini 78 13.9  < 0.001*\nLeChat 72 14.1 0.005*\nLlama 3.1 63 14.4 0.22\nLlama 3.3 55 12.8 0.61\nMistral 49 12.0 0.02#\nPhi 68 20.6 0.09\nQuen 62 12.8 0.49\nFig. 1 Boxplot of students’ percentage scores with overlays of mean percentage scores for Chat GPT: RUN 1, RUN 2 & RUN 3 per question\nPage 5 of 8\nRamlogan et al. BMC Medical Education          (2025) 25:727 \n \nTable 2 Comparison of Chat GPT- 4 Run 1 to Run 2 with excerpt questions and responses\nQuestion Chat GPT- 4 RUN 1 Chat GPT- 4 RUN 2 Comments\n2. What is the management of a patient \nwith these BPE codes?\n..indicate gingivitis (Code 1&2) with * furcation \ninvolvement\n..* indicating bleeding on probing RUN 2 incorrectly identifies * as bleeding \non probing and doesn’t mention furcation \ninvolvement or what Code 1 or 2 represents\n3. Give the vertical furcation classification \nas defined by Tarnow and Fletcher\n..defines Class A as 1–3 mm, Class B as 4–6 mm \nand Class C As > 6 mm\n..includes Grade I (incipient), Grade II (moderate) \nand Grade III through and through\nIncorrect information ascribed to the vertical clas-\nsification (Tarnow and Fletcher) in RUN 2\n8. How would you manage a gingival recession \ndefect of RT3?\n..management of.. right upper third molar \n(RT3) depends on the severity… aesthetics \nand hypersensitivity\n..includes surgical intervention such as CT \ngraft or GTR to cover exposed root surface \nand increase soft tissue volume\nRUN 1 incorrectly identified what RT3 meant \n(new classification). RUN 2 didn’t define this \nparameter and gave incorrect options\n17. What is the odds ratio for periodontitis \nin a diabetic?\n..in diabetics is estimated to be 2–3 times higher \ncompared to non-diabetics suggesting \nan increased risk\n..in diabetics is sig higher vs non-diabetics indi-\ncating a strong association between diabetes \nand the risk of periodontal disease\nLess detail in RUN2 compared to RUN1. Numeri-\ncal value of odds ratio left out in RUN 2\nPage 6 of 8Ramlogan et al. BMC Medical Education          (2025) 25:727 \nWith regards to questions 5 and 10 which addressed \nthe current knowledge of the latest classification for \nperiodontal disease (Staging and Grading), all three \nruns (‘Run 1’ , ‘Run 2’ , ‘Run 3’) underperformed in rela -\ntion to the students (Fig.  1). ‘Run 1’ also underperformed \nin relation to the students in question 2 due to a lack of \ndetail for Basic Periodontal Examination (BPE) codes and \nhigher order analysis of the clinical scenario [14]. In gen -\neral, for ‘Run 1’ full marks were not attained due to the \nuse of older terminology, lack of detail and lack of expla -\nnation for some of the answers. This older terminology \nincluded the former periodontology classification groups \nof Chronic and Aggressive Periodontitis.\nIn ‘Run 2’ , there were more incomplete answers lacking \ndetails which were previously given in ‘Run 1’ (Table  2; \nrefer to question 17). There were also wrong and inaccu -\nrate answers in ‘Run 2’ which were not given in ‘Run 1’ \n(Table 2; as shown in question 3). There continued to be \na problem with defining the new periodontology termi -\nnology in relation to gingival recession defects as viewed \nin question 8 for both ‘Run 1’ and ‘Run 2’ (Table  2). For \nquestion 2, the BPE codes were correctly identified in \n‘Run 1’ but were incorrectly identified in ‘Run 2’ (Table 2).\n‘Run 3’ was able to correctly identify the gingival reces -\nsion type, RT3 in question 8 unlike ‘Run 1’ and ‘Run 2’ . \nHowever, overall answers in ‘Run 3’ were similar to ‘Run \n1’ despite the separation of a 15 month timeline.\nThe comparison of the other non-subscription Large \nLanguage Models (LLMs) to student performance in \nthe written exam was also shown in Table  1. All models \nexcept Llama 3.3 and Mistral performed equally or better \nthan the students in the written exam. The models which \nproduced a statistically significantly (t-test) higher mean \npercentage score compared to the students were Claude \n(p < 0.001), DeepSeek (p < 0.001), Gemini (p < 0.001) \nand Le Chat (p = 0.005). These better performing mod -\nels together with ‘Run 1’ and ‘Run 3’ are graphically rep -\nresented across the 20 exam questions in relation to the \nstudent performance (Fig. 2).\nClaude produced statistically significantly (t-test) \nhigher mean scores compared to both ‘Run 1’ (p = 0.003) \nand ‘Run3’ (p = 0.002). However, Le Chat generated sta -\ntistically significantly (t-test) lower mean scores com -\npared to both ‘Run 1’ (p = 0.04) and ‘Run3’ (p = 0.04). \nBoth DeepSeek and Gemini did not produce statistically \nsignificant differences (t-test) in the mean scores from \neither ‘Run 1’ (DeepSeek p = 0.55; Gemini p = 0.61) or \n‘Run3’ (DeepSeek p = 0.44; Gemini p = 0.47).\nDiscussion\nBoth Chat GPT- 4 ‘Run 1’ and GPT- 4o ‘Run 3’ attained \nstatistically significantly (t-test; p < 0.001) greater per -\ncentage scores (‘Run 1′ 78%, ‘Run 3′ 77%) compared \nto students’ mean percentage (60%) score in this clini -\ncally oriented short-answer questions format of the final \nwritten periodontology exam. The performance of Chat \nGPT- 4/GPT4o in this format exam was similar to that \nreported in the literature for short essay questions [11]. \nThe Chat GPT- 4/GPT4o scores were also equivalent to \nthat of the best performing student in the class. When \nconsidering only ‘Run 1’ or ‘Run 3’ , Chat GPT- 4 or the \nupdated GPT- 4o may be assumed to be a good resource \nthat may aid students in providing information summa -\nries and model answers.\nHowever, the major limitations are the generalization \nof answers, lack of detail and the use of outdated termi -\nnology. Some of the information related to the classifica -\ntion of periodontal disease was dated due to the use and \naccess of older data sources by Chat GPT- 4/GPT- 4o. \nThe published knowledge cutoffs for Chat GPT- 4 and \nGPT- 4o models are December, 2023 and October, 2023 \nrespectively [15]. Although the new periodontal classifi -\ncation was published in 2018, five years earlier to these \ncutoffs, the information remains inaccurate or outdated \n[16]. This may indicate the drawback of Chat GPT- 4/\nGPT- 4o to provide current information with advancing \nacademic and clinical standards in dentistry. Students \ndeemed questions 5 and 10 as easier and performed bet -\nter in these questions on periodontal disease classifi -\ncation than Chat GPT- 4/GPT- 4o. This may be due to \nbetter clinical pedagogical strategy and motivation by \nthe lecturers in these current topics which reinforces the \nvalue of the human element in clinical education.\nChat GPT- 4, ‘Run 2’ underperformed compared to \nboth Chat GPT- 4, ‘Run 1’ and GPT- 4o, ‘Run 3’ . This was \nan unexpected outcome as it was assumed that this LLM \nwould produce either comparable or improved answers \nwith time through progressive learning. However, Chat \nGPT- 4, ‘Run 2’ surprisingly produced some incorrect \ninformation that was not generated in ‘Run 1’ . There was \nalso a reduction in detailed information in ‘Run 2’ com -\npared to ‘Run 1’ . Errors at the discipline speciality level \nby Chat GPT were also previously reported in the litera -\nture [6]. Open AI had suggested that while Chat-GPT- 4 \nwas an improvement on previous versions, it may still \nbe prone to ‘hallucinations’ and reasoning errors [17]. \nStudents reported difficulty with question 8 on gingival \nrecession classification and performed weakly on this \nquestion as it also required a higher order of analysis. \nChat GPT- 4 in ‘Run 2’ was unable to interpret the clini -\ncal scenario correctly in question 8 and scored below 40% \nwhile ‘Run 3’ performed better at 75%.\nA lack of detail may have been a random outcome \nas Chat GPT attempts to deliver original text in each \nresponse. This generated originality may be undetect -\nable by plagiarism software as reported by other authors \nPage 7 of 8\nRamlogan et al. BMC Medical Education          (2025) 25:727 \n \n[3]. Other possibilities include inherent LLM limitations \nat the level of the Chat GPT- 4 provider which may be \nconstrained by variations in online traffic, availability of \ndatasets and support resources. At the time of the study, \nthe current version of the subscription-based Chat GPT \n4 was used in this study to facilitate the best possible use \nof this LLM technology. Inaccuracies may also be the \nresult of biased data input sources which may skew the \ngenerated responses [9]. The limitation in word count \nwas applied equally to both ‘Run 1’ and ‘Run 2’ of Chat \nGPT- 4 and thus was not a confounding factor. However, \nin a non-examination scenario, Chat GPT- 4 may gener -\nate more complete responses if it is not limited by word \ncount.\nAmong the other investigated LLMs, Claude, Deep -\nSeek, Gemini and Le Chat performed statistically sig -\nnificantly better than the students. DeepSeek and Gemini \nperformed at the level of Chat GPT- 4/GPT- 4o while Le \nChat performed below the level of Chat GPT- 4/GPT- 4o. \nClaude performed statistically significantly better than \nChat GPT- 4/GPT- 4o as the best overall performing \nLLM with more comprehensive answers. Claude gave the \nbest responses to the questions on periodontal disease \nclassification but not on gingival recession classifica -\ntion. This pilot study may potentially indicate the value \nof Claude in generating responses for clinically oriented \nshort answer questions in periodontology. The non-sub -\nscription LLMs may have been limited in their full per -\nformance capacity as they were openly accessible to the \ngeneral public. Thus some reservation should be applied \nto this comparative analysis of the non-subscription \nLLMs.\nA study limitation was the use of one class of students \nas the reference standard. The performance of students \nmay vary from year to year and this one diet of exam \nresults may not be reflective of all students in periodon -\ntology. Additionally, as all students were included and a \npower calculation was not attempted, the results may be \ndeemed as a pilot study. A further limitation may be the \napplicability of this study to only (1) clinically oriented \nshort answer questions (format) and (2) periodontol -\nogy (subject). Chat GPT- 4 was used to generate answers \nwithout further prompting or interaction. This would \nhave restricted the refinement and correction of answers \nexpected in an ongoing chat with a LLM.\nConclusion\nChat GPT- 4/GPT- 4o, Claude, DeepSeek and Gemini may \ngenerate adequate clinically oriented short answers at the \ndental undergraduate level in periodontology. Variable \nFig. 2 Mean percentage score by question for statistically significant LLMs compared to students\nPage 8 of 8Ramlogan et al. BMC Medical Education          (2025) 25:727 \nresponses as demonstrated by Chat GPT- 4 over time and \ninaccuracies stemming from outdated information, hallu-\ncinations and other inherent LLM limitations may impact \non health care professional training. LLMs may be better \nsuited for use as an adjunctive educational aid to delivered \ncourses guided by the required tutors due to the necessity \nfor human oversight and validation. A more productive and \ndynamic use of this LLM would involve further interactive \nenquiry and critical engagement of the generated content.\nFuture work may look at the changes in the longitudi -\nnal responses of the other high performing non subscrip -\ntion LLMs in this study. As LLMs evolve and develop, the \napplication to more complex data such as clinical photos \nand radiographs may also be investigated. Finally, further \ninvestigations may also look at the impact of use of LLMs \nin assessments, identification of plagiarism and the impli-\ncations for academic integrity.\nAcknowledgements\nNot applicable\nAuthor’s contributions\nSR1, VR and SR2 were involved in the study formulation, data collection, \nanalyses and preparation of the manuscript. SR1 and VR were involved in the \nformulation of the exam, the marking scheme and correction of all scripts. SR2 \nwas involved in the generation of the Chat GPT reports and the blinding of \nthe script. All authors reviewed the manuscript.\nFunding\nThere was no funding for this study.\nData availability\nThe datasets generated and/or analysed during the current study are not \npublicly available but are available from the corresponding author on reason-\nable request.\nDeclarations\nEthics approval and Consent\nEthical approval was granted by the Campus Research Ethics Committee, The \nUniversity of the West Indies, St Augustine Campus (CREC-SA.3172/03/2025). \nThe study adhered to the Declaration of Helsinki. Individual student informed \nconsent to participate was not required by the Campus Research Ethics \nCommittee, The University of the West Indies, St Augustine Campus. This \nstudy relates to the following exemption regulation: FGP .P2 C 2010/2011 The \nUniversity of the West Indies Policy and Procedures on Research Ethics, The \nSchool for Graduate Studies and Research on page 11 refers to\"Exemptions \nfrom Review-Educational Tests and Measurements\" .\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 9 May 2024   Accepted: 18 April 2025\nReferences\n 1. Open AI, Release notes. At: https:// help. openai. com/ en/ artic les/ 68254 53- \nchatg pt- relea se- notes#h_ 0eb27 84e11. Accessed  27th March 2024.\n 2. Hello GPT4o. At: https:// openai. com/ index/ hello- gpt- 4o/. Accessed  25th \nJanuary 2025.\n 3. Moritz S, Romeike B, Stosch C, Tolks D. Generative AI (gAI) in medical \neducation: Chat-GPT and co. GMS J Med Educ. 2023;40(4):Doc54. https:// \ndoi. org/ 10. 3205/ zma00 1636. PMID: 37560050; PMCID: PMC10407583.\n 4. Gilson A, Safranek CW, Huang T, Socrates V, Chi L, Taylor RA, Chartash \nD. How Does ChatGPT Perform on the United States Medical Licens-\ning Examination (USMLE)? The Implications of Large Language Models \nfor Medical Education and Knowledge Assessment. JMIR Med Educ. \n2023;8(9):e45312. https:// doi. org/ 10. 2196/ 45312. Errat um. In: JMIRM \nedEduc. 2024F eb27; 10: e57594. PMID: 36753 318; PMCID: PMC99 47764.\n 5. Wójcik S, Rulkiewicz A, Pruszczyk P , Lisik W, Poboży M, Domienik-Karłowicz \nJ. Reshaping medical education: Performance of ChatGPT on a PES medi-\ncal examination. Cardiol J. 2023 Oct 13. https:// doi. org/ 10. 5603/ cj. 97517. \nEpub ahead of print. PMID: 37830257.\n 6. Johnson D, Goodman R, Patrinely J, Stone C, Zimmerman E, Donald R, \nChang S, Berkowitz S, Finn A, Jahangir E, Scoville E, Reese T, Friedman D, \nBastarache J, van der Heijden Y, Wright J, Carter N, Alexander M, Choe \nJ, Chastain C, Zic J, Horst S, Turker I, Agarwal R, Osmundson E, Idrees K, \nKieman C, Padmanabhan C, Bailey C, Schlegel C, Chambless L, Gibson \nM, Osterman T, Wheless L. Assessing the Accuracy and Reliability of AI-\nGenerated Medical Responses: An Evaluation of the Chat-GPT Model. Res \nSq [Preprint]. 2023 Feb 28:rs.3.rs-2566942. https:// doi. org/ 10. 21203/ rs.3. \nrs- 25669 42/ v1. PMID: 36909565; PMCID: PMC10002821.\n 7. Kumari KS, Anusha KS. An Esthetic Approach for Rehabilitation of Long-\nSpan Edentulous Arch Using Artificial Intelligence. Cureus. 2023;15(5): \ne38683. https:// doi. org/ 10. 7759/ cureus. 38683. PMID: 37292 565; PMCID: \nPMC10 244077.\n 8. Mago J, Sharma M. The Potential Usefulness of ChatGPT in Oral and Maxil-\nlofacial Radiology. Cureus. 2023;15(7):e42133. https:// doi. org/ 10. 7759/ \ncureus. 42133. PMID: 37476 297; PMCID: PMC10 355343.\n 9. Thorat VA, Rao P , Joshi N, Talreja P , Shetty A. The role of Chatbot GPT tech-\nnology in undergraduate dental education. Cureus. 2024;16(2):e54193. \nhttps:// doi. org/ 10. 7759/ cureus. 54193.\n 10. Kumar R. Faculty members’ use of artificial intelligence to grade student \npapers: a case of implications. Int J Educ Integr. 2023;19:9. https:// doi. org/ \n10. 1007/ s40979- 023- 00130-7.\n 11. Ali K, Barhom N, Tamimi F, Duggal M. ChatGPT-A double-edged sword for \nhealthcare education? Implications for assessments of dental students. \nEur J Dent Educ. 2024;28(1):206–11. https:// doi. org/ 10. 1111/ eje. 12937. \nEpub 2023 Aug 7 PMID: 37550893.\n 12. Lazy Programmer. Best LLMs of 2024 Better Than GPT-4 / ChatGPT At: \nhttps:// lazyp rogra mmer. me/ best- llms- of- 2024- better- than- gpt-4- chatg \npt/. Accessed  28th January 2025\n 13. Lucas HC, Upperman JS, Robinson JR. A systematic review of large \nlanguage models and their implications in medical education. Med Educ. \n2024;58(11):1276–85. https:// doi. org/ 10. 1111/ medu. 15402. Epub 2024 \nApr 19 PMID: 38639098.\n 14. Basic Periodontal Examination (BPE). British Society of Periodontology \n(BSP). At: https:// www. bsper io. org. uk/ assets/ downl oads/ BSP_ BPE_ Guide \nlines_ 2019. pdf Accessed 25th January 2025.\n 15. OpenAI Platform. Model Overview. At: https:// platf orm. openai. com/ docs/ \nmodels/ gpt- 4o# models- overv iew Accessed  28th January 2025.\n 16. Papapanou PN, Sanz M, Buduneli N, Dietrich T, Feres M, Fine DH, Flemmig \nTF, Garcia R, Giannobile WV, Graziani F, Greenwell H, Herrera D, Kao RT, \nKebschull M, Kinane DF, Kirkwood KL, Kocher T, Kornman KS, Kumar PS, \nLoos BG, Machtei E, Meng H, Mombelli A, Needleman I, Offenbacher S, \nSeymour GJ, Teles R, Tonetti MS. Periodontitis: Consensus report of work-\ngroup 2 of the 2017 World Workshop on the Classification of Periodontal \nand Peri-Implant Diseases and Conditions. J Periodontol. 2018;89(Suppl \n1):S173–82. https:// doi. org/ 10. 1002/ JPER. 17- 0721. PMID: 29926951.\n 17. Open AI (2024). GPT-4 Technical report. At: https:// arxiv. org/ pdf/ 2303. \n08774. pdf. Accessed  27th March 2024.\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations."
}