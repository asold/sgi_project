{
  "title": "Towards Benchmarking and Improving the Temporal Reasoning Capability of Large Language Models",
  "url": "https://openalex.org/W4385571357",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2242520679",
      "name": "Qingyu Tan",
      "affiliations": [
        "Alibaba Group (Cayman Islands)",
        "National University of Singapore"
      ]
    },
    {
      "id": "https://openalex.org/A2146810117",
      "name": "Hwee Tou Ng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2160800796",
      "name": "Lidong Bing",
      "affiliations": [
        "Alibaba Group (Cayman Islands)",
        "National University of Singapore"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2798898418",
    "https://openalex.org/W4288725442",
    "https://openalex.org/W2964015378",
    "https://openalex.org/W3176757281",
    "https://openalex.org/W3195376057",
    "https://openalex.org/W4281485769",
    "https://openalex.org/W2890410208",
    "https://openalex.org/W2736601468",
    "https://openalex.org/W2996371683",
    "https://openalex.org/W2251220668",
    "https://openalex.org/W2970170773",
    "https://openalex.org/W2251325107",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W4226278401",
    "https://openalex.org/W4221151629",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2283196293",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W2127795553",
    "https://openalex.org/W2573189311",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W3209273204",
    "https://openalex.org/W4385571452",
    "https://openalex.org/W4385573294",
    "https://openalex.org/W4283793506",
    "https://openalex.org/W4286987939",
    "https://openalex.org/W3099246072",
    "https://openalex.org/W3034671305",
    "https://openalex.org/W4385571238",
    "https://openalex.org/W3099700870",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W3170629081"
  ],
  "abstract": "Reasoning about time is of fundamental importance. Many facts are time-dependent. For example, athletes change teams from time to time, and different government officials are elected periodically. Previous time-dependent question answering (QA) datasets tend to be biased in either their coverage of time spans or question types. In this paper, we introduce a comprehensive probing dataset TempReason to evaluate the temporal reasoning capability of large language models. Our dataset includes questions of three temporal reasoning levels. In addition, we also propose a novel learning framework to improve the temporal reasoning capability of large language models, based on temporal span extraction and time-sensitive reinforcement learning. We conducted experiments in closed book QA, open book QA, and reasoning QA settings and demonstrated the effectiveness of our approach.",
  "full_text": "Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics\nVolume 1: Long Papers, pages 14820–14835\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nTowards Benchmarking and Improving the Temporal Reasoning Capability\nof Large Language Models\nQingyu Tan\n∗1, 2 Hwee Tou Ng\n†2 Lidong Bing1\n1DAMO Academy, Alibaba Group\n2Department of Computer Science, National University of Singapore\n{qingyu.tan,l.bing}@alibaba-inc.com\n{qtan6,nght}@comp.nus.edu.sg\nAbstract\nReasoning about time is of fundamental impor-\ntance. Many facts are time-dependent. For ex-\nample, athletes change teams from time to time,\nand different government officials are elected\nperiodically. Previous time-dependent question\nanswering (QA) datasets tend to be biased in\neither their coverage of time spans or question\ntypes. In this paper, we introduce a comprehen-\nsive probing dataset TEMP REASON to evaluate\nthe temporal reasoning capability of large lan-\nguage models. Our dataset includes questions\nof three temporal reasoning levels. In addition,\nwe also propose a novel learning framework\nto improve the temporal reasoning capability\nof large language models, based on temporal\nspan extraction and time-sensitive reinforce-\nment learning. We conducted experiments in\nclosed book QA, open book QA, and reasoning\nQA settings and demonstrated the effectiveness\nof our approach1.\n1 Introduction\nIn recent years, large language models (LLMs)\nhave achieved significant success in many natural\nlanguage processing (NLP) tasks, such as natural\nlanguage understanding (NLU) (Fei et al., 2023),\ninformation extraction (IE) (Ding et al., 2023), and\nquestion answering (QA) (Ye et al., 2023; Zhao\net al., 2023). Many facts and answers are dependent\non their related time scopes, such as ‘What soccer\nclub was Lionel Messi playing for?’. Chia et al.\n(2022) has pointed out around 48% of the qualifiers\nin the widely-used knowledge base Wikidata (Vran-\ndeˇci´c and Krötzsch, 2014) are time-related. That\nis, a significant number of the knowledge triples\nin the Wikidata KB have their expiry dates. Cor-\nrect understanding of temporal concepts is crucial\n∗Qingyu Tan is under the Joint PhD Program between\nAlibaba and NUS.\n†Corresponding author.\n1Our code and data are released onhttps://github.com/\nDAMO-NLP-SG/TempReason\nfor language models to be successful in real-world\napplications. To examine the temporal reasoning\ncapabilities of LLMs, the Time-Sensitive Question\nAnswering (TSQA) task has been proposed and sev-\neral evaluation datasets were published for research\npurposes. The Time-sensitive QA dataset (Chen\net al., 2021) and the TEMP LAMA dataset (Dhin-\ngra et al., 2022) were constructed based on the\nWikidata temporal KB. StreamingQA (Liska et al.,\n2022) was constructed by news article collections\nin English WMT challenges from 2007 to 2020.\nOne consensus of prior work is that time-sensitive\nQA is a challenging task and its performance is still\nfar below human performance. However, they did\nnot provide a systematic analysis of LM’s tempo-\nral reasoning capability. In this paper, we aim to\nsystematically analyze such capability and identify\nthe strengths and weaknesses of LMs on temporal\nreasoning.\nAs shown in Figure 1, humans’ understanding\nof temporal reasoning could be broken down into\nthree levels: time-time (L1) relation, time-event\n(L2) relation, and event-event (L3) relation. For the\nunderstanding of time-time relations, humans can\neasily determine the relation between two times-\ntamps t1 and t2 on the time axis. For example,\nwhen humans are asked ‘What is the year after\n2020?’, they are able to answer this question with-\nout any external information. This level of temporal\nunderstanding could be regarded as a set of logic\nrules and is highly generalizable across different\ntimes, while this type of reasoning was overlooked\nin prior TSQA research (Ning et al., 2020; Chen\net al., 2021; Dhingra et al., 2022). For time-event\nrelations, the reasoning process requires grounding\nevents to their specific time ranges. In this paper,\nthe concept of events includes time-dependent facts.\nHumans either memorize a large number of time-\nevent pairs or need to rely on relevant contexts\nto deduce such relations. An example question is\n‘What soccer club was Lionel Messi playing for in\n14820\nLevel - 1\nTime-Time Relation\nAbstractive concept, can be \nviewed as a set of logic rules.\nOnce human understands the \nconcepts, it is easily \ngeneralizable.\nExample Question:\nWhat is the year after 2010?\nTarget:\n2011\nLevel - 2\nTime-Event Relation\nKnowledge-intensive concept, \nwhere human will also need to \nmemorize such relations, or \nleverage external context to \ndeduce such relations.\nExample Question:\nWhat team did Leo Messi play for \nin 2010?\nTarget:\nBarcelona\nLevel - 3\nEvent-Event Relation\nRequires combination of \nmemorization, deduction and \nunderstanding. \nExample Question:\nWhat team did Leo Messi play for \nafter Barcelona?\nTarget:\nParis Saint-Germain\nFigure 1: Illustration of three levels of understanding towards time.\nDec 2010?’, where a time is specified in the ques-\ntion, and the answer changes based on the given\ntime. If this question is posed to a person who\nis unfamiliar with sports, this person also needs\nexternal information to provide the answer. An-\nswering this type of questions requires information\nretrieval and temporal grounding. For event-event\nrelations, there are multiple reasoning paths to de-\ntermine such relations. One possible path is to\nfirst identify the timestamps of different events and\nperform time-time reasoning. Another path is to\nsearch for the textual cues of relative relation, such\nas ‘before’, ‘after’, ‘during’, and ‘simultaneous’.\nWe first conducted a simple preliminary exper-\niment for probing LLM’s L1 temporal reasoning\ncapability. We found that not only do LMs perform\npoorly on the time-time relation task, but they are\nalso heavily biased in favor of contemporary years\n(2000 - 2020). This may be due to the imbalanced\nterm frequencies in the pre-training corpora. Most\nLLMs (such as BERT, GPT, and T5) are pre-trained\non raw texts from a snapshot at a specific times-\ntamp, typically around 2018 to 2020. Therefore,\nthe time expression vocabulary is highly dependent\non term frequencies in the pre-training corpora.\nTypically, year tokens that occur frequently will\nhave a smaller index in the vocabulary and the un-\ncommon years generally have larger indices or will\nbe split into subtokens. Take the T5 tokenizer as\nan example, the year ‘2014’ is tokenized as ‘2014’,\nhowever, the year ‘2021’ is tokenized as ‘20’ and\n‘21’. This means that language models only learn\nthe co-occurrences of time expressions and their\ncontext.\nGiven such findings, we found that the recently\nproposed TSQA TEMP LAMA dataset has several\nmain drawbacks. Firstly, the time span of the\ndataset is only from 2010 to 2020, which is a highly\nbiased distribution in favor of LM. Secondly, it only\nfocused on the questions of time-event relations. To\novercome these shortcomings, we created a more\ncomprehensive TSQA benchmark TEMP REASON ,\nwhich spans a longer time range and all three types\nof temporal understanding. We conducted compre-\nhensive experiments in closed book QA, open book\nQA, and reasoning QA settings. We found that the\ntemporal reasoning capabilities of LLMs are highly\nvariable with respect to the reference time in the\nquestion. LLMs perform well on the contemporary\nyears and poorly on low-resource years.\nMoreover, we proposed a novel temporal learn-\ning framework based on temporal span extraction\nand time-sensitive reinforcement learning. Our pro-\nposed framework encourages LMs to generate tem-\nporally correct answers while penalizing predic-\ntions that do not satisfy the temporal constraints.\nExperimental results showed that our proposed\nbenchmark TEMP REASON provides a more com-\nprehensive evaluation for LM’s temporal reasoning\ncapability and our model consistently outperforms\nstrong baselines.\nRef. Year Question Target\n2011 What is the year x years before 2011? 2011 - x\n2010 What is the year before 2010? 2009\n1949 What is the year x years after 1949? 1949 + x\n1905 What is the year after 1905? 1906\nTable 1: Templates used for year prediction (yearly\nlevel). The reference year and interval x are randomly\ngenerated, where the reference year is within a specified\ntime range and x ≤10. All the answers to this question\nare numeric representations of years.\n2 Preliminaries\nWe aim to examine the capability of LMs for simple\nyear prediction. We first design a set of question\ntemplates that reflects the basic concepts of tempo-\n14821\nral prediction, as shown in Table 1. Questions of\nthese kinds can be easily answered by humans and\nthis understanding is highly generalizable across\nthe years, and all the expected answers are years\nin numeric form. In order to have a more com-\nprehensive understanding of temporal expressions,\nwe divide 1900 to 2040 into seven 20-year time\nperiods. Then, we randomly generate 400 ques-\ntions for each 20-year time period. We then use\nthree language models to make predictions on such\nquestions. The first LM is T5-large model fine-\ntuned on the Natural Question dataset (T5-L-NQ,\nKwiatkowski et al., 2019). This QA dataset is one\nof the largest open domain QA datasets. Roberts\net al. (2020) has demonstrated that language mod-\nels fine-tuned on such data can achieve compet-\nitive performance on the open domain QA task.\nThe second LM is FLAN-T5-Large (Wei et al.,\n2022) model. This model is instruction-tuned on\ndata of more than 60 NLP tasks. The fine-tuned\nmodel demonstrated competitive zero-shot reason-\ning capability, and achieved strong performance on\nmany natural language understanding and genera-\ntion tasks. The third model is the popular ChatGPT\n(Ouyang et al., 2022) model. To ensure that the pre-\ndictions are consistent, we used the gpt-3.5-0301\nversion of ChatGPT. We aim to evaluate the tem-\nporal reasoning capability of the three language\nmodels. We evaluate the answers using the follow-\ning three metrics: (1) exact match (EM), which is\na standard metric for QA. Besides, since the ex-\npected answers are numeric, we also evaluate the\nanswers by (2) mean absolute error (MAE) and\n(3) trend accuracy (Trend Acc). Trend accuracy is\ncalculated by whether the predicted year is before\nor after the reference year. If the trend is correct,\nthe prediction is deemed to be correct.\nThe experimental results on year prediction are\nshown in Table 2. We report the scores of T5-L-\nNQ on the left, FLAN-T5-L in the middle, and\nChatGPT on the right. From these experiments, we\nhave several interesting observations: (1) The Chat-\nGPT model is able to solve this problem with high\naccuracy (99.6 overall EM). However, it still made\na few mistakes in the 1900-1940 time period. (2)\nThe first two LMs (T5-L-NQ and FLAN-T5-L) are\nbiased towards contemporary time ranges. We\ncan clearly see that the EM scores between 2000\nto 2020 are significantly higher than the rest of the\ntime ranges. This could be the result of the higher\nterm frequencies of the contemporary year tokens\nTime Range EM(↑) MAE(↓) Trend Acc(↑)\n1900-1920 17.5/6.8/99.5 28.0/7.4/0.0 99.5/96.8/100\n1920-1940 31.5/1.8/98.9 16.4/11.9/0.1 94.5/94.5/100\n1940-1960 17.5/3.3/100 7.7/9.2/0.0 100 /91.0/100\n1960-1980 22.5/3.5/100 17.1/7.5/0.0 94.0/92.0/100\n1980-2000 23.0/10.0/100 7.9/6.9/0.0 98.5/100/100\n2000-2020 47.5/20.0/100 51.2/2.3/0.0 97.0/100/100\n2020-2040 23.5/11.3/100 15.7/8.9/0.0 84.5/83.8/100\nAverage 26.1/8.1/99.6 20.6/7.7/0.0 95.4/94.0/100\nTable 2: Evaluation results of T5-L-NQ2 (Raffel et al.,\n2020) (left), FLAN-T5-large3 (Wei et al., 2022) (mid-\ndle), and ChatGPT (right) models on the year prediction\ntask across different time ranges. Bold scoresrefer to\nthe best performance of each model in each column.\nin the pre-training corpora. Since many large LMs\nare trained and released after 2018, the pre-training\ncorpora may contain more year expressions that are\ncloser to that date. In contrast, the first two LMs\nperform significantly worse in the past (1900-2000)\nand the future (2020-2040) years. (3) The first two\nLMs lack numeric reasoning abilitywith respect\nto time. The answers provided by these LMs for the\ntime prediction questions are in numeric form, indi-\ncating that the LMs understand what the questions\nare asking. However, the EM scores are all around\nthe 20-30 range, except for T5-L-NQ in the 2000-\n2020 time range. This indicates that LMs have poor\nestimation of temporal concepts. Besides, we find\nthat the FLAN-T5-L model has significantly lower\nEM scores compared to T5-L-NQ, but achieves\nlower MAE estimations across most of the time\nranges. This indicates that instruction tuning im-\nplemented in FLAN has implicitly improved the\nnumeric reasoning capability of T5. (4) On the\nother hand, All LMs are good at catching (be-\nfore/after) trends, indicating that at least the LMs\nunderstand the concepts of before/after well. We\ncan see that all LMs achieve over 90% performance\nacross time ranges before 2020. However, for the\nfirst two LMs, this capability is not able to general-\nize to the future, as the performance in 2020-2040\nis significantly worse than in other time periods.\n3 Comprehensive Benchmark for\nTemporal Reasoning\n3.1 T EMP REASON Dataset\nBased on the findings of the previous section, we\nfound that the recently proposed TEMP LAMA\nTSQA dataset (Dhingra et al., 2022) has several\n2https://huggingface.co/google/\nt5-large-ssm-nq\n3https://huggingface.co/google/flan-t5-large\n14822\nTrain Dev Test\nTime Range 1014-2022 634-2023 998-2023\nL1-Questions 400,000 4,000 4,000\nL2-Questions 16,017 5,521 5,397\nL3-Questions 13,014 4,437 4,426\nSubjects 3,000 1,000 1,000\nFacts 16,017 5,521 5,397\nFacts/subjects 5.3 5.5 5.4\nTable 3: Dataset statistics of TEMP REASON .\nmajor limitations. Firstly, it only contains ques-\ntions from 2010 to 2020, which are highly in fa-\nvor of LM’s temporal reasoning biases. Secondly,\nthe TEMP LAMA dataset is heavily biased towards\nlong-duration facts, as 70.69% of the questions of\nTEMP LAMA have the most frequent answer for\na given subject. That is, the TEMP LAMA dataset\nmay encourage models to learn shortcuts to mem-\norize the most frequent answers instead of learn-\ning temporal reasoning capability. If the research\non time-sensitive question answering only focuses\non adaptation to a short period of time, the main-\ntenance and continual adaptation shall be highly\nexpensive. As shown in the previous section, lan-\nguage models perform poorly on past and future\ntime spans. If the language model is not able to\nunderstand the changes from the past to the present,\nit is highly difficult for this model to understand the\nevolution from the present to the future. In order to\nprobe the temporal reasoning ability in a more sys-\ntematic manner, we constructed a new comprehen-\nsive dataset TEMP REASON . For the L1 time-time\nrelation reasoning, we extend the year prediction\ntask to month prediction, since year prediction can\nbe enumerated by several thousands of examples\nand LMs may simply memorize such examples.\nSpecifically, we randomly pick a reference time t\nwithin a specific time range and then synthesize\nquestions with respect to that time. The questions\nhave the form of ‘What is the date x years and y\nmonths before/after t?’. In this way, we can ran-\ndomly generate L1 questions and answers within\nthe time period. To avoid data leakage, we make\nsure each generated question is unique. We then\nrandomly split the questions to train, dev, and test\nsets. To evaluate the generalizability of L1 tempo-\nral reasoning, we also create a future test set from\n2022 to 2040.\nFor L2 and L3 reasoning, similar to Dhingra\net al. (2022) and Chen et al. (2021), we also lever-\nage the Wikidata KB as the knowledge source. We\nfirst preprocess the 20 Nov 2022 dump of the Wiki-\ndata (Vrandeˇci´c and Krötzsch, 2014) knowledge\nbase (KB) to extract all time-dependent facts. We\nthen keep the facts of 10 time-sensitive relations\nmentioned in the TEMP LAMA dataset. We pro-\ncess the knowledge triples and qualifiers into quin-\ntuplet format, (s,r,o,t s,te), where s is the sub-\nject, r is the relation, o is the object, ts and te\nare the start time and end time of this fact. We\ngroup all the temporal facts by s and r. In this\nway, facts in the same group are all relevant to\nthe subject s. The group of facts can be denoted\nas S = {(s,r,oi,tsi ,tei )|i∈1...N}and they are\nsorted chronologically, where N is the number of\nfacts within a group. Since we mainly want to fo-\ncus on questions whose answers change with time,\nwe only keep the groups that contain three or more\ntemporal facts. In this way, we make sure that each\ngroup has at least three time-dependent answers.\nMoreover, since the Wikidata KB is highly class-\nimbalanced, we only keep a maximum of 2,000 sub-\njects for each relation type. We then create cloze-\nstyle questions based on time-dependent facts. For\nthe time-event (L2) type of questions, we randomly\nselect a time tr between ts and te, and we then cre-\nate a question with the query (s,r,?,tr) and a set\nof manually-defined question templates. The tem-\nplates can be found in Table 13 in Appendx A. For\nthe event-event (L3) type of questions, we first iden-\ntify the ‘before/after’ relation pairs within group\nS (we only keep the 1-hop pairs). We then cre-\nate the event-event question for each ‘before/after’\npair using similar templates of the L2 questions\n(Table 13). The statistics of our TEMP REASON\ndataset can be found in Table 3. We also compared\nour datasets with prior works in Appendix C\n3.2 Problem Settings\nThe time-sensitive question answering (TSQA)\ntask is formally defined as follows: given an input\nquestion and its corresponding time (Figure 2), the\nmodel is asked to output the answer of this question,\nand the answers are evaluated by token-level F1 and\nexact match (EM) scores. Intuitively, the difficulty\nof the TSQA task is highly dependent on the con-\ntext provided for each question. The challenges of\nthe TSQA task can be broken down into three lev-\nels: (1) Answer Retrieval. The first challenge of\nTSQA is finding the possible answers, which is the\nsame challenge as normal open-domain question\nanswering. For questions in TEMP REASON , each\nquestion may have 5.3 to 5.5 possible answers (Ta-\n14823\nExample Questions:\nL1 Question: What is the time 4 years and \n5 months after May 2010?\nAnswer: Oct 2014\nL2 Question: What team did Leo Messi \nplay for in May 2020?\nAnswer: FC Barcelona\nL3 Question: What team did Leo Messi \nplay for after FC Barcelona?\nAnswer: Paris Saint-Germain\nWikipedia Page:\nLionel Andrés Messi also known as Leo \nMessi, is an Argentine professional footballer \nwho plays as a forward for Ligue 1 club Paris \nSaint-Germain and captains the Argentina \nnational team. Widely regarded as one of the \ngreatest players of all time, Messi has won a \nrecord seven Ballon d'Or awards, a record six \nEuropean Golden Shoes, and in 2020 was \nnamed to the Ballon d'Or Dream Team…\nStructured Facts:\nLionel Messi plays for:\nNewell's Old Boys from 1995 to 2000.\nFC Barcelona C from 2003 to 2004\nFC Barcelona Atlètic from 2004 to 2005 \nFC Barcelona from 2005 to 2021\nParis Saint-Germain  from 2021 till now \nSubject: Lionel Messi    Relation: P54 Member of Sports Team\n1995 2000 2003 2005\n2004\n2021\nFigure 2: Sample TEMP REASON questions and contexts. For humans, the L1 question can be answered without any\ncontext provided, whereas for L2 and L3 questions, humans will need to ground the events to timestamps and then\nperform temporal reasoning.\nble 3). (2) Time Grounding. The second challenge\nof TSQA is temporal grounding. That is, this sub-\ntask is to find the start time and end time of each\npossible answer. (3) Temporal Reasoning. The\nlast challenge is finding the correct answer among\nthe possible candidates based on the specified time\nconstraints.\nTo thoroughly examine the temporal reasoning\ncapability of large language models in different\naspects, we propose to tackle TSQA in three differ-\nent context settings: (1) closed book QA, (2) open\nbook QA, and (3) reasoning QA. We describe the\nthree problem settings as follows.\nClosed Book Question Answering (CBQA) .\nCBQA is a common task formulation in time-\nsensitive QA research (Dhingra et al., 2022; Liska\net al., 2022). In this setting, only the question is\nprompted to the language model, which is then\nasked to output the answer without access to any\nnatural language text. In Figure 2, the example\nquestion is asking about the soccer athlete Lionel\nMessi. The most difficult part of this question is the\nmemorization of Lionel Messi’s experiences, since\npeople who are not sports fans may not be able to\nanswer such questions easily.\nOpen Book Question Answering (OBQA). The\nOBQA formalization is a more realistic problem\nsetting, where external context in the form of natu-\nral language text is provided to help LMs to answer\nthe questions. As shown in middle of Figure 2, we\nuse the Wikipedia page of the subject entity as part\nof the prompt to the language model, together with\nthe question.\nReasoning QA. In this setting, all the rele-\nvant temporal facts within the group S =\n{(s,r,oi,tsi ,tei )|i∈1...N}are provided in struc-\ntured form as part of the prompt (right of Figure 2).\nThis is a simplified version of OBQA since all pos-\nsible answers and their time ranges are provided\nin the context. To avoid the models learning short-\ncuts, the provided facts are re-ordered randomly.\nEssentially, this setting resembles human tempo-\nral reasoning. The language models are required\nto deduce answers based on the time ranges of all\npossible answers. Human is able to deduce the an-\nswer by locating the query time within the group.\nIntuitively, human-level performance in this setting\ncan be regarded as 100%.\n4 Improving Temporal Reasoning\nIn order to improve the temporal reasoning capa-\nbilities, we propose a temporal training framework\nfor sequence-to-sequence language models. Firstly,\nwe pre-train the language model with a temporal\nspan extraction task to encourage the model to pay\nmore attention to the temporal and entity spans. We\nthen fine-tune the model on task-specific data in\nTEMP REASON . Finally, we further fine-tune the\nlanguage model by time-sensitive reinforcement\nlearning with our novel reward function.\nTemporal Span Extraction Pre-Training (TSE)\nConventional language model pre-training ran-\ndomly masks texts and reconstructs the original sen-\ntence. However, the relative importance of tokens\nand spans differs. Guu et al. (2020) first introduced\nsalient span masking, i.e, reconstructing masked\nnamed entities, as an intermediate pre-training tech-\nnique for language models. This approach has\nshown positive effects on the QA task. In order\nfor the language model to capture more knowledge\non time-related spans, we first pre-train on 100K\nWikipedia articles with a temporal and entity span\nextraction task. Specifically, we use the Spacy NER\ntagger to extract the temporal and entity spans in\n100K Wikipedia articles. The NER tagger is trained\n14824\non the Ontonotes 5.0 corpus (Weischedel et al.,\n2013). We randomly mask 50% of the entities and\ntemporal spans for a given paragraph and treat this\nparagraph as the input of T5 models. In this way,\nthe model pays more attention to the contexts that\nare relevant to temporal shifts. Then the pre-trained\nlanguage model will be used for fine-tuning with\nTEMP REASON question-answer pairs in different\nsettings.\nSupervised Fine-Tuning (SFT) The TSE pre-\ntrained language model with parameters θwill then\nbe fine-tuned on the task data in each setting. The\ninput prompt to the LM is the concatenation of\nquestion qand context c, and the objective of SFT\nis to maximize the probability of P(a|q,c), where\nais the correct answer.\nL2 Question: What team did Leo Messi play \nfor in May 2020?\nAnswer: FC Barcelona\nStructured Facts:\nLionel Messi plays for:\nNewell's Old Boys from 1995 to 2000.\nFC Barcelona C from 2003 to 2004\nFC Barcelona Atlètic from 2004 to 2005 \nFC Barcelona from 2005 to 2021\nParis Saint-Germain  from 2021 till now \nFigure 3: An example of time-sensitive reinforcement\nlearning (TSRL). The ground truth is highlighted in\ngreen color and the negative answers are highlighted in\nyellow color.\nTime-Sensitive Reinforcement Learning (TSRL)\nOne of the key challenges of temporal reasoning\nis that there are multiple possible answers for one\nsubject. For a given fact x = ( s,r,oj,tsj ,tej ),\nwe have the facts in the same group SN =\n{(s,r,oi,tsi ,tei )|i ∈1...N,i ̸= j}. These facts\nhave the same subject and relation as the given\nfact, but are in other time periods. Therefore, for a\nquestion related to the fact x, we are able to collect\nthe negative answers N = {oi|i ∈1...N,i ̸= j}\nwithin the same group as the negative sample set\nfor TSQA. An example of such negative examples\nis shown in Figure 3. For a given question related\nto fact x, we want to maximize the probability of\nthe correct answer oj while penalizing the model\nwhen it outputs temporally wrong answers. The\ncorrect answers and negative answers were used\nfor our reward function. We first calculate the posi-\ntive score p(x) of the model prediction θ(x) with\nrespect to the ground truth:\np(x) = F(θ(x),oj) (1)\nwhere F refers to the scoring function for reward\ncomputation. Specifically, we used the EM scor-\ning function as F. We then calculate the negative\nscore n(x) by:\nn(x) = max{F(θ(x),oi)|i̸= j} (2)\nThe negative score will be 1 if the model prediction\nreturns a temporally wrong answer. Finally, the\nreward function for TSRL is calculated as:\nR(x) =\n{ p(x) p(x) ≥n(x)\n−n(x) n(x) >p(x) (3)\nThe reward function is designed to give positive\nrewards for predictions that match the ground truth\nand negative rewards for predictions that match\nthe answers in the negative answer set N. We\nthen optimize the fine-tuned language model by\nthe Proximal Policy Optimization (Schulman et al.,\n2017) algorithm. We denote our final model as\nTempT5.\n5 Experiments\n5.1 Experimental Settings\nWe conduct experiments in each proposed setting\nin Section 3.2. The compared baselines are:FLAN-\nT5-Large (Wei et al., 2022). This model is fine-\ntuned on data from over 60 NLP tasks and the\nauthors showed that large-scale instruction tuning\nsignificantly improves the model’s performance on\nfew-shot reasoning. We evaluate the model’s zero-\nshot performance on temporal reasoning. Chat-\nGPT (Ouyang et al., 2022). This model is initial-\nized by GPT-3 and further trained to follow human\ninstructions. We used the gpt-3.5-0301 version of\nChatGPT for more consistent evaluation. Since this\nmodel is not open source and not free, we only ex-\namined its performance on 200 examples for each\nsetting. T5-SFT (Raffel et al., 2020). This baseline\nis based on supervised fine-tuning of the conven-\ntional T5 models. We use the T5-base model in our\nexperiments and we fine-tune this model on each\nsetting of TEMP REASON (Section 3.2).\n5.2 Experimental Results\nIn Table 4, we show the experimental results on the\ntest sets of TEMP REASON . We then analyze the\nperformance by each level of temporal understand-\ning.\nL1 Understanding. For L1 temporal understand-\ning, the performance of FLAN-T5-L and ChatGPT\n14825\nFLAN-T5-L ChatGPT T5-SFT TempT5\nQuestion Type Setting EM F1 EM F1 EM F1 EM F1 ∆F1\nL1: Time-Time CBQA 0.0 2.9 30.5 56.7 100 100 100 100 +0.0\nL2: Time-Event\nCBQA 0.5 9.2 6.5 11.5 1.4 23.2 1.5 23.4 +0.2\nReasonQA 57.3 66.3 47.5 51.0 82.6 87.1 84.8 88.9 +1.8\nOBQA 9.4 22.5 8.5 16.1 14.8 35.2 15.4 36.3 +1.1\nL3: Event-Event\nCBQA 0.4 10.5 12.0 21.8 12.1 25.3 12.3 25.4 +0.1\nReasonQA 36.3 47.5 49.5 52.3 78.2 83.0 81.1 86.1 +3.1\nOBQA 8.1 19.2 17.0 25.3 19.7 31.2 21.1 32.4 +1.2\nTable 4: Experimental results of each setting in TEMP REASON . ∆ F1 refers to the F1 difference between TempT5\nand T5-SFT. The reported results are the average scores of three runs.\nTempT5\nTime Range EM F1\n1000-2022 100 100\n2022-2040 94.4 97.1\nTable 5: L1 experimental results of TempT5 on in-\ndomain TEMP REASON test set and the future test set.\nsignificantly deteriorates compared to year predic-\ntion (Table 2). ChatGPT is able to achieve 99.6\nEM on year prediction, whereas it can only achieve\n30.5 EM on month prediction. The fine-tuned mod-\nels T5-SFT and TempT5 are able to achieve 100\nEM/F1 performance on this task. This showed that\neven though the L1 logic rules were not explicitly\nencoded in the language models, we can teach the\nlanguage model to learn such rules by creating ex-\namples of the rules on a large scale. We further\nevaluate the trained L1-TempT5 model on an out-\nof-domain futuristic test set (Table 5). The ques-\ntions of the futuristic test set have reference times\nfrom 2022 to 2040, which are disjoint from the\ntime period of TEMP REASON . The TempT5 model\nperforms decently on the future test set, achieving\n97.1 F1 score. However, this performance is still\nbelow the in-domain performance.\nL2 Understanding. The time-event relation is\nthe main question type of previous TSQA datasets.\nWhen we compare the performance of the three set-\ntings of L2 performance, we can see the problem\nsetting plays a significant role. For all three models,\nthe performance of CBQA is the lowest among the\nthree settings. This shows that it is highly difficult\nfor the LMs to answer temporal questions with-\nout any context. Meanwhile, ReasonQA has a sig-\nnificantly better performance compared to OBQA\nand CBQA. This shows that the language models\nare able to perform temporal reasoning when the\nrelevant facts were provided. That is, once the\npossible answers and the related timestamps are re-\ntrieved, fine-tuned language models (TempT5 and\nQuestion Type EM F1\nL2: CBQA P39 1.6 21.1\nOthers 1.3 19.9\nL3: CBQA P39 51.4 68.2\nOthers 0.6 12.1\nTable 6: Comparison of L2 and L3 performance of\nTempT5 in the CBQA setting.\nT5-SFT) can perform temporal reasoning relatively\nwell. It is worth noting that the ChatGPT model\nhas the worst performance in the L2 ReasonQA set-\nting while its performance is exceptionally high\nin the preliminary year prediction experiments.\nThis phenomenon shows that temporal understand-\ning at different levels may not be easily transfer-\nable. Last but not least, our proposed TempT5\nmodel achieves significant performance gains over\nT5-SFT in OBQA and ReasonQA, which is the\nstrongest baseline in our experiments.\nL3 Understanding. Similar to L2 understand-\ning, all models perform the best in ReasonQA, fol-\nlowed by OBQA and have the worst performance\nin CBQA. Besides, compared to L2 questions, most\nmodels have significantly worse performance on\nthe L3 questions in the ReasonQA setting (except\nfor ChatGPT), showing that L3 temporal reasoning\nis more challenging than L2. For the FLAN-T5-\nL model, the performance deterioration from L2\nto L3 is 18.8 F1 (L2: 66.3 vs L3: 47.5), whereas\nthe performance gaps of T5-SFT and TempT5 are\nmuch lower. It is worth noting that for the T5-\nSFT model, the exact match scores of L3 questions\nare significantly higher than those of L2 in the\nCBQA (L2:1.4 vs L3:12.1) and OBQA (L2:14.8\nvs L3:19.7) setting (same for TempT5). We found\nthat this counter-intuitive result is due to a rea-\nsoning shortcut of a specific question type ‘P39\nposition held’ (Table 13). We further analyze the\nCBQA performance by question type in Table 6.\nFor questions other than ‘P39’, L3 performance is\n14826\nReasonQA OBQA\nMetric EM F1 EM F1\nTempT5 84.8 88.9 15.4 36.3\n–TSE 84.0 88.0 14.8 35.5\n–TSRL 83.4 87.7 15.0 35.8\nTable 7: Ablation analysis of TempT5 based on L2\nquestions.\nFLAN-T5-L ChatGPT TempT5\nTime Range % Train F1 F1 F1\nbefore 1900 8.4 69.5 77.8 85.6\n1900-1920 4.1 67.9 78.7 87.5\n1920-1940 6.6 65.3 43.8 87.6\n1940-1960 7.5 71.9 47.9 88.7\n1960-1980 11.0 68.0 43.8 90.5\n1980-2000 18.3 65.6 43.9 89.6\n2000-2020 37.8 66.1 49.1 89.8\n2020-2040 6.3 68.5 72.7 82.6\nOverall 100 67.1 51.0 88.9\nTable 8: Performance breakdown of different models\nin L2 ReasonQA across different time periods. We can\nsee that ChatGPT has the worst performance among\nthe three models and its performance is highly variable\nacross different time periods.\nsignificantly worse than L2 (L3: 12.1 F1 vs L2:\n19.9 F1). However, the performance of L3 CBQA\non ‘P39’ questions is much higher than the other\nquestions. This is because there are reasoning short-\ncuts for ‘P39 position held’ questions from entity\nnames. For example, for the question ‘Which posi-\ntion did Nicholas Budgen hold before Member of\nthe 46th Parliament of the United Kingdom?’, the\nreasoning shortcut is to simply change the ‘46th’\nto ‘45th’. This shows that L3 temporal reasoning\ncan be achieved via different reasoning paths.\n5.3 Ablation Study\nIn Table 7, we showed the ablation study of\nTempT5 based on the L2 questions in the OBQA\nand ReasonQA settings. We can see that TSE and\nTSRL have different effects in the two settings.\nRemoving TSRL has a heavier impact on the Rea-\nsonQA setting, leading to a 1.2 F1 drop. On the\nother hand, TSE pre-training is more important\nin the OBQA setting and removing the TSE pre-\ntraining leads to a performance drop of 0.8 F1.\n5.4 Further Analysis\nIn this section, we examine the model biases in\nTEMP REASON . We first analyze the L2 reasoning\nperformance across different years in a similar man-\nner as Section 2. The performance breakdown can\nbe found in Table 8. We can see that for the FLAN-\nT5-L model and ChatGPT model, the L2 reasoning\nQuestion Type EM F1\nL2: ReasonQA Intra-year 80.5 86.3\nInter-year 86.9 90.3\nTable 9: Performance of TempT5 in L2 ReasonQA by\nquestion type. The intra-year question type refers to\nquestions that have multiple possible answers within\none year. In contrast, the inter-year question type only\nhas one possible answer in that specific year.\nExample 1 Error Type: Intra Year Error\nError Cause: Lack of monthly-level understanding.\nQuestion: Which position did Hirofumi Yoshimura hold in Jul\n2019?\nContext: Hirofumi Yoshimura holds the position of:\nGovernor of Osaka Prefecture from Apr 2019 to Dec 2022.\nMember of the House of Representatives of Japan from Dec 2014\nto Oct 2015.\nMayor of Osaka from Dec 2015 to Mar 2019.\nPrediction: Mayor of Osaka\nGround Truth: Governor of Osaka Prefecture\nTable 10: An example of intra-year error of TempT5 in\nL2 ReasonQA.\nperformance fluctuates across different time peri-\nods. FLAN-T5-L not only has higher performance\nbut also lower variability across the different time\nperiods. On the other hand, from the performance\nbreakdown of our proposed TempT5, we can see\nthat the temporal biases shown in the year predic-\ntion experiments (Table 2) were alleviated. The\nF1 scores from 1940 to 2020 were similar. How-\never, the F1 scores before 1900 and after 2020 are\nstill significantly worse than the other time periods.\nThis performance degradation is largely due to the\nlack of training data in those time periods.\nThe other major source of errors comes from the\nintra-year question type. The intra-year question\ntype refers to questions that have multiple possible\nanswers within one year. Therefore, it requires rea-\nsoning at the month level. As shown in Table 9, the\nperformance on intra-year questions is significantly\nworse than the performance on inter-year questions,\nespecially for the difference in the EM score (6.4,\n86.9 vs. 80.5). In Table 10, we show an example\nof an intra-year reasoning error. We can see that\nthe model fails to capture the intra-year position\nchange of the subject.\n6 Related Work\nTemporal Information Extraction Early efforts\non temporal NLP research primarily focus on event\ntemporal information extraction. Pustejovsky et al.\n(2003) constructed the TimeBank corpus, which is\n14827\na temporally annotated corpus that annotates events,\ntimes, and temporal relations (such as before/after).\nThe TIE task asks models to extract the events\nwithin a piece of text and to identify the temporal\nrelations between event pairs. The TempEval (Ver-\nhagen et al., 2010; Bethard et al., 2016) challenge\nis a popular TIE challenge with a similar annota-\ntion scheme as TimeBank. However, it is costly to\nexhaustively annotate the temporal relations among\nall events. Cassidy et al. (2014) proposed a dense\nannotation scheme and constructed the TimeBank-\nDense dataset, which has more complete annota-\ntion compared to TimeBank. Han et al. (2019)\nproposed a joint framework to extract events and\ntime in an end-to-end manner. Rajaby Faghihi and\nKordjamshidi (2021) proposed the Time-stamped\nLanguage Model to understand the flow of events.\nHowever, prior works in this field focused on ex-\ntracting events and temporal relations within one\npiece of document. The models trained on this task\ncannot perform global event-to-time grounding.\nTemporal Reasoning over KGsThe Temporal\nKnowledge Graph Completion (TKGC) field stud-\nies temporal reasoning in knowledge graphs. This\ntask aims to rank all entities in a knowledge graph\ngiven a temporal query. Many works in this field\n(TTransE, Jiang et al., 2016; TTransH, Dasgupta\net al., 2018; TNTComplEx, Lacroix et al., 2020)\nwere proposed as extensions to prior knowledge\ncompletion techniques, such as TransE (Bordes\net al., 2013), TransH (Wang et al., 2014), and Com-\nplEx (Kipf and Welling, 2017). With a similar con-\ncept as TKGC, several question answering datasets\nare proposed based on temporal knowledge graphs,\nsuch as TEQUILA (Jia et al., 2018b), TimeQues-\ntions (Jia et al., 2021), and CronQuesions (Saxena\net al., 2021). These datasets include more complex\nquestions in a natural language format, and the task\nsetting is also asking models to rank all the enti-\nties of a given knowledge graph. Mavromatis et al.\n(2022) proposed a joint model that unifies temporal\nKG embeddings and pre-trained language models\nfor this task. Shang et al. (2022) proposed a con-\ntrastive approach to improve the QA performance\nfor temporal KGs. Temporal reasoning in KGs is\nclosely related to our problem of interest. How-\never, the major difference is that KGQA presumes\nall the entities are known to the system and the\ntask is to rank all the possible entities that satisfy\nthe queries. In contrast, our task aims to answer\ntemporal questions based on natural text input only.\nTemporal Reasoning for LMsLarge language\nmodels (Devlin et al., 2019; Raffel et al., 2020;\nLiu et al., 2019) have demonstrated good perfor-\nmance on the question answering task (Rajpurkar\net al., 2016; Kwiatkowski et al., 2019). In re-\ncent years, several contemporary time-sensitive QA\ndatasets were proposed. Zhang and Choi (2021)\nproposed the SituatedQA dataset, which contains\nplenty of time-dependent question-answer pairs.\nThe TEMP LAMA dataset (Dhingra et al., 2022)\nwas proposed to evaluate the CBQA performance\nfor time-dependent questions from 2010 to 2020.\nHowever, the QA performance of TEMP LAMA\nmay be overestimated, since it only covers a short\ntime period and the period is in favor of LM’s tem-\nporal bias. Similarly, StreamingQA (Liska et al.,\n2022) has a similar disadvantage, since its time\ncoverage is from 2007 to 2020. The Time-sensitive\nQA dataset (Chen et al., 2021) covers a relatively\nlonger timespan (from 1367 to 2018), but it only\ncontains questions of time-event relation. The com-\nmon drawback of the previously proposed TSQA\ndatasets is the lack of coverage of temporal reason-\ning levels other than the time-event type of ques-\ntions.\n7 Conclusions and Future Work\nIn this paper, we tackled the under-explored tempo-\nral reasoning problem for large language models.\nWe found that large language models are highly\nsusceptible to biases of time, and their temporal rea-\nsoning capability varies depending on the specific\ntime given in the question. Besides, we proposed\na comprehensive time-sensitive QA dataset TEMP -\nREASON to evaluate LMs’ temporal reasoning ca-\npability in diverse settings. Lastly, we proposed a\nnovel training paradigm to improve language mod-\nels’ reasoning capability by temporal span extrac-\ntion pre-training and time-sensitive reinforcement\nlearning. We conducted extensive experiments and\ndemonstrated that our proposed model consistently\noutperformed strong baselines.\n8 Limitations\nThe focus of the TEMP REASON dataset is to exam-\nine language models’ temporal reasoning capabil-\nity. However, the temporal expressions of TEMP -\nREASON are only in the form of month in textual\nform and year in numeric form. One limitation of\nthe TEMP REASON benchmark is the lack of adver-\nsarial attacks in other temporal formats, such as\n14828\nall numeric dates and months. The robustness of\ntemporal reasoning is also important in real-world\napplications. Since the scope of this paper only\nfocuses on the reasoning aspect, the robustness of\nTEMP REASON will be left for future research. Be-\nsides, the knowledge triples of TEMP REASON are\nfrom the crowd-sourced Wikidata KB, and these\ntriples are used to construct the question-answer\npairs in this paper. Hence, it is possible that errors\nin the Wikidata KB propagate to the answers in\nTEMP REASON . However, such errors have mini-\nmal effect in the ReasonQA setting, for this task\nonly asks the models to infer from factual knowl-\nedge in the Wikidata KB.\n9 Ethics Statement\nIn this paper, we created a probing dataset TEMP -\nREASON for temporal reasoning evaluation. The\ndataset is constructed based on the matching of\nWikidata KB and Wikipedia articles. This approach\nis commonly used for distantly supervised data con-\nstruction. The Wikidata KB is under the public\ndomain4 and the Wikipedia articles are licensed un-\nder the Creative Commons AttributionShareAlike\n3.0 Unported License 5. Therefore, we are able\nto adapt these data to construct our dataset. We\nwill also release our data under the same license\nas Wikidata. The scope of our dataset is purely for\nscientific research of language models’ temporal\nreasoning capability. However, the contexts from\nthe Wikipedia articles may contain improper con-\ntent. The adoption of such content is not a decision\nof the authors, and all content in the dataset does\nnot reflect the views or stances of the authors of\nthis paper.\n10 Acknowledgements\nWe would like to thank all the reviewers for their\ninsightful comments and constructive feedback.\nReferences\nSteven Bethard, Guergana Savova, Wei-Te Chen, Leon\nDerczynski, James Pustejovsky, and Marc Verhagen.\n2016. SemEval-2016 task 12: Clinical TempEval. In\nProceedings of SemEval.\nAntoine Bordes, Nicolas Usunier, Alberto Garcia-\nDuran, Jason Weston, and Oksana Yakhnenko.\n4https://www.wikidata.org/wiki/Wikidata:\nLicensing\n5https://en.wikipedia.org/wiki/Wikipedia:\nCopyrights\n2013. Translating embeddings for modeling multi-\nrelational data. In Proceedings of NIPS.\nTaylor Cassidy, Bill McDowell, Nathanael Chambers,\nand Steven Bethard. 2014. An annotation framework\nfor dense event ordering. In Proceedings of ACL.\nWenhu Chen, Xinyi Wang, and William Yang Wang.\n2021. A dataset for answering time-sensitive ques-\ntions. In Proceedings of NIPS.\nYew Ken Chia, Lidong Bing, Sharifah Mahani Alju-\nnied, Luo Si, and Soujanya Poria. 2022. A dataset\nfor hyper-relational extraction and a cube-filling ap-\nproach. In Proceedings of EMNLP.\nShib Sankar Dasgupta, Swayambhu Nath Ray, and\nPartha Talukdar. 2018. HyTE: Hyperplane-based\ntemporally aware knowledge graph embedding. In\nProceedings of EMNLP.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of NAACL.\nBhuwan Dhingra, Jeremy R. Cole, Julian Martin\nEisenschlos, Daniel Gillick, Jacob Eisenstein, and\nWilliam W. Cohen. 2022. Time-Aware Language\nModels as Temporal Knowledge Bases. Transactions\nof ACL.\nBosheng Ding, Chengwei Qin, Linlin Liu, Yew Ken\nChia, Boyang Li, Shafiq Joty, and Lidong Bing. 2023.\nAre large language models good data annotators? a\nstudy on gpt-3, chatgpt and gpt-4. In Proceedings of\nACL.\nHao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, and\nTat-Seng Chua. 2023. Reasoning implicit sentiment\nwith chain-of-thought prompting. In Proceedings of\nACL.\nKelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat,\nand Mingwei Chang. 2020. Retrieval augmented lan-\nguage model pre-training. In Proceedings of ICML.\nRujun Han, Qiang Ning, and Nanyun Peng. 2019. Joint\nevent and temporal relation extraction with shared\nrepresentations and structured prediction. In Pro-\nceedings of EMNLP.\nZhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jan-\nnik Strötgen, and Gerhard Weikum. 2018a. Tem-\npquestions: A benchmark for temporal question an-\nswering. In Proceedings of WWW.\nZhen Jia, Abdalghani Abujabal, Rishiraj Saha Roy, Jan-\nnik Strötgen, and Gerhard Weikum. 2018b. Tequila:\nTemporal question answering over knowledge bases.\nIn Proceedings of CIKM.\nZhen Jia, Soumajit Pramanik, Rishiraj Saha Roy, and\nGerhard Weikum. 2021. Complex temporal question\nanswering on knowledge graphs. In Proceedings of\nCIKM.\n14829\nTingsong Jiang, Tianyu Liu, Tao Ge, Lei Sha, Baobao\nChang, Sujian Li, and Zhifang Sui. 2016. Towards\ntime-aware knowledge graph completion. In Pro-\nceedings of COLING.\nVladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick\nLewis, Ledell Wu, Sergey Edunov, Danqi Chen, and\nWen-tau Yih. 2020. Dense passage retrieval for\nopen-domain question answering. In Proceedings\nof EMNLP.\nJungo Kasai, Keisuke Sakaguchi, Yoichi Takahashi,\nRonan Le Bras, Akari Asai, Xinyan Yu, Dragomir\nRadev, Noah A Smith, Yejin Choi, and Kentaro Inui.\n2022. Realtime qa: What’s the answer right now? In\nProceedings of EMNLP.\nThomas N. Kipf and Max Welling. 2017. Semi-\nsupervised classification with graph convolutional\nnetworks. In Proceedings of ICLR.\nTom Kwiatkowski, Jennimaria Palomaki, Olivia Red-\nfield, Michael Collins, Ankur Parikh, Chris Alberti,\nDanielle Epstein, Illia Polosukhin, Jacob Devlin, Ken-\nton Lee, et al. 2019. Natural questions: a benchmark\nfor question answering research. Transactions of\nACL.\nTimothée Lacroix, Guillaume Obozinski, and Nicolas\nUsunier. 2020. Tensor decompositions for temporal\nknowledge base completion. In Proceedings of ICLR.\nAdam Liska, Tomas Kocisky, Elena Gribovskaya, Tay-\nfun Terzi, Eren Sezener, Devang Agrawal, D’Autume\nCyprien De Masson, Tim Scholtes, Manzil Zaheer,\nSusannah Young, et al. 2022. Streamingqa: A bench-\nmark for adaptation to new knowledge over time in\nquestion answering models. In Proceedings of ICML.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. In arXiv preprint arXiv:1907.11692.\nCostas Mavromatis, Prasanna Lakkur Subramanyam,\nVassilis N Ioannidis, Adesoji Adeshina, Phillip R\nHoward, Tetiana Grinberg, Nagib Hakim, and George\nKarypis. 2022. Tempoqr: temporal question reason-\ning over knowledge graphs. In Proceedings of AAAI,\nvolume 36, pages 5825–5833.\nQiang Ning, Hao Wu, Rujun Han, Nanyun Peng, Matt\nGardner, and Dan Roth. 2020. TORQUE: A reading\ncomprehension dataset of temporal ordering ques-\ntions. In Proceedings of EMNLP.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, et al.\n2022. Training language models to follow instruc-\ntions with human feedback. In arXiv preprint\narXiv:2203.02155.\nJames Pustejovsky, Patrick Hanks, Roser Sauri, Andrew\nSee, Robert Gaizauskas, Andrea Setzer, Dragomir\nRadev, Beth Sundheim, David Day, Lisa Ferro, et al.\n2003. The timebank corpus. In Corpus linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. JMLR.\nHossein Rajaby Faghihi and Parisa Kordjamshidi. 2021.\nTime-stamped language model: Teaching language\nmodels to understand the flow of events. In Proceed-\nings of NAACL.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. SQuAD: 100,000+ questions for\nmachine comprehension of text. In Proceedings of\nEMNLP.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In Proceedings of EMNLP.\nApoorv Saxena, Soumen Chakrabarti, and Partha Taluk-\ndar. 2021. Question answering over temporal knowl-\nedge graphs. In Proceedings of ACL.\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec\nRadford, and Oleg Klimov. 2017. Proximal pol-\nicy optimization algorithms. In arXiv preprint\narXiv:1707.06347.\nChao Shang, Guangtao Wang, Peng Qi, and Jing Huang.\n2022. Improving time sensitivity for question answer-\ning over temporal knowledge graphs. In Proceedings\nof ACL. Association for Computational Linguistics.\nMarc Verhagen, Roser Saurí, Tommaso Caselli, and\nJames Pustejovsky. 2010. SemEval-2010 task 13:\nTempEval-2. In Proceedings of SemEval.\nDenny Vrandeˇci´c and Markus Krötzsch. 2014. Wiki-\ndata: a free collaborative knowledgebase. Proceed-\nings of CACM.\nZhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng\nChen. 2014. Knowledge graph embedding by trans-\nlating on hyperplanes. In Proceedings of AAAI.\nJason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu,\nAdams Wei Yu, Brian Lester, Nan Du, Andrew M\nDai, and Quoc V Le. 2022. Finetuned language\nmodels are zero-shot learners. In Proceedings of\nICLR.\nRalph Weischedel, Martha Palmer, Mitchell Marcus, Ed-\nuard Hovy, Sameer Pradhan, Lance Ramshaw, Nian-\nwen Xue, Ann Taylor, Jeff Kaufman, Michelle Fran-\nchini, et al. 2013. Ontonotes release 5.0 ldc2013t19.\nLinguistic Data Consortium.\nHai Ye, Qizhe Xie, and Hwee Tou Ng. 2023. Multi-\nsource test-time adaptation as dueling bandits for\nextractive question answering. In Proceedings of\nACL.\n14830\nMichael Zhang and Eunsol Choi. 2021. SituatedQA:\nIncorporating extra-linguistic contexts into QA. In\nProceedings of EMNLP.\nRuochen Zhao, Shafiq Joty Xingxuan Li, Chengwei\nQin, and Lidong Bing. 2023. Verify-and-edit: A\nknowledge-enhanced chain-of-thought framework.\nIn Proceedings of ACL.\nA Realtime Adaptation of LMs\nBesides the experiments on our proposed TEMP -\nREASON dataset. We also examined our model in\nthe RealtimeQA (Kasai et al., 2022) leaderboard.\nThis leaderboard releases time-sensitive questions\nevery week based on weekly quizzes from news\nwebsites (such as CNN and CNBC). The Real-\ntimeQA challenge has two tracks: (1) multiple-\nchoice questions and (2) generation track. The\ngeneration track of this challenge is the same as\nOBQA in this paper. We examined our model along\nwith the two retrievers provided in the challenge:\n(1) Google custom search (GCS), and (2) Dense\nPassage Retrieval (DPR, Karpukhin et al., 2020).\nWe adapt our TempT5 model of L2 ReasonQA on\nthe question-answer pairs of RealtimeQA before\nDecember 2022. We then evaluate the adapted\nmodel on the questions released on 16th Decem-\nber 20226. Experimental results (Table 12) show\nthat our model performs competitively even when\nadapting to the most up-to-date TSQA challenge.\nB Implementation Details\nThis section describes the implementation details\nof our models and baselines. For temporal span\nextraction pre-training, we use the T5-base model\nfor initialization. We then train the model for 100K\nsteps with a batch size of 8 and a learning rate of\n2e-5. We use the maximum input length of 512 for\nTSE pre-training. For task-specific fine-tuning, we\nuse the same batch size and learning rate, whereas\nthe maximum input lengths are different for each\nsetting. For the CBQA setting, the maximum input\nlength is set as 128, since only the question is given\nto the model. For the ReasonQA setting, the max-\nimum input length is set as 512. The maximum\nlength of 1,024 is used for the OBQA setting, since\nthe context in this setting is the longest on aver-\nage. For each setting, we fine-tune the language\nmodel for 3 epochs, and evaluation is conducted\n6https://realtimeqa.github.io/docs/results/\n2022/20221216\nusing the final checkpoint. For time-sensitive re-\ninforcement learning, we followed the proximal\npolicy optimization (PPO, Schulman et al., 2017)\nalgorithm. Instead of using a reward model, we\nuse the reward function described in Section 4. For\nthis stage, we set the initial KL penalty coefficient\nas 0.05 and the target KL coefficient as 6. The\ndiscount factor γfor PPO is set to 0.99.\nC Comparison of TEMP REASON and\nPrior Datasets\nIn Table 11, we show the detailed comparison of\nour TEMP REASON dataset and prior time-sensitive\nquestion answering datasets. Our dataset is the first\nto include all three temporal reasoning types and\nthe ReasonQA setting.\nD T EMP REASON Templates\nThe templates that we used to create TEMP -\nREASON is shown in Table 13.\n14831\nDataset QA format Knowledge Corpus Closed/Open/Reason Time Coverage Size L1 L2 L3\nTEMPREASON Language Wikidata/Wikipedia Closed/Open/Reason 634-2023 52.8K✓ ✓ ✓\nTEMPLAMA (Dhingra et al., 2022) Language Wikidata Closed 2010-2020 50k ✗ ✓ ✗\nTime-Sensitive QA (Chen et al., 2021) Language Wikidata/Wikipedia Open 1367-2018 41.2k✗ ✓ ✗\nStreamingQA (Liska et al., 2022) Language WMT Closed/Open 2007-2020 147k ✗ ✓ ✗\nSituatedQA (Zhang and Choi, 2021) Language Wikipedia/Human Annotation Closed/Open 1270-2021 12.2k✗ ✓ ✗\nTempQuestions (Jia et al., 2018a) KG Wikipedia KG NA 1.2k ✗ ✓ ✗\nTimeQuestions (Jia et al., 2021) KG Wikidata KG NA 16.1k ✗ ✓ ✗\nCronQuestions (Saxena et al., 2021) KG Wikidata KG 34-2021 410k ✗ ✓ ✓\nTable 11: Dataset comparison of TEMP REASON and prior datasets.\nEM F1\nGPT3+GCS† 55.0 63.6\nTempT5-L+GCS 48.3 53.3\nRAG+GCS† 35.0 45.9\nGPT3+DPR† 17.2 23.0\nTempT5-L+DPR 10.3 18.4\nRAG+DPR† 0.0 3.1\nTable 12: Experimental results on the generation track\nof RealtimeQA leaderboard based on December 16,\n2022‘s questions. The task formulation of this track\nis the same as OBQA in this paper. Results with †are\ntaken from the URL in the footnote.\n14832\nWikiData ID KB Relation # Queries Template\nL1 Question Templates:\nNA NA NA What is the time xyear(s) andymonth(s) before/aftert?\nNA NA NA What is the time xyear(s) before/aftert?\nNA NA NA What is the time ymonth(s) before/aftert?\nL2 Question Templates:\nP54 member of sports team 4,087 Which team did <subject> play for int?\nP39 position held 3,133 Which position did <subject> hold in t?\nP108 employer 2,368 Which employer did <subject> work for in t?.\nP102 political party 500 Which political party did <subject> belong to in t?\nP286 head coach 1,153 Who was the head coach of <subject> in t?\nP69 educated at 750 Which school was <subject> attending in t?\nP488 chairperson 1,904 Who was the chair of <subject> in t?\nP6 head of government 1,627 Who was the head of the government of <subject> int?\nP35 head of state 250 Who was the head of the state of <subject> in t?\nP127 owned by 245 Who was the owner of <subject>in t?\nL3 Question Templates:\nP54 member of sports team 2,524 Which team did <subject> play for before/afteroj?\nP39 position held 2,538 Which position did <subject> hold before/after oj?\nP108 employer 1,991 Which employer did <subject> work for before/after oj?.\nP102 political party 433 Which political party did <subject> belong to before/after oj?\nP286 head coach 1,051 Who was the head coach of <subject> before/after oj?\nP69 educated at 594 Which school was <subject> attending before/after oj?\nP488 chairperson 1,881 Who was the chair of <subject> before/after oj?\nP6 head of government 1,535 Who was the head of the government of <subject> before/afteroj?\nP35 head of state 268 Who was the head of the state of <subject> before/after oj?\nP127 owned by 199 Who was the owner of <subject> before/after oj?\nTable 13: Templates used for converting Wikidata facts into natural questions. For the L2 questions, tis a randomly\nsampled time between the start time ts and end time te of the given fact. The format of t is month and year\n(examples shown in Figure 2). oj refers to the object entity name that is before or after the correct answer. The\nnumbers of queries are from the L2 and L3 training sets of TEMP REASON .\n14833\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSection 8\n□\u0013 A2. Did you discuss any potential risks of your work?\nSection 9\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSection 1\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nSection 3\n□\u0013 B1. Did you cite the creators of artifacts you used?\nSection 3\n□\u0013 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nSection 9\n□\u0013 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nSection 9\n□\u0013 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nSection 3, Section 9, and Appendix C\n□\u0013 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nSection 3 and appendix C.\n□\u0013 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nSection 3\nC □\u0013 Did you run computational experiments?\nSection 5\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nAppendix B\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n14834\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nSection 5\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nSection 5\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nSection 5\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNo response.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNo response.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNo response.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNo response.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNo response.\n14835",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7984377145767212
    },
    {
      "name": "Benchmarking",
      "score": 0.7973979711532593
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5693196058273315
    },
    {
      "name": "Language model",
      "score": 0.45068755745887756
    },
    {
      "name": "Natural language processing",
      "score": 0.44889479875564575
    },
    {
      "name": "Reinforcement learning",
      "score": 0.44849491119384766
    },
    {
      "name": "Question answering",
      "score": 0.421217679977417
    },
    {
      "name": "Machine learning",
      "score": 0.3579753339290619
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210086143",
      "name": "Alibaba Group (Cayman Islands)",
      "country": "KY"
    },
    {
      "id": "https://openalex.org/I165932596",
      "name": "National University of Singapore",
      "country": "SG"
    }
  ]
}