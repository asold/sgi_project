{
  "title": "Location Name Extraction from Targeted Text Streams using Gazetteer-based Statistical Language Models",
  "url": "https://openalex.org/W2744859620",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4298974162",
      "name": "Al-Olimat, Hussein S.",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2750436528",
      "name": "Thirunarayan, Krishnaprasad",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3162267379",
      "name": "Shalin, Valerie",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749286790",
      "name": "Sheth Amit",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2161677660",
    "https://openalex.org/W1967450521",
    "https://openalex.org/W2950608207",
    "https://openalex.org/W2022194882",
    "https://openalex.org/W1976339715",
    "https://openalex.org/W2107300377",
    "https://openalex.org/W8550301",
    "https://openalex.org/W1901600440",
    "https://openalex.org/W1505544955",
    "https://openalex.org/W560371024",
    "https://openalex.org/W2251333278",
    "https://openalex.org/W2046240631",
    "https://openalex.org/W2293755787",
    "https://openalex.org/W2036352651",
    "https://openalex.org/W1991352422",
    "https://openalex.org/W2096765155",
    "https://openalex.org/W1521125545",
    "https://openalex.org/W2337748376",
    "https://openalex.org/W2153848201",
    "https://openalex.org/W2770592549",
    "https://openalex.org/W1994943910",
    "https://openalex.org/W2250439311",
    "https://openalex.org/W2058497043",
    "https://openalex.org/W1989712965",
    "https://openalex.org/W2145956377",
    "https://openalex.org/W1574901103",
    "https://openalex.org/W2759336060",
    "https://openalex.org/W2426119782",
    "https://openalex.org/W2965924382",
    "https://openalex.org/W2143933463",
    "https://openalex.org/W2104583100",
    "https://openalex.org/W2151848474",
    "https://openalex.org/W2026037750",
    "https://openalex.org/W2047197963",
    "https://openalex.org/W2120545936"
  ],
  "abstract": "Extracting location names from informal and unstructured social media data requires the identification of referent boundaries and partitioning compound names. Variability, particularly systematic variability in location names (Carroll, 1983), challenges the identification task. Some of this variability can be anticipated as operations within a statistical language model, in this case drawn from gazetteers such as OpenStreetMap (OSM), Geonames, and DBpedia. This permits evaluation of an observed n-gram in Twitter targeted text as a legitimate location name variant from the same location-context. Using n-gram statistics and location-related dictionaries, our Location Name Extraction tool (LNEx) handles abbreviations and automatically filters and augments the location names in gazetteers (handling name contractions and auxiliary contents) to help detect the boundaries of multi-word location names and thereby delimit them in texts. We evaluated our approach on 4,500 event-specific tweets from three targeted streams to compare the performance of LNEx against that of ten state-of-the-art taggers that rely on standard semantic, syntactic and/or orthographic features. LNEx improved the average F-Score by 33-179%, outperforming all taggers. Further, LNEx is capable of stream processing.",
  "full_text": "Please cite: Hussein S. Al-Olimat, Krishnaprasad Thirunarayan, Valerie Shalin, and Amit Sheth. 2018. Location Name Extraction from Targeted Text\nStreams using Gazetteer-based Statistical Language Models. In Proceedings of the 27th International Conference on Computational Linguistics (COLING\n2018), pages 1986-1997. Association for Computational Linguistics. Online: https://www.aclweb.org/anthology/C18-1169.pdf\nLocation Name Extraction from Targeted Text Streams\nusing Gazetteer-based Statistical Language Models\nHussein S. Al-Olimat, Krishnaprasad Thirunarayan, Valerie L. Shalin and Amit P. Sheth\nKno.e.sis Center, Wright State University, Dayton, OH\n{hussein;tkprasad;valerie;amit}@knoesis.org\nAbstract\nExtracting location names from informal and unstructured social media data requires the identiﬁ-\ncation of referent boundaries and partitioning compound names. Variability, particularlysystem-\natic variability in location names (Carroll, 1983), challenges the identiﬁcation task. Some of this\nvariability can be anticipated as operations within a statistical language model, in this case drawn\nfrom gazetteers such as OpenStreetMap (OSM), Geonames, and DBpedia. This permits evalua-\ntion of an observed n-gram in Twitter targeted text as a legitimate location name variant from the\nsame location-context. Using n-gram statistics and location-related dictionaries, our Location\nName Extraction tool (LNEx) handles abbreviations and automatically ﬁlters and augments the\nlocation names in gazetteers (handling name contractions and auxiliary contents) to help detect\nthe boundaries of multi-word location names and thereby delimit them in texts.\nWe evaluated our approach on 4,500 event-speciﬁc tweets from three targeted streams to compare\nthe performance of LNEx against that of ten state-of-the-art taggers that rely on standard seman-\ntic, syntactic and/or orthographic features. LNEx improved the average F-Score by 33-179%,\noutperforming all taggers. Further, LNEx is capable of stream processing.1\n1 Introduction\nIn context-aware computing, location is a fundamental component that supports a wide-range of appli-\ncations (Hazas et al., 2004; Licht et al., 2017). During natural disasters, location is crucial for situational\nawareness during disaster response (Son et al., 2008). When available, targeted streams of social me-\ndia data are therefore particularly valuable for disaster response (Munro, 2011)2. For example, the tweet\n“water level in Ganapathy Colony is around 2 m” refers to a location. But, unless we know where “Gana-\npathy Colony” is, the water level data cannot enhance situational awareness and inform disaster response\napplications such as storm surge modeling/forecasting.\nHowever, pragmatic inﬂuences on writing style shorten names to reduce redundant content in social\nmedia. We call this the location name contraction problem . For example, “Balalok School”, appears\nin the Chennai ﬂood tweets in contrast to the full gazetteer name, “Balalok Matriculation Higher Sec-\nondary School”. Carroll (1983) examined the complex phenomenon of alternate name forms (called\nNameheads). He distinguishes between four shortening processes: (1) Appellation Formation, (2) Ex-\nplicit Metonomy, (3) Category Ellipsis, and (4) Location Ellipsis.\nAppellation Formation occurs when, for example, the author refers to the location name “The Erie\nCanal” as “The Canal”. People may also refer to the only airport in the affected area as just “The\nAirport”. Referring to “University of Michigan” as “Michigan” is an example of Explicit Metonomy.\nCommon ground or shared understanding between the author and the recipient establishes the referent\nThis work is licensed under a Creative Commons Attribution 4.0 International License. License details: http://\ncreativecommons.org/licenses/by/4.0/\n1Data and the tool is available at https://github.com/halolimat/LNEx\n2We deﬁne a targeted stream as a set of tweets that has the potential to satisfy an event-related information need (Piskorski\nand Ehrmann, 2013) crawled using keywords and hashtags, to contextualize the event (e.g., “#ChennaiFloods” for the ﬂoods in\nChennai, India).\narXiv:1708.03105v3  [cs.CL]  26 Apr 2020\n(Resnick et al., 1991). Both appelation formation and metonomy pose disambiguation problems, and\nrequire context such as the author’s location to resolve.\nIn contrast, Category Ellipsis and Location Ellipsis pose delimitation problems that resolve with a\nstatistical language model. Category ellipsis occurs when the author strips words related to the location\ncategory (e.g., any of the intermediate tokens inside “Balalok Matriculation Higher Secondary School”\nor “City” from “Houston City” to become “Houston”). Location Ellipsis occurs when an author drops\nthe speciﬁc location reference in the location name (e.g., when “New York Yankee Stadium” becomes\n“Yankee Stadium” or “Cars India - Adyar” becomes “Cars India”).\nThis distinction between delimitation and disambigution is important in the location extraction litera-\nture. Entity delimitation is typically the ﬁrst step of location extraction, to identify the boundaries of a\nlocation mention in the text. To address this problem, previous research (Liu et al., 2014; Malmasi and\nDras, 2015; Hoang and Mothe, 2018) has applied both syntactic heuristics (using lexical cues, e.g., “ in\nNew Orleans”) as well as semantic heuristics (i.e., content-based, for different types of locations such as\nbuildings and streets). These heuristics have serious limitations, such as failing to delimit metonyms and\nlocation names that begin sentences (i.e., outside locative expressions, e.g., “ New Orleans is ﬂooded”)\nand they cannot assist in hashtag segmentation (needed to extracted locations from hashtags). Moreover,\nsimply identifying a location name still leaves open the problem of linking the entity to a corresponding\ngazetteer record for geocoding. Simple ﬁxed phrase matching with gazetteers entries, as in (Middleton et\nal., 2014; Malmasi and Dras, 2015), solves the linking problem, but remains vulnerable in two respects.\nWith simple ﬁxed phrase matching, the tendency for authors to shorten names while the gazetteers extend\nnames, creates conﬂicting conditions causing poor recall. On the other hand, simply relaxing matching\ncriteria exacerbates the disambiguation problem.\nTo address delimitation, we treat location names as a sequence of ordered words known as colloca-\ntions (Manning and Sch ¨utze, 1999). Collocations are neither strictly compositional nor always atomic.\nWe cannot identify them with grammatical rules, and ﬁxed phrase matching is not reliable for longer\nnames. Fortunately, the gazetteer provides a resource to establish region-speciﬁc naming regularities.\nGiven a region-speciﬁc gazetteer, which retains the same location-context as the text, we can construct\na statistical model of the token sequences it contains. However, current gazetteers are overly speciﬁc in\ntwo respects. First, consistent with (Carroll, 1983), they do not represent category ellipsis and location\nellipsis. To mimic these processes, we judiciously apply a skip-gram method to token sequences in the\ngazetteers, thereby including, for example, “Balalok School” as a variant of the complete name. Sec-\nond, we eliminate auxiliary or ambiguous gazetteers content (e.g., “George, Washington”) that would\notherwise threaten recall.\nHere we answer the research question: Can we accurately and rapidly spot location mentions in\ntext solely relying on a statistical language model synthesized from augmented and ﬁltered region-\nspeciﬁc gazetteers? Although LNEx works on a targeted Twitter stream collected using event-speciﬁc\nkeywords, it does not rely on rarely available tweets geo-coordinates and it does not need any supervision\n(i.e., training data). It is well-suited for stream processing, and needs only freely available data. Our\ncontributions include:\n1. A method for preparing high-quality gazetteers from online open data, such as OSM, Geonames,\nand DBpedia, and deriving a language model from them; and a comprehensive analysis of the\ncontribution of gazetteer quality to overall performance.\n2. A referent corpus representing the full scope of location name extraction challenges and a challenge-\nbased categorization of place names found in the corpora resulting from targeted streams. We\nannotate three different Twitter streams from ﬂooding events in three different locations: the 2015\nChennai ﬂood, the 2016 Louisiana ﬂood, and the 2016 Houston ﬂood, for our own evaluation and\nalso for use by others.\n3. A demonstration that LNEx convincingly outperforms commercial-grade NER and Twitter-speciﬁc\ntools with at least a 33% improvement on average F-Score. Examples reveal the true challenges of\nlocation name extraction and the locus of tool failure in the face of these challenges.\nLNEx provides the foundation for localizing information, and with increased availability of open data,\nwe expect our approach based on region-speciﬁc knowledge to be widely applicable in practice.\n2 The LNEx Method\nWe discuss the details of LNEx in four subsections. First, we present the general idea of statistical\ninference via n-gram models; the core of LNEx is a statistical language model consisting of a probability\ndistribution over sequences of words (collocations) that represent location names in preexisting, region-\nspeciﬁc gazetteers. Then we separately discuss several modiﬁcations to both gazetteers and text samples,\nincluding gazetteer augmentation and ﬁltering, and tweet preprocessing. Finally, we illustrate the full\nlocation analysis and matching process that reliably spots location names.\n2.1 Statistical Inference vian-gram Models\nLNEx constructs an n-gram model from the collocations that existin the gazetteer to determine the valid\nlocation names (LNs) that might appear in tweets. Given tweet content such as“texas ave is closed”, the\nmodel can then check the validity of one to n-grams. From the gazetteer, “texas” and “ave” are valid\ngazetteer unigrams but “is” and “closed” are not. Similarly, “texas ave” is a valid and preferred bigram\n(over two unigrams).\nSpeciﬁcally, as shown in Algorithm 1, we ﬁrst tokenize all location names in the gazetteer to construct\nthe n-gram model and then save the resulting lists of unigrams, bigrams, and trigrams (Lines 2-5). Next,\nfor bigrams and trigrams, in Lines 6-9 we create conditional frequency distributions (CFD) to count the\ncollocations (i.e., c(·) in equations 1-2). Conditional probability distributions (CPD) are then constructed\nfrom the recorded n-grams using maximum likelihood estimation (MLE). We make the assumption that\nonly the previous two words determine the probability of the next word (Markovian assumption of order\ntwo)3. MLE assumes zero probability values to tokens missing from the gazetteers. This data sparsity\nproblem is mitigated by augmenting the gazetteers with location name variants(see Section 2.2). In lines\n11-13, we determine the validity of an n-gram (the string s) using the boolean function VALID-N-GRAM\nwith the help of equations 1-4, where c(wy\nx) ≡c(wxwx+1 . . . wy), wy\nx is the collocation count (i.e., the\noccurrences of the consecutive words, wx to wy), P(wz|wy\nx) is the conditional probability of a word wz\ngiven previous collocationwy\nx, and the chain of probabilitiesP1 (for unigrams), P2 (for bigrams), and P3\n(for tri or larger grams).\nP(wz|wy\nx) = c(wz\nx)\nc(wy\nx) (1)\nP1 = P(w1\n1) = c(w1)\n∑|unigrams|\ni=1 c(wi)\n(2)\nP2 = P(w2\n1) =P1 ×p(w2 |w1\n1) (3)\nP(wn\n1 ) =P2 ×\nn∏\ni=3\nP(wi |wi−1\ni−2), n≥3 (4)\n2.2 Gazetteer Augmentation and Filtering\nAlgorithm 1Language Model Generation\n1: procedure COMPUTE -MODEL (Gazetteer)\n2: for ln ∈Gazetteer do\n3: unigrams ←tokenize(ln);\n4: bigrams, trigrams←generate from unigrams;\n5: end for\n6: for n-grams ∈[bigrams, trigrams] do\n7: CFD ←create using n-grams;\n8: CPD ←create using CFD;\n9: end for\n10: end procedure\n11: procedure VALID -N-G RAM (string = s): boolean\n12: wn\n1 = (w1, . . . , wn) ←tokenize(s);\nreturn P(wn\n1 ) > 0 ⊿ calculated using the equations (1-4);\n13: end procedure\nWe faced two primary challenges when building our language model using raw gazetteers, which are not\nadequately explored in (Middleton et al., 2014; Weissenbacher et al., 2015):\n1. Conditional Collocation Contractions: Some atomic n-gram location names (collocations) can-\nnot be shortened, e.g., “New York”. However, contraction does preserve the meaning of some\nlonger names, especially when the ﬁrst and the last words denote a speciﬁc part and a generic part\nrespectively, such as in “Balalok School”.\n2. Auxiliary and Spurious Content: Gazetteer entries may contain extraneous content that can cause\nlocation matching to fail. Cleaning such entries improve matching reliability (see Table 1).\nTo address these challenges, we exploited Category Ellipsis (for collocation contraction) and Location\nEllipsis (for ﬁltering the auxiliary content) as follows:\n3We found in our dataset that roughly 98% of location mentions in tweets have less than three words.\n1. Skip-grams: Given a location name t1 . . . tn, we retain t1 and tn while varying t2 . . . tn−1. To\navoid adding “City York” as a legitimate variant of the location name “City College of New York”,\nwe require tn to be a location category name (e.g., building, road). Therefore, “Balalok Matric-\nulation Higher Secondary School” generates {Balalok School, Balalok Secondary School,\n...}. This technique results in a small number of contractions that are either useful collocations or\nare too random to cause many false positives (Guthrie et al., 2006).\n2. Filtering: To address bracketed auxiliary content, we compiled a generic list of phrases to remove\nspeciﬁc words on a case-by-case basis (e.g., 1-2 in Table 1). The remaining bracketed names are\ndeemed legitimate alternatives (e.g., 3-4 in Table 1). We treat hyphenated location names as Loca-\ntion Ellipsis and split them on the hyphen and add the two splits (e.g., 5 in Table 1). We expect that\nthe majority of these location names represent a partonomy relationship where the hyphen may be\nread as a “part of” relation between split tokens. We do not add the second token as a variant when\nit already exists in the gazetteer on its own as location name entity (e.g., “Hammond” in “Pilot -\nHammond”).\nContent Description Example Gazetteer Record\n1 Descriptive Tags (Private Road)\n2 Life-cycle/Status Tags Little Rock School (historical)\n3 Alternative/Old Names Scenic Road (Frontage Road)\n4 Acronyms International House of Pancakes (IHOP)\n5 Hyphenations Cars India - Adyar, Pilot - Hammond\nTable 1: Extraneous text in raw gazetteers\nThese two methods augment and ﬁlter partial OSM,\nGeonames, and DBpedia gazetteers sliced from the origi-\nnal sources using a bounding box. Further, we can attach\nthe metadata of the original location name to the gener-\nated variants. Moreover, by treating derived names as syn-\nonyms for existing names, we avoid creating additional\ndemands on disambiguation or equivalencing. We add the derived, variant location name to the gazetteer\nas long as it does not collide with an existing location name. Additional ﬁltering of proposed variants is\nrequired to prevent false alarms. Similar to the use case in (Weissenbacher et al., 2015; Gelernter and Bal-\naji, 2013), we compiled a list of 11,203 words including 678 inseparable bigrams, such as “Building A”,\nas gazetteer stop words. This list also includes unusual location names (e.g., “Boring” in Maryland and\n“Why” in Arizona) and proper nouns (e.g., “James” in Mississippi) that could appear as non-location\ntokens. We then eliminate from all gazetteers the location names that overlap with our gazetteer stop\nwords to reduce false positives.\n2.3 Tweet Preprocessing\nTo complement the gazetteer preprocessing, we also require potentially non-trivial tweet preprocessing.\nWe start by removing the retweet handles, URLs, non-ASCII characters, and all user mentions. Then,\nwe tokenize tweets using TweetMotif’s Twokenizer (O’Connor et al., 2010), which treats hashtags, men-\ntions, and emoticons as a single token. We do not tokenize on periods (e.g., “U.S.”).\nHashtag Segmentation: In our datasets, on average, around 29% of the hashtags include location\nnames. Excluding hashtags used to crawl the data, around 17% of the unique hashtags contain locations.\nAs the number of locations in hashtags is signiﬁcant, similar to (Malmasi and Dras, 2015), we adopted a\nstatistical word segmentation algorithm to break hashtags for location spotting (Norvig, 2009).\nSpelling Correction: We consider a tweet token as misspelled if it is an out-of-vocabulary token,\nwhere the vocabulary is gazetteer words and a large English vocabulary word list 4. LNEx corrects\nall misspelled tokens using the Symmetric Delete Spelling Correction algorithm (SymSpell) 5 that is six\norders of magnitude faster than Norvig’s spelling corrector (Norvig, 2009), which was used by (Gelernter\nand Zhang, 2013) in their location extraction tool. As we shall see, spelling correction has only a small\ninﬂuence on system accuracy.\n2.4 Extracting Location Names using LNEx\nn e w\na va d i\nrd\nro a d\nn e w  \na va d i\na va d i \nro a d\nn e w  \na v a d i \nro a d\nc h e n n a i\nN e w  \nA va d i \nR d  \nis \nclo se d  \n# ch e n n a iflo o d\nrd\nro a d\nn e w\na va d i\nch e n n a i\na\nb\nve cto rs\nFigure 1: Extracting Locations using LNEx\nAfter the modiﬁcations to gazetteers and texts, LNEx\nextracts locations as illustrated in Fig. 1. In a , LNEx\nreads the raw tweet text, preprocesses it (as in Section\n4https://github.com/norrissoftware/words3\n5https://github.com/wolfgarbe/symspell\n2.3) starting with case-folding. After tokenizing the\ntweet, the hashtag segmenter breaks hashtags into tokens. Later, stop words are used to split a tweet into\nconsecutive word fragments where each tweet split of sizen can have zero ton potential location names.\nWe custom build the tweet stop list starting with around 890 words 6 excluding the gazetteer unigrams.\nLNEx now takes each tweet split and converts each of its tokens into a vector of tokens v using two\ndictionaries: the USPS street sufﬁxes dictionary 7 and the English OSM abbreviations dictionary8. This\nadds possible expansions and abbreviations of a token (e.g., “Rd” to “Road”, and vice versa). This\novercomes the lexical variations between location mentions in tweets and their corresponding gazetteer\nentries.\nIn b , the language model is used to ﬁnd the valid n-grams from the Cartesian product of the con-\nsecutive vectors. It builds a bottom up tree for each tweet split starting from 1 to n-grams by gluing the\nconsecutive tokens together if they represent a valid segment in the gazetteer. We improve the speed of\nthe algorithm signiﬁcantly by splitting the tweet and eliminating invalid n-grams. LNEx then selects a\nsubset of valid n-grams from the tree; for the overlapping n-grams, we prefer the longest full mentions\n(e.g, “New Avadi Road” over “Avadi Road”) and keep both if they are of the same length. When full\nlocation names appear inside partial ones, we keep only full names (e.g., extracting “Louisiana” from\n“The Louisiana”).\nTime and Space Complexities: LNEx extracts and links a full location mention to its corresponding\ngazetteer entry through a simple dictionary lookup that takes constant timeO(1). The location extraction\ntime is bounded by the time for creating the bottom up tree of tokens which takes O(|v|s) where |v|is\nthe length of the longest vector of token synonyms (i.e., all the expansions and abbreviations of a token)\nand s is the largest number of tokens with synonyms in a location name. Splitting the tweet into smaller\nfragments signiﬁcantly lowers the asymptotic growth of the algorithm enabling stream processing. In\npractice, for our dictionaries and gazetteers, |v| ≤4 and s ≤3. So, a pessimistic upper bound on\nthe number of candidates for each location (though rarely realized) is 43. The space complexity of the\nmethod is bounded by the product of number of gazetteer entries, L, and the number of variants of a\nlocation name (Skip-gram method 1), that is,2m−2, where m is the number of tokens in a location name.\nEffectively, the space complexity is O(L.2m−2) where typically, 2 ≤m ≤5. Further, according to our\ntests, LNEx needed only up to 650 MB of memory and is able to process, on average, 200 tweets per\nsecond.\n3 Experimental Results\nTo demonstrate the effectiveness of our context-aware location extractor, we used a set of event-speciﬁc\nhashtags and keywords to collect 4,500 geographically limited, disaster-related tweets from three differ-\nent targeted streams corresponding to ﬂoods in Chennai, Louisiana, and Houston. Below, we categorize\nand annotate these tweets used for benchmarking each component of LNEx, and for comparing LNEx\nwith other state-of-the-art tools for the location extraction task.\n3.1 Benchmarking and Annotations\nConsistent with the problem of determining whether a location mention is inside the area of interest, our\nbenchmark categorization scheme is not content based as in (Matsuda et al., 2015; Gelernter and Balaji,\n2013). To better identify and characterize the challenges in extracting location names accurately, our\nannotation scheme is based on where these locations lie in relation to the area of interest. For example,\nwith respect to Chicago, IL, USA:\n1. inLOC: Locations inside the area of interest, (e.g., Millennium Park or Burlington Ave.)\n2. outLOC: Locations outside the area of interest, (e.g., Central Park, 5th Ave, New York.)\n3. ambLOC: Ambiguous locations that need context for identiﬁcation, (e.g., “our house”)\n6http://www.ranks.nl/stopwords\n7http://pe.usps.gov/text/pub28/28apc_002.htm\n8wiki.openstreetmap.org/wiki/Name_finder:Abbreviations\nIn contrast to (Matsuda et al., 2015; Gelernter and Balaji, 2013), our categorization is not based on\nlocation types (e.g., buildings, facilities, schools) but on the relative position (i.e., inLOC or outLOC)\nand the nature of the location mention (i.e., inLOC or ambLOC). This approach identiﬁes the true scope\nof challenges in extracting location names. Other schemes that annotate for a limited set of location\ntypes, such as “Geoparse Twitter Benchmark Dataset”, miss obvious location mentions in tweets, such\nas “New Zealand” and “Christchurch”, making the dataset incompatible for testing the tools mentioned\nin this paper including LNEx9.\nRT @GraceLimWeather: Evangeline Parish Sherif\n20 people evacuated from flooded homes in Ville Platte. #lawx\nT o p o n y m \n1\n2\nb r a t /HazardSEES-BRAT/Louisiana/final_set/764483789040918529\nOKLogin required to perform \"searchT\nambLoc inLoc inLoc\nFigure 2: Example Annotations using BRAT\nTweet Annotations Figure 2 shows an example of\nmanual annotation from the Louisiana ﬂood tweets us-\ning the BRAT tool (Stenetorp et al., 2012). It allows us\nto deﬁne search functionalities and additional resources for the annotators to use such as Google Maps.\nThe annotators annotated three datasets: the 2015 Chennai ﬂood, the 2016 Louisiana ﬂood, and the\n2016 Houston ﬂood. In Chennai, they spotted 4,589 location names (75% inLOC, 4% outLOC, and\n21% ambLOC); in Louisiana, 2,918 (66% inLOC, 13% outLOC, and 22% ambLOC); and in Houston,\n4,177 (66% inLOC, 7% outLOC, and 27% ambLOC). We randomly selected 1k tweets (500 each from\nChennai and Louisiana) as a development set and the remaining 3.5k as the test set for evaluation.\n3.2 Evaluation Strategy\nBecause BRAT records the start and the end character offsets of the annotated LNs, we evaluate the\nextraction task by checking the character offsets of the spotted location name in comparison with the\nannotated data. We used the standard comparison metrics: Precision, Recall, and the balanced F-Score.\nIn the case of overlapping or partial matches, we penalize all tools by adding 1\n2 FP (False Positive)\nand 1\n2 FN (False Negative) to the precision and recall equations (e.g., if the tool spots “The Louisiana”\ninstead of “Louisiana”).\nWe evaluate all tools based on the category of the extracted location in our annotation scheme. For\nthe inLOC mentions, we count all hits and misses of a tool and ignore all hits when the category of the\nextracted location is outLOC or ambLOC. However, we take a particularly conservative approach and\nadditionally penalize LNEx for extracting location names of outLOC and ambLOC categories, counting\nthem as false positives (FPs) as our tool is not supposed to extract these.\nSpell Checking: This led to 1% increase in recall but the F-Score decreased by 2% on average due\nto the inﬂuence of increased false positives on precision. In the ﬁnal system, we opted to exclude the\nspelling corrector component.\nHashtag Breaking: We evaluated the performance of the hashtag breaking component only on the\nhashtags that contain locations. The accuracies were 97%, 87%, and 93% for Chennai, Louisiana, and\nHouston respectively, reduced due to examples such as, “#lawx” which was broken into “law” and “x”.\nPicking a Gazetteer: The augmentation and ﬁltering of gazetteers improved the F-Scores (See Figure\n3-a). After this process, combinations of gazetteer sources had similar performance where the difference\nbetween the worst and the best was around 0.02 F-Score units (see Figure 3-b). In the ﬁnal system,\nwe relied on OSM, which performed the best. Moreover, DBpedia is not focused on geographical in-\nformation; therefore, it does not contain the metadata useful for the system’s future use (e.g., extents\nand full addresses). Also, OSM has more ﬁne-grained locations and more accurate geo-coordinates than\nGeonames (Gelernter et al., 2013).\nComparing LNEx with other tools:We compared LNEx with the following tools:\n1. Commercial Grade: Google NL 10, OpenCalais11, and Yahoo! BOSS PlaceFinder 12. All of these\ntools have REST APIs and are black box tools that use Machine Learning.\n9Our dataset can be used to test any location extraction tool by ignoring the optional additional expressivity, which makes\nour dataset more compatible than other available ones.\n10https://cloud.google.com/natural-language/\n11http://www.opencalais.com/\n12https://developer.yahoo.com/boss/geo/\n    0.68\n    0.70\n    0.72\n    0.74\n    0.76\n    0.78\n    0.80\n    0.82\n    0.84\n    0.86\n001 011 010 111 101 110 100\nAverage F-Score\nRAW AF(a)\n    0.72\n    0.74\n    0.76\n    0.78\n    0.80\n    0.82\n    0.84\n    0.86\n    0.88\n    0.90\n010 011 001 111 110 101 100\nF-Score\nGazetter-Combination\nAverage Chennai Louisiana Houston(b)\nGazetter-Combination\nFigure 3: (a) Raw vs. augmented and ﬁltered ( AF) gazetteers combinations. (b) Combinations Perfor-\nmance. Each of the seven combinations is a subset of {OSM Geonames DBpedia }.\n2. General Purpose NER: Stanford NER (SNER) and OpenNLP Name Finder. SNER learns a linear\nchain Conditional Random Field (CRF) sequence model (Finkel et al., 2005), while OpenNLP uses\nthe maximum entropy (ME) framework (Bender et al., 2003). We trained both tools interchange-\nably on our annotated datasets in addition to all the data from W-NUT ’16 13 while retaining the\nannotations and unifying the classes we consider as locations (i.e., geo-loc, company, facility) into\none type. We used LNEx-OSM gazetteer’s features while training SNER. Additionally, we used\nDBpedia Spotlight (Mendes et al., 2011).\n3. Twitter NLP: OSU Twitter NLP (Ritter et al., 2011) and TwitIE-Gate (Bontcheva et al., 2013). Both\nare pipelined systems of POS-tagging followed by NER. TwitIE-GATE also supports normalization,\ngazetteer lookup, and regular expression-based tagging. For fair comparison, we also augmented\nthem with LNEx OSM gazetteers.\n4. Twitter Location Extraction: Geolocator 3.0 (Gelernter and Zhang, 2013) and Geoparsepy (Mid-\ndleton et al., 2014). Geolocator 3.0 uses a tweet-trained CRF classiﬁer and other rule-based models\nto extract street names, building names, business names, and unnamed locations (i.e., location names\ncontaining a category such as “School”).\nGoogle NLP Location, Organization\nOpenCalais City, Company, Continent, Country, Facility,\nOrganization, ProvinceOrState, Region, TVStation\nDBpedia Spotlight Place, Organization\nOSU TwitterNLP Geo-Location, Company, Facility\nTwitIE-Gate Location, Organization\nTable 2: Types considered as Locations per tool\nAll tools have been evaluated using the same\nmetrics and on the same annotated data. In the\ncase of hashtags, we count all hits for all tools and\nwhen a tool missed, we penalized only the ones\nthat were designed to break hashtags (namely,\nTwitIE-Gate and LNEx).\nAdditionally, we consider all spotted mentions\nfrom PlaceFinder, Geolocator 3.0, Geoparsepy,\nSNER, and OpenNLP as location names.\n0%\n20%\n40%\n60%\n80%\n100%\n120%\nChennai Louisiana Houston\nFrequency\nLocations in Hashtags\nappreviations in Hashtags\nacronyms in Hashtags\nCompound mentions\nambLOC\ninLOC\ne.g., \"Houston-Texas\"\nFigure 4: Random Sample Evaluation.\nAs for the other tools, we consider only the en-\ntity types in Table 2 as locations.\nFig. 4) shows that the prevalence of various\nchallenges differ in the three corpora. Neverthe-\nless, LNEx outperformed all other tools on all\ndatasets in terms of F-Score, and the average F-\nScore (see Table 3). LNEx showed stability on\nthe test and development sets from Louisiana with\nonly a 0.2% F-Score reduction and around a 2.6%\nreduction on the test set from Chennai.\nDatasets Chennai Louisiana Houston A VG\nP R F P R F P R F F\nGoogle NLP 0.40 0.49 0.44 0.55 0.75 0.64 0.39 0.51 0.44 0.51\nOpenCalais 0.43 0.10 0.17 0.81 0.77 0.78 0.62 0.35 0.45 0.47\nDBpedia Spotlight 0.31 0.44 0.36 0.57 0.88 0.70 0.35 0.53 0.42 0.50\nYahoo! PLaceFinder 0.67 0.39 0.49 0.83 0.80 0.81 0.64 0.42 0.50 0.61\nStanford NER 0.72 0.29 0.41 0.78 0.42 0.55 0.74 0.32 0.45 0.47\nOpenNLP 0.55 0.15 0.24 0.62 0.19 0.29 0.60 0.23 0.34 0.29\nOSU TwitterNLP 0.74 0.40 0.52 0.84 0.69 0.76 0.66 0.39 0.49 0.59\nTwitIE-Gate 0.51 0.36 0.43 0.66 0.84 0.74 0.35 0.39 0.37 0.52\nGeolocator 3.0 0.43 0.54 0.48 0.32 0.71 0.44 0.38 0.58 0.46 0.46\nGeoparsepy 0.41 0.28 0.33 0.45 0.72 0.55 0.44 0.46 0.45 0.45\nLNEx-RawGaz 0.80 0.78 0.79 0.51 0.80 0.62 0.63 0.66 0.64 0.69\nLNEx-AFGaz 0.91 0.80 0.85 0.83 0.81 0.82 0.87 0.67 0.76 0.81\nTable 3: Tools vs. LNEx with a raw (RawGaz) or\naugmented and ﬁltered gazetteer (AFGaz).\nThe augmentation and ﬁltering method signif-\nicantly improved the average F-Score from 0.69\nto 0.81. However, limitations of the gazetteer\naugmentation and ﬁltering methods did contribute\nto lowering precision. For example, on average,\naround 5% of the extracted location names were\noutLOC and ambLOC, mistakenly extracted from\nChennai, Louisiana, and Houston tweets. Exam-\nple errors include the augmentation of location\n13http://noisy-text.github.io/2016\nnames such as “The x Apartments” to “The Apartments”, causing LNEx to extract the phrase “The\nApartments” as an actual full location name. Fixing such limitations should contribute to around 2%\nF-Score improvement on average.\nWe trained Stanford NER (SNER) and OpenNLP to emulate their use in other studies mentioned in\nSection 4. Their performances were calculated by interchangeably training them using three datasets at\na time and testing on the fourth one (the gazetteer of the area of the test data was also used in training the\nSNER models). We always used the W-NUT ’16 dataset to train the models with more than 10k tweets\neach time.\nWe observed that the ill-formatted text of tweets with ungrammatical text and missing orthographic\nfeatures impact the F-Score of tools we compared with LNEx. While the performance of each tool\ndiffers, we observed that Google heavily relies on orthographic features and expects grammatical texts\n(although it scored a 0.38 average F-Score). Additionally, TwitIE-GATE was not always successful in\nextracting location names from hashtags or text even if they are part of the gazetteers that we added to\nthe tool. Finally, OpenCalais extracts only well-known location names of coarser granularity than street\nand building levels unless a location has an attached location category (e.g., school or street).\nOriginal\nText\nsou th kr koil street near Oxford school.west mambalam..\nWe r lucky where I am in New Iberia. #PrayForLouisiana #lawx\nDidn’t Houston have a bad ﬂood last year now again poor htown\nManual\nAnnotations\n& Types\n(\nmisspelling\n  \nsou th kr\nT6\nkoil street) near\nT6\n  \n(Oxford school).(west mambalam)..\nWe r lucky where I am in\nT1\n  \n(New Iberia).\nT3\n  \n#PrayFor(Louisiana)\nT4\n  \n#( la\nT5\n)wx\nDidn’t(Houston)\n  \nT1\nhave a bad ﬂood last year now again poor (htown)\n  \nT5\nGoogle\nNLP\nsou th kr (koil street) near (Oxford) school.west (mambalam)..\nWe r lucky where I am in (New Iberia). #PrayForLouisiana #lawx\nDidn’t (Houston) have a bad ﬂood last year now again poor htown\nOpenCalais\nsou th kr koil street near (Oxford school).west mambalam..\nWe r lucky where I am in New Iberia. #PrayForLouisiana #lawx\nDidn’t (Houston) have a bad ﬂood last year now again poor htown\nDBpedia\nSpotlight\nsou th kr koil street near (Oxford) school.west (mambalam)..\nWe r lucky where I am in (New Iberia). #PrayForLouisiana #lawx\nDidn’t (Houston) have a bad ﬂood last year now again poor htown\nYahoo!\nPlaceFinder\nsou (th) kr koil street near (Oxford) school.west mambalam..\nWe r lucky where I am in (New Iberia). #PrayForLouisiana #lawx\nDidn’t Houston have a bad ﬂood last year now again poor htown\nStanford\nNER\nsou th kr koil street near Oxford school.west mambalam..\nWe r lucky where I am in (New Iberia). #PrayForLouisiana #lawx\nDidn’t Houston have a bad ﬂood last year now again poor htown\nOpenNLP\nsou th kr (koil street) near (Oxford) school.west mambalam..\nWe r lucky where I am in (New Iberia.) #PrayForLouisiana #lawx\nDidn’t Houston have a bad ﬂood last year now again poor htown\nOSU\nTwitterNLP\nsou th kr koil street near (Oxford) school.west mambalam..\nWe r lucky where I am in (New Iberia). #PrayForLouisiana #lawx\nDidn’t Houston have a bad ﬂood last year now again poor htown\nTwitIE-Gate\nsou th kr koil (street) near (Oxford) school.(west mambalam)..\nWe r lucky where I am in New Iberia. #PrayForLouisiana #lawx\nDidn’t Houston have a bad ﬂood last year now again poor htown\nGeolocator\n3.0\n(sou th) (kr) (koil) street near (Oxford school).(west mambalam)..\nWe r lucky where I am in (New Iberia). #PrayForLouisiana #lawx\nDidn’t (Houston) have a bad ﬂood last year now again poor (htown)\nGeoparsepy\nsou (th) (kr) koil street near (Oxford) school.west mambalam..\nWe r lucky where I am in (New (Iberia)). #PrayForLouisiana #lawx\nDidn’t (Houston) have a bad ﬂood last year now again poor htown\nsou th kr koil street near (Oxford school).(west mambalam)..\nLNEx We r lucky where I am in (New Iberia). #PrayFor(Louisiana) #lawx\nDidn’t (Houston) have a bad ﬂood last year now again poor htown\nTable 4: Example tool outputs: bracketed bold\ntext are the identiﬁed LNs and braces highlights\nthe types from Fig. 4.\nIllustrative Examples: Table 4 shows the com-\nparative handling of three tweets one each from\nChennai, Louisiana, and Houston datasets, cover-\ning most challenges by all the tools. The loca-\ntion name “Oxford school” allowed us to exam-\nine if a tool relies on capitalization for delimita-\ntion. Only OpenCalais, Geolocator and LNEx were\nable to extract the name correctly while the rest ei-\nther partially extracted it or missed it. For exam-\nple, PlaceFinder extracted “Oxford” and geocoded\nit with the geocodes of Oxford city in England. Al-\nthough SNER and OpenNLP were trained on the\nsame datasets, OpenNLP extracted Oxford while\nSNER did not, which suggests that the cue word\n“near” was insufﬁcient evidence for SNER to spot\nat least Oxford. Correspondingly, since “New\nIberia” is a correctly capitalized full location name,\nalmost all tools were able to extract it. However,\nTwitIE-Gate missed it although it is part of the\ngazetteer we added to the tool, and Geoparsepy ex-\ntracted Iberia in addition to the full mention, not\nfavoring the longest mention as LNEx. OpenCalais\nis a black box so we don’t know why it failed.\nRegarding T3-T5 annotations, LNEx and\nTwitIE-Gate are designed to break hashtags but\nTwitIE-Gate was not able to extract any locations\nfrom the hashtags in the table. LNEx extracted\n“Louisiana” but was not able to extract “la” from\n“#lawx” due to the statistical method which broke\nthe hashtag into “law” and “x” since this com-\nbination is more probable. Only Geolocator was\nable to extract the Houston nickname “htown”.\nIn the future, a dictionary of region-speciﬁc\nacronyms, abbreviations, and nicknames can\naugment LNEx’s region-speciﬁc gazetteers.\nGoogle NLP does not handle T6. Adding space\nbetween the dot and “west” to create “ . . .school. west . . .”, results in the extraction of “west mam-\nbalam” but omits “Oxford school”. Google NLP relies on capitalization and so that changing the case\nof “s” to create “Oxford School” does help. OpenCalais cannot extract “west mambalam” despite ﬁxing\nall grammatical mistakes, normalizing the orthographic features, and even introducing cue words. The\ntool only extracts well-known location names of coarser granularity than street and building levels unless\nthey have an attached location category (e.g., school or street). PlaceFinder, on the other hand, tries to\nﬁnd geocodable location names in text. Therefore, the tool extracts “th” as the country code of Thailand\nand “Oxford” as the city in England. Hence, geocoding is inﬂuencing some of the mistakes of the tool.\n4 Related Work\nTwitter messages (tweets) lack features exploited by main stream NLP tools. Informality, ill-formed\nwords, irregular syntax and non-standard orthographic features of tweets challenge such tools (Kauf-\nmann and Kalita, 2010). We agree with (Baldwin et al., 2013) that some issues might be exaggerated.\nIndeed we found that spelling corrections only contributed to 1% recall improvement. Nevertheless, text\nnormalization alone is insufﬁcient for NER (Derczynski et al., 2015). Specially designed tools such as\n(Ritter et al., 2011; Gelernter and Zhang, 2013) use pipelined systems of POS tagging followed by NER.\nThe latter also perform Regex tagging, normalization, and gazetteer lookup.\nRelying on the orthographic features for POS tagging or Regex tagging, previous methods extract\nlocations from the text chunks and phrases of sentences using the following techniques:\n1. Gazetteer search orn-gram matching: Li et al. (2014) and Gelernter and Zhang (2013) use a\ngazetteer matching technique that relies on a segment-based inverted index. Sultanik and Fink\n(2012) use an exhaustiven-gram technique. Middleton et al. (2014) use location-speciﬁc gazetteers\nfor matching phrases from tweets. TwitIE-GATE uses a gazetteer lookup component. All of these\ntechniques do not deal with the important issue of the gazetteers’ auxiliary content and noise.\n2. Handcrafted rules: Weissenbacher et al. (2015) and Malmasi and Dras (2015) use pattern and\nRegex matching which rely on cue words or orthographic features for POS-tagging. TwitIE-GATE\nadapts rules from ANNIE (Cunningham et al., 2002) for extraction.\n3. Supervised Methods: Tweet-trained models: The majority of the methods trained SNER on tweets\n(Gelernter and Zhang, 2013; Yin et al., 2014) or retrained OpenNLP (Lingad et al., 2013). News-\ntrained models: Malmasi and Dras (2015) use tools like SNER and OpenNLP.\n4. Semi-supervised methods: Ji et al. (2016) use beam search and structured perceptron for extraction\nand linking to Foursquare entities. However, they did not address the noise that is prevalent in such\nsources (e.g., “my sofa” or “our house”) (Dalvi et al., 2014).\nThe closest works to ours are TwiNER (Li et al., 2012) and LEX (Downey et al., 2007). Both use\nMicrosoft Web n-grams (which capture language statistics) for chunking but the former uses DBpedia\nfor entity linking. However, our method exploits a region-speciﬁc gazetteer for delimitation and linking.\nMoreover, LEX worked with web data and relies heavily on capitalization.\nFinally, few other methods extract locations from hashtags. Malmasi and Dras (2015) uses a statistical\nhashtag breaker technique similar to ours. Ji et al. (2016) removes only the # symbol and treats the hash-\ntag as a unigram. Yin et al. (2014) uses a greedy maximal matching method for breaking. TwitIE-GATE\nuses two methods for hashtag breaking: a dynamic programming-based method for ﬁnding subsequences\nand a camel-case-based method for tokenization.\n5 Conclusions and Future Work\nLNEx accurately spots locations in text relying solely on statistical language models synthesized from\naugmented and ﬁltered region-speciﬁc gazetteers. It outperforms state-of-the-art techniques and main-\nstream location name extractors. By exploiting the knowledge in the gazetteer, we retain the beneﬁts of\nn-gram matching to access location metadata. LNEx does not employ any training and does not depend\non syntactic analysis or orthographic conventions. We compensate for limitations in ﬁxed phrase match-\ning with gazetteer augmentation and ﬁltering. Although we do not solve the disambiguation problem\nhere, still the geo/geo ambiguity is reduced by preserving the spatial context through location-speciﬁc\ngazetteers. Furthermore, systematic gazetteer augmentation ties legitimate variants to known locations,\nminimizing potential ambiguity.\nCertainly, LNEx does not solve all location extraction problems. As the method is driven by the\nlinking procedure, it does not extract location names missing from gazetteers (e.g., “our house”). It\nactually presents an effective precision-recall trade-off apparent in the F-Score. In the future, a more\nsophisticated name model that ignores the generic parts and retains the speciﬁc parts when augmenting\na location name (e.g., adding “Sam’s” as a variant of “Sam’s Club”) can be used (Dalvi et al., 2014).\nAcknowledgments\nThis research was partially supported by the NSF award EAR-1520870 “Hazards SEES: Social and\nPhysical Sensing Enabled Decision Support for Disaster Management and Response”. We would like\nto also thank Jibril Ikharo for introducing us to the Nameheads work and our other colleagues from\nKno.e.sis for helping us in data annotation.\nReferences\n[Baldwin et al.2013] Timothy Baldwin, Paul Cook, Marco Lui, Andrew MacKinlay, and Li Wang. 2013. How\nnoisy social media text, how diffrnt social media sources? In International Joint Conference on Natural Lan-\nguage Processing, pages 356–364.\n[Bender et al.2003] Oliver Bender, Franz Josef Och, and Hermann Ney. 2003. Maximum entropy models for\nnamed entity recognition. In Proceedings of the Seventh Conference on Natural Language Learning at HLT-\nNAACL 2003 - Volume 4, CONLL ’03, pages 148–151, Stroudsburg, PA, USA. Association for Computational\nLinguistics.\n[Bontcheva et al.2013] Kalina Bontcheva, Leon Derczynski, Adam Funk, Mark A Greenwood, Diana Maynard, and\nNiraj Aswani. 2013. Twitie: An open-source information extraction pipeline for microblog text. In RANLP,\npages 83–90.\n[Carroll1983] John M Carroll. 1983. Nameheads. Cognitive science, 7(2):121–153.\n[Cunningham et al.2002] Hamish Cunningham, Diana Maynard, Kalina Bontcheva, and Valentin Tablan. 2002.\nGate: an architecture for development of robust hlt applications. In Proceedings of the 40th annual meeting on\nassociation for computational linguistics, pages 168–175. Association for Computational Linguistics.\n[Dalvi et al.2014] Nilesh Dalvi, Marian Olteanu, Manish Raghavan, and Philip Bohannon. 2014. Deduplicating\na places database. In Proceedings of the 23rd international conference on World wide web , pages 409–418.\nAssociation for Computing Machinery (ACM).\n[Derczynski et al.2015] Leon Derczynski, Diana Maynard, Giuseppe Rizzo, Marieke van Erp, Genevieve Gorrell,\nRapha¨el Troncy, Johann Petrak, and Kalina Bontcheva. 2015. Analysis of named entity recognition and linking\nfor tweets. Information Processing & Management, 51(2):32–49.\n[Downey et al.2007] Doug Downey, Matthew Broadhead, and Oren Etzioni. 2007. Locating complex named enti-\nties in web text. In IJCAI, volume 7, pages 2733–2739.\n[Finkel et al.2005] Jenny Rose Finkel, Trond Grenager, and Christopher Manning. 2005. Incorporating non-local\ninformation into information extraction systems by gibbs sampling. In Proceedings of the 43rd Annual Meeting\non Association for Computational Linguistics , ACL ’05, pages 363–370, Stroudsburg, PA, USA. Association\nfor Computational Linguistics.\n[Gelernter and Balaji2013] Judith Gelernter and Shilpa Balaji. 2013. An algorithm for local geoparsing of micro-\ntext. GeoInformatica, 17(4):635–667.\n[Gelernter and Zhang2013] Judith Gelernter and Wei Zhang. 2013. Cross-lingual geo-parsing for non-structured\ndata. In Proceedings of the 7th Workshop on Geographic Information Retrieval, pages 64–71. Association for\nComputing Machinery (ACM).\n[Gelernter et al.2013] Judith Gelernter, Gautam Ganesh, Hamsini Krishnakumar, and Wei Zhang. 2013. Automatic\ngazetteer enrichment with user-geocoded data. In Proceedings of the Second ACM SIGSPATIAL International\nWorkshop on Crowdsourced and Volunteered Geographic Information, pages 87–94. Association for Computing\nMachinery (ACM).\n[Guthrie et al.2006] David Guthrie, Ben Allison, Wei Liu, Louise Guthrie, and Yorick Wilks. 2006. A closer\nlook at skip-gram modelling. In Proceedings of the 5th international Conference on Language Resources and\nEvaluation (LREC-2006), pages 1–4.\n[Hazas et al.2004] Mike Hazas, James Scott, and John Krumm. 2004. Location-aware computing comes of age.\nComputer, 37(2):95–97.\n[Hoang and Mothe2018] Thi Bich Ngoc Hoang and Josiane Mothe. 2018. Location extraction from tweets. Infor-\nmation Processing & Management, 54(2):129–144.\n[Ji et al.2016] Zongcheng Ji, Aixin Sun, Gao Cong, and Jialong Han. 2016. Joint recognition and linking of ﬁne-\ngrained locations from tweets. In Proceedings of the 25th International Conference on World Wide Web, pages\n1271–1281. International World Wide Web Conferences Steering Committee.\n[Kaufmann and Kalita2010] Max Kaufmann and Jugal Kalita. 2010. Syntactic normalization of twitter messages.\nIn International conference on natural language processing, Kharagpur, India.\n[Li et al.2012] Chenliang Li, Jianshu Weng, Qi He, Yuxia Yao, Anwitaman Datta, Aixin Sun, and Bu-Sung Lee.\n2012. Twiner: named entity recognition in targeted twitter stream. In Proceedings of the 35th international\nACM SIGIR conference on Research and development in information retrieval, pages 721–730. Association for\nComputing Machinery (ACM).\n[Li et al.2014] Guoliang Li, Jun Hu, Jianhua Feng, and Kian-lee Tan. 2014. Effective location identiﬁcation from\nmicroblogs. In Data Engineering (ICDE), 2014 The Institute of Electrical and Electronics Engineers (IEEE)\n30th International Conference on, pages 880–891. The Institute of Electrical and Electronics Engineers (IEEE).\n[Licht et al.2017] Yehoshua Zvi Licht, David Allen Turner, and Joseph Arnold White. 2017. Location context\naware computing, November 30. US Patent App. 15/166,740.\n[Lingad et al.2013] John Lingad, Sarvnaz Karimi, and Jie Yin. 2013. Location extraction from disaster-related\nmicroblogs. In Proceedings of the 22nd International Conference on World Wide Web , pages 1017–1020.\nAssociation for Computing Machinery (ACM).\n[Liu et al.2014] Fei Liu, Maria Vasardani, and Timothy Baldwin. 2014. Automatic identiﬁcation of locative ex-\npressions from social media text: A comparative analysis. In Proceedings of the 4th International Workshop on\nLocation and the Web, pages 9–16. Association for Computing Machinery (ACM).\n[Malmasi and Dras2015] Shervin Malmasi and Mark Dras. 2015. Location mention detection in tweets and mi-\ncroblogs. In International Conference of the Paciﬁc Association for Computational Linguistics (PACL), pages\n123–134. Springer.\n[Manning and Sch¨utze1999] Christopher D. Manning and Hinrich Sch ¨utze. 1999. Foundations of Statistical Natu-\nral Language Processing. MIT Press, Cambridge, MA, USA.\n[Matsuda et al.2015] Koji Matsuda, Akira Sasaki, Naoaki Okazaki, and Kentaro Inui. 2015. Annotating geo-\ngraphical entities on microblog text. In The 9th Linguistic Annotation Workshop held in conjuncion with North\nAmerican Chapter of the Association for Computational Linguistics (NAACL) 2015, page 85.\n[Mendes et al.2011] Pablo N Mendes, Max Jakob, Andr ´es Garc´ıa-Silva, and Christian Bizer. 2011. Dbpedia spot-\nlight: shedding light on the web of documents. In Proceedings of the 7th international conference on semantic\nsystems, pages 1–8. ACM.\n[Middleton et al.2014] Stuart E Middleton, Lee Middleton, and Stefano Modafferi. 2014. Real-time crisis mapping\nof natural disasters using social media. The Institute of Electrical and Electronics Engineers (IEEE) Intelligent\nSystems, 29(2):9–17.\n[Munro2011] Robert Munro. 2011. Subword and spatiotemporal models for identifying actionable information in\nhaitian kreyol. In Proceedings of the ﬁfteenth conference on computational natural language learning , pages\n68–77. Association for Computational Linguistics (ACL).\n[Norvig2009] Peter Norvig. 2009. Natural language corpus data. In T. Segaran and J. Hammerbacher, editors,\nBeautiful Data, chapter 14, pages 219–242. O’Reilly Media.\n[O’Connor et al.2010] Brendan O’Connor, Michel Krieger, and David Ahn. 2010. Tweetmotif: Exploratory search\nand topic summarization for twitter. In ICWSM, pages 384–385.\n[Piskorski and Ehrmann2013] Jakub Piskorski and Maud Ehrmann. 2013. On named entity recognition in tar-\ngeted twitter streams in polish. In The 4th Biennial International Workshop on Balto-Slavic Natural Language\nProcessing: ACL, pages 84–93. Citeseer.\n[Resnick et al.1991] L.B. Resnick, J.M. Levine, and S.D. Teasley. 1991. Perspectives on Socially Shared Cogni-\ntion. American Psychological Association.\n[Ritter et al.2011] Alan Ritter, Sam Clark, Oren Etzioni, et al. 2011. Named entity recognition in tweets: an\nexperimental study. In Proceedings of the Conference on Empirical Methods in Natural Language Processing,\npages 1524–1534. Association for Computational Linguistics (ACL).\n[Son et al.2008] Jeongwook Son, Zeeshan Aziz, and Feniosky Pena-Mora. 2008. Supporting disaster response and\nrecovery through improved situation awareness. Structural survey, 26(5):411–425.\n[Stenetorp et al.2012] Pontus Stenetorp, Sampo Pyysalo, Goran Topi ´c, Tomoko Ohta, Sophia Ananiadou, and\nJun’ichi Tsujii. 2012. BRAT: a web-based tool for NLP-assisted text annotation. InProceedings of the Demon-\nstrations Session at EACL 2012, Avignon, France, April. Association for Computational Linguistics (ACL).\n[Sultanik and Fink2012] Evan A Sultanik and Clayton Fink. 2012. Rapid geotagging and disambiguation of social\nmedia text via an indexed gazetteer. Proceedings of International conference on Information Systems for Crisis\nResponse and Management (ISCRAM), 12:1–10.\n[Weissenbacher et al.2015] Davy Weissenbacher, Tasnia Tahsin, Rachel Beard, Mari Figaro, Robert Rivera,\nMatthew Scotch, and Graciela Gonzalez. 2015. Knowledge-driven geospatial location resolution for phylo-\ngeographic models of virus migration. Bioinformatics, 31(12):i348–i356.\n[Yin et al.2014] Jie Yin, Sarvnaz Karimi, and John Lingad. 2014. Pinpointing locational focus in microblogs. In\nProceedings of the 2014 Australasian Document Computing Symposium , page 66. Association for Computing\nMachinery (ACM).",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7875005006790161
    },
    {
      "name": "Context (archaeology)",
      "score": 0.651649534702301
    },
    {
      "name": "Natural language processing",
      "score": 0.6094896793365479
    },
    {
      "name": "Identification (biology)",
      "score": 0.5891947746276855
    },
    {
      "name": "Word (group theory)",
      "score": 0.5863523483276367
    },
    {
      "name": "Referent",
      "score": 0.5464840531349182
    },
    {
      "name": "Task (project management)",
      "score": 0.5177083015441895
    },
    {
      "name": "Artificial intelligence",
      "score": 0.44345590472221375
    },
    {
      "name": "Named-entity recognition",
      "score": 0.44111359119415283
    },
    {
      "name": "Information retrieval",
      "score": 0.37655484676361084
    },
    {
      "name": "Geography",
      "score": 0.17908820509910583
    },
    {
      "name": "Linguistics",
      "score": 0.16862580180168152
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    }
  ]
}