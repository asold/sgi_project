{
  "title": "Experimenting with Power Divergences for Language Modeling",
  "url": "https://openalex.org/W2970950715",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2251561025",
      "name": "Matthieu Labeau",
      "affiliations": [
        "University of Edinburgh",
        "Language Science (South Korea)"
      ]
    },
    {
      "id": "https://openalex.org/A2133337488",
      "name": "Shay B. Cohen",
      "affiliations": [
        "Language Science (South Korea)",
        "University of Edinburgh"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6632009484",
    "https://openalex.org/W2171928131",
    "https://openalex.org/W2962755094",
    "https://openalex.org/W2120861206",
    "https://openalex.org/W2963932686",
    "https://openalex.org/W2963938518",
    "https://openalex.org/W2150070703",
    "https://openalex.org/W2962832505",
    "https://openalex.org/W3127518054",
    "https://openalex.org/W152055444",
    "https://openalex.org/W1934041838",
    "https://openalex.org/W2963631907",
    "https://openalex.org/W1492191610",
    "https://openalex.org/W1938755728",
    "https://openalex.org/W2890487780",
    "https://openalex.org/W4299838440",
    "https://openalex.org/W2792376130",
    "https://openalex.org/W2175585630",
    "https://openalex.org/W1754156525",
    "https://openalex.org/W2140679639",
    "https://openalex.org/W1530235965",
    "https://openalex.org/W2117890631",
    "https://openalex.org/W2185726469",
    "https://openalex.org/W2963494889",
    "https://openalex.org/W2757047188",
    "https://openalex.org/W2962819663",
    "https://openalex.org/W2026928451",
    "https://openalex.org/W2963353830",
    "https://openalex.org/W2050728566",
    "https://openalex.org/W2062460000",
    "https://openalex.org/W2571859396",
    "https://openalex.org/W2096175520",
    "https://openalex.org/W2111305191",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W2519314406",
    "https://openalex.org/W2259472270",
    "https://openalex.org/W2951559648",
    "https://openalex.org/W2186272276",
    "https://openalex.org/W4298422451",
    "https://openalex.org/W2951672049",
    "https://openalex.org/W2962839844",
    "https://openalex.org/W2033468335",
    "https://openalex.org/W2963034893",
    "https://openalex.org/W2122882636",
    "https://openalex.org/W1968625547",
    "https://openalex.org/W2962883166",
    "https://openalex.org/W1558797106",
    "https://openalex.org/W2158084839",
    "https://openalex.org/W1979804848",
    "https://openalex.org/W2890560993",
    "https://openalex.org/W2152790380",
    "https://openalex.org/W2074959193",
    "https://openalex.org/W2152808281"
  ],
  "abstract": "International audience",
  "full_text": "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing\nand the 9th International Joint Conference on Natural Language Processing, pages 4104–4114,\nHong Kong, China, November 3–7, 2019.c⃝2019 Association for Computational Linguistics\n4104\nExperimenting with Power Divergences for Language Modeling\nMatthieu Labeau Shay B. Cohen\nInstitute for Language, Cognition and Computation\nSchool of Informatics, University of Edinburgh\n{mlabeau,scohen}@inf.ed.ac.uk\nAbstract\nNeural language models are usually trained us-\ning Maximum-Likelihood Estimation (MLE).\nThe corresponding objective function for MLE\nis derived from the Kullback-Leibler (KL) di-\nvergence between the empirical probability\ndistribution representing the data and the para-\nmetric probability distribution output by the\nmodel. However, the word frequency discrep-\nancies in natural language make performance\nextremely uneven: while the perplexity is usu-\nally very low for frequent words, it is espe-\ncially difﬁcult to predict rare words.\nTo address that, we experiment with several\nfamilies ( α, β and γ) of power divergences,\ngeneralized from the KL divergence, for learn-\ning language models with an objective differ-\nent than standard MLE. Intuitively, these di-\nvergences should affect the way the proba-\nbility mass is spread during learning, notably\nby prioritizing performance on high or low-\nfrequency words. In addition, we implement\nand experiment with various sampling-based\nobjectives, where the computation of the out-\nput layer is only done on a small subset of the\nvocabulary.\nThey are derived as power generalizations of\na softmax approximated via Importance Sam-\npling, and Noise Contrastive Estimation, for\naccelerated learning. Our experiments on the\nPenn Treebank and Wikitext-2 show that these\npower divergences can indeed be used to pri-\noritize learning on the frequent or rare words,\nand lead to general performance improve-\nments in the case of sampling-based learning.\n1 Introduction\nLanguage models are an important component\nin many NLP tasks, where they provide prior\nknowledge on the language used. They are con-\nditional models that aim to predict the next to-\nken in a sequence: they can be applied to ba-\nsic units ranging from individual characters to full\nwords, each approach coming with its own bene-\nﬁts and limitations (Merity et al., 2018a). Word-\nlevel language models have traditionally been\nbased on n-gram counts, obtaining good perfor-\nmance with smoothing techniques (Kneser and\nNey, 1995; Goodman, 2001). Recently, neural\nnetworks have shown strong results in language\nmodeling (Bengio et al., 2001), especially recur-\nrent neural networks (Mikolov et al., 2011b). As\nprevious approaches, like maximum entropy mod-\nels (Berger et al., 1996), neural language models\nare trained via Maximum Likelihood Estimation\n(MLE). Thus, their training cost grows linearly\nwith the number of words in the vocabulary, of-\nten making it prohibitively slow. This motivated a\nlarge amount of research work, bringing a variety\nof solutions (Chen et al., 2016).\nThe large vocabulary sizes encountered in train-\ning corpora arguably stem from the fact that\nthe frequency distribution of words in a corpus\nof natural language follows Zipf’s law (Pow-\ners, 1998). This also implies that the discrep-\nancy between counts of high-frequency and low-\nfrequency words increases with the size of the cor-\npus, as well as the number of those low-frequency\nwords. As a consequence, distributed word repre-\nsentations of low-frequency words are difﬁcult to\nlearn. Numerous approaches use decomposition\nof words with various sub-word units (Sennrich\net al., 2016; Kim et al., 2016), but the same phe-\nnomenon exists for low-frequency subwords. In\norder to deal with this issue and to accelerate train-\ning, Grave et al. (2017a) implement a dependency\nbetween embedding sizes and word frequencies in\nthe output layer, while Baevski and Auli (2019)\napply it to the input layer, comparing the possible\nchoices of which units to model. Using a different\napproach, Gong et al. (2018) attempt to learn word\nrepresentations that are less affected by these large\ndiscrepancies in word frequencies, with an adver-\n4105\nsarial training method to force the model to make\nfrequent and rare word embeddings hard to differ-\nentiate based on word frequency alone.\nThese improvements have been obtained by ex-\nplicitly incorporating in the model different ways\nof treating words according to their frequency.\nHowever, learning is always made via (or approx-\nimating) Maximum Likelihood Estimation, which\nﬁnds the distribution that maximizes entropy sub-\nject to the constraints given by training examples.\nIn this work, we explore the possibility of affect-\ning how words are learned depending on their fre-\nquency by using alternative loss functions. We\nspeciﬁcally explore power divergences, obtained\nthrough various generalizations of the Kullback-\nLeibler divergence, which is traditionally used to\nobtain the MLE objective function. This is moti-\nvated by the intuition that a well-suited power fac-\ntor may direct learning towards prioritizing high\nor low-frequency words, instead of learning uni-\nformally.\nIn this paper, we derive and experiment with\nthe objective functions obtained from three power-\ndivergences: the α, β and γ divergences. We\nalso derive objective functions for the correspond-\ning generalizations of two sampling-based meth-\nods: an Approximated Softmax obtained with im-\nportance sampling, and Noise Contrastive Estima-\ntion. We conduct a set of experiments comparing\nthese objectives and their effect of various parts of\nthe word frequency distribution, by training and\nevaluating models on two corpora: the Penn Tree-\nbank and Wikitext-2. Our experiments show that\ndepending on the vocabulary used and the choice\nof power divergence, it is indeed possible to gear\nlearning to focus on the most frequent or infre-\nquent words. We also observe that, while the\nMLE gives the best overall performance for exact\nobjectives, derived from the KL-divergence, our\ngeneralized objectives yield perplexity improve-\nments compared to baselines for both sampling-\nbased methods, up to 1 point in perplexity on both\ncorpora.\n2 Background\nLanguage modeling aims to learn a probability\ndistribution over a sequence of tokens from a ﬁ-\nnite target vocabulary Y. Such a distribution is\ndecomposed into a product of conditional distribu-\ntions of tokens overYgiven the previous tokens in\nthe sequence. Hence, we learn a parametric model\nof the form pθ(y|x), where x ∈X represents the\nsequence of previous tokens, yis a target label be-\nlonging to Y, and θis the set of model parameters.\nThey are obtained via maximum likelihood esti-\nmation (MLE), which consists in minimizing the\nnegative likelihood objective function:\nNLL(θ) =−\n∑\n(x,y)∈D\nlog pθ(y|x)\nover examples (x,y) corresponding to sequences\nof tokens drawn from the data D. This can be\nseen as minimizing the Kullback-Leibler diver-\ngence between the parametrized probability distri-\nbution pθ that we are learning and the distribution\npDdescribed by our training data:\nDKL(pD||pθ) =\n∑\n(x,y)∈X×Y\npD(y|x) logpD(y|x)\npθ(y|x) ,\nsince pD(y|x) = 1 if the sequence (x,y) ap-\npears in the training data, and equals 0 other-\nwise. Hence, the set of parameters θ∗minimizing\nDKL(pD||pθ) is the maximum likelihood distribu-\ntion on the training data D.\nIn this work, we will use several classes of di-\nvergences. A measure of discrepancy D between\ntwo probability densities pand qis a divergence if\nD(p||q) ≥0 with equality if and only ifp= q.1 In\nthe following, we will derive an objective function\nfrom a divergence Dwith the data distribution as\nﬁrst argument, and the second being the distribu-\ntion of the parametric model:\nObj(θ) =D(pD||pθ).\n3 Power Divergences\nA large number of divergence measures has been\nintroduced for a variety of applications (Bas-\nseville, 2013). Several families of divergences are\nnotably obtained from generalizing the Kullback-\nLeibler divergence by using a generalized loga-\nrithm function, which is a power function:\nlogα(x) = 1\n1 −α(x1−α −1), (1)\ndeﬁned by a parameterα, and that converges to the\nlogarithm as α →1. In this section, we will con-\nsider three families of divergences that can be gen-\nerated from this function; see Cichocki and Amari\n1Divergences are not necessarily metrics: they are de-\nﬁned only by non-negativity and positive deﬁniteness.\n4106\n(2010) for a review. For now, they are to be applied\nonly to normalized probability densities, implying\nthat ∑\ny∈Y\npθ(y|x) = 1.\n3.1 α-divergences\nThe notion of α-divergence was introduced by\nCsiszar (1967). The full expression of Dα(p||q)\nis shown in Table 1. It is a special case of f-\ndivergence (Ali and Silvey, 1966), derived from a\nstandardized version of the generalized logarithm2\n(Eq. 1). Applying α-divergences to parameter es-\ntimation generalizes MLE, and can be shown to\nhave similar properties, as consistency and asymp-\ntotic normality of the estimation error (Keziou,\n2003). Intuitively, the choice of αwill impact the\nimportance of the likelihood ratio p/q: while the\nlimiting cases α→1 and α→0 are the Kullback-\nLeibler DKL(p||q) and the reverse Kullback-\nLeibler divergencesDKL(q||p), having α≥1 will\nmake learning zero-avoiding 3 for q, and α ≤ 0\nzero-forcing4 (Minka, 2005). We should also note\nthat Dα(p||q) =D1−α(q||p). When working with\nnormalized densities, the α-divergence is linked\nto the R´enyi divergence (R´enyi, 1961), which has\nbeen used to measure domain similarity (Van Asch\nand Daelemans, 2010). Given that we are trying to\nlearn from conditional distributions which are zero\neverywhere except for one target token, we are in-\nterested in experimenting with values of α >1,\nwhich should push the model towards generaliz-\ning, while having α ∈ (0.5,1) should force the\nmodel to concentrate probability mass on training\nexamples. Since we are here working with nor-\nmalized distribution, we obtain the following ob-\njective:\nObjα(θ) = 1\nα(α−1)\n∑\n(x,y)∈D\n(pθ(y|x))1−α.\n3.2 β-divergences\nThe β-divergence, also called density power di-\nvergence was introduced by Basu et al. (1998)\nas a robust estimation method, which showed it\nto be consistent for parameter estimation, with\n2αand β-divergences are related to the Tsallis entropy,\nwhich can be seen as a deformation of the Shannon entropy\nusing this same generalized logarithm: see Appendix A from\nCichocki and Amari (2010) . Recently, Blondel et al. (2018)\nshowed how to build Fenchel-Young Losses from the same\nTsallis entropies.\n3Having pi >0 will enforce qi >0.\n4Having pi = 0will enforce qi = 0.\nasymptotic normality of the estimation error (Basu\net al., 1998, Theorem 2). The full expression of\nDβ(p||q) is shown in Table 1.5 It can be seen as a\nBregmandivergence (Bregman, 1967) also derived\nusing the generalized logarithm2 (Eq. 1). The mo-\ntivation is to obtain divergences that are robust to\noutliers, which is the case for values of β > 1;\nchoosing β = 2gives us the L2-loss, while β →1\ngives the Kullback-Leibler divergence DKL(p||q)\nas a limiting case. Hence, choosing a value close\nto 1 while larger is supposed to provide a com-\npromise between the efﬁciency of the Kullback-\nLeibler divergence and the robustness of the L2-\nloss. Similarly, we can expect to give more im-\nportance to outliers (which we suppose to be the\nlow-frequency tokens) by choosing β < 1. The\nobjective becomes:\nObjβ(θ) = 1\nβ(β−1)×\n∑\n(x,y)∈D\n\n(β−1)\n∑\ny′∈Y\n(pθ(y′|x))β−β(pθ(y|x))β−1\n\n.\n3.3 γ-divergences\nEguchi and Kato (2010) introduced the γ-\ndivergence as a modiﬁcation of the β-divergence,\nwith the speciﬁc goal of obtaining a scale-\ninvariant version of the robustβ-divergence,6 also\nshowing it to be consistent for parameter esti-\nmation, with asymptotic normality of the estima-\ntion error (Eguchi and Kato, 2010, Section 3).\nThis divergence has notably been used for the es-\ntimation of parameters without normalizing the\noutput probability distribution (Takenouchi and\nKanamori, 2015).7 The general expression of the\nγ-divergence is shown in Table 1. While the use of\nthe log non-linearity makes this divergence non-\nseparable, which at ﬁrst glance could be thought\nto complicate computation in practice, our moti-\nvation for using it is its scale-invariance, which we\nwill discuss in Section 4.3. Applied to our prob-\n5For readability, we keep the notation and values used in\nCichocki and Amari (2010) instead of Basu et al. (1998).\n6‘Scale-invariance’ here means that the divergence\nshould remain unchanged when one or both of the measures\nit is applied to are multiplied by scalars.\n7However, this particular method is not suitable in our\ncase because of the sparsity of our data.\n4107\nlem, the objective becomes:\nObjγ(θ) = ∑\n(x,y)∈D\n\nlogpθ(y|x)−1\nγlog∑\ny′∈Y\n(pθ(y′|x))γ\n\n.\n(2)\n4 Accelerating Learning\nIn order to use the previously deﬁned objec-\ntives, we need to compute the model probabilities\npθ(y|x), which are usually obtained using a soft-\nmax function:\npθ(y|x) = exp (sθ(x,y))∑\ny′∈Yexp (sθ(x,y′))\napplied on scores (or logits) sθ(x,y′) output by\nthe model for every possible target token y′ ∈\nY. However, as explained earlier, Ycan be very\nlarge, hence computing all the scores and sum-\nming them is extremely slow. We choose to fol-\nlow the strategies employed by Jozefowicz et al.\n(2016): approximating the softmax using impor-\ntance sampling (Bengio and S´en´ecal, 2003, 2008),\nand Noise Contrastive Estimation (NCE; Gutmann\nand Hyv ¨arinen 2010; Mnih and Teh 2012). Be-\nsides, the divergences presented in Section 3 can\nbe applied to positive measures. Hence, a possible\nthird direction is to instead approximate the ob-\njectives to the un-normalized model distributions.\nAll the objectives presented in this section are ex-\nplained in Appendix A.4.\n4.1 Approximated softmax\nPlugging the ﬁrst solution into our objectives is\nstraightforward: using self-importance sampling\nto approximate directly the multinomial classiﬁ-\ncation probability, we compute pθ via an approxi-\nmated softmax:8\npθ(y|x) ≈\nexp (sθ(x,y))\npn(y)\nexp (sθ(x,y))\npn(y) +\nk∑\ni=1\nˆyi∼pn\nexp (sθ(x,ˆyi))\npn(ˆyi)\nwhere pn is an auxiliary distribution chosen to re-\nﬂect the training data while still being easy to sam-\nple from, and k ≪|Y| is the number of samples\ndrawn.\n8The same objective is called ‘Ranking NCE’ by Ma and\nCollins (2018).\n4.2 Adapting Noise-Contrastive Estimation\nNoise-Contrastive Estimation consists of training\nthe model for a surrogate binary classiﬁcation task\nwhere, given examples from the mixture:\n1\nk+ 1pD+ k\nk+ 1pn\nwe learn to discriminate between examples com-\ning from data and samples coming from the auxil-\niary noise distribution pn. Minimizing the NCE\nloss function has been shown to be equivalent\nto minimizing a Bregman divergence (Gutmann\nand Hirayama, 2011); however, the transformation\nthat we need to apply to the associated generating\nfunction (Pihlaja et al., 2012) in order to obtain a\npower divergence is not straigthforward. Instead,\nwith the posterior classiﬁcation probability:\npθ(C = True|y,x) = pθ(y|x)\npθ(y|x) +kpn(y) (3)\nNoting pC\nθ the positive measure on X×Y :\npC\nθ : (x,y) →pθ(C = True|y,x)\nwe can show that the NCE objective function can\nbe derived from the following expression:9\nDKL(pC\nD||pC\nθ ) +DKL(1 −pC\nD||1 −pC\nθ )\nKnowing this, we simply need to replace DKL by\nthe other divergences to derive the α,β and γ ob-\njective functions associated to this surrogate clas-\nsiﬁcation task. It is interesting to note that the\npower transformations will here be applied on the\nposterior classiﬁcation probabilities pC\nθ instead of\ncategorical probabilities pθ.\n4.3 Working With Positive Measures\nThe three divergences presentend in Section 3 are\ndeﬁned on positive measures: in theory, we can\nsimply use the exp function on the scores sθ and\ndo not need to normalize them:\nObj(θ) =D(pD||exp (sθ))\nHowever, neither theαand βdivergences are scale\ninvariant (see right column of Table 1 and Ci-\nchocki and Amari 2010). We can show that work-\ning with an un-normalized model distribution will,\nin both those cases, give an objective proportional\nto the scale of the model, allowing the divergence\n9The full derivation is given in Appendix A.2.\n4108\nExpression Scaling properties\nα∈R\\{0,1} 1\nα(α−1)\nn∑\ni=1\n[pα\niq1−α\ni −αpi+ (α−1)qi\n] Dα(c×p||c×q) =c×Dα(p||q)\nβ∈R\\{0,1} 1\nβ(β−1)\nn∑\ni=1\n[\npβ\ni + (β−1)qβ\ni −βpiqβ−1\ni\n]\nDα(c×p||c×q) =cβ×Dα(p||q)\nγ∈R\\{0,1} 1\nγ(γ−1) log\nn∑\ni=1\npγ\ni + 1\nγ log\nn∑\ni=1\nqγ\ni − 1\nγ−1 log\nn∑\ni=1\npiqγ−1\ni Dγ(c1 ×p||c2 ×q) =Dγ(p||q)\nTable 1: Complete expressions of α, β, and γ divergences between two positive measures p = (pi)n\ni=1 and\nq= (qi)n\ni=1 on Rn\n+, as well as their scaling properties. Note that the α-divergence simpliﬁes if we restrict them to\nthe set of normalized probability densities (if ∑n\ni=1 pi = ∑n\ni=1 qi = 1).\n9.0\n9.5\n10.0\n10.5\n11.0\n11.5Low frequency bin\n=0.9\nKL\n=1.1\n9.0\n9.5\n10.0\n10.5\n11.0\n11.5\n =0.9\nKL\n=1.1\n9.0\n9.5\n10.0\n10.5\n11.0\n11.5\n =0.9\nKL\n=1.1\n0 20 40 60 80 100\n4.4\n4.6\n4.8\n5.0\n5.2Complete vocabulary\n0 20 40 60 80 100\n4.4\n4.6\n4.8\n5.0\n5.2\n0 20 40 60 80 100\n4.4\n4.6\n4.8\n5.0\n5.2\nValidation Cross-entropy\nTraining epochs\nFigure 1: Validation cross-entropy values obtained during the beginning of training models withObjα (left), Objβ\n(center) and Objγ (right) on the PTB with a full vocabulary. Words are grouped into 5 buckets of equal size,\nfollowing their frequencies. On the top is shown the cross-entropy obtained on the bucket of lowest frequency\nwords, while the global cross-entropy is displayed on the bottom.\nto be minimized by minimizing the scale alone,\nwith no actual learning. 10 This is not an issue\nfor the γ-divergence, since Dγ(pD||exp (sθ)) =\nDγ(pD||pθ). Hence, the objective of Eq. 2 may be\nsimpliﬁed as following:\nObjγ(θ) = ∑\n(x,y)∈D\n\nsθ(x,y)−1\nγlog∑\ny′∈Y\nexp (γsθ(x,y′))\n\n.\nWe can further accelerate learning by using impor-\ntance sampling on the second term, and because\nof the logarithm applied to the sum, we obtain an\nobjective that is equivalent to composing the ap-\nproximated softmax with γ-divergence, as can be\nseen in Appendix A.4.\n10The proof is given in Appendix A.1.\n5 Experiments\nOur goal is to compare the effect of the various ob-\njective functions we proposed in Sections 3 and 4,\nand especially study how the values of α, β and\nγaffect learning and performance, overall as well\nas on low-frequency words. Since each model\nis trained with a different objective function, the\ntraining scores are not comparable. Hence, we use\nthe validation cross-entropy and perplexity at each\nepoch as a way to track progress during training.\n5.1 Datasets\nWe perform our experiments on two widely used,\nreasonably sized datasets: the Penn Treebank\n(PTB; Mikolov et al. 2011a) and WikiText-2\n(WT2; Merity et al. 2017). The PTB, heavily pre-\nprocessed, retains the 10k most frequent words\nin its vocabulary, while the others tokens are re-\nplaced by a common <unk> token. The WT2,\n4109\nabout two times larger, retains words that appear\nat least three times in the training data in its vo-\ncabulary, which makes it 33,278 words. To study\nthe impact of our various objective functions on\na vocabulary containing very rare words, we also\nexperiment with a version on the PTB to which\nwe only apply limited preprocessing, allowing us\nto keep its full training vocabulary, which contains\n39030 words.\n5.2 Experimental setup\nWe based our setup on the ASGD weight-dropped\nLSTM (AWD-LSTM) models of Merity et al.\n(2018b), since to the best of our knowledge they\ngive state-of-the-art results on the PTB and WT2\nfor models that are build with a softmax output\nlayer and do not use any adaptative method (as\nthe pointer sentinel LSTM (Merity et al., 2017),\nthe neural cache (Grave et al., 2017b) and the dy-\nnamic evaluation (Krause et al., 2018)). Our mod-\nels11, replications of their 3-layer LSTMs with tied\ninput and output representations, were built using\ntheir implementation.12 For each dataset, we fol-\nlow their choice of hyperparameters, only modify-\ning the objective functions.\nFor our sampling-based objectives, we use k =\n1024 samples, and the unigram distribution as\npn. In the case of objectives derived from bi-\nnary NCE, to avoid issues with the phenomenon\nof self-normalization and consistency issues (Ma\nand Collins, 2018) we chose to use blackout (Ji\net al., 2015). Indeed, this method amounts to using\na noise distribution which depends on the model\nprobabilities, making the normalization term dis-\nappear from the posterior classiﬁcation probabili-\nties of Eq. 3. Hence, we have an objective func-\ntion that is fast to compute without any supple-\nmentary assumption, and the negative samples are\nstill drawn from the unigram distribution. For both\nAS and NCE, in our setting, the computation time\nis reduced to about 40% of the time taken by MLE\non the WT2, and 50% on the PTB.\n6 Results and discussion\nWe turn to describe the results of our experiments.\n11https://github.com/MatthieuLabeau/\npower-divergences-LM\n12https://github.com/salesforce/\nawd-lstm-lm.\n6.1 Qualitative results on the full vocabulary\nPTB\nTo observe the behavior of the proposed objectives\nfor various choices of α, βand γ, we plot the val-\nidation cross-entropy at the beginning of learning\nof models on PTB equipped with a full training\nvocabulary. We choose values of 0.9 and 1.1 for\nthe power parameters, to experiment with an ob-\njective on each side of the baseline MLE objec-\ntive. We split the words according to frequency\ninto 5 buckets, in order for the buckets to represent\nhave equal size based on word counts, and dis-\nplay both the global cross-entropy and the cross-\nentropy on the lowest frequency bin in Figure 1.\nA value of α >1 seems to initially behave bet-\nter, especially for rare words. This could be ex-\npected: intuitively, these values of αshould make\nthe model ‘stretch’ the probability mass. However,\nas learning progresses, this phenomenon lessens,\nand the performance on rare words gets worse. A\nvalue of β <1, supposed to make the model less\nrobust to outliers, seems to make learning faster\ninitially, particularly on rare words, but again im-\nprovements lessen. Choosing α < 1 or β > 1\ngives a worse cross-entropy overall. This ‘in-\nverted’ similarity between the behaviors induced\nby choices of αand β, and the links between the\ntwo divergences, have been explored by Patra et al.\n(2013). Finally, choosing γ >1 gives very inter-\nesting results, allowing for a better cross-entropy\non rare words while retaining the same overall per-\nformance than MLE.\n6.2 Penn Treebank and WikiText-2\nIn this section, we present the results of ex-\nploratory experiments. We fully train models with\nthe proposed objectives, for a variety of power pa-\nrameters. For the PTB, the ﬁnal validation per-\nplexities are presented in Table 2, while we present\nthe ﬁnal validation cross-entropies for the objec-\ntives derived from MLE, on 5 frequency buckets,\nin Figure 2. For the WT2, they are shown in Ta-\nble 3 and Figure 3.\nWith exact objectives: for both corpora, the re-\nsults for the high and low-frequency buckets seem\nto conﬁrm that values of α,γ that are smaller and\nlarger than 1 can help prioritizing learning towards\nthe frequent and rare words. The effect of β de-\npending on frequency seems lighter, especially on\nthe PTB: we hypothesize that this is due to the vo-\ncabulary being cut short, and containing no very\n4110\n2\n4\n6\n8\n10\n8.21\n7.83\n7.81\n7.80\n7.74\n=0.9\n=0.99\nKL\n=1.01\n=1.1\n2\n4\n6\n8\n10\n8.06\n7.84\n7.81\n7.82\n7.87\n=0.9\n=0.99\nKL\n=1.01\n=1.1\n2\n4\n6\n8\n10\n8.18\n7.83\n7.81\n7.80\n7.59\n=0.9\n=0.99\nKL\n=1.01\n=1.1\nValidation Cross-entropy\nFrequency bins\nFigure 2: Validation cross-entropy values for the best epoch obtained\nfor models trained withObjα (top), Objβ (middle) andObjγ (bot-\ntom) on the PTB. Words are grouped into5 buckets of equal size,\nfollowing their frequencies. We display values for each bucket from\nthe most frequent words (left) to less frequent ones (right).\nObjective Objα Objβ Objγ\nMLE\n0.9 65.8 63.3 62.3\n0.99 60.7 61.0 60.6\n1.0 60.1\n1.01 60.8 61.1 61.0\n1.1 63.0 63.9 63.3\nAS\n0.5 147.0 87.1 113.8\n0.9 61.3 62.8 61.0\n0.99 61.7 61.7 61.6\n1.0 61.7\n1.01 61.8 61.6 62.0\n1.1 65.6 61.6 64.1\n1.5 97.7 127.6 81.7\nNCE\n0.5 1407.5 88.0 117.2\n0.9 61.4 62.6 61.8\n0.99 61.1 61.4 61.7\n1.0 61.2\n1.01 61.3 61.1 61.8\n1.1 64.4 61.0 64.4\n1.5 97.9 123.7 81.2\nTable 2: Best validation perplexities\nobtained on the PTB withObjα, Objβ,\nand Objγ, derived from MLE, and ap-\nproximated objectives AS and NCE,\non single models with the same intial-\nization.\nObjective Objα Objβ Objγ\nDataset PTB WT2 PTB WT2 PTB WT2\n0.9 65.8 75.8 63.3 68.3 63.2 70.1\n39.7 42.4 107.9 130.0 62.4 69.0\n0.99 60.7 66.5 61.0 65.6 60.6 65.9\n57.9 63.0 64.0 69.3 60.6 65.9\n1.0 60.1 64.9 60.1 64.9 60.1 64.9\n1.01 60.8 65.6 61.1 65.4 61.0 66.0\n63.8 69.3 58.3 61.8 61.0 66.0\n1.1 63.0 66.7 63.9 68.3 63.3 68.1\n99.8 114.3 40.7 45.2 62.5 67.0\nTable 4: Final validation results obtained on the PTB\nand WT2, with the exact objectives Objα, Objβ, and\nObjγ, derived from MLE. In each cell we give on top\nthe validation perplexity, and below, the ‘counterpart’\nto perplexity corresponding to the training objective —\nwhich is the value being optimized. Each color corre-\nsponds to a different objective: values cannot be com-\npared for varying α, βand γ.\nrare words, which reduces the discrepancies be-\ntween higher and lower scoring examples. How-\never, no objective seems to be able to improve on\nMLE for overall performance.\nTo explain these results, we may argue that per-\nplexity is a biased measure as MLE directly opti-\nmizes it. Hence, we compute the ‘counterparts’ to\nperplexity corresponding to each objective. Equiv-\nalently to perplexity for MLE, they are directly op-\ntimized by their respective objectives, and should\ndecrease to 1 as the model distribution gets closer\nto the data distribution. Again, since they vary for\neach objective, they are not comparable between\nthemselves and with perplexity. We display these\nvalues for our models trained with exact objectives\nin Table 4. Asα, βand γare closer to 1, these val-\nues get closer to the perplexity of the model. Be-\nsides, they are especially close across all values of\nγ: we can assume that this indicates that the corre-\nsponding Objγ are ‘closer’ to the MLE objective.\nHowever, tracking these values during training 13\nshows that they all behave very similarly to per-\nplexity.\nWith approximated objectives: for objectives\nderived from AS and NCE, we observe far less\nimpact of the choice of the power parameter on\nfrequent or rare words, 14 which is probably due\nto the fact that only a small subset of the vocab-\n13See Appendix A.3.\n14See ﬁgures in Appendix A.5.\n4111\n2\n4\n6\n8\n10 9.018.46\n8.40\n8.43\n8.41\n=0.9\n=0.99\nKL\n=1.01\n=1.1\n2\n4\n6\n8\n10 8.80\n8.45\n8.40\n8.39\n8.73=0.9\n=0.99\nKL\n=1.01\n=1.1\n2\n4\n6\n8\n10 8.86\n8.47\n8.40\n8.42\n8.19\n=0.9\n=0.99\nKL\n=1.01\n=1.1\nValidation Cross-entropy\nFrequency bins\nFigure 3: Validation cross-entropy values for the best epoch obtained\nfor models trained withObjα (top), Objβ (middle) andObjγ (bot-\ntom) on the WT2. Words are grouped into5 buckets of equal size,\nfollowing their frequencies. We display values for each bucket from\nthe most frequent words (left) to less frequent ones (right).\nObjective Objα Objβ Objγ\nMLE\n0.9 75.8 68.3 70.1\n0.99 66.5 65.6 65.9\n1.0 64.9\n1.01 65.6 65.4 66.0\n1.1 66.7 68.3 68.1\nAS\n0.5 203.6 82.1 191.6\n0.9 69.9 65.6 68.0\n0.99 67.4 65.0 65.1\n1.0 65.3\n1.01 67.5 65.1 65.0\n1.1 69.8 67.8 64.8\n1.5 100.7 211.1 78.1\nNCE\n0.5 2401.2 84.1 193.1\n0.9 70.6 66.6 69.7\n0.99 66.8 65.7 67.5\n1.0 65.8\n1.01 65.8 65.9 65.8\n1.1 64.7 68.3 65.4\n1.5 83.8 196.4 77.3\nTable 3: Best validation perplexi-\nties obtained on the WT2 withObjα,\nObjβ, and Objγ, derived from MLE,\nand approximated objectives AS and\nNCE, on single models with the same\ninitialization.\n0.880.920.961.00\n60\n62PTB\n  decreasing\n1.00 1.04 1.08 1.12\n60\n62\n increasing\n1.00 1.04 1.08 1.12\nAS\n63\n65WT2\n increasing\n1.00 1.04 1.08 1.12\nNCE\n63\n65\n increasing\nValidation\nTest\nPerplexity\nPower Parameter\nFigure 4: Validation and test perplexities obtained for particular values ofα, βor γwith sampling-based objectives\nin 4 possible pairings of AS and NCE-based objectives trained on the PTB and WT2, on single models with the\nsame initialization.\nulary is updated during learning — and this sub-\nset is drawn according to word frequency. How-\never, some objectives provide modest improve-\nments over the corresponding KL-based baseline.\n6.3 Searching for the Optimal Power\nParameter\nIn order to verify the potential beneﬁts of our gen-\neralization of sampling-based objectives, we use\nthese preliminary results to search for the ‘best’\npower parameter, and check that improvements\nare consistent for several versions of the model,\n4112\nObjective Validation Test\nPTB\nMLE 60.7 58.6\nAS KL 62.2 59.8\nα= 0.95 61.2 58.9\nNCE KL 61.5 59.2\nβ= 1.1 61.2 59.0\nWT2\nMLE 65.5 62.8\nAS KL 65.1 62.6\nγ= 1.075 64.8 62.1\nNCE KL 65.8 63.4\nα= 1.1 64.7 62.0\nTable 5: Best validation and test perplexities obtained\non the best performing conﬁgurations of power param-\neter for both corpora and each category of objectives,\naveraged over 5 models with different initializations.\ninitialized with different seeds. This search is\nshown in Figure 4 for 4 different conﬁgurations\nchosen using the preliminary results presented ear-\nlier. In each case, we can see that the perplexity\nseems to decrease, reach a minimum, and then in-\ncrease monotonously. While verifying this would\nrequire a deeper investigation, we can make the\nhypothesis that the sampling procedure induces\nnoise that some of our proposed objectives can\nbe better suited for. In Table 5, we present per-\nplexity results averaged for different seeds, in or-\nder to check that model initialization does not dis-\ncernibly affect these improvements. They show\ngains of up to1 point in perplexity, and a slight im-\nprovement over the MLE baseline for WT2, con-\nﬁrming the results obtained on a single model.\nHowever, it is tedious to ﬁnd the optimal power\nparameter for a given conﬁguration, and we can\nexpect it to shift sensibly when hyper-parameters\nare modiﬁed. The size of the vocabulary and the\ndistribution of word frequencies should intuitively\nhave the biggest inﬂuence, as we can see that the\nbehavior of α, β and γ is quite different on the\nPTB and WT2. In general, ﬁnding the optimal di-\nvergence with the optimal parameter for a given\nproblem is very difﬁcult. Methods for doing so\nhave only been recently explored; for example,\nDikmen et al. (2015) use the Tweedie Likelihood\nto ﬁnd the optimalβ, which can lead to the optimal\nαand γ.\n7 Conclusion\nWe explore the use of generalizations of KL-\ndivergence into power ( α, β and γ) divergences\nfor training language models. We derive exact\nbut also approximated objectives, based on an ap-\nproximated softmax using importance sampling,\nand noise contrastive estimation. We show that\nin the case of exact objectives, a well-chosen γ-\ndivergence can be used to prioritize learning low\nor high-frequency words. In the case of approxi-\nmated objective, we show that our proposed objec-\ntives can improve on perplexity, and demonstrate\nit is the case for several conﬁgurations. Further\nresearch should investigate the potential gains of\nusing γ-divergences for appropriate downstream\ntasks.\nAcknowledgments\nWe thank the anonymous reviewers for helpful\nfeedback. We gratefully acknowledge the support\nof Huawei Technologies.\nReferences\nS. M. Ali and S. D. Silvey. 1966. A general class of\ncoefﬁcients of divergence of one distribution from\nanother. Journal of the Royal Statistical Society. Se-\nries B (Methodological), 28(1):131–142.\nAlexei Baevski and Michael Auli. 2019. Adaptive in-\nput representations for neural language modeling. In\nInternational Conference on Learning Representa-\ntions.\nMich`eLe Basseville. 2013. Review: Divergence mea-\nsures for statistical data processing-an annotated\nbibliography. Signal Process., 93(4):621–633.\nAyanendranath Basu, Ian R. Harris, Nils L. Hjort,\nand M. C. Jones. 1998. Robust and efﬁcient esti-\nmation by minimising a density power divergence.\nBiometrika, 85(3):549–559.\nYoshua Bengio, R´ejean Ducharme, and Pascal Vincent.\n2001. A neural probabilistic language model. In\nT. K. Leen, T. G. Dietterich, and V . Tresp, editors,\nAdvances in Neural Information Processing Systems\n13, pages 932–938. MIT Press.\nYoshua Bengio and Jean-S ´ebastien S ´en´ecal. 2003.\nQuick training of probabilistic neural nets by impor-\ntance sampling. In Proceedings of the conference on\nArtiﬁcial Intelligence and Statistics (AISTATS).\nYoshua Bengio and Jean-S ´ebastien S ´en´ecal. 2008.\nAdaptive importance sampling to accelerate train-\ning of a neural probabilistic language model. IEEE\nTrans. Neural Networks, 19(4):713–722.\nAdam L. Berger, Vincent J. Della Pietra, and Stephen\nA. Della Pietra. 1996. A maximum entropy ap-\nproach to natural language processing. Comput.\nLinguist., 22(1):39–71.\n4113\nMathieu Blondel, Andr´e F. T. Martins, and Vlad Nicu-\nlae. 2018. Learning classiﬁers with fenchel-young\nlosses: Generalized entropies, margins, and algo-\nrithms. Cite arxiv:1805.09717.\nL.M. Bregman. 1967. The relaxation method of ﬁnding\nthe common point of convex sets and its application\nto the solution of problems in convex programming.\nUSSR Computational Mathematics and Mathemati-\ncal Physics, 7(3):200 – 217.\nWenlin Chen, David Grangier, and Michael Auli. 2016.\nStrategies for training large vocabulary neural lan-\nguage models. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1975–\n1985, Berlin, Germany. Association for Computa-\ntional Linguistics.\nAndrzej Cichocki and Shun Amari. 2010. Families\nof alpha- beta- and gamma- divergences: Flexi-\nble and robust measures of similarities. Entropy,\n12(6):1532–1568.\nI. Csiszar. 1967. Information-type measures of differ-\nence of probability distributions and indirect obser-\nvation. Studia Scientiarum Mathematicarum Hun-\ngarica, 2:229–318.\nOnur Dikmen, Zhirong Yang, and Erkki Oja. 2015.\nLearning the information divergence. IEEE Trans.\nPattern Anal. Mach. Intell., 37(7):1442–1454.\nShinto Eguchi and Shogo Kato. 2010. Entropy and di-\nvergence associated with power function and the sta-\ntistical application. Entropy, 12(2):262–274.\nChengyue Gong, Di He, Xu Tan, Tao Qin, Liwei Wang,\nand Tie-Yan Liu. 2018. Frage: Frequency-agnostic\nword representation. In S. Bengio, H. Wallach,\nH. Larochelle, K. Grauman, N. Cesa-Bianchi, and\nR. Garnett, editors, Advances in Neural Information\nProcessing Systems 31 , pages 1334–1345. Curran\nAssociates, Inc.\nJoshua T. Goodman. 2001. A bit of progress in lan-\nguage modeling. Comput. Speech Lang., 15(4):403–\n434.\nEdouard Grave, Armand Joulin, Moustapha Ciss ´e,\nDavid Grangier, and Herv ´e J´egou. 2017a. Efﬁcient\nsoftmax approximation for gpus. In Proceedings\nof the 34th International Conference on Machine\nLearning, ICML 2017, Sydney, NSW, Australia, 6-11\nAugust 2017, pages 1302–1310.\nEdouard Grave, Armand Joulin, and Nicolas Usunier.\n2017b. Improving neural language models with a\ncontinuous cache. In 5th International Conference\non Learning Representations, ICLR 2017, Toulon,\nFrance, April 24-26, 2017, Conference Track Pro-\nceedings.\nMichael Gutmann and Junichiro Hirayama. 2011.\nBregman divergence as general framework to esti-\nmate unnormalized statistical models. In UAI.\nMichael Gutmann and Aapo Hyv ¨arinen. 2010. Noise-\ncontrastive estimation: A new estimation principle\nfor unnormalized statistical models. In Proceedings\nof the Thirteenth International Conference on Artiﬁ-\ncial Intelligence and Statistics, AISTATS 2010, Chia\nLaguna Resort, Sardinia, Italy, May 13-15, 2010 ,\npages 297–304.\nShihao Ji, S. V . N. Vishwanathan, Nadathur Satish,\nMichael J. Anderson, and Pradeep Dubey. 2015.\nBlackout: Speeding up recurrent neural network lan-\nguage models with very large vocabularies. CoRR,\nabs/1511.06909.\nRafal Jozefowicz, Oriol Vinyals, Mike Schuster, Noam\nShazeer, and Yonghui Wu. 2016. Exploring the lim-\nits of language modeling.\nAmor Keziou. 2003. Dual representation of phi-\ndivergences and applications. Comptes Rendus\nMathematique, 336(10):857 – 862.\nYoon Kim, Yacine Jernite, David Sontag, and Alexan-\nder M. Rush. 2016. Character-aware neural lan-\nguage models. In 30th AAAI Conference on Ar-\ntiﬁcial Intelligence, AAAI 2016 , pages 2741–2749.\nAAAI press.\nReinhard Kneser and Hermann Ney. 1995. Improved\nbacking-off for m-gram language modeling. In In\nProceedings of the IEEE International Conference\non Acoustics, Speech and Signal Processing , vol-\nume I, pages 181–184, Detroit, Michigan.\nBen Krause, Emmanuel Kahembwe, Iain Murray,\nand Steve Renals. 2018. Dynamic evaluation of\nneural sequence models. In Proceedings of the\n35th International Conference on Machine Learn-\ning, volume 80 of Proceedings of Machine Learning\nResearch, pages 2766–2775, Stockholmsm ¨assan,\nStockholm Sweden. PMLR.\nZhuang Ma and Michael Collins. 2018. Noise con-\ntrastive estimation and negative sampling for condi-\ntional models: Consistency and statistical efﬁciency.\nIn Proceedings of the 2018 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n3698–3707. Association for Computational Linguis-\ntics.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018a. An analysis of neural language mod-\neling at multiple scales. CoRR, abs/1803.08240.\nStephen Merity, Nitish Shirish Keskar, and Richard\nSocher. 2018b. Regularizing and optimizing LSTM\nlanguage models. In International Conference on\nLearning Representations.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture\nmodels. In 5th International Conference on Learn-\ning Representations, ICLR 2017, Toulon, France,\nApril 24-26, 2017, Conference Track Proceedings.\n4114\nTomas Mikolov, , Stefan Kombrink, Lukas Burget, and\nJan Honza Cernocky. 2011a. Empirical evaluation\nand combination of advanced language modeling\ntechniques. In Interspeech. ISCA.\nTomas Mikolov, Stefan Kombrink, Luk ´as Burget, Jan\nCernock´y, and Sanjeev Khudanpur. 2011b. Exten-\nsions of recurrent neural network language model.\nIn ICASSP, pages 5528–5531. IEEE.\nThomas Minka. 2005. Divergence measures and mes-\nsage passing. Technical report.\nAndriy Mnih and Yee Whye Teh. 2012. A fast and sim-\nple algorithm for training neural probabilistic lan-\nguage models. In Proceedings of the 29th Inter-\nnational Conference on Machine Learning, ICML\n2012, Edinburgh, Scotland, UK, June 26 - July 1,\n2012.\nSujayendu Patra, Avijit Maji, Ayanendranath Basu, and\nLeandro Pardo. 2013. The power divergence and the\ndensity power divergence families: The mathemati-\ncal connection. Sankhya B, 75.\nMiika Pihlaja, Michael Gutmann, and Aapo\nHyv¨arinen. 2012. A family of computationally\nefﬁcient and simple estimators for unnormalized\nstatistical models. CoRR, abs/1203.3506.\nDavid M. W. Powers. 1998. Applications and explana-\ntions of Zipf’s law. In New Methods in Language\nProcessing and Computational Natural Language\nLearning.\nAlfr´ed R´enyi. 1961. On measures of entropy and infor-\nmation. In Proceedings of the Fourth Berkeley Sym-\nposium on Mathematical Statistics and Probability,\nVolume 1: Contributions to the Theory of Statistics ,\npages 547–561, Berkeley, Calif. University of Cali-\nfornia Press.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers) , pages 1715–\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nTakashi Takenouchi and Takafumi Kanamori. 2015.\nEmpirical localization of homogeneous divergences\non discrete sample spaces. In C. Cortes, N. D.\nLawrence, D. D. Lee, M. Sugiyama, and R. Garnett,\neditors, Advances in Neural Information Processing\nSystems 28, pages 820–828. Curran Associates, Inc.\nVincent Van Asch and Walter Daelemans. 2010. Us-\ning domain similarity for performance estimation.\nIn Proceedings of the 2010 Workshop on Do-\nmain Adaptation for Natural Language Processing ,\nDANLP 2010, pages 31–36, Stroudsburg, PA, USA.\nAssociation for Computational Linguistics.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.6827985048294067
    },
    {
      "name": "Natural (archaeology)",
      "score": 0.5313622951507568
    },
    {
      "name": "Joint (building)",
      "score": 0.5219561457633972
    },
    {
      "name": "Natural language",
      "score": 0.48726025223731995
    },
    {
      "name": "Natural language processing",
      "score": 0.4532738924026489
    },
    {
      "name": "Power (physics)",
      "score": 0.4221112132072449
    },
    {
      "name": "Linguistics",
      "score": 0.3615903854370117
    },
    {
      "name": "History",
      "score": 0.21411940455436707
    },
    {
      "name": "Engineering",
      "score": 0.1635512113571167
    },
    {
      "name": "Philosophy",
      "score": 0.10826635360717773
    },
    {
      "name": "Archaeology",
      "score": 0.09887745976448059
    },
    {
      "name": "Architectural engineering",
      "score": 0.06890252232551575
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ]
}