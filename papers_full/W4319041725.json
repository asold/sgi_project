{
  "title": "Language models can identify enzymatic active sites in protein sequences",
  "url": "https://openalex.org/W4319041725",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A3210551175",
      "name": "Yves Gaetan Nana Teukam",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A3211791707",
      "name": "Loïc Kwate Dassi",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2573726791",
      "name": "Matteo Manica",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A1904819374",
      "name": "Daniel Probst",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2769065378",
      "name": "Philippe Schwaller",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2209038682",
      "name": "Teodoro Laino",
      "affiliations": [
        "IBM Research - Zurich"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6880033799",
    "https://openalex.org/W6600950960",
    "https://openalex.org/W6838461927",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3205068155",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3010145447",
    "https://openalex.org/W3157679602",
    "https://openalex.org/W3177828909",
    "https://openalex.org/W6600109629",
    "https://openalex.org/W2943495267",
    "https://openalex.org/W6763868836",
    "https://openalex.org/W3037620288",
    "https://openalex.org/W4239275769",
    "https://openalex.org/W3171387763",
    "https://openalex.org/W3186179742",
    "https://openalex.org/W4233253307",
    "https://openalex.org/W6784891704",
    "https://openalex.org/W2158714788",
    "https://openalex.org/W4213314063",
    "https://openalex.org/W6609822380",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3105895642",
    "https://openalex.org/W6803785435",
    "https://openalex.org/W2979585142",
    "https://openalex.org/W6631828510",
    "https://openalex.org/W2038702914",
    "https://openalex.org/W2134967712",
    "https://openalex.org/W3094382446",
    "https://openalex.org/W3146384714",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2108552973",
    "https://openalex.org/W3197928146",
    "https://openalex.org/W3151478830",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W3037888463",
    "https://openalex.org/W1975147762",
    "https://openalex.org/W2025816743",
    "https://openalex.org/W2102524821",
    "https://openalex.org/W4283026156",
    "https://openalex.org/W3088999551",
    "https://openalex.org/W4281763794",
    "https://openalex.org/W1485333490",
    "https://openalex.org/W3148323213",
    "https://openalex.org/W3045241251",
    "https://openalex.org/W2769423117",
    "https://openalex.org/W2140538197",
    "https://openalex.org/W2043886357",
    "https://openalex.org/W3168436232",
    "https://openalex.org/W4247259022",
    "https://openalex.org/W2169678694",
    "https://openalex.org/W2960015584",
    "https://openalex.org/W2090649877",
    "https://openalex.org/W3102948683",
    "https://openalex.org/W3097054638",
    "https://openalex.org/W2996428491",
    "https://openalex.org/W2617750324",
    "https://openalex.org/W3200409031",
    "https://openalex.org/W3134307371",
    "https://openalex.org/W4286908792",
    "https://openalex.org/W2886180730",
    "https://openalex.org/W4307653761",
    "https://openalex.org/W4242719469",
    "https://openalex.org/W4287724045",
    "https://openalex.org/W3146944767",
    "https://openalex.org/W4281557260",
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2951433247",
    "https://openalex.org/W4296060337",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3211718555",
    "https://openalex.org/W3183967699",
    "https://openalex.org/W4233120011",
    "https://openalex.org/W2611792444",
    "https://openalex.org/W29374554",
    "https://openalex.org/W3095583226",
    "https://openalex.org/W2061685714"
  ],
  "abstract": "Recent advances in language modeling have tremendously impacted how we handle sequential data in science. Language architectures have emerged as a hotbed of innovation and creativity in natural language processing over the last decade, and have since gained prominence in modeling proteins and chemical processes, elucidating structural relationships from textual/sequential data. Surprisingly, some of these relationships refer to three-dimensional structural features, raising important questions on the dimensionality of the information contained in sequential data. We demonstrate that the unsupervised use of a language model architecture to a language representation of bio-catalyzed chemical reactions can capture the signal at the base of the substrate-active site atomic interactions, identifying the three- dimensional active site position in unknown protein sequences. The language representation comprises a reaction-simplified molecular-input line-entry system (SMILES) for substrate and products, and amino acid sequence information for the enzyme. This approach can recover, with no supervision, 52.12% of the active site when considering co-crystallized substrate-enzyme structures as ground truth, vastly outperforming other attention-based models.",
  "full_text": "Language models can identify enzymatic active sites in protein\nsequences\nYves Gaetan Nana Teukam1,*, Lo¨ıc Kwate Dassi1, Matteo Manica1, Daniel Probst1,2, Philippe\nSchwaller1,2, and Teodoro Laino1,2\n1IBM Research Europe, Sa¨umerstrasse 4, 8803 R¨uschlikon, Switzerland\n2National Center for Competence in Research-Catalysis (NCCR-Catalysis), Switzerland\n*yna@zurich.ibm.com\nJanuary 12, 2023\nAbstract\nRecent advances in language modeling have tremendously impacted how we handle sequential data\nin science. Language architectures have emerged as a hotbed of innovation and creativity in natural\nlanguage processing over the last decade, and have since gained prominence in modeling proteins and\nchemical processes, elucidating structural relationships from textual/sequential data. Surprisingly, some\nof these relationships refer to three-dimensional structural features, raising important questions on the\ndimensionality of the information contained in sequential data. We demonstrate that the unsupervised\nuse of a language model architecture to a language representation of bio-catalyzed chemical reactions\ncan capture the signal at the base of the substrate-active site atomic interactions, identifying the three-\ndimensional active site position in unknown protein sequences. The language representation comprises\na reaction-simplified molecular-input line-entry system (SMILES) for substrate and products, and\namino acid sequence information for the enzyme. This approach can recover, with no supervision,\n52.12% of the active site when considering co-crystallized substrate-enzyme structures as ground truth,\nvastly outperforming other attention-based models.\n1\n1 Introduction\nLanguage Models (LMs) (e.g. BERT (1), GPT (2), and ELMo (3)) made the headlines being worldwide relevant for\ntasks such as information retrieval (4), text generation (5–7), and speech recognition (8). The ability of LMs to\nlearn a probability distribution over sequences of words in relation to a domain-specific language is the primary\nfactor contributing to their level of success. In essence, these architectures encode distinct vector representations\n(embeddings) of a word based on its context, uncovering linguistic relationships between the different words of the\ndomain-specific language.\nLarge Language Models (LLMs) (9, 10) trained on massive and diverse corpora are demonstrating critical new\nabilities, from writing innovative content (11) to resolving simple math problems ( 12). These models achieved\nrelatively high performance on new tasks for which they were not explicitly trained (also known as zero-shot\nlearning tasks) (13, 14), most likely because the ability of language architecture to generalize on new tasks is the\nresult of an unintentional multitask learning process. (14).\nLMs, especially transformers and their derivatives (e.g. BERT (1), ALBERT (15), RoBERTa (16), etc.), had\nalso a relevant impact on chemistry and biology, reaching state-of-the-art performance when fine-tuned on specific\ntasks (17–21). In chemistry, reagents, substrates, and products are usually depicted using a text representation\nsuch as SMILES (simplified molecular-input line-entry system) (22, 23). Using this domain-specific representation,\nscientists showed that LMs can learn to accurately map atoms between precursors and products with an unsupervised\nmasked language modeling (MLM) (24), or predicting molecular properties using a BERT-like model trained in\na semi-supervised way (25). With the extension of string-based representations to proteins, LMs can be used for\nuncovering hidden relationships in biological tasks. Unsupervised language models have been used for predicting\nmutational effect and secondary structure (26), improving long-range contact prediction (27), targeting binding\nsites (28) or capturing important biophysical properties governing protein shape (28, 29).\nThe identification of active site residues and the characterization of the corresponding protein function (30–32)\nis the next big scientific challenge, especially after the pioneering work on predicting proteins structure (20, 33).\nThe activity of a protein is directly related to the structure of the active site (34), a spatial region with a joint/disjoint\namino acid (AA) sequence evolved to interact with specific molecules under selective pressure. The fact that the\namino acids (AA) in the active site have been conserved more than the entire sequence during evolution reflects their\nimportance in providing unique structural features for enzyme function (35, 36). If the signal (or pattern) describing\nthe 3D interaction of amino acids with the corresponding target molecules in the active site could be learned from the\nAA sequence and the molecular representation, protein function could be predicted using only sequential data with\nno use of explicit 3D structural features derived from co-homology strategies (31, 37–42), or from protein-protein\ninteraction networks (43). Currently, only a few efforts, such as Pfam ( 44) and PSI-BLAST ( 45), took up the\nchallenge of identifying active sites solely based on sequence similarity information.\n2\nInspired by the work of (24), here we show that LMs can learn the signal characterizing the active sites AA using\na linguistic representation for proteins and their molecular substrates (see Figure 1). We use a publicly available\ncollection of enzymatic reactions (46), in which substrate molecules are represented with SMILES and the proteins\nwith their AA linear sequence. The unsupervised training can recover 52.12% of the active sites when considering\nco-crystallized substrate-enzyme structures as ground truth without supervision.\nThis work confirms the versatility of LMs in extracting complex structural information from a sequence-based\nrepresentation and demonstrates the effectiveness of using LMs for identifying protein functions.\nFigure 1: RXNAAMAPPER pipeline. A BERT model (1) is trained on a combination of organic and enzymatic\nreaction SMILES using MTL (47), leveraging atom-level tokenization and MLM (48) for the SMILES components,\nwhile Byte Pair Encoding (BPE) tokenization and n-gram MLM for the amino acid sequence part. The trained\nmodel is used in inference to define a score, based on the attention values computed on the reaction SMILES\nprovided as input, which allows the prediction of the active site of the enzyme bio-catalyzing the reaction with no\nsupervision or structural information. The active sites are represented in our plot as red regions in the molecule.\n3\n2 Results\nSequence compression and representation\nBefore language modeling, amino acid sequences must be numerically encoded using an encoding scheme that\ngives each amino acid sequence a unique vector representation. This encoding method acts as a map from the input\namino acids to a point in the protein representation space. The embedding should be able to capture key features\nof each element (also called a token) that is encoded and should be able to preserve the relationship between the\nencoded elements (typically expressed geometrically through a vectorial representation of the encoded tokens).\nHere we consider a Transformer model based on BERT that in a standard setup handles a maximum of 512 input\ntokens to generate the encoding vector. For architectures like ours, 512 input tokens size is a pretty strict limitation\ndue to the large memory footprint coming from self-attention layers, whose complexity scales quadratically with\nthe input sequence length (49). In protein modeling, the model must see the entire amino acid sequence in order\nto learn crucial structural information. Given that amino acid sequences can be prohibitively long, finding a good\ncompression and representation scheme becomes a fundamental task before the model training.\nWe trained different Byte-Pair Encoding (BPE) tokenizers with various settings (as described in the method\nsection) to find the set of parameters that maximizes the compression of the amino acid sequences in terms of\nvocabulary size and sequence length. The compression power of the tokenizers trained has been tested on a dataset\nof random sequences from Uniprot (n = 600K). Figure 2B shows a negative correlation between the vocabulary\nsize and the median number of tokens for the same dataset. This result confirms that by increasing the vocabulary\nsize we are implicitly increasing the length of BPE tokens in our vocabulary, as we merge the most frequently\noccurring fragment of sequences into single subwords or fragments. The BPE tokenizer giving the best compression\nrate (c = 66, 8%) has been trained on the dataset made of amino acid sequences of lengths between 600 and\n700, and setting the vocabulary size to 75K tokens. The comparison of the best performing BPE with a simple\ncharacter-Level tokenizer (ByChar), which splits sequences into single amino acids, shows that the median number\nof tokens per sequence in our test dataset drastically drops to 152 (see Figure 2A and Table S1). Therefore by using\nthis tokenization scheme, we overcome the architectural limitations and train our model on broader corpora.\nActive site prediction\nEnzyme binding sites are areas on an enzyme’s surface specifically intended to interact with other molecules.\nEnzymes can have many types of binding sites that perform distinct tasks and engage different molecules. The most\nsignificant is the active site, which includes catalytic residues to carry out the enzymatic reaction on a substrate. We\ntrained the RXNAAMapper (a BERT-base model combined with BPE tokenization on the amino acid sequences\n4\n(A)\n0 200 400 600 800 1000 1200 1400\nT okens per sequence\n10 k\n20 k\n30 k\n40 k\n50 k\n60 k\nCount\nByChar T okens\nBPE with seq. length 600-700\nMedian BPE = 152\nMedian ByChar = 459 (B)\n10k 20k 30k 50k 75k\nVocabulary size\n155\n160\n165\n170\n175\n180\n185\nMedian number of tokens per sequence\nT okenizer datasets\n250 - 900 \n400 - 500 \n600 - 750 \n900 - 1000\nFigure 2: BPE analysis. On the left is the density distribution plot of the number of tokens using the BPE tokenizer\nchosen vs the ByChar tokenizer. As compared to the ByChar tokenization, the BPE splits infrequent fragments into\ntwo or more fragments and also merges the most frequent ones into longer fragments. Its application results in\nshortening the sequences with respect to the ByChar tokenization scheme. On the right is the median of the BPE for\neach configuration of our grid search.\ndeveloped in this work), BERT-base, BERT-Large, ProtAlbert, and ProtBert (details in methods) and compared\nthem on the task of active site predictions using 777 amino acid sequences from PLIP (with related active sites)\nas ground truth. The predictions are based on the self-attention analysis of the models. Self-attention modules, a\nkey component in Transformers-based models, is an attention mechanism that connects distinct points in a single\nsequence to calculate a representation of the same sequence. To match the required output dimension, the separate\nattention ’heads’ are commonly concatenated and multiplied by a linear layer (50) forming a multi-head attention\nsystem. With this architecture, the specific heads in the model may selectively focus on useful sections of the input\nsequence and so capture the relationship between them. Throughout the analysis of the attention mechanism in\nour model, we found out that the specific combination of attention heads, the layer at which the attention scores\nare extracted, and the number of amino acid tokens to bind with each reactant’s atom (a.k.a topk), led to different\nperformances in the prediction of active sites. Given our architectural design, the combination giving us the highest\noverlap score when predicting the active sites is head = 10, layer = 5, and topk = 6.\nFor each trained model, we determined their performance by selecting the combination giving the highest\noverlap between the predictions and ground truth from PLIP (details in the methods section). We find that\nRXNAAMapper performs consistently better than the other unsupervised sequence-based methods, even though\nthe product information has been completely omitted, given the nature of the dataset. Among the active sites\npredicted by RXNAAMapper, up to 52.12% overlap with the ground truth whereas ProtAlbert, ProtBert, BERT-base,\nBERT-Large, and the random model, reached 18.27%, 20.01%, 15.26%, 45.88%, and 14.07%, respectively. As a\nreference, we report the overlap score obtained by homology-based on Pfam annotations (67.31%).\n5\nActive site prediction based on homology models, like the one obtained using Pfam annotations, can help\nrecover active sites. However, these approaches use heuristics-based methods, giving rise to high frequencies of\nfalse positive rates (Pfam-based = 61.68%). The high false positive rate suggests that the area predicted as active\nsites are too large concerning the actual size of active sites. While active sites usually account for just 10-20% of\nthe volume of an enzyme (51), Pfam’s-based active sites on average account for 61.68% of the size of the input\nsequences. Our model predicted shorter stretches of the input sequences as active sites (on average 41.5%) while\nmaintaining a lower false positive rate compared to the homology-based predictions (47.89%).\nOverlap Score False Positive Rate\nRandom Model 14.07% 13.80%\nBERT-base 15.26% 14.92%\nBERT-Large + BPE 45.88% 42.71%\nProtAlbert 18.27% 27.33%\nProtBert 20.01% 16.42%\nRXNAAMapper (ours) 52.12% 47.89%\nPfam-based 67.31% 61.68%\nTable 1: Performance on sequence-based active site prediction. Reported in the table are the overlap score and\nthe false positive rates for the active site prediction using PLIP as ground truth for the seven methods considered: a\nrandom model, Pfam alignment-based model, a pre-trained BERT-base model, a pre-trained BERT-Large model\ncoupled with a BPE tokenizer, ProtAlbert, ProtBert, and RXNAAMapper. Among these models, Pfam-based\npredictions are based on homology present within Pfam families. The others are attention based models extracted\nfrom unsupervised language models.\nWe inspect the performance of our model and the Pfam-based across different enzyme classes (see Figure 4A)\nand reaction classes (see Figure 4B). Certain types of enzymes (e.g. Transferases) and reaction classes (e.g.\nfunctional group interconversion (FGI)) have a better compromise of overlap score and false positive rate with\nrespect to other classes. In some cases, like with Lysases, our model have a very different behavior compared to the\nhomology-based predictions. Our model is more conservative by generating shorter active sites, which results in a\nlower false positive rate and overlap score. Pfam-based instead generates long active sites. The high false positive\nrate of Pfam-based prediction correlates with the inefficacy of sequence alignments to produce accurate results\nwhen the sequence identity goes below a certain threshold (52). Despite alignment-based methods, our methodology\n(an alignment-free approach) captures evolutionary events without the assumption that homologous sequences are\nthe consequence of a succession of linearly organized and more or less conserved sequence regions.\nWe further compared our prediction and those from homology-based by looking at the distance between the\nbarycenter of the grid boxes centered on the predicted active sites and the ground truths. Although our model has\nlower overlap scores compared to the Pfam-based, our ability to control the false positive rate is reflected on the\n6\n0 2 4 6 8 10 12 14 16 18 20 22 24 26 28 30\nDistance\n0\n50\n100\n150\n200Count\nRXNAAMapper Pfam-based\nFigure 3: Active sites distance from ground truth. Distribution plot depicting the distance of the predicted active\nsite from the PLIP annotations. For both predictions, the distribution is right skewed reflecting the correctness of the\npredictions. RXNAAMapper exhibits a distribution peak at lower values, confirming the superior accuracy of its\npredictions in comparison to Pfam annotations.\nbarycenter of our predictions to be spatially closer to the ground truth (see Figure 3 and Figure S1).\nFigure 5 shows a typical comparison of Pfam-based and RXNAAMapper predictions overlapped with the PLIP\nground truth. Notably, RXNAAMapper exhibits better false positive rates than Pfam alignments, while matching\nthe active site reported in PLIP.\nOur approach demonstrates the potential of language models to retrieve important structural information from\na text representation. This potential reflects in our model’s capacity to control false positives while generating\npredictions that are spatially closer to the actual active sites.\nStructural validation\nFor a chemical reaction to take place, the substrates must bind to the enzyme at its active site. This area is divided\ninto two parts: the binding site and the catalytic site. Some of the residues in the binding site aid in the substrate\n(reactants) binding to the enzyme. A chemical reaction is aided by the catalytic site. In addition to specificity,\nthe substrate binding site supplies binding energy to keep the substrate engaged on the active site throughout the\ncatalytic process. The interaction between the ligand and the enzyme does not occur at random locations but around\nthe active sites to produce specifically high binding energies toward the ligand.\nWe further evaluated the active site predictions from RXNAAMapper by using them to compute the binding\n7\n(A) Comparison at EC class level\n0.0\n0.5\n1.0Overlap Score\nOxidoreductases Transferases Hydrolases\n0.0 0.5 1.0\nFalse Positive Rate\n0.0\n0.5\n1.0Overlap Score\nLyases\n0.0 0.5 1.0\nFalse Positive Rate\nIsomerases\n0.0 0.5 1.0\nFalse Positive Rate\nLigases\nPfam-based RXNAAMapper\n(B) Comparison at reaction class level\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Overlap Score\nHeteroatom alkylation and arylation Acylation and related processes C-C bond formation Deprotections\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Overlap Score\nReductions\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\nOxidations\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\nFunctional group interconversion (FGI)\n0.0 0.2 0.4 0.6 0.8 1.0\nFalse Positive Rate\nFunctional group addition (FGA)\nPfam-based RXNAAMapper\nFigure 4: Pfam-based and RXNAAMapper performances with respect to the EC classes and reaction classes.\nThe models exhibit different performances for different types of reactions and enzymes, underlying the modeling\ncomplexity of certain types of reactions and enzymes. For almost every enzyme class and reaction class, our\nmodel’s predictions are on the bottom left of the figures (lower overlap score and false positive rate), while\nPfam-based predictions are on the symmetrically opposite side of the figure (higher overlap and false positive rate).\nThis highlights the ability of our model to predict active sites while keeping the false positive rate within descent\nfrequencies. The transparency in the figures correlates with the distance between the barycenter of the grid boxes\ncentered on the predicted active sites and the one centered on the ground truth. The closer the prediction is to the\nground truth, the more opaque the point.\n8\nFigure 5: Experiment results. Comparison of the prediction from Pfam alignments (left) and RXNAAMapper\n(right) using PLIP as a ground-truth for a hydrolase (PDB id: 4FJP) interacting with zinc (SMILES: [Zn]). The\narea in white represents the predicted active site region, while the blue area represents the ground truth of active\nsites. The red area depicts the backbone of the protein.\nenergy on a subset of 2213 enzyme-ligand pairs representing the maximum number of pairs extractable from the\nintersection of our dataset and PLIP database (for more details in the Methods section). We used Autodock Vina (53),\na molecular docking tool, to estimate the binding energies. We compare the predicted binding energy generated\nfrom RXNAAMapper’s predicted active sites with those from the corresponding experimental ones provided by\nPLIP. The binding energy calculations performed on the active site predicted by RXNAAMapper accurately match\nthe one using PLIP experimental information, with a difference of 0.37 kcal/mol between the two on average.\n3 Discussion\nThe prediction of protein active sites, which are critical and conserved functional areas of proteins, is crucial to\nimproving our understanding of protein function. Moreover, the ability to detect these regions in an unsupervised\nfashion, solely relying on AA sequence information, allows an initial characterization of novel proteins.\nHerein, we tackled the problem by introducing RXNAAMapper, a technology that uses pre-trained language\nmodels based on textual representations of biochemical reactions to identify active sites in long amino acid sequences.\nWhen tested on PLIP protein-ligand interactions, our approach outperforms other sequence-based methods by\nidentifying more than 52% of proteins’ active regions with a lower false-positive rate. The combination of a model\nlike a BERT-base and a BPE tokenization system leverage an as-of-now unexplored potential.\nOne of the main limitations in applying language models to enzymatic reactions is the computational burden\nintroduced by handling long sequences. Compressing the representations using efficient tokenization strategies\nmitigates the problem, but it has also the detrimental effect of discarding data points that may contain useful\n9\nFigure 6: Negative binding energy of 2213 enzyme-ligand pairs. The figure shows how the energy scores\nderiving from the RXNAAMapper active site predictions are in the same range with respect to those predicted from\nPLIP (R2=0.95).\ninformation. The use of a BPE tokenizer allowed us to train our model on entire amino acid sequences by\ncompressing sequences in a lossless fashion. Leveraging the full sequence is a key component of our model as\namino acids that in a protein sequence are far away may come together in the 3D representation. Unlike other\nmodels evaluated in this paper, RXNAAMapper demonstrated the ability to capture the syntax of bio-catalyzed\nreactions and grasp relevant features of the AA sequences by detecting regions of importance via reaction language\nmodeling.\nTo further validate the active sites predicted with RXNAAMapper, we compared the binding energy of a set\nof enzyme-ligand pairs using the active sites from PLIP as ground truth. The binding energy computed from our\npredicted active sites matched those predicted from the ground truth. This is particularly important because the\nidentification of the active sites helps to predict the binding energy and therefore model the binding sites. Residues\npresent in binding sites, particularly catalytic residues in active sites, are among the most critical residues in an\nenzyme structure. Thus, identifying them is critical for improving the design of biocatalytic processes.\nThis work set a stepping stone towards novel in-silico approaches for protein function identification, and it is\nfurther evidence of the amazing ability of language models to retrieve 3D structure information from 1D sequential\n10\nrepresentation. We are confident that future language models with yet-to-be-unveiled capabilities will continue to\noffer innovative solutions to complex tasks using domain-specific languages without supervision.\n4 Methods\nDataset\nWe consider a dataset of 1 million organic reactions from USPTO (54), split as reported in schwaller2019molecular,\ncombined with a dataset of bio-catalyzed reactions called ECREACT (46). ECREACT contains 62,222 reactions\nwith a unique reaction-EC combination. The entries from this dataset can be classified based on their EC numbers\nand grouped as in Table 2. ECREACT is the result of the combination of bio-catalyzed reactions coming from 4\ndatabases: Brenda (55), Rhea (56), PathBank (57), and MethaNetX (58). For our analysis, we consider the entire\ndataset regardless of the level of information contained in the EC level.\nGroups N o of reactions Level of information\nNo EC number 55,115 No information about the enzyme\nEC-level 1 (EC1) 55,707 Enzyme class\nEC-level 1-2 (EC2) 56,222 EC1 + Subclass\nEC-level 1-3 (EC3) 56,579 EC2 + sub-subclass\nEC-level 1-4 (EC4) 62,222 EC3 + serial number in the subclass\nTable 2: ECREACT dataset division divided into groups based on the level of information\nData processing\nUsing EC numbers as the single filter, we mapped the EC numbers to their corresponding AA sequences from\nUniprot (59). A filtering step is performed to reduce the overrepresentation of certain EC numbers by limiting to\n10K the maximum number of sequences per EC number. In case of exceeding for certain EC number, we randomly\nselected 10K AA sequences from its set. Then finally paired the substrate-product with the AA sequences.\nTokenization\nLanguage models operate on numerical data requiring text transformation into a numerical representation. An\nimportant step in the conversion of text to vectors is the tokenization step. Tokenization is the task of dividing a text\ninto its constituent parts. Two different tokenization approaches are used to deal with the dual representation of\nthe molecular entries in our dataset (SMILES and amino acids sequences). For SMILES, we use a Regex-based\ntransformation (60) (character-level tokenization). While for amino acid sequences, the selection of the tokenizer\n11\nis a bit more complex. The complexity derives from: (a) the length of the sequences; (b) the limited number of\ninput tokens supported by LMs. To overcome these limitations, compress our input sequences. We trained several\ntokenizers with various settings to maximize the sequence compression using a grid search over sequence length\nand vocabulary size.\nAll the tokenizers trained are BPE (Byte-Pair Encoding) tokenizers. To investigate the effect of the vocabulary\nsize on the tokenizers’ compression abilities, we set this variable to 10k, 20k, 30k, 50k, and 75k. We create\nfour small datasets of randomly selected AA sequences in the following ranges: 250-900, 400-500, 600-750, and\n900-1000. Each dataset consists of 400K sequences.\nModels and training procedure\nHerein, we consider different transformer architectures, i.e., BERT (1) and Albert (15), exploring various approaches:\ntraining from scratch, fine-tuning, and pre-trained models.\nIn training, to better control the token masking and handle the different lengths of the enzymatic reaction\ncomponents, the model variants have been jointly optimized with Masked Language Modeling (MLM) and an\nn-gram Masked Language Modeling (48) (n-gram MLM, by randomly masking out 15% of the input tokens). MLM\nand n-gram MLM have been applied to substrates/products and enzymes respectively. As the models are trained on\ndifferent datasets, i.e. ECREACT (46) and USPTO (54), Multi-task Transfer Learning (MTL) (47) has been adopted\non a combination of reaction SMILES representing organic reactions (weight assigned 0.1) and bio-catalyzed\nreactions (weight assigned 0.9) to create a task-specific language model able to understand bio-catalyzed reactions.\nThe use of USPTO in a transfer learning process is to ease the understanding of generic chemistry and SMILES\nsyntax. For enzymatic reactions, each example consists of a reaction SMILES complemented with the AA sequence\nrepresentation of the enzyme of interest (see Figure 1 for a depiction). As we train the model via MLM and n-gram\nMLM, we sparsely mask the reactants and the products and densely mask the enzyme sequence.\nSix million (6M) reactions subset of the preprocessed ECRACT was chosen at random and used as the training\nset for all language models. Another 2.5M ECREACT subset was chosen as the validation set to compute the\nvalidation loss. For all the language models we used default hyper-parameters from the HuggingFace implementation.\nAs an optimizer we adopted ADAM (61) for 50,000 training steps.\nIt has been recently shown that large pre-trained models on natural language can be fine-tuned on different data\nmodalities to attain comparative performance with respect to models trained on downstream tasks (62). Inspired\nby this seminal work, here we also decided to include in our study the following models: ProtAlbert ( 63) and\nProtBert (63) (pre-trained on protein data and fine-tune on bioctalyzed data), BERT-base (pre-trained on various\ndata modalities), BERT-base (1) and BERT-Large (1)(trained from scratch on bioctalyzed data).\n12\nActive site prediction\nThe prediction of the active regions of proteins is unsupervised and entirely based on the analysis of the attention\nvalues computed by the pre-trained language model after encoding a reaction. If we labelS ∈Rl×d (l = r + m + p)\nas the embedding of a given reaction and r, m, and p refer to the length of the reactants, the enzyme, and the\nproducts, respectively, a forward pass of S through the model yields a sequence S′with the same dimension as S.\nEach encoder block computes the attention matrix A ∈Rl×l of the sequence S provided as input (50). We construct\na matrix P ∈Rr×m by summing two sub-matrices of A, describing the link between reactants and enzymes:\nP = A[1 :r,1 :m] +A[r + 1 :r + 1 +m, 1 :r]T . We use the matrix P as shown in the Algorithm 1 to predict\nthe active regions via a consensus scheme where each reactant’s atom hask votes to choose its best-bound enzyme’s\ntoken. The selected enzyme’s tokens are uniquely gathered in a set and are considered the protein’s active region.\nHereinafter, the method combined with BERT-base and the BPE will be referred to as RXNAAMapper.\nAlgorithm 1Active Site Prediction\n1: procedure RXNAAMA P P E R(P ∈Rr×m, k)\n2: active site ←set()\n3: for i in 1..r do\n4: line ←P[i]\n5: for j in argmax(line, k) do\n6: active site.add(j)\n7: return active site\nEvaluation\nWe use a set of 5K co-crystallized ligand-protein pairs from the Protein-Ligand Interaction Profiler (PLIP) (64) as\nground-truth, to perform a two-fold evaluation: (1) a sequence-based assessment benchmarking RXNAAMapper\nagainst two fine-tuned protein language models (ProtAlbert and ProtBert), a statistical baseline (Random Model),\ntwo pre-trained BERT models on natural language (BERT-base and BERT-Large) and the alignments retrieved from\nPfam (44); (2) a structural validation with protein-ligand binding energies computed with docking. We used Pfam\nannotations for a fair assessment with existing methods using sequence information only. For the sequence-based\nevaluation, we use an overlap score between the prediction and the ground truth, as well as the false positive rate.\nThe overlap score is defined considering the active site as a set of non-overlapping segments in a sequence. If S\nwith |S|= n is a sequence of amino acid residues, the active region As of S is defined as As = {(ai, bi)}m\ni , where\nai and bi are the index boundaries of the segment i. The overlap score (OS(A, As)) between the predicted active\n13\nregion A = {(api, bpi)}n\ni and the ground-truth As = {(asi, bsi)}m\ni is defined as:\nOS(A, As) =\n∑n\ni\n∑m\nj max(0, min(bpi, bsj) −max(api, asj))∑m\ni (bsi −asi)\nBesides the overlap score, the false positive rate (FPR ) of the predictions is defined as:\nFPR =\n∑n\ni (bpi −api)1 m⋀\nj=1\n[api,bpi]∩[asj,bsj]=∅\n∑n\ni (bpi −api)\nFor the structural assessment, on a set of 2213 protein-ligand active site predictions, we evaluated the binding\nenergy computed with Autodock Vina (53,65) considering predicted active sites and the ground truth from PLIP. We\nchose these enzyme-ligand pairs by first matching PDBs and amino acid sequences with annotated active sites from\nPLIP. Then filtering reactions catalyzed by enzymes not present in our training set. We then selected the reactions\nhaving unique combinations of PDB, EC number, ligands, and predicted active sites. We computed the Cartesian\ncoordinates of the ligand and receptor molecules, which are generally retrieved from the PDB (66) or PDBQT (67)\nfor the protein, and PDB, PDBQT, or Mol2 for the ligand. To calculate the binding free energy of a ligand to an\nenzyme, we first computed a grid box centered on the active site where the ligand is to be docked. The box has been\nfound by averaging the 3D coordinates of the atoms of the active site and setting the box side length to 50 ˚A.\nData and code availability\nThe ECREACT data set is publicly available at the URL https://github.com/rxn4chemistry/biocatalysis-model. The\ncode is available at the URL https://github.com/rxn4chemistry/rxnaamapper. Structures of docked proteins and\nresults are available at the URL https://doi.org/10.5281/zenodo.7530180.\nAcknowledgments\nThis publication was created as part of NCCR Catalysis (grant number 180544), a National Centre of Competence\nin Research funded by the Swiss National Science Foundation.\n14\nReferences\n1. J. Devlin, M. Chang, K. Lee, K. Toutanova, BERT: Pre-training of Deep Bidirectional Transformers for\nLanguage Understanding(Association for Computational Linguistics, 2019), vol. 1, pp. 4171–4186.\n2. S.-Y . Su, Y .-S. Chuang, Y .-N. Chen,Dual Inference for Improving Language Understanding and Generation\n(arXiv, 2020).\n3. M. E. Peters, et al., Deep contextualized word representations(arXiv, 2018).\n4. S. Zhuang, H. Li, G. Zuccon, Deep Query Likelihood Model for Information Retrieval(2021), pp. 463–470.\n5. J. Li, T. Tang, W. X. Zhao, J.-Y . Nie, J.-R. Wen,Pretrained Language Models for Text Generation: A Survey\n(arXiv, 2022).\n6. A. Radford, et al., Language Models are Unsupervised Multitask Learners(2019).\n7. T. B. Brown, et al., Language Models are Few-Shot Learners(arXiv, 2020).\n8. T. Hori, J. Cho, S. Watanabe, End-to-end speech recognition with word-based rnn language models. 2018 IEEE\nSpoken Language Technology Workshop (SLT)pp. 389–396 (2018).\n9. F. Xu, U. Alon, G. Neubig, V . Hellendoorn,A systematic evaluation of large language models of code(2022),\npp. 1–10.\n10. J. Wei, et al., Emergent Abilities of Large Language Models(2022).\n11. T. B. Brown, et al., Language models are few-shot learners. CoRR abs/2005.14165 (2020).\n12. K. Noorbakhsh, M. Sulaiman, M. Sharifi, K. Roy, P. Jamshidi, Pretrained Language Models are Symbolic\nMathematics Solvers too!(arXiv, 2021).\n13. T. Kojima, S. Gu, M. Reid, Y . Matsuo, Y . Iwasawa,Large Language Models are Zero-Shot Reasoners(2022).\n14. V . Sanh,et al., Multitask prompted training enables zero-shot task generalization. CoRR abs/2110.08207\n(2021).\n15. L. Zhenzhong, et al., Albert: A lite bert for self-supervised learning of language representations. arXiv preprint\narXiv:1909.11942 (2019).\n16. Y . Liu,et al., RoBERTa: A Robustly Optimized BERT Pretraining Approach(2019).\n15\n17. P. Schwaller, et al., Predicting retrosynthetic pathways using transformer-based models and a hyper-graph\nexploration strategy. Chemical Science11 (2020).\n18. A. C. Vaucher,et al., Inferring experimental procedures from text-based representations of chemical reactions.\nNat. Commun.12, 2573 (2021).\n19. R. Rao, et al., MSA Transformer. bioRxiv (2021).\n20. J. Jumper, et al., Highly accurate protein structure prediction with alphafold. Nature 596, 1-11 (2021).\n21. A. Toniato, P. Schwaller, A. Cardinale, J. Geluykens, T. Laino, Unassisted noise reduction of chemical reaction\ndatasets. Nature Machine Intelligence3, 485–494 (2021).\n22. D. Weininger, SMILES, a chemical language and information system. 1. Introduction to methodology and\nencoding rules. Journal of Chemical Information and Computer Sciences28, 31–36 (1988). Publisher: American\nChemical Society.\n23. D. Weininger, A. Weininger, J. L. Weininger, SMILES. 2. Algorithm for Generation of Unique SMILES\nNotation. Journal of Chemical Information and Computer Sciences29, 97–101 (1989). Number: 2 Publisher:\nAmerican Chemical Society.\n24. P. Schwaller, B. Hoover, J.-L. Reymond, H. Strobelt, T. Laino, Extraction of organic chemistry grammar from\nunsupervised learning of chemical reactions. Science Advances7, eabe4166 (2021).\n25. S. Wang, Y . Guo, Y . Wang, H. Sun, J. Huang, SMILES-BERT: large scale unsupervised pre-training for\nmolecular property prediction(2019), pp. 429–436.\n26. A. Rives, et al., Biological structure and function emerge from scaling unsupervised learning to 250 million\nprotein sequences. Proceedings of the National Academy of Sciences118 (2021).\n27. R. Rao, et al., Evaluating protein transfer learning with tape. Advances in neural information processing\nsystems 32, 9689 (2019).\n28. J. Vig, et al., BERTology Meets Biology: Interpreting Attention in Protein Language Models(2020).\n29. A. Elnaggar, et al., Prottrans: towards cracking the language of life’s code through self-supervised deep learning\nand high performance computing. arXiv preprint arXiv:2007.06225(2020).\n30. A. Chatterjee, Protein active site structure prediction strategy and algorithm. International Journal of Current\nEngineering and Technology7, 1092-1096 (2017).\n16\n31. A. Yousaf, T. Shehzadi, A. Farooq, K. Ilyas, Protein active site prediction for early drug discovery and designing.\nInternational Review of Applied Sciences and Engineering13, 98 - 105 (2021).\n32. T.-D. Nguyen-Trinh, K. Lee, R. Kusuma, Y . Y . Ou, Prediction of atp-binding sites in membrane proteins using\na two-dimensional convolutional neural network. Journal of Molecular Graphics and Modelling92 (2019).\n33. M. Baek, et al., Accurate prediction of protein structures and interactions using a three-track neural network.\nScience 373, eabj8754 (2021).\n34. Z.-P. Liu, L.-Y . Wu, Y . Wang, X. Zhang, L. Chen, Bridging protein local structures and protein functions.\nAmino acids35, 627-50 (2008).\n35. A. Sharir-Ivry, Y . Xia, Quantifying evolutionary importance of protein sites: A tale of two measures.PLOS\nGenetics 17, e1009476 (2021).\n36. G. Bartlett, C. Porter, N. Borkakoti, J. Thornton, Analysis of catalytic residues in enzyme active sites. Journal\nof molecular biology324, 105-21 (2002).\n37. S. Sankararaman, F. Sha, J. F. Kirsch, M. I. Jordan, K. Sj¨olander, Active site prediction using evolutionary and\nstructural information. Bioinformatics 26, 617–624 (2010).\n38. J. Jim´enez, S. Doerr, G. Mart´ınez-Rosell, A. S. Rose, G. De Fabritiis, DeepSite: protein-binding site predictor\nusing 3D-convolutional neural networks. Bioinformatics 33, 3036–3042 (2017).\n39. I. Kozlovskii, P. Popov, Protein–Peptide Binding Site Detection Using 3D Convolutional Neural Networks.\nJournal of chemical information and modeling61, 3814–3823 (2021).\n40. J. Yang, A. Roy, Y . Zhang, Protein–ligand binding site recognition using complementary binding-specific\nsubstructure comparison and sequence profile alignment. Bioinformatics 29, 2588–2595 (2013).\n41. I. Kozlovskii, P. Popov, Spatiotemporal identification of druggable binding sites using deep learning. Communi-\ncations Biology3, 618 (2020).\n42. M. N. Wass, L. A. Kelley, M. J. E. Sternberg, 3DLigandSite: predicting ligand-binding sites using similar\nstructures. Nucleic Acids Research38, W469–W473 (2010).\n43. C. Zhang, P. L. Freddolino, Y . Zhang, COFACTOR: improved protein function prediction by combining\nstructure, sequence and protein–protein interaction information. Nucleic Acids Research45, W291-W299\n(2017).\n44. J. Mistry, et al., Pfam: The protein families database in 2021. Nucleic Acids Research49, D412-D419 (2020).\n17\n45. S. F. Altschul, et al., Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.\nNucleic Acids Research25, 3389-3402 (1997).\n46. D. Probst, et al., Biocatalysed synthesis planning using data-driven learning. Nat Commun13, 964(2022).\n47. G. Pesciullesi, P. Schwaller, T. Laino, J.-L. Reymond, Transfer learning enables the molecular transformer to\npredict regio-and stereoselective reactions on carbohydrates. Nature communications11, 1–8 (2020).\n48. D. Xiao, et al., Ernie-gram: ransraining with explicitly n-gram masked language modeling for natural language\nunderstanding. arXiv preprint arXiv:2010.12148(2020).\n49. S. Sun, K. Krishna, A. Mattarella-Micke, M. Iyyer,Do Long-Range Language Models Actually Use Long-Range\nContext? (2021), pp. 807–822.\n50. A. Vaswani, et al., Attention is all you need. CoRR abs/1706.03762 (2017).\n51. Enzymes Are Wonderful Catalysts(John Wiley Sons, Ltd, 2012), chap. 3, pp. 26–49.\n52. A. Chattopadhyay, N. Diar, D. Flower, A statistical physics perspective on alignment-independent protein\nsequence comparison. Bioinformatics (Oxford, England)31 (2015).\n53. E. J´erˆome, D. Santos-Martins, A. Tillack, S. Forli, Autodock vina 1.2.0: New docking methods, expanded force\nfield, and python bindings. Journal of chemical information and modeling(2021).\n54. D. M. Lowe, Extraction of chemical structures and reactions from the literature, Ph.D. thesis, University of\nCambridge (2012).\n55. A. J¨ade, et al., Brenda, the elixir core data resource in 2021: new developments and updates. Nucleic Acids\nResearch 49 (2020).\n56. P. Bansal, et al., Rhea, the reaction knowledgebase in 2022. Nucleic acids research50 (2021).\n57. D. Wishart, et al., Pathbank: a comprehensive pathway database for model organisms. Nucleic acids research\n48 (2019).\n58. M. Ganter, T. Bernard, S. Moretti, J. Stelling, M. Pagni, Metanetx.org: a website and repository for accessing,\nanalysing and manipulating metabolic networks. Bioinformatics (Oxford, England)29 (2013).\n59. T. U. Consortium, UniProt: the universal protein knowledgebase in 2021. Nucleic Acids Research49, D480-\nD489 (2020).\n18\n60. P. Schwaller, T. Gaudin, D. Lanyi, C. Bekas, T. Laino, ”found in translation”: Predicting outcomes of complex\norganic chemistry reactions using neural sequence-to-sequence models. Chemical Science9 (2017).\n61. D. P. Kingma, J. Ba, Adam: A Method for Stochastic Optimization(arXiv, 2014).\n62. K. Lu, A. Grover, P. Abbeel, I. Mordatch,Pretrained Transformers as Universal Computation Engines(2021).\n63. A. Elnaggar, et al., ProtTrans: Towards Cracking the Language of Life’s Code Through Self-Supervised Deep\nLearning and High Performance Computing(arXiv, 2020).\n64. S. Salentin, S. Schreiber, V . J. Haupt, M. F. Adasme, M. Schroeder, Plip: fully automated protein–ligand\ninteraction profiler. Nucleic acids research43, W443–W447 (2015).\n65. O. Trott, A. Olson, Autodock vina: Improving the speed and accuracy of docking with a new scoring function,\nefficient optimization, and multithreading. Journal of Computational Chemistry31 (2010).\n66. H. Berman, et al., The protein data bank. Nucleic acids research28, 235-42 (2000).\n67. N. O’Boyle, et al., Open babel: An open chemical toolbox. Journal of cheminformatics3, 33 (2011).\n19\nSupplementary Materials for\nLanguage models can identify enzymatic active sites in protein sequences\nYves Gaetan Nana Teukamet al.1\n*yna@zurich.ibm.com\nJanuary 12, 2023\nThis PDF file includes:\nFigure 1 and Table 1\n1\nBPE datasets\nVocabulary size 200-900 400-500 600-750 900-1K\n10K 182 185 181 182\n20K 170 174 169 170\n30K 164 168 163 164\n50K 158 162 156 157\n75K 153 157 152 153\nTable 1: Median number of tokens per sequence\n2\n(A) EC class level\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125Overlap Score\nOxidoreductases Transferases Hydrolases\n0 20 40\nFalse Positive Rate\n0.000\n0.025\n0.050\n0.075\n0.100\n0.125Overlap Score\nLyases\n0 20 40\nFalse Positive Rate\nIsomerases\n0 20 40\nFalse Positive Rate\nLigases\nPfam-based RXNAAMapper\n(B) Reaction class level\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14Overlap Score\nHeteroatom alkylation and arylation Acylation and related processes C-C bond formation Deprotections\n0 20 40\nFalse Positive Rate\n0.00\n0.02\n0.04\n0.06\n0.08\n0.10\n0.12\n0.14Overlap Score\nReductions\n0 20 40\nFalse Positive Rate\nOxidations\n0 20 40\nFalse Positive Rate\nFunctional group interconversion (FGI)\n0 20 40\nFalse Positive Rate\nFunctional group addition (FGA)\nPfam-based RXNAAMapper\nFigure 1:Distance between the predicted active sites and ground truth. The distance between the barycenter of\nthe grid boxes centred on the predicted active sites and the ground facts was used to compare our prediction to those\nfrom the homology-based. Figure 1A has been computed by grouping the points in out set by EC classes, while\nFigure 1B on reaction classes.\n3",
  "topic": "Active site",
  "concepts": [
    {
      "name": "Active site",
      "score": 0.6925065517425537
    },
    {
      "name": "Computer science",
      "score": 0.6608446836471558
    },
    {
      "name": "Representation (politics)",
      "score": 0.6109538674354553
    },
    {
      "name": "Substrate (aquarium)",
      "score": 0.5583218336105347
    },
    {
      "name": "Natural language",
      "score": 0.49534597992897034
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4442829191684723
    },
    {
      "name": "Natural language processing",
      "score": 0.4130592942237854
    },
    {
      "name": "Chemistry",
      "score": 0.2835056781768799
    },
    {
      "name": "Enzyme",
      "score": 0.1958027482032776
    },
    {
      "name": "Biology",
      "score": 0.1244117021560669
    },
    {
      "name": "Biochemistry",
      "score": 0.08401897549629211
    },
    {
      "name": "Political science",
      "score": 0.0
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Law",
      "score": 0.0
    },
    {
      "name": "Politics",
      "score": 0.0
    }
  ]
}