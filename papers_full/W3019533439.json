{
    "title": "Learning molecular dynamics with simple language model built upon long short-term memory neural network",
    "url": "https://openalex.org/W3019533439",
    "year": 2020,
    "authors": [
        {
            "id": "https://openalex.org/A4310059759",
            "name": "Sun-Ting Tsai",
            "affiliations": [
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A4284078001",
            "name": "En-Jui Kuo",
            "affiliations": [
                "Joint Quantum Institute",
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A2404999425",
            "name": "Pratyush Tiwary",
            "affiliations": [
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A4310059759",
            "name": "Sun-Ting Tsai",
            "affiliations": [
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A4284078001",
            "name": "En-Jui Kuo",
            "affiliations": [
                "Joint Quantum Institute",
                "University of Maryland, College Park"
            ]
        },
        {
            "id": "https://openalex.org/A2404999425",
            "name": "Pratyush Tiwary",
            "affiliations": [
                "University of Maryland, College Park"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1976068300",
        "https://openalex.org/W2079078520",
        "https://openalex.org/W2122585011",
        "https://openalex.org/W2143612262",
        "https://openalex.org/W2157331557",
        "https://openalex.org/W1485009520",
        "https://openalex.org/W2209610041",
        "https://openalex.org/W2064675550",
        "https://openalex.org/W2402268235",
        "https://openalex.org/W2118434577",
        "https://openalex.org/W1525783482",
        "https://openalex.org/W2981466594",
        "https://openalex.org/W2171865010",
        "https://openalex.org/W2782714865",
        "https://openalex.org/W2972246420",
        "https://openalex.org/W3080219767",
        "https://openalex.org/W3010340553",
        "https://openalex.org/W2790756470",
        "https://openalex.org/W2100596347",
        "https://openalex.org/W2318093595",
        "https://openalex.org/W2098514243",
        "https://openalex.org/W2528583500",
        "https://openalex.org/W2964149432",
        "https://openalex.org/W3023918710",
        "https://openalex.org/W2092144432",
        "https://openalex.org/W2018532993",
        "https://openalex.org/W2167468911",
        "https://openalex.org/W2035527380",
        "https://openalex.org/W2051116535",
        "https://openalex.org/W2130390906",
        "https://openalex.org/W2949765518",
        "https://openalex.org/W2783073729",
        "https://openalex.org/W2082418604",
        "https://openalex.org/W2132316084",
        "https://openalex.org/W1572086689",
        "https://openalex.org/W2053348581",
        "https://openalex.org/W2226825552",
        "https://openalex.org/W2143592395",
        "https://openalex.org/W2065281489",
        "https://openalex.org/W1981276685",
        "https://openalex.org/W1969798710",
        "https://openalex.org/W1981021420",
        "https://openalex.org/W1031578623",
        "https://openalex.org/W2970740744",
        "https://openalex.org/W2057477511"
    ],
    "abstract": null,
    "full_text": "ARTICLE\nLearning molecular dynamics with simple language\nmodel built upon long short-term memory neural\nnetwork\nSun-Ting Tsai1, En-Jui Kuo2 & Pratyush Tiwary3✉\nRecurrent neural networks have led to breakthroughs in natural language processing and\nspeech recognition. Here we show that recurrent networks, speciﬁcally long short-term\nmemory networks can also capture the temporal evolution of chemical/biophysical trajec-\ntories. Our character-level language model learns a probabilistic model of 1-dimensional\nstochastic trajectories generated from higher-dimensional dynamics. The model captures\nBoltzmann statistics and also reproduces kinetics across a spectrum of timescales. We\ndemonstrate how training the long short-term memory network is equivalent to learning a\npath entropy, and that its embedding layer, instead of representing contextual meaning of\ncharacters, here exhibits a nontrivial connectivity between different metastable states in the\nunderlying physical system. We demonstrate our model ’s reliability through different\nbenchmark systems and a force spectroscopy trajectory for multi-state riboswitch. We\nanticipate that our work represents a stepping stone in the understanding and use of\nrecurrent neural networks for understanding the dynamics of complex stochastic molecular\nsystems.\nhttps://doi.org/10.1038/s41467-020-18959-8 OPEN\n1 Department of Physics and Institute for Physical Science and Technology, University of Maryland, College Park, MD 20742, USA.2 Department of Physics\nand Joint Quantum Institute, University of Maryland, College Park, MD 20742, USA.3 Department of Chemistry and Biochemistry and Institute for Physical\nScience and Technology, University of Maryland, College Park, MD 20742, USA.✉email: ptiwary@umd.edu\nNATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications 1\n1234567890():,;\nR\necurrent neural networks (RNN) are a machine learning/\nartiﬁcial intelligence (AI) technique developed for model-\ning temporal sequences, with demonstrated successes\nincluding but not limited to modeling human languages1–7.A\nspeciﬁc and extremely popular instance of RNNs are long short-\nterm memory (LSTM)8 neural networks, which possess more\nﬂexibility and can be used for challenging tasks such as language\nmodeling, machine translation, and weather forecasting 6,9,10.\nLSTMs were developed to alleviate the limitation of previously\nexisting RNN architectures wherein they could not learn infor-\nmation originating from far past in time. This is known as the\nvanishing gradient problem, a term that captures how the gra-\ndient or force experienced by the RNN parameters vanishes as a\nfunction of how long ago did the change happen in the under-\nlying data11,12. LSTMs deal with this problem by controllingﬂows\nof gradients through a so-called gating mechanism where the\ngates can open or close determined by their values learned for\neach input. The gradients can now be preserved for longer\nsequences by deliberately gating out some of the effects. This way\nit has been shown that LSTMs can accumulate information for a\nlong period of time by allowing the network to dynamically learn\nto forget aspects of information. Very recently LSTMs have also\nbeen shown to have the potential to mimic trajectories produced\nby experiments or simulations13, making accurate predictions\nabout a short time into the future, given access to a large amount\nof data in the past. Similarly, another RNN variant named\nreservoir computing14 has been recently applied to learn and\npredict chaotic systems15. Such a capability is already useful for\ninstance in weather forecasting, where one needs extremely\naccurate predictions valid for a short period of time. In this work,\nwe consider an alternate and arguably novel use of RNNs, spe-\nciﬁcally LSTMs, in making predictions that in contrast to pre-\nvious work13,15, are valid for very long periods of time but only in\na statistical sense. Unlike domains such as weather forecasting or\nspeech recognition where LSTMs have allowed very accurate\npredictions albeit valid only for short duration of time, here we\nare interested in problems from chemical and biological physics,\nwhere the emphasis is more on making statistically valid pre-\ndictions valid for extremely long duration of time. This is typiﬁed\nfor example through the use of the ubiquitous notion of rate\nconstant for activated barrier crossing, where short-time move-\nments are typically treated as noise, and are not of interest for\nbeing captured through a dynamical model.\nHere we suggest an alternative way to use LSTM-based lan-\nguage model to learn a probabilistic model from the time\nsequence along some low-dimensional order parameters pro-\nduced by computer simulations or experiments of a high-\ndimensional system. We also show by our computer simulations\nof different model systems that the language model can produce\nthe correct Boltzmann statistics (as can other AI methods such as\nrefs.\n16,17) but also the kinetics over a large spectrum of modes\ncharacterizing the dynamics in the underlying data. We highlight\nhere a unique aspect of this calculation that the order parameter\nour framework needs could be arbitrarily far from the true\nunderlying slow mode, often called reaction coordinate. This in\nturn dictates how long of a memory kernel must be captured\nwhich is in general a very hard problem to solve18,19. Our fra-\nmework is agnostic to proximity from the true reaction coordi-\nnate and reconstructs statistically accurate dynamics in a wide\nrange of order parameters. We also show how the minimization\nof loss function leads to learning the path entropy of a physical\nsystem, and establish a connection between the embedding layer\nand transition probability. Followed by this connection, we also\nshow how we can de ﬁne a transition probability through\nembedding vectors. We provide tests for Boltzmann statistics and\nkinetics for Langevin dynamics of model potentials, MD\nsimulation of alanine dipeptide, and trajectory from single\nmolecule force spectroscopy experiment on a multi-state ribos-\nwitch20, respectively. We also compare our protocol with alter-\nnate approaches including Hidden Markov Models. Our work\nthus represents a new usage of a popular AI framework to per-\nform dynamical reconstruction in a domain of potentially high\nfundamental and practical relevance, including materials and\ndrug design.\nResults\nMolecular dynamics can be mapped into a sequence of char-\nacters. Our central rationale in this work is that molecular\ndynamics (MD) trajectories, adequately discretized in space and\ntime, can be mapped into a sequence of characters in some lan-\nguages. By using a character-level language model that is effective\nin predicting future characters given the characters so far in a\nsequence, we can learn the evolution of the MD trajectory that\nwas mapped into the characters. The model we use is stochastic\nsince it learns each character through the probability they appear\nin a corpus used for training. This language model consists of\nthree sequential parts shown schematically in Fig.1. First, there is\nan embedding layer mapping one-hot vectors to dense vectors,\nfollowed by an LSTM layer which connects input states and\nhidden states at different time steps through a trainable recursive\nfunction, and ﬁnally a dense layer to transform the output of\nLSTM to the categorical probability vector.\nSpeciﬁcally, here we consider as input a one-dimensional time\nseries produced by a physical system, for instance through\nLangevin dynamics being undergone by a complex molecular\nsystem. The time series consist of data points {ξ\n(t)}, wheret labels\nthe time step and ξ 2 R is some one-dimensional collective\nvariable or order parameter for the high-dimensional molecular\nsystem. In line with standard practice for probabilistic models, we\nLSTM layer\nActual value\nLoss function\nx(t)\nh(t−1)\nf(t)\ni(t)\n˜c(t)\no(t) o(t) tanh c(t) = h(t)\nf(t) c(t−1) + i(t) ˜c(t)\n= c(t)\nx(t) = Λs(t)\ny(t) = softmax(Ddh(t) + bd)\ny(t) = s(t+1)\nJ = − y(t)    ln y(t) \nEmbedding layer\nDense layer\nFig. 1 Neural network schematic.The schematic plot of the simple\ncharacter-level language model used in this work. The model consists of\nthree main parts: The embedding layer, the LSTM layer, and a dense output\nlayer. The embedding layer is a linear layer which multiplies the one-hot\ninput s(t) by a matrix and produces an embedding vectorx(t). The x(t) is\nthen used as the input of LSTM network, in which the forget gatef(t), the\ninput gate i(t), the output gateo(t), and the candidate value~cðtÞ are all\ncontrolled by (x(t), h(t−1)). The forget gate and input gate are then used to\nproduce the update equation of cell statec(t). The output gate decides how\nmuch information propagates to the next time step. The output layer\npredicts the probabilities^yðtÞ by parametrizing the transformation fromh(t)\nto ^y with learned weightsDd and learned biasesbd. Finally, we can compute\nthe cross entropy between the predicted probability distribution^yðtÞ and the\ntrue probability distributiony(t) = s(t+1).\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8\n2 NATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications\nconvert the data points to one-hot encoded representations that\nimplement spatial discretization. Thus each data point {ξ(t)}i s\nrepresented by a N-dimensional binary vector s(t), where N is\nthe number of discrete grid-points. An entry of one stands for the\nrepresentative value and all the other entries are set to zeros. The\nrepresentative values are in generalﬁnite if the order parameter is\nbounded, and are equally spaced in R with in total N\nrepresentative values. Note that the time series {ξ\n(t)} does not\nhave to be one-dimensional. For a higher-dimensional series, we\ncan always choose a set of representative values corresponding to\nlocations in the higher-dimensional space visited trajectory. This\nwould typically lead to a larger N in the one-hot encoded\nrepresentations, but the training set size itself will naturally stay\nthe same. Weﬁnd that the computational effort only depends on\nthe size of training set and very weakly onN, and thus the time\nspent for learning a higher dimensional time series does not\nincrease much relative to a one-dimensional series.\nIn the sense of modeling languages, the one-hot representation\non its own cannot capture the relation between different\ncharacters. Take for instance that there is no word in the English\nlanguage where the characterc is followed byx, unless of course\none allows for the possibility of a space or some other letter in\nbetween. To deal with this, computational linguists make use of\nan embedding layer. The embedding layer works as a look-up\ntable which converts each one-hot vectors(t) to a dense vector\nxðtÞ 2 RM by the multiplication of a matrixΛ which is called the\nembedding matrix, whereM is called the embedding dimension\nxðtÞ ¼ ΛsðtÞ ð1Þ\nThe sequence of dense representation x(t) accounts for the\nrelation between different characters as seen in the training time\nseries. x(t) is then used as the input of the LSTM layer. Eachx(t)\ngenerates an output hðtÞ 2 RL from LSTM layer, where L is a\ntunable hyperparameter. LargerL generally gives better learning\ncapability but needs more computational resources. The LSTM\nitself consists of the following elements: the input gatei(t), the\nforget gatef(t), the output gateo(t) the cell statec(t), the candidate\nvalue ~cðtÞ, and h(t) which is the hidden state vector and theﬁnal\noutput from the LSTM. Each gate processes information in\ndifferent aspects.8 Brieﬂy, the input gate decides which informa-\ntion to be written, the forget gate decides which information to be\nerased, and the output gate decides which information to be read\nfrom the cell state to the hidden state. The update equation of\nthese elements can be written as follows:\nf\nðtÞ ¼ σðWf xðtÞ þ Uf hðt/C0 1Þ þ bf Þ ð2Þ\niðtÞ ¼ σðWixðtÞ þ Uihðt/C0 1Þ þ biÞ ð3Þ\noðtÞ ¼ σðWoxðtÞ þ Uohðt/C0 1Þ þ boÞ ð4Þ\n~cðtÞ ¼ tanhðWcxðtÞ þ Uchðt/C0 1Þ þ bcÞ ð5Þ\ncðtÞ ¼ fðtÞ /C14 cðt/C0 1Þ þ iðtÞ /C14 ~cðtÞ ð6Þ\nhðtÞ ¼ oðtÞ /C14 tanhðcðtÞÞ ð7Þ\nwhere W and b are the corresponding weight matrices and bias\nvectors. The tanhðvÞ operates piecewise on each element of the\nvector v. The operation∘ is the Hadamard product21.\nThe ﬁnal layer in Fig. 1 is a simple dense layer with fully\nconnected neurons which converts the outputh(t) of the LSTM to\na vector y(t) in which each entry denotes the categorical\nprobability of the representative value for the next time stept\n+ 1. The loss functionJ for minimization during training at every\ntimestep t is then deﬁned as the cross entropy between the output\nof the model^yðtÞ and the actual probability for the next timestep\nyðtÞ which is just the one-hot vectorst+1\n^yðtÞ ¼ softmaxðDdhðtÞ þ bdÞ ð8Þ\nJ ¼/C0\nXT/C0 1\nt¼0\nyðtÞ /C1 ln ^yðtÞ ¼/C0\nXT/C0 1\nt¼0\nsðtþ1Þ /C1 ln ^yðtÞ ð9Þ\nwhere T is the total length of trajectory, and theﬁnal loss function\nis the sum over the whole time series. The softmax ðxÞi ¼\nexpðxiÞ=P\nj expðxjÞ is a softmax function mapping x to a\nprobability vector ^y.\nTraining the network is equivalent to learning path entropy.\nThe central ﬁnding of this work, which we demonstrate through\nnumerical results for different systems, is that a LSTM framework\nused to model languages can also be used to capture kinetic and\nthermodynamic aspects of dynamical trajectories prevalent in\nchemical and biological physics. In this section we demonstrate\ntheoretically as to why LSTMs possess such a capability. Before\nwe get into the mathematical reasoning detailed here, as well as in\nSupplementary Note 1, weﬁrst state our key idea. Minimizing the\nloss functionJ in LSTM (Eq. (9)), which trains the model at timet\nto generate output ^yðtÞ resembling the target output st+1,i s\nequivalent to minimizing the difference between the actual and\nLSTM-learned path probabilities. This difference between path\nprobabilities can be calculated as a cross-entropyJ0 deﬁned as:\nJ0 ¼/C0\nX\nxðTÞ:::xð0Þ\nPðxðTÞ:::xð0ÞÞln QðxðTÞ:::xð0ÞÞ ð10Þ\nwhere P(x(t+1),..., x(0)) and Q(x(t+1),..., x(0)) are the cor-\nresponding true and neural network learned path probabilities of\nthe system. Equation (10) can be rewritten\n22 as the sum of path\nentropy H(P) for the true distribution P and Kullback–Liebler\ndistance DKL between P and Q: J0 ¼ HðPÞþ DKLðPjjQÞ. Since\nDKL is strictly non-negative22 attaining the value of 0 iffQ = P,\nthe global minimum ofJ0 happens whenQ = P and J0 equals the\npath entropy H(P) of the system.23 Thus we claim that mini-\nmizing the loss function in LSTM is equivalent to learning the\npath entropy of the underlying physical model, which is what\nmakes it capable of capturing kinetic information of the dyna-\nmical trajectory.\nTo prove this claim we start with rewritingJ in Eq. (9). For a\nlong enough observation periodT or for a very large number of\ntrajectories, J can be expressed as the cross entropy between\nconditional probabilities:\nJ ¼/C0\nXT/C0 1\nt¼0\nX\nxðtþ1Þ\nPðxðtþ1ÞjxðtÞ:::xð0ÞÞ\n´ ln Qðxðtþ1ÞjxðtÞ:::xð0ÞÞ\nð11Þ\nwhere P(x(t+1)∣x(t)... x(0)) is the true conditional probability for\nthe physical system, andQ(x(t+1)∣x(t)... x(0)) is the conditional\nprobability learned by the neural network. The minimization of\nEq. (11) leads to minimization of the cross entropyJ0 as shown in\nthe SI. Here we conversely show how Eq. (10) reduces to Eq. (9)\nby assuming a stationaryﬁrst-order Markov process as in ref.23:\nPðxðTÞ:::xð0ÞÞ¼ PðxðTÞjxðT/C0 1ÞÞ:::Pðxð1Þjxð0ÞÞPðxð0ÞÞ\nQðxðTÞ:::xð0ÞÞ¼ QðxðTÞjxðT/C0 1ÞÞ:::Qðxð1Þjxð0ÞÞQðxð0ÞÞ\nð12Þ\nwhere Pðxðtþ1Þ\nj jxðtÞ\ni Þ/C17 Pij is the transition probability from state\nxi to statexj and Pðxð0Þ\nk Þ/C17 Pk is the occupation probability for the\nsingle statexk. Plugging Eq. (12) into Eq. (10), and following the\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8 ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications 3\nderivation in ref.23 with the constraintsX\nj\nPij ¼ 1\nX\ni\nPiPij ¼ Pj ð13Þ\nwe arrive at an expression for the cross-entropyJ, which is very\nsimilar to the path entropy type expressions derived for instance\nin the framework of Maximum Caliber23:\nJ0 ¼/C0\nX\ni\nPiln Qi /C0 T\nX\nlm\nPlPlmln ðQlmÞ ð14Þ\n!/C0 T\nX\nlm\nPðxlÞPðxmjxlÞln QðxmjxlÞ ð15Þ\nIn Eq. (14) as the trajectory lengthT increases, the second term\ndominates in the estimate ofJ leading to Eq. (15). This second\nterm is the ensemble average of a time-dependent quantity\n~JðxðtÞ\nl Þ/C17/C0 P\nmPðxðtþ1Þ\nm jxðtÞ\nl Þln Qðxðtþ1Þ\nm jxðtÞ\nl Þ. For a large enough\nT, the ensemble average can be replaced by the time average. By\nassuming ergodicity24:\nJ0 ¼/C0\nXT\nt¼1\nX\nm\nPðxðtþ1Þ\nm jxðtÞ\nl Þln Qðxðtþ1Þ\nm jxðtÞ\nl Þ ð16Þ\nfrom which we directly obtain Eq. (9). Therefore, under ﬁrst-\norder Markovianity and ergodicity, minimizing the loss functionJ\nof Eq. (9) is equivalent to minimizingJ0 and thereby learning the\npath entropy. In the SI we provide a proof for this statement that\nlifts the Markovianity assumption as well— the central idea there\nis similar to what we showed here.\nEmbedding layer captures kinetic distances. In word embedding\ntheory, the embedding layer provides a measure of similarity\nbetween words. However, from the path probability representa-\ntion, it is unclear how the embedding layer works since the\nderivation can be done without embedding vectorsx. To have an\nunderstanding to Qlm in the ﬁrst-order Markov process, weﬁrst\nwrite the conditional probability Qlm ¼ Qðxðtþ1Þ\nm jxðtÞ\nl Þ explicitly\nwith softmax deﬁned in Eq. (8) and embedding vectorsx deﬁned\nin Eq. (1):\nQlm ¼ expðsðtþ1Þ\nm /C1ð DdhðtÞ þ bdÞÞP\nk expðsk /C1ð DdhðtÞ þ bdÞÞ\n¼ expðsðtþ1Þ\nm /C1ð Ddf θðxðtÞÞþ bdÞÞP\nk expðsk /C1ð Ddf θðxðtÞÞþ bdÞÞ\nð17Þ\nwhere f is the recursive function h(t) = fθ(x(t), h(t−1)) ≈ fθ(x(t))\nwhich is deﬁned with the update equation in Eq. (2)–(7). In Eq.\n(17), θ denotes various parameters including all weight matrices\nand biases, and the summation index k runs over all possible\nstates. Now we can use multivariable Taylor ’s theorem to\napproximate fθ as the linear term around a pointa as long asa is\nnot at any local minimum offθ:\nf θðxðtÞÞ/C25 f θðaÞþ AθðxðtÞ /C0 aÞ ð18Þ\nwhere Aθ is the L by M matrix deﬁned to beðAθÞij ¼ ∂ðf θÞi\n∂xj\njx¼a.\nThen Eq. (17) becomes\nQlm ¼ expðCðtþ1Þ\nm Þ expðsðtþ1Þ\nm /C1 DdAθxðtÞ\nl Þ\nP\nk expðCkÞ expðsk /C1 DdAθxðtÞ\nl Þ\nð19Þ\nwhere Cðtþ1Þ\ni ¼ sðtþ1Þ\ni /C1½ Ddðf θðalÞþ AθalÞþ bd/C138 . We can see in\nEq. (19) how the embedding vectors come into the transition\nprobability. Speciﬁcally, there is a symmetric form between out-\nput one-hot vectors sðtþ1Þ\nm and the input one-hot vectorss(t),i n\nwhich x(t) = Λs(t) and Λ is the input embedding matrix,DdAθ can\nbe seen as the output embedding matrix, andCðtþ1Þ\ni is the cor-\nrection of time lag effect. While we do not have an explicit way to\ncalculate the output embedding matrix so de ﬁned, Eq. ( 19)\nmotivates us to deﬁne the following ansatz for the transition\nprobability:\nQlm ¼ QðxmjxlÞ¼ expðxm /C1 xlÞP\nk expðxk /C1 xlÞ ð20Þ\nwhere xm and xl are both calculated by the input embedding\nmatrix Λ. The expression in Eq. (20) is thus a tractable approx-\nimation to the more exact transition probability in Eq. ( 19).\nFurthermore, we show through numerical examples of test sys-\ntems that our ansatz for Qlm does correspond to the kinetic\nconnectivity between states. That is, the LSTM embedding layer\nwith the transition probability through Eq. (20) can capture the\naverage commute time between two states in the original physical\nsystem, irrespective of the quality of low-dimensional projection\nfed to the LSTM25–27.\nTest systems. To demonstrate our ideas, here we consider a range\nof different dynamical trajectories. These include three model\npotentials, the popular model molecule alanine dipeptide, and\ntrajectory from single molecule force spectroscopy experiments\non a multi-state riboswitch.20 The sample trajectories of these test\nsystems and the data preprocessing strategies are put in the\nSupplementary Note 5 and Supplementary Figs. 14–18 When\napplying our neural network to the model systems, the embed-\nding dimension M is set to 8 and LSTM unitL set to 64. When\nlearning trajectories for alanine dipeptide and riboswitch, we took\nM = 128 and L = 1024. All time series were batched into\nsequences with a sequence length of 100 and the batch size of 64.\nFor each model potential, the neural network was trained using\nthe method of stochastic gradient descent for 20 epochs until the\ntraining loss becomes smaller than the validation loss, which\nmeans an appropriate training has been reached. For alanine\ndipeptide, 40 training epochs were used. Our neural network was\nbuilt using TensorFlow version 1.10. Further system details are\nprovided in “Methods” section.\nBoltzmann statistics and kinetics for model potentials. Theﬁrst\ntest we perform for our LSTM set-up is its ability to capture the\nBoltzmann weighted statistics for the different states in each\nmodel potential. This is the probability distribution P or\nequivalently the related free energy F ¼/C0 1\nβ log P, and can be\ncalculated by direct counting from the trajectory. As can be seen\nin Fig. 2, the LSTM does an excellent job of recovering the\nBoltzmann probability within error bars.\nNext we describe our LSTM deals with a well-known problem\nin analyzing high-dimensional data sets through low-dimensional\nprojections. One can project the high-dimensional data along\nmany different possible low-dimensional order parameters, for\ninstance x, y, or a combination thereof in Fig.2. However most\nsuch projections will end up not being kinetically truthful and\ngive a wrong impression of how distant the metastable states\nactually are from each other in the underlying high-dimensional\nspace. It is in general hard to come up with a projection that\npreserves the kinetic properties of the high-dimensional space.\nConsequently, it is hard to design analysis or sampling methods\nthat even when giving a time-series along a sub-optimal\nprojection, still capture the true kinetic distance in the underlying\nhigh-dimensional space.\nHere we show how our LSTM model is agnostic to the quality\nof the low-dimensional projection in capturing accurate kinetics.\nGiven that for each of the 3 potentials the LSTM was provided\nonly the x−trajectory, we can expect that the chosen model\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8\n4 NATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications\npotentials constitute different levels of difﬁculties in generating\ncorrect kinetics. Speciﬁcally, a one-dimensional projection alongx\nis kinetically truthful for the linear 3-state potential in Fig.2a but\nnot for the triangular 3-state and the 4-state potentials in Fig.2b\nand c, respectively. For instance, Fig.2e gives the impression that\nstate C is kinetically very distant from state A, while in reality for\nthis potential all 3 pairs of states are equally close to each other.\nSimilar concerns apply to the 4-state potential.\nIn Figs. 3 and 4a–c and d–f we compare the actual versus\nLSTM-predicted kinetics for moving between different metastable\nstates for different model potentials, for all pairs of transitions in\nboth directions (i.e., for instance A to B and B to A). Speciﬁcally,\nFig. 3a–c and 3d–f shows results for moving between the 3 pairs\nof states in the linear and triangular 3-state potentials,\nrespectively. Figure 4 shows results for the 6 pairs of states in\nthe 4-state potential. Furthermore, for every pair of state, we\nanalyze the transition time between those states as a function of\ndifferent minimum commitment or commit time, i.e., the\nminimum time that must be spent by the trajectory in a given\nstate to be classiﬁed as having committed to it. A limiting value,\nand more speciﬁcally the rate at which the population decays to\nattain to such a limiting value, corresponds to the inverse of the\nrate constant for moving between those states28,29. Thus here we\nshow how our LSTM captures not just the rate constant, but time-\ndependent ﬂuctuations in the population in a given metastable\nstate as equilibrium is attained. The results are averaged over 20\nindependent segments taken from the trajectories of different\ntrials of training for the 3-state potentials and 10 independent\nsegments for the 4-state potential.\nAs can be seen in Figs. 3 and 4, the LSTM model does an\nexcellent job of reproducing well within errorbars the transition\ntimes between different metastable states for different model\npotentials irrespective of the quality of the low-dimensional\nprojection. Firstly, our model does tell the differences between\nlinear and triangular 3-state models (Fig. 3) even though the\nprojected free energies along thex variable input into LSTM are\nsame (Fig.2). The number of transitions between states A and C\nis less than the others; while for triangular conﬁguration, the\nnumbers of transitions between all pairs of states are similar. The\nrates at which the transition count decays as a function of\ncommitment time is also preserved between the input data and\nthe LSTM prediction.\nThe next part of our second test is the 4-state model potential.\nIn Fig. 4 we show comparisons for all 6 pairs of transitions in\nboth forward and reverse directions. A few features are\nimmediately striking here. Firstly, even though states B and C\nare perceived to be kinetically proximal from the free energy\n(Fig. 2), the LSTM captures that they are distal from each other\nand correctly assigns similar kinetic distance to the pairs B, C as it\ndoes to A, D. Secondly, there is asymmetry between the forward\nand backward directions (for e.g., A to D and D to A, indicating\nthat the input trajectory itself has not yet sufﬁciently sampled the\n–0.8\n2\n0\n–2\n0.8\n0.6\n0.4 Free energy0.2\n0.0\n0.4 Probability\n0.2\n0.0\n0.4\n0.2\n0.0\nC\nActual\nLSTM\nActual\nLSTM\nActual\nLSTM\nBA\nCBA CBA\nCBA D B CA\nDB CA\n0.4\n0.2\n0.0\n20\nX\n–2 2 0\nX\n–2 2 4 0\nX\n–2–4\n2\n0\n–2\n2\n4\n0\n–2\n–4\n20\nX\n–2 2 0\nX\n–2 2 4 0\nX\n–2–4\ny\n–0.4 0.0 0.4 0.8 –0.8 –0.4 0.0 0.4 0.8 –0.8 –0.4 0.0 0.4 0.8\nC\nB\nA C A\nB\nD\nB\nC\nA\nabc\nfed\nghi\nFig. 2 Boltzmann statistics for model systems.The analytical free energy generated froma linear 3-state,b triangular 3-state,c symmetric 4-state model\npotentials andd, e, f are the corresponding 1-dimensional projections alongx-direction. In the bottom, we compare the Boltzmann probabilities ofg linear\n3-state, h triangular 3-state, andi symmetric 4-state models for every labeled state generated from actual MD simulation and from our long short-term\nmemory (LSTM) network. The errorbars are calculated as standard errors.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8 ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications 5\nslow transitions in this potential. As can be seen from Fig.2c the\ninput trajectory has barely 1 or 2 direct transitions for the very\nhigh barrier A to D or B to C. This is a likely explanation for why\nour LSTM model does a bit worse than in the other two model\npotentials in capturing the slowest transition rates, as well as the\nhigher error bars we see here. In other words, so far we can\nconclude that while our LSTM model can capture equilibrium\nprobabilities and transition rates for different model potentials\nirrespective of the input projection direction or order parameter,\nit is still not a panacea for insufﬁcient sampling itself, as one\nwould expect.\nBoltzmann statistics and kinetics for alanine dipeptide. Finally,\nwe apply our LSTM model to the study of conformational tran-\nsitions in alanine dipeptide, a model biomolecular system com-\nprising 22 atoms, experiencing thermalﬂuctuations when coupled\nto a heat bath. The structure of alanine dipeptide is shown in\nFig. 5a. While the full system comprises around 63 degrees of\nfreedom, typically the torsional angles ϕ and ψ are used to\nidentify the conformations of this peptide. Over the years a large\nnumber of methods have been tested on this system in order to\nperform enhanced sampling of these torsions, as well as to con-\nstruct optimal reaction coordinates30–33. Here we show that our\nLSTM model can very accurately capture the correct Boltzmann\nstatistics, as well as transition rates for moving between the two\ndominant metastable states known asC7eq and C7ax. Importantly,\nthe reconstruction of the equilibrium probability and transition\nkinetics, as shown in Fig. 5 and Table 1 is extremely accurate\nirrespective of the choice of one-dimensional projection time\nseries fed into the LSTM. Speciﬁcally, we do this along sinϕ and\nsin ψ, both of which are known to quite distant from an opti-\nmized kinetically truthful reaction coordinate19,34, where again\nwe have excellent agreement between input and LSTM-predicted\nresults.\nLearning from single molecule force spectroscopy trajectory.I n\nthis section, we use our LSTM model to learn from single\nmolecule force spectroscopy experiments of a multi-state ribos-\nwitch performed with a constant force of 10.9 pN. The data points\nare measured at 10 kHz (i.e., every 100μs). Other details of the\nexperiments can be found in ref.20. The trajectory for a wide\nrange of extensions starting 685 nm up to 735 nm wasﬁrst spa-\ntially discretized into 34 labels, and then converted to a time\nseries of one hot vectors, before being fed into the LSTM model.\nThe results are shown in Fig.6. In Fig. 6a, we have shown an\nagreement between a proﬁle of probability density averaged over\n5 independent training sets with the probability density calculated\nfrom the experimental data. Starting from the highest extension,\nthe states are fully unfolded (U), longer intermediate (P3) and\nshorter intermediate (P2P3)20. From Fig. 6b–c, we see that the\nLSTM model captures the kinetics for moving between all 3 pairs\nof states for a very wide range of commitment times.\n28 ad\nbe\ncf\nActual: A to B\nActual: B to A\nLSTM: A to B\nLSTM: B to A\nActual: A to C\nActual: C to A\nLSTM: A to C\nLSTM: C to A\nActual: B to C\nActual: C to B\nLSTM: B to C\nLSTM: C to B\nActual: A to B\nActual: B to A\nLSTM: A to B\nLSTM: B to A\nActual: A to C\nActual: C to A\nLSTM: A to C\nLSTM: C to A\nActual: B to C\nActual: C to B\nLSTM: B to C\nLSTM: C to B\n24\n20\n16\n12\n10\n8\n6\n4\n2\n0\n10\n8\n6\n4\n2\n0\n10\n8\n6\n4\n2\n0\n8\n4\n0\n0 100 200 300 0 500 1000 1500 2000\n0 100 200\nCommit time Commit time\n300 0 500 1000 1500 2000\n0 100 200 300 0 500 1000 1500 2000\n28\n24\n20\n16\n12\n8\n4\n0\n28\n24\n20\n16\n12\n8\n4\n0\nCountCountCount\nFig. 3 Kinetics for 3-state model systems.Number of transitions between different pairs of metastable states as a function of commitment time deﬁned in\n“Results” section. The calculations for linear and triangular conﬁgurations are shown ina–c and d–f, respectively. Error bars are illustrated and were\ncalculated as standard errors.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8\n6 NATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications\nEmbedding layer based kinetic distance. In Eq. (19), we derived\na non-tractable relation for conditional transition probability in\nthe embedding layer, and then through Eq. (20) we introduced a\ntractable ansatz in the spirit of Eq. (19). Here we revisit and\nnumerically validate Eq. ( 20). Speci ﬁcally, given any two\nembedding vectorsxl and xm calculated from any two statesl and\nm, we estimate the conditional probabilityQlm using Eq. (20). We\nuse Qi to denotes the Boltzmann probability predicted by the\nLSTM model. We then write down the interconversion prob-\nability klm between states l and m as:\nklm ¼ QlQlm þ QmQml /C17 1=tlm ð21Þ\nFrom inverting this rate we then calculate an LSTM-kinetic time\nas tlm ≡ 1/klm = 1/(QlQlm + QmQml). In Fig. 7, we compare tlm\nwith the actual transition timeτlm obtained from the input data,\ndeﬁned as\nτlm ¼ T=hNlmið 22Þ\nHere Nlm is the mean number of transitions between statel and\nm. As this number varies with the precise value of commitment\ntime, we averageNlm over all commit times to get〈Nlm〉. These\ntwo timescales tlm and τlm thus represent the average commute\ntime or kinetic distance25,26 between two states l and m.T o\nfacilitate the comparison between these two very differently\nderived timescales or kinetic distances, we rescale and shift them\nto lie between 0 and 1. The results in Fig. 7 show that the\nembedding vectors display the connectivity corresponding to the\noriginal high-dimensional conﬁguration space rather than those\ncorresponding to the one-dimensional projection. The model\ncaptures the correct connectivity by learning kinetics, which is\nclear evidence that it is able to bypass the projection error along\nany degree of freedom. The result also explains how is it that no\nmatter what degree of freedom we use, our LSTM model still\ngives correct transition times. As long as the degree of freedom we\nchoose to train the model can be used to discern all metastable\nstates, we can even use Eq. (20) to see the underlying connectivity.\nTherefore, the embedding vectors in LSTM can deﬁne a useful\ndistance metric which can be used to understand and model\ndynamics, and are possibly part of the reason why LSTMs can\nmodel kinetics accurately inspite of quality of projection and\nassociated non-Markvoian effects.\nComparing with Markov state model and Hidden Markov\nModel. In this section, we brieﬂy compare our LSTM model with\n24Count\n21\n18\n5\n4\n3\n2\n1\n0\n24 Actual: A to Ba\nc\nb\nd\nef\nActual: B to A\nLSTM: A to B\nLSTM: B to A\nActual: A to D\nActual: D to A\nLSTM: A to D\nLSTM: D to A\nActual: B to D\nActual: D to B\nLSTM: B to D\nLSTM: D to B\nActual: C to D\nActual: D to C\nLSTM: C to D\nLSTM: D to C\nActual: B to C\nActual: C to B\nLSTM: B to C\nLSTM: C to B\nActual: A to C\nActual: C to A\nLSTM: A to C\nLSTM: C to A\nCount\n21\n18\n5\n4\n3\n2\n1\n0\n24Count\n21\n18\n15\n12\n9\n6\n3\n0\n24\n21\n18\n15\n12\n9\n6\n3\n0\n24\n21\n18\n5\n4\n3\n2\n1\n0\n24\n21\n18\n5\n4\n3\n2\n1\n0\n0 500 1000 1500\n0 500 1000 1500\n0 500\nCommit time Commit time\n1000 1500\n0 500 1000 1500\n0 500 1000 1500\n0 500 1000 1500\nFig. 4 Kinetics for 4-state model system.Number of transitions between different pairs of metastable states as a function of commitment time deﬁned in\n“Results” section for 4-state model system. Error bars are illustrated and were calculated as standard errors.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8 ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications 7\nstandard approaches for building kinetic models from trajec-\ntories, namely the Markov state model (MSM)35 and Hidden\nMarkov model (HMM)36–38. Compared to LSTM, the MSM and\nHMM have smaller number of parameters, making them faster\nand more stable for simpler systems. However, both MSM and\nHMM require choosing an appropriate number of states and lag\ntime\n35,38,39. Large number of pre-selected states or small lag time\ncan lead to non-Markovian behavior and result in an incorrect\nprediction. Even more critically, choosing a large lag time also\nsacriﬁces the temporal precision. On the other hand, there is no\nneed to determine the lag time and number of states using the\nLSTM network because LSTM does not rely on the Markov\nproperty. Choosing hyperparameters such as M and L may be\ncomparable to choosing number of hidden states for HMM, while\nvery similar values ofM and L worked for systems as different as\nMD trajectory of alanine dipeptide and single molecule force\nspectroscopy trajectory of a riboswitch. At the same time, LSTM\nalways generates the data points with the same temporal precision\nas it has in the training data irrespective of the intrinsic timescales\nit learns from the system. In Fig.8, we provide the results of using\nHMM and MSM for the riboswitch trajectory with the same\nbinning method and one-hot encoded input, to be contrasted\nwith similar plots using LSTM in Fig.6. Indeed both MSM and\nHMM achieve decent agreement with the true kinetics only if the\ncommit time is increased approximately beyond 10 ms, while\nLSTM as shown in Fig. 6 achieved perfect agreement for all\ncommit times. From this ﬁgure, it can be seen that the LSTM\nmodel achieves an expected agreement with asﬁne of a temporal\nprecision as desired, even though we use 20 labels for alanine\ndipeptide and 34 labels for experimental data to represent the\nstates. The computational efforts needed for the various\napproaches (LSTM, MSM, and HMM) are also provided in the\nSupplementary Note 3 and Supplementary Table 2–3, where it\ncan be seen that LSTM takes similar amount of effort as HMM.\nThe package we used to build the MSM and HMM is PyEMMA\nwith version 2.5.6\n40. The models were built with lag time= 0.5 ms\nfor MSM and lag time= 3 ms for HMM, where the HMM were\nbuilt with number of hidden states= 3. A more careful com-\nparison of the results along with analyses with other parameter\nchoices such as different number of hidden states for HMM are\nprovided in the Supplementary Note 4 and Supplementary\nFigs. 1–13, where weﬁnd all of these trends to persist.\nDiscussion\nIn summary we believe this work demonstrates potential for\nusing AI approaches developed for natural language processing\nsuch as speech recognition and machine translation, in unrelated\ndomains such as chemical and biological physics. This work\nrepresents a ﬁrst step in this direction, wherein we used AI,\nspeciﬁcally LSTMﬂavor of recurrent neural networks, to perform\nkinetic reconstruction tasks that other methods41,42 could have\nalso performed. We would like to argue that demonstrating the\nability of AI approaches to perform tasks that one could have\ndone otherwise is a crucial ﬁrst step. In future works we will\nexploring different directions in which the AI protocol developed\nhere could be used to perform tasks which were increasingly non-\ntrivial in non-AI setups. More speciﬁcally, in this work we have\nshown that a simple character-level language model based on\nLSTM neural network can learn a probabilistic model of a time\nseries generated from a physical system such as an evolution of\nLangevin dynamics or MD simulation of complex molecular\nmodels. We show that the probabilistic model can not only learn\nthe Boltzmann statistics but also capture a large spectrum of\nkinetics. The embedding layer which is designed for encoding the\ncontextual meaning of words and characters displays a nontrivial\nconnectivity and has been shown to correlate with the kinetic\nmap deﬁned for reversible Markov chains25,26,43. An interesting\nfuture line of work for the embedding layer can be to uncover\ndifferent states when they are incorrectly represented by the same\nreaction coordinate value, which is similar to ﬁnding different\ncontextual meaning of the same word or character. For different\nmodel systems considered here, we could obtain correct time-\nscales and rate constants irrespective of the quality of order\nparameter fed into the LSTM. As a result, we believe this kind of\nmodel outperforms traditional approaches for learning thermo-\ndynamics and kinetics, which can often be very sensitive to the\nchoice of projection. Finally, the embedding layer can be used to\ndeﬁne a new type of distance metric for high-dimensional data\nwhen one has access to only some low-dimensional projection.\nWe hope that this work represents aﬁrst step in the use of RNNs\nfor modeling, understanding and predicting the dynamics of\ncomplex stochastic systems found in biology, chemistry and\nphysics.\nMethods\nModel potential details. All model potentials have two degrees of freedomx and\ny. Ourﬁrst two models (shown in Fig.2a and b) have three metastable states with\ngoverning potential U(x, y) given by\nUðx; yÞ¼ Wðx6 þ y6Þ/C0 Gðx; x1ÞGðy; y1Þ\n/C0 Gðx; x2ÞGðy; y2Þ/C0 Gðx; x3ÞGðy; y3Þ ð23Þ\n30 20\n16\n12\n8\n4\n0\na\n24\n18\nC7eq\nC7eq\nC7ax C7ax\n12\n6\n0\n–1 0\nsin/p10\n1 –1 0\nsin/afii9820\n1\nactual\nLSTM\nactual\nLSTM\nFree energy (kJ/mol)\n/p10 /afii9820\nC7eq\n’\nC7eq\n’\nbc\nFig. 5 Boltzmann statistics for alanine dipeptide. aThe molecular structure\nof alanine dipeptide used in the actual MD simulation. The torsional anglesϕ\nand ψ as the collective variables (CVs) are shown.b and c The 1-dimensional\nfree energy curves along sinϕ and sinψ are calculated using actual MD data\nand the data generated from LSTM. For the calculation of a different epoch,\nplease see Supplementary Note 2 and Supplementary Table 1.\nTable 1 Kinetics for alanine dipeptide.\nAlanine dipeptide\nCVs Label C7eq to C7ax (ps) C7ax to C7eq (ps)\nsin ϕ Actual 5689.22 ± 962.366 107.93 ± 11.267\nLSTM 5752.16 ± 710.399 103.81 ± 14.268\nsin ψ Actual 5001.42 ± 643.943 105.70 ± 13.521\nLSTM 4325.01 ± 526.293 81.68 ± 10.288\nInverse of transition rates for conformational transitions in alanine dipetide calculated from\nactual MD trajectories of LSTM model. Here we show the calculation along two different CVs:\nsin ϕ and sinψ.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8\n8 NATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications\nwhere W = 0.0001 andGðx; x0Þ¼ e/C0 ðx/C0 x0 Þ2\n2σ2 denotes a Gaussian function centered at\nx0 with width σ = 0.8. We also build a 4-state model system with governing\ninteraction potential:\nUðx; yÞ¼ Wðx4 þ y4Þþ Gðx; 0:0ÞGðy; 0:0Þ\n/C0 Gðx; 2:0ÞGðy; /C0 1:0Þ/C0 Gðx; 0:5ÞGðy; 2:0Þ\n/C0 Gðx; /C0 0:5ÞGðy; /C0 2:0Þ/C0 Gðx; /C0 2:0ÞGðy; 1:0Þ\nð24Þ\nThe different local minima corresponding to the model potentials in Eq. (23) and\nEq. (24) are illustrated in Fig.2. We call these as linear 3-state, triangular 3-state,\nand 4-state models, respectively. The free energy surfaces generated from the\nsimulation of Langevin dynamics44 with these model potentials are shown in\nFig. 2a–c.\nMolecular dynamics details. The integration timestep for the Langevin\ndynamics simulation was 0.01 units, and the simulation was performed atβ =\n9.5 for linear 3-state and 4-state potentials andβ = 9.0 for triangular 3-state\npotential, where β = 1/kBT. The MD trajectory for alanine dipeptide was\nobtained using the software GROMACS 5.0.445,46, patched with PLUMED 2.447.\nThe temperature was kept constant at 450 K using the velocity rescaling\nthermostat 48.\n0.25\n480\n420\n360\n300\n240\n180 Count\n120\n60\n0\n01 0\nCommit time (ms)\nExp: P2P3 to P3\nExp: P3 to P2P3\nLSTM: P2P3 to P3\nLSTM: P3 to P2P3\nExp: P2P3 to U\nExp: U to P2P3\nLSTM: P2P3 to U\nLSTM: U to P2P3\nExp: P3 to U\nExp: U to P3\nLSTM: P3 to U\nLSTM: U to P3\nCommit time (ms) Commit time (ms)\n20 30 0 10 20 30 0 10 20 30\n0.20\nP3\nP2P3 U\nExperiment\nLSTM\n0.15\nProbability density\n0.10\n0.05\n0.00\n695 700 705 710\nExtension (nm)\n715 720 725\na\nbcd\nFig. 6 Boltzmann statistics and kinetics for riboswitch.Using LSTM model to learn thermodynamics and kinetics from a folding and unfolding trajectory\ntaken from a single molecule force spectroscopy measurement20: a Comparison between the probability density learned by the LSTM model and\ncalculated from the experimental data. The regions between errorbars deﬁned as standard errors areﬁlled with blue color.b–d Commit time plots\ncalculated by counting the transitions in the trajectory generated by LSTM and the experimental trajectory. The commit time is the minimum time that\nmust be spent by the trajectory in a given state to be classiﬁed as having committed to it. Error bars are illustrated and were calculated as standard errors.\n1.4 1.00\n0.75\n0.50\n0.25\n0.00\n1.2\n1.0\nactual\nactual\nactual\nLSTM\nLSTM\nLSTM\n0.8\n0.6\n0.4\n0.2\n0.0\n–0.2Rescaled distances\n–0.4\nAC\nSolid/empty markers: linear/triangular 4-state model system\nAB BC ACAB BC AD BD CD\nab\nFig. 7 Analysis of embedding layers for model systems.Our analysis of the embedding layer constructed fora the linear and triangular 3-state andb the\n4-state model systems. Ina, we use solid circle and empty square markers, respectively to represent linear and triangular 3-state model potentials. In each\nplot, the data points are shifted slightly to the right for clarity. The distances marked actual and LSTM represent rescaled mean transition times as per Eqs.\n(22) and (21), respectively. Error bars were calculated as standard errors over 50 different trajectories.\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8 ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications 9\nData availability\nThe single-molecule force spectroscopy experiment data for riboswitch was obtained\nfrom the authors of ref.20 and they can be contacted for the same. All the other data\nassociated with this work is available from the corresponding author on request.\nCode availability\nMSM and HMM analyses were conducted with PyEMMA version 2.5.6.40 and available\nat http://www.pyemma.org. A Python based code of the LSTM language model is\nimplemented using keras (https://keras.io/) with tensorﬂow-gpu (https://www.\ntensorﬂow.org/) as a backend, and available for public use athttps://github.com/\ntiwarylab/LSTM-predict-MD.\nReceived: 4 May 2020; Accepted: 23 September 2020;\nReferences\n1. Rico-Martinez, R., Krischer, K., Kevrekidis, I., Kube, M. & Hudson, J.\nDiscrete-vs. continuous-time nonlinear signal processing of cu\nelectrodissolution data. Chem. Engg. Commun.118,2 5–48 (1992).\n2. Gicquel, N., Anderson, J. & Kevrekidis, I. Noninvertibility and resonance in\ndiscrete-time neural networks for time-series processing.Phys. Lett. A238,\n8–18 (1998).\n3. Graves, A., Liwicki, M., Fernández, S., Bertolami, R. & Bunke, H. A novel\nconnectionist system for unconstrained handwriting recognition.IEEE Trans.\nPattern. Anal. Mach. Intell.31, 855–868 (2008).\n4. Graves, A., Mohamed, A.-r. & Hinton, G. Speech recognition with deep\nrecurrent neural networks. InInternational Conference on Acoustics, Speech,\nand Signal Processing.6645–6649 (2013).\n5. Cho, K., Van Merriënboer, B., Gulcehre, C., Bougares, F., Schwenk,\nH., Bahdanau, D. & Bengio, Y. Learning phrase representations using rnn\nencoder-decoder for statistical machine translation. InProceedings of the 2014\nConference on Empirical Methods in Natural Language Processing (EMNLP).\n1724–1734 (2014).\n6. Xingjian, S., Chen, Z., Wang, H. & Woo, W.-c. Convolutional lstm network: a\nmachine learning approach for precipitation nowcasting. InAdvances in\nNeural Information Processing Systems. 802–810 (2015).\n7. Chen, K., Zhou, Y. & Dai, F. A LSTM-based method for stock returns\nprediction: a case study of china stock market. InIEEE International\nConference on Big Data. 2823–2824 (2015).\n8. Hochreiter, S. & Schmidhuber, J. Long short-term memory.Neur. Comp. 9,\n1735–1780 (1997).\n9. Sundermeyer, M., Schlüter, R. & Ney, H. LSTM neural networks for language\nmodeling. In Thirteenth Annual Conference of the International Speech\nCommunication Association. (2012).\n10. Luong, M.-T., Sutskever, I., Le, Q. V., Vinyals, O. & Zaremba, W. Addressing\nthe rare word problem in neural machine translation. InProceedings of the 53rd\nAnnual Meeting of the Association for Computational Linguistics and the\n7th International Joint Conference on Natural Language Processing.11–19\n(2014).\n480\nMSM HMM\nad\nbe\ncf\n420\n360\n300\n240\n180Count\n120\n60\n0\n01 0 2 0 3 0 01 0 2 0 30\n01 0 2 0 3 0 01 0 2 0 3 0\n01 0 2 0 3 0 01 0 2 0 30\n480\n420\n360\n300\n240\n180Count\n120\n60\n0\n480\n420\n360\n300\n240\n180Count\n120\n60\n0\nCommit time (ms) Commit time (ms)\nExp: P2P3 to P3\nExp: P3 to P2P3\nMSM: P2P3 to P3\nMSM: P3 to P2P3\nExp: P2P3 to U\nExp: U to P2P3\nMSM: P2P3 to U\nMSM: U to P2P3\nExp: P3 to U\nExp: U to P3\nMSM: P3 to U\nMSM: U to P3\nExp: P3 to U\nExp: U to P3\nHMM: P3 to U\nHMM: U to P3\nExp: P2P3 to P3\nExp: P3 to P2P3\nHMM: P2P3 to P3\nHMM: P3 to P2P3\nExp: P2P3 to U\nExp: U to P2P3\nHMM: P2P3 to U\nHMM: U to P2P3\nFig. 8 Riboswitch kinetics through alternate approaches.Number of transitions between different pairs of metastable states as a function of commitment\ntime deﬁned in“Results” section for the single molecule spectroscopy trajectory as learned by MSM (left column) and HMM (right column). Associated\nerror bars calculated as standard errors are also provided.\nARTICLE NATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8\n10 NATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications\n11. Hochreiter, S. et al. Gradientﬂow in recurrent nets: the difﬁculty of learning\nlong-term dependencies. (2001).\n12. Agar, J. C., Naul, B., Pandya, S. & van Der Walt, S. Revealing ferroelectric\nswitching character using deep recurrent neural networks.Nat. Commun.10,\n1–11 (2019).\n13. Eslamibidgoli, M. J., Mokhtari, M. & Eikerling, M. H. Recurrent neural\nnetwork-based model for accelerated trajectory analysis in aimd simulations.\nPreprint at https://arxiv.org/abs/1909.10124 (2019).\n14. Luko ševičius, M. & Jaeger, H. Reservoir computing approaches to recurrent\nneural network training.Comp. Sci. Rev.3, 127–149 (2009).\n15. Pathak, J., Hunt, B., Girvan, M., Lu, Z. & Ott, E. Model-free prediction of large\nspatiotemporally chaotic systems from data: A reservoir computing approach.\nPhys. Rev. Lett.120, 024102 (2018).\n16. Noé, F., Olsson, S., Köhler, J. & Wu, H. Boltzmann generators: sampling\nequilibrium states of many-body systems with deep learning.Science 365,\neaaw1147 (2019).\n17. Sidky, H., Chen, W. & Ferguson, A. L. Molecular latent space simulators.\nChem. Sci. 11, 9459–9467 (2020).\n18. Bussi, G. & Laio, A. Using metadynamics to explore complex free-energy\nlandscapes. Nat. Rev. Phys.2, 200–212 (2020).\n19. Wang, Y., Ribeiro, J. M. L. & Tiwary, P. Past–future information bottleneck\nfor sampling molecular reaction coordinate simultaneously with\nthermodynamics and kinetics.Nat. Commun. 10,1 –8 (2019).\n20. Neupane, K., Yu, H., Foster, D. A., Wang, F. & Woodside, M. T. Single-\nmolecule force spectroscopy of the add adenine riboswitch relates folding to\nregulatory mechanism. Nucl. Acid. Res.39, 7677–7687 (2011).\n21. Goodfellow, I., Bengio, Y. & Courville, A.Deep Learning (MIT press, 2016).\n22. Cover, T. M. & Thomas, J. A.Elements of Information Theory(John Wiley &\nSons, 2012).\n23. Pressé, S., Ghosh, K., Lee, J. & Dill, K. A. Principles of maximum entropy and\nmaximum caliber in statistical physics.Rev. Mod. Phys.85, 1115 (2013).\n24. Moore, C. C. Ergodic theorem, ergodic theory, and statistical mechanics.Proc.\nNatl Acad. Sci. USA112, 1907–1911 (2015).\n25. Noe, F., Banisch, R. & Clementi, C. Commute maps: separating slowly mixing\nmolecular conﬁgurations for kinetic modeling.J. Chem. Theor. Comp.12,\n5620–5630 (2016).\n26. Noé, F. & Clementi, C. Kinetic distance and kinetic maps from molecular\ndynamics simulation.\nJ. Chem. Theor. Comp.11, 5002–5011 (2015).\n27. Tsai, S.-T. & Tiwary, P. On the distance between A and B in molecular\nconﬁguration space. Mol. Sim. 46,1 –8 (2020).\n28. Hänggi, P., Talkner, P. & Borkovec, M. Reaction-rate theory:ﬁfty years after\nkramers. Rev. Mod. Phys.62, 251 (1990).\n29. Berne, B. J., Borkovec, M. & Straub, J. E. Classical and modern methods in\nreaction rate theory.J. Phys. Chem.92, 3711–3725 (1988).\n30. Valsson, O., Tiwary, P. & Parrinello, M. Enhancing importantﬂuctuations:\nrare events and metadynamics from a conceptual viewpoint.Ann. Rev. Phys.\nChem. 67, 159–184 (2016).\n31. Salvalaglio, M., Tiwary, P. & Parrinello, M. Assessing the reliability of the\ndynamics reconstructed from metadynamics.J. Chem. Theor. Comp.10,\n1420–1425 (2014).\n32. Ma, A. & Dinner, A. R. Automatic method for identifying reaction coordinates\nin complex systems.J. Phys. Chem. B109, 6769–6779 (2005).\n33. Bolhuis, P. G., Dellago, C. & Chandler, D. Reaction coordinates of\nbiomolecular isomerization. Proc. Natl Acad. Sci. USA97, 5877–5882 (2000).\n34. Smith, Z., Pramanik, D., Tsai, S.-T. & Tiwary, P. Multi-dimensional spectral\ngap optimization of order parameters (sgoop) through conditional probability\nfactorization. J. Chem. Phys.149, 234105 (2018).\n35. Husic, B. E. & Pande, V. S. Markov state models: from an art to a science.J.\nAm. Chem. Soc.140, 2386–2396 (2018).\n36. Eddy, S. R. What is a hidden markov model?Nat. Biotechnol. 22, 1315–1316\n(2004).\n37. McKinney, S. A., Joo, C. & Ha, T. Analysis of single-molecule fret trajectories\nusing hidden markov modeling.Bioph. Jour. 91, 1941–1951 (2006).\n38. Blanco, M. & Walter, N. G. Analysis of complex single-molecule fret\ntime trajectories. InMethods in Enzymology, Vol. 472, 153–178 (Elsevier,\n2010).\n39. Bowman, G. R., Beauchamp, K. A., Boxer, G. & Pande, V. S. Progress and\nchallenges in the automated construction of markov state models for full\nprotein systems. J. Chem. Phys.131, 124101 (2009).\n40. Scherer, M. K. et al. Pyemma 2: a software package for estimation, validation,\nand analysis of markov models.J. Chem. Theor. Comp.11, 5525–5542 (2015).\n41. Pérez-Hernández, G., Paul, F., Giorgino, T., De Fabritiis, G. & Noé, F.\nIdentiﬁcation of slow molecular order parameters for markov model\nconstruction. J. Chem. Phys.139, 07B604_1 (2013).\n42. Chodera, J. D. & Noé, F. Markov state models of biomolecular conformational\ndynamics. Curr. Op. Struc. Bio. y.25, 135–144 (2014).\n43. Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S. & Dean, J. Distributed\nrepresentations of words and phrases and their compositionality. InAdvances\nin Neural Information Processing Systems.3111–3119 (2013).\n44. Bussi, G. & Parrinello, M. Accurate sampling using langevin dynamics.Phys.\nRev. E 75, 056707 (2007).\n45. Berendsen, H. J., van der Spoel, D. & van Drunen, R. Gromacs: a message-\npassing parallel molecular dynamics implementation.Comp. Phys. Commun.\n91,4 3–56 (1995).\n46. Abraham, M. J. et al. Gromacs: high performance molecular simulations\nthrough multi-level parallelism from laptops to supercomputers.SoftwareX 1,\n19–25 (2015).\n47. Bonomi, M., Bussi, G. & Camilloni, C. C. Promoting transparency and\nreproducibility in enhanced molecular simulations.Nat. Methods16, 670–673\n(2019).\n48. Bussi, G., Donadio, D. & Parrinello, M. Canonical sampling through velocity\nrescaling. J. Chem. Phys.126, 014101 (2007).\nAcknowledgements\nP.T. thanks Dr. Steve Demers for suggesting the use of LSTMs. The authors thank Carlos\nCuellar for the help in early stages of this project, Michael Woodside for sharing the\nsingle molecule trajectory with us, Yihang Wang for in-depth discussions, Dedi Wang,\nYixu Wang, Zachary Smith for their helpful insights and suggestions. Acknowledgment is\nmade to the Donors of the American Chemical Society Petroleum Research Fund for\npartial support of this research (PRF 60512-DNI6). We also thank Deepthought2,\nMARCC and XSEDE (projects CHE180007P and CHE180027P) for computational\nresources used in this work.\nAuthor contributions\nP.T., S.T., and E.K. designed research; P.T., S.T., and E.K. performed research; S.T.\nanalyzed data; S.T. and P.T. wrote the paper.\nCompeting interests\nThe authors declare no competing interests.\nAdditional information\nSupplementary informationis available for this paper athttps://doi.org/10.1038/s41467-\n020-18959-8.\nCorrespondence and requests for materials should be addressed to P.T.\nPeer review informationNature Communications thanks Simon Olsson and the other\nanonymous reviewer(s) for their contribution to the peer review of this work.\nReprints and permission informationis available athttp://www.nature.com/reprints\nPublisher’s noteSpringer Nature remains neutral with regard to jurisdictional claims in\npublished maps and institutional afﬁliations.\nOpen Access This article is licensed under a Creative Commons\nAttribution 4.0 International License, which permits use, sharing,\nadaptation, distribution and reproduction in any medium or format, as long as you give\nappropriate credit to the original author(s) and the source, provide a link to the Creative\nCommons license, and indicate if changes were made. The images or other third party\nmaterial in this article are included in the article’s Creative Commons license, unless\nindicated otherwise in a credit line to the material. If material is not included in the\narticle’s Creative Commons license and your intended use is not permitted by statutory\nregulation or exceeds the permitted use, you will need to obtain permission directly from\nthe copyright holder. To view a copy of this license, visithttp://creativecommons.org/\nlicenses/by/4.0/.\n© The Author(s) 2020\nNATURE COMMUNICATIONS | https://doi.org/10.1038/s41467-020-18959-8 ARTICLE\nNATURE COMMUNICATIONS|         (2020) 11:5115 | https://doi.org/10.1038/s41467-020-18959-8 | www.nature.com/naturecommunications 11"
}