{
  "title": "Mitigating the risk of health inequity exacerbated by large language models",
  "url": "https://openalex.org/W4410052828",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A4313279361",
      "name": "Yuelyu Ji",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A3200094056",
      "name": "Wenhe Ma",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A4282413341",
      "name": "Sonish Sivarajkumar",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A1984332501",
      "name": "Hang Zhang",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A4380618390",
      "name": "Eugene M. Sadhu",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A4213825000",
      "name": "Zhuochun Li",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2098579296",
      "name": "Xizhi Wu",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A1108061751",
      "name": "Shyam Visweswaran",
      "affiliations": [
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A2156867267",
      "name": "YanShan Wang",
      "affiliations": [
        "University of Pittsburgh Medical Center",
        "University of Pittsburgh"
      ]
    },
    {
      "id": "https://openalex.org/A4313279361",
      "name": "Yuelyu Ji",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3200094056",
      "name": "Wenhe Ma",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4282413341",
      "name": "Sonish Sivarajkumar",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1984332501",
      "name": "Hang Zhang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4380618390",
      "name": "Eugene M. Sadhu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4213825000",
      "name": "Zhuochun Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2098579296",
      "name": "Xizhi Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1108061751",
      "name": "Shyam Visweswaran",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2156867267",
      "name": "YanShan Wang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4388759569",
    "https://openalex.org/W4402827393",
    "https://openalex.org/W4404486407",
    "https://openalex.org/W3162922479",
    "https://openalex.org/W3173904029",
    "https://openalex.org/W6609311119",
    "https://openalex.org/W4384071683",
    "https://openalex.org/W4406152279",
    "https://openalex.org/W4392044798",
    "https://openalex.org/W4391750754",
    "https://openalex.org/W4390862266",
    "https://openalex.org/W4401079252",
    "https://openalex.org/W2469208519",
    "https://openalex.org/W2898155085",
    "https://openalex.org/W4392546128",
    "https://openalex.org/W4391753532",
    "https://openalex.org/W4360981050",
    "https://openalex.org/W4304944121",
    "https://openalex.org/W4390229867",
    "https://openalex.org/W4382394524",
    "https://openalex.org/W4311026115",
    "https://openalex.org/W4385880258",
    "https://openalex.org/W6816695178",
    "https://openalex.org/W6600710202",
    "https://openalex.org/W4407762318",
    "https://openalex.org/W4287623114"
  ],
  "abstract": null,
  "full_text": "npj |digital medicine Article\nPublished in partnership with Seoul National University Bundang Hospital\nhttps://doi.org/10.1038/s41746-025-01576-4\nMitigating the risk of health inequity\nexacerbated by large language models\nCheck for updates\nYuelyu Ji1, Wenhe Ma2, Sonish Sivarajkumar3,H a n gZ h a n g3, Eugene M Sadhu4,5,Z h u o c h u nL i1,X i z h iW u2,\nShyam Visweswaran4,5 & Yanshan Wang2,3,4,5,6\nRecent advancements in large language models (LLMs) have demonstrated their potential in\nnumerous medical applications, particularly in automating clinical trial matching for translational\nresearch and enhancing medical question-answering for clinical decision support. However, our study\nshows that incorporating non-decisive socio-demographic factors, such as race, sex, income level,\nLGBT+status, homelessness, illiteracy, disability, and unemployment, into the input of LLMs can lead\nto incorrect and harmful outputs. These discrepancies could worsen existing health disparities if LLMs\nare broadly implemented in healthcare. To address this issue, we introduce EquityGuard, a novel\nframework designed to detect and mitigate the risk of health inequities in LLM-based medical\napplications. Our evaluation demonstrates its effectiveness in promoting equitable outcomes across\ndiverse populations.\nLarge language models (LLMs)1–6 have demonstrated signiﬁcant promise\nacross a range of medical applications. Models such as GPT-4 can process\nvast amounts of text, generating human-like responses, summaries, and\ncontextually relevant insights. This capability holds signiﬁcant promise for\nadvancing both patient care and medical research. LLMs are particularly\nvaluable in tasks like clinical trial matching and medical question answering\n(MQA), which are crucial for translational research and clinical decision\nsupport, respectively. These applications underscore the transformative role\nLLMs can play in improving healthcareoutcomes and streamlining research\nefforts.\nHowever, despite these impressivecapabilities, LLMs may exacerbate\npersistent healthcare inequities worldwide. In many clinical settings, espe-\ncially in low-resource environments, biased decision-making can further\nexacerbate disparities in treatment and access to care. This urgent challenge\ncalls for artiﬁcial intelligence (AI) systems that are not only powerful but also\nfair and unbiased. To address this, wepropose EquityGuard, a novel fra-\nmework that employs contrastive learning to actively mitigate bias in LLM\noutputs. In this study, we validate EquityGuard on two primary medical\ntasks: clinical trial matching and medical question-answering.\nClinical trial matching (CTM), an essential process for accelerating\ntranslational research, involves identifying and pairing patients with\nappropriate clinical trials based on complex eligibility criteria derived from\npatient medical records and trial protocols\n7. Although LLMs offer trans-\nformative solutions by automating this process, they can inadvertently\npropagate bias, leading to the systematic exclusion of certain demographic\ngroups from clinical trials.\nSimilarly, medical question-answering (MQA) systems powered by\nLLMs8–14 hold great potential for enhancing clinical decision support by\nintegrating diverse sources such as clinical guidelines, research papers,\nand patient-speciﬁc information. Yet, biased outputs in MQA tasks may\nlead to misinformation and disproportionately affect underrepresented\ncommunities.\nO u re v a l u a t i o ni n c l u d e ss t a t e - o f - t h e - a r tm o d e l ss u c ha sG P T - 4a sw e l l\nas the latest releases, including Gemini and Claude (2024 versions).\nAlthough these models demonstrateremarkable performance improve-\nments, they still inherit biases from their training data.\nIn this study, we aim to address two key research questions:\n RQ1: To what extent do LLMs exhibit inequities across two major\nmedical applications, i.e., CTM and MQA tasks?\n RQ2: What techniques can be applied to mitigate inequities when\napplying LLMs in medical applications, and how effective are they in\npromoting health equity?\nUnderstanding how inequities manifest across healthcare care tasks is\nessential to address these issues. Previous research has identiﬁed several\nsources of inequity, including inherent biases in training data, under-\nrepresentation of certain groups, and algorithmic designﬂaws\n15–17.H o w -\never, there remains a need for focused investigations into how these\n1Department of Information Science, University of Pittsburgh, Pittsburgh, PA, USA.2Department of Health Information Management, University of Pittsburgh,\nPittsburgh, PA, USA.3Intelligent Systems Program, University of Pittsburgh, Pittsburgh, PA, USA.4Department of Biomedical Informatics, University of Pittsburgh,\nPittsburgh, PA, USA.5Clinical and Translational Science Institute, University of Pittsburgh, Pittsburgh, PA, USA.6University of Pittsburgh Medical Center,\nPittsburgh, PA, USA. e-mail: yanshan.wang@pitt.edu\nnpj Digital Medicine|           (2025) 8:246 1\n1234567890():,;\n1234567890():,;\ninequities affect speciﬁc healthcare tasks, such as CTM and MQA. This\npaper aims toﬁll that gap by identifying and mitigating inequities in these\napplications. Two examples are illustrated in Fig.1.\nThe proposed EquityGuard framework is based on contrastive\nlearning and could systematically evaluate and mitigate inequities in\nLLMs18–21. EquityGuard uses contrastive learning techniques22–24 to disen-\ntangle socio-demographic determinants of health (SDOH) factors from\ntask-related embeddings, ensuring that these attributes do not unduly\ninﬂuence model predictions. Through aseries of experiments, we show that\nEquityGuard could enhance equity in LLMs for medical applications, spe-\nciﬁcally CTM and MQA tasks. EquityGuard is designed to be adaptable\nacross diverse healthcare settings, including low-resource environments\n25,26,\nthereby effectively mitigating bias even when clinical data is scarce and\npromoting equitable outcomes in both CTM and MQA tasks.\nResults\nOur experiments focused on examining how race, sex, and SDOH factors\n(including low income, LGBT+, homeless, illiteracy, disabled, and unem-\nployed) inﬂuence the outputs of LLMs and potentially introduce inequity\nand inaccuracy. To address these issues, we proposed theEquityGuard\nframework, which leverages contrastive learning to mitigate the effects of\nirrelevant SDOH attributes by aligning embeddings of similar inputs. This\napproach aims to improve the fairness of LLM outputs by reducing the\ninﬂuence of sensitive demographic factors.\nWe evaluated the models onﬁve datasets across two key medical\napplications: CTM and MQA tasks. The CTM datasets include SIGIR\n2016\n27, TREC 2011, and TREC 202228, while the MQA datasets are MedQA8\nand MedMCQA9. We added speciﬁc terms for race, sex, and each SDOH\ncategory in the input to different LLMs,in the same way as illustrated in Fig.\n1, to examine the output. We tested four LLMs for the evaluation: GPT-4,\nGPT-4o Mini, Gemini (Gemini 1.5 Flash)29, and Claude (speciﬁcally,\nClaude-3-5-sonnet-20240620)30. For the EquityGuard implementation, we\nmainly used the open-source LLMs, including LLaMA3 8B and Mistral v0.3,\nand compared with baseline GPT-4 model. More details about the Equi-\ntyGuard framework and approach used in this study can be found in the\nMethod section.\nComparison of equity in LLMs\nFigure 2 presents radar plots compared the performance of the LLMs on\nCTM and MQA tasks when different SDOH factors are introduced into the\ndataset. Performance for the CTM task is measured using the Normalized\nDiscounted Cumulative Gain at rank 10 (NDCG@10), with higher values\nindicating better performance. For the MQA task, error rates are used, with\nlower values indicating better performance.\nAmong the evaluated models,GPT-4 consistently demonstrated the\nbest overall performance across a variety of SDOH factors. In the CTM task,\nGPT-4 maintained relatively stable NDCG@10 scores, even when different\nSDOH factors were included in the input. For instance, GPT-4 performed\nparticularly well across low-income, unemployed, and disabled groups,\nshowing minimal variation in NDCG@10 scores compared to other models.\nMoreover, GPT-4 exhibited balancedperformance across racial groups,\nincluding Black, White, and Hispanic, reﬂecting greater fairness in handling\ndiverse queries (Fig.2 left).\nIn contrast, bothGemini and Claude showed greater variability in\nperformance across SDOH factors. These models experienced signiﬁcant\ndeclines in NDCG@10 scores forNative American, Middle Eastern,a n d\nPaciﬁc Islandergroups, indicating poorer handling of less-represented racial\ncategories. Furthermore, they exhibited noticeably higher error rates in the\nMQA task when facing queries involvinghomelessness, unemployment,a n d\nlow incomecategories, revealing challenges in equity concerning SDOH\nfactors.\nWhile GPT-4 maintained lower error rates for most MQA\ncategories, especially in sex and race-related queries, Gemini and\nClaude demonstrated a higher p ropensity for errors in under-\nrepresented groups. For example , Gemini exhibited error rates as\nhigh as 0.31 forlow income cases and 0.29 forhomeless populations.\nFig. 1 | Inequities when applying LLMs to two major medical applications.\nClinical Trial Matching (left) and Medical Question Answering (right). On the left,\nincluding race and sex information (e.g.,“African-American” and “woman”)i nt h e\npatient note, despite being irrelevant to matching the correct clinical trials, resulted in\naltered clinical trial recommendations generated by the LLMs. On the right, adding\nrace information (e.g.,“Native American”) to the question, which should not affect the\nresponse, led to incorrect answers from the LLMs. These examples show that non-\ndecisive socio-demographic factors in different patient populations can lead to\nincorrect outputs from LLMs, which may lead to harmful clinical outcomes to these\npatient populations and eventually exacerbate healthcare inequities.\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 2\nThese disparities highlight that, while GPT-4 offers better equity,\nG e m i n ia n dC l a u d ea r em o r ep r o n et oproducing inequitable outputs\nfor vulnerable groups, particularly in MQA tasks.\nTo quantify fairness, we used the equal opportunity (EO) and demo-\ngraphic parity (DP) metrics. GPT-4 again outperformed the other models,\nshowing consistent results with higher EO and DP scores across different\nSDOH factors. Gemini and Claude, however, displayed greater disparities,\nparticularly in their treatment ofunemployed and low income groups,\nsuggesting that these models struggle to maintain fairness across diverse\npopulations (Fig.3).\nFairness and correlation analysis\nTo further investigate the inequities observed in the LLM models, we con-\nducted a correlation analysis between different categories. We evaluated\nwhether certain race, sex and SDOH factors tend to produce similar biased\noutcomes, which is critical for understanding systemic biases and improving\nfairness across categories.\nFor each pair of inequity categories, we calculated the correlation based\non the following criteria:\n If both categories result in the same wrong answer or wrong rerank\norder, a correlation of+1 is assigned.\n If one category is correct while the other is wrong, a correlation of -1\nis given.\n If both categories either get the same correct answer or the right rerank\norder, a correlation of 0 is assigned.\nIn the CTM task (Fig.4,l e f t ) ,s e v e r a lr a c e ,s e xa n dS D O Hf a c t o r s\nexhibited strong correlations, revealing compounded inequity\nFig. 2 | Performance of various LLMs when speciﬁc SDOH factors were intro-\nduced into the dataset.The clinical trial matching (CTM) performance is measured\nusing NDCG@10 (higher is better), while the medical question answering (MQA)\nperformance is measured using error rate (lower is better). SDOH factors include\nrace, sex, low income, LGBT+ status, homelessness, illiteracy, disability, and\nunemployment. Each sensitive attribute was incorporated into the input data for\nboth CTM and MQA tasks during the evaluation.\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 3\npatterns. The Black and Paciﬁc Islander categories displayed a high\ncorrelation coefﬁcient of 0.5, suggesting that model decisions were\nconsistently similar for these groups. Additionally, the socioeconomic\nfactors unemployed and low income showed a notable correlation of\n0.25, indicating that inequities related to SDOH heavily in ﬂuence\nmodel outputs in the CTM task.\nOther factors, such aslow income and Black, demonstrated mod-\nerate correlations (0.25), pointing to shared inequities between eco-\nnomic disadvantage and racial categories. Conversely, correlations\nbetween Hispanic and low income resulted in a negative correlation\n(-0.26), highlighting disparities in how the model treats these cate-\ngories. The unemployed and Mixed Race categories showed a weaker\npositive correlation of 0.2, indicating less interconnection between\nthese inequities compared to others.\nIn the MQA task (Fig.4, right), similar trends were observed. Strong\ncorrelations were found between race and SDOH factors. Theunemployed\ngroup was closely related to thedisabilitycategory, with correlation values\nexceeding 0.17. This implies that inequities in SDOH factors signiﬁcantly\nalign with racial inequities, further entrenching model biases when\nanswering medical questions.\nInterestingly, thelow incomecategory showed negative correlations\nwith all other categories. This suggests that, in this particular task, the model\ntreats low income as a distinct factor not strongly linked to other SDOH\nfactors or demographic attributes. One possible explanation is that the task\nmight focus more on clinical or health-related issues, and SDOH factors like\nincome level may not be as directly relevant. Consequently, the model pays\nless attention tolow-income as a critical feature in this context, leading to\nthese lower correlations.\nInequity mitigation in CTM\nWe evaluated the effectiveness of theEquityGuard framework in mitigating\ninequities in LLMs for the CTM task by analyzing model performance\nacross race, sex, and SDOH factors. The models assessed were LLaMA3 8B,\nMistral v0.3, both with and without the application of EquityGuard\nFig. 3 | Fairness metrics including equal opportunity (EO) and demographic parity (DP) to assess equity in LLMs.Higher EO and DP scores indicate better equity, with\nEO focusing on ensuring equal positive outcomes for qualiﬁed individuals across groups and DP evaluating overall equity across all groups.\nFig. 4 | Correlation heatmaps of inequity categories in CTM and MQA tasks.The\nleft plot shows the correlation between inequity categories in CTM tasks, illustrating\nhow different inequity-modiﬁed queries resulted in similar trial rankings or selec-\ntions by the models. The right plot shows the correlation between inequity categories\nin MQA tasks, displaying how often different inequity-modiﬁed queries led to the\nsame answers or error patterns. These heatmaps help analyze how inequities across\ncategories are interconnected, impacting model fairness.\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 4\n(denoted asw/ EquityGuardand w/o EquityGuard, respectively). GPT-4\nwas compared as a baseline model.\nTables 1 and 2 present the NDCG@10 scores across different race\nand sex categories. Models trained with EquityGuard exhibited more\nuniform performance across race and sex compared to their counter-\nparts without EquityGuard. For instance, LLaMA3 8Bw/ EquityGuard\nmaintained NDCG@10 scores around 70% across all categories,\nwhereas LLaMA3 8Bw/o EquityGuardshowed greater variability, with\nscores ranging from 67.7% (Native American) to 72.6% (Asian). The\nperformance disparities between these groups indicate inequities in\nmodels without EquityGuard.\nTable 3 extends the analysis to SDOH factors, including LGBT+,\nlow income, unemployed, disabled, illiterate and homeless. Models\nwith EquityGuard displayed more consistent performance across\nthese categories. For example, LLaMA3 8Bw/ EquityGuard achieved\nhigher NDCG@10 scores in the low income (89.8%) and unemployed\n(87.4%) categories compared tow/o EquityGuard (81.3% and 83.4%,\nrespectively). This improvement suggests that EquityGuard enhances\nfairness in CTM by mitigating inequities associated with SDOH\nfactors.\nInequity mitigation in MQA\nWe further assessed EquityGuard’si m p a c to nt h eM Q At a s ku s i n gt h e\nMedQA and MedMCQA datasets. Error rates across SDOH categories are\npresented in Tables4 and 5. Implementing EquityGuard led to a noticeable\nreduction in error rates across all race, sex and SDOH categories. For\ninstance, LLaMA3 8Bw/ EquityGuardachieved an average error rate of\n19.8%, compared to 21.2% forw/o EquityGuard, representing a relative\ndecrease of approximately1.3%.\nNotably, the reduction in error rates was signiﬁcantly greater in cate-\ngories that initially exhibited higher inequities. In the Black category,\nLLaMA3 8B’s error rate decreased from 22.0% (w/o EquityGuard) to 20.3%\n(w/ EquityGuard). Mistral v0.3 showed similar improvements, with error\nrates decreasing from 21.4% to 19.5% in the Black category after applying\nEquityGuard.\nTable 6 presents error rates across SDOH categories for the MQA\ntask. EquityGuard effectively reduced error rates in categories such as\nLGBT+, low-income, and unemployed. For LLaMA3 8B, the error rate\nin the low-income category decreased from 18.4% (w/o EquityGuard)\nto 12.7% ( w/ EquityGuard ). This signi ﬁcant reduction highlights\nEquityGuard ’s capability to mitigate inequities associated with SDOH\nfactors.\nEnhanced fairness metrics\nTo quantify the fairness improvements achieved by EquityGuard, we cal-\nculated the EO and DP differences for the LLaMA3 8B models (Fig.5).\nModels with EquityGuard (w/ EquityGuard) demonstrated reduced EO and\nDP differences across SDOH factors, indicating enhancedfairness. Speci-\nﬁcally, the EO difference increased by an average of 28%, and the DP\ndifference increased by approximately 32% compared to the models without\nEquityGuard.\nOverall impact of EquityGuard\nOur results demonstrate that EquityGuard signiﬁcantly mitigates inequities\nin LLMs across both CTM and MQA tasks. Key observations include:\n Uniform performance across demographics: Models with Equity-\nGuard provided more consistent NDCG@10 scores and lower error\nrates across all race, sex and SDOH categories, indicating reduced\ninequity.\n Improved fairness metrics: Enhanced EO and DP scores afﬁrm that\nEquityGuard promotes equitable model behavior, ensuring that sen-\nsitive demographic factors do not disproportionately in ﬂuence\npredictions.\nOverall, the application of EquityGuard contributes to more fair and\nequitable decision-making processes in healthcare AI systems by\nTable 1 | Performance comparison across race and sex categories in CTM task\nModel Base Male Female White Black Hispanic Asian Native American Paci ﬁc Islander Mixed Race\nLLaMA3 8Bw/ EquityGuard 70.3 ± 0.5% 70.0 ± 0.6% 70.2 ± 0.4% 70.4 ± 0.5% 70.6 ± 0.7% 70.4 ± 0.6% 70.5 ± 0.5% 70.1 ± 0.6% 70.2 ± 0.5% 70.3 ± 0.4%\nLLaMA3 8Bw/o EquityGuard 68.8 ± 0.8% 71.1 ± 0.9% 70.4 ± 0.7% 69.1 ± 0.8% 68.8 ± 0.9% 70.6 ± 0.8% 72.6 ± 1.0% 67.7 ± 0.9% 68.1 ± 0.8% 68.7 ± 0.7%\nMistral v0.3w/ EquityGuard 71.1 ± 0.6% 70.6 ± 0.5% 70.4 ± 0.7% 70.4 ± 0.6% 70.6 ± 0.6% 70.6 ± 0.7% 70.8 ± 0.5% 70.4 ± 0.6% 70.3 ± 0.6% 70.5 ± 0.7%\nMistral v0.3w/o EquityGuard 69.5 ± 0.7% 71.3 ± 0.8% 70.9 ± 0.7% 69.3 ± 0.7% 68.9 ± 0.8% 70.7 ± 0.9% 72.8 ± 0.8% 68.1 ± 0.7% 68.4 ± 0.8% 68.8 ± 0.7%\nGPT-4 71.4 ± 0.4% 71.0 ± 0.5% 71.2 ± 0.5% 71.0 ± 0.4% 73.2 ± 0.6% 73.3 ± 0.6% 72.5 ± 0.5% 71.0 ± 0.4% 71.0 ± 0.5% 71.1 ± 0.5%\nModels utilizing EquityGuard (w/ EquityGuard) exhibit more consistent NDCG@10 scores across categories. Values are reported as mean ±standard deviation overﬁve runs.\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 5\nminimizing the inﬂuence of sensitive attributes on model outputs. This is\ncritical for addressing health disparities and ensuring equitable healthcare\ndelivery.\nDiscussion\nLLMs are typically trained on vast amounts of publicly available text data\ndrawn from the internet, books, socialmedia, and other human-generated\ncontent. Because this data reﬂects societal norms, perspectives, and histor-\nical narratives, it inevitably contains biases— particularly those that mar-\nginalize or misrepresent disadvantaged populations. As a result, LLMs can\ninternalize and replicate these biases in the way they generate text, make\npredictions, or support decision-making. When such biased models are\ndeployed in health-related tasks— such as clinical decision support, clinical\ntrial matching, patient education, or triage— they risk propagating and even\namplifying existing disparities. This may lead to skewed outcomes that\ndisproportionately affect already underserved communities, ultimately\nexacerbating health inequities ratherthan alleviating them. In this study, we\nexamined and conﬁrmed the existence of such potential inequities using\nexisting LLMs. We then introducedEquityGuard,af r a m e w o r ke m p l o y i n g\ncontrastive learning to mitigate inequities in LLMs applied to healthcare\ntasks, speciﬁc a l l yC T Ma n dM Q A .B yr e d u c i n gt h eu n d u ei nﬂuence of race,\nsex and SDOH factors, EquityGuard enhances fairness in LLM outputs. Our\nexperiments acrossﬁve datasets demonstrated that even advanced models\nlike GPT-4, Claude, and Gemini are susceptible to inequities, which Equi-\ntyGuard effectively mitigated, leading to more equitable outcomes in\nboth tasks.\nDespite these promising results , there are limitations to our\napproach. One signiﬁcant challenge lies in accurately identifying and\nprocessing social demographic determinant factors within the data-\nsets. While we utilized Bio_ClinicalBERT for named entity recogni-\ntion and developed a custom pip eline to enhance accuracy, the\nd e t e c t i o no ft h e s ef a c t o r sis not foolproof. Misidenti ﬁcation or\nomission can adversely affect the e ffectiveness of bias mitigation.\nFuture work could explore more a dvanced methods for detecting\nSDOH factors, possibly incorporat ing additional context or lever-\naging unsupervised learning techniques\n31–33. Another limitation per-\ntains to balancing bias mitigation with task performance. While\nEquityGuard signiﬁcantly improves fairness metrics, it is observed\nthat models such as LLaMA3 8B and Mistral v0.3 exhibit a slight\nperformance decrease compared to GPT-4\n34. This can be attributed to\nthe fact that GPT-4, with its substantially larger number of para-\nmeters and more sophisticated arch itecture, inherently possesses\ngreater capacity for handling complex tasks. In contrast, the smaller\nmodels experience a trade-off between bias mitigation and task\naccuracy when the additional contrastive loss is incorporated. Future\nwork will explore parameter-ef ﬁcient techniques to better balance\nthis trade-off. The incorporation of additional loss components for\nbias mitigation may impact overall model performance. Future\nresearch could investigate adapti ve strategies or alternative loss\nfunctions that more effectively balance fairness and performance\n35,36.\nIt is important to note that LLMs i nherently exhibit stochastic\nbehavior. Although we report the average performance along with the\nstandard deviations, the variability in outputs may pose risks in\nhealthcare applications, such as inconsistent clinical recommenda-\ntions. Future work will investiga te ensemble methods and post-\nprocessing techniques to further mitigate such randomness. Note that\nthis study focuses exclusively on text-based clinical tasks (CTM and\nMQA) and does not address bias mitigation in medical imaging a\ntopic to be explored in future work.\nFurthermore, our study focused on a limited set of SDOH factors.\nThe complex nature of biases in healthcare suggests that other factors,\nsuch as age, or the intersectionality between attributes, could also\ncontribute to biased outcomes\n37–40. Expanding EquityGuard to account\nfor a broader range of factors would enhance its applicability and\nrobustness in real-world settings. The evaluation metrics used, DP and\nEO, provide insights into the models’fairness but may not capture all\ndimensions relevant in healthcare contexts 38,39,41.A l t h o u g hE q u i t y -\nGuard exhibits strong bias mitigation capabilities, its deployment at\nscale within large health care systems entails signi ﬁcant challenges\nrelated to computational efﬁciency and ethical oversight. Future work\ncould incorporate additional fairness metrics, such as Equalized Odds\nor calibration error, to provide a more comprehensive assessment of\nmodel fairness\n42,43.\nIn conclusion, while EquityGuard shows promise in mitigating\ninequities in LLMs for CTM and MQA tasks, addressing its current\nlimitations is crucial for the advancement of AI-driven healthcare\nsystems that are not only effective but also equitable. Future work will\nfocus on enhancing social demographic determinant detection, reﬁning\ninequity mitigation strategies, expanding the range of considered\ninequities, and exploring additional fairness metrics. By advancing\nTable 2 | Performance comparison across additional race categories\nModel Middle Eastern Indigenous African American South Asian East Asian\nLLaMA3 8Bw/ EquityGuard 70.0 ± 0.6% 69.9 ± 0.7% 70.2 ± 0.5% 70.2 ± 0.6% 70.3 ± 0.5%\nLLaMA3 8Bw/o EquityGuard 68.7 ± 0.8% 68.6 ± 0.8% 69.2 ± 0.7% 68.9 ± 0.8% 69.2 ± 0.7%\nMistral v0.3w/ EquityGuard 70.3 ± 0.6% 70.4 ± 0.5% 70.4 ± 0.6% 70.6 ± 0.5% 70.6 ± 0.5%\nMistral v0.3w/o EquityGuard 68.9 ± 0.7% 68.9 ± 0.8% 69.2 ± 0.7% 69.3 ± 0.8% 69.5 ± 0.7%\nGPT-4 71.1 ± 0.5% 71.1 ± 0.5% 74.1 ± 0.6% 73.1 ± 0.6% 71.2 ± 0.5%\nEquityGuard reduces variability in NDCG@10 scores among diverse racial groups. Values are reported as mean ±standard deviation overﬁve runs.\nTable 3 | NDCG@10 score comparison across SDOH categories for CTM task\nModel LGBT + Low Income Unemployed Disabled Illiterate Homeless\nLLaMA3 8Bw/ EquityGuard 65.6 ± 0.7% 89.8 ± 0.8% 87.4 ± 0.9% 59.9 ± 0.7% 64.0 ± 0.6% 71.3 ± 0.8%\nLLaMA3 8Bw/o EquityGuard 57.4 ± 0.8% 81.3 ± 0.9% 83.4 ± 0.8% 79.1 ± 0.9% 58.5 ± 0.8% 66.8 ± 0.7%\nMistral v0.3w/ EquityGuard 63.3 ± 0.7% 56.7 ± 0.8% 75.5 ± 0.9% 56.9 ± 0.8% 61.0 ± 0.7% 68.8 ± 0.8%\nMistral v0.3w/o EquityGuard 66.9 ± 0.8% 73.4 ± 0.9% 70.7 ± 0.8% 50.4 ± 0.7% 57.9 ± 0.8% 65.4 ± 0.9%\nGPT-4 85.7 ± 0.5% 75.0 ± 0.6% 70.3 ± 0.5% 75.8 ± 0.6% 78.5 ± 0.5% 80.4 ± 0.6%\nValues are reported as mean ± standard deviation overﬁve runs.\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 6\nthese areas, we aim to contribute to the development of AI models that\nsupport fair, transparent, and equitable decision-making in healthcare,\nultimately fostering more inclusive and trustworthy technologies for\ndiverse patient populations.\nMethods\nOverview\nWe proposed EquityGuard, a contrastive learning-based framework\ndesigned to mitigate inequities inLLMs applied to healthcare tasks22,44.\nContrastive learning is a self-supervised machine learning technique that\naims to learn effective data representations by contrasting positive and\nnegative pairs of samples. The core idea is to map similar data points closer\ntogether in the feature space while pushing dissimilar ones further apart.\nSpeciﬁcally, we focus on two tasks: CTM and MQA. EquityGuard aims to\nreduce the inﬂuence of race, sex, and SDOH factors on model predictions by\naligning embeddings through contrastive learning targeted at biased inputs.\nData processing\nOur experiments evaluated Equ ityGuard on two tasks CTM and\nMQA using ﬁve datasets. For CTM, we used the SIGIR 2016, TREC\n2021, and TREC 2022 datasets, which provide clinical trial descrip-\ntions and patient case reports. For MQA, we employed the MedQA\nand MedMCQA datasets containing complex medical questions and\ncorresponding answer options.\nTo assess the impact of sex, race, and SDOH factors on model pre-\ndictions, we applied Bio_ClinicalBERT for named entity recognition and\ncollaborated with medical experts toﬁlter out topics explicitly related to\nthese factors. Detailed data distributions are provided in Tables7 and 8.\nAdditionally, we generated counterfactual queries by systematically altering\nthese factors (e.g., changing“Black” to “White”) to enable controlled con-\ntrastive learning and bias evaluation.\nEquityGuard framework\nEquityGuard employs contrastive learning to minimize the inﬂuence of\nsocial demographic determinant fac t o r so nm o d e lo u t p u t sb yt a r g e t i n g\nbiased inputs. For each query, we construct triplets consisting of an anchor, a\npositive sample, and a negative sample:\n Anchor (xanchor): The original query without race, sex and SDOH\nfactors (neutral version).\n Positive (xpos): A query that includes a race, sex and SDOH factor,\ndiffering minimally from the anchor.\n Negative (xneg): A query that includes additional or different factors\ncompared to the anchor.\nT h eg o a li st oa l i g nt h em o d e l’s embeddings such that the anchor and\npositive samples (which share the same medical context) are close in the\nembedding space, while the negative sample (which introduces additional\ninequities) is farther away. Figure6 illustrates the overall process of Equi-\ntyGuard, where contrastive learning is employed to align the embeddings of\nthe original (anchor) and minimally perturbed (positive) queries while\nseparating those with additional bias (negative). This approach actively\nmitigates the inﬂuence of socio-demographic factors on theﬁnal model\npredictions.\nTable 9 provides examples of anchor, positive, and negative samples\nused in the contrastive learning process.\nModel architecture and training\nWe build upon the LLaMA model, extending it to handle both ranking and\nquestion-answering tasks. The modelshares a transformer-based backbone\nand is adapted for each task:\nClinical Trial Matching (Ranking Task): The model encodes patient\nnotes and trial eligibility criteria into embeddings. We compute a relevance\nscore between the patient note and each trial using a scoring function and\nrank the trials accordingly. The objective is to maximize the NDCG@10\nmetric while minimizing inequity.\nTable 4 | Error rate comparison across race and sex categories in MQA task\nModel Base Male Female White Black Hispanic Asian Native American Paci ﬁc Islander Mixed race\nLLaMA3 8Bw/ EquityGuard 19.3 ± 0.5% 19.6 ± 0.6% 19.9 ± 0.5% 20.2 ± 0.5% 20.3 ± 0.6% 19.8 ± 0.5% 19.5 ± 0.6% 20.1 ± 0.5% 20.2 ± 0.6% 19.9 ± 0.5%\nLLaMA3 8Bw/o EquityGuard 20.6 ± 0.7% 21.6 ± 0.8% 20.9 ± 0.7% 21.7 ± 0.8% 22.0 ± 0.8% 21.1 ± 0.7% 22.1 ± 0.8% 21.3 ± 0.8% 21.5 ± 0.7% 21.0 ± 0.8%\nMistral v0.3w/ EquityGuard 18.6 ± 0.6% 19.1 ± 0.5% 18.8 ± 0.6% 19.2 ± 0.5% 19.5 ± 0.6% 19.0 ± 0.5% 18.8 ± 0.6% 19.1 ± 0.5% 19.4 ± 0.6% 19.0 ± 0.5%\nMistral v0.3w/o EquityGuard 19.9 ± 0.7% 21.0 ± 0.8% 20.7 ± 0.7% 21.2 ± 0.8% 21.4 ± 0.7% 20.8 ± 0.8% 21.6 ± 0.7% 20.9 ± 0.8% 21.3 ± 0.7% 20.7 ± 0.8%\nGPT-4 17.3 ± 0.4% 17.5 ± 0.5% 17.8 ± 0.4% 18.1 ± 0.5% 18.4 ± 0.5% 18.0 ± 0.4% 17.7 ± 0.5% 18.0 ± 0.4% 18.3 ± 0.5% 17.9 ± 0.4%\nValues are reported as mean ±standard deviation over 5 runs.\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 7\nMedical Question Answering (Classiﬁcation Task):T h em o d e l\nencodes medical questions and predicts the correct answer choice. We use\ncross-entropy loss for training, aiming to minimize the error rate while\nreducing inequity.\nThe overall loss functionLcombines the task-speciﬁcl o s sL\ntask and the\ncontrastive lossLcontrastive for inequity mitigation:\nL ¼ Ltask þ λLcontrastive ð1Þ\nTable 5 | Error rate comparison across additional race categories in MQA task\nModel Middle Eastern Indigenous African American South Asian East Asian\nLLaMA3 8Bw/ EquityGuard 20.3 ± 0.6% 19.9 ± 0.6% 20.2 ± 0.5% 19.8 ± 0.6% 19.9 ± 0.5%\nLLaMA3 8Bw/o EquityGuard 22.2 ± 0.7% 21.1 ± 0.7% 21.8 ± 0.6% 21.4 ± 0.7% 21.3 ± 0.7%\nMistral v0.3w/ EquityGuard 19.2 ± 0.6% 19.0 ± 0.6% 19.3 ± 0.5% 19.0 ± 0.6% 18.9 ± 0.5%\nMistral v0.3w/o EquityGuard 21.4 ± 0.7% 21.3 ± 0.7% 21.6 ± 0.6% 21.4 ± 0.7% 21.1 ± 0.7%\nGPT-4 18.2 ± 0.5% 18.0 ± 0.5% 18.1 ± 0.5% 18.1 ± 0.5% 18.0 ± 0.5%\nValues are reported as mean ± standard deviation overﬁve runs.\nTable 6 | Error rate comparison across SDOH categories for MQA task\nModel LGBT + Low Income Unemployed Disabled Illiterate Homeless\nLLaMA3 8Bw/ EquityGuard 22.1 ± 0.6% 12.7 ± 0.5% 18.8 ± 0.6% 32.2 ± 0.7% 21.5 ± 0.6% 29.4 ± 0.7%\nLLaMA3 8Bw/o EquityGuard 26.7 ± 0.7% 18.4 ± 0.6% 18.5 ± 0.7% 35.7 ± 0.8% 24.6 ± 0.7% 31.6 ± 0.8%\nMistral v0.3w/ EquityGuard 24.6 ± 0.7% 14.2 ± 0.6% 15.2 ± 0.7% 33.4 ± 0.8% 22.8 ± 0.7% 31.0 ± 0.8%\nMistral v0.3w/o EquityGuard 27.8 ± 0.8% 19.8 ± 0.7% 19.2 ± 0.7% 36.2 ± 0.8% 25.6 ± 0.8% 32.9 ± 0.8%\nGPT-4 20.1 ± 0.5% 12.6 ± 0.5% 18.5 ± 0.5% 32.2 ± 0.6% 21.0 ± 0.5% 28.7 ± 0.5%\nValues are reported as mean ±standard deviation over 5 runs.\nFig. 5 | Equal opportunity (EO) and demographic parity (DP) metrics for LLaMA3 8B models.Models trained with EquityGuard (`w/EquityGuard̀ ) show reduced EO\nand DP differences, indicating enhanced fairness.\nTable 7 | Race composition across datasets (count and percentage)\nDataset Not mentioned White African-Am. Hispanic Total\nCount (%) Count (%) Count (%) Count (%)\nMedQA 1000 (72.1%) 185 (13.3%) 90 (6.5%) 50 (3.6%) 1387\nMedMCQA 6065 (98.6%) 8 (0.1%) 1 (0.02%) 75 (1.2%) 6149\nSIGIR 2016 40 (69.0%) 10 (17.2%) 4 (6.9%) 2 (3.4%) 58\nTREC 2021 45 (60.0%) 15 (20.0%) 8 (10.7%) 4 (5.3%) 75\nTREC 2022 35 (70.0%) 8 (16.0%) 3 (6.0%) 2 (4.0%) 50\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 8\nwhere λ is a hyperparameter controlling the trade-off between task\nperformance and inequity mitigation.\nFor the contrastive loss, we use the triplet loss function:\nLcontrastive ¼\nXN\ni¼1\nmax 0; m þ dðf ðxðiÞ\nanchorÞ; f ðxðiÞ\nposÞÞ /C0 dðf ðxðiÞ\nanchorÞ; f ðxðiÞ\nneg ÞÞ\n/C16/C17\nð2Þ\nwhered( ⋅ , ⋅ ) is a distance metric (e.g., cosine distance),m is the margin,f ( ⋅ )\nis the embedding function, andN is the number of triplets.\nWe trained the models using the Adam optimizer with a\nlearning rate of 1 × 10−5. The hyperparameter λ was set to 0.1, and\nthe margin m was set to 1.0, tuned on a validation set. The training\nwas conducted on four NVIDIA V100 GPUs with 32GB memory\neach. We repeated all experimentsﬁve times with different random\nseeds and report the average performance along with the standard\ndeviation to quantify the stability of the model outputs. We per-\nformed a sensitivity analysis for the contrastive loss weight λ over\nthe range 0.05 to 0.20. The experimental results (see Table 10)\nindicate that λ = 0.10 achieves the best trade-off between task per-\nformance and fairness improvement.\nEvaluation\nTo evaluate the effectiveness of EquityGuard, we measured both task per-\nformance and fairness metrics. For CTM, we used the Normalized Dis-\ncounted Cumulative Gain at rank 10 (NDCG@10) to evaluate the ranking\nquality. For MQA, we used the error rate to assess the accuracy of the model\nin answering questions.\nTo assess the models’fairness, we computed two metrics:\n Demographic parity (DP):Measures the difference in the probability\nof positive outcomes across different demographic groups.\n Equal opportunity (EO):M e a s u r e st h ed i f f e r e n c ei nt r u ep o s i t i v er a t e s\nacross different demographic groups.\nWe compared EquityGuard with several baseline models:\nLLaMA3 8B without inequity mitigation, Mistral v0.3 without\ninequity mitigation, and GPT-4 (a state-of-the-art LLM without\nexplicit inequity mitigation). We also included versions of LLaMA3\n8B and Mistral v0.3 with EquityGuard applied to assess the effec-\ntiveness of our proposed method. This approach promotes fairness\nand equity in healthcare applications by mitigating inequities in LLM\npredictions. The performance of the teacher model and the API cost\ncan also be seen in these Supplementary Tables 1–7.\nTable 8 | Sex composition across datasets\nDataset Male\ncount (%)\nFemale\ncount (%)\nNot\nmentioned (%)\nTotal\nMedQA 200 (14.4%) 150 (10.8%) 1037 (74.8%) 1387\nMedMCQA 850 (13.8%) 650 (10.6%) 4649 (75.6%) 6149\nSIGIR 2016 10 (17.2%) 8 (13.8%) 40 (69.0%) 58\nTREC 2021 16 (21.3%) 12 (16.0%) 47 (62.7%) 75\nTREC 2022 13 (26.0%) 9 (18.0%) 28 (56.0%) 50\nFig. 6 |An overview of the EquityGuard framework for inequity detection and correction.\nTable 9 | Examples of anchor, positive, and negative samples in contrastive learning for inequity mitigation\nAnchor Positive Negative\nA 40-year-old patient with Type 2 Diabetes is\nexperiencing poor blood glucose control despite\ntaking metformin. What is the next best treatment\noption?\nA 40-year-old African-American patient with Type 2\nDiabetes is experiencing poor blood glucose control\ndespite taking metformin. What is the next best\ntreatment option?\nA 40-year-old African-American patient with Type 2\nDiabetes and alow-income background is\nexperiencing poor blood glucose control despite taking\nmetformin. What is the next best treatment option?\nA 55-year-old patient with acute chest pain presents\nto the ER. What diagnostic test should be\nperformed?\nA 55-year-old woman with acute chest pain presents to\nthe ER. What diagnostic test should be performed?\nA 55-year-oldAsian woman with acute chest pain and a\nhistory of homelessness presents to the ER. What\ndiagnostic test should be performed?\nA 60-year-old patient with Stage 3 hypertension is\nnon-responsive to lifestyle changes. What is the\nrecommended drug therapy?\nA 60-year-old Black patient with Stage 3 hypertension is\nnon-responsive to lifestyle changes. What is the\nrecommended drug therapy?\nA 60-year-old Black patient with Stage 3 hypertension\nand illiterate is non-responsive to lifestyle changes.\nWhat is the recommended drug therapy?\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 9\nData availability\nThe datasets used in this work, including SIGIR 2016, TREC 2021 and 2022,\nMedQA, and MedMCQA, are publicly available.\nCode availability\nThe code used in the article is available in this GitHub repository:https://\ngithub.com/PittNAIL/EquityGuard.\nReceived: 25 October 2024; Accepted: 20 March 2025;\nReferences\n1. Achiam, J. et al. Gpt-4 technical report.arXiv https://arxiv.org/abs/\n2303.08774 (2023).\n2. Dubey, A. et al. The llama 3 herd of models.arXiv https://arxiv.org/abs/\n2407.21783 (2024).\n3. Grosse, R. et al. Studying large language model generalization with\ninﬂuence functions.arXiv https://arxiv.org/abs/2308.03296 (2023).\n4. Benary, M. et al. Leveraging large language models for decision\nsupport in personalized oncology.JAMA Network Open6,\ne2343689– e2343689 (2023).\n5. Zhou, L. et al. Larger and more instructable language models become\nless reliable.Nature 634,6 1– 68 (2024).\n6. Kaplan, J. et al. Scaling laws for neural language models.arXiv https://\narxiv.org/abs/2001.08361 (2020).\n7. Jin, Q. et al. Matching patients to clinical trials with large language\nmodels. Nat. Commun.15, 9074 (2023).\n8. Jin, D. et al. What disease does this patient have? a large-scale open\ndomain question answering dataset from medical exams.Appl. Sci.\n11, 6421 (2021).\n9. Pal, A., Umapathi, L. K. & Sankarasubbu, M. Medmcqa: a large-scale\nmulti-subject multi-choice dataset for medical domain question\nanswering. arXiv https://arxiv.org/abs/2203.14371 (2022).\n10. Acikgoz, E. C. et al. Hippocrates: an open-source framework for\nadvancing large language models in healthcare.arXiv https://arxiv.\norg/abs/2404.16621 (2024).\n11. Singhal, K. et al. Large language models encode clinical knowledge.\nNature 620, 172– 180 (2023).\n12. Nori, H., King, N., McKinney, S. M., Carignan, D. & Horvitz, E.\nCapabilities of gpt-4 on medical challenge problems.arXiv https://\narxiv.org/abs/2303.13375 (2023).\n13. Nori, H. et al. Can generalist foundation models outcompete special-\npurpose tuning? case study in medicine.arXiv https://arxiv.org/abs/\n2311.16452 (2023).\n14. Singhal, K. et al. Towards expert-level medical question answering\nwith large language models.Nat. Med.31, 943– 950 (2023).\n15. Bai, X., Wang, A., Sucholutsky, I. & Grifﬁths, T. L. Measuring implicit\nbias in explicitly unbiased large language models.arXiv https://arxiv.\norg/abs/2402.04105 (2024).\n16. Yu, C. et al. Credit card fraud detection using advanced transformer\nmodel. arXiv https://arxiv.org/abs/2406.03733 (2024).\n17. Dai, S. et al. Unifying bias and unfairness in information retrieval: a\nsurvey of challenges and opportunities with large language models.\narXiv https://arxiv.org/abs/2404.11457 (2024).\n18. Tu, T. et al. Towards conversational diagnostic AI.\narXiv https://arxiv.\norg/abs/2401.05654 (2024).\n19. Tu, T. et al. Towards generalist biomedical AI.NEJM AI1, AIoa2300138\n(2024).\n20. Tanno, R. et al. Consensus, dissensus and synergy between clinicians\nand specialist foundation models in radiology report generation (2024).\n21. Dash, D. et al. Evaluation of gpt-3.5 and gpt-4 for supporting real-\nworld information needs in healthcare delivery.arXiv https://arxiv.org/\nabs/2304.13714 (2023).\n22. Chuang, C.-Y., Robinson, J., Lin, Y.-C., Torralba, A. & Jegelka, S.\nDebiased contrastive learning.Adv. Neural Inf. Process Syst.33,\n8765– 8775 (2020).\n23. Tian, Y. et al. What makes for good views for contrastive learning?\nAdv. Neural Inf. Process Syst.33, 6827– 6839 (2020).\n24. Rim, D. N., Heo, D. & Choi, H. Adversarial training with contrastive\nlearning in nlp.arXiv https://arxiv.org/abs/2109.09075 (2021).\n25. V, J. P., S, A. A. V., P, G. K. & N,K, K. A novel attention-based cross-\nmodal transfer learning framework for predicting cardiovascular\ndisease. Comput. Biol. Med.170, 107977 (2024).\n26. Prakash, V. J. & Vijay, S. A. A. A uniﬁed framework for analyzing textual\ncontext and intent in social media.ACM Trans. Intell. Syst. Technol.15,\nhttps://doi.org/10.1145/3682064(2024).\n27. Koopman, B. & Zuccon, G. A test collection for matching patients to\nclinical trials.Proceedings of the 39th International ACM SIGIR\nconference on Research and Development in Information Retrieval.\nhttps://api.semanticscholar.org/CorpusID:5630619 (2016).\n28. Roberts, K., Demner-Fushman, D., Voorhees, E. M., Bedrick, S. &\nHersh, W. R. Overview of the trec 2022 clinical trials track.https://trec.\nnist.gov/pubs/trec31/papers/Overview_trials.pdf (2022).\n29. Team, G. et al. Gemini: a family of highly capable multimodal models.\narXiv https://arxiv.org/abs/2312.11805 (2023).\n30. The claude 3 model family: Opus, sonnet, haiku.https://api.\nsemanticscholar.org/CorpusID:268232499.\n31. Velupillai, S. et al. Using clinical natural language processing for health\noutcomes research: overview and actionable suggestions for future\nadvances. J. Biomed. Inform.88,1 1– 19 (2018).\n32. Nazi, Z. A. & Peng, W. Large language models in healthcare and\nmedical domain: a review.arXiv https://arxiv.org/abs/2401.06775\n(2024).\n33. Tavabi, N., Singh, M., Pruneski, J. & Kiapour, A. M. Systematic\nevaluation of common natural language processing techniques to\ncodify clinical notes.Plos One19, e0298892 (2024).\n34. Li, J. & Li, G. The triangular trade-off between robustness, accuracy\nand fairness in deep neural networks: a survey.ACM Comput. Surv.\n57, 6 (2024).\n35. Kirchdorfer, L. et al. Analytical uncertainty-based loss weighting in\nmulti-task learning.arXiv https://arxiv.org/abs/2408.07985 (2024).\n36. Wu, H., Li, B., Tian, L., Feng, J. & Dong, C. An adaptive loss weighting\nmulti-task network with attention-guide proposal generation for small\nsize defect inspection.Vis. Comput.\n40, 681– 698 (2024).\n37. Kundi, B., El Morr, C., Gorman, R. & Dua, E. Artiﬁcial intelligence and\nbias: a scoping review.AI Soc.10, 510 (2023).\n38. Ferrara, E. Fairness and bias in artiﬁcial intelligence: A brief survey of\nsources, impacts, and mitigation strategies.Sci 6, 3 (2023).\n39. Chen, R. J. et al. Algorithmic fairness in artiﬁcial intelligence for\nmedicine and healthcare.Nat. Biomed. Eng.7, 719– 742 (2023).\n40. Timmons, A. C. et al. A call to action on assessing and mitigating bias\nin artiﬁcial intelligence applications for mental health.Perspect.\nPsychol. Sci.18, 1062– 1096 (2023).\nTable 10 | Sensitivity analysis for the contrastive loss weightλ\nλ NDCG@10 (CTM) Error rate (MQA) Fairness improvement\n(EO diff)\n0.05 70.1 ± 0.6% 20.5 ± 0.7% 30%\n0.10 70.3 ± 0.5% 19.8 ± 0.5% 35%\n0.20 70.0 ± 0.7% 20.2 ± 0.6% 32%\nValues are reported as mean ±standard deviation over 5 runs.\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 10\n41. Polevikov, S. Advancing AI in healthcare: a comprehensive review of\nbest practices.Clin Chim. Acta548, 117519 (2023).\n42. Romano, Y., Bates, S. & Candes, E. Achieving equalized odds by\nresampling sensitive attributes.Adv. Neural Inform. Process. Syst.33,\n361– 371 (2020).\n43. Roelofs, R., Cain, N., Shlens, J. & Mozer, M. C. Mitigating bias in calibration\nerror estimation.arxivhttps://arxiv.org/abs/2012.08668(2022).\n44. Xiao, T., Wang, X., Efros, A. A. & Darrell, T. What should not be\ncontrastive in contrastive learning.arXiv https://arxiv.org/abs/2008.\n05659 (2020).\nAcknowledgements\nThe research reported in this article was partially supported by the National\nInstitutes of Health awards UL1 TR001857, U24 TR004111, U01MH136020,\nand R01 LM014306. The sponsors had no role in study design, data collection,\nanalysis, interpretation, report writing, or decision to submit the paper for\npublication. We would like to thank Qiao Jin, Yifan Yang, and Zhiyong Lu from\nthe National Center for Biotechnology Information for their insightful\nexplanations of the TrialGPT results, which greatly assisted our work.\nAuthor contributions\nY.J. conceptualized, designed, and organized this study, analyzed the\nresults, and wrote, reviewed, and revised the paper. W.M. analyzed the\nresults, and wrote, reviewed, and revised the paper. S.S., H.Z., E.M.S., Z.L.,\nX.W., and S.V. wrote, reviewed, and revised the paper. Y.W. conceptualized,\ndesigned, and directed this study, wrote, reviewed, and revised the paper.\nCompeting interests\nY.W. has ownership and equity in BonaﬁdeNLP, LLC, and S.V. has\nownership and equity in Kvatchii, Ltd., READE.ai, Inc., and ThetaRho, Inc.\nThe other authors declare no competing interests.\nAdditional information\nSupplementary informationThe online version contains\nsupplementary material available at\nhttps://doi.org/10.1038/s41746-025-01576-4\n.\nCorrespondenceand requests for materials should be addressed to\nYanshan Wang.\nReprints and permissions informationis available at\nhttp://www.nature.com/reprints\nPublisher’sn o t eSpringer Nature remains neutral with regard to jurisdictional\nclaims in published maps and institutional afﬁliations.\nOpen AccessThis article is licensed under a Creative Commons\nAttribution-NonCommercial-NoDerivatives 4.0 International License,\nwhich permits any non-commercial use, sharing, distribution and\nreproduction in any medium or format, as long as you give appropriate\ncredit to the original author(s) and the source, provide a link to the Creative\nCommons licence, and indicate if you modiﬁed the licensed material. You\ndo not have permission under this licence to share adapted material\nderived from this article or parts of it. The images or other third party\nmaterial in this article are included in the article’s Creative Commons\nlicence, unless indicated otherwise in a credit line to the material. If material\nis not included in the article’s Creative Commons licence and your intended\nuse is not permitted by statutory regulation or exceeds the permitted use,\nyou will need to obtain permission directly from the copyright holder. To\nview a copy of this licence, visithttp://creativecommons.org/licenses/by-\nnc-nd/4.0/\n.\n© The Author(s) 2025\nhttps://doi.org/10.1038/s41746-025-01576-4 Article\nnpj Digital Medicine|           (2025) 8:246 11",
  "topic": "Health equity",
  "concepts": [
    {
      "name": "Health equity",
      "score": 0.5140584707260132
    },
    {
      "name": "Health risk",
      "score": 0.45185840129852295
    },
    {
      "name": "Environmental health",
      "score": 0.35033345222473145
    },
    {
      "name": "Psychology",
      "score": 0.3456863760948181
    },
    {
      "name": "Medicine",
      "score": 0.28542155027389526
    },
    {
      "name": "Economics",
      "score": 0.27664950489997864
    },
    {
      "name": "Health care",
      "score": 0.19218480587005615
    },
    {
      "name": "Economic growth",
      "score": 0.14542633295059204
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I170201317",
      "name": "University of Pittsburgh",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210134769",
      "name": "University of Pittsburgh Medical Center",
      "country": "US"
    }
  ]
}