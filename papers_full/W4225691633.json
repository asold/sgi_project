{
  "title": "NoisyTune: A Little Noise Can Help You Finetune Pretrained Language Models Better",
  "url": "https://openalex.org/W4225691633",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2108092137",
      "name": "Chuhan Wu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2142281011",
      "name": "Fangzhao Wu",
      "affiliations": [
        "Microsoft Research Asia (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2040753340",
      "name": "Tao Qi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2117581150",
      "name": "Yongfeng Huang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3035390927",
    "https://openalex.org/W3199761064",
    "https://openalex.org/W4287815000",
    "https://openalex.org/W2980708516",
    "https://openalex.org/W2945260553",
    "https://openalex.org/W3102659883",
    "https://openalex.org/W3155368131",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4287692509",
    "https://openalex.org/W3104215796",
    "https://openalex.org/W3175327901",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W3007759824",
    "https://openalex.org/W3011574394",
    "https://openalex.org/W2975185270",
    "https://openalex.org/W3126074026",
    "https://openalex.org/W2970597249",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2009979684",
    "https://openalex.org/W2952357537",
    "https://openalex.org/W3100439847",
    "https://openalex.org/W3035579820",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W3033529678"
  ],
  "abstract": "Effectively finetuning pretrained language models (PLMs) is critical for their success in downstream tasks. However, PLMs may have risks in overfitting the pretraining tasks and data, which usually have gap with the target downstream tasks. Such gap may be difficult for existing PLM finetuning methods to overcome and lead to suboptimal performance. In this paper, we propose a very simple yet effective method named NoisyTune to help better finetune PLMs on downstream tasks by adding some noise to the parameters of PLMs before fine-tuning. More specifically, we propose a matrix-wise perturbing method which adds different uniform noises to different parameter matrices based on their standard deviations. In this way, the varied characteristics of different types of parameters in PLMs can be considered. Extensive experiments on both GLUE English benchmark and XTREME multilingual benchmark show NoisyTune can consistently empower the finetuning of different PLMs on different downstream tasks.",
  "full_text": "Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics\nVolume 2: Short Papers, pages 680 - 685\nMay 22-27, 2022c⃝2022 Association for Computational Linguistics\nNoisyTune: A Little Noise Can Help You Finetune\nPretrained Language Models Better\nChuhan Wu† Fangzhao Wu‡∗ Tao Qi† Yongfeng Huang†\n†Department of Electronic Engineering, Tsinghua University, Beijing 100084, China\n‡Microsoft Research Asia, Beijing 100080, China\n{wuchuhan15, wufangzhao, taoqi.qt}@gmail.com\nyfhuang@tsinghua.edu.cn\nAbstract\nEffectively finetuning pretrained language mod-\nels (PLMs) is critical for their success in down-\nstream tasks. However, PLMs may have risks\nin overfitting the pretraining tasks and data,\nwhich usually have gap with the target down-\nstream tasks. Such gap may be difficult for\nexisting PLM finetuning methods to overcome\nand lead to suboptimal performance. In this\npaper, we propose a very simple yet effective\nmethod named NoisyTune to help better fine-\ntune PLMs on downstream tasks by adding\nsome noise to the parameters of PLMs before\nfinetuning. More specifically, we propose a\nmatrix-wise perturbing method which adds dif-\nferent uniform noises to different parameter\nmatrices based on their standard deviations. In\nthis way, the varied characteristics of different\ntypes of parameters in PLMs can be consid-\nered. Extensive experiments on both GLUE\nEnglish benchmark and XTREME multilingual\nbenchmark show NoisyTune can consistently\nempower the finetuning of different PLMs on\ndifferent downstream tasks.\n1 Introduction\nIn recent years, pretrained language models\n(PLMs) have achieved huge success in NLP (Qiu\net al., 2020). Many PLMs such as BERT (De-\nvlin et al., 2019), RoBERTa (Liu et al., 2019)\nand UniLM (Dong et al., 2019) which are pre-\ntrained from large-scale unlabeled corpus in a self-\nsupervised way, have significantly improve vari-\nous downstream tasks such as reading comprehen-\nsion (Xu et al., 2019), machine translation (Brown\net al., 2020), text classification (Bao et al., 2020),\ndialog (Wu et al., 2020) and recommendation (Wu\net al., 2021) by finetuning on these tasks.\nHow to effectively finetune PLMs to better em-\npower downstream tasks is an important research\nproblem (Zheng et al., 2021). Many existing NLP\nmethods usually directly finetune PLMs with the\n∗Corresponding author.\n(a) Standard PLM Finetuning\n(b) NoisyTune\nFinetune\nAdd Noise\nTask-Specific\nLM\nPLM\nPerturbed \nPLM\nTask Data\nPLM\nFinetune\nTask-Specific\nLMTask Data\nFigure 1: Schematic comparisons between standard\nPLM finetuning and our NoisyTune.\nlabeled data in downstream tasks (Sun et al., 2019).\nOnly a few works explore more effective and ro-\nbust PLM finetuning methods (Chen et al., 2020;\nLee et al., 2020; Aghajanyan et al., 2021; Zhang\net al., 2021; Xu et al., 2021). For example, Chen\net al. (2020) proposed RecAdam that adds a penalty\nitem to minimize the L2 distance between the fine-\ntuned models and the pretrained models, where\nthe penalty intensity is time-variant during fine-\ntuning. Lee et al. (2020) proposed Mixout which\nrandomly replaces part of the parameters in the\nfinetuned model with their original weights in the\nPLMs. These PLM finetuning methods mainly\nfocus on preventing PLMs from overfitting the lim-\nited labeled data in downstream tasks. Besides the\noverfitting of downstream task data, a rarely stud-\nied problem is that the PLMs usually overfit the\npretraining tasks and data (Qi et al., 2020), which\nmay have significant gap with the downstream task\nand data. It is not easy for existing PLM finetun-\ning methods to overcome such gap (Roberts et al.,\n2020), which may lead to suboptimal performance\nespecially when labeled data in downstream tasks\nis insufficient.\nIn order to handle this problem, in this paper\nwe propose a very simple yet effective method\nnamed NoisyTune, which can help better finetune\nPLMs for downstream tasks. Different from the\n680\nstandard finetuning paradigm (Fig. 1 (a)) which\ndirectly finetunes PLMs on the downstream task\ndata, the key idea of NoisyTune is to add a small\namount of noise to perturb PLMs parameters before\nfinetuning (Fig. 1 (b)). It can help prevent PLMs\nfrom overfitting the tasks and data in the pretrain-\ning stage, and reduce the gap between pretraining\nand downstream tasks. Since PLMs have different\ntypes of parameters which usually own different\ncharacteristics, in NoisyTune we use a matrix-wise\nperturbing method that adds uniform noise with\ndifferent intensities to different parameter matri-\nces according to their standard deviations for bet-\nter adaptation. We conduct extensive experiments\non two widely used NLP benchmarks, namely,\nGLUE (Wang et al., 2018) for English language\nunderstanding and XTREME (Hu et al., 2020) for\nmultilingual language understanding. The results\nshow NoisyTune can empower the finetuning of dif-\nferent PLMs on many different downstream NLP\ntasks to consistently achieve better performance. In\naddition, the results show NoisyTune can be eas-\nily combined with many existing PLM finetuning\nmethods and further improve their performance.\n2 NoisyTune\nThe goal of NoisyTune is for more effective finetun-\ning of PLMs on downstream tasks. The motivation\nof NoisyTune is that PLMs are well pretrained on\nsome unlabeled corpus with some self-supervision\ntasks, and they may overfit these pretraining data\nand tasks (Qi et al., 2020), which usually have gap\nwith the downstream task and data. It may be diffi-\ncult for PLMs to effectively adapt to downstream\ntasks especially when labeled data in these tasks\nare limited, which is usually the case. Motivated by\nthe dueling bandits mechanism (Yue and Joachims,\n2009) that adds randomness to the model for explo-\nration, as shown in Fig. 1, we propose to add some\nnoise to the parameters of PLMs before finetuning\nthem on downstream tasks to do some “exploration”\nin parameter space and reduce the risk of overfitting\nthe pretraining tasks and data.\nPLMs usually have different kinds of parameter\nmatrices, such as query, key, value, and feedfor-\nward network matrices (Devlin et al., 2019). Differ-\nent parameter matrices in the PLMs usually have\ndifferent characteristics and scales. For example,\nsome researchers found that the self-attention pa-\nrameters and the feed-forward network parame-\nters in Transformers have very different properties,\nsuch as rank and density (Wang et al., 2020). Thus,\nadding unified noise to all parameter matrices in\nPLMs may not be optimal for keeping their good\nmodel utility. To handle this challenge, we propose\na matrix-wise perturbing method that adds noise\nwith different intensities to different parameter ma-\ntrices according to their variances. Denote the pa-\nrameter matrices (or scalars/vectors) in a PLM as\n[W1, W2, ...,WN ], where N is the number of pa-\nrameter matrix types. Denote the perturbed version\nof the parameter matrix Wi as ˜Wi, which is com-\nputed as follows:\n˜Wi = Wi + U(−λ\n2 , λ\n2 ) ∗ std(Wi), (1)\nwhere std stands for standard deviation. The func-\ntion U(a, b) represents uniform distribution noise\nranged from a to b, and λ is a hyperparameter\nthat controls the relative noise intensity.1 We can\nsee that in NoisyTune parameters in PLMs with\nhigher variance will be added with stronger noise.\nIn addition, in some PLMs there are some con-\nstant matrices, such as token type embeddings in\nRoBERTa (Liu et al., 2019). They will not be per-\nturbed because their standard deviation is 0. It\ncan ensure that these constant matrices will not be\naccidentally activated by additional noise.\nNoisyTune is a simple and general plug-and-play\ntechnique that can be applied to the finetuning of\nany PLM on any task, simply by inserting the fol-\nlowing PyTorch-style code before finetuning:\nfor name ,para in model.named parameters ():\nmodel.state dict[name][:] +=\n(torch.rand(para.size()) −0.5)\n*noise lambda *torch.std(para)\n3 Experiments\n3.1 Datasets and Experimental Settings\nWe conduct extensive experiments on two widely\nused benchmarks for PLM evaluation. The first one\nis GLUE (Wang et al., 2018), which is a benchmark\nfor English language understanding that contains\ndifferent tasks like natural language inference, sen-\ntiment analysis and sentence similarity evaluation.\nThe second one is XTREME (Hu et al., 2020),\nwhich is a benchmark for multilingual language\nunderstanding. It covers 40 languages and contains\n1Note that U(a, b) is a matrix with the same shape with\nWi rather than a scalar.\n681\nfour groups of tasks, including sentence classifica-\ntion, structured prediction, sentence retrieval and\nquestion answering. More details of these bench-\nmarks can refer to their original papers and official\nwebsites. Since the test labels of GLUE are not\nreleased, following (Bao et al., 2020) we report re-\nsults on the dev set of GLUE. The XTREME results\nare evaluated on the test set. The hyperparameter\nλ is 0.15 on GLUE and is 0.1 on XTREME. The\nsearching range of hyperparameters in our work\nare listed in Table 1.\nHyperparameters Range\nLearning rate {7e-6, 1e-5, 2e-5, 3e-5}\nEpoch {3, 5, 7, 10, 15, 20}\nBatch size {8, 16, 32}\nNoisy intensity {0, 0.05, 0.1, 0.15, 0.2, 0.25, 0.3}\nTable 1: Searching ranges of different hyperparameters\nin our experiments.\nFollowing (Zheng et al., 2021), in sentence re-\ntrieval tasks we first train the models on the XNLI\ndataset, and then use the average of token repre-\nsentations produced by the hidden layer that yields\nthe best performance. In order not to harm the\nalignment of token embeddings across different\nlanguages, we do not add noise to the token em-\nbeddings in multilingual PLMs. We repeat exper-\niments 5 times with different random seeds and\nreport the average scores.\n3.2 Performance Evaluation\nOn the GLUE benchmark, we compare the perfor-\nmance of directly finetuning the base version of\nBERT (Devlin et al., 2019), XLNET (Yang et al.,\n2019), RoBERTa (Liu et al., 2019) and ELEC-\nTRA (Clark et al., 2020) with that of finetuning\nthem after applying NoisyTune. On the XTREME\nbenchmark, we compare the performance of di-\nrectly finetuning both base and large versions of\nXLM-R (Conneau et al., 2020) with that of their\nvariants obtained by applying NoisyTune. The\nresults on these two benchmarks are shown in\nTables 2 and 3, respectively. On the XTREME\ndatasets, we report two types of results. The first\none is zero-shot crosslingual transfer from English\nto other languages, and the second one is learning\nmodels on both English and translated data.\nAccording to these results,NoisyTune can consis-\ntently improve the performance of different PLMs\non different tasks in both English and multilingual\nsettings. In addition, the performance improvement\nbrought by NoisyTune is usually larger on relatively\nsmall datasets (e.g., RTE, CoLA and WNLI). These\nresults indicate that when labeled data in down-\nstream tasks is insufficient, it is quite difficult to\neffectively finetune PLMs starting from the origi-\nnal parameters which usually overfit the pretraining\ntasks and data. The experimental results validate\nthat NoisyTune can properly perturb PLMs with a\nlittle noise to explore different parameter spaces\nand reduce the overfitting problem, making PLMs\neasier to be adapted to downstream tasks.\n3.3 Which Noise to Use and How?\nIn this section we study which kind of noise is\nmore suitable for NoisyTune. In addition, we ex-\nplore whether our proposed matrix-wise perturb-\ning method is better than using a unified global\nnoise for all model parameters in PLMs. We com-\npare five methods, including (1)NoisyTune without\nany noise; (2) NoisyTune with a global Gaussian\nnoise; (3) NoisyTune with a global uniform noise;\n(4) NoisyTune with matrix-wise Gaussian noise; (5)\nNoisyTune with matrix-wise uniform noise. The re-\nsults on GLUE are shown in Fig. 2, and the results\non XTREME show similar patterns. We find that\nadding global noise with the same distribution to\nall the PLM parameters will harm the model perfor-\nmance. This is because different parameter matri-\nces in PLMs have very different distributions and\ncharacteristics (Wang et al., 2020). Simply adding\na unified global noise to all the parameter matrices\nis not optimal. The results show that matrix-wise\nnoise is a much better choice, since the different\ncharacteristics of different parameter matrices can\nbe taken into consideration. In addition, we find an\ninteresting phenomenon that adding uniform noise\nis better than Gaussian noise. This may be because\nGaussian noise has wider ranges and some extreme\nvalues may affect the model performance. Thus,\nwe use matrix-wise uniform noise in NoisyTune.\n3.4 Combination with Existing PLM\nFinetuning Methods\nFrom Fig. 1, it is very clear that NoisyTune is in-\ndependent of the specific PLM finetuning method,\nsince it is applied at the stage before finetuning\nPLM on the task-specific data. Thus, it is very\neasy to combine NoisyTune with any kind of ex-\nisting PLM finetuning method. In this section, we\nexplore whether NoisyTune has the potential to em-\npower the existing PLM finetuning techniques to\nachieve better performance. Here we select two\n682\nModel MNLI QNLI QQP RTE SST MRPC CoLA STS WNLI\nAcc Acc Acc Acc Acc Acc MCC PCC Acc Avg.\nBERT 84.4 91.5 90.9 67.7 93.0 87.1 58.1 89.4 54.4 79.6\nBERT+NoisyTune 84.7 91.8 91.2 68.8 93.4 88.0 59.0 90.1 56.1 80.3\nXLNET 86.6 91.6 91.2 72.9 94.4 88.1 59.6 89.6 57.5 81.3\nXLNET+NoisyTune 86.9 91.9 91.4 73.8 94.7 88.6 60.1 90.0 58.6 81.8\nRoBERTa 87.5 92.7 91.7 77.1 94.5 90.1 62.9 90.8 59.2 82.9\nRoBERTa+NoisyTune 87.8 93.1 91.9 78.8 94.9 90.6 63.6 91.1 60.3 83.6\nELECTRA 88.4 92.9 91.7 75.2 94.9 88.2 64.2 90.1 62.0 83.1\nELECTRA+NoisyTune 88.7 93.2 92.1 76.4 95.2 88.7 64.9 90.5 63.4 83.7\nTable 2: Results of different methods on the GLUE dev set.\nModel Sentence Pair Structured Prediction Sentence Retrieval Question Answering\nXNLI PAWS-X POS NER BUCC Tatoeba XQuAD MLQA TyDiQA\nMetrics Acc Acc F1 F1 Acc Acc F1/EM F1/EM F1/EM Avg.\nFine-tune multilingual model on English training set (Cross-lingual Transfer)\nXLM-Rbase 74.8 84.8 75.5 61.6 77.6 73.8 71.9/56.6 65.2/47.0 55.5/38.4 70.0\nXLM-Rbase+NoisyTune 75.2 85.1 76.0 62.1 78.2 74.5 72.3/57.1 65.5/47.4 56.0/39.2 70.5\nXLM-Rlarge 79.0 86.3 72.7 62.3 79.2 76.0 76.2/60.4 71.4/53.0 65.0/45.0 72.4\nXLM-Rlarge+NoisyTune 79.3 86.5 73.5 63.2 79.9 76.8 76.7/61.0 71.9/53.6 65.4/45.6 73.0\nFine-tune multilingual model on all training sets (Translate-Train-All)\nXLM-Rbase 78.5 88.2 76.2 62.6 79.6 79.4 75.0/61.5 67.8/50.1 63.8/47.6 73.3\nXLM-Rbase+NoisyTune 78.9 88.6 76.8 63.1 80.0 79.8 75.4/61.8 68.0/50.4 64.1/48.1 73.7\nXLM-Rlarge 82.3 90.3 77.3 67.3 82.5 82.7 80.0/65.6 72.9/54.4 66.3/47.6 76.4\nXLM-Rlarge+NoisyTune 82.5 90.5 77.8 67.9 82.9 83.0 80.4/66.1 73.3/54.9 66.8/48.2 76.8\nTable 3: Results of different methods on the XTREMRE test set.\nwell-known PLM finetuning for experiments, i.e.,\nRecAdam (Chen et al., 2020) and Mixout (Lee\net al., 2020). The experimental results are summa-\nrized in Fig. 3. We find that combining NoisyTune\nwith existing PLM finetuning techniques can fur-\nther improve their performance. This is because\nNoisyTune aims to address the overfitting of pre-\ntraining signals while these methods aim to prevent\noverfitting in downstream tasks. Thus, NoisyTune\nand these PLM finetuning methods are complemen-\ntary, and they can be empowered by NoisyTune to\nachieve better performance.\n3.5 Empirical Analysis of NoisyTune\nNext, we empirically analyze why NoisyTune can\nhelp PLM finetuning. We compare the accuracy\nof BERT with and without NoisyTune finetuned\nwith different percentage of samples on the MRPC\ndataset.2 The results are shown in Fig. 4. We\nfind NoisyTune can consistently improve PLMs un-\nder different amounts of data, especially when less\ntraining data is used. This is because the perturbed\nPLMs may have lower risks of overfitting the pre-\ntraining tasks and have better generalization abil-\nities, which is especially beneficial for finetuning\n2We observe similar patterns on other datasets.\nBERT XLNET RoBERTa ELECTRA75.0\n77.0\n79.0\n81.0\n83.0\n85.0GLUE Average Score\nw/o noise\n+ global Gaussian noise\n+ global uniform noise\n+ matrix-wise Gaussian noise\n+ matrix-wise uniform noise\nFigure 2: Different noise types and perturbing methods.\nPLMs on downstream task with limited data.\nTo further study the impact of NoisyTune on\nPLM finetuning, we show the relative changes of\nthe L1-norms of different kinds of parameters in\nthe BERT model during finetuning on the MRPC\ndataset in Fig. 5. 3 Since the noise we added to\nPLMs in NoisyTune is zero-mean uniform noise,\nthe absolute parameter L 1-norm will not change\ntoo much. However, we can see that the relative\nchange of L1-norms becomes smaller when Noisy-\nTune is applied, which indicates that the PLMs can\nfind the (sub)optimal parameters for downstream\n3The patterns on other datasets are similar.\n683\nBERT XLNET RoBERTa ELECTRA77.0\n79.0\n81.0\n83.0\n85.0GLUE Average Score\nRecAdam\nRecAdam + NoisyTune\nMixout\nMixout + NoisyTune\nFigure 3: NoisyTune can empower many existing PLM\nfinetuning methods to achieve better performance.\n20 40 60 80 100\nPercentage of Training Data\n80.0\n82.0\n84.0\n86.0\n88.0\n90.0Accuracy\nFigure 4: Influence of NoisyTune on finetuning.\n0 1 2 3 4 5 6 7 8 9\nTraining Checkpoint\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0\n1.2\n1.4Parameter L1 Norm Changes\n1e-6\nquery (w/o NoisyTune)\nkey (w/o NoisyTune)\nvalue (w/o NoisyTune)\ndense (w/o NoisyTune)\nall (w/o NoisyTune)\nquery (w/ NoisyTune)\nkey (w/ NoisyTune)\nvalue (w/ NoisyTune)\ndense (w/ NoisyTune)\nall (w/ NoisyTune)\nFigure 5: Relative changes of the L1-norm of different\ntypes of parameters in PLM during finetuning.\ntasks more easily. This result validates directly fine-\ntuning PLMs may need more updates to adapt to\ndownstream tasks, which is due to the overfitting of\npretraining tasks, and NoisyTune can provide a sim-\nple way to alleviate this problem and help finetune\nPLMs on downstream tasks more effectively.\n0.00 0.05 0.10 0.15 0.20 0.25 0.3078.0\n79.0\n80.0\n81.0\n82.0\n83.0\n84.0GLUE Average Score\nBERT\nXLNET\nRoBERTa\nELECTRA\nFigure 6: Influence of noise intensity λ.\n3.6 Hyperparameter Analysis\nWe study the influence of the most important hy-\nperparameter in NoisyTune, i.e., λ, which controls\nthe relative noise intensity. The average GLUE\nscores w.r.t. different λ values are shown in Fig. 6.\nWe find that when λ is too small or too large, the\nperformance is not optimal. This is because when\nλ is too small, it is difficult for PLMs to do param-\neter space exploration and overcome the overfitting\nproblem. While when λ is too large, the useful pre-\ntrained knowledge in PLMs may be overwhelmed\nby random noise. Values between 0.1 and 0.15 are\nmore suitable for NoisyTune on the GLUE datasets.\n4 Conclusion\nIn this paper, we propose a very simple but effective\nmethod named NoisyTune, which can help better\nfinetune PLMs on downstream tasks by adding a\nlittle noise to them before finetuning. In NoisyTune,\nwe propose a matrix-wise perturbing method that\nadds noise with different intensities to different\nkinds of parameter matrices in PLMs according\nto their variances. NoisyTune is a very general\nmethod, and is PLM model agnostic, downstream\ntask agnostic, and finetuning method agnostic. Ex-\ntensive experiments on both monolingual GLUE\nbenchmark and multilingual XTREME benchmark\ndemonstrate NoisyTune can consistently empower\nthe finetuning of different PLMs on various down-\nstream tasks to achieve better performance.\nAcknowledgments\nThis work was supported by the National Natural\nScience Foundation of China under Grant num-\nbers U1936216, U1936208, and 61862002, and\nthe research initiation project of Zhejiang Lab (No.\n2020LC0PI01).\n684\nReferences\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta,\nNaman Goyal, Luke Zettlemoyer, and Sonal Gupta.\n2021. Better fine-tuning by reducing representational\ncollapse. In ICLR.\nHangbo Bao, Li Dong, Furu Wei, Wenhui Wang, Nan\nYang, Xiaodong Liu, Yu Wang, Jianfeng Gao, Song-\nhao Piao, Ming Zhou, et al. 2020. Unilmv2: Pseudo-\nmasked language models for unified language model\npre-training. In ICML, pages 642–652. PMLR.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. NeurIPS, 33:1877–1901.\nSanyuan Chen, Yutai Hou, Yiming Cui, Wanxiang Che,\nTing Liu, and Xiangzhan Yu. 2020. Recall and learn:\nFine-tuning deep pretrained language models with\nless forgetting. In EMNLP, pages 7870–7881.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzm´an, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In ACL,\npages 8440–8451.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In NAACL-HLT, pages 4171–4186.\nLi Dong, Nan Yang, Wenhui Wang, Furu Wei, Xi-\naodong Liu, Yu Wang, Jianfeng Gao, Ming Zhou,\nand Hsiao-Wuen Hon. 2019. Unified language model\npre-training for natural language understanding and\ngeneration. In NIPS, pages 13063–13075.\nJunjie Hu, Sebastian Ruder, Aditya Siddhant, Graham\nNeubig, Orhan Firat, and Melvin Johnson. 2020.\nXtreme: A massively multilingual multi-task bench-\nmark for evaluating cross-lingual generalisation. In\nICML, pages 4411–4421. PMLR.\nCheolhyoung Lee, Kyunghyun Cho, and Wanmo Kang.\n2020. Mixout: Effective regularization to finetune\nlarge-scale pretrained language models. In ICLR.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. arXiv preprint arXiv:1907.11692.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan\nDuan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.\n2020. Prophetnet: Predicting future n-gram for\nsequence-to-sequencepre-training. In EMNLP Find-\nings, pages 2401–2410.\nXipeng Qiu, Tianxiang Sun, Yige Xu, Yunfan Shao,\nNing Dai, and Xuanjing Huang. 2020. Pre-trained\nmodels for natural language processing: A survey.\nScience China Technological Sciences, pages 1–26.\nAdam Roberts, Colin Raffel, and Noam Shazeer. 2020.\nHow much knowledge can you pack into the parame-\nters of a language model? In EMNLP, pages 5418–\n5426.\nChi Sun, Xipeng Qiu, Yige Xu, and Xuanjing Huang.\n2019. How to fine-tune bert for text classification?\nIn CCL, pages 194–206. Springer.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel Bowman. 2018. Glue:\nA multi-task benchmark and analysis platform for\nnatural language understanding. In BlackboxNLP,\npages 353–355.\nSinong Wang, Belinda Z Li, Madian Khabsa, Han Fang,\nand Hao Ma. 2020. Linformer: Self-attention with\nlinear complexity. arXiv preprint arXiv:2006.04768.\nChien-Sheng Wu, Steven CH Hoi, Richard Socher, and\nCaiming Xiong. 2020. Tod-bert: Pre-trained natural\nlanguage understanding for task-oriented dialogue.\nIn EMNLP, pages 917–929.\nChuhan Wu, Fangzhao Wu, Tao Qi, and Yongfeng\nHuang. 2021. Empowering news recommendation\nwith pre-trained language models. In SIGIR, pages\n1652–1656. ACM.\nHu Xu, Bing Liu, Lei Shu, and S Yu Philip. 2019. Bert\npost-training for review reading comprehension and\naspect-based sentiment analysis. In NAACL-HLT,\npages 2324–2335.\nRunxin Xu, Fuli Luo, Zhiyuan Zhang, Chuanqi Tan,\nBaobao Chang, Songfang Huang, and Fei Huang.\n2021. Raise a child in large language model: To-\nwards effective and generalizable fine-tuning. In\nEMNLP, pages 9514–9528.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-\nbonell, Russ R Salakhutdinov, and Quoc V Le. 2019.\nXlnet: Generalized autoregressive pretraining for lan-\nguage understanding. In NeurIPS, pages 5753–5763.\nYisong Yue and Thorsten Joachims. 2009. Interactively\noptimizing information retrieval systems as a dueling\nbandits problem. In ICML, pages 1201–1208.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q Wein-\nberger, and Yoav Artzi. 2021. Revisiting few-sample\nbert fine-tuning. In ICLR.\nBo Zheng, Li Dong, Shaohan Huang, Wenhui Wang,\nZewen Chi, Saksham Singhal, Wanxiang Che, Ting\nLiu, Xia Song, and Furu Wei. 2021. Consistency\nregularization for cross-lingual fine-tuning. In ACL-\nIJCNLP, pages 3403–3417.\n685",
  "topic": "Overfitting",
  "concepts": [
    {
      "name": "Overfitting",
      "score": 0.8910359740257263
    },
    {
      "name": "Computer science",
      "score": 0.8050665855407715
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.7286443710327148
    },
    {
      "name": "Downstream (manufacturing)",
      "score": 0.6348011493682861
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5331546068191528
    },
    {
      "name": "Noise (video)",
      "score": 0.5212322473526001
    },
    {
      "name": "Machine learning",
      "score": 0.5032944083213806
    },
    {
      "name": "Artificial neural network",
      "score": 0.09236562252044678
    },
    {
      "name": "Engineering",
      "score": 0.06489071249961853
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Image (mathematics)",
      "score": 0.0
    },
    {
      "name": "Operations management",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210113369",
      "name": "Microsoft Research Asia (China)",
      "country": "CN"
    }
  ],
  "cited_by": 37
}