{
  "title": "Roles and Utilization of Attention Heads in Transformer-based Neural Language Models",
  "url": "https://openalex.org/W3034256339",
  "year": 2020,
  "authors": [
    {
      "id": "https://openalex.org/A2413902216",
      "name": "Jae Young Jo",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    },
    {
      "id": "https://openalex.org/A2710109642",
      "name": "Sung-Hyon Myaeng",
      "affiliations": [
        "Korea Advanced Institute of Science and Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2912351236",
    "https://openalex.org/W2964174820",
    "https://openalex.org/W2962776659",
    "https://openalex.org/W2953369973",
    "https://openalex.org/W2560864221",
    "https://openalex.org/W2963400886",
    "https://openalex.org/W4288347855",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W2972324944",
    "https://openalex.org/W2963403868",
    "https://openalex.org/W2888329843",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2964165804",
    "https://openalex.org/W2963446712",
    "https://openalex.org/W2525778437",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3104033643",
    "https://openalex.org/W2964204621",
    "https://openalex.org/W3099668342",
    "https://openalex.org/W4298170715",
    "https://openalex.org/W2563574619",
    "https://openalex.org/W2511550932",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W2515741950",
    "https://openalex.org/W2946417913",
    "https://openalex.org/W2934842096",
    "https://openalex.org/W2968219088",
    "https://openalex.org/W2790235966",
    "https://openalex.org/W2962784628",
    "https://openalex.org/W131533222",
    "https://openalex.org/W2951025380",
    "https://openalex.org/W4287824654",
    "https://openalex.org/W2970565456",
    "https://openalex.org/W4288351520"
  ],
  "abstract": "Sentence encoders based on the transformer architecture have shown promising results on various natural language tasks. The main impetus lies in the pre-trained neural language models that capture long-range dependencies among words, owing to multi-head attention that is unique in the architecture. However, little is known for how linguistic properties are processed, represented, and utilized for downstream tasks among hundreds of attention heads inside the pre-trained transformer-based model. For the initial goal of examining the roles of attention heads in handling a set of linguistic features, we conducted a set of experiments with ten probing tasks and three downstream tasks on four pre-trained transformer families (GPT, GPT2, BERT, and ELECTRA). Meaningful insights are shown through the lens of heat map visualization and utilized to propose a relatively simple sentence representation method that takes advantage of most influential attention heads, resulting in additional performance improvements on the downstream tasks.",
  "full_text": "Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3404‚Äì3417\nJuly 5 - 10, 2020.c‚Éù2020 Association for Computational Linguistics\n3404\nRoles and Utilization of Attention Heads in Transformer-based Neural\nLanguage Models\nJae-young Jo\nSchool of Computing, KAIST\nDingbro AI Research\nkevin.jo@dingbro.ai\nSung-hyon Myaeng\nSchool of Computing, KAIST\nmyaeng@kaist.ac.kr\nAbstract\nSentence encoders based on the transformer\narchitecture have shown promising results on\nvarious natural language tasks. The main im-\npetus lies in the pre-trained neural language\nmodels that capture long-range dependencies\namong words, owing to multi-head attention\nthat is unique in the architecture. However,\nlittle is known for how linguistic properties\nare processed, represented, and utilized for\ndownstream tasks among hundreds of atten-\ntion heads inside the pre-trained transformer-\nbased model. For the initial goal of examin-\ning the roles of attention heads in handling\na set of linguistic features, we conducted a\nset of experiments with ten probing tasks and\nthree downstream tasks on four pre-trained\ntransformer families (GPT, GPT2, BERT, and\nELECTRA). Meaningful insights are shown\nthrough the lens of heat map visualization and\nutilized to propose a relatively simple sentence\nrepresentation method that takes advantage of\nmost inÔ¨Çuential attention heads, resulting in\nadditional performance improvements on the\ndownstream tasks.\n1 Introduction\nSentence encoders in transformer architectures as\nin GPT, BERT (Vaswani et al., 2017; Radford,\n2018; Devlin et al., 2019) and ELECTRA (Clark\net al., 2020) have shown promising results on vari-\nous natural language understanding (NLU) tasks,\nsuch as question answering, text entailment and\nnatural language inference (NLI) (Bowman et al.,\n2015), owing to their pre-training capabilities in\nmodeling languages.\nThe pre-training effects of the transformer-based\napproaches are known to be crucial for obtaining\nsuperior performance in various downstream NLU\ntasks. The main impetus lies in capturing long-\nrange dependencies among words obtainable with\nbidirectional learning and self-attention (Devlin\net al., 2019) and sufÔ¨Åciently varied corpora of a\nlarge quantity (Radford et al., 2019).\nDespite all the recent successes of the\ntransformer-based models, little is known for how\nlinguistic properties are processed and represented\ninternally when the architectures are used. Given\nthat self-attention heads are unique in the family\nof transformer architectures, we attempt to answer\nthe question of how basic linguistic properties are\ncaptured with the attention heads across the models\nand used for downstream tasks. Once we Ô¨Ågure\nout the roles of attention heads in ‚Äústoring‚Äù various\nlinguistic properties, we should be able to modulate\nthem to maximize the performance of the down-\nstream tasks.\nGiven the motivation, we analyze several pub-\nlicly available pre-trained transformer encoders\n(BERT, GPT, GPT2, and ELECTRA) trained with\ndifferent model capacities ranging from 144 to 384\nattention heads and 12 to 24 layers. Considering\nthe output vector from each attention head of an en-\ncoder as a mini-sentence embedding, we examine\nwhether certain linguistic properties are ‚Äústored‚Äù\nin embeddings among ten sentence probing tasks\n(Conneau and Kiela, 2018) that cover surface, syn-\ntactic, and semantic information and require differ-\nent linguistic properties (e.g. the depth of a parsed\nsentence). Each of the probing tasks is treated as\nif it were a downstream task for the examination;\na classiÔ¨Åer is attached for each of the primitive\nlinguistic properties. In order to predict the depth\nof the parse tree, for example, an n-ary classiÔ¨Åer\nis connected, where n is the number of possible\ndepths.\nIn order to aggregate and summarize the perfor-\nmance results out of all the attention heads, we\nconstruct an accuracy heat map for each probing\ntask, where the patterns across layers and attention\nheads can be recognized easily. By examining the\nheat map, we can observe the patterns of how the\n3405\nattention heads contribute to the accuracy of each\nprobing task, including whether an individual at-\ntention head is contributing to multiple linguistic\nfeatures together or just specialized for a particular\nfeature.\nAiming at producing improved sentence repre-\nsentation, we use the analysis result that allows for\nselecting and concatenating the outputs of superior\nattention heads. The sentence representations from\nthe hidden layers and the top-n attention heads are\ncompared to check whether using only inÔ¨Çuential\nattention heads selectively could help certain down-\nstream tasks. This attempt is in contrast with the\ncommon approach of using the output of the last\nlayers of a transformer-based encoder as the repre-\nsentation that is fed into a downstream task. Our\nhypothesis is those Ô¨Ånal representations from the\ntop of the transformer-based encoders might not\nbe the best not only in carrying primitive linguistic\nproperties of the language but also for downstream\ntasks. All the source code is publicly available1.\nThe major contribution of our research is two-\nfold: 1) we suggest an analysis method which\nhelps understand where linguistic properties are\nlearned and represented along attention heads in\ntransformer architectures and 2) we show that us-\ning analysis results, attention heads can be max-\nimally utilized for performance gains during the\nÔ¨Åne-tuning process on the downstream tasks and\nfor capturing linguistic properties.\n2 Related Work\nSeveral studies looked into the representations\nlearned by a neural network for various language\nproperties (Adi et al., 2016; Qian et al., 2016a,b).\nA similar line of work focused on learned linguis-\ntic features inside the word and sentence embed-\ndings. They used downstream tasks in order to\nprobe surface information, syntactic and semantic\ninformation (Shi et al., 2016; Conneau et al., 2018).\nSome recent work looked inside the sentence en-\ncoders with various depths, by analyzing the hidden\nstates at a layer-level (Belinkov et al., 2017; Peters\net al., 2018) and even at a neuron-level (Dalvi et al.,\n2018). Tenney et al. (2019a,b) attempted to under-\nstand linguistic characteristics learned in a series\nof pre-trained encoder models by jointly analyzing\ntheir behaviors across different NLP tasks.\nFor studying attention mechanisms, there have\n1https://github.com/heartcored98/\ntransformer_anatomy\nbeen two streams of work: 1) visual analysis of at-\ntention weights to associate various functionalities\nand 2) analysis of the characteristics of the output\nrepresentations from individual attention heads.\nFor the Ô¨Årst category, Vig and Jesse (2019) de-\nveloped a visualization tool for attention weights\nof BERT and GPT2 and identiÔ¨Åed notable heads\nbut without any quantitative analysis. Ghader and\nMonz (2017) showed the extent to which attention\nagrees with traditional alignments in neural ma-\nchine translation (MT). Jain and Wallace (2019)\nand Brunner et al. (2019) on the other hand ar-\ngued that attention rarely provides an explanation\nof model predictions. They showed through atten-\ntion map analysis that attention weights frequently\nare not correlated with other measures of feature\nimportance.\nFor the second category that attempts to discover\nvarious roles attention heads play, Raganato and\nTiedemann (2018) studied the characteristics of in-\ndividual attention heads from the transformer, pre-\ntrained with an MT task and evaluated on a limited\nsuite of linguistic tasks, POS tagging, NER tag-\nging, and chunking. Similarly, Clark et al. (2019)\nshowed that some attention heads are specialized\nfor dependency parsing and coreference resolution.\nMichel et al. (2019) showed through an ablation\nstudy that some dedicated heads have a signiÔ¨Åcant\nrole in MT and revealed the dynamics of atten-\ntion heads during the training process. V oita et al.\n(2019) provided a method to identify the major\nrole of each attention head in a transformer model\ntrained for MT. The two studies are limited to MT\nand a particular transformer model, BERT.\nUnlike the recent studies mentioned above, our\nanalysis is more comprehensive in its scope for\ngeneralizability. The analysis probes a variety of\nsurface, syntactic, and semantic information at sen-\ntence levels with different transformer encoders\npre-trained on language modeling tasks. More im-\nportantly, our work goes beyond an analysis and\nsuggests a method of utilizing the analysis results\nfor performance gains on several downstream tasks.\nIt not only proposes a simple yet new method for\nthe downstream tasks but also validates the analy-\nsis of the attention mechanisms. To the best of our\nknowledge, this is the Ô¨Årst attempt to do an in-depth\nanalysis of the seven recent pre-trained encoders\nfor their internal workings in handling linguistic\nfeatures, not to mention the newly proposed way\nfor improvements on the downstream tasks.\n3406\nPositional \nEncoding\nInput Embeddingùë•!\n...\n‚Ñé\",$\t\n...\n‚Ñé\",%\t‚Ñé\",&\t\nMulti-Head Attention\nHidden States ùëß\"\t\nAdd & Normalization\nAdd & Normalization\nùëñ-th Encoding Layer\nùêø√ó\nFeed Forward\nI\nùë•!\nùëß'\t\n(a) Basic Architecture of    \nTransformer-based Encoder\nùë•\"\nhave ùë•#\nan ùë•$\napple\nùëß\"\t\nùëô\nùëô(s(ùëß\")\n(b) Layer-wise Evaluation\n...\nLinear\nSoftmax\nClassifier\n‚Ñé\",$\t\nùëô\nùëô(s(‚Ñé\",$)\n(c) Head-wise Evaluation\nLinear\nSoftmax\nClassifier\nFigure 1: (a) Basic architecture of a transformer-based\nencoder. (b) Evaluation scheme for a hidden state zi.\n(c) Evaluation scheme for an attention head outputhi,j.\nL and H denote the number of stacked encoding layers\nand the number of attention heads packed within each\nencoding layer, respectively.\n3 Methodology\nConsider a transformer-based encoder M, typ-\nically with a stack of L identical layers, each\nof which makes use of multi-head self-attention,\nand a two sub-layer feed-forward network cou-\npled with layer normalization and residual connec-\ntion (see Figure 1a). For a given input sequence\nx = (x1, x2, . . . , xn), each word embedding xis\nconcatenated with a positional encoding and fed\ninto the encoder layer to produce an attention head\noutput hi,j ‚ààR dhead where i and j indicate the\nindices of the layer and the attention head, respec-\ntively. Then a series of sub-layers produce hid-\nden states of the i-th encoding layer zi ‚ààR dmodel\nfor each encoder. For all pre-trained encoders,\ndhead = 64 and dmodel = H √ódhead where H\nis the number of attention heads per layer.\nSince the transformer-based encoders encode\nthe input sequence word by word, zi and hi,j are\nproduced individually for given word xk along the\ninput sequence x. In order to produce a sequence-\nlevel representation, we need to select one of the\ninput representations of the sequence. Since the\nselection method depends on the chosen pre-trained\nmodel, we defer a detailed discussion to Section 4.1.\nFor now, we assume zi and hi,j have been already\ndetermined with the speciÔ¨Åc word chosen from the\ninput sequence and consider it as the sentence-level\nrepresentation.\n(a) Selecting Influential \nAttention Head\n...\n...\n...\n...\n......\nRed color denotes the attention\nhead whose accuracy s(‚Ñé!,# ) is\nrelatively higher than others\nùëô\nùëô%\ns(‚Ñé$)\n(b) Reconstructed \nEmbedding Evaluation\n\t‚Ñé$\nLinear\nSoftmax\nClassifier\nFigure 2: Selecting inÔ¨Çuential attention head output\nbased on attention head-wise evaluation result. For ex-\nample, assume colored three attention heads produce\nmost superior representation then we concatenate the\noutput from those attention head and use it as a sen-\ntence embedding.\n3.1 Evaluating Hidden States on a Layer\nConsider a classiÔ¨Åcation task where the pre-trained\nencoder predicts a linguistic feature intended in a\nsentence probing task. Assume we have a labeled\ndataset containing pairs of a sentence and a linguis-\ntic property label (e.g. tense). For a given sentence\nx and a label l in the dataset, the pre-training model\n(e.g. BERT) encodes x and produces vectors corre-\nsponding to zi and hi,j.\nUsually, only the vector from the last layer zi=L\nis used as the input feature representing the sen-\ntence for the classiÔ¨Åcation task. However, in order\nto inspect the role of each internal layer for a lin-\nguistic property, we use {zi,l, l}for all i to train\na logistic regression classiÔ¨Åer on a train dataset\nand record classiÔ¨Åcation accuracy s(zi) on a test\ndataset (see Figure 1b). Each accuracy score is then\ncompared to the accuracy of the last layer, and then\nthe best performance among the encoding layers\nis measured. We consider this comparison as a\nway of generating primitive evidence that hidden\nstates from an internal layer provide more useful\nlinguistic information than the representation from\nthe last layer.\n3.2 Evaluating Attention Heads\nSimilar to Section 3.1, we also train a logistic re-\ngression classiÔ¨Åer on {hi,j, l}and record classiÔ¨Åca-\ntion accuracy s(hi,j) for all i and j. That is, every\nattention head is evaluated by feeding its own out-\nput vector to the classiÔ¨Åer as a feature (see Figure\n1c). We assume the more an attention head ‚Äústores‚Äù\nthe information essential to the probing task, the\nhigher its accuracy.\nWe construct a heat map of classiÔ¨Åcation accu-\n3407\nEncoder L H L√óH Parameters\nGPT 12 12 144 110M\nGPT2 12 12 144 117M\nBERTBASE 12 12 144 110M\nBERTLARGE 24 16 384 340M\nELECTRASMALL 12 4 48 14M\nELECTRABASE 12 12 144 110M\nELECTRALARGE 24 16 384 340M\nTable 1: SpeciÔ¨Åcation of the seven pre-trained en-\ncoders: the numbers of encoding layers ( L), attention\nheads per layer (H), all the attention heads used (L√óH)\nand trained parameters.\nracy for attention heads on x-axis and layers on\ny-axis, so that we can easily identify the distribu-\ntion of the excited attention heads for the linguistic\nproperty handled in the pre-trained model. The\noverall trend of a heat map indicates the extent to\nwhich the activation is widely distributed or local-\nized across different layers and attention heads.\n3.3 Using InÔ¨Çuential Attention Heads\nGiven the analysis results, we now propose a\nmethod for generating a new sentence representa-\ntion to improve not only the probing tasks but also\nother downstream tasks. New representations are\ntested within the chosen pre-trained models in this\nwork but can be applied to all other transformer-\nbased encoders.\nGiven an encoder model M, we sort the atten-\ntion heads along with their classiÔ¨Åcation ‚Äòvalida-\ntion‚Äô accuracy s(hi,j) measured on a validation\ndataset (in order to prevent look-ahead bias during\nthe selection process) for a given task, based on the\nattention head-wise evaluation method as in Sec-\ntion 3.2. Then top-n attention heads are selected\nand simply concatenated (see Figure 2) to form a\nnew representation. We expect that the resulting\nvector hn ‚ààR n√ódhead would be able to store more\nprecious information for the task than the vectors\nconstructed out of other attention heads since it\nconsists of superior attention heads.\nIn order to make comparisons against the em-\nbeddings from different encoding layers, we also\ntrain the classiÔ¨Åer with {hn, l}and record the corre-\nsponding classiÔ¨Åcation ‚Äòtest‚Äô accuracys(hn) mea-\nsured on the test dataset. For fair comparisons,\nhowever, we set n to H (the number of attention\nheads per layer) so that reconstructed sentence em-\nbedding hn could have the same dimension to that\nof hidden states, dmodel.\nTasks # ClassesTask Description\nLength 6 Predict input sequence length\nWordContent1000 Find words in a sentence\nDepth 8 Predict maximum depth of syntactic tree\nTopConst 20 Predict top-constituents\nBigramShift 2 Detect bigram order perturbation\nTense 2 Predict main verb‚Äôs tense\nSubjNum 2 Predict whether a subj is plural\nObjNum 2 Predict whether an obj is plural\nOddManOut 2 Detect noun or verb perturbation\nCoordInversion2 Detect clausal order perturbation\nTable 2: Summary of sentence probing tasks. Each task\nconsists of 100k train and 10k test samples.\n4 Attention Head-wise Analysis\n4.1 Pre-trained Transformer Encoders\nWe ran experiments for seven different encoders\nwith unique characteristics, as shown in Table 1.\nGPT (Radford, 2018) was trained by basic Lan-\nguage Modeling (LM) on the BookCorpus dataset.\nGPT2 (Radford et al., 2019) was originally trained\nwith the largest model capacity (1.5B parame-\nters) with massive text dataset and LM, but we\nselect base model for fair comparison. BERT\n(Devlin et al., 2019), which adopted masked LM\n(MLM) with next sentence prediction (NSP) for\nbetter contextualized embedding, was trained on\nBook-Corpus and English Wikipedia datasets. The\nmost recent one, ELECTRA, was trained with\nreplaced token detection (RTD) in the generator-\ndiscriminator mode.\nFor GPT and GPT2, we pulled the representative\nsentence embedding zi and hi,j from the last input\ntoken with Byte-Pair Encoding tokenizer (Sennrich\net al., 2016). For the BERT and ELECTRA fam-\nily, we appended a special token <CLS>, which\nwas originally designed to train sentence represen-\ntations, in front of every input sequence and pulled\nthe sentence embedding from it, using WordPiece\ntokenizer (Wu et al., 2016). Also, the implementa-\ntion of the all transformers in our work are utilized\nfrom the Huggingface‚Äôs transformers library (Wolf\net al., 2019).\n4.2 Evaluation on Sentence Probing Tasks\nTen sentence probing tasks enable us to check\nwhether the sentence embeddings generated by the\nencoders store the linguistic properties speciÔ¨Åc to\nthe individual tasks. Table 2 shows a description\nof each probing task with its number of classes,\nroughly indicating the difÔ¨Åculty of the task. For\neach probing task, we evaluated performance of\nthree types of representation; s(zi), s(hi,j) and\n3408\nBERTBASEBERTLARGEGPTGPT2ELECTRALARGE\nLengthDepthSubjNumBigramShiftCoordInversionOddManOut\nFigure 3: Heat maps of attention head-wise evaluation on sentence probing tasks. The rows correspond to the Ô¨Åve\npre-trained encoders (BERTBASE, BERTLARGE, GPT, GPT2, and ELECTRALARGE from the top). The six columns\ncorrespond to six tasks (Length, Depth, SubjNum, BigramShift, CoordInversion, and OddManOut, from the left).\nIn each heat map, x-axis and y-axis show the index values of the attention heads and the layer numbers (the lower,\nthe closer to the initial input), respectively. The brighter the color, the higher the accuracy for the attention head and\nhence more critical for the task. Note that the attention heads in the same layer are ordered by their classiÔ¨Åcation\naccuracy values (i.e., an attention head with the highest accuracy on a layer is at the left-most location).\ns(hn) for a given pre-trained encoder by training\nthe simple classiÔ¨Åer with 256 batch size on the\nRMSProp optimizer (details on Appendix A).\n4.3 Heat maps for Roles of Attention Heads\nAfter measuring the classiÔ¨Åcation accuracy for us-\ning the representation from each attention head,\ns(hi,j) for all i,j, we created a heat map showing\nthe accuracy distribution for a pre-trained encoder\nand a sentence probing task. Figure 3 shows 30 heat\nmaps arranged for seven pre-trained encoders and\nsix sentence probing tasks (full results are shown\nin Appendix B). For each heat map, the brighter\nthe color in a region, the higher the accuracy is for\nthe corresponding attention heads.\nComparing the heat maps along with the differ-\nent probing tasks for an encoder, we can see that the\ninÔ¨Çuential attention heads with bright colors appear\nin different layers, either localized or distributed.\nThis indicates that the information related to dif-\nferent tasks is processed at different locations and\nwith different levels of association among attention\nheads. For the Length and Depth tasks, requiring\nsurface and syntactic information, for example, the\naccuracy of the heads in the lower layers starts to\ndiminish from the mid-upper layers.\nOn the other hand, the attention heads in the mid-\nlayers are activated for SubjNum and CoordInver-\nsion, which are more or less syntactic information.\nFor BigramShift and OddManOut, which are more\nsemantic, the attention heads along the upper layers\nare mostly activated. These results provide more\ndetailed analyses and meaningful insights regard-\ning the behavior of attention heads on different\nlayers than the empirical results of Raganato and\nTiedemann (2018) who shows the attention heads\nin lower and upper layers of the basic transformer\ntend to embed syntactic information and seman-\ntic information, respectively. More interestingly,\nthe BigramShift and OddManOut heat maps show\nthat all of the Ô¨Åve encoder models represent word\norders and verb/noun contexts starting from the\n3409\nTasks\nEncoder\nBERTBASE BERTLARGE\nlast best top-12 last best top-16\nlayer layer heads layer layer heads\nLength 58.0 87 .8 95.0 54.8 94 .4 95.2\nWordContent25.2 25 .2 73.1 12.2 32 .2 79.8\nDepth 29.8 31 .7 38.3 27.8 33 .3 39.5\nTopConst 69.8 74 .7 84.2 62.8 78 .2 85.6\nBShift 78.1 78 .1 88.3 77.2 81 .1 90.9\nTense 86.0 87.0 89.0 85.6 86 .7 88.9\nSubjNum 82.0 84.7 88.2 80.0 87 .9 90.5\nObjNum 75.4 75.4 83.4 64.2 78 .0 84.4\nOddManOut59.6 59.6 65.1 55.5 59 .2 69.0\nCoordInv 65.5 65.9 74.6 64.9 70 .9 78.5\nTable 3: A summary of the probing tasks for three\ndifferent embedding methods used in the pre-trained\nBERT architectures.\nmid-layers.\nComparing the heat maps along with the trans-\nformer types, we can observe that the heatmaps\nwithin the same family show similar patterns, while\nthose from different families tend to show different\ndistributions of the superior attention heads. For ex-\nample, the GPT family tends to show cooperation\nwith a larger number of attention heads for the Sub-\njNum and CoordInversion tasks while the BERT\nfamily consists of only a few ‚Äúwell-educated‚Äù at-\ntention heads. In the case of BigramShift and Odd-\nManOut, the majority of upper attention heads of\nthe BERT family are more strongly associated with\nword order and verb/noun meanings with higher\naccuracy than those of the GPT family.\nInterestingly, ELECTRA LARGE shows unique\npatterns for most of the probing tasks; high-\nperformance heads are located on lower layers ex-\ncept for OddManOut, whereas the heads on the\nlower layers do not seem to deal with informa-\ntion for the probing tasks. ELECTRA SMALL and\nELECTRABASE model have similar heat maps (see\nAppendix B), but the ELECTRALARGE model is to-\ntally different from them. These tendency implies\nthat the learning behaviors on the attention heads\nare not strictly similar among each other for the\nsame pre-training tasks even with the same archi-\ntecture.\n4.4 Selecting InÔ¨Çuential Attention Heads\nHaving observed that different attention heads on\ndifferent layers play their roles for different prob-\ning tasks, we devised a method of producing new\nembeddings as in 3.3 and ran an experiment to\ncompare it against two baselines for the ten prob-\ning tasks. Table 3 reports on a comparison result of\nthree embeddings constructed by the BERT family:\nthe last layer zi=L, the best-performing layer zbest,\nand the reconstructed sentence embedding hn=H\nfor each task and each pre-trained encoder (full\nresults are in Appendix B). Comparing the accu-\nracy between the last and best layers, we observe\nthat the last layer is no better than the ‚Äúbest‚Äù layers\nfor any of the probing tasks. From this, we can\ninfer that certain linguistic features are dominantly\nprocessed on earlier layers and no further on later\nlayers.\nThe performance comparison between using the\noutput of the ‚Äúbest‚Äù layer and the reconstructed\nsentence embedding (proposed) clearly shows that\nclassiÔ¨Åcation accuracy is increased signiÔ¨Åcantly\n(19.22% in median) with the proposed method for\nalmost all the tasks. It strongly supports that the\nproposed method can be employed to discover su-\nperior attention heads that can make up the Ô¨Ånal\nrepresentation for processing speciÔ¨Åc linguistic in-\nformation. Note that the newly constructed sen-\ntence embeddings consist of attention head out-\nputs only. Our results imply that these embeddings\nmight possess substantial information as much as\nthe hidden states of the layers, which are produced\nby passing through the multi-head attention layers\nand the feed-forward network.\n5 Boosting Downstream Task\n5.1 Downstream Tasks from GLUE\nWe evaluated the new embedding construction\nmethod for more complex tasks in order to see\nwhether it extracts not only simple linguistic fea-\ntures but also rich sentence features from the\npre-trained encoder for such tasks. Three down-\nstream tasks (MRPC, STS-B, and SST-2) were\nselected from the General Language Understand-\ning Evaluation (GLUE) benchmark, which has the\nmost widely used datasets for evaluating language-\noriented task performances.\nMRPC Microsoft Research Paraphrase Corpus\nconsists of 4.1k train and 1.7k test sentence pairs\nautomatically extracted from online news sources,\nwith human annotations for whether the sentences\nin the pair are semantically equivalent (Dolan and\nBrockett, 2005).\nSTS-B The Semantic Textual Similarity Bench-\nmark is a collection of 5.7k train and 1.4k test sen-\ntence pairs drawn from news headlines and other\nsources (Cer et al., 2017). They were annotated\nwith a score from 1 to 5 denoting how similar the\n3410\nTasks\nEncoder\nBERTBASE BERTLARGE\nlast best top-12 last best top-16\nlayer layer heads layer layer heads\nMRPC (F1) 88.0 88.2 88.9 89.3 88.6 91.4\nMRPC (Acc)82.4 83.1 84.6 84.6 84.1 87.7\nSTS-B (P)* 88.2 74.6 88.6 89.5 54.8 89.4\nSTS-B (S) 87.9 73.5 88.3 89.1 53.6 88.7\nSST-2 (Acc)92.9 92.4 93.1 94.0 92.9 94.5\nTable 4: A summary of three downstream tasks on dev\nset for the ordinary Ô¨Åne-tuning method using the last\nlayer, best layer, and the proposed method of using top-\nn attention heads. The reported scores are the median\nover 5 random restarts. (* P and S denote the pearson\nscore and spearman score, respectively.)\ntwo sentences are for their semantics.\nSST-2 The Stanford Sentiment Treebank is a\nbinary single-sentence classiÔ¨Åcation task consisting\nof sentences extracted from movie reviews with\nhuman annotations of their sentiment (Socher et al.,\n2013) with 67k train and 1.8k test samples. It was\ndesigned to predict the sentiment score for a given\nsentence in binary scales.\n5.2 Fine-tuning with InÔ¨Çuential Heads\nFirst, we evaluated each of the attention heads on\nthe three downstream tasks, following the proce-\ndure in Section 4.2. Using the head-wise evaluation\nresults, we again reconstructed sentence embed-\ndings from the output vectors of superior attention\nheads and use them as input representations for\nthe downstream task classiÔ¨Åer. Since pre-trained\ntransformer encoders are usually Ô¨Åne-tuned when\napplied to the downstream tasks, we unfroze the pa-\nrameters of the pre-trained encoder and Ô¨Åne-tuned\nboth the classiÔ¨Åer and the encoder, end to end. Also,\nwe conducted regular Ô¨Åne-tuning experiments by\nadding a classiÔ¨Åer on the top of the last hidden vec-\ntors for each pre-trained encoder. We use a batch\nsize of 32 with a learning rate of 2e-5 and Ô¨Åne-tune\nfor 3 epochs over the data for all the three down-\nstream tasks, following the Ô¨Åne-tuning procedure in\n(Devlin et al., 2019). Each experiment is repeated\nÔ¨Åve times with different random seeds to provide\nfair comparisons against the performance variance\nof Ô¨Åne-tuning on small datasets.\nThe results are presented in Table 4. Both\nBERTBASE and BERTLARGE obtained additional\nperformance gains, 0.82% and 1.04% points for\nthe base and large models, respectively, over the\nmodel with the ordinary last-layer Ô¨Åne-tuning. We\nÔ¨Ånd that BERTLARGE receives an additional perfor-\nmance gain on the MRPC task by 2.1% and 3.1%\npoint improvements on F1 and accuracy, respec-\ntively. Fine-tuning with attention heads only gives a\nslightly negative result on STS-B with BERTLARGE.\nFine-tuning with the best-layer did not provide con-\nsistent performance increment. It is noteworthy\nthat the performance of an already pre-trained en-\ncoder model could be further improved by simply\npulling the output vectors from the inÔ¨Çuential at-\ntention heads.\n6 Discussion\n6.1 Heat Map Variations along Fine-tuning\nIn order to investigate the impact of the Ô¨Åne-tuning\nprocess toward the internal attention heads, we also\nconducted the attention head-wise evaluation on\neach encoder after three epochs of the Ô¨Åne-tuning\nprocess. Our question was whether the inÔ¨Çuential\nattention heads at the initial pre-trained state would\nremain superior after the Ô¨Åne-tuning or the spot of\ninÔ¨Çuential heads would be shifted toward the last\nlayer.\nThe results are presented in Figure 4. First, we\nagain observe that the regions of the inÔ¨Çuential\nheads vary among the downstream tasks. In the\nMRPC task, inÔ¨Çuential heads are distributed across\nthe entire layers and heads, but the ones with the\nSST-2 task are highly concentrated toward the very\nupper layer. Notably, the heat maps of the STS-\nB task are unusual in that there are two inÔ¨Çuen-\ntial regions in the lower (Ô¨Årst 25Àú30% layers) and\nthe upper layers. We can also observe that the\noverall heat map patterns are stretched while the\nmodel capacity is increased, as reported in (Ten-\nney et al., 2019a). From the way feature vectors\nare pulled from the encoder, we observe that Ô¨Åne-\ntuning with the reconstructed sentence embeddings\nobtained from the top- n attention heads results\nin the smoother heatmap ampliÔ¨Åcation, especially\nwith the BERTLARGE model.\nThe most interesting result is that the intensity\n(performance) of the initial heatmaps are ampliÔ¨Åed\nafter experiencing the Ô¨Åne-tuning process while\npreserving overall distribution patterns. Another\nphenomenon is that the attention heads adjacent to\nthe superior ones also give a slight performance in-\ncrease. These results imply that the Ô¨Åne-tuning pro-\ncess leverages the initial superior attention heads re-\ngardless of their corresponding locations inside the\nmodel rather than trains arbitrary attention heads.\nThis behavior might be the reason for explaining\n3411\nInitial Pre-trainedFine-tuned with Last LayerFine-tuned with Top-n HeadsInitial Pre-trainedFine-tuned with Last LayerFine-tuned with Top-n HeadsBERTBASE\n BERTLARGE\nMRPCSTS-BSST-2\nFigure 4: Heat maps of attention head-wise evaluation on downstream tasks. The rows correspond to the three\ntasks (MRPC, STS-B and SST-2 from the top). The Ô¨Årst three column groups are evaluation results of BERT BASE\nand second three column groups are evaluation results of BERT LARGE. Each column groups correspond to the\ninitial pre-trained state, Ô¨Åne-tuned with last layer and Ô¨Åne-tuned with top-n attention heads, respectively. In each\nheat map is drawn following the procedure of Figure 3. Note that the heat maps in the same row within the same\nencoder model share the same color bar range in order to compare performance changes.\nthe additional performance increment on the down-\nstream tasks. We conjecture that our reconstruction\nmethod could act as a partial residual connection\nas in DenseNet (Huang et al., 2017) during the\nÔ¨Åne-tuning process by feeding the reconstructed\nembedding to the input of the classiÔ¨Åer which cre-\nates the direct gradient Ô¨Çow from the Ô¨Ånal objec-\ntive loss of downstream tasks toward the internal\nsuperior attention heads. We believe that further\nwork by varying the number of concatenated the\nattention heads (especially, n > H) would provide\nadditional performance gain.\n6.2 Syntactic-Semantic Exclusivity\nOur analysis so far concentrated on the distribution\nof the inÔ¨Çuential attention heads on different layers\nfor given task as a way of differentiating their roles\nfor individual tasks. A pattern we observed was\nthat different number of heads are inÔ¨Çuential and\nthat upper, lower, or all the layers tend to be inÔ¨Çu-\nential, depending on the linguistic tasks. Our next\nquestion is whether individual heads on different\nlayers are ‚Äùresponsible‚Äù for processing syntactic or\nsemantic properties exclusively or in a coordinat-\ning fashion. In order to observe the performance of\nattention head hi,j for syntactic and semantic tasks,\nwe deÔ¨Åne a score for handling syntactic capabilities\nas an average of test accuracy scores, s(hi,j), from\nthe [Depth, TopConstituents, BigramShift] group\nand that for semantic capabilities from the [Tense,\nSubjNumber, ObjNumber, OddManOut, Coordina-\ntionInversion] group. We omit the accuracy results\nfrom the surface information group since they it is\ndifÔ¨Åcult to lablem as syntactic or semantic.\nFigure 5 shows the syntactic-semantic score dis-\ntributions of the attention heads for different pre-\ntrained transformer models. Each attention head\nseems to handle both syntactic and semantic in-\nformation in a balanced way. This is interesting\nbecause different attention heads or layers are often\nmore inÔ¨Çuential for many linguistic tasks. When\naveraged together over the tasks for either the syn-\ntactic or semantic group, however, it appears that\nprocessing syntactic and semantic information is\nshared by individual heads and layers. There is a\ntendency that the lower the layer, the less inÔ¨Çuen-\ntial on syntactic and semantic processing. However,\nthis tendency is not observed in the large models.\nFor BERTLARGE, the highest layers (purple col-\nors) contribute less for both syntactic and semantic\nproperties. For ELECTRALARGE, the purple heads\ncontribute the least. It re-conÔ¨Årms our hypothesis\nthat using the last layer representation is not always\nthe best. The linear relationship between syntac-\n3412\n50\n55\n60\n65\n70semantic\nBERT-Base\nlayer\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23\nBERT-Large\n50\n55\n60\n65\n70semantic\nGPT\n GPT2\n50\n55\n60\n65\n70semantic\nELECTRA-Small\n30 40 50\nsyntactic\nELECTRA-Base\n30 40 50\nsyntactic\n50\n55\n60\n65\n70semantic\nELECTRA-Large\nFigure 5: A distribution of syntactic and semantic\nscores of the attention heads. In each scatter plot, x-\naxis and y-axis show the syntactic and semantic scores,\nrespectively. The hue of a point represents the layer to\nwhich the corresponding attention head belongs.\ntic and semantic processing capabilities across the\nheads is considered a new Ô¨Ånding. Although differ-\nent layers and heads tend to play stronger or weaker\nroles for different linguistic properties as shown in\nthe heat maps, they contribute to both syntactic and\nsemantic processing in a well balanced way.\n7 Conclusion\nWhile recent research demonstrated the capability\nof the transformer-based encoders for generating\nrich sentence representations, the roles of individ-\nual self-attention heads were hardly unknown. Fur-\nthermore, little is known for whether and how we\ncan utilize them for better capturing linguistic prop-\nerties and eventually improving the performance\nof downstream tasks for which the embeddings are\nconstructed.\nOne of the major contributions of this paper is\nto Ô¨Åll the void by inspecting where and how the\nattention heads are ‚Äútrained‚Äù internally for classi-\nÔ¨Åcation tasks corresponding to different linguistic\nproperties and for the downstream tasks. The anal-\nysis results clearly show a tendency through the\ncomprehensive heat maps that syntactic and seman-\ntic information is mainly handled from the lower\nlayers to the upper layers. We also showed that un-\nderstanding the roles of attention heads in handling\ntask-speciÔ¨Åc information can help to develop adap-\ntive sentence representations, by selecting inÔ¨Çuen-\ntial attention heads and testing them for the three\ndownstream tasks. The additional performance\ngains obtained by the simple method show that this\napproach of using the anatomy of the transformer\nmodels and the attention heads is promising in uti-\nlizing expensive pre-trained transformer models to\ntheir maximal extent.\nFurthermore, we explored how the hundreds of\nattention heads underwent performance variation\nduring the Ô¨Åne-tuning process on the downstream\ntasks, revealing the internal behaviors with the pro-\nposed analysis method. The analysis of syntactic-\nsemantic score distributions revealed that individ-\nual attention heads capture both syntactic and se-\nmantic information. It also showed that the amount\nof both syntactic and semantic information handled\nby the heads vary from layer to layer, sometimes\nshowing that the last layer contributes much less\nespecially with large models.\nWhile the empirical results are strong, additional\nwork remains to further our understanding of the in-\nternal workings of the transformer architecture and\nits role in building such strong language models for\na variety of tasks. Immediate attention should be\npaid to the investigation of how heat maps would\nvary during the extensive pre-training so that we\nhave a better understanding of the dynamics of the\nlearning processes.\nAcknowledgments\nThis work was supported by Institute for Informa-\ntion & communications Technology Planning &\nEvaluation(IITP) grant funded by the Korea govern-\nment(MSIT) (No. 2013-0-00131, Development of\nKnowledge Evolutionary WiseQA Platform Tech-\nnology for Human Knowledge Augmented Ser-\nvices). We are grateful for the support of the GPU\nserver to IdeaFactory, Startup KAIST. We also ap-\npreciate Kyubyong Park, Seongok Ryu, and YJ for\nreviewing the earlier version of this paper.\n3413\nReferences\nYossi Adi, Einat Kermany, Yonatan Belinkov, Ofer\nLavi, and Yoav Goldberg. 2016. Fine-grained anal-\nysis of sentence embeddings using auxiliary predic-\ntion tasks. CoRR, abs/1608.04207.\nYonatan Belinkov, Nadir Durrani, Fahim Dalvi, Has-\nsan Sajjad, and James Glass. 2017. What do neu-\nral machine translation models learn about morphol-\nogy? In Proceedings of the 55th Annual Meeting of\nthe Association for Computational Linguistics (Vol-\nume 1: Long Papers), pages 861‚Äì872, Vancouver,\nCanada. Association for Computational Linguistics.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts,\nand Christopher D. Manning. 2015. A large anno-\ntated corpus for learning natural language inference.\nIn Proceedings of the 2015 Conference on Empiri-\ncal Methods in Natural Language Processing, pages\n632‚Äì642, Lisbon, Portugal. Association for Compu-\ntational Linguistics.\nGino Brunner, Yang Liu, Dami ¬¥an Pascual, Oliver\nRichter, and Roger Wattenhofer. 2019. On the va-\nlidity of self-attention as explanation in transformer\nmodels. arXiv preprint arXiv:1908.04211.\nDaniel Cer, Mona Diab, Eneko Agirre, I Àúnigo Lopez-\nGazpio, and Lucia Specia. 2017. SemEval-2017\ntask 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. In Proceedings\nof the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pages 1‚Äì14, Vancouver,\nCanada. Association for Computational Linguistics.\nKevin Clark, Urvashi Khandelwal, Omer Levy, and\nChristopher D. Manning. 2019. What does BERT\nlook at? an analysis of bert‚Äôs attention. CoRR,\nabs/1906.04341.\nKevin Clark, Minh-Thang Luong, Quoc V . Le, and\nChristopher D. Manning. 2020. ELECTRA: Pre-\ntraining text encoders as discriminators rather than\ngenerators. In ICLR.\nAlexis Conneau and Douwe Kiela. 2018. SentEval:\nAn evaluation toolkit for universal sentence repre-\nsentations. In Proceedings of the Eleventh Interna-\ntional Conference on Language Resources and Eval-\nuation (LREC-2018), Miyazaki, Japan. European\nLanguages Resources Association (ELRA).\nAlexis Conneau, German Kruszewski, Guillaume Lam-\nple, Lo¬®ƒ±c Barrault, and Marco Baroni. 2018. What\nyou can cram into a single $&!#* vector: Probing\nsentence embeddings for linguistic properties. In\nProceedings of the 56th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume 1:\nLong Papers), pages 2126‚Äì2136, Melbourne, Aus-\ntralia. Association for Computational Linguistics.\nFahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan\nBelinkov, Anthony Bau, and James R. Glass. 2018.\nWhat is one grain of sand in the desert? analyzing\nindividual neurons in deep nlp models. In AAAI.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, Volume 1 (Long and Short Papers),\npages 4171‚Äì4186, Minneapolis, Minnesota. Associ-\nation for Computational Linguistics.\nWilliam B. Dolan and Chris Brockett. 2005. Automati-\ncally constructing a corpus of sentential paraphrases.\nIn Proceedings of the Third International Workshop\non Paraphrasing (IWP2005).\nHamidreza Ghader and Christof Monz. 2017. What\ndoes attention in neural machine translation pay at-\ntention to? In Proceedings of the Eighth Interna-\ntional Joint Conference on Natural Language Pro-\ncessing (Volume 1: Long Papers) , pages 30‚Äì39,\nTaipei, Taiwan. Asian Federation of Natural Lan-\nguage Processing.\nGao Huang, Zhuang Liu, Laurens van der Maaten, and\nKilian Q. Weinberger. 2017. Densely connected con-\nvolutional networks. In The IEEE Conference on\nComputer Vision and Pattern Recognition (CVPR).\nSarthak Jain and Byron C. Wallace. 2019. Attention is\nnot explanation. CoRR, abs/1902.10186.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? In H. Wal-\nlach, H. Larochelle, A. Beygelzimer, F. dAlch¬¥e-Buc,\nE. Fox, and R. Garnett, editors, Advances in Neural\nInformation Processing Systems 32, pages 14014‚Äì\n14024. Curran Associates, Inc.\nMatthew Peters, Mark Neumann, Luke Zettlemoyer,\nand Wen-tau Yih. 2018. Dissecting contextual\nword embeddings: Architecture and representation.\nIn Proceedings of the 2018 Conference on Em-\npirical Methods in Natural Language Processing,\npages 1499‚Äì1509, Brussels, Belgium. Association\nfor Computational Linguistics.\nPeng Qian, Xipeng Qiu, and Xuanjing Huang. 2016a.\nAnalyzing linguistic knowledge in sequential model\nof sentence. In Proceedings of the 2016 Conference\non Empirical Methods in Natural Language Process-\ning, pages 826‚Äì835, Austin, Texas. Association for\nComputational Linguistics.\nPeng Qian, Xipeng Qiu, and Xuanjing Huang. 2016b.\nInvestigating language universal and speciÔ¨Åc prop-\nerties in word embeddings. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers), pages\n1478‚Äì1488, Berlin, Germany. Association for Com-\nputational Linguistics.\nAlec Radford. 2018. Improving language understand-\ning by generative pre-training.\nAlec Radford, Jeff Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners.\n3414\nAlessandro Raganato and J ¬®org Tiedemann. 2018. An\nanalysis of encoder representations in transformer-\nbased machine translation. In Proceedings of the\n2018 EMNLP Workshop BlackboxNLP: Analyzing\nand Interpreting Neural Networks for NLP, pages\n287‚Äì297, Brussels, Belgium. Association for Com-\nputational Linguistics.\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words\nwith subword units. In Proceedings of the 54th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 1: Long Papers), pages 1715‚Äì\n1725, Berlin, Germany. Association for Computa-\ntional Linguistics.\nXing Shi, Inkit Padhi, and Kevin Knight. 2016. Does\nstring-based neural MT learn source syntax? In Pro-\nceedings of the 2016 Conference on Empirical Meth-\nods in Natural Language Processing, pages 1526‚Äì\n1534, Austin, Texas. Association for Computational\nLinguistics.\nRichard Socher, Alex Perelygin, Jean Wu, Jason\nChuang, Christopher D. Manning, Andrew Ng, and\nChristopher Potts. 2013. Recursive deep models\nfor semantic compositionality over a sentiment tree-\nbank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npages 1631‚Äì1642, Seattle, Washington, USA. Asso-\nciation for Computational Linguistics.\nIan Tenney, Dipanjan Das, and Ellie Pavlick. 2019a.\nBERT rediscovers the classical NLP pipeline. In\nProceedings of the 57th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 4593‚Äì\n4601, Florence, Italy. Association for Computational\nLinguistics.\nIan Tenney, Patrick Xia, Berlin Chen, Alex Wang,\nAdam Poliak, R. Thomas McCoy, Najoung Kim,\nBenjamin Van Durme, Samuel R. Bowman, Dipan-\njan Das, and Ellie Pavlick. 2019b. What do you\nlearn from context? probing for sentence structure\nin contextualized word representations. In Interna-\ntional Conference on Learning Representations.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In I. Guyon, U. V . Luxburg, S. Bengio,\nH. Wallach, R. Fergus, S. Vishwanathan, and R. Gar-\nnett, editors, Advances in Neural Information Pro-\ncessing Systems 30, pages 5998‚Äì6008. Curran Asso-\nciates, Inc.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-head\nself-attention: Specialized heads do the heavy lift-\ning, the rest can be pruned. In Proceedings of the\n57th Annual Meeting of the Association for Com-\nputational Linguistics, pages 5797‚Äì5808, Florence,\nItaly. Association for Computational Linguistics.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R‚Äôemi Louf, Morgan Funtow-\nicz, and Jamie Brew. 2019. Huggingface‚Äôs trans-\nformers: State-of-the-art natural language process-\ning. ArXiv, abs/1910.03771.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V\nLe, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus\nMacherey, et al. 2016. Google‚Äôs neural machine\ntranslation system: Bridging the gap between hu-\nman and machine translation. arXiv preprint\narXiv:1609.08144.\nA Training and Evaluation Details\nA.1 Pre-trained Transformer\nThroughout the entire experiments, we mainly used\nhuggingface‚Äôs seven pre-trained transformers2, im-\nplemented with Pytorch. However, since the origi-\nnal implemented models do not return the output\nvectors of the internal attention heads, we devel-\noped the wrapper class that enables extracting the\noutput vectors from the created pre-trained model\nobjects. The implementation details and proce-\ndures for replicating the experimental results are\ndescribed in our repository3\nA.2 Probing Task Benchmark\nWe utilized the SentEval toolkit4 for both probing\nand downstream tasks. The probing task results re-\nported in the main text are obtained with a logistic\nregression layer attached to the pooled output vec-\ntor from the transformer. We trained the classiÔ¨Åer\nwith the batch size of 256 for all the experiments,\nfreezing the parameters of the transformer. A RM-\nSProp optimizer is used with the learning rate of\n0.1. Only the L2 regularization is tuned among\n[10-5, 10-4, 10-3, 10-2, 10-1]. During training,\nwe monitor the validation accuracy and stop the\ntraining process when it does not improve for the\nprevious 3 epochs (tenacity=3). Also, each classi-\nÔ¨Åer is tested with 5-fold cross validation. In section\n3.3, validation accuracy s(hi,j) is measured from\nthe Ô¨Åve-validation sets partitioned in a mutually\nexclusive way and averaged.\n2https://github.com/huggingface/\ntransformers\n3https://github.com/heartcored98/\ntransformer_anatomy.\n4https://github.com/facebookresearch/\nSentEval\n3415\nA.3 Downstream Task Benchmark\nThe downstream task results reported in the main\ntext are obtained with a logistic regression layer\nattached to the pooled output vector from the trans-\nformer. We trained the classiÔ¨Åer with the batch size\nof 256 for all experiments, freezing the parame-\nter of transformer, following the same procedure of\nA.2. Note that the metric for the STS-B task is Pear-\nson and Spearman scores. Therefore we measured\nthe validation Pearson score instead of validation\naccuracy for choosing inÔ¨Çuential attention heads\nfor the STS-B task.\nA.4 Fine-tuning on Downstream Tasks\nDuring the Ô¨Åne-tuning process with one of the three\ndifferent pooling methods (last-layer, best-layer,\nand top-n heads), we attached an additional linear\nlayer with a dropout layer (dropout rate=0.1) and\nTanh activation function, following the pooler archi-\ntecture implemented in Huggingface‚Äôs transformer.\nThen the logistic regression layer is attached to the\nactivation function. We trained the classiÔ¨Åer with\nthe batch size of 32 with a learning rate of 2e-5\nwith three epochs for all the experiments, unfreez-\ning all the parameters of the transformer and the\nregressor. Each experiment is repeated Ô¨Åve times\nwith different random seeds to provide fair com-\nparisons against the performance variance of the\nÔ¨Åne-tuning process conducted on small datasets.\nB Head-Wise Evaluation Results with\nProbing Tasks\nThe performance variation of the probing tasks\nis shown in Table 5 that provides full experimen-\ntal results with BERTBASE, BERTLARGE, GPT and\nGPT2.\nC Head-wise Evaluation Heatmaps\nSince Figure 3 provides partial results only, we\nprovide Figure 6 and 7 here to show the full exper-\nimental results with and without sorted attention\nheads on the same layer. The former helps un-\nderstanding how the inÔ¨Çuential heads are gathered\nfor their strengths while the latter is useful for un-\nderstanding how various linguistic capabilities are\nsupported in association by a particular attention\nhead.\nTask\nEncoder\nTask BERTBASE BERTLARGE GPT GPT2\nGroup last layer best layer top-12 heads last layer best layer top-16 heads last layer best layer top-12 heads last layer best layer top-12 heads\nSurface Length 58.0 87.8 (51.5) 95.0 (64.0) 54.8 94.4 (72.2) 95.2(73.6) 52.2 96.4 (84.7)96.2 (84.3) 57.8 88.9 (53.9) 92.8 (60.6)\nWordContent 25.2 25.2 (0.0) 73.1 (190.3) 12.2 32.2 (165.2) 79.8(556.4) 35.3 35.3 (0.0) 71.3 (102.0) 37.5 37.5 (0.0) 71.0 (89.5)\nSyntactic\nDepth 29.8 31.7 (6.5) 38.3 (28.7) 27.8 33.3 (19.6) 39.5(41.9) 27.2 30.6 (12.2) 38.9 (42.8) 28.0 31.0 (10.6) 40.0 (42.7)\nTopConstituents69.8 74.7 (7.0) 84.2 (20.6) 62.8 78.2 (24.6) 85.6(36.4) 53.0 65.1 (22.8) 79.5 (50.0) 57.3 63.1 (10.2) 82.8 (44.5)\nBigramShift 78.1 78.1 (0.0) 88.3 (13.1) 77.2 81.1 (5.1) 90.9(17.8) 69.3 69.3 (0.0) 80.7 (16.5) 68.8 70.5 (2.5) 78.8 (14.6)\nSemantic\nTense 86.0 87.0 (1.2) 89.0 (3.6) 85.6 86.7 (1.2) 88.9(3.8) 88.6 88.6 (0.0) 89.0 (0.5) 88.2 88.6 (0.5) 89.3 (1.2)\nSubjNumber 82.0 84.7 (3.4) 88.2 (7.6) 80.0 87.9 (9.9) 90.5(13.0) 78.8 78.8 (0.0) 84.2 (6.8) 83.5 83.5 (0.0) 87.7 (5.1)\nObjNumber 75.4 75.4 (0.0) 83.4 (10.6) 64.2 78.0 (21.5) 84.4(31.4) 71.2 71.9 (0.9) 80.5 (13.0) 70.2 74.1 (5.5) 82.5 (17.5)\nOddManOut 59.6 59.6 (0.0) 65.1 (9.3) 55.5 59.2 (6.6) 69.0(24.2) 55.0 58.2 (5.9) 63.0 (14.6) 54.6 54.9 (0.6) 5.5 (1.6)\nCoordInversion65.5 65.9 (0.5) 74.6 (13.8) 64.9 70.9 (9.2) 78.5(20.9) 57.7 60.7 (5.1) 70.2 (21.6) 58.3 60.0 (2.9) 67.6 (15.9)\nTable 5: A summary of the probing tasks for three different embedding methods used in the Ô¨Åve pre-trained architectures. The numbers in the parenthesis denote the percent\nincrement of accuracy compared to those of the last layer.\n3416\nLength DepthTopConstituentsBigramShiftCoordInversionOddManOutWordContent TenseSubjNumberSubjNumber\nBERTBASEBERTLARGEGPTGPT2ELECTRALARGE ELECTRABASE ELECTRASMALL\nFigure 6: Heatmaps of the attention head-wise evaluation on the ten sentence probing tasks with BERT BASE, BERTLARGE, GPT, GPT2, ELECTRASMALL, ELECTRABASE, and\nELECTRALARGE. 70 heat maps correspond to the ten tasks (Length, WordContent, Depth, TopConstituents, SubjNum, ObjNum, BigramShift, CoordInversion, OddManOut,\nand Tense from the left). In each heat map, x-axis and y-axis show the index values of the attention heads and the layer numbers (the lower, the closer to the initial input),\nrespectively. The brighter the color, the higher the accuracy for the attention head and hence more important for the task. Note that the attention heads on the same layer are\nordered by their classiÔ¨Åcation accuracy values (i.e. an attention head with the highest accuracy on a layer is at the left-most location).\n3417\nLength DepthTopConstituentsBigramShiftCoordInversionOddManOutWordContent TenseSubjNumberSubjNumber\nBERTBASEBERTLARGEGPTGPT2ELECTRALARGE ELECTRABASE ELECTRASMALL\nFigure 7: Unsorted heatmaps of the attention head-wise evaluation on the ten sentence probing tasks with BERT BASE, BERT LARGE, GPT, GPT2, ELECTRA SMALL,\nELECTRABASE, and ELECTRALARGE. 70 heat maps correspond to the ten tasks (Length, WordContent, Depth, TopConstituents, SubjNum, ObjNum, BigramShift, CoordInver-\nsion, OddManOut, and Tense from the left). In each heat map, x-axis and y-axis show the index values of the attention heads and the layer numbers (the lower, the closer to the\ninitial input), respectively. The brighter the color, the higher the accuracy for the attention head and hence more important for the task. Note that the attention heads on the same\nlayer are not ordered like Figure 6.",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.6328150033950806
    },
    {
      "name": "Computer science",
      "score": 0.6215763092041016
    },
    {
      "name": "Natural language processing",
      "score": 0.4290877878665924
    },
    {
      "name": "Artificial intelligence",
      "score": 0.38279128074645996
    },
    {
      "name": "Engineering",
      "score": 0.17202171683311462
    },
    {
      "name": "Electrical engineering",
      "score": 0.1398082673549652
    },
    {
      "name": "Voltage",
      "score": 0.08446618914604187
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I157485424",
      "name": "Korea Advanced Institute of Science and Technology",
      "country": "KR"
    }
  ],
  "cited_by": 32
}