{
  "title": "Evaluating the language abilities of Large Language Models vs. humans: Three caveats",
  "url": "https://openalex.org/W4394958222",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A4307413760",
      "name": "Leivada, Evelina",
      "affiliations": [
        "Institució Catalana de Recerca i Estudis Avançats"
      ]
    },
    {
      "id": "https://openalex.org/A4322816914",
      "name": "Dentella, Vittoria",
      "affiliations": [
        "Universidad Rovira i Virgili"
      ]
    },
    {
      "id": "https://openalex.org/A3094662989",
      "name": "Günther Fritz",
      "affiliations": [
        "Humboldt-Universität zu Berlin"
      ]
    },
    {
      "id": "https://openalex.org/A4307413760",
      "name": "Leivada, Evelina",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4322816914",
      "name": "Dentella, Vittoria",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3094662989",
      "name": "Günther Fritz",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W4389669106",
    "https://openalex.org/W4393955733",
    "https://openalex.org/W4382240547",
    "https://openalex.org/W4320482536",
    "https://openalex.org/W4393968405",
    "https://openalex.org/W4389518756",
    "https://openalex.org/W4391591605",
    "https://openalex.org/W1967877743",
    "https://openalex.org/W4381612673",
    "https://openalex.org/W4387799687",
    "https://openalex.org/W4401876203",
    "https://openalex.org/W3011421975",
    "https://openalex.org/W2996728628"
  ],
  "abstract": "We identify and analyze three caveats that may arise when analyzing the linguistic abilities of Large Language Models. The problem of unlicensed generalizations refers to the danger of interpreting performance in one task as predictive of the models’ overall capabilities, based on the assumption that because a specific task performance is indicative of certain underlying capabilities in humans, the same association holds for models. The human-like paradox refers to the problem of lacking human comparisons, while at the same time attributing human-like abilities to the models. Last, the problem of double standards refers to the use of tasks and methodologies that either cannot be applied to humans or they are evaluated differently in models vs. humans. While we recognize the impressive linguistic abilities of LLMs, we conclude that specific claims about the models’ human-likeness in the grammatical domain are premature.",
  "full_text": "Forum\nEvaluating the Language Abilities of Large Language \nModels vs. Humans: Three Caveats\nEvelina Leivada1,2, Vittoria Dentella3, Fritz Günther4\n[1] Department of Catalan Philology, Universitat Autònoma de Barcelona, Barcelona, Spain. [2] Institució Catalana de \nRecerca i Estudis Avançats (ICREA), Barcelona, Spain. [3] Department of English and German Studies, Universitat Rovira \ni Virgili, Tarragona, Spain. [4] Institut für Psychologie, Humboldt-Universitat zu Berlin, Berlin, Germany. \nBiolinguistics, 2024, Vol. 18, Article e14391, https://doi.org/10.5964/bioling.14391\nPublished (VoR): 2024-04-19\nHandling Editor: Patrick C. Trettenbrein, Max Planck Institute for Human Cognitive and Brain Sciences & \nUniversity of Göttingen, Germany\nCorresponding Author: Evelina Leivada, Universitat Autònoma de Barcelona, Departament de Filologia Catalana, \n08193 Barcelona, Spain. E-mail: evelina.leivada@uab.cat\nAbstract\nWe identify and analyze three caveats that may arise when analyzing the linguistic abilities of \nLarge Language Models. The problem of unlicensed generalizations refers to the danger of \ninterpreting performance in one task as predictive of the models’ overall capabilities, based on the \nassumption that because a specific task performance is indicative of certain underlying capabilities \nin humans, the same association holds for models. The human-like paradox refers to the problem \nof lacking human comparisons, while at the same time attributing human-like abilities to the \nmodels. Last, the problem of double standards refers to the use of tasks and methodologies that \neither cannot be applied to humans or they are evaluated differently in models vs. humans. While \nwe recognize the impressive linguistic abilities of LLMs, we conclude that specific claims about the \nmodels’ human-likeness in the grammatical domain are premature.\nKeywords\nArtificial Intelligence, grammaticality, Large Language Models, probabilities\n1. Introduction\nIn most subfields of Artificial Intelligence (AI), the success or failure of cutting-edge \napplications depends on the quantity and quality of the data. The quality of the data is \nalso relevant in the context of our evaluations of the capabilities of such applications. \nRecent Large Language Models (LLMs) have been heralded as the most important inno­\nThis is an open access article distributed under the terms of the Creative Commons \nAttribution 4.0 International License, CC BY 4.0 , which permits unrestricted use, \ndistribution, and reproduction, provided the original work is properly cited.\nvation of the last decades, having the potential to transform fields like health care and \neducation (Gates, 2023). In this context, rigorously determining the linguistic abilities of \nsuch models is critical. While data is important, language learning in humans, unlike \nLLMs, is not anchored exclusively on data-driven prediction ( Felin & Holweg, 2024 ). In \nthe process of language learning, humans also form hypotheses and theories about the \ninput, while LLMs are trained to predict the next token in a sequence of tokens. In light \nof such differences, do LLMs behave linguistically in a way that can be called human or \nhuman-like?\nTo answer this question, Dentella et al. (2023)  tapped into the grammatical under­\nstanding of LLMs. Specifically, they examined three models (GPT-3/text-davinci-002, \nGPT-3/text-davinci-003, and ChatGPT 3.5) and 80 humans in a judgment task that fea­\ntured different linguistic phenomena. The results suggested that humans performed bet­\nter than the tested LLMs, especially in recognizing the grammatically ill-formed prompts \nas such. Unlike humans, the LLMs showed a strong bias towards providing yes-responses \nirrespective of the grammaticality of the prompts. Moreover, upon repeated prompting of \na sentence, humans remained largely stable in their opinions about its well-formedness, \nwhereas the models oscillated a lot, revealing a stark absence of response stability \n(Dentella et al., 2023).\nHu et al. (in press)  provide an illuminatingly different perspective on these results. \nSpecifically, they reach different conclusions, arguing that the tested LLMs perform well \nto the point of aligning with human judgments on key grammatical constructions and \nof showing human-like grammatical generalization capabilities. Leivada et al. (in press) \nprovide a very brief reply to Hu et al., but given the space constraints, several important \nissues that merit clarification and correction have not been addressed. We address them \nin this work. Before delving into them, it is important to highlight that Hu et al. make \na valuable contribution towards understanding the language abilities of LLMs and, more \nimportantly, towards figuring out why some methods of LLM evaluation show different \nresults than others. Juxtaposing Dentella et al. (2023) and Hu et al. (in press), we identify \nthree specific caveats that explain why different teams of scholars reach diametrically \nopposite conclusions about the language abilities of LLMs.\n2. The Problem of Unlicensed Generalizations\nThe notion of unlicensed generalizations refers to the danger of interpreting results from \none task as indicative of the model’s overall capabilities, assuming that because a specific \ntask performance entails certain abilities in humans, the exact same relation holds ipso \nfacto for LLMs. Let us illustrate the problem with an example: Hu et al. (in press)  argue \nthat they re-evaluate LLM performance “using well-established practices and find that \nDGL’s [Dentella et al. ’s] data in fact provide evidence for how well LLMs capture human \nbehaviors”. Claiming that LLMs accurately capture human behavior on the basis of \nHumans vs. Large Language Models 2\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nobserving that their performance is equal or better than that of humans in a certain task \n(which it is not, but let us assume for now that it is; we will explain why this assumption \nis not correct in the section ‘ Τhe problem of double standards’) is analogous to claiming \nthat a clock that shows the right time works correctly. As Guest and Martin (2023) argue, \nin the context of evaluating LLMs, such analogies are unfounded, because they reverse \nthe order of the argument: If a clock works correctly, then it shows the correct time. \nSimilarly, if the models are “human-like”, we can infer that they will capture well human \nperformance across tasks. However, we cannot legitimately infer human-like competence \nbased on a task; similar to how we would be wrong to infer that a clock that shows the \ntarget time once a day works correctly. This reverses the nature of the argument. First, \nwe need to determine both computational and behavioral alikeness through systematic \ntesting that covers not only superficial similarity in terms of accuracy in a task, but also \nthe type of reasoning used to perform the task, and then we can assert human-likeness.\nIt is interesting to observe how such differences are framed, depending on whether \none finds that the models outperform humans or not. On the one hand, Dentella et al. \n(2023) suggest that their results challenge the claim that LLMs, at their current state of \ndevelopment, possess human-like language abilities, because they found that the tested \nmodels differ from humans in a specific task that taps into grammatical well-formedness. \nOn the other hand, Hu et al. (in press)  framed Dentella et al. (2023)  as if making a more \ngeneral claim about an alleged inability of LLMs to form linguistic generalizations. In fact, \nwhen presenting the scope of Dentella et al. in social media, they framed it in an even \nbroader way: “Are Large Language Models good at language? A recent paper by Dentella, \nGünther, & Leivada (DGL) argues no. ” 1 Yet, Dentella et al. (2023)  argue no such thing. \nInstead, they make a narrower claim about the inability of the tested models to discern \nthe boundaries of grammar on a par with humans, as evidenced through a judgment \ntask on specific grammatical phenomena. Similar to how target performance in a task \ndoes not license the generalization that LLMs have human-like capabilities, non-target \nperformance in a task does not predict a ubiquitous failure across all possible tasks. \nEffectively, Hu et al. (in press)  launch a strawman when they argue against a claim that \nDentella et al. (2023) never made.\n3. The Human-Like Paradox\nUnlicensed generalizations and inappropriate framing of the linguistic abilities of LLMs \nhave given rise to a paradox which we term ‘the human-like paradox’. This refers to the \nproblem of interpreting the results of an experiment through simultaneously affirming \nthat LLMs behave in a human or human-like way, while lacking human comparisons.\n1) https://twitter.com/_jennhu/status/1754891894704746789\nLeivada, Dentella, & Günther 3\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nTo illustrate this paradox with an example, Hu et al. (in press)  argue that “LLMs \nshow strong and human-like grammatical generalization capabilities”. Yet, as also noted \nin Leivada et al. (in press) , this claim is not backed up with human data. While Dentella \net al. compare humans and LLMs using the same judgment task, 2 Hu et al. suggest that \na better way of tapping into the language abilities of LLMs goes through obtaining \ndirect probability measurements. Hu et al. believe that the results obtained from the two \nmethodologies, grammaticality judgments and minimal-pair direct probabilities, align \n(i.e. the grammatical sentence in a ‘grammatical-ungrammatical’ pair is the one that both \nhumans and models “prefer”: Humans give it a higher rate of acceptability, while models \nfind it less surprising than its counterpart). Yet we argue that this is not evidence that the \ntested models are sensitive to grammaticality or that a generalization has been learned. \nAnother explanation for the observed alignment is possible: for instance, it could be \nthe case that LLMs are less surprised by the sentences that humans find well-formed \nbecause these are more abundant in their web-scrapped training data. However, there \nis another elephant in the room: these claims about alignment and human-like abilities \nin a specific task occur in the absence of human comparisons. Hu et al. compare prob­\nability measurements in models to judgments in humans. This happens because they \ndo not and cannot look directly into the minds of humans and observe probabilities \nover strings of words—assumptions about such internal representations and processes \nalways have to be inferred from some other associated outcome variable (behavioral, \nelectrophysiological, or neuroimaging, among others). In fact, the problem is not only \npractical; it is known that even when asked directly, humans are not good at probability \nassignment (Kahneman & Tversky, 1982). If “[t]he metalinguistic judgments elicited from \nLLMs through prompting are not the same as quantities directly derived from model \nrepresentations” ( Hu & Levy, 2023 ), the two methods are not the same, yet Hu et al. \ncompare them as if they were.\nEven if we grant that Hu et al. are right that LLMs perform well in probability \nmeasurements, this does not cast doubt on main claim put forth by Dentella et al.: The \ntested LLMs, at their current stage of development, do not perform in a human-like way \nin terms of providing judgments about grammatical well-formedness. On the other hand, \nthere are tasks in which LLMs will perform better than humans (e.g., name 100 animals \nthat start from ‘m’ in one minute); but outperforming humans would again run contrary \n2) While Hu et al. argue that the task reported in Dentella et al. was not exactly the same in humans vs. models—\nbecause humans were asked to press a button to answer as to whether a sentence was correct or not, while models \nanswered the same questions without pressing a button—, we believe that these are differences that come with the \nterritory. In fact, Hu et al. also elicited judgments, asking the models to respond with C or N (corresponding to \nthe buttons humans pressed), but this instruction was ignored. As discussed in the next section, the models often \nmentioned other (task-irrelevant) things in their replies, in parallel with or even in the absence of C or N. If Hu et \nal. indeed maintain that pressing a button changes the results, enhancing human accuracy, the straightforward path \nforward would be to rerun the test asking humans to respond writing their answer in a text box. Our prediction is \nthat the results will be the same, because pressing a button does not alter judgments in humans.\nHumans vs. Large Language Models 4\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nto the claim that the models behave in a human-like manner. The question is whether \none chooses to direct their attention exclusively to tasks that mask the differences \nbetween humans and models to sustain a claim that LLMs have human-like language \nabilities, and whether such claims rely on the right level of comparison between the two. \nAll in all, the buzzword ‘human-like’ should be used with caution for it is meaningful \nonly when one has established what counts as ‘human’ in a specific experimental setting, \nusing the right type of comparisons.\n4. The Problem of Double Standards\nComparisons fulfil their function when the standards of evaluation are kept uniform. To \nreach their conclusions, Hu et al. (in press) replace grammaticality judgments with mini­\nmal-pair probability measurements. In other words, they replace absolute judgments for \nindividual sentences with respect to a specific criterion (grammatical well-formedness) \nwith relative judgments for pairs of sentences without any target criterion. Yet, gramma­\nticality is not a matter of comparison. Humans are able to judge the well-formedness \nof individual sentences and they do not need minimal pairs to anchor or adjust their \njudgments, as demonstrated by the human participants’ performance in Dentella et al. \n(2023). Also, grammaticality is not a matter of degree either ( Leivada & Westergaard, \n2020). A sentence is either grammatical or ungrammatical, in the sense that either it \ncontains a violation of at least one rule of grammar, or it does not. From this perspective, \nasking the models ‘Which of the following two sentences is more grammatically correct \nin English?’, as Hu and Frank (2024)  do, is wrong. No rule of grammar can be violated \njust a bit in sentence A but significantly more in sentence B such that A is more \ngrammatical than B ( Leivada & Westergaard, 2020 ). Hence, the question cannot be one \nof degree.3 The question can be one of degree for humans, because unlike LLMs, humans \nalso have judgments of acceptability (see Dentella et al., 2023  for the distinction): A \nperson may like a sentence better than another one and assign it a higher acceptability \nrating. Models lack such preferences, likes, and dislikes; certain words do not make them \nfeel in different ways, and they do not voluntarily (i.e. unprompted) project a specific \nidentity through the use of specific words. This is probably the reason why the LLMs \ntested in Dentella et al. (2023)  oscillate a lot in their judgments, unlike humans. Humans \ncan consistently decide whether a sentence looks good or bad; LLMs cannot do so, thus \nthey oscillate to maximize the probability of providing some target answers.\n3) This has consequences for the obtained results too. Hu and Frank (2024) classify the sentences in absolute terms, \nassigning them the label ‘grammatical/good’ or ‘ungrammatical/bad’, when presenting the results in the repository. \nYet in the actual experiment, the question was one of degree (i.e. which sentence is more grammatically correct than \nthe other), not of kind. An answer that suggests that a sentence A is more grammatical than a sentence B does not \nlegitimize the inference that B is ungrammatical or bad, if the question is one of degree.\nLeivada, Dentella, & Günther 5\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nIf we evaluate humans on a judgment task and models through direct probability \nmeasurements, which is the comparison Hu et al. (in press)  make to find the alignment \nthey claim, essentially, we hold different standards of evaluation for the two agents. This \nis the problem of double standards: using tasks and methodologies that either cannot \nbe administered to both humans and models, or they are evaluated differently across \nthe two. Hu et al. (in press)  not only compare results obtained from different methods \n(judgments for humans vs. probabilities for LLMs), but they also adopt a different locus \nof comparison for each: individual sentences for humans vs. pairs of sentences for LLMs. \nAs Leivada et al. (in press)  note, one needs to compare apples to apples. This is a matter \nwith important consequences because double standards can help inflate the performance \nof the models, leading to erroneous claims about “human-like” LLM behavior.\nTake for instance Hu et al. ’s (in press) claim that a minimal-pair analysis of probabili­\nties shows “at- or near-ceiling performance” for LLMs. If we hold the models accountable \nat the same level as humans (i.e. individual sentences), a very different picture is ob­\nserved. Assuming that probabilities really are informative of grammaticality, a surprisal \nthreshold should exist in order to discriminate what is grammatical from what is not. \nYet, when re-analyzing the probabilities given in the Hu et al. (in press)  dataset, Leivada \net al. (in press)  find that the mean difference between ungrammatical and grammatical \nsentences exists in the context of a massive overlap between the two distributions. Even \nwith an optimal surprisal threshold that results in the highest possible classification \naccuracy, this accuracy is only at .60 for davinci2 and .58 for davinci3 4. While this is \nsignificantly better than random guessing ( p = .005 for davinci2, p = .019 for davinci3), \nit is very similar to the judgment-based overall accuracies of .59 (davinci2) and .56 \n(davinci3) already reported as significant by Dentella et al., and far from the “at- or \nnear-ceiling performance” that Hu et al. claim based on their minimal-pair comparisons \n(Leivada et al., in press ). To return to the critical assumption about human-likeness \nmentioned in Section 2, Hu et al. ’s results do not provide reliable “evidence for how well \nLLMs capture human behaviors”, because they have been compromised by the problem \nof double standards. Consequently, when the LLM probabilities are subjected to a test \nregime more similar to that applied to humans, the claims made by Hu et al. (in press) \nneed to be modified to a level where they are far more in line with the original findings \nby Dentella et al. (2023) . Of course, one may argue that the minimal-pair comparison \nis necessary to isolate grammaticality from other factors that may influence surprisal, \nsuch as word frequencies or semantic effects, and is thus necessary to arrive at controlled \ncomparisons. Rather than salvaging this approach, this argument further reveals its \nshortcomings: Surprisal in the manner derived by Hu et al. is just a function of the \n4) The accuracy rates are slightly lower when using alternative measures to the sum surprisal measure over all words \nin a sentence used by Hu et al. (in press) and for this analysis, namely the average surprisal (sum surprisal divided by \nthe number of words in a sentence) or the maximum surprisal for a word in a sentence.\nHumans vs. Large Language Models 6\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nraw probability of observing a sentence in a corpus, without being anchored to any \nspecific target criterion (such as grammaticality). If LLMs need specific comparisons in \norder to tell apart grammatical from ungrammatical sentences, this already counts as an \ninherent discrepancy from humans, who are able to make such judgments without such a \ncomparison.\nSelecting the right tasks for evaluating LLMs lies at the heart of the problem of \ndouble standards. Returning to the alleged superiority of probability measurements to \njudgment tasks, Hu and Frank (2024)  re-affirm this position. They obtain probabilities \nfrom 13 open-source autoregressive language models that range in size from 1B to 70B \nparameters, using two datasets: DGL ( Dentella et al., 2023 ) and BLiMP ( Warstadt et \nal., 2020 ). Their results lead them to put forth the claim that elevated task demands \n(which they take to be high in judgment tasks and low in probability measurements) \nmay mask the true linguistic abilities of smaller models. This is another instance of the \nproblem of double standards hampering the results and weakening the claims made on \ntheir basis: The association ‘judgments-high task demands’ and ‘probabilities-low task \ndemands’ does not hold for humans; hence it cannot serve as an adequate basis of \nexperimentally comparing humans and LLMs. We cannot obtain direct probabilities from \nhumans through peeking into their neurons, and definitely not by peeking directly into \ntheir mental representations either. Moreover, providing judgments of well-formedness is \nextremely easy for humans, making hard to justify the link of this task to high cognitive \ndemands.\nSince Hu and Frank (2024)  obtain new probability values for the two tested datasets, \nDGL and BLiMP, we re-analyzed both sets of probabilities individually for each of the \n13 models, along the lines described in Leivada et al. (in press)  for the re-analysis of \nHu et al. (in press) . Specifically, we considered the absolute sum surprisal of individu­\nal sentences, both grammatical and ungrammatical, and checked whether a surprisal \nthreshold that acts as a cut-off for grammaticality exists. The existence of this threshold \nis of paramount importance, because without it, the claim of Hu and Levy (2023)  and \nHu et al. (in press)  about the alleged superior ability of probability measurements to \ncapture the generalization capabilities of LLMs lacks foundation. Succinctly put, if the \nmost optimal of all possible surprisal thresholds does not boil down to a cut-off point \nthat permits mapping probabilities above it to grammatical sentences and probabilities \nbelow it to ungrammatical sentences, it is not clear that probabilities truly are an index of \ninternalized grammatical knowledge (i.e. of having internally generalized anything that \nhelps the model to tell apart well- from ill-formed sentences). If they are not, any study \nthat compares probabilities to judgments and finds the former faring better (e.g., Hu & \nFrank 2024; Hu & Levy, 2023 ; Hu et al., in press ) is possibly built on a foundation that \nmerits reconsideration.\nThe results of this new analysis confirm the findings of Leivada et al. (in press) . As \nFigure 1 shows for DGL and Figure 2 for BLiMP, accuracies in both datasets remain just \nLeivada, Dentella, & Günther 7\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nslightly above chance, ranging from .55 to .61. We find that roughly comparable parts of \nthe two distributions end up in areas above and below the optimal threshold to discrimi­\nnate grammatical from ungrammatical sentences. If probabilities were a good proxy for \ngrammaticality, one would expect a clearer difference between the distributions, with \nungrammatical sentences clustering together and showing a clear concentration above \nthe threshold, and vice versa for grammatical sentences.\nFigure 1\nDistributions of Sum Surprisal Using the DGL Dataset in 13 Models\nNote. The surprisal threshold that results in the classification with the highest accuracy is indicated by the \nhorizontal red line. Any sentence with a sum surprisal higher than or equal to that threshold is classified as \n“ungrammatical”, while any sentence with a sum surprisal lower than that threshold is classified as \n“grammatical”. The probabilities are taken from Hu and Frank (2024).\nHumans vs. Large Language Models 8\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nFigure 2\nDistributions of Sum Surprisal Using the BLiMP Dataset in 13 Models\nNote. The probabilities are taken from Hu and Frank (2024).\nThe problem of double standards also includes tasks that in principle can be applied \nto both humans and LLMs but their results are assessed differently across the two. To \nprovide an example, Hu et al. ’s (in press)  strongest argument comes from finding that \nGPT-3.5 Turbo and GPT-4 (not tested in Dentella et al., 2023) outperform humans in pro­\nviding judgments of grammatical well-formedness (note again that LLMs outperforming \nhumans would pull into question the alleged “human-like behavior” of LLMs). Before \nexplaining why this finding is spoiled by the problem of double standards, let us briefly \nLeivada, Dentella, & Günther 9\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nadd that this difference from the models tested in Dentella et al. does not entail that the \nmore recent models have developed an understanding of grammatical correctness that \neven surpasses that of humans sometime after the first testing took place. Hu et al. do \nnot acknowledge so, but potential alternative explanations for this better performance \nexist: for instance, that the DGL dataset was available online and discussed in social \nmedia by the time Hu et al. performed their testing. LLMs are trained on thousands of \nscientific papers (Frank, 2023), and algorithmic fudging based on continuous social media \nmonitoring has been noted, affecting performance in linguistic tasks too ( Leivada et al., \n2023).\nAdmittedly, it is easy to understand how this very good performance of the recent \nmodels could legitimize Hu et al. ’s claim that LLMs show human-like abilities. This \nlooks compelling, until one checks the raw material Hu et al. make available. The LLMs, \naccording to the task instructions, should return one of the two outcomes: C if a prompt \nis correct, and N if it is not. Yet the raw responses show some peculiar answers that \ndeviate from the instructions, such as “The teacher is going to teach a lesson on the Civil \nWar” or “I will be meeting with John and Karen C”. Instead of coding these answers \nas incorrect (since the target response C was not unambiguously provided), Hu et al. \nremoved the first one (which lacks either C or N) and coded the other as target/accurate. \nDespite the alleged alignment with human responses, it is very hard to imagine that a \nhuman would respond this way, and anyone would count it as target behavior in this \nspecific task.\nIf we recode all the “hallucinating replies” as non-target/inaccurate, Hu et al. ’s dataset \nfrom judgment prompting effectively replicates Dentella et al. ’s main finding: humans \nare more accurate than davinci2 and davinci3. More recent models such as GPT-3.5 \nTurbo and GPT-4 indeed do better, but as argued above, more than one reason could be \nresponsible for this better performance. Also, it is interesting to note that even the best \nperforming model (GPT-4) includes a paradoxical reply that asserts that a given prompt \nis both correct and incorrect. We interpret these results as showing that scaling mitigates \nbut does not completely cover the qualitative differences in the performance of humans \nvs. LLMs. The same is true for the problem of double standards: Holding the models \naccountable to a different level of performance may cover quantitative differences, but \ntheir distinctly non-human errors are still present and harder to explain away.\n5. Conclusion\nWe identified three caveats that may give rise to an inaccurate picture of the linguistic \nabilities of LLMs: the problem of unlicensed generalizations, the human-like paradox, \nand the problem of double standards. Moreover, we provided a detailed comparison of \ntwo different methodologies for tapping into the linguistic abilities of the models: judg­\nment tasks vs. direct probabilities. To establish the comparison, we re-analyzed Hu et \nHumans vs. Large Language Models 10\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nal. (in press) and Hu and Frank (2024), and we found that probabilities do not unambigu­\nously tell apart grammatical from ungrammatical sentences. This raises concerns about \nwhether this truly is a superior method for approaching the internalized grammatical \nabilities of LLMs. In relation to the validity of this method, it is also worth observing \nthat Hu et al. (in press)  did not obtain direct probability measurements—which is meth­\nodologically superior to judgments according to them (see also Hu & Levy, 2023, for the \nsame claim)—from the more recent models GPT-3.5 Turbo and GPT-4; they only elicited \njudgments from them. They do not explain why they chose to use the “inferior” method \nwith the recent models, but it is because OpenAI no longer returns log probabilities in \nmost of their models. This option has been disabled probably because it reveals too much \ninformation about the black-box training data of proprietary, closed-source models.\nRonald Coase once said that if data is tortured long enough, it will confess to any­\nthing. For models that have been linked to a tendency to amplify misinformation ( Kidd & \nBirhane, 2023), training data is the weakest link: If tortured long enough, they may give \nrise to uncomfortable confessions that are hard to reconcile with the current AI hype \nthat (at times, uncritically) endows LLMs with human-like abilities.\nFunding: The authors have no funding to report.\nAcknowledgments: We thank Jennifer Hu for her help in interpreting her datasets.\nCompeting Interests: The authors have declared that no competing interests exist.\nReferences\nDentella, V., Günther, F., & Leivada, E. (2023). Systematic testing of three Language Models reveals \nlow language accuracy, absence of response stability, and a yes-response bias. Proceedings of the \nNational Academy of Sciences of the United States of America, 120(51), Article e2309583120. \nhttps://doi.org/10.1073/pnas.2309583120\nFelin, T., & Holweg, M. (2024). Theory is all you need: AI, human cognition, and decision making. \nhttps://doi.org/10.2139/ssrn.4737265\nFrank, M. C. (2023). Baby steps in evaluating the capacities of large language models. Nature \nReviews Psychology, 2, 451–452. https://doi.org/10.1038/s44159-023-00211-x\nGates, B. (2023, March 21). The age of AI has begun. GatesNotes. \nhttps://www.gatesnotes.com/The-Age-of-AI-Has-Begun\nGuest, O., & Martin, A. E. (2023). On logical inference over brains, behaviour, and Artificial Neural \nNetworks. Computational Brain & Behavior, 6, 213–227. \nhttps://doi.org/10.1007/s42113-022-00166-x\nHu, J., & Frank, M. C. (2024). Auxiliary task demands mask the capabilities of smaller language \nmodels. arXiv. https://doi.org/10.48550/arXiv.2404.02418 \nLeivada, Dentella, & Günther 11\nBiolinguistics\n2024, Vol. 18, Article e14391\nhttps://doi.org/10.5964/bioling.14391\n\nHu, J., & Levy, R. (2023). Prompting is not a substitute for probability measurements in Large \nLanguage Models. In H. Bouamor, J. Pino, & K. Bali (Eds.), Proceedings of the 2023 Conference on \nEmpirical Methods in Natural Language Processing (pp. 5040–5060). Association for \nComputational Linguistics.\nHu, J., Mahowald, K., Lupyan, G., Ivanova, A., & Levy, R. (in press). Language models align with \nhuman judgments on key grammatical constructions. Proceedings of the National Academy of \nSciences of the United States of America.\nKahneman, D., & Tversky, A. (1982). On the study of statistical intuitions. Cognition, 11(2), 123–141. \nhttps://doi.org/10.1016/0010-0277(82)90022-1\nKidd, C., & Birhane, A. (2023). How AI can distort human beliefs. Science, 380(6651), 1222–1223. \nhttps://doi.org/10.1126/science.adi0248\nLeivada, E., Dentella, V., & Murphy, E. (2023). The quo vadis of the relationship between language \nand Large Language Models. To appear in J.-L. Mendívil-Giró (Ed.), Artificial knowledge of \nlanguage: A linguist’s perspective on its nature, origins and use. \nhttps://doi.org/10.48550/arXiv.2310.11146\nLeivada, E., Günther, F., & Dentella, V. (in press). Reply to Hu et al: Applying different evaluation \nstandards to humans vs. Large Language Models overestimates AI performance. Proceedings of \nthe National Academy of Sciences of the United States of America.\nLeivada, E., & Westergaard, M. (2020). Acceptable ungrammatical sentences, unacceptable \ngrammatical sentences, and the role of the cognitive parser. Frontiers in Psychology, 11, Article \n364. https://doi.org/10.3389/fpsyg.2020.00364\nWarstadt, A., Parrish, A., Liu, H., Mohananey, A., Peng, W., Wang, S.-F., & Bowman, S. R. (2020). \nBLiMP: The Benchmark of Linguistic Minimal Pairs for English. Transactions of the Association \nfor Computational Linguistics, 8, 377–392. https://doi.org/10.1162/tacl_a_00321\nHumans vs. Large Language Models 12\nPsychOpen GOLD is a publishing service by\nLeibniz Institute for Psychology (ZPID), Germany.\nwww.leibniz-psychology.org\n",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.5983380675315857
    },
    {
      "name": "Language model",
      "score": 0.5526028871536255
    },
    {
      "name": "Language evolution",
      "score": 0.5212286710739136
    },
    {
      "name": "Linguistics",
      "score": 0.5045186281204224
    },
    {
      "name": "Natural language processing",
      "score": 0.4704277515411377
    },
    {
      "name": "Cognitive science",
      "score": 0.42116445302963257
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4108714759349823
    },
    {
      "name": "Cognitive psychology",
      "score": 0.34606441855430603
    },
    {
      "name": "Psychology",
      "score": 0.2890850901603699
    },
    {
      "name": "Philosophy",
      "score": 0.1236920952796936
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I11932220",
      "name": "Institució Catalana de Recerca i Estudis Avançats",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I123044942",
      "name": "Universitat Autònoma de Barcelona",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I55952717",
      "name": "Universitat Rovira i Virgili",
      "country": "ES"
    },
    {
      "id": "https://openalex.org/I39343248",
      "name": "Humboldt-Universität zu Berlin",
      "country": "DE"
    }
  ]
}