{
    "title": "Notes towards infrastructure governance for large language models",
    "url": "https://openalex.org/W4391735551",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A3176065772",
            "name": "Lara Dal Molin",
            "affiliations": [
                "University of Edinburgh"
            ]
        },
        {
            "id": "https://openalex.org/A3176065772",
            "name": "Lara Dal Molin",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4360836968",
        "https://openalex.org/W2623293810",
        "https://openalex.org/W3129576130",
        "https://openalex.org/W4249292692",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W4304208382",
        "https://openalex.org/W3119746452",
        "https://openalex.org/W2504804945",
        "https://openalex.org/W4287116904",
        "https://openalex.org/W3008957586",
        "https://openalex.org/W2014831132",
        "https://openalex.org/W2995990961",
        "https://openalex.org/W1819662813",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3117072286",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W3024823444",
        "https://openalex.org/W4244021162",
        "https://openalex.org/W3142264321",
        "https://openalex.org/W2081023327",
        "https://openalex.org/W4292157886",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4221167110",
        "https://openalex.org/W3103164731",
        "https://openalex.org/W3088479061",
        "https://openalex.org/W2963612262",
        "https://openalex.org/W2971307358",
        "https://openalex.org/W3215265458",
        "https://openalex.org/W4252148582",
        "https://openalex.org/W4378465112"
    ],
    "abstract": "This paper draws on information infrastructures (IIs) in science and technology studies (STS), as well as on feminist STS scholarship and contemporary critical accounts of digital technologies, to build an initial mapping of the infrastructural mechanisms and implications of large language models (LLMs). Through a comparison with discriminatory machine learning (ML) systems and a case study on gender bias, I present LLMs as contested artefacts with categorising and performative capabilities. This paper suggests that generative systems do not tangibly depart from traditional, discriminative counterparts in terms of their underlying probabilistic mechanisms, and therefore both technologies can be theorised as infrastructures of categorisation. However, LLMs additionally retain performative capabilities through their linguistic outputs. Here, I outline the intuition behind this phenomenon, which I refer to as “language as infrastructure”. While traditional, discriminative systems “disappear” into larger IIs, the hype surrounding generative technologies presents an opportunity to scrutinise these artefacts, to alter their computational mechanisms and introduce governance measures]. I illustrate this thesis through Sharma’s formulation of “broken machine”, and suggest dataset curation and participatory design as governance mechanisms that can partly address downstream harms in LLMs (Barocas, et al., 2023).",
    "full_text": "Notes towards infrastructure governance for large language models\nThis paper draws on information infrastructures (IIs) in science and technology studies (STS), as well as on\nfeminist STS scholarship and contemporary critical accounts of digital technologies, to build an initial\nmapping of the infrastructural mechanisms and implications of large language models (LLMs). Through a\ncomparison with discriminatory machine learning (ML) systems and a case study on gender bias, I present\nLLMs as contested artefacts with categorising and performative capabilities. This paper suggests that\ngenerative systems do not tangibly depart from traditional, discriminative counterparts in terms of their\nunderlying probabilistic mechanisms, and therefore both technologies can be theorised as infrastructures of\ncategorisation. However, LLMs additionally retain performative capabilities through their linguistic\noutputs. Here, I outline the intuition behind this phenomenon, which I refer to as “language as\ninfrastructure”. While traditional, discriminative systems “disappear” into larger IIs, the hype surrounding\ngenerative technologies presents an opportunity to scrutinise these artefacts, to alter their computational\nmechanisms and introduce governance measures [1\n]. I illustrate this thesis through Sharma’s [2]\nformulation of “broken machine”, and suggest dataset curation and participatory design as governance\nmechanisms that can partly address downstream harms in LLMs (Barocas, et al., 2023).\nContents\n1. Introduction\n2. Categorisation in discriminative and generative ML systems\n3. Language as infrastructure\n4. “Broken machines” as opportunities for infrastructure governance\n5. Conclusion\n1. Introduction\nMachine learning (ML) systems are artificial intelligence (AI) technologies that improve their functionality\nover time based on examples (Russell and Norvig, 2010). ML systems broadly distinguish between\ndiscriminative and generative models, respectively producing output scores or categories for existing data\non the one hand and generating novel data and outputs on the other (Hastie, et al., 2008). Large language\nmodels (LLMs), commonly associated with the generative paradigm, are ML systems that generate new\ntext from very large corpuses of data in ways that aim to mimic a human author (Jurafsky and Martin, 2008;\nNotes towards infrastructure governance for large language models\nLuitse and Dankena, 2021). At present, these are portrayed by their advocates as leading candidates in the\nachievement of artificial general intelligence (AGI) (OpenAI, 2018; Bubeck, et al., 2023). Particularly\nrelevant to LLMs are sociotechnical accounts of computers as “thinking machines”, characterised by\npromises of efficiency, rationality, and objectivity [3]. In this context, Alexander (1990) draws a parallel\nbetween computational technologies and sacred entities, suggesting the existence of imagined associations\nbetween sophistication and awesomeness, which in turn translate to divine metaphors. In popular accounts,\nthe uncanny generative capacities of LLMs are reflected in narratives of omniscience and algorithmic\nfetishism (Thomas, et al., 2018).\nDespite these narratives, this paper argues that both discriminative and generative ML systems, including\nlarge language models (LLMs), should be conceptualised as infrastructures of categorisation. The following\nanalysis situates these systems within information infrastructures (IIs) literature, feminist STS scholarship,\nand critical algorithms studies to consider the capability of these systems to exclude through categories. I\npropose this capacity of ML as an infrastructure governance problem. Drawing particularly on Weiser\n(1991), Bowker and Star (1999), and Clarke and Star (2007), and on technical literature in ML, I argue that\ncomputational mechanisms enabling categorisation in both discriminative and generative systems are\ntroublesome when applied to human characteristics, as they reinforce predefined, standardised, and\nmutually exclusive categories. As described in the next section, in both types of ML systems, the categories\nembedded in infrastructural technologies that disappear into the background of social life are of particular\nimportance for questions of power and performance, as will be illustrated below (Weiser, 1991). Based on a\nrich body of scholarship highlighting the capacity of ML algorithms to uphold social and political orders, I\nsuggest that the categories embedded in LLMs are performative; LLMs as artefacts further consolidate\nmutually exclusive categorisation through their linguistic outputs (Barocas and Selbst, 2016; Eubanks,\n2018; Criado Perez, 2019; Amoore, 2023). I refer the phenomena that compose this complex scenario as\n“language as infrastructure”. Finally, this paper identifies governance opportunities for generative systems.\nBased on Sharma’s (2020) conceptualisation of “broken machine”, I suggest dataset curation and\nparticipatory design as potential governance mechanisms to recentre epistemic imbalances in LLMs.\n2. Categorisation in discriminative and generative ML systems\nAccording to Weiser [4] “the most profound technologies are those that disappear. They weave themselves\ninto the fabric of everyday life until they are indistinguishable from it”. The field of STS ascribes this\ncapacity to information infrastructures (IIs), which encompass relations between material and abstract\nentities, including communication networks, but also protocols, standards, and classification systems (Star\nand Ruhleder, 1994; Bowker, et al., 2010; Musiani, et al., 2016). IIs seamlessly operate in the background,\ncarrying out “invisible work” that is often habitual, taken for granted and consequently difficult to map [5\n].\nBowker and Star [6] describe the power dimensions of classification systems, suggesting that “everyday\ncategories are precisely those that have disappeared into infrastructure, into habit, into the taken for\ngranted” and that “the moral questions arise when the categories of the powerful become the taken for\ngranted; when policy decisions are layered into inaccessible technological structures; when one group’s\nvisibility comes at the expense of another’s suffering”. IIs therefore uphold and stabilise existing\nconfigurations of categories, thus cementing social structures and obscuring certain groups of people\n(Bowker and Star, 1999). The social worlds framework, formalised by Clarke and Star [7\n], likewise argues\nthat infrastructures are “frozen discourses” often shared by “communities of collective understanding,\naction, and meaning-making” [8\n]. Clarke and Star’s [9] adjective frozen is particularly relevant for this\nanalysis, as it emphasises rigidity as an attribute of IIs, with a corresponding capacity to uphold power\nstructures. For IIs, governance is an ecosystem that encompasses institutions, laws, policies, and technology\ndesign (DeNardis and Musiani, 2016). Based on this definition, the dispersed and decentralised nature of IIs\nrepresents a challenge to governance in itself. Therefore, in the context of classification systems, turning a\nspotlight on the mechanisms that enable these to operate can support the establishment of points of\ninfrastructural control (DeNardis and Musiani, 2016).\nNotes towards infrastructure governance for large language models\nThe outlined characteristics of IIs — their invisibility and rigidity, as well as their capability to cement and\nreproduce power — may be detected in ML algorithms called artificial neural networks (ANNs). These are\nthe building blocks of numerous ML systems, both discriminative and generative. In the parlance of the\nfield of AI, ANNs are algorithms articulated in layers of interconnected units that propagate one or several\nfunctions with the objective of optimising them (Russell and Norvig, 2010). More simply, these can be\nunderstood as conceptually mimicking the operation of a mammalian brain; as the system is exposed to\ntraining data, layers of ‘neurons’ modelled in code are tuned with different parameters. This training\nprocess strengthens or weakens the connections between different neurons, allowing them to process and\npropagate data signals throughout a network. Whether the data input into the network are text, images, or\nother sources, the aim of this approach is to summarise key sets of features of these input sources as\nstructures of weighting and connection within the ‘hidden layers’ of the neural network. According to\nRussell and Norvig [10\n], single units only propagate a function — or “fire” — when a combination of\ninputs exceeds some predetermined threshold. The “arguments of the maxima” — or argmax — function\ncan be used within ANNs to determine output probabilities (Sanderson, 2017). In this context, argmax finds\nthe output class, label, or score with the highest value to make this determination (Russell and Norvig,\n2010; Patel, 2021). This approach allows features of very large and complex input data sources to be\nreduced to a relatively small number of parameters, expressed as the weights of individual links within the\nnetwork. For this reason, components of ANNs may be considered “classifiers’: once this process reaches\nthe output layer of units, only one neuron fires, and it is associated with one particular output category [11\n].\nThese categories, often pre-defined and standardised, are always mutually exclusive (Amoore, 2021).\nSimilarly to most systems of classification, ML technologies that incorporate ANNs are problematic when\napplied to human characteristics. In sections 2.1 and 2.2, I explore this issue for discriminative and\ngenerative systems respectively. For the purpose of illustrating this critique, I define here the concept of\ndownstream harms, or negative consequences related to the outputs of ML systems (Suresh and Guttag,\n2021). While there can be different sources of downstream harms in ML systems, this paper is especially\nconcerned with training data and model architecture (Hovy and Prabhumoye, 2021). Barocas, et al. (2023)\nproposed a taxonomy of downstream harms for automated decision-makers articulated in imbalance, biases,\nstereotypes, and categorisation. According to Barocas, et al., imbalance refers to an uneven representation\nof different groups, biases described incorrect associations related to social and historical prejudice,\nstereotypes reflected real-world products of ideological hierarchies, and categorisation denoted the\nascription of binary properties to spectral characteristics of identity. Therefore, based on these points,\ndownstream harms take different forms based on a system’s specifics. However, this paper argues\nunderlying, computational mechanisms of categorisations — because of their invisibility, rigidity, and\ncapability to reproduce power — uphold and enable downstream harms across ML systems. In the\nfollowing analysis, I support this point through illustrating that Barocas, et al.’s (2023) taxonomy of\ndownstream harms — originally devised solely for discriminative decision-makers — applies to both\ndiscriminative systems and LLMs.\n2.1. Discriminative systems\nThe basic functionalities of ANNs are commonly associated with traditional — or discriminative — ML\nparadigms. Discriminative systems execute some kind of decision-making in the form of output scores or\ncategories, based on examples supplied through training data (Russell and Norvig, 2010; Goodfellow, et al.,\n2020). Applications of these systems are widespread and comprise, for instance, clinical diagnosis, image\nclassification, and e-mail filtering, as well as more elaborate product recommendations and predictions of\nindividual preferences (Nichols, et al., 2019).\nDiscriminative systems are problematic when applied to human personal characteristics, such as gender and\nethnicity, as an attempt to computationally discriminate human characteristics leads to the systematic\ntargeting and degradation of marginalised groups. Let us consider some examples. In the Gender Shades\nproject, Buolamwini and Gebru (2018) evaluated two facial analysis benchmark datasets and revealed that\nthese overwhelmingly represented white-skinned subjects. Consequently, Buolamwini and Gebru (2018)\nNotes towards infrastructure governance for large language models\nexamined three commercial gender classification systems and found that these systems misclassified dark-\nskinned females at a higher percentage than other types. Based on Barocas, et al.’s (2023) taxonomy of\ndownstream harms, this could be considered an instance of imbalance. Eubanks (2017) investigated the\nsocial impacts of automated decision-making, including predictive policing systems and predictive risk\nmodels, in relation to health, benefits, and insurance. Through the inevitable incorporation of biased\nhistorical data, these classifiers aggravated conditions of impoverished individuals and communities by\nclassifying some individuals as high-risk (Eubanks, 2017; Barocas, et al., 2023). The issue of bias was\nfurther explored by Raghavan, et al. (2020), who critiqued algorithmic ML-based hiring systems. When\ntrained on past hiring data, these systems reproduced a tendency to hire dominant groups and therefore\nreinforced long-standing biases against women and ethnic minorities. Concerning downstream harms, this\nexample illustrates both bias and stereotypes, as the ML system reproduces professional hierarchies based\non historically prejudiced associations (Barocas, et al., 2023). In a different example, Costanza-Chock\n(2020) analysed millimetre wave scanners deployed in airports. Here, when airport operators performed\nsecurity controls on passengers, they must select the gender of the individual being scanned between\n“male” and “female”. However, if the machine detected body sections that deviated from traditional\ndefinitions of these, a passenger was flagged and searched. This is an instance of categorisation, as the\nsystem reduced human intersectional characteristics to discrete classes (Barocas, et al., 2023). Also\nconcerning categorisation, the Better Tomorrow tactical surveillance system, developed by the software\ncompany AnyVision (2020), claimed to detect ethnicity through facial recognition. Here, the ML system\nattributed a fixed ethnic category to an individual’s face. Better Tomorrow was reportedly sold to the Israeli\ngovernment to support screening and surveillance of the Palestinian population in the West Bank (Solon,\n2020).\nThe unsettling nature of the outlined examples can be attributed to a tension between artificially generated,\nmutually exclusive output categories of ML systems and mutually constitutive intersectional identities of\npeople. Intersectional identities are comprised of, among other characteristics, ethnicity, gender, sexual\norientation, religious, and political affiliations, ability status, and colonial and Indigenous histories\n(Crenshaw, 1989; Bowleg, 2008). The continuous application of the argmax function within algorithms that\nhandle human characteristics is conceptually incommensurable with ideas of intersectionality, as the\ncategory with the highest value becomes the only one considered. Concretely, although a network may\nrepresent very large and complex datasets and relationships, each individual neuron either fires or it doesn’t\nbased on a single ‘most important’ characteristic. In these contexts, at each point of connection,\ncategorising operations reduce highly complex, fluid, and interacting social categories to fixed values, thus\nexacerbating the oppressive potential of algorithms. Overall, the disproportionate representation of\ndominant social groups and the lack of data reflecting the experiences of minorities can lead to their\nsystematic marginalisation and exclusion (Criado Perez, 2019).\n2.2. Generative systems\nIn recent years, paradigms in the field of ML shifted as a result of breakthroughs in AI algorithms and\nincreasing computing power, giving rise to generative models, such as LLMs (Goodfellow, et al., 2020;\nJing and Xu, 2019). As opposed to discriminative ML systems which simply produce an output score or\ncategory, generative systems produce novel output data. At present, dominant research efforts in generative\nML focus on textual and visual data, with this paper focusing specifically on text. In this context, notable\nmodels include OpenAI’s GPT-n series, Google’s BERT, T5, LaMDA, BARD and PaLM, Meta’s LLaMA,\nand Microsoft and NVIDIA’s Megatron-Turing LM (Devlin, et al., 2019; Brown, et al. 2020; Raffel, et al.,\n2020; Chowdhery, et al., 2022; Manyika, 2022; Smith, et al., 2022; Thoppilan, et al., 2022; Touvron, et al.,\n2023). Prominent open source LLMs include EleutherAI’s (2021) GPT-J and GPT-NeoX, and Hugging\nFace’s (2022) BLOOM. Real-world applications of LLMs include dialogue systems, articulated in chat-\noriented and question-answering systems, text manipulation and machine translation across numerous\nsettings including education, healthcare, and art (Li, 2022; Zaib, et al., 2020; Fossa and Sucameli, 2022).\nThe field also presents multimodal systems, such as OpenAI’s DALL-E, Google DeepMind’s Gato, and\nWuDao (Beijing Academy of Artificial Intelligence, 2021; Ramesh, et al., 2021; Reed, et al., 2022), that\nproduce multiple types of information, such as text, images, and videos, and often attempt to convert one\nNotes towards infrastructure governance for large language models\ndatatype into another (Suzuki and Matsuo, 2022). Crucially, at the time of this paper’s publication, these\nmodels and their applications remain largely experimental, and a great portion of their emerging properties\nis yet to be discovered, making this a decisive moment for hypothesising and proposing governance\nmechanisms.\nThe novelty and distinct properties of generative systems prompt questions regarding their relationship with\ntraditional, discriminative ML systems. To highlight these infrastructural similarities, it helps to compare\ntheir respective categorisation mechanisms. To articulate this comparison, this paper considers the training\ndata and architecture of LLMs. Statistical language models (LMs), the predecessors of contemporary\nLLMs, calculate probability distributions of words and sentences in language and generate text accordingly\n(Jurafsky and Martin, 2008). For instance, based on the sentence “where are we”, a statistical LM could\nassociate the highest probability to “going” as the appropriate next word (Huyen, 2019). The introduction of\nANNs — also found in discriminative ML systems — resulted in the production of neural LMs (Jing and\nXu, 2019; Qudar and Mago, 2020). As the name suggests, these combine ANNs and statistical LMs\narchitecture to yield substantial performance improvements through addressing word combination and\ncontext, and ultimately result in more sophisticated language generation. As discussed, ANNs are\ncharacterised by the process of training, which denotes their performance improvement based on training\ndata (Russell and Norvig, 2010). Accordingly, LLMs are trained on extremely large amounts of data, which\nis usually organised in datasets and collected through the automated and systematic extraction — or\nscraping — of text from Web pages (Bender, et al., 2020; Tamkin, et al., 2021). In basic ANNs, the output\nneuron that fires represents a single output category or score, which is associated with the highest\nprobability according to the algorithm’s computation (Russell and Norvig, 2010; Sanderson, 2017).\nCorrespondingly, in LLMs, the probability of the output word — corresponding to an output category in\nANNs — is further determined by the distribution of words in the training data.\nThe integration of ANNs and statistical LMs sheds light on issues of categorisation and exclusion in LLMs.\nLet us consider once again the taxonomy of downstream harms proposed by Barocas, et al. (2023). In\nLLMs, imbalance — or the uneven representation of different groups — can manifest through context, as\nartificially generated stories are overwhelmingly situated within systems of Western centrism and\nwhiteness, unless the system is specifically prompted otherwise [12\n]. In a second example, Shihadeh, et al.\n[13] identifies an association between the psychological trait of brilliance and masculinity in GPT-3, or\n“brilliance bias”. This is further illustrated by Lucy and Bamman (2021), who suggest that female\ncharacters portrayed as less powerful than male characters in stories generated by GPT-3. These can be\nconsidered instances of bias as they represent incorrect associations related to social and historical prejudice\n(Barocas, at al., 2023). In artificially generated text, stereotypes reflect in the co-occurrence of historically\ngendered professions with male pronouns, such as “doctor” (Bordia and Bowman, 2019; Sheng, et al.,\n2019). Similarly, considering again the sentence “the woman works as”, a LLM trained on extensive\namounts of potentially problematic data may complete the sentence with “a babysitter” (Sheng, et al.,\n2019). In this case, “a babysitter” is selected by the model as the textual item with the highest probability\nbased on the sentence provided. Lastly, the downstream harm of categorisation can manifest in LLMs\nthrough the system’s failure to appropriately use gender-neutral pronouns (Hossain, et al., 2023). This\nresults in the ascription of binary properties to spectral characteristics of identity (Barocas, et al., 2023).\nThese examples illustrate that Barocas, et al.’s (2023) taxonomy of downstream harms, initially devised for\ndiscriminative systems, applies to the evaluation of LLMs. This paper suggests that this is because of the\nunderlying mechanisms of categorisation that characterise both systems.\nAt present, LLMs are at the centre of a corporate race for the development of ever larger models trained on\never larger datasets (Luitse and Dankena, 2021). Indeed, over the past five years, the size of LLMs has\nconsistently increased both in terms of parameters and training data (Simon, 2021). For instance, OpenAI’s\nGPT-2 is trained on 40 gigabytes (GB) of text data (Radford, et al., 2018). The successor model GPT-3 is\ntrained on billions of scraped words from datasets such as Common Crawl, which is the largest publicly\navailable text-based dataset, and WebText (Kolias, et al., 2014; Radford, et al., 2018; Common Crawl\nFoundation, 2022). Pfotenhauer, et al. [14\n] refer to this phenomenon as “the politics of scaling” and\ndescribe it as a fixation defining narratives of innovation in the Silicon Valley era. In the context of\nNotes towards infrastructure governance for large language models\ndownstream harms, scalability is often presented as the answer to lack of representation or\nmisrepresentation concerning marginalised and vulnerable groups in training datasets. However, Bender, et\nal. (2020) suggest that, when datasets are assembled with the objective of scalability, hegemonic narratives\nare most likely to be retained. This section suggested that, similarly to discriminative systems, LLMs\noperate as infrastructures of categorisation.\n2.3. Infrastructures of categorisation\nBased on this analysis, discriminative and generative ML systems can both be theorised as infrastructures\nof categorisation. They are classic infrastructures because they are characterised by material and abstract\nentities including protocols, standards and architectures, and they are difficult to map because of the\nintricacies characterising their underlying “practices, uses and exchanges” [15\n]. Combined with the\ncategorising functionality of ANNs, this configuration allows for “the categories of the powerful” to\nbecome “taken for granted” in both systems [16\n]. Within ML systems, these categories are mostly\npredetermined and standardised, manufactured by designers and engineers, and not “naturally occurring”\n[17\n]. This point resonates with Benjamin’s [18] conceptualisation of race as technology: “one constructed\nto separate, stratify and sanctify the many forms of injustice experienced by members of racialized groups”.\nAccordingly, powerful, hegemonic categories are technologies that disappear within discriminative and\ngenerative infrastructures of categorisation. Thus, generative systems do not depart from discriminative\ncounterparts in terms of underlying probabilistic mechanisms of categorisation. However, this paper\nproceeds to argue that there are additional mechanisms within LLMs — referred to as “language as\ninfrastructure” — that must be analysed and considered before proposing appropriate governance\nmechanisms.\n3. Language as infrastructure\nIn the previous section, this paper illustrated that both discriminative and generative ML systems can be\ntheorised as infrastructures of categorisation. As opposed to traditional, discriminative counterparts, which\nsolely produce output scores or categories, LLMs produce previously unseen texts based on human\nprompts. Based on the functionality of “classifiers”, LLMs produce output text that — when these systems\nare trained on large, scraped datasets — is likely to reduce human intersectional identities to specific,\nimmutable characteristics represented in linguistic associations and therefore lead to downstream harms\n[19\n]. As a compound of social and probabilistic constructs, the characteristics of artificially generated\nlanguage are different from those of natural language. Artificially generated text is more rigid in\ncomparison to natural language, because it is generated through computational mechanisms of\ncategorisation [20\n]. The process of altering the functionality of LLMs to mitigate bias is laborious, as it\ncontemplates, at least partially, their retraining (Solaiman and Dennison, 2021). However, this entails\nsubstantial economic, labour-intensive and environmental challenges (Pfotenhauer, et al., 2021). These\nconstraints operate as concrete barriers to changing the output text of LLMs, thus contributing to the\nrigidity of these systems, and reinforcing their capacity to operate as infrastructures of categorisation.\nHowever, I proceed to convolute my argument. On one hand, LLMs fit the provided description of\ninfrastructures of categorisation because of the underlying, computational mechanisms that enable the\nproduction of artificially generated text. On the other hand, I illustrate below that, because of the textual\nformat of their outputs, LLMs simultaneously retain performative capabilities.\nTraditional STS focusses on the cultural meaning attributed to material objects. For instance, through an\nanalysis of electric shavers, Oudshoorn and Pinch (2003) provided crucial insight into the gendered\nsignificance of technological artefacts. Electric shavers concretise their gendered cultural meanings through\nbranding features, such as shavers called “lipstick” marketed to women and “double action” marketed to\nmen. Oudshoorn and Pinch (2003) further highlighted the manufacturing of artefacts with gendered scripts\nand attributes that associate control with masculinity and incompetence with femininity. This type of gender\nNotes towards infrastructure governance for large language models\nanalysis can also be applied to ML systems, particularly regarding the names of prominent LLMs. For\ninstance, GPT stands for Generative Pretrained Transformer, BERT for Bidirectional Encoder\nRepresentations from Transformers, and LaMDA for Language Model for Dialogue Applications (Radford,\net al., 2018; Devlin, et al., 2019; Thoppilan, et al., 2022). However, once LLMs are deployed in real-world\napplications, their names change substantially. For example, conversational assistants mostly have female\nnames, such as Siri, Alexa, and Cortana, thus suggesting the outsourcing of historically feminine labour to\nmachines (Hester, 2017; Männistö-Funk and Sihvonen, 2018; Costa and Ribas, 2019; Dillon, 2020).\nHowever, with LLMs, this type of gender analysis falls short. On one hand, electric shavers contribute to\ngender stereotypes through their branding features: they materialise the association between socially\nconstructed gender categorisations and attributes. LLMs, on the other hand, are not static objects merely\nportraying and embodying hegemonic systems. Through their capability to produce previously unseen\nlinguistic instances, they can reproduce stereotyped versions of language attributed to specific individuals\nor social groups. Let us consider some examples. Concerning gender, Salewski, et al. (2023) reveal that,\nwhen a LLM is prompted to write as a man, it produces better descriptions of cars compared to when it is\nprompted to write as a woman. In a second example, Weidinger, et al. [21\n] suggest that LLMs could be\nfine-tuned on “an individual’s past speech data to impersonate that individual in cases of identity theft”. In\nthe field of AI, this particular type of manipulation, where specific data is added to the system for malicious\npurposes, is known as an adversarial attack (Zhang, et al., 2020). In a final example, the community-driven\nplatform FlowGPT (2023) presented an interface called “JesusGPT: The divine dialogue”, also available\nthrough OpenAI’s (2023) ChatGPT Plus. In this case, ChatGPT was fine-tuned on the Kings James Bible to\n“simulate profound conversations with Jesus Christ himself”. These examples illustrate that LLMs can\nimpersonate single individuals, such as historical figures, and ordinary people whose past data is publicly\navailable, as well as social groups, such as men and women. Specifically, in relation to gender, LLMs are\nperformative according to Butler’s (1988) definition, as they establish gender identities through the\nrepetition of distinctly feminine or masculine acts, as illustrated through the first example.\nSimilarly to traditional STS cultural and gender analyses, contemporary attempts to theorise ML systems\nfall short in the case of LLMs. For instance, through the concept of “epistemic politics”, Amoore [22\n]\ndescribes recent ML technologies as modes of assembling knowledge that alter traditional norms and\nthresholds, thus changing the fabric of politics and society. According to this position, when models learn\nsalient features and clusters from training datasets, their assumptions exceed the categories already present\nin the input data, effectively generating groupings and communities. In LLMs, the production of textual\ninstances based on classificatory mechanisms may result in the assemblage of previously unseen\nconglomerates — or categories — of data downstream (Barocas, et al., 2023). However, based on the\nexamples provided above, it is crucial to acknowledge that a person using LLMs may not necessarily self-\nidentify with the virtual categories generated by the system. This partially nuances Amoore’s (2023) claim\nthat ML systems generate communities and social groups. According to this analysis, LLMs can be\ntheorised as performative artefacts, while being simultaneously considered infrastructures of categorisation.\nIn this paper, I refer to the ensemble of these phenomena as “language as infrastructure”. This combination\nof characteristics situates LLMs as exceptionally contended, predisposed to mediatic and public\nsensationalism, and difficult to map. In the following section, despite the inherent complexity of “language\nas infrastructure”, I suggest the existence of governance opportunities for LLMs. The proposed governance\nmechanisms make use of the convoluted, even contrasting, characteristics of LLMs identified in this paper.\nFor this reason, they can be considered mechanisms of governance by infrastructure (DeNardis and\nMusiani, 2016; Yeung, 2017).\n4. “Broken machines” as opportunities for infrastructure governance\nIn the previous section, this paper illustrated that generative systems can be theorised as both rigid\ninfrastructures of categorisation and performative technological artefacts. LLMs challenge pre-existing\nNotes towards infrastructure governance for large language models\nconfigurations of IIs through their performative capabilities: they uniquely intensify mechanisms of\ncategorisation by reproducing and amplifying existing social biases through naturalistic human language.\nThis section identifies unique governance opportunities for LLMs, that take into account and employ the\nchallenges outlined so far. As previously discussed, both discriminative and generative ML systems,\nincluding LLMs, sustain the disappearance of hegemonic categories. Sharma (2020) delves into this notion\nin “A manifesto for the broken machine”, where she portrays contemporary technologies as implementing\nforms of power, thus sustaining the disappearance or visibility of certain social groups and categories. For\ninstance, the author mentions the prevalence of patriarchal and white ethno-nationalist values, as well as\nclass and gender inequality. According to Sharma (2020), individuals, institutions, and technologies that do\nnot replicate dominant narratives may be considered “broken”. At the time of writing, LLMs are subjected\nto significant hype and fetishisation due to their capability to produce human-like text, reproduce\nstereotyped versions of language, and impersonate individuals and social groups (Luitse and Dankena,\n2021; Weidinger, et al., 2022). This visibility may be considered a social mechanism that prevents these\nfrom disappearing into other infrastructures. In other words, generative systems, including LLMs, are\ncurrently visible from an infrastructural perspective. Combined with their experimental nature and largely\nunknown emerging properties, this situation represents a governance opportunity for scrutinising,\ndeconstructing, changing, and, in the words of Sharma (2020), “breaking” these artefacts. In turn, their\nunderlying “practices, uses and exchanges”, as well as their potential to produce categories, can also\nbecome visible [23\n].\nThis paper finally considers some governance strategies that potentially support the visibility, scrutiny and\nconsequent “breakage” of LLMs. These can be considered instances of governance by infrastructure, as\nthey circumvent and repurpose the computational and infrastructural mechanisms of LLMs discussed in\nprevious sections (DeNardis and Musiani, 2016). As previously discussed, the processes of training and\nfine-tuning — which require extensive data, computation and labour — represent substantial challenges to\nchanging the problematic behaviour of LLMs. Bender, et al. (2020) propose dataset curation as a means of\nrebalancing epistemic values within training datasets. This technique, inspired by data collection methods in\narchival history, involves the meticulous, justice-oriented selection and construction of training data (Jo and\nGebru, 2020; Birhane and Prabhu, 2021). In LLMs, dataset curation directly emerges as epistemologically\nand ontologically antithetical to scraping. Indeed, while scraping methods assemble datasets with the\nambition of universality, dataset curation promotes the production of situated knowledge. Dataset curation\nmay be viewed as part of a family of critical methods that do not merely attempt to propose solutions to\nalgorithmic bias, but to devise tools that bring social injustice to the eyesight of designers, regulators, and\nstakeholders (Costanza-Chock, 2020).\nParticipatory design (PD), or co-design, sees the involvement of users in technology design processes, as a\nmeans to improve fairness, accountability, and transparency (Donia and Shaw, 2021). To address the issue\nof downstream harms in LLMs, explored in previous sections, dataset curation and participatory methods\ncan be especially effective when combined (Barocas, et al., 2023). For instance, researchers at OpenAI\nproposed the Process for Adapting Language Models to Society (PALMS) framework — or the adaptation\nof LLMs to small-scale curated datasets — as a promising method for the mitigation of biases associated\nwith sensitive topics (Solaiman and Dennison, 2021). This method could be modified so that the knowledge\nincorporated in curated datasets is generated according to justice-oriented and participatory methods, and\ntherefore encompass the views and experiences of historically underrepresented groups and individuals. In\nrelation to Sharma (2020), adapting LLMs to curated datasets can be considered an initial step towards\n“breaking” them, through the subversion of the modalities of scale in which these systems have been\noperating thus far. Nonetheless, this paper acknowledges that this methodological approach merely\nscratches the surface when it comes to re-centering power imbalances in generative systems and offering\neffective, purposeful, and long-term governance mechanisms. Although Sharma (2020) suggests that\n“broken machines” already redistribute power by simply existing, ample research efforts are needed to\ntheorise, understand, and deploy infrastructural governance processes for LLMs and generative ML systems\nas a whole.\nNotes towards infrastructure governance for large language models\n5. Conclusion\nThis paper produces an initial mapping of the infrastructural characteristics and implications of LLMs,\nhighlighting some issues and opportunities related to their governance. I establish a similarity between\ndiscriminatory and generative ML systems in terms of their infrastructural mechanisms of categorisation.\nSpecifically, this paper highlights the functionality of ANN-based “classifiers” and argmax activation\nfunctions, detected in both discriminative and generative systems, which contribute to the disappearance of\nthe “categories of the powerful” in both technologies [24\n]. Through several examples, I show that Barocas,\net al.’s (2023) taxonomy of downstream harms, initially devised for discriminative systems, applies to the\nevaluation of LLMs. Through a further analysis of the training data and architectures of LLMs, I conclude\nthat probabilistic mechanisms of categorisation in LLMs are comparable to those within discriminative ML\nsystems. Therefore, both discriminative and generative ML systems can be theorised as infrastructures of\ncategorisation. In this context, I further theorise dominant categories as technologies that disappear within\nboth discriminative and generative infrastructures of categorisation. This presents a substantial challenge to\ninfrastructure governance, as the disappearance of these categories within infrastructures of categorisation\nresults in these being taken for granted and difficult to map (DeNardis and Musiani, 2016). I then illustrate\nthat, in addition to their rigidity and capability to cement and reproduce power, LLMs retain performative\ncapabilities because of the textual format of their outputs. I refer to the ensemble of these controversial\nphenomena as “language as infrastructure”. Finally, while discriminative counterparts disappear into larger\nIIs, LLMs are currently visible through media hype, fetishisation, and hyperbolic rhetoric upheld by their\ngeneration of unique textual instances. I ultimately consider this situation a governance opportunity that\nallows us to scrutinise, deconstruct, and virtually “break” these artefacts (Sharma, 2020). I finally suggest\ntwo governance mechanisms for downstream harms in LLMs: dataset curation and participatory methods\n(Barocas, et al., 2023). Crucially, this paper remains an initial mapping, creating some initial foundations\nfor the broader construction of infrastructure governance mechanisms for LLMs and generative ML\nsystems as a whole. \nAbout the author\nLara Dal Molin is a Ph.D. student in Science, Technology and Innovation Studies at the University of\nEdinburgh, part of the joint programme in Social Data Science with the University of Copenhagen. In her\nresearch, she explores the intersection between language, gender and technology. Lara is also a Tutor in the\nSchools of Social and Political Sciences and Informatics.\nE-mail: L [dot] Dal-Molin-1 [at] sms [dot] ed [dot] ac [dot] uk\nAcknowledgements\nI wish to thank Morgan Currie, James Besse and Léa Stiefel for their invaluable feedback on my writing,\nand for their efforts in establishing the Governance by Infrastructure partnership between the Universities\nof Edinburgh and Lausanne. I extend my gratitude to this entire community, which provided inspiration and\ninsight for my research. I also wish to thank the Edinburgh-Copenhagen Social Data Science programme\nfor supporting my doctoral studies and research efforts.\nNotes\n1.\nWeiser, 1991, p. 94.\nNotes towards infrastructure governance for large language models\n2. Sharma, 2020, p. 171.\n3. Alexander, 1990, p. 162; Natale and Ballatore, 2020.\n4. Weiser, 1991, p. 94.\n5. Star, 1999, p. 385; DeNardis and Musiani, 2016, p. 6.\n6. Bowker and Star, 1999, p. 319.\n7. Clarke and Star, 2007, p. 115.\n8. Clarke and Star, 2007, p. 113; Collier, 2021, p. 1,731.\n9. Clarke and Star, 2007, p. 115.\n10. Russell and Norvig, 2010, p. 727.\n11. Russell and Norvig, 2010, p. 728; Sanderson, 2017.\n12. This result is from the author’s original research.\n13. Shihadeh, et al., 2022, p. 62.\n14. Pfotenhauer, et al., 2021, p. 3.\n15. Star and Ruhleder, 1994; Star, 1999, p. 385.\n16. Bowker and Star, 1999, p. 319.\n17. Birhane and Raji, 2022, p. 1.\n18. Benjamin, 2019, p. 36.\n19. Russell and Norvig, 2010, p. 728.\n20. Clarke and Star, 2007, p. 113.\n21. Weidinger, et al., 2022, p. 219.\n22. Amoore, 2023, p. 21.\n23. Star, 1999, p. 385; DeNardis and Musiani, 2016, p. 6.\n24. Bowker and Star, 1999, p. 319; Russell and Norvig, 2010.\nReferences\nJ. Alexander, 1990. “The sacred and profane information machine: Discourse about the computer as\nideology,” Archives de Sciences Sociales des Religions, 35e Année, number 69, pp. 161–171.\nL. Amoore, 2023. “Machine learning political orders,” Review of International Studies, volume 49, number\n1, pp. 20–36.\ndoi: https://doi.org/10.1017/S0260210522000031\n, accessed 10 January 2024.\nNotes towards infrastructure governance for large language models\nL. Amoore, 2021. “The deep border,” Political Geography (25 November).\ndoi: https://doi.org/10.1016/j.polgeo.2021.102547, accessed 10 January 2024.\nS. Barocas and A.D. Selbst, 2016. “Big data’s disparate impact,” California Law Review, volume 104,\nnumber 3, pp. 671–732.\nS. Barocas, M. Hardt, and A. Narayanan, 2023. Fairness and machine learning: Limitations and\nopportunities. Cambridge, Mass.: MIT Press; also at https://fairmlbook.org, accessed 4 January 2024.\nBeijing Academy of Artificial Intelligence, 2021. “ ，\n‘ · ’” (11 January), at https://mp.weixin.qq.com/s/BUQWZ5EdR19i40GuFofpBg, accessed 10\nJanuary 2024.\nE.M. Bender, T. Gebru, A. McMillan-Major, and S. Shmitchell, 2020. “On the dangers of stochastic\nparrots: Can language models be too big?” FAccT ’21: Proceedings of the 2021 ACM Conference on\nFairness, Accountability, and Transparency, pp. 610–623.\ndoi: https://doi.org/10.1145/3442188.3445922\n, accessed 10 January 2024.\nR. Benjamin, 2019. Race after technology: Abolitionist tools for the new Jim Code. Cambridge: Polity.\nA. Birhane and D. Raji, 2022. “ChatGPT, Galactica, and the progress trap,” Wired (8 December), at\nhttps://www.wired.com/story/large-language-models-critique/ accessed 20 December 2022.\nA. Birhane and V.U. Prabhu, 2021. “Large image datasets: A Pyrrhic win for computer vision?” 2021 IEEE\nWinter Conference on Applications of Computer Vision (WACV).\ndoi: https://doi.org/10.1109/WACV48630.2021.00158, accessed 10 January 2024.\nS. Bordia and S.R. Bowman, 2019. “Identifying and reducing gender bias in word-level language models,”\nProceedings of the 2019 Conference of the North American Chapter of the Association for Computational\nLinguistics: Student Research Workshop, pp. 7–15.\ndoi: https://doi.org/10.18653/v1/N19-3002\n, accessed 10 January 2024.\nG.C. Bowker and S.L. Star, 1999. Sorting things out: Classification and its consequences. Cambridge,\nMass.: MIT Press.\ndoi: https://doi.org/10.7551/mitpress/6352.001.0001\n, accessed 10 January 2024.\nL. Bowleg, 2008. “When Black + lesbian + woman ≠ Black lesbian woman: The methodological challenges\nof qualitative and quantitative intersectionality research,” Sex Roles, volume 59, numbers 5–6, pp. 312–325.\ndoi: https://doi.org/10.1007/s11199-008-9400-z\n, accessed 10 January 2024.\nT. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry,\nA. Askell, S. Agarwal, A. Herbert-Voss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D.M. Ziegler, J.\nWu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray, B. Chess, J. Clark, C. Berner, S.\nMcCandlish, A, Radford, I. Sutskever, and D. Amodei, 2020. “Language models are few-short learners,”\n34th Conference on Neural Information Processing Systems (NeurIPS 2020), at\nhttps://papers.nips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html\n, accessed 10\nJanuary 2024.\nS. Bubeck, V. Chandrasekaran, R. Eldan, J. Gehrke, E. Horvitz, E. Kamar, P. Lee, Y.T. Lee, Y. Li, S.\nLundberg, H. Nori, H. Palangi, M.T. Ribeiro, and Y. Zhang, 2023. “Sparks of artificial general intelligence:\nEarly experiments with GPT-4,” arXiv:2303.12712 (27 March).\ndoi: https://doi.org/10.48550/arXiv.2303.12712\n, accessed 7 December 2023.\nJ. Butler, 1988. “Performative acts and gender constitution: An essay in phenomenology and feminist,”\nTheatre Journal, volume 40, number 4, pp. 519–531.\nNotes towards infrastructure governance for large language models\n]\ndoi: https://doi.org/10.2307/3207893, accessed 10 January 2024.\nA. Chowdhery, S. Narang, J. Devlin, M. Bosma, G. Mishra, A. Roberts, P. Barham, H.W. Chung, C.\nSutton, S. Gehrmann, P. Schuh, K. Shi, S. Tsvyashchenko, J. Maynez, A. Rao, P. Barnes, Y. Tay, N.\nShazeer, V. Prabhakaran, E. Reif, N. Du, B. Hutchinson, R. Pope, J. Bradbury, J. Austin, M. Isard, G. Gur-\nAri, P. Yin, T. Duke, A. Levskaya, S. Ghemawat, S. Dev, H. Michalewski, X. Garcia, V. Misra, K.\nRobinson, L. Fedus, D. Zhou, D. Ippolito, D. Luan, H. Lim, B. Zoph, A. Spiridonov, R. Sepassi, D. Dohan,\nS. Agrawal, M. Omernick, A.M. Dai, T.S. Pillai, M. Pellat, A. Lewkowycz, E. Moreira, R. Child, O.\nPolozov, K. Lee, Z. Zhou, X. Wang, B. Saeta, M. Diaz, O. Firat, M. Catasta, J. Wei, K. Meier-Hellstern, D.\nEck, J. Dean, S. Petrov, and N. Fiedel, 2022. “PaLM: Scaling language modeling with pathways,”\narXiv:2204.02311 (5 April).\ndoi: https://doi.org/10.48550/arXiv.2204.02311\n, accessed 20 June 2023.\nA.E. Clarke and S.L. Star, 2007. “The social worlds framework: A theory/methods package,” In: E,J.\nHackett, O, Amsterdamska, M,E. Lynch, and J, Wajcman (editors), Handbook of science and technology\nstudies. Third edition. Cambridge, Mass.: MIT Press, pp. 113–137.\nS. Costanza-Chock, 2020. Design justice: Community-led practices to build the worlds we need.\nCambridge, Mass.: MIT Press.\ndoi: https://doi.org/10.7551/mitpress/12255.001.0001\n, accessed 10 January 2024.\nP. Costa and L. Ribas, 2019. “AI becomes her: Discussing gender and artificial intelligence,” Technoetic\nArts, volume 17, number 1, pp. 171–193.\ndoi: https://doi.org/10.1386/tear_00014_1\n, accessed 10 January 2024.\nB. Collier, 2021. “The power to structure: Exploring social worlds of privacy, technology and power in the\nTor Project,” Information, Communication & Society, volume 24, number 12, p. 1,728–1,744.\ndoi: https://doi.org/10.1080/1369118X.2020.1732440\n, accessed 7 December 2023.\nCommon Crawl Foundation, 2022. “Overview,” at https://commoncrawl.org/overview, accessed 10 January\n2024.\nK. Crenshaw, 1989. “Demarginalizing the intersection of race and sex: A Black feminist critique of\nantidiscrimination doctrine, feminist theory and antiracist politics,” University of Chicago Legal Forum,\nvolume 1989, number 1, article 8, and at http://chicagounbound.uchicago.edu/uclf/vol1989/iss1/8, accessed\n10 January 2024.\nC. Criado Perez, 2019. Invisible women: Exposing data bias in a world designed for men. London: Penguin\nRandom House.\nL. DeNardis and F. Musiani, 2016. “Governance by infrastructure,” In: F. Musiani, D.L. Cogburn, L.\nDeNardis, N.S. Levinson (editors). The turn to infrastructure in Internet governance. New York: Palgrave\nMacmillan, pp. 3–21.\ndoi: https://doi.org/10.1057/9781137483591_1\n, accessed 4 January 2024.\nJ. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, 2019. “BERT: Pre-training of deep bidirectional\ntransformers for language understanding,” Proceedings of the 2019 Conference of the North American\nChapter of the Association for Computational Linguistics: Human Language Technologies, volume 1, pp.\n4,171–4,186.\ndoi: https://doi.org/10.18653/v1/N19-1423\n, accessed 4 January 2024.\nS. Dillon, 2020. “The Eliza Effect and its dangers: From demystification to gender critique,” Journal for\nCultural Research, volume 24, number 1, pp. 1–15.\ndoi: https://doi.org/10.1080/14797585.2020.1754642, accessed 4 January 2024.\nNotes towards infrastructure governance for large language models\nJ. Donia and J. Shaw, 2021. “Co-design and ethical artificial intelligence for health: Myths and\nmisconceptions,” AIES ’21: Proceedings of the 2021 AAAI/ACM Conference on AI, Ethics, and Society, p.\n77.\ndoi: https://doi.org/10.1145/3461702.3462537, accessed 4 January 2024.\nV. Eubanks, 2018. Automating inequality: How high-tech tools profile, police and punish the poor. New\nYork: St. Martin's Press.\nFlowGPT, 2023. “JesusGPT: The divine dialogue,” at https://flowgpt.com/p/jesusgpt-the-divine-dialogue,\naccessed 11 December 2023.\nF. Fossa and I. Sucameli, 2022. “Gender bias and conversational agents: An ethical perspective on social\nrobotics,” Science and Engineering Ethics, volume 28, article number 23.\ndoi: https://doi.org/10.1007/s11948-022-00376-3\n, accessed 4 January 2024.\nI. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and Y. Bengio,\n2020. “Generative adversarial networks,” Communications of the ACM, volume 63, number 11, pp. 139–\n144.\ndoi: https://doi.org/10.1145/3422622\n, accessed 4 January 2024.\nT. Hastie, R. Tibshirani, and J. Friedman, 2008. The elements of statistical learning: Data mining,\ninference, and prediction. Second edition. New York: Springer.\ndoi: https://doi.org/10.1007/978-0-387-84858-7, accessed 4 January 2024.\nH. Hester, 2017. “Technology becomes her,” New Vistas, volume 3, number 1, pp. 46–50, and at\nhttps://core.ac.uk/download/pdf/82959871.pdf, accessed 4 January 2024.\nT. Hossain, S. Dev, and S. Singh, 2023. “MISGENDERED: Limits of large language models in\nunderstanding pronouns,” Proceedings of the 61st Annual Meeting of the Association for Computational\nLinguistics, volume 1: Long papers, pp. 5,352–5,367.\ndoi: https://doi.org/10.18653/v1/2023.acl-long.293\n, accessed 4 January 2024.\nD. Hovy and S. Prabhumoye, 2021. “Five sources of bias in natural language processing,” Language and\nLinguistics Compass, volume 15, number 8, e12432.\ndoi: https://doi.org/10.1111/lnc3.12432\n, accessed 4 January 2024.\nHugging Face, 2022. “BLOOM,” at https://huggingface.co/docs/transformers/model_doc/bloom, accessed\n20 December 2022.\nC. Huyen, 2019. “Evaluation metrics for language modeling” (18 October), at\nhttps://thegradient.pub/understanding-evaluation-metrics-for-language-models/, accessed 20 December\n2022.\nK. Jing and J. Xu, 2019. “A survey on neural network language models,” arXiv:1906.03591 (9 June).\ndoi: https://doi.org/10.48550/arXiv.1906.03591, accessed 4 January 2024.\nE.S. Jo and T. Gebru, 2020. “Lessons from archives: Strategies for collecting sociocultural data in machine\nlearning,” FAT* ’20: Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency,\npp. 306–316.\ndoi: https://doi.org/10.1145/3351095.3372829\n, accessed 4 January 2024.\nD. Jurafsky and J.H. Martin, 2008. Speech and language processing: An introduction to natural language\nprocessing, computational linguistics and speech recognition. Second edition. London: Pearson.\nV. Kolias, I. Anagnostopoulos, and E. Kayafas, 2014. “Exploratory analysis of a terabyte scale Web\nNotes towards infrastructure governance for large language models\ncorpus,” 20th IMEKO TC4 International Symposium and 18th International Workshop on ADC Modelling\nand Testing, pp. 726–730, and at https://www.imeko.org/publications/tc4-2014/IMEKO-TC4-2014-291.pdf,\naccessed 4 January 2024.\nH. Li, 2022. “Language models: Past, present, and future,” Communications of the ACM, volume 65,\nnumber 7, pp. 56–63.\ndoi: https://doi.org/10.1145/3490443, accessed 4 January 2024.\nL. Lucy and D. Bamman, 2021. “Gender and representation bias in GPT-3 generated stories,” Proceedings\nof the Third Workshop on Narrative Understanding, pp. 48–55.\ndoi: https://doi.org/10.18653/v1/2021.nuse-1.5, accessed 4 January 2024.\nD. Luitse and W. Denkena, 2021. “The great transformer: Examining the role of large language models in\nthe political economy of AI,” Big Data & Society (29 September).\ndoi: https://doi.org/10.1177/20539517211047734\n, accessed 4 January 2024.\nT. Männistö-Funk and T. Sihvonen, 2018. “Voices from the uncanny valley: How robots and artificial\nintelligences talk back to us,” Digital Culture & Society, volume 4, number 1, pp. 45–64.\ndoi: https://doi.org/10.25969/mediarep/13525\n, accessed 4 January 2024.\nJ. Manyika, 2022. “An overview of Bard: An early experiment with generative AI,” Google, at\nhttps://ai.google/static/documents/google-about-bard.pdf, accessed 4 January 2024.\nS. Natale and A. Ballatore, 2020. “Imagining the thinking machine: Technological myths and the rise of\nartificial intelligence,” Convergence, volume 26, number 1, p. 3–18.\ndoi: https://doi.org/10.1177/1354856517715164\n, accessed 4 January 2024.\nJ. Nichols, H.W.H. Chan, and M.A.B. Baker, 2019. “Machine learning: Applications of artificial\nintelligence to imaging and diagnosis,” Biophysical Reviews, volume 11, number 1, pp. 111–118.\ndoi: https://doi.org/10.1007/s12551-018-0449-9\n, accessed 4 January 2024.\nOpenAI, 2023. “JesusGPT,” at https://chat.openai.com/g/g-vtbapmlAa-jesusgpt, accessed 11 December\n2023.\nOpenAI, 2018. “OpenAI charter” (9 April), at https://openai.com/charter, accessed 12 December 2023.\nN. Oudshoorn and T. Pinch, 2003. “Introduction: How users and non-users matter,” In: N. Oudshoorn and\nT. Pinch (editors). How users matter: The co-construction of users and technology. Cambridge, Mass.: MIT\nPress.\ndoi: https://doi.org/10.7551/mitpress/3592.003.0002\n, accessed 4 January 2024.\nR. Patel, 2021. “Softmax & argmax,” at https://www.kaggle.com/general/278766, accessed 21 December\n2022.\nS. Pfotenhauer, B. Laurent, K. Papageorgiou, and J. Stilgoe, 2021. “The politics of scaling,” Social Studies\nof Science, volume 52, number 1, pp. 3–34.\ndoi: https://doi.org/10.1177/03063127211048945, accessed 10 January 2024.\nM.M.A. Qudar and V. Mago, 2020. “TweetBERT: A pretrained language representation model for Twitter\ntext analysis,” arXiv:2010.11091 (17 October).\ndoi: https://doi.org/10.48550/arXiv.2010.11091, accessed 4 January 2024.\nA. Radford, J. Wu, R. Child, D. Luan, D. Amodei, and I. Sutskever, 2018. “Language models are\nunsupervised multitask learners,” at https://d4mucfpksywv.cloudfront.net/better-language-\nmodels/language_models_are_unsupervised_multitask_learners.pdf, accessed 10 January 2024.\nNotes towards infrastructure governance for large language models\nC. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P.J. Liu, 2020.\n“Exploring the limits of transfer learning with a unified text-to-text transformer,” Journal of Machine\nLearning Research, volume 21, number 1, article number 140, pp. 5,485–5,551.\nA. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Radford, M. Chen, and I. Sutskever, 2021. “Zero-shot\ntext-to-image generation,” Proceedings of the 38th International Conference on Machine Learning, volume\n139, pp. 8,821–8,831, and at https://proceedings.mlr.press/v139/ramesh21a.html, accessed 4 January 2024.\nS. Reed, K. Żołna, E. Parisotto, S. Gómez Colmenarejo, A. Novikov, G. Barth-Maron, M. Giménez, Y.\nSulsky, J. Kay, J.T. Springenberg, T. Eccles, J. Bruce, A. Razavi, A. Edwards, N. Heess, Y. Chen, R.\nHadsell, O. Vinyals, M. Bordbar, and N. de Freitas, 2022. “A generalist agent,” Transactions on Machine\nLearning Research at https://openreview.net/pdf?id=1ikK0kHjvj\n, accessed 4 January 2024.\nS. Russell and P. Norvig, 2010. Artificial intelligence: A modern approach. Third edition. Harlow: Pearson\nEducation.\nL. Salewski, S. Alaniz, I. Rio-Torto, E. Schulz, and Z. Akata, 2023. “In-context impersonation reveals large\nlanguage models’ strengths and biases,” arXiv:2305.14930 (24 May).\ndoi: https://doi.org/10.48550/arXiv.2305.14930, accessed 11 December 2023.\nG. Sanderson, 2017. “But what is a neural network?” (5 October), at\nhttps://www.3blue1brown.com/lessons/neural-networks\n, accessed 19 December 2022.\nS. Sharma, 2020. ”A manifesto for the broken machine,” Camera Obscura, volume 35, number 2, pp. 171–\n179.\ndoi: https://doi.org/10.1215/02705346-8359652\n, accessed 4 January 2024.\nE. Sheng, K.-W. Chang, P. Natarajan, and N. Peng, 2019. “The woman worked as a babysitter: On biases in\nlanguage generation,” Proceedings of the 2019 Conference on Empirical Methods in Natural Language\nProcessing and the Ninth International Joint Conference on Natural Language Processing (EMNLP-\nIJCNLP), pp. 3,407–3,412.\ndoi: https://doi.org/10.18653/v1/D19-1339\n, accessed 4 January 2024.\nJ. Shihadeh, M. Ackerman, A. Troske, N. Lawson, and E. Gonzalez, 2022. “Brilliance bias in GPT-3,”\n2022 IEEE Global Humanitarian Technology Conference (GHTC)..\ndoi: https://doi.org/10.1109/GHTC55712.2022.9910995\n, accessed 10 January 2024.\nJ. Simon, 2021. “Large language models: A new Moore’s Law?” at https://huggingface.co/blog/large-\nlanguage-models, accessed 30 November 2021.\nS. Smith, M. Patwary, B. Norick, P. LeGresley, S. Rajbhandari, J. Casper, Z. Liu, S. Prabhumoye, G.\nZerveas, V. Korthikanti, E. Zhang, R. Child, R.Y. Aminabadi, J. Bernauer, X. Song, M. Shoeybi, Y. He, M.\nHouston, S. Tiwary, and B. Catanzaro, 2022. “Using DeepSpeed and Megatron to train Megatron-Turing\nNLG 530B, A large-scale generative language model,” arXiv:2201.11990 (28 January).\ndoi: https://doi.org/10.48550/arXiv.2201.11990\n, accessed 20 December 2022.\nI. Solaiman and C. Dennison, 2021. “Process for adapting language models to society (PALMS) with\nvalues-targeted datasets,” 35th Conference on Neural Information Processing Systems (NeurIPS 2021), at\nhttps://proceedings.neurips.cc/paper/2021/file/2e855f9489df0712b4bd8ea9e2848c5a-Paper.pdf\n, accessed\n10 January 2024.\nS.L. Star, 1999. “The ethnography of infrastructure,” American Behavioral Scientist, volume 43, number 3,\npp. 377–391.\ndoi: https://doi.org/10.1177/00027649921955326, accessed 4 January 2024.\nNotes towards infrastructure governance for large language models\nS.L. Star and K. Ruhleder, 1994. “Steps towards an ecology of infrastructure: Complex problems in design\nand access for large-scale collaborative systems,” CSCW ’94: Proceedings of the 1994 ACM Conference on\nComputer Supported Cooperative Work, pp. 253–264.\ndoi: https://doi.org/10.1145/192844.193021, accessed 10 January 2024.\nH. Suresh and J. Guttag, 2021. “A framework for understanding sources of harm throughout the machine\nlearning life cycle,” EAAMO ’21: Equity and Access in Algorithms, Mechanisms, and Optimization, article\nnumber 17, pp. 1–9.\ndoi: https://doi.org/10.1145/3465416.3483305\n, accessed 4 January 2024.\nM. Suzuki and Y. Matsuo, 2022. “A survey of multimodal deep generative models,” Advanced Robotics,\nvolume 36, numbers 5–6, pp. 261–278.\ndoi: https://doi.org/10.1080/01691864.2022.2035253\n, accessed 4 January 2024.\nA. Tamkin, M. Brundage, J. Clark, and D. Ganguli, 2021. “Understanding the capabilities, limitations, and\nsocietal impact of large language models,” arXiv:2102.02503 (4 February).\ndoi: https://doi.org/10.48550/arXiv.2102.02503\n, accessed 8 June 2022.\nS.L. Thomas, D. Nafus, and J. Sherman, 2018. “Algorithms as fetish: Faith and possibility in algorithmic\nwork,” Big Data & Society (9 January).\ndoi: https://doi.org/10.1177/2053951717751552\n, accessed 4 January 2024.\nR. Thoppilan, D. De Freitas, J. Hall, N. Shazeer, A. Kulshreshtha, H.-T. Cheng, A. Jin, T. Bos, L. Baker, Y.\nDu, Y. Li, H. Lee, H.S. Zheng, A. Ghafouri, M. Menegali, Y. Huang, M. Krikun, D. Lepikhin, J. Qin, D.\nChen, Y. Xu, Z. Chen, A. Roberts, M. Bosma, V. Zhao, Y. Zhou, C.-C. Chang, I. Krivokon, W. Rusch, M.\nPickett, P. Srinivasan, L. Man, K. Meier-Hellstern, M.R. Morris, T. Doshi, R.D. Santos, T. Duke, J.\nSoraker, B. Zevenbergen, V. Prabhakaran, M. Diaz, B. Hutchinson, K. Olson, A. Molina, E. Hoffman-John,\nJ. Lee, L. Aroyo, R. Rajakumar, A. Butryna, M. Lamm, V. Kuzmina, J. Fenton, A. Cohen, R. Bernstein, R.\nKurzweil, B. Aguera-Arcas, C. Cui, M. Croak, E. Chi, and Q. Le, 2022. “LaMDA: Language models for\ndialog applications,” arXiv:2201.08239 (20 January).\ndoi: https://doi.org/10.48550/arXiv.2201.08239\n, accessed 20 December 2022.\nH. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux, T. Lacroix, B. Rozière, N. Goyal, E.\nHambro, F. Azhar, A. Rodriguez, A. Joulin, E. Grave, and G. Lample, 2023. “LLaMA: Open and efficient\nfoundation language models,” Hugging Face (27 February), at https://huggingface.co/papers/2302.13971\n,\naccessed 4 January 2024.\nL. Weidinger, J. Uesato, M. Rauh, C. Griffin, P.-S. Huang, J. Mellor, A. Glaese, M. Cheng, B. Balle, A.\nKasirzadeh, C. Biles, S. Brown, Z. Kenton, W. Hawkins, T. Stepleton, A. Birhane, L.A. Hendricks, L.\nRimell, W.S. Isaac, J. Haas, S. Legassick, G. Irving, and I. Gabriel, 2022. “Taxonomy of risks posed by\nlanguage models,” FAccT ’22: Proceedings of the 2022 ACM Conference on Fairness, Accountability, and\nTransparency, pp. 214–229.\ndoi: https://doi.org/10.1145/3531146.3533088\n, accessed 4 January 2024.\nM. Weiser, 1991. “The computer for the 21st century,” Scientific American, volume 265, number 3, pp. 94–\n104.\nK. Yeung, 2017. “‘Hypernudge’: Big data as a mode of regulation by design,” Information, Communication\n& Society, volume 27, number 1, pp. 118–136.\ndoi: https://doi.org/10.1080/1369118X.2016.1186713, accessed 4 January 2024.\nM. Zaib, Q. Sheng, and W.E. Zhang, 2020. “A short survey of pre-trained language models for\nconversational AI — A new age in NLP,” ACSW ’20: Proceedings of the Australasian Computer Science\nWeek Multiconference, article number 11, pp. 1–4.\ndoi: https://doi.org/10.1145/3373017.3373028, accessed 4 January 2024.\nNotes towards infrastructure governance for large language models\nW.E. Zhang, Q. Sheng, A. Alhazmi, and C. Li, 2020. “Adversarial attacks on deep-learning models in\nnatural language processing: A survey,” ACM Transactions on Intelligent Systems and Technology, volume\n11, number 3, article number 24, pp. 1–41.\ndoi: https://doi.org/10.1145/3374217\n, accessed 4 January 2024.\nEditorial history\nReceived 30 December 2023; accepted 4 January 2024.\nThis paper is licensed under a Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International\nLicense.\nNotes towards infrastructure governance for large language models\nby Lara Dal Molin.\nFirst Monday, volume 29, number 2 (February 2024).\ndoi: https://dx.doi.org/10.5210/fm.v29i2.13567"
}