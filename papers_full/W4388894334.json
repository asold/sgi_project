{
    "title": "IDP-LM: Prediction of protein intrinsic disorder and disorder functions based on language models",
    "url": "https://openalex.org/W4388894334",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2963173732",
            "name": "Yihe Pang",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A1974433614",
            "name": "Bin Liu",
            "affiliations": [
                "Beijing Institute of Technology"
            ]
        },
        {
            "id": "https://openalex.org/A2963173732",
            "name": "Yihe Pang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A1974433614",
            "name": "Bin Liu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2624409632",
        "https://openalex.org/W1989511016",
        "https://openalex.org/W1994840640",
        "https://openalex.org/W2015115860",
        "https://openalex.org/W3092787915",
        "https://openalex.org/W2977849281",
        "https://openalex.org/W2120881407",
        "https://openalex.org/W2554784405",
        "https://openalex.org/W2074180389",
        "https://openalex.org/W2009455776",
        "https://openalex.org/W2147733973",
        "https://openalex.org/W3177500196",
        "https://openalex.org/W2093205346",
        "https://openalex.org/W1497456715",
        "https://openalex.org/W4220991280",
        "https://openalex.org/W6629841504",
        "https://openalex.org/W4200472293",
        "https://openalex.org/W3110645309",
        "https://openalex.org/W3196994021",
        "https://openalex.org/W2420922808",
        "https://openalex.org/W2972006361",
        "https://openalex.org/W2130479394",
        "https://openalex.org/W3104537585",
        "https://openalex.org/W3166142427",
        "https://openalex.org/W3156051371",
        "https://openalex.org/W3184265853",
        "https://openalex.org/W3214870576",
        "https://openalex.org/W2158714788",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W1978559946",
        "https://openalex.org/W2149472608",
        "https://openalex.org/W2008332643",
        "https://openalex.org/W3146944767",
        "https://openalex.org/W3011698511",
        "https://openalex.org/W2566370974",
        "https://openalex.org/W3043901899",
        "https://openalex.org/W2047094503",
        "https://openalex.org/W3094704314",
        "https://openalex.org/W3084798277",
        "https://openalex.org/W4307698996",
        "https://openalex.org/W4306811769",
        "https://openalex.org/W2912550851",
        "https://openalex.org/W3211795435",
        "https://openalex.org/W3177828909",
        "https://openalex.org/W2767649956",
        "https://openalex.org/W3117811820",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2187089797"
    ],
    "abstract": "Intrinsically disordered proteins (IDPs) and regions (IDRs) are a class of functionally important proteins and regions that lack stable three-dimensional structures under the native physiologic conditions. They participate in critical biological processes and thus are associated with the pathogenesis of many severe human diseases. Identifying the IDPs/IDRs and their functions will be helpful for a comprehensive understanding of protein structures and functions, and inform studies of rational drug design. Over the past decades, the exponential growth in the number of proteins with sequence information has deepened the gap between uncharacterized and annotated disordered sequences. Protein language models have recently demonstrated their powerful abilities to capture complex structural and functional information from the enormous quantity of unlabelled protein sequences, providing opportunities to apply protein language models to uncover the intrinsic disorders and their biological properties from the amino acid sequences. In this study, we proposed a computational predictor called IDP-LM for predicting intrinsic disorder and disorder functions by leveraging the pre-trained protein language models. IDP-LM takes the embeddings extracted from three pre-trained protein language models as the exclusive inputs, including ProtBERT, ProtT5 and a disorder specific language model (IDP-BERT). The ablation analysis shown that the IDP-BERT provided fine-grained feature representations of disorder, and the combination of three language models is the key to the performance improvement of IDP-LM. The evaluation results on independent test datasets demonstrated that the IDP-LM provided high-quality prediction results for intrinsic disorder and four common disordered functions.",
    "full_text": "RESEA RCH ARTICL E\nIDP-LM: Prediction of protein intrinsic\ndisorder and disorder functions based on\nlanguage models\nYihe Pang\n1\n, Bin Liu\nID\n1,2\n*\n1 School of Computer Science and Technology , Beijing Institute of Technolo gy, Beijing, China, 2 Advanced\nResearc h Institute of Multidiscipli nary Science, Beijing Institute of Technolo gy, Beijing, China\n* bliu@bli ulab.net\nAbstract\nIntrinsically disordered proteins (IDPs) and regions (IDRs) are a class of functionally impor-\ntant proteins and regions that lack stable three-dimensiona l structures under the native\nphysiologic conditions. They participate in critical biological processes and thus are associ-\nated with the pathogene sis of many severe human diseases. Identifying the IDPs/IDRs and\ntheir functions will be helpful for a comprehensive understanding of protein structures and\nfunctions, and inform studies of rational drug design. Over the past decades, the exponential\ngrowth in the number of proteins with sequence information has deepened the gap between\nuncharacterized and annotated disordered sequences. Protein language models have\nrecently demonstrated their powerful abilities to capture complex structural and functional\ninformation from the enormous quantity of unlabelled protein sequences, providing opportu-\nnities to apply protein language models to uncover the intrinsic disorders and their biological\nproperties from the amino acid sequences. In this study, we proposed a computational pre-\ndictor called IDP-LM for predicting intrinsic disorder and disorder functions by leveraging the\npre-trained protein language models. IDP-LM takes the embeddings extracted from three\npre-trained protein language models as the exclusive inputs, including ProtBERT, ProtT5\nand a disorder specific language model (IDP-BERT). The ablation analysis shown that the\nIDP-BERT provided fine-grained feature representations of disorder, and the combination of\nthree language models is the key to the performance improvement of IDP-LM. The evalua-\ntion results on independent test datasets demonstrated that the IDP-LM provided high-qual-\nity prediction results for intrinsic disorder and four common disordered functions.\nAuthor summary\nThe intrinsically disordered proteins (IDPs) and regions (IDRs) are functionally impor-\ntant proteins and regions without stable three-dimensional structures under the native\nphysiologic conditions. They are widespread in proteome and performed many critical\nbiological functions in organisms. The structural and functional abnormalities of IDRs/\nIDPs typically cause many severe diseases in humans. Therefore, computational\nPLOS COMP UTATIONAL  BIOLOGY\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 1 / 18\na1111111111\na1111111111\na1111111111\na1111111111\na1111111111\nOPEN ACCESS\nCitation: Pang Y, Liu B (2023) IDP-LM: Prediction\nof protein intrinsic disorder and disorder functions\nbased on language models. PLoS Comput Biol\n19(11): e1011657. https://doi.org/10 .1371/journal.\npcbi.1011657\nEditor: Nir Ben-Tal, Tel Aviv University, ISRAEL\nReceived: December 31, 2022\nAccepted: November 3, 2023\nPublished: November 22, 2023\nCopyright: © 2023 Pang, Liu. This is an open\naccess article distributed under the terms of the\nCreative Commons Attribution License, which\npermits unrestricte d use, distribu tion, and\nreproduction in any medium, provided the original\nauthor and source are credited.\nData Availabilit y Statement: Source codes for\nIDP-LM at https://github. com/YihePa ng/IDP-LM\nand The stand-alone package of IDP-LM is\navailable at http://bliulab .net/IDP_LM /.\nFunding: This work was supported by the Nationa l\nNatural Science Foundation of China (No.\n62325202 , 62271049, 62250028 and U22A2039 to\nBL). The funders had no role in study design, data\ncollection and analysis , decision to publish, or\npreparation of the manuscript.\nCompeting interests : The authors have declared\nthat no competing interests exist.\nidentification of the intrinsic disorder and its functions in protein is important for a com-\nprehensive understanding of the protein structure-function mechanism, thereby facilitat-\ning the research on disease and drug discovery. Recently, the pre-trained protein language\nmodel has been shown to be effective in discovering the structure and function informa-\ntion from the massive amino acid sequences. This allows us to uncover the biological\nproperties of intrinsic disorder from the sequences through the language models (LMs).\nIn this study, we proposed a disorder and its functions predictor referred to as IDP-LM by\napplying the pre-trained protein LMs. The IDP-LM takes the embeddings extracted from\nthree pre-trained protein language models as the exclusive inputs, including ProtBERT\nand ProtT5 and a disordered specific language model (IDP-BERT). The IDP-BERT pro-\nvides fine-grained feature representations for disorder at both the residue and sequence\nlevel, and its combination with two ProtTrans LMs is the key to the performance improve-\nment of IDP-LM. We evaluated the performance of IDP-LM for disorder and disorder\nfunction prediction on Critical Assessment of protein Intrinsic Disorder (CAID) dataset\nand TE176 independent test dataset, and the results demonstrated that IDP-LM provided\nhigh-quality prediction results for both intrinsic disorder and four common disordered\nfunctions, and significantly outperformed other comparative predictors.\nIntroduction\nThe protein segments that lack stable three-dimensional structures under the native physio-\nlogic conditions are referred to as intrinsically disordered regions (IDRs). Intrinsically disor-\ndered proteins (IDPs) with IDRs are widespread in nature, for instance, a larger fraction of\nproteins in eukaryotic organisms are disordered [1,2]. Although IDP/IDRs lack of well-defined\nstructural conformation, they performed many critical biological functions, such as being\npost-translational modification sites [3], regulation of signaling pathways [4], and mediating\nthe phase separation process [5,6]. The functional importance of IDP/IDRs makes them asso-\nciated with the pathogenesis of many severe diseases in humans [7–9]. Exploring the intrinsic\ndisorder and its functions in protein leads to a deeper understanding of the protein structure-\nfunction mechanism, thereby facilitating the research on disease and drug discovery [10,11].\nThe classical protein structure-function paradigm indicated that all information about pro-\ntein structure and function is encoded in their primary amino acid sequences. Over the past\ndecades, the exponential growth in the number of proteins with known amino acid sequences\ndeepens the gap between unannotated and experimentally characterized disordered sequences\n[3]. Seeking the vast wealth from the enormous quantity of unlabeled sequences is critical for\nbridging these gaps [12]. Recently, language models (LMs) have become increasingly impactful\nin natural language processing (NLP) research. The LMs are able to capture the complex syn-\ntax and semantics from the large-scaled unlabeled text corpus, and have been shown to reach\nstate-of-the-art (SOA) performance across a range of NLP tasks in practice. The protein\nsequences can be seen as the language of genetics sharing strong similarities with natural lan-\nguage [13]. The amino acid sequences adopt structures to determine specific functions, which\nmap with the words that follow the syntax to express meanings. Their analogies have stimu-\nlated the applying LMs to discover the structure and function information present in the\namino acid sequences. For example, the ProtTrans [12] provided novel protein language mod-\nels based on a series of Transformer architectures, and has appeared competitive in structure\nand function related prediction tasks, including secondary structure prediction, protein locali-\nzation and membrane protein classification. The IDP/IDRs perform critical functions in\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 2 / 18\norganisms despite lacking well-defined structures, which has redefined the protein structure-\nfunction paradigm. The potential of LMs allows us to uncover the biological properties of\nintrinsic disorder from the amino acid sequences.\nHere, we proposed IDP-LM, a predictor by applying the pre-trained protein LMs for pre-\ndicting intrinsic disorder and disorder functions of proteins (See Fig 1). Two pre-trained lan-\nguage models (ProtBERT and ProtT5) from ProtTrans that have shown especially success for\nprotein structure and function predictions were included in the IDP-LM [12,14]. Besides, con-\nsidering that the properties of disorder are particular and different from other structured\nregions, for example, the disorder tends to occur at the N’ and C’ terminal of sequences with a\nspecific amino acid composition tendency [15,16], we pre-trained a disorder specific protein\nlanguage model referred to as IDP-BERT, which is to capture fine-grained features of this spe-\ncial class of proteins/regions. The IDP-BERT employed the architecture of Bidirectional\nEncoder Representation (BERT) based on Transformer [17], and it was self-supervised pre-\ntrained as masked language modelling to learn the linguistic patterns of disordered regions.\nThe IDP-BERT provided more disorder related information at both the residue and sequence\nlevels, and its combination with two ProtTrans LMs is able to improve the performances of\nIDP-LM for predicting disorder and disorder functions. We evaluated the performances of\nIDP-LM for disorder prediction on the Critical Assessment of protein Intrinsic Disorder\n(CAID) test dataset, and the results demonstrated that IDP-LM achieved competitive perfor-\nmance among all comparable methods in the CAID for predicting protein disordered regions,\nfully disordered proteins, and disordered binding subregions. Besides, we linked disorder to\nits functions by transferring the trained IDP-LM disorder predictor to four common disor-\ndered functions prediction including disorder protein binding, DNA binding, RNA binding,\nand disorder flexible linkers. Benefiting from transfer learning, the IDP-LM predictors for dis-\norder function prediction significantly outperformed other comparative predictors in all four\ncommon disordered functions. The stand-alone package of IDP-LM is available at http://\nbliulab.net/IDP_LM/.\nFig 1. Overview of IDP-LM predictor . The input sequences were processed by three language models to genera te the embeddin g\nvector for each residue . The IDP-BERT disorder ed language model adopts the BERT architectur e by stacking multiple Transfor mer\nencoders, and it was self-super vised pre-trained with the sequences collected from the MobiDB and PDB database. Three prediction\nlayers in IDP-LM were used to calculate per-residue propensit y scores based on embeddings extracted from three language models,\nrespectively. Then the model outputs the final propensi ty scores and binary results by fusing the calculati ons from three prediction\nlayers.\nhttps://do i.org/10.1371/j ournal.pc bi.1011657. g001\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 3 / 18\nMaterials and Methods\nDatasets\nDataset for disordered protein language model. The protein language models (LMs) are\nbuilt on the ideal that the sequence-structure-fun ction information in proteins can be captured\nby effectively leveraging the massive unlabelled protein sequences. Although the functionally\nannotated disordered protein sequences are limited, the number of available protein sequences\nwith detected disordered regions is sufficient. This allows us to uncover the biological proper-\nties of intrinsic disorder from the sequences through the language model. We pre-trained the\ndisordered protein language model IDP-BERT on the sequences from the MobiDB database\n[18], which is a large integrated resource of disordered proteins. For obtaining high quality of\nprotein sequences containing disordered regions, we collected all the curated and derived\nannotated sequences in the MobiDB but excluded their annotation information, leading to\n68,700 disordered protein sequence data. Because the difference between structured and disor-\ndered is one of the most critical features for many computational prediction tasks related to\nintrinsic disorder [16,19–22], the IDP-BERT was pre-trained with the fully structured protein\nsequences to capture more differences between protein disordered and ordered. We searched\nall high-resolution (<2Å) protein monomers from the PDB data bank [23,24] and obtained\n36,148 fully structured sequences, where all amino acid structures of these sequences have\nbeen resolved and with minimal likelihood to be disordered [19]. The combination of 68,700\ndisordered sequences and 36,148 structured sequences results in a total of 104,848 protein\nsequences for the self-supervised pre-training of disordered protein language model IDP-\nBERT, which are available at http://bliulab.net/IDP_LM /download/.\nBenchmark datasets for intrinsic disorder and disorder function prediction. The infor-\nmation learned by pre-trained protein language models is referred as proteins’ language model\nembeddings [12,25], which are used as inputs for the supervised training of IDP-LM. In this\nstudy, we used two independent test datasets (CAID and TE176) to evaluated the perfor-\nmances of IDP-LM on intrinsic disorder and different disordered functions. Referring to the\nCritical Assessment of protein Intrinsic Disorder (CAID) prediction [26], the CAID dataset\nwas used to evaluated the predictive performances on protein disordered regions, fully disor-\ndered proteins and disordered binding regions. In addition, following the study [27], the\nTE176 independent test set of 176 functional disordered sequences was used to perform the\nevaluation on four specific disorder functions, including disorder protein binding, disorder\nDNA/RNA bindings, and disorder flexible linkers. Besides, the training and validation dataset\nused for the supervised training of the proposed IDP-LMs predictor were collected from the\nDisProt database [28] by Hu et al [27]. To avoid the overestimation of predictive performance\nand the potential overfitting in the supervised training caused by the sequence similarity, we\nused the PSI-BLAST [29] searching algorithm to remove the sequences in the training and vali-\ndation dataset that share sequence similarity higher than 25% to those in two independent test\nsets. Consequently, the resulting training set of 412 sequences and validation set of 90 sequences\nwere used for the model parameters optimization and hyper-parameters selection, respectively.\nAdditional descriptions of the disorder function datasets are provided in S1 Table. All the data-\nsets used in this study are available at http://bliulab.net/IDP_LM/downloa d/.\nRestrictive masked language model pre-training of IDP-BERT\nThe recent studies have reported that the protein language model employed the BERT architec-\nture achieves better performance on protein function predictions than models using other archi-\ntectures using the same number of pre-training sequences [12,14]. Inspired by these results, we\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 4 / 18\nemployed the BERT framework to train the disordered protein language model, IDP-BERT. The\nBERT architecture refers to the Bidirectional Encoder Representation from Transformers, which\nis naturally suitable for masked language modelling training [30]. Unlike the BERT trained with\nnatural language corpus and massive proteome [12], the IDP-BERT was trained, we named as a\nRestrictive Masked Language Model (ReMLM) for disordered regions. More specifically, the\nBERT model is trained to reconstruct the masked residues that are mostly located in the head and\ntail subsegments of sequences given the surroundings. The fundamental idea behind this novel\ntraining scheme is the observation that intrinsic disorder tends to occur at the N’ and C’ terminals\nof sequences and with a specific amino acid composition tendency [15,31–33].\nThe BERT architecture employed in IDP-BERT is almost identical to the original in NLP [17].\nIn IDP-BERT (see Fig 1), the residues were processed as the basic input units. Given a residue R\ni\n,\nthe combination of positional encoding PE\ni\nand amino acid embedding AE\ni\nis used as the initial\nrepresentation X\ni\n= [PE\ni\n; AE\ni\n]. Then the inputs go through the N layers of Tranformer encoder\nblocks [30], and the hidden vector from the last layer was extracted as the language model embed-\nding Y\ni\nfor each residue. All the model parameters were jointly optimized by minimizing the neg-\native log likelihood of predicted masked amino acid A\ni\ngiven the contextual A\n�\nM\n[25,34]:\nL ¼ \u0000\n1\nN\nX\nN\ni¼1\n\u0000 l o g P ðA\ni\njA\n�\nM\nÞ ð1Þ\nwhere N denotes the maximum number of model masked residues in the input sequence. The\nmodel was implemented by PyTorch framework and trained on a single NVIDIA GeForce RTX\n3080 GPU with a memory of 10GB. And the hyper-parameters of IDP-BERT are given in S2 Table.\nIDP-LM for disorder prediction\nAs the overview of IDP-LM shown in Fig 1, the amino acid sequence is first transformed into\nprotein embedding vectors by three pre-trained protein language models. Two language mod-\nels (ProtBERT and ProtT5) from ProtTrans [12] were used in IDP-LM, where the ProtBERT\nemploys the BERT model was trained on UniRef100 of 216 million proteins, and the ProtT5\nemploys the Transformer encoder-decoder model trained with UniRef50 of 45 million pro-\nteins [12]. ProtTrans language model trained with massive scale dataset captured the compre-\nhensive properties from the proteome. The IDP-BERT captures a fine-grained characteristic of\ndisorder. Then three prediction layers in IDP-LM produce per-residue disorder predictions\nusing the embeddings extracted from three language models. The prediction layers employed\nthe bidirectional LSTM (Bi-LSTM) networks that encode the global information from the for-\nward and backward direction of the input sequence. The preferred Bi-LSTM is inspired by pre-\nvious related researches [35–37]. Given a residue R\ni\n, the disorder propensity scores calculated\nby three Bi-LSTM layers are represented as p\n1\ni\n; p\n2\ni\nand p\n3\ni\n, respectively (see Fig 1). Next, these\npropensity scores were weighted and fused into the final predictive results P of IDP-LM, and\nthe optimal weights were selected by the genetic algorithm (GA) [38,39] according to the high-\nest AUC score on the validation dataset. All the parameters of prediction layers in IDP-LM\nwere jointly optimized by minimizing the binary cross-entropy loss [40] on the disordered val-\nidation dataset. The model hyper-parameters of IDP-LM are given in S3 Table.\nTransferring IDP-LM for disorder function prediction\nFunctional properties of proteins are often maintained in the natural protein amino acids\nsequences [25]. The language models pre-trained with massive protein sequence database dis-\ncovering functional features from sequences, hence, these captured features can be used for\nthe prediction of disordered functions. A key challenge of disordered function prediction is\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 5 / 18\nthat the available number of disordered sequences with functional annotations is relatively\nsmall for training a computational predictor [41]. According to previous studies [22,41,42],\nthe predictors trained with disordered sequence can be used to improve the performance of\ndisorder function prediction via transfer learning. Therefore, in this study, we linked disorder\nto its function by transferring the trained IDP-LM disorder predictor to the prediction of dis-\norder function. Specifically, the prediction layers in IDP-LM disorder predictor were sepa-\nrately fine-tuned with four disordered function annotated sequences, leading to four\ncorresponding functional prediction layers for disorder protein binding, disorder DNA bind-\ning, disorder RNA binding, and disorder flexible linker (see Fig 2). The parameters of four\nfunctional prediction layers of IDP-LM were independently optimized using the same loss\nfunction and optimizer but different learning rates as in the disorder prediction. For the\nhyper-parameters of IDP-LM for disorder function prediction please refer to S4 Table.\nEvaluation criteria\nThere are two forms of results produced by the IDP-LM predictor: the real-valued propensity\nscores and binary classification results for disorder and disordered functions. We evaluated\nthe real-valued prediction results with the area under the receiver operating characteristic\n(ROC) curve (AUC) and the maximum harmonic mean between precision and recall rate\nacross all thresholds (F\nmax\n), which fully consisted with the CAID evaluation [26]. The binary\nclassification results were transformed from the propensity scores. The Matthews correlation\ncoefficient (MCC) and balanced accuracy (BAC) were used to measure the binary classification\nresults [27]:\nMCC ¼\nTP � TN \u0000 FP � FN\nffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi\nðTP þ FPÞ � ðTP þ FN Þ � ðTN þ FPÞ � ðTN þ FN Þ\np ð2Þ\nFig 2. IDP-LM for disorder functi on predic tion. The IDP-LM disorder predictor was transferred for four commo n\ndisorder function predictions. Disorder prediction layers in IDP-LM were fine-tuned with disorder functions to\ngenerate Scores\nPB\n, Scores\nDB\n, Scores\nRB\nand Scores\nLinker\npropensi ty scores for predicting disorder protein bindin g\nfunction, disorder DNA bindin g function, disorder RNA binding function, and disorder flexible linker function,\nrespective ly.\nhttps://d oi.org/10.1371/j ournal.pc bi.1011657. g002\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 6 / 18\nBAC ¼\n1\n2\nTN\nTN þ FP\nþ\nTP\nTP þ FN\n� �\nð3Þ\nwhere TP represents the true positives, TN represents the true negatives, FP represents the\nfalse positives, FN represents the false negatives.\nResults and discussion\nProtein language models encode the disordered properties\nTo investigate how the pre-trained protein language models learn the properties of protein dis-\norder, we visualized the embedding vectors captured by three language models in 2D space by\nusing the t-SNE projection [43]. The output hidden vectors from the last layer of pre-trained\nlanguage model are extracted as the residue-level embeddings, and the average pooling of all\nresidue embeddings is used as the sequence-level embeddings. We randomly selected 500 fully\nordered sequences and 500 disordered sequences from the pre-trained dataset. The embedding\nprojection results derived by the three language models for a total of 1000 sequences were\nshown in Fig 3A. Subsequently, we randomly selected 500 disordered residues and 500\nordered residues in the above sampled disordered sequences, and the embedding projection of\n1000 residues extracted from the three language models were shown in Fig 3B. Compared to\nthe other two language models, embeddings derived from IDP-BERT are more clustered\nwithin ordered and disordered sequences/residues, and more discriminative between ordered\nand disordered sequences/residues. Furthermore, we utilized the point-biserial correlation\n(PBC) scores [20,21,44] to quantify the correlations between different feature representations\nand these sampled disordered residues and sequences:\nPBC ¼\nm\n1\n\u0000 m\n0\ns\nn\nffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi\nn\n0\n� n\n1\nn � n\nr\nð4Þ\nwhere m\n0\nand m\n1\nindicate the mean values of embedding vectors for ordered and disordered\nproteins/residues, respectively. s\nn\nis the standard deviation of all embedding vectors. n indicate\nthe total number of proteins/residues, n\n0\nand n\n1\nindicate the number of ordered and disor-\ndered proteins/residues, respectively. Fig 3C and 3D shown the PBC results when using tem-\nplate-free representation, MSA-based features, and embeddings generated from pre-trained\nlanguage models. From these figures, we observed that the feature representations extracted\nfrom IDP-BERT exhibit the highest correlations with disorder. These results are not surprising\nbecause there are significant differences in the distribution of amino acid biochemical proper-\nties between disordered region and ordered region in proteins (Fig 3E and 3F), and the lan-\nguage model trained for disordered proteins captured these biochemical properties of\ndisordered residues (Fig 3G).\nLanguage model combination and transfer learning for disorder and\ndisorder function prediction\nTo investigate how the IDP-BERT contributed to the predictions of disorder and disorder\nfunctions, we evaluated the performance of models using three language model embeddings\nand their different combinations on the validation dataset. Fig 4A shows the comparison of\ntrue positive rates (TPR) at the optimal threshold points on ROC curves of different models,\nfrom which we observed that IDP-BERT achieves more TPR compared to the other two lan-\nguage models, ProtBERT and ProtT5. These results contribute to the IDP- BERT being trained\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 7 / 18\nas a Restrictive Masked Language Model (ReMLM) to focus on the disordered regions mainly\nlocated in the head and tail segments of sequence (Fig 4B), thereby leading to a significant\nincreasement of true positive predictions in the disordered regions (Fig 4D). We further calcu-\nlated the statistical differences in propensity scores predicted by the three language models,\nand the results shown in S7 and S11 Tables demonstrated the significant differences among\nthe three language models in disorder and disorder function predictions. Therefore, IDP-LM\nintegrated three language models to leverage their complementary predictions, resulting in the\nhighest predictive performance (Fig 4A).\nThe disordered function predictor IDP-LM is transferred from the model trained for disor-\ndered region prediction. To demonstrate the contribution of model transferring, we compared\nthe performance between IDP-LM directly trained with disordered functions (IDP-LM-DT)\nand the fine-tuned model transferred from the pre-trained disordered region prediction, and\nthe results were shown in Fig 4C. From this figure, we can see that the transfer learning signifi-\ncantly reduced the false positive prediction rates for all four disordered functions. The pre-\ndicted results of disordered protein-binding functions for protein (DisProt ID: DP00803)\nFig 3. Protein disordered propert ies captured by language models . t-SNE projectio n visualization of disordered/\nstructur ed proteins’ (a) and residues’ (b) embeddin g vectors extracte d by three pre-trained protein language models,\nwhere the language model trained for disorder ed proteins (IDP-BER T) learned more fine-grained distinctions of\ndisorder and order. The compa ration of point-bise rial correlation (PBC) scores calculate d based on differe nt feature\nrepresentat ions of disordered proteins (c) and residues (d). We included template-fre e features (One-hot and\nblosum62 ), multipl e sequence alignment based feature (PSSM), and pre-traine d language model encodings\n(ProtBER T, ProtT5 , IDP-BERT-G and IDP-BERT) , where the IDP-BERT-G represents the features extracte d from the\nIDP-BERT pretrained with the general mask language modelling. Higher PBC value reflects the information provided\nby features more relevan t with disorder . According to our statistics in the DisProt database, disorder ed regions (e) are\nrich in polar residues compared with the ordered regions (f). The t-SNE projections of amino acids encoding vectors\ncaptured by IDP-BE RT in 2D space conform with their biochemi cal properties (g).\nhttps://d oi.org/10.1371/j ournal.pc bi.1011657. g003\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 8 / 18\nfrom the TE176 dataset are visualized in Fig 4E, which indicated that the IDP-LM model\ntransferred from disordered predictor provides fewer false positive predictions, leading to\nmore accurate results than the IDP-LM-DT directly trained with disordered functional\nsequences.\nIntrinsically disordered regions/proteins prediction\nFollowing the Critical Assessment of protein Intrinsic Disorder prediction (CAID) experi-\nment, we comprehensively evaluated the performance of IDP-LM for predicting the disorder\nin proteins, and compared it with other computational predictors. In CAID [26], the disorder\nprediction is divided into two categories: predicting intrinsically disordered regions (IDRs) in\nproteins and predicting fully intrinsically disordered proteins (IDPs). Two datasets are used\nfor IDR prediction, DisProt and DisProt-PDB, where the IDR annotations in the former were\ncollected from the DisProt database with experimental evidence, while the latter is based on\nthe former and limited the negatives to residues observed in the PDB database. To ease compa-\nrability, we used the same evaluation metrics as in CAID to report the predictive performance\nof different predictors, and the IDR predictive results on DisProt and DisProt-PDB datasets\nwere listed in Tables 1 and 2, respectively. From these results, we can see that the IDP-LM\nusing the language model embeddings as exclusive input outperformed other predictors on\nFig 4. Language models combin ation and transfer learning improve disorder and disorder function predictio n. (a), The true positive rates\n(TPR) of IDP-LMs for disorder and disorder function prediction on the validatio n dataset using differe nt combinatio ns of pre-trained protein\nlanguage models. (b), The position distributi on of the residues learned by the ProtTrans language model (upper) and the actual disorder ed residue s\nin the DisPort database (lower). (c), The false positive rates (FPR) comparison of IDP-LMs with and without (DT) model pre-traini ng for disorder\nfunction predicti on. (d) and (e) show the prediction results of IDP-LM for two proteins in the TE176 dataset: DisPro t ID: DP00719 and DisProt ID:\nDP008 03, where the structures of two proteins were obtained by AlphaFol d [45,46], and each residue in the sequences was colored based on the\nmodel confiden ce score, pLDDT.\nhttps:// doi.org/10.1371 /journal.pcbi.10 11657.g004\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 9 / 18\nthe DisProt dataset, and achieved comparable performance with the state-of-the-art method\nSPOT-Disorder2 on the DisProt-PDB dataset in term of BAC values. The receiver operating\ncharacteristic (ROC) and Precision-Recall curves shown in Figs 5 and 6 demonstrate the cor-\nresponding predictive performances. As in the CAID, proteins with at least 95% of disordered\nresidues are considered IDPs. The performance comparisons of different predictors for identi-\nfying IDPs in the DisProt dataset were listed in Table 3, from which we see that the IDP-LM\npredictor significantly outperformed. These results of IDP-LM are attributed to the fact that\nthe pre-trained language model learned structure information from an enormous quantity of\nsequences, and by combining the disordered language model captured fine-grained differences\nbetween structural order and disorder, resulting in the accurate prediction of protein disorder.\nWe obtained the confidence scores (pLDDT) produced by AlphaFold for all sequences in\nthe CAID test dataset from the AlphaFold DB [45,46] and calculated the pLDDT distributions\nof the true and predicted disordered regions in the dataset (see S1 Fig). From this figure, we\ncan observer that the majority of predicted disordered regions exhibit low or very low pLDDT\nscores, which is consistent with the true disordered regions. And the Pearson correlation coef-\nficient between the disorder propensity scores predicted by IDP-LM and the pLDDT of Alpha-\nFold2 was r = -0.307 (see S5 Table).\nTable 1. Evaluation results of IDP-LM and the ten top-ranki ng predictor s in CAID for disordered region prediction on the CAID DisProt dataset.\nMethods AUC F\nmax\nMCC BAC\nIDP-LM 0.833 0.516 0.415 0.762\nfIDPnn* 0.814 0.483 0.370 0.720\nfIDPlr* 0.793 0.452 0.330 0.693\nRawMSA* 0.780 0.445 0.328 0.714\nESpritz-D* 0.774 0.428 0.307 0.703\nDisoMine* 0.765 0.424 0.299 0.698\nSPOT-Di sorder2* 0.760 0.469 0.349 0.725\nAUCpreD* 0.757 0.433 0.318 0.712\nSPOT-Di sorder-Single* 0.757 0.432 0.315 0.710\nAUCpreD- np* 0.751 0.424 0.301 0.699\nPredisord er* 0.747 0.435 0.301 0.691\n* The results of correspo nding predictors were obtained from [26] evaluated on the same CAID DisProt dataset. Predic tors are sorted by their AUC values.\nhttps://do i.org/10.1371/j ournal.pc bi.1011657. t001\nTable 2. Evaluation results of IDP-LM and the ten top-ranki ng predictor s in CAID for disordered region prediction on the CAID DisProt-PDB dataset.\nMethods AUC F\nmax\nMCC BAC\nSPOT-Di sorder2* 0.920 0.792 0.706 0.836\nSPOT-Di sorder1* 0.918 0.790 0.696 0.846\nIDP-LM 0.910 0.766 0.662 0.836\nAUCpreD* 0.906 0.767 0.662 0.816\nSPOT-Di sorder-Single* 0.896 0.753 0.646 0.817\nRawMSA* 0.894 0.749 0.635 0.815\nAUCpreD- np* 0.883 0.731 0.615 0.797\nPredisord er* 0.878 0.729 0.619 0.788\nDISOPRED -3.1* 0.875 0.730 0.613 0.796\nfIDPnn* 0.873 0.710 0.576 0.782\nIsUnstruct * 0.868 0.710 0.585 0.779\n* The results of correspo nding predictors were obtained from [26] evaluated on the same CAID DisProt-P DB dataset. Predic tors are sorted by their AUC values.\nhttps://do i.org/10.1371/j ournal.pc bi.1011657. t002\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 10 / 18\nAnother major challenge of CAID is to predict the binding sites in protein disordered\nregions. The disordered binding sites are short interacting subregions in proteins, which are\nannotated as the features of disorder [47]. We evaluated the performance of IDP-LM predictor\non the DisProt-binding dataset from CAID. The comparison results of IDP-LM and other\nmethods are shown in Fig 7 and Table 4, from which we can see that the IDP-LM predictor\nachieved significantly outstanding results than other predictors in all evaluation metrics, dem-\nonstrating the application of pre-trained protein language models is useful for the special tar-\nget regions prediction found within IDRs.\nDisordered function prediction\nWe compared the proposed IDP-LM predictor with the recent state-of-the-art methods for\npredicting four common disorder functions. The evaluation results of four functions on disor-\nder protein binding, DNA binding, RNA binding, and flexible linker, by different predictors\nwere listed in Tables 5–8. From these tables, we can see that the IDP-LM and fIDPnn predic-\ntors [27] provided all four common functional predictions for IDRs, where the fIDPnn predic-\ntor aggregated various structural and functional features at residue, window and protein levels,\nand achieved the second-best performance. IDP-LM, utilizing the pre-trained language model\nembeddings as exclusive inputs, performed best among comparable methods for all four com-\nmon disordered functions. These results of IDP-LM are not surprising, because the disorder\nand disordered functional properties of protein maintained in their amino acid sequences, the\nFig 5. The predictive performan ce for IDP-LM and other disorder predictor s on the CAID DisProt dataset. The ROC (a) and\nPrecisio n–Recall (b) curves of IDP-LM and other ten top-ranking predictors in the CAID experime nts [26]. AUC, the area under the\nROC curve; F\nmax\n, the maximu m harmonic mean between precisio n and recall across all thresholds.\nhttps://d oi.org/10.1371/j ournal.pc bi.1011657. g005\nFig 6. The predictive performa nce for IDP-LM and other disorder predictors on the CAID DisProt- PDB dataset. The ROC (a)\nand Precision– Recall (b) curves of IDP-LM and other ten top-ranki ng predictors in the CAID experiments [26]. AUC, the area\nunder the ROC curve; F\nmax\n, the maximum harmonic mean between precision and recall across all thresholds.\nhttps://doi.o rg/10.1371/j ournal.pcbi. 1011657.g006\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 11 / 18\nTable 3. Evaluation results of IDP-LM and the ten top-ranki ng predictor s in CAID for predicting fully disordered proteins on the CAID DisProt dataset.\nMethods F\nmax\nTN FP FN TP MCC TNR TPR PPV BAC\nIDP-LM 0.680 588 19 12 33 0.657 0.969 0.733 0.635 0.851\nfIDPnn* 0.598 585 16 19 26 0.569 0.973 0.578 0.619 0.776\nRawMSA* 0.578 582 19 19 26 0.546 0.968 0.578 0.578 0.773\nVSL2B* 0.505 578 23 22 23 0.468 0.962 0.511 0.500 0.736\nfIDPlr* 0.505 566 35 18 27 0.468 0.942 0.600 0.435 0.771\nPredisord er* 0.500 589 12 26 19 0.479 0.980 0.422 0.613 0.701\nSPOT-Di sorder1* 0.458 572 29 23 22 0.416 0.952 0.489 0.431 0.720\nDisoMine* 0.455 551 50 17 28 0.421 0.917 0.622 0.359 0.770\nAUCpreD* 0.453 588 13 28 17 0.431 0.978 0.378 0.567 0.678\nSPOT-Di sorder2* 0.452 574 27 24 21 0.409 0.955 0.467 0.438 0.711\nSPOT-Di sorder-Single* 0.448 594 7 30 15 0.452 0.988 0.333 0.682 0.661\n* The results of correspo nding predictors were obtained from [26] evaluated on the same CAID DisProt dataset. Predic tors are sorted by their AUC values. TNR, true\nnegative rate; TPR, true positive rate; PPV, positive predictive value, i.e., precision.\nhttps://do i.org/10.1371/j ournal.pc bi.1011657. t003\nFig 7. The predic tive performa nce for IDP-LM and other binding predictor s on the CAID DisProt-bi nding dataset. The ROC\n(a) and Precisio n–recall (b) curves of IDP-LM and other ten top-ranki ng predicto rs in the CAID experime nts [26]. AUC, the area\nunder the ROC curve; F\nmax\n, the maximum harmonic mean between precision and recall across all thresholds.\nhttps://doi. org/10.1371/j ournal.pcb i.1011657.g 007\nTable 4. Evaluatio n results of IDP-LM and the ten top-ranki ng predic tors in CAID for disordered binding sites\npredictio n on the CAID DisProt-bi nding dataset.\nMethods AUC F\nmax\nMCC BAC\nIDP-LM 0.792 0.260 0.239 0.730\nANCHO R-2* 0.742 0.231 0.199 0.694\nDisoRDPb ind-prot ein* 0.729 0.216 0.198 0.697\nMoRFchi bi-light* 0.720 0.215 0.161 0.636\nMoRFchi bi-web* 0.702 0.202 0.143 0.631\nANCHO R* 0.694 0.200 0.148 0.651\nOPAL* 0.693 0.195 0.151 0.652\nDISOPRED -3.1-bindin g* 0.568 0.169 0.099 0.569\nfMoRFpr ed* 0.547 0.124 0.054 0.515\nDisoRDPb ind-DNA* 0.531 0.123 0.052 0.502\nDisoRDPb ind* 0.428 0.119 0.000 0.500\n* The results of correspondin g predictors were obtained from [26] evaluated on the same CAID DisProt-bin ding\ndataset. Predic tors are sorted by their AUC values.\nhttps://d oi.org/10.1371/j ournal.pc bi.1011657. t004\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 12 / 18\nTable 5. Performan ce of IDP-LM and other predictor s for disordered protein binding function predic tion on the\nTE167 dataset.\nMethods AUC F\nmax\nMCC\nIDP-LM 0.824 0.473 0.403\nflDPnn* 0.792 0.436 0.363\nDisoRDPb ind* 0.759 0.177 0.084\nANCHO R-2* 0.705 0.328 0.220\nMorfChibi Light* 0.680 0.269 0.160\nfMoRFpr ed* 0.535 0.066 0.036\nMorfChibi * 0.521 0.203 0.009\n* The results of correspondin g predictors were obtained from [27] evaluated on the same TE167 dataset. Predic tors\nare sorted by their AUC values.\nhttps://d oi.org/10.1371/j ournal.pc bi.1011657. t005\nTable 8. Performan ce of IDP-LM and other predictor s for disordered flexible linker prediction on the TE167\ndataset.\nMethods AUC F\nmax\nMCC\nIDP-LM 0.748 0.263 0.250\nflDPnn* 0.712 0.183 0.168\nDFLpred* 0.443 0.000 -0.003\n* The results of correspondin g predictors were obtained from [27] evaluated on the same TE167 dataset. Predic tors\nare sorted by their AUC values.\nhttps://d oi.org/10.1371/j ournal.pc bi.1011657. t008\nTable 6. Performan ce of IDP-LM and other predictor s for disordered DNA binding function predictio n on the\nTE167 dataset.\nMethods AUC F\nmax\nMCC\nIDP-LM 0.897 0.176 0.208\nflDPnn* 0.872 0.151 0.211\nDisoRDPb ind* 0.676 0.085 0.086\n* The results of correspondin g predictors were obtained from [27] evaluated on the same TE167 dataset. Predic tors\nare sorted by their AUC values.\nhttps://d oi.org/10.1371/j ournal.pc bi.1011657. t006\nTable 7. Performan ce of IDP-LM and other predictor s for disordered RNA binding function prediction on the\nTE167 dataset.\nMethods AUC F\nmax\nMCC\nIDP-LM 0.883 0.262 0.259\nflDPnn* 0.861 0.178 0.195\nDisoRDPb ind* 0.647 0.133 0.126\n* The results of correspondin g predictors were obtained from [27] evaluated on the same TE167 dataset. Predic tors\nare sorted by their AUC values.\nhttps://d oi.org/10.1371/j ournal.pc bi.1011657. t007\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 13 / 18\nprotein language models pre-trained with massive disordered protein sequences learning these\nkey structural and functional features. The IDP-LM takes advantage of the protein language\nmodels and maps the disorder-to-function by transfer learning from the disordered predictor,\nleading to the accurate predictions of four common disorder functions.\nConclusion\nWe proposed IDP-LM, a computational predictor for protein intrinsic disorder and disorder\nfunctions. The IDP-LM takes the embeddings extracted from three pre-trained protein lan-\nguage models as the exclusive inputs, including ProtBERT and ProtT5 and a disordered spe-\ncific language model (IDP-BERT). The IDP-BERT provides fine-grained feature\nrepresentations for disorder at both the residue and sequence levels. The combination of Prot-\nBERT and ProtT5 and the disordered language model IDP-BERT provides comprehensive\nrepresentations for disordered protein, which facilitates IDP-LM outperforming other compa-\nrable methods for intrinsic disorder prediction in the CAID experiments. We transferred the\ntrained IDP-LM disorder predictor into four disorder functional predictors, including disor-\nder protein binding, DNA binding, RNA binding, and disorder flexible linkers. Benefiting\nfrom model transfer, the IDP-LM made fewer false positives and provided high-quality predic-\ntion results for all four common disorder functions. We released the source codes for IDP-LM\nat https://github.com/YihePang/ IDP-LM, and we also provided a stand-alone package of\nIDP-LM at http://bliulab.net/IDP_LM /.\nSupporting information\nS1 Fig. The distribution of AlphaFold confidence scores (pLDDT) in the disordered\nregions of CAID dataset. The real-labelled and predicted disordered regions by IDP-LM are\nshown in (a) and (b), respectively. The predicted disordered regions were obtained by setting\nthe threshold for propensity scores to 0.352, which is the optimal value with maximum F1\nvalue.\n(TIF)\nS1 Table. The description of the disorder function benchmark datasets.\n(DOCX)\nS2 Table. The hyper-parameters of IDP-BERT.\n(DOCX)\nS3 Table. The hyper-parameters of IDP-LM for disorder prediction.\n(DOCX)\nS4 Table. The hyper-parameters of IDP-LM for disorder function prediction.\n(DOCX)\nS5 Table. Pearson correlation analysis between disorder propensity scores predicted by\nIDP-LM and per-residue confidence score (pLDDT) produced by AlphaFold on the CAID\ndataset.\n(DOCX)\nS6 Table. The differences in annotations of four disordered functions on the TE176 dataset\nmeasured by Pearson Chi-Square (Χ\n2\n) test.\n(DOCX)\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 14 / 18\nS7 Table. The statistical difference (P-value) between IDP-LM, ProtBERT, ProtT5, and\nIDP-BERT in predicting disorder on the validation dataset.\n(DOCX)\nS8 Table. The statistical difference (P-value) between IDP-LM, ProtBERT, ProtT5, and\nIDP-BERT in predicting disordered protein-binding on the validation dataset.\n(DOCX)\nS9 Table. The statistical difference (P-value) between IDP-LM, ProtBERT, ProtT5, and\nIDP-BERT in predicting disordered DNA-binding on the validation dataset.\n(DOCX)\nS10 Table. The statistical difference (P-value) between IDP-LM, ProtBERT, ProtT5, and\nIDP-BERT in predicting disordered RNA-binding on the validation dataset.\n(DOCX)\nS11 Table. The statistical difference (P-value) between IDP-LM, ProtBERT, ProtT5, and\nIDP-BERT in predicting disordered flexible linker on the validation dataset.\n(DOCX)\nS1 Data. The numerical data used in all figures.\n(XLSX)\nAuthor Contributions\nConceptualization: Yihe Pang, Bin Liu.\nData curation: Yihe Pang.\nFormal analysis: Yihe Pang.\nFunding acquisition: Bin Liu.\nInvestigation: Yihe Pang.\nMethodology: Yihe Pang.\nResources: Yihe Pang, Bin Liu.\nSoftware: Yihe Pang.\nSupervision: Bin Liu.\nValidation: Yihe Pang, Bin Liu.\nVisualization: Yihe Pang.\nWriting – original draft: Yihe Pang.\nWriting – review & editing: Yihe Pang, Bin Liu.\nReferences\n1. Ahrens JB, Nunez-Cas tilla J, Siltberg-L iberles J. Evolutio n of intrinsic disorder in eukaryotic proteins.\nCell Mol Life Sci. 2017; 74(17):316 3–74. Epub 2017/06/ 10. https://doi.or g/10.100 7/s00018-017- 2559-0\nPMID: 285972 95.\n2. Peng Z, Yan J, Fan X, Mizianty MJ, Xue B, Wang K, et al. Exception ally abunda nt exceptions: compre-\nhensive characteriz ation of intrinsic disorder in all domains of life. Cell Mol Life Sci. 2015; 72(1):137 –51.\nEpub 2014/06/ 19. https://doi.or g/10.100 7/s00018-014- 1661-9 PMID: 249396 92.\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 15 / 18\n3. van der Lee R, Buljan M, Lang B, Weatheritt RJ, Daughdr ill GW, Dunker AK, et al. Classifi cation of\nintrinsica lly disordered regions and proteins . Chem Rev. 2014; 114(13):65 89–631. Epub 2014/04/3 0.\nhttps://doi.or g/10.102 1/cr400525m PMID: 24773235; PubMed Central PMCID: PMC409591 2.\n4. Wright PE, Dyson HJ. Intrinsical ly disordered proteins in cellular signalli ng and regulation. Nat Rev Mol\nCell Biol. 2015; 16(1):18–2 9. Epub 2014/12/ 23. https://doi.or g/10.1038 /nrm3920 PMID: 255312 25;\nPubMed Central PMCID: PMC440515 1.\n5. Borcherds W, Bremer A, Borgia MB, Mittag T. How do intrinsically disordered protein regions encode a\ndriving force for liquid-liqu id phase separation? Curr Opin Struct Biol. 2021; 67:41–50. Epub 2020/10 /\n18. https://doi. org/10.1016/j .sbi.2020.0 9.004 PMID: 33069007; PubMed Central PMCID:\nPMC804426 6.\n6. You K, Huang Q, Yu C, Shen B, Sevilla C, Shi M, et al. PhaSepD B: a database of liquid-liqu id phase\nseparation relate d proteins. Nucleic Acids Res. 2020; 48(D1):D35 4–D9. Epub 2019/10 /05. https://doi.\norg/10.1093/ nar/gkz84 7 PMID: 31584089; PubMed Central PMCID: PMC69430 39.\n7. Iakoucheva LM, Brown CJ, Lawson JD, Obradovic Z, Dunker AK. Intrinsic disorder in cell-sign aling and\ncancer-a ssociated proteins. J Mol Biol. 2002; 323(3):573 –84. Epub 2002/10/17. https://doi.or g/10.\n1016/s00 22-2836(0 2)00969-5 PMID: 12381310.\n8. Melo AM, Coraor J, Alpha-Cobb G, Elbaum -Garfinkle S, Nath A, Rhoades E. A functional role for intrin-\nsic disorder in the tau-tubulin complex. Proc Natl Acad Sci U S A. 2016; 113(50):14 336–41. Epub 2016/\n12/03. https:// doi.org/10.10 73/pnas.1 610137113 PMID: 27911791; PubMed Central PMCID:\nPMC516714 3.\n9. Dev KK, Hofele K, Barbieri S, Buchman VL, van der Putten H. Part II: alpha-syn uclein and its molecular\npathophy siological role in neurodegen erative disease. Neuropharm acology. 2003; 45(1):14–4 4. Epub\n2003/06/ 20. https://doi.or g/10.101 6/s0028-3908( 03)00140- 0 PMID: 128146 57.\n10. Cheng Y, LeGall T, Oldfield CJ, Mueller JP, Van YY, Rome ro P, et al. Rational drug design via intrinsi-\ncally disordered protein. Trends Biotechn ol. 2006; 24(10):435 –42. Epub 2006/08/01. https://doi.or g/10.\n1016/j.tib tech.2006.0 7.005 PMID: 1687689 3.\n11. Uversky VN. Intrinsically disordered proteins and novel strategies for drug discov ery. Expert Opin Drug\nDiscov. 2012; 7(6):475–8 8. Epub 2012/05 /09. https://doi.or g/10.151 7/17460441. 2012.686489 PMID:\n22559227.\n12. Elnaggar A, Heinzinger M, Dallago C, Rihawi G, Wang Y, Jones L, et al. ProtTrans : towards cracking\nthe language of Life’s code throug h self-superv ised deep learning and high performa nce computing.\nIEEE Transaction s on Patter n Analysis and Machine Intellig ence. 2020; 44(10):711 2–27.\n13. Searls DB. The language of genes. Nature. 2002; 420(6912) :211–7. Epub 2002/11/15. https:// doi.org/\n10.1038/ nature01255 PMID: 12432405.\n14. Unsal S, Atas H, Albayrak M, Turhan K, Acar AC, Doğan T. Learning functional properties of proteins\nwith language models. Nature Machine Intellig ence. 2022; 4(3):227–4 5.\n15. Li X, Romero P, Rani M, Dunker AK, Obradovic Z. Predicting Protein Disorder for N-, C-, and Internal\nRegions. Genom e Inform Ser Workshop Genome Inform. 1999; 10:30–40. Epub 2000/11 /10. PMID:\n11072340.\n16. Tang YJ, Pang YH, Liu B. DeepID P-2L: protein intrinsica lly disordered region prediction by combining\nconvolutio nal attention network and hierarchical attention network. Bioinformatic s. 2021; 38(5):1252 –\n60. Epub 2021/12 /06. https://doi.or g/10.109 3/bioinformat ics/btab810 PMID: 34864847.\n17. Devlin J, Chang M-W, Lee K, Toutanova K. Bert: Pre-train ing of deep bidirectional transfor mers for lan-\nguage understa nding. Proceedings of the 2019 Conferen ce of the North American Chapter of the Asso-\nciation for Comp utational Linguistics2 019. p. 4171–86.\n18. Piovesan D, Necci M, Escobedo N, Monzon AM, Hatos A, Micetic I, et al. MobiDB: intrinsi cally disor-\ndered proteins in 2021. Nucleic Acids Res. 2021; 49(D1):D36 1–D7. Epub 2020/11/2 6. https://doi.or g/\n10.1093/ nar/gkaa1058 PMID: 33237329; PubMed Central PMCID: PMC777901 8.\n19. Katuwawala A, Zhao B, Kurgan L. DisoLipPr ed: Accurate predicti on of disordered lipid binding residues\nin protein sequences with deep recurrent networks and transfer learning. Bioinforma tics. 2021; 38\n(1):115–24 . Epub 2021/09 /07. https://doi.or g/10.109 3/bioinformat ics/btab640 PMID: 34487138.\n20. Meng F, Kurgan L. DFLpred: High-through put prediction of disorde red flexible linker regions in protein\nsequences. Bioinf ormatics. 2016; 32(12):i34 1–i50. Epub 2016/06 /17. https://doi.or g/10.109 3/\nbioinforma tics/btw280 PMID: 27307636; PubMed Central PMCID: PMC490836 4.\n21. Peng Z, Xing Q, Kurgan L. APOD: accurate sequence- based predictor of disordered flexible linkers.\nBioinformat ics. 2020;36 (Suppl_2): i754-i61. Epub 2021/01/01. https:// doi.org/10.10 93/bioinf ormatics/\nbtaa808 PMID: 33381830; PubMed Central PMCID: PMC777348 5.\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 16 / 18\n22. Hanson J, Litfin T, Paliwal K, Zhou Y. Identifying molecular recogni tion features in intrinsi cally disor-\ndered regions of proteins by transfer learning. Bioinformatics . 2020; 36(4):1107 –13. Epub 2019/09/11.\nhttps://doi.or g/10.109 3/bioinformat ics/btz691 PMID: 315041 93.\n23. Berman HM, Westbrook J, Feng Z, Gilliland G, Bhat TN, Weiss ig H, et al. The Protein Data Bank.\nNucleic Acids Res. 2000; 28(1):235– 42. Epub 1999/12/11. https:// doi.org/10.10 93/nar/28. 1.235 PMID:\n10592235; PubMed Central PMCID: PMC1 02472.\n24. Burley SK, Bhikadiya C, Bi C, Bittrich S, Chen L, Crichlow GV, et al. RCSB Protein Data Bank: powerful\nnew tools for exploring 3D structures of biologic al macromolecul es for basic and applied research and\neducation in fundamen tal biology, biomedi cine, biotechno logy, bioeng ineering and energy sciences.\nNucleic Acids Res. 2021; 49(D1):D43 7–D51. Epub 2020/11 /20. https://doi.o rg/10.1093/na r/gkaa10 38\nPMID: 332118 54; PubMed Central PMCID: PMC777900 3.\n25. Bepler T, Berger B. Learning the protein language: Evolutio n, structure, and function. Cell Syst. 2021;\n12(6):654– 69 e3. Epub 2021/06 /18. https://doi.o rg/10.1016/j.c els.2021.05. 017 PMID: 34139171;\nPubMed Central PMCID: PMC823839 0.\n26. Necci M, Piovesan D, Predict ors C, DisProt C, Tosatto SCE. Critical assessmen t of protein intrinsic dis-\norder prediction. Nat Methods. 2021; 18(5):472– 81. Epub 2021/04/21 . https://doi.or g/10.1038/ s41592-\n021-01117- 3 PMID: 338758 85; PubMed Central PMCID: PMC810517 2.\n27. Hu G, Katuwawala A, Wang K, Wu Z, Ghadermar zi S, Gao J, et al. flDPnn: Accurate intrinsic disorde r\nprediction with putative propensities of disorder functions. Nat Commun . 2021; 12(1):4438 . Epub 2021/\n07/23. https:// doi.org/10.10 38/s4146 7-021-247 73-7 PMID: 34290238; PubMed Central PMCID:\nPMC829526 5.\n28. Quaglia F, Meszaros B, Salladini E, Hatos A, Pancsa R, Chemes LB, et al. DisProt in 2022: improved\nquality and accessibility of protein intrinsic disorder annotati on. Nucleic Acids Res. 2022; 50(D1):D48 0–\nD7. Epub 2021/12/02. https://doi.or g/10.1093/n ar/gkab1082 PMID: 348501 35; PubMed Central\nPMCID: PMC872821 4.\n29. Altschul SF, Madden TL, Schaffer AA, Zhang J, Zhang Z, Miller W, et al. Gapped BLAST and PSI-\nBLAST: a new generation of protein database search programs. Nucleic Acids Res. 1997; 25\n(17):3389– 402. Epub 1997/09 /01. https://doi.or g/10.109 3/nar/25.17. 3389 PMID: 9254694; PubMed\nCentral PMCID: PMC1 46917.\n30. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attentio n is all you need. Pro-\nceeding s of the Thirty-fi rst Conferen ce on Neural Information Processin g System s 2017. p. 5998–6 008.\n31. Vondervis zt F, Ishima R, Akasaka K, Aizawa S. Terminal disorder: a common structural feature of the\naxial proteins of bacterial flagellum ? J Mol Biol. 1992; 226(3):575 –9. Epub 1992/08 /05. https://doi.or g/\n10.1016/ 0022-2836( 92)90616-r PMID: 1507216.\n32. Tompa P. Intrinsical ly unstructur ed proteins. Trends Biochem Sci. 2002; 27(10):527 –33. Epub 2002/10/\n09. https://doi. org/10.1016/s 0968-0004( 02)02169-2 PMID: 12368089.\n33. Habchi J, Tompa P, Longhi S, Uversky VN. Introduci ng protein intrinsic disorde r. Chem Rev. 2014; 114\n(13):6561– 88. Epub 2014/04 /18. https://doi.or g/10.102 1/cr400514h PMID: 24739139.\n34. Rives A, Meier J, Sercu T, Goyal S, Lin Z, Liu J, et al. Biological structure and function emerge from\nscaling unsupervis ed learning to 250 million protein sequences . Proc Natl Acad Sci U S A. 2021; 118\n(15):e2016 239118. Epub 2021/04 /21. https://do i.org/10.1073 /pnas.201 6239118 PMID: 33876751;\nPubMed Central PMCID: PMC805394 3.\n35. Hanson J, Paliwal KK, Litfin T, Zhou Y. SPOT-Dis order2: Improved Protein Intrinsic Disorder Prediction\nby Ensemble d Deep Learning. Genom ics Proteom ics Bioinform atics. 2019; 17(6):645– 56. Epub 2020/\n03/17. https:// doi.org/10.10 16/j.gpb .2019.01.004 PMID: 32173600; PubMed Central PMCID:\nPMC721248 4.\n36. Hanson J, Yang Y, Paliwal K, Zhou Y. Improving protein disorde r predicti on by deep bidirec tional long\nshort-term memory recurrent neural networks. Bioinform atics. 2017; 33(5):685– 92. Epub 2016/12 /25.\nhttps://doi.or g/10.109 3/bioinformat ics/btw678 PMID: 28011771.\n37. Tang YJ, Pang YH, Liu B. IDP-Seq 2Seq: identifica tion of intrinsica lly disordered regions based on\nsequence to sequence learning. Bioinforma tics. 2021; 36(21):517 7–86. Epub 2020/07 /24. https://doi.\norg/10.1093/ bioinformatic s/btaa667 PMID: 32702119.\n38. Whitley D. A genetic algorithm tutoria l. Statistics and computing. 1994; 4(2):65–85 .\n39. Katoch S, Chauhan SS, Kumar V. A review on genetic algorithm: past, present, and future. Multimedia\nTools and Applications . 2021; 80(5):8091 –126.\n40. Ruby U, Yendapalli V. Binary cross entropy with deep learning technique for image classific ation. Int J\nAdv Trends Comput Sci Eng. 2020; 9(10):5393 –97.\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 17 / 18\n41. Pang Y, Liu B. DMFpred: Predic ting protein disorder molecular functions based on protein cubic lan-\nguage model. PLoS Comput Biol. 2022; 18(10):e10 10668. Epub 2022/11/01. https://doi.or g/10.1371/\njournal.pcbi. 1010668 PMID: 36315580; PubMed Central PMCID: PMC967415 6.\n42. Pang YH, Liu B. TransDFL: Identificati on of Disordere d Flexible Linkers in Proteins by Transfer Learn-\ning. Genom ics, Proteom ics & Bioinform atics. 2023; 12(2):359– 369.\n43. Van der Maaten L, Hinton G. Visuali zing data using t-SNE. Journal of machine learning resear ch. 2008;\n9(11):2579 –605.\n44. Kornbrot D. Point biserial correlation. Wiley StatsRef: Statistics Reference Online. 2014.\n45. Varadi M, Anyang o S, Deshpande M, Nair S, Natassia C, Yordanov a G, et al. AlphaFold Protein Struc-\nture Database: massivel y expandi ng the structural coverage of protein-sequ ence space with high-accu -\nracy models. Nucleic Acids Res. 2022; 50(D1):D43 9–D44. Epub 2021/11/ 19. https://doi.or g/10.1093 /\nnar/gkab 1061 PMID: 34791371; PubMed Central PMCID: PMC87 28224.\n46. Jumper J, Evans R, Pritzel A, Green T, Figurnov M, Ronneber ger O, et al. Highly accurate protein struc-\nture prediction with AlphaFold. Nature. 2021; 596(7873): 583–9. Epub 2021/07/ 16. https://doi.or g/10.\n1038/s41 586-021-0 3819-2 PMID: 34265844; PubMed Central PMCID: PMC8 371605.\n47. Piovesan D, Tabaro F, Paladin L, Necci M, Micetic I, Camilloni C, et al. MobiDB 3.0: more annotatio ns\nfor intrinsic disorder, conformatio nal diversity and interactions in proteins. Nucleic Acids Res. 2018; 46\n(D1):D471– D6. Epub 2017/11/ 15. https://doi.or g/10.1093 /nar/gkx107 1 PMID: 29136219; PubMed Cen-\ntral PMCID: PMC575334 0.\nPLOS COMP UTATIONAL  BIOLOGY\nIDP-LM\nPLOS Computationa l Biology | https:/ /doi.org/10.13 71/journal.p cbi.1011657 Novemb er 22, 2023 18 / 18"
}