{
    "title": "A Performance Evaluation of Large Language Models in Keratoconus: A Comparative Study of ChatGPT-3.5, ChatGPT-4.0, Gemini, Copilot, Chatsonic, and Perplexity",
    "url": "https://openalex.org/W4403891030",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A2043277745",
            "name": "Ali Hakim Reyhan",
            "affiliations": [
                "Harran University"
            ]
        },
        {
            "id": "https://openalex.org/A4224916615",
            "name": "Çağrı Mutaf",
            "affiliations": [
                "Harran University"
            ]
        },
        {
            "id": "https://openalex.org/A4269040355",
            "name": "İrfan Uzun",
            "affiliations": [
                "Harran University"
            ]
        },
        {
            "id": "https://openalex.org/A2553397139",
            "name": "Funda Yuksekyayla",
            "affiliations": [
                "Harran University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W6855899711",
        "https://openalex.org/W4394674889",
        "https://openalex.org/W2750707006",
        "https://openalex.org/W4396952102",
        "https://openalex.org/W4392160361",
        "https://openalex.org/W4388931396",
        "https://openalex.org/W4205468340",
        "https://openalex.org/W1995880580",
        "https://openalex.org/W4383501206",
        "https://openalex.org/W6855625910",
        "https://openalex.org/W4388085114",
        "https://openalex.org/W4385998042",
        "https://openalex.org/W4386110374",
        "https://openalex.org/W4399052098",
        "https://openalex.org/W4396775758",
        "https://openalex.org/W4391018948",
        "https://openalex.org/W4388725043",
        "https://openalex.org/W4394873790",
        "https://openalex.org/W4399830266",
        "https://openalex.org/W4391815795",
        "https://openalex.org/W4400163553",
        "https://openalex.org/W4388922836",
        "https://openalex.org/W4393093628",
        "https://openalex.org/W4386046428",
        "https://openalex.org/W4323566298",
        "https://openalex.org/W4220983040",
        "https://openalex.org/W6853149371",
        "https://openalex.org/W3160966343",
        "https://openalex.org/W3133820498",
        "https://openalex.org/W4388014051",
        "https://openalex.org/W4398190964",
        "https://openalex.org/W6866642107",
        "https://openalex.org/W4390498094",
        "https://openalex.org/W4395044331",
        "https://openalex.org/W4391852215",
        "https://openalex.org/W3022329505",
        "https://openalex.org/W3107785991",
        "https://openalex.org/W4386332497",
        "https://openalex.org/W4386033569",
        "https://openalex.org/W4396745229",
        "https://openalex.org/W4375859242"
    ],
    "abstract": "Background: This study evaluates the ability of six popular chatbots; ChatGPT-3.5, ChatGPT-4.0, Gemini, Copilot, Chatsonic, and Perplexity, to provide reliable answers to questions concerning keratoconus. Methods: Chatbots responses were assessed using mDISCERN (range: 15–75) and Global Quality Score (GQS) (range: 1–5) metrics. Readability was evaluated using nine validated readability assessments. We also addressed the quality and accountability of websites from which the questions originated. Results: We analyzed 20 websites, 65% “Private practice or independent user” and 35% “Official patient education materials”. The mean JAMA benchmark score was 1.40 ± 0.91 (0–4 points), indicating low accountability. Reliability, measured using mDISCERN, ranged from 42.91 ± 3.15 (ChatGPT-3.5) to 46.95 ± 3.53 (Copilot). The most frequent question was “What is keratoconus?” with 70% of websites providing relevant information. This received the highest mDISCERN score (49.30 ± 4.91) and a relatively high GQS score (3.40 ± 0.56) with an Automated Readability Level Calculator score of 13.17 ± 2.13. Moderate positive correlations were determined between the website numbers and both mDISCERN (r = 0.265, p = 0.25) and GQS (r = 0.453, p = 0.05) scores. The quality of information, assessed using the GQS, ranged from 3.02 ± 0.55 (ChatGPT-3.5) to 3.31 ± 0.64 (Gemini) (p = 0.34). The differences between the texts were statistically significant. Gemini emerged as the easiest to read, while ChatGPT-3.5 and Perplexity were the most difficult. Based on mDISCERN scores, Gemini and Copilot exhibited the highest percentage of responses in the “good” range (51–62 points). For the GQS, the Gemini model exhibited the highest percentage of responses in the “good” quality range with 40% of its responses scoring 4–5. Conclusions: While all chatbots performed well, Gemini and Copilot showed better reliability and quality. However, their readability often exceeded recommended levels. Continuous improvements are essential to match information with patients’ health literacy for effective use in ophthalmology.",
    "full_text": null
}