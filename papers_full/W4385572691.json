{
    "title": "ELMER: A Non-Autoregressive Pre-trained Language Model for Efficient and Effective Text Generation",
    "url": "https://openalex.org/W4385572691",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A5100363220",
            "name": "Junyi Li",
            "affiliations": [
                "Universit√© de Montr√©al",
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5030566556",
            "name": "Tianyi Tang",
            "affiliations": [
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5037145565",
            "name": "Wayne Xin Zhao",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        },
        {
            "id": "https://openalex.org/A5018977183",
            "name": "Jian‚ÄêYun Nie",
            "affiliations": [
                "Universit√© de Montr√©al"
            ]
        },
        {
            "id": "https://openalex.org/A5025631695",
            "name": "Ji-Rong Wen",
            "affiliations": [
                "Beijing Institute of Big Data Research",
                "Renmin University of China"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2963825865",
        "https://openalex.org/W3126267552",
        "https://openalex.org/W4229023828",
        "https://openalex.org/W3037013468",
        "https://openalex.org/W4283650355",
        "https://openalex.org/W2888482885",
        "https://openalex.org/W4226474154",
        "https://openalex.org/W2963206148",
        "https://openalex.org/W2949644922",
        "https://openalex.org/W2963434219",
        "https://openalex.org/W2970832665",
        "https://openalex.org/W2970597249",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W2963748441",
        "https://openalex.org/W3176929804",
        "https://openalex.org/W2963536265",
        "https://openalex.org/W2767206889",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W3187134297",
        "https://openalex.org/W2154652894",
        "https://openalex.org/W3100439847",
        "https://openalex.org/W4229005744",
        "https://openalex.org/W4303874710",
        "https://openalex.org/W3035038672",
        "https://openalex.org/W2988975212",
        "https://openalex.org/W2001771035",
        "https://openalex.org/W2794421626",
        "https://openalex.org/W2962969034",
        "https://openalex.org/W2946375144",
        "https://openalex.org/W2952468927",
        "https://openalex.org/W4287327373",
        "https://openalex.org/W3173273620",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W2981757109",
        "https://openalex.org/W3206889547",
        "https://openalex.org/W3176740717",
        "https://openalex.org/W3174461835",
        "https://openalex.org/W2979826702",
        "https://openalex.org/W3170113752",
        "https://openalex.org/W3100753857",
        "https://openalex.org/W3034999214",
        "https://openalex.org/W2123301721"
    ],
    "abstract": "We study the text generation task under the approach of pre-trained language models (PLMs). Typically, an auto-regressive (AR) method is adopted for generating texts in a token-by-token manner. Despite many advantages of AR generation, it usually suffers from inefficient inference. Therefore, non-autoregressive (NAR) models are proposed to generate all target tokens simultaneously. However, NAR models usually generate texts of lower quality due to the absence of token dependency in the output text. In this paper, we propose ELMER: an efficient and effective PLM for NAR text generation to explicitly model the token dependency during NAR generation. By leveraging the early exit technique, ELMER enables the token generations at different layers, according to their prediction confidence (a more confident token will exit at a lower layer). Besides, we propose a novel pre-training objective, Layer Permutation Language Modeling, to pre-train ELMER by permuting the exit layer for each token in sequences. Experiments on three text generation tasks show that ELMER significantly outperforms NAR models and further narrows the performance gap with AR PLMs (ELMER (29.92) vs BART (30.61) ROUGE-L in XSUM) while achieving over 10 times inference speedup.",
    "full_text": "Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1044‚Äì1058\nDecember 7-11, 2022 ¬©2022 Association for Computational Linguistics\nELMER : A Non-Autoregressive Pre-trained Language Model for Efficient\nand Effective Text Generation\nJunyi Li1,3,4, Tianyi Tang1, Wayne Xin Zhao1,4‚àó\n, Jian-Yun Nie3 and Ji-Rong Wen1,2,4\n1Gaoling School of Artificial Intelligence, Renmin University of China\n2School of Information, Renmin University of China\n3DIRO, Universit√© de Montr√©al\n4Beijing Key Laboratory of Big Data Management and Analysis Methods\n{lijunyi,steven_tang}@ruc.edu.cn batmanfly@gmail.com\nAbstract\nWe study the text generation task under the ap-\nproach of pre-trained language models (PLMs).\nTypically, an auto-regressive (AR) method is\nadopted for generating texts in a token-by-\ntoken manner. Despite many advantages of AR\ngeneration, it usually suffers from inefficient in-\nference. Therefore, non-autoregressive (NAR)\nmodels are proposed to generate all target to-\nkens simultaneously. However, NAR models\nusually generate texts of lower quality due to\nthe absence of token dependency in the out-\nput text. In this paper, we propose ELMER :\nan Efficient and effective PLM for NAR tExt\ngeneRation to explicitly model the token de-\npendency during NAR generation. By leverag-\ning the early exit technique, ELMER enables\nthe token generations at different layers, ac-\ncording to their prediction confidence (a more\nconfident token will exit at a lower layer). Be-\nsides, we propose a novel pre-training objec-\ntive, Layer Permutation Language Modeling,\nto pre-train ELMER by permuting the exit layer\nfor each token in sequences. Experiments on\nthree text generation tasks show that ELMER\nsignificantly outperforms NAR models and fur-\nther narrows the performance gap with AR\nPLMs (e.g., ELMER (29.92) vs BART (30.61)\nROUGE-L in XSUM) while achieving over 10\ntimes inference speedup.\n1 Introduction\nSince the advant of GPT-2 (Radford et al., 2019),\npre-trained language models (PLMs) have achieved\nstate-of-the-art performance across text generation\ntasks, which aim to generate human-like texts on\ndemand (Brown et al., 2020; Li et al., 2022c).\nThese PLMs usually adopt an auto-regressive (AR)\nfashion to generate texts token-by-token: the next\ntoken is predicted based on all previously gener-\nated tokens. A major limitation of this approach is\nthat it is hard to be parallelized for the inference\n‚àóCorresponding author\nprocess, thus leading to a relatively high inference\nlatency (Gu et al., 2018). Such a limitation prevents\nAR models from wide deployment in online real-\ntime applications, such as query rewriting in search\nengines and online chat-bot. Moreover, AR models\nare prone to suffering from the exposure bias prob-\nlem since there is a gap between AR training and in-\nference (Zeng and Nie, 2021). These concerns have\nsparked extensive interests in non-autoregressive\n(NAR) models for text generation (Gu et al., 2018).\nCompared to AR models, NAR models predict\ntarget tokens in all positions simultaneously and\nindependently (Gu et al., 2018). This full paral-\nlelism leads to an efficient and low-latency infer-\nence process. However, the independence assump-\ntion prevents NAR models from learning the depen-\ndency among target tokens, resulting in accuracy\ndegradation (Zhan et al., 2022). One widely-used\nsolution to improve the NAR generation quality\nis to iteratively refine outputs (Gu et al., 2019;\nGhazvininejad et al., 2019), which however leads\nto the loss in the speed-up advantage. In addition,\nmany studies aim to learn the input-output mapping\nfor more accurate generation via embedding map-\nping (Guo et al., 2019), latent alignment (Libovick√Ω\nand Helcl, 2018), and discrete variables (Ma et al.,\n2019). While easing the difficulty of NAR gener-\nation to some extent, these methods still struggle\nfor generating complex sentences. Therefore, in-\nspired by Zhan et al. (2022), we argue that the key\nto NAR text generation is to enhance the learning\nof token dependency‚Äîeach token should be gener-\nated depending on forward and backward generated\ntokens.\nIn this paper, we propose ELMER : an Efficient\nand Effective PLM for NAR tExt geneRation, to\nexplicitly learn the bi-directional token dependency.\nTypically, most NAR models predict tokens simul-\ntaneously only at the last layer, thus making the\ntoken prediction unaware of tokens generated in\nother positions. To address this issue, we propose\n1044\nto generate tokens at different layers and the upper-\nlayer token generation can depend on lower-layer\ngenerated tokens from both left and right. In this\nway, our model can explicitly learn the dependency\nbetween tokens from different layers while enjoy-\ning full parallelism in NAR decoding, as shown in\nFigure 1. To this end, we propose to extend the\nearly exit technique (Li et al., 2021c) to NAR text\ngeneration: if there is sufficient confidence to gen-\nerate a token at a lower layer, the model is allowed\nto exit at this layer and make the prediction without\npassing through the upper layers.\nFurthermore, instead of exiting at a fixed layer\nfor a token, we aim to predict each token at differ-\nent layers for learning diverse token dependencies\nin NAR text generation. Thus, inspired by XLNet\n(Yang et al., 2019), we further propose a novel pre-\ntraining objective based on early exit, i.e., Layer\nPermutation Language Modeling (LPLM), to help\nELMER learn complex token dependencies. Given\na sequence, LPLM will permute the exit layer (from\n1 to the maximum layer) for each token and maxi-\nmize the NAR text generation probability w.r.t. all\npossible exit layer permutations of the sequence.\nThrough LPLM, each token is able to exit at dif-\nferent layers and attend to all other tokens from\nboth forward and backward positions. In this way,\nLPLM could effectively capture diverse token de-\npendencies from large-scale corpora. Pre-trained\nwith the general LPLM, ELMER can adapt to down-\nstream text generation tasks and datasets by using\nspecific early exit strategies.\nTo the best of our knowledge, we are the first\nto introduce the idea of early exit to NAR text\ngeneration. We fine-tune ELMER on three popu-\nlar text generation tasks. Experiments show that\nELMER significantly improves the best NAR mod-\nels by +5.71 ROUGE-1 on XSUM, +1.09 ME-\nTEOR on SQuAD v1.1, and +2.26 Distinct-2 on\nPersonaChat, and narrows the performance gap\nwith auto-regressive PLMs (e.g., ELMER (29.92) vs\nBART (30.61) ROUGE-L on XSUM) while achiev-\ning over 10x faster inference.\n2 Related Work\nPre-trained Language Models. Recent years have\nwitnessed remarkable achievement of PLMs in text\ngeneration tasks (Li et al., 2021b). Most PLMs\nadopt an AR paradigm to generate texts during\npre-training and fine-tuning. The work based on\nGPT (Radford et al., 2019; Brown et al., 2020)\nconverts different tasks into language modeling by\nsequentially predicting tokens. BART (Lewis et al.,\n2020) employs an auto-regressive decoder to re-\ncover the corrupted text in pre-training. T5 (Raffel\net al., 2020) masks word spans from input texts and\nthen sequentially predicts masked tokens. Tang\net al. (2022) pre-trains a text generation model us-\ning labeled datasets with multi-task learning. Li\net al. (2022b) leverages prompts to effectively fine-\ntune text generation models. Differently, our PLM,\nELMER , adopts a NAR schema to generate texts,\nwhich leads to a very low latency in inference.\nNon-autoregressive Text Generation. Recently,\nthere is a wide range of studies for NAR text genera-\ntion (Gu et al., 2018; Ghazvininejad et al., 2019; Qi\net al., 2021). Among them, Gu et al. (2018) is the\nfirst to propose NAR paradigm to reduce the infer-\nence latency. Ghazvininejad et al. (2019) iteratively\nmasks and predicts a fraction of tokens that the\nmodel is least confident about. Several groups aim\nto learn accurate input-output mapping. For exam-\nple, Saharia et al. (2020) and Libovick√Ω and Helcl\n(2018) use connectionist temporal classification to\nperform latent alignment in NAR models. Our\nwork is closely related to BANG (Qi et al., 2021),\na PLM bridging the NAR and AR generation. We\ndiffer in that we use early exit to predict tokens\nat different layers, which can help NAR models\nlearn the forward and backward token dependency.\nMoreover, we propose a novel pre-training objec-\ntive based on early exit, LPLM, for learning diverse\ntoken dependencies by permuting the exit layer for\neach token.\n3 Preliminaries\nGenerally, the goal of text generation is to model\nthe conditional probability Pr(Y|X), where X=\n‚ü®x1,...,x n‚ü©and Y= ‚ü®y1,...,y m‚ü©denote the in-\nput text and output text respectively and each con-\nsists of a sequence of tokens from a vocabulary V.\nThere are three common generation paradigms to\nmodel the conditional probability Pr(Y|X), i.e., au-\ntoregressive (AR), non-autoregressive (NAR), and\nsemi-nonautoregressive (Semi-NAR) generation.\nAR Generation. AR generation models predict the\noutput text based on a left-to-right factorization as:\nPr(Y|X) =\nm‚àè\nt=1\nPr(yt|y<t,X), (1)\nwhere each token yt is generated based on the input\n1045\ntext Xand previous tokens y<t. Note that AR gen-\neration only models the forward token dependency.\nThe token-by-token fashion makes AR generation\nprocess hard to be parallelized. Most of existing\ntext generation PLMs adopt AR approach (Radford\net al., 2019; Lewis et al., 2020; Raffel et al., 2020).\nNAR Generation. In contrast to AR models, NAR\ntext generation models predict each token in output\ntext simultaneously as follows, without modeling\nthe forward or backward token dependency:\nPr(Y|X) =\nm‚àè\nt=1\nPr(yt|X), (2)\nwhere each token yt is predicted only based on the\ninput text X. The independence assumption makes\nNAR generation process parallelizable, thus signif-\nicantly accelerating the inference speed (Gu et al.,\n2018). While, in the absence of token dependency,\nthe generation quality of NAR models is lower than\ntheir AR counterparts (Wang et al., 2019).\nSemi-NAR Generation. Semi-NAR generation is\nformalized between AR and NAR generation as:\nPr(Y|X) =\nm‚àè\nt=1\nPr(yt|Yct ,X), (3)\nwhere each token yt is conditioned on the input text\nXand a visible part Yct of the output text Y. Yct\nis designed differently to balance inference latency\nand accuracy (Stern et al., 2019; Lee et al., 2018).\nNote that the lower-layer generated tokens in our\nmodel is similar to the visible part Yct . While, our\nmodel keeps the advantage of full parallelism in\ncontrast to iterative Semi-NAR methods.\nIn this paper, we mainly focus on the NAR\napproach, considering both effectiveness and ef-\nficiency for text generation models.\n4 Approach\nOur proposed NAR text generation PLM, ELMER ,\nis depicted in Figure 1. ELMER aims to enhance\nthe modeling of token dependency for NAR mod-\nels. With early exit, tokens exiting at different lay-\ners can build the bi-directional token dependency\nwith each other. Moreover, we design Layer Per-\nmutation Language Modeling (LPLM) to pre-train\nELMER by permuting the exit layer for each token.\nNext, we will describe each part in detail.\nLayer 1\nLayer 2\nLayer 3\nLayer 4\nùë¶!\nùë¶\"\nùë¶#\nùë¶$\n[MASK] [MASK] [MASK] [MASK] [MASK]\nCopy hidden state\nEarly exit...\nLayer ùêø NAR Decoder\nLayer Permutation \nLanguage Modeling\nExit Layer Permutation  \nùí´% = 2, 3, 1, 4, 3\nExit Layer Permutation \nùí´& = 4, 3, 2, 1, 2\nùë¶!\nùë¶\"\nùë¶#\nùë¶$\nùë¶%\nùë¶!\nùë¶\"\nùë¶#\nùë¶$\nùë¶%...\nùë¶' token dependency\nFigure 1: Overview of our proposed model ELMER .\n4.1 Early Exit for Dependency Modeling\nMost NAR models simultaneously predict tokens\nonly at the last layer (Jiang et al., 2021; Zhan et al.,\n2022), which makes the current token generation\nunaware of the tokens generated in other positions.\nThus, to model the bi-directional token dependency\n(both forward and backward), we propose to predict\ntokens at different layers by leveraging the early\nexit technique (Li et al., 2021c). In this way, the\nupper-layer token generation can depend on tokens\ngenerated at lower layers from both left and right.\nNAR Transformer.ELMER is built on the Trans-\nformer encoder-decoder architecture (Vaswani\net al., 2017). Both encoder and decoder consist of\nLstacked layers where each layer contains several\nsub-layers (e.g., multi-head self-attention and feed-\nforward network). Unlike the original Transformer\ndecoder that auto-regressively generates text, our\nmodel uses NAR fashion to generate tokens simul-\ntaneously. Given a pair of input-output text‚ü®X,Y‚ü©,\nXis fed into the encoder and processed as hidden\nstates S = ‚ü®s1,..., sn‚ü©. We then feed a sequence\nof ‚Äú[MASK]‚Äù tokens into the NAR decoder to gen-\nerate every token in output text Yin parallel.\nSpecifically, we first replace the original masked\nmulti-head attention in decoder with bi-directional\nmulti-head attention akin to the encoder. For each\n‚Äú[MASK]‚Äù token in thet-th position, the Ldecoder\nlayers process it to hidden states {hl\nt}1‚â§l‚â§L as:\nhl\nt = Layerl(hl‚àí1\n1‚â§t‚â§T ,S), (4)\nh0\nt = Embed([MASK]), (5)\nwhere Layerl(¬∑) denotes the l-th layer, Embed(¬∑) is\nthe sum of word embedding and position embed-\nding, and T is the maximum length of the decoder.\n1046\nThe output distribution for predicting thet-th token\nyt is computed by feeding the last decoder statehL\nt\ninto a softmax classifier (parameterized by Wc) as:\nPr(yt|hL\nt ) =softmax(WchL\nt ). (6)\nPrior NAR models require to determine the output\nlength, thus an extra module for length prediction is\nalways needed. Instead, following Su et al. (2021),\nwe let ELMER dynamically adjust the output length\nby emitting an end token (i.e., [EOS]) at any posi-\ntion. After the entire generation is completed, the\nfinal sequence ranges from the beginning to the\nposition where the first end token is emitted.\nEarly Exit Off-Ramps. Instead of predicting all\ntokens simultaneously at the last layer, we propose\nto generate tokens at different layers for learning\nthe bi-directional token dependency. Hence, by\nleveraging early exit (Li et al., 2021c), if a token\nis predicted with sufficient confidence at a lower\nlayer, the model is allowed to exit and predict with-\nout passing through the upper layers. In this way,\nthe upper-layer tokens can be generated depending\non forward and backward tokens generated at the\nlower layers. The bi-directional token dependency\ncan further alleviate the hallucination issue in NAR\ngeneration (Gu et al., 2018), where the generated\ntexts tend to be ungrammatical with repetitions.\nSpecifically, we inject the ‚Äúoff-ramps‚Äù (Li et al.,\n2021c), which make predictions with intermediate\nhidden states, at each decoder layer. The off-ramp\ncan be simply implemented by a softmax classifier.\nFor an off-ramp at the l-th layer, it makes the token\nprediction as:\nPr(yt|hl\nt) = Off-Rampl(hl\nt), (7)\n= softmax(Wl\nchl\nt), (8)\nwhere the l-th off-ramp is parameterized by Wl\nc.\nThese off-ramps can be specified independently or\nshare the weights across Llayers. Different from\nprevious early-exit work that makes sentence-level\nprediction (Xin et al., 2020; Liao et al., 2021), our\nearly exit is built at token level.\nDuring training, if a token has been predicted at\nthe l-th layer early, the hidden state hl\nt will not be\nupdated in upper layers. Thus, in our model, we\ndirectly copy the last hidden statehl\nt (exit at layerl)\nto the subsequent layers following (Elbayad et al.,\n2020; Li et al., 2021c). Since the last hidden state\npredicts tokens with sufficient confidence, it has\ncontained the predicted token information provided\nfor the upper-layer tokens generation.\n4.2 Layer Permutation Pre-training\nThe NAR models equipped with early exit predict\neach token at a fixed layer. To learn diverse token\ndependencies, we design a novel pre-training ob-\njective based on early exit, Layer Permutation Lan-\nguage Modeling (LPLM), where each token can\nexit at different layers. This is different from most\nprior work that focuses on designing small-scale\nNAR models for specific tasks such as translation.\nIn contrast, we pre-train a general large-scale PLM\non massive corpora following Qi et al. (2021). Our\nPLM can adapt to various downstream tasks.\nLayer Permutation Language Modeling. Per-\nmutation language modeling was first proposed in\nXLNet (Yang et al., 2019) by permuting the factor-\nization order. For a sequence with length T, there\nare T! permutation orders to consider in autore-\ngressive factorization. In our LPLM, rather than\nperforming permutation on the sequence length, we\npermute the exit layer for each token. In particular,\nfor each token in a sequence, we assume that it can\nexit at any of Llayers. Therefore, there are LT exit\nlayer permutations for a sequence. Intuitively, if\nmodel parameters are shared across all exit layer\npermutations, each token can build diverse seman-\ntic dependencies with tokens in all positions.\nFormally, let PY = {p : ‚ü®l1,...,l t,...,l T ‚ü©}be\nthe set of all possible exit layer permutations of the\nsequence Ywith length T. We use lt (1 ‚â§lt ‚â§L)\nto denote the exit layer of the t-th token. Then, for\na permutation p ‚ààPY, the NAR generation proba-\nbility (Eq. 2) based on LPLM can be expressed as:\nPr(Y|X) =\nT‚àè\nt=1\nPr(yt|X) =\nT‚àè\nt=1\nPr(yt|hlt\nt ), (9)\nwhere the model exits at the lt-th decoder layer and\npredicts the t-th token yt with the hidden state hlt\nt .\nPr(yt|hlt\nt ) can be computed using Eq. 8.\nDuring pre-training, for a sequence in our cor-\npora, we sample klayer permutations at each time\nand compute the output probability with Eq. 9.\nWith the layer permutation operation, each token\ncan exit at different layers, thus LPLM can effec-\ntively learn diverse token dependencies from large-\nscale corpora. Moreover, typical early exit methods\nusually need to set thresholds to estimate the exit\nlayer (Elbayad et al., 2020; Li et al., 2021c), which\nis not flexible for large-scale pre-training. Our pro-\nposed layer permutation naturally avoids to make\nexit estimations in large-scale pre-training.\n1047\nPre-training Tasks. Following BART (Lewis et al.,\n2020), our model is trained by first feeding the cor-\nrupted text to encoder and then reconstructing the\noriginal text by decoder in a NAR manner using\nthe above LPLM. We mainly adopt two useful doc-\nument corruption methods:\n‚Ä¢ Sentence Shuffling: The original text is first\ndivided into sentences according to full stops, and\nthen these sentences are randomly shuffled.\n‚Ä¢ Text Infilling: Based on the shuffled text, we\nsample 15% spans with lengths drawn from a Pois-\nson distribution (Œª= 3). Following BART (Lewis\net al., 2020), each span is replaced with a sin-\ngle ‚Äú[MASK]‚Äù token and the model can learn how\nmany tokens in a span should be predicted.\n4.3 Downstream Fine-Tuning\nOur pre-trained model can be fine-tuned for various\ndownstream text generation tasks. In fine-tuning\nphase, it becomes possible to precisely estimate the\nexit layer for each token with small-scale and task-\nspecific datasets. Here, we mainly consider two\nearly exit methods, i.e., hard and soft early exit.\nHard Early Exit. The most straightforward way is\nto calculate the exit confidence and set a threshold.\nFollowing prior work (Xin et al., 2020), we quan-\ntify the exit confidence for token prediction using\nthe entropy of the output probability distribution:\nH(yt)entropy = ‚àí\n‚àë\nPr(yt|hl\nt) ¬∑log Pr(yt|hl\nt),\n(10)\nwhere Pr(yt|hl\nt) is computed as Eq. 8. In this way, a\nlow entropy means a high confidence. Specifically,\nat the l-th off-ramp, our ELMER model will com-\npute the entropy of its output distribution H(yt)\nand then compare with a pre-defined threshold Œ¥to\ndetermine whether the model should exit here or\ncontinue to the next layer.\nSoft Early Exit. The hard early exit method only\nmakes one prediction for each token. Following\nprior work (Huang et al., 2021), the soft variant\npredicts the outputs at every decoder layer and the\nprediction is fed into the next layer for further im-\nprovement. In particular, at the l-th layer for the\nt-th position, we predict the most probable word ÀÜyl\nt\nusing the l-th off-ramp (Eq. 8) as follows:\nÀÜyl\nt = arg maxPr(yl\nt|hl\nt). (11)\nThen, we concatenate the word embedding of ÀÜyl\nt\nwith the current hidden state hl\nt and process it by a\nlinear layer as follows:\nÀúhl\nt = W[Embed(ÀÜyl\nt); hl\nt], (12)\nwhere W is a learnable matrix and Àúhl\nt is the up-\ndated hidden state of the l-th layer, which will be\ntaken as input to the next l+ 1-th layer. Compared\nwith the hard early exit, the soft early exit is able\nto calibrate the token prediction at each layer.\n4.4 Time Complexity Analysis\nSince AR and NAR models adopt the same encoder\narchitecture, the difference in time complexity is\ndominated by decoders. To generate a sequence\nwith length T, an AR decoder with Llayers has\na time complexity O(LT2) quadratic in sequence\nlength. In contrast, a NAR decoder has a linear time\ncomplexity O(LT). This is because the attention\ncomputation in NAR decoders can be parallelized\nacross all positions. While, our ELMER model gen-\nerates tokens at different layers. Let L(< L) de-\nnote the average exit layer for a sequence, the time\ncomplexity will further be decreased to O(LT).\n5 Experiments\nIn this section, we detail the experimental setup and\nthen highlight the main takeaways of our results.\n5.1 Experimental Setup\nPre-training Setup. We pre-trainELMER based on\nthe 16GB corpus (including English Wikipedia and\nBookCorpus). For our model, we use 6 layers in\nboth encoder and decoder, with a dimension of 768\nin hidden states, consistent with the base version\nof many AR and NAR pre-trained models (Lewis\net al., 2020; Qi et al., 2020, 2021). We pre-train our\nmodel from scratch with a learning rate of 2e-4 and\na minibatch size of 4096. We adopt the dictionary\nfrom BART (Lewis et al., 2020). In pre-training,\nwe share the off-ramp weights across all layers and\nsample 10 exit layer permutations for a sequence.\nFine-Tuning Datasets. Following prior work (Qi\net al., 2021), we fine-tune ELMER on three text\ngeneration tasks and datasets: (1) XSUM (Narayan\net al., 2018) is a news summarization dataset con-\ntaining 227K news article and single-sentence sum-\nmary pairs; (2) SQuAD v1.1 (Rajpurkar et al.,\n2016) is a question generation dataset, contain-\ning 98K triples of passage, question, and answer.\nWe concatenate the passage and answer as input\nand predict the question; (3) PersonaChat (Zhang\n1048\nType Models\nXSUM Latency‚Üì\n(ms/Sample)\nSQuAD v1.1 Latency‚Üì\n(ms/Sample)R-1‚Üë R-2‚Üë R-L‚Üë R-L‚Üë B-4‚Üë MT‚Üë\nAR\nTransformer 30.66 10.80 24.48 262.47 (22.0x) 29.43 4.61 9.86 159.49 (13.4x)\nMASS 39.70 17.24 31.91 196.17 (16.4x) 49.48 20.16 24.41 132.46 (11.1x)\nBART 38.79 16.16 30.61 185.19 (15.5x) 42.55 17.08 23.19 114.00 ( 9.6x)\nProphetNet 39.89 17.12 32.07 817.80 (68.5x) 48.00 19.58 23.94 456.51 (38.4x)\nSemi-NAR\nInsT 17.65 5.18 16.05 63.37 (5.3x) 29.98 2.34 8.15 67.61 (5.7x)\niNAT 26.95 6.88 22.43 31.27 (2.6x) 32.34 3.16 9.18 31.59 (2.7x)\nCMLM 29.12 7.70 23.04 113.64 (9.5x) 29.60 3.89 9.70 106.84 (9.0x)\nLevT 25.33 7.40 21.48 101.01 (8.5x) 30.81 2.68 9.40 116.41 (9.8x)\nBANG 34.71 11.71 29.16 109.77 (9.2x) 47.39 17.62 21.69 111.11 (9.3x)\nNAR\nNAT 24.04 3.88 20.32 17.47 (1.5x) 31.51 2.46 8.86 17.11 (1.4x)\niNAT 24.02 3.99 20.36 16.94 (1.4x) 32.44 2.33 8.84 16.52 (1.4x)\nCMLM 23.82 3.60 20.15 16.88 (1.4x) 31.58 2.51 8.85 16.41 (1.4x)\nLevT 24.75 4.18 20.87 27.72 (2.3x) 31.38 2.27 9.14 27.52 (2.3x)\nBANG 32.59 8.98 27.41 15.97 (1.3x) 44.07 12.75 18.99 15.69 (1.3x)\nELMER -Hard 34.54 9.78 26.08 12.39 (1.0x) 37.94 11.77 18.01 12.24 (1.0x)\nELMER -Soft 38.30 14.17 29.92 11.94 (1.0x) 40.22 13.49 20.08 11.90 (1.0x)\nTable 1: A comparison between ELMER and baselines on XSUM and SQuAD v1.1 datasets. R-1/2/L, B-4, and MT\nare short-hands for ROUGE-1/2/L, BLEU-4, and METEOR. ELMER -Hard and ELMER -Soft denote fine-tuning\nELMER with hard and soft early exit strategies, respectively. Bold and underline fonts denote the best and second\nbest methods within NAR models. Our baseline results are collected from (Liu et al., 2021) and (Qi et al., 2021).\net al., 2018) is a dialog generation dataset, contain-\ning 150K triples of persona profile, dialogue his-\ntory, and response. We concatenate the profile and\ndialogue history as input and generate the response.\nThe statistics of datasets are shown in Appendix A.\nBaselines. We compare ELMER to existing popu-\nlar AR, NAR, and Semi-NAR generation models.\nFor AR generation, we experiment with a vanilla\nTransformer model and three PLMs:\n‚Ä¢ Transformer (Vaswani et al., 2017). It is an\nAR generation model without pre-training. To date,\nTransformer has become the backbone of many text\ngeneration PLMs and our ELMER model.\n‚Ä¢ MASS (Song et al., 2019), BART (Lewis et al.,\n2020), and ProphetNet (Qi et al., 2020). These are\nthree representative PLMs for AR text generation,\nwhose pre-training objectives vary from denoising\ntext to future n-gram prediction. For a fair compar-\nison, we adopt the base version of these PLMs.\nFor NAR and Semi-NAR generation, we evalu-\nate six models with varying decoding strategies:\n‚Ä¢ NAT (Gu et al., 2018). It is the first proposed\nNAR text generation model. This baseline adds a\nmodule in the encoder to predict fertilities, acting\nas a global plan for the parallel generation.\n‚Ä¢ InsT (Stern et al., 2019). It is a Semi-NAR text\ngeneration model leveraging insertion operations.\nIt repeatedly inserts tokens at multiple locations\nbased on the partially inserted sequence.\n‚Ä¢ iNAT (Lee et al., 2018), CMLM (Ghazvinine-\njad et al., 2019), LevT (Gu et al., 2019), and\nBANG (Qi et al., 2021). These four baselines are\nboth NAR and Semi-NAR text generation models.\nAmong them, BANG is the recent state-of-the-art\nPLM for NAR text generation.\nEvaluation Metrics. To evaluate the effectiveness\nof different models, we adopt four evaluation met-\nrics: ROUGE-nassesses the text quality by com-\nputing the overlapping n-grams between the gen-\nerated and real texts (Lin, 2004); BLEU- n com-\nputes the co-occurrence ratio of n-grams between\nthe generated and real texts (Papineni et al., 2002);\nMETEOR assesses word-to-word matches between\nthe generated and real texts based on the harmonic\nmean of the unigram precision and recall (Baner-\njee and Lavie, 2005); and Distinct-nmeasures the\ndiversity degree by calculating the number of dis-\ntinct n-grams in generated texts (Li et al., 2016).\nTo examine the efficiency, we set the batch size\nas 1 at inference to calculate the per-sample in-\nference latency under the same parameter setting\nfollowing (Qi et al., 2021). More details about our\nexperiments can be found in Appendix B.\n5.2 Main Results\nTable 1 and Table 2 display the results ofELMER\nand baselines on three text generation datasets.\nFirst, ELMER -Soft outperforms NAR and Semi-\nNAR baselines on almost all datasets and metrics.\n1049\nType Models\nPersonaChat Latency‚Üì\n(ms/Sample)BLEU-1‚Üë BLEU-2‚Üë Distinct-1‚Üë Distinct-2‚Üë Overall‚Üë\nAR\nTransformer 41.56 32.95 0.30 0.80 18.90 138.31 (11.9x)\nMASS 41.06 35.75 1.40 6.90 21.28 112.13 ( 9.7x)\nBART 47.60 39.36 1.10 6.10 23.54 106.15 ( 9.2x)\nProphetNet 46.00 38.40 1.30 7.30 23.25 392.79 (33.9x)\nSemi-NAR\nInsT 12.63 9.43 0.10 0.30 5.62 65.27 (5.6x)\niNAT 41.17 32.13 0.10 1.10 18.63 43.25 (3.7x)\nCMLM 44.38 35.18 0.10 0.80 20.12 105.82 (9.1x)\nLevT 24.89 18.94 0.10 0.60 11.13 80.26 (6.9x)\nBANG 39.82 30.72 1.90 14.20 21.66 109.17 (9.4x)\nNAR\nNAT 31.53 24.17 0.10 0.80 14.15 17.86 (1.5x)\niNAT 30.56 23.38 0.10 0.70 13.69 16.40 (1.4x)\nCMLM 31.44 24.06 0.10 0.60 14.05 16.26 (1.4x)\nLevT 26.92 20.47 0.00 0.40 11.95 27.56 (2.4x)\nBANG 31.11 23.90 2.50 22.70 20.05 14.89 (1.3x)\nELMER -Hard 29.43 21.89 1.56 21.45 18.58 12.01 (1.0x)\nELMER -Soft 31.45 23.99 3.66 24.96 21.02 11.59 (1.0x)\nTable 2: A comparison between ELMER and baselines on PersonaChat with respect to automatic evaluation metrics.\nCompared to the best NAR BANG, ELMER -Soft\nachieves prominent gains in effectiveness metrics\nsuch as +5.71 ROUGE-1 in XSUM, +1.09 ME-\nTEOR in SQuAD v1.1, and +2.26 Distinct-2 in Per-\nsonaChat. These considerable gains clearly demon-\nstrate the effectiveness of our ELMER model. In\ncontrast to these NAR and Semi-NAR models, our\nmodel leverages the early exit technique to explic-\nitly model the forward and backward token depen-\ndency during parallel NAR decoding.\nSecond, ELMER consistently achieves compa-\nrable or better results than the AR baseline with-\nout pre-training, i.e., Transformer, and further nar-\nrows the performance gap between NAR and AR\nPLMs. For example, the performance gap between\nBANG and BART in XSUM and SQuAD is 3.20\nROUGE-L and 4.20 METEOR, which has now\nbeen decreased by ELMER -Soft to 0.69 and 3.11.\nELMER -Soft also obtains the best Distinct scores\nin PersonaChat. Diversity is critical for dialogue\ngeneration to avoid boring or useless responses. Be-\nsides, the lower BLEU scores than NAT probably\nbecause the diverse generated texts byELMER -Soft\nmakes a reasonable difference from real texts. For\nthose models without pre-training such as CMLM\nand NAT, they tend to generate highly frequent\nwords and common phrases such as ‚Äú I, an, the ‚Äù,\nwhich share much overlapped and repetitive parts\nwith the target output. By contrast, our pre-trained\nmodel generates diverse responses, which is more\ncritical to avoid boring or useless responses in dia-\nlogue generation.\nFinally, in terms of efficiency, ELMER achieves\nModels\nXSUM\nROUGE-1 ROUGE-2 ROUGE-L\nNAR LevT 24.75 4.18 20.87\nELMER -Hard 34.54 9.78 26.08\n-w/o pre-training 28.56 5.33 21.96\nELMER -Soft 38.30 14.17 29.92\n-w/o pre-training 30.45 7.37 24.00\nTable 3: Ablation study on XSUM dataset.\nmuch faster inference speed than all NAR and AR\nmodels, w.r.t. the per sample inference latency. For\nexample, the inference latency of AR Transformer\nmodel in XSUM, SQuAD v1.1, and PersonaChat\nare 262.47ms, 159.49ms, and 138.31ms per sample,\nwhile those of ELMER -Soft are 11.94ms, 11.90ms,\nand 11.59ms, which amount to 22.0, 13.4, and 11.9\ntimes speedup. Compared to AR PLMs, we achieve\nover 10x inference speedup, especially the highest\n68.5x speedup in XSUM. Moreover, ELMER -Soft\nis slightly faster than other NAR baselines. We\nspeculate that this speedup is mainly due to the fact\nthat prior NAR models require to predict the output\nlength such as NAT or rely on multiple insertion\nand deletion operations such as LevT.\n5.3 Detailed Analysis\nWe report detailed analysis of our model on XSUM\ndataset ‚Äì we have similar findings on other datasets.\nAblation Study. Our ELMER model is the first\none to adopt the early exit technique to conduct\nNAR text generation. To learn more diverse token\n1050\ndependencies, we proposed the LPLM objective\n(Sec. 4.2) based on early exit to pre-train ELMER .\nHere, we evaluate the effects of early exit and large-\nscale LPLM training by directly applying the hard\nand soft early exit methods to NAR text generation\nwithout pre-training. We compare the early-exit\nvariants with the best non-pretrained NAR model\nin XSUM, i.e., LevT. The results are shown in Ta-\nble 3. We can observe that these two early-exit\nvariants without pre-training perform better than\nLevT. This indicates that introducing the early exit\nmechanism can improve NAR text generation qual-\nity due to the explicit modeling of the target-side\ndependency. Moreover, our proposed large-scale\npre-training based on the LPLM objective improves\nNAR results significantly and consistently in both\nearly exit strategies. By utilizing the general LPLM,\nour model can capture diverse token dependencies\nfrom large-scale corpora and further adapt to down-\nstream tasks with specific early exit methods.\nParameter Sensitivity Analysis. In ELMER -Hard,\nthe pre-defined entropy threshold Œ¥ is critical to\ndetermine at which layer our model will exit. Here,\nwe further examine the model performance and the\nfraction of tokens exiting at every layers (1 ‚àº6) by\nvarying the exit threshold in the set {0.0, 0.25, 0.5,\n0.75, 1.0}. As the threshold Œ¥increases gradually,\nmore tokens exit earlier. From the experiment, we\nfind that low and high thresholds lead to reverse\nearly exit situations shown by the fraction of tokens\nexiting at each layer. Therefore, we only present\nthe results for two representative thresholds (0.5\nand 1.0) in Figure 2. We can observe that: (1) when\nŒ¥= 0.5, a larger fraction of tokens are generated in\nhigher layers (‚â•3), which means a lower threshold\nneeds more computation to achieve enough confi-\ndence. These upper-layer ‚Äúdifficult‚Äù tokens can\ndepend on simple tokens such as ‚ÄúI‚Äù, ‚Äúthe‚Äù, ‚Äúhave‚Äù\ngenerated at lower layers. (2) when Œ¥ = 1.0, it\nshows an opposite trend and the performance drops\nslightly. The reason might be that 1.0 is so high\nthat the model makes a precipitate exiting decision.\nFinally, according to the model performance on the\nvalidation set, we set the threshold Œ¥as 0.5 in our\nexperiments.\n5.4 Human Evaluation\nDespite the effectiveness of automatic evaluation,\nhuman evaluation remains critical for text genera-\ntion (Celikyilmaz et al., 2020). Since human eval-\nuation is expensive, we only focus on comparing\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019\n/uni00000013\n/uni00000018/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000029/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni00000014/uni00000020/uni00000016/uni00000017/uni00000011/uni00000018/uni00000017\n/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni00000015/uni00000020/uni0000001c/uni00000011/uni0000001a/uni0000001b\n/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni0000002f/uni00000020/uni00000015/uni00000019/uni00000011/uni00000013/uni0000001b\n(a) Œ¥= 0.5\n/uni00000014/uni00000015/uni00000016/uni00000017/uni00000018/uni00000019\n/uni00000013\n/uni00000018/uni00000013\n/uni00000014/uni00000013/uni00000013/uni00000029/uni00000055/uni00000044/uni00000046/uni00000057/uni0000004c/uni00000052/uni00000051/uni00000003/uni00000052/uni00000049/uni00000003/uni00000057/uni00000052/uni0000004e/uni00000048/uni00000051/uni00000056/uni00000003/uni0000000b/uni00000008/uni0000000c\n/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni00000014/uni00000020/uni00000016/uni00000013/uni00000011/uni00000016/uni00000016\n/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni00000015/uni00000020/uni0000001a/uni00000011/uni00000019/uni0000001c\n/uni00000035/uni00000032/uni00000038/uni0000002a/uni00000028/uni00000010/uni0000002f/uni00000020/uni00000015/uni00000017/uni00000011/uni00000016/uni00000017 (b) Œ¥= 1.0\nFigure 2: Varying Œ¥for ELMER -Hard on XSUM.\nour best performing ELMER -Soft with two AR and\nNAR PLMs, i.e., BART and BANG.\nIn order to reduce the variance caused by human,\nthree workers were asked to score texts from four\naspects (Li et al., 2022a), i.e., fluency, informative-\nness, accuracy, and relevance. Fluency evaluates\nwhether the text is well-formed and logical to read;\nInformativeness measures whether the text contains\nuseful information; Accuracy tests whether the text\ndescribes the given content accurately; Relevance\nmeasures whether the text is relevant to the given\ncontext. These four scores are rated from 1 to 5. We\nfurther design a Turing test (Turing, 1950) where a\nhuman judge is asked to detect whether the given\ntext is generated by a human. For each method,\nwe average the scores from three human judges\nand then report the average results over 500 texts.\nFrom the results in Table 4, we can see that our\nmodel is better than NAR model BANG in terms of\nfluency (3.99 vs 3.42) and relevance (3.63 vs 3.38).\nThe major reason is that our model leverages the\nearly exit to explicitly model the token dependency,\nwhich could reduce token repetitions and improves\nthe fluency of texts. While, our model achieves\na slightly worse fluency score than the AR model\nBART. We speculate that BART can produce some\ncommon phrases by using the AR generation mode.\nWe also present some generated examples by\nour model in Appendix C. It can be observe that\nour model can generate some common phrases oc-\ncurred in real texts such as ‚Äúsurface-to-air‚Äù, ‚ÄúSouth\nChina Sea‚Äù, and ‚Äúmain seminary‚Äù. These phrases\ncan improve the fluency of NAR generated texts.\nBy incorporating early exit and generating tokens at\ndifferent layers, our model can explicitly learn the\nforward and backward token dependency, which\ncan deal with the multi-modality issue to some\nextent. While, as a NAR model, our model still\ninevitably generates some repetitive stop tokens\nsuch as ‚Äúon on on‚Äù, which can be further improved\nin future work.\n1051\nModels\nXSUM\nTT (%) Flu. Info. Acc. Rel.\nBART 51.01 4.11 3.99 3.64 3.79\nBANG 45.34 3.42 3.45 3.46 3.38\nELMER -Soft 48.90 3.99 3.52 3.49 3.63\nGold 55.67 4.12 4.29 3.95 4.05\nTable 4: Turing test (TT) and human scores on XSUM.\nFlu., Info., Acc. and Rel. denote fluency, informative-\nness, accuracy and relevance respectively.\n6 Conclusion\nThis paper presented an efficient and effective PLM\nfor NAR text generation, called ELMER . ELMER\nintroduced a token-level early exit mechanism to\nexplicitly model the semantic dependency between\ntarget tokens during parallel decoding. Moreover,\nwe proposed a pre-training objective, Layer Permu-\ntation Language Modeling, to pre-train ELMER on\nlarge-scale corpora by permuting the exit layer for\neach token in a sequence. Experiments on text sum-\nmarization, question generation, and dialogue gen-\neration demonstrate that our model can effectively\nimprove the NAR generation quality compared to\nhighly competitive NAR models and further nar-\nrow the performance gap with AR models while\nachieving higher inference efficiency. In future\nwork, we will investigate the efficacy of utilizing\nthe proposed model to iteratively generate texts and\nconsider more early exit strategies in fine-tuning.\n7 Limitations\nAn important limitation of ELMER compared with\nother NAR text generation models is the need for\ndefining an appropriate early exit strategy. In pre-\ntraining, ELMER utilizes a general strategy, i.e.,\nlayer permutation language modeling, by permut-\ning the exit layer for each token in a sequence.\nHowever, to apply ELMER to downstream specific\ntasks and datasets, we need to design an effective\nand suitable estimation method to decide at which\nlayer the model will exit and predict tokens. Also,\nas a PLM, ELMER may present biases learned from\nthe pre-training corpus in the output text.\nIn this study, we evaluate ELMER on three text\ngeneration tasks and datasets, but the output length\nof these tasks is relatively short. We should deal\nwith long-form text generation which raises more\nchallenges to the NAR paradigm.\n8 Ethical Concerns\nCurrent NAR text generation techniques achieve\nfaster inference speed but suffer from several issues\nlike multi-modality, lack of accuracy to the input,\ncommonsense issues etc., which makes their online\nreal-time deployment difficult. ELMER is an effort\nat rectifying some of these issues, with a focus of\nmodeling the semantic dependency between target\ntokens to improve NAR generation quality. How-\never, compared to AR models,ELMER outputs con-\ntinue to mix multiple candidates and repeat words\nat times. This should be strongly considered before\nany direct deployment of real-world systems.\nOn the other hand, the text generation technol-\nogy may be potentially misused for harmful appli-\ncations. When deploying ELMER to online real-\ntime platforms, the high-quality text generated by\nour work also makes it difficult to distinguish syn-\nthetic text from human-written text, such as fake\nnews and stories. It is somewhat difficult to an-\nticipate the harmful usages of our method since\nthey often involve repurposing our model in a to-\ntally different setting or for an unexpected purpose\nthan we planned. To alleviate this problem, we\ncan ask for help from some classic security risk\nassessment frameworks such as detecting threats\nand potential impacts, measuring likelihood, and\ndetermining risk as a combination of likelihood and\nimpact (Blank, 2011).\nAcknowledgement\nThis work was partially supported by Beijing Natu-\nral Science Foundation under Grant No. 4222027,\nand Beijing Outstanding Young Scientist Program\nunder Grant No. BJJWZYJH012019100020098,\nand the Outstanding Innovative Talents Cultivation\nFunded Programs 2021 of Renmin University of\nChina. Xin Zhao is the corresponding author.\nReproducibility\nFor reproducing and reusing our work, we release\nthe pre-trained ELMER model and source codes at\nthe link: https://github.com/RUCAIBox/ELMER.\nIn the future, we will integrate our model into Hug-\nging Face (Wolf et al., 2020) and TextBox (Li et al.,\n2021a) libraries for easy-to-use. We hope that these\nopen-source resources will facilitate and contribute\nto the advancement of related research.\n1052\nReferences\nSatanjeev Banerjee and Alon Lavie. 2005. METEOR:\nan automatic metric for MT evaluation with improved\ncorrelation with human judgments. In Proceedings\nof the Workshop on Intrinsic and Extrinsic Evalua-\ntion Measures for Machine Translation and/or Sum-\nmarization@ACL 2005, Ann Arbor, Michigan, USA,\nJune 29, 2005, pages 65‚Äì72. Association for Compu-\ntational Linguistics.\nRebecca M Blank. 2011. Guide for conducting risk\nassessments.\nTom B Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. arXiv preprint arXiv:2005.14165.\nAsli Celikyilmaz, Elizabeth Clark, and Jianfeng Gao.\n2020. Evaluation of text generation: A survey.\nCoRR, abs/2006.14799.\nMaha Elbayad, Jiatao Gu, Edouard Grave, and Michael\nAuli. 2020. Depth-adaptive transformer. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nMarjan Ghazvininejad, Omer Levy, Yinhan Liu, and\nLuke Zettlemoyer. 2019. Mask-predict: Parallel\ndecoding of conditional masked language models.\nIn Proceedings of the 2019 Conference on Empiri-\ncal Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural\nLanguage Processing, EMNLP-IJCNLP 2019, Hong\nKong, China, November 3-7, 2019, pages 6111‚Äì6120.\nAssociation for Computational Linguistics.\nJiatao Gu, James Bradbury, Caiming Xiong, Victor O. K.\nLi, and Richard Socher. 2018. Non-autoregressive\nneural machine translation. In 6th International Con-\nference on Learning Representations, ICLR 2018,\nVancouver, BC, Canada, April 30 - May 3, 2018,\nConference Track Proceedings. OpenReview.net.\nJiatao Gu, Changhan Wang, and Junbo Zhao. 2019.\nLevenshtein transformer. In Advances in Neural\nInformation Processing Systems 32: Annual Con-\nference on Neural Information Processing Systems\n2019, NeurIPS 2019, December 8-14, 2019, Vancou-\nver, BC, Canada, pages 11179‚Äì11189.\nJunliang Guo, Xu Tan, Di He, Tao Qin, Linli Xu, and\nTie-Yan Liu. 2019. Non-autoregressive neural ma-\nchine translation with enhanced decoder input. In\nThe Thirty-Third AAAI Conference on Artificial Intel-\nligence, AAAI 2019, The Thirty-First Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2019, The Ninth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2019, Hon-\nolulu, Hawaii, USA, January 27 - February 1, 2019,\npages 3723‚Äì3730. AAAI Press.\nChenyang Huang, Hao Zhou, Osmar R. Za√Øane, Lili\nMou, and Lei Li. 2021. Non-autoregressive transla-\ntion with layer-wise prediction and deep supervision.\nCoRR, abs/2110.07515.\nTing Jiang, Shaohan Huang, Zihan Zhang, Deqing\nWang, Fuzhen Zhuang, Furu Wei, Haizhen Huang,\nLiangjie Zhang, and Qi Zhang. 2021. Improving\nnon-autoregressive generation with mixup training.\nCoRR, abs/2110.11115.\nJason Lee, Elman Mansimov, and Kyunghyun Cho.\n2018. Deterministic non-autoregressive neural se-\nquence modeling by iterative refinement. In Proceed-\nings of the 2018 Conference on Empirical Methods\nin Natural Language Processing, Brussels, Belgium,\nOctober 31 - November 4, 2018, pages 1173‚Äì1182.\nAssociation for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Marjan\nGhazvininejad, Abdelrahman Mohamed, Omer Levy,\nVeselin Stoyanov, and Luke Zettlemoyer. 2020.\nBART: denoising sequence-to-sequence pre-training\nfor natural language generation, translation, and com-\nprehension. In Proceedings of the 58th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2020, Online, July 5-10, 2020, pages 7871‚Äì7880.\nAssociation for Computational Linguistics.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nNAACL HLT 2016, The 2016 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nSan Diego California, USA, June 12-17, 2016, pages\n110‚Äì119. The Association for Computational Lin-\nguistics.\nJunyi Li, Tianyi Tang, Zheng Gong, Lixin Yang, Zhuo-\nhao Yu, Zhipeng Chen, Jingyuan Wang, Xin Zhao,\nand Ji-Rong Wen. 2022a. Eliteplm: An empirical\nstudy on general language ability evaluation of pre-\ntrained language models. In Proceedings of the 2022\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022 , pages 3519‚Äì3539.\nAssociation for Computational Linguistics.\nJunyi Li, Tianyi Tang, Gaole He, Jinhao Jiang, Xiaox-\nuan Hu, Puzhao Xie, Zhipeng Chen, Zhuohao Yu,\nWayne Xin Zhao, and Ji-Rong Wen. 2021a. Textbox:\nA unified, modularized, and extensible framework for\ntext generation. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing: System Demon-\nstrations, pages 30‚Äì39.\nJunyi Li, Tianyi Tang, Jian-Yun Nie, Ji-Rong Wen, and\nXin Zhao. 2022b. Learning to transfer prompts for\ntext generation. In Proceedings of the 2022 Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\n1053\nLanguage Technologies, NAACL 2022, Seattle, WA,\nUnited States, July 10-15, 2022 , pages 3506‚Äì3518.\nAssociation for Computational Linguistics.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie,\nand Ji-Rong Wen. 2022c. A survey of pretrained\nlanguage models based text generation. CoRR,\nabs/2201.05273.\nJunyi Li, Tianyi Tang, Wayne Xin Zhao, and Ji-Rong\nWen. 2021b. Pretrained language model for text gen-\neration: A survey. In Proceedings of the Thirtieth\nInternational Joint Conference on Artificial Intelli-\ngence, IJCAI 2021, Virtual Event / Montreal, Canada,\n19-27 August 2021, pages 4492‚Äì4499. ijcai.org.\nXiaonan Li, Yunfan Shao, Tianxiang Sun, Hang Yan,\nXipeng Qiu, and Xuanjing Huang. 2021c. Accelerat-\ning BERT inference for sequence labeling via early-\nexit. In Proceedings of the 59th Annual Meeting of\nthe Association for Computational Linguistics and\nthe 11th International Joint Conference on Natural\nLanguage Processing, ACL/IJCNLP 2021, (Volume 1:\nLong Papers), Virtual Event, August 1-6, 2021, pages\n189‚Äì199. Association for Computational Linguistics.\nKaiyuan Liao, Yi Zhang, Xuancheng Ren, Qi Su,\nXu Sun, and Bin He. 2021. A global past-future\nearly exit method for accelerating inference of pre-\ntrained language models. In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, NAACL-HLT 2021, Online,\nJune 6-11, 2021, pages 2013‚Äì2023. Association for\nComputational Linguistics.\nJindrich Libovick√Ω and Jindrich Helcl. 2018. End-to-\nend non-autoregressive neural machine translation\nwith connectionist temporal classification. In Pro-\nceedings of the 2018 Conference on Empirical Meth-\nods in Natural Language Processing, Brussels, Bel-\ngium, October 31 - November 4, 2018, pages 3016‚Äì\n3021. Association for Computational Linguistics.\nChin-Yew Lin. 2004. Rouge: A package for automatic\nevaluation of summaries. In Text summarization\nbranches out, pages 74‚Äì81.\nDayiheng Liu, Yu Yan, Yeyun Gong, Weizhen Qi, Hang\nZhang, Jian Jiao, Weizhu Chen, Jie Fu, Linjun Shou,\nMing Gong, Pengcheng Wang, Jiusheng Chen, Daxin\nJiang, Jiancheng Lv, Ruofei Zhang, Winnie Wu,\nMing Zhou, and Nan Duan. 2021. GLGE: A new\ngeneral language generation evaluation benchmark.\nIn Findings of the Association for Computational Lin-\nguistics: ACL/IJCNLP 2021, Online Event, August\n1-6, 2021, volume ACL/IJCNLP 2021 of Findings of\nACL, pages 408‚Äì420. Association for Computational\nLinguistics.\nXuezhe Ma, Chunting Zhou, Xian Li, Graham Neu-\nbig, and Eduard H. Hovy. 2019. Flowseq: Non-\nautoregressive conditional sequence generation with\ngenerative flow. In Proceedings of the 2019 Confer-\nence on Empirical Methods in Natural Language Pro-\ncessing and the 9th International Joint Conference\non Natural Language Processing, EMNLP-IJCNLP\n2019, Hong Kong, China, November 3-7, 2019, pages\n4281‚Äì4291. Association for Computational Linguis-\ntics.\nShashi Narayan, Shay B. Cohen, and Mirella Lapata.\n2018. Don‚Äôt give me the details, just the summary!\ntopic-aware convolutional neural networks for ex-\ntreme summarization. In Proceedings of the 2018\nConference on Empirical Methods in Natural Lan-\nguage Processing, Brussels, Belgium, October 31 -\nNovember 4, 2018 , pages 1797‚Äì1807. Association\nfor Computational Linguistics.\nKishore Papineni, Salim Roukos, Todd Ward, and Wei-\nJing Zhu. 2002. Bleu: a method for automatic evalu-\nation of machine translation. In Proceedings of the\n40th annual meeting of the Association for Computa-\ntional Linguistics, pages 311‚Äì318.\nWeizhen Qi, Yeyun Gong, Jian Jiao, Yu Yan, Weizhu\nChen, Dayiheng Liu, Kewen Tang, Houqiang Li,\nJiusheng Chen, Ruofei Zhang, Ming Zhou, and Nan\nDuan. 2021. BANG: bridging autoregressive and\nnon-autoregressive generation with large scale pre-\ntraining. In Proceedings of the 38th International\nConference on Machine Learning, ICML 2021, 18-24\nJuly 2021, Virtual Event, volume 139 of Proceedings\nof Machine Learning Research , pages 8630‚Äì8639.\nPMLR.\nWeizhen Qi, Yu Yan, Yeyun Gong, Dayiheng Liu, Nan\nDuan, Jiusheng Chen, Ruofei Zhang, and Ming Zhou.\n2020. Prophetnet: Predicting future n-gram for\nsequence-to-sequence pre-training. In Proceedings\nof the 2020 Conference on Empirical Methods in Nat-\nural Language Processing: Findings, EMNLP 2020,\nOnline Event, 16-20 November 2020 , pages 2401‚Äì\n2410. Association for Computational Linguistics.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21:140:1‚Äì140:67.\nPranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and\nPercy Liang. 2016. Squad: 100, 000+ questions\nfor machine comprehension of text. In Proceedings\nof the 2016 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2016, Austin,\nTexas, USA, November 1-4, 2016, pages 2383‚Äì2392.\nThe Association for Computational Linguistics.\nChitwan Saharia, William Chan, Saurabh Saxena, and\nMohammad Norouzi. 2020. Non-autoregressive ma-\nchine translation with latent alignments. In Proceed-\nings of the 2020 Conference on Empirical Methods in\nNatural Language Processing, EMNLP 2020, Online,\nNovember 16-20, 2020, pages 1098‚Äì1108. Associa-\ntion for Computational Linguistics.\n1054\nKaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and Tie-\nYan Liu. 2019. MASS: masked sequence to sequence\npre-training for language generation. In Proceedings\nof the 36th International Conference on Machine\nLearning, ICML 2019, 9-15 June 2019, Long Beach,\nCalifornia, USA, volume 97 of Proceedings of Ma-\nchine Learning Research, pages 5926‚Äì5936. PMLR.\nMitchell Stern, William Chan, Jamie Kiros, and Jakob\nUszkoreit. 2019. Insertion transformer: Flexible se-\nquence generation via insertion operations. In Pro-\nceedings of the 36th International Conference on Ma-\nchine Learning, ICML 2019, 9-15 June 2019, Long\nBeach, California, USA, volume 97 of Proceedings\nof Machine Learning Research , pages 5976‚Äì5985.\nPMLR.\nYixuan Su, Deng Cai, Yan Wang, David Vandyke, Si-\nmon Baker, Piji Li, and Nigel Collier. 2021. Non-\nautoregressive text generation with pre-trained lan-\nguage models. In Proceedings of the 16th Conference\nof the European Chapter of the Association for Com-\nputational Linguistics: Main Volume, EACL 2021,\nOnline, April 19 - 23, 2021, pages 234‚Äì243. Associa-\ntion for Computational Linguistics.\nTianyi Tang, Junyi Li, Wayne Xin Zhao, and Ji-\nRong Wen. 2022. MVP: multi-task supervised pre-\ntraining for natural language generation. CoRR,\nabs/2206.12131.\nAlan M. Turing. 1950. Computing machinery and intel-\nligence. Mind, LIX(236):433‚Äì460.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In Advances in Neural Information Pro-\ncessing Systems 30: Annual Conference on Neural\nInformation Processing Systems 2017, December 4-9,\n2017, Long Beach, CA, USA, pages 5998‚Äì6008.\nYiren Wang, Fei Tian, Di He, Tao Qin, ChengXiang\nZhai, and Tie-Yan Liu. 2019. Non-autoregressive\nmachine translation with auxiliary regularization. In\nThe Thirty-Third AAAI Conference on Artificial Intel-\nligence, AAAI 2019, The Thirty-First Innovative Ap-\nplications of Artificial Intelligence Conference, IAAI\n2019, The Ninth AAAI Symposium on Educational\nAdvances in Artificial Intelligence, EAAI 2019, Hon-\nolulu, Hawaii, USA, January 27 - February 1, 2019,\npages 5377‚Äì5384. AAAI Press.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, R√©mi Louf, Morgan Funtowicz,\nJoe Davison, Sam Shleifer, Patrick von Platen, Clara\nMa, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le\nScao, Sylvain Gugger, Mariama Drame, Quentin\nLhoest, and Alexander M. Rush. 2020. Transformers:\nState-of-the-art natural language processing. In Pro-\nceedings of the 2020 Conference on Empirical Meth-\nods in Natural Language Processing: System Demon-\nstrations, EMNLP 2020 - Demos, Online, November\n16-20, 2020, pages 38‚Äì45. Association for Computa-\ntional Linguistics.\nJi Xin, Raphael Tang, Jaejun Lee, Yaoliang Yu, and\nJimmy Lin. 2020. Deebert: Dynamic early exiting\nfor accelerating BERT inference. In Proceedings of\nthe 58th Annual Meeting of the Association for Com-\nputational Linguistics, ACL 2020, Online, July 5-10,\n2020, pages 2246‚Äì2251. Association for Computa-\ntional Linguistics.\nZhilin Yang, Zihang Dai, Yiming Yang, Jaime G. Car-\nbonell, Ruslan Salakhutdinov, and Quoc V . Le. 2019.\nXlnet: Generalized autoregressive pretraining for\nlanguage understanding. In Advances in Neural In-\nformation Processing Systems 32: Annual Confer-\nence on Neural Information Processing Systems 2019,\nNeurIPS 2019, December 8-14, 2019, Vancouver, BC,\nCanada, pages 5754‚Äì5764.\nYan Zeng and Jian-Yun Nie. 2021. An investigation\nof suitability of pre-trained language models for dia-\nlogue generation - avoiding discrepancies. In Find-\nings of the Association for Computational Linguis-\ntics: ACL/IJCNLP 2021, Online Event, August 1-6,\n2021, volume ACL/IJCNLP 2021 ofFindings of ACL,\npages 4481‚Äì4494. Association for Computational\nLinguistics.\nJiaao Zhan, Qian Chen, Boxing Chen, Wen Wang,\nYu Bai, and Yang Gao. 2022. Non-autoregressive\ntranslation with dependency-aware decoder. CoRR,\nabs/2203.16266.\nSaizheng Zhang, Emily Dinan, Jack Urbanek, Arthur\nSzlam, Douwe Kiela, and Jason Weston. 2018. Per-\nsonalizing dialogue agents: I have a dog, do you have\npets too? In Proceedings of the 56th Annual Meet-\ning of the Association for Computational Linguistics,\nACL 2018, Melbourne, Australia, July 15-20, 2018,\nVolume 1: Long Papers, pages 2204‚Äì2213. Associa-\ntion for Computational Linguistics.\n1055\nAppendix\nWe provide some experiment-related information\nas supplementary materials. The appendix is orga-\nnized into three sections:\n‚Ä¢ Statistics of each dataset are presented in Ap-\npendix A;\n‚Ä¢ Training settings of baselines and our model\nELMER are presented in Appendix B;\n‚Ä¢ Generated examples by our model are pre-\nsented in Appendix C.\nA Statistics of Datasets\nThe detailed information of these three datasets is\nlisted in Table 5.\nDataset #Train #Valid #Test #Output\nXSUM 204,045 11,332 11,334 21.1\nSQuAD v1.1 75,722 10,570 11,877 11.6\nPersonaChat 122,499 14,602 14,056 11.9\nTable 5: Statistics of three datasets. #Output denotes\nthe average number of tokens in the output texts.\nB Experimental Details\nFor AR baselines, we adopt the hyper-parameters:\nlearning rate 2e-5, batch size 20, Adam optimizer,\nand the maximum input and output length of 512.\nWe fine-tune these models on each dataset for 50\nepochs and select the best model. For NAR and\nSemi-NAR baselines, the hyper-parameters are\nthe same as AR baselines except the number of\nfine-tuning epochs, since NAR models need more\nepochs to converge (Qi et al., 2021). We fine-tune\nthese baselines and ELMER for 50 epochs and save\ncheckpoints for every epoch. We select the best\ncheckpoint based on the performance on valida-\ntion set. Following BANG (Qi et al., 2021), the\ndifference between semi-NAR and NAR models\nlies in that: (1) we select the outputs from the\nfirst iteration as the NAR outputs, since the first\niteration only depends on the input text; (2) we\nselect the outputs from the maximum (we set as\n10) iteration as the semi-NAR outputs, since each\niteration depends on both the input text and the last\ngenerated texts. Most settings and results are col-\nlected from GLGE (Liu et al., 2021) and BANG (Qi\net al., 2021). To make a fair comparison with non-\npretrained baselines, we also compare ELMER with\nthe best NAR non-pretrained baseline LevT in ab-\nlation analysis (Table 3).\nC Case Study\nWe show some qualitative examples of these three\ndatasets in Table 6, Table 7, and Table 8.\nDue to the explicit modeling of the bi-direction\ntoken dependency, our model can generate some\ncommon phrases occurred in real texts such as\n‚Äúsurface-to-air‚Äù, ‚ÄúSouth China Sea‚Äù, and ‚Äúmain sem-\ninary‚Äù. These phrases can improve the fluency of\nNAR generated texts. But, as a NAR model, our\nmodel still inevitably generates some repetitive to-\nkens such as ‚Äúon on on‚Äù. The generated exam-\nples in PersonaChat also explain that our model\ngenerates more diverse responses, which are very\ndifferent from the real responses.\n1056\nNews Reports on Wednesday suggested more than one\niguana was actually filmed, with scenes then\nstitched together. But the BBC has said only one\nanimal was chased by the snakes - with other igua-\nnas only filmed for close-ups. The scene quickly\nwent viral when it was aired last year and later\nwon a Bafta for must-see moment. The iguana\nhatchling, filmed in the Galapagos, eventually got\naway - much to viewers‚Äô relief. The Daily Mail\nclaimed the episode was embroiled in a \"fakery\nrow\" after producer Elizabeth White told the Me-\ndia Production Show: \"It wasn‚Äôt the same iguana,\nno, and often we have to augment it with other\nclips. \"Unfortunately lizards, snakes and iguanas\naren‚Äôt good at ‚Äôtakes‚Äô.\" But the BBC defended the\nSir David Attenborough-fronted programme, with\na spokeswoman saying: \"The BBC strongly refutes\nany suggestion that the award-winning iguana v\nsnakes sequence was ‚Äôfaked‚Äô. \"The final iguana\nchase in which one iguana escapes the snakes was\n- unusually for natural history filming - shot using\ntwo cameras, allowing us to follow both the individ-\nual iguana and the snakes‚Äô point of view. \"What was\ncaptured in the field was extraordinary animal be-\nhaviour which had never been witnessed or filmed\nbefore.\" She added: \"As is common in natural his-\ntory film-making, pick-up shots were filmed sepa-\nrately - for example close-ups of iguana eyes - to\nmake the story of the sequence as clear as possible\nfor the audience. \"This is absolutely in keeping\nwith the norms of natural history film-making - and\nabsolutely in line with the BBC‚Äôs editorial policy\nguidelines, and was a true representation of animal\nbehaviour.\"\nSatellite images taken on 14 February appear to\nshow two batteries of eight missile launchers and a\nradar system on Woody or Yongxing Island in the\nParacels. The presence of missiles would signifi-\ncantly increase tensions in the acrimonious South\nChina Sea dispute. China‚Äôs Foreign Minister Wang\nYi said reports were a Western media invention.\nBut Mr Wang defended \"the limited and necessary\nself-defence facilities\" on islands inhabited by Chi-\nnese personnel as \"consistent with the right for self-\npreservation and self-protection.... under the inter-\nnational law\". Asked about the reports, US Secre-\ntary of State John Kerry attacked China‚Äôs increased\n\"militarisation\" of the contested region, saying it\nwas a \"serious concern\". Taiwan‚Äôs defence ministry\nsaid it had \"learned of an air defence missile sys-\ntem deployed\" by the Chinese on Woody Island.\nIt would not say how many missiles had been de-\nployed or when, but told the BBC they would be\ncapable of targeting civilian and military aircraft.\nThe commander of the US Pacific Fleet confirmed\nthe deployment to Reuters news agency. Adm Harry\nHarris said such a move would be \"a militarisation\nof the South China Sea in ways\" China‚Äôs President\nXi Jinping had pledged not to make. Japan‚Äôs Chief\nCabinet Secretary Yoshihide Suga said there were\n\"serious concerns\" over China‚Äôs \"unilateral move to\nchange the status quo\" in the region, and \"we cannot\naccept this fact\". China has been carrying out ex-\ntensive land reclamation work in the region, which\nit says is legal and for civilian purposes. But the\nwork has angered other countries which also claim\nthe territory, and there is growing concern about the\nimplications of the area becoming militarised.\nSummaries The BBC has denied claims award-winning series\nPlanet Earth II faked a nail-biting scene showing a\nbaby iguana being chased by racer snakes .\nChina has deployed surface-to-air missiles on a dis-\nputed island in the South China Sea, Taiwan says .\nELMER -Hard The BBC has defended claims the BBC nature doc-\numentary of the iguana footage of a \"faked storm \"\nby a Davidborough.\nChina has confirmed that new reports of on surface-\nair on on on of in the South China Sea .\nELMER -Soft The BBC has defended claims a BBC documentary\nabout a series of from natural drama Sir David Atten\nwas \"faked\".\nChina is the first of of air defence missiles on a\ndisputed disputed island of the South China Sea,\naccording to US\nTable 6: Qualitative examples on XSUM dataset.\n1057\nParagraphs Moreau Seminary [SEP] The university is the ma-\njor seat of the Congregation of Holy Cross ( albeit\nnot its official headquarters , which are in Rome ) .\nIts main seminary , Moreau Seminary , is located\non the campus across St . Joseph lake from the\nMain Building . Old College , the oldest building\non campus and located near the shore of St . Mary\nlake , houses undergraduate seminarians . Retired\npriests and brothers reside in Fatima House ( a for-\nmer retreat center ) , Holy Cross House , as well\nas Columba Hall near the Grotto . The university\nthrough the Moreau Seminary has ties to theologian\nFrederick Buechner . While not Catholic , Buechner\nhas praised writers from Notre Dame and Moreau\nSeminary created a Buechner Prize for Preaching .\neight [SEP] The College of Engineering was estab-\nlished in 1920 , however , early courses in civil and\nmechanical engineering were a part of the College\nof Science since the 1870s . Today the college ,\nhoused in the Fitzpatrick , Cushing , and Stinson -\nRemick Halls of Engineering , includes five depart-\nments of study ? @ S aerospace and mechanical\nengineering , chemical and biomolecular engineer-\ning , civil engineering and geological sciences ,\ncomputer science and engineering , and electrical\nengineering ? @ S with eight B . S . degrees of-\nfered . Additionally , the college offers five - year\ndual degree programs with the Colleges of Arts and\nLetters and of Business awarding additional B . A\n. and Master of Business Administration ( MBA )\ndegrees , respectively .\nQuestions What is the primary seminary of the Congregation\nof the Holy Cross ?\nHow many BS level degrees are offered in the Col-\nlege of Engineering at Notre Dame ?\nELMER -Hard What is the name of Frederick Binaryinary at St.\nJoseph ?\nHow many B.S degrees offered at the College of\nthe engineering ?\nELMER -Soft What is the name of the main seminary at St. Joseph\n?\nHow many B.S. degrees offered at the College of\ndepartments ?\nTable 7: Qualitative examples on SQuAD v1.1 dataset.\nHistories i love to meet new people . i have a turtle named\ntimothy . my favorite sport is ultimate frisbee . my\nparents are living in bora bora . autumn is my favorite\nseason . [SEP] hello , how are you doing tonight ?\ni am well an loving this interaction how are you ? i\nam great . i just got back from the club .\ni just bought a brand new house . i like to dance at\nthe club . i run a dog obedience school . i have a\nbig sweet tooth . i like taking and posting selkies .\n[SEP] hello , how are you doing tonight ? i am well\nan loving this interaction how are you ? i am great . i\njust got back from the club . this is my favorite time\nof the year season wise i would rather eat chocolate\ncake during this season . what club did you go to ?\nme an timothy watched tv i went to club chino . what\nshow are you watching ? lol oh okay kind of random\ndo you live in a house or apartment ? we watched a\nshow about animals like him i love those shows . i\nam really craving cake . why does that matter any ? i\nwent outdoors to play frisbee\nReponses this is my favorite time of the year season wise it matters because i have a sweet tooth .\nELMER -Hard what team of do you do ? ultimate i like to exercise with my dogs .\nELMER -Soft i love club! do you have new friends ? cool. i spend time with my dog .\nTable 8: Qualitative examples on PersonaChat dataset.\n1058"
}