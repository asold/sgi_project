{
    "title": "Transformer-Based Models for Question Answering on COVID19",
    "url": "https://openalex.org/W3122503268",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4224677645",
            "name": "Ngai, Hillary",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3145077191",
            "name": "Park Yoon-A",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2267814427",
            "name": "Chen John",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4287564413",
            "name": "Parsapoor, Mahboobeh",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2971193649",
        "https://openalex.org/W2988673764",
        "https://openalex.org/W2963846996",
        "https://openalex.org/W1840435438",
        "https://openalex.org/W2937055361",
        "https://openalex.org/W2101903972",
        "https://openalex.org/W3125175073",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W2963310665",
        "https://openalex.org/W2163571449",
        "https://openalex.org/W3102659883",
        "https://openalex.org/W2981039167",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W3020152921",
        "https://openalex.org/W1994842363",
        "https://openalex.org/W2912924812",
        "https://openalex.org/W2996428491",
        "https://openalex.org/W2978017171",
        "https://openalex.org/W2963339397",
        "https://openalex.org/W2427527485",
        "https://openalex.org/W2899154813",
        "https://openalex.org/W3082274269"
    ],
    "abstract": "In response to the Kaggle's COVID-19 Open Research Dataset (CORD-19) challenge, we have proposed three transformer-based question-answering systems using BERT, ALBERT, and T5 models. Since the CORD-19 dataset is unlabeled, we have evaluated the question-answering models' performance on two labeled questions answers datasets \\textemdash CovidQA and CovidGQA. The BERT-based QA system achieved the highest F1 score (26.32), while the ALBERT-based QA system achieved the highest Exact Match (13.04). However, numerous challenges are associated with developing high-performance question-answering systems for the ongoing COVID-19 pandemic and future pandemics. At the end of this paper, we discuss these challenges and suggest potential solutions to address them.",
    "full_text": "TRANSFORMER -BASED MODELS FOR QUESTION ANSWERING\nON COVID19\nA PREPRINT\nHillary Ngai1 2 Yoona Park 1 2 John Chen 1 2\nMahboobeh Parsapoor (Mah Parsa) 1 2\n1Vector Institute for Artiﬁcial Intelligence\n2University of Toronto,\n{hngai, ypark, johnc, mahparsa}@cs.toronto.edu\nJanuary 28, 2021\nABSTRACT\nIn response to the Kaggle’s COVID-19 Open Research Dataset (CORD-19) challenge, we have\nproposed three transformer-based question-answering systems using BERT, ALBERT, and T5 models.\nSince the CORD-19 dataset is unlabeled, we have evaluated the question-answering models’ per-\nformance on two labeled questions answers datasets —CovidQA and CovidGQA. The BERT-based\nQA system achieved the highest F1 score (26.32), while the ALBERT-based QA system achieved\nthe highest Exact Match (13.04). However, numerous challenges are associated with developing\nhigh-performance question-answering systems for the ongoing COVID-19 pandemic and future\npandemics. At the end of this paper, we discuss these challenges and suggest potential solutions to\naddress them.\nKeywords CORD-19 · COVID-19 · question-answering systems · ALBERT · BERT · T5\n1 Introduction\nQuestion Answering or QA systems may be useful for the medical research community to stay up-to-date when new\nliterature is rapidly growing. However, due to the lack of labeled question-answer pairs, developing accurate QA\nsystems is challenging. Thus, one solution is to ﬁne-tune pre-trained transformer-based QA systems 1 [1, 2, 3].\nIn this paper, we propose three QA systems developed for the Kaggle COVID-19 Open Research Dataset, or CORD-19\ndataset 2. We built the QA systems after carefully reviewing various QA systems submitted to the CORD-19 Kaggle\ncompetition. We have also presented our preliminary results obtained from evaluating the performance of three\ntransformers: Bidirectional Encoder Representations from Transformers or BERT [4], ALBERT, and Text-to-text\ntransfer transformer or T5 model on two new QA datasets —CovidQA and CovidGQA. Finally, we discuss the\nchallenges of developing a high-performance QA system for COVID-19-related research and suggest solutions to\nimprove performance.\n2 A Review of QA Systems for the Kaggle CORD-19 Dataset\nMore than 1,000 teams participated in the Kaggle CORD-19 dataset competition. They used various Natural Language\nProcessing) orNLP approaches, including BERT [4]). Since most QA systems in the competition were developed based\n1A typical transformer-based QA system uses a parallelizable architecture to efﬁciently ﬁnd answers (i.e., a segment of text) to a\nquery.\n2They launched a competition to provide a chance for the NLP community to develop QA systems for medical experts to ﬁnd\nanswers to high-priority medical questions related to COVID-19\narXiv:2101.11432v1  [cs.CL]  16 Jan 2021\nA PREPRINT - JANUARY 28, 2021\non BERT and Latent Dirichlet Allocation or (LDA), the following subsections just review LDA-based QA systems and\nBERT-based QA systems submitted to the competition.\n2.1 LDA-based QA Systems\nUsing LDA to develop QA systems is considerably uncomplicated [ 5, 6, 7, 8]. Thus, for the CORD-19 Kaggle\ncompetition, various LDA-based QA systems were developed. For example, one approach 3 [9] combined the whoosh\nsearch engine and used Jensen-Shannon distance to discover topics related to What do we know about COVID-19 risk\nfactors? and to ﬁnd documents similar to those topics. Another interesting LDA-based QA 4 was proposed in [10]; It\ncombined k-means clustering and LDA to cluster documents and discover topics from clusters to facilitate extracting\narticles from the CORD-19 dataset. In [ 11], a combination of LDA and Anserini (i.e., an open-source information\nretrieval approach) was proposed to develop a QA system that could ﬁnd articles related to non-pharmaceutical\nintervention. The main aim of this work was to discover new interventions in speciﬁc environments by incorporating\nthe context for each response in the search and guide policymakers to take appropriate actions to control spreading\nCOVID-19 virus.\n2.2 BERT-based QA systems for CORD-19\nSince one of BERT’s applications is in QA, different variations of BERT [12, 13, 14, 15] have been used. For the\ncompetition, many teams used BERT to develop QA systems [16, 17]. For instance, [16] used BERT to ﬁnd relevant\nanswers to keywords extracted from a question. The found solutions were ranked by Universal Sentence Encoder\nSemantic Similarity or USESS then Bayesian Additive Regression Treesor BART summarized the top results. Another\nteam employed BERT as a semantic search engine to ﬁnd answers. The QA system produced semantically meaningful\nsentence embedding on the paragraph extracted from the CORD-19 dataset and found ﬁve paragraphs and their\ncorresponding papers’ titles and abstracts. In [17], the QA systems were based on A little BERT or ALBERT [14] to\nﬁnd answers for questions related to COVID-19.\n3 Transformer-Based QA Systems\nFor the competition, we aimed to develop a QA system using a high performance Transformer. To do so, we developed\nthree QA systems using BERT-large, ALBERT-base andText-to-text transfer transformeror (T5)-large, pre-trained them\non various QA datasets and evaluated them on two labeled questions answers datasets —CovidQA, and CovidGQA as\ndescribed below.\n3.1 Datasets for Pre-training Transformers\nWe use various datasets to pre-train our QA systems: 1) SQuAD v1.1[18] —the Stanford Question Answering Dataset\ncontaining 100k question-answer pairs on more than 500 articles; 2) SNLI [19] —the Stanford Natural Language\nInference corpus containing 570k human-written English sentence pairs manually labeled for balance classiﬁcation with\nthe labels entailment, contradiction and neutral; 3) MultiNLI [20] —the Multi-Genre Natural Language Inference\ncorpus containing 433k crowd-sourced sentence pairs with the same format as SNLI except it includes a more diverse\nrange of text and a test set for cross-genre transfer evaluation; 4) STS —the Semantic Textual Similarity benchmark is a\ncareful selection of data from English STS shared tasks (2012-2017) comprising of 8.6k annotated examples of text\nfrom image captions, news headlines, and user forums; and 5) BioASQ 5 —the question-answering biomedical dataset\nconsists of 1k questions with “exact\" and “ideal\" answers. We speciﬁcally use BioASQ factoid QA pairs, excluding\nyes/no or list QA pairs, because the factoid dataset has a similar structure as SQuAD v1.1 [18].\n3.2 Datasets for Evaluating Transformers\nAfter pre-training our models, we evaluate our QA systems onCovidGQA and CovidQA. The former 6 is a COVID-19\ndataset created manually and encompasses 198 general question-text-answers related to COVID-197. The question-text\n3among 388 submitted kernels\n4It was received a lot of attention among other teams and was one of the competition’s kernels that obtained the highest number\nof votes around 783.\n5http://bioasq.org/\n6an example of a question in the CovidQA dataset is: What is the incubation period of the virus?\n7an example of a question in the CovidGQA dataset is: How can I protect myself from getting COVID-19?\n2\nA PREPRINT - JANUARY 28, 2021\nhas been extracted from medical websites, and medical subject-matter expertsor SMEs have provided answers. The\nlatter is a COVID-19 question-answering dataset built by hand from knowledge gathered from the Kaggle CORD-19\ndataset [21]. We merged the CovidQA dataset with the CORD-19 dataset to extract each article’s relevant text to answer\neach question in the dataset. The ﬁnal evaluation dataset contained 69 question-text-answer triplets.\n3.3 BERT-large\nOur BERT-large QA system is developed using a pre-trained QA BERT-large-uncased model with whole word masking\nfune-tuned on SQuAD v1.1 [18]. The model contains 24 Transformer blocks, 1024 hidden layers, 16 self-attention\nheads adding up to 340M parameters in total.\n3.4 ALBERT-base\nThe ALBERT-base QA system (Figure 2) is forned using a pre-trained QA ALBERT-base-uncased model ﬁne-tuned on\nSQuAD v1.1 [18]. The model contains 12 Transformer blocks, 768 hidden layers, 12 self-attention heads, adding up to\n12M parameters in total.\n3.5 T5-large\nThe T5-large QA system (see Figure 3) is based on the T5 model that is a modern, massive multitask model trained\nby uniting many NLP tasks in a uniﬁed text-to-text framework [22]. By leveraging extensive pre-training and transfer\nlearning, it has achieved state-of-the-art performance on a variety of NLP benchmark tasks, including the GLUE\nbenchmark [23]. Following work by [24], which explores the task of generative closed-book question answering, we\nexplore the efﬁcacy of generating (rather than extracting) COVID-19 answers directly from an input question, without\ncontext. Unlike our preceding two approaches, the T5 model explores generation of answers to questions, without\ncontext. Using the pre-trained T5 model with 770M parameters released by [22], we ﬁne-tune for 25000 steps on an\nequal-proportions mixture of three QA tasks using the Natural Questions dataset, Trivia QA dataset and the train split\nof the COVID-19 QA dataset [25, 26]. Only the queries are given as input and answers are generated using simple\ngreedy decoding. Evaluation is then performed on the test split of the COVID-19 dataset. We emphasize that these\nresults are not directly comparable to the other frameworks, as the model is faced with the challenging task of jointly\nlocalizing relevant information and then generating a coherent answer. The advantage of such a framework is that it is\ncontext-free, meaning that it requires the least data preparation and human intervention.\n3.6 Evaluation of QA Systems\nWe evaluate the above transformers using two datasets —CovidQA and CovidGQA. Using the same evaluation metrics\nas SQuAD v1.1, we calculate the macro-averaged F1 score and Exact Match or EM of the answer extraction methods\nof our QA systems on each dataset. Both BERT-large-uncased and ALBERT-base-uncased use whole word masking\nand are pre-trained on SQuAD v1.1, while T5-large was pre-trained on Colossal Clean Crawled Corpus or C4. Figure\n1 shows the comparison of generated answer lengths on the CovidGQA dataset.\nDataset QA System F1 Score EM\nALBERT-base 23.37 13.04\nCovidQA BERT-large 26.32 11.59\nT5-large 5.16 0.00\nALBERT-base 28.08 9.64\nCovidGQA BERT-large 29.96 5.08\nT5-large 5.95 0.00\nTable 1: Preliminary results obtained using QA systems on CovidQA and CovidGQA.\nBERT-large achieves the highest macro-averaged F1 score on both datasets. Furthermore, BERT-large outperforms\nALBERT-base on GLUE, RACE, and SQuAD benchmarks [14]. However, ALBERT-base unexpectedly outperforms\nBERT-large on EM, achieving the highest EM of the three models. Further experimentation is required to investigate\nthe cause of this. Despite T5-large having the most signiﬁcant number of parameters (770M), both BERT-large and\nALBERT-base outperform T5-large on all metrics on both datasets. This may be explained by the fact that T5-large was\nnot pre-trained on a QA task. The reasonable performance of these three transformers, motivated us to use them to\ndevelop QA systems for the CORD-19 Kaggle competition. The next section discusses the QA systems in more detail.\n3\nA PREPRINT - JANUARY 28, 2021\nFigure 1: Comparison of generated answer lengths on the CovidGQA dataset.\n4 QA Systems for COVID-19\nBased on the results showed in the previous section, we have developed two QA systems on the CORD-19 dataset\nwhich aim to help the medical community answer high-priority scientiﬁc questions such as “What is the efﬁcacy of\nnovel therapeutics being tested currently?\" or “What is the best method to combat the hypercoagulable state seen in\nCOVID-19?\". We designed two BERT-based question-answering systems and a T5 question-answering system. Each\nQA system is pre-trained on different datasets and evaluated on two COVID-19 datasets.\n4.1 SBERT-BERT-QA\nThe SBERT-BERT-QA system (see Figure 28) combines Sentence-BERT and BERT-large. We ﬁrst ﬁlter the articles\nusing a keyword search based on a set of pre-deﬁned keywords such asRNA virus, clinical, naproxen, clarithromycin.\nOnce ﬁltered, the top n articles were extracted by embedding the query and the article titles using a pre-trained Sentence-\nBERT model (i.e., a BERT-base model with mean-tokens trained on SNLI and MultiNLI corpora and then on STS\nBenchmark training set) [27]. Sentence-BERT (SBERT) is a modiﬁcation of the BERT network using Siamese and\ntriplet network structures to derive semantically meaningful sentence embeddings. The top n articles were extracted\nby taking the articles with the highest cosine similarity scores between the query embedding and each article title\nembedding. Once the top article were extracted, each article’s answer to the query was extracted using a pre-trained QA\nBERT-large-uncased model.\n4.2 LDA-ALBERT-QA\nThe LDA-ALBERT-QA (see Figure 29) combines LDA and pre-trained ALBERT-base. First, we ﬁlter out the irrelevant\narticles from the CORD-19 dataset using LDA to provide a dataset containing only the relevant articles to the query.\nThen, the ﬁltered documents are fed into a pre-trained QA ALBERT-base-uncased model to extract an excerpt from\narticles that are relevant to the query. We have utilizedSQuAD v1.1[18] and BioASQ 6b factoid QA pairs to develop\nthe pre-trained ALBERT model. The primary reason for using SQuAD v1.1 is to overcome the data shortage of BioASQ\n6b factoid, which contains less than 1k QA pairs. After pre-training the model on SQuAD v1.1, we further pre-train the\nmodel with BioASQ dataset to target biomedical domain. A single output layer on top of the pre-trained ALBERT\nmodel performs token-level classiﬁcation to compute the start/end index of a predicted answer from each article.\n5 Conclusion\nWe presented two transformer-based QA systems to compete in the competition. We selected transformers based on\nthe preliminary results obtained from BERT, ALBERT, and T5 model on two QA datasets. As our results indicated,\nBERT-large could achieve the highest F1-score for both datasets and one of our ﬁrst candidates to develop a QA system.\nOur results also showed that ALBERT-base could achieve the highest EM score for both datasets and was our second\nnominee to establish a QA system. We consider that one of the signiﬁcant limitations of transformers is that they require\n8The system diagram of SBERT-BERT-QA and LDA-ALBERT-QA. The dotted arrows and the white rectangles represent the\npath of SBERT-BERT-QA.\n9The dashed arrows and the dark rectangles represent the path of LDA-ALBERT-QA.\n4\nA PREPRINT - JANUARY 28, 2021\nFigure 2: BERT-based QAs\n Figure 3: Context-free T5 QA system\na lot of labeled QA pairs to reach acceptable performance. We aimed to resolve the limitation by developing a hybrid\nQA system that combines few-shot learning with a transformer. LDA also has some limitations; for example, we need\nto determine the number of topics and consider the articles that are not too short. To address these issues, we explore\nprediction-focused supervised LDA and identify topics in an online manner. Furthermore, a QA system should be\nexplainable and trustworthy to medical users. Developing such a QA system is possible if medical experts and NLP\nresearchers cooperate closely.\nAuthor’s contributions\nThe ﬁrst three authors equally worked with technical parts of the paper that has been summarized in Section 3 and 4. Dr.\nParsa wrote sections 1 and 2 and reviewed the manuscript and provided valuable feedback on the manuscript.\nReferences\n[1] KSD Ishwari, AKRR Aneeze, S Sudheesan, HJDA Karunaratne, A Nugaliyadde, and Y Mallawarrachchi.\nAdvances in natural language question answering: A review. arXiv preprint arXiv:1904.05276, 2019.\n[2] D. Lukovnikov, A. Fischer, and J. Lehmann. Pretrained transformers for simple question answering over knowledge\ngraphs, 2020.\n5\nA PREPRINT - JANUARY 28, 2021\n[3] Betty van Aken, Benjamin Winter, Alexander Löser, and Felix A. Gers. How does bert answer questions?\nProceedings of the 28th ACM International Conference on Information and Knowledge Management, Nov 2019.\n[4] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional\ntransformers for language understanding, 2019.\n[5] Asli Celikyilmaz, Dilek Hakkani-Tur, and Gokhan Tur. Lda based similarity modeling for question answering. In\nProceedings of the NAACL HLT 2010 Workshop on Semantic Search, pages 1–9, 2010.\n[6] Lin Cui and Caiyin Wang. An intelligent q&a system based on the lda topic model for the teaching of database\nprinciples. World Transactions on Engineering and Technology Education, 12:26–30, 01 2014.\n[7] Hamidreza Chinaei, Luc Lamontagne, François Laviolette, and Richard Khoury. A topic model scoring approach\nfor personalized qa systems. In International Conference on Text, Speech, and Dialogue, pages 84–92. Springer,\n2014.\n[8] Zongcheng Ji, Fei Xu, Bin Wang, and Ben He. Question-answer topic model for question retrieval in community\nquestion answering. In CIKM ’12, 2012.\n[9] Daniel Wolffram. discovid.ai - a search and recommendation engine. https://www.kaggle.com/\ndanielwolffram/discovid-ai-a-search-and-recommendation-engine , April 2020.\n[10] SoloNick Maksim Ekin. Covid-19 literature clustering. https://www.kaggle.com/maksimeren/\ncovid-19-literature-clustering , April 2020.\n[11] Jonathan Smith, Borna Ghotbi, Seungeun Yi, and Mahboobeh Parsapoor. Non-pharmaceutical intervention\ndiscovery with topic modeling, 2020.\n[12] Victor Sanh, Lysandre Debut, Julien Chaumond, and Thomas Wolf. Distilbert, a distilled version of bert: smaller,\nfaster, cheaper and lighter, 2019.\n[13] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke\nZettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv preprint\narXiv:1907.11692, 2019.\n[14] Zhenzhong Lan, Mingda Chen, Sebastian Goodman, Kevin Gimpel, Piyush Sharma, and Radu Soricut. Albert: A\nlite bert for self-supervised learning of language representations, 2020.\n[15] Ying-Hong Chan and Yao-Chung Fan. A recurrent bert-based model for question generation. In Proceedings of\nthe 2nd Workshop on Machine Reading for Question Answering, pages 154–162, 2019.\n[16] Peilin Yang, Hui Fang, and Jimmy Lin. Anserini: Reproducible ranking baselines using lucene. Journal of Data\nand Information Quality (JDIQ), 10(4):1–20, 2018.\n[17] Shivam Abhilash Sandyvarma et al. Covid-19: Bert + mesh enabled knowledge graph. https://www.kaggle.\ncom/sandyvarma/covid-19-bert-mesh-enabled-knowledge-graph , April 2020.\n[18] Rajpurkar, Zhang, Lopyrev, and Liang. Squad: 100,000+ questions for machine comprehension of text. arXiv\npreprint arXiv:1606.05250v3, 2016.\n[19] Samuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated corpus\nfor learning natural language inference. In Proceedings of the 2015 Conference on Empirical Methods in\nNatural Language Processing, pages 632–642, Lisbon, Portugal, September 2015. Association for Computational\nLinguistics.\n[20] Adina Williams, Nikita Nangia, and Samuel Bowman. A broad-coverage challenge corpus for sentence un-\nderstanding through inference. In Proceedings of the 2018 Conference of the North American Chapter of the\nAssociation for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers) , pages\n1112–1122. Association for Computational Linguistics, 2018.\n[21] Tang, Nogueira, Zhang, Gupta, Cam, Cho, and Lin. Albert: A lite bert for self-supervised learning of language\nrepresentations. arXiv preprint arXiv:2004.11339v1, 2020.\n[22] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei\nLi, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer, 2019.\n[23] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. Glue: A multi-task\nbenchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461, 2018.\n[24] Adam Roberts, Colin Raffel, and Noam Shazeer. How much knowledge can you pack into the parameters of a\nlanguage model? arXiv preprint arXiv:2002.08910, 2020.\n6\nA PREPRINT - JANUARY 28, 2021\n[25] Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redﬁeld, Michael Collins, Ankur Parikh, Chris Alberti, Danielle\nEpstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, et al. Natural questions: A benchmark for question answering\nresearch, 2019.\n[26] Mandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. Triviaqa: A large scale distantly supervised\nchallenge dataset for reading comprehension. In Proceedings of the 55th Annual Meeting of the Association for\nComputational Linguistics (Volume 1: Long Papers), pages 1601–1611, 2017.\n[27] Reimers and Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. arXiv preprint\narXiv:1908.10084, 2019.\n7"
}