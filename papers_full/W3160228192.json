{
    "title": "Medical Image Segmentation Using Squeeze-and-Expansion Transformers",
    "url": "https://openalex.org/W3160228192",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A2252250856",
            "name": "Li Shaohua",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226007090",
            "name": "Sui, Xiuchao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4221795752",
            "name": "Luo, Xiangde",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A667379501",
            "name": "Xu Xinxing",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2058749718",
            "name": "Liu Yong",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4226007096",
            "name": "Goh, Rick",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W3170841864",
        "https://openalex.org/W3092344722",
        "https://openalex.org/W2888358068",
        "https://openalex.org/W2556967412",
        "https://openalex.org/W3103242287",
        "https://openalex.org/W2963857746",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W2034786340",
        "https://openalex.org/W2799261915",
        "https://openalex.org/W3119786062",
        "https://openalex.org/W2464708700",
        "https://openalex.org/W3098085362",
        "https://openalex.org/W3111551570",
        "https://openalex.org/W3016719260",
        "https://openalex.org/W3015788359",
        "https://openalex.org/W2979448322",
        "https://openalex.org/W3001591165",
        "https://openalex.org/W3034785488",
        "https://openalex.org/W3028537938",
        "https://openalex.org/W3023353361",
        "https://openalex.org/W3127751679",
        "https://openalex.org/W2898910301",
        "https://openalex.org/W1901129140",
        "https://openalex.org/W3034971973",
        "https://openalex.org/W2999219213",
        "https://openalex.org/W3096609285",
        "https://openalex.org/W2963524571",
        "https://openalex.org/W3112701542",
        "https://openalex.org/W2953273646",
        "https://openalex.org/W1641498739",
        "https://openalex.org/W2955425717",
        "https://openalex.org/W2952339051",
        "https://openalex.org/W2928165649",
        "https://openalex.org/W2962914239",
        "https://openalex.org/W3014512070",
        "https://openalex.org/W2964309882",
        "https://openalex.org/W2951528897",
        "https://openalex.org/W2798122215",
        "https://openalex.org/W3028210663",
        "https://openalex.org/W3106728613",
        "https://openalex.org/W2626778328",
        "https://openalex.org/W2884436604",
        "https://openalex.org/W2751069891"
    ],
    "abstract": "Medical image segmentation is important for computer-aided diagnosis. Good segmentation demands the model to see the big picture and fine details simultaneously, i.e., to learn image features that incorporate large context while keep high spatial resolutions. To approach this goal, the most widely used methods -- U-Net and variants, extract and fuse multi-scale features. However, the fused features still have small \"effective receptive fields\" with a focus on local image cues, limiting their performance. In this work, we propose Segtran, an alternative segmentation framework based on transformers, which have unlimited \"effective receptive fields\" even at high feature resolutions. The core of Segtran is a novel Squeeze-and-Expansion transformer: a squeezed attention block regularizes the self attention of transformers, and an expansion block learns diversified representations. Additionally, we propose a new positional encoding scheme for transformers, imposing a continuity inductive bias for images. Experiments were performed on 2D and 3D medical image segmentation tasks: optic disc/cup segmentation in fundus images (REFUGE'20 challenge), polyp segmentation in colonoscopy images, and brain tumor segmentation in MRI scans (BraTS'19 challenge). Compared with representative existing methods, Segtran consistently achieved the highest segmentation accuracy, and exhibited good cross-domain generalization capabilities. The source code of Segtran is released at https://github.com/askerlee/segtran.",
    "full_text": "Medical Image Segmentation Using Squeeze-and-Expansion Transformers\nShaohua Li1âˆ— , Xiuchao Sui1 , Xiangde Luo2 , Xinxing Xu1 , Yong Liu1 , Rick Goh1\n1Institute of High Performance Computing, A*STAR, Singapore\n2University of Electronic Science and Technology of China, Chengdu, China\n{shaohua, xiuchao.sui}@gmail.com, xiangde.luo@std.uestc.edu.cn, {xuxinx, liuyong,\ngohsm}@ihpc.a-star.edu.sg\nAbstract\nMedical image segmentation is important for\ncomputer-aided diagnosis. Good segmentation de-\nmands the model to see the big picture and ï¬ne\ndetails simultaneously, i.e., to learn image features\nthat incorporate large context while keep high spa-\ntial resolutions. To approach this goal, the most\nwidely used methods â€“ U-Net and variants, ex-\ntract and fuse multi-scale features. However, the\nfused features still have small effective receptive\nï¬elds with a focus on local image cues, limiting\ntheir performance. In this work, we propose Seg-\ntran, an alternative segmentation framework based\non transformers, which have unlimited effective re-\nceptive ï¬elds even at high feature resolutions. The\ncore of Segtran is a novel Squeeze-and-Expansion\ntransformer: a squeezed attention block regularizes\nthe self attention of transformers, and an expan-\nsion block learns diversiï¬ed representations. Ad-\nditionally, we propose a new positional encoding\nscheme for transformers, imposing a continuity in-\nductive bias for images. Experiments were per-\nformed on 2D and 3D medical image segmentation\ntasks: optic disc/cup segmentation in fundus im-\nages (REFUGEâ€™20 challenge), polyp segmentation\nin colonoscopy images, and brain tumor segmen-\ntation in MRI scans (BraTSâ€™19 challenge). Com-\npared with representative existing methods, Segtran\nconsistently achieved the highest segmentation ac-\ncuracy, and exhibited good cross-domain general-\nization capabilities. The source code of Segtran is\nreleased at https://github.com/askerlee/segtran.\n1 Introduction\nAutomated Medical image segmentation, i.e., automated de-\nlineation of anatomical structures and other regions of inter-\nest (ROIs), is an important step in computer-aided diagnosis;\nfor example it is used to quantify tissue volumes, extract key\nquantitative measurements, and localize pathology [Schlem-\nper et al., 2019; Orlando et al., 2020]. Good segmentation\ndemands the model to see the big picture and ï¬ne details at\nâˆ—Corresponding Author.\nthe same time, i.e., learn image features that incorporate large\ncontext while keep high spatial resolutions to output ï¬ne-\ngrained segmentation masks. However, these two demands\npose a dilemma for CNNs, as CNNs often incorporate larger\ncontext at the cost of reduced feature resolution. A good mea-\nsure of how large a model â€œseesâ€ is theeffective receptive ï¬eld\n(effective RF) [Luo et al., 2016], i.e., the input areas which\nhave non-negligible impacts to the model output.\nSince the advent of U-Net [Ronneberger et al., 2015 ], it\nhas shown excellent performance across medical image seg-\nmentation tasks. A U-Net consists of an encoder and a de-\ncoder, in which the encoder progressively downsamples the\nfeatures and generates coarse contextual features that focus\non contextual patterns, and the decoder progressively upsam-\nples the contextual features and fuses them with ï¬ne-grained\nlocal visual features. The integration of multiple scale fea-\ntures enlarges the RF of U-Net, accounting for its good per-\nformance. However, as the convolutional layers deepen, the\nimpact from far-away pixels decay quickly. As a results, the\neffective RF of a U-Net is much smaller than its theoretical\nRF. As shown in Fig.2, the effective RFs of a standard U-Net\nand DeepLabV3+ are merely around 90 pixels. This implies\nthat they make decisions mainly based on individual small\npatches, and have difï¬culty to model larger context. How-\never, in many tasks, the heights/widths of the ROIs are greater\nthan 200 pixels, far beyond their effective RFs. Without â€œsee-\ning the bigger pictureâ€, U-Net and other models may be mis-\nled by local visual cues and make segmentation errors.\nMany improvements of U-Net have been proposed. A few\ntypical variants include: U-Net++ [Zhou et al., 2018] and U-\nNet 3+ [Huang et al., 2020], in which more complicated skip\nconnections are added to better utilize multi-scale contextual\nfeatures; attention U-Net [Schlemper et al., 2019], which em-\nploys attention gates to focus on foreground regions; 3D U-\nNet [Ã‡iÃ§ek et al., 2016 ] and V-Net [Milletari et al., 2016 ],\nwhich extend U-Net to 3D images, such as MRI volumes;\nEff-UNet [Baheti et al., 2020], which replaces the encoder of\nU-Net with a pretrained Efï¬cientNet [Tan and Le, 2019].\nTransformers [Vaswani et al., 2017] are increasingly pop-\nular in computer vision tasks. A transformer calculates\nthe pairwise interactions (â€œself-attentionâ€) between all input\nunits, combines their features and generates contextualized\nfeatures. The contextualization brought by a transformer is\nanalogous to the upsampling path in a U-Net, except that\narXiv:2105.09511v3  [eess.IV]  2 Jun 2021\nReshaping to \noriginal shape\nVisual featuresLearnable sinusoidal\npositional encoding\nâ€¦\nâ€¦\nSqueeze-and-Expansion \nTransformer layers\nLocal features\nContextualized features\nOutput FPN\nâ€¦\nSegmentation \nheadğ‘¥ğ‘–,ğ‘¦ğ‘—\nCNN backbone\nInput Encoder Transformer\nSpatially \nflattening\nHi-res features\nCoordinates\nInput FPN\n+\nFigure 1: Segtran architecture. It extracts visual features with a CNN backbone, combines them with positional encodings of the pixel\ncoordinates, and ï¬‚attens them into a sequence of local feature vectors. The local features are contextualized by a few Squeeze-and-Expansion\ntransformer layers. To increase spatial resolution, an input FPN and an output FPN upsamples the features before and after the transformers.\n(a) Fundus image   (b) Ground truth      (c)  U-Net       (d) DeeplabV3+     (e) Segtran\n500\n400\n300\n200\n100\n0\n250\n200\n150\n100\n50\n0\nFigure 2: Effective receptive ï¬elds of 3 models, indicated by non-\nnegligible gradients in blue blobs and light-colored dots. Gradients\nare back-propagated from the center of the image. Segtran has non-\nnegligible gradients dispersed across the whole image (light-colored\ndots). U-Net and DeepLabV3+ have concentrated gradients. Input\nimage: 576 Ã— 576.\nit has unlimited effective receptive ï¬eld, good at capturing\nlong-range correlations. Thus, it is natural to adopt trans-\nformers for image segmentation. In this work, we present\nSegtran, an alternative segmentation architecture based on\ntransformers. A straightforward incorporation of transform-\ners into segmentation only yields moderate performance. As\ntransformers were originally designed for Natural Language\nProcessing (NLP) tasks, there are several aspects that could\nbe improved to better suit image applications. To this end, we\npropose a novel transformer design Squeeze-and-Expansion\nTransformer, in which a squeezed attention block helps reg-\nularize the huge attention matrix, and an expansion block\nlearns diversiï¬ed representations. In addition, we propose a\nlearnable sinusoidal positional encoding that imposes a conti-\nnuity inductive bias for the transformer. Experiments demon-\nstrate that they lead to improved segmentation performance.\nWe evaluated Segtran on two 2D medical image segmen-\ntation tasks: optic disc/cup segmentation in fundus images\nof the REFUGEâ€™20 challenge, and polyp segmentation in\ncolonoscopy images. Additionally, we also evaluated it on\na 3D image segmentation task: brain tumor segmentation\nin MRI scans of the BraTSâ€™19 challenge. Segtran has con-\nsistently shown better performance than U-Net and its vari-\nants (UNet++, UNet3+, PraNet, and nnU-Net), as well as\nDeepLabV3+ [Chen et al., 2018].\n2 Related Work\nOur work is largely inspired by DETR [Carion et al., 2020].\nDETR uses transformer layers to generate contextualized fea-\ntures that represent objects, and learns a set of object queries\nto extract the positions and classes of objects in an image. Al-\nthough DETR is also explored to do panoptic segmentation\n[Kirillov et al., 2019], it adopts a two-stage approach which\nis not applicable to medical image segmentation. A followup\nwork of DETR, Cell-DETR [Prangemeier et al., 2020] also\nemploys transformer for biomedical image segmentation, but\nits architecture is just a simpliï¬ed DETR, lacking novel com-\nponents like our Squeeze-and-Expansion transformer. Most\nrecently, SETR [Zheng et al., 2021] and TransU-Net [Chen et\nal., 2021] were released concurrently or after our paper sub-\nmission. Both of them employ a Vision Transformer (ViT)\n[Dosovitskiy et al. , 2021 ] as the encoder to extract image\nfeatures, which already contain global contextual informa-\ntion. A few convolutional layers are used as the decoder to\ngenerate the segmentation mask. In contrast, in Segtran, the\ntransformer layers build global context on top of the local im-\nage features extracted from a CNN backbone, and a Feature\nPyramid Network (FPN) generates the segmentation mask.\n[Murase et al. , 2020 ] extends CNNs with positional en-\ncoding channels, and evaluates them on segmentation tasks.\nMixed results were observed. In contrast, we veriï¬ed through\nan ablation study that positional encodings indeed help Seg-\ntran to do segmentation to a certain degree.\nReceptive ï¬elds of U-Nets may be enlarged by adding\nmore downsampling layers. However, this increases the num-\nber of parameters and adds the risk of overï¬tting. Another\nway of increasing receptive ï¬elds is using larger stride sizes\nof the convolutions in the downsampling path. Doing so,\nhowever, sacriï¬ces spatial precision of feature maps, which is\noften disadvantageous for segmentation[Liu and Guo, 2020].\n3 Squeeze-and-Expansion Transformer\nThe core concept in a transformer is Self Attention , which\ncan be understood as computing an afï¬nity matrix between\ndifferent units, and using it to aggregate features:\nAtt_weight(X, X) =f(K(X), Q(X)) âˆˆRNÃ—N , (1)\nAttention(X) = Att_weight(X, X) Â·V (X), (2)\nXout = FFN(Attention(X)), (3)\nwhere K, Q, V are key, query, and value projections, respec-\ntively. f is softmax after dot product. Att_weight(X, X) is\nthe pairwise attention matrix between input units, whose i, j-\nth element deï¬nes how much the features of unitj contributes\nto the fused (contextualized) features of uniti. FFN is a feed-\nforward network to further transform the fused features.\nThe basic transformer above is extended to a multi-head\nattention (MHA) [Vaswani et al., 2017; V oita et al., 2019],\naiming to capture different types of associations between in-\nput units. Each of theNh heads computes individual attention\nwights and output features ( C/Nh-dimensional), and their\noutput features are concatenated along the channel dimen-\nsion into C-dimensional features. Different heads operate in\nexclusively different feature subspaces.\nWe argue that transformers can be improved in four aspects\nmake them better suited for images:\n1. In Eq.(2), the intermediate features Attention(X) are\nobtained by linearly combining the projected input fea-\ntures, where the attention matrix speciï¬es the combina-\ntion coefï¬cients. As the attention matrix is huge:NÃ—N,\nwith typically N > 1000, it is inherently vulnerable to\nnoises and overï¬tting. Reducing the attention matrix to\nlower rank matrices may help.\n2. In traditional transformers, the output features are\nmonomorphic: it has only one set of feature transfor-\nmations (the multi-head transformer also has one set\nof transformations after concatenation), which may not\nhave enough capacity to fully model data variations. Just\nlike a mixture of Gaussians almost always better depicts\na data distribution than a single Gaussian, data variations\ncan be better captured using a mixture ofk transformers.\n3. In traditional transformers, the key and query projec-\ntions are independently learned, enabling them to cap-\nture asymmetric relationships between tokens in natu-\nral language. However, the relationships between image\nunits are often symmetric, such as whether two pixels\nbelong to the same segmentation class.\n4. Pixels in an image have strong locality and semantic\ncontinuity. The two mainstream positional encoding\nğ‘¥1 ğ‘¥2 ğ‘¥3 â‹¯ ğ‘¥ğ‘\n(a)\nğ‘1â€²\nâ‹®\nğ‘ğ‘€â€²\nğ‘1 â‹¯ ğ‘ğ‘€\n(b)\nğ‘¥1\nğ‘¥2\nğ‘¥3\nâ‹®\nğ‘¥ğ‘\nğ‘¥1 ğ‘¥2 ğ‘¥3 â‹¯ ğ‘¥ğ‘ ğ‘¥1â€²\nğ‘¥2â€²\nğ‘¥3â€²\nâ‹®\nğ‘¥ğ‘â€²\nğ‘¥1â€²\nğ‘¥2â€²\nğ‘¥3â€²\nâ‹®\nğ‘¥ğ‘â€²\nğ‘¥1\nğ‘¥2\nğ‘¥3\nâ‹®\nğ‘¥ğ‘\nFigure 3: (a) Full self-attention ( N Ã— N) vs. (b) Squeezed At-\ntention Block (SAB). In SAB, ï¬rst input units x1, Â· Â· Â·, xN attend\nwith a codebook c1, Â· Â· Â·, cM , yielding projected codebook features\ncâ€²\n1, Â· Â· Â·, câ€²\nM , which then attend back with the input x1, Â· Â· Â·, xN .\nThe two attention matrices are N Ã— M and M Ã— N, respectively.\nHead 1\nHead ğ‘â„\nConcatenate\nMode ğ‘ğ‘š Aggregate\n(a)\n(b)\nğ¶/ğ‘â„\nğ¶ channels\nğ¶ channels\nğ¶\nğ¶\nğ¶\nğ¶\nğ¶/ğ‘â„\nğ¶/ğ‘â„\nâ‹¯\nMode 1\nFigure 4: (a) Multi-head attention (MHA) vs. (b) Expanded atten-\ntion block (EAB). In MHA, each head outputs an exclusive feature\nsubset. In contrast, EAB outputs Nm sets of complete features from\nNm modes, and aggregates them with dynamic mode attention.\nschemes [Carion et al., 2020; Dosovitskiy et al., 2021]\ndo not fully impose such an inductive bias. This bias\ncould be imposed by an improved positional encoding.\nThe Squeeze-and-Expansion Transformer aims to improve\nin all the four aspects. The Squeezed Attention Block com-\nputes attention between the input andM inducing points [Lee\net al., 2019], and compresses the attention matrices toNÃ—M.\nThe Expanded Attention Block is a mixture-of-experts model\nwith Nm modes (â€œexpertsâ€). In both blocks, the query projec-\ntions and key projections are tied to make the attention sym-\nmetric, for better modeling of the symmetric relationships be-\ntween image units. In addition, a Learnable Sinusoidal Posi-\ntional Encoding helps the model capture spatial relationships.\n3.1 Squeezed Attention Block\n[Lee et al. , 2019 ] proposes Induced Set Attention Block\n(ISAB) by bringing inducing points into the transformer. It\nwas originally designed to learn good features of a set of un-\nordered objects. Here we employ this design to â€œsqueezeâ€ the\nbloated attention matrix, so as to reduce noises and overï¬tting\nin image tasks. We rename ISAB asSqueezed Attention Block\n(SAB) to highlight its new role in this context1.\nIn SAB, inducing points are a set of M learned embed-\ndings c1, Â·Â·Â· , cM in an external discrete codebook. Usually\nM â‰ªN, the number of input units. The inducing points are\nï¬rst transformed into new embeddings Câ€²= câ€²\n1, Â·Â·Â· , câ€²\nM af-\nter attending with the input. The combination of these embed-\ndings form the output features Xout = xâ€²\n1, Â·Â·Â· , xâ€²\nN (Fig.3):\nCâ€²= Single-Head(X, C), (4)\nXout = EAB(Câ€², X), (5)\nwhere Single-Head (Â·, Â·) is a single-head transformer, and\nEAB(Â·, Â·) is an Expanded Attention Block presented in the\nnext subsection. In each of the two steps, the attention matrix\nis of N Ã—M, much more compact than vanilla transformers.\nSAB is conceptually similar to the codebook used for dis-\ncrete representation learning in [Esser et al., 2020], but the\ndiscretized features are further processed by a transformer.\nSAB can trace its lineage back to low-rank matrix factor-\nization, i.e., approximating a data matrix XnÃ—n â‰ˆPnÃ—d Â·\nQdÃ—n, which is a traditional regularization technique against\ndata noises and overï¬tting. Conï¬rmed by an ablation study,\nSAB helps ï¬ght against noises and overï¬tting as well.\n3.2 Expanded Attention Block\nThe Expanded Attention Block (EAB) consists ofNm modes,\neach an individual single-head transformer. They output Nm\nsets of contextualized features, which are then aggregated\ninto one set using dynamic mode attention:\nX(k)\nout = Mode(k)(X), (6)\nB(k) = Linear(k)(X(k)\nout), (7)\nwith k âˆˆ{1, Â·Â·Â· , Nm},\nG = softmax\n(\nB(1), Â·Â·Â· , B(Nm)\n)\n, (8)\nXout =\n(\nX(1)\nout, Â·Â·Â· , X(Nm)\nout\n)\nÂ·GâŠ¤, (9)\nwhere the mode attention G âˆˆRNuÃ—Nm is obtained by do-\ning a linear transformation of each mode features, and tak-\ning softmax over all the modes. Eq.(9) takes a weighted sum\nover the modes to get the ï¬nal output features Xout. This\ndynamic attention is inspired by the Split Attention of the\nResNest model [Zhang et al., 2020b].\nEAB is a type of Mixture-of-Experts[Shazeer et al., 2017],\nan effective way to increase model capacity. Although there is\nresemblance between multi-head attention (MHA) and EAB,\nthey are essentially different, as shown in Fig.4. In MHA,\neach head resides in an exclusive feature subspace and pro-\nvides unique features. In contrast, different modes in EAB\nshare the same feature space, and the representation power\nlargely remains after removing any single mode. The modes\njoin forces to offer more capacity to model diverse data, as\nshown in an ablation study. In addition, EAB is also different\n1We clarify that our contribution is a novel transformer architec-\nture that combines SAB with an Expanded Attention Block.\nfrom the Mixture of Softmaxes (MoS) transformer [Zhang et\nal., 2020a ]. Although MoS transformer also uses k sets of\nqueries and keys, it shares one set of value transformation.\n3.3 Learnable Sinusoidal Positional Encoding\nA crucial inductive bias for images is the pixel locality and\nsemantic continuity, which is naturally encoded by convolu-\ntional kernels. As the input to transformers is ï¬‚attened into\n1-D sequences, positional encoding (PE) is the only source\nto inject information about spatial relationships. On the one\nhand, this makes transformers ï¬‚exible to model arbitrary\nshapes of input. On the other hand, the continuity bias of\nimages is non-trivial to fully incorporate. This is a limitation\nof the two mainstream PE schemes: Fixed Sinusoidal Encod-\ning and Discretely Learned Encoding [Carion et al. , 2020;\nDosovitskiy et al., 2021]. The former is spatially continuous\nbut lacks adaptability, as the code is predeï¬ned. The latter\nlearns a discrete code for each coordinate without enforcing\nspatial continuity.\nWe propose Learnable Sinusoidal Positional Encoding ,\naiming to bring in the continuity bias with adaptability. Given\na pixel coordinate (x, y), our positional encoding vector\npos(x, y) is a combination of sine and cosine functions of\nlinear transformations of (x, y):\nposi(x, y) =\n{sin(aix + biy + ci) if i < C/2\ncos(aix + biy + ci) if i â‰¥C/2, (10)\nwhere i indexes the elements in pos, {ai, bi, ci}are learn-\nable weights of a linear layer, and C is the dimensionality of\nimage features. To make the PE behave consistently across\ndifferent image sizes, we normalize (x, y) into [0, 1]2. When\nthe input image is 3D, Eq.(10) is trivially extended by using\n3D coordinates (x, y, z).\nThe encoding in Eq.(10) changes smoothly with pixel co-\nordinates, and thus nearby units receive similar positional en-\ncodings, pushing the attention weights between them towards\nlarger values, which is the spirit of the continuity bias. The\nlearnable weights and sinusoidal activation functions make\nthe code both adaptable and nonlinear to model complex spa-\ntial relationships [Tancik et al., 2020].\n4 Segtran Architecture\nAs a context-dependent pixel-wise classiï¬cation task, seg-\nmentation faces a conï¬‚ict between larger context (lower reso-\nlution) and localization accuracy (higher resolution). Segtran\npartly circumvents this conï¬‚ict by doing pairwise feature con-\ntextualization, without sacriï¬cing spatial resolutions. There\nare ï¬ve main components in Segtran (Fig.1): 1) a CNN back-\nbone to extract image features, 2) input/output feature pyra-\nmids to do upsampling, 3) learnable sinusoidal positional en-\ncoding, 4) Squeeze-and-Expansion transformer layers to con-\ntextualize features, and 5) a segmentation head.\n4.1 CNN Backbone\nWe employ a pretrained CNN backbone to extract features\nmaps with rich semantics. Suppose the input image is X0 âˆˆ\nRH0Ã—W0Ã—D0 , where for a 2D image, D0 = 1 or 3 is the\nnumber of color channels. For a 3D image, D0 â‰«3 is the\nnumber of slices in the depth dimension. For 2D and 3D im-\nages, the extracted features are CNN (X0) âˆˆRCÃ—HÃ—W , and\nCNN(X0) âˆˆRCÃ—HÃ—WÃ—D, respectively.\nOn 2D images, typically ResNet-101 or Efï¬cientNet-D4 is\nused as the backbone. For increased spatial resolution, we\nchange the stride of the ï¬rst convolution from 2 to 1. Then\nH, W= H0/16, W0/16. On 3D images, 3D backbones like\nI3D [Carreira and Zisserman, 2017] could be adopted.\n4.2 Transformer Layers\nBefore being fed into the transformer, the visual features\nand positional encodings of each unit are added up be-\nfore being fed to the transformer: Xspatial = Xvisual +\npos(coordinates(X)). Xspatial is ï¬‚attened across spatial di-\nmensions to a 1-D sequence X0 âˆˆRNuÃ—C, where Nu is the\ntotal number of image units, i.e., points in the feature maps.\nThe transformer consists of a few stacked transformer lay-\ners. Each layer takes input features X, computes the pair-\nwise interactions between input units, and outputs contex-\ntualized features Xout of the same number of units. The\ntransformer layers used are our novel design Squeeze-and-\nExpansion Transformer (Section 3).\n4.3 Feature Pyramids and Segmentation Head\nAlthough the spatial resolution of features is not reduced after\npassing through the transformer layers, for richer semantics,\nthe input features to transformers are usually high-level fea-\ntures from the backbone. They are of a low spatial resolution,\nhowever. Hence, we increase their spatial resolution with an\ninput Feature Pyramid Network (FPN) [Liu et al., 2018] and\nan output FPN, which upsample the feature maps at the trans-\nformer input end and output end, respectively.\nWithout loss of generality, let us assume the Efï¬cientNet\nis the backbone. The stages 3, 4, 6, and 9 of the network\nare commonly used to extract multi-scale feature maps. Let\nus denote the corresponding feature maps as f1, f2, f3, f4,\nrespectively. Their shapes are fi âˆˆRCiÃ—HiÃ—Wi , with Hi =\nH0\n2i , Wi = W0\n2i .\nAs described above, f(X0) = f4 is 1/16 of the original\nimage, which is too coarse for accurate segmentation. Hence,\nwe upsample it with an input FPN, and obtain upsampled\nfeature maps f34:\nf34 = upsampleÃ—2(f4) + conv34(f3), (11)\nwhere conv34 is a 1 Ã—1 convolution that aligns the channels\nof f3 to f4, and upsampleÃ—2(Â·) is bilinear interpolation.\nf34 is 1/8 of the original image, and is used as the input\nfeatures to the transformer layers. As the transformer layers\nkeep the spatial resolutions unchanged from input to output\nfeature maps, the output feature maps g34 is also 1/8 of the\ninput image. Still, this spatial resolution is too low for seg-\nmentation. Therefore, we adopt an output FPN to upsample\nthe feature maps by a factor of 4 (i.e., 1/2 of the original im-\nages). The output FPN consists of two upsampling steps:\nf12 = upsampleÃ—2(f2) + conv12(f1),\ng1234 = upsampleÃ—4(g34) + conv24(f12), (12)\nwhere conv12 and conv24 are 1 Ã—1 convolutional layers that\nalign the channels of f1 to f2, and f2 to f4, respectively.\nFigure 5: Top: Optic disc/cup segmentation in fundus images into 3\nclasses: disc (grey), cup (white), and background (black). Bottom:\nPolyp segmentation in colonoscopy images into 2 classes: polyp\n(white) and background (black).\nThis FPN scheme is the bottom-up FPN proposed in [Liu\net al., 2018]. Empirically, it performs better than the original\ntop-down FPN [Lin et al., 2017], as richer semantics in top\nlayers are better preserved.\nThe segmentation head is simply a 1 Ã—1 convolutional\nlayer, outputting conï¬dence scores of each class in the mask.\n5 Experiments\nThree tasks were evaluated in our experiments:\nREFUGE20: Optic Disc/Cup Segmentation in Fundus\nImages. This task does segmentation of the optic disc and\ncup in fundus images, which are 2D images of the rear of eyes\n(Fig. 5). It is a subtask of the REFUGE Challenge 2 [Orlando\net al. , 2020 ], MICCAI 2020. 1200 images were provided\nfor training, and 400 for validation. We also used two ex-\ntra datasets, Drishti-GS dataset [Sivaswamy et al., 2015] and\nRIM-ONE v3 [Fumero et al., 2011] when training all models.\nThe Disc/Cup dice scores of validation images were obtained\nfrom the ofï¬cial evaluation server.\nPolyp: Polyp Segmentation in Colonoscopy Images.\nPolyps are ï¬‚eshy growths in the colon lining that may become\ncancerous. This task does polyp segmentation in colonoscopy\nimages (Fig. 5). Two image datasets [Fan et al., 2020] were\nused: CVC612 (CVC in short; 612 images) andKvasir (1000\nimages). Each was randomly split into 80% training and 20%\nvalidation, and the training images were merged into one set.\nBraTS19: Tumor Segmentation in MRI Images. This\ntask focuses on the segmentation of gliomas, a common brain\ntumor in MRI scans. It was a subtask of the BraTSâ€™19 chal-\nlenge3 [Menze et al. , 2015; Bakas et al. , 2017 ], MICCAI\n2019. It involves four classes: the whole tumor ( WT), the\ntumor core (TC), the enhancing tumor (ET) and background.\nAmong them, the tumor core consists of the necrotic regions\nand non-enhancing tumors (red), as well as the enhancing tu-\nmor (yellow). 335 scans were provided for training, and 125\n2https://refuge.grand-challenge.org/Home2020/\n3https://www.med.upenn.edu/cbica/brats-2019/\nfor validation. The dice scores of ET, WT and TC on the vali-\ndation scans were obtained from the ofï¬cial evaluation server.\n5.1 Ablation Studies\nTwo ablation studies were performed on REFUGE20 to\ncompare: 1) the Squeeze-and-Expansion Transformer versus\nMulti-Head Transformer; and 2) the Learnable Sinusoidal Po-\nsitional Encoding versus two schemes as well as not using PE.\nAll the settings were variants of the standard one, which\nused three layers of Squeeze-and-Expansion transformer with\nfour modes (Nm = 4), along with learnable sinusoidal posi-\ntional encoding. Both ResNet-101 and Efï¬cientNet-B4 were\nevaluated to reduce random effects from choices of the back-\nbone. We only reported the cup dice scores, as the disc seg-\nmentation task was relatively easy, with dice scores only vary-\ning Â±0.005 across most settings.\nType of Transformer Layers. Table 1 shows that Squeeze-\nand-Expansion transformer outperformed the traditional\nmulti-head transformers. Moreover, Both the squeeze atten-\ntion block and the expansion attention block contributed to\nimproved performance.\nTransformer Type ResNet-101 Eff-B4\nCell-DETR (Nh = 4) 0.846 0.857\nMulti-Head (Nh = 4) 0.858 0.862\nNo squeeze + Expansion (Nm = 4) 0.840 0.872\nSqueeze + Single-Mode 0.859 0.868\nSqueeze + Expansion (Nm = 4) 0.862 0.872\nTable 1: REFUGEâ€™20 Fundus Optic Cup dice scores change with\nthe type of transformer layers. Single-Mode implies No Expansion.\nCell-DETR uses a multi-head transformer and discretely learned PE.\nNh: number of attention heads in a MHA. Nm: number of modes\nin a Squeeze-and-Expansion transformer.\nPositional Encoding. Table 2 compares learnable sinu-\nsoidal positional encoding with the two mainstream PE\nschemes and no PE. Surprisingly, without PE, performance\nof Segtran only dropped 1~2%. A possible explanation is that\nthe transformer may manage to extract positional information\nfrom the CNN backbone features [Islam et al., 2020].\nPositional Encoding ResNet-101 Eff-B4\nNone 0.857 0.853\nDiscretely learned 0.852 0.860\nFixed Sinusoidal 0.857 0.849\nLearnable Sinusoidal 0.862 0.872\nTable 2: REFUGEâ€™20 Fundus Optic Cup dice scores change with\nthe type of positional encoding (PE) schemes.\nNumber of Transformer Layers. Table 3 shows that as\nthe number of transformer layers increased from 1 to 3, the\nperformance improved gradually. However, one more layer\ncaused performance drop, indicating possible overï¬tting.\n5.2 Comparison with Baselines\nTen methods were evaluated on the 2D segmentation tasks:\nNumber of layers ResNet101 Eff-B4\n1 0.856 0.854\n2 0.862 0.857\n3 0.862 0.872\n4 0.855 0.869\nTable 3: REFUGE20 Optic Cup dice scores change with the num-\nber of transformer layers. Best performers with each backbone are\nhighlighted.\nâ€¢ U-Net [Ronneberger et al. , 2015 ]: The implementa-\ntion in a popular library Segmentation Models.PyTorch\n(SMP) was used4. The pretrained ResNet-101 was cho-\nsen as the encoder. In addition, U-Net implemented in\nU-Net++ (below) was evaluated as training from scratch.\nâ€¢ U-Net++ [Zhou et al., 2018]: A popular PyTorch imple-\nmentation5. It does not provide options to use pretrained\nencoders, and thus was only trained from scratch.\nâ€¢ U-Net3+ [Huang et al., 2020]: The ofï¬cial PyTorch im-\nplementation6. It does not provide options to use pre-\ntrained encoders.\nâ€¢ PraNet [Fan et al. , 2020 ]: The ofï¬cial PyTorch im-\nplementation7. The pretrained Res2Net-50 [Gao et al.,\n2020] was recommended to be used as the encoder.\nâ€¢ DeepLabV3+ [Chen et al., 2018]: A popular PyTorch\nimplementation8, with a pretrained ResNet-101 as the\nencoder.\nâ€¢ Attention based U-Nets [Oktay et al. , 2018 ]: Atten-\ntion U-Net (AttU-Net) and AttR2U-Net (a combination\nof AttU-Net and Recurrent Residual U-Net) were eval-\nuated9. They learn to focus on important areas by com-\nputing element-wise attention weights (as opposed to the\npairwise attention of transformers).\nâ€¢ nnU-Net [Isensee et al. , 2021 ]: nnU-Net generates a\ncustom U-Net conï¬guration for each dataset based on its\nstatistics. It is primarily designed for 3D tasks, but can\nalso handle 2D images after converting them to pseudo-\n3D. The original pipeline is time-consuming, and we ex-\ntracted the generated U-Net conï¬guration and instanti-\nated it in our pipeline to do training and test.\nâ€¢ Deformable U-Net [Jin et al. , 2019 ]: Deformable U-\nNet (DUNet) uses deformable convolution in place of\nordinary convolution. The ofï¬cial implementation 10 of\nDUNet was evaluated.\nâ€¢ SETR [Zheng et al. , 2021 ]: SETR uses ViT as the\nencoder, and a few convolutional layers as the de-\ncoder. The SETR-PUP model in the ofï¬cial implemen-\ntation11 was evaluated, by ï¬ne-tuning the pretrained ViT\n4https://github.com/qubvel/segmentation_models.pytorch/\n5https://github.com/4uiiurz1/pytorch-nested-unet\n6https://github.com/ZJUGiveLab/UNet-Version\n7https://github.com/DengPingFan/PraNet\n8hhttps://github.com/VainF/DeepLabV3Plus-Pytorch\n9https://github.com/LeeJunHyun/Image_Segmentation\n10https://github.com/RanSuLab/DUNet-retinal-vessel-detection\n11https://github.com/fudan-zvg/SETR/\nREFUGE20 Polyp Avg.\nCup Disc Kvasir CVC\nU-Net 0.730 0.946 0.787 0.771 0.809\nU-Net (R101) 0.837 0.950 0.868 0.844 0.875\nU-Net++ 0.781 0.940 0.753 0.740 0.804\nU-Net3+ 0.819 0.943 0.708 0.680 0.788\nPraNet (res2net50) 0.781 0.946 0.898 0.899 0.881\nDeepLabV3+ (R101) 0.839 0.950 0.805 0.795 0.847\nAttU-Net 0.846 0.952 0.744 0.749 0.823\nAttR2U-Net 0.818 0.944 0.686 0.632 0.770\nDUNet 0.826 0.945 0.748 0.754 0.818\nnnU-Net 0.829 0.953 0.857 0.864 0.876\nSETR (ViT) 0.859 0.952 0.894 0.916 0.905\nTransU-Net (R50+ViT) 0.835 0.958 0.895 0.916 0.901\nSegtran (R101) 0.862 0.956 0.888 0.929 0.909\nSegtran (eff-B4) 0.872 0.961 0.903 0.931 0.917\nTable 4: Dice scores on REFUGE20 and Polyp validation sets.\nR101: ResNet-101; R50: ResNet-50; eff-B4: Efï¬cientNet-B4.\nweights.\nâ€¢ TransU-Net [Chen et al., 2021]: TransU-Net uses a hy-\nbrid of ResNet and ViT as the encoder, and a U-Net style\ndecoder. The ofï¬cial implementation 12 was evaluated,\nby ï¬ne-tuning their pretrained weights.\nâ€¢ Segtran: Trained with either a pretrained ResNet-101 or\nEfï¬cientNet-B4 as the backbone.\nThree methods were evaluated on the 3D segmentation task:\nâ€¢ Extension of nnU-Net [Wang et al., 2019]: An exten-\nsion of the nnU-Net13 with two sampling strategies.\nâ€¢ Bag of tricks (2nd place solution of the BraTSâ€™19\nchallenge) [Zhao et al., 2019]: The winning entry used\nan ensemble of ï¬ve models. For fairness, we quoted the\nbest single-model results (â€œBL+warmupâ€).\nâ€¢ Segtran-3D: I3D [Carreira and Zisserman, 2017 ] was\nused as the backbone.\n5.3 Training Protocols\nAll models were trained on a 24GB Titan RTX GPU with\nthe AdamW optimizer. The learning rate for the three\ntransformer-based models were 0.0002, and 0.001 for the\nother models. On REFUGE20, all models were trained with\na batch size of 4 for 10,000 iterations (27 epochs); on Polyp,\nthe total iterations were 14,000 (31 epochs). On BraTS19,\nSegtran was trained with a batch size of 4 for 8000 iterations.\nThe training loss was the average of the pixel-wise cross-\nentropy loss and the dice loss. Segtran used 3 transformer\nlayers on 2D images, and 1 layer on 3D images to save RAM.\nThe number of modes in each transformer layer was 4.\n5.4 Results\nTables 4 and 5 present the evaluation results on the 2D\nand 3D tasks, respectively. Overall, the three transformer\n12https://github.com/Beckschen/TransUNet\n13https://github.com/woodywff/brats_2019\nBraTS19\nET WT TC Avg.\nExtension of nnU-Net 0.737 0.894 0.807 0.813\nBag of tricks 0.729 0.904 0.802 0.812\nSegtran (i3d) 0.740 0.895 0.817 0.817\nTable 5: Dice scores on BraTS19 validation set. Only single-model\nperformance is reported.\nbased methods, i.e., SETR, TransU-Net and Segtran achieved\nbest performance across all tasks. With ResNet-101 as the\nbackbone, Segtran performed slightly better than SETR and\nTransU-Net. With Efï¬cientNet-B4, Segtran exhibited greater\nadvantages.\nIt is worth noting that, Segtran (eff-B4) was among\nthe top 5 teams in the semiï¬nal and ï¬nal leaderboards of\nthe REFUGE20 challenge. Among either REFUGE20 or\nBraTS19 challenge participants, although there were several\nmethods that performed slightly better than Segtran, they\nusually employed ad-hoc tricks and designs [Orlando et al.,\n2020; Wang et al., 2019; Zhao et al., 2019]. In contrast, Seg-\ntran achieved competitive performance with the same archi-\ntecture and minimal hyperparameter tuning, free of domain-\nspeciï¬c strategies.\n(a) RIM-One image    (b) Ground truth          (c) U-Net              (d) DeepLabV3+ (e) Segtran\nFigure 6: Soft segmentation masks produced by different methods\non a RIM-One image. The mask by Segtran has the fewest artifacts.\n5.5 Cross-Domain Generalization\nTo explore how well different methods generalize to new\ndomains, we trained three representative methods, U-Net,\nDeepLabV3+ and Segtran on the 1200 training images of\nREFUGE20. All the methods used a pretrained ResNet-101\nas the encoder/backbone. The trained models were evaluated\non both the REFUGE20 training images and the RIM-One\ndataset [Fumero et al., 2011]. As RIM-One images have dras-\ntically different characteristics from REFUGE20, all mod-\nels suffered severe performance drop, as shown in Table 6.\nNevertheless, Segtran had the least performance degradation,\nshowing the best cross-domain generalization. Fig.6 shows\na RIM-One image and the corresponding soft segmentation\nmasks (before thresholding) produced by different methods.\nThe mask produced by Segtran contains the fewest artifacts.\n5.6 Computational Efï¬ciency\nTable 7 presents the number of parameters and FLOPs of a\nfew representative methods. In general, transformer-based\nmethods consume more computation and GPU RAM than\nconventional methods.\nOur proï¬ling showed that the number of parame-\nters/FLOPs of Segtran are dominated by the output FPN,\nwhich vary drastically across different backbones. As the\nREFUGE RIM-One Drop\nU-Net 0.862 0.680 -0.182\nDeepLabV3+ 0.846 0.653 -0.193\nSegtran 0.938 0.796 -0.142\nTable 6: Generalization of three methods, measured by drop of Optic\nCup dice scores from the REFUGE20 training images to a new test\ndomain RIM-One. The smaller the drop is, the better. All used\nResNet-101 as the encoder/backbone.\nbottom-up FPNs we adopt are somewhat similar to Efï¬-\ncientDet [Tan et al., 2020], the model size/FLOPs are opti-\nmal when using Efï¬cientNets. With ResNets as the back-\nbone, Segtran has a signiï¬cantly higher model size/FLOPs,\nand hence this choice of backbone is not recommended for\nefï¬ciency-sensitive scenarios.\nParams (M) FLOPs (G)\nnnU-Net 41.2 16.3\nAttU-Net 34.9 51.0\nSETR (ViT) 307.1 91.1\nTransU-Net (R50+ViT) 93.2 32.2\nSegtran (R101) 166.7 152.8\nSegtran (eff-B4) 93.1 71.3\nTable 7: Number of parameters / FLOPs on a 256x256 input image.\n5.7 Impact of Pretraining\nModels for medical image tasks usually beneï¬t from initial-\nization with weights pretrained on natural images (e.g. Ima-\ngeNet [Deng et al., 2009]), as medical image datasets are typ-\nically small. To quantitatively study the impact of pretrain-\ning, Table 8 compares the performance of using pretrained\nweights vs. training from scratch of a few methods. Pretrain-\ning brought ~2.5% increase of average dice scores to the two\ntransformer-based models, and 1% to U-Net (ResNet-101).\nREFUGE20 Polyp Avg.\nCup Disc Kvasir CVC\nU-Net (R101 scratch) 0.827 0.953 0.847 0.835 0.865\nU-Net (R101 pretrain) 0.837 0.950 0.868 0.844 0.875\nTransU-Net (R50+ViT\nscratch) 0.817 0.943 0.869 0.872 0.875\nTransU-Net (R50+ViT\npretrained) 0.835 0.958 0.895 0.916 0.901\nSegtran (R101 scratch) 0.852 0.939 0.858 0.851 0.875\nSegtran (R101 pretrain) 0.862 0.956 0.888 0.929 0.909\nTable 8: Impact of using pretrained encoder weights.\n6 Conclusions\nIn this work, we present Segtran, a transformer-based medi-\ncal image segmentation framework. It leverages unlimited re-\nceptive ï¬elds of transformers to contextualize features. More-\nover, the transformer is an improved Squeeze-and-Expansion\ntransformer that better ï¬ts image tasks. Segtran sees both the\nglobal picture and ï¬ne details, lending itself good segmen-\ntation performance. On two 2D and one 3D medical image\nsegmentation tasks, Segtran consistently outperformed exist-\ning methods, and generalizes well to new domains.\nAcknowledgements\nWe are grateful for the help and support of Wei Jing. This\nresearch is supported by A*STAR under its Career Develop-\nment Award (Grant No. C210112016), and its Human-Robot\nCollaborative Al for Advanced Manufacturing and Engineer-\ning (AME) programme (Grant No. A18A2b0046).\nReferences\n[Baheti et al., 2020] B. Baheti, S. Innani, S. Gajre, and S. Talbar.\nEff-unet: A novel architecture for semantic segmentation in un-\nstructured environment. In CVPR Workshops, 2020.\n[Bakas et al., 2017] S. Bakas, H. Akbari, A. Sotiras, M. Bilello,\nM. Rozycki, J. S. Kirby, J. B. Freymann, K. Farahani, and C. Da-\nvatzikos. Advancing The Cancer Genome Atlas glioma MRI col-\nlections with expert segmentation labels and radiomic features.\nNature Scientiï¬c Data, 4, 2017.\n[Carion et al., 2020] Nicolas Carion, Francisco Massa, Gabriel\nSynnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey\nZagoruyko. End-to-End Object Detection with Transformers. In\nECCV, 2020.\n[Carreira and Zisserman, 2017] J. Carreira and A. Zisserman. Quo\nvadis, action recognition? a new model and the kinetics dataset.\nIn CVPR, 2017.\n[Chen et al., 2018] Liang-Chieh Chen, Yukun Zhu, George Papan-\ndreou, Florian Schroff, and Hartwig Adam. Encoder-decoder\nwith atrous separable convolution for semantic image segmen-\ntation. In ECCV, 2018.\n[Chen et al., 2021] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde\nLuo, Ehsan Adeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin\nZhou. Transunet: Transformers make strong encoders for medi-\ncal image segmentation. arXiv preprint arXiv:2102.04306, 2021.\n[Ã‡iÃ§ek et al., 2016] Ã–zgÃ¼n Ã‡iÃ§ek, Ahmed Abdulkadir, Soeren S.\nLienkamp, Thomas Brox, and Olaf Ronneberger. 3d u-net:\nLearning dense volumetric segmentation from sparse annotation.\nIn Sebastien Ourselin, Leo Joskowicz, Mert R. Sabuncu, Gozde\nUnal, and William Wells, editors, MICCAI, 2016.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li,\nKai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical im-\nage database. In CVPR, 2009.\n[Dosovitskiy et al., 2021] Alexey Dosovitskiy, Lucas Beyer,\nAlexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, et al.\nAn image is worth 16x16 words: Transformers for image\nrecognition at scale. In ICLR, 2021.\n[Esser et al., 2020] Patrick Esser, Robin Rombach, and BjÃ¶rn Om-\nmer. Taming transformers for high-resolution image synthesis.\narxiv:2012.09841, 2020.\n[Fan et al., 2020] Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng\nChen, Huazhu Fu, Jianbing Shen, and Ling Shao. Pranet: Paral-\nlel reverse attention network for polyp segmentation. InMICCAI,\n2020.\n[Fumero et al., 2011] F. Fumero, S. Alayon, J. L. Sanchez, J. Sigut,\nand M. Gonzalez-Hernandez. Rim-one: An open retinal image\ndatabase for optic nerve evaluation. In 24th International Sym-\nposium on CBMS, 2011.\n[Gao et al., 2020] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao,\nXin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr. Res2net: A\nnew multi-scale backbone architecture. IEEE TPAMI, 43, 2020.\n[Huang et al., 2020] H. Huang, L. Lin, R. Tong, H. Hu, Q. Zhang,\nY . Iwamoto, X. Han, Y . Chen, and J. Wu. Unet 3+: A full-scale\nconnected unet for medical image segmentation. In ICASSP,\n2020.\n[Isensee et al., 2021] Fabian Isensee, Paul F. Jaeger, Simon A. A.\nKohl, Jens Petersen, and Klaus H. Maier-Hein. nnu-net: a self-\nconï¬guring method for deep learning-based biomedical image\nsegmentation. Nature Methods, 18, 2021.\n[Islam et al., 2020] Md Amirul Islam, Sen Jia, and Neil DB Bruce.\nHow much position information do convolutional neural net-\nworks encode? In ICLR, 2020.\n[Jin et al., 2019] Qiangguo Jin, Zhaopeng Meng, Tuan D. Pham,\nQi Chen, Leyi Wei, and Ran Su. Dunet: A deformable network\nfor retinal vessel segmentation.Knowledge-Based Systems, 2019.\n[Kirillov et al., 2019] Alexander Kirillov, Kaiming He, Ross B.\nGirshick, Carsten Rother, and Piotr DollÃ¡r. Panoptic segmen-\ntation. In CVPR, 2019.\n[Lee et al., 2019] Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Ko-\nsiorek, Seungjin Choi, and Yee Whye Teh. Set transformer: A\nframework for attention-based permutation-invariant neural net-\nworks. In ICML, 2019.\n[Lin et al., 2017] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaim-\ning He, Bharath Hariharan, and Serge Belongie. Feature pyramid\nnetworks for object detection. In CVPR, July 2017.\n[Liu and Guo, 2020] Sunâ€™ao Liu and Xiaonan Guo. Improving\nbrain tumor segmentation with multi-direction fusion and ï¬ne\nclass prediction. In BrainLes workshop, MICCAI, 2020.\n[Liu et al., 2018] S. Liu, L. Qi, H. Qin, J. Shi, and J. Jia. Path ag-\ngregation network for instance segmentation. In CVPR, 2018.\n[Luo et al., 2016] Wenjie Luo, Yujia Li, Raquel Urtasun, and\nRichard Zemel. Understanding the effective receptive ï¬eld in\ndeep convolutional neural networks. In NeurIPS, 2016.\n[Menze et al., 2015] Bjoern H. Menze, Andras Jakab, Stefan Bauer,\nJayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, et al.\nThe multimodal brain tumor image segmentation benchmark\n(brats). IEEE TMI, 34, 2015.\n[Milletari et al., 2016] F. Milletari, N. Navab, and S. Ahmadi. V-\nnet: Fully convolutional neural networks for volumetric medical\nimage segmentation. In 3DV, 2016.\n[Murase et al., 2020] Rito Murase, Masanori Suganuma, and\nTakayuki Okatani. How Can CNNs Use Image Position for Seg-\nmentation? arXiv:2005.03463, 2020.\n[Oktay et al., 2018] Ozan Oktay, Jo Schlemper, LoÃ¯c Le Folgoc,\nMatthew C. H. Lee, Mattias P. Heinrich, Kazunari Misawa, Ken-\nsaku Mori, et al. Attention u-net: Learning where to look for the\npancreas. In MIDL, 2018.\n[Orlando et al., 2020] JosÃ© Ignacio Orlando, Huazhu Fu, JoÃ£o Bar-\nbosa Breda, Karel van Keer, Deepti R. Bathula, AndrÃ©s Diaz-\nPinto, et al. Refuge challenge: A uniï¬ed framework for evalu-\nating automated methods for glaucoma assessment from fundus\nphotographs. Medical Image Analysis, 59, 2020.\n[Prangemeier et al., 2020] Tim Prangemeier, Christoph Reich, and\nHeinz Koeppl. Attention-based transformers for instance seg-\nmentation of cells in microstructures. InIEEE International Con-\nference on Bioinformatics and Biomedicine, 2020.\n[Ronneberger et al., 2015] Olaf Ronneberger, Philipp Fischer, and\nThomas Brox. U-net: Convolutional networks for biomedical\nimage segmentation. In MICCAI, 2015.\n[Schlemper et al., 2019] Jo Schlemper, Ozan Oktay, Michiel\nSchaap, Mattias Heinrich, Bernhard Kainz, Ben Glocker, and\nDaniel Rueckert. Attention gated networks: Learning to lever-\nage salient regions in medical images. Medical Image Analysis,\n53, 2019.\n[Shazeer et al., 2017] Noam Shazeer, Azalia Mirhoseini, Krzysztof\nMaziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean.\nOutrageously large neural networks: The sparsely-gated mixture-\nof-experts layer. In ICLR, 2017.\n[Sivaswamy et al., 2015] Jayanthi Sivaswamy, Subbaiah Krish-\nnadas, Arunava Chakravarty, Gopal Joshi, and Ujjwal. A com-\nprehensive retinal image dataset for the assessment of glaucoma\nfrom the optic nerve head analysis. JSM Biomedical Imaging\nData Papers, 2, 2015.\n[Tan and Le, 2019] Mingxing Tan and Quoc V . Le. Efï¬cientnet:\nRethinking model scaling for convolutional neural networks. In\nICML, 2019.\n[Tan et al., 2020] Mingxing Tan, Ruoming Pang, and Quoc V . Le.\nEfï¬cientdet: Scalable and efï¬cient object detection. In CVPR,\nJune 2020.\n[Tancik et al., 2020] Matthew Tancik, Pratul P. Srinivasan, Ben\nMildenhall, Sara Fridovich-Keil, Nithin Raghavan, Utkarsh Sing-\nhal, Ravi Ramamoorthi, Jonathan T. Barron, and Ren Ng. Fourier\nFeatures Let Networks Learn High Frequency Functions in Low\nDimensional Domains. In NeurIPS, 2020.\n[Vaswani et al., 2017] Ashish Vaswani, Noam Shazeer, Niki Par-\nmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, undeï¬ne-\ndukasz Kaiser, and Illia Polosukhin. Attention is all you need. In\nNeurIPS, 2017.\n[V oitaet al., 2019] Elena V oita, David Talbot, Fedor Moiseev, Rico\nSennrich, and Ivan Titov. Analyzing multi-head self-attention:\nSpecialized heads do the heavy lifting, the rest can be pruned. In\nACL, 2019.\n[Wang et al., 2019] Feifan Wang, Runzhou Jiang, Liqin Zheng,\nChun Meng, and Bharat Biswal. 3d u-net based brain tumor seg-\nmentation and survival days prediction. In BrainLes Workshop,\nMICCAI, 2019.\n[Zhang et al., 2020a] Dong Zhang, Hanwang Zhang, Jinhui Tang,\nMeng Wang, Xiansheng Hua, and Qianru Sun. Feature pyramid\ntransformer. In ECCV, 2020.\n[Zhang et al., 2020b] Hang Zhang, Chongruo Wu, Zhongyue\nZhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas\nMuller, R. Manmatha, Mu Li, and Alexander Smola. Resnest:\nSplit-attention networks. arXiv:2004.08955, 2020.\n[Zhao et al., 2019] Yuan-Xing Zhao, Yan-Ming Zhang, and Cheng-\nLin Liu. Bag of tricks for 3d mri brain tumor segmentation. In\nBrainles Workshop, MICCAI, 2019.\n[Zheng et al., 2021] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao,\nXiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng\nFeng, Tao Xiang, Philip H.S. Torr, and Li Zhang. Rethinking\nsemantic segmentation from a sequence-to-sequence perspective\nwith transformers. In CVPR, 2021.\n[Zhou et al., 2018] Zongwei Zhou, Md Mahfuzur Rahman Sid-\ndiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: A nested\nu-net architecture for medical image segmentation. In DLMIA\nworkshop (MICCAI), 2018."
}