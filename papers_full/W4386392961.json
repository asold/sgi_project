{
  "title": "A Real-world Dataset and Benchmark For Foundation Model Adaptation in Medical Image Classification",
  "url": "https://openalex.org/W4386392961",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2146637585",
      "name": "Dequan Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory",
        "Shanghai Jiao Tong University"
      ]
    },
    {
      "id": "https://openalex.org/A2111650899",
      "name": "Xiaosong Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2119474867",
      "name": "Lilong Wang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2781480476",
      "name": "Mengzhang Li",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2354007410",
      "name": "Qian Da",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Ruijin Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2118932331",
      "name": "Xiaoqiang Liu",
      "affiliations": [
        "Shanghai Tenth People's Hospital",
        "Tongji University"
      ]
    },
    {
      "id": "https://openalex.org/A2097355614",
      "name": "Xiangyu Gao",
      "affiliations": [
        "Xuzhou Central Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A1988806642",
      "name": "Jun Shen",
      "affiliations": [
        "Shanghai Jiao Tong University",
        "Renji Hospital"
      ]
    },
    {
      "id": "https://openalex.org/A2136798775",
      "name": "Junjun He",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2001067978",
      "name": "Tian Shen",
      "affiliations": [
        "Group Sense (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2097586824",
      "name": "Qi Duan",
      "affiliations": [
        "Group Sense (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2100572704",
      "name": "Jie Zhao",
      "affiliations": [
        "First Affiliated Hospital of Zhengzhou University"
      ]
    },
    {
      "id": "https://openalex.org/A2047264473",
      "name": "Kang Li",
      "affiliations": [
        "Sichuan University",
        "West China Hospital of Sichuan University",
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2106105882",
      "name": "Yu Qiao",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2171003757",
      "name": "Shaoting Zhang",
      "affiliations": [
        "Shanghai Artificial Intelligence Laboratory"
      ]
    },
    {
      "id": "https://openalex.org/A2146637585",
      "name": "Dequan Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2111650899",
      "name": "Xiaosong Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2119474867",
      "name": "Lilong Wang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2781480476",
      "name": "Mengzhang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2354007410",
      "name": "Qian Da",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2118932331",
      "name": "Xiaoqiang Liu",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097355614",
      "name": "Xiangyu Gao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1988806642",
      "name": "Jun Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2136798775",
      "name": "Junjun He",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2001067978",
      "name": "Tian Shen",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2097586824",
      "name": "Qi Duan",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2100572704",
      "name": "Jie Zhao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2047264473",
      "name": "Kang Li",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2106105882",
      "name": "Yu Qiao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2171003757",
      "name": "Shaoting Zhang",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3046375318",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2253429366",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W6676297131",
    "https://openalex.org/W6778883912",
    "https://openalex.org/W3108975329",
    "https://openalex.org/W3042394582",
    "https://openalex.org/W3097337894",
    "https://openalex.org/W3175581533",
    "https://openalex.org/W3198377975",
    "https://openalex.org/W4240153047",
    "https://openalex.org/W4312310776",
    "https://openalex.org/W3159481202",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W6803870738",
    "https://openalex.org/W6810601475",
    "https://openalex.org/W3112191514",
    "https://openalex.org/W4281932055",
    "https://openalex.org/W2611650229",
    "https://openalex.org/W2977613223",
    "https://openalex.org/W2904183610",
    "https://openalex.org/W2963466845",
    "https://openalex.org/W2913279579",
    "https://openalex.org/W2995225687",
    "https://openalex.org/W6797395470",
    "https://openalex.org/W2794825826",
    "https://openalex.org/W4282933032",
    "https://openalex.org/W6921324023",
    "https://openalex.org/W3205249428",
    "https://openalex.org/W4214708455",
    "https://openalex.org/W4312804044",
    "https://openalex.org/W6803924480",
    "https://openalex.org/W4212774754",
    "https://openalex.org/W3102785203",
    "https://openalex.org/W3101156210"
  ],
  "abstract": "Abstract Foundation models, often pre-trained with large-scale data, have achieved paramount success in jump-starting various vision and language applications. Recent advances further enable adapting foundation models in downstream tasks efficiently using only a few training samples, e.g., in-context learning. Yet, the application of such learning paradigms in medical image analysis remains scarce due to the shortage of publicly accessible data and benchmarks. In this paper, we aim at approaches adapting the foundation models for medical image classification and present a novel dataset and benchmark for the evaluation, i.e., examining the overall performance of accommodating the large-scale foundation models downstream on a set of diverse real-world clinical tasks. We collect five sets of medical imaging data from multiple institutes targeting a variety of real-world clinical tasks (22,349 images in total), i.e., thoracic diseases screening in X-rays, pathological lesion tissue screening, lesion detection in endoscopy images, neonatal jaundice evaluation, and diabetic retinopathy grading. Results of multiple baseline methods are demonstrated using the proposed dataset from both accuracy and cost-effective perspectives.",
  "full_text": "1Scientific  Data |          (2023) 10:574  | https://doi.org/10.1038/s41597-023-02460-0\nwww.nature.com/scientificdata\na Real-world Dataset and \nBenchmark For Foundation  \nModel adaptation in Medical  \nImage Classification\nDequan Wang1,2,10, Xiaosong Wang  1,10, Lilong Wang1, Mengzhang Li1, Qian Da3, \nXiaoqiang Liu4, Xiangyu Gao5, Jun Shen6, Junjun He1, Tian Shen7, Qi Duan7, Jie Zhao8,  \nKang Li  1,9, Yu Qiao1 ✉ & Shaoting Zhang1 ✉\nFoundation models, often pre-trained with large-scale data, have achieved paramount success in jump-\nstarting various vision and language applications. Recent advances further enable adapting foundation \nmodels in downstream tasks efficiently using only a few training samples, e.g., in-context learning. \nYet, the application of such learning paradigms in medical image analysis remains scarce due to the \nshortage of publicly accessible data and benchmarks. In this paper, we aim at approaches adapting the \nfoundation models for medical image classification and present a novel dataset and benchmark for \nthe evaluation, i.e., examining the overall performance of accommodating the large-scale foundation \nmodels downstream on a set of diverse real-world clinical tasks. We collect five sets of medical imaging \ndata from multiple institutes targeting a variety of real-world clinical tasks (22,349 images in total), \ni.e., thoracic diseases screening in X-rays, pathological lesion tissue screening, lesion detection in \nendoscopy images, neonatal jaundice evaluation, and diabetic retinopathy grading. Results of multiple \nbaseline methods are demonstrated using the proposed dataset from both accuracy and cost-effective \nperspectives.\nBackground & Summary\nIn the new trend of training even larger and universal foundation models (e.g., Vision Transformers 1, GPTs2, \nPubmedBERT3, and CLIP4) using thousands of millions of data samples (sometimes in multiple modalities), \ndeveloping cost-effective model adaptation methods for detailed applications become the new gold, especially \nwhen it only demands very few data samples. On the other side, the shortage of publicly accessible datasets \nin medical imaging has largely blocked the development and application of large-scale deep learning models \n(training from scratch) in many clinical downstream tasks. It is because obtaining quality annotations remains \na tedious task for medical professionals, e.g., hand-label volumetric data repeatedly. Providing a few textbook \nsample cases is more logically feasible and complies with the training process of medical residents. In the domain \nof medical image analysis, it is even more valuable to promote such learning paradigms when diseased cases are \noften rare in comparison to the numerous amount of normal population.\nThe common fine-tuning scheme\n5 with ImageNet6 pre-trained models can diminish the need of large-scale \ndata for the train-from-scratch scheme. However, it still requires a fair amount of data for faster fine-tuning \nwhile avoiding overfitting. Alternatively, few-shot methods could leverage more on the distinctive representa\n-\ntion produced by the foundation models, which has succeeded in considerable language modeling 7 and \nvision8,9 tasks. The existing techniques of adapting foundation models in medical image analysis 10,11 demand  \n1Shanghai Ai  Laboratory, Shanghai, china. 2Shanghai Jiaotong University, Shanghai, china. 3Shanghai Ruijing \nHospital, School of Medicine, Shanghai Jiao tong University, Shanghai, china. 4Shanghai tenth People’s Hospital of \ntongji University, Shanghai, china. 5Xuzhou central Hospital, Xuzhou, china. 6Renji Hospital, School of Medicine, \nShanghai Jiao tong University, Shanghai, china. 7Sensetime Research, Shanghai, china. 8The First Affiliated Hospital of \nZhengzhou University, Zhengzhou, china. 9West china Hospital, Sichuan University, chengdu, china. 10these authors \ncontributed equally: Dequan Wang, Xiaosong Wang. ✉e-mail: qiaoyu@pjlab.org.cn; zhangshaoting@pjlab.org.cn\nData DeScR iptoR\nopeN\n\n2Scientific  Data |          (2023) 10:574  | https://doi.org/10.1038/s41597-023-02460-0\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nthe employment of dedicated medical pre-trained models that is hard to produce even if self-supervised learn-\ning is utilized. Recently, cutting-edge techniques, e.g., prompt-based learning12,13, can leverage the foundation \nmodels pre-trained (via self-supervised learning, e.g., DINO 14 and MAE15) using vast amounts of data from \nmultiple modalities and domains and transfer these universal representations to tasks with very limited data16,17. \nThe fundamental difference in technical routine has started reshaping the landscape of medical image analysis. \nTherefore, it is in urgent demand to set up datasets and benchmarks to promote innovation in this fast-marching \nresearch field and properly evaluate the performance gain and other cost-effective aspects. There are bench\n-\nmarks18,19 for the few-shot learning tasks. Nonetheless, they focus more on each individual data modality and \ntask. Here, we will instead promote the generalizability of the few-shot learning methods, i.e., strengthening \ntheir overall performance on various data modalities and tasks.\nIn this paper, we proposed a novel dataset, MedFMC, with 22,349 images in total, which encapsulates five \nrepresentative medical image classification tasks from real-world clinical daily routines. Fig. 1 presents sample \nimages from each subset, and Table 1 shows the summary of data, including modality, number of samples, image \nsize, classification tasks, and number of classes. Different from many existing public datasets in the medical \ndomain, e.g., Chest X-rays\n20–22, MSD23, and HAM1000024, the proposed dataset and benchmark do not target \nadvancing and evaluating the performance of each individual task with the conventional full-supervised train-\ning paradigm, which may require larger amount of data individually. Instead, we believe that this new dataset  \n(as a union) provides valuable support to develop and evaluate generalizable solutions of adapting foundation \nmodels to a variety of medical downstream applications, e.g., using few samples as the prompts and the rest as \ntesting standardly across all five tasks. In this study, we focus on 2D medical image classification as a start and \ncover the most common 2D medical imaging modalities. 3D data and other tasks, e.g., detection and segmenta\n-\ntion, will be expanded and investigated in future work.\nThe proposed datasets target promoting the following aspects of foundation model adaptation approaches:\n•\t Generalizability: The proposed dataset has the capacity to examine the generalizability of the evaluated \nmethod from multiple perspectives. First, the benchmarked approach should achieve superior performance \non all five prediction tasks, which are largely varied in data modality and image characteristics. Additionally, \nthe composed five subsets of data are diversified in image sizes, data sample numbers, and classification tasks \n(e.g., multi-class, multi-label, and regression ones), as shown in Fig. 1.\n•\t Performance on Rare Diseases (Tail Classes): The few-shot learning scheme fits perfectly for the long-tailed \nclassification scenario, which often has only a few cases available for rare diseases in training. We will also \nface data scarcity in the testing phase, and separate evaluation metrics need to be recruited. The performance \nof algorithms on these tail classes can better reveal the power of pre-trained models and their adaptation \ntechniques.\n•\t Prediction Accuracy and Adaptation Efficiency: Besides evaluating the prediction accuracy of algorithms, \nwe also pay attention to the efficiency of training (with fewer samples) in the cost of both data and compu-\ntation. By combining both the accuracy and cost aspects in the evaluation metrics, we expect the advanced \nmethods can further ease the effort of obtaining quality annotations and meanwhile lower the demand for \ncomputational resources.\nIllustratively, we present the benchmarking results of several common learning paradigms, e.g., fine-tuning and \nfew-shot approaches. During the training phase, a small amount of randomly picked data (a few samples, i.e., 1, 5, \nand 10) are utilized for the initial training, and the rest of the dataset is employed for the validation. Approaches with \nFig. 1 Sample images from five subsets.\nName Modality Dimension # Sample Image Size Target Task # Class\nChestDR X-ray 2D 4,848 2953*2965 Thoracic Abnormality Multi-label 19\nColonPath Pathology 2D 10,009 1024*1024 Gastrointestinal Lesion Binary 2\nEndo Endoscopy 2D 3,865 1280*1024 Colorectal Lesion Multi-label 4\nNeoJaundice Digital camera 2D 2,235 567*567 Neonatal Jaundice Binary 2\nRetino Retinography 2D 1,392 2736*1824 Diabetic Retinopathy Multi-class 5\nTable 1. Data summary of MedFMC.\n3Scientific  Data |          (2023) 10:574  | https://doi.org/10.1038/s41597-023-02460-0\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nadvanced cross-domain knowledge transfer techniques are expected to achieve higher performance scores in such a \nsetting. The final metrics are computed on an average of ten individual runs of the same testing process.\nMethods\nIRB Ethics review and exemption. The presented retrospective research study has been reviewed by each \ninvolved institute individually, and patients consent to data sharing and the open publication of the data (oth-\nerwise waived as detailed below). The ChestDR is approved by Fengcheng People’s Hospital Ethics Committee  \n(Ref. 2020 YiY anLunShen No.016) and Huanggang Hospital of Traditional Chinese Medicine Medical Research \nEthics Committee (Ref. 2020 LunShen No.003), and the committee waived the consent since the retrospective \nresearch will not change the examination process of the patients. All data were adequately anonymized, and \nthe risk of disclosing patient privacy via imaging data was minimal. The NeoJaundice was approved by Xuzhou \nCentral Hospital Ethics Committee (Ref. XZXYLQ-20180517-008), and patients’ consent to the data collection \nwas obtained from the guardians of the children. No identifying images are included, and the data are anonymized \nfrom the children. The Retino is approved by Shanghai Tenth People’s Hospital Ethics Committee (Ref. SHSY-\nIEC-4.1/20-154/01), which waived the consent since it is a retrospective research task and the risk of disclosing \npatient privacy via retinography images has been minimized. The Endo is approved by Renji Hospital Ethics \nCommittee. The committee reviewed and waived consent since the research was a retrospective study, and the \nrisk of disclosing patient privacy via the studied snapshot images was minimized. The ColonPath is derived from \npart of the DigestPath 2019 challenge data, accessible via https://digestpath2019.grand-challenge.org/Dataset/, \nwhich was originally approved by the Histo Pathology Diagnostic Center Ethics Committee. The committee also \nwaived the consent since It is a retrospective research task and the risk of disclosing patient privacy via pathology \nimages is minimal.\nShared pipeline for data collection and annotation.  Fig. 2 illustrates the general data sample col-\nlection and annotation pipeline. MedFMC is composed of data with five different modalities in medical imag-\ning, i.e., chest radiography, pathological images, endoscopy photos, dermatological images, and retinal images.  \nThe entire process consists of three major steps. First, the original data are listed and fetched from various systems, \ne.g., X-rays in the picture archiving and communication system (PACS), blood test results in Health Information \nSystem (HIS), endoscopy photos in the workstations, etc. Detailed processes are varied from modality to modal-\nity, which will be introduced in detail individually. Then, standardized anonymization of patient information \n(mainly the DICOM images) is performed before leaving the hospitals using the DICOM Anonymizer tool pro-\nvided by the RSNA MIRC\n25. All image data are converted into 12-bit PNG images while the original image sizes \nare preserved. All image samples are manually examined to redact any privacy-related text or objects recorded in \nthe images. Finally, a two-stage annotation process is conducted by first generating the initial labels, e.g., anno-\ntated by the medical trainees, blood test results extracted from the HIS, and grading prediction from a pre-trained \nmodel using public datasets. Senior professionals with over ten years of experience in their specialty, e.g., radiol-\nogist, pathologist, gastroenterologist, ophthalmologist, and pediatrician, verify the annotation for each image. In \nthe following sections, we will discuss specific settings for each subset.\nChestDR: Thoracic diseases screening in chest radiography. Chest X-ray is a regularly adopted imaging technique \nfor daily clinical routine. Many thoracic diseases are reported, and further examinations are recommended for \ndifferential diagnoses. Due to the large amount and fast reporting requirements in certain emergency facilities, \na swift screening and reporting of common thoracic diseases could largely improve the efficiency of the clinical \nprocess. Although a few chest x-ray datasets\n20–22 are now publicly available, images with quality annotations \n(preferably verified by radiologists) are still a desired resource for training and evaluating the models.\nA total of 4,848 frontal radiography images (from 4,848 patients) are provided in ChestDR, collected from \ntwo regional hospitals in Hubei and Jiangxi Province, China. A detailed distribution of 19 common thoracic dis-\neases is presented in Fig. 3, which is sorted with the number of samples. Tail classes are highlighted in Red. Each \nPNG image is converted from the original DICOM files using the default window level and width (stored in the \nFig. 2 Overview of MedFMC.\n4Scientific  Data |          (2023) 10:574  | https://doi.org/10.1038/s41597-023-02460-0\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nDICOM tags). The original image sizes are preserved. The initial disease labels are provided by a radiological \nresident (with the support of previously signed radiology reports) and then confirmed by a senior radiologist.\nColonPath: Lesion tissue screening in pathology patches. Pathology examination can support detecting \nearly-stage cancer cells in small tissue slices. In the pathologist’s daily routine, they are required to look over \nseveral dozens of tissue slides, a tiresome and tedious job. In clinical diagnosis, quantifying cancer cells and \nregions is the primary goal for pathologists. The approaches for the classification of pathological tissue patches \nare desired to ease this process. They can help screen whether it exists regions of malignant cells in the entire \nslide in a sliding window manner.\nThe pathology whole slide image (WSI) is originally collected from the Histo Pathology Diagnostic Center, \nwhich is also published and utilized in the DigestPath Challenge 2019\n26. Only the data for the lesion segmen -\ntation tasks are employed in this study. All WSIs were acquired during 2017–2019 with hematoxylin and eosin \n(HE) stains and scanned using the KF- BIO FK-Pro-120 slide scanner. Subsequently, the WSIs were re-scaled \nto ×20 magnification with a pixel resolution of 0.475 μm. Tissue patches are extracted from the WSI in a slid\n-\ning window fashion with a fixed size of 1024 × 1024 and a stride of 768. A total of 396 patients’ 10,009 large \ntissue patches (with a uniform size of 1024 × 1024) of colonoscopy pathology examination will be available in \nColonPath. Positive and negative patch samples (with and without the lesion tissue, computed based on the \nexisting lesion region labels) are illustrated in Fig.  4 along with the number of samples in each category. The \ninitial labels (whether containing lesion tissues) are provided by a trainee in the pathology specialty (with the \nsupport of computed labels) and then confirmed by a senior pathologist.\nNeoJaundice: Neonatal jaundice evaluation in skin photos. Jaundice commonly occurs in newborn infants. \nHowever, most jaundice is benign and does not require any interference. Conventionally, newborns must be \nmonitored by taking a blood test to examine the bilirubin level. The potential toxicity of bilirubin might lead to \nsevere hyperbilirubinemia and, in rare cases, acute bilirubin encephalopathy or kernicterus. Recent techniques \nutilized skin photos of three different parts of the infants, i.e., head, face, and chest, to estimate the total serum \nbilirubin in the blood so as to avoid the repeated invasive blood test for infants.\nFig. 3 Data samples and case summary of ChestDR.\nFig. 4 Data samples and case summary of (a) ColonPath and (b) NeoJaundice.\n5Scientific  Data |          (2023) 10:574  | https://doi.org/10.1038/s41597-023-02460-0\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nA total of 745 infants’ 2,235 images (with an average size of 567 × 567) are collected in the NeoJaundice \ndataset from the Xuzhou central hospital. The initial binary labels are generated using the total serum bilirubin \nreadings extracted from the hospital’s health information system with a threshold of 12.9 mg/dL and then con-\nfirmed by a senior experienced pediatrician. Samples of both low and high bilirubin levels are illustrated in Fig. 4 \nalong with the number of samples in each category. Three images are acquired for each infant on body skins of \nthe head, face, and chest, using digital cameras. The skin regions are surrounded by a standardized color card \nfor color calibration purposes.\nEndo: Lesion classification in colonoscopy images. Colorectal cancer is one of the most common and fatal can\n-\ncers among men and women around the world. Abnormalities like polyps and ulcers are precursors to colorectal \ncancer and are often found in colonoscopy screening of people aged above 50. The risks largely increase along \nwith aging. Colonoscopy is the gold standard for the detection and early diagnosis of such abnormalities with \nnecessary biopsy on site, which could significantly affect the survival rate from colorectal cancer. Automatic \ndetection of such lesions during the colonoscopy procedure could prevent missing lesions and ease the workload \nof gastroenterologists in colonoscopy.\nA total of 80 patients’ 3,865 images (with an average size of 1280 × 1024) recorded during the colonoscopy \nexamination on the workstations in Renji Hospital are produced in the Endo dataset. Four types of lesions, i.e., \nulcer, erosion, polyp, and tumor, are included, which are illustrated in Fig. 5 along with the number of samples \nin each category. Non-relevant images are already excluded, while some noisy and degraded recordings remain \nto reflect the real-world data distribution. These noisy data are mainly caused by motions during the operation, \nwhich only occupy a small portion (<5%) of the images and often are labeled without any of the target lesions. \nThe initial labels of lesions are performed by a junior gastroenterologist (with the support of health records and \nreports) and then confirmed by a senior experienced gastroenterologist.\nRetino: Diabetic retinopathy grading in retina images. Diabetic retinopathy (DR) can lead to vision loss and \nblindness in patients with diabetes, mainly affecting the blood vessel in the retina. Therefore, it is important to \nhave an exam of the retina each year for the early detection of DR. Currently, DR grading requires a trained oph\n-\nthalmologist to manually evaluate color fundus photos of the retina, which is time-consuming and may delay the \ntreatment of patients. Automated screening of DR has long been recognized and desired.\nA total of 1,392 patients’ fundus images (one from each patient with an average size of 2736  × 1824) from \nShanghai Tenth People’s Hospital are included in the Retino dataset, which is extracted from the retinal imaging \nworkstations after the examination. Images are captured by Canon nonmydriatic fundus cameras that mainly \nadopted the 45° macula-centered imaging protocol. Samples of retina images in each of the five grades are illus\n-\ntrated in Fig. 5 along with the number of samples in each grade. A DenseNet-121 (with ImageNet pre-trained \nmodel weights) is first fine-tuned using the dataset from Kaggle’s “Diabetic Retinopathy Detection” challenge \nand produced the prediction for each image. Then, an ophthalmologist with over ten years of experience exam-\nined again based on the automated generated prediction, i.e., the presence of diabetic retinopathy on a scale of 0 \nto 4 (0: No DR; 1: Mild; 2: Moderate; 3: Severe; 4: Proliferative DR).\nData Records\nThe MedFMC Dataset is published via figshare 27. Each dataset in MedFMC consists of all image data in a \n“images” folder and associated image-level labels for each image in a CSV file. Multi-label tasks (i.e., ChestDR \nand Endo) will have multiple columns with either 1 or 0 that represent the existence of corresponding disease \npatterns. Binary and multi-class classification tasks (i.e., ColonPath, NeoJaundice, and Retino) will have only a \nsingle label with the individual class number. The images are named differently across institutes, i.e., named with \na random ID (ChestDR and NeoJaundice) and with a random ID together with the data of collection, not the \nexamination (ColonPath, Endo, and Retino).\ntechnical Validation\nDataset partition. Each image subset is divided into two parts: the few-shot pool and testing subsets. The \nfew-shot pool consists of samples with about 20% randomly selected patients, and the count of each class must \nbe larger than 10. The remaining samples are used for testing. In transfer learning, we use all the images in the \nfew-shot pool for training and validate the deep-learning-based classifier models using testing. In the few-shot \nFig. 5 Data samples and case summary of (a) Endo and (b) Retino.\n6Scientific  Data |          (2023) 10:574  | https://doi.org/10.1038/s41597-023-02460-0\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nsetting, we randomly picked images of 1, 5, and 10 patients for each class from the few-shot pool to build the sup-\nport set, and the testing subset is reserved for the model evaluation. We provide the data list of the few-shot pool \nand testing set together with sample lists of few-shot images in the repository (see the Code availability section).\nFew-shot learning baseline.  In the experiment, we employ two few-shot baseline methods, i.e., \nMeta-Baseline28 and Visual Prompt Tuning (VPT)16. Meta-Baseline28 is chosen here as a classic few-shot method \nto evaluate across all five datasets. The input images are converted to the embedding features via three backbone \nnetworks and pre-trained model settings, including DenseNet 121 layers (Dense121) with ImageNet pre-trained \nweights in supervised learning (SL) and a Swin Transformer (Swin-base) with pre-trained weights from both \nfully-supervised and self-supervised learning (SSL) schemes (SimMIM\n29, a form of Masked Auto-Encoder 15). \nSettings are specified when reporting the performance as shown in the left column of Table 2. We cluster the class \ncenters in the support set using the extracted features and compute the cosine similarities between one image \nin the testing set and the class centers to determine the category. Additionally, we include VPT as an advanced \nmethod in training visual prompts for the few-shot classification tasks. In this case, a vanilla pre-trained model \nfrom the Swin-transformer repository (pre-trained on ImageNet21K and finetuned on ImageNet1k) is utilized to \ninitialize the VPT-based few-shot tuning. We repeat the experiment 10 times (randomly picking few-shot sam-\nples) on the five medical image datasets and report the averaged testing results.\nTransfer learning baseline. We run the fine-tuning experiments using three representative networks, \nincluding DenseNet, EfficientNet, and Swin Transformer, on the five medical image datasets. The Swin trans-\nformer model is pre-trained on ImageNet21K with self-supervised learning and then finetuned on ImageNet1k \nwith labels. The others are also pre-trained using ImageNet but with supervised learning. In our experiments, \nthe fine-tuning is performed as linear probing, i.e., only tuning the classifier (fully connected) layers since the \nparameters in the representation layers are also frozen for the few-shot baseline methods. We also experimented \nwith finetuning the entire network, which could generally improve the performance by 1–2% in accuracy. During \nthe training and inference stage, all the input images are padded and rescaled to 384*384 pixels. Common data \naugmentation tricks, i.e., random crop, resize, and horizontal flip, are adopted. The cross-entropy loss is employed \nas the loss function for the multi-class classification of three datasets, including ColonPath, NeoJaundice, and \nRetino, while the binary cross-entropy loss is computed for the multi-label classification of the remaining two \ndatasets, i.e., ChestDR and Endo. The model parameters (except the fully connected classifier layer) are initialized \nby the ImageNet pre-trained model weights and frozen during the tuning. SGD optimizers with initial learning \nrates of 0.002 and 0.01 are applied for the model training of DenseNet and EfficientNet, respectively. The Swin \ntransformer model is optimized by AdamW with an initial learning rate of 0.001. We trained these classification \nmodels on a single NVIDIA A100 for 20 epochs at a batch size of 8, using the framework of MMClassification\n30.\nEvaluation metrics. To evaluate the performance of transfer learning and few-shot learning baseline exper-\nimental results, we compute the overall accuracy (Acc) and area under the receiver operating characteristic curve \n(AUC) for the multi-class classification tasks in the datasets of ColonPath, NeoJaundice, and Retino, and the mean \naverage precision (mAP) and AUC for the multi-label classification tasks in the datasets of ChestDR and Endo. \nAccuracy reflects the overall correct predictions among all the test images. The predicted label is determined with \nthe maximum softmax outputs in the multi-class classification task. AUC is computed for each class to measure \nMeta-baseline \nDense121(ImageNet, SL)\nChestDR ColonPath NeoJaundice Endo Retino\nmAP AUC Acc AUC Acc AUC mAP AUC Acc AUC\n1-shot 0.110 0.534 0.682 0.798 0.531 0.567 0.202 0.657 0.306 0.696\n5-shot 0.114 0.546 0.731 0.856 0.547 0.582 0.205 0.647 0.432 0.780\n10-shot 0.114 0.550 0.735 0.863 0.556 0.593 0.208 0.658 0.464 0.794\nMeta-baseline  \nSwin-base(ImageNet, SL)\nChestDR ColonPath NeoJaundice Endo Retino\nmAP AUC Acc AUC Acc AUC mAP AUC Acc AUC\n1-shot 0.111 0.538 0.647 0.742 0.514 0.531 0.155 0.521 0.284 0.627\n5-shot 0.121 0.571 0.709 0.861 0.601 0.644 0.167 0.531 0.403 0.743\n10-shot 0.135 0.604 0.762 0.884 0.601 0.652 0.168 0.546 0.425 0.775\nMeta-baseline  \nSwin-base(SimMIM, SSL)\nChestDR ColonPath NeoJaundice Endo Retino\nmAP AUC Acc AUC Acc AUC mAP AUC Acc AUC\n1-shot 0.108 0.533 0.714 0.797 0.536 0.564 0.147 0.534 0.327 0.647\n5-shot 0.125 0.579 0.751 0.882 0.574 0.611 0.154 0.543 0.392 0.739\n10-shot 0.131 0.595 0.767 0.881 0.597 0.642 0.163 0.568 0.458 0.797\nVPT  \nSwin-base (ImageNet, SSL)\nChestDR ColonPath NeoJaundice Endo Retino\nmAP AUC Acc AUC Acc AUC mAP AUC Acc AUC\n1-shot 0.131 0.565 0.776 0.847 0.584 0.559 0.197 0.622 0.415 0.645\n5-shot 0.171 0.648 0.893 0.961 0.644 0.686 0.239 0.675 0.456 0.727\n10-shot 0.190 0.667 0.912 0.971 0.667 0.727 0.256 0.714 0.527 0.752\nTable 2. Results of few-shot learning baseline on MedFMC.\n7Scientific  Data |          (2023) 10:574  | https://doi.org/10.1038/s41597-023-02460-0\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nthe capability of distinguishing between positive and negative classes at various threshold settings. The AP is the \nweighted average of precisions, while the mAP for all samples is the mean value of the AP scores for each class.\nBenchmarking results.  Results of few-shot baselines.  The classification performance of few-shot base -\nlines on each dataset is shown in Table  2. More data can often provide better support for distinguishing the \nrepresentations of testing data, but it comes with a higher data demand and more extensive computation cost. \nThe classification performance on five datasets varies significantly, which indeed indicates the diverse task diffi-\nculty. The Meta-baseline also performs better on parts of the five sets and also has mixed results for multi-class \nand multi-label classifications. VPT clearly achieves the best overall performance considering additional tuning \nparameters (visual prompts) and a network fine-tuning process included in the approach. Regarding the net-\nwork backbone, advanced architectures, e.g., Swin-transformer, does not always produce superior performance \nover convolutional neural network counterparts when using the same ImageNet pre-trained model (via either \nsupervised learning-based or self-supervised learning-based schemes). Furthermore, the detailed performance \nof each disease/lesion class for the three multi-label and multi-class classification tasks are illustrated in Table 3. \nEspecially the results for thoracic diseases classification are listed for head and tail classes separately. Higher or \nequivalent AUCs for these rare classes (tail ones) are achieved, which indicates that few-shot methods can benefit \nthe classification of rare classes more than the common learning paradigms.\nChoices of few-shot samples. Since we repeat the experiment 10 times (randomly picking few-shot samples) \non the five medical image datasets. The choice can affect the classification performance of the averaged testing \nresults. We list the STD in addition to the mean accuracy and AUC, as shown in Table 3. The variances of accu-\nracy and AUCs are often fluctuant less than 5% in the example 10-shot setting.\nChestDR Meta-Baseline ith Swin-base(ImageNet, SL) and 10-shot\nFindings(Head) mAP AUC Findings(Tail) mAP AUC\ncardiomegaly 0.342 ± 0.056 0.577 ± 0.060 TB 0.090 ± 0.016 0.583 ± 0.043\npleural_effusion 0.411 ± 0.063 0.655 ± 0.072 pneumothorax 0.109 ± 0.020 0.640 ± 0.037\npneumonia 0.220 ± 0.041 0.551 ± 0.071 atelectasis 0.069 ± 0.015 0.615 ± 0.058\nhilar_enlargement 0.207 ± 0.047 0.562 ± 0.075 emphysema 0.071 ± 0.022 0.671 ± 0.064\nnodule 0.144 ± 0.016 0.513 ± 0.035 calcification 0.027 ± 0.002 0.537 ± 0.016\naortic_calcification 0.191 ± 0.025 0.618 ± 0.052 pulmonary_edema 0.104 ± 0.009 0.811 ± 0.015\ntortuous_aorta 0.208 ± 0.037 0.657 ± 0.066 increased_lung_marks 0.015 ± 0.003 0.567 ± 0.029\nfibrosis 0.122 ± 0.024 0.509 ± 0.071 consolidation 0.010 ± 0.001 0.624 ± 0.014\nthickness_pleura 0.102 ± 0.012 0.503 ± 0.042 elevated_diaphragm 0.006 ± 0.001 0.706 ± 0.006\nfracture_old 0.119 ± 0.014 0.585 ± 0.036 Avg(Tail) 0.056 ± 0.010 0.639 ± 0.031\nAvg(Head) 0.205 ± 0.034 0.573 ± 0.058 Avg(All classes) 0.135 ± 0.024 0.604 ± 0.045\nEndo Retino\nLesion Types mAP AUC Grade Acc AUC\nulcer 0.251 ± 0.028 0.586 ± 0.035 0 0.567 ± 0.056 0.865 ± 0.036\nerosion 0.320 ± 0.056 0.577 ± 0.040 1 0.665 ± 0.258 0.919 ± 0.028\npolyp 0.093 ± 0.010 0.544 ± 0.031 2 0.341 ± 0.065 0.581 ± 0.086\ntumor 0.009 ± 0.001 0.478 ± 0.028 3 0.525 ± 0.102 0.729 ± 0.030\nAvg 0.168 ± 0.015 0.546 ± 0.015 4 0.416 ± 0.044 0.779 ± 0.017\nAvg 0.425 ± 0.046 0.775 ± 0.023\nTable 3. Results of sub-classes with meta-baseline and 10-shot patient data.\nFine-tuning \n10-shot\nChestDR (Head) ChestDR (Tail) ColonPath NeoJaundice Endo Retino\nmAP AUC mAP AUC Acc AUC Acc AUC mAP AUC Acc AUC\nDenseNet-121 0.188 0.568 0.061 0.647 0.755 0.883 0.610 0.637 0.200 0.622 0.359 0.753\nEfficient-b4 0.270 0.654 0.075 0.664 0.820 0.901 0.595 0.641 0.233 0.673 0.576 0.853\nSwin-base 0.247 0.631 0.064 0.611 0.792 0.893 0.652 0.687 0.233 0.688 0.473 0.757\nFine-tuning 20%\nChestDR (Head) ChestDR (Tail) ColonPath NeoJaundice Endo Retino\nmAP AUC mAP AUC Acc AUC Acc AUC mAP AUC Acc AUC\nDenseNet-121 0.348 0.745 0.130 0.761 0.961 0.991 0.742 0.814 0.414 0.802 0.699 0.910\nEfficient-b4 0.377 0.744 0.196 0.804 0.970 0.996 0.752 0.829 0.370 0.782 0.696 0.912\nSwin-base 0.415 0.782 0.194 0.789 0.956 0.982 0.716 0.794 0.414 0.794 0.787 0.948\nSwin-base (No \nAug) 0.407 0.763 0.142 0.75 0.951 0.986 0.727 0.796 0.419 0.795 0.758 0.925\nTable 4. Results of transfer learning baseline on MedFMC with 10-shot and 20% patient data.\n8Scientific  Data |          (2023) 10:574  | https://doi.org/10.1038/s41597-023-02460-0\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nResults of fine-tuning baselines. Table 4 shows the results of fine-tuning-based classification frameworks \nwith all 20% patient data from the few-shot pool and with 10-shot sample data individually. We further list \nthe results of head and tail classes (two columns as shown in Fig. 3). There is still quite a gap between the clas-\nsification accuracies for these two groups of methods when all 20% of data in the few-shot pool are utilized in \nfine-tuning, which is reasonable, considering more training samples are utilized. Nonetheless, the fine-tuning \nperformance decrease to an equivalent level to the few-shot learning paradigms when only ten patients’ data are \nemployed. When there is a scarcity of sample data, it is highly advantageous to utilize few-shot-based techniques. \n \nWe do not show the results of fine-tuning using fewer (1 and 5) samples since we find the training hard to \naccomplish (either overfitting or underfitting) using very few data points, which reveals a critical limitation for \nthe fine-tuning-based methods. Moreover, we provided the finetuning results with and without data augmenta\n-\ntion at the bottom of Table 4. The difference between them is rather marginal.\nUsage Notes\nThe provided dataset is publicly available under the Creative Commons Zero (CC0) Attribution. Please note the \npresented datasets are not intended for the development of diagnosis-oriented algorithms and models. It should \nalso not be utilized as the sole base of the clinical evaluation for each classification task.\nCode availability\nThe code repository of the presented few-shot methods can be accessed via https://github.com/wllfore/MedFMC_\nfewshot_baseline. No custom code was used to generate or process the data described in the manuscript.\nReceived: 6 April 2023; Accepted: 9 August 2023;\nPublished: xx xx xxxx\nReferences\n 1. Dosovitskiy, A. et al. An image is worth 16 × 16 words: Transformers for image recognition at scale. In International Conference on \nLearning Representations (2021).\n 2. Radford, A., et al. Improving language understanding by generative pre-training. OpenAI (2018).\n 3. Gu, Y . et al. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Comput. \nfor Healthc. (HEALTH) 3, 1–23 (2020).\n 4. Radford, A. et al. Learning transferable visual models from natural language supervision. In International conference on machine \nlearning, 8748–8763 (2021).\n 5. Shin, H.-C. et al. Deep convolutional neural networks for computer-aided detection: Cnn architectures, dataset characteristics and \ntransfer learning. IEEE transactions on medical imaging 35, 1285–1298 (2016).\n 6. Deng, J. et al . Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern \nrecognition, 248–255 (2009).\n 7. Brown, T. et al. Language models are few-shot learners. Advances in neural information processing systems 33, 1877–1901 (2020).\n 8. Dhillon, G. S., Chaudhari, P ., Ravichandran, A. & Soatto, S. A baseline for few-shot image classification. arXiv preprint \narXiv:1909.02729 (2019).\n 9. Tian, Y ., Wang, Y ., Krishnan, D., Tenenbaum, J. B. & Isola, P . Rethinking few-shot image classification: a good embedding is all you \nneed? In Proceedings of the European Conference on Computer Vision, 266–282 (Springer, 2020).\n 10. Ouyang, C. et al . Self-supervision with superpixels: Training few-shot medical image segmentation without annotation. In \nProceedings of the European Conference on Computer Vision, 762–780 (Springer, 2020).\n 11. Singh, R. et al. Metamed: Few-shot medical image classification using gradient-based meta-learning. Pattern Recognition 120, \n108111 (2021).\n 12. Zhou, K., Y ang, J., Loy, C. C. & Liu, Z. Learning to prompt for vision-language models. International Journal of Computer Vision 130, \n2337–2348 (2022).\n 13. Zhou, K., Y ang, J., Loy, C. C. & Liu, Z. Conditional prompt learning for vision-language models. In Proceedings of the IEEE/CVF \nConference on Computer Vision and Pattern Recognition, 16816–16825 (2022).\n 14. Caron, M. et al. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on \nComputer Vision (ICCV) (2021).\n 15. He, K. et al. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and \nPattern Recognition, 16000–16009 (2022).\n 16. Jia, M. et al. Visual prompt tuning. In Avidan, S., Brostow, G., Cisse, M., Farinella, G. M. & Hassner, T. (eds.) Computer Vision – \nECCV 2022, 709–727 (Springer Nature Switzerland, Cham, 2022).\n 17. Qin, Z., Yi, H., Lao, Q. & Li, K. Medical image understanding with pretrained vision language models: A comprehensive study. In \nICLR (2023).\n 18. Sun, L. et al. Few-shot medical image segmentation using a global correlation network with discriminative embedding https://doi.\norg/10.48550/arXiv.2012.05440 (2020).\n 19. Shakeri, F . et al. Fhist: A benchmark for few-shot classification of histological images https://doi.org/10.48550/arXiv.2206.00092 (2022).\n 20. Wang, X. et al. Chestx-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization \nof common thorax diseases. 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) 3462–3471 (2017).\n 21. Irvin, J. A. et al. Chexpert: A large chest radiograph dataset with uncertainty labels and expert comparison. In AAAI (2019).\n 22. Johnson, A. E. W . et al. Mimic-cxr: A large publicly available database of labeled chest radiographs. Sci. Data 6 (2019).\n 23. Antonelli, M. et al. The medical segmentation decathlon. Nature Communications 13 (2021).\n 24. Tschandl, P ., Rosendahl, C. & Kittler, H. The ham10000 dataset, a large collection of multi-source dermatoscopic images of common \npigmented skin lesions. Scientific Data 5 (2018).\n 25. The RSNA MIRC project. Dicom anonymizer. http://mirc.rsna.org/download.\n 26. Da, Q. et al. Digestpath: A benchmark dataset with challenge review for the pathological detection and segmentation of digestive-\nsystem. Medical Image Analysis 80, 102485 (2022).\n 27. Wang, D. et al. A real-world dataset and benchmark for foundation1 model adaptation in medical image classification, figshare , \nhttps://doi.org/10.6084/m9.figshare.c.6476047.v1 (2023).\n 28. Chen, Y ., Liu, Z., Xu, H., Darrell, T. & Wang, X. Meta-baseline: Exploring simple meta-learning for few-shot learning. 2021 IEEE/\nCVF International Conference on Computer Vision (ICCV) 9042–9051 (2020).\n 29. Xie, Z. et al. Simmim: A simple framework for masked image modeling. In International Conference on Computer Vision and Pattern \nRecognition (CVPR) (2022).\n 30. Mmclassification. https://github.com/open-mmlab/mmclassification.\n\n9Scientific  Data |          (2023) 10:574  | https://doi.org/10.1038/s41597-023-02460-0\nwww.nature.com/scientificdatawww.nature.com/scientificdata/\nacknowledgements\nJ.S. is supported by grants from Shanghai Science and Technology Innovation Initiative (21SQBS02302), and \nCultivated Funding for Clinical Research Innovation, Ren Ji Hospital, Shanghai Jiao Tong University School of \nMedicine [RJPY-LX-004]. Q.D. is supported by Shanghai Municipal Science and Technology Key Project (Grant \nNo. 20511100302).\nAuthor contributions\nD.W . and X.W . conceptualized and compiled the dataset, created annotation protocols, and wrote most of the \nmanuscript. L.W . and M.L. performed the technical validation. Q.D., X.L., X.G. and J.S. contributed to dataset \ncuration and annotation. Q.D., T.S. and J.H. contributed to the dataset curation. J.Z., K.L., Y .Q. and S.Z. provided \nimportant scientific input and contributed to the writing of the manuscript. All authors read and approved the \nfinal version of the manuscript.\ncompeting interests\nThe authors declare no competing interests.\nAdditional information\nCorrespondence and requests for materials should be addressed to Y .Q. or S.Z.\nReprints and permissions information is available at www.nature.com/reprints.\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.\nOpen Access This article is licensed under a Creative Commons Attribution 4.0 International \nLicense, which permits use, sharing, adaptation, distribution and reproduction in any medium or \nformat, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Cre-\native Commons licence, and indicate if changes were made. The images or other third party material in this \narticle are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not per-\nmitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the \ncopyright holder. To view a copy of this licence, visit http://creativecommons.org/licenses/by/4.0/.\n \n© The Author(s) 2023",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7518665790557861
    },
    {
      "name": "Benchmarking",
      "score": 0.6089041233062744
    },
    {
      "name": "Machine learning",
      "score": 0.5878710150718689
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5851587653160095
    },
    {
      "name": "Benchmark (surveying)",
      "score": 0.5618748664855957
    },
    {
      "name": "Context (archaeology)",
      "score": 0.47646093368530273
    },
    {
      "name": "Medical imaging",
      "score": 0.4340512752532959
    },
    {
      "name": "Economic shortage",
      "score": 0.42576029896736145
    },
    {
      "name": "Scale (ratio)",
      "score": 0.41460299491882324
    },
    {
      "name": "Data science",
      "score": 0.3699861764907837
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Business",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    },
    {
      "name": "Marketing",
      "score": 0.0
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Government (linguistics)",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4391012619",
      "name": "Shanghai Artificial Intelligence Laboratory",
      "country": null
    },
    {
      "id": "https://openalex.org/I2801556517",
      "name": "Ruijin Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I183067930",
      "name": "Shanghai Jiao Tong University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210132741",
      "name": "Shanghai Tenth People's Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I116953780",
      "name": "Tongji University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210132906",
      "name": "Xuzhou Central Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I2800570007",
      "name": "Renji Hospital",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210128910",
      "name": "Group Sense (China)",
      "country": "HK"
    },
    {
      "id": "https://openalex.org/I4210156804",
      "name": "First Affiliated Hospital of Zhengzhou University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I24185976",
      "name": "Sichuan University",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I4210089761",
      "name": "West China Hospital of Sichuan University",
      "country": "CN"
    }
  ],
  "cited_by": 39
}