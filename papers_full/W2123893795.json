{
    "title": "Probabilistic Top-Down Parsing and Language Modeling",
    "url": "https://openalex.org/W2123893795",
    "year": 2001,
    "authors": [
        {
            "id": "https://openalex.org/A2165864196",
            "name": "Brian Roark",
            "affiliations": [
                "Brown University"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1996903695",
        "https://openalex.org/W3021713638",
        "https://openalex.org/W2540043677",
        "https://openalex.org/W3021928671",
        "https://openalex.org/W145738457",
        "https://openalex.org/W2103005629",
        "https://openalex.org/W1649222155",
        "https://openalex.org/W2087165009",
        "https://openalex.org/W2134495021",
        "https://openalex.org/W2140842551",
        "https://openalex.org/W1549364818",
        "https://openalex.org/W1955233831",
        "https://openalex.org/W1986543644",
        "https://openalex.org/W2949237929",
        "https://openalex.org/W2122472280",
        "https://openalex.org/W1597533204",
        "https://openalex.org/W1496659931",
        "https://openalex.org/W2098379588",
        "https://openalex.org/W186828541",
        "https://openalex.org/W1491178396",
        "https://openalex.org/W1781980207",
        "https://openalex.org/W1700641177",
        "https://openalex.org/W2007709031",
        "https://openalex.org/W2130747650",
        "https://openalex.org/W2092654472",
        "https://openalex.org/W2096466920",
        "https://openalex.org/W1620568205",
        "https://openalex.org/W1818785862",
        "https://openalex.org/W2963847008",
        "https://openalex.org/W1508165687",
        "https://openalex.org/W93150917",
        "https://openalex.org/W1574901103",
        "https://openalex.org/W1567570606",
        "https://openalex.org/W1551104980",
        "https://openalex.org/W4205167092",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W4241850027",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W4231741839",
        "https://openalex.org/W3021452258",
        "https://openalex.org/W1542224353",
        "https://openalex.org/W2094052937",
        "https://openalex.org/W1535015163",
        "https://openalex.org/W2123884736",
        "https://openalex.org/W1749358154",
        "https://openalex.org/W1512626953",
        "https://openalex.org/W2002089154",
        "https://openalex.org/W2099111195"
    ],
    "abstract": "This paper describes the functioning of a broad-coverage probabilistic top-down parser, and its application to the problem of language modeling for speech recognition. The paper first introduces key notions in language modeling and probabilistic parsing, and briefly reviews some previous approaches to using syntactic structure for language modeling. A lexicalized probabilistic top-down parser is then presented, which performs very well, in terms of both the accuracy of returned parses and the efficiency with which they are found, relative to the best broad-coverage statistical parsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and empirical results show that it improves upon previous work in test corpus perplexity. Interpolation with a trigram model yields an exceptional improvement relative to the improvement observed by other models, demonstrating the degree to which the information captured by our parsing model is orthogonal to that captured by a trigram model. A small recognition experiment also demonstrates the utility of the model.",
    "full_text": "Probabilistic Top-Down Parsing and\nLanguage Modeling\nBrian Roark\n/\u0003\nBrown University\nThis paper describes the functioning of a broad-coverage probabilistic top-down parser, and its\napplication to the problem of language modeling for speech recognition. The paper ﬁrst introduces\nkey notions in language modeling and probabilistic parsing, and brieﬂy reviews some previous\napproaches to using syntactic structure for language modeling. A lexicalized probabilistic top-\ndown parser is then presented, which performs very well, in terms of both the accuracy of returned\nparses and the efﬁciency with which they are found, relative to the best broad-coverage statistical\nparsers. A new language model that utilizes probabilistic top-down parsing is then outlined, and\nempirical results show that it improves upon previous work in test corpus perplexity. Interpolation\nwith a trigram model yields an exceptional improvement relative to the improvement observed\nby other models, demonstrating the degree to which the information captured by our parsing\nmodel is orthogonal to that captured by a trigram model. A small recognition experiment also\ndemonstrates the utility of the model.\n1. Introduction\nWith certain exceptions, computational linguists have in the past generally formed\na separate research community from speech recognition researchers, despite some\nobvious overlap of interest. Perhaps one reason for this is that, until relatively re-\ncently, few methods have come out of the natural language processing community\nthat were shown to improve upon the very simple language models still standardly\nin use in speech recognition systems. In the past few years, however, some improve-\nments have been made over these language models through the use of statistical meth-\nods of natural language processing, and the development of innovative, linguistically\nwell-motivated techniques for improving language models for speech recognition is\ngenerating more interest among computational linguists. While language models built\naround shallow local dependencies are still the standard in state-of-the-art speech\nrecognition systems, there is reason to hope that better language models can and will\nbe developed by computational linguists for this task.\nThis paper will examine language modeling for speech recognition from a nat-\nural language processing point of view. Some of the recent literature investigating\napproaches that use syntactic structure in an attempt to capture long-distance depen-\ndencies for language modeling will be reviewed. A new language model, based on\nprobabilistic top-down parsing, will be outlined and compared with the previous liter-\nature, and extensive empirical results will be presented which demonstrate its utility.\nTwo features of our top-down parsing approach will emerge as key to its success.\nFirst, the top-down parsing algorithm builds a set of rooted candidate parse trees from\nleft to right over the string, which allows it to calculate a generative probability for\n/\u0003 Department of Cognitive and Linguistic Sciences, Box 1978, Brown University, Providence, RI 02912\nc\n/\r 2001 Association for Computational Linguistics\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\neach preﬁx string from the probabilistic grammar, and hence a conditional probability\nfor each word given the previous words and the probabilistic grammar. A left-to-\nright parser whose derivations are not rooted, i.e., with derivations that can consist of\ndisconnected tree fragments, such as an LR or shift-reduce parser, cannot incrementally\ncalculate the probability of each preﬁx string being generated by the probabilistic\ngrammar, because their derivations include probability mass from unrooted structures.\nOnly at the point when their derivations become rooted (at the end of the string) can\ngenerative string probabilities be calculated from the grammar. These parsers can\ncalculate word probabilities based upon the parser state—as in Chelba and Jelinek\n(1998a)—but such a distribution is not generative from the probabilistic grammar.\nA parser that is not left to right, but which has rooted derivations, e.g., a head-\nﬁrst parser, will be able to calculate generative joint probabilities for entire strings;\nhowever, it will not be able to calculate probabilities for each word conditioned on\npreviously generated words, unless each derivation generates the words in the string\nin exactly the same order. For example, suppose that there are two possible verbs that\ncould be the head of a sentence. For a head-ﬁrst parser, some derivations will have the\nﬁrst verb as the head of the sentence, and the second verb will be generated after the\nﬁrst; hence the second verb’s probability will be conditioned on the ﬁrst verb. Other\nderivations will have the second verb as the head of the sentence, and the ﬁrst verb’s\nprobability will be conditioned on the second verb. In such a scenario, there is no\nway to decompose the joint probability calculated from the set of derivations into the\nproduct of conditional probabilities using the chain rule. Of course, the joint probability\ncan be used as a language model, but it cannot be interpolated on a word-by-word\nbasis with, say, a trigram model, which we will demonstrate is a useful thing to do.\nThus, our top-down parser allows for the incremental calculation of generative\nconditional word probabilities, a property it shares with other left-to-right parsers\nwith rooted derivations such as Earley parsers (Earley 1970) or left-corner parsers\n(Rosenkrantz and Lewis II 1970).\nA second key feature of our approach is that top-down guidance improves the\nefﬁciency of the search as more and more conditioning events are extracted from the\nderivation for use in the probabilistic model. Because the rooted partial derivation is\nfully connected, all of the conditioning information that might be extracted from the\ntop-down left context has already been speciﬁed, and a conditional probability model\nbuilt on this information will not impose any additional burden on the search. In\ncontrast, an Earley or left-corner parser will underspecify certain connections between\nconstituents in the left context, and if some of the underspeciﬁed information is used\nin the conditional probability model, it will have to become speciﬁed. Of course, this\ncan be done, but at the expense of search efﬁciency; the more that this is done, the\nless beneﬁt there is from the underspeciﬁcation. A top-down parser will, in contrast,\nderive an efﬁciency beneﬁt from precisely the information that is underspeciﬁed in\nthese other approaches.\nThus, our top-down parser makes it very easy to condition the probabilistic gram-\nmar on an arbitrary number of values extracted from the rooted, fully speciﬁed deriva-\ntion. This has lead us to a formulation of the conditional probability model in terms\nof values returned from tree-walking functions that themselves are contextually sen-\nsitive. The top-down guidance that is provided makes this approach quite efﬁcient in\npractice.\nThe following section will provide some background in probabilistic context-free\ngrammars and language modeling for speech recognition. There will also be a brief\nreview of previous work using syntactic information for language modeling, before\nwe introduce our model in Section 4.\n250\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nFigure 1\nThree parse trees: (a) a complete parse tree; (b) a complete parse tree with an explicit stop\nsymbol; and (c) a partial parse tree.\n2. Background\n2.1 Grammars and Trees\nThis section will introduce probabilistic (or stochastic) context-free grammars (PCFGs),\nas well as such notions as complete and partial parse trees, which will be important\nin deﬁning our language model later in the paper.\n1 In addition, we will explain some\nsimple grammar transformations that will be used. Finally, we will explain the notion\nof c-command, which will be used extensively later as well.\nPCFGs model the syntactic combinatorics of a language by extending conventional\ncontext-free grammars (CFGs). A CFG G /= /( V, T, P, S\ny\n/) , consists of a set of nonterminal\nsymbols V, a set of terminal symbols T,as t a r ts y m b o lS\ny\n/2 V,a n das e to fr u l e\nproductions P of the form: A /! /\u000b ,w h e r e/\u000b /2 /( V /[ T/)\n/\u0003\n. These context-free rules\ncan be interpreted as saying that a nonterminal symbol A expands into one or more\neither nonterminal or terminal symbols, /\u000b /= X0\n/:/:/: Xk.2 A sequence of context-free rule\nexpansions can be represented in a tree, with parents expanding into one or more\nchildren below them in the tree. Each of the individual local expansions in the tree\nis a rule in the CFG. Nodes in the tree with no children are called leaves. A tree\nwhose leaves consist entirely of terminal symbols is complete. Consider, for example,\nthe parse tree shown in (a) in Figure 1: the start symbol is S\ny\n, which expands into an\nS. The S node expands into an NP followed by a VP . These nonterminal nodes each\nin turn expand, and this process of expansion continues until the tree generates the\nterminal string, “\nSpot chased the ball ”, as leaves.\nAC F GG deﬁnes a language LG, which is a subset of the set of strings of terminal\nsymbols, including only those that are leaves of complete trees rooted at S\ny\n, built\nwith rules from the grammar G. We will denote strings either as w or as w0w1\n/:/:/: wn,\nwhere wn is understood to be the last terminal symbol in the string. For simplicity in\ndisplaying equations, from this point forward let w j\ni be the substring wi\n/:/:/: wj.L e tTwn\n0\n1 For a detailed introduction to PCFGs, see Manning and Sch ¨ utze (1999), for example.\n2 For ease of exposition, we will ignore epsilon productions for now. An epsilon production has the\nempty string ( /\u000f ) on the right-hand side, and can be written A /! /\u000f . Everything that is said here can be\nstraightforwardly extended to include such productions.\n251\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nbe the set of all complete trees rooted at the start symbol, with the string of terminals\nwn\n0 as leaves. We call Twn\n0\nthe set of complete parses of wn\n0.\nA PCFG is a CFG with a probability assigned to each rule; speciﬁcally, each right-\nhand side has a probability given the left-hand side of the rule. The probability of a\nparse tree is the product of the probabilities of each rule in the tree. Provided a PCFG\nis consistent (or tight), which it always will be in the approach we will be advocating,\nthis deﬁnes a proper probability distribution over completed trees.\n3\nA PCFG also deﬁnes a probability distribution over strings of words (terminals)\nin the following way:\nP/( wn\n0\n/) /=\nX\nt/2 Twn\n0\nP/( t/) /( 1/)\nThe intuition behind Equation 1 is that, if a string is generated by the PCFG, then it\nwill be produced if and only if one of the trees in the set Twn\n0\ngenerated it. Thus the\nprobability of the string is the probability of the set Twn\n0\n,i . e . ,t h es u mo fi t sm e m b e r s ’\nprobabilities.\nUp to this point, we have been discussing strings of words without specifying\nwhether they are “complete” strings or not. We will adopt the convention that an\nexplicit beginning of string symbol, h s i , and an explicit end symbol, h /= s i ,a r ep a r to f\nthe vocabulary, and a string wn\n0 is a complete string if and only if w\n/0\nis h s i and wn\nis h /= s i . Since the beginning of string symbol is not predicted by language models,\nbut rather is axiomatic in the same way that S\ny\nis for a parser, we can safely omit it\nfrom the current discussion, and simply assume that it is there. See Figure 1(b) for the\nexplicit representation.\nWhile a complete string of words must contain the end symbol as its ﬁnal word,\na string preﬁx does not have this restriction. For example, “ Spot chased the ball\nh /= si ” is a complete string, and the following is the set of preﬁx strings of this com-\nplete string: “ Spot ”; “ Spot chased ”; “ Spot chased the ”; “ Spot chased the ball ”;\nand “ Spot chased the ball h /= si ”. A PCFG also deﬁnes a probability distribution\nover string preﬁxes, and we will present this in terms of partial derivations. A partial\nderivation (or parse) d is deﬁned with respect to a preﬁx string w j\n0 as follows: it is the\nleftmost derivation of the string, with wj on the right-hand side of the last expansion\nin the derivation. 4 Let Dw j\n0\nbe the set of all partial derivations for a preﬁx string w j\n0.\nThen\nP/( w j\n0\n/) /=\nX\nd/2 D\nw j\n0\nP/( d/) /( 2/)\nWe left-factor the PCFG, so that all productions are binary, except those with a\nsingle terminal on the right-hand side and epsilon productions. 5 We do this because\nit delays predictions about what nonterminals we expect later in the string until we\nhave seen more of the string. In effect, this is an underspeciﬁcation of some of the\npredictions that our top-down parser is making about the rest of the string. The left-\nfactorization transform that we use is identical to what is called right binarization\nin Roark and Johnson (1999). See that paper for more discussion of the beneﬁts of\n3 A PCFG is consistent or tight if there is no probability mass reserved for inﬁnite trees. Chi and Geman\n(1998) proved that any PCFG estimated from a treebank with the relative frequency estimator is tight.\nAll of the PCFGs that are used in this paper are estimated using the relative frequency estimator.\n4 A leftmost derivation is a derivation in which the leftmost nonterminal is always expanded.\n5T h eo n l y/\u000f -productions that we will use in this paper are those introduced by left-factorization.\n252\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nFigure 2\nTwo parse trees: (a) a complete left-factored parse tree with epsilon productions and an\nexplicit stop symbol; and (b) a partial left-factored parse tree.\nfactorization for top-down and left-corner parsing. For a grammar G,w ed e ﬁ n ea\nfactored grammar Gf as follows:\ni. ( A /! BA -B) /2 Gf iff (A /! B/\f ) /2 G,s . t .B /2 V and /\f /2 V\n/\u0003\nii. ( A-/\u000b /! BA -/\u000b B) /2 Gf iff (A /! /\u000b B/\f ) /2 G,s . t .B /2 V, /\u000b /2 V\n/+\n,a n d /\f /2 V\n/\u0003\niii. ( A-/\u000b B /! /\u000f ) /2 Gf iff (A /! /\u000b B) /2 G,s . t .B /2 V and /\u000b /2 V\n/\u0003\niv. ( A /! a) /2 Gf iff (A /! a) /2 G,s . t .a /2 T\nWe can see the effect of this transform on our example parse trees in Figure 2. This\nunderspeciﬁcation of the nonterminal predictions (e.g., VP-VBD in the example in\nFigure 2, as opposed to NP), allows lexical items to become part of the left context,\nand so be used to condition production probabilities, even the production probabil-\nities of constituents that dominate them in the unfactored tree. It also brings words\nfurther downstream into the look-ahead at the point of speciﬁcation. Note that partial\ntrees are deﬁned in exactly the same way (Figure 2b), but that the nonterminal yields\nare made up exclusively of the composite nonterminals introduced by the grammar\ntransform.\nThis transform has a couple of very nice properties. First, it is easily reversible,\ni.e., every parse tree built with G\nf corresponds to a unique parse tree built with G.\nSecond, if we use the relative frequency estimator for our production probabilities, the\nprobability of a tree built with G\nf is identical to the probability of the corresponding\ntree built with G.\nFinally, let us introduce the term c-command. We will use this notion in our condi-\ntional probability model, and it is also useful for understanding some of the previous\nwork in this area. The simple deﬁnition of c-command that we will be using in this\npaper is the following: a node A c-commands a node B if and only if (i) A does not\ndominate B; and (ii) the lowest branching node (i.e., non-unary node) that dominates\n253\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nA also dominates B.6 Thus in Figure 1(a), the subject NP and the VP each c-command\nthe other, because neither dominates the other and the lowest branching node above\nboth (the S) dominates the other. Notice that the subject NP c-commands the object\nNP , but not vice versa, since the lowest branching node that dominates the object NP\nis the VP , which does not dominate the subject NP .\n2.2 Language Modeling for Speech Recognition\nThis section will brieﬂy introduce language modeling for statistical speech recognition.\n7\nIn language modeling, we assign probabilities to strings of words. To assign a\nprobability, the chain rule is generally invoked. The chain rule states, for a string of\nk+1 words:\nP/( wk\n0\n/) /= P/( w0\n/)\nk\nY\ni/= 1\nP/( wi\nj wi/; 1\n0\n/) /( 3/)\nA Markov language model of order n truncates the conditioning information in the\nchain rule to include only the previous n words.\nP/( wk\n0\n/) /= P/( w0\n/) P/( w1\nj w0\n/) /:/:/: P/( wn/; 1\nj wn/; 2\n0\n/)\nk\nY\ni/= n\nP/( wi\nj wi/; 1\ni/; n\n/) /( 4/)\nThese models are commonly called n-gram models. 8 The standard language model\nused in many speech recognition systems is the trigram model, i.e., a Markov model\nof order 2, which can be characterized by the following equation:\nP/( wn/; 1\n0\n/) /= P/( w0\n/) P/( w1\nj w0\n/)\nn/; 1\nY\ni/= 2\nP/( wi\nj wi/; 1\ni/; 2\n/) /( 5/)\nTo smooth the trigram models that are used in this paper, we interpolate the\nprobability estimates of higher-order Markov models with lower-order Markov models\n(Jelinek and Mercer 1980). The idea behind interpolation is simple, and it has been\nshown to be very effective. For an interpolated ( n\n/+ 1)-gram:\nP/( wi\nj wi/; 1\ni/; n\n/) /= /\u0015\nn\n/( wi/; 1\ni/; n\n/)\nb\nP/( wi\nj wi/; 1\ni/; n\n/) /+ /( 1 /; /\u0015\nn\n/( wi/; 1\ni/; n\n/)/) P/( wi\nj wi/; 1\ni/; n/+ 1\n/) /( 6/)\nHere\nb\nP is the empirically observed relative frequency, and /\u0015\nn is a function from Vn to\n/[ 0, 1/] . This interpolation is recursively applied to the smaller-order n-grams until the\nbigram is ﬁnally interpolated with the unigram, i.e., /\u0015\n0\n/= 1.\n3. Previous Work\nThere have been attempts to jump over adjacent words to words farther back in the\nleft context, without the use of dependency links or syntactic structure, for example\nSaul and Pereira (1997) and Rosenfeld (1996, 1997). We will focus our very brief review,\nhowever, on those that use grammars or parsing for their language models. These can\nbe divided into two rough groups: those that use the grammar as a language model,\n6An o d e A dominates a node B in a tree if and only if either (i) A is the parent of B; or (ii) A is the\nparent of a node C that dominates B.\n7 For a detailed introduction to statistical speech recognition, see Jelinek (1997).\n8T h e n in n-gram is one more than the order of the Markov model, since the n-gram includes the word\nbeing conditioned.\n254\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nand those that use a parser to uncover phrasal heads standing in an important relation\n(c-command) to the current word. The approach that we will subsequently present uses\nthe probabilistic grammar as its language model, but only includes probability mass\nfrom those parses that are found, that is, it uses the parser to ﬁnd a subset of the total\nset of parses (hopefully most of the high-probability parses) and uses the sum of their\nprobabilities as an estimate of the true probability given the grammar.\n3.1 Grammar Models\nAs mentioned in Section 2.1, a PCFG deﬁnes a probability distribution over strings\nof words. One approach to syntactic language modeling is to use this distribution\ndirectly as a language model. There are efﬁcient algorithms in the literature (Jelinek\nand Lafferty 1991; Stolcke 1995) for calculating exact string preﬁx probabilities given\na PCFG. The algorithms both utilize a left-corner matrix, which can be calculated in\nclosed form through matrix inversion. They are limited, therefore, to grammars where\nthe nonterminal set is small enough to permit inversion. String preﬁx probabilities can\nbe straightforwardly used to compute conditional word probabilities by deﬁnition:\nP\n/( wj/+ 1\nj w j\n0\n/) /=\nP/( w j/+ 1\n0\n/)\nP/( w j\n0\n/)\n/( 7/)\nStolcke and Segal (1994) and Jurafsky et al. (1995) used these basic ideas to es-\ntimate bigram probabilities from hand-written PCFGs, which were then used in lan-\nguage models. Interpolating the observed bigram probabilities with these calculated\nbigrams led, in both cases, to improvements in word error rate over using the observed\nbigrams alone, demonstrating that there is some beneﬁt to using these syntactic lan-\nguage models to generalize beyond observed n-grams.\n3.2 Finding Phrasal Heads\nAnother approach that uses syntactic structure for language modeling has been to use\na shift-reduce parser to “surface” c-commanding phrasal headwords or part-of-speech\n(POS) tags from arbitrarily far back in the preﬁx string, for use in a trigram-like model.\nA shift-reduce parser operates from left to right using a stack and a pointer to the\nnext word in the input string.\n9 Each stack entry consists minimally of a nonterminal\nlabel. The parser performs two basic operations: (i) shifting, which involves pushing\nthe POS label of the next word onto the stack and moving the pointer to the following\nword in the input string; and (ii) reducing, which takes the top k stack entries and\nreplaces them with a single new entry, the nonterminal label of which is the left-hand\nside of a rule in the grammar that has the k top stack entry labels on the right-hand\nside. For example, if there is a rule NP\n/! DT NN, and the top two stack entries are\nNN and DT, then those two entries can be popped off of the stack and an entry with\nthe label NP pushed onto the stack.\nGoddeau (1992) used a robust deterministic shift-reduce parser to condition word\nprobabilities by extracting a speciﬁed number of stack entries from the top of the cur-\nrent state, and conditioning on those entries in a way similar to an n-gram. In empirical\ntrials, Goddeau used the top two stack entries to condition the word probability. He\nwas able to reduce both sentence and word error rates on the ATIS corpus using this\nmethod.\n9 For details, see Hopcroft and Ullman (1979), for example.\n255\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nFigure 3\nTree representation of a derivation state.\nThe structured language model (SLM) used in Chelba and Jelinek (1998a, 1998b,\n1999), Jelinek and Chelba (1999), and Chelba (2000) is similar to that of Goddeau,\nexcept that (i) their shift-reduce parser follows a nondeterministic beam search, and\n(ii) each stack entry contains, in addition to the nonterminal node label, the headword\nof the constituent. The SLM is like a trigram, except that the conditioning words are\ntaken from the tops of the stacks of candidate parses in the beam, rather than from\nthe linear order of the string.\nTheir parser functions in three stages. The ﬁrst stage assigns a probability to the\nword given the left context (represented by the stack state). The second stage predicts\nthe POS given the word and the left context. The last stage performs all possible parser\noperations (reducing stack entries and shifting the new word). When there is no more\nparser work to be done (or, in their case, when the beam is full), the following word\nis predicted. And so on until the end of the string.\nEach different POS assignment or parser operation is a step in a derivation. Each\ndistinct derivation path within the beam has a probability and a stack state associated\nwith it. Every stack entry has a nonterminal node label and a designated headword of\nthe constituent. When all of the parser operations have ﬁnished at a particular point in\nthe string, the next word is predicted as follows: For each derivation in the beam, the\nheadwords of the two topmost stack entries form a trigram with the conditioned word.\nThis interpolated trigram probability is then multiplied by the normalized probability\nof the derivation, to provide that derivation’s contribution to the probability of the\nword. More precisely, for a beam of derivations D\ni\nP/( wi/+ 1\nj wi\n0\n/) /=\nP\nd/2 Di\nP/( wi/+ 1\nj h0d, h1d\n/) P/( d/)\nP\nd/2 Di\nP/( d/)\n/( 8/)\nwhere h0d and h1d are the lexical heads of the top two entries on the stack of d.\nFigure 3 gives a partial tree representation of a potential derivation state for the\nstring “the dog chased the cat with spots ”, at the point when the word “ with ”i s\nto be predicted. The shift-reduce parser will have, perhaps, built the structure shown,\nand the stack state will have an NP entry with the head “\ncat ” at the top of the stack,\nand a VBD entry with the head “ chased ” second on the stack. In the Chelba and\nJelinek model, the probability of “ with ” is conditioned on these two headwords, for\nthis derivation.\nSince the speciﬁc results of the SLM will be compared in detail with our model\nwhen the empirical results are presented, at this point we will simply state that they\nhave achieved a reduction in both perplexity and word error rate over a standard\ntrigram using this model.\nThe rest of this paper will present our parsing model, its application to language\nmodeling for speech recognition, and empirical results.\n256\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\n4. Top-Down Parsing and Language Modeling\nStatistically based heuristic best-ﬁrst or beam-search strategies (Caraballo and Char-\nniak 1998; Charniak, Goldwater, and Johnson 1998; Goodman 1997) have yielded an\nenormous improvement in the quality and speed of parsers, even without any guaran-\ntee that the parse returned is, in fact, that with the maximum likelihood for the proba-\nbility model. The parsers with the highest published broad-coverage parsing accuracy,\nwhich include Charniak (1997, 2000), Collins (1997, 1999), and Ratnaparkhi (1997),\nall utilize simple and straightforward statistically based search heuristics, pruning the\nsearch-space quite dramatically.\n10 Such methods are nearly always used in conjunction\nwith some form of dynamic programming (henceforth DP). That is, search efﬁciency\nfor these parsers is improved by both statistical search heuristics and DP . Here we\nwill present a parser that uses simple search heuristics of this sort without DP . Our\napproach is found to yield very accurate parses efﬁciently, and, in addition, to lend\nitself straightforwardly to estimating word probabilities on-line, that is, in a single\npass from left to right. This on-line characteristic allows our language model to be in-\nterpolated on a word-by-word basis with other models, such as the trigram, yielding\nfurther improvements.\nNext we will outline our conditional probability model over rules in the PCFG,\nfollowed by a presentation of the top-down parsing algorithm. We will then present\nempirical results in two domains: one to compare with previous work in the parsing\nliterature, and the other to compare with previous work using parsing for language\nmodeling for speech recognition, in particular with the Chelba and Jelinek results\nmentioned above.\n4.1 Conditional Probability Model\nA simple PCFG conditions rule probabilities on the left-hand side of the rule. It\nhas been shown repeatedly—e.g., Briscoe and Carroll (1993), Charniak (1997), Collins\n(1997), Inui et al. (1997), Johnson (1998)—that conditioning the probabilities of struc-\ntures on the context within which they appear, for example on the lexical head of a\nconstituent (Charniak 1997; Collins 1997), on the label of its parent nonterminal (John-\nson 1998), or, ideally, on both and many other things besides, leads to a much better\nparsing model and results in higher parsing accuracies.\nOne way of thinking about conditioning the probabilities of productions on con-\ntextual information (e.g., the label of the parent of a constituent or the lexical heads of\nconstituents), is as annotating the extra conditioning information onto the labels in the\ncontext-free rules. Examples of this are bilexical grammars—such as Eisner and Satta\n(1999), Charniak (1997), Collins (1997)—where the lexical heads of each constituent\nare annotated on both the right- and left-hand sides of the context-free rules, under\nthe constraint that every constituent inherits the lexical head from exactly one of its\nchildren, and the lexical head of a POS is its terminal item. Thus the rule S\n/! NP VP\nbecomes, for instance, S /[ barks/] /! NP/[ dog/] VP/[ barks/] . One way to estimate the probabil-\nities of these rules is to annotate the heads onto the constituent labels in the training\ncorpus and simply count the number of times particular productions occur (relative\nfrequency estimation). This procedure yields conditional probability distributions of\n10 Johnson et al. (1999), Henderson and Brill (1999), and Collins (2000) demonstrate methods for choosing\nthe best complete parse tree from among a set of complete parse trees, and the latter two show\naccuracy improvements over some of the parsers cited above, from which they generated their\ncandidate sets. Here we will be comparing our work with parsing algorithms, i.e., algorithms that\nbuild parses for strings of words.\n257\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nconstituents on the right-hand side with their lexical heads, given the left-hand side\nconstituent and its lexical head. The same procedure works if we annotate parent infor-\nmation onto constituents. This is how Johnson (1998) conditioned the probabilities of\nproductions: the left-hand side is no longer, for example, S, but rather S\n/\"\nSBAR, i.e., an\nS with SBAR as parent. Notice, however, that in this case the annotations on the right-\nhand side are predictable from the annotation on the left-hand side (unlike, for ex-\nample, bilexical grammars), so that the relative frequency estimator yields conditional\nprobability distributions of the original rules, given the parent of the left-hand side.\nAll of the conditioning information that we will be considering will be of this\nlatter sort: the only novel predictions being made by rule expansions are the node\nlabels of the constituents on the right-hand side. Everything else is already speciﬁed\nby the left context. We use the relative frequency estimator, and smooth our production\nprobabilities by interpolating the relative frequency estimates with those obtained by\n“annotating” less contextual information.\nThis perspective on conditioning production probabilities makes it easy to see that,\nin essence, by conditioning these probabilities, we are growing the state space. That\nis, the number of distinct nonterminals grows to include the composite labels; so does\nthe number of distinct productions in the grammar. In a top-down parser, each rule\nexpansion is made for a particular candidate parse, which carries with it the entire\nrooted derivation to that point; in a sense, the left-hand side of the rule is annotated\nwith the entire left context, and the rule probabilities can be conditioned on any aspect\nof this derivation.\nWe do not use the entire left context to condition the rule probabilities, but rather\n“pick-and-choose” which events in the left context we would like to condition on.\nOne can think of the conditioning events as functions, which take the partial tree\nstructure as an argument and return a value, upon which the rule probability can be\nconditioned. Each of these functions is an algorithm for walking the provided tree and\nreturning a value. For example, suppose that we want to condition the probability of\nthe rule A\n/! /\u000b . We might write a function that takes the partial tree, ﬁnds the parent\nof the left-hand side of the rule and returns its node label. If the left-hand side has no\nparent (i.e., it is at the root of the tree), the function returns the null value (NULL). We\nmight write another function that returns the nonterminal label of the closest sibling to\nthe left of A, and NULL if no such node exists. We can then condition the probability\nof the production on the values that were returned by the set of functions.\nRecall that we are working with a factored grammar, so some of the nodes in the\nfactored tree have nonterminal labels that were created by the factorization, and may\nnot be precisely what we want for conditioning purposes. In order to avoid any con-\nfusions in identifying the nonterminal label of a particular rule production in either its\nfactored or nonfactored version, we introduce the function\nconstituent/( A/) for every\nnonterminal in the factored grammar Gf , which is simply the label of the constituent\nwhose factorization results in A. For example, in Figure 2, constituent/( NP-DT-NN/)\nis simply NP .\nNote that a function can return different values depending upon the location in\nthe tree of the nonterminal that is being expanded. For example, suppose that we have\na function that returns the label of the closest sibling to the left of constituent/( A/)\nor NULL if no such node exists. Then a subsequent function could be deﬁned as\nfollows: return the parent of the parent (the grandparent) of\nconstituent/( A/) only if\nconstituent/( A/) has no sibling to the left—in other words, if the previous function\nreturns NULL; otherwise return the second closest sibling to the left ofconstituent/( A/) ,\nor, as always, NULL if no such node exists. If the function returns, for example, NP ,\nthis could either mean that the grandparent is NP or the second closest sibling is\n258\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nFigure 4\nConditional probability model represented as a decision tree, identifying the location in the\npartial parse tree of the conditioning information.\nNP; yet there is no ambiguity in the meaning of the function, since the result of the\nprevious function disambiguates between the two possibilities.\nThe functions that were used for the present study to condition the probability\nof the rule, A /! /\u000b , are presented in Figure 4, in a tree structure. This is a sort of\ndecision tree for a tree-walking algorithm to decide what value to return, for a given\npartial tree and a given depth. For example, if the algorithm is asked for the value\nat level 0, it will return A, the left-hand side of the rule being expanded.\n11 Suppose\nthe algorithm is asked for the value at level 4. After level 2 there is a branch in the\ndecision tree. If the left-hand side of the rule is a POS, and there is no sibling to the left\nof\nconstituent/( A/) in the derivation, then the algorithm takes the right branch of the\ndecision tree to decide what value to return; otherwise the left branch. Suppose it takes\nthe left branch. Then after level 3, there is another branch in the decision tree. If the\nleft-hand side of the production is a POS, then the algorithm takes the right branch of\nthe decision tree, and returns (at level 4) the POS of the closest c-commanding lexical\nhead to A, which it ﬁnds by walking the parse tree; if the left-hand side of the rule is\nnot a POS, then the algorithm returns (at level 4) the closest sibling to the left of the\nparent of\nconstituent/( A/) .\nThe functions that we have chosen for this paper follow from the intuition (and\nexperience) that what helps parsing is different depending on the constituent that is\nbeing expanded. POS nodes have lexical items on the right-hand side, and hence can\nbring into the model some of the head-head dependencies that have been shown to\nbe so effective. If the POS is leftmost within its constituent, then very often the lexical\n11 Recall that A can be a composite nonterminal introduced by grammar factorization. When the function\nis deﬁned in terms of constituent/( A/) , the values returned are obtained by moving through the\nnonfactored tree.\n259\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nTable 1\nLevels of conditioning information, mnemonic labels, and a brief description of the\ninformation level for empirical results.\nConditioning Mnemonic Label Information Level\n0,0,0 none Simple PCFG\n2,2,2 par+sib Small amount of structural context\n5,2,2 NT struct All structural (non-lexical) context for non-POS\n6,2,2 NT head Everything for non-POS expansions\n6,3,2 POS struct More structural info for leftmost POS expansions\n6,5,2 attach All attachment info for leftmost POS expansions\n6,6,4 all Everything\nitem is sensitive to the governing category to which it is attaching. For example, if the\nPOS is a preposition, then its probability of expanding to a particular word is very\ndifferent if it is attaching to a noun phrase than if it is attaching to a verb phrase,\nand perhaps quite different depending on the head of the constituent to which it is\nattaching. Subsequent POSs within a constituent are likely to be open-class words, and\nless dependent on these sorts of attachment preferences.\nConditioning on parents and siblings of the left-hand side has proven to be very\nuseful. To understand why this is the case, one need merely to think of VP expansions.\nIf the parent of a VP is another VP (i.e., if an auxiliary or modal verb is used), then the\ndistribution over productions is different than if the parent is an S. Conditioning on\nhead information, both POS of the head and the lexical item itself, has proven useful\nas well, although given our parser’s left-to-right orientation, in many cases the head\nhas not been encountered within the particular constituent. In such a case, the head\nof the last child within the constituent is used as a proxy for the constituent head. All\nof our conditioning functions, with one exception, return either parent or sibling node\nlabels at some speciﬁc distance from the left-hand side, or head information from c-\ncommanding constituents. The exception is the function at level 5 along the left branch\nof the tree in Figure 4. Suppose that the node being expanded is being conjoined with\nanother node, which we can tell by the presence or absence of a CC node. In that case,\nwe want to condition the expansion on how the conjoining constituent expanded. In\nother words, this attempts to capture a certain amount of parallelism between the\nexpansions of conjoined categories.\nIn presenting the parsing results, we will systematically vary the amount of con-\nditioning information, so as to get an idea of the behavior of the parser. We will refer\nto the amount of conditioning by specifying the deepest level from which a value\nis returned for each branching path in the decision tree, from left to right in Fig-\nure 4: the ﬁrst number is for left contexts where the left branch of the decision tree\nis always followed (non-POS nonterminals on the left-hand side); the second number\nis for a left branch followed by a right branch (POS nodes that are leftmost within\ntheir constituent); and the third number is for the contexts where the right branch is\nalways followed (POS nodes that are not leftmost within their constituent). For exam-\nple, (4,3,2) would represent a conditional probability model that (i) returns NULL for\nall functions below level 4 in all contexts; (ii) returns NULL for all functions below\nlevel 3 if the left-hand side is a POS; and (iii) returns NULL for all functions below\nlevel 2 for nonleftmost POS expansions.\nTable 1 gives a breakdown of the different levels of conditioning information used\nin the empirical trials, with a mnemonic label that will be used when presenting results.\nThese different levels were chosen as somewhat natural points at which to observe\n260\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nhow much of an effect increasing the conditioning information has. We ﬁrst include\nstructural information from the context, namely, node labels from constituents in the\nleft context. Then we add lexical information, ﬁrst for non-POS expansions, then for\nleftmost POS expansions, then for all expansions.\nAll of the conditional probabilities are linearly interpolated. For example, the prob-\nability of a rule conditioned on six events is the linear interpolation of two probabilities:\n(i) the empirically observed relative frequency of the rule when the six events co-occur;\nand (ii) the probability of the rule conditioned on the ﬁrst ﬁve events (which is in turn\ninterpolated). The interpolation coefﬁcients are a function of the frequency of the set\nof conditioning events, and are estimated by iteratively adjusting the coefﬁcients so\nas to maximize the likelihood of a held-out corpus.\nThis was an outline of the conditional probability model that we used for the\nPCFG. The model allows us to assign probabilities to derivations, which can be used\nby the parsing algorithm to decide heuristically which candidates are promising and\nshould be expanded, and which are less promising and should be pruned. We now\noutline the top-down parsing algorithm.\n4.2 Top-Down Probabilistic Parsing\nThis parser is essentially a stochastic version of the top-down parser described in Aho,\nSethi, and Ullman (1986). It uses a PCFG with a conditional probability model of the\nsort deﬁned in the previous section. We will ﬁrst deﬁne candidate analysis (i.e., a\npartial parse), and then a derives relation between candidate analyses. We will then\npresent the algorithm in terms of this relation.\nThe parser takes an input string w\nn\n0 ,aP C F G G, and a priority queue of candi-\ndate analyses. A candidate analysis C /= /( D, S , PD, F, wn\ni\n/) consists of a derivation D,a\nstack S , a derivation probability PD,aﬁ g u r eo fm e r i tF,a n das t r i n gwn\ni remaining\nto be parsed. The ﬁrst word in the string remaining to be parsed, wi, we will call the\nlook-ahead word. The derivation D consists of a sequence of rules used from G.T h e\nstack S contains a sequence of nonterminal symbols, and an end-of-stack marker $ at\nthe bottom. The probability PD is the product of the probabilities of all rules in the\nderivation D. F is the product of PD and a look-ahead probability, LAP( S ,wi), which\nis a measure of the likelihood of the stack S rewriting with wi at its left corner.\nWe can deﬁne a derives relation, denoted /) , between two candidate analyses as\nfollows. /( D, S , PD, F, wn\ni\n/) /) /( D\n/0\n, S\n/0\n, PD\n/0\n, F\n/0\n, wn\nj\n/) if and only if 12\ni. D\n/0\n/= D /+ A /! X0\n/:/:/: Xk\nii. S /= A/\u000b $;\niii. either S\n/0\n/= X0\n/:/:/: Xk\n/\u000b $a n d j /= i\nor k /= 0, X0\n/= wi, j /= i /+ 1, and S\n/0\n/= /\u000b $;\niv. PD\n/0\n/= PDP/( A /! X0\n/:/:/: Xk\n/) ;a n d\nv. F\n/0\n/= PD\n/0\nLAP/( S\n/0\n, wj\n/)\nThe parse begins with a single candidate analysis on the priority queue: ( hi , S\ny\n$,\n1, 1, wn\n0). Next, the top ranked candidate analysis, C /= /( D, S , PD, F, wn\ni\n/) , is popped from\nthe priority queue. If S /= $a n d wi\n/= h /si , then the analysis is complete. Otherwise,\nall C\n/0\nsuch that C /) C\n/0\nare pushed onto the priority queue.\n12 Again, for ease of exposition, we will ignore /\u000f -productions. Everything presented here can be\nstraightforwardly extended to include them. The /+ in (i) denotes concatenation. To avoid confusion\nbetween sets and sequences, /;; will not be used for empty strings or sequences, rather the symbol hi\nwill be used. Note that the script S is used to denote stacks, while S\ny\nis the start symbol.\n261\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nWe implement this as a beam search. For each word position i,w eh a v eas e p a r a t e\npriority queue Hi of analyses with look-ahead wi. When there are “enough” analyses\nby some criteria (which we will discuss below) on priority queue Hi/+ 1, all candidate\nanalyses remaining on Hi are discarded. Since wn\n/= h /si , all parses that are pushed\nonto Hn/+ 1 are complete. The parse on Hn/+ 1 with the highest probability is returned\nfor evaluation. In the case that no complete parse is found, a partial parse is returned\nand evaluated.\nThe LAP is the probability of a particular terminal being the next left-corner of a\nparticular analysis. The terminal may be the left corner of the topmost nonterminal\non the stack of the analysis or it might be the left corner of the nth nonterminal, after\nthe top n\n/; 1 nonterminals have rewritten to /\u000f . Of course, we cannot expect to have\nadequate statistics for each nonterminal/word pair that we encounter, so we smooth\nto the POS. Since we do not know the POS for the word, we must sum the LAP for\nall POS labels.\n13\nFor a PCFG G,as t a c kS /= A0\n/:/:/: An$ (which we will write An\n0$) and a look-ahead\nterminal item wi, we deﬁne the look-ahead probability as follows:\nLAP/( S , wi\n/) /=\nX\n/\u000b /2 /( V/[ T/)\n/\u0003\nPG\n/( An\n0\n/?\n/! wi\n/\u000b /) /( 9/)\nWe recursively estimate this with two empirically observed conditional probabilities\nfor every nonterminal Ai:\nb\nP/( Ai\n/?\n/! wi\n/\u000b /) and\nb\nP/( Ai\n/?\n/! /\u000f /) . The same empirical probability,\nb\nP/( Ai\n/?\n/! X/\u000b /) , is collected for every preterminal X as well. The LAP approximation for\na given stack state and look-ahead terminal is:\nPG\n/( An\nj\n/?\n/! wi\n/\u000b /) /\u0019 PG\n/( Aj\n/?\n/! wi\n/\u000b /) /+\nb\nP/( Aj\n/?\n/! /\u000f /) PG\n/( An\nj/+ 1\n/?\n/! wi\n/\u000b /) /( 10/)\nwhere\nPG\n/( Aj\n/?\n/! wi\n/\u000b /) /\u0019 /\u0015\nAj\nb\nP/( Aj\n/?\n/! wi\n/\u000b /) /+ /( 1 /; /\u0015\nAj\n/)\nX\nX/2 V\nb\nP/( Aj\n/?\n/! X/\u000b /)\nb\nP/( X /! wi\n/) /( 11/)\nT h el a m b d a sa r eaf u n c t i o no ft h ef r e q u e n c yo ft h en o n t e r m i n a lAj,i nt h es t a n d a r d\nway (Jelinek and Mercer 1980).\nThe beam threshold at word wi is a function of the probability of the top-ranked\ncandidate analysis on priority queue Hi/+ 1 and the number of candidates on Hi/+ 1.T h e\nbasic idea is that we want the beam to be very wide if there are few analyses that have\nbeen advanced, but relatively narrow if many analyses have been advanced. If\n/~\np is the\nprobability of the highest-ranked analysis on Hi/+ 1, then another analysis is discarded\nif its probability falls below\n/~\npf /( /\r , j Hi/+ 1\nj /) ,w h e r e/\r is an initial parameter, which we\ncall the base beam factor . For the current study, /\r was 10\n/; 11, unless otherwise noted,\nand f /( /\r , j Hi/+ 1\nj /) /= /\r j Hi/+ 1\nj\n3. Thus, if 100 analyses have already been pushed onto Hi/+ 1,\nthen a candidate analysis must have a probability above 10\n/; 5\n/~\np to avoid being pruned.\nAfter 1,000 candidates, the beam has narrowed to 10\n/; 2\n/~\np. There is also a maximum\nnumber of allowed analyses on Hi, in case the parse fails to advance an analysis to\nHi/+ 1. This was typically 10,000.\nAs mentioned in Section 2.1, we left-factor the grammar, so that all productions are\nbinary, except those with a single terminal on the right-hand side and epsilon produc-\ntions. The only\n/\u000f -productions are those introduced by left-factorization. Our factored\n13 Equivalently, we can split the analyses at this point, so that there is one POS per analysis.\n262\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\ngrammar was produced by factoring the trees in the training corpus before grammar\ninduction, which proceeded in the standard way, by counting rule frequencies.\n5. Empirical Results\nThe empirical results will be presented in three stages: (i) trials to examine the accuracy\nand efﬁciency of the parser; (ii) trials to examine its effect on test corpus perplexity\nand recognition performance; and (iii) trials to examine the effect of beam variation\non these performance measures. Before presenting the results, we will introduce the\nmethods of evaluation.\n5.1 Evaluation\nPerplexity is a standard measure within the speech recognition community for com-\nparing language models. In principle, if two models are tested on the same test corpus,\nthe model that assigns the lower perplexity to the test corpus is the model closest to\nthe true distribution of the language, and thus better as a prior model for speech\nrecognition. Perplexity is the exponential of the cross entropy, which we will deﬁne\nnext.\nGiven a random variable X with distribution p and a probability model q,t h ec r o s s\nentropy, H\n/( p, q/) is deﬁned as follows:\nH/( p, q/) /= /;\nX\nx/2 X\np/( x/) log q/( x/) /( 12/)\nLet p be the true distribution of the language. Then, under certain assumptions, given\na large enough sample, the sample mean of the negative log probability of a model\nwill converge to its cross entropy with the true model. 14 That is\nH/( p, q/) /= /; lim\nn/!/1\n1\nn log q/( wn\n0\n/) /( 13/)\nwhere wn\n0 is a string of the language L. In practice, one takes a large sample of the\nlanguage, and calculates the negative log probability of the sample, normalized by its\nsize.\n15 The lower the cross entropy (i.e., the higher the probability the model assigns\nto the sample), the better the model. Usually this is reported in terms of perplexity,\nwhich we will do as well. 16\nSome of the trials discussed below will report results in terms of word and/or\nsentence error rate, which are obtained when the language model is embedded in a\nspeech recognition system. Word error rate is the number of deletion, insertion, or\nsubstitution errors per 100 words. Sentence error rate is the number of sentences with\none or more errors per 100 sentences.\nStatistical parsers are typically evaluated for accuracy at the constituent level,\nrather than simply whether or not the parse that the parser found is completely correct\nor not. A constituent for evaluation purposes consists of a label (e.g., NP) and a span\n(beginning and ending word positions). For example, in Figure 1(a), there is a VP that\nspans the words “\nchased the ball ”. Evaluation is carried out on a hand-parsed test\ncorpus, and the manual parses are treated as correct. We will call the manual parse\n14 See Cover and Thomas (1991) for a discussion of the Shannon-McMillan-Breiman theorem, under the\nassumptions of which this convergence holds.\n15 It is important to remember to include the end marker in the strings of the sample.\n16 When assessing the magnitude of a perplexity improvement, it is often better to look at the reduction\nin cross entropy, by taking the log of the perplexity. It will be left to the reader to do so.\n263\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nGOLD and the parse that the parser returns TEST. Precision is the number of common\nconstituents in GOLD and TEST divided by the number of constituents in TEST. Recall\nis the number of common constituents in GOLD and TEST divided by the number of\nconstituents in GOLD. Following standard practice, we will be reporting scores only\nfor non-part-of-speech constituents, which are called labeled recall (LR) and labeled\nprecision (LP). Sometimes in ﬁgures we will plot their average, and also what can be\ntermed the parse error, which is one minus their average.\nLR and LP are part of the standard set of\nPARSEVAL measures of parser qual-\nity (Black et al. 1991). From this set of measures, we will also include the crossing\nbracket scores: average crossing brackets (CB), percentage of sentences with no cross-\ning brackets (0 CB), and the percentage of sentences with two crossing brackets or\nfewer (\n/\u0014 2 CB). In addition, we show the average number of rule expansions con-\nsidered per word, that is, the number of rule expansions for which a probability was\ncalculated (see Roark and Charniak [2000]), and the average number of analyses ad-\nvanced to the next priority queue per word.\nThis is an incremental parser with a pruning strategy and no backtracking. In such\na model, it is possible to commit to a set of partial analyses at a particular point that\ncannot be completed given the rest of the input string (i.e., the parser can “garden\npath”). In such a case, the parser fails to return a complete parse. In the event that\nno complete parse is found, the highest initially ranked parse on the last nonempty\npriority queue is returned. All unattached words are then attached at the highest\nlevel in the tree. In such a way we predict no new constituents and all incomplete\nconstituents are closed. This structure is evaluated for precision and recall, which is\nentirely appropriate for these incomplete as well as complete parses. If we fail to\nidentify nodes later in the parse, recall will suffer, and if our early predictions were\nbad, both precision and recall will suffer. Of course, the percentage of these failures\nare reported as well.\n5.2 Parser Accuracy and Efﬁciency\nThe ﬁrst set of results looks at the performance of the parser on the standard corpora\nfor statistical parsing trials: Sections 2–21 (989,860 words, 39,832 sentences) of the\nPenn Treebank (Marcus, Santorini, and Marcinkiewicz 1993) served as the training\ndata, Section 24 (34,199 words, 1,346 sentences) as the held-out data for parameter\nestimation, and Section 23 (59,100 words, 2,416 sentences) as the test data. Section\n22 (41,817 words, 1,700 sentences) served as the development corpus, on which the\nparser was tested until stable versions were ready to run on the test data, to avoid\ndeveloping the parser to ﬁt the speciﬁc test data.\nTable 2 shows trials with increasing amounts of conditioning information from the\nleft context. There are a couple of things to notice from these results. First, and least\nsurprising, is that the accuracy of the parses improved as we conditioned on more\nand more information. Like the nonlexicalized parser in Roark and Johnson (1999),\nwe found that the search efﬁciency, in terms of number of rule expansions consid-\nered or number of analyses advanced, also improved as we increased the amount of\nconditioning. Unlike the Roark and Johnson parser, however, our coverage did not\nsubstantially drop as the amount of conditioning information increased, and in some\ncases, coverage improved slightly. They did not smooth their conditional probability\nestimates, and blamed sparse data for their decrease in coverage as they increased the\nconditioning information. These results appear to support this, since our smoothed\nmodel showed no such tendency.\nFigure 5 shows the reduction in parser error, 1\n/;\nLR/+ LP\n2 , and the reduction in\nrule expansions considered as the conditioning information increased. The bulk of\n264\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nTable 2\nResults conditioning on various contextual events, standard training and testing corpora.\nConditioning LR LP CB 0 CB /\u0014 2 CB Percent Average Rule Average\nFailed Expansions Analyses\nConsidered\ny\nAdvanced\ny\nSection 23: 2245 sentences of length /\u0014 40\nnone 71.1 75.3 2.48 37.3 62.9 0.9 14,369 516.5\npar+sib 82.8 83.6 1.55 54.3 76.2 1.1 9,615 324.4\nNT struct 84.3 84.9 1.38 56.7 79.5 1.0 8,617 284.9\nNT head 85.6 85.7 1.27 59.2 81.3 0.9 7,600 251.6\nPOS struct 86.1 86.2 1.23 60.9 82.0 1.0 7,327 237.9\nattach 86.7 86.6 1.17 61.7 83.2 1.2 6,834 216.8\nall 86.6 86.5 1.19 62.0 82.7 1.3 6,379 198.4\nSection 23: 2416 sentences of length\n/\u0014 100\nattach 85.8 85.8 1.40 58.9 80.3 1.5 7,210 227.9\nall 85.7 85.7 1.41 59.0 79.9 1.7 6,709 207.6\ny\nper word\n(0,0,0) (2,2,2) (5,2,2) (6,2,2) (6,3,2) (6,5,2) (6,6,4)\n−60\n−50\n−40\n−30\n−20\n−10\n0\nConditioning information\nPercentage Reduction\nParse error               \nRule expansions           \nFigure 5\nReduction in average precision/recall error and in number of rule expansions per word as\nconditioning increases, for sentences of length\n/\u0014 40.\nthe improvement comes from simply conditioning on the labels of the parent and\nthe closest sibling to the node being expanded. Interestingly, conditioning all POS\nexpansions on two c-commanding heads made no difference in accuracy compared to\nconditioning only leftmost POS expansions on a single c-commanding head; but it did\nimprove the efﬁciency.\nThese results, achieved using very straightforward conditioning events and con-\nsidering only the left context, are within one to four points of the best published\n265\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\n0 10 20 30 40 50 60 700\n5\n10\n15\n20\n25\n30\nSentence Length\nSeconds\nIndividual parses          \nRatnaparkhi observed time  \nFigure 6\nObserved running time on Section 23 of the Penn Treebank, with the full conditional\nprobability model and beam of 10\n/; 11, using one 300 MHz UltraSPARC processor and 256MB\nof RAM of a Sun Enterprise 450.\naccuracies cited above. 17 Of the 2,416 sentences in the section, 728 had the totally cor-\nrect parse, 30.1 percent tree accuracy. Also, the parser returns a set of candidate parses,\nfrom which we have been choosing the top ranked; if we use an oracle to choose the\nparse with the highest accuracy from among the candidates (which averaged 70.0 in\nnumber per sentence), we ﬁnd an average labeled precision/recall of 94.1, for sentences\nof length\n/\u0014 100. The parser, thus, could be used as a front end to some other model,\nwith the hopes of selecting a more accurate parse from among the ﬁnal candidates.\nWhile we have shown that the conditioning information improves the efﬁciency in\nterms of rule expansions considered and analyses advanced, what does the efﬁciency\nof such a parser look like in practice? Figure 6 shows the observed time at our standard\nbase beam of 10\n/; 11 with the full conditioning regimen, alongside an approximation\nof the reported observed (linear) time in Ratnaparkhi (1997). Our observed times look\npolynomial, which is to be expected given our pruning strategy: the denser the com-\npetitors within a narrow probability range of the best analysis, the more time will be\nspent working on these competitors; and the farther along in the sentence, the more\nchance for ambiguities that can lead to such a situation. While our observed times are\nnot linear, and are clearly slower than his times (even with a faster machine), they are\nquite respectably fast. The differences between a k-best and a beam-search parser (not\nto mention the use of dynamic programming) make a running time difference unsur-\n17 Our score of 85.8 average labeled precision and recall for sentences less than or equal to 100 on\nSection 23 compares to: 86.7 in Charniak (1997), 86.9 in Ratnaparkhi (1997), 88.2 in Collins (1999), 89.6\nin Charniak (2000), and 89.75 in Collins (2000).\n266\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nprising. What is perhaps surprising is that the difference is not greater. Furthermore,\nthis is quite a large beam (see discussion below), so that very large improvements in\nefﬁciency can be had at the expense of the number of analyses that are retained.\n5.3 Perplexity Results\nThe next set of results will highlight what recommends this approach most: the ease\nwith which one can estimate string probabilities in a single pass from left to right across\nthe string. By deﬁnition, a PCFG’s estimate of a string’s probability is the sum of the\nprobabilities of all trees that produce the string as terminal leaves (see Equation 1).\nIn the beam search approach outlined above, we can estimate the string’s probability\nin the same manner, by summing the probabilities of the parses that the algorithm\nﬁnds. Since this is not an exhaustive search, the parses that are returned will be a\nsubset of the total set of trees that would be used in the exact PCFG estimate; hence\nthe estimate thus arrived at will be bounded above by the probability that would be\ngenerated from an exhaustive search. The hope is that a large amount of the probability\nmass will be accounted for by the parses in the beam. The method cannot overestimate\nthe probability of the string.\nRecall the discussion of the grammar models above, and our deﬁnition of the set\nof partial derivations D\nw j\n0\nwith respect to a preﬁx string w j\n0 (see Equations 2 and 7).\nBy deﬁnition,\nP/( wj/+ 1\nj w j\n0\n/) /=\nP/( w j/+ 1\n0\n/)\nP/( w j\n0\n/)\n/=\nP\nd/2 D\nw j/+ 1\n0\nP/( d/)\nP\nd/2 D\nw j\n0\nP/( d/)\n/( 14/)\nNote that the numerator at word wj is the denominator at word wj/+ 1, so that the\nproduct of all of the word probabilities is the numerator at the ﬁnal word, namely, the\nstring preﬁx probability.\nWe can make a consistent estimate of the string probability by similarly summing\nover all of the trees within our beam. Let H\ninit\ni be the priority queue Hi before any\nprocessing has begun with word wi in the look-ahead. This is a subset of the possi-\nble leftmost partial derivations with respect to the preﬁx string wi/; 1\n0 .S i n c eH\ninit\ni/+ 1 is\nproduced by expanding only analyses on priority queue H\ninit\ni ,t h es e to fc o m p l e t e\ntrees consistent with the partial derivations on priority queue H\ninit\ni/+ 1 is a subset of the\nset of complete trees consistent with the partial derivations on priority queue H\ninit\ni ,\nthat is, the total probability mass represented by the priority queues is monotoni-\ncally decreasing. Thus conditional word probabilities deﬁned in a way consistent with\nEquation 14 will always be between zero and one. Our conditional word probabilities\nare calculated as follows:\nP\n/( wi\nj wi/; 1\n0\n/) /=\nP\nd/2 H\ninit\ni/+ 1\nP/( d/)\nP\nd/2 H\ninit\ni\nP/( d/)\n/( 15/)\nAs mentioned above, the model cannot overestimate the probability of a string,\nbecause the string probability is simply the sum over the beam, which is a subset of\nthe possible derivations. By utilizing a ﬁgure of merit to identify promising analyses,\nwe are simply focusing our attention on those parses that are likely to have a high\nprobability, and thus we are increasing the amount of probability mass that we do\ncapture, of the total possible. It is not part of the probability model itself.\nSince each word is (almost certainly, because of our pruning strategy) losing some\nprobability mass, the probability model is not “proper”—the sum of the probabilities\nover the vocabulary is less than one. In order to have a proper probability distribution,\n267\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nwe would need to renormalize by dividing by some factor. Note, however, that this\nrenormalization factor is necessarily less than one, and thus would uniformly increase\neach word’s probability under the model, that is, any perplexity results reported below\nwill be higher than the “true” perplexity that would be assigned with a properly\nnormalized distribution. In other words, renormalizing would make our perplexity\nmeasure lower still. The hope, however, is that the improved parsing model provided\nby our conditional probability model will cause the distribution over structures to\nbe more peaked, thus enabling us to capture more of the total probability mass, and\nmaking this a fairly snug upper bound on the perplexity.\nOne ﬁnal note on assigning probabilities to strings: because this parser does gar-\nden path on a small percentage of sentences, this must be interpolated with another\nestimate, to ensure that every word receives a probability estimate. In our trials, we\nused the unigram, with a very small mixing coefﬁcient:\nP\n/( wi\nj wi/; 1\n0\n/) /= /\u0015 /( wi/; 1\n0\n/)\nP\nd/2 H\ninit\ni/+ 1\nP/( d/)\nP\nd/2 H\ninit\ni\nP/( d/)\n/+ /( 1 /; /\u0015 /( wi/; 1\n0\n/)/)\nb\nP/( wi\n/) /( 16/)\nIf\nP\nd/2 H\ninit\ni\nP/( d/) /= 0 in our model, then our model provides no distribution over\nfollowing words since the denominator is zero. Thus,\n/\u0015 /( wi/; 1\n0\n/) /=\n/(\n0 if\nP\nd/2 H\ninit\ni\nP/( d/) /= 0\n/: 999 otherwise\n/( 17/)\nChelba and Jelinek (1998a, 1998b) also used a parser to help assign word probabili-\nties, via the structured language model outlined in Section 3.2. They trained and tested\nthe SLM on a modiﬁed, more “speech-like” version of the Penn Treebank. Their mod-\niﬁcations included: (i) removing orthographic cues to structure (e.g., punctuation);\n(ii) replacing all numbers with the single token N; and (iii) closing the vocabulary\nat 10,000, replacing all other words with the UNK token. They used Sections 00–20\n(929,564 words) as the development set, Sections 21–22 (73,760 words) as the check set\n(for interpolation coefﬁcient estimation), and tested on Sections 23–24 (82,430 words).\nWe obtained the training and testing corpora from them (which we will denote C&J\ncorpus), and also created intermediate corpora, upon which only the ﬁrst two modiﬁ-\ncations were carried out (which we will denote no punct). Differences in performance\nwill give an indication of the impact on parser performance of the different modiﬁca-\ntions to the corpora. All trials in this section used Sections 00–20 for counts, held out\n21–22, and tested on 23–24.\nTable 3 shows several things. First, it shows relative performance for unmodiﬁed,\nno punct, and C&J corpora with the full set of conditioning information. We can see\nthat removing the punctuation causes (unsurprisingly) a dramatic drop in the accuracy\nand efﬁciency of the parser. Interestingly, it also causes coverage to become nearly total,\nwith failure on just two sentences per thousand on average.\nWe see the familiar pattern, in the C&J corpus results, of improving performance\nas the amount of conditioning information grows. In this case we have perplexity\nresults as well, and Figure 7 shows the reduction in parser error, rule expansions, and\nperplexity as the amount of conditioning information grows. While all three seem to be\nsimilarly improved by the addition of structural context (e.g., parents and siblings), the\naddition of c-commanding heads has only a moderate effect on the parser accuracy,\nbut a very large effect on the perplexity. The fact that the efﬁciency was improved\nmore than the accuracy in this case (as was also seen in Figure 5), seems to indicate\nthat this additional information is causing the distribution to become more peaked, so\nthat fewer analyses are making it into the beam.\n268\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nTable 3\nResults conditioning on various contextual events, Sections 23–24, modiﬁcations following\nChelba and Jelinek.\nCorpora Conditioning LR LP Percent Perplexity Average Rule Average\nFailed Expansions Analyses\nConsidered\ny\nAdvanced\ny\nSections 23–24: 3761 sentences /\u0014 120\nunmodiﬁed all 85.2 85.1 1.7 7,206 213.5\nno punct all 82.4 82.9 0.2 9,717 251.8\nC&J corpus par+sib 75.2 77.4 0.1 310.04 17,418 457.2\nC&J corpus NT struct 77.3 79.2 0.1 290.29 15,948 408.8\nC&J corpus NT head 79.2 80.4 0.1 255.85 14,239 363.2\nC&J corpus POS struct 80.5 81.6 0.1 240.37 13,591 341.3\nC&J corpus all 81.7 82.1 0.2 152.26 11,667 279.7\ny\nper word\n(2,2,2) (5,2,2) (6,2,2) (6,3,2) (6,6,4)\n−60\n−50\n−40\n−30\n−20\n−10\n0\nConditioning information\nPercentage Reduction\nParse error         \nRule expansions     \nPerplexity          \nFigure 7\nReduction in average precision/recall error, number of rule expansions, and perplexity as\nconditioning increases.\nTable 4 compares the perplexity of our model with Chelba and Jelinek (1998a,\n1998b) on the same training and testing corpora. We built an interpolated trigram\nmodel to serve as a baseline (as they did), and also interpolated our model’s perplexity\nwith the trigram, using the same mixing coefﬁcient as they did in their trials (taking\n36 percent of the estimate from the trigram).\n18 The trigram model was also trained\non Sections 00–20 of the C&J corpus. Trigrams and bigrams were binned by the total\n18 Our optimal mixture level was closer to 40 percent, but the difference was negligible.\n269\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nTable 4\nComparison with previous perplexity results.\nPerplexity\nPaper Trigram Baseline Model Interpolation, /\u0015 /= /: 36\nChelba and Jelinek (1998a) 167.14 158.28 148.90\nChelba and Jelinek (1998b) 167.14 153.76 147.70\nCurrent results 167.02 152.26 137.26\ncount of the conditioning words in the training corpus, and maximum likelihood\nmixing coefﬁcients were calculated for each bin, to mix the trigram with bigram and\nunigram estimates. Our trigram model performs at almost exactly the same level as\ntheirs does, which is what we would expect. Our parsing model’s perplexity improves\nupon their ﬁrst result fairly substantially, but is only slightly better than their second\nresult.\n19 However, when we interpolate with the trigram, we see that the additional\nimprovement is greater than the one they experienced. This is not surprising, since our\nconditioning information is in many ways orthogonal to that of the trigram, insofar\nas it includes the probability mass of the derivations; in contrast, their model in some\ninstances is very close to the trigram, by conditioning on two words in the preﬁx\nstring, which may happen to be the two adjacent words.\nThese results are particularly remarkable, given that we did not build our model as\na language model per se, but rather as a parsing model. The perplexity improvement\nwas achieved by simply taking the existing parsing model and applying it, with no\nextra training beyond that done for parsing.\nThe hope was expressed above that our reported perplexity would be fairly close\nto the “true” perplexity that we would achieve if the model were properly normal-\nized, i.e., that the amount of probability mass that we lose by pruning is small. One\nway to test this is the following: at each point in the sentence, calculate the condi-\ntional probability of each word in the vocabulary given the previous words, and sum\nthem.\n20 If there is little loss of probability mass, the sum should be close to one. We\ndid this for the ﬁrst 10 sentences in the test corpus, a total of 213 words (including\nthe end-of-sentence markers). One of the sentences was a failure, so that 12 of the\nword probabilities (all of the words after the point of the failure) were not estimated\nby our model. Of the remaining 201 words, the average sum of the probabilities over\nthe 10,000-word vocabulary was 0.9821, with a minimum of 0.7960 and a maximum\nof 0.9997. Interestingly, at the word where the failure occurred, the sum of the proba-\nbilities was 0.9301.\n5.4 Word Error Rate\nIn order to get a sense of whether these perplexity reduction results can translate to\nimprovement in a speech recognition task, we performed a very small preliminary\nexperiment on n-best lists. The DARPA ’93 HUB1 test setup consists of 213 utter-\nances read from the Wall Street Journal, a total of 3,446 words. The corpus comes with\na baseline trigram model, using a 20,000-word open vocabulary, and trained on ap-\nproximately 40 million words. We used Ciprian Chelba’s A\n/?\ndecoder to ﬁnd the 50\nbest hypotheses from each lattice, along with the acoustic and trigram scores. 21 Given\n19 Recall that our perplexity measure should, ideally, be even lower still.\n20 Thanks to Ciprian Chelba for this suggestion.\n21 See Chelba (2000) for details on the decoder.\n270\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nTable 5\nWord and sentence error rate results for various models, with differing training and\nvocabulary sizes, for the best language model factor for that particular model.\nPercentage Percentage\nTraining Vocabulary LM Word Error Sentence\nModel Size Size Weight Rate Error Rate\nLattice trigram 40M 20K 16 13.7 69.0\nChelba (2000) (\n/\u0015 /= /: 4) 20M 20K 16 13.0\nCurrent model 1M 10K 15 15.1 73.2\nTreebank trigram 1M 10K 5 16.5 79.8\nNo language model 0 16.8 84.0\nthe idealized circumstances of the production (text read in a lab), the lattices are rel-\natively sparse, and in many cases 50 distinct string hypotheses were not found in\na lattice. We reranked an average of 22.9 hypotheses with our language model per\nutterance.\nOne complicating issue has to do with the tokenization in the Penn Treebank\nversus that in the HUB1 lattices. In particular, contractions (e.g.,\nhe/'s ) are split in the\nPenn Treebank ( he /'s ) but not in the HUB1 lattices. Splitting of the contractions is\ncritical for parsing, since the two parts oftentimes (as in the previous example) fall\nin different constituents. We follow Chelba (2000) in dealing with this problem: for\nparsing purposes, we use the Penn Treebank tokenization; for interpolation with the\nprovided trigram model, and for evaluation, the lattice tokenization is used. If we are to\ninterpolate our model with the lattice trigram, we must wait until we have our model’s\nestimate for the probability of both parts of the contraction; their product can then be\ninterpolated with the trigram estimate. In fact, interpolation in these trials made no\nimprovement over the better of the uninterpolated models, but simply resulted in\nperformance somewhere between the better and the worse of the two models, so we\nwill not present interpolated trials here.\nTable 5 reports the word and sentence error rates for ﬁve different models: (i) the\ntrigram model that comes with the lattices, trained on approximately 40M words, with\na vocabulary of 20,000; (ii) the best-performing model from Chelba (2000), which was\ninterpolated with the lattice trigram at\n/\u0015 /= 0/: 4; (iii) our parsing model, with the same\ntraining and vocabulary as the perplexity trials above; (iv) a trigram model with the\nsame training and vocabulary as the parsing model; and (v) no language model at all.\nThis last model shows the performance from the acoustic model alone, without the\ninﬂuence of the language model. The log of the language model score is multiplied\nby the language model (LM) weight when summing the logs of the language and\nacoustic scores, as a way of increasing the relative contribution of the language model\nto the composite score. We followed Chelba (2000) in using an LM weight of 16 for\nthe lattice trigram. For our model and the Treebank trigram model, the LM weight\nthat resulted in the lowest error rates is given.\nThe small size of our training data, as well as the fact that we are rescoring n-best\nlists, rather than working directly on lattices, makes comparison with the other models\nnot particularly informative. What is more informative is the difference between our\nmodel and the trigram trained on the same amount of data. We achieved an 8.5 percent\nrelative improvement in word error rate, and an 8.3 percent relative improvement\nin sentence error rate over the Treebank trigram. Interestingly, as mentioned above,\ninterpolating two models together gave no improvement over the better of the two,\nwhether our model was interpolated with the lattice or the Treebank trigram. This\n271\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nTable 6\nResults with full conditioning on the C&J corpus at various base beam factors.\nBase LR LP Percentage Perplexity Perplexity Average Rule Words Per\nBeam Failed\n/\u0015 /= 0 /\u0015 /= /: 36 Expansions Second\nFactor Considered\ny\nSections 23–24: 3761 sentences /\u0014 120\n10\n/; 11 81.7 82.1 0.2 152.26 137.26 11,667 3.1\n10\n/; 10 81.5 81.9 0.3 154.25 137.88 6,982 5.2\n10\n/; 9 80.9 81.3 0.4 156.83 138.69 4,154 8.9\n10\n/; 8 80.2 80.6 0.6 160.63 139.80 2,372 15.3\n10\n/; 7 78.8 79.2 1.2 166.91 141.30 1,468 25.5\n10\n/; 6 77.4 77.9 1.5 174.44 143.05 871 43.8\n10\n/; 5 75.8 76.3 2.6 187.11 145.76 517 71.6\n10\n/; 4 72.9 73.9 4.5 210.28 148.41 306 115.5\n10\n/; 3 68.4 70.6 8.0 253.77 152.33 182 179.6\ny\nper word\ncontrasts with our perplexity results reported above, as well as with the recognition\nexperiments in Chelba (2000), where the best results resulted from interpolated models.\nThe point of this small experiment was to see if our parsing model could provide\nuseful information even in the case that recognition errors occur, as opposed to the\n(generally) fully grammatical strings upon which the perplexity results were obtained.\nAs one reviewer pointed out, given that our model relies so heavily on context, it may\nhave difﬁculty recovering from even one recognition error, perhaps more difﬁculty\nthan a more locally oriented trigram. While the improvements over the trigram model\nin these trials are modest, they do indicate that our model is robust enough to provide\ngood information even in the face of noisy input. Future work will include more\nsubstantial word recognition experiments.\n5.5 Beam Variation\nThe last set of results that we will present addresses the question of how wide the beam\nmust be for adequate results. The base beam factor that we have used to this point\nis 10\n/; 11, which is quite wide. It was selected with the goal of high parser accuracy;\nbut in this new domain, parser accuracy is a secondary measure of performance. To\ndetermine the effect on perplexity, we varied the base beam factor in trials on the\nChelba and Jelinek corpora, keeping the level of conditioning information constant,\nand Table 6 shows the results across a variety of factors.\nThe parser error, parser coverage, and the uninterpolated model perplexity (\n/\u0015 /= 1)\nall suffered substantially from a narrower search, but the interpolated perplexity re-\nmained quite good even at the extremes. Figure 8 plots the percentage increase in\nparser error, model perplexity, interpolated perplexity, and efﬁciency (i.e., decrease in\nrule expansions per word) as the base beam factor decreased. Note that the model per-\nplexity and parser accuracy are quite similarly affected, but that the interpolated per-\nplexity remained far below the trigram baseline, even with extremely narrow beams.\n6. Conclusion and Future Directions\nThe empirical results presented above are quite encouraging, and the potential of this\nkind of approach both for parsing and language modeling seems very promising.\n272\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\n−11 −10 −9 −8 −7 −6 −5 −4 −30\n10\n20\n30\n40\n50\n60\n70\n80\n90\n100\nlog10 of base beam factor\nPercentage Increase\nParse error               \nModel Perplexity          \nInterpolated Perplexity   \nEfficiency  (decrease in  \n     rule expansions)     \nFigure 8\nIncrease in average precision/recall error, model perplexity, interpolated perplexity, and\nefﬁciency (i.e., decrease in rule expansions per word) as base beam factor decreases.\nWith a simple conditional probability model, and simple statistical search heuristics,\nwe were able to ﬁnd very accurate parses efﬁciently, and, as a side effect, were able to\nassign word probabilities that yield a perplexity improvement over previous results.\nThese perplexity improvements are particularly promising, because the parser is pro-\nviding information that is, in some sense, orthogonal to the information provided by\na trigram model, as evidenced by the robust improvements to the baseline trigram\nwhen the two models are interpolated.\nThere are several important future directions that will be taken in this area. First,\nthere is reason to believe that some of the conditioning information is not uniformly\nuseful, and we would beneﬁt from ﬁner distinctions. For example, the probability\nof a preposition is presumably more dependent on a c-commanding head than the\nprobability of a determiner is. Yet in the current model they are both conditioned\non that head, as leftmost constituents of their respective phrases. Second, there are\nadvantages to top-down parsing that have not been examined to date, e.g., empty\ncategories. A top-down parser, in contrast to a standard bottom-up chart parser, has\nenough information to predict empty categories only where they are likely to occur.\nBy including these nodes (which are in the original annotation of the Penn Treebank),\nwe may be able to bring certain long-distance dependencies into a local focus. In\naddition, as mentioned above, we would like to further test our language model in\nspeech recognition tasks, to see if the perplexity improvement that we have seen can\nlead to signiﬁcant reductions in word error rate.\nOther parsing approaches might also be used in the way that we have used a top-\ndown parser. Earley and left-corner parsers, as mentioned in the introduction, also\nhave rooted derivations that can be used to calculated generative string preﬁx proba-\n273\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nbilities incrementally. In fact, left-corner parsing can be simulated by a top-down parser\nby transforming the grammar, as was done in Roark and Johnson (1999), and so an\napproach very similar to the one outlined here could be used in that case. Perhaps\nsome compromise between the fully connected structures and extreme underspeciﬁca-\ntion will yield an efﬁciency improvement. Also, the advantages of head-driven parsers\nmay outweigh their inability to interpolate with a trigram, and lead to better off-line\nlanguage models than those that we have presented here.\nDoes a parsing model capture exactly what we need for informed language mod-\neling? The answer to that is no. Some information is simply not structural in nature\n(e.g., topic), and we might expect other kinds of models to be able to better handle\nnonstructural dependencies. The improvement that we derived from interpolating the\ndifferent models above indicates that using multiple models may be the most fruitful\npath in the future. In any case, a parsing model of the sort that we have presented\nhere should be viewed as an important potential source of key information for speech\nrecognition. Future research will show if this early promise can be fully realized.\nAcknowledgments\nThe author wishes to thank Mark Johnson\nfor invaluable discussion, guidance, and\nmoral support over the course of this\nproject. Many thanks also to Eugene\nCharniak for the use of certain grammar\ntraining routines, and for an enthusiastic\ninterest in the project. Thanks also to four\nanonymous reviewers for valuable and\ninsightful comments, and to Ciprian Chelba,\nSanjeev Khudanpur, and Frederick Jelinek\nfor comments and suggestions. Finally, the\nauthor would like to express his\nappreciation to the participants of\ndiscussions during meetings of the Brown\nLaboratory for Linguistic Information\nProcessing (BLLIP); in addition to Mark and\nEugene: Yasemin Altun, Don Blaheta,\nSharon Caraballo, Massimiliano Ciaramita,\nHeidi Fox, Niyu Ge, and Keith Hall. This\nresearch was supported in part by NSF\nIGERT Grant #DGE-9870676.\nReferences\nAho, Alfred V ., Ravi Sethi, and Jeffrey D.\nUllman. 1986. Compilers, Principles,\nTechniques, and Tools. Addison-Wesley,\nReading, MA.\nBlack, Ezra, Steven Abney, Dan Flickenger,\nClaudia Gdaniec, Ralph Grishman, Philip\nHarrison, Donald Hindle, Robert Ingria,\nFrederick Jelinek, Judith Klavans, Mark\nL i b e r m a n ,M i t c h e l lP .M a r c u s ,S a l i m\nRoukos, Beatrice Santorini, and Tomek\nStrzalkowski. 1991. A procedure for\nquantitatively comparing the syntactic\ncoverage of english grammars. In DARP A\nSpeech and Natural Language Workshop,\npages 306–311.\nBriscoe, Ted and John Carroll. 1993.\nGeneralized probabilistic\nlr parsing of\nnatural language (corpora) with\nuniﬁcation-based grammars.\nComputational Linguistics, 19(1):25–60.\nCaraballo, Sharon and Eugene Charniak.\n1998. New ﬁgures of merit for best-ﬁrst\nprobabilistic chart parsing. Computational\nLinguistics, 24(2):275–298.\nCharniak, Eugene. 1997. Statistical parsing\nwith a context-free grammar and word\nstatistics. In Proceedings of the Fourteenth\nNational Conference on Artiﬁcial Intelligence,\npages 598–603, Menlo Park. AAAI\nPress/MIT Press.\nCharniak, Eugene. 2000. A\nmaximum-entropy-inspired parser. In\nProceedings of the 1st Conference of the North\nAmerican Chapter of the Association for\nComputational Linguistics, pages 132–139.\nCharniak, Eugene, Sharon Goldwater, and\nMark Johnson. 1998. Edge-based best-ﬁrst\nchart parsing. In Proceedings of the Sixth\nWorkshop on Very Large Corpora,\npages 127–133.\nChelba, Ciprian. 2000. Exploiting Syntactic\nStructure for Natural Language Modeling.\nPh.D. thesis, The Johns Hopkins\nUniversity.\nChelba, Ciprian and Frederick Jelinek.\n1998a. Exploiting syntactic structure for\nlanguage modeling. In Proceedings of the\n36th Annual Meeting of the Association for\nComputational Linguistics and 17th\nInternational Conference on Computational\nLinguistics, pages 225–231.\nChelba, Ciprian and Frederick Jelinek.\n1998b. Reﬁnement of a structured\nlanguage model. In International\nConference on Advances in Pattern\n274\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nRoark Top-Down Parsing\nRecognition, pages 275–284.\nChelba, Ciprian and Frederick Jelinek. 1999.\nRecognition performance of a structured\nlanguage model. In Proceedings of the 6th\nEuropean Conference on Speech\nCommunication and Technology (Eurospeech),\npages 1,567–1,570.\nChi, Zhiyi and Stuart Geman. 1998.\nEstimation of probabilistic context-free\ngrammars. Computational Linguistics,\n24(2):299–305.\nCollins, Michael J. 1997. Three generative,\nlexicalised models for statistical parsing.\nIn Proceedings of the 35th Annual Meeting,\npages 16–23. Association for\nComputational Linguistics.\nCollins, Michael J. 1999. Head-Driven\nStatistical Models for Natural Language\nParsing.P h . D .t h e s i s ,U n i v e r s i t yo f\nPennsylvania.\nCollins, Michael J. 2000. Discriminative\nreranking for natural language parsing. In\nThe Proceedings of the 17th International\nConference on Machine Learning,\npages 175–182.\nCover, Thomas M. and Joy A. Thomas.\n1991. Elements of Information Theory.J o h n\nWiley and Sons, Inc., New York.\nEarley, Jay. 1970. An efﬁcient context-free\nparsing algorithm. Communications of the\nACM, 6(8):451–455.\nEisner, J. and G. Satta. 1999. Efﬁcient\nparsing for bilexical context-free\ngrammars and head automaton\ngrammars. In Proceedings of the 37th Annual\nMeeting, pages 457–464. Association for\nComputational Linguistics.\nGoddeau, David. 1992. Using probabilistic\nshift-reduce parsing in speech recognition\nsystems. In Proceedings of the 2nd\nInternational Conference on Spoken Language\nProcessing, pages 321–324.\nGoodman, Joshua. 1997. Global\nthresholding and multiple-pass parsing.\nIn Proceedings of the Second Conference on\nEmpirical Methods in Natural Language\nProcessing (EMNLP-97), pages 11–25.\nHenderson, John C. and Eric Brill. 1999.\nExploiting diversity in natural language\nprocessing: Combining parsers. In\nProceedings of the Fourth Conference on\nEmpirical Methods in Natural Language\nProcessing (EMNLP-99), pages 187–194.\nHopcroft, John E. and Jeffrey D. Ullman.\n1979. Introduction to Automata Theory,\nLanguages and Computation.\nAddison-Wesley.\nInui, Kentaro, Virach Sornlertlamvanich,\nHozummi Tanaka, and Takenobu\nTokunaga. 1997. A new formalization of\nprobabilistic\nglr parsing. In Proceedings of\nthe 5th International Workshop on Parsing\nTechnol ogi es, pages 123–134.\nJelinek, Frederick. 1997. Statistical Methods\nfor Speech Recognition. MIT Press,\nCambridge, MA.\nJelinek, Frederick and Ciprian Chelba. 1999.\nPutting language into language modeling.\nIn Proceedings of the 6th European Conference\non Speech Communication and Technology\n(Eurospeech), pages KN1–6.\nJelinek, Frederick and John D. Lafferty. 1991.\nComputation of the probability of initial\nsubstring generation by stochastic\ncontext-free grammars. Computational\nLinguistics, 17(3):315–323.\nJelinek, Frederick and Robert L. Mercer.\n1980. Interpolated estimation of Markov\nsource parameters from sparse data. In\nProceedings of the Workshop on Pattern\nRecognition in Practice, pages 381–397.\nJohnson, Mark. 1998.\nPCF G models of\nlinguistic tree representations.\nComputational Linguistics, 24(4):617–636.\nJohnson, Mark, Stuart Geman, Stephen\nCanon, Zhiyi Chi, and Stefan Riezler.\n1999. Estimators for stochastic\n“uniﬁcation-based” grammars. In\nProceedings of the 37th Annual Meeting,\npages 535–541. Association for\nComputational Linguistics.\nJurafsky, Daniel, Chuck Wooters, Jonathan\nSegal, Andreas Stolcke, Eric Fosler, Gary\nTajchman, and Nelson Morgan. 1995.\nUsing a stochastic context-free grammar\nas a language model for speech\nrecognition. In Proceedings of the IEEE\nConference on Acoustics, Speech, and Signal\nProcessing, pages 189–192.\nManning, Christopher D. and Hinrich\nSch ¨utze. 1999. Foundations of Statistical\nNatural Language Processing. MIT Press,\nCambridge, MA.\nMarcus, Mitchell P ., Beatrice Santorini, and\nMary Ann Marcinkiewicz. 1993. Building\na large annotated corpus of English: The\nPenn Treebank. Computational Linguistics,\n19(2):313–330.\nRatnaparkhi, Adwait. 1997. A linear\nobserved time statistical parser based on\nmaximum entropy models. In Proceedings\nof the Second Conference on Empirical\nMethods in Natural Language Processing\n(EMNLP-97), pages 1–10.\nRoark, Brian and Eugene Charniak. 2000.\nMeasuring efﬁciency in high-accuracy,\nbroad-coverage statistical parsing. In\nProceedings of the COLING-2000 Workshop\non Efﬁciency in Large-scale Parsing Systems,\npages 29–36.\nRoark, Brian and Mark Johnson. 1999.\nEfﬁcient probabilistic top-down and\n275\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025\nComputational Linguistics Volume 27, Number 2\nleft-corner parsing. In Proceedings of the\n37th Annual Meeting, pages 421–428.\nAssociation for Computational\nLinguistics.\nRosenfeld, Ronald. 1996. A maximum\nentropy approach to adaptive statistical\nlanguage modeling. Computer, Speech and\nLanguage, 10:187–228.\nRosenfeld, Ronald. 1997. A whole sentence\nmaximum entropy language model. In\nProceedings of IEEE Workshop on Speech\nRecognition and Understanding,\npages 230–237.\nRosenkrantz, Daniel J. and Philip M.\nLewis II. 1970. Deterministic left corner\nparsing. In IEEE Conference Record of the\n11th Annual Symposium on Switching and\nAutomata, pages 139–152.\nSaul, Lawrence and Fernando C. N. Pereira.\n1997. Aggregate and mixed-order Markov\nmodels for statistical language processing.\nIn Proceedings of the Second Conference on\nEmpirical Methods in Natural Language\nProcessing (EMNLP-97), pages 81–89.\nStolcke, Andreas. 1995. An efﬁcient\nprobabilistic context-free parsing\nalgorithm that computes preﬁx\nprobabilities. Computational Linguistics,\n21(2):165–202.\nStolcke, Andreas and Jonathan Segal. 1994.\nPrecise n-gram probabilities from\nstochastic context-free grammars. In\nProceedings of the 32nd Annual Meeting,\npages 74–79. Association for\nComputational Linguistics.\n276\nDownloaded from http://www.mitpressjournals.org/doi/pdf/10.1162/089120101750300526 by guest on 05 November 2025"
}