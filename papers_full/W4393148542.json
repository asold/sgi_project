{
  "title": "TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and Highlight Detection",
  "url": "https://openalex.org/W4393148542",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A5100778533",
      "name": "Hao Sun",
      "affiliations": [
        "Central China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5101323442",
      "name": "Mingyao Zhou",
      "affiliations": [
        "Central China Normal University"
      ]
    },
    {
      "id": "https://openalex.org/A5100430110",
      "name": "Wenjing Chen",
      "affiliations": [
        "Hubei University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5037590964",
      "name": "Wei Xie",
      "affiliations": [
        "Central China Normal University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3201832684",
    "https://openalex.org/W6778485988",
    "https://openalex.org/W2619082050",
    "https://openalex.org/W2904824998",
    "https://openalex.org/W1924770834",
    "https://openalex.org/W2966848258",
    "https://openalex.org/W6756911974",
    "https://openalex.org/W4361230899",
    "https://openalex.org/W2611788449",
    "https://openalex.org/W2901800653",
    "https://openalex.org/W2593116425",
    "https://openalex.org/W2931886155",
    "https://openalex.org/W3094751268",
    "https://openalex.org/W3172048555",
    "https://openalex.org/W2939519298",
    "https://openalex.org/W6755199642",
    "https://openalex.org/W2905354574",
    "https://openalex.org/W3214448253",
    "https://openalex.org/W3001461369",
    "https://openalex.org/W3184735396",
    "https://openalex.org/W4226297940",
    "https://openalex.org/W4385474448",
    "https://openalex.org/W4226013992",
    "https://openalex.org/W1958932515",
    "https://openalex.org/W4221166663",
    "https://openalex.org/W2970401629",
    "https://openalex.org/W4394659899",
    "https://openalex.org/W2996163128",
    "https://openalex.org/W3105607218",
    "https://openalex.org/W4361189674",
    "https://openalex.org/W3016811308",
    "https://openalex.org/W6691431627",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W6677316912",
    "https://openalex.org/W1686810756",
    "https://openalex.org/W2951818954",
    "https://openalex.org/W6640109428",
    "https://openalex.org/W2975357369",
    "https://openalex.org/W3106775073",
    "https://openalex.org/W3199096350",
    "https://openalex.org/W2919974746",
    "https://openalex.org/W2552027021",
    "https://openalex.org/W4379806215",
    "https://openalex.org/W2894280539",
    "https://openalex.org/W3196253003",
    "https://openalex.org/W6839710751",
    "https://openalex.org/W4367694235",
    "https://openalex.org/W4205717223",
    "https://openalex.org/W3202168743",
    "https://openalex.org/W3087792975",
    "https://openalex.org/W2968356596",
    "https://openalex.org/W2902516314",
    "https://openalex.org/W3132753749",
    "https://openalex.org/W4221164553",
    "https://openalex.org/W6713349023",
    "https://openalex.org/W2991773160",
    "https://openalex.org/W4309118232",
    "https://openalex.org/W2990503944",
    "https://openalex.org/W3035339529",
    "https://openalex.org/W3129089995",
    "https://openalex.org/W2964089981",
    "https://openalex.org/W2963662190",
    "https://openalex.org/W2116341502",
    "https://openalex.org/W2997429269",
    "https://openalex.org/W3166396011",
    "https://openalex.org/W3104862079",
    "https://openalex.org/W1924343884",
    "https://openalex.org/W4386083094",
    "https://openalex.org/W2962869524",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W4386071583",
    "https://openalex.org/W4287757777",
    "https://openalex.org/W4376226279",
    "https://openalex.org/W4327852044",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4312544061",
    "https://openalex.org/W2891456603",
    "https://openalex.org/W4312245888",
    "https://openalex.org/W3030520226",
    "https://openalex.org/W2963919999",
    "https://openalex.org/W3094550259"
  ],
  "abstract": "Video moment retrieval (MR) and highlight detection (HD) based on natural language queries are two highly related tasks, which aim to obtain relevant moments within videos and highlight scores of each video clip. Recently, several methods have been devoted to building DETR-based networks to solve both MR and HD jointly. These methods simply add two separate task heads after multi-modal feature extraction and feature interaction, achieving good performance. Nevertheless, these approaches underutilize the reciprocal relationship between two tasks. In this paper, we propose a task-reciprocal transformer based on DETR (TR-DETR) that focuses on exploring the inherent reciprocity between MR and HD. Specifically, a local-global multi-modal alignment module is first built to align features from diverse modalities into a shared latent space. Subsequently, a visual feature refinement is designed to eliminate query-irrelevant information from visual features for modal interaction. Finally, a task cooperation module is constructed to refine the retrieval pipeline and the highlight score prediction process by utilizing the reciprocity between MR and HD. Comprehensive experiments on QVHighlights, Charades-STA and TVSum datasets demonstrate that TR-DETR outperforms existing state-of-the-art methods. Codes are available at https://github.com/mingyao1120/TR-DETR.",
  "full_text": "TR-DETR: Task-Reciprocal Transformer for Joint Moment Retrieval and\nHighlight Detection\nHao Sun1,2,3*, Mingyao Zhou1,2,3*, Wenjing Chen4†, Wei Xie1,2,3†\n1Hubei Provincial Key Laboratory of Artiﬁcial Intelligence and Smart Learning,\nCentral China Normal University, Wuhan, China\n2School of Computer Science, Central China Normal University, Wuhan, China\n3National Language Resources Monitoring and Research Center for Network Media,\nCentral China Normal University, Wuhan, China\n4School of Computer Science, Hubei University of Technology, Wuhan, China\nhaosun@ccnu.edu.cn, zhoumingyao@mails.ccnu.edu.cn, chenwenjing@hbut.edu.cn, XW@mail.ccnu.edu.cn\nAbstract\nVideo moment retrieval (MR) and highlight detection (HD)\nbased on natural language queries are two highly related\ntasks, which aim to obtain relevant moments within videos\nand highlight scores of each video clip. Recently, several\nmethods have been devoted to building DETR-based net-\nworks to solve both MR and HD jointly. These methods sim-\nply add two separate task heads after multi-modal feature ex-\ntraction and feature interaction, achieving good performance.\nNevertheless, these approaches underutilize the reciprocal re-\nlationship between two tasks. In this paper, we propose a\ntask-reciprocal transformer based on DETR (TR-DETR) that\nfocuses on exploring the inherent reciprocity between MR\nand HD. Speciﬁcally, a local-global multi-modal alignment\nmodule is ﬁrst built to align features from diverse modali-\nties into a shared latent space. Subsequently, a visual fea-\nture reﬁnement is designed to eliminate query-irrelevant in-\nformation from visual features for modal interaction. Finally,\na task cooperation module is constructed to reﬁne the re-\ntrieval pipeline and the highlight score prediction process by\nutilizing the reciprocity between MR and HD. Comprehen-\nsive experiments on QVHighlights, Charades-STA and TV-\nSum datasets demonstrate that TR-DETR outperforms ex-\nisting state-of-the-art methods. Codes are available at https:\n//github.com/mingyao1120/TR-DETR.\nIntroduction\nWith the ubiquity of digital devices and the expansion of\nthe Internet, the number and variety of videos are rapidly\nincreasing (Foo et al. 2023). How to quickly search out the\ndesired moments from massive videos (called moment re-\ntrieval, MR) (Gao et al. 2017) and efﬁciently browse videos\n(called highlight detection, HD) (Molino and Gygli 2018)\naccording to the needs of users has attracted widespread\nattention. In practical applications, user needs can be ex-\npressed in natural language queries (Wang et al. 2022). Due\nto the complexity of video content as well as the diversity\n*These authors contributed equally.\n†Corresponding authors.\nCopyright © 2024, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nof user needs, MR&HD based on user-provided natural lan-\nguage queries is extremely challenging.\nThe goal of MR is to precisely search for semantically\nrelated moments from whole videos guided by natural lan-\nguage queries (Li et al. 2022). The common pipeline of MR\ninvolves several steps. Firstly, pre-trained networks are uti-\nlized to extract features from the input video and text. Sub-\nsequently, cross-modal interaction is performed based on the\nextracted features to obtain the query relevance score of the\ncandidate moment or the frame-level start-end probability\nof the relevant moment (Zhang et al. 2023). HD based on\nqueries strives to assign highlight scores to each video clip\nbased on considering the user needs (Guo et al. 2022). Ex-\nisting methods (Liu et al. 2022b; Xiong and Wang 2023) uti-\nlize transformers (Vaswani et al. 2017) or graph neural net-\nworks (Scarselli et al. 2008) to perform single-modal feature\nencoding or cross-modal interaction.\nDue to the task similarity between MR and HD based on\nqueries, and the commonality between their methods involv-\ning multi-modal feature extraction, feature interaction, etc.,\nsome works (Lei, Berg, and Bansal 2021; Lin et al. 2023)\nhave devoted to designing various multi-task networks for\njoint MR&HD. For example, Moment-DETR (Lei, Berg,\nand Bansal 2021) pioneers the application of DETR (Car-\nion et al. 2020) for joint MR&HD. QD-DETR (Moon et al.\n2023) introduces a query-dependent video representation\nmodule, making moment predictions reliant on user queries.\nMH-DETR (Xu et al. 2023) introduces a pooling operation\ninto the encoder and incorporates a cross-modality interac-\ntion module to fuse visual and query features. In these meth-\nods, two isolated task heads are added after the shared multi-\nmodal feature extraction and feature interaction modules for\njoint MR&HD. These methods generally focus on improv-\ning the discrimination of multi-modal feature extraction and\nfeature interaction through a multi-task learning scheme,\nachieving good performance. However, the reciprocity be-\ntween MR and HD tasks is ignored.\nFor MR, the highlight scores from HD based on user-\nprovided queries can be utilized to assist in eliminating\nquery-irrelevant clips, thereby boosting moment retrieval ac-\ncuracy. In turn, for HD based on queries, the results of mo-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4998\nment retrieval can be used to improve the understanding of\nvideos and user needs. Therefore, MR and HD based on\nqueries are reciprocal.\nTo fully exploit the reciprocal relationship between the\ntwo tasks, we propose a task-reciprocal transformer based\non DETR, named TR-DETR, for joint MR&HD. Firstly, vi-\nsual features and textual features are extracted from user-\nprovided videos and queries through pre-trained networks.\nThen, we introduce a local-global multi-modal alignment\nmodule to perform local and global semantic alignment be-\nfore modal interaction, respectively. This module encour-\nages the model to distinguish video clips that are semanti-\ncally similar but irrelevant to the query. Subsequently, we\npropose a visual feature reﬁnement module for modal inter-\naction, which employs aligned textual features to ﬁlter out\nquery-irrelevant information in visual features to avoid it in-\nterfering with joint features. Finally, to exploit the comple-\nmentarities between MR and HD, we propose a task cooper-\nation module consisting of HD2MR and MR2HD. The for-\nmer explicitly infuses highlight score information into the\nmoment retrieval process, enhancing localization accuracy.\nThe latter exploits localization outcomes to derive clip-level\nrelevant scores, offering visual support for highlight detec-\ntion. Extensive experiments on QVHighlights (Lei, Berg,\nand Bansal 2021), Charades-STA (Gao et al. 2017) and TV-\nSum (Song et al. 2015) demonstrate that the proposed TR-\nDETR outperforms the state-of-the-art methods. The contri-\nbutions of this paper are summarized as follows:\n• We highlight the reciprocity between MR and HD. In\naddition, we introduce an innovative TR-DETR network\nthat leverages this reciprocity between tasks to optimize\nperformance.\n• We introduce the local and global alignment regulators.\nThese regulators are designed to facilitate semantic align-\nment between video clips and the query, which serves to\ngenerate discriminative joint representations.\n• To explore the intrinsic complementarity between the\ntwo tasks, we construct a task cooperation module. This\nmodule explicitly exploits the complementarity between\nMR and HD by injecting highlight scores into the mo-\nment retrieval pipeline and using the retrieved moments\nto reﬁne the initial highlight distribution.\nRelated Works\nMR and HD\nVideo moment retrieval is originally introduced by the liter-\nature (Gao et al. 2017), with the objective of retrieving mo-\nments from a video based on a given natural language query.\nMoment retrieval typically includes two types of methods:\nproposal-based and proposal-free methods. In the proposal-\nbased methods, candidate moments are initially generated\nthrough techniques such as sliding windows (Gao et al.\n2017), proposal generation networks (Xu et al. 2019), or\n2D-Maps (Zhang et al. 2020). These candidates are subse-\nquently ranked based on the similarity scores to the query,\nwhere the candidate with the highest score is used as the\nresult. Although these methods have high accuracy, they\nnecessitate additional pre- and post-processing steps, intro-\nducing computational redundancy. Moreover, their perfor-\nmance heavily relies on the quality of candidate moments.\nOn the other hand, proposal-free methods (Ghosh et al.\n2019; Zhang et al. 2021; Mun, Cho, and Han 2020) directly\npredict start-end probabilities for target moments within a\nvideo, which eliminates the need to rank a large number of\ncandidate moments, thereby improving training efﬁciency.\nIn contrast, highlight detection concentrates on measuring\nthe signiﬁcance of each clip within a given video. Slightly\ndifferent from moment retrieval, highlight detection initially\nis proposed as a single-modal task and does not rely on text\nqueries. However, highlight determination is often a subjec-\ntive matter and users’ preferences should be taken into ac-\ncount. Therefore, the literature (Kudi and Namboodiri 2017)\nproposes to integrate text queries as supplementary infor-\nmation for highlight detection. Nonetheless, this work relies\nsolely on text ranking algorithms to rank video descriptions\nin the text domain to guide video clip ranking. It does not en-\ntail a direct alignment of text and highlights. Subsequently,\nin video thumbnail generation, which closely parallels high-\nlight detection, Yuanet al.(Yuan, Ma, and Zhu 2019) delves\ninto text queries and uses graph convolutional networks to\nmodel the interaction between each clip and text.\nConventionally, moment retrieval and highlight detection\nare addressed in isolation, lacking an integrated framework\nfor joint learning. Recent research (Lei, Berg, and Bansal\n2021) constructs the QVHighlights dataset to facilitate joint\nlearning of MR&HD and proposes a baseline model based\non DETR. Building upon this, Liu et al.(Liu et al. 2022b)\nincorporates audio modality into the model, catering to sce-\nnarios for missing queries. Additionally, Moon et al.(Moon\net al. 2023) prioritizes full integration of provided query in-\nformation into the joint representation, enabling the text to\nguide both moment retrieval and highlight detection. Differ-\nent from previous methods, this paper focuses on exploiting\nthe natural reciprocity between two tasks.\nMulti-Modal Alignment\nRecently, researchers in the multimodal ﬁeld have focused\non constructing contrastive losses to ﬁt the interactions and\ncorrespondences between different modalities (Luo et al.\n2020; Sun et al. 2020; Miech et al. 2020; Yan et al. 2023).\nFor example, the literature (Ging et al. 2020) introduces a\ncycle consistency loss to align video clip-level features and\nquery word-level features. Similarly, the literature (Zhang\net al. 2022) introduces a multi-level contrast loss to capture\nmulti-granular interactive alignment details within queries\nand videos, enhancing the performance of moment retrieval.\nAlthough these methods share similarities with the multi-\nmodal alignment in our approach, they do not explicitly\nalign the semantic information of different modalities before\nmodality interaction, resulting in insufﬁcient discrimination\nof joint features.\nMethod\nThe overview of TR-DETR is shown in Figure 1. TR-\nDETR comprises four core modules: feature extraction,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n4999\nFigure 1: The proposed TR-DETR involves several key steps. Initially, two frozen pre-trained networks are employed to extract\nvisual and textual features from videos and queries. Subsequently, a local-global multi-modal alignment module is constructed\nto effectively align the extracted visual and textual features. Then, the visual features are reﬁned under the guidance of textual\nfeatures for obtaining discriminative joint features. Finally, a task cooperation module is implemented to enhance prediction\noutcomes based on task reciprocity. Additionally, two multi-head self-attention components share weights.\nlocal-global multi-modal alignment, visual feature reﬁne-\nment for modal interaction, and task cooperation. Details are\nintroduced as follows.\nFeature Extraction\nVisual Features. Following the literature (Lei, Berg, and\nBansal 2021), the video is ﬁrst divided into non-overlapping\nclips according to a certain time interval, such as 2s. Then\nthe pre-trained ViT-B/32 in CLIP (Radford et al. 2021) and\nSlowFast (Feichtenhofer et al. 2019) are utilized to extract\nclip-level visual features Fv =\n\u0002\nf1\nv ;f2\nv ;:::;f L\nv\n\u0003\n2RL\u0002dv ,\nwhere Land dv are the number of clips and the visual feature\ndimension, respectively. Following the way that UMT (Liu\net al. 2022b) uses audio information, we use the pre-trained\naudio feature extractor to extract the audio features Fa 2\nRL\u0002da , and then splice them behind the visual features Fv.\nSee the experimental settings for details.\nTextual Features. For a natural language query, we use\nthe textual encoder in the pre-trained CLIP to extract textual\nfeatures Ft =\n\u0002\nf1\nt ;f2\nt ;:::;f N\nt\n\u0003\n2RN\u0002dt , where N and dt\nare the number of words and the textual feature dimension,\nrespectively.\nLocal-Global Multi-Modal Alignment\nExisting methods (Moon et al. 2023; Lei, Berg, and Bansal\n2021; Liu et al. 2022b) for joint MR&HD directly input the\nextracted visual and textual features into the modal interac-\ntion module to obtain joint features. However, there is a nat-\nural information mismatch between visual features and tex-\ntual features, resulting in insufﬁcient discrimination of joint\nfeatures (Xu, Zhu, and Clifton 2022). In this study, to re-\nduce the modal gap, we propose a local-global multi-modal\nalignment module, comprising local and global regulariza-\ntion components. The local regulator helps the model dis-\ntinguish semantically similar but undesired clips, while the\nglobal regulator ensures that both modalities share a uniﬁed\nsemantic space. Integrating these alignment regulators can\nsigniﬁcantly promote multimodal associations and facilitate\nsubsequent modal interactions.\nGiven the clip-level visual features Fv of the video and\nthe word-level textual features Ft of the query, we ﬁrst map\nthem into the same dimension dby using three-layer multi-\nlayer perceptions (MLP).\nbFv = MLPv(Fv); (1)\nbFt = MLPt(Ft): (2)\nFor the local regulator, we calculate the cosine similarity be-\ntween each clip and each word by using the following for-\nmula, obtaining a similarity matrix Sloc 2RL\u0002N .\nSloc = \u001b\n bFv bFT\nt\nkbFvk2kbFtk2\n!\n; (3)\nwhere \u001b is the sigmoid function. We employ mean-pooling\nto get bSloc = MeanPooling(Sloc) 2RL, which measures\nthe similarity between each video clip and the global textual\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5000\nfeatures. Then, a local regular loss Lloc is used to encourage\ndistinguishing video clips that are irrelevant to the query.\nLlocal = \u0000\nLX\ni=1\n\u0010\nCi log( bSi\nloc) + (1\u0000Ci) log(1\u0000bSi\nloc)\n\u0011\n;\n(4)\nwhere bSi\nloc is the similarity score between thei-th video clip\nand the global textual features, and Ci indicates whether the\ni-th video clip and the query are actually relevant. Specif-\nically, according to ground truth in MR, if the i-th clip is\nrelevant to the query, Ci is 1, otherwise 0. For the global\nregulator, a multi-modal contrastive loss (Li et al. 2021) is\nemployed to promote the similarity of global representations\nof paired videos and queries.\nLglobal = \u00001\nB\nBX\ni=1\nlog exp((Gi\nv) (Gi\nt)T)\nPB\ni=1\nPB\nj=1 exp((Giv) (Gj\nt)T)\n;\n(5)\nwhere B is the batch size, Gi\nv 2Rd and Gi\nt 2Rd are the\nglobal feature of the i-th video and the i-th query in a train-\ning batch, respectively. Speciﬁcally, Gi\nv is obtained by av-\neraging all clip features bFv within the i-th video, and Gi\nt is\nderived by averaging word-level featuresbFt in the i-th query.\nVisual Feature Reﬁnement for Modal Interaction\nThe goal of modal interaction is to generate discriminative\njoint features from visual and textual features (Lei, Berg,\nand Bansal 2021), which play a key role in joint MR&HD.\nIn the literature (Lei, Berg, and Bansal 2021), visual and tex-\ntual features are simply concatenated for modal interaction.\nHowever, videos generally contain a large number of clips\nirrelevant to the textual query, which may cause the model\nto pay too much attention to these irrelevant contents, result-\ning in ignoring the really important clips.\nTo suppress the interference of query-irrelevant informa-\ntion in visual features, we introduce a query-guided visual\nfeature reﬁnement module inspired by the literature (Xiong,\nZhong, and Socher 2017) for modal interaction. This mod-\nule employs the textual query as a guide to reﬁne clip-level\nvisual features to effectively suppress irrelevant information\npresent in the video and retain temporal cues. The similarity\nmatrix between aligned clip-level visual features and word-\nlevel textual features is calculated as:\nA= Linear( bFv) Linear( bFt)T\np\nd\n; (6)\nwhere A2RL\u0002N is the similarity matrix and Linear(\u0001) rep-\nresents the linear projection layer. Then the similarity ma-\ntrix is used to weigh and sum the query and video features\nrespectively to obtain preliminary reﬁnement features.\nFv2q = Ar bFt; (7)\nFq2v = Ar AT\nc bFv; (8)\nwhere Ar and Ac represent the results after row softmax nor-\nmalization and column softmax normalization of A, Fv2q\nand Fq2v are the clip-level textual features and word-level\nvisual features, respectively. Finally, to further use text\nqueries to optimize clip-level visual features bFv, we perform\nthe following feature concatenation and obtain the ﬁnal re-\nﬁned clip features\nFv through linear projection.\nFCat\nv =\nh\nbFvkFv2qkbFv \fFv2qkbFv \fFq2vkFG\nt\ni\n; (9)\nFv = Linear(FCat\nv ); (10)\nwhere FG\nt 2RL\u0002d is a matrix formed by copying and splic-\ning the text global features obtained through the pooling op-\neration, [\u0001k\u0001] means concatenation, and \fis the Hadamard\nproduct. Then, modality fusion is performed using a cross-\nattention layer to further incorporate query features into\nthe joint features, where textual features are from the re-\nﬁned clip feature Qv = Linearq(\nFv), key and value fea-\ntures are from the textual features Kt = Lineark( bFt) and\nVt = Linearv( bFt).\nZ = Attention(Qv;Kt;Vt) =Softmax\n\u0012QvKT\ntp\nd\n\u0013\nVt;\n(11)\nwhere Z 2RL\u0002d represents joint features through modal\ninteraction between reﬁned visual features and textual fea-\ntures.\nTask Cooperation\nAlthough previous methods (Lei, Berg, and Bansal 2021;\nLiu et al. 2022b; Moon et al. 2023) have attempted to jointly\nsolve MR and HD, these methods usually focus on optimiz-\ning the shared multi-modal feature extraction and feature in-\nteraction modules to improve the discrimination of joint fea-\ntures using a multi-task learning framework. However, the\ninherent complementarity between MR and HD tasks is un-\nderutilized.\nIn essence, video clips with high highlight scores are often\nstrong candidates for MR. Because highlight-worthy clips\ntend to possess enhanced visual signiﬁcance and attraction.\nAdditionally, clips within the moment relevant to the current\nquery probably cover the highlights, too. This is because\nquery-relevant moments also contain visual expressions of\nuser needs, which helps to reﬁne the highlight score distri-\nbution from the visual perspective. Given these insights, we\npropose a task cooperation module consisting of HD2MR\nand MR2HD components.\nHD2MR MR can leverage the highlight scores obtained\nby HD to empower the exclusion of irrelevant or less attrac-\ntive video clips. We ﬁrst use the multi-head attention mech-\nanism and a linear layer to obtain clip-level highlight scores\nfrom the joint features Z.\nH = Linear(MHA(Z)); (12)\nwhere MHA(\u0001) represents multi-head attention that is em-\nployed to model video temporal information and H 2RL is\nthe predicted highlight scores.\nTo ﬁlter out non-highlight information inZand explicitly\ninject highlight scores information into the MR pipeline, we\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5001\nmultiply the clip-level highlight scores Hwith the joint fea-\ntures Z to obtain the enhanced joint features Z 2RL\u0002d.\nThen, Z is input into the MHA again for joint features en-\ncoding.\nZ = Softmax(H) \fZ;\nbZ = MHA\n\u0000\nZ+ Z\n\u0001\n;\n(13)\nwhere bZ is the joint features of the perceived highlight\nscores. Finally, these enhanced features bZ are fed into the\ntransformer decoder and prediction head from the litera-\nture (Liu et al. 2022a) to obtain the ultimate retrieved mo-\nments.\nMR2HD HD, in turn, gains a deeper understanding of\nvideo content and user needs by leveraging the text query\nand retrieved moments from MR. We employ the gated re-\ncurrent unit (GRU) (Chung et al. 2014) to effectively capture\nglobal information from the retrieved moments.\nFM\nv = GRU(m); (14)\nwhere mrepresents the clip feature vectors in bFv of the re-\ntrieved moments from HD2MR and FM\nv 2Rd is the global\nfeature vector of these retrieved moments. To use the vi-\nsual information of the retrieved moments to reﬁne high-\nlight scores prediction, we calculate similarity scores be-\ntween FM\nv and visual features bFv.\nSref = FM\nv bFT\nv\nkFMv k2 kbFvk2\n; (15)\nwhere Sref 2RL is the correlation between clips and FM\nv .\nThe highlight score reﬁnement process involves multiplying\nthe clip-level correlation scores by bZ, then adding them to\nZ, and ﬁnally obtaining reﬁned scores by linear projection.\nThe formulation is as follows:\nH = Linear(Z+ Softmax(Sref ) \fbZ); (16)\nwhere H 2RL is the reﬁned highlight scores.\nObjective Losses\nThe objective losses of TR-DETR include three parts: MR\nloss Lmom, HD loss Lhigh, regulators losses Llocal and\nLglobal.\nLtotal = Lmom + Lhigh + \u0015lg(Llocal + Lglobal); (17)\nwhere \u0015lg is the coefﬁcient of local-global regulators losses.\nLmom and Lhigh are consistent with QD-DETR (Moon et al.\n2023).\nExperiment\nDatasets\nQVHighlights dataset (Lei, Berg, and Bansal 2021) com-\nprises 10,148 content-rich videos from YouTube. Each video\nis accompanied by at least one manually annotated text\nquery, where the highlight clips are located within the cor-\nresponding moment. The evaluation process of this dataset\nis particularly fair as the annotations of the test set are inac-\ncessible. The prediction results of the model need to be up-\nloaded to the QVHighlights server’s CodaLab competition\nplatform1 for impartial performance assessment.\nCharades-STA dataset (Gao et al. 2017) contains 9,848\nvideos capturing daily indoor activities and 16,128 human-\ntagged query texts. Following QD-DETR(Moon et al. 2023),\nwe allocate 12,408 samples for training while the remaining\n3,720 samples are for testing.\nTVSum dataset (Song et al. 2015) is a benchmark dataset\nfor HD. It contains 10 different categories of videos, and\neach category comprises 5 videos. To ensure consistency\nwith QD-DETR (Moon et al. 2023), 80% of the dataset is\nutilized for training and the remaining for testing.\nMetrics and Experimental Settings\nWe use common metrics from recent studies like Moment-\nDETR, UMT, QD-DETR, and MH-DETR. For QVHigh-\nlights, we calculate Recall@1 with IoU 2 f0:5;0:7gand\nmean average precision (mAP) with IoU2f0:5; 0:75g. Fol-\nlowing Lei et al.(Lei, Berg, and Bansal 2021), we also uni-\nformly sample 10 IoU thresholds from f0:5;0:95gto calcu-\nlate mAP, and take the average as the average mAP metric.\nFor highlight detection, we use mAP and HIT@1. Charades-\nSTA involves Recall@1 with IoU 2 f0:5;0:7g, while for\nTVSum, top-5 mAP is the main metric.\nIn addition, we introduce implementation details and hy-\nperparameters as follows. The hidden layer dimension dis\n256, and \u0015lg is set to 0.3. We use PANN (Kong et al. 2020)\ntrained on the AudioSet dataset (Gemmeke et al. 2017)\nto extract audio features. For QVHighlights, we use Slow-\nFast (Feichtenhofer et al. 2019) and CLIP to extract visual\nfeatures and the text encoder in CLIP to extract textual fea-\ntures. The training phase involves 200 epochs, a batch size of\n32, and a learning rate of 1e-4. For TVSum, we use the I3D\npre-trained on Kinetics-400 for visual features and CLIP for\ntextual features. Training spans 2000 epochs with a batch\nsize of 4 and a learning rate of 1e-3. In Charades-STA, we\nextract visual features with VGG (Simonyan and Zisserman\n2015), I3D (Carreira and Zisserman 2017), SlowFast, and\nCLIP, and use GLoVe (Pennington, Socher, and Manning\n2014) for textual features. The training phase includes 100\nepochs, a batch size of 8, and a learning rate of 1e-4. More-\nover, all our experiments are conducted on Nvidia RTX 4090\nand Gen Intel(R) Core(TM) i7-12700 CPU.\nComparison with Other Methods\nTable 1 reports the TR-DETR’s performance on joint mo-\nment retrieval and highlight detection tasks. Meanwhile, Ta-\nbles 2 and 3 list the results of different methods on moment\nretrieval and highlight detection, respectively.\nIn Table 1, we evaluate the performance of moment re-\ntrieval and highlight detection simultaneously based on the\nQVHighlights dataset. For a fair comparison, we compare\nthe performance with UniVTG (Lin et al. 2023) without\npre-training. As shown in Table 1, our TR-DETR method\n1https://codalab.lisn.upsaclay.fr/competitions/6937\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5002\nMethod Src\nMoment Retrieval HD\nR1 mAP \u0015Very Good\n@0.5 @0.7 @0.5 @0.75 Avg. mAP HIT@1\nBeautyThumb (Song et al. 2016) V - - - - - 14.36 20.88\nDVSE (Liu et al. 2015) V - - - - - 18.75 21.79\nMCN (Hendricks et al. 2018) V 11.41 2.72 24.94 8.22 10.67 - -\nCAL (Escorcia et al. 2019) V 25.49 11.54 23.40 7.65 9.89 - -\nXML (Lei et al. 2020) V 41.83 30.35 44.63 31.73 32.14 34.49 55.25\nXML+ (Lei, Berg, and Bansal 2021) V 46.69 33.46 47.89 34.67 34.90 35.38 55.06\nMDETR (Lei, Berg, and Bansal 2021) V 52.89 33.02 54.82 29.40 30.73 35.69 55.60\nQD-DETR (Moon et al. 2023) V 62.40\n44.98 62.62 39.88 39.86 38.64 62.40\nUniVTG (Lin et al. 2023) V 58.86 40.86 57.60 35.59 35.47 38.20 60.96\nTR-DETR V 64.66 48.96 63.98 43.73 42.62 39.91 63.42\nUMT (Liu et al. 2022b) V+A 56.23 41.18 53.38 37.01 36.12 38.18 59.99\nQD-DETR (Moon et al. 2023) V+A 63.06 45.10 63.04 40.10 40.19 39.04 62.87\nTR-DETR V+A 65.05 47.67 64.87 42.98 43.10 39.90 63.88\nTable 1: Experimental results on the QVHighlights test set. HD represents the results of highlight detection. ‘V’ and ‘A’ repre-\nsent using video and audio features, respectively. Bold letters indicate the best results, while underlined results are suboptimal.\n(b)\n(a)\nFigure 2: Qualitative results of TR-DETR on QVHighlights val set.\noutperforms the current best approach on all metrics. Espe-\ncially with visual features only, TR-DETR exhibits a signif-\nicant increase in performance under more stringent metrics\nand high IOU thresholds. Compared with previous methods,\nTR-DETR improves R1@0.7 and mAP@0.75 by 3.98% and\n3.75%, respectively. In addition, after introducing audio in-\nformation, the performance of a few indicators decreases.\nThis may be because the audio features are spliced directly\nbehind the video features, causing misaligned multi-modal\nfeatures to be combined and thus impairing modal interac-\ntions.\nIn Table 2, we use VGG, C3D, and SF+C features to com-\nprehensively evaluate the performance of TR-DETR on the\nCharades-STA dataset. For each feature of VGG, C3D and\nSF+C, we follow the data preparation settings of UMT (Liu\net al. 2022b), VSLNet (Zhang et al. 2021), and Moment-\nDETR (Lei, Berg, and Bansal 2021), respectively. As shown\nin Table 2, our TR-DETR shows comparable performance\non VGG and SF+C features. Also, performance on some\nmetrics degrades with the introduction of audio, possibly\ndue to insufﬁcient modal interaction. Compared with using\nonly VGG features, the performance of the proposed method\nis slightly different from UniVTG when using SF+C fea-\ntures. We believe the reasons are as follows: semantic in-\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5003\n(a)\n(b)\nFigure 3: Qualitative results of TR-DETR on TVSum val set.\nMethod Feat R1@0.5 R1@0.7\nSAP (Chen and Jiang 2019) VGG 27.42 13.36\nTripNet (Hahn et al. 2020) VGG 36.61 14.50\nMAN (Zhang et al. 2019) VGG 41.24 20.54\n2D-TAN (Zhang et al. 2020) VGG 40.94 22.85\nFVMR (Li et al. 2021) VGG 42.36 24.14\nUMT† (Liu et al. 2022b) VGG 48.31 29.25\nQD-DETR (Moon et al. 2023) VGG 52.77 31.13\nQD-DETR† (Moon et al. 2023) VGG 55.51 34.17\nTR-DETR VGG 53.47 30.81\nTR-DETR† VGG 54.49 32.37\nCTRL (Gao et al. 2017) C3D 23.63 8.89\nACL (Ge et al. 2019) C3D 30.48 12.20\nMAN (Zhang et al. 2019) C3D 46.53 22.72\nDEBUG (Lu et al. 2019) C3D 37.39 17.69\nVSLNet (Zhang et al. 2021) I3D 47.31 30.19\nQD-DETR (Moon et al. 2023) I3D 50.67\n31.02\nTR-DETR I3D 55.51 33.66\nQD-DETR (Moon et al. 2023) SF+C 57.31 32.55\nUniVTG (Lin et al. 2023) SF+C 58.01 35.65\nTR-DETR SF+C 57.61 33.52\nTable 2: Experimental results on the Charades-STA test set.\n‘†’ represents using audio features.\nformation of features extracted by different-scale feature ex-\ntractors (e.g. VGG and PANN) varies greatly. In our method,\nthe local-global multi-modal alignment module is used to\nforce the alignment of the visual features of VGG, the audio\nfeatures of PANN, and the text features of GLoVe, which\nis challenging and results in relatively weak performance.\nHowever, when text, visual and audio features are all de-\nrived from large models, such as CLIP, our method shows\nexcellent performance on the QVHighlights dataset.\nConsistent with previous work on highlight detection, we\nevaluate the performance of the proposed TR-DETR on each\nvideo category and calculate the top-5 mAP scores. The re-\nsults are shown in Table 3. In addition, to comprehensively\nevaluate the overall performance of TR-DETR, we calculate\nFigure 4: The impact of local-global alignment loss and \u0015lg\nbased on QVHighlights val set, introducing audio features.\nthe average value of top-5 mAP on 10 categories. The pro-\nposed TR-DETR exceeds the previous method by approxi-\nmately 3.1% when using only video features, which demon-\nstrates the powerful performance of TR-DETR in solving\nHD alone.\nVisualization\nIn Figures 2 and 3, we visualize the qualitative analy-\nsis results of TR-DETR on the QVHighlights and TV-\nSum datasets, respectively. In Figure 2, compared with QD-\nDETR, TR-DETR shows more reasonable and accurate re-\nsults in terms of retrieved accuracy and highlight score dis-\ntribution. In Figure 3 a), the proposed TR-DETR can ac-\ncurately ﬁt the highlight score distribution. We believe that\nthese performance improvements are due to the combination\nof the proposed modules. In addition, in Figure 3 b), it may\nbe that the model only noticed the concept of “puppy dog”,\nresulting in unreasonable high highlight scores in the middle\nof the result.\nAblation\nTo verify the effect of each module in the proposed TR-\nDETR, we conduct a comprehensive ablation experiment,\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5004\nMethod Src VT VU GA MS PK PR FM BK BT DS Avg\nsLSTM (Zhang et al. 2016) V 41.1 46.2 46.3 47.7 44.8 46.1 45.2 40.6 47.1 45.5 45.1\nSG (Yuan et al. 2020) V 42.3 47.2 47.5 48.9 45.6 47.3 46.4 41.7 48.3 46.6 46.2\nLIM-S (Xiong et al. 2019) V 55.9 42.9 61.2 54.0 60.3 47.5 43.2 66.3 69.1 62.6 56.3\nTrailer (Wang et al. 2020) V 61.3 54.6 65.7 60.8 59.1 70.1 58.2 64.7 65.6 68.1 62.8\nSL-Module (Xu et al. 2021) V 86.5 68.7 74.9 86.2 79.0 63.2 58.9 72.6 78.9 64.0 73.3\nQD-DETR (Moon et al. 2023) V 88.2\n87.4 85.6 85.0 85.8 86.9 76.4 91.3 89.2 73.7 85.0\nUniVTG (Lin et al. 2023) V 83.9 85.1 89.0 80.1 84.6 87.0 70.9 91.7 73.5 69.3 81.0\nTR-DETR V 89.3 93.0 94.3 85.1 88.0 88.6 80.4 91.3 89.5 81.6 88.1\nMINI-Net (Hong et al. 2020) V+A 80.6 68.3 78.2 81.8 78.1 65.8 75.8 75.0 80.2 65.5 73.2\nTCG (Ye et al. 2021) V+A 85.0 71.4 81.9 78.6 80.2 75.5 71.6 77.3 78.6 68.1 76.8\nJoint-V A (Badamdorj et al. 2021) V+A 83.7 57.3 78.5 86.1 80.1 69.2 70.0 73.0 97.4 67.5 76.3\nUMT (Liu et al. 2022b) V+A 87.5 81.5 88.2 78.8 81.4 87.0 76.0 86.9 84.4 79.6 83.1\nQD-DETR (Moon et al. 2023) V+A 87.6 91.7 90.2 88.3 84.1 88.3 78.7 91.2 87.8 77.7 86.6\nTR-DETR V+A 90.6 92.4 91.7 81.3 86.9 85.5 79.8 93.4 88.3 81.0 87.1\nTable 3: Experimental results on the TVSum val set. ‘V’ and ‘A’ represent using video and audio features, respectively.\nSetting LGAM VFR MR2HD HD2MR\nMoment Retrieval HD\nR1 mAP \u0015Very Good\n@0.5 @0.7 @0.5 @0.75 Avg. mAP HIT@1\n(a) 57.72 42.35 59.10 38.16 38.03 36.76 57.44\n(b) X 63.10 44.97 63.13 40.22 40.47 39.92 63.87\n(c) X 64.19 47.61 63.50 42.90 41.74 39.71 64.13\n(d) X 58.39 42.71 59.28 39.19 38.76 37.80 58.8\n(e) X 59.61 42.26 60.91 39.28 39.26 37.67 58.45\n(f) X X 59.81 44.71 60.25 39.33 39.80 37.86 57.94\n(g) X X X 62.13 47.16 62.00 42.79 41.21 39.76 62.65\n(h) X X X 63.23 46.90 63.30 42.47 41.64 38.12 59.55\n(i) X X 66.32 50.71 65.71 44.82 43.95 40.35 64.90\n(j) X X X X 67.10 51.48 66.27 46.42 45.09 40.55 64.77\nTable 4: Comparison with the baseline (Moment-DETR with cross-attention module and DAB-DETR’s decoder (Liu et al.\n2022a)) with different module combinations on QVHighlights val set. LGAM represents the local-global alignment module,\nand VFR is the visual feature reﬁnement module.\nand the results are listed in Table 4. Settings (b) to (e) show\nthe performance of each component on the baseline model\ncompared to setting (a). Setting (f) demonstrates the exis-\ntence of task reciprocity. Compared with setting (c), the rea-\nson for the performance degradation in setting (h) may be\nthe semantic mismatch between modalities, resulting in mu-\ntual degradation of tasks. Setting (i) shows the huge perfor-\nmance improvement of the proposed local-global alignment\nloss combined with visual feature reﬁnement.\nTo further verify the effect of the proposed local-global\nalignment loss, we also conduct ablation experiments on\nits coefﬁcients. As shown in Figure 4, after adding the lo-\ncal global regularization term, the model’s performance has\nbeen signiﬁcantly improved by about 5%. In addition, as the\nvalue of the hyperparameter\u0015lg gradually increases, the per-\nformance improvement becomes more signiﬁcant. When\u0015lg\nis set to 0.3, the model performance reaches its peak and\nthen begins to decline slowly. Comparing the hyperparam-\neter values of 0 and 0.3, the model performance has been\nimproved by about 7% in total, conﬁrming the signiﬁcant\nrole of the local-global alignment regulators.\nConclusion\nThis paper proposes a TR-DETR to explore the reciprocity\nbetween HD and MR tasks. First, local-global alignment\nregulators are designed to align visual and textual fea-\ntures. Then, a visual feature reﬁnement module is con-\nstructed to obtain discriminative joint features. Finally, a\ntask-reciprocal module is proposed to inject highlight score\ninformation into the moment retrieval pipeline and optimize\nhighlight score prediction by utilizing retrieved moments.\nExtensive experiments on several datasets demonstrate the\neffectiveness of TR-DETR. However, TR-DETR cannot efﬁ-\nciently utilize data from the audio modality. In the future, we\nwill study novel multi-modal feature interaction networks to\ncoordinate information from multiple modalities.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5005\nAcknowledgments\nThis work was supported in part by National Natural Science\nFoundation of China under Grant 62201222 and 62377026,\nin part by Hubei Provincial Natural Science Foundation\nof China under Grant 2022CFB954, in part by Knowl-\nedge Innovation Program of Wuhan-Shuguang Project un-\nder Grant 2023010201020377 and 2023010201020382, in\npart by self-determined research funds of CCNU from the\ncolleges’ basic research and operation of MOE under Grant\nCCNU22QN014, CCNU22JC007 and CCNU22XJ034, and\nin part by Hubei Provincial Key Laboratory of Artiﬁcial\nIntelligence and Smart Learning NO. 2023AISL003 and\n2023AISL010.\nReferences\nBadamdorj, T.; Rochan, M.; Wang, Y .; and Cheng, L. 2021.\nJoint Visual and Audio Learning for Video Highlight Detec-\ntion. In IEEE ICCV, 8107–8117.\nCarion, N.; Massa, F.; Synnaeve, G.; Usunier, N.; Kirillov,\nA.; and Zagoruyko, S. 2020. End-to-End Object Detection\nwith Transformers. In ECCV, 213–229. Springer.\nCarreira, J.; and Zisserman, A. 2017. Quo Vadis, Action\nRecognition? A New Model and the Kinetics Dataset. In\nIEEE CVPR, 4724–4733.\nChen, S.; and Jiang, Y . 2019. Semantic Proposal for Activity\nLocalization in Videos via Sentence Query. In AAAI, 8199–\n8206.\nChung, J.; Gulcehre, C.; Cho, K.; and Bengio, Y . 2014. Em-\npirical evaluation of gated recurrent neural networks on se-\nquence modeling. In NeurIPS 2014 Workshop on Deep\nLearning.\nEscorcia, V .; Soldan, M.; Sivic, J.; Ghanem, B.; and Russell,\nB. C. 2019. Temporal Localization of Moments in Video\nCollections with Natural Language. CoRR, abs/1907.12763.\nFeichtenhofer, C.; Fan, H.; Malik, J.; and He, K. 2019. Slow-\nFast Networks for Video Recognition. InIEEE ICCV, 6201–\n6210.\nFoo, L. G.; Gong, J.; Fan, Z.; and Liu, J. 2023. System-\nStatus-Aware Adaptive Network for Online Streaming\nVideo Understanding. In IEEE CVPR, 10514–10523.\nGao, J.; Sun, C.; Yang, Z.; and Nevatia, R. 2017. TALL:\nTemporal Activity Localization via Language Query. In\nIEEE ICCV, 5277–5285.\nGe, R.; Gao, J.; Chen, K.; and Nevatia, R. 2019. MAC: Min-\ning Activity Concepts for Language-Based Temporal Local-\nization. In IEEE WACV, 245–253.\nGemmeke, J. F.; Ellis, D. P. W.; Freedman, D.; Jansen, A.;\nLawrence, W.; Moore, R. C.; Plakal, M.; and Ritter, M.\n2017. Audio Set: An ontology and human-labeled dataset\nfor audio events. In IEEE ICASSP, 776–780.\nGhosh, S.; Agarwal, A.; Parekh, Z.; and Hauptmann, A.\n2019. ExCL: Extractive Clip Localization Using Natural\nLanguage Descriptions. In NAACL, 1984–1990. Minneapo-\nlis, Minnesota: ACL.\nGing, S.; Zolfaghari, M.; Pirsiavash, H.; and Brox, T. 2020.\nCOOT: Cooperative Hierarchical Transformer for Video-\nText Representation Learning. In NeurIPS.\nGuo, Z.; Zhao, Z.; Jin, W.; Wang, D.; Liu, R.; and Yu, J.\n2022. TaoHighlight: Commodity-Aware Multi-Modal Video\nHighlight Detection in E-Commerce.IEEE TMM, 24: 2606–\n2616.\nHahn, M.; Kadav, A.; Rehg, J. M.; and Graf, H. P. 2020.\nTripping through time: Efﬁcient Localization of Activities\nin Videos. In BMVC.\nHendricks, L. A.; Wang, O.; Shechtman, E.; Sivic, J.; Dar-\nrell, T.; and Russell, B. C. 2018. Localizing Moments in\nVideo with Temporal Language. In EMNLP, 1380–1390.\nACL.\nHong, F.; Huang, X.; Li, W.; and Zheng, W. 2020. MINI-\nNet: Multiple Instance Ranking Network for Video High-\nlight Detection. In ECCV, 345–360. Springer.\nKong, Q.; Cao, Y .; Iqbal, T.; Wang, Y .; Wang, W.; and\nPlumbley, M. D. 2020. PANNs: Large-Scale Pretrained Au-\ndio Neural Networks for Audio Pattern Recognition. IEEE\nTASLP, 28: 2880–2894.\nKudi, S.; and Namboodiri, A. M. 2017. Words speak for\nactions: Using text to ﬁnd video highlights. In ACPR, 322–\n327. IEEE.\nLei, J.; Berg, T. L.; and Bansal, M. 2021. Detecting\nMoments and Highlights in Videos via Natural Language\nQueries. In NeurIPS, 11846–11858.\nLei, J.; Yu, L.; Berg, T. L.; and Bansal, M. 2020. TVR: A\nLarge-Scale Dataset for Video-Subtitle Moment Retrieval.\nIn ECCV, 447–463. Springer.\nLi, J.; Selvaraju, R. R.; Gotmare, A.; Joty, S. R.; Xiong,\nC.; and Hoi, S. C. 2021. Align before Fuse: Vision and\nLanguage Representation Learning with Momentum Distil-\nlation. In NeurIPS, 9694–9705.\nLi, J.; Xie, J.; Qian, L.; Zhu, L.; Tang, S.; Wu, F.; Yang, Y .;\nZhuang, Y .; and Wang, X. E. 2022. Compositional Temporal\nGrounding with Structured Variational Cross-Graph Corre-\nspondence Learning. In IEEE CVPR, 3022–3031.\nLin, K. Q.; Zhang, P.; Chen, J.; Pramanick, S.; Gao, D.;\nWang, A. J.; Yan, R.; and Shou, M. Z. 2023. Uni-\nVTG: Towards Uniﬁed Video-Language Temporal Ground-\ning. CoRR, abs/2307.16715.\nLiu, S.; Li, F.; Zhang, H.; Yang, X.; Qi, X.; Su, H.; Zhu, J.;\nand Zhang, L. 2022a. DAB-DETR: Dynamic Anchor Boxes\nare Better Queries for DETR. In ICLR.\nLiu, W.; Mei, T.; Zhang, Y .; Che, C.; and Luo, J.\n2015. Multi-task deep visual-semantic embedding for video\nthumbnail selection. In IEEE CVPR, 3707–3715.\nLiu, Y .; Li, S.; Wu, Y .; Chen, C.-W.; Shan, Y .; and Qie,\nX. 2022b. Umt: Uniﬁed multi-modal transformers for joint\nvideo moment retrieval and highlight detection. In IEEE\nCVPR, 3042–3051.\nLu, C.; Chen, L.; Tan, C.; Li, X.; and Xiao, J. 2019. DEBUG:\nA Dense Bottom-Up Grounding Approach for Natural Lan-\nguage Video Localization. In EMNLP-IJCNLP, 5143–5152.\nACL.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5006\nLuo, H.; Ji, L.; Shi, B.; Huang, H.; Duan, N.; Li, T.; Li, J.;\nBharti, T.; and Zhou, M. 2020. Univl: A uniﬁed video and\nlanguage pre-training model for multimodal understanding\nand generation. ArXiv preprint ArXiv:2002.06353.\nMiech, A.; Alayrac, J.; Smaira, L.; Laptev, I.; Sivic, J.; and\nZisserman, A. 2020. End-to-End Learning of Visual Rep-\nresentations From Uncurated Instructional Videos. In IEEE\nCVPR, 9876–9886.\nMolino, A. G. D.; and Gygli, M. 2018. PHD-GIFs: Person-\nalized Highlight Detection for Automatic GIF Creation. In\nMM, 600–608. ACM.\nMoon, W.; Hyun, S.; Park, S.; Park, D.; and Heo, J.-P. 2023.\nQuery-dependent video representation for moment retrieval\nand highlight detection. In IEEE CVPR, 23023–23033.\nMun, J.; Cho, M.; and Han, B. 2020. Local-Global Video-\nText Interactions for Temporal Grounding. In IEEE CVPR,\n10807–10816. IEEE.\nPennington, J.; Socher, R.; and Manning, C. D. 2014. Glove:\nGlobal Vectors for Word Representation. InEMNLP, 1532–\n1543. ACL.\nRadford, A.; Kim, J. W.; Hallacy, C.; Ramesh, A.; Goh, G.;\nAgarwal, S.; Sastry, G.; Askell, A.; Mishkin, P.; Clark, J.;\nKrueger, G.; and Sutskever, I. 2021. Learning Transfer-\nable Visual Models From Natural Language Supervision. In\nICML, volume 139, 8748–8763.\nScarselli, F.; Gori, M.; Tsoi, A. C.; Hagenbuchner, M.; and\nMonfardini, G. 2008. The graph neural network model.\nTNNLS, 20(1): 61–80.\nSimonyan, K.; and Zisserman, A. 2015. Very Deep Convo-\nlutional Networks for Large-Scale Image Recognition. In\nICLR.\nSong, Y .; Redi, M.; Vallmitjana, J.; and Jaimes, A. 2016.\nTo Click or Not To Click: Automatic Selection of Beautiful\nThumbnails from Videos. In ACM CIKM, 659–668.\nSong, Y .; Vallmitjana, J.; Stent, A.; and Jaimes, A. 2015. TV-\nSum: Summarizing web videos using titles. In IEEE CVPR,\n5179–5187.\nSun, C.; Baradel, F.; Murphy, K.; and Schmid, C. 2020.\nLearning Video Representations using Contrastive Bidirec-\ntional Transformer.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Ł.; and Polosukhin, I. 2017. At-\ntention is all you need. In NeurIPS, 5998–6008.\nWang, L.; Liu, D.; Puri, R.; and Metaxas, D. N. 2020.\nLearning Trailer Moments in Full-Length Movies with Co-\nContrastive Attention. In ECCV, 300–316. Springer.\nWang, Z.; Wang, L.; Wu, T.; Li, T.; and Wu, G. 2022. Nega-\ntive Sample Matters: A Renaissance of Metric Learning for\nTemporal Grounding. In AAAI, 2613–2623. AAAI Press.\nXiong, B.; Kalantidis, Y .; Ghadiyaram, D.; and Grauman,\nK. 2019. Less Is More: Learning Highlight Detection From\nVideo Duration. In IEEE CVPR, 1258–1267.\nXiong, C.; Zhong, V .; and Socher, R. 2017. Dynamic Coat-\ntention Networks For Question Answering. In ICLR.\nXiong, Z.; and Wang, H. 2023. Dual-Stream Multimodal\nLearning for Topic-Adaptive Video Highlight Detection. In\nICMR, 272–279. ACM.\nXu, H.; He, K.; Plummer, B. A.; Sigal, L.; Sclaroff, S.; and\nSaenko, K. 2019. Multilevel Language and Vision Integra-\ntion for Text-to-Clip Retrieval. In AAAI, 9062–9069.\nXu, M.; Wang, H.; Ni, B.; Zhu, R.; Sun, Z.; and Wang, C.\n2021. Cross-category Video Highlight Detection via Set-\nbased Learning. In IEEE ICCV, 7950–7959.\nXu, P.; Zhu, X.; and Clifton, D. A. 2022. Multimodal Learn-\ning With Transformers: A Survey.IEEE TPAMI, 45: 12113–\n12132.\nXu, Y .; Sun, Y .; Li, Y .; Shi, Y .; Zhu, X.; and Du, S.\n2023. MH-DETR: Video Moment and Highlight De-\ntection with Cross-modal Transformer. ArXiv preprint\nArXiv:2305.00355.\nYan, Z.; Chen, Y .; Song, J.; and Zhu, J. 2023. Multimodal\nfeature fusion based on object relation for video captioning.\nCAAI TRIT, 8(1): 247–259.\nYe, Q.; Shen, X.; Gao, Y .; Wang, Z.; Bi, Q.; Li, P.; and Yang,\nG. 2021. Temporal Cue Guided Video Highlight Detection\nwith Low-Rank Audio-Visual Fusion. InIEEE ICCV, 7930–\n7939.\nYuan, L.; Tay, F. E. H.; Li, P.; and Feng, J. 2020. Unsuper-\nvised Video Summarization With Cycle-Consistent Adver-\nsarial LSTM Networks. IEEE TMM, 22(10): 2711–2722.\nYuan, Y .; Ma, L.; and Zhu, W. 2019. Sentence Speciﬁed\nDynamic Video Thumbnail Generation. InMM, 2332–2340.\nACM.\nZhang, B.; Yang, C.; Jiang, B.; and Zhou, X. 2022. Video\nMoment Retrieval with Hierarchical Contrastive Learning.\nIn MM, 346–355. ACM.\nZhang, D.; Dai, X.; Wang, X.; Wang, Y .; and Davis, L. S.\n2019. MAN: Moment Alignment Network for Natural Lan-\nguage Moment Retrieval via Iterative Graph Adjustment. In\nIEEE CVPR, 1247–1257.\nZhang, H.; Sun, A.; Jing, W.; Zhen, L.; Zhou, J. T.; and Goh,\nR. S. M. 2021. Natural language video localization: A revisit\nin span-based question answering framework. IEEE trans-\nactions on pattern analysis and machine intelligence, 44(8):\n4252–4266.\nZhang, H.; Sun, A.; Jing, W.; and Zhou, J. T. 2023. Tem-\nporal Sentence Grounding in Videos: A Survey and Future\nDirections. IEEE TPAMI, 45(8): 10443–10465.\nZhang, K.; Chao, W.; Sha, F.; and Grauman, K. 2016. Video\nSummarization with Long Short-Term Memory. In ECCV,\n766–782. Springer.\nZhang, S.; Peng, H.; Fu, J.; and Luo, J. 2020. Learning 2D\nTemporal Adjacent Networks for Moment Localization with\nNatural Language. In AAAI, 12870–12877.\nThe Thirty-Eighth AAAI Conference on Artiﬁcial Intelligence (AAAI-24)\n5007",
  "topic": "Reciprocal",
  "concepts": [
    {
      "name": "Reciprocal",
      "score": 0.5943654775619507
    },
    {
      "name": "Transformer",
      "score": 0.4948360323905945
    },
    {
      "name": "Computer science",
      "score": 0.47024956345558167
    },
    {
      "name": "Information retrieval",
      "score": 0.44380781054496765
    },
    {
      "name": "Engineering",
      "score": 0.18565142154693604
    },
    {
      "name": "Electrical engineering",
      "score": 0.09259340167045593
    },
    {
      "name": "Voltage",
      "score": 0.05618581175804138
    },
    {
      "name": "Linguistics",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ]
}