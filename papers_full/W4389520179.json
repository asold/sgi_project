{
  "title": "Knowledge is a Region in Weight Space for Fine-tuned Language Models",
  "url": "https://openalex.org/W4389520179",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5006303856",
      "name": "Almog Gueta",
      "affiliations": [
        null,
        "Technion – Israel Institute of Technology",
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A5016892024",
      "name": "Elad Venezian",
      "affiliations": [
        null,
        "Technion – Israel Institute of Technology",
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A5045077843",
      "name": "Colin Raffel",
      "affiliations": [
        null,
        "Technion – Israel Institute of Technology",
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A5017159143",
      "name": "Noam Slonim",
      "affiliations": [
        null,
        "Technion – Israel Institute of Technology",
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A5087402916",
      "name": "Yoav Katz",
      "affiliations": [
        null,
        "Technion – Israel Institute of Technology",
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    },
    {
      "id": "https://openalex.org/A5040286212",
      "name": "Leshem Choshen",
      "affiliations": [
        null,
        "Technion – Israel Institute of Technology",
        "University of North Carolina Health Care",
        "University of North Carolina at Chapel Hill"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W6600386477",
    "https://openalex.org/W2954226438",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W2139801605",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W1061737",
    "https://openalex.org/W6837793271",
    "https://openalex.org/W6600804061",
    "https://openalex.org/W2608377846",
    "https://openalex.org/W6604582260",
    "https://openalex.org/W4254947497",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2163455955",
    "https://openalex.org/W2963748441",
    "https://openalex.org/W2916132663",
    "https://openalex.org/W3022671937",
    "https://openalex.org/W2807333695",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2978670439",
    "https://openalex.org/W2963846996",
    "https://openalex.org/W2951936329",
    "https://openalex.org/W2943552823",
    "https://openalex.org/W4221155125",
    "https://openalex.org/W2113459411",
    "https://openalex.org/W2946659172",
    "https://openalex.org/W4313908941",
    "https://openalex.org/W2927103915",
    "https://openalex.org/W2170240176",
    "https://openalex.org/W4286856923",
    "https://openalex.org/W4287323488",
    "https://openalex.org/W4300886482",
    "https://openalex.org/W4286906081",
    "https://openalex.org/W2788838181",
    "https://openalex.org/W4318619660",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2145755360",
    "https://openalex.org/W2996074092",
    "https://openalex.org/W4292291719",
    "https://openalex.org/W4322614701",
    "https://openalex.org/W3132646476",
    "https://openalex.org/W3098649723",
    "https://openalex.org/W2187089797",
    "https://openalex.org/W4288804650",
    "https://openalex.org/W4385573419",
    "https://openalex.org/W2962977603",
    "https://openalex.org/W4385572602",
    "https://openalex.org/W3026404337",
    "https://openalex.org/W4364387746",
    "https://openalex.org/W4253067820",
    "https://openalex.org/W4226037452",
    "https://openalex.org/W4385574336",
    "https://openalex.org/W3118384492",
    "https://openalex.org/W4379528218",
    "https://openalex.org/W2251939518",
    "https://openalex.org/W3099215402",
    "https://openalex.org/W2804897457",
    "https://openalex.org/W1599016936",
    "https://openalex.org/W2942810103",
    "https://openalex.org/W1533861849",
    "https://openalex.org/W4389532453",
    "https://openalex.org/W4312091579",
    "https://openalex.org/W4256625259",
    "https://openalex.org/W4295689550",
    "https://openalex.org/W2070246124",
    "https://openalex.org/W1546425147",
    "https://openalex.org/W131533222",
    "https://openalex.org/W4281681074",
    "https://openalex.org/W2130158090",
    "https://openalex.org/W4309216950",
    "https://openalex.org/W4310998073",
    "https://openalex.org/W4307206164",
    "https://openalex.org/W2963177779",
    "https://openalex.org/W2950819771",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3093248298"
  ],
  "abstract": "Research on neural networks has focused on understanding a single model trained on a single dataset. However, relatively little is known about the relationships between different models, particularly those trained or tested on different datasets. We address this by studying how the weight space and the underlying loss landscape of different models are interconnected. Specifically, we demonstrate that finetuned models that were optimized for high performance, reside in well-defined regions in weight space, and vice versa – that any model that resides anywhere in those regions also exhibits high performance. Notably, we show that language models that have been finetuned on the same dataset form a tight cluster in the weight space, while models finetuned on different datasets from the same underlying task form a looser cluster. Moreover, traversing around the region between the models leads to new models that perform comparably or even better than models obtained via finetuning, even on tasks that the original models were not finetuned on. Our findings provide insight into the relationships between models, demonstrating that a model positioned between two similar models can acquire the knowledge of both. We leverage this and design a method for selecting a better model for efficient finetuning. Specifically, we show that starting from the center of the region is as effective, if not more, than using the pretrained model in 11 out of 12 datasets, resulting in an average accuracy improvement of 3.06.",
  "full_text": "Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1350–1370\nDecember 6-10, 2023 ©2023 Association for Computational Linguistics\nKnowledge is a Region in Weight Space for Finetuned Language Models\nAlmog Gueta ∗\nTechnion - IIT\nalmoggu@gmail.com\nElad Venezian\nIBM Research\neladv@il.ibm.com\nColin Raffel\nUNC Chapel Hill\ncraffel@gmail.com\nNoam Slonim\nIBM Research\nnoams@il.ibm.com\nYoav Katz\nIBM Research\nkatz@il.ibm.com\nLeshem Choshen\nIBM Research\nleshem.choshen@il.ibm.com\nAbstract\nResearch on neural networks has focused on\nunderstanding a single model trained on a sin-\ngle dataset. However, relatively little is known\nabout the relationships between different mod-\nels, particularly those trained or tested on differ-\nent datasets. We address this by studying how\nthe weight space and the underlying loss land-\nscape of different models are interconnected.\nSpecifically, we demonstrate that finetuned\nmodels that were optimized for high perfor-\nmance, reside in well-defined regions in weight\nspace, and vice versa – that any model that re-\nsides anywhere in those regions also exhibits\nhigh performance. Notably, we show that lan-\nguage models that have been finetuned on the\nsame dataset form a tight cluster in the weight\nspace, while models finetuned on different\ndatasets from the same underlying task form a\nlooser cluster. Moreover, traversing around the\nregion between the models leads to new mod-\nels that perform comparably or even better than\nmodels obtained via finetuning, even on tasks\nthat the original models were not finetuned on.\nOur findings provide insight into the rela-\ntionships between models, demonstrating\nthat a model positioned between two similar\nmodels can acquire the knowledge of both.\nWe leverage this and design a method for\nselecting a better model for efficient finetuning.\nSpecifically, we show that starting from the\ncenter of the region is as effective, if not more,\nthan using the pretrained model in 11 out of\n12 datasets, resulting in an average accuracy\nimprovement of 3.06.\n1 Introduction\nModels that share the same architecture but differ\nin their weights can have dramatically different\ncapabilities. As an example, finetuned variants of\na pretrained model all share an architecture, yet\nthey are specialized for different tasks. This study\n∗Research done during internship in IBM Research.\nFigure 1: A schematic view of the weight space. Fine-\ntuning ends up in a region determined by the dataset\n(deep blue) which resides in the task (light blue) and\nlanguage tasks regions (outer blue). Any combination\nof finetuned weights is found within the region. Each\nregion is characterized by a low loss on the correspond-\ning: dataset, task datasets, or diverse linguistic datasets.\nGenerally, loss is lower inside the region than outside\nor in its boundaries.\nexplores the relationship between the weights of\ndifferent finetuned models and the capabilities they\nexhibit. We analyze the weight space, where each\nmodel is represented by a weight vector θ ∈Rn.\nFor simplicity, we refer to both a point in weight\nspace and the neural network itself as a “model”.\nWe find that distance characterizes models’\nknowledge and similarity. Particularly, after fine-\ntuning a pretrained model on similar datasets, the\nresulting models are close to each other in weight\nspace (§2.3). Throughout the paper, we consider 3\ngranularities (§3.1), showing that (i) models fine-\ntuned on the same data are closer to each other\nthan to other models; (ii) models finetuned on the\nsame task also cluster together; and (iii) models\nfinetuned on general language tasks are not spread\narbitrarily around the pretrained model, but fall in\na constrained region in space.\nWe find that different finetuning runs on the same\ndata tend to converge on similar points in weight\nspace rather than dispersed points. Loosely, those\npoints embed the necessary knowledge to perform\n1350\nthe task. This leads to the hypothesis that other\npoints in the proximity of finetuned models might\nalso perform the task well. Notably, such points\nin weight space might not necessarily be reached\nvia finetuning, but rather via spatial transforma-\ntions. Indeed, we replicate the finding (Entezari\net al., 2021, c.f. §8) that models finetuned on the\nsame dataset are linearly connected, i.e., points on\nthe line between the two models attain similar or\neven lower loss (§5.1). We expand this finding\nto the convex hull between the finetuned models\n(§5.2), suggesting that knowledge is shared across\nthe region in space. That is, finetuned models\ndefine a connected basin of low loss, and every\npoint within it performs well. To show this, we\ntest models sampled from the region and find they\neven outperform the models achieved by finetun-\ning. Moreover, we replicate the findings in all the\naforementioned granularities: regions per dataset,\ntask, and in general. For each, we observe a low\nloss across datasets, beyond the loss the individ-\nual models optimized. Furthermore, we show in §6\nthat these regions are relatively tight, in the sense\nthat extrapolating (rather than interpolating) can\nquickly produce a poorly performing model.\nOur empirical findings have intriguing implica-\ntions, suggesting, for example, that the best models\nmay not lie at the edges of the region, but rather\ncloser to its center, while finetuning often yields\nmodels at the edge of the region. Motivated by\nthese findings, we demonstrate in §7 that a model\ncreated by averaging the weights of finetuned\nmodels from the same region outperforms the\npretrained model on a variety of tasks after\nsubsequent finetuning, even on tasks that the\noriginal finetuned models were not trained on.\nOverall, our work contributes to the growing\nbody of knowledge about the loss landscape, find-\ning connectivity in a whole bounded region rather\nthan mere linear connectivity, finding connectivity\nbetween models not trained on the same task, and\nfinding connectivity in generalization, evaluating\nmodels on multiple losses. We also provide initial\ncontext to empirical findings about fusing models.\nWe discuss the relations to previous works in §8.\n2 Experimental Setup\nWe conduct two main types of experiments. In\none we train models with different characteristics\n(e.g., dataset or task, see §3.1) and examine their\nrepresentation in weight space using clustering. In\nthe second experiment type, we compare losses of\none group of models to another. Below, we describe\nthe datasets (§2.1), settings (§2.2), and granularity\nlevels of comparison between models (§3.1).\n2.1 Datasets\nWe finetune and evaluate models on 36 datasets.\nThose datasets can be categorized into a few fami-\nlies: natural language inference (NLI), Sentiment\nanalysis and Topic classification tasks, Twitter do-\nmain, and a collection of general datasets that cov-\ners a wide range of capabilities. We chose classifi-\ncation datasets for reliable evaluation. The details\nof each dataset family are found in App. A. We\nmostly rely on the MNLI (Williams et al., 2018b)\ndataset, the NLI family, and the General group, as\ncase studies, and elaborate on them below:\nGeneral dataset family contains 12 text classifi-\ncation datasets from GLUE (Wang et al., 2018) and\nSuperGLUE (Wang et al., 2019), excluding test-\nonly (AX-b (Wang et al., 2019), AX-g (Poliak et al.,\n2018)) and regression (STS-B (Cer et al., 2017))\ndatasets. We further exclude WSC (Levesque et al.,\n2012) and CoPA (Roemmele et al., 2011) which\nare small and therefore produce unstable results\n(e.g., finetuning results were sometimes lower than\npretrained model results). The datasets consist of a\nwide range of classification tasks, from sentiment\nanalysis to linguistic acceptability to NLI.\nNLI family is composed of 6 natural language\ninference (NLI) datasets: MNLI (Williams et al.,\n2018a), QNLI Rajpurkar et al. 2016, RTE (Da-\ngan et al., 2005; Bar-Haim et al., 2006; Giampic-\ncolo et al., 2007; Bentivogli et al., 2009), WNLI\n(Levesque et al., 2011), ESNLI (Camburu et al.,\n2018), and adversarial NLI (Nie et al., 2020).\n2.2 Training Approaches\nWe experiment with RoBERTa-base (Liu et al.,\n2019) as our base pretrained model, except in\nApp. B where we analyze different pretrained mod-\nels. For finetuning, we follow the standard hyper-\nparameters (Liu et al., 2019), with a larger batch\nsize of 256 and a learning rate of 5e−5. Most ex-\nperiments analyze 5 different seeds, and the same-\ndataset clustering 20 seeds (§3.1). Those seeds\ncontrol randomly initialized weights in the classifi-\ncation head as well as data shuffling.\n1351\nt-SNE x\nt-SNE y\nBoolQ\nCB\nCoLA\nCOPA\nMNLI\nMRPC\nMultiRC\nQNLI\nQQP\nRTE\nSST2\nWNLI\n(a) Clustering models by dataset.\nt-SNE x\nt-SNE y\nNLI\nTOPIC\nSENT (b) Clustering models by dataset family.\nFigure 2: Clusters of finetuned models on different datasets or tasks, projected by t-SNE. We find that both datasets\nand dataset families correspond to regions in space. In each figure, each model is represented as a dot, where the\ninner color is the color of the dataset/task the model was finetuned with and the outer color is the color of the most\ncommon dataset/task in the cluster (representing the cluster label). Datasets/tasks names are shown in legends.\n2.3 Clustering Approach\nIn the clustering experiments, we qualitatively ex-\nplore whether models trained on similar data end up\nclose together in weight space. We experimented\nwith various distance metrics and clustering algo-\nrithms. While many metrics worked well, we found\nthat subtracting the pretrained weight values from\nthe finetuned values (referred to as “task vectors”\nby Ilharco et al. (2022)) and measuring distance via\ncosine similarity was conceptually simple, cheap\nto compute, and provided qualitatively reasonable\nresults compared to more sophisticated methods\n(Kornblith et al., 2019; Toledo et al., 2022). We\nalso tested Euclidean distance but it did not produce\nclear clusters. This is likely caused by the weights’\nnorm growth during training (Merrill et al., 2020)\nthat is unrelated to the data at hand (§C). This can\nalso explain questions that were previously left\nopen (Qin et al., 2022). As a clustering algorithm,\nwe use Spectral Clustering with as many clusters\nas datasets or dataset families (Pedregosa et al.,\n2011). For visualization, we project the 120M di-\nmensional weight vectors into 2 dimensions using\nt-SNE (Van der Maaten & Hinton, 2008).\n3 Methodology: Comparing Models\nIn this work, we compare models that share an\narchitecture, but were trained on different data. To\ndo so, we investigate the space of weights ω∈Rd\nwhere each model has a weight vector and each\npoint in space represents a model. We adopt the\ntypical perspective that the model fθ consists of\na representation encoder fω followed by a task-\nspecific classifier fϕ, i.e. fθ = fϕ ◦fω := fϕ,ω\n(Choshen et al., 2022a; Ram’e et al., 2022).\nIdeally, we would compare finetuned models by\ntheir loss. Unfortunately, the loss is often incompa-\nrable across datasets or tasks. Hence, we compare\nby preserving each encoder, and fitting a classifica-\ntion head to each model for each target dataset.\nSpecifically, to calculate the loss of a model we\nperform the following: First, we remove any ex-\nisting masked language modeling layers or clas-\nsification heads and replace them with a new ran-\ndomly initialized classification head. This leaves\nthe rest of the weights i.e., the encoder fω, fixed.\nWe then perform linear probing, i.e., we train only\nthe new classification head on a desired target\ndata xtrain and its labels ytrain. Lastly, we pass\nthe test data xtest through the model (including\nthe classifier fϕ on top of it) and report the loss\nwith respect to the labels ytest. Formally, for the\nmodel fϕ,ω and loss function l, we report the gen-\neralized loss lg(ω) = l(fϕ,ω(xtest),ytest) where\nfϕ = arg minϕl(fϕ,ω(xtrain),ytrain). This ap-\nproach has a desirable trait: When considering the\ntask on which the model was originally finetuned,\nour loss lg is equal to the original finetuning loss l.\nFurthermore, since fitting a linear classifier given a\nfixed representation is a convex optimization prob-\nlem, we observe similar results across runs.\nThe generalized loss lg enables comparing mod-\nels finetuned on different datasets. It is hence unde-\nsirable to test only on one of the datasets. We thus\nconsider a loss on a dataset, but also the average\nloss on a family of datasets. For example, the aver-\nage loss across all entailment datasets rather than\nthe loss on a particular dataset.\n3.1 Levels of Granularity\nTo study the relationship between weights of simi-\nlarly trained models, we experiment with 3 levels\n1352\nof granularity for dataset similarity. At each level,\nwe analyze models finetuned on source datasets\nsharing some traits. In each level’s setting, we de-\nfine an interior group (hereafter In) of datasets that\nshare a trait as well as an exterior group (hereafter\nEx) of models not sharing the trait. By default,\nwe report on each group the average loss over all\nsource datasets used for finetuning In models.\nSame-Dataset. In the most specific case, mod-\nels are similar if they were finetuned on the same\ndataset. Interior models are finetuned on MNLI\n(Williams et al., 2018a) and Ex on the rest of the\nGeneral datasets. We report the loss over MNLI.\nSame-Task. At this broader granularity, we con-\nsider the group of models trained on the same task.\nIn that case, In contains models finetuned on NLI\ndatasets and Ex contains models finetuned on all\nother datasets. We report loss over all NLI datasets,\nexcept for ANLI which is not intended for such test\npurposes. ANLI is made with adversarial examples\nthat cause misclassifications for NLI-trained mod-\nels. In initial trials, it showed similar trends, but\nwe omit it from the test for good practice.\nGeneral. In the most general case, we consider\nany model finetuned on any of the General datasets\nas In. This leaves little to consider as exterior,\nso we construct Ex by perturbing the pretrained\nmodel’s weights in a random direction. We apply\na perturbation whose norm is equal to the average\ntask vector norm of In models. Since there is no\nclear prior to sampling a random direction in space,\nwe aim for a prior that prefers points in the weight\nspace that represent \"reasonable\" networks. We use\nXavier initialization (Glorot & Bengio, 2010) to de-\nfine such a prior. The prior is an i.i.d. Gaussian\ndistribution over each weight with zero mean and\nwhere variance depends on the layer characteristics.\nThis choice reduces the probability of sampling net-\nworks with exploding or vanishing outputs, which\nwould stand as a weak baseline.\n4 Analysis in Weight Space\nWe start our analysis by showing that the models\ntrained on similar data fall into the same region\nin weight space - i.e., they are clustered together.\nWe leave the inverse claim (i.e. showing that mod-\nels within the cluster obtain a lower loss than the\nmodels outside the cluster) to §5.1 and §5.2.\nSpecifically, we find (see Fig. 2) that finetuning\non similar data results in closer weight space mod-\nels compared to models that have been trained on\ndifferent datasets or tasks. Notably, despite the fact\nthat neural networks implement highly non-linear\nfunctions, finetuning similarity is expressed in the\nEuclidean space of their weights. Moreover, we\nshow in App. §C that the direction in space is de-\ntermined by the type of training data and not by its\namount. In App. B, we show that this proximity is\ncontingent on starting from the same base model.\nSimilarity Per Dataset. In the simplest case, for\neach dataset in the General group, we finetune mod-\nels with 20 random seeds and cluster the resulting\n280 models into 12 clusters. As seen in Fig. 2(a),\nfor the most part, models finetuned on the same\ndataset are clustered together. Accordingly, the\noverall clustering accuracy is 98%, with all but 3\nclusters perfectly matched.\nSimilarity Per Task. In this experiment, we\nshow that models finetuned on datasets from the\nsame task are also close in weight space (we discuss\nsame-domain proximity in App. D). As explained\nin §2.1 we have dataset families for 3 tasks: NLI,\nTopic, and Sentiment. For each dataset in each\nfamily, We finetuned models with 5 random seeds.\nThen, we cluster all models into 3 clusters. As\nseen in Fig. 2(b), models that were finetuned on\nthe same task family are closer to each other and\nare clustered together (clustering accuracy of 90%).\nWe report the F1 Score per group in App. D.\nSimilarity in General. Unlike datasets or tasks,\nwe can not create multiple distinct general groups\nand can not expect multiple clusters to occur.\nTherefore, we do not present clustering for this\ngranularity level. However, we can still infer that\nthis general region does not encompass the whole\nspace around the pretrained model, and has a supe-\nrior loss in general (see §5.2).\n4.1 Cause: Data Type, not Size\nSupposedly, a confounding factor may explain the\nabove results, wherein the finetuned model moves\nmore with more data. To test this, we finetune\nmodels on sub-samples with different sample sizes\n(200, 400, 800, 1.6K, 3K). For consistency, we take\nonly the 9 datasets from General family that contain\nat least 3K training samples. We then cluster the\nfinetuned models into kclusters, with kthe number\nof datasets or the number of dataset sizes.\nThe resulting clusters (App. C) are clustered\nby data type, not by the amount of data, similar to\n1353\nFig. 2. Choosing kto be the number of data-sizes\ndoes not cluster by data size either. We conclude\nthat the observed similarity comes from the nature\nof the data, and not from the size of a given dataset.\n5 Loss in the Region between Models\nIn §4, we claim that models trained on similar data\nconverge near each other, but is this area to which\nthey converge meaningful? In this section, we show\nthat models falling in the entire region around these\nclusters correspond to performant models.\nThe models we analyzed so far were the outcome\nof a gradient-based optimization process searching\nfor the minimum loss. The locality we observed\nin weight space indicates that the points found\nthrough this procedure are concentrated in rela-\ntively small regions. We hypothesize that a whole\nregion of low losses (corresponding to performant\nmodels) exists between the separate points found\nduring finetuning. For example, the \"NLI region\"\ncontains MNLI, SNLI and QNLI models but also\nother points that reflect models that might not have\nbeen found through gradient-based optimization on\na specific dataset but exhibit the general abilities\nneeded to perform natural language inference.\nWe test this hypothesis by interpolating pairs of\nsimilarly trained models and show in § 5.1 that\nthe points between the models perform comparably\nto or even better than the original finetuned mod-\nels. This suggests that indeed there are regions in\nweight space where all points encode the knowl-\nedge or behaviour required for a particular task. We\nexpand this claim in §5.2 and show that the whole\nregion that lies between these models (their convex\nhull) corresponds to models that perform well.\n5.1 Interpolation: Lines Between Model Pairs\nIn this experiment, we consider the points in weight\nspace between pairs of finetuned models. Given\na pair of models, we shift from one model to\nthe other by linearly interpolating between their\nweights, i.e., we take the model’s weights ω1,ω2 ∈\nRd, and consider weighted sums of their weights:\nω1 ∗α+ ω2 ∗ (1 −α). where α ∈[0,1]. We\nthen evaluate each interpolated model both on the\ndatasets the original models were finetuned on, and\non additional datasets unseen by the models. We\ninterpolate pairs of different models finetuned on\nthe same dataset, or on two different datasets. We\nreport the average losses produced by repeating the\nexperiment with finetuning using different seeds.\nResults ( Fig. 3) show that interpolated models\nperform comparably or even better than the models\nthey are created from. We present further results\ntesting the groups on different losses in App. §E\nand find performance is often best somewhere in\nthe interpolation between the two models. We now\nelaborate on each granularity level separately.\nInterpolation Per Dataset. We interpolate 5 fine-\ntuned models on the MNLI dataset (resulting in a\ntotal of 10 pairs) and evaluate on MNLI. We report\nan analogous experiment with SST2 in App. §E.\nFigure 3(a) shows that the interpolated models per-\nform well on average and even outperform the orig-\ninal models they are created from. Similar results\nwere found in other settings (e.g.; Wortsman et al.,\n2022b) and we discuss those works in §8.\nInterpolation Per Task. We interpolate 5 models\nfinetuned on MNLI with 5 models finetuned on\nESNLI, both from the NLI task, resulting in 25\npairs, and evaluate on all NLI test datasets. We\nreplicate the results of the previous experiment and\nfind the interpolated models are performant on all\ntargets on average, as can be seen in Fig. 3(b).\nInterpolation In General. We interpolate 5 mod-\nels finetuned on MNLI with 5 models finetuned on\nSST2, both from the General family, resulting in\n25 pairs and evaluate on all General datasets as\ntargets. Fig. 3(c) shows improved performance in\nthis extended group and better performance in the\ninterpolated models than in the finetuned ones.\n5.2 Comparison between Region losses\nThus far, we showed that models on the line be-\ntween model pairs perform well. We now extend\nthe analysis to show that models in the whole re-\ngion between similar models perform well. How-\never, visualizing or searching a whole multidimen-\nsional region (the convex hull) is not feasible. In-\nstead, we sample models in the region and show\nthey outperform their external counterparts.\nLet In be a group of models and In’ be the con-\nvex hull between all the models in In, making each\nmodel in In’ a weighted average of models in In:∑|In|\ni=0 αi·ωiwhere ∑|In|\ni=0 αi = 1and ωi ∈In. Prac-\ntically, as In’ is infinite, we estimate it by sampling\n|In|models uniformly from the region they convey.\nWe note that weighted averaging in this manner\nwas shown to be practical and work well in many\nscenarios, either in efficient finetuning (§7 Yadav\n1354\n(a) Interpolation per dataset.\nMNLI0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9ESNLI\n0.1500\n0.1525\n0.1550\n0.1575\n0.1600\n0.1625\n0.1650Losses\nNLI Avg (b) Interpolation per task.\n (c) Interpolation in General.\nFigure 3: Losses of linearly interpolated models created between pairs of similar models. The best loss often lies\nbetween models. In each figure, the solid line is the losses’ average during interpolations for different αvalues, the\nedges of the lines represent the average loss pure finetuned models we interpolate, the Y axis is the average loss\nvalue, and the X axis is the position determined by α. The shade is the standard deviation of the losses’ average.\n0.125 0.150 0.175 0.200 0.225 0.250 0.275\nloss\n0\n20\n40\n60\n80\n100Percent\ngroup\nEx\nIn\nIn'\n(a) Losses in the dataset region\n0.20 0.25 0.30 0.35\nLoss\n0\n20\n40\n60\n80Percent\ngroup\nEx\nIn\nIn' (b) Losses in the task region\n0.21 0.22 0.23 0.24 0.25 0.26\nLoss\n0\n20\n40\n60\n80\n100Percent\ngroup\nEx\nIn\nIn' (c) Losses in the general region\nFigure 4: Loss distributions of 3 groups: In (similarly finetuned models), In’ (models between models in In), and Ex\n(baseline models). Fig. 4(a) shows 5 models from MNLI region tested on the MNLI loss. Fig. 4(b) shows models\nfrom NLI region tested on NLI losses. Fig. 4(c) shows models from the General region tested on the General losses.\net al., 2023) or in full finetuning (Choshen et al.,\n2022b; Matena & Raffel, 2021, c.f. §8).\nWe define a metric to compare two groups of\nmodels. Given In models group and the exterior\nmodels group Ex, we calculate PB as the probabil-\nity that an In model outperforms an Ex one:\nPB = E\ni∈In,j∈Ex\n1 {lg(ωi) ≤lg(ωj)}.\nPB can also be applied to In’ and Ex.\nAs a loss function, we take the average loss over\nthe source datasets used to create the In models.\nTesting models from In and In’ groups, we find\nthey indeed outperform Ex models on the tasks the\nIn models were trained on. We find this is true in\nall granularity levels – models in the dataset region\nare better than other models, and more broadly any\nfinetuned model is better than models that have\nbeen randomly shifted by the same distance from\nthe pretrained model. Moreover, we again find\n(as in §5.1) that In’ is even better than the In. In\naddition to the bottom-line metric PB, we depict\nthe loss distributions across those models in Fig. 4.\nLoss Per Dataset. We test the performance of\nmodels between models finetuned on a dataset. We\nconsider the case where In is the group of finetuned\nmodels on MNLI and Ex is the group of finetuned\nmodels on General datasets. Both groups are evalu-\nated on the MNLI dataset. We find PB is 100% for\nIn, meaning that all MNLI models outperform on\nMNLI than all the rest of the models. More surpris-\ning is that the same is true forIn’, PB of 100% – all\nthe models between MNLI models are better than\nEx. In fact, in 88% of the times In’ models are also\nbetter than In – i.e. models finetuned on MNLI!\nLoss Per Task. We compare models from a task\nregion with models from other regions. Here, In\nare the models finetuned on NLI task and Ex on the\nrest of the datasets described in §2.1. Both groups\nare evaluated on all NLI test datasets. NLIIn group\nmodels are better inPB = 75.3% of the cases, and\nthe In’ models in 100%. In’ is also better than In\nwith PB = 96.7%.\nLoss In General. We define In to be finetuned\nmodels on General datasets and Ex to be random\nmodels as defined in §3.1. Both are evaluated on\n1355\nthe General datasets. We find again that In is better\nthan Ex (PB = 89.8%) but worse than In’ (PB =\n90%) which is also better than Ex (PB = 100%).\nTo conclude, we consistently see that the region\nbetween finetuned models not only provide models\nthat are better than the baseline but also provides\nmodels that are better than the finetuned models\ndefining the edges of region.\n6 Region Edges\nAbove, we have shown that there are spacial re-\ngions that specify learnt generalizations. We now\nlook for the boundaries of those regions, where loss\nis no longer similarly low. To do that we traverse\nin the opposite way to the interpolation. We also\ntest the edges going from the center of the region\nto other directions in App. F.\n6.1 Extrapolation: Lines Between Models\nIn Section 5.1, we took pairs of models and found\nthat the linear path between them passes through\na region of low loss. We now continue on this\npath and check how far in the opposite directions\n(i.e. away from the model being interpolated to) do\nwe need to move in order for the loss to rise. We\nreproduce the interpolations settings of §5.1, but\napply linear extrapolation, i.e., test αvalues out\nof range [0,1]. We make 10 steps in logarithmic\nadvances from 1 to 32 and similarly from 0 to -31.\nFigure 5 depicts the results for the Same-Dataset\ngranularity level. We provide more detailed results\nin App. G. We find that for all granularity levels ex-\ntrapolation rapidly reaches bad performance. This\nimplies the converged models are near the edge of\nthe loss basin. We further observe that the region\nhas a relatively flat base and steep cliffs, implying\nthat the regions we find are small basins and not\ne.g. a subspace. In a sense, we discover a bounded\nregion that characterizes the loss region (of e.g.,\nMNLI dataset) where the models within have a low\nloss and the models beyond have a high loss.\n7 Practical Takes\nOur work has several practical implications. First,\nwe observed (§5.2) that models inside the region\n(In’) are often superior to the finetuned models\ndefining the region (In). Practically, one can aver-\nage models from the same region and cautiously\nexpect the resulting model to perform better. This\nmodel can be used without further finetuning, in\nFigure 5: Losses of linearly extrapolated models created\nfrom pairs of models finetuned on MNLI. The solid line\nis the average losses, the vertical dashed lines indicate\nthe average loss of the pure models we extrapolate (α=\n0 or α= 1), and the X axis is the position (meaning the\nαand (1 −α) values used in the extrapolation). The\nshade is the standard deviation across runs.\nthe Same-Dataset region, as has indeed been used\nin practice (c.f. §8; Wortsman et al., 2022b,a).\nWe provide another implication of our findings.\nIf indeed models in In’ share partial information\nwith models from In, this aggregated information\nmay be general and useful for other tasks. In prac-\ntice, there are two common uses for a trained model,\neither for the immediate classification of unseen\nexamples or as a starting point for further training.\nWe focus on the later use as a low loss directly\nindicates it could be useful in that setting.\nWe hypothesize that points in the region could be\nbetter for finetuning than finetuning the pretrained\nmodel itself. As there are endless possibilities of\npoints in the region with no preference to any spe-\ncific, we practically pick the centroid of the region,\ni.e., the average between models in In. The cen-\ntroid point is equally influenced by each model\ndefining the region, and without further informa-\ntion may be stronger than arbitrary points in the\nregion (see App. §E), but also be suboptimal (see\n§5.1, App. §E).\nFor subsequent training, we employ parameter-\nefficient finetuning. Specifically, BitFit (Ben Zaken\net al., 2022), one of the most parameter-efficient\nmethods, which has been shown to attain strong\nperformance. Changing only a small subset of\nthe weights reduces the complex effects of training\ndynamics and eases attributing improvements to the\ninitialization weights. We avoid giving an unfair\nadvantage to our method and for each target dataset\nchoose the centroid of all models excluding ones\nfinetuned on the target dataset itself.\nWe find (Fig. 6 and App. H) that starting from\n1356\nthe centroid results in a better performing model\nthan starting from a pretrained model, by 4.04% on\naverage. The centroid is better in almost all cases,\noutperforming the pretrained in 9 cases, matching\nthe results in 2, and underperforming in 1 case.\nEfficient finetuning is especially interesting in\nthe scenario of scarce data (App. H). We hence\nreplicate the results in a few-shot scenario limiting\nthe training examples to 1K. The general trend is\nreplicated, only that improvements reach as high as\n34% improvement and above 10.66% on average.\n2\n 0 2 4 6 8 10\nAccuracy Difference\nWNLI\nMultiRC\nBoolQ\nCoLA\nWIC\nMRPC\nQQP\nSST2\nCB\nQNLI\nMNLI\nRTE\nRTE\nMNLI\nQNLI\nCB\nSST2\nQQP\nMRPC\nWIC\nCoLA\nBoolQ\nMultiRC\nWNLI\nFigure 6: Centroid model gains over the pretrained.\nModels efficiently finetuned(BitFit) over target datasets.\n8 Explaining previous results\nA great deal of prior work considered the con-\nnectivity between models, i.e. whether the path in\nweight space between two networks has a low loss\nthroughout. Early work demonstrated that models\ntrained on the same dataset have such a path but\nthat the path is not necessarily linear (Garipov et al.,\n2018; Frankle et al., 2020). This non-linearity\nwas often explained by the fact that networks can\nrepresent the same function after their weights are\npermuted (Ainsworth et al., 2022; Jordan et al.,\n2022; Chen et al., 1993; Hecht-Nielsen, 1990).\nTaking into account these symmetries and/or using\nthe same initialization was then shown to produce\na linear path of low loss (McMahan et al., 2017;\nEntezari et al., 2021). Benton et al. (2021) even\nconsidered simplexes of low loss, rather than linear\npaths. In addition, Mirzadeh et al. (2020) showed\nthat multitask learning converges to a point with\nlow loss for both tasks, and in parallel work Qin\net al. (2022) showed that the minima are connected\nfor two datasets of the same task. We generalize\nthose notions in the context of finetuned models.\nSpecifically, we confirm that indeed there is a linear\npath between two models, but further that there is\na whole region with low loss through which the\nlinear path moves. Intriguingly, we have observed\nthat these low-loss regions are unique for each\nspecific dataset or task, whereas Juneja et al. (2022)\nhas reported the existence of multiple basins per\neach. We also generalize this finding to models that\nwere not trained on the same data and are tested\non different data. Qin et al. (2022) is the only work\nwe know to compare models trained on different\ntasks. However, they report random chance perfor-\nmance in this case. To enable meaningful model\ncomparison, we proposed the generalized loss (§3).\nOur results also support and provide some pre-\nliminary explanations of recent practical findings.\nSome works show that starting from a finetuned\nmodel helps when finetuning on a different target\ndataset (Choshen et al., 2022a; Phang et al., 2018),\nwhich may be related to the fact that the initial\nfinetuning stage moves the model into the general\n\"language\" region (or, even better, the region of\nspace corresponding to the target task). Moreover,\na growing literature has shown improvements from\naveraging two or more finetuned models. Some of\nthose average models trained on the same dataset\n(Wortsman et al., 2022b,a), which we show picks a\nmodel from inside the dataset region. Others show\nthat averages between models can improve models\nfrom tasks that they were not trained on (Choshen\net al., 2022b; Matena & Raffel, 2021), which agrees\nwith our more general findings. Ilharco et al. (2022)\nfurther suggests that some attributes can be added\nto the model by moving in certain directions in the\nweight space. In parallel work, Ram’e et al. (2022)\nconsiders two finetuning stages before averaging.\nLu et al. (2022) and Talman et al. (2023) propose\noptimization methods featuring Stochastic Weight\nAveraging (SW A). Our results may indicate that the\nsuccess of such methods may be partly attributed to\nits tendency to fall within a region, rather than on\nits borders. More recent work considers iterative\nmodel averaging, where in each iteration multiple\nmodels are trained in parallel from the same initial\npoint and then averaged to aggregate their knowl-\nedge. Such a procedure has been demonstrated\nboth for self-supervised pretraining (Li et al., 2022)\nand as a supervised pretraining, similar to a mas-\nsively multitask learning scenario (Don-Yehiya\net al., 2022). Future work could focus on under-\nstanding how those processes move through the\nweight space and whether they move to areas of\nloss space outside of the region corresponding to\na single iteration of averaging finetuned models.\n1357\n9 Conclusion and Discussion\nCombining all of our results together conveys a con-\nsistent message: There are regions in weight space\ncorresponding to good performance on a dataset, a\ntask, or in general. From §2.3 we can conclude that\nperformant models are centered in certain areas\n(or more specifically basins) in weight space. We\nfind in §5.1 that these form one basin rather than\nmultiple nearby points falling into multiple basins\nand, in §5.2, that this basin is a convex region and\nnot simply a line between two points. Finally, the\nextrapolations in §6 show those areas do not ex-\nceed far beyond the finetuned models. Moreover,\nour results suggest that models found via finetun-\ning typically lie on the boundaries of these regions\nand are often suboptimal, prompting future work in\nexploring the limitations of gradient-based training.\n10 Limitations\nWe discuss limitations where relevant throughout\nthe work, but also provide this section for general\ndiscussion of limitations.\nOur work was only evaluated on finetuning a pre-\ntrained model, and hence may not hold in general\nwhen randomly initializing. They also focused on\nEnglish classification data.\nWhile our results were very robust when refer-\nring to tasks, we did not find many groups of\ndatasets of distinct domains to test on and got\nmixed results in those aspects. We discuss the\nresults in App. D.\nThe scope of our experiments is broad in some\naspects it is less so in others. While our exper-\niments included thousands of finetuned models,\ntrained on 36 datasets and also evaluated on 36\ndatasets. We did not replicate it on many pretrained\nmodels as well.\n1358\nReferences\nAinsworth, S. K., Hayase, J., and Srinivasa, S. Git\nre-basin: Merging models modulo permutation sym-\nmetries. arXiv preprint arXiv:2209.04836, 2022.\nBar-Haim, R., Dagan, I., Dolan, B., Ferro, L., Giampic-\ncolo, D., and Magnini, B. The second pascal recog-\nnising textual entailment challenge. 2006.\nBarbieri, F., Camacho-Collados, J., Ronzano, F.,\nEspinosa-Anke, L., Ballesteros, M., Basile, V ., Patti,\nV ., and Saggion, H. SemEval-2018 Task 2: Mul-\ntilingual Emoji Prediction. In Proceedings of the\n12th International Workshop on Semantic Evaluation\n(SemEval-2018), New Orleans, LA, United States,\n2018. Association for Computational Linguistics.\nBarbieri, F., Camacho-Collados, J., Espinosa Anke,\nL., and Neves, L. TweetEval: Unified bench-\nmark and comparative evaluation for tweet classi-\nfication. In Findings of the Association for Com-\nputational Linguistics: EMNLP 2020 , pp. 1644–\n1650, Online, November 2020. Association for Com-\nputational Linguistics. doi: 10.18653/v1/2020.\nfindings-emnlp.148. URL https://aclanthology.\norg/2020.findings-emnlp.148.\nBasile, V ., Bosco, C., Fersini, E., Nozza, D., Patti, V .,\nRangel Pardo, F. M., Rosso, P., and Sanguinetti, M.\nSemEval-2019 task 5: Multilingual detection of hate\nspeech against immigrants and women in Twitter. In\nProceedings of the 13th International Workshop on\nSemantic Evaluation, pp. 54–63, Minneapolis, Min-\nnesota, USA, June 2019. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/S19-2007. URL\nhttps://aclanthology.org/S19-2007.\nBen Zaken, E., Goldberg, Y ., and Ravfogel, S.\nBitFit: Simple parameter-efficient fine-tuning for\ntransformer-based masked language-models. In Pro-\nceedings of the 60th Annual Meeting of the As-\nsociation for Computational Linguistics (Volume\n2: Short Papers) , pp. 1–9, Dublin, Ireland, May\n2022. Association for Computational Linguistics.\ndoi: 10.18653/v1/2022.acl-short.1. URL https:\n//aclanthology.org/2022.acl-short.1.\nBentivogli, L., Clark, P., Dagan, I., and Giampiccolo,\nD. The sixth pascal recognizing textual entailment\nchallenge. In TAC, 2009.\nBenton, G., Maddox, W., Lotfi, S., and Wilson, A. G. G.\nLoss surface simplexes for mode connecting volumes\nand fast ensembling. In International Conference on\nMachine Learning, pp. 769–779. PMLR, 2021.\nCamburu, O.-M., Rocktäschel, T., Lukasiewicz, T., and\nBlunsom, P. e-snli: Natural language inference with\nnatural language explanations. In NeurIPS, 2018.\nCer, D. M., Diab, M. T., Agirre, E., Lopez-Gazpio, I.,\nand Specia, L. Semeval-2017 task 1: Semantic tex-\ntual similarity multilingual and crosslingual focused\nevaluation. In International Workshop on Semantic\nEvaluation, 2017.\nChen, A. M., Lu, H.-m., and Hecht-Nielsen, R. On\nthe geometry of feedforward neural network error\nsurfaces. Neural Computation, 5(6):910–927, 1993.\ndoi: 10.1162/neco.1993.5.6.910.\nChoshen, L., Venezian, E., Don-Yehia, S., Slonim, N.,\nand Katz, Y . Where to start? analyzing the poten-\ntial value of intermediate models. arXiv preprint\narXiv:2211.00107, 2022a.\nChoshen, L., Venezian, E., Slonim, N., and Katz, Y .\nFusing finetuned models for better pretraining. arXiv\npreprint arXiv:2204.03044, 2022b.\nClark, C., Lee, K., Chang, M.-W., Kwiatkowski, T.,\nCollins, M., and Toutanova, K. BoolQ: Exploring\nthe surprising difficulty of natural yes/no questions.\nIn Proceedings of the 2019 Conference of the North\nAmerican Chapter of the Association for Compu-\ntational Linguistics: Human Language Technolo-\ngies, Volume 1 (Long and Short Papers), pp. 2924–\n2936, Minneapolis, Minnesota, June 2019. Associa-\ntion for Computational Linguistics. doi: 10.18653/\nv1/N19-1300. URL https://aclanthology.org/\nN19-1300.\nDagan, I., Glickman, O., and Magnini, B. The pascal\nrecognising textual entailment challenge. In MLCW,\n2005.\nde Marneffe, M.-C., Simons, M., and Tonhauser,\nJ. The CommitmentBank: Investigating projec-\ntion in naturally occurring discourse. 2019. To\nappear in Proceedings of Sinn und Bedeutung 23 .\nData can be found at https://github.com/mcdm/\nCommitmentBank/.\nDolan, W. B. and Brockett, C. Automatically con-\nstructing a corpus of sentential paraphrases. In\nProceedings of the Third International Workshop\non Paraphrasing (IWP2005) , 2005. URL https:\n//aclanthology.org/I05-5002.\nDon-Yehiya, S., Venezian, E., Raffel, C., Slonim, N.,\nKatz, Y ., and Choshen, L. Cold fusion: Collaborative\ndescent for distributed multitask finetuning. 2022.\nElazar, Y ., Kassner, N., Ravfogel, S., Feder, A.,\nRavichander, A., Mosbach, M., Belinkov, Y ., Schütze,\nH., and Goldberg, Y . Measuring causal effects of data\nstatistics on language model’s ‘factual’ predictions.\n2022.\nEntezari, R., Sedghi, H., Saukh, O., and Neyshabur,\nB. The role of permutation invariance in linear\nmode connectivity of neural networks. arXiv preprint\narXiv:2110.06296, 2021.\nFrankle, J., Dziugaite, G. K., Roy, D., and Carbin, M.\nLinear mode connectivity and the lottery ticket hy-\npothesis. In III, H. D. and Singh, A. (eds.), Pro-\nceedings of the 37th International Conference on\nMachine Learning, volume 119 of Proceedings of\nMachine Learning Research, pp. 3259–3269. PMLR,\n13–18 Jul 2020. URL https://proceedings.mlr.\npress/v119/frankle20a.html.\n1359\nGaripov, T., Izmailov, P., Podoprikhin, D., Vetrov, D. P.,\nand Wilson, A. G. Loss surfaces, mode connectivity,\nand fast ensembling of dnns. Advances in neural\ninformation processing systems, 31, 2018.\nGiampiccolo, D., Magnini, B., Dagan, I., and Dolan,\nW. B. The third pascal recognizing textual entailment\nchallenge. In ACL-PASCAL@ACL, 2007.\nGlorot, X. and Bengio, Y . Understanding the difficulty\nof training deep feedforward neural networks. In\nTeh, Y . W. and Titterington, M. (eds.),Proceedings\nof the Thirteenth International Conference on Artifi-\ncial Intelligence and Statistics, volume 9 of Proceed-\nings of Machine Learning Research , pp. 249–256,\nChia Laguna Resort, Sardinia, Italy, 13–15 May 2010.\nPMLR. URL https://proceedings.mlr.press/\nv9/glorot10a.html.\nHe, R. and McAuley, J. Ups and downs: Modeling\nthe visual evolution of fashion trends with one-class\ncollaborative filtering. In proceedings of the 25th\ninternational conference on world wide web, pp. 507–\n517, 2016.\nHecht-Nielsen, R. On the algebraic structure\nof feedforward network weight spaces. In\nECKMILLER, R. (ed.), Advanced Neural Com-\nputers, pp. 129–135. North-Holland, Amsterdam,\n1990. ISBN 978-0-444-88400-8. doi: https:\n//doi.org/10.1016/B978-0-444-88400-8.50019-4.\nURL https://www.sciencedirect.com/\nscience/article/pii/B9780444884008500194.\nIlharco, G., Ribeiro, M. T., Wortsman, M., Gururan-\ngan, S., Schmidt, L., Hajishirzi, H., and Farhadi, A.\nEditing models with task arithmetic. arXiv preprint\narXiv:2212.04089, 2022.\nJordan, K., Sedghi, H., Saukh, O., Entezari, R., and\nNeyshabur, B. Repair: Renormalizing permuted\nactivations for interpolation repair. arXiv preprint\narXiv:2211.08403, 2022.\nJuneja, J., Bansal, R., Cho, K., Sedoc, J., and Saphra, N.\nLinear connectivity reveals generalization strategies.\narXiv preprint arXiv:2205.12411, 2022.\nKhashabi, D., Chaturvedi, S., Roth, M., Upadhyay, S.,\nand Roth, D. Looking beyond the surface: A chal-\nlenge set for reading comprehension over multiple\nsentences. In Proceedings of the Conference of the\nNorth American Chapter of the Association for Com-\nputational Linguistics: Human Language Technolo-\ngies (NAACL-HLT). Association for Computational\nLinguistics, 2018. URL https://www.aclweb.\norg/anthology/papers/N/N18/N18-1023/.\nKornblith, S., Norouzi, M., Lee, H., and Hinton, G.\nSimilarity of neural network representations revisited.\nIn International Conference on Machine Learning,\npp. 3519–3529. PMLR, 2019.\nLevesque, H., Davis, E., and Morgenstern, L. The Wino-\ngrad schema challenge. In Thirteenth International\nConference on the Principles of Knowledge Repre-\nsentation and Reasoning , 2012. URL http://dl.\nacm.org/citation.cfm?id=3031843.3031909.\nLevesque, H. J., Davis, E., and Morgenstern, L. The\nwinograd schema challenge. In KR, 2011.\nLi, M., Gururangan, S., Dettmers, T., Lewis, M., Althoff,\nT., Smith, N. A., and Zettlemoyer, L. Branch-train-\nmerge: Embarrassingly parallel training of expert\nlanguage models. arXiv preprint arXiv:2208.03306,\n2022.\nLi, X. and Roth, D. Learning question classifiers. In\nCOLING 2002: The 19th International Conference\non Computational Linguistics, 2002. URL https:\n//aclanthology.org/C02-1150.\nLiu, Y ., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D.,\nLevy, O., Lewis, M., Zettlemoyer, L., and Stoyanov,\nV . Roberta: A robustly optimized bert pretraining\napproach. ArXiv, abs/1907.11692, 2019.\nLu, P., Kobyzev, I., Rezagholizadeh, M., Rashid, A., Gh-\nodsi, A., and Langlais, P. Improving generalization\nof pre-trained language models via stochastic weight\naveraging. arXiv preprint arXiv:2212.05956, 2022.\nMaas, A. L., Daly, R. E., Pham, P. T., Huang, D.,\nNg, A. Y ., and Potts, C. Learning word vectors\nfor sentiment analysis. In Proceedings of the 49th\nAnnual Meeting of the Association for Computa-\ntional Linguistics: Human Language Technologies,\npp. 142–150, Portland, Oregon, USA, June 2011.\nAssociation for Computational Linguistics. URL\nhttp://www.aclweb.org/anthology/P11-1015.\nMalo, P., Sinha, A., Korhonen, P., Wallenius, J., and\nTakala, P. Good debt or bad debt: Detecting seman-\ntic orientations in economic texts. Journal of the\nAssociation for Information Science and Technology,\n65(4):782–796, 2014.\nMatena, M. and Raffel, C. Merging models\nwith fisher-weighted averaging. arXiv preprint\narXiv:2111.09832, 2021.\nMcMahan, B., Moore, E., Ramage, D., Hampson, S.,\nand y Arcas, B. A. Communication-efficient learn-\ning of deep networks from decentralized data. In\nArtificial intelligence and statistics, pp. 1273–1282.\nPMLR, 2017.\nMerrill, W., Ramanujan, V ., Goldberg, Y ., Schwartz,\nR., and Smith, N. A. Parameter norm growth during\ntraining of transformers. 2020.\nMirzadeh, S. I., Farajtabar, M., Gorur, D., Pascanu,\nR., and Ghasemzadeh, H. Linear mode connectivity\nin multitask and continual learning. arXiv preprint\narXiv:2010.04495, 2020.\nMohammad, S. M. and Bravo-Marquez, F. Emotion\nintensities in tweets. In Proceedings of the sixth joint\nconference on lexical and computational semantics\n(*Sem), Vancouver, Canada, 2017.\n1360\nNie, Y ., Williams, A., Dinan, E., Bansal, M., Weston,\nJ., and Kiela, D. Adversarial NLI: A new benchmark\nfor natural language understanding. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pp. 4885–4901, Online,\nJuly 2020. Association for Computational Linguistics.\ndoi: 10.18653/v1/2020.acl-main.441. URL https:\n//aclanthology.org/2020.acl-main.441.\nPang, B. and Lee, L. Seeing stars: Exploiting class\nrelationships for sentiment categorization with re-\nspect to rating scales. In Proceedings of the 43rd\nAnnual Meeting of the Association for Computa-\ntional Linguistics (ACL’05), pp. 115–124, Ann Arbor,\nMichigan, June 2005. Association for Computational\nLinguistics. doi: 10.3115/1219840.1219855. URL\nhttps://aclanthology.org/P05-1015.\nPedregosa, F., Varoquaux, G., Gramfort, A., Michel,\nV ., Thirion, B., Grisel, O., Blondel, M., Prettenhofer,\nP., Weiss, R., Dubourg, V ., Vanderplas, J., Passos,\nA., Cournapeau, D., Brucher, M., Perrot, M., and\nDuchesnay, E. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research, 12:\n2825–2830, 2011.\nPhang, J., Févry, T., and Bowman, S. R. Sentence\nencoders on stilts: Supplementary training on inter-\nmediate labeled-data tasks. ArXiv, abs/1811.01088,\n2018.\nPilehvar, M. T. and Camacho-Collados, J. WiC:\nThe word-in-context dataset for evaluating context-\nsensitive meaning representations. In Proceedings\nof the Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies (NAACL-HLT). Asso-\nciation for Computational Linguistics, 2019. URL\nhttps://arxiv.org/abs/1808.09121.\nPoliak, A., Haldar, A., Rudinger, R., Hu, J. E., Pavlick,\nE., White, A. S., and Durme, B. V . Collecting diverse\nnatural language inference problems for sentence rep-\nresentation evaluation. In Conference on Empirical\nMethods in Natural Language Processing, 2018.\nQin, Y ., Qian, C., Yi, J., Chen, W., Lin, Y ., Han, X., Liu,\nZ., Sun, M., and Zhou, J. Exploring mode connectiv-\nity for pre-trained language models. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing, pp. 6726–6746, Abu\nDhabi, United Arab Emirates, December 2022. Asso-\nciation for Computational Linguistics. URL https:\n//aclanthology.org/2022.emnlp-main.451.\nRajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P.\nSQuAD: 100,000+ questions for machine compre-\nhension of text. In Proceedings of the 2016 Con-\nference on Empirical Methods in Natural Language\nProcessing, pp. 2383–2392, Austin, Texas, Novem-\nber 2016. Association for Computational Linguis-\ntics. doi: 10.18653/v1/D16-1264. URL https:\n//aclanthology.org/D16-1264.\nRam’e, A., Ahuja, K., Zhang, J., Cord, M., Bottou,\nL., and Lopez-Paz, D. Recycling diverse mod-\nels for out-of-distribution generalization. ArXiv,\nabs/2212.10445, 2022.\nRoemmele, M., Bejan, C. A., and Gordon, A. S. Choice\nof plausible alternatives: An evaluation of common-\nsense causal reasoning. In 2011 AAAI Spring Sympo-\nsium Series, 2011.\nRosenthal, S., Farra, N., and Nakov, P. SemEval-2017\ntask 4: Sentiment analysis in Twitter. In Proceed-\nings of the 11th International Workshop on Semantic\nEvaluation (SemEval-2017), pp. 502–518, Vancou-\nver, Canada, August 2017. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/S17-2088. URL\nhttps://aclanthology.org/S17-2088.\nScherer, K. R. and Wallbott, H. G. Evidence for univer-\nsality and cultural variation of differential emotion\nresponse patterning. Journal of personality and so-\ncial psychology, 66(2):310, 1994.\nSheng, E. and Uthus, D. Investigating societal biases\nin a poetry composition system. In Proceedings of\nthe Second Workshop on Gender Bias in Natural\nLanguage Processing, pp. 93–106, Barcelona, Spain\n(Online), December 2020. Association for Compu-\ntational Linguistics. URL https://aclanthology.\norg/2020.gebnlp-1.9.\nSocher, R., Perelygin, A., Wu, J., Chuang, J., Manning,\nC. D., Ng, A., and Potts, C. Recursive deep mod-\nels for semantic compositionality over a sentiment\ntreebank. In Proceedings of the 2013 Conference on\nEmpirical Methods in Natural Language Processing,\npp. 1631–1642, Seattle, Washington, USA, Octo-\nber 2013. Association for Computational Linguistics.\nURL https://aclanthology.org/D13-1170.\nTalman, A., Celikkanat, H., Virpioja, S., Heinonen, M.,\nand Tiedemann, J. Uncertainty-aware natural lan-\nguage inference with stochastic weight averaging.\narXiv preprint arXiv:2304.04726, 2023.\nToledo, A., Venezian, E., and Slonim, N. Revisiting\nsequential information bottleneck: New implementa-\ntion and evaluation. Entropy, 24(8):1132, 2022.\nVan der Maaten, L. and Hinton, G. Visualizing data\nusing t-sne. Journal of machine learning research, 9\n(11), 2008.\nVan Hee, C., Lefever, E., and Hoste, V . SemEval-\n2018 task 3: Irony detection in English tweets. In\nProceedings of The 12th International Workshop\non Semantic Evaluation , pp. 39–50, New Orleans,\nLouisiana, June 2018. Association for Computational\nLinguistics. doi: 10.18653/v1/S18-1005. URL\nhttps://aclanthology.org/S18-1005.\nWang, A., Singh, A., Michael, J., Hill, F., Levy, O.,\nand Bowman, S. GLUE: A multi-task benchmark\n1361\nand analysis platform for natural language under-\nstanding. In Proceedings of the 2018 EMNLP Work-\nshop BlackboxNLP: Analyzing and Interpreting Neu-\nral Networks for NLP, pp. 353–355, Brussels, Bel-\ngium, November 2018. Association for Computa-\ntional Linguistics. doi: 10.18653/v1/W18-5446.\nURL https://aclanthology.org/W18-5446.\nWang, A., Pruksachatkun, Y ., Nangia, N., Singh, A.,\nMichael, J., Hill, F., Levy, O., and Bowman, S. R.\nSuperglue: A stickier benchmark for general-purpose\nlanguage understanding systems. In NeurIPS, 2019.\nWarstadt, A., Singh, A., and Bowman, S. R. Neural\nnetwork acceptability judgments. Transactions of the\nAssociation for Computational Linguistics , 7:625–\n641, 2019. doi: 10.1162/tacl_a_00290. URL https:\n//aclanthology.org/Q19-1040.\nWilliams, A., Nangia, N., and Bowman, S. A broad-\ncoverage challenge corpus for sentence understand-\ning through inference. In Proceedings of the 2018\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Hu-\nman Language Technologies, Volume 1 (Long Pa-\npers), pp. 1112–1122, New Orleans, Louisiana,\nJune 2018a. Association for Computational Linguis-\ntics. doi: 10.18653/v1/N18-1101. URL https:\n//aclanthology.org/N18-1101.\nWilliams, A., Nangia, N., and Bowman, S. A broad-\ncoverage challenge corpus for sentence understand-\ning through inference. In Proceedings of the Con-\nference of the North American Chapter of the As-\nsociation for Computational Linguistics: Human\nLanguage Technologies (NAACL-HLT). Association\nfor Computational Linguistics, 2018b. URL http:\n//aclweb.org/anthology/N18-1101.\nWortsman, M., Gururangan, S., Li, S., Farhadi, A.,\nSchmidt, L., Rabbat, M., and Morcos, A. S. lo-fi: dis-\ntributed fine-tuning without communication. arXiv\npreprint arXiv:2210.11948, 2022a.\nWortsman, M., Ilharco, G., Gadre, S. Y ., Roelofs, R.,\nGontijo-Lopes, R., Morcos, A. S., Namkoong, H.,\nFarhadi, A., Carmon, Y ., Kornblith, S., and Schmidt,\nL. Model soups: averaging weights of multiple fine-\ntuned models improves accuracy without increasing\ninference time. 2022b.\nYadav, P., Tam, D., Choshen, L., Raffel, C., and Bansal,\nM. Resolving interference when merging models.\nArXiv, abs/2306.01708, 2023. URL https://api.\nsemanticscholar.org/CorpusID:259064039.\nZampieri, M., Malmasi, S., Nakov, P., Rosenthal, S.,\nFarra, N., and Kumar, R. Predicting the Type and\nTarget of Offensive Posts in Social Media. In Pro-\nceedings of NAACL, 2019.\nZhang, X., Zhao, J., and LeCun, Y . Character-level con-\nvolutional networks for text classification. Advances\nin neural information processing systems, 28, 2015.\n1362\nA Dataset List\nMost datasets could be downloaded from huggingface datasets. We explicitly state the download link\nwhen relevant. As we used groups of datasets we report here the full list of datasets they contain.\nGeneral: CoLA (Warstadt et al., 2019), SST2 (Socher et al., 2013), MRPC (Dolan & Brockett, 2005),\nQQP (data.quora.com/First-Quora-Dataset-Release-Question-Pairs), MNLI (Williams et al.,\n2018a), QNLI Rajpurkar et al. 2016, RTE (Dagan et al., 2005; Bar-Haim et al., 2006; Giampiccolo\net al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011) BoolQ (Clark et al., 2019), CB (de\nMarneffe et al., 2019), CoPA (Roemmele et al., 2011), MULTIRC (Khashabi et al., 2018), WIC (Pilehvar\n& Camacho-Collados, 2019)\nNLI datasets: MNLI (Williams et al., 2018a), QNLI Rajpurkar et al. 2016, RTE (Dagan et al., 2005;\nBar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009), WNLI (Levesque et al., 2011),\nESNLI (Camburu et al., 2018), adversarial NLI (Nie et al., 2020).\nTwitter domain datasets (collected by TweetEval (Barbieri et al., 2020)) EmoInt (Mohammad & Bravo-\nMarquez, 2017), Emoji (Barbieri et al., 2018), Irony (Van Hee et al., 2018), OffenseEval (Zampieri et al.,\n2019), HatEval (Basile et al., 2019), Sentiment Analysis (Rosenthal et al., 2017)\nSentiment Analysis: Poem Sentiment (Sheng & Uthus, 2020), IMDB (Maas et al., 2011), Rotten\nTomatoes (Pang & Lee, 2005), SST 5bins (Socher et al., 2013), SST2 (Socher et al., 2013), Amazon\nreviews (He & McAuley, 2016) ,Financial Phrasebank (Malo et al., 2014)\nTopic Classification: AG news (Zhang et al., 2015), ISEAR (Scherer & Wallbott, 1994), Yahoo answers\n(Zhang et al., 2015), DBpedia (Zhang et al., 2015), 20 newsgroup (Zhang et al., 2015), TREC in both\nfine-grained and coarse-grained labels (Li & Roth, 2002)\nB Similarity Per Dataset, when Starting from different Pretrained Models\nAfter seeing in §2.3 the repeated behavior on several granularity levels, we were curious whether we\ncould receive the same behavior on a larger granularity level - models starting from different pretrained\nRoBERTa models, and finetuned on the same datasets. In this experiment, we employ two pretrained\nRoBERTa models, the original RoBERTa-base and the re-implementation of RoBERTa-base created by\nElazar et al. (2022). We finetune each one on the same datasets, from the General family. Results show\nthat the models get clustered according to the pretrained model they were created from, regardless to\nthe finetuning they went through. This might arise from the low distances moved from the initialization,\npretraining changes the model’s weights much more than finetuning. Therefore, since we start from\ndifferent pretrained models, the resulted finetuned models are more similar to the pretrained model they\nstarted from.\nAs the results on both pretrained models are comparable, we deduce that there is not one unique basin\nor region for each ability, but many. However, around a starting point it seems there are distinct regions\nwithin reach.\nC Cause: Data Type, not Size\nWe provide the clustering and visualize with t-SNE in Fig. 7. We see that the clustering and the data type\nagree in all but one of the cases.\nWe provide in Fig. 8 a detailed view of the similarities between each pair of models by dataset and\namount of data seen in training. We find that with relatively little data, the direction in space is already\ndetermined, i.e., similar datasets go to similar direction even with limited amount of data.\nD Similarity Per Task and Domain\nAs noted in 2.1, the datasets we use can be separated into specific four dataset families in addition to the\ngeneral group: NLI, Sentiment analysis, Topic, and Twitter. while the first three are characterized by their\ntask, the last group is characterized by the domain of the dataset it contained. As one can see in Fig. 9 and\n1 although the clustering shows good separation between task groups, it struggles to separate the Twitter\ndomain group models from the other groups. Separating the space into 4 clusters and labeling them in a\n1363\nFigure 7: Clusters of finetuned models on different datasets, with increasing train set sizes, projected by t-SNE.\nEach model is represented as a dot, where inner color is the color of the dataset the model was finetuned with, and\nouter color is the color of the most common dataset in the cluster (representing the cluster label). Datasets names\nare shown in legend.\nExperiment/class Twitter NLI Topic Sentiment Avg\nF1 Cluster Tasks and Domain 30 100 61 71 65\nF1 Cluster Tasks 100 87 83 90\nTable 1: F1 Score - Classification performance by cluster majority. In columns, model group names, in rows the\ntwo clustering settings, with and without the domain group (Twitter).\n1-to-1 mapping to maximize accuracy, we find 31 f-score on the Twitter cluster and 62,71,1 on the Topic,\nSentiment and NLI groups respectively.\nA possible explanation may be that the domain feature is orthogonal to the task feature, in the sense\nthat some datasets should be assigned to two groups at the same time (for example TweetEval Sentiment\nAnalysis (Rosenthal et al., 2017) is part of the Twitter domain group, as well as the Sentiment analysis\ntask group). This gives place to two competing hypotheses that we leave unanswered. Either the regions\nof domains overlap with regions of tasks; or, even if less likely, domains are not separated into regions in\nspace, unlike tasks.\nE Interpolation Between Models\nWe provide a more comprehensive interpolation experiment. In it we show the interpolation between\npairs of models and report the loss of each of the datasets used to create the pair of models, as well as the\naverage reported in the main paper.\nIn Fig. 10, one can see not only the interpolation between models in In, but interpolation between\nthe centroids. We take the average of all the models in one group from which we interpolate (e.g., all\nMNLI models) and set it as a centroid. We then repeat it on the other group and interpolate between those\ncentroids instead of interpolating between actual finetuned models. We find that although now we are\ninterpolating between two points that were both not the outcome of traditional ways of optimization, we\nfind comparable and often even lower losses than before. This also motivates the practical experiments\nreported in §7.\n1364\nFigure 8: Cosine similarity between models trained on different datasets, with varying data sizes (blocks). The\ndiagonal per block is blurred at the beginning of training, but with still a small amount of data models are highly\nsimilar to models trained on similar data. We do not observe similarity between models of similar size.\nF Loss Region Outside of Models in Other Directions\nAfter seeing that we can reach outside of regions by performing linear-extrapolation, we test the perfor-\nmance of models when we move away to different directions. To test it, we start with several models of\nthe same region, calculate their centroid by averaging their weights, and gradually move away from this\ncentroid according to the same procedure as in section 3.1. We move away from the centroid towards one\nof two directions: towards the origin of the axis, or towards random directions. We evaluate on the same\ndatasets the In models were finetuned on.\nFigure 11 shows the results for the first and third granularity levels.\nA detailed analysis for each level follows.\nOutside of the Dataset Region. We compare the performance of three types of models: finetuned\nmodels on MNLI, models moving from the centroid of MNLI models to the origin, and models moving\nfrom it to random directions.\nResults show that when the distance of the generated models from the centroid is similar to the distance\nof the finetuned models (radius ≤1), the generated models perform as well as the finetuned models,\nmeaning we are still inside the MNLI region and all models share the knowledge needed for the MNLI\ntarget task. It also implies the directions in which finetuned models vary are not special, most changes\naround the center are equally acceptable.\nWhen the distance increases and we move farther away from the centroid, the performance of the\nrandomly generated models decreases significantly, indicating the end of the region. A surprising finding\nis that this happens on random directions, but not when going towards the origin. The performance in that\ncase is similar to the performance of the finetuned models, even when the models are farther from the\ncentroid then the finetuned models. While we did not expect this phenomenon or have an explanation to it,\nwe report it as an avenue for future work.\nOutside of the Finetuning Region. We compare the performance of three types of models: finetuned\nmodels on datasets from the General family, models starting from the centroid of those models towards\nthe origin or towards random directions. Each time, we evaluate all above models on a single dataset\nfrom the General family, separating the performance of the model finetuned on the target dataset (called\nsource model), to the rest of finetuned models (called non-source models), resulting in total of four types\nof models in the comparison, including the two types of generated models starting from the centroid.\n1365\nt-SNE x\nt-SNE y\nTWEET\nNLI\nTOPIC\nSENT\nFigure 9: Clusters of finetuned models, trained on datasets groups, distinct by task and domain. The models\nprojected by t-SNE, where each model is represented as a dot, where the inner color is the color of the task/domain\nthe model was finetuned with and the outer color is the color of the most common task/domain in the cluster\n(representing the cluster label). We find that tasks are can be easily distinguished, while it is hard to separate Twitter\ndomain models.\nWe average the performance of each type on all target datasets we evaluate on, and show the results in\nFigure 11(b). We can see that the source model outperforms all other models. For small distances from the\ncentroid, the non-source models underperform the generated models, and for large distances it outperform\nthe generated models going towards random directions. The generated models going towards the origin\noutperform the two above types of models, for all distances. These results suggest that when staying\nclose enough to the centroid, roaming from the centroid to different directions might be superior to a\nfinetuned model on a different dataset. However, when distancing far from the centroid, finetuned models\non other datasets then the target dataset perform better than generated models going towards random\ndirections, since the last are probably outside of the region. Worth noticing, the standard deviation of the\nlast is meaningfully larger than the rest of the models, and of the one of generated models in the Dataset\ngranularity level.\nG Extrapolation Between Models\nFig. 12 presents the same behaviour for all three granularity levels- extrapolation rapidly reaches bad\nperformances.\nWe provide a more comprehensive extrapolation experiment showing each time the extrapolation with\nthe loss of each of the datasets used to create the pair of models, and the average reported in the main\npaper. We find (see Fig. 13(b)) that despite all of our datasets called and framed as natural language\ninference, WNLI (Levesque et al., 2011) behaves differently and might be considered not strictly a part of\nthe region. This may also explain the long tail in Fig. 4(b).\nH Efficient Finetuning\nWe provide in this section the full results of efficiently finetuning. We provide the full results of the regular\nfinetuning in Table 2 and the few-shot setting in Table 3 and Fig. 14.\n1366\n(a) Interpolation Per Dataset\nFigure 10: Losses of linearly interpolated models created between pairs of similar models. In each figure, the solid\nline is the losses’ average during interpolations for different αvalues, the edges of the lines represent the pure\nfinetuned models we interpolated, Y axis is the average loss value, X axis is the position determined by α, N is\nthe number of pairs we interpolated between. The minimum average loss during the interpolation is noted and the\nshade is the standard deviation of the losses average. The purple line provides the average loss of the interpolation\nbetween centroids of models.\ndataset name boolq cb cola mnli mrpc multirc qnli qqp rte sst2 wic wnli mean\nPretrain 62.17 50.36 69.13 53.17 68.38 57.20 64.88 74.49 50.40 78.78 55.14 54.08 61.51\nFuse 62.23 56.79 69.49 63.01 69.46 57.14 73.77 79.91 61.59 84.91 55.52 52.68 65.54\nGain 0.06 6.43 0.36 9.85 1.08 -0.06 8.89 5.42 11.19 6.12 0.38 -1.41 4.03\nTable 2: Gains of efficient finetuning starting from the centroid or the pretrained model. In columns names of\ndatasets (mean is their average) and in rows the choice of base model and their difference, the gain.\n(a) Outside of Dataset Region\n (b) Outside of Finetuning Region\nFigure 11: Performance of the finetuned and the generated models from the centroid to the origin and to random\ndirections, with respect to the distance from the region. In each graph, Y axis is the accuracy, X axis is the radius\n(which is the αvalues used for generating the models. Only relevant for the constant lines), the solid lines present\nthe average accuracy of the generated models, the dashed lines present the average accuracy of the finetuned models\n(a constant value), and the shade is the standard deviation of the accuracies average. Models’ groups in legend.\n1367\n(a) Extrapolation per dataset.\n (b) Extrapolation per task.\n (c) Extrapolation in General.\nFigure 12: Losses of linearly extrapolated models created from pairs of similar models. In each figure, the solid\nline is the average losses during extrapolations for different αvalues, the vertical dashed lines indicate the average\nloss of the pure models we extrapolate (α= 0or α= 1), the Y axis is the average loss value, and the X axis is the\nposition (meaning the αand (1 −α) values used in the extrapolation). The shade is the standard deviation of the\nlosses’ average across runs.\ndataset name boolq cb cola mnli mrpc multirc qnli qqp rte sst2 wic wnli mean\nPretrain 62.17 50.36 69.13 34.04 68.38 57.20 50.72 63.18 48.52 50.92 49.91 54.08 54.88\nFuse 62.23 56.79 69.49 63.01 69.46 57.14 73.77 79.91 61.59 84.91 55.52 52.68 65.54\nGain 0.06 6.43 0.36 28.97 1.08 -0.06 23.04 16.74 13.07 33.99 5.61 -1.41 10.66\nTable 3: Gains of efficient finetuning with up to 1K examples, starting from the centroid or the pretrained model. In\ncolumns names of datasets (mean is their average) and in rows the choice of base model and their difference, the\ngain.\n1368\n(a) Extrapolation Per Task and mixed\n(b) Extrapolation Per Domain\nFigure 13: Losses of linearly extrapolation models created between pairs of similar models. In each figure, the solid\nline is the average losses during extrapolations for different αvalues, the vertical dashed lines indicate the average\nloss of the pure models we extrapolate (α= 0or α= 1), Y axis is the average loss value, X axis is the position\n(meaning the αand (1 −α) values used in the extrapolation), N is the number of pairs we extrapolated between, the\nvalues on top of the line are the loss at the edges and at the minimum average loss during the extrapolation, and the\nshade is the standard deviation of the losses average. Each Column represents extrapolation between different types\nof models and each row evaluates those same models and their extrapolations on a different target tasks.\n1369\n0 5 10 15 20 25 30 35\nAccuracy Difference\nWNLI\nMultiRC\nBoolQ\nCoLA\nMRPC\nWIC\nCB\nRTE\nQQP\nQNLI\nMNLI\nSST2\nFigure 14: Losses of pretrained and centroid models on several target datasets, where both models were efficiently\nfinetuned using BitFit in a few-shot scenario limiting training data to 1K.\n1370",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.9050071835517883
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.8445073366165161
    },
    {
      "name": "Language model",
      "score": 0.5526354312896729
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5430611371994019
    },
    {
      "name": "Traverse",
      "score": 0.5411095023155212
    },
    {
      "name": "Space (punctuation)",
      "score": 0.4917815029621124
    },
    {
      "name": "Task (project management)",
      "score": 0.4594792425632477
    },
    {
      "name": "Machine learning",
      "score": 0.41496020555496216
    },
    {
      "name": "Theoretical computer science",
      "score": 0.3261567950248718
    },
    {
      "name": "Geodesy",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Management",
      "score": 0.0
    },
    {
      "name": "Geography",
      "score": 0.0
    }
  ]
}