{
  "title": "Masked Vision-language Transformer in Fashion",
  "url": "https://openalex.org/W4322207844",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A4220442372",
      "name": "Ge-Peng Ji",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3120004174",
      "name": "Mingchen Zhuge",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2125701661",
      "name": "Dehong Gao",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2242721764",
      "name": "Deng-Ping Fan",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2745932223",
      "name": "Christos Sakaridis",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A2103920730",
      "name": "Luc Van Gool",
      "affiliations": [
        "ETH Zurich"
      ]
    },
    {
      "id": "https://openalex.org/A4220442372",
      "name": "Ge-Peng Ji",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A3120004174",
      "name": "Mingchen Zhuge",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2125701661",
      "name": "Dehong Gao",
      "affiliations": [
        "Alibaba Group (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2242721764",
      "name": "Deng-Ping Fan",
      "affiliations": [
        "Vision Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2745932223",
      "name": "Christos Sakaridis",
      "affiliations": [
        "Vision Australia"
      ]
    },
    {
      "id": "https://openalex.org/A2103920730",
      "name": "Luc Van Gool",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3138516171",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W3202031169",
    "https://openalex.org/W6779879114",
    "https://openalex.org/W3170767867",
    "https://openalex.org/W6790978476",
    "https://openalex.org/W3172514680",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2966715458",
    "https://openalex.org/W3090449556",
    "https://openalex.org/W2996545740",
    "https://openalex.org/W2795117763",
    "https://openalex.org/W3035485997",
    "https://openalex.org/W3165938948",
    "https://openalex.org/W3131500599",
    "https://openalex.org/W3107492437",
    "https://openalex.org/W3035726724",
    "https://openalex.org/W2970231061",
    "https://openalex.org/W2969876226",
    "https://openalex.org/W2962964995",
    "https://openalex.org/W2778940641",
    "https://openalex.org/W3091588028",
    "https://openalex.org/W3122006940",
    "https://openalex.org/W3202461379",
    "https://openalex.org/W6796761347",
    "https://openalex.org/W4313156423",
    "https://openalex.org/W3176362845",
    "https://openalex.org/W3126792443",
    "https://openalex.org/W3184784418",
    "https://openalex.org/W4312940158",
    "https://openalex.org/W3168640669",
    "https://openalex.org/W3171915994",
    "https://openalex.org/W3154596443",
    "https://openalex.org/W2972801466",
    "https://openalex.org/W1901129140",
    "https://openalex.org/W2970869018",
    "https://openalex.org/W2774267535",
    "https://openalex.org/W2965848243",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W1861492603",
    "https://openalex.org/W2998356391",
    "https://openalex.org/W4285606598",
    "https://openalex.org/W4304080188",
    "https://openalex.org/W4220834633",
    "https://openalex.org/W4210593721"
  ],
  "abstract": null,
  "full_text": " \nMasked Vision-language Transformer in Fashion\nGe-Peng Ji 1†          Mingchen Zhuge 1†          Dehong Gao 1          Deng-Ping Fan 2*\nChristos Sakaridis 2          Luc Van Gool 2\n1 International Core Business Unit, Alibaba Group, Hangzhou 310051, China\n2 Computer Vision Lab, ETH Zürich, Zürich 8092, Switzerland\n \nAbstract:   We present a masked vision-language transformer (MVLT) for fashion-specific multi-modal representation. Technically, we\nsimply utilize the vision transformer architecture for replacing the bidirectional encoder representations from Transformers (BERT) in\nthe pre-training model, making MVLT the first end-to-end framework for the fashion domain. Besides, we designed masked image recon-\nstruction (MIR) for a fine-grained understanding of fashion. MVLT is an extensible and convenient architecture that admits raw multi-\nmodal inputs without extra pre-processing models (e.g., ResNet), implicitly modeling the vision-language alignments. More importantly,\nMVLT can easily generalize to various matching and generative tasks. Experimental results show obvious improvements in retrieval\n(rank@5: 17%) and recognition (accuracy: 3%) tasks over the Fashion-Gen 2018 winner, Kaleido-BERT. The code is available at\nhttps://github.com/GewelsJI/MVLT.\nKeywords:   Vision-language, masked image reconstruction, transformer, fashion, e-commercial.\nCitation:   G. P. Ji, M. Zhuge, D. Gao, D. P. Fan, C. Sakaridis, L. V. Gool. Masked vision-language transformer in fashion. Machine\nIntelligence Research, vol.20, no.3, pp.421–434, 2023. http://doi.org/10.1007/s11633-022-1394-4\n \n 1   Introduction\nThe emergence of transformer is drawing enormous at-\ntention from the academic community, facilitating the ad-\nvancement of computer vision (CV)[1, 2] and natural lan-\nguage processing (NLP)[3, 4]. Benefiting from the robust-\nness  of  transformers,  researchers  also  contribute  to  the\nvision-language (VL) field[5−9] with zeal. To better utilize\nthe pre-trained models in CV and NLP, existing general\nVL  models  are  mainly  based  on  the  BERT  model[10] or\nadopt the well-pretrained vision extractors[11, 12] or both.\nHowever, general VL methods[13−15] still struggle when ap-\nplied to the fashion domain in e-commerce because they\nsuffer from two main issues: 1) Insufficient Granular-\nity. Unlike  the  general  objects  with  complex  back-\ngrounds, only focusing on coarse-grained semantics is in-\nsufficient for a fashion product[16−18], as it would lead the\nnetwork  to  generate  sub-optimal  results.  Contrarily,  the\nfashion-oriented  framework  requires  more  fine-grained\nrepresentations,  such  as  a  suit  with  different  materials\n(e.g., wool, linen, and cotton) or collars (e.g., band, camp,\nand Windsor). 2) Bad Transferability. The pre-extrac-\nted visual features are not discriminative for fashion-ori-\nented tasks, restricting the cross-modal representations.\nTo  address  the  above  issues,  we  present  a  novel  VL\nframework  termed  masked  vision-language  transformer\n(MVLT).  Specifically,  we  introduce  a  generative  task,\nmasked  image  reconstruction  (MIR),  for  the  fashion-\nbased VL framework. Compared to previous pre-training\ntasks, such as masked image modeling (regression task) or\nmasked image classification (classification task), MIR en-\nables the network to learn more fine-grained representa-\ntions via pixel-level visual knowledge (see Fig. 1). Further,\ninspired by pyramid vision transformer (PVT)[21], we util-\nize a pyramid architecture for our VL transformer. Then,\nwe introduce the MIR task. These two improvements sig-\nnificantly enhance the ability to adapt to fashion-specific\nunderstanding  and  generative  tasks  and  can  conduct  in\nan end-to-end manner. To this end, MVLT can directly\nprocess the raw multi-modal inputs in dense formats (i.e.,\nlinguistic tokens and visual patches) without extra (e.g.,\nResNet) pre-processing models[22, 23]. Our main contribu-\ntions are summarized as follows:\n1) We introduce a novel MIR task, which is the first\nreal  pixel-level  generative  strategy  utilized  in  VL  pre-\ntraining.\n2) Based on the MIR task, we present an end-to-end\nVL  framework,  called  MVLT,  for  the  fashion  domain,\ngreatly promoting the transferability to the downstream\ntasks and large-scale web applications.\n3)  Extensive  experiments  show  that  MVLT  signific-\nantly  outperforms  the  state-of-the-art  models  in  match-\ning and generative tasks.\n \nResearch Article\nManuscript  received  on  May  24,  2022;  accepted  on  October  14,\n2022; published online on February 27, 2023\nRecommended by Associate Editor Chun-Hua Shen\n \nColored figures are available in the online version at https://link.\nspringer.com/journal/11633\n*Corresponding author. Work was done while Ge-Peng Ji was a\nresearch intern at Alibaba Group.\n†These aughors contribute equally to this paper\n©The Author(s) 2023\n \nMachine \nIntelligence Research\nwww.mi-research.net\n20(3), June 2023, 421-434\nDOI: 10.1007/s11633-022-1394-4\n \n\n 2   Background\nIn recent years, BERT-based pre-training models have\nbeen widely investigated in VL tasks. Many previous at-\ntempts, such as LXMERT[24], VL-BERT[25], and Fashion-\nBERT\n[19], were successful in a wide range of downstream\napplications.  Experiments  and  discussions  show  that\nBERT is a powerful method for learning multi-modal rep-\nresentations,  outperforming  several  previous  CNN-\nbased\n[26] or  LSTM-based[27, 28] approaches.  Compared  to\nprevious studies, this paper aims to develop a more effi-\ncient  self-supervised  objective  that  can  be  easily  imple-\nmented  in  pre-training  and  provides  better  representa-\ntions  for  real-world  applications.  Thus,  we  review  re-\nsearch  on  masked  learning  strategies  and  end-to-end\nmulti-modal schemes that inspired us the most.\n \n2.1   Masked learning strategies\nMasked  modeling  is  a  vital  self-supervised  task  in\nBERT[10] and initially demonstrates outstanding abilities\nin  natural  language  processing.  Researchers  have  replic-\nated its strength in language models because of its utility\nin multi-modal and vision tasks. Most VL works[14, 25\n, 29]\ntransfer masked modeling into visual tokens and use a re-\ngression  task  to  construct  the  token  feature  from  non-\nsense-replace or a classification task to predict the token's\nattribute.  To  reduce  the  difficulty  in  learning,  Kaleido-\nBERT\n[2] optimizes masked modeling by employing a Kal-\neido strategy that facilitates coherent learning for multi-\ngrained semantics. Although this work improves the \nper-\nformance of VL-related tasks in fashion indeed, we argue\nthat  the  token-patch  pre-alignment  scheme  by  using  an\nauxiliary  tool[30, 31] is  still  complex  and  impedes  the  ap-\nplication  to  practical  settings.  Another  work\n[32] intro-\nduces the masked language and image modeling (MLIM)\napproach  that  strengthens  masked  image  modeling  with\nan image reconstruction task, which shares a similar idea\nto ours. However, our experiments showed that requiring\na model to reconstruct the entire image without any \nre-\nminder  is  too  difficult.  Recently,  BEiT[33] and  MAE[34]\nutilized  a  BERT-style  pre-training  as  part  of  the  visual\nlearner, and they discovered that models are effective at\nlearning semantics with such a scheme. These two works\nstrengthen  our  conviction  that  converting  the  original\nmasked  image  modeling  (i.e.,  a  regression  task)  to  a\nmasked  image  reconstruction  task  is  possible.  However,\nour  primary  goal  is  to  design  a  generative  pretext  task\nthat makes the multi-modal modeling in VL pre-training\neasier  while  eliminating  the  need  for  using  prior  know-\nledge. It will be extremely helpful in our practical applic-\nation setting with billion-level data.\n \n2.2   End-to-end multi-modal schemes\nPixel-BERT[35] is the first method to consider end-to-\nend  pre-training.  It  employs  2×2  max-pooling  layers  to\nreduce the spatial dimension of image features, with each\nimage being downsampled 64 times. Although this work\nsets  a  precedent  for  end-to-end  training,  such  a  coarse\nand rigid method cannot work well in practical settings\nbecause it is simply combined with a ResNet[11] as part of\njoint  pre-training  without  considering  the  loss  in  speed\nand  performance.  Recently,  VX2TEXT[36] proposed  to\nconvert all modalities into a language space and perform\nend-to-end  pre-training  using  a  relaxation  scheme.\nThough it is exciting to translate all the modalities into a\nunified latent space, it ignores that the usage of data \nex-\ntracted  by  pre-trained  methods  as  input  to  the  model\ncannot be regarded as an end-to-end framework. Accord-\ning to the timeline, ViLT [37] is the first method that in-\ndeed investigates an end-to-end framework via replacing\nregion-based or grid-based features with patch-based pro-\njections.  However,  without  other  designs,  it  cannot  ob-\ntain competitive performance since it is just a vanilla ex-\ntension of ViT[1]. Grid-VLP[38] is similar to ViLT, but it\ntakes a further step by demonstrating that using a pre-\ntrained  CNN  network  as  the  visual  backbone  can  im-\nprove performance on downstream tasks. SOHO\n[39] takes\nthe entire image as input and creates a visual dictionary\nto affine the local region. However, this method does not\nfit fashion-specific applications due to the lack of reliable\nalignment information. As a result, the vision dictionary\nmay merely learn the location of the background or fore-\nground  rather  than  complex  semantics.  FashionVLP[40]\nuses a feedback strategy to achieve better retrieval per-\nformance. In practice, they use the well-pretrained know-\nledge extracted from ResNet and then model the whole,\ncropped, and landmark representations. Besides, they ad-\nopt Faster-RCNN as an object detector for popping out\nRoI candidates. Besides, some works are designed for end-\nto-end  pre-training[41−43],  but  they  are  used  for  specific\ntasks and are not directly applicable to our research.\nDespite  existing  methods  employing  different  ap-\n \nImage patches\nImage patches\nMasked image modeling1\n1\n2\n3\nMasked image reconstruction (Ours)\n2 048-dim prediction\n{0.8, 0.3, …, 0.2}\nResNet {0.9, 0.1, …, 0.2} {M, M, …, M}\n2 048-dim\nM padding\n2 048-dim\nfeature\nM-ViLT\nGenerative\npatch\nMasked\npatch\nBERT\n \nM\nFig. 1     Different  visual  reconstruction  tasks  for  VL\npretraining[19, 20] utilize masked image modeling (top) with the\nrandom masking strategy (i.e., to use    padding to replace raw\nvectors), which reconstructs pre-extracted visual semantics (i.e.,\nprobabilities)  at  the  feature-level.  We  introduce  a  generative\ntask  named  masked  image  reconstruction  (bottom),  which\ndirectly reconstructs image patches at the pixel level.\n \n 422 Machine Intelligence Research 20(3), June 2023\n \n\nproaches  to  construct  an  end-to-end  scheme,  solutions\nthat forgo pre-trained methods (e.g., ResNet, BERT) and\nuse raw data (i.e., text, image) as inputs remain under-\nexplored and are needed urgently in multi-modal applica-\ntions.\nRemark.\n As shown in Fig. 2, similar to the existing\ntwo fashion-based approaches, i.e., FashionBERT (a) and\nKaleido-BERT  (b),  the  proposed  MVLT  (c)  is  also  a\npatch-based VL learner, which extends the pyramid vis-\nion transformer\n[21] to an architecture that adaptively ex-\ntracts hierarchical representations for fashion cross-mod-\nal tasks. It is the first model that solves the end-to-end\nproblem of VL pre-training in fashion, which allows us to\nsimplify the implementation of our MVLT in the fashion\nindustry using a twin-tower architecture\n[44].\n 3   Masked vision-language transformer\nOur goal is to build an end-to-end VL framework for\nthe fashion domain. The overall pipeline of our MVLT is\ndepicted  in Fig. 3.  Like  PVT,  our  architecture  inherits\nfour  stages' properties  and  generates  different-sized  fea-\ntures.  Two  keys  of  the  proposed  architecture  are  the\nmulti-modal  encoder  (Section.  3.1)  and  the  pre-training\nobjectives (Section. 3.2).\n \n3.1   Multi-modal encoder\n[MASK]\nAs shown in  Fig. 3, MVLT accepts visual and verbal\ninputs. On the language side, we first tokenize the cap-\ntion of a fashion product and use the specific token \n \nImage patches\nMulti-modal BERT\nEmbedding ResNet\nFrozen\n(a) FashionBERT\nLanguage\ntask VL task\nVision task\n(regression)\nText tokens\nMulti-modal BERT\nEmbedding\nText tokens Kaleido patches\nResNet\nFrozen\n(b) Kaleido-BERT\nLanguage\ntask VL task\nKaleido\nvision task\n(regression)\nMulti-modal PVT\nEmbedding\nText tokens Image patches\nEmbedding\n(c) M-ViLT (ours)\nLanguage\ntask VL task\nMasked\nvision task\n(reconstruction)\nFig. 2     Comparison of MVLT to cutting-edge fashion-oriented VL frameworks. FashionBERT (a) utilizes a language-based encoder\n(i.e., BERT) to extract VL representations with single-scale visual input (i.e., image patches). Kaleido-BERT (b) extends it with two\nupgrades: adds five fixed-scale inputs (i.e., Kaleido patches) to acquire hierarchical visual features and designs Kaleido vision tasks to\nfully learn VL representations. However, the visual embedding of these models is frozen (i.e., without parameter updating); thus, a lack\nof domain-specific visual knowledge severely hinders their transferability. Differently, our MVLT (c) adaptively learns hierarchical\nfeatures by introducing masked vision tasks in an end-to-end framework, significantly boosting the VL-related understanding and\ngeneration.\n \n \nVL transformer encoder\nSpatial embed\nConv2D\nNormFlatten\nNorm\nNorm\nLinear\nNorm\nLinear embed\nReduce\nMulti-head\nattention\nFeed forward Reshape\nVisual\nembedding\nLanguage\nembedding\nDivide Visual\nembedding\nLanguage\nembedding\nPosition\nembedding\nV/L feature\nC\nConcat\nAdd\nSRA\nC\nMLM\nITM\nMIR\n[MASK] Sleeveless\n[MASK] Dress\nWomen′s sleeveless\nlong dress\nStage 2\nStage 1\nStage 3\nStage 4\nD\nD\nI (H×W×3)\nStage k\nT\nT 1(L×64) T 2(L×128) T 3(L×320) T 4(L×512)\nV\nT k\nV k\nmk\n×Mk\nzk\nzk+1\nmk+1\nT k+1\nV k+1\nnk+1\nnk\nP k\nv\nP k\nt\nV 1( H × W × 64)4 4 V 2( H × W × 128)88 V 3( H × W × 320)16 16 V 4( H × W × 512)32 32\n\u0002Mk\nFig. 3     Pipeline of our MVLT framework. Our overall architecture consists of four stages containing language and visual embeddings\nand multiple transformer encoders ( ).  Introducing the masking strategy for three sub-tasks, i.e., masked image reconstruction\n(MIR), image-text matching (ITM), and masked language modeling (MLM), our MVLT can be trained in an end-to-end manner. More\ndetails can be found in Section 3.\n \nG. P. Ji et al. / Masked Vision-language Transformer in Fashion 423 \n \n\nrl\n[CLS]\n[PAD]\nT 2 RL = ⟨t1; \u0001\n\u0001 \u0001 ; tL⟩\nI 2 RH\u0002W\u00023\nH\nW\nV 2\nRN\u0002P\u0002P\u00023 = ⟨v1; \u0001\n\u0001 \u0001 ; vN⟩\nN = H\nW\nP2\nP\nrv\nto randomly mask out the caption tokens with the mask-\ning ratio1 .  Following the masking procedure, we obtain\na sequence of word tokens. Then, we insert a specific \ntoken at the head of this sequence. Besides, we pad the\nsequence to a unified length L using the  token if the\nlength is shorter than 128. This procedure generates the\nlanguage  input  ids .\n  On  the  vision\nside, we treat  as visual input, where  and\n denote the height and width of the given input. This\ninput  is  sliced  into  multiple  grid-like  patches \n,\n  where  is  the  total\nnumber of patches, and  denotes the patch size. Simil-\narly, the split patches are masked out with mask ratio .\nWe  provide  more  details  about  the  above  masking\nstrategy for the language and vision parts in Section 3.2.\nk 2\nf1; 2; 3; 4g\nT1\nV 1\nT\nV\nk\nTk 2 RL\u0002Dk\nmk 2 RL\u0002Dk+1\nThe above multi-modal inputs are embedded and fed\ninto  the  consequent  four  VL  interaction  stages  (i.e.,\n).  In the first stage, we generate the vision\nand  language  embeddings,  and ,  respectively,  via\nthe  given  inputs  (  and ).  Regarding  the  subsequent\nstages, we consider only the -th  stage to have concise il-\nlustrations.  As  shown  in  the  bottom  part  of Fig. 3,  we\nfirst  embed  the  language  embedding  into\nthe language hidden feature ,  which is for-\nmulated as\nmk = Tk \u0003 Wk\nt + Pk\nt (1)\nWk\nt 2 RDk\u0002Dk+1\nPk\nt 2 RL\u0002Dk+1\nDk\n\u0003\nwhere  and  are  the\nlearnable  linear  embedding  and  position  embedding\nmatrices,  \n is the size of the hidden feature embedding,\n denotes the matrix multiplication.\nV k 2 R\nH\nRk\n\u0002 W\nRk\n\u0002Dk\nRk\nV k\nnk 2 R(H\nW/R2\nk+1)\u0002Dk+1\nRH\nW/R2\nk\nRH\nW/R2\nk+1\nWk\nv 2 RDk\u0002Kk\u0002Kk\u0002Dk+1\nKk\nSk\nThe visual embeddings are ,  where\n denotes the spatial reduction factor of visual embed-\nding. To acquire pyramid visual features,  \n is then em-\nbedded  and  flattened  into  the  visual  hidden  feature\n via  a  two-dimensional  projection\n(i.e.,  Conv2D  block).  In  particular,  this  projection  en-\nforces  the  network  to  reduce  the  equivalent  spatial  di-\nmension  from  \n to  by  utilizing  the\nconvolutional  kernel  with  ker-\nnel  size  and  stride  length .  This  could  be  formu-\nlated as follows:\nnk = F\nlatten(V k \u0003 Wk\nv) + Pk\nv (2)\nPk\nv 2 RN\u0002Dk+1\n\u0003\nzk = ⟨mk; nk⟩\nMk\nzk+1 =\n⟨mk+1; nk+1⟩\nTk+1 =\nwhere  denotes  the  position  embedding\nmatrix,  denotes  the  matrix  multiplication.  We  then\nconcatenate these two VL hidden features \nand  feed  them  into  multiple  ( )  VL  transformer\nencoders.  Each  encoder  contains  the  multi-head  self-\nattention layer with spatial reduction (i.e., reduce block),\nmulti-layer perceptron, and layer normalization. Finally,\nwe  obtain  the  encoded  multi-modal  feature \n \n and divide it into a language part \nmk+1\nV k+1 =\nR\neshape(nk+1)\nR\neshape(\u0001)\n and  a  visual  part ,  where\nthe  operation  consists  in  recovering  the\nspatial dimension of the given feature.\nfTkg4\nk=1\nfV kg4\nk=1\nAfter four VL interaction stages, we generate the four\ntext  embeddings  and  four  pyramid  vision  em-\nbeddings ,  respectively. Table  1 presents  more\ndetailed hyperparameter settings of our method.\n \n \nTable 1    Hyperparameter of our multi-modal encoders\nHyperparameter\nk =\n1\nk =\n2\nk =\n3\nk =\n4\nMk\nLayer number  2 2 2 2\nDk\nHidden size  64 128 320 512\nRk\nReduction size \n4\n8\n16\n32\nKk\nKernel size  4 2 2 2\nSk\nStride length  4 2 2 2\n 3.2   Pre-training objectives\nTo acquire discriminative multi-modal representations,\nwe adopt three pre-training tasks to establish the inter-\nand  intra-relationships  between  the  most  primitive  VL\nmodalities,  including  vision  (masked  image  reconstruc-\ntion,  MIR),  language  (i.e.,  masked  language  modeling,\nMLM), and VL (image-text matching, ITM) modalities.\nV =\nfvngN\nn=1 2 RN\u0002P\u0002P\u00023\nVn\b\nObjective  1:  Masked  image  reconstruction\n(MIR). As for the general domain, models are enough to\nlearn the coarse-grained semantics from the patch-based\nor  region-based  objectives  and  achieve  satisfactory  res-\nults.  However,  the  fashion-specific  models  require  more\nfine-grained representations, such as a suit with different\nmaterials  (e.g.,  wool)  or  collars  (e.g.,  Windsor),  which\nneeds  a  pixel-to-pixel  vision  pre-training  objective.  In-\nspired  by  masked  language  modeling[10],  we  attempt  to\nbuild  pixel-to-pixel  relationships  from  the  perspective  of\ngenerative tasks, which promote the scalability of visual\nrepresentations.  We  designed  the  masked  image  recon-\nstruction to accomplish this idea. To help our model learn\nbetter  by  MIR,  we  utilize  the  pyramid  characteristic  of\nthe  PVT  architecture[21] to  design  a  flexible  masking\nstrategy.  Unlike  the  ViT-based  method  (a)  in Fig. 4,\nPVT-based  architecture  (b)  masks  out  the  input  image\naccording to the masking unit matrix that contains small-\ngrained  patches.  Given  the  patch  sequence \n,\n  the  masked-out  sequence  is\ndefined as\nVn\b = FM(fM(q; \u000b;\n\b)gQ\nq ; fvngN\nn=1) =\n{[ZERO]; if M(q; \u000b; \b) = 1\nvn; if M(q; \u000b; \b) = 0 (3)\nFM(\u0001; \u0001)\nq\n[ZERO]\nwhere  represents a function (or procedure) of our\nmasking strategy,  is the randomly selected area of the\nmasking unit, and  means that we use a pixel value1 We follow the default setting in BERT[10].\n 424 Machine Intelligence Research 20(3), June 2023\n \n\nfM(q; \u000b;\n\b)gQ\nq=1\nof  zero2 to  fill  the  selected  areas.  The  masking  units\n are derived from the indicator function:\nM(q; \u000b;\n\b) = 1(q) =\n{\n1; if q 2 \b\n0; if q /2 \b\n(4)\n\b\n[1;\nQ]\nrv\nQ = H\u0002W\n(\u000b\u0002P)2\n\u000b\n\u000b =\n4\nwhere  each  value  in  a  set  of  integers  is  randomly\nselected from range  with ratio .   is the\ntotal number of masking units. For instance, in Fig. 4(b),\nwe can define   from 1 to 8. In our default settings, we\nset  to capture more fine-grained semantics3.\nℓ1\nVn\b\nSince  the  smooth-  loss  is  less  sensitive  to  the  out-\nliers,  we  use  it  as  the  pre-training  objective  to  recon-\nstruct the whole image via the masked-out sequence .\nIt is defined as\nLMIR =\n{\n0:5 \u0002 (I′\n(x;y) \u0000 I(x;y))2; if I′\n(x;y) \u0000 I(x;y) < 1\nj I′\n(x;y) \u0000 I(x;y) j\n\u00000:5; otherwise\n(5)\nI′\n(x;y)\nI(x;y)\n(x;\ny)\nI′\nI\nI′ = FMIR (Vn\b; WMIR )\nWMIR\nFMIR (\u0001; WMIR )\nfV kg4\nk=1\nwhere  and  denote  the  pixel  at  coordinate\n in the reconstructed image  and the input image\n,  respectively.  is  parameterized\nby  learnable  weights .  Function \ndenotes  a  standard  four-level  U-Net[45] decoder,  which\nadmits  four  pyramidal  vision  embeddings  as\ninputs.\nT4\nFITM (\u0001; WITM )\nWITM\nFITM\npITM = FITM (⟨T; V ⟩; WITM )\nObjective  2:  Image-text  matching  (ITM). The\nappended  classification  embedding  in  the  last  language\nembedding, , is used to couple the representations from\nVL modalities. We utilize the function  to\ndenote a fully connected (FC) and softmax layers, para-\nmeterized  by  the  weights .\n  outputs  a  two-\nclass  probability  vector ,\nrepresenting  whether  the  input  fashion  image  and  cap-\ntion match (i.e., positive pair) or not (i.e., negative pair).\nThe  positive  pairs  are  selected  from  the  same  fashion\nproduct category, whereas the negative pairs are chosen\nrandomly  from  different  entries.  The  binary  cross-en-\ntropy loss function finally constrains this task:\nLITM = \u0000 E⟨T;V ⟩[yITM log(pITM )+\n(1 \u0000 yITM ) log(1 \u0000 pITM )] (6)\nyITM\n1\n0\nwhere  denotes  the  ground-truth  label,  i.e.,  for\nmatched pairs and  for unmatched pairs.\n[MASK]\nT = ft1; \u0001\n\u0001 \u0001 ; tLg\nTni = ft1; \u0001\n\u0001 \u0001 ; [MASK]i; \u0001 \u0001 \u0001 ;\ntLg\nObjective  3:  Masked  language  modeling\n(MLM). Following  [46],  we  randomly  use  the  specific\ntoken  to replace the original text tokens. The tar-\nget  of  the  MLM  is  to  predict  the  text  content  for  the\nmasked  tokens  using  the  unmasked  tokens  and  patches.\nGiven a tokenized sequence , the masked-\nout  sequence  is  denoted  by \n. We use the cross-entropy loss to model this object-\nive:\nLMLM = \u0000ET [log(pMLM )] (7)\npMLM = FMLM (Tni; WMLM )\n[MASK]i\nTni\nFMLM (\u0001; WMLM )\nWMLM\nwhere  denotes  the  predicted\nprobability  for  each  masked-out  token  using .\nThe function  represents the parameters\n of a classifier. The final pre-training objective of\nthe  proposed  MVLT  is  a  combination  of  the  three\nobjectives:\nLtotal = w1 \u0002\nLMIR + w2 \u0002 LITM + w3 \u0002 LMLM : (8)\n 3.3   Downstream tasks\n1 \u0000 6\nFor a fair comparison, we follow the same training/in-\nference  protocols  as  in  [19, 20]  and  adopt  the  Fashion-\nGen 2018[47] benchmark as the base of our experiments.\nThis dataset contains 67 666 fashion products (i.e., 60 147\nentries for training and 7 519 entries for testing) and their\nassociated  product  descriptions.  Each  product  corres-\nponds to an image set (including  samples) at vari-\nous  viewing  angles.  As  a  result,  we  utilize  260 480  and\n35\n 528 image-text pairs as training and testing partitions,\nrespectively. For a fair comparison, we tested MVLT and\ncompared models on Fashion-Gen using the following four\nfashion-related VL downstream tasks.\n \nSplit patches\nSplit patch\nα=2\nα=3\nα=4\nα×P\nα×P\n(a) ViT-based\nP\nP\nP\nP P\nP\n(b) PVT-based (ours)\nMasking\nunit\n \nP2\n(\u000b \u0002 P)2\n\u000b 2\nf1; 2; \u0001 \u0001 \u0001 ; 8g\n\u000b\nFig. 4     PVT-based  architectures  offer  more  options  for\ndesigning the masking strategy. The vanilla ViT-based method\n(a) only selects a fixed-scale patch to mask, i.e.,  . However,\nthe PVT-based method (b) is more versatile because it combines\nmore fine-grained patches as a basic masking unit, i.e.,  ,\nwhere  .  These  masked  patches  are  not\noverlapped  with  each  other.  This  characteristic  provides  a\nflexible way to learn the suitable semantics by using different\nvalues for  . Notably, we adopt a fixed scale factor of masking\nunits in an individual experiment.\n \n[ZERO]\n= 10\u00006\nP =\n32\n\u000b =\n8; P = 4\n2 In  fact,  we  set  to  bring  better  optimization\nstability and less pattern degradation.\n3 The vanilla masking strategy in Fig. 4(a) with  becomes\na  special  case  of  our  masking  strategy  in Fig.  4(b)  when\n.\nG. P. Ji et al. / Masked Vision-language Transformer in Fashion 425 \n \n\nTask  1:  Text-image  retrieval  (TIR). The  TIR\ntask  requires  the  model  to  find  a  text  with  the  highest\nsimilarity  value  with  different  query  images.  In  particu-\nlar, we take a product title and its corresponding image\nas a positive image-text pair, while the negative pairs are\nrandomly selected from a pool of mismatched images. To\nincrease our experiment's difficulty, we constrain a set of\nimage-text candidates (i.e., a positive pair and 100 negat-\nive pairs) in the same sub-category, making them as sim-\nilar as possible.\nR\nR\nR\nTask  2:  Image-text  retrieval  (ITR). As  the  re-\nverse process of the TIR task, the ITR task aims to re-\ntrieve a matching image given a sequence of text entries\nof fashion description, where these bidirectional retrieval\ntasks (i.e., TIR and ITR) become prominent members of\ncross-modal  research.  Similar  to  the  above  selection\nstrategy in the TIR, we prepare a set of candidate image-\ntext  pairs,  including  a  positive  pair  and  100  negative\npairs from the same sub-category. We evaluate the zero-\nshot learning ability of our MVLT without further fine-\ntuning for these two retrieval tasks. We utilize three ac-\ncuracy metrics (i.e., @1,\n  @5, and @10) for the eval-\nuation by ranking a series of predicted probabilities.\n48\n122\nfM \u0000 CR = SWEATERS; S \u0000 CR = CREWNECKSg\nT4\nA\nF\nTask  3:  Category  recognition  (M-CR  and  S-\nCR). This task has two parts: main-category recognition\n(M-CR)  and  sub-category  recognition  (S-CR).  These\ntasks  are  the  fundamental  role  of  practical  e-commerce\napplications that offer the specific category of the quer-\nied product. We expect that the model should possess the\nability  to  recognize  differences  under  different  granular-\nity levels:  \n main-categories and  sub-categories, such\nas .  After the\nclass embedding in the last language embedding ,  we\nadd  two  independent  FC  layers  to  generate  the  final\nprobabilities for two different recognition tasks. This pro-\ncedure requires additional fine-tuning with recognition la-\nbels. We utilize two recognition-related metrics to evalu-\nate  performance:  accuracy  ( )\n  and  macro  F-measure\n(macro- ).\nrv\nTask 4: Masked image generation (MIG). The\nMIG  task  can  be  viewed  as  a  pixel-wise  reconstruction\ntask. Each patch in the image is randomly masked with\nthe probability  \n (refer to the pre-training task MIR in\nSection  3.2).  Then,  we  ask  the  model  to  recreate  the\nwhole image using the uncovered areas as visual clues.\n 4   Experiments\nThis  section  will  detail  our  experiment  to  determine\nthe factors leading to the success of the proposed MVLT.\n 4.1   Settings\nThis part provides the hyperparameter settings for our\ntraining  procedure: 1)  Pre-training. We  utilize  PyT-\norch to implement our method, which is accelerated by 8\nTesla V100 GPUs. We adopt an AdamW optimizer with\n0:9\n150\n10\u00004\n2:5 \u0002 10\u00003\nH = W =\n256\nP =\n4\nL =\n128\nrv =\n0:5\nrl =\n0:15\nfw1 =\n10; w2 = 1 ; w3 = 1 g\na momentum value of ,  a mini-batch size of 1 200 (i.e.,\n per  GPU),  and  a  weight  decay  of .  To  avoid\nover-fitting, we initialize MVLT on ImageNet pre-trained\nweights[21]. The learning rate is initially set to \nand is changed using a cosine learning schedule. For the\nvisual  side,  the  input  image  is  resized  to \nand split into multiple sub-patches with a size of .\nFor the language side, all the product captions are token-\nized  and  padded  to  tokens  with  a  unified  length  of\n,\n including  classification,  caption,  and  padding\ntokens.  The  mask  probabilities  for  vision  and  language\nare set to  \n and , respectively. We empir-\nically  set  weighting  factors  to\nbalance the orders of magnitude of different loss values.\n2) Fine-tuning. We transfer the pre-trained VL repres-\nentation to each downstream application via fine-tuning\nin  an  end-to-end  manner,  whose  settings  are  consistent\nwith the pre-training process.\n 4.2   Results\nAs described in Section 3.3, we provide the details of\nfour downstream fashion-related tasks. Experimental res-\nults  show  that  our  MVLT  outperforms  all  competitors,\nincluding  VSE[48],  VSE++[49],  SCAN[26],  PFAN[50],  ViL-\nBERT[14],  ImageBERT[13],  FashionBERT[19],  VL-\nBERT[25],  OSCAR[29], and  Kaleido-BERT[20],  which\ndemonstrate  the  superiority  for  handling  the  VL  under-\nstanding and generation tasks.\n21\nR@5\nR@10\nR@5\nR@10\nTIR and ITR. As shown in Table 2, our MVLT sur-\npasses the best method (i.e., Kaleido-BERT-CVPR )  on\nthe TIR task by margins of +17.40% and +20.91% across\nthe  \nand .  As  for  ITR,  our  method  delivers\nmore competitive results, with improvements of +17.11%\nand  +22.73%  on  the  and  metrics,  respect-\nively. In any case, these results strongly support that our\nmodel is powerful enough to match vision and language.\nThey also show how 1) MIR and 2) end-to-end pre-train-\ning are useful in fashion. We believe that MVLT would\nset a precedent in many industrial applications because it\nis a simple, cost-effective, and powerful architecture. Be-\nsides, we present the visualization results of these two re-\ntrieval tasks in Fig. 5.\nF\nC\nM-CR and S-CR. Compared with BERT-based ar-\nchitectures[13, 19, 20, 29], we also achieve top-1 performances\nin these two tasks, demonstrating our method has an ex-\ncellent  VL  understanding  capability.  Moreover,  com-\npared with the best method Kaleido-BERT, our architec-\nture improves by 0.193 in the macro-  \n metric for the S-\nCR task. In addition, the mean improvements in terms of\nthe Sum  metric (i.e., M-CR: +21.39 and S-CR: +24.80)\nare very significant. Since this metric is very sensitive to\ndata distribution, it demonstrates that MVLT has super-\nstrong  robustness.  We  also  present  the  recognition  res-\nults of M-CR and S-CR in Fig. 6.\nMIG. As shown in Fig. 7, we showcase reconstructed\n \n426 Machine Intelligence Research 20(3), June 2023\n \n\nimages  on  the  validation  part  of  Fashion-Gen  2018  (a)\nand our e-commerce website (b). As seen, the reconstruc-\ntion performance is truly remarkable. Since it requires our\nmethod to learn the fashion semantics truly, such results\ndemonstrate the generative ability of our approach.\n 4.3   Ablation studies\nrv\n0:10\n0:30\n0:70\n0:90\n0:50\nMask Ratio. Table 3 (a) presents four variants for\ndifferent mask probability  (i.e.,  (A1),  (A2),\n (A3)  and  (A4))  and  our  choice:  (Final).\nR@5\n75:70% ! 78:00\n73:80\nrv\nThe  rises steadily with the masking probability un-\ntil it reaches the sweet spot ( %); then it\nreaches performance plummets ( %). We argue that\nincreasing the  will make MIR more complex, allowing\nMVLT to learn better semantics in a more restricted situ-\nation. However, masking out too much region will natur-\nally  result  in  losing  valid  visual  information,  leading  to\nbad results.\nMasked unit size. Thanks  to  PVT's  flexibility,  we\ncan easily try different sizes of masked patches. As shown\nin\n Table 3 (b), we derive four variants with masked unit\n \n\"\nSumR\nR\nR\nR\n\u0002100\nSumC\n(A + macro\nF) \u0002 100\nTable 2    Retrieval (i.e., TIR and ITR) and recognition (i.e., M-CR and S-CR) performances on the Fashion-Gen dataset.   means the\nlarger, the better. Here,  =( @1+ @5+ @10)     and  = - .  “N/A” means the score is not available.\n“Diff” means the numerical difference between the performance of the second-ranked competitor and our MVLT.\nVSE VSE++ SCAN PFAN ViLBERT ImageBERT FashionBERT VL-BERT OSCAR Kaleido-BERT MVLT\nTask Metric\n14 arXiv\n18 BMVC\n18 ECCV\n19 arXiv\n19 NeurIPS\n20 arXiv\n20 SIGIR\n20 ICLR\n20 ECCV\n21 CVPR\n22 OUR Diff\nTIR\nR\n@1\n\" 4.350% 4.600% 4.300% 6.200% 21.12% 24.78% 26.75% 22.63% 25.10% 33.88% 34.60% +0.72%\nR\n@5\n\" 12.76% 16.89% 13.00% 20.79% 37.23% 45.20% 46.48% 36.48% 49.14% 60.60% 78.00%+17.40%\nR\n@10\n\" 20.91% 28.99% 22.30% 31.52% 50.11% 55.90% 55.74% 48.52% 56.68% 68.59% 89.50%+20.91%\nSumR\n\"\n38.02 50.48 39.6 58.51 108.46 125.88 128.97 107.63 130.92 163.07 202.1\n+39:03\nITR\nR\n@1\n\" 4.010% 4.590% 4.590% 4.290% 20.97% 22.76% 23.96% 19.26% 23.39% 27.99% 33.10% +5.11%\nR\n@5\n\" 11.03% 14.99% 16.50% 14.90% 40.49% 41.89% 46.31% 39.90% 44.67% 60.09% 77.20%+17.11%\nR\n@10\n\" 22.14% 24.10% 26.60% 24.20% 48.21% 50.77% 52.12% 46.05% 52.55% 68.37% 91.10%+22.73%\nSumR\n\"\n37.18 43.68 47.69 43.39 109.67 115.42 122.39 105.21 120.61 156.45 201.4\n+44:95\nM-CR\nA\n\"\nN/A N/A N/A N/A N/A 90.77% 91.25% N/A 91.79% 95.07% 98.26% +3.19%\nmacro\nF\n-\n\" N/A N/A N/A N/A N/A 0.699 0.705 N/A 0.727 0.714 0.896\n+0:169\nSumC\n\"\nN/A N/A N/A N/A N/A 160.67 161.75 N/A 164.49 166.47 187.86\n+21:39\nS-CR\nA\n\"\nN/A N/A N/A N/A N/A 80.11% 85.27% N/A 84.23% 88.07% 93.57% +5.50%\nmacro\nF\n-\n\" N/A N/A N/A N/A N/A 0.575 0.620 N/A 0.591 0.636 0.829\n+0:193\nSumC\n\"\nN/A N/A N/A N/A N/A 137.61 147.27 N/A 143.33 151.67 176.47\n+24:80\n \n \nShort sleeve cotton jersey\nt-shirt in black. Rib knit\ncrewneck collar. Knot at\ncuff. Tonal stitching.\nTIR\n(Rank@5)\n99.77% 99.76% 99.69% 99.62% 99.49%\nCropped poplin t-shirt in\nwhite. Ribbed crewneck\ncollar. Boxy fit. Tonal\nstitching.\n99.82% 99.77% 99.39% 98.42% 98.38%\nLong sleeve mohair-silk\nknit sweater in cream.\nRibbed crewneck collar,\ncuffs, and hem. Tonal\nstitching.\n99.96% 99.96% 99.88% 99.64% 99.54%\nStraight-leg old denim\njeans in indigo. Mid-rise.\nFour-pocket styling.\nButton-fly. Tonal stitching.\nApprox. 7.5\" leg opening.\n99.80% 99.25% 99.20% 98.64% 97.71%\nTIR\n(Rank@5)\nTIR\n(Rank@5)\nTIR\n(Rank@5)\nITR\n(Rank@5)\n \n \n \n \n \n \n \n \n \n \nLong sleeve 'fully fashioned French terry' knit cashmere pullover in light grey. Rib knit crewneck collar, cuffs, and hem. Raglan sleeves. Rib knit\npanel at armscyes and side-seams. Signature 'four bar' striping knit in white at upper sleeve. Signature tricolor grosgrain pull-tab at back yoke. Tonal\nstitching. (99.96%, Matched)\nLong sleeve rib knit silk and cotton-blend sweater in navy. Crewneck collar. Red trim at hem. Dropped shoulders. Tonal stitching. (99.91%)\nCotton knit pullover in black. Ribbed crewneck collar, cuffs, and hem. Intarsia skull pattern knit at front in black and grey. Tonal stitching. (85.90%)\nLong sleeve cotton sweater in navy. Rib knit crewneck collar, cuffs, and hem. Raglan sleeves. Tonal stitching. (0.12%)\nLong sleeve rib knit alpaca and wool-blend sweater in off-white. Distressing throughout. Crewneck collar. Tonal stitching. (0.01%)\nLong sleeve 'chunky' knit wool sweater in off-white. Rib knit V-neck collar, cuffs, and hem. Patch pocket at body. Tonal stitching. (99.95%,\nMatched)\nLong sleeve rib knit cotton off-the-shoulder sweater in black. V-neck collar. Tonal stitching. (99.94%)\nLong sleeve rib knit merino wool off-the-shoulder pullover in black. Off-white rib knit cotton tank top-style underlay at V-neck collar. Tonal\nstitching. (64.74%)\nLong sleeve knit crepe sweater in black. V-neck collar. Drop-tail hem. Tonal stitching. (0.50%)\nLong sleeve boxy ribbed knit sweater in black. V-neck collar. Trim in white at collar and cuffs. Raglan sleeves. Tonal stitching. (0.03%)\nITR\n(Rank@5)\nFig. 5     Visualization results on the TIR and ITR tasks in terms of top-five ranked probabilities predicted by our MVLT. “Matched”\nindicates the ground-truth image-text pair.\n \nG. P. Ji et al. / Masked Vision-language Transformer in Fashion 427 \n \n\n\u000b\nsize  (i.e., 1 (B1), 2 (B2), 8 (B3), 16 (B4)) to compare\nwith our setting: 4 (Final). We found that the perform-\nance is sensitive to this factor. It makes sense, revealing\nhow vital it is to learn a robust fashion-related represent-\nation with moderate granularity.\nMasking style. As shown in\n Fig. 8, we designed four\ntypes  of  masking  strategies  for  the  MIR  task,  whose\nquantitative differences are presented in Table 3 (c), i.e.,\ngrid (C1), stroke (C2), center (C3) and our random grid\n(Final)  masking  strategies.  As  can  be  seen,  the  random\ngrid  masking  (Final)  yields  the  best  results,  while  the\nother three perform poorly. We believe this is because, in\ncomparison  to  the  grid  (C1)  and  center  (C3),  random\ngrid masking (Final) can help MVLT construct compre-\nhensive representations. As our strategy (Final) does, the\nstroke (C2) also randomly masks the image given, yet it\nmore  or  less  leaves  unmasked  visual  cues  in  the  sub-\npatches. Our strategy enables the model to easily predict\nthe  masked  region  because  semantics  in  the  image  are\nwell preserved, enhancing the model's robustness to learn-\ning in-sight knowledge.\nR@5\n74:10\n76:00\n76:20\nR@5\n70:80\nPre-training objectives. As shown in Table 3 (d),\nwe  derive  four  different  variants  from  investigating  the\ncontribution  of  each  objective,  including  ITM  (D1),\nITM+MIR  (D2),  ITM+MLM  (D3),  and  our  ITM+\nMIR+MLM (Final). When comparing D3 to D1 and D2\nin the TIR task, we can see that D3 has a better perform-\nance in the  metric: % (D1) <\n % (D2) <\n% (D3). We conclude MLM task can help the mod-\nel thoroughly learn the language knowledge, so it provides\na more precise query to recall better-matching images. In\nthe ITR task, we find a similar conclusion when \ncompar-\ning (D2) to (D1) and D3 in  metric: % (D1) <\n75:50\n76:30\n% (D2) < % (D3). It indicates that better visu-\nal learning leads to an accurate image query to match the\nmost appropriate caption.\n77:20% ! 71:50%\nR@5\n93:57% ! 92:90%\nA\nLoading pre-trained weight.  As  seen  in Table  4,\nwe add an experiment to demonstrate it is very import-\nant to load the PVT′s weight pre-trained on ImageNet[51].\nIf  not,  it  is  obvious  that  our  MVLT  will  suffer  fierce\ndrops  (i.e.,  ITR:  in ,  S-CR:\n in ). It is reasonable because a meth-\nod pre-trained on large-scale general datasets can be more\napplicable  in  a  specific  field.  It  has  already  learned  in-\nformation such as color, texture, shape, etc.\n 4.4   More discussions\nHow does MVLT perform in general domains?\nWe discuss two extended questions to investigate the po-\ntential abilities in general settings further.  1) Can the gen-\neral  models  be  directly  transferred  to  the  fashion  domain? In-\nspired  by  the  huge  impact  of  general  vision-language\nmodels, as in\n Table 5, we further investigate the zero-shot\nperformance  of  two  typical  general  models  (i.e.,  ViL-\nBERT[14] and  CLIP[52]).  This  has  once  again  demon-\nstrated  the  necessity  and  superiority  of  MVLT  pre-\ntrained on specific domains. 2) Can MVLT also work well in\nthe general domain? We further verify the potential ability\nof our MVLT in the general domain. Table 6 reports the\nperformance  on  the  MS-COCO  2014  dataset\n[53],  where\nMVLT follows the same training standards as in [ 37]. It\nshows  that  MVLT  achieves  promising  results  compared\nto  the  latest  models  (i.e.,  Unicoder-VL\n[54],  UNITER[15],\nand ViLT[37]) without extra training data and special re-\ntrieval losses during the training. It indicates that MVLT\n \nGround-truth label\nPrediction\n{M-CR = JACKETS & COATS}\n{S-CR = BOMBERS}\nGround-truth label\n{M-CR = TOPS}\n{S-CR = T-SHIRTS}\n{M-CR = JACKETS & COATS}\n{S-CR = BOMBERS}\nPrediction\n{M-CR = TOPS}\n{S-CR = T-SHIRTS}\nGround-truth label\n{M-CR = JEANS}\n{S-CR = JEANS\nPrediction\n{M-CR = JEANS}\n{S-CR = JEANS}\nGround-truth label\n{M-CR = SHIRTS}\n{S-CR = SWEATSHIRTS}\nPrediction\n{M-CR = SHIRTS}\n{S-CR = SWEATSHIRTS}\nGround-truth label\n{M-CR = PANTS}\n{S-CR = TROUSERS}\nPrediction\n{M-CR = PANTS}\n{S-CR = TROUSERS}\nGround-truth label\n{M-CR = TOPS}\n{S-CR = SHIRTS}\nPrediction\n{M-CR = TOPS}\n{S-CR = SHIRTS}\nFig. 6     The visualization of main-/sub-category recognition results on Fashion-Gen. The green predictions hit the targets.\n \n \n(a) Fashion-gen (In-domain) (b) Data from e-commercial website (out-of-domain)\nFig. 7     Visualization of samples generated by our MVLT. The gray blocks represent the masked regions.\n \n 428 Machine Intelligence Research 20(3), June 2023\n \n\n \nTable 3    Ablation studies of five key pre-training factors on our MVLT. More relevant analyses refer to Section 4.3.\nrv\n(a) Mask ratio ( )\n\u000b (b) Masking unit size ( ) (c) Masking style (d) Pre-training tasks (e) Pre-train MVLT\n(A1) (A2) (A3) (A4) (B1) (B2) (B3) (B4) (C1) (C2) (C3) (D1) (D2) (D3) (E1) ( Final)\nApp. Metric\n0:10\n0:30\n0:70\n0:90\n1\n2\n8\n16 Grid Stroke Center ITM ITM+MIR ITM+MLM\nw/o  PVT\nTIR\nR\n@1 31.10% 33.50% 30.50% 30.70% 31.90% 30.30% 30.00% 32.20% 32.20% 31.40% 30.40% 30.40% 32.20% 32.90% 29.00% 34.60%\nR\n@5 75.70% 76.00% 75.50% 73.80% 75.30% 75.60% 73.90% 76.90% 75.30% 76.10% 75.10% 74.10% 76.00% 76.20% 72.20% 78.00%\nR\n@10 88.60% 88.70% 88.00% 88.60% 89.60% 88.60% 88.20% 88.60% 88.50% 89.20% 87.20% 83.50% 87.20% 88.60% 86.60% 89.50%\nSumR\n195.40 198.20 194.00 193.10 196.80 194.50 192.10 197.70 196.00 196.70 192.70 188.00 195.40 197.70 187.80 202.10\nDiff\n\u00006:70\n\u00003:90\n\u00008:10\n\u00009:00\n\u00005:30\n\u00007:60\n\u000010:00\n\u00004:40\n\u00006:10\n\u00005:40\n\u00009:40\n\u000014:10\n\u00006:70\n\u00004:40\n\u000014:30\n\u0000\nITR\nR\n@1 30.00% 29.90% 29.90% 28.50% 29.00% 29.70% 29.00% 28.90% 31.40% 31.10% 30.10% 29.30% 30.40% 28.40% 25.60% 33.10%\nR\n@5 75.70% 74.90% 76.50% 75.00% 76.90% 77.10% 74.20% 77.30% 77.40% 74.50% 73.90% 70.80% 75.50% 76.30% 71.50% 77.20%\nR\n@10 88.80% 89.00% 89.20% 88.20% 89.40% 87.70% 88.00% 89.90% 89.60% 88.50% 87.80% 86.80% 87.80% 88.80% 85.90% 91.10%\nSumR\n194.50 193.80 195.60 191.70 195.30 194.50 191.20 196.10 198.40 194.10 191.80 186.90 193.70 193.50 183.00 201.40\nDiff\n\u00006:90\n\u00007:60\n\u00005:80\n\u00009:70\n\u00006:10\n\u00006:90\n\u000010:20\n\u00005:30\n\u00003:00\n\u00007:30\n\u00009:60\n\u000014:50\n\u00007:70\n\u00007:90\n\u000018:40\n\u0000\nM-CR\nA\n98.16% 97.87% 98.09% 98.06% 98.03% 98.04% 98.11% 98.01% 98.12% 98.07% 98.04% 96.49% 97.11% 98.08% 97.92% 98.26%\nF\nmacro- 0.870 0.860 0.890 0.870 0.870 0.880 0.850 0.870 0.869 0.877 0.870 0.806 0.853 0.876 0.879 0.896\nSumC\n185.16 183.87 187.09 185.06 185.03 186.04 183.11 185.01 185.02 185.77 185.04 177.09 182.41 185.68 185.82 187.86\nDiff\n\u00002:70\n\u00003:99\n\u00000:77\n\u00002:80\n\u00002:83\n\u00001:82\n\u00004:75\n\u00002:85\n\u00002:84\n\u00002:09\n\u00002:82\n\u000010:77\n\u00005:45\n\u00002:18\n\u00002:04\n\u0000\nS-CR\nA\n93.10% 93.34% 93.36% 93.23% 93.29% 93.34% 93.32% 93.32% 93.37% 93.21% 93.59% 89.64% 90.87% 93.29% 92.90% 93.57%\nF\nmacro- 0.800 0.810 0.820 0.810 0.810 0.810 0.800 0.799 0.794 0.814 0.830 0.703 0.728 0.809 0.790 0.829\nSumC\n173.10 174.34 175.36 174.23 174.29 174.34 173.32 173.22 172.77 174.61 176.59 159.94 163.67 174.19 171.90 176.47\nDiff\n\u00003:37\n\u00002:13\n\u00001:11\n\u00002:24\n\u00002:18\n\u00002:13\n\u00003:15\n\u00003:25\n\u00003:70\n\u00001:86\n+0:12\n\u000016:53\n\u000012:80\n\u00002:28\n\u00004:57\n\u0000\n \nG. P. Ji et al. / Masked Vision-language Transformer in Fashion 429 \n \n\nis  also  a  promising  solution  when  extended  to  general\nscenes.\nWhy do pyramid architecture and MIR bene-\nfit? As mentioned in the introduction, there are two un-\nderstudied problems in the fashion domain. To solve the\ntransferability  problem,  pyramidal  architecture[21] takes\nraw data as input without complex pre-processing, which\nessentially alleviates the applied burden in industry. \nBe-\nsides, MIR does not need human annotations like classi-\nfication tags, bounding boxes, or pixel-wise segmentation\nlabels. For the granularity problem[55], the pyramidal ar-\nchitecture\n[21] provides  multi-scale  features  with  rich  se-\nmantics.  Combined  with  the  MIR  task,  our  framework\ncan  represent  multi-grained  fashion  knowledge  (e.g.,\ndress,  V-neck).  These  features  are  helpful  and  urgently\nrequired in this field.\nA  VL  model  that  performs  well  for  semantic  under-\nstanding tasks (e.g., retrieval[56], classification) can serve\nas  a  good  foundation  and  be  easily  applied  to  down-\nstream tasks (e.g., text-to-image synthesis[57], image cap-\ntioning)  by  utilizing  an  additional  decoder.  We  did  not\nconduct  image  captioning  experiments  because  we  fo-\ncused  on  basic  representation  learning  in  fashion  this\ntime.\nMVLT VS. MAE[34]. MAE learns general represent-\nations by allowing the model to explore pixel-to-pixel as-\nsociations. Therefore, MVLT and MAE are similar in this\nregard. However, our MVLT is the first that introduces\nthe vision reconstruction-alike pre-training for multi-mod-\nal research (e.g., fashion domain).\n \n5   Conclusions\nWe  present  a  vision-language  framework  named\nMVLT, which provides two contributions in this field: 1)\na newly-designed masked image reconstruction (MIR) ob-\njective and 2) an end-to-end pre-training scheme. The ex-\nperimental and ablative analysis demonstrates the superi-\nority  of  various  matching  and  generative  tasks.  MVLT\noutperforms the cutting-edge method Kaleido-BERT with\nlarge  margins  on  retrieval  and  recognition  tasks,  which\nwould catalyze the fashion domain. The designed out-of-\nbox method working end-to-end could simplify the work-\nflow (e.g., data pre-processing and model training) for the\nactual  engineering  value,  which  improves  development\n \nTable\n 4    Ablation study for the contribution of loading PVT′s weights pre-trained on ImageNet[51]\nTIR ITR M-CR S-CR\nR@5\nR@10\nR@5\nR@10\nA\nF\nmacro-\nA\nF macro-\nw/o\n PVT 72.20% 86.60% 71.50% 85.90% 97.92% 0.879 92.90% 0.790\nw/\n PVT 78.00% 89.50% 77.20% 91.10% 98.26% 0.896 93.57% 0.829\nDiff +5.80% +2.90% +5.70% +5.20% +0.34% +1.7% +0.67% +3.9%\n \n \nTable 5    The comparison of zero-shot retrieval results on the Fashion-Gen dataset\nTIR ITR\nR\n\"\n@1\nR\n\" @5\nR\n\" @10\nR\n\" @1\nR\n\" @5\nR\n\" @10\nViLBERT (Zero-shot) 7.18% 18.73% 29.84% 8.99% 15.34% 26.14%\nCLIP (Zero-shot) 16.30% 40.60% 55.60% 13.60% 43.10% 57.60%\nMVLT (OUR) 34.60% 78.00% 89.50% 33.10% 77.20% 91.10%\n \n \ny\nTable 6    Retrieval results on the MS-COCO 2014 dataset.   means using an extra feature extractor (e.g., Faster RCNN).\nTIR task (5K Test) ITR task (5K Test)\nR\n\"\n@1\nR\n\" @5\nR\n\" @10\nR\n\" @1\nR\n\" @5\nR\n\" @10\ny\nUnicoder-VL 48.40% 76.70% 85.90% 62.30% 87.10% 92.80%\ny\nUNITER-Base 50.30% 78.50% 87.20% 64.40% 87.40% 93.10%\nViLT-Base/32 41.30% 72.00% 82.50% 61.80% 86.20% 92.60%\nMVLT (OUR) 49.66% 79.88% 87.50% 65.38% 90.04% 93.60%\n \n \n(C1) Grid (C2) Stroke (C3) Center Random grid\n \nFig. 8     We designed four strategies to mask fashion images. The\nrandom grid performs the best.\n \n 430 Machine Intelligence Research 20(3), June 2023\n \n\nand  business  efficiency  on  large-scale  e-commerce  web-\nsites by approximately 50%.\nIn  the  future,  we  will  continue  to  investigate  an  ex-\ntremely efficient method in this field using famous tech-\nnologies such as hashing[58], network pruning, and know-\nledge distillation to alleviate the storage and computing\nlimitations in real-world e-commerce applications.\n Acknowledgements\nThis work is funded by Toyota Motor Europe via the\nresearch project TRACE-Zürich. The authors also would\nlike  to  thank  the  anonymous  reviewers  and  editor  for\ntheir helpful comments on this manuscript. Open access\nfunding  provided  by  Swiss  Federal  Institute  of  Techno-\nlogy Zurich.\n Conflicts of Interests\nThe authors declared that they have no conflicts of in-\nterest in this work. We declare that we do not have any\ncommercial or associative interest that represents a con-\nflict of interest in connection with the work submitted.\n Open Access\nThis article is licensed under a Creative Commons At-\ntribution  4.0  International  License,  which  permits  use,\nsharing, adaptation, distribution and reproduction in any\nmedium or format, as long as you give appropriate credit\nto the original author(s) and the source, provide a link to\nthe  Creative  Commons  licence,  and  indicate  if  changes\nwere made.\nThe images or other third party material in this art-\nicle  are  included  in  the  article′s  Creative  Commons  li-\ncence,  unless  indicated  otherwise  in  a  credit  line  to  the\nmaterial. If material is not included in the article′s Creat-\nive  Commons  licence  and  your  intended  use  is  not  per-\nmitted by statutory regulation or exceeds the permitted\nuse, you will need to obtain permission directly from the\ncopyright holder.\nTo  view  a  copy  of  this  licence,  visit http://creative-\ncommons.org/licenses/by/4.0/.\nReferences\n A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,\nX. H. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G.\nHeigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An image\nis worth 16 ×16 words: Transformers for image recognition\nat scale. In \nProceedings of the 9th International Confer-\nence on Learning Representations, 2021.\n[1]\n Z. Liu, Y. T. Lin, Y. Cao, H. Hu, Y. X. Wei, Z. Zhang, S.\nLin,  B.  N.  Guo.  Swin  transformer:  Hierarchical  vision\ntransformer  using  shifted  windows.  In  Proceedings of\nIEEE/CVF International Conference on Computer Vision,\nIEEE, Montreal, Canada, pp. 9992–10002, 2021. DOI: 10.\n1109/ICCV48922.2021.00986.\n[2]\n A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,\nA. N. Gomez, L. Kaiser, I. Polosukhin. Attention is all you\n[3]\nneed. In Proceedings of the 31st International Conference\non Neural Information Processing Systems, Long Beach,\nUS, pp. 6000–6010, 2017.\n T. X. Sun, X. Y. Liu, X. P. Qiu, X. J. Huang. Paradigm\nshift in natural language processing. Machine Intelligence\nResearch, vol. 19, no. 3, pp. 169–183, 2022. DOI: 10.1007/\ns11633-022-1331-6.\n[4]\n S. Agarwal, G. Krueger, J. Clark, A. Radford, J. W. Kim,\nM.  Brundage.  Evaluating  CLIP:  Towards  characteriza-\ntion of broader capabilities and downstream implications,\n[Online],  Available:  https://arxiv.org/abs/2108.02818,\nAugust 05, 2021.\n[5]\n M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, I.\nSutskever. Generative pretraining from pixels. In Proceed-\nings of the 37th International Conference on Machine\nLearning, Article number 233, 2020.\n[6]\n J. Y. Lin, R. Men, A. Yang, C. Zhou, Y. C. Zhang, P.\nWang, J. R. Zhou, J. Tang, H. X. Yang. M6: Multi-modal-\nity-to-multi-modality multitask mega-transformer for uni-\nfied pretraining. In Proceedings of the 27th ACM SIGK-\nDD Conference on Knowledge Discovery & Data Mining,\n2021. DOI: 10.1145/3447548.3467206.\n[7]\n A. Ramesh, M. Pavlov, G. Goh, S. Gray, C. Voss, A. Rad-\nford, M. Chen, I. Sutskever. Zero-shot text-to-image gen-\neration. In Proceedings of the 38th International Confer-\nence on Machine Learning, pp. 8821–8831, 2021.\n[8]\n H. Wu, Y. P. Gao, X. X. Guo, Z. Al-Halah, S. Rennie, K.\nGrauman, R. Feris. Fashion IQ: A new dataset towards re-\ntrieving images by natural language feedback. In Proceed-\nings of IEEE/CVF Conference on Computer Vision and\nPattern Recognition,  IEEE,  Nashville,  USA,  pp. 11302–\n11312, 2021. DOI: 10.1109/CVPR46437.2021.01115.\n[9]\n J. Devlin, M. W. Chang, K. Lee, K. Toutanova. BERT:\nPre-training  of  deep  bidirectional  transformers  for  lan-\nguage understanding. In Proceedings of Conference of the\nNorth American Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,  Min-\nneapolis,  USA,  pp. 4171–4186, 2019.  DOI:  10.18653/v1/\nN19-1423.\n[10]\n K. M. He, X. Y. Zhang, S. Q. Ren, J. Sun. Deep residual\nlearning  for  image  recognition.  In  Proceedings of IEEE\nConference on Computer Vision and Pattern Recognition,\nIEEE, Las Vegas, USA, pp. 770–778, 2016. DOI: 10.1109/\nCVPR.2016.90.\n[11]\n S. Q. Ren, K. M. He, R. Girshick, J. Sun. Faster R-CNN:\nTowards real-time object detection with region proposal\nnetworks. In Proceedings of 2015 Annual Conference on\nNeural Information Processing Systems,  Montreal,\nCanada, pp. 91–99, 2015.\n[12]\n D. Qi, L. Su, J. Song, E. Cui, T. Bharti, A. Sacheti. Image-\nBERT: Cross-modal pre-training with large-scale weak-su-\npervised image-text data, [Online], Available: https://arx-\niv.org/abs/2001.07966, January 23, 2020.\n[13]\n J. S. Lu, D. Batra, D. Parikh, S. Lee. Vilbert: Pretraining\ntask-agnostic visiolinguistic representations for vision-and-\nlanguage tasks. In Proceedings of the 33rd Conference on\nNeural Information Processing Systems,  Vancouver,\nCanada, pp. 13–23, 2019.\n[14]\n Y. C. Chen, L. J. Li, L. C. Yu, A. El Kholy, F. Ahmed, Z.\nGan, Y. Cheng, J. J. Liu. UNITER: UNiversal image-TExt\nrepresentation  learning.  In  Proceedings of the 16th\nEuropean Conference on Computer Vision, Springer, Glas-\n[15]\nG. P. Ji et al. / Masked Vision-language Transformer in Fashion 431 \n \n\ngow,  UK,  pp. 104–120,  2020.  DOI:  10.1007/978-3-030-\n58577-8_7.\n W. L. Hsiao, I. Katsman, C. Y. Wu, D. Parikh, K. Grau-\nman. Fashion++: Minimal edits for outfit improvement.\nIn Proceedings of IEEE/CVF International Conference On\nComputer Vision,  IEEE,  Montreal,  Canada,  pp. 5046–\n5055, 2019. DOI: 10.1109/ICCV.2019.00515.\n[16]\n M. I. Vasileva, B. A. Plummer, K. Dusad, S. Rajpal, R.\nKumar, D. Forsyth. Learning type-aware embeddings for\nfashion  compatibility.  In  Proceedings of the 15th Eur-\nopean Conference on Computer Vision, Springer, Munich,\nGermany,  pp. 405–421,  2018.  DOI:  10.1007/978-3-030-\n01270-0_24.\n[17]\n D. P. Fan, M. C. Zhuge, L. Shao. Domain Specific Pre-\nTraining  of  Cross  Modality  Transformer  Model,\nUS20220277218, September 2022.\n[18]\n D. H. Gao, L. B. Jin, B. Chen, M. H. Qiu, P. Li, Y. Wei, Y.\nHu, H. Wang. FashionBERT: Text and image matching\nwith adaptive loss for cross-modal retrieval. In Proceed-\nings of the 43rd International ACM SIGIR Conference on\nResearch and Development in Information Retrieval,\nACM,  pp. 2251–2260,  2020.  DOI:  10.1145/3397271.\n3401430.\n[19]\n M. C. Zhuge, D. H. Gao, D. P. Fan, L. B. Jin, B. Chen, H.\nM. Zhou, M. H. Qiu, L. Shao. Kaleido-BERT: Vision-lan-\nguage pre-training on fashion domain. In Proceedings of\nIEEE/CVF Conference on computer vision and pattern re-\ncognition, IEEE, Nashville, USA, pp. 12642–12652, 2021.\nDOI: 10.1109/CVPR46437.2021.01246.\n[20]\n W. H. Wang, E. Z. Xie, X. Li, D. P. Fan, K. T. Song, D.\nLiang, T. Lu, P. Luo, L. Shao. Pyramid vision transformer:\nA versatile backbone for dense prediction without convolu-\ntions. In Proceedings of IEEE/CVF International Confer-\nence on Computer Vision,  IEEE,  Montreal,  Canada,\npp. 548–558, 2021. DOI: 10.1109/ICCV48922.2021.00061.\n[21]\n X. W. Yang, H. M. Zhang, D. Jin, Y. R. Liu, C. H. Wu, J.\nC. Tan, D. L. Xie, J. Wang, X. Wang. Fashion captioning:\nTowards generating accurate descriptions with semantic\nrewards. In Proceedings of the 16th European Conference\non Computer Vision,  Springer,  Glasgow,  UK,  pp. 1–17,\n2020. DOI: 10.1007/978-3-030-58601-0_1.\n[22]\n Z. Al-Halah, K. Grauman. From Paris to Berlin: Discover-\ning fashion style influences around the world. In Proceed-\nings of IEEE/CVF Conference on Computer Vision and\nPattern Recognition,  IEEE,  Seattle,  USA,  pp. 10133–\n10142, 2020. DOI: 10.1109/CVPR42600.2020.01015.\n[23]\n H.  Tan,  M.  Bansal.  LXMERT:  Learning  cross-modality\nencoder  representations  from  transformers.  In  Proceed-\nings of Conference on Empirical Methods in Natural Lan-\nguage Processing and the 9th International Joint Confer-\nence on Natural Language Processing, Hong Kong, China,\npp. 5100–5111, 2019. DOI: 10.18653/v1/D19-1514.\n[24]\n W. J. Su, X. Z. Zhu, Y. Cao, B. Li, L. W. Lu, F. R. Wei, J.\nF. Dai. VL-BERT: Pre-training of generic visual-linguistic\nrepresentations.  In  Proceedings of the 8th International\nConference on Learning Representations,  Addis  Ababa,\nEthiopia, 2020.\n[25]\n K. H. Lee, X. Chen, G. Hua, H. D. Hu, X. D. He. Stacked\ncross attention for image-text matching. In Proceedings of\nthe 15th European Conference on Computer Vision,\nSpringer, Munich, Germany, pp. 212–228, 2018. DOI: 10.\n1007/978-3-030-01225-0_13.\n[26]\n Z. X. Niu, M. Zhou, L. Wang, X. B. Gao, G. Hua. Hier-\narchical multimodal LSTM for dense visual-semantic em-\nbedding. In Proceedings of IEEE International Conference\non Computer Vision, IEEE, Venice, Italy, pp. 1899–1907,\n2017. DOI: 10.1109/ICCV.2017.208.\n[27]\n J.  Xia,  M.  Zhuge,  T.  Geng,  S.  Fan,  Y.  Wei,  Z.  He,  F.\nZheng. Skating-mixer: Multimodal MLP for scoring figure\nskating,  [Online],  Available:  https://arxiv.org/abs/2203.\n03990, 2022.\n[28]\n X. J. Li, X. Yin, C. Y. Li, P. C. Zhang, X. W. Hu, L.\nZhang, L. J. Wang, H. D. Hu, L. Dong, F. R. Wei, Y. J.\nChoi,  J.  F.  Gao.  Oscar:  Object-semantics  aligned  pre-\ntraining for vision-language tasks. In Proceedings of the\n16th European Conference on Computer Vision, Springer,\nGlasgow, UK, pp. 121–137, 2020. DOI: 10.1007/978-3-030-\n58577-8_8.\n[29]\n M. C. Zhuge, D. P. Fan, N. Liu, D. W. Zhang, D. Xu, L.\nShao. Salient object detection via integrity learning. IEEE\nTransactions on Pattern Analysis and Machine Intelli-\ngence,  to  be  published.  DOI:  10.1109/TPAMI.2022.\n3179526.\n[30]\n K.  Xu,  J.  Ba,  R.  Kiros,  K.  Cho,  A.  C.  Courville,  R.\nSalakhutdinov, R. S. Zemel, Y. Bengio. Show, attend and\ntell: Neural image caption generation with visual atten-\ntion. In Proceedings of the 32nd International Conference\non Machine Learning, Lille, France, pp. 2048–2057, 2015.\n[31]\n T. Arici, M. S. Seyfioglu, T. Neiman, Y. Xu, S. Train, T.\nChilimbi, B. Zeng, I. Tutar. MLIM: Vision-and-language\nmodel pre-training with masked language and image mod-\neling,  [Online],  Available:  https://arxiv.org/abs/2109.\n12178, September 24, 2021.\n[32]\n H. B. Bao, L. Dong, S. L. Piao, F. R. Wei. BEiT: BERT\npre-training of image transformers. In Proceedings of the\n10th International Conference on Learning Representa-\ntions, 2022.\n[33]\n K. M. He, X. L. Chen, S. N. Xie, Y. H. Li, P. Dollár, R.\nGirshick.  Masked  autoencoders  are  scalable  vision\nlearners.  In  Proceedings of IEEE/CVF Conference on\nComputer Vision and Pattern Recognition,  IEEE,  New\nOrleans, USA, pp. 15979–15988, 2022. DOI: 10.1109/CV-\nPR52688.2022.01553.\n[34]\n Z. C. Huang, Z. Y. Zeng, B. Liu, D. M. Fu, J. L. Fu. Pixel-\nBERT:  Aligning  image  pixels  with  text  by  deep  multi-\nmodal  transformers,  [Online],  Available:  https://arxiv.\norg/abs/2004.00849, June 22, 2020.\n[35]\n X. D. Lin, G. Bertasius, J. Wang, S. F. Chang, D. Parikh,\nL.  Torresani.  VX2TEXT:  End-to-end  learning  of  video-\nbased text generation from multimodal inputs. In Proceed-\nings of IEEE/CVF Conference on Computer Vision and\nPattern Recognition,  IEEE,  Nashville,  USA,  pp. 7001–\n7011, 2021. DOI: 10.1109/CVPR46437.2021.00693.\n[36]\n W. Kim, B. Son, I. Kim. ViLT: Vision-and-language trans-\nformer without convolution or region supervision. In Pro-\nceedings of the 38th International Conference on Machine\nLearning, pp. 5583–5594, 2021.\n[37]\n M. Yan, H. Y. Xu, C. L. Li, B. Bi, J. F. Tian, M. Gui, W.\nWang. Grid-VLP: Revisiting grid features for vision-lan-\nguage pre-training, [Online], Available: https://arxiv.org/\nabs/2108.09479, August 21, 2021.\n[38]\n Z. C. Huang, Z. Y. Zeng, Y. P. Huang, B. Liu, D. M. Fu, J.\nL. Fu. Seeing out of the box: End-to-end pre-training for\nvision-language representation learning. In Proceedings of\n[39]\n 432 Machine Intelligence Research 20(3), June 2023\n \n\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition,  IEEE,  Nashville,  USA,  pp. 12971–12980,\n2021. DOI: 10.1109/CVPR46437.2021.01278.\n S. Goenka, Z. H. Zheng, A. Jaiswal, R. Chada, Y. Wu, V.\nHedau, P. Natarajan. Fashionvlp: Vision language trans-\nformer for fashion retrieval with feedback. In Proceedings\non Conference on computer vision and pattern recognition,\nIEEE, New Orleans, USA, pp. 14085–14095, 2022. DOI: 10.\n1109/CVPR52688.2022.01371.\n[40]\n J. Lei, L. J. Li, L. W. Zhou, Z. Gan, T. L. Berg, M. Bansal,\nJ. J. Liu. Less is more: Clipbert for video-and-language\nlearning  via  sparse  sampling.  In  Proceedings of\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, IEEE, Nashville, USA, pp. 7327–7337, 2021.\nDOI: 10.1109/CVPR46437.2021.00725.\n[41]\n H. Y. Xu, M. Yan, C. L. Li, B. Bi, S. F. Huang, W. M.\nXiao,  F.  Huang.  E2E-VLP:  End-to-end  vision-language\npre-training enhanced by visual learning. In Proceedings of\nthe 59th Annual Meeting of the Association for Computa-\ntional Linguistics and the 11th International Joint Confer-\nence on Natural Language Processing, pp. 503–513, 2021.\n[42]\n H.  Akbari,  L.  Z.  Yuan,  R.  Qian,  W.  H.  Chuang,  S.  F.\nChang, Y. Cui, B. Q. Gong. VATT: Transformers for mul-\ntimodal self-supervised learning from raw video, audio and\ntext.  In  Proceedings of the 34th Annual Conference on\nNeural Information Processing Systems, pp. 24206–24221,\n2021.\n[43]\n X. Y. Yi, J. Yang, L. C. Hong, D. Z. Cheng, L. Heldt, A.\nKumthekar, Z. Zhao, L. Wei, E. Chi. Sampling-bias-cor-\nrected neural modeling for large corpus item recommenda-\ntions. In Proceedings of the 13th ACM Conference on Re-\ncommender Systems,  ACM,  Copenhagen,  Denmark,\npp. 269–277, 2019. DOI: 10.1145/3298689.3346996.\n[44]\n O. Ronneberger, P. Fischer, T. Brox. U-Net: Convolution-\nal networks for biomedical image segmentation. In Pro-\nceedings of the 18th International Conference on Medical\nImage Computing and Computer-Assisted Intervention,\nSpringer, Munich, Germany, pp. 234–241, 2015. DOI: 10.\n1007/978-3-319-24574-4_28.\n[45]\n C. Alberti, J. Ling, M. Collins, D. Reitter. Fusion of detec-\nted objects in text for visual question answering. In Pro-\nceedings of Conference on Empirical Methods in Natural\nLanguage Processing and the 9th International Joint Con-\nference on Natural Language Processing,  Hong  Kong,\nChina, pp. 2131–2140, 2019. DOI: 10.18653/v1/D19-1219.\n[46]\n N. Rostamzadeh, S. Hosseini, T. Boquet, W. Stokowiec, Y.\nZhang,  C.  Jauvin,  C.  Pal.  Fashion-gen:  The  generative\nfashion dataset and challenge, [Online], Available: https://\narxiv.org/abs/1806.08317v1, July 30, 2018.\n[47]\n R. Kiros, R. Salakhutdinov, R. S. Zemel. Unifying visual-\nsemantic  embeddings  with  multimodal  neural  language\nmodels,  [Online],  Available:  https://arxiv.org/abs/1411.\n2539, 2014.\n[48]\n F. Faghri, D. J. Fleet, J. R. Kiros, S. Fidler. VSE++: Im-\nproving visual-semantic embeddings with hard negatives.\nIn  Proceedings of British Machine Vision Conference,\nNewcastle, UK, 2018.\n[49]\n Y. X. Wang, H. Yang, X. M. Qian, L. Ma, J. Lu, B. Li, X.\nFan.  Position  focused  attention  network  for  image-text\nmatching. In Proceedings of the 28th International Joint\nConference on Artificial Intelligence,  Macao,  China,\npp. 3792–3798, 2019.\n[50]\n  J. Deng, W. Dong, R. Socher, L. J. Li, K. Li, F. F. Li. Im-\nageNet: A large-scale hierarchical image database. In Pro-\nceedings of IEEE Conference on Computer Vision and\nPattern Recognition,  IEEE,  Miami,  USA,  pp. 248–255,\n2009. DOI: 10.1109/CVPR.2009.5206848.\n[51]\n A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S.\nAgarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G.\nKrueger, I. Sutskever. Learning transferable visual models\nfrom natural language supervision. In Proceedings of the\n38th International Conference on Machine Learning,\npp. 8748–8763, 2021.\n[52]\n T. Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D.\nRamanan,  P.  Dollár,  C.  L.  Zitnick.  Microsoft  COCO:\nCommon objects in context. In Proceedings of the 13th\nEuropean Conference on Computer Vision,  Springer,\nZürich, Switzerland, pp. 740–755, 2014. DOI: 10.1007/978-\n3-319-10602-1_48.\n[53]\n G. Li, N. Duan, Y. J. Fang, M. Gong, D. Jiang. Unicoder-\nVL: A universal encoder for vision and language by cross-\nmodal pre-training. In Proceedings of the 34th AAAI Con-\nference on Artificial Intelligence, the 32nd Innovative Ap-\nplications of Artificial Intelligence Conference, the 10th\nAAAI Symposium on Educational Advances in Artificial\nIntelligence, New York, USA, pp. 11336–11344, 2020.\n[54]\n L. Wu, D. Y. Liu, X. J. Guo, R. C. Hong, L. C. Liu, R.\nZhang. Multi-scale spatial representation learning via re-\ncursive  hermite  polynomial  networks.  In  Proceedings of\nthe 31st International Joint Conference on Artificial Intel-\nligence,  Vienna,  Austria,  pp. 1465–1473,  2022.  DOI:  10.\n24963/ijcai.2022/204.\n[55]\n D. P. Chen, M. Wang, H. B. Chen, L. Wu, J. Qin, W.\nPeng. Cross-modal retrieval with heterogeneous graph em-\nbedding. In Proceedings of the 30th ACM International\nConference on Multimedia,  ACM,  Lisboa,  Portugal,\npp. 3291–3300, 2022. DOI: 10.1145/3503161.3548195.\n[56]\n D. Y. Liu, L. Wu, F. Zheng, L. Q. Liu, M. Wang. Verbal-\nperson  nets:  Pose-guided  multi-granularity  language-to-\nperson  generation.  IEEE Transactions on Neural Net-\nworks and Learning Systems, to be published. DOI: 10.\n1109/TNNLS.2022.3151631.\n[57]\n Z. Zhang, H. Y. Luo, L. Zhu, G. M. Lu, H. T. Shen. Modal-\nity-invariant asymmetric networks for cross-modal hash-\ning. IEEE Transactions on Knowledge and Data Engineer-\ning, to be published. DOI: 10.1109/TKDE.2022.3144352.\n[58]\n \n Ge-Peng Ji received the M. Sc. degree in\ncommunication  and  information  systems\nfrom Wuhan University, China in 2021. He\nis  currently  a  Ph.D.  degree  candidate  at\nAustralian  National  University,  super-\nvised by Professor Nick Barnes, majoring\nin  engineering  and  computer  science.  He\nhas  published  about  10  peer-reviewed\njournal and conference papers. In 2021, he\nreceived the Student Travel Award from Medical Image Com-\nputing and Computer-assisted Intervention Society.\n     His research interests lie in computer vision, especially in a\nvariety of dense prediction tasks, such as video analysis, medical\nimage segmentation, camouflaged object segmentation, and sali-\nency detection.\n     E-mail: gepengai.ji@gmail.com\n     ORCID iD: 0000-0001-7092-2877 \nG. P. Ji et al. / Masked Vision-language Transformer in Fashion 433 \n \n\nMingchen Zhuge received the M. Sc. de-\ngree in computer science from China Uni-\nversity of Geosciences, China in 2021. He is\na Ph. D. degree candidate in King Abdul-\nlah University of Science and Technology\n(KAUST)  under  the  supervision  of  Prof.\nJuergen Schmidhuber. In 2019, he won the\nchampionship in the ZTE algorithm com-\npetition.  He  has  worked  as  an  intern  at\nAlibaba  Group  and  IIAI,  as  well  as  a  visiting  scholar  at\nSUSTech. Besides, he has been invited to serve as a top confer-\nence reviewer for CVPR, ICML, ECCV, NeurIPS, etc.\n     His research interests include multi-modal learning and rein-\nforcement learning.\n     E-mail: mczhuge@gmail.com\n     ORCID iD: 0000-0003-2561-7712\n \n Dehong  Gao received the Ph. D. degree\nfrom  The  Hong  Kong  Polytechnic  Uni-\nversity, China in 2014. He is now working\nas an associate professor in Northwestern\nPolytechnical University, China.\n     His research interests include informa-\ntion  retrieval,  recommendation,  natural\nlanguage processing and machine learning.\n     E-mail: gaodehong_polyu@163.com, de-\nhong.gdh@alibaba-inc.com\n     ORCID iD: 0000-0002-6636-5702\n \n Deng-Ping  Fan  received  the  Ph. D.  de-\ngree from the Nankai University, China in\n2019. He joined Inception Institute of Arti-\nficial Intelligence (IIAI), UAE in 2019. He\nhas  published  about  50  top  journal  and\nconference papers such as TPAMI, IJCV,\nTIP, TNNLS, TMI, CVPR, ICCV, ECCV,\nIJCAI, etc. He won the Best Paper Final-\nist Award at IEEE CVPR 2019, the Best\nPaper Award Nominee at IEEE CVPR 2020. He was recognized\nas the CVPR 2019 outstanding reviewer with a special mention\naward, the CVPR 2020 outstanding reviewer, the ECCV 2020\nhigh-quality reviewer, and the CVPR 2021 outstanding review-\ner. He served as a program committee board (PCB) member of\nIJCAI 2022−2024, a senior program committee (SPC) member of\nIJCAI 2021, a program committee members (PC) of CAD&CG\n2021,  a  committee  member  of  China  Society  of  Image  and\nGraphics  (CSIG),  area  chair  in  NeurIPS  2021  Datasets  and\nBenchmarks Track, area chair in MICCAI2020 Workshop.\n     His research interests include computer vision, deep learning,\nand visual attention, especially the human vision on co-salient\nobject detection, RGB salient object detection, RGB-D salient\nobject detection, and video salient object detection.\n     E-mail: dengpfan@gmail.com (Corresponding author)\n     ORCID iD: 0000-0002-5245-7518\n \n Christos  Sakaridis  received  the  M. Sc.\ndegree  in  computer  science  from  ETH\nZürich, Switzerland in 2016 and his Dip-\nloma in electrical and computer engineer-\ning  from  the  National  Technical  Uni-\nversity of Athens, Greece in 2014, conduct-\ning his Diploma thesis at CVSP Group un-\nder the supervision of Prof. Petros Mara-\ngos. He received the Ph.D. degree in elec-\ntrical engineering and information technology from ETH Zürich,\nSwitzerland in 2021, working at Computer Vision Lab and su-\npervised by Prof. Luc Van Gool. He is a postdoctoral researcher\nat Computer Vision Lab, ETH Zürich, Switzerland. Since 2021,\nhe is the Principal Engineer in TRACE-Zürich, a project on com-\nputer vision for autonomous cars running at Computer Vision\nLab and funded by Toyota Motor Europe. Moreover, he is the\nteam leader in the EFCL project Sensor Fusion, in which they\ndevelop adaptive sensor fusion architectures for high-level visual\nperception.\n     His broad research fields are computer vision and machine\nlearning. The focus of his research is on high-level visual percep-\ntion, involving adverse visual conditions, domain adaptation, se-\nmantic segmentation, depth estimation, object detection, syn-\nthetic data generation, and fusion of multiple sensors (including\nlidar, radar and event cameras, with emphasis on their applica-\ntion to autonomous cars and robots).\n     E-mail: csakarid@vision.ee.ethz.ch\n     ORCID iD: 0000-0003-1127-8887\n \n Luc Van Gool received the Ph. D. degree\nin electromechanical engineering at Kath-\nolieke  Universiteit  Leuven,  Belgium  in\n1981. Currently, he is a professor at Kath-\nolieke Universiteit Leuven, in Belgium and\nETH  Zürich,  Switzerland.  He  leads  com-\nputer  vision  research  at  both  places  and\nalso teaches at both. He has been a pro-\ngram committee member of several major\ncomputer  vision  conferences.  He  received  several  Best  Paper\nawards, won a David Marr Prize and a Koenderink Award, and\nwas  nominated  Distinguished  Researcher  by  the  IEEE  Com-\nputer Science committee. He is a co-founder of 10 spin-off com-\npanies.\n     \nHis interests include 3D reconstruction and modeling, object\nrecognition,  tracking,  gesture  analysis,  and  a  combination  of\nthose.\n     \nE-mail: vangool@vision.ee.ethz.ch\n     ORCID iD: 0000-0002-3445-5711\n 434 Machine Intelligence Research 20(3), June 2023\n \n",
  "topic": "Transformer",
  "concepts": [
    {
      "name": "Transformer",
      "score": 0.8377759456634521
    },
    {
      "name": "Computer science",
      "score": 0.7872588634490967
    },
    {
      "name": "Architecture",
      "score": 0.6340944766998291
    },
    {
      "name": "Artificial intelligence",
      "score": 0.523104190826416
    },
    {
      "name": "Encoder",
      "score": 0.5098256468772888
    },
    {
      "name": "Modal",
      "score": 0.4946255385875702
    },
    {
      "name": "Computer vision",
      "score": 0.4286070466041565
    },
    {
      "name": "Generative grammar",
      "score": 0.42316934466362
    },
    {
      "name": "Natural language processing",
      "score": 0.3845755457878113
    },
    {
      "name": "Speech recognition",
      "score": 0.34043657779693604
    },
    {
      "name": "Engineering",
      "score": 0.09288862347602844
    },
    {
      "name": "Voltage",
      "score": 0.07532152533531189
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Chemistry",
      "score": 0.0
    },
    {
      "name": "Polymer chemistry",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I45928872",
      "name": "Alibaba Group (China)",
      "country": "CN"
    },
    {
      "id": "https://openalex.org/I35440088",
      "name": "ETH Zurich",
      "country": "CH"
    }
  ],
  "cited_by": 18
}