{
    "title": "Towards Reasoning in Large Language Models: A Survey",
    "url": "https://openalex.org/W4385571689",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2104755790",
            "name": "Jie Huang",
            "affiliations": [
                "University of Illinois Urbana-Champaign"
            ]
        },
        {
            "id": "https://openalex.org/A4201822862",
            "name": "Kevin Chen-Chuan Chang",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W4319997768",
        "https://openalex.org/W4302011807",
        "https://openalex.org/W4389520103",
        "https://openalex.org/W4293998609",
        "https://openalex.org/W3177813494",
        "https://openalex.org/W4306295121",
        "https://openalex.org/W4281250694",
        "https://openalex.org/W2794325560",
        "https://openalex.org/W4281690148",
        "https://openalex.org/W4385572867",
        "https://openalex.org/W4287758766",
        "https://openalex.org/W1937795883",
        "https://openalex.org/W4221151371",
        "https://openalex.org/W4281483047",
        "https://openalex.org/W4309953112",
        "https://openalex.org/W3177100526",
        "https://openalex.org/W3100436891",
        "https://openalex.org/W4288089799",
        "https://openalex.org/W3134642945",
        "https://openalex.org/W4281387391",
        "https://openalex.org/W4385574286",
        "https://openalex.org/W4210451781",
        "https://openalex.org/W4283815582",
        "https://openalex.org/W4385573855",
        "https://openalex.org/W4303648904",
        "https://openalex.org/W4221143046",
        "https://openalex.org/W4320087317",
        "https://openalex.org/W4307534981",
        "https://openalex.org/W3005742798",
        "https://openalex.org/W4319793302",
        "https://openalex.org/W4291991132",
        "https://openalex.org/W4294753225",
        "https://openalex.org/W4283768109",
        "https://openalex.org/W341374992",
        "https://openalex.org/W4389523706",
        "https://openalex.org/W4307079201",
        "https://openalex.org/W4385571045",
        "https://openalex.org/W4286892945",
        "https://openalex.org/W4310629099",
        "https://openalex.org/W4296556663",
        "https://openalex.org/W4298187912",
        "https://openalex.org/W4385570291",
        "https://openalex.org/W3170403598",
        "https://openalex.org/W4302305823",
        "https://openalex.org/W2962833140",
        "https://openalex.org/W3034723486",
        "https://openalex.org/W2997351066",
        "https://openalex.org/W4385573007",
        "https://openalex.org/W4320858367",
        "https://openalex.org/W4313483736",
        "https://openalex.org/W4311992335",
        "https://openalex.org/W4301259831",
        "https://openalex.org/W4320005767",
        "https://openalex.org/W4292779060",
        "https://openalex.org/W4312107795",
        "https://openalex.org/W4303441863",
        "https://openalex.org/W4312052651",
        "https://openalex.org/W4251428751",
        "https://openalex.org/W4280496127",
        "https://openalex.org/W1679534293",
        "https://openalex.org/W3122241445",
        "https://openalex.org/W3159959439",
        "https://openalex.org/W1985876015",
        "https://openalex.org/W4386566488",
        "https://openalex.org/W4306294746",
        "https://openalex.org/W4283026156",
        "https://openalex.org/W4311728219",
        "https://openalex.org/W4226369848",
        "https://openalex.org/W4385569588",
        "https://openalex.org/W4285428875",
        "https://openalex.org/W4281975731",
        "https://openalex.org/W4231857150",
        "https://openalex.org/W4390874280",
        "https://openalex.org/W4385567216",
        "https://openalex.org/W4226146865",
        "https://openalex.org/W3195577433",
        "https://openalex.org/W4305009356",
        "https://openalex.org/W3173798466",
        "https://openalex.org/W3102187933",
        "https://openalex.org/W4309591663",
        "https://openalex.org/W4224308101",
        "https://openalex.org/W4385571157",
        "https://openalex.org/W4385569771",
        "https://openalex.org/W4281557260",
        "https://openalex.org/W2963995027",
        "https://openalex.org/W4385571260",
        "https://openalex.org/W4311642023",
        "https://openalex.org/W3203211567",
        "https://openalex.org/W2962800603",
        "https://openalex.org/W4229005866",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4385573733",
        "https://openalex.org/W3034643750",
        "https://openalex.org/W4230677720",
        "https://openalex.org/W2996132992",
        "https://openalex.org/W4310625358",
        "https://openalex.org/W2251935656",
        "https://openalex.org/W4287854450",
        "https://openalex.org/W4385572965",
        "https://openalex.org/W4285594979",
        "https://openalex.org/W4385573569",
        "https://openalex.org/W2963899988",
        "https://openalex.org/W4221161695",
        "https://openalex.org/W3194456469",
        "https://openalex.org/W2949134692",
        "https://openalex.org/W4304194220",
        "https://openalex.org/W2087107422",
        "https://openalex.org/W2951939640",
        "https://openalex.org/W4296605665",
        "https://openalex.org/W2964120615",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4221152848",
        "https://openalex.org/W4366736258"
    ],
    "abstract": "Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",
    "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 1049–1065\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nTowards Reasoning in Large Language Models: A Survey\nJie Huang Kevin Chen-Chuan Chang\nDepartment of Computer Science, University of Illinois at Urbana-Champaign\n{jeffhj, kcchang}@illinois.edu\nAbstract\nReasoning is a fundamental aspect of human\nintelligence that plays a crucial role in activi-\nties such as problem solving, decision making,\nand critical thinking. In recent years, large\nlanguage models (LLMs) have made signifi-\ncant progress in natural language processing,\nand there is observation that these models may\nexhibit reasoning abilities when they are suf-\nficiently large. However, it is not yet clear to\nwhat extent LLMs are capable of reasoning.\nThis paper provides a comprehensive overview\nof the current state of knowledge on reasoning\nin LLMs, including techniques for improving\nand eliciting reasoning in these models, meth-\nods and benchmarks for evaluating reasoning\nabilities, findings and implications of previous\nresearch in this field, and suggestions on future\ndirections. Our aim is to provide a detailed and\nup-to-date review of this topic and stimulate\nmeaningful discussion and future work.1\n1 Introduction\nReasoning is a cognitive process that involves using\nevidence, arguments, and logic to arrive at conclu-\nsions or make judgments. It plays a central role in\nmany intellectual activities, such as problem solv-\ning, decision making, and critical thinking. The\nstudy of reasoning is important in fields like psy-\nchology (Wason and Johnson-Laird, 1972), philoso-\nphy (Passmore, 1961), and computer science (Huth\nand Ryan, 2004), as it helps individuals make deci-\nsions, solve problems, and think critically.\nRecently, large language models (LLMs)\n(Brown et al., 2020; Chowdhery et al., 2022; Chung\net al., 2022; OpenAI, 2022,inter alia) such as Chat-\nGPT have made significant advancements in natu-\nral language processing and related fields. It has\nbeen shown that these models exhibit emergent be-\nhaviors, including the ability to “reason”, when\n1Paperlist can be found at https://github.com/\njeffhj/LM-reasoning.\nthey are large enough (Wei et al., 2022a). For ex-\nample, by providing the models with “ chain of\nthoughts”, i.e., reasoning exemplars, or a simple\nprompt “Let’s think step by step ”, these models\nare able to answer questions with explicit reason-\ning steps (Wei et al., 2022b; Kojima et al., 2022),\ne.g., “all whales are mammals, all mammals have\nkidneys; therefore, all whales have kidneys.” This\nhas sparked considerable interest in the commu-\nnity since reasoning ability is a hallmark of human\nintelligence that is frequently considered missed\nin current artificial intelligence systems (Marcus,\n2020; Russin et al., 2020; Mitchell, 2021; Bom-\nmasani et al., 2021).\nHowever, despite the strong performance of\nLLMs on certain reasoning tasks, it remains unclear\nwhether LLMs are actually reasoning and to what\nextent they are capable of reasoning. For exam-\nple, Kojima et al. (2022) claim that “LLMs are de-\ncent zero-shot reasoners (p. 1)”, while Valmeekam\net al. (2022) conclude that “LLMs are still far\nfrom achieving acceptable performance on com-\nmon planning/reasoning tasks which pose no issues\nfor humans to do (p. 2).” This limitation is also\nstated by Wei et al. (2022b):\n“we qualify that although chain of thought emu-\nlates the thought processes of human reasoners,\nthis does not answer whether the neural network\nis actually reasoning (p. 9).”\nTherefore, in this paper, we aim to provide a\ncomprehensive overview and engage in an insight-\nful discussion on the current state of knowledge on\nthis fast-evolving topic. We initiate our exploration\nwith a clarification of the concept of reasoning (§2).\nSubsequently, we turn our attention to the tech-\nniques for enhancing/eliciting reasoning in LLMs\n(§3), the methods and benchmarks for evaluating\nreasoning in LLMs (§4), and the key findings and\nimplications in this field (§5). Finally, we reflect\non and discuss the current state of the field (§6).\n1049\nReasoning in LLMs\nTechniques (§3)\nFully Supervised Finetuning (§3.1)\nPrompting & In-Context Learning (§3.2)\nChain of Thought and Its Variants (§3.2.1)\nRationale Engineering (§3.2.2)\nProblem Decomposition (§3.2.3)\nOthers (§3.2.4)\nHybrid Method (§3.3)\nReasoning-Enhanced Training & Prompting (§3.3.1)\nBootstrapping & Self-Improving (§3.3.2)\nEvaluation & Analysis (§4)\nEnd Task Performance (§4.1)\nAnalysis on Reasoning (§4.2)\nFindings & Implications (§5)\nReflection, Discussion & Future Directions (§6)\nFigure 1: The structure of the paper.\n2 What is Reasoning?\nReasoning is the process of thinking about some-\nthing in a logical and systematic way, using evi-\ndence and past experiences to reach a conclusion or\nmake a decision (Wason and Johnson-Laird, 1972;\nWason, 1968; Galotti, 1989; Fagin et al., 2004;\nMcHugh and Way, 2018). Reasoning involves mak-\ning inferences, evaluating arguments, and drawing\nlogical conclusions based on available information.\nAlthough “reasoning” is a term that is commonly\nused in literature and daily life, it is also an abstract\nconcept that can refer to many things. To help the\nreader better understand this concept, we summa-\nrize several main categories of reasoning that are\ncommonly recognized:\nDeductive reasoning. Deductive reasoning is a\ntype of reasoning in which a conclusion is drawn\nbased on the truth of the premises. In deductive\nreasoning, the conclusion must necessarily follow\nfrom the premises, meaning that if the premises are\ntrue, the conclusion must also be true. For example:\n• Premise: All mammals have kidneys.\n• Premise: All whales are mammals.\n• Conclusion: All whales have kidneys.\nInductive reasoning. Inductive reasoning is a type\nof reasoning in which a conclusion is drawn based\non observations or evidence. The conclusion is\nlikely to be true based on the available evidence,\nbut it is not necessarily certain. For example:\n• Observation: Every time we see a creature\nwith wings, it is a bird.\n• Observation: We see a creature with wings.\n• Conclusion: The creature is likely to be a bird.\nAbductive reasoning. Abductive reasoning is a\ntype of reasoning in which a conclusion is drawn\nbased on the best explanation for a given set of\nobservations. The conclusion is the most likely\nexplanation based on the available evidence, but it\nis not necessarily certain. For example:\n• Observation: The car cannot start and there is\na puddle of liquid under the engine.\n• Conclusion: The most likely explanation is\nthat the car has a leak in the radiator.\nOther types of reasoning includeanalogical reason-\ning, which involves making comparisons between\ntwo or more things in order to make inferences\nor arrive at conclusions; causal reasoning, which\ninvolves identifying and understanding the causes\nand effects of events or phenomena; andprobabilis-\ntic reasoning, which involves making decisions or\narriving at conclusions based on the likelihood or\nprobability of certain outcomes.\nFormal Reasoning vs Informal Reasoning. For-\nmal reasoning is a systematic and logical process\nthat follows a set of rules and principles, often used\nin mathematics and logic. Informal reasoning is a\nless structured approach that relies on intuition, ex-\nperience, and common sense to draw conclusions\nand solve problems, and is often used in everyday\nlife. Formal reasoning is more structured and reli-\nable, while informal reasoning is more adaptable\nand open-ended, but may also be less reliable. We\nrefer the reader to Galotti (1989); Bronkhorst et al.\n(2020) for a detailed distinction between them.\nReasoning in Language Models. The concept of\nreasoning in language models has been around for\nsome time, but there is not a clear definition of\nwhat it entails. In the literature, the term “reason-\ning” is often used to refer to informal reasoning,\nalthough it is not always explicitly stated that it\nis informal (Cobbe et al., 2021; Wei et al., 2022b,\n1050\ninter alia). Different forms of reasoning may be\nused depending on the task, benchmark, or method\nbeing used, e.g., deductive reasoning (Cobbe et al.,\n2021; Creswell et al., 2022; Han et al., 2022b, in-\nter alia), inductive reasoning (Yang et al., 2022;\nMisra et al., 2022, inter alia) or abductive reason-\ning (Wiegreffe et al., 2022; Lampinen et al., 2022;\nJung et al., 2022, inter alia). In this paper, we\nencompass various forms of reasoning, with a par-\nticular focus on “informal deductive reasoning” in\nlarge language models since it is a widely used\nform in which the conclusion is guaranteed to be\ntrue as long as the premises are true.\n3 Towards Reasoning in Large Language\nModels\nReasoning, particularly multi-step reasoning, is of-\nten seen as a weakness in language models and\nother NLP models (Bommasani et al., 2021; Rae\net al., 2021; Valmeekam et al., 2022). Recent re-\nsearch has suggested that reasoning ability may\nemerge in language models at a certain scale, such\nas models with over 100 billion parameters (Wei\net al., 2022a,b; Cobbe et al., 2021). In this paper,\nwe follow Wei et al. (2022a) in considering rea-\nsoning as an ability that is rarely present in small-\nscale models like GPT-2 (Radford et al., 2019) and\nBERT (Devlin et al., 2019), and therefore focus\non techniques applicable to improving or eliciting\n“reasoning”2 in LLMs such as GPT-3 (Brown et al.,\n2020) and PaLM (Chowdhery et al., 2022).\n3.1 Fully Supervised Finetuning\nBefore discussing reasoning in large language mod-\nels, it is worth mentioning there is research work-\ning on eliciting/improving reasoning in small lan-\nguage models through fully supervised finetuning\non specific datasets. For example, Rajani et al.\n(2019) finetune a pretrained GPT model (Radford\net al., 2018) to generate rationales that explain\nmodel predictions with the built CoS-E dataset,\nand find that models trained with explanations\nperform better on commonsense question answer-\ning tasks (Talmor et al., 2019). Talmor et al.\n(2020) train RoBERTa (Liu et al., 2019) to per-\nform reasoning/inference based on both implicit\npre-trained knowledge and explicit free-text state-\nments. Hendrycks et al. (2021) finetune pretrained\n2It is important to note that the term “reasoning” in this\npaper does not necessarily imply that LLMs are truly capable\nof reasoning or that they are able to reason in the same way\nthat humans do. We will discuss this issue in more detail in §6.\nlanguage models to solve competition mathematics\nproblems by generating full step-by-step solutions,\nthough the accuracy is relatively low. Nye et al.\n(2022) train language models to do multi-step rea-\nsoning for program synthesis/execution by generat-\ning “scratchpads”, i.e., intermediate computations,\nbefore producing the final answers. We refer the\nreader to Helwe et al. (2021); Bhargava and Ng\n(2022)’s survey for more studies in this line.\nThere are two major limitations of fully super-\nvised finetuning. First, it requires a dataset contain-\ning explicit reasoning, which can be difficult and\ntime-consuming to create. Additionally, the model\nis only trained on a specific dataset, which limits\nits application to a specific domain and may result\nin the model relying on artifacts in the training data\nrather than actual reasoning to make predictions.\n3.2 Prompting & In-Context Learning\nLarge language models such as GPT-3 (Brown\net al., 2020) have demonstrated remarkable few-\nshot performance across a variety of tasks through\nin-context learning. These models can be prompted\nwith a question and a few⟨input, output⟩ exemplars\nto potentially solve a problem through “reasoning”,\neither implicitly or explicitly. However, research\nhas shown that these models still fall short when\nit comes to tasks that require multiple steps of rea-\nsoning to solve (Bommasani et al., 2021; Rae et al.,\n2021; Valmeekam et al., 2022). This may be due\nto a lack of exploration into the full capabilities of\nthese models, as recent studies have suggested.\n3.2.1 Chain of Thought and Its Variants\nTo encourage LLMs to engage in reasoning rather\nthan simply providing answers directly, we may\nguide LLMs to generate “reasoning” explicitly.\nOne approach for doing this is chain-of-thought\nprompting, proposed by Wei et al. (2022b). This\napproach involves providing a few examples of\n“chain of thought” (CoT), which are intermediate\nnatural language reasoning steps, in the prompt to\nLLMs (Figure 2). Specifically, in CoT prompting,\n⟨input, output⟩ demonstrations are replaced with\n⟨input, chain of thought, output⟩ triples, e.g., “[in-\nput] Roger has 5 tennis balls. He buys 2 more cans\nof tennis balls. Each can has 3 tennis balls. How\nmany tennis balls does he have now? [ chain of\nthought] Roger started with 5 balls. 2 cans of 3\ntennis balls each is 6 tennis balls. 5 + 6 = 11. [out-\nput] The answer is 11.” In this way, given a target\nquestion, the model learns to generate explicit ratio-\n1051\nLLMRationale* 1Input A, Rationale A, Output A\nOutput*ExemplarsInput B, Rationale B, Output BInput C, Rationale C, Output C\nRationale Refinement\nRationale* 2Rationale* 3Rationale Exploration\nRationale Verification\n[Input]Rogerhas5tennisballs.Hebuys2morecansoftennisballs.Eachcanhas3tennisballs.Howmanytennisballsdoeshehavenow?[Rationale]Rogerstartedwith5balls.2cansof3tennisballseachis6tennisballs.5+6=11.[Output]Theansweris11.\nInput*\nFigure 2: An illustration of Chain-of-Thought Prompting and Rationale Engineering, where asterisk (*) denotes the\ntarget problem to be solved.\nnale before producing the final answer. Experimen-\ntal results show that this simple idea can improve\nLLMs’ few-shot performance on arithmetic, sym-\nbolic, and commonsense reasoning tasks, some-\ntimes to a striking degree.\nThere are several variants of chain-of-thought\nprompting that have been proposed in the literature,\nin a different form or to solve a specific problem.\nDifferent Form: Kojima et al. (2022) intro-\nduce Zero-shot-CoT, in which LLMs are simply\nprompted with the phrase “Let’s think step by step”\nafter the input, in order to elicit reasoning without\nthe need for few-shot demonstrations. Madaan et al.\n(2022); Gao et al. (2022); Chen et al. (2022) find\nthat LLMs trained with code, e.g., Codex (Chen\net al., 2021), can achieve better performance on\nreasoning tasks by framing reasoning as code gen-\neration. Wang et al. (2022a) propose to iteratively\nprompt chain of thought. He et al. (2023) attempt\nto retrieve external knowledge in CoT to improve\nfaithfulness of reasoning.\nSpecific Problem/Setting: Before chain of\nthought, Nye et al. (2022) also try to use intermedi-\nate computations, named “scratchpads”, to improve\nlanguage models’ reasoning performance in both\nfinetuning and few-shot regimes, with a particular\nfocus on programs. Shi et al. (2022) attempt to\nsolve multilingual reasoning tasks with CoT in the\nnative language, CoT in English (regardless of the\nproblem language), and CoT in English (with the\nproblem translated to English). Chen (2022) apply\nCoT to table-based reasoning, finding that LLMs\ncan achieve strong performance on table tasks with\nonly one exemplar. Prystawski et al. (2022) demon-\nstrate that CoT can improve LLMs’ performance\non paraphrase selection for metaphors. Lu et al.\n(2022) apply chain of thought to solve multimodal\nscience questions.\n3.2.2 Rationale Engineering\nThe original version of chain-of-thought prompting,\nproposed by Wei et al. (2022b), relies on manually\ncrafted examples of intermediate reasoning steps\nand applies greedy decoding in the generation. Ra-\ntionale engineering aims to more effectively elicit\nor utilize reasoning in LLMs. This can be achieved\nthrough rationale refinement, which involves cre-\nating more effective examples of reasoning steps,\nor through rationale exploration and rationale ver-\nification, which involve exploring and verifying\nthe rationales produced by LLMs. A summary of\nraltionale engineering is illustrated in Figure 2.\nRationale refinement. The choice of exemplars\ncan significantly affect the few-shot performance of\nLLMs, as demonstrated in research such as Liu et al.\n(2022b), which also appears in chain-of-thought\nprompting. Rationale refinement aims to create\nand refine rationale examples that are better able to\nelicit reasoning in LLMs. Fu et al. (2022b) propose\ncomplexity-based prompting to create rationales\nwith more reasoning steps. Their experiments show\nthat the performance of LLMs improves with the in-\ncreased rationale complexity. Similarly, Zhou et al.\n(2022c) propose algorithmic prompting, which sug-\ngests that providing more thorough examples of\nsolutions can help improve reasoning performance\non some simple math calculations. Zhang et al.\n(2022b) design Auto-CoT to automatically con-\nstruct exemplars by partitioning questions from\na given dataset into clusters and then using Zero-\nShot-CoT (Kojima et al., 2022) to generate the\nrationale for a representative question from each\n1052\ncluster. The analysis shows that making exemplars\ndiverse is important in prompting LLMs to produce\nbetter rationales.\nRationale exploration. In addition to providing\nbetter exemplars, we can allow LLMs to fully ex-\nplore various ways of reasoning to improve their\nperformance on reasoning tasks, named rationale\nexploration. Based on the idea that complex prob-\nlems often admit multiple ways of thinking that\ncan lead to their unique correct answer, Wang et al.\n(2022c) present a decoding strategy called self-\nconsistency to improve upon the traditional greedy\ndecoding used in chain-of-thought prompting. This\nstrategy involves sampling a diverse set of ratio-\nnales, rather than just the greedy one, and selecting\nthe most consistent answer by marginalizing out\nthe sampled rationales. The idea is also used in\nFu et al. (2022b) to vote over the top complex ra-\ntionales. To further improve performance, Li et al.\n(2022b) suggest providing different demonstrations\nfor each question by sampling exemplars from an\nexemplar base, in order to increase the diversity of\nthe sampled rationales.\nRationale verification. Ensuring that the ratio-\nnales produced by LLMs are valid is critical, as in-\ncorrect rationales can lead to incorrect final predic-\ntions (Ye and Durrett, 2022). To address this issue,\nthe process of rationale verification aims to verify\nwhether the rationales produced by LLMs lead to\nthe correct final answers. Cobbe et al. (2021) pro-\npose augmenting LLMs with a trained verifier that\nassigns a score to each rationale and solution gen-\nerated by the LLM, selecting the highest-ranked\nsolution as the final answer when solving math\nword problems. Li et al. (2022b) also use this tech-\nnique to guide rationale selection, in conjunction\nwith the process of rationale exploration. Differ-\nent from the above methods that train an external\nverifier to verify the rationales, Weng et al. (2022)\nsuggest using LLMs themselves as the verifiers.\n3.2.3 Problem Decomposition\nChain-of-thought prompting, while effective for\neliciting reasoning in LLMs, can struggle with com-\nplex tasks, e.g., tasks that require compositional\ngeneralization (Lake and Baroni, 2018; Keysers\net al., 2020). To solve a complex problem, it is\nhelpful to first break it down into smaller, more\nmanageable subproblems. By solving each of these\nsubproblems, we can effectively solve the complex\nproblem. This technique is called problem decom-\nposition or divide and conquer (Talmor and Berant,\n2018; Min et al., 2019; Perez et al., 2020).\nBased on this idea, Zhou et al. (2022a) pro-\npose least-to-most prompting, which consists of\ntwo steps: decomposing the complex problem into\nsubproblems and solving these subproblems in a\nspecific order, with each subproblem being facil-\nitated by the answers obtained from previously\nsolved subproblems. As follow-up work, Droz-\ndov et al. (2022) introduce dynamic least-to-most\nprompting, which is designed to solve more realis-\ntic semantic parsing problems by decomposing the\nproblems with prompting-based syntactic parsing\nand dynamically selecting exemplars based on the\ndecomposition. In addition, Khot et al. (2022) de-\nsign decomposed prompting, which breaks down\na complex problem into subproblems that can be\nhandled by a shared library of prompting-based\nLLMs, each specialized in a particular subprob-\nlem. Furthermore, Dua et al. (2022) develop suc-\ncessive prompting, which iteratively decomposes a\ncomplex problem into a simple problem, with the\nnext subproblem prediction having access to the\nanswers to the previous subproblems. While the\nabove methods decompose or solve compositional\nquestions with multiple forward passes, Press et al.\n(2022) suggest decomposing and solving the input\nquestion in one forward pass using CoT prompting.\nOverall, these techniques show promise for helping\nLLMs to solve complex tasks by decomposing the\nproblem into more manageable subproblems.\n3.2.4 Others\nThere are other techniques that have been devel-\noped to facilitate reasoning in LLMs for specific\ntasks or settings. For instance, Creswell et al.\n(2022); Creswell and Shanahan (2022) introduce a\nselection-inference framework that uses LLMs as\nmodules to select and infer reasoning steps from\na set of facts that culminate in the final answer.\nKazemi et al. (2022) suggest using backward chain-\ning, i.e., from goal to the set of facts that support\nit, instead of forward chaining like Creswell et al.\n(2022); Creswell and Shanahan (2022). In addition,\nJung et al. (2022) propose a method for solving\nbinary questions by prompting LLMs abductively\nand recursively to rationalize each option. Zhou\net al. (2022b) design a technique for performing\nnumerical reasoning on complex numbers by re-\nplacing the complex numbers with simple numbers\nto produce simpler expressions, and then using\nthese expressions to perform calculations on the\n1053\ncomplex numbers. There are also efforts to distill\nreasoning from LLMs into smaller models, such\nas the work by Li et al. (2022a); Shridhar et al.\n(2022); Magister et al. (2022). Finally, we refer the\nreader to Dohan et al. (2022)’s position paper on\nlanguage model cascade, which presents a unify-\ning framework for understanding chain-of-thought\nprompting and research in this line.\n3.3 Hybrid Method\nWhile “prompting” techniques can help elicit or\nbetter utilize reasoning in large language models\nto solve reasoning tasks, they do not actually im-\nprove the reasoning capabilities of the LLMs them-\nselves, as the parameters of the models remain un-\nchanged. In contrast, the “hybrid approach” aims to\nsimultaneously improve the reasoning capabilities\nof LLMs and make better use of these models in\norder to solve complex problems. This approach in-\nvolves both enhancing the reasoning capabilities of\nthe LLMs and using techniques such as prompting\nto effectively utilize these capabilities.\n3.3.1 Reasoning-Enhanced Training and\nPrompting\nOne approach to improving the reasoning capabili-\nties of LLMs is to pretrain or finetune the models\non datasets that include “reasoning”. Lewkowycz\net al. (2022); Taylor et al. (2022) find that LLMs\ntrained on datasets containing scientific and math-\nematical data can achieve better performance on\nreasoning tasks like quantitative reasoning prob-\nlems when using CoT prompting3. Pi et al. (2022)\nshow that continually pretraining with SQL data\ncan boost the performance of language models, e.g.,\nT5 (Raffel et al., 2020), on natural language rea-\nsoning such as numerical reasoning and logical rea-\nsoning. Furthermore, Chung et al. (2022) develop\nFlan models by finetuning PaLM (Chowdhery et al.,\n2022) and T5 (Raffel et al., 2020) with 1.8k fine-\ntuning tasks, including CoT data, and find that\nCoT data are critical to keeping reasoning abilities.\nSimilarly, Yu et al. (2022) finetune OPT (Zhang\net al., 2022a) on 10 reasoning datasets and observe\nthat it can improve some reasoning capabilities of\nLLMs. Anil et al. (2022) study the length gener-\nalization abilities of LLMs, i.e., whether LLMs\nlearned with short problem instances can general-\nize to long ones. They discover that the combina-\ntion of few-shot scratchpad (or chain of thought)\n3This may also be true for models trained with code (Chen\net al., 2021; Fu et al., 2022a).\nfinetuning and scratchpad prompting results in a\nsignificant improvement in LLMs’ ability to gener-\nalize to longer problems, while this phenomenon\nis not observed in the standard fully supervised\nfinetuning paradigm.\n3.3.2 Bootstrapping & Self-Improving\nInstead of finetuning LLMs on pre-built datasets\nthat include reasoning, there are studies that have\nexplored the idea of using LLMs to self-improve\ntheir reasoning abilities through a process known\nas bootstrapping. One example of this is the Self-\nTaught Reasoner (STaR)introduced by Zelikman\net al. (2022), in which a LLM is trained and refined\non its own output iteratively. Specifically, with CoT\nprompting, the model first generates initial ratio-\nnales. And then, the model is finetuned on ratio-\nnales that lead to correct answers. This process can\nbe repeated, with each iteration resulting in an im-\nproved model that can generate better training data,\nwhich in turn leads to further improvements. As a\nfollow-up to this work, Huang et al. (2022a) show\nthat LLMs are able to self-improve their reasoning\nabilities without the need for supervised data by\nleveraging the self-consistency of reasoning (Wang\net al., 2022c).\n4 Measuring Reasoning in Large\nLanguage Models\nWe summarize methods and benchmarks for evalu-\nating reasoning abilities of LLMs in this section.\n4.1 End Task Performance\nOne way to measure reasoning abilities of LLMs is\nto report their performance, e.g., accuracy, on end\ntasks that require reasoning. We list some common\nbenchmarks as follows.\nArithmetic Reasoning. Arithmetic reasoning is\nthe ability to understand and apply mathemat-\nical concepts and principles in order to solve\nproblems involving arithmetic operations. This\ninvolves using logical thinking and mathemat-\nical principles to determine the correct course\nof action when solving mathematical problems.\nRepresentative benchmarks for arithmetic rea-\nsoning include GSM8K (Cobbe et al., 2021),\nMath (Hendrycks et al., 2021), MathQA (Amini\net al., 2019), SV AMP (Patel et al., 2021), AS-\nDiv (Miao et al., 2020), AQuA (Ling et al., 2017),\nand MAWPS (Roy and Roth, 2015). It is worth\n1054\nmentioning that Anil et al. (2022) generate the Par-\nity Datasets and the Boolean Variable Assignment\nDataset for analyzing the length generalization ca-\npabilities of LLMs (§3.3.1).\nCommonsense Reasoning. Commonsense Rea-\nsoning is the use of everyday knowledge and under-\nstanding to make judgments and predictions about\nnew situations. It is a fundamental aspect of human\nintelligence that enables us to navigate our envi-\nronment, understand others, and make decisions\nwith incomplete information. Benchmarks that can\nbe used for testing commonsense reasoning abili-\nties of LLMs include CSQA (Talmor et al., 2019),\nStrategyQA (Geva et al., 2021), and ARC (Clark\net al., 2018). We refer the reader to Bhargava and\nNg (2022)’s survey for more work in this domain.\nSymbolic Reasoning. Symbolic reasoning is a\nform of reasoning that involves the manipulation\nof symbols according to formal rules. In symbolic\nreasoning, we use abstract symbols to represent\nconcepts and relationships, and then manipulate\nthose symbols according to precise rules in order\nto draw conclusions or solve problems. Two bench-\nmarks of symbolic reasoning are presented in Wei\net al. (2022b), including Last Letter Concatenation\nand Coin Flip.\nOthers. In practice, there are many benchmarks\nthat can be used to evaluate reasoning abilities\nof LLMs (indirectly), as long as the downstream\ntask involves reasoning. BIG-bench (Srivastava\net al., 2022), for example, includes over 200 tasks\nthat test a range of reasoning skills, including\ntasks like Date Understanding, Word Sorting, and\nCausal Judgement. Other benchmarks, such as\nSCAN (Lake and Baroni, 2018) and the one pro-\nposed by Anil et al. (2022), focus on evaluating\ngeneralization ability. LLMs can also be tested on\ntheir table reasoning abilities using benchmarks\nsuch as WikiTableQA (Pasupat and Liang, 2015),\nFetaQA (Nan et al., 2022), as suggested by Chen\n(2022). In addition, there are benchmarks for eval-\nuating LLMs’ generative relational reasoning abil-\nities, such as CommonGen (Lin et al., 2020; Liu\net al., 2022a) and Open Relation Modeling (Huang\net al., 2022b,d).\n4.2 Analysis on Reasoning\nAlthough LLMs have demonstrated impressive per-\nformance on various reasoning tasks, the extent to\nwhich their predictions are based on true reasoning\nor simple heuristics is not always clear. This is\nbecause most existing evaluations focus on their ac-\ncuracy on end tasks, rather than directly assessing\ntheir reasoning steps. While some error analysis\nhas been conducted on the generated rationales of\nLLMs (Wei et al., 2022b; Kojima et al., 2022,inter\nalia), this analysis has often been limited in depth.\nThere have been some efforts to develop metrics\nand benchmarks that enable a more formal/deep\nanalysis of reasoning in LLMs. Golovneva et al.\n(2022) design ROSCOE, a set of interpretable, de-\ntailed step-by-step evaluation metrics covering vari-\nous perspectives including semantic alignment, log-\nical inference, semantic similarity, and language\ncoherence. Saparov and He (2022) create a syn-\nthetic dataset called PrOntoQA that is generated\nfrom real or fictional ontologies. Each example\nin the dataset has a unique proof, which can be\nconverted to simple sentences and back again, al-\nlowing for a formal analysis of each reasoning step.\nHan et al. (2022a) introduce a dataset called FO-\nLIO to test the first-order logic reasoning capabil-\nities of LLMs. FOLIO contains first-order logic\nreasoning problems that require models to deter-\nmine the correctness of conclusions given a set of\npremises. In addition, Wang et al. (2022b) conduct\nablation experiments on CoT and find that LLMs\nmay also perform reasoning while prompting with\ninvalid rationals. Their study also suggests that be-\ning relevant to the query and correctly ordering the\nreasoning steps are important for CoT prompting.\nIn summary, most existing studies primarily re-\nport the performance of the models on downstream\nreasoning tasks, without a detailed examination of\nthe quality of the rationales produced. This leaves\nopen the question of whether the models are ac-\ntually able to reason in a way that is similar to\nhuman reasoning, or whether they are simply able\nto achieve good performance on the tasks through\nother means. Further research is needed to more\nformally analyze the reasoning abilities of LLMs.\n5 Findings and Implications\nIn this section, we summarize the important find-\nings and implications of studies on reasoning in\nlarge language models.\nReasoning seems an emergent ability of LLMs.\nWei et al. (2022a,b); Suzgun et al. (2022) show that\nreasoning ability appears to emerge only in large\nlanguage models like GPT-3 175B, as evidenced by\nsignificant improvements in performance on rea-\nsoning tasks at a certain scale (e.g., 100 billion\n1055\nparameters). This suggests that it may be more ef-\nfective to utilize large models for general reasoning\nproblems rather than training small models for spe-\ncific tasks. However, the reason for this emergent\nability is not yet fully understood. We refer the\nreader to Wei et al. (2022a); Fu et al. (2022a) for\nsome potential explanations.\nChain of thought elicits “reasoning” of LLMs.\nThe use of chain-of-thought (CoT) prompts (Wei\net al., 2022b) has been shown to improve the per-\nformance of LLMs on various reasoning tasks,\nas demonstrated in the experiments of Wei et al.\n(2022a,b); Suzgun et al. (2022). Additionally,\nSaparov and He (2022) (§4.2) find that, when us-\ning CoT prompts, LLMs are able to produce valid\nindividual proof steps, even when the synthetic on-\ntology is fictional or counterfactual. However, they\nmay sometimes choose the wrong steps when mul-\ntiple options are available, leading to incomplete\nor incorrect proofs. Moreover, for many reasoning\ntasks where the performance of standard prompting\ngrows smoothly with model scale, chain-of-thought\nprompting can lead to dramatic performance im-\nprovement. In addition to these benefits, the use of\nCoT prompts has been shown to improve the out-of-\ndistribution robustness of LLMs (Wei et al., 2022b;\nZhou et al., 2022a; Anil et al., 2022, inter alia),\nan advantage that is not typically observed with\nstandard prompting or fully supervised finetuning\nparadigms.\nLLMs show human-like content effects on rea-\nsoning. According to Dasgupta et al. (2022), LLMs\nexhibit reasoning patterns that are similar to those\nof humans as described in the cognitive literature.\nFor example, the models’ predictions are influ-\nenced by both prior knowledge and abstract rea-\nsoning, and their judgments of logical validity are\nimpacted by the believability of the conclusions.\nThese findings suggest that, although language\nmodels may not always perform well on reasoning\ntasks, their failures often occur in situations that\nare challenging for humans as well. This provides\nsome evidence that language models may “reason”\nin a way that is similar to human reasoning.\nLLMs are still unskilled at complex reasoning.\nAlthough LLMs seem to possess impressive rea-\nsoning capabilities with the techniques described\nin §3, they still struggle with more complex rea-\nsoning tasks or those involving implicature, accord-\ning to studies such as Valmeekam et al. (2022);\nHan et al. (2022a); Ruis et al. (2022). For in-\nstance, Valmeekam et al. (2022) find that even in\nrelatively simple commonsense planning domains\nthat humans would have no trouble navigating,\nLLMs such as GPT-3 (Brown et al., 2020) and\nBLOOM (Scao et al., 2022) struggle to perform\neffectively. These findings suggest that existing\nbenchmarks may be too simple to accurately gauge\nthe true reasoning abilities of LLMs, and that more\nchallenging tasks may be needed to fully evaluate\ntheir abilities in this regard.\n6 Reflection, Discussion, and Future\nDirections\nWhy reasoning? Reasoning is the process of think-\ning about something in a logical and systematic\nway, and it is a key aspect of human intelligence.\nBy incorporating reasoning capabilities into lan-\nguage models, we can enable them to perform tasks\nthat require more complex and nuanced thinking,\nsuch as problem solving, decision making, and\nplanning (Huang et al., 2022e,f; Song et al., 2022).\nThis can improve the performance of these mod-\nels on downstream tasks and increase their out-of-\ndistribution robustness (Wei et al., 2022a,b; Suzgun\net al., 2022; Zhou et al., 2022a; Anil et al., 2022).\nIn addition, reasoning can make language models\nmore explainable and interpretable, as it provides\nexplicit rationales for their predictions.\nRight task/application? As Valmeekam et al.\n(2022) point out, current benchmarks may not ade-\nquately reflect the reasoning capabilities of LLMs.\nIn addition, tasks such as solving simple math prob-\nlems and concatenating letters in strings (§4.1) are\nartificial and do not accurately reflect real-world\nsituations. To truly understand the reasoning ability\nof LLMs, it is important to consider more realistic\nand meaningful applications such as decision mak-\ning (Edwards, 1954), legal reasoning (Levi, 2013),\nand scientific reasoning (Zimmerman, 2000). Our\nultimate goal should not be to enable LLMs to solve\nsimple math problems, which can be simply done\nwith other programs. When conducting relevant\nresearch, it is essential to ask whether the specific\ntask being tackled is meaningful and whether the\nproposed method can be generalized to more real-\nistic tasks and applications.\nAre language models really able to reason?\nThere are several indications that LLMs are able\nto reason, including 1) high performance on vari-\nous tasks requiring reasoning (Suzgun et al., 2022);\n1056\n2) the ability to reason step-by-step with chain-\nof-thought prompting (Wei et al., 2022b); and 3)\nthe reflection of human-like content effects on rea-\nsoning (Dasgupta et al., 2022). However, these\nfindings are not sufficient to conclude that LLMs\ncan truly reason. For 1), it is not clear whether the\nmodels are making predictions based on reasoning\nor heuristics (Patel et al., 2021). For many existing\nbenchmarks on reasoning, actually, we can design\na program with heuristic rules to achieve very high\nperformance. We usually do not think a program\nrelying on heuristic rules is capable of reasoning.\nFor 2), although the models seem to reason step-\nby-step, the generated rationales may be incorrect\nand inconsistent. It is possible that the models are\n“generating reasoning-like response” rather than\n“reasoning step-by-step”. For 3), while LLMs dis-\nplay some human-like reasoning patterns, this does\nnot necessarily mean that they behave like humans.\nAdditionally, there are several observations that\nsuggest LLMs may not be capable of reasoning:\n1) LLMs still struggle with tasks that require com-\nplex reasoning (Valmeekam et al., 2022; Han et al.,\n2022a; Ruis et al., 2022). If LLMs are really de-\ncent reasoners, they should handle tasks that can\nbe simply solved by humans through reasoning;\n2) LLMs make mistakes in their reasoning, as ex-\nplained above; 3)#4 The performance of LLMs on\ndownstream tasks has been found to be sensitive to\nthe frequency of certain terms, such as numbers, in\nthe training data (Razeghi et al., 2022; Jung et al.,\n2022), which would not be expected if the models\nwere solving mathematical problems through rea-\nsoning; 4)# Language models have been found to\nstruggle with associating relevant information that\nthey have memorized (Huang et al., 2022c).\nOverall, it is still too early to draw a conclusion\nabout the proposed question. In fact, there is also an\nongoing debate about whether language models can\nactually understand language or capture meaning\n(Bender and Koller, 2020; Li et al., 2021; Manning,\n2022; Piantasodi and Hill, 2022). Further in-depth\nanalysis of factors such as training data, model\narchitecture, and optimization objectives is needed,\nas well as the development of better benchmarks\nfor measuring the reasoning capabilities of LLMs.\nHowever, it is clear that the current models are not\nyet capable of robust reasoning.\nImproving reasoning capabilities of LLMs.\n4 #indicates the finding has not been carefully examined\nin language models with more than 100 billion parameters.\nWhile techniques like chain-of-thought prompt-\ning (Wei et al., 2022b) may help to elicit reasoning\nabilities in large language models, they cannot en-\nable the models to solve tasks beyond their current\ncapabilities. To truly enhance reasoning in LLMs,\nwe need to utilize training data, model architecture,\nand optimization objectives that are designed to\nencourage reasoning. For example, finetuning a\nmodel with a dataset including CoT data has been\nshown to improve reasoning (Chung et al., 2022),\nand models can also self-improve through the pro-\ncess of bootstrapping their reasoning (Zelikman\net al., 2022; Huang et al., 2022a). There is still\nmuch research that needs to be done in this area,\nand we look forward to future progress in improv-\ning reasoning in large language models.\n7 Conclusion\nIn this paper, we have provided a detailed and up-\nto-date review of the current state of knowledge\non reasoning in large language models. We have\ndiscussed techniques for improving and eliciting\nreasoning in LLMs, methods and benchmarks for\nevaluating reasoning abilities, and the findings and\nimplications of previous studies in this topic. While\nLLMs have made significant progress in natural\nlanguage processing and related fields, it remains\nunclear to what extent they are capable of true rea-\nsoning or whether they are simply using memorized\npatterns and heuristics to solve problems. Further\nresearch is needed to fully understand the reason-\ning abilities of LLMs, improve LLMs’ reasoning\ncapabilities, and determine their potential for use\nin a variety of applications. We hope that this paper\nwill serve as a useful overview of the current state\nof the field and stimulate further discussion and\nresearch on this interesting and important topic.\nLimitations\nIn this paper, we provide an overview of the current\nstate of knowledge on reasoning in large language\nmodels. Reasoning is a broad concept that encom-\npasses various forms, making it impractical to sum-\nmarize all related work in a single paper. Therefore,\nwe focus on deductive reasoning, as it is the most\ncommonly studied in the literature. Other forms of\nreasoning such as inductive reasoning (Yang et al.,\n2022; Misra et al., 2022, inter alia) and abductive\nreasoning (Wiegreffe et al., 2022; Lampinen et al.,\n2022; Jung et al., 2022, inter alia) may not be dis-\ncussed in depth.\n1057\nAdditionally, given the rapid evolution and sig-\nnificance of reasoning within large language mod-\nels, it is crucial to note that new contributions may\nhave emerged in the field concurrent with the writ-\ning of this paper. An additional resource to consider\nis a parallel survey by Qiao et al. (2022), which em-\nphasizes reasoning via language model prompting.\nOur coverage may not extend to papers released\nduring or after 2023 such as evaluation on Chat-\nGPT (Bang et al., 2023; Zheng et al., 2023). As\nsuch, we recommend readers to check the papers\nthat cite this survey for a more comprehensive and\nupdated understanding of this field.\nAcknowledgements\nWe would like to thank Jason Wei (OpenAI) and\nDenny Zhou (Google DeepMind) for their valu-\nable advice and constructive feedback on this work.\nThis material is based upon work supported by\nthe National Science Foundation IIS 16-19302 and\nIIS 16-33755, Zhejiang University ZJU Research\n083650, IBM-Illinois Center for Cognitive Com-\nputing Systems Research (C3SR) and IBM-Illinois\nDiscovery Accelerator Institute (IIDAI), gift grants\nfrom eBay and Microsoft Azure, UIUC OVCR\nCCIL Planning Grant 434S34, UIUC CSBS Small\nGrant 434C8U, and UIUC New Frontiers Initiative.\nAny opinions, findings, and conclusions or recom-\nmendations expressed in this publication are those\nof the author(s) and do not necessarily reflect the\nviews of the funding agencies.\nReferences\nAida Amini, Saadia Gabriel, Shanchuan Lin, Rik\nKoncel-Kedziorski, Yejin Choi, and Hannaneh Ha-\njishirzi. 2019. MathQA: Towards interpretable math\nword problem solving with operation-based for-\nmalisms. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n2357–2367, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nCem Anil, Yuhuai Wu, Anders Andreassen, Aitor\nLewkowycz, Vedant Misra, Vinay Ramasesh, Am-\nbrose Slone, Guy Gur-Ari, Ethan Dyer, and Behnam\nNeyshabur. 2022. Exploring length generaliza-\ntion in large language models. ArXiv preprint ,\nabs/2207.04901.\nYejin Bang, Samuel Cahyawijaya, Nayeon Lee, Wen-\nliang Dai, Dan Su, Bryan Wilie, Holy Lovenia, Ziwei\nJi, Tiezheng Yu, Willy Chung, et al. 2023. A multi-\ntask, multilingual, multimodal evaluation of chatgpt\non reasoning, hallucination, and interactivity. ArXiv\npreprint, abs/2302.04023.\nEmily M. Bender and Alexander Koller. 2020. Climbing\ntowards NLU: On meaning, form, and understanding\nin the age of data. In Proceedings of the 58th Annual\nMeeting of the Association for Computational Lin-\nguistics, pages 5185–5198, Online. Association for\nComputational Linguistics.\nPrajjwal Bhargava and Vincent Ng. 2022. Common-\nsense knowledge reasoning and generation with pre-\ntrained language models: A survey. Proceedings of\nthe AAAI Conference on Artificial Intelligence.\nRishi Bommasani, Drew A Hudson, Ehsan Adeli,\nRuss Altman, Simran Arora, Sydney von Arx,\nMichael S Bernstein, Jeannette Bohg, Antoine Bosse-\nlut, Emma Brunskill, et al. 2021. On the opportuni-\nties and risks of foundation models. ArXiv preprint,\nabs/2108.07258.\nHugo Bronkhorst, Gerrit Roorda, Cor Suhre, and Mar-\ntin Goedhart. 2020. Logical reasoning in formal\nand everyday reasoning tasks. International Journal\nof Science and Mathematics Education, 18(8):1673–\n1694.\nTom B. Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, Sandhini Agarwal, Ariel Herbert-V oss,\nGretchen Krueger, Tom Henighan, Rewon Child,\nAditya Ramesh, Daniel M. Ziegler, Jeffrey Wu,\nClemens Winter, Christopher Hesse, Mark Chen, Eric\nSigler, Mateusz Litwin, Scott Gray, Benjamin Chess,\nJack Clark, Christopher Berner, Sam McCandlish,\nAlec Radford, Ilya Sutskever, and Dario Amodei.\n2020. Language models are few-shot learners. In Ad-\nvances in Neural Information Processing Systems 33:\nAnnual Conference on Neural Information Process-\ning Systems 2020, NeurIPS 2020, December 6-12,\n2020, virtual.\nMark Chen, Jerry Tworek, Heewoo Jun, Qiming\nYuan, Henrique Ponde de Oliveira Pinto, Jared Ka-\nplan, Harri Edwards, Yuri Burda, Nicholas Joseph,\nGreg Brockman, et al. 2021. Evaluating large lan-\nguage models trained on code. ArXiv preprint ,\nabs/2107.03374.\nWenhu Chen. 2022. Large language models are few (1)-\nshot table reasoners. ArXiv preprint, abs/2210.06710.\nWenhu Chen, Xueguang Ma, Xinyi Wang, and\nWilliam W Cohen. 2022. Program of thoughts\nprompting: Disentangling computation from reason-\ning for numerical reasoning tasks. ArXiv preprint,\nabs/2211.12588.\nAakanksha Chowdhery, Sharan Narang, Jacob Devlin,\nMaarten Bosma, Gaurav Mishra, Adam Roberts,\nPaul Barham, Hyung Won Chung, Charles Sutton,\nSebastian Gehrmann, et al. 2022. Palm: Scaling\nlanguage modeling with pathways. ArXiv preprint,\nabs/2204.02311.\n1058\nHyung Won Chung, Le Hou, Shayne Longpre, Bar-\nret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi\nWang, Mostafa Dehghani, Siddhartha Brahma, et al.\n2022. Scaling instruction-finetuned language models.\nArXiv preprint, abs/2210.11416.\nPeter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot,\nAshish Sabharwal, Carissa Schoenick, and Oyvind\nTafjord. 2018. Think you have solved question an-\nswering? try arc, the ai2 reasoning challenge. ArXiv\npreprint, abs/1803.05457.\nKarl Cobbe, Vineet Kosaraju, Mohammad Bavar-\nian, Jacob Hilton, Reiichiro Nakano, Christopher\nHesse, and John Schulman. 2021. Training veri-\nfiers to solve math word problems. ArXiv preprint,\nabs/2110.14168.\nAntonia Creswell and Murray Shanahan. 2022. Faith-\nful reasoning using large language models. ArXiv\npreprint, abs/2208.14271.\nAntonia Creswell, Murray Shanahan, and Irina Higgins.\n2022. Selection-inference: Exploiting large language\nmodels for interpretable logical reasoning. ArXiv\npreprint, abs/2205.09712.\nIshita Dasgupta, Andrew K Lampinen, Stephanie CY\nChan, Antonia Creswell, Dharshan Kumaran,\nJames L McClelland, and Felix Hill. 2022. Lan-\nguage models show human-like content effects on\nreasoning. ArXiv preprint, abs/2207.07051.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nDavid Dohan, Winnie Xu, Aitor Lewkowycz, Ja-\ncob Austin, David Bieber, Raphael Gontijo Lopes,\nYuhuai Wu, Henryk Michalewski, Rif A Saurous,\nJascha Sohl-Dickstein, et al. 2022. Language model\ncascades. ArXiv preprint, abs/2207.10342.\nAndrew Drozdov, Nathanael Schärli, Ekin Akyürek,\nNathan Scales, Xinying Song, Xinyun Chen, Olivier\nBousquet, and Denny Zhou. 2022. Compositional\nsemantic parsing with large language models. ArXiv\npreprint, abs/2209.15003.\nDheeru Dua, Shivanshu Gupta, Sameer Singh, and\nMatt Gardner. 2022. Successive prompting for\ndecomposing complex questions. ArXiv preprint,\nabs/2212.04092.\nWard Edwards. 1954. The theory of decision making.\nPsychological bulletin, 51(4):380.\nRonald Fagin, Joseph Y Halpern, Yoram Moses, and\nMoshe Vardi. 2004. Reasoning about knowledge .\nMIT press.\nYao Fu, Hao Peng, and Tushar Khot. 2022a. How does\ngpt obtain its ability? tracing emergent abilities of\nlanguage models to their sources.\nYao Fu, Hao Peng, Ashish Sabharwal, Peter Clark,\nand Tushar Khot. 2022b. Complexity-based prompt-\ning for multi-step reasoning. ArXiv preprint ,\nabs/2210.00720.\nKathleen M Galotti. 1989. Approaches to studying for-\nmal and everyday reasoning. Psychological bulletin,\n105(3):331.\nLuyu Gao, Aman Madaan, Shuyan Zhou, Uri Alon,\nPengfei Liu, Yiming Yang, Jamie Callan, and Gra-\nham Neubig. 2022. Pal: Program-aided language\nmodels. ArXiv preprint, abs/2211.10435.\nMor Geva, Daniel Khashabi, Elad Segal, Tushar Khot,\nDan Roth, and Jonathan Berant. 2021. Did aristotle\nuse a laptop? a question answering benchmark with\nimplicit reasoning strategies. Transactions of the\nAssociation for Computational Linguistics , 9:346–\n361.\nOlga Golovneva, Moya Chen, Spencer Poff, Martin\nCorredor, Luke Zettlemoyer, Maryam Fazel-Zarandi,\nand Asli Celikyilmaz. 2022. Roscoe: A suite of\nmetrics for scoring step-by-step reasoning. ArXiv\npreprint, abs/2212.07919.\nSimeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting\nQi, Martin Riddell, Luke Benson, Lucy Sun, Eka-\nterina Zubova, Yujie Qiao, Matthew Burtell, et al.\n2022a. Folio: Natural language reasoning with first-\norder logic. ArXiv preprint, abs/2209.00840.\nSimon Jerome Han, Keith Ransom, Andrew Perfors,\nand Charles Kemp. 2022b. Human-like property\ninduction is a challenge for large language models.\nHangfeng He, Hongming Zhang, and Dan Roth. 2023.\nRethinking with retrieval: Faithful large language\nmodel inference. ArXiv preprint, abs/2301.00303.\nChadi Helwe, Chloé Clavel, and Fabian M Suchanek.\n2021. Reasoning with transformer-based models:\nDeep learning, but shallow reasoning. In 3rd Confer-\nence on Automated Knowledge Base Construction.\nDan Hendrycks, Collin Burns, Saurav Kadavath, Akul\nArora, Steven Basart, Eric Tang, Dawn Song, and\nJacob Steinhardt. 2021. Measuring mathematical\nproblem solving with the math dataset. In Proceed-\nings of the Neural Information Processing Systems\nTrack on Datasets and Benchmarks, volume 1.\nJiaxin Huang, Shixiang Shane Gu, Le Hou, Yuexin\nWu, Xuezhi Wang, Hongkun Yu, and Jiawei Han.\n2022a. Large language models can self-improve.\nArXiv preprint, abs/2210.11610.\nJie Huang, Kevin Chang, Jinjun Xiong, and Wen-mei\nHwu. 2022b. Open relation modeling: Learning\nto define relations between entities. In Findings of\nthe Association for Computational Linguistics: ACL\n1059\n2022, pages 297–308, Dublin, Ireland. Association\nfor Computational Linguistics.\nJie Huang, Hanyin Shao, and Kevin Chen-Chuan Chang.\n2022c. Are large pre-trained language models leak-\ning your personal information? In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022, pages 2038–2047, Abu Dhabi, United Arab\nEmirates. Association for Computational Linguistics.\nJie Huang, Kerui Zhu, Kevin Chen-Chuan Chang, Jinjun\nXiong, and Wen-mei Hwu. 2022d. DEER: Descrip-\ntive knowledge graph for explaining entity relation-\nships. In Proceedings of the 2022 Conference on\nEmpirical Methods in Natural Language Processing,\npages 6686–6698, Abu Dhabi, United Arab Emirates.\nAssociation for Computational Linguistics.\nWenlong Huang, Pieter Abbeel, Deepak Pathak, and\nIgor Mordatch. 2022e. Language models as zero-\nshot planners: Extracting actionable knowledge for\nembodied agents. In Proceedings of the 39th Inter-\nnational Conference on Machine Learning, volume\n162 of Proceedings of Machine Learning Research,\npages 9118–9147. PMLR.\nWenlong Huang, Fei Xia, Ted Xiao, Harris Chan, Jacky\nLiang, Pete Florence, Andy Zeng, Jonathan Tompson,\nIgor Mordatch, Yevgen Chebotar, et al. 2022f. Inner\nmonologue: Embodied reasoning through planning\nwith language models. In 2022 Conference on Robot\nLearning.\nMichael Huth and Mark Ryan. 2004. Logic in Computer\nScience: Modelling and reasoning about systems .\nCambridge university press.\nJaehun Jung, Lianhui Qin, Sean Welleck, Faeze Brah-\nman, Chandra Bhagavatula, Ronan Le Bras, and\nYejin Choi. 2022. Maieutic prompting: Logically\nconsistent reasoning with recursive explanations.The\n2022 Conference on Empirical Methods for Natural\nLanguage Processing.\nSeyed Mehran Kazemi, Najoung Kim, Deepti Bhatia,\nXin Xu, and Deepak Ramachandran. 2022. Lam-\nbada: Backward chaining for automated reasoning in\nnatural language. ArXiv preprint, abs/2212.13894.\nDaniel Keysers, Nathanael Schärli, Nathan Scales,\nHylke Buisman, Daniel Furrer, Sergii Kashubin,\nNikola Momchev, Danila Sinopalnikov, Lukasz\nStafiniak, Tibor Tihon, Dmitry Tsarkov, Xiao Wang,\nMarc van Zee, and Olivier Bousquet. 2020. Measur-\ning compositional generalization: A comprehensive\nmethod on realistic data. In 8th International Confer-\nence on Learning Representations, ICLR 2020, Addis\nAbaba, Ethiopia, April 26-30, 2020. OpenReview.net.\nTushar Khot, Harsh Trivedi, Matthew Finlayson, Yao\nFu, Kyle Richardson, Peter Clark, and Ashish Sab-\nharwal. 2022. Decomposed prompting: A modular\napproach for solving complex tasks. ArXiv preprint,\nabs/2210.02406.\nTakeshi Kojima, Shixiang Shane Gu, Machel Reid, Yu-\ntaka Matsuo, and Yusuke Iwasawa. 2022. Large lan-\nguage models are zero-shot reasoners. In Advances\nin Neural Information Processing Systems.\nBrenden M. Lake and Marco Baroni. 2018. General-\nization without systematicity: On the compositional\nskills of sequence-to-sequence recurrent networks. In\nProceedings of the 35th International Conference on\nMachine Learning, ICML 2018, Stockholmsmässan,\nStockholm, Sweden, July 10-15, 2018, volume 80 of\nProceedings of Machine Learning Research, pages\n2879–2888. PMLR.\nAndrew K Lampinen, Ishita Dasgupta, Stephanie CY\nChan, Kory Matthewson, Michael Henry Tessler, An-\ntonia Creswell, James L McClelland, Jane X Wang,\nand Felix Hill. 2022. Can language models learn\nfrom explanations in context? In Findings of the\nAssociation for Computational Linguistics: EMNLP\n2022.\nEdward H Levi. 2013. An introduction to legal reason-\ning. University of Chicago Press.\nAitor Lewkowycz, Anders Andreassen, David Dohan,\nEthan Dyer, Henryk Michalewski, Vinay Ramasesh,\nAmbrose Slone, Cem Anil, Imanol Schlag, Theo\nGutman-Solo, et al. 2022. Solving quantitative\nreasoning problems with language models. ArXiv\npreprint, abs/2206.14858.\nBelinda Z. Li, Maxwell Nye, and Jacob Andreas. 2021.\nImplicit representations of meaning in neural lan-\nguage models. In Proceedings of the 59th Annual\nMeeting of the Association for Computational Lin-\nguistics and the 11th International Joint Conference\non Natural Language Processing (Volume 1: Long\nPapers), pages 1813–1827, Online. Association for\nComputational Linguistics.\nShiyang Li, Jianshu Chen, Yelong Shen, Zhiyu Chen,\nXinlu Zhang, Zekun Li, Hong Wang, Jing Qian,\nBaolin Peng, Yi Mao, et al. 2022a. Explanations\nfrom large language models make small reasoners\nbetter. ArXiv preprint, abs/2210.06726.\nYifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen,\nJian-Guang Lou, and Weizhu Chen. 2022b. On the\nadvance of making language models better reasoners.\nArXiv preprint, abs/2206.02336.\nBill Yuchen Lin, Wangchunshu Zhou, Ming Shen, Pei\nZhou, Chandra Bhagavatula, Yejin Choi, and Xiang\nRen. 2020. CommonGen: A constrained text gen-\neration challenge for generative commonsense rea-\nsoning. In Findings of the Association for Computa-\ntional Linguistics: EMNLP 2020, pages 1823–1840,\nOnline. Association for Computational Linguistics.\nWang Ling, Dani Yogatama, Chris Dyer, and Phil Blun-\nsom. 2017. Program induction by rationale genera-\ntion: Learning to solve and explain algebraic word\nproblems. In Proceedings of the 55th Annual Meet-\ning of the Association for Computational Linguistics\n1060\n(Volume 1: Long Papers), pages 158–167, Vancouver,\nCanada. Association for Computational Linguistics.\nChenzhengyi Liu, Jie Huang, Kerui Zhu, and Kevin\nChen-Chuan Chang. 2022a. Dimongen: Diver-\nsified generative commonsense reasoning for ex-\nplaining concept relationships. ArXiv preprint ,\nabs/2212.10545.\nJiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan,\nLawrence Carin, and Weizhu Chen. 2022b. What\nmakes good in-context examples for GPT-3? In\nProceedings of Deep Learning Inside Out (DeeLIO\n2022): The 3rd Workshop on Knowledge Extrac-\ntion and Integration for Deep Learning Architectures,\npages 100–114, Dublin, Ireland and Online. Associa-\ntion for Computational Linguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized bert pretraining ap-\nproach. ArXiv preprint, abs/1907.11692.\nPan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-\nWei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter\nClark, and Ashwin Kalyan. 2022. Learn to explain:\nMultimodal reasoning via thought chains for science\nquestion answering. In Advances in Neural Informa-\ntion Processing Systems.\nAman Madaan, Shuyan Zhou, Uri Alon, Yiming Yang,\nand Graham Neubig. 2022. Language models of code\nare few-shot commonsense learners. In Proceedings\nof the 2022 Conference on Empirical Methods in\nNatural Language Processing (EMNLP).\nLucie Charlotte Magister, Jonathan Mallinson, Jakub\nAdamek, Eric Malmi, and Aliaksei Severyn. 2022.\nTeaching small language models to reason. ArXiv\npreprint, abs/2212.08410.\nChristopher D Manning. 2022. Human language under-\nstanding & reasoning. Daedalus, 151(2):127–138.\nGary Marcus. 2020. The next decade in ai: four steps\ntowards robust artificial intelligence. ArXiv preprint,\nabs/2002.06177.\nConor McHugh and Jonathan Way. 2018. What is rea-\nsoning? Mind, 127(505):167–196.\nShen-yun Miao, Chao-Chun Liang, and Keh-Yih Su.\n2020. A diverse corpus for evaluating and developing\nEnglish math word problem solvers. In Proceedings\nof the 58th Annual Meeting of the Association for\nComputational Linguistics, pages 975–984, Online.\nAssociation for Computational Linguistics.\nSewon Min, Victor Zhong, Luke Zettlemoyer, and Han-\nnaneh Hajishirzi. 2019. Multi-hop reading compre-\nhension through question decomposition and rescor-\ning. In Proceedings of the 57th Annual Meeting of\nthe Association for Computational Linguistics, pages\n6097–6109, Florence, Italy. Association for Compu-\ntational Linguistics.\nKanishka Misra, Julia Taylor Rayz, and Allyson Et-\ntinger. 2022. A property induction framework\nfor neural language models. ArXiv preprint ,\nabs/2205.06910.\nMelanie Mitchell. 2021. Abstraction and analogy-\nmaking in artificial intelligence. Annals of the New\nYork Academy of Sciences, 1505(1):79–101.\nLinyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria\nLin, Neha Verma, Rui Zhang, Wojciech Kry´sci´nski,\nHailey Schoelkopf, Riley Kong, Xiangru Tang,\nMutethia Mutuma, Ben Rosand, Isabel Trindade,\nRenusree Bandaru, Jacob Cunningham, Caiming\nXiong, Dragomir Radev, and Dragomir Radev. 2022.\nFeTaQA: Free-form table question answering. Trans-\nactions of the Association for Computational Linguis-\ntics, 10:35–49.\nMaxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,\nHenryk Michalewski, Jacob Austin, David Bieber,\nDavid Dohan, Aitor Lewkowycz, Maarten Bosma,\nDavid Luan, Charles Sutton, and Augustus Odena.\n2022. Show your work: Scratchpads for interme-\ndiate computation with language models. In Deep\nLearning for Code Workshop.\nOpenAI. 2022. Chatgpt: Optimizing language models\nfor dialogue. OpenAI.\nJohn Arthur Passmore. 1961. Philosophical reasoning.\nPanupong Pasupat and Percy Liang. 2015. Composi-\ntional semantic parsing on semi-structured tables. In\nProceedings of the 53rd Annual Meeting of the As-\nsociation for Computational Linguistics and the 7th\nInternational Joint Conference on Natural Language\nProcessing (Volume 1: Long Papers), pages 1470–\n1480, Beijing, China. Association for Computational\nLinguistics.\nArkil Patel, Satwik Bhattamishra, and Navin Goyal.\n2021. Are NLP models really able to solve simple\nmath word problems? In Proceedings of the 2021\nConference of the North American Chapter of the\nAssociation for Computational Linguistics: Human\nLanguage Technologies, pages 2080–2094, Online.\nAssociation for Computational Linguistics.\nEthan Perez, Patrick Lewis, Wen-tau Yih, Kyunghyun\nCho, and Douwe Kiela. 2020. Unsupervised question\ndecomposition for question answering. In Proceed-\nings of the 2020 Conference on Empirical Methods\nin Natural Language Processing (EMNLP) , pages\n8864–8880, Online. Association for Computational\nLinguistics.\nXinyu Pi, Qian Liu, Bei Chen, Morteza Ziyadi, Zeqi Lin,\nYan Gao, Qiang Fu, Jian-Guang Lou, and Weizhu\nChen. 2022. Reasoning like program executors. In\nProceedings of the 2022 Conference on Empirical\nMethods in Natural Language Processing (EMNLP).\nSteven T Piantasodi and Felix Hill. 2022. Meaning\nwithout reference in large language models. ArXiv\npreprint, abs/2208.02957.\n1061\nOfir Press, Muru Zhang, Sewon Min, Ludwig Schmidt,\nNoah A Smith, and Mike Lewis. 2022. Measuring\nand narrowing the compositionality gap in language\nmodels. ArXiv preprint, abs/2210.03350.\nBen Prystawski, Paul Thibodeau, and Noah Goodman.\n2022. Psychologically-informed chain-of-thought\nprompts for metaphor understanding in large lan-\nguage models. ArXiv preprint, abs/2209.08141.\nShuofei Qiao, Yixin Ou, Ningyu Zhang, Xiang Chen,\nYunzhi Yao, Shumin Deng, Chuanqi Tan, Fei Huang,\nand Huajun Chen. 2022. Reasoning with lan-\nguage model prompting: A survey. ArXiv preprint,\nabs/2212.09597.\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya\nSutskever, et al. 2018. Improving language under-\nstanding by generative pre-training.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, Ilya Sutskever, et al. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\nblog, 1(8):9.\nJack W Rae, Sebastian Borgeaud, Trevor Cai, Katie\nMillican, Jordan Hoffmann, Francis Song, John\nAslanides, Sarah Henderson, Roman Ring, Susan-\nnah Young, et al. 2021. Scaling language models:\nMethods, analysis & insights from training gopher.\nArXiv preprint, abs/2112.11446.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, Peter J Liu, et al. 2020. Exploring the limits\nof transfer learning with a unified text-to-text trans-\nformer. J. Mach. Learn. Res., 21(140):1–67.\nNazneen Fatema Rajani, Bryan McCann, Caiming\nXiong, and Richard Socher. 2019. Explain your-\nself! leveraging language models for commonsense\nreasoning. In Proceedings of the 57th Annual Meet-\ning of the Association for Computational Linguistics,\npages 4932–4942, Florence, Italy. Association for\nComputational Linguistics.\nYasaman Razeghi, Robert L Logan IV , Matt Gardner,\nand Sameer Singh. 2022. Impact of pretraining term\nfrequencies on few-shot reasoning. ArXiv preprint,\nabs/2202.07206.\nSubhro Roy and Dan Roth. 2015. Solving general arith-\nmetic word problems. In Proceedings of the 2015\nConference on Empirical Methods in Natural Lan-\nguage Processing, pages 1743–1752, Lisbon, Portu-\ngal. Association for Computational Linguistics.\nLaura Ruis, Akbir Khan, Stella Biderman, Sara Hooker,\nTim Rocktäschel, and Edward Grefenstette. 2022.\nLarge language models are not zero-shot communi-\ncators. ArXiv preprint, abs/2210.14986.\nJacob Russin, Randall C O’Reilly, and Yoshua Bengio.\n2020. Deep learning needs a prefrontal cortex. Work\nBridging AI Cogn Sci, 107:603–616.\nAbulhair Saparov and He He. 2022. Language models\nare greedy reasoners: A systematic formal analysis of\nchain-of-thought. ArXiv preprint, abs/2210.01240.\nTeven Le Scao, Angela Fan, Christopher Akiki, El-\nlie Pavlick, Suzana Ili ´c, Daniel Hesslow, Roman\nCastagné, Alexandra Sasha Luccioni, François Yvon,\nMatthias Gallé, et al. 2022. Bloom: A 176b-\nparameter open-access multilingual language model.\nArXiv preprint, abs/2211.05100.\nFreda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang,\nSuraj Srivats, Soroush V osoughi, Hyung Won Chung,\nYi Tay, Sebastian Ruder, Denny Zhou, et al. 2022.\nLanguage models are multilingual chain-of-thought\nreasoners. ArXiv preprint, abs/2210.03057.\nKumar Shridhar, Alessandro Stolfo, and Mrinmaya\nSachan. 2022. Distilling multi-step reasoning ca-\npabilities of large language models into smaller mod-\nels via semantic decompositions. ArXiv preprint,\nabs/2212.00193.\nChan Hee Song, Jiaman Wu, Clayton Washington,\nBrian M Sadler, Wei-Lun Chao, and Yu Su. 2022.\nLlm-planner: Few-shot grounded planning for em-\nbodied agents with large language models. ArXiv\npreprint, abs/2212.04088.\nAarohi Srivastava, Abhinav Rastogi, Abhishek Rao,\nAbu Awal Md Shoeb, Abubakar Abid, Adam Fisch,\nAdam R Brown, Adam Santoro, Aditya Gupta, Adrià\nGarriga-Alonso, et al. 2022. Beyond the imitation\ngame: Quantifying and extrapolating the capabilities\nof language models. ArXiv preprint, abs/2206.04615.\nMirac Suzgun, Nathan Scales, Nathanael Schärli, Se-\nbastian Gehrmann, Yi Tay, Hyung Won Chung,\nAakanksha Chowdhery, Quoc V Le, Ed H Chi, Denny\nZhou, et al. 2022. Challenging big-bench tasks and\nwhether chain-of-thought can solve them. ArXiv\npreprint, abs/2210.09261.\nAlon Talmor and Jonathan Berant. 2018. The web as\na knowledge-base for answering complex questions.\nIn Proceedings of the 2018 Conference of the North\nAmerican Chapter of the Association for Computa-\ntional Linguistics: Human Language Technologies,\nVolume 1 (Long Papers), pages 641–651, New Or-\nleans, Louisiana. Association for Computational Lin-\nguistics.\nAlon Talmor, Jonathan Herzig, Nicholas Lourie, and\nJonathan Berant. 2019. CommonsenseQA: A ques-\ntion answering challenge targeting commonsense\nknowledge. In Proceedings of the 2019 Conference\nof the North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4149–4158, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nAlon Talmor, Oyvind Tafjord, Peter Clark, Yoav Gold-\nberg, and Jonathan Berant. 2020. Leap-of-thought:\nTeaching pre-trained models to systematically rea-\nson over implicit knowledge. In Advances in Neural\n1062\nInformation Processing Systems 33: Annual Confer-\nence on Neural Information Processing Systems 2020,\nNeurIPS 2020, December 6-12, 2020, virtual.\nRoss Taylor, Marcin Kardas, Guillem Cucurull, Thomas\nScialom, Anthony Hartshorn, Elvis Saravia, Andrew\nPoulton, Viktor Kerkez, and Robert Stojnic. 2022.\nGalactica: A large language model for science. ArXiv\npreprint, abs/2211.09085.\nKarthik Valmeekam, Alberto Olmo, Sarath Sreedharan,\nand Subbarao Kambhampati. 2022. Large language\nmodels still can’t plan (a benchmark for llms on plan-\nning and reasoning about change). In NeurIPS 2022\nFoundation Models for Decision Making Workshop.\nBoshi Wang, Xiang Deng, and Huan Sun. 2022a. Itera-\ntively prompt pre-trained language models for chain\nof thought. In The 2022 Conference on Empirical\nMethods for Natural Language Processing.\nBoshi Wang, Sewon Min, Xiang Deng, Jiaming Shen,\nYou Wu, Luke Zettlemoyer, and Huan Sun. 2022b.\nTowards understanding chain-of-thought prompting:\nAn empirical study of what matters. ArXiv preprint,\nabs/2212.10001.\nXuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le,\nEd Chi, and Denny Zhou. 2022c. Self-consistency\nimproves chain of thought reasoning in language\nmodels. ArXiv preprint, abs/2203.11171.\nPeter C Wason. 1968. Reasoning about a rule. Quar-\nterly journal of experimental psychology, 20(3):273–\n281.\nPeter Cathcart Wason and Philip Nicholas Johnson-\nLaird. 1972. Psychology of reasoning: Structure\nand content, volume 86. Harvard University Press.\nJason Wei, Yi Tay, Rishi Bommasani, Colin Raffel,\nBarret Zoph, Sebastian Borgeaud, Dani Yogatama,\nMaarten Bosma, Denny Zhou, Donald Metzler, et al.\n2022a. Emergent abilities of large language models.\nTransactions on Machine Learning Research.\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten\nBosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,\nand Denny Zhou. 2022b. Chain of thought prompt-\ning elicits reasoning in large language models. In\nAdvances in Neural Information Processing Systems.\nYixuan Weng, Minjun Zhu, Shizhu He, Kang Liu,\nand Jun Zhao. 2022. Large language models are\nreasoners with self-verification. ArXiv preprint ,\nabs/2212.09561.\nSarah Wiegreffe, Jack Hessel, Swabha Swayamdipta,\nMark Riedl, and Yejin Choi. 2022. Reframing\nhuman-AI collaboration for generating free-text ex-\nplanations. In Proceedings of the 2022 Conference\nof the North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies, pages 632–658, Seattle, United States.\nAssociation for Computational Linguistics.\nZonglin Yang, Li Dong, Xinya Du, Hao Cheng, Erik\nCambria, Xiaodong Liu, Jianfeng Gao, and Furu\nWei. 2022. Language models as inductive reason-\ners. ArXiv preprint, abs/2212.10923.\nXi Ye and Greg Durrett. 2022. The unreliability of\nexplanations in few-shot prompting for textual rea-\nsoning. Advances in neural information processing\nsystems.\nPing Yu, Tianlu Wang, Olga Golovneva, Badr\nAlkhamissy, Gargi Ghosh, Mona Diab, and Asli Ce-\nlikyilmaz. 2022. Alert: Adapting language models\nto reasoning tasks. ArXiv preprint, abs/2212.08286.\nEric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Good-\nman. 2022. STar: Bootstrapping reasoning with rea-\nsoning. In Advances in Neural Information Process-\ning Systems.\nSusan Zhang, Stephen Roller, Naman Goyal, Mikel\nArtetxe, Moya Chen, Shuohui Chen, Christopher De-\nwan, Mona Diab, Xian Li, Xi Victoria Lin, et al.\n2022a. Opt: Open pre-trained transformer language\nmodels. ArXiv preprint, abs/2205.01068.\nZhuosheng Zhang, Aston Zhang, Mu Li, and Alex\nSmola. 2022b. Automatic chain of thought prompt-\ning in large language models. ArXiv preprint ,\nabs/2210.03493.\nShen Zheng, Jie Huang, and Kevin Chen-Chuan Chang.\n2023. Why does chatgpt fall short in providing truth-\nful answers? ArXiv preprint, abs/2304.10513.\nDenny Zhou, Nathanael Schärli, Le Hou, Jason Wei,\nNathan Scales, Xuezhi Wang, Dale Schuurmans,\nOlivier Bousquet, Quoc Le, and Ed Chi. 2022a.\nLeast-to-most prompting enables complex reason-\ning in large language models. ArXiv preprint ,\nabs/2205.10625.\nFan Zhou, Haoyu Dong, Qian Liu, Zhoujun Cheng,\nShi Han, and Dongmei Zhang. 2022b. Reflection of\nthought: Inversely eliciting numerical reasoning in\nlanguage models via solving linear systems. ArXiv\npreprint, abs/2210.05075.\nHattie Zhou, Azade Nova, Hugo Larochelle, Aaron\nCourville, Behnam Neyshabur, and Hanie Sedghi.\n2022c. Teaching algorithmic reasoning via in-\ncontext learning. ArXiv preprint, abs/2211.09066.\nCorinne Zimmerman. 2000. The development of\nscientific reasoning skills. Developmental review,\n20(1):99–149.\n1063\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nSee the limitation section\n□ A2. Did you discuss any potential risks of your work?\nNot applicable. Left blank.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nSee the abstract and introduction section\n□\u0013 A4. Have you used AI writing assistants when working on this paper?\nWe wrote part of the appendix with ChatGPT assistance (e.g., to generate an initial description for\ncommonsense reasoning). The generated text is carefully revised and examined by the authors.\nB □ Did you use or create scientiﬁc artifacts?\nNot applicable. Left blank.\n□ B1. Did you cite the creators of artifacts you used?\nNot applicable. Left blank.\n□ B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nNot applicable. Left blank.\n□ B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nNot applicable. Left blank.\n□ B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nNot applicable. Left blank.\n□ B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nNot applicable. Left blank.\n□ B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nNot applicable. Left blank.\nC □\u0017 Did you run computational experiments?\nLeft blank.\n□ C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nNo response.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n1064\n□ C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nNot applicable. Left blank.\n□ C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nNot applicable. Left blank.\n□ C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nNot applicable. Left blank.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n1065"
}