{
  "title": "Zero-Shot Traffic Flow Prediction with Large Language Models: A Comparison with Deep Learning Approaches",
  "url": "https://openalex.org/W4410215282",
  "year": 2025,
  "authors": [
    {
      "id": "https://openalex.org/A2096669023",
      "name": "Yue Li",
      "affiliations": [
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A2222097429",
      "name": "Qunshan Zhao",
      "affiliations": [
        "University of Glasgow"
      ]
    },
    {
      "id": "https://openalex.org/A2166618581",
      "name": "Mingshu Wang",
      "affiliations": [
        "University of Glasgow"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W3123191313",
    "https://openalex.org/W4387641291",
    "https://openalex.org/W1984006069",
    "https://openalex.org/W3034294191",
    "https://openalex.org/W4391047399",
    "https://openalex.org/W4391374655",
    "https://openalex.org/W4388681972",
    "https://openalex.org/W4390547377",
    "https://openalex.org/W4385347734",
    "https://openalex.org/W4243418366",
    "https://openalex.org/W4392489787",
    "https://openalex.org/W4385287316",
    "https://openalex.org/W4366987638",
    "https://openalex.org/W4213372447",
    "https://openalex.org/W4200099066",
    "https://openalex.org/W3119276537",
    "https://openalex.org/W4322755838",
    "https://openalex.org/W3185846556",
    "https://openalex.org/W4389459353",
    "https://openalex.org/W4388451009",
    "https://openalex.org/W4392358260",
    "https://openalex.org/W4392781393",
    "https://openalex.org/W3033388173",
    "https://openalex.org/W3014250360",
    "https://openalex.org/W3105507389",
    "https://openalex.org/W3121139527",
    "https://openalex.org/W3139307343",
    "https://openalex.org/W4281256318",
    "https://openalex.org/W3021752193",
    "https://openalex.org/W3135400423",
    "https://openalex.org/W4220717846",
    "https://openalex.org/W3184178144",
    "https://openalex.org/W4297380734",
    "https://openalex.org/W4307079201",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W4383994288",
    "https://openalex.org/W4387634898",
    "https://openalex.org/W4392224296",
    "https://openalex.org/W4327810158",
    "https://openalex.org/W4306803525",
    "https://openalex.org/W4387635581",
    "https://openalex.org/W4407408304",
    "https://openalex.org/W2109764844",
    "https://openalex.org/W2563118655",
    "https://openalex.org/W2724431948",
    "https://openalex.org/W3092194021",
    "https://openalex.org/W4389230177",
    "https://openalex.org/W4392940048",
    "https://openalex.org/W2171234954",
    "https://openalex.org/W2090192376",
    "https://openalex.org/W2146399611",
    "https://openalex.org/W2924028299",
    "https://openalex.org/W2912462370",
    "https://openalex.org/W2533328922",
    "https://openalex.org/W3038719270",
    "https://openalex.org/W3100805595",
    "https://openalex.org/W4387435481",
    "https://openalex.org/W4384918448",
    "https://openalex.org/W4362515116",
    "https://openalex.org/W4291186635",
    "https://openalex.org/W4288088047",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W4224308101",
    "https://openalex.org/W4280581140",
    "https://openalex.org/W4362689842",
    "https://openalex.org/W4220665834",
    "https://openalex.org/W7071692843",
    "https://openalex.org/W4312937689",
    "https://openalex.org/W4312201182",
    "https://openalex.org/W4293195584",
    "https://openalex.org/W4386440150",
    "https://openalex.org/W4285145831",
    "https://openalex.org/W3193429343",
    "https://openalex.org/W4313855753",
    "https://openalex.org/W4285286407",
    "https://openalex.org/W4384297406",
    "https://openalex.org/W4377197269",
    "https://openalex.org/W4386132230",
    "https://openalex.org/W4396930134",
    "https://openalex.org/W2117671523",
    "https://openalex.org/W2064675550",
    "https://openalex.org/W3135257712",
    "https://openalex.org/W4376616630",
    "https://openalex.org/W1973207880",
    "https://openalex.org/W2117829824",
    "https://openalex.org/W3020796509",
    "https://openalex.org/W4313343845",
    "https://openalex.org/W2990138404",
    "https://openalex.org/W3103553187",
    "https://openalex.org/W4401867042",
    "https://openalex.org/W2612690371"
  ],
  "abstract": "<title>Abstract</title> Traffic flow prediction plays an important role in managing urban transportation systems, helping to reduce congestion and improve road safety. Although existing deep learning models improve their prediction accuracy with complex structures, they always require large datasets for task-specific training. Recently, the rapidly developed pre-trained large language models (LLMs) have shown outstanding performance in time series prediction. Motivated by the development, we apply two foundation models, Lag-Llama and Chronos, for zero-shot traffic flow prediction and compare their accuracy against traditional deep learning models. Our results show that LLMs outperform deep learning models in traffic flow prediction under both normal conditions and disruptive events. Unlike deep learning models, which require large-scale historical data and extensive training time for each task, pre-trained LLMs can be directly applied to datasets with different data sizes, traffic dynamics, and context lengths. We also find that LLMs with longer context lengths and larger model sizes achieve higher prediction accuracy but require increased inference times. Selecting an appropriate LLM is also crucial – models trained on a comprehensive dataset are more likely to achieve superior zero-shot performance, making them a practical and efficient choice for real-world traffic prediction applications.",
  "full_text": "Page 1/27\nZero-Shot Tra\u0000c Flow Prediction with Large\nLanguage Models: A Comparison with Deep\nLearning Approaches\nYue Li \nUniversity of Glasgow\nQunshan Zhao \nUniversity of Glasgow\nMingshu Wang \nUniversity of Glasgow\nArticle\nKeywords: Tra\u0000c \u0000ows, Time-series prediction, Deep learning, Large language models (LLMs)\nPosted Date: May 8th, 2025\nDOI: https://doi.org/10.21203/rs.3.rs-6572761/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nPage 2/27\nAbstract\nTra\u0000c \u0000ow prediction plays an important role in managing urban transportation systems, helping to\nreduce congestion and improve road safety. Although existing deep learning models improve their\nprediction accuracy with complex structures, they always require large datasets for task-speci\u0000c\ntraining. Recently, the rapidly developed pre-trained large language models (LLMs) have shown\noutstanding performance in time series prediction. Motivated by the development, we apply two\nfoundation models, Lag-Llama and Chronos, for zero-shot tra\u0000c \u0000ow prediction and compare their\naccuracy against traditional deep learning models. Our results show that LLMs outperform deep learning\nmodels in tra\u0000c \u0000ow prediction under both normal conditions and disruptive events. Unlike deep\nlearning models, which require large-scale historical data and extensive training time for each task, pre-\ntrained LLMs can be directly applied to datasets with different data sizes, tra\u0000c dynamics, and context\nlengths. We also \u0000nd that LLMs with longer context lengths and larger model sizes achieve higher\nprediction accuracy but require increased inference times. Selecting an appropriate LLM is also crucial –\nmodels trained on a comprehensive dataset are more likely to achieve superior zero-shot performance,\nmaking them a practical and e\u0000cient choice for real-world tra\u0000c prediction applications.\nIntroduction\nTra\u0000c \u0000ow prediction is a critical task of intelligent transportation systems (ITS) 1,2, focusing on\npredicting future tra\u0000c \u0000ow conditions based on historical and real-time data 3–6. Accurate tra\u0000c\nprediction plays a crucial role in urban planning, infrastructure management, and tra\u0000c control, which\nhelps to reduce tra\u0000c congestion, enhance road safety, and lower environmental impacts 6–9. Due to the\nrapid growth of urban populations and vehicle ownership worldwide, cities face increasing challenges to\nmaintain smooth tra\u0000c \u0000ow 10. Effective prediction of tra\u0000c \u0000ows provides transportation authorities,\ncity planners, and travellers with timely information that enables better decision-making, more e\u0000cient\nresource allocation, and an improved quality of life 2,11,12. \nThe tra\u0000c \u0000ow prediction models have evolved from traditional statistical approaches to deep learning\nmodels in recent years 5. Initially, statistical models such as Autoregressive Integrated Moving Average\n(ARIMA) dominated the \u0000eld due to their simplicity, interpretability 13, and effectiveness in modelling\nlinear 14 and stationary time-series data. However, these models often struggled to capture complex,\nnon-linear relationships in real-world tra\u0000c conditions 15. To overcome these limitations, deep learning\nmodels have been applied broadly, such as Convolutional Neural Networks (CNNs) and Long Short-Term\nMemory (LSTM) networks 2,6,15–17. CNNs are good at capturing local temporal patterns in tra\u0000c data by\napplying convolution operations along the time dimension 18, while LSTMs are effective in modelling\nlong-term temporal dependencies because of their gated mechanisms 19–21. Despite their improved\naccuracy and adaptability to complex scenarios, deep learning models typically require extensive\ncomputational resources and large datasets for training 11, which presents challenges for practical\nPage 3/27\nimplementation and real-time applications. Besides, these models are often developed for speci\u0000c tasks\n22, which may lead to over\u0000tting and reducing their ability to generalise effectively to new or unseen data.\n In the context of government mobility interventions and large-scale disruptive events, such as COVID-19,\ntra\u0000c \u0000ow prediction becomes even more challenging. The imposed restrictions, such as lockdowns\nand social distancing measures, caused rapid changes in travel behavior, leading to increased\nirregularities and deviations from historical tra\u0000c trends 23–29. Most existing tra\u0000c prediction models\nonly trained on pre-pandemic data 12,13,21,30, which limits their ability to make accurate predictions on\ntra\u0000c \u0000ows during periods of signi\u0000cant disruption. Limited models have attempted to address this\nchallenge by considering external factors, such as the effects of imposed response measure 31 and the\nevolving status of COVID-19 32. Another research involved decomposing irregular tra\u0000c \u0000ows into\ndistinct attributes and predicting them separately to improve model performance during the pandemic\n33. However, these models often rely on the availability of extensive labeled datasets that capture the\neffects of mobility restrictions and event status over time, making them di\u0000cult to implement in real-\ntime and across various regions with differing policies.\n Recently, the development of Large Language Models (LLMs) has introduced new opportunities for\ntra\u0000c \u0000ow prediction 5. Initially designed for natural language processing tasks, LLMs learn extensive\ngeneral knowledge by pre-training on large amounts of textual data 34–36. A distinctive strength of pre-\ntrained LLMs is their capability for zero-shot prediction, allowing them to perform various tasks without\nrequiring task-speci\u0000c training examples 37–39. Models such as GPT have demonstrated impressive zero-\nshot performance across numerous language understanding and generation tasks from different\ndomains 34,40–42. Motivated by these advances, researchers have started exploring pre-trained LLMs for\ntime series prediction 38,39,43. By speci\u0000cally pre-training existing transformer-based language model\narchitectures on large-scale time series datasets, these models learn to capture the temporal patterns\nand dynamics in sequential data effectively 22,44. Inspired by this capability, we are motivated to apply\npre-trained time series LLMs to zero-shot tra\u0000c \u0000ow prediction tasks, enabling accurate forecasting of\ntra\u0000c conditions without the need for large amounts of task-speci\u0000c datasets. \n The overarching goal of this research is to present a comprehensive analysis to compare the\nperformance of traditional deep learning models and pre-trained time series LLMs for tra\u0000c \u0000ow\nprediction under normal conditions and disruptive events. The contribution of this paper is threefold.\nFirst, it bridges the research gap in applying pre-trained LLMs to tra\u0000c \u0000ow prediction by comparing the\nperformance of two groups of time-series foundation models and traditional deep learning models on\nthe SCOOT dataset 45. Second, it evaluates prediction accuracy on heterogeneous and unusual tra\u0000c\npatterns, an area that has been sparsely explored in previous research. It utilises a long-term tra\u0000c \u0000ow\ndataset that includes a unique global pandemic period, which allows models to capture long-term tra\u0000c\ntrends, seasonal \u0000uctuations, and emergency-related variations, contributing to more robust predictive\nperformance. Third, it highlights the performance gap between pre-trained LLMs with different model\nsize and the diversity and temporal coverage of its training data. A well-trained LLM with comprehensive\nPage 4/27\ndatasets is more likely to achieve superior zero-shot performance, making it a practical and e\u0000cient\nchoice for real-world tra\u0000c \u0000ow prediction applications. The research outputs will support city planners\nin integrating pre-trained time-series LLMs into intelligent tra\u0000c control systems, enhancing their ability\nto respond effectively to both routine tra\u0000c conditions and unexpected disruptions. It is valuable in\nhelping transportation authorities and urban policymakers make informed, data-driven decisions in\ntra\u0000c management for future large-scale emergencies.\nBackground\nTra\u0000c Flow Prediction\nTra\u0000c prediction aims to forecast key factors such as vehicle \u0000ow, speed, and congestion levels 46–\n51,51,52. Tra\u0000c \u0000ow prediction is one of the most fundamental and widely studied tasks in ITS. Traditional\napproaches typically rely on statistical models, such as ARIMA 53 or Kalman \u0000lter 54, to capture tra\u0000c\npatterns and seasonalities, often serving as a strong baseline when data exhibit relatively stable trends\n14. While these models are relatively straightforward and interpretable 13, they may struggle with irregular\n\u0000uctuations in large-scale transportation networks 15. To address these complexities, researchers\nintroduced machine learning models such as Random Forests 55 and Support Vector Machines 56. By\nintegrating a richer set of input features, these methods can account for additional factors like weather\nconditions, special events, or road incidents 57. Although more \u0000exible than purely statistical techniques,\nthey often struggle to achieve consistently robust performance across diverse tra\u0000c scenarios.\nIn recent years, deep learning approaches have shown signi\u0000cant promise due to their capability for\nautomatic feature extraction and handling complex dependencies. Convolutional Neural Networks\n(CNNs), traditionally used for image data, have been adapted for time series prediction by applying\nconvolutional \u0000lters along the temporal dimension 18. This allows CNNs to extract local features, detect\nshort-term patterns, and reduce noise in tra\u0000c data. Recurrent Neural Networks (RNNs) further enhance\nsequence modelling by processing time series data, capturing the dynamic behaviour of tra\u0000c \u0000ow 58–\n60. To overcome limitations like vanishing gradients in standard RNNs, Long Short-Term Memory (LSTM)\nnetworks, an advanced type of RNN, employ gating mechanisms to maintain long-term dependencies 61–\n65, making them especially effective for predicting complex temporal patterns such as rush-hour peaks\nor irregular tra\u0000c \u0000ows.\nLarge Language Models\nRecent progress in computer hardware and the availability of large text datasets have led to the\ndevelopment of transformer-based LLMs that demonstrate impressive performance on various natural\nlanguage processing tasks 11,35,66,67. Language models are designed to predict the next token in a\nsequence by estimating the probability of each token based on those that have already appeared 22.\nTokens may be characters, subwords 68, or words from a vocabulary. The transformer architecture 69\nPage 5/27\nwas initially developed as an encoder-decoder system for machine translation 22 and is currently applied\nin many popular models, such as BART 70 and T5 71. In these models, the input text is \u0000rst converted into\na continuous representation using an encoder, after which the decoder generates output tokens\nsequentially based on the representation and previous tokens. Alternatively, a decoder-only architecture,\nused in models like GPT-3 34 and Llama 2 66, only considers tokens before the current token when\nmaking predictions. This architecture simpli\u0000es the model's design while still achieving robust\nperformance.\nLLMs are trained on extensive collections of text and can have millions to hundreds of billions of\nparameters 71,72. Researchers have found that increasing the number of parameters in these models\nleads to better performance 34. When the number of parameters becomes large enough, LLMs perform\ntraditional language tasks more accurately and show new abilities that smaller models lack 11. Zero-shot\ngeneralisation is one such ability where the model makes predictions on tasks it was not explicitly\ntrained for 22. For example, Brown et al. (2020) demonstrated that as the number of parameters grows,\nLLMs acquire the skill to handle new tasks without additional, task-speci\u0000c training. This connection\nbetween model parameters and zero-shot generalisation highlights that LLMs not only improve their\n\u0000exibility and power in language understanding and generation but also become capable of tackling\nchallenges such as forecasting time series data.\nLarge Language Models for Time-series Prediction\nLLMs have recently developed as useful tools for time series forecasting by using their powerful\nsequence modelling and pattern recognition capabilities 37. PromptCast 43 \u0000rst treats time series\nforecasting as a natural language generation task, converting numerical inputs and outputs into textual\nprompts, thus allowing general-purpose language models to serve as core forecasting engines. However,\nPromptCast often requires carefully designed prompts, which can be time-consuming in complex or\ndomain-speci\u0000c scenarios. LLMTime 38 addresses these limitations by directly tokenising time series\ndata and treating forecasting as next-token prediction. This tokenisation strategy not only avoids\nextensive prompt engineering but also allows pre-trained LLMs, like GPT-3 and LLaMA, to produce robust\nzero-shot forecasts across a variety of benchmark datasets 22,38. However, LLMTime can be\ncomputationally and memory-intensive due to the large size of models, and it requires careful rescaling\nof data to handle varying magnitudes or precision.\nUnlike PromptCast and LLMTime repurpose large pre-trained LLMs with textual or digit-based prompts,\nresearchers have further developed time series-speci\u0000c LLMs by training foundation models with large,\ndiverse time series datasets. Rasul et al. (2024) propose a foundation model (Lag-Llama) designed\nexplicitly for univariate time series forecasting. Built on a decoder-only transformer architecture that\nuses lag features as covariates, Lag-Llama is pre-trained on a broad collection of real-world time series\nacross multiple domains including energy, transportation, economics, environmental science, air quality\nand cloud operations. This large-scale pre-training process allows it to capture a wide range of time\nPage 6/27\nseries patterns, enabling strong performance in zero-shot generalisation. Recent concurrent work,\nChronos 22, offers a similarly broad framework for pretrained time series forecasting but adapts\nstandard language model architectures T5 71 to treat real-valued time series as discrete tokens. Using\nscaling and uniform binning, Chronos converts continuous sequences into a \u0000xed vocabulary. Once\ntokenised, it trains a language model on an extensive collection of public and synthetic time series\ndatasets, thus learning to model a wide range of temporal patterns. Chronos demonstrates superior\nperformance across 42 benchmark datasets, outperforming in-domain and zero-shot scenarios.\nResults\nModel Performance Comparison\nThis study compares tra\u0000c \u0000ow prediction model performance across the entire dataset and post-\nCOVID-19 dataset at different context lengths (input lengths) between deep learning and LLMs (Fig. 2–\n4). The evaluation results clearly distinguish between model performance when trained on the post-\nCOVID-19 dataset and the entire dataset. Across all models, evaluation metrics (MAE, MAPE and RMSE)\nare consistently lower for the post-COVID-19 dataset. This suggests that deep learning and LLMs\nperform better with stable tra\u0000c patterns. Speci\u0000cally, the improvements of deep learning models are\nmoderate, with a slight decrease in RMSE and MAPE when predicted on the post-COVID-19 dataset,\nsuggesting that traditional deep learning models may be less sensitive to different data patterns. In\ncontrast, LLMs demonstrate a noticeable performance gap between different datasets. When predicted\non post-COVID-19 data, the reduction in MAE, MAPE and RMSE is more evident than deep learning\nmodels, particularly for Lag-LLaMA, indicating improved model adaptability to stable tra\u0000c dynamics.\nAccording to the context length, increasing the context length leads to improved prediction performance\nacross LLMs but limited improvement in deep learning models. CNN and LSTM show relatively worse\nperformance with increased context length, especially for the entire dataset. While LSTM maintains a\nrelatively stable trend, CNN exhibits more \u0000uctuations, particularly for longer context lengths, suggesting\npotential over\u0000tting or ine\u0000ciencies in capturing long-term dependencies. LLMs consistently reduce\nMAE, MAPE and RMSE as context length increases, although with slight \u0000uctuations. Lag-LlaMA, in\nparticular, demonstrates the most signi\u0000cant improvement, reinforcing its ability to apply long historical\nsequences effectively. These \u0000ndings highlight the superior capacity of LLMs to process and utilize long-\nterm dependencies in time series prediction.\nAlthough LLMs generally bene\u0000t from longer context lengths, their performance declines when the\ncontext is short—often falling behind traditional deep learning models. In particular, Lag-LLaMA\nconsistently yields higher MAE and RMSE values than both CNN and LSTM across all context lengths\nwhen evaluated on the full dataset. This can be attributed to the zero-shot nature of these pre-trained\nmodels, which rely on broadly learned universal patterns from large-scale, high-quality data rather than\ntask-speci\u0000c training 11. The entire dataset, which includes the more heterogeneous and unusual tra\u0000c\npatterns, would make the zero-shot prediction more demanding on these models. In contrast, the post-\nPage 7/27\nCOVID-19 dataset exhibits more stable and universal tra\u0000c \u0000ow dynamics, enabling the LLMs to utilise\ntheir extensive pre-training more effectively and outperform the traditional deep learning models.\nBesides, the consistent performance improvements observed with longer context lengths demonstrate\nthe importance of providing LLMs with su\u0000cient historical information to enhance their zero-shot\npredictions in time series prediction.\nTraining Time and Inference Time Analysis\nFigure 5 illustrates the trade-off between training time and Mean Absolute Error (MAE) for CNN and\nLSTM models on the post-COVID-19 and entire datasets. The CNN consistently achieves lower MAE and\ndemonstrates superior computational e\u0000ciency on both datasets, outperforming the LSTM by all\nmeasures. Speci\u0000cally, the CNN completes training epochs in considerably less time, suggesting that its\nconvolution-based structure may be faster to capture features of time series data with fewer\nparameters. Meanwhile, the LSTM exhibits an evident increase in training time, requiring more than twice\nthat of the CNN and tends to produce higher MAE values, especially on the entire dataset. This\ndifference is likely due to the larger data size and the sequential nature of LSTM, which requires more\ncomplex computations per timestep. However, the increased training time and more considerable\ndataset help bridge the performance gap between LSTM and CNN. This suggests that LSTM may\nachieve better accuracy by capturing complex temporal dependencies when provided with su\u0000cient data\nand training time.\nWe also compare the inference time for all the models, including deep learning and LLMs. It is important\nto note that Lag-Llama is intentionally omitted from this \u0000gure due to its exceptionally high inference\ntimes 5. It requires approximately 132 seconds per epoch on the post-COVID-19 dataset and 875\nseconds per epoch on the entire dataset, substantially longer than the inference times observed for the\nother models. For both post-COVID-19 and entire datasets, CNN and LSTM exhibit relatively short\ninference times but moderately higher MAEs, while the Chronos models show a broader range of\ninference times, generally increasing with model size. Similar to the training time comparison, LSTM\nconsistently exhibits longer inference times and higher MAEs than CNN. This highlights that the\nsequential structure of LSTM requires signi\u0000cantly more computational resources, which may limit its\napplicability in time-sensitive and resource-constrained scenarios compared to CNN.\nFor Chronos models, it can be seen that the larger Chronos con\u0000gurations tend to achieve lower MAEs at\nthe cost of longer inference durations. This is because Chronos are probabilistic time series models that\nrely on autoregressively sampling from the predicted distribution 22, which leads to longer inference\ntimes than deep learning models producing point predictions. This effect is particularly signi\u0000cant for\nlarger Chronos models with more parameters, leading to increased computational resources. A key\nobservation is that Chronos (Small) achieves shorter inference times and better performance than\nChronos (Mini). This indicates that speci\u0000c Chronos con\u0000gurations may effectively balance\nPage 8/27\ncomputational effort and predictive accuracy. Beyond this point, increasing model size leads to\nsigni\u0000cantly longer inference times but only slightly increased performance.\nModel Size Analysis\nFigure 7 compares the average performance of different context lengths with model size, which is\nmeasured by the number of parameters. Speci\u0000cally, the number of parameters for CNN and LSTM\ndepends on their architecture, hyperparameters, and context length, while pre-trained LLMs maintain a\n\u0000xed parameter number. From Fig. 7, it is clear that deep learning models and LLMs demonstrate\ndifferent performance trends as their sizes change. For LLMs, increasing the number of parameters\ngenerally leads to improved prediction accuracy on both the post-COVID-19 dataset and the entire\ndataset, suggesting that additional parameters help capture complex tra\u0000c patterns. Meanwhile, CNN\nslightly outperforms LSTM on the post-COVID-19 dataset, which might be because CNN is more\neffective at capturing local temporal features on smaller data sizes. LSTM remains stable on the entire\ndataset, possibly due to its ability to apply longer sequence dependencies for complex tra\u0000c \u0000ow\nprediction. Besides, Lag-Llama's performance decreases obviously on the entire dataset, while Chronos\nmodels demonstrate robust performance across different sizes, indicating that they effectively balance\ncomplexity and accuracy.\nDiscussion\nIn this study, we compare the performance of tra\u0000c \u0000ow prediction on traditional deep learning models\nand cutting-edge LLMs. The deep learning models, CNN and LSTM, are trained on the SCOOT dataset\nourselves, while Lag-Llama and Chronos are pre-trained time series LLMs, which can be applied for zero-\nshot prediction. We have found that LLMs with longer context lengths and larger model sizes tend to\nachieve higher prediction accuracy, while deep learning models show limited improvement. This\nsuggests that deep learning models may suffer from over\u0000tting or ine\u0000ciencies in capturing long-term\ndependencies. In contrast, LLMs demonstrate a superior ability to process and utilise long-term\ndependencies in time series prediction. However, there is a trade-off between model performance and\ninference time, as increasing context length and model size require more signi\u0000cant computational\nresources. Moreover, although LLMs are more sensitive to tra\u0000c patterns, they outperform traditional\ndeep learning models in both usual and unusual tra\u0000c conditions, with Chronos demonstrating\nparticularly strong performance.\nIn our experiments, we evaluate training and inference times separately to provide a clear understanding\nof the computational demands of each stage. However, for a comprehensive comparison between deep\nlearning models and LLMs, we calculate the total running time, combining training and inference\ndurations for the deep learning models. Speci\u0000cally, we calculate the cumulative running time over 100\ntraining epochs for deep learning models and one prediction epoch for all models. As shown in Fig. 8,\nthe running times of Chronos are signi\u0000cantly lower than those of other models, particularly for the small\ndataset. While the running time of CNN is shorter than that of Lag-Llama in this case, it overlooks\nPage 9/27\nhyperparameter tuning, which is essential in the deep learning training process for each prediction task.\nThe tuning process involves testing dozens of hyperparameter combinations 101,102, with each test\nrequiring an amount of time equivalent to the running time observed here (since the inference times of\nCNN and LSTM are too short to be considered). This leads to a practical running time multiple times\ngreater than the running time here. As a result, the running time for LLMs is shorter than that of deep\nlearning models. Besides, deep learning models require separate training for each prediction task. In\ncontrast, pre-trained LLMs can be directly applied to different prediction tasks across various datasets\nwith varying context lengths. This signi\u0000cantly simpli\u0000es model deployment and streamlines forecasting\npipelines, eliminating the need for task-speci\u0000c training.\nA key limitation of LLMs is the performance gap between different models. Our \u0000ndings indicate that\nLag-Llama shows only limited improvements in prediction accuracy compared to deep learning models,\nwhile Chronos consistently demonstrates strong performance across various context lengths. To further\nillustrate this performance gap, we used another publicly available and widely used tra\u0000c \u0000ow dataset\ncollected by the Caltrans Performance Measurement System (PeMS) in California, USA from January 1\nto December 31, 2018. As shown in Fig. 9, the results are consistent with those from the SCOOT dataset\nin our research, with Chronos signi\u0000cantly outperforming Lag-Llama across all context lengths. Since\nboth Chronos and Lag-Llama are pre-trained on a diverse set of publicly available datasets, the observed\nperformance difference may stem from their training data. Comparing their datasets, we \u0000nd that\nChronos is trained on seven different datasets, while Lag-Llama uses only three. Besides, Chronos\nincorporates synthetic data generated using Gaussian processes to enhance its training process. The\ntraining data for Chronos ranges from 2009 to 2022, while Lag-Llama's training data is limited to 2009\nand 2014–2016. This suggests that training LLMs on a more extensive corpus of time series data\nimproves zero-shot performance. Moreover, model size plays a crucial role in performance. Chronos\noffers models ranging from 8M (Tiny) to 710M (Large) parameters, which are signi\u0000cantly larger than\nLag-Llama's 2.45M parameters and are likely contributing to its superior predictive accuracy.\nIn summary, this research highlights that LLMs can achieve excellent zero-shot performance in tra\u0000c\n\u0000ow prediction under both normal conditions and disruptive events. Unlike traditional deep learning\nmodels, which require extensive task-speci\u0000c training and domain expertise, pre-trained LLMs can be\ndirectly applied to datasets with different data sizes, tra\u0000c dynamics, and context lengths. Besides,\nwhile deep learning models require large-scale historical data for training and validation, LLMs can make\naccurate zero-shot predictions with only a small subset of contextual data. Those advantages can\naddress critical limitations of traditional deep learning methods, such as time-consuming training\nprocesses, over\u0000tting due to task-speci\u0000c model training, and limited generalisation capabilities.\nAdditionally, they can contribute to the practical deployment of tra\u0000c prediction models across diverse\nand dynamically changing urban scenarios. However, choosing an appropriate LLM is crucial, as\nperformance depends on factors such as training data diversity, time coverage, and model size. A well-\ntrained LLM with a comprehensive dataset is more likely to achieve superior zero-shot performance,\nmaking it a practical and e\u0000cient choice for real-world tra\u0000c prediction applications.\nPage 10/27\nUsing LLMs for tra\u0000c \u0000ow prediction and other time series analysis tasks in urban settings presents\nboth opportunities and challenges. The development of LLMs has been one of the fastest-growing areas\nover the past two and a half years, since OpenAI released the \u0000rst version of ChatGPT 3.5 on November\n2022. As foundation models continue to evolve in both capability and e\u0000ciency, the prediction accuracy\nof LLM-based time series models is expected to improve correspondingly. As we observed in this\nresearch, although LLMs exhibit superior performance in this research, with faster inference and higher\naccuracy, they still have several limitations. Firstly, the training process of LLMs is typically very\nexpensive and time-consuming, making it less accessible for academic institutions or small research\ngroups to extend or revise pre-trained models. Furthermore, the e\u0000ciency of LLMs heavily relies on the\nunderlying foundation models, and the most advanced foundation models are often closed-source and\ndeveloped by leading companies in generative AI. As a result, most researchers can only rely on \"less\nadvanced\" or \"older generation\" publicly available foundation models to design \u0000ne-tuned models for\ntime series prediction. In the future, a better collaboration between AI companies and academia is\nnecessary to enable further customised model development. Lastly, the limited scope and diversity of\ntraining data 22 for time-series LLMs lead to performance disparities and prediction biases across\nfoundation models. Future work can focus on building and maintaining large-scale, diverse tra\u0000c\ndatasets to improve model training and predictive accuracy across various scenarios. The model can\nalso be further evaluated in regions with limited data, such as those in the Global South, to assess the\neffectiveness of using foundation models trained on datasets from Global North countries in different\ngeographical contexts. Governments can encourage collaborative data-sharing initiatives between public\nand private sectors to expand the availability of high-quality tra\u0000c data for model development.\nData and Methods\nDatasets\nThis section details the dataset employed to evaluate the predictive performance of deep learning and\nLLMs, with real-world tra\u0000c data collected via a Split Cycle Offset Optimisation Technique (SCOOT)\nbased Urban Tra\u0000c Control system 45. The SCOOT uses a network of sensors to capture tra\u0000c \u0000ow data\nacross the road network. The dataset includes tra\u0000c \u0000ows from the Glasgow City Council area over four\nconsecutive years, from October 1, 2019, to September 30, 2023, which includes the COVID-19 pandemic\nperiod 9. There are 470 sensors in the SCOOT dataset which record tra\u0000c \u0000ows at 60-minute intervals.\nFigure 1 compares the attributes of the SCOOT dataset with other tra\u0000c \u0000ow datasets applied in recent\ntra\u0000c \u0000ow prediction research from 2022 to 2024 15,21,30,52,73–82. Most datasets cover no more than one\nyear during normal periods and use time intervals of less than 30 minutes 20,83–86,86–93, while the SCOOT\ndataset covers longer than many previous studies. Although one study utilised a seven-year tra\u0000c\ndataset, which is longer than the SCOOT dataset, it only covers a period of stable tra\u0000c conditions. In\ncontrast, the SCOOT dataset captures tra\u0000c \u0000ows before, during, and after COVID-19, providing valuable\ninsights into the drastic changes in human mobility patterns in response to government mobility\nPage 11/27\ninterventions during a period of signi\u0000cant disruption. Its long-term coverage allows models to capture\nlong-term tra\u0000c trends, seasonal \u0000uctuations, and emergency-related variations, contributing to more\nrobust predictive performance. Besides, The bubble size in Fig. 1 represents the number of data points,\nand the SCOOT dataset contains a relatively large volume of observations. This large volume of data\nenhances deep learning and LLMs by improving their ability to learn complex tra\u0000c patterns and\nreducing the risk of over\u0000tting.\nDeep Learning and Large Language Models\nWe select two widely used deep learning models for time series analysis, CNN 94 and LSTM 95, and two\nrecently developed LLM-based time series prediction models, Lag-Llama 44 and Chronos 22, to assess\nthe performance of tra\u0000c \u0000ow prediction. The details of the models are outlined as follows:\nConvolutional Neural Network (CNN)\nCNN 94 captures temporal patterns by applying convolutional \u0000lters to sliding windows of sequential\ndata. This approach effectively detects local trends and short-term dependencies, enhancing prediction\naccuracy. In our CNN model, two one-dimensional convolutional layers are employed, with each utilising\nthe ReLU activation function. The \u0000rst layer speci\u0000es the input shape based on the sequence length and\nextracts initial local features from the tra\u0000c \u0000ow data. A subsequent convolutional layer further re\u0000nes\nthese features. A max-pooling layer then reduces the dimensionality of the resulting feature maps,\npreserving essential representations while reducing computational cost. Finally, the network is \u0000attened\nand regularised using a dropout layer to prevent over\u0000tting before a dense layer produces forecasts over\n6-time steps.\nLong Short-Term Memory (LSTM)\nLSTM 95 effectively models long-term dependencies and sequential relationships in time series data\nthrough gated memory cells, which retain relevant historical information while addressing vanishing\ngradient issues. Our LSTM model includes two hidden layers. The \u0000rst LSTM layer, con\u0000gured with the\ntanh activation function and set to return sequences, processes the input sequence to extract temporal\nfeatures and passes the entire sequence to the subsequent layer. The second LSTM layer further re\u0000nes\nthese features using the tanh activation function. Each LSTM layer is followed by a dropout layer to\nreduce the risk of over\u0000tting. Finally, a dense layer with six neurons is employed for multi-step prediction.\nLag-Llama\nLag-Llama 44 is a foundation model for univariate probabilistic time series prediction, built on a decoder-\nonly transformer architecture, LLaMA 36. Lag-Llama tokenises input data by constructing lagged feature\nvectors using historical observations at predetermined lag intervals. These intervals include multiple\nstandard frequencies such as quarterly, monthly, weekly, daily, hourly, and second-level frequencies. Each\ntoken also incorporates temporal covariates derived from date-time features such as hour-of-day, day-of-\nweek, and month-of-year, enriching the representation and providing contextual information to the\nPage 12/27\nmodel. The input tokens, composed of lagged features and temporal covariates, are projected into a\nhidden representation and passed through a series of causally masked transformer decoder layers,\nemploying RMSNorm and Rotary Positional Encoding (RoPE) at each attention layer. The \u0000nal output\nfrom the transformer decoder is fed into a distribution head designed to predict parameters of a\nStudent's t-distribution (degrees of freedom, mean, and scale) used for probabilistic forecasting.\nLag-Llama applies a robust scaling procedure using median and interquartile range (IQR) normalisation\nto handle numerical scale variations across different time series, signi\u0000cantly improving training stability\nand forecast accuracy. During training, Lag-Llama minimises the negative log-likelihood of the forecast\ndistribution for future values. Lag-Llama is pre-trained on 27 datasets categorised into six domains: air\nquality, transportation, economics, nature, energy, and cloud operations. The pre-training corpus includes\n7,965 univariate series consisting of about 352 million data tokens. This extensive and diverse corpus\nimproves Lag-Llama's ability to generalise and deliver strong zero-shot forecasting performance.\nChronos\nChronos 22 is a pre-trained probabilistic forecasting framework designed speci\u0000cally for time series, built\non transformer-based language models. The core innovation of Chronos is its approach to treating time\nseries forecasting similarly to natural language modelling tasks. It achieves this by tokenising\ncontinuous time series data into discrete tokens using a two-step approach: scaling and quantisation.\nFirstly, Chronos tokenises time series data by scaling each series individually using mean scaling, which\nnormalises the data based on the mean of absolute historical values. Then, the scaled data are\nquantised into discrete bins, forming tokens from a \u0000xed-size vocabulary. This vocabulary includes\nnumerical bins and special tokens such as PAD (for padding sequences to equal lengths) and EOS (end-\nof-sequence).\nChronos primarily employs variants of the T5 family of transformer-based language models, ranging\nfrom smaller models with approximately 8 million parameters to larger models of up to 710 million 71.\nThese models are trained in 5 sizes, named Tiny (8M), Mini (20M), Small (46M), Base (200M) and Large\n(710M), using a cross-entropy loss function, effectively framing regression as a classi\u0000cation task over\ndiscrete quantised bins. Chronos models provide probabilistic forecasts by autoregressively sampling\nfrom the learned categorical distributions and subsequently mapping these sampled tokens back to\ncontinuous numerical values via dequantisation and inverse scaling. To enhance training, Chronos\nutilises data augmentation methods: TSMixup, which creates augmented series through convex\ncombinations of existing series, and KernelSynth, which generates synthetic series using Gaussian\nprocesses. Chronos was pre-trained on 28 datasets comprising publicly available datasets, including\ntransport, retail, energy, \u0000nance, healthcare, and climate science, complemented by synthetic datasets.\nThe comprehensive benchmark evaluation involved 42 datasets to assess in-domain and zero-shot\nforecasting performance.\nModel Implementations\nPage 13/27\nTo evaluate the model performance on usual tra\u0000c patterns and unusual tra\u0000c dynamics, we divide the\nSCOOT dataset into two subgroups – the entire dataset including pandemic period, and the post-COVID-\n19 dataset. Based on the Stringency Index, the entire dataset contains hourly tra\u0000c \u0000ow data from\nOctober 1, 2019, to September 30, 2023, while the post-COVID-19 dataset includes data from June 3,\n2022 96. Each subgroup is chronologically divided into training (60%), validation (20%), and testing (20%)\nsets, with a 60 ‐ minute interval for both training and prediction. To assess the impact of context length on\nprediction accuracy, we train models with varying context lengths. Speci\u0000cally, the context length is set\nto 24×n hours, where n ranges from 1 to 21, limited by the available computational memory. These\ncontext lengths are used to predict tra\u0000c \u0000ow over the next 6 hours, a common forecasting horizon in\nexisting research 19,83,97,98.\nWe conduct experiments with different hyperparameters for each context length and dataset to train\ndeep learning models, selecting the best con\u0000guration for comparison. All the experiments are repeated\n10 times, and we record the mean value of evaluation metrics to reduce the randomness of individual\ntraining runs. The Adam optimizer is employed over 100 epochs, and the best hyperparameters of each\nmodel are shown in Table A1. Mean Square Error (MSE) is used as the loss function during the model\ntraining 99:\n1\nAll the models are implemented in Python 3.12.4, and executed on a 64-bit Ubuntu server with Intel Xeon\nGold 6334 8-Core Processor × 2 @ 3.60GHz CPU, 125 GB of RAM, and an NVIDIA A100 GPU with 24 GB\nof memory. The deep learning models are developed using TensorFlow 2.17.0, and the LLMs are\nconducted with PyTorch 2.3.1.\nEvaluation Metrics\nThe accuracy of tra\u0000c prediction models is typically evaluated using performance metrics that quantify\ntheir ability to forecast tra\u0000c conditions. In this research, we employ three widely recognised metrics:\nRoot Mean Square Error (RMSE), Mean Absolute Error (MAE), and Mean Absolute Percentage Error\n(MAPE). RMSE and MAE assess absolute errors, while MAPE evaluates relative errors 100. In all metrics,\nlower values indicate better prediction performance. The formulas are as follows:\nLoss= ∑ni=1(ˆyi −yi)21\nn\nPage 14/27\n(2)\n(3)\n(4)\nwhere  and  represent the ground truth and the predicted value for the th tra\u0000c \u0000ow sample.  is\nthe total number of the prediction samples.\nDeclarations\nAcknowledgement\nThe \u0000rst author is funded by the China Scholarship Council (CSC) from the Ministry of Education of P.R.\nChina. Dr Qunshan Zhao has received the ESRC's ongoing support for the Urban Big Data Centre (UBDC)\n[ES/L011921/1 and ES/S007105/1], and Royal Society International Exchange Scheme\n[IEC\\NSFC\\223042]. The authors want to thank the anonymous reviewers for their insightful comments\nand suggestions on an earlier version of this manuscript.\n CRediT authorship contribution statement\nYue Li: Conceptualization; Data curation; Formal analysis; Methodology; Visualization; Writing - original\ndraft; Writing – review and editing. Qunshan Zhao: Conceptualization; Writing - review & editing;\nSupervision; Resources; Project administration; Funding acquisition. Mingshu Wang: Conceptualization;\nWriting – review & editing; Supervision.\n Data availability\nThe data used in this paper is publicly available. Full details about the data acquisition can be found in\nthe documentation available at the GitHub repository: https://github.com/YueLi-\n0816/tra\u0000cFlowPrediction.\n Declaration of competing interest\nThe authors declare that they have no known competing \u0000nancial interests or personal relationships that\ncould have appeared to in\u0000uence the work reported in this paper.\nReferences\nRMSE=√∑\nn\ni=1(ˆyi −yi)21n\nMAE= ∑\nn\ni=1|ˆyi −yi|1n\nMAPE= ∑\nn\ni=1\n∣\n∣\n∣\n∣×1001n ˆyi−yiyi\nyi ˆyi n n\nPage 15/27\n1. Yin, X. et al. Deep Learning on Tra\u0000c Prediction: Methods, Analysis, and Future Directions. IEEE\nTransactions on Intelligent Transportation Systems 23, 4927–4943 (2022).\n2. Liu, Y., Rasouli, S., Wong, M., Feng, T. & Huang, T. RT-GCN: Gaussian-based spatiotemporal graph\nconvolutional network for robust tra\u0000c prediction. Information Fusion 102, 102078 (2024).\n3. Chen, H. & Rakha, H. A. Real-time travel time prediction using particle \u0000ltering with a non-explicit\nstate-transition model. Transportation Research Part C: Emerging Technologies 43, 112–126\n(2014).\n4. Guo, G. & Yuan, W. Short-term tra\u0000c speed forecasting based on graph attention temporal\nconvolutional networks. Neurocomputing 410, 387–393 (2020).\n5. Liu, C. et al. Spatial-Temporal Large Language Model for Tra\u0000c Prediction. Preprint at\nhttps://doi.org/10.48550/arXiv.2401.10134 (2024).\n\u0000. Kim, Y., Tak, H., Kim, S. & Yeo, H. A hybrid approach of tra\u0000c simulation and machine learning\ntechniques for enhancing real-time tra\u0000c prediction. Transportation Research Part C: Emerging\nTechnologies 160, 104490 (2024).\n7. Chen, J. et al. Tra\u0000c \u0000ow matrix-based graph neural network with attention mechanism for tra\u0000c\n\u0000ow prediction. Information Fusion 104, 102146 (2024).\n\u0000. Fan, J. et al. RGDAN: A random graph diffusion attention network for tra\u0000c prediction. Neural\nNetworks 172, 106093 (2024).\n9. Li, Y., Zhao, Q. & Wang, M. Understanding urban tra\u0000c \u0000ows in response to COVID-19 pandemic with\nemerging urban big data in Glasgow. Cities 154, 105381 (2024).\n10. Kala š ová, A. & Stacho, M. Smooth tra\u0000c \u0000ow as one of the most important factors for safety\nincrease in road transport. Transport (2006).\n11. Ren, Y. et al. TPLLM: A Tra\u0000c Prediction Framework Based on Pretrained Large Language Models.\nPreprint at https://doi.org/10.48550/arXiv.2403.02221 (2024).\n12. Sattarzadeh, A. R., Kutadinata, R. J., Pathirana, P. N. & Huynh, V. T. A novel hybrid deep learning\nmodel with ARIMA Conv-LSTM networks and shu\u0000e attention layer for short-term tra\u0000c \u0000ow\nprediction. Transportmetrica A: Transport Science (2025).\n13. Zhang, Y., Tang, S. & Yu, G. An interpretable hybrid predictive model of COVID-19 cases using\nautoregressive model and LSTM. Sci Rep 13, 6708 (2023).\n14. Wang, Y., Jia, R., Dai, F. & Ye, Y. Tra\u0000c Flow Prediction Method Based on Seasonal Characteristics\nand SARIMA-NAR Model. Applied Sciences 12, 2190 (2022).\n15. Kashyap, A. A. et al. Tra\u0000c \u0000ow prediction models - A review of deep learning techniques. COGENT\nENGINEERING 9, (2022).\n1\u0000. Li, Y., Chai, S., Ma, Z. & Wang, G. A Hybrid Deep Learning Framework for Long-Term Tra\u0000c Flow\nPrediction. IEEE Access 9, 11264–11271 (2021).\n17. Méndez, M., Merayo, M. G. & Núñez, M. Long-term tra\u0000c \u0000ow forecasting using a hybrid CNN-\nBiLSTM model. Engineering Applications of Arti\u0000cial Intelligence 121, 106041 (2023).\nPage 16/27\n1\u0000. Li, Y. et al. Modeling Temporal Patterns with Dilated Convolutions for Time-Series Forecasting. ACM\nTransactions on Knowledge Discovery from Data 16, 14:1-14:22 (2021).\n19. Wu, D., Peng, K., Wang, S. & Leung, V. C. M. Spatial-Temporal Graph Attention Gated Recurrent\nTransformer Network for Tra\u0000c Flow Forecasting. IEEE INTERNET OF THINGS JOURNAL 11,\n14267–14281 (2024).\n20. Xia, Z., Zhang, Y., Yang, J. & Xie, L. Dynamic spatial-temporal graph convolutional recurrent networks\nfor tra\u0000c \u0000ow forecasting. EXPERT SYSTEMS WITH APPLICATIONS 240, (2024).\n21. Zhao, Y. et al. Dual \u0000ow fusion graph convolutional network for tra\u0000c \u0000ow prediction.\nINTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS 15, 3425–3437 (2024).\n22. Ansari, A. F. et al. Chronos: Learning the Language of Time Series. Preprint at\nhttps://doi.org/10.48550/arXiv.2403.07815 (2024).\n23. Parr, S., Wolshon, B., Renne, J., Murray-Tuite, P. & Kim, K. Tra\u0000c Impacts of the COVID-19 Pandemic:\nStatewide Analysis of Social Separation and Activity Restriction. Natural Hazards Review 21,\n04020025 (2020).\n24. Warren, M. S. & Skillman, S. W. Mobility Changes in Response to COVID-19. Preprint at\nhttps://doi.org/10.48550/arXiv.2003.14228 (2020).\n25. Borkowski, P., Ja ż d ż ewska-Gutta, M. & Szmelter-Jarosz, A. Lockdowned: Everyday mobility changes\nin response to COVID-19. Journal of Transport Geography 90, 102906 (2021).\n2\u0000. Nouvellet, P. et al. Reduction in mobility and COVID-19 transmission. Nat Commun 12, 1090 (2021).\n27. Patra, S. S., Chilukuri ,Bhargava Rama & and Vanajakshi, L. Analysis of road tra\u0000c pattern changes\ndue to activity restrictions during COVID-19 pandemic in Chennai. Transportation Letters 13, 473–\n481 (2021).\n2\u0000. Ebrahim Shaik, Md. & Ahmed, S. An overview of the impact of COVID-19 on road tra\u0000c safety and\ntravel behavior. Transportation Engineering 9, 100119 (2022).\n29. Hu, Y. et al. Impacts of Covid-19 mode shift on road tra\u0000c. Preprint at\nhttps://doi.org/10.48550/arXiv.2005.01610 (2023).\n30. Ma, C., Dai, G. & Zhou, J. Short-Term Tra\u0000c Flow Prediction for Urban Road Sections Based on Time\nSeries Analysis and LSTM_BILSTM Method. IEEE TRANSACTIONS ON INTELLIGENT\nTRANSPORTATION SYSTEMS 23, 5615–5624 (2022).\n31. Ghanim, M. S., Muley, D. & Kharbeche, M. ANN-Based tra\u0000c volume prediction models in response\nto COVID-19 imposed measures. Sustainable Cities and Society 81, 103830 (2022).\n32. Liapis, S. et al. A methodology using classi\u0000cation for tra\u0000c prediction: Featuring the impact of\nCOVID-19. Integrated Computer-Aided Engineering 28, 417–435 (2021).\n33. Li, H. et al. Tra\u0000c Flow Forecasting in the COVID-19: A Deep Spatial-temporal Model Based on\nDiscrete Wavelet Transformation. ACM Trans. Knowl. Discov. Data 17, 64:1-64:28 (2023).\n34. Brown, T. et al. Language Models are Few-Shot Learners. in Advances in Neural Information\nProcessing Systems vol. 33 1877–1901 (Curran Associates, Inc., 2020).\nPage 17/27\n35. Chung, H. W. et al. Scaling Instruction-Finetuned Language Models. Preprint at\nhttps://doi.org/10.48550/arXiv.2210.11416 (2022).\n3\u0000. Touvron, H. et al. LLaMA: Open and E\u0000cient Foundation Language Models. Preprint at\nhttps://doi.org/10.48550/arXiv.2302.13971 (2023).\n37. Mirchandani, S. et al. Large Language Models as General Pattern Machines. Preprint at\nhttps://doi.org/10.48550/arXiv.2307.04721 (2023).\n3\u0000. Gruver, N., Finzi, M., Qiu, S. & Wilson, A. G. Large Language Models Are Zero-Shot Time Series\nForecasters. Preprint at https://doi.org/10.48550/arXiv.2310.07820 (2024).\n39. Liu, H., Zhao, Z., Wang, J., Kamarthi, H. & Prakash, B. A. LSTPrompt: Large Language Models as\nZero-Shot Time Series Forecasters by Long-Short-Term Prompting. Preprint at\nhttps://doi.org/10.48550/arXiv.2402.16132 (2024).\n40. OpenAI et al. GPT-4 Technical Report. Preprint at https://doi.org/10.48550/arXiv.2303.08774 (2024).\n41. Radford, A., Narasimhan, K., Salimans, T. & Sutskever, I. Improving Language Understanding by\nGenerative Pre-Training. (2018).\n42. Radford, A. et al. Language Models are Unsupervised Multitask Learners. (2019).\n43. Xue, H. & Salim, F. D. PromptCast: A New Prompt-based Learning Paradigm for Time Series\nForecasting. Preprint at https://doi.org/10.48550/arXiv.2210.08964 (2023).\n44. Rasul, K. et al. Lag-Llama: Towards Foundation Models for Probabilistic Time Series Forecasting.\nPreprint at https://doi.org/10.48550/arXiv.2310.08278 (2024).\n45. Li, Y., Zhao, Q. & Wang, M. High-resolution tra\u0000c \u0000ow data from the urban tra\u0000c control system in\nGlasgow. Sci Data 12, 253 (2025).\n4\u0000. Park, J. et al. Real time vehicle speed prediction using a Neural Network Tra\u0000c Model. in The 2011\nInternational Joint Conference on Neural Networks 2991–2996 (2011).\ndoi:10.1109/IJCNN.2011.6033614.\n47. Jia, Y., Wu, J. & Du, Y. Tra\u0000c speed prediction using deep learning method. in 2016 IEEE 19th\nInternational Conference on Intelligent Transportation Systems (ITSC) 1217–1222 (2016).\ndoi:10.1109/ITSC.2016.7795712.\n4\u0000. Jia, Y., Wu, J., Ben-Akiva, M., Seshadri, R. & Du, Y. Rainfall-integrated tra\u0000c speed prediction using\ndeep learning method. IET Intelligent Transport Systems 11, 531–536 (2017).\n49. Akhtar, M. & Moridpour, S. A Review of Tra\u0000c Congestion Prediction Using Arti\u0000cial Intelligence.\nJournal of Advanced Transportation 2021, 8878011 (2021).\n50. Chen, C., Liu, Z., Wan, S., Luan, J. & Pei, Q. Tra\u0000c Flow Prediction Based on Deep Learning in Internet\nof Vehicles. IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS 22, 3776–3789\n(2021).\n51. Aljebreen, M. et al. Enhancing Tra\u0000c Flow Prediction in Intelligent Cyber-Physical Systems: A Novel\nBi-LSTM-Based Approach With Kalman Filter Integration. IEEE TRANSACTIONS ON CONSUMER\nELECTRONICS 70, 1889–1902 (2024).\nPage 18/27\n52. Alvi, M., Minerva, R., Rajapaksha, P., Crespi, N. & Alvi, U. Tra\u0000c Flow Prediction in Sensor-Limited\nAreas Through Synthetic Sensing and Data Fusion. IEEE SENSORS LETTERS 8, (2024).\n53. Van Der Voort, M., Dougherty, M. & Watson, S. Combining kohonen maps with arima time series\nmodels to forecast tra\u0000c \u0000ow. Transportation Research Part C: Emerging Technologies 4, 307–318\n(1996).\n54. Okutani, I. & Stephanedes, Y. J. Dynamic prediction of tra\u0000c volume through Kalman \u0000ltering theory.\nTransportation Research Part B: Methodological 18, 1–11 (1984).\n55. Leshem, G. & Ritov, Y. Tra\u0000c Flow Prediction using Adaboost Algorithm with Random Forests as a\nWeak Learner. International Journal of Electrical and Computer Engineering 21, (2007).\n5\u0000. Tang, J. et al. Tra\u0000c \u0000ow prediction based on combination of support vector machine and data\ndenoising schemes. Physica A: Statistical Mechanics and its Applications 534, 120642 (2019).\n57. Yang, S. & Qian, S. Understanding and Predicting Travel Time with Spatio-Temporal Features of\nNetwork Tra\u0000c Flow, Weather and Incidents. IEEE Intelligent Transportation Systems Magazine 11,\n12–28 (2019).\n5\u0000. Tian, Y. & Pan, L. Predicting Short-Term Tra\u0000c Flow by Long Short-Term Memory Recurrent Neural\nNetwork. in 2015 IEEE International Conference on Smart City/SocialCom/SustainCom (SmartCity)\n153–158 (2015). doi:10.1109/SmartCity.2015.63.\n59. Zhu, H. et al. A Novel Tra\u0000c Flow Forecasting Method Based on RNN-GCN and BRB. Journal of\nAdvanced Transportation 2020, 7586154 (2020).\n\u00000. Lu, S., Zhang, Q., Chen, G. & Seng, D. A combined method for short-term tra\u0000c \u0000ow prediction based\non recurrent neural network. Alexandria Engineering Journal 60, 87–94 (2021).\n\u00001. Xiao, Y. & Yin, Y. Hybrid LSTM Neural Network for Short-Term Tra\u0000c Flow Prediction. INFORMATION\n10, (2019).\n\u00002. Wang, S., Zhao, J., Shao, C., Dong, C. D. & Yin, C. Truck Tra\u0000c Flow Prediction Based on LSTM and\nGRU Methods With Sampled GPS Data. IEEE ACCESS 8, 208158–208169 (2020).\n\u00003. Xiong, L., Ding, W., Huang, X. & Huang, W. CLSTAN: ConvLSTM-Based Spatiotemporal Attention\nNetwork for Tra\u0000c Flow Forecasting. MATHEMATICAL PROBLEMS IN ENGINEERING 2022, (2022).\n\u00004. Wang, J.-D. & Susanto, C. O. N. Tra\u0000c Flow Prediction with Heterogenous Data Using a Hybrid CNN-\nLSTM Model. CMC-COMPUTERS MATERIALS & CONTINUA 76, 3097–3112 (2023).\n\u00005. Guo, C., Zhu, J. & Wang, X. MVHS-LSTM: The Comprehensive Tra\u0000c Flow Prediction Based on\nImproved LSTM via Multiple Variables Heuristic Selection. APPLIED SCIENCES-BASEL 14, (2024).\n\u0000\u0000. Touvron, H. et al. Llama 2: Open Foundation and Fine-Tuned Chat Models. Preprint at\nhttps://doi.org/10.48550/arXiv.2307.09288 (2023).\n\u00007. Zhao, W. X. et al. A Survey of Large Language Models. Preprint at\nhttps://doi.org/10.48550/arXiv.2303.18223 (2024).\n\u0000\u0000. Sennrich, R., Haddow, B. & Birch, A. Neural Machine Translation of Rare Words with Subword Units.\nPreprint at https://doi.org/10.48550/arXiv.1508.07909 (2016).\nPage 19/27\n\u00009. Vaswani, A. et al. Attention is All you Need. in Advances in Neural Information Processing Systems\nvol. 30 (Curran Associates, Inc., 2017).\n70. Lewis, M. et al. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language\nGeneration, Translation, and Comprehension. Preprint at https://doi.org/10.48550/arXiv.1910.13461\n(2019).\n71. Raffel, C. et al. Exploring the Limits of Transfer Learning with a Uni\u0000ed Text-to-Text Transformer.\nPreprint at https://doi.org/10.48550/arXiv.1910.10683 (2023).\n72. Chowdhery, A. et al. PaLM: Scaling Language Modeling with Pathways. Preprint at\nhttps://doi.org/10.48550/arXiv.2204.02311 (2022).\n73. Chen, Z. et al. Spatial-temporal short-term tra\u0000c \u0000ow prediction model based on dynamical-learning\ngraph convolution mechanism. INFORMATION SCIENCES 611, 522–539 (2022).\n74. Chen, J. et al. Node Connection Strength Matrix-Based Graph Convolution Network for Tra\u0000c Flow\nPrediction. IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY 72, 12063–12074 (2023).\n75. Gao, H., Jia, H. & Yang, L. An Improved CEEMDAN-FE-TCN Model for Highway Tra\u0000c Flow\nPrediction. JOURNAL OF ADVANCED TRANSPORTATION 2022, (2022).\n7\u0000. Huang, X., Tang, J., Yang, X. & Xiong, L. A time-dependent attention convolutional LSTM method for\ntra\u0000c \u0000ow prediction. APPLIED INTELLIGENCE 52, 17371–17386 (2022).\n77. Xu, X., Liu, C., Zhao, Y. & Lv, X. Short-term tra\u0000c \u0000ow prediction based on whale optimization\nalgorithm optimized BiLSTM_Attention. CONCURRENCY AND COMPUTATION-PRACTICE &\nEXPERIENCE 34, (2022).\n7\u0000. Xu, X., Yang, C., Bilal, M., Li, W. & Wang, H. Computation O\u0000oading for Energy and Delay Trade-Offs\nWith Tra\u0000c Flow Prediction in Edge Computing-Enabled IoV. IEEE TRANSACTIONS ON INTELLIGENT\nTRANSPORTATION SYSTEMS 24, 15613–15623 (2023).\n79. He, R., Xiao, Y., Lu, X., Zhang, S. & Liu, Y. ST-3DGMR: Spatio-temporal 3D grouped multiscale ResNet\nnetwork for region-based urban tra\u0000c \u0000ow prediction. INFORMATION SCIENCES 624, 68–93 (2023).\n\u00000. Zhou, S. et al. Short-Term Tra\u0000c Flow Prediction of the Smart City Using 5G Internet of Vehicles\nBased on Edge Computing. IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n24, 2229–2238 (2023).\n\u00001. Naheliya, B., Redhu, P. & Kumar, K. MFOA-Bi-LSTM: An optimized bidirectional long short-term\nmemory model for short-term tra\u0000c \u0000ow prediction. PHYSICA A-STATISTICAL MECHANICS AND\nITS APPLICATIONS 634, (2024).\n\u00002. Tan, G. et al. A noise-immune and attention-based multi-modal framework for short-term tra\u0000c \u0000ow\nforecasting. SOFT COMPUTING 28, 4775–4790 (2024).\n\u00003. Duan, Y. et al. FDSA-STG: Fully Dynamic Self-Attention Spatio-Temporal Graph Networks for\nIntelligent Tra\u0000c Flow Prediction. IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY 71, 9250–\n9260 (2022).\nPage 20/27\n\u00004. Yan, B., Wang, G., Yu, J., Jin, X. & Zhang, H. Spatial-Temporal Chebyshev Graph Neural Network for\nTra\u0000c Flow Prediction in IoT-Based ITS. IEEE INTERNET OF THINGS JOURNAL 9, 9266–9279\n(2022).\n\u00005. Huo, G. et al. Hierarchical Spatio-Temporal Graph Convolutional Networks and Transformer Network\nfor Tra\u0000c Flow Forecasting. IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS\n24, 3855–3867 (2023).\n\u0000\u0000. Lai, Q., Tian, J., Wang, W. & Hu, X. Spatial-Temporal Attention Graph Convolution Network on Edge\nCloud for Tra\u0000c Flow Prediction. IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION\nSYSTEMS 24, 4565–4576 (2023).\n\u00007. Narmadha, S. & Vijayakumar, V. Spatio-Temporal vehicle tra\u0000c \u0000ow prediction using multivariate\nCNN and LSTM model. Materials Today: Proceedings 81, 826–833 (2023).\n\u0000\u0000. Wang, Z., Sun, P., Hu, Y. & Boukerche, A. A novel hybrid method for achieving accurate and timeliness\nvehicular tra\u0000c \u0000ow prediction in road networks. COMPUTER COMMUNICATIONS 209, 378–386\n(2023).\n\u00009. Wu, K. et al. Error-distribution-free kernel extreme learning machine for tra\u0000c \u0000ow forecasting.\nENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE 123, (2023).\n90. Xing, H., Chen, A. & Zhang, X. RL-GCN: Tra\u0000c \u0000ow prediction based on graph convolution and\nreinforcement for smart cities. DISPLAYS 80, (2023).\n91. Yang, D. & Lv, L. A Graph Deep Learning-Based Fast Tra\u0000c Flow Prediction Method in Urban Road\nNetworks. IEEE ACCESS 11, 93754–93763 (2023).\n92. Jia, Q., Zang, J. & Liu, S. Deep learning based tra\u0000c \u0000ow prediction model on highway research. in\n(eds. Ghanizadeh, A. & Jia, H.) vol. 13064 (2024).\n93. Lu, W. et al. Tra\u0000c \u0000ow prediction for highway vehicle detectors through decomposition and\nmachine learning. TRANSPORTATION LETTERS-THE INTERNATIONAL JOURNAL OF\nTRANSPORTATION RESEARCH (2024) doi:10.1080/19427867.2024.2339631.\n94. Waibel, A., Hanazawa, T., Hinton, G., Shikano, K. & Lang, K. J. Phoneme recognition using time-delay\nneural networks. IEEE Transactions on Acoustics, Speech, and Signal Processing 37, 328–339\n(1989).\n95. Hochreiter, S. & Schmidhuber, J. Long Short-Term Memory. Neural Computation 9, 1735–1780\n(1997).\n9\u0000. Hale, T. et al. A global panel database of pandemic policies (Oxford COVID-19 Government\nResponse Tracker). Nat Hum Behav 5, 529–538 (2021).\n97. Huang, X., Lan, Y., Ye, Y., Wang, J. & Jiang, Y. Tra\u0000c Flow Prediction Based on Multi-Mode Spatial-\nTemporal Convolution of Mixed Hop Diffuse ODE. ELECTRONICS 11, (2022).\n9\u0000. Su, Z., Liu, T., Hao, X. & Hu, X. Spatial-temporal graph convolutional networks for tra\u0000c \u0000ow\nprediction considering multiple tra\u0000c parameters. JOURNAL OF SUPERCOMPUTING 79, 18293–\n18312 (2023).\nPage 21/27\n99. Wang, Z. & Bovik, A. C. Mean squared error: Love it or leave it? A new look at Signal Fidelity\nMeasures. IEEE Signal Processing Magazine 26, 98–117 (2009).\n100. De Gooijer, J. G. & Hyndman, R. J. 25 years of time series forecasting. International Journal of\nForecasting 22, 443–473 (2006).\n101. Yi, H. & Bui, K.-H. N. An Automated Hyperparameter Search-Based Deep Learning Model for Highway\nTra\u0000c Prediction. IEEE Trans. Intell. Transport. Syst. 22, 5486–5495 (2021).\n102. Hyperparameter Tuning for Machine and Deep Learning with R: A Practical Guide. (Springer Nature,\n2023). doi:10.1007/978-981-19-5170-1.\nFigures\nFigure 1\nComparison of tra\u0000c \u0000ow datasets by time span, time interval, and data volume.\nPage 22/27\nFigure 2\nComparison of the post-COVID-19 and entire dataset performance (MAE) of models across different\ninput lengths.\nPage 23/27\nFigure 3\nComparison of the post-COVID-19 and entire dataset performance (MAPE) of models across different\ninput lengths.\nPage 24/27\nFigure 4\nComparison of the post-COVID-19 and entire dataset performance (RMSE) of models across different\ncontext lengths.\nPage 25/27\nFigure 5\nTraining time of CNN and LSTM.\nFigure 6\nInference time of models.\nPage 26/27\nFigure 7\nComparison of the post-COVID-19 and entire dataset performance of models across different model\nsizes.\nFigure 8\nComparison of the cumulative running time of models.\nPage 27/27\nFigure 9\nComparison of model performance across different context lengths.\nSupplementary Files\nThis is a list of supplementary \u0000les associated with this preprint. Click to download.\nAppendix.docx",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7388468384742737
    },
    {
      "name": "Deep learning",
      "score": 0.6930969953536987
    },
    {
      "name": "Context (archaeology)",
      "score": 0.6814674139022827
    },
    {
      "name": "Machine learning",
      "score": 0.6235994100570679
    },
    {
      "name": "Inference",
      "score": 0.6193920969963074
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6149408221244812
    },
    {
      "name": "Task (project management)",
      "score": 0.5703691840171814
    },
    {
      "name": "Traffic flow (computer networking)",
      "score": 0.45345377922058105
    },
    {
      "name": "Predictive modelling",
      "score": 0.4313836991786957
    },
    {
      "name": "Engineering",
      "score": 0.11449867486953735
    },
    {
      "name": "Geography",
      "score": 0.0975024402141571
    },
    {
      "name": "Computer security",
      "score": 0.0973091721534729
    },
    {
      "name": "Archaeology",
      "score": 0.0
    },
    {
      "name": "Systems engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I7882870",
      "name": "University of Glasgow",
      "country": "GB"
    }
  ]
}