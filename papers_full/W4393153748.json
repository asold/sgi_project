{
    "title": "Vision Transformer Off-the-Shelf: A Surprising Baseline for Few-Shot Class-Agnostic Counting",
    "url": "https://openalex.org/W4393153748",
    "year": 2024,
    "authors": [
        {
            "id": "https://openalex.org/A5100429173",
            "name": "Zhicheng Wang",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5101322968",
            "name": "Liwen Xiao",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5036358447",
            "name": "Zhiguo Cao",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        },
        {
            "id": "https://openalex.org/A5033681666",
            "name": "Hao LÃ¼",
            "affiliations": [
                "Huazhong University of Science and Technology"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3113657794",
        "https://openalex.org/W2895051362",
        "https://openalex.org/W4225829036",
        "https://openalex.org/W4282813861",
        "https://openalex.org/W4312392894",
        "https://openalex.org/W6809961208",
        "https://openalex.org/W6803870738",
        "https://openalex.org/W6687483927",
        "https://openalex.org/W3175251060",
        "https://openalex.org/W2738612274",
        "https://openalex.org/W2886443245",
        "https://openalex.org/W3126792443",
        "https://openalex.org/W2883929025",
        "https://openalex.org/W6755486901",
        "https://openalex.org/W2734511492",
        "https://openalex.org/W2899128648",
        "https://openalex.org/W2519281173",
        "https://openalex.org/W6793653683",
        "https://openalex.org/W4221160666",
        "https://openalex.org/W6848579055",
        "https://openalex.org/W3027606690",
        "https://openalex.org/W4224992933",
        "https://openalex.org/W6788884515",
        "https://openalex.org/W4221148940",
        "https://openalex.org/W1910776219",
        "https://openalex.org/W3184721837",
        "https://openalex.org/W4293818652",
        "https://openalex.org/W3175725565",
        "https://openalex.org/W4313156423",
        "https://openalex.org/W3170764266",
        "https://openalex.org/W4388009985",
        "https://openalex.org/W3206005599",
        "https://openalex.org/W4312820606",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4312826597",
        "https://openalex.org/W4312791242",
        "https://openalex.org/W2908510526",
        "https://openalex.org/W4312702383",
        "https://openalex.org/W4385964818",
        "https://openalex.org/W4289330707",
        "https://openalex.org/W2962921175",
        "https://openalex.org/W4312446817",
        "https://openalex.org/W3120979957",
        "https://openalex.org/W4311000001",
        "https://openalex.org/W2194775991",
        "https://openalex.org/W4384918448"
    ],
    "abstract": "Class-agnostic counting (CAC) aims to count objects of interest from a query image given few exemplars. This task is typically addressed by extracting the features of query image and exemplars respectively and then matching their feature similarity, leading to an extract-then-match paradigm. In this work, we show that CAC can be simplified in an extract-and-match manner, particularly using a vision transformer (ViT) where feature extraction and similarity matching are executed simultaneously within the self-attention. We reveal the rationale of such simplification from a decoupled view of the self-attention.The resulting model, termed CACViT, simplifies the CAC pipeline into a single pretrained plain ViT. Further, to compensate the loss of the scale and the order-of-magnitude information due to resizing and normalization in plain ViT, we present two effective strategies for scale and magnitude embedding. Extensive experiments on the FSC147 and the CARPK datasets show that CACViT significantly outperforms state-of-the-art CAC approaches in both effectiveness (23.60% error reduction) and generalization, which suggests CACViT provides a concise and strong baseline for CAC. Code will be available.",
    "full_text": "Vision Transformer Off-the-Shelf: A Surprising Baseline for\nFew-Shot Class-Agnostic Counting\nZhicheng Wang, Liwen Xiao, Zhiguo Cao, Hao Lu\nKey Laboratory of Image Processing and Intelligent Control, Ministry of Education; School of Artiï¬cial Intelligence and\nAutomation, Huazhong University of Science and Technology, Wuhan 430074, China\nzhicheng wang@hust.edu.cn\nAbstract\nClass-agnostic counting (CAC) aims to count objects of in-\nterest from a query image given few exemplars. This task is\ntypically addressed by extracting the features of query im-\nage and exemplars respectively and then matching their fea-\nture similarity, leading to an extract-then-match paradigm. In\nthis work, we show that CAC can be simpliï¬ed in an extract-\nand-match manner, particularly using a vision transformer\n(ViT) where feature extraction and similarity matching are\nexecuted simultaneously within the self-attention. We reveal\nthe rationale of such simpliï¬cation from a decoupled view of\nthe self-attention. The resulting model, termed CACViT, sim-\npliï¬es the CAC pipeline into a single pretrained plain ViT.\nFurther, to compensate the loss of the scale and the order-\nof-magnitude information due to resizing and normalization\nin plain ViT, we present two effective strategies for scale and\nmagnitude embedding. Extensive experiments on the FSC147\nand the CARPK datasets show that CACViT signiï¬cantly\noutperforms state-of-the-art CAC approaches in both effec-\ntiveness (23:60% error reduction) and generalization, which\nsuggests CACViT provides a concise and strong baseline for\nCAC. Code will be available.\nIntroduction\nObject counting aims to estimate the number of objects from\na query image. Most prior object counting approaches tar-\nget a speciï¬c domain, e.g., crowd (Zhang et al. 2015; Shu\net al. 2022; Zou et al. 2021), plant (Lu et al. 2017; Madec\net al. 2019), and car (Onoro-Rubio and LÂ´opez-Sastre 2016).\nThey often require numerous class-speciï¬c training data to\nlearn a good model (Wang et al. 2020). In contrast, Class-\nAgnostic Counting (CAC), whose goal is to estimate the\ncounting value of arbitrary categories given only few exem-\nplars, has recently received much attention due to its poten-\ntial to generalize to unseen scenes and reduced reliance on\nclass-speciï¬c training data (Lu, Xie, and Zisserman 2019;\nRanjan et al. 2021; Shi et al. 2022; Liu et al. 2022).\nCAC is ï¬rst introduced by Lu et al. (Lu, Xie, and Zis-\nserman 2019), which is by default formulated as a tem-\nplate matching problem, leading to an extract-then-match\nparadigm. Previous models (Ranjan et al. 2021; Shi et al.\n2022; Lin et al. 2022) use shared CNN for query images\nCopyright Â© 2024, Association for the Advancement of Artiï¬cial\nIntelligence (www.aaai.org). All rights reserved.\nâ€¦\nQuery Feature \nExtraction\n(Self-Attention)\nExemplar\n Feature \nExtraction\n(CNN)\nSimilarity Metric\n(Cross-Attention) Pred Density Map\nPred Density Map\nQuery \nFeature \nExtraction\nSimilarity \nMetric\nClass \nInformation\nExemplar \nFeature \nExtraction\nâ€¦\nâŠ•\nâŠ•\nconcat\n(a) Prior Arts: Extract-then-Match\nExemp\nlars with SE\nME from exemplars\n(b) Ours: Extract-and-Match\nSelf-Attention in Decouple View\nFigure 1: High-level ideas between prior arts and ours. (a)\nPrevious ViT-based class-agnostic counting framework fol-\nlows the extract-then-match paradigm with unshared feature\nextractors (e.g., a ViT and a CNN) for the query image and\nthe exemplars and post-matching such as cross-attention af-\nter feature extraction; (b) Our ViT-based framework follows\nan extract-and-match paradigm using self-attention in a de-\ncoupled view, with additional aspect-ratio-aware scale em-\nbedding (SE) and the order-of-magnitude embedding (ME)\nfor compensating the information loss of the scale in ViT.\nand exemplars feature extraction, as the bottom-up feature\nextraction approach of the CNN can adapt to images of en-\ntirely different sizes. Witnessing the ability of marking the\nresponses on the attention map by cross-attention mecha-\nnism, some models such as CounTR (Liu et al. 2022) em-\nploys cross-attention to match the features of query image\nand exemplars. However, in CounTR the query feature and\nexemplar feature are embedded separately by a ViT and a\nCNN, and the matching part is achieved by an extra cross-\nattention stage. This strategy introduces much redundancy\nand task-speciï¬c designs, which is not in line with the trend\nof task-agnostic foundation models.\nRecently, the computer vision community has witnessed\ngreat success with plain ViT in large multi-modal architec-\ntures (Touvron et al. 2023; Yu et al. 2022). Soon much work\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5832\nemerges for better adaptation of ViT on downstream vision\ntasks, such as object detection (Li et al. 2022; Lin et al.\n2023), pose estimation (Xu et al. 2022, 2023) and image\nmatting (Yao et al. 2023). As a template matching task, CAC\nis essentially suitable for using ViT with its attention mech-\nanism; however, there is little focus on the adaptation of ViT\non CAC task.\nIn this work, we share insights that the attention mecha-\nnism in plain ViT has the ability to extract the features for\nboth the query image and the exemplars and perform feature\nmatching for them. By grouping the query and exemplar to-\nkens into concatenation and feeding them to a plain ViT, the\nself-attention process in ViT can be divide into two groups\nof self-attention, and two groups of cross-attention. The for-\nmer self-attentions are to extract features for the query im-\nage and the exemplars, while the latter cross-attentions con-\ntains the matching process between the query image and\nthe exemplars. Therefore, without multiple feature extrac-\ntors or extra post-matching, it produces a novel extract-and-\nmatch paradigm. Compared with prior arts, the extra atten-\ntion from the query feature to the exemplars would further\nprovide additional class information to the query image in\nthis paradigm, enabling better perception of objects.Based\non this idea, we propose a framework for CAC that mainly\ncontains a single pretrained ViT, which veriï¬es the feasibil-\nity of plain ViT for CAC task.\nFor better adaptation of ViT to the speciï¬c CAC task,\nwe introduce more insights closely related to CAC task in\nour model design. Speciï¬cally, we observe that certain re-\nstrictions or functions such as resizing and softmax nor-\nmalization within this architecture can result in the loss of\nscale information and the order of magnitude of counting\nvalues. First, the exemplars must be resized to ï¬t the ViT in-\nput, which introduces size ambiguity during matching. Prior\nCNN-based models (Shi et al. 2022) attempt to compensate\nfor the scale information with scale embedding for exem-\nplars; however, they neglect the information of aspect ra-\ntios, which is crucial for classes with abnormal ratios. This\nis largely overlooked in the existing literature. Second, the\nattention map with softmax function can represent the rela-\ntive distribution of objects in the query image and therefore\nweakens the awareness of the model to the number of ob-\njects. We address this by restoring the magnitude order in\nthe normalized attention map. Both the proposed scale em-\nbedding and magnitude embedding are easy to implement.\nBy infusing the scale and the magnitude information into the\nplain ViT architecture, we acquire a surprisingly simple yet\nhighly effective ViT baseline for CAC. The resulting model,\ntermed CACViT, fully leverages the self-attention mecha-\nnism in ViT while also being tuned to mitigate the defects of\nthis architecture in this task.\nExperiments on the public benchmark FSC147 (Ranjan\net al. 2021) show that CACVit outperforms the previous best\napproaches by large margins, with relative error reductions\nof 19:04% and 23:60% on the validation and test sets, re-\nspectively, in terms of mean absolute error. Its cross-dataset\ngeneralization is also demonstrated on a car counting dataset\nCARPK (Hsieh, Lin, and Hsu 2017). We also provide exten-\nsive ablation studies to justify our propositions.\nIn a nutshell, our contributions are three-fold:\nâ€¢ A novel extract-and-match paradigm: we show that si-\nmultaneous feature extraction and matching can be made\npossible in CAC;\nâ€¢ CACViT: a simple and strong ViT-based baseline, sets\nthe new state-of-the-art on the FSC-147 benchmark;\nâ€¢ We introduce two effective strategies to embed scale, as-\npect ratio, and order of magnitude information tailored to\nCACViT.\nRelated Work\nThe task of CAC is composed of two main components: fea-\nture extraction and feature matching. We ï¬rst review each\ncomponent in previous counting models, then discuss jointly\nfeature extraction and matching in the other ï¬elds.\nFeature Extraction in Class-Agnostic Counting.The\ninvestigation of feature extraction in counting ï¬rst began\nwith class-speciï¬c counting (Abousamra et al. 2021; Cao\net al. 2018; He et al. 2021; Idrees et al. 2018; Laradji et al.\n2018; Cheng et al. 2022). In class-speciï¬c counting, most\nworks are designed to address the challenges posed by quan-\ntity variance and scale variance. For class-agnostic counting,\nthe core of feature extraction include uniï¬ed matching space\napart from challenges as above. To obtain a uniï¬ed match-\ning space, most previous work (Ranjan et al. 2021; Shi et al.\n2022; You et al. 2023) uses the shared CNN-based feature\nextractors for query images and exemplars. CounTR (Liu\net al. 2022), which ï¬rst introduces the ViT for feature extrac-\ntion in CAC, uses different feature extractors for the query\nimages (a ViT) and exemplars (a CNN). Hence, a two-stage\ntraining scheme is used for unifying the feature space.\nFeature Matching in Class-Agnostic Counting.Com-\npared with feature extraction, matching strategies in CAC\nhave garnered more attention. The key points of the match-\ning include the following two: 1) robustness to appearance\nvariance, and 2) ability to characterize quantity levels. In\nthe early attempt, naive inner product (Ranjan et al. 2021;\nYang et al. 2021) is used, which is not robust to the ap-\npearance variance of objects to be counted. Shi et al. (Shi\net al. 2022) developed a bilinear matching network (BM-\nNet) that expands the ï¬xed inner product to a learnable bi-\nlinear similarity metric, which improves the robustness com-\npared with the inner product. The recent ViT-based model\nCounTR (Liu et al. 2022) uses cross-attention for match-\ning, which seems a natural choice for a transformer-based\nsolution at ï¬rst glance. However, we show that, in our plain\nViT model CACViT, we can perform feature matching at the\nsame time of extracting features by self-attention.\nJointly Feature Extraction and Matching. For tem-\nplate matching and multi-modal tasks, feature extraction and\nmatching are two main components. In tracking and detec-\ntion tasks, MixFormer network (Chen et al. 2022) and FCT\nnetwork (Han et al. 2022) were proposed to enhance the cor-\nrelation between the target object and the image, thereby\nobtaining enhanced features for localization head. In multi-\nmodal tasks, ViLT (Kim, Son, and Kim 2021) strengthens\nthe interaction between text and image during the feature ex-\ntraction stage, resulting in efï¬cient multi-modal features that\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5833\nQuery Image X\nExemplar Z\ncat\ncat\nâ€¦\nViTBlock\nâ€¦\nÃ— ğ‘³ğ‘³\nViT Block\nLayerNorm\nLayerNorm\nMLP\nK Q V\nâŠ•\nâŠ•\nMulti-Head\nAttention\ncat\nExtract-and-Match\nRegression \nDecoder\nâ€¦\nME\nMean\nK heads\nqueryA\nexpA\nclassA\nmatchA\nmatchA\nDecouple Mean\n Reshape\nVisualization\nPred Density MapAttention Map\nFigure 2: The framework of CAC Vision Transformer (CACViT). A query image and exemplars with scale embedding are spilt\ninto patches to form tokens. Then the ï¬‚attened tokens are concatenated and fed into the transformer encoder. Afterward, the\noutput feature of query image and similarity metric from attention map with magnitude embedding (ME) are concatenated for\nregression. Finally, a regression decoder predicts the density map. It noted that the attention map is similar to density map.\nbeneï¬t the performance of downstream tasks. To the best of\nour knowledge, we are the ï¬rst to simultaneously consider\nfeature extraction and matching in CAC, and we provide a\ndecoupled analysis of the feasibility of this paradigm in the\nCAC, thereby streamlining the workï¬‚ow of CAC task.\nClass-Agnostic Counting Vision Transformer\nIn this section, we ï¬rst introduce the overview of our frame-\nwork for CAC, named Class-Agnostic Counting Vision\nTransformer (CACViT). Then, we provide an analysis of\nwhy the pure self-attention layers could serve as the substi-\ntution for previous architectures. After the ViT-style model\nis established, we spot the problem of scale information loss\nwhen directly applying ViT, and provide customized solu-\ntions for scale compensation.\nOverview of Approach\nThe pipeline of CACViT is presented in Figure 2. The query\nimage Xand corresponding exemplars Z, which are resized\nto a ï¬xed exemplar size, are split into tokens. Then they are\nconcatenated and processed by self-attention layers. After-\nward, the output feature of the query image and the similar-\nity map from the last attention map are concatenated to form\nthe ï¬nal density map. The `2 loss is adopted to supervise the\ndensity map.\nA Decoupled View of Self-Attention\nWe follow the standard formulation of self-attention in\nCACViT. Yet, in the presence of both query and exemplar\ntokens, we have a different interpretation of self-attention in\nCAC.\nWe ï¬rst revisit self-attention in transformer. Given an in-\nput token sequence I, we normalize it and transform it to a\ntriplet of Q, K and V, through linear layers. Then we em-\nploy the scaled dot-product attention mechanism to compute\nthe attention values between the queriesQand keysK. Each\noutput token is a weighted sum of all tokens using the atten-\ntion values as weights, formulated as:\nAttention(Q; K; V) =softmax(Q>K=\np\nD)V; (1)\nwhere the attention map A = softmax(Q>K=\np\nD).\nIn the presence of both query and exemplar tokens, we\nhave A 2R(M+Mz)\u0002(M+Mz), where M and MZ are the\ntoken number of the query image and the token number of\nexemplars, respectively. Note that, with the help of skip con-\nnection (He et al. 2016) in the self-attention block (Vaswani\net al. 2017), the physical meanings of the query tokens and\nexemplar tokens are preserved through layers, which is a\nprerequisite of our approach and analysis.\nBy considering A directly, it is not intuitive to see how\nthe self-attention mechanism can function as the feature ex-\ntractor and matcher. Here we provide a fresh look at self-\nattention in CAC with a decoupled view.\nFirst, we can split the token-to-token attention mapA into\n4 sub-attention maps as follow:\nA =\n\u0014\nAquery Aclass\nAmatch Aexp\n\u0015\n; (2)\nwhere Aquery 2 RM\u0002M , Aclass 2 RM\u0002MZ , Amatch 2\nRMZ\u0002M , and Aexp 2RMZ\u0002MZ .\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5834\nqueryA classA\nmatchA expA\nqueryQ queryK\nexpKqueryQ\nFigure 3: Decoupled view of self-attention in CACViT. The\ntop-left Aquery can be regarded as self-attention of the query\nimage. The bottom-left Amatch can be interpreted as cross-\nattention between query images and exemplars, despite be-\ning implemented in self-attention.\nAs Figure 3 shows, Aquery can be regarded as the self-\nattention map of the query image, which represents the most\nsigniï¬cant information within the image relevant to itself.\nAquery performs feature extraction for the query image iden-\ntical to the ViT or the CNN backbone in previous work. Sim-\nilarly, Aexp represents the self-attention map of the exem-\nplars, which works as the feature extraction for the exem-\nplars as in previous work.\nAmatch shown in Figure 3 can be regarded as cross-\nattention between the query image and exemplars, which\nthus can replace cross-attention layers used in previous ViT-\nbased methods, taking the query image as the query and\nthe exemplars as the key. In this way, Amatch can perform\nthe feature matching. In summary, compared with previous\nwork, self-attention in CACViT executes feature extraction\nand matching in a simple and natural way.\nIn addition to the matching and extraction, we acquire\nan additional Aclass. It is noted that the meaning of Aclass\nis different from Amatch. Every row i in Aclass represents\nthe attention from query token i to exemplar tokens. In the\nshallow layers, due to weak self-attention ability, query to-\nkens with foreground objects can more quickly acquire cor-\nresponding category information through additional exem-\nplar tokens. Conversely, background receives less attention\nfrom exemplar tokens. Thus, in the shallow layers, AT\nclass is\nsimilar to Amatch, which focuses on the foreground objects.\nAs the layers get deeper, the self-attention ability gets better.\nFor query tokens containing foreground information, due to\nthe repeated objects in the query image, self-attention will be\nstronger compared to query tokens containing background\ninformation. Moreover, since the total attention of the query\nimage to itself and to the exemplar remains ï¬xed, query to-\nkens with background information will exhibit stronger at-\ntention to exemplars than query tokens with foreground in-\nformation. As a result, the transposed Aclass will focus on\nthe background region.\nThe visualization results of decoupled attention are shown\nin 4, which verify our analysis. The more detailed analysis\nis on the supplementary.\nShallow Layers\nqueryA bgA matchA\nâ€¦\nDeep Layers\nShallow Layers\nqueryA classA matchA\nâ€¦\nDeep Layers\nInput\nGT\nPred\nâŠ•\nbroadcast\nScale Embedding with Aspect Ratio\n0\nğ‘Šğ‘Šğ‘˜ğ‘˜\nğ»ğ»ğ‘˜ğ‘˜\nğ‘Šğ‘Šğ‘§ğ‘§\nğ‘Šğ‘Šğ‘˜ğ‘˜0\nğ»ğ»ğ‘§ğ‘§\nğ»ğ»ğ‘˜ğ‘˜\nğ‘ğ‘ğ‘§ğ‘§\nğ‘ğ‘ğ‘§ğ‘§ ğ‘ğ‘ğ‘§ğ‘§ ğ‘ğ‘ğ‘§ğ‘§\nğ‘ğ‘ğ‘§ğ‘§\nFigure 4: Visualizations of attention maps in a decoupled\nview for different layers. (a) Aquery and Amatch highlight\nthe foreground and suppress the background. (b) In the shal-\nlow layers, Aclass favors foreground; but in the deep layers,\nAclass highlights background.\nScale and Magnitude Priors\nWhile the self-attention mechanism in ViT suits the CAC\ntask, certain restrictions or functions within this structure\ncan result in information loss. First, exemplars must be re-\nsized to ï¬t the ï¬xed-size tokens. As shown in Figure 5, the\nscale size level and aspect ratio of objects vary largely. So\nthe resized exemplars will lose the scale information and\ncomplicate the matching process. Second, thesoftmax used\nto normalize the attention map will weaken the ability to ex-\npress the number of objects. Per Figure 6, for query images\nwith the same distribution but different counting values, the\nattention maps would be similar due to normalization, which\nresults in the information loss of the order of magnitude.\nIn order to compensate for the loss and make our CACViT\nmore suitable for the CAC task, we introduce aspect-ratio-\naware scale embedding and order-of-magnitude embedding.\nAspect-Ratio-Aware Scale Embedding. While scale-\nlevel information has been studied in previous work (Shi\net al. 2022), aspect ratio information is often overlooked.\nHere we propose the scale embedding with further consider-\nation of the aspect ratio.\nThe embedding procedure is shown in Figure 7. Speciï¬-\ncally, given an exemplarzk, whose original width and height\nare Wk and Hk respectively. After resizing the function, we\nobtain a ï¬xed size exemplar zk 2RWz\u0002Hz\u0002C, where Wz\nand Hz are the ï¬xed-size height and width for every exem-\nplar. First, to keep the width information of the exemplar,\nwe create a linear vector with Wk points between 0 and Wk.\nThen we can broadcast the vector to acquire a width map\n^Wk 2RWz\u0002Hz . In the same way, we can obtain the height\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5835\nScale size level\nAspect Ratio\nAspect Ratio Scale Size Level\nFigure 5: Examples of images with different scale levels and\naspect ratio levels. The ï¬rst and second rows display the\nvariations in scale and in aspect ratio in the FSC147 dataset,\nrespectively.\nIdeal Attention Map\nSame Distribution\n0.43\n0.29\n0.14\n0.14\nOrder of Magnitude: 3\nOrder of Magnitude: 1\nIdeal\nAttention\nSame Distribution\nDensity Map\n0.43\n0.29\n0.14\n0.14\nOrder of Magnitude: 3\nOrder of Magnitude: 1\nCount: 21\nCount: 7\nFigure 6: Objective of magnitude embedding. For query im-\nages with the same distribution but different counting values,\nthe ideal attention maps should be the same due to normal-\nization, which results in the information loss of the order of\nmagnitude.\nmap ^Hk 2RWz\u0002Hz . Finally, we sum ^Wk and ^Hk as the\nscale embedding Sk = ^Wk + ^Hk. Then we can concatenate\nSk with resized exemplar zk to restore the scale informa-\ntion. Note that, in this way, we also encode the positional\ninformation for every patch by scale embedding.\nMagnitude Embedding. The order-of-magnitude infor-\nmation can be roughly represented by the ratio of the image\nsize to the exemplar size. Speciï¬cally, we assume an exem-\nplar is of width Wk and height Hk. Considering the patch\nsize is of Wp \u0002Hp pixels, the maximum capacity of a patch\nconditioned on the exemplar zk can be represented by:\nMEk = Wp \u0002Hp\nWk \u0002Hk\n: (3)\nIf we have K exemplars, we can compute the mean mag-\nnitude embedding ME from exemplars. Then we multiply\nthe embedding with the similarity scores from the attention\nmap to obtain the ï¬nal similarity map.\nExperiments\nImplementation Details\nTo demonstrate the superiority of our method, we con-\nduct extensive experiments on FSC147 (Ranjan et al. 2021),\nwhich is the ï¬rst large-scale dataset for CAC. Similar to (Shi\net al. 2022), we also adopt Mean Absolute Error (MAE) and\nMean Squared Error (MSE) as the evaluation metrics.\nâŠ•\nbroadcast\nScale Embedding with Aspect Ratio\nğ‘Šğ‘Šğ‘˜ğ‘˜\nğ»ğ»ğ‘˜ğ‘˜\nğ‘Šğ‘Šğ‘§ğ‘§\nğ‘Šğ‘Šğ‘˜ğ‘˜0\nğ»ğ»ğ‘§ğ‘§\nğ»ğ»ğ‘˜ğ‘˜\nğ‘ğ‘ğ‘§ğ‘§\nğ‘ğ‘ğ‘§ğ‘§ ğ‘ğ‘ğ‘§ğ‘§ ğ‘ğ‘ğ‘§ğ‘§\nğ‘ğ‘ğ‘§ğ‘§\n0\nFigure 7: An illustration on how to compute the aspect-ratio-\naware scale embedding for exemplar. For a exemplar with\nwidth Wk and height Hk, which will be resized to ï¬xed\nwidth Wz and ï¬xed height Hz.\nNetwork Structure:The network takes the image of size\n384 \u0002384 as the input, which is ï¬rst split into patches of\nsize 16 \u000216. Each exemplar is of size 64 \u000264, then split\ninto patches of size 16 \u000216. Our feature extractor, pre-\ntrained with MAE (He et al. 2022), consists of 12 trans-\nformer encoder blocks with a hidden dimension of 768, and\neach multi-head self-attention layer contains 12 heads. The\nfollowing 3 extra transformer blocks with a hidden dimen-\nsion of 512 are adopted to enhance the feature and reduce\nthe dimension for upsampling. Our regression decoder con-\nsists of 4 up-sampling layers with a hidden dimension of256\nas in CounTR (Liu et al. 2022).\nTraining Details:For fair comparison, we use the same\ndata augmentation, test-time cropping and normalization as\nCounTR (Liu et al. 2022). We apply AdamW (Loshchilov\nand Hutter 2017) as the optimizer with a batch size of 8.\nThe model is trained for 200 epochs with a learning rate of\n1e \u00004, a weight decay rate of 0:05, and 10 epochs for warm\nup. Our model is trained and tested on NVIDIA GeForce\nRTX 3090. Note that the CACViT consumes about14GB of\nmemory on a single GPU for 12 hours to train.\nComparison with State of the Art\nQuantitative Results. In this section, we compare our\nCACViT with state-of-the-art methods on the FSC-147\ndataset. As shown in Table 1, CACViT outperforms all the\ncompared methods on both 1-shot and 3-shot settings sig-\nniï¬cantly. In the 1-shot setting, CACViT achieves a rel-\native improvement of 13:23% w.r.t validation MAE and\n28:52% w.r.t test MAE compared with CounTR. Besides,\nCACViT reduces the validation RMSE by 17:45% and the\ntest RMSE by 66:76%. In the 3-shot setting, compared\nwith CounTR, CACViT generates a relative improvement of\n19:04% w.r.t validation MAE and 23:60% w.r.t test MAE.\nOne can also observe that CACViT reduces the validation\nRMSE by 23:84% and the test RMSE by 46:33% compared\nwith CounTR, which validates the robustness of our method.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5836\nModel Backbone Resolution Shots Val MAE Val RMSE Test MAE Test RMSE\nFamNet+ (Ranjan et al. 2021) ResNet50 384 1 26.55 77.01 26.76 110.95\nBMNet+ (Shi et al. 2022) ResNet50 [384,1584] 1 17.89 61.12 16.89 96.65\nCounTR (Liu et al. 2022) ViT-B & CNN 384 1 13.15 49.72 12.06 90.01\nCACViT (Ours) ViT-B 384 1 11.41 41.04 8.62 29.92\nFamNet+ (Ranjan et al. 2021) ResNet-50 384 3 23.75 69.07 22.08 99.54\nRCAC (Gong et al. 2022) ResNet-50 384 3 20.54 60.78 20.21 81.86\nBMNet+ (Shi et al. 2022) ResNet-50 [384,1584] 3 15.74 58.53 14.62 91.83\nSAFECount (You et al. 2023) ResNet-18 512 3 15.28 47.20 14.32 85.54\nSPDCN (Lin et al. 2022) ResNet-18 576 3 14.59 49.97 13.51 96.80\nCounTR (Liu et al. 2022) ViT-B & CNN 384 3 13.13 49.83 11.95 91.23\nCACViT (Ours) ViT-B 384 3 10.63 37.95 9.13 48.96\nTable 1: Comparison with the state-of-the-art CAC approaches on the FSC-147 dataset. The upper part of the table presents\nthe results on the 1-shot setting while the lower presents the 3-shots results. CounTR (Liu et al. 2022) need two-stage training\nregime (He et al. 2022). Note that our 1-shot CACViT outperforms all of the previous methods even in the 3-shots setting.\nmethod low MAE low RMSE high MAE high RMSE\nBMNet+ 6.23 29.96 23.21 127.62\nSPDCN 5.06 31.53 22.25 133.59\nCounTR 5.44 36.96 18.56 124.11\nCACViT 3.14 6.26 15.33 69.54\nTable 2: Comparison with the state-of-the-art CAC ap-\nproaches on two sub-test sets of low and high density, each\ncovers object counts from 8-37 and 37-3701 respectively,\nand each subset contains roughly 600 images.\nIt is noticed that the performance of the 1-shot is even\nbetter than that of the 3-shot in the test set, which is against\nour intuition. The phenomenon lies in the quality of annota-\ntion for dense environments, we provide more detail on the\nsupplementary.\nPerformance of Different Densities.In order to further\nvalidate the performance of the model under different ob-\nject densities on the 3-shot setting. we divide the test set into\ntwo sub-test sets of low and high density, each covers object\ncounts from 8-37 and 37-3701 respectively, and each sub-\nset contains roughly 600 images. As shown in Table 2, our\nmethod outperforms other methods in both the low-density\nand high-density environments, which validates the robust-\nness of our method.\nRunning Cost Evaluation.To verify the efï¬ciency of our\nmethod, we compare our model size, ï¬‚oating point opera-\ntions (FLOPS) with CounTR (Liu et al. 2022), which em-\nploys the same backbone as CACViT. The results are re-\nported in Table 3. Note that CounTR does not report its\nepochs for ï¬netuning in their paper, so we get the data from\nits ofï¬cial GitHub page. Our training setting is similar to\nCounTR. However, 1) we exclude the self-supervised (He\net al. 2022) pre-training on the FSC147 dataset compared\nwith CounTR, and 2) we do not require an additional CNN\nto extract features of exemplars. One can easily observe that\nour CACViT can signiï¬cantly reduce the training epochs\nmethod Model Size (M) GFLOPs #epochs\nCounTR pretrain 112M 27G 300\nCounTR ï¬netune 99M 84G 1000*\nCACViT (Ours) 99M 89G 200\nTable 3: Comparison of the model size and FLOPs. * indi-\ncates that we obtain the #epochs on its ofï¬cial project page,\nwhich is not mentioned on paper.\nfrom 1300 to 200 and simplify the training strategy into one\nstage while keeping the model size and FLOPs comparable\nto that of CounTR.\nQualitative Analysis.For qualitative analysis, we com-\npare our CACViT with the state-of-the-art CNN-based\nmodel BMNet+ and state-of-the-art ViT-based model\nCounTR. As shown in Fig 8, CACViT outperforms other\nmethods in different scenes, including images with multiple\nclasses, or in dense environments and colorful environments.\nFor hard cases involving interference from other classes (the\n1st and 2nd column), our method allows better learning of\nthe characteristics of the objects to be counted.\nAblation Study\nWe perform the ablation study on FSC147 and provide quan-\ntitative results in Table 4. The plain version of CACViT is\nemployed as the baseline which only consists of standard\nfeature extraction and matching blocks by self-attention, and\nthe Aclass part in attention map is masked (B1 and B6).\nAdditional class information ofAclass. By comparing\nB6 and B7 in Table 4, one can observe that background sup-\npression can introduce a relative improvement of 15:44%\nw.r.t. MAE and23:12% w.r.t. RMSE on the test set. The per-\nformance boost indicates that exemplars can provide valid\ninformation for query features.\nAspect-ratio-aware scale embedding.As shown in Ta-\nble 4, the aspect-ratio-aware scale embedding brings a rel-\native improvement of 14:44% MAE and 16:87% RMSE on\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5837\n16\n65\n129\n17 16\n99\n119\n127\n100\n60\n46\n54\n59\n370\n437\n266\n360\n512\n410\n649\n533\nInputsGTBMNet+CounTROurs\nFigure 8: Qualitative results on the FSC147 dataset. Differ-\nent challenges are shown in the selected inputs, including\nimages with multiple classes, dense environments, and col-\norful environments. Our method consistently outperforms\nprevious methods with more precise locations.\nthe validation set and 5:80% MAE and 13:69% RMSE on\nthe test set (cf. B2 vs. B3 and B7 vs. B8), which veriï¬es\nthat the aspect-ratio-aware scale embedding can restore orig-\ninal size information for exemplars, and therefore matters\nin CAC task. Note that, the performance on RMSE further\ndemonstrates that the SE module can boost the robustness\nof the model. Besides, we provide a comparison with other\nscale embedding methods in the supplementary material.\nMagnitude Embedding. The comparison of B2 vs. B4\nand B7 vs. B9 demonstrates that magnitude embedding im-\nproves the validation RMSE by 6.2 and test RMSE by 7.35.\nHowever, the performance boost on MAE is marginal or\neven declines slightly. The reason may lies in that resized\nexemplars provide a wrong prior of object size which con-\nfuses the model.\nCooperation between Scale Embedding and Magni-\ntude Embedding.By Comparing B8 with B10 in Table 4,\none can observe that magnitude embedding can lead to a rel-\native improvement of 21:36% w.r.t. MAE and 41:05% w.r.t.\nRMSE on the test set. Note that the improvement is much\nmore signiï¬cant compared with the situation without scale\nembedding, which indicates that the combination of scale\nembedding and magnitude embedding can further boost the\nperformance. The synergy between the scale embedding and\nmagnitude embedding can provide scale prior for input and\noutput separately, therefore CACViT can capture the scale\nprior all the time.\nNo. CLS SE ME Val MAE Val RMSE\nB1 13.00 48.37\nB2 X 12.40 48.62\nB3 X X 10.61 40.42\nB4 X X 12.32 42.42\nB5 X X X 10.62 37.95\nNo. CLS SE ME Test MAE Test RMSE\nB6 13.67 117.60\nB7 X 11.56 90.41\nB8 X X 10.89 78.03\nB9 X X 11.61 83.06\nB10 X X X 9.13 48.96\nTable 4: Ablation study on the FSC147 dataset. CLS denotes\nthe additional component Aclass in self-attention compared\nwith previous methods, SE refers to scale embedding, and\nME denotes magnitude embedding.\nMethod Fine-tuned MAE RMSE\nFamNet+ Ã— 28.84 44.47\nRCAC Ã— 17.98 24.21\nSAFECount Ã— 16.66 24.08\nBMNet+ Ã— 10.44 13.77\nCACViT (Ours) Ã— 8.30 11.18\nFamNet+ X 18.19 33.66\nRCAC X 13.62 19.08\nSAFECount X 5.33 7.04\nBMNet+ X 5.76 7.83\nCounTR X 5.75 7.45\nCACViT (Ours) X 4.91 6.49\nTable 5: Generalization performance on the CARPK dataset.\nAll models are pre-trained on the FSC147 dataset without\nthe â€™carsâ€™ class. â€ï¬ne-tunedâ€ denotes whether the pre-trained\nmodels are further ï¬ne-tuned on the CARPK dataset.\nCross-Dataset Generalization\nWe test the generality of the model on a car counting dataset\nCARPK(Hsieh, Lin, and Hsu 2017). We exclude the â€œcarsâ€\ncategory within FSC147 to ensure that training and test cate-\ngories have no overlap. Results are reported in Table 5. One\ncan observe that our model exhibits strong generality. Com-\npared with BMNet+(Shi et al. 2022), CACViT generates a\nrelative performance gain of 20:50% MAE.\nConclusions\nIn this work, we propose a simple yet efï¬cient ViT-based\nmodel CACViT for CAC. Speciï¬cally, we show that the ViT\nis naturally suitable for the CAC task from a decoupled view.\nAnd we propose a ViT-based extract-and-match paradigm\nfor CAC. Then we introduce aspect-ratio-aware scale em-\nbedding and magnitude embedding to compensate for the in-\nformation loss. Our CACViT achieves stat-of-the-art results\non FSC147, and we also verify the generality on CARPK.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5838\nReferences\nAbousamra, S.; Hoai, M.; Samaras, D.; and Chen, C. 2021.\nLocalization in the crowd with topological constraints. In\nProceedings of the AAAI Conference on Artiï¬cial Intelli-\ngence, volume 35, 872â€“881.\nCao, X.; Wang, Z.; Zhao, Y .; and Su, F. 2018. Scale aggre-\ngation network for accurate and efï¬cient crowd counting. In\nProceedings of the European conference on computer vision\n(ECCV), 734â€“750.\nChen, Q.; Wu, Q.; Wang, J.; Hu, Q.; Hu, T.; Ding, E.; Cheng,\nJ.; and Wang, J. 2022. MixFormer: Mixing Features Across\nWindows and Dimensions. InProceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition\n(CVPR), 5249â€“5259.\nCheng, Z.-Q.; Dai, Q.; Li, H.; Song, J.; Wu, X.; and Haupt-\nmann, A. G. 2022. Rethinking spatial invariance of con-\nvolutional networks for object counting. In Proceedings of\nthe IEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 19638â€“19648.\nGong, S.; Zhang, S.; Yang, J.; Dai, D.; and Schiele, B. 2022.\nClass-Agnostic Object Counting Robust to Intraclass Diver-\nsity. In Computer Visionâ€“ECCV 2022: 17th European Con-\nference, Tel Aviv, Israel, October 23â€“27, 2022, Proceedings,\nPart XXXIII, 388â€“403. Springer.\nHan, G.; Ma, J.; Huang, S.; Chen, L.; and Chang, S.-F. 2022.\nFew-Shot Object Detection With Fully Cross-Transformer.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition (CVPR), 5321â€“5330.\nHe, K.; Chen, X.; Xie, S.; Li, Y .; Doll Â´ar, P.; and Girshick,\nR. 2022. Masked autoencoders are scalable vision learners.\nIn Proceedings of the IEEE/CVF Conference on Computer\nVision and Pattern Recognition, 16000â€“16009.\nHe, K.; Zhang, X.; Ren, S.; and Sun, J. 2016. Deep resid-\nual learning for image recognition. In Proceedings of the\nIEEE conference on computer vision and pattern recogni-\ntion, 770â€“778.\nHe, Y .; Ma, Z.; Wei, X.; Hong, X.; Ke, W.; and Gong, Y .\n2021. Error-aware density isomorphism reconstruction for\nunsupervised cross-domain crowd counting. In Proceedings\nof the AAAI conference on artiï¬cial intelligence, volume 35,\n1540â€“1548.\nHsieh, M.-R.; Lin, Y .-L.; and Hsu, W. H. 2017. Drone-based\nobject counting by spatially regularized regional proposal\nnetwork. In Proceedings of the IEEE international confer-\nence on computer vision, 4145â€“4153.\nIdrees, H.; Tayyab, M.; Athrey, K.; Zhang, D.; Al-Maadeed,\nS.; Rajpoot, N.; and Shah, M. 2018. Composition loss for\ncounting, density map estimation and localization in dense\ncrowds. In Proceedings of the European conference on com-\nputer vision (ECCV), 532â€“546.\nKim, W.; Son, B.; and Kim, I. 2021. ViLT: Vision-and-\nLanguage Transformer Without Convolution or Region Su-\npervision. In Meila, M.; and Zhang, T., eds., Proceedings\nof the 38th International Conference on Machine Learning,\nvolume 139 of Proceedings of Machine Learning Research,\n5583â€“5594. PMLR.\nLaradji, I. H.; Rostamzadeh, N.; Pinheiro, P. O.; Vazquez,\nD.; and Schmidt, M. 2018. Where are the blobs: Counting\nby localization with point supervision. InProceedings of the\neuropean conference on computer vision (ECCV), 547â€“562.\nLi, Y .; Mao, H.; Girshick, R.; and He, K. 2022. Exploring\nPlain Vision Transformer Backbones for Object Detection.\narXiv:2203.16527.\nLin, W.; Yang, K.; Ma, X.; Gao, J.; Liu, L.; Liu, S.; Hou, J.;\nYi, S.; and Chan, A. B. 2022. Scale-Prior Deformable Con-\nvolution for Exemplar-Guided Class-Agnostic Counting.\nLin, Y .; Yuan, Y .; Zhang, Z.; Li, C.; Zheng, N.; and Hu, H.\n2023. DETR Doesnâ€™t Need Multi-Scale or Locality Design.\narXiv:2308.01904.\nLiu, C.; Zhong, Y .; Zisserman, A.; and Xie, W. 2022.\nCountr: Transformer-based generalised visual counting.\narXiv preprint arXiv:2208.13721.\nLoshchilov, I.; and Hutter, F. 2017. Decoupled weight decay\nregularization. arXiv preprint arXiv:1711.05101.\nLu, E.; Xie, W.; and Zisserman, A. 2019. Class-agnostic\ncounting. In Computer Visionâ€“ACCV 2018: 14th Asian\nConference on Computer Vision, Perth, Australia, Decem-\nber 2â€“6, 2018, Revised Selected Papers, Part III 14, 669â€“\n684. Springer.\nLu, H.; Cao, Z.; Xiao, Y .; Zhuang, B.; and Shen, C. 2017.\nTasselNet: counting maize tassels in the wild via local\ncounts regression network. Plant methods, 13(1): 1â€“17.\nMadec, S.; Jin, X.; Lu, H.; De Solan, B.; Liu, S.; Duyme, F.;\nHeritier, E.; and Baret, F. 2019. Ear density estimation from\nhigh resolution RGB imagery using deep learning technique.\nAgricultural and forest meteorology, 264: 225â€“234.\nOnoro-Rubio, D.; and L Â´opez-Sastre, R. J. 2016. Towards\nperspective-free object counting with deep learning. In\nComputer Visionâ€“ECCV 2016: 14th European Conference,\nAmsterdam, The Netherlands, October 11â€“14, 2016, Pro-\nceedings, Part VII 14, 615â€“629. Springer.\nRanjan, V .; Sharma, U.; Nguyen, T.; and Hoai, M. 2021.\nLearning to count everything. In Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern\nRecognition, 3394â€“3403.\nShi, M.; Lu, H.; Feng, C.; Liu, C.; and Cao, Z. 2022. Repre-\nsent, compare, and learn: A similarity-aware framework for\nclass-agnostic counting. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition ,\n9529â€“9538.\nShu, W.; Wan, J.; Tan, K. C.; Kwong, S.; and Chan, A. B.\n2022. Crowd counting in the frequency domain. In Pro-\nceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, 19618â€“19627.\nTouvron, H.; Martin, L.; Stone, K.; Albert, P.; Almahairi, A.;\nBabaei, Y .; Bashlykov, N.; Batra, S.; Bhargava, P.; Bhosale,\nS.; Bikel, D.; Blecher, L.; Ferrer, C. C.; Chen, M.; Cucu-\nrull, G.; Esiobu, D.; Fernandes, J.; Fu, J.; Fu, W.; Fuller, B.;\nGao, C.; Goswami, V .; Goyal, N.; Hartshorn, A.; Hosseini,\nS.; Hou, R.; Inan, H.; Kardas, M.; Kerkez, V .; Khabsa, M.;\nKloumann, I.; Korenev, A.; Koura, P. S.; Lachaux, M.-A.;\nLavril, T.; Lee, J.; Liskovich, D.; Lu, Y .; Mao, Y .; Martinet,\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5839\nX.; Mihaylov, T.; Mishra, P.; Molybog, I.; Nie, Y .; Poul-\nton, A.; Reizenstein, J.; Rungta, R.; Saladi, K.; Schelten, A.;\nSilva, R.; Smith, E. M.; Subramanian, R.; Tan, X. E.; Tang,\nB.; Taylor, R.; Williams, A.; Kuan, J. X.; Xu, P.; Yan, Z.;\nZarov, I.; Zhang, Y .; Fan, A.; Kambadur, M.; Narang, S.; Ro-\ndriguez, A.; Stojnic, R.; Edunov, S.; and Scialom, T. 2023.\nLlama 2: Open Foundation and Fine-Tuned Chat Models.\narXiv:2307.09288.\nVaswani, A.; Shazeer, N.; Parmar, N.; Uszkoreit, J.; Jones,\nL.; Gomez, A. N.; Kaiser, Å.; and Polosukhin, I. 2017. At-\ntention is all you need. Advances in neural information pro-\ncessing systems, 30.\nWang, Q.; Gao, J.; Lin, W.; and Li, X. 2020. NWPU-crowd:\nA large-scale benchmark for crowd counting and localiza-\ntion. IEEE transactions on pattern analysis and machine\nintelligence, 43(6): 2141â€“2149.\nXu, Y .; Zhang, J.; ZHANG, Q.; and Tao, D. 2022. ViTPose:\nSimple Vision Transformer Baselines for Human Pose Esti-\nmation. In Koyejo, S.; Mohamed, S.; Agarwal, A.; Belgrave,\nD.; Cho, K.; and Oh, A., eds., Advances in Neural Informa-\ntion Processing Systems, volume 35, 38571â€“38584. Curran\nAssociates, Inc.\nXu, Y .; Zhang, J.; Zhang, Q.; and Tao, D. 2023. ViTPose++:\nVision Transformer Foundation Model for Generic Body\nPose Estimation. arXiv:2212.04246.\nYang, S.-D.; Su, H.-T.; Hsu, W. H.; and Chen, W.-C. 2021.\nClass-agnostic few-shot object counting. In Proceedings of\nthe IEEE/CVF Winter Conference on Applications of Com-\nputer Vision, 870â€“878.\nYao, J.; Wang, X.; Yang, S.; and Wang, B. 2023. ViTMatte:\nBoosting Image Matting with Pretrained Plain Vision Trans-\nformers. arXiv:2305.15272.\nYou, Z.; Yang, K.; Luo, W.; Lu, X.; Cui, L.; and Le, X. 2023.\nFew-shot object counting with similarity-aware feature en-\nhancement. In Proceedings of the IEEE/CVF Winter Con-\nference on Applications of Computer Vision, 6315â€“6324.\nYu, W.; Luo, M.; Zhou, P.; Si, C.; Zhou, Y .; Wang, X.; Feng,\nJ.; and Yan, S. 2022. MetaFormer Is Actually What You\nNeed for Vision. arXiv:2111.11418.\nZhang, C.; Li, H.; Wang, X.; and Yang, X. 2015. Cross-\nscene crowd counting via deep convolutional neural net-\nworks. In Proceedings of the IEEE conference on computer\nvision and pattern recognition, 833â€“841.\nZou, Z.; Qu, X.; Zhou, P.; Xu, S.; Ye, X.; Wu, W.; and Ye,\nJ. 2021. Coarse to ï¬ne: Domain adaptive crowd counting\nvia adversarial scoring network. In Proceedings of the 29th\nACM International Conference on Multimedia, 2185â€“2194.\nThe Thirty-Eighth AAAI Conference on Artiï¬cial Intelligence (AAAI-24)\n5840"
}