{
    "title": "DAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented System for Multilingual Named Entity Recognition",
    "url": "https://openalex.org/W4385571222",
    "year": 2023,
    "authors": [
        {
            "id": "https://openalex.org/A2635846673",
            "name": "Zeqi Tan",
            "affiliations": [
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2110550660",
            "name": "Shen Huang",
            "affiliations": [
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2988623467",
            "name": "Zixia Jia",
            "affiliations": [
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2128399176",
            "name": "Jiong Cai",
            "affiliations": [
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2100215807",
            "name": "Ying-hui Li",
            "affiliations": [
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2112122693",
            "name": "Weiming Lu",
            "affiliations": [
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2110072997",
            "name": "Yueting Zhuang",
            "affiliations": [
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A2630071572",
            "name": "Kewei Tu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2944937469",
            "name": "Pengjun Xie",
            "affiliations": [
                "University Town of Shenzhen"
            ]
        },
        {
            "id": "https://openalex.org/A1936961387",
            "name": "Fei Huang",
            "affiliations": [
                "University Town of Shenzhen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W4301683486",
        "https://openalex.org/W4288087322",
        "https://openalex.org/W4287854446",
        "https://openalex.org/W4385573951",
        "https://openalex.org/W2095705004",
        "https://openalex.org/W4287887065",
        "https://openalex.org/W4226082499",
        "https://openalex.org/W4294294857",
        "https://openalex.org/W4221167171",
        "https://openalex.org/W3177049011",
        "https://openalex.org/W4224217521",
        "https://openalex.org/W2913295164",
        "https://openalex.org/W2952087486",
        "https://openalex.org/W3104415840",
        "https://openalex.org/W4385572425",
        "https://openalex.org/W2768282280",
        "https://openalex.org/W2963341956",
        "https://openalex.org/W4389519817",
        "https://openalex.org/W4293791009",
        "https://openalex.org/W3162486530",
        "https://openalex.org/W4252076394",
        "https://openalex.org/W3184537812",
        "https://openalex.org/W3156534109",
        "https://openalex.org/W2880875857",
        "https://openalex.org/W2574344498",
        "https://openalex.org/W3204342617",
        "https://openalex.org/W3035390927",
        "https://openalex.org/W2171184311",
        "https://openalex.org/W3027879771",
        "https://openalex.org/W4225513420",
        "https://openalex.org/W3173727191",
        "https://openalex.org/W4226278401",
        "https://openalex.org/W2103076621",
        "https://openalex.org/W3091432621",
        "https://openalex.org/W4389519502"
    ],
    "abstract": "Zeqi Tan, Shen Huang, Zixia Jia, Jiong Cai, Yinghui Li, Weiming Lu, Yueting Zhuang, Kewei Tu, Pengjun Xie, Fei Huang, Yong Jiang. Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023). 2023.",
    "full_text": "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 2014–2028\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nDAMO-NLP at SemEval-2023 Task 2: A Unified Retrieval-augmented\nSystem for Multilingual Named Entity Recognition\nZeqi Tan†, Shen Huang⋆†, Zixia Jia☼†, Jiong Cai☼†, Yinghui Li♥†, Weiming Lu\nYueting Zhuang, Kewei Tu☼, Pengjun Xie⋆, Fei Huang⋆, Yong Jiang⋆∗\n⋆DAMO Academy, Alibaba Group\nCollege of Computer Science and Technology, Zhejiang University\n☼School of Information Science and Technology, ShanghaiTech University\n♥Tsinghua Shenzhen International Graduate School, Tsinghua University\n{zqtan,yzhuang,luwm}@zju.edu.cn liyinghu20@mails.tsinghua.edu.cn\n{jiazx,caijiong,tukw}@shanghaitech.edu.cn\n{pangda,chengchen.xpj,f.huang,yongjiang.jy}@alibaba-inc.com\nAbstract\nThe MultiCoNER II shared task aims to tackle\nmultilingual named entity recognition (NER)\nin fine-grained and noisy scenarios, and it in-\nherits the semantic ambiguity and low-context\nsetting of the MultiCoNER I task. To cope with\nthese problems, the previous top systems in the\nMultiCoNER I either incorporate the knowl-\nedge bases or gazetteers. However, they still\nsuffer from insufficient knowledge, limited con-\ntext length, single retrieval strategy. In this\npaper, our team DAMO-NLP proposes a uni-\nfied retrieval-augmented system (U-RaNER)\nfor fine-grained multilingual NER. We perform\nerror analysis on the previous top systems and\nreveal that their performance bottleneck lies in\ninsufficient knowledge. Also, we discover that\nthe limited context length causes the retrieval\nknowledge to be invisible to the model. To en-\nhance the retrieval context, we incorporate the\nentity-centric Wikidata knowledge base, while\nutilizing the infusion approach to broaden the\ncontextual scope of the model. Also, we ex-\nplore various search strategies and refine the\nquality of retrieval knowledge. Our system 1\nwins 9 out of 13 tracks in the MultiCoNER II\nshared task. Additionally, we compared our\nsystem with ChatGPT, one of the large lan-\nguage models which have unlocked strong ca-\npabilities on many tasks. The results show that\nthere is still much room for improvement for\nChatGPT on the extraction task.\n1 Introduction\nThe MultiCoNER series shared task (Malmasi\net al., 2022b; Fetahu et al., 2023b) aims to iden-\ntify complex named entities (NE), such as titles\n∗: project lead. †: equal contributions.\nThis work was done during Zeqi Tan, Zixia Jia, Jiong Cai, and\nYinghui Li’s internship at DAMO Academy, Alibaba Group.\n1We will release the dataset, code, and scripts of\nour system at https://github.com/modelscope/\nAdaSeq/tree/master/examples/U-RaNER.\nIt is ﬁrst conceptualized by Erving Goffman in 1959.\nWrong prediction in RaNER:\nInput with gold annotations:\nIt is ﬁrst conceptualized by  \nErving Goffman in 1959.\nKnowledge used in RaNER:\nGoffman's The Presentation of Self in Everyday \nLife was published in 1956, with a revised \nedition in 1959. He had developed the book's \ncore ideas from his doctoral dissertation.\nArtist\nOtherPER\nFigure 1: An example of wrong prediction in RaNER\n(Wang et al., 2022b) (one of the top systems in the\nMultiCoNER I task (Malmasi et al., 2022b)) . This case\nillustrates that the knowledge covered is not sufficient\nfor fine-grained complex NER.\nof creative works, which do not possess the tra-\nditional characteristics of named entities, such as\npersons, locations, etc. It is challenging to identify\nthese ambiguous complex entities based on short\ncontexts (Ashwini and Choi, 2014; Meng et al.,\n2021; Fetahu et al., 2022). The MultiCoNER I task\n(Malmasi et al., 2022b) focuses on the problem of\nsemantic ambiguity and low context in multilin-\ngual named entity recognition (NER). In addition,\nthe MultiCoNER II task (Fetahu et al., 2023b) this\nyear poses two major new challenges: (1) a fine-\ngrained entity taxonomy with 6 coarse-grained cat-\negories (Location, Creative Work, Group,\nPerson, Product and Medical) and 33 fine-\ngrained categories, and (2) simulated errors added\nto the test set to make the task more realistic and\ndifficult, like the presence of spelling mistakes.\nThe previous top systems (Wang et al., 2022b;\nChen et al., 2022) of the MultiCoNER I task\nincorporate additional knowledge in pre-trained\nlanguage models, either a knowledge base or a\ngazetteer. RaNER (Wang et al., 2022b) builds a\nmultilingual knowledge base based on Wikipedia\nand the original input sentences are then aug-\nmented with retrieved contexts from the knowl-\n2014\nedge base, allowing the model to access more\nrelevant knowledge. GAIN (Chen et al., 2022)\nproposes a gazetteer-adapted integration network\nwith a gazetteer built from Wikidata to improve the\nperformance of language models. Although these\nsystems achieve impressive results, they still have\nsome drawbacks. First, insufficient knowledge is\na common problem. As shown in Figure 1, the\nknowledge used in RaNER can help the model to\nidentify Erving Goffman as a person, but cannot fur-\nther determine the fine-grained category Artist.\nSecond, these methods mostly suffer from the lim-\nited context length. Wang et al. (2022b) discards\nstitched text that is longer than 512 after tokeniz-\ning, which means that plenty of retrieved context is\nnot visible to the model, leading to resource waste.\nThird, these systems have a single retrieval strat-\negy. Wang et al. (2022b) acquires knowledge by\ntext retrieval, while Chen et al. (2022) accesses\nknowledge by dictionary matching. This single\nway of knowledge acquisition will result in the\nunderutilization of knowledge.\nTo tackle these problems, we propose a unified\nretrieval-augmented system (U-RaNER) for fine-\ngrained multilingual NER. We use both Wikipedia\nand Wikidata knowledge bases to build our retrieval\nmodule so that more diverse knowledge can be\nconsidered. As shown in Figure 1, if we locate the\nentry for Erving Goffman in Wikidata, we can make\nuse of fine-grained entity category information to\nfacilitate predictions. Also, we discover that the\nretrieval context dropped by the model may also\ncontain useful knowledge. Thus, we explore the\ninfusion approach to make more context visible to\nthe model. In addition, we use multiple retrieval\nstrategies to obtain the most relevant knowledge\nfrom two knowledge bases, further improving the\nmodel performance.\nOur main contributions are as follows:\n1. We propose a unified retrieval-augmented sys-\ntem for fine-grained multilingual NER. Our sys-\ntem incorporates more diverse knowledge bases\nand significantly improves the system perfor-\nmance compared to baseline systems (Section\n§ 4, § 5)\n2. We initiated our investigation by identifying\nthe primary bottleneck of the previous top-\nperforming system, which we determined to\nbe insufficient knowledge. Consequently, we\nfocused on exploring both data and model en-\nhancements to improve system performance.\n(Section § 3)\n3. We employ multiple retrieval strategies to ob-\ntain entity information from Wikidata, in order\nto complement the missing entity knowledge.\n(Section § 4.1)\n4. Additionally, we utilize the infusion approach\nto provide a more extensive contextual view to\nthe model, thus enabling better utilization of the\nretrieved context (Section § 4.2).\n5. Extensive experimental analysis demonstrates\nthe effectiveness of diverse knowledge sources\nand broader contextual scopes for improving\nmodel performance. (Section § 5)\n2 Related Work\nNamed Entity Recognition (NER) (Sundheim,\n1995) is a fundamental task in Natural Language\nProcessing. Because of the long-term attention\nand the rapid development of pre-trained language\nmodels, various models (Akbik et al., 2018; De-\nvlin et al., 2019; Yamada et al., 2020; Wang et al.,\n2020, 2021a) have achieved state-of-the-art re-\nsults and performance in general NER scenarios\nand datasets, such as CoNLL 2002 (Sang, 2002),\nCoNLL 2003 (Tjong Kim Sang and De Meul-\nder, 2003), and OntoNotes 5.0 (Pradhan et al.,\n2013). Considering that the previous task set-\ntings or datasets are monolingual and scenario-\nconstrained, the task of Multilingual Complex\nNamed Entity Recognition (MultiCoNER) is pro-\nposed to promote the NER research to be more\noriented to real scenarios (Malmasi et al., 2022b).\nOur work focuses on this task and we will intro-\nduce the related work from the dataset and method\nof MultiCoNER respectively:\nChallenges of MultiCoNER Dataset To address\ncontemporary in the NER field, Malmasi et al. con-\nstruct MultiCoNER, a large and complex dataset\nfor Multilingual Complex Named Entity Recogni-\ntion. This 26M token dataset covers 3 domains\n(including Wiki, question, and search query) and\n11 languages (12 languages for SemEval-2023). In\nparticular, aiming at the main challenges of NER\nresearch, the MultiCoNER dataset sets 4 key char-\nacteristics: (1) Low Context: Existing NER meth-\nods perform poorly if the context is less informa-\ntive (Meng et al., 2021), thus, texts in MultiCoNER\nare low in context to assess the model’s perfor-\nmance on the more realistic and difficult setting.\n(2) Sufficient Diversity: MultiCoNER contains\nan rich variety of entity types, both simple and\n2015\ndifficult, which makes it possible to evaluate the\nmodel more comprehensively. (3) Reasonable Dis-\ntribution: Considering the non-negligible long-tail\ndistribution problem faced by the previous datasets\nmakes the construction of training data extremely\ndifficult, MultiCoNER ensures that the distribu-\ntion of its entities is more even and reasonable\nso that it can be evaluated comprehensively. (4)\nHigh Complexity: Increasing the complexity of\nthe dataset can effectively improve the quality of\nthe dataset (Fetahu et al., 2021). Therefore, in ad-\ndition to monolingual subsets, MultiCoNER also\ndistinctively contains a multilingual subset and a\ncode-mixed one, which makes it more challenging.\nNote that in the dataset version of SemEval-2023,\nthis challenge and setting do not exist.\nProgress of MultiCoNER Methods With the\nMultiCoNER dataset as the core, the SemEval-\n2022 Task 11 attracts 236 participants, and 55\nteams successfully submit their system (Malmasi\net al., 2022b). Among them, there are many\nsuccessful and excellent works worthy of discus-\nsion. DAMO-NLP (Wang et al., 2022b) proposes\na knowledge-based method that gets multilingual\nknowledge from Wikipedia to provide informative\ncontext for the NER model. And they achieve the\nprevious best overall performance on the Multi-\nCoNER dataset. USTC-NELSLIP (Chen et al.,\n2022) proposes a gazetteer-adapted integration net-\nwork to improve the model performance for rec-\nognizing complex entities. QTrade AI (Gan et al.,\n2022) designs kinds of data augmentation strategies\nfor the low-resource mixed-code NER task. Previ-\nous efforts and studies on the MultiCoNER dataset\nhave shown that external data and beneficial knowl-\nedge are essential to improve the performance of\nNER models on it.\nRetrieval-augmented NLP Methods Retrieval-\naugmented techniques have proven to be highly\neffective in various natural language processing\n(NLP) tasks, as evidenced by the exceptional per-\nformance achieved in prior studies (Lewis et al.,\n2020; Khandelwal et al., 2019; Borgeaud et al.,\n2022). These approaches usually contain two parts:\nan information retrieval module and a task-specific\nmodule. Specifically, in the context of named entity\nrecognition (NER), Wang et al. (2021b) proposes\nleveraging off-the-shelf search engines like Google\nto retrieve external information and enhance the\ncontextual representations of tokens in the input\nLanguage Data Type P R F1 Ratio\nBN\nTotal 90.99 92.60 91.79 1.00\nIn-context 92.86 94.66 93.75 0.69\nOut-of-context 88.06 89.39 88.72 0.31\n∆ 4.80 5.27 5.03 -\nDE\nTotal 81.83 83.00 82.41 1.00\nIn-context 83.80 88.11 85.90 0.54\nOut-of-context 80.17 78.98 79.57 0.46\n∆ 3.63 9.13 6.33 -\nZH\nTotal 76.71 78.40 77.54 1.00\nIn-context 79.27 83.87 81.50 0.26\nOut-of-context 76.04 77.02 76.53 0.74\n∆ 3.23 6.85 4.97 -\nTable 1: The performance and ratio for different types\nof data on BN, DE and ZH.\ntext, resulting in improved performance. Further-\nmore, subsequent research has focused on devel-\noping task-specific retrieval systems for domain-\nspecific NER and multi-modal NER tasks, respec-\ntively (Zhang et al., 2022b; Wang et al., 2022a).\nDrawing upon these insights, our proposed sys-\ntem is designed and optimized with guidance from\nthese previous works.\n3 Data\nThe MultiCoNER II corpus (Fetahu et al., 2023a)\naims to recognize the complex named entities and\npose new challenges for current NER systems. To\nmeet these challenges, we first reproduce the results\nof the top system (Wang et al., 2022b) and perform\nerror analysis on validation sets. We observe that\nthe performance bottleneck of the system lies in the\nlack of knowledge. Then, we investigate to break\nthis bottleneck from data and model perspectives\nand improve model robustness.\nFollowing Wang et al. (2022b), we build a mul-\ntilingual KB based on Wikipedia of the 12 lan-\nguages to search for the related documents. We\ndownload the latest (2022.10.21) version of the\nWikipedia dump from Wikimedia2 and convert it\nto plain texts. We execute the official system on\nMultiCoNER II corpus and categorize the results\naccording to whether the annotated entity appears\nin the retrieval context or not. As shown in Table 1,\nthe F1-measure on different types of test data dif-\nfers significantly, e.g., 6.33% on DE and 4.97% on\nZH. This indicates that the lack of knowledge about\nentities in the retrieval context can have a signifi-\ncant impact on the model performance. With this\ninsight, we consider data and model dimensions to\ncompensate for this lack of knowledge.\n2https://dumps.wikimedia.org/\n2016\nRetrieval Strategy Query Retrieval Result\nTEXT2TEXT from 1995 to 2011 deal hudson\nwas the magazine’s publisher.\n1. In 1995 Hudson became publisher of the conservative\nRoman Catholic magazine, Crisis.\n2. Hudson is the former publisher and editor of\n3. Hudson also hosts the radio show Church and Culture\non Ave Maria Radio...\nTEXT2ENT from 1995 to 2011 deal hudson\nwas the magazine’s publisher.\n1. Deal W. Hudson\n2. Deal Wyatt Hudson\n3. S. Hudson...\nENT2ENT [deal hudson]\nType: human\nDescription: Hudson is the former publisher and editor\nof Crisis Magazine and InsideCatholic.com.\nTable 2: Examples of different retrieval strategies related to the input sentence: \"from 1995 to 2011 deal hudson was\nthe magazine’s publisher.\"with its corresponding entity \"deal hudson\".\nBN DE HI ZH\nEntity Coverage\n73.4\n64.3\n77.9\n35.3\n78.0\n69.3\n81.6\n37.0\n77.1\n72.3\n82.4\n47.9\nBase\nMore Context\nMore Database\nFigure 2: Entity coverage of the retrieval context for the\nannotated entities within the query sentence.\nWhile Chen et al. (2022) uses Wikidata to build\ntheir gazetteer, we explore to enhance our retrieval\nsystem with Wikidata. Wikidata is a free and entity-\ncentric knowledge base. Every entity of Wikidata\nhas a page consisting of a label, several aliases, de-\nscriptions, and one or more entity types. As shown\nin Figure 2,Base indicates that only the Wikipedia\nknowledge base is used, and More Database\nindicates that we use both Wikipedia and Wikidata\nknowledge bases. The entity coverage improves\non all 4 languages and achieves the maximum gain\nof 12.6% on ZH. In addition, as More Context\nshows, expanding the length of the retrieval context\nalso brings more entity knowledge. Thus, we use\nthe infusion approach to make more retrieval con-\ntext visible to model. More details are described in\nSection § 4.2.\n4 Methodology\nOverview As depicted in Fig. 3, U-RaNER is\ncomprised of two parts: a retrieval augmentation\nmodule and a NER module. The retrieval augmen-\ntation module utilizes multiple retrieval strategies\nand the NER module adopts a modified transformer\nstructure to utilize the retrieved knowledge. Given\nan input sentence, U-RaNER retrieves similar texts\nand entities as external knowledge, which are then\nutilized in the form of text and vectors to help the\nNER module obtain improved predictions.\n4.1 Retrieval Augmentation Module\nIn the retrieval augmentation module, we de-\nsign three different retrieval strategies, namely\nTEXT2TEXT, TEXT2ENT, and ENT2ENT, which\naim to obtain a variety of useful information from\ndifferent sources to enhance our NER model.\nTEXT2TEXT The TEXT2TEXT retrieval strat-\negy is to obtain texts related to input sentences\nfrom Wikipedia by the way of sparse retrieval (Mc-\nDonell, 1977; Robertson and Zaragoza, 2009).\nThrough this form of retrieval, the goal is to obtain\nadditional and useful relevant information as much\nas possible to alleviate the low-context problem of\nMultiCoNER. Specifically, we first parse the latest\nWikipedia dumps and use ElasticSearch 3 to index\nthem. And finally, we use each sentence in the\ndataset as the query and use the BM25 retrieval al-\ngorithm that comes with ElasticSearch to search in\nthe built index database to obtain the Top-K docu-\nments related to the input sentence from Wikipedia,\nas shown in the first example of Table 2. Note that\nthe TEXT2TEXT strategy is used by Wang et al.\n(2022b) to win 10 out of 13 tracks when competing\nin the SemEval-2022 Task 11.\nTEXT2ENT The TEXT2ENT retrieval strategy\naims to retrieve candidate entities that may be men-\n3https://github.com/elastic/\nelasticsearch\n2017\nIt is ﬁrst conceptualized by\nErving Goﬀman in 1959\nInput Sentence NER Predictions \nIt is ﬁrst conceptualized by\nErving Goﬀman in 1959\nArtist\nAdd & Norm\nFFN\nAdd & Norm\nMulti-Head Attention\nNER Module\nCRF\nEmbedding\nWikipedia\nTEXT2TEXT \nTEXT2ENT ENT2ENT \nRetrieval Augmented Module\nPLM\nVectorize\ntext \nvectors \nPre-Infusion Post-Infusion\nLayers\nWikidata\nFigure 3: Overall architecture of U-RaNER.\ntioned in input sentences, as illustrated in the sec-\nond example of Table 2. We believe that if the\ncandidate entities that may be mentioned in the\nsentence can be retrieved in advance, the related\nknowledge might be helpful to build a stronger en-\ntity recognition model. The TEXT2ENT strategy is\ninspired by the related technologies of dictionary\ndisambiguation (Harige and Buitelaar, 2016) and\nentity linking (Cao et al., 2021). But dictionary dis-\nambiguation can only perform hard matching, and\nthere is no detailed annotation information for en-\ntity linking (that is, the corresponding information\nbetween span and entity), so these two traditional\nmethods cannot be directly applied to our scene.\nTherefore, in this part of the specific practice, we\ntried two different retrieval methods, namely sparse\nretrieval and dense retrieval. The details of these\ntwo retrieval methods are in the Appendix A.4.\nENT2ENT The ENT2ENT retrieval strategy aims\nto retrieve some entities and their corresponding\ninformation from Wikidata. Wikidata integrates\nbillions of structural information between millions\nof entities, such as the alias of entities and the\nrelationships of entity pairs. And intuitively, such\ninformation is beneficial to our NER model.\nIn the process of ENT2ENT retrieval, we want\nto find out external entity types which maybe in-\nspire the entity labeling of the input sentence. Con-\ncretely, for each given entity, we first retrieve Wiki-\ndata to get its relevant Wikidata entities. Next, we\ngather and utilize the properties of the Wikidata en-\ntities from their corresponding Wikidata pages. In\nparticular, we take the “instance of\" and “sub-class\nof\" properties as the entity types. For example,\nas shown in Table 2, with entity “deal hudson” as\nthe query, ENT2ENT strategy will retrieve its type\n(i.e., “human”) and description text. Finally, all\nrelevant Wikidata entities and their types are as the\nretrieved augmented data. The detailed procedure\nfor ENT2ENT is in the Appendix A.5.\n4.2 Named Entity Recognition Module\nBERT-CRF We use xlm-roberta-large (XLM-\nR) (Conneau et al., 2020) as the PLMs for\nall the tracks. Given an input sentence x =\nx1,x2,...,x n, transformer-based standard fine-\ntuning for NER first feeds the input sentence x\ninto the PLMs to get the token representations h.\nThe token representations h are fed into a CRF\nlayer to get the conditional probability pθ(y |h),\nand the model is trained by maximizing the condi-\ntional probability and minimizing the cross entropy\nloss: L= −log pθ(y |h).\nRaNER Given the retrieval context ˜ x, we define\na neural network parameterized by θ that learns\nfrom a concatenated input [x; ˜x]. We feed the input\nand retrieve the representation [h; ˜h]:\n[h;˜h] = [h(1),...h(n),˜h(1),...˜h(n)] = embed([x;˜x]) (1)\nWe then feed h into the CRF layer and train by\nminimizing the conditional probability pθ(y |h)\nas mentioned above.\nU-RaNER To exploit more retrieval contexts,\nwe first slice ˜x by model-limited input length\nas ˜x = ˜x0,˜x1,..., ˜xm. Then, we keep ˜x0 as\nthe text for concatenation, and feed the rest con-\ntext list into PLM as [(x; ˜x1),..., (x; ˜xm)], which\nis used in Lewis et al. (2020) for better infor-\nmation interaction, and get the token vector list\n2018\n[(h1; ˜h1),..., (hm; ˜hm)]. Afterwards, we consider\ntwo infusion (Pre-Infusion and Post-Infusion) ap-\nproaches using the representation [˜h1,..., ˜hm] and\n[h1,..., hm], respectively.\nFor Pre-Infusion, we fetch the token vec-\ntors of the corresponding positions of the anchors\nfrom the vector list [˜h1,..., ˜hm]. Then, we per-\nform the mean operation to obtain the set of anchor\nvectors V∈ Rp×d, pis the number of anchors, and\nd is the hidden size. Considering that the word\nembedding layer in XLM-R has two input modes,\nincluding vocabulary index input as well as word\nembedding input, we first perform the former for\n[x; ˜x0] to obtain the input text embedding E, and\nlater concatenate E and the anchor vectors Vto\nform the word embedding input. Finally, we get\nthe representation [h; ˜h0; ˜hv]. We only use h to\npass the CRF layer.\nFor Post-Infusion, we first feed [x; ˜x0] to\nXLM-R and get the token representation [h; ˜h0].\nFor input representation list [h; h1,..., hm], we\nperform the max operation on the token dimen-\nsion to obtain the final representation hmax. Then,\nwe use hmax for calculation as in BERT-CRF. No-\ntably, we find that the post-infusion method is supe-\nrior to the pre-infusion method in our preliminary\nexperiments, and the default infusion method in the\nexperimental section is post-infusion.\n4.3 Ensemble Module\nGiven predictions {ˆyθ1 ,··· ,ˆyθm}from mmodels\nwith different random seeds, we use majority vot-\ning to generate the final prediction ˆy. Following\nYamada et al. (2020); Wang et al. (2022b), the mod-\nule ranks all spans in the predictions by the number\nof votes in descending order and selects the spans\nwith more than 50% votes into the final prediction.\nThe spans with more votes are kept if the selected\nspans have overlaps and the longer spans are kept\nif the spans have the same votes.\n5 Experimental Setup\n5.1 Datasets and Evaluation Metrics\nWe use the official MultiCoNER II dataset (Fetahu\net al., 2023a) in all tracks to train our models. The\ndetailed data statistics is in the Appendix A.1 and\nA.3. The results on the leaderboard are evaluated\nwith the entity-level macro F1 scores, which treat\nall the labels equally 4.\n4In comparison, most of the publicly available NER\ndatasets (e.g., CoNLL 2002, 2003 datasets) are evaluated with\n5.2 Training Strategy\nNER Model Training Our final NER models\nare trained on the combined dataset including both\nthe training and development sets on each track to\nfully utilize the labeled data. For models trained\non the combined dataset, we use the final model\ncheckpoint after training. The detailed system con-\nfigurations is in the Appendix A.2\nMulti-stage Fine-tuning Multi-stage fine-tuning\n(MSF) aims at transferring the parameters of fine-\ntuned embeddings in a model at an early stage into\nother models in the next stage Shi and Lee (2021).\nThe approach stores the checkpoint of fine-tuned\nXLM-R embeddings at the early stage and uses it as\nthe initialization of XLM-R embeddings for model\ntraining at the next stage. Wang et al. (2022b) ex-\nperimentally demonstrates that MSF can leverage\nthe annotations from all tracks and thus improve\nperformance and accelerate training. In addition,\nwe observe that inconsistent training set sizes on\ndifferent language tracks can also lead to degra-\ndation of model performance. We use increasing\nbatch size and upsampling strategy to address this\nissue. The details are shown in the Appendix B.1.\n5.3 Baselines\nIn this paper, we compare the proposed U-RaNER\nwith the following baseline models:\n• BERT-CRF, as introduced in 4.2, is com-\nposed of a BERT-like encoder and a CRF de-\ncoder . It is widely used for sequence labeling\ntasks. We use xlm-roberta-large (XLM-R)\n(Conneau et al., 2020) as the pretrained back-\nbone for all the tracks.\n• RaNER, as introduced in 4.2, improves\nBERT-CRF by incorporating retrieval con-\ntexts as input for better performance. Re-\ntrieval augmented methods have proven to\nbe highly effective in the NER task(Wang\net al., 2021b; Zhang et al., 2022b; Wang et al.,\n2022a).\n• RaNER-MSF (Wang et al., 2022b) achieves\nthe previous best overall performance on the\nMulti-CoNER I dataset, which exploits multi-\nstage fine-tuning to leverage the annotations\nthe entity-level micro F1 scores, which emphasize common\nlabels (Akbik et al., 2018; Devlin et al., 2019; Yamada et al.,\n2020; Wang et al., 2022b). Except for the results in Table 3,\nthe following results are entity-level micro F1 scores if not\notherwise specified.\n2019\nSystem EN ES SV UK PT FR FA DE ZH HI BN IT MULTI A VG.\nBERT-CRF 62.80 65.34 68.68 67.68 64.37 66.05 60.70 69.44 62.02 73.08 71.82 68.15 63.27 66.42\nNLPeople 71.81 72.76 75.08 73.41 70.16 72.85 70.76 77.67 65.96 78.50 78.24 73.71 78.38 73.79\nUSTC-NELSLIP 72.15 74.44 75.47 74.37 71.26 74.25 68.85 78.71 66.57 82.14 80.59 75.70 75.6274.62\nIXA/Cogcomp 72.82 73.81 76.54 75.25 72.28 74.52 69.49 80.35 64.86 79.56 78.95 74.67 78.17 74.71\nCAIR-NLP 79.33 83.63 82.88 81.29 80.16 83.08 77.50 74.71 58.43 72.23 69.46 83.78 79.16 77.36\nPAI 80.00 71.67 72.38 71.28 81.61 86.17 68.46 88.09 74.87 80.96 84.39 84.88 77.00 78.60\nNetEase.AI - - - - - - - - 84.05 - - - - -\nOurs 85.53 89.78 89.57 89.02 85.97 89.59 87.93 84.97 75.98 78.56 81.60 89.79 84.48 85.60\nTable 3: Part of the official results on the leaderboard. BERT-CRF is the post-evaluation results of our baseline\nsystem (BERT-CRF) on the released test set.\nfrom all tracks and thus improve performance\nand accelerate training of RaNER.\n• ChatGPT5, also known as\ngpt-3.5-turbo, is the most capable\nGPT-3.5 (Ouyang et al., 2022) model and\noptimized for chat. Following (Lai et al.,\n2023), our prompt structure for ChatGPT\nconsists of a task description, a note for\noutput format, and an input sentence. Despite\na Single-turn prompting strategy, we\nadditionally try two enhanced prompting\nstrategies: Multi-turn and Multi-ICL.\nMulti-turn first performs the task in 6\ncoarse-grained categories, and later performs\nfiner-grained NER. Multi-ICL constructs\ndemonstrations spliced after the note part\nby randomly selecting examples from the\ntraining set. The detailed prompting pro-\ncedure for Single-turn, Multi-turn\nand Multi-ICL is in the Appendix A.6.\n6 Results and Analysis\n6.1 Main Results\nThere are 45 teams that participated in the Multi-\nCoNER II shared task. Due to limited space, we\nonly compare our system with the systems from\nteams NLPeople, USTC-NELSLIP, IXA/Cogcomp,\nCAIR-NLP, PAI and NetEase.AI6. As NetEase.AI\nsolely took part in the Chinese track, which means\nwe only have access to their results for this specific\ntrack. In the post-evaluation phase, we evaluate\nthe baseline system without the use of additional\nknowledge bases to further show the effectiveness\nof our retrieval-augmented system. The official\nresults and the results of our baseline system are\nshown in Table 3. Our system performs the best\n5https://openai.com/blog/chatgpt/\n6Please refer to https://multiconer.github.\nio/results for more details about the results.\non 9 out of 13 tracks with the average result ex-\nceeding the second-place system by the absolute\nF1-measure of 7.0%. Moreover, our system out-\nperforms our baseline by the 19.18% F1-measure\non average, which demonstrates that the retrieval-\naugmented system based on multiple knowledge\nbases is extremely helpful in identifying complex\nentities, leading to significant improvement on\nmodel performance.\nIn addition, we use three prompting strategies\nto evaluate ChatGPT. Due to the overwhelming\nnumber of test sets (millions of levels), the expense\nof invoking the OpenAI interface is unaffordable.\nWe experiment on the validation set and the results\nare in Table 4. We observe that ChatGPT’s per-\nformance on the multilingual NER dataset is quite\npoor, with an average F1-score of only 14.78% by\nthe best strategy. Even on the coarse-grained level\nthe result is merely 29.70% (Table 5), which is com-\nparable to the result measured on MultiCoNER I\n(Malmasi et al., 2022b) by Lai et al. (2023).\n6.2 Ablation Study\nIn this section, we perform extensive ablation ex-\nperiments to show the effectiveness of various set-\ntings in our retrieval-augmented system. Following\nWang et al. (2022b), we employ the multi-stage\nfine-tuning (MSF) training strategy. As shown in\nTable 4, the model performance improves from\n87.95% to 89.92%, which illustrates the effective-\nness of the multi-stage training. Note that the fol-\nlowing five rows in Table 4 all use the MSF training\nstrategy.\nFor the different knowledge sources, the use\nof Wikipedia data achieves the gain of 12.61%\n(RaNER-MSF vs. BERT-CRF), the use of wiki-\ndata data achieves the gain of 13.16% (ENT2ENT⋆\nvs. BERT-CRF), and using both together achieves\nthe maximum gain of 15.46% ( ENT2ENT vs.\nBERT-CRF). This shows that knowledge is highly\n2020\nMethod △ † ‡BN DE EN ES FA FR HI IT PT SV UK ZH A VG.\nChatGPT w/\nSingle-turn✗ ✗ ✗ 7.24 10.06 13.36 12.44 10.94 11.05 9.04 16.32 17.27 18.03 10.88 5.02 11.80\nMulti-turn✗ ✗ ✗ 8.12 14.57 15.38 15.52 12.75 13.60 9.17 17.81 17.70 20.38 14.25 5.60 13.74\nMulti-ICL✗ ✗ ✗ 9.76 14.84 17.65 16.28 14.11 13.95 10.48 18.63 18.84 20.94 15.57 6.34 14.78\nBERT-CRF ✗ ✗ ✗ 86.98 76.08 72.61 75.66 69.37 74.44 85.46 80.70 76.54 78.48 76.30 75.1177.31\nRaNER ✗ ✓ ✗ 92.30 84.29 84.32 88.81 87.85 86.77 91.75 91.08 88.45 89.74 88.46 81.5587.95\nRaNER-MSF✗ ✓ ✗ 93.11 86.81 86.82 90.90 89.52 88.99 93.97 92.42 90.75 91.93 90.93 82.8389.92\nU-RaNER w/\nTEXT2ENT⋆ ✗ ✗ ✓ 89.87 85.83 87.54 88.03 86.44 83.86 86.82 91.19 78.92 86.20 84.26 85.6286.22\nENT2ENT⋆ ✗ ✗ ✓ 94.45 88.85 88.11 91.34 89.70 89.96 94.68 91.53 90.15 91.68 88.21 87.0290.47\nTEXT2TEXT✓ ✓ ✗ 94.36 87.79 88.07 92.57 90.91 91.80 94.25 93.60 91.94 93.02 91.40 84.1191.15\nTEXT2ENT ✓ ✓ ✓94.77 89.48 89.88 93.46 90.80 90.83 94.57 93.83 92.12 93.20 91.12 89.4191.96\nENT2ENT ✓ ✓ ✓94.96 90.36 90.62 93.51 91.85 92.88 95.12 94.60 92.90 94.45 91.57 90.3892.77\nTable 4: The top bar shows ChatGPT’s performance (micro-F1 scores) using three prompting strategies, the former\ntwo being zero-shot learning and Multi-ICL being few-shot learning. Following the comparison between the\ntop system (Wang et al., 2022b) in the MultiCoNER I and the three variants of our method on the validation set. ⋆\nindicates that we merely use the Wikidata knowledge base. △means we scale the model horizon with the infusion\napproach. †and ‡indicate the use of the Wikipedia or Wikidata knowledge base.\nMethod BN ES PT SV ZH A VG.\nCoarse\nChatGPT 21.26 33.86 35.27 40.11 18.0129.70\nRaNER 95.92 96.17 96.79 97.55 91.9495.67\nU-RaNER 97.48 98.30 98.33 98.49 95.5597.63\n∆ +1.56+2.13+1.54+0.94+3.61+1.96\nFine\nChatGPT 9.76 16.28 18.84 20.94 6.34 14.43\nRaNER 93.11 90.90 90.75 91.93 82.8389.90\nU-RaNER 94.96 93.51 92.90 94.45 90.3893.24\n∆ +1.85+2.61+2.15+2.52+7.55+3.34\nTable 5: Comparison of the performance between Chat-\nGPT, RaNER and U-RaNER at coarse-and-fine grained\ncategories.\nuseful for system performance and illustrates the\ncomplementarity of the two knowledge bases.\nFor the different knowledge acquisition meth-\nods, the ENT2ENT approach is superior to the\nTEXT2ENT approach (90.47% vs. 86.22%). In\naddition, we use the infusion approach to further\nimprove the model performance (RaNER-MSF vs.\nTEXT2TEXT), which suggests that guaranteeing\nknowledge to be visible to model is also important.\nThe default infusion method in our experiments is\npost-infusion. We also analyze the impact of the\ntwo different infusion methods on performance in\nthe Appendix B.2.\n6.3 Coarse-and-fine Category Analysis\nTo illustrate the advantages of U-RaNER on fine-\ngrained NER, we transform the model predictions\nto the coarse-grained level according to the offi-\ncial topology of fine-grained categories. We use\nthe models of RaNER-MSF and U-RaNER w/\nBN\nENTITY2ENTITY\nTEXT2ENTITY\nTEXT2TEXT\nEN FA\nFR UK HI\nFigure 4: The distribution of the character-level IoU\nbetween query and its retrieval result. Each subplot is\nthe histograms of different retrieval strategies on the\ncorresponding dataset, where the x-axis indicates the\nIoU values ranging from 0 to 1.\nENT2ENT in Table 4 for the analysis. As shown\nin the Table 5, the improvements in coarse-grained\nmetrics are significantly lower than those of fine-\ngrained metrics, differing by 1.38% (3.91% on the\nZH track). It suggests that the proposed U-RaNER\nis better at coping with complex scenarios of fine-\ngrained classification. Besides, the average F1 for\nChatGPT at different granularity is significant dis-\ntinct (29.70% vs. 14.43%), which shows the diffi-\nculty in identifying fine-grained complex entities.\n6.4 Query Relevance\nWe define a relevance metric to compute the rele-\nvance between the query and retrieval result. The\nmetric calculates the Intersection-over-Union (IoU)\nbetween the characters 7 of the query and those\n7We take repeat characters as different characters.\n2021\nRAW 128 512 1024 2048\nContext Length\n72.5\n75.0\n77.5\n80.0\n82.5\n85.0\n87.5\n90.0\n92.5F1-mearsure\nEN\nES\nPT\nMULTI\nFigure 5: F1-measure with different length of context.\nRAW indicates that no external context is appended.\nof the retrieved result. We plot the results on the\ntraining set of 6 tracks in Figure 4. It can be ob-\nserved that the IoU values ofTEXT2TEXT strategy\nform a larger cluster than those of TEXT2ENT and\nENT2ENT, which indicates that TEXT2TEXT re-\ntrieval would focus more on the context instead of\nmerely the entities in the query text. Additionally,\nwe observe that the distributions ofENT2ENT have\nlarger medians than those of TEXT2ENT. This\nmight due to ENT2ENT would retrieve more rele-\nvant entities from the Wikidata than TEXT2ENT.\nBy employing diverse retrieval techniques, we can\nleverage data with distinct attributes to improve the\neffectiveness of the model.\n6.5 Context Length Analysis\nIn this section, we focus on analyzing the impact of\ndifferent context length on model performance. We\nconduct a series of experiments on EN, ES, PT and\nMULTI datasets with the context length ranging\nfrom 128 to 2048. We can observe from Figure 5\nthat the model performance increases as the con-\ntext length grows. However, when the context list\nlength exceeds 1024, the trend of performance im-\nprovement on all four datasets slows down. This\nindicates that the knowledge capacity in the con-\ntexts saturates as the length of the context increases.\nFor better performance, we need to find comple-\nmentary and highly relevant contextual pieces as\nadditional knowledge sources.\n6.6 Error Analysis\nWe divided the NER task into two stages: mention\ndetection to locate entity spans, and entity typing\nto classify the spans with pre-defined labels. To fur-\nther analyze the limitations of our proposed model,\nwe present the experimental results on 12 languages\nin Table 7 in Appendix. The experimental results\nreveal that the average F1 score for mention de-\ntection is 97.21, whereas the accuracy for entity\ntyping is 90.35. These results provide evidence\nthat the bottleneck in fine-grained NER is typing.\nMore detailed discussion, including the different\nretrieval methods and case study, is in the Appendix\nB.3 and B.4.\n7 Conclusion\nIn this paper, we propose a unified retrieval-\naugmented system (U-RaNER) for the Multi-\nCoNER II shared task, which wins 9 out of 13\ntracks in the shared task. We expose that the bot-\ntleneck of the previous top system is the lack of\nknowledge. Accordingly, we use both Wikipedia\nand Wikidata knowledge bases with three retrieval\napproaches so that more diverse knowledge can\nbe considered. Also, we explore the infusion ap-\nproach to make more context visible to the model\nso as to make the best use of the resources. And\nthe error analysis indicates that the entity typing\nsub-task is the bottleneck in the current system. In\nthe future, we plan to exploit the knowledge in the\nlarge language model such as ChatGPT or LLaMA\nby self-verification or fine-tuning some adapters, in\norder to achieve robust generalization performance.\nReferences\nAlan Akbik, Duncan Blythe, and Roland V ollgraf. 2018.\nContextual string embeddings for sequence label-\ning. In Proceedings of the 27th International Con-\nference on Computational Linguistics, pages 1638–\n1649, Santa Fe, New Mexico, USA. Association for\nComputational Linguistics.\nSandeep Ashwini and Jinho D. Choi. 2014. Targetable\nnamed entity recognition in social media. CoRR,\nabs/1408.0782.\nSebastian Borgeaud, Arthur Mensch, Jordan Hoff-\nmann, Trevor Cai, Eliza Rutherford, Katie Milli-\ncan, George Bm Van Den Driessche, Jean-Baptiste\nLespiau, Bogdan Damoc, Aidan Clark, et al. 2022.\nImproving language models by retrieving from tril-\nlions of tokens. In International conference on ma-\nchine learning, pages 2206–2240. PMLR.\nNicola De Cao, Gautier Izacard, Sebastian Riedel, and\nFabio Petroni. 2021. Autoregressive entity retrieval.\nIn 9th International Conference on Learning Repre-\nsentations, ICLR 2021, Virtual Event, Austria, May\n3-7, 2021. OpenReview.net.\nBeiduo Chen, Jun-Yu Ma, Jiajun Qi, Wu Guo, Zhen-\nHua Ling, and Quan Liu. 2022. USTC-NELSLIP\n2022\nat SemEval-2022 task 11: Gazetteer-adapted integra-\ntion network for multilingual complex named entity\nrecognition. In Proceedings of the 16th International\nWorkshop on Semantic Evaluation (SemEval-2022),\npages 1613–1622, Seattle, United States. Association\nfor Computational Linguistics.\nAlexis Conneau, Kartikay Khandelwal, Naman Goyal,\nVishrav Chaudhary, Guillaume Wenzek, Francisco\nGuzmán, Edouard Grave, Myle Ott, Luke Zettle-\nmoyer, and Veselin Stoyanov. 2020. Unsupervised\ncross-lingual representation learning at scale. In Pro-\nceedings of the 58th Annual Meeting of the Asso-\nciation for Computational Linguistics, pages 8440–\n8451, Online. Association for Computational Lin-\nguistics.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. In Proceedings of the 2019 Conference of\nthe North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies, Volume 1 (Long and Short Papers), pages\n4171–4186, Minneapolis, Minnesota. Association for\nComputational Linguistics.\nBesnik Fetahu, Zhiyu Chen, Sudipta Kar, Oleg\nRokhlenko, and Shervin Malmasi. 2023a. Multi-\nCoNER v2: a Large Multilingual dataset for Fine-\ngrained and Noisy Named Entity Recognition.\nBesnik Fetahu, Anjie Fang, Oleg Rokhlenko, and\nShervin Malmasi. 2021. Gazetteer Enhanced Named\nEntity Recognition for Code-Mixed Web Queries. In\nProceedings of the 44th International ACM SIGIR\nConference on Research and Development in Infor-\nmation Retrieval, pages 1677–1681.\nBesnik Fetahu, Anjie Fang, Oleg Rokhlenko, and\nShervin Malmasi. 2022. Dynamic gazetteer inte-\ngration in multilingual models for cross-lingual and\ncross-domain named entity recognition. In Proceed-\nings of the 2022 Conference of the North Ameri-\ncan Chapter of the Association for Computational\nLinguistics: Human Language Technologies, pages\n2777–2790, Seattle, United States. Association for\nComputational Linguistics.\nBesnik Fetahu, Sudipta Kar, Zhiyu Chen, Oleg\nRokhlenko, and Shervin Malmasi. 2023b. SemEval-\n2023 Task 2: Fine-grained Multilingual Named En-\ntity Recognition (MultiCoNER 2). In Proceedings of\nthe 17th International Workshop on Semantic Evalua-\ntion (SemEval-2023). Association for Computational\nLinguistics.\nWeichao Gan, Yuanping Lin, Guangbo Yu, Guimin\nChen, and Qian Ye. 2022. Qtrade AI at SemEval-\n2022 task 11: An unified framework for multilingual\nNER task. In Proceedings of the 16th International\nWorkshop on Semantic Evaluation (SemEval-2022),\npages 1654–1664, Seattle, United States. Association\nfor Computational Linguistics.\nRavindra Harige and Paul Buitelaar. 2016. Generating a\nlarge-scale entity linking dictionary from Wikipedia\nlink structure and article text. In Proceedings of\nthe Tenth International Conference on Language\nResources and Evaluation (LREC’16), pages 2431–\n2434, Portorož, Slovenia. European Language Re-\nsources Association (ELRA).\nUrvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke\nZettlemoyer, and Mike Lewis. 2019. Generalization\nthrough memorization: Nearest neighbor language\nmodels. arXiv preprint arXiv:1911.00172.\nViet Dac Lai, Nghia Trung Ngo, Amir Pouran Ben Vey-\nseh, Hieu Man, Franck Dernoncourt, Trung Bui, and\nThien Huu Nguyen. 2023. Chatgpt beyond english:\nTowards a comprehensive evaluation of large lan-\nguage models in multilingual learning.\nPatrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio\nPetroni, Vladimir Karpukhin, Naman Goyal, Hein-\nrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rock-\ntäschel, et al. 2020. Retrieval-augmented generation\nfor knowledge-intensive nlp tasks. Advances in Neu-\nral Information Processing Systems, 33:9459–9474.\nIlya Loshchilov and Frank Hutter. 2017. Fixing\nweight decay regularization in adam. CoRR,\nabs/1711.05101.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022a. MultiCoNER: a\nLarge-scale Multilingual dataset for Complex Named\nEntity Recognition.\nShervin Malmasi, Anjie Fang, Besnik Fetahu, Sudipta\nKar, and Oleg Rokhlenko. 2022b. SemEval-2022\nTask 11: Multilingual Complex Named Entity Recog-\nnition (MultiCoNER). In Proceedings of the 16th\nInternational Workshop on Semantic Evaluation\n(SemEval-2022). Association for Computational Lin-\nguistics.\nKen J. McDonell. 1977. An inverted index implementa-\ntion. Comput. J., 20(2):116–123.\nTao Meng, Anjie Fang, Oleg Rokhlenko, and Shervin\nMalmasi. 2021. GEMNET: Effective gated gazetteer\nrepresentations for recognizing complex entities in\nlow-context input. In Proceedings of the 2021 Con-\nference of the North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies, pages 1499–1512.\nLong Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-\nroll L. Wainwright, Pamela Mishkin, Chong Zhang,\nSandhini Agarwal, Katarina Slama, Alex Ray, John\nSchulman, Jacob Hilton, Fraser Kelton, Luke Miller,\nMaddie Simens, Amanda Askell, Peter Welinder,\nPaul Christiano, Jan Leike, and Ryan Lowe. 2022.\nTraining language models to follow instructions with\nhuman feedback.\nSameer Pradhan, Alessandro Moschitti, Nianwen Xue,\nHwee Tou Ng, Anders Björkelund, Olga Uryupina,\n2023\nYuchen Zhang, and Zhi Zhong. 2013. Towards ro-\nbust linguistic analysis using OntoNotes. In Proceed-\nings of the Seventeenth Conference on Computational\nNatural Language Learning, pages 143–152, Sofia,\nBulgaria. Association for Computational Linguistics.\nStephen E. Robertson and Hugo Zaragoza. 2009. The\nprobabilistic relevance framework: BM25 and be-\nyond. Found. Trends Inf. Retr., 3(4):333–389.\nErik F. Tjong Kim Sang. 2002. Introduction to the conll-\n2002 shared task: Language-independent named en-\ntity recognition. In Proceedings of the 6th Confer-\nence on Natural Language Learning, CoNLL 2002,\nHeld in cooperation with COLING 2002, Taipei, Tai-\nwan, 2002. ACL.\nTianze Shi and Lillian Lee. 2021. TGIF: Tree-graph\nintegrated-format parser for enhanced UD with two-\nstage generic- to individual-language finetuning. In\nProceedings of the 17th International Conference\non Parsing Technologies and the IWPT 2021 Shared\nTask on Parsing into Enhanced Universal Dependen-\ncies (IWPT 2021), pages 213–224, Online. Associa-\ntion for Computational Linguistics.\nNitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,\nIlya Sutskever, and Ruslan Salakhutdinov. 2014.\nDropout: A simple way to prevent neural networks\nfrom overfitting. Journal of Machine Learning Re-\nsearch, 15(56):1929–1958.\nBeth M. Sundheim. 1995. Named entity task definition,\nversion 2.1. In Proceedings of the Sixth Message\nUnderstanding Conference, pages 319–332.\nErik F. Tjong Kim Sang and Fien De Meulder.\n2003. Introduction to the CoNLL-2003 shared task:\nLanguage-independent named entity recognition. In\nProceedings of the Seventh Conference on Natural\nLanguage Learning at HLT-NAACL 2003, pages 142–\n147.\nXinyu Wang, Jiong Cai, Yong Jiang, Pengjun Xie,\nKewei Tu, and Wei Lu. 2022a. Named entity and re-\nlation extraction with multi-modal retrieval. In Find-\nings of the Association for Computational Linguistics:\nEMNLP 2022, pages 5925–5936, Abu Dhabi, United\nArab Emirates. Association for Computational Lin-\nguistics.\nXinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang,\nZhongqiang Huang, Fei Huang, and Kewei Tu. 2021a.\nAutomated Concatenation of Embeddings for Struc-\ntured Prediction. In the Joint Conference of the\n59th Annual Meeting of the Association for Compu-\ntational Linguistics and the 11th International Joint\nConference on Natural Language Processing (ACL-\nIJCNLP 2021). Association for Computational Lin-\nguistics.\nXinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang,\nZhongqiang Huang, Fei Huang, and Kewei Tu. 2021b.\nImproving named entity recognition by external con-\ntext retrieving and cooperative learning. In Proceed-\nings of the 59th Annual Meeting of the Association for\nComputational Linguistics and the 11th International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers), pages 1800–1812, Online.\nAssociation for Computational Linguistics.\nXinyu Wang, Yong Jiang, Nguyen Bach, Tao Wang,\nHuang Zhongqiang, Fei Huang, and Kewei Tu. 2020.\nMore embeddings, better sequence labelers? In Find-\nings of EMNLP, Online.\nXinyu Wang, Yongliang Shen, Jiong Cai, Tao Wang, Xi-\naobin Wang, Pengjun Xie, Fei Huang, Weiming Lu,\nYueting Zhuang, Kewei Tu, Wei Lu, and Yong Jiang.\n2022b. DAMO-NLP at SemEval-2022 task 11: A\nknowledge-based system for multilingual named en-\ntity recognition. In Proceedings of the 16th Interna-\ntional Workshop on Semantic Evaluation (SemEval-\n2022), pages 1457–1468, Seattle, United States. As-\nsociation for Computational Linguistics.\nIkuya Yamada, Akari Asai, Hiroyuki Shindo, Hideaki\nTakeda, and Yuji Matsumoto. 2020. LUKE: Deep\ncontextualized entity representations with entity-\naware self-attention. In Proceedings of the 2020\nConference on Empirical Methods in Natural Lan-\nguage Processing (EMNLP), pages 6442–6454, On-\nline. Association for Computational Linguistics.\nWenzheng Zhang, Wenyue Hua, and Karl Stratos. 2022a.\nEntqa: Entity linking as question answering. In The\nTenth International Conference on Learning Repre-\nsentations, ICLR 2022, Virtual Event, April 25-29,\n2022. OpenReview.net.\nXin Zhang, Yong Jiang, Xiaobin Wang, Xuming Hu,\nYueheng Sun, Pengjun Xie, and Meishan Zhang.\n2022b. Domain-specific NER via retrieving cor-\nrelated samples. In Proceedings of the 29th Inter-\nnational Conference on Computational Linguistics,\npages 2398–2404, Gyeongju, Republic of Korea. In-\nternational Committee on Computational Linguistics.\nA Detailed Experimental Setup\nA.1 MultiCoNER II Corpus\nThe multilingual NER II corpus (MultiCoNER II8)\naims to recognize the complex named entities, like\nthe titles of creative works which are not simple\nnouns, and pose challenges for current NER sys-\ntems. With the same set of tags, the 12 multilin-\ngual datasets specifically include: BN-Bangla, DE-\nGerman, EN-English, ES-Spanish, FA-Farsi, FR-\nFrench, HI-Hindi, IT-Italian, PT-Portuguese, SV-\nSwedish, UK-Ukrainian and ZH-Chinese. Table 6\nshows the detailed dataset statistics.\nA.2 System Setup\nFor fair comparison with prior systems, we use\nxlm-roberta-large (Conneau et al., 2020) as our ini-\ntial checkpoint. We use the AdamW (Loshchilov\n8https://multiconer.github.io/dataset\n2024\nLanguage Training Validataion Test\nBN-Bangla 9,708 507 19,859\nDE-German 9,785 512 20,145\nEN-English 16,778 871 249,980\nES-Spanish 16,453 854 246,900\nFA-Farsi 16,321 855 219,168\nFR-French 16,548 857 249,786\nHI-Hindi 9,632 514 18,399\nIT-Italian 16,579 858 247,881\nPT-Portuguese 16,469 854 229,490\nSV-Swedish 16,363 856 231,190\nUK-Ukrainian 16,429 851 238,296\nZH-Chinese 9,759 506 20,265\nMUL-Multilingual 170,824 8,895 358,668\nTable 6: Dataset statistics on MultiCoNER II.\nand Hutter, 2017) optimizer with a linear warmup-\ndecay learning schedule and a dropout (Srivastava\net al., 2014) of 0.1. We set the batch size and learn-\ning rate to 16 and 2e-5, and train models over 4\nrandom seeds. According to the dataset sizes, we\ntrain the models for 5 epochs and 20 epochs for\nmultilingual and monolingual models respectively.\nAnd all our experiments are conducted on a sin-\ngle NVIDIA A100 80GB GPU. For the ensemble\nmodule, we train about 4 models for each track.\nA.3 Fine-grained Taxonomy\nThe tagset of MultiCoNER II is a fine-grained\ntagset including 6 coarse-grained categories and\n33 fine-grained categories. The coarse-to-fine map-\nping of the tags are as follows:\n• Location (LOC): Facility, OtherLOC, Hu-\nmanSettlement, Station;\n• Creative Work (CW): VisualWork, Musical-\nWork, WrittenWork, ArtWork, Software;\n• Group (GRP): MusicalGRP, PublicCORP, Pri-\nvateCORP, AerospaceManufacturer, Sports-\nGRP, CarManufacturer, ORG;\n• Person (PER): Scientist, Artist, Athlete, Politi-\ncian, Cleric, SportsManager, OtherPER;\n• Product (PROD): Clothing, Vehicle, Food,\nDrink, OtherPROD;\n• Medical (MED): Medication/Vaccine, Med-\nicalProcedure, AnatomicalStructure, Symp-\ntom, Disease.\nThe Figure 6 shows the fine-grained taxonomy.\nFigure 6: The taxonomy of fine-grained categories on\nMultiCoNER II from the official webpage.\nLanguage F1-entity F1-mention Acc-typing\nBN 92.30 97.33 94.83\nDE 84.29 95.00 88.73\nEN 84.32 98.15 85.91\nES 88.81 98.13 90.50\nFA 87.85 97.21 90.37\nFR 86.77 97.34 89.14\nHI 91.75 97.15 94.44\nIT 91.08 98.53 92.44\nPT 88.45 98.45 89.84\nSV 89.74 98.60 91.01\nUK 88.46 98.33 89.96\nZH 81.55 92.25 87.00\nA VG. 87.84 97.21 90.35\nTable 7: Model performance of mention-detection and\nentity-typing on the 12 multilingual datasets.\nA.4 Detailed Procedure for TEXT2ENT\nFor sparse retrieval, we find the relevant entities\nfrom Wikidata which contains millions of entities.\nAs in the TEXT2TEXT strategy, we utilize the de-\nscription and alias information in the Wikidata and\nindex them with ElasticSearch. We use each sen-\ntence in the dataset as the query and retrieve the\ncandidate entity with the BM25 algorithm. In or-\nder to find candidate entities as much as possible,\nwe apply an iterative retrieval procedure in which\nwe construct a new query by masking the retrieved\nentities in the query text from the previous retrieval.\nFor dense retrieval, we utilize the title informa-\ntion and paragraph information 9 from Wikipedia\nto construct the knowledge base for dense entity\n9Considering the memory limit of dense retrieval model\ntraining, we truncate the paragraph information in wikipedia,\nand reserve the first 128 tokens for the construction of the\nknowledge base.\n2025\nMethod BN DE EN ES FA FR HI IT PT SV UK ZH A VG.\nRaNER w/ one stage 91.79 82.41 84.32 87.49 85.69 85.48 90.68 89.51 87.46 88.54 87.82 76.4586.47\nRaNER w/ bs 4 82.02 80.82 85.60 88.46 85.27 87.53 86.56 89.80 87.26 89.77 89.17 68.59 85.07\nRaNER w/ bs 128 88.09 83.23 85.87 89.40 85.59 88.18 89.57 91.84 88.97 90.01 88.97 72.11 86.82\nRaNER w/ scale up 90.82 86.27 85.86 89.88 86.15 88.70 90.99 91.50 89.24 90.85 88.95 75.71 87.91\nTable 8: The model performance with different training strategies.\nMethod BN DE EN ES FA FR HI IT PT SV UK ZH A VG.\nRaNER 89.81 80.55 79.98 82.99 81.17 81.73 90.57 87.48 83.61 84.43 83.69 77.30 83.61\nU-RaNER w/ Pre-infusion 91.35 82.80 83.71 86.73 86.63 85.88 91.07 89.08 87.18 89.16 88.69 80.4186.89\nU-RaNER w/ Post-infusion 91.82 83.24 84.50 86.85 87.64 87.21 91.23 90.36 87.98 90.47 90.02 81.1587.71\nTable 9: The model performance with different infusion approaches.\nretrieval, then use the input sentence as the query\nto retrieve its related Top-K entities in the knowl-\nedge base. The dense retrieval model we use is\nthe widely used Bi-Encoder architecture (Zhang\net al., 2022a). Different from sparse retrieval, the\ndense retrieval model is trainable to better perceive\nthe semantic characteristics of the MultiCoNER\ndataset. Therefore, in practice, we first preprocess\nthe train/dev sets of MultiCoNER into the data for-\nmat for dense retrieval model training. Specifically,\nbecause the train/dev sets provide the golden entity\nannotation of the sentence, we can fuzzy match\nthe span in the sentence with the entity title in our\nknowledge base to link each span to a specific en-\ntity id. Then we use reconstructed training data to\ntrain a dense entity retrieval model with reliable\nperformance, which will be finally applied to the\ntest set to obtain candidate entities for the sentences\nin the test set.\nA.5 Detailed Procedure for ENT2ENT\nSuppose that we have already retrieved the bound-\naries of possible or relative entities of a sentence,\nwe want to encode more knowledge about these\nentities to benefit the prediction of target entities\nand their types. A good choice is leveraging Wiki-\ndata which integrates billions of structural infor-\nmation between millions of entities, such as the\nalias of entities and the relationships of entity pairs.\nTherefore, we adopt the following steps to acquire\nENT2ENT knowledge to augment the data so as to\nenhance the entity recognition ability of our model.\n1. We preprocess Wikidata to construct two dic-\ntionaries of each language in this task. One\ntakes each entity name and each alias string\nof each entity in Wikidata as keys and the in-\ndex (called “Qid\") of each entity as values.\nThe other takes Qid of each entity as keys\nand two attributes (called “subclass of\" and\n“sub-instance of\") content of each entity as val-\nues. It is worth mentioning that the values of\nthe two attributes associated with each entity\nin Wikidata are themselves entities. There-\nfore, this method is referred to as ENT2ENT\nretrieval. For the following description, we\ncall the first dictionary String-to-Qid and the\nsecond dictionary Qid-to-Types.\n2. For each language, we retrieve argumentation\ndata according to pre-retrieved entities and\nthe knowledge dictionaries from Step1. Con-\ncretely, for each retrieved entity, we first ex-\ntract the corresponding Qid if it can match one\nkey from the String-to-Qid dictionary. Next,\nif the first operation succeeds, we leverage\nthe Qid to query the Qid-to-Types dictionary\nto get the values of “subclass of\" and “sub-\ninstance of\" as types of the retrieved entity. It\nis possible that the values of some Qid in the\nQid-to-Types dictionary of a specific language\nare NULL. In this situation, we try to get en-\ntity types from the Qid-to-Types dictionary of\nEnglish except for processing English itself.\n3. If we get the language-specific types or En-\nglish types of some pre-retrieved entities\nfrom Step2, we sequentially splice these pre-\nretrieved entities and their retrieved types after\nthe original sentence. For those pre-retrieved\nentities without retrieved types, we only splice\nthe pre-retrieved entities.\nA.6 Detailed Procedure for Prompting\nFollowing (Lai et al., 2023), our Multi-turn\nprompt structure for ChatGPT consists of a task\n2026\nSentence Span Gold Tag BERT-CRF RaNER U-RaNER\npudendal nerve entrapment can\noccur when the ... pudendal nerve entrapment Disease - Symptom Disease\nhe debuted for gloucestershire\nin 1887 at the age of ... gloucestershire SportsGRP SportsGRP HS SportsGRP\nthe main event featured\nthales leites taking on jesse taylor\nthales leites\njesse taylor\nOtherPER\nOtherPER\nAthlete\nOtherPER\nAthlete\nAthlete\nAthlete\nAthlete\nTable 10: Examples of three NER systems. The entity type HS refers to HumanSettlement.\ndescription, a note for output format, and an in-\nput sentence. Since the experiments in Lai et al.\n(2023) indicate that English prompts work better\nthan multilingual ones, we use English prompts\nfor all languages. As shown in Figure 7, the task\ndescription part is used to explain the task and list\nthe entity categories, the note part indicates the an-\nnotation scheme and output format, and finally we\nadd the input text. In our experiment, {...}is filled\nby the content in the Appendix A.3.\nMulti-turn first performs the task in 6\ncoarse-grained categories, and later performs finer-\ngrained NER. In our experiment, {...}is filled by\nthe response of ChatGPT and the content from in\nthe Appendix A.3.\nMulti-ICL constructs demonstrations spliced\nafter the note part by randomly selecting examples\nfrom the training set. xxx is replaced with the\nselected example. The corresponding prompts can\nbe found in Figure 8.\nB More Analysis\nB.1 Multi-stage Fine-tuning\nWe observe that inconsistent training set sizes on\ndifferent language tracks will lead to degradation\nof model performance from 86.47% to 85.07%. We\nuse increasing batch size and scaling up strategy\nto address this issue. From the Table 8, increasing\nbatch size from 4 to 128 can improve the model\nperformance from 85.07% to 86.82%. Furthermore,\nscaling up the training data size on BN, DE, HI and\nZH can also result in a gain of +1.09%\nB.2 Two Infusion Approaches\nIn the section § 4.2, we propose two infusion meth-\nods (Pre-Infusion and Post-Infusion) to make more\ncontext visible to the model. Here, we make a\nquantitative comparison of their effects on model\nperformance. As shown in the Table 9, we ob-\nserve that the post-infusion method is superior\nto the pre-infusion method in all language track.\nWe attribute this to the fact that the pre-infusion\nTask Description: You are working as a named\nentity recognition expert and your task is to la-\nbel a given text with named entity labels. Your\ntask is to identify and label any named entities\npresent in the text. The named entity labels that\nyou will be using are 33 categories, as shown\nbelow {...}.\nNote: Please use BIO annotation schema to\ncomplete this task. Please make sure to label\neach word of the entity with the appropriate pre-\nfix (“B” for the first word of the entity, “I” for\nany non-initial word of the entity). For words\nwhich are not part of any named entity, you\nshould return “O”. Your output format should\nbe a list of tuples, where each tuple consists of a\nword from the input text and its corresponding\nnamed entity label.\nInput: [“from”, “1995”, “to”, “2011”, “deal”,\n“hudson”, “was”, “the”, “magazine’s”, “pub-\nlisher”, “.”]\nOutput:\nFigure 7: Input prompt for Single-turn.\nmethod only considers the anchor information and\nignores other contextual information, while the\npost-infusion method uses more contextual knowl-\nedge and achieves better performance.\nB.3 Different Retrieval Methods\nTo deeply analyze the effectiveness of the two\nTEXT2ENT retrieval strategies we design, we com-\npare their retrieval performance (i.e., Recall@50)\nand the enhanced NER performance (i.e., F1) based\non their respective retrieval results. From Table 11,\nwe find that the retrieval performance of sparse re-\ntrieval does not seem to be worse than dense, and\nits recall is higher than dense retrieval for both PT\nand SV languages. In addition, for the BN and DE\nlanguages, although their recall results of sparse\nretrieval are lower than those of dense retrieval,\ntheir final performance of NER is higher than that\n2027\nTask Description:You are working as a named\nentity recognition expert and your task is to label a\ngiven text with named entity labels. Your task is to\nidentify and label any named entities present in the\ntext. The named entity labels that you will be using\nare PER (person), LOC (location), CW (creative\nwork), GRP (group of people), PROD (product),\nand MED (medical).\nNote:Please use BIO annotation schema to com-\nplete this task. Please make sure to label each word\nof the entity with the appropriate prefix (“B” for\nthe first word of the entity, “I” for any non-initial\nword of the entity). For words which are not part\nof any named entity, you should return “O”.\nDemonstrations:Optional. [Input:xxx, Output:\nxxx].\nInput: [“from”, “1995”, “to”, “2011”, “deal”,\n“hudson”, “was”, “the”, “magazine’s”, “publisher”,\n“.”]\nOutput:{...}.\nInput:Please complete the above task at a finer\ngranularity based on the fine-grained taxonomy be-\nlow {...}.\nOutput:\nFigure 8: Input prompt for Multi-turn and Multi-ICL.\nof dense retrieval. We think this is mainly due to\nthe different retrieval sources of the two retrieval\nstrategies. Our sparse strategy is retrieved from\nWikidata, while the dense strategy is retrieved from\nWikipedia. The retrieval quality of Wikipedia is\neasily disturbed by the existence of entity alias. In\naddition, because the dense retrieval requires us to\ntrain the model, we actually truncate the paragraph\ninformation in Wikipedia for model training and\nretrieval, so the information that can be used for\ndense retrieval is also limited. However, from the\nZH language, we know that the robustness of the\ndense retrieval strategy for different languages is\nbetter than the sparse retrieval strategy. Therefore,\nwhen dealing with retrieval in different languages,\nwe can flexibly choose different strategies based\non the quality of the retrieval resources in the cor-\nresponding language to obtain better performance.\nB.4 Case Study\nTable 10 provides a closer examination of the\npredicted results of BERT-CRF, RaNER, and U-\nRaNER respectively. We selected three cases from\nthe English language dev data to analyze in detail.\nIn the first case, fine-grained NER necessitates\ncomprehensive information to accurately classify\nlong-tail entity spans. By utilizing knowledge from\nMetric BN DE PT SV ZH A VG.\nRecall-Sparse 79.32 71.09 98.22 98.20 37.7676.92\nRecall-Dense 93.26 85.18 87.84 89.19 79.8085.25\nF1-Baseline 86.98 85.46 76.54 78.48 75.1180.51\nF1-Sparse 89.81 90.57 83.61 84.43 77.30 85.14\nF1-Dense 88.45 89.83 77.23 80.54 78.00 82.81\nTable 11: Comparison of retrieval performance and im-\npact on NER between the sparse and dense TEXT2ENT\nstrategies on the dev set.\nmultiple sources, U-RaNER successfully predicts\n\"pudendal nerve entrapment\" in the first case.\nIn the second case, RaNER’s typical ambigu-\nity problem is evident, where the context retrieved\nfrom merely Wikipedia source lacks pertinent in-\nformation about the target entity \"gloucestershire\"\nwhich could refer to either a county or a sports\nclub.\nHowever, in the third case, the retrieval-based\nsystems wrongly predict \"theles leites\" and \"jesse\ntaylor\" as \"Athlete\" due to retrieved knowledge\nindicating that they are both mixed martial arts\nfighters. This demonstrates that the use of retrieved\ninformation can sometimes be misleading and even\nharmful.\n2028"
}