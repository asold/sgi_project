{
  "title": "Do Massively Pretrained Language Models Make Better Storytellers?",
  "url": "https://openalex.org/W2975003218",
  "year": 2019,
  "authors": [
    {
      "id": "https://openalex.org/A2778789062",
      "name": "Abigail See",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2718987615",
      "name": "Aneesh Pappu",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2811107670",
      "name": "Rohun Saxena",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2977065444",
      "name": "Akhila Yerukola",
      "affiliations": [
        "Stanford University"
      ]
    },
    {
      "id": "https://openalex.org/A2149153931",
      "name": "Christopher D. Manning",
      "affiliations": [
        "Stanford University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2962739339",
    "https://openalex.org/W2962883855",
    "https://openalex.org/W2996287690",
    "https://openalex.org/W2890276793",
    "https://openalex.org/W2962814079",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W2996068536",
    "https://openalex.org/W2970102799",
    "https://openalex.org/W2888779557",
    "https://openalex.org/W2152198378",
    "https://openalex.org/W2951080837",
    "https://openalex.org/W1974991592",
    "https://openalex.org/W2140676672",
    "https://openalex.org/W2963672599",
    "https://openalex.org/W2222235228",
    "https://openalex.org/W2752172973",
    "https://openalex.org/W2964265128",
    "https://openalex.org/W1507711477",
    "https://openalex.org/W2963096510",
    "https://openalex.org/W2963903950",
    "https://openalex.org/W2798888952",
    "https://openalex.org/W2963963856",
    "https://openalex.org/W2898658996",
    "https://openalex.org/W2071516704",
    "https://openalex.org/W2250539671",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W2530647954",
    "https://openalex.org/W2916772188",
    "https://openalex.org/W2963206148",
    "https://openalex.org/W2963466651",
    "https://openalex.org/W2001833328"
  ],
  "abstract": "Large neural language models trained on massive amounts of text have emerged as a formidable strategy for Natural Language Understanding tasks. However, the strength of these models as Natural Language Generators is less clear. Though anecdotal evidence suggests that these models generate better quality text, there has been no detailed study characterizing their generation abilities. In this work, we compare the performance of an extensively pretrained model, OpenAI GPT2-117 (Radford et al., 2019), to a state-of-the-art neural story generation model (Fan et al., 2018). By evaluating the generated text across a wide variety of automatic metrics, we characterize the ways in which pretrained models do, and do not, make better storytellers. We find that although GPT2-117 conditions more strongly on context, is more sensitive to ordering of events, and uses more unusual words, it is just as likely to produce repetitive and under-diverse text when using likelihood-maximizing decoding algorithms.",
  "full_text": "Proceedings of the 23rd Conference on Computational Natural Language Learning, pages 843–861\nHong Kong, China, November 3-4, 2019.c⃝2019 Association for Computational Linguistics\n843\nDo Massively Pretrained Language Models Make Better Storytellers?\nAbigail See, Aneesh Pappu∗, Rohun Saxena∗, Akhila Yerukola∗, Christopher D. Manning\nStanford University\n{abisee,apappu,rohun,akhilay,manning}@cs.stanford.edu\nAbstract\nLarge neural language models trained on mas-\nsive amounts of text have emerged as a\nformidable strategy for Natural Language Un-\nderstanding tasks. However, the strength of\nthese models as Natural Language Generators\nis less clear. Though anecdotal evidence sug-\ngests that these models generate better quality\ntext, there has been no detailed study charac-\nterizing their generation abilities. In this work,\nwe compare the performance of an extensively\npretrained model, OpenAI GPT2-117 (Rad-\nford et al., 2019), to a state-of-the-art neural\nstory generation model (Fan et al., 2018). By\nevaluating the generated text across a wide va-\nriety of automatic metrics, we characterize the\nways in which pretrained models do, and do\nnot, make better storytellers. We ﬁnd that al-\nthough GPT2-117 conditions more strongly on\ncontext, is more sensitive to ordering of events,\nand uses more unusual words, it is just as\nlikely to produce repetitive and under-diverse\ntext when using likelihood-maximizing decod-\ning algorithms.\n1 Introduction\nIn 2018, large-scale neural models such as ELMo\n(Peters et al., 2018), BERT (Devlin et al., 2019)\nand OpenAI GPT (Radford et al., 2018) emerged\nas a dominant approach in NLP. By pretraining\non massive amounts of unlabeled text (often or-\nders of magnitude larger than the the target task’s\nlabeled dataset), these models achieve state-of-\nthe-art performance across a variety of Natural\nLanguage Understanding benchmarks. In partic-\nular, the OpenAI GPT2 language model (Rad-\nford et al., 2019) achieves state-of-the-art perfor-\nmance on several language modeling benchmarks,\neven in a zero-shot setting. While GPT2’s perfor-\nmance as a language model is undeniable, its per-\nformance as a text generator is much less clear.\n∗equal contribution\nThough the model has generated certain impres-\nsive samples of text – such as a widely-circulated\npassage about Ovid’s Unicorn (Radford et al.,\n2019) – there has been no detailed study to for-\nmalize these observations.\nIn this work, we perform an in-depth study\nof the properties of text generated by GPT2-117\n(the smallest version of GPT2) in the context of\nstory generation. By comparing to a state-of-the-\nart, specialized-architecture neural story genera-\ntion model (Fan et al., 2018), we ask the follow-\ning questions. In what ways does a large amount\nof open-domain pretraining data change the char-\nacteristics of generated text? In what ways does it\nmake no difference? And is a task-speciﬁc archi-\ntecture necessary?\nFor any probabilistic language model, the gen-\nerated text is strongly affected by the choice of de-\ncoding algorithm – this is especially true foropen-\nended text generation tasks such as storytelling\nand chitchat dialogue (Kulikov et al., 2018; Holtz-\nman et al., 2019). Nevertheless, most natural lan-\nguage generation papers evaluate only one decod-\ning algorithm – this is often due to the time and\nexpense required for human evaluation. For ex-\nample, Fan et al. use top- k sampling (a decoding\nalgorithm in which k governs the quality-diversity\ntradeoff), but only evaluate one value of k. How-\never, evaluating onek gives an incomplete view of\nthe generation system – several researchers have\nemphasized the importance of evaluating genera-\ntion systems over the entire quality-diversity spec-\ntrum, rather than a single point on it (Caccia et al.,\n2018; Hashimoto et al., 2019).\nIn this study, we prioritize evaluating text across\nthe whole k spectrum, and measuring many dif-\nferent automatic metrics, rather than a few hu-\nman metrics. Though the lack of human evalu-\nation limits our ability to measure overall quality\n(Liu et al., 2016; Novikova et al., 2017; Hashimoto\n844\net al., 2019), we are able to produce an objectively\ndeﬁned, richly detailed and reproducible evalua-\ntion of the generated text. To our knowledge, this\nwork is the ﬁrst comprehensive analysis of the\ncharacteristics of GPT2-generated text. Our study\nprovides insight into the effect of large-scale pre-\ntraining on open-ended natural language genera-\ntion, as well as the effect of k on text generated\nwith top-k sampling. We hope our results will in-\nform other researchers’ choice of models, pretrain-\ning schemes, and decoding algorithms – decisions\nthat can often feel like blind choices. To enable\nreaders to browse the generated text, conduct their\nown evaluations, or run our evaluations on their\nown text, we publicly release our generated stories\nand evaluation code.1\n2 Background\nWritingPrompts dataset WritingPrompts (Fan\net al., 2018) is a story generation dataset contain-\ning 303,358 human-written ( prompt, story) pairs\ncollected from the /r/WritingPrompts subreddit –\na forum where Reddit users compose short stories\ninspired by other users’ prompts. An example can\nbe seen at the top of Table 2. The mean prompt\nlength is 28.4 words and the mean story length is\n734.5 words. The dataset is 887MB of text in total,\ncontains 200 million story words, and is divided\ninto 90% train, 5% validation and 5% test splits.\nThe Fusion Model The Fusion Model is a\nstate-of-the-art neural story generation architec-\nture trained on the WritingPrompts dataset (Fan\net al., 2018). It is based on the Convolutional\nSeq2seq model of Gehring et al. (2017) and aims\nto improve two aspects of story generation: mod-\neling long-range context and increasing relevance\nof the story to the prompt. To achieve the former,\nthe model uses a multi-scale gated self-attention\nmechanism. For the latter, the model uses a fu-\nsion mechanism (Sriram et al., 2018) in which one\nseq2seq model is trained on the task, then frozen,\nand a second seq2seq model is trained on the\ntask with access to the ﬁrst model’s hidden states.\nCompared to the Convolutional Seq2seq model\nand other baselines, the Fusion Model achieves\nimproved perplexity, story-prompt relevance and\nhuman preference scores. The Fusion Model has\na vocabulary of 104,960 words, a 3-layer encoder\nand 8-layer decoder in the ﬁrst seq2seq model, and\n1Code and generated stories available at https://\ngithub.com/abisee/story-generation-eval\na 5-layer encoder and 5-layer decoder in the sec-\nond model – in total, 255.4 million parameters.\nGPT2-117 GPT2 (Radford et al., 2019) is a\nlarge Transformer language model trained on\nWebText, a diverse corpus of internet text (not\npublicly released) containing over 8 million doc-\numents equalling 40GB of text in total. The full-\nsize GPT2 model, which has 1542 million pa-\nrameters, obtains state-of-the-art results on a va-\nriety of language modeling and other Natural Lan-\nguage Understanding benchmarks. At the time\nof our experiments, Radford et al. had only re-\nleased the smallest of the models, known as GPT2-\n117.2 This model, which we use for our experi-\nments, has 12 layers and 117 million parameters.\nLike the full-size GPT2 model, it has a vocabu-\nlary of 50,257 byte-pair-encoding (BPE) tokens.\nThe BPE encoding allows the model to encode\nand generate any Unicode string, regardless of pre-\nprocessing, tokenization, or vocabulary size. The\nmodel has a context size of 1024, meaning it can\nprocess text up to 1024 BPE tokens in length.\nDecoding algorithms Inspired by Neural Ma-\nchine Translation, most early attempts at open-\nended neural text generation (such as conversa-\ntional response generation) used the beam search\ndecoding algorithm (Shang et al., 2015; Serban\net al., 2016). Like greedy decoding, beam search\nis a likelihood-maximizing decoding algorithm –\ngiven the input sequence x, the objective is to ﬁnd\nan output sequence y which maximizes P(y|x).\nHowever, researchers have shown that for open-\nended generation tasks (including storytelling),\nbeam search produces repetitive, generic and de-\ngenerate text (Holtzman et al., 2019).\nMore recently, top-k sampling has emerged as\na primary decoding algorithm for open-ended text\ngeneration (Fan et al., 2018; Radford et al., 2019).\nIn top- k sampling, on each step of the decoder\nthe probability distribution over the vocabulary is\ntruncated to the top k tokens, then re-normalized.\nThe next token is sampled from the new distribu-\ntion. Top- k sampling can be regarded as some-\nwhere between a likelihood maximizing algorithm\n(when k = 1; greedy decoding) and an unbiased\nsampling algorithm (when k = vocabulary size).\nFan et al. use top- k sampling (with k = 10) to\n2Since conducting our experiments, larger models have\nbeen publicly released. At the time of writing, the full-size\nGPT2 model has not been publicly released.\n845\ngenerate stories, and Radford et al. show impres-\nsive samples of generated text (primarily from the\nfull-size GPT2 model) for k = 40.\n3 Experimental Details\nPreprocessing Fan et al. truncate Writing-\nPrompts stories to 1000 words before training and\ntesting. Due to the limited context size of GPT2-\n117, we additionally exclude ( prompt, story) ex-\namples that are longer than 1024 BPE tokens when\nconcatenated. The resulting dataset, which we\ncall WritingPrompts-1024, has 192,364 training,\n11,115 validation, and 10,686 test examples.\nThe Fusion Model We use the pretrained ver-\nsion of the Fusion Model, which is available in\nthe Fairseq framework (Ott et al., 2019). For com-\nparability with GPT2-117, we evaluate the Fusion\nModel on WritingPrompts-1024 (see Table 1), ob-\ntaining perplexities similar to those reported by\nFan et al. on the full WritingPrompts dataset.\nGPT2-117 In order for the model to condition\non prompts and generate stylistically correct sto-\nries, we ﬁnetune GPT2-117 on WritingPrompts-\n1024.3 We frame WritingPrompts as a language\nmodeling task, representing the prompt and story\nas a single sequence separated by a delimiter to-\nken. We ﬁnetune the pretrained model until con-\nvergence using the default hyperparameters pro-\nvided in the HuggingFace repository (though we\nreduce batch size to ﬁt on a single GPU), and use\nthe ﬁnetuned model for all further evaluations.\nWe compute the word-level perplexity of the\nﬁnetuned GPT2-117 on the WritingPrompts-1024\ndataset. That is, we normalize the total negative\nlog probability of the target text by the number\nof word-level (i.e. Fusion Model) tokens, not the\nnumber of BPE tokens. This enables us to com-\npare the perplexities of the two models, despite\nthe tokenization difference (Radford et al., 2019).\nThe ﬁnetuned GPT2-117 obtains a test set word-\nperplexity of 31.54 4 – six points lower than the\nFusion Model.\nGeneration settings For both models, we gen-\nerate stories using top-k sampling, obtaining 1000\nstories (from 1000 different test set prompts) for\n3We use the PyTorch re-implementation of GPT2-117\navailable at https://github.com/huggingface/\npytorch-transformers\n4This is similar to other GPT2-117 WritingPrompts ﬁne-\ntuning experiments (Mao et al., 2019; Ziegler et al., 2019).\nModel Valid ppl Test ppl\nFusion Model 37.05 37.54\nGPT2-117 31.13 31.54\nTable 1: Word-level perplexities on WritingPrompts-\n1024 for the Fusion Model and ﬁnetuned GPT2-117.\nseveral values of k ranging from 1 to vocabulary\nsize. We use softmax temperature 1. Like Fan\net al., we generate exactly 150-word stories and\nblock the Fusion Model from generating <UNK>.\nTo obtain human-written stories for compari-\nson, we truncate WritingPrompts-1024 test set sto-\nries to 150 words (discarding those shorter than\n150 words). To reduce variance, measurements\nfor human stories are computed over this entire set\n(rather than just 1000 stories).\n4 Story-prompt relatedness\nPrior research has observed that seq2seq systems\nfrequently produce text that is unrelated to the\nprovided context – particularly under likelihood-\nmaximizing decoding algorithms such as beam\nsearch. The issue has inspired multiple explana-\ntions (Jiang and de Rijke, 2018) and multiple so-\nlutions – such as alternative training objectives (Li\net al., 2016), decoding objectives (Baheti et al.,\n2018; See et al., 2019), and architectural changes\n(Fan et al., 2018). In this section, we measure how\nstrongly the models condition on the prompt.\nPrompt ranking accuracy For both models, we\ncompute prompt ranking accuracy (Fan et al.,\n2018), which measures the language model’s sen-\nsitivity to the provided prompt. Following the\nmethodology of Fan et al., we randomly select\n1000 human-written stories from the test set, and\nmeasure the probability (according to the model)\nof each story conditioned on 10 different prompts\n– the true prompt, plus nine randomly selected\nprompts. The prompt ranking accuracy of a model\nis the percentage of cases in which the model as-\nsigns a higher probability to the story under its\ntrue prompt than under all of the other nine. We\nﬁnd that GPT2-117 scores 80.16% on this task,\nwhile the Fusion Model scores 39.8%.5 Random\nchance scores 10%. This striking result indicates\n5Fan et al. (2018) report a prompt ranking accuracy of\n16.3% for the Fusion Model. We provided the authors with\nour prompt ranking accuracy code (which was built on top of\nthe authors’ code). The authors indicated that the discrepancy\nmay be due to some code version changes between the time\nof their original experiments and their code release.\n846\n100 101 102 103 104 105\nk (Top-k sampling)\n0.05\n0.10Story-prompt sent sim\nHuman\nFusion Model\nGPT2-117\nFigure 1: Compared to the Fusion Model, GPT2-117\nproduces stories that are more semantically similar to\nthe prompt. Similarity decreases as k increases.\nthat GPT2-117 conditions on the prompt much\nmore strongly than the Fusion Model. This is no-\ntable, especially because the fusion technique is\nintended to improve story-prompt relevance.\nN-gram similarity For n = 1, 2, 3, we measure\nthe percentage of generated n-grams that also ap-\npear in the prompt. For all n and k, we ﬁnd that\nGPT2-117 has a higher overlap (i.e. copies more\nfrom the prompt) than the Fusion Model – see Fig-\nure 6 in the Appendix. Furthermore, for k <100,\nthe GPT2-117 overlap is generally much higher\nthan human levels. Both these phenomena can be\nseen in Table 2, where, for k = 10, GPT2-117\ncopies words such as queen more often than both\nthe Fusion Model and the human-written story.\nSentence embedding similarity To capture a\nhigher-level notion of semantic similarity, we\nmeasure story-prompt sentence similarity– the co-\nsine similarity of story-prompt sentence pairs, av-\neraged by taking the mean over all pairs. Sen-\ntences are represented by the embedding method\nof Arora et al. (2017) – a weighted average of\nthe GloVe embeddings (Pennington et al., 2014)\nof the words, with the ﬁrst principal component\nremoved. As shown in Figure 1, we ﬁnd a similar\npattern as for n-gram similarity: GPT2-117 gener-\nates sentences that are more similar to the prompt\nthan the Fusion Model for all k, and both models’\nprompt similarity decreases as k increases.\nNamed entity usage Generally, most named en-\ntities mentioned in the prompt (such as Queen and\nEngland in Table 2), should also be mentioned in\nthe story. Using the spaCy named entity recog-\nnizer,6 we measure the prompt entity usage rate ,\nwhich is the percentage of all prompt named enti-\n6https://spacy.io\nties that appear in the story. 7 As shown in Figure\n7 in the Appendix, we ﬁnd that GPT2-117 uses\nmore of the prompt named entities than the Fusion\nModel (as well as more named entities overall),\nbut both models use fewer named entities than hu-\nmans when k is less than vocabulary size.\nThese patterns can be seen in Table 2: GPT2-\n117 uses the prompt entities Queen and England\nwhereas the Fusion Model does not (for either k),\nand GPT2-117 uses speciﬁc time entities such as\nThursday and 3:26 PM . While the human story\nintroduces highly-related entities such as Charles\nWindsor and Prince of Wales that were not in the\nprompt, neither model does this (for either k).\nConclusion In this section, we found that\nGPT2-117 conditions on the prompt much more\nstrongly than the Fusion Model – a result which\nholds both in language modeling and generation\nsettings. The latter result supports Radford et al.’s\ninformal observation that GPT2 has a ‘chameleon-\nlike’ ability to ‘adapt to the style and content of the\nconditioning text’.8 We speculate that GPT2-117’s\nstronger conditioning ability may derive from its\nTransformer decoder architecture, whose power-\nful self-attention is used for story-prompt atten-\ntion. Though the Fusion Model uses a similar\nself-attention mechanism in the decoder (i.e., story\nside), the prompt-story attention has a simpler for-\nmulation – for example, there are no separate key\nand value vectors (Gehring et al., 2017). Lastly,\nwe note that very strong prompt-conditioning is\nnot always a good thing – GPT2-117 often gen-\nerates stories that copy too much or too literally\nfrom the prompt when k is small (this can be seen\nin Figure 6 in the Appendix).\n5 Coherence\nA good story generation model should produce co-\nherent text with a logical ordering of events. Sim-\nilarly, the underlying language model should be a\ngood coherence scorer – assigning higher proba-\nbility to coherent text than incoherent text. Barzi-\nlay and Lapata (2008) evaluate a coherence scorer\nby measuring its ability to rank shufﬂed human-\nwritten text as less coherent than the original un-\nshufﬂed text. We use this method to evaluate our\nstory generation models.\n7Given that we limit stories to 150 words, this percentage\nis lower than it would be if we generated longer stories.\n8https://openai.com/blog/\nbetter-language-models/\n847\n1 2 3 4 5 6 7 8 9 10 11 12 13 14\nPosition of swapped sentences\n6.5\n7.0\n7.5Mean rank (1-14)\nFusion Model\nGPT2-117\nFigure 2: Sensitivity of the models to swapped sen-\ntences in different positions. A higher mean rank in-\ndicates higher sensitivity (i.e. the model assigns lower\nprobability) relative to other positions. Both models are\nless sensitive to swapped sentences at the beginning of\nthe text, compared to later. GPT2-117 shows this pat-\ntern more strongly, indicating greater use of context.\nFor each story in the test set, we select the ﬁrst\n15 sentences. We then produce 14 corrupted ver-\nsions of the story by switching each pair of ad-\njacent sentences. We use the language model to\ncompute the probability of each of the 14 cor-\nrupted stories, as well as the original story. The\nmodel’s error rate is the percentage of cases in\nwhich it rates any of the 14 corrupted candidates\nbetter than the original candidate. Random guess-\ning yields 93.33% error. Both models perform\nwell on this task – the Fusion Model has an er-\nror rate of 3.44% and GPT2-117 an error rate of\n2.17%. This 36.92% error reduction indicates that\nGPT2-117 is more sensitive to ordering of events.\nWe also investigate how the position of the swap\naffects its plausibility (relative to other positions).\nFigure 2 shows, for each swap position, the mean\nrank assigned to that swap by the model (where\nrank 1 is the most probable of the 14 corrupted\ncandidates, and rank 14 the least probable). GPT2-\n117 assigns a much lower rank to the ﬁrst few\nswap positions (i.e., rates them more probable)\nthan the later positions. The Fusion Model shows\na similar but less pronounced pattern. This shows\nthat both models are less sensitive to out-of-order\nsentences that occur at the beginning of the text,\nthan those occurring later. 9 The stronger pattern\nfor GPT2-117 may be due to its stronger context\nconditioning (as shown in Section 4) – thus be-\ncoming more sensitive as context increases. How-\never, even for the ﬁrst three swaps, GPT2-117 is\nmore accurate than the Fusion Model at distin-\nguishing the swapped text from the original.\n9It’s also possible that out-of-order sentences are inher-\nently harder to detect at the beginning of text.\n100 101 102 103 104 105\nk (Top-k sampling)\n0.2\n0.4\n0.6Distinct-1\nHuman\nFusion Model\nGPT2-117\nFigure 3: Repetition (low distinct-1) is primarily\ncaused by choice of decoding algorithm (here low k),\nnot insufﬁcient training data. GPT2-117 is trained on\n45×more data than the Fusion Model, but is similarly\nrepetitive for all k.\n6 Repetition and rareness\nGeneric, under-diverse and repetitive text is a\nwell-documented problem in neural text genera-\ntion (Jiang and de Rijke, 2018). While there are\nmany proposed solutions to the problem (Li and\nJurafsky, 2016; Vijayakumar et al., 2018; Baheti\net al., 2018; Zhang et al., 2018; See et al., 2019), it\nhas been shown that a primary cause is likelihood-\nmaximizing decoding algorithms such as greedy\ndecoding and beam search (Holtzman et al., 2019).\nIn this section we investigate the role of large-scale\npretraining, and the role of k, in this problem.\nN-gram repetition The distinct-n metric of a\npiece of text is the number of unique n-grams di-\nvided by the total number of generated n-grams\n(Li et al., 2016). We measure distinct- n of the\ngenerated stories for n = 1, 2, 3. A high ratio\nindicates a high level of within-story lexical di-\nversity, while a low ratio indicates a large amount\nof within-story repetition. As shown in Figure 3,\nboth models’ unigram diversity is far below that\nof human text when k is small. For example, at\nk = 10 (the setting used by Fan et al.), the Fu-\nsion Model obtains a distinct-1 of 42.4%; much\nless than the human level of60.0%. This results in\na high level of repetition, as shown in Table 2: for\nk = 10, both models repeat many phrases (such as\nalways, so scared, and ﬁnally).\nFor bigrams and trigrams, the pattern is similar\nto unigrams (see Figure 9 in the Appendix). For\nboth models, distinct- n increases as k increases,\nconverging to a value close to the human level\nas k approaches vocabulary size. Though GPT2-\n117 has a slightly higher distinct- n than the Fu-\nsion Model for most values of k, the difference\nis negligible compared to the inﬂuence of k. We\n848\nmake three conclusions from these patterns: (1)\nOur ﬁndings support Holtzman et al.’s observation\nthat repetition is strongly related to choice of de-\ncoding algorithm, and that likelihood maximizing\nalgorithms (such as top- k sampling with low k)\nare a primary cause of repetition. (2) The models\nhave in fact learned the correct rate of repetition in\nhuman text – they are able to match this rate when\nthey sample from their full (untruncated) distribu-\ntion. (3) Repetition is unlikely to be solved by\nmore pretraining data alone – even though GPT2-\n117 is trained on 45 times as much data as the Fu-\nsion Model, it produces text that is almost equally\nrepetitive (for equal k).\nRare word usage We compute the mean log\nunigram probability of the words in the gener-\nated story10 – a high value indicates using fewer\nrare words while a low value indicates more rare\nwords. As shown in Figure 12 in the Appendix,\nword rareness is primarily governed by k – how-\never, GPT2-117 has a lower mean log unigram\nprobability (i.e., uses more rare words) than the\nFusion Model for all equal values of k ≥2. This\ncan be seen for example in Table 2, where GPT2-\n117 generates rarer words such as idle and copi-\nous for k = 1000. GPT2-117 also generates fewer\nstopwords than the Fusion Model, for all equal k.\nGPT2-117’s slightly higher rare word usage\n(compared to the Fusion Model) might be ex-\nplained by: (1) its BPE encoding, which allows it\nto generate new words, not just those in a ﬁxed vo-\ncabulary; (2) pretraining on a large amount of di-\nverse text, allowing it to learn to produce a greater\nvariety of words; (3) stronger conditioning on the\nprompt as described in Section 4 – which may in-\nject more rareness into the generated text.\nConclusion Choice of decoding algorithm is a\nprimary factor in diversity and repetition prob-\nlems, with likelihood-maximizing algorithms the\nmain culprit. Although GPT2-117 generates more\nrare words and is very slightly less repetitive than\nthe Fusion Model, the difference is small com-\npared to the effect of k, indicating that training\ndata alone is unlikely to solve these problems.\n7 Syntactic style and complexity\nA well-trained story generation model should\nmatch both the syntactic style and complexity of\n10The unigram probability distribution was calculated with\nrespect to the WritingPrompts training set.\nits training data. Low complexity can be a sign of\nless sophisticated writing, while high complexity\ncan be a sign of poor readability (Beers and Nagy,\n2009; McNamara et al., 2010). In this section,\nwe measure some features related to the syntactic\nstyle and complexity of the generated stories.\nSentence length Sentence length is a simple but\neffective feature to estimate readability and syn-\ntactic complexity of text (Kincaid et al., 1975;\nRoemmele et al., 2017). We ﬁnd that both models\ngenerate sentences that are on average shorter than\nhuman sentences when k is small, but converge to\napproximately human length as k increases (see\nFigure 8 in the Appendix).\nPart-of-speech usage It has been shown that the\ndistribution of parts-of-speech (POS), and more\ngenerally the distribution of POS n-grams11 is a\nuseful feature to represent the style of a piece\nof text (Argamon et al., 1998; Ireland and Pen-\nnebaker, 2010; Roemmele et al., 2017).\nFirstly, we compare the part-of-speech distri-\nbutions of the model-generated text and the hu-\nman text (see Figure 11 in the Appendix). Both\nmodels (especially GPT2-117) closely ﬁt the hu-\nman POS distribution as k approaches vocabulary\nsize.12 This implies that, as with lexical diver-\nsity, the models have no difﬁculty ﬁtting the sta-\ntistical distribution of human syntax. However,\nunder likelihood-maximizing decoding algorithms\nsuch as low k, a completely different distribution\nemerges, in which text contains more verbs and\npronouns than human text, and fewer nouns, ad-\njectives and proper nouns.\nSecondly, we measure the syntactic diversity of\nthe text using the distinct- n metric for POS n-\ngrams ( n = 1, 2, 3) – see Figure 10 in the Ap-\npendix. As with lexical diversity (see Section 6),\nwe ﬁnd that syntactic diversity is similar for the\ntwo models, is very low when k is small, and\nmatches human level as k approaches vocabulary\nsize. It’s likely that for lowk, the syntactic under-\ndiversity of the text is largely caused by lexical\nunder-diversity (i.e. repetition). However, we note\nthat as k increases, lexical diversity reaches human\nlevel sooner than syntactic diversity – for exam-\nple, GPT2-117’s lexical distinct-3 reaches human\nlevel at k = 600(Figure 9c), but its POS distinct-\n11For example, the sentence I like cats has the POS bi-\ngrams PRONOUN VERB and VERB NOUN.\n12One exception is Proper Noun: both models fail to pro-\nduce enough of these even as k approaches vocabulary size.\n849\n0 50 100 150\nToken index\n0.0\n0.5\n1.0Token probability\n(a) Fusion Model (k = 2): I had\nnever seen a man so young before. I\nhad never seen him before, but he had\nalways seemed to be a man of a man.\nHe was young, and he was young. He\nwas a man of a man, and a man who\nwas young, and a man who was [...]\n0 50 100 150\nToken index\n0.0\n0.5\n1.0Token probability\n(b) Human Text: “Looks like the\nrain’s stopped. ” I peered out the\nwindow. Art was right; time to get to\nwork. “Alright, let’s move out. ” I\ncould hear the scraping of the stone\narmor as the men slowly stood.\nDespite the training, [...]\n0 50 100 150\nToken index\n0.0\n0.5\n1.0Token probability\n(c) GPT2-117 (k = 2): I’ve always\nbeen a man of the people. I’ve always\nbeen a strong man. I’ve always been\na strong man. I was born in the city, I\nwas raised in the country. I was\nraised in a family that wasn’t very\ngood. I ’m not a good man. [...]\nFigure 4: Under top- k sampling with small k (k = 2), the two models (left and right) produce text that falls into\nincreasingly conﬁdent repeating loops. By contrast, human text (center) maintains an irregular pattern of surprising\n(low probability) tokens. The human text probabilities are measured with respect to the Fusion Model, but similar\npatterns hold for GPT2-117. Inspired by Holtzman et al. 2019’s ﬁgure showing probabilities under beam search.\n3 reaches human level at k = 6000 (Figure 10c).\nThis implies that, even when the text is no more\nrepetitive than human text, it may still be syntacti-\ncally repetitive (using the same part-of-speech pat-\nterns repeatedly).\nConclusion We ﬁnd when k is small, syntac-\ntic complexity of generated text is low, consist-\ning of shorter sentences and a narrower range of\nsyntactic patterns. However, as k approaches vo-\ncabulary size, the syntactic style of generated text\nclosely matches human syntactic patterns. As with\nn-gram diversity in Section 6, our results show\nthat syntactic under-diversity is primarily caused\nby low k, not insufﬁcient training data.\n8 The element of surprise\nModel conﬁdence over time Several re-\nsearchers have observed that model over-\nconﬁdence (the model placing high probability on\na small range of tokens) can cause poor quality\ngeneration (Jiang and de Rijke, 2018; Holtzman\net al., 2019). In particular, they show that for\nlikelihood-maximizing decoding algorithms such\nas beam search, model conﬁdence can increase in\na snowball-like effect, getting stuck in a loop of\nrepetitive but increasingly self-conﬁdent text. We\nobserve this problem in both our models when k\nis small. For example, in Figure 4, both models\nfall into self-reinforcing repetitive loops with\nrising conﬁdence. The loop is difﬁcult to break\n– the Fusion Model brieﬂy escapes (shown as a\nsudden downwards spike), but quickly returns. By\ncontrast, the human text does not show a strong\nrising trend in probability, and intermittently uses\nlow probability words throughout.13\nWe formalize these anecdotal observations by\nmeasuring the average probability of each of the\nﬁrst 150 word-level tokens in the story (Figure\n5). We ﬁnd that even when teacher-forcing on hu-\nman text, the token probabilities increase slightly\nas the story progresses. This is likely due to the\nusefulness of additional context, which increases\nthe model’s prediction accuracy. By comparison,\nwe ﬁnd that when generating with top-k sampling,\nthe probabilities increase more rapidly, and the in-\ncrease is even more rapid for smaller k. This con-\nﬁrms that likelihood-maximizing decoding algo-\nrithms (such as top-k sampling with small k) lead\nto more rapidly increasing model over-conﬁdence.\nFurthermore, we ﬁnd this pattern holds for both\nmodels, with probabilities increasing at a similar\nrate for equal k. This indicates that, like rep-\netition, model over-conﬁdence is unlikely to be\nsolved by more training data, and is largely gov-\nerned by choice of k.\nOverall model conﬁdence We also measure the\nmodels’ overall conﬁdence, as represented by the\ntotal log probability (according to the model) of\nthe generated story. For both models, we ﬁnd\nthat story probability decreases as k increases\n– see Figure 13 in the Appendix. This makes\nsense, as higher k means sampling tokens with\nlower probability. As k approaches the vocab-\nulary size, the Fusion Model’s generated story\n13Gehrmann et al. (2019) also identify presence of low\nprobability words as an indicator of human-generated text.\n850\n0 25 50 75 100 125 150\nToken index\n0.10\n0.15\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45Mean probability of token\nTop-k sampling, k=5\nTop-k sampling, k=20\nTeacher-force on human text\nFigure 5: Mean probability for each of the ﬁrst 150\nword-level story tokens. When teacher-forcing the\nmodel on human text, probability increases slowly.\nWhen generating with top- k sampling, probability in-\ncreases faster, especially for smaller k. This plot is for\nthe Fusion Model; similar patterns hold for GPT2-117.\nprobability matches the probability it assigns to\nhuman-written WritingPrompts stories. Interest-\ningly however, the same is not true for GPT2-\n117, which converges to a story probability that\nis lower than the probability it assigns the human\nstories. This means that under full (non-truncated)\nsampling, the Fusion Model produces text that\nis equally surprising (to itself) as the Writing-\nPrompts stories, whereas GPT2-117 produces text\nthat is more surprising to itself. Explaining this\nobservation is an open question – we speculate that\nGPT2-117’s WebText pretraining may cause it to\ngenerate (under high k) text in a style or genre that\nis less predictable than WritingPrompts stories.\n9 Concreteness\nBrysbaert et al. (2014) deﬁne the concreteness of\na word as ‘the degree to which the concept de-\nnoted by a word refers to a perceptible entity’.\nConcrete words are generally easier to remem-\nber than abstract words, and psycholinguists have\ntheorized they may be learned differently (i.e.,\nconcrete words by direct experience and abstract\nwords by text and discourse). Brysbaert et al. pro-\nvide human concreteness ratings for 40,000 com-\nmon English lemmas rated on a scale from 1 to\n5.14 We use these ratings to measure the mean\nconcreteness of the nouns and verbs in the story\n14For example, the nounstelevision, darkness, and idea are\nrated 4.83, 3.85 and 1.61 respectively, and the verbstalk, see,\nand hope are rated 4.07, 3.21 and 1.25 respectively.\ntext – see Figure 14 in the Appendix.\nWe ﬁnd that, for the same k, GPT2-117 tends\nto generate more concrete words than the Fusion\nModel, and that for both models, concreteness\nconverges to approximately human levels as k in-\ncreases. Interestingly, however, when k is small,\nthe noun concreteness is much higher than hu-\nman levels, whereas the verb concreteness is much\nlower. This indicates that for small k, both models\nproduce stories that, compared to human-written\nstories, have too many physical objects (as op-\nposed to abstract nouns), and too few physical\nactions (as opposed to abstract verbs). This re-\nﬂects the trend demonstrated in Table 2: whenk is\nsmall, the models tend to generate descriptive sen-\ntences with mostly is verbs (e.g. I was always so\nexcited), and physical nouns (e.g. mother, father,\nqueen). Only when k increases do we see more\ntangible actions (e.g. The bar patrons snickered )\nand abstract nouns (e.g. pain, glances). A detailed\nexample, with all nouns and verbs annotated with\nconcreteness, is in the Appendix (Table 3).\n10 Conclusions\nThe effect of massive pretraining In this study,\nwe ﬁnd that GPT2-117 is a better story genera-\ntion model than the Fusion Model in several spe-\nciﬁc ways: it conditions much more strongly on\nthe provided context, is more sensitive to cor-\nrect ordering of events, and generates text that\nis more contentful (using more rare words, con-\ncrete words, and named entities). In particu-\nlar, the stronger conditioning result is striking, as\nthe Fusion Model is a complex task-speciﬁc ar-\nchitecture designed to increase story-prompt rel-\nevance. This demonstrates that sometimes, a\ngeneral-purpose model architecture can outper-\nform a complex task-speciﬁc architecture when\nprovided with enough pretraining data.\nHowever, we ﬁnd that in other aspects, GPT2-\n117 performs no better than the Fusion Model:\nwhen k is small, the models generate text that\nis equally lexically under-diverse, syntactically\nunder-complex, and repetitive – with a tendency\nto fall into a snowball effect of increasing over-\nconﬁdence. However, these problems correct\nthemselves (i.e., the metrics match human levels)\nwhen the models generate from their untruncated\ndistribution. Our results show that these oft-cited\nneural generation problems are not the fault of\nthe models themselves (which are in fact statisti-\n851\nPrompt: You just discovered England’s biggest secret: every time they say long live the queen it extends the queens life.\nHuman-Written Story: I am immortal, I have inside me blood of kings. My name in Charles Windsor and I am heir to a\nthrone. Prince of Wales, they’ve called me that my entire life and now I lay on my death bed and my mother - still going\nstrong. I’m not sure how it’s possible but she seems ageless, almost timeless. My whole life I’ve waited for that [...]\nFusion Model (k = 10): I have no idea when my father\nwould come to visit this place, I was always so excited. I\nwas always so scared. I had to go back to work. I was a\nlittle scared of him, but when I was younger, he had never\ncome in for a visit. His mother told me, “I’m so scared”, as\nhe always did. She never talked to me about anything [...]\nGPT2-117 (k = 10): * ” So what are we going to do? “ *\nIt was the ﬁrst time she asked me this question that I ever\nhad asked her. It has taken years but ﬁnally I am ﬁnally able\nto do it. It seems that this queen is the queen of England\nand she’s ﬁnally the one to tell me that this queen is the\nreason why she’s not being allowed to die.[...]\nFusion Model (k = 1000): “Where am I? What\nhappened?” “Having been working on my job for over 6\nhours now, I do not know how you worked!” “I have been\nworking for the last three years. Surely I am an ancient god\nnow. ” The bar patrons snickered. “Hello?” “Those last\nthree years have been worse than a year ago. ” Pain.[...]\nGPT2-117 (k = 1000): It was an odd occasion for the\nQueen of England to meet with her. The plane sat idle at\n3:26 PM on a Thursday night. Yesterday, the Queen had\ntaken it upon herself to try and get a good look at the plane\nwhich had recently been found abandoned. A copious\namount of curious glances from around the room until[...]\nTable 2: A prompt and human story from the dataset, plus the models’ top-k generated stories, for two values ofk.\ncally well-trained to match human text for these\nmetrics), nor caused by too little training data (as\nthese problems are not improved by GPT2-117’s\nextensive pretraining). Instead, they are primarily\ncaused by likelihood-maximizing decoding algo-\nrithms – such as greedy decoding, beam search,\nand top-k sampling with low k.\nThe effect of k This study detailed the typical\ncharacteristics of long-form text generated by neu-\nral language models in open-ended settings, under\nboth high entropy (largek) and low entropy (small\nk) decoding algorithms. The negative characteris-\ntics of low k output (genericness, repetition, over-\nsimplicity) are by now familiar to researchers.\nHowever, we also uncovered some less obvious\ncharacteristics of low-k generated text: compared\nto human-written text, it tends to copy more from\nthe provided context (particularly GPT2-117); it\ncontains more verbs and pronouns but fewer nouns\nand adjectives; its nouns are more concrete but its\nverbs are less concrete; and it uses a smaller range\nof syntactic patterns (a phenomenon that can’t be\nentirely attributed to n-gram repetition).\nAs k increases to vocabulary size, we ﬁnd that\nthe model-generated text closely ﬁts the human\ntext on most of the metrics we measured. How-\never, it is clear by inspection that the high- k\nmodel-generated text lacks many crucial aspects\nsuch as commonsense reasoning, world knowl-\nedge and multi-sentence coherence – an example\nof this superﬁcially ﬂuent but nonsensical text can\nbe seen in Table 4 in the Appendix. We believe\nthat true progress in open-ended Natural Language\nGeneration will come from attempting to address\nthese high k problems – i.e., strategies to imbue\nthe language model with better reasoning, knowl-\nedge and planning abilities – rather than continu-\ning to seek ways to mitigate the diversity and rep-\netition problems of the low k setting.\nLimitations of this study This study uses only\nthe smallest version of GPT2. It is likely that\nthe larger versions of GPT2 may exhibit stronger\nstatistical differences for the metrics we examine.\nSuch a study would illustrate the effect of larger\nmodel capacity, and more fully reveal the possible\nbeneﬁts of massive pretraining. We release our an-\nnotation code so that other researchers may repeat\nour study on more models and datasets.\nThis study did not include human evaluation,\nwhich is currently the only reliable way to assess\noverall text quality, as well as quantify the deﬁ-\nciencies of high k output described above (coher-\nence, reasoning, and world knowledge). As such,\nthis study quantiﬁes the diversity side more than\nthe quality side of the quality-diversity tradeoff.\nConsequently, this study demonstrates the impor-\ntance of developing better methods to computa-\ntionally quantify notions such as text coherence,\nlogicality and commonsense correctness – an ef-\nfort that may ultimately hold the key to generating\ntext with those desirable attributes.\n11 Acknowledgments\nThis work was funded by the Gerald J. Lieberman\nFellowship, Tencent, and the DARPA CwC pro-\ngram under ARO prime contract no. W911NF-\n15-1-0462. We also thank the reviewers for their\nhelpful comments.\n852\nReferences\nShlomo Argamon, Moshe Koppel, and Galit Avneri.\n1998. Routing documents according to style. In\nFirst International workshop on innovative informa-\ntion systems.\nSanjeev Arora, Yingyu Liang, and Tengyu Ma. 2017.\nA simple but tough-to-beat baseline for sentence em-\nbeddings. In International Conference on Learning\nRepresentations.\nAshutosh Baheti, Alan Ritter, Jiwei Li, and Bill Dolan.\n2018. Generating more interesting responses in\nneural conversation models with distributional con-\nstraints. In Empirical Methods in Natural Language\nProcessing.\nRegina Barzilay and Mirella Lapata. 2008. Modeling\nlocal coherence: An entity-based approach. Compu-\ntational Linguistics, 34(1):1–34.\nScott F Beers and William E Nagy. 2009. Syntactic\ncomplexity as a predictor of adolescent writing qual-\nity: Which measures? which genre? Reading and\nWriting, 22(2):185–200.\nMarc Brysbaert, Amy Beth Warriner, and Victor Ku-\nperman. 2014. Concreteness ratings for 40 thousand\ngenerally known english word lemmas. Behavior\nResearch Methods, 46(3):904–911.\nMassimo Caccia, Lucas Caccia, William Fedus, Hugo\nLarochelle, Joelle Pineau, and Laurent Charlin.\n2018. Language gans falling short. In NeurIPS\nWorkshop on Critiquing and Correcting Trends in\nMachine Learning.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. Bert: Pre-training of deep\nbidirectional transformers for language understand-\ning. In North American Chapter of the Association\nfor Computational Linguistics: Human Language\nTechnologies.\nAngela Fan, Mike Lewis, and Yann Dauphin. 2018. Hi-\nerarchical neural story generation. In Association\nfor Computational Linguistics.\nJonas Gehring, Michael Auli, David Grangier, Denis\nYarats, and Yann N Dauphin. 2017. Convolutional\nsequence to sequence learning. In International\nConference on Machine Learning.\nSebastian Gehrmann, Hendrik Strobelt, and Alexan-\nder M Rush. 2019. Gltr: Statistical detection\nand visualization of generated text. arXiv preprint\narXiv:1906.04043.\nTatsunori Hashimoto, Hugh Zhang, and Percy Liang.\n2019. Unifying human and statistical evaluation\nfor natural language generation. In North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies.\nAri Holtzman, Jan Buys, Maxwell Forbes, and Yejin\nChoi. 2019. The curious case of neural text degen-\neration. arXiv preprint arXiv:1904.09751.\nMolly E Ireland and James W Pennebaker. 2010. Lan-\nguage style matching in writing: Synchrony in es-\nsays, correspondence, and poetry. Journal of per-\nsonality and social psychology, 99(3):549.\nShaojie Jiang and Maarten de Rijke. 2018. Why\nare sequence-to-sequence models so dull? Under-\nstanding the low-diversity problem of chatbots. In\nEMNLP Workshop on Search-Oriented Conversa-\ntional AI.\nJ Peter Kincaid, Robert P Fishburne Jr, Richard L\nRogers, and Brad S Chissom. 1975. Derivation of\nnew readability formulas (automated readability in-\ndex, Fog count and Flesch reading ease formula) for\nnavy enlisted personnel.\nIlya Kulikov, Alexander H Miller, Kyunghyun Cho,\nand Jason Weston. 2018. Importance of a search\nstrategy in neural dialogue modelling. arXiv\npreprint arXiv:1811.00907.\nJiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,\nand Bill Dolan. 2016. A diversity-promoting ob-\njective function for neural conversation models. In\nNorth American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nJiwei Li and Dan Jurafsky. 2016. Mutual information\nand diverse decoding improve neural machine trans-\nlation. arXiv preprint arXiv:1601.00372.\nChia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-\nworthy, Laurent Charlin, and Joelle Pineau. 2016.\nHow not to evaluate your dialogue system: An em-\npirical study of unsupervised evaluation metrics for\ndialogue response generation. In Empirical Meth-\nods in Natural Language Processing.\nHuanru Henry Mao, Bodhisattwa Prasad Majumder,\nJulian McAuley, and Garrison W. Cottrell. 2019.\nImproving neural story generation by targeted com-\nmon sense grounding. In Empirical Methods in Nat-\nural Language Processing.\nDanielle S McNamara, Scott A Crossley, and Philip M\nMcCarthy. 2010. Linguistic features of writing qual-\nity. Written communication, 27(1):57–86.\nJekaterina Novikova, Ond ˇrej Du ˇsek, Amanda Cercas\nCurry, and Verena Rieser. 2017. Why we need new\nevaluation metrics for NLG. In Empirical Methods\nin Natural Language Processing.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela\nFan, Sam Gross, Nathan Ng, David Grangier, and\nMichael Auli. 2019. Fairseq: A fast, extensible\ntoolkit for sequence modeling. In North American\nChapter of the Association for Computational Lin-\nguistics: Human Language Technologies.\n853\nJeffrey Pennington, Richard Socher, and Christo-\npher D. Manning. 2014. Glove: Global vectors for\nword representation. In Empirical Methods in Natu-\nral Language Processing.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt\nGardner, Christopher Clark, Kenton Lee, and Luke\nZettlemoyer. 2018. Deep contextualized word repre-\nsentations. In North American Chapter of the Asso-\nciation for Computational Linguistics: Human Lan-\nguage Technologies.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and\nIlya Sutskever. 2018. Improving language under-\nstanding by generative pre-training. OpenAI tech re-\nport.\nAlec Radford, Jeffrey Wu, Rewon Child, David Luan,\nDario Amodei, and Ilya Sutskever. 2019. Language\nmodels are unsupervised multitask learners. OpenAI\ntech report.\nMelissa Roemmele, Andrew S Gordon, and Reid\nSwanson. 2017. Evaluating story generation sys-\ntems using automated linguistic analyses. In KDD\nWorkshop on Machine Learning for Creativity.\nAbigail See, Stephen Roller, Douwe Kiela, and Jason\nWeston. 2019. What makes a good conversation?\nhow controllable attributes affect human judgments.\nIn North American Chapter of the Association for\nComputational Linguistics: Human Language Tech-\nnologies.\nIulian V Serban, Alessandro Sordoni, Yoshua Bengio,\nAaron Courville, and Joelle Pineau. 2016. Building\nend-to-end dialogue systems using generative hier-\narchical neural network models. In AAAI Confer-\nence on Artiﬁcial Intelligence.\nLifeng Shang, Zhengdong Lu, and Hang Li. 2015.\nNeural responding machine for short-text conversa-\ntion. In Association for Computational Linguistics.\nAnuroop Sriram, Heewoo Jun, Sanjeev Satheesh, and\nAdam Coates. 2018. Cold fusion: Training seq2seq\nmodels together with language models. In Proc. In-\nterspeech 2018, pages 387–391.\nAshwin K Vijayakumar, Michael Cogswell, Ram-\nprasath R Selvaraju, Qing Sun, Stefan Lee, David\nCrandall, and Dhruv Batra. 2018. Diverse beam\nsearch: Decoding diverse solutions from neural se-\nquence models. In AAAI Conference on Artiﬁcial\nIntelligence.\nRuqing Zhang, Jiafeng Guo, Yixing Fan, Yanyan Lan,\nJun Xu, and Xueqi Cheng. 2018. Learning to con-\ntrol the speciﬁcity in neural response generation. In\nAssociation for Computational Linguistics.\nZachary M Ziegler, Luke Melas-Kyriazi, Sebastian\nGehrmann, and Alexander M Rush. 2019. Encoder-\nagnostic adaptation for conditional language gener-\nation. arXiv preprint arXiv:1908.06938.\n854\nAppendix\n100 101 102 103 104 105\nk (Top-k sampling)\n16%\n18%\n20%\n22%% Story unigrams in prompt\nHuman\nFusion Model\nGPT2-117\n(a) Percent of all story unigrams that are in the prompt.\n100 101 102 103 104 105\nk (Top-k sampling)\n0%\n1%\n2%\n3%\n4%\n5%% Story bigrams in prompt\nHuman\nFusion Model\nGPT2-117\n(b) Percent of all story bigrams that are in the prompt.\n100 101 102 103 104 105\nk (Top-k sampling)\n0.0%\n0.5%\n1.0%\n1.5%\n2.0%\n2.5%\n3.0%% Story trigrams in prompt\nHuman\nFusion Model\nGPT2-117\n(c) Percent of all story trigrams that are in the prompt.\nFigure 6: n-gram similarity between prompt and story, forn = 1, 2, 3, for both models and allk. GPT2-117 copies\nmany more n-grams from the prompt than the Fusion Model. See Section 4 for discussion.\n855\n100 101 102 103 104 105\nk (Top-k sampling)\n0%\n5%\n10%\n15%Prompt entity usage rate\nHuman\nFusion Model\nGPT2-117\n(a) The proportion of all prompt named entities that are\nused in the story.\n100 101 102 103 104 105\nk (Top-k sampling)\n0\n1\n2\n3\n4# Unique named entities per story\nHuman\nFusion Model\nGPT2-117\n(b) The number of unique named entities that appear in\nthe story.\nFigure 7: Prompt entity usage rate (left) and mean number of unique named entities in the story (right), for both\nmodels and all k. GPT2-117 generally uses a larger proportion of the prompt named entities, and more named\nentities overall, than the Fusion Model. Both models generally use fewer named entities than human text when k\nis less than vocabulary size. See Section 4 for discussion.\n100 101 102 103 104 105\nk (Top-k sampling)\n12\n13\n14\n15Mean sent len (# words)\nHuman\nFusion Model\nGPT2-117\nFigure 8: Mean sentence length for both models and all k. For both models, sentence length increases as k\nincreases. The spike at k = 1is due to long repeating sequences with no sentence-ending token. See Section 7 for\ndiscussion.\n856\n100 101 102 103 104 105\nk (Top-k sampling)\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6Distinct-1\nHuman\nFusion Model\nGPT2-117\n(a) Distinct-1 (ratio of unique unigrams in the story to\ntotal number of generated unigrams in the story).\n100 101 102 103 104 105\nk (Top-k sampling)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Distinct-2\nHuman\nFusion Model\nGPT2-117\n(b) Distinct-2 (ratio of unique bigrams in the story to total\nnumber of generated bigrams in the story).\n100 101 102 103 104 105\nk (Top-k sampling)\n0.0\n0.2\n0.4\n0.6\n0.8\n1.0Distinct-3\nHuman\nFusion Model\nGPT2-117\n(c) Distinct-3 (ratio of unique trigrams in the story to total\nnumber of generated trigrams in the story).\nFigure 9: Distinct- n for n = 1, 2, 3, for both models and all k. The ratios, which represent lexical diversity,\nincrease as k increases, with GPT2-117 reaching human levels at k = 2000 for unigrams, k = 800 for bigrams\nand k = 600for trigrams. Lexical diversity is slightly higher for GPT2-117 than for the Fusion Model for equalk,\nbut the primary determining factor is k. See Section 6 for discussion.\n857\n100 101 102 103 104 105\nk (Top-k sampling)\n0.03\n0.04\n0.05\n0.06\n0.07\n0.08Distinct-1 for POS tags\nHuman\nFusion Model\nGPT2-117\n(a) POS tag distinct-1 (ratio of unique POS unigrams in\nthe story to total number of generated POS unigrams in\nthe story).\n100 101 102 103 104 105\nk (Top-k sampling)\n0.0\n0.1\n0.2\n0.3\n0.4Distinct-2 for POS tags\nHuman\nFusion Model\nGPT2-117\n(b) POS tag distinct-2 (ratio of unique POS bigrams in\nthe story to total number of generated POS bigrams in\nthe story).\n100 101 102 103 104 105\nk (Top-k sampling)\n0.0\n0.2\n0.4\n0.6Distinct-3 for POS tags\nHuman\nFusion Model\nGPT2-117\n(c) POS tag distinct-3 (ratio of unique POS trigrams in\nthe story to total number of generated POS trigrams in\nthe story).\nFigure 10: POS tag distinct- n metric for n = 1, 2, 3, for both models and all k. The ratios, which represent\nsyntactic diversity, increase as k increases, with GPT2-117 reaching human levels at k = 6000 for unigrams,\nk = 9000for bigrams, and k = 6000for trigrams. Syntactic diversity is slightly higher for GPT2-117 than for the\nFusion Model for equal k, but the primary determining factor is k. See Section 7 for discussion.\n858\n100 101 102 103 104 105\nk (Top-k sampling)\n5.0%\n10.0%\n15.0%\n20.0%Verb usage Human\nFusion Model\nGPT2-117\n100 101 102 103 104 105\nk (Top-k sampling)\n8.0%\n10.0%\n12.0%\n14.0%\n16.0%\n18.0%Noun usage\nHuman\nFusion Model\nGPT2-117\n100 101 102 103 104 105\nk (Top-k sampling)\n2.0%\n4.0%\n6.0%\n8.0%Adverb usage\nHuman\nFusion Model\nGPT2-117\n100 101 102 103 104 105\nk (Top-k sampling)\n1.0%\n2.0%\n3.0%\n4.0%\n5.0%Adjective usage\nHuman\nFusion Model\nGPT2-117\n100 101 102 103 104 105\nk (Top-k sampling)\n2.0%\n4.0%\n6.0%\n8.0%\n10.0%\n12.0%Determiner usage\nHuman\nFusion Model\nGPT2-117\n100 101 102 103 104 105\nk (Top-k sampling)\n8.0%\n10.0%\n12.0%\n14.0%\n16.0%\n18.0%Pronoun usage\nHuman\nFusion Model\nGPT2-117\n100 101 102 103 104 105\nk (Top-k sampling)\n0.0%\n0.2%\n0.4%\n0.6%\n0.8%Numeral usage\nHuman\nFusion Model\nGPT2-117\n100 101 102 103 104 105\nk (Top-k sampling)\n0.0%\n0.5%\n1.0%\n1.5%\n2.0%\n2.5%Proper Noun usage\nHuman\nFusion Model\nGPT2-117\nFigure 11: Usage of different POS tags in the generated stories. GPT2-117 tends to ﬁt the human distribution\nmore closely than the Fusion Model as k approaches vocabulary size, in particular producing more speciﬁc POS\ncategories such as Numeral and Proper Noun. When k is small, generated text is characterized by more verbs and\npronouns, and fewer nouns, adjectives, numerals and proper nouns, than human text. See Section 7 for discussion.\n859\n100 101 102 103 104 105\nk (Top-k sampling)\n6.50\n6.25\n6.00\n5.75\n5.50\n5.25\nMean log unigram prob\nHuman\nFusion Model\nGPT2-117\n(a) The mean log unigram probability of generated words.\nHigher values indicate using fewer rare words while\nlower values indicate using more rare words.\n100 101 102 103 104 105\nk (Top-k sampling)\n60%\n65%\n70%\n75%% Words that are stopwords\nHuman\nFusion Model\nGPT2-117\n(b) The percent of generated words that are stopwords,\nfor both models, across different k. We use the NLTK\nEnglish stopword list.\nFigure 12: Rare word usage metrics for both models and all k. GPT2-117 produces slightly more rare words (left)\nand slightly fewer stopwords (right) than the Fusion Model, for equal values of k. These rareness metrics do not\nreach human levels until k is close to vocabulary size. See Section 6 for discussion.\n100 101 102 103 104 105\nk (Top-k sampling)\n600\n500\n400\n300\n200\n100\n0\nMean total log prob of story\nFusion Model, teacher-\nforce on human text\nFusion Model, generating\n100 101 102 103 104 105\nk (Top-k sampling)\n600\n500\n400\n300\n200\n100\n0\nMean total log prob of story\nGPT2-117, teacher-\nforce on human text\nGPT2-117, generating\nFigure 13: The mean total log probability of the story (150 words), as measured by the models on their own gener-\nated output and on human-written stories. Interestingly, the Fusion Model (left) converges to the same probability\nit assigns to human-written stories as k approaches vocabulary size, whereas GPT2-117 (right) converges to a\nlower probability. See Section 8 for discussion.\n860\nPrompt: A man ﬁnally discovers his superpower... well into his 80’s.\nFusion Model (k = 10): GPT2-117 (k = 10):\nI be a child . bear into a world of darkness , and in the\nworld , there be only a few people . My mother and I be\nalways alone . She have never be like this . But , she have\nno idea what we be together to be . We be always alone .\nWe be always together . And , of course , she have never\nexist . But , I never think about it , because she be always\nalone . In the world , it was n’t like she have a power . I be\nalways alone , and there be a time when the light would\nturn on . There be a time when I could see the light , and\nI could see it in her eyes , and I could see the light , and I\ncould see it\nMean noun concreteness: 3.892\nMean verb concreteness: 2.173\nA man in an old white coat sit in a corner. He wear a\nblack jacket with a blue tie. His hair be grey. His eyes be\nblack, but his voice be deep and clear. He look to where\nthe man be sit, and he see a smile on his lips. It be a\nsmile he know he would see from his own eyes. But he\nbe too late. He be on the sidewalk by the river when the\nman come. He be wear a black coat with a purple tie. He\nhave a black tie and a white shirt. But he be still wear a\nwhite suit. And it seem he would look back at him. A\nsmile on his face. A look his friend do n’t recognize. He\nhave no\nMean noun concreteness: 4.720\nMean verb concreteness: 2.488\nFusion Model (k = 1000): GPT2-117 (k = 1000):\nFor a brief moment, the dark green of his eyes ﬂash the\nlast lines of reality into existence, ﬁnally fade slowly into\nexistence so that he could not feel it. Only the millions\nof pixels ﬂoat in his peripheral vision almost disappear.\nHis radio respond to the pinging very hard silence of the\noutside world. Seven people have visit his main internal\nnetwork for what seem like a lifetime but this time, the\nonly switch to an external supply system that he could\nsimply take advantage of. Unable to convey feelings\nabout the last words he would have to endure, but it\nhave respond to the innumerable messages and countless\nsleepless hours. Most of them be always available on\nits surface, just to make sure. In his quest for to spend\neternity on death, he send\nMean noun concreteness: 3.201\nMean verb concreteness: 2.435\n( First time poster, hope its ok ) The young boy, watch\ntv, spot the television onscreen, before glance around to\nsee the screen start the countdown on the tv, point to the\nscreen in “ It ’s both the same. ” “... let ’s... let ’s try this\nand... we will team up so that... we can... have the same\npower....like... so we can use this superpower over and\nover again. ” A brief silence. Only a familiar conversation,\ninterrupt his mad dash movement, follow with his high\npitch slurred and wither voice : “ I ca n’t stand anyone\ntalk like that son*s*. ” More casual conversation that\ninterrupt his childish step be rush to the scissors.\nMean noun concreteness: 3.793\nMean verb concreteness: 3.162\nTable 3: Generated stories from both models, under k = 10 and k = 1000. Nouns are highlighted in green\nand verbs in yellow. The highlighting intensity reﬂects the word’s concreteness rating. For equal k, GPT2-117\ngenerally generates more concrete words than the Fusion Model. For both models, low k is characterized by high\nnoun concreteness (e.g. physical objects such as jacket) and low verb concreteness (e.g. non-physical actions such\nas be). Conversely, high k is characterized by low noun concreteness (e.g. abstract concepts such as reality) and\nhigh verb concreteness (e.g. physical actions such as talk). See Section 9 for discussion.\n861\n100 101 102 103 104 105\nk (Top-k sampling)\n3.7\n3.8\n3.9\n4.0\n4.1\n4.2\n4.3Mean noun concreteness\nHuman\nFusion Model\nGPT2-117\n(a) Mean concreteness rating (1-5) of nouns in the story.\n100 101 102 103 104 105\nk (Top-k sampling)\n2.2\n2.4\n2.6\n2.8\n3.0Mean verb concreteness\nHuman\nFusion Model\nGPT2-117 (b) Mean concreteness rating (1-5) of verbs in the story.\nFigure 14: Mean concreteness rating of the nouns and verbs in the story, for both models and all k. GPT2-117\ngenerally produces nouns and verbs that are more concrete than the Fusion Model for the samek. For both models,\nas k increases, noun concreteness reduces and verb concreteness increases. See Section 9 for discussion.\nPrompt: In an alternative reality where sleep is non-existent among living beings, our protagonist (spontaneously or\nafter an event) falls asleep in which he/she experiences for the ﬁrst time in human history what a dream is.\nGPT2-117 (k = 1000): I sat in my bed as my girlfriend sat behind me, buzzing into her e-reader, letting the day’s\nstories write themselves on her’s monitor. Like a blur, all the usual high-asyllabic drivel and senseless ramblings that\nnormally attracted the attention of a horrid sleeping creature huddled about me like a faucet. She did not know how I\nplaced this car with her. But I tried, ﬁrst tried to ignore that I had hired the services of a dog to help and then quietly\nused it in a desperate bid to drive the car through the lawn. Each and every day, I watched her drool down an old dusty\nhardwood mattress her beady eyes trying desperately to think of this rotting dream.[...]\nTable 4: An example of syntactically natural but nonsensical text, generated with high k. Though the text is ﬂuent\nand readable, it is surreal and bizarre. See Section 10 for discussion.",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7382196187973022
    },
    {
      "name": "Variety (cybernetics)",
      "score": 0.7301726341247559
    },
    {
      "name": "Language model",
      "score": 0.6444058418273926
    },
    {
      "name": "Context (archaeology)",
      "score": 0.5736632347106934
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5538828372955322
    },
    {
      "name": "Natural language processing",
      "score": 0.504050612449646
    },
    {
      "name": "Decoding methods",
      "score": 0.495108038187027
    },
    {
      "name": "Natural language",
      "score": 0.44204556941986084
    },
    {
      "name": "Natural language understanding",
      "score": 0.41785088181495667
    },
    {
      "name": "History",
      "score": 0.07505851984024048
    },
    {
      "name": "Algorithm",
      "score": 0.06551221013069153
    },
    {
      "name": "Archaeology",
      "score": 0.0
    }
  ]
}