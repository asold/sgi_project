{
  "title": "Diachronic degradation of language models: Insights from social media",
  "url": "https://openalex.org/W2799068938",
  "year": 2018,
  "authors": [
    {
      "id": "https://openalex.org/A2741332919",
      "name": "Kokil Jaidka",
      "affiliations": [
        "University of Pennsylvania"
      ]
    },
    {
      "id": "https://openalex.org/A2282327256",
      "name": "Niyati Chhaya",
      "affiliations": [
        "Adobe Systems (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2139503071",
      "name": "Lyle Ungar",
      "affiliations": [
        "University of Pennsylvania"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2741040861",
    "https://openalex.org/W1570098300",
    "https://openalex.org/W4294170691",
    "https://openalex.org/W2740751204",
    "https://openalex.org/W2953384591",
    "https://openalex.org/W1922399266",
    "https://openalex.org/W2119595472",
    "https://openalex.org/W2251812186",
    "https://openalex.org/W2063804110",
    "https://openalex.org/W2757124792",
    "https://openalex.org/W2153579005",
    "https://openalex.org/W2949709688",
    "https://openalex.org/W2402144811",
    "https://openalex.org/W2771849569",
    "https://openalex.org/W2811293343",
    "https://openalex.org/W2963780471",
    "https://openalex.org/W2307020448",
    "https://openalex.org/W2769358515",
    "https://openalex.org/W2963099212"
  ],
  "abstract": "Natural languages change over time because they evolve to the needs of their users and the socio-technological environment. This study investigates the diachronic accuracy of pre-trained language models for downstream tasks in machine learning and user profiling. It asks the question: given that the social media platform and its users remain the same, how is language changing over time? How can these differences be used to track the changes in the affect around a particular topic? To our knowledge, this is the first study to show that it is possible to measure diachronic semantic drifts within social media and within the span of a few years.",
  "full_text": "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 195–200\nMelbourne, Australia, July 15 - 20, 2018.c⃝2018 Association for Computational Linguistics\n195\nDiachronic degradation of language models:\nInsights from social media\nKokil Jaidka\nComputer & Information Science\nUniversity of Pennsylvania\njaidka@sas.upenn.edu\nNiyati Chhaya\nBig Data Experience Lab\nAdobe Research\nnchhaya@adobe.com\nLyle H. Ungar\nComputer & Information Science\nUniversity of Pennsylvania\nungar@cis.upenn.edu\nAbstract\nNatural languages change over time be-\ncause they evolve to the needs of their\nusers and the socio-technological envi-\nronment. This study investigates the di-\nachronic accuracy of pre-trained language\nmodels for downstream tasks in machine\nlearning and user proﬁling. It asks the\nquestion: given that the social media plat-\nform and its users remain the same, how\nis language changing over time? How\ncan these differences be used to track the\nchanges in the affect around a particular\ntopic? To our knowledge, this is the ﬁrst\nstudy to show that it is possible to mea-\nsure diachronic semantic drifts within so-\ncial media and within the span of a few\nyears.\n1 Introduction\nNatural languages are dynamic–they are con-\nstantly evolving and adapting to the needs of their\nusers and the environment of their use (Frermann\nand Lapata, 2016). The arrival of large-scale col-\nlections of historic texts and online libraries and\nGoogle Books have greatly facilitated computa-\ntional investigations of language change over the\nspan of decades. Diachronic differences measure\nsemantic drift speciﬁcally for languages over time.\nFor instance, the meaning of the word ‘follow’\nhas changed from a reference, then to surveil-\nlance, and ﬁnally to the act of subscribing to a\nsocial media user’s feed. In a quantitative anal-\nysis, diachronic differences may explain why pre-\ndictive models go ‘stale’. For instance, a sentiment\nmodel trained on Victorian-era language would la-\nbel ‘aweful’ as positive sentiment; however, in\ncontemporary usage, ‘awful’ is considered a nega-\ntive word (Wijaya and Yeniterzi, 2011). Thus mo-\ntivated, we raise the following research questions:\n• How do language models trained at one point\nin time, perform at predicting age and gender\non language from a subsequent time?\n• What is the practical beneﬁt of measuring di-\nachronic differences on Twitter?\nTo our knowledge, there is no existing work which\nhas investigated whether, and how, language mod-\nels degrade over time, i.e. why predictive models\ntrained on an older sample of language, may fail\nto work on contemporary language. While pre-\nvious studies have explored the change in word\nmeanings spanning decades or hundreds of years,\nwe address a research gap by exploring ﬁner tem-\nporal granularity and using a more accessible lan-\nguage corpus. Twitter’s1 discourse is rather differ-\nent from traditional English writing. So far, word\nembeddings trained on Twitter (Kulkarni et al.,\n2015; Mikolov et al., 2013) have considered it a\nstatic corpus, and have not used it to study short\nterm changes in word connotations. It contributes\nwith the following observations:\n• Diachronic differences are greater (hence,\nlanguage change is faster) for younger social\nmedia users than older social media users.\n• Diachronic language differences enable the\nmeasurement of the change in social attitudes\n(captured by word embeddings).\nIn order to study this phenomenon, we deﬁne the\nnotion of temporal cohortsas a set of social me-\ndia users who have posted on Twitter during the\nsame time period, e.g., in the year 2011. In this\nstudy, we evaluate the linguistic differences be-\ntween temporal cohorts, e.g. 20-year-olds in 2011\nvs. 20-year-olds in 2015.\n1https://twitter.com/\n196\n2 Related work\nA number of studies have built language models\nto predict users’ age and gender (Sap et al., 2014),\npersonality (Schwartz et al., 2013) and other traits\n(Jaidka et al., 2018a) with high accuracy from a\nsample of their social media posts. We offer the\nexplanation that these language models may have\n‘degraded’ due to the diachronic changes in lan-\nguage over the past few years, as compared to the\npredictions on their posts in 2011, which is closer\nto the time period for which their model was actu-\nally trained (see Figure 1).\nThe work by Frerman and Lapata (2016) quan-\ntiﬁed meaning change in terms of emerging mean-\nings over many time periods, on a corpus collating\ndocuments spanning the years 1700-2010. Studies\nmeasuring semantic drift using word embedding\nmodels trained on Twitter corpora, such as Twit-\nter GloVe and Word2Vec (Mikolov et al., 2013;\nKulkarni et al., 2015), have considered microblog\nposts a static resource, reﬂective of modern lan-\nguage usage at a single point in time. Szymanski\n(2017) highlights the need to explore it in contem-\nporary language e.g. social media. We illustrate\nthat it is possible to measure diachronic semantic\ndrifts within social media and within the span of a\nfew years. Furthermore, we are arguing that year-\nrelated change affects different cohorts differently.\nWe use part-of-speech information about word\nembeddings to better understand semantic drift in\nterms of the adjective and affective words used\nto connotate everyday concepts. In doing so, we\nfollow the approach outlined in previous work by\nGart et al. (2017) and Hamilton et al. (2016a) to\nconsider different kinds of contexts (e.g., adjec-\ntives, verbs and emotion words) to learn and com-\npare distributional representations of target words.\n3 Method\nWe ﬁrst establish the diachronic validity of\nlanguage-based models through predictive evalu-\nations. We then use topic models and word em-\nbeddings as the quantitative lens through which to\nstudy the diachronic differences in the language\nof social media users, and linear methods to eas-\nily interpret the differences between standardized\ncoefﬁcients as diachronic differences in user trait\nprediction from language.\n3.1 Predictive validity\nWe test the predictive performance of language\nmodels trained on a year’s worth of social media\nposts from a subset of users who have provided\ntheir age and gender information.\n• We train language models on the age- and\ngender-labeled primary dataset and evalu-\nate their diachronic validity (Hamilton et al.,\n2016b), i.e. their predictive performance on\nsubsequently collected language samples.\n• We identify age groups which drift faster than\nothers by reporting predictive performance\non users, stratiﬁed by year of birth.\n3.2 Language insights about diachronic\ndifferences\nWe use language for the following insights into di-\nachronic drift:\n• Important changes: For each temporal co-\nhort, we identify the language features which\nhave the most drift in terms of recalibrated\ncoefﬁcients, in regression models trained in\n2011 vs. 2015. In doing so, we follow the\napproach described by Rieman et al. (2017)\nto compare the standardized coefﬁcients of\nthe age and gender models trained on the lan-\nguage samples from 2011 and 2015.\n• High drift concepts: We use word embed-\ndings to identify the semantic differences in\nthe connotations around common concepts,\nfor two sets of users who are a generation\napart. Following the framework proposed by\nGarg et al. (2017), we calculate semantic drift\nas the changes in the relative normalized dis-\ntance for the context words describing a set\nof target concepts.\nTarget concepts comprise a group of words rep-\nresenting a single idea (Garg et al., 2017), for in-\nstance, positive emotion, derived from the LIWC\npsycholinguistic dictionary (Pennebaker et al.,\n2007), and sexual orientation and gender expres-\nsion ( LGBTQ issues), based on a glossary on\nLGBTQ terms provided by the Human Rights\nCampaign 2. We use the relative norm distance to\nidentify the contextual words (mainly adjectives,\nadverbs and sentiment words) that are most differ-\nent across the two word embedding models. Reg-\nular expression matching for part-of-speech, sen-\ntiment and emotion words was based on LIWC\nand the NRC emotion lexicon (Mohammad et al.,\n2https://www.hrc.org/resources/glossary-of-terms\n197\n2013). Among a variety of distance metrics, eu-\nclidean distances provided the most interpretable\nresults.\n4 Datasets and Pre-processing\nPrimary data: Our primary dataset consists of\nthe Twitter posts of adults in the United States\nof America who were recruited by Qualtrics (a\ncrowdsourcing platform similar to Amazon Me-\nchanical Turk) for an online survey, and consented\nto share access to their Twitter posts. This data\nwas collected in a previous study by Preot ¸iuc-\nPietro et al. (2017) and is available online 3. We\nrestrict our analysis to tweets posted between Jan-\nuary 2011 and December 2015, by those users\nwho indicated English as a primary language, have\nwritten at least 10 posts in their posts in each\nyear, and have reported age and binary gender as\na part of the survey. This resulted in a dataset of\nN = 554 users, who wrote a mean of 265 and a\nmedian of 156 posts per year and over13.5 million\nwords collectively. The mean age of the popula-\ntion was 33.54 years. 59% of them self-identiﬁed\nas male.\nDecahose (Twitter 10%) dataset: For insights\nbased on word embeddings, we used the decahose\nsamples for the years 2011 and 2014 collected\nby the TrendMiner project (Preotiuc-Pietro et al.,\n2012), which comprises a 10% random sample\nof the real-time Twitter ﬁrehose using a realtime\nsampling algorithm. To match with our primary\ndata, we used bounding boxes to consider only\nthose tweets with geolocation information which\nwere posted in the United States. In this manner,\nwe obtained 130 and 179 million Twitter posts for\n2011 and 2014 respectively.\nPre-processing: In a dataset of 554 users, the ab-\nsolute vocabulary overlap may be low. By convert-\ning each users string of words into their probabilis-\ntic usage of 2000 topics, we expected to get more\nstable estimates than using word-based language\nmodels. We represent the language of each user as\na probabilistic distribution of 2000 topics derived\nusing Latent Dirichlet Allocation (LDA) with α\nset to 0.30 to favor fewer topics per document.\nThese topics are modeled as open-ended clusters\nof words from actual distributions in social media\nover approximately 18 million Facebook updates,\nand are provided as an open-sourced resource in\nthe DLATK python toolkit (Schwartz et al., 2017).\n3https://web.sas.upenn.edu/danielpr/resources/\nPredictive evaluation: We use Python’s sklearn\nlibrary to conduct a ten-fold cross-validation and\ntrain weighted linear regression models for age,\nand binary logistic regression models for gender,\non the LDA-derived features for users in nine\nfolds, and test on the users in the held out fold.\nWe use feature selection, elastic-net regulariza-\ntion, and randomized PCA to avoid over-ﬁtting.\nAlthough we tested other linguistic features such\nas n-grams, the best predictive performance was\nfor models trained on the topic features.\nWord embeddings: We separately train word\nembeddings on the language of the Twitter 10%\nsample from 2011, and the sample from 2014.\nWe use Google’s Tensorﬂow framework (Abadi\net al., 2016) to optimize the prediction of co-\noccurrence relationships using an approximate ob-\njective known as skip-gram with negative sam-\npling (Mikolov et al., 2013) with incremental ini-\ntialization and optimizing our embeddings with\na stochastic gradient descent. Embeddings were\ntrained using the top-50000 words by their average\nfrequency over the entire time period. A similar\nthreshold has also been applied in previous papers\n(Hamilton et al., 2016a,b). We experimented with\ndifferent window sizes and parameter settings, ﬁ-\nnally choosing a window size of 4, embeddings\nwith 1000 dimensions, and the negative sample\nprior α set to log(5) and the number of negative\nsamples set to 500.4\n5 Results\n5.1 Predictive performance\nIn Figure 1, we report performance error in pre-\ndicting age as the mean of(actual−predicted) in\norder to better understand the model bias towards\npredicting younger or older ages.\nWe observe that the age- and gender- predic-\ntive models by Sap et al. (2014) degrade in per-\nformance on language samples from more re-\ncent years. They have a lower mean error in age\nprediction and higher accuracy in gender predic-\ntion on the a language sample from 2011 as com-\npared to 2015; yet, our test sets are ostensibly\ndrawn from the same corpus as the original train-\ning data. This shows that even models trained\non large datasets show performance degradation\nif they are tested against newer language samples\nfor the same set of users. We observe the same\n4The trained word embedding models can be downloaded\nfrom http://www.wwbp.org/publications.html\n198\nFigure 1: Cross-year performance for predicting (a) age (re-\nported as MeanError = Ageactual−Agepredicted) and (b)\ngender (reported as Accuracy). The columns depict the train-\ning set for regression models: language samples posted in a\nparticular year. The rows depict the test sets. Deeper shades\nof blue reﬂect higher underestimation errors; deeper shades\nof red reﬂect higher overestimation errors. Deeper shades of\ngreen depict higher accuracy.\ntrends on in-sample models trained and tested on\nour primary data (see Figure 1). Age and gender\nmodels perform the best when tested on a sam-\nple from the same year, but age models degrade in\nperformance over time, with older models tending\nto over-predict the age on subsequent samples.\nNewer models tend to under-predict the age on\nolder samples of language. Taken together, these\ninsights suggest that the rate of change in age (1\nunit per year) is less than the rate of change in lan-\nguage use. Gender models demonstrate an approx-\nimately 7-12% drop in accuracy for subsequent or\nolder years.\nIn the next step, we attempt to understand the rate\nof change of language for social media users of\ndifferent ages, which show larger variance as com-\npared to gender. Figure 2 provides the results for\na language model trained on the language sample\nof 2011 and tested to predict age from a language\nsample from 2011 and the subsequent years. The\ncolumns depict the birth year for users in the test\nset. This ﬁgure provides some important insights:\n1. The ﬁrst row has the smallest mean errors,\nwhich is expected since it is an in-sample pre-\ndiction of a 2011-trained language model on\nitself.\n2. The largest mean errors are seen at the bottom\nleft, where the model is consistently under-\npredicting the age of older social media\nusers whose language usage has little change\nover 5 years.\n3. In the bottom-right, social media users born\nbetween 1992-1997 observe the highestover-\nestimation errors as they ‘sound’ older than\nthey are. Despite their annual increment in\nage, the model still overpredicts their age by\napproximately twice as many years.\n5.2 Insights about Diachronic Differences\nWe want to understand performance degradation\nin terms of the change in the associations of lin-\nguistic features associated with higher age or with\none of the genders. The age range in 2011 were\n[14,41] years and in 2015 were [ 18,45] years\nrespectively. To explore diachronic differences\nin topic usage across two age-matched popula-\ntions, we subset the population to the subjects\nto the age range of [ 18,41] years during 2011-\n2015 (N=429).\nImportant changes: In Table 1, we compare the\nstandardized coefﬁcients (p <.001) of the predic-\ntors in models trained on the language of the year\n2011 against those of 2015. We observe that a lot\nof the topics typically associated with older social\nmedia users in the2011 model, 5 such as swearing,\ntiredness and sleep, changed their age bias. On the\nother hand, topics popular among younger social\nmedia users – for instance, topics mentioning em-\nployable skills and meetings, percolated upward as\nthe early adopters of social media grew older. In\nthe case of gender, topics related to business meet-\nings, the government, computers, and money were\nno longer predictive of males, while topics associ-\nated with proms, relationships, and hairstyles were\nno longer predictive of females. 6\nAge β2011 β2015\nEmail communication:(send, email, message, contact)168.5 -53.7Accommodation(place, stay, found, move)162.2 -101.8Sleep(bed, lay, sleep, head, tired) 59.6 -88.5Swear(wtf, damn, sh**, wth, wrong, pissed)38.1 -46.0Tiredness(i’m, sick, tired, feeling, hearing)33.6 -98.3Hacking(virus, called, open, steal, worm, system)-99.6 253.5Software(computer, error, photoshop, server, website)-87.2 80.7Feeling(feeling, weird, awkward, strange, dunno)-70.5.0 23.2Meetings(meeting, conference, student, council, board )-44.0 38.6Skills(management, business, learning, research)-26.4 158.0Gender β2011∗ β2015∗Apple products(iphone, apple, ipad, mac, download)4.1 (0)Sports(win, lose, game, betting, streak, change)3.2 (0)Bills(pay, money, paid, job, rent) 2.8 (0)Government(government, freedom, country, democracy)2.8 (0)Prom(dress, prom, shopping, formal, homecoming)1.8 (0)Hairstyles(hair, blonde, dye, color, highlights)1.7 (0)Relationships(amazing, boyfriend, wonderful, absolutely)1.7 (0)Negative emotions(inside, deep, feel, heart, pain, empty)1.6 (0)\nTable 1: The features whose coefﬁcients had the biggest\nchange and ﬂipped sign when comparing the age and gen-\nder prediction models trained on 2011 language against those\ntrained on 2015 language. (0) depicts that the feature was no\nlonger signiﬁcant in the 2015 model. *:(X10−4)\nHigh drift concepts: We now illustrate di-\nachronic differences in terms of the changing con-\ntext around the same concept, in Table 2. We have\n5See Schwartz et al. (2013)\n6We also estimated feature importance through an abla-\ntion analysis, according to the difference made to the overall\nprediction (∑wixβi), which also yielded similar results.\n199\nFigure 2: Cross-year performance for predicting (a) age (reported as Mean Error = actualage− predictedage). The rows\nreﬂect the test sets: language samples posted in the same or different year. The columns reﬂect users stratiﬁed according to their\nyear of birth. Deeper shades of blue reﬂect higher underestimation errors; deeper shades of red reﬂect higher overestimation\nerrors.\nidentiﬁed the words which show the largest drift\nbetween 2011-2014, in terms of their association\nwith LGBTQ issues and positive emotion. We ob-\nserve that this method of comparing the relative\ndifferences in distances, proposed by Garg et al.\n(2017), is able to capture social attitudes towards\ngender issues, as well as the emerging trends in\nnetspeak. Speciﬁcally, in the discussions around\nLGBTQ issues in 2014, the words that emerge are\ncloser to the actual experiences of the group, with\nwords referring to ‘passing’ (a reference to trans-\nsexuals) and ‘coping’, as well as more positive\nemotion words (‘yayy’, ‘harmony’).\nConcept YearContext words\nLGBTQ issues2011strippers, conservative, pedophile,subjective, shocking2014coping, passed, balance, yayy, ﬁnally, harmony\nPositive emotion2011fagazy, bomb, totally, awesomeness, tight, ﬂy2014kickback, swag, winning,dontgiveafuck, bi*ch, thicka**\nTable 2: Context words for concepts in the language of Twit-\nter 2011 vs. 2014, selected among the words with the highest\nrelative norm difference in distances from the concepts in the\nﬁrst column, between the two sets of Twitter embeddings.\n6 Discussion\nTo summarize, our ﬁndings show that diachronic\ndifferences in language can be observed on so-\ncial media and their effect differs for social me-\ndia users of varying ages. In Figure 2, consider\nagain the users of the same age at different points\nof time. For instance, compare the errors for 22-\nyear old users in2011 (born in 1989) against those\nfor 22-year-old users in 2012 (born in 1990), and\nso on. The variance in error along these diagonals,\nare high in the right side of the table. This suggests\nthat in every subsequent year, the language of late-\nteens and early-twenties is more different from the\nlanguage of their contemporaries from the year be-\nfore. On the other hand, compare the errors for\n37-year-olds in 2011 (born in 1974) against those\nfor 37-year-olds in 2012 (born in 1975). The er-\nrors have a low variance along the diagonals in the\nleft of the Figure. Among social media users in\ntheir late thirties, the language of each cohort of\n35-year-olds changes little over the previous year.\nNext, consider the quantitative insights from Ta-\nble 1. The results suggest that over time, young\nusers from 2011 continued to use certain topics,\nwhile older users adopted newer trends. We found\nthat if we do not use age-matched samples in this\nexperiment, the coefﬁcients for other topics are\nalso ﬂipped, but this effect is noticeably dimin-\nished with an age-matched subset. This suggests\nthat indeed, a part of the language drift appeared\nbecause 1/5th of the population was shifting along\nthe temporal axis, which also associates their dis-\ntinct topical preferences with an older age group.\n7 Conclusion & Future Work\nThis study offers an empirical study of how gender\nand age classiﬁers degrade over time, a qualitative\nstudy of the features whose coefﬁcients change the\nmost, and concepts that drift in meaning over time.\nThe language of social media posts can be used\nto study semantic drift over short periods of time,\neven from a dataset of 554 social media users.\nThese methods can also ﬁnd application in the\nstudy of other linguistic phenomena such as pol-\nysemy (Hamilton et al., 2016b; Szymanski, 2017).\nHowever, there is a need to disentangle which dif-\nferences are due to the changing use of language\nfrom the ones due to changes in topics and trends\non social media.\nLanguage models degrade over time, but it not\nalways feasible to retrain models with new data. In\nfuture work, we plan to explore whether domain\nadaptation techniques can resolve diachronic per-\nformance differences, in addition to generalizing\nlanguage models to other platforms (Jaidka et al.,\n2018b) or scaling to measure communities (Rie-\nman et al., 2017).\n200\nReferences\nMart´ın Abadi, Paul Barham, Jianmin Chen, Zhifeng\nChen, Andy Davis, Jeffrey Dean, Matthieu Devin,\nSanjay Ghemawat, Geoffrey Irving, Michael Isard,\net al. 2016. Tensorﬂow: A system for large-scale\nmachine learning. In OSDI. volume 16, pages 265–\n283.\nLea Frermann and Mirella Lapata. 2016. A bayesian\nmodel of diachronic meaning change. Transac-\ntions of the Association for Computational Linguis-\ntics 4:31–45.\nNikhil Garg, Londa Schiebinger, Dan Jurafsky, and\nJames Zou. 2017. Word embeddings quantify 100\nyears of gender and ethnic stereotypes. arXiv\npreprint arXiv:1711.08412.\nWilliam L Hamilton, Jure Leskovec, and Dan Jurafsky.\n2016a. Cultural shift or linguistic drift? comparing\ntwo computational measures of semantic change. In\nProceedings of the Conference on Empirical Meth-\nods in Natural Language Processing. Conference on\nEmpirical Methods in Natural Language Process-\ning. NIH Public Access, volume 2016, page 2116.\nWilliam L Hamilton, Jure Leskovec, and Dan Jurafsky.\n2016b. Diachronic word embeddings reveal statisti-\ncal laws of semantic change. In Proceedings of the\n54th Annual Meeting of the Association for Compu-\ntational Linguistics (Volume 1: Long Papers). vol-\nume 1, pages 1489–1501.\nKokil Jaidka, Anneke Buffone, Salvatore Giorgi, Jo-\nhannes Eichstaedt, Masoud Rouhizadeh, and Lyle\nUngar. 2018a. Modeling and visualizing locus of\ncontrol with facebook language. In Proceedings of\nthe International AAAI Conference on Web and So-\ncial Media.\nKokil Jaidka, Sharath Chandra Guntuku, Anneke Buf-\nfone, H. Andrew Schwartz, and Lyle Ungar. 2018b.\nFacebook vs. twitter: Differences in self-disclosure\nand trait prediction. In Proceedings of the Interna-\ntional AAAI Conference on Web and Social Media.\nVivek Kulkarni, Rami Al-Rfou, Bryan Perozzi, and\nSteven Skiena. 2015. Statistically signiﬁcant detec-\ntion of linguistic change. In Proceedings of the 24th\nInternational Conference on World Wide Web. In-\nternational World Wide Web Conferences Steering\nCommittee, pages 625–635.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-\nrado, and Jeff Dean. 2013. Distributed representa-\ntions of words and phrases and their compositional-\nity. In Advances in neural information processing\nsystems. pages 3111–3119.\nSaif M Mohammad, Svetlana Kiritchenko, and Xiao-\ndan Zhu. 2013. Nrc-canada: Building the state-\nof-the-art in sentiment analysis of tweets. arXiv\npreprint arXiv:1308.6242.\nJames W Pennebaker, Roger J Booth, and Martha E\nFrancis. 2007. Linguistic inquiry and word count:\nLiwc [computer software]. Austin, TX: liwc. net.\nDaniel Preot ¸iuc-Pietro, Ye Liu, Daniel Hopkins, and\nLyle Ungar. 2017. Beyond binary labels: political\nideology prediction of twitter users. In Proceed-\nings of the 55th Annual Meeting of the Association\nfor Computational Linguistics (Volume 1: Long Pa-\npers). volume 1, pages 729–740.\nDaniel Preotiuc-Pietro, Sina Samangooei, Trevor\nCohn, Nicholas Gibbins, and Mahesan Niranjan.\n2012. Trendminer: An architecture for real time\nanalysis of social media text .\nDaniel Rieman, Kokil Jaidka, H Andrew Schwartz, and\nLyle Ungar. 2017. Domain adaptation from user-\nlevel facebook models to county-level twitter pre-\ndictions. In Proceedings of the Eighth International\nJoint Conference on Natural Language Processing\n(Volume 1: Long Papers). volume 1, pages 764–773.\nMaarten Sap, Gregory Park, Johannes Eichstaedt, Mar-\ngaret Kern, David Stillwell, Michal Kosinski, Lyle\nUngar, and Hansen Andrew Schwartz. 2014. Devel-\noping age and gender predictive lexica over social\nmedia. In Proceedings of the 2014 Conference on\nEmpirical Methods in Natural Language Processing\n(EMNLP). pages 1146–1151.\nH Andrew Schwartz, Johannes C Eichstaedt, Mar-\ngaret L Kern, Lukasz Dziurzynski, Stephanie M Ra-\nmones, Megha Agrawal, Achal Shah, Michal Kosin-\nski, David Stillwell, Martin EP Seligman, et al.\n2013. Personality, gender, and age in the language\nof social media: The open-vocabulary approach.\nPloS one8(9):e73791.\nH Andrew Schwartz, Salvatore Giorgi, Maarten Sap,\nPatrick Crutchley, Lyle Ungar, and Johannes Eich-\nstaedt. 2017. Dlatk: Differential language analysis\ntoolkit. In Proceedings of the 2017 Conference on\nEmpirical Methods in Natural Language Process-\ning: System Demonstrations. pages 55–60.\nTerrence Szymanski. 2017. Temporal word analogies:\nIdentifying lexical replacement with diachronic\nword embeddings. In Proceedings of the 55th An-\nnual Meeting of the Association for Computational\nLinguistics (Volume 2: Short Papers). volume 2,\npages 448–453.\nDerry Tanti Wijaya and Reyyan Yeniterzi. 2011. Un-\nderstanding semantic change of words over cen-\nturies. In Proceedings of the 2011 international\nworkshop on DETecting and Exploiting Cultural di-\nversiTy on the social web. ACM, pages 35–40.",
  "topic": "Profiling (computer programming)",
  "concepts": [
    {
      "name": "Profiling (computer programming)",
      "score": 0.7425728440284729
    },
    {
      "name": "Computer science",
      "score": 0.7382293939590454
    },
    {
      "name": "Social media",
      "score": 0.6958012580871582
    },
    {
      "name": "Natural language processing",
      "score": 0.5007288455963135
    },
    {
      "name": "Artificial intelligence",
      "score": 0.4500081539154053
    },
    {
      "name": "Affect (linguistics)",
      "score": 0.43350058794021606
    },
    {
      "name": "Data science",
      "score": 0.3340802788734436
    },
    {
      "name": "Linguistics",
      "score": 0.29650068283081055
    },
    {
      "name": "World Wide Web",
      "score": 0.23585611581802368
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Philosophy",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I79576946",
      "name": "University of Pennsylvania",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I1306409833",
      "name": "Adobe Systems (United States)",
      "country": "US"
    }
  ],
  "cited_by": 34
}