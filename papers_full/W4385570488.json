{
  "title": "John-Arthur at SemEval-2023 Task 4: Fine-Tuning Large Language Models for Arguments Classification",
  "url": "https://openalex.org/W4385570488",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2477178208",
      "name": "Georgios Balikas",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3122890974",
    "https://openalex.org/W2980282514",
    "https://openalex.org/W2101234009",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W4285295903",
    "https://openalex.org/W4385571883",
    "https://openalex.org/W4318908710"
  ],
  "abstract": "This paper presents the system submissions of the John-Arthur team to the SemEval Task 4 “ValueEval: Identification of Human Values behind Arguments”. The best system of the team was ranked 3rd and the overall rank of the team was 2nd (the first team had the two best systems). John-Arthur team models the ValueEval problem as a multi-class, multi-label text classification problem. The solutions leverage recently proposed large language models that are fine-tuned on the provided datasets. To boost the achieved performance we employ different best practises whose impact on the model performance we evaluate here. The code ispublicly available at github and the model onHuggingface hub.",
  "full_text": "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023), pages 1428–1432\nJuly 13-14, 2023 ©2023 Association for Computational Linguistics\nJohn-Arthur at SemEval-2023 Task 4: Fine-Tuning Large Language\nModels for Arguments Classification\nGeorgios Balikas\nSalesforce.com\ngeompalik@hotmail.com\nAbstract\nThis paper presents the system submissions of\nthe John-Arthur team to the SemEval Task 4\n“ValueEval: Identification of Human Values be-\nhind Arguments”. The best system of the team\nwas ranked 3rd and the overall rank of the team\nwas 2nd (the first team had the two best sys-\ntems). John-Arthur team models the ValueEval\nproblem as a multi-class, multi-label text clas-\nsification problem. The solutions leverage re-\ncently proposed large language models that are\nfine-tuned on the provided datasets. To boost\nthe achieved performance we employ differ-\nent best practises whose impact on the model\nperformance we evaluate here. The code is\npublicly available at github and the model on\nHuggingface hub.\n1 Introduction\nSemEval Task 4 “ValueEval: Identification of Hu-\nman Values behind Arguments” has as a goal to\nclassify a textual argument across one or more hu-\nman value categories (Kiesel et al., 2022, 2023).\nUnderstanding and identifying human values in ar-\nguments is a difficult task as they are implicit and\ntheir definitions are often vague. At the same time\nthey are studied in various domains like social stud-\nies and formal argumentation. As a result, having a\nsystem aiding to the task could have a measurable\nimpact on such studies.\nTo build an NLP system that classifies input text\ndata among human values we build on recent work\non large language models (LLMs). These models\nare capable of “understanding” English text and our\nhope is that they can be used to achieve satisfactory\nperformance on the task. The ValueEval input data\nconsist of three pieces of information:\n• The argument’s stance: which is “in favour”\nor “against”\n• The argument’s premise, and\n• The argument’s conclusion.\nOn the other hand, the prediction step is to rec-\nognize the human value for each input among 20\nvalues. This is a multi-class, multi-label classifi-\ncation problem as most inputs belong to several\nhuman values.\nIn our experiments we discovered that LLMs are\na suitable solution to the task. Properly encoding\nthe input information for the LLMs and fine-tuning\nthe models with the data provided by the organiz-\ners quickly improves the performance. Also, we\nfound that bigger models achieve better results if\ntuned properly. Adding data to the training set also\nhas a big impact on the obtained performance. We\ntried various ideas in the prototyping phase: from\ndifferent ways to encode the model input to vari-\nous architecture choices on how to use the model\noutput. Compared to more traditional machine\nmodeling lifecycle where a lot of effort is put on\nfeature engineering, we often realized that while\nsome feature engineering benefits the performance\nusing smaller models, its impact on larger model is\ndiminishing.\nThe best system of the John-Arthur team is cur-\nrently ranked at position 3 of the private leader-\nboard. In terms of teams, John-Arthur is ranked\n2nd, behind the Adam Smith team, who submit-\nted the 2 best performing systems. We make the\ncode that trains a model publicly available1 and the\nmodel of the best submission also available.2\n2 Background\nTask 4 of SemEval 2023 consists of a single track.\nTask submissions were handled using the TIRA\nplatform (Fröbe et al., 2023). The dataset of the\nchallenge is described in (Mirzakhmedova et al.,\n2023). An example input to be classified is as\nfollows:\n1https://github.com/balikasg/\nSemEval2023-Task4-John-Arthur\n2https://huggingface.co/balikasg/\nSemEval2023Task4\n1428\n• Stance: in favour of\n• Premise: We should ban human cloning as it\nwill only cause huge issues when you have a\nbunch of the same humans running around all\nacting the same.\n• Conclusion: We should ban human cloning.\n• Target: “Security: societal” (among 20\nclasses)\nThis is framed as a multi-label, multi-class text\nclassification problem. The evaluation measure the\norganizers chose is the macro-averaged F1 score.\n3 System Overview\nWe model the problem as a text classification prob-\nlem and we intend to use LLMs on the premise that\nthey can uncover and model the implicit semantics\nof the input data to be able to successfully predict\nthe human values.\n3.1 Input Encoding\nThe first decision we are faced with is on how to\nencode the input data. We employ the notion of a\ncross-encoder system where in its inputs different\ninformation pieces are joined using separators. For\nour use-case we encode the input as:\ninput = Stance + separator+\nPremise + separator + Conclusion\nwhere “+” refers to the text concatenation opera-\ntion. We experimented with different ways of this\nencoding. In particular, we concluded that using\nthe model’s separator token consistently performed\nbetter compared to other (new) separator tokens.\nWe also found that for the low-cardinality values\nof Stance (in favour of vs against) it is beneficial\nto use separate tokenizer symbols to model them.\nAs a result, we used ‘[Favour]‘ and ‘[Against]‘ to\nmodel them. This gave a small lift across the mod-\nels we tried. As a conclusion, the input encoding\nof the best performing model is for the example of\nSection 2 is “[Favour][SEP]We should ban .. the\nsame[SEP]We should ban human cloning[SEP]”\nwhere the actual text is lower-cased.\n3.2 Model Selection\nIn the preliminary phase of our solution develop-\nment we identified the families of Roberta (Liu\net al., 2019) and deberta (He et al., 2021) mod-\nels to be promising model architectures. As a re-\nsult, we iterated on these models and tried differ-\nent hyper-parameters on batch-size and learning\nrates to verify the convergence and how the model\nwould fit in a GPU. To fit the models in the Google\nColab GPUs we used a batch size of 16 and a learn-\ning rate of 2e −5 for the base models. Moving\nto smaller or bigger models, we used a rule of\nthumb and adjusted the batch size (dividing or mul-\ntiplying by 2) and doing the inverse operation for\nthe learning rate. Towards the submission dead-\nline of the competition, we rented a Google Cloud\nvirtual machine with a A100 GPU to be able to\nsubmit a run with the biggest deberta model avail-\nable: “microsoft/deberta-v2-xxlarge”3. The code\nheavily relies on the popular Huggingface Trans-\nformers python package (Wolf et al., 2019). Table\n?? shows the results of fine-tuning different model\narchitectures for 6 epochs with a batch size of 4, a\nlearning rate of 0.5e −05 where the training data\nare the “arguments-training.csv” the task organiz-\ners provided and the F 1 score is reported on the\n“‘arguments-validation.csv” dataset. From the Ta-\nble we observe that “microsoft/deberta-v2-xxlarge”\nclearly outperforms the rest of the models and this\nis why we ended up using it in the final submis-\nsions.\nWhile we experimented with several modeling\nchoices we found that in the xxlarge model using\nthe model’s output ([CLS] token) without any post-\nprocessing or extra hidden layer performed the best\naccording to the validation metrics. We particu-\nlarly experimented with what type of model output\nto use on the classification head and tried among\ndifferent choices between using:\n• the [CLS] token representation\n• mean or max or concatenation of mean and\nmax pooling of the the other token representa-\ntions\n• concatenate pooling of not only the last but\nthe last 3 output layers of the model with or\nwithout weights\nbut while some of these worked on small Roberta\nmodels, they did not on the xxlarge deberta model.4\n3https://huggingface.co/microsoft/\ndeberta-v2-xxlarge\n4A description of such techniques is avail-\nable at https://www.kaggle.com/code/rhtsingh/\nutilizing-transformer-representations-efficiently .\n1429\nTest set / Approach All\nSelf-direction: thought\nSelf-direction: action\nStimulation\nHedonism\nAchievement\nPower: dominance\nPower: resources\nFace\nSecurity: personal\nSecurity: societal\nTradition\nConformity: rules\nConformity: interpersonal\nHumility\nBenevolence: caring\nBenevolence: dependability\nUniversalism: concern\nUniversalism: nature\nUniversalism: tolerance\nUniversalism: objectivity\nMain\nBest per category .59 .61 .71 .39 .39 .66 .50 .57 .39 .80 .68 .65 .61 .69 .39 .60 .43 .78 .87 .46 .58\nBest approach .56 .57 .71 .32 .25 .66 .47 .53 .38 .76 .64 .63 .60 .65 .32 .57 .43 .73 .82 .46 .52\nBERT .42 .44 .55 .05 .20 .56 .29 .44 .13 .74 .59 .43 .47 .23 .07 .46 .14 .67 .71 .32 .33\n1-Baseline .26 .17 .40 .09 .03 .41 .13 .12 .12 .51 .40 .19 .31 .07 .09 .35 .19 .54 .17 .22 .46\nxxlarge deberta .53 .51 .69 .28 .20 .63 .43 .55 .37 .71 .58 .62 .55 .50 .17 .53 .41 .72 .87 .43 .57\nxxlarge deberta (full data) .55 .56 .70 .27 .25 .65 .50 .52 .39 .76 .60 .63 .60 .69 .24 .55 .41 .74 .86 .44 .58\nTable 1: Achieved F1-score of team John-Arthur per test dataset, from macro-precision and macro-recall (All) and\nfor each of the 20 value categories. Approaches marked with * were not part of the official evaluation. Approaches\nin gray are shown for comparison: an ensemble using the best participant approach for each individual category; the\nbest participant approach; and the organizer’s BERT and 1-Baseline.\nModel Name F 1 score\nmicrosoft/deberta-v3-small 0.3563\nmicrosoft/deberta-v3-base 0.3776\nroberta-base 0.4260\nmicrosoft/deberta-v3-large 0.4585\nroberta-large 0.501\nmicrosoft/deberta-v2-xlarge 0.5178\nmicrosoft/deberta-v2-xxlarge 0.5268\nTable 2: Macro-F1 score on the validation set depend-\ning on the model architecture that is fine-tuned. All\nmodels are fine-tuned for 6 epochs with a batch size of\n4, a learning rate of 0.5 e −05. For the predictions, a\nthreshold of 0.2 is used.\nTo cope with the multi-label aspect of the prob-\nlem we used a binary cross-entropy loss that is\napplied on the model logits.5\n3.3 Model Output Post-processing\nWe pass the model outputs from a sigmoid function\nis order to get float values in [0, 1] to resemble\nclass probabilities. The task, in the predictions\nfile requires a binary prediction for each human\nvalue, where 1 indicates that the input example\nbelongs to this human value. To go from a float\nvalue p ∈[0, 1] to a binary output a threshold is\nrequired. Typically one chooses a threshold of\n0.5 but this is a parameter that can be tuned on\n5More information on how to do multi-\nlabel classification with Huggingface on this\nthread https://discuss.huggingface.co/t/\nfine-tune-for-multiclass-or-multilabel-multiclass/\n4035?page=2\nthe validation set. In our submissions we used a\nthreshold of 0.20. The macro-averaged F1 metric\nwas implemented as a callback on the Trainer\nclass using scikit-learn (Pedregosa et al., 2011).\nWhile we chose to use a global threshold i.e.,\na single value across all classes, for the submis-\nsions, the is some room for improvement. One can\ncome up with a strategy where each class has a\ndifferent threshold that is learned on the validation\ndata. In fact, in early experiments we validated\nthe effectiveness of this approach as a promising\npost-processing trick, but in the end we did not\nimplement it. We leave this as future work.\n4 Experimental Setup\nThe organizers released several data splits. The\nmain dataset comprised 3 splits (train, test, val-\nidation) that consist of 5,393, 1,897 and 1,576\ndata points respectively. Also, 100 more labeled\ndata points were released as “arguments-validation-\nzhihu.tsv” from related work. In early iterations we\nfound that the model greatly improved with more\ntraining data. We discovered this by observing\na lift across all model architectures when adding\nas few as 100 extra data points in the training data\n(“arguments-validation-zhihu.tsv”) which were few\ncompared to the 5,393 data points of the training\ndataset.\nMotivated by this observation that our systems\nbenefits a lot from extra training data, we de-\ncided to submit two runs with the same modeling\n1430\nTeam Name Top-Score\nAdam-Smith 0.561\nJohn-Arthur 0.553\nTheodor-winger 0.538\nmao-zedong 0.533\nconfucius 0.531\nBERT 0.420\nTable 3: Top-5 teams of SemEval 2022 Task 4. BERT\nwas the strong baseline provided by the Task organiz-\ners. The best performing system beat this baseline by a\nvery large margin, for example the John-Arthur system\nachieves a score of 13.2 absolute points more (0.553 vs\n0.42).\nand training configurations. The first run uses as\ntraining data the training data and the “arguments-\nvalidation-zhihu.tsv” dataset while the second run\nuses as training data the original training and vali-\ndation datasets (5,393 + 1,897 data points) and as\nvalidation the 100 points of “arguments-validation-\nzhihu.tsv”. We opted for having a validation set\nbecause we wanted to adhere to best neural network\nmodeling practises and monitor the evaluation loss\nand the evaluation metrics to ensure that the model\ntrains properly. We leave using the concatenation of\nall available data for training as well as performing\nsome form of data augmentation or distant supervi-\nsion using unlabeled data as promising directions\nof future work.\n5 Results\nTable 1 summarizes the results of the 2 runs. Re-\ncall that the main difference between these runs is\nthat the second includes more training data and\na smaller evaluation dataset. There are several\nclasses where the systems of John-Arthur achieved\nthe best performance obtained in the task. We did\nnot perform further error analysis to evaluate how\nmuch these results can be improved using, for ex-\nample, a different threshold per class or even dif-\nferent models per class. We leave this for future\nwork.\nTable 3 shows the best system scores of the top-\n5 team. John-Arthur team is ranked 2nd which\nspeaks to the high performance that LLMs can\nachieve when carefully tuned on text analysis tasks.\nWhat is more, these top-ranked system beat the\nstring baseline (BERT) provided by the competi-\ntion organizers by a very large margin (often >10\nabsolute points of the macro-averaged F1 measure)\nwhich we believe is a promising and valuable out-\ncome for the study of human values.\n6 Conclusion\nIn this paper we presented the submission of the\nJohn-Arthur team in Task 4 of SemEval 2023. The\nbest submission of the team is ranked in position 3\nof the leaderboard while the team itself is ranked\n2nd. The core of the solution is an deberta-v2-\nxxlarge fine-tuned model with some pre-processing\nand post-processing custom operations. The code\nof the best performing system of the team are open-\nsourced with the hope that it can benefit the com-\nmunity6.\n7 Acknowledgments\nWe would like to thank the machine learning com-\nmunity and Huggingface for developing the amaz-\ning open-source “transformers” library that pow-\nered our systems. Our systems also benefit from the\nnumerous system discussions and code resources in\nKaggle, and notably Jeremy Howard’s \"Iterate Like\na Grandmaster\" notebook that inspired our solution,\nwho we would like to thank for making available\nsuch high quality content. Last, we would like to\nthank Google Colab for making GPU resources\navailable for free in a notebooking environment.\nReferences\nMaik Fröbe, Matti Wiegmann, Nikolay Kolyada, Bas-\ntian Grahm, Theresa Elstner, Frank Loebe, Matthias\nHagen, Benno Stein, and Martin Potthast. 2023. Con-\ntinuous Integration for Reproducible Shared Tasks\nwith TIRA.io. In Advances in Information Retrieval.\n45th European Conference on IR Research (ECIR\n2023), Lecture Notes in Computer Science, Berlin\nHeidelberg New York. Springer.\nPengcheng He, Xiaodong Liu, Jianfeng Gao, and\nWeizhu Chen. 2021. Deberta: Decoding-enhanced\nbert with disentangled attention. In International\nConference on Learning Representations.\nJohannes Kiesel, Milad Alshomary, Nicolas Handke,\nXiaoni Cai, Henning Wachsmuth, and Benno Stein.\n2022. Identifying the Human Values behind Argu-\nments. In 60th Annual Meeting of the Association for\nComputational Linguistics (ACL 2022), pages 4459–\n4471. Association for Computational Linguistics.\nJohannes Kiesel, Milad Alshomary, Nailia Mirzakhme-\ndova, Maximilian Heinrich, Nicolas Handke, Hen-\nning Wachsmuth, and Benno Stein. 2023. Semeval-\n2023 task 4: Valueeval: Identification of human\nvalues behind arguments. In Proceedings of the\n17th International Workshop on Semantic Evaluation,\n6https://github.com/balikasg/\nSemEval2023-Task4-John-Arthur\n1431\nToronto, Canada. Association for Computational Lin-\nguistics.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoberta: A robustly optimized BERT pretraining\napproach. CoRR, abs/1907.11692.\nNailia Mirzakhmedova, Johannes Kiesel, Milad Al-\nshomary, Maximilian Heinrich, Nicolas Handke, Xi-\naoni Cai, Barriere Valentin, Doratossadat Dastgheib,\nOmid Ghahroodi, Mohammad Ali Sadraei, Ehsaned-\ndin Asgari, Lea Kawaletz, Henning Wachsmuth, and\nBenno Stein. 2023. The Touché23-ValueEval Dataset\nfor Identifying Human Values behind Arguments.\nCoRR, abs/2301.13771.\nF. Pedregosa, G. Varoquaux, A. Gramfort, V . Michel,\nB. Thirion, O. Grisel, M. Blondel, P. Prettenhofer,\nR. Weiss, V . Dubourg, J. Vanderplas, A. Passos,\nD. Cournapeau, M. Brucher, M. Perrot, and E. Duch-\nesnay. 2011. Scikit-learn: Machine learning in\nPython. Journal of Machine Learning Research,\n12:2825–2830.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz,\net al. 2019. Huggingface’s transformers: State-of-\nthe-art natural language processing. arXiv preprint\narXiv:1910.03771.\n1432",
  "topic": "SemEval",
  "concepts": [
    {
      "name": "SemEval",
      "score": 0.9146634936332703
    },
    {
      "name": "Leverage (statistics)",
      "score": 0.8755578994750977
    },
    {
      "name": "Computer science",
      "score": 0.8041326403617859
    },
    {
      "name": "Task (project management)",
      "score": 0.6446468830108643
    },
    {
      "name": "Language model",
      "score": 0.6185158491134644
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5669223070144653
    },
    {
      "name": "Natural language processing",
      "score": 0.5196182131767273
    },
    {
      "name": "Code (set theory)",
      "score": 0.5124679207801819
    },
    {
      "name": "Rank (graph theory)",
      "score": 0.512346625328064
    },
    {
      "name": "Identification (biology)",
      "score": 0.509231686592102
    },
    {
      "name": "Class (philosophy)",
      "score": 0.469326913356781
    },
    {
      "name": "Language identification",
      "score": 0.4314597249031067
    },
    {
      "name": "Machine learning",
      "score": 0.3953596353530884
    },
    {
      "name": "Natural language",
      "score": 0.2647658586502075
    },
    {
      "name": "Programming language",
      "score": 0.14690905809402466
    },
    {
      "name": "Management",
      "score": 0.11552432179450989
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Economics",
      "score": 0.0
    },
    {
      "name": "Combinatorics",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    },
    {
      "name": "Set (abstract data type)",
      "score": 0.0
    }
  ],
  "institutions": []
}