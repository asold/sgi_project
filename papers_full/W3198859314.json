{
  "title": "ReGen: Reinforcement Learning for Text and Knowledge Base Generation using Pretrained Language Models",
  "url": "https://openalex.org/W3198859314",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A4361058491",
      "name": "Pierre Dognin",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2768377415",
      "name": "Inkit Padhi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2132303832",
      "name": "Igor Melnyk",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2211214061",
      "name": "Payel Das",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W3029256614",
    "https://openalex.org/W3098824823",
    "https://openalex.org/W2133512280",
    "https://openalex.org/W2176263492",
    "https://openalex.org/W4229543565",
    "https://openalex.org/W2612675303",
    "https://openalex.org/W3034999214",
    "https://openalex.org/W2900677334",
    "https://openalex.org/W4324258509",
    "https://openalex.org/W3034080136",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W3112707533",
    "https://openalex.org/W4288089799",
    "https://openalex.org/W2758950307",
    "https://openalex.org/W3171434230",
    "https://openalex.org/W2119717200",
    "https://openalex.org/W2994900646",
    "https://openalex.org/W41554520",
    "https://openalex.org/W4298134534",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2967827612",
    "https://openalex.org/W3082274269",
    "https://openalex.org/W3105008414",
    "https://openalex.org/W2121863487",
    "https://openalex.org/W3012648429",
    "https://openalex.org/W2963248296",
    "https://openalex.org/W2963167310",
    "https://openalex.org/W2982399380",
    "https://openalex.org/W2963084599",
    "https://openalex.org/W3042876905",
    "https://openalex.org/W3131933120",
    "https://openalex.org/W2964352247"
  ],
  "abstract": "Automatic construction of relevant Knowledge Bases (KBs) from text, and generation of semantically meaningful text from KBs are both long-standing goals in Machine Learning. In this paper, we present ReGen, a bidirectional generation of text and graph leveraging Reinforcement Learning to improve performance. Graph linearization enables us to re-frame both tasks as a sequence to sequence generation problem regardless of the generative direction, which in turn allows the use of Reinforcement Learning for sequence training where the model itself is employed as its own critic leading to Self-Critical Sequence Training (SCST). We present an extensive investigation demonstrating that the use of RL via SCST benefits graph and text generation on WebNLG+ 2020 and TekGen datasets. Our system provides state-of-the-art results on WebNLG+ 2020 by significantly improving upon published results from the WebNLG 2020+ Challenge for both text-to-graph and graph-to-text generation tasks. More details at https://github.com/IBM/regen.",
  "full_text": "Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1084–1099\nNovember 7–11, 2021.c⃝2021 Association for Computational Linguistics\n1084\nReGen: Reinforcement Learning for Text and Knowledge Base\nGeneration using Pretrained Language Models\nPierre L. Dognin\nIBM Research\npdognin@us.ibm.com\nInkit Padhi\nIBM Research\ninkpad@ibm.com\nIgor Melnyk\nIBM Research\nigor.melnyk@ibm.com\nPayel Das\nIBM Research\ndaspa@us.ibm.com\nAbstract\nAutomatic construction of relevant Knowl-\nedge Bases (KBs) from text, and generation\nof semantically meaningful text from KBs are\nboth long-standing goals in Machine Learning.\nIn this paper, we present ReGen, a bidirec-\ntional generation of text and graph leveraging\nReinforcement Learning (RL) to improve per-\nformance. Graph linearization enables us to\nre-frame both tasks as a sequence to sequence\ngeneration problem regardless of the genera-\ntive direction, which in turn allows the use\nof Reinforcement Learning for sequence train-\ning where the model itself is employed as its\nown critic leading to Self-Critical Sequence\nTraining (SCST). We present an extensive in-\nvestigation demonstrating that the use of RL\nvia SCST beneﬁts graph and text generation\non WebNLG+ 2020 and T EKGEN datasets.\nOur system provides state-of-the-art results\non WebNLG+ 2020 by signiﬁcantly improv-\ning upon published results from the WebNLG\n2020+ Challenge for both text-to-graph and\ngraph-to-text generation tasks. More details in\nhttps://github.com/IBM/regen.\n1 Introduction\nGraph representation of knowledge is a power-\nful tool to capture real-world information where\ncomplex relationships between node entities can\nbe efﬁciently encoded. Automatic generation of\nKnowledge Bases (KBs) from free-form text and its\ncounterpart of generating semantically relevant text\nfrom KBs are both active and challenging research\ntopics.\nRecently, there has been an increased interest\nin leveraging Pretrained Language Models (PLMs)\nto improve performance for text generation from\ngraph, or graph-to-text (G2T) task (Ribeiro et al.,\n2020). Indeed, large PLMs like T5 (Raffel et al.,\n2020) and BART (Lewis et al., 2020) that have been\npretrained on vast amount of diverse and variedly\nstructured data, are particularly good candidates\nfor generating natural looking text from graph data.\nBART- and T5-related models have been em-\nployed by top performers in public challenges such\nas the WebNLG+ 2020 Challenge (Castro Ferreira\net al., 2020b) where both graph-to-text and text-\nto-graph (T2G) tasks are offered, under the names\nRDF-to-Text and Text-to-RDF (semantic parsing)\nrespectively; RDF stands for Resource Descrip-\ntion Framework, a standard for describing web re-\nsources. One can notice that more teams entered\nthe competition for the G2T task than for T2G as\nthe latter is a much harder task. Best models gen-\nerally use PLMs and ﬁne-tune them for the target\nmodality at hand (either graph or text). This is pos-\nsible by re-framing the T2G and G2T generations\nas a sequence to sequence (Seq2Seq) generation\nproblem, which suits ﬁne-tuning PLMs well. One\ncan therefore hope to leverage the large pretraining\nof PLMs to improve the overall generation quality.\nThe Seq2Seq formulation requires any input\ngraph to be linearized as a sequence, which is not\nunique. This creates an opportunity for data aug-\nmentation where multiple linearizations are pro-\nvided to the model at training time so the model\nlearns the content represented by the graph, not the\norder of its sequential representation.\nIn this work, we are interested in leveraging the\npower of PLMs for both G2T and T2G generation\ntasks, and will demonstrate the strength of our ap-\nproach by improving upon the best results of the\nWebNLG+ 2020 Challenge (rev 3.0) as reported\nby Castro Ferreira et al. (2020a) for both T2G (Se-\nmantic Parsing) and G2T (Data-to-Text) tasks. We\nwill also present results for the TEKGEN Corpus\n(Agarwal et al., 2021) to show performance on a\ndifferent, much larger dataset. To illustrate the task\nof generation, Fig. 1 provides examples of G2T\nand T2G outputs obtained using the proposed gen-\neration framework. The ﬁrst two sentences of the\nabstract of this paper were used as input for T2G\nusing our best model. The model generates a graph\nfrom the input text by simultaneously extracting\n1085\nFigure 1: Actual examples of generation for Text-to-Graph and Graph-to-Text tasks using our best RL models. The\nﬁrst two sentences of the abstract were processed through our best models. First, a graph was created capturing\nthe facts from the input sentences. Then, this graph was used as input to generate text. Despite a strong domain\nmismatch between input data and models, the generated paragraph is capturing most of the original sentences\ncontent. Both models were trained using RL, speciﬁcally Self-Critical Sequence Training (SCST).\nrelevant nodes and linking them coherently. For\nthe G2T task, another model starts from the gen-\nerated graph and generates semantically relevant\ntext from it. As one can appreciate, the ﬁnal text\nis quite readable and captures most facts from the\noriginal abstract sentences despite a strong domain\nmismatch between input data and training data,\nwhich both models were built on.\nSince both T2G and G2T generative tasks can\nbe formulated as a Seq2Seq problem, we propose\nto use Reinforcement Learning (RL) as part of the\nPLMs ﬁne-tuning on the target domain data. For\nboth G2T and T2G tasks, a differentiable func-\ntion such as the cross-entropy (CE) loss function\nis often used, since minimizing it results in max-\nimizing the probability of generating the correct\ntoken/word. However, when it comes to evaluat-\ning a model’s performance, benchmarks often use\nBLEU (Pa Pa Aung et al., 2020), METEOR (Lavie\nand Agarwal, 2007), and chrF++ (Popovi´c, 2017)\nfor G2T, or simply F1, Precision, and Recall scores\nfor T2G, none of which being differentiable. Dur-\ning training, one hopes that by minimizing the CE\nloss, the model will tend towards better prediction\nof the target tokens, hence improving on evaluation\nmetrics as a beneﬁcial by-product. Thankfully, RL\nprovides a framework where we can update our\nmodel parameters so to improve evaluation metrics\ndirectly. Mixed Incremental Cross-Entropy Rein-\nforce from Ranzato et al. (2016) introduced using\nREINFORCE (Williams, 1992) for sequence train-\ning. We propose to use one of its variant known as\nSelf-Critical Sequence Training (SCST) (Rennie\net al., 2017) for both T2G and G2T training.\nIn summary, our main contributions are:\n•We propose to use RL-based sequence training,\nspeciﬁcally SCST, for both G2T and T2G tasks.\nThis is the ﬁrst time that RL based training is pro-\nposed to the bi-directional generation of text and\ngraph. To the best of our knowledge, the present\nwork is the ﬁrst time it is introduced for a T2G task.\n•We demonstrate that our approach provides better\nperformance than the best systems reported for the\nWebNLG 2020+ Challenge.\n•We provide a thorough investigation of SCST-\nbased training for both T2G and G2T tasks, includ-\ning best rewards combination.\n•We constructed subject and relation-object bound-\naries from TEKGEN sentence-triples pairs and\nshowed performance of our approach for both T2G\nand G2T tasks.\n•We adapted the large-scale TEKGEN corpus\n(Agarwal et al., 2021) for T2G and G2T tasks and\nconﬁrmed the beneﬁt of SCST-based ﬁne-tuning\napproach over CE-trained baselines.\n2 Related work\nIn the WebNLG+ 2020 Challenge, most top per-\nforming models relied on ﬁne-tuning of PLMs. In-\nterestingly, all four top teams in this Challenge\nproposed quite different approaches while leverag-\ning PLMs. 1st place Amazon AI (Guo et al., 2020a)\npipelined a relational graph convolutional network\n(R-GCN) and a T5 PLM with some canonicaliza-\ntion rules. 2 nd place OSU Neural NLG (Li et al.,\n2020), the closest to our approach in spirit, used T5\nand mBART PLMs to ﬁne-tune after special data\npreprocessing. 3 rd place FBConvAI (Yang et al.,\n2020) used BART PLM and multiple strategies to\nmodel input RDFs. 4 th place bt5 employed a T5\nPLM trained in a bi-lingual approach on English\nand Russian, even using WMT English/Russian\nparallel corpus.\nRecently, Dognin et al. (2020); Guo et al. (2020b,\n2021) proposed models trained to generate in both\nT2G and G2T directions, with consistency cycles\ncreated to enable the use of unsupervised datasets.\n1086\nIn contrast, our approach of ﬁne-tuning a T5 PLM\nis fully supervised but can produce either the spe-\ncialized models for T2G and G2T tasks alone, or\na hybrid model that can handle both T/G inputs\nsimultaneously to generate the corresponding trans-\nlated G/T outputs.\nNote that in contrast to many WebNLG+ 2020\nChallenge participants, e.g. Li et al. (2020), no pre-\nprocessing of the data is performed for text, while\nfor graph triples, we add tokens to mark subject,\npredicate, and object positions in their linearized\nsequence representation. Moreover, data augmenta-\ntion is performed by allowing random shufﬂing of\ntriples order in graph linearization to avoid a model\nto learn the exact order of triples, especially for the\nT2G task.\nWhile the use of RL training in PLM has been\nexplored in many works, the approach of Chen\net al. (2020) is closest to ours. However, their\nwork focuses on the improved text generation in\nthe context of natural question generation, while\nin our algorithm we use it for graph-to-text and\ntext-to-graph generations.\n3 Models\nModels are trained on a dataset Dcomposed of\na set of (xT,xG)i samples, where superscript i\ndenotes the i-th sample in D, xT is made of\ntext (one or more sentences), and xG is a cor-\nresponding graph represented as a list of triples\nxG = [(s1,p1,o1),..., (sK,pK,oK)], where the\nk-th triple is composed of a subject sk, predicate\n(relationship) pk, and object ok. For G2T, the\nmodel is given xG as input and must generate ˆxT. A\ncross-entropy loss is computed as an expectation:\nLT\nCE = E\nxT ∼D\n[\nlog pG2T\nθ (xT)\n]\n, (1)\nwhere pG2T\nθ (xT) is the distribution of the generated\nsequence ˆxT = TG2T(xG), TG2T(.) being the trans-\nformation from graph to text. Our model is param-\neterized by θ, and xT is effectively sampled from\nthe marginal distribution of text samples from D.\nˆxT = [ ˆw1, ˆw2,..., ˆwT] is a sequence of generated\ntokens/words. Similarly, for training a T2G model,\nthe cross-entropy loss used in training is simply\nLG\nCE = E\nxG ∼D\n[\nlog pT2G\nθ (xG)\n]\n, (2)\nwhere pT2G\nθ (xG) is the distribution of the generated\ngraph ˆxG = TT2G(xT), TT2G(.) being the transfor-\nmation from text to graph, and where xG is drawn\nfrom the marginal distribution of graph samples\nfrom D.\nIn both Eq. (1) and Eq. (2), xG must be ex-\npressed as a sequence of tokens tj such that\na list of triples xG turns into a list of tokens\n[t1,t2,··· ,tM]. This is simply done by adding\ntokens marking the subject, predicate, and ob-\nject boundaries in the sequence such that each\ntriple (sk,pk,ok) is turned into a sequence such as\n[<S>,ws\n1,<P>,wp\n1,wp\n2,<O>,wo\n1,wo\n2,wo\n3], assum-\ning our subject is made of 1 token, our predicate of\n2 tokens, and our object of 3 tokens in this example.\n<S>,< P>, and <O> are just special marker tokens\nto help the model know where subject, predicate\nand objects are located in the sequence.\nWe start from a pretrained encoder-decoder M\nmodel that we ﬁne-tune on either T2G to get MT,\nor G2T task to get MG. We also propose a third\nkind of model MT+G to be ﬁne-tuned on both T2G\nand G2T samples, i.e. the model will learn to gen-\nerate in any direction, by supplying an input sam-\nple x= [xT; xG]⊤and corresponding target for it.\nInput from each modality is preﬁxed by a task spe-\nciﬁc string to distinguish transfer directions (\"Text\nto Graph:\" for xT and \"Graph to Text:\" for xG).\nFor MT+G models, the cross-entropy loss is sim-\nilarly deﬁned as for Eq. (1) and Eq. (2) such that\nLT+G\nCE = E\nx∼D\n[ log pθ(x)]. All models are shown\nin Fig. 2. By convention, we refer to models in this\npaper by their input modality T, G, or T+G.\n3.1 Reinforcement Learning\nSequence generation can be seen as an agent mak-\ning sequential decisions of picking words from\na given vocabulary. The agent reacts to its envi-\nronment by accounting for past predictions and\ngetting rewarded along the way, while its state\nis deﬁned by the partial sequence generated so\nfar. This interpretation enables the reformulation\nof Seq2Seq generation within the Reinforcement\nLearning (RL) framework (Sutton and Barto, 2018;\nSilver, 2015). More precisely, a sequence gen-\neration task can be recast as a Markov Decision\nProcess (MDP) where the agent behavior follows a\npolicy π(at|st). Action at corresponds to picking\na particular word wt at time tfrom a vocabulary\nV, conditioned on state st expressed as the par-\ntial sequence generation st = ˆx1:t = [ ˆw1,..., ˆwt],\nthat is sequence of words/tokens already picked.\nπ(at|st) is a stochastic policy that deﬁnes a proba-\nbility distribution of at. Once the action at is taken,\n1087\nEncoderxt Decoder xtg\nLce(xtg, xg)\nLrl(xtg, xg)\nMt\nEncoderxg Decoder xgt\nLce(xgt, xt)\nLrl(xgt, xt)\nMg\nSpecialized modelsMt, Mg\n[\nxt\nxg\n]\nEncoder Decoder\n[\nxtg\nxgt\n] Lce\n([xtg\nxgt\n]\n,\n[xg\nxt\n])\nLrl\n([xtg\nxgt\n]\n,\n[xg\nxt\n])\nHybrid modelMt+g\nNotations:\nx.t: text\nx.g: graph\nFigure 2: Specialized and hybrid models rely on the same losses for ﬁne-tuning. However, specialized models are\ndedicated to a particular generation task while hybrid models can handle both generation directions.\nthe agent receives a reward rt = r(st,at) before\nit transitions to the next state st+1. A sequence\nof actions a1:T = [a1,...,a T] is selected until the\nend of generation is reached. The agent aims at\nmaximizing the expectation of cumulative reward\nJ(π) =Eτ\n[ T∑\nt=1\nγtrt\n]\n= Eτ [R(τ)] (3)\nwhere γis a discounting factor used to control the\nhorizon of the cumulative reward, γ ∈[0,1]. The\nexpectation is taken over trajectories τ, sequences\nmade of {s1,a1,r1,...,s T,aT,rT}, where atwas\nchosen from policy π(at|st). RL provides both\non-policy and off-policy approaches to maximize\nJ(π) in Eq. (3). We are particularly interested in\non-policy techniques that rely on data samples gen-\nerated from the model to train, especially since our\nmodels start from large ﬁne-tuned PLMs that can\nalready generate good samples. This helps avoid\nthe common drawback of on-policy techniques\nof generating poor samples at ﬁrst when trained\nfrom scratch. These policy-based (Williams, 1992;\nZaremba and Sutskever, 2016) and actor-critic\nbased techniques (Bahdanau et al., 2017; Rennie\net al., 2017) have been studied for text generation\nand often update the underlying model with policy\ngradient (Ranzato et al., 2016; Li et al., 2016; Tan\net al., 2019; Paulus et al., 2017). Policy-based meth-\nods focus on a parameterized policy πθ where θis\noptimized to maximizeJ(πθ). The policy πθ(at|st)\nis the PLM generative model pθ, CE ﬁne-tuned as\ndescribed at the beginning of Section 3.\nREINFORCE, presented by Williams (1992), al-\nlows the optimization of a model’s parameters θ\nby maximizing the expected value of the word-\nbased reward Rw(ˆxT) of generated sequence ˆxT =\n[ ˆw1,..., ˆwT]. For notation convenience, note that\nRw(ˆxT) = R(τ) since we are now dealing with\nsequence of words/tokens ˆxT selected by the ac-\ntions in trajectory τ. We will also use the R(ˆxT)\nnotation for simplicity. In order to match common\nDeep Learning conventions, we can minimize a\nloss expressed as the negative value of the expected\ncumulative reward:\nLRL =\n∑\n[ ˆw1,...,ˆwT]\npθ( ˆw1,..., ˆwT)Rw( ˆw1,..., ˆwT)\n= E[ ˆw1,...,ˆwT]∼pθRw( ˆw1,..., ˆwT),\n= EˆxT ∼pθRw(ˆxT). (4)\nRw(ˆxT) is the reward for the generated text which\nis often associated with non-differentiable metrics\nsuch as BLEU, METEOR, chrF, etc. Note that in\nsequence generation, these metrics-based rewards\nare available only once a whole sequence is gen-\nerated, trading sparsity/delay of reward for quality\n(i.e. we use the full sequence reward, not an esti-\nmation of partial future reward). We circumvent\nthe non-differentiability issue by using the REIN-\nFORCE policy gradient method:\n∇θLRL ∝ (R(ˆxT) b) ∇θlog pθ(ˆxT), (5)\nwhere bis a baseline used to reduce the variance of\nour gradient estimate. bcan be any function, even\na random variable, as long as it is independent of\nthe actions taken to generate ˆxT, as described in\nChapter 13.4 from Sutton and Barto (2018). In Self-\nCritical Sequence Training (SCST) (Rennie et al.,\n2017), bis chosen to be the reward ofx∗\nT, the output\ngenerated by the model by greedy max generation,\nhence the model serving as its own critic:\n∇θLSCST ∝ (R(ˆxT) R(x∗\nT)) ∇θlog pθ(ˆxT), (6)\nwhere ˆxT is sampled from our model and x∗\nT is gen-\nerated by greedy max. An interesting property of\nthe baseline is that if R(ˆxT) >R(x∗\nT), sampled ˆxT\nhas higher reward than x∗\nT, then the model is up-\ndated to reinforce the choices made by this genera-\ntion. In the opposite case where R(ˆxT) < R(x∗\nT),\n1088\nthe model update will take the negative gradient to\nsubdue such generation. When R(ˆxT) = R(x∗\nT),\nno update is performed on the model since the gra-\ndient is effectively zeroed out, regardless of the\nindividual values R(ˆxT) and R(x∗\nT). This happens\nwhen ˆxT and x∗\nT are identical (greedy-max and sam-\npled sequences are the same). In that case the sam-\nple is lost for RL as no update to the model will\nresult from this sample. Basically, REINFORCE is\na Monte Carlo method of learning where a gradient\nupdate is applied in the direction decided by how\nR(ˆxT) compares to baseline b, the role of bbeing to\nreduce the variance of the gradient estimate. Varia-\ntions around REINFORCE exist on how to apply\nthe gradients, such as MIXER from Ranzato et al.\n(2016), or on how to evaluate the baseline (Luo,\n2020) to minimize the gradient variance.\nIn our training, PLMs are ﬁrst ﬁne-tuned using\nLCE loss. Once they reach a good generation qual-\nity, the training is switched to RL ﬁne-tuning by\nminimizing LSCST .\n4 Experimental Setup\nIn this Section, we present the experimental setup\nused for all the results reported in this paper.\nModels We used T5 PLMs from Wolf et al. (2020)\nfor our experiments for two distinct models, t5-\nlarge (770M parameters) and t5-base (220M pa-\nrameters), with a special focus on t5-large as it\nis the best performing of the two on various NLP\ntasks. Models were ﬁne-tuned to be either special-\nized on T2G (MT) or G2T (MG) task, or to accom-\nmodate both directions of generation (MT+G).\nData processingGraphs are often represented as\nlist of triples. However our model expects a se-\nquence of input words/tokens to work on. The\nlinearization of graph triples is obviously ambigu-\nous as there are many ways to traverse a graph\n(Breadth First Search, Depth First Search, random\nwalk, etc.). In practice, we linearize the triples in\nthe order of the list provided by the dataset, but\nuse this inherent linearization ambiguity as an op-\nportunity to do data-augmentation. Indeed, models\nare ﬁrst ﬁne-tuned using cross-entropy loss that\nstrongly penalizes generation if it is in any differ-\nent order than the ground truth order. To avoid the\nmodel to overﬁt to our data and memorize observed\ntriples order, we augment the data by including a\nfew permutations of the graph triples.\nDuring graph linearization, we encode the sub-\nject, predicate, and object positions by using\n<S>,< P>,< O> tokens. In practice, we expand the\nmodel vocabulary with these special indivisible to-\nkens that are not split during tokenization. No other\npreprocessing is done on the data for training. We\nexplored masked and span-masked LM ﬁne-tuning\nto match T5 pretraining (Raffel et al., 2020) which\ndid not lead to any noticeable improvements.\n4.1 Datasets\nWebNLG+ 2020We report results on WebNLG+\n2020 (v3.0) used in the WebNLG 2020 Challenge\n(Castro Ferreira et al., 2020b). The Challenge com-\nprises of two tasks: RDF-to-text generation (G2T),\nand Text-to-RDF semantic parsing (T2G). The Re-\nsource Description Framework (RDF) language is\nused to encode DBpedia and is commonly used\nin linked data framework. WebNLG+ uses RDF\nto encode graphs as sets of triples which are as-\nsociated to one or more lexicalizations of one or\nmore sentences each. Data for English and Russian\nare provided, but we only worked on the English\nsubset made of 13,211 train, 1,667 dev, 2,155 testA\n(semantic parsing), and 1,779 testB (data-to-text)\nsamples (triples sets w/ lexicalizations). The data\nis clustered semantically into 16 categories seen\nin train and dev sets (Airport, Astronaut, Build-\ning, etc.), while 3 categories (Film, Scientist, and\nMusical-Work) were introduced in test and are un-\nseen, i.e. not present in training; see Castro Fer-\nreira et al. (2020a) for more details. Results are\naggregated for all, seen, and unseen categories dur-\ning evaluation. Note that in the literature, prior\nworks sometimes report ‘WebNLG’ results on pre-\nvious dataset version, with completely different\nperformance ranges. We compare all our results to\nWebNLG+ 2020 (v3.0) numbers reported by Cas-\ntro Ferreira et al. (2020a) in their Table 6 for G2T,\nand Table 10 for T2G tasks, using the provided\nofﬁcial scoring scripts.\nTEKGEN To further study the robustness of our\nsystem, we also provide experiments using TEK-\nGEN dataset recently introduced in Agarwal et al.\n(2021). The graph-sentence alignments are cu-\nrated using Wikipedia and Wikidata. This serves\nas a perfect large scale test-bed for both G2T and\nT2G tasks. Unfortunately, this dataset lacks in en-\ntity/relation/object boundaries, which makes it dif-\nﬁcult to evaluate systems for T2G tasks. In order to\naddress this issue, we further process the triple-text\n(with no triple boundaries) to create list of triples\nusing Wikidata properties lookup, via Wikidata\n1089\nWebNLG G2T BLEU ↑ BLEU↑\nNLTK\nMETEOR↑ chrF++↑\nTeam/model\nAmazon AI (Shanghai) (Guo et al., 2020a) 0.540 0.535 0.417 0.690\nOSU Neural NLG (Li et al., 2020) 0.535 0.532 0.414 0.688\nFBConvAI (Yang et al., 2020) 0.527 0.523 0.413 0.686\nbt5 (Agarwal et al., 2020) 0.517 0.517 0.411 0.679\nReGen (Ours) G2T.CE t5-large 0.553 0.549 0.418 0.694\nReGen (Ours) G2T.RL t5-large 0.563 0.559 0.425 0.706\nReGen (Ours) G2T.CE.ES t5-base (early CE) 0.522 0.518 0.404 0.675\nReGen (Ours) G2T.RL.ES t5-base (early CE) 0.531 0.527 0.410 0.686\nReGen (Ours) G2T.CE.best t5-base (best CE) 0.524 0.520 0.404 0.677\nReGen (Ours) G2T.RL.best t5-base (best CE) 0.527 0.523 0.408 0.681\nTable 1: G2T Best results on WebNLG 2020 Challenge (v3.0) dataset. The ﬁrst four rows were the top performers\nof the Challenge. Results for CE and RL models are presented for our ReGen systems so to show gains from using\nSCST. Our G2T.RL is the best system overall, ﬁne-tuning a t5-large model using METEOR reward. G2T.RL.ES\nand G2T.RL.best show the impact of using early stopping (ES) or best CE selection for starting SCST ﬁne-tuning\non a t5-base smaller model while using BLEU_NLTK reward.\nQuery Service. Additionally, we limit the valida-\ntion set and test set to 5K and 50K sentence-triples\npairs respectively. Our training split after process-\ning contains 6.3 million sentence-triples pairs. As a\ncontribution to the work, we will present the steps\nto augment TEKGEN dataset with appropriate sub-\nject, object and relation boundaries, which enables\nconventional evaluation of research systems. An\nexample of the processed TEKGEN is shown in\nFig. 3 in Appendix.\nMetrics WebNLG+ 2020 provides automatic met-\nrics to evaluate models. For G2T, we used BLEU,\nBLEU_NLTK, METEOR, and chrF++ that are pro-\nvided by the challenge. For T2G, F1, Precision,\nand Recall scores are utilized and computed for\n4 levels of match: Exact, Ent_Type, Partial and\nStrict as described in Castro Ferreira et al. (2020a),\nwhich loosely correspond to different levels of re-\nlaxation of how close a match of an entity must\nbe to the ground truth in content and position in\na triple. Note that when generating graphs/RDFs,\nscoring metrics explore all possible permutations\nof a graph edges. For TEKGEN, we use the same\nmetrics as for WebNLG+ 2020.\n5 Results\nFor all experiments, PLMs were ﬁrst exposed to\nthe target datasets (WebNLG+, TEKGEN) by ﬁne-\ntuning using LCE loss. They were then switched\nto RL training by optimizing the LSCST loss. Al-\nthough no exact recipe has been established for\nSeq2Seq RL-training, starting from a good CE\nmodel helps RL training performance in practice\n(Ranzato et al., 2016; Rennie et al., 2017). There-\nfore, we followed the subsequent simple approach:\nDuring ﬁne-tuning, the evaluations are conducted\non the validation set. From the CE phase, the best\nperforming model iteration is selected based on the\nMETEOR and F1 score for the G2T and T2G tasks,\nrespectively, to pursue RL ﬁne-tuning. In case of\nG2T, potential ties in METEOR scores among can-\ndidate models, are resolved by using BLEU_NLTK,\nfollowed by the chrF++ metric. Note that early\nstopping selection of CE models led to good per-\nformance for t5-base models as well. During the\nSCST phase, the best model iteration on the valida-\ntion set is selected and its performance numbers on\nthe test set are reported in our tables.\nWebNLG+ 2020 G2TFor the WebNLG+ 2020\nChallenge, the results of the top four systems for\nRDF-to-text task can be found in Tab. 1 for all cat-\negories (results for seen and unseen categories are\ngiven in Tab. 5 in the Appendix), while descriptions\nthe top teams’ systems were given in Section 2. We\nreport our G2T results for both t5-large and t5-\nbase models as well. For t5-large, ReGen G2T.CE\nis the best model from CE ﬁne-tuning. ReGen\nG2T.RL is best model performance for SCST train-\ning while using METEOR as reward when starting\nfrom G2T.CE model. Tab. 1 shows that our CE\nmodel is better than models from all top teams, and\nthe SCST results further improve signiﬁcantly in\n1090\nWebNLG T2G Match F1 ↑ Precision↑ Recall↑\nTeam/model\nAmazon AI (Shanghai) (Guo et al., 2020a)\nExact 0.689 0.689 0.690\nEnt_Type 0.700 0.699 0.701\nPartial 0.696 0.696 0.698\nStrict 0.686 0.686 0.687\nbt5 (Agarwal et al., 2020)\nExact 0.682 0.670 0.701\nEnt_Type 0.737 0.721 0.762\nPartial 0.713 0.700 0.736\nStrict 0.675 0.663 0.695\nReGen (Ours) T2G.CE\nExact 0.723 0.714 0.738\nEnt_Type 0.807 0.791 0.835\nPartial 0.767 0.755 0.788\nStrict 0.720 0.713 0.735\nReGen (Ours) T2G.RL\nExact 0.720 0.712 0.734\nEnt_Type 0.804 0.789 0.829\nPartial 0.764 0.752 0.784\nStrict 0.717 0.709 0.731\nTable 2: T2G Best results on WebNLG+ 2020 (v3.0) dataset. The top two teams were the ﬁrst and second place\nwinner of the Challeneg. Our T2G.CE model improves upon all metrics for all matching schemes, providing a new\nstate-of-the-art results for this Challenge task. T2G.RL models, while still better than previous best results, does\nnot improve upon its CE counterpart.\nTEKGEN G2T BLEU ↑ BLEU↑\nNLTK\nMETEOR↑ chrF++↑\nModel\nReGen-CE Val 0.240 0.241 0.231 0.400\nTest 0.241 0.242 0.233 0.405\nReGen-SCST Val 0.258 0.259 0.240 0.418\nTest 0.262 0.262 0.242 0.422\nTable 3: G2T Results for T EKGEN dataset. ReGen-CE establishes a baseline on this dataset. ReGen-SCST\nconsistently improve on the baseline on all metrics, for validation and test sets.\nall metrics achieving state-of-the-art results to our\nknowledge. The gain obtained by SCST alone is\nquite signiﬁcant and demonstrates the beneﬁts of\nRL ﬁne-tuning for this task. We report our best\nmodel results in Tab. 1, as well as mean and stan-\ndard deviation results for multiple random number\ngenerator seeds in Tab. 10 in Appendix. When av-\neraging results for few seeded models, sustained\ngains from SCST are seen for all metrics.\nMultiple reward candidates were investigated\n(BLEU, BLEU_NLTK, METEOR, chrF) as well\nas some linear combinations of pairs of them, as\ncan be seen in Tab. 7 in Appendix. In Tab. 7, for\nt5-large, METEOR is consistently the best SCST\nreward, and improves all the other metrics scores\nas well. However, for ‘smaller’ models such as\nt5-base, BLEU_NLTK is revealed to be the best\nreward for improving BLEU performance as ex-\npected. Again, SCST brings signiﬁcant gains\nacross all the metrics in that case. Note that for\nt5-base model, selecting a METEOR reward im-\nproves METEOR results signiﬁcantly as reported\nin Tab. 9 in Appendix.\nAnother interesting fact is that early stopping of\nCE model G2T.CE.ES (at 5 epochs) leads to the\nbest SCST model G2T.RL.ES for t5-base, while\nselecting the best CE model G2T.CE.best (at 11\nepochs) still showed some gains from SCST model\nG2T.RL.best. SCST needs a good starting point,\nbut a better CE model that has seen a lot more\nepochs of our dataset maybe harder for SCST to\nstir in a better solution in the parameter space.\n1091\nMoreover, the test split contains unseen categories\nnot present in the validation dataset which render\nchoices based on validation sub-optimal for the test\ndataset. The best models we report in this work are\nspecialized models MG. Early in our investigation,\nhybrid models were the best performing model for\nG2T reaching 0.547 BLEU, 0.543 BLEU_NLTK\nand 0.417 METEOR, and ﬁrst to beat the Challenge\nwinning team. However, when batch size became\nlarger (20-24 samples), the specialized models took\nthe lead and retain it still.\nFor training, we optimized all our models us-\ning AdamW (Loshchilov and Hutter, 2017), vari-\nant of the Adam optimizer with default values of\nβ = [0.9,0.999] and weight decay of 10−2. For\nlearning rate, we used 5.10−6 for all our experi-\nments as it was better than 10−5 and 10−6 as seen\nin Tab. 8 in Appendix. All our models were trained\nwith 20-24 minibatch size on WebNLG. Further\ndetails on our experimental setup are provided in\nthe Appendix in Section A.\nWebNLG+ 2020 T2GResults for the Text-to-RDF\ntask are reported in Tab. 2 for all categories. Results\nfor our best model on seen and unseen categories\nare given in Tab. 6 in Appendix. Amazon AI and\nbt5 are the top performing teams. Again, the pro-\nposed ReGen T2G.CE model shows strong results\nthat are better in term of all metrics, for all match-\ning categories. In themselves, these numbers are\na de-facto new state-of-the-art for this dataset, as\nfar as we know. SCST model T2G.RL fails to im-\nprove on this model though. The exact F1 metric\nwas used as reward, but the model could never pull\nahead of the CE model in our experiments. The\nexact F1 metric may not be a strong enough reward\nto really capture the dynamics of graph generation\nproperly for WebNLG+ as it is very rigid in its\nmeasure (one must have an exact match), although\nthe same reward gave good results on our second\ndataset TEKGEN. A more sensitive metric could\npossibly help. We even tried to use n-gram based\nmetrics (like BLEU) but to no avail. We further\naddress this issue at the end on this Section.\nTEKGEN G2T For the TEKGEN dataset, we\npresent our results on Graph-to-Text generation\nin Tab. 3. Similar to the experiments in WebNLG+,\nwe pick the best model during the CE ﬁne-tuning\nbased on the METEOR score and proceed with the\nRL ﬁne-tuning. We observe that the RL ﬁne-tuning\nstep helps boost the test split scores on all metrics.\nIt is worth noting that the scores are slightly under-\nT2G F1 ↑ P↑ R↑\nModel\nReGen-CE Val 0.622 0.608 0.647\nTest 0.619 0.605 0.643\nReGen-SCST Val 0.615 0.600 0.640\nTest 0.623 0.610 0.647\nTable 4: T2G TEKGEN Results: ReGen-CE establishes\na baseline of the dataset. ReGen-SCST improves re-\nsults on the test set compared to ReGen-CE.\nestimating the potential of our system because of\nthe nature of the sentences in the TEKGEN dataset.\nUnlike WebNLG+, in a paired text-graph sample\nin TEKGEN, the linearized graph does not usually\ncover all the concepts described in the correspond-\ning text. This leads to underestimating when the\nhypothesis is scored against the reference using\nn-gram metrics.\nTEKGEN T2G Results for the Text-to-Graph for\nTEKGEN are reported in Tab. 4. Once the CE ﬁne-\ntuning is done, we continue with the RL ﬁne-tuning\nusing exact F1 as reward. The performance is con-\nsistent with what we observe in G2T task for TEK-\nGEN, where SCST step boosts the performance of\nthe model. Since, we reformulate this dataset (refer\nSection 4.1) to offer as T2G and G2T tasks, our\napproach is the ﬁrst attempt in understanding the\nnature of TEKGEN dataset and our methods pro-\nvide a baseline for future research. Please note that\nfor both T2G and G2T tasks in TEKGEN, we only\nstart a t5-large PLM.\nSummary Results on WebNLG+ 2020 and TEK-\nGEN demonstrated that RL ﬁne-tuning of models\nleads to signiﬁcant improvements of results for\nT2G and G2T, establishing new state-of-the-art\nresults for both tasks. For WebNLG+, T2G was\na challenging task for RL ﬁne-tuning. In further\nwork, we plan to address this issue by investigating\ntwo points: First, look into a more sensible graph-\ndependent sampling for graph structures, rather\nthan the current multinomial sampling of the best\ntokens at each generation step. Second, try a differ-\nent reward schemes where the reward is more at-\ntuned to the challenges of graph generation as well\nas graph structure, allowing for some curriculum\nlearning, or increasing the harshness of rewards\ngradually during training. Results on TEKGEN\nshowed that RL ﬁne-tuning is a viable option even\non large-scale datasets. To enrich this quantitative\n1092\nstudy of ReGen, we provide a few qualitative cherry\npicked results in Tab. 11 and Tab. 12 in Appendix.\n6 Conclusions\nIn this paper, we proposed to use RL for improv-\ning upon current generation for text-to-graph and\ngraph-to-text tasks for the WebNLG+ 2020 Chal-\nlenge dataset using pre-trained LMs. We not only\ndeﬁned a novel Seq2Seq training of models in T2G\nand G2T generation tasks, but we established state-\nof-the-art results for WebNLG+ for both tasks, sig-\nniﬁcantly improving on the previously published\nresults. We provided extensive analyses of our\nresults and of the steps taken to reach these im-\nprovements. We then expanded our approach to\nlarge scale training by means of TEKGEN where\nwe demonstrated that RL ﬁne-tuning provides a\nrobust way to improve upon regular model ﬁne-\ntuning within a dataset that is orders of magnitude\nlarger than the WebNLG+ starting point. We es-\ntablished gains despite a weaker content overlap\nin text-graph data pairs for TEKGEN. Along the\nway, we constructed subject, and relation-object\nboundaries from TEKGEN sentence-triples pairs\nthat we plan on releasing to beneﬁt the research\ncommunity.\nFuture work will focus on developing a vari-\nant of SCST that leverages the unique structure of\ngraph by either performing of more sensible graph-\ndependent sampling, or by investigating different\nreward schemes more attuned to integrating the\ncontent and structure of graphs.\n7 Broader Impact Statement\nThe techniques proposed in this paper are inher-\nently dependent on the training data and the PLMs\nused for ﬁne-tuning on this data. The models do\nbeneﬁt from the large amount of data seen by the\nPLM they are derived from, however it is fair to\nassume that any detectable bias in the original data\nor PLMs would most likely be transferred to the\ntext-to-graph and graph-to-text generative models.\nThis is something to keep in mind when building\nthese generative models. Public datasets were used\nfor all experiments. The TEKGEN with recreated\nboundaries does not change the underlying data\nand should not add any further noise nor bias to the\noriginal data.\nReferences\nOshin Agarwal, Heming Ge, Siamak Shakeri, and\nRami Al-Rfou. 2021. Knowledge graph based syn-\nthetic corpus generation for knowledge-enhanced\nlanguage model pre-training. In Proceedings of the\n2021 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Hu-\nman Language Technologies, pages 3554–3565, On-\nline. Association for Computational Linguistics.\nOshin Agarwal, Mihir Kale, Heming Ge, Siamak Shak-\neri, and Rami Al-Rfou. 2020. Machine transla-\ntion aided bilingual data-to-text generation and se-\nmantic parsing. In Proceedings of the 3rd Interna-\ntional Workshop on Natural Language Generation\nfrom the Semantic Web (WebNLG+), pages 125–130,\nDublin, Ireland (Virtual). Association for Computa-\ntional Linguistics.\nDzmitry Bahdanau, Philemon Brakel, Kelvin Xu,\nAnirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron C.\nCourville, and Yoshua Bengio. 2017. An actor-critic\nalgorithm for sequence prediction. In 5th Inter-\nnational Conference on Learning Representations,\nICLR 2017, Toulon, France, April 24-26, 2017, Con-\nference Track Proceedings. OpenReview.net.\nThiago Castro Ferreira, Claire Gardent, Nikolai\nIlinykh, Chris van der Lee, Simon Mille, Diego\nMoussallem, and Anastasia Shimorina. 2020a. The\n2020 bilingual, bi-directional WebNLG+ shared\ntask: Overview and evaluation results (WebNLG+\n2020). In Proceedings of the 3rd International Work-\nshop on Natural Language Generation from the Se-\nmantic Web (WebNLG+), pages 55–76, Dublin, Ire-\nland (Virtual). Association for Computational Lin-\nguistics.\nThiago Castro Ferreira, Claire Gardent, Nikolai\nIlinykh, Chris van der Lee, Simon Mille, Diego\nMoussallem, and Anastasia Shimorina, editors.\n2020b. Proceedings of the 3rd International Work-\nshop on Natural Language Generation from the Se-\nmantic Web (WebNLG+). Association for Computa-\ntional Linguistics, Dublin, Ireland (Virtual).\nYu Chen, Lingfei Wu, and Mohammed J. Zaki. 2020.\nReinforcement learning based graph-to-sequence\nmodel for natural question generation. In 8th Inter-\nnational Conference on Learning Representations,\nICLR 2020, Addis Ababa, Ethiopia, April 26-30,\n2020. OpenReview.net.\nPierre Dognin, Igor Melnyk, Inkit Padhi, Cicero\nNogueira dos Santos, and Payel Das. 2020. Du-\nalTKB: A Dual Learning Bridge between Text and\nKnowledge Base. In Proceedings of the 2020 Con-\nference on Empirical Methods in Natural Language\nProcessing (EMNLP), pages 8605–8616, Online. As-\nsociation for Computational Linguistics.\nQipeng Guo, Zhijing Jin, Ning Dai, Xipeng Qiu, Xi-\nangyang Xue, David Wipf, and Zheng Zhang. 2020a.\n√2: A plan-and-pretrain approach for knowledge\n1093\ngraph-to-text generation. In Proceedings of the 3rd\nInternational Workshop on Natural Language Gen-\neration from the Semantic Web (WebNLG+) , pages\n100–106, Dublin, Ireland (Virtual). Association for\nComputational Linguistics.\nQipeng Guo, Zhijing Jin, Xipeng Qiu, Weinan Zhang,\nDavid Wipf, and Zheng Zhang. 2020b. CycleGT:\nUnsupervised graph-to-text and text-to-graph gener-\nation via cycle training. In Proceedings of the 3rd In-\nternational Workshop on Natural Language Genera-\ntion from the Semantic Web (WebNLG+), pages 77–\n88, Dublin, Ireland (Virtual). Association for Com-\nputational Linguistics.\nQipeng Guo, Zhijing Jin, Ziyu Wang, Xipeng Qiu,\nWeinan Zhang, Jun Zhu, Zheng Zhang, and David\nWipf. 2021. Fork or fail: Cycle-consistent train-\ning with many-to-one mappings. In The 24th In-\nternational Conference on Artiﬁcial Intelligence and\nStatistics, AISTATS 2021, April 13-15, 2021, Vir-\ntual Event, volume 130 of Proceedings of Machine\nLearning Research, pages 1828–1836. PMLR.\nAlon Lavie and Abhaya Agarwal. 2007. METEOR: An\nautomatic metric for MT evaluation with high levels\nof correlation with human judgments. In Proceed-\nings of the Second Workshop on Statistical Machine\nTranslation, pages 228–231, Prague, Czech Repub-\nlic. Association for Computational Linguistics.\nMike Lewis, Yinhan Liu, Naman Goyal, Mar-\njan Ghazvininejad, Abdelrahman Mohamed, Omer\nLevy, Veselin Stoyanov, and Luke Zettlemoyer.\n2020. BART: Denoising sequence-to-sequence pre-\ntraining for natural language generation, translation,\nand comprehension. In Proceedings of the 58th An-\nnual Meeting of the Association for Computational\nLinguistics, pages 7871–7880, Online. Association\nfor Computational Linguistics.\nJiwei Li, Will Monroe, Alan Ritter, Michel Galley, Jian-\nfeng Gao, and Dan Jurafsky. 2016. Deep reinforce-\nment learning for dialogue generation.\nXintong Li, Aleksandre Maskharashvili, Symon Jory\nStevens-Guille, and Michael White. 2020. Lever-\naging large pretrained models for WebNLG 2020.\nIn Proceedings of the 3rd International Workshop\non Natural Language Generation from the Seman-\ntic Web (WebNLG+) , pages 117–124, Dublin, Ire-\nland (Virtual). Association for Computational Lin-\nguistics.\nIlya Loshchilov and Frank Hutter. 2017. Fixing\nweight decay regularization in adam. CoRR,\nabs/1711.05101.\nRuotian Luo. 2020. A better variant of self-critical se-\nquence training.\nSan Pa Pa Aung, Win Pa Pa, and Tin Lay Nwe.\n2020. Automatic Myanmar image captioning us-\ning CNN and LSTM-based language model. In\nProceedings of the 1st Joint Workshop on Spoken\nLanguage Technologies for Under-resourced lan-\nguages (SLTU) and Collaboration and Computing\nfor Under-Resourced Languages (CCURL) , pages\n139–143, Marseille, France. European Language Re-\nsources association.\nRomain Paulus, Caiming Xiong, and Richard Socher.\n2017. A deep reinforced model for abstractive sum-\nmarization.\nMaja Popovi ´c. 2017. chrF++: words helping charac-\nter n-grams. In Proceedings of the Second Con-\nference on Machine Translation , pages 612–618,\nCopenhagen, Denmark. Association for Computa-\ntional Linguistics.\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine\nLee, Sharan Narang, Michael Matena, Yanqi Zhou,\nWei Li, and Peter J. Liu. 2020. Exploring the limits\nof transfer learning with a uniﬁed text-to-text trans-\nformer.\nMarc’Aurelio Ranzato, Sumit Chopra, Michael Auli,\nand Wojciech Zaremba. 2016. Sequence level train-\ning with recurrent neural networks. In 4th Inter-\nnational Conference on Learning Representations,\nICLR 2016, San Juan, Puerto Rico, May 2-4, 2016,\nConference Track Proceedings.\nSteven J. Rennie, Etienne Marcheret, Youssef Mroueh,\nJerret Ross, and Vaibhava Goel. 2017. Self-critical\nsequence training for image captioning. In 2017\nIEEE Conference on Computer Vision and Pattern\nRecognition, CVPR 2017, Honolulu, HI, USA, July\n21-26, 2017, pages 1179–1195. IEEE Computer So-\nciety.\nLeonardo F. R. Ribeiro, Martin Schmitt, Hinrich\nSchütze, and Iryna Gurevych. 2020. Investigating\npretrained language models for graph-to-text gener-\nation.\nDavid Silver. 2015. Lectures on reinforcement learn-\ning. URL : https://www.davidsilver.uk/\nteaching/.\nRichard S. Sutton and Andrew G. Barto. 2018. Rein-\nforcement Learning: An Introduction . A Bradford\nBook, Cambridge, MA, USA.\nBowen Tan, Zhiting Hu, Zichao Yang, Ruslan Salakhut-\ndinov, and Eric Xing. 2019. Connecting the dots be-\ntween mle and rl for sequence prediction.\nRonald J Williams. 1992. Simple statistical gradient-\nfollowing algorithms for connectionist reinforce-\nment learning. Machine learning, 8(3-4):229–256.\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien\nChaumond, Clement Delangue, Anthony Moi, Pier-\nric Cistac, Tim Rault, Remi Louf, Morgan Funtow-\nicz, Joe Davison, Sam Shleifer, Patrick von Platen,\nClara Ma, Yacine Jernite, Julien Plu, Canwen Xu,\nTeven Le Scao, Sylvain Gugger, Mariama Drame,\n1094\nQuentin Lhoest, and Alexander Rush. 2020. Trans-\nformers: State-of-the-art natural language process-\ning. In Proceedings of the 2020 Conference on Em-\npirical Methods in Natural Language Processing:\nSystem Demonstrations, pages 38–45, Online. Asso-\nciation for Computational Linguistics.\nZixiaofan Yang, Arash Einolghozati, Hakan Inan,\nKeith Diedrick, Angela Fan, Pinar Donmez, and\nSonal Gupta. 2020. Improving text-to-text pre-\ntrained models for the graph-to-text task. In Pro-\nceedings of the 3rd International Workshop on Nat-\nural Language Generation from the Semantic Web\n(WebNLG+), pages 107–116, Dublin, Ireland (Vir-\ntual). Association for Computational Linguistics.\nWojciech Zaremba and Ilya Sutskever. 2016. Rein-\nforcement learning neural turing machines - revised.\n1095\nA Training Setup\nAll our experiments were run using NVIDIA V100\nGPUs for training and validation, some trainings\nwere done on A100. We distributed our training to\n2-4 GPUs depending on availability. Each training\nepoch for CE ranged from 30 minutes to 1 hour\ndepending on number of GPUs utilized.\nValidation and testing (1,779 and 2,155 samples\nfor testA and testB of WebNLG+ 2020) lasted from\n40 minutes to 1 hour depending on machines. Com-\nputation was dominated by beam search generation\nas we used beam search with beam size of 5 and a\nmax sequence length of 192 (since linearized graph\nsequence can be quite long). We used the ofﬁcial\nscoring scripts released by WebNLG+ 2020 Chal-\nlenge to score all our experiments. The evaluation\nof graph being the most computationally expensive\nas all possible matching combinations are tested in\nwhat looks like a factorial complexity, taking scor-\ning of set of triples larger than 8 from impractical\nto not feasible.\nAll our models were built using PyTorch. Total\neffective batch sizes were set to either 20 or 24\nsamples for our distributed training. We adjusted\nthe batch size on each worker to ensure consistent\nglobal batch size of 20 or 24.\nWe did some search on learning rates for t5-\nlarge training and SCST rewards, see discussion\nand results in Section C.\nAll our trainings have a seeded random number\ngenerator for reproducibility. We also report re-\nsults on WebNLG+ 2020 G2T tasks for each train-\ning setup by showing results for 3 models from\ndifferent seeds, and provide means and standard\ndeviations of these results in Tab. 10.\nB WebNLG+ 2020 Results per\nCategories for Best G2T and T2G\nModels\nIn Tab. 5, we are reporting results for all WebNLG+\n2020 categories for our best CE and RL models.\nWhile results for unseen categories are much worse\nthan for seen categories, RL ﬁne-tuning manages\nto improve on both seen and unseen categories.\nTab. 6 provides results for seen, unseen and all\ncategories for our best CE model ReGen T2G.CE\nwhich established state-of-the-art results on T2G\ntask of WebNLG+ 2020 Challenge dataset.\nC Ablation Studies\nIn Tables 7 and 8 we present ablation studies of\ndifferent optimized metrics and learning rates for\nSCST training. As can be seen from Table 7, when\nMETEOR is used as a reward, we get the best per-\nformance across all the metrics. We also tried using\na combination of multiple rewards with different\nscaling but did not get any gain over the single\nmetric rewards. In Table 8. we also show the ef-\nfect of learning rate on SCST performance. Using\nlr= 5·10−6 gave us the best performance, while\nhigher rates, such as 10−4, led to unstable training\nand collapse of SCST.\nD G2T Results t5-base models for SCST\nwith METEOR Reward\nResults for SCST ﬁne-tuning of t5-base models\nusing a METEOR reward are compiled in Tab. 9.\nClearly, these models achieve better METEOR re-\nsults as expected since they are RL optimized on\nthis metric.\nE G2T Results for Models from Multiple\nRandom Seeds\nAll our training have a seeded random number gen-\nerator for reproducibility. We also report the mean\nand standard deviations for all our G2T models.\nEach model setup was run 3 times using three in-\ndependent and distinct seeds, following the same\nexact process. This is to ensure that our results are\nnot just the product of a lucky system conﬁguration\nor otherwise advantageous random shufﬂing of our\ntraining dataset. All results are reported in Tab. 10.\nThe gain reported between CE and RL for our t5-\nlarge models are clearly still showing after average\nof all 3 models from distinct random seeds. For\nt5-base, gains between CE and RL are still present,\nalbeit smaller than for our best systems.\nF Generation Examples for G2T and\nT2G\nWe present some cherry-picked examples for G2T\nin Tab. 12 and for T2G in Tab. 11 for both WebNLG\nand TEKGEN datasets.\nG Processed T EKGEN Dataset\nIn Fig. 3 we show an example of our processing of\nTEKGEN dataset in establishing subject, relation,\nobject boundaries. This enables both training and\nevaluating systems for T2G and G2T tasks.\n1096\nWebNLG G2T Best Models Category BLEU ↑ BLEU↑\nNLTK\nMETEOR↑ chrF++↑\nOurs t5-large ReGen-CE unseen 48.76 0.489 0.397 0.653\nseen 59.73 0.592 0.433 0.722\nall 55.26 0.549 0.418 0.694\nOurs t5-large ReGen-SCST unseen 49.06 0.493 0.404 0.665\nseen 61.22 0.605 0.440 0.734\nall 56.25 0.559 0.425 0.706\nTable 5: G2T: Results for seen, unseen, and all categories subsets in WebNLG+ 2020 Challenge Test dataset. As\nexpected, unseen categories much worse results than for seen categories. RL ﬁne-tuning manages to improve on\nboth seen and unseen categories.\nWebNLG T2G Match F1 ↑ Precision↑ Recall↑\nReGen T2G.CE\nunseen\nExact 0.5809 0.5662 0.6069\nEnt_Type 0.7014 0.6741 0.7497\nPartial 0.6453 0.6241 0.6826\nStrict 0.5754 0.5608 0.6012\nseen\nExact 0.8322 0.8286 0.8384\nEnt_Type 0.8878 0.8811 0.8998\nPartial 0.8604 0.8553 0.8696\nStrict 0.8317 0.8282 0.8379\nall\nExact 0.7229 0.7144 0.7376\nEnt_Type 0.8067 0.7910 0.8345\nPartial 0.7668 0.7547 0.7882\nStrict 0.7202 0.7118 0.7349\nTable 6: T2G: Results for seen, unseen, and all categories subsets in WebNLG+ 2020 Challenge Test dataset. As\nexpected the performance drops signiﬁcantly for unseen categories and are the best for seen categories.\nSCST Reward BLEU ↑ BLEU↑\nNLTK\nMETEOR↑ chrF++↑\nBLEU 0.556 0.552 0.420 0.698\nBLEU NLTK 0.558 0.554 0.422 0.700\nMETEOR 0.563 0.559 0.425 0.706\nchrF++ 0.554 0.551 0.423 0.701\n1/2·METEOR+1/2·BLEU NLTK 0.555 0.551 0.421 0.699\n2/3·METEOR+1/3·BLEU NLTK 0.547 0.543 0.419 0.697\nTable 7: Ablation study of metrics used as rewards in SCST for t5-large models. The results shown are on the test\nsplit.\nLearning Rate BLEU ↑ BLEU↑\nNLTK\nMETEOR↑ chrF++↑\n10−6 0.553 0.549 0.420 0.698\n5 ·10−6 0.558 0.554 0.422 0.700\n10−5 0.544 0.542 0.419 0.696\nTable 8: Ablation study on learning rates in SCST (using BLEU NLTK as the optimized metric)\n1097\nWebNLG G2T BLEU ↑ BLEU↑\nNLTK\nMETEOR↑ chrF++↑\nTeam/model\nReGen G2T.RL.ES.meteor t5-base (early CE) 0.527 0.523 0.413 0.689\nReGen G2T.RL.best.meteor t5-base (best CE) 0.528 0.526 0.412 0.681\nTable 9: G2T: Best results for t5-base ﬁne-tuned with SCST using METEOR as reward.\nTeam Name BLEU ↑ BLEU↑\nNLTK\nMETEOR↑ chrF++↑\nReGen G2T.CE t5-large 0.543 ±0.007 0.540 ±0.007 0.416 ±0.002 0.691 ±0.002\nReGen G2T.RL t5-large 0.553 ±0.007 0.550 ±0.007 0.422 ±0.002 0.702 ±0.003\nReGen G2T.CE.ES t5-base (early CE) 0.521 ±0.004 0.517 ±0.004 0.404 ±0.001 0.675 ±0.002\nReGen G2T.RL.ES t5-base (early CE) 0.528 ±0.007 0.523 ±0.007 0.408 ±0.002 0.682 ±0.003\nReGen G2T.CE.best t5-base (best CE) 0.524 ±0.000 0.520 ±0.001 0.404 ±0.000 0.670 ±0.000\nReGen G2T.RL.best t5-base (best CE) 0.525 ±0.007 0.522 ±0.007 0.407 ±0.002 0.681 ±0.003\nReGen G2T.RL.ES.meteor t5-base (early CE) 0.525 ±0.007 0.521 ±0.007 0.412 ±0.002 0.687 ±0.003\nReGen G2T.RL.best.meteor t5-base (best CE) 0.527 ±0.007 0.524 ±0.007 0.410 ±0.002 0.686 ±0.003\nTable 10: Results means and standard deviations (SD), shown as mean ±SD, for CE and SCST trained models\n(including our best results model) for a total of 3 different random number generator seeds used in training.\nFigure 3: An example from the processed T EKGEN dataset. The original dataset lacks KG boundaries, which\nmakes it difﬁcult to evaluate T2G systems efﬁciently.\n1098\nType Sentence / Graph\nSource The Pontiac Rageous began and ended its production in 1997 on an assembly line in Detroit, a city in\nMichigan.\nGold Pontiac_Rageous ♦ productionStartYear ♦ 1997 ♦ Pontiac_Rageous ♦ assembly ♦ Michigan ♦\nPontiac_Rageous ♦ assembly ♦ Detroit ♦ Pontiac_Rageous ♦ productionEndYear ♦ 1997 ♦ Detroit\n♦ type ♦ City_(Michigan)\nHyp-CE Pontiac_Rageous ♦ assembly ♦ Detroit ♦ Pontiac_Rageous ♦ modelYears ♦ 1997 ♦ Pontiac_Rageous\n♦ modelYears ♦ 1997 ♦ Detroit ♦ isPartOf ♦ Michigan\nHyp-SCST Pontiac_Rageous ♦ assembly ♦ Detroit ♦ Pontiac_Rageous ♦ modelYears ♦ 1997 ♦ Pontiac_Rageous\n♦ assembly ♦ Michigan\nSource In the United States, where Abraham A, Ribicoff was born, African Americans are one of the ethnic\ngroups. Abraham A. Ribicoff was married to Ruth Ribicoff.\nGold Abraham_A._Ribicoff ♦ spouse ♦ \"Ruth Ribicoff\" ♦ Abraham_A._Ribicoff ♦ birthPlace ♦\nUnited_States ♦ United_States ♦ ethnicGroup ♦ African_Americans ♦ Abraham_A._Ribicoff ♦\nnationality ♦ United_States\nHyp-CE Abraham_A._Ribicoff ♦ birthPlace ♦ United_States ♦ Abraham_A._Ribicoff ♦ spouse ♦ \"Ruth\nRibicoff\" ♦ United_States ♦ ethnicGroup ♦ African_Americans\nHyp-SCST Abraham_A._Ribicoff ♦ birthPlace ♦ United_States ♦ Abraham_A._Ribicoff ♦ spouse ♦ \"Ruth\nRibicoff\" ♦ Abraham_A._Ribicoff ♦ nationality ♦ American ♦ United_States ♦ ethnicGroup ♦\nAfrican_Americans\nSource Super Capers, edited by Stacy Katzman, is a 98 minute ﬁlm starring Michael Rooker and Tom Sizemore.\nGold Super_Capers ♦ editing ♦ Stacy_Katzman ♦ Super_Capers ♦ starring ♦ Michael_Rooker ♦ Su-\nper_Capers ♦ starring ♦ Tom_Sizemore ♦ Super_Capers ♦ runtime | 98.0\nHyp-CE Super_Capers ♦ starring ♦ Tom_Sizemore ♦ Super_Capers ♦ timeOut ♦ \"980.0\"(minutes) ♦ Su-\nper_Capers ♦ starring ♦ Michael_Rooker ♦ Super_Capers ♦ editor ♦ Stacy_Katzman\nHyp-SCST Super_Capers ♦ starring ♦ Tom_Sizemore ♦ Super_Capers ♦ length ♦ 98.0 (minutes) ♦ Super_Capers\n♦ starring ♦ Michael_Rooker ♦ Super_Capers ♦ editor ♦ Stacy_Katzman\nSource Doctor George Cary (1611-1680), Professor of Sacred Theology, lord of the manor of Clovelly, Devon,\nwas Dean of Exeter between 1663 and 1680 (amongst other duties responsible for the maintenance and\ndecoration of Exeter Cathedral).\nGold George Cary (1611-1680) ♦ position held ♦ Dean of Exeter ♦ start time ♦ 01 January 1663 ♦ date of\nbirth ♦ 00 1611 ♦ date of death ♦ 00 1680\nHyp-CE George Cary (priest) ♦ date of birth ♦ 01 January 1611 ♦ date of death ♦ 01 January 1680\nHyp-SCST George Cary (priest) ♦ position held ♦ Dean of Exeter ♦ date of birth ♦ 01 January 1611 ♦ date of\ndeath ♦ 01 January 1680\nSource Early general elections were held in the Bahamas on 10 April 1968.\nGold 1968 Bahamian general election ♦ point in time ♦ 10 April 1968 ♦ country ♦ The Bahamas ♦ applies\nto jurisdiction ♦ The Bahamas\nHyp-CE 1968 Bahamian general election ♦ point in time ♦ 10 April 1968\nHyp-SCST 1968 Bahamian general election ♦ point in time ♦ 10 April 1968 ♦ country ♦ The Bahamas\nSource The school was established on 6 January 1930, by former education minister, CWW Kannangara, who\nadditionally founded two other colleges located in central Ceylon.\nGold Kattankudy Central College ♦ instance of ♦ School\nHyp-CE Government Polytechnic , Colombo ♦ inception ♦ 00 1930\nHyp-SCST Government Polytechnic , Colombo ♦ inception ♦ 00 1930 ♦ instance of ♦ School\nTable 11: Few cherry-picked generation for T2G task for WebNLG+ 2020 (top three) and T EKGEN (bottom\nthree). For each source (Text), we show the ground truth (Gold) and system generated hypothesis from the best\nCE (Hyp-CE) and SCST models ( Hyp-SCST). Note that the set of triples in WebNLG+ takes the form xG =\n[(s1♦p1♦o1),..., (sK♦pK♦oK)], whereas the same for TEKGEN is of form xG = [s♦(p1♦o1),..., (pK♦oK)]\n1099\nType Graph / Sentence\nSource McVeagh_of_the_South_Seas ♦ starring ♦ Harry_Carey_(actor_born_1878) ♦\nMcVeagh_of_the_South_Seas ♦ writer ♦ Harry_Carey_(actor_born_1878)\nGold Born in 1878, Harry Carey later grew up to write and star in the movie McVeagh of the South Seas.\nHarry Carey, born in 1878, wrote and appeared in the movie McVeagh of the South Seas. Harry Carey,\nwho was born in 1878, wrote and starred the ﬁlm of McVeagh of the South Seas.\nHyp-CE McVeagh of the South Seas was written by Harry Carey, who was born in 1878.\nHyp-SCST McVeagh of the South Seas was written by Harry Carey and starred the actor Harry Carey who was born\nin 1878.\nSource Aleksandr_Prudnikov ♦ height ♦ 185.0 (centimetres) ♦ Aleksandr_Prudnikov ♦ youthclub ♦\nFC_Spartak_Moscow ♦ FC_Spartak_Moscow ♦ ground ♦ Otkrytiye_Arena\nGold Aleksandr Prudnikov, 185cm tall played for FC Spartak Moscow’s youth team. FC Spartak Moscow is\nbased in the Otkrytiye Arena. Aleksandr Prudnikov who is 185 cm tall is a member of the youth side of\nFC Spartak Moscow. The home ground of FC Spartak Moscow is Otkrytiye Arena. Aleksandr Prudnikov\nis 185.0 cm tall and played for the FC Spartak Moscow at the Otkrytiye Arena.\nHyp-CE Aleksandr Prudnikov is 185 cm tall and played for FC Spartak Moscow’s youth team at the Otkrytiye\nArena.\nHyp-SCST Aleksandr Prudnikov is 185 cm tall and played for the youth team of FC Spartak Moscow whose home\nground is the Otkrytiye Arena.\nSource Baku_Turkish_Martyrs’_Memorial ♦ nativeName ♦ \"Türk ¸ Sehitleri Anıtı\" ♦\nBaku_Turkish_Martyrs’_Memorial♦ location ♦ Azerbaijan\nGold The Native name of the Baku Turkish Martyrs’ Memorial is \"Türk ¸ Sehitleri Anıtı\" which is located in\nAzerbaijan. The native name of the Baku Turkish Martyrs’ Memorial is Türk ¸ Sehitleri Anıtı located in\nAzerbaijan. The native name for the Baku Turkish Martyrs’ Memorial is Türk ¸ Sehitleri Anıtı, which is\nlocated in Baku, Azerbaijan.\nHyp-CE The native name of the Baku Turkish Martyrs’ Memorial in Azerbaijan is Türk ¸ Sehitleri Anıtı.\nHyp-SCST The Baku Turkish Martyrs’ Memorial is located in Azerbaijan and is known locally as Türk ¸ Sehitleri\nAnıtı.\nSource John Banister (anatomist) ♦ occupation ♦ Surgeon ♦ date of birth ♦ 01 January 1533 ♦ date of death\n♦ 01 January 1610\nGold John Banister (1533-1610) was an English anatomist, surgeon and teacher.\nHyp-CE John Banister (1533-1610) was an English surgeon.\nHyp-SCST John Banister (1533-1610) was an English surgeon and anatomist.\nSource WNPT (TV) ♦ country ♦ United States ♦ instance of ♦ Television station\nGold WNPT, virtual channel 8 (VHF digital channel 7), is a PBS member television station licensed to\nNashville, Tennessee, United States.\nHyp-CE WNPT, virtual channel 3 (UHF digital channel 15), is a Fox-afﬁliated television station licensed to\nPortland, Oregon, United States.\nHyp-SCST WNPT, virtual channel 4 (UHF digital channel 16), is a Public Broadcasting Service (PBS) member\ntelevision station licensed to Portland, Oregon, United States.\nSource Our Lady of the Presentation Cathedral, Natal ♦ inception ♦ 21 November 1988\nGold Our Lady of the Presentation Cathedral, Natal was inaugurated on November 21, 1988, and is located in\nthe district of Cidade Alta in Natal, capital of the Brazilian state of Rio Grande do Norte.\nHyp-CE Our Lady of the Presentation Cathedral, Natal was built in 1988.\nHyp-SCST Our Lady of the Presentation Cathedral, Natal was consecrated on 21 November 1988.\nTable 12: Few cherry-picked generation for G2T task for WebNLG+ 2020 (top three) and TEKGEN (bottom three).\nFor each source ( Graph), we show the ground truth ( Gold) and system generated hypothesis from the best CE\n(Hyp-CE) and SCST models ( Hyp-SCST). Note that the set of triples in WebNLG+ 2020 takes the form xG =\n[(s1♦p1♦o1),..., (sK♦pK♦oK)], whereas the same for TEKGEN is of form xG = [s♦(p1♦o1),..., (pK♦oK)]",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.8041858673095703
    },
    {
      "name": "Reinforcement learning",
      "score": 0.7380073070526123
    },
    {
      "name": "Graph",
      "score": 0.6256349086761475
    },
    {
      "name": "Text generation",
      "score": 0.5912240743637085
    },
    {
      "name": "Generative grammar",
      "score": 0.5840809941291809
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5661967992782593
    },
    {
      "name": "Sequence (biology)",
      "score": 0.4343436658382416
    },
    {
      "name": "Frame (networking)",
      "score": 0.4226728677749634
    },
    {
      "name": "Machine learning",
      "score": 0.37298327684402466
    },
    {
      "name": "Natural language processing",
      "score": 0.33788660168647766
    },
    {
      "name": "Theoretical computer science",
      "score": 0.20231404900550842
    },
    {
      "name": "Telecommunications",
      "score": 0.0
    },
    {
      "name": "Genetics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    }
  ]
}