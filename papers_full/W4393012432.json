{
  "title": "Conversing with business process-aware Large Language Models: the BPLLM framework",
  "url": "https://openalex.org/W4393012432",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A2167511984",
      "name": "Mario Luca Bernardi",
      "affiliations": [
        "University of Sannio"
      ]
    },
    {
      "id": "https://openalex.org/A5094205151",
      "name": "Angelo Casciani",
      "affiliations": [
        "Sapienza University of Rome"
      ]
    },
    {
      "id": "https://openalex.org/A2171208828",
      "name": "Marta Cimitile",
      "affiliations": [
        "Unitelma Sapienza University"
      ]
    },
    {
      "id": "https://openalex.org/A2085768537",
      "name": "Andrea Marrella",
      "affiliations": [
        "Unitelma Sapienza University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4365512576",
    "https://openalex.org/W4367059011",
    "https://openalex.org/W3186681724",
    "https://openalex.org/W4290927839",
    "https://openalex.org/W2737332333",
    "https://openalex.org/W4396540853",
    "https://openalex.org/W2905034244",
    "https://openalex.org/W4388775241",
    "https://openalex.org/W2848080124",
    "https://openalex.org/W4315647142",
    "https://openalex.org/W2132075088",
    "https://openalex.org/W2146708003",
    "https://openalex.org/W17314131",
    "https://openalex.org/W1490765632",
    "https://openalex.org/W3003245753",
    "https://openalex.org/W4294816544",
    "https://openalex.org/W2969362444",
    "https://openalex.org/W4388341803",
    "https://openalex.org/W3155917364",
    "https://openalex.org/W6601375367",
    "https://openalex.org/W4375869058",
    "https://openalex.org/W4386566590",
    "https://openalex.org/W4367680667",
    "https://openalex.org/W1981276685",
    "https://openalex.org/W6600445788",
    "https://openalex.org/W4301118033",
    "https://openalex.org/W2092346144"
  ],
  "abstract": "<title>Abstract</title> Traditionally, process-aware Decision Support Systems (DSSs) have been enhanced with AI functionalities to facilitate quick and informed decision-making. In this context, AI-Augmented Business Process Management Systems have emerged as innovative human-centric information systems, blending flexibility, autonomy, and conversational capability. Large Language Models (LLMs) have significantly boosted such systems, showcasing remarkable natural language processing capabilities across various tasks. Despite the potential of LLMs to support human decisions in business contexts, empirical validations of their effectiveness for process-aware Decision Support are scarce in the literature.In this paper, we propose the Business Process Large Language Model (BPLLM) framework, a novel approach for enacting actionable conversations with human workers. BPLLM couples Retrieval-Augmented Generation with fine-tuning, to enrich process-specific knowledge.Additionally, a process-aware chunking approach is incorporated to enhance the BPLLM pipeline. The approach has been evaluated in various experimental scenarios to assess its ability to generate accurate and contextually relevant answers to users' questions. The empirical study shows the promising performance of the framework in identifying the presence of particular activities and sequence flows within the considered process model, offering insights into its potential for enhancing process-aware DSSs.",
  "full_text": "Conversing with business process-aware Large\nLanguage Models: the BPLLM framework\nMario Luca Bernardi \nUniversity of Sannio\nAngelo Casciani \nSapienza University of Rome\nMarta Cimitile \nUnitelma Sapienza University\nAndrea Marrella \nUnitelma Sapienza University\nResearch Article\nKeywords: Business Process, Decision Support Systems, LLM, RAG\nPosted Date: March 20th, 2024\nDOI: https://doi.org/10.21203/rs.3.rs-4125790/v1\nLicense:   This work is licensed under a Creative Commons Attribution 4.0 International License.  \nRead Full License\nAdditional Declarations: No competing interests reported.\nVersion of Record: A version of this preprint was published at Journal of Intelligent Information Systems\non October 4th, 2024. See the published version at https://doi.org/10.1007/s10844-024-00898-1.\nConversing with business process-aware\nLarge Language Models: the BPLLM\nframework\nMario Luca Bernardi 1, Angelo Casciani 2*, Marta Cimitile 3,\nAndrea Marrella 2\n1 Department of Engineering, University of Sannio, Piazza Roma 21,\nBenevento, 82100, Italy .\n2 Department of Computer , Control and Management Engineering,\nSapienza University of Rome, Via Ariosto 25, Rome, 00185, Italy .\n3 Department of Law and Digital Society , UnitelmaSapienza, Piazza\nSassari, Rome, 00185, Italy .\n*Corresponding author(s). E-mail(s): angelo.casciani@uniroma1.it;\nContributing authors: bernardi@unisannio.it;\nmarta.cimitile@unitelasapienza.it; andrea.marrella@uniroma1.it;\nAbstract\nT raditionally , process-aware Decision Support Systems (DSSs) have been\nenhanced with AI functionalities to facilitate quick and informed decision-\nmaking. In this context, AI-Augmented Business Process Management Systems\nhave emerged as innovative human-centric information systems, blending ﬂexi-\nbility , autonomy , and conversational capability . Large Language Models (LLMs)\nhave signiﬁcantly boosted such systems, showcasing remarkable natural language\nprocessing capabilities across various tasks. Despite the potential of LLMs to\nsupport human decisions in business contexts, empirical validations of their effec-\ntiveness for process-aware Decision Support are scarce in the literature. In this\npaper , we propose the Business Process Large Language Model (BPLLM) frame-\nwork, a novel approach for enacting actionable conversations with human work-\ners. BPLLM couples Retrieval-Augmented Generation with ﬁne-tuning, to enrich\nprocess-speciﬁc knowledge. Additionally , a process-aware chunking approach is\nincorporated to enhance the BPLLM pipeline. The approach has been evaluated\nin various experimental scenarios to assess its ability to generate accurate and\ncontextually relevant answers to users’ questions. The empirical study shows the\n1\npromising performance of the framework in identifying the presence of particu-\nlar activities and sequence ﬂows within the considered process model, offering\ninsights into its potential for enhancing process-aware DSSs.\nKeywords: Business Process, Decision Support Systems, LLM, RAG\n1 Introduction\nAI-Augmented Business Process Management Systems (ABPMSs) represent new\nhuman-centered information systems characterized by great ﬂexibility , autonomy ,\nand huge conversational and self-improving capabilities [\n1]. In this context, the\ntraditional process-aware Decision Support Systems (DSS) are empowered with new\nAI functionalities for ensuring quick and quality decisions by understanding and\nexplaining the factors that led to the choices [\n2]. A huge impulse through these\nkinds of systems can be given by the adoption of Large Language Models (LLMs) [ 3].\nLLMs are emerging machine learning models showing a great capability to achieve\na variety of natural language processing tasks [\n4]. Given their numerous beneﬁts,\nthese models are increasingly more employed in several contexts [ 5] promising great\nbeneﬁts for industries and business functions and revolutionizing the way humans\ninteract with management systems [\n6]. However , LLMs have been transforming\nseveral organizations by driving them towards the paradigm of autonomous enter-\nprise, characterized by the automation of numerous activities and operations. In\nthis paradigm, ABPMSs play a pivotal role in supporting human tasks and deci-\nsions throughout all stages of the system life cycle. [\n6]. Starting from the business\nprocesses, LLMs are expected to embed the ability to overrun the local reasoning\ncontext, handle different scenarios, and improve the comprehension of the business\nactivities within their outcomes [\n6]. In front of the recognized potentiality of LLMs\nto assist human decisions in business context [ 1], this topic is few explored in litera-\nture [ 6] and to the best of our knowledge there is not an empirical validation about\nthe effectiveness of LLMs for process-aware decision support. Starting from these\nconsiderations, this study introduces a novel approach to business process analysis\nand description based on the adoption of Large Language Models to implement a\nconversational process-aware DSS. T o enhance the conversational abilities of a LLM\nfor answering business process-related tasks, we propose to adopt a process-aware\nRetrieval-Augmented Generation (RAG) framework in conjunction with ﬁne-tuning.\nThis strategy can leverage ﬁne-tuning to extend process- and domain-speciﬁc struc-\ntural and behavioral knowledge leaving the RAG in charge of incorporating the\ncontextual knowledge related to the speciﬁc user requests. The overall system,\ncalled Business Process LLM (BPLLM), ﬁne-tuned on a speciﬁc process model (dif-\nferent process model formats are evaluated), supports the user in a wide range of\nprocess comprehension and execution tasks using natural language. In the proposed\nframework, a LLaMA 2 [\n7] LLM is combined with a variant of RAG [ 8] [ 7] tailored\nto deal with the speciﬁc aspects of the structural and behavioral representation of\nbusiness processes. Moreover , a process-aware chunking approach [\n9] along with\na suitable prompting strategy is included. T o implement the BPLLM pipeline, we\n2\ntested different embedding models to investigate the most suitable ones. In addition\nto the BPLLM pipeline, we also propose to ﬁne-tune the LLM model to improve the\nknowledge of the process deﬁnition. The capability of BPLLM to generate accurate\nand contextually relevant answers to users’ questions about the process is evalu-\nated. This work investigates the following research questions aimed to evaluate the\nBPLLM performance within its components and its different settings:\nRQ1: How does the adoption of Retrieval-Augmented Generation (RAG), in the frame of\nBPLLM architecture, impacts the end-to-end performance?\nRQ2: What’s the inﬂuence of chunking and prompting on the performance of the BPLLM?\nRQ3: How does the choice of process representation format and related embedding model\naffect the performance of the BPLLM?\nRQ4: What is the effect of ﬁne-tuning along with the use of RAG on the performance of\nthe BPLLM? Does the number of processes included in the knowledge base of a single LLM\nimpact performance?\nThe evaluation is performed on a novel process model called Food Delivery .\nThe best BPLLM conﬁguration is also used on different models within their com-\nbinations to evaluate how the BPLLM performance changes when more process\nmodels are used.\nThe paper is structured into seven sections. Section\n2, reports a description of the\nrelated work. Section 3 reports background information useful to better understand\nand contextualize the proposed approach as reported in Section 4. Section 5 describes\nthe set of experiments carried out and discusses the results. Section 6 details the\nthreats to the validity of the study and, ﬁnally , the conclusions are discussed in\nSection\n7.\n2 Related work\nGreat interest has been pointed in the last years to DSSs given their utility to assist\nhumans in making high-quality decisions based on domain-speciﬁc information [\n10].\nStarting from the work proposed in [ 11], some studies are focused on the develop-\nment of recommendation services to drive decisions during process execution by\ngiving suggestions on possible next steps. For example, in [\n12] authors propose a\nrecommendation system able to suggest risk-informed decisions. Some additional\nstudies [\n13] propose the Markov Decision Process (MDP) [ 14] to identify the best\ndecision on the base of the probability distributions of all possible decisions. The\nMDP is also used in [\n14] to realize a DSS for declarative artifact-centric process mod-\nels. Other recent studies focused on the identiﬁcation of new DSSs [ 2] that radically\nevolve to capture the new opportunities derived by the emerging Artiﬁcial Intel-\nligence models [\n15, 16]. In this direction, authors in [ 2] introduce an end-to-end\nprocess-aware DSS able to predict the decisions and explain which factors inﬂuence\nthe prediction. This study , according to the recent literature is strongly focused on\nidentifying new methodologies based on the extraction of features from business\n3\nFig. 1 A process model by [ 22] speciﬁed in BPMN.\nobjects to perform business process prediction tasks. Differently from the described\nliterature, in this study , we focused on generative AI and conversational systems\napplied to process-aware support systems. The main idea is that the emerging LLM\ntechnologies can contribute to improving process-based decision systems. Accord-\ning to this in a recent work [\n17] the possible implications of conversational systems\napplied to the process of decision support are discussed. The authors highlight the\nimportance of transforming business questions and hypotheses written in natural\nlanguage, into executable speciﬁcations able to generate relevant reports for the busi-\nness stakeholders [\n18]. The same authors highlight that conversational systems can\nalso contribute to presenting the results of process mining analysis using a simple\nnatural language by creating a process description from an event log, or a concise\nreport describing the process performance [\n19– 21]. In front of this strong awareness\nabout the importance of exploring the new opportunities of LLMs in the business\nprocess communities, the implementation of the LLM-based approach in process-\nbased DSSs is still missed. According to this, the present study represents a novelty\nwith respect to the more recent literature since it introduces and experiments with an\nLLM model to improve the efﬁciency of DSS. The study also proposes, for the ﬁrst\ntime in the business process representation, the adoption of RAG integrated in the\nLLM model to ensure a greater adherence with the speciﬁc aspects of the business\nprocess domain. Finally , the impact of the proposed approach within its components\nis evaluated and discussed.\n3 Background\n3.1 Business Process models and description\nA process model serves as a representation of the business process structure, encom-\npassing the activities to be executed and the constraints governing their sequence.\nIt also encapsulates criteria indicating the start and termination of the process,\nalong with details concerning participants, IT systems, and data [\n23]. Several pro-\ncess models have been proposed in recent years [ 24] within their application in real\ndomains [ 25, 26]. Among the simplest notations for representing a business process\nlies the Directly-Follows Graph (DFG) [ 27]. This graphical representation comprises\n4\nnodes denoting individual activities, while directed arcs delineate the sequential\nrelationships between them, signifying direct succession.\nAnother common modeling language for business processes is Business Processing\nModeling Notation (BPMN). It is a standard language, proposed by the Object Man-\nagement Group (OMG), to design business processes [\n9]. BPMN deﬁnes a process\nmodel that includes a set of graphical constructs divided into: (i) ﬂow objects, (ii)\ndata, (iii) connecting objects, and (iv) swimlanes. Flow objects deﬁne the behavior of\na process, as the one reported in Figure 1. They can be classiﬁed into events, activities,\nand gateways. Events model the occurrence of states in the real world that are relevant\nfor processes and, more generally , anything that can happen instantaneously (e.g., an\ninvoice has been received). Activities represent units of work performed during pro-\ncesses that, differently from the events, have a certain duration (e.g., pay an invoice).\nGateways are used to represent the split and join behavior of the control ﬂow when\nthere is a need to model speciﬁc conditions like mutual exclusion or concurrence.\n3.2 Large Language Models and RAG\nLLMs represent a signiﬁcant advancement in natural language processing, lever-\naging deep neural network architectures to analyze and generate human-like text.\nThese models undergo extensive training on massive amounts of text data to learn\npatterns and entity relationships within natural language. Intending to predict and\ngenerate coherent language, LLMs estimate the likelihood of tokens or sequences\nof tokens occurring within larger sequences [\n28]. Over time, LLMs have evolved\nsigniﬁcantly , transitioning from earlier statistical methodologies to more sophisti-\ncated transformer-based architectures like BERT , GPT , and their successors [\n29].\nThese architectures are capable of generating entire documents and have fostered a\nsurge in research contributions across a spectrum of areas, including architectural\nenhancements, training methodologies, context expansion, ﬁne-tuning techniques,\nand the development of multi-modal LLMs [\n30]. With their escalating size, often\ncontaining billions of parameters, LLMs have witnessed substantial performance\nenhancements, enabling them to excel across diverse natural language processing\ntasks. Nonetheless, challenges persist in terms of contextual appropriateness and text\naccuracy in the output generated by LLMs [\n31]. These limitations can be attributed,\nin part, to outdated and biased knowledge embedded within their extensive training\ndatasets [\n32].\nFine-tuning has emerged as a pivotal technique for LLMs, enabling the cus-\ntomization of pre-trained models to suit speciﬁc tasks or domains. Indeed, while\npre-trained LLMs demonstrate strong performance across a wide range of tasks,\nthey may not always excel in speciﬁc domains with specialized requirements [\n33].\nFine-tuning addresses this limitation by updating the parameters of a pre-trained\nmodel using task-speciﬁc data, thereby adapting the model’s knowledge and capa-\nbilities to better address the nuances of a particular task. The ﬁne-tuning process\ntypically involves several key steps. First, a pre-trained LLM that better aligns with\nthe task requirements is selected. Next, it is acquired a dataset relevant to the tar-\nget task, which may include labeled examples or annotated text data, that will serve\nas the training data for ﬁne-tuning the LLM. During ﬁne-tuning, the pre-trained\nLLM’s parameters are adjusted accordingly to the task-speciﬁc dataset through a\n5\nprocess known as backpropagation [ 34]. This process involves iteratively updat-\ning the model’s parameters to minimize a deﬁned loss function, thereby optimizing\nthe model’s performance on the target task [\n35]. While ﬁne-tuning offers numer-\nous beneﬁts, it also comes with drawbacks, primarily due to its resource-intensive\nnature, necessitating substantial computational resources and data to be effective.\nHowever , these limitations can be mitigated through speciﬁc techniques such as\nParameter-Efﬁcient Fine-T uning (PEFT) [\n36]. This method employs various deep\nlearning techniques to minimize the number of trainable parameters while retain-\ning comparable performance to full ﬁne-tuning, updating only a limited number of\nadditional parameters or a subset of pre-trained parameters.\nAn encouraging solution to improve LLM accuracy and credibility (especially in\nknowledge-intensive tasks) and avoid the effort required by complex ﬁne-tuning is\nrepresented by the RAG [\n8] which is becoming a popular paradigm in LLM’s sys-\ntems. The underlying idea of the RAG approach is the merging of LLMs’ knowledge\nwith specialized, vast, and dynamic data coming from external repositories [\n8]. The\ninitial query prompts the external retrieval of pertinent information via search algo-\nrithms. The obtained information is then sent to the LLM’s prompts which provides\nfurther context information [\n32]. According to this, the RAG approach combines\ninformation retrieval mechanisms with In-Context Learning (ICL) [ 37] to improve\nthe LLM’s performance. The RAG approach includes a retriever and a generator [ 8]\nand consists of three steps (retrieve, augment, and generate). In the retrieval step, the\nuser query x is used to retrieve relevant context (text documents z) from an external\nknowledge source by the retriever pη (z|x) with parameters η returning distributions\nover text documents given x. Using an embedding model, the query is embedded\ninto a vector space and included as the additional context in the vector database.\nAccording to the similarities between vectors and query , the k closest documents\nfrom the vector database are selected. In the augment step, the initial query and the\nobtained additional context are combined into a prompt template. In the last step,\nthe retrieval-augmented prompt is fed to the LLM which generates an answer to the\nquestion on the base of the contextual information obtained by retrieved chunks.\n3.2.1 LLaMA\nLLaMA [\n7] is a set of foundation LLMs spacing from 7B to 65B parameters. They\nare trained on a huge amount of tokens coming out from public datasets obtain-\ning higher performance with respect to state-of-the-art approaches. Looking for an\nexample of the 65B parameters model, recent studies [\n7] show that at the higher end\nof the scale, it is competitive with Chinchilla or PaLM-540B LLMs. The training step\nis performed on large transformers by using a standard optimizer and the training\nstability is improved by normalizing the input of each transformer sub-layer . The\ntraining datasets are very heterogeneous spacing from different domains and includ-\ning all the publicly available datasets that have been used for LLM training. LLaMA\n2 represents a signiﬁcant breakthrough in the realm of open-source LLM, introduc-\ning a new family of pre-trained and ﬁne-tuned models with scales spanning from 7B\nto 70B parameters [\n38]. These models have demonstrated remarkable performance\non metrics pertaining to helpfulness and safety , outperforming existing open-source\ncounterparts and rivaling proprietary alternatives. Moreover , LLaMA 2 ensures\n6\nsafety through a range of measures, including safety-speciﬁc data annotation, red-\nteaming exercises, and iterative assessments. The training process for LLaMA 2\nadopts a multi-stage methodology , starting with the pre-training of the model using\nan extensive array of publicly accessible datasets. This initial phase plays an essen-\ntial role in instructing the model with a comprehensive understanding of linguistic\npatterns and structures. Subsequently , the model undergoes supervised ﬁne-tuning\ntailored speciﬁcally to accommodate dialogue-oriented applications, thereby reﬁn-\ning its responses and interactions with users. Furthermore, LLaMA 2 integrates\ninnovative techniques such as grouped-query attention and Reinforcement Learning\nwith Human Feedback (RLHF) techniques to improve its performance and safety .\n4 The Business Process LLM (BPLLM)\nIn this section, we present our BPLLM framework summarized in Figure\n2. The\nﬁgure shows the operational steps utilized by the BPLLM pipeline for answering\nqueries pertaining to business processes. The overarching workﬂow unfolds in three\nmajor phases as follows:\n• RAG Knowledge Augmentation : Initially , the BPLLM pipeline ingests the target\nprocess model, which undergoes the initial stage of chunking. Here, the process\nmodel representation is segmented into semantically meaningful chunks. Subse-\nquently , the embedding phase proceeds by processing the previously generated\nchunks and converting them into embeddings for indexing in the vector database.\nFollowing this, the produced embeddings are stored in the designated vector\ndatabase, to implement subsequent semantic searches based on cosine similarity .\n• LLM Fine-tuning : The chosen LLM undergoes a process of ﬁne-tuning using a\nsupervised dataset enriched with business process-speciﬁc information. This ﬁne-\ntuning procedure serves to enhance the LLM’s understanding of the process and\nto tailor it to the speciﬁc task of responding to inquiries pertinent to the business\nprocess model.\n• RAG Querying: At runtime, during the retrieving stage, a semantic search is con-\nducted to retrieve contextually relevant chunks of the process model from the\nvector database, based on the user query . These retrieved chunks are then com-\nbined with the input query to generate the prompt, which is sent to the LLM to\ngenerate the output. Finally , the crafted prompt, derived from the previous stage,\nis fed into the ﬁne-tuned LLM, which generates an answer containing grounded\ninformation concerning the process model.\nIn the following, we provide an in-depth discussion of each phase within the overall\nprocess. From a technological standpoint, the backbone of the pipeline was realized\nby leveraging the Langchain framework\n1 .\n4.1 RAG Knowledge Augmentation\nThe BPLLM Knowledge Augmentation is composed of the three stages that are\ndescribed in this section: ( i) Chunking, ( ii) Embedding, and ( iii) Storing.\n1 https://www .langchain.com/\n7\nVector Store\nEmbedding Model\nChunking Processor\nProcess Model\nChunks\nProcess Model\nEmbeddings\nFine-tuning\nLLM Fine-tuning (Offline)\nLarge Language Model\nSupervised\nDataset\nFine-tuned\nLarge Language Model\nAnswer\nRAG - Knowledge Augmentation (Offline)\nUser Question\nQuery\n          RAG - Querying (Online)\nEmbedding Model\nQuestion\nEmbedding\nRetrieved\nChunks\nPrompt\nProcess Model\nFig. 2 The BPLLM framework.\n4.1.1 Chunking\nThe initial step in the proposed BPLLM pipeline takes the business process model\nin input, aiming to generate multiple relevant chunks for easier comprehension by\nthe LLM in subsequent responses. W e considered both a natural language DFG\nrepresentation and a BPMN representation of the process model.\nDue to the limited context windows of LLMs, i.e., the number of tokens the\nLLM can receive in input to generate the output text, we are prevented from pass-\ning the entire process model in the query . For this reason, chunking is an essential\noperation for RAG and it consists in breaking down extensive contents into smaller ,\nmanageable segments. Efﬁcient chunking optimizes the relevance of responses by\nensuring that only pertinent context is provided to the LLM. Balancing performance\nand accuracy is crucial in determining the optimal chunking strategy , as excessive\nchunking can disrupt content ﬂow . Striving for meaningful chunks and trying to\navoid unnatural segmentation of the process model, different chunking strategies\nwere evaluated for each considered process model format. Referring to the DFG tex-\ntual process description, both ﬁxed-size chunking and recursive chunking have been\nconsidered. Recursive chunking divides text hierarchically using separators until\ndesired chunk sizes or structures are attained. Furthermore, ﬁxed-size chunking and\nrecursive chunking were evaluated when the BPMN process is used.\n8\nFig. 3 BPMN chunking example.\nBPMN-speciﬁc chunking\nFor the BPMN representation, an additional tailored chunking technique was\ndevised to intelligently segment the model without sacriﬁcing the semantics of the\nprocess model. Our chunking processor parses BPMN ﬁles by splitting them in cor-\nrespondence with relevant tags, while disregarding irrelevant graphical elements\nto preserve contextual relevance. Notably , it scans the BPMN ﬁle to isolate self-\ncontained semantic chunks deﬁned by the content within BPMN tags, subsequently\nadding them to the ﬁnal list to store in the vector database. An illustration of its exe-\ncution on a BPMN ﬁle is provided in Figure\n3. For instance, when processing the\ndepicted BPMN extract, the chunking processor identiﬁes the start of the <task>\ntag and generates a corresponding semantically meaningful chunk. It continues its\nanalysis by isolating chunks related to other elements such as the Exclusive Gateway\nand the Send T ask.\n4.1.2 Embedding\nThe embedding phase plays a crucial role in the representation of the input data,\naiming to transform the process model chunks generated in the chunking step\ninto process model embeddings. The process model chunks are transformed into a\nnumerical format that can be efﬁciently processed and stored in a vector database.\nNotably , the embedding phase transforms the raw input into dense, low-dimensional\nvectors, capturing semantic information and contextual relationships. These embed-\ndings serve as the foundation for downstream tasks, such as retrieval and generation,\nin the pipeline. The choice of embedding model signiﬁcantly impacts the effec-\ntiveness of the BPLLM pipeline. Each embedding model has its strengths and\nweaknesses, depending on the nature of the input data and the requirements of the\ndownstream tasks.\n9\nW e used various embedding models suitable for semantic representation obtain-\ning the best results with the all-MiniLM-L6-v22 model.\n4.1.3 Storing\nIn the storing phase of our pipeline, the embeddings of process model chunks are\nstored in a dedicated vector index . This vector index plays a crucial role in support-\ning similarity search operations and, consequently , in facilitating efﬁcient retrieval of\nprocess model chunks. In this study , the adoption of different vector index solutions\nhas been explored.\nAfter careful consideration, we opted for qDrant\n3 , an open-source vector\ndatabase designed for storing and querying high-dimensional embeddings. This\ndecision aligned with our broader objective of leveraging open-source technolo-\ngies throughout the pipeline implementation. Starting from qDrant, the vector index\nleveraging cosine similarity is used for semantic search. Cosine similarity is vastly\nused and well-suited for measuring the similarity between high-dimensional vec-\ntors, making it ideal for our use case in process-related information retrieval. Finally ,\nthe local connection to the qDrant vector index is obtained using gRPC\n4 as the\ncommunication protocol. This choice ensured efﬁcient and reliable communication\nbetween our pipeline and the qDrant database.\n4.2 LLM Fine-tuning\nT o perform the task of generating answers, we employed the\nLlama-2-13b-chat-hf\n5 language model, which is granted for use from Meta and\nHuggingFace. This model is well-suited for natural language understanding and\ngeneration tasks, making it an ideal choice for our querying phase.\nIn the direction of specializing the aforementioned LLM in providing grounded\nresponses, we leveraged the AutoT rain Advanced\n6 library from HuggingFace.\nNotably , we employed the PEFT methodology introduced in Section 3 to reduce\nthe computational cost associated with adapting the model to the task at hand,\nwhile still capitalizing on the main beneﬁts of ﬁne-tuning the LLM. Speciﬁcally , we\nﬁne-tuned the LLaMA 2-13B model in a supervised fashion using a dataset contain-\ning information about a speciﬁc business process model (see Section\n5 for more\ndetails). This dataset comprised process-speciﬁc questions paired with correspond-\ning binary answers pertaining to the structural and behavioral information of the\nprocess model. By structural information , we denote the presence of activities, events,\nand gateways within the process model, whereas behavioral information encompasses\ndetails concerning the sequence ﬂows linking these entities.\n4.3 RAG Querying\nThis phase includes the following steps: Retrieval and Answering.\n2 https://huggingface.co/sentence- transformers/all- MiniLM- L6- v2\n3 https://qdrant.tech/\n4 https://grpc.io/docs/what- is- grpc/introduction/\n5 https://huggingface.co/meta- llama/Llama- 2- 13b- chat- hf\n6 https://github.com/huggingface/autotrain- advanced\n10\n4.3.1 Retrieval\nIn this stage, the relevant process model chunks useful to generate accurate and\ncontextually relevant answers to the user queries are retrieved. This component has\nbeen implemented by integrating the vector store (qDrant) within a retriever compo-\nnent (the RetrievalQA chain provided by Langchain). The RetrievalQA chain serves\nas the backbone for our retrieval process. This chain combines advanced retrieval\ntechniques with question-answering capabilities, allowing us to efﬁciently extract\nrelevant information from process model data. The chain ﬁrst performs a retrieval\nstep to fetch relevant chunks, then it passes those textual segments into an LLM to\ngenerate a response.\nT o leverage qDrant within the RetrievalQA chain, a qDrant client needs to be\nconﬁgured as the retriever component. Such client interacts with the qDrant vector\nindex, querying it to retrieve process model chunks that are relevant to user queries.\nThrough experimental analysis, we also determined the optimal number of\nchunks that yields optimal results. This number was found to strike a balance\nbetween providing sufﬁcient context for generating accurate answers and respecting\nthe context window of the language model. Retrieving a sufﬁcient number of process\nmodel chunks is fundamental for ensuring that the retrieved content contains all the\nnecessary context for generating accurate answers.\n4.3.2 Answering\nIn the answering step, accurate and contextually relevant answers to user queries are\ngenerated. This phase is essential for providing grounded responses based on the\ninput query and the context extracted from relevant process model chunks retrieved\nin the previous phase, enhancing the overall effectiveness of the BPLLM pipeline.\nThe answering stage requires two main components: a LLM and the correspond-\ning tokenizer of the model. First, the prompt is crafted by combining the user query\nwith the contextually relevant chunks retrieved in the previous phase as reported in\nFigure\n4. The tokenizer has the task of preprocessing the prompt and converting it\ninto a format that can be understood by the model. T okenization involves breaking\ndown the input text into individual tokens and encoding them into numerical repre-\nsentations that the model can process. Once the prompt is prepared and tokenized,\nwe feed it into the LLM to generate responses. Relying on the context provided by\nthe retrieved process model chunks, the language model can generate answers that\nare not only accurate but also contextually relevant to the user query .\n5 Experimental evaluation\nIn this section, we discuss the experiments conducted to answer the research ques-\ntions reported in Section\n1. T o this end, in the following sub-sections, the process\nmodels used for the experiments, the experimental setup, and results are described.\n5.1 The adopted process models\nIn this work, we considered three distinct business processes. Speciﬁcally , the ﬁrst\nprocess is analyzed using textual DFG descriptions of activities and sequence ﬂows,\n11\nFig. 4 Example of the prompt crafted as input to the LLM.\nalongside the corresponding BPMN process model. In contrast, the remaining two\nprocesses are evaluated solely through their BPMN models.\nW e provide a brief overview of the business processes under consideration.\n• The initial process, called Food Delivery involves a food delivery system that\nreceives user orders as input and veriﬁes their status as existing clients. If they are\nnot present in the customer database, it creates an account for them. Subsequently ,\nthe system checks the speciﬁed payment methods, assigns the processed order\nto an accepting rider for delivery , and communicates the estimated waiting time\nto the customer . Upon the rider collecting the order , a notiﬁcation is sent to the\nclient, who, upon order receipt, notiﬁes the system accordingly . Finally , the rider\nis remunerated, concluding the process.\n• The second process, called E-commerce, mirrors the previous one. This business\nprocess centers on fulﬁlling orders placed by customers through an e-commerce\nplatform. Upon receipt of a new order , the system veriﬁes the customer ’s account\nstatus; if not present, it generates one and stores the payment methods. The system\nthen records the new order , veriﬁes courier availability for delivery , and selects\nthe optimal courier . Subsequently , the system assigns the package to the chosen\ncourier , and upon its conﬁrmation, calculates and communicates the delivery date\nto the customer . After the courier retrieves the package, a conﬁrmation is dis-\npatched to the customer , triggering a reciprocal notiﬁcation upon order receipt by\nthe customer . Following this, the customer completes and sends back a satisfaction\nquestionnaire, and the courier receives payment, thereby concluding the process.\n• The last business process, called Reimbursement, is entirely distinct from the previ-\nous ones and revolves around an expense reimbursement process. It begins with\nthe system’s reception of an expense report from an employee, followed by a noti-\nﬁcation to the employee. In the absence of an existing account, the system creates\none for the employee, and a clerk reviews the amount, reformulating it if necessary\nbefore submission to the manager . If the amount falls below a speciﬁed thresh-\nold, it is promptly approved; otherwise, if it exceeds this threshold, the manager\nevaluates it. In case of rejection, the process terminates immediately . However ,\nif approved, the corresponding amount is transferred to the applicant’s bank\n12\nFig. 5 Extacts of the DFG in natural language provided to the LLM.\naccount, accompanied by an approval notiﬁcation, thus successfully concluding\nthe process.\n5.2 Experimental setting\nThe proposed BPLLM system is based on generative models and hence it generates\nfeedback for the user in natural language. This kind of system requires a twofold\nvalidation based on both qualitative and quantitative aspects. For this reason, to eval-\nuate the effectiveness of BPLLM in helping users understand business processes, we\ndesigned an experiment covering these two different perspectives.\nMore precisely , the quantitative validation covers the correctness (in terms of\naccuracy) of the answers with respect to the entities and relationships that are both\nin the model provided by the user and in the LLM answer .\nConversely , the qualitative validation is based on the analysis from the user-\neffectiveness standpoint of experts judging the effectiveness of answers. W e elabo-\nrate on both these aspects formalizing the process.\nThe conclusion drawn from the study focuses on the overall utility of the BPLLM\nsystem in aiding business process users, discussing its potential applications in real-\nworld settings. Based on the outcomes of this experiment, recommendations for\nsystem improvements are suggested, along with proposals for further studies to\ncontinue validating and enhancing the system’s capabilities.\nIn the following, both quantitative and qualitative aspects are discussed.\nQuantitative evaluation\nDifferent experiments are proposed in this section to answer the research questions\nreported in Section\n1. The surrounding idea is to evaluate the BPLLM performance\nconsidering different aspects (RAG, chunking, embedding, and ﬁne-tuning). All the\nevaluations are performed using the Food Delivery process described in the previous\nsection. The Food Delivery process model is represented using the DFG in natural\nlanguage (in the following we use the term ”natural text” for brieﬂy). Figure\n5 illus-\ntrates the natural language description of the DFG, identifying the Food Delivery\nprocess fed to the LLM. The queries\n7 adopted in these experiments require to be\nanswered in order to recognize structural information and behavioral aspects within\nthe model. Figure 6 sketches the typology of checks performed on an excerpt of the\n7 https://tinyurl.com/t7dk7kwt\n13\nFig. 6 Prompts for structural and behavioral evaluation.\nFood Delivery reference process. As shown, for each kind of check (activity pres-\nence/absence and direct ﬂow) different prompts are generated to obtain a precise\nanswer from the model.\nSpeciﬁcally , for structural information correctness analysis, we queried the pres-\nence of speciﬁc activities within the business process model, prompting the pipeline\nto answer with a simple ”yes” or ”no” and to provide relevant contextual references\nif available.\nIn evaluating behavioral aspects, we formulated questions to determine the exis-\ntence of sequence ﬂows between designated activities, considering only one-way\ntransitions, and asked the LLM to indicate their presence in a binary way along with\ncontextual references if provided.\nAs previously , we examined all single-pass ﬂows existing within the process,\nan equal number of ﬂows from one activity to another existing in the process but\nnot directly linked, and an equivalent number of sequence ﬂows between activities\nabsent in the process model. Across the two evaluation phases, we checked both\nexisting and non-existing activities and sequence ﬂows to provide a comprehensive\nassessment of the framework.\nThe RQ1 aims to estimate the performance of the RAG-based LLM with respect to\nthe simple LLM to address the posed queries concerning the speciﬁed process model.\nT o this aim, we evaluated the efﬁciency and accuracy of LLaMA 2 13B compared to\nthe RAG-based LLM in the context of the Food Delivery process model.\nW e assess the effectiveness of LLaMA and the RAG-based LLM in describing\nbusiness process-related aspects based on accuracy metric. T o this end, we devised\n14\na methodology to evaluate the pipeline’s performance using binary response ques-\ntions (”yes” or ”no”), enabling a rigorous assessment of the responses generated by\nthe LLM (RAG-based LLM and LLaMA). Accuracy measures the percentage of exact\npredictions made by the LLM in addressing such inquiries out of the total number\nof expected answers. W e designate answers provided by the framework that align\nwith positive expected responses as true positives (TP), and those matching negative\nexpected responses as true negatives (TN). Conversely , false positives (FP) occur when\nthe pipeline yields positive responses contrary to the negative expected ones, while\nfalse negatives (FN) arise from the framework providing negative responses despite\npositive expectations. Thus, we deﬁned accuracy as follows:\nAccuracy = T P + T N\nT P + F P + T N + F N (1)\nRQ2 evaluates the effects of employing various chunking techniques within the\nBPLLM pipeline, alongside investigating how prompt engineering can further aug-\nment the performance of the framework. T o this aim, different chunking techniques\nare explored when the input is in natural text and BPMN format.\nIn the DFG case, ﬁxed-size chunking and recursive chunking with different sizes\n(16, 64, 128, 256, 512) are tested. For the BPMN format also a tailored chunking is\nevaluated. In each case, the accuracy (see Formula\n1) of the BPLLM in answering the\nqueries is evaluated. The RQ2 also aims to evaluate the effects of prompts reﬁnement\non the answer accuracy . T o this aim, for each question, two levels of reﬁnement have\nbeen considered (not reﬁned question, reﬁned question). The adopted reﬁned and\nnot reﬁned questions are reported in the replication package associated with this\nstudy .\nRQ3 examines the impact of employing different representations for the busi-\nness process models within the BPLLM pipeline, as well as whether different\nembedding models affect our pipeline. In this direction, the natural text and\nthe BPMN model representation for the Food Delivery process is evaluated as\ndescribed in the previous RQs. For each process model representation, three\ndifferent models are considered: all-MiniLM-L6-v2, bert-finetuned-bpmn,\nparaphrase-xlm-r-multilingual-v1.\nThe ﬁrst considered model is all-MiniLM-L6-v2\n8 model, which projects sen-\ntences into a 384-dimensional dense vector space.\nAnother model is bert-finetuned-bpmn9 . It is a ﬁne-tuned version of\nbert-base-cased, trained on a dataset comprising textual process descriptions.\nFinally , the paraphrase-xlm-r-multilingual-v110 maps sentences to a 768-\ndimensional dense vector space.\nRQ4 explores the effect of ﬁne-tuning along with the use of RAG on the per-\nformance of the BPLLM. T o address RQ4, we undertake ﬁne-tuning of the BPLLM\npipeline evaluating its impact on the overall accuracy of the BPLLM pipeline. T o\nthis aim the BPMN model representation for the Food Delivery process is evaluated\nas described in the previous RQs. T wo variants of ﬁne-tuning are explored in this\n8 https://huggingface.co/sentence- transformers/all- MiniLM- L6- v2\n9 https://huggingface.co/jtlicardo/bert- ﬁnetuned- bpmn\n10 https://huggingface.co/sentence- transformers/paraphrase- xlm- r- multilingual- v1\n15\nT able 1 Comparison of the evaluation results for pure LLM and RAG framework.\nMethodology Process Model Representation Accuracy\nPure LLM None 45.78%\nRAG-based LLM Natural T ext 79.52%\nRAG-based LLM BPMN 54.22%\nexperimentation, employing quantization with intervals of int4 and int8. Quantiza-\ntion refers to the process of reducing the precision of numerical values, often used\nto compress LLMs for more efﬁcient deployment. The main difference between int4\nand int8 quantization lies in the level of precision of the numerical values used to\nrepresent the model parameters, with int8 offering higher precision at the cost of\nincreased computational requirements.\nFinally , the RQ4 also aims to evaluate if the number of processes included in\nthe knowledge base of the LLM impacts the BPLLM performance. T o answer this\nquestion, starting from the best BPLLM conﬁguration, different combinations of pro-\ncesses (Food Delivery , Reimbursement, and E-commerce) are included in the initial\nknowledge base and the obtained BPLLM performances are evaluated.\nThe evaluation is conducted using an oracle speciﬁcally designed for this pur-\npose. The oracle receives the question and the expected binary answer as input,\ncompares it with the response generated by the LLM, and calculates accuracy as the\npercentage of correct results out of the total number of tests in the speciﬁc evaluation.\nIn our experimentation we found that by retrieving the top 20 chunks, we were\nalways able to capture a comprehensive overview of the processes model, enabling\nthe language model to generate informed responses.\nThe experiments were conducted on a workstation equipped with Linux/Ubuntu\n22.04.3 L TS operating system and powered by an NVIDIA A100 GPU.\n5.2.1 Qualitative evaluation\nSticking to the optimal BPLLM conﬁguration, we conducted an additional quali-\ntative assessment of the framework by posing more nuanced inquiries. This was\nundertaken to simulate real-world interactions that users may experience with the\ntool, aimed at gaining deeper insights into the underlying business process model or\naddressing speciﬁc issues related to their work.\nThe Food Delivery process, introduced in the preceding section, served as the\nfocal point for this qualitative evaluation.\n5.3 Evaluation results\nW e proceed to analyze the results obtained during the evaluation phase under\nvarious experimental conditions for each RQ introduced in Section\n1.\n16\nT able 2 Evaluation results after chunking and prompts reﬁnements.\nProcess Model Representation Chunking Prompts Accuracy\nNatural T ext No Chunking Not Reﬁned 79.52%\nNatural T ext Fixed Not Reﬁned 79.52%\nNatural T ext Fixed Reﬁned 84.34%\nNatural T ext Recursive Not Reﬁned 79.52%\nNatural T ext Recursive Reﬁned 84.34%\nBPMN No Chunking Not Reﬁned 54.22%\nBPMN Fixed Not Reﬁned 56.63%\nBPMN Fixed Reﬁned 56.63%\nBPMN Recursive Not Reﬁned 66.27%\nBPMN Recursive Reﬁned 73.49%\nBPMN BPMN-speciﬁc Reﬁned 91.57%\nRQ1: How does the adoption of Retrieval-Augmented Generation (RAG), in\nthe frame of BPLLM architecture, impacts the end-to-end performance?\nT able\n1 reports the accuracy obtained when pure LLM and RAG-based LLM are\nused on the Food Delivery model described in natural text and BPMN. It is note-\nworthy that, to adhere to the context window of the LLM, we removed the graphical\ninformation from the BPMN ﬁle, as it was irrelevant to our objectives. The table\nshows that a signiﬁcant enhancement in performance has been observed when RAG-\nbased LLM is used, aligning with our expectations. This improvement shows good\naccuracy ( 79,52 percent ) of the RAG-based LLM leveraging the natural language\nrepresentation to drive more informed and accurate decision-making.\nOur observations revealed instances of hallucination, wherein the pure LLM\nwould provide responses despite lacking pertinent information about the process\nmodel, occasionally asserting familiarity with certain activities even when such\nknowledge was absent.\nRQ2: What’s the inﬂuence of chunking and prompting on the performance of\nthe BPLLM?\nT able\n2 reports the accuracy evaluated on different process model representations\n(natural text and BPMN) and different chunking techniques (no chunking, ﬁxed-size,\nrecursive, BPMN-speciﬁc).\nWhen natural text is used, similar results are obtained by relying on ﬁxed chunk-\ning and recursive chunking. In both scenarios, the optimal chunk size is determined\nto be 128 tokens with a 10-token overlap. This ﬁnding can be attributed to the process\nmodel’s relatively small size, with its content nearly included within a single chunk.\nThis phenomenon also elucidates why the case of ”no chunking” yields similar\nresults.\nWhen the BPMN format is used, the results are quite different. However , a\ngreat difference in accuracy can be observed when different chunking models are\nemployed. This is in line with the idea that the process model size inﬂuences the\nchunking strategy since the BPMN model size is greater than the natural text model\nsize. The best results are obtained when applying the BPMN-speciﬁc chunking,\n17\nshowing remarkable accuracy ( 91.57 percent ). This conﬁrms the idea that a custom\ntechnique for chunking BPMN representations into semantically valid segments\nresulted in a signiﬁcant enhancement in accuracy for the BPLLM pipeline, under-\nscoring the efﬁcacy of chunking strategies tailored to the speciﬁc process model\nrepresentation. Moreover , favorable outcomes are obtained when ﬁxed chunking\nand recursive chunking are used. In both cases, the optimal chunk size is 32 tokens\nwith a 10-token overlap.\nFurthermore, T able\n1 reports, in the third column, the reﬁnement level ( not\nreﬁned and reﬁned ) of the adopted prompt. In all the cases, the reﬁned prompts\ndemonstrated greater accuracy in the answer . For BPMN model representation, the\ndifference in accuracy between the not reﬁned prompts and reﬁned prompts cases\nis signiﬁcant, exhibiting a high inﬂuence of the prompts reﬁnement level on the\nobtained results.\nRQ3: How does the choice of process representation format and related\nembedding model affect the performance of the BPLLM?\nIn tackling RQ3, we delved into the impact of the process representation format and\ncorresponding embedding model on the accuracy of the BPLLM pipeline. T able\n3\nsummarizes the insights collected to estimate the impact of process representation\nformat and embedding model on the BPLLM accuracy .\nSpeciﬁcally , we observed satisfactory outcomes with the natural language\nrepresentation. In this case, optimal performance ( 86,75 percent ) is attained\nwhen employing the embedding model tailored for BPMN terminology , namely\nbert-finetuned-bpmn\n11 . Conversely , for the BPMN process model, superior\nresults ( 91,57 percent ) are achieved by leveraging the all-MiniLM-L6-v2 model.\nIt is worth noticing that such results surpass even the performance of the BPLLM\npipeline relying on the natural language representation for the same business pro-\ncess. Additionally , we evaluated the paraphrase-xlm-r-multilingual-v1\n12\nmodel, which maps sentences to a 768-dimensional dense vector space. However ,\nwe did not observe any signiﬁcant improvements in the accuracy of our BPLLM\npipeline.\nBased on the obtained results, we generated t-SNE charts for the most effective\ncombinations, as depicted in Figure\n7. In the left chart, illustrating the embeddings\nderived from the bert-finetuned-bpmn model, noticeable proximity between the\nprompts (comprising the entire prompt containing the process-related query and the\nrelevant context retrieved from the vector database based on cosine similarity) and\nthe chunks of the natural text can be observed, thus conﬁrming the reliability of the\nresults. Similarly , the t-SNE on the right illustrates that the embeddings produced\nby the all-MiniLM-L6-v2 model are more suitable for the BPMN representation\nof the process model, as both the prompts and the BPMN chunks are concentrated\nwithin the same area.\n11 https://huggingface.co/jtlicardo/bert- ﬁnetuned- bpmn\n12 https://huggingface.co/sentence- transformers/paraphrase- xlm- r- multilingual- v1\n18\nT able 3 Evaluation results for different representations and embedding models.\nProcess Model Representation Embedding model Accuracy\nNatural T ext all-MiniLM-L6-v2 84.34%\nNatural T ext paraphrase-xlm-r-multilingual-v1 81.93%\nNatural T ext bert-ﬁnetuned-bpmn 86.75%\nBPMN all-MiniLM-L6-v2 91.57%\nBPMN paraphrase-xlm-r-multilingual-v1 86.75%\nBPMN bert-ﬁnetuned-bpmn 73.49%\nFig. 7 T-SNE charts depicting the embeddings of the prompts and chunks derived from the natural text\n(left) and the BPMN (right) process model. The embeddings are computed using BERT-ﬁne-tuned BPMN\nmodel (left) and all-MiniLM-L6-v2 (right).\nRQ4: What is the effect of ﬁne-tuning along with the use of RAG on the\nperformance of the BPLLM? Does the number of processes included in the\nknowledge base of a single LLM impact performance?\nThe outcomes of the experiments aimed to evaluate the inﬂuence of ﬁne-tuning\non the performance of the BPLLM are reported in T able\n4. The table shows the\nBPLLM accuracy for different PEFT quantization ( int4 and int8). W e considered in\nthe T able, for brieﬂy , the best conﬁguration of parameters (according to the results\ndescribed in the other subsections, i.e., BPMN as input, BPMN-speciﬁc chunking,\nall-MiniLM-L6-v2 as embedding model). The table reports, in the ﬁrst two rows,\nthe results when LLaMA2-13 is used as base LLM. In this case, the int4 quantiza-\ntion demonstrated superior performance ( 96,39 percent ) not only in terms of reduced\ncomputational costs but also in the ﬁnal accuracy of the BPLLM. The comparison\nof T able\n4 and the best case reported in T able 3 shows increased accuracy when\nﬁne-tuning is performed. Additionally , in the direction of discovering any potential\ninverse scaling phenomenon within our task, conducting ﬁne-tuning experiments\nwith the Llama-2-7b-chat-hf\n13 model under specular conditions.\n13 https://huggingface.co/meta- llama/Llama- 2- 7b- chat- hf\n19\nT able 4 Evaluation results for different PEFT quantization and LLM.\nBase LLM PEFT Quantization Accuracy\nLLaMA-2-13b-chat-hf int4 96.39%\nLLaMA-2-13b-chat-hf int8 93.98%\nLLaMA-2-7b-chat-hf int4 59.04%\nLLaMA-2-7b-chat-hf int8 57.83%\nT able 5 Evaluation results for enlarged knowledge bases.\nT ested Processes Fine-T uning Accuracy\nFood Delivery , Reimbursement No 87.04%\nFood Delivery , Reimbursement Y es 92.59%\nFood Delivery , E-commerce No 71.69%\nFood Delivery , E-commerce Y es 77.71%\nIndeed, conventional understanding suggests that LLMs generally show con-\nsistent performance improvements with increased scale. However , contrary to this\nexpectation, there are instances where this trend is reversed, indicating that mere\naugmentation of model scale does not necessarily translate to improved task perfor-\nmance. This phenomenon, illustrated in [\n31], underscores the importance of various\nfactors such as training objectives and data quality in optimizing the performance\nof LLMs. Unfortunately , our experiments with the LLaMA2-7B model did not yield\nfavorable outcomes (the accuracy is reported in the last two rows of T able\n4),\nhighlighting the complexity of optimizing LLM performance within the BPLLM\npipeline.\nContinuing our examination of the results, we delved into the potential effects\nof enlarging the knowledge base within the vector database by incorporating mul-\ntiple process models to provide to the LLM. Our investigation considered both\nprocess models having similar and different activities and objectives to assess the\nperformance implications on the BPLLM pipeline.\nInitially , we examined the impact of integrating two distinct business processes,\nnamely the previously introduced Food Delivery and Reimbursement process mod-\nels, into the knowledge base. Our ﬁndings indicated a slight degradation in the\nperformance of the BPLLM pipeline. However , through ﬁne-tuning the LLM on both\nmodels, the adverse effects were mitigated, demonstrating improved accuracy .\nConversely , when incorporating two similar process models, such as Food Deliv-\nery and E-commerce, we observed a more signiﬁcant decline in accuracy for both\nthe base BPLLM pipeline and the pipeline utilizing the ﬁne-tuned LLM. This out-\ncome was expected, as the LLM’s decision-making process can be inﬂuenced by\nthe similarities in activity names between the process models, leading to potential\nmisinterpretations.\nT able\n5 reports the results of the previously discussed experiments. These ﬁnd-\nings underscore the importance of carefully managing the size and composition\nof the knowledge base within the LLM, particularly when incorporating multiple\n20\nFig. 8 Results from the qualitative evaluation.\nprocess models with varying degrees of similarity . Additionally , our results high-\nlight the potential beneﬁts of ﬁne-tuning the LLM to enhance its adaptability and\nperformance across diverse process contexts.\n5.3.1 Qualitative evaluation discussion\nThe capability of the BPLLM framework to respond appropriately to less structured\nand nuanced inquiries is also evaluated. Different questions are made to the BPLLM\nby a group of potential users involved in this experimentation. Figure\n8 illustrates the\nsame examples of the BPLLM’s responses to such questions. The users, based on the\nobtained answers posit that the framework exhibits the ability to provide satisfactory\nresponses promptly , thereby establishing its reliability for everyday usage.\n6 Threats to validity\nSome considerations about the threats to the validity of the study are reported in this\nsection. The ﬁrst point (threat to internal validity) concerns of discussion regards the\nchoice to use in our BPLLM approach open LLM models since there are closed mod-\nels (like ChatGPT) largely recognized by the community that can give good results.\nAccording to this, we consider that in a research setting the adoption of a closed\nmodel is impractical due to ﬂexibility issues, the high costs of these solutions, and the\npossibility to control and evaluate the development process. Referring to the choice\nof adopting open LLM models, another internal validity concern regards the rapid\nadvancement of LLM models (new versions are developed in a few months) that\nmakes the obtained results outdated in time. T o counter this, in the proposed BPLLM,\nthe more recent open LLM models of the LLaMA family are evaluated. However , the\nconsidered BPLLM solution can be easily adapted to new overruling LLaMA models.\nAn external validity to this study is also given to the difﬁculty in replicat-\ning the proposed experiments. However , LLMs can give different responses to the\nsame prompts and there is a way to avoid this. T o overcome this critical issue, the\nset of adopted prompts is made available online and as the possible answer only\nthe ”yes” or ”not” are evaluated. This makes the experiments more repetible and\nreduces the differences due to how questions or commands are phrased. Moreover ,\nwe performed multiple runs along the experiment to ensure the correctness of the\nanswer .\n21\nFinally , referring to the external validity , we will focus on ensuring the generaliz-\nability of our results by extending the experimentation on a larger number of process\nmodels.\n7 Conclusion\nIn this paper , we introduced BPLLM, a novel methodology aimed at enabling\ngrounded conversations with a LLM to enhance process-aware DSSs. The framework\nis designed for the analysis and description of business processes, boosting the con-\nversational capabilities of LLMs in addressing process-related tasks. It achieves this\nby integrating a RAG framework to ingest contextual knowledge relevant to speciﬁc\nuser queries with ﬁne-tuning, thus extending process-speciﬁc structural and behav-\nioral knowledge. In this way , BPLLM ﬁne-tuned on a designated process model\ncan assist users in various process comprehension and execution tasks through nat-\nural language interactions. Furthermore, we introduced a process-aware chunking\napproach and evaluated the BPLLM pipeline using different embedding models\nto identify the most suitable ones. Additionally , we proposed ﬁne-tuning the LLM\nmodel to enhance the understanding of process deﬁnitions. W e assess BPLLM’s\ncapability to generate accurate and contextually relevant responses to user queries\nabout processes. Lastly , we conducted an evaluation of the proposed approach across\nvarious business process models, examining its performance with an expanded\nknowledge base.\nAs part of our future work in the realm of process discovery [\n25], we aim to\ninclude an analysis of the execution aspects of business processes in the study . This\nanalysis will involve considering data pertaining to the execution times and costs of\nactivities, enabling us to offer valuable insights into process execution to users.\nAdditionally , we plan to explore various embedding models to enable the com-\nputation of proximity or distance between distinct execution traces within an event\nlog generated during the execution of a business process.\nFuture research directions also encompass enhancing the retrieval phase by\ninvestigating the integration of knowledge graphs into our approach to determine if\nthey can improve such step.\nUltimately , exploring the combination of the BPLLM framework with symbolic\nAI solvers may represent another interesting future work, aiming to incorporate\naspects of reasoning into the methodology .\nReferences\n[1] Dumas, M., Fournier , F ., Limonad, L., Marrella, A., Montali, M., et al.: AI-\naugmented Business Process Management Systems: A Research Manifesto.\nACM T rans. Manage. Inf. Syst. 14(1) (2023)\nhttps://doi.org/10.1145/3576047\n[2] Agarwal, P ., Gao, B., Huo, S., Reddy , P ., et al. : A Process-A ware Decision Sup-\nport System for Business Processes. In: Proceedings of the 28th ACM SIGKDD\nConference on Knowledge Discovery and Data Mining. KDD ’22, pp. 2673–\n2681. Association for Computing Machinery , New Y ork, NY , USA (2022).\nhttps:\n//doi.org/10.1145/3534678.3539088\n22\n[3] Casciani, A., Bernardi, M.L., Cimitile, M., Marrella, A.: Conversational Systems\nfor AI-Augmented Business Process Management. In: Proceedings of the 18th\nResearch Challenges in Information Science (RCIS 2024) (2024)\n[4] Ozkaya, I.: Application of Large Language Models to Software Engineering\nT asks: Opportunities, Risks, and Implications. IEEE Software 40(3), 4–8 (2023)\nhttps://doi.org/10.1109/MS.2023.3248401\n[5] Ray , P .P .: ChatGPT: A comprehensive review on background, applications, key\nchallenges, bias, ethics, limitations and future scope. Internet of Things and\nCyber-Physical Systems 3, 121–154 (2023)\n[6] Fahland, D., Fournier , F ., Limonad, L., Skarbovsky , I., et al.: How well can large\nlanguage models explain business processes? (2024)\n[7] T ouvron, H., Lavril, T ., Izacard, G., Martinet, X., et al.: LLaMA: Open and\nEfﬁcient Foundation Language Models (2023)\n[8] Lewis, P ., Perez, E., Piktus, A., Petroni, F ., et al. : Retrieval-Augmented Genera-\ntion for Knowledge-Intensive NLP T asks. In: Proceedings of the 34th Interna-\ntional Conference on Neural Information Processing Systems. NIPS’20. Curran\nAssociates Inc., Red Hook, NY , USA (2020)\n[9] Object Management Group: Business Process Model and Notation (BPMN),\nV ersion 2.0 (2011).\nhttp://www .omg.org/spec/BPMN/2.0\n[10] Mozannar , H., Sontag, D.: Consistent Estimators for Learning to Defer to an\nExpert (2021)\n[11] Schonenberg, H., W eber , B., Dongen, B., Aalst, W .: Supporting Flexible Processes\nthrough Recommendations Based on History . In: Business Process Manage-\nment, pp. 51–66. Springer , Berlin, Heidelberg (2008)\n[12] Conforti, R., Leoni, M., Rosa, M.L., Aalst, W .M.P ., Hofstede, A.H.M.: A rec-\nommendation system for predicting risks across multiple business process\ninstances. Decis. Support Syst. 69, 1–19 (2015)\n[13] Bennett, C.C., Hauser , K.: Artiﬁcial intelligence framework for simulating clin-\nical decision-making: A Markov decision process approach. Artif. Intell. Med.\n57(1), 9–19 (2013)\nhttps://doi.org/10.1016/j.artmed.2012.12.003\n[14] V oorberg, S., Eshuis, R., Jaarsveld, W ., Houtum, G.-J.: Decision Support for\nDeclarative Artifact-Centric Process Models. In: Business Process Management\nForum, pp. 36–52. Springer , Cham (2019)\n[15] Bansal, G., Nushi, B., Kamar , E., W eld, D.S., et al. : Updates in human-AI\nteams: understanding and addressing the performance/compatibility tradeoff.\n23\nIn: Proceedings of the Thirty-Third AAAI Conference on Artiﬁcial Intelli-\ngence and Thirty-First Innovative Applications of Artiﬁcial Intelligence Con-\nference and Ninth AAAI Symposium on Educational Advances in Artiﬁcial\nIntelligence. AAAI’19/IAAI’19/EAAI’19 (2019).\nhttps://doi.org/10.1609/aaai.\nv33i01.33012429\n[16] Ali, R., Hussain, A., Nazir , S., Khan, S., Khan, H.U.: Intelligent Decision\nSupport Systems: An Analysis of Machine Learning and Multicriteria Decision-\nMaking Methods. Applied Sciences 13(22) (2023)\nhttps://doi.org/10.3390/\napp132212426\n[17] Chapela-Campa, D., Dumas, M.: From process mining to augmented process\nexecution. Software and Systems Modeling 22(6), 1977–1986 (2023) https://doi.\norg/10.1007/s10270-023-01132-2\n[18] Berti, A., Schuster , D., Aalst, W .M.P .: Abstractions, Scenarios, and Prompt\nDeﬁnitions for Process Mining with LLMs: A Case Study (2023)\n[19] Fontenla-Seco, Y ., Lama, M., Bugar ´ın, A.: Process-T o-T ext: A Framework for\nthe Quantitative Description of Processes in Natural Language, 212–219 (2021)\nhttps://doi.org/10.1007/978-3-030-73959-1 19\n[20] Aa, H., Carmona, J., Leopold, H., et al. : Challenges and opportunities of apply-\ning natural language processing in business process management. In: COLING\n2018 : The 27th International Conference on Computational Linguistics, Pro-\nceedings of the Conference, August 20-26, 2018, Santa Fe, New Mexico, USA,\npp. 2791–2801. Association for Computational Linguistics, ACL, Stroudsburg,\nP A (2018).\nhttps://madoc.bib.uni-mannheim.de/58335/\n[21] L ´opez, A., S `anchez-Ferreres, J., Carmona, J., Padr ´o, L.: From Process Models\nto Chatbots. In: International Conference on Advanced Information Systems\nEngineering (2019).\nhttps://api.semanticscholar .org/CorpusID:169031222\n[22] Lohrmann, M., Reichert, M.: Effective application of process improvement pat-\nterns to business processes. Software & Systems Modeling 15(2), 353–375 (2016)\nhttps://doi.org/10.1007/s10270-014-0443-z\n[23] Dumas, M., La Rosa, M., Mendling, J., Reijers, H.A., et al.: Fundamentals of\nBusiness Process Management, Springer (2013)\n[24] V aisman, A.: An Introduction to Business Process Modeling, pp.\n29–61. Springer , Berlin, Heidelberg (2013). https://doi.org/10.1007/\n978-3-642-36318-4 2\n[25] Bernardi, M.L., Cimitile, M., Di Francescomarino, C., Maggi, F .M.: Using dis-\ncriminative rule mining to discover declarative process models with non-atomic\nactivities. Lecture Notes in Computer Science (including subseries Lecture\nNotes in Artiﬁcial Intelligence and Lecture Notes in Bioinformatics) 8620 LNCS,\n281–295 (2014)\nhttps://doi.org/10.1007/978-3-319-09870-8 21\n24\n[26] Agostinelli, S., Luzi, F .D., Canito, U., et al. : A data-centric approach to design\nresilient-aware process models in BPMN. In: Business Process Management\nForum - BPM 2022 Forum, M ¨unster , Germany , September 11-16, 2022, Proceed-\nings. Lecture Notes in Business Information Processing, vol. 458, pp. 38–54.\nSpringer (2022).\nhttps://doi.org/10.1007/978-3-031-16171-1 3\n[27] van der Aalst, W .M.P .: A practitioner ’s guide to process mining: Limitations\nof the directly-follows graph. Procedia Computer Science 164, 321–328 (2019)\nhttps://doi.org/10.1016/j.procs.2019.12.189 . CENTERIS 2019 - International\nConference on ENTERprise Information Systems / ProjMAN 2019 - Inter-\nnational Conference on Project MANagement / HCist 2019 - International\nConference on Health and Social Care Information Systems and T echnologies,\nCENTERIS/ProjMAN/HCist 2019\n[28] Zhao, W .X., Zhou, K., Li, J., T ang, T ., et al.: A survey of large language models.\narXiv preprint arXiv:2303.18223 (2023)\n[29] V aswani, A., Shazeer , N., Parmar , N., Uszkoreit, J., et al.: Attention is all you\nneed. Advances in neural information processing systems 30 (2017)\n[30] Naveed, H., Khan, A.U., Qiu, S., Saqib, M., et al.: A Comprehensive Overview\nof Large Language Models (2024)\n[31] McKenzie, I.R., Lyzhov , A., Pieler , M., Parrish, A., et al.: Inverse Scaling: When\nBigger Isn’t Better. arXiv preprint arXiv:2306.09479 (2023)\n[32] Gao, Y ., Xiong, Y ., Gao, X., Jia, K., et al.: Retrieval-Augmented Generation for\nLarge Language Models: A Survey (2024)\n[33] Xu, L., W ang, W .: Improving aspect-based sentiment analysis with contrastive\nlearning. Natural Language Processing Journal 3, 100009 (2023)\n[34] Hosseini, M.T ., Ghaffari, A., T ahaei, M.S., Rezagholizadeh, M., et al. : T owards\nFine-tuning Pre-trained Language Models with Integer Forward and Backward\nPropagation. In: Findings of the Association for Computational Linguistics:\nEACL 2023, pp. 1867–1876 (2023)\n[35] Xu, L., Xie, H., Li, Z., W ang, F .L., et al. : Contrastive learning models for sentence\nrepresentations. ACM T ransactions on Intelligent Systems and T echnology\n14(4), 1–34 (2023)\n[36] Xu, L., Xie, H., Qin, S.-Z.J., T ao, X., W ang, F .L.: Parameter-efﬁcient ﬁne-tuning\nmethods for pretrained language models: A critical review and assessment.\narXiv preprint arXiv:2312.12148 (2023)\n[37] Dong, Q., Li, L., Dai, D., et al.: A Survey on In-context Learning (2023)\n[38] T ouvron, H., Martin, L., Stone, K., Albert, P ., et al.: Llama 2: Open Foundation\nand Fine-T uned Chat Models (2023)\n25",
  "topic": "Process (computing)",
  "concepts": [
    {
      "name": "Process (computing)",
      "score": 0.5546754002571106
    },
    {
      "name": "Process management",
      "score": 0.532051682472229
    },
    {
      "name": "Computer science",
      "score": 0.5268995761871338
    },
    {
      "name": "Business process",
      "score": 0.4586929678916931
    },
    {
      "name": "Business",
      "score": 0.3239448368549347
    },
    {
      "name": "Programming language",
      "score": 0.1928541660308838
    },
    {
      "name": "Work in process",
      "score": 0.15955206751823425
    },
    {
      "name": "Marketing",
      "score": 0.05999493598937988
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I16337185",
      "name": "University of Sannio",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I861853513",
      "name": "Sapienza University of Rome",
      "country": "IT"
    },
    {
      "id": "https://openalex.org/I4210130905",
      "name": "Unitelma Sapienza University",
      "country": "IT"
    }
  ]
}