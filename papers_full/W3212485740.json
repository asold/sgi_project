{
    "title": "Hyperparameter Power Impact in Transformer Language Model Training",
    "url": "https://openalex.org/W3212485740",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A3214639421",
            "name": "Lucas Høyberg Puvis de Chavannes",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A3214658916",
            "name": "Mads Guldborg Kjeldgaard Kongsbak",
            "affiliations": [
                "IT University of Copenhagen"
            ]
        },
        {
            "id": "https://openalex.org/A3214059828",
            "name": "Timmie Rantzau",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2003830579",
            "name": "Leon Derczynski",
            "affiliations": [
                "IT University of Copenhagen"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W3001279689",
        "https://openalex.org/W1673310716",
        "https://openalex.org/W2963809228",
        "https://openalex.org/W2965373594",
        "https://openalex.org/W4287726890",
        "https://openalex.org/W2769900398",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2896457183",
        "https://openalex.org/W3156686209",
        "https://openalex.org/W3094045953",
        "https://openalex.org/W4287993739",
        "https://openalex.org/W2113207845",
        "https://openalex.org/W3103412034",
        "https://openalex.org/W3034340181",
        "https://openalex.org/W4302023899",
        "https://openalex.org/W3034936478",
        "https://openalex.org/W3012320681",
        "https://openalex.org/W2995647371",
        "https://openalex.org/W3008851394",
        "https://openalex.org/W2106411961",
        "https://openalex.org/W4297736277",
        "https://openalex.org/W3106210592",
        "https://openalex.org/W2983981554",
        "https://openalex.org/W3133702157",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W3004006153",
        "https://openalex.org/W2799042347"
    ],
    "abstract": "Training large language models can consume a large amount of energy. We hypothesize that the language model's configuration impacts its energy consumption, and that there is room for power consumption optimisation in modern large language models. To investigate these claims, we introduce a power consumption factor to the objective function, and explore the range of models and hyperparameter configurations that affect power. We identify multiple configuration factors that can reduce power consumption during language model training while retaining model quality.",
    "full_text": "Proceedings of the 2nd Workshop on Simple and Efﬁcient Natural Language Processing, pages 96–118\nNovember 10, 2021. ©201 Association for Computational Linguistics\n96\nHyperparameter Power Impact\nin Transformer Language Model Training\nLucas Høyberg Puvis de Chavannes\nIT University of Copenhagen\nM47 Labs\nlucas.puvis@m47labs.com\nMads Kongsbak\nIT University of Copenhagen\nmkon@itu.dk\nTimmie Mikkel Rantzau Lagermann\nIT University of Copenhagen\ntimn@itu.dk\nLeon Derczynski\nIT University of Copenhagen\nld@itu.dk\nAbstract\nTraining large language models can consume\na large amount of energy. We hypothesize that\nthe language model’s configuration impacts its\nenergy consumption, and that there is room for\npower consumption optimisation in modern\nlarge language models. To investigate these\nclaims, we introduce a power consumption fac-\ntor to the objective function, and explore the\nrange of models and hyperparameter configu-\nrations that affect power. We identify multi-\nple configuration factors that can reduce power\nconsumption during language model training\nwhile retaining model quality.\n1 Introduction\nLarge language models have pushed the boundaries\nof accuracy and performance in various NLP tasks,\nat the cost of energy efficiency. This is due to the in-\ncreasing amount of compute time and power needed\nto train these models ( Amodei, 2018), thus increas-\ning the amount of energy the computers training\nthe models need to consume.\nThe Robustly Optimized BERT approach\n(RoBERTa) (Liu et al. , 2019) achieved this by im-\nproving the Bidirectional Encoder Representations\nfrom Transformers (BERT) ( Devlin et al. , 2019) in\nmultiple ways, such as increasing the training time\nthrough more epochs and a larger amount of data,\nwith BERT already requiring 1507 kWh of electric-\nity and emitting 652 kg of CO2. Other strategies,\nsuch as NAS, an English to German machine trans-\nlation model, consumed 656,347 kWh of electricity,\ncorresponding to 248,019 kg of CO2 (Strubell et al.,\n2019). While these models show great potential, it\ncomes at the cost of high CO2 emissions. The core\nissue is that the electricity consumed is not guar-\nanteed to be environmentally friendly, and often\ncomes from sources such as coal or gas. According\n: These authors contributed to the paper equally.\nto Strubell et al. (2019), we must cut CO2 emis-\nsions by half to slow natural disasters. However,\nmuch research in the field ignores the perspective\nof energy efficiency. When looking at papers from\nthree top AI conferences, namely ACL, NeurIPS,\nand CVPR, work tends to focus on accuracy rather\nthan efficiency, or a mixture (Schwartz et al., 2019).\nAn added benefit to developing more energy-\nefficient models is a reduced barrier of entry to NLP\nresearch. Researchers with good ideas may not be\nable to execute those ideas, given that state-of-the-\nart results are locked behind large-scale compute\n(Strubell et al. , 2019; Bender et al. , 2021).\nThis study investigates how to reduce power con-\nsumption in training transformer language models.\nWe seek to address the issue of high-power models\nby analysing the resulting models’ hyperparameters,\nenergy consumption, and perplexity and providing\ninitial parameter guidelines for low-power, high-\nperformance transformers, and an opening into the\nresearch of low-power transformers.\nOur research question is: How can we reduce the\nenergy consumption of models to both lower the\nbarrier of entry and reduce CO2 emissions, while\nstill keeping an effective model?\nFollowing Strubell et al. (2019), a possible ap-\nproach to this problem could be the use of Bayesian\nhyperparameter search. Throughout our work, we\nmanaged to identify hyperparameter configurations\nthat provide strong entries for both perplexity and\nenergy consumption. These configurations were\nfound through our methodology utilising Bayesian\noptimisation, by combining the libraries Hyperopt\nand PyTorch. The optimal configurations collec-\ntively spanned the identified Pareto frontier.\n2 Related Work\nThere is a body of work on making language models\nand transformers efficient, for varying definitions\nof efficiency, but we’re not aware of any that algo-\n97\nrithmically integrate power consumption into the\nloss function or architecture search.\nThe closest related work is that on task-specific\nnetwork reduction and on low-power language\nprocessing. The former can be achieved through\ndistillation, pruning, quantisation, or all of the\nabove. For example, Wasserblat et al. (2020) re-\nduce trained BERT models in size by orders of\nmagnitude while retaining task performance. Sim-\nilarly, Kim et al. (2019) present highly efficient\nnetworks that as a result can process translations\nvery quickly. However, all these techniques require\nthe training of a large network first, thus only offer-\ning power savings at inference time. Furthermore,\nKaplan et al. (2020) presents scaling laws for neural\nlanguage models, which can assist in more efficient\ntraining when applied.\n3 Method\nOur general approach is as follows. We specify a\ndataset, task, objective function, and hyperparame-\nter space. We then explore hyperparameter space,\nrepeatedly training models over the same data and\nevaluating them in terms of task performance and\npower consumption. This exploration optimises for\ngood task performance and low power consumption,\nbut is limited to a certain volume of model config-\nurations. Once complete, we analyse these model\nconfigurations further, investigating per-epoch per-\nformance, and common factors in high and low\npower consumption and task performance.\n3.1 Data\nThe dataset chosen is theCC-News dataset (Macken-\nzie et al. , 2020). This is a subset of the English-\nlanguage portion of the entire CC-News dataset.\nThis specific set of data was chosen due to it be-\ning partially what RoBERTa was trained on, and\nhaving the longer document length typical to the\nnews genre. Only the first 100 000 examples are\nused as train, primarily chosen in an attempt to keep\nthe energy consumption of the trained models to\na minimum. Each document comes with multiple\ndata fields, with only the text field being used for\ntraining. The reduced amount of data introduces\nan assumption that these results will scale, which\nwe address later in the paper when investigating\ncommon factors in efficient and inefficient models.\n3.2 Task: Language modelling\nThe task used for this paper is Masked Language\nModelling, also referred to as Masked LM or MLM.\nThe procedure is very simple: mask some words in\nthe input sentences with the token ( [MASK]), and\nthen attempt to predict what these words are. An\nexample of such a sentence is “The borders of Paris\nare [MASK]”, where [MASK] is the word to be\npredicted. Perplexity is a widely used metric for\nthe evaluation of language models. Low perplexity\nmeans better performance, which makes it a useful\nmetric to evaluate language models in general. We\nuse perplexity as defined by huggingface, as the\nexponentiated average negative log-likelihood of a\nsequence (HuggingFace, 2021).\n3.3 Perplexity-Energy Product\nWe used a simple multiplication of the total\nperplexity and energy consumption of a model,\nmperplexity, menergy to act as the return value,\nPerplexity-Energy Product (PEP), to be reduced:\nPEP = mperplexity · menergy (1)\nWhere mperplexity is the perplexity of the trained\nLM, and menergy is the energy consumption for\ntraining the LM measured by Carbontracker ( An-\nthony et al. , 2020). We chose to call this return\nvalue, PEP, as shown in the equation . The reason\nfor using a composite expression of both perplexity\nand energy is to make the optimization focus on\nparameters that affect both of these. We chose to\nmultiply the two values as we hypothesised it would\npunish high values a lot more since multiplication\nis a lot more violent than addition. An issue with\nthis is that as soon as either value is below 1, the\nloss is simply a linearly scaled-down expression\nof the other number. A worst-case scenario would\nbe that Hyperopt would prioritise optimising one\nvalue and neglect the other, but the resulting loss\nwould still be acceptable for the optimiser. A lower\nPEP value is better, since the aim is to minimize\nthe energy cost and the perplexity.\nThere are underlying issues with assessing both\nthe quality and efficiency of models in a single\nmetric. One of the issues is that when efficiency is\nbelow one kilowatt-hour, the resulting PEP value\nwill essentially be a scaled-down perplexity. So\npotentially, a model with a good balance between\nefficiency and quality would have a higher PEP\nvalue than a model that is very efficient, but lacking\nin quality. Additionally, using a single metric of the\n98\nParameter Interval\nvocab_size [1,30522]\nhidden_size_multiplier [1,100]\nnum_hidden_layers [1,12]\nnum_attention_heads [1,18]\nintermediate_size [1,3072]\nhidden_act (relu, silu, gelu, gelu_new)\nhidden_dropout_prob [0.1,1]\nattention_probs_dropout_prog [0.1,1]\nmax_position_embeddings [512,512]\ntype_vocab_size [1,1]\ninitializer_range [0.02,0.02]\nlayer_norm_eps [1.00E-12,1.00E-12]\nposition_embedding_type (absolute, relative_key,\nrelative_key_query)\nTable 1: Hyperopt parameter search space\nproduct of cost and quality makes an assumption\n- that changes in either component have a linear\nimpact. This could be regulated by adding a weight\nto both the metrics in order to regulate the difference\nof change within the respective metric.\n3.4 Hardware\nThe experiment platform was V100 GPUs on i7\nCPUs running over a SLURM service. As stated\nby (Lasse, 2021), Carbontracker also works slurm,\nonly measuring GPU devices available for the given\njob, so the reported consumption figures are con-\nsistent but could be slight underestimates.\n3.5 Hyperparameters\nTo investigate hyperparameter impact on Trans-\nformer model power use, we chose to specifically\nlook at the parameters related to its size, such as the\nnumber of its hidden layers or number of attention\nheads, alongside a few key parameters such as the\ntype of positional encoding, activation functions\nand dropout probabilities. The parameters picked\nwere chosen on the assumption that they were the\nones most likely to affect both model perplexity,\nand model energy consumption during training.\nTable 1 shows our final search space and parame-\nter value intervals. Please note that the hidden_size\nof the model is given as:\nhsize = hsize_mult · #Aheads (2)\nWhere hsize is hidden size, hsize_mult is hidden\nsize multiplier and # Aheads is number of attention\nheads. This is due to the constraint that the hidden\nsize of the model has to be a multiple of the number\nattention heads in the model ( Vaswani et al., 2017).\nThis leaves a parameter space of size 1026 between\n[1, 1800], most of which are centred around the\nlower end of the interval.\n3.6 Search algorithm\nWe use Bayesian optimisation to traverse hyperpa-\nrameter space in search of low-power transformer\ntraining configurations. Bayesian optimisation is\nsuited to cases where one wishes to find optimal\nparameter configurations, for some definition of op-\ntimal, but individual trials can be expensive. This\nmakes it a good match for transformer hyperparam-\neter tuning. Search is executed in parallel over mul-\ntiple GPUs and GPU hosts. As noted in ( Bergstra\net al., 2011), “The consequence of parallelization\nis that each proposal x∗ is based on less feedback.\nThis makes search less efficient, though faster in\nterms of wall time” . Research indicates Bayesian\nhyperparameter search techniques are more effi-\ncient than brute force techniques ( Strubell et al. ,\n2019), such as grid search; this family is often the\nleast efficient in terms of time-to-viable-solutions.\nFurther, our general focus on energy efficiency mo-\ntivates choosing efficient search algorithms.\nSamples follow a uniform distribution. This was\na deliberate choice, as we have no knowledge over\nwhich parameters would perform best. We hypoth-\nesised giving a uniform distribution for all param-\neters would yield the best results. The specific\nalgorithm used to minimise the loss is the Tree of\nParzen Estimators (TPE) (Bergstra et al. , 2013).\n3.7 Model Selection and Per-epoch\nMeasurements\nHyperopt was left to run over the data and hyper-\nparameters for a fixed number of days. Models\nwere trained with a batch size of 2, over 3 epochs.\nThe batch size was chosen due to stability, as the\nrelatively low number of epochs could make the\nresults unstable. According to Masters and Luschi\n(2018), the most stable and best generalization re-\nsults have been obtained with a batch size of 32 or\nsmaller, but the best results were with batch sizes\nas low as 2. We also logged all of the parameter\nconfigurations alongside the energy consumption\nand perplexity of each model. The result was 154\ndifferent hyperparameter configurations.\nWe then retrained the 154 hyperparameter con-\nfigurations we found over 10 epochs to see a further\nevaluation of how each model would evolve per\nepoch in terms of power consumption and perfor-\nmance. For each model, a callback implementing\nCarbontracker was used to gather data about the\n99\nenergy consumption after each epoch, and the same\ncallback was used to log the perplexity of each\nmodel. Another callback was created to save a\nmodel for each epoch, resulting in 1540 different\nmodels being saved. Each model was trained on a\nsingle Tesla V100 32GB graphics card.\n4 Results\n4.1 Correlation Matrices\nThe majority of the result section is going to utilise\ncorrelation matrices to analyse the data. We have\ntwo categorical data entries, namely the activation\nfunction and the position embedding type. For this,\nwe use categorical correlation to showcase both\nthe activation function and the position embedding\ntype alongside the rest of the data. The primary\nreason for using correlation matrices is its ability\nto quickly visualise and pinpoint patterns in the\ndata. We have three different ways of evaluating\nthe different hyperparameters, namely with (i) en-\nergy consumption, (ii) perplexity, and (iii) PEP\nvalue, and therefore correlation matrices make it\neasy to visualise the hyperparameters. In general,\nthe primary concern is to see correlations between\nthe evaluation methods and hyperparameters.\n4.2 PEP\nTable 2 contain overviews of the best 15% of mod-\nels and worst 15% of models, for PEP value, with re-\ngard to average parameter size. A count of what po-\nsition embedding type and activation function was\nused can be found in table 3 The models were sorted\nby lowest PEP value, and the best 15% and worst\n15% were chosen. Table 2 presents an overview\nof average energy consumption and perplexity of\nthe best 15% of models, to compare with the worst\n15% of models in terms of PEP value.\nThe data of the best 15% of models is introduced\nto analyse tendencies that provide the PEP values.\nThe data of the worst 15% of models is introduced\nto analyse what not to do when choosing parame-\nters for a new model. Three different correlation\nmatrices, for all models, for the best and worst 15%\nof models are given in Appendix A.\n5 Energy\nThe data here is presented in the same format as\nthe previous section. The table 4 show findings of\nthe best 15% models in terms of low energy con-\nsumption, and the worst 15%, alongside several\ncorrelation matrices in appendix B. Furthermore,\nFigure 1: The Pareto curve for epoch 10. An animated\nvisualisation of all 10 epochs can be found at this link.\nAs these results are extracted from models that have\nbeen trained through hyperopt with a specific loss\nfunction, the resulting parameters are not chosen\nto achieve the lowest energy consumption possi-\nble, but rather the lowest PEP-value. These results\ncan then possibly indicate which parameters can be\ntweaked to reduce energy consumption specifically,\nwhile retaining some performance.\n5.1 Identifying optimal models\nWhile PEP is a suitable metric, we also want to be\nable to identify models which can’t be optimised\nfurther with respect to perplexity or energy con-\nsumption without a penalty in the other. Therefore\nwe identified the Pareto-optimal models.\nThe next graphs, Figure 1 showcases energy con-\nsumption related to perplexity for each model evolv-\ning over the 10 epochs. The colour of each dot\nrepresents a specific model throughout all the mod-\nels, hence it is possible to see how each model\nprogresses throughout the graphs. Besides a visual-\nisation of how each model evolves, the graph also\nhighlights the Pareto curve for each epoch, which is\nan indicator of the best-performing models for each\nepoch. While some of these best models might not\nhave a particularly good PEP value, they are still\na part of the Pareto curve, and thus can’t improve\neither perplexity or energy consumption without a\ndecrease in the other.\nFurthermore, as can be seen in figure 1, there\nare a few models which permanently reside at 2000\nperplexity. These are all models which have low\nvocab size, high dropout probabilities, low hidden\nsize, or a combination of the three. This, combined\nwith a relatively high number of hidden layers and\n100\nBest 15% Mean Best 15% Std. Dev Worst 15% Mean Worst 15% Std. Dev\nvocab_size 19458.52 5741.79 16072.78 9451.63\nactual_hidden_size 273.17 101.14 670.52 468.80\nnum_hidden_layers 1.91 0.92 6.52 3.24\nnum_attention_heads 8.17 4.13 10.47 4.70\nintermediate_size 716.17 531.30 1237.08 581.32\nhidden_dropout_prob 0.18 0.06 0.52 0.24\nattention_probs_dropout_prog 0.25 0.10 0.46 0.26\nenergy comsumption 1.70 0.51 5.70 3.11\nperplexity 27.23 7.80 1809.32 501.77\nTable 2: Mean and standard deviation of hyperparameters for the best 15% and worst 15% of models wrt: PEP\nvalue.\nBest 15% Count Worst 15% Count\nrelative_key_query 20 7\nrelative_key 0 8\nabsolute 3 8\nGELU 14 8\nGELU_new 2 7\nReLU 1 5\nSiLU 6 3\nTable 3: Count of activation functions and position em-\nbedding types wrt: PEP value.\nattention heads result in models which have a hard\ntime learning.\n6 Analysis\nFor the analysis, we start with the most trivial op-\ntimisation steps and progress towards less trivial\nones.\n6.1 Parameter correlations\nWhen looking at the best 15% models in terms of\nlow PEP value, analysing the resulting correlation\nmatrix given in Figure 3 in appendix A can give\nus insight into whether our approach has resulted\nin a good balance between energy consumption\nand perplexity. It can also give clues as to what\nparameter values can be chosen to reduce a mod-\nels energy consumption without affecting perplex-\nity. Additionally, we define a good balance for a\nhyperparameter between energy consumption and\nperplexity as the absolute value of the correlation\none parameter to perplexity roughly equalling the\nabsolute value of the correlation between the same\nparameter and energy consumption. Specifically:\n|corrparamn,P P L| ≈ | corrparamn,energy |\nThe correlation between perplexity and energy\nconsumption is at -0.88, which indicates that the\nuse of hyperopt and our PEP loss function has given\na good balance between the quality and energy use\nduring optimisation. The three key parameters ap-\npear to be the number of hidden layers, the hid-\nden activation function and the position embedding\ntype. Hidden layer count has a correlation of 0.71\nand -0.64 between energy consumption and per-\nplexity respectively, this being the hyperparameter\nwith the greatest impact on both of these values.\nActivation function also has a high impact, again\nwith -0.63 and 0.68 with energy consumption and\nperplexity respectively ( Derczynski, 2020). Lastly,\nthe positional embedding is at 0.49 and -0.65 re-\nspectively. As in Table 2 and 3, the optimal number\nof hidden layers averaged out at 1.91. The most\nused activation functions and positional encodings\nare GELU and relative_key_query and 14 and 20\nappearances. It is important to note that as the\npositional encoding defines the way attention is cal-\nculated, there may be an underlying link between\nthe number of attention heads and encoding choice.\nIt is important to note that correlation is a useful\ntool to find linear tendencies. While a correlation\nclose to negative and positive indicates a correlation\nbetween the two values, a value of zero doesn’t\nguarantee a lack of correlation since there could\nstill exist a non-linear correlation.\n6.2 What predicts a good or bad model?\nLooking at Table 2, the overall tendency is that\nthe models, on average, are much smaller than the\nconfig of RoBERTa BASE (Liu et al., 2019). The\nnumber of hidden layers in our best models in terms\nof PEP value is on average 2, down from 12. With\nhidden layers having a correlation of 0.71 with en-\nergy consumption and -0.64 with perplexity, as\nseen in Figure 3, it is by far one of the most volatile\nparameters to adjust. Increasing the number of hid-\n101\nBest 15% Mean Best 15% Std. Dev Worst 15% Mean Worst 15% Std. Dev\nvocab_size 21187.73 6426.71 18545.04 9141.88\nactual_hidden_size 116.43 84.20 727.78 27.33\nnum_hidden_layers 1.52 0.77 7.82 3.26\nnum_attention_heads 8.08 5.27 12.78 4.48\nintermediate_size 890.47 554.19 1149.65 739.19\nhidden_dropout_prob 0.37 0.23 0.40 0.15\nattention_probs_dropout_prog 0.28 0.14 0.47 0.28\nenergy comsumption 0.99 0.17 6.18 1.93\nperplexity 338.51 576.74 1236.04 962.74\nTable 4: Mean and standard deviation of hyperparameters for the best 15% and worst 15% of models wrt: energy\nconsumption.\nBest 15% Count Worst 15% Count\nrelative_key_query 15 14\nrelative_key 4 6\nabsolute 4 3\nGELU 3 11\nGELU_new 3 5\nReLU 6 3\nSiLU 11 4\nTable 5: Count of activation functions and position em-\nbedding types wrt: energy consumption.\nden layers will increase energy consumption, but\ndecrease perplexity. Interestingly, with it having a\ncorrelation of 0.056 to PEP value, it could suggest\nthat hyperopt has found a good compromise in the\nnumber of hidden layers that gives a good balance\nbetween low energy consumption and perplexity.\nThe number of attention heads is at 8, which is\nhigher than what our initial assumption was, but\nwith a deviation of 4.13, it varies a lot from model\nto model. It is important to note that this parameter\nis dependent on the type of positional embedding\nused, as the way attention is calculated is heavily\ndependent on it.\nWhile these models have good performance, with\nthe energy consumption averaging at 1.70 and per-\nplexity at 27.23, it is important to note that these two\nmetrics have a negative correlation with each other\nof -0.88. If one is reduced, the other one increases.\nThis could suggest that we have hit a point where\nwe can no longer make our models smaller without\nseverely affecting our end performance. RoBERTa\nreported perplexities between 3.68 and 3.99 in ta-\nble 3 in ( Liu et al. , 2019). While our perplexities\non average are roughly 7.4 times higher, both our\namount of training data and model size are vastly\nsmaller by a factor of 10, thus probably leading to\na shorter downstream task fine-tuning time, and as\na result, lower energy consumption.\nOn the opposite end of the spectrum are the worst\n15% of the models in terms of PEP value. We\nassume that these models would be bigger, both\nin terms of hidden layers, their hidden size, and\nintermediate size, as these would most likely result\nin longer training times than smaller ones, thus\nconsuming more energy. Comparing Table 2 to\nthe best 15% supports this analysis. The number\nof hidden layers has increased from an average\nof 1.91 to 6.52, hidden size from 273 to 670, and\nintermediate size from 716 to 1237. The energy\nconsumption is indeed also higher, as it has gone\nfrom 1.7 kWh to 5.7 kWh. The perplexity is also\nvery bad, being at an average of 1809.\n6.3 Reducing LLM training power\nconsumption without reducing quality\nThere is also a slight correlation between hidden\ndropout probability and perplexity, but there is al-\nmost no correlation between that probability and\nenergy consumption. These correlations suggest\nthat a low hidden dropout probability results in a\nlower perplexity, without impacting the energy con-\nsumption of the models.\nThe top 15% of models at η = 10(Figure 3) indi-\ncate interesting correlations. This matrix suggests\na stronger negative correlation between energy con-\nsumption and perplexity. These results indicate\nthat it is hard to reduce both perplexity and en-\nergy consumption at the same time, since lowering\none tends to increase the other - a trade-off. This\ncould be because the models are already perform-\ning decently well and have reached the point where\nsubsequent iterations bring diminishing returns in\nperplexity, while still incurring a linear increase in\nenergy consumption. Looking at the models, 10 of\nthe 23 models in the best 15% appear on the Pareto-\n102\noptimal curve as seen in Figure 1. This suggests\nthat the remaining 13 models are very close to be-\ning optimal, whereas the 10 models which appear\non the Pareto curve already are. The same trend\nhappens in the number of hidden layers, where the\ncorrelations indicate that increasing the number of\nhidden layers will also increase energy consump-\ntion, but will decrease perplexity. Interestingly, the\ncorrelation previously seen with the hidden dropout\nprobability has disappeared. This indicates that the\nbest-performing models already have a low enough\nhidden dropout probability for this correlation to no\nlonger be the case. When looking at the data, the\nentire dataset features an average hidden dropout\nprobability of 0.385, where the best 15% of mod-\nels have an average of 0.189, which supports the\nprevious indication.\n6.4 Different parameters have different\neffects in different realms\nMost of our well-performing models – no matter\nif looking only at energy, perplexity, or PEP value\n– use the relative key query positional embedding.\nWhile this is a slightly more complex calculation\nfor self-attention, this suggests that it doesn’t have\na huge impact on energy consumption, while it has\na noticeable impact on perplexity. Relative key\nquery introduces another 143K parameters but is\nrelatively insignificant when compared to the 103M\nBERT already has ( Huang et al. , 2020). While\nour dataset contains 107 models with “relative key\nquery” as the positional embedding type, when\nlooking at the worst-performing models, the dis-\ntribution looks close to uniform, at least when look-\ning at perplexity and PEP. Furthermore, Table 13\nshows the distribution in the first cluster, which\nfeatures 94 out of the 107 models with relative key\nquery, further strengthening the claim.\nA similar trend appears for the choice of ac-\ntivation functions, with GELU being the best-\nperforming activation function for our best mod-\nels in terms of perplexity and energy loss. If one\nwants an activation function purely for energy effi-\nciency, SiLU is the most prevalent activation func-\ntion among the models with the lowest power con-\nsumption. When looking at Table 13, it’s seen\nthat GELU and SiLU appear the most frequent, but\nReLU is still close behind, in line with previous\nresults (Derczynski, 2020).\nOur findings were focused on hyperparameter\noptimization, and no compression of the LMs at all.\nJacobsen et al. (2021) provide methods for measur-\nEnergy Consumption\nHyperopt (1 Epoch) 50.53 kWh\nHyperopt (3 Epochs) 187.57 kWh\n10 Epochs 393.56 kWh\nAd-hoc tests 62.42 kWh\nTotal 694.10 kWh\nTable 6: Energy consumption for this work\ning model size. Li et al. (2020) show that training\nlarger models for longer times and then compress-\ning them leads to more efficient training. Though\nthe previous analysis on layers and attention heads\nis less relevant for this approach, our findings re-\ngarding loss functions and embedding types should\nhold if choosing to train a larger model and then\ncompress.\n6.5 Our energy consumption\nThis paper investigates alternatives to high-\nperformance, high-energy consumption top-of-the-\nline models by showing that other approaches can\nprovide acceptable performance while cutting the\nsize and training time. As this topic is relevant due\nto the environmental impact it can have, such as\naccelerating rising sea levels ( Veng and Andersen ,\n2020), and as training NLP models has an actual im-\npact on the environment due to high energy usage,\nreducing their consumption is, as a topic, both im-\nportant and very unexplored. This also means that\nwe, as the authors, have to stress the importance\nof noting that this is not a complete guide on how\nto create low-power, low-perplexity transformer\nmodels. As an example, reducing the number of\nhidden layers might have a positive effect in certain\nmodels, but not necessarily in others - one has to\nlook at all the model parameters together.\nTable 6 shows the total amount of energy we’ve\nspent training our various models. As the price\nof 1 kWh in Denmark is roughly €0.28 ( Eurostat,\n2021), the total amount of money we spent training\nall of our models totals roughly €195 in electric-\nity. While not a lot in comparison to the models\npresented in table 3 in Strubell et al. (2019), our\nmodels are also trained on a smaller dataset, and\nsome of them have a configuration that slims down\ntheir size. In terms of watt-hour usage, the number\nof models we trained consumed half the amount of\nenergy that the BERT base did. As a comparison,\n700 kWh can power a Tesla Model 3 for a whole\n4337 kilometres (standard range of 16kWh per 100\nkilometres ( Wikipedia, 2021)), which is roughly\n103\nfrom Copenhagen to Barcelona and back.\n6.6 Limitations\nIn this subsection, we discuss possible limitations\nof our study and indicate where future work could\nexplore further.\nGiven the fact that this work was done on a small\nsubset of the CC-News dataset, the best performing\nmodels may be smaller models where the small\namount of data is not a downside. If the size of\nthe corpus is increased, it would give space for\nthe larger models to learn more, and maybe per-\nform better than the smaller models deemed as the\nmost efficient. Even though other efficient mod-\nels might be uncovered, this will not change the\ncorrelation between the number of hidden layers\nand efficiency. There is also a small possibility\nthat perplexity scores are inflated, as CC-News was\nused to pre-train RoBERTa (Liu et al. , 2019), and\nthis could occur non-linearly across hyperparam-\neter configurations – but we expect the relations\nbetween network architecture components, power\nconsumption, and performance to hold relative to\neach other overall.\nWe also note that this work was monolingual and\nfocused on English, to the exclusion of other lan-\nguages (Bender, 2019). While transformer-based\nLLMs seem to exhibit some properties in com-\nmon across natural languages, there are also many\nlanguage-specific considerations (e.g. for Finnish,\nVirtanen et al. (2019)). Our results cannot be guar-\nanteed to generalise across other natural languages.\nGiven the fact that a heterogeneous cluster was\nutilized for training all of the models, there is no\nguarantee that the precise hardware configuration\nwas used to train all 154 models. All compute nodes\nhave the same type of graphics card but contain dif-\nferent server-grade CPUs. Since CPU and DRAM\ncontribute to power consumption ( Anthony et al.,\n2020), the variations in clock speed and core count\ncould potentially have a small effect on the final\nenergy consumption.\nWe used a relatively low epoch count for the\nsearch of hyperparameter space. This might not\nreflect high-epoch hyperparameter optima, and so\nthe relations between perplexity scores are at best\nadvisory. However, power consumption tends to\nbe stable per epoch, and so this component of the\nfunction performs well.\nWe evaluate using perplexity, which has its own\nissues and is unlikely to approximate many NLU\nmetrics. However, it does lend itself well to our\nexploratory compound loss function, retains agnos-\nticity regarding the many possible NLU tasks (such\nas those in SuperGLUE). The correlations between\nevaluated language model performance and their\nfinal performance are not of maximum certainty;\nhowever, these results can guide hyperparameter\nsearch/tuning in terms of good balances, and we\nhope to see much more research in this area of trans-\nformer efficiency and quality/energy use trade-off.\nWe fixed the epoch count for studies. An alter-\nnative would be to fix the FLOPS available. Gor-\ndon et al. (2018) present a regulariser that opti-\nmises FLOPS usage. Because perplexity tends to\nbe asymptotic with zero for useful models, making\nabsolute differences between model scores tend to\nbe smaller as time goes on. This makes the metric\na little noisy and differences hard to note. Fixed\nFLOPS allowances will lead to high epoch counts\nfor smaller models, increasing the risk of model per-\nformances being hard to tease apart. Thus, fixing\nepochs instead of FLOPS during exploration gives\nfiner granularity for very small, or very efficient\nmodels, though FLOPS is a closer approximation\nto power consumption.\nAs NVIDIA (2021) highlights, some size multi-\npliers are more efficient when it comes to matrix-\nmatrix multiplication. Allowing the search space\nas denoted on table 1 to take non-efficient values\ncan affect reproducibility because of hardware id-\niosyncrasies. While general trends can be extracted,\nprecise figures might not generalise well beyond\nthe test setup.\n7 Conclusion\nWe investigated how hyperparameters affected\nthe power consumption and model quality of\ntransformer-based language models. This paper has\npresented a method for low-power investigation of\nhyperparameter tuning, integrating power consump-\ntion measurement. We identified factors that in-\ncrease power without giving quality and identified\nfactors that increase quality without taking power.\nThere are many possible extensions to this work\nand it is our hope that power consumption measure-\nment will be improved and will be integrated with\nmore architecture searches during model training.\n104\nReferences\nDario Amodei. 2018. AI and Compute. https://ope-\nnai.com/blog/ai-and-compute/.\nLasse F. Wolff Anthony, Benjamin Kanding, and\nRaghavendra Selvan. 2020. Carbontracker: Track-\ning and Predicting the Carbon Footprint of Training\nDeep Learning Models. arXiv:2007.03051 [cs, eess,\nstat]. ArXiv: 2007.03051.\nEmily M Bender. 2019. The #benderrule: On naming\nthe languages we study and why it matters. The Gra-\ndient, 14.\nEmily M. Bender, Timnit Gebru, Angelina McMillan-\nMajor, and Shmargaret Shmitchell. 2021. On the\ndangers of stochastic parrots: Can language models\nbe too big?\n . In Proceedings of the 2021 ACM\nConference on Fairness, Accountability, and Trans-\nparency, FAccT ’21, page 610–623, New York, NY,\nUSA. Association for Computing Machinery.\nJames Bergstra, Rémi Bardenet, Yoshua Bengio, and\nBalázs Kégl. 2011. Algorithms for hyper-parameter\noptimization. In Proceedings of the 24th Interna-\ntional Conference on Neural Information Processing\nSystems, NIPS’11, page 2546–2554, Red Hook, NY,\nUSA. Curran Associates Inc.\nJames Bergstra, Daniel Yamins, and David Cox. 2013.\nMaking a science of model search: Hyperparameter\noptimization in hundreds of dimensions for vision ar-\nchitectures. In International conference on machine\nlearning, pages 115–123. PMLR.\nLeon Derczynski. 2020. Power consumption vari-\nation over activation functions. arXiv preprint\narXiv:2006.07237.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2019. BERT: Pre-training\nof Deep Bidirectional Transformers for Language\nUnderstanding. arXiv:1810.04805 [cs] . ArXiv:\n1810.04805.\nEurostat. 2021. Electricity price statistics .\nAriel Gordon, Elad Eban, Ofir Nachum, Bo Chen, Hao\nWu, Tien-Ju Yang, and Edward Choi. 2018. Mor-\nphnet: Fast & simple resource-constrained structure\nlearning of deep networks. In Proceedings of the\nIEEE conference on computer vision and pattern\nrecognition, pages 1586–1595.\nZhiheng Huang, Davis Liang, Peng Xu, and Bing Xi-\nang. 2020. Improve Transformer Models with Better\nRelative Position Embeddings . arXiv:2009.13658\n[cs]. ArXiv: 2009.13658.\nHuggingFace. 2021. Perplexity of fixed-length models.\nMagnus Jacobsen, Mikkel H Sørensen, and Leon Der-\nczynski. 2021. Optimal size-performance trade-\noffs: Weighing PoS tagger models. arXiv preprint\narXiv:2104.07951.\nJared Kaplan, Sam McCandlish, Tom Henighan,\nTom B. Brown, Benjamin Chess, Rewon Child,\nScott Gray, Alec Radford, Jeffrey Wu, and Dario\nAmodei. 2020. Scaling Laws for Neural Language\nModels. arXiv:2001.08361 [cs, stat] . ArXiv:\n2001.08361.\nYoung Jin Kim, Marcin Junczys-Dowmunt, Hany\nHassan, Alham Fikri Aji, Kenneth Heafield, Ro-\nman Grundkiewicz, and Nikolay Bogoychev. 2019.\nFrom research to production and back: Ludicrously\nfast neural machine translation. In Proceedings of\nthe 3rd Workshop on Neural Generation and Trans-\nlation, pages 280–288.\nLasse. 2021. lfwa/carbontracker. Original-date: 2020-\n04-21T12:01:38Z.\nZhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin,\nKurt Keutzer, Dan Klein, and Joseph E. Gonza-\nlez. 2020. Train Large, Then Compress: Rethink-\ning Model Size for Efficient Training and Inference\nof Transformers . arXiv:2002.11794 [cs] . ArXiv:\n2002.11794.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A Robustly Optimized BERT Pretrain-\ning Approach . arXiv:1907.11692 [cs] . ArXiv:\n1907.11692.\nJ. Sander M. Ester, H. P. Kriegel and X. Xu. 1996. A\ndensity-based algorithm for discovering clusters in\nlarge spatial databases with noise .\nJoel Mackenzie, Rodger Benham, Matthias Petri, Jo-\nhanne R Trippas, J Shane Culpepper, and Alistair\nMoffat. 2020. CC-News-En: A large English news\ncorpus. In Proceedings of the 29th ACM Inter-\nnational Conference on Information & Knowledge\nManagement, pages 3077–3084.\nDominic Masters and Carlo Luschi. 2018. Revisit-\ning Small Batch Training for Deep Neural Networks.\narXiv:1804.07612 [cs, stat] . ArXiv: 1804.07612.\nNVIDIA. 2021. Dl performance matrix multiplication .\nRoy Schwartz, Jesse Dodge, Noah A. Smith, and Oren\nEtzioni. 2019. Green AI . arXiv:1907.10597 [cs,\nstat]. ArXiv: 1907.10597.\nEmma Strubell, Ananya Ganesh, and Andrew McCal-\nlum. 2019. Energy and Policy Considerations for\nDeep Learning in NLP . In Proceedings of the 57th\nAnnual Meeting of the Association for Computa-\ntional Linguistics, pages 3645–3650, Florence, Italy.\nAssociation for Computational Linguistics.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N. Gomez, Lukasz\nKaiser, and Illia Polosukhin. 2017. Attention Is\nAll You Need . arXiv:1706.03762 [cs] . ArXiv:\n1706.03762.\n105\nTadea Veng and Ole Andersen. 2020. Consolidating\nsea level acceleration estimates from satellite altime-\ntry. Advances in Space Research .\nAntti Virtanen, Jenna Kanerva, Rami Ilo, Jouni Luoma,\nJuhani Luotolahti, Tapio Salakoski, Filip Ginter, and\nSampo Pyysalo. 2019. Multilingual is not enough:\nBert for finnish. arXiv preprint arXiv:1912.07076 .\nMoshe Wasserblat, Oren Pereg, and Peter Izsak. 2020.\nExploring the boundaries of low-resource bert distil-\nlation. In Proceedings of SustaiNLP: Workshop on\nSimple and Efficient Natural Language Processing ,\npages 35–40.\nWikipedia. 2021. Tesla Model 3 . Page Version ID:\n1024502085.\n106\nA PEP\nFigure 2, 3, and 4 showcase the correlation matrices\nof the 154 trained models, the best 15% of mod-\nels with regard to PEP value, and the worst 15%\nof models with regard to PEP value respectively.\nThese correlation matrices are used to find relation-\nships between different hyperparamters and PEP,\nin order to figure out which hyperparameters have\na huge effect on PEP, and which do not have an\neffect.\nB Energy\nFigure 5, and 6 showcase the correlation matrices\nfor the best 15% of models with regard to energy\nconsumption, and the worst 15% of models with\nregard to energy consumption respectively. Hav-\ning a correlation matrix over all 154 models would\nproduce an identical result to Figure 2, so that’s not\nincluded here. These correlation matrices are pri-\nmarily used to find hyperparamters which greatly\naffect energy consumption, or which hyperparam-\neters barely affect energy consumption. Both of\nthese can be used to find hyperparamters which\nneed to be tuned, and to find hyperparamters which\ncan be tuned with regard to perplexity, given they\ndo not affect energy consumption.\nC Early optimisation results\nIn this section, we will present the preliminary re-\nsult after the hyperparameter tuning done with hy-\nperopt. This includes some of the first observable\ntendencies and was used to start the analytical pro-\ncess.\nThe loss saved from each model produced by\nhyperopt is the average overall three epoch, instead\nof the actual loss at the end of the third epoch. This\nresults in difficulty calculating perplexity, as the\nloss is now skewed heavily towards the mean, and\nin return also results in energy loss not being com-\nparable to other parts of the project. Energy con-\nsumption has been logged after the end of the third\nepoch, and can therefore still be compared to the\nrest of the project. Furthermore, it’s not possible\nto try and calculate what the actual perplexity is,\ngiven that there is no real relation between perplex-\nity and the number of epochs. The data will still be\npresented, but given the fact that the data collection\nhas been different from the rest of the project, it\nwill mostly not be compared or analysed further.\nAfter hyperopt training all 154 models on 3\nepochs was done, an analysis of the different model\nconfigurations was performed in order to spot early\ntrends in the data. A correlation matrix was con-\nstructed for all variable parameters, and compared\nto energy consumption, perplexity, and energy loss,\nas can be seen in Figure 7. When speaking about\nenergy consumption, hyperparameters such as the\nnumber of hidden layers and the size of said hidden\nlayers plays a big role. As either the number of\nhidden layers or their size increases, so does the\nenergy consumption needed to train the model. Fur-\nthermore, when looking at perplexity, there are no\ncorrelations that are as strong as there were for en-\nergy consumption. The biggest thing to note here is\nthat there is a positive correlation between hidden\nlayer dropout probability and perplexity. The same\ngoes for hidden layers and hidden size, although\nthis correlation is not as strong as with energy con-\nsumption.\nThe last interesting thing to note here is that there\nis a positive correlation between energy consump-\ntion and perplexity. If one looks at the correlation\nbetween energy consumption and energy loss, or\nperplexity and energy loss, you see a closer correla-\ntion between those two parameters, which indicates\nthat the ratio between energy consumption and per-\nplexity, albeit a positive one, is a weaker correlation\nthan the others. There are multiple explanations\nfor this. It could be that this correlation shows that\nmodels can be improved in either department with-\nout sacrificing the other metric when looking at all\nmodels as a whole. It could also be that the afore-\nmentioned outliers are having an effect on the data.\nFollowing this, the correlation matrix for the best\n15% of our models was constructed, as can be seen\nin Figure 8. When filtering for the best performing\nmodels, the strongest correlation is between energy\nconsumption and perplexity, with a correlation of\n−0.86. This indicates that as energy consumption\ngoes higher perplexity will go lower, and the other\nway around.\nD Perplexity\nAs with the previous section where we went through\nthe models sorted by the best and worst 15% in\nterms of energy consumption, this section will do\nthe same but in terms of perplexity instead. By\nanalysing the parameters, we might be able to find\na relation between certain of these that can cause\nboth low energy consumption and perplexity, thus\nreducing the model size without affecting perfor-\nmance.\n107\nFigure 2: Correlation matrix over hyperparameters for 10 epochs.\nFigure 3: Correlation matrix over hyperparameters for the best 15% of models for 10 epochs.\n108\nFigure 4: Correlation matrix over hyperparameters for the worst 15% of models for 10 epochs.\nFigure 5: Correlation matrix of the best 15% of models wrt. power consumption.\n109\nFigure 6: Correlation matrix of the most power-hungry 15% of models.\nFigure 7: Correlation matrix over hyperparameters for 3 epochs of training.\n110\nFigure 8: Correlation matrix over hyperparameters for the best 15% of models for 3 epochs of training.\nFigure 9: A correlation matrix of the best 15% of models with regard to perplexity.\n111\nAverage Std. Deviation\nvocab_size 18002.17 7986.21\nactual_hidden_size 404.91 231.93\nnum_hidden_layers 3.69 2.09\nnum_attention_heads 10.34 4.62\nintermediate_size 851.47 531.30\nhidden_dropout_prob 0.21 0.08\nattention_dropout_prob 0.37 0.24\nenergy consumption 3.40 1.58\nperplexity 18.79 3.82\nTable 7: Table with average hyperparameters of the\nbest 15% of models wrt: perplexity.\nThe general point with the best-performing mod-\nels in terms of perplexity is that they are slightly\nlarger than both the best performing energy loss,\nand energy only models, as can be seen on Table 7.\nMore hidden layers, more attention heads, bigger\nfeed forward neural networks. A lot of these param-\neters do have a high standard deviation compared\nto their averages, such as the intermediate size and\nthe number of hidden layers, meaning that there is\nroom for reduction to reduce energy consumption\nthrough lower training times. What is interesting\nto point out is that while the models are slightly\nbigger, both their average perplexity and energy\nconsumption are vastly better than the worst mod-\nels in regard to energy consumption. It is possible\nthat our best-performing perplexity models can, due\nto a slimmer size, reach good performance more\neasily than compared with the bigger models of\nthe worst energy consumption. This energy con-\nsumption is still on average 2.4 kWh higher than\nour best performing models in terms of energy con-\nsumption, being at 1 kWh, but with a vastly better\nperplexity. It could suggest that energy loss, as a\nvalue to minimise for hyperopt, has been effective\nin finding compromises between performance and\nenergy consumption. As can be seen on the corre-\nsponding correlation matrix for the best perplexity\nmodels, Figure 9, there is a negative correlation\nbetween energy consumption and perplexity of -\n0.42. This means that reducing one increases the\nother, which also suggests that hyperopt has found\na compromise between the two. This matrix also\nshows a very high correlation between both energy\nconsumption, perplexity, and the number of hidden\nlayers. For energy consumption, it is 0.76, and for\nperplexity, it is -0.49. Increasing the hidden layers\nwill drastically increase energy consumption, but it\nwill also lower perplexity a lot. As mentioned ear-\nlier, since this adds extra training time to the model,\nit will automatically increase energy consumption.\nThis could possibly suggest that the number of hid-\nden layers has a direct effect on how well a model\nperforms.\nAppearances\nrelative_key_query 21\nrelative_key 2\nabsolute 0\nGELU 11\nGELU_new 5\nReLU 3\nSiLU 4\nTable 8: Count of activation functions and position em-\nbedding types in the best 15% of models with regard to\nperplexity.\nInterestingly, as can be observed in Table 8, the\ndistribution over positional embedding and activa-\ntion function resembles the distribution of the ones\nsorted by energy loss a lot more than the ones sorted\nby energy consumption only, with some slight vari-\nations. They almost exclusively use relative key\nquery positional embedding and rely more on a\nGELU activation function.\nAverage Std. Deviation\nvocab_size 17920.60 8478.86\nactual_hidden_size 569.26 441.47\nnum_hidden_layers 6.21 3.32\nnum_attention_heads 10 4.45\nintermediate_size 1190.04 587.58\nhidden_dropout_prob 0.54 0.26\nattention_dropout_prob 0.43 0.26\nenergy consumption 5.20 3.40\nperplexity 1891.61 364.25\nTable 9: Table with average hyperparameters of the\nworst 15% of models wrt: perplexity.\nThe primary assumption with the models that\nperform terribly in terms of perplexity is that they\nare big, and thus have not had enough training time\nto fully develop. When comparing Figure 1 to Ta-\nble 9, there are a couple of models that follow this\ntrend, with one taking a significant leap down in\nperplexity between epoch 8 to 9. As that model\nfollows the trend of badly performing models that\nare right on top of each other in terms of perplexity,\nit could be assumed that more epochs are what is\nneeded for perplexity to drop. When looking at the\ngeneral trend of the parameters, both the hidden\nsize, number of hidden layers, attention heads and\nintermediate size are higher compared to the mod-\n112\nFigure 10: A correlation matrix of the worst 15% of models with regard to perplexity.\nels that perform well in terms of perplexity. When\nlooking at Table 9, the parameters for the models in\nthe top 15% worst percentage are definitely larger\nthan those in the top 15% best percentage for per-\nplexity in most aspects. Hidden size and layers\nare much increased, the same with the intermedi-\nate size. All dropout probabilities are also higher,\nwhich could be a reason as to why some of these\nmodels keep performing terribly - whatever they\nlearn, they end up forgetting, thus making it harder\nto train a bigger model.\nAppearances\nrelative_key_query 8\nrelative_key 7\nabsolute 8\nGELU 7\nGELU_new 7\nReLU 5\nSiLU 4\nTable 10: Count of activation functions and position\nembedding types in the worst 15% of models wrt: per-\nplexity.\nWhen looking at Figure 10, the activation func-\ntion has a negligible effect on both perplexity and\npower consumption in terms of its correlation, but\nthe positional embedding type has a significant\nimpact on perplexity, with a correlation of -0.43.\nLooking at the difference between the choice of\nthese between the best and worst models in terms\nof perplexity from Table 10 and Table 8, the better\nmodels all tend to use relative key query, whereas\nthe distribution for the worse performing models\nis more uniform over the three choices. As most\nof the best models in terms of energy consumption\nand energy loss primarily use relative key query,\nthe results suggest that there is little to no reason to\nuse another type.\nE Clustering\nIn this section, the data will be clustered in order\nto find commonalities among the different models,\nand group them into the three found clusters. Those\nclusters will then be described in detail, pointing\nout interesting characteristics in each one.\nThe data were clustered using DBSCAN algo-\nrithm, which is a density-based clustering algo-\nrithm designed to do spatial clustering with han-\ndling of noise ( M. Ester and Xu , 1996). The\ndensity-based clustering method that is DBSCAN\n113\ndoes not handle varying axis ranges well, and\ngiven that energy consumption ranges from\n[0.529478; 12.986432] and perplexity ranges from\n[15.14847702; 2021.91427], normalization has to\nbe done. The intent for this normalization is to have\nboth axes have a mean of 0 and a standard deviation\nof 1. One of the consequences of doing this is that\nthe distance used to specify clusters loses some of\nits intuitive understanding of what distance is now\nthat both axes have been normalised. While this\nis true, the distance can now be used to properly\nbe used to identify clusters for both metrics, rather\nthan constraining the perplexity to the range of en-\nergy consumption. This enables us to find more\nthan just vertical clusters.\nFor the actual clustering, a distance of 0.4 was\nchosen with the requirement of needing 5 sam-\nples to form a cluster. The clustering showcases\nthe aforementioned outliers which ended at around\n2000 perplexity but also finds another cluster close\nto our primary cluster, as well as 20 different out-\nliers who were unable to be clustered, marked as\nblack dots on Figure 11.\nThe exact distribution of the clusters can be seen\nhere on Table 11. It’s interesting to note that had\nthe minimum samples been a little higher, cluster 3\nwould not have existed.\nNumber of Models\nCluster 1 118\nCluster 2 10\nCluster 3 6\nOutliers 20\nTable 11: Table showcasing the three cluster distribu-\ntions and outliers for the DBSCAN clustering\nGiven that these three clusters were found by\nour algorithm, it’s only right to analyse those clus-\nters further by constructing correlation matrices for\neach cluster. The first cluster, which can be seen in\nFigure 12, is the cluster featuring all the best per-\nforming models, and also subsequently the biggest\ncluster out of the three. The trends for this cluster\nare similar to what has been previously seen - the\ncorrelation matrix indicates that the number of hid-\nden layers and energy consumption have a strong\ncorrelation, and the same goes for hidden size to a\nmuch lesser degree. Furthermore, the hidden size\nhas a negative correlation with perplexity, and the\nhidden dropout probability has a correlation with\nperplexity. This indicates the higher the hidden\nsize, the lower the perplexity, and the lower the hid-\nden dropout probability, the lower the perplexity.\nInterestingly, energy consumption and perplexity\nhas a negative correlation, but it is incredibly weak,\nand therefore not indicative of anything. Most of\nour Pareto entries are in this cluster, which con-\ntributes to the negative correlation, but given that\nthere are 118 entries here, as seen on Table 11, it\nindicates that a predominant amount of the models\ncan still be optimised on both parameters.\nWhen looking at the average parameters for the\nfirst cluster, it’s decently similar to that of the best\n15% of models, as seen in Table 2 in section 4.2.\nMostly all parameters follow an increasing trend\ncompared to the other table, except for the actual\nhidden size, which remains slightly lower than\nwhen looking at the best 15% of models. As can vi-\nsually be seen in Figure 11, this cluster features pre-\ndominantly low perplexity, at an average of 56.28,\nwhereas the energy consumption is slightly higher,\nsitting at an average of 2.34.\nAverage Std. Deviation\nvocab_size 20491.32 7580.56\nactual_hidden_size 263.55 208.74\nnum_hidden_layers 3.34 2.68\nnum_attention_heads 9.16 4.87\nintermediate_size 891.15 730.67\nhidden_dropout_prob 0.33 0.16\nattention_dropout_prob 0.36 0.20\nTable 12: Table with average hyperparameters of the\ntop 15% of models in cluster 1.\nFurthermore, it’s also seen that there is a heavy\nbias towards the position embedding type rela-\ntive_key_query, which was also the case among\nour best 15% of models. The activation function\nremains slightly more spread out, with GELU and\nSiLU being the predominant activation functions,\nfollowed closely by ReLU. Among the best 15% of\nmodels, GELU was the predominant function, with\nSiLU at half of the occurrences, as can be seen on\nTable 2\n114\nFigure 11: The data clustered using DBSCAN with axes scaled and translated for a mean of 0 and a standard\ndeviation of 1.\n115\nFigure 12: The correlation matrix for the first cluster\nAppearances\nrelative_key_query 94\nrelative_key 12\nabsolute 12\nGELU 37\nGELU_new 15\nReLU 29\nSiLU 37\nTable 13: Count of activation functions and position\nembedding types for the first cluster.\nWhen looking at the second cluster on Figure 11,\nit can already be seen from this figure that there\nmight be some linear tendencies in the models from\nthis cluster. And as can be seen in Figure 13, energy\nconsumption, perplexity, and energy loss all have\na correlation close to 1, which indicates this to be\nthe case. The correlation matrix also indicates lin-\nearity in the parameters, given that all parameters\nhave very close correlation ratios between energy\nconsumption, perplexity, and energy loss.\nBecause of the previously explained linear ten-\ndency, a linear regression was done on the models\nin cluster 2 as can be seen in Figure 14. If one\nextends beyond the scope of the cluster, it’s pos-\nsible that optimal models which decrease in both\nenergy consumption and perplexity, which also lay\non this line. Given that our hyperopt optimization\nwas done on a limited scope, it’s not possible to see\nif this is the case, although this linear regression\nstrongly indicates that there are more models along\nthis line that have not been explored.\nThe third cluster, being the cluster with all of\nthe detected high perplexity models, can be seen\nin Figure 15. It features two strong correlations\nbetween the number of attention heads and energy\nconsumption, as well as the hidden layer size and\nenergy consumption. This indicates that the high\namount of attention heads and hidden layer size\nhave a strong influence on the increase of energy\nconsumption. If one looks at the specific models\nof this cluster, it can be seen that they on average\nhave 9.5 attention heads and a hidden size of 620.5.\nIf one compares it to the results seen in Table 2, the\naverage number of attention heads increases by ap-\nproximately 1.5 and the average hidden size by 350.\nThere is also a strong correlation between position\nembedding type and perplexity with 0.98, which\nstrongly indicates that the position embedding type\nis tied to an increase in perplexity. Looking at the\nmodels in the cluster, it’s seen that there’s a uniform\n116\nFigure 13: The correlation matrix for the second cluster\nFigure 14: Regression done on the models of cluster 2\n117\ndistribution between the three-position embedding\ntypes in the 6 models in the cluster. When referring\nback to Figure 10 it is seen that when looking at the\n15% worst models with regard to perplexity, the\ncorrelation is still there, but not nearly as strong.\nFurthermore, the same uniform distribution of po-\nsition embedding types can be seen on Table 10,\nwhich indicates that there are too few models to say\nanything about this correlation.\nIn Table 14 the 19 models situated on the Pareto\ncurves through all 10 epochs are summarised along\nwith the frequencies of occurrences, and for which\nepochs they occurred on the Pareto curve. Further-\nmore, the results with regard to energy consumption\nand perplexity are also showcased for the models af-\nter the 10th epoch, regardless of whether the model\nwas a part of the Pareto curve at this point. Contin-\nuing on this point it is also important to note that\ntowards the end of the Pareto curve, that the mod-\nels most likely won’t be the most effective models.\nThis is for example the case with model number\n103, which is a part of the Pareto curve for the 10th\nepoch, but with a perplexity of 1274.87. Model\nnumber 103 will probably not be a model you would\nwant to focus on(especially since it was trained over\n10 epochs) but can be used as a boundary to attempt\nto narrow down the search space, and thus still pro-\nvide valuable information. Similarly, a model such\nas number 25, has a really strong score with regard\nto perplexity, but there is a number of models with\nsimilar perplexity scores while still having a much\nlower energy consumption. An important note here\nis to mention the possibility of training some of the\nlow-cost models for even more epochs, to fairly\ncompare their energy consumption vs perplexity\nwith those of similar models. The reasoning here\nis if a low-cost model, such as model 103, would\nbe comparable in perplexity to some of the other\nmodels after say 20 or 30 epochs, its energy con-\nsumption might still be in a relatively comfortable\nspot compared to another model with an energy con-\nsumption similar to that of model 25. The reasoning\nbehind this argument comes from the linear cost\nof energy consumption for each epoch, such that\neven after 30 epochs of training, assuming a con-\ntinuously linear tendency for the following epochs,\nmodel number 103 would have an approximate en-\nergy consumption of 0.58(the energy consumption\nfor model 103 after 10 epochs) times 3 (to go from\n10 to 30 epochs) would be 0.58 · 3 ≈ 1.76.\n118\nFigure 15: The correlation matrix for the third cluster\nid energy_epoch_10 PPL_epoch_10 occurrence Epoch Occurrence\n10 1.73 kWh 23.66 10 1,2,3,4,5,6,7,8,9,10\n25 3.63 kWh 14.31 10 1,2,3,4,5,6,7,8,9,10\n29 1.86 kWh 17.61 10 1,2,3,4,5,6,7,8,9,10\n81 0.72 kWh 151.56 10 1,2,3,4,5,6,7,8,9,10\n97 1.15 kWh 31.77 10 1,2,3,4,5,6,7,8,9,10\n103 0.58 kWh 1274.87 10 1,2,3,4,5,6,7,8,9,10\n48 0.90 kWh 46.47 9 2,3,4,5,6,7,8,9,10\n62 2.52 kWh 15.71 9 2,3,4,5,6,7,8,9,10\n88 1 kWh 37.25 9 2,3,4,5,6,7,8,9,10\n106 1.26 kWh 29 7 4,5,6,7,8,9,10\n111 1.59 kWh 29.27 7 1,2,3,4,5,6,7\n58 1.63 kWh 27.19 6 4,6,7,8,9,10\n136 1.78 kWh 19.38 6 1,2,3,6,9,10\n87 3.66 kWh 13.65 5 6,7,8,9,10\n133 4.52 kWh 13.12 5 6,7,8,9,10\n147 1.28 kWh 30.43 4 1,2,3,4\n114 3.42 kWh 15.51 3 8,9,10\n24 1.80 kWh 23.48 2 9,10\n99 0.94 kWh 54.87 2 1,2\nTable 14: Table over pareto entries, featuring energy consumption and perplexity at 10 epochs, the number of\noccurrences in the pareto curves of each epoch, and the specific epochs at which it occurs on the pareto curve."
}