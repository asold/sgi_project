{
  "title": "Incorporating Domain Knowledge Into Language Models by Using Graph Convolutional Networks for Assessing Semantic Textual Similarity: Model Development and Performance Comparison",
  "url": "https://openalex.org/W3119848647",
  "year": 2021,
  "authors": [
    {
      "id": "https://openalex.org/A2101248632",
      "name": "David Chang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2098248671",
      "name": "Eric Lin",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2046099468",
      "name": "Cynthia Brandt",
      "affiliations": [
        "Yale University",
        "VA Connecticut Healthcare System"
      ]
    },
    {
      "id": "https://openalex.org/A2396393586",
      "name": "Richard Andrew Taylor",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2101248632",
      "name": "David Chang",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2098248671",
      "name": "Eric Lin",
      "affiliations": [
        "Yale University"
      ]
    },
    {
      "id": "https://openalex.org/A2046099468",
      "name": "Cynthia Brandt",
      "affiliations": [
        "Yale University",
        "VA Connecticut Healthcare System"
      ]
    },
    {
      "id": "https://openalex.org/A2396393586",
      "name": "Richard Andrew Taylor",
      "affiliations": [
        "Yale University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2032503600",
    "https://openalex.org/W3094834348",
    "https://openalex.org/W1550258693",
    "https://openalex.org/W2889272240",
    "https://openalex.org/W2971258845",
    "https://openalex.org/W2951048068",
    "https://openalex.org/W2970986510",
    "https://openalex.org/W2953356739",
    "https://openalex.org/W2998385486",
    "https://openalex.org/W2612872092",
    "https://openalex.org/W2159583324",
    "https://openalex.org/W4236053279",
    "https://openalex.org/W3037439385",
    "https://openalex.org/W2979826702",
    "https://openalex.org/W2963716420",
    "https://openalex.org/W2911489562",
    "https://openalex.org/W2396881363",
    "https://openalex.org/W2786016794",
    "https://openalex.org/W2962946486",
    "https://openalex.org/W3037860573",
    "https://openalex.org/W2997522493",
    "https://openalex.org/W2971007555",
    "https://openalex.org/W2970771982",
    "https://openalex.org/W2963854351",
    "https://openalex.org/W3037063616",
    "https://openalex.org/W2888120268",
    "https://openalex.org/W2970986790",
    "https://openalex.org/W2739351760",
    "https://openalex.org/W4252194527",
    "https://openalex.org/W4250877149",
    "https://openalex.org/W3104033643"
  ],
  "abstract": "Background Although electronic health record systems have facilitated clinical documentation in health care, they have also introduced new challenges, such as the proliferation of redundant information through the use of copy and paste commands or templates. One approach to trimming down bloated clinical documentation and improving clinical summarization is to identify highly similar text snippets with the goal of removing such text. Objective We developed a natural language processing system for the task of assessing clinical semantic textual similarity. The system assigns scores to pairs of clinical text snippets based on their clinical semantic similarity. Methods We leveraged recent advances in natural language processing and graph representation learning to create a model that combines linguistic and domain knowledge information from the MedSTS data set to assess clinical semantic textual similarity. We used bidirectional encoder representation from transformers (BERT)–based models as text encoders for the sentence pairs in the data set and graph convolutional networks (GCNs) as graph encoders for corresponding concept graphs that were constructed based on the sentences. We also explored techniques, including data augmentation, ensembling, and knowledge distillation, to improve the model’s performance, as measured by the Pearson correlation coefficient (r). Results Fine-tuning the BERT_base and ClinicalBERT models on the MedSTS data set provided a strong baseline (Pearson correlation coefficients: 0.842 and 0.848, respectively) compared to those of the previous year’s submissions. Our data augmentation techniques yielded moderate gains in performance, and adding a GCN-based graph encoder to incorporate the concept graphs also boosted performance, especially when the node features were initialized with pretrained knowledge graph embeddings of the concepts (r=0.868). As expected, ensembling improved performance, and performing multisource ensembling by using different language model variants, conducting knowledge distillation with the multisource ensemble model, and taking a final ensemble of the distilled models further improved the system’s performance (Pearson correlation coefficients: 0.875, 0.878, and 0.882, respectively). Conclusions This study presents a system for the MedSTS clinical semantic textual similarity benchmark task, which was created by combining BERT-based text encoders and GCN-based graph encoders in order to incorporate domain knowledge into the natural language processing pipeline. We also experimented with other techniques involving data augmentation, pretrained concept embeddings, ensembling, and knowledge distillation to further increase our system’s performance. Although the task and its benchmark data set are in the early stages of development, this study, as well as the results of the competition, demonstrates the potential of modern language model–based systems to detect redundant information in clinical notes.",
  "full_text": "Original Paper\nIncorporating Domain Knowledge Into Language Models by Using\nGraph Convolutional Networks for Assessing Semantic Textual\nSimilarity: Model Development and Performance Comparison\nDavid Chang1, DPhil; Eric Lin2, MD; Cynthia Brandt1,3,4, MPH, MD; Richard Andrew Taylor1,3, MD\n1Yale Center for Medical Informatics, Yale University, New Haven, CT, United States\n2Department of Psychiatry, Yale University School of Medicine, New Haven, CT, United States\n3Department of Emergency Medicine, Yale University School of Medicine, New Haven, CT, United States\n4West Haven Campus, Veterans Affairs Connecticut Healthcare System, West Haven, CT, United States\nCorresponding Author:\nRichard Andrew Taylor, MD\nYale Center for Medical Informatics\nYale University\nSuite 501\n300 George St\nNew Haven, CT\nUnited States\nPhone: 1 2037854058\nEmail: richard.taylor@yale.edu\nAbstract\nBackground: Although electronic health record systems have facilitated clinical documentation in health care, they have also\nintroduced new challenges, such as the proliferation of redundant information through the use of copy and paste commands or\ntemplates. One approach to trimming down bloated clinical documentation and improving clinical summarization is to identify\nhighly similar text snippets with the goal of removing such text.\nObjective: We developed a natural language processing system for the task of assessing clinical semantic textual similarity.\nThe system assigns scores to pairs of clinical text snippets based on their clinical semantic similarity.\nMethods: We leveraged recent advances in natural language processing and graph representation learning to create a model\nthat combines linguistic and domain knowledge information from the MedSTS data set to assess clinical semantic textual similarity.\nWe used bidirectional encoder representation from transformers (BERT)–based models as text encoders for the sentence pairs\nin the data set and graph convolutional networks (GCNs) as graph encoders for corresponding concept graphs that were constructed\nbased on the sentences. We also explored techniques, including data augmentation, ensembling, and knowledge distillation, to\nimprove the model’s performance, as measured by the Pearson correlation coefficient (r).\nResults: Fine-tuning the BERT_base and ClinicalBERT models on the MedSTS data set provided a strong baseline (Pearson\ncorrelation coefficients: 0.842 and 0.848, respectively) compared to those of the previous year’s submissions. Our data augmentation\ntechniques yielded moderate gains in performance, and adding a GCN-based graph encoder to incorporate the concept graphs\nalso boosted performance, especially when the node features were initialized with pretrained knowledge graph embeddings of\nthe concepts (r=0.868). As expected, ensembling improved performance, and performing multisource ensembling by using\ndifferent language model variants, conducting knowledge distillation with the multisource ensemble model, and taking a final\nensemble of the distilled models further improved the system’s performance (Pearson correlation coefficients: 0.875, 0.878, and\n0.882, respectively).\nConclusions: This study presents a system for the MedSTS clinical semantic textual similarity benchmark task, which was\ncreated by combining BERT-based text encoders and GCN-based graph encoders in order to incorporate domain knowledge into\nthe natural language processing pipeline. We also experimented with other techniques involving data augmentation, pretrained\nconcept embeddings, ensembling, and knowledge distillation to further increase our system’s performance. Although the task\nand its benchmark data set are in the early stages of development, this study, as well as the results of the competition, demonstrates\nthe potential of modern language model–based systems to detect redundant information in clinical notes.\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 1https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n(JMIR Med Inform 2021;9(11):e23101) doi: 10.2196/23101\nKEYWORDS\nnatural language processing; graph neural networks; National NLP Clinical Challenges; bidirectional encoder representation from\ntransformers\nIntroduction\nElectronic health records (EHRs) have introduced efficiencies\nin clinical documentation via the automatic insertion of\ncommonly used documentation phrases and the use of the copy\nand paste command, which copies the content of one day’s notes\ninto that of the next day’s notes, but at the same time, these\ntools have resulted in notes becoming increasingly bloated with\nsometimes outdated, irrelevant, and even erroneous information\n[1]. To trim down bloated clinical documentation, one approach\nof interest is to identify highly similar text snippets for the goal\nof removing such text. Wang et al [2,3] created the MedSTS\ndata set—a clinical analogue of the natural language\nunderstanding benchmark task of assessing semantic textual\nsimilarity (STS)—to be a resource for this line of study. In this\npaper, we show the model, as well as subsequent improvements,\nthat was used in the August 2019 National NLP Clinical\nChallenges (n2c2)/Open Health Natural Language Processing\n(OHNLP) Consortium semantic similarity shared task challenge\n[2], which featured the MedSTS data set.\nIn the broader natural language processing (NLP) community,\nSTS assessment is a task in which the similarity of semantic\nmeanings and content among natural language texts is calculated\n[3], and at the time of its release in late 2018, the bidirectional\nencoder representation from transformers (BERT) language\nmodel had the best published performance on the commonly\nused general English STS Benchmark (STS-B) data set [4]. For\nthe MedSTS data set, it was shown that a BERT model that was\nfine-tuned to the biomedical domain also outperformed most\nprior state-of-the-art models [5]. The first iteration of the\nMedSTS challenge in 2018 (ie, prior to the release of BERT)\nsaw 4 submissions involving the mixed use of traditional\nmachine learning models, like random forests, and more recent\ndeep learning architectures, like recurrent neural networks and\nconvolutional neural networks. The 2019 MedSTS challenge\nsaw over 30 submissions, and the majority of these submissions\nused BERT in some capacity. The increased number of\nsubmissions, as well as the increased average performance of\nsubmissions, can be attributed in large part to the recent progress\nin the development of language models, of which BERT is a\npopular example.\nDespite such advances, researchers have noted that although\nlanguage models demonstrate a small degree of commonsense\nreasoning and basic knowledge, such models are very limited\nin terms of their ability to generate factually correct text or even\nrecall explicit facts from training data [6]. The attempts to\nmitigate these shortcomings of language models have often\ninvolved the use of graph representation learning techniques\n[7-9], which provide a natural way for working with knowledge\nin the form of graphs.\nRecent progress in graph representation learning has given rise\nto 2 promising classes of methods that can be used in\nconjunction with NLP models to incorporate knowledge (either\ndomain knowledge or commonsense knowledge)—graph\nconvolutional networks (GCNs) [10] and knowledge graph\nembeddings (KGEs) [11].\nGCNs generalize the notion of convolution from images to\ngraph-structured data, thereby enabling the application of deep\nlearning techniques on graphs. KGE methods are used to encode\nentities (nodes) and relationships (edges) in a knowledge graph\ninto dense vector representations, much like word embeddings.\nKGEs provide a way of obtaining embeddings of concepts, and\nGCNs provide a natural way of using that information in the\ncontext of graph-based learning. For instance, GCNs can be\nused to initialize node features with pretrained KGEs.\nIn this study, we leveraged these recent advances in NLP and\ngraph representation learning to develop a more\nknowledge-aware approach to assessing the MedSTS benchmark\ndata set. We further investigated the benefits of other techniques,\nsuch as data augmentation, multisource ensembling, and\nknowledge distillation, and they resulted in competitive\nperformance values for the 2019 n2c2/OHNLP Consortium\nsemantic similarity shared task challenge.\nMethods\nData Set\nMedSTS is a data set of sentence pairs that were gathered from\nthe clinical EHRs at Mayo Clinic. Deidentified sentences were\nselected based on their frequency of appearance and an\nassumption that frequently appearing sentences tend to contain\nless protected health information. Sentence pairings were\narranged so that they had at least some degree of surface-level\nsimilarity. This was based on a combination of surface lexical\nsimilarity metrics. Broadly speaking, sentences generally fell\ninto the following four categories: signs and symptoms,\ndisorders, procedures, and medications. Further details are\ndiscussed in the original MedSTS paper [3]. For the 2019\nn2c2/OHNLP competition and this study, a subset of annotated\nsentence pairs was examined; of the 2054 sentence pairs in this\nsubset, 1652 (80.4%) were included in the training set, and 412\n(20.1%) were included in the test set [2]. This subset was\nindependently scored by 2 medical experts for semantic\nsimilarity. A 6-point (range: 0-5) rubric was provided to the\nannotators; 0 denotes complete dissimilarity, 1 indicates that 2\nsentences are topically related but are otherwise not equivalent,\nand 5 represents complete similarity. The agreement between\nthe two annotators received a weighted Cohen κ score of 0.67.\nThe average of the two scores served as the gold standard against\nwhich STS systems would be evaluated [3].\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 2https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nConcept Graph Construction\nFor each sentence in the MedSTS data set, we constructed a\ncorresponding concept graph to represent the domain knowledge\naspect of the data set. The concept graphs consisted of concepts\nthat were tagged with a domain-specific tagger called MetaMap\n[12] and were mapped to a specified medical terminology. The\nidea was that such a graph would provide an additional\nrepresentation of data containing explicit domain knowledge\nin the form of mapped concepts and their connections.\nThe Unified Medical Language System (UMLS) [13] is an\nimportant resource in biomedical and health care research that\nintegrates many health and biomedical vocabularies and\nterminologies under a unified, interoperable system. MetaMap\n[12] is a widely used NLP tool that maps concepts in biomedical\nand clinical text to the UMLS Metathesaurus. We applied\nMetaMap on the MedSTS data set to extract biomedical and\nclinical entities that belonged to the Systematized Nomenclature\nof Medicine Clinical Terms (SNOMED CT) terminology of the\nUMLS. Thus, for each sentence, we obtained a corresponding\nlist of extracted concepts, their concept unique identifiers, and\nsemantic type information.\nWe then constructed a graph of SNOMED CT terminology from\nthe raw UMLS files by using the concepts (MRCONSO.RRF\nfiles) as nodes and by using the relationships (MRREL.RRF\nfiles) among them as edges. For simplicity, we only considered\nthe connectivity information among the concepts and left the\nsemantic information in the relation types for future work. Once\nwe had a full SNOMED CT graph, we induced subgraphs for\neach sentence from MedSTS by taking the shortest paths\nbetween the concepts that were extracted from the sentences.\nMore concretely, this was done by using the shortest path\nmethod via the Dijkstra algorithm in the Networkx [14] library.\nAlthough there are many possible ways of constructing such\nsentence graphs, we decided to use the simple and heuristic\nshortest path method to obtain a connected graph that represents\neach sentence. Examples of such concept graphs, along with\ntheir original sentences, are shown in (Figure 1).\nFigure 1. An example sentence pair, its similarity score, and a visualization of the corresponding concept graphs constructed from the concepts in the\nsentences.\nData Augmentation\nGiven the small size of the data set, we decided to augment it\nby including additional domain knowledge from the MetaMap\noutput files. Notably, there were 2 pieces of information that\nwe chose to use—the preferred name of the mapped concept in\nthe source terminology and the semantic type of the concept\nwithin the UMLS Semantic Network. The preferred name of a\nmapped concept can often be the same as how the concept\nappears in the text, but the preferred name sometimes provides\npotentially valuable information in the form of synonyms or\nabbreviation expansions. For example, in the text snippet “the\npatient was taken to the pacu in stable condition,” the term pacu\nis mapped to the UMLS concept postoperative anesthesia care\nunit (PACU), thereby providing the full description of the\nabbreviated term. The strings of the preferred names of mapped\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 3https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nconcepts were simply appended to the original sentences in the\ndata set. Likewise, the semantic types of the mapped concepts\n(eg, Health Care Related Organization for the term pacu) were\nappended to the original sentences. Another method we used\nwas doubling the data set size by simply feeding the model a\ncopy of the data set that included sentences formatted in the\nreverse order (ie, “sentence2:sentence1”). This yielded slightly\nbetter results than those that were obtained by simply doubling\nthe number of training epochs, suggesting that feeding the model\nthe reverse copy of the data set might have given it more explicit\nhints that the task was agnostic to the order of the sentences.\nAlthough the data augmentation techniques we used were simple\nand yielded moderate improvements in performance, a recent\npaper [15] provides more interesting approaches to data\naugmentation. In the paper [15], the authors used\nback-translation and performed segment reordering to augment\nthe MedSTS data set.\nThe BERT Model\nThe BERT model is a widely used NLP model that is part of\nthe recently emerging class of language models that use\ntransformers [16] as the building blocks. The BERT model\nstacks multiple layers of transformer-based modules that\nprimarily use the multiheaded self-attention mechanism to\nencode text into dense embeddings. The model is trained by\nusing the masked language modeling objective and the next\nsentence prediction objective, and pretrained models for BERT\n(and other similar models) are readily available on the\nHuggingFace Transformers library [17]. Shortly after the BERT\nmodel dominated the general NLP field, several variations of\nthe BERT model that were adapted to the biomedical and\nclinical domains also became available [5,18,19]. These\ndomain-adapted versions of the BERT model were trained on\nsome combination of the Medical Information Mart for Intensive\nCare version 3 [20], PubMed [21], and PubMed Central [22]\ndatabases, and these versions have been shown to outperform\nthe original BERT model in several clinical NLP tasks,\nsuggesting that they are more appropriate for working with\nclinical text data sets like MedSTS.\nThe GCN Method\nKipf et al [10] contributed to the popularization of graph neural\nnetworks by providing an efficient implementation method for\nGCNs and demonstrating their effectiveness in analyzing several\nbenchmark graph data sets for graph classification, node\nclassification, and link prediction. Variants of GCNs were soon\napplied successfully to various domains and problems, including\nthe modeling of interactions in physical systems [23], drug-drug\ninteractions [24], and text classification [25]. GCNs have\nbecome a popular deep learning model for working with\ngraph-structured data, and we used GCNs to encode the concept\ngraphs.\nKGE Methods\nKGEs are a relatively novel class of methods for learning dense\nvector representations of entities and relations in\nmulti-relational, heterogeneous knowledge graphs. Essentially,\na KGE model maps entities and relations to embedding spaces\nby using a predefined scoring function. Due to their growing\npopularity and the availability of implementation methods,\nKGEs have recently been applied to various domains, including\nbiomedical knowledge graphs [26]. Chang et al [26] showed\nthat using KGEs for learning concept embeddings from medical\nterminologies and knowledge graphs is arguably a more\nprincipled and effective approach than using previous methods\nbased on skip-gram–based models like Cui2Vec [27] or network\nembedding–based models like Snomed2Vec [28]. Although we\ninitially used Cui2Vec for our entity vectors at the time of\nsubmission, we later used SNOMED CT KGEs after they\nbecame available in recent months.\nAugmenting BERT With KGEs for MedSTS\nWe combined the components of GCNs and KGEs into a single\nmodel in the following way: we used a BERT-based model as\nour text encoder for the sentence pairs in MedSTS, used a\nGCN-based model as our graph encoder for the concept graphs\nthat corresponded to the sentence pairs, initialized the node\nembeddings in the graphs by using pretrained SNOMED CT\nKGEs, concatenated the outputs of the text and graph encoders,\nand passed the final concatenated vector to a fully connected\nlayer to obtain a semantic similarity score. We also tested the\nbenefits of using the SNOMED CT KGEs by comparing this\nmethod to random initialization and initialization with Cui2Vec\nembeddings. A visualization of the pipeline is shown in Figure\n2.\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 4https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nFigure 2. A simplified diagram of our pipeline. We passed the sentences through MetaMap to extract concepts belonging to the SNOMED CT and\ninduced concept graphs by using the relationships among the terminology. We then passed the augmented sentence pairs to the text encoder and passed\nthe concept graphs to the graph encoder. The outputs from the encoders were concatenated and passed to a fully connected layer to obtain an S. BERT:\nbidirectional encoder representation from transformers; FFN: feed-forward network; GCN: graph convolutional network; S: similarity score; S1: sentence\n1; S2: sentence 2; SNOMED CT: Systematized Nomenclature of Medicine Clinical Terms.\nEnsemble and Knowledge Distillation\nAfter training our model, we took an ensemble to further\nimprove the model’s performance. In accordance with Xu et al\n[29], we performed multisource ensembling with the following\nvariants of BERT: BERT_base [4], SciBERT [30],\nClinicalBERT [18], multi-task deep neural networks\n(MT-DNNs) [31], and BlueBERT [32]. Afterward, we\nperformed knowledge distillation—an effective model\ncompression method in which a smaller model is trained to\nmimic a larger model (ie, the ensemble). We used the predictions\nof the multisource ensemble model as soft labels in a teacher\nbounded regression loss function, in accordance with Chen et\nal [33], to train more individual models and obtain a final\nensemble of the knowledge-distilled models.\nResults\nWe split the provided training set of MedSTS into 1313 training\nexamples and 329 validation examples and reported the Pearson\ncorrelation coefficient for the held-out test set of 412 examples.\nThe Pearson correlation coefficient was the chosen metric for\nthe competition. We used the HuggingFace Transformers library\nfor implementations related to language models, and we used\nPyTorch Geometric [34] for implementations of GCNs. Many\nof the default training and fine-tuning hyperparameters were\nused, while the following hyperparameters were tuned on the\nvalidation set: a learning rate of 1e−4 for BERT-based models\n(chosen from 5e−5, 1e−4, and 5e−4), a learning rate of 1e−3 for\nGCNs (chosen from 1e−2, 1e−3, and 1e−4), and 4 epochs (chosen\nfrom 3, 4, and 5 epochs).\nTable 1 shows the contributions of the different components in\nthe pipeline. Simply using the off-the-shelf BERT_base model\nand fine-tuning it on MedSTS yielded higher performance values\ncompared to those of the 2018 submissions. Using ClinicalBERT\nand using our previously described data augmentation technique\neach yielded moderate gains.\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 5https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nTable 1. The results for the base model and each model version (an additional component was added to each system). The columns under Pearson\ncorrelation analysis show the scores for the test set (all) and the four subsets of the test set, which included sentences regarding patients’ conditions or\nstatuses (status), patients’ education or interactions (education), patients’ medications (meds), and miscellaneous topics (miscellaneous).\nPearson correlation analysisModel name\nMiscellaneous, rMeds, rEducation, rStatus, rAll, r\n0.4140.5220.7210.6430.842BERT_base\n0.4250.5410.7350.6620.848ClinicalBERT\n0.4320.5530.7370.6710.855ClinicalBERT-DAa\n0.4270.5320.7420.6750.861ClinicalBERT-DA + GCN_rand\n0.4420.5360.7530.6820.863ClinicalBERT-DA + GCN_cui2vec\n0.4630.5620.7610.6930.868ClinicalBERT-DA + GCN_snomedkge\naThe ClinicalBERT-DA model refers to the ClinicalBERT model after data augmentation.\nAdding a graph encoder, in addition to our other modifications,\nto incorporate the concept graphs resulted in minor\nimprovements when the node embeddings were either initialized\nrandomly or initialized with pretrained Cui2Vec embeddings.\nHowever, using SNOMED CT KGEs as the node features in\nthe GCN resulted in an increase in performance, that is, an\nincrease of 1.3% above the performance of ClinicalBERT (ie,\nafter data augmentation), suggesting that SNOMED CT KGEs\nserved as better starting representations of the concepts. It is\nworth noting that since the BERT-based text encoder is\ninitialized with a pretrained checkpoint, it might be especially\nimportant to initialize the graph encoder with decent pretrained\nembeddings to allow the graph encoder to “catch up” with the\ntext encoder. We called this best performing setting\nClinicalBERT_all.\nWe also manually categorized the sentence pairs into the\nfollowing four categories: sentences related to patients’\nconditions and statuses (status), education or interactions with\npatients (education), medications (meds), and miscellaneous or\nclearly dissimilar topics (miscellaneous). The columns in Table\n1 (those under Pearson correlation analysis) show the scores\nfor the test set (all) and for the four categories described.\nSentence pairs in the status and education categories received\nrelatively higher scores, as expected, since many of the sentences\nand text snippets in these categories often repeated. Specifically,\ntext snippets beginning with “patient arrives...,” “discussed the\nrisks...,” or “identified illness as a learning need...” recurred\nnoticeably in these two categories. Further, the medication and\nmiscellaneous categories received relatively low correlation\nscores. For the miscellaneous category, this was expected, since\nmany of the sentence pairs in this category were more difficult\nfor the model to learn due to their greater variability. For the\nmedication category, the gold-standard scores assigned by the\nannotators proved to be rather inconsistent and challenging to\npredict, even upon manual review by a medical expert.\nTable 2 shows the results for ensembling and knowledge\ndistillation. First, we took the ensemble of 10 ClinicalBERT_all\nmodels with slightly varied hyperparameters and saw a moderate\nincrease in performance, as expected of ensembles. Second, in\naccordance with Xu et al [29], we took an ensemble of 10\nmodels consisting of a variety of model types (BERT_base,\nSciBERT, ClinicalBERT, MT-DNNs, and BlueBERT), along\nwith the graph encoder, based on their validation performance\nand saw a slight improvement. Finally, by using a teacher\nbounded regression loss function [33], we used the outputs of\nthe multisource ensemble model as soft labels to train more\nbest-setting models of different types and took an ensemble\nconsisting of 10 such knowledge-distilled models for slight\nperformance gain.\nTable 2. Results for the ensembling of the best performing models from (ClinicalBERT_all), the ensembling of multiple language models (LMs; each\nwith a graph convolutional network), and the ensembling of knowledge-distilled (KD) multisource ensembles.\nPerformance, %Ensemble type\n87.5Ensemble of ClinicalBERT_all\n87.8Ensemble with multiple LMs\n88.2Ensemble of KD models\n90.1IBM-N2C2a\naThe best performing model from the IBM team at the time of the competition was included for reference.\nDiscussion\nMain Findings\nWe implemented a list of techniques in our pipeline for the\nclinical MedSTS benchmark task and reported slight to moderate\nimprovements in performance for each technique. Using a\npretrained, off-the-shelf, BERT-based model and fine-tuning it\nalone served as a strong baseline that outperformed all\npre-BERT systems in the task. We found that our data\naugmentation technique helped slightly, but again, Wang et al\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 6https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n[15] has provided more interesting and effective data\naugmentation approaches for MedSTS.\nAdding a graph encoder to incorporate concept graphs into the\npipeline yielded decent gains, especially when the graph encoder\nwas initialized by using pretrained SNOMED CT KGEs. We\nstress that since the graph encoder was trained jointly with a\npretrained text encoder, it is important to consider providing\nthe graph encoder with pretrained embeddings as well, so that\nit does not fall too far behind in training.\nAs expected, ensembling leads to improved performance.\nFurther improvements can be achieved by using language\nmodels from different sources as well as by performing\nknowledge distillation, which can be followed by the ensembling\nof the distilled models.\nWe also attempted to use several other techniques that did not\nyield any performance gains. First, we tried multi-task learning\nby using different general and clinical domain NLP data sets,\nincluding the Medical Natural Language Inference [35],\nRecognizing Question Entailment [36], and English STS-B [37]\ndata sets, following an implementation of multi-task learning\nfor MT-DNNs, but this approach did not result in any\nimprovements and substantially increased the training time.\nSecond, we tried manually annotating the MedSTS data for\ndifferent sentence categories (medication, status, education, and\nmiscellaneous). This was done as an auxiliary classification\ntask (also an example of multi-task learning), but this did not\nlead to noticeable gains in performance. Lastly, we tried\nexperimenting with different variants of GCNs, but we found\nthat training multiple types of graph neural networks jointly\nwith a large language model was difficult in terms of\nhyperparameter tuning and decided to limit our analysis to basic\nGCNs.\nLimitations of the Method\nAlthough the results show that the strategies for data\naugmentation and the incorporation of domain knowledge\nthrough concept embeddings and GCNs do confer some benefit,\nwe address some of the limitations in this section.\nThe data augmentation techniques we used involved including\nadditional textual and semantic information from the MetaMap\noutput and reversing the sentence order to double the data set\nsize. There are many other potential data augmentation\ntechniques in the general NLP field that could be useful.\nNotably, Wang et al [15] recently performed segment reordering\nand back-translation to substantially improve their model’s\nperformance on a task.\nAs for the pretrained concept embeddings and GCNs, combining\nthem with a large pretrained language model is still largely\nexperimental. This can be improved by using recent\ndevelopments in the field of graph representation learning, such\nas graph attention networks [38] and graph matching networks\n[39].\nLimitations of the Data Set\nBoth the positive and negative findings should be considered\nwith caution due to the abundance of the potential ways of\nimplementing each component as well as the size and quality\nof the data set, which was relatively smaller and of lower quality\ncompared to data sets in mainstream, nonclinical NLP domains\nthat have less complicated access to labeled data.\nAfter working closely with the data set for several months, we\nnoticed that certain sentence pairs had large irregularities in\nterms of their scores from the two annotators of the data set.\nThis was the most notable in the sentence pairs that discussed\nmedications; often, these sentence pairs described the\nprescribing of medications to patients and differed in terms of\ndosing or drug class. At one level of categorization, the\nsimilarity of a sentence pair related to prescribing could be seen\nas high, regardless of the medication class or dosing. At another\nlevel of categorization, it appeared that several such pairs were\nnoted to be of low similarity when the medications or dosing\nregimens differed. This discrepancy in scoring also seemed to\ndiffer depending on the drug classes being mentioned. Without\nknowing which annotator was responsible for a given score, it\nis difficult to speak conclusively, but we speculate that certain\ndrug classes were of greater salience to each annotator. As an\nexample, someone with a specialty in a mental health may\nsubjectively perceive 2 different psychiatric medications of\ndifferent classes to be quite different but view cardiology drugs\nto be subjectively more similar. In contrast, an individual in the\nfield of cardiology may perceive various cardiology drugs as\nbeing different but may perceive drugs in the psychiatric\nmedications category overall as being more similar. Such\ndifferences in perspectives may also be influenced by aspects\nof an annotator’s practice, such as whether their practice occurs\nin inpatient settings, outpatient settings, the operating room, or\nthe medical clinic.\nMany of the scoring irregularities may have been related to the\nnature of the task of rating subjective similarity. One approach\nto mitigating annotator bias, as discussed in the original MedSTS\npaper [3], is to increase the number of annotators and set the\naverage score as the gold standard. For example, in the English\nSTS-B, 5 annotators were used for each sentence, and annotators\nwere limited to a certain number of sentence pairs that they\ncould annotate [37]. Although such an approach can be\nprohibitively expensive due to the need to hire enough medical\nannotators and be very cumbersome to implement for clinical\ntext due to patient privacy protections, another approach for the\ncase of having few annotators could be to reveal potentially\nbiasing factors toward annotation, such as clinical background,\nor to assign an annotator ID to each score. Stating the biases or\nallowing teams to model the annotator biases may help with\nunderstanding scoring irregularities that may be difficult to\nresolve without the use of specifically tailored algorithm designs\nor features, which require specific domain knowledge to adapt\nto unique annotator biases.\nDespite our concerns with the fundamental difficulty of\nobjectively rating subjective semantic similarity, the high\nPearson correlation coefficient achieved by our model suggests\nthat the task is still largely tractable. MedSTS also remains one\nof the few, if not only, publicly available data sets for studying\nclinical STS in EHRs. We hope that our suggestions may\nintroduce additional strategies for modeling the variance from\nsubjective elements and provide some insights to future data\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 7https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\nset annotation processes for this important yet challenging\nproblem.\nConclusions\nAs participants of the 2019 n2c2/OHNLP shared task challenge,\nwe developed a system for the clinical MedSTS benchmark task\nby combining BERT-based text encoders and GCN-based graph\nencoders in order to incorporate domain knowledge into the\nNLP pipeline. We also experimented with other techniques\ninvolving data augmentation, pretrained concept embeddings,\nensembling, and knowledge distillation to further increase our\nmodel’s performance. Although our results lagged behind those\nof the top scoring model at the n2c2 workshop, the incorporation\nof domain knowledge into deep learning NLP models via\ngraph-based methods was a new advance in clinical NLP. We\nhighlight our concerns about the impact of specific difficulties\nwith subjective semantic similarities in data set annotation, but\noverall, we believe that clinical semantic similarity remains an\nimportant topic of study, and continued work on the MedSTS\nbenchmark—one of the few clinical STS data sets\navailable—will yield advances in processing valuable\nunstructured data in EHRs. The MedSTS data set should\ncontinue to be improved and enlarged through the further careful\nannotation of the original pool of sentence pairs, and future\nwork should explore novel methods that can effectively leverage\nboth linguistic and domain knowledge.\nAcknowledgments\nThis project was supported by the National Institutes of Health Training Grant (grant 5T15LM007056-33), the National Institute\nof Mental Health Training Grant (grant 2R25MH071584-12), and the Veterans Affairs Boston Medical Informatics Fellowship.\nConflicts of Interest\nNone declared.\nReferences\n1. Hirschtick RE. A piece of my mind. Copy-and-paste. JAMA 2006 May 24;295(20):2335-2336. [doi:\n10.1001/jama.295.20.2335] [Medline: 16720812]\n2. Wang Y, Fu S, Shen F, Henry S, Uzuner O, Liu H. The 2019 n2c2/OHNLP track on clinical semantic textual similarity:\noverview. JMIR Med Inform 2020 Nov 27;8(11):e23375 [FREE Full text] [doi: 10.2196/23375] [Medline: 33245291]\n3. Wang Y, Afzal N, Fu S, Wang L, Shen F, Rastegar-Mojarad M, et al. MedSTS: a resource for clinical semantic textual\nsimilarity. Lang Resour Eval 2018 Oct 24;54:57-72. [doi: 10.1007/s10579-018-9431-1]\n4. Devlin J, Chang MW, Lee K, Toutanova K. Bert: Pre-training of deep bidirectional transformers for language understanding.\narXiv Preprint posted online on May 24, 2019. [FREE Full text]\n5. Peng Y, Yan S, Lu Z. Transfer learning in biomedical natural language processing: An evaluation of BERT and ELMo on\nten benchmarking datasets. 2019 Aug Presented at: The 18th BioNLP Workshop and Shared Task; August 1, 2019; Florence,\nItaly p. 58-65 URL: https://aclanthology.org/W19-5006.pdf [doi: 10.18653/v1/w19-5006]\n6. Logan IV RL, Liu NF, Peters ME, Gardner M, Singh S. Barack's wife Hillary: Using knowledge-graphs for fact-aware\nlanguage modeling. 2019 Jul Presented at: The 57th Annual Meeting of the Association for Computational Linguistics;\nJuly 28 to August 2, 2019; Florence, Italy p. 5962-5971 URL: https://aclanthology.org/P19-1598.pdf [doi:\n10.18653/v1/p19-1598]\n7. Peters ME, Neumann M, Logan IV RL, Schwartz R, Joshi V, Singh S, et al. Knowledge enhanced contextual word\nrepresentations. 2019 Nov Presented at: The 2019 Conference on Empirical Methods in Natural Language Processing and\nthe 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP); November 3-7, 2019; Hong\nKong, China p. 43-54 URL: https://aclanthology.org/D19-1005.pdf [doi: 10.18653/v1/d19-1005]\n8. Zhang Z, Han X, Liu Z, Jiang X, Sun M, Liu Q. ERNIE: Enhanced language representation with informative entities. 2019\nJul Presented at: The 57th Annual Meeting of the Association for Computational Linguistics; July 28 to August 2, 2019;\nFlorence, Italy p. 1441-1451 URL: https://aclanthology.org/P19-1139.pdf [doi: 10.18653/v1/p19-1139]\n9. Liu W, Zhou P, Zhao Z, Wang Z, Ju Q, Deng H, et al. K-BERT: Enabling language representation with knowledge graph.\nIn: Proc Conf AAAI Artif Intell. 2020 Feb Presented at: The Thirty-Fourth AAAI Conference on Artificial Intelligence;\nFebruary 7-12, 2020; New York, New York, USA p. 2901-2908. [doi: 10.1609/aaai.v34i03.5681]\n10. Kipf TN, Welling M. Semi-supervised classification with graph convolutional networks. arXiv Preprint posted online on\nFebruary 22, 2017. [FREE Full text]\n11. Goyal P, Ferrara E. Graph embedding techniques, applications, and performance: A survey. Knowl Based Syst 2018 Jul\n01;151:78-94. [doi: 10.1016/j.knosys.2018.03.022]\n12. Aronson AR. Effective mapping of biomedical text to the UMLS Metathesaurus: the MetaMap program. Proc AMIA Symp\n2001:17-21 [FREE Full text] [Medline: 11825149]\n13. Bodenreider O. The Unified Medical Language System (UMLS): integrating biomedical terminology. Nucleic Acids Res\n2004 Jan 01;32(Database issue):D267-D270 [FREE Full text] [doi: 10.1093/nar/gkh061] [Medline: 14681409]\n14. Hagberg A, Swart P, Schult D. Exploring network structure, dynamics, and function using NetworkX. 2008 Presented at:\nThe 7th Python in Science Conference (SciPy2008); August 21, 2008; Pasadena, California URL: https://permalink.lanl.gov/\nobject/tr?what=info:lanl-repo/lareport/LA-UR-08-05495 [doi: 10.2172/425288]\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 8https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n15. Wang Y, Liu F, Verspoor K, Baldwin T. Evaluating the utility of model configurations and data augmentation on clinical\nsemantic textual similarity. 2020 Jul Presented at: The 19th SIGBioMed Workshop on Biomedical Language Processing;\nJuly 9, 2020; Online URL: https://aclanthology.org/2020.bionlp-1.11.pdf [doi: 10.18653/v1/2020.bionlp-1.11]\n16. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, et al. Attention is all you need. 2017 Dec Presented\nat: The 31st International Conference on Neural Information Processing Systems; December 4-9, 2017; Long Beach,\nCalifornia, USA p. 6000-6010.\n17. Wolf T, Debut L, Sanh V, Chaumond J, Delangue C, Moi A, et al. Transformers: State-of-the-art natural language processing.\n2020 Oct Presented at: The 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations;\nNovember 16-20, 2020; Online p. 38-45 URL: https://aclanthology.org/2020.emnlp-demos.6.pdf [doi:\n10.18653/v1/2020.emnlp-demos.6]\n18. Alsentzer E, Murphy JR, Boag W, Weng WH, Jin D, Naumann T, et al. Publicly available clinical BERT embeddings.\n2019 Jun Presented at: The 2nd Clinical Natural Language Processing Workshop; June 7, 2019; Minneapolis, Minnesota\np. 72-78 URL: https://aclanthology.org/W19-1909.pdf [doi: 10.18653/v1/w19-1909]\n19. Lee J, Yoon W, Kim S, Kim D, Kim S, So CH, et al. BioBERT: a pre-trained biomedical language representation model\nfor biomedical text mining. Bioinformatics 2020 Feb 15;36(4):1234-1240 [FREE Full text] [doi:\n10.1093/bioinformatics/btz682] [Medline: 31501885]\n20. Johnson AEW, Pollard TJ, Shen L, Lehman LWH, Feng M, Ghassemi M, et al. MIMIC-III, a freely accessible critical care\ndatabase. Sci Data 2016 May 24;3:160035 [FREE Full text] [doi: 10.1038/sdata.2016.35] [Medline: 27219127]\n21. PubMed. National Library of Medicine. URL: https://pubmed.ncbi.nlm.nih.gov/ [accessed 2021-11-09]\n22. Home - PMC - NCBI. National Center for Biotechnology Information. URL: https://www.ncbi.nlm.nih.gov/pmc/ [accessed\n2021-11-09]\n23. Kipf T, Fetaya E, Wang KC, Welling M, Zemel R. Neural relational inference for interacting systems. arXiv Preprint posted\nonline on June 6, 2018. [FREE Full text]\n24. Zitnik M, Agrawal M, Leskovec J. Modeling polypharmacy side effects with graph convolutional networks. Bioinformatics\n2018 Jul 01;34(13):i457-i466 [FREE Full text] [doi: 10.1093/bioinformatics/bty294] [Medline: 29949996]\n25. Yao L, Mao C, Luo Y. Graph convolutional networks for text classification. In: Proc Conf AAAI Artif Intell. 2019 Presented\nat: The Thirty-Third AAAI Conference on Artificial Intelligence; January 27 to February 1, 2019; Honolulu, Hawaii, USA\np. 7370-7377. [doi: 10.1609/aaai.v33i01.33017370]\n26. Chang D, Balazevic I, Allen C, Chawla D, Brandt C, Taylor RA. Benchmark and best practices for biomedical knowledge\ngraph embeddings. 2020 Jul Presented at: The 19th SIGBioMed Workshop on Biomedical Language Processing; July 9,\n2020; Online p. 167-176 URL: https://aclanthology.org/2020.bionlp-1.18.pdf [doi: 10.18653/v1/2020.bionlp-1.18]\n27. Beam AL, Kompa B, Schmaltz A, Fried I, Weber G, Palmer N, et al. Clinical concept embeddings learned from massive\nsources of multimodal medical data. arXiv Preprint posted online on August 20, 2019. [FREE Full text] [doi:\n10.1142/9789811215636_0027]\n28. Agarwal K, Eftimov T, Addanki R, Choudhury S, Tamang S, Rallo R. Snomed2Vec: Random walk and Poincaré embeddings\nof a clinical knowledge base for healthcare analytics. arXiv Preprint posted online on July 19, 2019. [FREE Full text]\n29. Xu Y, Liu X, Li C, Poon H, Gao J. DoubleTransfer at MEDIQA 2019: Multi-source transfer learning for natural language\nunderstanding in the medical domain. 2019 Aug Presented at: The 18th BioNLP Workshop and Shared Task; August 1,\n2019; Florence, Italy p. 399-405 URL: https://aclanthology.org/W19-5042.pdf [doi: 10.18653/v1/w19-5042]\n30. Beltagy I, Lo K, Cohan A. SciBERT: A pretrained language model for scientific text. 2019 Nov Presented at: The 2019\nConference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural\nLanguage Processing (EMNLP-IJCNLP); November 3-7, 2019; Hong Kong, China p. 3615-3620 URL: https://aclanthology.\norg/D19-1371.pdf [doi: 10.18653/v1/d19-1371]\n31. Liu X, He P, Chen W, Gao J. Multi-task deep neural networks for natural language understanding. 2019 Jul Presented at:\nThe 57th Annual Meeting of the Association for Computational Linguistics; July 28 to August 2, 2019; Florence, Italy p.\n4487-4496 URL: https://aclanthology.org/P19-1441.pdf [doi: 10.18653/v1/p19-1441]\n32. Peng Y, Chen Q, Lu Z. An empirical study of multi-task learning on BERT for biomedical text mining. 2020 Jul Presented\nat: The 19th SIGBioMed Workshop on Biomedical Language Processing; July 9, 2020; Online p. 205-214 URL: https:/\n/aclanthology.org/2020.bionlp-1.22.pdf [doi: 10.18653/v1/2020.bionlp-1.22]\n33. Chen G, Choi W, Yu X, Han T, Chandraker M. Learning efficient object detection models with knowledge distillation.\n2017 Dec Presented at: The 31st International Conference on Neural Information Processing Systems; December 4-9, 2017;\nLong Beach, California, USA p. 742-751.\n34. Fey M, Lenssen JE. Fast graph representation learning with PyTorch Geometric. arXiv Preprint posted online on April 25,\n2019. [FREE Full text]\n35. Romanov A, Shivade C. Lessons from natural language inference in the clinical domain. 2018 Presented at: The 2018\nConference on Empirical Methods in Natural Language Processing; October 31 to November 4, 2018; Brussels, Belgium\np. 1586-1596 URL: https://aclanthology.org/D18-1187.pdf [doi: 10.18653/v1/d18-1187]\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 9https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX\n36. Abacha AB, Shivade C, Demner-Fushman D. Overview of the MEDIQA 2019 shared task on textual inference, question\nentailment and question answering. 2019 Aug Presented at: The 18th BioNLP Workshop and Shared Task; August 1, 2019;\nFlorence, Italy URL: https://aclanthology.org/W19-5039.pdf [doi: 10.18653/v1/w19-5039]\n37. Cer D, Diab M, Agirre E, Lopez-Gazpio I, Specia L. SemEval-2017 task 1: Semantic textual similarity multilingual and\ncrosslingual focused evaluation. 2017 Aug Presented at: The 11th International Workshop on Semantic Evaluation\n(SemEval-2017); August 3-4, 2017; Vancouver, Canada p. 1-14 URL: https://aclanthology.org/S17-2001.pdf [doi:\n10.18653/v1/s17-2001]\n38. Veličković P, Cucurull G, Casanova A, Romero A, Lio P, Bengio Y. Graph attention networks. arXiv Preprint posted\nonline on February 4, 2018. [FREE Full text]\n39. Li Y, Gu C, Dullien T, Vinyals O, Kohli P. Graph matching networks for learning the similarity of graph structured objects.\n2019 Jun Presented at: The 36th International Conference on Machine Learning; June 10-15, 2019; Long Beach, California\nURL: http://proceedings.mlr.press/v97/li19d/li19d.pdf [doi: 10.1093/oso/9780198788348.003.0005]\nAbbreviations\nBERT: bidirectional encoder representation from transformers\nEHR: electronic health records\nGCN: graph convolutional network\nKGE: knowledge graph embedding\nMT-DNN: multi-task deep neural network\nn2c2: National NLP Clinical Challenges\nNLP: natural language processing\nOHNLP: Open Health Natural Language Processing Consortium\nSNOMED CT: Systematized Nomenclature of Medicine Clinical Terms\nSTS:  semantic textual similarity\nSTS-B: Semantic Textual Similarity Benchmark\nUMLS: Unified Medical Language System\nEdited by Y Wang; submitted 31.07.20; peer-reviewed by A Trifan, B Dandala, M Torii; comments to author 22.10.20; revised version\nreceived 15.12.20; accepted 14.01.21; published 26.11.21\nPlease cite as:\nChang D, Lin E, Brandt C, Taylor RA\nIncorporating Domain Knowledge Into Language Models by Using Graph Convolutional Networks for Assessing Semantic Textual\nSimilarity: Model Development and Performance Comparison\nJMIR Med Inform 2021;9(11):e23101\nURL: https://medinform.jmir.org/2021/11/e23101\ndoi: 10.2196/23101\nPMID:\n©David Chang, Eric Lin, Cynthia Brandt, Richard Andrew Taylor. Originally published in JMIR Medical Informatics\n(https://medinform.jmir.org), 26.11.2021. This is an open-access article distributed under the terms of the Creative Commons\nAttribution License (https://creativecommons.org/licenses/by/4.0/), which permits unrestricted use, distribution, and reproduction\nin any medium, provided the original work, first published in JMIR Medical Informatics, is properly cited. The complete\nbibliographic information, a link to the original publication on https://medinform.jmir.org/, as well as this copyright and license\ninformation must be included.\nJMIR Med Inform 2021 | vol. 9 | iss. 11 | e23101 | p. 10https://medinform.jmir.org/2021/11/e23101\n(page number not for citation purposes)\nChang et alJMIR MEDICAL INFORMATICS\nXSL•FO\nRenderX",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.800104022026062
    },
    {
      "name": "Natural language processing",
      "score": 0.6392613649368286
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5557592511177063
    },
    {
      "name": "Semantic similarity",
      "score": 0.545170783996582
    },
    {
      "name": "Information retrieval",
      "score": 0.5270208120346069
    },
    {
      "name": "Encoder",
      "score": 0.5012097358703613
    },
    {
      "name": "Automatic summarization",
      "score": 0.4945513904094696
    },
    {
      "name": "Sentence",
      "score": 0.47230055928230286
    },
    {
      "name": "Documentation",
      "score": 0.4683838188648224
    },
    {
      "name": "Unified Medical Language System",
      "score": 0.43074074387550354
    },
    {
      "name": "Graph",
      "score": 0.4285811185836792
    },
    {
      "name": "Knowledge base",
      "score": 0.4271523952484131
    },
    {
      "name": "Theoretical computer science",
      "score": 0.1850011944770813
    },
    {
      "name": "Operating system",
      "score": 0.0
    },
    {
      "name": "Programming language",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I32971472",
      "name": "Yale University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I4210086971",
      "name": "VA Connecticut Healthcare System",
      "country": "US"
    }
  ]
}