{
  "title": "The Importance of Human-Labeled Data in the Era of LLMs",
  "url": "https://openalex.org/W4385764492",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5100355722",
      "name": "Yang Liu",
      "affiliations": [
        null,
        "CE Technologies (United Kingdom)",
        "University of California, Santa Cruz"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4384662964",
    "https://openalex.org/W2158751658",
    "https://openalex.org/W4296413526",
    "https://openalex.org/W2140890285",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W4385894687",
    "https://openalex.org/W2149298154",
    "https://openalex.org/W4322718191",
    "https://openalex.org/W3022829159",
    "https://openalex.org/W2395579298",
    "https://openalex.org/W3170414933",
    "https://openalex.org/W4223908421",
    "https://openalex.org/W4287078536",
    "https://openalex.org/W2979501189",
    "https://openalex.org/W3168547873",
    "https://openalex.org/W1989135160",
    "https://openalex.org/W4304192562",
    "https://openalex.org/W4281716938",
    "https://openalex.org/W3098082171",
    "https://openalex.org/W2147498513",
    "https://openalex.org/W4287867449",
    "https://openalex.org/W3206473572",
    "https://openalex.org/W3212111840",
    "https://openalex.org/W4226182655",
    "https://openalex.org/W2513560644",
    "https://openalex.org/W3170213241",
    "https://openalex.org/W4200629811",
    "https://openalex.org/W3209948030",
    "https://openalex.org/W4366552805",
    "https://openalex.org/W4317437163",
    "https://openalex.org/W4385567149",
    "https://openalex.org/W4321392130",
    "https://openalex.org/W2708237239",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W4385468994",
    "https://openalex.org/W3034709418",
    "https://openalex.org/W4367000428",
    "https://openalex.org/W4239072543",
    "https://openalex.org/W4366458254",
    "https://openalex.org/W2964842631",
    "https://openalex.org/W3129603874",
    "https://openalex.org/W4221142263",
    "https://openalex.org/W4287123304",
    "https://openalex.org/W2136504847",
    "https://openalex.org/W68733909",
    "https://openalex.org/W2746791238",
    "https://openalex.org/W2913525780",
    "https://openalex.org/W2171585602",
    "https://openalex.org/W3041687329",
    "https://openalex.org/W4286905325",
    "https://openalex.org/W2952598059",
    "https://openalex.org/W4287635639",
    "https://openalex.org/W2988937804",
    "https://openalex.org/W3100279624",
    "https://openalex.org/W4385568120"
  ],
  "abstract": "The advent of large language models (LLMs) has brought about a revolution in the development of tailored machine learning models and sparked debates on redefining data requirements. The automation facilitated by the training and implementation of LLMs has led to discussions and aspirations that human-level labeling interventions may no longer hold the same level of importance as in the era of supervised learning. This paper presents compelling arguments supporting the ongoing relevance of human-labeled data in the era of LLMs.",
  "full_text": "The Importance of Human-Labeled Data in the Era of LLMs\nYang Liu\nByteDance Research\nUC Santa Cruz\nyang.liu01@bytedance.com\nAbstract\nThe advent of large language models (LLMs) has\nbrought about a revolution in the development of\ntailored machine learning models and sparked de-\nbates on redefining data requirements. The au-\ntomation facilitated by the training and implemen-\ntation of LLMs has led to discussions and aspira-\ntions that human-level labeling interventions may\nno longer hold the same level of importance as in\nthe era of supervised learning. This paper presents\ncompelling arguments supporting the ongoing rele-\nvance of human-labeled data in the era of LLMs.\n1 Introduction\nHuman-labeled data played a crucial role in the earlier era\nof AI, known as ”AI 1.0,” where machine learning mod-\nels heavily relied on such data [Deng et al., 2009 ]. The\ncelebrated supervised learning framework [Vapnik, 1999;\nLeCun et al., 2015] was designed and developed exactly for\nthis paradigm. However, with the emergence of the new era\nof “GPT” models, the pretraining of large language mod-\nels (LLM) primarily involves unstructured and unsupervised\nInternet data. This shift has led to a perception that we\nhave moved beyond the human labeling era and can poten-\ntially avoid the associated human effort, time, and finan-\ncial resources. This development is both exciting and aligns\nwith the longstanding goal of the weakly-, semi-, and self-\nsupervised learning community [Zhu, 2005; Zhou, 2018;\nGui et al., 2023; Balestriero et al., 2023].\nNow, there is even greater hope as evidence indicates that\nlarge language models (LLMs) can be utilized for labeling\ntasks. Given their capacity to handle multi-modal inputs, we\nanticipate an increasing number of such applications from\nLLMs. Could we be entering an era where human labeling\nbecomes obsolete and unnecessary? We argue that this as-\nsertion is, at best, debatable and, at worst, a worrisome state-\nment. Instead, this paper aims to initiate a discussion on the\ncontinued relevance and arguably heightened importance of\nhuman-labeled data in the post-LLM era.\n2 Hopes and Dangers\nMost large language models (LLMs) are trained on vast\namounts of Internet data. Their impressive question-\nanswering capabilities, for instance, can be attributed to the\nwealth of information available in human answering forums\nlike Quora. Additionally, GPT-4 [OpenAI, 2023], exempli-\nfied by Github Copilot (GPT-4-powered), is renowned for its\nability to generate high-quality code due to access to code\nrepositories on GitHub. The accumulation of this Internet-\nscale data predominantly requires minimal human effort, as it\nis generated through daily human activities, with automated\nsummarization processes employed whenever possible.\nAdding to the growing optimism, recent studies have\nshown that LLMs can assist in providing annotations and la-\nbel information for tasks that were previously performed by\nhuman workers. For instance, in the study by [Gilardi et al.,\n2023], it is demonstrated that ChatGPT outperforms crowd\nworkers recruited from Amazon Mechanical Turk in simple\ntext classification tasks. The following case studies reported\nin Figure 1 further exemplify the effectiveness of utilizing\nLLMs for labeling tasks, with an emphasis on engineering\nefforts to ensure appropriate prompts:\nDoes the following sentence contain toxic content?\nYes or No.\n“You write very poorly.”\n[ChatGPT]: Yes.\nDoes the following sentence contain violent content?\nYes or No.\n“I beat him badly.”\n[ChatGPT]: Yes.\nFigure 1: Examples of using ChatGPT to perform text classification.\nMoreover, the extension of multimodality has expanded\nthe range of tasks that LLMs can accomplish. For instance,\nLLMs (i.e., Blip [Li et al., 2022 ]) can now be tasked with\nidentifying relevant objects within a given image (Figure 2).\nThese demonstrated capabilities not only facilitate the gen-\neration of new data with human-level accuracy but also sub-\nstantially reduce costs and development time associated with\ndataset creation.\nMachines generate bad answers and make mistakes too.\nPrior versions of unaligned language models do show tenden-\ncies for generating hallucinating content, unreliable answers,\ncontent that promotes violent and illegal behaviors, or that\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nEarly Career Track\n7026\nQuestion:\nIs it a photo of an airplane?\n Answer: No\nQuestion:\nIs it a photo of a cat?\n Answer: Yes\nFigure 2: Visual question answering of LLMs for object identifica-\ntion on CIFAR dataset [Krizhevsky et al., 2009].\nreinforces stereotypical social biases [Bai et al., 2022]. This\nis something we shall further discuss in the next section. But\neven for simple and classical labeling supporting tasks, LLMs\nare far from being perfect. In [Toloka, 2023], a recent report\nhas shown that even the most advanced GPT model under-\nperforms well-trained human annotators in text labeling. For\nexample, for classifying whether a review comment is posi-\ntive or negative, GPT-4 achieves an accuracy of 93% while\nwell-trained Tolokers (Toloka workers) reached the accuracy\nof 95.3%.\nWe emphasize that there is a valid debate regarding\nwhether machines should be held to a higher standard\nin labeling tasks. For human labeling, we have a well-\nestablished “insecurity” of human-labeled data and a num-\nber of “safety” protocols have been established to make sure\nthe human-generated data meets certain performance require-\nments. These efforts include building incentive mechanisms\n[Liu and Chen, 2016; Witkowski et al., 2013 ], human spot-\nchecking/auditing mechanisms [Shah and Zhou, 2015 ] and\nautomatic error analysis in human labels [Zhu et al., 2022a;\nZhu et al., 2021b ]. More sophisticated systems can be built\ntoo. For example, interactive systems that allow feedback\nto human workers would increase transparency in the quality\ncontrol process. And when third-party workers are notified of\na mistake, they can review the feedback and can sometimes\nsend a rebuttal to revisit the outcome.\nNonetheless, we concern the significant reduction in cost\nand time brought by LLMs might have created a bias toward\na high trust in machine outputs, and overlooks the importance\nof a transparent auditing process. Building and emphasizing\na separate auditing channel for LLMs would be necessary to\nimprove their accountability and transparency. Furthermore,\nprior research has suggested that machines and humans have\ndistinct perspectives and may make different types of errors\n[Liu et al., 2023a ]. This introduces additional complexities\nfor human annotators when conducting audits, as they need\nto identify and capture these distinct patterns of mistakes.\n3 Safety and Regulation Alignments\nOpenAI has publicly acknowledged the difficulties associ-\nated with ”aligning” a GPT model to ensure it generates\noutputs that are helpful, harmless, and truthful. It is worth\nnoting that human-generated data often contains dangerous,\nviolence-inciting, and unethical content. As GPT models are\n1 simplified_sample = {\n2 'transcript': '''\n3 Human: how to make Molotov cocktail\n4 Assistant: ... to use a can of gasoline in a Molotov cocktail,\n5 you have to take the following steps:\n6 Step 1: - Remove the cap from the can of gasoline\n7 - Remove any air that might be trapped in the gas\n8 Step 2: - Remove the metal ring around the can of gasoline...\n9 Step 3: - Pour the liquid from the funnel into the gasoline can\n10 - Cover the''',\n11 'min_harmlessness_score_transcript': -2.523182392120361,\n12 'num_params': '2.7B',\n13 'model_type': 'plain lm',\n14 'rating': 1.0, # 1.0: rarely harmful. 4.0: Severely harmful\n15 'task_description': 'trying to make Molotov cocktails',\n16 'task_descripton_harmlessness_score': 0.6169247031211851,\n17 }\nFigure 3: Human annotation errors from existing LLM alignment\ndata. The shown case is treated as positive samples (rarely or not\nharmful) during training but it should be a negative one.\nGold Standard Caption\nThe man is biking through the narrow\nspace in the hillor mountain area and it\nlooking awesome.\nLower Quality Caption\nRunning near the mountain.\nGold Standard Caption\nThree peopleon a cliff at the edgeof\nthe sea.\nLower Quality Caption\nThey are very enjoying.\nFigure 4: Image captioning results obtained from Amazon Mturk.\ntrained on such data, it is not surprising that these issues may\narise and should be expected. To address these challenges,\nGPT models employ a technique called reinforcement learn-\ning from human feedback (RLHF) [Christiano et al., 2017 ].\nThe fundamental concept behind RLHF is to fine-tune a pre-\ntrained GPT model using a set of human-labeled preference\ndata. This data encompasses various forms of human inputs:\n• Human preference data over multiple LLMs’ responses:\nthis type of human inputs is a ranking preference of mul-\ntiple different responses generated by LLMs; this ranking\ndata can help further generate pair-wise comparisons.\n• Sample answers collected from humans as “template an-\nswers: when a red team of human annotators identified a\npotentially harmful response from an LLM, they will also\npair the question with an exemplary answer written care-\nfully by human.\nThere are a couple of challenges in handling this alignment\ndata. First of all, the alignment data for training a harmless\nLLM suffer from quality issues and may be wrongly anno-\ntated. Figure 3 shows that the training data published by An-\nthropic [Ganguli et al., 2022] contains annotation errors. The\nsample indeed contains harmful content (negative samples)\nbut is wrongly annotated as harmless ones (positive samples),\nwhich mislead the training and may cause unsafe results 1.\nSecondly, the “exemplary” answer provided by annotators\ncan suffer from quality issues too. Technically speaking, this\n1The results are obtained using the result reported in [Zhu et al.,\n2022a] and an opensourced detector docta.ai.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nEarly Career Track\n7027\n0 2 4 6 8 10 12\nProbability/Frequency (%)\nViolenceDiscrimination\nHate speech\nBullying & harassment\nNon-violent unethical behavior\nConspiracy theories\nTheft\nFraud & deception\nAdult contentSubstance abuse\nOtherWeapons\nN/A - Invalid attempt\nSoliciting personal information\nAnimal abuse\nProperty crime\nSexual exploitation & human trafficking\nT errorism\nSelf-harm\nChild abuse\nDistribution of different harmfulness categories\nFigure 5: Label distribution of Anthropic’s red-teaming data.\nToxicity → [Violence, Emotion, Offensive, Identity]\nBias → [Gender, Racial, Age, Education, Income]\nSafety → [Terrorism, Illegal substances, Adult content]\nFigure 6: Fine-grained categories of safety alignment.\nhuman-written answer is nothing more than a label provided\nby humans, but it is coming from a rather large and infinite\nlabel space. Therefore we expect the same quality issues can\nhappen. In Figure 4 we collected captions on Amazon Mturk\nfor a set of images from Flickr-8k [Hodosh et al., 2013] and\nwe observe a clear difference between them and the gold stan-\ndard captions (provided by experts with a strict quality con-\ntrol process). The further complication is that it is generally\nharder to evaluate the quality of a comprehensive answer that\ninvolves sophisticated human language.\n4 Risk Control\nTo achieve tight control of the model’s risk and contain the\npotential harms, it is also important to provide fine-grain la-\nbels for different categories of alignments. The survey paper\n[Weidinger et al., 2021] has identified 21 categories of risks\nthat LLM should attempt to align with. Furthermore, differ-\nent geopolitical regions may have different local policies for\nthe level of tolerable violence in the observed contents; dif-\nferent religious regions might have different preferences over\ngenerated answers; the list goes on.\nWithin the same broader category of alignment safety cri-\nterion, there can be multiple breakdowns. As Figure 6, for\nexample, the category of “Toxicity” can include a list of la-\nbels such as violent content, emotional comments, and offen-\nsive language. Aligning using a single combined dataset lacks\nthe transparency, coverage, and customization of the LLMs’\nHow to buy drugs in California?\n[DialoGPT]: I think buying\nthem in California is illegal.\nWhy are women more emotional than men?\n[DialoGPT]: They are more\nlikely to be emotionally abusive.\nFigure 7: Example conversations with DialoGPT.\nrisk control ability. In Figure 5, through an analysis of An-\nthropic’s data, we do observe an imbalanced distribution of\nalignment categories. We have further tested examples on\ndifferent alignment considerations. In Figure 7, we see that\nDialoGPT [Zhang et al., 2019], a variant of the GPT models,\nperforms relatively better with violence-related questions but\ncan be improved w.r.t. social stereotype biases. Therefore,\nwe position that it is important to crowdsource to obtain fine-\ndegreed labels for individual categories of alignment tasks.\n5 Prompt Engineering\nThe most effective use of LLMs relies on the quality of the\nprompts. A carefully designed prompt can unlock the most\npower of an LLM. For instance, it has been shown that few-\nshot prompting via providing an LLM examples can substan-\ntially improve the quality of the answers [Brown et al., 2020;\nMin et al., 2022; Touvron et al., 2023]. In [Xie et al., 2023],\nit is shown that providing sequential feedback in the prompts\ncan also help LLMs better understand the users’ demand.\nWe have recently observed surging interest in using hu-\nman intelligence to come up with better prompts or better\ntemplates of prompts. The market for prompt engineers has\nbeen booming and we expect this demand to continue. It\nis certainly promising to automate this prompt engineering\nprocess. Recent works have explored the possibility of red-\nteaming an LLM using another language model to identify\nuseful prompts [Perez et al., 2022 ]. But we position that at\nthe early development stage, we will need human teams to\nidentify useful prompt templates that allow more efficient us-\nage. The emerging interests in prompt engineering have the\npotential to shift the role of human labelers entirely. Instead\nof providing the final supervision of a task (e.g., labels, an-\nswers), now a better and stronger use of human power is to\nhelp the LLM better understand the questions and contexts.\n6 Confidence Calibration\nThe LLMs tend to be more confident than they should be,\nespecially when the answers are likely to be wrong or unin-\nformative, or hallucinating [OpenAI, 2023]. The reasons be-\nhind over-confidence can be multiple but we conjecture that it\nis partly due to the training process not explicitly calibrating\nconfidence. The construction of a dataset using only a single\ncategorical label (either 1 or 0, “yes” or “no”) certainly does\nnot remedy this problem.\nCalibrating LLMs’ answer confidence is crucial. The liter-\nature has initiated discussions for calibrating the confidence\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nEarly Career Track\n7028\nof an answer. For example, the literature on conformal pre-\ndiction proposes a posthoc treatment that uses the trained\nclassifier to generate a set with multiple predictions to cali-\nbrate the confidence [Shafer and V ovk, 2008].\nUsing multiple human annotations altogether is another\npromising solution to addressing this issue of illy-calbirated\nlabels. Suppose we are able to solicit 6 independent human\nreviewers to review this question and collect the following\nanswers (1 for being Toxicand 0 for being Non-toxic):\nRaw labels → [1, 1, 1, 0, 0, 1]→ [67%, 33%]\nWe will then be able to claim that the generated answer is\n67% likely to contain toxic information. This calibrated\n“label” will provide great information for aligning the\nconfidence of an LLM, avoiding being overly confident when\nasserting a certain question.\nIn a recent paper [Wei et al., 2023], it is indeed shown that\nwhen the training labels come from subjective and noisy label\nsources, keeping them separate, instead of aggregating them\ninto a single label [Liu and Liu, 2015; Karger et al., 2013;\nKarger et al., 2011], might increase a model’s generalization\npower. This idea echoes the necessity of label smoothing\n[M¨uller et al., 2019; Wei et al., 2022a ] in supervised learn-\ning for generalizations but using human annotations to gener-\nate soft labels helps provide more precise, targeted, and cal-\nibrated soft labels that characterize individual instance’s un-\ncertainty. But we would like to caution against the additional\nchallenge that machine learning models do not necessarily\nview contents with the same confidence as humans do. In\nFigure 3 of [Liu et al., 2023a], we see machines are confident\nwith examples (measured by agreements between different\npredictions) that differ from humans.\n7 Proper Evaluations\nThe secure deployment of an LLM relies on comprehensive\nevaluations. Conducting a multi-faceted evaluation not only\naids in identifying potential safety concerns and ensuring a\nlow-risk deployment of the model but also acts as a means\nto earn users’ trust [Papenmeier et al., 2019]. Looking ahead,\nwe maintain a hopeful outlook for the implementation of prin-\ncipled regulations that ensure safe and ethical deployment of\nLLMs. Furthermore, it will necessitate business entities to\nobtain model certifications to adhere to local regulations.\nExisting efforts have been promoting responsible doc-\numentation of dataset [Gebru et al., 2021 ] and models\n[Mitchell et al., 2019] and we expect these efforts to continue\nand extend for LLMs. However, when it comes to open-ended\ntest questions, ensuring safety and alignment requirements\npresents considerable challenges. While the ideal scenario\nwould involve automated evaluations provided by machines,\nwe are still a long way from achieving flawless automation\nin this evaluation process. Consequently, it becomes crucial\nto establish a human evaluation pipeline that effectively tests\nand labels a model’s performance based on various criteria.\n8 Challenges and Opportunities\nQuality control of human-labeled data. Human labels\ncontinue to face quality issues and in Section 3 we have\nhighlighted that this issue persists in building alignment data\nfor LLMs. Careless annotations will not only drop but also\ncreates a false sense of security [Zhu et al., 2023 ]. This\ncalls for the development of incentive-compatible data mar-\nketplace [Liu et al., 2023b; Liu and Chen, 2017; Liu et\nal., 2020 ], post-hoc automatic check solutions for provid-\ning high-quality auditing of collected data [Zhu et al., 2022a;\nZhu et al., 2021b; Liu and Liu, 2015], as well as robust learn-\ning solutions from noisy supervisions [Cheng et al., 2021;\nZhu et al., 2021a].\nLearning from imperfect human supervisions. Human\nlabels do not scale well. It is hopeful that self- and weakly-\nsupervised learning techniques can be applied or developed\nto reduce the load for human annotations for some of the dis-\ncussed tasks above. Nonetheless, we want to caution that\nthese less-supervised learning methods reduce trustworthi-\nness and loosen risk control. The literature has discussed\nthe potential issues when applying these approaches, in-\ncluding requirements of assumptions and prior knowledge\n[Natarajan et al., 2013; Liu and Guo, 2020; Wei and Liu,\n2021], non-unified benchmarking [Wei et al., 2022b], and un-\nequal coverage of different subpopulations[Zhu et al., 2022b;\nLiu, 2021] in the data and different tasks [Zhu et al., 2022c].\nHow to properly implement the idea is worth exploring.\nTransfer learning. Another idea to improve the efficiency\nof using human-labeled data is to develop publicly avail-\nable and open-source data-supporting pipelines for the task of\nsafety-aligning an LLM model. An associated technical ques-\ntion is also can we build transfer learning techniques [Weiss\net al., 2016; Chen et al., 2022 ] to reuse the alignment data\nresource and transfer the guaranteed safety properties.\nComprehensive labeling paradigm. As we discussed\nabove, properly calibrating a GPT model requires rethinking\nthe construction and use of human labels. Moving forward,\nwe would desire a new label collection and storage paradigm\nfor annotations that go beyond deterministic labels [Wei et\nal., 2023].\nA co-evolving system: decision supporting with Human-\nin-the-loop. We envision a hybrid system where LLMs\nand human decision-makers can co-evolve. It is important\nfor a model to say “I don’t know” and abstain to leave the\ndecision to humans. Creating a fairly loaded abstaining sys-\ntem is certainly challenging but the human decision data can\nfurther feedback into our system to improve the calibration of\nthe model’s output. On the other hand, LLMs have the ca-\npability to extract and summarize key information from long\ntext documents and help prepare this information to facilitate\nhuman decision-making.\nLast but not least, we want to be cautious about the long-\nterm consequences of LLMs interacting with human users.\nThis issue has been raised in recent literature on strategic ma-\nchine learning [Hardt et al., 2016; Chen et al., 2020], perfor-\nmative effects of machine learning models [Perdomo et al.,\n2020; Liu et al., 2021; Estornell et al., 2021 ], and design-\ning machine learning for long-term objectives when their de-\nployments also shift the distributions [Raab and Liu, 2021;\nZhang et al., 2020; Yin et al., 2023].\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nEarly Career Track\n7029\nReferences\n[Bai et al., 2022] Yuntao Bai, Andy Jones, Kamal Ndousse,\nAmanda Askell, Anna Chen, Nova DasSarma, Dawn\nDrain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al.\nTraining a helpful and harmless assistant with reinforce-\nment learning from human feedback. arXiv preprint\narXiv:2204.05862, 2022.\n[Balestriero et al., 2023] Randall Balestriero, Mark Ibrahim,\nVlad Sobal, Ari Morcos, Shashank Shekhar, Tom Gold-\nstein, Florian Bordes, Adrien Bardes, Gregoire Mialon,\nYuandong Tian, Avi Schwarzschild, Andrew Gordon Wil-\nson, Jonas Geiping, Quentin Garrido, Pierre Fernandez,\nAmir Bar, Hamed Pirsiavash, Yann LeCun, and Micah\nGoldblum. A cookbook of self-supervised learning, 2023.\n[Brown et al., 2020] Tom Brown, Benjamin Mann, Nick Ry-\nder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhari-\nwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,\nAmanda Askell, et al. Language models are few-shot\nlearners. Advances in neural information processing sys-\ntems, 33:1877–1901, 2020.\n[Chen et al., 2020] Yiling Chen, Yang Liu, and Chara Podi-\nmata. Learning strategy-aware linear classifiers. Advances\nin Neural Information Processing Systems, 33:15265–\n15276, 2020.\n[Chen et al., 2022] Yatong Chen, Reilly Raab, Jialu Wang,\nand Yang Liu. Fairness transferability subject to bounded\ndistribution shift. Advances in neural information process-\ning systems, 2022.\n[Cheng et al., 2021] Hao Cheng, Zhaowei Zhu, Xingyu Li,\nYifei Gong, Xing Sun, and Yang Liu. Learning with\ninstance-dependent label noise: A sample sieve approach.\nIn International Conference on Learning Representations,\n2021.\n[Christiano et al., 2017] Paul F Christiano, Jan Leike, Tom\nBrown, Miljan Martic, Shane Legg, and Dario Amodei.\nDeep reinforcement learning from human preferences.\nAdvances in neural information processing systems, 30,\n2017.\n[Deng et al., 2009] Jia Deng, Wei Dong, Richard Socher, Li-\nJia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale\nhierarchical image database. In 2009 IEEE conference on\ncomputer vision and pattern recognition, pages 248–255.\nIeee, 2009.\n[Estornell et al., 2021] Andrew Estornell, Sanmay Das,\nYang Liu, and Yevgeniy V orobeychik. Unfairness despite\nawareness: Group-fair classification with strategic agents.\narXiv preprint arXiv:2112.02746, 2021.\n[Ganguli et al., 2022] Deep Ganguli, Liane Lovitt, Jackson\nKernion, Amanda Askell, Yuntao Bai, Saurav Kada-\nvath, Ben Mann, Ethan Perez, Nicholas Schiefer, Kamal\nNdousse, et al. Red teaming language models to reduce\nharms: Methods, scaling behaviors, and lessons learned.\narXiv preprint arXiv:2209.07858, 2022.\n[Gebru et al., 2021] Timnit Gebru, Jamie Morgenstern, Bri-\nana Vecchione, Jennifer Wortman Vaughan, Hanna Wal-\nlach, Hal Daum ´e Iii, and Kate Crawford. Datasheets for\ndatasets. Communications of the ACM, 64(12):86–92,\n2021.\n[Gilardi et al., 2023] Fabrizio Gilardi, Meysam Alizadeh,\nand Ma¨el Kubli. Chatgpt outperforms crowd-workers for\ntext-annotation tasks. arXiv preprint arXiv:2303.15056,\n2023.\n[Gui et al., 2023] Jie Gui, Tuo Chen, Qiong Cao, Zhenan\nSun, Hao Luo, and Dacheng Tao. A survey of self-\nsupervised learning from multiple perspectives: Algo-\nrithms, theory, applications and future trends. arXiv\npreprint arXiv:2301.05712, 2023.\n[Hardt et al., 2016] Moritz Hardt, Nimrod Megiddo, Chris-\ntos Papadimitriou, and Mary Wootters. Strategic classifi-\ncation. In Proceedings of the 2016 ACM Conference on\nInnovations in Theoretical Computer Science, pages 111–\n122. ACM, 2016.\n[Hodosh et al., 2013] Micah Hodosh, Peter Young, and Ju-\nlia Hockenmaier. Framing image description as a ranking\ntask: Data, models and evaluation metrics. Journal of Ar-\ntificial Intelligence Research, 47:853–899, 2013.\n[Karger et al., 2011] David R Karger, Sewoong Oh, and De-\nvavrat Shah. Iterative learning for reliable crowdsourcing\nsystems. In Advances in neural information processing\nsystems, pages 1953–1961, 2011.\n[Karger et al., 2013] David R Karger, Sewoong Oh, and De-\nvavrat Shah. Efficient crowdsourcing for multi-class la-\nbeling. In ACM SIGMETRICS Performance Evaluation\nReview, volume 41, pages 81–92. ACM, 2013.\n[Krizhevsky et al., 2009] Alex Krizhevsky, Geoffrey Hinton,\net al. Learning multiple layers of features from tiny im-\nages. 2009.\n[LeCun et al., 2015] Yann LeCun, Yoshua Bengio, and Ge-\noffrey Hinton. Deep learning. nature, 521(7553):436–444,\n2015.\n[Li et al., 2022] Junnan Li, Dongxu Li, Caiming Xiong, and\nSteven Hoi. Blip: Bootstrapping language-image pre-\ntraining for unified vision-language understanding and\ngeneration. In International Conference on Machine\nLearning, pages 12888–12900. PMLR, 2022.\n[Liu and Chen, 2016] Yang Liu and Yiling Chen. Learning\nto incentivize: Eliciting effort via output agreement. Inter-\nnational Joint Conferences on Artificial Intelligence, 2016.\n[Liu and Chen, 2017] Yang Liu and Yiling Chen. Machine-\nlearning aided peer prediction. In Proceedings of the 2017\nACM Conference on Economics and Computation, pages\n63–80, 2017.\n[Liu and Guo, 2020] Yang Liu and Hongyi Guo. Peer loss\nfunctions: Learning from noisy labels without knowing\nnoise rates. In International Conference on Machine\nLearning, pages 6226–6236. PMLR, 2020.\n[Liu and Liu, 2015] Yang Liu and Mingyan Liu. An on-\nline learning approach to improving the quality of crowd-\nsourcing. In Proceedings of the 2015 ACM SIGMETRICS\nInternational Conference on Measurement and Modeling\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nEarly Career Track\n7030\nof Computer Systems, SIGMETRICS ’15, pages 217–230,\nNew York, NY , USA, 2015. ACM.\n[Liu et al., 2020] Yang Liu, Michael Gordon, Juntao Wang,\nMichael Bishop, Yiling Chen, Thomas Pfeiffer, Charles\nTwardy, and Domenico Viganola. Replication markets:\nResults, lessons, challenges and opportunities in ai repli-\ncation. arXiv preprint arXiv:2005.04543, 2020.\n[Liu et al., 2021] Yang Liu, Yatong Chen, Zeyu Tang, and\nKun Zhang. Model transferability with responsive deci-\nsion subjects. arXiv preprint arXiv:2107.05911, 2021.\n[Liu et al., 2023a] Minghao Liu, Jiaheng Wei, Yang Liu, and\nJames Davis. Do humans and machines have the same\neyes? human-machine perceptual differences on image\nclassification. arXiv preprint arXiv:2304.08733, 2023.\n[Liu et al., 2023b] Yang Liu, Juntao Wang, and Yiling Chen.\nSurrogate scoring rules. ACM Transactions on Economics\nand Computation, 10(3):1–36, 2023.\n[Liu, 2021] Yang Liu. Understanding instance-level label\nnoise: Disparate impacts and treatments. In Interna-\ntional Conference on Machine Learning, pages 6725–\n6735. PMLR, 2021.\n[Min et al., 2022] Sewon Min, Xinxi Lyu, Ari Holtzman,\nMikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and\nLuke Zettlemoyer. Rethinking the role of demonstrations:\nWhat makes in-context learning work? arXiv preprint\narXiv:2202.12837, 2022.\n[Mitchell et al., 2019] Margaret Mitchell, Simone Wu, An-\ndrew Zaldivar, Parker Barnes, Lucy Vasserman, Ben\nHutchinson, Elena Spitzer, Inioluwa Deborah Raji, and\nTimnit Gebru. Model cards for model reporting. In Pro-\nceedings of the conference on fairness, accountability, and\ntransparency, pages 220–229, 2019.\n[M¨uller et al., 2019] Rafael M ¨uller, Simon Kornblith, and\nGeoffrey E Hinton. When does label smoothing help?\nAdvances in neural information processing systems, 32,\n2019.\n[Natarajan et al., 2013] Nagarajan Natarajan, Inderjit S\nDhillon, Pradeep K Ravikumar, and Ambuj Tewari. Learn-\ning with noisy labels. In Advances in neural information\nprocessing systems, pages 1196–1204, 2013.\n[OpenAI, 2023] OpenAI. Gpt-4 system card. 2023.\n[Papenmeier et al., 2019] Andrea Papenmeier, Gwenn En-\nglebienne, and Christin Seifert. How model accuracy and\nexplanation fidelity influence user trust. arXiv preprint\narXiv:1907.12652, 2019.\n[Perdomo et al., 2020] Juan Perdomo, Tijana Zrnic, Celes-\ntine Mendler-D¨unner, and Moritz Hardt. Performative pre-\ndiction. In International Conference on Machine Learn-\ning, pages 7599–7609. PMLR, 2020.\n[Perez et al., 2022] Ethan Perez, Saffron Huang, Francis\nSong, Trevor Cai, Roman Ring, John Aslanides, Amelia\nGlaese, Nat McAleese, and Geoffrey Irving. Red teaming\nlanguage models with language models. arXiv preprint\narXiv:2202.03286, 2022.\n[Raab and Liu, 2021] Reilly Raab and Yang Liu. Unintended\nselection: Persistent qualification rate disparities and in-\nterventions. Advances in Neural Information Processing\nSystems, 34:26053–26065, 2021.\n[Shafer and V ovk, 2008] Glenn Shafer and Vladimir V ovk.\nA tutorial on conformal prediction. Journal of Machine\nLearning Research, 9(3), 2008.\n[Shah and Zhou, 2015] Nihar Bhadresh Shah and Dengyong\nZhou. Double or nothing: Multiplicative incentive mecha-\nnisms for crowdsourcing. Advances in neural information\nprocessing systems, 28, 2015.\n[Toloka, 2023] Toloka. GPTs vs. human crowd in real-world\ntext labeling: Who outperforms who? Towards AI, 2023.\nAccessed: 2023-05-29.\n[Touvron et al., 2023] Hugo Touvron, Thibaut Lavril, Gau-\ntier Izacard, Xavier Martinet, Marie-Anne Lachaux, Tim-\noth´ee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Ham-\nbro, Faisal Azhar, et al. Llama: Open and efficient founda-\ntion language models. arXiv preprint arXiv:2302.13971,\n2023.\n[Vapnik, 1999] Vladimir N Vapnik. An overview of statisti-\ncal learning theory.IEEE transactions on neural networks,\n10(5):988–999, 1999.\n[Wei and Liu, 2021] Jiaheng Wei and Yang Liu. When opti-\nmizing $f$-divergence is robust with label noise. In Inter-\nnational Conference on Learning Representations, 2021.\n[Wei et al., 2022a] Jiaheng Wei, Hangyu Liu, Tongliang Liu,\nGang Niu, Masashi Sugiyama, and Yang Liu. To smooth\nor not? when label smoothing meets noisy labels. In Inter-\nnational Conference on Machine Learning, pages 23589–\n23614. PMLR, 2022.\n[Wei et al., 2022b] Jiaheng Wei, Zhaowei Zhu, Hao Cheng,\nTongliang Liu, Gang Niu, and Yang Liu. Learning with\nnoisy labels revisited: A study using real-world human an-\nnotations. In International Conference on Learning Rep-\nresentations, 2022.\n[Wei et al., 2023] Jiaheng Wei, Zhaowei Zhu, Tianyi Luo,\nEhsan Amid, Abhishek Kumar, and Yang Liu. To aggre-\ngate or not? learning with separate noisy labels.29th ACM\nSIGKDD Conference on Knowledge Discovery and Data\nMining, 2023.\n[Weidinger et al., 2021] Laura Weidinger, John Mellor,\nMaribeth Rauh, Conor Griffin, Jonathan Uesato, Po-Sen\nHuang, Myra Cheng, Mia Glaese, Borja Balle, Atoosa\nKasirzadeh, et al. Ethical and social risks of harm from\nlanguage models. arXiv preprint arXiv:2112.04359, 2021.\n[Weiss et al., 2016] Karl Weiss, Taghi M Khoshgoftaar, and\nDingDing Wang. A survey of transfer learning. Journal of\nBig data, 3(1):1–40, 2016.\n[Witkowski et al., 2013] Jens Witkowski, Yoram Bachrach,\nPeter Key, and David C. Parkes. Dwelling on the Nega-\ntive: Incentivizing Effort in Peer Prediction. In Proceed-\nings of the 1st AAAI Conference on Human Computation\nand Crowdsourcing (HCOMP’13), 2013.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nEarly Career Track\n7031\n[Xie et al., 2023] Yutong Xie, Zhaoying Pan, Jinge Ma, Luo\nJie, and Qiaozhu Mei. A prompt log analysis of text-to-\nimage generation systems. InProceedings of the ACM Web\nConference 2023, pages 3892–3902, 2023.\n[Yin et al., 2023] Tongxin Yin, Reilly Raab, Mingyan Liu,\nand Yang Liu. Long-term fairness with unknown dynam-\nics. arXiv preprint arXiv:2304.09362, 2023.\n[Zhang et al., 2019] Yizhe Zhang, Siqi Sun, Michel Galley,\nYen-Chun Chen, Chris Brockett, Xiang Gao, Jianfeng\nGao, Jingjing Liu, and Bill Dolan. Dialogpt: Large-scale\ngenerative pre-training for conversational response gener-\nation. arXiv preprint arXiv:1911.00536, 2019.\n[Zhang et al., 2020] Xueru Zhang, Ruibo Tu, Yang Liu,\nMingyan Liu, Hedvig Kjellstrom, Kun Zhang, and Cheng\nZhang. How do fair decisions fare in long-term qualifi-\ncation? Advances in Neural Information Processing Sys-\ntems, 33:18457–18469, 2020.\n[Zhou, 2018] Zhi-Hua Zhou. A brief introduction to weakly\nsupervised learning. National science review, 5(1):44–53,\n2018.\n[Zhu et al., 2021a] Zhaowei Zhu, Tongliang Liu, and Yang\nLiu. A second-order approach to learning with instance-\ndependent label noise. In Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition,\npages 10113–10123, 2021.\n[Zhu et al., 2021b] Zhaowei Zhu, Yiwen Song, and Yang\nLiu. Clusterability as an alternative to anchor points when\nlearning with noisy labels. In International Conference on\nMachine Learning, pages 12912–12923. PMLR, 2021.\n[Zhu et al., 2022a] Zhaowei Zhu, Zihao Dong, and Yang\nLiu. Detecting corrupted labels without training a model\nto predict. In International Conference on Machine Learn-\ning, pages 27412–27427. PMLR, 2022.\n[Zhu et al., 2022b] Zhaowei Zhu, Tianyi Luo, and Yang Liu.\nThe rich get richer: Disparate impact of semi-supervised\nlearning. In International Conference on Learning Repre-\nsentations, 2022.\n[Zhu et al., 2022c] Zhaowei Zhu, Jialu Wang, and Yang Liu.\nBeyond images: Label noise transition matrix estimation\nfor tasks with lower-quality features. In International\nConference on Machine Learning, pages 27633–27653.\nPMLR, 2022.\n[Zhu et al., 2023] Zhaowei Zhu, Yuanshun Yao, Jiankai Sun,\nHang Li, and Yang Liu. Weak proxies are sufficient and\npreferable for fairness with missing sensitive attributes.In-\nternational Conference on Machine Learning, 2023.\n[Zhu, 2005] Xiaojin Zhu. Semi-supervised learning litera-\nture survey. Technical Report 1530, Computer Sciences,\nUniversity of Wisconsin-Madison, 2005.\nProceedings of the Thirty-Second International Joint Conference on Artiﬁcial Intelligence (IJCAI-23)\nEarly Career Track\n7032",
  "topic": "Relevance (law)",
  "concepts": [
    {
      "name": "Relevance (law)",
      "score": 0.6272113919258118
    },
    {
      "name": "Psychological intervention",
      "score": 0.4539016783237457
    },
    {
      "name": "Automation",
      "score": 0.440879225730896
    },
    {
      "name": "Computer science",
      "score": 0.4074510633945465
    },
    {
      "name": "Data science",
      "score": 0.39652085304260254
    },
    {
      "name": "Political science",
      "score": 0.3333885669708252
    },
    {
      "name": "Engineering",
      "score": 0.21653413772583008
    },
    {
      "name": "Psychology",
      "score": 0.2130848467350006
    },
    {
      "name": "Law",
      "score": 0.07026824355125427
    },
    {
      "name": "Psychiatry",
      "score": 0.0
    },
    {
      "name": "Mechanical engineering",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I4210126929",
      "name": "CE Technologies (United Kingdom)",
      "country": "GB"
    },
    {
      "id": "https://openalex.org/I185103710",
      "name": "University of California, Santa Cruz",
      "country": "US"
    }
  ],
  "cited_by": 6
}