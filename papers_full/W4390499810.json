{
  "title": "A Transformer-based network intrusion detection approach for cloud security",
  "url": "https://openalex.org/W4390499810",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1992353383",
      "name": "Zhenyue Long",
      "affiliations": [
        "China Southern Power Grid (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2123796334",
      "name": "Huiru Yan",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2558232835",
      "name": "Gui-Quan Shen",
      "affiliations": [
        "China Southern Power Grid (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2099777403",
      "name": "Xiaolu Zhang",
      "affiliations": [
        "China Southern Power Grid (China)"
      ]
    },
    {
      "id": "https://openalex.org/A2606934593",
      "name": "Haoyang He",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2099247833",
      "name": "Long Cheng",
      "affiliations": [
        "China Southern Power Grid (China)",
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2123796334",
      "name": "Huiru Yan",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2606934593",
      "name": "Haoyang He",
      "affiliations": [
        "North China Electric Power University"
      ]
    },
    {
      "id": "https://openalex.org/A2099247833",
      "name": "Long Cheng",
      "affiliations": [
        "North China Electric Power University",
        "China Southern Power Grid (China)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4385731874",
    "https://openalex.org/W4384938269",
    "https://openalex.org/W4323022485",
    "https://openalex.org/W3014099641",
    "https://openalex.org/W4383370840",
    "https://openalex.org/W4207060264",
    "https://openalex.org/W4318586189",
    "https://openalex.org/W2981025625",
    "https://openalex.org/W2548647095",
    "https://openalex.org/W2295564429",
    "https://openalex.org/W3120223472",
    "https://openalex.org/W433644524",
    "https://openalex.org/W2142889610",
    "https://openalex.org/W2116065364",
    "https://openalex.org/W3093410479",
    "https://openalex.org/W3006503311",
    "https://openalex.org/W2986887798",
    "https://openalex.org/W4307042912",
    "https://openalex.org/W4285731979",
    "https://openalex.org/W2540500299",
    "https://openalex.org/W2775053830",
    "https://openalex.org/W3180015740",
    "https://openalex.org/W2783741806",
    "https://openalex.org/W2399941526",
    "https://openalex.org/W2112796928",
    "https://openalex.org/W2786585314",
    "https://openalex.org/W2335999708",
    "https://openalex.org/W3118808706",
    "https://openalex.org/W2898963524",
    "https://openalex.org/W4323275401",
    "https://openalex.org/W4385471846",
    "https://openalex.org/W4290876096",
    "https://openalex.org/W3125783724",
    "https://openalex.org/W4376643890",
    "https://openalex.org/W2105742458"
  ],
  "abstract": "Abstract The distributed architecture of cloud computing necessitates robust defense mechanisms to secure network-accessible resources against a diverse and dynamic threat landscape. A Network Intrusion Detection System (NIDS) is pivotal in this context, with its efficacy in cloud environments hinging on its adaptability to evolving threat vectors while mitigating false positives. In this paper, we present a novel NIDS algorithm, anchored in the Transformer model and finely tailored for cloud environments. Our algorithm melds the fundamental aspects of network intrusion detection with the sophisticated attention mechanism inherent to the Transformer model, facilitating a more insightful examination of the relationships between input features and diverse intrusion types, thereby bolstering detection accuracy. We provide a detailed design of our approach and have conducted a thorough comparative evaluation. Our experimental results demonstrate that the accuracy of our model is over 93%, which is comparable to that of the CNN-LSTM model, underscoring the effectiveness and viability of our Transformer-based intrusion detection algorithm in bolstering cloud security.",
  "full_text": "Long et al. Journal of Cloud Computing            (2024) 13:5  \nhttps://doi.org/10.1186/s13677-023-00574-9\nRESEARCH Open Access\n© The Author(s) 2023. Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which \npermits use, sharing, adaptation, distribution and reproduction in any medium or format, as long as you give appropriate credit to the \noriginal author(s) and the source, provide a link to the Creative Commons licence, and indicate if changes were made. The images or \nother third party material in this article are included in the article’s Creative Commons licence, unless indicated otherwise in a credit line \nto the material. If material is not included in the article’s Creative Commons licence and your intended use is not permitted by statutory \nregulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder. To view a copy of this \nlicence, visit http://creativecommons.org/licenses/by/4.0/.\nJournal of Cloud Computing:\nAdvances, Systems and Applications\nA Transformer-based network intrusion \ndetection approach for cloud security\nZhenyue Long1, Huiru Yan2, Guiquan Shen1, Xiaolu Zhang1, Haoyang He2 and Long Cheng1,2* \nAbstract \nThe distributed architecture of cloud computing necessitates robust defense mechanisms to secure network-accessi-\nble resources against a diverse and dynamic threat landscape. A Network Intrusion Detection System (NIDS) is pivotal \nin this context, with its efficacy in cloud environments hinging on its adaptability to evolving threat vectors while mit-\nigating false positives. In this paper, we present a novel NIDS algorithm, anchored in the Transformer model and finely \ntailored for cloud environments. Our algorithm melds the fundamental aspects of network intrusion detection \nwith the sophisticated attention mechanism inherent to the Transformer model, facilitating a more insightful exami-\nnation of the relationships between input features and diverse intrusion types, thereby bolstering detection accuracy. \nWe provide a detailed design of our approach and have conducted a thorough comparative evaluation. Our experi-\nmental results demonstrate that the accuracy of our model is over 93%, which is comparable to that of the CNN-LSTM \nmodel, underscoring the effectiveness and viability of our Transformer-based intrusion detection algorithm in bolster-\ning cloud security.\nKeywords Cloud computing, Network intrusion detection, Transformer model, Attention mechanism, Network \nsecurity\nIntroduction\nIn recent years, cloud computing has grown rapidly \nbecause it provides on-demand, simplified network \naccess and a shared pool of configurable computing \nresources that can be quickly provisioned and released \nwith little management or service provider interac -\ntion [1]. Based on these advantages, cloud computing is \nbeing deployed in more and more areas and has attracted \nan increasing number of applications to migrate to cloud \nenvironments [2, 3].\nAs cloud services are delivered over the Internet, the \nsecurity and privacy of the cloud resources and services \nbeing deployed are also receiving increased attention [4, 5]. \nAt the network layer, cloud suffers from traditional attacks \nsuch as IP spoofing, Address Resolution Protocol (ARP) \nspoofing, Routing Information Protocol (RIP) attack, \nDNS poisoning, man-in-the-middle attack, port scanning, \ninsider attack, Denial of Service (DoS), Distributed Denial \nof Service (DDoS), etc  [6]. To address such issues, major \ncloud providers (such as Amazon EC2, Microsoft Azure, \nOpen Nebula, etc.) use the firewalls. Firewalls protect the \nfront-end access points of a system and are considered \nthe first line of defense. However, since firewalls only sniff  \nnetwork packets at the network perimeter, they cannot \ndetect intrusion attacks. In addition, some DoS or DDoS \nattacks are too sophisticated for traditional firewalls to \ndetect. Therefore, using only a traditional firewall to block \nall intrusions is not an effective solution, extra protective \nmeasures are required.\nBesides, integrating a Network Intrusion Detection \nSystem (NIDS) in the cloud is a prevalent approach to \nsafeguard against attacks. NIDS serves as an alert mecha -\nnism, enhancing the security posture by identifying and \n*Correspondence:\nLong Cheng\nlcheng@ncepu.edu.cn\n1 Joint Laboratory on Cyberspace Security, China Southern Power Grid, \nGuangzhou, China\n2 School of Control and Computer Engineering, North China Electric \nPower University, Beijing, China\nPage 2 of 11Long et al. Journal of Cloud Computing            (2024) 13:5 \nflagging network breaches that successfully infiltrate the \nsystem. In recent years, the use of machine learning and \ndeep learning algorithms to construct detection models \nfor NIDS has become widespread [7]. Despite the signifi -\ncant performance improvements compared to traditional \ntechniques, most models rely on ample data of attack \ninstances. In real-world scenarios, an organization’s \nor enterprise’s network system generates lower quality \nattack sample data for training. As a result, deep intru -\nsion detection models have limited detection capabilities \nbased on this data [8]. Thus, an efficient network intru -\nsion detection method must be developed.\nRecently, Transformer [9] and its variants, which utilize \nself-attention mechanisms, have achieved significant suc-\ncess in performing Natural Language Processing (NLP) \ntasks such as text classification, dialogue recognition, \nand machine translation. The key idea of Transformer \nis to pre-train on a large text corpus and then apply the \ntrained model to a smaller task-specific dataset for fine \ntuning. Moreover, enlightened by Transformer’s ability to \nhandle ordered sequences of data, some researchers have \nused Transformer to detect intrusions and anomalies, \nproving its robustness in many scenarios [10].\nSince network intrusion is usually a continuous behav -\nior in time, most of the existing models do not have the \nability to learn time series features and lose time series \nfeatures. Although some methods based on Recurrent \nNeural Network (RNN) can learn time series features, \ntheir serial-based training methods have the problems of \nlong training time and low convergence efficiency. The \nattention mechanism of the Transformer can effectively \nlearn the temporal correlation of network intrusion data, \nthereby advancing the accuracy of network intrusion \ndetection. Therefore, in this paper, we propose a network \nintrusion detection method based on the Transformer \nmodel, explore the feasibility of using the Transformer \nmodel for network intrusion detection on the CIC-IDS \n2018 dataset, and verify its prediction effect.\nIn general, the main contributions of our work are \nsummarized as follows:\n• We unveil a novel intrusion detection methodology \ngrounded in Transformer technology, tailored for \ncloud ecosystems, showcasing adeptness in analyzing \nintrusion behavior characteristics, and offering pro -\ntection against a broad spectrum of attacks.\n• We provide a thorough discussion on the process of \nnetwork intrusion detection. Specifically, we initially \ndelineate the architecture of the Transformer model, \nfollowed by an in-depth elucidation of the process \nentailed in our devised intrusion detection model, \nstructured into pivotal stages: data preprocessing, \nmodel training, and label prediction.\n• We conduct a thorough evaluation of our methodol -\nogy using well-established datasets and performance \nmetrics, elaborating on the experimental setup, envi -\nronment, and dataset. The diverse experimental out -\ncomes highlight the robustness and effectiveness of \nour algorithm.\nThe remainder of this paper is organized as follows. \nFirstly, we traverse through pertinent research that \nunderpins our work. Subsequently, we unveil the archi -\ntecture of our designed model, delineate the envi -\nronment, and expound on the algorithm employed. \nThereafter, we present the results of our experiments, fol-\nlowed by a conclusion summarizing our work.\nRelated work\nNetwork security and intrusion detection\nIn this day and age, network security is highly impera -\ntive given the considerable increase in the utilization of \ncomputer networks. Consequently, sustaining network \nsecurity has become increasingly challenging  [11]. Spe -\ncifically, an intrusion is the act of attempting to violate \nsecurity policies or bypass computer and network secu -\nrity mechanisms [12]. It refers to any actions that violate \na computer system’s security policy and jeopardize the \naccuracy, confidentiality, and accessibility of a network \nresource. There are a variety of measures that can be \ntaken to address the intrusion challenge in the network, \nsuch as firewalls and intrusion detection systems. For \nexample, the work [13] develops a centralized NIDS and \nfirewall within a Software Defined Network (SDN [14]) to \nenhance SDN security against multiple types of attacks.\nThe concept of intrusion detection was first proposed \nby James Anderson in 1980  [15], and with the develop -\nment of network technology, intrusion detection systems \nbegan to develop continuously. Network intrusion detec -\ntion refers to the analysis of network behavior, security \nlogs, and other related data to detect whether the user \nhas the purpose of breaking in or breaking out of the \nsystem  [16]. It usually needs to analyze the character -\nistics of IP address for routing and forwarding, port for \napplication server access, transport protocol held, packet \narrival time and payload, and based on this, the detection \nmodel is established  [17, 18]. The model monitors the \npackets entering and leaving the network. When a sus -\npicious packet enters the network, the model blocks or \npermits the packet, and reports the abnormal situation \nto the responsible person. After receiving the report, the \nresponsible person can take further measures.\nNIDS as a common solution for solving network \nintrusion problem, has gained much attention for dec -\nades  [19]. NIDS can generally be categorized into two \ntypes: signature-based NIDS and anomaly-based NIDS. \nPage 3 of 11\nLong et al. Journal of Cloud Computing            (2024) 13:5 \n \nSignature-based NIDS identify threats by comparing net -\nwork activities with known Indicators of Compromise \n(IOC) [20], and anomaly-based NIDS analyze all network \noperations by measuring them against a pre-established \nand standardized baseline that depicts the system’s nor -\nmal behavior [21]. To detect and prevent cyber attacks on \nnetworks, researchers have devoted significant effort to \nproposing a variety of NIDS. For instance, In work [22], \na self-supervised Graph Neural Network (GNN) is imple-\nmented, which integrates and utilizes edge features for \nthe detection of network intrusions and anomalies. And \nthe work  [23] proposes a novel intrusion detection sys -\ntemapproach to deal with unbalanced and high-dimen -\nsional traffic.\nNetwork intrusion detection in clouds\nAs mentioned previously, network intrusions are being \nstudied extensively, with intrusions into cloud environ -\nments becoming a trending topic as well. This also drives \nthe development of related algorithms, numerous net -\nwork security researchers have formulated various detec -\ntion algorithms and presented solutions for intrusion \ndetection in networks. For example, the work  [24] pro -\nposes an enhanced genetic algorithm to improve intru -\nsion detection models based on support vector machines. \nIn addition, a fitness function is devised based on classifi -\ncation accuracy, false alarm rate, and data feature dimen -\nsions. And the work [25] proposes a weighted naive Bayes \nintrusion detection model based on particle swarms, \nwhich combines rough set theory and an improved par -\nticle swarm algorithm to improve the detection capabili -\nties of NIDS.\nThe works  [26, 27] use the features available in pcap \nfile format to obtain tracking analysis metadata, and pro -\nvides a general solution for network anomaly detection \nthrough k-means clustering algorithm on the basis of \nparallel hardware. Then Ji Saihua et al. [28] improve the \nk-means algorithm and applied it to intrusion detection. \nFurthermore, there are many studies that have proposed \nintrusion detection algorithms in networks using deep \nlearning. For example, the work  [29] proposes nonsym -\nmetric deep autoencoder for unsupervised feature learn -\ning and presents a deep learning classification model, \naddressing concerns about NIDS. And the work [30] uses \nSelf-Taught Learning (STL), a deep learning based tech -\nnique, on the NSL-KDD network intrusion dataset to \npropose an approach for developing such an efficient and \nflexible NIDS.\nMoreover, the performance of related algorithms has \nbeen greatly improved. Convolutional Neural Network \n(CNN) is prominent in the fields of image processing and \nNLP , among which the LeNet-5 Network [31] model pro-\nposed by Yann LeCun et al. achieves a low false positive \nrate on the MNIST dataset. Based on this model, Wang \nYong et al. [32] propose six different CNN structures for \nnetwork intrusion detection to improve the overall clas -\nsification accuracy. RNN has a good effect on the pro -\ncessing of text, speech, and sequence data, but with the \nincrease of text or sequence length, the forgetting charac-\nteristics of RNN will become more and more obvious. Jih-\nvun Kim propose the Long Short-Term Memory (LSTM) \nstructure of short-duration memory network [33], which \neffectively alleviates the disadvantage of gradient disap -\npearance in RNN. These have driven NIDS utilization \nalgorithms toward more efficient and practical.\nAs a result, many well-established network intrusion \ndetection methods have been proposed for cloud envi -\nronments. For example, the work [34] develops a useful \nintrusion detection system for the cloud environment \nby utilizing ensemble feature selection and classifica -\ntion methodologies. It is employed for selecting feature \nsets with reduced and valuable data from the intrusion \ndatasets. And the work [35] designs an effective security \nsystem that specifically targets cloud-based DoS/DDoS \nattacks utilizing a Multithreaded Network Intrusion \nDetection System (PM-NIDS) tailored to various proto -\ncols. As mentioned above, Transformer has many advan -\ntages and has been used in many areas in recent years, \nincluding network intrusion detection. For example, the \nwork [36] proposes a new intrusion detection model uti -\nlizing n-gram frequency and time-aware Transformer. \nThis model can hierarchically learn traffic features from \nboth session and packet levels while minimizing informa-\ntion. Transformer-based network detection algorithms \nhave also been proposed for cloud environments  [37]. \nSimilarly, In this paper, we propose a Transformer-based \nnetwork intrusion detection approach in a cloud environ-\nment. In addition, the model has a certain portability. By \nadjusting and optimizing the parameters according to the \nnew dataset or other types of network environment, the \nmodel can effectively adapt to the new dataset or net -\nwork environment.\nDescription and design\nIn this section, we mainly introduce the basics and the \ndetailed implementation of our algorithm and the flow of \nnetwork intrusion detection based on Transformer.\nImplementation of our algorithm\nAttention mechanism\nThe Seq2Seq (sequence-to-sequence) model is a mapping \nof one sequence to another, often used in machine dia -\nlogue, machine translation, and so on. Seq2Seq belongs \nto encoder-decoder structure, which is composed of \nencoder and decoder. In the Seq2Seq model, two RNN \nstructures act as the two modules respectively. The \nPage 4 of 11Long et al. Journal of Cloud Computing            (2024) 13:5 \nencoder encodes the input into vector C and transmits it \nto the decoder. The model structure is shown in Fig.  1a. \nAccording to the structure, in an ideal case, the complete \ninformation of the input is saved in the last state hn of \nthe encoder part. However, if the input is very long, the \ninformation in hn is incomplete, and then the information \nobtained by the decoder part will be incomplete, result -\ning in incomplete output content.\nIn order to solve the problem that Seq2Seq model uses \nincomplete information when the input is long, which \nleads to incomplete output, the attention mechanism is \nintroduced. The structure is shown in the Fig.  1b. The \nSeq2Seq model whose decoder module no longer uses a \nsingle vector C as input, but has multiple codecs such as \nC1 , C2 , and C3 . When we predict y1 , maybe y1 is focused \non C1 , so C1 is encoded semantically; when we predict y2 , \ny2 is focused on C2 , so C2 is encoded semantically, and so \non, so we simulate the human attention.\nTransformer model structure\nTransformer model adopts self-attention mechanism and \ncompletely abandons the structure of traditional RNN \nand CNN, which is widely used in NLP tasks and has \nachieved good results. Therefore, we propose a Trans -\nformer-based intrusion detection method to analyze the \ndata characteristics of intrusion behaviors.\nConsider that the encoder-decoder structure is pri -\nmarily utilized to deal with variable-length sequences, \nsuch as translation problems, while network intrusion \ndetection is fundamentally a classification problem. \nTherefore, this paper exclusively leverages the Trans -\nformer’s encoder structure, discarding the decoder. The \nencoder output serves as the global feature, onto which \na fully connected layer and a Softmax layer are added \nto produce the final output. The linear layer converts \nthe weight matrix of the encoder output into the final \nrequired dimension, and the Softmax function normal -\nizes the linear layer output into a value between (0,1), \nwhich is treated as a probability distribution and used \nas the target prediction value. The overall architecture \nof designed Transformer model is shown in Fig. 2 .\nThe original dataset is preprocessed to obtain the fea -\nture vector X , which is used as the input of the encoder. \nIn this paper, we stack the encoder with multiple lay -\ners, because with the increase of the number of layers, \nthe capacity of the network is larger and the expressive \npower is stronger, which can make the model better \nhandle long sequence data. The encoder consists of a \nmulti-head attention layer, a feed-forward neural net -\nwork layer, and a summation and normalization layer. \nThe layers are described as follows.\nMulti-head attention mechanism: The multi-head \nattention mechanism consists of multiple self-attention, \nwhich calculates the input matrix to obtain the output \nmatrix Z i , and the multi-head attention mechanism \nconcatenates the Z i to obtain the final output matrix Z . \nThe essence of this layer is to weight the input informa -\ntion and the model allocates its attention according to \nthe weights.\nFig. 1 Attention mechanism\nPage 5 of 11\nLong et al. Journal of Cloud Computing            (2024) 13:5 \n \nFeedforward neural network: The feedforward neural \nnetwork is a two-layer fully connected layer, the first layer \nis activated using the ReLU function and the second layer \nis not activated using the activation function.\nSummation and normalization layer: Two operations \nare performed in this layer: one is the residual join, which \nconcatenates the input X of the previous layer with the \noutput. The other is Layer Normalization, which normal -\nizes the hidden layers to a standard normal distribution \nto speed up convergence.\nNIDS based on Transformer\nThe overall architecture of network intrusion detection \nbased on Transformer model is shown in Fig.  3, which \nis divided into three stages: data preprocessing, model \ntraining, and prediction and model evaluation.\nData preprocessing\nAs illustrated in Fig.  3, the preprocessing stage involves \nlabel coding, normalization, and batch processing of the \ndataset. Initially, the dataset is divided into a 7:3 ratio of \ntraining set and test set by label coding and normaliza -\ntion. Subsequently, the training set is utilized for model \ntraining while the test set is used for prediction and eval -\nuation. This sequence of operations is described in fur -\nther detail below.\nLabel coding: In the original dataset, the labels are in \nthe form of strings that cannot be processed directly by \nthe model. Therefore, it is necessary to convert them to \nnumerical data for mathematical calculation and analy -\nsis. Label encoding involves encoding of the label of a \ndata type into a one-hot vector. For example, botnets are \ncoded as [0,1,0,0,0,0,0].\nFig. 2 The architecture of designed Transformer model\nFig. 3 The architecture of NIDS based on Transformer\nPage 6 of 11Long et al. Journal of Cloud Computing            (2024) 13:5 \nNormalization: Due to the large difference of data \nin the dataset, which is not conducive to the training of \nthe model, the dataset should be normalized first. In this \npaper, we use the MinMaxScaler function from sklearn, \nwhich normalizes each column of the data to maintain \nthe shape of the original data, and the data range is (0,1) \nafter processing. The equations are as follows:\nWhere x is the data in any column of dataset, xmin is the \nminimum value of the column data, xmax is the maximum \nvalue of the column data, xstd is the result after stand -\nardization, xscaled is the data after the final normalization, \nmax is the maximum value of the final mapping interval, \nmin is the minimum value of the final mapping interval.\nBatch data processing:  Since there are tens of thou -\nsands of data in the dataset, in order to avoid the prob -\nlems of insufficient memory and slow training speed, the \ndata cannot be directly input into the model for training \nat one time, so the dataset is processed in batches first. \nIn this paper, the DataLoader function in Pytorch is used \nfor batch processing. The specific operation process is \ndivided into three steps. Firstly, the TensorDataSet func -\ntion was used to convert the processed training data. The \nconverted data type was TensorDataSet, which could be \nidentified by Pytorch. Secondly, input the dataset con -\nverted from the first step into the DataLoader function, \nand the function will generate an iterator to facilitate the \ntraining of subsequent batches of data. The batch size can \nbe set through the function parameter batchsize. Thirdly, \nthe iterator in the second step is used to obtain a small \nbatch of data, and then the model is trained.\nModel training\nAfter the data is input to the model, the predicted value \ncorresponding to the input data is output through for -\nward propagation. Then the loss function is used to cal -\nculate the difference between the predicted value and the \nreal value, that is, the loss value, and the model param -\neters are updated through reverse propagation to reduce \nthe loss, so that the predicted value of the model is con -\nstantly close to the real value. After several iterations, the \nmodel with better performance is found by observing the \ndecline rate of the loss value. Adam is used as the opti -\nmizer and CrossEntropyLoss function is used as the loss \nfunction. The two realize back propagation in the train -\ning of the whole model, and update the parameters of the \nmodel until the model converges.\nThe loss function is a measure of the difference between \nthe predicted value of the model and the true value. The \n(1)xstd = x − xmin\nxmax − xmin\n(2)xscaled = xstd ∗(max − min ) + min\nsmaller the loss, the better the performance of the model. \nThe CrossEntropyLoss function is used in this model, the \nfunction formula is shown in Eq. 3.\nWhere y represents the predicted output of the model, \nand lable is the true label of the sample.\nExperimental evaluation\nIn this section, we test the designed scheme and evalu -\nate its feasibility. Firstly, the experimental setup is given, \nincluding the configuration of experimental environment \nand the description of dataset. Secondly, the data pre -\nprocessing process and evaluation index are described. \nFinally, the feasibility of the proposed method is verified \nby comparative analysis of different experimental results.\nExperimental settings\nExperimental environment and parameter configuration\nIn the experiment, the programming language we used \nis Python, and the Pytorch deep learning development \nframework is used to complete the design of the entire \nmodel. The parameter configuration of the experiment is \nshown in Table 1.\nDataset\nThe dataset we used is CIC-IDS 2018, which was devel -\noped by the Communications Security Establishment \n(CSE) and the Canadian Institute for Cybersecurity \nResearch for the purpose of intrusion detection research. \nThe IDS 2018 dataset contains normal network traffic \nand attack data, totaling 12,212,461 items, and contains \nupdated attack types compared to the previous dataset. \nTable  2 describes the concepts and quantities of each \nattack type. Each piece of data has 78 characteristic val -\nues in addition to a label value, including packet size, data \nstream length, duration, and data stream payload size.\n(3)loss(y, lable) =− log exp (y[lable])\nj\nexp (y[j])\nTable 1 Parameter configuration of the model\nParameters Value\nBatch size 1024\nEncoder layer 3,4,5\nThe number of neurons in the hidden layer of a fully connected \nnetwork\n512\nQ,K,V dimensions 80\nHeads of attention 8\nLoss rate 0.1\nLearning rate 0.001\nPage 7 of 11\nLong et al. Journal of Cloud Computing            (2024) 13:5 \n \nModel evaluation\nDetection performance\nWe have adopted the normally used indicators: Accuracy, \nprecision, recall, and F 1 − score for the evaluation of the \nproposed system, which are defined as Eq.  4, 5, 6 and 7, \nrespectively.\nWhere TP represents the amount of data that is actu -\nally normal and is predicted to be normal; TN indicates \nthe amount of data that is actually abnormal and pre -\ndicted to be abnormal. FP  represents the amount of data \nthat is actually abnormal but predicted to be normal; FN  \nrepresents the amount of data that is actually normal but \npredicted to be abnormal.\nExperimental results\nIn this part, we first conduct an experimental exploration \nof the Transformer-based network intrusion detection \nalgorithm. After normalization and other preprocess -\ning operations, the dataset is divided into test set and \ntraining set according to the ratio of 7:3, and sent to the \nmodel for training. Then, we train the model in different \nscenarios (the number of encoder layers is 3, 4, or 5) and \nevaluate the prediction effect. Finally, we compare the \n(4)Accuracy= TP + TN\nTP + TN + FP + FN\n(5)Precision= TP\nTP + FP\n(6)Recall= TP\nTP + FP\n(7)F 1 − score= 2 ∗Precision∗Recall\nPrecision+ Recall\nmodel with the CNN-LSTM model to evaluate the model \nperformance.\nIn this experiment, the entire training set was used for \ntraining. When the number of encoder layers is 3, the \nnumber of training rounds is gradually increased, and \nthen the performance of the model is tested using the \ntest set. The detection results for each type of attack are \nshown in Table 3. It can be seen that the intrusion detec -\ntion algorithm based on Transformer model has satisfac -\ntory detection effect on Botnet, DoS, Brute-force, and \nDDoS. Its predictive index value Accuracy, Precision, \nRecall, and F 1 − score can reach 94% or even 99%.\nNext, we gradually increase the number of encoder \nlayers of the model for training. When the number of \nencoder layers is 3, 4, or 5, the comparison of the training \naccuracy trends and the comparison of the detection pre-\ncision trends are shown in Figs. 4 and 5, respectively. And \nthe comparison of the overall detection performance is \nshown in Fig. 6.\nAs can be seen from Fig.  4, with the increase of \nencoder layers, the fluctuation of model training accu -\nracy becomes smaller and smaller, and the corresponding \nprediction performance is also improved. Specifically, the \npeak accuracy difference is 8.93% when there are three \nencoder layers, it is 6.13% when there are four encoder \nTable 2 The concept and number of each attack type\nAttack Type Attack Description Number of Attack\nBenign Normal. 10856019\nBotnet By infecting a large number of hosts with bot program viruses, a one-to-many control network is formed \nbetween the controller and the infected hosts.\n144535\nInfiltration Exploit an application vulnerability to execute a backdoor on the victim’s computer, using the victim’s com-\nputer to scan the internal network and carry out an attack on other computers.\n144336\nDDoS Attack Multiple distributed servers are used to send requests to the target, resulting in responses that affect correct \nand legitimate requests.\n775955\nDoS Attack Attackers overload the system by carrying out a large number of attacks in a short period of time, making \nlegitimate requests unresponsive.\n196631\nWeb Attack Web programs scan websites for attacks on vulnerable sites, such as SQL injection. 94101\nBrute-force Attack A common form of attack that uses programs to crack passwords by brute force, often to gain unauthorized \naccess.\n884\nTable 3 Detection of each type of attack\nAccuracy Precision Recall F1-score\nNormal 93.572 94.6515 98.3251 96.4533\nBotnet 99.9891 99.7199 99.3589 99.5391\nInfilteration 98.7943 44.2619 11.776 13.228\nDDoS 94.7668 64.4014 39.4349 48.9167\nDoS 99.8151 99.2901 89.1539 93.9494\nWeb 99.8675 85.5708 99.5962 92.0523\nBrute-force 99.9612 17.2378 36.9811 12.1062\nPage 8 of 11Long et al. Journal of Cloud Computing            (2024) 13:5 \nlayers, and it is 3.57% when there are five encoder lay -\ners. At the same time, it can be seen from Fig.  5 that the \ndetection precision curve for the encoder with five layers \nis slightly higher than the other two curves.\nFigure  6 describes the numerical results of the four \nevaluation indicators when the number of encoder layers \nis 3, 4, or 5. When the number of encoder layers is 3, the \nvalues of four evaluation indexes were 93.38%, 91.7%, \n93.38%, and 92.39%. When the number of encoder layers \nis 4, they are 93.36%, 92.16%, 93.36%, and 92.10%. And \nwhen the number of encoder layers is 5, they are 93.46%, \n92.19%, 93.4%, and 92.16%. By comparison, with the \nFig. 4 Comparison of training accuracy trends when Encoder=3,4,5\nFig. 5 Comparison of detection precision trends when Encoder=3,4,5\nPage 9 of 11\nLong et al. Journal of Cloud Computing            (2024) 13:5 \n \nincrease of the number of encoder layers, the prediction \naccuracy increased by 0.46% and 0.03%.\nIn order to better evaluate the detection effect of the \nmodel, we compare the model with the CNN-LSTM \nmodel, and the experimental comparison results are \nshown in Table  4. Through comparison, it can be seen \nthat under the current experimental conditions, our \nmodel has reached the accuracy of CNN-LSTM, and has \nsome practical value initially.\nConclusion\nIn this paper, we proposed a Transformer-based net -\nwork intrusion detection algorithm tailored for cloud \nenvironments, and assessed its viability and predictive \naccuracy. By harnessing the attention mechanism of the \nTransformer model along with network intrusion detec -\ntion principles, we designed and implemented this algo -\nrithm. Our procedure entailed preprocessing the dataset, \ninitially training the model with three encoder layers, and \nevaluating its predictive performance. Subsequently, we \nincrementally increased the encoder layers for further \ntraining and benchmarked our model against the CNN-\nLSTM model. The conclusive results reveal that under \nthe specified experimental conditions, our Transformer-\nbased network intrusion detection model attained a \nprediction accuracy surpassing 93%, on par with the \nlatest method on CNN-LSTM model, showcasing its \nefficacy in predicting network intrusions within cloud \nenvironments.\nFig. 6 Comparison of detection performance indicators when Encoder=3,4,5\nTable 4 Comparison of intrusion detection capability with CNN-LSTM model\nIndicators Benign Botnet Infilteration DDoS Web Attacks Brute-force DoS\nOur Accuracy 0.9357 0.9998 0.9879 0.9477 0.9988 0.9997 0.9982\nPrecision 0.9465 0.9971 0.4427 0.6441 0.8557 0.1724 0.9929\nRecall 0.9833 0.9936 0.1178 0.3944 0.996 0.3698 0.8916\nF1-score 0.9645 0.9954 0.1323 0.4892 0.9205 0.1211 0.9395\nCNN-LSTM Accuracy 0.9457 0.9997 0.9476 0.9985 0.9988 0.9992 0.9993\nPrecision 0.9074 0.9952 0.6373 0.9953 0.0165 0.9837 0.9912\nRecall 0.9816 0.9986 0.1747 0.9986 0.7143 0.9944 0.9996\nF1-score 0.9452 0.9992 0.2722 0.9975 0.0323 0.989 0.9953\nPage 10 of 11Long et al. Journal of Cloud Computing            (2024) 13:5 \nOur future work mainly lies in the incorporation of \nGraph Neural Networks [38] into our Transformer-based \nmodel to enhance its ability to identify complex intru -\nsion patterns in distributed environments such as cloud \ndatacenters  [39]. Moreover, we also plan to extend the \napplication of our algorithm to more challenging envi -\nronments, such as edge cloud systems  [40]. These envi -\nronments, with their decentralized nature, pose unique \nchallenges that our model must adapt to. Generally, our \nlong-term goal is to develop a system which can sig -\nnificantly enhance network intrusion detection across \ndiverse cloud environments.\nAcknowledgements\nNot applicable.\nAuthors’ contributions\nZhenyue Long: Conceptualization, Writing - review & editing. Huiru Yan: \nMethodology, Implementation, Writing - original draft. Guiquan Shen: Writ-\ning- review & editing. Xiaolu Zhang: Writing - review & editing. Haoyang He: \nMethodology, Writing - review & editing. Long Cheng: Conceptualization, \nMethodology, Writing - review & editing.\nFunding\nThis work was supported by the Open Project Program of the Joint Laboratory \non Cyberspace Security, China Southern Power Grid (No. CSS2022KF01).\nAvailability of data and materials\nThe datasets generated during and/or analyzed during the current study are \navailable from the corresponding author on reasonable request.\nDeclarations\nEthics approval and consent to participate\nNot applicable.\nConsent for publication\nNot applicable.\nCompeting interests\nThe authors declare no competing interests.\nReceived: 17 October 2023   Accepted: 12 December 2023\nReferences\n 1. Cheng L, Wang Y, Cheng F, Liu C, Zhao Z, Wang Y (2023) A deep reinforce-\nment learning-based preemptive approach for cost-aware cloud job \nscheduling. IEEE Trans Sustain Comput\n 2. Zhang J, Cheng L, Liu C, Zhao Z, Mao Y (2023) Cost-aware schedul-\ning systems for real-time workflows in cloud: An approach based on \ngenetic algorithm and deep reinforcement learning. Expert Syst Appl \n234:120972\n 3. Liu F, Huang J, Wang X (2023) Joint task offloading and resource alloca-\ntion for device-edge-cloud collaboration with subtask dependencies. \nIEEE Trans Cloud Comput 11(3):3027–3039\n 4. Sun P (2020) Security and privacy protection in cloud computing: Discus-\nsions and challenges. J Netw Comput Appl 160:102642\n 5. Zhang X, Cui L, Shen W, Zeng J, Du L, He H, Cheng L (2023) File process-\ning security detection in multi-cloud environments: a process mining \napproach. J Cloud Comput 12(1):100\n 6. Jangjou M, Sohrabi MK (2022) A comprehensive survey on security \nchallenges in different network layers in cloud computing. Arch Comput \nMethods Eng 29(6):3587–3608\n 7. Li J, Tong X, Liu J, Cheng L (2023) An efficient federated learning system \nfor network intrusion detection. IEEE Syst J 17(2):2455–2464\n 8. Aldweesh A, Derhab A, Emam AZ (2020) Deep learning approaches for \nanomaly-based intrusion detection systems: A survey, taxonomy, and \nopen issues. Knowl-Based Syst 189:105124\n 9. Vaswani A, Shazeer N, Parmar N, Uszkoreit J, Jones L, Gomez AN, Kaiser Ł, \nPolosukhin I (2017) Attention is all you need. Adv Neural Inf Process Syst \n30:1–11\n 10. Dosovitskiy A, Beyer L, Kolesnikov A, Weissenborn D, Zhai X, Unterthiner \nT, Dehghani M, Minderer M, Heigold G, Gelly S, et al (2020) An image \nis worth 16x16 words: Transformers for image recognition at scale. \narXiv preprint arXiv:201011929\n 11. Garg A, Maheshwari P (2016) A hybrid intrusion detection system: A \nreview. In: 2016 10th International Conference on Intelligent Systems and \nControl (ISCO). IEEE, Coimbatore pp 1–5\n 12. Scarfone K, Mell P et al (2007) Guide to intrusion detection and preven-\ntion systems (idps). NIST Spec Publ 800(2007):94\n 13. Mantur B, Desai A, Nagegowda K (2015) Centralized control signature-\nbased firewall and statistical-based network intrusion detection system \n(nids) in software defined networks (sdn). In: Emerging Research in \nComputing, Information, Communication and Applications: ERCICA 2015, \nvol 1. Springer, Bangalore pp 497–506\n 14. Liu Q, Cheng L, Alves R, Ozcelebi T, Kuipers F, Xu G, Lukkien J, Chen S \n(2021) Cluster-based flow control in hybrid software-defined wireless \nsensor networks. Comput Netw 187:107788\n 15. Liao HJ, Lin CHR, Lin YC, Tung KY (2013) Intrusion detection system: A \ncomprehensive review. J Netw Comput Appl 36(1):16–24\n 16. Northcutt S, Novak J (2002) Network Intrusion Detection: An Analyst’s \nHandbook, 3rd edn. New Riders Publishing, USA\n 17. García-Teodoro P , Díaz-Verdejo J, Maciá-Fernández G, Vázquez E (2009) \nAnomaly-based network intrusion detection: Techniques, systems and \nchallenges. Comput Secur 28(1):18–28\n 18. Wang K, Stolfo SJ (2004) Anomalous payload-based network intrusion \ndetection. Recent Advances in Intrusion Detection (RAID 2004). Sophia \nAntipolis, France, pp 203–222\n 19. Ahmad Z, Shahid Khan A, Wai Shiang C, Abdullah J, Ahmad F (2021) Net-\nwork intrusion detection system: A systematic study of machine learning \nand deep learning approaches. Trans Emerg Telecommun Technol \n32(1):e4150\n 20. Erlacher F, Dressler F (2020) On high-speed flow-based intrusion detec-\ntion using snort-compatible signatures. IEEE Trans Dependable Secure \nComput 19(1):495–506\n 21. Alamiedy TA, Anbar M, Alqattan ZN, Alzubi QM (2020) Anomaly-based \nintrusion detection system using multi-objective grey wolf optimisation \nalgorithm. J Ambient Intell Humanized Comput 11:3735–3756\n 22. Caville E, Lo WW, Layeghy S, Portmann M (2022) Anomal-e: A self-\nsupervised network intrusion detection system based on graph neural \nnetworks. Knowl-Based Syst 258:110030\n 23. Mhawi DN, Aldallal A, Hassan S (2022) Advanced feature-selection-based \nhybrid ensemble learning algorithms for network intrusion detection \nsystems. Symmetry 14(7):1461\n 24. Teng L, Teng S, Tang F, Zhu H, Zhang W, Liu D, Liang L (2014) A collaborative \nand adaptive intrusion detection based on svms and decision trees. In: 2014 \nIEEE International Conference on Data Mining Workshop. IEEE, Shenzhen pp \n898–905\n 25. Ren X, Jiao W, Zhou D (2016) Intrusion detection model of weighted \nnavie bayes based on particle swarm optimization algorithm. Comput \nEng Appl 52(7):122–126\n 26. Velea R, Ciobanip C, Margarit L, Bica I (2017) Network traffic anomaly \ndetection using shallow packet inspection and parallel k-means data \nclustering. Stud Inform Control 26(4):387–396\n 27. Ji S, Huang S (2021) Intrusion detection algorithm based on improved \nk-means. Comput Digit Eng 49(11):2184–2188\n 28. Wu, Fei, Ting Li, Zhen Wu, ShuLin Wu, and ChuanQi Xiao (2021) Research \non network intrusion detection technology based on machine learning. \nInt J Wireless Inf Netw 28(no. 3):262–275\nPage 11 of 11\nLong et al. Journal of Cloud Computing            (2024) 13:5 \n \n 29. Shone N, Ngoc TN, Phai VD, Shi Q (2018) A deep learning approach \nto network intrusion detection. IEEE Trans Emerg Top Comput Intell \n2(1):41–50\n 30. Javaid A, Niyaz Q, Sun W, Alam M (2016) A deep learning approach for \nnetwork intrusion detection system. In: Proceedings of the 9th EAI Inter-\nnational Conference on Bio-inspired Information and Communications \nTechnologies (formerly BIONETICS). New York City, pp 21–26\n 31. LeCun Y, Bottou L, Bengio Y, Haffner P (1998) Gradient-based learning \napplied to document recognition. Proc IEEE 86(11):2278–2324\n 32. Zhou H, Wang Y, Lei X, Liu Y (2017) A method of improved cnn traffic \nclassification. In: 2017 13th International Conference on Computational \nIntelligence and Security (CIS). Hong Kong, pp 177–181\n 33. Kim J, Kim J, Thi Thu HL, Kim H (2016) Long short term memory recurrent \nneural network classifier for intrusion detection. In: 2016 International \nConference on Platform Technology and Service (PlatCon). Jeju, pp 1–5\n 34. Krishnaveni S, Sivamohan S, Sridhar S, Prabakaran S (2021) Efficient fea-\nture selection and classification through ensemble method for network \nintrusion detection on cloud computing. Clust Comput 24(3):1761–1779\n 35. Patil R, Dudeja H, Gawade S, Modi C (2018) Protocol specific multi-threaded \nnetwork intrusion detection system (pm-nids) for dos/ddos attack detec-\ntion in cloud. In: 2018 9th International Conference on Computing, Com-\nmunication and Networking Technologies (ICCCNT). IEEE, Bengaluru pp \n1–7\n 36. Han X, Cui S, Liu S, Zhang C, Jiang B, Lu Z (2023) Network intrusion detec-\ntion based on n-gram frequency and time-aware transformer. Comput \nSecur 128:103171\n 37. Ingle D, Ingle D (2023) An enhanced blockchain based security and \nattack detection using transformer in iot-cloud network. J Adv Res Appl \nSci Eng Technol 31(2):142–156\n 38. Wu L, Cui P , Pei J, Zhao L, Guo X (2022) Graph neural networks: founda-\ntion, frontiers and applications. In: Proceedings of the 28th ACM SIGKDD \nConference on Knowledge Discovery and Data Mining. Washington DC, \npp 4840–4841\n 39. Cheng L, Wang Y, Liu Q, Epema DH, Liu C, Mao Y, Murphy J (2021) \nNetwork-aware locality scheduling for distributed data operators in data \ncenters. IEEE Trans Parallel Distrib Syst 32(6):1494–1510\n 40. Chen Y, Zhao J, Hu J, Wan S, Huang J (2023) Distributed task offloading and \nresource purchasing in noma-enabled mobile edge computing: Hierarchi-\ncal game theoretical approaches. ACM Trans Embed Comput Syst\nPublisher’s Note\nSpringer Nature remains neutral with regard to jurisdictional claims in pub-\nlished maps and institutional affiliations.",
  "topic": "Intrusion detection system",
  "concepts": [
    {
      "name": "Intrusion detection system",
      "score": 0.854704737663269
    },
    {
      "name": "Computer science",
      "score": 0.7897969484329224
    },
    {
      "name": "Cloud computing",
      "score": 0.7800451517105103
    },
    {
      "name": "Transformer",
      "score": 0.6430786848068237
    },
    {
      "name": "False positive paradox",
      "score": 0.6246247291564941
    },
    {
      "name": "Adaptability",
      "score": 0.6055710315704346
    },
    {
      "name": "Network security",
      "score": 0.4707385003566742
    },
    {
      "name": "Distributed computing",
      "score": 0.4580288529396057
    },
    {
      "name": "Intrusion tolerance",
      "score": 0.4415286183357239
    },
    {
      "name": "Intrusion",
      "score": 0.44080281257629395
    },
    {
      "name": "Computer security",
      "score": 0.3992280066013336
    },
    {
      "name": "Data mining",
      "score": 0.35804852843284607
    },
    {
      "name": "Real-time computing",
      "score": 0.3380773663520813
    },
    {
      "name": "Artificial intelligence",
      "score": 0.33596330881118774
    },
    {
      "name": "Engineering",
      "score": 0.12905246019363403
    },
    {
      "name": "Operating system",
      "score": 0.08559387922286987
    },
    {
      "name": "Ecology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Geochemistry",
      "score": 0.0
    },
    {
      "name": "Electrical engineering",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ]
}