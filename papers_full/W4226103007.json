{
  "title": "Video Joint Modelling Based on Hierarchical Transformer for Co-summarization",
  "url": "https://openalex.org/W4226103007",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A2097931270",
      "name": "Haopeng Li",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A2495350316",
      "name": "Qiuhong Ke",
      "affiliations": [
        "University of Melbourne",
        "Monash University"
      ]
    },
    {
      "id": "https://openalex.org/A2129560754",
      "name": "Mingming Gong",
      "affiliations": [
        "University of Melbourne"
      ]
    },
    {
      "id": "https://openalex.org/A311940369",
      "name": "Rui Zhang",
      "affiliations": [
        "Tsinghua University"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W2338907589",
    "https://openalex.org/W2560681555",
    "https://openalex.org/W2963919999",
    "https://openalex.org/W2737677090",
    "https://openalex.org/W2781922022",
    "https://openalex.org/W2766630207",
    "https://openalex.org/W2946527862",
    "https://openalex.org/W2798970487",
    "https://openalex.org/W2902616437",
    "https://openalex.org/W2972712288",
    "https://openalex.org/W3045623285",
    "https://openalex.org/W2607326921",
    "https://openalex.org/W3150815828",
    "https://openalex.org/W1926243429",
    "https://openalex.org/W6739901393",
    "https://openalex.org/W1987366351",
    "https://openalex.org/W2109152179",
    "https://openalex.org/W2111494488",
    "https://openalex.org/W2964158702",
    "https://openalex.org/W2964167369",
    "https://openalex.org/W2908469318",
    "https://openalex.org/W3107252718",
    "https://openalex.org/W3135176014",
    "https://openalex.org/W3172837290",
    "https://openalex.org/W3103358580",
    "https://openalex.org/W3112815474",
    "https://openalex.org/W2962898677",
    "https://openalex.org/W3096609285",
    "https://openalex.org/W6784094891",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W3035029089",
    "https://openalex.org/W2895758197",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W6780226713",
    "https://openalex.org/W2139009685",
    "https://openalex.org/W6755207826",
    "https://openalex.org/W3085139254",
    "https://openalex.org/W2529272619",
    "https://openalex.org/W1924343884",
    "https://openalex.org/W2062903088",
    "https://openalex.org/W2963026017",
    "https://openalex.org/W4246193833",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W6631190155",
    "https://openalex.org/W2967038491",
    "https://openalex.org/W2993980108",
    "https://openalex.org/W2982672255",
    "https://openalex.org/W2999428529",
    "https://openalex.org/W3108154605",
    "https://openalex.org/W2967219836",
    "https://openalex.org/W1522734439",
    "https://openalex.org/W2016053056",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W4385245566",
    "https://openalex.org/W4287667694",
    "https://openalex.org/W4394666973",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W3092462694"
  ],
  "abstract": "Video summarization aims to automatically generate a summary (storyboard or video skim) of a video, which can facilitate large-scale video retrieval and browsing. Most of the existing methods perform video summarization on individual videos, which neglects the correlations among similar videos. Such correlations, however, are also informative for video understanding and video summarization. To address this limitation, we propose Video Joint Modelling based on Hierarchical Transformer (VJMHT) for co-summarization, which takes into consideration the semantic dependencies across videos. Specifically, VJMHT consists of two layers of Transformer: the first layer extracts semantic representation from individual shots of similar videos, while the second layer performs shot-level video joint modelling to aggregate cross-video semantic information. By this means, complete cross-video high-level patterns are explicitly modelled and learned for the summarization of individual videos. Moreover, Transformer-based video representation reconstruction is introduced to maximize the high-level similarity between the summary and the original video. Extensive experiments are conducted to verify the effectiveness of the proposed modules and the superiority of VJMHT in terms of F-measure and rank-based evaluation.",
  "full_text": "JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 1\nVideo Joint Modelling Based on Hierarchical\nTransformer for Co-summarization\nHaopeng Li, Qiuhong Ke, Mingming Gong, and Rui Zhang\nAbstract—Video summarization aims to automatically generate a summary (storyboard or video skim) of a video, which can facilitate\nlarge-scale video retrieval and browsing. Most of the existing methods perform video summarization on individual videos, which\nneglects the correlations among similar videos. Such correlations, however, are also informative for video understanding and video\nsummarization. To address this limitation, we propose Video Joint Modelling based on Hierarchical Transformer (VJMHT) for\nco-summarization, which takes into consideration the semantic dependencies across videos. Speciﬁcally, VJMHT consists of two\nlayers of Transformer: the ﬁrst layer extracts semantic representation from individual shots of similar videos, while the second layer\nperforms shot-level video joint modelling to aggregate cross-video semantic information. By this means, complete cross-video\nhigh-level patterns are explicitly modelled and learned for the summarization of individual videos. Moreover, Transformer-based video\nrepresentation reconstruction is introduced to maximize the high-level similarity between the summary and the original video. Extensive\nexperiments are conducted to verify the effectiveness of the proposed modules and the superiority of VJMHT in terms of F-measure\nand rank-based evaluation.\nIndex Terms—Video summarization, co-summarization, hierarchical Transformer, representation reconstruction.\n!\n1 I NTRODUCTION\nT\nHE amount of video data has been increasing expo-\nnentially on account of the popularization of online\nvideo platforms such as YouTube, Vimeo, and Facebook\nWatch. According to statistics in 2019, more than 500 hours\nof videos were uploaded to YouTube every minute 1. To\nput this into perspective, it would take a person about\n82 years to watch all the videos uploaded to YouTube in\none hour. As a result, it is difﬁcult to efﬁciently browse or\nretrieve useful information in the video data. To address\nthis problem, numerous video summarization techniques\nhave been developed in recent years [1], [2], [3], [4], [5],\n[6], [7], [8], [9], [10]. Video summarization aims to automat-\nically generate a short version of a video, which contains\nthe important people, objects, and events in the original\nvideo. Two forms of video summary are widely exploited\nin previous works, i.e., keyframe-based summary and key-\nshot-based summary. The former picks important frames to\nform a static summary (i.e., storyboard), while the latter ﬁrst\nsegments a video into shots and then selects informative\nshots to form a dynamic summary ( i.e., video skim). In this\nwork, we aim to generate the key-shot-based summary due\nto its wide applications [6], [8], [11], [12], [13].\nMost of the existing methods perform video summariza-\n• Haopeng Li is with the School of Computing and Information Systems,\nUniversity of Melbourne. E-mail: haopeng.li@student.unimelb.edu.au.\n• Qiuhong Ke is with the Department of Data Science & AI, Monash Uni-\nversity and the School of Computing and Information Systems, University\nof Melbourne. E-mail: qiuhong.ke@monash.edu.\n• Mingming Gong (mingming-gong.github.io) is with the School of\nMathematics and Statistics, University of Melbourne. E-mail: ming-\nming.gong@unimelb.edu.au.\n• Rui Zhang (www.ruizhang.info) is with Tsinghua University. E-mail:\nrayteam@yeah.net.\n• Corresponding author: Qiuhong Ke and Rui Zhang.\n1. https://www.tubeﬁlter.com/2019/05/07/\nnumber-hours-video-uploaded-to-youtube-per-minute\nCross-video Information Aggregation\nVideo 1\nVideo 2\nImportance Score Prediction\nSummary of Video 1 Summary of Video 2\nShot 1\n Shot 2\n Shot m\n Shot 1\n Shot 2\n Shot n\nFig. 1. An illustration of co-summarization for two semantically similar\nvideos. Cross-video shot-level information aggregation is performed in\nour method to model the pair-wise dependencies between two arbitrary\nshots in the two videos. Then the shot-wise importance score is pre-\ndicted based on the shot representation that contains the cross-video\ninformation. Finally, the summary of each video is generated according\nto the scores.\ntion on individual videos, which neglects the correlations\namong different videos [6], [7], [8], [9]. To resolve this issue,\nvisual concepts across similar videos are leveraged to co-\nsummarize the videos and improve the overall summariza-\ntion performance [14]. However, the method proposed in\n[14] 1) merely ﬁnds shots that co-occur most frequently\nacross videos to generate the summaries and 2) considers\nonly visual similarity between two shots but neglects the\nimportant high-level information in shots. To address these\narXiv:2112.13478v2  [cs.CV]  29 Jun 2022\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 2\nlimitations and further exploit the correlations among differ-\nent videos, in this paper we propose video joint modelling\nfor co-summarization, where complete cross-video high-\nlevel patterns are explicitly modelled and learned for the\nsummarization of individual videos. Speciﬁcally, similar\nvideos are encoded simultaneously and the semantic in-\nformation is aggregated across videos in our framework.\nAn example of co-summarization by the proposed video\njoint modelling is shown in Fig. 1. It is worth noting that\nthe proposed video co-summarization method is not a new\nsetting of video summarization but a new framework to\naddress the classic video summarization task, and we per-\nform video joint modelling only during the training of the\nsummarization model. Speciﬁcally, during training, several\nsemantically similar videos in the training set are encoded\nsimultaneously, and the information is aggregated across\nvideos. By applying supervision on all input videos, the\nsummarization model can learn the pattern of important\ncontents from multiple sources. After the model is trained,\nplain video summarization is performed on each video in\nthe testing set individually without video joint modeling.\nIn this case, the proposed method is comparable to existing\nvideo summarization methods.\nIntuitively, video joint modelling can be performed in the\nframe level, where similar videos are combined into a whole\nsequence and the frame-level dependencies are captured by\nthe recurrent neural network (RNN) or Transformer [15].\nTherefore, video joint modelling should not be performed\nin the frame level plainly, and more effective network archi-\ntectures are required. In this case, we develop a hierarchical\nTransformer for video joint modelling. The reasons why we\nuse Transformer instead of RNNs are: 1) RNNs struggle\nto capture the long-range dependencies in long sequences\n[6], which greatly limits their representation ability; 2) The\nencoding of the current step relies on the output of the\nprevious step in RNNs, which signiﬁcantly increases the\ntime consumption in both training and testing. In contrast,\nTransformer captures the global dependencies in sequences\nby the multi-head attention mechanism, which encodes all\ntime steps in parallel [15]. Speciﬁcally, the ﬁrst layer of the\nhierarchical Transformer is used to extract semantic repre-\nsentation from individual shots of similar videos, while the\nsecond layer performs shot-level video joint modelling to\naggregate cross-video semantic information. By this means,\neach encoded shot representation contains cross-video se-\nmantic information. Since our motivation is to construct a\ntask-speciﬁc architecture for video joint modelling, we do\nnot modify the inner structure of the standard Transformer\nand we use it as the basic building block. Besides, to retain\nthe video-speciﬁc information, a special token is designed\nfor each video in our framework, whose output is regarded\nas the video representation. Finally, the encoded shot rep-\nresentation (containing cross-video information) and the\nvideo representation (containing video-speciﬁc information)\nare combined to predict the shot-wise importance score.\nIn summary, we propose Video Joint Modelling\nbased on Hierarchical Transformer ( VJMHT) for co-\nsummarization, which takes into consideration the semantic\ndependencies across videos and the internal hierarchical\nstructure of videos to obtain high-level video representa-\ntions. Speciﬁcally, VJMHT consists of two layers of Trans-\nformer to capture the intra-shot (frame-level) and cross-\nvideo inter-shot (shot-level) dependencies hierarchically, by\nwhich each encoded shot contains the semantic information\nacross videos. Additionally, based on the video representa-\ntion extracted by the hierarchical Transformer, we present a\nreconstruction loss between the representation of the video\nand that of its summary to maximize their similarity.\nOur contributions are summarized as follows:\n• Video joint modelling for video co-summarization\nis proposed, where semantically similar videos are\nencoded simultaneously and the information is ag-\ngregated across videos.\n• A hierarchical Transformer is proposed for video\njoint modelling. By encoding the frames in each shot\nand the shots from all videos hierarchically, each\nshot representation contains high-level cross-video\ninformation.\n• Transformer-based video representation reconstruc-\ntion is introduced to maximize the high-level simi-\nlarity between each video and its summary.\nThe rest of this paper is organized as follows. We review\nsome related works on video summarization, inter-video\ncommunication, and Transformer in Section 2. Before intro-\nducing our method formally, we explain several prelimi-\nnaries in Section 3, including the problem deﬁnition and a\nrevisit of the standard Transformer. We then propose a novel\nmethod, i.e., video joint modelling based on hierarchical\nTransformer for co-summarization, in Section 4. In Section 5,\nwe conduct extensive experiments (including comparisons\nwith existing methods, ablation studies and sensitivity anal-\nysis) to prove the effectiveness of the proposed method.\nFinally, we provide conclusions in Section 6.\n2 R ELATED WORK\nIn this section, we ﬁrst review some related work on video\nsummarization, including the unsupervised methods and\nsupervised ones. Then, literature on inter-video communi-\ncation in various ﬁelds is summarized. Finally, since our\nmodel is constructed based on Transformer, we then give a\nbrief introduction of the development and the applications\nof Transformer in computer vision (CV).\n2.1 Video Summarization\nVarious video summarization methods have been devel-\noped in previous works, which can be roughly classiﬁed\ninto two categories: unsupervised methods and supervised\nmethods. Each category of methods is reviewed as follows.\n2.1.1 Unsupervised Video Summarization\nThe unsupervised methods focus on manually designed\ncriteria such as the representativeness of the summary with\nrespect to the original video [16], [17] and the diversity\nof the frames/shots in the summary [5]. Conventional un-\nsupervised methods are based on classic machine learn-\ning techniques such as clustering [16], [17] and dictionary\nlearning [18]. VSUMM [16] extracts color features from\nvideo frames and uses k-means to group the frames into\nclusters. The keyframes are determined based on the cluster\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 3\ncenters. A uniﬁed framework is proposed in [17], which\nemploys the normalized cut algorithm to segment a video\ninto clusters. A temporal graph is constructed based on the\nclusters to inherently describe the perceptual importance of\nvideo segments. Video summarization is reformulated as an\nL2,0-constrained sparse dictionary selection problem in [18],\nwhose approximate solution is obtained by a simultaneous\northogonal matching pursuit (SOMP) algorithm. Recently,\ndeep-learning-based unsupervised methods for video sum-\nmarization are developed [4], [5]. Adversarial learning is\nleveraged in [4], where the discriminator is forced to clas-\nsify the summary feature sequences from the video feature\nsequences. DR-DSN [5] models the video summarization\ntask as a sequential decision-making process. The summary\ngenerator predicts the probability of a frame being selected,\nwhich is optimized by reinforcement learning in an end-to-\nend fashion. Different from previous unsupervised meth-\nods, we use a single Transformer-based model to extract\nsemantic representations from videos and summaries. The\ndistance between the representations of generated sum-\nmaries and their corresponding videos is used as the loss to\ntrain the model. Our method has the following advantages\nwith respect to previous ones: a) The proposed framework\nconsists of only one model (hierarchical Transformer) to\ngenerate summaries and perform video representation re-\nconstruction; b) The optimization of our model is much\nmore efﬁcient since no adversarial learning or reinforcement\nlearning is involved.\n2.1.2 Supervised Video Summarization\nRNN-based models dominate the supervised methods for\nvideo summarization in recent years. A number of vari-\nants of RNN are proposed to encode and summarize the\nvideo from various aspects. dppLSTM [3] uses bi-directional\nLSTM to model the temporal dependencies in the video.\nConsidering the hierarchical structure of videos, hierarchi-\ncal LSTM is developed in [6], which uses two layers of\nLSTM to model the intra-shot dependencies and inter-shot\ndependencies. Furthermore, structure-adaptive hierarchical\nLSTM is adopted in [8], where shot boundary detection\nis performed as an extra task. Instead of using LSTM\nto model videos, SUM-FCN [19] captures the long-range\ndependencies within a video by a one-dimension fully\nconvolutional network (FCN). By a stack of convolutions\nand pooling layers, the effective context size grows as it\ngoes deeper in the network. Considering the deﬁciencies\nof the recurrent models, VASNet [9] introduces the atten-\ntion mechanism to capture the global dependencies among\nvideo frames. Video summarization is reformulated as a\nsequence-to-sequence task in [20], where an LSTM-based\nencoder-decoder network with an intermediate attention\nlayer is leveraged. Besides considering the temporal depen-\ndencies, a three-dimension convolutional neural network\n(3D-CNN) is used to extract spatial-temporal features of\nvideos in [7], and the temporal dependencies are further\nmodelled by the recurrent network. A novel loss function\ncalled Sobolev loss is proposed in [7] to capture the lo-\ncal dependencies. Spatiotemporal features are extracted to\ngenerate the inter-frames motion curve in [21], which is\nused for video segmentation. Then a self-attention model\nis employed to select keyframes inside the shots. SumGraph\n[22] is a graph-based video summarization method, where\nthe global dependencies of frames are modeled by recursive\ngraph modeling networks. Despite that they achieve great\nsuccess in video summarization, previous methods have\nthe limitation of considering only the correlations among\nframe/shots within individual videos. To address this issue,\nwe propose video joint modelling to capture the correlations\namong semantically similar videos for video summariza-\ntion. To perform video joint modelling, a novel hierarchical\nTransformer is proposed to aggregate shot-level information\nacross videos. Different from previous supervised methods,\nthe proposed model not only considers the inner structure\nvideos (frame-shot-video), but also captures the global de-\npendencies within individual videos and across videos.\n2.2 Inter-video Communication\nInter-video communication has been exploited in various\ncomputer vision ﬁelds such as action localization [23],\n[24], video object detection [25], [26], and video person re-\nidentiﬁcation [27]. Inter-Video Proposal Relation module\nis proposed in [25] to learn robust object representations\nby capturing the correlations of hard samples. Self-and-\nCollaborative Attention Network (SCAN) [27] takes a pair\nof videos as input and measures their similarity to perform\nperson re-identiﬁcation. In SCAN, the attention mechanism\nis leveraged to reﬁne the representations of videos by con-\nsidering both the intra-sequence and inter-sequence corre-\nlations. In [26], correspondence estimation is achieved by\nsimultaneously modelling both the intra-video and inter-\nvideo representation associations for self-supervised learn-\ning. The learned representations are successfully applied to\nvideo object segmentation and tracking. I2Net [23] exploits\nboth the intra-video and inter-video attention to address\nthe issues of lacking long-term relationships and action\npattern uncertainty in temporal action localization. Cross-\nvideo relationships are mined for weakly supervised tempo-\nral action localization in [24], where cross-video modelling\nis performed in the sub-action granularity. Different from\nprevious works, we proposed shot-level cross-video infor-\nmation aggregation which not only considers the internal\nstructure of videos, but also decreases the computational\ncomplexity when dealing with long videos.\n2.3 Transformer in CV\nTransformer has revealed great power in computer vision\ntasks such as object detection and action recognition. Object\ndetection is modelled as a sequential-to-sequential task and\naddressed using Transformer in [28] and [29]. They do not\nneed many hand-designed components like anchor gener-\nation and non-maximum suppression. Transformer is also\nexploited for the recognition of actions of individuals or\ngroups in [30] and [31] by spatial-temporal context aggre-\ngation. In this paper, we propose hierarchical Transformer\nto model the dependencies of frames within each shot and\nthose of shots among different videos successively for video\nco-summarization.\n3 P RELIMINARIES\nIn this section, we ﬁrst introduce the task of video sum-\nmarization, including the purpose and the formulation. We\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 4\nthen revisit the standard Transformer since our model is\nconstructed based on it.\n3.1 Video Summarization\nVideo summarization aims to automatically generate a short\nversion of a video, which contains the important people,\nobjects, and events. By skimming through only the sum-\nmary, a viewer can infer the main content of the original\nvideo. In most previous works [3], [19], [32] and our paper,\nvideo summarization is deﬁned as a frame-wise binary\nclassiﬁcation task. Fully consistent with previous works, the\naim and process of video summarization are described in\nan abstract and mathematical fashion as follows. Formally,\ngiven a video sequence {Xj}M\nj=1 (M is the number of\nframes and Xj ∈RH×W×3 is the j-th frame), video sum-\nmarization ( VS) maps the sequence to a binary sequence\n{yj}M\nj=1 (yj ∈ {0,1}) whose elements indicate whether\nframes are selected into the summary, i.e.,\nVS\n(\n{Xj}M\nj=1\n)\n= {yj}M\nj=1, (1)\nwhere yj = 1 means the j-th frame is selected into the\nsummary, while j = 0 means the opposite. In this case, the\nvideo summary is formed as\nS = {Xj|yj = 1,j = 1,2,··· ,M}. (2)\nNote that, in most video summarization methods [3], [8],\n[13], [20], there is a limit to the length of the video summary,\ni.e., |S|≤ γM, where |S|is the number of frames in S, and\nγ is a pre-deﬁned ratio. Such constraint requires the video\nsummarization methods generate summaries which contain\nmore important frames and less redundancy.\n3.2 Standard Transformer Revisit\nIn this section, we revisit the standard Transformer, which\nis the crucial component in our model. Transformer is a\nsequence-to-sequence model consisting of an encoder and\na decoder, each of which is elaborated as follows.\nThe Transformer encoder takes as input a sequence of\nfeature vectors and outputs a sequence of encoded features.\nThe output is usually referred to as the memory of the\nTransformer encoder [15], [28]. Speciﬁcally, the encoder is\ncomposed of a stack of identical layers. Each layer has two\nsub-layers: a multi-head self-attention mechanism and a\nposition-wise fully connected feed-forward network. Each\nsub-layer is equipped with a residual connection [33] and\nlayer normalization [34]. The structure of the ﬁrst layer of\nTransformer encoder is shown in Fig. 2. Formally, given an\ninput sequence X ∈RN×d, where N is the number of time-\nsteps of the sequence and d is the feature dimension, the\noutputs of the two sub-layers (denoted as X′∈RN×d and\nX′′∈RN×d) can be expressed as:\nX′= LayerNorm(MultiHead(X,X,X) + X), (3)\nX′′= LayerNorm(FFN(X′) + X′), (4)\nwhere MultiHead(·,·,·) is the multi-head attention mecha-\nnism, LayerNorm(·) is the layer normalization, and FFN(·)\nis the position-wise fully connected feed-forward network.\nThe multi-head attention mechanism is a crucial compo-\nnent in Transformer, which takes as input a query matrix Q,\nMulti-Head Self-Attention\nAdd & Normalize\nFeed Forward Feed Forward\nLayerNorm\nPositional\nEncoding\n( )\nPositional\nEncoding\nFig. 2. The structure of the ﬁrst layer of Transformer encoder. For\nsimplicity, only two tokens are shown. The positional encodings are\nadded to the input sequence at the beginning. In each layer, the self-\nattention mechanism is applied to the sequence, followed by a resid-\nual connection and layer normalization. Then, a positional-wise feed-\nforward network is used to transform each feature into a hidden space.\nAt last, a residual connection and layer normalization are equipped.\na key matrix K and a value matrix V (X = Q = K = V\nin the multi-head self-attention), and captures global de-\npendencies using scaled dot-product attention in different\nsubspaces. Formally,\nMultiHead(Q,K,V ) = Concat (H1,··· ,Hh) Wo, (5)\nwhere\nHi = softmax\n(\nQWq\ni\n(\nKW k\ni\n)T\n√dh\n)\nV Wv\ni , (6)\nwhere his the number of heads. dh = d/h. Wq\ni , Wk\ni ,Wv\ni ∈\nRd×dh and Wo ∈ Rd×d are parameters. By transforming\nthe concatenated attention outputs from all subspaces, the\nmodel attends to information from different aspects.\nThe position-wise fully connected feed-forward network\nconsists of two fully connected layers with a ReLU acti-\nvation in between, which transforms the feature in each\nposition separately and identically, i.e.,\nFFN(X′) = max {0,X′W1 + b1}W2 + b2, (7)\nwhere W1 ∈ Rd×d′\n,W2 ∈ Rd′×d,b1 ∈ Rd′\n,b2 ∈ Rd are\nlearnable parameters. Note that the positional encodings\n(learned or ﬁxed) are ﬁrst added to the input features of\nthe ﬁrst layer to make use of the order information.\nIn terms of the Transformer decoder, it is similar to the\nencoder except that in each layer, an extra multi-head cross-\nattention mechanism is positioned after the multi-head self-\nattention mechanism. In our model, only the Transformer\nencoder is used to encode the videos in a hierarchical\nfashion, while the decoder is discarded. For the prediction\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 5\nF-Transformer\nVideo Video \nF-Transformer\nS-Transformer\nF-Transformer\n F-Transformer\nReconstruction Loss\nS-Transformer\nDirect Input\nLinear ProjectionFrame \nFeature\nFig. 3. The overview of video joint modelling based on hierarchical Transformer (VJMHT) for co-summarization. Without loss of generality, only\ntwo videos are shown, and we assume each video consists of two shots and each shot consists of two frames. Two semantically similar videos\nare sent into VJMHT simultaneously. The frames in each shot are encoded by F-Transformer in parallel to obtain the shot embedding. The shots\nare aggregated across videos in S-Transformer. The encoded shot embeddings along with the video representation are combined to predict the\nshot-wise importance scores. In addition, video representation reconstruction is performed to minimize the distance between the representation of\nthe summary and the video. Transformers/linear projections in the same level have the same structure and share parameters.\nof importance of video shots, a simple linear projection is\napplied after the encoder. In this case, Transformer refers to\nthe Transformer encoder throughout this paper.\n4 T HE PROPOSED METHOD\nIn this section, we elaborate Video Joint Modelling based on\nHierarchical Transformer (VJMHT) for co-summarization.\nThe overview of VJMHT is illustrated in Fig. 3. Specif-\nically, the structure of the proposed framework is ﬁrst\nexplained, including intra-shot dependency modelling and\ncross-video inter-shot dependency modelling. We then per-\nform Transformer-based video representation reconstruc-\ntion to maximize the similarity between the summary and\nthe video. Finally, we explain the optimization of VJMHT\nand the details of inference.\n4.1 Video Joint Modelling for Co-summarization\nConventional video summarization methods generate sum-\nmaries of videos separately, without considering the con-\nnections between videos. However, different videos of the\nsimilar topic may contain the same important information\nwhich can be leveraged to improve the the summary of\neach video [14]. In this paper, based on the hierarchical\nstructure, video joint modelling is developed for video co-\nsummarization. The proposed framework consists of two\nlayers of Transformer: F-Transformer for intra-shot (frame-\nlevel) dependencies capturing and S-Transformer for cross-\nvideo inter-shot (shot-level) dependencies capturing, each\nof which is explained as follows.\n4.1.1 Intra-shot Dependency Modelling\nSince the intra-shot dependency modelling for each video\nis independent and identical, we explain this process using\nonly one video. Given a video sequence V = {fj}M\nj=1 (M\nis the number of frames and fj ∈Rdf is the feature of the\nj-th frame), the shot boundary detection algorithm, Kernel-\nbased Temporal Segmentation (KTS) [35], is ﬁrst applied to\nobtain the shot boundaries {bi}P\ni=0, where P is the number\nof shots, and b0 = 1,bP = M represent the indexes of the\nﬁrst frame and the last one. In this case, the i-th shot Vi is a\nsub-sequence of V, i.e., Vi =\n{\nfbi−1+1,fbi−1+2,··· ,fbi\n}\n⊆\nV. For the sake of clarity, j and i are used as the index of\nframe and that of shot, respectively, throughout this paper.\nSince the intra-shot dependency modelling aggregates infor-\nmation within each shot Vi, we explain this process using\nonly one shot as follows.\nFor each shot in a video, inspired by the [CLS] token\nin BERT [36], a learnable shot embedding s ∈ Rdf is\nrandomly initialized and prepended to the frames in the\nshot, whose state at the output of F-Transformer serves\nas the shot representation. Fixed sinusoidal positional en-\ncodings [15] are added to the frame features to retain the\norder information. Formally, given a shot starting at the t-th\nframe and consisting of L frames {fj}t+L−1\nj=t , the input of\nF-Transformer is\nF = [s; ft; ft+1; ··· ; ft+L−1] + Ef\npos ∈R(1+L)×df ,\nwhere Ef\npos is the positional encoding matrix for frames.\nAfter being encoded by F-Transformer, the dependencies\nwithin the shot are retained in the memory of F-Transformer.\nAs stated above, the ﬁrst embedding in the memory is\nregarded as the representation of the shot. Assuming that\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 6\na video consists of P shots, by encoding P shots in parallel,\na sequence of shot representations {si}P\ni=1 (si ∈ Rds) is\nobtained, where the dimension transformation is performed\nby linear projection ( Rdf → Rds). {si}P\ni=1 is sent into S-\nTransformer for shot-level modeling.\nNote that in F-Transformer, all the videos are encoded\nin parallel, which means the information is not aggregated\nacross videos. Frame-level aggregation is not performed for\ntwo reasons: 1) The quadratic time and memory complexity\nwith respect to sequence length seriously restricts Trans-\nformer from processing all frames in all videos as a whole\nsequence [37]; 2) There exists a great deal of redundancy\nin frame-level modelling since adjacent frames are visually\nand semantically similar; 3) Our purpose is to capture the\nshot-level semantic similarity among videos, while individ-\nual frames are not capable of representing such complex\nconcepts. In this case, video information aggregation is only\nperformed in S-Transformer as explained in the next part.\n4.1.2 Cross-video Inter-shot Dependency Modelling\nDifferent to the intra-shot dependency modelling that is\nindependent and identical for each video, the inter-shot\nmodelling involves cross-video information aggregation.\nGiven N semantically similar videos in the training set (the\nsimilarity between videos is explained in Section 5.1), they\nare encoded by F-Transformer independently as explained\nabove, and the shot representations of the N videos are de-\nnoted as\n{\ns1\ni\n}P1\ni=1 ,\n{\ns2\ni\n}P2\ni=1 ,··· ,\n{\nsN\ni\n}PN\ni=1, where {Pn}N\nn=1\nare the shot numbers of N videos. Superscripts are used as\nthe video indexes throughout this paper.\nSince\n{\ns1\ni\n}P1\ni=1 ,\n{\ns2\ni\n}P2\ni=1 ,··· ,\n{\nsN\ni\n}PN\ni=1 represent the se-\nmantic information of shots inNvideos, they are sent into S-\nTransformer simultaneously to perform information aggre-\ngation across videos. Formally, the input of S-Transformer is\nexpressed as\nS =\n[\nv; s1\n1; ··· ; s1\nP1 ; v; s2\n1; ··· ; s2\nP2 ; v; sN\n1 ; ··· ; sN\nPN\n]\n+Es\npos.\nNote that an identical learnable video embedding v ∈Rds\nis prepended to the shot embeddings of each video. In\naddition, a mask is applied to restrict each v from at-\ntending to shots of other videos. Hence, the video embed-\ndings can only aggregate information from shots of their\ncorresponding videos, while the dependencies among all\nshot embeddings are captured. An extra linear projection\n(Rds → Rdv ) is applied to the output of S-Transformer\nto obtain the the co-representation of the N videos. The\ntransformed representation is denoted as\nR =\n[\nr1; r1\n1; ··· ; r1\nP1 ; r2; r2\n1; ··· ; r2\nP2 ; rN ; rN\n1 ; ··· ; rN\nPN\n]\n,\nwhere {rn}N\nn=1 are the representations of N videos, con-\ntaining the video-speciﬁc information, while {rn\ni }Pn\ni=1 are the\nrepresentations of the shots in the n-th video, containing the\ncross-video information. {rn}N\nn=1 and {rn\ni }Pn\ni=1 are used for\nvideo co-summarization as shown in Fig. 3.\nTo predict the importance scores, each encoded shot\nrepresentation rn\ni is concatenated with the encoded repre-\nsentation of its corresponding video rn. In this way, the\nconcatenated representation contains not only the informa-\ntion across shots from N videos, but also the speciﬁc content\nof the n-th video. Finally, the concatenated representation is\nused to predict the score of the i-th shot in the n-th video by\nlinear projection, i.e.,\npn\ni = Concat(rn\ni ,rn)Wp + bp, (8)\nwhere Wp ∈ R2dv ,bp ∈ R are the learnable projection\nparameters. For frame-wise importance score, each frame\nis assigned with the score of the shot it belongs to. The score\nthe j-th frame in the n-th video is denoted as ¯pn\nj .\n4.2 Transformer-based Video Representation Recon-\nstruction\nA satisfactory video summary is supposed to contain the\nmain content of the original video. This property is referred\nto as reconstruction capacity, which is exploited to enhance\nthe performance of video summarization [4], [5]. However,\nmost of the existing methods perform reconstruction using\nthe RNN-based representations, which is limited since: 1)\nRNNs struggle to capture the long-term dependencies in\nvideos; 2) the structural information of videos is not re-\ntained. To address these issues, we propose Transformer-\nbased video representation reconstruction in this work.\nSince the video representations extracted by the proposed\nhierarchical Transformer encode the main content and the\nstructure of the videos, it is utilized for the high-level video\nreconstruction in our method.\nMore speciﬁcally, by co-summarization, the score of the\ni-th shot in the n-th video pn\ni is obtained, which is regarded\nas the weight of the shot to form the summary. In this case,\nthe shot embeddings of the summary are computed as the\nweighted shot embeddings of the original video, i.e.,\n{\np1\ni s1\ni\n}P1\ni=1 ,\n{\np2\ni s2\ni\n}P2\ni=1 ,··· ,\n{\npN\ni sN\ni\n}PN\ni=1\n,\neach of which is sent into S-Transformer independently.\nNote that, the video joint modelling is not performed when\nencoding the shots of summaries. Following the process\nin Section 4.1, the representations of N summaries are\nobtained by the S-Transformer and denoted as {rn\nsum}N\nn=1\nas shown in Fig. 3. Since rn\nsum encodes the content of the n-\nth summary, it is supposed to be close to rn which encodes\nthe content of the n-th video. In this case, a Transformer-\nbased video representation reconstruction loss is proposed,\nLrec = 1\ndvN\nN∑\nn=1\n∥rn\nsum −rn∥2\n2, (9)\nwhere dv is the dimension of the video representations.\n4.3 Optimization and Inference\nThe proposed architecture is optimized in an end-to-end\nfashion. Speciﬁcally, the loss function Lis the linear com-\nbination of three terms: the supervision loss Lsup, the re-\nconstruction loss Lrec (as proposed in Section 4.2) and the\nregularization Lreg, i.e.,\nL= Lsup + αLrec + βLreg, (10)\nwhere α,β are hyper-parameters to balance the three terms.\nFor each video, the supervision loss is the mean squared\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 7\nTABLE 1\nA summary of the datasets used in the experiments.\nDataset # of Videos Duration (min) Genres/Topics Annotations Usage\nSumMe [38] 25 1–6 Holidays, cooking and sports 15–18 sets of key-shots Training&Testing\nTVSum [39] 50 2–10 Beekeeping, parade and dog show 20 set shot-level scores Training&Testing\nYouTube [16] 50 1–10 Sports, news and TV-shows 5 sets of key-frames Training\nOVP [16] 39 1–4 Documentaries 5 sets of key-frames Training\nerror (MSE) between the predicted frame-wise importance\nscores and the ground truth ones, i.e.,\nLsup = 1\nM\nM∑\nj=1\n(¯pj −gj)2, (11)\nwhere M is the number of frames, and {¯pj}M\nj=1 ,{gj}M\nj=1\nare the predicted frame importance scores and the ground\ntruth ones, respectively. Following [4], [5], we introduce the\nregularization to prevent the model from predicting high\nscores for all frames indiscriminately. Speciﬁcally, for each\nvideo, the regularization is computed as follows,\nLreg =\n\n 1\nM\nM∑\nj=1\n¯pj −ε\n\n\n2\n, (12)\nwhere εis a hyper-parameter controlling the percentage of\nframes to be selected into the summary. The proposed model\ncan also be trained in an unsupervised manner, where Lsup\nin Eq. (10) is discarded. We also evaluate the unsupervised\nVJMHT in the experiments.\nTo generate a summary, shots are selected to maximize\nthe total score while ensuring the length of the summary\nis less than a pre-deﬁned limit, which is set to 15% of\nthe original video as in [3], [4], [5]. The maximization is\nmodelled as the 0/1 Knapsack problem [39], which is solved\nby dynamic programming.\nNote that the proposed video joint modelling is per-\nformed only during training, and the testing process is\nthe same as plain video summarization. Speciﬁcally, during\ntraining, several semantically similar videos in the train-\ning set are encoded simultaneously, and the information\nis aggregated across videos. By applying supervision on\nall input videos, the summarization model can learn the\npattern of important contents from multiple sources. Af-\nter the model is trained, plain video summarization is\nperformed on each video in the testing set individually\nwithout video joint modeling. The reasons why we do\nnot perform video co-summarization during testing are: 1)\nFinding semantically similar videos for the testing videos\nis not practical, as no prior information about the testing\nvideos is available in practical applications; 2) Video co-\nsummarization requires much more computational resource\ncompared with plain video summarization, so the efﬁciency\nof testing cannot be guaranteed if video joint modelling\nis applied; 3) As the topic prior of videos is used in co-\nsummarization, we perform plain video summarization in-\nstead of co-summarization to ensure fair comparisons with\nother methods. 4) The trained model is capable of modelling\nthe complete high-level patterns of important contents, so it\nis also effective when being applied to single-video summa-\nrization. Additionally, compared with traditional training,\nthe training with video joint modelling is supervised by\nmultiple correlated samples. Such training manner renders\nthe model robust to complicated videos and capable of ex-\ntracting powerful features for video summarization during\ntesting.\n5 E XPERIMENTS\nIn this section, we conduct extensive experiments to veriry\nthe effectiveness of the proposed methods. First, we elabo-\nrate the experiment settings, including the datasets, the im-\nplementation details and the evaluation metrics. To demon-\nstrate the superiority of the proposed VJMHT, we compare\nit with existing methods in terms of F-measure and rank\ncorrelation coefﬁcients. We then conduct ablation studies to\nillustrate the impact of crucial components in our model. Be-\nsides, we visualize the summary generated by our methods\nas well as the importance score output by VJMHT. Finally,\nwe conduct sensitivity analysis on the structure parameters\nof the proposed model.\n5.1 Experiment Settings\n5.1.1 Datasets\nTwo public benchmarks for video summarization are used\nto evaluate the proposed method: SumMe [38] and TVSum\n[39]. SumMe consists of 25 videos with a coverage of various\nevents such as holidays and cooking. The length of the\nvideos in SumMe varies from 1.5 min to 6.5 min. Each\nvideo in SumMe was annotated by 15–18 persons with key\nshots as summaries. TVSum has 50 videos of 10 categories\n(e.g., Beekeeping and Parade) from TRECVid Multimedia\nEvent Detection (MED) [40]. The length of the videos in\nTVSum varies from 2 min to 10 min. Similarly, each video\nwas annotated by 20 persons with frame-wise importance\nscores. Additionally, YouTube [16] and OVP 2 are used for\ntraining. The videos in YouTube cover a variety of topics\nsuch as sports and news, while the videos in OVP are mostly\ndocumentaries. YouTube has 39 videos (cartoons are ex-\ncluded) and OVP has 50 videos, which were annotated with\nkeyframe-based summaries. The keyframes are converted to\nimportance scores for training as in [3]. A summary of the\ndatasets used in the experiments is shown in TABLE 1.\nFollowing previous works on video summarization [13],\n[19], [32], [41], we compare our methods with the state of the\narts in three settings: the canonical setting, the augmented\nsetting, and the transfer setting. Speciﬁcally, the canonical\nsetting is the standard supervised learning setting where the\ntraining set and the testing set are subsets of one dataset.\nConsidering that SumMe and TVSum are relatively small\n2. Open video project: https://open-video.org\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 8\nfor training, the augmented setting is adopted to augment\nthe training samples. In this setting, the videos in the\nYouTube dataset and the OVP dataset are added into the\ntraining set, and the testing set remains the same. As for the\ntransfer setting, it is exploited to measure the transfer ability\nof the summarization model, where the model is trained on\nSumMe (TVSum) and tested on TVSum (SumMe). Note that\nthe training samples are also augmented with the videos in\nYouTube and OVP .\n5.1.2 Implementation Details\nPre-processing and Feature Extraction.Following [3], each\nvideo is sub-sampled to 2 frames per second to remove\nredundancy. The output of the penultimate layer (pool 5) of\nthe GoogLeNet [42] pre-trained on ImageNet [43] is adopted\nas the frame feature (df = 1024).\nNetwork Structure.Since F-Transformer and S-Transformer\nencode frames and shots respectively, they have different\nstructures. Speciﬁcally, in F-Transformer, 2 identical layers\nare stacked and the dimension of the feed-forward network\nis set to 4,096 in all layers. The dimensions of the shot\nembedding and video embedding are set to 512 ( ds = dv =\n512). In S-Transformer, 3 identical layers are stacked and the\ndimension of the feed-forward network is set to 2,048 in all\nlayers. The number of heads in the multi-head attention is\nset to 2 in both F-Transformer and S-Transformer. Dropout is\ndiscarded in VJMHT. Hyper-parameter sensitivity analysis\nis conducted in Section 5.6.\nVideo Joint Modelling. Two semantically similar videos\nare sent into VJMHT simultaneously considering the mem-\nory limitation ( N = 2 ), and the video reconstruction is\nperformed on only one of them. The details of the clus-\ntering process, training, and testing are explained as fol-\nlows. Firstly, we pretrain the hierarchical Transformer on\nYouTube and OVP . Then, we use the pretrained Transformer\nto extract semantic representations of the training videos in\nSumMe, TVSum, YouTube, and OVP . After obtaining the\nvideo representations, we utilize K-Means to group all the\ntraining videos into 25 clusters. By this means, the videos in\nthe training set of SumMe/TVSum are divided into several\ngroups based on the above clusters. Taking the training\non TVSum in the canonical setting for example, several\nvideos are sampled from the same group and sent into the\nproposed framework for summarization. After the model\nis trained, the videos in the testing set of TVSum are sent\ninto the model individually for regular summarization. The\nsame goes for SumMe and other settings. In this case, no\nadditional prior information of videos is used in the testing\nphase. Therefore, all the comparisons with other methods in\nour experiments are fair.\nTraining. α,β in Eq. (10) are set to 0.01 and 0.1, respectively.\nε in the regularization is set to 0.5. VJMHT is optimized\nby Adam [44] for 60 epochs with batch size 1 and initial\nlearning rate 10−5, which is reduced to 10−6 at the 30th\nepoch. Since SumMe and TVSum are relatively small, ﬁve-\nfold cross-validation is performed to make a fair comparison\nas in existing methods [5], [41], [45]. All experiments are\nconducted using PyTorch on NVIDIA Tesla P100 GPUs.\n5.1.3 Evaluation Metrics\nIn this paper, the summaries are evaluated by two kinds\nof metrics: F-measure and rank correlation coefﬁcients\n(Kendall’sτ and Spearman’sρ).\nF-measure. F-measure is widely used in previous works\nto compare video summarization methods. Given a video\nsummary Vs and its corresponding ground truth summary\nVgt, the precision P and recall Rare computed as follows,\nP = |Vs ∩Vgt|\n|Vs| ,R = |Vs ∩Vgt|\n|Vgt| , (13)\nwhere |·| denotes the number of frames in the sequence.\nThe F-measure F is the harmonic average of P and R, i.e.,\nF = 2PR\nP + R. (14)\nA higher F-measure indicates the summary greatly overlaps\nwith the ground truth while containing less redundancy.\nRank-based Evaluation.It has been pointed out in [50] that\n1) the distribution of shot lengths has a great impact on\nF-measure; 2) randomly generated summaries sometimes\nachieve considerable F-measure. In this case, F-measure\nalone is not sufﬁcient enough to evaluate video summa-\nrization. In this paper, we also adopt the rank-based eval-\nuation [50] to measure the performance of different meth-\nods. Speciﬁcally, given the predicted frame-level importance\nscores and the ground truth ones, two rank correlation\ncoefﬁcients, Kendall’s τ and Spearman’sρ, are computed.\n5.2 Comparisons with Existing Methods\nWe compare the proposed VJMHT with several existing\nmethods using F-measure on SumMe and TVSum. Among\nthe compared methods, dppLSTM [3], H-RNN [6] and\nHSA-RNN [8] use bi-LSTM to model temporal dependen-\ncies, while our method exploits the Transformer to capture\nglobal dependencies; SUM-GAN [4] and ACGAN [45] per-\nform unsupervised video summarization in GAN, while\nour method maximizes the similarity between the video\nand its summary by Transformer-based video representa-\ntion reconstruction; SUM-FCN [19], GLRPE [49] and RSGN\n[13] capture the global dependencies by FCN, attention\nmechanism and graph network, respectively, while our\nmethod models temporal dependencies hierarchically. Since\nthe Transformer-based video representation reconstruction\nis introduced in our method, VJMHT can be trained in\nan unsupervised manner, where Lsup in Eq. (10) is dis-\ncarded. The unsupervised version of VJMHT is denoted\nas VJMHTuns. The results are shown in TABLE 2. Note\nthat the methods above VJMHT uns are unsupervised meth-\nods or weakly supervised ones, while the methods below\nVJMHTuns are supervised methods.\nAs shown in TABLE 2, VJMHT uns is comparable to the\nunsupervised methods and some supervised ones. Specif-\nically, VJMHT uns performs remarkably in the canonical\nsetting and the augmented setting on SumMe. Besides, it\nachieves the best performance in the augmented settings\non TVSum among unsupervised methods. Therefore, by\nTransformer-based video reconstruction, our model is capa-\nble of identifying the important shots without supervision.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 9\nTABLE 2\nThe results (F-measure) in different settings. The methods above VJMHTuns are unsupervised methods or weakly supervised ones, while the\nmethods below VJMHTuns are supervised methods. The best and the second-best results are in bold and underlined, respectively.\nMethods SumMe TVSum\nCanonical Augmented Transfer Canonical Augmented Transfer\nSUM-GAN [4] 0.387 0.417 — 0.508 0.589 —\nDR-DSN [5] 0.414 0.428 0.424 0.576 0.584 0.578\nSUM-FCNuns [19] 0.415 — — 0.527 — —\nPCDL [46] 0.427 — — 0.584 — —\nACGAN [45] 0.460 0.470 0.445 0.585 0.589 0.578\nUnpairedVSN [47] 0.475 — — 0.556 — —\nWS-HRL [48] 0.436 0.445 — 0.584 0.585 —\nRSGNuns [13] 0.423 0.436 0.412 0.580 0.591 0.597\nVJMHTuns (Ours) 0.471 0.490 0.436 0.573 0.597 0.571\ndppLSTM [3] 0.386 0.429 0.418 0.547 0.596 0.587\nSUM-GANsup [4] 0.417 0.436 — 0.563 0.612 —\nH-RNN [6] 0.421 0.438 — 0.579 0.619 —\nHSA-RNN [8] 0.423 0.421 — 0.587 0.598 —\nre-SEQ2SEQ [32] 0.425 0.449 — 0.603 0.639 —\nSUM-FCN [19] 0.475 0.511 0.441 0.568 0.592 0.582\nCSNetsup [41] 0.486 0.487 0.441 0.585 0.571 0.574\nACGANsup [45] 0.472 — — 0.594 — —\nGLRPE [49] 0.502 — — 0.591 — —\nRSGN [13] 0.450 0.457 0.440 0.601 0.611 0.600\nVJMHT (Ours) 0.506 0.517 0.464 0.609 0.619 0.589\nTABLE 3\nThe comparison results (rank correlation coefﬁcients) on SumMe and TVSum. The methods above VJMHTuns are unsupervised methods or\nweakly supervised ones, while the methods below VJMHTuns are supervised methods. The results of randomly generated and human-generated\nsummaries are reported as well. The best and the second-best results are in bold and underlined, respectively.\nMethods SumMe TVSum\nKendall’sτ Spearman’sρ Kendall’sτ Spearman’sρ\nRandom 0.000 0.000 0.000 0.000\nHuman 0.205 0.213 0.177 0.204\nSUM-GAN [4] — — 0.024 0.032\nWS-HRL [48] — — 0.078 0.116\nDR-DSN [5] 0.047 0.048 0.020 0.026\nRSGNuns [13] 0.071 0.073 0.048 0.052\nVJMHTuns (Ours) 0.061 0.063 0.070 0.075\ndppLSTM [3] — — 0.042 0.055\nCSNetsup [41] — — 0.025 0.034\nGLRPE [49] — — 0.070 0.091\nSumGraph [22] — — 0.094 0.138\nHSA-RNN [8] 0.064 0.066 0.082 0.088\nRSGN [13] 0.083 0.085 0.083 0.090\nVJMHT (Ours) 0.106 0.108 0.097 0.105\nIn terms of the supervised methods, VJMHT achieves the\nbest results in three settings on SumMe, and the canonical\nsetting on TVSum. Moreover, it outperforms most meth-\nods in other settings on TVSum. Although some methods\nachieve results close to (or better than) ours in certain set-\ntings, our method performs well CONSISTENTLY in three\nsettings on two datasets. For example, SUM-FCN achieves\ngood results as ours in the augmented setting on SumMe,\nbut its results in the canonical/transfer setting are inferior;\nre-SEQ2SEQ is better than VJMHT in the augmented setting\non TVSum, while VJMHT greatly outperforms it on SumMe.\nNone of other methods achieve as consistently good perfor-\nmance as ours. In summary, in terms of F-measure, VJMHT\nperforms remarkably on two datasets compared to the state-\nof-the-art methods.\n5.3 Rank-based Evaluation\nComplementary to F-measure, rank-based evaluation is uti-\nlized in recent works [41], [49]. We also adopt it to compare\nour method with existing ones, as well as the randomly\ngenerated and human-generated summaries. The compar-\nison results are shown in TABLE 3. Note that the results of\nthe human summary are computed using the leave-one-out\napproach as in [50].\nAs shown in TABLE 3, VJMHTuns outperforms most un-\nsupervised methods and some supervised methods, which\nmeans the Transformer-based video representation recon-\nstruction models the relative importance among shots rea-\nsonably. With the supervision of human annotations, the\nperformance of VJMHT is improved signiﬁcantly and sur-\npasses most methods on two datasets in terms of both\nKendall’s τ and Spearman’s ρ correlation coefﬁcients. We\nconclude that the predicted frame-wise importance scores\nare well consistent with human annotations.\n5.4 Ablation Study\nIn this section, we perform extensive ablation studies on\nthe proposed VJMHT. Their exist three crucial compo-\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 10\nTABLE 4\nThe results (F-measure) of ablation studies. CS, HT and REC are the abbreviations for video co-summarization, the hierarchical Transformer and\nTransformer-based reconstruction, respectively. The best and the second-best results are inbold and underlined, respectively.\nCS HT REC SumMe TVSum\nCanonical Augmented Transfer Canonical Augmented Transfer\nBaseline 0.485 0.496 0.445 0.595 0.608 0.561\n✓ 0.483 0.498 0.440 0.592 0.598 0.558\n✓ 0.494 0.501 0.454 0.601 0.614 0.562\n✓ 0.488 0.499 0.451 0.598 0.612 0.565\n✓ ✓ 0.501 0.503 0.457 0.607 0.615 0.571\n✓ ✓ 0.496 0.507 0.461 0.608 0.614 0.566\n✓ ✓ ✓ 0.506 0.517 0.464 0.609 0.619 0.583\nnents in VJMHT: video co-summarization by the video\njoint modelling ( CS), the hierarchical Transformer ( HT)\nand Transformer-based video representation reconstruction\n(REC). To verify the effectiveness of each of them, we con-\nstruct simpliﬁcations of VJMHT as in TABLE 4. The baseline\nmodel and the models without the hierarchical structure are\nconstructed by a plain Transformer to encode all the frames\nsimultaneously. The model with only video joint modelling\nperforms frame-level information aggregation across similar\nvideos. The model with only Transformer-based video rep-\nresentation reconstruction uses the encoded special token\nin the plain Transformer as the video representation. The\nmodel with all components is the proposed VJMHT.\nAs shown in TABLE 4, the baseline model performs\nconsiderably, which veriﬁes the effectiveness of Transformer\nfor video summarization. However, with the frame-level\nvideo joint modelling, the performance drops slightly. We\nspeculate the reason is that frame-level information aggrega-\ntion involves too much redundancy and the attention in the\nmodel is dispersed. By modeling videos hierarchically, the\nperformance in different settings on two datasets improves\nsigniﬁcantly, which means the hierarchical structure is more\nproper for videos than plain sequential models. With the\nframe-level video representation reconstruction, the perfor-\nmance is improved with respect to the baseline. With the\nshot-level video joint modelling, the model captures the\nsemantic correlations across videos, so the performance in\nthe canonical setting on the small-scale SumMe is raised\nobviously. With the hierarchical structure and Transformer-\nbased reconstruction, the results in the transfer setting on\ntwo datasets are improved remarkably. We believe the rea-\nson is that the reconstruction takes predominant effect when\nthere exists a huge domain gap between the training set and\nthe testing set. In conclusion, the proposed components are\nof great inportance for VJMHT.\n5.5 Visualization\nSome generated summaries as well as the predicted scores\nby VJMHT are illustrated in Fig. 4. As we can see from\nthe keyframes (sampled from the summaries generated by\nour method), the main content of the original videos is\ncontained in the summaries. By only skimming through\nthe summaries, a viewer can infer the main content in the\nvideo. In terms of the predicted scores (blue line), they ﬁt\nthe ground truth scores (orange line) well, which means the\nproposed method can capture the temporal dependencies\nand measure the importance of shots accurately.\nTABLE 5\nThe F-measure of different numbers of layers in the F-Transformer, as\nwell as the inference time on TVSum.\n#Layers SumMe TVSum Runtime (s)\n1 0.481 0.589 12.49\n2 0.494 0.601 14.65\n3 0.495 0.605 18.32\nTABLE 6\nThe F-measure of different numbers of layers in the S-Transformer, as\nwell as the inference time on TVSum.\n#Layers SumMe TVSum Runtime (s)\n1 0.472 0.593 13.67\n2 0.498 0.598 14.18\n3 0.494 0.601 14.65\n4 0.490 0.597 15.35\n5.6 Sensitivity Analysis\nIn this subsection, we conduct sensitivity analysis on the\nproposed method in the canonical setting. Speciﬁcally, we\nstudy the impact of the number of layers in Transformer, the\nnumber of heads in the multi-head attention mechanism,\nand the dimension of shot embeddings. To eliminate the\nimpact of co-summarization and reconstruction, the video\njoint modelling and Transformer-based video representa-\ntion reconstruction are discarded in the those experiments.\nBesides, we also conduct experiments to demonstrate the\nimpact of the number of videos and the sampling modes of\nvideos for joint modeling, the impact of Transformer-based\nframe-level modelling, and the quantitative performance of\nvideo joint modelling. All settings are the same as men-\ntioned in Section 5.1.2 unless speciﬁed otherwise.\n5.6.1 Impact of the Number of Layers in Transformer\nThe number of layers in Transformer are crucial to the\nperformance and efﬁciency. Generally, with the increase in\nthe number of layers, the representation ability of the model\nis improved, but it takes more time to train and test the\nmodel. Since our model consists of two Transformers (F-\nTransformer and S-Transformer), we study the impact of the\nnumber of layers in F-Transformer and S-Transformer.\nTABLE 5 shows the impact of the number of layers\nin the F-Transformer. As we can see from the results, the\none-layer F-Transformer achieves the worst results on two\ndatasets, which means single-layer structure is insufﬁcient\nto capture the dependencies between frames in each shot.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 11\ni3wAGJaaktw\ni3wAGJaaktw\n Hl-__g2gn_A\nHl-__g2gn_A\neQu1rNs0an0\neQu1rNs0an0\nxmEERLqJ2kU\nxmEERLqJ2kU\nFig. 4. The generated summaries and the predicted scores by VJMHT of four videos from TVSum. The keyframes in the ﬁrst row are sampled from\nthe summaries generated by our method. As for the curves in the second row, the blue lines depict the ground truth scores, while the orange lines\ndepict the predicted ones. The video names are on the upper right.\nTABLE 7\nThe F-measure of different numbers of heads in the F-Transformer.\n#Heads SumMe TVSum\n1 0.491 0.599\n2 0.494 0.601\n4 0.493 0.604\nWith more layers, the F-measure increases considerably.\nHowever, when more layers are equipped, the improvement\nis less signiﬁcant, and the efﬁciency decreases greatly. In this\ncase, we use the two-layer F-Transformer in our model.\nTABLE 6 shows the impact of the number of layers in\nthe S-Transformer. As we can see from the results, the S-\nTransformer with two layers achieves the best result on\nSumMe, while the S-Transformer with three layers achieves\nthe best result on TVSum. Besides, the performance drops\non both datasets when more layers are added, which im-\nplies over-ﬁtting. Moreover, the number of layers in the S-\nTransformer has moderate impact on the inference time. We\nbelieve the reason is that the numbers of shots in videos are\ntoo small to have great inﬂuence on the computational com-\nplexity of Transformer which is quadratic to the sequence\nlength. Therefore, we set the number of layers in the S-\nTransformer to 3 in VJMHT.\n5.6.2 Impact of the Number of Heads\nDifferent to the traditional attention mechanism, the multi-\nhead attention mechanism ﬁrst transforms the features into\nseveral subspaces, and then applies the attention mecha-\nnism in different subspaces, respectively. The ﬁnal output of\nthe multi-head attention mechanism the transformed con-\ncatenation of the attention outputs in different subspaces. By\nthis means, Transformer captures the global dependencies\nfrom different aspects. We conduct experiments to study the\nimpact of the number of heads in the F-Transformer and\nS-Transformer, respectively.\nTABLE 7 shows the impact of the number of heads\nin the F-Transformer. As we can see from the results, the\nnumber of heads in the F-Transformer has little impact on\nthe performance on two datasets. We speculate the reason\nTABLE 8\nThe F-measure of different numbers of heads in the S-Transformer.\n#Heads SumMe TVSum\n1 0.476 0.589\n2 0.494 0.601\n4 0.490 0.605\nTABLE 9\nThe F-measure of different dimensions of shot embeddings.\nDimension SumMe TVSum\n256 0.459 0.585\n512 0.494 0.601\n1,024 0.472 0.596\nis that the frames in a shot are semantically similar, so a\nsimple attention mechanism is powerful enough to capture\ntheir dependencies.\nTABLE 8 shows the impact of the number of heads\nin the S-Transformer. As we can see from the results, the\nS-Transformer with the single-head attention mechanism\nachieves the worst performance on two datasets, which\nmeans the correlations between shots are complicated and\nthe simple attention mechanism is not capable of modelling\nsuch correlations. With more heads, the performance is\nimproved signiﬁcantly. Speciﬁcally, the S-Transformer with\ntwo heads achieves the best result on SumMe, while the\nS-Transformer with three heads achieves the best result on\nTVSum. We speculate that the difference between the two\ndatasets in contents accounts for the inconsistency of the\nimpact of number of heads on performance. Taking into con-\nsideration both the F-Transformer and the S-Transformer,\nthe head number in the multi-head mechanism is set to 2.\n5.6.3 Impact of the Dimension of Shot Embeddings\nTABLE 9 shows the impact of the dimension of shot em-\nbeddings. As we can see from the results, the model with\nthe small dimension of shot embeddings achieves poor F-\nmeasure, which indicates the representation ability of the\nmodel is limited. When increasing the dimension to 512,\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 12\nTABLE 10\nThe results of different numbers of videos for joint modelling on TVSum.\n#Videos Kendall’s τ Spearman’sρ\n1 0.088 0.095\n2 0.097 0.105\n3 0.092 0.099\n4 0.085 0.091\nTABLE 11\nThe results of different the sampling mode on TVSum.\nMode Kendall’s τ Spearman’sρ\nw/o VJM 0.088 0.095\nRandom 0.085 0.903\nInter-Cluster 0.084 0.903\nIntra-Cluster 0.097 0.105\nthe performance reaches the best on two datasets. However,\nwith the large dimension of 1,024, the performance drops.\nWe assume over-ﬁtting occurs in this setting. In this case,\nwe choose to set the dimension of shot embeddings to 512\nthroughout the experiments.\n5.6.4 Impact of the Number of Videos for Joint Modelling\nTo demonstrate the impact of the number of videos for joint\nmodelling, we conduct experiments on TVSum, and the val-\nidation results are shown in TABLE 10. Note that the results\nof the model without video joint modelling are shown in\nthe ﬁrst row of the table, where the number of videos is 1.\nAs shown in the table, the rank correlation coefﬁcients ﬁrst\nincrease and then decrease, where the peak is reached at two\nvideos. We also ﬁnd that too more videos for joint modelling\nmay compromise the performance compared to the model\nwithout video joint modelling. One possible reason is that\nwith the increase in the number of videos, the number\nof shots for joint modelling increases signiﬁcantly and the\nattention for each shot is greatly diminished. Considering\nboth performance and memory use, we choose to use two\nvideos for joint modelling in this work.\n5.6.5 Impact of the Sampling Modes of Videos\nTo demonstrate the impact of the semantic relation between\nthe videos for joint modelling, we conduct experiments on\nTVSum, where 1) the videos are randomly selected from\nthe training set instead of the same cluster (Random), and\n2) the videos are randomly selected from different clus-\nters (Inter-Cluster). The results are shown in TABLE 11,\nincluding the model without video joint modelling (w/o\nVJM) as baseline. As shown in TABLE 11, the results of\nrandom selection and inter-cluster selection are similar to\nthose of the training without video joint modelling, which\nindicates that video joint modelling using unrelated videos\nhardly compromises the baseline performance. Addition-\nally, by using semantically similar videos for video joint\nmodelling during training (Intra-Cluster), the performance\nis improved signiﬁcantly compared with the baseline, which\nmeans the related videos are beneﬁcial to learning common\nimportance patterns in videos.\nTABLE 12\nThe results of different frame-level modelling techniques on TVSum.\nTechniques F-measure Kendall’s τ Spearman’sρ\nC3D 0.572 0.055 0.060\nTransformer 0.609 0.097 0.105\n5.6.6 Impact of Transformer-based Frame-level Modelling\nIn our method, the frame-level dependencies within each\nshot are captured by Transformer. To demonstrate whether\n3D-CNN [51] is effective to capture the frame-level de-\npendencies, we conduct experiments by replacing the F-\nTransformer with 3D-CNN to obtain the shot represen-\ntations. Speciﬁcally, for each shot, we utilize the feature\nextractor pre-trained on Sports-1M dataset [52] to extract\nC3D (conv5) layer features. Since C3D only accepts clips of\n16 frames as input, we ﬁrst segment the shot into several\n16-frame clips, and then extract features from each clip. The\nﬁnal semantic representation of each shot is computed as\nthe average feature of the clips within it. Besides, spatial\naverage pooling is also applied to decrease the feature\ndimension. Finally, a representation of 512D is extracted\nfrom each shot, which is used for cross-video inter-shot\ndependency modelling as proposed in our paper. The results\nof using C3D for frame-level dependencies capturing on\nTVSum are reported in TABLE 12. As we can see from the\nresults, once we use the C3D features as the representations\nof shots, the performance of video summarization drops\nsigniﬁcantly compared with our method. The results indi-\ncate that the proposed F-Transformer is not substitutable\nin terms of frame-level correlations capturing. The inferior\nperformance of C3D is expected, because C3D captures only\nthe local dependencies within a clip, which is insufﬁcient\nto model the complex correlations among all frames in the\nshot. In conclusion, the proposed F-Transformer is crucial\nfor frame-level modelling and cannot be replaced with the\nC3D network.\n5.6.7 Qualitative Evaluation of Video Joint Modelling\nApart from evaluating the proposed video joint modelling\nquantitatively, we also visualize the predicted important\nscores to qualitatively demonstrate the effectiveness of\nvideo joint modelling. The visualization results of four\nvideos from the TVSum dataset are shown in Fig. 5. As we\ncan see from the ﬁgure, the model with video joint mod-\nelling (red line) ﬁts the ground truth (green line) better than\nthe model without video joint modelling (blue line) does.\nThe results indicate that video joint modelling is beneﬁcial\nfor measuring the relative importance among different shots\nin videos. In conclusion, video joint modelling is effective in\nboth quantitative and qualitative evaluations.\n6 C ONCLUSION\nIn this paper, we propose Video Joint Modelling based on\nHierarchical Transformer (VJMHT) for co-summarization.\nSpeciﬁcally, VJMHT consists of two layers of Transformer\nto model the videos hierarchically. The ﬁrst Transformer en-\ncodes the frames in each shot, while the second Transformer\ncaptures the global dependencies among all shots. To exploit\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 13\nFig. 5. The visualization of the predicted important scores by the model\nwithout video joint modelling (blue line) and the model with video joint\nmodelling (red line). The green lines show the ground truth scores.\nthe semantic similarity across videos, video joint modelling\nis designed and incorporated in the second Transformer,\nwhere the shots of similar videos are encoded simulta-\nneously. Moreover, Transformer-based video representation\nreconstruction is introduced in the training process to min-\nimize the semantic distance between the summary and the\nvideo. The experiment results have proved the effectiveness\nof the proposed components, and the superiority of VJMHT\nin terms F-measure and rank correlation coefﬁcients.\nACKNOWLEDGEMENTS\nThis research was undertaken using the LIEF HPC-GPGPU\nFacility. This Facility was established with the assistance of\nLIEF Grant LE170100200. This work was also supported by\nGrant 2022ECR008. MG was supported by DE210101624.\nREFERENCES\n[1] A. Mitra, S. Biswas, and C. Bhattacharyya, “Bayesian modeling of\ntemporal coherence in videos for entity discovery and summariza-\ntion,” IEEE transactions on pattern analysis and machine intelligence ,\nvol. 39, no. 3, pp. 430–443, 2016.\n[2] M. Sun, A. Farhadi, B. Taskar, and S. Seitz, “Summarizing un-\nconstrained videos using salient montages,” IEEE transactions on\npattern analysis and machine intelligence , vol. 39, no. 11, pp. 2256–\n2269, 2016.\n[3] K. Zhang, W.-L. Chao, F. Sha, and K. Grauman, “Video summa-\nrization with long short-term memory,” in European conference on\ncomputer vision. Springer, 2016, pp. 766–782.\n[4] B. Mahasseni, M. Lam, and S. Todorovic, “Unsupervised video\nsummarization with adversarial lstm networks,” in Proceedings of\nthe IEEE conference on Computer Vision and Pattern Recognition, 2017,\npp. 202–211.\n[5] K. Zhou, Y. Qiao, and T. Xiang, “Deep reinforcement learn-\ning for unsupervised video summarization with diversity-\nrepresentativeness reward,” in Proceedings of the AAAI Conference\non Artiﬁcial Intelligence, vol. 32, no. 1, 2018.\n[6] B. Zhao, X. Li, and X. Lu, “Hierarchical recurrent neural network\nfor video summarization,” in Proceedings of the 25th ACM interna-\ntional conference on Multimedia, 2017, pp. 863–871.\n[7] Y. Yuan, H. Li, and Q. Wang, “Spatiotemporal modeling for video\nsummarization using convolutional recurrent neural network,”\nIEEE Access, vol. 7, pp. 64 676–64 685, 2019.\n[8] B. Zhao, X. Li, and X. Lu, “Hsa-rnn: Hierarchical structure-\nadaptive rnn for video summarization,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition, 2018, pp. 7405–\n7414.\n[9] J. Fajtl, H. S. Sokeh, V . Argyriou, D. Monekosso, and P . Remagnino,\n“Summarizing videos with attention,” in Asian Conference on Com-\nputer Vision. Springer, 2018, pp. 39–54.\n[10] Z. Wei, B. Wang, M. Hoai, J. Zhang, X. Shen, Z. Lin, R. Mech,\nand D. Samaras, “Sequence-to-segments networks for detecting\nsegments in videos,” IEEE transactions on pattern analysis and\nmachine intelligence, 2019.\n[11] T. Hussain, K. Muhammad, W. Ding, J. Lloret, S. W. Baik, and\nV . H. C. de Albuquerque, “A comprehensive survey of multi-view\nvideo summarization,” Pattern Recognition , vol. 109, p. 107567,\n2021.\n[12] X. Li, B. Zhao, and X. Lu, “A general framework for edited\nvideo and raw video summarization,” IEEE Transactions on Image\nProcessing, vol. 26, no. 8, pp. 3652–3664, 2017.\n[13] B. Zhao, H. Li, X. Lu, and X. Li, “Reconstructive sequence-graph\nnetwork for video summarization,” IEEE Transactions on Pattern\nAnalysis and Machine Intelligence, 2021.\n[14] W.-S. Chu, Y. Song, and A. Jaimes, “Video co-summarization:\nVideo summarization by visual co-occurrence,” in Proceedings of\nthe IEEE Conference on Computer Vision and Pattern Recognition ,\n2015, pp. 3584–3592.\n[15] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N.\nGomez, Ł. Kaiser, and I. Polosukhin, “Attention is all you need,”\nAdvances in neural information processing systems , vol. 30, pp. 5998–\n6008, 2017.\n[16] S. E. F. De Avila, A. P . B. Lopes, A. da Luz Jr, and A. de Al-\nbuquerque Ara ´ujo, “Vsumm: A mechanism designed to produce\nstatic video summaries and a novel evaluation method,” Pattern\nRecognition Letters, vol. 32, no. 1, pp. 56–68, 2011.\n[17] C.-W. Ngo, Y.-F. Ma, and H.-J. Zhang, “Automatic video summa-\nrization by graph modeling,” in Proceedings Ninth IEEE Interna-\ntional Conference on Computer Vision. IEEE, 2003, pp. 104–109.\n[18] S. Mei, G. Guan, Z. Wang, M. He, X.-S. Hua, and D. D. Feng, “L 2, 0\nconstrained sparse dictionary selection for video summarization,”\nin 2014 IEEE international conference on multimedia and expo (ICME).\nIEEE, 2014, pp. 1–6.\n[19] M. Rochan, L. Ye, and Y. Wang, “Video summarization using fully\nconvolutional sequence networks,” in Proceedings of the European\nConference on Computer Vision (ECCV), 2018, pp. 347–363.\n[20] Z. Ji, K. Xiong, Y. Pang, and X. Li, “Video summarization with\nattention-based encoder–decoder networks,” IEEE Transactions on\nCircuits and Systems for Video Technology , vol. 30, no. 6, pp. 1709–\n1717, 2019.\n[21] C. Huang and H. Wang, “A novel key-frames selection framework\nfor comprehensive video summarization,” IEEE Transactions on\nCircuits and Systems for Video Technology , vol. 30, no. 2, pp. 577–\n589, 2019.\n[22] J. Park, J. Lee, I.-J. Kim, and K. Sohn, “Sumgraph: Video summa-\nrization via recursive graph modeling,” in Computer Vision–ECCV\n2020: 16th European Conference, Glasgow, UK, August 23–28, 2020,\nProceedings, Part XXV 16. Springer, 2020, pp. 647–663.\n[23] W. Zhang, B. Wang, S. Ma, Y. Zhang, and Y. Zhao, “I2net: Mining\nintra-video and inter-video attention for temporal action localiza-\ntion,” Neurocomputing, vol. 444, pp. 16–29, 2021.\n[24] B. Wang, X. Zhang, and Y. Zhao, “Exploring sub-action granu-\nlarity for weakly supervised temporal action localization,” IEEE\nTransactions on Circuits and Systems for Video Technology, 2021.\n[25] M. Han, Y. Wang, X. Chang, and Y. Qiao, “Mining inter-video pro-\nposal relations for video object detection,” in European Conference\non Computer Vision. Springer, 2020, pp. 431–446.\n[26] N. Wang, W. Zhou, and H. Li, “Contrastive transformation\nfor self-supervised correspondence learning,” arXiv preprint\narXiv:2012.05057, 2020.\n[27] R. Zhang, J. Li, H. Sun, Y. Ge, P . Luo, X. Wang, and L. Lin,\n“Scan: Self-and-collaborative attention network for video person\nre-identiﬁcation,” IEEE Transactions on Image Processing , vol. 28,\nno. 10, pp. 4870–4882, 2019.\n[28] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and\nS. Zagoruyko, “End-to-end object detection with transformers,” in\nEuropean Conference on Computer Vision . Springer, 2020, pp. 213–\n229.\nJOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015 14\n[29] X. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai, “Deformable detr:\nDeformable transformers for end-to-end object detection,” arXiv\npreprint arXiv:2010.04159, 2020.\n[30] R. Girdhar, J. Carreira, C. Doersch, and A. Zisserman, “Video\naction transformer network,” inProceedings of the IEEE/CVF Confer-\nence on Computer Vision and Pattern Recognition, 2019, pp. 244–253.\n[31] K. Gavrilyuk, R. Sanford, M. Javan, and C. G. Snoek, “Actor-\ntransformers for group activity recognition,” in Proceedings of the\nIEEE/CVF Conference on Computer Vision and Pattern Recognition ,\n2020, pp. 839–848.\n[32] K. Zhang, K. Grauman, and F. Sha, “Retrospective encoders for\nvideo summarization,” in Proceedings of the European Conference on\nComputer Vision (ECCV), 2018, pp. 383–399.\n[33] K. He, X. Zhang, S. Ren, and J. Sun, “Deep residual learning for\nimage recognition,” inProceedings of the IEEE conference on computer\nvision and pattern recognition, 2016, pp. 770–778.\n[34] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[35] D. Potapov, M. Douze, Z. Harchaoui, and C. Schmid, “Category-\nspeciﬁc video summarization,” in European conference on computer\nvision. Springer, 2014, pp. 540–555.\n[36] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-\ntraining of deep bidirectional transformers for language under-\nstanding,” arXiv preprint arXiv:1810.04805, 2018.\n[37] Y. Tay, M. Dehghani, D. Bahri, and D. Metzler, “Efﬁcient trans-\nformers: A survey,” arXiv preprint arXiv:2009.06732, 2020.\n[38] M. Gygli, H. Grabner, H. Riemenschneider, and L. Van Gool,\n“Creating summaries from user videos,” in European conference on\ncomputer vision. Springer, 2014, pp. 505–520.\n[39] Y. Song, J. Vallmitjana, A. Stent, and A. Jaimes, “Tvsum: Summa-\nrizing web videos using titles,” in Proceedings of the IEEE conference\non computer vision and pattern recognition , 2015, pp. 5179–5187.\n[40] A. F. Smeaton, P . Over, and W. Kraaij, “Evaluation campaigns and\ntrecvid,” in Proceedings of the 8th ACM international workshop on\nMultimedia information retrieval, 2006, pp. 321–330.\n[41] Y. Jung, D. Cho, D. Kim, S. Woo, and I. S. Kweon, “Discrimina-\ntive feature learning for unsupervised video summarization,” in\nProceedings of the AAAI Conference on Artiﬁcial Intelligence , vol. 33,\nno. 01, 2019, pp. 8537–8544.\n[42] C. Szegedy, W. Liu, Y. Jia, P . Sermanet, S. Reed, D. Anguelov,\nD. Erhan, V . Vanhoucke, and A. Rabinovich, “Going deeper with\nconvolutions,” in Proceedings of the IEEE conference on computer\nvision and pattern recognition, 2015, pp. 1–9.\n[43] O. Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma,\nZ. Huang, A. Karpathy, A. Khosla, M. Bernstein et al., “Imagenet\nlarge scale visual recognition challenge,” International journal of\ncomputer vision, vol. 115, no. 3, pp. 211–252, 2015.\n[44] D. P . Kingma and J. Ba, “Adam: A method for stochastic optimiza-\ntion,” arXiv preprint arXiv:1412.6980, 2014.\n[45] X. He, Y. Hua, T. Song, Z. Zhang, Z. Xue, R. Ma, N. Robertson,\nand H. Guan, “Unsupervised video summarization with attentive\nconditional generative adversarial networks,” in Proceedings of the\n27th ACM International Conference on Multimedia , 2019, pp. 2296–\n2304.\n[46] B. Zhao, X. Li, and X. Lu, “Property-constrained dual learning\nfor video summarization,” IEEE transactions on neural networks and\nlearning systems, vol. 31, no. 10, pp. 3989–4000, 2019.\n[47] M. Rochan and Y. Wang, “Video summarization by learning from\nunpaired data,” in Proceedings of the IEEE/CVF Conference on Com-\nputer Vision and Pattern Recognition, 2019, pp. 7902–7911.\n[48] Y. Chen, L. Tao, X. Wang, and T. Yamasaki, “Weakly supervised\nvideo summarization by hierarchical reinforcement learning,” in\nProceedings of the ACM Multimedia Asia , 2019, pp. 1–6.\n[49] Y. Jung, D. Cho, S. Woo, and I. S. Kweon, “Global-and-local relative\nposition embedding for unsupervised video summarization,” in\nEuropean Conference on Computer Vision, ECCV 2020 . Springer,\n2020.\n[50] M. Otani, Y. Nakashima, E. Rahtu, and J. Heikkila, “Rethinking the\nevaluation of video summaries,” in Proceedings of the IEEE/CVF\nConference on Computer Vision and Pattern Recognition , 2019, pp.\n7596–7604.\n[51] D. Tran, L. Bourdev, R. Fergus, L. Torresani, and M. Paluri, “Learn-\ning spatiotemporal features with 3d convolutional networks,” in\nProceedings of the IEEE international conference on computer vision ,\n2015, pp. 4489–4497.\n[52] A. Karpathy, G. Toderici, S. Shetty, T. Leung, R. Sukthankar, and\nL. Fei-Fei, “Large-scale video classiﬁcation with convolutional\nneural networks,” in Proceedings of the IEEE conference on Computer\nVision and Pattern Recognition, 2014, pp. 1725–1732.\nHaopeng Li is a Ph.D student in the School of\nComputing and Information Systems, University\nof Melbourne. He received his B.S. degree in the\nSchool of Science, Northwestern Polytechnical\nUniversity, and the Master degree in the School\nof Artiﬁcial Intelligence, Optics and Electronics\n(iOPEN), Northwestern Polytechnical University.\nHis research interests include computer vision,\nvideo understanding, and artiﬁcial intelligence.\nQiuhong Kereceived her PhD degree from The\nUniversity of Western Australia in 2018. She\nis a Lecturer (Assistant Professor) at Monash\nUniversity. Before that, she was a Postdoctoral\nResearcher at Max Planck Institute for Informat-\nics and a Lecturer at University of Melbourne.\nHer thesis “Deep Learning for Action Recogni-\ntion and Prediction” has been awarded “Dean’s\nList-Honourable mention” by The University of\nWestern Australia in 2018. She was awarded\n“1962 Medal” for her work in video recognition\ntechnology by the Australian Computer Society in 2019. She was also\nawarded Early Career Researcher Award by Australia Pattern Recogni-\ntion Society in 2020. Her research interests include computer vision and\nmachine learning.\n.\nMingming Gongis a Lecturer (Assistant Profes-\nsor) in data science with the School of Math-\nematics and Statistics, the University of Mel-\nbourne. His research interests include causal\nreasoning, machine learning, and computer vi-\nsion. He has authored and co-authored 40+ re-\nsearch papers on top venues such as ICML,\nNeurIPS, UAI, AISTATS, IJCAI, AAAI, CVPR,\nICCV, ECCV, with 10+ oral/spotlight presenta-\ntions and a best paper ﬁnalist at CVPR19. He\nhas served as area chairs of NeurIPS’21 and\nICLR’21, senior program committee members of AAAI’19-20, IJCAI’20-\n21, program committee members of ICML, NeurIPS, UAI, CVPR, ICCV,\nand reviewers of TPAMI, AIJ, MLJ, TIP , TNNLS, etc. He received the\nDiscovery Early Career Researcher Award from Australian Research\nCouncil in 2021.\nRui Zhang(www.ruizhang.info) is a visiting pro-\nfessor at Tsinghua University and was a Pro-\nfessor at the University of Melbourne. His re-\nsearch interests include AI and big data, par-\nticularly in the areas of recommender systems,\nknowledge bases, chatbot, and spatial and tem-\nporal data analytics. Professor Zhang has won\nseveral awards including Future Fellowship by\nthe Australian Research Council in 2012, Chris\nWallace Award for Outstanding Research by the\nComputing Research and Education Association\nof Australasia in 2015, and Google Faculty Research Award in 2017.",
  "topic": "Automatic summarization",
  "concepts": [
    {
      "name": "Automatic summarization",
      "score": 0.9397872686386108
    },
    {
      "name": "Computer science",
      "score": 0.8675376176834106
    },
    {
      "name": "Storyboard",
      "score": 0.7446869611740112
    },
    {
      "name": "Transformer",
      "score": 0.5855918526649475
    },
    {
      "name": "Video tracking",
      "score": 0.5840100646018982
    },
    {
      "name": "Video compression picture types",
      "score": 0.5622907876968384
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5470569133758545
    },
    {
      "name": "Information retrieval",
      "score": 0.4217653274536133
    },
    {
      "name": "Computer vision",
      "score": 0.3361778259277344
    },
    {
      "name": "Video processing",
      "score": 0.3312101662158966
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.32692205905914307
    },
    {
      "name": "Multimedia",
      "score": 0.25741255283355713
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I165779595",
      "name": "The University of Melbourne",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I56590836",
      "name": "Monash University",
      "country": "AU"
    },
    {
      "id": "https://openalex.org/I99065089",
      "name": "Tsinghua University",
      "country": "CN"
    }
  ]
}