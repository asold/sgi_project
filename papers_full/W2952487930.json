{
    "title": "Training Hybrid Language Models by Marginalizing over Segmentations",
    "url": "https://openalex.org/W2952487930",
    "year": 2019,
    "authors": [
        {
            "id": "https://openalex.org/A2114720862",
            "name": "Edouard Grave",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A2035043562",
            "name": "Sainbayar Sukhbaatar",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A1882694979",
            "name": "Piotr Bojanowski",
            "affiliations": [
                "Meta (Israel)"
            ]
        },
        {
            "id": "https://openalex.org/A2512114774",
            "name": "Armand Joulin",
            "affiliations": [
                "Meta (Israel)"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W2998704965",
        "https://openalex.org/W2963088785",
        "https://openalex.org/W2525246036",
        "https://openalex.org/W4299548129",
        "https://openalex.org/W4302375066",
        "https://openalex.org/W4298422451",
        "https://openalex.org/W2963304263",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2963735467",
        "https://openalex.org/W2906625520",
        "https://openalex.org/W2609370997",
        "https://openalex.org/W179875071",
        "https://openalex.org/W2964325845",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W2963831883",
        "https://openalex.org/W2592647456",
        "https://openalex.org/W4385245566",
        "https://openalex.org/W4294555862",
        "https://openalex.org/W2727642071",
        "https://openalex.org/W2963983719",
        "https://openalex.org/W2963899393",
        "https://openalex.org/W2962784628",
        "https://openalex.org/W2510842514",
        "https://openalex.org/W2963077280",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W2963494889",
        "https://openalex.org/W2963357986",
        "https://openalex.org/W2964019776",
        "https://openalex.org/W1966812932"
    ],
    "abstract": "In this paper, we study the problem of hybrid language modeling, that is using models which can predict both characters and larger units such as character ngrams or words. Using such models, multiple potential segmentations usually exist for a given string, for example one using words and one using characters only. Thus, the probability of a string is the sum of the probabilities of all the possible segmentations. Here, we show how it is possible to marginalize over the segmentations efficiently, in order to compute the true probability of a sequence. We apply our technique on three datasets, comprising seven languages, showing improvements over a strong character level language model.",
    "full_text": "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1477–1482\nFlorence, Italy, July 28 - August 2, 2019.c⃝2019 Association for Computational Linguistics\n1477\nTraining Hybrid Language Models by Marginalizing over Segmentations\nEdouard Grave Sainbayar Sukhbaatar Piotr Bojanowski Armand Joulin\nFacebook AI Research\n{egrave,sainbar,bojanowski,ajoulin}@fb.com\nAbstract\nIn this paper, we study the problem of hy-\nbrid language modeling, that is using models\nwhich can predict both characters and larger\nunits such as character ngrams or words. Us-\ning such models, multiple potential segmenta-\ntions usually exist for a given string, for ex-\nample one using words and one using charac-\nters only. Thus, the probability of a string is\nthe sum of the probabilities of all the possi-\nble segmentations. Here, we show how it is\npossible to marginalize over the segmentations\nefﬁciently, in order to compute the true prob-\nability of a sequence. We apply our technique\non three datasets, comprising seven languages,\nshowing improvements over a strong character\nlevel language model.\n1 Introduction\nStatistical language modeling is the problem of\nestimating a probability distribution over text\ndata (Bahl et al., 1983). Most approaches formu-\nlate this problem at the word level, by ﬁrst seg-\nmenting the text using a ﬁxed vocabulary. A limi-\ntation of these methods is that they cannot generate\nnew words, or process out of vocabulary words. A\npopular alternative is to directly model sequences\nat the character level. These models can poten-\ntially generate any sequence, and are thus some-\ntimes referred to as open vocabulary. However,\nthey tend to underperform compared to word level\nmodels when trained on the same data.\nFor these reasons, a few works have proposed\nhybrid models, that work both at the character and\nword level (or sometimes groups of characters). A\nﬁrst class of hybrid models switch between word\nand character level representations, depending on\nwhether they predict that the upcoming word is\nin the vocabulary or not (Kawakami et al., 2017;\nMielke and Eisner, 2019). For example, a ﬁrst\nmodel can be trained on tokenized data, where out-\nof-vocabulary words are replaced by the <unk>\ntoken. A second model is then used to generate\nthe character sequences corresponding to out-of-\nvocabulary words. Another approach, which does\nnot require tokenization, is to process groups of\ncharacters, which are obtained based on linguis-\ntic knowledge or low level statistics. These in-\nclude merging characters using mutual informa-\ntion (Mikolov et al., 2012) or the byte pair en-\ncoding algorithm (Sennrich et al., 2016). This ap-\nproach ﬁrst produces a segmentation for the text,\nand then learns a language model on it. However,\nsome sequences have multiple possible segmenta-\ntions, and a model considering a single one might\nunderestimate the true probability of the sequence.\nThus, it is important to marginalize over the set of\nsegmentations to obtain the true probability of a\nsequence (van Merri¨enboer et al., 2017; Buckman\nand Neubig, 2018).\nIn this paper, we propose an alternative ap-\nproach to address this limitation, and in particu-\nlar, to train models by marginalizing over the set\nof segmentations. As the number of possible seg-\nmentations grows exponentially with the sequence\nsize, using an efﬁcient algorithm such as dynamic\nprogramming is important. Computing the repre-\nsentation of the context at the character level al-\nlows to apply dynamic programming to this prob-\nlem, without using approximations. This tech-\nnique was previously considered in the context of\nautomatic speech recognition (Wang et al., 2017)\nor to copy tokens from the input for code genera-\ntion (Ling et al., 2016). We evaluate our method\non three datasets for character level language mod-\neling, showing that adding n-grams to the predic-\ntions improve the perplexity of the model.\n1478\n2 Approach\nThe goal of character level language modeling is\nto learn a probability distribution over sequences\nof characters c1, ..., cT . Using the chain rule, such\na distribution can be factorized as the product of\nthe probability distribution of a character condi-\ntioned on its history:\np(c1, ..., cT ) =\nT∏\nt=1\np(ct |c0, ..., ct−1),\nwhere c0 is a special symbol indicating the begin-\nning of the sequence. In this paper, we learn these\nconditional probability distributions using neural\nnetworks. For each time step t, a neural network\nbuilds a representation ht from the history that is\nused to predict the upcoming character. This rep-\nresentation can be obtained from any architecture,\nsuch as feedforward (Bengio et al., 2003) or re-\ncurrent networks (Mikolov et al., 2010). We fo-\ncus on the transformer network, recently intro-\nduced by Vaswani et al. (2017), because of its high\nperformance on character level language model-\ning (Dai et al., 2018). We refer to Vaswani et al.\n(2017) for the details of this architecture.\n2.1 Hybrid language models\nHybrid language models predict multiple tokens,\ninstead of one, at each time step. One way to per-\nform this is to add n-grams to the output vocabu-\nlary of the model. Under such models, a charac-\nter sequence has multiple segmentations, and the\nmodel estimates its probability by summing the\nprobability of all its segmentations. For example,\nif the model predicts bigrams in addition to char-\nacters, the word dog can be decomposed as\n[d], [o], [g] or [do], [g] or [d], [og].\nThus, the probability of the sequence of characters\ndog is given by\np(dog) =p(d) ×p(o |d) ×p(g |do)\n+ p(do) ×p(g |do) +p(d) ×p(og |d).\nMore formally, let us denote by S(c1:T ) the set\nof all possible segmentations of a given sequence\nc1:T = c1, ..., cT . Then, the probability of the\ncharacter sequence is\np(c1:T ) =p(c1, ..., cT ) =\n∑\ns∈S(c)\np(s). (1)\nThe set of all possible segmentations grows expo-\nnentially with the sequence size, making it imprac-\ntical to evaluate this probability by directly sum-\nming over all segmentations.\n2.2 Factorization of the segmentation\nprobabilities\nA segmentation s can be decomposed into a se-\nquence s1, ..., sK of consecutive atoms in the vo-\ncabulary on which we apply the chain rule to get:\np(s) =\nK∏\nk=1\np(sk |s0, ..., sk−1).\nUsing this factorization of the probability distri-\nbution, it is not possible to directly apply dynamic\nprogramming to compute the probability of a se-\nquence. The reason is that the conditional distri-\nbution of symbols depends on the segmentation,\npreventing to reuse computation across different\nsegmentations. For example, previous work pro-\nposed to use the segmentation both in the input\nand output of the model. The hidden representa-\ntions ht of the neural network were thus intrin-\nsically linked to the segmentation, preventing to\nshare computations. A potential workaround is\nto merge the different representations correspond-\ning to all the segmentations ending at the same\ncharacter, for example by avergaging them (van\nMerri¨enboer et al., 2017; Buckman and Neubig,\n2018). In our case, we usen-grams only in the out-\nput, making the representations ht independent of\nthe segmentations, and the application of dynamic\nprogramming straightforward.\nTo do so, we deﬁne the conditional distribu-\ntion using characters, instead of the segmentation.\nGiven a sequence s1, ..., sK of n-grams, we intro-\nduce the concatenation operator concat, such that\nconcat(s1, ..., sK) =c1, ..., cJ\ncorresponds to the sequence of J characters that\ncompose the segmentation sequence. For exam-\nple, the two segmentations [do], [g], [s] and\n[d], [og], [s] of the word dogs share the same\noutput from the concat operator:\nconcat([do], [g], [s]) = d, o, g, s,\nconcat([d], [og], [s]) = d, o, g, s.\nWe now deﬁne the conditional distribution as\np(sk |s1:k−1) =p(sk |concat(s1:k−1)). (2)\n1479\nThis reformulation is exact under the conditional\nindependence assumption, i.e., that the symbol at\nposition t in the character sequence is indepen-\ndent of the segmentation, given the characters up\nto time t−1. In the next section, we show how, un-\nder this assumption, the probability of a sequence\ncan be computed with dynamic programming.\n2.3 Dynamic programming\nFor this section, we restrict ourselves to predict-\ning characters and bigrams for simplicity. How-\never, our approach is straightforward to apply to\nn-grams or words. Given a sequence of character\nc = c1, ..., cT , all segmentations end with either\nthe character cT or the bigram cT−1cT . More pre-\ncisely, we can decompose the probability of c as:\np(c1:T ) =\n∑\ns∈S(c1:T−1)\np(cT |s)p(s)\n+\n∑\ns∈S(c1:T−2)\np(cT−1cT |s)p(s).\nUsing the reformulation of the conditional prob-\nability of Eq. (2) under the conditional indepen-\ndence assumption on segmentations, we get\np(c1:T ) =\n∑\ns∈S(c1:T−1)\np(cT |c1:T−1)p(s)\n+\n∑\ns∈S(c1:T−2)\np(cT−1cT |c1:T−2)p(s).\nWe now move the conditional probabilities out of\nthe sums:\np(c1:T ) =p(cT |c1:T−1)\n∑\ns∈S(c1:T−1)\np(s)\n+ p(cT−1cT |c1:T−2)\n∑\ns∈S(c1:T−2)\np(s).\nFinally, using Eq. (1), we obtain a recurrence rela-\ntion over the sequence probabilities:\np(c1:T ) =p(cT |c1:T−1)p(c1:T−1)\n+ p(cT−1cT |c1:T−2)p(c1:T−2).\nWe can thus optimize over all the possible segmen-\ntations using dynamic programing.\n2.4 Conditional distribution of symbols\nIn this section, we brieﬂy describe how to model\nthe conditional probability distribution of sym-\nbols, either characters or ngrams, given the char-\nacter history. We learn a character level neural net-\nwork to encode the context with hidden represen-\ntation ht for each character t. The probability dis-\ntribution of the next symbol, either a character or\na n-gram, is obtained by taking the softmax over\nthe full vocabulary, which includes both characters\nand longer elements:\np(·|c0, ..., ct−1) =softmax(Wht).\nNote that we get only one probability distribution\nover n-grams of different lengths.\n2.5 Training procedure\nWe learn the parameters of our model by min-\nimizing the negative log-likelihood of the train-\ning data, using the probability introduced in Eq.\n(1). We rely on automatic differentiation to com-\npute the gradients, and thus, only need to imple-\nment the forward computation, which relies on\ndynamic programming. Empirically, we observed\nthat training a model from scratch with this objec-\ntive is sometimes unstable. We thus consider an al-\nternative training objective, used at the beginning\nof training. For each position, this loss is equal\nto the sum of the negative log-probabilities of the\nn-grams corresponding to that position. More for-\nmally, given a sequence of lengthT, this objective\nis equal to\n−\nT∑\nt=1\nN−1∑\nn=1\nlog (p(ct:t+n |c1:t−1)) ,\nand N is the size of the longest n-grams consid-\nered (we can pad n-grams when t + n > T or\nexclude them from this loss).\n3 Experiments\nIn this section, we describe the experiments that\nwe performed to evaluate our approach on charac-\nter level language modeling.\n3.1 Datasets\nWe consider 3 datasets derived from Wikipedia ar-\nticles, but with different preprocessing.\nText8. The text8 dataset of M. Mahoney1 con-\ntains 100 million characters from Wikipedia, and\nwas preprocessed to only contains the lowercase\nletters a-z and nonconsecutive spaces.\n1http://mattmahoney.net/dc/textdata\n1480\nModel Cs De En Es Fi Fr Ru Avg.\nHCLM (Kawakami et al., 2017) 2.035 1.641 1.622 1.555 1.796 1.508 1.810 1.710\nHCLM cache (Kawakami et al., 2017) 1.984 1.588 1.538 1.498 1.711 1.467 1.761 1.649\nFull (Mielke and Eisner, 2019) 2.240 1.618 1.506 1.469 1.896 1.434 1.969 1.733\nFull (tok) (Mielke and Eisner, 2019) 1.928 1.465 1.387 1.363 1.751 1.319 1.709 1.560\nBPE (Mielke and Eisner, 2019) 1.897 1.455 1.439 1.403 1.685 1.365 1.643 1.555\nBPE (tok) (Mielke and Eisner, 2019) 1.856 1.414 1.386 1.362 1.652 1.317 1.598 1.512\nTransformer baseline 1.777 1.406 1.393 1.37 1.525 1.34 1.616 1.489\nOur approach 1.715 1.352 1.341 1.326 1.445 1.299 1.508 1.426\nTable 1: Test set bpc on the MWC dataset. The hyperparameters for our method are chosen on the validation set of\nWikiText2. Note that Mielke and Eisner (2019) applied the BPE baseline and their method to both tokenized\nand non-tokenized data. All the other methods were applied on non-tokenized data only.\nModel Test\nBN LSTM (Cooijmans et al., 2016) 1.36\nHM LSTM (Chung et al., 2016) 1.29\nRHN (Zilly et al., 2017) 1.27\nLarge mLSTM (Krause et al., 2016) 1.27\n12L Transf. (Al-Rfou et al., 2018) 1.18\nTransformer baseline 1.176\nOur approach 1.156\nTable 2: Test set bpc on the text8 dataset.\nWikiText2. The WikiText2 dataset was intro-\nduced by Merity et al. (2017) with a different pre-\nprocessing from text8: numbers, capital letters\nand special characters are kept. The vocabulary\nsize is 1152. 2 We use the raw version of the\ndataset, which is tokenized but where rare words\nare not replaced by the <unk> token. The train-\ning data contains 10.9 millions characters.\nMWC. The multilingual Wikipedia corpus\n(MWC) of Kawakami et al. (2017) is very simi-\nlar in size and preprocessing as WikiText2,\nbut contains documents in 7 languages: Czech\n(cs), German ( de), English ( en), Spanish ( es),\nFinnish ( fi), French ( fr) and Russian ( ru).\nUnlike Wikitext2, the MWC dataset is not\ntokenized. The training sets range from 6.1M\ncharacters for Czech to 15.6M characters for\nEnglish, and we refer the reader to Kawakami\net al. (2017) for detailed statistics on this corpus.3\n2As opposed to previous work, we keep all characters that\nappears in the train, validation or test splits of the data.\n3Again, we keep all characters that appears in the data.\nModel Test\nHCLM (Kawakami et al., 2017) 1.670\nHCLM cache (Kawakami et al., 2017) 1.500\nBPE (Mielke and Eisner, 2019) 1.468\nFull (Mielke and Eisner, 2019) 1.455\nTransformer baseline 1.417\nOur approach 1.366\nTable 3: Test set bpc on the WikiText2 dataset.\n3.2 Technical details\nFollowing recent work on character language\nmodeling with transformers, we use a model with\n12 layers of dimension 512, and 4 attention heads.\nWe use a feedforward block of dimension 2048\nfor MWC and WikiText2, and 3072 for text8.\nWe set the attention length to 512, and the batch\nsize to 8. We use Adagrad (Duchi et al., 2011)\nto learn the parameters of our models. Follow-\ning Vaswani et al. (2017), we start with a learn-\ning rate of 0, increase it linearly for k timesteps,\nthen keep it constant, before halving it at every\nepochs for the last 10 epochs. We use a learn-\ning rate of 0.04 and warmup of 16k steps for the\ntext8 dataset, and a learning rate of 0.025 and\nwarmup of 8k steps for the WikiText2 and MWC\ndatasets. In order to have an efﬁcient model at\ninference time, we use the caching mechanism\nfrom Dai et al. (2018) to store the hidden repre-\nsentations of the previous batch, as well as rel-\native position weights. We pick a dropout rate\nin the set {0.1, 0.2, 0.3, 0.4, 0.5}, using the vali-\ndation set. In the experiments, we use n-grams\nof size up to 4, excluding n-grams appearing less\nthan 200 times (1000 times ontext8) to limit the\n1481\nsize of the vocabulary. Thus, segmentations which\ncontain out-of-vocabulary n-grams have a proba-\nbility equal to zero.\n3.3 Results\nIn Table 1, we report results on the MWC\ndataset, comparing our approach to the models of\nKawakami et al. (2017) and Mielke and Eisner\n(2019). Our approach signiﬁcantly improves the\nstate of the art on this dataset. Some of the gain is\ndue to the change of architecture for a transformer.\nHowever, we observe that marginalizing over seg-\nmentations also improves over the character level\ntransformer baseline, showing the beneﬁts of our\nmethod. Finally, as opposed to Mielke and Eisner\n(2019), our approach does not need to tokenize the\ndata to perform well on this dataset.\nIn Table 2 and Table 3, we report results on the\ntext8 and wikitext2 datasets respectively.\nAs for the MWC dataset, our approach signiﬁcantly\nimproves the perplexity compared to our character\nlevel transformer baseline. Note that the state of\nthe art on text8 is 1.08 bpc on the test set with\na 24-layer transformer network (Dai et al., 2018).\nThis model is signiﬁcantly larger than ours, con-\ntaining almost 8 times more parameters.\n4 Conclusion\nIn this paper, we study the problem of hybrid\nlanguage modeling, where models can predict n-\ngrams, instead of unigrams only. A technical chal-\nlenge for learning these models is that a given\nstring can have multiple segmentations, and one\nneeds to marginalize over the set of segmentations.\nWe introduce a simple technique to do so, allow-\ning to apply dynamic programming for learning\nand inference. Using this approach, we improve\nthe state of the art on the MWC and WikiText2\ndatasets, used to evaluate hybrid language models.\nAcknowledgements\nWe thank the anonymous reviewers for their help-\nful comments.\nReferences\nRami Al-Rfou, Dokook Choe, Noah Constant, Mandy\nGuo, and Llion Jones. 2018. Character-level lan-\nguage modeling with deeper self-attention. arXiv\npreprint arXiv:1808.04444.\nLalit R Bahl, Frederick Jelinek, and Robert L Mercer.\n1983. A maximum likelihood approach to continu-\nous speech recognition. IEEE Transactions on Pat-\ntern Analysis & Machine Intelligence, (2):179–190.\nYoshua Bengio, R´ejean Ducharme, Pascal Vincent, and\nChristian Jauvin. 2003. A neural probabilistic lan-\nguage model. JMLR.\nJacob Buckman and Graham Neubig. 2018. Neural lat-\ntice language models. TACL.\nJunyoung Chung, Sungjin Ahn, and Yoshua Bengio.\n2016. Hierarchical multiscale recurrent neural net-\nworks. arXiv preprint arXiv:1609.01704.\nTim Cooijmans, Nicolas Ballas, C´esar Laurent, C ¸ a˘glar\nG¨ulc ¸ehre, and Aaron Courville. 2016. Re-\ncurrent batch normalization. arXiv preprint\narXiv:1603.09025.\nZihang Dai, Zhilin Yang, Yiming Yang, William W\nCohen, Jaime Carbonell, Quoc V Le, and Ruslan\nSalakhutdinov. 2018. Transformer-xl: Language\nmodeling with longer-term dependency.\nJohn Duchi, Elad Hazan, and Yoram Singer. 2011.\nAdaptive subgradient methods for online learning\nand stochastic optimization. JMLR.\nKazuya Kawakami, Chris Dyer, and Phil Blunsom.\n2017. Learning to create and reuse words in open-\nvocabulary neural language modeling. In ACL.\nBen Krause, Liang Lu, Iain Murray, and Steve Renals.\n2016. Multiplicative lstm for sequence modelling.\narXiv preprint arXiv:1609.07959.\nWang Ling, Edward Grefenstette, Karl Moritz Her-\nmann, Tom ´aˇs Ko ˇcisk`y, Andrew Senior, Fumin\nWang, and Phil Blunsom. 2016. Latent predic-\ntor networks for code generation. arXiv preprint\narXiv:1603.06744.\nStephen Merity, Caiming Xiong, James Bradbury, and\nRichard Socher. 2017. Pointer sentinel mixture\nmodels. In ICLR.\nBart van Merri ¨enboer, Amartya Sanyal, Hugo\nLarochelle, and Yoshua Bengio. 2017. Multiscale\nsequence modeling with a learned dictionary. arXiv\npreprint arXiv:1707.00762.\nSebastian J Mielke and Jason Eisner. 2019. Spell once,\nsummon anywhere: A two-level open-vocabulary\nlanguage model. In AAAI.\nTom´aˇs Mikolov, Martin Karaﬁ ´at, Luk ´aˇs Burget, Jan\nˇCernock`y, and Sanjeev Khudanpur. 2010. Recurrent\nneural network based language model. In Eleventh\nannual conference of the international speech com-\nmunication association.\nTom´aˇs Mikolov, Ilya Sutskever, Anoop Deoras, Hai-\nSon Le, Stefan Kombrink, and Jan Cernocky.\n2012. Subword language modeling with neu-\nral networks. preprint (http://www. ﬁt. vutbr.\ncz/imikolov/rnnlm/char. pdf), 8.\n1482\nRico Sennrich, Barry Haddow, and Alexandra Birch.\n2016. Neural machine translation of rare words with\nsubword units. In ACL.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob\nUszkoreit, Llion Jones, Aidan N Gomez, Łukasz\nKaiser, and Illia Polosukhin. 2017. Attention is all\nyou need. In NIPS.\nChong Wang, Yining Wang, Po-Sen Huang, Abdel-\nrahman Mohamed, Dengyong Zhou, and Li Deng.\n2017. Sequence modeling via segmentations. In\nICML.\nJulian Georg Zilly, Rupesh Kumar Srivastava, Jan\nKoutn´ık, and J¨urgen Schmidhuber. 2017. Recurrent\nhighway networks. In ICML."
}