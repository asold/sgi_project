{
    "title": "Language Model Pre-training for Hierarchical Document Representations",
    "url": "https://openalex.org/W2907644564",
    "year": 2022,
    "authors": [
        {
            "id": "https://openalex.org/A4223922037",
            "name": "Chang, Ming-Wei",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4202085630",
            "name": "Toutanova, Kristina",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4227328976",
            "name": "Lee, Kenton",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A4224379200",
            "name": "Devlin, Jacob",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2896457183",
        "https://openalex.org/W2889518897",
        "https://openalex.org/W2952729433",
        "https://openalex.org/W2951777553",
        "https://openalex.org/W2612431505",
        "https://openalex.org/W2117130368",
        "https://openalex.org/W2271328876",
        "https://openalex.org/W2064675550"
    ],
    "abstract": "Hierarchical neural architectures are often used to capture long-distance dependencies and have been applied to many document-level tasks such as summarization, document segmentation, and sentiment analysis. However, effective usage of such a large context can be difficult to learn, especially in the case where there is limited labeled data available. Building on the recent success of language model pretraining methods for learning flat representations of text, we propose algorithms for pre-training hierarchical document representations from unlabeled data. Unlike prior work, which has focused on pre-training contextual token representations or context-independent {sentence/paragraph} representations, our hierarchical document representations include fixed-length sentence/paragraph representations which integrate contextual information from the entire documents. Experiments on document segmentation, document-level question answering, and extractive document summarization demonstrate the effectiveness of the proposed pre-training algorithms.",
    "full_text": "LANGUAGE MODEL PRE-TRAINING FOR HIERARCHI -\nCAL DOCUMENT REPRESENTATIONS\nMing-Wei Chang, Kristina Toutanova, Kenton Lee, Jacob Devlin\nGoogle AI Language\n{mingweichang, kristout, kentonl, jacobdevlin}@google.com\nABSTRACT\nHierarchical neural architectures are often used to capture long-distance depen-\ndencies and have been applied to many document-level tasks such as summa-\nrization, document segmentation, and sentiment analysis. However, effective us-\nage of such a large context can be difﬁcult to learn, especially in the case where\nthere is limited labeled data available. Building on the recent success of language\nmodel pretraining methods for learning ﬂat representations of text, we propose\nalgorithms for pre-training hierarchical document representations from unlabeled\ndata. Unlike prior work, which has focused on pre-training contextual token rep-\nresentations or context-independent sentence/paragraph representations, our hi-\nerarchical document representations include ﬁxed-length sentence/paragraph rep-\nresentations which integrate contextual information from the entire documents.\nExperiments on document segmentation, document-level question answering, and\nextractive document summarization demonstrate the effectiveness of the proposed\npre-training algorithms.\n1 I NTRODUCTION\nWhile many natural language processing (NLP) tasks have been posed as isolated prediction prob-\nlems with limited context (often a single sentence or paragraph), this does not reﬂect how humans\nunderstand natural language. When reading text, humans are sensitive to much more context, such\nas the rest of the document or even other relevant documents. In this paper, we focus on tasks that\nrequire document-level understanding. We build upon two existing separate lines of work that are\nuseful for these tasks: (1) document-level models with hierarchical architectures, which include sen-\ntence representations contextualized with respect to entire documents (Ruder et al., 2016; Cheng &\nLapata, 2016; Koshorek et al., 2018; Yang et al., 2016), and (2) contextual representation learning\nwith language-model pretraining (Peters et al., 2018; Radford et al., 2018).\nIn this work, we show for the ﬁrst time that these two ideas can be successfully combined. We ﬁnd\nthat pretraining hierarchical document representations raises several unique challenges compared to\nexisting representation learning. First, it is not clear how to pretrain the higher level representations.\nUsing existing techniques, it is possible to initialize the lower level of the hierarchical representation,\nto represent words in the context of individual sentences e.g. using ELMo (Peters et al., 2018), or,\nin addition, initialize sentence vectors that are not dependent on the context from the full document\nusing Skip-Thought vectors (Kiros et al., 2015), but it is not possible to initialize the full hierarchical\nneural structure. Second, bidirectional representations are standard in state-of-the-art NLP tasks\n(Wu et al., 2016; Weissenborn et al., 2017), and may be particularly important for document-level\nrepresentations due to the long-distance dependencies. However, current pretraining algorithms\noften use left-to-right and right-to-left language models to pretrain the representations. This restricts\nthe expressiveness of the hierarchical representations, as left and right contextual information do not\nfuse together when forming higher-level hierarchical representations.\nIn this paper, we address these challenges by proposing two novel approaches to pre-train document-\nlevel hierarchical representations. Both methods pre-train representations from unlabeled documents\ncontaining thousands of tokens. Our ﬁrst approach generalizes the method of Peters et al. (2018)\nto pre-train hierarchical left-to-right and right-to-left document representations. To allow the hierar-\nchical representations to learn to fuse left and right contextual information from abundant unlabeled\n1\narXiv:1901.09128v1  [cs.CL]  26 Jan 2019\ntext, our second approach extends the masked language modeltechnique (Devlin et al., 2018) to\nefﬁciently pre-train bidirectional hierarchical document-level representations.\nWe evaluate the impact of the novel aspects of our pre-trained representations on three tasks: docu-\nment segmentation, answer passage retrieval for document-level question answering, and extractive\ntext summarization. We ﬁrst pretrain document-level representations on unlabeled documents and\nthen ﬁne-tune with task-speciﬁc labeled data and a light-weight task network. Experiments show\nthat pre-training hierarchical document representations brings signiﬁcant beneﬁts on all tasks, and\nthat pretraining the higher level of the document representation results in larger improvements than\npre-training only the locally contextualized lower word level in most tasks. On the CNN/Daily Mail\nsummarization task, we obtain strong results without using reinforcement learning methods em-\nployed by the previous state-of-the-art models. On the TriviaQA answer passage retrieval task, our\nmodel improves over prior work (Clark & Gardner, 2018) by 6% absolute.\n2 B ACKGROUND\nIn this section, we review pretraining of contextual token representations using language models and\ndiscuss how the language model probability decomposition imposes constraints on the architectures.\nLanguage Model Pretraining Given a sequence of tokens (x1,...,x n), a representation mod-\nule Vθ encodes each word into a vector: Vθ(x1,...,x n) = ( v1,...,v n), where each word-level\ncontextual representation vi could potentially depend on the whole sequence. Common choices\nfor the representation modules are LSTMs, CNNs, and self-attention architectures (Hochreiter &\nSchmidhuber, 1997; LeCun & Bengio, 1998; Vaswani et al., 2017). A representation module is\nparameterized by an underlying neural architecture with parameters θ.\nThe typical left-to-right language model aims to maximize the probability of observed sequences:\nn∏\nt=1\nP(xt|x1,...x t−1; θ), (1)\nThe pretrained parameters θcan then be used to initialize the model for a downstream task.\nUni-directionality constraint The formulation of the language model implicitly imposes a di-\nrectionality constraint on the Vθ, which we term uni-directionality constraint. In Equation 1, the\nword xt can only depend on the (representations of) the previous words. Therefore, the contextual\nrepresentations v1,...,v t−1 can not depend on the representations of xt,...,x n, resulting in the\nuni-directionality constraint on the representations.\nOne common type of representation modules that satisfy the uni-directionality constraint are uni-\ndirectional recurrent neural networks (RNNs). Given a sequence of words, a left-to-right LSTM\nmodel generates a sequence of representations Vθ(x1,...,x n) = ( − →h1,..., − →hn) that satisﬁes the\nuni-directionality constraints, where − →ht only depends on − →h1,− →h2,... − →ht−1. Modules that sat-\nisfy the uni-directionality constraint can also be derived from self-attention, convolution, and feed-\nforward architectures (e.g. (Radford et al., 2018)).\nAs mentioned in the introduction, for many downstream tasks, bidirectional representations bring\nlarge improvements over uni-directional ones. Therefore, Peters et al. (2018), pre-trained two en-\ncoders using two language models: a left-to-right and a right-to-left model, using a left-to-right and a\nright-to-left LSTM encoder, respectively. The token representations are formed by the concatenation\nof outputs from the two uni-directional encoders. We term this style of language model pre-training\nL+R-LM. More speciﬁcally, the representation of the t-th word becomes vt =\n[− →ht\n← −ht\n]\n, where\n← −ht is generated from a right-to-left LSTM. Tasks using these input representations must learn to\nmerge the left and right contextual information from scratch using only downstream task labeled\ndata.\nThe general strategy we adopt in this paper is to pretrain the hierarchical representations on unla-\nbeled data and then to ﬁne-tune the representations with task-speciﬁc labeled data. Therefore, the\nuni-directionality constraint also impacts the task-speciﬁc networks.\n2\nI-th block\nLocal\nLSTM\nx1\ni\nChar\nCNN\nx2\ni x3\ni xT\ni\nh1\ni h2\ni h3\ni hT\ni\nx1\ni+1\nChar\nCNN\nh1\ni+1 h2\ni+1 h3\ni+1 hT\ni+1\ni+1 block\nx2\ni+1 x3\ni+1 xT\ni+1\nLocal\nLSTM\nLocal\nLSTM\nLocal\nLSTM\nPool + Project Pool + Project\nGlobal\nLSTM\npi pi+1\n(a) BI-HLSTM\nI-th block\nLocal\nLSTM\nx1\ni\nChar\nCNN\nx2\ni x3\ni xT\ni\nh1\ni h2\ni h3\ni hT\ni\nx1\ni+1\nChar\nCNN\nh1\ni+1 h2\ni+1 h3\ni+1 hT\ni+1\ni+1 block\nx2\ni+1 x3\ni+1 xT\ni+1\nLocal\nLSTM\nLocal\nLSTM\nLocal\nLSTM\nPool + Project Pool + Project\nGlobal\nLSTM\npi pi+1 (b) left-to-right HLSTM\nFigure 1: Document-level representations used in this paper. (a) Bi-HLSTM: the bidirectional hi-\nerarchical representation, which does not satisfy the uni-directionality constraint. (b) left-to-right\nHLSTM (L-TO-R-HLSTM): a uni-directional version of BI-HLSTM.\n3 H IERARCHICAL DOCUMENT -LEVEL REPRESENTATIONS\nAs our goal is to learn ﬁxed-length contextualized representations of text segments to be used in\ndocument-level tasks, we choose hierarchical neural architectures that build representations of to-\nkens and text segments in context. Here we describe the speciﬁc architectures studied in this paper.\nThe novel contribution of our work – pre-training such representations from unlabeled text, will be\ndetailed in Section 4.\nWe choose LSTM-based architectures with a standard hierarchical structure that has been useful for\ncapturing long-term context in document-level tasks (Serban et al., 2016; Koshorek et al., 2018). We\nexperiment with two related document-level representations. The ﬁrst one, called BI-HLSTM, is a\nstandard hierarchical bidirectional LSTM encoder.BI-HLSTM does not satisfy the uni-directionality\nconstraint and cannot be pre-trained using a left-to-right language model likelihood objective func-\ntion. Our second document-level representation is L+R-HLSTM, which consists of two concate-\nnated uni-directional versions of BI-HLSTM, and can be seen as a hierarchical document-level ex-\ntension of ELMo (Peters et al., 2018). Both document-level representations view documents as\nsequences of text blocks, each consisting of a sequence of tokens. The text blocks can be sentences,\nparagraphs, or sections, depending on the task. Here we experiment with sentences and paragraphs\nas text blocks.\nThe BI-HLSTM architecture fuses left and right contextual information more effectively and directly\ncorresponds to state-of-the-art architectures used for multiple NLP tasks.\nBI-HLSTM BI-HLSTM has a two-level hierarchy. Every block is ﬁrst independently processed\nby a bidirectional LSTM (Bi-LSTM) to encode the local context. The contextual representations of\nthe words in each block are then pooled and projected to a single vector to represent the block. The\nvector representations of the sequence of blocks in the document are then processed by a global Bi-\nLSTM, resulting in block representations contextualized by the entire document. The overall design\nis shown in Figure 1a, and the encoding procedure is formalized next.\nA document Dconsists of K blocks D = {U1,U2,...U K}, and each block consists of a sequence\nof tokens where Ui = {xi\n1,xi\n2,...x i\nT}.1 For a block Ui, we start with character representations of\nthe words, where ¯xi\nt = CNNchar(xi\nt).Then the character-based word representations are processed\nby the local Bi-LSTM, where {hi\n1,...,h i\nT}= BI-LSTM Local({¯xi\n1,..., ¯xi\nT}).\n1In our experiments, the value of T depends on i. For simplicity, we slightly abuse notation and use the\nsame T for all blocks.\n3\nBefore generating the globally-contextualized representations, the encoder generates representations\nof all blocks by pooling the locally-contextualized word representations generated from the local\nencoder. The block representations are obtained using both max and average-pooling on the word\nrepresentations, followed by a feed-forward transformation:\nci = FFNN (POOL max({hi\n1,...,h i\nT}); POOL avg({hi\n1,...,h i\nT})).\nThe resulting local block representations ci are then processed by the global Bi-LSTM to generate\ndocument-contextualized block representationspi: {p1,...,p K}= BI-LSTM Global({c1,...,c K}).\nIn downstream tasks, this document representation is used by inputting the text block contextual\nvector representations {pi}to light-weight task-speciﬁc networks.\nL+R-HLSTM As previously mentioned, when using left-to-right language models to pre-train\nrepresentations, the representation of the i-th word can only depend on the representations of the\nprevious words (i−1) words. Similarly, the representation of thej-th text block can only depend on\nthe representations of the previous text blocks. To design a document-level representation similar to\nthe one deﬁned by theBI-HLSTM but that can still be trained using a uni-directional language model\ndecomposition, we replace all of the bi-directional LSTMs in BI-HLSTM with either left-to-right or\nright-to-left LSTMs. The left-to-right version of the resulting hierarchical encoder is presented in\nFigure 1b.\nGiven a document D = {U1,U2,...U K}, let the encoding results of the left-to-right HLSTM be\ndenoted as {− →p1,..., − →pK}= L-TO-R-HLSTM (D; θl).We use another right-to-left hierarchical\nencoder to produce representations: {← −p1,..., ← −pK}= R-TO-L-HLSTM (D; θr). We then con-\ncatenate the ﬁnal representations of the two encoders: ˆpi =\n[− →pi\n← −pi\n]\n.\nWe term the resulting document-level encoderL+R-HLSTM. Note that the L+R-HLSTM combines\nthe left and right information only at the top level, and there is no interaction between the two uni-\ndirectional contextualizers. While at the lowest layer, theBI-HLSTM similarly concatenates left and\nright information, the second local BI-LSTM layer can fuse the two directions. Similarly, the left\nand right contexts at the text segment level are fused at the second global BI-LSTM layer.\nIn principle, both BI-HLSTM and L+R-HLSTM can capture long distance dependencies via their\nhierarchical structure, but it is unclear whether such dependencies can be effectively learned from\nlimited amounts of supervised data labeled for down-stream tasks of interest. In the next section,\nwe discuss how such document encoders can be trained to provide rich text segment and token\nrepresentations using large unlabeled document collections.\n4 P RE-TRAINING HIERARCHICAL DOCUMENT -LEVEL REPRESENTATIONS\nHere we propose language model-based methods for pre-training hierarchical document-level rep-\nresentations. In order to train the hierarchical representations with language models, our main idea\nis conceptually simple: we combine the sentence representations contextualized with respect to the\nwhole document and the token representations with respect to the sentence to perform missing word\npredictions. Intuitively, from the loss of word prediction, both the contextual sentence and token\nrepresentations will be updated.\nNote that there are interactions between the contextual sentence and token representations, as part\nof the contextual sentence representations are constructed from token representations. Therefore,\nthe language model pretraining algorithms need to be designed carefully. In Section 4.1, we present\nways to pre-train uni-directional hierchical document-level representations. In Section 4.2, we de-\nscribe a language prediction model that can pre-train bi-directional hierarchical representations.\n4.1 U NI-DIRECTIONAL REPRESENTATIONS\nStandard language model pre-training (L+R-LM) can be used to pre-train the uni-directional token-\nlevel representations in local text block context. Starting from Equation 1, and using a standard\nLSTM language model, we can deﬁne the predictive probability of the next word given prior text\nblock-internal context as: Plocal\nL-LM(xi\nt|xi\nt−1,...x i\n1) = P(xi\nt|− →hi\nt−1), and train the representations us-\n4\ning this language model. As in Peters et al. (2018), this only pre-trains token representations in local\ncontext and does not provide a way to derive contextualized single vector text block representations.\nHere we propose to use hierarchical language models to pre-train uni-directional hierarchical\ndocument-level representations. We refer to this algorithm asL+R-LMglobal. When pre-training with\nhierarchical language models, we treat the uni-directional document-level representation in Figure\n1b as the representation used by a hierarchical language model and use entire unlabeled documents\nto pre-train it.\nThe hierarchical language model predicts the next word in a document using the contextual encoder\nrepresentations. The left-to-right hierarchical model can use the contextual information from pre-\nvious tokens in the same document to predict the next word. The previous tokens with respect to a\ncurrent token can be categorized into two sets: the preceding tokens in the same text block, and all\ntokens in the preceding text blocks. More speciﬁcally, the predictive probability for the next token\nis deﬁned as: Pglobal\nL-LM (xi\nt|xi\nt−1,...x i\n1,Ui−1,...,U 1) = P(xi\nt|− →hi\nt−1,− →pi−1).\nTherefore, the objective function for L+R-LMglobal is as follows:\nmin\nθl,θr\n∑\nt,i\nlog P(xi\nt|− →hi\nt−1,− →pi−1,θl) + logP(xi\nt|− →hi\nt+1,← −pi+1,θr),\nwhere we use a feed-forward network to combine the information from (− →hi\nt−1,− →pi−1) and← −hi\nt+1,← −pi+1, respectively. These feed-forward networks are not used in down-stream tasks.\n4.2 B I-DIRECTIONAL REPRESENTATIONS\nWe have shown a method to pre-train uni-directional encoders with hierarchical language mod-\nels. However, it is still not clear how we can pre-train representations that do not satisfy the\nuni-directionality constraint from unlabeled text. Here we adapt masked language models ( MASK -\nLM) (Devlin et al., 2018) to the task of pre-training hierarchical document representations. Masked\nlanguage models alleviate the architectural constraints imposed by standard language model pre-\ntraining. They can be seen as a special form of denoising auto-encoders (Vincent et al., 2008),\nwhere the decoder network has minimal parameters. Unlike BERT (Devlin et al., 2018), the only\ntype of noise we use is word masking, and the decoder only makes predictions for positions with\nnoise (from the terminology in Vincent et al. (2008), full emphasis is on reconstructing the corrupted\ndimensions).\nObama was born in 1961 in Honolulu , Hawaii , two years after the \nterritory was admitted to the Union as the 50th state . Raised largely in \nHawaii , he also spent one year of his childhood in Washington state \nand four years in Indonesia. \nObama was ▭ in 1961 in Honolulu , Hawaii , ▭  ▭  after the territory was \nadmitted to the ▭  as the 50th ▭ . Raised largely in ▭ , he also spent one \nyear of his ▭  in Washington state and four years in Indonesia . \nMasked Document\nEncoder\nv1  v2  v3 v4 .. v11 v12 ….v20… v24... v33  …..v47                                                                                              \n        Masks \n(3, 11, 12, 20, 24, 29, 33)\n born     two     years       Union    state     Hawaii       childhood\n \n     Document\n        Encode\nPredict Masked Words\nFigure 2: The general procedure of using masked\nlanguage model to train encoders. Note that\nmasks are randomly generated. See text for more\ndetails.\nFigure 2 demonstrates the method of training\nrepresentations with MASK -LM. For a given\ndocument, we ﬁrst generate a set of random\nindices and mask out (hide) the words at the\ncorresponding positions in the document. Af-\nter a document has been masked, we provide\nit as input to a document-level encoder to pro-\nduce contextual word and text block represen-\ntations. These contextual representations are\nused to predict the words at the masked posi-\ntions.\nMore formally, let Z denote a set of in-\ndices for the masked words. Let ζ(D; Z) =\n{ζ(x1\n1),ζ(x1\n2),...,ζ (xi\nt),...ζ (xK\nT )} indicate\nthe masked word sequence, where:\nζ(xi\nt) =\n{ ,∀(i,t) ∈Z,\nxi\nt,Otherwise.\nNote that is a special symbol to indicate that\nthe word is masked out (hidden). Given that Z is generated randomly, masked language models\n5\nessentially optimize the following objective function where θrepresent the parameters of the repre-\nsentation:2 maxθ\n(\nEZ\n[∑\n(i,t)∈Z log P(xi\nt|ζ(D; Z),θ)\n])\n.\nUnlike the auto-encoder method of Hill et al. (2016), in MASK -LM we assume the probabilities\nof all masked words in a sequence can be predicted independently and do not employ a separate\nauto-regressive decoder network.\nThere are two ways to apply MASK -LM to pre-train parts of the BI-HLSTM. Analogously to the\nlocal setting of L+R-LM, we could pre-train with MASK -LM using local context. We deﬁne the\nprobabilities of masked words to depend only on the locally contextualized token representations,\nand apply MASK -LM to only pre-train the local bi-LSTM by using: Plocal\nMASK -LM(xi\nt|ζ(D; Z); θ) =\nP(xi\nt|hi\nt; θ), where the hi\nt is the word-level contextual representation for masked word xi\nt. Note\nthat if (i,t) ∈Z, ζ(xi\nt) = . Therefore, hi\nt, which is generated by the bi-directional local LSTM\nencoder, needs to carry the contextual information in order to recover the masked word xi\nt.\nSimilarly, in the MASK -LMglobal setting, the predictive probabilities are deﬁned as:\nPglobal\nMASK -LM(xi\nt|ζ(D; Z); θ) = P(xi\nt|hi\nt,pi; θ), where we use another feed-forward network to com-\nbine the information from hi\nt and pi.\n5 D OWN -STREAM TASKS\nWe perform experiments on three document-level downstream tasks, which require predictions at\nthe level of text blocks in full documents. The predictions are made using light-weight task speciﬁc\nnetworks which take as input single-vector document-contextualized text block representations pi,\ngenerated by the hierarchical document encoders described in Section 3. We brieﬂy deﬁne the\ndownstream tasks and the task-speciﬁc architectures used. We provide additional details in the\nExperiments section.\nDocument SegmentationIn the document segmentation task, we take as input a document repre-\nsented as a sequence of sentences. The goal is to predict, for each sentence, whether it is the last\nsentence of a group of sentences on the same topic, thereby segmenting the input document in topi-\ncal segments. We use a feed-forward network with one hidden layer with RELU activations on top\nof sentence representations pi to deﬁne scores of the task speciﬁc labels (segment boundary or not).\nAnswer Passage RetrievalGiven a document and a question, the task of answer passage retrieval\nis to select passages from the document that answer the question. While most QA systems focus\non predicting short answer spans given a relevant answer passage, answer passage retrieval is a\nprerequisite task for open-domain question answering. While there are a multitude of architectures\npossible for this task, we propose a modular approach that enables extremely fast answer passage\nretrieval during serving time. Our model works as follows. The given document is encoded and\nits contextual passage representations pi are generated. The questions are treated as documents\nconsisting of a single sentence and are similarly encoded in representations q. Inspired by Peters\net al. (2018), instead of using the representationspidirectly, we use a linear combination of the layers\nof pi with positive weights summing to 1: p′\ni = ∑\nlslpi,l, where sl is a task-speciﬁc learned scalar\nweight for the l-th layer, and pi,l represents the l-th layer of the representations of the i-th block.\nTo score a passage-question pair, we use the following architecture using a learned task-speciﬁc\nnetwork combining the following features: s0(pi,q) = ⟨p′\ni,q′⟩is a dot product of the passage and\nquestion representations, s1,...,s 5 are ﬁve light-weight features such as paragraph position and\ntf-idf borrowed from Clark & Gardner (2018). These features are sent to a feed-forward network\nwith one hidden layer with RELU activations to generate the ﬁnal score.\nExtractive Summarization Given a document, the task of extractive summarization is to select\nrepresentative sentences to form a summary for the whole document (Kupiec et al., 1995). Our\nsummarization model is quite similar to our answer passage selection model with two key differ-\nences. First, we add another randomly initialized LSTM on top of all sentence representations to\nform reﬁned sentence representations. We found that the extra layer generally improves results.\n2Note that the model does not learn a probability distribution over possible texts and is thus not formally a\nlanguage model. Nevertheless, for ease of exposition and in line with prior work on learning representations\nfrom unlabeled text (Collobert & Weston, 2008) we refer to this model as a language model.\n6\nSecond, we perform max-pooling over the contextual representations of all sentences p′\ni to form a\nﬁxed-length vector representation dfor the entire document. The representationdis then treated like\nthe ”question” representation in the passage selection task. Third, instead of using a dot-product as\nthe scoring function, for each sentence, we concatenate dand p′\ni to form a combined vector which\nis sent to a feed-forward network with one hidden layer with RELU activations to generate the ﬁnal\nscore. As for the Answer Passage Retrieval task, we add several task-speciﬁc features such as the\nposition of the sentence in the document and the number of words in the sentence.\n6 E XPERIMENTS\nThe main goal of our experiments is to examine the value of unsupervised pre-training of hierarchical\ndocument-level representations, focusing on the novel aspects of our methods. We ﬁrst present\nthe experimental settings and pre-training details in Section 6.1. We then present and analyze the\nexperimental results on the three tasks.\nAmong the pre-training methods, L+R-LMglobal and MASK -LMglobal are proposed in this paper. The\nlocal version of L+R-LM is similar to ELMo (Peters et al., 2017) and MASK -LM is similar to\nthe method in Devlin et al. (2018), but the effectiveness of such methods when used to initialize\ndocument-level hierarchical representations has not been studied before.\n6.1 S ETTINGS\nWe use standard LSTM cells for our sequence encoders. For word embeddings, we use character-\nbased convolutions similarly to (Peters et al., 2018), and project the embeddings to 512 dimensions.\nOur local encoder is a 2-layer LSTM with 1024 hidden units in each direction. Our global encoder\nis another 2-layer LSTM with 1024 hidden units.\nSince L+R-LM requires the representation to satisfy the uni-directionality constraint, while there are\nno directionality constraints on the representations when pre-training with MASK -LM, in the exper-\niments we always pair each encoder with its natural pre-training method: we pre-train BI-HLSTM\nwith MASK -LM, and L+R-HLSTM with L+R-LM. For each of the two encoders, we compare no\npre-training on unlabeled data, versus pre-training at the local sentence level only, versus pre-training\nat the global document level.\nIf we initialize theBI-HLSTM representation with locally pre-trainedMASK -LM, this means that we\ninitialize the local Bi-LSTM and learn the global Bi-LSTM only based on the labeled down-stream\ntask data. If we initialize both the local and global Bi-LSTMs of BI-HLSTM with MASK -LMglobal\npre-trained models, only the task-speciﬁc feed-forward network is randomly initialized, and all\nparameters are ﬁne-tuned with the labeled set. When comparing hierarchical (global) and non-\nhierarchical (local) pretraining methods, we use thesame document-level representation architecture\nfor the downstream tasks, pre-train on the same amount of unlabeled text, and then ﬁne-tune with\nthe same task-speciﬁc network architectures.\nIn all tasks, we also include comparisons to skip-thought vectors (Kiros et al., 2015), where the\nmodel is learned to construct sentence embeddings.3 Additionally, we compare to ELMOpool, which\nforms a representation for a text block by pooling the token-level representations generated by the\nmodel pretrained in (Peters et al., 2018). Perone et al. (2018) have shown that ELM Opool produces\nstrong sentence representations and outperforms many other sentence representation models includ-\ning skip-thought vectors.4 At a high level, the contextualized token representations obtained by our\nlocal L+R-LM implementation are similar to the ones obtained by ELMo. However, ELMo was\ntrained on a larger dataset and uses higher-dimensional hidden vector representations. Following\nprior work, skip-thought and ELMo vectors are used as ﬁxed features and are not ﬁne-tuned.\nTo pre-train the hierarchical representations, we use documents in Wikipedia and ﬁlter out ones\nwith fewer than three sections or less than 800 tokens. We sample from the ﬁltered document set\nand form a collection containing approximately 320k documents with about 0.9 billion tokens. We\nselect the most frequent 250k words as the vocabulary for all models. We limit documents to at most\n3We use the bi-directional model for the skip-thought vectors.\n4We use the ”all layers” and ”original” settings according to (Perone et al., 2018).\n7\nMASK -LM L+R-LM\nPre-training BI-HLSTM L+R-HLSTM\nNo 42.0 (0.0) 41.7 (0.0)\nLocal 50.6 (8.6) 48.4 (6.7)\nGlobal 51.8 (9.8) 54.9 (13.2)\nLSTM+ELMopool 44.6\nLSTM+Skip-Thought 46.0\n(a) Document Segmentation\nMASK -LM L+R-LM\nPre-training BI-HLSTM L+R-HLSTM\nNo 77.24 (0.0) 77.20 (0.0)\nLocal 79.17 (1.9) 78.36 (1.2)\nGlobal 79.92 (2.7) 79.57 (2.4)\n(Clark & Gardner, 2018) 73.31\n(b) Answer Passage Retrieval\nTable 1: Downstream task performance for the segmentation and answer passage retrieval tasks.\nThe improvements over corresponding baselines are indicated in brackets. (a) F1 on document seg-\nmentation. (b) Precision at one (P@1) for answer passage retrieval in TriviaQA-Wiki. We compare\nour models to the answer passage retrieval module developed by Clark & Gardner (2018).\n75 sentences and limit each sentence length to 75 tokens. Sentences are used as text blocks for pre-\ntraining and pre-trained models are applied to both sentence-block and paragraph-block documents\nin the down-stream tasks. To capture the full hierarchical information, we did not use truncated\nback-propagation-through-time (tBPTT) (Rumelhart et al., 1985), but used full back propagation\non documents with up to 5k tokens. For MASK -LM, we randomly mask 20% of the words in the\ndocuments for training. All of the down-stream task experiments are performed using the same set\nof pre-trained models.\n6.2 D OCUMENT SEGMENTATION\nWe create labeled data for the segmentation task by taking Wikipedia articles and forming the labels\nusing the section information. Given a sentence in a Wikipedia article, the label of the sentence\nis positive if it is the last sentence of a Wikipedia section. To ensure fair comparisons, the section\nboundary information is never used during pre-training. We select 5k documents each for training,\ndevelopment, and test sets. In our dataset, only 5% of the sentences are positively labeled; we use F1\nscore as the evaluation metric due to this imbalanced class distribution. All reported experimental\nresults for the segmentation task are averages of ﬁve different training runs on the task-speciﬁc\nlabeled data.\nResults The experimental results for the segmentation experiments are in Table 1a. Our base-\nline system that does not use any pretraining of representations is similar to the one proposed\nin Koshorek et al. (2018) with the difference that our system uses character information to gen-\nerate the word embeddings. Note that pretaining results in substantial improvements over the base-\nline systems in all settings. We next compare pre-training with hierarchical language models (the\nglobal setting) versus pre-training with non-hierarchical language models (the local setting). Global\npre-training of the hierarchical document-level representations improves upon local pre-training of\nthe sentence-contextualized token representations. In the case of the L+R-HLSTM encoder paired\nwith L+R-LMglobal, the improvement is over 6% F1. For this task, we found that L+R-LMglobal is\nmuch better than MASK -LMglobal. We hypothesize that by predicting words in the next sentence,\nthe L+R-LMglobal model is more sensitive to the topical changes between sentences. The Table also\nshows results of experiments with a setting where we applied a Bi-LSTM on top of the ELM Opool\nand skip-thought vectors; as seen both methods under-perform both our local and global pre-training\napproaches. In addition to not providing a method to pre-train the higher-level LSTMs, these meth-\nods do not ﬁne-tune the local paragraph representations on downstream task data.\n6.3 A NSWER PASSAGE RETRIEVAL\nWe apply the document-level encoders to the task of answer passage retrieval in the next set of\nexperiments. To evaluate the impact of pre-training a document-level encoder for this task, we use\nthe Wikipedia subset of the TriviaQA dataset (Joshi et al., 2017), a large scale distantly supervised\nquestion answering dataset. While TriviaQA was originally designed for short answer detection,\n8\nMASK -LM L+R-LM\nPre-training BI-HLSTM L+R-HLSTM\nNo 37.2 37.1\nLocal 37.4 37.3\nGlobal 37.6 37.4\n(a) Evaluating the impact of different pretraining\nalgorithms. Rouge-L only is shown for brevity\nhere.\nPre-trained Vectors P. Retrieval (P@1)\nMASK-LMglobal 66.2\nL+R-LMglobal 63.3\nL+R-LMlocal pool 53.8\nELMOpool 56.8\nSkip-Thought 37.2\n(b) To inspect the quality of the pretrained rep-\nresentations, we show a zero-shot setting for the\nanswer passage retrieval task.\nTable 2: Analysis of the impact of different pretraining methods on document summarization and\nzero-shot answer passage retrieval.\nModels R-1 R-2 R-L\nLEAD (See et al., 2017) 39.6 17.7 36.2\nNeuralSum (Cheng & Lapata, 2016) 35.5 14.7 32.2\nSummaRuNNer (Nallapati et al., 2017) 39.6 16.2 35.3\nREFRESH (Narayan et al., 2018) 40.0 18.2 36.6\nBottom-up Summarization (Gehrmann et al., 2018) [Extractive] 40.7 18.0 37.0\nMASK-LMglobal(Ours) 41.2 19.1 37.6\nLSTM+Elmopool 41.0 18.9 37.4\nLSTM+Skip-Thought 40.8 18.7 37.2\nNeuSum (Zhou et al., 2018) 41.6 19.0 38.0\nTable 3: Comparative evaluation of our summarization models with respect to other extractive sum-\nmarization systems. ROUGE-1 (R-1), ROUGE-2 (R-2), and ROUGE-L (R-L) F1 scores are re-\nported. The ﬁrst part of the table indicates system results with non-autoregressive sentence selection\nmodels, including prior work and our best proposed approach. The second part of the table shows\nthe performance of (non-autoregressive) models using pre-trained sentence embeddings input to a\ndocument-level Bi-LSTM. NeuSum (Zhou et al., 2018) is included in the third part as it is speciﬁ-\ncally trained to score combinations of sentences with auto-regressive selection models.\nwe use it to evaluate methods on the answer-passage selection task, as it requires retrieving relevant\npassages from one or more full documents as an important sub-task. 5\nWe use the paragraph breaker implementation used in Clark & Gardner (2018). As in other work\non TriviaQA, we set 400 to be the maximum number of tokens in a paragraph during training. We\nalso use 400 tokens in testing. We report the top-1 passage selection accuracy on the development\nset as our main metric. As the TriviaQA test data labels are not publicly available, we cannot obtain\npassage accuracy test metrics and thus do not report these.\nThe passage retrieval results are in Table 1b. Note that pre-training leads to substantial improve-\nments in all settings. For both MASK -LM and L+R-LM, global pre-training of the full hierarchical\nrepresentations improves upon local pre-training of the lower-level contextual token representations.\nOur models improve upon the passage selection model proposed by Clark & Gardner (2018) by more\nthan 6% absolute. The latter work focuses on evaluation of their end-to-end short answer selection\nsystem and does not report passage retrieval performance for TriviaQA-Wiki. We measured the\nperformance using the authors’ codebase6.\nAnalysis Our answer passage retrieval model uses a dot-product between the question and passage\nrepresentations to score passages. If we restrict our models to not use additional features apart from\nthis dot product of the question and passage representations, we can derive task-speciﬁc models\n5We did not study the TriviaQA Web subset as the ﬁrst paragraph baseline has a strong accuracy of 83% for\nit, reducing the importance of strong passage selection systems in this dataset.\n6https://github.com/allenai/document-qa\n9\nin a ”zero-shot” setting, where we do not use labeled data from TriviaQA to ﬁne-tune pre-trained\nrepresentations. We use this zero-shot setting to further investigate whether the pre-trained repre-\nsentations have learned to assess the semantic similarity between questions and passages based on\nunlabeled text. The results are presented in Table 2b. In this setting, MASK -LMglobal achieves 66.2,\noutperforming L+R-LMglobal, ELM Opool and skip-thought vectors. The max-pooled version of our\nlocal L+R-LM method under-performs ELM Opool, which is explained by the difference in size of\nthe pre-trained representations.\n6.4 E XTRACTIVE DOCUMENT SUMMARIZATION\nWe use the CNN/DailyMail dataset (Hermann et al., 2015) to evaluate our summarization model.\nWe follow the standard data pre-processing and evaluation settings for extractive summarization\nfrom (See et al., 2017), and use ROUGE (Lin, 2004) as the evaluation metric. In the training data,\nthere are only gold summaries without sentence labels. To generate labels for sentence selection,\nwe follow Nallapati et al. (2017) to create extractive oracle summaries by starting with an empty\nsummary and greedily adding one sentence at a time (up to a total of three sentences) to optimize\nthe ROUGE score between the selected sentences and the gold summarizes. Our models are trained\nto choose these selected sentences. We add three loss functions during training for selecting the ﬁrst,\nsecond, and third sentence, respectively. The ROUGE score gain attributed to each sentence is used\nto weight the loss functions. During testing, we simply select the top three sentences according to\nour models.\nThe results of our summarization system are presented in Table 3. Compared to other extractive\nsummarization systems, our model improves upon the prior work of Narayan et al. (2018) by 0.9 in\nROUGE-1 and 0.7 in ROUGE-L, without using the sophisticated reinforcement learning techniques\nemployed by that work. Our model is competitive to NeuSum (Zhou et al., 2018) for ROUGE-2, but\nhas worse ROUGE-1 and ROUGE-L. We hypothesize that this is due to the fact that NeuSum (Zhou\net al., 2018) employed an auto-regressive sentence selection model. Recently, abstractive summa-\nrization systems were able to outperform extractive ones (Paulus et al., 2017; Gehrmann et al., 2018).\nWe expect that the novel aspects of our work can also bring improvements in that non-extractive set-\nting and are complementary to the use of auto-regressive sentence selectors. We also compared our\nmodels to models using ELM Opool and skip-thought vectors as ﬁxed features input to a document-\nlevel Bi-LSTM. As seen these methods are also competitive, but are not as strong as our global\nhierarchical pre-training approach.\nThe impact of pretraining is analyzed in Table 2a. For brevity, we only include ROUGE-L scores,\nbut ROUGE-1 and ROUGE-2 show similar trends. As for answer passage retrieval, MASK -LMglobal\nis the best overall pretraining algorithm.\n7 R ELATED WORK\nPrior work in supervised learning of neural representations for full documents has shown the ef-\nfectiveness of hierarchical neural structures, which contain representations of both the sentences\nand their component words in a two-level hierarchy. In the lower hierarchy level, the contextual\nword representation is formed with respect to the sentence. In the higher level, the contextual sen-\ntence representation is formed with respect to the document. Such structures enable the models\nto integrate long-distance context from the documents and have been used for labeling sentence\nsentiment (Ruder et al., 2016), document summarization (Cheng & Lapata, 2016), text segmenta-\ntion (Koshorek et al., 2018), and text classiﬁcation (Yang et al., 2016),inter alia. These hierarchical\nneural representations have been largely learned based on task-speciﬁc labeled data, posing a chal-\nlenge for applications with a limited number of annotated examples.\nUnsupervised pretraining for hierarchical document representations has received relatively little at-\ntention despite the recent success on language model pre-training (Peters et al., 2018; Radford et al.,\n2018). Prior work has shown how to use unlabeled text to pre-train representations of individ-\nual words, e.g. (Mikolov et al., 2013), or ﬂat (relatively short) sequences of words, where each\nword representation is contextualized with respect to the sequence (Peters et al., 2017; 2018; Salant\n& Berant, 2018; Radford et al., 2018). Similarly, ﬁxed-length vector representations of full sen-\ntences/paragraphs have also been pre-trained from unlabeled text (Le & Mikolov, 2014; Kiros et al.,\n10\n2015; Dai & Le, 2015; Logeswaran & Lee, 2018), where the sentence/paragraph representations\nare generated based on the content of the sentence/paragraph alone, ignoring other sentences in the\ndocument.7\nLi et al. (2015) showed how to train hierarchical document representations from unlabeled text using\ndocument auto-encoders but did not use such representations in extrinsic downstream document-\nlevel tasks. To the best of our knowledge, no prior work has transferred hierarchical unsupervised\ndocument representations to downstream tasks, and evaluated the impact of pretraining local and\nglobal document-level contextualizers. In addition, bidirectional hierarchical representations have\nnot been pretrained without a sophisticated decoder in an auto-encoder framework before.\n8 C ONCLUSION\nIn this paper, we proposed methods for pre-training hierarchical document representations, includ-\ning contextual token and sentence/paragraph representations, integrating context from full docu-\nments. We demonstrated the impact of pre-training such representations on three document-level\ndownstream tasks: text segmentation, passage retrieval for document-level question answering, and\nextractive summarization.\nREFERENCES\nJianpeng Cheng and Mirella Lapata. Neural summarization by extracting sentences and words. In\nProceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), pp. 484–494, Berlin, Germany, August 2016. Association for Computational\nLinguistics. URL http://www.aclweb.org/anthology/P16-1046.\nChristopher Clark and Matt Gardner. Simple and effective multi-paragraph reading comprehension.\nIn ACL, 2018.\nRonan Collobert and Jason Weston. A uniﬁed architecture for natural language processing: Deep\nneural networks with multitask learning. In Proceedings of the 25th International Conference on\nMachine Learning, ICML ’08, 2008.\nAndrew M Dai and Quoc V Le. Semi-supervised sequence learning. In Advances in neural infor-\nmation processing systems, pp. 3079–3087, 2015.\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep\nbidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\nSebastian Gehrmann, Yuntian Deng, and Alexander M Rush. Bottom-up abstractive summarization.\narXiv preprint arXiv:1808.10792, 2018.\nKarl Moritz Hermann, Tomas Kocisky, Edward Grefenstette, Lasse Espeholt, Will Kay, Mustafa\nSuleyman, and Phil Blunsom. Teaching machines to read and comprehend. In Advances in\nNeural Information Processing Systems, pp. 1693–1701, 2015.\nFelix Hill, Kyunghyun Cho, and Anna Korhonen. Learning distributed representations of sentences\nfrom unlabelled data. In Proceedings of the 2016 Conference of the North American Chapter of\nthe Association for Computational Linguistics: Human Language Technologies. Association for\nComputational Linguistics, 2016.\nSepp Hochreiter and J ¨urgen Schmidhuber. Long short-term memory. Neural computation, 9(8):\n1735–1780, 1997.\nMandar Joshi, Eunsol Choi, Daniel S Weld, and Luke Zettlemoyer. TriviaQa: A large scale distantly\nsupervised challenge dataset for reading comprehension. In ACL, 2017.\n7Note that most approaches (e.g. (Kiros et al., 2015)), pretrain the representations by asking a model to\npredict the words in other sentences during training, but the encoders look at sentences in isolation to generate\nrepresentations at inference time.\n11\nRyan Kiros, Yukun Zhu, Ruslan R Salakhutdinov, Richard Zemel, Raquel Urtasun, Antonio Tor-\nralba, and Sanja Fidler. Skip-thought vectors. In Advances in neural information processing\nsystems, pp. 3294–3302, 2015.\nOmri Koshorek, Adir Cohen, Noam Mor, Michael Rotman, and Jonathan Berant. Text segmentation\nas a supervised learning task. In arXiv preprint arXiv:1803.09337, 2018.\nJulian Kupiec, Jan Pedersen, and Francine Chen. A trainable document summarizer. InProceedings\nof the 18th Annual International ACM SIGIR Conference on Research and Development in Infor-\nmation Retrieval, SIGIR ’95, pp. 68–73, New York, NY , USA, 1995. ACM. ISBN 0-89791-714-6.\ndoi: 10.1145/215206.215333. URL http://doi.acm.org/10.1145/215206.215333.\nQuoc Le and Tomas Mikolov. Distributed representations of sentences and documents. In Interna-\ntional Conference on Machine Learning, pp. 1188–1196, 2014.\nYann LeCun and Yoshua Bengio. The handbook of brain theory and neural networks. chapter Con-\nvolutional Networks for Images, Speech, and Time Series, pp. 255–258. MIT Press, Cambridge,\nMA, USA, 1998.\nJiwei Li, Thang Luong, and Dan Jurafsky. A hierarchical neural autoencoder for paragraphs and\ndocuments. In Proceedings of the 53rd Annual Meeting of the Association for Computational\nLinguistics and the 7th International Joint Conference on Natural Language Processing (Vol-\nume 1: Long Papers), pp. 1106–1115, Beijing, China, July 2015. Association for Computational\nLinguistics. URL http://www.aclweb.org/anthology/P15-1107.\nChin-Yew Lin. Rouge: A package for automatic evaluation of summaries. Text Summarization\nBranches Out, 2004.\nLajanugen Logeswaran and Honglak Lee. An efﬁcient framework for learning sentence representa-\ntions. arXiv preprint arXiv:1803.02893, 2018.\nTomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. Distributed represen-\ntations of words and phrases and their compositionality. In Advances in Neural Information\nProcessing Systems 26, pp. 3111–3119. Curran Associates, Inc., 2013.\nRamesh Nallapati, Feifei Zhai, and Bowen Zhou. Summarunner: A recurrent neural network based\nsequence model for extractive summarization of documents. In AAAI, pp. 3075–3081. AAAI\nPress, 2017.\nShashi Narayan, Shay B Cohen, and Mirella Lapata. Ranking sentences for extractive summariza-\ntion with reinforcement learning. In NAACL, 2018.\nRomain Paulus, Caiming Xiong, and Richard Socher. A deep reinforced model for abstractive\nsummarization. arXiv preprint arXiv:1705.04304, 2017.\nChristian S Perone, Roberto Silveira, and Thomas S Paula. Evaluation of sentence embeddings in\ndownstream and linguistic probing tasks. arXiv preprint arXiv:1806.06259, 2018.\nMatthew Peters, Waleed Ammar, Chandra Bhagavatula, and Russell Power. Semi-supervised se-\nquence tagging with bidirectional language models. In ACL, 2017.\nMatthew Peters, Mark Neumann, Mohit Iyyer, Matt Gardner, Christopher Clark, Kenton Lee, and\nLuke Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.\nAlec Radford, Karthik Narasimhan, Tim Salimans, and Ilya Sutskever. Improving language under-\nstanding with unsupervised learning. Technical report, OpenAI, 2018.\nSebastian Ruder, Parsa Ghaffari, and John G Breslin. A hierarchical model of reviews for aspect-\nbased sentiment analysis. arXiv preprint arXiv:1609.02745, 2016.\nDavid E Rumelhart, Geoffrey E Hinton, and Ronald J Williams. Learning internal representations\nby error propagation. Technical report, California Univ San Diego La Jolla Inst for Cognitive\nScience, 1985.\n12\nS. Salant and J. Berant. Contextualized word representations for reading comprehension. In North\nAmerican Association for Computational Linguistics (NAACL), 2018.\nAbigail See, Peter J Liu, and Christopher D Manning. Get to the point: Summarization with pointer-\ngenerator networks. In ACL, 2017.\nIulian Vlad Serban, Alessandro Sordoni, Yoshua Bengio, Aaron C Courville, and Joelle Pineau.\nBuilding end-to-end dialogue systems using generative hierarchical neural network models. In\nAAAI, volume 16, pp. 3776–3784, 2016.\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,\nLukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-\nmation Processing Systems, pp. 6000–6010, 2017.\nPascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and\ncomposing robust features with denoising autoencoders. In Proceedings of the 25th international\nconference on Machine learning, pp. 1096–1103. ACM, 2008.\nDirk Weissenborn, Georg Wiese, and Laura Seiffe. Making neural qa as simple as possible but not\nsimpler. In Proceedings of the 21st Conference on Computational Natural Language Learning\n(CoNLL 2017), pp. 271–280, Vancouver, Canada, August 2017. Association for Computational\nLinguistics.\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V Le, Mohammad Norouzi, Wolfgang Macherey,\nMaxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, et al. Google’s neural machine trans-\nlation system: Bridging the gap between human and machine translation. arXiv preprint\narXiv:1609.08144, 2016.\nZichao Yang, Diyi Yang, Chris Dyer, Xiaodong He, Alexander J. Smola, and Eduard H. Hovy.\nHierarchical attention networks for document classiﬁcation. In HLT-NAACL, 2016.\nQingyu Zhou, Nan Yang, Furu Wei, Shaohan Huang, Ming Zhou, and Tiejun Zhao. Neural document\nsummarization by jointly learning to score and select sentences.arXiv preprint arXiv:1807.02305,\n2018.\n13"
}