{
  "title": "Neural Architecture Search for Parameter-Efficient Fine-tuning of Large Pre-trained Language Models",
  "url": "https://openalex.org/W4385570673",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A2556072012",
      "name": "Neal Lawton",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2110342584",
      "name": "Anoop Kumar",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A3187256100",
      "name": "Govind Thattai",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A2117285942",
      "name": "Aram Galstyan",
      "affiliations": [
        "Amazon (United States)"
      ]
    },
    {
      "id": "https://openalex.org/A1808124004",
      "name": "Greg Ver Steeg",
      "affiliations": [
        "Amazon (United States)"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4294925020",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W3174784402",
    "https://openalex.org/W2707890836",
    "https://openalex.org/W4287122891",
    "https://openalex.org/W2951104886",
    "https://openalex.org/W4287391717",
    "https://openalex.org/W1522301498",
    "https://openalex.org/W2978017171",
    "https://openalex.org/W2896457183",
    "https://openalex.org/W2964303773",
    "https://openalex.org/W4205991051",
    "https://openalex.org/W4286981949",
    "https://openalex.org/W3202099651",
    "https://openalex.org/W3168867926",
    "https://openalex.org/W3103368673",
    "https://openalex.org/W2963211188",
    "https://openalex.org/W4292779060",
    "https://openalex.org/W3174702398",
    "https://openalex.org/W3015233032",
    "https://openalex.org/W4206281850",
    "https://openalex.org/W3099793224",
    "https://openalex.org/W2946794439",
    "https://openalex.org/W3105966348",
    "https://openalex.org/W2970352191",
    "https://openalex.org/W4385573610",
    "https://openalex.org/W3153675281",
    "https://openalex.org/W3098267758",
    "https://openalex.org/W3034457371",
    "https://openalex.org/W3205949070",
    "https://openalex.org/W2805003733",
    "https://openalex.org/W3101498587",
    "https://openalex.org/W3103616906",
    "https://openalex.org/W2923014074",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3176828726",
    "https://openalex.org/W3034199299",
    "https://openalex.org/W2970925270",
    "https://openalex.org/W3174770825"
  ],
  "abstract": "Parameter-efficient tuning (PET) methods fit pre-trained language models (PLMs) to downstream tasks by either computing a small compressed update for a subset of model parameters, or appending and fine-tuning a small number of new model parameters to the pre-trained network. Hand-designed PET architectures from the literature perform well in practice, but have the potential to be improved via automated neural architecture search (NAS). We propose an efficient NAS method for learning PET architectures via structured and unstructured pruning. We present experiments on GLUE demonstrating the effectiveness of our algorithm and discuss how PET architectural design choices affect performance in practice.",
  "full_text": "Findings of the Association for Computational Linguistics: ACL 2023, pages 8506–8515\nJuly 9-14, 2023 ©2023 Association for Computational Linguistics\nNeural Architecture Search for Parameter-Efficient Fine-tuning of Large\nPre-trained Language Models\nNeal Lawton1∗ Anoop Kumar2 Govind Thattai2 Aram Galstyan2 Greg Ver Steeg2\n1Information Sciences Institute 2Amazon Alexa AI\nlawton@isi.edu {anooamzn,thattg,argalsty,gssteeg}@amazon.com\nAbstract\nParameter-efficient tuning (PET) methods fit\npre-trained language models (PLMs) to down-\nstream tasks by either computing a small com-\npressed update for a subset of model param-\neters, or appending and fine-tuning a small\nnumber of new model parameters to the pre-\ntrained network. Hand-designed PET architec-\ntures from the literature perform well in prac-\ntice, but have the potential to be improved via\nautomated neural architecture search (NAS).\nWe propose an efficient NAS method for learn-\ning PET architectures via structured and un-\nstructured pruning. We present experiments on\nGLUE demonstrating the effectiveness of our\nalgorithm and discuss how PET architectural\ndesign choices affect performance in practice.\n1 Introduction\nFine-tuning a large pre-trained language model is\na popular method for solving many downstream\nnatural language processing (NLP) tasks. Full fine-\ntuning involves fine-tuning all parameters of the\nbase PLM, resulting in a fine-tuned copy of the\nmodel. However, full fine-tuning becomes cum-\nbersome when fine-tuning on multiple downstream\ntasks due to the massive size of state-of-the-art lan-\nguage models, which range from the millions (De-\nvlin et al., 2018; Liu et al., 2019) to billions (Brown\net al., 2020) and now trillions (Fedus et al., 2022)\nof parameters. Full fine-tuning also carries a risk\nof catastrophic forgetting (Jang et al., 2021; Chen\net al., 2022), wherein the PLM’s learned useful\nrepresentation of natural language data is forgotten\nduring fine-tuning.\nTo address those problems, recent research\nhas focused on parameter-efficient tuning (PET).\nRather than fine-tuning all parameters of the base\nPLM, PET methods choose a small subset of pa-\nrameters to fine-tune (Zaken et al., 2021; Guo et al.,\n2020), or compute compressed parameter updates\n*Work done while at Amazon Alexa AI\n(Hu et al., 2021; Mahabadi et al., 2021), or append\nand fine-tune a small subset of new parameters\n(Houlsby et al., 2019; Li and Liang, 2021; Ham-\nbardzumyan et al., 2021; He et al., 2021). Each of\nthese methods has their own advantages and dis-\nadvantages, but one question relevant to all these\nmethods is which parts of the network are most ef-\nficient to fine-tune, and what is the most parameter-\nefficient way to fine-tune them?\nHere we answer this question by designing and\napplying a fine-grain NAS method for learning PET\narchitectures. Our method uses a first order approx-\nimation of the loss function and is computationally\nefficient. We compare our approach with several\nhand-designed PET methods and find that the archi-\ntectures learned by our method generally achieve\ncomparable or higher development set performance\non GLUE tasks (Wang et al., 2018) for the same\nnumber of parameters. We conclude by examining\nthe PET architectures learned by our method and\ndiscuss the affect of architecture design choices on\nparameter efficiency.\n2 Related work\nMany different PET methods exist in the literature.\nAdapter networks insert and fine-tune small adapter\nmodules to a base PLM. Rebuffi et al. (2017) in-\ntroduced adapter networks to the visual domain,\nand Houlsby et al. (2019) introduced adapters to\ntransformers. Adapters have been applied to text\ngeneration (Lin et al., 2020), translation (Bapna\net al., 2019), and multi-task learning (Pfeiffer et al.,\n2020c,a). Peters et al. (2019) compare adaptation\nwith full fine-tuning. AdapterHub (Pfeiffer et al.,\n2020b) enables easy sharing of adapter models.\nAdditionally, Mosbach et al. (2020) propose best\npractices for producing strong full fine-tuning base-\nlines.\nPrompt-tuning methods fine-tune a PLM by in-\nserting prompt tokens into the input sequence.\nContinuous prompts (Li and Liang, 2021; Lester\n8506\net al., 2021; Hambardzumyan et al., 2021) or\ndiscrete prompts (Shin et al., 2020) can be\nlearned or engineered (Brown et al., 2020). Gu\net al. (2021) demonstrate the effectiveness of pre-\ntraining prompts for low resource tasks.\nSome methods fine-tune a subset of parameters\n(Zaken et al., 2021; Guo et al., 2020), or compute\ncompressed parameter updates (Hu et al., 2021;\nMahabadi et al., 2021). These methods fine-tune\nthe PLM without increasing test-time inference\nlatency. He et al. (2021) and Mao et al. (2021)\ncombine multiple PET methods.\nBeyond parameter-efficient tuning, NAS has pre-\nviously been used to discover more parameter-\nefficient base language models. Cheong and Daniel\n(2019) use magnitude pruning to reduce the number\nof parameters in BERT. Many efforts at pruning\nBERT have focused on pruning attention heads\nfrom the multi-head attention (MHA) modules\n(Michel et al., 2019; V oita et al., 2019; Li et al.,\n2021). Sajjad et al. (2020) evaluate different ad-\nhoc strategies for shrinking the depth of a BERT\nencoder. So et al. (2019) use an evolutionary NAS\nmethod to learn an improved transformer cell. In\ncontrast to NAS, distillation can be used to com-\npress language models (Sanh et al., 2019; Jiao et al.,\n2019; Sun et al., 2020).\nIn our experiments section, we examine the ar-\nchitectures learned by our algorithm and consider\nwhat they say about which parts of the network\nare most parameter-efficient to fine-tune. Mer-\nchant et al. (2020) explore a similar question, prob-\ning the network activations to understand how the\nnetwork’s representation of natural language data\nchanges during full fine-tuning.\n3 Method\nThe architecture search space we choose for our\nNAS method is based on BitFit (Zaken et al., 2021)\nand LoRA (Hu et al., 2021), two of the most popu-\nlar methods for parameter-efficient fine-tuning in\nthe literature. We consider both structured and\nunstructured variants of each of these, where the\nnon-zero pattern of the learned PET parameters is\nrestricted or unrestricted, respectively. Specifically,\nour search space consists of the following:\n1. Learning an update ∆bfor each vector of bias\nparameters b. In structured bias-tuning, for\neach PLM module, the NAS algorithm must\nchoose whether ∆b = 0 or not. In unstruc-\ntured bias-tuning, for each PLM module, the\nNAS algorithm must choose which compo-\nnents of ∆bshould be zero or non-zero.\n2. Learning a low-rank (LoRA Hu et al., 2021)\nupdate ∆W = UV ⊤for each user-specified\nparameter matrix W. The maximum possi-\nble rank for the update is also user-specified.\nIn structured LoRA, for each parameter ma-\ntrix W, the NAS algorithm must decide what\nthe rank of the update UV ⊤ should be. In\nunstructured LoRA, the NAS algorithm must\ndecide which components of U and V should\nbe non-zero.\nThe collection of updates ∆band ∆W are the\nPET parameters. In this search space, any number\nof the above PET modules can be applied to a base\nPLM without increasing the latency of inference,\njust like BitFit (Zaken et al., 2021) and LoRA (Hu\net al., 2021).\n3.1 Pruning\nWe perform NAS via pruning. Our NAS method\nbegins by training a PET architecture of a maxi-\nmum user-specified size: for each bias tuning mod-\nule, we fine-tune all bias parameters, and for each\nLoRA update module, we learn a dense low-rank\nupdate with a user-specified rank (in all our experi-\nments, we use rank-16 initial LoRA updates). After\ntraining the initial PET architecture, our method de-\ncides which PET parameters to prune and which to\nkeep. Then we re-initialize and re-train the pruned\narchitecture before evaluating on the validation set.\nThe criteria that we use to decide which PET pa-\nrameters to prune is based on a first-order approx-\nimation of the change in training loss that results\nfrom pruning a PET parameter θ:\n−θ·∂L\n∂θ.\nNote that this is a common pruning criterion, e.g.,\nsee Molchanov et al. (2016). This criterion is\nstraight forward to use when deciding whether to\nprune a single PET parameter, as in unstructured\nbias-tuning and unstructured LoRA. For structured\nbias-tuning, we sum this criterion over the entire\nbias update ∆b, and for structured LoRA, when\nconsidering what column of U and V to prune, we\nsum the criterion over each column of U.\nPruning via evaluating the criterion at the end of\ntraining does not yield better-than-random archi-\ntectures. We observe that the value of the pruning\n8507\nMethod #params MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.\nFFT 355M 90.6 96.0 89.2 66.8 94.6 91.6 85.2 91.5 88.2\nBitFit 273k 89.2 95.6 88.2 65.0 93.9 88.1 81.9 91.4 86.7\nAdapters† 3.0M 90.2 96.1 90.2 68.3 94.8 91.9 83.8 92.1 88.4\nLoRA 3.4M 90.7 95.3 89.7 65.1 93.8 90.3 84.8 91.7 87.7\nMaM 3.4M 90.6 95.3 89.7 65.1 93.8 90.3 84.8 91.7 87.7\nS-MaM 3.4M 90.6 95.9 90.4 66.3 94.5 90.6 85.2 91.6 88.1\nU-MaM 3.4M 90.3 95.8 90.7 66.8 94.1 90.8 85.9 91.8 88.3\nW ARP† 25k 88.2 96.0 90.8 60.6 93.5 84.5 75.8 88.6 84.8\nS-BitFit 25k 84.1 94.2 70.6 40.2 88.9 83.8 56.0 76.8 74.3\nU-BitFit 25k 88.8 95.5 85.3 62.1 93.5 87.7 74.0 90.3 84.6\nTable 1: GLUE development set score for learned and hand-crafted PET architectures. We report the result for\nW ARP† from Hambardzumyan et al. (2021) and for Adapters† from Hu et al. (2021).\ncriterion may change drastically from one stochas-\ntic gradient descent (SGD) step to the next. To\nmaximally smooth the noise introduced by SGD,\nwe instead average the pruning criterion over all\ntraining SGD steps. This yields the most consistent\nindication of which PET parameters are efficient to\nprune.\nOur NAS algorithm takes as input a parameter\nbudget specifying the desired maximum number of\nparameters in the learned PET architecture. After\ntraining the initial PET architecture and evaluating\neach pruning criterion, we apply each pruning oper-\nation in increasing criterion order until the number\nof parameters in the PET architecture falls below\nthe parameter budget. This way, pruning operations\nthat are estimated to increase the training loss the\nleast are applied first.\n3.2 Initialization\nCorrect initialization is important for successfully\napplying this algorithm. After pruning, we re-\ninitialize and re-train the learned PET architecture\nbefore evaluating on the validation set. We find that\nit is important to use the same initialization after\npruning as before. We believe this is a consequence\nof the lottery ticket hypothesis (Frankle and Carbin,\n2018).\nWe always initialize bias parameter updates as\nzero, as do other works, and find this works well.\nHowever, we find that the initialization for LoRA\ngiven in the original paper (Hu et al., 2021), which\ninitializes the matrix U with zeros and V with a\nGaussian distribution, is not ammenable to unstruc-\ntured LoRA pruning. Because the parameters in\nthe matrix U are initialized zero, the magnitudes of\nthose parameters are likely to remain small through-\nout training relative to the magnitudes of the pa-\nrameters in V⊤. Consequently, the pruning cri-\nterion for unstructured LoRA updates is likely to\nfavor pruning parameters from U over V, leading\nto an unbalanced, parameter-inefficient LoRA up-\ndate. Instead, following the same reasoning given\nfor Kaiming initialization (He et al., 2015), we rec-\nommend the following initialization:\nU ∼N(0,1/√m) V ∼N(0,1/√n), (1)\nwhere mis the first dimension of the matrix U\n(i.e., the \"fan-in\"), and nis the second dimension\nof the matrix V⊤(i.e., the \"fan-out\"). With this\ninitialization, the expected square gradients for the\nparameters of U and V are equal.\n4 Experiments\nDetails of our experimental setup, including hyper-\nparameter choices, are available in the appendix. In\nall experiments we report median validation score\nat the end of training over 5 random initializations\nusing the GLUE development set for validation.\n4.1 Comparing to Full Fine-tuning\nHere we present results for training larger PET ar-\nchitectures with the aim of achieving performance\nsimilar to full fine-tuning, but with fewer param-\neters. In addition to structured or unstructured\nbias-tuning, our learned PET architectures add\nstructured or unstructured LoRA updates to the\nMHA query modules, key modules, and the dense\nfeed forward network (FFN) modules. In Table 1,\nour learned structured PET architecture is labeled\nS-MaM, and our learned unstructured PET architec-\nture is labeledU-MaM. We compare our method with\n8508\n0 20 40 60 80 100\nParameters (% of biases in layer)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23Layer\nAverage Structured Bias Tuning Architecture \n RoBERT a large, parameter budget = 50k.\nattention.self.query\nattention.self.key\nattention.self.value\nattention.output.dense\nattention.output.LayerNorm\nintermediate.dense\noutput.dense\noutput.LayerNorm\n(a)\n0 20 40 60 80 100\nParameters (% of biases in layer)\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n10\n11\n12\n13\n14\n15\n16\n17\n18\n19\n20\n21\n22\n23Layer\nAverage Unstructured Bias Tuning Architecture \n RoBERT a large, parameter budget = 50k.\nattention.self.query\nattention.self.key\nattention.self.value\nattention.output.dense\nattention.output.LayerNorm\nintermediate.dense\noutput.dense\noutput.LayerNorm (b)\nFigure 1: Average learned architecture for (a) structured bias-tuning and (b) unstructured bias-tuning.\na LoRA baseline (Hu et al., 2021) and a baseline\nsimilar to Mix-and-Match (MaM) (He et al., 2021).\nOur LoRA baseline fine-tunes all bias parameters\nand adds rank-8 updates to all MHA query and key\nmodules. Our MaM-like baseline fine-tunes all bias\nparameters and adds rank- 8 updates to all MHA\nquery and key modules and all FFN modules.\nResults for this experiment with parameter bud-\nget 3.4M are in Table 1. In our S-MaM and U-MaM\nexperiments, we prune from an initial architecture\nwith 6.8M parameters. We observe that our S-MaM\narchitecture achieves slightly higher average GLUE\n(Wang et al., 2018) validation score over our MaM-\nlike baseline, and our U-MaM architecture slightly\nhigher average GLUE validation score over our\nS-MaM architecture. We conclude that structured\narchitecture search provides a small positive ben-\nefit over the uniform-rank baseline architecture,\nand that unstructured architecture search provides\na small positive benefit over structured architec-\nture search. We also observe our U-MaM architec-\nture achieves average GLUE validation score on\npar with full fine-tuning while fine-tuning approxi-\nmately 100 times fewer parameters.\n4.2 Very Small PETs\nHere we examine our learned PET architectures\nwith parameter budget less than the total num-\nber of bias parameters in the base PLM. For\nroberta-large, this is about 273k.\nWe use our method to learn structured and un-\nstructured bias-tuning architectures. We compare\nour method with WARP (Hambardzumyan et al.,\n2021) using parameter budget 25k in Table 1, and\nreport results for our method for other parame-\nter budgets in the appendix. Our learned struc-\ntured and unstructured bias-tuning architectures\nare labeled S-BitFit and U-BitFit, respectively.\nIn our S-BitFit and U-BitFit experiments, we\nprune from a PET architecture with 273k param-\neters that fine-tuens all bias parameters, the same\nas BitFit. We observe that the unstructured bias-\ntuning architecture achieves significantly higher\nvalidation performance than the structured bias-\ntuning architecture with the same parameter budget.\nWe conclude that the subset of bias parameters that\nare \"good\" to fine-tune are not concentrated in a\nfew modules, but rather are distributed throughout\nthe network. Our learned unstructured bias-tuning\narchitecture with <50k parameters fine-tunes only\n18% of all bias parameters while achieving valida-\ntion GLUE score only slightly less than fine-tuning\nall bias parameters ( 86.5 versus 86.7). We con-\nclude that a vast majority of bias parameters do not\nneed to be fine-tuned to achieve performance com-\nparable to fine-tuning all bias parameters. With a\nparameter budget of 25k, unstructured bias tuning\nachieves similar performance compared to W ARP,\nbeating or tying WARP on a majority of GLUE\ntasks but achieving slightly worse average perfor-\nmance. We conclude that both methods are about\nequally effective.\n4.3 Interpreting Learned Architectures\nHere we examine the architectures learned by our\nalgorithm and consider what they say about which\nparts of the network are most parameter-efficient to\nfine-tune. Each illustration discussed in this section\naverages the architectures learned by our method\nover all GLUE tasks and five random initializa-\n8509\ntions per task. Figure 1a illustrates the architecture\nlearned by our method for structured bias-tuning\nwith parameter budget 50k. We observe a clear\npreference by our algorithm for fine-tuning the bi-\nases of the intermediate.dense modules in the\nmiddle of the network. Figure 1b illustrates the ar-\nchitecture learned by our method for unstructured\nbias tuning with parameter budget 50k. We ob-\nserve a weak preference for fine-tuning the bias\nparameters of modules in the middle of the net-\nwork, but not for any particular module type within\neach transformer block. We conclude that the bi-\nases that are most parameter-efficient to fine-tune\nare in the middle layers of the network.\n5 Conclusion\nIn this paper, we considered the question which\nparts of the network are most efficient to fine-tune,\nand what is the most parameter-efficient way to\nfine-tune them? To answer that question, we de-\nveloped a NAS algorithm based on structured and\nunstructured pruning. We presented experimen-\ntal results on RoBERTa Large demonstrating the\neffectiveness of our algorithm, achieving GLUE\nvalidation performance similar to WARP at 25k\nparameters (9% of all biases), similar to BitFit at\n50k parameters (18% of all biases), and similar\nto full fine-tuning at 3.4M parameters (10% of all\nparameters). From our learned architectures we ob-\nserved that the bias parameters in the middle layers\nof the network are most efficient to fine-tune. We\nconclude that it is important to consider where to\nfine-tune as well as how.\nLimitations\nDifferences in experimental setup may make it dif-\nficult to accurately and fairly compare published\nresults. For example, to prevent data leakage, we\nreport validation performance at the end of train-\ning and do not perform early stopping. This is in\ncontrast to most other papers which report peak\nvalidation performance. Results reported for other\nmethods are reproduced in the same learning en-\nvironment as our method unless explicitly stated\notherwise. This takes into account recent work\ndemonstrating problems with fairly and accurately\nevaluating PET methods that use early stopping\nimproperly (Chen et al., 2022).\nAlthough many pruning criteria exist in the liter-\nature, in this paper we only consider one pruning\ncriterion. Although not presented in this paper,\nexperiments we conducted with various formula-\ntions of magnitude pruning did not produce better\nresults.\nAlthough prompt tuning is a popular PET\nmethod, we do not perform NAS for prompt tuning\nto determine the most efficient positions for insert-\ning prompt tokens into the input. Pruning may or\nmay not prove to be a successful strategy for this\nproblem.\nOther NAS strategies exist in the literature be-\nsides pruning, such as evolutionary, reinforcement\nlearning, and DARTS (Liu et al., 2018). However,\nour pruning method seems to give a good trade-off\nbetween validation performance and computational\nexpense.\nEthics Statement\nPowerful language models can be used for unethi-\ncal purposes, such as generating offensive or decep-\ntive content. Although researchers today are mak-\ning a greater effort to establish protections against\nthe unethical use of their models, bad actors may\nstill find ways to circumvent those protections. One\navenue for attack could involve fine-tuning a PLM\non a nefarious dataset to produce unethical con-\ntent. In this paper, we showed that a PLM can\nbe successfully fine-tuned on a downstream task\nby fine-tuning a small number of parameters, or\nadding a low-rank update to a few select parameter\nmatrices. Thus researchers should consider the risk\nposed by unethical parameter-efficient fine-tuning\nbefore publishing a fine-tuneable version of their\nmodel.\nReferences\nAnkur Bapna, Naveen Arivazhagan, and Orhan Firat.\n2019. Simple, scalable adaptation for neural machine\ntranslation. arXiv preprint arXiv:1909.08478.\nTom Brown, Benjamin Mann, Nick Ryder, Melanie\nSubbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind\nNeelakantan, Pranav Shyam, Girish Sastry, Amanda\nAskell, et al. 2020. Language models are few-shot\nlearners. Advances in neural information processing\nsystems, 33:1877–1901.\nGuanzheng Chen, Fangyu Liu, Zaiqiao Meng, and\nShangsong Liang. 2022. Revisiting parameter-\nefficient tuning: Are we really there yet? arXiv\npreprint arXiv:2202.07962.\nRobin Cheong and Robel Daniel. 2019. transformers.\nzip: Compressing transformers with pruning and\nquantization. Technical report, tech. rep., Stanford\nUniversity, Stanford, California.\n8510\nJacob Devlin, Ming-Wei Chang, Kenton Lee, and\nKristina Toutanova. 2018. BERT: Pre-training of\ndeep bidirectional transformers for language under-\nstanding. arXiv preprint arXiv:1810.04805.\nWilliam Fedus, Barret Zoph, and Noam Shazeer. 2022.\nSwitch transformers: Scaling to trillion parame-\nter models with simple and efficient sparsity. The\nJournal of Machine Learning Research, 23(1):5232–\n5270.\nJonathan Frankle and Michael Carbin. 2018. The lottery\nticket hypothesis: Finding sparse, trainable neural\nnetworks. arXiv preprint arXiv:1803.03635.\nYuxian Gu, Xu Han, Zhiyuan Liu, and Minlie Huang.\n2021. PPT: Pre-trained prompt tuning for few-shot\nlearning. arXiv preprint arXiv:2109.04332.\nDemi Guo, Alexander M Rush, and Yoon Kim. 2020.\nParameter-efficient transfer learning with diff prun-\ning. arXiv preprint arXiv:2012.07463.\nKaren Hambardzumyan, Hrant Khachatrian, and\nJonathan May. 2021. W ARP: Word-level adversarial\nreprogramming. arXiv preprint arXiv:2101.00121.\nJunxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-\nKirkpatrick, and Graham Neubig. 2021. Towards a\nunified view of parameter-efficient transfer learning.\narXiv preprint arXiv:2110.04366.\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian\nSun. 2015. Delving deep into rectifiers: Surpassing\nhuman-level performance on ImageNet classification.\nIn Proceedings of the IEEE International Conference\non Computer Vision, pages 1026–1034.\nNeil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski,\nBruna Morrone, Quentin De Laroussilhe, Andrea\nGesmundo, Mona Attariyan, and Sylvain Gelly. 2019.\nParameter-efficient transfer learning for NLP. In In-\nternational Conference on Machine Learning, pages\n2790–2799. PMLR.\nEdward J Hu, Yelong Shen, Phillip Wallis, Zeyuan\nAllen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang,\nand Weizhu Chen. 2021. LoRA: Low-rank adap-\ntation of large language models. arXiv preprint\narXiv:2106.09685.\nJoel Jang, Seonghyeon Ye, Sohee Yang, Joongbo Shin,\nJanghoon Han, Gyeonghun Kim, Stanley Jungkyu\nChoi, and Minjoon Seo. 2021. Towards contin-\nual knowledge learning of language models. arXiv\npreprint arXiv:2110.03215.\nXiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao\nChen, Linlin Li, Fang Wang, and Qun Liu. 2019.\nTinyBERT: Distilling BERT for natural language un-\nderstanding. arXiv preprint arXiv:1909.10351.\nDiederik P Kingma and Jimmy Ba. 2014. Adam: A\nmethod for stochastic optimization. arXiv preprint\narXiv:1412.6980.\nBrian Lester, Rami Al-Rfou, and Noah Constant. 2021.\nThe power of scale for parameter-efficient prompt\ntuning. arXiv preprint arXiv:2104.08691.\nJiaoda Li, Ryan Cotterell, and Mrinmaya Sachan. 2021.\nDifferentiable subset pruning of transformer heads.\nTransactions of the Association for Computational\nLinguistics, 9:1442–1459.\nXiang Lisa Li and Percy Liang. 2021. Prefix-tuning:\nOptimizing continuous prompts for generation. arXiv\npreprint arXiv:2101.00190.\nZhaojiang Lin, Andrea Madotto, and Pascale Fung.\n2020. Exploring versatile generative language model\nvia parameter-efficient transfer learning. arXiv\npreprint arXiv:2004.03829.\nHanxiao Liu, Karen Simonyan, and Yiming Yang. 2018.\nDARTS: Differentiable architecture search. arXiv\npreprint arXiv:1806.09055.\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man-\ndar Joshi, Danqi Chen, Omer Levy, Mike Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. 2019.\nRoBERTa: A robustly optimized BERT pretraining\napproach. arXiv preprint arXiv:1907.11692.\nRabeeh Karimi Mahabadi, James Henderson, and Se-\nbastian Ruder. 2021. Compacter: Efficient low-\nrank hypercomplex adapter layers. arXiv preprint\narXiv:2106.04647.\nYuning Mao, Lambert Mathias, Rui Hou, Amjad Alma-\nhairi, Hao Ma, Jiawei Han, Wen-tau Yih, and Madian\nKhabsa. 2021. UniPELT: A unified framework for\nparameter-efficient language model tuning. arXiv\npreprint arXiv:2110.07577.\nAmil Merchant, Elahe Rahimtoroghi, Ellie Pavlick,\nand Ian Tenney. 2020. What happens to BERT\nembeddings during fine-tuning? arXiv preprint\narXiv:2004.14448.\nPaul Michel, Omer Levy, and Graham Neubig. 2019.\nAre sixteen heads really better than one? Advances\nin neural information processing systems, 32.\nPavlo Molchanov, Stephen Tyree, Tero Karras, Timo\nAila, and Jan Kautz. 2016. Pruning convolutional\nneural networks for resource efficient inference.\narXiv preprint arXiv:1611.06440.\nMarius Mosbach, Maksym Andriushchenko, and Diet-\nrich Klakow. 2020. On the stability of fine-tuning\nBERT: Misconceptions, explanations, and strong\nbaselines. arXiv preprint arXiv:2006.04884.\nMatthew E Peters, Sebastian Ruder, and Noah A Smith.\n2019. To tune or not to tune? Adapting pretrained\nrepresentations to diverse tasks. arXiv preprint\narXiv:1903.05987.\nJonas Pfeiffer, Aishwarya Kamath, Andreas Rücklé,\nKyunghyun Cho, and Iryna Gurevych. 2020a.\nAdapterFusion: Non-destructive task composition for\ntransfer learning. arXiv preprint arXiv:2005.00247.\n8511\nJonas Pfeiffer, Andreas Rücklé, Clifton Poth, Aishwarya\nKamath, Ivan Vuli´c, Sebastian Ruder, Kyunghyun\nCho, and Iryna Gurevych. 2020b. AdapterHub: A\nframework for adapting transformers. arXiv preprint\narXiv:2007.07779.\nJonas Pfeiffer, Ivan Vuli´c, Iryna Gurevych, and Sebas-\ntian Ruder. 2020c. MAD-X: An adapter-based frame-\nwork for multi-task cross-lingual transfer. arXiv\npreprint arXiv:2005.00052.\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea\nVedaldi. 2017. Learning multiple visual domains\nwith residual adapters. Advances in neural informa-\ntion processing systems, 30.\nHassan Sajjad, Fahim Dalvi, Nadir Durrani, and\nPreslav Nakov. 2020. Poor man’s BERT: Smaller\nand faster transformer models. arXiv preprint\narXiv:2004.03844.\nVictor Sanh, Lysandre Debut, Julien Chaumond, and\nThomas Wolf. 2019. DistilBERT, a distilled version\nof BERT: smaller, faster, cheaper and lighter. arXiv\npreprint arXiv:1910.01108.\nTaylor Shin, Yasaman Razeghi, Robert L Logan IV ,\nEric Wallace, and Sameer Singh. 2020. AutoPrompt:\nEliciting knowledge from language models with\nautomatically generated prompts. arXiv preprint\narXiv:2010.15980.\nDavid So, Quoc Le, and Chen Liang. 2019. The evolved\ntransformer. In International Conference on Machine\nLearning, pages 5877–5886. PMLR.\nZhiqing Sun, Hongkun Yu, Xiaodan Song, Renjie Liu,\nYiming Yang, and Denny Zhou. 2020. MobileBERT:\na compact task-agnostic bert for resource-limited de-\nvices. arXiv preprint arXiv:2004.02984.\nElena V oita, David Talbot, Fedor Moiseev, Rico Sen-\nnrich, and Ivan Titov. 2019. Analyzing multi-\nhead self-attention: Specialized heads do the heavy\nlifting, the rest can be pruned. arXiv preprint\narXiv:1905.09418.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix\nHill, Omer Levy, and Samuel R Bowman. 2018.\nGLUE: A multi-task benchmark and analysis plat-\nform for natural language understanding. arXiv\npreprint arXiv:1804.07461.\nElad Ben Zaken, Shauli Ravfogel, and Yoav Gold-\nberg. 2021. BitFit: Simple parameter-efficient\nfine-tuning for transformer-based masked language-\nmodels. arXiv preprint arXiv:2106.10199.\nA Experiment Setup\nIn all experiments we use the Adam optimizer\n(Kingma and Ba, 2014) and a linear learning rate\nscheduler with 6% warm-up steps. We observe\nthat training with a higher peak learning rate works\nbest when fine-tuning a small number of parame-\nters. We use different peak learning rates for differ-\nent experiments depending on the maximum num-\nber of parameters being fine-tuned, ranging from\n10−5 for full fine-tuning to 3 ×10−4 for training\nour smallest PETs. We also train for a different\nnumber of epochs for each GLUE tasks. We train\nfor 20 epochs on MRPC, RTE, CoLA, and STSB;\n5 epochs on SST-2 and QNLI; and 2 epochs for\nMNLI and QQP. We observe that extending the\nnumber of training epochs beyond these limits does\nnot substantially affect validation performance. In\nall experiments, we use batch size 16 and maxi-\nmum sequence length 128.\nB Additional Experimental Results\nWe report results for our learned structured and un-\nstructured bias-tuning architecture with parameter\nbudgets 10k, 25k, 50k, 100k, and 200k in Table\n2. We observe that unstructured bias-tuning holds\nan advantage over structured bias-tuning across all\nparameter budgets. We also observe that the perfor-\nmance of unstructured bias-tuning begins to fall off\nafter decreasing the parameter budget below 50k.\nW ARP with a parameter budget of 11k significantly\noutperforms our U-BitFit method with a parame-\nter budget of 10k on the MRPC and COLA tasks.\nThis difference might be explained by the differ-\nence in experimental setup (e.g., Hambardzumyan\net al. (2021) reports peak validation score whereas\nwe report end-of-training validation score), or the\nsmall difference in parameter budget. We believe\nthat our method can be improved in the very small\nparameter budget regime using iterative, rather than\none-shot, pruning.\n8512\nMethod #params MNLI SST-2 MRPC CoLA QNLI QQP RTE STS-B Avg.\nW ARP† 11k 87.6 93.0 83.8 72.9 95.4 85.6 57.4 81.0 82.1\nW ARP† 25k 88.2 96.0 90.8 60.6 93.5 84.5 75.8 88.6 84.8\nS-BitFit 10k 70.1 92.1 70.6 0.0 73.1 73.3 52.7 22.2 56.8\nS-BitFit 25k 84.1 94.2 70.6 40.2 88.9 83.8 56.0 76.8 74.3\nS-BitFit 50k 87.1 94.3 72.1 51.5 91.4 86.2 59.6 86.9 78.6\nS-BitFit 100k 88.2 95.0 87.7 58.8 92.4 87.4 78.7 90.4 84.8\nS-BitFit 200k 89.1 95.6 88.2 63.1 93.8 87.9 81.9 91.4 86.4\nU-BitFit 10k 87.4 95.1 71.1 58.8 92.2 86.3 59.6 88.3 79.8\nU-BitFit 25k 88.8 95.5 85.3 62.1 93.5 87.7 74.0 90.3 84.6\nU-BitFit 50k 89.1 95.8 88.5 64.8 93.8 88.0 80.9 91.1 86.5\nU-BitFit 100k 89.3 95.8 88.5 63.6 93.9 87.7 81.9 91.3 86.5\nU-BitFit 200k 89.4 95.6 88.5 64.8 93.9 86.5 81.9 91.4 86.5\nTable 2: GLUE development set score for structured (S-BitFit) and unstructured (U-BitFit) bias-tuning architectures\nlearned by our method for different parameter budgets. The results for W ARP† are reported from Hambardzumyan\net al. (2021).\n8513\nACL 2023 Responsible NLP Checklist\nA For every submission:\n□\u0013 A1. Did you describe the limitations of your work?\nThis is discussed in the section titled \"Limitations\" after section 5.\n□\u0013 A2. Did you discuss any potential risks of your work?\nWe provide an Ethics Statement after section 5.\n□\u0013 A3. Do the abstract and introduction summarize the paper’s main claims?\nThe abstract is presented before section 1.\n□\u0017 A4. Have you used AI writing assistants when working on this paper?\nLeft blank.\nB □\u0013 Did you use or create scientiﬁc artifacts?\nWe used the GLUE datasets in our experiments discussed in section 4.\n□\u0013 B1. Did you cite the creators of artifacts you used?\nWe provide the citation for GLUE on line 227 in section 4.\n□\u0017 B2. Did you discuss the license or terms for use and / or distribution of any artifacts?\nWe did not discuss license for GLUE due to space constraints.\n□\u0017 B3. Did you discuss if your use of existing artifact(s) was consistent with their intended use, provided\nthat it was speciﬁed? For the artifacts you create, do you specify intended use and whether that is\ncompatible with the original access conditions (in particular, derivatives of data accessed for research\npurposes should not be used outside of research contexts)?\nWe did not discuss the intended use for GLUE as we properly use GLUE for its intended use and\nbecause GLUE is a widely known dataset.\n□\u0017 B4. Did you discuss the steps taken to check whether the data that was collected / used contains any\ninformation that names or uniquely identiﬁes individual people or offensive content, and the steps\ntaken to protect / anonymize it?\nWe did not discuss whether GLUE contains any non-anonymized or offensive data because GLUE is\na widely known dataset.\n□\u0017 B5. Did you provide documentation of the artifacts, e.g., coverage of domains, languages, and\nlinguistic phenomena, demographic groups represented, etc.?\nWe did not provide documentation for GLUE because GLUE is a widely known dataset.\n□\u0017 B6. Did you report relevant statistics like the number of examples, details of train / test / dev splits,\netc. for the data that you used / created? Even for commonly-used benchmark datasets, include the\nnumber of examples in train / validation / test splits, as these provide necessary context for a reader\nto understand experimental results. For example, small differences in accuracy on large test sets may\nbe signiﬁcant, while on small test sets they may not be.\nWe did not present train/test/dev split counts for GLUE because of the tight space constraint and\nbecause we used the default train/test/dev split for each GLUE task.\nThe Responsible NLP Checklist used at ACL 2023 is adopted from NAACL 2022, with the addition of a question on AI writing\nassistance.\n8514\nC □\u0013 Did you run computational experiments?\nSection 4\n□\u0013 C1. Did you report the number of parameters in the models used, the total computational budget\n(e.g., GPU hours), and computing infrastructure used?\nWe reported the number of parameters used, but not the computational budget or the computing\ninfrastructure.\n□\u0013 C2. Did you discuss the experimental setup, including hyperparameter search and best-found\nhyperparameter values?\nWe discuss experimental setup in the appendix.\n□\u0013 C3. Did you report descriptive statistics about your results (e.g., error bars around results, summary\nstatistics from sets of experiments), and is it transparent whether you are reporting the max, mean,\netc. or just a single run?\nAt the beginning of section 4, we specify that we report the median of 5 runs.\n□\u0013 C4. If you used existing packages (e.g., for preprocessing, for normalization, or for evaluation), did\nyou report the implementation, model, and parameter settings used (e.g., NLTK, Spacy, ROUGE,\netc.)?\nWe specify the hugging face model that we use and speciﬁc modules within that model in section 4.\nD □\u0017 Did you use human annotators (e.g., crowdworkers) or research with human participants?\nLeft blank.\n□ D1. Did you report the full text of instructions given to participants, including e.g., screenshots,\ndisclaimers of any risks to participants or annotators, etc.?\nNot applicable. Left blank.\n□ D2. Did you report information about how you recruited (e.g., crowdsourcing platform, students)\nand paid participants, and discuss if such payment is adequate given the participants’ demographic\n(e.g., country of residence)?\nNot applicable. Left blank.\n□ D3. Did you discuss whether and how consent was obtained from people whose data you’re\nusing/curating? For example, if you collected data via crowdsourcing, did your instructions to\ncrowdworkers explain how the data would be used?\nNot applicable. Left blank.\n□ D4. Was the data collection protocol approved (or determined exempt) by an ethics review board?\nNot applicable. Left blank.\n□ D5. Did you report the basic demographic and geographic characteristics of the annotator population\nthat is the source of the data?\nNot applicable. Left blank.\n8515",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.829767107963562
    },
    {
      "name": "Pruning",
      "score": 0.7423743009567261
    },
    {
      "name": "Language model",
      "score": 0.6298213005065918
    },
    {
      "name": "Architecture",
      "score": 0.6253625750541687
    },
    {
      "name": "Artificial intelligence",
      "score": 0.5626947283744812
    },
    {
      "name": "Artificial neural network",
      "score": 0.5295194983482361
    },
    {
      "name": "Machine learning",
      "score": 0.46950244903564453
    },
    {
      "name": "Fine-tuning",
      "score": 0.44637688994407654
    },
    {
      "name": "Visual arts",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Art",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Agronomy",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    }
  ]
}