{
  "title": "Supervised Contrastive Learning for Pre-trained Language Model Fine-tuning",
  "url": "https://openalex.org/W3096565276",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4224812803",
      "name": "Gunel, Beliz",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2748019825",
      "name": "Du, Jingfei",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4221359244",
      "name": "Conneau, Alexis",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A4226508962",
      "name": "Stoyanov, Ves",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2971155163",
    "https://openalex.org/W2917551568",
    "https://openalex.org/W2138204974",
    "https://openalex.org/W2949736877",
    "https://openalex.org/W2963026768",
    "https://openalex.org/W3034781633",
    "https://openalex.org/W2889326796",
    "https://openalex.org/W2555897561",
    "https://openalex.org/W3100859887",
    "https://openalex.org/W3035524453",
    "https://openalex.org/W2293778248",
    "https://openalex.org/W2943152387",
    "https://openalex.org/W3099782249",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W2963012544",
    "https://openalex.org/W3034850762",
    "https://openalex.org/W2965373594",
    "https://openalex.org/W3043462782",
    "https://openalex.org/W2788496822",
    "https://openalex.org/W2944828972",
    "https://openalex.org/W3037057938",
    "https://openalex.org/W2964013229",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2798991696",
    "https://openalex.org/W3100345210",
    "https://openalex.org/W2970206392",
    "https://openalex.org/W3118608800",
    "https://openalex.org/W1544827683",
    "https://openalex.org/W2962788840",
    "https://openalex.org/W3108655343",
    "https://openalex.org/W2163614729",
    "https://openalex.org/W2962369866",
    "https://openalex.org/W2887997457",
    "https://openalex.org/W2097732278",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W2973562770",
    "https://openalex.org/W2970941190",
    "https://openalex.org/W1840435438",
    "https://openalex.org/W2963759070",
    "https://openalex.org/W2171849160",
    "https://openalex.org/W2963060032",
    "https://openalex.org/W3034978746",
    "https://openalex.org/W2842511635",
    "https://openalex.org/W1821462560",
    "https://openalex.org/W2963341956",
    "https://openalex.org/W3035204084",
    "https://openalex.org/W3026732421",
    "https://openalex.org/W2963656735",
    "https://openalex.org/W2913939497",
    "https://openalex.org/W2913881544",
    "https://openalex.org/W3099206234",
    "https://openalex.org/W3034709122",
    "https://openalex.org/W2963310665",
    "https://openalex.org/W2183341477",
    "https://openalex.org/W3035160371",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2117539524",
    "https://openalex.org/W2933138175",
    "https://openalex.org/W3010874390",
    "https://openalex.org/W3005700362"
  ],
  "abstract": "State-of-the-art natural language understanding classification models follow two-stages: pre-training a large language model on an auxiliary task, and then fine-tuning the model on a task-specific labeled dataset using cross-entropy loss. However, the cross-entropy loss has several shortcomings that can lead to sub-optimal generalization and instability. Driven by the intuition that good generalization requires capturing the similarity between examples in one class and contrasting them with examples in other classes, we propose a supervised contrastive learning (SCL) objective for the fine-tuning stage. Combined with cross-entropy, our proposed SCL loss obtains significant improvements over a strong RoBERTa-Large baseline on multiple datasets of the GLUE benchmark in few-shot learning settings, without requiring specialized architecture, data augmentations, memory banks, or additional unsupervised data. Our proposed fine-tuning objective leads to models that are more robust to different levels of noise in the fine-tuning training data, and can generalize better to related tasks with limited labeled data.",
  "full_text": "Published as a conference paper at ICLR 2021\nSUPERVISED CONTRASTIVE LEARNING FOR\nPRE-TRAINED LANGUAGE MODEL FINE -TUNING\nBeliz Gunel†∗, Jingfei Du‡, Alexis Conneau‡, Ves Stoyanov‡\n†Stanford University, ‡Facebook AI\nABSTRACT\nState-of-the-art natural language understanding classiﬁcation models follow two-\nstages: pre-training a large language model on an auxiliary task, and then ﬁne-\ntuning the model on a task-speciﬁc labeled dataset using cross-entropy loss. How-\never, the cross-entropy loss has several shortcomings that can lead to sub-optimal\ngeneralization and instability. Driven by the intuition that good generalization\nrequires capturing the similarity between examples in one class and contrasting\nthem with examples in other classes, we propose a supervised contrastive learning\n(SCL) objective for the ﬁne-tuning stage. Combined with cross-entropy, our pro-\nposed SCL loss obtains signiﬁcant improvements over a strong RoBERTa-Large\nbaseline on multiple datasets of the GLUE benchmark in few-shot learning settings,\nwithout requiring specialized architecture, data augmentations, memory banks, or\nadditional unsupervised data. Our proposed ﬁne-tuning objective leads to models\nthat are more robust to different levels of noise in the ﬁne-tuning training data, and\ncan generalize better to related tasks with limited labeled data.\n1 I NTRODUCTION\nState-of-the-art for most existing natural language processing (NLP) classiﬁcation tasks is achieved\nby models that are ﬁrst pre-trained on auxiliary language modeling tasks and then ﬁne-tuned on the\ntask of interest with cross-entropy loss (Radford et al., 2019; Howard & Ruder, 2018; Liu et al.,\n2019; Devlin et al., 2019). Although ubiquitous, the cross-entropy loss – the KL-divergence between\none-hot vectors of labels and the distribution of model’s output logits – has several shortcomings.\nCross entropy loss leads to poor generalization performance (Liu et al., 2016; Cao et al., 2019), and\nit lacks robustness to noisy labels (Zhang & Sabuncu, 2018; Sukhbaatar et al., 2015) or adversarial\nexamples (Elsayed et al., 2018; Nar et al., 2019). Effective alternatives have been proposed to modify\nthe reference label distributions through label smoothing (Szegedy et al., 2016; M¨uller et al., 2019),\nMixup (Zhang et al., 2018), CutMix (Yun et al., 2019), knowledge distillation (Hinton et al., 2015) or\nself-training (Yalniz et al., 2019; Xie et al., 2020).\nFine-tuning using cross entropy loss in NLP also tends to be unstable across different runs (Zhang\net al., 2020; Dodge et al., 2020), especially when supervised data is limited, a scenario in which\npre-training is particularly helpful. To tackle the issue of unstable ﬁne-tuning and poor generalization,\nrecent works propose local smoothness-inducing regularizers (Jiang et al., 2020) and regularization\nmethods inspired by the trust region theory (Aghajanyan et al., 2020) to prevent representation\ncollapse. Empirical evidence suggests that ﬁne-tuning for more iterations, reinitializing top few\nlayers (Zhang et al., 2020), and using debiased Adam optimizer during ﬁne-tuning (Mosbach et al.,\n2020) can make the ﬁne-tuning stage more stable.\nInspired by the learning strategy that humans utilize when given a few examples, we seek to ﬁnd\nthe commonalities between the examples of each class and contrast them with examples from\nother classes. We hypothesize that a similarity-based loss will be able to hone in on the important\ndimensions of the multidimensional hidden representations hence lead to better few-shot learning\nresults and be more stable while ﬁne-tuning pre-trained language models. We propose a novel\nobjective for ﬁne-tuning that includes a supervised contrastive learning (SCL) term that pushes the\nexamples from the same class close and the examples from different classes further apart. The SCL\n∗Work done during Facebook AI research internship, correspondence to bgunel@stanford.edu.\n1\narXiv:2011.01403v3  [cs.CL]  2 Apr 2021\nPublished as a conference paper at ICLR 2021\nterm is similar to the contrastive objectives used in self-supervised representation learning across\nimage, speech, and video domains. (Sohn, 2016; Oord et al., 2018; Wu et al., 2018; Bachman et al.,\n2019; H´enaff et al., 2019; Baevski et al., 2020; Conneau et al., 2020; Tian et al., 2020; Hjelm et al.,\n2019; Han et al., 2019; He et al., 2020; Misra & Maaten, 2020; Chen et al., 2020a;b). Unlike these\nmethods, however, we use a contrastive objective for supervised learning of the ﬁnal task, instead of\ncontrasting different augmented views of examples.\nIn few-shot learning settings (20, 100, 1000 labeled examples), the addition of the SCL term to the ﬁne-\ntuning objective signiﬁcantly improves the performance on several natural language understanding\nclassiﬁcation tasks from the popular GLUE benchmark (Wang et al., 2019) over the very strong\nbaseline of ﬁne-tuning RoBERTa-Large with cross-entropy loss only. Furthermore, pre-trained\nlanguage models ﬁne-tuned with our proposed objective are not only robust to noise in the ﬁne-tuning\ntraining data, but can also exhibit improved generalization to related tasks with limited labeled task\ndata. Our approach does not require any specialized network architectures (Bachman et al., 2019;\nH´enaff et al., 2019), memory banks (Wu et al., 2018; Tian et al., 2020; Misra & Maaten, 2020), data\naugmentation of any kind, or additional unsupervised data. To the best of our knowledge, our work is\nthe ﬁrst to successfully integrate a supervised contrastive learning objective for ﬁne-tuning pre-trained\nlanguage models. We empirically demonstrate that the new objective has desirable properties across\nseveral different settings. Our contributions in this work are listed in the following:\n• We propose a novel objective for ﬁne-tuning pre-trained language models that includes a\nsupervised contrastive learning term, as described in Section 2.\n• We obtain strong improvements in the few-shot learning settings (20, 100, 1000 labeled\nexamples) as shown in Table 2, leading up to 10.7 points improvement on a subset of GLUE\nbenchmark tasks (SST-2, QNLI, MNLI) for the 20 labeled example few-shot setting, over a\nvery strong baseline – RoBERTa-Large ﬁne-tuned with cross-entropy loss.\n• We demonstrate that our proposed ﬁne-tuning objective is more robust, in comparison\nto RoBERTa-Large ﬁne-tuned with cross-entropy loss, across augmented noisy training\ndatasets (used to ﬁne-tune the models for the task of interest) with varying noise levels as\nshown in Table 3 – leading up to 7 points improvement on a subset of GLUE benchmark\ntasks (SST-2, QNLI, MNLI) across augmented noisy training datasets. We use a back-\ntranslation model to construct the augmented noisy training datasets of varying noise levels\n(controlled by the temperature parameter), as described in detail in Section 4.2.\n• We show that the task-models ﬁne-tuned with our proposed objective have improved gener-\nalizability to related tasks despite having limited availability of labeled task data (Table 7).\nThis led to a 2.9 point improvement on Amazon-2 over the task model ﬁne-tuned with\ncross-entropy loss only. Moreover, it considerably reduced the variance across few-shot\ntraining samples, when transferred from the source SST-2 sentiment analysis task model.\n2 A PPROACH\nWe propose a novel objective that includes a supervised contrastive learning term for ﬁne-tuning\npre-trained language models. The loss is meant to capture the similarities between examples of the\nsame class and contrast them with the examples from other classes.\nFor a multi-class classiﬁcation problem with C classes, we work with a batch of training examples of\nsize N, {xi,yi}i=1,...N . Φ(·) ∈Rd denotes an encoder that outputs the l2 normalized ﬁnal encoder\nhidden layer before the softmax projection; Nyi is the total number of examples in the batch that\nhave the same label as yi; τ >0 is an adjustable scalar temperature parameter that controls the\nseparation of classes; yi,c denotes the label and ˆyi,c denotes the model output for the probability of\nthe ith example belonging to the class c; λis a scalar weighting hyperparameter that we tune for each\ndownstream task and setting. The overall loss is then given in the following:\n2\nPublished as a conference paper at ICLR 2021\nL= (1 −λ)LCE + λLSCL (1)\nLCE = −1\nN\nN∑\ni=1\nC∑\nc=1\nyi,c ·logˆyi,c (2)\nLSCL =\nN∑\ni=1\n− 1\nNyi −1\nN∑\nj=1\n1i̸=j1yi=yj log exp (Φ(xi) ·Φ(xj)/τ)∑N\nk=1 1i̸=k exp (Φ(xi) ·Φ(xk)/τ)\n(3)\nThe overall loss is a weighted average of CE and the proposed SCL loss, as given in the equation (1).\nThe canonical deﬁnition of the multi-class CE loss that we use is given in equation (2). The novel\nSCL loss is given in the equation (3).\nThis loss can be applied using a variety of encoders Φ(·) ∈Rd – for example a ResNet for a\ncomputer vision application or a pre-trained language model such as BERT for an NLP applica-\ntion. In this work, we focus on ﬁne-tuning pre-trained language models for single sentence and\nsentence-pair classiﬁcation settings. For single sentence classiﬁcation, each example xi consists of\nsequence of tokens prepended with the special [CLS] token xi = [[CLS],t1,t2,...,t L,[EOS]].\nThe length of sequence L is constrained such that L < Lmax. Similarly, for sentence-pair clas-\nsiﬁcation tasks, each example xi is a concatenation of two sequences of tokens [t1,t2,...t L]\nand [s1,s2,...,s M ] corresponding to the sentences with special tokens delimiting them: xi =\n[[CLS],t1,t2,...,t L,[SEP],s1,s2,...,s M ,[EOS]]. The length of concatenated sequences is con-\nstrained such that L+M <Lmax. In both cases, Φ(xi) ∈Rd uses the embedding of [CLS] token as\nthe representation for example xi. These choices follow standard practices for ﬁne-tuning pre-trained\nlanguage models for classiﬁcation (Devlin et al., 2019; Liu et al., 2019).\nFigure 1: Our proposed objective includes a cross-entropy term (CE) and a supervised contrastive\nlearning (SCL) term, and it is formulated to push examples from the same class close and examples\nfrom different classes further apart. We show examples from the SST-2 sentiment analysis dataset\nfrom the GLUE benchmark, where class A (shown in red) is negative movie reviews and class\nB (shown in blue) is positive movie reviews. Although we show a binary classiﬁcation case for\nsimplicity, the loss is generally applicable to any multi-class classiﬁcation setting.\nEmpirical observations show that both l2 normalization of the encoded embedding representations\nand an adjustable scalar temperature parameter τ improve performance. Lower temperature increases\nthe inﬂuence of examples that are harder to separate, effectively creating harder negatives. Using\nhard negatives has been previously shown to improve performance in the context of margin-based\nloss formulations such as triplet loss (Schroff et al., 2015). The empirical behavior of the adjustable\ntemperature parameter is consistent with the observations of previous work related to supervised\ncontrastive learning. (Chen et al., 2020a; Khosla et al., 2020).\nRelationship to Self-Supervised Contrastive LearningSelf-supervised contrastive learning has\nshown success in learning powerful representations, particularly in the computer vision domain. (Chen\net al., 2020a; He et al., 2020; Tian et al., 2020; Mnih & Kavukcuoglu, 2013; Gutmann & Hyv¨arinen,\n2012; Kolesnikov et al., 2019) Self-supervised learning methods do not require any labeled data;\ninstead they sample a mini batch from unsupervised data and create positive and negative examples\n3\nPublished as a conference paper at ICLR 2021\nfrom these samples using strong data augmentation techniques such as AutoAugment (Cubuk et al.,\n2019) or RandAugment (Cubuk et al., 2020) for computer vision. Positive examples are constructed\nby applying data augmentation to the same example (cropping, ﬂipping, etc. for an image), and\nnegative examples are simply all the other examples in the sampled mini batch. Intuitively, self-\nsupervised contrastive objectives are learning representations that are invariant to different views of\npositive pairs; while maximizing the distance between negative pairs. The distance metric used is\noften the inner product or the Euclidean distance between vector representations of the examples.\nFor a batch of size N, self-supervised contrastive loss is deﬁned as:\nLself =\n2N∑\ni=1\n−log exp (Φ(x′\n2i−1) ·Φ(x′\n2i)/τ)\n∑2N\nk=1 1i̸=k exp (Φ(x′\ni) ·Φ(x′\nk)/τ)\n(4)\nwhere Φ(·) ∈Rd denotes an encoder that outputs the l2 normalized ﬁnal encoder hidden layer before\nthe softmax projection; τ >0 is a scalar temperature parameter. A is deﬁned as a data augmentation\nblock that generates two randomly generated augmented examples, x′\n2i and x′\n2i−1 from the original\nexample xi: A({xi,yi}i=1,...N ) = {x′\ni,y′\ni}i=1,...2N . As an example, A can be RandAugment for a\ncomputer vision application; or it could be a back-translation model for an NLP application.\n3 R ELATED WORK\nTraditional Machine Learning and Theoretical UnderstandingSeveral works have analyzed\nthe shortcomings of the widely adopted cross-entropy loss, demonstrating that it leads to poor\ngeneralization performance due to poor margins (Liu et al., 2016; Cao et al., 2019), and lack of\nrobustness to noisy labels (Zhang & Sabuncu, 2018; Sukhbaatar et al., 2015) or adversarial examples\n(Elsayed et al., 2018; Nar et al., 2019). On the other hand, there has been a body of work that has\nexplored the performance difference for classiﬁers trained with discriminative (i.e., optimizing for\np(y|x), where y is the label and x is the input) losses such as cross-entropy loss and generative\nlosses (i.e. optimizing for p(x|y)). Ng & Jordan (2001) show that classiﬁers trained with generative\nlosses can outperform their counterparts trained with discriminative losses in the context of Logistic\nRegression and Naive Bayes. Raina et al. (2003) show that a hybrid discriminative and generative\nobjective outperforms both solely discriminative and generative approaches. In the context of\ncontrastive learning, Saunshi et al. (2019) propose a theoretical framework for analyzing contrastive\nlearning algorithms through hypothesizing that semantically similar points are sampled from the\nsame latent class, which allows showing formal guarantees on the quality of learned representations.\nContrastive LearningThere has been several recent investigations for the use of contrastive ob-\njectives for self-supervised, semi-supervised, and supervised learning methods, primarily in the\ncomputer vision domain. Chen et al. (2020a) propose a framework for contrastive learning of vi-\nsual representations without specialized architectures or a memory bank, and show state-of-the-art\nresults on ImageNet ILSVRC-2012 (Russakovsky et al., 2015) – outperforming previous methods\nfor self-supervised, semi-supervised and transfer learning. Similarly, Khosla et al. (2020) propose\na supervised contrastive loss that outperforms cross entropy loss and gets state-of-the-art results\non ImageNet on both ResNet-50 and ResNet-200 (He et al., 2016) with AutoAugment (Cubuk\net al., 2019) data augmentation. They also show increased robustness on the ImageNet-C dataset\n(Hendrycks & Dietterich, 2019), and demonstrate that supervised contrastive loss is less sensitive to\ndifferent hyperparameter settings for optimizers or data augmentations compared to the cross-entropy\nloss. Liu & Abbeel (2020) propose a hybrid discriminative-generative training of energy-based\nmodels where they approximate the generative term with a contrastive loss using large batch sizes and\nshow improved classiﬁcation accuracy of WideResNet-28-10 (Zagoruyko & Komodakis, 2016) on\nCIFAR-10 and CIFAR-100 (Krizhevsky, 2009) datasets, outperforming state-of-the-art discriminative\nand generative classiﬁers. They also demonstrate improved performance for WideResNet-28-10 on\nrobustness, out-of-distribution detection, and calibration, compared to other state-of-the-art gener-\native and hybrid models. Finally, Fang & Xie (2020) propose pre-training language models using\na self-supervised contrastive learning objective at the sentence level using back-translation as the\naugmentation method, followed by ﬁne-tuning by predicting whether two augmented sentences\noriginate from the same sentence – demonstrating improvements over ﬁne-tuning BERT on a subset\nof GLUE benchmark tasks.\n4\nPublished as a conference paper at ICLR 2021\nStability and Robustness of Fine-tuning Pre-trained Language ModelsThere has been recent\nworks on analyzing the stability and robustness of ﬁne-tuning pre-trained language models, since\nthey have been shown to overﬁt to the labeled task data while ﬁne-tuning and hence fail to generalize\nto unseen data when there is limited labeled data for the task (Aghajanyan et al., 2020). To improve\nthe generalization performance, Jiang et al. (2020) propose a local smoothness-inducing regularizer\nto manage the complexity of the model and a Bregman proximal point optimization method, an\ninstance of trust-region methods, to prevent aggressive updating of the model during ﬁne-tuning.\nThey show state-of-the-art performance on GLUE, SNLI (Bowman et al., 2015), SciTail (Khot\net al., 2018), and ANLI (Nie et al., 2020) natural language understanding benchmarks. Similarly,\nAghajanyan et al. (2020) propose a regularized ﬁne-tuning procedure inspired by trust-region theory\nthat replaces adversarial objectives with parametric noise sampled from normal or uniform distribution\nin order to prevent representation collapse during ﬁne-tuning for better generalization performance,\nwithout hurting the performance. They show improved performance on a range of natural language\nunderstanding and generation tasks including DailyMail/CNN (Hermann et al., 2015), Gigaword\n(Napoles et al., 2012), Reddit TIFU (Kim et al., 2019), and the GLUE benchmark. There has\nalso been some empirical analysis that suggests ﬁne-tuning for more epochs, reinitializing top few\nlayers (Zhang et al., 2020) instead of only the classiﬁcation head, and using debiased Adam optimizer\ninstead of BERTAdam (Devlin et al., 2019) during ﬁne-tuning (Mosbach et al., 2020) can make the\nﬁne-tuning procedure more stable across different runs.\n4 E XPERIMENTAL SETUP\n4.1 D ATASETS AND TRAINING DETAILS\nWe use datasets from the GLUE natural language understanding benchmark (Wang et al., 2019) for\nevaluation. We include both single sentence classiﬁcation tasks and sentence-pair classiﬁcation tasks\nto test whether our hypothesis is generally applicable across tasks. We summarize each dataset based\non their main task, domain, number of training examples, and number of classes in Table 1.\nIn our few-shot learning experiments, we sample half of the original validation set of the GLUE\nbenchmark and use it as our test set, and sample ∼500 examples for our validation set from the\noriginal GLUE validation set, both taking the label distribution of the original validation set into\naccount. For each task, we want the validation set to be small enough to avoid easy overﬁtting on\nthe validation set, and big enough to avoid high-variance when early-stopping at various epochs for\nthe few-shot learning experiments. For full dataset experiments, such as the ones shown in Table 5,\nTable 6, Table 8, and Table 9, we sample a validation set from the original training set of the GLUE\nbenchmark based on the size of the original validation set of GLUE, and report our test results on the\noriginal validation set of GLUE.\nWe run each experiment with 10 different seeds, and report the average test accuracy, standard\ndeviation, along with p-values with respect to the baseline. We pick the best hyperparameter\ncombination based on the average validation accuracy across 10 seeds. For few-shot learning\nexperiments, such as the ones shown in Table 2, Table 3, and Table 10, we sample 10 different\ntraining set samples based on the total number of examples N speciﬁed from the original training set\nof the GLUE benchmark, taking the label distribution of the original training set into account. We\nreport the average and the standard deviation of the test accuracies of the top 3 models based on their\nvalidation accuracies out of 10 random training set samples. Best hyperparameter combination is\npicked based on the average validation accuracy of the top 3 models. The reason why we focus on the\ntop 3 models for this setting is that we would like to reduce the variance across training set samples.\nWe use fairseq Ott et al. (2019) library and the open-source RoBERTa-Large model for all of our\nexperiments. During all the ﬁne-tuning runs, we use Adam optimizer with a learning rate of 1e-5,\nbatch size of 16 (unless speciﬁed otherwise), and dropout rate of 0.1. For each experiment that includes\nthe SCL term, we conduct a grid-based hyperparameter sweep for λ∈{0.1,0.3,0.5,0.7,0.9,1.0}\nand τ ∈{0.1,0.3,0.5,0.7}. We observe that models with best test accuracies across all experimental\nsettings overwhelmingly use the hyperparameter combination τ = 0.3 and λ= 0.9.\n5\nPublished as a conference paper at ICLR 2021\nDataset Task Domain #Train #Classes\nSST-2 sentiment analysis movie reviews 67k 2\nCoLA grammatical correctness linguistic publications 8.5k 2\nMRPC paraphrase news 3.7k 2\nRTE textual entailment news/Wikipedia 2.5k 2\nQNLI question answering/textual entailment Wikipedia 105k 2\nMNLI textual entailment multi-domain 393k 3\nTable 1: GLUE Benchmark datasets used for evaluation.\n4.2 C ONSTRUCTING AUGMENTED NOISY TRAINING DATASETS\nMachine learning researchers or practitioners often do not know how noisy their datasets are, as input\nexamples might be corrupted or ground truth labeling might not be perfect. Therefore, it is preferable\nto use robust training objectives that can get more information out of datasets of different noise levels,\neven where there is limited amount of labeled data. We construct augmented noisy training datasets\n(used to ﬁne-tune the pre-trained language models for the task of interest) of different noise levels\nusing a back-translation model (Edunov et al., 2018), where we increase the temperature parameter\nto create more noisy examples. Back-translation refers to the procedure of translating an example\nin language A into language B and then translating it back to language A, and it is a commonly\nused data augmentation procedure for NLP applications, as the new examples obtained through\nback-translation provide targeted inductive bias to the model while preserving the meaning of the\noriginal example. Speciﬁcally, we use WMT’18 English-German and German-English translation\nmodels, use random sampling to get more diverse examples, and employ and augmentation ratio of\n1:3 for supervised examples:augmented examples. We observe that employing random sampling with\na tunable temperature parameter is critical to get diverse paraphrases for the supervised examples,\nconsistent with the previous work (Edunov et al., 2018; Xie et al., 2019), since commonly used beam\nsearch results in very regular sentences that do not provide diversity to the existing data distribution.\nWe keep the validation and test sets same with the experiments shown in Table 2.\n5 A NALYSIS AND RESULTS\n5.1 GLUE B ENCHMARK FEW-SHOT LEARNING RESULTS\nWe proposed adding the SCL term inspired by the learning strategy of humans when they are given\nfew examples. In Table 2, we report our few-shot learning results on SST-2, QNLI, and MNLI from\nthe GLUE benchmark with 20, 100, 1000 labeled training examples. Details of the experimental\nsetup are explained in Section 4. We use a very strong baseline of ﬁne-tuning RoBERTa-Large\nwith cross-entropy loss. We observe that the SCL term improves performance over the baseline\nsigniﬁcantly across all datasets and data regimes, leading to 10.7 points improvement on QNLI, 3.4\npoints improvement on MNLI, and 2.2 points improvement on SST-2, where we have 20 labeled\nexamples for ﬁne-tuning. This shows that our proposed objective is effective both for binary single\nsentence classiﬁcation such as sentiment analysis; and sentence pair classiﬁcation tasks such as\ntextual entailment and paraphrasing – when we are given only few labeled examples for the task. We\nsee that as we increase the number of labeled examples, performance improvement over the baseline\ndecreases, leading to 1.9 points improvement on MNLI for 100 examples and 0.6 points improvement\non QNLI for 1000 examples. We also would like to acknowledge that improvements over the baseline\nwhen N=1000 on both SST-2 and MNLI are not statistically signiﬁcant. In addition, we conduct\nan ablation study where we investigate the importance of l2 normalization and temperature scaling\nwhere we replace SCL loss with CE loss but keep the l2 normalization and temperature scaling, as\nshown in Table 10 in the Appendix under the method name CE+CE.\nIn Figure 2, we show tSNE plots of the learned representations of the CLS embeddings on SST-2 test\nset when RoBERTa-Large is ﬁne-tuned with 20 labeled examples, comparing CE with and without\nthe SCL term. We can clearly see that the SCL term enforces more compact clustering of examples\nwith the same label; while the distribution of the embeddings learned with CE is close to random.\nWe include a more detailed comparison for CE and CE+SCL showing learned representations of\n6\nPublished as a conference paper at ICLR 2021\nexamples as tSNE plots, where we have 20, 100 labeled examples and full dataset respectively for\nﬁne-tuning in Figure 3 in the Appendix.\nModel Loss N SST-2 QNLI MNLI\nRoBERTaLarge CE 20 85.9 ±2.1 65.0 ±2.0 39.3±2.5\nRoBERTaLarge CE + SCL 20 88.1±3.3 75.7 ±4.8 42.7 ±4.6\np-value 5e-10 1e-46 1e-8\nRoBERTaLarge CE 100 91.1 ±1.3 81.9 ±0.4 59.2 ±2.1\nRoBERTaLarge CE + SCL 100 92.8±1.3 82.5 ±0.4 61.1 ±3.0\np-value 3e-17 1e-20 2e-4\nRoBERTaLarge CE 1000 94.0 ±0.6 89.2 ±0.6 81.4 ±0.2\nRoBERTaLarge CE + SCL 1000 94.1±0.5 89.8 ±0.4 81.5 ±0.2\np-value 0.6 1e-12 0.5\nTable 2: Few-shot learning test results on the GLUE benchmark where we have N=20,100,1000\nlabeled examples for training. Reported results are the mean and the standard deviation of the test\naccuracies of the top 3 models based on validation accuracy out of 10 random training set samples,\nalong with p-values for each experiment.\nFigure 2: tSNE plots of the learned CLS embeddings on the SST-2 test set in the few-shot learning\nsetting of having 20 labeled examples to ﬁne-tune on – comparing RoBERTa-Large ﬁne-tuned with\nCE only (left) and with our proposed objective CE+SCL (right) for the SST-2 sentiment analysis task.\nBlue: positive examples; red: negative examples.\n5.2 R OBUSTNESS ACROSS AUGMENTED NOISY TRAINING DATASETS\nIn Table 3, we report our results on augmented noisy training sets with varying levels of noise. We\nhave 100 labeled examples for ﬁne-tuning for each task, and we augment their training sets with\nnoisy examples using a back-translation model, as described in detail in Section 4.2. Note that we\nuse the back-translation model to simulate training datasets of varying noise levels and not as a\nmethod to boost model performance. Experimental setup follows what is described in Section 4 for\nfew-shot learning experiments. T is the temperature for the back-translation model used to augment\nthe training sets, and higher temperature corresponds to more noise in the augmented training set.\nWe observe consistent improvements over the RoBERTa-Large baseline with our proposed objective\nacross all datasets across all noise levels, with 0.4 points improvement on SST-2, 2.5 points improve-\nment on QNLI, and 7 points improvement on MNLI on average across augmented training sets. The\nimprovement is particularly signiﬁcant for inference tasks (QNLI, MNLI) when the noise levels are\nhigher (higher temperature), leading to 7.7 points improvement on MNLI when T=0.7, and 4.2 points\nimprovement on QNLI when T=0.9. We show some samples of the augmented examples used in this\nrobustness experiment in Table 4. For T=0.3, examples mostly stay the same with minor changes in\ntheir phrasing, while for T=0.9, some grammatical mistakes and factual errors are introduced.\n7\nPublished as a conference paper at ICLR 2021\nDataset Loss Original T=0.3 T=0.5 T=0.7 T=0.9 Average\nSST-2 CE 91.1 ±1.3 92.0 ±1.3 91.4 ±1.0 91.7±1.3 90.0±0.5 91.3 ±1.2\nSST-2 CE + SCL 92.8±1.3 92.6 ±0.9 91.5 ±1.0 91.2±0.6 91.5±1.0 91.7 ±1.0\nQNLI CE 81.9 ±0.4 81.1 ±2.3 80.0 ±2.9 78.9 ±3.7 75.9 ±4.0 79.0 ±3.5\nQNLI CE + SCL 82.5±0.4 82.7 ±1.9 81.9 ±2.5 81.3 ±0.6 80.1 ±2.5 81.5 ±2.0\nMNLI CE 59.2 ±2.1 54.0 ±1.1 55.3 ±2.4 54.6 ±2.2 47.0 ±1.8 52.7 ±3.9\nMNLI CE + SCL 61.1±3.0 61.2 ±2.3 62.1 ±0.9 62.3 ±1.1 53.0 ±2.1 59.7 ±4.3\nTable 3: Results on the GLUE benchmark for robustness across noisy augmented training sets.\nAverage shows the average performance across augmented training sets.\nDataset Type Sentence\nSST-2 Original As possibly the best actor working in movies today.\nSST-2 Augmented (T=0.3) As perhaps the best actor who now stars in ﬁlms.\nSST-2 Original The young stars are too cute; the story and ensuing complications are too manipulative.\nSST-2 Augmented (T=0.9) The babies are too cute, the image and complications that follow too manipulative.\nQNLI Original Brain tissue is naturally soft, but can be stiffened with what liquid?\nQNLI Augmented (T=0.3) Brain tissue is omitted naturally, but with what ﬂuid it can be stiffened?\nQNLI Original In March 1968, CBS and Sony formed CBS/Sony Records, a Japanese business joint venture.\nQNLI Augmented (T=0.9) CBS was founded by CBS and Sony Records in March 1962, a Japanese company.\nMNLI Original However, the link did not transfer the user to a comment box particular to the rule at issue.\nMNLI Augmented (T=0.3) However, the link did not send the user to a comment ﬁeld speciﬁcally for the rule.\nMNLI Original Tenants could not enter the apartment complex due to a dangerous chemical spill.\nMNLI Augmented (T=0.9) Tenants were banned from entering the medical property because of a blood positive substance.\nTable 4: Sample of augmented examples with different noise levels for the robustness experiment\nshown in Table 3. Higher temperature (T) corresponds to more noise in the augmented training set.\n5.3 GLUE B ENCHMARK FULL DATASET RESULTS\nIn Table 5, we report results using our proposed objective on six downstream tasks from the GLUE\nbenchmark. We use a very strong baseline of ﬁne-tuning RoBERTa-Large with cross-entropy loss,\nwhich is currently the standard practice for the state-of-the-art NLP classiﬁcation models. Details of\nthe experimental setup are explained in Section 4.\nWe observe that adding the SCL term to the objective improves the performance over the RoBERTa-\nLarge baseline that lead to 3.1 points improvement on MRPC, 3.5 points improvement on QNLI,\nand an average improvement of 1.2 points across all 6 datasets. We conduct these experiments\nto investigate the effect of the SCL term in high-data regimes, as we observe that it’s effective in\nfew-shot learning settings. We acknowledge that only MRPC and QNLI results are statistically\nsigniﬁcant, and we report the results on the other datasets as a ﬁnding for the sake of completeness.\nWe hypothesize larger batch sizes lead to better performance, but we leave that for future work as that\nrequires additional engineering effort. We show evidence for this hypothesis in our ablation studies\nthat we show in Table 6, where we conduct the full dataset experiments for CE+SCL with the same\nexperimental setup described here for Table 5 on SST-2, CoLA, QNLI, and MNLI for batch sizes\n16, 64, and 256 using RoBERTa-Base. We observe that as we increase the batch size, performance\nimproves signiﬁcantly across all datasets. Speciﬁcally, we observe 0.3 points improvement on SST-2,\n0.8 points improvement on CoLA, 0.4 points improvement on QNLI, and 1.3 points improvement on\nMNLI, when we increase the batch size from 16 to 256 for CE+SCL. We also investigate the effect\nof SCL term in the overall training speed, and we measure that with average updates per second\nmetric, shown in Table 6. For batch size 16, the batch size we use throughout the paper across all\nexperimental settings, effect of SCL is negligible – decreasing average updates per second from 15.9\nto 15.08. As we increase the batch size, effect of SCL to training speed becomes more signiﬁcant –\ndecreasing average updates per second from 2.46 to 1.54 for batch size 256. In addition, we conduct\nan ablation study where we investigate the importance of l2 normalization and temperature scaling\nwhere we replace SCL loss with CE loss but keep the normalization and scaling (denoted as CE+CE)\nboth for full dataset results in Table 8, and for batch size ablation in Table 9 in the Appendix.\n8\nPublished as a conference paper at ICLR 2021\nModel Loss SST-2 CoLA MRPC RTE QNLI MNLI Avg\nRoBERTaLarge CE 96.0 ±0.4 86.0±0.5 86.4±2.4 85.5±1.8 90.4±0.8 88.4±1 88.8\nRoBERTaLarge CE + SCL 96.3±0.4 86.1±0.8 89.5±0.9 85.7±0.5 93.9±0.7 88.6±0.7 90\np-value 0.07 0.63 0.01 0.06 0.01 0.16\nTable 5: Test results on the validation set of GLUE benchmark. We compare ﬁne-tuning RoBERTa-\nLarge with CE with and without SCL. Best hyperparameter conﬁguration picked based on average\nvalidation accuracy. We report average accuracy across 10 seeds for the model with best hyperparam-\neter conﬁguration, its standard deviation, and p-values.\nModel Loss Bsz SST-2 CoLA QNLI MNLI Avg ups/sec\nRoBERTaBase CE 16 94.1 ±0.5 83.3 ±0.7 88.2 ±0.8 84 ±0.6 15.9\nRoBERTaBase CE + SCL 16 94.9 ±0.6 83.7 ±0.9 92.5 ±0.4 85.3 ±0.5 15.08\nRoBERTaBase CE 64 94.2 ±0.4 83.3 ±0.5 89.2 ±0.5 84 ±0.4 8.43\nRoBERTaBase CE + SCL 64 94.7 ±0.2 83.8 ±0.6 92.6 ±0.5 85.7 ±0.7 7.44\nRoBERTaBase CE 256 94.1 ±0.4 84 ±0.5 90 ±0.7 84.4 ±0.6 2.46\nRoBERTaBase CE + SCL 256 95.2±0.3 84.5 ±0.5 92.9 ±0.3 86.6 ±0.6 1.54\nTable 6: Ablation study on performance and training speed shown as average updates per second\n(Avg ups/sec) for ﬁne-tuning RoBERTa-Base with respect to the batch size (Bsz).\n5.4 G ENERALIZATION ABILITY OF TASK MODELS\nIn this experiment, we ﬁrst ﬁne-tune RoBERTa-Large on SST-2 using its full training set and get\na task model with and without SCL term. Then, we transfer this task model to two related single\nsentence sentiment analysis binary classiﬁcation tasks for the movie reviews domain – Amazon-2\nand Yelp-2 (Zhang et al., 2015). For both, we sample 20 labeled examples for each class, and follow\nthe few-shot learning experimental setup described in Section 4. In Table 7, we demonstrate that\nusing the SCL term for both source (SST-2) and target domains (Amazon-2, Yelp-2) lead to better\ngeneralization ability, with 2.9 points improvement on Amazon-2 and 0.4 points improvement on\nYelp-2 along with signiﬁcant reduction in variance across training set samples.\nModel Loss N Amazon-2 Yelp-2\nRoBERTaLarge CE 40 87.4 ±6.4 90.8 ±2.2\nRoBERTaLarge CE + SCL 40 90.3±0.6 91.2 ±0.4\nTable 7: Generalization of the SST-2 task model (ﬁne-tuned using the full training set) to related\ntasks (Amazon-2, Yelp-2) where there are 20 labeled examples for each class.\n6 C ONCLUSION\nWe propose a supervised contrastive learning objective for ﬁne-tuning pre-trained language models\nand demonstrate signiﬁcant improvements over a strong RoBERTa-Large baseline on multiple\ndatasets of the GLUE benchmark in the few-shot learning settings. We also show that our proposed\nobjective leads to models that are more robust to different levels of noise in the training data and can\ngeneralize better to related tasks with limited labeled task data. Currently, data augmentation methods\nin NLP and their effects on the downstream tasks are neither as effective nor as well understood\nas their counterparts in the computer vision domain. In future work, we plan to study principled\nand automated data augmentation techniques for NLP that would allow extending our supervised\ncontrastive learning objective to both semi-supervised and self-supervised learning settings.\nREFERENCES\nArmen Aghajanyan, Akshat Shrivastava, Anchit Gupta, Naman Goyal, Luke Zettlemoyer, and Sonal\nGupta. Better ﬁne-tuning by reducing representational collapse. ArXiv, abs/2008.03156, 2020.\n9\nPublished as a conference paper at ICLR 2021\nPhilip Bachman, R. Devon Hjelm, and William Buchwalter. Learning representations by maximizing\nmutual information across views. In NeurIPS, 2019.\nAlexei Baevski, Henry Zhou, Abdelrahman Mohamed, and Michael Auli. wav2vec 2.0: A framework\nfor self-supervised learning of speech representations. In NeurIPS, 2020.\nSamuel R. Bowman, Gabor Angeli, Christopher Potts, and Christopher D. Manning. A large annotated\ncorpus for learning natural language inference. In EMNLP, 2015.\nKaidi Cao, Colin Wei, Adrien Gaidon, N. Ar´echiga, and Tengyu Ma. Learning imbalanced datasets\nwith label-distribution-aware margin loss. In NeurIPS, 2019.\nTing Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. A simple framework for\ncontrastive learning of visual representations. In ICML, 2020a.\nTing Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey E. Hinton. Big\nself-supervised models are strong semi-supervised learners. In NeurIPS, 2020b.\nAlexis Conneau, Alexei Baevski, Ronan Collobert, Abdelrahman Mohamed, and Michael Auli.\nUnsupervised cross-lingual representation learning for speech recognition. arXiv preprint\narXiv:2006.13979, 2020.\nE. Cubuk, Barret Zoph, Dandelion Man´e, V . Vasudevan, and Quoc V . Le. Autoaugment: Learning\naugmentation strategies from data. 2019 IEEE/CVF Conference on Computer Vision and Pattern\nRecognition (CVPR), pp. 113–123, 2019.\nE. D. Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V . Le. Randaugment: Practical automated\ndata augmentation with a reduced search space. 2020 IEEE/CVF Conference on Computer Vision\nand Pattern Recognition Workshops (CVPRW), pp. 3008–3017, 2020.\nJ. Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep\nbidirectional transformers for language understanding. In NAACL-HLT, 2019.\nJesse Dodge, Gabriel Ilharco, Roy Schwartz, Ali Farhadi, Hannaneh Hajishirzi, and Noah A. Smith.\nFine-tuning pretrained language models: Weight initializations, data orders, and early stopping.\nArXiv, abs/2002.06305, 2020.\nSergey Edunov, Myle Ott, Michael Auli, and David Grangier. Understanding back-translation at\nscale. In EMNLP, 2018.\nGamaleldin F. Elsayed, Dilip Krishnan, Hossein Mobahi, Kevin Regan, and Samy Bengio. Large\nmargin deep networks for classiﬁcation. In NeurIPS, 2018.\nHongchao Fang and Pengtao Xie. Cert: Contrastive self-supervised learning for language understand-\ning. ArXiv, abs/2005.12766, 2020.\nM. Gutmann and A. Hyv ¨arinen. Noise-contrastive estimation of unnormalized statistical models,\nwith applications to natural image statistics. J. Mach. Learn. Res., 13:307–361, 2012.\nTengda Han, Weidi Xie, and Andrew Zisserman. Video representation learning by dense predictive\ncoding. 2019 IEEE/CVF International Conference on Computer Vision Workshop (ICCVW), pp.\n1483–1492, 2019.\nKaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition.\n2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778, 2016.\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross B. Girshick. Momentum contrast for\nunsupervised visual representation learning. 2020 IEEE/CVF Conference on Computer Vision and\nPattern Recognition (CVPR), pp. 9726–9735, 2020.\nOlivier J. H´enaff, A. Srinivas, J. Fauw, Ali Razavi, C. Doersch, S. Eslami, and A. Oord. Data-efﬁcient\nimage recognition with contrastive predictive coding. ArXiv, abs/1905.09272, 2019.\nDan Hendrycks and Thomas G. Dietterich. Benchmarking neural network robustness to common\ncorruptions and perturbations. In ICLR, 2019.\n10\nPublished as a conference paper at ICLR 2021\nK. Hermann, Tom´as Kocisk´y, Edward Grefenstette, Lasse Espeholt, W. Kay, Mustafa Suleyman, and\nP. Blunsom. Teaching machines to read and comprehend. In NeurIPS, 2015.\nGeoffrey E. Hinton, Oriol Vinyals, and J. Dean. Distilling the knowledge in a neural network. In\nNeurIPS Deep Learning and Representation Learning Workshop, 2015.\nR. Devon Hjelm, A. Fedorov, Samuel Lavoie-Marchildon, Karan Grewal, Adam Trischler, and Yoshua\nBengio. Learning deep representations by mutual information estimation and maximization. In\nICLR, 2019.\nJeremy Howard and Sebastian Ruder. Universal language model ﬁne-tuning for text classiﬁcation. In\nProceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume\n1: Long Papers), volume 1, pp. 328–339, 2018.\nHaoming Jiang, Pengcheng He, Weizhu Chen, Xiaodong Liu, Jianfeng Gao, and Tuo Zhao. Smart:\nRobust and efﬁcient ﬁne-tuning for pre-trained natural language models through principled regu-\nlarized optimization. In ACL, 2020.\nPrannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron\nMaschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. In NeurIPS, 2020.\nTushar Khot, A. Sabharwal, and Peter Clark. Scitail: A textual entailment dataset from science\nquestion answering. In AAAI, 2018.\nByeongchang Kim, Hyunwoo Kim, and Gunhee Kim. Abstractive summarization of reddit posts\nwith multi-level memory networks. In NAACL-HLT, 2019.\nA. Kolesnikov, Xiaohua Zhai, and Lucas Beyer. Revisiting self-supervised visual representation\nlearning. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n1920–1929, 2019.\nA. Krizhevsky. Learning multiple layers of features from tiny images. 2009.\nHao Liu and P. Abbeel. Hybrid discriminative-generative training via contrastive learning. ArXiv,\nabs/2007.09070, 2020.\nWeiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolu-\ntional neural networks. In ICML, 2016.\nY . Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, M. Lewis,\nLuke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach.\nArXiv, abs/1907.11692, 2019.\nI. Misra and L. V . D. Maaten. Self-supervised learning of pretext-invariant representations. 2020\nIEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 6706–6716,\n2020.\nA. Mnih and K. Kavukcuoglu. Learning word embeddings efﬁciently with noise-contrastive estima-\ntion. In NeurIPS, 2013.\nMarius Mosbach, Maksym Andriushchenko, and Dietrich Klakow. On the stability of ﬁne-tuning\nbert: Misconceptions, explanations, and strong baselines. ArXiv, abs/2006.04884, 2020.\nR. M¨uller, Simon Kornblith, and Geoffrey E. Hinton. When does label smoothing help? In NeurIPS,\n2019.\nCourtney Napoles, Matthew R. Gormley, and Benjamin Van Durme. Annotated gigaword. In\nAKBC-WEKEX@NAACL-HLT, 2012.\nK. Nar, O. Ocal, S. Sastry, and K. Ramchandran. Cross-entropy loss and low-rank features have\nresponsibility for adversarial examples. ArXiv, abs/1901.08360, 2019.\nAndrew Y . Ng and Michael I. Jordan. On discriminative vs. generative classiﬁers: A comparison of\nlogistic regression and naive bayes. In NeurIPS, 2001.\n11\nPublished as a conference paper at ICLR 2021\nYixin Nie, Adina Williams, Emily Dinan, Mohit Bansal, J. Weston, and Douwe Kiela. Adversarial\nnli: A new benchmark for natural language understanding. 2020.\nA. Oord, Y . Li, and Oriol Vinyals. Representation learning with contrastive predictive coding.ArXiv,\nabs/1807.03748, 2018.\nMyle Ott, Sergey Edunov, Alexei Baevski, Angela Fan, Sam Gross, Nathan Ng, David Grangier,\nand Michael Auli. fairseq: A fast, extensible toolkit for sequence modeling. In Proceedings of\nNAACL-HLT 2019: Demonstrations, 2019.\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models\nare unsupervised multitask learners. 2019.\nRajat Raina, Yirong Shen, Andrew Y . Ng, and Andrew McCallum. Classiﬁcation with hybrid\ngenerative/discriminative models. In NeurIPS, 2003.\nOlga Russakovsky, J. Deng, H. Su, J. Krause, S. Satheesh, S. Ma, Zhiheng Huang, A. Karpathy,\nA. Khosla, M. Bernstein, A. Berg, and Li Fei-Fei. Imagenet large scale visual recognition challenge.\nInternational Journal of Computer Vision, 115:211–252, 2015.\nNikunj Saunshi, Orestis Plevrakis, Sanjeev Arora, Mikhail Khodak, and Hrishikesh Khandeparkar. A\ntheoretical analysis of contrastive unsupervised representation learning. volume 97 of Proceedings\nof Machine Learning Research, pp. 5628–5637, Long Beach, California, USA, 09–15 Jun 2019.\nPMLR. URL http://proceedings.mlr.press/v97/saunshi19a.html.\nFlorian Schroff, D. Kalenichenko, and J. Philbin. Facenet: A uniﬁed embedding for face recognition\nand clustering. 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp.\n815–823, 2015.\nKihyuk Sohn. Improved deep metric learning with multi-class n-pair loss objective. In NeurIPS,\n2016.\nSainbayar Sukhbaatar, Joan Bruna, Manohar Paluri, Lubomir D. Bourdev, and Rob Fergus. Training\nconvolutional networks with noisy labels. In ICLR, 2015.\nChristian Szegedy, V . Vanhoucke, S. Ioffe, Jon Shlens, and Z. Wojna. Rethinking the inception archi-\ntecture for computer vision. 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), pp. 2818–2826, 2016.\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In ECCV, 2020.\nAlex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman.\nGLUE: A multi-task benchmark and analysis platform for natural language understanding. In\nICLR, 2019. URL https://openreview.net/forum?id=rJ4km2R5t7.\nZhirong Wu, Yuanjun Xiong, S. Yu, and D. Lin. Unsupervised feature learning via non-parametric\ninstance discrimination. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition,\npp. 3733–3742, 2018.\nQizhe Xie, Zihang Dai, Eduard H. Hovy, Minh-Thang Luong, and Quoc V . Le. Unsupervised data\naugmentation for consistency training. arXiv: Learning, 2019.\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V Le. Self-training with noisy student\nimproves imagenet classiﬁcation. In Proceedings of the IEEE/CVF Conference on Computer Vision\nand Pattern Recognition, pp. 10687–10698, 2020.\nI Zeki Yalniz, Herv´e J´egou, Kan Chen, Manohar Paluri, and Dhruv Mahajan. Billion-scale semi-\nsupervised learning for image classiﬁcation. arXiv preprint arXiv:1905.00546, 2019.\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon\nYoo. Cutmix: Regularization strategy to train strong classiﬁers with localizable features. 2019\nIEEE/CVF International Conference on Computer Vision (ICCV), pp. 6022–6031, 2019.\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. ArXiv, abs/1605.07146, 2016.\n12\nPublished as a conference paper at ICLR 2021\nHongyi Zhang, M. Ciss ´e, Yann Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk\nminimization. In ICLR, 2018.\nTianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Weinberger, and Yoav Artzi. Revisiting few-sample\nbert ﬁne-tuning. ArXiv, abs/2006.05987, 2020.\nX. Zhang, J. Zhao, and Y . LeCun. Character-level convolutional networks for text classiﬁcation. In\nNeurIPS, 2015.\nZhilu Zhang and Mert R. Sabuncu. Generalized cross entropy loss for training deep neural networks\nwith noisy labels. In NeurIPS, 2018.\n13\nPublished as a conference paper at ICLR 2021\nA A PPENDIX\nFigure 3: tSNE plots of learned CLS embedding on SST-2 test set where we have 20, 100 labeled\nexamples, and full dataset respectively, comparing CE with and without SCL term. Blue: positive\nexamples; red: negative examples.\nModel Loss SST-2 CoLA MRPC RTE QNLI MNLI Avg\nRoBERTaLarge CE 96.0 ±0.4 86.0±0.5 86.4±2.4 85.5±1.8 90.4±0.8 88.4±1 88.8\nRoBERTaLarge CE + SCL 96.3±0.4 86.1±0.8 89.5±0.9 85.7±0.5 93.9±0.7 88.6±0.7 90\np-value 0.07 0.63 0.01 0.06 0.01 0.16\nRoBERTaLarge CE + CE 96 ±0.4 86.3 ±0.4 89 ±1 84.9 ±1 93.9 ±0.8 89±1 89.9\np-value 0.39 0.13 0.01 0.1 0.01 0.12\nRoBERTaLarge Khosla et al. (2020) 96±0.3 86.7±1 89.3±1.2 85.2 ±1 92.4 ±0.7 88.8 ±0.9 89.7\np-value 0.4 0.42 0.01 0.22 0.01 0.13\nTable 8: Test results on the validation set of GLUE benchmark. We compare ﬁne-tuning RoBERTa-\nLarge with CE with and without SCL, CE+CE and the two-stage method of Khosla et al. (2020). Best\nhyperparameter conﬁguration is picked based on the average validation accuracy. We report average\naccuracy across 10 seeds for the model with the best hyperparameter conﬁguration, its standard\ndeviation, and p-values. CE+CE refers to the case where we replace SCL loss with the CE loss but\nkeep l2 normalization and temperature scaling.\n14\nPublished as a conference paper at ICLR 2021\nModel Loss Bsz SST-2 CoLA QNLI MNLI Avg ups/sec\nRoBERTaBase CE 16 94.1 ±0.5 83.3 ±0.7 88.2 ±0.8 84 ±0.6 15.9\nRoBERTaBase CE + SCL 16 94.9 ±0.6 83.7 ±0.9 92.5 ±0.4 85.3 ±0.5 15.08\nRoBERTaBase CE + CE 16 94.8 ±0.7 83.6 ±0.4 91.6 ±0.5 85 ±0.3 15.25\nRoBERTaBase CE 64 94.2 ±0.4 83.3 ±0.5 89.2 ±0.5 84 ±0.4 8.43\nRoBERTaBase CE + SCL 64 94.7 ±0.2 83.8 ±0.6 92.6 ±0.5 85.7 ±0.7 7.44\nRoBERTaBase CE + CE 64 94.6 ±0.7 83.5 ±0.6 92.1 ±0.8 85 ±0.8 7.64\nRoBERTaBase CE 256 94.1 ±0.4 84 ±0.5 90 ±0.7 84.4 ±0.6 2.46\nRoBERTaBase CE + SCL 256 95.2±0.3 84.5 ±0.5 92.9 ±0.3 86.6 ±0.6 1.54\nRoBERTaBase CE + CE 256 94.3 ±0.5 83.5 ±0.3 91.9 ±0.4 84.6 ±0.8 1.77\nTable 9: Ablation on performance and ﬁne-tuning speed shown as average updates per second (Avg\nups/sec) for ﬁne-tuning RoBERTa-Base with respect to the batch size (Bsz). CE+CE refers to the\ncase where we replace SCL loss with the CE loss but keep l2 normalization and temperature scaling.\nModel Loss N SST-2 QNLI MNLI\nRoBERTaLarge CE 20 85.9 ±2.1 65.0 ±2.0 39.3±2.5\nRoBERTaLarge CE + SCL 20 88.1±3.3 75.7 ±4.8 42.7 ±4.6\np-value 5e-10 1e-46 1e-8\nRoBERTaLarge CE + CE 20 86.5 ±2.2 75.1 ±3.5 40.8 ±3.7\np-value 0.03 4e-68 3e-4\nRoBERTaLarge CE 100 91.1 ±1.3 81.9 ±0.4 59.2 ±2.1\nRoBERTaLarge CE + SCL 100 92.8±1.3 82.5 ±0.4 61.1 ±3.0\np-value 3e-17 1e-20 2e-4\nRoBERTaLarge CE + CE 100 91.7 ±0.5 81.7 ±0.5 56 ±4.0\np-value 1e-4 3e-4 2e-8\nRoBERTaLarge CE 1000 94.0 ±0.6 89.2 ±0.6 81.4 ±0.2\nRoBERTaLarge CE + SCL 1000 94.1±0.5 89.8 ±0.4 81.5 ±0.2\np-value 0.6 1e-12 0.5\nRoBERTaLarge CE + CE 1000 94 ±0.7 89.3 ±1 81.2 ±0.2\np-value 0.78 0.06 0.12\nTable 10: Few-shot learning test results on the GLUE benchmark where we have N=20,100,1000\nlabeled examples for ﬁne-tuning. Reported results are the mean and the standard deviation of the\ntest accuracies of the top 3 models based on the validation accuracy out of 10 random training set\nsamples, along with p-values for each experiment. CE+CE refers to the case where we replace SCL\nloss with the CE loss but keep l2 normalization and temperature scaling.\n15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7626043558120728
    },
    {
      "name": "Cross entropy",
      "score": 0.712165117263794
    },
    {
      "name": "Artificial intelligence",
      "score": 0.645849883556366
    },
    {
      "name": "Machine learning",
      "score": 0.5027022361755371
    },
    {
      "name": "Entropy (arrow of time)",
      "score": 0.4935656785964966
    },
    {
      "name": "Generalization",
      "score": 0.4928476810455322
    },
    {
      "name": "Fine-tuning",
      "score": 0.4466014802455902
    },
    {
      "name": "Natural language processing",
      "score": 0.3558711111545563
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.34628868103027344
    },
    {
      "name": "Mathematics",
      "score": 0.10077917575836182
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I97018004",
      "name": "Stanford University",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I145311948",
      "name": "Johns Hopkins University",
      "country": "US"
    }
  ]
}