{
  "title": "SelfCP: Compressing over-limit prompt via the frozen large language model itself",
  "url": "https://openalex.org/W4399116278",
  "year": 2024,
  "authors": [
    {
      "id": "https://openalex.org/A1966621648",
      "name": "Gao Jun",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2369550054",
      "name": "Cao, Ziqiang",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A1967989049",
      "name": "Li Wenjie",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W6840426214",
    "https://openalex.org/W6779709467",
    "https://openalex.org/W3180037928",
    "https://openalex.org/W4385800286",
    "https://openalex.org/W6682631176",
    "https://openalex.org/W4285247752",
    "https://openalex.org/W4391876619",
    "https://openalex.org/W6851812595",
    "https://openalex.org/W4390576078",
    "https://openalex.org/W4391589750",
    "https://openalex.org/W6809443998"
  ],
  "abstract": null,
  "full_text": "Highlights\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nJun Gao,Ziqiang Cao,Wenjie Li\nâ€¢ Toourknowledge,wearethefirsttousethefrozenLLMitselftocompressover-limitpromptsinto1/12memory\ntokens, which is more general and less expensive.\nâ€¢ We propose a more dialectical prompt compression perspective that achieves balances among training cost,\ninference efficiency, and generation quality,\narXiv:2405.17052v2  [cs.CL]  18 Jun 2024\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large\nLanguage Model Itselfâ‹†,â‹†â‹†\nJun Gao, Ziqiang Caoâˆ— and Wenjie Li\nInstitute of Artificial Intelligence, Soochow University, Suzhou, China\nARTICLE INFO\nKeywords:\nLarge Language Models\nPrompt Compression\nEfficient/Low-Resource Methods\nABSTRACT\nLong prompt leads to huge hardware costs when using transformer-based Large Language\nModels (LLMs). Unfortunately, many tasks, such as summarization, inevitably introduce long\ndocuments, and the wide application of in-context learning easily makes the prompt length\nexplode. This paper proposes a Self-Compressor (SelfCP), which employs the target LLM\nitself to compress over-limit prompts into dense vectors while keeping the allowed prompts\nunmodified. Dense vectors are then projected into dense tokens via a learnable connector to\nmake the same LLM unburden to understand. The connector is supervised-tuned under the\nlanguagemodelingobjectiveoftheLLMonrelativelylongtextsselectedfrompubliclyaccessed\ndatasets, involving an instruction dataset to make SelfCP respond to various prompts, while\nthe target LLM keeps frozen during training. We build the lightweight SelfCP upon 2 different\nbackboneswithmerely17Mlearnableparametersoriginatingfromtheconnectorandalearnable\nembedding. Evaluation on both English and Chinese benchmarks demonstrate that SelfCP\neffectively substitutes 12Ã—over-limit prompts with dense tokens to reduce memory costs and\nbooster inference throughputs, yet improving response quality. The outstanding performance\nbringsanefficientsolutionforLLMstotacklelongpromptswithouttrainingLLMsfromscratch.\n1. Introduction\nFigure 1:SelfCP generates memory tokens for each segment to substitute the original in-context demonstrations, guiding\nthe LLM to respond to the query correctly.\nTransformer-based Large Language Models (LLMs) exhibit general spectacular emergent abilities Wang, Xie,\nDing, Feng and Xia (2023b); Yang, Li, Zhang, Chen and Cheng (2023); Wei, Cui, Cheng, Wang, Zhang, Huang, Xie,\nXu, Chen, Zhang et al. (2023); Wang, Liang, Meng, Shi, Li, Xu, Qu and Zhou (2023a). However, the performance\nâ‹†\nThe work described in this paper was supported by the National Natural Science Foundation of China (NSFC 62106165) and Project Funded\nby the Priority Academic Program Development of Jiangsu Higher Education Institutions.\nâ‹†â‹†\nThe code and data has been released in https://github.com/jungao1106/SelfCP\nâˆ—Corresponding author\njunegao1106@gmail.com (J. Gao);zqcao@suda.edu.cn (Z. Cao);cswjli@comp.polyu.edu.hk (W. Li)\nORCID(s):0000-0002-1077-9033 (Z. Cao)\nJun Gao et al.:Preprint submitted to Elsevier Page 1 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nof LLMs heavily relies on well-designed prompts, while lengthy prompts result in memory explosion and bring other\nefficiency problems. Unfortunately, prompts for many tasks have to include long inputs, such as question answering\n(QA) Ghosal, Shen, Majumder, Mihalcea and Poria (2022); Li, Yang, Wang, Wei and Li (2023b), summarization\nNarayan,CohenandLapata(2018);Sun,Yuan,Li,CaoandLi(2024);Wang,Zhao,Ji,Jiang,Li,HuandLu(2024),and\nquery-focused summarization (QFS) Copeck, Inkpen, Kazantseva, Kennedy, Kipp, Nastase and Szpakowicz (2006).\nMeanwhile, the wide application of In-Context Learning (ICL) further surges the length of prompts by introducing\nmany demonstrations.\nExceptforeffortsinexpandingtheinputwindowZheng,WangandKong(2022);Wu,Rabe,HutchinsandSzegedy\n(2022); Ding, Ma, Dong, Zhang, Huang, Wang and Wei (2023); Bulatov, Kuratov, Kapushev and Burtsev (2023),\nprevious works focus more on prompt pruning Jiang, Wu, Lin, Yang and Qiu (2023) or replacing prompts with soft\nprompts Wingate, Shoeybi and Sorensen (2022); Ge, Hu, Wang, Chen and Wei (2023); Mu, Li and Goodman (2023).\nHowever, LLMs with million context windows still struggle to overcome performance degradation Liu, Lin, Hewitt,\nParanjape, Bevilacqua, Petroni and Liang (2024) in addressing long prompts. Prompt pruning methods are faced with\nOut-of-Distribution(OoD)problems,requiringexpensivealignmentbetweencompressorsandtargetLLMsLi(2023);\nJiangetal.(2023).Previoussoftpromptcompressionmethodsrespondtoqueriesmerelyconditionedonsoftprompts\nwiththepursuittominimizepromptlengthGeetal.(2023),whichincreasesgenerationdifficultyandrequirestraining\nthe model from scratch Mu et al. (2023); Wingate et al. (2022). These challenges form an impossible triangle: we\ncannot simultaneously manage training costs, inference latency, and response quality. Distinguished from previous\nattempts on soft prompt compression, we provide a dialectical perspective to compress prompts, achieving a tradeoff\namong these factors.\nWith these goals in mind, we propose SelfCP as illustrated in Figure 1, which leverages the comprehension\ncapabilities of LLMs developed during pre-training to compress over-limit prompts. Therefore, an extra compression\nmodule is not necessary to introduce since SelfCP employs the target LLM to compress prompts, reducing GPU\nmemory for both training and inference initially. Furthermore, motivated by the success of Perceiver-based Visual\nLanguage Models (VLMs) that extracted visual features via a frozen Visual Transformer (ViT) Dosovitskiy, Beyer,\nKolesnikov,Weissenborn,Zhai,Unterthiner,Dehghani,Minderer,Heigold,Gellyetal.(2020),wekeepthetargetLLM\nfrozenduringtraining 1,reducingthemajortrainingcostsfromtheunderlyingbackbone.However,asthebuilt-inLLM\nofVLMfailstounderstandthevisualfeaturesextractedfromViTwithoutanadapter(e.g.,Q-FormerLi,Li,Savarese\nandHoi(2023a)orMulit-LayerPerceptron(MLP)),aconnectorisimportantforSelfCPtoconverttheoutputfeatures\nof the compressor into readable compressed tokens (called memory tokens in this paper) for the generator. We simply\nutilizealinearlayersupervised-tunedunderthelanguagemodelingobjectiveofthegenerator,servingastheconnector\nbetween the compressor and the generator, and we call for more exploration for the selection of the adapter. Previous\nstudies responded to the query conditioned on plain soft prompts, which increased task difficulty, training costs, and\nwasted reasonable context window usage of their generator. Inspired by the fact that some segments within prompts\nare valuable and informative while compressing them seems not necessary2, we explore an intermediate solution that\nallows SelfCP to only compress over-limit prompts (e.g., the originally truncated or relatively less important ones)\nrather than compressing everything casually. The rest unmodified prompts are then fed into the generator directly.\nThis solution takes full advantage of the allowed windows wasted in the previous studies and reduces the\ncompression and generation difficulty, but it is still faced with the following challenges: (1) The compressed soft\npromptcanâ€™tbeposedinfrontalwayslikebefore,sincetheover-limitpromptshavethepotentialtobeboththeformer\nand the latter part. (2) The over-limit prompts may be still too long for the compressor to accept while dropping\ndirectly conflicts with our goal. Distinguished from previous studies fixing the compression mode, we diversify the\ncompression strategies to cater to further requirements in various scenarios to approach these challenges: (a)Former\nCompressioncompressestheformer halfofthepromptandputsthememorytokensinfrontofthelatter uncompressed\none. (b)Latter Compressioncompresses the laterhalf of prompts and puts the memory tokens behind the former\nuncompressed one. (c)Concatenated Compressioncompress several sub-prompts (e.g., in-context demonstrations)\ninto local memory tokens independently and concatenate them to formulate global memory tokens. We build SelfCP\nupon two backbones (Vicuna-7b and BlueLM-7b) to verify the generalizability and evaluate SelfCP across of a scope\nout-domaintasksincludinggenerativeandunderstandingtasks.Then,thein-domainvalidationexperimentsshowthat\nSelfCP optimized with the three compression strategies effectively substitutes the12Ã—over-limit prompt with soft\nprompts. Our main contributions are as follows:\n1For clarity, the frozen LLM plays two roles in SelfCP for compression and generation is calledcompressorand generatorfollowing.\n2The former part of documents in text summarization are empirically more important and more informative.\nJun Gao et al.:Preprint submitted to Elsevier Page 2 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nâ€¢ Toourknowledge,wearethefirsttousethefrozenLLMitselftocompressover-limitpromptsinto1/12memory\ntokens, which is more general and less expensive.\nâ€¢ We propose a more dialectical prompt compression perspective that achieves balances among training cost,\ninference efficiency, and generation quality,\n2. Related Work\n2.1. Soft Prompt\nDifferentfromnormalpromptsconsistingofdiscreteactualtokens,eachcorrespondingtopre-trainedembedding,\nsoft prompts were continuous newly initialed embeddings. On the one hand, soft prompts were usually used as a\nparameter-efficient method, such as Prefix-TuningLi and Liang (2021) and P-Tuning Liu, Ji, Fu, Tam, Du, Yang\nand Tang (2022), while keeping the backbone frozen and tuning the newly initialized embeddings for each task.\nOn the other hand, researchers attempted to utilize soft prompts to compact prompts from concrete sequences to\nvirtual tokens. Mostly from a distillation perspective, Wingate et al. (2022) aligned the teacher model and the student\nmodel, where the teacher model accepted the actual prompt while the student model fed the soft prompt. The main\ndrawback of this approach was the lack of generalization that necessitated training for each lexical different task-\nspecific instruction. To tackle the generalization problem, Mu et al. (2023) proposed to learn a Llama-7b to compress\ninstruction to virtual tokens, preceding attention past key values similar to Prefix Li and Liang (2021), but only\ncompressinstructionswasnotpowerfulenoughsincethedemonstrationsorinputwasmuchlongerthaninstructionin\nmany tasks such as summarization and QA. To compress the long prompt, Chevalier, Wettig, Ajith and Chen (2023)\nproposed AutoCompressor to generate compressed virtual tokens based on a fine-tuned OPT-2.7b Zhang, Roller,\nGoyal, Artetxe, Chen, Chen, Dewan, Diab, Li, Lin et al. (2022). They first randomized segmented the texts with\nthousands of words into model-accepted range and then recursively generated soft prompts for each segment, and\ntheprevioussoftpromptswouldbeconcatenatedwiththecurrentsegmenttogeneratenewsoftprompts.However,not\nonly did AutoCompressor require fine-tuning on a large training set, but also encoding soft prompts would be much\nslower and bring extra memory cost due to Recurrent Memory Transformer (RMT) Bulatov, Kuratov and Burtsev\n(2022).Similarly,Geetal.(2023)proposedICAEthatemployedaLoRA-adoptedLlama-7bTouvron,Lavril,Izacard,\nMartinet, Lachaux, Lacroix, RoziÃ¨re, Goyal, Hambro, Azhar et al. (2023) to compress the plain processed prompts to\ncompressedvirtualtokens.However,ICAEstillstruggledtoaddresspromptslongerthantheallowableinputwindow.\n2.2. Extractive Compression\nApart from employing soft prompts, researchers also endeavored to shorten prompts by extracting informative\ntokens from the original ones Li (2023); Jiang et al. (2023), namely token pruning Kim, Shen, Thorsley, Gholami,\nKwon, Hassoun and Keutzer (2022) or token merging Bolya, Fu, Dai, Zhang, Feichtenhofer and Hoffman (2022).\nRecent works like LLMLingua Jiang et al. (2023) and Selective Context Li (2023) shared similarities but diverged\non whether to eliminate tokens with high or low Perplexity (PPL). LLMLingua emphasized tokens with high PPL,\nattributing them as more influential, resulting in achieving state-of-the-art (SOTA) performance. As mentioned in\ntheir paper extractive compression methods encountered Out-of-Distribution (OoD) issues between the extractor and\nthe target LLM. To reconcile this, they fine-tuned Alpaca-7b Taori, Gulrajani, Zhang, Dubois, Li, Guestrin, Liang\nandHashimoto(2023)orGPT2-AlpacausingtheAlpacadatasetTaorietal.(2023)toaligntotargetLLMs.However,\nextractivecompressionmethodsheavilyhingedonthetargetLLMâ€™sabilitytounderstanddiscretetokens,andalignment\nwas usually quite expensive and tailored to each target.\n2.3. Long Input Transformer\nSignificant training costs and hardware support limited the input length for pre-trained language models. A\nseries of previous works focused on sparsifying the full attention window to linear attention Dai, Yang, Yang,\nCarbonell, Le and Salakhutdinov (2019); Child, Gray, Radford and Sutskever (2019); Beltagy, Peters and Cohan\n(2020), with a trade-off between efficiency and attention horizon. Other works approximated or replaced the entire\nattention mechanismKatharopoulos, Vyas, Pappas and Fleuret (2020); Choromanski, Likhosherstov, Dohan, Song,\nGane, Sarlos, Hawkins, Davis, Mohiuddin, Kaiser et al. (2020); Lee-Thorp, Ainslie, Eckstein and Ontanon (2021).\nHowever, rarefying and approximating or replacing changed the architecture of the standard transformer or training\nobjectiveZhong,LeiandChen(2022)andthereforenecessitatedtrainingfromscratch,whichwasexpensive,especially\nscaling the model or training sets. Bertsch, Alon, Neubig and Gormley (2023) proposed to offload the cross-attention\nJun Gao et al.:Preprint submitted to Elsevier Page 3 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nFigure 2:Differences of compression methods based on soft prompt in formulating soft prompts. SelfCP takes advantage\nof the unlimited input window in AutoCompressor and the constant compression time complexity in ICAE.\ncomputation to a singleğ‘˜-nearest-neighbor (ğ‘˜NN) index. Althoughğ‘˜NN was parameter-free, retrieving hidden states\noftheencoderineachgenerationstepwouldslowinference,andtheirapproacheswouldbreakdownwhenfacedwith\ndecoder-only models.\nSelfCP keeps the LLM frozen and has no limitation on building upon existing powerful LLMs. Hence, the above\napproaches can further empower SelfCP theoretically.\n3. Methodology\nWe propose the SelfCP, a parameter-efficient model that compresses prompts via the LLM itself. As for the\nselection of the underlying LLM, previous work has proved that the Decoder-only model performs better than the\nEncoder-DecodermodelincompressioninstructionsMuetal.(2023).WefollowthisconclusionandemployVicuna-\n7b Zheng, Chiang, Sheng, Zhuang, Wu, Zhuang, Lin, Li, Li, Xing et al. (2023) and BlueLM-7b Team (2023) as our\ntwo independent backbones, as the representation of LLMs majoring in English and Chinese.\nMost compression methods based on soft prompts innovate on the compressor. We illustrate the differences in the\nsoft prompt formulation process for SelfCP, AutoCompressor, and ICAE in Figure 2. To explain plainly, we ideally\nassume the compressor and the generator of three compression methods have the window limitation ofğ¿, have the\nsame compression ratio, and now comes a prompt has a lengthğ‘, whereğ‘ = 3Ã— ğ¿, ignoring the length of soft\nprompts and the query. Considering AutoCompressor, the prompt will be divided into three segments(ğ‘†1,ğ‘†2,ğ‘†3),\nandAutoCompressorcompresseseachsegmentstep-by-step.Notably,weprovideAutoCompressoratolerancesetting\nthat the prompt is evenly divided where they were randomly segmented originally, but AutoCompressor still requires\n3 times non-parallel compression. When it comes to ICAE, merely 1/3 prompt is accessible for the compressor and\notherswillbereadbynomeans.Inthiscase,ICAErequiresonly1timetoperformcompressionalways,sincetherest\nis dropout.\nAutoCompressor shows advantages in the readable prompt length, but is short in efficiency, while ICAE has a\nconstantcompressionperplexitybutstrugglestoapproachquitelongprompts.SelfCPlearnsfromeachotherskillfully\nthatitscompressorcompresseseachsegmentasynchronouslyandconcatenatesthemastheAutoCompressor.Although\nSelfcpcompressesonelesssegmentthanAutoCompressor,thesegmentisdirectlyprovidedtothegeneratortobalance\nthe overload between the compressor and the generator. We are inclined to consider this as a trade-off among training\ncosts, inference efficiency, and response quality. This trade-off is more worthwhile when the unmodified segment is\nmore important than others.\nThe number of practical compression steps can be calculated asâŒˆğ‘âˆ’ğ¿\nğ¿âˆ—ğ‘˜ âŒ‰, whereğ‘˜ indicates that a single GPU is\ncapableofcompressing ğ‘˜segmentsinabatch.WhentheGPUcapacityissufficient, ğ‘˜equalsâŒˆğ‘\nğ¿ âŒ‰,whichisthescenario\nof ICAE that compresses all segments in a time but drops nothing, while when the GPU capacity is only sufficient to\nsetğ‘˜ =1, it degenerates to the AutoCompressor scenario that compress segments step by step.\nJun Gao et al.:Preprint submitted to Elsevier Page 4 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\n3.1. Training\nThe key to SelfCP is the underlying target LLM plays the role of both the compressor and the generator while\nkeeping frozen during training. Except for the learnable linear layer, we introduce a learnable special embedding for\nSelfCP, the memory tag[ğ‘€], initialized from a rarely used embedding of the target LLM. To cater to future practical\nemployment, We introduce the involved compressing strategies during training:\n3.1.1. Former & Latter Compression\nPrevious compressed methods based on soft prompts place the soft prompt at the beginning like Prefix-Tuning\nwhich ignores the basic fact that practical queries have no strict relative position relationship with corresponding\nprompts. For example, we can formulate the input asSummarize the document below: [DOC]or [DOC] Summarize\nthe document above. Thereby, we introduce Former Compression and Later Compression instances into training.\nSpecifically, for prompt instances shorter than2Ã— ğ¿ tokens, namely, two times window limitation, we evenly split\nthem into two segments[ğ‘†ğ‘,ğ‘†ğ‘¢]and randomly compress the former or the latter.\nGiven the segment to be compressedğ‘†ğ‘, the memory tag sequenceîˆ¹ = [ğ‘€]Ã— ğ‘˜, and the current queryğ‘„, we\nformulate the input of compressorğ¼ğ¶ asğ¼ğ¶ =ğ‘„ âŠ• ğ‘†ğ‘ âŠ• îˆ¹, whereâŠ• represents horizon concatenation.\nMemory tags signal the built-in LLM to play the role of compressor at the input end. Then they serve as the\ncontainer to absorb dense information from the preceded segment through the forward propagation of the transformer\nstackwithinthecompressor.SelfCPobtainsthehiddenstatesofthelastTransformerlayerontopofattachedmemory\ntags ğ»ğ‘š =(â„ğ‘š\n1,â„ğ‘š\n2,...,â„ ğ‘š\nğ‘˜ )while disregarding others:\n_,ğ» ğ‘š =ğ¶ğ‘œğ‘šğ‘ğ‘Ÿğ‘’ğ‘ ğ‘ ğ‘œğ‘Ÿ(ğ¼ğ¶). (1)\nThen, SelfCP projectsğ»ğ‘š sourcing from the output space of the compressor into LLM-acceptable memory tokens\nÌƒğ»ğ‘š via the connector, whereğ‘Šğ‘ is the weight of the connector:\nÌƒğ»ğ‘š =ğ‘Šğ‘ â‹… ğ»ğ‘š. (2)\nAssuming the prompt has the uncompressed segmentğ‘†ğ‘¢ behind the compressed oneğ‘†ğ‘, the input of generatorğ¼ğº is\nformulated asğ¼ğº = ğ‘„ âŠ•Ìƒğ»ğ‘š âŠ• ğ‘†ğ‘¢. Given a golden responseğ‘Œ = (ğ‘¦1,ğ‘¦2,...,ğ‘¦ |ğ‘Œ |)to the current query and prompt,\nthe connector within SelfCP is supervised-tuned based on the language modeling objective of the target LLM in the\nteacher-forcing manner, whereÎ˜âˆˆ{ ğ‘Šğ‘,[ğ‘€]}:\nğ‘™ğ‘œğ‘ ğ‘  =maximizeÎ˜\n|ğ‘Œ |âˆ\nğ‘–=0\nîˆ¼(ğ‘¦ğ‘–|ğ¼ğº âŠ• ğ‘¦<ğ‘–). (3)\n3.1.2. Concatenated Compression\nPrevious studies still struggle to process quite long prompts with efficiency or truncation problems, whose length\nexceedstheallowedinputwindowoflanguagemodels.WefurtherintroduceConcatenatedCompressiontoupgradethe\npreviousâ€œFormerCompressionandLatterCompressionâ€intothetrainingofSelfCPtotacklethisproblem.Specifically,\nboth the compressed segmentsğ‘†ğ‘ and the uncompressed ones will exceedğ¿ tokens when prompts longer than2Ã— ğ¿\ntokens after even partition. In this scenario, given a prompt hasğ‘ tokens in total, whereğ‘ > 2Ã— ğ¿, SelfCP first\nallocates uncompressed segmentsğ‘†ğ‘¢ with ğ¿ tokens, and then evenly splits the rest intoâŒˆğ‘\nğ¿ âˆ’1âŒ‰ non-overlapping\nlocal segments. Due to the segments being non-overlapping, the compressor compresses each of them independently\nasEqu.1andconvertsthehiddenstatesontopofmemorytagstolocalmemorytokensasEqu.2.Theglobalmemory\ntokensareconfiguredbyconcatenatinglocalmemorytokenshorizontally,fedintothegeneratortooptimizeSelfCPas\nEqu. 3.\n3.2. Improvement for In-Context Learning\nSpecifically,inthescenarioofICL,weconsiderthepromptcontainingthedemonstrationsandthequerycontaining\nrelatedtaskinputsandtaskinstructions.In-contextdemonstrationsequencetypicallyincreasesthelengthoftheprompt,\nwespeciallydevelopstrategiestooptimizeboththeefficiencyandeffectivenessofICLthroughcachingandretrieving.\nSelfCPallocateseachdemonstrationwithasegmentandtruncatesthelattertoguaranteeindependenceamongeach\nsegment. Hence, SelfCP compresses each demonstration independently and caches its memory tokens to construct a\nJun Gao et al.:Preprint submitted to Elsevier Page 5 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nFigure 3:Kernel density estimation of training data. The horizontal axis is the number of tokens in each instance.\nTable 1\nThe experiment details of test sets used in this paper. ICL represents performing ICL or not. LA stands for linguistic\nacceptability.\nTask Dataset In-Domain ICL # Test Instances\nSUM XSUM ! % 11,334\nQA CICERO ! % 10,898\nQFS DUC % % 45\nCVG CLCV % % 2,000\nSUM XSUM ! ! 1,500\nSUM ARXIV % ! 1,500\nLA CoLA % ! 1,041\nMemory Demonstrations Bank (MDB) for reusing. With the help of MDB, SelfCP can respond to queries by directly\nwithdrawing target memory tokens without repetitive compression, making SelfCP more efficient in ICL.\nRandomly sampled demonstrations have unstable ICL performance gains. We further empower MDB to support\ndemonstration selection by treating the first memory tokenğ‘˜ğ‘– of ğ‘–âˆ’th demonstration as the key used for retrieval.\nSelfCP requires compressing the queryğ‘„ to obtain the first memory tokenğ‘ for directional demonstration selection.\nThen, the demonstration scores of theğ‘–-th demonstration in MDB are calculated based on cosine similarity:\nğ‘ ğ‘ğ‘œğ‘Ÿğ‘’ğ‘– =ğ‘ğ‘œğ‘ ğ‘–ğ‘›ğ‘’(ğ‘,ğ‘˜ ğ‘–). (4)\nSelfCPselectsthetargetdemonstrationsaccordingtothedemonstrationscoresfromhightolow,beingmoreeffective\nin ICL.\n4. Experiment\n4.1. Datasets\nWe mix up XSum, CICERO, and an instruction dataset as our training set, containing 42k instances, and the\ninstance length kernel estimation is illustrated in Figure 3. The details of in- and out-domain test sets are shown in\nTable 1. Notably, we donâ€™t perform in-domain evaluation in the instruction dataset as its responses are somewhat\nopen-ended and it solely has a training set. We use the entire test set of XSUM and CICERO as the in-domain set\nto confirm details of prompt compression. In out-domain evaluation, we employ the entire DUC 2007 as the test set,\nJun Gao et al.:Preprint submitted to Elsevier Page 6 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nand we collect a Chinese verdict generation (CVG) dataset called CLCV (Chinese Legal Case Verdict). CLCV has\n2,000instancescollectedfromChinaProsecutorialNetwork 3.EachinstanceofCLCVcontainstheindictmentandthe\ncorresponding judgment with an average of 540 words for indictment and 230 words for judgment. When evaluating\nthe ICL performance of SelfCP, we sampled 1,500 instances from the XSUM and ARXIV summarization datasets\nrespectively, with an average length of about 580 words. Additionally, we evaluate SelfCP on the entire linguistic\nacceptability dataset CoLA through close-ended evaluation in ICL.\n4.2. Evaluation Metrics\nROUGE Lin (2004) is a widely adopted metric in many generative tasks that evaluate how similar the generated\nhypothesis is to the golden label. Therefore, ROUGE is used in our experiments to evaluate the quality responses\ngeneratedconditionedoncompressedvirtualtokens.WereporttheF-1scoresofROUGE-1,ROUGE-2,andROUGE-\nL (abbreviated R-1, R-2, R-L in the following), and we employed the files2rouge4 library in practice. Additionally,\nas CLCV is a Chinese dataset, we use the Chinese-ROUGE5 library combined with jieba word-cutting library6 to\nevaluate the generated result. For CoLA, we report the accuracy.\n4.3. Baselines\nWecomparetheperformanceofSelfCPwiththenaiveLLMsfedactualpromptsbasedonVicuna-7bandBlueLM-\n7b, respectively. In addition, we introduce AutoCompressor Chevalier et al. (2023) and ICAE Ge et al. (2023) which\nconverts the entire prompts into virtual tokens, and LLMlingua Jiang et al. (2023) which drops uninformative tokens\nin the prompt.\nAutoCompressor AutoCompressor7 is the recent work that compresses prompts into virtual tokens recurrently with\nthefine-tunedLLMsZhangetal.(2022).WeemploytheirofficiallyreleasedweightbasedonLlama2-7bandcompare\nits performance with SelfCP on out-domain datasets, setting the compression ratio the same as SelfCP.\nICAE ICAE8 compressesentirepromptswithgivenwindowlimitationbyaLoRA-adaptedLlama2.Weemploytheir\nofficially released Llama2-7b version and compared its performance with SelfCP in out-domain datasets, setting the\ncompression ratio the same as SelfCP.\nLLMLingua LLMLingua is a recent coarse-to-fine prompt compression method based on dropping uninformative\nwords and achieves powerful performance. We employ LLMLingua from their official code9, and compare its\nperformance with SelfCP in all out-domain datasets, setting the compression ratio the same as SelfCP by limiting\nthenumberofdroppeddiscretetokens.Notably,LLMLingua,intheirpaper,isdesignedtoemployasmallcompressor\n(Llama or GPT-2), instruct-tuned to align with the target LLM (GPT-3.5-Turbo or Claude-v1.3). For a meaningful\ncomparison, we substitute their target LLMs with the underlying LLM in SelfCP.\n4.4. Settings\nConsidering the max tokens in all involved datasets and computation efficiency, we set the max allowed input\nwindow limitationğ¿ to 512. Additionally, we fix the learning rate to 8e-5 and use Adam as the optimizer, and the\neffective batch size is 32 (8 GPUs data parallelism and 4 steps gradient accumulation). Additionally, we conduct all\nexperiments on 8*NVIDIA A5000 24G GPUs based on BFloat 16 data type.\nWe compress thelatter part in XSUM, DUC, and CICERO, since the former part in these tasks is empirically\nimportant, while we compress theformer part in CLCV because the involved person is introduced at the front\nof the indictment which is relatively unimportant. Additionally, we divide ICL into two situations: (1) In the low-\nresourcesituation,wefixthedemonstrationsforeachquery.(2)Inthehigh-resourcesituation,SelfCPretrievessimilar\ndemonstrations from MDB by measuring the cosine similarity. We consider the training set as the demonstration pool\nand construct the MDB for each dataset.\n3https://www.12309.gov.cn/12309/zjxflws/index.shtml.\n4https://github.com/pltrdy/files2rouge.\n5https://pypi.org/project/rouge-chinese.\n6https://pypi.org/project/jieba.\n7https://github.com/princeton-nlp/AutoCompressors.\n8https://github.com/getao/icae.\n9https://aka.ms/LLMLingua.\nJun Gao et al.:Preprint submitted to Elsevier Page 7 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nTable 2\nThe in-domain results of XSUM and CICERO. 1k represents extending window limitation to 1k, and the others is 512.\nBackbone Method XSUM CICERO\nR-1 R-2 R-L R-1 R-2 R-L\nVicuna-7b\nVicuna 19.9 5.0 13.5 17.3 3.3 14.3\n+LoRA 25.4 7.5 17.3 28.1 10.5 25.6\nVicuna-1k 27.3 8.7 19.7 30.5 11.3 27.4\n+LoRA 31.2 11.0 23.1 34.1 13.5 30.2\nSelfCP 30.5 10.8 22.7 33.3 12.9 29.2\nBlueLM-7b\nBlueLM 15.0 3.6 10.4 17.6 3.1 15.0\n+LoRA 23.1 7.6 17.4 21.9 7.8 19.8\nBlueLM-1k 28.1 9.9 22.8 25.1 9.2 23.1\n+LoRA 30.8 10.5 24.6 31.2 10.8 27.4\nSelfCP 30.9 10.6 24.4 29.9 10.6 26.9\nTable 3\nThe out-domain results of DUC and CICV.\nBackbone Method DUC CLCV\nR-1 R-2 R-L R-1 R-2 R-L\nLlama2-7b AutoCompressor 31.7 7.3 16.4 20.8 6.1 17.0\nICAE 30.6 6.9 15.7 22.3 6.4 19.2\nVicuna-7b\nVicuna 24.0 4.6 13.1 20.6 6.0 16.5\nLLMLingua 32.5 7.6 16.8 21.0 6.3 17.2\nSelfCP 33.6 7.8 17.3 21.4 6.3 18.0\nBlueLM-7b\nBlueLM 16.8 3.6 10.0 33.9 17.5 24.8\nLLMLingua 21.2 4.1 11.4 35.1 18.6 25.9\nSelfCP 25.5 4.8 13.4 36.6 20.8 27.2\n4.5. Results\n4.5.1. In-Domain Evaluation\nWe conduct the in-domain evaluation on XSUM and CICERO in Table 2. SelfCP significantly outperforms the\nbaselineVicunaandBlueLMwith512and1024windowlimitationsaftersupervisedtuning.Tosimulategainsbrought\nbythetrainedconnector,wealsoLoRA-tuneVicunaandBlueLMonourtrainingsetwith17Mtrainableparametersby\nsettingtheLoRArankto32(referringto +ğ¿ğ‘œğ‘…ğ´inTab.2).Inthiscase,SelfCPoutperformsLoRA-adaptedbackbones\nwith 512 allowed windows even the model being tuned, while SelfCP is comparable with them with 1,024 allowed\nwindows.TheseresultshighlightthatextremetruncationmakesLLMsconfusedtorespondandthecompressedvirtual\ntokens effectively filter noise information and recover the informative parts to a large extent.\n4.5.2. Out-Domain Evaluation\nWe test the out-domain performance of SelfCP on the DUC and CLCV to evaluate its generalized capability and\ncross-lingua ability, as demonstrated in Table 3.\nSelfCP employs concatenation compression to compress the query-related documents into memory tokens.\nCompared with the truncation that occurs in naive Vicuna and BlueLM, SelfCP effectively broadens the in-context\nwindow, achieving nearly +10 ROUGE-1 gains. While in CLCV, BlueLM-based SelfCP achieves better performance\ncomparedwithVicuna-basedonessinceBlueLMisspecific-tunedandgoodatChinese,provingthatSelfCPimplicitly\nleverages the strengths of diverse backbones during learning prompt compression. Additionally, the compression on\nboth the former of CLCV and the latter part of DUC indicates that SelfCP makes no limitation on the position of\nmemory tokens. As for AutoCompressor, the 7b version underperforms Vicuna-based SelfCP in English tasks (DUC)\nandunderperformesBlueLM-basedSelfCPinChinesetasks(CLCV).Meanwhile,itisnâ€™tsurprisingtofindthatSelfCP\noutperforms LLMLingua in out-domain evaluation since their algorithm leverages the understanding ability of the\ntargetLLMwhileChatGPT-3.5-turboismuchstrongerthanLLMwith7bparameters.Therefore,Vicuna-orBlueLM-\n7b may sometimes be confused about the meaningless discreet tokens.\nJun Gao et al.:Preprint submitted to Elsevier Page 8 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nTable 4\nThe in-context learning results in XSUM and ARXIV of SelfCP based on Vicuna.â€ represents the demonstrations retrieved\nfrom MDB, and others uses fixed demonstrations.\nMethod #-shots XSUM ARXIV CoLA\nR-1 R-2 R-L R-1 R-2 R-L Acc.\nVicuna 0-shot 19.9 5.0 13.4 34.3 9.1 27.4 56.2\n1-shot 21.2 5.8 14.5 34.4 9.1 27.5 57.4\nAutoCompressor\n1-shot 20.3 6.3 13.7 26.4 8.2 25.8 40.9\n2-shot 21.4 6.4 14.1 26.2 7.9 25.4 56.3\n5-shot 21.7 6.7 14.3 33.7 9.1 27.9 58.8\nICAE\n1-shot 21.9 7.8 20.3 24.6 7.1 22.9 30.9\n2-shot 23.2 8.4 20.8 25.5 7.6 24.3 30.9\n5-shot 24.9 8.8 21.6 26.9 7.7 25.8 30.9\nLLMLingua\n1-shot 19.7 5.2 14.4 33.1 8.7 27.1 55.0\n2-shot 20.0 5.1 14.1 32.0 8.1 25.9 55.7\n5-shot 18.6 4.9 14.3 29.7 7.4 24.6 56.9\nSelfCP\n1-shot 25.5 9.1 20.0 34.7 10.2 27.9 58.0\n2-shot 26.8 9.8 20.9 35.2 10.4 28.2 61.0\n5-shot 27.6 10.1 21.4 35.4 10.2 28.1 61.8\nSelfCPâ€ \n1-shot 26.2 9.4 20.7 34.7 10.3 27.8 58.7\n2-shot 28.9 10.5 21.7 34.3 10.4 28.3 61.2\n5-shot 30.0 11.2 22.3 35.3 10.8 27.7 61.5\nTable 5\nThe in-context learning results of SelfCP based on BlueLM.\nMethod #-shots XSUM ARXIV CoLA\nR-1 R-2 R-L R-1 R-2 R-L Acc.\nBlueLM 0-shot 15.0 3.6 10.4 30.9 7.7 24.7 71.6\n1-shot 19.1 4.8 12.1 23.0 3.6 19.0 69.6\nLLMLingua\n1-shot 18.0 2.7 13.2 28.0 6.3 23.2 58.1\n2-shot 18.6 3.4 13.4 26.5 5.5 22.2 60.3\n5-shot 18.3 3.3 13.3 26.8 5.6 22.4 62.5\nSelfCP\n1-shot 24.0 6.9 18.0 31.4 7.7 25.2 69.6\n2-shot 25.0 7.3 18.8 30.8 7.3 24.8 70.1\n5-shot 25.3 7.4 19.1 31.9 7.8 26.0 71.8\nSelfCPâ€ \n1-shot 24.7 7.2 18.5 31.0 7.5 24.9 70.1\n2-shot 25.1 7.4 19.0 31.2 7.7 25.1 70.3\n5-shot 26.3 7.6 20.0 31.5 7.9 25.3 71.1\n4.5.3. In-Context Learning Evaluation\nWeevaluatetheICLabilityofSelfCPinTable4and5.Memorytokenstakeequalorevenbettereffectsthanactual\ndemonstrations, which verifies the ability of SelfCP to capture the core semantics of demonstrations during compres-\nsion. Regarding ARXIV, the original ICL is not helpful enough and causes significant degradation in BlueLM, due to\ntherelativelylongdocumentsinARXIV,whichleavelittleroomforLLMtoreaddemonstrations.AutoCompressorre-\ncursivelycompressesconcatenateddemonstrationsintosoftpromptsstep-by-step.However,AutoCompressorstillun-\nderperformsSelfCP.Weattributethistotheinformationlostduetorecursivecompressioninaddressinglongprompts.\nMoreover,demonstrationsfilteredbyLLMlinguagenerallyunderperform0-shotinbothXSUMandARXIVwithtwo\nbackbonesprovingthatthediscretetokensfailtoguideLLMinfew-shotsettings.WeevaluateSelfCPonCoLAthrough\nclosed-end evaluation, which measures the perplexity (PPL)10 of candidate labels (acceptable/unacceptable) for the\ngivensentenceinthefollowingtemplate:[Sentence] Grammatically, the above sentence is {acceptable/unacceptable}.\n10https://huggingface.co/docs/transformers/perplexity.\nJun Gao et al.:Preprint submitted to Elsevier Page 9 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nTable 6\nCase studies on the DUC dataset. Theitalic text in Input represents the document to be condensed, while thebold text\nindicates the salient information derived from the compressed prompts.\nDUC Case\nInput According to the topic: Write an account of the sequence of events involving the Kennedy family...\nSummarize the following document: John F. Kennedy Jr. was a relatively new pilot...the plane could\nglide if the single engine failed </P><P> Kennedy had registered the plane,a Piper Saratoga...\nReference At approximately 9:40pm Friday, ..., when aPiper Saratoga, piloted by John, ... An at sea burial of all\nthree was conducted ...\nVicuna The sequence of events involving the Kennedy family following the plane crash that killed John F.\nKennedy Jr., his wife, Carolyn Bessette Kennedy...\nLLMLingua The sequence of events involving the Kennedy family during and following the plane crash that killed\nJohn F. Kennedy Jr., his wife, Carolyn Bessette Kennedy...\nAutoCompressor </P><P> The plane was a twin-engine, single-seat Cessna Citation500, which costs about $1.5 million,\naccording to the manufacturer.</P><P>...\nSelfCP On July 17, 1999, John F. Kennedy Jr., his wife, ...The plane was a Piper Saratoga, a twin-engine\naircraft that was owned by Kennedy...\nFigure 4:The visualization result between condensed prompt and their virtual tokens. To clarify, we sample 4 virtual tokens\nand select some representative actual tokens.\nLabels with PPL closer to 1 will be judged as the prediction. Notably, ICAE always returns â€œacceptableâ€, resulting in\na consent 30.9% accuracy.\nFinally,weevaluateSelfCPonselectingsuitabledemonstrationsfromMDB,whichcontains5krandomlysampled\ndemonstrations. SelfCP further achieves performance gains by retrieving more powerful in-context demonstrations\nfromMDB.TheresultindicatesthatthecompressedvirtualtokengeneratedbySelfCPisgoodatmeasuringsimilarity\namong documents and then finding more favorable in-context demonstrations for ICL.\n4.6. Case Study\nWe demonstrate a case study on DUC to provide an intuitive comparison among SelfCP based on Vicuna, direct\ntransaction, AutoCompressor, and LLMLingua in Table 6. This case describes that a plane crashed into the Atlantic\nOcean, while the details of the plane are over-length and condensed. Vicuna canâ€™t generate satisfying summaries as\nsomesalientpartsofthedocumentsaretruncated.Bycontrast,SelfCPsuccessfullyrestorestheimportantinformation\nfrom the compact virtual tokens, such as the aircraft type â€œPiper Saratogaâ€.\nFurthermore, we depict the other scenario by gauging the similarity between actual and their virtual tokens,\nillustrated in Figure 4. Warmer colors signify a greater degree of similarity. The origin document describes the\nStuckbarks and its cooperation with Magic Johnson, with the information about â€œMagic Johnsonâ€ compressed.\nHowever, SelfCP recovers this information in the generated response. It is plausible that the virtual tokens effectively\nabsorb pertinent information, resulting in a comparatively higher similarity to these tokens.\nJun Gao et al.:Preprint submitted to Elsevier Page 10 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nFigure 5:The sensitivity analysis of compression ratio.\nTable 7\nThe efficiency of SelfCP with the backbone of Vicuna. 1k stands for the extended 1k window limitation, while others have\na 512 window limitation (ignoring the length of memory tokens).\nMethod GPUHours TFLOPs TMACs\nVicuna 1.5 86,20 4,309\nVicuna-1k 1.9 31,664 15,832\nSelfCP 1.6 22,437 11,218\n5. Analysis\n5.1. Compression Ratio\nThe compression ratio is randomly sampled from 2 to 16 during the training of SelfCP. We mix up 2,000\ninstances from the in-domain validation set, 1,000 for XSUM, and 1,000 for CICERO to select the compression ratio.\nSpecifically,SelfCPconductsconditionalcompressionthatcompressesthelattercut-offpartwhilekeepingtheformer\nuncompressed. Therefore, we can measure the information quality of the same content with different compression\nratios by ROUGE-1 since it is more sensitive to token-level differences.\nFor both BlueLM and Vicuna, the performance is relative smoothing when the compression ratio changes from\n4Ã—to 12Ã—. However, when it comes to16Ã—, a significant drop occurs in Vicuna, with relatively large performance\ndegradation occurring in BlueLM as well compared to the previous ratios. Therefore, we set the compression ratio to\n12 by default and apply this ratio to all experiments. Additionally, in our experiment setting, the window limitation is\n512, and the512Ã—compression ratio is equal to compressing anything to a single virtual token.\n5.2. Efficiency Analysis\nIn SelfCP, we incorporate an additional 17M trainable parameters into the 7b backbone, accounting for an\napproximate increase of 0.24%.\nTo quantify the efficiency difference brought by the projection layer, we mainly focus on SelfCP built on Vicuna\nbecause BlueLM has a comparable parameter size and model architecture. We first report the GPU Hours, TFLOPs,\nand TMACs11 of SelfCP and Vicuna on a single NVIDIA A5000 GPU. Specifically, we use 1000 random but legal\nnumber sequences of 1024 length as input ids, avoiding special tokens, and ask the model to always generate 128\ntokens.SelfCPcompressesthelater512tokensinto43memorytokens(12 Ã—compression),andtheformer512tokens\n11https://github.com/MrYxJ/calculate-flops.pytorch.\nJun Gao et al.:Preprint submitted to Elsevier Page 11 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nFigure 6: Efficiency comparison on throughputs (left) and memory costs (right) in the CoLA dataset. The upper limit of\nmemory is 24G, and the excess part is marked with *, corresponding to the breaking point in the line chart.\nalong with memory tokens are fed into Vicuna to perform generation (555 tokens in total). To maintain a consistent\ninputlengthforgeneration,theformer555tokensaredirectlyfedintoVicuna.InTable7,SelfCPachievesnearlythree\ntimes the TFLOPs and TMACs than the naive backbone to exchange double readable windows due to the additional\ncompression and projection processes. However, the forward propagation for compressing and the projection support\nparallel computing, and it only brings minimal GPU Hours increments in practice. Notably, when allowing Vicuna to\nread the entire 1024 input ids without compression, the extra 512 tokens surge both computation and GPU Hours and\noverwhelm SelfCP during generation (referring to Vicuna-1k in Tab. 7).\nExcept for the floats computation, in the ICL scenario, we illustrate the practical variations of throughput and\nmemory costs with the number of demonstrations increasing among SelfCP and other compression methods in\nFigure 6. AutoCompressor exhibits poor inference throughputs due to recursively compression and ICAE performs\nsub-weakly in throughputs because of switching LoRA weights between the compressor and the generator. Given\na 24 GB memory allocation, Vicuna-7b only performs up to the 4-shot setting, while ICAE and AutoCompressor\nreadupto32demonstrations,butSelfCPstillworksinthe64-shotsetting.SelfCPsupportscachingmemorytokensto\nconfiguretheMemoryDemonstrationBankinadvanceforreusing.SelfCPcanrespondtoquerieswithoutcompressing\ndemonstration repeatedly in this case (referring to SelfCP + Caching), making inference more efficient.\n6. Conclusion and Further Work\nThis paper proposes SelfCP, using a frozen LLM to compress long prompts into memory tokens. In SelfCP, the\nLLMplaystheroleofthecompressortocompressthepromptandthegeneratortorespondtoqueriesconditionedonthe\nmemory tokens and the rest uncompressed prompt. SelfCP only contains 17M trainable parameters due to the frozen\nbackboneandallowsforadaptationacrossvariousbackbones.Weconductextensivein-andout-domainexperiments,\ncoveringsituationsofICLand over-lengthpromptsandweanalyzetheefficiency ofSelfCP.Theresultsshowthatthe\ngenerated memory tokens can effectively substitute the 12Ã—longer actual over-limit prompt.\nWe believe there is much improvement room for SelfCP. On the one hand, we will scale the backbone of SelfCP\nto larger and higher-performance LLMs in various domains. On the other hand, our intention involves incorporating\ncompression as one of the fundamental pre-training goals of LLMs, expecting to enhance their compression ability\nfurther.\nCRediT authorship contribution statement\nJunGao: Conceivedanddesignedthework,Conductedexperiments,Performedtheanalysis,andWrotethepaper.\nZiqiang Cao:Conceived and designed the analysis, Performed the analysis, and Supervised the work.Wenjie Li:\nConceived and designed the work, and Reviewed the work.\nJun Gao et al.:Preprint submitted to Elsevier Page 12 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nReferences\nBeltagy, I., Peters, M.E., Cohan, A., 2020. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150 .\nBertsch, A., Alon, U., Neubig, G., Gormley, M.R., 2023. Unlimiformer: Long-range transformers with unlimited length input. arXiv preprint\narXiv:2305.01625 .\nBolya,D.,Fu,C.Y.,Dai,X.,Zhang,P.,Feichtenhofer,C.,Hoffman,J.,2022. Tokenmerging:Yourvitbutfaster. arXivpreprintarXiv:2210.09461.\nBulatov,A.,Kuratov,Y.,Burtsev,M.,2022. Recurrentmemorytransformer. AdvancesinNeuralInformationProcessingSystems35,11079â€“11091.\nBulatov,A.,Kuratov,Y.,Kapushev,Y.,Burtsev,M.S.,2023.Scalingtransformerto1mtokensandbeyondwithrmt.arXivpreprintarXiv:2304.11062\n.\nChevalier, A., Wettig, A., Ajith, A., Chen, D., 2023. Adapting language models to compress contexts. arXiv preprint arXiv:2305.14788 .\nChild, R., Gray, S., Radford, A., Sutskever, I., 2019. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509 .\nChoromanski, K., Likhosherstov, V., Dohan, D., Song, X., Gane, A., Sarlos, T., Hawkins, P., Davis, J., Mohiuddin, A., Kaiser, L., et al., 2020.\nRethinking attention with performers. arXiv preprint arXiv:2009.14794 .\nCohan, A., Dernoncourt, F., Kim, D.S., Bui, T., Kim, S., Chang, W., Goharian, N., 2018. A discourse-aware attention model for abstractive\nsummarization of long documents. arXiv preprint arXiv:1804.05685 .\nCopeck, T., Inkpen, D., Kazantseva, A., Kennedy, A., Kipp, D., Nastase, V., Szpakowicz, S., 2006. Leveraging duc, in: proceedings of DUC.\nDai, Z., Yang, Z., Yang, Y., Carbonell, J., Le, Q.V., Salakhutdinov, R., 2019. Transformer-xl: Attentive language models beyond a fixed-length\ncontext. arXiv preprint arXiv:1901.02860 .\nDing, J., Ma, S., Dong, L., Zhang, X., Huang, S., Wang, W., Wei, F., 2023. Longnet: Scaling transformers to 1,000,000,000 tokens. arXiv preprint\narXiv:2307.02486 .\nDosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., et al.,\n2020. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 .\nGe, T., Hu, J., Wang, X., Chen, S.Q., Wei, F., 2023. In-context autoencoder for context compression in a large language model. arXiv preprint\narXiv:2307.06945 .\nGhosal,D.,Shen,S.,Majumder,N.,Mihalcea,R.,Poria,S.,2022. Cicero:Adatasetforcontextualizedcommonsenseinferenceindialogues. arXiv\npreprint arXiv:2203.13926 .\nJiang, H., Wu, Q., Lin, C.Y., Yang, Y., Qiu, L., 2023. Llmlingua: Compressing prompts for accelerated inference of large language models. arXiv\npreprint arXiv:2310.05736 .\nKatharopoulos, A., Vyas, A., Pappas, N., Fleuret, F., 2020. Transformers are rnns: Fast autoregressive transformers with linear attention, in:\nInternational conference on machine learning, PMLR. pp. 5156â€“5165.\nKim, S., Shen, S., Thorsley, D., Gholami, A., Kwon, W., Hassoun, J., Keutzer, K., 2022. Learned token pruning for transformers, in: Proceedings\nof the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 784â€“794.\nLee-Thorp, J., Ainslie, J., Eckstein, I., Ontanon, S., 2021. Fnet: Mixing tokens with fourier transforms. arXiv preprint arXiv:2105.03824 .\nLi,J.,Li,D.,Savarese,S.,Hoi,S.,2023a. Blip-2:Bootstrappinglanguage-imagepre-trainingwithfrozenimageencodersandlargelanguagemodels.\narXiv preprint arXiv:2301.12597 .\nLi, X.L., Liang, P., 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190 .\nLi, Y., 2023. Unlocking context constraints of llms: Enhancing context efficiency of llms with self-information-based content filtering. arXiv\npreprint arXiv:2304.12102 .\nLi,Y.,Yang,N.,Wang,L.,Wei,F.,Li,W.,2023b.Generativeretrievalforconversationalquestionanswering.InformationProcessing&Management\n60, 103475.\nLin, C.Y., 2004. Rouge: A package for automatic evaluation of summaries, in: Text summarization branches out, pp. 74â€“81.\nLiu,N.F.,Lin,K.,Hewitt,J.,Paranjape,A.,Bevilacqua,M.,Petroni,F.,Liang,P.,2024. Lostinthemiddle:Howlanguagemodelsuselongcontexts.\nTransactions of the Association for Computational Linguistics 12, 157â€“173.\nLiu, X., Ji, K., Fu, Y., Tam, W., Du, Z., Yang, Z., Tang, J., 2022. P-tuning: Prompt tuning can be comparable to fine-tuning across scales and tasks,\nin: Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pp. 61â€“68.\nMu, J., Li, X.L., Goodman, N., 2023. Learning to compress prompts with gist tokens. arXiv preprint arXiv:2304.08467 .\nNarayan, S., Cohen, S.B., Lapata, M., 2018. Donâ€™t give me the details, just the summary! topic-aware convolutional neural networks for extreme\nsummarization. ArXiv abs/1808.08745.\nSun, S., Yuan, R., Li, W., Cao, Z., Li, S., 2024. Dialogue acts enhanced extractâ€“abstract framework for meeting summarization. Information\nProcessing & Management 61, 103635.\nTaori, R., Gulrajani, I., Zhang, T., Dubois, Y., Li, X., Guestrin, C., Liang, P., Hashimoto, T.B., 2023. Stanford alpaca: An instruction-following\nllama model.\nTeam, B., 2023. Bluelm: An open multilingual 7b language model.https://github.com/vivo-ai-lab/BlueLM.\nTouvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.A., Lacroix, T., RoziÃ¨re, B., Goyal, N., Hambro, E., Azhar, F., et al., 2023. Llama:\nOpen and efficient foundation language models. arXiv preprint arXiv:2302.13971 .\nWang, J., Liang, Y., Meng, F., Shi, H., Li, Z., Xu, J., Qu, J., Zhou, J., 2023a. Is chatgpt a good nlg evaluator? a preliminary study. arXiv preprint\narXiv:2303.04048 .\nWang, L., Zhao, M., Ji, H., Jiang, Z., Li, R., Hu, Z., Lu, X., 2024. Dialogue summarization enhanced response generation for multi-domain\ntask-oriented dialogue systems. Information Processing & Management 61, 103668.\nWang,Y.,Mishra,S.,Alipoormolabashi,P.,Kordi,Y.,Mirzaei,A.,Arunkumar,A.,Ashok,A.,Dhanasekaran,A.S.,Naik,A.,Stap,D.,etal.,2022.\nSuper-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705 .\nWang,Z.,Xie,Q.,Ding,Z.,Feng,Y.,Xia,R.,2023b. Ischatgptagoodsentimentanalyzer?apreliminarystudy. arXivpreprintarXiv:2304.04339.\nWei,X.,Cui,X.,Cheng,N.,Wang,X.,Zhang,X.,Huang,S.,Xie,P.,Xu,J.,Chen,Y.,Zhang,M.,etal.,2023. Zero-shotinformationextractionvia\nchatting with chatgpt. arXiv preprint arXiv:2302.10205 .\nJun Gao et al.:Preprint submitted to Elsevier Page 13 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nWingate,D.,Shoeybi,M.,Sorensen,T.,2022.Promptcompressionandcontrastiveconditioningforcontrollabilityandtoxicityreductioninlanguage\nmodels. arXiv preprint arXiv:2210.03162 .\nWu, Y., Rabe, M.N., Hutchins, D., Szegedy, C., 2022. Memorizing transformers. arXiv preprint arXiv:2203.08913 .\nYang,X.,Li,Y.,Zhang,X.,Chen,H.,Cheng,W.,2023. Exploringthelimitsofchatgptforqueryoraspect-basedtextsummarization. arXivpreprint\narXiv:2302.08081 .\nZhang, S., Roller, S., Goyal, N., Artetxe, M., Chen, M., Chen, S., Dewan, C., Diab, M., Li, X., Lin, X.V., et al., 2022. Opt: Open pre-trained\ntransformer language models. arXiv preprint arXiv:2205.01068 .\nZheng, L., Chiang, W.L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., et al., 2023. Judging llm-as-a-judge with\nmt-bench and chatbot arena. arXiv preprint arXiv:2306.05685 .\nZheng, L., Wang, C., Kong, L., 2022. Linear complexity randomized self-attention mechanism, in: International conference on machine learning,\nPMLR. pp. 27011â€“27041.\nZhong, Z., Lei, T., Chen, D., 2022. Training language models with memory augmentation. arXiv preprint arXiv:2205.12674 .\nJun Gao et al.:Preprint submitted to Elsevier Page 14 of 15\nSelfCP: Compressing Over-Limit Prompt via the Frozen Large Language Model Itself\nTable 8\nUsed instructions in involved datasets.\nXSUM Summarize the following document: [DOC]\nCICERO [DOC] According to the above text, answer the query: [Query]\nARXIV Summarize the following document: [DOC]\nDUC According to the topic: [Query] Summarize the following documents: [DOC1][DOC2]...\nCLCV Generating the judgment according to the following indictment: [DOC]\nCoLA [Sentence] Grammatically, the above sentence is {acceptable/unacceptable}\nA. Dataset\nWe briefly introduce each dataset involved in this paper as follows:\nXSUM XSUM Narayan et al. (2018) is a popular abstractive summarization dataset. The documents in the XSUM\ndatasetcomefromarticlesinvariousfields,includingnewsstories,encyclopediaentries,forumposts,andmore.Each\ninstance contains a document and its summary.\nCICERO CICEROGhosaletal.(2022)isagenerativequestion-answeringdataset.Eachinstancecontainsapersonal\ndialogue,atargetspeech,aquery,andahuman-writtenanswer.Languagemodelsneedtoanswerthequeryconditioned\non the target speech.\nSUPER-NI SUPER-NIWang,Mishra,Alipoormolabashi,Kordi,Mirzaei,Arunkumar,Ashok,Dhanasekaran,Naik,\nStap et al. (2022) is a comprehensive instruction dataset including thousands of common natural language processing\ntasks. Each instance contains an instruction and a response. We introduce this dataset to make SelfCP able to respond\nto diverse queries.\nDUC DUC 2007 Copeck et al. (2006) is a popular Query Focused Summarization (QFS) dataset. Each instance\ncontains a topic and a few documents.\nARXIV ARXIV Cohan, Dernoncourt, Kim, Bui, Kim, Chang and Goharian (2018) is a summarization dataset\ncollected from arXiv. Each instance of this dataset contains the body of a paper and its abstract. Therefore, ARXIV is\nrelatively longer than normal datasets, in which documents contain 5.9k words on average.\nCoLA The Corpus of Linguistic Acceptability (CoLA) is a popular linguistic acceptability dataset consisting of\n10657sentencesfrom23linguisticspublications,expertlyannotatedforacceptability(grammaticality)bytheiroriginal\nauthors.\nJun Gao et al.:Preprint submitted to Elsevier Page 15 of 15",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7812730669975281
    },
    {
      "name": "Transformer",
      "score": 0.6609821319580078
    },
    {
      "name": "Embedding",
      "score": 0.583472490310669
    },
    {
      "name": "Inference",
      "score": 0.534838855266571
    },
    {
      "name": "Booster (rocketry)",
      "score": 0.5315321087837219
    },
    {
      "name": "Limit (mathematics)",
      "score": 0.495731920003891
    },
    {
      "name": "Automatic summarization",
      "score": 0.4724992513656616
    },
    {
      "name": "Context (archaeology)",
      "score": 0.441545307636261
    },
    {
      "name": "Artificial intelligence",
      "score": 0.3432852029800415
    },
    {
      "name": "Paleontology",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Mathematics",
      "score": 0.0
    },
    {
      "name": "Mathematical analysis",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Astronomy",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Voltage",
      "score": 0.0
    }
  ]
}