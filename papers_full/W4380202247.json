{
  "title": "Apple ripeness identification from digital images using transformers",
  "url": "https://openalex.org/W4380202247",
  "year": 2023,
  "authors": [
    {
      "id": "https://openalex.org/A5039140889",
      "name": "Bingjie Xiao",
      "affiliations": [
        "Auckland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5042399868",
      "name": "Minh Nguyen",
      "affiliations": [
        "Auckland University of Technology"
      ]
    },
    {
      "id": "https://openalex.org/A5064286235",
      "name": "Wei Qi Yan",
      "affiliations": [
        "Auckland University of Technology"
      ]
    }
  ],
  "references": [
    "https://openalex.org/W4206545219",
    "https://openalex.org/W2789850637",
    "https://openalex.org/W3034024180",
    "https://openalex.org/W3161603460",
    "https://openalex.org/W3197486099",
    "https://openalex.org/W3179888767",
    "https://openalex.org/W4231786826",
    "https://openalex.org/W3165924482",
    "https://openalex.org/W4225271216",
    "https://openalex.org/W2970463581",
    "https://openalex.org/W2075749116",
    "https://openalex.org/W3197187755",
    "https://openalex.org/W3122390253",
    "https://openalex.org/W3090873304",
    "https://openalex.org/W3109992649",
    "https://openalex.org/W3000518373",
    "https://openalex.org/W3169323098",
    "https://openalex.org/W2931250519",
    "https://openalex.org/W3154571917",
    "https://openalex.org/W4367281768",
    "https://openalex.org/W2963037989",
    "https://openalex.org/W3152778632",
    "https://openalex.org/W3161977502",
    "https://openalex.org/W3109539969",
    "https://openalex.org/W2900055483",
    "https://openalex.org/W3085201316",
    "https://openalex.org/W4319068683",
    "https://openalex.org/W3137103197",
    "https://openalex.org/W3139273499"
  ],
  "abstract": "Abstract We describe a non-destructive test of apple ripeness using digital images of multiple types of apples. In this paper, fruit images are treated as data samples, artificial intelligence models are employed to implement the classification of fruits and the identification of maturity levels. In order to obtain the ripeness classifications of fruits, we make use of deep learning models to conduct our experiments; we evaluate the test results of our proposed models. In order to ensure the accuracy of our experimental results, we created our own dataset, and obtained the best accuracy of fruit classification by comparing Transformer model and YOLO model in deep learning, thereby attaining the best accuracy of fruit maturity recognition. At the same time, we also combined YOLO model with attention module and gave the fast object detection by using the improved YOLO model.",
  "full_text": "Vol.:(0123456789)\nMultimedia Tools and Applications (2024) 83:7811–7825\nhttps://doi.org/10.1007/s11042-023-15938-1\n1 3\nApple ripeness identification from digital images using \ntransformers\nBingjie Xiao1 · Minh Nguyen1 · Wei Qi Yan1\nReceived: 2 February 2022 / Revised: 29 March 2023 / Accepted: 29 May 2023 /  \nPublished online: 10 June 2023 \n© The Author(s) 2023\nAbstract\nWe describe a non-destructive test of apple ripeness using digital images of multiple types \nof apples. In this paper, fruit images are treated as data samples, artificial intelligence mod-\nels are employed to implement the classification of fruits and the identification of maturity \nlevels. In order to obtain the ripeness classifications of fruits, we make use of deep learning \nmodels to conduct our experiments; we evaluate the test results of our proposed models. \nIn order to ensure the accuracy of our experimental results, we created our own dataset, \nand obtained the best accuracy of fruit classification by comparing Transformer model and \nYOLO model in deep learning, thereby attaining the best accuracy of fruit maturity recog-\nnition. At the same time, we also combined YOLO model with attention module and gave \nthe fast object detection by using the improved YOLO model.\nKeywords YOLO · Transformer · Object detection\n1  Background\nWith the deepening of deep learning research, a spate of deep learning models appear in \nthe field of natural language processing (NLP) as one of the essential directions of artifi-\ncial intelligence. Compared with conventional machine learning methods, the advantages \nof deep learning are mainly in that the complex feature extraction process is not required. \nTherefore, crucial deep learning structures such as convolution neural networks (CNN or \nConvNet) and recurrent neural networks (RNN) have been boardly employed in natural \nlanguage processing (NLP) in recent years and have achieved significant progress [17, 18]. \nVisual object classification is able to reduce the consumption of human labor through com-\nputer vision which is an essential work of digital image and video processing as well as \ncomputer vision. Because of the extensive use of deep learning, visual object classification \nalgorithms have been updated and prompted quickly [8, 12].\nVisual object detection is harnessed in face detection, vehicle detection, self-driving cars, \npedestrian counting, security systems, etc. [14, 15, 22]. This paper is to utilize the character-\nistics of visual object detection to realize object detection of fruit maturity [35, 36].\n * Wei Qi Yan \n dcsyanwq@gmail.com\n1 Auckland University of Technology, Auckland, New Zealand\n7812 Multimedia Tools and Applications (2024) 83:7811–7825\n1 3\nThe labor shortage in New Zealand when the fruit is ripe has inspired our deep \nlearning applications. We need to construct a model so that the model can automati-\ncally classify the shape, size, and maturity of fruit, so as to help agriculture imple-\nment the automatic picking of robotic arms. Our project aims to make use of the \nTransformer model from natural language processing combined with deep learning \nmodels to achieve the classification of fruits with different classes of fruit ripeness. \nIn this paper, we extensively investigate fruit classification methods in indoor and \noutdoor environments, choose apples and pears as the experimental targets. We are \nuse of mobile phones to take pictures of fruits and generate the images for our data-\nset. We classify the fruits in the supermarkets, and distinguish the ripeness of various \nfruits, such as ripe apple, overripe apple, ripe pear, overripe pear. In our experiments, \nwe mainly introduce two types of object detection methods, i.e., YOLO and Trans-\nformer [2 , 3, 5].\nAs shown in Fig.  1, we take use of rectangular boxes to locate fruits in the image and \nlabel the classes and maturities (i.e., ripe apple, overripe apple, ripe pear, overripe pear). \nThen, we train the model by using Transformer encoder and decoder, get the prediction \nresults of fruit category and maturity. The dotted lines in Fig.  1 show the block of the \nTransformer, which is actually a process of training the detector. In the YOLO model, we \nmake use of the blocks of YOLO to train the detector. Figure  1 shows a complete experi-\nmental procedure. We are use of PyTroch platform for experiments. After the data is \nmarked, it is sent to the model for our experimentation.\nWe will introduce our Transformer model and its implementation for our experiments in \nthe second part of this article. The third part of the paper will introduce the specific process \nof the experiment. We show the experimental results in the fourth part. Finally, we con-\nclude and envision future work at the end of the paper.\n2  Related work\nNatural Language Processing (NLP) is one hotspot of artificial intelligence research. NLP \nperforms tasks such as translation, semantic analysis, chatbot development, text mimicry, \nand text-to-speech [28]. Deep learning provides multilayer perceptron models to train deep \nFig. 1  The workflow of fruit ripeness recognition using Transformer model\n7813Multimedia Tools and Applications (2024) 83:7811–7825 \n1 3\nnets with abstract data and to discover complicated structures in big data. Therefore, nowa-\ndays, the primary task of visual object detection is still a very challenging job, with a great \npotential and a large room for betterment [28, 31].\nIn the practical application of digital images, object detection is a complex image \nretrieval guided by visual saliency. In real life, our human eyes can detect the sali-\nency of complex scenes by using the attention selection mechanism [34, 38], and \naccurately find the target of interest through semantic analysis [18, 25]. The object \ndetection is also based on the retrieval performance problems existing in the exist-\ning image retrieval system, through drawing on the attention selection mechanism of \nthe human visual system to establish a visual saliency calculation model. At present, \nthe low-level visual features such as color and texture extracted by the most of image \ncontent retrieval systems are quite different from the semantic features by humans in \nimage understanding, resulting in the problem of semantic gap in retrieval. However, \nTransformer object detection establishes a computational model of attention selection \nmechanism from the bottom up, making the model consistent with the saliency model \nof the human visual perception system. Visual object detection is based on the sali-\nency image feature extraction and matching strategy, the combination of local features \nand global features for feature description is probed, and finally an image retrieval \nmethod combining top-down attention selection and bottom-up attention selection is \nrealized.\nFruit detection is based on the inspiration of New Zealand’s demand for smart agricul-\nture. With the rapid development of visual object detection, fruit detection methods can \nnow achieve high efficiency and high precision.\nThe “location + classification” issue is related to pattern classification for visual \nobject detection, which simply goes from visual object classification to the position \nof visual object, and further to the class label and position of multiple objects [11, \n26, 29]. Fruit harvesting robots [20, 30] have been developed by using YOLOv3 and \nYOLOv5. The undetected apples of the YOLOv3 model only have 9.2% of the entire \ndataset, while YOLOv5 can accurately detect fruits and only 2.8% of undetected apples \n[19, 23, 30].\nTang [29] et al. proposed a coordinate-based anchor-free (CBAF) [24] module for visual \nobject detection. The output coordinates are directly predicted by using SSD or a branch \nof RetinaNet [7]. The focus is on visual object detection of small target objects. For our \nexperiments, it is impossible to have only a single apple or pear in the images while col-\nlecting data. Usually, in our dataset, apples may be on trees or pears in fruit bowl, how to \naccurately extract small objects in the image is a problem [16]. An algorithm based on an \nimproved YOLOv5 [10] is offered to solve the problem of loss rate of small visual objects \nand weak classifiers. The characteristics of the small target datasets are applied to optimize \nthe deep learning network, increase the residual network, converge the algorithm speed to \nprevent the gradient from small or vanishing, and improve the detection outcome of the \nmodel to prevent overfitting [26].\nThe difficulty of NLP is that most NLP algorithms are based on complex deep neural \nnetworks, such as RNNs, LSTMs, and GRU models. Alkalouti and Masre [4] combined \nYOLO model and LSTM for visual object detection. For our experiments, how to adjust \nthe YOLO model so that the model adapts to our experiments, thereby, how to improve the \ndetection speed and accuracy is one of the current experimental goals.\nDai [9 ] et  al. made improvements to the Detection Transformer (DETR) model \nfor large objects rather than small ones. The backbone of R-CNN can be frozen, a \n7814 Multimedia Tools and Applications (2024) 83:7811–7825\n1 3\nDETR is referenced that can find object localization and preferences in the task, \nwhich extends it to object query shuffle and attention mask so as to carry out multi-\nquery localization [37]. The relevant experiments [21] proved that DETR has limita-\ntions in visual object detection. In contrast, YOLOv5 can detect visual objects faster \nand accurater.\nArkin [6] et  al. analyzed the pros and cons of CNN models and transformer models. \nCNN extracts visual feature information from the local to the global through convolutional \nlayer, while the transformer model directly grasps global information and then makes use \nof each patch to achieve self-attention. Transformer does not have bionic features, Trans-\nformer needs data samples to achieve better results under the same parameters. However, \nthe transmission result of the Transformer is better than that of R-CNN [33]. How to adjust \nthe parameters to combine the advantages of the two models is also a problem to be con-\nsidered in our experiments.\nAbozeid [1] et  al. improved Swin Transformer model, comprising of an encoder, a \ndecoder, and skip connections. For outdoor weather changes due to ambient noises, we \nexperiment with object stacking, computation of objects, and upsampling the feature size \nwith a dedicated patch [32].\nHendria [13] integrates Swin Transformer and CNN models, by using Non-Maximum \nSuppression (NMS), soft-NMS, Non-Maximum Weighting (NMW), and weighted box \nfusion (WBF) through combining predictions from multiobject detection models [27].\nWe selected YOLOv5 model for deep learning DETR and Swin Transformer model and \ntrain our model by comparing the advantages and disadvantages between different models. \nYOLO model has the characteristics of fast detection speed and high accuracy. The Trans-\nformer makes use of the advantages of self-attention to avoid the limitations of conven-\ntional CNN-based or RNN-based models, which is easy to combine multimodal data and \nprovide greater flexibility for the model.\n3  Methodology\n3.1  YOLO\nThe series of YOLO algorithms segment the input image into grids, each grid is only respon-\nsible for visual objects whose center points are fallen into the grid and marked with the height \nand width of the bounding boxes. If the point of an object is fallen into a grid, then this grid \ncorresponds to the detecting object.\nRegarding the output of deep neural networks, three bounding boxes are generated for \neach grid cell. The output of each bounding box has three parameters: The box parameter \nof the object, which has four values in total, are represented by ( x, y, h, w  ). Confidence is \na probability within an interval [0,1.0]. The confidence of the bounding box equals to the \nprobability of the object multiplied by the IOU of the bounding box and the actual bound-\ning box of the visual object as shown in Eq. (1).\nwhere conditional class probabilities are all values in the interval [0,1.00]. In YOLOv5, \nC j\ni represents the area estimation of the detection target. P r(Object) is the probability that \n(1)C j\ni = P r(Object)× IOU truth\npred\n7815Multimedia Tools and Applications (2024) 83:7811–7825 \n1 3\nhas an object in the bounding box. If there is an object, P r(Object) equals to 1.0, otherwise \nP r(Object) equals to 0. IOU truth\npred is the ground truth and predicted box cross-combination ratio\nApart from the basic composition of the YOLO series, the YOLO network has also \nundergone various changes from YOLO basis model to YOLOv5.\nYOLOv5 initializes with 9 anchors, which are employed to the three feature maps, \nrespectively. Each grid cell of each feature map has three anchors for prediction. In the \nYOLOv5 model, the feature map with a larger scale is closer to the front, and the down-\nsampling rate of the original image is smaller, and the receptive field is smaller. Therefore, \nit is relatively possible to predict visual objects with a smaller scale (small targets), and \nthe assigned anchors are smaller. The smaller the scale of the feature map, the higher the \ndownsampling rate related to the original image, and the larger the receptive field is, larger \nscale objects (large objects) can be predicted. The detection rates of YOLOv5 vary with the \nsize of the feature map.\nYOLO takes use of the mean squared error as the loss function to optimize the model \nparameters; namely, the mean squared error of the S × S × (B × 5 + C) dimensional vector \noutput by using the vector of the corresponding real image as shown in Eq. (2)\nwhere coordError represents the coordinate error between the predicted and real data, \niouError shows the IoU error, and classError displays the classification error. The regres-\nsion of YOLOv5 roughly has a region range, and then adjusts the bounding box of the can-\ndidate regions to be closer to the bounding box. The deviation of adjusting the bounding \nbox of the candidate region to be closer to the real bounding box is the translation from the \noffset object position to the anchor point position.\nThe grid-based offset indicates that the anchor location is fixed, the offset is equal to \nthe translation between the object position and the anchor position, where  pw and ph are \ndenoted as the width and height which are applied to predict the offsets directly tw  and th . \nThe offset is shown as Eqs. (3), 4, 5 and (6):\nwhere cx and cy stand for rectangular coordinates, tx, ty, tw , th indicate the offsets to be \npredicted, bx, by, bw , bh represent the final results obtained by using the proposed model, \nrespecitvely. The pseudocode of our algorithm is listed as follows.\n(2)loss= coordError + iouError + classError\n(3)tx = log\n/parenleft.s4\nbbox x − cx\n1 − /parenleft.s1bbox x − cx\n/parenright.s1\n/parenright.s4\n(4)ty = log\n/parenleft.s4\nbbox y − cy\n1 − /parenleft.s1bbox y − cy\n/parenright.s1\n/parenright.s4\n(5)tw = log\n/parenleft.s3gtw\npw\n/parenright.s3\n(6)th = log\n/parenleft.s3gth\nph\n/parenright.s3\n7816 Multimedia Tools and Applications (2024) 83:7811–7825\n1 3\nA loop is applied to show the iteration process of the entire model. In this algorithm, we \ninput the entire image, then extract the feature map, execute the regression algorithm, and \nobtain the classification output.\n3.2  Transformer\nThe attention mechanism is an essential module in the transformer model. The attention \nmechanism generates a mask through the operations. The mask is scored to evaluate the \nfruit that needs to be recognized. As shown in Fig.  2, first of all, the model needs to per -\nform an embedding operation on the input data. After the embedding is completed, it will \nbe the input to the encoder layer. After the self-attention layer processes the data, the data \nis sent to the feedforward neural network. The calculation of the feeforward neural network \ncan be parallelized to obtain the output. The output will be input to the next encoder.\nThe essence of natural language processing is vectors, namely, converting images into \nvectors. The self-attention input is the entire sequence, and the input is also a sequence, \nwhich is the same as the input sequence. Each output vector must take into account the \ninformation of the entire input sequence, so self-attention introduces three matrices query, \nkey, and value. We convert the dataset images to vectors according to Eqs. (7) , (8) and  (9). \n(7)/u1D410= /u1D417× /u1D416/u1D42A\n(8)/u1D40A= /u1D417× /u1D416/u1D424\n(9)/u1D415= /u1D417× /u1D416/u1D42F\n\n7817Multimedia Tools and Applications (2024) 83:7811–7825 \n1 3\nwhere Q represents the query vector, K  (key vector) means a vector representing the rel-\nevance of the queried information and other information, V (value vector) shows the vector \nof the information being queried. The attention weight of X corresponding information V is \nproportional to Q multiply K. It is equivalent to saying: Attention weight is determined by \nX itself, so it is called self-attention. /u1D416/u1D42A , /u1D416/u1D424 , and /u1D416/u1D42F will be updated and changed accord-\ning to the task goal, ensuring the effectiveness of the self-attention mechanism. Q, K, and V \nform the inner product of vectors, which means that the angle between two vectors is repre-\nsented, and the projection of one vector on another vector. A large projection value means \nthat the correlation between the two vectors is high. The meaning of softmax is normaliza-\ntion. The row vector /u1D40AT in Eq. (10) indicates the inner product with itself and other two row \nvectors respectively, and shows the result of the inner product operation between each vector \nand itself as well as other vectors. As Transformer limits the computation of attention to \neach window, thereby we reduce the amount of computation as shown in Eq. (10):\n(10)Attention(/u1D410,/u1D40A,/u1D415)= Softmax\n�\n/u1D410/u1D40AT\n√\nd\n+ /u1D401\n�\n/u1D415\nInput\nEmbedding\nMulti-Head\nAttention\nAdd&Norm\nMulti-Head\nAttention\nAdd&Norm\nFeed\nForward\nAdd&Norm\nInputs\nOutput\nEmbedding\nMulti-Head\nAttention\nAdd&Norm\nInputs\nFeed\nForward\nAdd&Norm\nLinear\nSoftmax\nOutput\nProbablities\nPositional \nEncoding\n1.Input\n2.Encoder \nblock\n3.Decoder\nblock\n4.Output\nFig. 2  Transformer structure\n7818 Multimedia Tools and Applications (2024) 83:7811–7825\n1 3\nwhere B represents bounding box position, T means matrix transpose, the relative position \ncoding is added to Q and K in the Swin Tansformer model.\n3.3  Detection transformer,swin transformer and CornerNet\nDetection Transformer (DETR) was employed for visual object detection or panoramic \nsegmentation. This is the first object detection framework that successfully integrates \nTransformer as the central building block of the detection model. Compared with previous \ndetection methods, DETR effectively eliminates the need for hand-designed components, \nsuch as non-maximum suppression (NMS) program, anchor point generation, and so on. \nDETR is a straightforward end-to-end framework.\nIn Fig.  3, the input of the network is a 3-channel RGB image wherer the backbone is \nCNN, which extracts the features, and then combines the position information and input \nit into the encoder and decoder of the transformer model to obtain the detection results of \nthe model. Each output is a bounding box, where each box represents a tuple, including the \ncategory of the object and the position of the detection box.\nThe image input by using Swin Transformer is subjected to a convolutional layer for \npatch mapping, and each small block is mapped into a pixel to expand the channel. The \nfeature map is initially input to a stage, which comprises two layers of transformers. There \nis a pooling operation between stages to reduce the size of the data to be processed, from \nthe initial local information search to the extraction of global information.\nIn Fig.  4, black box shows patch, red box represents a window perform self-attention. \nWindow attention is applied to divide the image into different windows according to a \ngiven size. Each time, the attention of the Transformer is only calculated inside the win-\ndow. At this point, we see that the Swin Transformer and ResNet are designed with a net-\nwork with noticeable hierarchical results. The structure at the bottom tackles more and \nmore local data, and the network at the top processes fewer data but more semantic infor -\nmation. The difference is that Swin Transformers mainly takes advantage of Transformers \nto extract information, while ResNet takes use of a convolution kernel.\nLike YOLOv5 and Transformer, CornerNet also use of a backbone network. CornerNet \nis an anchor-free network, which simply expresses the anchor box as a key point symmetri-\ncal between the upper left corner and the lower right corner, then adopts a new pooling \nlayer corner pooling to find diagonal points better.\nThe CornerNet network is use of the Hourglass network as the backbone network, fol-\nlowed by two prediction models to predict the upper-left corner and lower-right corner \nof the box respectively. Each channel of CornerNet’s heatmap is a binary mask, which \napple\nEncoder\nEncoder\nEncoder\nEncoder\nEncoder\nEncoder\nDecoder\nDecoder\nDecoder\nDecoder\nDecoder\nDecoder\nBackbone\nImage features\nCNN\nPositional \nEncoding FFN\nFFN\nFFN\nFFN\nFFN\nFFN\nNo object\nClass, box\nNo object\nNo object\nClass, box\nClass, box\nPrediction headsEncoders, Decoders\nFig. 3  DETR structure\n7819Multimedia Tools and Applications (2024) 83:7811–7825 \n1 3\npredicts the position of each object key point pair, and each corner point makes a distance \nprediction with other corner points as an embeddings vector, and then similar embeddings \nor belong to the same distance between them, if a box is small, the anchor box will be gen-\nerated and the predicted result will be obtained.\n3.4  Experiment setting and evaluation method\nWe have utilized a mobile camera to take 2,000 photos of apples and pears as a dataset for \nsample labeling and model training, we ensure that each fruit category has enough samples \nto extract features.The training dataset for apple classification is essentially a set of images \nthat we manually labelled with the ground truth and the coordinates of the bounding box \npositions. We took explicit pictures of apples and pears in a well-lit environment, as shown \nin the input image, to ensure the quality of the experimental data. The fruits were classified \nfrom skin as the criterion for the ripeness classification. We define smooth fruit surface for \nthe maturity, rotten apple is labelled as an overripe class. The purpose of the experiment is \nto compare and find the best model for fruit detection, we choose epoch as the experimen-\ntal parameter. It is not enough to pass the complete dataset once in the neural network. We \nneed to pass the complete dataset multiple times. We justify the number of model iterations \nby adjusting the epoch value to compare the pros and cons of the model. Other param-\neters are set to batch size as 1, learning rate as 1.00 × 10 −4 , betas range (0.90, 0.99), and \nweight_decay as 5.00 × 10−2 . In our experiments, we make use of PyTorch as our comput-\ning platform. We are use of LabelMe (PyTorch) as the tool to make the dataset. In Fig.  1, \nthe orange box is the data set produced by LabelMe.\nIn fruit ripeness identification, the samples are classified into four groups: True positive \n(TP), false positive (FP), true negative (TN), and false-negative (FN) based on the combi-\nnation of their actual classes and the predicted classes. Precision means that the prediction \nrate of the model is correct in the classes of the samples, as shown in Eq. (11).\n(11)Precision= TP\nTP + FP\nLayer1 Layer1+1\nMSA W-MSA\nFig. 4  A sample of window attention and shift window attention\n7820 Multimedia Tools and Applications (2024) 83:7811–7825\n1 3\nIn our experimental results, AP50 means that the IOU threshold of the detector is higher \nthan 0.50. AP@50:5:95 refers to the value of IOU taken from 50.00% to 95.00%, the step \nlength is 5.00%, and the average of precisions under these IOUs is calculated.\n4  Results\nWe apply four YOLOv5 weights from small to large. In the process of training the model, \nwe are use of the validation dataset to test the current model in each epoch to obtain the \nloss and accuracy of the model, verify the loss and accuracy of each epoch. After the model \nis created and trained, we make use of the test dataset to test the model and get the accu-\nracy. In the test of YOLOv5 model, the larger the weight file of the model, the longer the \ntraining time required under the same epoch. YOLOv5 has the same overall architecture \nfor the size (s, m, l, x), but it takes use of different depths and widths in each sub-module, \nrespectively responding to the depth_multiple and width_multiple parameters in the yaml \nfile, where the parameters s, m, l, x of the YOLOv5 model will only be downsampled to 32 \ntimes and 3 prediction feature layers are deployed.\nWe find that too much data will lead to redundancy, the model cannot show good \nresults. In our experiments, we solved the problem of data redundancy. In Tables  1, 2, 3 \nand 4, we chose the smallest number of epochs to train the model. We see that if the num-\nber of model iterations is small, the feature map cannot be transmitted in the deep nets in \ntime, the model cannot be trained enough by using the characteristics of apple images. \nWe also observe that when the number of iterations reaches a balance, the model achieves \nTable 1  The results of precisions \nby using YOLOv5x weights Model Epoch Classes AP50 AP@0.5:0.95\nYOLOv5x 30 Ripe apple 0.9995 0.9640\nOverripe apple 0.9995 0.9230\nRipe pear 0.9995 0.9390\nOverripe pear 0.9995 0.9320\n50 Ripe apple 0.9995 0.9760\nOverripe apple 0.9996 0.9180\nRipe pear 0.9995 0.9750\nOverripe pear 0.9995 0.9770\nTable 2  The results of precisions \nby using YOLOv5l weights Model Epoch Classes AP50 AP@0.5:0.95\nYOLOv5l 30 Ripe apple 0.9995 0.9550\nOverripe apple 0.9995 0.9580\nRipe pear 0.9995 0.9370\nOverripe pear 0.9995 0.9320\n50 Ripe apple 0.9994 0.9850\nOverripe apple 0.9996 0.9340\nRipe pear 0.9994 0.9820\nOverripe pear 0.9994 0.9790\n7821Multimedia Tools and Applications (2024) 83:7811–7825 \n1 3\nbetter results. In Tables 1 and 4, as the IOU threshold increases, we see that the model with \na considerable weight produces a minor accuracy difference, hence, model is more stable.\nYOLOv5x has considerable weights and parameters, better results have been obtained, \nnot only the precision rate is better, but also the classification accuracy rate is also more \nthan the average. We see our experimental results of Faster R-CNN in the three-class clas-\nsification experiment; for a specific class, better results are obtained, but for the rest of the \ntwo classes, it is relatively poor. The classification results may be affected by the data dis-\ntribution in the training set.\nAs the number of epochs increases, the number of weights in the deep nets also \ngrows. The diversity of image data will affect the number of epochs. We need to con-\ntinuously adjust the number of epochs according to the characteristics of the fruit that \nthe model is trained.\nIf the weight gradually reduces, the mean average precision rate dips. The rate of \none class is slightly lower than that of other classes. So far, we have got the best model \nby using YOLOv5x. We have solved the problems in our experiments by increasing the \nnumber of samples in the dataset and adjusting the weights in the training process to \nmake the cost function drop. We have to consider the weight decay method to adjust \nYOLOv5 parameters. In view of our current experimental data that has eliminated the \nenvironmental noises, we need to consider whether the region of interest (ROI) is able \nto be segmented with much detail.\nIn the current experiments, we have improved the results significantly. However, \nbased on the dataset and the results of our experiments, we need a more powerful deep \nnet for further exploration. Pertaining to the YOLO models, we adjust the architecture \nand delete unnecessary parameters to make the model lighter with the same accuracy \nTable 3  The results of precisions \nby using YOLOv5m weights Model Epoch Classes AP50 AP@0.5:0.95\nYOLOv5m 30 Ripe apple 0.9991 0.9540\nOverripe apple 0.9995 0.9260\nRipe pear 0.9995 0.9020\nOverripe pear 0.9995 0.8980\n50 Ripe apple 0.9996 0.9370\nOverripe apple 0.9995 0.8960\nRipe pear 0.9995 0.9440\nOverripe pear 0.9996 0.8620\nTable 4  The results of precisions \nby using YOLOv5s weights Model Epoch Classes AP50 AP@0.5:0.95\nYOLOv5s 30 Ripe apple 0.9994 0.8390\nOverripe apple 0.9995 0.9390\nRipe pear 0.9995 0.8970\nOverripe pear 0.9995 0.8970\n50 Ripe apple 0.9994 0.9060\nOverripe apple 0.9995 0.8800\nRipe pear 0.9995 0.9230\nOverripe pear 0.9995 0.9220\n7822 Multimedia Tools and Applications (2024) 83:7811–7825\n1 3\nin future. But in Table  5, we see that the YOLO model with Swin Transformer model \nadded does not perform well. Although the model keeps the faster training speed of \nthe YOLO model, if the Transformer decoder is used to predict the bounding box of \nthe target object directly, the model has a long convergence time and poor tracking \nperformance.\nWe see the results of detection transformer in Table  6. The DETR model did not show \ngood results in the recognition of the ripeness of the fruit. We infer that the fruit is a small \nobject. The reason is that no fancy network optimization is needed, such as adding FPN or \nBiFPN part.\nThe shortcomings of detection transformer are very obvious. In the test part, the posi-\ntioning is not accurate. DETR makes up the shortcomings of the anchor-free algorithm. \nFor example, it is useful for processing overlapping scene objects. We infer that the detec-\ntion transformer model is better for large object detection.\nDifferent from Table 5, Swin Transformer in Table 7 combines Mask R-CNN to exhibit \na sequence-to-sequence model feature that makes it easier to combine multimodal data, \nthereby providing greater flexibility in network architecture design. We see that as the \nnumber of iterations increases, the model shows better results.\nTable 5  The precision of YOLO \nusing Swin Transformer module Model Epoch AP50 AP@0.5:0.95\nSwin + YOLO 10 0.1980 0.6360\n20 0.2050 0.5400\n30 0.5150 0.9950\n50 0.1850 0.5200\nTable 6  The precision of DETR with ResNet-50\nModel Epoch Classes AP@0.5:0.95 Average \ninference \ntime(seconds)\nDETR + ResNet-50 10 Ripe apple 0.7268 0.002\nOverripe apple 0.7374\nRipe pear 0.7958\nOverripe pear 0.7714\n20 Ripe apple 0.6819 0.004\nOverripe apple 0.7327\nRipe pear 0.9663\nOverripe pear 0.7046\n30 Ripe apple 0.7953 0.004\nOverripe apple 0.7959\nRipe pear 0.6832\nOverripe pear 0.8560\n50 Ripe apple 0.7296 0.002\nOverripe apple 0.7764\nRipe pear 0.7427\nOverripe pear 0.7450\n7823Multimedia Tools and Applications (2024) 83:7811–7825 \n1 3\nIn Tables  6 and 8 , we observe that when the accuracy rate is similar, the same \nanchor-free and backbone model CornerNet requires a longer average inference time. \nThis means that in the application, CornerNet cannot achieve a good detection speed. \nIn Tables  6 and 7 , both Swin Transformer and DETR can achieve a faster detection \nspeed. At the same time, Swin Transformer performs better precision than DETR with \nthe similar computing speed.\n5  Conclusion\nIn this paper, we have implemented fruit classification with the classes of multilevel \nripeness. Our models are able to automatically classify the ripeness degree of fruits. The \nexperimental results also show that the number of epochs needs to be controlled. Too \nmany iterations will lead to model redundancy, while too few samples make the model \nunderfitting, ultimately unable to get the natural characteristics of the apple images, and \nthus unable to get satisfactory results.\nOur contribution from this paper is to find the best model for fruit maturity recognition. \nThe model is able to be applied to automated multiple fruits classification in the agricul-\ntural industry. We attained the optimal model by adjusting the model and parameters. At the \nsame time, we also analyzed the reasons that affect our experiments, which are helpful for \nour subsequent experiments. From the detection results, the YOLO model has better results \nin detection and is more stable. The Swin Transformer model and Mask RCNN can also \nachieve fast and stable detection. However, the addition of the transformer mechanism to the \nYOLO model did not yield better results. The mask module set by the attention mechanism \nof Swin Transformer can better accomplish the shift of the feature map in the window.\nIn the future, we will have multistage goals in our experiment. High-accuracy fruit \ndetection in unstructured orchard environments remains particularly challenging due to \nvarying lighting conditions and occlusions from shadows that affect experimental accuracy. \nTable 7  The precision of Swin \nTransformer by using Mask \nR-CNN\nModel Epoch AP50 AP@0.5:0.95 Average \ninference \ntime(seconds)\nSwin + Mask \nR-CNN\n10 0.9360 0.8220 0.042\n20 0.9360 0.8360 0.048\n30 0.9580 0.9580 0.042\n50 0.9670 0.9670 0.044\nTable 8  The precision of \nCornerNet Model Epoch AP@0.5:0.95 Average \ninference \ntime(seconds)\nCorner-\nNet + Hourglass\n10 0.718 0.60\n20 0.245 0.58\n50 0.789 0.56\n7824 Multimedia Tools and Applications (2024) 83:7811–7825\n1 3\nIn this paper, we optimized the fruit dataset, but in the complex background, environmental \nnoises, dynamic detection and tracking of fruit targets from different perspectives, and the \naccuracy of identifying occluded visual targets with a fixed perspective, the recognition \nprocess, positioning, the impact of these issues on the experiments are also research topics \nfor our future projects.\nFunding  Open Access funding enabled and organized by CAUL and its Member Institutions\nData Availability The data is available upon request.\nDeclarations \nConflict of interest This work has not any funding support, it has not any conflicts of interests or competing \ninterests.\nOpen Access  This article is licensed under a Creative Commons Attribution 4.0 International License, \nwhich permits use, sharing, adaptation, distribution and reproduction in any medium or format, as long \nas you give appropriate credit to the original author(s) and the source, provide a link to the Creative Com-\nmons licence, and indicate if changes were made. The images or other third party material in this article \nare included in the article’s Creative Commons licence, unless indicated otherwise in a credit line to the \nmaterial. If material is not included in the article’s Creative Commons licence and your intended use is not \npermitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly \nfrom the copyright holder. To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/.\nReferences\n 1. Abozeid A, Alanazi R, Elhadad A, Taloba AI, Abd El-Aziz RM. (2022). A large-scale dataset and deep learning \nmodel for detecting and counting olive trees in satellite imagery. Computational Intelligence and Neuroscience\n 2. Agushinta RD, Medyawati H, Jatnika I. (2017). A method of cloud and image-based tracking for Indonesia \nfruit recognition. IEEE Int Conf Eng Tech Soc Sci (ICETSS). pp. 1–5\n 3. Ahmad T, Ma Y, Yahya M, Ahmad B, Nazir S (2020) Object detection through modified YOLO neural net-\nwork. Scientific Programming 2020:1–10. https:// doi. org/ 10. 1155/ 2020/ 84032 62\n 4. Alkalouti H, Masre M. (2021) Encoder-decoder model for automatic video captioning using YOLO algo-\nrithm. IEEE Int IOT, Electron Mechatron Conf (IEMTRONICS). pp. 1–4\n 5. Arivazhagan S, Shebiah RN, Nidhyanandhan SS, Ganesan L (2010) Fruit recognition using color and texture \nfeatures. J Emerg Trends Comput Inform Sci 1(2):90–94\n 6. Arkin E, Yadikar N, Muhtar Y, Ubul K. (2021). A survey of object detection based on CNN and Transformer. \nIEEE International Conference on Pattern Recognition and Machine Learning (PRML), pp. 99–108 \n 7. Bochkovskiy A, Wang CY, Liao HYM. (2020). YOLOv4: Optimal speed and accuracy of object detection. \narXiv preprint arXiv: 2004. 10934 \n 8. Chen Q, Wang Y, Yang T, Zhang X, Cheng J, Sun, J. (2021). You only look one-level feature. IEEE/CVF \nConfComput Vis Patt Recog. pp. 13039–13048 \n 9. Choudhury A, Biswas A, Prateek M, Chakrabarti A (2021) Agricultural informatics: Automation using the \nIoT and machine learning. John Wiley & Sons, Incorporated\n 10. Dai Z, Cai B, Lin Y, Chen J. (2021). Up-DETR: Unsupervised pre-training for object detection with trans-\nformers. IEEE/CVF Conf Comp Vis PattRecog. pp. 1601–1610 \n 11. Dou Q, Yan M (2021) Ocean small target detection in SAR image based on YOLOv5. J Eng 7(3):167–173\n 12. Fu Y, Nguyen M, Yan WQ (2022) Grading methods for fruit freshness based on deep learning. SN Com-\nput Sci 3(4):264\n 13. Glorot X, Bordes A, Bengio Y.(2011) Domain adaptation for large-scale sentiment classification: A deep \nlearning approach. ICML\n 14. Han X, Dang Y, Mei L, Wang Y, Li S, Zhou X. (2019) A novel part of speech tagging framework for \nNLP-based business process management. IEEE International Conference on Web Services (ICWS) pp. \n383–387\n 15. Hendria WF, Phan QT, Adzaka F, Jeong C. (2021) Combining Transformer and CNN for object detection \nin UAV imagery. ICT Express\n7825Multimedia Tools and Applications (2024) 83:7811–7825 \n1 3\n 16. Jiménez AR, Jain AK, Ceres R, Pons JL (1999) Automatic fruit recognition: A survey and new results \nusing range/attenuation images. Pattern Recogn 32(10):1719–1736\n 17. Kabir MS, Ndukwe IK, Awan EZS. (2021) Deep learning inspired vision based frameworks for drone \ndetection. IEEE International Conference on Electrical, Communication, and Computer Engineering \n(ICECCE), pp. 1–5\n 18. Kim S, Kim H (2021) Zero-centered fixed-point quantization with iterative retraining for deep convolu-\ntional neural network-based object detectors. IEEE Access 9:20828–20839\n 19. Kousik N, Natarajan Y, Raja RA, Kallam S, Patan R, Gandomi AH. (2021) Improved salient object detec-\ntion using hybrid convolution recurrent neural network. Expert Syst Appl. 166, 114064 \n 20. Kuznetsova A, Maleva T, Soloviev V. (2020) Detecting apples in orchards using YOLOv3 and YOLOv5 \nin general and close-up images. Int Symp Neural Netw. pp. 233–243\n 21. Mauri A, Khemmar R, Decoux B, Ragot N, Rossi R, Trabelsi R, Savatier X (2020) Deep learning for \nreal-time 3D multi-object detection, localization, and tracking: Application to smart mobility. Sensors \n20(2):532\n 22. Mekhalfi M, Nicolò C, Bazi Y, Al Rahhal M, Al Sharif N, Al Maghayreh E. (2021) Contrasting YOLOv5, \nTransformer, and EfficientDet detectors for crop circle detection in desert. IEEE Geosci Remote Sens Lett\n 23. Mhalla A, Chateau T, Amara NEB (2019) Spatio-temporal object detection by deep learning: Video-inter-\nlacing to improve multi-object tracking. Image Vis Comput 88:120–131\n 24. Pal SK, Pramanik A, Maiti J, Mitra P. (2021). Deep learning in multi-object detection and tracking: State \nof the art. Appl Intell. pp.1–30 \n 25. Qi J, Nguyen M, Yan W (2022). Waste classification from digital images using ConvNeXt. In Pacific-Rim \nSymp Image Vid Technol \n 26. Redmon J, Divvala S, Girshick R, Farhadi A. (2016). You Only Look Once: Unified, real-time object \ndetection. IEEE Conf Comput Vis Patt Recog. pp. 779–788 \n 27. Tang Z, Yang J, Pei Z, Song X. (2021). Coordinate-based anchor-free module for object detection. Appl \nIntell. 1–15\n 28. Tang Z, Yang J, Pei Z, Song X (2021) Coordinate-based anchor-free module for object detection. Appl \nIntell 22:1–5\n 29. Tang Y, Zhang Y, Zhu Y. (2020) A research on the fruit recognition algorithm based on the multi-fea-\nture fusion. International Conference on Mechanical, Control and Computer Engineering (ICMCCE) pp. \n1865–1869\n 30. Tsai M, Tseng H (2021) Enhancing the identification accuracy of deep learning object detection using \nnatural language processing. J Supercomput 2:1–6\n 31. Ünal HB, Vural E, Savaş BK, Becerikli Y. (2020). Fruit recognition and classification with deep learning \nsupport on embedded system (FruitNet). Innovations in Intelligent Systems and Applications Conference \n(ASYU), pp. 1–5 \n 32. Wang X, Hua X, Xiao F, Li Y, Hu X, Sun P (2018) Multi-object detection in traffic scenes based on \nimproved SSD. Electronics 7(11):302\n 33. Wu D, Lv S, Jiang M, Song H (2020) Using channel pruning-based YOLOv4 deep learning algorithm \nfor the real-time and accurate detection of apple flowers in natural environments. Comput Electron Agric \n178:105742\n 34. Xia Y, Nguyen M, Yan WQ. (2022). A real-time kiwifruit detection based on improved YOLOv7. In \nIVCNZ, pp. 48–61 \n 35. Xiao B. (2019) Apple Ripeness Identification Using Deep Learning. Rearch Report, Auckland. University \nof Technology, New Zealand\n 36. Xiao B, Nguyen M, Yan W. (2021). Apple ripeness identification using deep learning. In: Int Symp Geom \nVis (ISGV). pp.53–67\n 37. Zhang X, Wan F, Liu C, Ji X, Ye Q. (2021) Learning to match anchors for visual object detection. IEEE \nTrans Patt Anal Mach Intell (2021)\n 38. Zhao K, Yan WQ. (2021) Fruit detection from digital images using CenterNet. In Int Symp Geomet Vis \n(ISGV)\nPublisher’s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and \ninstitutional affiliations.",
  "topic": "Ripeness",
  "concepts": [
    {
      "name": "Ripeness",
      "score": 0.9837337732315063
    },
    {
      "name": "Computer science",
      "score": 0.8039840459823608
    },
    {
      "name": "Artificial intelligence",
      "score": 0.7050081491470337
    },
    {
      "name": "Transformer",
      "score": 0.514339804649353
    },
    {
      "name": "Identification (biology)",
      "score": 0.46333327889442444
    },
    {
      "name": "Deep learning",
      "score": 0.41895297169685364
    },
    {
      "name": "Computer vision",
      "score": 0.40634703636169434
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.40536025166511536
    },
    {
      "name": "Machine learning",
      "score": 0.3737327456474304
    },
    {
      "name": "Horticulture",
      "score": 0.0567341148853302
    },
    {
      "name": "Voltage",
      "score": 0.0
    },
    {
      "name": "Physics",
      "score": 0.0
    },
    {
      "name": "Biology",
      "score": 0.0
    },
    {
      "name": "Ripening",
      "score": 0.0
    },
    {
      "name": "Quantum mechanics",
      "score": 0.0
    },
    {
      "name": "Botany",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I39854758",
      "name": "Auckland University of Technology",
      "country": "NZ"
    }
  ],
  "cited_by": 19
}