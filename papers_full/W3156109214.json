{
  "title": "Multiscale Vision Transformers",
  "url": "https://openalex.org/W3156109214",
  "year": 2022,
  "authors": [
    {
      "id": "https://openalex.org/A4225907968",
      "name": "Fan, Haoqi",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2122779520",
      "name": "Xiong, Bo",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3176074116",
      "name": "Mangalam, Karttikeya",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2744284997",
      "name": "Li, Yanghao",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2261397833",
      "name": "Yan Zhicheng",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A2749440676",
      "name": "Malik, Jitendra",
      "affiliations": []
    },
    {
      "id": "https://openalex.org/A3160965359",
      "name": "Feichtenhofer, Christoph",
      "affiliations": []
    }
  ],
  "references": [
    "https://openalex.org/W2618799552",
    "https://openalex.org/W3034572008",
    "https://openalex.org/W2938458886",
    "https://openalex.org/W3034609440",
    "https://openalex.org/W3134565071",
    "https://openalex.org/W2963399829",
    "https://openalex.org/W2984287396",
    "https://openalex.org/W3117707723",
    "https://openalex.org/W3147387781",
    "https://openalex.org/W2998508940",
    "https://openalex.org/W2793904650",
    "https://openalex.org/W2518108298",
    "https://openalex.org/W3126721948",
    "https://openalex.org/W3132503749",
    "https://openalex.org/W2952444318",
    "https://openalex.org/W2626778328",
    "https://openalex.org/W2768282280",
    "https://openalex.org/W2337252826",
    "https://openalex.org/W2147800946",
    "https://openalex.org/W197865394",
    "https://openalex.org/W2022735534",
    "https://openalex.org/W2963150697",
    "https://openalex.org/W2962835968",
    "https://openalex.org/W3128723389",
    "https://openalex.org/W3000514857",
    "https://openalex.org/W3164543136",
    "https://openalex.org/W2108598243",
    "https://openalex.org/W2462831000",
    "https://openalex.org/W2156303437",
    "https://openalex.org/W3126337037",
    "https://openalex.org/W2302255633",
    "https://openalex.org/W3168649818",
    "https://openalex.org/W3184564979",
    "https://openalex.org/W3034885317",
    "https://openalex.org/W2949605076",
    "https://openalex.org/W2806331055",
    "https://openalex.org/W2968124245",
    "https://openalex.org/W2342662179",
    "https://openalex.org/W3128099838",
    "https://openalex.org/W2622263826",
    "https://openalex.org/W2981385151",
    "https://openalex.org/W3016719260",
    "https://openalex.org/W2899663614",
    "https://openalex.org/W2955874753",
    "https://openalex.org/W2990152177",
    "https://openalex.org/W2992308087",
    "https://openalex.org/W2963563276",
    "https://openalex.org/W3129436779",
    "https://openalex.org/W3034429256",
    "https://openalex.org/W3129576130",
    "https://openalex.org/W3135367836",
    "https://openalex.org/W2097117768",
    "https://openalex.org/W1536680647",
    "https://openalex.org/W2946948417",
    "https://openalex.org/W2963091558",
    "https://openalex.org/W2970608575",
    "https://openalex.org/W2981165461",
    "https://openalex.org/W2770804203",
    "https://openalex.org/W3114896399",
    "https://openalex.org/W3034588855",
    "https://openalex.org/W3035452548",
    "https://openalex.org/W3130071011",
    "https://openalex.org/W3102631365",
    "https://openalex.org/W3034445277",
    "https://openalex.org/W1968245656",
    "https://openalex.org/W3045923047",
    "https://openalex.org/W3033210410",
    "https://openalex.org/W2963524571",
    "https://openalex.org/W2613718673",
    "https://openalex.org/W2163605009",
    "https://openalex.org/W3091156754",
    "https://openalex.org/W2983446232",
    "https://openalex.org/W2619947201",
    "https://openalex.org/W2964308564",
    "https://openalex.org/W3096833468",
    "https://openalex.org/W3153465022",
    "https://openalex.org/W3112160422",
    "https://openalex.org/W2963241429",
    "https://openalex.org/W3102892879",
    "https://openalex.org/W3037784242",
    "https://openalex.org/W2963722382",
    "https://openalex.org/W3043840704",
    "https://openalex.org/W2625366777",
    "https://openalex.org/W2940744433",
    "https://openalex.org/W2928165649",
    "https://openalex.org/W3124149278",
    "https://openalex.org/W3121523901",
    "https://openalex.org/W2194775991",
    "https://openalex.org/W3116489684",
    "https://openalex.org/W3127751679",
    "https://openalex.org/W3035303837",
    "https://openalex.org/W2963093735",
    "https://openalex.org/W1904365287",
    "https://openalex.org/W2331143823",
    "https://openalex.org/W2964080601",
    "https://openalex.org/W2103504761",
    "https://openalex.org/W2773514261",
    "https://openalex.org/W3143320354",
    "https://openalex.org/W1677182931",
    "https://openalex.org/W3030163527",
    "https://openalex.org/W2887051120",
    "https://openalex.org/W2949718784",
    "https://openalex.org/W3033529678",
    "https://openalex.org/W3094502228",
    "https://openalex.org/W3099388488",
    "https://openalex.org/W3108995912",
    "https://openalex.org/W2883275382",
    "https://openalex.org/W3174068320",
    "https://openalex.org/W2111624873"
  ],
  "abstract": "We present Multiscale Vision Transformers (MViT) for video and image recognition, by connecting the seminal idea of multiscale feature hierarchies with transformer models. Multiscale Transformers have several channel-resolution scale stages. Starting from the input resolution and a small channel dimension, the stages hierarchically expand the channel capacity while reducing the spatial resolution. This creates a multiscale pyramid of features with early layers operating at high spatial resolution to model simple low-level visual information, and deeper layers at spatially coarse, but complex, high-dimensional features. We evaluate this fundamental architectural prior for modeling the dense nature of visual signals for a variety of video recognition tasks where it outperforms concurrent vision transformers that rely on large scale external pre-training and are 5-10x more costly in computation and parameters. We further remove the temporal dimension and apply our model for image classification where it outperforms prior work on vision transformers. Code is available at: https://github.com/facebookresearch/SlowFast",
  "full_text": "Multiscale Vision Transformers\nHaoqi Fan *, 1 Bo Xiong *, 1 Karttikeya Mangalam *, 1, 2\nYanghao Li*, 1 Zhicheng Yan1 Jitendra Malik 1, 2 Christoph Feichtenhofer *, 1\n1Facebook AI Research 2UC Berkeley\nAbstract\nWe present Multiscale Vision Transformers (MViT) for\nvideo and image recognition, by connecting the seminal idea\nof multiscale feature hierarchies with transformer models.\nMultiscale Transformers have several channel-resolution\nscale stages. Starting from the input resolution and a small\nchannel dimension, the stages hierarchically expand the\nchannel capacity while reducing the spatial resolution. This\ncreates a multiscale pyramid of features with early lay-\ners operating at high spatial resolution to model simple\nlow-level visual information, and deeper layers at spatially\ncoarse, but complex, high-dimensional features. We eval-\nuate this fundamental architectural prior for modeling the\ndense nature of visual signals for a variety of video recog-\nnition tasks where it outperforms concurrent vision trans-\nformers that rely on large scale external pre-training and\nare 5-10×more costly in computation and parameters. We\nfurther remove the temporal dimension and apply our model\nfor image classiﬁcation where it outperforms prior work\non vision transformers. Code is available at: https:\n//github.com/facebookresearch/SlowFast.\n1. Introduction\nWe begin with the intellectual history of neural network\nmodels for computer vision. Based on their studies of cat\nand monkey visual cortex, Hubel and Wiesel [55] developed\na hierarchical model of the visual pathway with neurons\nin lower areas such as V1 responding to features such as\noriented edges and bars, and in higher areas to more spe-\nciﬁc stimuli. Fukushima proposed the Neocognitron [32], a\nneural network architecture for pattern recognition explic-\nitly motivated by Hubel and Wiesel’s hierarchy. His model\nhad alternating layers of simple cells and complex cells, thus\nincorporating downsampling, and shift invariance, thus incor-\nporating convolutional structure. LeCun et al. [65] took the\nadditional step of using backpropagation to train the weights\nof this network. But already the main aspects of hierarchy of\nvisual processing had been established: (i) Reduction in spa-\ntial resolution as one goes up the processing hierarchy and\n(ii) Increase in the number of different “channels”, with each\n*Equal technical contribution.\nWHC\nInput scale1 scale2 scale3\nFigure 1. Multiscale Vision Transformerslearn a hierarchy from\ndense (in space) and simple (in channels) to coarse and complex\nfeatures. Several resolution-channel scale stages progressively\nincrease the channel capacity of the intermediate latent sequence\nwhile reducing its length and thereby spatial resolution.\nchannel corresponding to ever more specialized features.\nIn a parallel development, the computer vision com-\nmunity developed multiscale processing, sometimes called\n“pyramid” strategies, with Rosenfeld and Thurston [85], Burt\nand Adelson [8], Koenderink [61], among the key papers.\nThere were two motivations (i) To decrease the computing re-\nquirements by working at lower resolutions and (ii) A better\nsense of “context” at the lower resolutions, which could then\nguide the processing at higher resolutions (this is a precursor\nto the beneﬁt of “depth” in today’s neural networks.)\nThe Transformer [98] architecture allows learning ar-\nbitrary functions deﬁned over sets and has been scalably\nsuccessful in sequence tasks such as language comprehen-\nsion [26] and machine translation [7]. Fundamentally, a\ntransformer uses blocks with two basic operations. First,\nis an attention operation [4] for modeling inter-element re-\nlations. Second, is a multi-layer perceptron (MLP), which\nmodels relations within an element. Intertwining these oper-\nations with normalization [2] and residual connections [44]\nallows transformers to generalize to a wide variety of tasks.\nRecently, transformers have been applied to key com-\nputer vision tasks such as image classiﬁcation. In the spirit\nof architectural universalism, vision transformers [25, 95]\napproach performance of convolutional models across a va-\nriety of data and compute regimes. By only having a ﬁrst\nlayer that ‘patchiﬁes’ the input in spirit of a 2D convolu-\ntion, followed by a stack of transformer blocks, the vision\ntransformer aims to showcase the power of the transformer\narchitecture using little inductive bias.\n1\narXiv:2104.11227v1  [cs.CV]  22 Apr 2021\nIn this paper, our intention is to connect the seminal idea\nof multiscale feature hierarchies with the transformer model.\nWe posit that the fundamental vision principle of resolution\nand channel scaling, can be beneﬁcial for transformer models\nacross a variety of visual recognition tasks.\nWe present Multiscale Vision Transformers (MViT), a\ntransformer architecture for modeling visual data such as im-\nages and videos. Consider an input image as shown in Fig. 1.\nUnlike conventional transformers, which maintain a constant\nchannel capacity and resolution throughout the network,\nMultiscale Transformers have several channel-resolution\n‘scale’ stages. Starting from the image resolution and a small\nchannel dimension, the stages hierarchically expand the\nchannel capacity while reducing the spatial resolution. This\ncreates a multiscale pyramid of feature activations inside the\ntransformer network, effectively connecting the principles\nof transformers with multi scale feature hierarchies.\nOur conceptual idea provides an effective design advan-\ntage for vision transformer models. The early layers of our\narchitecture can operate at high spatial resolution to model\nsimple low-level visual information, due to the lightweight\nchannel capacity. In turn, the deeper layers can effectively\nfocus on spatially coarse but complex high-level features\nto model visual semantics. The fundamental advantage of\nour multiscale transformer arises from the extremely dense\nnature of visual signals, a phenomenon that is even more\npronounced for space-time visual signals captured in video.\nA noteworthy beneﬁt of our design is the presence of\nstrong implicit temporal bias in video multiscale models. We\nshow that vision transformer models [25] trained on natural\nvideo suffer no performance decay when tested on videos\nwith shufﬂed frames. This indicates that these models are not\neffectively using the temporal information and instead rely\nheavily on appearance. In contrast, when testing our MViT\nmodels on shufﬂed frames, we observe signiﬁcant accuracy\ndecay, indicating strong use of temporal information.\nOur focus in this paper is video recognition, and we de-\nsign and evaluate MViT for video tasks (Kinetics [59, 10],\nCharades [86], SSv2 [38] and A V A [39]). MViT provides\na signiﬁcant performance gain over concurrent video trans-\nformers [78, 6, 1],without any external pre-training data.\nIn Fig. A.4 we show the computation/accuracy trade-off\nfor video-level inference, when varying the number of tem-\nporal clips used in MViT. The vertical axis shows accuracy\non Kinetics-400 and the horizontal axis the overall infer-\nence cost in FLOPs for different models, MViT and concur-\nrent ViT [25] video variants: VTN [78], TimeSformer [6],\nViViT [1]. To achieve similar accuracy level as MViT, these\nmodels require signiﬁcant more computation and parameters\n(e.g. ViViT-L [1] has 6.8×higher FLOPs and 8.5×more pa-\nrameters at equal accuracy, more analysis in §A.1) and need\nlarge-scale external pre-training on ImageNet-21K (which\ncontains around 60×more labels than Kinetics-400).\nIN-1K\nIN-21K\nIN-21KIN-21K\n+4.6% acc\nat 1/5 FLOPs\nat 1/3 Params \nwithout ImageNet \nMViT-B 16x4\nMViT-B 32x2\n[1] ViViT-L ImageNet-21K\n[6] TimeSformer ImageNet-21K\n[78] VTN ImageNet-1K / 21K\nInference cost per video in TFLOPs (# of multiply-adds x 1012)\nKinetics top-1 val accuracy (%) \nFigure 2. Accuracy/complexity trade-off on Kinetics-400 for\nvarying # of inference clips per video shown in MViT curves.\nConcurrent vision-transformer based methods [78,6,1] require over\n5×more computation and large-scale external pre-training on\nImageNet-21K (IN-21K), to achieve equivalent MViT accuracy.\nWe further apply our architecture to an image classiﬁ-\ncation task on ImageNet [21], by simply removing the tem-\nporal dimension of the video model found with ablation\nexperiments on Kinetics, and show signiﬁcant gains over\nsingle-scale vision transformers for image recognition.\n2. Related Work\nConvolutional networks (ConvNets). Incorporating down-\nsampling, shift invariance, and shared weights, ConvNets\nare de-facto standard backbones for computer vision tasks\nfor image [65, 62, 88, 90, 46, 12, 15, 34, 93, 81, 41] and\nvideo [87, 31, 11, 79, 69, 106, 96, 30, 105, 35, 29, 117, 57].\nSelf-attention in ConvNets. Self-attention mechanisms\nhas been used for image understanding [82, 114, 52], un-\nsupervised object recognition [74] as well as vision and\nlanguage [77, 66]. Hybrids of self-attention operations and\nconvolutional networks have also been applied to image\nunderstanding [51] and video recognition [101].\nVision Transformers. Much of current enthusiasm in ap-\nplication of Transformers [98] to vision tasks commences\nwith the Vision Transformer (ViT) [25] and Detection Trans-\nformer [9]. We build directly upon [25] with a staged model\nallowing channel expansion and resolution downsampling.\nDeiT [95] proposes a data efﬁcient approach to training ViT.\nOur training recipe builds on, and we compare our image\nclassiﬁcation models to, DeiT under identical settings.\nAn emerging thread of work aims at applying transform-\ners to vision tasks such as object detection [5], semantic\nsegmentation [115, 99], 3D reconstruction [72], pose estima-\ntion [107], generative modeling [14], image retrieval [27],\nmedical image segmentation [13, 97, 111], point clouds [40],\nvideo instance segmentation [103], object re-identiﬁcation\n[47], video retrieval [33], video dialogue [64], video object\ndetection [110] and multi-modal tasks [73, 23, 80, 53, 108].\nA separate line of works attempts at modeling visual data\nwith learnt discretized token sequences [104, 83, 14, 109, 18].\n2\nEfﬁcient Transformers. Recent works [100, 60, 17, 94, 20,\n16, 67] reduce the quadratic attention complexity to make\ntransformers more efﬁcient for natural language processing\napplications, which is complementary to our approach.\nThree concurrent works propose a ViT-based architecture\nfor video [78, 6, 1]. However, these methods rely on pre-\ntraining on vast amount of external data such as ImageNet-\n21K [21], and thus use the vanilla ViT [25] with minimal\nadaptations. In contrast, our MViT introduces multiscale\nfeature hierarchies for transformers, allowing effective mod-\neling of dense visual input without large-scale external data.\n3. Multiscale Vision Transformer (MViT)\nOur generic Multiscale Transformer architecture builds\non the core concept ofstages. Each stage consists of multiple\ntransformer blocks with speciﬁc space-time resolution and\nchannel dimension. The main idea of Multiscale Transform-\ners is to progressively expand the channel capacity, while\npooling the resolution from input to output of the network.\n3.1. Multi Head Pooling Attention\nWe ﬁrst describe Multi Head Pooling Attention (MHPA),\na self attention operator that enables ﬂexible resolution mod-\neling in a transformer block allowing Multiscale Transform-\ners to operate at progressively changing spatiotemporal reso-\nlution. In contrast to original Multi Head Attention (MHA)\noperators [98], where the channel dimension and the spatio-\ntemporal resolution remains ﬁxed, MHPApools the sequence\nof latent tensors to reduce the sequence length (resolution)\nof the attended input. Fig. 3 shows the concept.\nConcretely, consider a D dimensional input tensor X\nof sequence length L, X ∈RL×D. Following MHA [25],\nMHPA projects the input X into intermediate query tensor\nˆQ ∈RL×D, key tensor ˆK ∈RL×D and value tensor ˆV ∈\nRL×D with linear operations\nˆQ= XWQ ˆK = XWK ˆV = XWV\n/ with weights WQ,WK,WV of dimensions D×D. The\nobtained intermediate tensors are then pooled in sequence\nlength, with a pooling operator Pas described below.\nPooling Operator. Before attending the input, the interme-\ndiate tensors ˆQ, ˆK, ˆV are pooled with the pooling operator\nP(·; Θ) which is the cornerstone of our MHPA and, by ex-\ntension, of our Multiscale Transformer architecture.\nThe operator P(·; Θ) performs a pooling kernel com-\nputation on the input tensor along each of the dimensions.\nUnpacking Θ as Θ := ( k,s,p), the operator employs a\npooling kernel k of dimensions kT ×kH ×kW , a stride s\nof corresponding dimensions sT ×sH ×sW and a padding\np of corresponding dimensions pT ×pH ×pW to reduce an\nLinear\nX\n   PoolQ\nMatMul & Scale\nSoftmax\nMatMul\nTHW × D   \nAdd & Norm\nLinear Linear\n   PoolK\n^K ^V^Q\nTHW × D   THW × D   THW × D  \nK\nTHW × D   \nQ\n~~~THW × D   ^^^\nV\nTHW × D   ~~~THW ×    ^^^ THW ~~~\nTHW × D   ^^^\n   PoolV   PoolQ\nFigure 3. Pooling Attention is a ﬂexible attention mechanism that\n(i) allows obtaining the reduced space-time resolution ( ˆT ˆH ˆW) of\nthe input (THW ) by pooling the query, Q= P( ˆQ; ΘQ), and/or\n(ii) computes attention on a reduced length ( ˜T ˜H ˜W) by pooling the\nkey, K = P( ˆK; ΘK), and value, V = P( ˆV; ΘV ), sequences.\ninput tensor of dimensions L = T ×H×W to ˜L given by,\n˜L =\n⌊L + 2p −k\ns\n⌋\n+ 1\nwith the equation applying coordinate-wise. The pooled\ntensor is ﬂattened again yielding the output of P(Y; Θ) ∈\nR˜L×D with reduced sequence length, ˜L= ˜T ×˜H× ˜W.\nBy default we use overlapping kernels k with shape-\npreserving padding p in our pooling attention operators, so\nthat ˜L, the sequence length of the output tensor P(Y; Θ),\nexperiences an overall reduction by a factor of sT sHsW .\nPooling Attention. The pooling operator P(·; Θ)is applied\nto all the intermediate tensors ˆQ, ˆK and ˆV independently\nwith chosen pooling kernels k, stride s and padding p. De-\nnoting θyielding the pre-attention vectors Q= P( ˆQ; ΘQ),\nK = P( ˆK; ΘK) and V = P( ˆV; ΘV ) with reduced se-\nquence lengths. Attention is now computed on these short-\nened vectors, with the operation,\nAttention(Q,K,V ) = Softmax(QKT /\n√\nD)V.\nNaturally, the operation induces the constraints sK ≡sV\non the pooling operators. In summary, pooling attention is\ncomputed as,\nPA(·) = Softmax(P(Q; ΘQ)P(K; ΘK)T /\n√\nd)P(V; ΘV ),\nwhere\n√\ndis normalizing the inner product matrix row-wise.\nThe output of the Pooling attention operation thus has its\nsequence length reduced by a stride factor of sQ\nT sQ\nHsQ\nW fol-\nlowing the shortening of the query vector Qin P(·).\n3\nstage operators output sizes\ndata layer stride τ×1×1 T×H×W\npatch1\n1×16×16, D D×T×H\n16 ×W\n16stride 1×16×16\nscale2\n[\nMHA(D)\nMLP(4D)\n]\n×N D×T×H\n16 ×W\n16\nTable 1. Vision Transformers (ViT) base model starts from a\ndata layer that samples visual input with rateτ×1×1 to T×H×W\nresolution, where T is the number of framesHheight and W width.\nThe ﬁrst layer, patch1 projects patches (of shape1×16×16) to form\na sequence, processed by a stack of N transformer blocks (stage2)\nat uniform channel dimension (D) and resolution (T×H\n16 ×W\n16 ).\nMultiple heads. As in [98] the computation can be paral-\nlelized by considering hheads where each head is perform-\ning the pooling attention on a non overlapping subset ofD/h\nchannels of the Ddimensional input tensor X.\nComputational Analysis. Since attention computation\nscales quadratically w.r.t. the sequence length, pooling the\nkey, query and value tensors has dramatic beneﬁts on the\nfundamental compute and memory requirements of the Mul-\ntiscale Transformer model. Denoting the sequence length\nreduction factors by fQ, fK and fV we have,\nfj = sj\nT ·sj\nH ·sj\nW , ∀j ∈{Q,K,V }.\nConsidering the input tensor to P(; Θ) to have dimensions\nD×T ×H ×W, the run-time complexity of MHPA is\nO(THWD/h (D+ THW/fQfK)) per head and the mem-\nory complexity is O(THWh (D/h+ THW/fQfK)).\nThis trade-off between the number of channels D and\nsequence length term THW/fQfK informs our design\nchoices about architectural parameters such as number of\nheads and width of layers. We refer the reader to the sup-\nplement for a detailed analysis and discussions on the time-\nmemory complexity trade-off.\n3.2. Multiscale Transformer Networks\nBuilding upon Multi Head Pooling Attention (Sec. 3.1),\nwe describe the Multiscale Transformer model for visual\nrepresentation learning using exclusively MHPA and MLP\nlayers. First, we present a brief review of the Vision Trans-\nformer Model that informs our design.\nPreliminaries: Vision Transformer (ViT). The Vision\nTransformer (ViT) architecture [25] starts by dicing the input\nvideo of resolution T×H×W, where T is the number of\nframes H the height and W the width, into non-overlapping\npatches of size 1×16×16 each, followed by point-wise ap-\nplication of linear layer on the ﬂattened image patches to to\nproject them into the latent dimension, D, of the transformer.\nThis is equivalent to a convolution with equal kernel size\nand stride of 1×16×16 and is shown as patch1 stage in the\nmodel deﬁnition in Table 1.\nNext, a positional embedding E ∈RL×D is added to\neach element of the projected sequence of length L with\nstages operators output sizes\ndata layer stride τ×1×1 D×T×H×W\ncube1\ncT ×cH×cW , D D×T\nsT\n×H\n4 ×W\n4stride sT ×4×4\nscale2\n[\nMHPA(D)\nMLP(4D)\n]\n×N2 D×T\nsT\n×H\n4 ×W\n4\nscale3\n[\nMHPA(2D)\nMLP(8D)\n]\n×N3 2D×T\nsT\n×H\n8 ×W\n8\nscale4\n[\nMHPA(4D)\nMLP(16D)\n]\n×N4 4D×T\nsT\n×H\n16 ×W\n16\nscale5\n[\nMHPA(8D)\nMLP(32D)\n]\n×N5 8D×T\nsT\n×H\n32 ×W\n32\nTable 2. Multiscale Vision Transformers (MViT) base model.\nLayer cube1, projects dense space-time cubes (of shapect×cy×cw)\nto Dchannels to reduce spatio-temporal resolution to T\nsT\n×H\n4 ×W\n4 .\nThe subsequent stages progressively down-sample this resolution\n(at beginning of a stage) with MHPA while simultaneously increas-\ning the channel dimension, in MLP layers, (at the end of a stage).\nEach stage consists ofN∗ transformer blocks, denoted in [brackets].\ndimension Dto encode the positional information and break\npermutation invariance. A learnable class embedding is\nappended to the projected image patches.\nThe resulting sequence of length of L+ 1 is then pro-\ncessed sequentially by a stack of N transformer blocks, each\none performing attention (MHA [98]), multi-layer percep-\ntron (MLP) and layer normalization ( LN [3]) operations.\nConsidering X to be the input of the block, the output of a\nsingle transformer block, Block(X) is computed by\nX1 = MHA(LN(X)) + X\nBlock(X) = MLP(LN(X1)) + X1.\nThe resulting sequence after N consecutive blocks is layer-\nnormalized and the class embedding is extracted and passed\nthrough a linear layer to predict the desired output (e.g. class).\nBy default, the hidden dimension of the MLP is 4 D. We\nrefer the reader to [25, 98] for details.\nIn context of the present paper, it is noteworthy that ViT\nmaintains a constant channel capacity and spatial resolution\nthroughout all the blocks (see Table 1).\nMultiscale Vision Transformers (MViT). Our key con-\ncept is to progressively grow the channel resolution (i.e. di-\nmension), while simultaneously reducing the spatiotemporal\nresolution (i.e. sequence length) throughout the network. By\ndesign, our MViT architecture hasﬁne spacetime (and coarse\nchannel) resolution in early layers that is up-/downsampled\nto a coarse spacetime (and ﬁne channel) resolution in late\nlayers. MViT is shown in Table 2.\nScale stages. A scale stage is deﬁned as a set of N trans-\nformer blocks that operate on the same scale with identi-\ncal resolution across channels and space-time dimensions\nD×T×H×W. At the input (cube1 in Table 2), we project\nthe patches (or cubes if they have a temporal extent) to a\nsmaller channel dimension (e.g. 8×smaller than a typical\nViT model), but long sequence (e.g. 4×4 = 16×denser than\na typical ViT model; cf. Table 1).\n4\nstage operators output sizes\ndata stride 8×1×1 8×224×224\npatch1\n1×16×16, 768 768×8×14×14stride 1×16×16\nscale2\n[\nMHA(768)\nMLP(3072)\n]\n×12 768×8×14×14\n(a) ViT-B with 179.6G FLOPs, 87.2M param,\n16.8G memory, and 68.5% top-1 accuracy.\nstage operators output sizes\ndata stride 4×1×1 16×224×224\ncube1\n3×7×7, 96 96×8×56×56stride 2×4×4\nscale2\n[\nMHPA(96)\nMLP(384)\n]\n×1 96×8×56×56\nscale3\n[\nMHPA(192)\nMLP(768)\n]\n×2 192×8×28×28\nscale4\n[\nMHPA(384)\nMLP(1536)\n]\n×11 384×8×14×14\nscale5\n[\nMHPA(768)\nMLP(3072)\n]\n×2 768×8×7×7\n(b) MViT-B with 70.5G FLOPs, 36.5M param,\n6.8G memory, and 77.2% top-1 accuracy.\nstage operators output sizes\ndata stride 4×1×1 16×224×224\ncube1\n3×8×8, 128 128×8×28×28stride 2×8×8\nscale2\n[\nMHPA(128)\nMLP(512)\n]\n×3 128×8×28×28\nscale3\n[\nMHPA(256)\nMLP(1024)\n]\n×7 256×8×14×14\nscale4\n[\nMHPA(512)\nMLP(2048)\n]\n×6 512×8×7×7\n(c) MViT-S with 32.9G FLOPs, 26.1M param,\n4.3G memory, and 74.3% top-1 accuracy.\nTable 3. Comparing ViT-B to two instantiations of MViT with varying complexity, MViT-S in (c) and MViT-B in (b). MViT-S operates at\na lower spatial resolution and lacks a ﬁrst high-resolution stage. The top-1 accuracy corresponds to 5-Center view testing on K400. FLOPs\ncorrespond to a single inference clip, and memory is for a training batch of 4 clips. See Table 2 for the general MViT-B structure.\nAt a stage transition (e.g. scale1 to scale2 to in Table 2),\nthe channel dimension of the processed sequence is up-\nsampled while the length of the sequence is down-sampled.\nThis effectively reduces the spatio-temporal resolution of the\nunderlying visual data while allowing the network to assimi-\nlate the processed information in more complex features.\nChannel expansion. When transitioning from one stage\nto the next, we expand the channel dimension by increas-\ning the output of the ﬁnal MLP layer in the previous stage\nby a factor that is relative to the resolution change intro-\nduced at the stage. Concretely, if we down-sample the\nspace-time resolution by 4 ×, we increase the channel di-\nmension by 2×. For example, scale3 to scale4 changes reso-\nlution from 2D×T\nsT\n×H\n8 ×T\n8 to 4D×T\nsT\n×H\n16 ×T\n16 in Table 2.\nThis roughly preserves the computational complexity across\nstages, and is similar to ConvNet design principles [87, 45].\nQuery pooling. The pooling attention operation affords\nﬂexibility not only in the length of key and value vectors\nbut also in the length of the query, and thereby output, se-\nquence. Pooling the query vector P(Q; k; p; s) with a kernel\ns ≡(sQ\nT ,sQ\nH,sQ\nW ) leads to sequence reduction by a factor of\nsQ\nT ·sQ\nH ·sQ\nW . Since, our intention is to decrease resolution\nat the beginning of a stage and then preserve this resolution\nthroughout the stage, only the ﬁrst pooling attention operator\nof each stage operates at non-degenerate query stridesQ >1,\nwith all other operators constrained to sQ ≡(1,1,1).\nKey-Value pooling. Unlike Query pooling, changing the se-\nquence length of keyKand value V tensors, does not change\nthe output sequence length and, hence, the space-time resolu-\ntion. However, they play a key role in overall computational\nrequirements of the pooling attention operator.\nWe decouple the usage of K,V and Q pooling, with\nQ pooling being used in the ﬁrst layer of each stage and\nK,V pooling being employed in all other layers. Since the\nsequence length of key and value tensors need to be identical\nto allow attention weight calculation, the pooling stride used\non K and value V tensors needs to be identical. In our\ndefault setting, we constrain all pooling parameters (k; p; s)\nto be identical i.e. ΘK ≡ΘV within a stage, but vary s\nadaptively w.r.t. to the scale across stages.\nSkip connections. Since the channel dimension and se-\nquence length change inside a residual block, we pool the\nskip connection to adapt to the dimension mismatch between\nits two ends. MHPA handles this mismatch by adding the\nquery pooling operator P(·; ΘQ) to the residual path. As\nshown in Fig. 3, instead of directly adding the input X of\nMHPA to the output, we add the pooled input X to the\noutput, thereby matching the resolution to attended query Q.\nFor handling the channel dimension mismatch between\nstage changes, we employ an extra linear layer that operates\non the layer-normalized output of our MHPA operation. Note\nthat this differs from the other (resolution-preserving) skip-\nconnections that operate on the un-normalized signal.\n3.3. Network instantiation details\nTable 3 shows concrete instantiations of the base mod-\nels for Vision Transformers [25] and our Multiscale Vision\nTransformers. ViT-Base [25] (Table 3b) initially projects\nthe input to patches of shape 1 ×16×16 with dimension\nD = 768 , followed by stacking N = 12 transformer\nblocks. With an 8×224×224 input the resolution is ﬁxed to\n768×8×14×14 throughout all layers. The sequence length\n(spacetime resolution + class token) is 8 ·14 ·14 + 1 = 1569.\nOur MViT-Base (Table 3b) is comprised of4 scale stages,\neach having several transformer blocks of consistent channel\ndimension. MViT-B initially projects the input to a channel\ndimension of D= 96 with overlapping space-time cubes of\nshape 3×7×7. The resulting sequence of length 8∗56∗56 +\n1 = 25089 is reduced by a factor of 4 for each additional\nstage, to a ﬁnal sequence length of 8 ∗7 ∗7 + 1 = 393 at\nscale4. In tandem, the channel dimension is up-sampled by\na factor of 2 at each stage, increasing to 768 at scale4. Note\nthat all pooling operations, and hence the resolution down-\nsampling, is performed only on the data sequence without\ninvolving the processed class token embedding.\nWe set the number of MHPA heads toh= 1 in the scale1\nstage and increase the number of heads with the channel\ndimension (channels per-head D/hremain consistent at 96).\nAt each stage transition, the previous stage output MLP\ndimension is increased by 2×and MHPA pools onQtensors\nwith sQ = (1,2,2) at the input of the next stage.\n5\nmodel pre-train top-1 top-5 FLOPs×views Param\nTwo-Stream I3D [11] - 71.6 90.0 216 ×NA 25.0\nip-CSN-152 [96] - 77.8 92.8 109×3×10 32.8\nSlowFast8×8 +NL [30] - 78.7 93.5 116×3×10 59.9\nSlowFast16×8 +NL [30] - 79.8 93.9 234×3×10 59.9\nX3D-M [29] - 76.0 92.3 6.2×3×10 3.8\nX3D-XL [29] - 79.1 93.9 48.4×3×10 11.0\nViT-B-VTN [78] ImageNet-1K 75.6 92.4 4218×1×1 114.0\nViT-B-VTN [78] ImageNet-21K 78.6 93.7 4218×1×1 114.0\nViT-B-TimeSformer [6] ImageNet-21K 80.7 94.7 2380×3×1 121.4\nViT-L-ViViT [1] ImageNet-21K 81.3 94.7 3992×3×4 310.8\nViT-B (our baseline) ImageNet-21K 79.3 93.9 180×1×5 87.2\nViT-B (our baseline) - 68.5 86.9 180×1×5 87.2\nMViT-S - 76.0 92.1 32.9×1×5 26.1\nMViT-B, 16×4 - 78.4 93.5 70.5×1×5 36.6\nMViT-B, 32×3 - 80.2 94.4 170×1×5 36.6\nMViT-B, 64×3 - 81.2 95.1 455×3×3 36.6\nTable 4. Comparison with previous work on Kinetics-400. We\nreport the inference cost with a single “view\" (temporal clip with\nspatial crop) ×the number of views (FLOPs×viewspace×viewtime).\nMagnitudes are Giga (109) for FLOPs and Mega (106) for Param.\nAccuracy of models trained with external data is de-emphasized.\nWe employ K,V pooling in all MHPA blocks, with\nΘK ≡ ΘV and sQ = (1 ,8,8) in scale 1 and adaptively\ndecay this stride w.r.t. to the scale across stages such that the\nK,V tensors have consistent scale across all blocks.\n4. Experiments: Video Recognition\nDatasets. We use Kinetics-400 [59] (K400) (∼240k train-\ning videos in 400 classes) and Kinetics-600 [11]. We fur-\nther assess transfer learning performance for on Something-\nSomething-v2 [38], Charades [86], and A V A [39].\nWe report top-1 and top-5 classiﬁcation accuracy (%) on\nthe validation set, computational cost (in FLOPs) of a single,\nspatially center-cropped clip and the number of clips used.\nTraining. By default, all models are trained from random\ninitialization (“from scratch”) on Kinetics, without using\nImageNet [22] or other pre-training. Our training recipe and\naugmentations follow [30,95]. For Kinetics, we train for 200\nepochs with 2 repeated augmentation [50] repetitions.\nWe report ViT baselines that are ﬁne-tuned from Ima-\ngeNet, using a 30-epoch version of the training recipe in [30].\nFor the temporal domain, we sample a clip from the full-\nlength video, and the input to the network are T frames with\na temporal stride of τ; denoted as T ×τ [30].\nInference. We apply two testing strategies following [30,\n29]: (i) Temporally, uniformly samplesKclips (e.g. K=5)\nfrom a video, scales the shorter spatial side to 256 pixels and\ntakes a 224×224 center crop, and (ii), the same as (i) tempo-\nrally, but take 3 crops of 224×224 to cover the longer spatial\naxis. We average the scores for all individual predictions.\nAll implementation speciﬁcs are in §D.\n4.1. Main Results\nKinetics-400. Table 4 compares to prior work. From top-\nto-bottom, it has four sections and we discuss them in turn.\nmodel pretrain top-1 top-5 GFLOPs×views Param\nSlowFast 16×8 +NL [30] - 81.8 95.1 234×3×10 59.9\nX3D-M - 78.8 94.5 6.2×3×10 3.8\nX3D-XL - 81.9 95.5 48.4×3×10 11.0\nViT-B-TimeSformer [6] IN-21K 82.4 96.0 1703×3×1 121.4\nViT-L-ViViT [1] IN-21K 83.0 95.7 3992×3×4 310.8\nMViT-B, 16×4 - 82.1 95.7 70.5×1×5 36.8\nMViT-B, 32×3 - 83.4 96.3 170×1×5 36.8\nMViT-B-24, 32×3 - 83.8 96.3 236×1×5 52.9\nTable 5. Comparison with previous work on Kinetics-600.\nThe ﬁrst Table 4 section shows prior art using ConvNets.\nThe second section shows concurrent work using Vision\nTransformers [25] for video classiﬁcation [78, 6]. Both ap-\nproaches rely on ImageNet pre-trained base models. ViT-B-\nVTN [78] achieves 75.6% top-1 accuracy, which is boosted\nby 3% to 78.6% by merely changing the pre-training from\nImageNet-1K to ImageNet-21K. ViT-B-TimeSformer [6]\nshows another 2.1% gain on top of VTN, at higher cost of\n7140G FLOPs and 121.4M parameters. ViViT improves\naccuracy further with an even larger ViT-L model.\nThe third section in Table 4 shows our ViT baselines. We\nﬁrst list our ViT-B, also pre-trained on the ImageNet-21K,\nwhich achieves 79.3%, thereby being 1.4% lower than ViT-B-\nTimeSformer, but is with 4.4×fewer FLOPs and 1.4×fewer\nparameters. This result shows that simply ﬁne-tuning an\noff-the-shelf ViT-B model from ImageNet-21K [25] provides\na strong baseline on Kinetics. However, training this model\nfrom-scratch with the same ﬁne-tuning recipe will result\nin 34.3%. Using our “training-from-scratch” recipe will\nproduce 68.5% for this ViT-B model, using the same 1×5,\nspatial ×temporal, views for video-level inference.\nThe ﬁnal section of Table 4 lists ourMViT results. All our\nmodels are trained-from-scratch using this recipe, without\nany external pre-training. Our small model, MViT-S pro-\nduces 76.0% while being relatively lightweight with 26.1M\nparam and 32.9×5=164.5G FLOPs, outperforming ViT-B\nby +7.5% at 5.5×less compute in identical train/val setting.\nOur base model, MViT-B provides 78.4%, a +9.9% accu-\nracy boost over ViT-B underidentical settings, while having\n2.6×/2.4×fewer FLOPs/parameters. When changing the\nframe sampling from 16×4 to 32×3 performance increases\nto 80.2%. Finally, we take this model and ﬁne-tune it for 5\nepochs with longer 64 frame input, after interpolating the\ntemporal positional embedding, to reach 81.2% top-1 using\n3 spatial and 3 temporal views for inference (it is sufﬁcient\ntest with fewer temporal views if a clip has more frames).\nFurther quantitative and qualitative results are in §A.\nKinetics-600 [11] is a larger version of Kinetics. Results\nare in Table 5. We train MViT from-scratch, without any\npre-training. MViT-B, 16×4 achieves 82.1% top-1 accu-\nracy. We further train a deeper 24-layer model with longer\nsampling, MViT-B-24, 32×3, to investigate model scale on\nthis larger dataset. MViT achieves state-of-the-art of 83.4%\nwith 5-clip center crop testing while having 56.0 ×fewer\nFLOPs and 8.4 ×fewer parameters than ViT-L-ViViT [1]\nwhich relies on large-scale ImageNet-21K pre-training.\n6\nmodel pretrain top-1 top-5 FLOPs×views Param\nTSM-RGB [71] IN-1K+K400 63.3 88.2 62.4×3×2 42.9\nMSNet [63] IN-1K 64.7 89.4 67×1×1 24.6\nTEA [68] IN-1K 65.1 89.9 70×3×10 -\nViT-B-TimeSformer [6] IN-21K 62.5 - 1703×3×1 121.4\nViT-B (our baseline) IN-21K 63.5 88.3 180×3×1 87.2\nSlowFast R50, 8×8 [30]\nK400\n61.9 87.0 65.7×3×1 34.1\nSlowFast R101, 8×8 [30] 63.1 87.6 106×3×1 53.3\nMViT-B, 16×4 64.7 89.2 70.5×3×1 36.6\nMViT-B, 32×3 67.1 90.8 170×3×1 36.6\nMViT-B, 64×3 67.7 90.9 455×3×1 36.6\nMViT-B, 16×4\nK600\n66.2 90.2 70.5×3×1 36.6\nMViT-B, 32×3 67.8 91.3 170×3×1 36.6\nMViT-B-24, 32×3 68.7 91.5 236×3×1 53.2\nTable 6. Comparison with previous work on SSv2.\nSomething-Something-v2 (SSv2) [38] is a dataset with\nvideos containing object interactions, which is known as\na ‘temporal modeling‘ task. Table 6 compares our method\nwith the state-of-the-art. We ﬁrst report a simple ViT-B\n(our baseline) that uses ImageNet-21K pre-training. Our\nMViT-B with 16 frames has 64.7% top-1 accuracy, which is\nbetter than the SlowFast R101 [30] which shares the same\nsetting (K400 pre-training and 3×1 view testing). With more\ninput frames, our MViT-B achieves 67.7% and the deeper\nMViT-B-24 achieves 68.7% using our K600 pre-trained\nmodel of above. In general, Table 6 veriﬁes the capability of\ntemporal modeling for MViT.\nmodel pretrain mAP FLOPs×views Param\nNonlocal [101] IN-1K+K400 37.5 544×3×10 54.3\nSTRG +NL [102] 39.7 630×3×10 58.3\nTimeception [56]\nK400\n41.1 N/A×N/A N/A\nLFB +NL [105] 42.5 529×3×10 122\nSlowFast 50, 8×8 [30] 38.0 65.7×3×10 34.0\nSlowFast 101+NL, 16×8 [30] 42.5 234×3×10 59.9\nX3D-XL [29] 43.4 48.4×3×10 11.0\nMViT-B, 16×4 40.0 70.5×3×10 36.4\nMViT-B, 32×3 44.3 170×3×10 36.4\nMViT-B, 64×3 46.3 455×3×10 36.4\nSlowFast R101+NL, 16×8 [30]\nK600\n45.2 234×3×10 59.9\nX3D-XL [29] 47.1 48.4×3×10 11.0\nMViT-B, 16×4 43.9 70.5×3×10 36.4\nMViT-B, 32×3 47.1 170×3×10 36.4\nMViT-B-24, 32×3 47.7 236×3×10 53.0\nTable 7.Comparison with previous work on Charades.\nCharades [86] is a dataset with longer range activities. We\nvalidate our model in Table 7. With similar FLOPs and\nparameters, our MViT-B 16×4 achieves better results (+2.0\nmAP) than SlowFast R50 [30]. As shown in the Table, the\nperformance of MViT-B is further improved by increasing\nthe number of input frames and MViT-B layers and using\nK600 pre-trained models.\nA V A[39] is a dataset with for spatiotemporal-localization\nof human actions. We validate our MViT on this detection\ntask. Details about the detection architecture of MViT can\nbe found in §D.2. Table 8 shows the results of our MViT\nmodels compared with SlowFast [30] and X3D [29]. We\nobserve that MViT-B can be competitive to SlowFast and\nX3D using the same pre-training and testing strategy.\nmodel pretrain val mAP FLOPs Param\nSlowFast, 4×16, R50 [30]\nK400\n21.9 52.6 33.7\nSlowFast, 8×8, R101 [30] 23.8 138 53.0\nMViT-B, 16×4 24.5 70.5 36.4\nMViT-B, 32×3 26.8 170 36.4\nMViT-B, 64×3 27.3 455 36.4\nSlowFast, 8×8 R101+NL [30]\nK600\n27.1 147 59.2\nSlowFast, 16×8 R101+NL [30] 27.5 296 59.2\nX3D-XL [29] 27.4 48.4 11.0\nMViT-B, 16×4 26.1 70.5 36.3\nMViT-B, 32×3 27.5 170 36.4\nMViT-B-24, 32×3 28.7 236 52.9\nTable 8. Comparison with previvous work on A V A v2.2. All\nmethods use single center crop inference following [29].\n4.2. Ablations on Kinetics\nWe carry out ablations on Kinetics-400 (K400) using 5-\nclip center 224×224 crop testing. We show top-1 accuracy\n(Acc), as well as computational complexity measured in\nGFLOPs for a single clip input of spatial size 224 2. Infer-\nence computational cost is proportional as a ﬁxed number\nof 5 clips is used (to roughly cover the inferred videos with\nT×τ=16×4 sampling.) We also report Parameters in M(106)\nand training GPU memory in G(109) for a batch size of 4. By\ndefault all MViT ablations are with MViT-B, T×τ=16×4\nand max-pooling in MHSA.\nmodel shufﬂing FLOPs (G) Param (M) Acc\nMViT-B 70.5 36.5 77.2\nMViT-B ✓ 70.1 (−7.1)\nViT-B 179.6 87.2 68.5\nViT-B ✓ 68.4 (−0.1)\nTable 9. Shufﬂing frames in inference. MViT -B severely drops\n(−7.1%) for shufﬂed temporal input, but ViT-B models appear to\nignore temporal information as accuracy remains similar (−0.1%).\nFrame shufﬂing. Table 9 shows results for randomly shuf-\nﬂing the input frames in time during testing. All models are\ntrained without any shufﬂing and have temporal embeddings.\nWe notice that our MViT-B architecture suffers a signiﬁcant\naccuracy drop of -7.1% (77.2 →70.1) for shufﬂing infer-\nence frames. By contrast, ViT-B is surprisingly robust for\nshufﬂing the temporal order of the input.\nThis indicates that a naïve application of ViT to video\ndoes not model temporal information, and the temporal po-\nsitional embedding in ViT-B seems to be fully ignored. We\nalso veriﬁed this with the 79.3% ImageNet-21K pre-trained\nViT-B of Table 4, which hasthe same accuracyof 79.3% for\nshufﬂing test frames, suggesting that it implicitly performs\nbag-of-frames video classiﬁcation in Kinetics.\nvariant [N1, N2] FLOPs (G) Mem (G) Acc\nViT-B [12, 0] 179.6 16.8 68.5\n2-scale ViT-B,Qpool [6, 6] 111.1 (−68.5) 9.8 (−7.0) 71.0 (+1.5)\nViT-B,K,V pool [12, 0] 148.4 (−31.2) 8.9 (−7.9) 69.1 (+0.6)\nTable 10. Query (scale stage) and Key-Value pooling on ViT-\nB. Introducing a single extra resolution stage into ViT-B boosts\naccuracy by +1.5%. Pooling K,V provides +0.6% accuracy. Both\ntechniques allow dramatic FLOPs/memory savings.\n7\nTwo scales in ViT. We provide a simple experiment that\nablates the effectiveness of scale-stage design on ViT-B. For\nthis we add a single scale stage to the ViT-B model. To\nisolate the effect of having different scales in ViT, we do\nnot alter the channel dimensionality for this experiment. We\ndo so by performing Q-Pooling with sQ ≡(1,2,2) after 6\nTransformer blocks (cf. Table 3). Table 10 shows the results.\nAdding a single scale stage to the ViT-B baseline boosts accu-\nracy by +1.5% while deceasing FLOPs and memory cost by\n38% and 41%. Pooling Key-Value tensors reduces compute\nand memory cost while slightly increasing accuracy.\npositional embedding Param (M) Acc\n(i) none 36.2 75.8\n(ii) space-only 36.5 76.7\n(iii) joint space-time 38.6 76.5\n(iv) separate in space & time 36.5 77.2\nTable 11. Effect of separate space-time positional embedding.\nBackbone: MViT-B, 16×4. FLOPs are 70.5G for all variants.\nSeparate space & time embeddings in MViT.In Table 11,\nwe ablate using (i) none, (ii) space-only, (iii) joint space-time,\nand (iv) a separate space and time (our default), positional\nembeddings. We observe that no embedding (i) decays ac-\ncuracy by -0.9% over using just a spatial one (ii) which is\nroughly equivalent to a joint spatiotemporal one (iii). Our\nseparate space-time embedding (iv) is best, and also has\n2.1M fewer parameters than a joint spacetime embedding.\nT ×τ cT ×cH×cW sT ×sH×sW FLOPs Param Acc\n8×8 1×4×4 1×4×4 69.4 36.5 74.5\n8×8 1×7×7 1×4×4 69.6 36.5 75.6\n8×8 3×7×7 1×4×4 70.5 36.5 75.9\n16×4 3×7×7 2×4×4 70.5 36.5 77.2\n32×2 3×7×7 4×4×4 70.5 36.5 77.2\n32×2 7×7×7 4×4×4 70.5 36.5 77.3\nTable 12. Input sampling: We vary sampling rate T ×τ, the size\nc=cT ×cH×cW and stride of s=sT ×sH×sW the cube1 layer that\nprojects space-time cubes. Cubes with temporal extent cT >1 are\nbeneﬁcial. Our default setting is underlined.\nInput Sampling Rate. Table 12 shows results for different\ncubiﬁcation kernel size c and sampling stride s (cf. Table 2).\nWe observe that sampling patches, cT = 1, performs worse\nthan sampling cubes with cT >1. Further, sampling twice\nas many frames, T= 16, with twice the cube stride, sT = 2,\nkeeps the cost constant but boosts performance by +1.3%\n(75.9% →77.2%). Also, sampling overlapping input cubes\ns < c allows better information ﬂow and beneﬁts perfor-\nmance. While cT >1 helps, very large temporal kernel size\n(cT = 7) doesn’t futher improve performance.\nStage distribution. The ablation in Table 13 shows the re-\nsults for distributing the number of transformer blocks in\neach individual scale stage. The overall number of trans-\nformer blocks, N=16 is consistent. We observe that having\nmore blocks in early stages increases memory and having\nvariant [N2, N3, N4, N5] FLOPs Param Mem Acc\nV1 [2, 6, 6, 2] 90.2 29.5 11.0 76.3\nV2 [2, 4, 6, 4] 86.9 42.8 10.3 75.9\nV3 [2, 4, 8, 2] 88.3 32.2 10.5 76.6\nV4 [2, 2, 8, 4] 85.0 45.5 9.7 76.7\nV5 [1, 2, 11, 2] 83.6 36.5 9.1 77.1\nV6 [2, 2, 10, 2] 86.4 34.9 11.3 76.9\nTable 13. Scale blocks: We ablate the stage conﬁguration as the\nnumber of blocks N in stages of MViT-B (i.e. where to pool Q).\nThe overall number of transformer blocks is constant with N=16.\nmore blocks later stages the parameters of the architecture.\nShifting the majority of blocks to the scale4 stage (Variant\nV5 and V6 in Table 13) achieves the best trade-off.\nstride s adaptive FLOPs Mem Acc\nnone n/a 130.8 16.3 77.6\n1×4×4 71.4 8.2 75.9\n2×4×4 64.3 6.6 74.8\n2×4×4 ✓ 83.6 9.1 77.1\n1×8×8 ✓ 70.5 6.8 77.2\n2×8×8 ✓ 63.7 6.3 75.8\nTable 14. Key-Value pooling: Vary stride s = sT ×sH ×sW , for\npooling Kand V. “adaptive” reduces stride w.r.t. stage resolution.\nKey-Value pooling. The ablation in Table 14 analyzes the\npooling stride s = sT ×sH ×sW , for pooling K and V\ntensors. Here, we compare an “adaptive” pooling that uses a\nstride w.r.t. stage resolution, and keeps theK,V resolution\nﬁxed across all stages, against a non-adaptive version that\nuses the same stride at every block. First, we compare the\nbaseline which uses no K,V pooling with non-adaptive\npooling with a ﬁxed stride of 2×4×4 across all stages: this\ndrops accuracy from 77.6% to 74.8 (and reduces FLOPs\nand memory by over 50%). Using an adaptive stride that is\n1×8×8 in the scale1 stage, 1×4×4 in scale2, and 1×2×2 in\nscale3 gives the best accuracy of 77.2% while still preserving\nmost of the efﬁciency gains in FLOPs and memory.\nkernel k pooling func Param Acc\ns max 36.5 76.1\n2s + 1 max 36.5 75.5\ns + 1 max 36.5 77.2\ns + 1 average 36.5 75.4\ns + 1 conv 36.6 78.3\n3×3×3 conv 36.6 78.4\nTable 15. Pooling function: Varying the kernel k as a function of\nstride s. Functions are average or max pooling and conv which is a\nlearnable, channel-wise convolution.\nPooling function. The ablation in Table 15 looks at the\nkernel size k w.r.t. the stride s, and the pooling function\n(max/average/conv). First, we see that having equivalent\nkernel and stride k=s provides 76.1%, increasing the kernel\nsize to k=2s+1 decays to 75.5%, but using a kernelk=s+1\ngives a clear beneﬁt of 77.2%. This indicates that overlap-\nping pooling is effective, but a too large overlap (2s+1) hurts.\n8\nSecond, we investigate average instead of max-pooling and\nobserve that accuracy decays by from 77.2% to 75.4%.\nThird, we use conv-pooling by a learnable, channelwise\nconvolution followed by LN. This variant has +1.2% over\nmax pooling and is used for all experiments in §4.1 and §5.\nmodel clips/sec Acc FLOPs×views Param\nX3D-M [29] 7.9 74.1 4.7×1×5 3.8\nSlowFast R50 [30] 5.2 75.7 65.7×1×5 34.6\nSlowFast R101 [30] 3.2 77.6 125.9×1×5 62.8\nViT-B [25] 3.6 68.5 179.6×1×5 87.2\nMViT-S, max-pool 12.3 74.3 32.9×1×5 26.1\nMViT-B, max-pool 6.3 77.2 70.5×1×5 36.5\nMViT-S, conv-pool 9.4 76.0 32.9×1×5 26.1\nMViT-B, conv-pool 4.8 78.4 70.5×1×5 36.6\nTable 16. Speed-Accuracy tradeoff on Kinetics-400. Training\nthroughput is measured in clips/s. MViT is fast and accurate.\nSpeed-Accuracy tradeoff. In Table 16, we analyze the\nspeed/accuracy trade-off of our MViT models, along with\ntheir counterparts vision transformer (ViT [25]) and Con-\nvNets (SlowFast 8 ×8 R50, SlowFast 8 ×8 R101 [30], &\nX3D-L [29]). We measure training throughput as the num-\nber of video clips per second on a single M40 GPU.\nWe observe that both MViT-S and MViT-B models are\nnot only signiﬁcantly more accurate but also much faster\nthan both the ViT-B baseline and convolutional models. Con-\ncretely, MViT-S has3.4×higher throughput speed (clips/s),\nis +5.8% more accurate (Acc), and has 3.3×fewer param-\neters (Param) than ViT-B. Using a conv instead of max-\npooling in MHSA, we observe a training speed reduction of\n∼20% for convolution and additional parameter updates.\n5. Experiments: Image Recognition\nWe apply our video models on static image recognition by\nusing them with single frame, T = 1, on ImageNet-1K [22].\nTraining. Our recipe is identical to DeiT [95] and summa-\nrized in the supplementary material. Training is for 300\nepochs and results improve for training longer [95].\n5.1. Main Results\nFor this experiment, we take our models which were de-\nsigned by ablation studies for video classiﬁcation on Kinetics\nand simply remove the temporal dimension. Then we train\nand validate them (“from scratch”) on ImageNet.\nTable 17 shows the comparison with previous work. From\ntop to bottom, the table contains RegNet [81] and Efﬁcient-\nNet [93] as ConvNet examples, and DeiT [95], with DeiT-B\nbeing identical to ViT-B [25] but trained with the improved\nrecipe in [95]. Therefore, this is the vision transformer coun-\nterpart we are interested in comparing to.\nThe bottom section in Table 17 shows results for our\nMultiscale Vision Transformer (MViT) models.\nmodel Acc FLOPs (G) Param (M)\nRegNetZ-4GF [24] 83.1 4.0 28.1\nRegNetZ-16GF [24] 84.1 15.9 95.3\nEfﬁcientNet-B7 [93] 84.3 37.0 66.0\nDeiT-S [95] 79.8 4.6 22.1\nDeiT-B [95] 81.8 17.6 86.6\nDeiT-B ↑3842 [95] 83.1 55.5 87.0\nMViT-B-16, max-pool 82.5 7.8 37.0\nMViT-B-24, max-pool 83.1 10.9 53.5\nMViT-B-24-wide-3202, max-pool 84.3 32.7 72.9\nMViT-B-16 83.0 7.8 37.0\nMViT-B-24-wide-3202 84.8 32.7 72.9\nTable 17. Comparison to prior work on ImageNet . RegNet\nand EfﬁcientNet are ConvNet examples that use different training\nrecipes. DeiT/MViT are ViT-based and use identical recipes [95].\nWe show models of different depth, MViT-B-Depth, (16,\n24, and 32), where MViT-B-16 is our base model and the\ndeeper variants are simply created by repeating the num-\nber of blocks N∗ in each scale stage (cf. Table 3b). “wide”\ndenotes a larger channel dimension of D = 112. All our\nmodels are trained using the identical recipe as DeiT [95].\nWe make the following observations:\n(i) Our lightweight MViT-B-16 achieves 82.5% top-1\naccuracy, with only 7.8 GFLOPs, which outperforms the\nDeiT-B counterpart by +0.7% with lower computation cost\n(2.3×fewer FLOPs and Parameters). If we use conv instead\nof max-pooling, this number is increased by +0.5% to 83.0%.\n(ii) Our deeper model MViT-B-24, provides a gain of\n+0.6% accuracy at slight increase in computation.\n(iii) A larger model, MViT-B-24-wide with input resolu-\ntion 3202 reaches 84.3%, corresponding to a +1.2% gain, at\n1.7×fewer FLOPs, over DeiT-B↑3842. Using convolutional,\ninstead of max-pooling elevates this to 84.8%.\nThese results suggest that Multiscale Vision Transformers\nhave an architectural advantage over Vision Transformers.\n6. Conclusion\nWe have presented Multiscale Vision Transformers that\naim to connect the fundamental concept of multiscale feature\nhierarchies with the transformer model. MViT hierarchically\nexpands the feature complexity while reducing visual reso-\nlution. In empirical evaluation, MViT shows a fundamental\nadvantage over single-scale vision transformers for video\nand image recognition. We hope that our approach will foster\nfurther research in visual recognition.\nAppendix\nIn this appendix, §A contains further ablations for Kinet-\nics (§A.1) & ImageNet (§A.2), §C contains an analysis on\ncomputational complexity of MHPA, and §B qualitativeob-\nservations in MViT and ViT models. §D contains additional\nimplementation details for: Kinetics (§D.1), A V A (§D.2),\nCharades (§D.3), SSv2 (§D.4), and ImageNet (§D.5).\n9\nIN-1K\nIN-21K\nIN-21KIN-21K\nSlowFast 16x8, R101+NL\nSlowFast 8x8, R101+NL\nX3D-XL\nX3D-M\nMViT-B 16x4\nMViT-B 32x2\nX3D-S\nViViT-L ImageNet-21K\nTimeSformer ImageNet-21K\nVTN ImageNet-1K / 21K\nInference cost per video in TFLOPs (# of multiply-adds x 1012)\nKinetics top-1 val accuracy (%) \nIN-1K\nIN-21K\nInference cost per video in FLOPs (# of multiply-adds), log-scale\nKinetics top-1 val accuracy (%) \nSlowFast 16x8, R101+NL\nSlowFast 8x8, R101+NL\nX3D-XL\nX3D-M\nMViT-B 16x4\nMViT-B 32x2\nX3D-S\nViViT-L ImageNet-21K\nTimeSformer ImageNet-21K\nVTN ImageNet-1K / 21K\nFigure A.4. Accuracy/complexity trade-off on K400-val for varying # of inference clips per video. The top-1 accuracy (vertical axis) is\nobtained by K-Center clip testing where the number of temporal clips K ∈{1,3,5,7,10}is shown in each curve. The horizontal axis\nmeasures the full inference cost per video. The left-sided plots show a linear and the right plots a logarithmic (log) scale.\nA. Additional Results\nA.1. Ablations: Kinetics Action Classiﬁcation\nInference cost. In the spirit of [29] we aim to provide fur-\nther ablations for the effect of using fewer testing clips for\nefﬁcient video-level inference. In Fig. A.4 we analyze the\ntrade-off for the full inference of a video, when varying the\nnumber of temporal clips used. The vertical axis shows\nthe top-1 accuracy on K400-val and the horizontal axis the\noverall inference cost in FLOPs for different model fami-\nlies: MViT, X3D [29], SlowFast [30], and concurrent ViT\nmodels, VTN [78] ViT-B-TimeSformer [6] ViT-L-ViViT [1],\npre-trained on ImageNet-21K.\nWe ﬁrst compare MViT with concurrent Transformer-\nbased methods in the left plot in Fig. A.4. All these methods,\nVTN [78], TimeSformer [6] and ViViT [1], pre-train on\nImageNet-21K and use the ViT [25] model with modiﬁca-\ntions on top of it. The inference FLOPs of these methods\nare around 5-10×higher than MViT models with equivalent\nperformance; for example, ViT-L-ViViT [1] uses 4 clips of\n1446G FLOPs (i.e. 5.78 TFLOPs) each to produce 80.3%\naccuracy while MViT-B, 32×3 uses 5 clips of 170G FLOPs\n(i.e. 0.85 TFLOPs) to produce 80.2% accuracy. Therefore,\nMViT-L can provide similar accuracy at 6.8×lower FLOPs\n(and 8.5×lower parameters), than concurrent ViViT-L [1].\nMore importantly, the MViT result is achievedwithout exter-\nnal data. All concurrent Transformer based works [78, 6, 1]\nrequire the huge scale ImageNet-21K to be competitive, and\nthe performance degrades signiﬁcantly (-3% accuracy, see\nIN-1K in Fig. A.4 for VTN [78]). These works further report\nfailure of training without ImageNet initialization.\nThe plot in Fig. A.4 right shows this same plot with a\nlogarithmic scale applied to the FLOPs axis. Using this scal-\ning it is clearer to observe that smaller models convolutional\nmodels (X3D-S and X3D-M) can still provide more efﬁcient\ninference in terms of multiply-add operations and MViT-B\ncompute/accuracy trade-off is similar to X3D-XL.\nAblations on skip-connections. Recall that, at each scale-\nstage transition in MViT, we expand the channel dimension\nby increasing the output dimension of the previous stages’\nMLP layer; therefore, it is not possible to directly apply\nthe original skip-connection design [25], because the input\nchannel dimension ( Din) differs from the output channel\ndimension (Dout). We ablate three strategies for this:\n(a) First normalize the input with layer normalization\nand then expand its channel dimension to match the output\ndimension with a linear layer (Fig. A.5a); this is our default.\n(b) Directly expand the channel dimension of the input\nby using a linear layer to match the dimension (Fig. A.5b).\n(c) No skip-connection for stage-transitions (Fig. A.5c).\nMLP\n+\nNorm\nLinear (Din, Dout)\nX\nTHW × Din   \nTHW × Dout   \n(a) normalized skip-\nconnection\nX\nTHW × Din   \nTHW × Dout   \nMLP\n+\nNorm\nLinear (Din, Dout)\n(b) unnormalized skip-\nconnection\nX\nTHW × Din   \nTHW × Dout   \nMLP\nNorm\n(c) no skip-\nconnection\nFigure A.5. Skip-connections at stage-transitions. Three skip-\nconnection variants for expanding channel dimensions: (a) ﬁrst\nnormalize the input with layer normalization (Norm) and then\nexpand its channel dimension; (b) directly expand the channel\ndimension of the input; (c) no skip-connection at stage-transitions.\n10\nmethod top-1 top-5\n(a) normalized skip-connection 77.2 93.1\n(b) unnormalized skip-connection 74.6 91.3\n(c) no skip-connection 74.7 91.8\nTable A.1. Skip-connections at stage-transitions on K400. We\nuse our base model, MViT-B 16 ×4. Normalizing the skip-\nconnection at channel expansion is essential for good performance.\nTable A.1 shows the Kinetics-400 ablations for all 3 vari-\nants. Our default of using a normalized skip-connection (a)\nobtains the best results with 77.2% top-1 accuracy, while\nusing an un-normalized skip-connection after channel ex-\npansion (b) decays signiﬁcantly to 74.6% and using no skip-\nconnection for all stage-transitions (c) has a similar result.\nWe hypothesize that for expanding the channel dimension,\nnormalizing the signal is essential to foster optimization, and\nuse this design as our default in all other experiments.\nbackbone recipe Acc\nSlowFast R50, 8×8 [30] 77.0\nSlowFast R50, 8×8 MViT 67.4\nSlowFast R101, 8×8 [30] 78.0\nSlowFast R101, 8×8 MViT 61.6\nTable A.2. SlowFast models with MViT recipe on Kinetics-400.\nThe default recipe is using the recipe from the original paper. Ac-\ncuracy is evaluated on 10×3 views.\nSlowFast with MViT recipe. To investigate if our training\nrecipe can beneﬁt ConvNet models, we apply the same aug-\nmentations and training recipe as for MViT to SlowFast in\nTable A.2. The results suggest that SlowFast models do not\nbeneﬁt from the MViT recipe directly and more studies are\nrequired to understand the effect of applying our training-\nfrom-scratch recipe to ConvNets, as it seems higher capacity\nConvNets (R101) perform worse when using our recipe.\nA.2. Ablations: ImageNet Image Classiﬁcation\nWe carry out ablations on ImageNet with theMViT-B-16\nmodel with 16 layers, and show top-1 accuracy (Acc) as well\nas computational complexity measured in GFLOPs (ﬂoating-\npoint operations). We also report Parameters in M(106) and\ntraining GPU memory in G(109) for a batch size of 512.\nstride s FLOPs Mem Acc\n8×8 7.2 9.0 81.6\n4×4 7.8 11.9 82.5\n2×2 9.0 13.2 81.8\nnone 10.4 17.3 82.3\nTable A.3. ImageNet: Key-Value pooling: We vary stride sH ×\nsW , for pooling Kand V. We use “adaptive” pooling that reduces\nstride w.r.t. stage resolution.\nKey-Value pooling for image classiﬁcation. The ablation\nin Table A.3 analyzes the pooling stride s = sH ×sW , for\npooling Kand V tensors. Here, we use our default ‘adaptive’\npooling that uses a stride w.r.t. stage resolution, and keeps\nthe K,V resolution ﬁxed across all stages.\nFirst, we compare the baseline which uses pooling with\na ﬁxed stride of 4×4 with a model has a stride of 8×8: this\ndrops accuracy from 82.5% to 81.6%, and reduces FLOPs\nand memory by 0.6G and 2.9G.\nSecond, we reduce the stride to 2 ×2, which increases\nFLOPs and memory signiﬁcantly but performs 0.7% worse\nthan our default stride of 4×4.\nThird, we remove the K,V pooling completely which\nincreases FLOPs by 33% and memory consumption by 45%,\nwhile providing lower accuracy than our default.\nOverall, the results show that our K,V pooling is an\neffective technique to increase accuracy and decrease cost\n(FLOPs/memory) for image classiﬁcation.\nB. Qualitative Experiments: Kinetics\nIn Figure A.6, we plot the mean attention distance for all\nheads across all the layers of our Multiscale Transformer\nmodel and its Vision Transformer counterpart, at initializa-\ntion with random weights, and at convergence after training.\nEach head represents a point in the plots (ViT-B has more\nheads). Both the models use the exact same weight initial-\nization scheme and the difference in the attention signature\nstems purely from the multiscale skeleton in MViT. We ob-\nserve that the dynamic range of attention distance is about\n4×larger in the MViT model than ViT at initialization it-\nself (A.6a vs. A.6b). This signals the strong inductive bias\nstemming from the multiscale design of MViT. Also note\nthat while at initialization, every layer in ViT has roughly the\nsame mean attention distance, the MViT layers have strik-\ningly different mean attention signatures indicating distinct\npredilections towards global and local features.\nThe bottom row of Fig. A.6 shows the same plot for a\nconverged Vision Transformer (A.6c) and Multiscale Vision\nTransformer (A.6d) model.\nWe notice very different trends between the two models\nafter training. While the ViT model (A.6c) has a consistent\nincrease in attention distance across layers, the MViT model\n(A.6d) is not monotonic at all. Further, the intra-head varia-\ntion in the ViT model decreases as the depth saturates, while,\nfor MViT, different heads are still focusing on different fea-\ntures even in the higher layers. This suggests that some of\nthe capacity in the ViT model might indeed be wasted with\nredundant computation while the lean MViT heads are more\njudiciously utilizing their compute. Noticeable is further a\nlarger delta (between initialization in Fig. A.6a and conver-\ngence in A.6c) in the overall attention distance signature in\nthe ViT model, compared to MViT’s location distribution.\n11\n(a) ViT-B at initialization\n (b) MViT-B at initialization\n(c) ViT-B at convergence\n (d) MViT-B at convergence\nFigure A.6. Mean attention distance across layers at initialization/convergencefor Vision Transformer (a)/(c) & Multiscale Vision\nTransformers (b)/(d). Each point shows the normalized average attention distance (weighted by the attention scores, with 1.0 being maximum\npossible distance) for each head in a layer. MViT attends close and distant features throughout the network hierarchy.\nC. Computational Analysis\nSince attention is quadratic in compute and memory com-\nplexity, pooling the key, query and value vectors have di-\nrect beneﬁts on the fundamental compute and memory re-\nquirements of the pooling operator and by extension, on\nthe complete Multiscale Transformer model. Consider an\ninput tensor of dimensions T ×H×W and corresponding\nsequence length L = T ·H ·W. Further, assume the key,\nquery and value strides to be sK, sQ and sV . As described\nin Sec. 3.1 in main paper, each of the vectors would experi-\nence a sptio-temporal resolution downsampling by a factor\nof their corresponding strides. Equivalently, the sequence\nlength of query, key and value vectors would be reduced by\na factor of fQ, fK and fV respectively, where,\nfj = sj\nT ·sj\nH ·sj\nW , ∀j ∈{Q,K,V }.\nComputational complexity. Using these shorter sequences\nyields a corresponding reduction in space and runtime com-\nplexities for the pooling attention operator. Considering\nkey, query and value vectors to have sequence lengths L/fk,\nL/fq and L/fv after pooling, the overall runtime complex-\nity of computing the key, query and value embeddings is\nO(THWD 2/h) per head, where his the number of heads\nin MHPA. Further, the runtime complexity for calculating the\nfull attention matrix and the weighed sum of value vectors\nwith reduced sequence lengths is O(T2H2W2D/fqfhh)\nper head. Computational complexity for pooling is\n12\nT(P(·; Θ)) = O\n(\nTHW ·D·kT kW kH\nsT sW sH\n)\n,\nwhich is negligible compared to the quadratic complexity\nof the attention computation and hence can be ignored in\nasymptotic notation. Thus, the ﬁnal runtime complexity of\nMHPA is O(THWD (D+ THW/fqfk)).\nMemory complexity. The space complexity for storing\nthe sequence itself and other tensors of similar sizes is\nO(THWD ). Complexity for storing the full attention ma-\ntrix is O(T2H2W2h/fqfk). Thus the total space complex-\nity of MHPA is O(THWh (D/h+ THW/fqfk)).\nDesign choice. Note the trade-off between the number of\nchannels D and the sequence length term THW/fqfk in\nboth space and runtime complexity. This tradeoff in multi\nhead pooling attention informs two critical design choices\nof Multiscale Transformer architecture.\nFirst, as the effective spatio-temporal resolution decreases\nwith layers because of diminishing THW/fqfk, the channel\ncapacity is increased to keep the computational time spent\n(FLOPs) roughly the same for each stage.\nSecond, for a ﬁxed channel dimension, D, higher number\nof heads hcause a prohibitively larger memory requirement\nbecause of the (D+ h∗THW/fqfk) term. Hence, Multi-\nscale Transformer starts with a small number of heads which\nis increased as the resolution factor THW/fqfk decreases,\nto hold the effect of(D+h∗THW/fqfk) roughly constant.\nD. Additional Implementation Details\nWe implement our model with PySlowFast [28]. Code\nand models are available at: https://github.com/\nfacebookresearch/SlowFast.\nD.1. Details: Kinetics Action Classiﬁcation\nArchitecture details. As in original ViT [25], we use resid-\nual connections [45] and Layer Normalization (LN) [2] in\nthe pre-normalization conﬁguration that applies LN at the\nbeginning of the residual function, and our MLPs consist of\ntwo linear layers with GELU activation [48], where the ﬁrst\nlayer expands the dimension from Dto 4D, and the second\nrestores the input dimension D, except at the end of a scale-\nstage, where we increase this channel dimensions to match\nthe input of the next scale-stage. At such stage-transitions,\nour skip connections receive an extra linear layer that takes\nas input the layer-normalized signal which is also fed into\nthe MLP. In case of Q-pooling at scale-stage transitions, we\ncorrespondingly pool the skip-connection signal.\nOptimization details. We use the truncated normal dis-\ntribution initialization in [42] and adopt synchronized\nAdamW [76] training on 128 GPUs following the recipe\nin [95, 30]. For Kinetics, we train for 200 epochs with 2\nrepeated augmentation [50] repetitions. The mini-batch size\nis 4 clips per GPU (so the overall batchsize is 512).\nWe adopt a half-period cosine schedule [75] of learning\nrate decaying: the learning rate at the n-th iteration is η·\n0.5[cos( n\nnmax\nπ) + 1], where nmax is the maximum training\niterations and the base learning rate η is set as 1.6 ·10−3.\nWe linearly scale the base learning rate w.r.t. the overall\nbatch-size, η= 1.6·10−3 batchsize\n512 , and use a linear warm-up\nstrategy in the ﬁrst 30 epochs [37]. The cosine schedule is\ncompleted when reaching a ﬁnal learning rate of 1.6 ·10−5.\nWe extract the class token after the last stage and use it as\nthe input to the ﬁnal linear layer to predict the output classes.\nFor Kinetics-600 all hyper-parameters are identical to K400.\nRegularization details. We use weight decay of 5 ·10-2,\na dropout [49] of 0.5 before the ﬁnal classiﬁer, label-\nsmoothing [92] of 0.1 and use stochastic depth [54] ( i.e.\ndrop-connect) with rate 0.2.\nOur data augmentation is performed on input clips by\napplying the same transformation across all frames. To each\nclip, we apply a random horizontal ﬂip, Mixup [113] with\nα= 0.8 to half of the clips in a batch and CutMix [112] to\nthe other half, Random Erasing [116] with probability 0.25,\nand Rand Augment [19] with probability of 0.5 for 4 layers\nof maximum magnitude 7.\nFor the temporal domain, we randomly sample a clip\nfrom the full-length video, and the input to the network are\nT frames with a temporal stride of τ; denoted as T ×τ [30].\nFor the spatial domain, we use Inception-style [91] cropping\nthat randomly resizes the input area between a [min, max],\nscale of [0.08, 1.00], and jitters aspect ratio between 3/4 to\n4/3, before taking an H×W = 224×224 crop.\nFine-tuning from ImageNet. To ﬁne-tune our ViT-B base-\nline, we extend it to take a video clip of T = 8 frames\nas input and initialize the model weights from the ViT-B\nmodel [25] pre-trained on ImageNet-21K dataset. The posi-\ntional embedding is duplicated for each frame. We ﬁne-tune\nthe model for 30 epochs with SGD using the recipe in [30].\nThe mini-batch size is 2 clips per GPU and a half-period\ncosine learning rate decay is used. We linearly scale the base\nlearning rate w.r.t. the overall batch-size,η= 10−3 batchsize\n16 .\nWeight decay is set to 10−4.\nD.2. Details: A V A Action Detection\nDataset. The A V A dataset [39] has bounding box annota-\ntions for spatiotemporal localization of (possibly multiple)\nhuman actions. It has 211k training and 57k validation video\nsegments. We follow the standard protocol reporting mean\nAverage Precision (mAP) on 60 classes [39] on A V A v2.2.\nDetection architecture. We follow the detection architec-\nture in [30] to allow direct comparison of MViT against\nSlowFast networks as a backbone.\n13\nFirst, we reinterpret our transformer spacetime cube out-\nputs from MViT as a spatial-temporal feature map by con-\ncatenating them according to the corresponding temporal\nand spatial location.\nSecond, we employ a the detector similar to Faster R-\nCNN [84] with minimal modiﬁcations adapted for video.\nRegion-of-interest (RoI) features [36] are extracted at the\ngenerated feature map from MViT by extending a 2D pro-\nposal at a frame into a 3D RoI by replicating it along the\ntemporal axis, similar as done in previous work [39, 89, 58],\nfollowed by application of frame-wise RoIAlign [43] and\ntemporal global average pooling. The RoI features are then\nmax-pooled and fed to a per-class, sigmoid classiﬁer for\nprediction.\nTraining. We initialize the network weights from the Ki-\nnetics models and adopt synchronized SGD training on 64\nGPUs. We use 8 clips per GPU as the mini-batch size and a\nhalf-period cosine schedule of learning rate decaying. The\nbase learning rate is set as 0.6. We train for 30 epochs\nwith linear warm-up [37] for the ﬁrst 5 epochs and use a\nweight decay of 10 −8 and stochastic depth [54] with rate\n0.4. Ground-truth boxes, and proposals overlapping with\nground-truth boxes by IoU >0.9, are used as the samples\nfor training. The region proposals are identical to the ones\nused in [30].\nInference. We perform inference on a single clip with\nT frames sampled with stride τ centered at the frame that is\nto be evaluated.\nD.3. Details: Charades Action Classiﬁcation\nDataset. Charades [86] has ∼9.8k training videos and 1.8k\nvalidation videos in 157 classes in a multi-label classiﬁcation\nsetting of longer activities spanning ∼30 seconds on average.\nPerformance is measured in mean Average Precision (mAP).\nTraining. We ﬁne-tune our MViT models from the Kinetics\nmodels. A per-class sigmoid output is used to account for\nthe multi-class nature. We train with SGD on 32 GPUs for\n200 epochs using 8 clips per GPU. The base learning rate\nis set as 0.6 with half-period cosine decay. We use weight\ndecay of 10-7 and stochastic depth [54] with rate 0.45. We\nperform the same data augmentation schemes as for Kinetics\nin §D.1, except of using Mixup.\nInference. To infer the actions over a single video, we\nspatio-temporally max-pool prediction scores from multiple\nclips in testing [30].\nD.4. Details: Something-Something V2 (SSv2)\nDataset. The Something-Something V2 dataset [38] con-\ntains 169k training, and 25k validation videos. The videos\nshow human-object interactions to be classiﬁed into 174\nclasses. We report accuracy on the validation set.\nTraining. We ﬁne-tune the pre-trained Kinetics models. We\ntrain for 100 epochs using 64 GPUs with 8 clips per GPU\nand a base learning rate of 0.02 with half-period cosine\ndecay [75]. Weight decay is set to 10 −4 and stochastic\ndepth rate [54] is 0.4. Our training augmentation is the same\nas in §D.1, but as SSv2 requires distinguishing between\ndirections, we disable random ﬂipping in training. We use\nsegment-based input frame sampling [70] that splits each\nvideo into segments, and from each of them, we sample one\nframe to form a clip.\nInference. We take single clip with 3 spatial crops to form\npredictions over a single video in testing.\nD.5. Details: ImageNet\nDatasets. For image classiﬁcation experiments, we per-\nform our experiments on ImageNet-1K [22] dataset that\nhas ∼1.28M images in 1000 classes. We train models on the\ntrain set and report top-1 and top-5 classiﬁcation accuracy\n(%) on the val set. Inference cost (in FLOPs) is measured\nfrom a single center-crop with resolution of 2242 if the input\nresolution was not speciﬁcally mentioned.\nTraining. We use the training recipe of DeiT [95] and sum-\nmarize it here for completeness. We train for 100 epochs\nwith 3 repeated augmentation [50] repetitions (overall com-\nputation equals 300 epochs), using a batch size of 4096 in\n64 GPUs. We use truncated normal distribution initializa-\ntion [42] and adopt synchronized AdamW [76] optimization\nwith a base learning rate of 0.0005 per 512 batch-size that\nis warmed up and decayed as half-period cosine, as in [95].\nWe use a weight decay of 0.05, label-smoothing [92] of\n0.1. Stochastic depth [54] ( i.e. drop-connect) is also used\nwith rate 0.1 for model with depth of 16 (MViT-B-16), and\nrate 0.3 for deeper models (MViT-B-24). Mixup [113] with\nα= 0.8 to half of the clips in a batch and CutMix [112] to\nthe other half, Random Erasing [116] with probability 0.25,\nand Rand Augment [19] with maximum magnitude 9 and\nprobability of 0.5 for 4 layers (for max-pooling) or 6 layers\n(for conv-pooling).\nAcknowledgements\nWe are grateful for discussions with Chao-Yuan Wu,\nRoss Girshick, and Kaiming He.\nReferences\n[1] Anurag Arnab, Mostafa Dehghani, Georg Heigold, Chen\nSun, Mario Lu ˇci´c, and Cordelia Schmid. Vivit: A video\nvision transformer. arXiv preprint arXiv:2103.15691, 2021.\n2, 3, 6, 10\n[2] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hin-\nton. Layer normalization. arXiv preprint arXiv:1607.06450,\n2016. 1, 13\n14\n[3] Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E Hinton.\nLayer normalization. arXiv:1607.06450, 2016. 4\n[4] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio.\nNeural machine translation by jointly learning to align and\ntranslate. arXiv preprint arXiv:1409.0473, 2014. 1\n[5] Josh Beal, Eric Kim, Eric Tzeng, Dong Huk Park, Andrew\nZhai, and Dmitry Kislyuk. Toward transformer-based object\ndetection. arXiv preprint arXiv:2012.09958, 2020. 2\n[6] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is\nspace-time attention all you need for video understanding?\narXiv preprint arXiv:2102.05095, 2021. 2, 3, 6, 7, 10\n[7] Tom B Brown, Benjamin Mann, Nick Ryder, Melanie Sub-\nbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakan-\ntan, Pranav Shyam, Girish Sastry, Amanda Askell, et al.\nLanguage models are few-shot learners. arXiv preprint\narXiv:2005.14165, 2020. 1\n[8] Peter J Burt and Edward H Adelson. The laplacian pyramid\nas a compact image code. In Readings in computer vision,\npages 671–679. Elsevier, 1987. 1\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas\nUsunier, Alexander Kirillov, and Sergey Zagoruyko. End-\nto-end object detection with transformers. In Proc. ECCV,\npages 213–229. Springer, 2020. 2\n[10] Joao Carreira, Eric Noland, Andras Banki-Horvath, Chloe\nHillier, and Andrew Zisserman. A short note about Kinetics-\n600. arXiv:1808.01340, 2018. 2\n[11] Joao Carreira and Andrew Zisserman. Quo vadis, action\nrecognition? a new model and the kinetics dataset. In Proc.\nCVPR, 2017. 2, 6\n[12] Chun-Fu Chen, Quanfu Fan, Neil Mallinar, Tom Sercu, and\nRogerio Feris. Big-little net: An efﬁcient multi-scale feature\nrepresentation for visual and speech recognition. arXiv\npreprint arXiv:1807.03848, 2018. 2\n[13] Jieneng Chen, Yongyi Lu, Qihang Yu, Xiangde Luo, Ehsan\nAdeli, Yan Wang, Le Lu, Alan L Yuille, and Yuyin Zhou.\nTransunet: Transformers make strong encoders for medi-\ncal image segmentation. arXiv preprint arXiv:2102.04306,\n2021. 2\n[14] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Hee-\nwoo Jun, David Luan, and Ilya Sutskever. Generative pre-\ntraining from pixels. In Proc. ICML, pages 1691–1703.\nPMLR, 2020. 2\n[15] Yunpeng Chen, Haoqi Fang, Bing Xu, Zhicheng Yan, Yannis\nKalantidis, Marcus Rohrbach, Shuicheng Yan, and Jiashi\nFeng. Drop an octave: Reducing spatial redundancy in con-\nvolutional neural networks with octave convolution. arXiv\npreprint arXiv:1904.05049, 2019. 2\n[16] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever.\nGenerating long sequences with sparse transformers. arXiv\npreprint arXiv:1904.10509, 2019. 3\n[17] Krzysztof Choromanski, Valerii Likhosherstov, David Do-\nhan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter\nHawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser,\net al. Rethinking attention with performers. arXiv preprint\narXiv:2009.14794, 2020. 3\n[18] Xiangxiang Chu, Bo Zhang, Zhi Tian, Xiaolin Wei, and\nHuaxia Xia. Do we really need explicit position encodings\nfor vision transformers? arXiv preprint arXiv:2102.10882,\n2021. 2\n[19] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V\nLe. Randaugment: Practical automated data augmentation\nwith a reduced search space. In Proc. CVPR, 2020. 13, 14\n[20] Zihang Dai, Guokun Lai, Yiming Yang, and Quoc V\nLe. Funnel-transformer: Filtering out sequential redun-\ndancy for efﬁcient language processing. arXiv preprint\narXiv:2006.03236, 2020. 3\n[21] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li,\nand Li Fei-Fei. Imagenet: A large-scale hierarchical image\ndatabase. In Proc. CVPR, pages 248–255. Ieee, 2009. 2, 3\n[22] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei.\nImageNet: A large-scale hierarchical image database. In\nProc. CVPR, 2009. 6, 9, 14\n[23] Karan Desai and Justin Johnson. Virtex: Learning visual\nrepresentations from textual annotations. arXiv preprint\narXiv:2006.06666, 2020. 2\n[24] Piotr Dollár, Mannat Singh, and Ross Girshick. Fast and\naccurate model scaling. In Proc. CVPR, 2021. 9\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\nvain Gelly, et al. An image is worth 16x16 words: Trans-\nformers for image recognition at scale. arXiv preprint\narXiv:2010.11929, 2020. 1, 2, 3, 4, 5, 6, 9, 10, 13\n[26] Sergey Edunov, Myle Ott, Michael Auli, and David Grang-\nier. Understanding back-translation at scale. arXiv preprint\narXiv:1808.09381, 2018. 1\n[27] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and\nHervé Jégou. Training vision transformers for image re-\ntrieval. arXiv preprint arXiv:2102.05644, 2021. 2\n[28] Haoqi Fan, Yanghao Li, Bo Xiong, Wan-Yen Lo, and\nChristoph Feichtenhofer. PySlowFast.https://github.\ncom/facebookresearch/slowfast, 2020. 13\n[29] Christoph Feichtenhofer. X3d: Expanding architectures for\nefﬁcient video recognition. In Proc. CVPR, pages 203–213,\n2020. 2, 6, 7, 9, 10\n[30] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and\nKaiming He. SlowFast networks for video recognition. In\nProc. ICCV, 2019. 2, 6, 7, 9, 10, 11, 13, 14\n[31] Christoph Feichtenhofer, Axel Pinz, and Andrew Zisserman.\nConvolutional two-stream network fusion for video action\nrecognition. In Proc. CVPR, 2016. 2\n[32] Kunihiko Fukushima and Sei Miyake. Neocognitron: A\nself-organizing neural network model for a mechanism of\nvisual pattern recognition. In Competition and cooperation\nin neural nets, pages 267–285. Springer, 1982. 1\n[33] Valentin Gabeur, Chen Sun, Karteek Alahari, and Cordelia\nSchmid. Multi-modal transformer for video retrieval. In\nProc. ECCV, volume 5. Springer, 2020. 2\n[34] Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu\nZhang, Ming-Hsuan Yang, and Philip HS Torr. Res2net:\nA new multi-scale backbone architecture. IEEE PAMI, 2019.\n2\n[35] Rohit Girdhar, Joao Carreira, Carl Doersch, and Andrew\nZisserman. Video action transformer network. In Proc.\nCVPR, 2019. 2\n15\n[36] Ross Girshick. Fast R-CNN. In Proc. ICCV, 2015. 14\n[37] Priya Goyal, Piotr Dollár, Ross Girshick, Pieter Noord-\nhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch,\nYangqing Jia, and Kaiming He. Accurate, large minibatch\nSGD: training ImageNet in 1 hour. arXiv:1706.02677, 2017.\n13, 14\n[38] Raghav Goyal, Samira Ebrahimi Kahou, Vincent Michal-\nski, Joanna Materzynska, Susanne Westphal, Heuna Kim,\nValentin Haenel, Ingo Fruend, Peter Yianilos, Moritz\nMueller-Freitag, et al. The “Something Something\" video\ndatabase for learning and evaluating visual common sense.\nIn ICCV, 2017. 2, 6, 7, 14\n[39] Chunhui Gu, Chen Sun, David A. Ross, Carl V on-\ndrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijaya-\nnarasimhan, George Toderici, Susanna Ricco, Rahul Suk-\nthankar, Cordelia Schmid, and Jitendra Malik. A V A: A video\ndataset of spatio-temporally localized atomic visual actions.\nIn Proc. CVPR, 2018. 2, 6, 7, 13, 14\n[40] Meng-Hao Guo, Jun-Xiong Cai, Zheng-Ning Liu, Tai-Jiang\nMu, Ralph R Martin, and Shi-Min Hu. Pct: Point cloud\ntransformer. arXiv preprint arXiv:2012.09688, 2020. 2\n[41] Zhang Hang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi\nZhang, Haibin Lin, and Yue Sun. Resnest: Split-attention\nnetworks. 2020. 2\n[42] Boris Hanin and David Rolnick. How to start training:\nThe effect of initialization and architecture. arXiv preprint\narXiv:1803.01719, 2018. 13, 14\n[43] Kaiming He, Georgia Gkioxari, Piotr Dollár, and Ross Gir-\nshick. Mask R-CNN. In Proc. ICCV, 2017. 14\n[44] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDelving deep into rectiﬁers: Surpassing human-level per-\nformance on imagenet classiﬁcation. In Proc. CVPR, 2015.\n1\n[45] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nDeep residual learning for image recognition. In Proc.\nCVPR, 2016. 5, 13\n[46] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\nIdentity mappings in deep residual networks. InProc. ECCV,\n2016. 2\n[47] Shuting He, Hao Luo, Pichao Wang, Fan Wang, Hao Li,\nand Wei Jiang. Transreid: Transformer-based object re-\nidentiﬁcation. arXiv preprint arXiv:2102.04378, 2021. 2\n[48] Dan Hendrycks and Kevin Gimpel. Gaussian error linear\nunits (GELUs). arXiv preprint arXiv:1606.08415, 2016. 13\n[49] Geoffrey E Hinton, Nitish Srivastava, Alex Krizhevsky, Ilya\nSutskever, and Ruslan R Salakhutdinov. Improving neural\nnetworks by preventing co-adaptation of feature detectors.\narXiv:1207.0580, 2012. 13\n[50] Elad Hoffer, Tal Ben-Nun, Itay Hubara, Niv Giladi, Torsten\nHoeﬂer, and Daniel Soudry. Augment your batch: Improving\ngeneralization through instance repetition. In Proc. CVPR,\npages 8129–8138, 2020. 6, 13, 14\n[51] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen\nWei. Relation networks for object detection. In Proc. CVPR,\npages 3588–3597, 2018. 2\n[52] Han Hu, Zheng Zhang, Zhenda Xie, and Stephen Lin. Local\nrelation networks for image recognition. In Proc. ICCV,\npages 3464–3473, 2019. 2\n[53] Ronghang Hu and Amanpreet Singh. Transformer is all\nyou need: Multimodal multitask learning with a uniﬁed\ntransformer. arXiv preprint arXiv:2102.10772, 2021. 2\n[54] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q\nWeinberger. Deep networks with stochastic depth. In Proc.\nECCV, 2016. 13, 14\n[55] DH Hubel and TN Wiesel. Receptive ﬁelds of optic nerve\nﬁbres in the spider monkey. The Journal of physiology ,\n154(3):572–580, 1960. 1\n[56] Noureldien Hussein, Efstratios Gavves, and Arnold WM\nSmeulders. Timeception for complex action recognition. In\nProc. CVPR, 2019. 7\n[57] Boyuan Jiang, MengMeng Wang, Weihao Gan, Wei Wu, and\nJunjie Yan. Stm: Spatiotemporal and motion encoding for\naction recognition. In Proc. CVPR, pages 2000–2009, 2019.\n2\n[58] Huaizu Jiang, Deqing Sun, Varun Jampani, Ming-Hsuan\nYang, Erik Learned-Miller, and Jan Kautz. Super slomo:\nHigh quality estimation of multiple intermediate frames for\nvideo interpolation. In Proc. CVPR, 2018. 14\n[59] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,\nChloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,\nTim Green, Trevor Back, Paul Natsev, et al. The kinetics\nhuman action video dataset. arXiv:1705.06950, 2017. 2, 6\n[60] Nikita Kitaev, Łukasz Kaiser, and Anselm Levskaya.\nReformer: The efﬁcient transformer. arXiv preprint\narXiv:2001.04451, 2020. 3\n[61] Jan J Koenderink. The structure of images. Biological\ncybernetics, 50(5):363–370, 1984. 1\n[62] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.\nImageNet classiﬁcation with deep convolutional neural net-\nworks. In NIPS, 2012. 2\n[63] Heeseung Kwon, Manjin Kim, Suha Kwak, and Minsu Cho.\nMotionsqueeze: Neural motion feature learning for video\nunderstanding. In Proc. ECCV, pages 345–362. Springer,\n2020. 7\n[64] Hung Le, Doyen Sahoo, Nancy F Chen, and Steven CH\nHoi. Multimodal transformer networks for end-to-\nend video-grounded dialogue systems. arXiv preprint\narXiv:1907.01166, 2019. 2\n[65] Yann LeCun, Bernhard Boser, John S Denker, Donnie\nHenderson, Richard E Howard, Wayne Hubbard, and\nLawrence D Jackel. Backpropagation applied to handwritten\nzip code recognition. Neural computation, 1(4):541–551,\n1989. 1, 2\n[66] Liunian Harold Li, Mark Yatskar, Da Yin, Cho-Jui Hsieh,\nand Kai-Wei Chang. Visualbert: A simple and perfor-\nmant baseline for vision and language. arXiv preprint\narXiv:1908.03557, 2019. 2\n[67] Rui Li, Chenxi Duan, and Shunyi Zheng. Linear attention\nmechanism: An efﬁcient attention for semantic segmenta-\ntion. arXiv preprint arXiv:2007.14902, 2020. 3\n[68] Yan Li, Bin Ji, Xintian Shi, Jianguo Zhang, Bin Kang, and\nLimin Wang. Tea: Temporal excitation and aggregation for\naction recognition. In Proc. CVPR, pages 909–918, 2020. 7\n[69] Zhenyang Li, Kirill Gavrilyuk, Efstratios Gavves, Mihir Jain,\nand Cees GM Snoek. VideoLSTM convolves, attends and\n16\nﬂows for action recognition. Computer Vision and Image\nUnderstanding, 166:41–50, 2018. 2\n[70] Ji Lin, Chuang Gan, and Song Han. Temporal shift module\nfor efﬁcient video understanding. In Proc. ICCV, 2019. 14\n[71] Ji Lin, Chuang Gan, and Song Han. Tsm: Temporal shift\nmodule for efﬁcient video understanding. In Proc. CVPR,\npages 7083–7093, 2019. 7\n[72] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end hu-\nman pose and mesh reconstruction with transformers. arXiv\npreprint arXiv:2012.09760, 2020. 2\n[73] Wei Liu, Sihan Chen, Longteng Guo, Xinxin Zhu, and Jing\nLiu. Cptr: Full transformer network for image captioning.\narXiv preprint arXiv:2101.10804, 2021. 2\n[74] Francesco Locatello, Dirk Weissenborn, Thomas Un-\nterthiner, Aravindh Mahendran, Georg Heigold, Jakob\nUszkoreit, Alexey Dosovitskiy, and Thomas Kipf. Object-\ncentric learning with slot attention. arXiv preprint\narXiv:2006.15055, 2020. 2\n[75] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gra-\ndient descent with warm restarts. arXiv:1608.03983, 2016.\n13, 14\n[76] Ilya Loshchilov and Frank Hutter. Fixing weight decay\nregularization in adam. 2018. 13, 14\n[77] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert:\nPretraining task-agnostic visiolinguistic representations for\nvision-and-language tasks. arXiv preprint arXiv:1908.02265,\n2019. 2\n[78] Daniel Neimark, Omri Bar, Maya Zohar, and Dotan As-\nselmann. Video transformer network. arXiv preprint\narXiv:2102.00719, 2021. 2, 3, 6, 10\n[79] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-\ntemporal representation with pseudo-3d residual networks.\nIn Proc. ICCV, 2017. 2\n[80] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya\nRamesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,\nAmanda Askell, Pamela Mishkin, Jack Clark, et al. Learning\ntransferable visual models from natural language supervi-\nsion. arXiv preprint arXiv:2103.00020, 2021. 2\n[81] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick,\nKaiming He, and Piotr Dollár. Designing network design\nspaces. In Proc. CVPR, June 2020. 2, 9\n[82] Prajit Ramachandran, Niki Parmar, Ashish Vaswani, Irwan\nBello, Anselm Levskaya, and Jonathon Shlens. Stand-\nalone self-attention in vision models. arXiv preprint\narXiv:1906.05909, 2019. 2\n[83] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott\nGray, Chelsea V oss, Alec Radford, Mark Chen, and Ilya\nSutskever. Zero-shot text-to-image generation. arXiv\npreprint arXiv:2102.12092, 2021. 2\n[84] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun.\nFaster R-CNN: Towards real-time object detection with re-\ngion proposal networks. In NIPS, 2015. 14\n[85] Azriel Rosenfeld and Mark Thurston. Edge and curve de-\ntection for visual scene analysis. IEEE Transactions on\ncomputers, 100(5):562–569, 1971. 1\n[86] Gunnar A Sigurdsson, Gül Varol, Xiaolong Wang, Ali\nFarhadi, Ivan Laptev, and Abhinav Gupta. Hollywood in\nhomes: Crowdsourcing data collection for activity under-\nstanding. In ECCV, 2016. 2, 6, 7, 14\n[87] Karen Simonyan and Andrew Zisserman. Two-stream con-\nvolutional networks for action recognition in videos. In\nNIPS, 2014. 2, 5\n[88] Karen Simonyan and Andrew Zisserman. Very deep convo-\nlutional networks for large-scale image recognition. In Proc.\nICLR, 2015. 2\n[89] Chen Sun, Abhinav Shrivastava, Carl V ondrick, Kevin Mur-\nphy, Rahul Sukthankar, and Cordelia Schmid. Actor-centric\nrelation network. In ECCV, 2018. 14\n[90] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proc. CVPR, 2015. 2\n[91] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,\nScott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent\nVanhoucke, and Andrew Rabinovich. Going deeper with\nconvolutions. In Proc. CVPR, 2015. 13\n[92] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe,\nJonathon Shlens, and Zbigniew Wojna. Rethinking the in-\nception architecture for computer vision. arXiv:1512.00567,\n2015. 13, 14\n[93] Mingxing Tan and Quoc V Le. Efﬁcientnet: Rethinking\nmodel scaling for convolutional neural networks. arXiv\npreprint arXiv:1905.11946, 2019. 2, 9\n[94] Yi Tay, Dara Bahri, Liu Yang, Donald Metzler, and Da-\nCheng Juan. Sparse sinkhorn attention. In Proc. ICML,\npages 9438–9447. PMLR, 2020. 3\n[95] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco\nMassa, Alexandre Sablayrolles, and Hervé Jégou. Training\ndata-efﬁcient image transformers & distillation through at-\ntention. arXiv preprint arXiv:2012.12877, 2020. 1, 2, 6, 9,\n13, 14\n[96] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli.\nVideo classiﬁcation with channel-separated convolutional\nnetworks. In Proc. ICCV, 2019. 2, 6\n[97] Jeya Maria Jose Valanarasu, Poojan Oza, Ilker Hacihaliloglu,\nand Vishal M Patel. Medical transformer: Gated axial-\nattention for medical image segmentation. arXiv preprint\narXiv:2102.10662, 2021. 2\n[98] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-\nreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Il-\nlia Polosukhin. Attention is all you need. arXiv preprint\narXiv:1706.03762, 2017. 1, 2, 3, 4\n[99] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille,\nand Liang-Chieh Chen. Max-deeplab: End-to-end panop-\ntic segmentation with mask transformers. arXiv preprint\narXiv:2012.00759, 2020. 2\n[100] Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and\nHao Ma. Linformer: Self-attention with linear complexity.\narXiv preprint arXiv:2006.04768, 2020. 3\n[101] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaim-\ning He. Non-local neural networks. In Proc. CVPR, 2018.\n2, 7\n[102] Xiaolong Wang and Abhinav Gupta. Videos as space-time\nregion graphs. In Proc. ECCV, 2018. 7\n17\n[103] Yuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua\nShen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-\nto-end video instance segmentation with transformers. arXiv\npreprint arXiv:2011.14503, 2020. 2\n[104] Bichen Wu, Chenfeng Xu, Xiaoliang Dai, Alvin Wan,\nPeizhao Zhang, Masayoshi Tomizuka, Kurt Keutzer, and\nPeter Vajda. Visual transformers: Token-based image repre-\nsentation and processing for computer vision. arXiv preprint\narXiv:2006.03677, 2020. 2\n[105] Chao-Yuan Wu, Christoph Feichtenhofer, Haoqi Fan, Kaim-\ning He, Philipp Krähenbühl, and Ross Girshick. Long-term\nfeature banks for detailed video understanding. In Proc.\nCVPR, 2019. 2, 7\n[106] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and\nKevin Murphy. Rethinking spatiotemporal feature learning\nfor video understanding. arXiv:1712.04851, 2017. 2\n[107] Sen Yang, Zhibin Quan, Mu Nie, and Wankou Yang. Trans-\npose: Towards explainable human pose estimation by trans-\nformer. arXiv preprint arXiv:2012.14214, 2020. 2\n[108] Jun Yu, Jing Li, Zhou Yu, and Qingming Huang. Multimodal\ntransformer with multi-view visual representation for image\ncaptioning. IEEE transactions on circuits and systems for\nvideo technology, 30(12):4467–4480, 2019. 2\n[109] Li Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi,\nFrancis EH Tay, Jiashi Feng, and Shuicheng Yan. Tokens-\nto-token vit: Training vision transformers from scratch on\nimagenet. arXiv preprint arXiv:2101.11986, 2021. 2\n[110] Zhenxun Yuan, Xiao Song, Lei Bai, Wengang Zhou, Zhe\nWang, and Wanli Ouyang. Temporal-channel transformer for\n3d lidar-based video object detection in autonomous driving.\narXiv preprint arXiv:2011.13628, 2020. 2\n[111] Boxiang Yun, Yan Wang, Jieneng Chen, Huiyu Wang, Wei\nShen, and Qingli Li. Spectr: Spectral transformer for hy-\nperspectral pathology image segmentation. arXiv preprint\narXiv:2103.03604, 2021. 2\n[112] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk\nChun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regu-\nlarization strategy to train strong classiﬁers with localizable\nfeatures. In Proc. ICCV, 2019. 13, 14\n[113] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and\nDavid Lopez-Paz. Mixup: Beyond empirical risk minimiza-\ntion. In Proc. ICLR, 2018. 13, 14\n[114] Hengshuang Zhao, Jiaya Jia, and Vladlen Koltun. Exploring\nself-attention for image recognition. In Proc. CVPR, pages\n10076–10085, 2020. 2\n[115] Hengshuang Zhao, Li Jiang, Jiaya Jia, Philip Torr, and\nVladlen Koltun. Point transformer. arXiv preprint\narXiv:2012.09164, 2020. 2\n[116] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and\nYi Yang. Random erasing data augmentation. InProceedings\nof the AAAI Conference on Artiﬁcial Intelligence, volume 34,\npages 13001–13008, 2020. 13, 14\n[117] Bolei Zhou, Alex Andonian, Aude Oliva, and Antonio Tor-\nralba. Temporal relational reasoning in videos. In ECCV,\n2018. 2\n18",
  "topic": "Computer science",
  "concepts": [
    {
      "name": "Computer science",
      "score": 0.7498148083686829
    },
    {
      "name": "Computation",
      "score": 0.7186647653579712
    },
    {
      "name": "Transformer",
      "score": 0.6957193613052368
    },
    {
      "name": "Artificial intelligence",
      "score": 0.6572334170341492
    },
    {
      "name": "Computer vision",
      "score": 0.5229678750038147
    },
    {
      "name": "High resolution",
      "score": 0.4784453213214874
    },
    {
      "name": "Image resolution",
      "score": 0.4489811360836029
    },
    {
      "name": "Pattern recognition (psychology)",
      "score": 0.3693399727344513
    },
    {
      "name": "Algorithm",
      "score": 0.12626227736473083
    },
    {
      "name": "Engineering",
      "score": 0.11985406279563904
    },
    {
      "name": "Voltage",
      "score": 0.1166708767414093
    },
    {
      "name": "Electrical engineering",
      "score": 0.08717823028564453
    },
    {
      "name": "Remote sensing",
      "score": 0.0
    },
    {
      "name": "Geology",
      "score": 0.0
    }
  ],
  "institutions": [
    {
      "id": "https://openalex.org/I2252078561",
      "name": "Meta (Israel)",
      "country": "IL"
    },
    {
      "id": "https://openalex.org/I134446601",
      "name": "Berkeley College",
      "country": "US"
    },
    {
      "id": "https://openalex.org/I95457486",
      "name": "University of California, Berkeley",
      "country": "US"
    }
  ]
}