{
    "title": "Investigating the Vision Transformer Model for Image Retrieval Tasks",
    "url": "https://openalex.org/W3118734924",
    "year": 2021,
    "authors": [
        {
            "id": "https://openalex.org/A5007966326",
            "name": "Socratis Gkelios",
            "affiliations": [
                "Democritus University of Thrace"
            ]
        },
        {
            "id": "https://openalex.org/A5011949101",
            "name": "Yiannis S. Boutalis",
            "affiliations": [
                "Democritus University of Thrace"
            ]
        },
        {
            "id": "https://openalex.org/A5072311081",
            "name": "Savvas A. Chatzichristofis",
            "affiliations": [
                "Neapolis University Pafos"
            ]
        }
    ],
    "references": [
        "https://openalex.org/W1524680991",
        "https://openalex.org/W2062118960",
        "https://openalex.org/W2123229215",
        "https://openalex.org/W2088866137",
        "https://openalex.org/W1922773808",
        "https://openalex.org/W2295537791",
        "https://openalex.org/W6749947744",
        "https://openalex.org/W2017447430",
        "https://openalex.org/W2587013007",
        "https://openalex.org/W3110536152",
        "https://openalex.org/W6778883912",
        "https://openalex.org/W2212363941",
        "https://openalex.org/W2963542740",
        "https://openalex.org/W2108598243",
        "https://openalex.org/W1556531089",
        "https://openalex.org/W2050697589",
        "https://openalex.org/W2148809531",
        "https://openalex.org/W2141362318",
        "https://openalex.org/W2076145548",
        "https://openalex.org/W6673644053",
        "https://openalex.org/W2018057603",
        "https://openalex.org/W1976794880",
        "https://openalex.org/W2963091558",
        "https://openalex.org/W2964080601",
        "https://openalex.org/W6779879114",
        "https://openalex.org/W6739901393",
        "https://openalex.org/W2499468060",
        "https://openalex.org/W1504650083",
        "https://openalex.org/W2163502583",
        "https://openalex.org/W2789027062",
        "https://openalex.org/W6695676441",
        "https://openalex.org/W2340690086",
        "https://openalex.org/W204268067",
        "https://openalex.org/W2890448425",
        "https://openalex.org/W2963129433",
        "https://openalex.org/W2912723187",
        "https://openalex.org/W2963557772",
        "https://openalex.org/W2793934511",
        "https://openalex.org/W2963403868",
        "https://openalex.org/W3094502228",
        "https://openalex.org/W3117450517",
        "https://openalex.org/W3037784242",
        "https://openalex.org/W2092287537",
        "https://openalex.org/W3030163527",
        "https://openalex.org/W2128237624",
        "https://openalex.org/W2963166708",
        "https://openalex.org/W3034445277",
        "https://openalex.org/W2963685250",
        "https://openalex.org/W2963341956"
    ],
    "abstract": "This paper introduces a plug-and-play descriptor that can be effectively adopted for image retrieval tasks without prior initialization or preparation. The description method utilizes the recently proposed Vision Transformer network while it does not require any training data to adjust parameters. In image retrieval tasks, the use of Handcrafted global and local descriptors has been very successfully replaced, over the last years, by the Convolutional Neural Networks (CNN)-based methods. However, the experimental evaluation conducted in this paper on several benchmarking datasets against 36 state-of-the-art descriptors from the literature demonstrates that a neural network that contains no convolutional layer, such as Vision Transformer, can shape a global descriptor and achieve competitive results. As fine-tuning is not required, the presented methodology's low complexity encourages adoption of the architecture as an image retrieval baseline model, replacing the traditional and well adopted CNN-based approaches and inaugurating a new era in image retrieval approaches.",
    "full_text": "Investigating the Vision Transformer Model for\nImage Retrieval Tasks\nSocratis Gkelios\nDepartment of Electrical and\nComputer Engineering\nDemocritus University of Thrace\nXanthi, Greece.\nsgkelios@ee.duth.gr\nYiannis Boutalis\nDepartment of Electrical and\nComputer Engineering\nDemocritus University of Thrace\nXanthi, Greece.\nybout@ee.duth.gr\nSavvas A. Chatzichristoﬁs\nIntelligent Systems Laboratory\nDepartment of Computer Science\nNeapolis University Pafos\nPafos, Cyprus\ns.chatzichristoﬁs@nup.ac.cy\nAbstract—This paper introduces a plug-and-play descriptor\nthat can be effectively adopted for image retrieval tasks without\nprior initialization or preparation. The description method uti-\nlizes the recently proposed Vision Transformer network while\nit does not require any training data to adjust parameters.\nIn image retrieval tasks, the use of Handcrafted global and\nlocal descriptors has been very successfully replaced, over the\nlast years, by the Convolutional Neural Networks (CNN)-based\nmethods. However, the experimental evaluation conducted in\nthis paper on several benchmarking datasets against 36 state-\nof-the-art descriptors from the literature demonstrates that a\nneural network that contains no convolutional layer, such as\nVision Transformer, can shape a global descriptor and achieve\ncompetitive results. As ﬁne-tuning is not required, the pre-\nsented methodology’s low complexity encourages adoption of\nthe architecture as an image retrieval baseline model, replacing\nthe traditional and well adopted CNN-based approaches and\ninaugurating a new era in image retrieval approaches.\nIndex Terms—Vision Transformer, Image Retrieval, CBIR\nI. I NTRODUCTION\nContent-based image retrieval (CBIR) is one of the fun-\ndamental scientiﬁc challenges that the multimedia, computer\nvision, and robotics communities have thoroughly researched\nfor decades. Three eras that vary in how researchers export\nthe different features that deﬁne an image’s visual content\ncharacterize content-based image retrieval [1]. The literature\nstrongly focuses on global low-level descriptors during the\nearly years, shaping the ﬁrst CBIR era. A single vector was\nused to represent different aspects of an image, such as color,\nshape, and texture. In the sequel, the community started to con-\ncentrate on representations focused on the extraction and use\nof local features. From 2003 onwards, new approaches adopt\nlocal image descriptors to search for salient image patches\nand points-of-interest, such as edges, corners, and blobs. Since\n2015, image retrieval research strongly relies on approaches\nfocused on Deep Learning (DL) and Convolutional Neural\nNetworks (CNN). Although DL-based techniques require a\nlarge amount of data for training, the pre-trained networks have\nbeen demonstrated in several studies to be particularly useful\nas feature extractors and achieve high retrieval performance.\n*Corresponding author: s.chatzichristoﬁs@nup.ac.cy\nMeanwhile, in natural language processing (NLP), the self-\nattention-based architecture, particularly Transformers, is now\nconsidered as the new standard [2]. The Transformer is a\ntype of deep-neural network mainly based on self-attention\nmechanism [3]. Recently, researchers have expanded trans-\nformers for computer vision tasks inspired by the inﬂuence\nof the Transformer in NLP [4]–[6]. For example, the authors\nin [7] have trained a sequence transformer to auto-regressively\npredict pixels and achieve competitive results with CNNs on an\nimage classiﬁcation task. Recently, the authors in [8] have at-\ntempted to apply a typical transformer directly to images, with\nthe fewest adjustments possible shaping an image recognition\nnetwork. Vision Transformer network (ViT) provides better\nperformance than a traditional convolutional neural network\nin image recognition tasks.\nThis paper builds upon the Vision Transformer architec-\nture to shape a global descriptor for image retrieval tasks.\nFollowing the procedure that the vast majority of the deep-\nlearning-based image retrieval models adopt, we discard the\nfully connected layers of the Transformer’s network and use\nthe last layer as a feature to describe the visual content of the\nimage. Best we can tell, this is the ﬁrst attempt to evaluating\nthe performance of the transformers on image retrieval tasks.\nThe remainder of this paper is structured as follows. Section\nII brieﬂy shows how the vision transformer’s architecture\noperates and presents the process of shaping the image re-\ntrieval descriptor. Section III offers a thorough experimental\nevaluation of the descriptor complemented by a discussion of\nresults. Finally, Section IV concludes the paper, and discusses\npotential opportunities for future work.\nII. V IT DESCRIPTOR\nThe Transformer architecture was ﬁrst introduced in [2] for\nneural machine translation exhibiting state-of-the-art perfor-\nmance and replacing models, such as Long short-term memory\nartiﬁcial recurrent neural networks and Gated recurrent units\n(GRUs) in several Natural Language Processing (NLP) tasks.\nThe architecture of the Transformer usually consists of stacked\nTransformer layers, each of which takes as input a sequence\nof vectors and outputs a new sequence of vectors of the same\nshape. The critical element of the Transformer architectures\narXiv:2101.03771v1  [cs.CV]  11 Jan 2021\nFig. 1. Graphical Abstract of the ViT Extraction Procedure.\nresides in the attention mechanism. The transformer develop-\nment’s primary motivation was to overcome the limitations of\nthe recurrent neural networks (RNN) and mainly to capture\nlong-term global dependencies. Based on this foundation,\nmany noteworthy approaches emerged, for example, Bidirec-\ntional Encoder Representations from Transformers (BERT)\n[9], and GPT-3 [10], both pre-trained in an unsupervised man-\nner from the unlabeled text. These methods demonstrated rich\nrepresentation capability without ﬁne-tuning. Both methods\nare conceptually simple and empirically powerful.\nThe application of Transformers in computer vision tasks\nposes a signiﬁcant challenge for two main reasons:\n• Images contain much more information compared to\nwords or sentences.\n• Attention to every pixel is computationally exhaustive.\nThe authors in [8] proposed a novel approach called Vision\nTransformer (ViT) to tackle these challenges. ViT borrows the\nencoder part of the NLP Transformer. The standard Trans-\nformer receives a 1D token embedding sequence as input. In\nthis case, the images are split into ﬁxed-sized patches and\nfeed into the model. A learnable positional embedding vector\nis assigned to every patch to utilize the order of the input\nsequence. Besides, a special token is added, just like in the\ncase of BERT. In a nutshell, a set of patches is constructed\nfor each image multiplied with an embedding matrix, which\nis ﬁnally fused with a positional embedding vector, forming\nthe Transformer input. In all its layers, the Transformer uses\nconstant latent vector size, so with a trainable linear projec-\ntion, the patches are ﬂattened to map these dimensions. This\nprojection’s output is referred to as patch embedding.\nEach encoder consists of two sub-layers. In the ﬁrst sub-\nlayer, the inputs ﬂow through a self-attention module, while\nin the second, the outputs of the self-attention operation are\npassed to a position-wise feed-forward neural network. Fur-\nthermore, skip connections [11] are incorporated around each\nsub-layer that undergo layer normalization. The architecture\nof the encoder is depicted in Figure 2. Every layer contains\na constant linear projection mapping of the ﬂattened patches\nof dimension D reﬁned during training. D is equal to 768\nfor ViT-B architectures, whereas in ViT-L, the corresponding\nvalue is 1024. Even though the encoder layers are composed\nof identical structural elements, they do not share weights.\nThe self-attention module consists of three learnable com-\nponents, Query (Q), Key (K), and Value (V). A score matrix\nis assembled by Q and K dot product multiplication that\nportrays the attention of each ‘word’ to each other ‘word’\n(in our approach, each ‘word’ corresponds to one of the\nimage patches). The score matrix is scaled accordingly and is\npassed to a softmax layer to convert scores into probabilities.\nIn the sequel, the softmax output is multiplied with the V\nvector to highlight the important words. Multi-headed attention\nimproves the performance of the Transformer. In particular, the\nmodel performs multiple parallel attention functions to differ-\nent linear projections of the vectors Q, V , and K instead of\nutilizing a single one. Thus, this procedure enables the model\nto focus on different positions and representation subspaces.\nPosition-wise neural network module is a fully connected\nfeed-forward neural network (FFNN), which consists of two\nlinear transformations with a ReLU activation in between.\nLayer Layernorm (LN) is applied before each module, and\nresidual connections after every block, followed by layer\nnormalization [12].\nIn this paper, we adopted the pre-trained architecture of [8].\nMore speciﬁcally, this paper adopts the ‘Base’ models (ViT-B)\nand the ‘Large’ models (ViT-L) from BERT. Two variants of\neach architecture regarding their input patch size are applied.\nViT-L16 corresponds to 16 ×16 input batch size, while ViT-\nL32 to 32×32 patch size. The same notations are used for the\nViT-B model accordingly. The models with smaller patch sizes\nare more resource-intensive due to the inversely proportional\nrelationship between the Transformer’s length sequence and\nthe patch size square.\nThe adopted architectures have been trained on 21k-\nImageNet (21k classes/14 million images) and have been ﬁne-\ntuned on the ILSVRC-2012 ImageNet (1k classes/1.3 million\nimages) [13]. ViT-B features 12 encoder layers with an FFNN\nof size 3072. On the other hand, ViT-L contains 24 encoder\nlayers and an FFNN of size 4096.\nIn the case of ViT descriptor, initially, all the images\nare resized to 384 ×384 pixel size. A pre-preprocessing\nunit subtracts 127.5 to all pixels and scales by 255 to have\npixel values ranging [-1, 1]. The pre-processed images ﬂow\nthrough the Transformer Encoder. The last softmax layer of\nthe models is removed, leaving the normalized last encoder’s\noutput of dimension D as the ﬁnal layer. This procedure\nshapes a representation vector for each image. The ﬁnal step\nFig. 2. Modelling the Transformer Encoder Architecture.\nis comprised of a normalization procedure to shape the ViT\ndescriptor. Figure1 illustrates the proposed CBIR architecture.\nIII. E XPERIMENTAL SETUP\nThis section provides details about the experiments con-\nducted for the evaluation of the ViT descriptor. The presented\nimage representation is evaluated on four well-known image\ndatasets:\n• INRIA Holidays [14]: This dataset consists of 1491\ncellphone-taken pictures with a wide range of holiday\nscenes and objects. The number of images varies from\n2 to 13 images per group. The INRIA Holidays dataset\nprovides many query images instead of the UKBench\ndatabase. The ground truth comprises images of a visual\ndeﬁnition identical to the query image, without indicating\nthe same object’s co-occurrence.\n• UKBench [15]: The UKBench dataset consists of 10,200\nimages arranged in 2250 categories dataset. There are\nfour pictures of a single object in each category, taken\nfrom various views and under varying lighting conditions.\nThe top-4 candidate (NS) score [15] is used for this\ndataset during the evaluation process to calculate the\nretrieval accuracy.\n• Paris6K [16]: 6412 photographs representing unique Paris\nlandmarks are included in the Paris6k Dataset. This\ncollection consists of 55 pictures of buildings and mon-\numents from requests. There is more variety in the\nlandmarks in Paris6k than those in Oxford5k.\n• Oxford5k [17]: This building’s dataset is made up of 5062\nFlickr images. The set has been manually annotated to\nproduce a detailed ground truth for 11 distinct landmarks,\neach represented by ﬁve potential queries. The index,\noverall, consists of 55 requests. In this dataset, completely\ndifferent views of the same building are labeled by the\nsame name, making the collection challenging for image\nretrieval tasks [18].\nFor the INRIA, Oxford5k, and Paris6K datasets, the mean\nAverage Precision (mAP) [19] is used as an evaluation metric.\nIn the perfect retrieval case, mAP is equal to 100, while\nas the number of the nonrelevant images ranked above a\nretrieved relevant image increases, the mAP approaches the\nvalue 0. On the other hand, the retrieval performance on the\nUKBench dataset is evaluated using the recall rate for the top-4\ncandidates (NS) as an evaluation metric. In the case of perfect\nretrieval, NS is equal to 4.\nTo calculate the similarity between the descriptors, we use\nmultiple distance metrics. There are speciﬁc advantages and\ndrawbacks of each similarity metric, each one being more ap-\npropriate fro particular data types [20]. This paper assesses the\nuse of Manhattan, Euclidean, Cosine, Bray-Curtis, Canberra,\nChebyshev, and Correlation distance metrics in the current\nanalysis. The Manhattan and Euclidean distance metrics are\nconsidered well-known, and therefore, no further details are\ngiven in this paper. Here we only note that the Manhattan\nDistance (also known as city block distance) is preferred\nover the Euclidean distance metric as the data’s dimension\nincreases.\nThe Cosine distance counts the image descriptors’ inner-\nproduct space, considering their orientation and not their\nmagnitude.\ndCosine(p, q) = pq\n∥p∥∥q∥=\n∑n\ni=1 piqi√∑n\ni=1 (pi)2\n√∑n\ni=1 (qi)2 (1)\nBray-Curtis and Canberra are mutated from Manhattan,\nwhere, as seen in the following equation, the sum of the dif-\nferences between the coordinates of the vectors is normalized:\ndCanberra (p, q) =\nn∑\ni=1\n|pi −qi|\n|pi|+ |qi| (2)\ndBray−Curtis(p, q) =\n∑n\ni=1 |pi −qi|∑n\ni=1(pi + qi) (3)\nFurthermore, the Chebyshev distance between two feature\nvectors is estimated as the maximum variation along any\ncoordinate dimension:\ndChebyshev (p, q) = max\ni\n(\n⏐⏐pi −qi\n⏐⏐) (4)\nFinally, the Correlation distance measures the dependency\nbetween the two feature vectors:\ndCorrel. (p, q) =\n∑n\ni=1(pi −¯p)(qi −¯q)√∑n\ni=1(pi −¯p)2\n√∑n\ni=1(qi −¯q)2 (5)\nIn all metrics, p and q are the corresponding image descrip-\ntors.\nAdditionally, this section also evaluates the appropriate\nnormalization technique. The batch normalization technique\npromotes training by speciﬁcally normalizing each layer’s\ninputs to have zero mean and unit variance. Weight normal-\nization reparameterizes the weight vectors in a neural network\nthat decouples the length of those weight vectors from their\ndirection [21], inspired by batch normalization. In other words,\nby their L2-norm or L1-norm [22], weight normalization\nreparameterizes incoming weights.\nThe L1-norm uses the sum of all the values providing equal\npenalty to all parameters, enforcing sparsity:\nX′\ni = Xi∑n\nk=0 Xk\n(6)\nSimilarly, the L2-norm uses the square root of the sum of\nall the squared values.\nX′\ni = Xi∑n\nk=0 X2\nk\n(7)\nIn all cases, Xi is the value to be normalized, and X′\ni is the\nnormalized score.\nThe post-processing normalization method consists of the\nindependent normalization of each derived descriptor (L1-\nnorm and L2-norm Axis-1), the normalization of each char-\nacteristic (L1-norm and L2-norm Axis-0), and the scaling of\nROBUST.\nX′\ni = Xi −q1(Xi)\nq2(Xi) −q1(Xi) (8)\nwhere q1 and q2 are quantiles.\nThe scaling normalization of ROBUST eliminates the me-\ndian. It scales the data according to the quantile range,\nmeaning that the sample mean and variance are not affected\nby the outliers.\nA. Retrieval Results\nTables I, II, III and IV list the image retrieval performance\non the INRIA, UKBench, Paris6K and Oxford5K datasets\nrespectively. The ﬁrst conclusion that emerges by observing\nthe results is that the ViT descriptor performs remarkably\nwell with almost all the different similarity metrics and all\nthe normalization techniques in all four image datasets. In all\nexperiments, the Chebyshev metric signiﬁcantly lags behind\nthe effectiveness of the other distance metrics.\nTABLE I\nEXPERIMENTAL RESULTS ON INRIA DATABASE\nINRIA\nModel Manh. Eucl. Cos BC Canb. Cheb. Correl.\nViT-L16\nL2 Axis=1 84.95 84.98 84.98 84.88 84.62 76.24 84.97\nL2 Axis=0 85.02 85.24 85.36 85.07 84.54 77.67 85.36\nL1 Axis=1 84.77 84.88 84.98 84.91 84.67 76.26 84.97\nL1 Axis=0 85.02 85.23 85.53 85.08 84.54 78.02 85.52\nROBUST 84.98 85.10 86.31 86.08 85.50 76.71 86.31\nViT-L32\nL2 Axis=1 86.84 86.56 86.56 86.77 86.32 75.90 86.56\nL2 Axis=0 86.65 86.61 86.96 86.92 86.30 76.34 86.96\nL1 Axis=1 86.88 86.57 86.56 86.78 86.32 75.45 86.56\nL1 Axis=0 86.65 86.64 86.97 86.92 86.30 76.52 86.97\nROBUST 86.61 86.61 87.03 87.05 86.80 76.83 87.03\nViT-B16\nL2 Axis=1 87.16 87.09 87.09 87.17 86.57 75.77 87.09\nL2 Axis=0 86.79 86.53 86.76 87.26 86.55 77.95 87.67\nL1 Axis=1 87.18 86.97 87.09 87.18 86.58 75.30 87.09\nL1 Axis=0 86.78 86.51 87.67 87.29 86.55 77.31 87.67\nROBUST 86.42 86.33 87.99 87.81 87.32 77.07 87.99\nViT-B32\nL2 Axis=1 85.07 84.80 84.80 85.19 84.61 62.52 84.80\nL2 Axis=0 84.97 85.04 85.61 85.38 84.74 77.96 85.61\nL1 Axis=1 85.31 84.96 84.80 85.19 84.76 61.84 84.80\nL1 Axis=0 84.98 84.93 85.63 85.38 84.74 77.25 85.63\nROBUST 84.96 85.07 86.11 86.22 85.32 76.85 86.11\nTABLE II\nEXPERIMENTAL RESULTS ON UKB ENCH DATABASE\nUKBench\nModel Manh. Eucl. Cos BC Canb. Cheb. Correl.\nViT-L16\nL2 Axis=1 3.782 3.785 3.785 3.781 3.764 3.561 3.785\nL2 Axis=0 3.778 3.78 3.785 3.782 3.764 3.575 3.785\nL1 Axis=1 3.781 3.783 3.785 3.781 3.764 3.56 3.785\nL1 Axis=0 3.778 3.781 3.785 3.782 3.764 3.574 3.785\nROBUST 3.777 3.779 3.784 3.781 3.763 3.567 3.784\nViT-L32\nL2 Axis=1 3.75 3.752 3.752 3.75 3.73 3.416 3.752\nL2 Axis=0 3.735 3.737 3.754 3.751 3.731 3.455 3.754\nL1 Axis=1 3.752 3.751 3.752 3.75 3.731 3.412 3.752\nL1 Axis=0 3.735 3.737 3.754 3.751 3.731 3.453 3.754\nROBUST 3.735 3.737 3.749 3.747 3.729 3.455 3.749\nViT-B16\nL2 Axis=1 3.745 3.746 3.746 3.745 3.728 3.345 3.746\nL2 Axis=0 3.733 3.737 3.758 3.752 3.729 3.496 3.758\nL1 Axis=1 3.746 3.746 3.746 3.746 3.729 3.34 3.746\nL1 Axis=0 3.733 3.737 3.758 3.752 3.729 3.496 3.758\nROBUST 3.733 3.737 3.759 3.752 3.733 3.496 3.759\nViT-B32\nL2 Axis=1 3.712 3.702 3.702 3.712 3.692 2.971 3.702\nL2 Axis=0 3.709 3.714 3.725 3.718 3.693 3.442 3.725\nL1 Axis=1 3.711 3.701 3.702 3.712 3.693 2.952 3.702\nL1 Axis=0 3.709 3.714 3.725 3.717 3.693 3.439 3.725\nROBUST 3.709 3.714 3.726 3.722 3.697 3.45 3.726\nA more in-depth analysis of the results also reveals that the\nViT-B16 descriptor performs robustly well in all databases.\nOf course, in the UKBench dataset, the ViT-L16 descriptor\nmanages to exceed the performance of the ViT-B16 descriptor\nslightly, but the difference is not signiﬁcant to generalize a\nconclusion. The ViT-B16 descriptor presents high retrieval\naccuracy, promoting robustness in all datasets and almost all\nthe different similarity metrics.\nRegarding the normalization techniques, ROBUST scaling\nappears to be the optimal solution. Moreover, one quickly\nconcludes that the ROBUST normalization technique, together\nwith the Cosine distance as the similarity metric, shape the\ndescriptor’s most effective setup. This combination manages\nto outperform any other design in all performed experiments.\nThus, it is safe to conclude that the combination of the ViT-\nB16 model, with ROBUST normalization and Cosine distance\nas similarity metric, shapes a robust plug-and-play solution for\neffective image retrieval.\nTABLE III\nEXPERIMENTAL RESULTS ON PARIS 6K DATABASE\nParis6k\nModel Manh. Eucl. Cos BC Canb. Cheb. Correl.\nViT-L16\nL2 Axis=1 86.24 86.23 86.23 86.25 85.82 77.34 86.25\nL2 Axis=0 86.27 86.24 86.63 86.46 85.82 76.16 86.63\nL1 Axis=1 86.17 86.15 86.23 86.25 85.82 77.17 86.25\nL1 Axis=0 86.25 86.23 86.64 86.47 85.82 75.97 86.64\nROBUST 86.18 86.13 86.74 86.71 86.38 76.16 86.74\nViT-L32\nL2 Axis=1 85.29 85.34 85.34 85.41 85.09 72.56 85.34\nL2 Axis=0 84.48 84.47 85.68 85.65 85.11 69.76 85.68\nL1 Axis=1 85.37 85.42 85.34 85.41 85.1 72.6 85.34\nL1 Axis=0 84.45 84.45 85.69 85.65 85.11 69.57 85.69\nROBUST 84.4 84.35 85.62 85.61 85.18 69.04 85.62\nViT-B16\nL2 Axis=1 86.94 87.07 87.07 86.85 86.25 75.96 87.07\nL2 Axis=0 86.28 86.23 87.21 86.97 86.27 77.26 87.21\nL1 Axis=1 86.93 87.06 87.07 86.84 86.25 75.72 87.07\nL1 Axis=0 86.27 86.2 87.23 86.99 86.27 77.06 87.23\nROBUST 86.43 86.46 87.83 87.63 87.06 76.61 87.83\nViT-B32\nL2 Axis=1 85.53 85.29 85.29 85.56 85.24 64.04 85.28\nL2 Axis=0 85.34 85.29 85.95 85.83 85.23 75.96 85.94\nL1 Axis=1 85.5 85.19 85.29 85.56 85.23 63.55 85.28\nL1 Axis=0 85.34 85.27 85.96 85.83 85.23 75.77 85.96\nROBUST 85.29 85.24 86.32 86.35 85.9 75.51 86.32\nTABLE IV\nEXPERIMENTAL RESULTS ON OXFORD 5K DATABASE\nOxford5k\nModel Manh. Eucl. Cos BC Canb. Cheb. Correl.\nViT-L16\nL2 Axis=1 60.84 60.82 60.82 60.78 59.87 51.49 60.83\nL2 Axis=0 60.9 61.20 61.50 61.15 59.85 53.26 61.50\nL1 Axis=1 60.73 60.74 60.82 60.78 59.85 51.21 60.83\nL1 Axis=0 60.91 61.13 61.58 61.2 59.85 53.12 61.58\nROBUST 60.82 60.86 62.37 62.09 61.49 52.45 62.37\nViT-L32\nL2 Axis=1 55.17 55.3 55.30 55.01 53.84 41.26 55.28\nL2 Axis=0 54.92 55.14 55.55 55.21 53.84 41.45 55.55\nL1 Axis=1 55.35 55.43 55.3 55.01 53.86 41.39 55.28\nL1 Axis=0 54.93 55.18 55.56 55.23 53.84 41.31 55.56\nROBUST 54.93 55.19 56.30 55.88 55.16 41.78 56.30\nViT-B16\nL2 Axis=1 63.24 63.59 63.59 63.22 62.17 52.44 63.59\nL2 Axis=0 62.74 63.16 64.19 63.53 62.12 56.79 64.19\nL1 Axis=1 63.13 63.68 63.59 63.26 62.13 52.3 63.59\nL1 Axis=0 62.79 63.14 64.22 63.55 62.12 56.48 64.22\nROBUST 62.72 63.05 64.68 64.09 62.84 56.02 64.68\nViT-B32\nL2 Axis=1 63.18 62.49 62.49 63.08 62.84 41.78 62.49\nL2 Axis=0 63.34 63.01 63.56 63.52 62.82 53.96 63.56\nL1 Axis=1 63.19 62.41 62.49 63.1 62.8 41.33 62.49\nL1 Axis=0 63.35 62.99 63.59 63.54 62.82 53.78 63.59\nROBUST 63.35 63.09 64.43 64.28 63.35 53.99 64.43\nB. Comparison with the State-of-the-art\nThis subsection compares the ViT descriptor’s performance\nwith the state-of-the-art local, global, and deep convolutional-\nbased approaches from the recent literature. The proposed\ndescriptor was compared to 36 descriptors from the literature.\nThe choice of the descriptors used for experimentation was\nprimarily based on their reported performance and overall\npopularity. The descriptors are listed in Table V and sorted\nbased on their retrieval performance on the INRIA dataset.\nThe reason for choosing this sorting method is related to the\nfact that most of the descriptors/approaches reported in this\nsubsection have been evaluated on the INRIA dataset and their\nretrieval results are publicly available or reproducible.\nExperimental ﬁndings show that the ViT descriptor performs\nrobustly and provides remarkable results in all benchmarking\ndatasets. The results in Table V demonstrate that the presented\napproach performs as good as the state-of-the-art approaches\non all datasets. Moreover, the acquired retrieval results are\namong the leading published ones and are directly comparable\nwith the literature’s best reported.\nTABLE V\nRETRIEVAL SCORE PER IMAGE COLLECTION . THIS PAPER USES M AP\n(MEAN AVERAGE PRECISION ) TO EVALUATE RETRIEVAL ACCURACY ON\nTHE INRIA (H OLIDAYS ), PARIS 6K, AND OXFORD 5K DATASETS . THE N-S\nSCORE IS SPECIFICALLY USED ON THE UKB ENCH DATASET . (†) REFERS\nTO RESULTS THAT ARE REPORTED IN [23], [24]\nRetrieval Method INRIA UKBen. Paris Oxford\nMMF-HC [25] 94.1 3.87 - -\nMR Spatial search [26] 89.6 - 87.9 84.3\nGatedSQU [23] 88.8 3.74 81.3 69.4\nViT-16B 88.0 3.76 87.8 64.7\nASMK+SA (large) [27] 88.0 - 78.7 82.0\nASMK+MA (large) [27] 86.5 - 80.5 83.8\nZheng Et al. (PPS) [28] 85.2 3.79 - -\nR-mac [29] 85.2(†) 3.74(†) 83.0 66.9\nR-mac-RPN [24] 86.7 - 87.1 83.1\nCroW [30] 85.1 3.63(†) 79.7 70.8\nMMF-SIFT [25] 84.4 3.94 - -\nCNNaug-ss [31] 84.3 - 46.0 36.0\nSMK+SA (large) [27] 84.0 - 73.2 78.5\nHE+WGC [32] 81.3 3.42 61.5 51.6\nGoogleNet [31] 83.6 - 58.3 55.8\nReDSL.FC1 [33] - - 94.7 78.3\nR101-DELG [34] - - 82.9 78.5\nVarga et al. [35] - - - 74.4\nSMK+MA (large) [27] 82.9 - 74.2 79.3\nCVLAD [36] 82.7 3.62 - 51.4\nOxfordNet [31] 81.6 - 59.0 59.3\nLocal CoMo [37] 81.1 - - -\nCNN+BoVW [38] 80.2 - - -\nMOP-CNN [39] 80.2 - - -\nPatch-CKN [40] 79.3 3.76 - 56.5\nNeural codes [41] 79.3 3.29 - 54.5\nAlexet-conv3 [42] 79.3 3.76 - 43.4\nLBOW [15] 78.9 3.50 - -\nAlexet-conv4 [42] 77.1 3.73 - 34.3\nSaCoCo [43] 76.1 3.33 - -\nAlexet-conv5 [42] 75.3 3.69 - 47.7\nPhilippNet [44] 74.1 3.66 - -\nCEDD [45] 72.6 3.06 - -\nCoMo [37] 72.6 - - -\nAlexet-conv2 [42] 62.7 3.19 - 12.5\nAlexet-conv1 [42] 59.0 3.33 - 18.8\nBoVW [15] 57.2 2.95 - -\nIn the INRIA and UKBench datasets, the shaped solu-\ntion presents the fourth-best reported result. In the case of\nthe Paris6K dataset, the ViT descriptor reports the third-\nbest performance. Finally, in the Oxford5K dataset, the ViT\ndescriptor’s effectiveness is comparable with the CNN-based\nimage retrieval approaches. It is noteworthy that the full image\nis used as a query in both Oxford5K and Paris6K datasets\ninstead of the annotated region of interest.\nIn a nutshell, the ViT descriptor signiﬁcantly outperforms\nall the handcrafted features and most machine-crafted, CNN-\nbased ones in all the datasets. Of course, the literature contains\nmore sophisticated methods and algorithms that exceed the\nshaped descriptor’s retrieval accuracy. But once again, it is\nworth noting that the ViT descriptor is ready to use, plug-and-\nplay descriptor, and no training or parameter ﬁtting is needed.\nThe vast majority of the listed deep-learning-based approaches\nare considerably more complicated and demanding than the\nViT solution.\nFor example, in [25], a multi-index fusion scheme for\nimage retrieval was proposed based on AlexNet and ResNet50\nnetworks. The method is called MMF and inherits the core\nidea of Collaborative Index Embedding to fuse different visual\nrepresentations on an index level. Furthermore, MMF explores\nthe high-order information assumed by considering the sparse\nindex structure for retrieval, an inverted index structure’s\nintrinsic property. The impressive performance reported by\n[25] requires the creation and learning of an index-speciﬁc\nfunctional matrix to propagate similarities and the application\nof an Augmented Lagrange Multiplier (ALM) to optimize\nmulti-index fusion. By taking into account various factors\nsuch as geometric invariance, layers, and search efﬁciency,\nan optimized CNN architecture was created and trained. This\narchitecture is known as MR Spatial search [26]. It is important\nto note that the MR Spatial search performs a costly spatial\nveriﬁcation at test-time. The authors in [23] focuses on global\nfeature pooling over CNN activations for image instance\nretrieval. The descriptor is called GatedSQU, and it contains\nthe design of a channel-wise pooling and learning with triplet\nLoss. The authors in [35] combined a semantic-level sim-\nilarity and a feature-level similarity to efﬁciently calculate\nthe similarity distance. The authors perform a comprehensive\nseries of analytical studies in [33], aimed at forming more\nefﬁcient representations of features. Finally, it is worth noting\nthat ReDSL.FC1 method signiﬁcantly outperforms all reported\nmethods on Paris and Oxford datasets as the authors ﬁne-tuned\nthe pre-trained CNN architecture directly on these landmarks\nusing similarity learning.\nThe experiments were conducted on a workstation with\nAMD Ryzen 7 1700, 32GB RAM, and GTX 1080 GPU using\nthe KERAS framework [46], a well-known high-level Python\nneural network library that runs on top of TensorFlow or\nTheano. Vision Transformer’s implementation is based on the\nopen-source release of the network available on GitHub 1.\nIV. C ONCLUSIONS\nThis paper presents a fully unsupervised, parameter-free\nimage retrieval descriptor. Overall, the shaped solution appears\nto be suitable for content-based image retrieval tasks. The\nexperimental evaluation ﬁndings on several different datasets\nclearly shown that the proposed approach managed to outper-\nform many state-of-the-art, more sophisticated, and complex\nsolutions from the literature. Although the ViT descriptor does\nnot supersede all the other methods, it is safe to conclude\nthat it brings a new approach to the image retrieval domain.\nA critical direction to improve image retrieval performance\nis to revisit the well-established CNN-based methods in the\nliterature and replace the backbone pre-trained network with\nthe vision transformers. A new chapter in the image retrieval\nscientiﬁc subject is looming.\n1https://github.com/faustomorales/vit-keras\nREFERENCES\n[1] L. Zheng, Y . Yang, and Q. Tian, “Sift meets cnn: A decade survey of\ninstance retrieval,” IEEE transactions on pattern analysis and machine\nintelligence, vol. 40, no. 5, pp. 1224–1244, 2018.\n[2] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez,\nŁ. Kaiser, and I. Polosukhin, “Attention is all you need,” Advances in\nneural information processing systems, vol. 30, pp. 5998–6008, 2017.\n[3] X. Wang, R. Girshick, A. Gupta, and K. He, “Non-local neural net-\nworks,” in Proceedings of the IEEE conference on computer vision and\npattern recognition, 2018, pp. 7794–7803.\n[4] K. Han, Y . Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y . Tang, A. Xiao,\nC. Xu, Y . Xu et al., “A survey on visual transformer,” arXiv preprint\narXiv:2012.12556, 2020.\n[5] F. Locatello, D. Weissenborn, T. Unterthiner, A. Mahendran, G. Heigold,\nJ. Uszkoreit, A. Dosovitskiy, and T. Kipf, “Object-centric learning with\nslot attention,” arXiv preprint arXiv:2006.15055, 2020.\n[6] H. Hu, J. Gu, Z. Zhang, J. Dai, and Y . Wei, “Relation networks for\nobject detection,” in Proceedings of the IEEE Conference on Computer\nVision and Pattern Recognition, 2018, pp. 3588–3597.\n[7] M. Chen, A. Radford, R. Child, J. Wu, H. Jun, D. Luan, and I. Sutskever,\n“Generative pretraining from pixels,” in International Conference on\nMachine Learning. PMLR, 2020, pp. 1691–1703.\n[8] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai,\nT. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly et al.,\n“An image is worth 16x16 words: Transformers for image recognition\nat scale,” arXiv preprint arXiv:2010.11929, 2020.\n[9] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-training\nof deep bidirectional transformers for language understanding,” arXiv\npreprint arXiv:1810.04805, 2018.\n[10] T. B. Brown, B. Mann, N. Ryder, M. Subbiah, J. Kaplan, P. Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal, A. Herbert-\nV oss, G. Krueger, T. Henighan, R. Child, A. Ramesh, D. M. Ziegler,\nJ. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin, S. Gray,\nB. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford, I. Sutskever,\nand D. Amodei, “Language models are few-shot learners,” 2020.\n[11] Q. Wang, B. Li, T. Xiao, J. Zhu, C. Li, D. Wong, and L. Chao, “Learning\ndeep transformer models for machine translation,” 01 2019, pp. 1810–\n1822.\n[12] J. L. Ba, J. R. Kiros, and G. E. Hinton, “Layer normalization,” arXiv\npreprint arXiv:1607.06450, 2016.\n[13] J. Deng, W. Dong, R. Socher, L. Li, Kai Li, and Li Fei-Fei, “Imagenet:\nA large-scale hierarchical image database,” in 2009 IEEE Conference\non Computer Vision and Pattern Recognition, 2009, pp. 248–255.\n[14] H. Jegou, M. Douze, and C. Schmid, “Hamming embedding and weak\ngeometric consistency for large scale image search,” in Computer Vision\n– ECCV 2008, D. Forsyth, P. Torr, and A. Zisserman, Eds. Berlin,\nHeidelberg: Springer Berlin Heidelberg, 2008, pp. 304–317.\n[15] C. Wengert, M. Douze, and H. J ´egou, “Bag-of-colors for improved\nimage search,” in Proceedings of the 19th ACM International\nConference on Multimedia, ser. MM ’11. New York, NY , USA: ACM,\n2011, pp. 1437–1440. [Online]. Available: http://doi.acm.org/10.1145/\n2072298.2072034\n[16] J. Philbin, O. Chum, M. Isard, J. Sivic, and A. Zisserman, “Lost in\nquantization: Improving particular object retrieval in large scale image\ndatabases,” in Computer Vision and Pattern Recognition, 2008. CVPR\n2008. IEEE Conference on. IEEE, 2008, pp. 1–8.\n[17] ——, “Object retrieval with large vocabularies and fast spatial match-\ning,” in 2007 IEEE conference on computer vision and pattern recog-\nnition. IEEE, 2007, pp. 1–8.\n[18] V . R. Chandrasekhar, D. M. Chen, S. S. Tsai, N.-M. Cheung, H. Chen,\nG. Takacs, Y . Reznik, R. Vedantham, R. Grzeszczuk, J. Bachet al., “The\nstanford mobile visual search data set,” in Proceedings of the second\nannual ACM conference on Multimedia systems, 2011, pp. 117–122.\n[19] S. Chatzichristoﬁs, C. Iakovidou, Y . Boutalis, and E. Angelopoulou,\n“Mean normalized retrieval order (mnro): a new content-\nbased image retrieval performance measure,” Multimedia Tools\nand Applications , pp. 1–32, 2012. [Online]. Available:\nhttp://dx.doi.org/10.1007/s11042-012-1192-z\n[20] S. Patil and S. Talbar, “Content based image retrieval using various\ndistance metrics,” in Data Engineering and Management, R. Kannan\nand F. Andres, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg,\n2012, pp. 154–161.\n[21] T. Salimans and D. P. Kingma, “Weight normalization: A simple\nreparameterization to accelerate training of deep neural networks,” in\nAdvances in neural information processing systems, 2016, pp. 901–909.\n[22] S. Wu, G. Li, L. Deng, L. Liu, D. Wu, Y . Xie, and L. Shi, “ l1-norm\nbatch normalization for efﬁcient training of deep neural networks,” IEEE\ntransactions on neural networks and learning systems, vol. 30, no. 7,\npp. 2043–2051, 2018.\n[23] Z. Chen, J. Lin, V . Chandrasekhar, and L.-Y . Duan, “Gated square-root\npooling for image instance retrieval,” in 2018 25th IEEE International\nConference on Image Processing (ICIP). IEEE, 2018, pp. 1982–1986.\n[24] A. Gordo, J. Almaz ´an, J. Revaud, and D. Larlus, “Deep image retrieval:\nLearning global representations for image search,” in European confer-\nence on computer vision. Springer, 2016, pp. 241–257.\n[25] Z. Zhang, Y . Xie, W. Zhang, and Q. Tian, “Effective image retrieval\nvia multilinear multi-index fusion,” IEEE Transactions on Multimedia,\n2019.\n[26] A. S. Razavian, J. Sullivan, S. Carlsson, and A. Maki, “Visual instance\nretrieval with deep convolutional networks,” ITE Transactions on Media\nTechnology and Applications, vol. 4, no. 3, pp. 251–258, 2016.\n[27] G. Tolias, Y . S. Avrithis, and H. J ´egou, “Image search with selective\nmatch kernels: Aggregation across single and multiple images,” Inter-\nnational Journal of Computer Vision, vol. 116, no. 3, pp. 247–261, 2016.\n[28] L. Zheng, S. Wang, and Q. Tian, “Coupled binary embedding for large-\nscale image retrieval,” Image Processing, IEEE Transactions on, vol. 23,\nno. 8, pp. 3368–3380, 2014.\n[29] G. Tolias, R. Sicre, and H. J ´egou, “Particular object retrieval with inte-\ngral max-pooling of cnn activations,” arXiv preprint arXiv:1511.05879,\n2015.\n[30] Y . Kalantidis, C. Mellina, and S. Osindero, “Cross-dimensional weight-\ning for aggregated deep convolutional features,” in European conference\non computer vision. Springer, 2016, pp. 685–701.\n[31] J. Yue-Hei Ng, F. Yang, and L. S. Davis, “Exploiting local features\nfrom deep networks for image retrieval,” in Proceedings of the IEEE\nconference on computer vision and pattern recognition workshops, 2015,\npp. 53–61.\n[32] H. J ´egou, M. Douze, and C. Schmid, “Improving bag-of-features for\nlarge scale image search,” vol. 87, no. 3, 2010, pp. 316–336. [Online].\nAvailable: https://doi.org/10.1007/s11263-009-0285-2\n[33] J. Wan, D. Wang, S. C. H. Hoi, P. Wu, J. Zhu, Y . Zhang, and J. Li, “Deep\nlearning for content-based image retrieval: A comprehensive study,” in\nProceedings of the 22nd ACM international conference on Multimedia,\n2014, pp. 157–166.\n[34] B. Cao, A. Araujo, and J. Sim, “Unifying deep local and global\nfeatures for image search,” in European Conference on Computer Vision.\nSpringer, 2020, pp. 726–743.\n[35] D. Varga and T. Szir ´anyi, “Fast content-based image retrieval using\nconvolutional neural network and hash function,” in 2016 IEEE inter-\nnational conference on systems, man, and cybernetics (SMC). IEEE,\n2016, pp. 002 636–002 640.\n[36] W.-L. Zhao, H. J ´egou, and G. Gravier, “Oriented pooling for dense and\nnon-dense rotation-invariant features,” 2013.\n[37] S. A. Vassou, N. Anagnostopoulos, K. Christodoulou, A. Amanatiadis,\nand S. A. Chatzichristoﬁs, “Como: a scale and rotation invariant compact\ncomposite moment-based descriptor for image retrieval,” Multimedia\nTools and Applications, pp. 1–24, 2018.\n[38] A. Sharif Razavian, H. Azizpour, J. Sullivan, and S. Carlsson, “Cnn\nfeatures off-the-shelf: an astounding baseline for recognition,” in Pro-\nceedings of the IEEE conference on computer vision and pattern\nrecognition workshops, 2014, pp. 806–813.\n[39] Y . Gong, L. Wang, R. Guo, and S. Lazebnik, “Multi-scale orderless\npooling of deep convolutional activation features,” in Computer Vision -\nECCV 2014 - 13th European Conference, Zurich, Switzerland, Septem-\nber 6-12, 2014, Proceedings, Part VII, 2014, pp. 392–407.\n[40] M. Paulin, M. Douze, Z. Harchaoui, J. Mairal, F. Perronnin, and\nC. Schmid, “Local convolutional features with unsupervised training for\nimage retrieval,” in 2015 IEEE International Conference on Computer\nVision, ICCV 2015, Santiago, Chile, December 7-13, 2015, 2015, pp.\n91–99.\n[41] A. Babenko, A. Slesarev, A. Chigorin, and V . Lempitsky, “Neural\ncodes for image retrieval,” in European conference on computer vision.\nSpringer, 2014, pp. 584–599.\n[42] M. Paulin, M. Douze, Z. Harchaoui, J. Mairal, F. Perronin, and\nC. Schmid, “Local convolutional features with unsupervised training for\nimage retrieval,” in Proceedings of the IEEE international conference\non computer vision, 2015, pp. 91–99.\n[43] C. Iakovidou, N. Anagnostopoulos, M. Lux, K. Christodoulou, Y . S.\nBoutalis, and S. A. Chatzichristoﬁs, “Composite description based on\nsalient contours and color information for CBIR tasks,” IEEE Trans.\nImage Processing, vol. 28, no. 6, pp. 3115–3129, 2019. [Online].\nAvailable: https://doi.org/10.1109/TIP.2019.2894281\n[44] P. Fischer, A. Dosovitskiy, and T. Brox, “Descriptor matching with\nconvolutional neural networks: a comparison to sift,” arXiv preprint\narXiv:1405.5769, 2014.\n[45] S. A. Chatzichristoﬁs and Y . S. Boutalis, “Cedd: Color and edge\ndirectivity descriptor: A compact descriptor for image indexing and\nretrieval,” in ICVS, 2008, pp. 312–322.\n[46] A. Gulli and S. Pal, Deep learning with Keras. Packt Publishing Ltd,\n2017."
}