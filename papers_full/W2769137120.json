{
    "title": "Slim Embedding Layers for Recurrent Neural Language Models",
    "url": "https://openalex.org/W2769137120",
    "year": 2018,
    "authors": [
        {
            "id": "https://openalex.org/A2112430461",
            "name": "Zhongliang Li",
            "affiliations": [
                "Wright State University"
            ]
        },
        {
            "id": "https://openalex.org/A2186866813",
            "name": "Raymond Kulhanek",
            "affiliations": [
                "Wright State University"
            ]
        },
        {
            "id": "https://openalex.org/A2000707899",
            "name": "Shaojun Wang",
            "affiliations": [
                "Baidu (China)"
            ]
        },
        {
            "id": "https://openalex.org/A2042121132",
            "name": "Yunxin Zhao",
            "affiliations": [
                "University of Missouri"
            ]
        },
        {
            "id": "https://openalex.org/A2097576564",
            "name": "Shuang Wu",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2112430461",
            "name": "Zhongliang Li",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2186866813",
            "name": "Raymond Kulhanek",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2000707899",
            "name": "Shaojun Wang",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2042121132",
            "name": "Yunxin Zhao",
            "affiliations": []
        },
        {
            "id": "https://openalex.org/A2097576564",
            "name": "Shuang Wu",
            "affiliations": []
        }
    ],
    "references": [
        "https://openalex.org/W2152808281",
        "https://openalex.org/W6680532216",
        "https://openalex.org/W6685405536",
        "https://openalex.org/W2510403588",
        "https://openalex.org/W2146502635",
        "https://openalex.org/W6675140444",
        "https://openalex.org/W2119144962",
        "https://openalex.org/W6640598943",
        "https://openalex.org/W2546915671",
        "https://openalex.org/W1632114991",
        "https://openalex.org/W2171928131",
        "https://openalex.org/W2153579005",
        "https://openalex.org/W2120861206",
        "https://openalex.org/W2574772032",
        "https://openalex.org/W2024592335",
        "https://openalex.org/W2116594867",
        "https://openalex.org/W2998704965",
        "https://openalex.org/W1899794420",
        "https://openalex.org/W2519314406",
        "https://openalex.org/W4294555862",
        "https://openalex.org/W2964299589",
        "https://openalex.org/W2100714283",
        "https://openalex.org/W1800356822",
        "https://openalex.org/W2259472270",
        "https://openalex.org/W2595715041",
        "https://openalex.org/W2951900777",
        "https://openalex.org/W2952432176",
        "https://openalex.org/W4297813615",
        "https://openalex.org/W2124807415",
        "https://openalex.org/W4294170691",
        "https://openalex.org/W2952899695",
        "https://openalex.org/W2951714314",
        "https://openalex.org/W2949563612",
        "https://openalex.org/W1591801644",
        "https://openalex.org/W2175585630",
        "https://openalex.org/W2950797609",
        "https://openalex.org/W2611669587",
        "https://openalex.org/W2952339051",
        "https://openalex.org/W2172166488",
        "https://openalex.org/W2949059244",
        "https://openalex.org/W2549416390",
        "https://openalex.org/W2951559648",
        "https://openalex.org/W2567070169",
        "https://openalex.org/W1938755728",
        "https://openalex.org/W2963347649",
        "https://openalex.org/W2950075229",
        "https://openalex.org/W2950133940",
        "https://openalex.org/W4285719527",
        "https://openalex.org/W2473934411",
        "https://openalex.org/W4293718192",
        "https://openalex.org/W2161591461"
    ],
    "abstract": "Recurrent neural language models are the state-of-the-art models for language modeling. When the vocabulary size is large, the space taken to store the model parameters becomes the bottleneck for the use of recurrent neural language models. In this paper, we introduce a simple space compression method that randomly shares the structured parameters at both the input and output embedding layers of the recurrent neural language models to significantly reduce the size of model parameters, but still compactly represent the original input and output embedding layers. The method is easy to implement and tune. Experiments on several data sets showthat the new method can get similar perplexity and BLEU score results whileonly using a very tiny fraction of parameters.",
    "full_text": "Slim Embedding Layers for Recurrent Neural Language Models\nZhongliang Li, Raymond Kulhanek\nWright State University\n{li.141, kulhanek.5}@wright.edu\nShaojun Wang\nSV AIL, Baidu Research\nswang.usa@gmail.com\nYunxin Zhao\nUniversity of Missouri\nzhaoy@missouri.edu\nShuang Wu\nYitu. Inc\nshuang.wu@gmail.com\nAbstract\nRecurrent neural language models are the state-of-the-art\nmodels for language modeling. When the vocabulary size\nis large, the space taken to store the model parameters be-\ncomes the bottleneck for the use of recurrent neural language\nmodels. In this paper, we introduce a simple space compres-\nsion method that randomly shares the structured parameters\nat both the input and output embedding layers of the recur-\nrent neural language models to signiﬁcantly reduce the size\nof model parameters, but still compactly represent the origi-\nnal input and output embedding layers. The method is easy to\nimplement and tune. Experiments on several data sets show\nthat the new method can get similar perplexity and BLEU\nscore results while only using a very tiny fraction of parame-\nters.\nIntroduction\nNeural language models are currently the state of the art\nmodel for language modeling. These models encode words\nas vectors (word embeddings) and then feed them into the\nneural network (Bengio, Ducharme, and Vincent 2003). The\nword vectors are normally trained together with the model\ntraining process. In the output layer, the hidden states are\nprojected to a vector with the same size as the vocabulary,\nand then a softmax function translates them into probabili-\nties.\nTraining neural language models is time consuming,\nmainly because it requires estimating the softmax function\nat every time stamp. There have been many efforts that try\nto reduce the time complexity of the training algorithm,\nsuch as hierarchical softmax (Goodman 2001; Kim et al.\n2016), importance sampling (IS) (Bengio and Sen´ecal 2008;\nJozefowicz et al. 2016), and noise contrastive estimation\n(NCE) (Mnih and Teh 2012). It is also desirable to train very\ncompact language models for several reasons: 1. Smaller\nmodels are easier to use and deploy in real world systems.\nIf the model is too large, it is possible that it will need mul-\ntiple server nodes. 2. Mobile devices have limited memory\nand space, which makes it impossible to use large models\nwithout server access. 3. Smaller models also decrease the\ncommunication overhead of distributed training of the mod-\nels.\nCopyright c⃝ 2018, Association for the Advancement of Artiﬁcial\nIntelligence (www.aaai.org). All rights reserved.\nIt has been shown there is signiﬁcant redundancy in the\nparametrization of deep learning models (Denil et al. 2013).\nVarious pruning and parameter reduction methods have been\nproposed. In general, there are two types of neural net-\nwork compression techniques. The ﬁrst one involves retrain-\ning. First a full size model is trained, and its weights are\npruned. Then the model is retrained (Han, Mao, and Dally\n2016). The second one is to encode parameter sharing into\nthe model and directly train the compressed model, such as\nHashNet (Chen et al. 2015) and LightRNN (Li et al. 2016).\nThe approach proposed here belongs to the second type.\nThe input layer and output layer contain the largest por-\ntion of parameters in neural language models since the num-\nber is dependent on the vocabulary size. In this paper, we\nmainly focus on reducing the number of parameters in the\nembedding layers. The main contribution is introducing a\nsimple space efﬁcient model compression method that ran-\ndomly shares structured parameters, and can be used in both\nthe input layer and the output layer. The method is easy to\nimplement and tune. It can also be viewed as a regulariza-\ntion that leads to improved performance on perplexity and\nBLEU scores in certain cases.\nRelated Work\nThere are many efforts that improve the space efﬁciency of\nneural language models. Kim et al. (2016) works with char-\nacter level input, and combines convolutional neural net-\nworks (CNN) with highway networks to reduce the number\nof parameters. And later Jozefowicz et al. (2016) extends the\nCNN embedding idea to the output layer, comes up with a\nnew CNN softmax layer, and also scales the method to a one\nbillion word corpus (Chelba et al. 2013). Ling et al. (2015)\nintroduces a model for constructing vector representations of\nwords by composing characters using bidirectional LSTMs.\nAlthough the above models use the character level infor-\nmation to reduce the model size of embedding layers, there\nhave been many approaches that try to reduce the param-\neters without using this additional information. Mikolov et\nal. (2011) introduces the compression layer between the re-\ncurrent layer and the output layer, which not only reduces\nthe number of parameters in the output layer, but also re-\nduces the time complexity of training and inference. Grave\net al. (2016) improves the hierarchical softmax by assigning\nword clusters with different sizes of embeddings; it tries to\nThe Thirty-Second AAAI Conference\non Artificial Intelligence (AAAI-18)\n5220\nutilize the power of GPU computation more efﬁciently, but\nalso reduces the number of parameters signiﬁcantly.\nChen et al. (2016) proposes to represent rare words by\nsparse linear combinations of common already learned ones.\nThe sparse code and embedding for each word are precom-\nputed and are ﬁxed during the language model training pro-\ncess. The method we propose here is different in that the\ncodes for each word are selected randomly and the embed-\ndings are learned in the process of model training and the\nsub-vectors are concatenated together to form the ﬁnal word\nembedding.\nLi et al. (2016) uses 2-component shared embedding as\nthe word representation in LightRNN. It uses parameter\nsharing to reduce the model size, which is similar to the\nmethod proposed here. But the two components for a word\nare fed into RNN in two different time stamps in LightRNN.\nThe method proposed here is most similar to the model used\nin Suzuki and Nagata (2016), but their work is not about\nlanguage modeling.\nThe model proposed here can be understood as introduc-\ning random weight sharing into the embedding layers for\nlanguage models, which shares the same idea with HashNet\n(Chen et al. 2015), but uses a different sharing scheme.\nRandom Parameter Sharing at Input and\nOutput Embedding Layers\nWe use deep Long short-term memory (LSTM) as our neural\nlanguage model. In each time stamp t, the word vector h0\nt is\nused as the input. We use subscripts to denote time stamps\nand superscripts to denote layers. Assume L is the number\nof layers in deep LSTM neural language model, then h\nL\nt is\nused to predict the next word. The dynamics of an LSTM\ncell, following (Zaremba, Sutskever, and Vinyals 2014), are:\n⎛\n⎜⎝\ni\nf\no\ng\n⎞\n⎟⎠ =\n⎛\n⎜⎝\nsigm\nsigm\nsigm\ntanh\n⎞\n⎟⎠T\n2n,4n\n(\nD(hl−1\nt )\nhl\nt−1\n)\ncl\nt = f ⊙ cl\nt−1 +i ⊙ g\nhl\nt = o ⊙ tanh(cl\nt)\nIn the formula, ⊙ is element-wise multiplication, Tn,m :\nRn → Rm is an afﬁne transform, and D is the dropout oper-\nator that sets a random subset of its argument to zero.\nAssuming the vocabulary size is V , and both the word\nvector size and the number of hidden nodes in the recur-\nrent hidden states are N, then the total number of pa-\nrameters in the embedding layer is N ∗ V . The embed-\nding layers of character level models (Kim et al. 2016;\nLing et al. 2015) are related in that the word embeddings be-\ntween different words are dependent on each other. Updating\nthe word embedding for each word will affect the embed-\ndings for other words. Dependent word embedding helps re-\nduce the number of parameters tremendously. In this paper,\nwe design a simple model compression method that allows\nthe input word embedding layer and softmax output layer to\nshare weights randomly to effectively reduce the model size\nand yet maintain the performance.\nw1\nw2\nw3\nw4\na1\na2\na3\nw4\nw3\nw2\nw1\na3\na2\na1\na1\na1\na3\na3\na2\nNew embedding\nOriginal embedding\nFigure 1: Toy example of the original embedding layer and\nnew embedding layer. In this paper the concatenated word\nvector has the same size as the original one. The assign-\nment of sub-vectors to each word are randomly selected and\nﬁxed before the training process. The parameters in the sub-\nvectors are updated in the training process.\nCompressing Input Embedding Layer\nAssume we divide the input word embedding vector h0\nt ∈\nRN into K even parts, such that the input representation\nof the current word is the concatenation of the K parts\nh0\nt =[ a1, ..., aK ], and each part is a sub-vector with N\nK\nparameters. For a vocabulary of V words, the input word\nembedding matrix thus is divided into V ∗ K sub-vectors,\nand we map these sub-vectors into M sub-vectors randomly\nbut as uniformly as possible in the following manner: we\ninitialize a list L with K ∗V elements, which contains\nK∗V\nM\ncopies of the sequence [1...M]. Then the list is shufﬂed with\nthe Fisher-Yates shufﬂe algorithm and the ith word’s vec-\ntor is formed with [aLK∗(i−1)+1 ...aLK∗i ]. This helps to make\nsure that the numbers of times each sub-vector is used are\nnearly equal.\nIn this way, the total number of parameters in the input\nembedding layer isM ∗\nN\nK instead of V ∗N, which makes the\nnumber of parameters independent from the size of vocabu-\nlary. The K sub-vectors for each word are drawn randomly\nfrom the set of M sub-vectors.\nFor example, as shown in Fig 1, if in total there are\nfour words in the corpus ( V =4), and each word vector\nis formed by two sub-vectors ( K=2), and therefore there\nare in total eight sub-vectors in the input embedding ma-\ntrix, assume that these eight sub-vectors are mapped into\nthree sub-vectors ( M=3), which are indexed as a\ni,i ∈\n(1, 2, 3). Then the word vectors can be assigned like this:\n[a1,a 2], [a1,a 3], [a2,a 3], [a3,a 1]. In this example, the com-\npression ratio is 3/8, and the number of parameters in the\nnew embedding layer size is only 37.5%of the original one.\nIf the number of sub-vectors is large enough and none of\n5221\nthe word vectors share sub-vectors, then the input embed-\ndings will become equivalent to normal word embeddings.\nWe use stochastic gradient descent with backpropagation\nthrough time to train our compressed neural language model.\nDuring each iteration of the training process, all words that\nshare the same sub-vectors with the current word will be af-\nfected. If we assume that the number of words that share\nsub-vectors is small, then only a small number of word em-\nbedding vectors will be affected.\nCompressing Output Embedding Layer\nThe output matrix can be compressed in a similar way. In\nthe output layer, the context vectorh is projected to a vector\nwith the same size as the vocabulary, such that for each word\nw, we compute z\nw = hT ew, which is then normalized by a\nsoftmax non-linearity: p(w)= exp(zw)\nΣw′∈V exp(zw′). If we treat\neach ew as a word embedding, we can then use a similar pa-\nrameter sharing technique to the one used in the input layer,\nand let e\nw =[ aw1, ..., awK ] where ai are sub-vectors.\nThe structured shared parameters in the output layer\nmake it possible to speed up the computation during both\ntraining and inference. Let S be K sets of sub-vectors,\nS\n1,S 2, ..., SK , such that Si ∩ Sj = ∅, ∀i ̸= j. The ﬁrst\nsub-vector in each word’s embedding will be selected from\nS\n1, the second from S2, and so on. If we also divide the\ncontext vector into K even parts h =[ h1, ..., hK ], then\nzw =Σ i=K\ni=1 hT\ni awi. We can see that hi will only be mul-\ntiplied by the sub-vectors in Si. Because many words share\nthe same sub-vectors, for each uniquehiawi, we just need to\ncompute the partial dot product once. So in order to evaluate\nall z\nw, we need two steps with dynamic programming:\n1) We ﬁrst compute all the uniquehiawi values. It is easy\nto see that the total number of unique dot product expres-\nsions will be the same as the total number of sub-vectors.\nThe complexity of this step is O(\nMH\nK ), where M is the total\nnumber of sub-vectors. This step can be done with K dense\nmatrix multiplications.\n2) Each zw is the sum of K partial dot products. Because\nthe dot product results are already known from the ﬁrst step,\nall we need to do is sum the K values for each word. The\ncomplexity of this step is O(VK ).\nIn summary, the complexity of evaluating the new soft-\nmax layer will be O(\nMH\nK +VK ), instead of O(VH )for the\noriginal softmax layer. The inference algorithm is listed in\nAlgorithm 1.\n1 Divide the hidden vector h into K even parts;\n2 Evaluate the partial dot products for each (hidden\nstate sub-vector, embedding) pair and cache the\nresults;\n3 Sum the result for each word according to the\nsub-vector mapping table;\nAlgorithm 1:Inference Algorithm\nConnection to HashNet, LightRNN and\nCharacter Aware Language model\nThe most similar work to our method is the HashNet de-\nscribed in Chen et al. (2015). In HashNet, all elements in a\nparameter matrix are mapped into a vector through a hash\nfunction. However in our approach, we randomly share sub-\nvectors instead of single elements. There are three advan-\ntages in our approach, 1) Our method is more cache friendly:\nsince the elements of the sub-vectors are adjacent, it is very\nlikely that they will be in the same cache line, thus it ac-\ncesses the memory more efﬁciently than HashNet, where the\nﬁrst step of the output layer computation is K dense ma-\ntrix multiplications. 2) Our method actually decreases the\nmemory usage during training. When training HashNet on\nGPUs, the parameter mapping is usually cached, thus saving\nno space. With our method, it’s possible to train models with\n4096 hidden states on the BillionW dataset using one GPU,\nin which case the uncompressed output embedding is more\nthan 12GB when each number uses 32 bits. 3) As shown\nin the previous section, it is possible to use dynamic pro-\ngramming to reduce the time complexity of the output layer\nwith a simple modiﬁcation. If the sub-vector’s size is equal\nto 1 (K=H), and the random shufﬂe is replaced with the hash\nfunction, then HashNet could be treated as a special case of\nour model.\nOur approach differs from LightRNN (Li et al. 2016) in\nthat our approach is able to control the compression ratio\nto any arbitrary value, while LightRNN can only compress\nat the rate of square or cube root of vocabulary size, which\ncould be too harsh in practical applications.\nThe character-aware language model can be explained as\na parameter sharing word-level language model, where each\nword shares the same character embedding vectors and a\nconvolutional neural network (CNN). Conversely this model\ncan also be explained as a simpliﬁed character-aware lan-\nguage model from Kim et al. (2016) and Jozefowicz et al.\n(2016). In the character-aware language model, each char-\nacter in a word is ﬁrst encoded as a character embedding,\nand then it uses a CNN to extract character n-gram features,\nand then these features are concatenated and fed through\nseveral layers of highway network to form the ﬁnal word\nembedding. In this model, if we treat the sequence of sub-\nvector ids (virtual characters) as each word’s representation,\nthe word embedding then can be treated as concatenated un-\nigram character feature vectors. The advantage of using the\nreal character representation is that it can deal with out-of-\nvocabulary words nicely, but the cost is that the model is\nmore complicated and to speed up inference, it needs to pre-\ncompute the word embeddings for the words, so it couldn’t\nstay in its compact form during inference. The model pro-\nposed here is much simpler, and easier to tune. And dur-\ning inference, it uses much less space and can even decrease\nthe complexity of inference. With the same space constraint,\nthis will enable us to train language models with even larger\nnumber of hidden states.\n5222\nTable 1: Corpus Statistics\nDataset #Token V ocabulary Size\nPTB 1M 10K\n44M 44M 60K\nWMT12 58M 35K\nACLW-Spanish 56M 152K\nACLW-French 57M 137K\nACLW-English 20M 60K\nACLW-Czech 17M 206K\nACLW-German 51M 339K\nACLW-Russian 25M 497K\nBillionW 799M 793K\nExperiments\nWe test our method of compressing the embedding lay-\ners on various publicly available standard language model\ndata sets ranging from the smallest corpus, PTB (Marcus,\nMarcinkiewicz, and Santorini 1993), to the largest, Google’s\nBillionW corpus (Chelba et al. 2013). 44M is the 44 mil-\nlion word subset of the English Gigaword corpus (Graff and\nCieri 2003) used in Tan et al. (2012). The description of the\ndatasets is listed in Table 1.\nThe weights are initialized with uniform random values\nbetween -0.05 and 0.05. Mini-batch stochastic gradient de-\ncent (SGD) is used to train the models. For all the datasets\nexcept the 44M and BillionW corpora, all the non-recurrent\nlayers except the word embedding layer to the LSTM layer\nuse dropout. Adding dropout did not improve the results for\n44M and BillionW, and so the no-dropout results are shown.\nWe use Torch to implement the models, and the code is\nbased on the code open sourced from Kim et al. (2016). The\nmodels are trained on a single GPU. In the experiments, the\ndimension of the embeddings is the same as the number of\nhidden states in the LSTM model. Perplexity (PPL) is used\nto evaluate the model performance. Perplexity over the test\nset with length of T is given by\nPPL =e x p (−1\nT\nT∑\ni=1\nlog(p(wi|w<i)).\nWhen counting the number of parameters, for convenience,\nwe don’t include the mapping table that maps each word to\nits sub-vector ids. In all the experiments, the mapping table\nis ﬁxed before the training process. For particularly large\nvalues of K, the mapping table’s size could be larger than\nthe size of parameters in its embedding layer. It is possible to\nreplace the mapping table with hash functions that are done\nin HashNet (Chen et al. 2015). We added end of sentence to-\nkens to all the datasets with the exception of the experiments\nin table 4. Those experiments omit the end of sentence token\nfor comparison with other baselines.\nSimilar to the work in Jozefowicz et al. (2016), compress-\ning the output layers turns out to be more challenging. We\nﬁrst report the results when just compressing the input layer,\nand then report the results when both input layers and output\nlayers are compressed. In the end, we do reranking experi-\nments for machine translation and also compare the compu-\ntation efﬁciency of these models.\nTable 2: Test perplexities on PTB with 300 hidden nodes,\nK=10\nModel Dropout PPL Size\nNE 0 89.54 1\nNE 0.1 88.56 1\nNE 0.2 88.33 1\nNE 0.5 91.10 1\nSE (M=20K) 0 89.34 20%\nSE (M=20K) 0.1 88.19 20%\nSE (M=10K) 0 89.06 10%\nSE (M=10K) 0.1 88.37 10%\nSE (M=6.25K) 0 89.00 6.25%\nSE (M=5K) 0 89.54 5%\nTable 3: Test perplexities on PTB with 650 hidden nodes,\nK=10\nModel Dropout PPL Size\nNE 0 85.33 1\nNE 0.1 82.59 1\nNE 0.2 83.51 1\nNE 0.5 82.91 1\nSE (M=10K) 0 82.14 10%\nSE (M=5K) 0 82.41 5%\nSE (M=5K) 0.1 81.14 5%\nSE (M=1K) 0 82.62 1%\nExperiments on Slim Embedding for Input Layer\nFor the input layer, we compare two cases. The ﬁrst case is\nthe one just using the original word embedding (NE), a sec-\nond case is the one compressing the input embedding layer\nwith different ratio (SE). The ﬁrst case is the uncompressed\nmodel that uses the same number of hidden states and uses\nthe same full softmax layer and has much larger number\nof parameters. We ﬁrst report the results on Penn Treebank\n(PTB) dataset. For PTB, the vocabulary size is 10K, and has\n1 million words.\nTables 2 and 3 show the experimental results on the PTB\ncorpus when using 300 and 650 hidden nodes respectively.\nIn both tables, the column Dropout denotes the dropout\nprobability that is used from the input embedding layer to\nthe LSTM layer; all other non-recurrent layers use dropout\nprobability of 0.5 in both NE and SE. Size is the number of\nparameters in the compressed input word embedding layer\nrelative to the original input word embedding. The experi-\nment on the input layer shows the compression of the in-\nput layer has almost no inﬂuence on the performance of the\nmodel. The SE model with 650 hidden states manages to\nkeep the PPL performance almost unchanged even when the\ninput layer just uses 1% of trainable parameters. And when\nthe input layer is trained with dropout, it gives better results\nthan the baseline.\nFig 2 and Fig 3 are the results on 44M giga world sub-\ncorpus where 512 hidden notes are used in the two layer\nLSTM model. Baseline denotes the result using the origi-\nnal LSTM model. Fig 2 shows the perplexity results on the\ntest datasets, where we divide each word input embedding\n5223\n0 100 200 300 400 50090\n100\n110\n120\n130\n140\nUncompressed Model\nReciprocal of Compression Rate\nTest PPL\nFigure 2: Test perplexities on 44M with 512 hidden nodes\nand each 512 dimensional input embedding vector is divided\ninto eight parts. Only the input word embedding layer is\ncompressed.\nvector into eight sub-vectors (K =8 ), and vary the number\nof new embedding sub-vectors, M, thus varying the com-\npressed model size, i.e., compression ratio, from1to 1/512.\nWe can see that the perplexity results remain almost the\nsame and are quite robust and insensitive to the compres-\nsion ratio: they decrease slightly to a minimum of 96.30\nwhen the compression ratio is changing from 1 to 1/8, but in-\ncrease slightly to 103.61 when the compression ratio reaches\n1/512. Fig 3 shows the perplexity results where we divide\neach word input embedding vector into different numbers\nof sub-vectors from 1 to 512, and at the same time vary the\nnumber of sub-vectors,M, so as to keep the compression ra-\ntio constant, 1/8 in this case. We can see that the perplexity\nresults remain almost the same, are quite robust and insensi-\ntive to the size of the sub-vector except in the case where\neach word contains only one sub-vector, i.e. K =1 .I n\nthis case, multiple words share identical input embeddings,\nwhich leads to worse perplexity results as we would expect.\nWhen the dimension of input embedding is the same as the\nnumber sub-vectors each embedding has ( K = 512), it can\nbe seen as a HashNet model that uses a different hash func-\ntion; the PPL is 95.73. When we use xxhash\n1 to generate the\nmapping table which is used in HashNet, the PPL is 97.35.\nExperiments on Slim Embedding for Both Input\nand Output Layers\nIn this section we report experimental results when both in-\nput and output layers are compressed using our proposed\napproach.\nFig 4 and Fig 5 are the results on the 44M corpus where\n512 hidden nodes are used in the two layers of the LSTM\nmodel. Uncompressed model denotes the result using the\noriginal LSTM model. Similarly Fig 4 shows the perplexity\nresults where we divide each word input embedding vector\n1https://code.google.com/archive/p/xxhash/\n0 100 200 300 400 50090\n100\n110\n120\n130\n140\nUncompressed Model\nK\nTest PPL\nFigure 3: Test perplexities on 44M with 512 hidden nodes\nwith 1/8 original Size. Only the input word embedding layer\nis compressed.\n0 100 200 300 400 500\n100\n200\n300\n400\nUncompressed Model\nReciprocal of Compression Rate\nTest PPL\nSE\nPM\nSM\nFigure 4: Test perplexities on 44M with 512 hidden nodes\nand each 512 dimensional vector divided into eight parts.\nBoth input and output embedding layers are compressed.\ninto eight sub-vectors (K =8 ), and vary the number of sub-\nvectors, M, thus varying the compression ratio, from 1 to\n1/256. In Fig 4, we also show two other baselines, 1) Regu-\nlar LSTM model but using smaller number of hidden states\n(SM), 2) Regular LSTM model with an additional projection\nlayer (PM): before the embedding is fed into LSTM layer,\nembedding vectors are ﬁrst projected to size 512. Unlike the\ncase when only the input embedding layer is compressed,\nwe can see that the perplexity results become monotoni-\ncally worse when the compression ratio is changed from1to\n1/256. When the compression rate is large, SE’s perplexity\nis lower.\nAgain similarly to the case of only compressing the in-\nput embedding layer, Figure 5 shows the perplexity results\nwhere we divide each word input embedding vector into dif-\nferent numbers of sub-vectors from1to 512, and at the same\ntime vary the size of sub-vectors(M), thus keeping the com-\n5224\nTable 4: PPL results in test set for various linguistic datasets on ACLW datasets. Note that all the SE models just use 300 hidden\nstates. #P means the number of parameters.\nMethod English/#P Russian/#P Spanish/#P French/#P Czech/#P German/#P\nHSM (Kim et al. 2016) 236/25M 353/200M 186/61M 202/56M 701/83M 347/137M\nC-HSM (Kim et al. 2016) 216/20M 313/152M 169/48M 190/44M 578/64M 305/104M\nLightRNN (Li et al. 2016) 191/17M 288/19M 157/18M 176/17M 558/18M 281/18M\nSE 187/7M 274/19M 149/8M 162/12M 528/17M 261/17M\n0 100 200 300 400 500\n100\n120\n140\n160\n180\n200\n220\nUncompressed Model\nK\nTest PPL\nFigure 5: Test perplexities on 44M with 512 hidden nodes\nwhen embedding compressed to 1/8. The whole model size\nis less than 20% of the uncompressed model.\npressing ratio constant, 1/8 in this case. We can see that the\nperplexity results almost remain the same, reach a minimum\nwhen K =4 , and are not sensitive to the size of the sub-\nvector except in the case where each word contains only one\nsub-vector. In that case, multiple words share identical input\nembeddings, which leads to expected bad perplexity results.\nWhen K = 512, the PPL is 111.0, and when using xxhash,\nthe PPL is 110.4. The results are also very close to HashNet.\nGood perplexity results on PTB corpus are reported when\nparameter tying is used at both input and output embedding\nlayers (Inan, Khosravi, and Socher 2016; Zilly et al. 2016).\nHowever we don’t observe further perplexity improvement\nwhen both parameter sharing and tying are used at both input\nand output embedding layers.\nWe next compare our model with LightRNN (Li et al.\n2016), which also focuses on training very compact lan-\nguage models. We also report the best result we have got\non the one billion word dataset. SE denotes the results us-\ning compressed input and output embedding layers. Table 4\nshows the results of our model. Because these datasets have\nvery different vocabulary sizes, we use different compres-\nsion rates for the models in order to make the model smaller\nthan LightRNN, yet still have better performance. In these\nexperiments, we change to NCE training and tune the pa-\nrameters with the Adagrad (Duchi, Hazan, and Singer 2011)\nalgorithm. NCE helps reduce the memory usage during the\ntraining process and also speeds up the training process.\nIn the one billion word experiments, the total memory\nduring training used on the GPU is about 7GB, and is\nsmaller if a larger compression rate is used. We use a ﬁxed\nsmoothed unigram distribution (unigram distribution raised\nto 0.75) as the noise distribution. Table 5 shows our results\non the one billion word dataset. For the two layer model, the\ncompression rate for the input layer is 1/32 and the output\nlayer is 1/8, and the total number of parameters is 322 mil-\nlion. For the three layer model, the compression rates for the\ninput and output layer are 1/32 and 1/16, and the total num-\nber of parameters is 254 million. Both experiments using\nNCE take about seven days of training on a GTX 1080 GPU.\nJozefowicz et al. (2016) suggests importance sampling (IS)\ncould perform better than the NCE model, so we ran the ex-\nperiment using IS and we used 4000 noise samples for each\nmini-batch. The PPL decreased to 38.3 after training for 8\ndays. As far as we know, the 3 layer model is the most com-\npact recurrent neural language model that has a perplexity\nbelow 40 on this dataset. The LSTM-2048-512 shown in Ta-\nble 5 uses projection layers; it has many more parameters\nand also a higher PPL.\nMachine Translation Reranking Experiment\nWe want to see whether the compressed language model\nwill affect the performance of machine translation rerank-\ning. In this experiment, we used the Moses toolkit (Koehn\net al. 2007) to generate a 200-best list of candidate transla-\ntions. Moses was conﬁgured to use the default features, with\na 5-gram language model. Both the language and transla-\ntion models were trained using the WMT12 data (Callison-\nBurch et al. 2012), with the Europarl v7 corpus for training,\nnewstest2010 for validation, and newstest2011 for test, all\nlowercased. The scores used for reranking were linear com-\nbinations of the Moses features and the language models.\nZMERT (Zaidan 2009) was used to determine the coefﬁ-\ncients for the features.\nWe trained a two layer LSTM language model with 512\nhidden states, and also a compressed language model that\ncompresses the input layer to 1/8 and output layer to 1/4\nusing NCE. For the baseline, we rerank the n-best list using\nonly the Moses feature scores, including a 5-gram model\nwhich has a perplexity of 251.7 over the test data, which\nyields a BLEU score of 25.69. When we add the normal\nLSTM language model, having a perplexity of 124 on test\ndata, as another feature, the BLEU score changed to 26.11,\nand for the compressed language model, having a perplexity\n134 on test data, the BLEU score changed to 26.25, which\nonly has a small difference with the normal LSTM language\nmodel.\n5225\nTable 5: Perplexity results for single models on BillionW. Bold number denotes results on a single GPU.\nModel Perplexity #P[Billions]\nInterpolated Kneser-Ney 5-gram (Chelba et al. 2013) 67.6 1.76\n4-layer IRNN-512 (Le, Jaitly, and Hinton 2015) 69.4\nRNN-2048 + BlackOut sampling (Ji et al. 2015) 68.3\nRNN-1024 + MaxEnt 9-gram (Chelba et al. 2013) 51.3 20\nLSTM-2048-512 (Grave et al. 2016) 43.7 0.83\nLightRNN (Li et al. 2016) 66.0 0.041\nLSTM-2048-512 (Jozefowicz et al. 2016) 43.7 0.83\n2-layer LSTM-8192-1024 (Jozefowicz et al. 2016) 30.6 1.8\n2-layer LSTM-8192-1024 + CNN inputs (Jozefowicz et al. 2016) 30.0 1.04\n2-layer LSTM-8192-1024 + CNN inputs + CNN softmax (Jozefowicz et al. 2016) 39.8 0.29\nLSTM-2048 Adaptive Softmax (Grave et al. 2016) 43.9 >0.29\n2-layer LSTM-2048 Adaptive Softmax (Grave et al. 2016) 39.8\nGCNN-13 (Dauphin et al. 2016) 38.1\nMOE (Shazeer et al. 2017) 28.0 >4.37\nSE (2-layer 2048 LSTM NCE) 39.9 0.32\nSE (3-layer 2048 LSTM NCE) 39.5 0.25\nSE (3-layer 2048 LSTM IS ) 38.3 0.25\nTable 6: Reranking Experiment\nBaseline NE SE\nPPL 251.7 124.1 134.8\nBLEU 25.69 26.11 26.25\nComputational Efﬁciency\nIn this section we compare the computational efﬁciency be-\ntween the HashNet and SE models. We compare the time\nspent on the output layer for each minibatch on Google’s\nBillionW corpus during inference. Each minibatch contains\n20 words and the number of hidden nodes in LSTM layer is\n2048.\nWe report the time used on both CPU and GPU. Table 7\nshows the inference time usage. All the computations use 32\nbit ﬂoating point numbers. On CPU, HashNet is slower than\nthe normal uncompressed model, mainly because of two\nreasons: 1) The uncompressed model uses optimized ma-\ntrix multiplication subroutines, 2) The hash function used in\nHashNet is cheap, but it still has overhead compared with the\nuncompressed model. The SE model runs faster mainly be-\ncause it uses matrix multiplication subroutines and has lower\ntime complexity with the help of dynamic programming.\nTable 7: Time Usage Comparison\nModel CPU(seconds) GPU (milliseconds)\nUncompressed 2.7 38\nHashNet 80.6 -\nSE 0.7 25\nOn GPU, SE’s time usage is smaller than the uncom-\npressed model when K is small. SE’s inference has two\nsteps, the ﬁrst step is K matrix multiplications, and the sec-\nond step is summing up the partial dot products. In the\nbenchmark, the implementation uses Torch. A more opti-\nmized implementation is possible.\nHashNet’s focus is mainly on reducing the space com-\nplexity. If we want to make it faster, we could just cache\nthe full matrix from HashNet, whose speed is the same as\nthe uncompressed model. There are many techniques that\ncould be used to make the inference faster, such as using\nlow-precision ﬂoating point calculations. Because the model\nstays in its compressed form, the memory usage of SE dur-\ning inference is much lower than the baseline.\nConclusion\nIn this paper, we introduced a space efﬁcient structured pa-\nrameter sharing method to compress word embedding lay-\ners. Even through the sub-vectors are randomly assigned\nand ﬁxed during training, experiments on several datasets\nshow good results. A better data-driven approach could pre-\ntrain an embedding matrix using Skipgram (Mikolov et al.\n2013) to get an estimate of sub-vectors, then use a clus-\ntering method to assign the sub-vectors, and ﬁnally run the\ntraining algorithm proposed in this paper. Embedding layers\nhave been used in many tasks of natural language process-\ning, such as sequence to sequence models for neural ma-\nchine translation and dialog systems. It would be useful to\nexplore the results of using this technique in these models.\nAcknowledgments\nThanks a lot for the code and processed data from Kim et al.\n(2016), and also thanks the insightful comments and sugges-\ntions from anonymous reviewers. This research is supported\nin part by AFOSR under grant FA9550-10-1-0335, NSF un-\nder grant IIS:RI-small 1218863, DoD under grant FA2386-\n13-1-3023, and a Google research award. We would like to\nthank the Ohio Supercomputer Center for an allocation of\ncomputing time as well as NVIDIA Corporation for the do-\nnation of Tesla K40 GPUs to make this research possible.\n5226\nReferences\nBengio, Y ., and Sen ´ecal, J.-S. 2008. Adaptive impor-\ntance sampling to accelerate training of a neural probabilis-\ntic language model. IEEE Transactions on Neural Networks\n19(4):713–722.\nBengio, Y .; Ducharme, R.; and Vincent, P. 2003. A neural\nprobabilistic language model. Journal of Machine Learning\nResearch 3:1137–1155.\nCallison-Burch, C.; Koehn, P.; Monz, C.; Post, M.; Sori-\ncut, R.; and Specia, L., eds. 2012. WMT ’12: Proceedings\nof the Seventh W orkshop on Statistical Machine Transla-\ntion. Stroudsburg, PA, USA: Association for Computational\nLinguistics. http://www.statmt.org/wmt12/translation-task.\nhtml.\nChelba, C.; Mikolov, T.; Schuster, M.; Ge, Q.; Brants, T.;\nKoehn, P.; and Robinson, T. 2013. One billion word bench-\nmark for measuring progress in statistical language model-\ning. arXiv preprint arXiv:1312.3005.\nChen, W.; Wilson, J.; Tyree, S.; Weinberger, K.; and Chen,\nY . 2015. Compressing neural networks with the hashing\ntrick. In The 32nd International Conference on Machine\nLearning (ICML), 2285–2294.\nChen, Y .; Mou, L.; Xu, Y .; Li, G.; and Jin, Z. 2016. Com-\npressing neural language models by sparse word representa-\ntions. The 54th Annual Meeting of the Association for Com-\nputational Linguistics, (ACL)226–235.\nDauphin, Y . N.; Fan, A.; Auli, M.; and Grangier, D. 2016.\nLanguage modeling with gated convolutional networks.\narXiv preprint arXiv:1612.08083.\nDenil, M.; Shakibi, B.; Dinh, L.; de Freitas, N.; et al. 2013.\nPredicting parameters in deep learning. In Advances in Neu-\nral Information Processing Systems, 2148–2156.\nDuchi, J.; Hazan, E.; and Singer, Y . 2011. Adaptive subgra-\ndient methods for online learning and stochastic optimiza-\ntion. Journal of Machine Learning Research12(Jul):2121–\n2159.\nGoodman, J. 2001. Classes for fast maximum entropy\ntraining. In IEEE International Conference on Acoustics,\nSpeech, and Signal Processing, (ICASSP), volume 1, 561–\n564. IEEE.\nGraff, D., and Cieri, C. 2003. English gigaword ldc2003t05.\nLinguistic Data Consortium, Philadelphia.\nGrave, E.; Joulin, A.; Ciss ´e, M.; Grangier, D.; and J ´egou,\nH. 2016. Efﬁcient softmax approximation for gpus. arXiv\npreprint arXiv:1609.04309.\nHan, S.; Mao, H.; and Dally, W. J. 2016. Deep compression:\nCompressing deep neural networks with pruning, trained\nquantization and huffman coding. International Conference\non Learning Representations.\nInan, H.; Khosravi, K.; and Socher, R. 2016. Tying word\nvectors and word classiﬁers: A loss framework for language\nmodeling. arXiv preprint arXiv:1611.01462.\nJi, S.; Vishwanathan, S.; Satish, N.; Anderson, M. J.; and\nDubey, P. 2015. Blackout: Speeding up recurrent neural net-\nwork language models with very large vocabularies. arXiv\npreprint arXiv:1511.06909.\nJozefowicz, R.; Vinyals, O.; Schuster, M.; Shazeer, N.; and\nWu, Y . 2016. Exploring the limits of language modeling.\narXiv preprint arXiv:1602.02410.\nKim, Y .; Jernite, Y .; Sontag, D.; and Rush, A. M. 2016.\nCharacter-aware neural language models. The 30th AAAI\nConference on Artiﬁcial Intelligence (AAAI).\nKoehn, P.; Hoang, H.; Birch, A.; Callison-Burch, C.; Fed-\nerico, M.; Bertoldi, N.; Cowan, B.; Shen, W.; Moran, C.;\nZens, R.; et al. 2007. Moses: Open source toolkit for statis-\ntical machine translation. In Proceedings of the 45th annual\nmeeting of the ACL on interactive poster and demonstration\nsessions, 177–180. Association for Computational Linguis-\ntics.\nLe, Q. V .; Jaitly, N.; and Hinton, G. E. 2015. A simple\nway to initialize recurrent networks of rectiﬁed linear units.\narXiv preprint arXiv:1504.00941.\nLi, X.; Qin, T.; Yang, J.; Hu, X.; and Liu, T. 2016.\nLightrnn: Memory and computation-efﬁcient recurrent neu-\nral networks. In Advances In Neural Information Processing\nSystems, 4385–4393.\nLing, W.; Lu\n´ıs, T.; Marujo, L.; Astudillo, R. F.; Amir,\nS.; Dyer, C.; Black, A. W.; and Trancoso, I. 2015.\nFinding function in form: Compositional character models\nfor open vocabulary word representation. arXiv preprint\narXiv:1508.02096.\nMarcus, M. P.; Marcinkiewicz, M. A.; and Santorini, B.\n1993. Building a large annotated corpus of english: The\npenn treebank. Computational linguistics 19(2):313–330.\nMikolov, T.; Kombrink, S.; Burget, L.; ˇCernock`y, J.; and\nKhudanpur, S. 2011. Extensions of recurrent neural net-\nwork language model. In 2011 IEEE International Confer-\nence on Acoustics, Speech and Signal Processing (ICASSP),\n5528–5531. IEEE.\nMikolov, T.; Sutskever, I.; Chen, K.; Corrado, G. S.; and\nDean, J. 2013. Distributed representations of words and\nphrases and their compositionality. In Burges, C. J. C.;\nBottou, L.; Welling, M.; Ghahramani, Z.; and Weinberger,\nK. Q., eds., Advances in Neural Information Processing Sys-\ntems 26. Curran Associates, Inc. 3111–3119.\nMnih, A., and Teh, Y . W. 2012. A fast and simple al-\ngorithm for training neural probabilistic language models.\nIn The 29th International Conference on Machine Learning\n(ICML), 1751–1758.\nShazeer, N.; Mirhoseini, A.; Maziarz, K.; Davis, A.; Le, Q.;\nHinton, G.; and Dean, J. 2017. Outrageously large neu-\nral networks: The sparsely-gated mixture-of-experts layer.\narXiv preprint arXiv:1701.06538.\nSuzuki, J., and Nagata, M. 2016. Learning compact neural\nword embeddings by parameter space sharing. In Kamb-\nhampati, S., ed., The Twenty-Fifth International Joint Con-\nference on Artiﬁcial Intelligence, (IJCAI), 2046–2052. IJ-\nCAI/AAAI Press.\nTan, M.; Zhou, W.; Zheng, L.; and Wang, S. 2012. A\nscalable distributed syntactic, semantic, and lexical language\nmodel. Computational Linguistics 38(3):631–671.\n5227\nZaidan, O. F. 2009. Z-MERT: A fully conﬁgurable open\nsource tool for minimum error rate training of machine\ntranslation systems. The Prague Bulletin of Mathematical\nLinguistics 91:79–88.\nZaremba, W.; Sutskever, I.; and Vinyals, O. 2014. Re-\ncurrent neural network regularization. arXiv preprint\narXiv:1409.2329.\nZilly, J.; Srivastava, R.; Koutnk, J.; and Schmidhuber,\nJ. 2016. Recurrent highway networks. arXiv preprint\narXiv:1607.03474.\n5228"
}